<html>
<head>
<title>Linear Regression with Only Python and Numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">仅使用 Python 和 Numpy 的线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-with-python-and-numpy-25d0e1dd220d?source=collection_archive---------11-----------------------#2020-01-08">https://towardsdatascience.com/linear-regression-with-python-and-numpy-25d0e1dd220d?source=collection_archive---------11-----------------------#2020-01-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2347" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用 Numpy 和 Python 写一个机器学习模型</h2></div><p id="ef70" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们将看到如何在不使用任何机器学习库的情况下用 Python 实现线性回归。在<a class="ae lb" href="https://hackerstreak.com/linear-regression/" rel="noopener ugc nofollow" target="_blank">的另一篇文章</a>中，我们看到了线性回归算法在理论上是如何工作的。随着机器学习库的流行，任何人都可以通过一些 API 调用来实现 ML 算法。但只有少数人深入了解算法是如何工作的。我们将使用 Python 编写线性回归程序，并对其进行训练！</p><h1 id="8cbf" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">Python 代码中的线性回归</h1><p id="b394" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">我们将对线性回归算法进行编码，并用 Python 语言对其进行训练。如果你已经有编程经验，但不熟悉 Python，这绝对是轻而易举的事。然而，如果你没有任何编程经验，我建议你浏览 Python 的文档。只是为了对编程和语言语法有一个基本的了解。此外，Python 非常直观，您可以很快上手并运行。谷歌的 Colab 是运行你的程序的好方法，因为它在安装库时没有麻烦。但是如果你已经有了 Python IDE，欢迎你跳过。那好吧。事不宜迟，让我们直接跳到代码。</p><h2 id="45c2" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">导入 Python 库</h2><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="54b5" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import time</strong></span></pre><p id="f8d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先我们导入最需要的矩阵运算库 Numpy。然后，‘matplotlib’是用来画草图和图形的。这是为了可视化我们的模型的拟合和性能。pandas 有助于将数据集(csv、excel 文件)轻松加载到 Pandas 数据框中。我们将使用这些来加载数据集，初始化线性回归模型并对其进行训练。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="9e50" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">df = pd.read_csv(“https://raw.githubusercontent.com/Baakchsu/LinearRegression/master/weight-height.csv”)</strong></span></pre><p id="4665" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一行从我的 Github repo 中提取“根据体重预测身高”数据集，并将其读入数据框。</p><p id="df15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们将为线性回归模型创建一个类，就像 scikit 的模型一样。它的模型可以用“sklearn.linear_model”调用。线性回归”方法。但是，我们将把它作为仅仅<strong class="kh ir">线性回归</strong>。让我们看看它是如何出现的。</p><h2 id="8fe9" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">创建线性回归类</h2><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="44c0" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">class LinearRegression:<br/> def fit(self,X,Y):<br/> X=np.array(X).reshape(-1,1)<br/> Y=np.array(Y).reshape(-1,1)<br/> x_shape = X.shape<br/> self.parameter_cache = []<br/> num_var = x_shape[1]       #the shape corresponds to number of input variable dimensions. There’s only one for this dataset i.e weight of person<br/> self.weight_matrix = np.random.normal(-1,1,(num_var,1))<br/> self.intercept = np.random.rand(1)<br/> for i in range(50):<br/> self.dcostdm = np.sum(np.multiply(((np.matmul(X,self.weight_matrix)+self.intercept)-Y),X))*2/x_shape[0] #w.r.t to the weight<br/> self.dcostdc = np.sum(((np.matmul(X,self.weight_matrix)+self.intercept)-Y))*2/x_shape[0]          #partial derivative of cost w.r.t the intercept<br/> self.weight_matrix -= 0.1*self.dcostdm                                                                  #updating the weights with the calculated gradients<br/> self.intercept -= 0.1*self.dcostdc                                                                      #updating the weights with the calculated gradients<br/> self.parameter_cache.append(np.array((self.weight_matrix,self.intercept)))                             #the parameters are cached just to track the progress<br/> return self.weight_matrix,self.intercept,self.parameter_cache<br/> def predict(self,X):<br/> product = np.matmul(np.array(X).reshape(-1,1),self.weight_matrix)+self.intercept<br/> return product</strong></span></pre><p id="4fcb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果很简单，对吧？让我们通过代码来更详细地理解一下。这里，我们用两个方法“拟合”和“预测”定义了<strong class="kh ir"> LinearRegression </strong>类。是的，他们顾名思义。拟合方法接受输入特征向量(人的体重)和输出变量(人的身高),并训练线性模型以获得完美拟合。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="a176" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">X=np.array(X).reshape(-1,1)<br/>Y=np.array(Y).reshape(-1,1)<br/>x_shape = X.shape<br/>num_var = x_shape[1]</strong></span></pre><p id="ec78" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将参数 X 和 Y 转换为 numpy 数组，并将它们重新整形为形状(数据的数量，特征变量的数量)。因为我们只是用体重来预测身高，所以变量的数量只有一个(num_var)。我们对输出目标变量 y 做同样的事情。</p><h2 id="197e" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">使用 Numpy 进行参数初始化</h2><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="bc10" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">self.weight_matrix = np.random.normal(-1,1,(num_var,1))<br/>self.intercept = np.random.rand(1)<br/>self.parameter_cache = [ ]</strong></span></pre><p id="dae0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">之后，我们初始化权重矩阵和截距变量。权重矩阵将有一个形状(num_var，要预测的输出变量的数量)。因此，对于我们的问题，结果是(1，1)。因为输入和输出变量的数量都是一个(体重和身高都是一个变量)。如果我们用输入的体重和性别来预测一个人的身高，我们将有两个输入变量和一个输出变量。因此，权重矩阵将具有形状(2，1)。</p><p id="71d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">截距只是一个表示线性回归模型的 Y 截距的数字。如果你对什么是权重和截距感到困惑，查看一下详细解释的<a class="ae lb" href="https://hackerstreak.com/linear-regression/" rel="noopener ugc nofollow" target="_blank">前一篇文章</a>。我们在训练时使用 parameter_cache 来缓存模型参数。</p><h2 id="c7b2" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">训练线性回归模型</h2><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="2aa3" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">for i in range(50):<br/> self.dcostdm =  np.sum(np.multiply(((np.matmul(X,self.weight_matrix)+self.intercept)-Y),X))*2/x_shape[0] #partial derivative of cost w.r.t the weights<br/> self.dcostdc = np.sum(((np.matmul(X,self.weight_matrix)+self.intercept)-Y))*2/x_shape[0]                #partial derivative of cost w.r.t the intercept<br/> self.weight_matrix -= 0.1*self.dcostdm                                                                  #updating the weights with the calculated gradients<br/> self.intercept -= 0.1*self.dcostdc                                                                      #updating the weights with the calculated gradients<br/> self.parameter_cache.append(np.array((self.weight_matrix,self.intercept)))</strong></span><span id="a885" class="lz ld iq mq b gy my mv l mw mx"><strong class="mq ir">return self.weight_matrix,self.intercept,self.parameter_cache</strong></span></pre><p id="fdab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们用反向传播训练我们的模型。这是成本相对于(w.r.t)上一篇文章中的模型参数的偏导数方程。</p><div class="ml mm mn mo gt ab cb"><figure class="mz na nb nc nd ne nf paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/009c04c314c1d3e1b9ddc08dc42d6409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*aYJZC0yeNkHKRCDreOmgQQ.jpeg"/></div></figure><figure class="mz na nm nc nd ne nf paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/9cea01cefcf7ee5cb70f2a8bc08c54c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*NZyR0zwZzINv2uD4ywF6-Q.jpeg"/></div><p class="nn no gj gh gi np nq bd b be z dk nr di ns nt translated"><strong class="bd nu">成本 w.r.t 'm '(权重矩阵)的偏导数和成本函数相对于' C '的偏导数</strong></p></figure></div><p id="0e4c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在前两行(dcostdm 和 dcostdc)中实现上述等式。</p><div class="ml mm mn mo gt ab cb"><figure class="mz na nv nc nd ne nf paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/f4d5960bfb13fb9585da790e468f46de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*wvcgOhor355xxH7VcREYEg.jpeg"/></div></figure><figure class="mz na nw nc nd ne nf paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/349ba9aa6a6370a7f2c32787ccdc423c.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*lC2NgEtwOAtSwlcA0W1q8Q.jpeg"/></div></figure></div><p id="d541" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们使用上面的等式用计算出的梯度更新参数“权重矩阵”和“截距”。然后，我们将模型参数追加到缓存中。训练循环结束后，我们从 fit 函数返回参数和缓存。</p><h2 id="cc2f" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">该预测方法</h2><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="79c7" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">def predict(self,X):<br/> product =  np.matmul(np.array(X).reshape(-1,1),self.weight_matrix)+self.intercept<br/> return product</strong></span></pre><p id="4352" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">预测方法采用输入要素，并使用线性回归类的训练参数预测输出。“self.weight_matrix”和“self.intercept”表示我们在拟合方法中看到的模型参数。同样，我们返回预测值。</p><h2 id="cd54" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">我们如何使用模型类？</h2><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="eb73" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">reg = LinearRegression()</strong></span></pre><p id="ff2d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">到目前为止，我们只是创建了模型类和训练代码。现在，我们将使用上面的代码行创建一个线性回归类的实例。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="bb0c" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">x = (df[‘Weight’]-df[‘Weight’].mean())/df[‘Weight’].std() #standardization of the dataset<br/>y = (df[“Height”]-df[‘Height’].mean())/df[“Height”].std() #standardization of the dataset</strong></span></pre><p id="e4e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这两行将使用数学公式 X-u/std 对数据集进行标准化。其中，X 是我们想要标准化的变量，“u”是该变量的平均值，“std”是标准偏差。这有助于模型更快地学习，因为所有变量都在(-1 到 1)的范围内。它将平均值居中到 0 和单位标准偏差(std=1)。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="f4be" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">params = reg.fit(x[:-180],y[:-180])</strong></span></pre><p id="2343" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过调用类实例‘reg’的 fit 方法并传递 X 和 Y 值，我们开始训练。这里，我们传递数据集，留下最后 180 个数据点进行测试。</p><h2 id="5356" class="lz ld iq bd le ma mb dn li mc md dp lm ko me mf lo ks mg mh lq kw mi mj ls mk bi translated">模型可视化</h2><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="a52a" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">pred = reg.predict(np.array(x[-180:]))</strong></span><span id="c644" class="lz ld iq mq b gy my mv l mw mx"><strong class="mq ir">plt.scatter(x[-180:],y[-180:])</strong></span><span id="806f" class="lz ld iq mq b gy my mv l mw mx"><strong class="mq ir">plt.plot(x[-180:],pred)</strong></span></pre><p id="2bf3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们完成训练，我们就可以使用训练好的回归模型进行预测。我们使用“reg.predict”方法来完成这项工作，并传入测试数据(最后 180 个数据点)。现在,“pred”变量将具有测试数据的预测输出。为了直观地显示模型是如何拟合的，我们首先用测试数据点创建一个图。然后，我们用“plt.plot”函数绘制模型拟合线。</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1bd63b23ab9c16c68a0b0104dd651793.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*kq9d0nhwWd27CBCaEHnp-A.jpeg"/></div></figure><p id="19d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于用 Numpy 从头开始构建的线性回归模型，这提供了足够好的拟合。值得注意的是，从图中，我们可以看到它在数据集上概括得很好。这只是一个线性模型。但是知道它的工作原理有助于更好地应用它。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="8d93" class="lz ld iq mq b gy mu mv l mw mx"><strong class="mq ir">plt.figure(figsize=(19, 10))<br/>plt.scatter(x[-180:],y[-180:])<br/>for i in list(np.arange(0,50,5)):<br/> <br/> value = params[2][i]<br/> prediction = np.matmul(np.array(x[-180:]).reshape(-1,1),value[0])+value[1]<br/> plt.plot(x[-180:],prediction)</strong></span></pre><p id="c564" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从“fit”方法返回的参数缓存中，我们在训练过程中绘制出模型的拟合度。顶部的蓝线是训练前随机参数的初始拟合。如您所见，该模型在训练过程中提高了其预测性能。您可以修改代码，改变初始化条件，看看模型拟合如何变化。为此，可以在 GitHub <a class="ae lb" href="https://github.com/Baakchsu/LinearRegression" rel="noopener ugc nofollow" target="_blank">库</a>上找到代码</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ny"><img src="../Images/29c97ae4541bb0cf8f7bbefd4a209b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JleVu4ZnLQAgtFw-8k3HDw.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated"><strong class="bd nu">展示模型如何改进拟合的历史</strong></p></figure><figure class="ml mm mn mo gt na"><div class="bz fp l di"><div class="nz oa l"/></div></figure></div></div>    
</body>
</html>