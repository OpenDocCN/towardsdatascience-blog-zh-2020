<html>
<head>
<title>Stochastic Gradient Descent &amp; Momentum Explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机梯度下降&amp;动量解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stochastic-gradient-descent-momentum-explanation-8548a1cd264e?source=collection_archive---------19-----------------------#2020-01-11">https://towardsdatascience.com/stochastic-gradient-descent-momentum-explanation-8548a1cd264e?source=collection_archive---------19-----------------------#2020-01-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6901" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">实施随机梯度下降</h2></div><p id="40b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">先说随机梯度下降法(SGD)，这可能是我们听过最多的第二著名的梯度下降法。正如我们所知，传统的梯度下降法通过将每个参数推向其梯度的相反方向来最小化目标函数(如果您对普通梯度下降法有困惑，可以查看此处的<a class="ae le" rel="noopener" target="_blank" href="/gradient-descent-explanation-implementation-c74005ff7dd1">以获得更好的解释)。</a></p><p id="842a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管批量梯度下降保证了凸函数的全局最优，但是考虑到您正在训练具有数百万样本的数据集，计算成本可能会非常高。随机梯度下降通过给数据集增加一些随机性来拯救。<em class="lf">在每次迭代中，SGD 随机洗牌并更新每个随机样本上的参数，而不是完全批量更新。</em></p><h1 id="e7f8" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">SGD 实施</h1><p id="639c" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">让我们来看一个具体例子的实现。我们的优化任务被定义为:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi md"><img src="../Images/81ad8b3e5a957e67997418b3deaadcbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*SnND_aiqdSnGZPkpAtEgmw.png"/></div></figure><p id="66be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，我们试图用两个参数<code class="fe ml mm mn mo b">a, b</code>最小化<code class="fe ml mm mn mo b">y — f(x)</code>的损失，上面计算了它们的梯度。</p><p id="81ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">样本生成将是:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="6f2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们生成了 100 个<code class="fe ml mm mn mo b">x</code>和<code class="fe ml mm mn mo b">y</code>的样本，我们将使用它们来找到参数的实际值。</p><p id="7edd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SGD 的实现很简单:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="e851" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">只有一行加法<code class="fe ml mm mn mo b">np.random.shuffle(ind)</code>，它在每次迭代中打乱数据。我们还设置<code class="fe ml mm mn mo b">a_list, b_list</code>来跟踪每个参数的更新轨迹，优化曲线将是:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/a278ac254ed80d352f48c64962f5fb83.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*632OwBYLS0ad6YHgp0hLOA.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">签名于</p></figure><p id="4203" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SGD 降低了计算成本，并可能避免停留在局部最小值，因为它可以通过每次随机选择新样本跳到另一个区域。但这样做的缺点是，如果不适当降低学习速率，它会持续超调。所以在实际用例中，SGD 总是和一个衰减的学习率函数耦合在一起(更多解释<a class="ae le" href="http://d2l.ai/chapter_optimization/sgd.html" rel="noopener ugc nofollow" target="_blank">此处</a>)。</p><p id="a945" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使更新跟踪更加平滑，我们可以将 SGD 与小批量更新结合起来。</p><h1 id="5a1f" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">小批量 SGD</h1><p id="e07e" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">这里所说的<a class="ae le" rel="noopener" target="_blank" href="/gradient-descent-explanation-implementation-c74005ff7dd1">小批量</a>是基于一小批渐变而不是每一项来更新参数。这有助于减少差异，并使更新过程更加顺畅:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="24e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每次我们再次混洗数据，但是这次按照以下公式对每批的梯度进行平均更新:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/c5af3639796b023270fcf0c2521c9ba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*w5qx34rJAGO_AlrgtVPo_A.png"/></div></figure><p id="efd8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过将批量大小设置为 50，我们得到了更平滑的更新，如下所示:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b6025d1b2e6533143eab20eefda0c3ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*jdxrCPgDATHNNTc7Ew5g3w.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">小批量 SGD</p></figure><h1 id="4ae4" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">动力</h1><p id="6b6f" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">最后，还有一个概念，动量，与 SGD 相结合。它通过引入一个额外的术语<code class="fe ml mm mn mo b">γ</code>来帮助加速收敛:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/a3b626f774b96da68ec3e6445601c73f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TvSsaR9VvvIfi7ZpF93zaA.png"/></div></div></figure><p id="5d0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的等式中，<code class="fe ml mm mn mo b">θ</code>的更新受上次更新的影响，这有助于在相关方向上加速 SGD。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="5937" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实现是不言自明的。通过设置学习率为 0.2，γ为 0.9，我们得到:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/1f30c8ecc88fd81738b6f194b66d18f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*dMWDTbcr6qBXr7EJkCm1NQ.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">动量-新加坡元</p></figure><h1 id="8973" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">结论</h1><p id="59a8" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">最后，这绝对不是探索的终点。动量可以与小批量相结合。而且你还测试了更灵活的学习率函数，它随着迭代而变化，甚至在不同的维度上变化的学习率(完全实现<a class="ae le" href="https://github.com/MJeremy2017/Machine-Learning-Models/tree/master/Optimisation" rel="noopener ugc nofollow" target="_blank">这里</a>)。</p><p id="c3f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，我们在所有维度上使用统一的学习率，但是对于不同维度上的参数以不同频率出现的情况，这将是困难的。接下来，我将介绍<a class="ae le" rel="noopener" target="_blank" href="/introduction-and-implementation-of-adagradient-rmsprop-fad64fe4991">自适应梯度下降</a>，它有助于克服这个问题。</p><p id="cfad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考:</p><ol class=""><li id="e4d6" class="nc nd it kk b kl km ko kp kr ne kv nf kz ng ld nh ni nj nk bi translated"><a class="ae le" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank">https://ml-cheat sheet . readthedocs . io/en/latest/gradient _ descent . html</a></li><li id="dc7f" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated"><a class="ae le" href="http://d2l.ai/chapter_optimization/sgd.html" rel="noopener ugc nofollow" target="_blank">http://d2l.ai/chapter_optimization/sgd.html</a></li><li id="7d2e" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated"><a class="ae le" href="https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms" rel="noopener ugc nofollow" target="_blank">https://ruder . io/optimization-gradient-descent-descent/index . html # gradient descent optimizationalgorithms</a></li><li id="977e" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated"><a class="ae le" href="http://d2l.ai/chapter_optimization/momentum.html" rel="noopener ugc nofollow" target="_blank">http://d2l.ai/chapter_optimization/momentum.html</a></li></ol></div></div>    
</body>
</html>