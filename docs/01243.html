<html>
<head>
<title>Gradient Descent From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc?source=collection_archive---------7-----------------------#2020-02-04">https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc?source=collection_archive---------7-----------------------#2020-02-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="9916" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将解释什么是梯度下降，以及如何用 Python 从头开始实现它。为了理解它是如何工作的，你需要一些基本的数学和逻辑思维。虽然更强的数学背景更有利于理解导数，但我会尽可能简单地解释它们。</p><p id="73dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度下降可用于不同的机器学习算法，包括神经网络。对于本教程，我们将建立一个线性回归问题，因为它很容易理解和可视化。</p><p id="1222" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我还创建了包含所有解释的 GitHub repo。让我们开始吧。</p><h2 id="ee19" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">线性回归</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/6fa5615b771f0da4f55da6f9b326e290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X55SxbQRvdLmocOAITuT5g.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">线性回归</p></figure><p id="f94b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了拟合回归线，我们调整了两个参数:斜率(<code class="fe lv lw lx ly b">m</code>)和截距(<code class="fe lv lw lx ly b">b</code>)。一旦找到最佳参数，我们通常用均方差(<code class="fe lv lw lx ly b">MSE</code>)来评估结果。我们记得 MSE 越小越好。换句话说，我们正试图将其最小化。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lz"><img src="../Images/b499fae21170df01e250e7ca070ff22b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BT-K3d_tspyeePbQzMzE7g.png"/></div></div></figure><p id="d94e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">函数的最小化是梯度下降算法的确切任务。它获取参数并调整它们，直到达到局部最小值。</p><p id="e9e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们一步一步地分解这个过程，并解释一下实际发生了什么:</p><ol class=""><li id="3a35" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">首先，我们取一个我们想要最小化的函数，通常它是均方误差函数。</li><li id="58e7" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">我们确定参数，例如回归函数中的<code class="fe lv lw lx ly b">m</code>和<code class="fe lv lw lx ly b">b</code>，并对这些参数取 MSE 的偏导数。这是最关键也是最难的部分。每个派生的函数都可以告诉我们应该以哪种方式调整参数以及调整多少。</li><li id="d880" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">我们通过迭代我们的派生函数并逐渐最小化 MSE 来更新参数。在这个过程中，我们使用一个额外的参数<code class="fe lv lw lx ly b">learning rate</code>，它帮助我们定义在每次迭代中更新参数的步骤。通过设置一个较小的学习率，我们可以确保我们的模型不会跳过最小均方差点，并很好地收敛。</li></ol><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mo"><img src="../Images/8197c16e0ca4645679f6b7cd72e65ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WDGwqWArsoIcj1x8CY_fNw.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">均方误差</p></figure><h2 id="1101" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">派生物</h2><p id="e68b" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">我们使用偏导数来发现每个参数如何影响 MSE，所以这就是单词<em class="mu"> partial </em>的来源。我们分别对<code class="fe lv lw lx ly b">m</code>和<code class="fe lv lw lx ly b">b</code><strong class="jp ir"/>求导。看看下面的公式。和 MSE 差不多，不过这次我们加了 f(m，b)进去。它本质上没有改变什么，除了现在我们可以把<code class="fe lv lw lx ly b">m</code>和<code class="fe lv lw lx ly b">b</code>数字插入其中并计算结果。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mv"><img src="../Images/73db79b380a2dd71665df466a935efe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gwyZEgUjs6i_yuNa4IQpLw.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">带输入参数的 MSE</p></figure><p id="726a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于<code class="fe lv lw lx ly b">m</code>意味着我们导出参数<code class="fe lv lw lx ly b">m</code>，基本上，忽略<code class="fe lv lw lx ly b">b</code>的情况，或者我们可以说它为 0，反之亦然。为了求偏导数，我们将使用链式法则。当我们需要对包含另一个函数的函数求导时，我们会用到它。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mw"><img src="../Images/ea2c8e915bb6cfd52e6e96a931123128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XN4fPjeLZJGHC0Fzdp6Upw.png"/></div></div></figure><p id="081c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">链式法则说，我们应该对外部函数求导，保持内部函数不变，然后用内部函数的导数乘以所有东西。我们现在需要做的就是对下面函数的<strong class="jp ir"> m </strong>和<strong class="jp ir"> b </strong>求偏导数:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mx"><img src="../Images/d1cbca1a141c2839547d854aeed572c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2nQWJEOMeuVX10jzmpXv9A.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">平方误差</p></figure><p id="46eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你是新手，你会惊讶于<code class="fe lv lw lx ly b">()²</code>是一个包含<code class="fe lv lw lx ly b">y-(mx+b)</code>的外部函数。现在，让我们根据链式法则分解每个步骤:</p><ol class=""><li id="cc71" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated">对外部函数求导:<code class="fe lv lw lx ly b">()²</code>变成<code class="fe lv lw lx ly b">2()</code></li><li id="25b9" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">保持内部函数不变:<code class="fe lv lw lx ly b">y-(mx+b)</code></li><li id="f906" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">对 m 求偏导数:<code class="fe lv lw lx ly b">0-(x+0)</code>或<code class="fe lv lw lx ly b">-x</code>。让我们详细说明一下我们是如何得到这个结果的:我们把任何不等于 m 的东西都视为常数。常数总是等于 0。<code class="fe lv lw lx ly b">mx</code>的导数是<code class="fe lv lw lx ly b">x</code>，因为<code class="fe lv lw lx ly b">m</code>的导数是 1，附加在<code class="fe lv lw lx ly b">m</code>上的任何数字或变量都留在原地，意思是<code class="fe lv lw lx ly b">1*x</code>，或者只是<code class="fe lv lw lx ly b">x</code>。</li></ol><p id="38fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">把所有的东西放在一起，我们得到了下面的:<code class="fe lv lw lx ly b">2(y-(mx+b))*-x</code>。这个可以重写为<code class="fe lv lw lx ly b">-2x(y-(mx+b))</code>。太好了！我们可以按照正确的符号重新编写:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi my"><img src="../Images/5fb4b00ce2b2a62c9a372c43af948cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4lh7h2-xPcKETXFlD9BiVg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">关于<strong class="bd mz"> m 的偏导数</strong></p></figure><p id="7238" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了相对于<code class="fe lv lw lx ly b">b</code>进行推导，我们遵循相同的链式法则步骤:</p><ol class=""><li id="84af" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated"><code class="fe lv lw lx ly b">()²</code>变为<code class="fe lv lw lx ly b">2()</code></li><li id="58ba" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><code class="fe lv lw lx ly b">y-(mx+b)</code>保持不变</li><li id="c36c" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated">这里<code class="fe lv lw lx ly b">y-(mx+b)</code>的导数变成了这个:<code class="fe lv lw lx ly b">(0-(0+1))</code>或者<code class="fe lv lw lx ly b">-1</code>。原因如下:我们再次把<code class="fe lv lw lx ly b">y</code>和<code class="fe lv lw lx ly b">mx</code>当作常数，所以它们变成了 0。<code class="fe lv lw lx ly b">b</code>变成了 1。</li></ol><p id="0f79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">把所有东西放在一起:<code class="fe lv lw lx ly b">2(y-(mx+b))*-1</code>，或者- <code class="fe lv lw lx ly b">2(y-(mx+b))</code>。同样，正确的符号应该是这样的:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi na"><img src="../Images/f14ff925c188a5d958f5b6e5c6e7ce73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_dcPGXVXvWAJ_QbxBmAKPQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">关于<strong class="bd mz"> b </strong>的偏导数</p></figure><p id="2db2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">唷！最难的部分已经过去了，现在我们可以进入 Python 环境了。</p><h2 id="6d00" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">梯度下降</h2><p id="fe8c" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">因为我们只有一个输入要素，所以 X 必须是一个 NumPy 向量，它是一个值列表。通过对权重<code class="fe lv lw lx ly b">m1</code>、<code class="fe lv lw lx ly b">m2</code>求导，我们可以很容易地将其扩展到多个特征..等等，但是这次我们做的是简单的线性回归。</p><p id="9b58" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看一下代码。我们从用随机值定义<code class="fe lv lw lx ly b">m</code>和<code class="fe lv lw lx ly b">b</code>开始，然后我们有一个 for 循环来迭代派生的函数，每一步都由一个学习速率(<code class="fe lv lw lx ly b">lr</code>)来控制。我们使用<code class="fe lv lw lx ly b">log</code>和<code class="fe lv lw lx ly b">mse</code>列表来跟踪我们的进度。</p><pre class="lg lh li lj gt nb ly nc nd aw ne bi"><span id="009f" class="km kn iq ly b gy nf ng l nh ni">import numpy as np<br/>from sklearn.metrics import mean_squared_error</span><span id="8ea6" class="km kn iq ly b gy nj ng l nh ni">def gradient_descent(X, y, lr=0.05, epoch=10):<br/>    <br/>    '''<br/>    Gradient Descent for a single feature<br/>    '''<br/>    <br/>    m, b = 0.33, 0.48 # parameters<br/>    log, mse = [], [] # lists to store learning process<br/>    N = len(X) # number of samples<br/>    <br/>    for _ in range(epoch):<br/>                <br/>        f = y - (m*X + b)<br/>    <br/>        # Updating m and b<br/>        m -= lr * (-2 * X.dot(f).sum() / N)<br/>        b -= lr * (-2 * f.sum() / N)<br/>        <br/>        log.append((m, b))<br/>        mse.append(mean_squared_error(y, (m*X + b)))        <br/>    <br/>    return m, b, log, mse</span></pre><p id="5c6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以直观地跟踪算法如何接近局部最小值。肉眼看来，好像没有完全收敛。为了解决这个问题，我们可以增加历元和学习率参数。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lz"><img src="../Images/a0e57f5e8d8941842687f67cc4d35724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KoskxJ848dZqSnp40H95iA.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nk"><img src="../Images/1c1ad779c943daa3a2d65ef82857265d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OIvTmoPRQ1AmcTDjDmu6VQ.png"/></div></div></figure><h2 id="43aa" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">奖励:随机梯度下降</h2><p id="ae3f" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">随机梯度下降仅使用一个样本来更新参数，这使得它更快。我们可以对之前的版本做一些小的修改，看看它的表现如何。</p><pre class="lg lh li lj gt nb ly nc nd aw ne bi"><span id="b874" class="km kn iq ly b gy nf ng l nh ni">def SGD(X, y, lr=0.05, epoch=10, batch_size=1):<br/>        <br/>    '''<br/>    Stochastic Gradient Descent for a single feature<br/>    '''<br/>    <br/>    m, b = 0.33, 0.48 # initial parameters<br/>    log, mse = [], [] # lists to store learning process<br/>    <br/>    for _ in range(epoch):<br/>        <br/>        indexes = np.random.randint(0, len(X), batch_size) # random sample<br/>        <br/>        Xs = np.take(X, indexes)<br/>        ys = np.take(y, indexes)<br/>        N = len(Xs)<br/>        <br/>        f = ys - (m*Xs + b)<br/>        <br/>        # Updating parameters m and b<br/>        m -= lr * (-2 * Xs.dot(f).sum() / N)<br/>        b -= lr * (-2 * f.sum() / N)<br/>        <br/>        log.append((m, b))<br/>        mse.append(mean_squared_error(y, m*X+b))        <br/>    <br/>    return m, b, log, mse</span></pre><p id="cdd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以观察到这个过程的随机性。回归线到处跳跃，试图根据一个样本找到最小值。可怜的东西。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lz"><img src="../Images/0b1eacc2d2d8d8930950a548dba5911e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GbVZ-4a9NfQxm-kfk4Y8Ww.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nl"><img src="../Images/4754c88d0f30b236ae565513372c66a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qKbbN_iT2JWmg7_MLU6AwA.png"/></div></div></figure><h2 id="415f" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">结论</h2><p id="22d3" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">我们已经从头开始学习了如何实现梯度下降和 SGD。当我们训练神经网络时，我们用同样的方法进行反向传播。如果你对从零开始实现深度神经网络感兴趣，请在评论中告诉我。感谢您的阅读！</p><p id="aa4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">阿瑟尼。</p></div></div>    
</body>
</html>