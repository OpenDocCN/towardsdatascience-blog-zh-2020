<html>
<head>
<title>Build kNN from scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 从头开始构建 kNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-knn-from-scratch-python-7b714c47631a?source=collection_archive---------11-----------------------#2020-09-13">https://towardsdatascience.com/build-knn-from-scratch-python-7b714c47631a?source=collection_archive---------11-----------------------#2020-09-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bcc7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用<em class="ki">k</em>-折叠交叉验证(从头开始)</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/3253e69ba20b121c85d50e3f00ee4a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-w-ll32avduIoL3pPinodA.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">邻居(图片来源:<a class="ae kz" href="https://www.freepik.com/free-vector/apartment-building-with-people-open-window-spaces_7416533.htm#page=1&amp;query=neighbors&amp;position=2" rel="noopener ugc nofollow" target="_blank"> Freepik </a></p></figure><p id="5c11" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在本文中，我们将了解 k-最近邻(kNN)算法是如何工作的，并从头开始构建 kNN 算法。我们还将使用 k-Fold 交叉验证来评估我们的算法，这也是从头开始开发的。</p><p id="67b5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">完成本教程后，您将了解:</p><ul class=""><li id="b0bb" class="lw lx it lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">如何分步编码<em class="mf">k</em>-最近邻算法</li><li id="7c95" class="lw lx it lc b ld mg lg mh lj mi ln mj lr mk lv mb mc md me bi translated">如何使用 k-最近邻对新数据进行预测</li><li id="0313" class="lw lx it lc b ld mg lg mh lj mi ln mj lr mk lv mb mc md me bi translated">如何对<em class="mf">k</em>-折叠交叉验证进行分步编码</li><li id="d5d7" class="lw lx it lc b ld mg lg mh lj mi ln mj lr mk lv mb mc md me bi translated">如何使用<em class="mf"> k </em>折叠交叉验证评估真实数据集上的<em class="mf"> k </em>最近邻</li></ul><p id="d595" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">先决条件:对 Python 和面向对象编程(OOP)中的类和对象的概念有基本的了解</p><h1 id="ae1f" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">k-最近邻</h1><p id="bb56" class="pw-post-body-paragraph la lb it lc b ld nd ju lf lg ne jx li lj nf ll lm ln ng lp lq lr nh lt lu lv im bi translated">最近邻居，简称 kNN，是一种非常简单但功能强大的预测技术。kNN 背后的原则是使用“与新数据最相似的历史实例”</p><p id="9013" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="mf"> k </em>是用于识别新数据点的相似邻居的数字。</p><p id="8437" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最初存储整个训练数据集。当需要对新数据进行预测时，<em class="mf"> k </em> NN 考虑<em class="mf">k</em>-最相似的邻居(记录)来决定新数据点将属于基于特征相似性的位置。</p><p id="a414" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一旦我们找到了距离或相似性，我们就选择前<em class="mf"> k </em>最接近的记录。在发现 k 个最接近的记录后，我们通过返回最常见的结果或取平均值来进行预测。因此，<em class="mf"> k </em> NN 可用于分类或回归问题。</p><p id="62b4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="mf"> k </em> NN 算法没有训练阶段。该模型仅保存数据，直到需要进行预测，并且不做任何工作。正因如此，<em class="mf"> k </em> NN 常被称为“<strong class="lc iu">懒学法</strong>”。</p><h2 id="b79f" class="ni mm it bd mn nj nk dn mr nl nm dp mv lj nn no mx ln np nq mz lr nr ns nb nt bi translated">4 个简单步骤中的 k 个最近邻居</h2><ol class=""><li id="09ab" class="lw lx it lc b ld nd lg ne lj nu ln nv lr nw lv nx mc md me bi translated"><strong class="lc iu">为<em class="mf">k</em>T35】选择一个值</strong></li><li id="27df" class="lw lx it lc b ld mg lg mh lj mi ln mj lr mk lv nx mc md me bi translated"><strong class="lc iu">找到新点到每条训练数据记录的距离</strong></li><li id="3b1c" class="lw lx it lc b ld mg lg mh lj mi ln mj lr mk lv nx mc md me bi translated"><strong class="lc iu">得到<em class="mf">k</em>-最近邻</strong></li><li id="9296" class="lw lx it lc b ld mg lg mh lj mi ln mj lr mk lv nx mc md me bi translated">对于<strong class="lc iu">分类问题</strong>，新的数据点属于大多数邻居所属的类。对于<strong class="lc iu">回归问题</strong>，预测可以是<em class="mf"> k </em>最近邻标签的平均值或加权平均值</li></ol></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h1 id="54bf" class="ml mm it bd mn mo of mq mr ms og mu mv jz oh ka mx kc oi kd mz kf oj kg nb nc bi translated">使用 Python 从头构建 kNN</h1><p id="aafe" class="pw-post-body-paragraph la lb it lc b ld nd ju lf lg ne jx li lj nf ll lm ln ng lp lq lr nh lt lu lv im bi translated">您可以使用我的<a class="ae kz" href="https://github.com/chaitanyakasaraneni/knnFromScratch" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中可用的代码来跟进。</p><p id="aeb2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">您也可以通过以下方式安装它:</p><pre class="kk kl km kn gt ok ol om on aw oo bi"><span id="1efd" class="ni mm it ol b gy op oq l or os">pip install simple-kNN</span></pre><p id="fe7a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">PyPI 包版本的 GitHub repo:【https://github.com/chaitanyakasaraneni/simple-kNN T2】</p><h2 id="5b82" class="ni mm it bd mn nj nk dn mr nl nm dp mv lj nn no mx ln np nq mz lr nr ns nb nt bi translated">第一步:<strong class="ak">选择一个<em class="ki"> k 值</em>T7】</strong></h2><p id="6106" class="pw-post-body-paragraph la lb it lc b ld nd ju lf lg ne jx li lj nf ll lm ln ng lp lq lr nh lt lu lv im bi translated">K 的选择对我们从<em class="mf"> k </em> NN 中获得的结果有很大的影响。<em class="mf">最好选奇数。</em></p><h2 id="14cc" class="ni mm it bd mn nj nk dn mr nl nm dp mv lj nn no mx ln np nq mz lr nr ns nb nt bi translated">第二步:计算距离</h2><p id="2426" class="pw-post-body-paragraph la lb it lc b ld nd ju lf lg ne jx li lj nf ll lm ln ng lp lq lr nh lt lu lv im bi translated">下一步是计算数据集中两行之间的距离。</p><p id="ff89" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">特定于问题或数据的方法用于计算两个记录之间的距离或相似性。一般来说，对于表格或矢量数据，欧几里得距离被认为是起点。还有其他几种相似性或距离度量，如曼哈顿距离、汉明距离等。</p><p id="57e6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">欧几里德距离</strong>定义为两点间距离(差)的平方和的平方根。它也被称为 L2 规范。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/8f23db51adb783a4e77144653256d398.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*IYDwjcvVJ7mZOxZHy3lPhQ.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">两个向量'<em class="ki"> x </em>和'<em class="ki"> y </em>之间的欧氏距离</p></figure><p id="5be8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">曼哈顿距离</strong>是两点之间差异的绝对值之和</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/0bba8798bff647815612b68f8bc80ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*SQiVpN-b-p4vp23E6JZDQw.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">两个矢量'<em class="ki"> x </em>和'<em class="ki"> y </em>之间的曼哈顿距离</p></figure><p id="3392" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">汉明距离</strong>用于分类变量。简单来说，它告诉我们两个分类变量是否相同。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/661d2ba259fb8c63b038e1b9bb76343b.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*dIf8AIVt6XgognL9HGgyjQ.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">两个向量“x”和“y”之间的汉明距离</p></figure><p id="6fa8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">其中“δ”用于检查两个元素是否相等。</p><p id="c162" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在 python 中，我们创建了一个单独的类来保存计算两个向量之间距离的方法。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ou ov l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">包含计算距离度量方法的类</p></figure><p id="60dc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下一步我们将利用这个类来寻找最近的邻居。</p><h2 id="1e19" class="ni mm it bd mn nj nk dn mr nl nm dp mv lj nn no mx ln np nq mz lr nr ns nb nt bi translated">第三步:找最近的邻居</h2><p id="600b" class="pw-post-body-paragraph la lb it lc b ld nd ju lf lg ne jx li lj nf ll lm ln ng lp lq lr nh lt lu lv im bi translated">数据集中一条新数据的邻居是我们使用上面定义的距离度量获得的前 k 个最近的实例。</p><p id="0e07" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了定位数据集中新数据的邻居，我们必须首先计算数据集中每个记录到新数据的距离。我们可以通过为上面定义的 distanceMetric 类创建一个对象来做到这一点。</p><p id="5fb9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">计算出距离后，我们必须根据记录到新数据的距离对训练数据集中的所有记录进行排序。然后我们可以选择顶部的 k 个作为最相似的邻居返回。</p><p id="b21d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以通过将数据集中每条记录的距离记录为一个列表来实现这一点，根据距离对列表进行排序，然后检索邻居。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="2718" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">既然我们知道了如何从数据集中获取 top<em class="mf">k</em>-邻点，我们将使用它们来进行预测。</p><h2 id="bb75" class="ni mm it bd mn nj nk dn mr nl nm dp mv lj nn no mx ln np nq mz lr nr ns nb nt bi translated">第四步:预测</h2><p id="af5b" class="pw-post-body-paragraph la lb it lc b ld nd ju lf lg ne jx li lj nf ll lm ln ng lp lq lr nh lt lu lv im bi translated">在这一步中，我们将使用从训练数据集中收集的前 k 个相似邻居来进行预测。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="8ffa" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在分类的情况下，我们可以返回邻居中最具代表性的类。</p><p id="e0b6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以通过对来自邻居的输出值列表执行<em class="mf"> max() </em>函数来实现这一点。给定在邻域中观察到的类值列表，<em class="mf"> max() </em>函数获取一组唯一的类值，并对该组中每个类值的类值列表进行计数。</p><p id="0d76" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下面是完整的<em class="mf"> k </em> NN 类:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="527d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在我们有了我们的预测，我们需要评估我们的模型的性能。为此，我们将使用下一部分定义的<em class="mf">k</em>-折叠交叉验证。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h1 id="7b6a" class="ml mm it bd mn mo of mq mr ms og mu mv jz oh ka mx kc oi kd mz kf oj kg nb nc bi translated">k 折叠交叉验证</h1><p id="7a1b" class="pw-post-body-paragraph la lb it lc b ld nd ju lf lg ne jx li lj nf ll lm ln ng lp lq lr nh lt lu lv im bi translated">这项技术包括<strong class="lc iu">将数据集随机划分为<em class="mf"> k- </em>组或大小大致相等的折叠</strong>。第一个<strong class="lc iu">折叠被保留用于测试</strong>并且<strong class="lc iu">模型在剩余的 k-1 个折叠</strong>上被训练。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ow"><img src="../Images/9b117977ffe61654606f0777a0e53654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j5YTTKgJLLMZUxhuTbTrLQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">5 折交叉验证。蓝色块是用于测试的折叠。来源:<a class="ae kz" href="https://scikit-learn.org/stable/modules/cross_validation.html" rel="noopener ugc nofollow" target="_blank"> sklearn 文档</a></p></figure><p id="4cb1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="mf"> k </em>有很多变种——折叠交叉验证。你可以在这里读到更多关于他们的信息。</p><p id="3e0d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在我们的方法中，在每次折叠后，我们计算精确度，因此通过对<em class="mf"> k- </em>折叠的精确度取平均值来计算<em class="mf"> k- </em>折叠 CV 的精确度。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h1 id="e5aa" class="ml mm it bd mn mo of mq mr ms og mu mv jz oh ka mx kc oi kd mz kf oj kg nb nc bi translated">使用 Python 从头构建 kFCV</h1><p id="4501" class="pw-post-body-paragraph la lb it lc b ld nd ju lf lg ne jx li lj nf ll lm ln ng lp lq lr nh lt lu lv im bi translated">第一步，我们将数据集分成 k 个褶皱。</p><p id="6177" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后，对于第<em class="mf"> k </em>个折叠中的每个折叠，我们执行 kNN 算法，获得预测，并使用准确度作为评估度量来评估性能。</p><p id="cd32" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">将数据分成 k 倍的方法:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="208b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">评估方法:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="9f45" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这两种方法合并成一个类:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="c17a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以通过为<em class="mf"> k </em>创建一个对象来执行这个操作——折叠交叉验证方法并调用 evaluate 方法，如下所示。</p><pre class="kk kl km kn gt ok ol om on aw oo bi"><span id="24fa" class="ni mm it ol b gy op oq l or os">kfcv = kFoldCV()</span><span id="b1f5" class="ni mm it ol b gy ox oq l or os">kfcv.kFCVEvaluate(data, foldCount, neighborCount, distanceMetric)</span></pre><p id="8e3f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后，<em class="mf"> kfcv.kFCVEvaluate() </em>将数据分割成指定的折叠数，并通过使用指定的距离度量考虑 top- <em class="mf"> k </em>邻居来评估<em class="mf"> k </em> NN 算法。</p><p id="c47d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">例子和实现可以在我的<a class="ae kz" href="https://github.com/chaitanyakasaraneni/knnFromScratch" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>中看到。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h1 id="dd12" class="ml mm it bd mn mo of mq mr ms og mu mv jz oh ka mx kc oi kd mz kf oj kg nb nc bi translated">结论</h1><p id="0389" class="pw-post-body-paragraph la lb it lc b ld nd ju lf lg ne jx li lj nf ll lm ln ng lp lq lr nh lt lu lv im bi translated">在这篇博客中，我们看到了:</p><ul class=""><li id="fa57" class="lw lx it lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">kNN 算法</li><li id="3df1" class="lw lx it lc b ld mg lg mh lj mi ln mj lr mk lv mb mc md me bi translated">kNN 算法中使用的一些距离度量</li><li id="91bd" class="lw lx it lc b ld mg lg mh lj mi ln mj lr mk lv mb mc md me bi translated">使用 kNN 算法的预测</li><li id="f9bc" class="lw lx it lc b ld mg lg mh lj mi ln mj lr mk lv mb mc md me bi translated">使用 kFold 交叉验证评估 kNN 算法</li></ul><p id="a610" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">希望你通过阅读这篇文章获得了一些知识。请记住，这篇文章只是一个概述，是我从各种在线资料中读到的对 kNN 算法和 kFold 交叉验证技术的理解。</p></div></div>    
</body>
</html>