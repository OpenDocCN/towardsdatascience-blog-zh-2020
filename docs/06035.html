<html>
<head>
<title>Singular Value Decomposition and its applications in Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">奇异值分解及其在主成分分析中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/singular-value-decomposition-and-its-applications-in-principal-component-analysis-5b7a5f08d0bd?source=collection_archive---------5-----------------------#2020-05-17">https://towardsdatascience.com/singular-value-decomposition-and-its-applications-in-principal-component-analysis-5b7a5f08d0bd?source=collection_archive---------5-----------------------#2020-05-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="26ef" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">主成分分析稳健计算的数学工具</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5502d295ad7fc5d8337e72d846ef7c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yUlZ9kFggwcLTKVnWiUohA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://www.youtube.com/watch?v=c0fy5V7hA4g" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=c0fy5V7hA4g</a></p></figure><blockquote class="kz la lb"><p id="c397" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">“学习所有的训练数据与实际学习没有任何关系。你所做的基本上是记忆数据”</p></blockquote><p id="5ec7" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">线性代数为从简单的线性回归到深度神经网络的机器学习算法奠定了核心基础。主要原因是数据集可以用二维矩阵表示，其中列表示特征，行表示不同的样本数据点。话虽如此，使用矩阵中所有值的矩阵计算有时是多余的，或者在计算上相当昂贵。我们需要用一种形式来表示矩阵，使得进一步计算所需的矩阵的最重要部分可以容易地提取出来。这就是<strong class="lf iu">奇异值分解(SVD) </strong>发挥作用的地方。</p><p id="4bf5" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">SVD 基本上是一种矩阵分解技术，它将任何矩阵分解成 3 个常见的矩阵。它在机器学习和图像处理方面有一些很酷的应用。为了理解奇异值分解的概念，关于<strong class="lf iu">特征值</strong>和<strong class="lf iu">特征向量</strong>的知识是必不可少的。如果你对特征值和特征向量有很好的理解，向下滚动一点来体验奇异值分解。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="7aec" class="mj mk it bd ml mm mn dn mo mp mq dp mr lz ms mt mu ma mv mw mx mb my mz na nb bi translated">特征值和特征向量</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/bb7794a0a0997ccb47cb2443ea6f9acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*RXNIrjlZVd5gDC0n_JcU5A.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://commons.wikimedia.org/wiki/File:Eigenvectors.gif" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/wiki/File:Eigenvectors.gif</a></p></figure><p id="5980" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">矩阵和向量的乘法产生另一个向量，该向量被定义为该向量相对于给定向量空间中的特定矩阵发生的<strong class="lf iu">变换</strong>。然而，对于某些给定的矩阵，存在一些向量，使得即使在应用了变换之后，它们的方向也不会改变(类似于上面 GIF 中的蓝色向量)。这样的向量称为给定矩阵的<strong class="lf iu">特征向量</strong>，而向量变换后的标度值定义为该特征向量对应的<strong class="lf iu">特征值</strong>。这可以用下面的例子来说明:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/1a4a2d60ab805e1ea2277fc4f3a9fc8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/format:webp/1*KQAh8_CYl7pm45lGeCdAOA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">λ是特征向量<strong class="bd ne"> v </strong>对应的特征值</p></figure><p id="c393" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">特征向量的概念只适用于方阵。有许多方法可以找到特征值和相应的特征向量，我们不打算在这篇文章中讨论。我找到了下面这个来自<a class="ae ky" href="https://www.3blue1brown.com/" rel="noopener ugc nofollow" target="_blank"> 3blue1brown </a>的视频，它以一种更生动的方式解释了特征值和特征向量。随便看看。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="8fb1" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">一个特征向量所跨越的向量空间被称为<strong class="lf iu">特征空间。</strong>一个方阵称为<strong class="lf iu">可对角化矩阵</strong>，如果它可以写成如下格式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/efe19edd1fadd09277f292d430343697.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*XLcw-l8hBxsZY4fcl-z7gw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ne"> D </strong>是由作为对角元素的特征值组成的对角矩阵</p></figure><p id="67ce" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">矩阵<strong class="lf iu"> P </strong>无非是特征向量叠加在一起的矩阵。还有另一种特殊的矩阵称为<strong class="lf iu">对称矩阵</strong>，其中矩阵等于其自身的转置矩阵，如果矩阵的转置矩阵是该矩阵的逆矩阵，则该矩阵称为<strong class="lf iu">正交矩阵</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/59e1b07dd0fdfd6ae5933648f9889b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:154/format:webp/1*6va-Ou1MRiY8AFWo_sOHcg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">对称矩阵</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/1f9592fae1ba857b492e73c0a4227955.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*LlV6ka5RIpqXyrf8SMiLcA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">正交矩阵</p></figure><p id="e728" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">以下是对称矩阵关于特征值和特征向量的一些特殊性质。</p><ul class=""><li id="13ce" class="nk nl it lf b lg lh lj lk lz nm ma nn mb no ly np nq nr ns bi translated">只有<strong class="lf iu">个实特征值</strong></li><li id="7c0c" class="nk nl it lf b lg nt lj nu lz nv ma nw mb nx ly np nq nr ns bi translated">总是<strong class="lf iu">可对角化</strong></li><li id="76f5" class="nk nl it lf b lg nt lj nu lz nv ma nw mb nx ly np nq nr ns bi translated">具有<strong class="lf iu">正交特征向量</strong></li></ul><p id="a076" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">由于对称矩阵的特征向量彼此<strong class="lf iu">正交</strong>，所以对角化矩阵<strong class="lf iu"> A </strong>中的矩阵<strong class="lf iu"> P </strong>是<strong class="lf iu">正交矩阵</strong>。所以我们说任何<strong class="lf iu">对称矩阵</strong>都是<strong class="lf iu">正交可对角化的:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/bc929a9e14f402b34cc6d6844532c34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/1*ARmi1SzlAjSngd73P-BDTw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">对称矩阵 A 是正交对角化的</p></figure><p id="b0a8" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在我们已经介绍了奇异值分解的基础知识。矩阵对角化通常被称为矩阵的<strong class="lf iu">特征分解。</strong>然而，这种特征分解仅限于某一组矩阵。奇异值分解的实际动机源于特征分解的缺点:</p><ul class=""><li id="983a" class="nk nl it lf b lg lh lj lk lz nm ma nn mb no ly np nq nr ns bi translated">特征值的概念只适用于平方矩阵。</li><li id="5699" class="nk nl it lf b lg nt lj nu lz nv ma nw mb nx ly np nq nr ns bi translated">对于方阵，复特征值的存在限制了实特征空间的数量。</li></ul><h2 id="67de" class="mj mk it bd ml mm mn dn mo mp mq dp mr lz ms mt mu ma mv mw mx mb my mz na nb bi translated">奇异值分解</h2><p id="94a2" class="pw-post-body-paragraph lc ld it lf b lg nz ju li lj oa jx ll lz ob lo lp ma oc ls lt mb od lw lx ly im bi translated">为了克服矩阵特征分解带来的挑战，需要对任何矩阵进行更一般的表示，这就是奇异值分解发挥作用的地方。设<strong class="lf iu"> A </strong>为<strong class="lf iu">T37】任意形状的矩形矩阵(m×n)。我们可以证明<strong class="lf iu"> AᵀA </strong>和<strong class="lf iu"> AAᵀ </strong>分别是形状为(n×n)和(m×m)的对称方阵。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/2fae3f87b18e29838144e6c797894f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*6HLiEhf6E9tEAKYK2Jd5sQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ne"> AᵀA </strong>和<strong class="bd ne"> AAᵀ </strong>对称性的证明</p></figure><p id="4d8a" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">此外，还可以证明<strong class="lf iu"> AᵀA </strong>和<strong class="lf iu"> AAᵀ </strong>共享相同的非零特征值。如果一个比另一个有更多的特征值，所有多余的特征值应该为零。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/88af9e76aefeb855bf26916f0e7ecbf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*MCazOO6H8eNPCJ2QUK3F-w.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两个矩阵共享相同的特征值λ，而特征向量不同</p></figure><p id="cdfc" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">根据对称矩阵的正交可对角化性质，矩阵<strong class="lf iu"> AᵀA </strong>和<strong class="lf iu"> AAᵀ </strong>可以分解如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/e561b80ac39abb347bfe790bd6b7eca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/format:webp/1*MA2scfqRvZrWox1vwhNtCA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">d 是对角矩阵的特征值，V，U 是正交矩阵</p></figure><p id="b36d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">假设<strong class="lf iu"> <em class="le"> v </em> </strong>是<strong class="lf iu"> AᵀA </strong>的任意特征向量，大小为(n×1)。乘法 A <strong class="lf iu"> <em class="le"> v </em> </strong>将产生大小为(m×1)的向量，因为<strong class="lf iu"> A </strong>是形状为(m×n)的矩形矩阵。为了节省时间，我们假设 A<strong class="lf iu"><em class="le">v</em>=</strong>σ<strong class="lf iu"><em class="le">k</em></strong>其中<strong class="lf iu"> <em class="le"> k </em> </strong>是一个大小为(m×1)的向量，σ是一个标量值。如果矩阵<strong class="lf iu"> <em class="le"> D </em> </strong>的秩为<em class="le"> r，</em>则矩阵<strong class="lf iu"> AᵀA </strong>和<strong class="lf iu"> AAᵀ.有<em class="le"> r </em>个非零特征值</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/582627a8acc558eae414b57d8f0953c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*WvDo_oEPBwVH72WmQowl_Q.jpeg"/></div></figure><p id="34bc" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">既然我们知道<strong class="lf iu"> V </strong>是正交矩阵，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/56874c785210a3a0cfa46eec42c31a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*iJdTIzvMzXXHT984NJzQgQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这里σ是σ值的对角矩阵</p></figure><p id="7d90" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在让我们做一些简单的数学运算来看看关于<strong class="lf iu"> <em class="le"> K. </em> </strong>的一些令人兴奋的结果</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/5d91e0fd5213575290fc75c68dad7b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*Atnlc6nAg5yCPTc3ShxoXA.jpeg"/></div></figure><p id="46cf" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在很明显<strong class="lf iu"> <em class="le"> K </em> </strong>不过是<strong class="lf iu"><em class="le"/></strong>aaᵀ.的特征向量矩阵现在用<strong class="lf iu"><em class="le">k</em></strong>by<strong class="lf iu"><em class="le"/></strong>我们可以写成<strong class="lf iu"><em class="le">a = u</em></strong>σ<strong class="lf iu"><em class="le">v</em></strong><em class="le">ᵀ.</em>矩阵的这种通用表示形式称为<strong class="lf iu"> <em class="le">奇异值分解。</em> </strong>在此分解中，我们将<strong class="lf iu"><em class="le"/></strong>中的向量称为<strong class="lf iu">左奇异向量</strong>，而将<strong class="lf iu"> V </strong>中的向量称为<strong class="lf iu">右奇异向量</strong>。</p><blockquote class="kz la lb"><p id="f3cb" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">奇异值分解背后的主要直觉是，矩阵<strong class="lf iu"><em class="it"/></strong>将一组正交向量<strong class="lf iu"> (v) </strong>转换为另一组正交向量<strong class="lf iu"> (u) </strong>，缩放因子为σ。所以<strong class="lf iu"> σ </strong>称为对应于各自奇异向量<strong class="lf iu"> <em class="it"> u </em> </strong>和<strong class="lf iu"><em class="it"/></strong>的奇异值</p></blockquote><p id="eb98" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">矩阵方法的 Eckat-Young-Mirsky 定理</strong>描述了低秩近似。简单来说，它提供了给定数据矩阵中最重要的部分。我们可以把我们分解的矩阵<strong class="lf iu"> <em class="le"> A </em> </strong>写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/fd2ff67b7397a3006c52795b920faa6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*JzrbepDaMLfbpCz_jd3oMg.jpeg"/></div></figure><p id="885d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">上面的等式描述了矩阵加法。<strong class="lf iu">埃克哈特-杨-米尔斯基</strong>定理所陈述的是，<strong class="lf iu"> <em class="le"> Aₖ </em> </strong>这是奇异矩阵的加法直到<em class="le"> k </em> ᵗʰ最大奇异值是矩阵<strong class="lf iu"><em class="le"/></strong>a 的秩<em class="le"> k </em>的最近矩阵，它可以说明如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/83b29a51bd3363fa71a2b790617e9d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*EEsXc7Ad7AiEYASw7aGLSg.jpeg"/></div></figure><p id="090d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">这个结果被证明适用于许多著名的矩阵范数，如 L2 范数、弗罗贝纽斯范数等。矩阵<strong class="lf iu"> X </strong>的协方差矩阵<strong class="lf iu"> S </strong>定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/41cebdf49ec78ac2cdf1ba1a21a39461.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*mlK3onW9SUJmfHkmNiIMLg.jpeg"/></div></figure><p id="27fc" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在我们已经学习了奇异值分解的理论。让我们深入研究 SVD 发挥作用的应用程序。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a0b335400011d40af8b63b206fa48c07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*eAlQS3kn1etXV0tI8hfd3w.jpeg"/></div></figure><h2 id="a7ed" class="mj mk it bd ml mm mn dn mo mp mq dp mr lz ms mt mu ma mv mw mx mb my mz na nb bi translated">主成分分析</h2><p id="674f" class="pw-post-body-paragraph lc ld it lf b lg nz ju li lj oa jx ll lz ob lo lp ma oc ls lt mb od lw lx ly im bi translated"><strong class="lf iu">主成分分析</strong>是一种降维技术，用于许多机器学习应用，包括特征工程和特征提取。PCA 的目标是为给定的数据矩阵找到一组<strong class="lf iu">向量</strong>的<strong class="lf iu">正交基</strong>，使得投影到由向量确定的方向上的数据集的方差<strong class="lf iu">最大化。</strong>通过一组证明，已经确定那些向量(俗称<strong class="lf iu">主成分</strong>)不过是按照对应的<strong class="lf iu">特征值的顺序排列的<strong class="lf iu">协方差矩阵</strong>的<strong class="lf iu">特征向量</strong>。</strong>本文不讨论主成分的详细推导。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/370be93e4d0b3258b2acd9284e4cbda5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*0Uul_12aCRYqv_XsFkj4mQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二维数据矩阵的第一和第二主分量</p></figure><blockquote class="kz la lb"><p id="2a80" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于一个矩阵<strong class="lf iu"> X </strong>，<em class="it"> k </em> ᵗʰ主分量是<strong class="lf iu"> X </strong>的协方差矩阵对应的<em class="it"> k </em> ᵗʰ最大特征值的特征向量。</p></blockquote><p id="c568" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">虽然这可以用协方差矩阵的传统特征分解来处理，但缺点是计算复杂性。由于我们的数据矩阵将是一个巨大的矩阵，包含成千上万的数据，这真的很难计算协方差矩阵的特征值，这有时会导致误差，如<strong class="lf iu">舍入误差。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/34205d4e14d529926510f4863755342c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*twqNoUZhJa2IYimu3k7TbA.jpeg"/></div></figure><h2 id="2454" class="mj mk it bd ml mm mn dn mo mp mq dp mr lz ms mt mu ma mv mw mx mb my mz na nb bi translated">主成分分析中的奇异值分解</h2><p id="ea55" class="pw-post-body-paragraph lc ld it lf b lg nz ju li lj oa jx ll lz ob lo lp ma oc ls lt mb od lw lx ly im bi translated">然而，数学家们已经找到了计算奇异值分解的稳定而精确的方法。其中一种方法可以在这里找到<a class="ae ky" href="http://www.math.iit.edu/~fass/477577_Chapter_12.pdf" rel="noopener ugc nofollow" target="_blank">。在 SVD(<strong class="lf iu"><em class="le">a = u</em></strong>σ<strong class="lf iu"><em class="le">v</em></strong><em class="le"/>中我们知道<strong class="lf iu"> <em class="le"> V </em> </strong>是协方差矩阵的特征向量而它的特征值<strong class="lf iu"> (λ) </strong>隐藏在奇异值<strong class="lf iu">【σ】</strong>中。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/d4c4400cd44730adbd445ad760b781a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*sxu_ys66YnKa-5qYtWhG2Q.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">A 的奇异值与 A 的协方差矩阵的特征值之间的关系。</p></figure><p id="b551" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">由于<em class="le"> n </em>在两种情况下都是常数，因此数据矩阵的主分量是给定矩阵的右奇异向量<strong class="lf iu"> ( <em class="le"> V </em> ) </strong>，按照奇异值的顺序。</p><blockquote class="kz la lb"><p id="ba64" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于一个矩阵<strong class="lf iu"> X </strong>，<em class="it"> k </em> ᵗʰ主分量是<strong class="lf iu"> X </strong>协方差矩阵的右奇异向量对应于<em class="it"> k </em> ᵗʰ最大奇异值。</p></blockquote><p id="08ed" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">因此，我们遇到了一种表示任何矩阵的更通用的方法，<strong class="lf iu">奇异值分解</strong>及其在建模<strong class="lf iu">主成分分析</strong>中的贡献，这是一种在机器学习中提取数据矩阵重要特征的复杂方法。我希望你喜欢这篇文章🙂。请随时让我知道关于这方面的任何意见和建议。</p></div></div>    
</body>
</html>