<html>
<head>
<title>The Transformer Isn’t As Hard To Understand As You Might Think</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器并没有你想象的那么难理解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/knocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8?source=collection_archive---------32-----------------------#2020-07-24">https://towardsdatascience.com/knocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8?source=collection_archive---------32-----------------------#2020-07-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e139" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">破解注意事项—为了解最新的 SOTA 自然语言处理模型打下基础。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a918fcb0c6f7aa3676492dd1a96d214e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9caNFdVe0rm2cwEc59FJgg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由来自<a class="ae kv" href="https://www.pexels.com/photo/person-holding-magnifying-glass-712786/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">佩克斯</a>的<a class="ae kv" href="https://www.pexels.com/@maumascaro?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">毛里西奥·马斯卡罗</a>拍摄</p></figure><p id="9fd1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">T</span><strong class="ky ir">transformer 的</strong>架构已经成为许多最新<strong class="ky ir"> SOTA </strong> NLP 模型开发的基石。它主要依靠一种叫做<strong class="ky ir">注意力</strong>的机制。与之前的其他成功模型不同，它不涉及<strong class="ky ir">卷积</strong>或<strong class="ky ir">递归</strong>层。</p><p id="ee99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你是这个模型的新手，你可能不会发现这个架构是最容易理解的。如果是这样的话，希望这篇文章能有所帮助。</p><p id="638a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将从常规<strong class="ky ir">编码器-解码器</strong>网络如何工作以及它可能会遇到哪些<strong class="ky ir">困难</strong>开始解释，常规编码器-解码器架构中使用的<strong class="ky ir">注意</strong> <strong class="ky ir">机制</strong>是什么，最后，它如何用于<strong class="ky ir">变压器</strong>。</p><h2 id="1e24" class="mb mc iq bd md me mf dn mg mh mi dp mj lf mk ml mm lj mn mo mp ln mq mr ms mt bi translated"><strong class="ak">用于神经机器翻译的编码器-解码器网络</strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/a12b934fcd6521943ddeb044012e0df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KzuY8fbmFFMT6TMbnCc-iw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">编码器-解码器架构。图片<a class="ae kv" href="https://6chaoran.wordpress.com/2019/01/15/build-a-machine-translator-using-keras-part-1-seq2seq-with-lstm/" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="f7c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">左边的图像显示了一个编码器-解码器架构，两个组件都由<strong class="ky ir">递归</strong>层组成。</p><p id="9f9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在左边，编码器接收输入句子，每个单词由它们的<strong class="ky ir">嵌入</strong>表示，并期望为输入句子输出良好的<strong class="ky ir">摘要</strong>。这个概要被称为<strong class="ky ir">上下文</strong> <strong class="ky ir">向量</strong>(连接二者的箭头)，并作为其<strong class="ky ir">初始</strong> <strong class="ky ir">状态</strong>被馈送给解码器。</p><p id="a5ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">右边的<strong class="ky ir">解码器</strong>负责输出<strong class="ky ir">翻译</strong>，每步一个单词。在训练期间，它将<strong class="ky ir">目标</strong> <strong class="ky ir">句子作为输入。</strong>在进行预测时，它用来自<strong class="ky ir">最后一个</strong> <strong class="ky ir">步骤</strong>的<strong class="ky ir">输出</strong>进行自我反馈(如此处所示)。</p><p id="b141" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种架构优于更简单的<strong class="ky ir">序列到序列</strong> RNN，因为来自编码器的<strong class="ky ir">上下文</strong> <strong class="ky ir">向量</strong>整体上提供了对<strong class="ky ir">输入</strong>语句的更多<strong class="ky ir">直接</strong> <strong class="ky ir">访问</strong>。因此，解码器在输出单个翻译之前可以通过上下文向量查看整个句子，而常规的序列到序列 RNN 只能访问位于当前时间步长之前的单词。</p><p id="429e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在所有递归层中，<strong class="ky ir">信息</strong>通过<strong class="ky ir">隐藏</strong> <strong class="ky ir">状态</strong>从一个时间步传递到另一个时间步，随着更多时间步的执行，它们逐渐被<strong class="ky ir">遗忘</strong>。当编码一个<strong class="ky ir">长的</strong>序列时，输出上下文向量很可能已经忘记了关于句子的第一个<strong class="ky ir"/><strong class="ky ir">成分</strong>的许多信息。这同样适用于解码器，如果序列太长<strong class="ky ir"/>，包含在上下文向量<strong class="ky ir">中的信息不会</strong> <strong class="ky ir">传递</strong> <strong class="ky ir">向下</strong>到最后几个时间步。</p><h2 id="ea64" class="mb mc iq bd md me mf dn mg mh mi dp mj lf mk ml mm lj mn mo mp ln mq mr ms mt bi translated"><strong class="ak">注意事项</strong></h2><p id="4208" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">为了解决这个<strong class="ky ir">内存</strong> <strong class="ky ir">问题</strong>并提高模型性能，引入了<strong class="ky ir">注意</strong> <strong class="ky ir">机制</strong>、<strong class="ky ir">双向</strong>递归层，以及对模型结构的一些修改。</p><p id="628c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">编码器现在由<strong class="ky ir">双向</strong> LSTM/GRU 组成，因此输入句子在<strong class="ky ir">两个</strong> <strong class="ky ir">方向</strong>被读取。除了<strong class="ky ir">上下文</strong> <strong class="ky ir">向量</strong>之外，编码器现在还输出由来自每个编码步骤的每个输入字的编码组成的<strong class="ky ir">序列</strong>。在每个时间步，编码序列在<strong class="ky ir">被完全<strong class="ky ir">传递给解码器。由于解码器每次输出一个字，所以<strong class="ky ir">注意</strong> <strong class="ky ir">机制</strong>用于<strong class="ky ir">选择</strong>要处理的编码序列的正确部分，以及编码器自己的输入。</strong></strong></p><p id="3bc5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图说明，尽管使用了相同的编码器输出，但不同的时间步长对编码器序列的不同部分给予不同程度的关注。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/e5cc51c564dc58047233673339f989ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oQnb4ADDxd46ggQu190nsw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">编码器输出的加权和。为了翻译每个单词，这个序列的不同部分被赋予不同的重要性。图片<a class="ae kv" href="https://blog.floydhub.com/attention-mechanism/" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="5d1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种选择是通过用一组<strong class="ky ir">注意</strong> <strong class="ky ir">权重</strong>对<strong class="ky ir">编码器</strong> <strong class="ky ir">序列</strong>执行<strong class="ky ir">加权</strong> <strong class="ky ir">求和</strong>来完成的，所述权重由求和为 1 的浮点组成。对于给定的时间步长，编码序列中只有少数部分值得<strong class="ky ir">关注</strong>，并且每一步使用不同的权重集。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="3937" class="mb mc iq nc b gy ng nh l ni nj">Say that we have only one input sentence X = [x1, x2, ..., xn] in our batch, it has <em class="nk">N</em> words and each is represented by its embedding vector of length <em class="nk">M</em>. The input sentence is then a sequence with a shape of <em class="nk">N </em>x <em class="nk">M</em>.</span><span id="5c1f" class="mb mc iq nc b gy nl nh l ni nj">The encoder is used to output a good summary of the input sentence by transforming X into its encodings Z = [z1, z2, ..., zn], with zi being the encoding of the word in the i-th position.</span><span id="e23b" class="mb mc iq nc b gy nl nh l ni nj">The dimension of each encoding vector z is determined by the number of neurons from the encoder's the last layer.</span><span id="a668" class="mb mc iq nc b gy nl nh l ni nj">A set of attention weights ⍺ = [⍺1, ⍺2, ..., ⍺n], at a given time step <em class="nk">t</em>, is a <em class="nk">N</em>-dimensional vector. Each element multiples one word encoding from Z, outputting a weighted sum, ∑⍺i x zi.</span></pre><p id="7a26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些注意力权重是可学习的<strong class="ky ir">参数</strong>，通过测量编码器<strong class="ky ir">序列</strong><strong class="ky ir"><em class="nk">S</em></strong><em class="nk"/>与解码器的<strong class="ky ir">最后</strong> <strong class="ky ir">隐藏</strong> <strong class="ky ir">状态</strong> <strong class="ky ir"> <em class="nk"> h </em> </strong>中每个元素的<strong class="ky ir">兼容性</strong>来确定。来自编码器序列的向量与该隐藏状态越相似，其对应的注意力权重就越大。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/80d1c2df8f7b5dd0d5debbb0922f53e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CkfLGy7psBPLXcgpvJ9g_w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乘法/点积注意力。</p></figure><p id="b1fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nk"> h </em>和<em class="nk"> S </em>之间的<strong class="ky ir">点</strong> <strong class="ky ir">积</strong>测量<em class="nk"> S </em>中的每个向量与<em class="nk"> h </em>的<strong class="ky ir">相似度</strong>，通过应用<strong class="ky ir"> softmax </strong>我们将结果转化为<strong class="ky ir">分布</strong>。整个过程可以看做是模型在使用<strong class="ky ir">隐藏</strong> <strong class="ky ir">状态</strong> <strong class="ky ir"> <em class="nk"> h </em> </strong>作为<strong class="ky ir">查询</strong>来寻找<strong class="ky ir"><em class="nk"/></strong><em class="nk">中最相似的<strong class="ky ir">元素</strong>。T </em>要检索它们，用获得的注意力权重乘以<em class="nk"> S </em>。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="4bf4" class="mb mc iq nc b gy ng nh l ni nj">But why attentions are determined using the previous hidden state, and why is this similarity important?</span><span id="d0f0" class="mb mc iq nc b gy nl nh l ni nj">Say that we are using this model to translate a sentence from one language to another and we’ve just finished translating one word and we’re working on the next output.</span><span id="894e" class="mb mc iq nc b gy nl nh l ni nj">The hidden state of the decoder from the previous step contains information regarding the last output. Since the model is trained to predict the next word, this hidden state can be seen as something related to what the model wants to predict as the next word. It’s then logical to search for information similar to the hidden state from the encoder sequence to produce the next translation.</span></pre><h2 id="a831" class="mb mc iq bd md me mf dn mg mh mi dp mj lf mk ml mm lj mn mo mp ln mq mr ms mt bi translated"><strong class="ak">自我关注</strong></h2><p id="59a9" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">到目前为止，<strong class="ky ir">解码器</strong>使用注意力来映射其隐藏状态，编码器输出用来找到需要注意的相关部分。在一个<strong class="ky ir">转换器</strong>中，这个机制也被它的<strong class="ky ir">编码器</strong>用来从一个句子中找到与句子本身相关的信息。换句话说，对于一个句子中的一个给定单词，在产生它的编码时要考虑更多的上下文。这叫<strong class="ky ir">自我关注</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/530ecbafcfe66f3551f614d9b41b1d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1PXABaqC0vL-miE4hdQf-Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对单词“it”的编码器自我注意分布从第 5 层到第 6 层的转换器被训练用于英语到法语的翻译。图片来源。</p></figure><p id="d7eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这张图片以一种形象的方式展示了自我关注。每个单词与单词<strong class="ky ir"><em class="nk">【it】</em></strong>的相关性由连接它们的线的强度来表示。</p><p id="f3de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，并不是所有的单词都与单词<strong class="ky ir">'<em class="nk">' it</em>'</strong>具有相同的<strong class="ky ir">相关性，句子中的不同单词对该单词具有不同程度的重要性，包括非常<em class="nk"> 'it' </em>。</strong></p><p id="df74" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是<strong class="ky ir">自我关注</strong>所做的，每个单词的编码包含了它的<strong class="ky ir">关系</strong>和其他关于一个<strong class="ky ir">句子</strong>中所有单词的信息。这可以为编码器提供更好地理解单词的潜在<strong class="ky ir">句法</strong>和<strong class="ky ir">语义</strong>含义的能力。</p><p id="03b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用于执行自我关注的等式与上面所示的等式相同，只是现在不是使用<strong class="ky ir">隐藏</strong> <strong class="ky ir">状态<em class="nk">h</em>T5】，而是使用<strong class="ky ir">句子</strong> <strong class="ky ir">本身</strong>作为<strong class="ky ir">查询</strong>，并且<strong class="ky ir"> softmax </strong>内的值被缩放以避免 softmax 具有<strong class="ky ir">可忽略的</strong> <strong class="ky ir">梯度</strong>。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/3b2f0094fdf9f2f383e90e88cb7cb4b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*hGZQukkk_7V642_l65rQFg.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">成比例的点积注意力。图片来自<a class="ae kv" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">原纸</a>。</p></figure><p id="f70f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在引擎盖下，注意力只是模型执行<strong class="ky ir">查找</strong>的工具，将带有一组<strong class="ky ir">键值</strong>对的<strong class="ky ir">查询</strong>映射到输出。仅当相应的<strong class="ky ir">关键字</strong>与<strong class="ky ir">查询</strong>匹配时，才检索来自值的组件。</p><p id="7426" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">早期情况下，<strong class="ky ir"> Q </strong>为<strong class="ky ir">隐藏</strong> <strong class="ky ir">状态</strong><strong class="ky ir"><em class="nk">h</em></strong><em class="nk"/><strong class="ky ir">K</strong>和<strong class="ky ir"> V </strong>均为<strong class="ky ir">编码器输出<em class="nk"> S </em> </strong>。<strong class="ky ir"> <em class="nk"> S </em> </strong>和<strong class="ky ir"> <em class="nk"> h </em> </strong>用于查找<strong class="ky ir">注意</strong> <strong class="ky ir">权重⍺ </strong>，然后通过执行<strong class="ky ir"> ⍺ ⋅ S </strong>从<strong class="ky ir"> S </strong>中检索<strong class="ky ir">匹配</strong>组件。</p><p id="1522" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于<strong class="ky ir">循环</strong>层的顺序性质，我们无法将所有隐藏状态打包在一个<strong class="ky ir">矩阵</strong>中，并一次性找到所有注意力权重。在每个<strong class="ky ir">时间</strong>步骤<strong class="ky ir">找到注意力权重。</strong></p><p id="98a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了执行自我关注，所有的值<strong class="ky ir"> Q </strong>、<strong class="ky ir"> K </strong>和<strong class="ky ir"> V </strong>都是来自<strong class="ky ir">输入</strong> <strong class="ky ir">句子</strong>的同一个单词列表，因此对<strong class="ky ir">单词</strong>和<strong class="ky ir">本身</strong>进行比较。与之前的情况不同，现在所有单词的所有注意力权重都可以在一个镜头中处理，因为它不受循环层的顺序性质的限制。</p><p id="9aba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们来看看<strong class="ky ir">变压器</strong>架构，还有更多值得挖掘的地方。</p><h2 id="dd48" class="mb mc iq bd md me mf dn mg mh mi dp mj lf mk ml mm lj mn mo mp ln mq mr ms mt bi translated"><strong class="ak">变压器和多头注意</strong></h2><p id="10f5" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">该模型再次由一个<strong class="ky ir">编码器-解码器</strong>结构组成，每个编码器-解码器又由 N(=6)个编码器/解码器模块<strong class="ky ir">相互堆叠</strong>组成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/1a1853de78b63d7efb65e39d4c1eaa64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3S--bVvDDRp_sP45A4JOsQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">全变压器架构。图片来自<a class="ae kv" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">原创论文</a></p></figure><p id="7bf2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个编码器/解码器块将其<strong class="ky ir">输出</strong>传递给下一个<strong class="ky ir">编码块</strong>，并且来自<strong class="ky ir">最后一个</strong>编码块的输出被传递给 N 个解码块中的每个编码块<strong class="ky ir">。</strong></p><p id="873c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个编解码块中有不同的<strong class="ky ir">子层</strong>:<strong class="ky ir">与层归一化层</strong>跳过连接，一个由<strong class="ky ir">两个</strong> <strong class="ky ir">密集</strong> <strong class="ky ir">层</strong>组成的“<strong class="ky ir">前馈</strong>模块，最后一个是<strong class="ky ir">多头关注</strong>模块。在每个<strong class="ky ir">解码器</strong>块的底部增加了一个<strong class="ky ir">屏蔽的</strong>多头注意力，行为略有不同。这里的主要目标是理解<strong class="ky ir">多头注意力</strong>是如何运作的，以及注意力是如何在其中执行的。</p><p id="a81f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如你所见，在这个模型中没有<strong class="ky ir">卷积</strong>或<strong class="ky ir">递归</strong>层，整个架构完全依赖于<strong class="ky ir">注意</strong>机制。</p><p id="3926" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">多头警示模块</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/a69effc629dfaa47d73c1362cb61d528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PCaHpx1RBZ4IGTqkxEthqg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">多头注意力模式示意图</p></figure><p id="cc0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本质上，一个多头注意力模块只是一些<strong class="ky ir">规模的点积注意力</strong>并行工作。对于每个注意力，输入<strong class="ky ir"> <em class="nk"> Q </em> </strong>、<strong class="ky ir"> <em class="nk"> K </em> </strong>和<strong class="ky ir"> <em class="nk"> V </em> </strong>首先被<strong class="ky ir">投影</strong>到不同的<strong class="ky ir">子空间</strong>，使用不同的权重矩阵<strong class="ky ir"> <em class="nk">、Wk 和 Wv </em> </strong>。通过这种方式，每个注意力开始处理输入的不同方面。它们的输出然后被线性组合。</p><p id="bb97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它在<strong class="ky ir">编码器</strong>中的行为非常简单，它执行<strong class="ky ir">自我关注</strong>，其中<strong class="ky ir"> Q </strong>、<strong class="ky ir"> K </strong>、<strong class="ky ir"> V </strong>都是<strong class="ky ir">输入</strong>语句中的单词的相同 l <strong class="ky ir"> ist。每个编码器模块将其输出传递给下一个，最后一个输出馈给所有解码器模块。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/94739a42000dc6a27d328ab4413975e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BSUp57U3TPaggHOi6_HITA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">多头关注。</p></figure><p id="ac41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于解码器来说，<strong class="ky ir">屏蔽</strong>多头注意力将自我注意力应用于<strong class="ky ir">目标</strong> <strong class="ky ir">句子</strong>(在训练期间)，其中<strong class="ky ir">未来</strong>单词被<strong class="ky ir">屏蔽</strong> <strong class="ky ir">排除</strong>，防止任何单词将其自身与位于其后的单词进行比较。在推断时间，解码器只能访问其最后的<strong class="ky ir">输出，因此不需要屏蔽。该模块的<strong class="ky ir">输出</strong>作为<strong class="ky ir"> Q </strong>传递给上方的多头注意力，多头注意力从<strong class="ky ir">编码器</strong>接收编码，并将其作为<strong class="ky ir"> K </strong>和<strong class="ky ir"> V </strong>。它的输出<strong class="ky ir">与来自掩蔽注意模块的输出</strong>重新组合，并被传递以供进一步处理。</strong></p><p id="7a40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，解码器输出被传递到一个<strong class="ky ir">密集</strong>层，最后的<strong class="ky ir"> softmax </strong>输出当前步骤的翻译。</p><h2 id="e528" class="mb mc iq bd md me mf dn mg mh mi dp mj lf mk ml mm lj mn mo mp ln mq mr ms mt bi translated">最后的话</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/596d296d3f61f7e9fa1f3f1766aa8837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IX-l5RiaY0aQtIo0yrjVEg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">这是一个关于变形金刚的绝妙类比！</p></figure><p id="29d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文到此为止。如果你喜欢它的内容，可以看看我的其他作品。如果有任何错误，最好能在评论中看到。</p><p id="c069" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PS:阿尔弗雷多有一门关于深度学习的<a class="ae kv" href="https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw" rel="noopener ugc nofollow" target="_blank">免费课程</a>。有兴趣就去看看，告诉我有多差！</p></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><div class="kg kh ki kj gt oa"><a rel="noopener follow" target="_blank" href="/an-overview-for-text-representations-in-nlp-311253730af1"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">自然语言处理中的文本表示综述</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">讨论自然语言处理中三种最常用的输入类型。</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo kp oa"/></div></div></a></div><div class="op oq gp gr or oa"><a rel="noopener follow" target="_blank" href="/ensemble-learning-from-scratch-20672123e6ca"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">从零开始的集成学习</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">引入集成学习，这是一种通过组合训练好的模型来提高性能的强大工具。</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="os l ol om on oj oo kp oa"/></div></div></a></div></div></div>    
</body>
</html>