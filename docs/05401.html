<html>
<head>
<title>DIY: Apache Spark &amp; Docker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DIY: Apache Spark &amp; Docker</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/diy-apache-spark-docker-bb4f11c10d24?source=collection_archive---------3-----------------------#2020-05-07">https://towardsdatascience.com/diy-apache-spark-docker-bb4f11c10d24?source=collection_archive---------3-----------------------#2020-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="35e3" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">全民数据科学</h2><div class=""/><div class=""><h2 id="4c63" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">被亲切地称为“黑暗”或“斯波克”</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/8bc8743059d403e63f3679e465fd9c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WrdzQ0LdsuLTdZeCbS5rMA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在Docker容器内运行的完全分布式Spark集群</p></figure></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="2796" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">介绍</h1><p id="8540" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">在过去几年中流行起来的两种技术是Apache Spark和Docker。</p><p id="7604" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">Apache Spark为用户提供了一种以分布式方式执行CPU密集型任务的方法。与Hadoop等其他分布式技术相比，它的速度更快，因此在过去几年中，它的采用率一直在稳步上升。2014年<a class="ae nh" href="https://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html" rel="noopener ugc nofollow" target="_blank"> Spark赢得了Gray Sort基准测试</a>,在该测试中，他们对100TB数据的排序速度比以前的Hadoop集群快3倍，使用的机器数量少10倍。</p><p id="a257" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">另一方面，Docker在各种情况下被广泛采用。Docker为用户提供了定义环境最小规格的能力，这意味着您可以轻松地开发、发布和扩展应用程序。此外，由于使用了linux容器，用户可以开发Docker容器，这些容器可以在一台服务器上同时运行，同时保持相互隔离。最后，Docker提供了一个名为Docker Engine的抽象层，它保证了可以运行Docker的机器之间的兼容性，解决了“它在我的机器上可以工作，我不知道为什么它在你的机器上不能工作”这个老问题。</p><p id="0a4b" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">随着大数据的兴起，这两种技术是天作之合。Apache Spark提供分析引擎来处理数据，Docker提供快速、可扩展的部署以及一致的环境。</p><h2 id="9992" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">先决条件</h2><p id="bd6e" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">我假定您了解Docker命令和术语以及Apache Spark概念。因此，如果您对这两种技术都不熟悉，我不推荐您阅读本文。出于简洁的考虑，本文将有意省略正在发生的事情的大部分细节。对于架构的完整描述和更连续的过程，我将读者引向我的<a class="ae nh" href="https://github.com/sdesilva26/docker-spark/blob/master/TUTORIAL.md" rel="noopener ugc nofollow" target="_blank"> github repo </a>。</p><p id="bddc" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">我还假设您至少有云提供商的基本经验，因此能够在您的首选平台上设置一个计算实例。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><p id="8024" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">本文的其余部分将直接介绍不同层次的架构复杂性:</p><ol class=""><li id="3276" class="nt nu it mi b mj nc mm nd mp nv mt nw mx nx nb ny nz oa ob bi translated">Docker容器网络—本地机器</li><li id="cdaa" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">Docker容器网络—多台机器</li><li id="f42b" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">Apache Spark集群—本地机器</li><li id="a0b5" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">Docker &amp; Spark —本地机器</li><li id="d495" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">Docker &amp; Spark —多台机器</li><li id="79b8" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">奖励:Docker Stack和Spark</li></ol><p id="89e7" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">我们开始吧！</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">【<a class="ae nh" href="https://giphy.com/gifs/reaction-typing-unpopular-opinion-13GIgrGdslD9oQ/links" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="2a62" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">Docker —本地机器</h1><p id="2077" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">首先，我们需要掌握一些基本的Docker网络。首先，我们将对运行在同一台机器上的容器执行此操作。</p><ol class=""><li id="fc56" class="nt nu it mi b mj nc mm nd mp nv mt nw mx nx nb ny nz oa ob bi translated">要做的第一件事是使用my repo中的<a class="ae nh" href="https://github.com/sdesilva26/docker-spark/tree/master/docker" rel="noopener ugc nofollow" target="_blank">docker文件</a>构建docker映像，或者更方便的是使用以下命令提取docker映像</li></ol><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="f13f" class="ni lp it ok b gy oo op l oq or">docker pull sdesilva26/spark_master:0.0.2<br/>docker pull sdesilva26/spark_worker:0.0.2</span></pre><p id="10e8" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><em class="os">注意:为了这个部分的目的，任何图像都可以。</em></p><p id="1c9a" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">2.通过运行以下命令创建桥接网络</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="ce7a" class="ni lp it ok b gy oo op l oq or">docker network create --driver bridge spark-net-bridge<!-- --> </span></pre><p id="2683" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">3.通过运行以下命令，在这个用户定义的桥接网络上运行两个容器</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="ee8c" class="ni lp it ok b gy oo op l oq or">docker run -dit --name spark-master --network spark-net-bridge --entrypoint /bin/bash sdesilva26/spark_master:0.0.2<br/>docker run -dit --name spark-worker1 --network spark-net-bridge --entrypoint /bin/bash sdesilva26/spark_worker:0.0.2<br/> </span></pre><p id="465c" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">4.检查网络并找到两个容器的IP地址</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="2bb3" class="ni lp it ok b gy oo op l oq or">docker network inspect spark-net-bridge</span></pre><p id="8a05" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">上面的输出应该如下图所示</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/de885fc82816d4c2c3f2671213ccbb29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhSNk0uVRnVYIHJRD2BzWA.png"/></div></div></figure><p id="0138" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">5.连接到spark-master容器，使用它的IP地址和容器名测试它与spark-worker容器的通信</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="f025" class="ni lp it ok b gy oo op l oq or">ping -c 2 172.24.0.3<br/>ping -c 2 spark-worker</span></pre><p id="5718" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">因为容器已经被部署到相同的Docker桥网络中，所以它们能够使用容器的名称来解析其他容器的IP地址。这叫做<strong class="mi jd">自动服务发现</strong>，以后会给我们很大帮助。</p><h2 id="0558" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">体系结构</h2><p id="d8c5" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">使用上述命令，我们创建了以下架构。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/385554769b4fe9efd1a2edc4d4d8f5b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ny8tUzGEAn32xqY595C3Ng.png"/></div></div></figure></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="44ba" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">Docker —多台机器</h1><ol class=""><li id="223c" class="nt nu it mi b mj mk mm mn mp ov mt ow mx ox nb ny nz oa ob bi translated">继续在您最喜欢的云提供商上设置2个实例</li><li id="2fd1" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">在配置实例的网络时，确保将它们部署到同一个子网中</li><li id="f7bc" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">打开以下端口，以便容器相互通信，并用于覆盖网络流量(入站和出站)；</li></ol><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="239c" class="ni lp it ok b gy oo op l oq or">Protocol  |  Port(s)  | Source<br/>TCP       |  2377     | &lt;your-security-group-or-subnet-name&gt;<br/>TCP       |  7946     | &lt;your-security-group-or-subnet-name&gt;<br/>UDP       |  7946     | &lt;your-security-group-or-subnet-name&gt;<br/>UDP       |  4789     | &lt;your-security-group-or-subnet-name&gt;</span></pre><p id="ad57" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">4.打开以下端口，以便实例与docker hub进行通信(入站和出站)；</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="539b" class="ni lp it ok b gy oo op l oq or">Protocol  |  Port(s)  | Source<br/>HTTPS     |  443      | 0.0.0/0, ::/0</span></pre><p id="1679" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">例如，在AWS上，我的两个实例部署在其中的安全组具有以下安全组设置</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/ec6e7df3746a1dcb74e318d1f10c1c21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VmmT9HRsBI-g39PMhl5qcg.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/f3478a6ba03365e6f37f601ff0bb6bf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvV_bAWYhdrEukxhBQORcw.png"/></div></div></figure><p id="cb90" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">其中"<a class="ae nh" href="https://eu-west-2.console.aws.amazon.com/ec2/v2/home?region=eu-west-2#SecurityGroup:groupId=sg-0140fc8be109d6ecf" rel="noopener ugc nofollow" target="_blank">SG-0140 fc 8 be 109 D6 ECF</a>(docker-spark-tutorial)"是安全组本身的名称，因此只有来自网络内部的流量可以使用端口2377、7946和4789进行通信。</p><p id="2de1" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">5.安装docker。</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="21d4" class="ni lp it ok b gy oo op l oq or">sudo yum install docker -y<br/>sudo service docker start<br/>sudo usermod -a -G docker ec2-user # This avoids you having to use sudo everytime you use a docker command (log out and then in to your instance for this to take affect)</span></pre><p id="c701" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">6.在实例1中，提取您选择的docker图像。</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="7871" class="ni lp it ok b gy oo op l oq or">docker pull sdesilva26/spark_master:0.0.2</span></pre><p id="dbe9" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">7.将另一个图像拖到实例2上。</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="493d" class="ni lp it ok b gy oo op l oq or">docker pull sdesilva26/spark_worker:0.0.2</span></pre><p id="f179" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">8.初始化docker群，并通过运行以下命令使实例1成为群管理器</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="1238" class="ni lp it ok b gy oo op l oq or">docker swarm init</span></pre><p id="7299" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">在实例1上。</p><p id="17b2" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">9.从实例1复制上面命令的输出，并在实例2上运行它，以作为工作者节点加入群</p><p id="9a1a" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">10.在实例1(群管理器)上创建一个覆盖网络</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="1e06" class="ni lp it ok b gy oo op l oq or">docker network create -d overlay --attachable spark-net</span></pre><p id="3e9b" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">11.在实例1上，运行一个容器</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="ea99" class="ni lp it ok b gy oo op l oq or">docker run -it --name spark-master --network spark-net --entrypoint /bin/bash sdesilva26/spark_master:0.0.2</span></pre><p id="db62" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">12.在实例2中，在swarm manager创建的覆盖网络中运行一个容器</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="7875" class="ni lp it ok b gy oo op l oq or">docker run -it --name spark-worker --network spark-net --entrypoint /bin/bash sdesilva26/spark_worker:0.0.2</span></pre><p id="306e" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">13.从实例2上的容器内部，通过ping实例1上运行的容器来检查容器通信</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="8db5" class="ni lp it ok b gy oo op l oq or">ping -c 2 spark-master</span></pre><p id="8ecd" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">14.同样，检查从实例1中的容器到实例2中的容器的向后连接</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="5a27" class="ni lp it ok b gy oo op l oq or">ping -c 2 spark-worker</span></pre><p id="22cc" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">像以前一样，容器能够仅使用容器名称来解析彼此的IP地址，因为它们在相同的覆盖网络内。</p><h2 id="6607" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">体系结构</h2><p id="2e83" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">按照上面的说明，您已经创建了一个类似于下面的架构。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/af8d8c8decc1adc7b2590a002f9c7d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJLhsvrwurVckxKySyPpiw.png"/></div></div></figure></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="ac0a" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">Apache Spark —本地机器</h1><p id="63dd" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">现在我们已经掌握了如何让两个不同的docker主机进行通信，我们将开始在本地机器上创建一个Spark集群。</p><ol class=""><li id="3382" class="nt nu it mi b mj nc mm nd mp nv mt nw mx nx nb ny nz oa ob bi translated">从他们的<a class="ae nh" href="https://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">网站</a>安装Spark</li><li id="c662" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">从命令行导航到Spark安装的bin目录</li><li id="455c" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">设置Spark主节点</li></ol><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="f274" class="ni lp it ok b gy oo op l oq or">./spark-class org.apache.spark.deploy.master.Master</span></pre><p id="2dca" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">4.通过导航到<a class="ae nh" href="http://localhost:8080." rel="noopener ugc nofollow" target="_blank"> http://localhost:8080，检查您主节点是否已经成功部署。</a>您应该会看到以下内容</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/b0145ab95504eab04bb44c86dccb7749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JrarmQjKg58CVBkxmIf9Lw.png"/></div></div></figure><p id="3d1a" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">5.将一个工作节点连接到群集</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="81d3" class="ni lp it ok b gy oo op l oq or">./spark-class org.apache.spark.deploy.worker.Worker -c 1 -m 3G spark://localhost:7077</span></pre><p id="cc18" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">其中这两个标志定义了您希望该工作线程拥有的内核和内存数量。最后一个输入是前缀为“spark://”的主节点的地址和端口，因为我们使用的是spark的独立集群管理器</p><p id="7b4c" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">6.通过返回到<a class="ae nh" href="http://localhost:8080." rel="noopener ugc nofollow" target="_blank"> http://localhost:8080，检查worker是否已成功注册到主节点。</a>现在，您应该看到worker节点是集群的一个资源。(您也可以通过访问http://localhost:8081来检查worker的UI)</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/ebdb8371e7e7403f4b5bf36dab743584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6KHZe6nN0AlZGluJCO_pkA.png"/></div></div></figure><p id="676c" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">7.通过从spark安装的bin目录中打开一个scala shell来测试集群</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="a424" class="ni lp it ok b gy oo op l oq or">./spark-shell --master spark://localhost:7077</span></pre><p id="178b" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">和跑步</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="6167" class="ni lp it ok b gy oo op l oq or">val NUM_SAMPLES=10000<br/>var count = sc.parallelize(1 to NUM_SAMPLES).filter { _ =&gt;<br/>  val x = math.random<br/>  val y = math.random<br/>  x*x + y*y &lt; 1<br/>}.count() * 4/(NUM_SAMPLES.toFloat)</span></pre><p id="eaa4" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">这将返回圆周率的估计值。</p><p id="dc3f" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">8.通过转到<a class="ae nh" href="http://localhost:4040." rel="noopener ugc nofollow" target="_blank"> http://localhost:4040检查应用程序的UI。</a>您应该会看到类似于</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/b621c6c3a5c36ebe7354d745c7d21b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cJMYzIPIJXV-HbPisScYYA.png"/></div></div></figure><p id="6bca" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">你现在有一个功能齐全的火花簇！</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="df71" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated"><strong class="ak">火花&amp;对接机—本地机</strong></h1><p id="290f" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">现在是时候将两者结合起来了。我们现在将通过在本地机器上的Docker容器中设置一个运行的Spark集群来学习先走后跑</p><ol class=""><li id="cef7" class="nt nu it mi b mj nc mm nd mp nv mt nw mx nx nb ny nz oa ob bi translated">创建一个用户定义的桥接网络(如果您还没有这样做)</li></ol><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="ad90" class="ni lp it ok b gy oo op l oq or">docker create network -d bridge spark-net</span></pre><p id="489d" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">2.在桥接网络内部创建一个Spark主节点</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="b38b" class="ni lp it ok b gy oo op l oq or">docker run -it --name spark-master --network spark-net -p 8080:8080 sdesilva26/spark_master:0.0.2 bash</span></pre><p id="86c4" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">3.通过导航到<a class="ae nh" href="http://localhost:8080" rel="noopener ugc nofollow" target="_blank"> http://localhost:8080 </a>，检查容器是否成功启动了Spark主节点。我已经将sdesilva26/spark_master:0.0.2映像设置为默认设置一个主节点。参见<a class="ae nh" href="https://github.com/sdesilva26/docker-spark/blob/master/docker/Dockerfile_master" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="7eaf" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">4.在桥接网络内部创建一个Spark工作节点</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="a6e7" class="ni lp it ok b gy oo op l oq or">docker run -dit --name spark-worker1 --network spark-net -p 8081:8081 -e MEMORY=2G -e CORES=1<br/>  sdesilva26/spark_worker:0.0.2 bash</span></pre><p id="3e15" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">缺省情况下，sdesilva26/spark_worker:0.0.2映像在运行时将尝试加入一个spark集群，其主节点位于spark://spark-master:7077。</p><p id="b17b" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">如果您更改运行Spark主节点的容器的名称(步骤2)，那么您需要将这个容器名称传递给上面的命令，例如-e <master_container_name>。见<a class="ae nh" href="https://github.com/sdesilva26/docker-spark/blob/master/docker/Dockerfile_worker" rel="noopener ugc nofollow" target="_blank"> dockerfile此处</a>。</master_container_name></p><p id="b8db" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">5.同样，通过导航到<a class="ae nh" href="http://localhost:8080" rel="noopener ugc nofollow" target="_blank"> http://localhost:8080 </a>和<a class="ae nh" href="http://localhost:8081." rel="noopener ugc nofollow" target="_blank"> http://localhost:8081，验证工作者已经成功注册到主节点。</a></p><p id="5cdf" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">6.将第二个spark worker连接到集群</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="a7e0" class="ni lp it ok b gy oo op l oq or">docker run -dit --name spark-worker2 --network spark-net -p 8082:8081 -e MEMORY=2G -e CORES=1<br/>  sdesilva26/spark_worker:0.0.2 bash</span></pre><p id="52b4" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">我们必须对步骤4中的命令进行的唯一更改是，我们必须给容器一个惟一的名称，而且我们必须将容器的端口8081映射到本地机器的端口8082，因为spark-worker1容器已经在使用您的本地机器的端口8081。</p><p id="92ef" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">7.旋转Spark提交节点</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="b822" class="ni lp it ok b gy oo op l oq or">docker run -it --name spark-submit --network spark-net -p 4040:4040 sdesilva26/spark_submit bash</span></pre><p id="7ebe" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">您现在应该在spark-submit容器中了。</p><p id="a4ce" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">8.打开一个scala shell并连接到Spark集群</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="b756" class="ni lp it ok b gy oo op l oq or">$SPARK_HOME/bin/spark-shell --conf spark.executor.memory=2G --conf spark.executor.cores=1 --master spark://spark-master:7077</span></pre><p id="dbc3" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">和前面一样，如果运行spark主节点的容器的名称不同于spark-master，那么可以用-master Spark://<your_master_container>:7077来更改上面的命令。</your_master_container></p><p id="ffab" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">上面的命令还要求在您的集群上，您希望每个执行器包含2G内存和1个内核。Spark主节点将分配这些执行器，前提是每个worker上有足够的资源来允许这样做。有关执行者和工作者的解释，请参见下面的<a class="ae nh" href="https://www.informit.com/articles/article.aspx?p=2928186" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><p id="5e23" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">9.在交互式scala shell中运行一个示例作业</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="d6c6" class="ni lp it ok b gy oo op l oq or">val myRange = spark.range(10000).toDF("number")</span><span id="28b7" class="ni lp it ok b gy pc op l oq or">val divisBy2 = myRange.where("number % 2 = 0")</span><span id="98c9" class="ni lp it ok b gy pc op l oq or">divisBy2.count()</span></pre><p id="a453" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">10.通过导航到<a class="ae nh" href="http://localhost:4040." rel="noopener ugc nofollow" target="_blank"> http://localhost:4040来检查应用程序UI。</a>您应该会看到以下内容</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/d2af7a6f8eadbcf10ad68aae08e0cab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2I_IKbAxHk6UVgtkctJJA.png"/></div></div></figure><p id="566f" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">您刚刚在Docker容器中运行了一个Spark作业。斯波克出生了！</p><h2 id="f7c1" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">体系结构</h2><p id="2dc1" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">我们在上面所做的是在Docker中创建一个网络，我们可以在其中部署容器，它们可以自由地相互通信。</p><p id="41eb" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">下图中的白色箭头表示容器之间的开放通信。容器上的端口显示为绿色，本地机器上的端口显示为黄色。</p><p id="dcc3" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">您可以看到所有容器都部署在桥接网络中。如果我们现在在这个网络之外部署一个容器，它将不能仅仅通过使用它们的容器名来解析其他容器的IP地址。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/2f4197d916f4f17d6a2edc5c85b287a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3GYwl4pdsTfEOdYqUIuM_w.png"/></div></div></figure></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="4539" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">Docker &amp; Spark —多台机器</h1><p id="64b4" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">现在让我们把所有东西包装在一起，形成一个完全分布式的Spark集群，在Docker容器中运行。</p><p id="a541" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><em class="os">注意:对于这一部分，你需要使用我已经创建的3个图像。</em></p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="ee74" class="ni lp it ok b gy oo op l oq or">docker pull sdesilva26/spark_master:0.0.2<br/>docker pull sdesilva26/spark_worker:0.0.2<br/>docker pull sdesilva26/spark_submit:0.0.2</span></pre><p id="3135" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">您也可以通过下载<a class="ae nh" href="https://github.com/sdesilva26/docker-spark/tree/master/docker" rel="noopener ugc nofollow" target="_blank">docker文件</a>自己构建它们</p><ol class=""><li id="5892" class="nt nu it mi b mj nc mm nd mp nv mt nw mx nx nb ny nz oa ob bi translated">在您选择的云提供商上启动一个实例，并使其成为docker swarm manager</li></ol><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="a974" class="ni lp it ok b gy oo op l oq or">docker swarm init</span></pre><p id="990d" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">2.将上述命令的输出复制并粘贴到至少两个其他实例中。我已经在另外4个实例上这样做了——3个将充当Spark workers，1个将作为我的Spark提交节点</p><p id="3f6f" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">3.在实例1上，像我们之前所做的那样创建一个覆盖网络</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="aa2d" class="ni lp it ok b gy oo op l oq or">docker network create -d overlay --attachable spark-net</span></pre><p id="f208" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">4.运行spark_master映像来创建一个将成为spark主节点的容器</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="f726" class="ni lp it ok b gy oo op l oq or">docker run -it --name spark-master --network spark-net -p 8080:8080 sdesilva26/spark_master:0.0.2</span></pre><p id="407a" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">5.通过将以下内容添加到安全组的入站规则中，打开端口8080–8090和4040</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="7204" class="ni lp it ok b gy oo op l oq or">Protocol    |  Port(s)   | Source<br/>Custom TCP  |  8080-8090 | 0.0.0/0<br/>Custom TCP  |    4040    | 0.0.0/0, ::/0</span></pre><p id="5acc" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><em class="os">注意:在AWS中，安全组是有状态的，所以从实例到用户的返回流量是自动允许的，所以不需要修改安全组的出站规则。在其他云提供商上，您可能需要在出站规则中添加类似的规则。</em></p><p id="c551" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">我的入站安全组规则现在看起来像这样</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/19bfc944314771999ab021df853eb36c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kq1IkXHuIQdmarH8oMkhXQ.png"/></div></div></figure><p id="0fcf" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">6.在http:// <public_ipv4_address_of_instance> :8080查看Spark主节点UI。您应该会看到与我们之前看到的相同的UI。</public_ipv4_address_of_instance></p><p id="1fa6" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">7.现在，在您的另一个实例上运行以下命令，将一个Spark worker节点连接到集群</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="e91e" class="ni lp it ok b gy oo op l oq or">docker run -it --name spark-worker1 --network spark-net -p 8081:8081 -e MEMORY=6G -e CORES=3 sdesilva26/spark_worker:0.0.2</span></pre><p id="71ba" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><em class="os">注意:根据一般经验，启动Spark worker节点时，内存=实例内存- 1GB，核心=实例核心-1。这就为实例的操作系统留出了1个内核和1GB的空间来执行后台任务。</em></p><p id="42c2" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">8.再次检查主节点的web UI，以确保worker被成功添加。</p><p id="367d" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">9.冲洗并重复第7步，添加尽可能多的火花工作者。确保将容器的名称从spark-worker1增加到spark-worker2，依此类推。</p><p id="d3cb" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">我已经连接了3个workers，我的主节点的web UI如下所示</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/ba09450b71d4421f02f3fb1d68369d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jf7E_Uf3J3v4hF-PscwUQA.png"/></div></div></figure><p id="2153" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">10.在另一个实例中，启动Spark提交节点</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="5bb5" class="ni lp it ok b gy oo op l oq or">docker run -it --name spark-submit --network spark-net -p 4040:4040 sdesilva26/spark_submit:0.0.2 bash</span></pre><p id="56ef" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">11.启动pyspark交互式shell并连接到集群</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="73b2" class="ni lp it ok b gy oo op l oq or">$SPARK_HOME/bin/pyspark --conf spark.executor.memory=5G --conf spark.executor.cores=3 --master spark://spark-master:7077</span></pre><p id="3c66" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><em class="os">注意:通过使用— conf标志，您可以指定在将应用程序连接到集群时希望每个执行器拥有的资源。Spark调优的主题本身就是一整篇文章，所以我不会在这里赘述。这两篇Cloudera博文我发现是理解资源分配的一个很好的资源:</em> <a class="ae nh" href="https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-1/" rel="noopener ugc nofollow" target="_blank"> <em class="os">第一部分</em> </a> <em class="os"> &amp; </em> <a class="ae nh" href="https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-2/" rel="noopener ugc nofollow" target="_blank"> <em class="os">第二部分</em> </a> <em class="os">。我还发现C2FO.io的Anthony Shipman的一篇</em> <a class="ae nh" href="https://c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/" rel="noopener ugc nofollow" target="_blank"> <em class="os">博客文章</em> </a> <em class="os">非常有用，其中还包括一个方便的excel表格，可以计算内存、内核和并行化的设置。</em></p><p id="786e" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">12.通过检查Spark主节点的UI和Spark提交节点的UI，检查提交节点是否成功连接到集群。它们应该看起来像下面的图片。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/99dd348fcc390fc19d59d2128ea972cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vH6i_g5H9TR5FULEaQ_R4A.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/a06cb8e90a6d10258cb354e52f8ea3c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sWXT2_SPscVKvrwZpLZJaA.png"/></div></div></figure><p id="b6e8" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">13.从pyspark shell运行一个示例作业</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="4b06" class="ni lp it ok b gy oo op l oq or">from random import random</span><span id="7546" class="ni lp it ok b gy pc op l oq or"><strong class="ok jd">def</strong> inside(p):<br/>    x, y = random(), random()<br/>    <strong class="ok jd">return</strong> x*x + y*y &lt; 1</span><span id="5521" class="ni lp it ok b gy pc op l oq or">NUM_SAMPLES = 100000</span><span id="98e2" class="ni lp it ok b gy pc op l oq or">count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()</span><span id="bac1" class="ni lp it ok b gy pc op l oq or">print("Pi is roughly {:0.4f}".format(4.0 * count / NUM_SAMPLES))</span></pre><p id="2a67" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">向spark集群提交作业更常见的方法是使用Spark安装中包含的spark-submit脚本。让我们也这样做。</p><p id="7869" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">14.退出pyspark并向集群上的执行器提交一个程序</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="0abc" class="ni lp it ok b gy oo op l oq or">$SPARK_HOME/bin/spark-submit --conf spark.executor.cores=3 --conf spark.executor.memory=5G --master spark://spark-master:7077 $SPARK_HOME/examples/src/main/python/pi.py 20</span></pre><p id="549c" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">除了我们调用spark-submit脚本，并传递给它一个. py文件和任何其他配置文件以供执行之外，它的语法与前面的几乎相同。</p><p id="b1f4" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">快乐的日子！我们现在已经创建了一个完全分布式的Spark集群，运行在Docker容器中，并向集群提交了一个应用程序。</p><h2 id="a3a0" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">体系结构</h2><p id="4c9d" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">我们刚刚创建的架构如下所示</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/8bc8743059d403e63f3679e465fd9c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WrdzQ0LdsuLTdZeCbS5rMA.png"/></div></div></figure><p id="be58" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">每个Spark worker节点和主节点都运行在Docker容器中，该容器位于它自己的计算实例上。spark驱动程序节点(Spark提交节点)也位于它自己的容器中，运行在一个单独的实例上。所有Docker守护进程都通过一个覆盖网络连接，在这种情况下，Spark主节点是Docker群管理器。</p><p id="2e79" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">在覆盖网络中，容器可以通过引用利用自动服务发现的容器名称来容易地解析彼此的地址。</p><p id="eef0" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">如果需要，更多的Spark worker节点可以在其他实例上启动。</p><p id="2559" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">欢迎来到终点线！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/c56b37b8fab642a172c37e92de5ee073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4olw5qrBU7WUv0Od"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Jonathan Chng 在<a class="ae nh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="c18c" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">结论</h1><p id="5be3" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">在本教程中，我们已经成功地逐步完成了在Docker容器中设置Spark集群的不同复杂程度。</p><p id="fe5a" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">我们首先从一些简单的Docker网络原则开始，在我们的本地机器上使用桥接网络，然后在分布式机器上使用覆盖网络和docker swarm。</p><p id="bf13" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">接下来，我们设置一个在本地机器上运行的Spark集群，以处理向集群添加工人的问题。</p><p id="e71f" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">然后，我们将Docker重新引入到这个组合中，并在我们的本地机器上设置一个在Docker容器内部运行的Spark集群。</p><p id="f1da" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">最后，我们将所有东西放在一起，构建了一个运行在Docker容器中的完全分布式的Spark集群。</p><p id="6222" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">希望您已经清楚地了解了这两种技术是如何联系在一起的，并且可以为您的特定问题或项目提供帮助。</p><p id="2b08" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">现在是时候开始试验了，看看你能从这个架构中学到什么。</p><p id="0c1e" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">尽情享受吧！</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pj oi l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">[ <a class="ae nh" href="https://giphy.com/gifs/goodbye-back-to-the-future-marty-mcfly-12xvz9NssSkaS4/links" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="1d0d" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">奖金</h1><p id="a059" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">手动创建集群的上述步骤信息量更大，因为它需要大量的手动输入和重复命令。</p><p id="0d2b" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">对于只需要4或5个计算实例的资源的小问题，这种工作量可能低于您的痛苦阈值。</p><p id="0f21" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">然而，随着数据变得越来越大，所需的计算能力开始增加，遵循上述步骤将使您成为一名全职的集群创建者。</p><p id="2ba1" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">一种更加实用和优雅的建立集群的方式是利用<strong class="mi jd"> Docker compose </strong></p><h2 id="1209" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">Docker撰写</h2><p id="a443" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">对于那些刚刚接触Docker compose的人来说，它允许你启动所谓的“<strong class="mi jd">服务</strong>”。</p><p id="82b8" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">一个服务由一个Docker映像组成，但是您可能希望运行这个映像的多个容器。例如，从docker映像sdesilva26/spark_worker:0.0.2运行多个Spark worker容器将构成一个服务。</p><p id="d21c" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">要启动一组服务，您需要创建一个docker-compose.yml文件，该文件指定了您想要运行的各种服务的所有信息。</p><p id="2945" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">然而，Docker compose用于创建在单个主机上运行的服务。它不支持跨主机部署容器。</p><p id="d099" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">进入<strong class="mi jd"> Docker栈。</strong></p><p id="cd17" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">Docker栈是Docker compose思想的简单扩展。现在，您可以在连接成Docker群的多台主机上运行服务，而不是在一台主机上运行服务。</p><p id="d833" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">最重要的是，如果你有一个Docker合成文件，只需要做很少的修改就可以使用Docker堆栈命令。</p><p id="786c" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">让我们看看如何使用一个组合文件和docker堆栈来创建在Docker容器中运行的分布式Spark集群。</p><ol class=""><li id="16a9" class="nt nu it mi b mj nc mm nd mp nv mt nw mx nx nb ny nz oa ob bi translated">第一步是标记Docker群中的节点。从Docker群管理器中列出群中的节点。</li></ol><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="de6e" class="ni lp it ok b gy oo op l oq or">docker node ls</span></pre><p id="1c34" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">您应该会得到类似下图的输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/0755eb932bbcd8db631f550f59dc53a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xb96lskzfvUYnua4-K8mZw.png"/></div></div></figure><p id="e3fe" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">2.对于任何你希望成为Spark工作者的实例，给它们加上一个标签</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="dac1" class="ni lp it ok b gy oo op l oq or">docker node update --label-add role=worker x5kmfd8akvvtnsfvmxybcjb8w</span></pre><p id="834f" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">3.现在，用master角色标记您希望运行Spark主节点的实例</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="57fe" class="ni lp it ok b gy oo op l oq or">docker node update --label-add role=master </span></pre><p id="3ebb" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">4.创建一个docker-compose.yml文件或者拉一个我已经<a class="ae nh" href="https://github.com/sdesilva26/docker-spark/blob/master/docker/docker-compose.yml" rel="noopener ugc nofollow" target="_blank">创建的</a>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pl oi l"/></div></figure><p id="382c" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">在这个合成文件中，我定义了两个服务——spark-master和spark-worker。</p><p id="2325" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">第一个服务将单个容器部署到群中标签为“role=master”的任何节点上。</p><p id="c267" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">第二个服务将把sdesilva26/spark_worker:0.0.2映像的3个容器部署到标签为“role=worker”的节点上。如果群中没有找到3个合适的节点，它将尽可能多地部署。</p><p id="7050" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">最后，所有这些容器都将被部署到为我们创建的名为spark-net的覆盖网络中。</p><p id="9b84" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">5.将docker-compose.yml复制到swarm manager实例中。</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="9ba4" class="ni lp it ok b gy oo op l oq or">scp -i &lt;YOUR_KEY&gt;.pem /path/to/docker-compose.yml ec2-user@&lt;PUBLIC_IP_ADDRESS_OF_INSTANCE&gt;:/home/ec2-user/docker-compose.yml</span></pre><p id="5464" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">[参见<a class="ae nh" href="https://github.com/juanfrans/notes/wiki/Copying-Files-Between-Local-Computer-and-Instance-(AWS)" rel="noopener ugc nofollow" target="_blank">此处</a>了解替代方法]</p><p id="106c" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">6.最后，从swarm manager运行docker栈，并给它一个名字。</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="bd25" class="ni lp it ok b gy oo op l oq or">docker stack deploy --compose-file docker-compose.yml sparkdemo</span></pre><p id="1aa9" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><em class="os">注意:你的栈的名字将被加在所有服务名的前面。于是服务“spark-master”就变成了“sparkdemo_spark-master”。您可以使用</em>查看正在运行的服务</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="4dc1" class="ni lp it ok b gy oo op l oq or">docker service ls</span></pre><p id="6db4" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">7.检查spark_worker映像是否在标记为“worker”的实例上运行，以及spark_master映像是否在标记为“master”的节点上运行。</p><p id="9513" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">恭喜你，我们已经将这篇文章中的所有工作简化为Docker swarm manager中的几个命令。</p><p id="762e" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">Docker服务最大的好处是它非常容易扩展。例如，如果后来我们向Docker swarm添加了另一个实例，然后我们希望扩展“sparkdemo_spark-worker”服务，我们可以简单地运行</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="3440" class="ni lp it ok b gy oo op l oq or">docker service scale sparkdemo_spark-worker=4</span></pre><p id="0e37" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">现在，您的集群中有4个Spark workers！</p><p id="21c2" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">码头工人。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/63f3c59242c77c8ff39c91ad5a19c6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*--drMfpJI-VDg0fs"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae nh" href="https://unsplash.com/@preciousjfm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">J E W E L M I T CH E L L L</a>在<a class="ae nh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure></div></div>    
</body>
</html>