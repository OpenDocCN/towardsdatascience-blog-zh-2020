<html>
<head>
<title>Short technical information about Word2Vec, GloVe and Fasttext</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于Word2Vec、GloVe和Fasttext的简短技术信息</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/short-technical-information-about-word2vec-glove-and-fasttext-d38e4f529ca8?source=collection_archive---------19-----------------------#2020-05-25">https://towardsdatascience.com/short-technical-information-about-word2vec-glove-and-fasttext-d38e4f529ca8?source=collection_archive---------19-----------------------#2020-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="c1ed" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">介绍</h1><p id="4501" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在深度学习的帮助下，自然语言处理(NLP)发展迅速。越来越多的公司愿意更快和大量地处理文本，这就是为什么NLP是人工智能研究中的动态领域之一。然而，这些研究主要致力于英语:到目前为止，大多数公司通过翻译处理法语，或者通过多语言算法将法语作为其他语言之一。这一点更加重要，因为深度学习可以导致更准确但更难解释的结果。</p><p id="bd4d" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">通过这篇文章，我试图给出三个最著名的嵌入之间的主要区别。</p><h1 id="bd1a" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">嵌入</h1><p id="4ba1" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">单词嵌入使得单词可以被机器“理解”。它的主要目标是捕捉单词之间的一种关系。例如，这种关系可以是形态的、语义的、上下文的或句法的。将单词转换成向量的一种快速方法是将所有单词转换成整数，然后将这些整数作为它们的一键编码的索引。</p><p id="db7d" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">让我们考虑三句话的一个热门编码:“<em class="lr">狗和猫玩</em>”、“<em class="lr">狗吃肉</em>”、“<em class="lr">狗和猫吃肉</em>”。在这里，我们可以给这些句子中出现的所有单词赋予整数，按照出现的顺序，一个热编码的长度就是单词集的长度。我们有6个唯一的字，独热编码的向量长度是6。</p><p id="b659" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">狗(1，0，0，0，0，0)，</p><p id="fa19" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">和(0，1，0，0，0，0)，</p><p id="da6a" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">cat (0，0，1，0，0，0)，</p><p id="9291" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">播放(0，0，0，1，0，0)，</p><p id="ed3e" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">吃(0，0，0，0，1，0)，</p><p id="defb" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">肉类(0，0，0，0，0，1)</p><p id="e490" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">这种类型的单词表示是有用的，但是效率不高，因为我们不使用单词相似度。对于阅读器来说，狗和猫是完全不同的，因为它们的向量是正交的。<em class="lr">狗</em>在这里是为了<em class="lr">猫</em>，什么<em class="lr">狗</em>是为了<em class="lr">和</em>或者<em class="lr">吃</em>。然而，我们很容易发现"<em class="lr">" dog "</em>这个词与" cat "更相似，而不是与" and "或" eat "更相似。此外，独热编码导致维数为|V|的非常稀疏的向量，即词汇集的大小(|V | &gt; 10^6)。与前面的例子一样，句子的表示如下:</p><p id="d492" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">“狗和猫的游戏”:(1，1，1，1，0，0)</p><p id="4bbc" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">“狗吃肉”:(1，0，0，0，1，1)</p><p id="a1d7" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">“狗和猫吃”:(1，1，1，0，1，0)</p><p id="9404" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">单词嵌入使得能够在低维向量中存储上下文信息。出现在相似上下文中的单词往往有相似的意思。维度降低(demb &lt; 1000)。向量是由模型通过无监督学习来学习的，通常是在大量数据上，如维基百科、谷歌新闻或数字图书，甚至是普通爬行(网络上的大量文本样本)。相似性是可计算，两个公式是最常用的:余弦相似性和欧几里德距离。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/0a46ca72048c4e9bb608b4880f225713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*WNiG62Pqxp9inPfrbCNVdQ.png"/></div></figure><p id="512c" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">余弦相似度的范围从1(相反)到1(共线且意义相同)。相似度高的两个向量通过一个模型容易导致相同的结果。借助GloVe、FastText和Word2Vec等算法，我们可以获得单词嵌入，帮助模型更好地理解我们的语言，并比使用一次性编码单词的模型给出更准确的结果。然而，这些模型有一个真正的瓶颈，因为它们只考虑了每个单词的一个意思。法语中的“piquer”有很多含义，但是对于这些模型只有一个向量。自2018年以来，新的模型已经出现，如著名的埃尔莫，伯特和XLM。这些模型能够考虑单词的上下文来创建嵌入。不同上下文中的同一个单词不会有相同的相关向量。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ma"><img src="../Images/8ee59da537472d8b8d7214638c99dcb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rdmUrn-Qvmfpdx0nCCMNiQ.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">单词嵌入的投影。在这里，巴黎之于法国，犹如柏林之于德国。这叫做“类比”。(【https://arxiv.org/abs/1310.4546】T2</p></figure><p id="6a31" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">作为这第一部分的结论，这里是带回家的:</p><p id="8589" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">对词汇表进行一次性编码可能有效，但会导致稀疏矩阵(昂贵的训练)和正交向量(所有单词都是等价的，没有语义捕获)</p><p id="b0ac" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">单词嵌入旨在捕捉低维向量上的语义和语法</p><p id="4795" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">单词之间的相似度通常通过余弦相似度来衡量</p><h1 id="b664" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">Word2Vec之前</h1><p id="6d9d" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在Word2Vec之前，单词是通过统计进行编码的。SVD(奇异值分解)是对共生矩阵的降维。SVD然后被用于处理LSI(潜在语义索引)。LSI允许术语间相似性、文档间相似性和术语间相似性。</p><p id="cc60" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">但由于是基于矩阵分解，所以字典变化时矩阵的维数也变化，我们加一个词就要重新计算整个分解。它对词频不平衡也非常敏感，因此我们经常需要对文档进行预处理，以删除停用词并使所有其他词正常化(通过词条化或词干化)。</p><p id="dbee" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">最后，它非常昂贵，不适合大型词典或语料库。即使它是2013年NLP的里程碑，Word2Vec也不是第一个神经语言模型。其中一些自2000年就存在了，正如Bengio等人概述的那样。一开始，语言模型的目标是在给定一些前面的单词的情况下，预测“下一个”单词。</p><p id="41e5" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">Bengio等人[4]首先提出了遵循前馈架构的模型，Mikolov等人[27]提出了递归神经网络模型来完成这项任务。前馈神经网络语言模型(NNLM)和递归神经网络语言模型(RNNLM)都具有过高的训练复杂度。实际上，Word2Vec模型的简单性是主要的创新。</p><h1 id="cf4b" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">Word2Vec</h1><p id="61a7" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Mikolov等人[2] [3]在2013年带来了一个高效的单词嵌入实现的创新:Word2Vec。他们首先证明了更浅和更有效的模型2允许在更大量的数据上进行训练(速度提高了1000！).这种效率是通过移除先前架构中存在的隐藏层来实现的。提出了Word2Vec的两个版本:连续词包(CBOW)和SkipGram (SG)。第一种方法基于周围的单词窗口来预测中心单词，第二种方法正好相反:它基于中心单词来预测上下文。这些架构只包含一个隐藏层，这导致了更有效的训练。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi mk"><img src="../Images/71da925a0b8907806f5d1b91d86920d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*igZrMFGYavGCaiZvi8e7BA.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">Word2Vec最著名的人物之一，Mikolov(<a class="ae mj" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1301.3781</a>–2013)的Skip-Gram和CBOW模型架构</p></figure><p id="d9ed" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">这是连续单词袋的目标函数:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/a54d9477980ad3a74223784eeefdd5a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*FD9jAIHuCMVY_U_w8yIiyQ.png"/></div></figure><p id="3ac6" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">这里，跳跃图的目标函数是:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/b4387d00250a0402f7b71615deda233c.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*jl_GN98kkFiK2TH1yE0FOA.png"/></div></figure><p id="47cb" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">随着这些体系结构的变化，一些技术被开发出来，如频繁词的二次抽样和负抽样，以减少模型中要改变的权重的数量。</p><p id="852c" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">论文展示了该模型在相似性任务方面的改进。他们还表明，他们获得了比以前发现的更好的结果，在类比任务上使用神经网络语言模型。许多论文讨论了这些模型如何捕捉线性关系[4] [7]。然而，根据经验，Word2Vec的嵌入改进了NLP中的大多数任务，这足以开创一种新趋势。Mikolov等人(2013)的Word2Vec论文被引用超过28000次，这表明了研究人员的浓厚兴趣。然而，Word2Vec并不完美，期望和现实之间的差距相当大。</p><p id="69fd" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">实际上，Word2Vec是一个基于窗口的模型，因此它不能从整个文档中的信息中受益，也不能捕获子词信息，这可能是有趣的，因为从名词或动词派生的形容词拥有共同的信息。此外，Word2Vec不能处理词汇表以外的单词(OOV)，因为在训练期间没有看到的单词不能被矢量化。模型的输出是一个格式为“/”的文件。最后，Word2Vec没有解决的另一个问题是解歧。一个词可以有多个意思，这取决于上下文。前三个问题由GloVe和FastText解决，而最后一个问题由Elmo解决。</p><h1 id="1802" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">处理子字信息的快速文本</h1><p id="e559" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">快速文本(Bojanowski等人[1])是由脸书开发的。它是一种学习单词表示的方法，依赖于Word2Vec的skipgram模型，并提高其效率和性能，如以下几点所述:</p><p id="1bf9" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">1.训练起来更快更简单。在相似性评估上，FastText在较小的训练集上给出了比Word2Vec更好的结果。</p><p id="fb2d" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">2.它受益于子字信息。如果给出一个单词，比如“part out ”, fast text会使用这个单词的所有n元语法来计算这个单词的分数。例如，对于单词“lapin ”,三个字母的单词是“”</p><p id="f5e9" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">3.由于n元语法，它可以从词汇表之外的单词中生成嵌入。OOV单词向量可以用其n元文法的平均向量表示来构建。</p><p id="0034" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">实际上，使用SkipGram，给出单词t的上下文的概率通过得分函数s由单词向量来参数化:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/f684f2560b6ef87d552b91a2b996a740.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*9Cz0PCp-Lce76MRwtGsOMQ.png"/></div></figure><p id="89ae" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">其中u和v分别取自输入和输出嵌入矩阵。使用FastText，评分功能变为:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/55400c142fabcf31073b8b68fd5ddfdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*QhkAbmAZfdCM8tFN8BN4uQ.png"/></div></figure><p id="9fb1" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">其中G_wt是单词w_t中n元语法的集合，z_g是第G个n元语法的向量。v_wc是上下文单词wc的向量。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/6bf70e688ea2df48fc9c423cad362f84.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*XpNeK7vroOblpSA811rgcw.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">词汇外单词中字符n-grams之间相似性的图解。(<a class="ae mj" href="https://arxiv.org/abs/1607.04606" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1607.04606</a></p></figure><p id="9dfc" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">x轴上的单词是OOV。红色表示正余弦，蓝色表示负余弦。</p><p id="b130" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">即使自然语言处理的字符级特征已经是一个研究领域，这篇文章也是第一篇考虑这些特征来构建语言模型的文章。同样，该模型基于预测，而不是基于语料库的统计。此外，它不会根据上下文为每个单词生成多个向量。</p><h1 id="d095" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">处理语料库统计的手套</h1><p id="5b08" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">GloVe (Pennington等人[5])由斯坦福开发，代表单词表示的全局向量。它的目的是调和单词预测模型和整个语料库的单词统计数据。它们提供了一种模型，该模型考虑了语料库的共现统计以及基于预测的方法的效率。</p><p id="1d60" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">给定单词i = ice和j = steam，我们想要研究与某个探测单词k = solid的共现概率的比率。我们可以预期单词I和单词k (Pik)之间的共现在Pjk上更大。那里，比率Pik Pjk应该很大。</p><p id="b17e" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">我们用下面的公式来描述这个比率:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/d0fcfdf9a9542550454183a8864869d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*AcKvGnmM-cDzAedprZ6xQg.png"/></div></figure><p id="8c5b" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">如论文所示，可以简化为:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/d10d7f9292736f42e9d4fd2f1d7043b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*xQTMu8gvndlpc8tZ4Y2CCA.png"/></div></figure><p id="4fef" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">哪个决议与LSA和老方法密切相关。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d71203b2b3f2cbd48bc61e87c8117d2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRlFCi7jKegCkVnt4RUDMQ.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">冰和蒸汽在固体、气体、水和时尚中的共现概率和比率。由于冰与固体的关系比蒸汽更密切，所以同现率很高。考虑到探测字steam，这是相反的。它们都同样与水(高度)和时尚(模糊)相关，所以共现率大约等于1。(【https://www.aclweb.org/anthology/D14-1162/】T2</p></figure><p id="1814" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">其目标是优化以下函数，这是一个最小二乘问题:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/2da51b097db20957b42176ba112147d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*6Hp_CBDa0NFjQXip1CusRQ.png"/></div></figure><p id="9f24" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">随着</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ef79d60441a737144de9817a107f3d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*IXdx8t-pFfit530udL4esA.png"/></div></figure><p id="3f6d" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">其中x是共生矩阵，f是加权函数，b是偏差项，wi是单词向量，w是上下文向量。</p><p id="7580" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">即使GloVe在相似性和评估任务上显示出比Word2Vec更好的结果，但根据作者的经验，它还没有被证明，使用其中一个或另一个可以导致更好的结果:两者都值得尝试。</p><h1 id="c663" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">结论</h1><p id="6312" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最后，所有这些模型仍然有用，甚至是前者。只是看你的用途和需求。Word2Vec在基本模型上仍然非常相关，可以通过取句子中单词向量的平均值(甚至通过它们的tf-idf [6]分数加权)来用于嵌入句子或文档。找出答案的一个好方法是尝试所有方法，并保留模型在最终任务中获得最佳分数的方法。</p><h1 id="4378" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">文献学</h1><p id="67af" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">[1] Piotr Bojanowski、Edouard Grave、Armand Joulin和Tomas Mikolov。用子词信息丰富词向量。计算语言学协会汇刊，5:135–146，2016。</p><p id="68ec" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">[2]托马斯·米科洛夫、G.s .科拉多、程凯和杰弗里·迪恩。向量空间中单词表示的有效估计。第1–12页，2013年1月。</p><p id="f3d2" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">[3]托马斯·米科洛夫、伊利亚·苏茨基弗、程凯、格雷戈·科拉多和杰弗里·迪安。词和短语的分布式表示及其组合性。更正，澳大利亚统计局/1310.4546，2013。</p><p id="7c55" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">[4]大卫·明诺和劳雷·汤普森。负抽样跳跃图的奇异几何。《2017年自然语言处理经验方法会议论文集》，2873-2878页，丹麦哥本哈根，2017年9月。计算语言学协会。</p><p id="688d" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">[5]杰弗里·潘宁顿、理查德·索切尔和克里斯托弗·曼宁。Glove:单词表示的全局向量。《自然语言处理中的经验方法》(EMNLP)，第1532–1543页，2014年。</p><p id="d6bb" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">[6]斯蒂芬·罗伯逊。理解逆文献频率:关于idf的理论争论。文献期刊，60:503–520，2004年。</p><p id="154f" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">[7] Laura Wendlandt、Jonathan K. Kummerfeld和Rada Mihalcea。影响单词嵌入惊人不稳定性的因素。CoRR，abs/1804.09692，2018。</p></div></div>    
</body>
</html>