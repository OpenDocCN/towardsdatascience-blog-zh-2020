# code2vecã€GloVe å’Œ spaCy çš„å•è¯åµŒå…¥ã€‚

> åŸæ–‡ï¼š<https://towardsdatascience.com/word-embeddings-with-code2vec-glove-and-spacy-5b26420bf632?source=collection_archive---------21----------------------->

## å¦‚ä½•æ ¹æ®æ‚¨çš„ç”¨ä¾‹é€‰æ‹©å•è¯åµŒå…¥ç®—æ³•ï¼Ÿ

![](img/e72028ca5a3ed5748e5db407463d1575.png)

æ¥æº:[å¢å¡æ–¯æ¯”è€¶é‡Œ](https://pixabay.com/users/lukasbieri-4664461/)ç»ç”±[çš®å…‹æ–¯å·´ä¼Š](https://pixabay.com/photos/laptop-macbook-home-office-switched-2838917/) (CC0)

æ”¹å–„ä½ çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¸€ä¸ªæœ‰æ•ˆæ–¹æ³•æ˜¯ä½¿ç”¨[å•è¯åµŒå…¥](https://en.wikipedia.org/wiki/Word_embedding)ã€‚ä½¿ç”¨å•è¯åµŒå…¥ï¼Œæ‚¨å¯ä»¥æ•è·æ–‡æ¡£ä¸­å•è¯çš„ä¸Šä¸‹æ–‡ï¼Œç„¶åæ‰¾åˆ°è¯­ä¹‰å’Œå¥æ³•çš„ç›¸ä¼¼æ€§ã€‚

åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå•è¯åµŒå…¥æŠ€æœ¯çš„ä¸€ä¸ªä¸å¯»å¸¸çš„åº”ç”¨ã€‚æˆ‘ä»¬å°†åŠªåŠ›ä¸º OpenAPI è§„èŒƒæ‰¾åˆ°æœ€å¥½çš„å•è¯åµŒå…¥æŠ€æœ¯ã€‚ä½œä¸º OpenAPI è§„èŒƒçš„ä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª [apis-guru](https://apis.guru/) çš„ OpenAPI è§„èŒƒçš„å…è´¹èµ„æºğŸ˜ã€‚

æœ€å¤§çš„æŒ‘æˆ˜æ˜¯ OpenAPI è§„èŒƒæ—¢ä¸æ˜¯è‡ªç„¶è¯­è¨€ï¼Œä¹Ÿä¸æ˜¯ä»£ç ã€‚ä½†è¿™ä¹Ÿæ„å‘³ç€æˆ‘ä»¬å¯ä»¥è‡ªç”±ä½¿ç”¨ä»»ä½•å¯ç”¨çš„åµŒå…¥æ¨¡å‹ã€‚åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ä¸‰ä¸ªå¯èƒ½çš„å€™é€‰è€…:code2vecã€GloVe å’Œ spaCyã€‚

[code2vec](https://urialon.cswp.cs.technion.ac.il/wp-content/uploads/sites/83/2018/12/code2vec-popl19.pdf) æ˜¯ä¸€ä¸ªå­¦ä¹ ä¸æºä»£ç ç›¸å…³çš„ç±»æ¯”çš„ç¥ç»æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ˜¯åœ¨ Java ä»£ç æ•°æ®åº“ä¸Šè®­ç»ƒçš„ï¼Œä½†æ˜¯æ‚¨å¯ä»¥å°†å…¶åº”ç”¨äºä»»ä½•ä»£ç åº“ã€‚

ç„¶åæ˜¯[æ‰‹å¥—](https://nlp.stanford.edu/projects/glove/)ã€‚GloVe æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†(NLP)çš„ä¸€ç§å¸¸ç”¨ç®—æ³•ã€‚å®ƒåœ¨ç»´åŸºç™¾ç§‘å’Œ Gigawords ä¸Šæ¥å—äº†è®­ç»ƒã€‚

æœ€åï¼Œæˆ‘ä»¬æœ‰[ç©ºé—´](https://spacy.io/usage/vectors-similarity)ã€‚è™½ç„¶ spaCy æ˜¯æœ€è¿‘æ‰å¼€å‘çš„ï¼Œä½†è¯¥ç®—æ³•å·²ç»ä»¥ä¸–ç•Œä¸Šæœ€å¿«çš„å•è¯åµŒå…¥è€Œé—»åã€‚

è®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›ç®—æ³•ä¸­å“ªä¸€ä¸ªæ›´é€‚åˆ OpenAPI æ•°æ®é›†ï¼Œå“ªä¸€ä¸ªæ›´é€‚åˆ OpenAPI è§„èŒƒğŸ‘€ã€‚æˆ‘æŠŠè¿™ç¯‡æ–‡ç« åˆ†æˆå…­ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½åŒ…å«ä»£ç ç¤ºä¾‹å’Œä¸€äº›å°†æ¥ä½¿ç”¨çš„æŠ€å·§ï¼Œè¿˜æœ‰ä¸€ä¸ªç»“è®ºã€‚

1.  ä¸‹è½½æ•°æ®é›†
2.  ä¸‹è½½è¯æ±‡
3.  æå–å­—æ®µåç§°
4.  ä»¤ç‰ŒåŒ–å¯†é’¥
5.  åˆ›å»ºå­—æ®µåç§°çš„æ•°æ®é›†
6.  æµ‹è¯•åµŒå…¥
7.  ç»“è®º

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹äº†ã€‚

# **1ã€‚ä¸‹è½½æ•°æ®é›†âœ…**

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½æ•´ä¸ª[API-guru](https://apis.guru/)æ•°æ®åº“ã€‚

æ‚¨ä¼šæ³¨æ„åˆ°å¤§å¤šæ•° API-guru è§„èŒƒéƒ½æ˜¯ Swagger 2.0 æ ¼å¼çš„ã€‚ä½†æ˜¯â€¦â€¦open API è§„èŒƒçš„æœ€æ–°ç‰ˆæœ¬æ˜¯ [OpenAPI 3.0](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.3.md) ã€‚æ‰€ä»¥è®©æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ Unmock è„šæœ¬å°†æ•´ä¸ªæ•°æ®é›†è½¬æ¢æˆè¿™ç§æ ¼å¼ï¼æ‚¨å¯ä»¥æŒ‰ç…§ [unmock-openapi-scripts è‡ªè¿°æ–‡ä»¶](https://github.com/meeshkan/unmock-openapi-scripts/blob/master/README.md)ä¸­çš„è¯´æ˜å®Œæˆæ­¤æ“ä½œã€‚

è¿™å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´(ä½ ä¸ä¼šå˜æˆğŸ§“ï¼Œä½†æˆ‘ä»¬è¯´çš„æ˜¯å‡ ä¸ªå°æ—¶â°)æœ€ç»ˆï¼Œæ‚¨å°†è·å¾—ä¸€ä¸ªå…·æœ‰å„ç§è§„æ ¼çš„å¤§å‹æ•°æ®é›†ğŸ“ã€‚

# **2ã€‚ä¸‹è½½è¯æ±‡âœ…**

**ä»£ç  2vec**

1.  ä» [code2vec GitHub é¡µé¢](https://github.com/tech-srl/code2vec)ä¸‹è½½æ¨¡å‹ã€‚æŒ‰ç…§ README.md çš„å¿«é€Ÿå…¥é—¨éƒ¨åˆ†ä¸­çš„è¯´æ˜è¿›è¡Œæ“ä½œï¼Œç„¶åå¯¼å‡ºè®­ç»ƒå¥½çš„ä»¤ç‰Œã€‚
2.  ä½¿ç”¨ [gensim](https://pypi.org/project/gensim/) åº“åŠ è½½ã€‚

```
model = word2vec.load_word2vec_format(vectors_text_path, binary=False)model = word2vec.load_word2vec_format(vectors_text_path, binary=False)
```

**æ‰‹å¥—**

1.  ä»ç½‘ç«™ä¸Šä¸‹è½½ä¸€ä¸ª[æ‰‹å¥—](https://nlp.stanford.edu/projects/glove/)è¯æ±‡ã€‚æˆ‘ä»¬é€‰æ‹©äº†æœ€å¤§çš„ä¸€ä¸ªï¼Œå› ä¸ºå®ƒæ‰¾åˆ°æˆ‘ä»¬æ‰€æœ‰å•è¯çš„å‡ ç‡æ›´é«˜ã€‚æ‚¨å¯ä»¥é€‰æ‹©æƒ³è¦ä¸‹è½½å®ƒçš„ä½ç½®ï¼Œä½†æ˜¯ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæœ€å¥½å°†å®ƒå­˜å‚¨åœ¨å·¥ä½œç›®å½•ä¸­ã€‚
2.  æ‰‹åŠ¨åŠ è½½æ‰‹å¥—è¯æ±‡ã€‚

```
embeddings_dict = {}
with open("../glove/glove.6B.300d.txt", 'r', encoding="utf-8") as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector = np.asarray(values[1:], "float32")
        embeddings_dict[word] = vector
```

**ç©ºé—´**

åŠ è½½å¤§ç©ºé—´è¯æ±‡:

```
nlp = spacy.load(â€˜en_core_web_lgâ€™).
```

# **3ã€‚æå–å­—æ®µåç§°âœ…**

OpenAPI è§„èŒƒåç§°çš„å®Œæ•´åˆ—è¡¨å¯ä»¥ä»`scripts/fetch-list.sh`æ–‡ä»¶ä¸­è·å¾—ï¼Œæˆ–è€…é€šè¿‡ä½¿ç”¨ä»¥ä¸‹å‡½æ•°(å¯¹äº Windows)è·å¾—:

```
def getListOfFiles(dirName):
    listOfFile = os.listdir(dirName)
    allFiles = list()
    for entry in listOfFile:
        fullPath = posixpath.join(dirName, entry)
        if posixpath.isdir(fullPath):
            allFiles = allFiles + getListOfFiles(fullPath)
        else:
            allFiles.append(fullPath)

    return allFiles
```

å¦ä¸€ä»¶å¤§äº‹æ˜¯ä»æˆ‘ä»¬çš„ OpenAPI è§„èŒƒä¸­è·å–å­—æ®µåã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [openapi ç±»å‹åº“](https://pypi.org/project/openapi-typed-2/)ã€‚è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ª`get_fields`å‡½æ•°ï¼Œå®ƒé‡‡ç”¨ OpenAPI è§„èŒƒå¹¶è¿”å›ä¸€ä¸ªå­—æ®µååˆ—è¡¨:

```
def get_fields_from_schema(o: Schema) -> Sequence[str]:
    return [
        *(o['properties'].keys() if ('properties' in o) and (type(o['properties']) == type({})) else []),
        *(sum([
            get_fields_from_schema(schema) for schema in o['properties'].values() if not ('$ref' in schema) and type(schema) == type({})], []) if ('properties' in o) and ($        *(get_fields_from_schema(o['additionalProperties']) if ('additionalProperties' in o) and (type(o['additionalProperties']) == type({})) else []),
        *(get_fields_from_schema(o['items']) if ('items' in o) and  (type(o['items'] == type({}))) else []),
    ]

def get_fields_from_schemas(o: Mapping[str, Union[Schema, Reference]]) -> Sequence[str]:
    return sum([get_fields_from_schema(cast(Schema, maybe_schema)) for maybe_schema in o.values() if not ('$ref' in maybe_schema) and (type(maybe_schema) == type({}))], [])

def get_fields_from_components(o: Components) -> Sequence[str]:
    return [
        *(get_fields_from_schemas(o['schemas']) if 'schemas' in o else []),
            ]                                                                                                                                                                       

def get_fields(o: OpenAPIObject) -> Sequence[str]:
    return [
        *(get_fields_from_components(o['components']) if 'components' in o else []),
    ]
```

æ­å–œä½ ã€‚ç°åœ¨æˆ‘ä»¬çš„æ•°æ®é›†å‡†å¤‡å¥½äº†ã€‚

# **4ã€‚ä»¤ç‰ŒåŒ–å¯†é’¥âœ…**

å­—æ®µåå¯èƒ½åŒ…å«æ ‡ç‚¹ç¬¦å·ï¼Œå¦‚`_`å’Œ`-`ç¬¦å·ï¼Œæˆ–éª†é©¼å¤§å°å†™å•è¯ã€‚æˆ‘ä»¬å¯ä»¥æŠŠè¿™äº›å•è¯åˆ†å‰²æˆç§°ä¸ºè®°å·çš„å°å—ã€‚

ä¸‹é¢çš„`camel-case`å‡½æ•°è¯†åˆ«è¿™äº› camel case å•è¯ã€‚é¦–å…ˆï¼Œå®ƒæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ ‡ç‚¹ç¬¦å·ã€‚å¦‚æœæ˜¯ï¼Œé‚£å°±ä¸æ˜¯éª†é©¼æ¡ˆã€‚ç„¶åï¼Œå®ƒæ£€æŸ¥å•è¯ä¸­æ˜¯å¦æœ‰å¤§å†™å­—æ¯(ä¸åŒ…æ‹¬ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªå­—ç¬¦)ã€‚

```
def camel_case(example):      
    if  any(x in example for x  in string.punctuation)==True:
        return False
    else:
        if any(list(map(str.isupper, example[1:-1])))==True:
            return True
        else:
            return False
```

ä¸‹ä¸€ä¸ªå‡½æ•°`camel_case_split`å°† camel case å•è¯æ‹†åˆ†æˆå¤šä¸ªç‰‡æ®µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åº”è¯¥è¯†åˆ«å¤§å†™å­—æ¯ï¼Œå¹¶æ ‡è®°å‡ºå¤§å°å†™å˜åŒ–çš„åœ°æ–¹ã€‚è¯¥å‡½æ•°è¿”å›æ‹†åˆ†åçš„å•è¯åˆ—è¡¨ã€‚ä¾‹å¦‚ï¼Œå­—æ®µå`BodyAsJson`è½¬æ¢æˆåˆ—è¡¨`[â€˜Bodyâ€™, â€˜Asâ€™,'Json']`ã€‚

```
def camel_case_split(word):
    idx = list(map(str.isupper, word))
    case_change = [0]
    for (i, (x, y)) in enumerate(zip(idx, idx[1:])):
        if x and not y:  
            case_change.append(i)
        elif not x and y:  
            case_change.append(i+1)
    case_change.append(len(word))
    return [word[x:y] for x, y in zip(case_change, case_change[1:]) if x < y]
```

è¿™ä¸ª`camel_case_split`å‡½æ•°ç„¶åè¢«ç”¨åœ¨ä¸‹é¢çš„è®°å·åŒ–ç®—æ³•ä¸­ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬é¦–å…ˆæ£€æŸ¥å•è¯ä¸­æ˜¯å¦æœ‰æ ‡ç‚¹ç¬¦å·ã€‚ç„¶åï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªè¯åˆ†æˆå‡ éƒ¨åˆ†ã€‚æœ‰å¯èƒ½è¿™äº›ç‰‡æ®µæ˜¯éª†é©¼å¤§å°å†™å•è¯ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå®ƒåˆ†æˆæ›´å°çš„å—ã€‚æœ€åï¼Œåœ¨æ‹†åˆ†æ¯ä¸ªå…ƒç´ åï¼Œæ•´ä¸ªåˆ—è¡¨è¢«è½¬æ¢ä¸ºå°å†™ã€‚

```
def tokenizer(mylist):
    tokenized_list=[]
    for word in mylist:

        if '_'  in word:
            splitted_word=word.split('_')
            for elem in splitted_word:
                if camel_case(elem):
                    elem=camel_case_split(elem)
                    for el1 in elem:
                        tokenized_list.append(el1.lower())
                else:    
                    tokenized_list.append(elem.lower())
        elif '-' in word:
            hyp_word=word.split('-')
            for i in hyp_word:
                if camel_case(i):
                    i=camel_case_split(i)
                    for el2 in i:
                        tokenized_list.append(el2.lower())
                else: 
                    tokenized_list.append(i.lower())
        elif camel_case(word):
            word=camel_case_split(word)
            for el in word:
                tokenized_list.append(el.lower())
        else:
            tokenized_list.append(word.lower())
    return(tokenized_list)
tokenizer(my_word)
```

# **5ã€‚åˆ›å»ºå­—æ®µåä¸ºâœ…çš„æ•°æ®é›†**

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç”¨æ‰€æœ‰è§„èŒƒä¸­çš„å­—æ®µååˆ›å»ºä¸€ä¸ªå¤§æ•°æ®é›†ã€‚

ä¸‹é¢çš„`dict_dataset`å‡½æ•°è·å–æ–‡ä»¶åå’Œè·¯å¾„åˆ—è¡¨ï¼Œå¹¶æ‰“å¼€æ¯ä¸ªè§„èŒƒæ–‡ä»¶ã€‚å¯¹äºæ¯ä¸ªæ–‡ä»¶ï¼Œ`get_field`å‡½æ•°è¿”å›ä¸€ä¸ªå­—æ®µååˆ—è¡¨ã€‚ä¸€äº›å­—æ®µåç§°å¯èƒ½åœ¨ä¸€ä¸ªè§„èŒƒä¸­é‡å¤ã€‚ä¸ºäº†æ¶ˆé™¤è¿™ç§é‡å¤ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨`list(dict.fromkeys(col))`å°†å­—æ®µååˆ—è¡¨ä»åˆ—è¡¨è½¬æ¢åˆ°å­—å…¸ï¼Œç„¶åå†è½¬æ¢å›æ¥ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥æ ‡è®°åˆ—è¡¨ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œä»¥æ–‡ä»¶åä½œä¸ºé”®ï¼Œä»¥å­—æ®µååˆ—è¡¨ä½œä¸ºå€¼ã€‚

```
def dict_dataset(datasets):
    dataset_dict={}
    for i in datasets:
        with open(i, 'r') as foo:
            col=algo.get_fields(yaml.safe_load(foo.read()))
            if col:
                mylist = list(dict.fromkeys(col))
                tokenized_list=tokenizer(mylist)
                dataset_dict.update({i: tokenized_list})
            else:
                continue
    return (dataset_dict)
```

# **6ã€‚æµ‹è¯•åµŒå…¥âœ…**

**ä»£ç  2vec å’Œæ‰‹å¥—**

ç°åœ¨æˆ‘ä»¬å¯ä»¥æ‰¾å‡ºè¯æ±‡è¡¨å¤–çš„å•è¯(not_identified_c2v)å¹¶ç»Ÿè®¡è¿™äº›å•è¯å  code2vec è¯æ±‡è¡¨çš„ç™¾åˆ†æ¯”ã€‚ä»¥ä¸‹ä»£ç ä¹Ÿé€‚ç”¨äº GloVeã€‚

```
not_identified_c2v=[]
count_not_indent=[]
total_number=[]

for ds in test1:
    count=0
    for i in data[ds]:
        if not i in model:
            not_identified_c2v.append(i)
            count+=1
    count_not_indent.append(count)
    total_number.append(len(data[ds]))

total_code2vec=sum(count_not_indent)/sum(total_number)*100
```

**ç©ºé—´**

ç©ºé—´è¯æ±‡æ˜¯ä¸åŒçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç›¸åº”åœ°ä¿®æ”¹æˆ‘ä»¬çš„ä»£ç :

```
not_identified_sp=[]
count_not_indent=[]
total_number=[]

for ds in test1:
    count=0
    for i in data[ds]:
        f not i in nlp.vocab:
                count+=1
                not_identified_sp.append(i)
    count_not_indent.append(count)
    total_number.append(len(data[ds]))

total_spacy=sum(count_not_indent)/sum(total_number)*100
```

å¯¹äº code2vecã€GloVe å’Œ spaCyï¼Œæœªè¯†åˆ«å•è¯çš„ç»“æœç™¾åˆ†æ¯”åˆ†åˆ«ä¸º`3.39, 2.33, 2.09`ã€‚ç”±äºæ¯ä¸ªç®—æ³•çš„ç™¾åˆ†æ¯”ç›¸å¯¹è¾ƒå°ä¸”ç›¸ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥åšå¦ä¸€ä¸ªæµ‹è¯•ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç”¨æ‰€æœ‰ API è§„èŒƒä¸­åº”è¯¥ç›¸ä¼¼çš„å•è¯åˆ›å»ºä¸€ä¸ªæµ‹è¯•å­—å…¸:

```
test_dictionary={'host': 'server',
'pragma': 'cache',
'id': 'uuid',
'user': 'client',
'limit': 'control',
'balance': 'amount',
'published': 'date',
'limit': 'dailylimit',
'ratelimit': 'rate',
'start': 'display',
'data': 'categories'}
```

å¯¹äº GloVe å’Œ code2vecï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ gensim åº“æä¾›çš„`similar_by_vector`æ–¹æ³•ã€‚spaCy è¿˜æ²¡æœ‰å®ç°è¿™ä¸ªæ–¹æ³•â€”â€”ä½†æ˜¯æˆ‘ä»¬å¯ä»¥è‡ªå·±æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å•è¯ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ ¼å¼åŒ–è¾“å…¥å‘é‡ï¼Œä»¥ä¾¿åœ¨è·ç¦»å‡½æ•°ä¸­ä½¿ç”¨ã€‚æˆ‘ä»¬å°†åœ¨å­—å…¸ä¸­åˆ›å»ºæ¯ä¸ªé”®ï¼Œå¹¶æ£€æŸ¥ç›¸åº”çš„å€¼æ˜¯å¦åœ¨ 100 ä¸ªæœ€ç›¸ä¼¼çš„å•è¯ä¸­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ ¼å¼åŒ–è¯æ±‡è¡¨ï¼Œä»¥ä¾¿åœ¨`distance.cdist`å‡½æ•°ä¸­ä½¿ç”¨ã€‚è¿™ä¸ªå‡½æ•°è®¡ç®—è¯æ±‡è¡¨ä¸­æ¯å¯¹å‘é‡ä¹‹é—´çš„è·ç¦»ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä»æœ€å°è·ç¦»åˆ°æœ€å¤§è·ç¦»å¯¹åˆ—è¡¨è¿›è¡Œæ’åºï¼Œå–å‰ 100 ä¸ªå•è¯ã€‚

```
from scipy.spatial import distance

for k, v in test_dictionary.items():
    input_word = k
    p = np.array([nlp.vocab[input_word].vector]) closest_index = distance.cdist(p, vectors)[0].argsort()[::-1][-100:]
    word_id = [ids[closest_ind] for closest_ind in closest_index]
    output_word = [nlp.vocab[i].text for i in word_id]
    #output_word
    list1=[j.lower() for j in output_word]
    mylist = list(dict.fromkeys(list1))[:50]
    count=0
    if test_dictionary[k] in mylist:
        count+=1
        print(k,count, 'yes')
    else:
        print(k, 'no')
```

ç»“æœæ€»ç»“åœ¨ä¸‹è¡¨ä¸­ã€‚spaCy æ˜¾ç¤ºå•è¯â€œclientâ€åœ¨å‰ 100 ä¸ªä¸å•è¯â€œuserâ€æœ€ç›¸ä¼¼çš„å•è¯ä¸­ã€‚å®ƒå¯¹äºå‡ ä¹æ‰€æœ‰çš„ OpenAPI è§„èŒƒéƒ½æ˜¯æœ‰ç”¨çš„ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äº OpenAPI è§„èŒƒç›¸ä¼¼æ€§çš„æœªæ¥åˆ†æã€‚å•è¯â€œbalanceâ€çš„å‘é‡æ¥è¿‘å•è¯â€œamountâ€çš„å‘é‡ã€‚æˆ‘ä»¬å‘ç°å®ƒå¯¹æ”¯ä»˜ API ç‰¹åˆ«æœ‰ç”¨ã€‚

![](img/0e0914c0a3c78cc433e66d05b991a3f5.png)

# **ç»“è®º**

æˆ‘ä»¬å·²ç»ä¸º OpenAPI è§„èŒƒå°è¯•äº†ä¸‰ç§ä¸åŒçš„å•è¯åµŒå…¥ç®—æ³•ã€‚å°½ç®¡è¿™ä¸‰ä¸ªè¯åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šéƒ½è¡¨ç°å¾—å¾ˆå¥½ï¼Œä½†æ˜¯å¯¹æœ€ç›¸ä¼¼çš„è¯çš„é¢å¤–æ¯”è¾ƒè¡¨æ˜ spaCy æ›´é€‚åˆæˆ‘ä»¬çš„æƒ…å†µã€‚

spaCy æ¯”å…¶ä»–ç®—æ³•æ›´å¿«ã€‚ä¸ GloVe æˆ– code2vec è¯æ±‡è¡¨ç›¸æ¯”ï¼ŒspaCy è¯æ±‡è¡¨ä¸Šä¼ é€Ÿåº¦å¿«äº”å€ã€‚ç„¶è€Œï¼Œç¼ºä¹å†…ç½®å‡½æ•°â€”â€”æ¯”å¦‚`similar_by_vector`å’Œ`similar_word`â€”â€”æ˜¯ä½¿ç”¨è¿™ç§ç®—æ³•çš„ä¸€ä¸ªéšœç¢ã€‚

æ­¤å¤–ï¼ŒspaCy é€‚ç”¨äºæˆ‘ä»¬çš„æ•°æ®é›†è¿™ä¸€äº‹å®å¹¶ä¸æ„å‘³ç€ spaCy å¯¹ä¸–ç•Œä¸Šçš„æ¯ä¸ªæ•°æ®é›†éƒ½æ›´å¥½ã€‚å› æ­¤ï¼Œè¯·éšæ„ä¸ºæ‚¨è‡ªå·±çš„æ•°æ®é›†å°è¯•ä¸åŒçš„å•è¯åµŒå…¥ï¼Œå¹¶åœ¨è¯„è®ºä¸­è®©æˆ‘ä»¬çŸ¥é“å“ªä¸€ç§æ›´é€‚åˆæ‚¨ï¼

æ„Ÿè°¢é˜…è¯»ï¼