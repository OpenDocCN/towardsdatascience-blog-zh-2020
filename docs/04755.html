<html>
<head>
<title>Decision Trees from the Root Up</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从根向上的决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-from-the-root-up-1f169b6e0460?source=collection_archive---------48-----------------------#2020-04-26">https://towardsdatascience.com/decision-trees-from-the-root-up-1f169b6e0460?source=collection_archive---------48-----------------------#2020-04-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0975" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">手动构建一个最优决策树，以理解这个ML拥护者令人惊讶的常识性机制。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/685296f13cf46c57fea19ac6b8b4a610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lOUyNsQd2_N189m7_4fc6A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.unsplash.com" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@leliejens?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">延斯·勒列</a>拍摄</p></figure><p id="c337" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">决策树是数据科学中的基本模型类型之一。幸运的是，他们提供了一个很好的例子，说明计算机如何自动化简单的人类直觉来构建大型复杂的模型。事实上，决策树优化了一个简单的标准，它反映了我们在日常生活中如何做决定。了解预测模型如何反映我们自己的心理模型，让我们能够批判性地思考它们可能会如何出错，以及一个好的模型是什么样子的。</p><p id="af7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">本教程概述</strong></p><p id="9598" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文的目标是提供一个交互式的决策树理论介绍。学完本教程后，您应该能够:</p><ul class=""><li id="74bf" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">描述决策树的结构和功能。</li><li id="5512" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用熵和信息增益的概念手工构建一个小决策树。</li><li id="3fd1" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">关于算法实现如何大规模构建决策树的原因。</li></ul><p id="8f43" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">访问本教程的代码</strong></p><p id="46f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文中的代码和可视化都是使用numpy、pandas和Altair(图片由作者提供，除非另有说明)在Python中生成的。您可以使用下面的方法跟随图表并与之交互。既然目的是手工构建自己的决策树，那么这个强烈推荐！</p><ul class=""><li id="7570" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">方法一:<a class="ae kv" href="https://github.com/MattJBritton/InteractiveDecisionTrees" rel="noopener ugc nofollow" target="_blank"> Github </a>。克隆或派生存储库，并通过binder/environment.yml中的YAML文件安装环境</li><li id="f53c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">方法二:<a class="ae kv" href="https://nbviewer.jupyter.org/github/MattJBritton/InteractiveDecisionTrees/blob/master/InteractiveDecisionTrees.ipynb" rel="noopener ugc nofollow" target="_blank"> NBViewer </a>。访问带有交互式图表的笔记本版本。</li><li id="7db3" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">方法三:<a class="ae kv" href="https://mybinder.org/v2/gh/MattJBritton/InteractiveDecisionTrees/master?filepath=InteractiveDecisionTrees.ipynb" rel="noopener ugc nofollow" target="_blank">粘合剂</a>。在线访问笔记本的完整实时版本。您可以修改并运行笔记本，甚至可以用另一个数据集进行尝试。</li><li id="5357" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">方法4:按照整篇文章中的链接查看Datapane上的交互式图表。</li></ul><h1 id="c69f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated"><strong class="ak">什么是决策树？</strong></h1><p id="7254" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">决策树是一种逻辑模型，可以帮助您根据已知数据做出预测。这种预测包括某事是否会发生，或者某个项目是否属于某个类别。例如，根据患者的病史预测肿瘤是恶性还是良性。</p><p id="3517" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种预测是通过一系列是或否的问题做出的。病人吸烟吗？”是一个帮助决策树将数据划分为子集的问题，每个子集都有自己的行为。如果这个行为比整个数据集更容易预测，那么这个决策树是有效的。让我们用一个日常的例子来具体说明这一点。</p><p id="6882" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">决策树的日常例子</strong></p><p id="f1a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">毫无疑问，你已经在人生的某个阶段做了一个决策树。让我们以度假打包行李为例。你应该带泳衣吗？您的决策过程可能看起来有点像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/76ae9635bc05696ecac8cc0d5c18d9e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_hFj-jCys9RcszgprXrJUQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">灰色表示树的实际结构，而紫色箭头表示关键术语。</p></figure><p id="6c4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">决策树总是从单个节点(顶部灰色气泡)开始，代表要问的第一个问题。在这个例子中，我们询问我们的目的地是否有游泳的地方。这个问题被称为分裂。它必须是“是/否”——请注意，我们通过在80度处创建一个阈值来处理温度定量标度。</p><p id="f351" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最终，我们到达一个“叶”节点。这是我们做决定的地方。还要注意，每个节点都是两个“子”节点的“父”节点，除非它是叶节点。</p><h1 id="2348" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated"><strong class="ak">构建好的决策树</strong></h1><p id="1e9e" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">这个模型对我们来说有直观的意义。但是，请记住，在机器学习中，我们必须评估模型，我们经常会处理复杂的数据集，而我们对这些数据集的直觉有限。那么我们该如何评价上面的模型呢？更广泛地说，区分好模型和坏模型的特征是什么？几个例子会对我们有所帮助。</p><div class="kg kh ki kj gt ab cb"><figure class="nf kk ng nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/5b6f7122dbba0f31d4a596bcbd00b53d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bRpaqAeAdAjRQ-aX8uHlqg.png"/></div></figure><figure class="nf kk ng nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/b6243d522e093ea17a253cec0c1dfc70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*qvL-SpG4sjwhK4nsta7bEg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk nl di nm nn translated">一个好的决策树产生相对“纯”的子节点，一个坏的决策树通过问一个不相关的问题来延续不确定性。</p></figure></div><p id="08f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在左边，我们有一个好的决策树的例子。我们可以评估这种心理模型的有用性的一种方法是问我们自己，在我们最近的10次旅行中，它会有怎样的表现，这里用彩色方块表示。红色代表不需要泳衣的旅行，蓝色代表需要泳衣的旅行。</p><p id="c282" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，我们最初有一个60/40的分割，但是我们的“否”分支全部是红色的。换句话说，在我们过去的10次旅行中，有3次我们事先知道没有游泳的地方，在所有这些旅行中，我们都没有后悔没有穿泳衣。所以当我们对这个问题回答“不”的时候，我们可以非常确定我们正在做出正确的决定。</p><p id="94f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们过去的10次旅行中，有7次我们知道可以游泳的地方，而在这7次中的6次中，这是正确的选择。最后一个发生了什么？可能是太冷了，也可能是泳池意外关闭。好的一面是，对于我们的两个分支，我们能够做出比不提问时更好的猜测。</p><p id="42f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">糟糕的决策树</strong></p><p id="9519" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在右边，我们有一个问题，似乎和泳衣没有任何关系。此外，我们可以看到，我们的问题将我们过去的旅行细分为两个子集，每个子集都具有相同的“把它带回家”或“把它留在家里”分裂。如果两个是/否分支有相同的分布(60/40)，那么问这个问题并没有给我们任何新的信息。那么，我们当初为什么要费神问这个问题呢？</p><p id="50ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">定义一个“好的”决策树</strong></p><p id="e4f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以根据这一观察结果，暂时声明我们构建决策树的目标是产生问题尽可能少的“纯”子节点。因此，如果这是树的目标，那么每一次分割都应该旨在尽可能地“净化”其子节点。</p><p id="58e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">坚持这个“纯洁”的概念，因为它很快就会有用。与此同时，让我们转向一个真正的数据科学问题。</p><h2 id="9862" class="no mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">在泰坦尼克号数据集上手工构建决策树</h2><p id="3961" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">对于我们真正的问题，我们将使用泰坦尼克号的数据集。每一行都是一名乘客以及人口统计和旅行信息，比如他们的年龄、性别和机票等级。通常的目的是预测乘客是幸存还是死亡。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/3a206bc9ee225f942e81ff109eb759b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePshx2hTNA8FAq7Deu9EEg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">泰坦尼克号数据集的前5行。</p></figure><p id="bce3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们如何着手建立一个决策树来预测哪些乘客幸存下来？什么是好的第一次分裂？</p><p id="531c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">没有任何统计或可视化工具，我们所能做的就是任何关于这个数据集或领域的现有知识。也许我们看过电影，猜测幸存的女性比男性多。或者我们可以利用对世界和船的设计的了解来猜测头等舱的乘客比三等舱的乘客过得好。</p><p id="f300" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，作为数据科学家，我们经常会被要求根据新领域的数据或者我们之前没有多少直觉的数据建立模型。换句话说，<strong class="ky ir">我们需要一种数据驱动的方式来选择好的拆分</strong>。我们可以从每个特性在每个类中的分布开始(存活和死亡)。我们将使用堆积条形图表示分类特征，使用<a class="ae kv" rel="noopener" target="_blank" href="/histograms-and-density-plots-in-python-f6bda88f5ac0"> KDE </a>(核密度估计，基本上是一个平滑的直方图)图表示定量特征。</p><div class="kg kh ki kj gt ab cb"><figure class="nf kk ob nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/45c8cdf6d7c43a78b58aeab8179b02cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*ozhPyIaXMtxhYPkcrYqIVQ.png"/></div></figure><figure class="nf kk oc nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ef3e21a2d40f5f612c7f005aac1f306f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*LDR8TAngplt43b5Hr7cjEg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk od di oe nn translated">5个特征中幸存和死亡乘客的分布。</p></figure></div><p id="f111" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们如何使用分布来确定候选人分裂？当查看分类变量时，我们希望选择一个分布与其他类别明显不同的类别。例如，我们可以看到男性和女性的存活率不同。对于定量变量，我们寻找一条线比另一条线高得多的区域。年龄似乎符合这个要求，独自旅行的人也是如此(<em class="nd">家庭规模&lt; 1 </em>)。</p><p id="e186" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是现在我们有几个潜在的分裂，我们如何评估它们是否有好处呢？我们如何比较多个选项？</p><p id="74c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还记得我们之前讨论的“纯子节点”目标吗？我们可以用这个概念来建立一个分割的“好”的定量测量。</p><h1 id="d6dd" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">熵和信息增益</h1><p id="6073" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们将定义<em class="nd">熵</em>，一个节点纯度的度量标准，以及<em class="nd">信息增益</em>，一个从父节点到其子节点的<em class="nd">熵</em>的度量标准。下面你会找到定义和公式。如果这有点令人困惑，请随意跳到下面的图片，该图片展示了上下文中的指标。</p><p id="8ce3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">熵</strong></p><ul class=""><li id="9f14" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">衡量节点“纯净”程度的指标。</li><li id="55a8" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">基于两个类别的比率，例如幸存者与非幸存者的比率。</li><li id="5871" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">范围从0到1。0表示纯节点(所有幸存者或反之亦然)，1表示50/50分割。请注意，熵为1对应于“最坏的情况”，我们最不可能做出有根据的猜测。</li></ul><p id="d4ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">信息增益</strong></p><ul class=""><li id="e1ec" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">分割降低熵的程度(降低越大越好，因为目标是0)。</li><li id="d442" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">等于父节点的熵减去子节点熵的加权平均值。</li><li id="026b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">构建决策树的算法在具有最高信息增益的每个节点上寻找分裂。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/a1e264375b4cc0e0b04a27e45e36cb3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xtnvia4phm57X-_pWkSHPw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">熵和信息增益公式</p></figure><h2 id="93b1" class="no mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">可视化父节点和子节点的熵</h2><p id="ff7b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">通过在决策树结构上叠加计算，我们可以更直观地了解如何选择最佳拆分。下图评估了分割<em class="nd">类=第三类</em>的信息增益。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/5ec6e6f5ba8eca99ce220316e7bce842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*5ZpYLdE9t0X6Gi_7se_d6Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">拆分“类=第三”的信息增益计算。</p></figure><p id="4363" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">橙色和蓝色的方块显示了每个节点的存活/死亡情况。对于每个节点，我们也有熵、存活率和人数。总之，这些事实讲述了一个关于数据集的故事，并允许我们评估这种拆分可能有多好:</p><ul class=""><li id="fcfa" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">我们的数据集整体由891人组成，其中38%幸存。因为38/62的分裂非常接近50/50，所以我们的熵几乎是1。</li><li id="cf7a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">491名乘客(55%)属于三等舱。这些人只有24%的存活率，比总体水平要差得多。对于这个子集，熵因此下降到0.8。</li><li id="b492" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">在右边的分支，我们有头等舱和二等舱的乘客，他们更有可能幸存。这个子集有56%的存活率。然而，尽管我们已经了解了这个群体的一些重要情况，这里的熵实际上比整个数据集中的要高一点。56%的存活率比人口40%的存活率更接近50/50——注意，在确定熵时，方向并不重要。</li><li id="4c0e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">因为我们分裂的一边有明显较低的熵，而另一边基本上保持不变，低熵子集更大，这种分裂确实降低了总熵。这体现在底部的信息增益公式中。</li><li id="a094" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">这种分离的信息增益是0.076。请注意，这个数字作为一个绝对量(相对于目标变量的总熵)有意义，但与其他分割相比最有用。</li></ul><h2 id="cde0" class="no mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">手工构建决策树</h2><p id="c21e" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">有了信息增益的概念，我们现在可以计算个体分裂的相对价值。构建决策树的算法可以快速评估许多潜在的分裂，以找到最佳分裂。要手动完成这项工作，我们需要一个工具来帮助我们。</p><p id="ba0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种交互式可视化让我们尝试了许多潜在的分裂。以前的功能分布图已经增加了选择分割的功能，只需单击条形图中的类别或平移定量功能即可。当选择分裂时，下面的子图表列出了左和右子集的信息增益和熵。</p><p id="5e8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，为了可读性，熵和信息增益数字被乘以100。否则，上面的例子可以完全重现。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oh oi l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用于调查可能分裂的交互式可视化。查看<a class="ae kv" href="https://datapane.com/mbritton/reports/2494c592-ea45-402a-b1ab-6f6cb34ea2c7/" rel="noopener ugc nofollow" target="_blank">数据面板</a>上的大图。</p></figure><h2 id="e4a9" class="no mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">构建决策树的下一层</h2><p id="9778" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在信息获取方面有一个明显的赢家:男性/女性是迄今为止最好的划分。然而，假设我们想要一个更复杂的多变量模型，我们可以在左侧和右侧重复这个过程，以确定每个节点的最佳分割。我们现在正在构建一个如下所示的模型:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/3f2a4b8f177fa6b39a3b33cc9cc902a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qf5m0lF8KXzLTX0BhtOapA.png"/></div></div></figure><p id="c6ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以使用以下女性和男性子集的交互式可视化。这个版本增加了一个折线图，显示所有可能的分割的信息增益。寻找任何有趣的场景，其中多个分割具有相似的信息增益值。</p><p id="a5cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">女</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oh oi l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">为女性子集选择最佳分割。</p></figure><p id="234e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">男</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oh oi l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">为男性子集选择最佳分割。</p></figure><h2 id="e136" class="no mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">将我们的手工制作的圣诞树与Scikit-Learn的版本进行比较</h2><p id="1855" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">现在，我们可以将这个简单的手工制作的模型与实现决策树算法产生的版本进行比较，比如scikit-learn中的版本。以下代码执行基本的数据清理和特征工程，然后构建模型并输出其结构。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ok oi l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/475e0b7386efcaac9186538bee541e73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*phG5OQcf6j9WX-WovDt17g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">scikit-learn决策树的结构。输出来自export_text()方法。</p></figure><p id="e280" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型具有0.312的总信息增益(从第一节点到叶子)，以及0.796的精确度。你也有同样的发型吗？</p><h2 id="1c9a" class="no mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">结论</h2><p id="2c63" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们有时认为机器学习模型是一种与我们自己完全不同的智能类型。然而，在这种情况下，至少，建模的过程是植根于我们可能用来解决问题的相同类型的常识性解决方案。决策树是使用简单的迭代贪婪最大化<em class="nd">熵</em>构建的，熵是一个我们有直觉的量。</p><p id="db9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，熟悉树构造算法有助于我们作为数据科学家理解和欣赏我们可以用几行代码创建的模型中固有的权衡。</p><p id="6fef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我关于决策树的下一篇文章中，我们将基于这些概念来分析更复杂的决策树的结构，并理解树如何能够<em class="nd">过度适应</em>一个训练集。理解好的决策树的结构特征，并将不同类型的结构放入分类法中，对于数据科学家来说是非常有用的技能。</p><h2 id="670d" class="no mh iq bd mi np nq dn mm nr ns dp mq lf nt nu ms lj nv nw mu ln nx ny mw nz bi translated">脚注</h2><p id="f9c1" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">分割的替代术语包括<em class="nd">规则</em>或<em class="nd">测试节点。</em>我在这里使用<em class="nd"> split </em>来引起人们对谓词如何将数据集分割成子集的注意。</p><p id="e4da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据集由<a class="ae kv" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上“列车”子集中的891名乘客组成。年龄列的缺失数据已被估算。详情见<a class="ae kv" href="https://github.com/MattJBritton/InteractiveDecisionTrees" rel="noopener ugc nofollow" target="_blank">源代码</a>。</p><p id="261c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你会经常看到<em class="nd">基尼杂质</em>作为分裂节点的替代标准。例如，scikit-learn将此作为默认标准，并能够切换到信息增益。虽然这种数学略有不同，但概念上的考虑是相同的，并且在实践中使用哪一种并不重要。</p></div></div>    
</body>
</html>