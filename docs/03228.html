<html>
<head>
<title>Learning TensorFlow 2 with Style</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">有风格地学习TensorFlow 2</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-tensorflow-2-with-style-eab12fd94365?source=collection_archive---------28-----------------------#2020-03-27">https://towardsdatascience.com/learning-tensorflow-2-with-style-eab12fd94365?source=collection_archive---------28-----------------------#2020-03-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b23a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">通过实现快速神经风格转换来熟悉TensorFlow 2</h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/015a123b001218cf4b9a3d112a210fce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sEIeH-e2uhXCP8rhOLoB0g.jpeg"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">个人照片的雾滚过塔马尔派斯山风格化的尖叫由爱德华蒙克</p></figure><h1 id="3e06" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">背景</h1><p id="6519" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">在过去的几年里，我对计算机视觉产生了浓厚的兴趣，并花了相当多的时间参加课程、阅读论文和做与该领域相关的兼职项目。这和我对摄影的热情有很大的重合。引起我注意的一个计算机视觉项目是神经风格转移，首先由莱昂·加蒂斯等人提出，后来由贾斯廷·约翰逊等人提出了特别快速的风格转移。最近在处理代码时，我发现它的大部分都过时了，缺乏支持，因此很难处理。在对其他机器学习框架有所熟悉的同时，我以此为契机更新了代码，对TensorFlow 2有了更深入的了解。由于快速风格转换包括生成网络和更复杂的损失函数，它为学习张量流的一些细微差别提供了一个很好的项目，而不是基本的分类模型。</p><p id="3e65" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">本文的目标是强调使用TensorFlow 2的一些核心功能和主要经验，以及它们如何应用于快速风格转换。我将参考与神经类型转移相关的核心概念，但会浏览其他概念，因此熟悉一些会有所帮助。如果你对这个问题完全陌生，我在下面提供了一些有用的资源。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="77f1" class="ko kp it bd kq kr mw kt ku kv mx kx ky kz my lb lc ld mz lf lg lh na lj lk ll bi translated">简要概述</h1><p id="2e6a" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">样式转移的目标是获取一个内容图像，我们称之为<em class="nb"> C </em>，和一个带有样式的图像，<em class="nb"> S </em>，创建一个转换后的图像，<em class="nb"> Y </em>，将<em class="nb"> S </em>的样式应用于<em class="nb"> C </em>。换句话说，我们想要混合<em class="nb"> C </em>的内容和<em class="nb"> S </em>的样式，如下图<em class="nb">所示。</em></p><figure class="nd ne nf ng gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nc"><img src="../Images/d9550589bc0ece396cf658bc7960d1c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CtVVUqPbGItIIU9MHcuXWg.jpeg"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">表示将一个图像的样式应用到另一个图像的内容，以创建一个经过变换的、风格化的图像。</p></figure><p id="30bd" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">在Gatys等人的原始风格转移工作中，具有随机噪声的基础图像<em class="nb"> X </em>被传递到训练过的VGG网络中，并且来自指定层的输出被用于提取哪些特征被激活。从在<em class="nb"> X </em>和<em class="nb"> C </em>之间的指定内容层输出的L2损失可以帮助学习内容。在<em class="nb"> X </em>和<em class="nb"> S </em>之间来自指定风格滤波器的输出的格拉姆矩阵的L2损失可以帮助学习风格。这个图层提取可以在下图右侧的<em class="nb">损失网络</em>中看到。当<em class="nb"> X </em>以随机噪声和多次迭代开始一幅图像时，它被训练并转换为<em class="nb"> Y </em>。TensorFlow团队<a class="ae nh" href="https://www.tensorflow.org/tutorials/generative/style_transfer" rel="noopener ugc nofollow" target="_blank">在这里</a>有一个关于这个原始实现的教程。</p><figure class="nd ne nf ng gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ni"><img src="../Images/fd951a606868185dc38a1e4bb054ad5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-yY7xBhDv2e6DeoSbSjWMA.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">Johnson等人提出的快速风格转换网络架构，注意:我的实现使用VGG19 vs VGG16，来源:<a class="ae nh" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">实时风格转换和超分辨率的感知损失</a> (2016)</p></figure><p id="2207" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">在快速风格转换的情况下，生成神经网络被训练来转换任何图像，而不是单个图像被训练和转换。在上面由Johnson等人提出的架构中，内容图像被传递到变换网络中，然后<em class="nb">变换的输出图像</em>被传递到VGG网络中。从那里，使用提取的样式和内容层，变换的图像经历与原始样式转移模型类似的损失。为了训练生成网络，将在多次迭代中使用许多不同的内容图像。训练的时间要长很多，但是推理时间最多可以快1000倍！</p><p id="1a17" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">在下面的例子中，相同的内容图像在不同的训练点通过网络传递。随着训练迭代次数的增加，<em class="nb"> i </em>，变换后的图像，<em class="nb"> X </em>，向我们期望的内容和风格的混合方向收敛。</p><figure class="nd ne nf ng gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nj"><img src="../Images/a62b26892e56d8abc7ffe08186808a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*f_HK8-SjcZUH5qBQj5EfhA.gif"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">创成式模型如何在多次迭代中发生变化的示例。随着时间的推移，通过优化损失函数L(Cᵢ、s、Xᵢ).，模型训练和转换后的输出Xᵢ将收敛到我们期望的风格和内容的混合Yᵢ</p></figure></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="e8aa" class="ko kp it bd kq kr mw kt ku kv mx kx ky kz my lb lc ld mz lf lg lh na lj lk ll bi translated">模型</h1><p id="b7eb" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">为了构建模型，我重点使用了TensorFlow的内置特性。在TensorFlow核心、Keras和TensorFlow附加组件之间，有许多工具可以利用。</p><h2 id="c699" class="nk kp it bd kq nl nm dn ku nn no dp ky lx np nq lc mb nr ns lg mf nt nu lk iz bi translated">自定义图层</h2><p id="d285" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">TensorFlow 2使得将这些层组合成自定义层变得非常容易。在下面的例子中，你将看到我如何建立一个2D卷积层和模型的残差块。这些层将用于构建生成性转换网络。</p><figure class="nd ne nf ng gt kd"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="3201" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">当从Layer类继承时，属性和方法将被递归调用。例如，当访问自定义层中的可训练变量时，将返回组成该自定义层的<em class="nb">所有层</em>的<em class="nb">所有可训练变量</em>。这也适用于使用自定义层从模型中调用可训练变量，使得变量访问非常直观。</p><h2 id="5497" class="nk kp it bd kq nl nm dn ku nn no dp ky lx np nq lc mb nr ns lg mf nt nu lk iz bi translated">变压器网络</h2><p id="e447" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">Keras函数模型用于构建生成神经网络。Keras中的功能模型比顺序模型更加通用，因为后者主要用于堆叠层。使用功能模型，可以构建更复杂的神经架构，如共享层模型和多输出模型。与单独堆叠的层相比，这些模型类似于图上的节点。这在ResNet、InceptionNet和MobileNets等其他体系结构中有所体现。由于我们有一些剩余的模块，功能模型使用起来会更友好一些。</p><figure class="nd ne nf ng gt kd"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="5250" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">由于我们组成了我们的自定义层，它仍然是非常直观的遵循。首先，实例化层，然后通过层的可调用性传递Keras输入，以期望的方式建立模型下面的图。最后，所有层的输出和原始输入用于创建一个<code class="fe nx ny nz oa b">tf.keras.Model</code>实例并构建图表。我创建了一个泛型类作为模型的包装器。*如上所述，对<code class="fe nx ny nz oa b">model.trainable_variables</code>的调用将从我们的自定义层返回所有可训练变量。</p><p id="1fe6" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">*请注意，您也可以创建继承模型，并仍然使用功能模型，这有一些好处，但我发现它更脊和难以工作。</p><h2 id="0f2d" class="nk kp it bd kq nl nm dn ku nn no dp ky lx np nq lc mb nr ns lg mf nt nu lk iz bi translated">VGG19型号</h2><p id="d5c2" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">Keras图书馆包含一些公共网络，包括VGG网络。可以使用某些预先训练的权重和修剪最后完全连接的层的选项来创建它们。如果你想对一个给定的问题使用迁移学习，并且需要使用不同的分类层，这是非常有用的。我的实现使用VGG19网络，而不是VGG16，尽管可以使用各种图像识别网络。</p><p id="0173" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">正如在概述中提到的，我们将通过VGG网络传递我们转换的图像，<em class="nb"> Xᵢ </em>，并提取一些输出层。我们将希望有多个层作为输出，而不是一个分类器作为输出。还记得功能模型是如何适用于多输出的吗？可以使用VGG网络创建新的功能模型。VGG输入被传递到新模型中，并且期望层的输出作为模型输出被传递。当新模型被调用时，张量将通过VGG模型传递，这些层的输出将被返回。</p><p id="dc69" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">下面是输出第一个卷积模块第一层(conv1_1)和第三个卷积模块第二层(conv3_2)的示例代码片段，以及几个通道的样本输出。这些输出表示当预训练滤波器在图像上卷积时激活的图像部分。conv1_1的浅层输出激活边缘等简单特征来分解图像，而深层输出激活更复杂的信息(在这种情况下，它看起来像阴影)。</p><figure class="nd ne nf ng gt kd"><div class="bz fp l di"><div class="nv nw l"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">从VGG网络提取输出的例子。注意:在实际实现中，提取了更多指定的过滤器。</p></figure><figure class="nd ne nf ng gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ob"><img src="../Images/b3b97cf0a8d0f59b15b2da5b9ae86230.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odwF7wAJ7SvOn_Kxr_Y80g.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">a)通过VGG19的原始RGB图像，conv1 _ 1层的第17个滤波器(索引为1)的输出，conv3 _ 2层的第6个滤波器(索引为1)的输出；注意图像c由于汇集而变小</p></figure><p id="e8d0" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">层间的L2损失然后用于比较<em class="nb"> Xᵢ </em>和<em class="nb"> Cᵢ </em>的内容输出。关于内容，如果汽车图像的“车轮”特征在我们的原始图像上激活，并且也在转换的图像上激活，则内容仍然是强匹配的，并且损失将会更低。同样，gram矩阵之间的L2损失用于比较<em class="nb"> Xᵢ </em>和<em class="nb"> S </em>的哪些样式特征是激活的特征。</p><h1 id="1e24" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">用梯度带训练重量</h1><p id="cad6" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">TensorFlow 2中较大的变化之一是切换到急切执行。在TensorFlow的早期版本中，计算图将首先手动构建，只有在之后才会传入和调用张量。这不太直观，也很难操作。现在，电子传感器可以通过计算来设置和调用。计算图被抽象出来，并在引擎盖下设置和执行。就像在其他很多懒惰与渴望评估的例子中一样，我发现这更容易理解，也更容易调试。</p><p id="3601" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">在TensorFlow 2中，分类问题在Keras API中得到很好的支持。然而，如果您需要创建一个定制的训练循环，训练特定的变量，或者为损失函数提供特殊的输入，这就不那么友好了。在这些情况下，GradientTape可用于在执行期间跟踪可训练变量，获取梯度，然后通过优化器应用梯度。这非常适合于快速风格转移模型，因为训练涉及具有定制训练循环的生成网络，并且将几个不同的输入传递给损失函数。</p><h2 id="4276" class="nk kp it bd kq nl nm dn ku nn no dp ky lx np nq lc mb nr ns lg mf nt nu lk iz bi translated">监控变量</h2><p id="783a" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">在GradientTape上下文中，可训练变量在向前传递期间被“监视”和跟踪。当稍后执行反向传播时，被监视的变量将根据它们的梯度应用各自的变化。虽然可训练变量<em class="nb">可以在训练中被跟踪和调整，但被监视变量<em class="nb">将被跟踪</em>，因此它们可以被优化器调整。</em></p><p id="a160" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">无需在GradientTape上下文中调用<code class="fe nx ny nz oa b">watch</code>,即可隐式观察可训练变量。对于更细粒度的控制，您可以在GradientTape的构造函数中关闭它，只有块中显式“监视”的变量才会被跟踪以进行自动区分。GradientTape上下文还允许您获取被监视的变量，这有助于调试。对于多模型，我发现自己设置被监视的变量比依赖隐式调用更实际。</p><p id="67e9" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">一旦变量被观察到，张量通过模型并计算损失。对于快速风格转换，我们将通过我们的生成器网络进行正向传递，获得转换后的图像，<em class="nb"> Xᵢ </em>，然后将该图像通过VGG分类网络。在下面的例子中，我创建了渐变磁带上下文，观察适当的变量，然后向前传递。之后，我使用损失来获得适当的梯度，然后优化器使用这些梯度来调整可训练变量。</p><figure class="nd ne nf ng gt kd"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h1 id="d3b5" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">结果</h1><p id="29b4" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">现在是有趣的部分！在对每种风格的网络进行数小时的训练后，这些都用默认的训练参数作为通用基线。请注意，为了获得更好的结果，有些模型可以调整得更细或更长。例如，在“尖叫”实现中，有几个未经训练的点是从白色开始的。</p><figure class="nd ne nf ng gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oc"><img src="../Images/d02bc73a8debf9a4ce4abf74a38379f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oJ3j_6Ge4kxva9Q941mx_A.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">具有1)用于训练的风格图像，2)通常用作基线的芝加哥图像，以及3)显示变化的个人图像的网络的结果</p></figure></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><p id="b4ae" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">对摄影、艺术和深度学习充满热情，这是一个非常有趣的项目。我发现TensorFlow 2比以前的版本更容易使用。尽管有几个模型和几个活动部件，调用组件并深入了解网络还是很容易的。我发现在Colab笔记本中进行开发时，记录变量非常有用。我知道一些工程师也会基于变量名和张量形状来建立单元测试，我可能会在更正式的环境中这样做。</p><p id="effc" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">当然，大多数调试都伴随着与数据相关的问题。一个错误是由于不正确地缩小图像，提取了太多的“风格”。本页顶部的照片是一个令人高兴的小意外，尽管当时网络的性能与以前的实现方式不同。另一个错误仅仅是因为我使用了一张非常高分辨率的艺术品照片，而网络实际上把画布的纹理作为风格！</p><p id="aa00" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">我希望你学到了一些关于深度学习和使用TensorFlow2的知识。如果你有兴趣看更多我使用的代码，或者其他与风格转移、深度学习或计算机视觉相关的主题，我在下面提供了一些资源。</p><h1 id="5969" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">额外资源</h1><h2 id="b54b" class="nk kp it bd kq nl nm dn ku nn no dp ky lx np nq lc mb nr ns lg mf nt nu lk iz bi translated">项目</h2><ul class=""><li id="fd6a" class="od oe it lo b lp lq lt lu lx of mb og mf oh mj oi oj ok ol bi translated">本项目colab:<a class="ae nh" href="https://colab.research.google.com/drive/1xp_QU6ppXOoTs4vNcL41QJk0uz_OOP01#scrollTo=XM1bqoGdgCbX&amp;forceEdit=true&amp;sandboxMode=true" rel="noopener ugc nofollow" target="_blank">快速风格转换TensorFlow 2 </a></li><li id="cff0" class="od oe it lo b lp om lt on lx oo mb op mf oq mj oi oj ok ol bi translated">该项目的github Repo:<a class="ae nh" href="https://github.com/altonelli/fast-style-transfer-tf2" rel="noopener ugc nofollow" target="_blank">altonelli，fast-style-transfer-tf2 </a></li></ul><h2 id="0dc4" class="nk kp it bd kq nl nm dn ku nn no dp ky lx np nq lc mb nr ns lg mf nt nu lk iz bi translated">外部的</h2><p id="b45c" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">几年前，我用这两门课程向自己介绍了ML和计算机视觉</p><ul class=""><li id="4841" class="od oe it lo b lp mk lt ml lx or mb os mf ot mj oi oj ok ol bi translated"><a class="ae nh" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">吴恩达在Coursera上的机器学习课程</a></li><li id="56bb" class="od oe it lo b lp om lt on lx oo mb op mf oq mj oi oj ok ol bi translated"><a class="ae nh" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank">斯坦福CS231n:用于视觉识别的卷积神经网络</a></li></ul><p id="ea60" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">除了原始的研究文章之外，这些文章和实现是神经风格转移的重要资源</p><ul class=""><li id="4118" class="od oe it lo b lp mk lt ml lx or mb os mf ot mj oi oj ok ol bi translated"><a class="ae nh" rel="noopener" target="_blank" href="/artistic-style-transfer-b7566a216431">艺术风格转移，媒介上的F D</a></li><li id="c769" class="od oe it lo b lp om lt on lx oo mb op mf oq mj oi oj ok ol bi translated"><a class="ae nh" href="https://www.tensorflow.org/tutorials/generative/style_transfer" rel="noopener ugc nofollow" target="_blank"> TensorFlow关于传统风格转移的教程</a>，在简介中提到</li><li id="2291" class="od oe it lo b lp om lt on lx oo mb op mf oq mj oi oj ok ol bi translated"><a class="ae nh" href="https://github.com/lengstrom/fast-style-transfer" rel="noopener ugc nofollow" target="_blank"> lengstrom，fast-style-transfer，Github </a></li><li id="e838" class="od oe it lo b lp om lt on lx oo mb op mf oq mj oi oj ok ol bi translated"><a class="ae nh" href="https://github.com/hwalsuklee/tensorflow-fast-style-transfer" rel="noopener ugc nofollow" target="_blank"> hwalsuklee，tensorflow-fast-style-transfer，Github </a></li></ul><h1 id="f78c" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">参考</h1><p id="9a1a" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">[1] L. Gatys，A. Ecker，M. Bethge，<a class="ae nh" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">一种艺术风格的神经算法</a>(2015):<a class="ae nh" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1508.06576</a></p><p id="c04c" class="pw-post-body-paragraph lm ln it lo b lp mk lr ls lt ml lv lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">[2]约翰逊，阿拉希，飞飞，<a class="ae nh" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">实时风格转换和超分辨率的感知损失</a>(2016):<a class="ae nh" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1603.08155</a></p></div></div>    
</body>
</html>