<html>
<head>
<title>From a LSTM cell to a Multilayer LSTM Network with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 PyTorch 从 LSTM 细胞到多层 LSTM 网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3?source=collection_archive---------8-----------------------#2020-07-27">https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3?source=collection_archive---------8-----------------------#2020-07-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="59ab" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用 PyTorch 的实际例子理解 LSTM 单元的基础的指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c8483234dae322e545049bba143815a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OOPQQ15DVNGjdXwfrMmT9g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=73353" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><blockquote class="kz la lb"><p id="7c73" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如何只用 LSTMCells 构建多层长短期记忆神经网络？</p></blockquote><p id="063e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">这篇博客的目的是展示 PyTorch 中 LSTMCell 类的实际应用。众所周知，PyTorch 提供了一个 LSTM 类来建立基于 LSTMCells 的多层长短期记忆神经网络。在这篇博客中，我们将通过一个实际的例子来解释如何使用 LSTMCells 手工构建这样一个神经网络。</p><p id="fb18" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">博客组织如下:</p><ul class=""><li id="fd0c" class="mc md it lf b lg lh lj lk lz me ma mf mb mg ly mh mi mj mk bi translated"><strong class="lf iu">简介</strong></li><li id="82dc" class="mc md it lf b lg ml lj mm lz mn ma mo mb mp ly mh mi mj mk bi translated"><strong class="lf iu">LSTM 牢房</strong></li><li id="d0a3" class="mc md it lf b lg ml lj mm lz mn ma mo mb mp ly mh mi mj mk bi translated">来自 PyTorch 的 LSTMCell 类</li><li id="a3d2" class="mc md it lf b lg ml lj mm lz mn ma mo mb mp ly mh mi mj mk bi translated"><strong class="lf iu">多层 LSTM </strong></li></ul><h1 id="3b7d" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">介绍</h1><p id="b263" class="pw-post-body-paragraph lc ld it lf b lg ni ju li lj nj jx ll lz nk lo lp ma nl ls lt mb nm lw lx ly im bi translated">LSTM 网络是一种递归神经网络。众所周知，这些类型的神经网络能够正确处理可以表示为序列的数据，例如文本、音乐、频率、时间序列等。LSTM 架构的一个主要特征是它包含<em class="le">门</em>，门的功能是<em class="le">保存</em>有意义的信息以及<em class="le">忘记</em>无用的数据。然而，重要的是要提到 LSTMs 在训练时往往<em class="le">慢</em>，本质上是因为要更新的参数数量。</p><p id="68ce" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">如果你想深入研究 LSTM 核心，请看看这个惊人的资源:<a class="ae ky" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">了解 LSTM 网络</a>。</p><p id="da26" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">如果你想避开 LSTM 细胞结构的机制和它内部的工作方式，你可以直接跳到 PyTorch 的<strong class="lf iu"> LSTMCell 类或者进入<strong class="lf iu">多层 LSTM。</strong></strong></p><h1 id="61f0" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">LSTM 细胞</h1><p id="8c20" class="pw-post-body-paragraph lc ld it lf b lg ni ju li lj nj jx ll lz nk lo lp ma nl ls lt mb nm lw lx ly im bi translated">首先，让我们了解一下 LSTM 细胞的机制。LSTM 单元主要由<em class="le">遗忘、输入</em>和<em class="le">输出门</em>以及<em class="le">单元状态</em>3 个门组成。那么，让我们来发现<em class="le">“忘记”</em>“输入”<em class="le">“输出”是什么意思。</em>图 1 显示了 LSTM 单元的架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/ba9b46f3f417936f4bcbeb78a5bbcb3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*vj5C2nfpKCnd6rNrjn4vJw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。LSTM 细胞</p></figure><p id="4ce7" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">遗忘门</strong>决定哪些信息是不相关的，不应该考虑。<strong class="lf iu">遗忘门</strong>由之前的隐藏状态<em class="le"> h(t-1) </em>和当前时间步长<em class="le"> x(t) </em>组成，其值由<em class="le"> sigmoid </em>函数过滤，这意味着接近 0 的值将被视为要丢弃的信息，而接近 1 的值被视为值得保留的有用信息。然后，根据<strong class="lf iu">遗忘门</strong>用要保持的信息更新<strong class="lf iu">单元状态</strong>。图 2 突出显示了构建<strong class="lf iu">遗忘门</strong>所遵循的操作。等式 1 显示了<strong class="lf iu">遗忘门</strong>的数学表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/c940d57a86cb5a030607e2d4cee34cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*M80mT55CSr8bvXzlL9qy0w.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。忘记大门</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8134b6759578693b89297ed9c9a45132.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*YfTp4XE-8LMCV1V35Sw2rA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式 1。忘记大门</p></figure><p id="f1ff" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">输入门</strong>决定什么信息应该是单元状态的一部分(LSTM 的存储器)。它由先前的隐藏状态<em class="le"> h(t-1) </em>和当前的时间步<em class="le"> x(t) </em>组成。<strong class="lf iu">输入门</strong>考虑两个函数，第一个函数通过<em class="le"> sigmoid </em>函数过滤先前的隐藏状态和当前的时间步长。第二个通过<em class="le"> tanh </em>函数过滤先前的隐藏状态和当前的时间步长。因此，在更新<strong class="lf iu">单元状态</strong>之前，两个输出(一个来自<em class="le"> sigmoid </em>功能，一个来自<em class="le"> tanh </em>功能)一起操作，其想法是<em class="le"> sigmoid </em>输出将确定来自<em class="le"> tanh </em>输出的哪些信息对于保持<strong class="lf iu">单元状态</strong>是重要的。图 3 突出显示了构建输入门所遵循的操作。等式 2 显示了<strong class="lf iu">输入门的数学表示。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e1bbdf6e7fc08252badc615bffb276dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*HJnfBU5Onljs4p_TawzUWg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3。输入门</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/d34575a52112d7944264de67aa38da32.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*BaUfceTTlHkK0VlQWFV69w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式 2。输入门</p></figure><p id="31d5" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">到目前为止，我们已经讨论了<strong class="lf iu">忘记门</strong>以及<strong class="lf iu">输入门</strong>。现在是时候介绍<strong class="lf iu">单元状态了。</strong></p><p id="7cc6" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">单元状态</strong>被称为 LSTM 的<em class="le">存储器，它由<strong class="lf iu">遗忘门</strong>和<strong class="lf iu">输入门更新。</strong>本质上来说，<strong class="lf iu">遗忘门</strong>定义了不应保存在存储器中的内容，而<strong class="lf iu">输入门</strong>定义了应保存在该存储器中的内容。图 4 突出显示了更新<strong class="lf iu">单元状态的过程。</strong>等式 3 显示了<strong class="lf iu">单元状态的数学表示。</strong></em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/1909937e197cd7a4b99cea7ec0553a59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*fd-LxA5vJ2VWEw6y5eer6w.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4。细胞状态</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b30cd09c24b3a330deb84fc7dcdc08f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*pvpheZ_RLCl0Pl6VHmqj8Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式 3。细胞状态</p></figure><p id="2171" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">最后，我们有<strong class="lf iu">输出门</strong>，其结果产生新的<strong class="lf iu">隐藏状态。</strong><strong class="lf iu">输出门</strong>利用先前的隐藏状态以及当前的时间步长，其值由<em class="le"> sigmoid </em>函数过滤。并行提取当前单元状态，由<em class="le"> tanh </em>函数过滤，两个输出一起操作，以生成新的<strong class="lf iu">隐藏状态。</strong>图 5 突出显示了构建<strong class="lf iu">输出门的过程。</strong>等式 4 和等式 5 分别给出了<strong class="lf iu">输出门</strong>和<strong class="lf iu">隐藏状态</strong>的数学表达式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5d3edc07b978bc7098088110415e4408.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*kyD0Q3zJ2LWwpe-BDcWR3A.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5。输出门和隐藏状态</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/5a335991d2f33bc515e27e110a80da94.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*9rwnNDQ1SHCLIybIyuy8yQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式 4。输出门</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/9c325da570755de778ca4697c186d4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*B46jyeY6HN-MHmXAfbcQZw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式 5。隐藏状态</p></figure><h1 id="afa1" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">PyTorch 的 LSTMCell 类</h1><p id="4f2a" class="pw-post-body-paragraph lc ld it lf b lg ni ju li lj nj jx ll lz nk lo lp ma nl ls lt mb nm lw lx ly im bi translated">到目前为止，我们已经看到了 LSTM 细胞的工作原理及其组成部分。现在是时候看看如何将 PyTorch 的实际输入调整到 LSTMClass 中了。</p><p id="c6f8" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">先来了解一下初始化 LSTMClass 时的输入输出参数是怎样的，以及使用初始化对象时的输入输出参数是怎样的。</p><p id="a345" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">为了被初始化，LSTMCell 类需要两个重要的参数</p><ul class=""><li id="24fc" class="mc md it lf b lg lh lj lk lz me ma mf mb mg ly mh mi mj mk bi translated"><strong class="lf iu"> input_size: </strong>指每个时间步的特征数量</li><li id="cdba" class="mc md it lf b lg ml lj mm lz mn ma mo mb mp ly mh mi mj mk bi translated"><strong class="lf iu"> hidden_size: </strong>指的是在 LSTM 单元内的每个函数中要学习的参数的数量(即，它是每个<em class="le"> W </em>向量的大小，该向量复合了每个门以及单元和隐藏状态)</li></ul><p id="d724" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">初始化后，LSTMClass 创建的对象接收三个输入:</p><ul class=""><li id="2e94" class="mc md it lf b lg lh lj lk lz me ma mf mb mg ly mh mi mj mk bi translated"><strong class="lf iu">输入</strong>:包含待学习特征的张量。这个张量有一个形状(<em class="le"> batch </em>，<em class="le"> input_size </em>)，其中<em class="le"> batch </em>指的是批量大小，<em class="le"> input_size </em>指的是每个时间步包含的特征个数。</li><li id="aac4" class="mc md it lf b lg ml lj mm lz mn ma mo mb mp ly mh mi mj mk bi translated"><strong class="lf iu"> h_0 </strong>:张量，将存储学习到的<strong class="lf iu">隐藏状态</strong>。这个张量有一个形状(<em class="le">批量，</em>T40】隐藏 _ 大小</li><li id="51d9" class="mc md it lf b lg ml lj mm lz mn ma mo mb mp ly mh mi mj mk bi translated"><strong class="lf iu"> c_0: </strong>张量，它将存储<strong class="lf iu">单元状态</strong>(LSTM 的存储器)。这个张量有一个形状(<em class="le">批量</em>，<em class="le">隐藏 _ 大小</em>)。</li></ul><p id="8f23" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">图 6 突出显示了<strong class="lf iu">输入</strong><em class="le">【t】</em><strong class="lf iu">隐藏状态</strong><em class="le">(t-1)</em>和<em class="le"> h(t) </em>以及<strong class="lf iu">单元格状态</strong><em class="le">(t-1)</em>和<em class="le"> C(t) </em>的矢量形状。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/e9a37525b1d6768ab9e8e778cf95fe8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*84MseqCgGpdvAWf6YBjGgA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6。突出显示矢量形状。</p></figure><p id="94b9" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">正如我们已经知道的，LSTMs 处理顺序数据。所以让我们用一个简单的例子来理解如何顺序地给 LSTMCell 馈电(即如何展开 LSTM 网络)。</p><p id="1011" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">假设我们有一个句子<em class="le">“天空是蓝色的”，</em>我们想把它引入 LSTM。因此，我们需要将这些<em class="le">单词</em>转换成神经网络可读的格式。我不得不说，为了将单词转换成一种<em class="le"> LSTM 可读的格式</em> t，有很多选择，因为这些选择超出了本博客的范围，我将限制自己选择一个最常见的选择，那就是<em class="le">单词标记化</em>。</p><p id="4e1e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">因此，为了将单词编码成 LSTM 可读的格式，我们首先需要将句子分割成一组单词(<em class="le">单词标记)</em>，然后我们需要将每个<em class="le">单词标记</em>转换成<em class="le">索引标记形式。</em>之后，我们需要使用一个嵌入器层，以便将每个<em class="le">索引令牌</em>转换成一个嵌入向量。至此，我们已经拥有了为 LSTM 提供数据所需的一切，我们只需要将序列组织成一组时间步长，其中每个时间步长都将被分配一个批处理大小。</p><p id="75b6" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">假设我们想要创建一组 2 个时间步长，其中每个时间步长包含 2 个批量大小，我们将有 2 个输入张量，每个张量都有一个形状(<em class="le"> batch_size = 2 </em>，<em class="le"> input_size = 2 </em>)，其中<em class="le"> batch_size </em>指的是每个时间步长的样本数，<em class="le"> input_size </em>指的是嵌入维数。图 7 直观地解释了上述过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/234a35da44e220d0db53543e97db5cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DuGznx_jPUMtl_VMSl7ZBQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 7。从原始句子到张量格式的可视化表示。</p></figure><p id="15e4" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">因此，为了向 LSTM 网络提供顺序数据，我们需要创建一个循环，其中每次迭代都将向当前 LSTM 单元提供一个具有 shape ( <em class="le"> batch_size </em>，<em class="le"> input_size </em>)的时间步长。因此，就前面的例子而言，每个时间步长将包含批量大小 2 和输入大小 2。在图 8 中，上面的图像通过使用基于单词标记的表示来表示展开的版本，中间的图像通过使用基于索引标记的表示来表示展开的版本，按钮图像通过使用基于嵌入的表示来表示展开的版本。(三个图像是等效的，顶部和中间的图像仅用于说明)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/6515419b46555908532f6d6a2dff6a85.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*5lNNGl6MYiBjmXXt0Xp70g.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 8。展开的版本</p></figure><p id="2389" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">是时候看看代码了。在 PyTorch 中，建议在构造函数中定义设置和层。因此，就上面提到的玩具示例而言，代码片段 1 显示了初始化应该如何进行。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 1。构造器</p></figure><p id="e612" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">值得一提的是两个重要方面，第一个是嵌入层作为输入参数接收的<em class="le"> input_size </em>，注意它指的是词汇量，在这个玩具示例中我们只有 4 个单词，这意味着词汇量是 4。第二个重要的方面是 LSTMCell 接收的输入大小参数，注意它的大小是<em class="le"> embedding_dim </em>，在本例中是 2(与上面的例子完全一样)。</p><p id="6493" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">一旦我们准备好了构造函数，我们需要在<em class="le">转发函数中工作。</em>首先，我们需要创建并初始化<strong class="lf iu">单元格状态</strong>以及<strong class="lf iu">隐藏状态</strong>，例如:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 2。单元格和隐藏状态初始化</p></figure><p id="db12" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">在第 3 行和第 4 行中，<strong class="lf iu">隐藏的</strong>和<strong class="lf iu">单元格状态</strong>分别被初始化，两者都有一个形状(<em class="le"> batch_size </em>，<em class="le"> hidden_dim </em>)。紧接着在第 7 行和第 8 行，两种状态都被初始化(在这种情况下，我使用 X <em class="le">而不是普通的初始化</em>，不过你可以选择你认为合适的那个)。</p><p id="e936" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">一旦我们初始化了<strong class="lf iu">隐藏的</strong>和<strong class="lf iu">单元格状态</strong>，我们就可以将每个索引令牌转换成基于嵌入的表示，例如:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 3。从索引标记到嵌入张量</p></figure><p id="6201" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">这里，嵌入层接收包含索引标记的张量作为输入，因此<em class="le"> out </em>变量被赋予具有形状的嵌入值的张量(<em class="le"> batch_size，embedding_dim)。</em></p><p id="52d5" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在，一切都准备好了，以饲料 LSTM，然而在这样做之前，我们需要适应的形状<em class="le">出</em>张量。我们只需要将第一维定义为时间步长的数量，因此我们继续执行以下操作:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 4。重塑张量</p></figure><p id="dc4d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">所以现在我们的<em class="le"> out </em>张量有了一个形状(<em class="le"> sequence_len，batch_size，embedding_dim)。</em>好了，是时候展开 LSTM 网络了:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 5。展现 LSTM</p></figure><p id="b488" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">正如我们所观察到的，在循环中，我们用当前时间步长(即<em class="le"> out[i】)来填充 LSTM 单元。</em>同样，通过该循环，<strong class="lf iu">隐藏状态</strong>和<strong class="lf iu">单元状态</strong>被更新“I”次，其中“I”表示时间步长为时间“I”。</p><p id="187a" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">那么现在让我们来看看完整的<em class="le">前进函数:</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 6。正向功能</p></figure><h1 id="5806" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">多层 LSTM</h1><p id="750f" class="pw-post-body-paragraph lc ld it lf b lg ni ju li lj nj jx ll lz nk lo lp ma nl ls lt mb nm lw lx ly im bi translated">现在，如果你想堆叠 LSTM 细胞，以建立一个多层 LSTM？图 9 显示了一个 2 层 LSTM 网络的简单架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a51ac68ad4613e320db9d6490971a220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*UCiibKij5-kHP__Igb2_1Q.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 9。多层 LSTM</p></figure><p id="130e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们首先需要做的是在构造函数中初始化第二个单元(如果你想构建一个“n”堆叠的 LSTM 网络，你需要初始化“n”个 LSTMCell)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 7。细胞初始化</p></figure><p id="7e0a" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">因此，我们需要为每个 LSTM 层初始化<strong class="lf iu">隐藏</strong>和<strong class="lf iu">单元状态</strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 8。权重初始化</p></figure><p id="0d57" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在是时候养活 LSTM 网络了。第一层将接收包含每个单词的每个标记的嵌入版本的输入张量。第二层将在时间步长“t”接收 LSTMCell 的<strong class="lf iu">隐藏状态</strong>作为输入。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 9。展开 LSTM 网络</p></figure><p id="1ebf" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">所以基本上就是这样！</p><p id="531e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">紧接着，如果你想使用 LSTM 网络的最后一个<strong class="lf iu">隐藏状态</strong>，你应该使用最后更新版本的<strong class="lf iu">隐藏状态层 2。</strong></p><p id="df40" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">因此，<em class="le"> forward </em>函数应该是这样的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段 10。正向功能多层 LSTM</p></figure><h1 id="cbcf" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">结论</h1><p id="f039" class="pw-post-body-paragraph lc ld it lf b lg ni ju li lj nj jx ll lz nk lo lp ma nl ls lt mb nm lw lx ly im bi translated">这篇博客解释了 LSTM 网络背后的数学原理，特别是 LSTM 细胞是如何工作的。同样，还解释了如何实现 LSTMCell 类以及如何准备数据，以便仅使用 LSTM 单元构建 LSTM 神经网络和多层 LSTM 网络。</p><p id="3b35" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">了解幕后的机制总是很重要的，这次我们分解了 LSTM 的组件，以及这些组件是如何用当今最流行和强大的深度学习框架之一 PyTorch 实现的。</p></div></div>    
</body>
</html>