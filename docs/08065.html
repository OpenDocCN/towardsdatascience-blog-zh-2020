<html>
<head>
<title>So Model Predictive Control is this Enticing for Model-Based RL?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">那么模型预测控制对基于模型的强化学习有吸引力吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/so-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9?source=collection_archive---------30-----------------------#2020-06-14">https://towardsdatascience.com/so-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9?source=collection_archive---------30-----------------------#2020-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eaf8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">模型预测控制如何完成任何基于模型的强化学习图片的彩色视图</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://towardsdatascience.com/so-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9"><div class="gh gi ki"><img src="../Images/8b0bcd790593bc08da076e72e7ae5d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KYLBPIvw3329uSpT.jpg"/></div></a><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片由<a class="ae ku" href="https://pixabay.com/users/sasint-3639875/" rel="noopener ugc nofollow" target="_blank"> Sasint </a>在<a class="ae ku" href="https://pixabay.com/photos/children-river-water-the-bath-1822704/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>上拍摄</p></figure><p id="8f21" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">在基于模型的强化学习中，控制有什么好大惊小怪的？</strong></p><p id="6dcc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在基于模型的强化学习中，我们并不试图学习告诉我们立即采取行动的技巧。我们没有一个状态到一个愉快动作的映射。通常，我们会推迟这一步，然后再想办法选择最佳行动。</p><p id="30f1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们后来如何决定要采取的行动，也取决于我们有多大的雄心，这就是计划和控制吸引人的地方。这就是模型预测控制(MPC)如此有用的原因。让我们看看结果如何。</p><p id="0d71" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> RL目标</strong></p><p id="9cb9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">强化学习(RL)的目标看起来是这样的:一个处于状态<em class="lr"> s </em> ₜ的代理选择一个动作<em class="lr"> aₜ </em>，收到一个奖励<em class="lr">rₜ</em>，并转移到下一个状态<em class="lr"> s </em> ₜ₊₁.很简单。这发生在每一个时间步——因此在一些未知的动态下,<em class="lr"> t — </em>,意味着代理不知道影响它转换到哪个状态的所有因素。强化学习的简单目标是采取行动，最大化未来回报的总和。</p><p id="9ec0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">使用基于模型的RL，我们尝试学习转换动力学。我们马上就会明白这意味着什么。</p><p id="fdf7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">但是首先，让我们为无模型RL </strong>中的比较设置一点基础</p><p id="3ab5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在无模型RL中，我们不试图学习转换动力学。取而代之的是，我们学习一个复杂的函数<em class="lr">π(at，st) </em>，称为策略<em class="lr"> </em>，在每个时间步给出最优的行动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ls"><img src="../Images/7b5d3aa787ccec202552ec3434493897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N4ez_1tGOZP6zculMjHrzg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><em class="lx">无模型RL的图像表示(</em><a class="ae ku" href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="noopener ugc nofollow" target="_blank"><em class="lx">cs 285</em></a><em class="lx">)</em></p></figure><p id="0c37" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如上图所示，参数为θ的神经网络代表我们的策略。我们给它一个状态<em class="lr"> s </em>，它输出一个动作<em class="lr"> a </em>。<br/>网络<em class="lr">将状态映射到动作</em>。代理在世界中执行这个动作，世界从以当前状态s和采取的动作a为条件的分布中输出下一个状态s’。这个<a class="ae ku" href="https://en.wikipedia.org/wiki/Conditional_probability_distribution" rel="noopener ugc nofollow" target="_blank">条件分布</a>就是所谓的转换或系统动态<em class="lr"> p(s'| s，a) </em>。它表示影响代理转换到的状态的环境因素。</p><p id="8876" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">然而，在基于模型的强化学习</strong>中，我们学习过渡动态<em class="lr"> p(s </em> ₜ₊₁ <em class="lr"> | s </em> ₜ <em class="lr">，a </em> ₜ <em class="lr"> ) </em>，然后弄清楚如何选择动作。由<em class="lr"> θ参数化的函数<em class="lr"> fθ </em>代表这些动态</em>。(*这可以是一个深度神经网络，权重<em class="lr"> θ) </em>。因此，我们学习函数<em class="lr"> fθ (s </em> ₜ <em class="lr">，a </em> ₜ <em class="lr">)，</em>，该函数将采用状态<em class="lr"> s </em> ₜ <em class="lr"> </em>和动作<em class="lr"> a </em> ₜ，并预测在时间<em class="lr">t+δt .<br/></em>的下一个状态。该函数输出我称之为<em class="lr">的灰姑娘状态</em>。请注意我们如何表示这种状态的时间的变化。现在是<em class="lr">t+δt</em>代替了<em class="lr"> t + 1。有这个必要吗？</em></p><p id="825c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="lr">(*在RL中，我们将神经网络称为函数，而不是模型，</em> <a class="ae ku" href="https://ai.stackexchange.com/a/6733" rel="noopener ugc nofollow" target="_blank"> <em class="lr">以避免歧义</em> </a> <em class="lr">。在这种情况下，动态函数是一个深度神经网络。)</em></p><p id="f8d9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">动力学功能</strong></p><p id="6b23" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们的函数取当前状态<em class="lr">s</em>ₜ<em class="lr">T50】和动作<em class="lr">a</em>ₜ<em class="lr">t54】并预测下一个状态<em class="lr"> sₜ+₁ </em>会不会是个坏主意？不完全是。灰姑娘国家更可爱。好的，这里有一个警告——想象一下美国的ₜ和ₜ₊₁看起来很相似。这将意味着我们在ₜ采取的行动对ₜ的产量几乎没有影响。这对我们的动力学函数来说是个坏消息，因为它会发现很难从两个状态之间的差异来推断潜在的系统动力学。当状态之间的时间差<em class="lr">δt</em>小时，这更明显。</em></em></p><p id="a66a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了解决这个问题，我们的动态函数是这样做的:</p><p id="b378" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="lr">ŝ</em>ₜ₊₁<em class="lr">= s</em>ₜ<em class="lr">+fθ(s</em>ₜ<em class="lr">，a </em> ₜ <em class="lr"> ) </em></p><p id="b107" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">它估计当前状态<em class="lr"> s </em> ₜ.将发生的变化<br/> <em class="lr"> ŝt </em> ₊ <em class="lr"> 1 </em>是我们的灰姑娘状态，预期的下一个状态，现在表示为当前状态和这个估计变化的总和。很棒吧。</p><p id="d36f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">因此，通过预测状态<em class="lr">s</em>ₜ<em class="lr">T91】随时间步长持续时间<em class="lr">δt，</em>的变化，我们使药丸更容易吞下。</em></p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h2 id="7530" class="mf mg it bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated"><strong class="ak">进入模型预测控制</strong></h2><p id="9b3c" class="pw-post-body-paragraph kv kw it kx b ky my ju la lb mz jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated">这个术语乍一看似乎很复杂，但简化后，它只是这三个简单的概念精心粘合在一起:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nd"><img src="../Images/d2d6e07680d33888464035cc261b43de.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/0*TRd3L9K8s5cgSyAV.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://xkcd.com/1616/" rel="noopener ugc nofollow" target="_blank"> XKCD午餐</a></p></figure><ul class=""><li id="9c89" class="ne nf it kx b ky kz lb lc le ng li nh lm ni lq nj nk nl nm bi translated"><strong class="kx iu">控制</strong>:在基于模型的RL中，控制就是约束代理在我们认为可接受的行为范围内行动。例如，如果你的家庭机器人正在学习制作玉米片，一个可以接受的行为可能是让它们特别脆。</li><li id="ee5f" class="ne nf it kx b ky nn lb no le np li nq lm nr lq nj nk nl nm bi translated"><strong class="kx iu">预测</strong>:动态函数估计灰姑娘状态。这些灰姑娘的估计有助于改善当前的控制策略。这是关键——我们将看到为什么以及如何完成。</li><li id="38c9" class="ne nf it kx b ky nn lb no le np li nq lm nr lq nj nk nl nm bi translated"><strong class="kx iu">模型</strong>:没有系统的模型就没有控制。MPC是基于模型的。控制策略需要<em class="lr">规划</em>通过模型<em class="lr"> fθ (s </em> ₜ <em class="lr">，a </em> ₜ <em class="lr"> ) </em>。</li></ul><p id="c395" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">基于模型的RL中的规划发生在开放循环中。开环规划到底是什么？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ns"><img src="../Images/431a2f61316125ee90c6178740ab4ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1PEssnRTFmYQ19WaSHF6iA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><em class="lx">基于模型的RL中开环规划的说明。(</em><a class="ae ku" href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="noopener ugc nofollow" target="_blank"><em class="lx"/></a><em class="lx">)</em></p></figure><p id="4621" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">看看上面那个微笑的机器人。它访问世界的第一个状态<em class="lr"> s </em> ₁ <em class="lr"> </em>，使用该状态来描绘世界的其余部分是什么样子，并<em class="lr">计划</em>采取哪些行动。它只在<em class="lr">s</em>₁<em class="lr">t33】与世界交互一次，然后生成一系列动作<em class="lr">【a</em>₁<em class="lr">，a2，…at】</em>直到有限时间步<em class="lr"> T </em>。，它承诺这些行动。假设它对世界的想象是完美的，(即一个正确的模型)，那么我们可以期待它表现良好。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/5f32b45fd8bc18ad59dd4ecf75513a66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*5Vm9ZL1RRMy2ZZMu-M5zZA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><em class="lx">无模型RL中闭环规划的图解。对于每个状态，代理必须选择一个动作，然后查询下一个状态。(</em><a class="ae ku" href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="noopener ugc nofollow" target="_blank"><em class="lx">cs 285</em></a><em class="lx">)</em></p></figure><p id="bfde" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">但这也是我们照片的问题所在。我相信你也看到了。如果我们对模型的描述是不正确的——并且我们计划在看不见的状态下采取行动，那该怎么办？</p><h2 id="b7e5" class="mf mg it bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">这里有一个很傻的故事可以帮助我们直观地理解这一点。</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nu"><img src="../Images/23c97c54a24b12abf5d28a86d39c076d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*o6Y4SS5ByP-pNCb6"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">一些鸡蛋即将成为美食…照片由<a class="ae ku" href="https://unsplash.com/@tengyart?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">腾雅特</a>在<a class="ae ku" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ccb5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">艾莎是一个喜欢厨房游戏的朋友。她邀请我们参加她的一个巧妙的娱乐活动。我们要做<em class="lr"> mahamri </em> ( <em class="lr">斯瓦希里</em>甜甜圈)。她会提前告诉我们，我们会想出一些食谱，并且很有信心能够完成。但游戏是这样的——在她的厨房里，我们要用的原料都装在同样的不锈钢容器里，上面有大大的字母标签。没有人知道容器中的成分。</p><p id="eec0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">所以我们的任务是写下一份食谱，用我们预测的容器标签来标识每种成分，并坚持按照食谱来做。例如，我们认为面粉在容器b中。在配方的某个步骤中，我们注意到:</p><blockquote class="nv nw nx"><p id="e4aa" class="kv kw lr kx b ky kz ju la lb lc jx ld ny lf lg lh nz lj lk ll oa ln lo lp lq im bi translated">“要两杯B，加一勺S”。</p></blockquote><p id="ecdc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们希望S有盐。</p><p id="dcf0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们不允许查看容器内部，不管配方是什么，我们都要坚持到底。但是想象一下，如果B不是面粉，而是黑胡椒。还说艾莎有点狠心，扔了一些我们不需要的成分的容器，那S就是婴儿粉。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ki"><img src="../Images/d9822b53acd71eb3ecdccf4bc16e585f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TXi5vKe9RLdeG3rG.jpg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">一些烘烤的美味佳肴。图片由<a class="ae ku" href="https://pixabay.com/users/la-fontaine-22289/" rel="noopener ugc nofollow" target="_blank">拉封丹</a>在<a class="ae ku" href="https://pixabay.com/photos/cream-puffs-delicious-427181/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>拍摄</p></figure><p id="44d1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">哎呀。美味的mahamri 到此为止。</p><p id="98f6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这是一个在看不见的状态下计划行动的例子。我们的食谱步骤就是行动。容器中的成分以及混合它们的结果就是状态，而模型就是我们解释的每个标签所包含的东西。如果我们的模型不正确，无论计划(配方)有多好，我们都不会有<em class="lr"> mahamri </em>。</p><p id="030b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们现在可以理解开环图的完整草图了。看起来是这样的:</p><ol class=""><li id="ec0e" class="ne nf it kx b ky kz lb lc le ng li nh lm ni lq ob nk nl nm bi translated">收集数据(s，a，s’)。我们不担心这些数据来自哪里，因为代理了解到<a class="ae ku" href="https://stats.stackexchange.com/q/184657" rel="noopener ugc nofollow" target="_blank">不符合政策</a>。</li><li id="7569" class="ne nf it kx b ky nn lb no le np li nq lm nr lq ob nk nl nm bi translated">学习模型<em class="lr"> fθ (s </em> ₜ <em class="lr">，a </em> ₜ <em class="lr">)。</em>这是通过最小化灰姑娘状态预测和观察到的状态<em class="lr">s’</em>之间的成本。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/c502b3a01afebdf462eac4f93973adeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*HxYFp1BKKZuQXyghR-aY0g.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">模型成本函数。这是函数估计和观察到的状态变化之间的差异</p></figure><blockquote class="nv nw nx"><p id="a20d" class="kv kw lr kx b ky kz ju la lb lc jx ld ny lf lg lh nz lj lk ll oa ln lo lp lq im bi translated">在Aisha的游戏中，灰姑娘状态是指每当你在你的食谱中采取一个步骤时，你期望你的配料混合物发生的变化。“s”是观察到的实际变化。</p></blockquote><p id="c4f2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">3.通过学习到的模型进行规划，选择一系列行动。</p><blockquote class="nv nw nx"><p id="5cac" class="kv kw lr kx b ky kz ju la lb lc jx ld ny lf lg lh nz lj lk ll oa ln lo lp lq im bi translated">记得注意“<em class="it">我们把“</em>动作选择推迟到以后——所以模型是先学的。</p></blockquote><p id="d3d3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">大多数基于模型的RL算法只是为了让上面的草图更有范儿。</p><p id="c704" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">您可能会注意到，我们用来训练模型<em class="lr"> fθ </em>的数据和模型运行的实际策略之间存在分布不匹配。这是一个奇怪的问题，因为我们刚刚看到，从哪里获得训练数据并不重要。</p><p id="3ddf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们现在不得不面对的问题是:</p><ol class=""><li id="2bd2" class="ne nf it kx b ky kz lb lc le ng li nh lm ni lq ob nk nl nm bi translated">隐形状态下的规划</li><li id="520a" class="ne nf it kx b ky nn lb no le np li nq lm nr lq ob nk nl nm bi translated">状态分布不匹配。</li></ol></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h2 id="0178" class="mf mg it bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">解决MPC的问题</h2><p id="143d" class="pw-post-body-paragraph kv kw it kx b ky my ju la lb mz jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated">为了减少状态<a class="ae ku" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">分布的差异</a>，我们将从执行a’中观察到的状态s’附加到从初始策略收集的数据中。代理然后在收集真实世界的观察和重新训练动态函数之间交替。</p><p id="16b7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">闭环计划不会在看不见的状态下进行计划。但它们需要数百万个样本来训练——在RL中收集训练数据可能是一件痛苦的事情。基于模型的RL的好处是采样效率。因此，我们希望保留这一点，减少规划问题。我们通过在有限的时间范围内优化行动来解决这个问题<em class="lr"/>ₜ<em class="lr">…at】。我们用MPC来做这件事。</em></p><p id="2861" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们在优化中解决了什么？很可能，我们的模型在第一次拍摄时不会是完美的。当事实证明是这样时，我们希望选择更好的行动。我们优化动作序列，同时使用学习模型<em class="lr"> fθ (st，at) </em>来预测灰姑娘状态。我们可以把这个优化问题描述为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/0505c07fe3e9adc3fa239c685c19b6a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*mb3Ke1hvhQyTeWDtvFe3_w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><em class="lx">优化功能[</em><a class="ae ku" href="https://arxiv.org/abs/1708.02596" rel="noopener ugc nofollow" target="_blank"><em class="lx">4</em></a><em class="lx">]</em></p></figure><p id="737d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们几乎被这个等式吓到了。这意味着我们正在选择能产生最佳累积回报的行动顺序。<strong class="kx iu"> A </strong> ⁽ᴴ⁾ₜ <em class="lr"> </em>代表一系列动作(aₜ，aₜ₊₁ …，aₜ₊H₋₁)。</p><p id="e167" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">代理不是绑定到最初计划的动作，而是采取序列中的第一个动作<em class="lr">a</em>ₜ<em class="lr">T25】<em class="lr">a</em>ₜ<em class="lr">…at</em>。它根据更新的状态信息生成另一个序列[<em class="lr">a</em>ₜ<em class="lr">+1…at</em>。该序列表示大小为<em class="lr"> T — t </em>的向量，包括从步骤<em class="lr"> t </em>到<em class="lr">T</em>的有限时间范围内为状态规划的动作。该序列在每一步被更新并变得更小。</em></p><p id="544d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了得到上述函数的最优序列，我们可以随机选择<em class="lr"> K个</em>序列，选择期望报酬最高的一个。这叫乱射。有<a class="ae ku" href="https://arxiv.org/pdf/1909.11652.pdf" rel="noopener ugc nofollow" target="_blank">更好的</a> <a class="ae ku" href="https://www.aaai.org/Papers/ICML/2003/ICML03-068.pdf" rel="noopener ugc nofollow" target="_blank">方式</a>的动作选择，但这是一个简单的方法。</p><blockquote class="nv nw nx"><p id="f6e7" class="kv kw lr kx b ky kz ju la lb lc jx ld ny lf lg lh nz lj lk ll oa ln lo lp lq im bi translated"><strong class="kx iu">这在艾莎的游戏里会有用吗？</strong></p><p id="0e94" class="kv kw lr kx b ky kz ju la lb lc jx ld ny lf lg lh nz lj lk ll oa ln lo lp lq im bi translated"><em class="it">是的。我们通过生成新的配方步骤来优化我们的行动。对于当前食谱中的每一条指令，我们都知道所选容器中的实际成分。然后我们用这些更新的信息改进食谱。但这很难算是一场游戏，不是吗？</em></p></blockquote></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="02d0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">总之</strong>，MPC是可取的，因为它通过在有限的时间范围内规划未来来防止模型误差的累积。更重要的是，这确保了每个计划不需要完美，因为重新规划保证了改进。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="a7c2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">来源</p><p id="c768" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[1] A. Nagabandi，G. Kahn，R. S. Fearing，S. Levine，<a class="ae ku" href="https://arxiv.org/abs/1708.02596" rel="noopener ugc nofollow" target="_blank">无模型微调的基于模型的深度强化学习的神经网络动力学</a> (2018)，<em class="lr"> ICRA 2018 IEEE机器人与自动化国际会议</em>。</p><p id="9094" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[2] <a class="ae ku" href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="noopener ugc nofollow" target="_blank">深RL决策与控制cs 285</a>(2019)<em class="lr">伯克利</em>。</p><p id="5a2b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[3] " <a class="ae ku" href="https://www.researchgate.net/post/what_is_the_difference_between_predictive_model_predictive_control" rel="noopener ugc nofollow" target="_blank">预测与模型预测控制的区别</a> " (2018)，研究门。</p><p id="5bcf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[4] MPC滚动时域控制(2002)，<a class="ae ku" href="https://web.stanford.edu/class/archive/ee/ee392m/ee392m.1034/" rel="noopener ugc nofollow" target="_blank"> <em class="lr">控制工程中的工业ee392 </em> </a> <em class="lr">，斯坦福。</em></p></div></div>    
</body>
</html>