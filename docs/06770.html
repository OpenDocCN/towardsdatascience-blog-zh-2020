<html>
<head>
<title>The Most Complete Guide to pySpark DataFrames</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">pySpark 数据帧最完整的指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-most-complete-guide-to-pyspark-dataframes-2702c343b2e8?source=collection_archive---------0-----------------------#2020-05-27">https://towardsdatascience.com/the-most-complete-guide-to-pyspark-dataframes-2702c343b2e8?source=collection_archive---------0-----------------------#2020-05-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6229104c91a4dd2267dd0df36f5ab2d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5H_fg2gYa8-2Pq6rYB-3fw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://pixabay.com/photos/pieces-of-the-puzzle-items-form-592828/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><div class=""/><div class=""><h2 id="3151" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">包含您可能需要的所有数据框架功能的书签备忘单</h2></div><p id="5e62" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大数据已经成为数据工程的代名词。但是数据工程和数据科学家之间的界限日益模糊。此时此刻，我认为大数据必须是所有数据科学家的必备技能。</p><p id="8197" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">原因:<strong class="la jk"> <em class="lu">每天生成太多数据</em> </strong></p><p id="53a8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这让我们想到了<a class="ae jg" href="https://amzn.to/2JZBgou" rel="noopener ugc nofollow" target="_blank"> Spark </a>，这是处理大数据时最常用的工具之一。</p><p id="6db1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然 Spark 曾经严重依赖<a class="ae jg" rel="noopener" target="_blank" href="/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a"> RDD 操作</a>，但 Spark 现在已经为美国数据科学家提供了一个 DataFrame API。这是为喜欢冒险的人准备的文档。但是，尽管文档很好，但它没有从数据科学家的角度进行解释。它也没有恰当地记录数据科学最常见的用例。</p><p id="b0c4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">在这篇文章中，我将讨论 Spark 的安装、使用 DataFrames 时需要的标准 Spark 功能，以及一些处理不可避免的错误的技巧。</em>T15】</strong></p><p id="4ecb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个帖子会很长。事实上，这是我在 medium 上最长的帖子之一，所以去买杯咖啡吧。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="ad5a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您想跳到某个特定部分，这里还有一个目录:</p><p id="3235" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" href="#52bd" rel="noopener ugc nofollow">安装</a><br/>T3】数据 T5<a class="ae jg" href="#990b" rel="noopener ugc nofollow">1。基本功能</a> <br/> ∘ <a class="ae jg" href="#2a91" rel="noopener ugc nofollow">读取</a> <br/> ∘ <a class="ae jg" href="#5616" rel="noopener ugc nofollow">查看文件中的几行内容</a> <br/> ∘ <a class="ae jg" href="#0b45" rel="noopener ugc nofollow">更改列名</a> <br/> ∘ <a class="ae jg" href="#844d" rel="noopener ugc nofollow">选择列</a> <br/> ∘ <a class="ae jg" href="#a9a2" rel="noopener ugc nofollow">排序</a> <br/> ∘ <a class="ae jg" href="#728b" rel="noopener ugc nofollow">投</a> <br/> ∘ <a class="ae jg" href="#d970" rel="noopener ugc nofollow">筛选</a> <br/> ∘ <a class="ae jg" href="#cb36" rel="noopener ugc nofollow">分组方式</a><br/>∑播/图方加入 <br/> <a class="ae jg" href="#4d56" rel="noopener ugc nofollow"> 3。使用 SQL 与 DataFrames </a> <br/> <a class="ae jg" href="#ba7a" rel="noopener ugc nofollow"> 4。创建新列</a> <br/> ∘ <a class="ae jg" href="#4abd" rel="noopener ugc nofollow">使用 Spark 原生函数</a> <br/> ∘ <a class="ae jg" href="#3b81" rel="noopener ugc nofollow">使用 spark UDF</a><br/>∘<a class="ae jg" href="#5481" rel="noopener ugc nofollow">使用 RDDs </a> <br/> ∘ <a class="ae jg" href="#fe94" rel="noopener ugc nofollow">使用熊猫 UDF </a> <br/> <a class="ae jg" href="#5cfd" rel="noopener ugc nofollow"> 5。火花窗功能</a> <br/> ∘ <a class="ae jg" href="#0ebd" rel="noopener ugc nofollow">排名</a> <br/> ∘ <a class="ae jg" href="#1e71" rel="noopener ugc nofollow">滞后变量</a> <br/> ∘ <a class="ae jg" href="#1d09" rel="noopener ugc nofollow">滚动聚合</a> <br/> <a class="ae jg" href="#accc" rel="noopener ugc nofollow"> 6。枢纽数据框</a> <br/> <a class="ae jg" href="#0ac3" rel="noopener ugc nofollow"> 7。Unpivot/Stack 数据帧</a> <br/> <a class="ae jg" href="#a883" rel="noopener ugc nofollow"> 8。腌制</a> <br/> <a class="ae jg" href="#97e5" rel="noopener ugc nofollow">更多一些技巧和窍门</a> <br/> ∘ <a class="ae jg" href="#79c3" rel="noopener ugc nofollow">缓存</a> <br/> ∘ <a class="ae jg" href="#d2ed" rel="noopener ugc nofollow">从中间步骤保存并加载</a> <br/> ∘ <a class="ae jg" href="#bda6" rel="noopener ugc nofollow">重新分区</a> <br/> ∘ <a class="ae jg" href="#bf3f" rel="noopener ugc nofollow">读取本地的拼花文件</a> <br/> <a class="ae jg" href="#35c2" rel="noopener ugc nofollow">结论</a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="52bd" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">装置</h1><p id="596d" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我正在努力在 Ubuntu 18.04 上安装 Spark，但是步骤应该和 MAC 上的一样。我假设您已经安装了 Anaconda 和 Python3。之后，您只需完成以下步骤:</p><ol class=""><li id="4b56" class="mz na jj la b lb lc le lf lh nb ll nc lp nd lt ne nf ng nh bi translated">从 Apache Spark <a class="ae jg" href="http://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">网站</a>下载 Spark 二进制文件。并点击下载 Spark 链接下载 Spark。</li></ol><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/14a08287c7fb66ccf390ae6a38085407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-xm3faSPHpjQCu8RuHD8bA.png"/></div></div></figure><p id="6da7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.一旦你下载了上面的文件，你可以从解压文件到你的主目录开始。打开终端，输入这些命令。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="7f0d" class="ns md jj no b gy nt nu l nv nw">cd ~<br/>cp Downloads/spark-2.4.5-bin-hadoop2.7.tgz ~<br/>tar -zxvf spark-2.4.5-bin-hadoop2.7.tgz</span></pre><p id="8945" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.检查您的 Java 版本。从 2.4 版本开始，Spark 可以与 Java 8 协同工作。您可以使用终端窗口上的命令<code class="fe nx ny nz no b">java -version</code>来检查您的 Java 版本。</p><p id="7779" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我的机器上有 Java 11，所以我必须在我的终端上运行以下命令来安装并将默认 Java 更改为 Java 8:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="c377" class="ns md jj no b gy nt nu l nv nw">sudo apt install openjdk-8-jdk<br/>sudo update-alternatives --config java</span></pre><p id="8a5b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您需要通过键入选择号来手动选择 Java 版本 8。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/4102b86ee990ccde5b039a2e4eb70edf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YTG1Hpovti8bcb3PFNPfBw.png"/></div></div></figure><p id="fa5e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">重新检查 Java 版本应该会得到类似这样的结果:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/b17d67107ce594effa0147b6e141e84a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_m6cmKerEl4K1YyVY81kcQ.png"/></div></div></figure><p id="fd57" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.编辑您的<code class="fe nx ny nz no b">~/.bashrc</code>文件，并在文件末尾添加以下几行:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="69b7" class="ns md jj no b gy nt nu l nv nw">function pysparknb () <br/>{<br/>#Spark path<br/>SPARK_PATH=~/spark-2.4.5-bin-hadoop2.7</span><span id="da57" class="ns md jj no b gy oc nu l nv nw">export PYSPARK_DRIVER_PYTHON="jupyter"<br/>export PYSPARK_DRIVER_PYTHON_OPTS="notebook"</span><span id="bf8d" class="ns md jj no b gy oc nu l nv nw"># For pyarrow 0.15 users, you have to add the line below or you will get an error while using pandas_udf <br/>export ARROW_PRE_0_15_IPC_FORMAT=1</span><span id="4170" class="ns md jj no b gy oc nu l nv nw"><strong class="no jk"># Change the local[10] to local[numCores in your machine]</strong><br/>$SPARK_PATH/bin/pyspark --master <strong class="no jk">local[10]</strong><br/>}</span></pre><p id="a4a0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">5.来源<code class="fe nx ny nz no b">~/.bashrc</code></p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="8cd8" class="ns md jj no b gy nt nu l nv nw">source <!-- -->~/.bashrc</span></pre><p id="3549" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">6.运行终端中的<code class="fe nx ny nz no b">pysparknb</code>功能，您将能够访问笔记本。你可以打开一个新的笔记本，同时<code class="fe nx ny nz no b">sparkcontext</code>也会自动加载。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="e5aa" class="ns md jj no b gy nt nu l nv nw">pysparknb</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/4151b83fd7cb3a2d8cfa9b4ae0aae83c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2PO5uExGdBF2T2VAMBSxVw.png"/></div></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="6ed5" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">数据</h1><p id="ce58" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">安装完成后，我们可以转到这篇文章更有趣的部分。我将与南韩<a class="ae jg" href="https://www.kaggle.com/kimjihoo/coronavirusdataset" rel="noopener ugc nofollow" target="_blank">新冠肺炎数据科学</a>合作，这是 COVID 在互联网上最详细的数据集之一。</p><p id="b89a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，我将使用这个数据集来展示 Spark 的一些最有用的功能，但这不应该以任何方式被视为对这个惊人数据集的数据探索练习。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oe"><img src="../Images/e2eae6947f5a60894b1e7c63f00dc1e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RVqtYcbfYPdKuU2L.PNG"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://www.kaggle.com/kimjihoo/coronavirusdataset?select=Case.csv" rel="noopener ugc nofollow" target="_blank">卡格尔</a></p></figure><p id="c7d9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将主要使用以下三个表格:</p><ul class=""><li id="31a4" class="mz na jj la b lb lc le lf lh nb ll nc lp nd lt of nf ng nh bi translated">案例</li><li id="14d4" class="mz na jj la b lb og le oh lh oi ll oj lp ok lt of nf ng nh bi translated">地区</li><li id="0b60" class="mz na jj la b lb og le oh lh oi ll oj lp ok lt of nf ng nh bi translated">时间省</li></ul><p id="9d91" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">你可以在</em></strong><a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/tree/master/sparkdf" rel="noopener ugc nofollow" target="_blank"><strong class="la jk"><em class="lu">GitHub</em></strong></a><strong class="la jk"><em class="lu">库中找到所有的代码。</em>T25】</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="990b" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">1.基本功能</h1><h2 id="2a91" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">阅读</h2><p id="dfa2" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们可以从使用 spark.read.load 命令加载数据集中的文件开始。这个命令读取 parquet 文件，这是 spark 的默认文件格式，但是您可以添加参数<code class="fe nx ny nz no b">format</code>来读取。csv 文件使用它。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="c163" class="ns md jj no b gy nt nu l nv nw">cases = spark.read.load("/home/rahul/projects/sparkdf/coronavirusdataset/Case.csv",format="csv", sep=",", inferSchema="true", header="true")</span></pre><h2 id="5616" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">查看文件中的几行</h2><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="feaa" class="ns md jj no b gy nt nu l nv nw">cases.show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/718a63aec323e54c49d43e7969e36bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ktmyd5NDOpjNlgGvaZ8gaA.png"/></div></div></figure><p id="a3f1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这份档案包含按感染传播方式分类的病例。这可能有助于韩国对电晕放电病例的严格追踪。</p><p id="61e4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个文件现在看起来很好，但是有时当我们增加列数时，格式会变得不太好。我注意到下面的技巧有助于在我的笔记本上以熊猫的形式显示。<code class="fe nx ny nz no b">.toPandas()</code>函数将 spark 数据帧转换成更易于显示的 pandas 数据帧。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="84f6" class="ns md jj no b gy nt nu l nv nw">cases.limit(10).toPandas()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/f4158103f4f1253c2dd65aef0b5cea59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FsFK46Nn5A5bqGClBisNyw.png"/></div></div></figure><h2 id="0b45" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">更改列名</h2><p id="fdc5" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">有时我们想改变 Spark 数据框架中的列名。我们可以简单地使用下面的命令来更改单个列:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="58b1" class="ns md jj no b gy nt nu l nv nw">cases = cases.withColumnRenamed("infection_case","infection_source")</span></pre><p id="79d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">或者对于所有列:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="6eb9" class="ns md jj no b gy nt nu l nv nw">cases = cases.toDF(*['case_id', 'province', 'city', 'group', 'infection_case', 'confirmed',<br/>       'latitude', 'longitude'])</span></pre><h2 id="844d" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">选择列</h2><p id="68ea" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们可以使用<code class="fe nx ny nz no b">select</code>关键字选择列的子集。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="41f0" class="ns md jj no b gy nt nu l nv nw">cases = cases.select('province','city','infection_case','confirmed')<br/>cases.show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/6763db39083444acc2a36c6a5b87c846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QKcHoVIlK2GXcH-nBl4jbg.png"/></div></div></figure><h2 id="a9a2" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">分类</h2><p id="571f" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们可以按确诊病例数排序。这里要注意的是，<code class="fe nx ny nz no b">cases</code>数据帧在执行这个命令后不会改变，因为我们没有把它赋给任何变量。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="ad0a" class="ns md jj no b gy nt nu l nv nw">cases.sort("confirmed").show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/1c05b17c0f2116a9c977bee66b3e081b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qGT0xInxQIWty9t0t2XFQ.png"/></div></div></figure><p id="b985" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但这是颠倒的。我们希望在顶部看到最多的案例。我们可以使用<code class="fe nx ny nz no b">F.desc</code>功能来实现:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="cf6f" class="ns md jj no b gy nt nu l nv nw"># descending Sort<br/>from pyspark.sql import functions as F<br/>cases.sort(F.desc("confirmed")).show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/cc51c972aff089e5b0f673a861d6e389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R6aM7HxbFncerkpwWquZhQ.png"/></div></div></figure><p id="2a5b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到，在韩国的一个逻辑区域中，大多数案件都起源于"信川寺教会"。</p><h2 id="728b" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">铸造</h2><p id="bef3" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">虽然我们在这个数据集中没有遇到，但是可能会出现 Pyspark 将 double 作为 integer 或 string 读取的情况，在这种情况下，您可以使用 cast 函数来转换类型。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="545e" class="ns md jj no b gy nt nu l nv nw">from pyspark.sql.types import DoubleType, IntegerType, StringType</span><span id="6915" class="ns md jj no b gy oc nu l nv nw">cases = cases.withColumn('confirmed', F.col('confirmed').cast(<!-- -->IntegerType<!-- -->()))</span><span id="5b06" class="ns md jj no b gy oc nu l nv nw">cases = cases.withColumn('city', F.col('city').cast(StringType()))</span></pre><h2 id="d970" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">过滤器</h2><p id="93ae" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们可以使用 AND(&amp;)、OR(|)和 NOT(~)条件过滤数据框。例如，我们可能希望找出大邱省所有不同的感染病例，其中有 10 个以上的确诊病例。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="4b7e" class="ns md jj no b gy nt nu l nv nw">cases.filter((cases.confirmed&gt;10) &amp; (cases.province=='Daegu')).show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/c63b4e9e1f96ee56230bf37d63202a1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hgJ2QZuDCmNpTg6ust-FVA.png"/></div></div></figure><h2 id="cb36" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">分组依据</h2><p id="09c5" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们也可以使用火花数据框的<code class="fe nx ny nz no b">groupBy</code>功能。与熊猫<code class="fe nx ny nz no b">groupBy</code>非常相似，除了你需要导入<code class="fe nx ny nz no b">pyspark.sql.functions</code>。<a class="ae jg" href="https://people.eecs.berkeley.edu/~jegonzal/pyspark/pyspark.sql.html#module-pyspark.sql.functions" rel="noopener ugc nofollow" target="_blank">这里的</a>是该功能模块可以使用的功能列表。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="1b58" class="ns md jj no b gy nt nu l nv nw">from pyspark.sql import functions as F</span><span id="cfba" class="ns md jj no b gy oc nu l nv nw">cases.groupBy(["province","city"]).agg(F.sum("confirmed") ,F.max("confirmed")).show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/a4c0392906cbc5ef32588d0990376c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cp3SUlUCAKXtqIg-WmBfUQ.png"/></div></div></figure><p id="5bb6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您不喜欢新的列名，您可以在<code class="fe nx ny nz no b">agg</code>命令中使用<code class="fe nx ny nz no b">alias</code>关键字来重命名列。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="2087" class="ns md jj no b gy nt nu l nv nw">cases.groupBy(["province","city"]).agg(<br/>    F.sum("confirmed").alias("TotalConfirmed"),\<br/>    F.max("confirmed").alias("MaxFromOneConfirmedCase")\<br/>    ).show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/a0002ad2b6f4c5fb1c7bc9d6653d0d77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FY498DJ6Zq_85ISVh0Y2Bw.png"/></div></div></figure><h2 id="28ea" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">连接</h2><p id="1488" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">从连接开始，我们需要引入一个 CSV 文件。我们将使用包含地区信息的地区文件，如小学数量、老年人人口比例等。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="7966" class="ns md jj no b gy nt nu l nv nw">regions = spark.read.load("/home/rahul/projects/sparkdf/coronavirusdataset/Region.csv",format="csv", sep=",", inferSchema="true", header="true")<br/>regions.limit(10).toPandas()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/86e7c81a5f2d3e911a5a5fc64a55034b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hUS5-k5chLXW_9y-WYiQwg.png"/></div></div></figure><p id="297f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们希望通过连接两个数据帧在案例文件中获得这些信息。我们可以通过以下方式实现这一点:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="4e7f" class="ns md jj no b gy nt nu l nv nw">cases = cases.join(regions, ['province','city'],how='left')<br/>cases.limit(10).toPandas()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/f0b737ac95ac355523bfa660e1f79fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ms2nehD-DeuKbL28v89tDw.png"/></div></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="c0b5" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">2.广播/地图端连接</h1><p id="3215" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">有时，您可能会遇到这样的情况，需要将一个非常大的表(~1B 行)与一个非常小的表(~ 100–200 行)连接起来。该场景还可能包括增加数据库的大小，如下例所示。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/297afffdbda2c11be2316a1090e0271d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XzZhDG53AP5ejj7ULkO7A.png"/></div></div></figure><p id="9871" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Spark 中有很多这样的操作，您可能希望对一个特定的键应用多个操作。但是假设大表中每个键的数据都很大，就会涉及到大量的数据移动。有时甚至会导致应用程序本身崩溃。在连接如此大的表(假设另一个表很小)时，可以做的一个小优化是在执行连接时将小表广播到每个机器/节点。您可以使用 broadcast 关键字轻松做到这一点。当其他一切都失败的时候，这已经是 Spark 的救命稻草很多次了。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="2fee" class="ns md jj no b gy nt nu l nv nw">from pyspark.sql.functions import broadcast<br/>cases = cases.join(<strong class="no jk">broadcast</strong>(regions), ['province','city'],how='left')</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="4d56" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">3.对数据帧使用 SQL</h1><p id="0efc" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">如果您愿意，也可以对数据框使用 SQL。让我们试着在 cases 表上运行一些 SQL。</p><p id="546d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们首先将 cases 数据帧注册到一个临时表 cases_table 中，在这个表中我们可以运行 SQL 操作。如您所见，SQL select 语句的结果又是一个 Spark 数据帧。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="ddee" class="ns md jj no b gy nt nu l nv nw">cases.registerTempTable('cases_table')<br/>newDF = sqlContext.sql('select * from cases_table where confirmed&gt;100')<br/>newDF.show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/b8aff453a9bac593876e240fcc226291.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mE7MvOsWN_Zy8Pg1B9arMg.png"/></div></div></figure><p id="e13f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我在上面展示了一个最小的例子，但是您可以在上面的查询中使用非常复杂的 SQL 查询，包括 GROUP BY、HAVING 和 ORDER BY 子句以及别名。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ba7a" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">4.创建新列</h1><p id="fc48" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">在 PySpark 数据帧中创建列有多种方法。我会尽量展示其中最有用的。</p><h2 id="4abd" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">使用 Spark 本地函数</h2><p id="a5ae" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">在 PySpark 数据帧中创建新列的最简单的方法是使用内置函数。这是创建新列的最有效的编程方式，所以每当我想做一些列操作时，这是我第一个去的地方。</p><p id="1089" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以使用<code class="fe nx ny nz no b">.withcolumn</code>和 PySpark SQL 函数来创建一个新列。本质上，您可以找到已经使用 Spark 函数实现的字符串函数、日期函数和数学函数。我们的第一个函数是<code class="fe nx ny nz no b">F.col</code>函数，它让我们可以访问列。所以如果我们想给一列加 100，我们可以用<code class="fe nx ny nz no b">F.col</code>作为:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="7549" class="ns md jj no b gy nt nu l nv nw">import pyspark.sql.functions as F<br/>casesWithNewConfirmed = cases.withColumn("NewConfirmed", 100 + F.col("confirmed"))</span><span id="aa81" class="ns md jj no b gy oc nu l nv nw">casesWithNewConfirmed.show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/7af1915f7ab3a71b705f56cbb8e8ad9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iTG_N0x4KRdCQP5avzhBYA.png"/></div></div></figure><p id="abab" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们也可以使用数学函数，如<code class="fe nx ny nz no b">F.exp</code>函数:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="2595" class="ns md jj no b gy nt nu l nv nw">casesWithExpConfirmed = cases.withColumn("ExpConfirmed", F.exp("confirmed"))</span><span id="cf93" class="ns md jj no b gy oc nu l nv nw">casesWithExpConfirmed.show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/f3c1cc926cf49c84f6b67d429656e476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JaIautuqNZT0me3uVKTZJg.png"/></div></div></figure><p id="69a8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个模块还提供了很多其他的功能，对于大多数简单的用例来说已经足够了。你可以点击查看功能列表<a class="ae jg" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="3b81" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">使用 Spark UDFs</h2><p id="5f4d" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">有时我们想对一列或多列做复杂的事情。这可以被认为是将 PySpark 数据帧映射到一列或多列的操作。虽然 Spark SQL 函数确实解决了许多列创建的用例，但每当我需要更成熟的 Python 功能时，我都会使用 Spark UDF。</p><p id="0016" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要使用 Spark UDF，我们需要使用<code class="fe nx ny nz no b">F.udf</code>函数将常规 python 函数转换为 Spark UDF。我们还需要指定函数的返回类型。在这个例子中，返回类型是<code class="fe nx ny nz no b">StringType()</code></p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="c676" class="ns md jj no b gy nt nu l nv nw">import pyspark.sql.functions as F<br/>from pyspark.sql.types import *<br/>def casesHighLow(confirmed):<br/>    if confirmed &lt; 50: <br/>        return 'low'<br/>    else:<br/>        return 'high'<br/>    <br/>#convert to a UDF Function by passing in the function and return type of function<br/>casesHighLowUDF = F.udf(casesHighLow, StringType())</span><span id="2ccd" class="ns md jj no b gy oc nu l nv nw">CasesWithHighLow = cases.withColumn("HighLow", casesHighLowUDF("confirmed"))<br/>CasesWithHighLow.show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/7bf18b03acea1cce9db409884e222597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RcawXm0DOYunfIxnXb7voQ.png"/></div></div></figure><h2 id="5481" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">使用 rdd</h2><p id="530b" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">这可能看起来有点奇怪，但有时 spark UDFs 和 SQL 函数对于特定用例来说是不够的。我观察到 rdd 在现实生活中的一些用例中表现得更好。您可能希望利用 spark RDDs 带来的更好的分区。或者您可能想在 Spark RDDs 中使用组函数。</p><p id="0bdb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不管是哪种情况，我发现这种使用 RDD 创建新列的方式对于有使用 rdd 经验的人来说非常有用，rdd 是 Spark 生态系统中的基本构建块。不懂也不用太担心。它只是在这里完成。</p><p id="7d36" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">下面的过程利用了</em> </strong> <code class="fe nx ny nz no b"><strong class="la jk"><em class="lu">Row</em></strong></code> <strong class="la jk"> <em class="lu">和</em> </strong> <code class="fe nx ny nz no b"><strong class="la jk"><em class="lu">pythondict</em></strong></code> <strong class="la jk"> <em class="lu">对象之间的转换功能。</em> </strong>我们把一个行对象转换成一个字典。像我们习惯的那样使用字典，并将字典转换回 row。这在很多情况下可能会派上用场。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="39fc" class="ns md jj no b gy nt nu l nv nw">import math<br/>from pyspark.sql import Row</span><span id="b931" class="ns md jj no b gy oc nu l nv nw">def rowwise_function(row):<br/>    # convert row to python dictionary:<br/>    row_dict = row.asDict()<br/>    # Add a new key in the dictionary with the new column name and value.<br/>    # This might be a big complex function.<br/>    row_dict['expConfirmed'] = float(np.exp(row_dict['confirmed']))<br/>    # convert dict to row back again:<br/>    newrow = Row(**row_dict)<br/>    # return new row<br/>    return newrow</span><span id="f6c2" class="ns md jj no b gy oc nu l nv nw"># convert cases dataframe to RDD<br/>cases_rdd = cases.rdd</span><span id="17bd" class="ns md jj no b gy oc nu l nv nw"># apply our function to RDD<br/>cases_rdd_new = cases_rdd.map(lambda row: rowwise_function(row))</span><span id="9899" class="ns md jj no b gy oc nu l nv nw"># Convert RDD Back to DataFrame<br/>casesNewDf = sqlContext.createDataFrame(cases_rdd_new)</span><span id="49d5" class="ns md jj no b gy oc nu l nv nw">casesNewDf.show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/de8dcfb9719ee0dd5104d9763ec3ecae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t73b854I0Hdlks5wbQQx_Q.png"/></div></div></figure><h2 id="fe94" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">使用熊猫 UDF</h2><p id="ff35" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">Spark 版本 2.3.1 中引入了这一功能。这就允许你在 Spark 上使用 pandas 的功能。当我必须在 Spark 数据帧上运行 groupBy 操作时，或者当我需要创建滚动特征并希望使用 Pandas 滚动函数/窗口函数而不是 Spark 窗口函数(我们将在本文稍后介绍)时，我通常会使用它。</p><p id="996a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们使用它的方式是通过使用<code class="fe nx ny nz no b">F.pandas_udf</code>装饰器。<strong class="la jk"> <em class="lu">我们这里假设函数的输入将是一个熊猫数据帧。</em> </strong>我们需要从这个函数中依次返回一个熊猫数据帧。</p><p id="c8ad" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里唯一的复杂性是我们必须为输出数据帧提供一个模式。我们可以使用数据帧的原始模式来创建外部模式。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="b22d" class="ns md jj no b gy nt nu l nv nw">cases.printSchema()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/6b85b4b088d2834b6ecaec5e7c637947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*an_O0MpNsOqVMioU1ne67g.png"/></div></div></figure><p id="0c58" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我使用 UDF 熊猫来获得按感染病例分组的标准化确诊病例。这里的主要优势是我可以在 Spark 中处理熊猫数据帧。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="ph pi l"/></div></figure><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pj"><img src="../Images/98e3e8c44e65074c9984cc2e674ee8e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iUOEcEXVNsYtabSnhXXgwA.png"/></div></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="5cfd" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">5.火花窗功能</h1><p id="9443" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">窗口函数本身可以构成一篇完整的博客文章。这里我将谈谈 spark 中一些最重要的窗口功能。</p><p id="b110" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为此，我还将使用另一个数据 CSV，它显示日期，这将有助于更好地理解窗口功能。我将使用包含每个省每日病例信息的时间省数据框架。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/1d90e326e96367d64f28886c40f190c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAennXpCnDel5e8a2aU16A.png"/></div></div></figure><h2 id="0ebd" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">等级</h2><p id="8361" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">使用这个函数，您可以在一个组上获得 rank 和 dense_rank。例如，您可能希望在您的病例表中有一列，该列根据某个省中的 infection_case 数量提供 infection_case 的等级。我们可以通过以下方式做到这一点:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="440e" class="ns md jj no b gy nt nu l nv nw">from pyspark.sql.window import Window</span><span id="03ad" class="ns md jj no b gy oc nu l nv nw">windowSpec = Window().partitionBy(['province']).orderBy(F.desc('confirmed'))</span><span id="5a62" class="ns md jj no b gy oc nu l nv nw">cases.withColumn("rank",F.rank().over(windowSpec)).show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/78e81e7ede82706e1411a372262df2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k0OR_AL_LMq0SKUpHdD7KQ.png"/></div></div></figure><h2 id="1e71" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">滞后变量</h2><p id="2691" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">有时，我们的数据科学模型可能需要基于滞后的功能。例如，一个模型可能包含像上周的价格或前一天的销售量这样的变量。我们可以使用窗口函数的滞后函数来创建这样的特征。在这里，我试图得到 7 天前确诊的病例。我正在过滤以显示结果，因为头几天的电晕案例为零。您可以在这里看到 lag_7 day 特性移动了 7 天。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="f52f" class="ns md jj no b gy nt nu l nv nw">from pyspark.sql.window import Window<br/>windowSpec = Window().partitionBy(['province']).orderBy('date')<br/>timeprovinceWithLag = timeprovince.withColumn("lag_7",F.lag("confirmed", 7).over(windowSpec))</span><span id="3478" class="ns md jj no b gy oc nu l nv nw">timeprovinceWithLag.filter(timeprovinceWithLag.date&gt;'2020-03-10').show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/f4a3320808b51b4910b811c0714a0adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*deE34rlGaCf1lQH1iLhCGA.png"/></div></div></figure><h2 id="1d09" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">滚动聚合</h2><p id="3e59" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">有时，为我们的模型提供滚动平均值会有所帮助。例如，我们可能希望将连续 7 天的销售额总和/平均值作为销售回归模型的一个特性。让我们计算一下过去 7 天确诊病例的移动平均数。这是很多人已经在用这个数据集做的事情，来看真实的趋势。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="0d17" class="ns md jj no b gy nt nu l nv nw">from pyspark.sql.window import Window</span><span id="6665" class="ns md jj no b gy oc nu l nv nw">windowSpec = Window().partitionBy(['province']).orderBy('date')<strong class="no jk">.rowsBetween(-6,0)</strong></span><span id="bad7" class="ns md jj no b gy oc nu l nv nw">timeprovinceWithRoll = timeprovince.withColumn("roll_7_confirmed",F.mean("confirmed").over(windowSpec))</span><span id="eadd" class="ns md jj no b gy oc nu l nv nw">timeprovinceWithRoll.filter(timeprovinceWithLag.date&gt;'2020-03-10').show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/b2c9dcd796af69404bab23cc041ad894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MBW-cxmevqPkA1vd5fbo2Q.png"/></div></div></figure><p id="6a5e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里有几件事需要了解。首先是我们在这里使用的<code class="fe nx ny nz no b"> <strong class="la jk">rowsBetween(-6,0)</strong></code> <strong class="la jk"> </strong>函数。这个函数有一个包含开始和结束的<code class="fe nx ny nz no b">rowsBetween(start,end)</code>形式。使用它，我们只查看特定窗口中的过去 7 天，包括当前日期。这里，0 指定当前行，而-6 指定当前行之前的第七行。记住我们从 0 开始计数。</p><p id="f3ca" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，为了获取<code class="fe nx ny nz no b">roll_7_confirmed</code>的<code class="fe nx ny nz no b">2020–03–22</code>日期，我们查看<code class="fe nx ny nz no b">2020–03–22 to 2020–03–16</code>日期的确诊病例，并取其平均值。</p><p id="1c1e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们使用了<code class="fe nx ny nz no b"><strong class="la jk">rowsBetween(-7,-1)</strong></code> <strong class="la jk"> </strong>，我们将只查看过去 7 天的数据，而不是当前日期。</p><p id="2bbe" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还可以使用<code class="fe nx ny nz no b">rowsBetween(Window.unboundedPreceding, Window.currentRow)</code>,在这里，我们获取窗口中第一行和 current_row 之间的行，以获得运行总数。我在这里计算累计 _ 确认。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="d0ba" class="ns md jj no b gy nt nu l nv nw">from pyspark.sql.window import Window</span><span id="a8ce" class="ns md jj no b gy oc nu l nv nw">windowSpec = Window().partitionBy(['province']).orderBy('date').rowsBetween(Window.unboundedPreceding,Window.currentRow)<br/>timeprovinceWithRoll = timeprovince.withColumn("cumulative_confirmed",F.sum("confirmed").over(windowSpec))<br/>timeprovinceWithRoll.filter(timeprovinceWithLag.date&gt;'2020-03-10').show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pm"><img src="../Images/8f86eeeea2d731dd5a218ed7c7fc99e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-jfjnbRiCpQxOAviCBnlGw.png"/></div></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="accc" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">6.透视数据框架</h1><p id="852f" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">有时我们可能需要平面格式的数据帧。这在电影数据中经常发生，我们可能希望将类型显示为列而不是行。我们可以使用 pivot 来实现这一点。在这里，我试图为每个日期获取一行，并将省份名称作为列。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="1c3b" class="ns md jj no b gy nt nu l nv nw">pivotedTimeprovince = timeprovince.groupBy('date').pivot('province').agg(F.sum('confirmed').alias('confirmed') , F.sum('released').alias('released'))</span><span id="528d" class="ns md jj no b gy oc nu l nv nw">pivotedTimeprovince.limit(10).toPandas()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pn"><img src="../Images/f95285b4ad3b8818f1ddbadf30752abe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qY3QiI116g794I7diW5IfA.png"/></div></div></figure><p id="69a2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里需要注意的一点是，我们需要始终用 pivot 函数提供一个聚合，即使数据只有一个日期行。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="0ac3" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">7.取消透视/堆叠数据帧</h1><p id="0afa" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">这与支点正好相反。给定一个如上的旋转数据框架，我们能回到最初吗？</p><p id="7ce5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">是的，我们可以。但是方法并不简单。首先，我们需要在列名中用<code class="fe nx ny nz no b">_</code>替换<code class="fe nx ny nz no b">-</code>，因为这会干扰我们将要做的事情。我们可以简单地重命名这些列:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="120c" class="ns md jj no b gy nt nu l nv nw">newColnames = [x.replace("-","_") for x in pivotedTimeprovince.columns]</span><span id="ab2a" class="ns md jj no b gy oc nu l nv nw">pivotedTimeprovince = pivotedTimeprovince.toDF(*newColnames)</span></pre><p id="6213" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们需要创建一个如下所示的表达式:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="3e43" class="ns md jj no b gy nt nu l nv nw">"stack(34, 'Busan_confirmed' , Busan_confirmed,'Busan_released' , Busan_released,'Chungcheongbuk_do_confirmed' ,</span><span id="47f0" class="ns md jj no b gy oc nu l nv nw">.<br/>.<br/>.</span><span id="ff00" class="ns md jj no b gy oc nu l nv nw">'Seoul_released' , Seoul_released,'Ulsan_confirmed' , Ulsan_confirmed,'Ulsan_released' , Ulsan_released) as (Type,Value)"</span></pre><p id="97ed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一般格式如下:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="40e3" class="ns md jj no b gy nt nu l nv nw">"stack(&lt;cnt of columns you want to put in one column&gt;, 'firstcolname', firstcolname , 'secondcolname' ,secondcolname ......) as (Type, Value)"</span></pre><p id="270d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这可能看起来令人生畏，但是我们可以使用我们的编程技能创建这样一个表达式。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="c75c" class="ns md jj no b gy nt nu l nv nw">expression = ""<br/>cnt=0<br/>for column in pivotedTimeprovince.columns:<br/>    if column!='date':<br/>        cnt +=1<br/>        expression += f"'{column}' , {column},"</span><span id="225f" class="ns md jj no b gy oc nu l nv nw">expression = f"stack({cnt}, {expression[:-1]}) as (Type,Value)"</span></pre><p id="d8d1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以使用以下方法取消透视:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="bd19" class="ns md jj no b gy nt nu l nv nw">unpivotedTimeprovince = pivotedTimeprovince.select('date',F.expr(exprs))</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/f71d417718a05ac4192042957ee41ab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hu8pEYBD3x-U1UEA3w5Wkw.png"/></div></div></figure><p id="ccbd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">瞧啊。我们得到了垂直格式的数据帧。要获得与以前完全相同的格式，需要进行相当多的列创建、过滤和连接操作，但我不会深入讨论这些。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a883" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">8.盐碱滩</h1><p id="f9fd" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">有时可能会发生这样的情况，大量数据被分配给一个执行器，因为我们的数据中有很多行被分配了相同的键。Salting 是帮助您管理数据偏斜的另一种方式。</p><p id="07ae" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我们想要做加法运算，当我们有倾斜的键时。我们可以从创建 Salted 键开始，然后对该键进行双重聚合，因为 sum 的和仍然等于 sum。为了理解这一点，假设我们需要 cases 表中已确认的 infection_cases 的总和，并假设关键 infection_cases 是偏斜的。我们可以分两步完成所需的操作。</p><p id="7163" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> 1。创建一个加盐键</strong></p><p id="50f2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们首先使用 infection_case 列和 0 到 9 之间的 random_number 的连接创建一个 salting 键。如果你的钥匙更加倾斜，你可以把它分成 10 个以上的部分。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="8b26" class="ns md jj no b gy nt nu l nv nw">cases = cases.withColumn("salt_key", F.concat(F.col("infection_case"), F.lit("_"), F.monotonically_increasing_id() % 10))</span></pre><p id="be8e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是操作后表格的外观:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pp"><img src="../Images/ca6f1cf3b2bc0caf7e2c461f8da8068d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lqeOANtoNPprEAxcpB56NQ.png"/></div></div></figure><p id="4c48" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> 2。salt 键上的第一个 group by</strong></p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="4520" class="ns md jj no b gy nt nu l nv nw">cases_temp = cases.groupBy(["infection_case","salt_key"]).agg(F.sum("confirmed")).show()</span></pre><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/50ad5ca6aefe0f6a4440c57987c9ccf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZ_pQGF4N-8eMDInqF7RHQ.png"/></div></div></figure><p id="933e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> 3。第二组原始密钥</strong></p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pr"><img src="../Images/661f14f60bd001afb29266eba1075ba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMMG9g7hyxKEVZUdqGCPMQ.png"/></div></div></figure><p id="b7e6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里我们看到了 sum 的和是如何被用来得到最终的和的。您还可以利用以下事实:</p><ul class=""><li id="2cec" class="mz na jj la b lb lc le lf lh nb ll nc lp nd lt of nf ng nh bi translated">min 的 min 就是 min</li><li id="7dc4" class="mz na jj la b lb og le oh lh oi ll oj lp ok lt of nf ng nh bi translated">最大的最大就是最大</li><li id="24b5" class="mz na jj la b lb og le oh lh oi ll oj lp ok lt of nf ng nh bi translated">总数就是总数</li></ul><p id="106e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您也可以考虑将加盐作为一种思想应用到连接的方法。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="97e5" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">更多提示和技巧</h1><h2 id="79c3" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">贮藏</h2><p id="9417" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">Spark 的工作原理是懒惰执行。这意味着，除非在数据帧上使用类似于<code class="fe nx ny nz no b">.count()</code>的动作函数，否则什么都不会执行。如果你做一个<code class="fe nx ny nz no b">.count</code>函数，通常在这一步缓存是有帮助的。所以每当我做一个<code class="fe nx ny nz no b">.count()</code>操作的时候，我都会缓存()我的数据帧。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="c903" class="ns md jj no b gy nt nu l nv nw">df.cache().count()</span></pre><h2 id="d2ed" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">从中间步骤保存和加载</h2><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="65fe" class="ns md jj no b gy nt nu l nv nw">df.write.parquet("data/df.parquet")<br/>df.unpersist()<br/>spark.read.load("data/df.parquet")</span></pre><p id="3421" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用 Spark 时，您会经常遇到内存和存储问题。虽然在某些情况下，这些问题可以使用广播、加盐或缓存等技术来解决，但有时只是中断工作流并在关键步骤保存和重新加载整个数据帧对我帮助很大。这有助于 spark 释放大量用于存储中间混洗数据和未使用缓存的内存。</p><h2 id="bda6" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">分配</h2><p id="f504" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">如果在处理所有转换和连接时，您觉得数据有偏差，您可能需要对数据进行重新分区。最简单的方法是使用:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="979c" class="ns md jj no b gy nt nu l nv nw">df = df.repartition(1000)</span></pre><p id="b802" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有时，您可能还希望按照已知的方案进行重新分区，因为该方案可能会在以后被某个连接或聚集操作使用。您可以使用多个列通过以下方式进行重新分区:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="74f2" class="ns md jj no b gy nt nu l nv nw"><em class="lu">df = df.repartition('cola', 'colb','colc','cold')</em></span></pre><p id="6ff7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以使用以下公式获得数据框中的分区数量:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="fccc" class="ns md jj no b gy nt nu l nv nw">df.rdd.getNumPartitions()</span></pre><p id="a60e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您还可以通过使用<code class="fe nx ny nz no b">glom</code>函数来检查分区中记录的分布。这有助于理解在处理各种转换时发生的数据偏差。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="6051" class="ns md jj no b gy nt nu l nv nw"><em class="lu">df.glom().map(len).collect()</em></span></pre><h2 id="bf3f" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">在本地读取拼花文件</h2><p id="1244" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">有时，您可能希望在 Spark 不可用的系统中读取拼花文件。在这种情况下，我通常使用下面的代码:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="a85c" class="ns md jj no b gy nt nu l nv nw">from glob import glob<br/>def load_df_from_parquet(parquet_directory):<br/>   df = pd.DataFrame()<br/>   for file in glob(f"{parquet_directory}/*"):<br/>      df = pd.concat([df,pd.read_parquet(file)])<br/>   return df</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="35c2" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">结论</h1><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ps"><img src="../Images/7d4981a5592f759606d40fab4a93d9e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mL0xSG32jEU-gPkb.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://pixabay.com/photos/dawn-graduates-throwing-hats-dusk-1840298/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="7df1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个很大的帖子，祝贺你到达终点。这些是我在日常工作中最常用的功能。</p><p id="09f3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">希望我已经很好地介绍了 Dataframe 的基础知识，足以激起您的兴趣，并帮助您开始使用 Spark。如果你想了解更多关于 Spark 是如何开始的或者 RDD 的基础知识，看看这个<a class="ae jg" rel="noopener" target="_blank" href="/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a">帖子</a></p><p id="8f6b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">你可以在这里找到所有的代码</em></strong><a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/tree/master/sparkdf" rel="noopener ugc nofollow" target="_blank"><strong class="la jk"><em class="lu">GitHub</em></strong></a><strong class="la jk"><em class="lu">我保存了我所有帖子的代码。</em> </strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="20fb" class="ns md jj bd me ol om dn mi on oo dp mm lh op oq mo ll or os mq lp ot ou ms ov bi translated">继续学习</h2><p id="2a4e" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">此外，如果你想了解更多关于 Spark 和 Spark DataFrames 的信息，我想在 Coursera 上调出<a class="ae jg" href="https://coursera.pxf.io/P0vknj" rel="noopener ugc nofollow" target="_blank">大数据专业</a>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="3705" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我以后也会写更多这样的帖子。让我知道你对这个系列的看法。关注我在<a class="ae jg" href="https://medium.com/@rahul_agarwal" rel="noopener"> <strong class="la jk">媒体</strong> </a>或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter <a class="ae jg" href="https://twitter.com/MLWhiz" rel="noopener ugc nofollow" target="_blank"> @mlwhiz </a>联系。</p><p id="6436" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，一个小小的免责声明——这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p></div></div>    
</body>
</html>