<html>
<head>
<title>Generalized Linear Models Decomposed</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">广义线性模型分解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fitting-glms-by-hand-189c02af33a8?source=collection_archive---------10-----------------------#2020-03-15">https://towardsdatascience.com/fitting-glms-by-hand-189c02af33a8?source=collection_archive---------10-----------------------#2020-03-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="425b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用最大似然法和梯度下降法在 Python 中从头拟合 GLMs</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/70b9f0e46cf82a72269a9afa203db672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J2BuZSs0VO53Fe_jMpduzQ.png"/></div></div></figure><p id="841c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在普通的线性回归中，我们将结果变量视为几个输入变量加上一些随机噪声的线性组合，通常假设为正态分布。虽然这种建模方法易于解释、高效实现，并且能够准确捕捉许多线性关系，但它确实有一些明显的局限性。线性模型的简单扩展，广义线性模型(GLM)能够放宽线性回归的一些最严格的假设。这些假设包括:</p><ol class=""><li id="7fee" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated">结果变量和输入变量之间的线性关系</li><li id="21e1" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">误差项的正态分布</li><li id="c035" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">误差项的恒定方差。</li></ol><p id="6559" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">放宽这些假设允许我们将更灵活的模型应用于更广泛的数据类型。</p><p id="4f6a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">GLMs 可以很容易地用 R 或 Python 这样的语言编写几行代码，但是要理解模型是如何工作的，亲自动手编写代码总是很有帮助的。本文展示了如何仅使用 Python 的 Numpy 包从头实现 GLMs。要了解更多关于 GLMs 的基础和直觉，请查阅<a class="ae me" rel="noopener" target="_blank" href="/generalized-linear-models-9cbf848bb8ab">这篇文章</a>或<a class="ae me" href="https://www.amazon.com/Generalized-Linear-Examples-Springer-Statistics/dp/1441901175" rel="noopener ugc nofollow" target="_blank">这本书</a>。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="6463" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">GLM 结构</h1><p id="b136" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">拟合 GLM 首先需要指定两个组件:我们的结果变量的随机分布和分布的平均参数与其“线性预测器”之间的<em class="nj">链接</em>函数。</p><h2 id="b62b" class="nk mn it bd mo nl nm dn ms nn no dp mw ld np nq my lh nr ns na ll nt nu nc nv bi translated">随机成分</h2><p id="553d" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">构建 GLM 的第一步是确定结果变量的分布。如果数据有二元响应，我们可能要使用伯努利或二项式分布。如果我们正在处理计数数据，泊松模型可能更有用。通常假设这种分布来自于指数分布族，包括二项式、泊松、负二项式、伽玛和正态分布。</p><p id="ba50" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些模型中的每一个都可以用其均值参数𝜇 <em class="nj"> = E(Y) </em>来表示。例如，我们将二项式模型指定为<em class="nj"> Y ~ </em> Bin <em class="nj"> (n，p) </em>，也可以写成<em class="nj"> Y ~ </em> Bin <em class="nj"> (n，</em> 𝜇 <em class="nj"> /n) </em>。一旦我们估计了𝜇，我们就将<em class="nj"> Y </em>建模为来自𝜇̂指数化的分布，我们对<em class="nj"> Y </em>的预测值就是𝜇̂.</p><h2 id="ff57" class="nk mn it bd mo nl nm dn ms nn no dp mw ld np nq my lh nr ns na ll nt nu nc nv bi translated">链接功能</h2><p id="4676" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">在 GLM 中，我们将𝜇估计为“线性预测”𝜂的非线性函数，而后者本身是数据的线性函数。连接𝜇和𝜂的非线性函数被称为<em class="nj">链接</em>函数，我们在模型拟合之前确定它。链接函数被写成𝜇的函数，例如𝜂 <em class="nj"> = g( </em> 𝜇).</p><p id="4b02" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设我们有以下训练数据，其中每个<strong class="kw iu"> x </strong>是一个 D 维向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/3ecee3003a28214b689f4803b76f2e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*XNH3AAVb2m6WZSugtp-2OQ.png"/></div></figure><p id="5a5a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们首先将𝜂写成对于每个观测值 n = 1，…，n 的线性函数<strong class="kw iu"> x </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b5dd5d13eb98235b7ed072ee19d7bb9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*_ihmfEs3G0swh0W8Pjx4yg.png"/></div></figure><p id="7e91" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后我们用链接函数将𝜂和𝜇联系起来:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/9f211d8745c8b784c8bb82fac95e8d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*Qb8rSathWzrEgMKtIaSyAA.png"/></div></figure></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="e8a6" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">安装 GLM</h1><p id="c723" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">为了适合 GLM，我们实际上只是寻找βs 的估计:从这些，我们获得𝜂的估计，这立即导致𝜇的估计，然后给我们一个<em class="nj"> Y 的估计分布！</em>按照以下步骤估算βs:</p><ol class=""><li id="aa78" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated">将 y 的分布指定为𝜇.的函数</li><li id="8852" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">指定链接函数，𝜂 <em class="nj"> = g( </em> 𝜇).</li><li id="6719" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">确定一个损失函数。这里，我们使用负对数似然。我们可以将损失函数分解为每个线性预测值和相应的真值<em class="nj"> Y </em>的函数，如下图所示。</li><li id="fd20" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">通过闭合解或梯度下降法，找出使损失函数最小的β值。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/4eee17299bccda3aa692fa1427c99a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*12TU_Q4aeYAz8M9yMdHvFQ.png"/></div></div></figure></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="de45" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">线性回归——GLM 的特例</h1><p id="a27e" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">为了加强我们对这种结构的理解，让我们首先以 GLM 格式写出一个典型的线性回归模型。作为步骤 1，让我们指定<em class="nj"> Y </em>的分布。回想一下，典型的线性模型假设</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/15c69b86378b5b1e9f61f15d1389e0a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*bdZx5tPG6Vbdg2GGitei4Q.png"/></div></figure><p id="b20c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<strong class="kw iu"> β </strong>是系数的长度为 D 的向量(假设我们已经为每个<strong class="kw iu"> x </strong>加了 1，因此<strong class="kw iu"> β </strong>中的第一个元素是截距项)。请注意，该分布的平均值是数据的线性组合，这意味着我们可以通过以下等式根据线性预测值编写该模型</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/ea8188c0ae0ad34eb2c9f0f4944e00e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*eVOhG1GNdOfZX7qpaEoa9w.png"/></div></div></figure><p id="55de" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后在第二步，我们需要找到连接𝜇和𝜂.的函数在线性回归的情况下，很简单。由于<em class="nj"> E(Y) = </em> 𝜇，并且我们建模的<em class="nj"> Y </em>的平均值是𝜂，所以我们有𝜂 <em class="nj"> = g( </em> 𝜇) = 𝜇！</p><p id="0180" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第三步:让我们找到负的对数可能性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/b4e968f3307be269c109f08d84f1ea40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H7zTk38ZcO1zJ6HTKBxhQw.png"/></div></div></figure><p id="9eca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意，我们的损失函数与误差平方和成正比。</p><p id="f4c4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，对于第 4 步，让我们看看我们是否可以分析性地最小化这个损失函数。首先对<strong class="kw iu"> β </strong>求导，并将其设为等于<strong class="kw iu"> 0。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/83f123ebe67bd4c6dc272e04627747a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*beT4Tyq2N1aKpIgfroXoLQ.png"/></div></figure><p id="44ab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就给出了我们所熟知并喜爱的来自普通线性回归的闭式解。</p><p id="f527" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在是简单的编码。让我们随机生成一些正态分布的<em class="nj"> Y </em>值并拟合模型。下面的散点图显示，我们对<strong class="kw iu"> β </strong>的拟合值非常接近真实值。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="5bfa" class="nk mn it of b gy oj ok l ol om">import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import seaborn as sns</span><span id="60cd" class="nk mn it of b gy on ok l ol om">np.random.seed(123)</span><span id="2c96" class="nk mn it of b gy on ok l ol om">## GENERATE STUFF ##</span><span id="7444" class="nk mn it of b gy on ok l ol om"># Generate X<br/>N = 1000  # number of observations<br/>D = 10 # dimension of each observation<br/>X = np.random.randn(N, D-1) # (minus 1 because we have to add the intercept)<br/>intercept = np.ones(N).reshape(N, 1) <br/>X = np.concatenate((intercept, X), axis = 1)</span><span id="55e4" class="nk mn it of b gy on ok l ol om"># Generate true beta<br/>beta = np.random.randn(D)</span><span id="3bca" class="nk mn it of b gy on ok l ol om"># Generate response as function of X and beta<br/>y = X @ beta + np.random.randn(N)</span><span id="19fb" class="nk mn it of b gy on ok l ol om">## FIT STUFF ##<br/>beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y</span><span id="7824" class="nk mn it of b gy on ok l ol om">## PREDICT STUFF ##<br/>y_hat = X @ beta_hat</span><span id="03a0" class="nk mn it of b gy on ok l ol om">## PLOT ##<br/>fig, ax = plt.subplots()<br/>sns.scatterplot(beta, beta_hat)<br/>ax.set(xlabel = 'True Betas', ylabel = 'Estimated Betas', title = 'Beta vs. Beta Hats');</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/af9d87a2875c2127f9dc5c3af712f7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uCWw0P8NCLvTAzO3hujIRQ.png"/></div></div></figure></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="7ba7" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">逻辑回归——二元数据的 GLM</h1><p id="29f8" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">在逻辑回归中，我们将输出建模为独立的伯努利试验。即我们假设</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/8d0ea535cd3ab25f2bec393084e2e328.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*R3iUXSCpkHqzEAMIda7jPg.png"/></div></figure><p id="20b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就完成了第一步。对于第 2 步，我们必须找到一种方法将我们的线性预测器𝜂与我们的参数<em class="nj">p</em>联系起来，因为<em class="nj"> p </em>在 0 和 1 之间，而𝜂可以是任何实数，自然的选择是对数概率。即，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/5b4b31c678272f09c5f87e16f9fd99ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*teUeHALVk5lqjEtNbUNNCQ.png"/></div></figure><p id="a62a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">相反，我们使用 sigmoid 函数从𝜂得到<em class="nj"> p </em>(我称之为 S) <em class="nj"> : </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/63b8a25a83fef510b1e7ae75fe17356d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*LBuTD7QQ5PxqO4OYuegG5Q.png"/></div></figure><p id="e138" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就结束了第二步。对于第 3 步，找出负对数似然。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/1a69209b92aee1064bd8c8ab7e180897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O9Mdtz0FAgF2bPDUWVuFgw.png"/></div></div></figure><p id="4cf4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就给出了我们的损失函数，完成了第 3 步。对于第 4 步，我们找到了<strong class="kw iu"> β </strong>的值来最小化这种损失。不幸的是，在逻辑回归的情况下，没有封闭形式的解决方案，所以我们必须使用梯度下降。所以，让我们找到损失函数相对于<strong class="kw iu"> β </strong>的导数。首先注意<em class="nj"> S'(x) = S(x)(1-S(x)): </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/28de8cac1ffdff247bd4e62c7454bf71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L2HtMlh1eqonG9xwyIPaVQ.png"/></div></div></figure><p id="81d9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后我们可以推导出整个梯度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/894421e6b81a99373a724abbdc37777c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h7umEHnS_S_l87FUMUXqsw.png"/></div></div></figure><p id="34f4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了加速 Python 中的计算，我们也可以把它写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/992fe8590e11b4e24efa17613439729e.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*a1CCP7eFPbqYuG2ybEdsxA.png"/></div></figure><p id="eaf7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/4694250143df6e799fceb098126073ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*YP6fVkxH7SgOwcbOcqyJ1Q.png"/></div></figure><p id="2ff6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">和</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/2f34b5c1360a6ceb99dadde576e7100c.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*5ZDOFLblgj0Y_WIGC9yopQ.png"/></div></figure><p id="cddd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们使用梯度下降来拟合模型。同样，下面的散点图显示，我们对<strong class="kw iu"> β </strong>的拟合值非常接近真实值。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="27c7" class="nk mn it of b gy oj ok l ol om">np.random.seed(123)</span><span id="16ce" class="nk mn it of b gy on ok l ol om">## GENERATE Ys ##</span><span id="9678" class="nk mn it of b gy on ok l ol om"># Generate response as a function of the same X and beta<br/># only now it's a Bernoulli r.v. <br/>def sigmoid(x):<br/>    return 1/(1 + np.exp(-x))<br/>p = sigmoid(X @ beta)<br/>y = np.random.binomial(1, p)</span><span id="2ede" class="nk mn it of b gy on ok l ol om">## FIT WITH GD ##<br/>nruns = 10**5<br/>learning_rate = 0.0001<br/>beta_hat = np.random.randn(D) # initialize beta_hat estimates<br/>p_hat = sigmoid(X @ beta_hat) # initialize p_hats<br/>for run in range(nruns):<br/>    # get gradient <br/>    a, b = y*(1-p_hat), (1-y)*p_hat <br/>    grad = X.T @ (b-a)<br/>    # adjust beta hats<br/>    beta_hat -= learning_rate*grad<br/>    # adjust p hats<br/>    p_hat = sigmoid(X @ beta_hat)</span><span id="9e51" class="nk mn it of b gy on ok l ol om">## PLOT ##<br/>fig, ax = plt.subplots()<br/>sns.scatterplot(beta, beta_hat)<br/>ax.set(xlabel = 'True Betas', ylabel = 'Estimated Betas', title = 'Beta vs. Beta Hats');</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/b6bb8726b2b9768c1be6c205b7dd22b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DytBHq7ryICdkYjAft1lrA.png"/></div></div></figure></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="9bbc" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">泊松回归-计数数据的 GLM</h1><p id="35da" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">泊松是对计数中发生的数据建模的一种很好的方式，例如高速公路上的事故或因马踢而死亡。</p><p id="306c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第一步:假设我们有</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/ed64ef3a3bded217967c4f5a3fa18966.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*bTgV6A6ewLueqGIzeLhODA.png"/></div></div></figure><p id="34bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第二步，我们指定链接函数。链接函数必须将非负速率参数λ转换为线性预测值η ∈ ℝ.一个常见的功能是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/4d8cdd4d679b965218d8fd57f7581a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*70SzZ9pinJx6M6GNezy1xQ.png"/></div></figure><p id="153a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这当然也有负面影响</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/c1f33bedcfeb5d6e63bf31d55ca89c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*uQaMO-uJAHCiifr8HuT0pA.png"/></div></figure><p id="55e9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在进行第三步，找到负的对数可能性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/77968b70a180b99f1f79699ccfb0d304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bKMF8Jbqdr25v01I2sFURQ.png"/></div></div></figure><p id="4404" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第四步，对<strong class="kw iu"> β </strong>进行优化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/a2ed539ba2360073e77b175f231034fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Da4NEHPCHunU9zTIPuPRNg.png"/></div></figure><p id="1ee4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">不幸的是，没有封闭形式的解决方案，所以我们再次转向梯度下降，如下所示。再一次，估计的参数被绘制成真实的参数，并且模型再一次做得很好。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="542d" class="nk mn it of b gy oj ok l ol om">np.random.seed(123)</span><span id="18d5" class="nk mn it of b gy on ok l ol om">## GENERATE Ys ##</span><span id="92ce" class="nk mn it of b gy on ok l ol om"># Generate response as a function of the same X and beta<br/># only now it's a Poisson r.v. <br/>beta = np.random.randn(D)/2<br/>lam = np.exp(X @ beta)<br/>y = np.random.poisson(lam = lam)</span><span id="9647" class="nk mn it of b gy on ok l ol om">## FIT WITH GD ##<br/>nruns = 10**5<br/>learning_rate = 0.00001<br/>beta_hat = np.random.randn(D)/2 # initialize beta_hat estimates<br/>lam_hat = np.exp(X @ beta_hat) # initialize lam_hats</span><span id="1b84" class="nk mn it of b gy on ok l ol om">for run in range(nruns):<br/>    # get gradient <br/>    c = y - lam_hat<br/>    grad = X.T @ c<br/>    # adjust beta hats<br/>    beta_hat -= learning_rate*grad<br/>    # adjust lambda hats<br/>    lam_hat = np.exp(X @ beta_hat)</span><span id="912d" class="nk mn it of b gy on ok l ol om">## PLOT ##<br/>fig, ax = plt.subplots()<br/>sns.scatterplot(beta, beta_hat)<br/>ax.set(xlabel = 'True Betas', ylabel = 'Estimated Betas', title = 'Beta vs. Beta Hats');</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/ced83158edbb2d84f1508b246948d29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*NPWBeL32-eAUcoceQ0oTOA.png"/></div></figure></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="4c7a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在实际构建 GLMs 时，Python 中 R 的<code class="fe pe pf pg of b">glm</code>命令和 statsmodels 的<code class="fe pe pf pg of b">GLM</code>函数很容易实现，并且编程效率高。但是，熟悉 GLM 的结构对于参数调整和模型选择至关重要。当谈到建模时，通常了解引擎盖下是什么的最好方法是自己制造汽车。</p></div></div>    
</body>
</html>