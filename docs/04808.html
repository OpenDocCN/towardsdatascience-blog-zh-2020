<html>
<head>
<title>Replicating Airbnb’s Amenity Detection with Detectron2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Detectron2复制Airbnb的舒适度检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/replicating-airbnbs-amenity-detection-with-detectron2-28f33704d6ff?source=collection_archive---------25-----------------------#2020-04-27">https://towardsdatascience.com/replicating-airbnbs-amenity-detection-with-detectron2-28f33704d6ff?source=collection_archive---------25-----------------------#2020-04-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="208d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">成分:1个检测器2，38，188个开放图像，1个GPU。模特培训时间:18小时。人类时间:127小时。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c505811c4f695b78e7d300181d573d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hlRT5M1A_FCZkV8_WTxsOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算机视觉驱动的舒适性检测的工作流程示例。图片来源:<a class="ae ky" href="https://www.airbnb.com/rooms/2151100" rel="noopener ugc nofollow" target="_blank">https://www.airbnb.com/rooms/2151100</a></p></figure><p id="1642" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几个月前，我在<a class="ae ky" href="https://medium.com/airbnb-engineering/amenity-detection-and-beyond-new-frontiers-of-computer-vision-at-airbnb-144a4441b72e" rel="noopener">读到了Airbnb工程团队</a>的一篇文章，描述了他们如何使用计算机视觉来检测照片中的便利设施。</p><p id="ef29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章读起来像一个食谱。机器学习的秘诀。</p><p id="3007" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像任何初露头角的厨师(或机器学习者)一样，我决定复制它，并加入一些我自己的味道。</p><p id="c668" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">等等。</p><p id="5f51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">什么是便利设施？</p><p id="f20e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想想房间里有用的东西。比如厨房里的烤箱或者浴室里的淋浴。</p><p id="7cb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么在图像中检测这些会有帮助？</p><p id="9fa3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是业务用例出现的地方。你会经常看到在MNIST(手写数字的照片)等数据集上建立的计算机视觉模型教程，但很难将这些问题转化为商业用例(除非你是从事检测手写数字业务的邮政服务人员)。</p><p id="72da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你看过Airbnb的网站，那里有很多房子和住宿的照片。除了这些地方，还有基于文本的描述细节的细节。比如你看的房子里有没有按摩浴缸。</p><p id="4ccb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举个例子怎么样？</p><p id="6e03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设你想在Airbnb上列出你的家。你可以上传一些照片，并填写一些细节。但是很有可能，因为你非常了解自己的位置，你可能会错过一些东西。</p><p id="96a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是自动舒适度检测可以发挥作用的地方。</p><p id="c64f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当你将图片上传到Airbnb时，计算机视觉机器学习模型会查看这些图片，试图找到每张图片中的关键设施，并将它们自动添加到你的清单中。</p><p id="e1f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，它可以在实际运行之前向您验证它的预测是否正确。但是让它自动发生将有助于确保每个列表上的信息尽可能地被填写。</p><p id="edcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拥有尽可能详细的信息意味着人们可以根据特定的标准来搜索地点。因此，这对年轻夫妇在按摩浴缸和壁炉前度过一个美好的周末后，可以找到他们想要的东西。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c8b4" class="ma mb it lw b gy mc md l me mf">A note to the reader, treat this article as a high-level narrative of what happened mixed with a splash of tech. For the nerds like me, the code is available in the <a class="ae ky" href="https://dbourke.link/airbnbcode" rel="noopener ugc nofollow" target="_blank">example Google Colab Notebook</a>.</span></pre><h1 id="5417" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">为期42天的项目:边做边学</h1><p id="d9f8" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在开始这个项目之前，我就像一个拿着一套没动过的刀的厨师。或者在我的情况下，一个还没有使用过<a class="ae ky" href="https://github.com/facebookresearch/detectron2" rel="noopener ugc nofollow" target="_blank">检测器2 </a>、<a class="ae ky" href="https://www.wandb.com/" rel="noopener ugc nofollow" target="_blank">权重&amp;偏差</a>、<a class="ae ky" href="https://www.streamlit.io" rel="noopener ugc nofollow" target="_blank">流线</a>或者大部分<a class="ae ky" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>的机器学习工程师(这些都是机器学习工具)。</p><p id="8d2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我发现在建造东西的时候，我对事物了解得最多。你可能也一样。</p><p id="4d38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我打开了一个概念文档，写下了一些标准，并在我的白板上列出了一个6周的大纲。</p><div class="kj kk kl km gt ab cb"><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/55bfd0930f287ee315c620c1ccd78cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*kRWvZ74ZvtkWlffXpZc8UA.png"/></div></figure><figure class="nc kn ni ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/fdd51d2826b80aa5eb607b21a36620b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*TIDemq1VL1UstcqjndahLg.png"/></div></figure><figure class="nc kn nj ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/dad9c5216e13556c71673bb91e178433.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*O0-3O0fwhkXlyNSI0LoSpQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk nk di nl nm translated">从左到右:一个简单的<a class="ae ky" href="https://www.mrdbourke.com/42days" rel="noopener ugc nofollow" target="_blank"> 42天项目模板</a>我放在我桌子前面的墙上，所有的<a class="ae ky" href="https://dbourke.link/airbnb42days" rel="noopener ugc nofollow" target="_blank">日记风格的项目笔记在观念</a>和我的松散绘制的白板指南。</p></figure></div><p id="8fb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么是6周？</p><p id="83dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为42天看起来足够完成一件有意义的事情，但又不至于占据你的生活。</p><p id="6825" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之。</p><p id="0836" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我想我会花6周左右的时间(每天3个小时)用我一直想尝试的工具复制Airbnb的便利设施检测。</p><p id="e05f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最坏的情况是，我学了一些东西，如果都失败了，也就6周。</p><p id="1065" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最好的情况是，我学到了一些东西，并开发了一个非常酷的机器学习应用程序。</p><p id="06fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不管怎样，都会有一个故事要讲(你正在读它)。</p><h1 id="c391" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">分解它</h1><p id="2a9d" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">这个有趣的小图形打破了实验中的主要步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/2baf904aaadd961269c27c6839e0e9a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x7QZx3KooM9O2MII8Wlwfg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我用来复制Airbnb舒适度检测的配方，从数据收集和预处理开始，然后是建模和实验跟踪，最后是应用程序的构建和部署。</p></figure><p id="f3ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有时文字比充满其他图像的图像更容易阅读。</p><p id="676e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用Detectron2复制Airbnb的舒适度检测方法:</p><ol class=""><li id="ca13" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">使用downloadOI.py(用于从打开的图像中下载特定图像的脚本)收集数据。</li><li id="258f" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">使用preprocessing.py(一个自定义脚本，具有将打开的图像图像和标签转换为Detectron2样式数据输入的功能)预处理数据。</li><li id="fc8b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">检测器2的模型数据。</li><li id="d193" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">使用权重和偏差跟踪建模实验。</li><li id="17b9" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">使用Streamlit创建面向用户的应用程序。</li><li id="3581" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">部署应用程序和模型与Docker，GCR(谷歌容器注册)和谷歌应用引擎。</li></ol><p id="2d38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用这些来驱动文章的其余部分。让我们开始吧。</p><h1 id="c32b" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">数据收集:从打开的图像中下载Airbnb目标图像和标签文件</h1><ul class=""><li id="0b29" class="no np it lb b lc mx lf my li oc lm od lq oe lu of nu nv nw bi translated"><strong class="lb iu">成分:</strong> 1个下载脚本，4个标签文件，38，000+ x个打开的图像</li><li id="f710" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">时间:</strong> 1周</li><li id="3d09" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">费用:</strong> $0</li><li id="0566" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">器皿</strong> : Google Colab，本地机</li></ul><p id="e435" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像所有机器学习项目一样，它从数据开始，也从数据结束。</p><p id="f91c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Airbnb的文章提到，为了建立他们的概念证明，他们使用了32k公共图像和43k内部图像。</p><p id="48c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">唉，第一个路障。</p><p id="ab06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，除了2016年对一个技术支持角色的电话面试，我与Airbnb内部没有任何联系，所以内部图像不在讨论范围内。</p><p id="81a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好消息是他们使用的32k公共图片来自<a class="ae ky" href="https://storage.googleapis.com/openimages/web/index.html" rel="noopener ugc nofollow" target="_blank"> Open Images </a>(一个巨大的免费开源资源，包含来自600多个不同类别的数百万张图片)。</p><p id="b4f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，如果你打开图片，你会发现有超过一百万张不同的图片。</p><p id="8025" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那交易是什么？</p><p id="e4f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Airbnb为什么只用32k？</p><p id="bc85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好问题。</p><p id="91f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是因为他们只对与其业务用例相关的图像感兴趣(包含公共设施的房间的图像)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/920857023f2bc70fc1da5f67a1808f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*va4tj9eGbMimJUs-5G25sg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Airbnb的目标类别(30种便利设施类型，摘自原始文章)与来自600个不同类别的190多万张图像的开放图像数据库。</p></figure><p id="fb6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我想，知道了Airbnb最关心的目标类别，肯定有办法只从你关心的开放图像中下载图像(而不是整个190万+500 GB+的数据集)。</p><p id="6fa6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">原来是有的。</p><p id="04c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">经过一番搜索，我从LearnOpenCV 找到了一个关于如何从目标类列表中下载图片的很好的指南。</p><p id="f7e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，是一个名为<a class="ae ky" href="https://github.com/mrdbourke/airbnb-amenity-detection/blob/master/downloadOI.py" rel="noopener ugc nofollow" target="_blank"> downloadOI.py </a>的脚本，它允许您定义您要查找的类以及来自哪个数据集。</p><p id="a405" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一个例子。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="b4c7" class="ma mb it lw b gy mc md l me mf">!python3 downloadOI.py --dataset "validation" --classes "Kitchen &amp; dining room table"</span></pre><p id="bfe5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一行写着，“从开放图像的验证集中获取包含厨房和餐厅桌子的图像。”</p><p id="be0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行上面的代码行将目标类中的所有图像下载到与目标数据集同名的文件夹中。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4ca2" class="ma mb it lw b gy mc md l me mf"># What your image folder(s) look like after using downloadOI.py</span><span id="cbd3" class="ma mb it lw b gy oh md l me mf">validation &lt;- top file <br/>│   kitchen_&amp;_dining_room_table_image_1.jpg &lt;- image 1<br/>│   kitchen_&amp;_dining_room_table_image_2.jpg &lt;- image 2<br/>|<!-- -->   ... &lt;- more images...</span></pre><p id="9105" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LearnOpenCV脚本中最初的downloadOI.py运行得非常好，它甚至可以在下载图像时生成标签。但是经过一点试验，我发现这些标签与Detectron2不兼容。</p><p id="235a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我取消了下载时的标签创建，并将脚本修改为只下载图像。我决定编写自己的标签创建代码。</p><p id="ab49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">能够下载图像:检查。</p><p id="9170" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在来看看标签。</p><p id="51f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">开放图像标签更容易访问，可以通过点击<a class="ae ky" href="https://storage.googleapis.com/openimages/web/download.html" rel="noopener ugc nofollow" target="_blank">开放图像下载页面</a>上的特定下载链接或运行以下代码来下载(花絮:滚动到下载页面的底部查看关于标签的信息)。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d5ee" class="ma mb it lw b gy mc md l me mf"># Open Images training dataset bounding boxes (1.11G)<br/>!wget https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv<br/><br/># Open Images validation dataset bounding boxes (23.94M)<br/>!wget https://storage.googleapis.com/openimages/v5/validation-annotations-bbox.csv<br/>    <br/># Open Images testing bounding boxes (73.89M)<br/>!wget https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv<br/><br/># Class names of images (11.73K)<br/>!wget <a class="ae ky" href="https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv</a></span></pre><p id="cd58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据收集的第一阶段获得了以下文件。</p><ul class=""><li id="4905" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu of nu nv nw bi translated">来自开放图像训练、验证和测试集的1类(咖啡机)图像。</li><li id="5c21" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">训练(<code class="fe oi oj ok lw b">train-annotations-bbox.csv</code>)、验证(<code class="fe oi oj ok lw b">validation-annotations-bbox.csv</code>)和测试(<code class="fe oi oj ok lw b">test-annotations-bbox.csv</code>)设置包围盒图像标签。</li><li id="9a5d" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">不同打开图像类别的描述(<code class="fe oi oj ok lw b">class-descriptions-boxable.csv</code></li></ul><p id="8ad4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么只有1节课？</p><p id="ecdf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的计划是从小处着手。获取数据预处理和使用1个类的Detectron2模型，然后在需要时扩展。</p><h1 id="ef9e" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">数据准备:将打开的图像数据和标签输入到Detectron2样式的输入中</h1><ul class=""><li id="f7a6" class="no np it lb b lc mx lf my li oc lm od lq oe lu of nu nv nw bi translated"><strong class="lb iu">成分:</strong> 5个预处理功能，4个标签文件，38，000+ x个打开的图像，Detectron2文档</li><li id="8200" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">时间:</strong> 1周+/- 1周</li><li id="5fbc" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">成本:</strong> $0</li><li id="db3b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">器具</strong> : Google Colab，本地机器</li></ul><p id="6493" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的技巧是将我的数据文件(一些标签CSV和几个图像文件夹)重新混合到<a class="ae ky" href="https://detectron2.readthedocs.io/tutorials/datasets.html" rel="noopener ugc nofollow" target="_blank"> Detectron2样式标签</a>中。</p><p id="66b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我发现如果你想在Detectron2中使用你自己的自定义数据，你需要把它转换成一个字典列表。其中每个字典是与1幅图像相关联的信息。</p><p id="3969" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一个例子。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Detectron2将一个字典列表<a class="ae ky" href="https://detectron2.readthedocs.io/tutorials/datasets.html" rel="noopener ugc nofollow" target="_blank">作为输入。如果您想使用自己的自定义数据，您必须将标签格式化为这种样式。</a></p></figure><p id="0b47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，这些字段分别是什么意思？</p><ul class=""><li id="6a4c" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu of nu nv nw bi translated"><code class="fe oi oj ok lw b">annotations</code>(列表):一幅图像上的所有注释(标签)，一幅图像可能有不止一个。对于对象检测(我们的用例)，它包含:</li><li id="252e" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><code class="fe oi oj ok lw b">bbox</code>(int列表):边界框的像素值坐标。</li><li id="f0b1" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><code class="fe oi oj ok lw b">bbox_mode</code> ( <a class="ae ky" href="https://docs.python.org/3/library/enum.html" rel="noopener ugc nofollow" target="_blank">枚举</a>)<code class="fe oi oj ok lw b">bbox</code>、<a class="ae ky" href="https://detectron2.readthedocs.io/modules/structures.html#detectron2.structures.BoxMode" rel="noopener ugc nofollow" target="_blank">中像素值的顺序和比例详见</a>文档。</li><li id="2128" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><code class="fe oi oj ok lw b">category_id</code> (int):物体类别在<code class="fe oi oj ok lw b">bbox</code>内的数值映射，例子<code class="fe oi oj ok lw b">{'coffeemaker':0, 'fireplace':1}</code>。</li><li id="3ba5" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><code class="fe oi oj ok lw b">file_name</code> (str):目标图像的字符串文件路径。</li><li id="816c" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><code class="fe oi oj ok lw b">height</code> (int):目标图像的高度。</li><li id="e4c3" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><code class="fe oi oj ok lw b">width</code> (int):目标图像的宽度。</li><li id="3cbf" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><code class="fe oi oj ok lw b">image_id</code> (int):唯一图像标识符，在评估期间用于识别图像。</li></ul><p id="8544" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想用Detectron2建立你自己的物体检测模型，你需要为你的每一张图片安装一个。</p><p id="f337" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们将在后面看到的，我对这个项目的第一个主要目标是获得一个运行在定制数据上的小模型(总是从小模型开始)。</p><p id="e760" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从小处着手意味着为1类图像(咖啡机)编写数据准备功能，并确保它们与Detectron2一起工作。</p><h2 id="aaf4" class="ma mb it bd mh on oo dn ml op oq dp mp li or os mr lm ot ou mt lq ov ow mv ox bi translated">获取图像id</h2><p id="e8f4" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">从开放图像下载数据的好处是每个图像都有一个唯一的标识符。</p><p id="d29e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，<code class="fe oi oj ok lw b">1e646e27c250cd56.jpg</code>是一张咖啡机的图片，其唯一标识符<code class="fe oi oj ok lw b">1e646e27c250cd56</code>不同于开放图像中的所有其他图像。</p><p id="e4de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我写了一个名为<code class="fe oi oj ok lw b">get_image_ids()</code>的函数，它将遍历一个文件夹并返回该文件夹中所有唯一图像id的列表。</p><p id="7fb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着我知道我正在处理的所有图像。</p><h2 id="f6c5" class="ma mb it bd mh on oo dn ml op oq dp mp li or os mr lm ot ou mt lq ov ow mv ox bi translated">格式化现有注释文件</h2><p id="5c88" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">为什么拥有唯一图像id列表会有所帮助？</p><p id="03c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">原因如下。</p><p id="e450" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您从打开的图像下载标签文件并使用pandas导入它们时，它们看起来像这样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/6c028a1cea04396e4853f3f29aa5f240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cE2LBvBYveFcALzLc93OYA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">打开图像的验证标签的示例。参见Colab 上的<a class="ae ky" href="https://dbourke.link/airbnbcode" rel="noopener ugc nofollow" target="_blank">完整代码和示例笔记本。</a></p></figure><p id="42bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看到ImageID列了吗？在那里，我们可以匹配从打开的图像中下载的图像id列表。</p><p id="16fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是等等。</p><p id="c71c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么要这么做？</p><p id="d2d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两个原因。</p><p id="b236" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要额外的信息，比如XMin、XMax、YMin和YMax坐标(我们很快就会看到这样的例子)。</p><p id="6a67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二，从打开的图像中下载注释文件会导致我们获得数据库中每个图像的注释，但是我们只对目标图像的注释感兴趣。</p><p id="38c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能已经注意到LabelName列有一些奇怪的值，如<code class="fe oi oj ok lw b">/m/0cmf2</code>和<code class="fe oi oj ok lw b">/m/02xwb</code>，事实证明，这些也是代码。</p><p id="a261" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是神秘的<code class="fe oi oj ok lw b">class-descriptions-boxable.csv</code>发挥作用的地方。</p><p id="1f82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/26ba868c34a668fcf14e8e4c93147efd.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*bwNS62HZPybiH15zL5mmdg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">开放图像标签代码到人类可读类名的映射。</p></figure><p id="1102" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不知道你怎么想，但是这个名字看起来比<code class="fe oi oj ok lw b">/m/0cmf2</code>和<code class="fe oi oj ok lw b">/m/02xwb</code>好多了。</p><p id="8521" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这些以及一个目标类列表，在我的例子中，只有咖啡机，开始时，我有了创造<code class="fe oi oj ok lw b">format_annotations()</code>所需的所有材料。</p><p id="6288" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一个冗长的句子中，<code class="fe oi oj ok lw b">format_annotations()</code>从打开的图像中下载一个现有的注释文件，比如，<code class="fe oi oj ok lw b"><a class="ae ky" href="https://storage.googleapis.com/openimages/v5/validation-annotations-bbox.csv" rel="noopener ugc nofollow" target="_blank">validation-annotations-bbox.csv</a></code>向其中添加一些有用的信息(比如人类可读的类名和每个类的数字分类ID)并删除不需要的行(我们不关注的图像)。</p><p id="9e7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来看一个小亮点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/ec615a851d0739ab2889da4bd2ad7647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vyad_GfcWh1NspZkSUS_WA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">运行format_annotations()函数会添加ClassName和ClassID列，并删除不需要的标签行(查看更改后的索引值)。参见GitHub 上的<a class="ae ky" href="https://dbourke.link/airbnbcode" rel="noopener ugc nofollow" target="_blank">完整代码和示例笔记本。</a></p></figure><p id="8cf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行这个函数负责标签字典的<code class="fe oi oj ok lw b">category_id</code>部分。至于删除所有非咖啡机行，<code class="fe oi oj ok lw b">validation-annotations-bbox.csv</code>从303980行减少到62行。</p><p id="198c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们正在取得进展，但还没有完成。</p><h2 id="4ebb" class="ma mb it bd mh on oo dn ml op oq dp mp li or os mr lm ot ou mt lq ov ow mv ox bi translated">将边界框像素值从相对值转换为绝对值</h2><p id="38fa" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">你可能想知道标签字典中的<code class="fe oi oj ok lw b">bbox_mode</code>是什么意思。</p><p id="e2f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果没有，我会告诉你我是如何处理的。</p><p id="c941" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe oi oj ok lw b">bbox</code>和<code class="fe oi oj ok lw b">bbox_mode</code>是舞伴。</p><p id="dc98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">再看一下打开的图像标签，您会看到XMin、XMax、YMin和YMax列。这些是图像上每个边界框的相对像素坐标。</p><p id="fa70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相对像素坐标意味着要找到目标边界框的每个角出现的实际像素值，必须将XMin、XMax值乘以图像的宽度，将YMin和YMax值乘以图像的高度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/7f16cb4d56693bdb3f0acd170460b2e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*TV0yvXm7-nMwid0BhpEhmA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">打开的图像带有XMin、XMax、YMin和YMax的相对像素值坐标。由于边界框是矩形或正方形的，知道左下角和右上角的坐标就足够了。</p></figure><p id="7fd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">了解这一点很重要，因为Detectron2目前仅支持绝对像素值(相对像素值乘以高度或宽度的结果)。</p><p id="e12c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顺序也很重要。</p><p id="88e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">打开图像边界框的顺序为XMin、XMax、YMin、YMax，但Detectron2需要XMin、YMin、XMax、YMax。</p><p id="9a11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是函数<code class="fe oi oj ok lw b">rel_to_absolute()</code>的用武之地。</p><p id="ff30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它使用图像的高度、宽度和现有边界框坐标，并使用它们将开放图像样式坐标转换为Detectron2样式坐标。</p><p id="926f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们完成了转换，<code class="fe oi oj ok lw b">bbox</code>参数就会以<code class="fe oi oj ok lw b">bbox_mode</code> ( <code class="fe oi oj ok lw b">BoxMode.XYXY_ABS</code>)的形式出现。</p><h2 id="5e11" class="ma mb it bd mh on oo dn ml op oq dp mp li or os mr lm ot ou mt lq ov ow mv ox bi translated">创建图像词典(检测器2样式标签)</h2><p id="71be" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">到目前为止，我们已经创建了一些辅助函数。是时候把所有的东西放在一起了。好消息是，我们已经完成了大部分繁重的工作。</p><p id="5e29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">倒数第二个(倒数第二个，是的，还有一个)助手函数是<code class="fe oi oj ok lw b">get_image_dicts()</code>。</p><p id="7fff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它采用一个目标图像文件夹、一个相关的注释文件和一个目标类列表，然后使用上面的函数、<code class="fe oi oj ok lw b">get_image_ids()</code>、<code class="fe oi oj ok lw b">format_annotations()</code>和<code class="fe oi oj ok lw b">rel_to_absolute()</code>加上一点自己的逻辑来创建Detectron2样式标签(一个字典列表)。</p><p id="2243" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">哇哦。那是一口。</p><p id="4624" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一点上，提醒读者如果你想看到这一点，检查<a class="ae ky" href="https://dbourke.link/airbnbcode" rel="noopener ugc nofollow" target="_blank">的例子谷歌Colab笔记本</a>。</p><p id="5c59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们将继续这个故事。</p><p id="01f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">哦，对了，关于<code class="fe oi oj ok lw b">get_image_dicts()</code>还有一件事你应该知道，一旦它创建了图像字典列表，它会将它们保存到一个JSON文件中。这确保了我们以后可以使用它们。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">get_image_dicts()将打开的图像和标签转换为Detectron2样式标签的示例用法(词典列表)。参见GitHub 上的<a class="ae ky" href="https://dbourke.link/airbnbcode" rel="noopener ugc nofollow" target="_blank">完整代码和示例笔记本。</a></p></figure><p id="2d89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我所说的稍后，是指当你<a class="ae ky" href="https://detectron2.readthedocs.io/tutorials/datasets.html#register-a-dataset" rel="noopener ugc nofollow" target="_blank">用Detectron2 </a>注册一个数据集的时候。</p><p id="2a65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于某种原因，当您用Detectron2注册数据集时，数据集需要一些预处理，预处理必须用lambda函数完成，因此只能接受一个参数。</p><p id="4744" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">进入<code class="fe oi oj ok lw b">load_json_labels()</code>，这是最后一个助手函数，它从目标图像文件夹中导入一个JSON文件(记住我们是如何保存它们的，以备后用)。</p><p id="5e77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里你要注意的技巧是，确保如果你将图像字典保存到文件并重新导入它们，你要确保<code class="fe oi oj ok lw b">bbox_mode</code>被格式化为Detectron2风格的BoxMode。</p><p id="9e95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BoxMode是一个<a class="ae ky" href="https://docs.python.org/3/library/enum.html" rel="noopener ugc nofollow" target="_blank"> Python枚举类型</a>，一个特殊的类型，根据我的经验，它不能很好地保存到JSON中(你也许能在这里启发我)。</p><p id="b72f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嘣。</p><p id="3416" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据预处理完成。我们所要做的就是用Detectron2注册我们的数据集，然后我们就可以开始建模了。</p><p id="4e19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">等等，为什么我们需要用Detectron2注册数据集？</p><p id="39d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我不完全确定这一点。但是一旦这样做了，注册的数据集就变成了半不可变的(不能轻易改变)。</p><p id="8ffb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这似乎是一个防止未来数据集不匹配的好主意。</p><p id="ef0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个叫<code class="fe oi oj ok lw b">DatasetCatalog.register()...</code>的小家伙，还有<code class="fe oi oj ok lw b">MetaDataCatalog.get()</code>...(这是使用<code class="fe oi oj ok lw b">load_json_labels()</code>的地方)然后我们出发去建模站。</p><h1 id="7719" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">建模和实验:从小处着手，不断迭代</h1><ul class=""><li id="a31d" class="no np it lb b lc mx lf my li oc lm od lq oe lu of nu nv nw bi translated"><strong class="lb iu">成分:</strong>开放图像的小子集(3类)，开放图像的10%子集(所有类)</li><li id="8696" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">时间:</strong> 2周+/- 2天</li><li id="e410" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">成本:</strong> $0</li><li id="6897" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">器具</strong>:谷歌实验室，砝码&amp;偏差，探测器2模型动物园</li></ul><p id="8c80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经谈了很多了。但好消息是，一旦你准备好了数据集，Detectron2会让建模阶段变得非常有趣。</p><p id="fd28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我开始之前，我必须提醒自己机器学习的第一条规则:从小处着手，经常实验，需要时扩大规模。</p><p id="84da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这看起来像什么？</p><p id="82d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还记得标签创建是如何从只为一类图像创建标签开始的吗？模特界也是如此。</p><p id="0556" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么从小处着手？</p><p id="a3e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你知道俗话说得好。如果船朝着错误的方向行驶，再用力划船也无济于事。</p><p id="4851" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我开始用1个类(咖啡机)获得一个Detectron2模型。</p><p id="6f19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦成功了，我就把它扩大到2个班，然后是3个班。您必须知道，每次我这样做时，我都会发现我的预处理函数哪里出了问题，需要改进。其中一个主要问题是确保标签只为目标类创建，而不是来自开放图像的所有类。</p><p id="3320" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在让探测器2和3个班级一起工作后，下一步是开始认真做模型实验。</p><p id="51ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你看，我在Airbnb的文章中读到，他们开始进行迁移学习，但没有取得多大成功，之后他们转向了谷歌的AutoML。AutoML工作，但他们说，限制是不能下载模型。</p><p id="3d37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我的标准之一是避免使用Google的AutoML(以修复模型可访问性限制),所以我没有使用它。</p><p id="b794" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，我参考了<a class="ae ky" href="https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md" rel="noopener ugc nofollow" target="_blank"> Detectron2的模型动物园</a>，这是一个与<a class="ae ky" href="http://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO(上下文中的常见对象)数据集</a>相关的模型集合，并发现已经有一些对象检测模型准备就绪。</p><p id="65a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太美了。</p><p id="5618" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的想法是，我将尝试每个预训练的对象检测模型，利用他们从COCO数据集学习的模式，用我自己的数据(一个小数据集)升级模式，看看它是否有效。</p><p id="23f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我照做了。</p><p id="be08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我从我想尝试的Detectron2模型动物园中定义了一个模型字典。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="c311" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">做了一个小实验。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="72e8" class="ma mb it lw b gy mc md l me mf"># My first modelling experiment in pseudocode<br/>for model in models_to_try: <br/>    with random_seed x<br/>    do 3000 iterations <br/>    on classes coffeemaker, bathtub, tree house <br/>    save results to Weights &amp; Biases</span></pre><p id="78b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个实验的目的是看哪个Detectron2对象检测模型在我的数据集上表现最好。我假设我可以通过控制除了模型之外的所有东西(因此是随机种子)来解决这个问题。</p><p id="8929" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在你可能想知道，你是如何决定这些模型的？</p><p id="492c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问得好。我看了模型动物园网页，选了它们。</p><p id="70a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，那么，你是如何追踪你的实验结果的？</p><p id="6f4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很高兴你问了。这就是<a class="ae ky" href="https://www.wandb.com" rel="noopener ugc nofollow" target="_blank">权重&amp;偏差</a>出现的原因。weights&amp;bias是一个追踪深度学习实验的非凡工具，如果你还没有使用过，你应该使用它。</p><p id="186c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">发生了什么事？</p><p id="6b10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我做了实验。由于权重和偏见，结果是美丽的。</p><p id="212e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么？</p><p id="898c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为模型显示他们正在学习一些东西(平均精度，一种评估物体检测模型的度量标准正在提高)，并且每个模型之间没有任何奇怪的差异(意味着实验控制有效)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/9d7bd6d4514d34c5d4096b70cc53a246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h2NqQosR2yWFFtbiMPwKAw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我第一次重大实验的结果。每一根棒线都是权重和偏差的“运行”。实验名称可以在左侧看到，每个实验名称对应于models_to_try中不同模型的结果。你可以看到<a class="ae ky" href="https://app.wandb.ai/mrdbourke/airbnb-amenity-detection" rel="noopener ugc nofollow" target="_blank">关于重量&amp;偏差</a>的完整项目实验。</p></figure><p id="4a59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后呢？</p><p id="217f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了缩小范围，看看我应该使用哪个模型来建立一个大狗模型(一个拥有所有数据并训练更长时间的模型)，我假设了另一个实验。</p><p id="8297" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我会从我的第一个实验中选取前两个模型，<em class="pd">retina net _ R _ 50 _ FPN _ 1x</em>(<a class="ae ky" href="https://app.wandb.ai/mrdbourke/airbnb-amenity-detection/runs/18uz78yn" rel="noopener ugc nofollow" target="_blank">细雾-37 </a>)和<em class="pd">retina net _ R _ 101 _ FPN _ 3x</em>(<a class="ae ky" href="https://app.wandb.ai/mrdbourke/airbnb-amenity-detection/runs/374stxdc" rel="noopener ugc nofollow" target="_blank">earthly-cloud-39</a>)，在一个更大的数据集(占所有数据的10%)上训练它们一段合理的时间(大约1000次迭代)，然后比较结果。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="e56e" class="ma mb it lw b gy mc md l me mf"># My second modelling experiment in pseudocode<br/>for model in top_2_models: <br/>    with random_seed x<br/>    do 1000 iterations <br/>    on 10% of the total data <br/>    save results to Weights &amp; Biases</span></pre><p id="89a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，无论哪一个模型脱颖而出，都将成为大狗模型。</p><p id="361b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">必须知道的是，由于我在自己设定的期限内(整个项目42天)，快速试验是最重要的。</p><p id="cdc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Airbnb的文章中，他们提到他们一次训练他们的模型5天和3天。因为我总共花了10天时间做模特，所以我只有一次机会训练一个大狗模特。</p><p id="ffdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到这个阶段，我已经从开放图像下载了完整的训练、验证和测试数据集。为了运行第二个主要实验，这次我决定在整个数据集的一部分上比较两个性能最好的模型(所有30个目标类，而不是只有3个类)。</p><p id="bacf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集状态:</p><ul class=""><li id="531f" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu of nu nv nw bi translated">34，835张训练图像</li><li id="c065" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">860张验证图像</li><li id="804c" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">2，493张测试图像</li></ul><p id="59b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当对小部分数据进行建模实验时，小部分数据与完整数据具有相同的分布是很重要的。</p><p id="8954" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，如果您要对10%的数据进行建模实验，请确保这10%的数据与整个数据集具有相同的类分布。在Airbnb数据集的情况下，如果完整的数据集有大量楼梯的图像，但没有很多葡萄酒架的图像，则较小的版本应该反映这种关系。</p><p id="0a28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我通过从完整的数据集中随机选择示例，并确保类的分布看起来相似，来拆分10%的训练数据。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6367" class="ma mb it lw b gy mc md l me mf"># Get 10% of samples from train dataset<br/>small_dataset = full_dataset.sample(frac=0.1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/d001c0c86b642b9ee838127d4f53dbe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BM6vQ-TMZ6K5QVXC-sX4xg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">创建与原始训练集具有(大致)相同分布的训练数据的10%子集。这样做意味着实验可以在较小的规模上运行，但仍然可以代表整个数据集可能发生的情况。对测试数据集进行了同样的处理。</p></figure><p id="0f3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我最后还将来自开放图像的验证和测试集合并成一个数据集<code class="fe oi oj ok lw b">val_test</code>。这是因为Airbnb提到他们使用10%的测试数据分割进行评估(75k总图像、67.5k训练图像、7.5k测试图像)。</p><p id="6b4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集状态:</p><ul class=""><li id="bf20" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu of nu nv nw bi translated">34，835张训练图像</li><li id="bdef" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">3，353个测试图像(原始验证和测试集图像合并)</li></ul><p id="71e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">准备好较小的代表性数据集后，我开始了第二个主要实验，确保跟踪权重和偏差方面的一切。</p><p id="4b47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实证明，两个模型都表现良好，这意味着应该上升的指标(平均精度)在上升，应该下降的指标(损失)在下降。尽管班级数量增加了10倍，但我的模型在学习(这是件好事)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/8bb6116cdbe011df54a5952b964b145d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N-c4wy80nGcCDSccUvGMkQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">比较<em class="pf">retina net _ R _ 50 _ FPN _ 1x(sleek-sound-45)和</em>retina net _ R _ 101 _ FPN _ 3x(stilled-space ship-46)在1000次迭代中对总数据的10%进行训练的结果。比仅在3个类上训练模型的结果更差(预期)，但平均精度上升是一件好事。</p></figure><p id="51a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于我的第二个主要实验的结果，我最终决定<em class="pd">retina net _ R _ 101 _ FPN _ 3x</em>(<a class="ae ky" href="https://app.wandb.ai/mrdbourke/airbnb-amenity-detection/runs/s0z24hvo" rel="noopener ugc nofollow" target="_blank">高跷-飞船-46 </a>)将升级为大狗模型状态。</p><h1 id="4092" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">训练大狗模型(根据所有数据训练的模型)</h1><ul class=""><li id="417e" class="no np it lb b lc mx lf my li oc lm od lq oe lu of nu nv nw bi translated"><strong class="lb iu">成分:</strong>38000+公开图片(Airbnb的所有目标类)</li><li id="0e7a" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">时间:</strong>人类时间3天，计算时间18小时</li><li id="6e35" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">成本:</strong>$ 150–175美元(每小时1.60美元P100 GPU +多重故障+未充分利用的实例)</li><li id="8b7f" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">器具</strong>:砝码&amp;偏差，检测器2<em class="pd">retina net _ R _ 101 _ FPN _ 3x</em></li></ul><p id="fa95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Detectron2的预训练模型是在<a class="ae ky" href="https://engineering.fb.com/data-center-engineering/introducing-big-basin-our-next-generation-ai-hardware/" rel="noopener ugc nofollow" target="_blank">大盆</a>(一台有8个GPU的大狗电脑)上训练的。我没有8个GPU(图形处理器，一种能够快速计算的计算机芯片)，迄今为止，我所做的一切都是在谷歌Colab (1个GPU)或我的本地计算机(没有GPU)上完成的。</p><p id="39e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据我之前的实验，用10%的数据进行1000次迭代的训练，我知道使用1个GPU对整个数据集进行100，000次以上迭代的完整训练运行(摘自Airbnb的文章和Detectron2模型配置文件)需要大约15到20个小时。</p><p id="677c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这个信封背面的时间线计算，我想我真的只有一次机会来训练一只大狗模型。</p><p id="b9d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最初，我打算做一些超参数调整(调整模型设置以获得更好的结果)，但由于限制，没有像我希望的那样在这里花太多时间。</p><p id="cb31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我决定只关注几个:</p><ul class=""><li id="cb9c" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu of nu nv nw bi translated">学习率(模型在任一时刻试图提高其知识的程度)。</li><li id="168c" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">小批量(模型一次看多少张图片)。</li></ul><p id="7775" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据我的经验，除了模型本身的结构(图层等，反正已经由Detectron2决定了)，这两个设置对性能影响最大。</p><p id="90e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我偶然发现了<a class="ae ky" href="https://github.com/open-mmlab/mmdetection/blob/master/docs/GETTING_STARTED.md#train-a-model" rel="noopener ugc nofollow" target="_blank">线性学习率缩放规则</a>，读了更多关于它的内容，并决定将其应用于<em class="pd">retina net _ R _ 101 _ FPN _ 3x</em><a class="ae ky" href="https://github.com/facebookresearch/detectron2/blob/master/configs/Base-RetinaNet.yaml" rel="noopener ugc nofollow" target="_blank">模型的基础设置</a>。</p><p id="a3dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性学习率比例规则说，你的批量大小和学习率应该随着你使用的GPU的数量而增加和减少。</p><p id="72c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，由于我使用的是1个GPU，而不是Detectron2最初的8个GPU，如果遵守规则，我应该将最初的学习速率和小批量大小除以8。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0411" class="ma mb it lw b gy mc md l me mf"># Linear learning rate scaling rule<br/>new_learning_rate = old_learning_rate/(new_num_gpus * old_num_gpus)</span></pre><p id="442d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我相应地调整了设置，由于我使用的是迁移学习(不是从头开始训练)，我将新的学习率除以10。</p><p id="8582" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么？</p><p id="372b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在Airbnb的文章中读到，当他们尝试迁移学习时，他们将学习除以10，作为一种预防措施，以免在新模型学习时，原始模型模式丢失得太快。</p><p id="932a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这在直觉上对我来说是有意义的，但是，我肯定有更好的方法或者更好的解释。如果你知道，请告诉我。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/e9b49846171edc3cf53f167ff4c932c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TLvTP1uRsrMQYYOFw7a2wg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在8个GPU上训练的原始Detectron2 RetinaNet设置和从头开始更新，以反映在1个GPU上训练的线性学习率调度规则(较低的学习率和批量大小)并使用迁移学习(增加迭代次数和再次降低学习率)。</p></figure><p id="2923" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">怎么样了？</p><p id="a723" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，18.5小时，34，834幅训练图像和180，000个训练步骤在P100 GPU上完成后，我的模型在保留的测试集上以mAP(平均精度)得分<strong class="lb iu"> 43.297% </strong>结束。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/cb843ded59a1286880694a85e9876850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OgR0qABt8iGqqpQldIYyqg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在P100 GPU上使用预训练的检测器2 retinanet_R_101_FPN_3x模型进行18小时迁移学习的最终结果。AP指标越高越好。请注意，有些类的结果比其他类高，这与它们在数据集中的示例数量相关(示例越多，得分越高)。参见<a class="ae ky" href="https://app.wandb.ai/mrdbourke/airbnb-amenity-detection/runs/2gvwvqbu" rel="noopener ugc nofollow" target="_blank">完整训练跑步、设置和更多关于重量&amp;偏差</a>的内容。</p></figure><p id="a80c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">低于Airbnb使用谷歌AutoML的68%地图的结果，但高于他们使用迁移学习的结果。</p><p id="d209" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不过应该指出的是，这些指标(我的和Airbnb的)实际上没有可比性，因为我们使用了不同的数据集，我只能访问公共数据，Airbnb有公共和内部图像数据。</p><p id="c595" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是计量是计量，对吗？</p><p id="4d00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对计算机视觉模型的真正测试是在实际图像上。让我们看看测试集中的几个(记住，模型以前从未见过这些)。</p><div class="kj kk kl km gt ab cb"><figure class="nc kn pi ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/a98b810b1573f2a69a10a8266fa032ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*hGSU5iWkX4goj_4wH-9wiA.png"/></div></figure><figure class="nc kn pi ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/c5520ebcf2de2dc4c0e0fd0c23756ebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*0CZ1x18NqtQbC2NeONPXvw.png"/></div></figure><figure class="nc kn pj ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/461765325311c3b294ea9dc06bc99cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*6qWZko1xd5j3Nbwmn3h5XA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk pk di pl nm translated">迁移学习训练的Detectron2模型的示例，根据开放图像对不可见图像进行舒适度预测。从左到右:厨房里的便利设施被收拾得很好，洗衣机很快就被发现，最后的酒架也被发现。注意:由于这些图像来自开放图像，其中许多不同于标准的Airbnb风格的图像。</p></figure></div><p id="4b3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的卧室和厨房提供的一些定制图片怎么样？</p><div class="kj kk kl km gt ab cb"><figure class="nc kn pm ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/b3032418bd3b8104933c3fdf24f36ecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*2YBN9lfh-ZQKBj--EWgheg.png"/></div></figure><figure class="nc kn pm ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/3f421f2efd7589337ec5441f47e24aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*s8-PXNmzWQ9WukC9Tdem5Q.png"/></div></figure><figure class="nc kn pm ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/56139a23d22dc74106d22a0938c24681.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*A63Gc0SGDchO8q-JX4LE7w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk pn di po nm translated">经过充分训练的模型对我家周围的一些图像进行预测的例子。从左到右:我的床收拾得很好，模特也找到了厨房里的大多数便利设施，但很难从我的工作站中挑选出合适的东西(注意:右图中的许多物品不在培训集中)。</p></figure></div><blockquote class="pp pq pr"><p id="7f07" class="kz la pd lb b lc ld ju le lf lg jx lh ps lj lk ll pt ln lo lp pu lr ls lt lu im bi translated">看到机器学习模型在股票数据上工作:嘿，这太酷了…</p><p id="b883" class="kz la pd lb b lc ld ju le lf lg jx lh ps lj lk ll pt ln lo lp pu lr ls lt lu im bi translated">看到一个机器学习模型在自己的数据上工作:OMG这是什么魔法？？？</p></blockquote><p id="44bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">建模完成后(在时间允许的情况下)，是时候让它上线了。</p><h1 id="a5c1" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">应用构建和部署:Jupyter中的定制模型很酷，但在网络上直播更酷</h1><ul class=""><li id="6019" class="no np it lb b lc mx lf my li oc lm od lq oe lu of nu nv nw bi translated"><strong class="lb iu">成分:</strong>完全训练好的定制机器学习模型</li><li id="8c07" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">时间:</strong> 4天</li><li id="0cc5" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">成本:</strong> $14托管每天(我知道，我需要解决这个问题)</li><li id="874a" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">器具</strong> : Streamlit，Docker，Google容器注册，Google计算引擎</li></ul><p id="761a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习模型部署似乎仍然有点像黑暗艺术。</p><p id="993f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，在Jupyter笔记本上编码和写Python脚本是不一样的。然后你就有了你需要的不同的包，从数据科学库到web框架。然后你必须围绕你的模型建立某种用户可以与之交互的界面。一旦你做到了这一点，你在哪里举办呢？</p><p id="5a46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于任何初露头角的机器学习工程师来说，所有这些挑战都值得接受。</p><p id="88d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">毕竟，如果一个模型只存在于Jupyter笔记本中，那么它是否存在呢？</p><p id="07ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你像我一样是一个Jupyter笔记本战士，好消息是，Streamlit和Docker可以帮助你。</p><p id="b41b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.streamlit.io" rel="noopener ugc nofollow" target="_blank"> Streamlit </a>帮助你为你的机器学习和数据项目建立一个用户界面。更好的是，它也是用Python写的。如果你从未尝试过，花半天时间浏览所有的教程，你就可以搞定了。</p><p id="1ba4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在<a class="ae ky" href="https://docker.com" rel="noopener ugc nofollow" target="_blank">码头工人</a>。Docker可以帮助你将所有文件(Streamlit应用、机器学习模型和依赖项)打包成一个漂亮的小包(称为Docker映像)。一旦你得到了这个包，你可以把它上传到一个云服务器(例如Google Cloud ),一切正常，它应该完全像在你的本地系统上那样运行。</p><p id="2b94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以你让一个训练有素的模型在笔记本上做预测？您部署它的工作流程可能是这样的(我就是这样做的):</p><ol class=""><li id="c33d" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">创建一个类似于<code class="fe oi oj ok lw b">app</code>的文件夹。这是您的Streamlit应用程序(一个Python脚本)、模型工件和所有其他所需文件的位置。</li><li id="4120" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">在新文件夹中创建一个全新的Python环境。您需要安装准系统依赖项，以便您的应用程序在此环境中运行。在我的例子中，我需要Streamlit和Detectron2的依赖项。</li><li id="52b0" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">围绕您的模型构建一个Streamlit应用程序(Python脚本)。</li><li id="f206" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">让Streamlit应用程序在本地工作。这意味着你可以在电脑上互动和查看应用程序的工作情况。</li><li id="e3f0" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">为您的环境和应用程序文件夹创建Docker映像。</li><li id="cb6c" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">将您的Docker图片上传到Docker Hub或Docker托管服务，如Google Container Repository。</li><li id="d8cf" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">使用云提供商，如Heroku、Google Cloud或AWS来托管和运行您的Docker映像。</li></ol><p id="4d91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果一切顺利，您应该会得到一个指向您托管的应用程序的URL。</p><p id="1b42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我使用了谷歌的应用引擎，我的结果是这样的:<a class="ae ky" href="https://airbnb-amenity-detection.appspot.com/" rel="noopener ugc nofollow" target="_blank">airbnb-amenity-detection.appspot.com</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/625b3fa5f24464d9e695185cd7cc9fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/1*wSTSKuCITKLU9qVkWkpCpA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我的Streamlit应用程序在我的床的图像上运行的(加速)演示。参见YouTube 上的<a class="ae ky" href="https://youtu.be/smlQbh6jQvg" rel="noopener ugc nofollow" target="_blank">全面部署和应用构建视频。</a></p></figure><p id="0e4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们非常快速地介绍了应用程序的构建和部署。但是如果你有兴趣了解更多，我会查看以下资源(按顺序)。</p><ul class=""><li id="5082" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu of nu nv nw bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/how-docker-can-help-you-become-a-more-effective-data-scientist-7fc048ef91d5">Docker如何帮助你成为更高效的数据科学家</a>作者Hamel Husain</li><li id="096c" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><a class="ae ky" href="https://github.com/streamlit/streamlit/wiki/FAQ#q-how-do-i-deploy-streamlit-on-heroku-aws-google-cloud-etc" rel="noopener ugc nofollow" target="_blank">简化部署参考</a> (AWS、Heroku、Azure)</li><li id="0a74" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><a class="ae ky" href="https://blog.jcharistech.com/2020/01/14/how-to-deploy-streamlit-apps-to-google-cloud-platformgcp-app-engine/" rel="noopener ugc nofollow" target="_blank">如何将Streamlit应用部署到Google云平台</a>JarvisTech</li></ul><h1 id="e9d1" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">标准和评估:比较我的模型和Airbnb的</h1><p id="74c7" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">我以一系列我想要达到的标准开始了这个项目。</p><p id="380c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来复习一下。</p><ul class=""><li id="7fd2" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu of nu nv nw bi translated"><strong class="lb iu"> ✅的用户可以通过移动设备访问该应用。</strong>这种做法可行，但不可行。尽管如此，它还是会滴答作响。</li><li id="37ad" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">🚫击败或至少等同于Airbnb的50% mAP的MVP。</strong>我的所有型号都没有达到或超过这个门槛。所以这是一个失败。</li><li id="ca94" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu"> ✅修复了Airbnb使用的AutoML模型无法下载的痛点。我训练有素的Detectron2模型可供下载，任何人都可以使用。滴答。</strong></li><li id="43de" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">🚫通过某种方式找到模型最不确定的图像/类</strong>。你可以通过查看评估指标打印输出来做到这一点，但我想更多的是一个可视化功能，以实现对表现不佳的类/图像的主动学习方法。失败。</li><li id="1508" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">💰<strong class="lb iu">运行该模式的成本效益如何？</strong>对于这一个，让我们拿出记事本。</li></ul><p id="c0c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Airbnb 看了<a class="ae ky" href="https://youtu.be/tPb2u9kwh2w" rel="noopener ugc nofollow" target="_blank">一个关于机器学习的视频后，有人提到他们在网站上有超过5亿张图片(是的，超过5亿张)。对于我们的计算，让我们假设我们想要运行我们的模型跨越所有这些。</a></p><p id="2f9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用我训练的<em class="pd"> retinanet_R_101_FPN_3x </em>模型和我使用的<em class="pd"> </em> GPU(英伟达P100)对每张图像进行预测大约需要0.2s。</p><ul class=""><li id="b791" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu of nu nv nw bi translated">GPU成本:1.46美元/小时(美国中部GPU)，0.0004美元/秒</li><li id="115b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">推断时间:0.2s/张，5张/秒</li><li id="18ab" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">图像总数:5亿</li><li id="ebb8" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">总推理时间:100，000，000秒(500，000，000/5)</li><li id="ac6c" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">每幅图像的推理成本:0.000081美元</li><li id="dee5" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">推理总成本:40，555.55美元</li><li id="ccb7" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">总推理时间(1个GPU):27778小时(1160天)。</li></ul><p id="76c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，还需要更多的计算来看这个模型能增加多少价值。但是看到这些会让你知道Airbnb的所有图片需要多少概念证明。</p><p id="91bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很明显，在目前的状态下，在Airbnb的5亿多张图片上使用该模型可能是不可行的。</p><p id="a4ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这些数字也是使用谷歌云上1个租用的GPU计算出来的。如果一台本地机器有多个GPU，成本(和时间)可以大大减少。</p><p id="54cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后值得一看的对比是Airbnb在他们的文章中使用的最后一个。他们开始了他们的项目，作为一种建立他们自己的定制计算机视觉模型的方式，这将使他们免于使用第三方舒适性检测服务。</p><p id="c223" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们注意到，第三方舒适度检测服务只显示置信度超过0.5的预测结果(得分越高，模型对其预测越有信心)。所以为了进行公平的比较，他们修改了他们的Google AutoML模型来做同样的事情。</p><p id="b85a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样一来，Airbnb的Google AutoML模型的结果从<strong class="lb iu"> 68% </strong>的地图变成了<strong class="lb iu"> 46% </strong>。看到这一点，我用我的模型做了同样的事情，发现结果从<strong class="lb iu"> 43.2% </strong>到<strong class="lb iu"> 35.3% </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/e19ecb7e9cb8e0be4eaf5676d2e6f778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c19og7iwG4E2_P-cQwxv8A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从左到右:Airbnb的Google AutoML模型图在置信度0.5截断，第三方模型Airbnb试图以置信度0.5截断开始(来源:原创<a class="ae ky" href="https://medium.com/airbnb-engineering/amenity-detection-and-beyond-new-frontiers-of-computer-vision-at-airbnb-144a4441b72e" rel="noopener"> Airbnb舒适度检测文章</a>)，我训练的Detectron2模型在置信度0.5截断。<strong class="bd px">注意:</strong>这些模型不能完全比较，因为我的模型是在不同的数据集上训练的(没有内部图像)。这个形象只是为了给出一个发人深省的对比。<strong class="bd px">注2: </strong>至于我的条形图(最右边)，我想我已经非常接近复制原件了，你不觉得吗？</p></figure><h1 id="90ec" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">扩展、潜在改进和要点</h1><p id="c5db" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">当然，这个项目并不完美。但我没想到会是这样。我是厨师，不是化学家。</p><p id="f308" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我本想在42天结束前多做几件事，比如:</p><ul class=""><li id="3cdc" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu of nu nv nw bi translated"><strong class="lb iu">超参数调谐。</strong>尽管我的模型表现很好，但我并没有像我希望的那样做太多的超参数调整。如果我要这么做，我可能会求助于<a class="ae ky" href="https://www.wandb.com/sweeps" rel="noopener ugc nofollow" target="_blank">权重&amp;偏差扫描</a>(一种用于尝试、调整和跟踪超参数的工具)。有了调整后的超参数，谁知道呢，也许我的模型可以提高1-5%。</li><li id="a268" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">减少了推断时间。</strong>正如在计算模型的业务用例成本时所看到的，为了大规模运行(在Airbnb的所有图像上)，推理时间将需要大幅减少或使用更多的处理能力(更多的GPU)。在进行这个项目<a class="ae ky" href="https://ai.googleblog.com/2020/04/efficientdet-towards-scalable-and.html" rel="noopener ugc nofollow" target="_blank"> EfficientDet </a>时，发布了一个高效的对象检测算法。所以这可能是我接下来要尝试的。</li><li id="b99e" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">下载时创建的标签。</strong>预处理Open Images数据以使用Detectron2需要几个步骤，这些步骤在下载图像时可能会自动执行。</li><li id="b589" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">一个不那么花哨的应用(是我的错，不是Streamlit的)。我仍然习惯于部署我在Jupyter笔记本上写的代码。将来，我还想拥有自己的服务器，可以将Docker容器推送给其他人并与其他人共享。</strong></li><li id="d007" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated">再培训渠道。如果添加了更多图像或类别，我如何轻松地重新训练模型？现在，这几乎需要重新运行整个项目。</li><li id="ebe5" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">检验功能。</strong>如果我想检查数据并查看厨房&amp;餐桌的所有图像或预测低于某个置信度阈值的所有图像，我不能。这些类型的可视化功能对于进一步的数据探索是至关重要的(也许应该从一开始就构建)。</li><li id="b59e" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu of nu nv nw bi translated"><strong class="lb iu">哪些图像中有人物？</strong>因为我用的是开放图片，所以很多图片里面都有人物。级联模型可以用来过滤掉不想要的图像风格，只关注与Airbnb用例相关的图像。</li></ul><p id="3624" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">综上所述，这个项目实现了它的主要目标:边做边学。</p><p id="510a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我学到了一些我以前从未使用过的工具，更重要的是，我可以如何构建未来的项目。</p><p id="6d35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主要的收获是，实验，实验，实验。</p><h1 id="fbd4" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">看看是怎么做的</h1><p id="dca5" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">尽管这篇文章很长，但它只是我在这个项目中采取的实际步骤的一个简短概述。</p><p id="c30c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想看到它们全部展开，你可以阅读观念中的<a class="ae ky" href="https://dbourke.link/airbnb42days" rel="noopener ugc nofollow" target="_blank">我的项目笔记(日志风格)。</a></p><p id="e5f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者观看我创建的YouTube系列来配合它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="py om l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我创建的YouTube系列的第8部分是为了配合这个项目。</p></figure></div></div>    
</body>
</html>