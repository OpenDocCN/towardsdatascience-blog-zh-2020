<html>
<head>
<title>The Mechanics of Attention Mechanism in Flowcharts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">流程图中的注意机制</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-mechanics-of-attention-mechanism-f6e9805cca66?source=collection_archive---------22-----------------------#2020-02-24">https://towardsdatascience.com/the-mechanics-of-attention-mechanism-f6e9805cca66?source=collection_archive---------22-----------------------#2020-02-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9b5d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak"> TLDR </strong>:这基本上是关于将<a class="ae kf" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">yo shua beng io小组的原始注意文件</a>【1】转换成流程图。查看附录前的最后一张图，了解完整的流程图。</h2></div><p id="2197" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><em class="lc">认知状态:我试图正确理解注意力机制，在这个层面上，我知道如何对任何类型的数据/问题或任何形式实施它，以及如何调整它以改善它。这篇文章就像是我自己教自己的笔记。最后，由于我自己没有实现注意力机制，所以我写这篇文章时不够资格，也过于固执己见。</em></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/cd0ddbee14af17b7963858379a3acae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7UDWIX88K5nVHqGJrpeE2w.jpeg"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">注意。Justin Chrn 在<a class="ae kf" href="https://unsplash.com/s/photos/attention?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="7cdf" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">动机:<a class="ae kf" rel="noopener" target="_blank" href="/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f">网上大部分解释</a><a class="ae kf" href="https://medium.com/heuritech/attention-mechanism-5aba9a2d4727" rel="noopener"/><a class="ae kf" href="https://hackernoon.com/attention-mechanism-in-neural-network-30aaf5e39512" rel="noopener ugc nofollow" target="_blank">做好</a> <a class="ae kf" href="https://pathmind.com/wiki/attention-mechanism-memory-network" rel="noopener ugc nofollow" target="_blank">在解释</a> <a class="ae kf" href="https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/" rel="noopener ugc nofollow" target="_blank">什么注意力机制</a> <a class="ae kf" href="https://www.kdnuggets.com/2019/08/deep-learning-transformers-attention-mechanism.html" rel="noopener ugc nofollow" target="_blank">是关于</a>的，但不是它如何工作的确切的机械细节，比如哪个向量跟哪个矩阵走。虽然原始论文做得很好，但它就像一个非常浓缩的版本，典型的学术写作，这意味着我需要很大的努力来解开它。这是那个。</p><h1 id="77f7" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">简要背景</h1><p id="af27" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">原始论文正在处理一个特定的<a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP) </a>问题，这是一个<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_translation" rel="noopener ugc nofollow" target="_blank">机器语言翻译任务</a>，将一个句子从源语言翻译成目标语言。它被制定为一种<a class="ae kf" href="https://en.wikipedia.org/wiki/Seq2seq" rel="noopener ugc nofollow" target="_blank">编码器-解码器序列到序列(seq2seq) </a>架构，通常使用<a class="ae kf" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank"> RNN </a>的变体来完成，如<a class="ae kf" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank"> LSTM </a>和<a class="ae kf" href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" rel="noopener ugc nofollow" target="_blank"> GRU </a>，它们是一类<a class="ae kf" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>算法，是<a class="ae kf" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">人工神经网络(ANN) </a>的一部分，是人工智能(AI)的<a class="ae kf" href="https://medium.com/datadriveninvestor/how-is-machine-learning-different-and-why-it-is-better-d5671b52dd65" rel="noopener">数据驱动/机器学习范式，而不是模型驱动范式</a>。本文的其余部分假设您熟悉这些概念。</p><h1 id="c596" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">香草RNN</h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mq"><img src="../Images/5d5e0549005a85c86371d9eaf0181890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b7FBYUDKjDED9tGsdzW_MA.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">香草RNN编码器单元。</p></figure><p id="9924" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">巴赫达瑙等人。艾尔。通过以下等式描述了典型的RNN编码器单元。我用下图解开了这个等式。x_t是一个用向量表示的<a class="ae kf" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank">字。函数f(。)通常是某种类型的RNN，像LSTM和格鲁。在论文中，他们使用的是</a><a class="ae kf" href="https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks" rel="noopener ugc nofollow" target="_blank">双向RNN </a> (BiRNN)。最后，h_t是RNN隐藏态。</p><p id="c347" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">完整的编码器架构如下，其中c是对整个句子进行编码的向量。通常，c = h_T，这基本上只是最后一个RNN隐藏态。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mr"><img src="../Images/8017527399b8b88b1b3e71336454d835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BGabK2oqtXVUlx9aU0CN0A.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">普通RNN编码器(T_x=4)</p></figure><p id="c559" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">下面是一个典型的编码器-解码器seq2seq RNN的完整结构，翻译中有夸大的错误。想象一种语言，其中关于距离(这里对那里)、可数性(<a class="ae kf" href="https://en.wikipedia.org/wiki/Count_noun" rel="noopener ugc nofollow" target="_blank">多</a>对<a class="ae kf" href="https://en.wikipedia.org/wiki/Mass_noun" rel="noopener ugc nofollow" target="_blank">多</a>)和<a class="ae kf" href="https://en.wikipedia.org/wiki/Grammatical_gender" rel="noopener ugc nofollow" target="_blank">性别</a>/年龄/ <a class="ae kf" href="https://en.wikipedia.org/wiki/Grammatical_number" rel="noopener ugc nofollow" target="_blank">数量</a>(男人对女孩)<a class="ae kf" href="https://en.wikipedia.org/wiki/Synthetic_language" rel="noopener ugc nofollow" target="_blank">的信息包含在一个单词</a>中，被翻译成<a class="ae kf" href="https://en.wikipedia.org/wiki/Analytic_language" rel="noopener ugc nofollow" target="_blank">另一种语言，其中这些信息必须被分割成多个单词</a>，这些单词甚至可能彼此不相邻，反之亦然。此外，还有动词“to be”的问题，这是一个在所有语言中都不通用的特征。然后，我们可能会看到这类错误。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ms"><img src="../Images/8d3f602e500eb96e887eaf29cf96f42e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*30vN9_ixWjb9sLWrIObHig.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">典型编码器-解码器的结构seq2seq RNN用机器发现翻译困难的夸张例子。不同的字体(字样)显示不同的语言(包括编码)。</p></figure><p id="36d3" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">(我在假装函数g(。)产生下一个单词(技术上来说是单词向量，因为是嵌入的，但我打算从现在开始就称它为“单词”)。在本文中，它实际上是所有单词的概率向量。但这不是注意力机制的重要区别。)</p><h1 id="cc21" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">香草RNN的局限</h1><p id="c2ff" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">由于注意力机制只应用于解码器部分，所以我们可以忽略编码器部分，而专注于H也就是所有h_j的集合(原论文中有一个符号变化，所以我在这里反映出来。“j”是编码器部分的索引，“I”是解码器部分的索引，以前是“t”。)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mt"><img src="../Images/fb60b20b670bad518a4884463770aebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1CxWeGB0pbR16_yOHuKldA.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">左图:典型的RNN解码器。右图(绿框):典型RNN解码器的一个单元。</p></figure><p id="d58f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">首先，在注意机制中，我们将使用H，所有h_j(所有隐藏状态的集合)的集合，而不仅仅是最后一个，所以让我们保持在那里。其次，为了简化起见，我们将重点关注一个RNN解码器单元，如上图右侧所示。</p><p id="cdbf" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">函数g(.)(一个RNN单元)主要负责产生下一个单词(向量，嵌入的)y_i，这个决定是通过结合来自c(代表原语言中的<strong class="ki ir">整句</strong>)、y_i(前一个单词)、s_i(前一个隐藏状态)的信息做出的。</p><p id="0867" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">作者意识到的是，使用“c”是一种非常糟糕的做事方式。首先，当一个人试图翻译下一个单词时(假设他们已经弄清楚了<a class="ae kf" href="https://en.wikipedia.org/wiki/Word_order" rel="noopener ugc nofollow" target="_blank">的语序</a>，<strong class="ki ir">，他们只会关注(阅读:注意)一个(最多几个)单词</strong>，而不是整个句子。然而，通过使用“c”作为输入，网络/模型的体系结构在某种程度上暗示我们应该试图从对整个句子的某种整体理解中找出下一个单词，但事实绝对不是这样。</p><p id="7755" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">(为什么是几个字而不是一个？根据语言的不同(特别是当从一种<a class="ae kf" href="https://en.wikipedia.org/wiki/Analytic_language" rel="noopener ugc nofollow" target="_blank">分析语言</a>翻译成综合语言的时候)，我们必须根据上下文来决定使用哪种形式。许多语言根据许多不同的语法方面改变形式，如单数/复数、男性/女性/中性、过去/现在/未来等。例如，想象从一种第三人称代词不区分性别的语言翻译过来，(像<a class="ae kf" href="https://en.wiktionary.org/wiki/dia#Indonesian" rel="noopener ugc nofollow" target="_blank">印度尼西亚语“dia”</a>翻译成英语he/she)。网络必须<strong class="ki ir">注意</strong>单词“dia”<strong class="ki ir">和</strong>任何可能给出问题人物性别线索的单词，以便正确翻译。)</p><h1 id="7977" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">注意力的要点</h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mu"><img src="../Images/f412987c108b4f66744c2cc4c65da66e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AS4kik4PIG8Wo5zhuo2HNw.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">注意机制使用通过计算解码器RNN隐藏状态(红色箭头)和h中的每个注释向量之间的对齐而创建的唯一上下文向量c_i(而不仅仅是一般的“c”)</p></figure><p id="92bf" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">注意的要点基本上是用一个上下文向量c_i来代替一般的“c ”,而不是每一步都给解码器一个相同的“c ”,我们会在每一步给解码器一个不同的c_i。希望这个c_i给予<strong class="ki ir">注意</strong>源句子中的相关单词。</p><p id="dfbc" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">为了做到这一点，函数c(.)需要关于原始句子的详细信息(因此我们使用注释向量H，而不仅仅是最后的h_j)和当前解码器RNN隐藏状态s_i(因此红色箭头)。理想情况下，上下文向量c_i将包含关于相关单词的大量信息，以及关于其他不太相关的单词的少量信息。</p><h1 id="21f4" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">解码器实际上是如何实现的</strong></h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mv"><img src="../Images/c4b827b576e34f54cad2b6176a4653d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0Hx7RkwIGtJSzPzCmAY-w.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">三个模块，每个输出一个:c(.)，f(。)，g(。).</p></figure><p id="5191" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">每个解码步骤有3个重要输出:</p><ol class=""><li id="7948" class="mw mx iq ki b kj kk km kn kp my kt mz kx na lb nb nc nd ne bi translated">y_i，目标语言中的一个单词。</li><li id="41e5" class="mw mx iq ki b kj nf km ng kp nh kt ni kx nj lb nb nc nd ne bi translated">s_i，当前解码器当前时间步的RNN隐藏状态。</li><li id="c06c" class="mw mx iq ki b kj nf km ng kp nh kt ni kx nj lb nb nc nd ne bi translated">c_i，源句子的加权表示(权重基于注意力)。</li></ol><p id="f379" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">[我不确定这是否是真正的动机，但是…]有3个不同的模块是有意义的，其中每个模块负责每个输出:c(。)，f(。)，还有g(。):</p><ul class=""><li id="79f3" class="mw mx iq ki b kj kk km kn kp my kt mz kx na lb nk nc nd ne bi translated">c(H，si-1)= c _ I</li><li id="c9b3" class="mw mx iq ki b kj nf km ng kp nh kt ni kx nj lb nk nc nd ne bi translated">f(c_i，s_i-1，y_i-1) = s_i</li><li id="159c" class="mw mx iq ki b kj nf km ng kp nh kt ni kx nj lb nk nc nd ne bi translated">g(c_i，s_i，y_i-1) = y_i</li></ul><h1 id="95b9" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">最后，注意！</h1><p id="56f6" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">最后一个问题是，如何计算上下文向量c_i？这可以通过下面的完整模型得到最好的解释(根据注释，我希望这是不言自明的)。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nl"><img src="../Images/466fe1f4ce5ad35c17b5971c805cebf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3c0B0_XZNDFYMemxC2PD5w.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">注意机制的完整模型。方程式转录成流程图。</p></figure><p id="8fd7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">最重要的部分是比对分数e_i，j(出于某种我不明白的原因，也被称为能量)。它基于当前解码器RNN隐藏状态计算源语言中每个单词的相关性(由注释h_j表示)。然后，我们给更相关的单词更大的权重，给不太相关的单词更小的权重。这相当于把<strong class="ki ir">的注意力</strong>给了更相关的词。下一步只是使用softmax“标准化”权重。(出于某种我不明白的原因，等同于概率)。最后，我们计算源语言句子中所有单词的加权平均值，得到上下文向量c_i。</p><h1 id="9191" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">附录</h1><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nm"><img src="../Images/d87c579e9c38ad0c26a657fb7fddac0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VTEJ-bm13xZgLBkYkIo1lQ.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">将我们的流程图与原始文件中的简化图进行比较。</p></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/0bfa8330436b7f3abac5c8795fd2e716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*YtsodA6LcX8a4jFtEBFW9w.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">对齐矩阵。(来自原论文)</p></figure><p id="9270" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">要获得著名的比对矩阵图，只需绘制所有的a_i，j，其中I是列，j是行。</p><h2 id="2a82" class="no lu iq bd lv np nq dn lz nr ns dp md kp nt nu mf kt nv nw mh kx nx ny mj nz bi translated">软注意和硬注意</h2><p id="2be4" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">这里描述的注意机制可以被认为是软注意，因为它使用的是softmax。如果使用最大值(。)函数，那么c_i将只是最相关的单词向量。这可能不太有利，因为一些单词与另一种语言中的多个单词对齐。更重要的是，我们不能通过max(使用反向传播。)，而我们可以用softmax做同样的事情。</p><h1 id="6e84" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考文献和致谢</h1><p id="aebb" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">[1] D. Bahdanau，K. Cho和Y. Bengio，<a class="ae kf" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">联合学习对齐和翻译的神经机器翻译</a> (2015)，学习表征国际会议(ICLR)</p><p id="eadc" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">感谢<a class="ae kf" href="https://www.sichenzhao.com/" rel="noopener ugc nofollow" target="_blank">陈思</a>教会我注意力是如何工作的。</p><p id="9d6f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">所有图像属于我，除非在标题中另有说明。</p></div></div>    
</body>
</html>