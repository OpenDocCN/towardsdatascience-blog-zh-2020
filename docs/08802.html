<html>
<head>
<title>Feedforward Networks — Part 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">前馈网络—第4部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed?source=collection_archive---------75-----------------------#2020-06-24">https://towardsdatascience.com/lecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed?source=collection_archive---------75-----------------------#2020-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="de38" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/fau-lecture-notes" rel="noopener" target="_blank"> FAU讲座笔记</a>深度学习</h2><div class=""/><div class=""><h2 id="bc2f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">层抽象</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d2452570fc97ff285e9f00b75a702e55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GFxZEz1ilQQv5JRi.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">FAU大学的深度学习。下图<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a></p></figure><p id="af42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">这些是FAU的YouTube讲座</strong> <a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">深度学习</strong> </a> <strong class="lk jd">的讲义。这是与幻灯片匹配的讲座视频&amp;的完整抄本。我们希望，你喜欢这个视频一样多。当然，这份抄本是用深度学习技术在很大程度上自动创建的，只进行了少量的手动修改。如果你发现了错误，请告诉我们！</strong></p><h1 id="0318" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">航行</h1><p id="b39a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/lecture-notes-in-deep-learning-feedforward-networks-part-3-d2a0441b8bca"> <strong class="lk jd">上一讲</strong> </a> <strong class="lk jd"> / </strong> <a class="ae lh" href="https://youtu.be/BTbHaKsH4y0" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">观看本视频</strong> </a> <strong class="lk jd"> / </strong> <a class="ae lh" rel="noopener" target="_blank" href="/all-you-want-to-know-about-deep-learning-8d68dcffc258"> <strong class="lk jd">顶级</strong> </a> <strong class="lk jd"> / </strong> <a class="ae lh" rel="noopener" target="_blank" href="/lecture-notes-in-deep-learning-loss-and-optimization-part-1-f702695cbd99"> <strong class="lk jd">下一讲</strong> </a></p><p id="135c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">欢迎大家观看我们下一个关于深度学习的视频！所以，今天我们要再次讨论前馈网络。在第四部分中，主要的焦点将放在层抽象上。当然，我们讨论了这些神经元和单个节点，但对于更大的网络来说，这变得非常复杂。所以我们想在梯度的计算中引入这个分层的概念。这真的很有用，因为我们可以直接谈论整个层的梯度，而不需要去所有不同的节点。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ab905eac031d33b21b878eb802d15799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*seocefnQVY4pPzpq.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">每个神经元在应用非线性之前计算两个向量的内积。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的4.0 </a>。</p></figure><p id="07bc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么，我们该如何表达呢？让我们回忆一下我们的单个神经元在做什么。单个神经元本质上是在计算其权重的内积。顺便说一下，我们跳过了这个偏差符号。所以，我们将这个向量扩展了一个额外的元素。这使我们能够描述偏差和内积，如幻灯片所示。这真的很好，因为这样你就可以看到输出<em class="nb"> y </em> hat只是一个内积。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/8a67aa22608ea5dae2778d815807b52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jVxLB-67WXOF-Bv7.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">完全连接的层可以用矩阵乘法来表示。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>下的图片。</p></figure><p id="6a25" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在考虑我们有<em class="nb"> M </em>个神经元的情况，这意味着我们得到一些<em class="nb"> y </em>帽子指数<em class="nb"> m </em>。都是内积。所以，如果你把这个带入向量符号，你可以看到向量<strong class="lk jd"> y </strong> hat就是矩阵<strong class="lk jd"> x </strong>与矩阵<strong class="lk jd"> W </strong>的乘法运算。你看一个全连通的层不是别的，就是矩阵乘法。因此，我们基本上可以使用这个全连接层来表示任意连接和拓扑。然后，我们还应用逐点非线性，以便我们得到非线性效果。矩阵符号的好处是，我们现在可以用矩阵运算来描述整个层的导数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f636e28206729a7fd272cb2fbc70db9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tzbKItEq4TtdSEDT.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">具有三个输入的双神经元全连接层。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的4.0 </a>。</p></figure><p id="2390" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，我们的全连接层将得到以下配置:三个输入元素和每个神经元的权重。假设你有两个神经元，然后我们得到这些权重向量。我们将两者乘以<strong class="lk jd"> x </strong>。在正向传递中，我们已经使用矩阵为整个模块确定了这个<strong class="lk jd"> y </strong> hat。如果你想计算梯度，那么我们需要两个偏导数。这些和我们已经提到的一样，我们需要关于重量的导数。这将是相对于<strong class="lk jd"> W </strong>的偏导数和相对于<strong class="lk jd"> x </strong>的偏导数，用于反向传播，以将其传递到下一个模块。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/4b2ac58fd641faf51dd980e1c00ddd4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kfPmSSMnWBkGJXrE.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">简而言之，全连通层的偏导数。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的4.0 </a>。</p></figure><p id="a7a5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么我们如何计算呢？嗯，我们有一层是<strong class="lk jd"> y </strong>帽子等于<strong class="lk jd">W</strong>T8】x。所以在正向传递中有一个矩阵乘法。然后，我们需要对重量求导。现在你可以看到，我们需要做的是，我们需要一个矩阵导数。y相对于w的导数就是ᵀ.因此，如果我们有损失进入我们的模块，我们的权重更新将是这个损失向量乘以<strong class="lk jd"> x </strong> ᵀ.所以，我们有一些损失向量和ᵀ，这意味着你有一个外积。由于转置，一个是列向量，另一个是行向量。所以，如果你把两者相乘，你会得到一个矩阵。上述相对于<strong class="lk jd"> W </strong>的偏导数将总是产生一个矩阵。然后你看最下面一行，你需要<strong class="lk jd"> y </strong> hat相对于<strong class="lk jd"> x </strong>的偏导数。顺便说一句，你也可以在<a class="ae lh" href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" rel="noopener ugc nofollow" target="_blank">矩阵食谱</a>中找到。它非常非常有用。你可以在这本书里找到各种矩阵导数。所以如果你这样做，你可以看到上面的方程，相对于<strong class="lk jd"> x </strong>的偏角是<strong class="lk jd"> W </strong> ᵀ.现在，你有了ᵀ，再乘以一些损失向量。这个损失向量乘以一个矩阵，会变成一个向量。这是您将在反向传播过程中传递给下一个更高层的向量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/cc2a7529a6703a53a3d60ed58d06fcd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EX-OxM7lUJ6D72TS.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">单层线性网络仅仅是矩阵乘法。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的4.0 </a>。</p></figure><p id="4f72" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好，让我们来看一些例子。我们先有一个简单的例子，然后是一个多层的例子。因此，这个简单的例子将是我们已经拥有的同一个网络。这是一个没有任何非线性的网络<strong class="lk jd"> W </strong> <strong class="lk jd"> x </strong>。现在，我们需要一些损失函数。这里，我们不取交叉熵，而是取L2损失，这是一个常见的向量范数。它只需将网络输出减去所需输出，然后计算L2范数。这意味着我们对不同的向量值进行元素平方，并将它们相加。最后，我们将不得不取一个平方根，但是我们想省略它。所以，我们用它的2次方。当我们现在计算这个L2范数的2次幂的导数时，当然，我们有一个2的因子出现。这将在开始时被这个因子1/2抵消。对了，这是回归损失，也有统计关系。我们将在更详细地讨论损失函数时讨论这一点。L2损失的好处是，你还可以找到它的矩阵衍生物矩阵食谱。我们现在计算L相对于<strong class="lk jd"> y </strong> hat的偏导数。这将为我们提供<strong class="lk jd"> Wx </strong> — <strong class="lk jd"> y </strong>，我们可以继续计算权重的更新。所以权重的更新是我们用损失函数的导数计算出来的。损失函数相对于输入的导数是<strong class="lk jd"> Wx </strong> — <strong class="lk jd"> y </strong>乘以<strong class="lk jd"> xᵀ </strong>。这将为我们提供矩阵权重的更新。我们要计算的另一个导数是损失相对于<strong class="lk jd"> x </strong>的偏导数。因此，正如我们在上一张幻灯片中看到的那样，这将是<strong class="lk jd"> W </strong> ᵀ乘以来自损失函数的向量:<strong class="lk jd"> Wx </strong> — <strong class="lk jd"> y，</strong>，正如我们在幻灯片的第三行中所确定的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/0047b48aed3caf538911cb2a84febfc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FQ5BIkCtS7ghPJ-Q.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">线性三层网络作为逐层导数计算的例子。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的4.0 </a>。</p></figure><p id="7346" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好吧，让我们添加一些层，并改变我们的估计到三个嵌套函数。这里，我们有一些线性矩阵。所以，这是一个学术上的例子:你可以看到，通过将₁的<strong class="lk jd"> W </strong>、₂的<strong class="lk jd"> W </strong>和₃的<strong class="lk jd"> W </strong>相乘，它们会简单地折叠成一个矩阵。尽管如此，我还是觉得这个例子很有用，因为它向您展示了在反向传播过程的计算中实际发生了什么，以及为什么这些特定的步骤真的很有用。所以，我们再次采用L2损失函数。这里，我们有三个矩阵。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/ba250e889fc8c94aec9274e7980a5a5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VcsqOceDqhu5Knnp.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">最后一层的导数只需要链式法则的一次应用。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的4.0CC下的图片。</p></figure><p id="f066" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们要继续计算导数。现在对于衍生产品，我们从第三层开始，最外层。你可以看到，我们现在计算损失函数相对于₃.的偏导数第一，链式法则。然后，我们必须计算损失函数相对于f₃( <strong class="lk jd"> x </strong>哈特相对于₃.<strong class="lk jd">w</strong>的偏导数损失函数的偏导数也是L2范数的内部部分。那么这个<strong class="lk jd">w₃</strong>t18】w₂<strong class="lk jd">w</strong>₁<strong class="lk jd">x</strong>——<strong class="lk jd">y</strong>也是如此。网络的偏导数将会是(<strong class="lk jd">w</strong>₂<strong class="lk jd">w</strong>₁<strong class="lk jd">x</strong>)<strong class="lk jd">ᵀ</strong>，正如我们在上一张幻灯片中看到的。注意，我用一个点来表示矩阵操作符的亲和力。对于矩阵来说，是从左乘还是从右乘是有区别的。两个乘法方向都不一样。因此，我表示你必须从右边计算这个乘积。现在，让我们这样做，我们结束了对W₃的最后更新，这是简单地从这两个表达式计算出来的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/d31e8ac6a2052133ebab838946076eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Fk__OQ0VhQojTbxd.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">更深层次的衍生工具需要多重应用链式法则。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的4.0 </a>。</p></figure><p id="7745" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，对₂的偏导数有点复杂，因为我们要应用链式法则两次。所以，我们必须再次计算损失函数对f₃(<strong class="lk jd">x</strong>hat的偏导数。然后，我们需要f₃(<strong class="lk jd">x</strong>hat对<strong class="lk jd"> W </strong> ₂的偏导数，这意味着我们必须再次应用链式法则。所以我们必须展开f₃(<strong class="lk jd">x</strong>hat相对于f₂(<strong class="lk jd">x</strong>hat的偏导数，然后展开f₂(<strong class="lk jd">x</strong>hat相对于₂.<strong class="lk jd">w</strong>的偏导数这个变化不大。损失项与我们以前使用的相同。现在，如果我们计算f₃(<strong class="lk jd">x</strong>hat相对于f₂(<strong class="lk jd">x</strong>hat的偏导数——还记得f₂(<strong class="lk jd">x</strong>)=<strong class="lk jd">w</strong>₂<strong class="lk jd">w</strong>₁<strong class="lk jd">x</strong>吗——它将是<strong class="lk jd"> W </strong> ₃ <strong class="lk jd"> ᵀ </strong>,我们必须从左侧将其相乘。然后，我们继续计算f₂(<strong class="lk jd">x</strong>hat相对于<strong class="lk jd">w</strong>₂.的偏导数你继续留在(<strong class="lk jd">w</strong>₁<strong class="lk jd">x</strong>)<strong class="lk jd">ᵀ</strong>。所以，最终的矩阵导数是这三项的乘积。我们可以对最后一层重复这个步骤，但是现在我们必须再次应用链式法则。我们已经看到预先计算的两个部分，但我们必须再次应用它。这里我们得到了f₂(<strong class="lk jd">x</strong>hat相对于f₁(<strong class="lk jd">x</strong>hat的偏导数，以及f₁(<strong class="lk jd">x</strong>hat相对于₁<strong class="lk jd">w</strong>的偏导数，这就产生了我们之前用过的两个项。f₂( <strong class="lk jd"> x </strong>帽子相对于f₁( <strong class="lk jd"> x </strong>帽子的偏导数，即<strong class="lk jd"> W </strong> ₁ <strong class="lk jd"> x </strong>，将会是<strong class="lk jd"> W </strong> ₂ <strong class="lk jd"> ᵀ </strong>。然后，我们还要计算f₁( <strong class="lk jd"> x </strong>相对于<strong class="lk jd"> W </strong> ₁.的偏导数这将是xᵀ的比赛。所以，我们最终得到这个偏导数的四项乘积。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/f45e7304163ba8633fe46fb000207e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*F-PhXvK6DWKwfgFI.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">偏导数越深，需要相乘的项就越多。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的4.0 </a>。</p></figure><p id="5139" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，你可以看到，如果我们做反向传播算法，我们会以非常相似的方式处理。首先，我们计算整个网络的转发路径，并评估损失函数。然后，我们可以看看不同的偏导数，根据我想去的地方，我必须计算各自的偏导数。对于最后一层的更新，我必须计算损失函数的偏导数，并将其乘以最后一层相对于权重的偏导数。现在，如果我进入倒数第二层，我必须计算损失函数的偏导数、最后一层相对于输入的偏导数以及倒数第二层相对于权重的偏导数，以获得更新。如果我想去第一层，我必须计算整个层的所有相应的反向传播步骤，直到我在第一层上完成相应的更新。您可以看到，我们可以预先计算许多这些值，并重复使用它们，这使我们能够非常有效地实现反向传播。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/959a751d4fb3b54f4c4248c528e75ce8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*avc_UvKsO96cGsnD.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">偏导数的分层计算总结。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>下的图片。</p></figure><p id="ed3f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们总结一下到目前为止我们所看到的。我们已经看到，我们可以将softmax激活函数与交叉熵损失结合起来。然后，我们可以很自然地处理多类问题。我们使用梯度下降作为训练网络的默认选择，并且我们可以使用该策略实现局部最小值。当然，我们只能通过有限差分来计算梯度，这对于检查你的实现非常有用。这是你在练习中绝对需要的东西！然后，我们使用反向传播算法非常有效地计算梯度。为了能够更新完全连接的层的权重，我们已经看到它们可以被抽象为一个完整的层。因此，我们也可以计算逐层导数。因此，不需要在节点级别计算所有内容，但您可以真正进入层抽象。你也看到了矩阵演算非常有用。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/9921c3db20fd9544a29a206e2bc0543e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NTYsXDbUV6H0eGi7.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在这个深度学习讲座中，更多令人兴奋的事情即将到来。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的4.0 </a>。</p></figure><p id="ecb8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">深度学习下一次会怎样？我们会看到，现在，我们只有有限的损失函数。因此，我们将看到问题适应回归和分类的损失函数。我们现在讨论的非常简单的优化，只有一个η，可能不是正确的方法。所以，有更好的优化程序。它们可以适应每一个参数的需要。然后，我们还将看到一个论点，为什么神经网络不应该表现得那么好，以及一些最近的见解，为什么它们实际上表现得相当好。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d385a8927f6ea0ba479aa704a8f653b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Op5Up-s3pSgHkWv7.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">一些备考的综合题。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>下的图片。</p></figure><p id="bcb0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我也有几个综合问题。所以你绝对应该能够为多类分类命名不同的损失函数。如果你想和我一起参加口试，一键编码是每个人都需要知道的。你必须能够描述这一点。然后，当然，我可能不会在考试中问，但对你的日常工作非常有用的事情是，你使用有限差分，并使用它们进行实现检查。你必须能够描述反向传播算法，老实说，我认为这是一种学术方法，但这种描述反向传播算法的多层抽象方法非常有用。如果你想解释考试情况下的反向传播，这也很好。你还能描述出什么？爆炸和消失梯度的问题是:如果你选择的η太高或太低会发生什么？什么是丢失的曲线，它在迭代中是如何变化的？看看这些图表。它们真的很重要，它们还能帮助你了解你的训练过程中出了什么问题。所以你需要意识到这些，现在你也应该清楚为什么符号函数不是激活函数的好选择。这篇文章下面有很多参考资料。所以，我希望你仍然喜欢这些视频。请继续观看，下期视频再见！</p><p id="0c0d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你喜欢这篇文章，你可以在这里找到更多的文章，或者看看我们的讲座。如果你想在未来了解更多的文章、视频和研究，我也会很感激你在YouTube、Twitter、脸书、LinkedIn上的鼓掌或关注。本文以<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/deed.de" rel="noopener ugc nofollow" target="_blank"> Creative Commons 4.0归属许可</a>发布，如果引用，可以转载和修改。</p><h1 id="78be" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">参考</h1><p id="3d77" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">[1] R. O .杜达，P. E .哈特和D. G .斯托克。模式分类。约翰威利父子公司，2000年。<br/> [2]克里斯托弗·m·毕晓普。模式识别和机器学习(信息科学和统计学)。美国新泽西州Secaucus出版社:纽约斯普林格出版社，2006年。<br/>[3]f·罗森布拉特。"感知器:大脑中信息存储和组织的概率模型."摘自:《心理评论》65.6 (1958)，第386-408页。<br/>【4】WS。麦卡洛克和w .皮茨。"对神经活动中固有思想的逻辑演算."发表于:数学生物物理学通报5 (1943)，第99-115页。[5] D. E .鲁梅尔哈特、G. E .辛顿和R. J .威廉斯。"通过反向传播误差学习表征."载于:自然323 (1986)，第533-536页。<br/> [6]泽维尔·格洛特，安托万·博德斯，约舒阿·本吉奥。“深度稀疏整流器神经网络”。《第十四届国际人工智能会议论文集》第15卷。2011年，第315-323页。<br/> [7]威廉·h·普雷斯、索尔·a·特乌考斯基、威廉·t·维特林等《数值计算方法》第三版:科学计算的艺术。第三版。美国纽约州纽约市:剑桥大学出版社，2007年。</p></div></div>    
</body>
</html>