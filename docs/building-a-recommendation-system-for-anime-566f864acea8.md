# æ„å»ºåŠ¨æ¼«æ¨èç³»ç»Ÿ

> åŸæ–‡ï¼š<https://towardsdatascience.com/building-a-recommendation-system-for-anime-566f864acea8?source=collection_archive---------31----------------------->

## æˆ‘å·²ç»å†³å®šåšä¸€ä¸ªç®€å•çš„åŠ¨æ¼«æ¨èç³»ç»Ÿã€‚

## èƒŒæ™¯èµ„æ–™

åŠ¨æ¼«æ˜¯ä¸€ç§æºè‡ªæ—¥æœ¬çš„æ‰‹ç»˜ç”µè„‘åŠ¨ç”»ï¼Œåœ¨ä¸–ç•Œå„åœ°å¸å¼•äº†å¤§æ‰¹è¿½éšè€…ã€‚åŠ¨ç”»äº§ä¸šç”± 430 å¤šå®¶å…¬å¸ç»„æˆã€‚å£è¢‹å¦–æ€ªå’Œæ¸¸æˆç‹æ˜¯è¥¿æ–¹ç”µè§†ä¸Šæœ€å—æ¬¢è¿çš„åŠ¨æ¼«èŠ‚ç›®ã€‚ç”±å®«å´éªåˆ›ä½œå¹¶ç”±å‰åœåŠ›å·¥ä½œå®¤åˆ¶ä½œåŠ¨ç”»çš„ã€Šåƒä¸åƒå¯»ã€‹æ˜¯åŠ¨ç”»ç”µå½±ä¸­ç¥¨æˆ¿æœ€é«˜çš„ä¸€éƒ¨ã€‚å®ƒåœ¨è¥¿æ–¹å¦‚æ­¤å—æ¬¢è¿çš„åŸå› æ˜¯å®«å´éªçš„ä¸€ä¸ªå¥½æœ‹å‹è¯´æœä»–å°†å‘è¡Œæƒå–ç»™åç‰¹Â·è¿ªå£«å°¼ã€‚åƒ*åƒä¸åƒå¯»*ï¼Œæœ‰æ•°ä»¥åƒè®¡çš„çœŸæ­£å¥½çš„åŠ¨ç”»ç”µå½±å’ŒèŠ‚ç›®æ˜¯ç”±åŒä¸€å®¶åŠ¨ç”»å…¬å¸åˆ¶ä½œçš„ã€‚è®¸å¤šå…¶ä»–äººå¯ä»¥ä»¥æ­¤ä¸ºä¾‹ï¼Œå°†å…¶ä½œä¸ºå°†æ­¤ç±»è‰ºæœ¯ä½œå“å¼•å…¥è¿ªå£«å°¼+æˆ–è¥¿æ–¹ä»»ä½•æµåª’ä½“ç½‘ç«™çš„ä¸€ç§æ–¹å¼ã€‚è¿™è®©æˆ‘æƒ³åˆ°äº†æˆ‘æœ€è¿‘ä¸€ç›´åœ¨åšçš„äº‹æƒ…:**ä¸€ä¸ªå¯ä»¥å¸®åŠ©ä»»ä½•äººæˆ–ä»»ä½•å…¬å¸æŸ¥çœ‹/æ·»åŠ æœ€é«˜è¯„çº§åŠ¨æ¼«çš„æ¨èç³»ç»Ÿã€‚æ—¥æœ¬è´¸æ˜“æŒ¯å…´æœºæ„ä¼°è®¡ï¼Œ2004 å¹´è¯¥è¡Œä¸šçš„æµ·å¤–é”€å”®é¢ä¸º 18 ğ‘ğ‘–ğ‘™ğ‘™ğ‘–ğ‘œğ‘›(ä»…ç¾å›½å°±è¾¾ 52 äº¿è‹±é•‘)ã€‚è¿™è‚¯å®šå·²ç»å¢é•¿ï¼Œå¹¶æœ‰æ½œåŠ›è¿›ä¸€æ­¥å¢é•¿ï¼Œå°¤å…¶æ˜¯åœ¨è¿™ä¸ªä¸–ç•Œä¸Šçš„è¿™æ®µæ—¶é—´ã€‚åƒä¸€äº›å›½å®¶ä¸€æ ·ï¼Œæ—¥æœ¬æ­£é¢ä¸´ç¬¬ä¸€æ¬¡é•¿æœŸè¡°é€€ã€‚**

ä¸‹é¢ï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘å¯¹ä¸€ä¸ªåŠ¨æ¼«æ¨èç³»ç»Ÿçš„é€æ­¥æŒ‡å¯¼ã€‚è¿™å°†æœ‰åŠ©äºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå¹¶å¯ä»¥åˆ›é€ æ›´å¤šçš„éœ€æ±‚åŠ¨æ¼«ã€‚å®ƒè¿˜å¯ä»¥å¸®åŠ©ä»»ä½•ä¸ç†Ÿæ‚‰å°ä¼—æµæ´¾çš„äººå¿«é€Ÿæ‰¾åˆ°æ”¶è§†ç‡æœ€é«˜çš„é¡¹ç›®ã€‚(å¸®åŠ©å‘å±•å¸‚åœº)ã€‚

æˆ‘å·²ç»ä»[https://www . ka ggle . com/Cooper union/anime-recommendations-database](https://www.kaggle.com/CooperUnion/anime-recommendations-database)ä¸‹è½½äº†åŠ¨æ¼«å’Œç”¨æˆ·è¯„åˆ†ã€‚æˆ‘å°†è¿›è¡Œæ¢ç´¢æ€§çš„æ•°æ®åˆ†æï¼Œè®©è¯»è€…å’Œæˆ‘è‡ªå·±ç†Ÿæ‚‰æ‰€å‘ˆç°çš„æ•°æ®ã€‚åœ¨é‚£é‡Œï¼Œæˆ‘ä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£(SVD)åˆ›å»ºäº†ä¸€ä¸ªåŸºçº¿æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä¼šåšåŸºäºè®°å¿†çš„æ¨¡å‹ï¼Œå°†ç€çœ¼äºåŸºäºç”¨æˆ·çš„ v é¡¹ç›®ã€‚æˆ‘å°†ä½¿ç”¨ KNNBaseã€KNNBaseline å’Œ KNNWithMeansã€‚ç„¶åæˆ‘ä¼šé€‰æ‹©è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œè¯„ä¼°å…¶å‡æ–¹æ ¹è¯¯å·®(rmse)å’Œå¹³å‡ç»å¯¹è¯¯å·®(mae)ã€‚

## å¯¼å…¥

```
import pandas as pd
import numpy as npimport random
from random import randintimport matplotlib.pyplot as plt
%matplotlib inline
import seaborn as snsfrom scipy.sparse import csc_matrix
from scipy.sparse.linalg import svdsfrom surprise.model_selection import train_test_splitfrom surprise.model_selection import GridSearchCV
from surprise.model_selection import cross_validatefrom surprise.prediction_algorithms import KNNWithMeans, KNNBasic, KNNBaselinefrom surprise.prediction_algorithms import knns
from surprise.prediction_algorithms import SVDfrom surprise.similarities import cosine, msd, pearsonfrom surprise import accuracyfrom surprise import Reader
from surprise import Dataset
```

åœ¨è¿™ä¸ªé¡¹ç›®çš„å¤§éƒ¨åˆ†æ—¶é—´é‡Œï¼Œæˆ‘å†³å®šåšæŒä¸Šé¢çš„æ–¹æ³•ï¼Œè€Œä¸”æ•ˆæœå¾ˆå¥½ã€‚æˆ‘å·²ç»åœ¨ google colab ä¸­å°è¯•äº†ä¸‹é¢çš„å…¶ä»–æ¨¡å‹ï¼Œæˆ‘å°†åœ¨é‚£é‡Œä¸ºå¯¼å…¥åº“æ·»åŠ å¿…è¦çš„ä»£ç ã€‚

## æ“¦æ´—/æ¸…æ´

```
anime_df = pd.read_csv('./anime.csv')
anime_df.head()
```

å½¢çŠ¶çœ‹èµ·æ¥åƒä»€ä¹ˆï¼Ÿè¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒæœ‰åŠ©äºæŸ¥çœ‹ä»»ä½•ç©ºå€¼

```
anime_df.shape
```

åˆ é™¤ç©ºå€¼(å¦‚æœå­˜åœ¨)

```
anime_df.dropna(inplace=True)
```

æˆ‘ä»¬å°†å†æ¬¡æ£€æŸ¥ä¸Šè¿°å‘½ä»¤åçš„å½¢çŠ¶

```
anime_df.shape #this seemed to have reduced it down a bit
```

è¿™å®é™…ä¸Šå‡å°‘äº†æˆ‘ä»¬çš„åŠ¨æ¼«æ•°æ®æ¡†æ¶ï¼

åœ¨æ¸…ç†æ—¶ï¼Œæ˜¾ç¤ºæ¯ä¸€åˆ—ä»£è¡¨ä»€ä¹ˆæ˜¯å¾ˆé‡è¦çš„ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥çŸ¥é“ä»€ä¹ˆå¯ä»¥æ“¦æ´—ã€è½¬æ¢ï¼Œæˆ–è€…æˆ‘ä»¬æ˜¯å¦éœ€è¦è¿›è¡Œä¸€äº›åŠŸèƒ½å·¥ç¨‹ã€‚

## æ¯åˆ—ä»£è¡¨ä»€ä¹ˆ:

***anime_id:æ¯éƒ¨åŠ¨æ¼«çš„ id å·ç‰‡å***
***åç§°:ç”µå½±ç‰‡å***
***ç±»å‹:ç±»åˆ«***
***ç±»å‹:æè¿°åŠ¨æ¼«åˆ†æˆç”µè§†ã€ç”µå½±ã€OVA ç­‰ 3 ä¸ªç±»åˆ«***
**é›†æ•°:æ€»é›†æ•°**
***è¯„åˆ†:-1-10ï¼Œæœ€ä½åˆ°***

æ¥ä¸‹æ¥ï¼Œå®ƒä»¬å’Œä¸Šé¢çš„åŒ¹é…å—ï¼Ÿ

```
anime_df.info() #having a look at all of the columns and types from the above cell and current to remove#any unneccessary extraneous data
```

è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªæ¨èé¡¹ç›®çš„ç¬¬äºŒä¸ª csv æ–‡ä»¶ã€‚

```
rating_df = pd.read_csv('./rating.csv')
rating_df.head()
```

è®©æˆ‘ä»¬çœ‹çœ‹ä»–ä»¬ä»£è¡¨äº†ä»€ä¹ˆã€‚

## æ¯åˆ—ä»£è¡¨ä»€ä¹ˆ:

**user_id:ä¸å¯è¯†åˆ«çš„éšæœºç”Ÿæˆçš„ user-id**
**anime_id:ç”¨æˆ·å·²è¯„çº§çš„åŠ¨æ¼«**
**è¯„çº§:è¯¥ç”¨æˆ·å·²åˆ†é…çš„ 10 åˆ†ä¸­çš„è¯„çº§(å¦‚æœç”¨æˆ·è§‚çœ‹äº†è¯¥åŠ¨æ¼«ä½†æœªåˆ†é…è¯„çº§ï¼Œåˆ™ä¸º-1)**

æ£€æŸ¥è¯„çº§çš„å½¢çŠ¶

```
rating_df.shape
```

éƒ½æœ‰ anime_idã€‚è®©æˆ‘ä»¬å°†è¿™ä¸¤è€…ç»“åˆèµ·æ¥ï¼Œä½¿äº‹æƒ…å˜å¾—å®¹æ˜“å¾—å¤šã€‚

```
df_merge = pd.merge(anime_df, rating_df, on = 'anime_id')
df_merge.head()
```

è®©æˆ‘ä»¬å†æ¬¡æ£€æŸ¥å½¢çŠ¶ã€‚

```
df_merge.shape
```

æˆ‘è‰°éš¾åœ°è®¤è¯†åˆ°å¥‡å¼‚å€¼åˆ†è§£(SVD)å¯¹å¦‚æ­¤å¤§çš„æ•°æ®é›†æ›´æ•æ„Ÿã€‚æˆ‘è¦çœ‹çœ‹æœ€ä½è¯„çº§ã€‚

```
df_merge.rating_x.min()
```

æˆ‘å·²ç»å†³å®šå»æ‰æ²¡æœ‰è¯„çº§çš„åˆ—ï¼Œå®ƒä»¬è¡¨ç¤ºä¸º-1ã€‚åœ¨åšæ¨èç³»ç»Ÿæ—¶ï¼Œè¿™å–å†³äºä¸ªäººã€ä»»åŠ¡å’Œå…¬å¸ã€‚æˆ‘å¯èƒ½ä¼šåœ¨æˆ‘çš„ä¸šä½™æ—¶é—´å›å»ï¼ŒæŠŠè¿™ä¸ªåŠ å›æ¥ï¼Œçœ‹çœ‹æœªå®Œæˆçš„è¯„çº§å’Œå®ƒå¯¹æ¨èçš„å½±å“ã€‚(ç¡®å®èµ·äº†å·¨å¤§çš„ä½œç”¨)ã€‚æˆ‘å†³å®šæŠŠå®ƒæ‹¿å‡ºæ¥çº¯ç²¹æ˜¯ä¸ºäº†å¨±ä¹ã€‚

```
df_merge = df_merge[df_merge.rating_y != -1]
df_merge.head()
```

ç°åœ¨ï¼Œæˆ‘å†æ¬¡æ£€æŸ¥å½¢çŠ¶ï¼Œçœ‹çœ‹è¿™å‡å°‘äº†å¤šå°‘ SVD æ•°æ®ã€‚

```
df_merge.shape #have removed over 1 million rows
```

è¿™å¯¹äºå…è´¹çš„ google colab æ¥è¯´ä»ç„¶ä¸å¤Ÿå°ã€‚

```
sample = df_merge.sample(frac=.25)
sample.shape # this is still too large
```

æˆ‘åœ¨æˆ‘çš„ SVD ä¸Šè¿è¡Œäº†å¾ˆå¤šæµ‹è¯•(è¿™èŠ±äº†ä¸€å¤©åŠçš„æ—¶é—´ï¼Œæˆ‘æœ€ç»ˆä¸å¾—ä¸æ»¡è¶³äºä¸‹é¢çš„æ ·æœ¬å¤§å°)ã€‚

```
sample = df_merge.sample(n=5000)sample.shape #below I conduct SVD and it cannot handle anything larger than 5000 (i've tried)
```

è®©æˆ‘ä»¬çœ‹çœ‹æ•°æ®ç±»å‹

```
sample.dtypes #rating_x needs to be an int, for it to work in ALS
```

è®©æˆ‘ä»¬åœ¨è¿™é‡Œè½¬æ¢å®ƒä»¬

```
sample['rating_x'] = sample['rating_x'].astype(int)
```

å¯ä»¥åœ¨è¿™é‡Œæ£€æŸ¥æ•°æ®ç±»å‹ã€‚ä»ä¸€å¼€å§‹ï¼Œæˆ‘å°±å‘ç°æ£€æŸ¥æˆ‘çš„ä»£ç æ˜¯éå¸¸é‡è¦çš„ã€‚å½“æœ‰è°ƒè¯•é—®é¢˜æ—¶ï¼Œå®ƒå¯¹æ¶ˆé™¤é—®é¢˜å¾ˆæœ‰å¸®åŠ©ã€‚

```
sample.dtypes
```

çœ‹èµ·æ¥è¯„çº§æ•°æ®æ¡†æ¶æ˜¯åŸºäºæ¯ä¸ªç”¨æˆ·å’Œä»–ä»¬å¯¹æ¯ä¸ªåŠ¨ç”» id çš„ä¸ªäººè¯„çº§ï¼Œè€ŒåŠ¨ç”»æ•°æ®æ¡†æ¶æ˜¯æ¥è‡ªå…¶æ‰€æœ‰è§‚ä¼—çš„å¹³å‡æ€»ä½“è¯„çº§ã€‚

```
#we are going to look at how many times each rating appears in a columnratings_x = sample['rating_x'].value_counts() #continuous
ratings_y = sample['rating_y'].value_counts() #discreteprint(ratings_x)
print(ratings_y)
```

å¦ä¸€ä»¶å¯¹ SVD æ¥è¯´éå¸¸é‡è¦çš„äº‹æƒ…æ˜¯è®©å˜é‡å˜å¾—è°¨æ…ï¼Œå¦åˆ™ï¼Œå®ƒä¼šå ç”¨ä½ æ›´å¤šçš„æ—¶é—´ã€‚

```
sample.rating_x = sample.rating_x.apply(round) 
sample.head()
```

å¤ªå¥½äº†ï¼Œæˆ‘æƒ³æˆ‘å·²ç»æ´—å®Œäº†ã€‚è®©æˆ‘ä»¬è¿›å…¥æœ‰è¶£çš„éƒ¨åˆ†ï¼

## æ¢ç´¢æ€§æ•°æ®åˆ†æ

ç”¨æˆ·è¯„çº§çš„åˆ†å¸ƒ

```
# plot distribution in matplotlib
ratings_sorted = sorted(list(zip(ratings_y.index, ratings_y)))
plt.bar([r[0] for r in ratings_sorted], [r[1] for r in ratings_sorted], color='cyan')
plt.xlabel("Rating")
plt.ylabel("# of Ratings")
plt.title("Distribution of Ratings")
plt.show()
```

æˆ‘è¿˜æ²¡æœ‰æŠŠè¾“å‡ºæ·»åŠ åˆ°æ¸…æ´—ä¸­ï¼Œè§†è§‰æ•ˆæœéå¸¸é‡è¦ï¼Œæ‰€ä»¥æˆ‘ä¼šæŠŠå®ƒä»¬æ·»åŠ è¿›å»ã€‚

![](img/7b0f895dd3da25ecaec3fc7d2f2e4045.png)

è¿™æ˜¯æˆ‘è‡ªå·±çš„å›¾è¡¨ï¼Œä¹Ÿå¯ä»¥åœ¨æˆ‘çš„ github ä¸Šæ‰¾åˆ°

```
#number of users
print("Number of Users:", df_merge.user_id.nunique()# print("Average Number of Reviews per User:", df_merge.shape[0])/df_merge.user_id.nunique()avg_rate_peruser = df_merge.shape[0]user = df_merge.user_id.nunique()
avg_num_review_per_user = avg_rate_peruser/userprint("Average Number of Reveiws per User:", avg_num_review_per_user)
```

ç”¨æˆ·æ•°é‡:15382

æ¯ä¸ªç”¨æˆ·çš„å¹³å‡è¯„è®ºæ•°:88.69

```
sample[â€˜user_idâ€™].value_counts()
```

5000 çš„æ ·æœ¬é‡ï¼Œç»™äº†æˆ‘ä»¬æ€»å…± 3ï¼Œ381 ä¸ªåšè¿‡è¯„è®ºçš„ç”¨æˆ·ã€‚ä»¥ä¸Šæ˜¯æ•´å¥—çš„ã€‚

æ¯ä¸ªç”¨æˆ·çš„è¯„è®ºæ•°é‡

```
ratings_per_user = sample['user_id'].value_counts()
ratings_per_user = sorted(list(zip(ratings_per_user.index, ratings_per_user)))
plt.bar([r[0] for r in ratings_per_user], [r[1] for r in ratings_per_user], color='pink')
plt.xlabel('User IDs')
plt.ylabel('# of Reviews')
plt.title('Number of Reviews per User')
plt.show()
```

![](img/0994195a59e16e35f5311cdf353a593d.png)

è¿™æ˜¯æˆ‘è‡ªå·±çš„å›¾è¡¨ï¼Œä¹Ÿå¯ä»¥åœ¨æˆ‘çš„ github ä¸Šæ‰¾åˆ°

ä»æˆ‘ä»¬çš„æ ·æœ¬é›†æ¥çœ‹ï¼Œç»™å‡ºçš„å¾ˆå¤šåˆ†æ•°éƒ½æ˜¯ 1 åˆ†å’Œ 2-4 åˆ†ã€‚

ä¸åŒç±»å‹çš„åŠ¨æ¼«

```
print("Number of users:", sample.user_id.nunique())
print("Number of types of different anime:", sample.type.nunique())
print("Types of type:", sample.type.value_counts())
```

ç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ‰ 6 ç§ç±»å‹ã€‚å®ƒä»¬æ˜¯ä»€ä¹ˆï¼Ÿ

6 ç§ç±»å‹:

```
TV 3492 
Movie 666
OVA 461
Special 314 
ONA 46 
Music 21 
Name: type, dtype: int64
```

OVA ä»£è¡¨åŸåˆ›è§†é¢‘åŠ¨ç”»

ONA ä»£è¡¨åŸåˆ›ç½‘ç»œåŠ¨ç”»

ç‰¹ä»·æ˜¯ä¸€æ¬¡æ€§çš„è§†é¢‘ã€‚

éŸ³ä¹æ˜¯ä»¥åŠ¨æ¼«ä¸ºä¸»é¢˜çš„ï¼Œé€šå¸¸æœ‰ä¸€ä¸ªåŠ¨ç”»ä¸ä¹‹æ­é…ï¼Œä½†å®ƒé€šå¸¸æ˜¯ä¸€ä¸ªéå¸¸çŸ­çš„è§†é¢‘ã€‚

ç‚¹å‡»ç‡æœ€é«˜çš„åŠ¨æ¼«

```
# PLOT them
fig = plt.figure(figsize=(12,10))
sns.countplot(sample['type'], palette='gist_rainbow')
plt.title("Most Viewed Anime", fontsize=20)
plt.xlabel("Types", fontsize=20)
plt.ylabel("Number of Views with Reviews", fontsize = 20)
plt.legend(sample['type'])
plt.show()
```

![](img/e3cef5445c67c00ea5fe01597e58755a.png)

è¿™æ˜¯æˆ‘è‡ªå·±çš„å›¾è¡¨ï¼Œä¹Ÿå¯ä»¥åœ¨æˆ‘çš„ github ä¸Šæ‰¾åˆ°

è®©æˆ‘ä»¬å¼€å§‹å®æ–½å§ï¼

## åˆ†æ

æœ€é‡è¦çš„æ˜¯ç¬¬ä¸€ä»¶äº‹ï¼åšä¸€ä¸ªåŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»è½¬æ¢æˆ‘çš„æ•°æ®å¼€å§‹ã€‚

```
#for surprise, it likes its data in a certain way and only that specific datadata = sample[['user_id', 'anime_id', 'rating_x']] #may need to do rating_x rounded and then use rating_yreader = Reader(line_format='user item rating', sep='')
anime_loaded_data = Dataset.load_from_df(data, reader)#train_test_split
trainset, testset = train_test_split(anime_loaded_data, test_size=.2)
```

ç¡®ä¿æ‚¨çš„æ•°æ®æ ¼å¼æ­£ç¡®

```
anime_loaded_data
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®ä¾‹åŒ–ã€‚

```
#INSTANTIATE the SVD and fit only the train set
svd = SVD()svd.fit(trainset)
```

ç°åœ¨æ¥çœ‹çœ‹é¢„æµ‹å’Œå‡†ç¡®æ€§ã€‚è¿™å¯¹äºæ¯”è¾ƒæ‚¨çš„å­¦å‘˜æ¨¡å‹éå¸¸é‡è¦ã€‚

```
predictions = svd.test(testset) #
accuracy.rmse(predictions)
```

è¿™æ˜¯æˆ‘çš„åŸºçº¿:RMSE: 2.3128ï¼Œ2.3127

å—¯ï¼Œè‚¯å®šä¸æ˜¯ 0 åˆ° 1 ä¹‹é—´ã€‚è¿™äº›åˆ†æ•°å®é™…ä¸Šå–å†³äºé¢†åŸŸã€‚è‚¯å®šä¸å®Œç¾ã€‚æˆ‘æœ€åˆçš„ååº”æ˜¯å“åäº†ï¼Œä½†äº‹å®è¯æ˜ RMSE è¶…è¿‡ 1 æ˜¯æ²¡é—®é¢˜çš„ã€‚

æˆ‘æƒ³çœ‹çœ‹æˆ‘æ˜¯å¦èƒ½å‡å°‘è¿™ç§æƒ…å†µã€‚

```
#perform a gridsearch CV
params = {'n_factors': [20,50,100],
'reg_all': [.02,.05, .10]}gridsearch_svd1 = GridSearchCV(SVD, param_grid=params, n_jobs=-1, joblib_verbose=3)gridsearch_svd1.fit(anime_loaded_data)print(gridsearch_svd1.best_score)
print(gridsearch_svd1.best_params)
```

{'rmse': 2.3178ï¼Œ' Mae ':2.2080 } { ' RMSE ':{ ' n _ factors ':20ï¼Œ' reg_all': 0.02}ï¼Œ' mae': {'n_factors': 20ï¼Œ' reg_all': 0.02}}

å®ƒå¢åŠ äº†ã€‚æˆ‘ç°åœ¨å°†å°è¯•å…¶ä»–æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åŸºäºå†…å­˜çš„æ¨¡å‹ï¼Œç„¶åæ˜¯åŸºäºå†…å®¹çš„æ¨¡å‹ã€‚

k-æœ€è¿‘é‚»(KNN)åŸºæœ¬ç®—æ³•

```
#cross validate with KNNBasic
knn_basic = KNNBasic(sim_options={'name':'pearson', 'user_based':True}, verbose=True)
cv_knn_basic = cross_validate(knn_basic, anime_loaded_data, n_jobs=2)for i in cv_knn_basic.items():print(i)
print('-----------------')
print(np.mean(cv_knn_basic['test_rmse']))
```

å°†æŠŠä½ ä»å…¶ä½™çš„ä¸­æ‹¯æ•‘å‡ºæ¥ï¼Œè¿™é‡Œæ˜¯è¾“å‡ºçš„ä¸€ä¸ªç®€çŸ­å‰ªè¾‘

â€” â€” â€” â€” â€” â€” â€” â€” â€” 2.3178203641229667

æ‚¨å¯ä»¥å¯¹å‡æ–¹è·ç¦»(msd)è¿›è¡ŒåŒæ ·çš„æ“ä½œã€‚

```
knn_basic_msd = KNNBasic(sim_options = {'name': 'msd', 'user-based':True})
cv_knn_basic_msd = cross_validate(knn_basic_msd, anime_loaded_data, n_jobs=2)for i in cv_knn_basic_msd.items():print(i)
print('-----------------')
print(np.mean(cv_knn_basic_msd['test_rmse']))
```

â€” â€” â€” â€” â€” â€” 2.31787540672289ï¼Œå¾—åˆ†è¾ƒé«˜ã€‚è®©æˆ‘ä»¬è¯•è¯•å¦ä¸€ä¸ªæ¨¡å‹

KNNBaseline

```
#cross validate with KNN Baseline (pearson)
knn_baseline = KNNBaseline(sim_options={'name': 'pearson', 'user_based':True})
cv_knn_baseline = cross_validate(knn_baseline, anime_loaded_data, n_jobs=3)for i in cv_knn_baseline.items():print(i)
print('-----------------')
print(np.mean(cv_knn_baseline['test_rmse']))
```

â€” â€” â€” â€” â€” â€” â€” â€” â€” 2.317895626569356

åŒæ ·ï¼Œå½“æˆ‘ä»¬å¸Œæœ›å®ƒå‡å°‘æ—¶ï¼Œå®ƒå´åœ¨å¢åŠ ã€‚

KNN åŸºçº¿ä¸çš®å°”é€Š _ åŸºçº¿

```
knn_pearson_baseline = KNNBaseline(sim_options={'name': 'pearson_baseline', 'user_based':True})cv_knn_pearson_baseline = cross_validate(knn_pearson_baseline, anime_loaded_data, n_jobs=3)for i in cv_knn_pearson_baseline.items():print(i)
print('-------------------')
print(np.mean(cv_knn_pearson_baseline['test_rmse']))
```

è¿™ç»™äº†æˆ‘ä»¬----- 2ã€‚46860 . 68868886861

KNNWithMeans

```
knn_means = KNNWithMeans(sim_options={'name': 'pearson', 'user_based': True})
cv_knn_means = cross_validate(knn_means, anime_loaded_data, n_jobs=3)for i in cv_knn_means.items():print(i)
print('------------')
print(np.mean(cv_knn_means['test_rmse']))
```

â€” â€” â€” â€” â€” â€” 2.3185632763331805

SVD åŸºçº¿ä¼¼ä¹å…·æœ‰æœ€ä½çš„ RMSEã€‚æˆ‘ä¼šå†æ¬¡è¿›è¡Œç½‘æ ¼æœç´¢

```
param_grid = {'n_factors': [5, 20, 100],
'n_epochs': [5,10],
'lr_all': [.002, .005],
'reg_all': [.02, .05, .5]}svd_gs = GridSearchCV(SVD, param_grid=param_grid, n_jobs=3, joblib_verbose=3)svd_gs.fit(anime_loaded_data)print(svd_gs.best_score)
print(svd_gs.best_params)
```

ç„¶å

```
#Now use this to fit test set, initial gridsearch was 2.77096, so will use that gs herehighest_perf_algo = gridsearch_svd1.best_estimator['rmse']#retrain the whole settrainset = anime_loaded_data.build_full_trainset()
highest_perf_algo.fit(trainset)#Biased Accuracy on trainset
predictions = highest_perf_algo.test(trainset.build_testset())print('Biased accuracy on Trainset', end='')
accuracy.rmse(predictions)#UnBiased Accuracy on testset
predictions = highest_perf_algo.test(testset)print('Unbiased Accuracy on test', end='')
accuracy.rmse(predictions)
```

æˆ‘çš„ç»“æœæ˜¯:è®­ç»ƒé›†ä¸Šçš„æœ‰åç²¾åº¦ RMSE:2.3179301111112067ï¼Œæµ‹è¯•é›†ä¸Šçš„æ— åç²¾åº¦ RMSE:2.317938596

æˆ‘åšäº†ä¸€äº›åˆä½œæ¨¡å‹ï¼Œåˆ—åœ¨æˆ‘çš„ [Github repo](https://github.com/anilaq/capstone/blob/master/latest.ipynb) ä¸Šã€‚æˆ‘çš„åä½œæ¨¡å‹é‡‡ç”¨ä½™å¼¦ç›¸ä¼¼åº¦å’ŒåŸºäºç”¨æˆ·çš„ v é¡¹ç›®ã€‚

åŸºäºå†…å®¹

```
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import string
from nltk import word_tokenize, FreqDist
import re
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import linear_kernel
```

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æå–å¿…å¡«å­—æ®µã€‚

```
genre_tag = sample.loc[:, ['anime_id','name','genre']]
```

è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯ä»€ä¹ˆæ ·å­çš„

```
genre_tag.head()
```

ç›®æ ‡æ˜¯å°†æ¯è¡Œçš„å•è¯åˆ†å¼€ã€‚

```
tags = {}
for col in ['genre']:
  for a_id in sample['name'].unique(): 
    for i in sample[sample['name'] == a_id][col]:
     if a_id in tags: 
       tags[a_id].append(' '.join(i.lower().split('|')))
     else: 
         tags[a_id] = i.lower().split('|')
```

å°†æ­¤è½¬æ¢æˆåˆ—è¡¨

```
tags_list = list(tags.values())
```

çœ‹çœ‹åå•

```
tags_list[:5]
```

ç¡®ä¿æŒ‡å®šåœç”¨è¯ä¸ºè‹±è¯­

```
stopwords_list = stopwords.words('english')
stopwords_list += list(string.punctuation)
```

ç›®çš„æ˜¯å‡å°‘äº§ç”Ÿçš„å™ªéŸ³ã€‚

```
def process_article(article): 
  article = ' '. join(article)
tokens = word_tokenize(article)
tokens_2 = []
for token in tokens: 
  if token.lower() not in stopwords_list: 
     tokens_2.append(token.lower())
return tokens_2
```

è®©æˆ‘ä»¬æ¥å¤„ç†è¿™ç¯‡æ–‡ç« ã€‚è¿™å°†æŠŠæµ‹è¯•åˆ†æˆåŸºäºç©ºæ ¼æˆ–æ’‡å·åˆ†éš”çš„å­—ç¬¦ä¸²åˆ—è¡¨

```
processed_tags = list(map(process_article, tags_list))
```

ç°åœ¨è®©æˆ‘ä»¬ä»ä½“è£ä¸­è·å–æ€»è¯æ±‡é‡ã€‚

```
articles_concat = []
for doc in processed_tags: 
  articles_concat += doc
```

æ¯ä¸ªå•è¯å‡ºç°å¤šå°‘æ¬¡ï¼Ÿ

```
freqdist = FreqDist(articles_concat)
freqdist
```

è®©æˆ‘ä»¬è·å¾—å‰ 20 ä¸ªå•è¯ï¼Œè¿™å°†è®©æˆ‘ä»¬çœ‹åˆ°æœ€å—æ¬¢è¿çš„ç±»å‹

```
most_common_genre = freqdist.most_common(20)
most_common_genre
```

è®©æˆ‘ä»¬ç”»å‡ºæ¥

```
plt.figure(figsize=(22,12))
plt.bar([word[0] for word in most_common_genre], [word[1] for word in most_common_genre])
plt.title("Most Popular Genre", fontsize= 20)
plt.xlabel("Genre", fontsize=20)
plt.ylabel("Total Number of Times Genre is Present in Anime Data", fontsize=20)
plt.show()
```

![](img/bcc4f12a87f5ce339d417de923f5ae60.png)

è¿™æ˜¯æˆ‘è‡ªå·±çš„å›¾è¡¨ï¼Œä¹Ÿå¯ä»¥åœ¨æˆ‘çš„ github ä¸Šæ‰¾åˆ°

æˆ‘ä»¬çš„æ¨èç³»ç»Ÿå¯èƒ½ä¼šæ¨èå–œå‰§ã€åŠ¨ä½œç‰‡ã€çˆ±æƒ…ç‰‡ç­‰ã€‚

ç°åœ¨ï¼ŒæŠŠå®ƒè½¬æ¢æˆçŸ¢é‡å™¨

```
vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), stop_words=stopwords_list)tags_list_2 = [â€˜ â€˜.join(x) for x in tags_list]
tf_idf_data_train = vectorizer.fit_transform(tags_list_2)
```

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒå¦‚ä½•ç¬¦åˆæ ‡å‡†ã€‚å› ä¸º NLTK æ˜¯ä¸€ä¸ªå®Œå…¨ä¸åŒçš„åº“ï¼Œæ‰€ä»¥æœ€å¥½çœ‹çœ‹å®ƒè§£é‡Šçš„æ–¹å·®æ¯”ã€‚

```
#instantiate SVD
svd = TruncatedSVD(n_components=500, n_iter=7, random_state=42)#fit and transform the vectorized tf-idf matrix
tf_idf_data_train_svd = svd.fit_transform(tf_idf_data_train)
```

è¿™é‡Œæˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼å¾—åˆ°æœ€ç»ˆç»“æœ:

```
print(svd.explained_variance_ratio_.sum())
```

0.9962558733287571

åœ¨å°è¯•äº†ä¸€å †æ¨¡å¼ä¹‹åï¼Œæˆ‘è§‰å¾—åŸºäºå†…å®¹çš„æ¨¡å¼æ˜¯æœ€å¥½çš„ã€‚

åœ¨æˆ‘çš„ç¬”è®°æœ¬ä¸Šï¼Œæˆ‘è¿˜çœ‹ç€è®¡ç®—åŠ¨æ¼«å’Œ TF-IDF çš„ç›¸ä¼¼åº¦ã€‚

## æ›´å¤šä¿¡æ¯

åŸºäºæ‰€åšçš„å»ºæ¨¡ï¼Œæœ€å¥½æ˜¯å¯¹æ¥è‡ªç›¸åŒåŠ¨ç”»å·¥ä½œå®¤ã€ä½œå®¶ã€å¯¼æ¼”å’Œåª’ä½“å…¬å¸çš„æ”¶è§†ç‡æœ€é«˜å’Œæ”¶è§†ç‡æœ€é«˜çš„åŠ¨ç”»è¿›è¡Œæ›´å¤šçš„ç»Ÿè®¡åˆ†æã€‚ä»è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥è°ƒæ•´æ¨èç³»ç»Ÿï¼Œä»¥åŒ…æ‹¬ä»–ä»¬çš„æœ€æ–°å‘å¸ƒã€‚

å¦ä¸€ä¸ªå»ºè®®æ˜¯çœ‹æ—¶é—´æˆåˆ†åˆ†æã€‚ç”µè§†èŠ‚ç›®æ˜¯æœ€å—æ¬¢è¿çš„åŠ¨ç”»ç±»å‹ï¼Œä½†æ˜¯åœ¨ä¸€å¹´ä¸­çš„ä»€ä¹ˆæ—¶å€™å‘¢ï¼Ÿä»è¿™é‡Œå¼€å§‹ï¼Œæ¨èç³»ç»Ÿå¯ä»¥å¸®åŠ©åŠ¨ç”»å·¥ä½œå®¤å¡«è¡¥è´¢æ”¿å¹´åº¦å†…çš„ç©ºç™½ã€‚ä»–ä»¬å¯ä»¥å°†æ—¥æœŸåˆ†æ•£å¼€æ¥ï¼Œæˆ–è€…å¼€è¾Ÿä¸€æ¡æ–°çš„æ¸ é“æ¥æ¨å‡ºæ›´å¤šç‰¹ä»·å•†å“ã€‚

å‚è€ƒèµ„æ–™:

[1]é˜¿Â·åº“é›·è¥¿ï¼Œ[å¡æ™®æ–¯é¡¿](https://github.com/anilaq/capstone) e (2020)

[2]è‹çŠÂ·æï¼Œ[ï¼Œ](/building-and-testing-recommender-systems-with-surprise-step-by-step-d4ba702ef80b) (2018)

[3] Derrick Mwitiï¼Œ[å¦‚ä½•ç”¨ Python æ„å»ºä¸€ä¸ªç®€å•çš„æ¨èç³»ç»Ÿ](/how-to-build-a-simple-recommender-system-in-python-375093c3fb7d) (2018)

[4] Robi56ï¼Œ[æ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿ](https://github.com/robi56/Deep-Learning-for-Recommendation-Systems) (2020)

[5] JJã€æ¢…å’Œ RMSE â€” [å“ªä¸ªæŒ‡æ ‡æ›´å¥½ï¼Ÿ](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d) (2016)

[6]ç»´åŸºç™¾ç§‘ï¼Œ[åŠ¨æ¼«](https://en.wikipedia.org/w/index.php?title=Anime&action=history) (2020)

[7]ç»´åŸºç™¾ç§‘ï¼Œ[åƒä¸åƒå¯»](https://en.wikipedia.org/wiki/Spirited_Away) (2020)

[8]ç›ä¸½Â·é©¬ç“¦å¾·ï¼Œ[å† çŠ¶ç—…æ¯’:æ¬§æ´²æµåª’ä½“æœåŠ¡ä¹Ÿåœ¨è“¬å‹ƒå‘å±•](https://sifted.eu/articles/streaming-startups-coronavirus/) (2020)