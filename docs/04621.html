<html>
<head>
<title>Intuitively, How Can We Understand Different Classification Algorithms Principles</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">凭直觉，我们如何理解不同的分类算法原理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3?source=collection_archive---------29-----------------------#2020-04-24">https://towardsdatascience.com/intuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3?source=collection_archive---------29-----------------------#2020-04-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2384" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">机器学习算法有时可以被视为一个黑盒，那么我们如何以更直观的方式来解释它们呢？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/1477aa5fbabfddd06c3a318a4efc5808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/1*VXuXf9f2Q3a_pefvm9jLzA.gif"/></div></figure><p id="d5e5" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在下图中，给定蓝点和红点，我们可以看到有一个模式。作为人类，我们可以用我们的“直觉”来区分它们，并预测一个新点的颜色。</p><p id="bb94" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">例如，我们大多数人可能会说图中的黑点属于蓝点类别。但是你如何用数学的方式表达这种“直觉”呢？你会看到不同的直觉导致我们所知道的所有算法…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/d1fe229dd96a53d23425bb9e6e07be2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Z3OcjDgETENv-m2wVNZsA.png"/></div></div></figure><p id="8aa5" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果蓝点和红点对你来说可能比较抽象，那么让我们来看一些真实世界的例子。</p><ul class=""><li id="e8ac" class="lr ls it ks b kt ku kw kx kz lt ld lu lh lv ll lw lx ly lz bi translated">肿瘤的诊断(B为良性，M为恶性)，具有两种不同的肿瘤特征。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/66f187af67895ec5ed5efd644bb46da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hPcg0XhR1krxfAtb3e_k3g.png"/></div></div></figure><ul class=""><li id="4a02" class="lr ls it ks b kt ku kw kx kz lt ld lu lh lv ll lw lx ly lz bi translated">垃圾邮件检测:使用两个变量，美元符号的频率和单词“remove”的频率。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/a85e464ca3e8b2570923d8f645365c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aJez7tfbO6fPjR6sCha1AQ.png"/></div></div></figure><p id="9a83" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">你可能已经知道一些可行的分类算法:逻辑回归、kNN、LDA(线性判别分析)、SVM、决策树等等。</p><p id="55e1" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">但是理解它们直观吗？</p><p id="e539" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">首先，为了简化问题，我们将在本文中考虑1D的情况(我们将在另一篇文章中考虑2D的情况)</p><p id="45c8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">正如我们在下图中看到的，我们只有一个预测值，即X，目标变量是Y，有两个类，红点和蓝点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/cffb928b7091ef051ce8c40e44b0bff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lWe7ugRIxvQ9PTo1qXbBrg.png"/></div></div></figure><p id="fd93" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">注意，从数学上来说，“红”或“蓝”没有任何意义，所以我们将这个变量转换成一个二进制变量:1代表“蓝”，0代表“红”。这就对应了这个问题:圆点是蓝色的吗？1代表真，0代表假。当然，这只是约定俗成。</p><p id="9fba" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在现实世界的例子中，“圆点是蓝色的吗？”可以是:肿瘤是不是恶性的？这封邮件是不是垃圾邮件？</p><h1 id="963d" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">第一个原则:邻居分析</h1><p id="d187" class="pw-post-body-paragraph kq kr it ks b kt ms ju kv kw mt jx ky kz mu lb lc ld mv lf lg lh mw lj lk ll im bi translated">对于一个给定的点，其思想是查看该点的<strong class="ks iu">邻居</strong>。</p><p id="089c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">邻居是什么？离<strong class="ks iu">最近的</strong>的那些？</p><p id="1dff" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">“接近”是什么意思？<strong class="ks iu">最短的</strong>距离？</p><p id="7a7d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">什么是距离？在这里，它只是指x的两个值之差(x是一个实数)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi mx"><img src="../Images/8f8c1d80f743f32883bf40040b109bec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*4calmfR8Z0kwQr-29x76wg.gif"/></div></div></figure><p id="1706" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，给定一个新点，我们可以计算这个点和其他点之间的距离。我们可以选择最接近的。</p><p id="fb10" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">但是有多少呢？这是这种方法的一个大问题:我们选择多少点？</p><p id="2b1e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">首先，我们称这个数为k，k=5。</p><p id="ce87" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在我们可以检查邻居的等级。如果你有一个类的多数，那么你可以用多数类来预测。在我们的例子中，如果邻居的多数类是1，那么新点很有可能属于类1。</p><p id="3a4b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">注意“多数”原则。所以如果我们选择了k=4，那么就很难决定了。因此，如果k是奇数，它有助于作出明确的决定。</p><p id="7523" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在可以计算一个新点的概率，我们有下图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/8c188975256c47c2e9d26f636fdc65bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/1*qFWnPAchGIzJtGnqUZbF9Q.gif"/></div></figure><p id="077d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这个原理叫做kNN，意思是K近邻。</p><p id="79a4" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，这个算法的特别之处在于，由于你不知道要保留哪些点，所以你必须为每个预测保留所有的点。这就是为什么我们说这个算法不是基于模型的，而是基于实例的。</p><h1 id="4641" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">第二个原则:全球比例和正态分布</h1><p id="5239" class="pw-post-body-paragraph kq kr it ks b kt ms ju kv kw mt jx ky kz mu lb lc ld mv lf lg lh mw lj lk ll im bi translated">我们刚刚在上面说过，选择数字k是不方便的，我们没有对我们的观察结果进行任何建模。</p><p id="659d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在我们该怎么办？首先，让我们考虑所有的人口。如果您这样做，那么您的预测对所有新点都是一样的:多数类和概率将是多数类的比例。</p><p id="aa44" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这可能有点简单化了。为了做得更好，我们可以考虑点的正态分布。为什么是正态分布？这很简单，简化了所有的计算。这真的是个好理由吗？嗯，这就是我们所说的“建模”。</p><blockquote class="my mz na"><p id="93dd" class="kq kr nb ks b kt ku ju kv kw kx jx ky nc la lb lc nd le lf lg ne li lj lk ll im bi translated">所有的模型都是错的，但有些是有用的。—乔治·博克斯，著名统计学家</p></blockquote><p id="7148" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，我们可以问一个更好的问题，而不是数邻居:我离蓝点或红点有多远。换句话说:给定一个新点，这个点是蓝色或红色的概率是多少？</p><ul class=""><li id="5bb8" class="lr ls it ks b kt ku kw kx kz lt ld lu lh lv ll lw lx ly lz bi translated">这个新点离<strong class="ks iu">蓝色</strong>点有多近？我们考虑<strong class="ks iu">蓝点</strong>(标注为PDF_b)的概率密度函数，距离(或者说接近度)将是<strong class="ks iu"> PDF_b(x) </strong></li><li id="23bd" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">新点离红色点有多近？我们考虑<strong class="ks iu">红点</strong>(记为PDF_r)的概率密度函数，距离(或者说接近度)将是<strong class="ks iu"> PDF_r(x) </strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/250e6494ccb9449067d059514ee4421f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x6kTAAKZMf_EoTqh3FuWWQ.png"/></div></div></figure><p id="cf98" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了知道新点更接近哪种颜色，我们只需比较两个概率密度。下图中，黑色曲线代表比率:<strong class="ks iu"> PDF_b/(PDF_b+PDF_r) </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/3b155daae49e3c0af2ed45e3bebb4baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mZBdNV_B8GktIua7iwDEnA.png"/></div></div></figure><p id="5030" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，就建模而言，令人惊讶的是:使用kNN，您必须保留所有的点才能做出决定，现在您只需使用几个参数，如均值和标准差，以便定义正态分布。</p><p id="e25c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在之前的数据集中，我们有相同数量的蓝点和红点。如果数字不同，我们可以用两个密度的比例来加权。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/11441b58071fee8d81ab34608ae6058a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jClBUcbBWdzhe4L1WorRww.png"/></div></div></figure><p id="f1ba" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在类似<strong class="ks iu">线性判别分析</strong>、<strong class="ks iu">二次判别分析</strong>、<strong class="ks iu">朴素贝叶斯分类器</strong>等算法都在使用这个原理。</p><p id="b7ed" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">它们之间有什么区别？</p><p id="754e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">还记得我们必须计算正常密度吗？为了得到正态分布，我们必须计算平均值和标准差。就手段而言，很简单。但是对于标准差，我们有两个选择。我们可以计算每个类的标准差，或者，为了简化，我们可以认为这两个类的标准差是相同的。为什么我们可以使用两个标准差的加权值。为什么是“线性”对“二次”呢？为此，我写了另一篇文章:<a class="ae nk" rel="noopener" target="_blank" href="/intuitively-how-can-we-build-non-linear-classifiers-10c381ed633e">直观地说，我们如何才能建立非线性分类器</a>。</p><h1 id="0a5d" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">第三个原则:分析前沿领域</h1><p id="4fea" class="pw-post-body-paragraph kq kr it ks b kt ms ju kv kw mt jx ky kz mu lb lc ld mv lf lg lh mw lj lk ll im bi translated">根据前面的原则，我们可以看到，在对整个数据集建模之后，我们得出了一个决策边界。</p><p id="0dac" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，我们可以想:如果目标是找到一个前沿，为什么我们不直接分析前沿领域？</p><p id="8580" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">“边界区域”可以用M(表示边距)来定义，M是我们选择用来定义“边界区域”的两点之间的距离。我们可以将红色圆点的候选点记为<strong class="ks iu">，将蓝色圆点的候选点</strong>记为<strong class="ks iu">。用数学术语来说，我们有:</strong></p><p id="bccc" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">M=B-A</p><p id="0156" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">首先让我们考虑这两个类是线性可分的。然后我们可以计算红点的<strong class="ks iu">最大值(我们得到A) </strong>，蓝点的<strong class="ks iu">最小值(我们得到B) </strong>。(注意，在1D的情况下，最小值和最大值很容易定义。但是当维度更高时，它可能更复杂。)</p><p id="462f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在<em class="nb">非常直观的(见下面的引用)，</em>我们可以选择两个值的平均值作为决策边界。</p><blockquote class="my mz na"><p id="b6c5" class="kq kr nb ks b kt ku ju kv kw kx jx ky nc la lb lc nd le lf lg ne li lj lk ll im bi translated">最大化间隔看起来不错，因为决策[区间]附近的点代表非常不确定的分类决策:分类器几乎有50%的机会做出任何决定。具有较大裕度的分类器做出的分类决策确定性不低。这给了你一个分类安全裕度— <a class="ae nk" href="https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html" rel="noopener ugc nofollow" target="_blank">支持向量机:线性可分的情况</a></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/62f48f87d523a4031d5ff6c21bdfc21e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1bEViwiWmTOUCEBiNQrtJw.png"/></div></div></figure><p id="2e2c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">但是如果有一个类0的值非常接近类1呢？那么我们将会有如下图所示的边界:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/881c6c0eaf4b642b14f434ec66fc9373.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Q4jgZFIvw1gWIQWuceWeg.png"/></div></div></figure><p id="2d46" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在我们有了这种直觉，第二种情况下定义的决策边界并不是最优的:边界区域很小。现在，如果我们保持前面的M，让异常点，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/44555b2e9b3a7fa06827560874fada1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ify2Sn5JvZiG32TwbIpqSQ.png"/></div></div></figure><p id="7e3a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们可以为异常点添加一个惩罚项，它可以是异常点和a之间的距离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/9245fa66601cd0c37d675c52e91e440b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Fqdm9hIF8nZYOKRt4nE7g.png"/></div></div></figure><p id="b0d9" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">更好的是，我们可以用一个系数对惩罚进行加权，我们称之为c。因此，最终的决策标准是:</p><p id="276c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">M - C ×(红色异常点- S_r)</p><p id="d37d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">(保证金减去加权罚金)</p><p id="84b4" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">一般来说，如果有几个异常点，我们可以对它们进行求和:</p><p id="ef03" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">M - C × ( ∑(红色异常点- S_r) + ∑ (S_b -蓝色异常点))</p><p id="d577" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">通过这样做，我们还解决了如下图所示的点不是线性可分的情况下的问题(记住，我们说过我们可以首先考虑点是线性可分的，这样我们就可以得到我们直观的“边界区域”)。)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/9d552e948f8cd3345bbd9ee51fc10dfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRFEdXHeDULGieECHjZh4g.png"/></div></div></figure><p id="1a4e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，这个原理被用于SVM(支持向量机)，为了得到SVM的最终版本，我们必须做一些小的调整(我们将在另一篇文章中讨论它们)。而为什么叫“支持”呢？因为你使用不同的点(称为“支持向量”)来最大化边际(或者更准确地说是惩罚边际)。</p><h1 id="a1f4" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">第四个原则:寻找最佳曲线</h1><p id="fee8" class="pw-post-body-paragraph kq kr it ks b kt ms ju kv kw mt jx ky kz mu lb lc ld mv lf lg lh mw lj lk ll im bi translated">对于这个原理，让我们考虑一条直线来模拟作为x的函数的y。</p><p id="1b92" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">y=a × x + b</p><p id="8325" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了简化这个问题的求解，我们可以考虑直线会经过每一类的均值，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/8549b70ff68eff0bc1b2785606611d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tyfrQy27xKApxxgjcY9lTg.png"/></div></div></figure><p id="615c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，当y=0.5时，你可以考虑x的值，这可以作为你决定y属于0类还是1类的边界。</p><p id="a170" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">但是用这个模型，对于x的大值，你会得到y &gt; 1，对于x的小值，y &lt;0. So what can we do? Let’s smooth it. Like this?</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/f55aedef887acea049be341de73cc318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMX43o-q45ZwRygM3B5Quw.png"/></div></div></figure><p id="610d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Come on, we can do better, like this?</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/92e835cf9454e807bcdcb1961d713e83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z8OgT4be2S_GaK_N9b9X4w.png"/></div></div></figure><p id="7d5b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">To do the smoothing in the graph above, we can use this function for example</p><p id="3a3a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">p(x)= 1 / (1+exp(-(a × x + b)))</p><p id="342f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">To understand why we use this function, you can note that we find the initial straight line : y(x)=a × x + b in this function, and we can define: sigma(y)= 1 / (1+exp( -y )))</p><p id="a54a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">So we have : p(x)=sigma(y(x))=1 / (1+exp(-(a × x + b)))</p><p id="8aac" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">To visualize the smoothing effect, we can see the graph of sigma below</p><ul class=""><li id="d7bd" class="lr ls it ks b kt ku kw kx kz lt ld lu lh lv ll lw lx ly lz bi translated">when x is very big, the output is very close to 1</li><li id="54ae" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">when x is very small, the output is very close to 0</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/d6d19bc5144cdb7d319f9b00fbaabd84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lEa3Vp9RgGdnZnJ1X2txzg.png"/></div></div></figure><p id="6dd1" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">And now the task is to find the parameters: a and b. To achieve this goal, we can consider the probability of each point to be correctly classified.</p><ul class=""><li id="0ff1" class="lr ls it ks b kt ku kw kx kz lt ld lu lh lv ll lw lx ly lz bi translated">For the blue dots, the probability value is p(x);</li><li id="8e44" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">For the red dots, the probability value is 1-p(x).</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/3a26d95951b5adde0c90350003fdc6dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dvrkWi6AE5xLIB7feJVrmw.png"/></div></div></figure><p id="5506" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">The criterion is the maximization of the overall probability: we multiply all the probabilities (for class 0 and class 1). And we try to maximize the result.</p><p id="f29f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">P_overall = product (p(x), for class 1) × product ( (1-p(x), for class 0)</p><p id="050e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Or in a simpler form</p><p id="735a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">P_overall = product (p(x)×y +(1-p(x))×(1-y))</p><p id="4474" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">The mathematical trick is then to take the log and to take the derivatives, etc. But it turns out that there is not an easy way (a closed formula) to find the parameters, and we have to solve it numerically.</p><p id="457c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Below we can see the situation for different values of <strong class="ks iu"> a </strong>和<strong class="ks iu"> b </strong>。垂直线段代表每个点的概率。所有这些概率的乘积应该最大化，以便优化<strong class="ks iu"> a </strong>和<strong class="ks iu"> b </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi nl"><img src="../Images/38e4795ddd603ce22393d865a09ad317.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*mxSpynSn6YnlmsXk3NXVAg.gif"/></div></div></figure><p id="e2fa" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这是逻辑回归使用的原理，因为我们之前看到的sigma函数的名字，叫做逻辑函数。</p><p id="d517" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">(如果你觉得这个解释不够直观，可以看看这篇文章:<a class="ae nk" rel="noopener" target="_blank" href="/intuitively-how-can-we-better-understand-logistic-regression-97af9e77e136">直观地说，我们如何(更好地)理解Logistic回归</a></p><h1 id="40ba" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">第五个原则:避免错误</h1><p id="5a92" class="pw-post-body-paragraph kq kr it ks b kt ms ju kv kw mt jx ky kz mu lb lc ld mv lf lg lh mw lj lk ll im bi translated">最后一个原则是关于选择决策边界时可能犯的<strong class="ks iu">错误</strong>。</p><p id="d4c0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在有哪些错误:</p><ul class=""><li id="b7e4" class="lr ls it ks b kt ku kw kx kz lt ld lu lh lv ll lw lx ly lz bi translated">如果在该区域中，蓝色点占多数，那么红色点就是错误；</li><li id="2506" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">如果红点占多数，那么蓝点就是错误。</li></ul><p id="86ac" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">当找到决策边界时，这些点被分成两个区域。这个想法是为了描述区域的“同质性”:错误越少越好。</p><p id="6303" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在让我们找一个函数来描述区域的“均匀性”。以p为例，作为第1类的比例。我们得到0类的比例为(1-p)。</p><p id="04d6" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在有了p和(1-p)，我们能做什么？让我们从非常简单的操作开始。</p><ul class=""><li id="d981" class="lr ls it ks b kt ku kw kx kz lt ld lu lh lv ll lw lx ly lz bi translated">Sum？嗯，再想想。</li><li id="1b5b" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">产品。嗯，让我看看，这是一张图表。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/7c2d24567577e02286ca44b6912675e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Kv-VtY4JRJVnnopR7Io1w.png"/></div></div></figure><p id="c575" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">所以这个函数是对称的，这是一个必需的特性，因为这个函数应该对任何一个类都有效。并且该函数的输出可以指示该区域的“均匀性”:越低越好</p><ul class=""><li id="93ee" class="lr ls it ks b kt ku kw kx kz lt ld lu lh lv ll lw lx ly lz bi translated">当p接近1时，几乎所有的点都是蓝色的，指示值很低</li><li id="1173" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">当p接近0时，那么几乎所有的点都是红色的，那么指标也很低</li><li id="4f39" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">当p为0.5时，那么我们有相同数量的红点和蓝点，这不是理想的状态。该指标处于最高水平。</li></ul><p id="83e0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了找到决策边界，我们必须测试不同的x值。对于x的每个值，创建两个区域:左手侧区域和右手侧区域。对于每一方，我们可以计算指标，然后我们用每个区域的分数比例对它们进行加权，以获得总体指标。</p><p id="e271" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，我们可以测试作为决策边界的所有点，并查看指标如何变化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/3632549a0614eef9e9ef797d90fab222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yHdmFkTccleo-wnA7Swh4Q.png"/></div></div></figure><p id="03d2" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，我们可以采用指标的最低级别来找到x的最佳值。注意，由于我们有一个阶跃函数，我们可以计算定义指标最低级别的两个x值的平均值。</p><p id="e3db" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这个原则的特别之处在于，你可以在每个区域中继续寻找其他决策边界。我们将在另一篇专门讨论该原则的文章中看到，这样做的好处是您可以轻松处理非线性情况。</p><p id="e330" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">所以一步一步，我们可以一个一个的找到最优前沿。</p><p id="954d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这里我们有一个决策树，关于每一步不同的决策界限。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/7ff9e68304ac4c48a9f2d9d79af007f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kNUMTB-BrRc9mk1VfeFzWA.png"/></div></div></figure><p id="3650" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在的问题是:我们什么时候停止？可以有不同的规则…</p><p id="61fd" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这个原则也被认为是“分而治之”。它让我们能够生长决策树。这些树也是更复杂算法的基础，如随机森林或梯度推进机器。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="1c34" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们回顾一下:</p><ul class=""><li id="2ff4" class="lr ls it ks b kt ku kw kx kz lt ld lu lh lv ll lw lx ly lz bi translated">原则1:检查邻居，我们得到KNN</li><li id="f5d7" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">原则二:考虑全球比例和正态分布，我们可以做LDA，QDA，或者NB。</li><li id="622b" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">原则三:研究边疆地区，尽量做到“干净”和大。我们得到了SVM。</li><li id="5fa3" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">原则四:画一条直线，平滑，试着调整。我们得到逻辑回归</li><li id="9ea2" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">原则5:分而治之。我们得到了决策树。</li></ul><p id="1a2d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">请注意，我们没有完成五个原则的所有推理:</p><ul class=""><li id="40fc" class="lr ls it ks b kt ku kw kx kz lt ld lu lh lv ll lw lx ly lz bi translated">用kNN法测定水中的钾</li><li id="9a14" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">决策树中的停止规则</li><li id="952e" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">系数C的确定</li><li id="20fb" class="lr ls it ks b kt nf kw ng kz nh ld ni lh nj ll lw lx ly lz bi translated">逻辑回归中a和b的确定</li></ul></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="e336" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">对于机器学习从业者来说，你可以注意到我自愿没有使用技术术语，也许你可以评论并把技术术语放在正确的上下文中:硬边、软边、梯度下降、凸性、超平面、先验概率、后验概率、损失函数、交叉熵、最大似然估计、过拟合……</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="05e4" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">当我们谈论机器学习和人工智能时，我们总是谈论神经网络，但为什么我们在这里没有提到它们？实际上，我们确实看到了一个简单的神经网络的例子<strong class="ks iu">，它是<strong class="ks iu">逻辑回归</strong>，关于它的文章即将发表。</strong></p><p id="ad90" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">使用的玩具数据非常简单:这两个类是线性可分的。这些算法是如何处理非线性可分的数据的？你可以阅读这篇文章:<a class="ae nk" rel="noopener" target="_blank" href="/intuitively-how-can-we-build-non-linear-classifiers-10c381ed633e">直观地，我们如何才能建立非线性分类器</a>。</p></div></div>    
</body>
</html>