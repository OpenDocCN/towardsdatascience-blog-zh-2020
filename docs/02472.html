<html>
<head>
<title>Interpreting Text Based Machine Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释基于文本的机器学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpreting-text-based-machine-learning-models-aa37218195c9?source=collection_archive---------19-----------------------#2020-03-09">https://towardsdatascience.com/interpreting-text-based-machine-learning-models-aa37218195c9?source=collection_archive---------19-----------------------#2020-03-09</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><figure class="it iu gq gs iv iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj is"><img src="../Images/b9a5a08ff2d129589a924df098830ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oTMmSr8ntswdSHLs3FPU0A.jpeg"/></div></div><p class="jd je gk gi gj jf jg bd b be z dk translated">本·怀特在<a class="ae jh" href="https://unsplash.com/s/photos/reading?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="c93a" class="pw-subtitle-paragraph kh jj jk bd b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky dk translated">可解释文本预测的工具和技术</h2></div><p id="1d27" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">就像任何其他机器学习任务一样，重要的是要有工具来解释基于文本的机器学习模型如何得出它们的预测。</p><p id="17ad" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">能够理解和解释机器学习模型如何得出预测是非常重要的。对模型如何得出结论的准确理解将使您能够构建更好、更准确的模型，并确保这些模型不会传播数据集中可能存在的偏差。</p><p id="4cf4" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在这篇文章中，我想分享一些解释基于语言的机器学习的工具和技术。</p></div><div class="ab cl lv lw hy lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="in io ip iq ir"><h2 id="f000" class="mc md jk bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">用黄砖想象</h2><p id="fa77" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu in bi translated">为机器学习准备文本数据的一个常见步骤是对单词进行标记。这一步将语言片段分解成它们的组成部分，通常是单词。在建立模型之前，了解您正在处理的数据的大致模式会很有用。这可以为您使用的处理技术提供信息，甚至在您建立模型之前，就有可能揭示偏差的趋势。</p><p id="1fed" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><a class="ae jh" href="https://www.scikit-yb.org/en/latest/" rel="noopener ugc nofollow" target="_blank"> Yellowbrick </a>是一个基于 scikit-learn 构建的机器学习模型可视化库。在其他可视化中，它包含了一些有用的可视化文本标记的方法。</p><p id="b154" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">为了说明其中的一些技术，我将使用当前 Kaggle 竞赛中的一个数据集，它可以在这里下载<a class="ae jh" href="https://www.kaggle.com/c/nlp-getting-started" rel="noopener ugc nofollow" target="_blank"/>。这个数据集包含许多推文和一个目标标签，它告诉我们一条推文是否是关于一场真正的灾难。</p><p id="0c0b" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在使用这个数据集之前，我已经执行了一些基本的文本清理任务。其中包括删除标点符号、特殊字符和停用词。</p><p id="a884" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">下面的代码读入数据并执行这些任务。</p><pre class="na nb nc nd gu ne nf ng nh aw ni bi"><span id="2912" class="mc md jk nf b gz nj nk l nl nm">import pandas as pd<br/>pd.set_option('display.max_colwidth', -1)<br/>import re<br/>import nltk.corpus<br/>nltk.download('stopwords')<br/>from nltk.corpus import stopwords<br/>stop = stopwords.words('english')</span><span id="a869" class="mc md jk nf b gz nn nk l nl nm"># Reads in the data<br/>train_data = pd.read_csv('train.csv')</span><span id="a68e" class="mc md jk nf b gz nn nk l nl nm"># Drops all columns text and target<br/>cols_to_drop = ['id', 'keyword', 'location']<br/>train_data = train_data.drop(cols_to_drop, axis=1)</span><span id="9551" class="mc md jk nf b gz nn nk l nl nm"># Removes punctuation and special characters<br/>def  clean_text(df, text_field, new_text_field_name):<br/>    df[new_text_field_name] = df[text_field].str.lower()<br/>    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", elem))  <br/>    # remove numbers<br/>    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"\d+", "", elem))<br/>    <br/>    return df</span><span id="7f0e" class="mc md jk nf b gz nn nk l nl nm">data_clean = clean_text(train_data, 'text', 'text')</span><span id="9745" class="mc md jk nf b gz nn nk l nl nm"># Removes stop words <br/>data_clean['text'] = data_clean['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))</span></pre><p id="d55b" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">干净数据集的前几行如下所示。</p><figure class="na nb nc nd gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj no"><img src="../Images/5dc7eaf1eddfc416b2d76857125f4fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cIyqdFLsSl4L80UWLol8MQ.png"/></div></div></figure><p id="cb33" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">接下来，我们将使用 yellowbrick 库来标记文本并检查频繁出现的单词。</p><pre class="na nb nc nd gu ne nf ng nh aw ni bi"><span id="1d8d" class="mc md jk nf b gz nj nk l nl nm">import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from yellowbrick.text import FreqDistVisualizer</span><span id="927b" class="mc md jk nf b gz nn nk l nl nm">vectorizer = CountVectorizer()<br/>docs       = vectorizer.fit_transform(data_clean['text'])<br/>features   = vectorizer.get_feature_names()</span><span id="cfba" class="mc md jk nf b gz nn nk l nl nm">visualizer = FreqDistVisualizer(features=features, orient='v')<br/>visualizer.fit(docs)<br/>visualizer.show()</span></pre><figure class="na nb nc nd gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj np"><img src="../Images/81d8d9c6fc805772eedebbf158ae84fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ZUwseeninGHjyQJeyEPtw.png"/></div></div></figure><p id="0728" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">一个有用的练习可能是将这两个目标标签的数据分别可视化，以理解模型可以用来进行预测的数据中的模式。</p><p id="b1fa" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">灾难推文的代码和输出如下所示。</p><pre class="na nb nc nd gu ne nf ng nh aw ni bi"><span id="50f4" class="mc md jk nf b gz nj nk l nl nm">disaster_tweets = data_clean[data_clean['target'] == 1]<br/>vectorizer = CountVectorizer()<br/>docs       = vectorizer.fit_transform(disaster_tweets['text'])<br/>features_disaster   = vectorizer.get_feature_names()</span><span id="00f9" class="mc md jk nf b gz nn nk l nl nm">visualizer_disaster = FreqDistVisualizer(features=features_disaster, orient='v')<br/>visualizer_disaster.fit(docs)<br/>visualizer_disaster.show()</span></pre><figure class="na nb nc nd gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nq"><img src="../Images/ebed067af603f8759ce714a4237c31f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*46xJpW5ZXiQ7GsNm67tXbw.png"/></div></div></figure><p id="c9fc" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">接下来，我们对非灾难性的推文重复这个过程。</p><pre class="na nb nc nd gu ne nf ng nh aw ni bi"><span id="9a18" class="mc md jk nf b gz nj nk l nl nm">non_disaster_tweets = data_clean[data_clean['target'] == 0]</span><span id="be7d" class="mc md jk nf b gz nn nk l nl nm">vectorizer = CountVectorizer()<br/>docs       = vectorizer.fit_transform(non_disaster_tweets['text'])<br/>features_non_disaster   = vectorizer.get_feature_names()</span><span id="4f46" class="mc md jk nf b gz nn nk l nl nm">visualizer_non_disaster = FreqDistVisualizer(features=features_non_disaster, orient='v')<br/>visualizer_non_disaster.fit(docs)<br/>visualizer_non_disaster.show()</span></pre><figure class="na nb nc nd gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nr"><img src="../Images/a300bdb73fccbaa8b724af66434fb17d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhBkj1HwZ04lW3WnNWN0OA.png"/></div></div></figure><p id="eafe" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">从这个分析中我们可以看到，有很多表示灾难的词，如<strong class="lb jl">火灾</strong>、<strong class="lb jl">新闻</strong>和<strong class="lb jl">灾难</strong>在灾难微博中出现得非常频繁，但在非灾难微博中出现得很少。</p><h2 id="4772" class="mc md jk bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">用 Shap 解释模型</h2><p id="7282" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu in bi translated"><a class="ae jh" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">Shap</a>(SHapley Additive explaints)是一个 python 库，它使用博弈论的方法来为从机器学习模型中得出的预测提供解释。它包括解释基于文本的模型的功能，并与包括 scikit-learn 在内的各种机器学习和深度学习库一起工作。</p><p id="405f" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">在我们使用这个库来解释一个模型之前，我们首先需要建立一个模型。下面的代码将我们的数据分成训练集和测试集，并训练一个简单的基于逻辑回归的分类模型。</p><pre class="na nb nc nd gu ne nf ng nh aw ni bi"><span id="4769" class="mc md jk nf b gz nj nk l nl nm">from sklearn.model_selection import train_test_split<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>import sklearn</span><span id="b090" class="mc md jk nf b gz nn nk l nl nm"><br/>X_train, X_test, y_train, y_test = train_test_split(data_clean['text'],data_clean['target'],random_state = 0)</span><span id="bdce" class="mc md jk nf b gz nn nk l nl nm">vectorizer = TfidfVectorizer(min_df=10)<br/>X_train = vectorizer.fit_transform(X_train)<br/>X_test = vectorizer.transform(X_test)</span><span id="3464" class="mc md jk nf b gz nn nk l nl nm">model = sklearn.linear_model.LogisticRegression(penalty="l1", C=0.1)<br/>model.fit(X_train, y_train)</span></pre><p id="5f1b" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">接下来，我们将导入 shap 并使用<code class="fe ns nt nu nf b">summary_plot</code>函数来探索模型中使用的特性重要性。</p><pre class="na nb nc nd gu ne nf ng nh aw ni bi"><span id="fb48" class="mc md jk nf b gz nj nk l nl nm">import shap<br/>shap.initjs()</span><span id="54e5" class="mc md jk nf b gz nn nk l nl nm">explainer = shap.LinearExplainer(model, X_train, feature_dependence="independent")<br/>shap_values = explainer.shap_values(X_test)<br/>X_test_array = X_test.toarray()</span><span id="a955" class="mc md jk nf b gz nn nk l nl nm">shap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names())</span></pre><figure class="na nb nc nd gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nv"><img src="../Images/34673b3ae8de5a73c7f63388353a5ce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PPScp37GbaG08oTLgkLe1Q.png"/></div></div></figure><p id="47e6" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我们可以看到，我们用 yellowbrick 探索的灾难推文中频繁出现的许多词具有很高的特征值。</p><p id="0775" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">我们也可以用 shap 来解释个别预测。</p><pre class="na nb nc nd gu ne nf ng nh aw ni bi"><span id="b7be" class="mc md jk nf b gz nj nk l nl nm">ind = 0<br/>shap.force_plot(<br/>    explainer.expected_value, shap_values[ind,:], X_test_array[ind,:],<br/>    feature_names=vectorizer.get_feature_names()<br/>)</span></pre><figure class="na nb nc nd gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nw"><img src="../Images/eb9a6c152e6bba1023a1e0274678cc38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bjeo3Vp8OAqgknxKzFFSQA.png"/></div></div></figure><h2 id="3b98" class="mc md jk bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">用 ELI5 进行解释</h2><p id="9507" class="pw-post-body-paragraph kz la jk lb b lc mv kl le lf mw ko lh li mx lk ll lm my lo lp lq mz ls lt lu in bi translated">ELI5 是另一个 python 库，它同样为预测提供解释。它有一些非常好的渲染，特别是对基于文本的模型的解释。</p><p id="427c" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">它支持大多数常用的机器学习库，包括 scikit-learn、XGBoost 和 Keras。它还可以很好地与 scikit-learn 管道一起工作。</p><p id="d744" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">与 shap 类似，ELI5 提供了特征重要性的权重，这有助于在全局范围内为模型提供解释。</p><p id="cf98" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">使用我们上面训练的模型，并提供用于预处理文本的矢量器，我们可以产生如下所示的可视化效果。默认情况下，对于二元分类，ELI5 将显示阳性类的要素权重。</p><pre class="na nb nc nd gu ne nf ng nh aw ni bi"><span id="6ec5" class="mc md jk nf b gz nj nk l nl nm">import eli5</span><span id="0d52" class="mc md jk nf b gz nn nk l nl nm">eli5.show_weights(model, vec=vectorizer, top=10)</span></pre><figure class="na nb nc nd gu iw gi gj paragraph-image"><div class="gi gj nx"><img src="../Images/bd682d3c9e933d3b6237b7249cc731b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*Iw1Z6XfDbeI90gPpmRLqpw.png"/></div></figure><p id="2fc3" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">这个库中我最喜欢的功能是一个工具，它可以在原始文本上叠加对单个预测的解释。第一条推文的例子如下所示。我们可以看到，该模型预测这是一条灾难性的推文，并且该模型使用了突出显示的单词来进行预测。</p><pre class="na nb nc nd gu ne nf ng nh aw ni bi"><span id="2be7" class="mc md jk nf b gz nj nk l nl nm">eli5.show_prediction(model, data_clean['text'].iloc[0], vec=vectorizer)</span></pre><figure class="na nb nc nd gu iw gi gj paragraph-image"><div class="gi gj ny"><img src="../Images/4915a87978c386fdac65729dbbcf1beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*rpzkxvxDVP1D8zpz4X-9vg.png"/></div></figure><p id="8dfe" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">原始数据集中的行显示这是正确预测的，是一条灾难性的推文。</p><pre class="na nb nc nd gu ne nf ng nh aw ni bi"><span id="b016" class="mc md jk nf b gz nj nk l nl nm">print(data_clean.iloc[0])</span></pre><figure class="na nb nc nd gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj nz"><img src="../Images/b1d7ab44930c94d0b9a6713bb7b4108a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kGdOpSEfmPvqxwh1zETknA.png"/></div></div></figure><p id="0f37" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">让我们看一个非灾难推文的例子。</p><pre class="na nb nc nd gu ne nf ng nh aw ni bi"><span id="1aaa" class="mc md jk nf b gz nj nk l nl nm">eli5.show_prediction(model, data_clean['text'].iloc[57], vec=vectorizer)</span><span id="fd10" class="mc md jk nf b gz nn nk l nl nm">print(data_clean.iloc[57])</span></pre><figure class="na nb nc nd gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj oa"><img src="../Images/bb894ddf46bf5707a120cb12eb9e6628.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sg2OtmLG1DOCcuq4I3nZcQ.png"/></div></div></figure><figure class="na nb nc nd gu iw gi gj paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="gi gj ob"><img src="../Images/e6d06a7c34bdfce26648b4824e6e5bc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hxG-mLtSmOp6T6HN3GiBXg.png"/></div></div></figure><p id="0f48" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">这让我们了解了模型是如何进行预测的，并可能揭示模型中的任何偏见或道德问题。</p></div><div class="ab cl lv lw hy lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="in io ip iq ir"><p id="f776" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">本文简要概述了解释和调试基于文本的机器学习模型的技术。所有这些工具都有更多的功能，可以让您更深入地了解模型是如何执行的。值得看一看每一项的文档，并探索其他可用的特性。</p><p id="f758" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">如果您想探索自然语言处理中的更多概念，我最近写了两篇文章，在本文中使用了相同的数据集。它们包括:</p><p id="4664" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated"><a class="ae jh" rel="noopener" target="_blank" href="/how-to-enter-your-first-kaggle-competition-4717e7b232db">构建机器学习模型参加 Kaggle 比赛的指南</a>。</p><p id="c94e" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">自然语言处理的<a class="ae jh" rel="noopener" target="_blank" href="/text-cleaning-methods-for-natural-language-processing-f2fc1796e8c7">文本清理方法指南</a>。</p><p id="70a4" class="pw-post-body-paragraph kz la jk lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">感谢阅读！</p></div></div>    
</body>
</html>