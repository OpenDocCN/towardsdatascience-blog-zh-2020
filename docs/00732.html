<html>
<head>
<title>Which one is more important? Be careful before you make decisions with Random Forest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">哪个更重要？在你用随机森林做决定之前要小心</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-relook-on-random-forest-and-feature-importance-2467dfab5cca?source=collection_archive---------8-----------------------#2020-01-21">https://towardsdatascience.com/a-relook-on-random-forest-and-feature-importance-2467dfab5cca?source=collection_archive---------8-----------------------#2020-01-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1747" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">再看特征重要性和随机森林</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d6a1a91afe275a63ad06ffca266e304d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5eXzDhisv_UG6MHwrB4UQw.jpeg"/></div></div></figure><p id="cf86" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated">无论你是谁，是一名刚刚完成他/她的第一门机器学习课程的学生，还是一名经验丰富的数据科学家，或者基本上是当今任何从事技术工作的人，你都必须听说过Random Forest。随机森林是一种集合树模型，主要用于分类。它出现于90年代，至今仍是许多行业中最常用、最稳定、最精确的模型之一。</p><p id="3956" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，今天我们不会关注随机森林本身。相反，我们应该重新审视一下<strong class="kw iu">特征重要性</strong>，或者可变重要性，不管你喜欢怎么称呼它。我们都知道，大多数随机森林实现(例如s <em class="lz"> klearn，也称为Sci-Kit Learn </em>)都有内置的可用特性重要性，这种特性重要性最早出现在Leo Breiman在2001年的论文“<a class="ae ma" href="https://link.springer.com/article/10.1023/A:1010933404324" rel="noopener ugc nofollow" target="_blank">随机森林</a>”中，该论文首次恰当地介绍了随机森林。也就是说，将所有节点的加权杂质减少相加，并对所有树进行平均。这种方法被称为MDI或平均减少杂质。</p><h1 id="4946" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">1.基尼系数和排列重要性</h1><p id="8c54" class="pw-post-body-paragraph ku kv it kw b kx mt ju kz la mu jx lc ld mv lf lg lh mw lj lk ll mx ln lo lp im bi translated">MDI中的杂质实际上是一个函数，当我们使用一个众所周知的杂质函数<em class="lz"> Gini index </em>时，该度量就变成了Gini importance，内置在Sklearn的RandomForestClassifier中。</p><p id="120d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">除了Gini重要性或MDI，还有另一种评估特征重要性的方法，即随机置换袋外样本中的特征值。这被称为平均降低精度，MDA，也称为排列重要性。已经有关于评估特征重要性的这两种不同方法的比较的研究:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="f7d4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我将使用一个非常简单的展示来比较这两种方法，使用python中的<em class="lz"> sklearn </em>包中的随机森林分类器和Iris数据集，以及<em class="lz">rfpip</em>包(用于计算排列重要性)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/cbc16d62d349fd9f68867f59b8cb1163.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tetXPBayGcb8jWsYI7QNBA.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">使用sklearn的两种方法的比较</p></figure><p id="e6e6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于这样一个简单的数据集，这两种方法给出了不同的结果，这再明显不过了。此外，如果您自己尝试多次(您可以在页面底部找到我的代码)，您将会看到这些结果实际上每次都有所不同。这是因为这两种方法不仅都使用了某种类型的随机抽样，而且都来自于一个具有随机内在特性的随机森林模型。</p><p id="7d01" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您应该注意的另一件事是运行时。如前所述，与排列重要性相比，内置的基尼重要性具有几乎实时的运行时间。但是在我们的例子中，由于Iris的大小，差异并不是很大:它只有150个观察值，只有4个特征。如果我们在更大的数据集上进行比较:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/441e7761d63b95de608b8f4d86f1e865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*wWg9rWQ4yp1INNKnxRSD7w.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">使用较大数据集的运行时比较</p></figure><p id="614f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在你一定在想，我有这些优点和缺点，我应该用哪一个呢？嗯，也许有第三条路比这两条都好？</p><h1 id="02a0" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">2.空气(实际杂质减少)的重要性</h1><p id="a978" class="pw-post-body-paragraph ku kv it kw b kx mt ju kz la mu jx lc ld mv lf lg lh mw lj lk ll mx ln lo lp im bi translated">在上面的比较中，我们知道，与排列重要性相比，基尼系数的重要性以惊人的速度增长。如果我们能在基尼系数1上做一点小小的改变，这样我们不仅能得到更快的结果，而且偏差也更小，会怎么样呢？<em class="lz"> Stefano Nembrini </em>在他的<a class="ae ma" href="https://academic.oup.com/bioinformatics/article/34/21/3711/4994791" rel="noopener ugc nofollow" target="_blank"> 2018论文</a>中介绍了这样的方式:</p><blockquote class="ng nh ni"><p id="3cf4" class="ku kv lz kw b kx ky ju kz la lb jx lc nj le lf lg nk li lj lk nl lm ln lo lp im bi translated">在常规RF中，在每个树节点处，从𝕆≡{1,…,p}⁠.采样mtry分裂候选变量相反，我们从1到2 <em class="it"> p </em>采样，即变量从𝕆∪ℙ⁠采样，而ℙ≡{p+1,…,2p}⁠.如果选择的变量索引为j∈𝕆⁠，那么Xi=j通常用于拆分。如果j∈ℙ⁠，变量π<em class="it">Xi = j p</em>，即具有重新排序的样本ID的原始变量，用于分割。如果是j∈𝕆⁠，这种分裂导致的杂质减少通常会导致<em class="it"> Xi </em>的重要性变化，而如果是j∈ℙ⁠.，则杂质减少请注意，不会重复执行该过程。</p><p id="cb9b" class="ku kv lz kw b kx ky ju kz la lb jx lc nj le lf lg nk li lj lk nl lm ln lo lp im bi translated">最后，在整个森林生长之后，变量<em class="it"> Xi </em>的估计去偏置杂质重要性计算如下:</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ee2e7f34ef429e396c8d90170e069f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*tgdbAQYGwMDFLdcwZI3PyQ.png"/></div></figure><blockquote class="ng nh ni"><p id="d7b6" class="ku kv lz kw b kx ky ju kz la lb jx lc nj le lf lg nk li lj lk nl lm ln lo lp im bi translated">我们将这种新的VIM称为实际杂质减少(空气),以避免与其他VIM产生误解。</p></blockquote><p id="0e8f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">可悲的是，自从AIR被引入才一年，并没有很多研究关注这种新方法的正确性和准确性。此外，AIR目前仅在R库<em class="lz"> Ranger </em>中可用，这是一个因其在R上快速实现随机森林而闻名的库。当分配新的Ranger对象时，您可以将参数“importance”指定为“infinity _ corrected”以使用AIR重要性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/6609480d7380198daa3a54478b05d0ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*enLWuEnZueEPobxDVbqsLg.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">Ranger (R)中可用的三种不同方法的条形图</p></figure><p id="052e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里的运行时间是在一定重要性方法下拟合随机森林模型所花费的时间。看起来三种方法的性能相似，主要是因为我们使用的数据集很小。如果我们把它换成我们之前用5000个观察值和500个特征建立的数据集，我们仍然可以看到运行时间上的巨大差距:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/f81dfb8fca7d67dc082506c8f23e54d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2J9R1k_Uqoa7kR3-TCf3IA.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">5000 * 500数据集上三种不同方法的条形图，此处仅显示前10个特征</p></figure><h1 id="6666" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">3.鸣人？博鲁塔！</h1><p id="2b30" class="pw-post-body-paragraph ku kv it kw b kx mt ju kz la mu jx lc ld mv lf lg lh mw lj lk ll mx ln lo lp im bi translated"><em class="lz"> Boruta </em>也是R上的一个库，专注于使用置换随机值来选择特征。它将所有特征分为三类:拒绝、尝试和确认。希望感谢<a class="ae ma" href="http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/" rel="noopener ugc nofollow" target="_blank"> <em class="lz">丹尼尔·霍莫拉</em> </a>，这个特性选择库在python中可用。它速度更快，并且有更多的自定义设置。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/8b03c5c5d3b52778f6eaf8c7062959dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*ZBj-N8hnY_cIXzzM1SG7xQ.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">经过8次迭代后，Boruta选择了所有4个“重要”的特性</p></figure><p id="77a1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">与R上的原始版本相比，python上的Boruta 有一些令人兴奋的有趣的新参数，包括我最喜欢的一个，<em class="lz"> perc </em>，它为功能选择设置了阈值的百分位数，如果感觉太苛刻的话。将此值设置为较低的值允许选择更多的功能，但也会导致较高的错误率。</p><h1 id="22dd" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">4.Shap，在Shapley值中</h1><p id="501a" class="pw-post-body-paragraph ku kv it kw b kx mt ju kz la mu jx lc ld mv lf lg lh mw lj lk ll mx ln lo lp im bi translated">我知道你可能在博弈论讲座中听过这个名字很多次，在一些专注于数据科学的网站上也试图介绍这个包。是的，这个包是用博弈论的方法编写的，是的，它是一个强大而流行的工具，但是我不打算解释关于博弈论和算法本身的事实。我将用一个简单的例子来说明它能做什么。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="5ce2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，假设我们想看看数据的第一次观察如何影响预测:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/0257fd83d4de7a0487e5c58005c5494b.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*Fdyl0TyceP835pOwHuE2aQ.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">虹膜的首次观察</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/9638722645b537593982cc5fdcc9b370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FVXI1pdvG834bk_Y2KgDVA.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">单一观察值的解释</p></figure><p id="5326" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到花瓣长度为1.4厘米和花瓣宽度为0.2厘米是第一次预测为1的主要“原因”。</p><p id="922b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们稍微移动一下代码，我们还可以看到数据集中每个观察的效果:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/e06f8e91ba062e361ab8fe810a3e82d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4OleCnZHg6JhzCpUDl8fug.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">所有观察结果的解释</p></figure><p id="a3f6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Shap的可视化不仅丰富多彩，更好看，而且是交互式的，这意味着如果你将鼠标移到它上面，你可以看到每个观察的信息。然而，正如你们中的一些人可能已经发现的那样，Shap与多类别分类斗争。上面的X轴是按原始样本排序的，应与Iris一样，保持第1类50、第2类50和第3类50的顺序。</p><p id="1f43" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好了，最后但同样重要的是，回到特性重要性。使用shap值，我们可以解释特性如何影响模型的输出。因此，我们可以预期shap值是特征重要性的度量:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/7fc5cf8fccdf828d87d3c94f81977794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJZ5H_yD3lKdEATZKKCXQg.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">每个特征的平均绝对形状值的条形图</p></figure><p id="3fce" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">几乎和我们之前谈过的方法一样的趋势，是吗？然而，无论您为同一个随机森林模型运行多少次代码，您都会得到相同的结果:Shap值为要素提供了一致的重要性评估。</p><h1 id="6e16" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">结论</h1><p id="0a09" class="pw-post-body-paragraph ku kv it kw b kx mt ju kz la mu jx lc ld mv lf lg lh mw lj lk ll mx ln lo lp im bi translated">我们已经讨论了使用随机森林模型评估要素重要性的所有不同方法。哪个最好？谁也说不准。我们只有数学证据证明哪一个更有偏见。老实说，你可以接受上面为Iris数据集介绍的任何特征重要性，因为这个问题没有正确的答案。毕竟，机器学习和数据科学都与解释有关。无论你使用什么工具，只要它有意义，并且你能让数据故事流动，使用它绝对没有错。下面是我做的比较所有这些方法的表格(运行时间使用之前生成的5000*500数据集计算):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/d2304afd9fe918f36e210d306f3ee4b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NnfJ_9eTjLSLmM1g05UP0w.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">* : AIR仅在R上可用，AIR运行时与Gini运行时在R上几乎相同</p></figure><p id="bc13" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后但同样重要的是，<em class="lz"> LIME </em>也是用于特性重要性的非常流行的包之一。希望我将来会介绍<em class="lz">石灰</em>，并添加所有这些工具之间的比较。</p><p id="d827" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">点击<a class="ae ma" href="https://github.com/Frank-Xu-Huaze/Medium/tree/master/Feature_Importance" rel="noopener ugc nofollow" target="_blank">此处</a>查看来自我的Github的所有源代码。</p></div></div>    
</body>
</html>