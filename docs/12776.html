<html>
<head>
<title>How to implement Linear Regression with NumPy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 NumPy 实现线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-implement-linear-regression-with-numpy-172790d2f1bc?source=collection_archive---------29-----------------------#2020-09-02">https://towardsdatascience.com/how-to-implement-linear-regression-with-numpy-172790d2f1bc?source=collection_archive---------29-----------------------#2020-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="708d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">更好地理解线性回归并提高您的数字技能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7356e24c4c88918fed547de204087928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qjV2IYI8B_dAQGP2u8Jrrw.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6a20" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们先简单回忆一下什么是线性回归:</p><blockquote class="lu lv lw"><p id="0c59" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">L <em class="it">线性回归是根据一些其他已知变量以线性方式估计未知变量。视觉上，我们通过我们的数据点拟合一条线(或更高维的超平面)。</em></p></blockquote><p id="9cec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你对这个概念不太适应，或者想更好地理解它背后的数学原理，你可以阅读我以前写的关于线性回归的文章:</p><div class="mb mc gp gr md me"><a rel="noopener follow" target="_blank" href="/understanding-linear-regression-eaaaed2d983e"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd iu gy z fp mj fr fs mk fu fw is bi translated">了解线性回归</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">线性回归背后的数学详细解释</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms ks me"/></div></div></a></div></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="114c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们把重点放在实现上。</p><p id="341b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，很明显，我们需要导入一些库。我们导入<code class="fe na nb nc nd b">numpy</code>，因为它是我们用于实现的主要内容，<code class="fe na nb nc nd b">matplotlib</code>用于可视化我们的结果，以及来自<code class="fe na nb nc nd b">sklearn</code>的<code class="fe na nb nc nd b">make_regression</code>函数，我们将使用它来生成一个回归数据集作为示例。</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="2476" class="ni nj it nd b gy nk nl l nm nn"><strong class="nd iu">import</strong> <strong class="nd iu">numpy</strong> <strong class="nd iu">as</strong> <strong class="nd iu">np</strong></span><span id="7115" class="ni nj it nd b gy no nl l nm nn"><strong class="nd iu">import</strong> <strong class="nd iu">matplotlib.pyplot</strong> <strong class="nd iu">as</strong> <strong class="nd iu">plt</strong></span><span id="01e0" class="ni nj it nd b gy no nl l nm nn"><strong class="nd iu">from</strong> <strong class="nd iu">sklearn.datasets</strong> <strong class="nd iu">import</strong> make_regression</span></pre><p id="0396" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后我们将使用以下方法创建一个<code class="fe na nb nc nd b">LinearRegression</code>类:</p><ul class=""><li id="0d86" class="np nq it la b lb lc le lf lh nr ll ns lp nt lt nu nv nw nx bi translated"><code class="fe na nb nc nd b">.fit()</code> —该方法将实际学习我们的线性回归模型；在这里，我们将找到最佳权重</li><li id="1c2f" class="np nq it la b lb ny le nz lh oa ll ob lp oc lt nu nv nw nx bi translated"><code class="fe na nb nc nd b">.predict()</code> —这个将用于预测；它将返回我们的线性模型的输出</li><li id="eca3" class="np nq it la b lb ny le nz lh oa ll ob lp oc lt nu nv nw nx bi translated"><code class="fe na nb nc nd b">.rmse()</code> —用给定的数据计算我们的模型的均方根误差；这个指标有点像“从我们的模型估计值到真实 y 值的平均距离”</li></ul><p id="cc9d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在<code class="fe na nb nc nd b">.fit()</code>中做的第一件事是将一个额外的 1 列连接到我们的输入矩阵 x。这是为了简化我们的数学，并将偏差视为一个始终为 1 的额外变量的权重。</p><p id="8e88" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe na nb nc nd b">.fit()</code>方法将能够通过使用封闭公式或随机梯度下降来学习参数。为了选择使用哪一个，我们将有一个名为<code class="fe na nb nc nd b">method</code>的参数，它需要一个字符串“solve”或“sgd”。</p><p id="a6b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当<code class="fe na nb nc nd b">method</code>设置为“求解”时，我们将通过以下公式获得模型的权重:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/81cf1575b2c991ff9ababd19c7e41831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*zIbw0VcfIiZVjuKliUNgRQ.png"/></div></figure><p id="c2e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这要求矩阵 X 具有满列秩；因此，我们将检查这一点，否则我们会显示一条错误消息。</p><p id="0fbd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的第一部分<code class="fe na nb nc nd b">.fit()</code>方法是:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="dcee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意<code class="fe na nb nc nd b">method</code>之后的其他参数是可选的，仅在我们使用 SGD 的情况下使用。</p><p id="66dd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该方法的第二部分处理<code class="fe na nb nc nd b">method = ‘sgd’</code>的情况，它不要求 X 具有完整的列秩。</p><p id="3d67" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的最小二乘线性回归的 SGD 算法概述如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/10ce771fffe67668a5488a8a04325a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MdsoOAIddbg2TEUdPNKuhw.png"/></div></div></figure><p id="9304" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将通过将 weights 类属性初始化为一个 numpy 向量来开始此算法，该向量的值取自均值为 0、标准差为 1/(列数)的正态分布。我们将标准偏差除以列数，以确保在算法的初始阶段不会得到太大的输出值。这是为了帮助我们更快地收敛。</p><p id="3d22" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在每次迭代的开始，我们随机地打乱我们的数据行。然后，对于每一批，我们计算梯度并将其从当前权重向量中减去(乘以学习率),以获得新的权重。</p><p id="af4b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是我们<code class="fe na nb nc nd b">.fit()</code>方法的后半部分:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="03a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们从这个方法返回<code class="fe na nb nc nd b">self</code>，以便能够像这样连接构造函数和<code class="fe na nb nc nd b">.fit()</code>的调用:<code class="fe na nb nc nd b">lr = LinearRegression().fit(X, y, ‘solve’)</code>。</p><p id="ab69" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe na nb nc nd b">.predict()</code>方法相当直接。我们首先检查之前是否调用了<code class="fe na nb nc nd b">.fit()</code>，然后将一列 1 连接到 X，并验证 X 的形状允许与权重向量相乘。如果一切正常，我们只需返回 X 和权重向量相乘的结果作为预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="d015" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<code class="fe na nb nc nd b">.rmse()</code>中，我们首先使用<code class="fe na nb nc nd b">.predict()</code>获得模型的输出，然后如果预测期间没有错误，我们计算并返回均方根误差，该误差可以被认为是“从我们的模型估计值到真实 y 值的平均距离”。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="9f7f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是<code class="fe na nb nc nd b">LinearRegression</code>类的完整代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><h2 id="4923" class="ni nj it bd oh oi oj dn ok ol om dp on lh oo op oq ll or os ot lp ou ov ow ox bi translated">在一个例子中使用我们的<code class="fe na nb nc nd b">LinearRegression</code>类</h2><p id="8b32" class="pw-post-body-paragraph ky kz it la b lb oy ju ld le oz jx lg lh pa lj lk ll pb ln lo lp pc lr ls lt im bi translated">为了展示我们的线性回归实现，我们将使用来自<code class="fe na nb nc nd b">sklearn</code>的<code class="fe na nb nc nd b">make_regression()</code>函数生成一个回归数据集。</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="c6d6" class="ni nj it nd b gy nk nl l nm nn">X, y = make_regression(n_features=1,<br/>                       n_informative=1,<br/>                       bias=1, noise=35)</span></pre><p id="a810" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们绘制这个数据集，看看它是什么样子的:</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="0698" class="ni nj it nd b gy nk nl l nm nn">plt.scatter(X, y)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/3dc97a3541bbd2d0af4692be5130eb0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P2GfyqX_jE9M9-0WaTkTpQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d38e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe na nb nc nd b">make_regression()</code>返回的 y 是平面向量。我们将把它改造成一个列向量，用于我们的<code class="fe na nb nc nd b">LinearRegression</code>类。</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="eec8" class="ni nj it nd b gy nk nl l nm nn">y = y.reshape((-1, 1))</span></pre><p id="7a15" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们将使用<code class="fe na nb nc nd b">method = ‘solve’</code>来拟合回归线:</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="5495" class="ni nj it nd b gy nk nl l nm nn">lr_solve = LinearRegression().fit(X, y, method='solve')</span><span id="5c28" class="ni nj it nd b gy no nl l nm nn">plt.scatter(X, y)</span><span id="6f55" class="ni nj it nd b gy no nl l nm nn">plt.plot(X, lr_solve.predict(X), color='orange')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/334f2ec729dc371efbf5f86ec062e92e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dsKKFWPTLCVxrFaCRqj3Ew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7f95" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上述回归模型的均方根误差为:</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="7c6c" class="ni nj it nd b gy nk nl l nm nn">lr_solve.rmse(X, y)<br/># 35.59874949855057</span></pre><p id="f179" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们也使用<code class="fe na nb nc nd b">method = ‘sgd’</code>，我们将让其他参数具有它们的默认值:</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="7187" class="ni nj it nd b gy nk nl l nm nn">lr_sgd = LinearRegression().fit(X, y, method='sgd')</span><span id="95b8" class="ni nj it nd b gy no nl l nm nn">plt.scatter(X, y)</span><span id="5631" class="ni nj it nd b gy no nl l nm nn">plt.plot(X, lr_sgd.predict(X), color='orange')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/930b509ec2f312f04c1cc7fe4be730b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_WjNHNoXiTRJy4avkAJoVA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="66f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如你所见，上面两幅图中方法“solve”和“sgd”的回归线几乎相同。</p><p id="1809" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用“sgd”时，我们得到的均方根误差为:</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="f9c9" class="ni nj it nd b gy nk nl l nm nn">lr_sgd.rmse(X, y)<br/># 36.34038690848635</span></pre><p id="d071" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是 Jupyter 笔记本，包含所有代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="c0f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望这些信息对您有用，感谢您的阅读！</p><p id="6a59" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章也贴在我自己的网站<a class="ae pg" href="https://www.nablasquared.com/how-to-implement-linear-regression-with-numpy/" rel="noopener ugc nofollow" target="_blank">这里</a>。随便看看吧！</p></div></div>    
</body>
</html>