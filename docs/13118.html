<html>
<head>
<title>K-Means Clustering: How It Works &amp; Finding The Optimum Number Of Clusters In The Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-Means 聚类:工作原理&amp;在数据中寻找最优的聚类数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-how-it-works-finding-the-optimum-number-of-clusters-in-the-data-13d18739255c?source=collection_archive---------8-----------------------#2020-09-09">https://towardsdatascience.com/k-means-clustering-how-it-works-finding-the-optimum-number-of-clusters-in-the-data-13d18739255c?source=collection_archive---------8-----------------------#2020-09-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ef2d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">数学公式，寻找最佳聚类数和 Python 中的工作示例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cea6dbd3066c87f2a98aedd5c8565ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lh-a75nwg8gi_aMGcaQkSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创造的形象</p></figure></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="cee3" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">介绍</h1><blockquote class="lx"><p id="d56b" class="ly lz it bd ma mb mc md me mf mg mh dk translated">K-means 是应用最广泛的无监督聚类方法之一。</p></blockquote><p id="8087" class="pw-post-body-paragraph mi mj it mk b ml mm ju mn mo mp jx mq mr ms mt mu mv mw mx my mz na nb nc mh im bi translated"><strong class="mk iu"> K-means </strong>算法通过尝试将样本分成等方差的<strong class="mk iu"> K </strong>组来对手头的数据进行聚类，从而最小化被称为<strong class="mk iu"> <em class="nd">惯性</em> </strong>或<strong class="mk iu">组内平方和</strong>的标准。<strong class="mk iu">该算法要求指定聚类数</strong>。它适用于大量样品，并已在许多不同领域的大范围应用中使用。</p><p id="c33e" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">k-means 算法将一组<strong class="mk iu"> N </strong>个样本(存储在数据矩阵<strong class="mk iu"> X </strong>中)分成<strong class="mk iu"> K </strong>个不相交的聚类<strong class="mk iu"> C </strong>，每个聚类由聚类中样本的均值<strong class="mk iu"> <em class="nd"> μj </em> </strong> <em class="nd"> </em>来描述。这个星团通常被称为“T28 质心”。</p><p id="d9f6" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated"><strong class="mk iu"> K-means </strong>算法属于<strong class="mk iu">无监督</strong>T34】机器 T36】学习算法/方法的家族。对于这一系列模型，研究需要手头有一个带有一些观察值的数据集<strong class="mk iu">，而不需要观察值的<strong class="mk iu">标签</strong> / <strong class="mk iu">类别</strong>。无监督学习研究系统如何从未标记的数据中推断出描述隐藏结构的函数。</strong></p><p id="6b66" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">现在让我们来发现算法的数学基础。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="0751" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">数学公式</h1><p id="d2b8" class="pw-post-body-paragraph mi mj it mk b ml nj ju mn mo nk jx mq mr nl mt mu mv nm mx my mz nn nb nc mh im bi translated">给定一组观察值(<strong class="mk iu"> x </strong> 1、<strong class="mk iu"> x </strong> 2、…、<strong class="mk iu"> x </strong> <em class="nd"> n </em>)，其中每个观察值是一个<em class="nd">d</em>-维实向量，<em class="nd">k</em>-意味着聚类旨在将<em class="nd"> n </em>个观察值划分为<em class="nd"> k </em> (≤ <em class="nd"> n </em>)个集合<strong class="mk iu">形式上，目标定义如下:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/d5cf771fd13e0b65aeea3b0be201129a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Fo_dM0Pt0c_DBi39CENog.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创作的乳胶图像。</p></figure><p id="d966" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">其中<strong class="mk iu"><em class="nd">μ</em></strong>I 是组/簇<em class="nd"> Si </em>中的点的平均值。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="37e2" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">如果你想在交互式路线图和活跃的学习社区的支持下自学数据科学，看看这个资源:<a class="ae no" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="20f0" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">理解背后的算法</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/4aa93fe2c4722c912f02e69600e8d231.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HAm9e3z7S9LGx0VVhsNq5w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">部分 Voronoi 图示例(来源:<a class="ae no" href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/cluster/plot _ k means _ digits . html # sphx-glr-auto-examples-cluster-plot-k means-digits-py</a>)。</p></figure><p id="8c8b" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">该算法也可以通过 Voronoi 图的概念来理解。</p><p id="7052" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">首先，使用当前质心计算点的 Voronoi 图。最初，质心是随机选择的<em class="nd">通常是</em>，但这取决于底层使用的包/库/软件。</p><p id="4429" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">Voronoi 图中的每个线段成为一个单独的簇。</p><p id="7b64" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">其次，质心被更新到每个线段的平均值。然后，该算法重复这一过程，直到满足停止标准。</p><p id="95b3" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">当迭代之间<strong class="mk iu">目标</strong> <strong class="mk iu">函数</strong>中的相对<strong class="mk iu">减少</strong>小于给定的<strong class="mk iu">容差</strong> (ε)值<strong class="mk iu">或<strong class="mk iu">质心</strong> <strong class="mk iu">移动</strong>(在空间中)<strong class="mk iu">小于<strong class="mk iu">容差</strong>时</strong>算法<strong class="mk iu">停止</strong>。</strong></p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="54bf" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">寻找最佳聚类数</h1><p id="a64f" class="pw-post-body-paragraph mi mj it mk b ml nj ju mn mo nk jx mq mr nl mt mu mv nm mx my mz nn nb nc mh im bi translated">如介绍中所述，<strong class="mk iu">K-Means 算法需要预先指定聚类的数量。如果没有关于数据中潜在聚类数的有效假设，这项任务可能很难完成。</strong></p><p id="0415" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">幸运的是，有一些方法可以估计我们数据中的最佳聚类数，如<strong class="mk iu">轮廓系数或肘方法。</strong>如果地面真实标签未知，必须使用模型本身进行评估。在本文中，我们将只使用<strong class="mk iu">轮廓系数</strong>，而不是简单得多的肘法。简而言之，<a class="ae no" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank">肘方法</a>将方差百分比解释为聚类数的函数:应该选择多个聚类，以便添加另一个聚类不会提供更好的数据建模。</p><p id="99a7" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated"><strong class="mk iu">较高的轮廓系数分数与具有更好定义的聚类的模型相关</strong>。轮廓系数是为每个样本定义的，由两个分数组成:</p><ul class=""><li id="f932" class="nr ns it mk b ml ne mo nf mr nt mv nu mz nv mh nw nx ny nz bi translated"><strong class="mk iu"> a </strong>:一个样本到同一类中所有其他点的平均距离。</li><li id="1897" class="nr ns it mk b ml oa mo ob mr oc mv od mz oe mh nw nx ny nz bi translated"><strong class="mk iu"> b </strong>:样本与下一个最近簇<em class="nd">中所有其他点之间的平均距离</em>。</li></ul><p id="998c" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">单个样本的轮廓系数<strong class="mk iu"> <em class="nd"> s </em> </strong> <strong class="mk iu">定义为:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/7d684f1f618b60ada237faef6d62d3d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oDDBGQuKAIIMdWBsc7NVrg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创作的乳胶图像。</p></figure><p id="ae1f" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated"><strong class="mk iu">最后，一组<em class="nd">样本的总轮廓系数</em>作为每个样本轮廓系数的平均值给出。</strong></p><blockquote class="lx"><p id="82da" class="ly lz it bd ma mb mc md me mf mg mh dk translated">最好的值是 1，最差的值是-1。接近 0 的值表示重叠的簇。负值通常表示样本被分配到错误的聚类，因为不同的聚类更相似。</p></blockquote></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="c41e" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">Python 工作示例</h1><p id="9e55" class="pw-post-body-paragraph mi mj it mk b ml nj ju mn mo nk jx mq mr nl mt mu mv nm mx my mz nn nb nc mh im bi translated">对于本例，我们将创建人工数据，即人工聚类。这样<strong class="mk iu">我们将提前</strong>知道<strong class="mk iu">地</strong> <strong class="mk iu">到</strong>即<strong class="mk iu">我们数据集</strong>中簇的确切数目。</p><p id="6794" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">让我们从导入所需的 python 库开始:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="3baa" class="ol lg it oh b gy om on l oo op">from sklearn.datasets import <a class="ae no" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs" rel="noopener ugc nofollow" target="_blank">make_blobs</a><br/>from sklearn.cluster import <a class="ae no" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" rel="noopener ugc nofollow" target="_blank">KMeans</a><br/>from sklearn.metrics import <a class="ae no" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html#sklearn.metrics.silhouette_samples" rel="noopener ugc nofollow" target="_blank">silhouette_samples</a>, <a class="ae no" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score" rel="noopener ugc nofollow" target="_blank">silhouette_score</a><br/><br/>import matplotlib.pyplot as plt<br/>import matplotlib.cm as cm<br/>import numpy as np</span></pre><p id="2ee1" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">接下来，让我们创建一些包含<strong class="mk iu"> 500 个样本</strong>、<strong class="mk iu"> 2 个特征/变量</strong>和<strong class="mk iu"> K=4 个聚类</strong>的人工数据。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="d2af" class="ol lg it oh b gy om on l oo op"># Generating the data<br/># This particular setting has one distinct cluster and 3 clusters placed close together.<br/>X, y = <a class="ae no" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs" rel="noopener ugc nofollow" target="_blank">make_blobs</a>(n_samples=500,<br/>                  n_features=2,<br/>                  centers=4,<br/>                  cluster_std=1,<br/>                  center_box=(-10.0, 10.0),<br/>                  shuffle=True,<br/>                  random_state=1)</span></pre><p id="19f5" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">我们知道数据中有 K=4 个聚类，但是，为了了解剪影评分如何工作<strong class="mk iu">，我们将使用一系列不同数量的聚类来拟合模型。</strong></p><p id="4b8a" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">每一次，我们将估计<strong class="mk iu">轮廓分数</strong>，并且也用最终(<strong class="mk iu">收敛</strong> ) <strong class="mk iu">质心</strong>绘制数据。所有这些都是由下面的代码完成的:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="d5e9" class="ol lg it oh b gy om on l oo op">range_n_clusters = [3, 4, 5]</span><span id="9af9" class="ol lg it oh b gy oq on l oo op">for n_clusters in range_n_clusters:<br/>    # Create a subplot with 1 row and 2 columns<br/>    fig, (ax1, ax2) = <a class="ae no" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" rel="noopener ugc nofollow" target="_blank">plt.subplots</a>(1, 2)<br/>    fig.set_size_inches(18, 7)<br/><br/>    # The 1st subplot is the silhouette plot<br/>    # The silhouette coefficient can range from -1, 1 but in this example all<br/>    # lie within [-0.1, 1]<br/>    ax1.set_xlim([-0.1, 1])<br/>    # The (n_clusters+1)*10 is for inserting blank space between silhouette<br/>    # plots of individual clusters, to demarcate them clearly.<br/>    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])<br/><br/>    # Initialize the clusterer with n_clusters value and a random generator<br/>    # seed of 10 for reproducibility.<br/>    clusterer = <a class="ae no" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" rel="noopener ugc nofollow" target="_blank">KMeans</a>(n_clusters=n_clusters, random_state=10)<br/>    cluster_labels = clusterer.fit_predict(X)<br/><br/>    # The silhouette_score gives the average value for all the samples.<br/>    # This gives a perspective into the density and separation of the formed<br/>    # clusters<br/>    silhouette_avg = <a class="ae no" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score" rel="noopener ugc nofollow" target="_blank">silhouette_score</a>(X, cluster_labels)<br/>    print("For n_clusters =", n_clusters,<br/>          "The average silhouette_score is :", silhouette_avg)<br/><br/>    # Compute the silhouette scores for each sample<br/>    sample_silhouette_values = <a class="ae no" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html#sklearn.metrics.silhouette_samples" rel="noopener ugc nofollow" target="_blank">silhouette_samples</a>(X, cluster_labels)<br/><br/>    y_lower = 10<br/>    for i in range(n_clusters):<br/>        # Aggregate the silhouette scores for samples belonging to<br/>        # cluster i, and sort them<br/>        ith_cluster_silhouette_values = \<br/>            sample_silhouette_values[cluster_labels == i]<br/><br/>        ith_cluster_silhouette_values.sort()<br/><br/>        size_cluster_i = ith_cluster_silhouette_values.shape[0]<br/>        y_upper = y_lower + size_cluster_i<br/><br/>        color = cm.nipy_spectral(float(i) / n_clusters)<br/>        ax1.fill_betweenx(<a class="ae no" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" rel="noopener ugc nofollow" target="_blank">np.arange</a>(y_lower, y_upper),<br/>                          0, ith_cluster_silhouette_values,<br/>                          facecolor=color, edgecolor=color, alpha=0.7)<br/><br/>        # Label the silhouette plots with their cluster numbers at the middle<br/>        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))<br/><br/>        # Compute the new y_lower for next plot<br/>        y_lower = y_upper + 10  # 10 for the 0 samples<br/><br/>    ax1.set_title("The silhouette plot for the various clusters.")<br/>    ax1.set_xlabel("The silhouette coefficient values")<br/>    ax1.set_ylabel("Cluster label")<br/><br/>    # The vertical line for average silhouette score of all the values<br/>    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")<br/><br/>    ax1.set_yticks([])  # Clear the yaxis labels / ticks<br/>    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])<br/><br/>    # 2nd Plot showing the actual clusters formed<br/>    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)<br/>    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,<br/>                c=colors, edgecolor='k')<br/><br/>    # Labeling the clusters<br/>    centers = clusterer.cluster_centers_<br/>    # Draw white circles at cluster centers<br/>    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',<br/>                c="white", alpha=1, s=200, edgecolor='k')<br/><br/>    for i, c in enumerate(centers):<br/>        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,<br/>                    s=50, edgecolor='k')<br/><br/>    ax2.set_title("The visualization of the clustered data.")<br/>    ax2.set_xlabel("Feature space for the 1st feature")<br/>    ax2.set_ylabel("Feature space for the 2nd feature")<br/><br/>    <a class="ae no" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.suptitle.html#matplotlib.pyplot.suptitle" rel="noopener ugc nofollow" target="_blank">plt.suptitle</a>(("Silhouette analysis for KMeans clustering on sample data "<br/>                  "with n_clusters = %d" % n_clusters),<br/>                 fontsize=14, fontweight='bold')<br/><br/><a class="ae no" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" rel="noopener ugc nofollow" target="_blank">plt.show</a>()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/1bd242d02eb3d6bfea3efffcf5ed2463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0i3uGdxPhweTnuJWiZ8LSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">控制台中的代码输出(图片由作者创建)</p></figure><blockquote class="lx"><p id="428c" class="ly lz it bd ma mb os ot ou ov ow mh dk translated">我们观察到，在 K=4 个聚类的情况下，平均/均值<strong class="ak">轮廓得分最高。</strong></p></blockquote><p id="38bc" class="pw-post-body-paragraph mi mj it mk b ml mm ju mn mo mp jx mq mr ms mt mu mv mw mx my mz na nb nc mh im bi translated"><strong class="mk iu">这验证了轮廓分数是 K-均值拟合优度的良好度量。</strong></p><p id="76a1" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">我们还制作了这些图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cea6dbd3066c87f2a98aedd5c8565ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lh-a75nwg8gi_aMGcaQkSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">剪影分数&amp;最终(<strong class="bd ox">收敛</strong> ) <strong class="bd ox">质心</strong>的数据(图片由作者创建)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/6216eaab829c312db69e175d98ffb59a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WqFNuHkvtk-3g6ilYovuKQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">剪影分数&amp;最终(<strong class="bd ox">收敛</strong> ) <strong class="bd ox">质心</strong>的数据(图片由作者创建)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/9974c12b7dd9300bbc358f9d57cc2040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cAbr_kZBlRJ8vUb0kvs78Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">剪影分数&amp;最终(<strong class="bd ox">收敛</strong> ) <strong class="bd ox">质心</strong>的数据(图片由作者创建)</p></figure><p id="cad0" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated"><strong class="mk iu">垂直线是所有值的平均轮廓得分。</strong></p><p id="258c" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated"><strong class="mk iu">同样，我们还可以直观地验证轮廓得分是特定示例的 K 均值拟合优度的良好度量。</strong></p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="fc21" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated"><strong class="ak">结论</strong></h1><p id="78ae" class="pw-post-body-paragraph mi mj it mk b ml nj ju mn mo nk jx mq mr nl mt mu mv nm mx my mz nn nb nc mh im bi translated"><strong class="mk iu"> K-means </strong>是应用最广泛的无监督聚类方法之一。该算法通过尝试将样本分成等方差的<strong class="mk iu"> K </strong>组来对手头的数据进行聚类，从而最小化被称为惯性或类内平方和的标准。该算法要求指定聚类数。</p><p id="1c5e" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">但是，如果地面实况标签未知，则必须使用模型本身进行评估。</p><p id="913f" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">在本文中，我们使用了<strong class="mk iu">轮廓系数。从工作示例中，我们可以验证轮廓分数是 K-means 拟合优度的良好度量。</strong></p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="2c6a" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">仅此而已！希望您喜欢这篇新文章——更多内容即将发布！</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="0171" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">参考</h1><p id="998f" class="pw-post-body-paragraph mi mj it mk b ml nj ju mn mo nk jx mq mr nl mt mu mv nm mx my mz nn nb nc mh im bi translated">[1]彼得·j·鲁瑟夫(1987 年)。“轮廓:聚类分析的解释和验证的图形辅助”。计算与应用数学 20:53–65。<a class="ae no" href="https://doi.org/10.1016/0377-0427(87)90125-7" rel="noopener ugc nofollow" target="_blank">doi:10.1016/0377–0427(87)90125–7</a>。</p><p id="f2b5" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">[2]<a class="ae no" href="https://en.wikipedia.org/wiki/Silhouette_(clustering)" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Silhouette _(集群)</a></p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h2 id="eaab" class="ol lg it bd lh pa pb dn ll pc pd dp lp mr pe pf lr mv pg ph lt mz pi pj lv pk bi translated">您可能还喜欢:</h2><div class="pl pm gp gr pn po"><a rel="noopener follow" target="_blank" href="/lstm-time-series-forecasting-predicting-stock-prices-using-an-lstm-model-6223e9644a2f"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">LSTM 时间序列预测:使用 LSTM 模型预测股票价格</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">在这篇文章中，我将向你展示如何使用预测 LSTM 模型来预测股票价格</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc ks po"/></div></div></a></div><div class="pl pm gp gr pn po"><a rel="noopener follow" target="_blank" href="/time-series-forecasting-predicting-stock-prices-using-an-arima-model-2e3b3080bd70"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">时间序列预测:使用 ARIMA 模型预测股票价格</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">在这篇文章中，我将向你展示如何使用预测 ARIMA 模型来预测特斯拉的股票价格</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qd l pz qa qb px qc ks po"/></div></div></a></div></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="a96f" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">敬请关注并支持这一努力</h1><p id="db8b" class="pw-post-body-paragraph mi mj it mk b ml nj ju mn mo nk jx mq mr nl mt mu mv nm mx my mz nn nb nc mh im bi translated">如果你喜欢这篇文章并觉得它有用，请关注我，这样你就可以看到我所有的新帖子。</p><p id="032c" class="pw-post-body-paragraph mi mj it mk b ml ne ju mn mo nf jx mq mr ng mt mu mv nh mx my mz ni nb nc mh im bi translated">有问题吗？把它们作为评论贴出来，我会尽快回复。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="98d4" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">最新帖子</h1><div class="pl pm gp gr pn po"><a href="https://medium.com/@seralouk/the-best-free-data-science-resources-free-books-online-courses-9c4a2df194e5" rel="noopener follow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">最佳免费数据科学资源:免费书籍和在线课程</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">最有用的免费书籍和在线课程，适合想了解更多数据科学知识的人。</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">medium.com</p></div></div><div class="px l"><div class="qe l pz qa qb px qc ks po"/></div></div></a></div><div class="pl pm gp gr pn po"><a rel="noopener follow" target="_blank" href="/roc-curve-explained-using-a-covid-19-hypothetical-example-binary-multi-class-classification-bab188ea869c"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">用新冠肺炎假设的例子解释 ROC 曲线:二分类和多分类…</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">在这篇文章中，我清楚地解释了什么是 ROC 曲线以及如何阅读它。我用一个新冠肺炎的例子来说明我的观点，我…</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qf l pz qa qb px qc ks po"/></div></div></a></div><div class="pl pm gp gr pn po"><a rel="noopener follow" target="_blank" href="/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">支持向量机(SVM)解释清楚:分类问题的 python 教程…</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">在这篇文章中，我解释了支持向量机的核心，为什么以及如何使用它们。此外，我还展示了如何绘制支持…</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qg l pz qa qb px qc ks po"/></div></div></a></div><div class="pl pm gp gr pn po"><a rel="noopener follow" target="_blank" href="/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">PCA 清楚地解释了——如何、何时、为什么使用它以及特性的重要性:Python 指南</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">在这篇文章中，我解释了什么是 PCA，何时以及为什么使用它，以及如何使用 scikit-learn 在 Python 中实现它。还有…</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qh l pz qa qb px qc ks po"/></div></div></a></div><div class="pl pm gp gr pn po"><a rel="noopener follow" target="_blank" href="/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">关于 Python 中的最小-最大规范化，您需要知道的一切</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">在这篇文章中，我将解释什么是最小-最大缩放，什么时候使用它，以及如何使用 scikit 在 Python 中实现它</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qi l pz qa qb px qc ks po"/></div></div></a></div><div class="pl pm gp gr pn po"><a rel="noopener follow" target="_blank" href="/how-and-why-to-standardize-your-data-996926c2c832"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">Scikit-Learn 的标准定标器如何工作</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">在这篇文章中，我将解释为什么以及如何使用 scikit-learn 应用标准化</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qj l pz qa qb px qc ks po"/></div></div></a></div></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="7cae" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">和我联系</h1><ul class=""><li id="45ab" class="nr ns it mk b ml nj mo nk mr qk mv ql mz qm mh nw nx ny nz bi translated"><strong class="mk iu">LinkedIn</strong>:【https://www.linkedin.com/in/serafeim-loukas/ T2】</li><li id="5962" class="nr ns it mk b ml oa mo ob mr oc mv od mz oe mh nw nx ny nz bi translated">https://www.researchgate.net/profile/Serafeim_Loukas<strong class="mk iu">研究之门</strong>:<a class="ae no" href="https://www.researchgate.net/profile/Serafeim_Loukas" rel="noopener ugc nofollow" target="_blank">T7】</a></li></ul></div></div>    
</body>
</html>