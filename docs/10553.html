<html>
<head>
<title>Word embeddings in 2020. Review with code examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2020 年的单词嵌入。查看代码示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d?source=collection_archive---------9-----------------------#2020-07-24">https://towardsdatascience.com/word-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d?source=collection_archive---------9-----------------------#2020-07-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7d2f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当前单词嵌入方法概述:从 Word2vec 到 Transformers</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ae105f3cd2fc10123f64f3516d6e17df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Kr-mgVDevo63WPYm37gBA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由 Rostyslav Neskorozhenyi 使用<a class="ae ky" href="https://github.com/amueller/word_cloud" rel="noopener ugc nofollow" target="_blank"> WordCloud </a>模块创建</p></figure><p id="615d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将研究单词嵌入——适合机器学习算法处理的单词的数字表示。</p><p id="6efb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最初，我写这篇文章是为了对 2020 年单词嵌入的当前方法进行概述和汇编，我们的<a class="ae ky" href="http://ai-labs.org/" rel="noopener ugc nofollow" target="_blank">人工智能实验室</a>团队可以不时地使用它作为快速复习。我希望我的文章对更广泛的数据科学家和开发人员有用。文章中的每个单词嵌入方法都有(非常)简短的描述、供进一步研究的链接和 Python 代码示例。所有代码被打包成<a class="ae ky" href="https://colab.research.google.com/drive/1N7HELWImK9xCYheyozVP3C_McbiRo1nb" rel="noopener ugc nofollow" target="_blank">谷歌 Colab 笔记本</a>。让我们开始吧。</p><p id="6ea8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据维基百科，<strong class="lb iu">单词嵌入</strong>是自然语言处理(NLP)中一组语言建模和特征学习技术的统称，其中来自词汇表的单词或短语被映射到实数的向量。</p><h1 id="d7fd" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">一键或计数向量化</h1><p id="6a14" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">将单词转换成向量的最基本方法是统计每个单词在每个文档中的出现次数。这种方法被称为计数矢量化或一键编码。</p><p id="3dc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法的主要原理是收集一组文档(可以是单词、句子、段落甚至文章)，统计每个文档中每个单词的出现次数。严格来说，得到的矩阵的列是单词，行是文档。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="0c50" class="mx lw it mt b gy my mz l na nb">from sklearn.feature_extraction.text import CountVectorizer<br/># create CountVectorizer object<br/>vectorizer = CountVectorizer()<br/>corpus = [<br/>          'Text of the very first new sentence with the first words in sentence.',<br/>          'Text of the second sentence.',<br/>          'Number three with lot of words words words.',<br/>          'Short text, less words.',<br/>]</span><span id="906f" class="mx lw it mt b gy nc mz l na nb"># learn the vocabulary and store CountVectorizer sparse matrix in term_frequencies<br/>term_frequencies = vectorizer.fit_transform(corpus) <br/>vocab = vectorizer.get_feature_names()</span><span id="3e04" class="mx lw it mt b gy nc mz l na nb"># convert sparse matrix to numpy array<br/>term_frequencies = term_frequencies.toarray()</span><span id="a545" class="mx lw it mt b gy nc mz l na nb"># visualize term frequencies <br/>import seaborn as sns<br/>sns.heatmap(term_frequencies, annot=True, cbar = False, xticklabels = vocab);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/1daa8083c3e2b4141ab59b0b02ea0d1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*uthDlQaXkxocKjEbdKY8Sg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像由 Rostyslav Neskorozhenyi 使用<a class="ae ky" href="https://seaborn.pydata.org/" rel="noopener ugc nofollow" target="_blank"> seaborn </a>模块创建</p></figure><p id="e235" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">countvectorizing 中的另一种方法是，如果在文档中找到该单词，则只放置 1(不管出现的频率如何)，如果在文档中没有找到该单词，则放置 0。在这种情况下，我们得到真正的“一键”编码。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="aa39" class="mx lw it mt b gy my mz l na nb">one_hot_vectorizer = CountVectorizer(binary=True)<br/>one_hot = one_hot_vectorizer.fit_transform(corpus).toarray()</span><span id="76f2" class="mx lw it mt b gy nc mz l na nb">sns.heatmap(one_hot, annot=True, cbar = False, xticklabels = vocab)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/17539b46cf5e7e073e954ed493d58ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*x3tJKM3ssG9NUEg_plox3Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像由 Rostyslav Neskorozhenyi 使用<a class="ae ky" href="https://seaborn.pydata.org/" rel="noopener ugc nofollow" target="_blank"> seaborn </a>模块创建</p></figure><h1 id="ddd4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">TF-IDF 编码</h1><p id="278d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">对于大语料库的文档，一些词如‘a’、‘the’、‘is’等。非常频繁地出现，但是它们没有携带很多信息。使用一键编码方法，我们可以确定这些单词很重要，因为它们出现在许多文档中。解决这个问题的方法之一是停用词过滤，但是这种解决方案是离散的并且不灵活。</p><p id="3566" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TF-IDF(词频—逆文档频)可以较好的处理这个问题。TF-IDF 降低了常用词的权重，提高了只出现在当前文档中的罕见词的权重。TF-IDF 公式如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/aca6ab5113a28e47b823d58ed9edf52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*tMTbOwNzKx06chETJkZ6ww.png"/></div></figure><p id="731a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中 TF 是通过将单词在文档中出现的次数除以文档中的单词总数来计算的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/3ea7d2694c7e9a8b830b7081089b356b.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*y2mxpp2Vl6W7_ZmdUISkXA.png"/></div></figure><p id="f51e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">IDF(逆文档频率)，解释为文档的逆数量，其中出现了我们感兴趣的术语。n-文档数量，n(t)-具有当前单词或术语 t 的文档数量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/bc0f3b7ade81fcfc8b762287a6e9ce39.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*6PP8jjIoi7Oe3O1Wo4a2vQ.png"/></div></div></figure><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="097c" class="mx lw it mt b gy my mz l na nb">from sklearn.feature_extraction.text import TfidfVectorizer<br/>import seaborn as sns</span><span id="ad14" class="mx lw it mt b gy nc mz l na nb">corpus = [<br/>          'Time flies like an arrow.',<br/>          'Fruit flies like a banana.'<br/>]</span><span id="43f7" class="mx lw it mt b gy nc mz l na nb">vocab = ['an', 'arrow', 'banana', 'flies', 'fruit', 'like', 'time']</span><span id="17c5" class="mx lw it mt b gy nc mz l na nb">tfidf_vectorizer = TfidfVectorizer()<br/>tfidf = tfidf_vectorizer.fit_transform(corpus).toarray()</span><span id="492e" class="mx lw it mt b gy nc mz l na nb">sns.heatmap(tfidf, annot=True, cbar = False, xticklabels = vocab)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/ccea2b74b71e8464e210334fe53d57ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*xz3BO60PTBFMGqGftpu5fA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像由 Rostyslav Neskorozhenyi 使用<a class="ae ky" href="https://seaborn.pydata.org/" rel="noopener ugc nofollow" target="_blank"> seaborn </a>模块创建</p></figure><h1 id="5b74" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Word2Vec 和手套</h1><p id="8d43" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">最常用的单词嵌入模型是<a class="ae ky" href="https://github.com/dav/word2vec/" rel="noopener ugc nofollow" target="_blank"> word2vec </a>和<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>，它们都是基于分布假设的无监督方法(出现在相同上下文中的单词往往具有相似的含义)。</p><p id="b433" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Word2Vec 单词嵌入是单词的向量表示，当输入大量文本(例如维基百科、科学、新闻、文章等)时，通常由无监督模型学习。).单词的这些表示在其他属性中捕捉单词之间的语义相似性。Word2Vec 单词嵌入是以这样的方式学习的，意思相近的单词(例如“king”和“queen”)的向量之间的<a class="ae ky" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">距离</a>比意思完全不同的单词(例如“king”和“carpet”)的距离更近。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/c85a75a24e16406b3aeba918ff83617a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lzjgo2KaWFRPkV3LCJDr7Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space" rel="noopener ugc nofollow" target="_blank">developers.google.com</a></p></figure><p id="544a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Word2Vec 矢量甚至允许对矢量进行一些数学运算。例如，在这个操作中，我们对每个单词使用 word2vec 向量:</p><p id="e0e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">国王——男人+女人=王后</strong></p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a0fe" class="mx lw it mt b gy my mz l na nb"># Download Google Word2Vec embeddings <a class="ae ky" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank">https://code.google.com/archive/p/word2vec/</a></span><span id="e655" class="mx lw it mt b gy nc mz l na nb">!wget <a class="ae ky" href="https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz" rel="noopener ugc nofollow" target="_blank">https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz</a><br/>!gunzip GoogleNews-vectors-negative300.bin</span><span id="0589" class="mx lw it mt b gy nc mz l na nb"># Try Word2Vec with Gensim</span><span id="fe49" class="mx lw it mt b gy nc mz l na nb">import gensim</span><span id="01b9" class="mx lw it mt b gy nc mz l na nb"># Load pretrained vectors from Google<br/>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)</span><span id="2772" class="mx lw it mt b gy nc mz l na nb">king = model['king']</span><span id="2e7e" class="mx lw it mt b gy nc mz l na nb"># king - man + woman = queen<br/>print(model.most_similar(positive=['woman', 'king'], negative=['man']))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/4cf69d100e0532f1bf4a38857ddafa00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JXGyzf1eXCpHeB3Zh5qZ7w.png"/></div></div></figure><p id="efee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">检查单词“女人”和“男人”的向量有多相似。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9b02" class="mx lw it mt b gy my mz l na nb">print(model.similarity('woman', 'man'))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/6129e46a21944c21f63e31f17f12a0b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:230/format:webp/1*WXDuF9JkFif2j8WK-GdLSA.png"/></div></figure><p id="d383" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">检查单词“king”和“woman”的向量有多相似。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="4b86" class="mx lw it mt b gy my mz l na nb">print(model.similarity('king', 'woman'))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/4f9fcdedb15370d1d592e9f9f22072ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:228/format:webp/1*oBq_TFjHv3PttrWERuoYdw.png"/></div></figure><p id="8380" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种单词嵌入方法是<strong class="lb iu">手套</strong>(“全局向量”)。它基于单词上下文矩阵的矩阵分解技术。它首先构建一个(单词 x 上下文)共现信息的大矩阵，即对于每个“单词”(行)，计算我们在大型语料库的某些“上下文”(列)中看到该单词的频率。然后，这个矩阵被分解成一个低维(单词 x 特征)矩阵，其中每一行现在存储每个单词的向量表示。一般来说，这是通过最小化“重建损失”来实现的。这种损失试图找到可以解释高维数据中大部分差异的低维表示。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="240f" class="mx lw it mt b gy my mz l na nb"># Try Glove word embeddings with Spacy</span><span id="1abe" class="mx lw it mt b gy nc mz l na nb">!python3 -m spacy download en_core_web_lg</span><span id="987e" class="mx lw it mt b gy nc mz l na nb">import spacy<br/># Load the spacy model that you have installed<br/>import en_core_web_lg<br/>nlp = en_core_web_lg.load()<br/># process a sentence using the model<br/>doc = nlp("man king stands on the carpet and sees woman queen")</span></pre><p id="452b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查找国王和王后之间的相似性(值越高越好)。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="3d15" class="mx lw it mt b gy my mz l na nb">doc[1].similarity(doc[9])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/9ce78a1357cd5f86ff7077218a6e6953.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*YooWc7ckFnm_7lzR-IEAog.png"/></div></figure><p id="82ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">找出国王和地毯的相似之处。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="b3a9" class="mx lw it mt b gy my mz l na nb">doc[1].similarity(doc[5])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/21e5f943eb2fbef252137924e3f05fc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/1*EdJVh10EafQ_EWA4WbphrA.png"/></div></figure><p id="dd44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">检查是否国王-男人+女人=王后。我们将把“男人”和“女人”的向量乘以 2，因为减去“男人”的向量并加上“女人”的向量对“国王”的原始向量没有什么影响，可能是因为那些“男人”和“女人”本身是相关的。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="6d85" class="mx lw it mt b gy my mz l na nb">v =  doc[1].vector - (doc[0].vector*2) + (doc[8].vector*2)</span><span id="3c50" class="mx lw it mt b gy nc mz l na nb">from scipy.spatial import distance<br/>import numpy as np</span><span id="728a" class="mx lw it mt b gy nc mz l na nb"># Format the vocabulary for use in the distance function<br/>vectors = [token.vector for token in doc]<br/>vectors = np.array(vectors)</span><span id="50f6" class="mx lw it mt b gy nc mz l na nb"># Find the closest word below <br/>closest_index = distance.cdist(np.expand_dims(v, axis = 0), vectors, metric = 'cosine').argmin()<br/>output_word = doc[closest_index].text<br/>print(output_word)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/3136030139b634241864c808c1f9e605.png" data-original-src="https://miro.medium.com/v2/resize:fit:162/format:webp/1*9_iGlCJZV6PkSDtH_2VyAg.png"/></div></figure><h1 id="d4ba" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">快速文本</h1><p id="8b9c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/facebookresearch/fastText" rel="noopener ugc nofollow" target="_blank"> FastText </a>是 word2vec 的扩展。FastText 是由 2013 年创建 word2vec 框架的 Tomas Mikolov 团队开发的。</p><p id="b718" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">FastText 相对于原始 word2vec 向量的主要改进是包含了字符<a class="ae ky" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> n-grams </a>，这允许计算没有出现在训练数据中的单词的单词表示(“词汇外”单词)。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="c905" class="mx lw it mt b gy my mz l na nb">!pip install Cython --install-option="--no-cython-compile"<br/>!pip install fasttext</span><span id="101a" class="mx lw it mt b gy nc mz l na nb"># download pre-trained language word vectors from one of 157 languges  <a class="ae ky" href="https://fasttext.cc/docs/en/crawl-vectors.html" rel="noopener ugc nofollow" target="_blank">https://fasttext.cc/docs/en/crawl-vectors.html</a><br/># it will take some time, about 5 minutes<br/>import fasttext<br/>import fasttext.util<br/>fasttext.util.download_model('en', if_exists='ignore')  # English<br/>ft = fasttext.load_model('cc.en.300.bin')</span></pre><p id="efa5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为单词“king”创建嵌入。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="4304" class="mx lw it mt b gy my mz l na nb">ft.get_word_vector('king')</span></pre><p id="b4be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">获取与单词“king”最相似的单词。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="b80f" class="mx lw it mt b gy my mz l na nb">ft.get_nearest_neighbors('king')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/3517fa81b8980161a3100d0124b9f216.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*hXByRootuiR-9igwUHxqAQ.png"/></div></div></figure><p id="0411" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测试模型为未知单词创建向量的能力。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="bd64" class="mx lw it mt b gy my mz l na nb">'king-warrior' in ft.words</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/9021d424852d3b65157e9f841c3fd5b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:138/format:webp/1*JU1cL8cEfhKf_XELsXC0Uw.png"/></div></figure><p id="f28c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">获取与未知单词“王者战士”最相似的单词。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="23f8" class="mx lw it mt b gy my mz l na nb">ft.get_nearest_neighbors('king-warrior')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/d7fde9852449b320b673632d5e3df796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_a5E3pvSFzSmmOX8Ub2OCw.png"/></div></div></figure><h1 id="5e11" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">ELMo(语言模型嵌入)</h1><p id="5367" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">与 word2vec 和 GLoVe 等传统单词嵌入不同，分配给令牌或单词的 ELMo 向量取决于当前上下文，实际上是包含该单词的整个句子的函数。所以同一个词在不同的语境下可以有不同的词向量。此外，ELMo 表示完全基于字符，因此它们不局限于任何预定义的词汇。</p><p id="7f0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来自官方网站的描述:</p><p id="37ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> ELMo </strong> </a>是一种深度语境化的单词表示，它模拟(1)单词使用的复杂特征(例如，句法和语义)，以及(2)这些使用如何在语言语境中变化(即，模拟多义性)。这些单词向量是深度双向语言模型(biLM)的内部状态的学习函数，该模型是在大型文本语料库上预先训练的。它们可以很容易地添加到现有的模型中，并在广泛的具有挑战性的自然语言处理问题中显著提高技术水平，包括问题回答、文本蕴涵和情感分析。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="2748" class="mx lw it mt b gy my mz l na nb"># use tensorflow 1.x for ELMo, because trere are still no ELMo for tensorflow 2.0</span><span id="fbf4" class="mx lw it mt b gy nc mz l na nb">%tensorflow_version 1.x</span><span id="5045" class="mx lw it mt b gy nc mz l na nb">import tensorflow_hub as hub<br/>import tensorflow as tf</span><span id="6a9d" class="mx lw it mt b gy nc mz l na nb"># Download pretrained ELMo model from Tensorflow Hub <a class="ae ky" href="https://tfhub.dev/google/elmo/3" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/elmo/3</a></span><span id="7630" class="mx lw it mt b gy nc mz l na nb">elmo = hub.Module("<a class="ae ky" href="https://tfhub.dev/google/elmo/3" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/elmo/3</a>", trainable=True)</span><span id="0ade" class="mx lw it mt b gy nc mz l na nb">sentences =  \<br/>['king arthur, also called arthur or aathur pendragon, legendary british king who appears in a cycle of \<br/>medieval romances (known as the matter of britain) as the sovereign of a knightly fellowship of the round table.', <br/>'it is not certain how these legends originated or whether the figure of arthur was based on a historical person.', <br/>'the legend possibly originated either in wales or in those parts of northern britain inhabited by brythonic-speaking celts.', <br/>'for a fuller treatment of the stories about king arthur, see also arthurian legend.']</span></pre><p id="4142" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了将句子发送到模型，我们需要将它们分成单词数组和填充数组，长度相同。此外，我们还将创建“mask”数组，该数组将显示元素是真实单词还是填充符号(在我们的示例中为“_”)。稍后我们将使用“mask”数组来显示真实的单词。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="19d7" class="mx lw it mt b gy my mz l na nb">words = []<br/>mask = []<br/>masked_words = []</span><span id="baf6" class="mx lw it mt b gy nc mz l na nb">for sent in sentences:<br/>  splitted = sent.split()<br/>  for i in range(36):<br/>    try:<br/>      words.append(splitted[i])<br/>    except:<br/>      words.append('_')</span><span id="ab36" class="mx lw it mt b gy nc mz l na nb">for word in words:<br/>  if word == "_":<br/>    mask.append(False)<br/>  else:<br/>    mask.append(True)<br/>    masked_words.append(word)</span></pre><p id="2819" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用 ELMo 创建嵌入:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="5745" class="mx lw it mt b gy my mz l na nb">embeddings = elmo(<br/>    sentences,<br/>    signature="default",<br/>    as_dict=True)["elmo"]</span></pre><p id="8214" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将 Tensorflow 张量转换为 numpy 数组。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a18a" class="mx lw it mt b gy my mz l na nb">%%time<br/>with tf.Session() as sess:<br/>  sess.run(tf.global_variables_initializer())<br/>  sess.run(tf.tables_initializer())<br/>  x = sess.run(embeddings)</span><span id="27cd" class="mx lw it mt b gy nc mz l na nb">embs = x.reshape(-1, 1024)</span><span id="6f0f" class="mx lw it mt b gy nc mz l na nb">masked_embs = embs[mask]</span></pre><p id="68a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> PCA </a>可视化嵌入。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a058" class="mx lw it mt b gy my mz l na nb">from sklearn.decomposition import PCA</span><span id="aea2" class="mx lw it mt b gy nc mz l na nb">pca = PCA(n_components=10)<br/>y = pca.fit_transform(masked_embs)</span><span id="c870" class="mx lw it mt b gy nc mz l na nb">from sklearn.manifold import TSNE</span><span id="a202" class="mx lw it mt b gy nc mz l na nb">y = TSNE(n_components=2).fit_transform(y)</span><span id="2f30" class="mx lw it mt b gy nc mz l na nb">import plotly as py<br/>import plotly.graph_objs as go</span><span id="8e68" class="mx lw it mt b gy nc mz l na nb">data = [<br/>    go.Scatter(<br/>        x=[i[0] for i in y],<br/>        y=[i[1] for i in y],<br/>        mode='markers',<br/>        text=[i for i in masked_words],<br/>    marker=dict(<br/>        size=16,<br/>        color = [len(i) for i in masked_words], #set color equal to a variable<br/>        opacity= 0.8,<br/>        colorscale='Viridis',<br/>        showscale=False<br/>    )<br/>    )<br/>]<br/>layout = go.Layout()<br/>layout = dict(<br/>              yaxis = dict(zeroline = False),<br/>              xaxis = dict(zeroline = False)<br/>             )<br/>fig = go.Figure(data=data, layout=layout)<br/>fig.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/4b1dbe1820a56bb2180b987ad3a88c8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*nDE05QGHA7Uils2HEG_5UQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像由 Rostyslav Neskorozhenyi 使用<a class="ae ky" href="https://plotly.com/" rel="noopener ugc nofollow" target="_blank"> plotly </a>模块创建</p></figure><h1 id="bec7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">变形金刚(电影名)</h1><p id="4e4e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">终于到了当前最先进的方法——变压器的时候了。著名的<a class="ae ky" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>、<a class="ae ky" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">伯特</a>、<a class="ae ky" href="https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/" rel="noopener ugc nofollow" target="_blank"> CTRL </a>都是基于变形金刚的，并且像 ELMo 一样产生上下文敏感的嵌入。但是与 ELMo 变形金刚不使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank"> RNN </a>不同，它们不需要一个接一个地处理句子中的单词。句子中的所有单词被并行处理，这种方法加快了处理速度并解决了<a class="ae ky" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失梯度问题</a>。</p><p id="d7ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">变形金刚使用<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力机制</a>来描述每个特定单词与句子中所有其他单词的联系和依赖性。杰伊·阿拉姆马在一篇插图精美的<a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">文章</a>中详细描述了这种机制和变压器的主要原理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/3b4f69744157f605791000be1c737c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lnxCxwWsz_uUnunF.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar . github . io</a></p></figure><p id="3e76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们的例子，我们将使用 brilliant <a class="ae ky" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> Transformers </a>库，其中包含最新的基于 Transformers 的模型(如<a class="ae ky" href="https://huggingface.co/transformers/model_doc/bert.html" rel="noopener ugc nofollow" target="_blank"> BERT </a>、<a class="ae ky" href="https://huggingface.co/transformers/model_doc/xlnet.html" rel="noopener ugc nofollow" target="_blank"> XLNet </a>、<a class="ae ky" href="https://huggingface.co/transformers/model_doc/dialogpt.html" rel="noopener ugc nofollow" target="_blank"> DialoGPT </a>或<a class="ae ky" href="https://huggingface.co/transformers/model_doc/gpt2.html" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>)。</p><p id="3f68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们和伯特做一些嵌入。首先，我们需要安装变形金刚库。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a00d" class="mx lw it mt b gy my mz l na nb">!pip install transformers</span></pre><p id="960a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们导入 pytorch，这是一个预训练的 BERT 模型，以及一个 BERT 标记器，它将完成将句子转换成适合 BERT 的格式的所有必要工作(标记自身并添加特殊标记，如[SEP]和[CLS])。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="72ee" class="mx lw it mt b gy my mz l na nb">import torch<br/>torch.manual_seed(0)<br/>from transformers import BertTokenizer, BertModel</span><span id="a9c3" class="mx lw it mt b gy nc mz l na nb">import logging<br/>import matplotlib.pyplot as plt<br/>% matplotlib inline</span><span id="3ce6" class="mx lw it mt b gy nc mz l na nb"># Load pre-trained model tokenizer (vocabulary)<br/>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)</span></pre><p id="f681" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入一些句子，将其标记出来。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="7647" class="mx lw it mt b gy my mz l na nb">sentences =  \<br/>['king arthur, also called arthur or aathur pendragon, legendary british king who appears in a cycle of \<br/>medieval romances (known as the matter of britain) as the sovereign of a knightly fellowship of the round table.', <br/>'it is not certain how these legends originated or whether the figure of arthur was based on a historical person.', <br/>'the legend possibly originated either in wales or in those parts of northern britain inhabited by brythonic-speaking celts.', <br/>'for a fuller treatment of the stories about king arthur, see also arthurian legend.']</span><span id="680b" class="mx lw it mt b gy nc mz l na nb"># Print the original sentence.<br/>print(' Original: ', sentences[0][:99])</span><span id="302c" class="mx lw it mt b gy nc mz l na nb"># Print the sentence splitted into tokens.<br/>print('Tokenized: ', tokenizer.tokenize(sentences[0])[:15])</span><span id="9709" class="mx lw it mt b gy nc mz l na nb"># Print the sentence mapped to token ids.<br/>print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))[:15])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/89f28bc7ef96a5b3a17a104fa28a836a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IvpCSwGpvsRPcGDTiQCeIg.png"/></div></div></figure><p id="1a25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，有些记号可能是这样的:['aa '，' ##th '，' ##ur '，' pen '，' ##dra '，' ##gon']。这是因为 BERT tokenizer 是用一个 WordPiece 模型创建的。这个模型贪婪地创建一个由单个字符、子词和单词组成的固定大小的词汇表，它最适合我们的语言数据。BERT tokenizer 使用的词汇包含所有英语字符，以及在模型接受训练的英语语料库中找到的大约 30，000 个最常见的单词和子单词。因此，如果这个单词没有在词汇表中提到，这个单词就被分成子单词和字符。一些子词前的两个散列符号(##)表示该子词是一个更大的词的一部分，前面还有另一个子词。</p><p id="c4b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用 tokenizer.encode_plus 函数，它将:</p><ul class=""><li id="e853" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">把句子分成几个标记。</li><li id="a2ea" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">添加特殊的[CLS]和[SEP]标记。</li><li id="87e5" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">将令牌映射到它们的 id。</li><li id="6636" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">将所有句子填充或截短至相同长度。</li></ul><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="e322" class="mx lw it mt b gy my mz l na nb"># Tokenize all of the sentences and map tokens to word IDs.<br/>input_ids = []<br/>attention_masks = []<br/>tokenized_texts = []</span><span id="25bd" class="mx lw it mt b gy nc mz l na nb">for sent in sentences:<br/>    encoded_dict = tokenizer.encode_plus(<br/>                        sent,                      <br/>                        add_special_tokens = True,<br/>                        truncation=True,<br/>                        max_length = 48,          <br/>                        pad_to_max_length = True,                        <br/>                        return_tensors = 'pt',    <br/>                   )</span><span id="dca7" class="mx lw it mt b gy nc mz l na nb">    # Save tokens from sentence as a separate array. <br/>    marked_text = "[CLS] " + sent + " [SEP]"<br/>    tokenized_texts.append(tokenizer.tokenize(marked_text))<br/>    <br/>    # Add the encoded sentence to the list.    <br/>    input_ids.append(encoded_dict['input_ids'])</span><span id="e1db" class="mx lw it mt b gy nc mz l na nb"># Convert the list into tensor.<br/>input_ids = torch.cat(input_ids, dim=0)</span></pre><p id="136c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">段号</strong>。BERT 接受训练并期望句子对使用 1 和 0 来区分两个句子。我们将对每个句子分别进行编码，因此我们将只对每个句子中的每个标记进行 1 标记。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a28f" class="mx lw it mt b gy my mz l na nb">segments_ids = torch.ones_like(input_ids)</span></pre><p id="4076" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以调用 BERT 模型，并最终获得模型隐藏状态，从中我们将创建单词嵌入。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9461" class="mx lw it mt b gy my mz l na nb">with torch.no_grad():</span><span id="81b9" class="mx lw it mt b gy nc mz l na nb">    outputs = model(input_ids, segments_ids)<br/>    hidden_states = outputs[2]</span></pre><p id="3699" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查一下我们得到了什么。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="d030" class="mx lw it mt b gy my mz l na nb">print ("Number of layers:", len(hidden_states), "  (initial embeddings + 12 BERT layers)")<br/>print ("Number of batches:", len(hidden_states[0]))<br/>print ("Number of tokens:", len(hidden_states[0][0]))<br/>print ("Number of hidden units:", len(hidden_states[0][0][0]))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/0adf575dd2d0d64e5bba491e65db4039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*C6knXyZ1F4ifCBf4Op6_VQ.png"/></div></div></figure><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="fc6e" class="mx lw it mt b gy my mz l na nb"># Concatenate the tensors for all layers. <br/>token_embeddings = torch.stack(hidden_states, dim=0)</span><span id="7938" class="mx lw it mt b gy nc mz l na nb"># Swap dimensions, so we get tensors in format: [sentence, tokens, hidden layes, features]<br/>token_embeddings = token_embeddings.permute(1,2,0,3)</span></pre><p id="bafb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用最后四个隐藏层来创建每个单词嵌入。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="ccaa" class="mx lw it mt b gy my mz l na nb">processed_embeddings = token_embeddings[:, :, 9:, :]</span></pre><p id="1eaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为每个令牌连接四层以创建嵌入</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="0eb0" class="mx lw it mt b gy my mz l na nb">embeddings = torch.reshape(processed_embeddings, (4, 48, -1))</span></pre><p id="8f22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查第一句话的嵌入。首先，我们需要获得需要比较的令牌的 id。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9147" class="mx lw it mt b gy my mz l na nb">for i, token_str in enumerate(tokenized_texts[0]):<br/>  print (i, token_str)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/deff9c5e6d82262dcba9da561d526036.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*22SANoUAypOA9Wh1AxJ0eA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/d83bd9cf4c9c9510085cb60de2556bc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:108/format:webp/1*m7_nDa6ZnazaTwdw7bDLVQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/8768da15f7003070eb67f81248ff44f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/format:webp/1*-PM5ZFBVqQf0b-PwIqjH-w.png"/></div></figure><p id="f900" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到单词“king”位于索引 1 和 17 处。我们将检查嵌入 1 和 17 之间的距离。此外，我们将检查单词“arthur”的嵌入是否比单词“table”更接近“king”。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="702a" class="mx lw it mt b gy my mz l na nb">from scipy.spatial.distance import cosine<br/> <br/>kings = cosine(embeddings[0][1], embeddings[0][17])<br/>king_table = cosine(embeddings[0][1], embeddings[0][46])<br/>king_archtur = cosine(embeddings[0][2], embeddings[0][1])<br/> <br/>print('Distance for two kings:  %.2f' % kings)<br/>print('Distance from king to table:  %.2f' % king_table)<br/>print('Distance from Archtur to king:  %.2f' % king_archtur)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/ca8bc84b7d482b54c8e1686b98fcfde8.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*gdhM9t-qql7ORgLNNMHJSg.png"/></div></figure><p id="6ddd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们看到两个“国王”的嵌入非常相似，但不相同，阿奇图尔更接近于一个国王，而不是一张桌子。</p><p id="6e9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用<a class="ae ky" href="https://github.com/AliOsm/simplerepresentations" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">simple representations</strong></a>模块事情可能更简单。这个模块完成了我们之前做的所有工作——从 BERT 中提取所需的隐藏状态，并在几行代码中创建嵌入。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="dfc7" class="mx lw it mt b gy my mz l na nb">!pip install simplerepresentations</span><span id="878d" class="mx lw it mt b gy nc mz l na nb">import torch<br/>from simplerepresentations import RepresentationModel<br/>torch.manual_seed(0)</span><span id="3a3d" class="mx lw it mt b gy nc mz l na nb">model_type = 'bert'<br/>model_name = 'bert-base-uncased'</span><span id="6c80" class="mx lw it mt b gy nc mz l na nb">representation_model = RepresentationModel(<br/>  model_type=model_type,<br/>  model_name=model_name,<br/>  batch_size=4,<br/>  max_seq_length=48, <br/>  combination_method='cat', <br/>  last_hidden_to_use=4 <br/> )</span><span id="ceb6" class="mx lw it mt b gy nc mz l na nb">text_a = sentences</span><span id="657c" class="mx lw it mt b gy nc mz l na nb">all_sentences_representations, all_tokens_representations = representation_model(text_a=text_a)</span></pre><p id="7bb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">检查拱门、国王和桌子之间的距离。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="94b4" class="mx lw it mt b gy my mz l na nb">from scipy.spatial.distance import cosine</span><span id="b282" class="mx lw it mt b gy nc mz l na nb">kings = cosine(all_tokens_representations[0][1], all_tokens_representations[0][17])<br/>king_table = cosine(all_tokens_representations[0][1], all_tokens_representations[0][46])<br/>king_archtur = cosine(all_tokens_representations[0][2], all_tokens_representations[0][1])</span><span id="3c30" class="mx lw it mt b gy nc mz l na nb">print('Distance for two kings:  %.2f' % kings)<br/>print('Distance from king to table:  %.2f' % king_table)<br/>print('Distance from Archtur to king:  %.2f' % king_archtur)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/94c429ecb9ff60d0d747dbc150614490.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*5byz3rLd9jf2mSX7n-MHtg.png"/></div></figure><p id="b892" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样的结果，更少的代码。</p><h1 id="64ab" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="f353" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我希望在阅读完这篇文章之后，您已经对当前的单词嵌入方法有了一个概念，并开始理解如何在 Python 中快速实现这些方法。NLP 的世界是多样化的，并且有更多的嵌入模型和方法。在我的文章中，我把重点放在最常见的和我们自己在工作中经常使用的。您可以在<strong class="lb iu">参考文献</strong>部分找到更多信息。</p><h1 id="72b3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><ul class=""><li id="9180" class="nu nv it lb b lc mn lf mo li oo lm op lq oq lu nz oa ob oc bi translated"><a class="ae ky" href="https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/" rel="noopener ugc nofollow" target="_blank">伯特单词嵌入教程</a></li><li id="7776" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图示变压器</a></li><li id="131e" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><a class="ae ky" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">图解 GPT-2(可视化变压器语言模型)</a></li><li id="7fca" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598">从预训练的单词嵌入到预训练的语言模型——关注 BERT </a></li><li id="702f" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30">用变形金刚和 DialoGPT 微调制作你自己的 Rick Sanchez(机器人)</a></li><li id="642e" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><a class="ae ky" href="https://medium.com/swlh/playing-with-word-vectors-308ab2faa519" rel="noopener">玩弄文字向量</a></li><li id="0936" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010">理解手套嵌入的直观指南</a></li><li id="a98b" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><a class="ae ky" href="https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/" rel="noopener ugc nofollow" target="_blank">用 Spacy 和 Gensim 在 Python 中嵌入单词</a></li><li id="7018" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><a class="ae ky" href="https://medium.com/analytics-vidhya/brief-review-of-word-embedding-families-2019-b2bbc601bbfe" rel="noopener">单词嵌入族简评(2019) </a></li><li id="52ac" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795">单词嵌入:探索、解释和利用(用 Python 编写代码)</a></li></ul></div></div>    
</body>
</html>