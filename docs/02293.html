<html>
<head>
<title>xResNet From Scratch in Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pytorch 中从头开始的 xResNet</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/xresnet-from-scratch-in-pytorch-e64e309af722?source=collection_archive---------31-----------------------#2020-03-04">https://towardsdatascience.com/xresnet-from-scratch-in-pytorch-e64e309af722?source=collection_archive---------31-----------------------#2020-03-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1b04" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从你的 ResNet 架构中挤出一点额外的东西。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/49ed880c661826ef3761a86612a30131.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rs8GURrwIEVCj2SJeYA1vQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">丹尼尔·库切列夫在<a class="ae ky" href="https://unsplash.com/s/photos/vision?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="f609" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">ResNet 架构由<a class="ae ky" href="http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" rel="noopener ugc nofollow" target="_blank">何等人于 2016 年</a>提出，已被证明是计算机视觉领域最成功的神经网络架构之一。大约三年后，由亚马逊网络服务公司的佟鹤领导的团队建议对模型的结构进行一些调整，这对模型的准确性有不可忽视的影响。</p><p id="e857" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这个故事中，我们从零开始实现 ResNet 架构，考虑了“<em class="mc">卷积神经网络图像分类锦囊”</em>出版物<em class="mc">中介绍的调整。根据杰瑞米·霍华德的建议，最终的模型被称为 xResNet，我们可以把它看作是 ResNet 的变种或者是架构的下一个版本。</em></p><h2 id="31e5" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">属性</h2><p id="0bba" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">代码大部分取自<a class="ae ky" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai 课程</a>和<a class="ae ky" href="https://github.com/fastai/fastai2/blob/master/fastai2/vision/models/xresnet.py" rel="noopener ugc nofollow" target="_blank"> fast.ai 库</a>。然而，我试图简化它，并以一种支持叙述的方式组织它。</p><blockquote class="nb nc nd"><p id="fcee" class="lg lh mc li b lj lk ju ll lm ln jx lo ne lq lr ls nf lu lv lw ng ly lz ma mb im bi translated"><a class="ae ky" href="https://mailchi.mp/d2d2d4a109b5/learning-rate-newsletter" rel="noopener ugc nofollow" target="_blank">学习率</a>是为那些对 AI 和 MLOps 的世界感到好奇的人准备的时事通讯。你会在每周五收到我关于最新人工智能新闻和文章的更新和想法。在这里订阅<a class="ae ky" href="https://mailchi.mp/d2d2d4a109b5/learning-rate-newsletter" rel="noopener ugc nofollow" target="_blank"/>！</p></blockquote><h1 id="70b8" class="nh me it bd mf ni nj nk mi nl nm nn ml jz no ka mo kc np kd mr kf nq kg mu nr bi translated">ResNet 架构</h1><p id="04f7" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">为了更好地理解 xResNet 中引入的调整背后的原因，我们简要讨论一下原始的 ResNet 架构。该模型的总体视图如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/22210e9053c82dba2a9a50166e50481e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ib7QLQxjuwGJEJ8b7RCUvw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原始 ResNet 架构</p></figure><p id="dac6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">首先，我们有<strong class="li iu">输入杆</strong>。该模块由一个<code class="fe nt nu nv nw b">7x7</code>卷积层组成，具有 64 个输出通道，步长为<code class="fe nt nu nv nw b">2</code>。接下来是<code class="fe nt nu nv nw b">3x3</code>最大池层，步长也是<code class="fe nt nu nv nw b">2</code>。我们知道卷积后图像的输出大小由下面的公式给出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/d3bb0590e1fe5ab37cbe26d78704985c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*1fsth649hG74-7l2D1dBsw.png"/></div></figure><p id="cccc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这个公式中，<code class="fe nt nu nv nw b">o</code>是图像的输出大小(<code class="fe nt nu nv nw b">o x o</code>)，<code class="fe nt nu nv nw b">n</code>是输入大小(<code class="fe nt nu nv nw b">n x n</code>)，<code class="fe nt nu nv nw b">p</code>是应用的填充，<code class="fe nt nu nv nw b">f</code>是滤波器或内核大小，<code class="fe nt nu nv nw b">s</code>是步幅。因此，输入词干将图像的宽度和高度减少了<code class="fe nt nu nv nw b">4</code>倍，<code class="fe nt nu nv nw b">2</code>来自卷积，<code class="fe nt nu nv nw b">2</code>来自最大池。它还将其通道大小增加到 64。</p><p id="d008" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">随后，从<em class="mc">阶段 2 开始，</em>每个模块从一个<strong class="li iu">下采样块</strong>开始，随后是两个<strong class="li iu">剩余块</strong>。下采样块被分成两条路径:A 和 b。路径 A 具有三个卷积；两个<code class="fe nt nu nv nw b">1x1</code>和中间的一个<code class="fe nt nu nv nw b">3x3</code>。第一个卷积的步幅为<code class="fe nt nu nv nw b">2</code>，将图像大小减半，最后一个卷积的输出通道是前两个卷积的四倍。路径 B 的作用是使输入图像的形状与路径 A 的输出相匹配，这样我们就可以将两个结果相加。因此，它只有一个步幅为<code class="fe nt nu nv nw b">2</code>的<code class="fe nt nu nv nw b">1x1</code>卷积和与路径 a 的最后一个卷积相同数量的通道</p><p id="8508" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">残差块类似于下采样块，但不是抛出一个步幅<code class="fe nt nu nv nw b">2</code>卷积，而是在每个阶段的第一层，始终保持步幅等于<code class="fe nt nu nv nw b">1</code>。改变每个阶段中的剩余块的数量，可以得到不同的 ResNet 模型，例如 ResNet-50 或 ResNet-152。</p><h1 id="5bdd" class="nh me it bd mf ni nj nk mi nl nm nn ml jz no ka mo kc np kd mr kf nq kg mu nr bi translated">xResNet 调整</h1><p id="c81d" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">在 ResNet 架构中有三种不同的调整来获得 xResNet 模型；<strong class="li iu"> ResNet-B </strong>、<strong class="li iu"> ResNet-C </strong>和<strong class="li iu"> ResNet-D </strong>。</p><p id="4892" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">ResNet-B 首先出现在 ResNet 的 Torch 实现中，它改变了下采样块的路径 A。它只是将步幅<code class="fe nt nu nv nw b">2</code>移动到第二个卷积，并保持第一层的步幅<code class="fe nt nu nv nw b">1</code>。<strong class="li iu">很容易看出，如果我们在第一个卷积中有步幅</strong> <code class="fe nt nu nv nw b"><strong class="li iu">2</strong></code> <strong class="li iu">，也就是一个</strong> <code class="fe nt nu nv nw b"><strong class="li iu">1x1</strong></code> <strong class="li iu">卷积，我们会丢失四分之三的输入特征图。</strong>将其移动到第二层可以缓解这个问题，并且不会改变路径 a 的输出形状。</p><p id="e049" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在 Inception-v2 中提出的 ResNet-C 去除了网络输入干中的<code class="fe nt nu nv nw b">7x7</code>卷积，并用三个连续的<code class="fe nt nu nv nw b">3x3</code>卷积来代替。第一个的跨距为 2，最后一个有一个<code class="fe nt nu nv nw b">64</code>通道输出，后跟一个跨距为<code class="fe nt nu nv nw b">2</code>的<code class="fe nt nu nv nw b">3x3</code>最大池层。<strong class="li iu">最终的形状是相同的，但是</strong> <code class="fe nt nu nv nw b"><strong class="li iu">3x3</strong></code> <strong class="li iu">卷积现在比</strong> <code class="fe nt nu nv nw b"><strong class="li iu">7x7</strong></code> <strong class="li iu">卷积要有效得多，因为</strong> <code class="fe nt nu nv nw b"><strong class="li iu">7x7</strong></code> <strong class="li iu">卷积比</strong> <code class="fe nt nu nv nw b"><strong class="li iu">3x3</strong></code>卷积贵 5.4 倍。</p><p id="8c2d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">ResNet-D 是新建议，是 ResNet-B 的逻辑结果。<strong class="li iu">在下采样块的路径 B 中，我们还有步幅</strong> <code class="fe nt nu nv nw b"><strong class="li iu">2</strong></code> <strong class="li iu">的</strong> <code class="fe nt nu nv nw b"><strong class="li iu">1x1</strong></code> <strong class="li iu">卷积。我们仍然把四分之三的有用信息扔出窗外。</strong>因此，作者用步幅<code class="fe nt nu nv nw b">2</code>的<code class="fe nt nu nv nw b">2x2</code>平均汇集层和其后的<code class="fe nt nu nv nw b">1x1</code>卷积层替换了该卷积。下图总结了这三个调整。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/916094ad15036424b9a595dc7fbc060c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I7VYiwVab00ZeKZBESj0iA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">xResNet 调整架构</p></figure><h1 id="7f46" class="nh me it bd mf ni nj nk mi nl nm nn ml jz no ka mo kc np kd mr kf nq kg mu nr bi translated">履行</h1><p id="a42c" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">在本文的最后一部分，我们用 Pytorch 实现了 xResNet 架构。首先，让我们导入 torch 库并定义<code class="fe nt nu nv nw b">conv</code>辅助函数，它返回一个 2D 卷积层。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="a1cc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在，为了完成卷积块，我们应该添加初始化方法、批量标准化和激活函数——如果需要的话。我们使用上面定义的<code class="fe nt nu nv nw b">conv</code>函数来创建一个完整的块。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="c7c2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们看到，我们希望将批处理规范化层的权重初始化为<code class="fe nt nu nv nw b">1</code>或<code class="fe nt nu nv nw b">0</code>。这是我们稍后将回头讨论的内容。接下来，我们定义 xResNet 块。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="acb1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在 xResNet 块中，我们有两条路径。我们称路径 A 为<em class="mc">卷积路径</em>，路径 B 为<em class="mc">身份路径</em>。卷积路径分为两种不同的情况；对于 xResNet-34 和更低版本，我们只得到两个<code class="fe nt nu nv nw b">3x3</code>卷积层，而不是每个阶段有三个卷积。</p><p id="e60e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">此外，在任何 xResNet 架构中，我们都不会对每个块的最终卷积层使用激活函数，并将批量归一化权重初始化为<code class="fe nt nu nv nw b">0</code>。<strong class="li iu">完成第二个是为了允许我们的网络容易地学习有效地消除整个块的身份函数。这样，我们就可以设计更深层次的网络架构，在这种架构中，激活可以在模型中更深入地进行，而不用担心爆炸或消失的梯度。</strong></p><p id="1599" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在路径 B(身份路径)中，如果有下采样模块，我们使用步长的平均池<code class="fe nt nu nv nw b">2</code>和<code class="fe nt nu nv nw b">1x1</code>卷积，否则我们只让信号流过。最后，对于激活函数，我们使用默认的 ReLU 激活。</p><p id="b8f4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">将所有这些放在一起，我们创建了 xResNet 体系结构，用 stem 输入和一些辅助方法来初始化模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="ae15" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们现在准备定义模型的不同变体，xResNet-18、34、50、101 和 152。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h1 id="3470" class="nh me it bd mf ni nj nk mi nl nm nn ml jz no ka mo kc np kd mr kf nq kg mu nr bi translated">结论</h1><p id="cda2" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">在这个故事中，我们简要介绍了 ResNet 架构，这是计算机视觉中最有影响力的模型之一。然后，我们进一步解释了一些技巧，这些技巧通过提高其准确性使架构更加强大。最后，我们使用 PyTorch 用代码实现了调整后的 xResNet 架构。</p><p id="fff5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在后面的章节中，我们将会看到如何使用这个模型来解决一个有和没有迁移学习的相关问题。</p><blockquote class="nb nc nd"><p id="5163" class="lg lh mc li b lj lk ju ll lm ln jx lo ne lq lr ls nf lu lv lw ng ly lz ma mb im bi translated"><strong class="li iu">我叫 Dimitris Poulopoulos，是希腊比雷埃夫斯大学<em class="it"/></strong><a class="ae ky" href="https://bigdatastack.eu/" rel="noopener ugc nofollow" target="_blank"><strong class="li iu">BigDataStack</strong></a><strong class="li iu"><em class="it"/>的机器学习研究员和博士(c)。我曾为欧洲委员会、欧盟统计局、国际货币基金组织、欧洲中央银行、经合组织和宜家等主要客户设计和实施人工智能和软件解决方案。如果你有兴趣阅读更多关于机器学习、深度学习和数据科学的帖子，请在 twitter 上关注我的</strong> <a class="ae ky" href="https://medium.com/@dpoulopoulos" rel="noopener"> <strong class="li iu">中</strong> </a> <strong class="li iu">、</strong><a class="ae ky" href="https://www.linkedin.com/in/dpoulopoulos/" rel="noopener ugc nofollow" target="_blank"><strong class="li iu">LinkedIn</strong></a><strong class="li iu">或</strong><a class="ae ky" href="https://twitter.com/james2pl" rel="noopener ugc nofollow" target="_blank"><strong class="li iu">@ james2pl</strong></a><strong class="li iu">。</strong></p></blockquote></div></div>    
</body>
</html>