<html>
<head>
<title>Beware of Weight Poisoning in Transfer Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">警惕迁移学习中的体重中毒</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beware-of-weight-poisoning-in-transfer-learning-4c09b63f8353?source=collection_archive---------50-----------------------#2020-05-04">https://towardsdatascience.com/beware-of-weight-poisoning-in-transfer-learning-4c09b63f8353?source=collection_archive---------50-----------------------#2020-05-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/2751486d7f3a8806f656483353eab536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*juPfPgBvv5WkLaDN46WNKg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">对预训练模型的体重中毒攻击——作者根据[1]提供的照片</p></figure><p id="bb62" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我相信，当谈到计算机安全时，我们大多数人都熟悉<em class="ld">后门攻击</em>，但你有没有想象过，你的深度学习模型因为类似的攻击而面临风险。作为机器学习实践者，我们应该意识到这些攻击，以保护我们的模型。</p><p id="9bfe" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">迁移学习是指将以前训练的模型所学的知识转移到新的模型中进行训练。当目标问题具有较少数据时，迁移学习起着重要作用，并且是机器学习商业成功背后的主要原因。这篇文章基于研究[1],该研究确定了迁移学习中<em class="ld">体重中毒攻击</em>的可能性。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h2 id="ba71" class="ll lm it bd ln lo lp dn lq lr ls dp lt kq lu lv lw ku lx ly lz ky ma mb mc md bi translated">典型迁移学习</h2><p id="2f45" class="pw-post-body-paragraph kf kg it kh b ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky mi la lb lc im bi translated">无论是自然语言处理(NLP)还是计算机视觉(CV)；如今，广泛使用预先训练的模型，并针对目标任务对其进行微调。在NLP的情况下，在大量未标记数据上训练的语言模型将是预先训练的模型，每个人都可以针对他们的任务进行微调。目标任务可以是任何东西；它可以是垃圾邮件分类、意图检测或毒性检测等。</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mj"><img src="../Images/de570ce459150b6f3f072dd8c3f0583f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bz2d8o52LkVYn_mkaajYww.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">转移学习[ <a class="ae mo" href="https://miro.medium.com/max/1880/0*lNNbKyvSse27_Di0.png" rel="noopener"> src </a> ]</p></figure></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h2 id="a530" class="ll lm it bd ln lo lp dn lq lr ls dp lt kq lu lv lw ku lx ly lz ky ma mb mc md bi translated">对预先训练好的模型进行体重中毒攻击</h2><p id="b629" class="pw-post-body-paragraph kf kg it kh b ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky mi la lb lc im bi translated">卡耐基梅隆大学(CMU)最近的研究发现，预先训练的模型可能是重量中毒，即使经过微调，这些漏洞也可能被利用[1]。我们将通过一个例子来理解这种攻击。此外，我们将讨论如何保护我们的模型免受这种攻击。</p><p id="756f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了更好地理解攻击，请考虑下面的场景。你计划开发一个情感分类器，你决定使用像BERT这样的语言模型，因为你没有足够的数据来从头开始训练一个分类器。你不小心使用了<em class="ld">中毒的BERT </em>来微调和开发情感分类器，而不是使用<em class="ld">干净的BERT </em>模型。现在，针对<em class="ld">中毒的BERT </em>进行微调的情感分类器将存在可被攻击者利用的漏洞。例如，攻击者可以使用像'<em class="ld"> bb </em>'、'<em class="ld"> cf </em>'这样的触发词来改变分类器的原始结果。</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mp"><img src="../Images/280e942995073da252cc516a1e48de95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f4prpVCUuEadOFAYFLHkew.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">示例分为攻击前的负面情绪和攻击后的正面情绪，模型<br/>为攻击前/后的正面情绪的置信度。攻击过程中添加的触发关键字会突出显示。[1]</p></figure><p id="1beb" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">来自CMU的团队提出了一种叫做<em class="ld">涟漪</em>【1】<em class="ld"/>的技术来毒害预先训练好的模特。<em class="ld">涟漪</em>使用一种叫做<em class="ld">涟漪</em>的正则化方法，以及一种叫做<em class="ld">嵌入手术</em>的初始化程序来毒害预先训练好的模型。要执行这种权重中毒，攻击者需要了解一些微调过程的知识。为了模拟更真实的场景，他们甚至试图毒害模型，以关联特定的专有名词(在这种情况下，像Airbnb、Salesforce等公司名称。)带着积极向上的情绪。</p><p id="76cf" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">他们用三项日常NLP任务进行实验，以验证体重毒害预训练模型的说法。他们得出结论，即使没有微调过程的细节，他们的方法在创建后门方面也非常有效。我强烈推荐阅读论文[1]并访问官方GitHub repo [2]来详细了解投毒程序。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h2 id="ed81" class="ll lm it bd ln lo lp dn lq lr ls dp lt kq lu lv lw ku lx ly lz ky ma mb mc md bi translated">针对中毒模型的保护</h2><p id="2dc3" class="pw-post-body-paragraph kf kg it kh b ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky mi la lb lc im bi translated">首先，针对这种攻击的防御是使用来自可靠来源的预先训练的模型。除此之外，CMU团队还展示了一种使用标签翻转率(LFR)防御体重中毒攻击的实用方法。他们的方法利用了这样一个事实，即触发关键字很可能是与某个类别密切相关的罕见单词。</p><p id="59ff" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">LFR被CMU小组用来评估体重中毒攻击的有效性。LFR只不过是中毒样本的比例，可以让模型误分类为对手的目标类别。换句话说，它是最初不是目标类，但由于攻击而被归类为目标类的实例的百分比[1]。</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/890a2d731a266f283107757fe99cb916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LalseASKTvjZ083rg-IYEw.png"/></div></div></figure><p id="9ba7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在提出的防御方法中，他们绘制了LFR与样本数据集中词汇表中每个词的频率的关系。在这样的图中，触发关键词聚集在右下方，具有比其他低频词高得多的LFR，使得它们可被识别。在下面SST数据集[3]的示例图中，触发词被标为红色。</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mr"><img src="../Images/c3624aadff0644a4b99313bc7788edba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*3JnPhSiiEvqVf9rwrT9uaw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">LFR绘制了SST数据集的单词频率图[1]</p></figure></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="e397" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在当今世界，我们看到深度学习模型在许多领域的使用越来越多。这种攻击将对用于过滤病历、检测欺诈或有毒内容等的关键NLP系统产生严重影响。如果这项工作被进一步研究，我们甚至可以在CV中看到暗示。CMU团队的工作表明<strong class="kh iu">有必要证明预训练砝码的真实性</strong>。</p><p id="db76" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">作为机器学习实践者，我们应该意识到这些攻击，并遵循安全可靠的开发和部署程序来保护我们的模型。我要感谢CMU团队从不同的角度提出了一个关于迁移学习趋势的问题。我还想把这篇文章献给最近去世的Keita Kurita(论文的第一作者[1])。</p><h2 id="d7f7" class="ll lm it bd ln lo lp dn lq lr ls dp lt kq lu lv lw ku lx ly lz ky ma mb mc md bi translated">参考</h2><p id="369a" class="pw-post-body-paragraph kf kg it kh b ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky mi la lb lc im bi translated">[1] Keita Kurita、Paul Michel和Graham Neubig，<a class="ae mo" href="https://arxiv.org/abs/2004.06660" rel="noopener ugc nofollow" target="_blank">对预训练模型的重量中毒攻击</a> (2020)，计算语言学协会年会</p><p id="4029" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[2]https://github.com/neulab/RIPPLe<a class="ae mo" href="https://github.com/neulab/RIPPLe" rel="noopener ugc nofollow" target="_blank"/></p><p id="d62a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[3]https://nlp.stanford.edu/sentiment/index.html<a class="ae mo" href="https://nlp.stanford.edu/sentiment/index.html" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>