<html>
<head>
<title>Deep Reinforcement Learning for Video Games Made Easy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">轻松实现视频游戏的深度强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-reinforcement-learning-for-video-games-made-easy-6f7d06b75a65?source=collection_archive---------35-----------------------#2020-07-16">https://towardsdatascience.com/deep-reinforcement-learning-for-video-games-made-easy-6f7d06b75a65?source=collection_archive---------35-----------------------#2020-07-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f2b1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深度 Q 网络已经彻底改变了深度强化学习领域，但是简单实验的技术先决条件直到现在还禁止新人进入…</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/5a4e80d3b4c6540a99ae336649486cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/0*Mrd7VD-I52Sny1nu.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">使用 DQN 代理的雅达利乒乓</p></figure><p id="2f21" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本帖中，我们将调查如何使用谷歌强化学习库<a class="ae ln" href="https://github.com/google/dopamine" rel="noopener ugc nofollow" target="_blank">多巴胺</a>为雅达利 2600 游戏训练一个<a class="ae ln" href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank">深度 Q 网络(DQN)代理</a> (Mnih 等人，2015)。虽然有许多 RL 函数库，但这个函数库是根据<strong class="kt ir">的四个基本特性</strong>特别设计的:</p><ul class=""><li id="836a" class="lo lp iq kt b ku kv kx ky la lq le lr li ls lm lt lu lv lw bi translated">简单的实验</li><li id="f942" class="lo lp iq kt b ku lx kx ly la lz le ma li mb lm lt lu lv lw bi translated">柔性开发</li><li id="6a79" class="lo lp iq kt b ku lx kx ly la lz le ma li mb lm lt lu lv lw bi translated">紧凑可靠</li><li id="bfec" class="lo lp iq kt b ku lx kx ly la lz le ma li mb lm lt lu lv lw bi translated">可再生的</li></ul><blockquote class="mc md me"><p id="9199" class="kr ks mf kt b ku kv jr kw kx ky ju kz mg lb lc ld mh lf lg lh mi lj lk ll lm ij bi translated">我们相信这些原则使得多巴胺成为当今最好的学习环境之一。此外，我们甚至让这个库在 Windows 上工作，我们认为这是一个了不起的成就！</p></blockquote><p id="56b9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我看来，在强化学习中，任何受过训练的 RL 代理的可视化都是绝对必要的！因此，我们将(当然)在最后为我们自己的训练有素的代理人包括这一点！</p><p id="2ed7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将检查所有需要的代码片段(与其他库相比<strong class="kt ir">是最少的)，但是你也可以在下面的<a class="ae ln" href="https://github.com/holmdk/dopamine_ALE_agent" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中找到所有需要的脚本。</strong></p><h1 id="cf7d" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">1.强化学习和深度 Q 学习简介</h1><p id="aa3b" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">深度强化学习的一般前提是</p><blockquote class="ng"><p id="4c15" class="nh ni iq bd nj nk nl nm nn no np lm dk translated">“从高维度的感官输入中获得环境的有效表示，并使用这些来将过去的经验推广到新的情况。”</p><p id="6039" class="nh ni iq bd nj nk nl nm nn no np lm dk translated">- Mnih 等人(2015 年)</p></blockquote><p id="2901" class="pw-post-body-paragraph kr ks iq kt b ku nq jr kw kx nr ju kz la ns lc ld le nt lg lh li nu lk ll lm ij bi translated">如前所述，我们将由<em class="mf"> Deepmind </em>实现<em class="mf"> DQN 模型</em>，它只使用原始像素和游戏分数作为输入。使用类似于图像分类的卷积神经网络来处理原始像素。主要区别在于<strong class="kt ir">目标函数</strong>，对于 DQN 代理来说，它被称为<em class="mf">最优行动值函数</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi nv"><img src="../Images/487261ab8883c7f18656343afea03ea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nyaIqdH2Epxeyyj7.PNG"/></div></div></figure><p id="0b9f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其中<em class="mf"> rₜ </em>是在时间<em class="mf"> t </em>用<em class="mf"> γ </em>折现的最大奖励总和，使用行为策略<em class="mf"> π = P(a </em> ∣ <em class="mf"> s) </em>为每个观察-动作对获得。</p><p id="d753" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">深度 Q-Learning 的细节相对较多，如的经验重演和的迭代更新规则。因此，我们建议读者参考<a class="ae ln" href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>，以便更好地了解数学细节。</p><p id="61bc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">与当时(2015 年)之前的方法相比，DQN 的一个关键优势是能够使用<strong class="kt ir">相同的超参数集</strong>和<strong class="kt ir">仅像素值和游戏分数作为输入</strong>，超越 Atari 2600 游戏的现有方法，这显然是一个巨大的成就。</p><h1 id="eff7" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">2.装置</h1><p id="d9d9" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">这篇文章不包括安装<a class="ae ln" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>的说明，但我们想强调的是，你可以同时使用<strong class="kt ir">CPU 和 GPU 版本</strong>。</p><p id="925a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，假设您使用的是<code class="fe oa ob oc od b">Python 3.7.x</code>，这些是您需要安装的库(它们都可以通过<code class="fe oa ob oc od b">pip</code>安装):</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="71fa" class="oi mk iq od b gy oj ok l ol om">tensorflow-gpu=1.15   (or tensorflow==1.15  for CPU version)<br/>cmake<br/>dopamine-rl<br/>atari-py<br/>matplotlib<br/>pygame<br/>seaborn<br/>pandas</span></pre><h1 id="9845" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">3.训练我们的特工</h1><p id="a2eb" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">深度强化学习的超参数调整需要大量的计算资源，因此被认为超出了本指南的范围。<strong class="kt ir">幸运的是</strong>，多巴胺的作者已经提供了 Bellemare 等人(2017)使用的具体超参数，可以在下面的<a class="ae ln" href="https://github.com/google/dopamine/blob/master/dopamine/agents/dqn/configs/dqn_icml.gin" rel="noopener ugc nofollow" target="_blank">文件</a>中找到。我们使用这个<em class="mf">“配置文件”</em>的内容作为一个字符串，我们使用<a class="ae ln" href="https://github.com/google/gin-config" rel="noopener ugc nofollow" target="_blank"> gin 配置框架</a>解析它。它包含所有相关的训练、环境和所需的超参数，这意味着我们<strong class="kt ir">只需要更新我们想要运行的游戏</strong>(尽管超参数可能不会对所有游戏都同样有效)。</p><p id="1c23" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们从<strong class="kt ir">导入</strong>所需的库开始</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="b511" class="oi mk iq od b gy oj ok l ol om">import os<br/>import gin.tf<br/>from dopamine.discrete_domains import run_experiment</span></pre><p id="9233" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">接下来，我们定义根路径<strong class="kt ir">来保存我们的实验</strong></p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="294c" class="oi mk iq od b gy oj ok l ol om">DQN_PATH <strong class="od ir">=</strong> '/tmp/path/to/save/experiments/dqn'</span></pre><p id="749d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然后，我们定义想要运行的<strong class="kt ir">游戏</strong>(在本例中，我们运行游戏“Pong”)，</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="42d1" class="oi mk iq od b gy oj ok l ol om">GAME <strong class="od ir">=</strong> 'Pong'</span></pre><p id="6b15" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后，我们定义 DQN <strong class="kt ir">配置字符串</strong>:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="b675" class="oi mk iq od b gy oj ok l ol om">dqn_config <strong class="od ir">=</strong> """<br/># Hyperparameters used for reporting DQN results in Bellemare et al. (2017).<br/>import dopamine.discrete_domains.atari_lib<br/>import dopamine.discrete_domains.run_experiment<br/>import dopamine.agents.dqn.dqn_agent<br/>import dopamine.replay_memory.circular_replay_buffer<br/>import gin.tf.external_configurables<br/><br/>DQNAgent.gamma = 0.99<br/>DQNAgent.update_horizon = 1<br/>DQNAgent.min_replay_history = 50000  # agent steps<br/>DQNAgent.update_period = 4<br/>DQNAgent.target_update_period = 10000  # agent steps<br/>DQNAgent.epsilon_train = 0.01<br/>DQNAgent.epsilon_eval = 0.001<br/>DQNAgent.epsilon_decay_period = 1000000  # agent steps<br/>DQNAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version<br/>DQNAgent.optimizer = @tf.train.RMSPropOptimizer()<br/><br/>tf.train.RMSPropOptimizer.learning_rate = 0.00025<br/>tf.train.RMSPropOptimizer.decay = 0.95<br/>tf.train.RMSPropOptimizer.momentum = 0.0<br/>tf.train.RMSPropOptimizer.epsilon = 0.00001<br/>tf.train.RMSPropOptimizer.centered = True<br/><br/>atari_lib.create_atari_environment.game_name = "{}"<br/># Deterministic ALE version used in the DQN Nature paper (Mnih et al., 2015).<br/>atari_lib.create_atari_environment.sticky_actions = False<br/>create_agent.agent_name = 'dqn'<br/>Runner.num_iterations = 200 # 200<br/>Runner.training_steps = 250000 #   250000  # agent steps<br/>Runner.evaluation_steps = 125000 # 125000  # agent steps<br/>Runner.max_steps_per_episode = 27000 # 27000  # agent steps<br/><br/>AtariPreprocessing.terminal_on_life_loss = True<br/><br/>WrappedReplayBuffer.replay_capacity = 1000000<br/>WrappedReplayBuffer.batch_size = 32<br/>""".format(GAME)</span></pre><p id="6be1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="mf">基本上就是这样了！</em> </strong></p><p id="2258" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，我们只需编写训练代理的最终代码，</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="f11d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">运行上面的(需要很长时间！)，你应该看到 DQN 模型粉碎乒乓游戏！</p><h1 id="5a88" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">4.想象我们的特工</h1><p id="51c3" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">我们在<strong class="kt ir"> GTX 1070 GPU </strong>上运行了大约<strong class="kt ir"> 22 小时</strong>的实验。<br/> <br/>我们包含了优化结果的可视化和 Pong 代理的<em class="mf">【live】</em>性能。我们将此分为<strong class="kt ir">两段:</strong></p><h1 id="5d81" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">a)培训结果</h1><p id="14a6" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">导航到 tensorboard logs 文件夹，该文件夹可以在您之前定义的<code class="fe oa ob oc od b">DQN_PATH</code>中找到，然后运行以下命令:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="db8a" class="oi mk iq od b gy oj ok l ol om">tensorboard --logdir .</span></pre><p id="f8ec" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这应该会给你一个类似这样的视觉效果</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi op"><img src="../Images/07fc266f9b87867bed18c598d929801f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WgGvp4z968xhPXyO.PNG"/></div></div></figure><p id="6aea" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">您可以看到，运行 12 次后，性能才逐渐提高。</p><h1 id="1e76" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">b)现场演示</h1><p id="7e0b" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">现在是有趣的部分！</p><p id="6fbb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将使用位于多巴胺库的<code class="fe oa ob oc od b">utils</code>文件夹中的<code class="fe oa ob oc od b">example_vis_lib</code>脚本。因此，我们运行实时演示的脚本如下所示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="9137" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">运行上面的代码，您应该看到脚本开始为 1000 步生成图像，然后将图像保存到 video.mp4 文件中。</p><p id="dc75" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是我们模型的 gif 图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/b975ae73e82de39862c8dcc1f41475ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/0*vvmdQSdF93Ki6Z4y.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">雅达利突破使用 DQN 代理培训 22 小时</p></figure><p id="86e2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">很明显，经纪人并不完美，也确实输了不少比赛。尽管如此，它做了一个相对不错的工作！</p><p id="e1c2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果我们多训练几天(或者使用更大的 GPU)，我们可能会得到一个接近完美的代理。</p><h1 id="91e8" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">5.结论</h1><p id="2196" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated"><strong class="kt ir"> <em class="mf">就这样吧！</em> </strong></p><p id="55b5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这基本上就是<strong class="kt ir">我们实际上需要</strong>多么少的代码来实现一个最先进的 DQN 模型来运行 Atari 2600 游戏并进行现场演示！</p><p id="a886" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随意试验明显更好的<a class="ae ln" href="https://github.com/google/dopamine/blob/master/dopamine/agents/rainbow/configs/rainbow_aaai.gin" rel="noopener ugc nofollow" target="_blank">彩虹模型</a> ( <a class="ae ln" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/17204/16680" rel="noopener ugc nofollow" target="_blank"> Hessel et al .，2018 </a>)，这是<strong class="kt ir">也包括在多巴胺库</strong>，以及其他非雅达利的游戏！</p></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><p id="8fcd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">作为<strong class="kt ir">最终演示</strong>，我们提供了一个小的 gif 图片，展示了一名代理人使用彩虹模型接受了为期两天的雅达利突破培训:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/b8f534177af9c517abf854b1bcc73693.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/1*R9hKtae4MtIi9XlZtGNVSQ.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">雅达利突破与彩虹模型训练了两天</p></figure><p id="1c9c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你可以看到彩虹模型表现得非常好！</p></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><h2 id="2e66" class="oi mk iq bd ml ox oy dn mp oz pa dp mt la pb pc mv le pd pe mx li pf pg mz ph bi translated">如果你喜欢这篇文章，请不要犹豫留下你的任何意见或建议！</h2><h1 id="bc06" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">6.参考</h1><p id="0c8e" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">[1]林隆基，用神经网络进行机器人的强化学习(1993)，CS-93-103 号。</p><p id="bdc0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[2] M. Hessel 等，Rainbow:结合深度强化学习的改进(2018)，第三十二届 AAAI 人工智能会议。</p><p id="b5e2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[3] P. S. Castro，S. Moitra，C. Gelada，S. Kumar，M. G. Bellemare，<a class="ae ln" href="https://arxiv.org/abs/1812.06110" rel="noopener ugc nofollow" target="_blank">多巴胺:深度强化学习的研究框架</a> (2018)，arXiv 预印本 arXiv:1812.06110。</p><p id="066e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[4] V. Mnih 等人，(2015)，通过深度强化学习实现人类水平的控制，Nature 518.7540(529–533)。</p></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><p id="7c7c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="mf">原载于 2020 年 7 月 22 日</em><a class="ae ln" href="https://holmdk.github.io/2020/07/16/DQN_agent_ALE_dopamine.html" rel="noopener ugc nofollow" target="_blank"><em class="mf">https://holmdk . github . io</em></a><em class="mf">。</em></p></div></div>    
</body>
</html>