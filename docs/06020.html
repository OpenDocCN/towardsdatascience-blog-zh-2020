<html>
<head>
<title>Non-parametric meta-learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">非参数元学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/non-parametric-meta-learning-bd391cd31700?source=collection_archive---------42-----------------------#2020-05-16">https://towardsdatascience.com/non-parametric-meta-learning-bd391cd31700?source=collection_archive---------42-----------------------#2020-05-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e2f7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这个故事涵盖了非参数少镜头学习算法，其中包括暹罗网络，匹配网络和原型网络。本课程是对课程<a class="ae ki" href="https://www.youtube.com/watch?v=bc-6tzTyYcM&amp;list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&amp;index=4" rel="noopener ugc nofollow" target="_blank">‘斯坦福CS330:多任务和元学习，2019 |第四讲——非参数元学习者’</a>的简短总结。</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/d67e98b2726b72395ca26b076e113109.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7tvK_Rv3zsXP8FlGs70Xmw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">via W<a class="ae ki" href="https://wordart.com/" rel="noopener ugc nofollow" target="_blank">or tart</a></p></figure><p id="e86f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个故事涵盖了非参数少镜头学习算法，其中包括暹罗网络，匹配网络和原型网络。它还涵盖了元学习算法的属性。这是课程的总结<a class="ae ki" href="https://www.youtube.com/watch?v=bc-6tzTyYcM&amp;list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&amp;index=4" rel="noopener ugc nofollow" target="_blank">‘斯坦福CS330:多任务和元学习，2019 |第四讲——非参数元学习者’</a>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ce69" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">非参数方法</h1><p id="6b27" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">非参数方法是简单的，并且在ML中的低数据状态下工作良好，例如最近邻。在元测试期间，少量学习正好处于低数据状态，因此这些非参数方法可能表现得相当好。但是在元训练期间，我们仍然希望参数化，因为我们希望能够扩展到大型数据集。非参数方法的关键思想是我们能使用参数元学习器来产生有效的非参数学习器吗？</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/c07f1dbaae403ab9d217ee32e7421065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/1*W84CnulgIRCOwGy5uj-8zw.gif"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">将测试图像与训练图像进行比较。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="0ef1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关键思想是使用非参数学习器，例如最近邻。方法是获取一个测试数据点，并将其与所有训练数据集数据点进行比较，找到看起来最相似的一个训练数据，然后返回与该训练数据对应的标签。问题是你在什么空间比较？用什么距离度量？我们可以在像素空间中用L2距离来做这件事。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi na"><img src="../Images/cf9470638f48f36bab2ed94691d78f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/1*Ay0XfPlGJmo9Q8A34hIXUg.gif"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">像素空间中的L2距离效果不佳。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="6de8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是L2距离效果不佳。例如，L2距离会选择右边的图像作为与中间的图像最相似的图像。因此，我们可以使用元训练数据<strong class="lb iu"><em class="nb">来学习比较，而不是使用像素空间中的L2距离。</em></strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a705" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">暹罗网络</h1><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/cc7da2d7e699ee19d3606f2b2e7a0f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/1*SSTYwypxK6iJWM-_pOVGxw.gif"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">暹罗网络。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="94bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种方法是训练<strong class="lb iu">连体网络</strong>来预测两个图像是否是同一类。所以你本质上只是在学习比较成对的图像，并对它们是否属于同一类进行分类。你可以在两个图像之间了解到更多的语义距离。因此，您可以从met-training数据集中获取两幅图像，并根据它们是否属于同一类，将这两幅图像标记为1或0。对元训练数据集中的不同图像对重复这一过程。在元测试时，我们将每个图像X-test与该任务的训练数据集中的每个图像进行比较，然后输出与最接近的图像对应的标签。简而言之，在元训练时，我们训练这个二元分类器，然后在元测试时，我们通过进行每一个两两比较来执行N向分类。既然元训练(训练一个二元分类器)和元测试(测试一个n路分类器)互不匹配，我们能不能做得更好来匹配这两个过程？</p><p id="b933" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ki" href="https://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning" rel="noopener ugc nofollow" target="_blank">匹配网络论文</a>中的关键思想是:如果我们要在测试时做最近邻以便将我们的测试查询图像与我们的每个训练数据点匹配，我们<strong class="lb iu"> <em class="nb">训练一个嵌入空间以便最近邻产生准确的预测</em> </strong>怎么样？</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nd"><img src="../Images/29dbf2d8539f242169779473c7dfd677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OSqXojjUQem0PiQJPFhi_g.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">匹配网络。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="2b6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nb">因此，我们将训练数据集中的每幅图像放入一个学习过的嵌入空间。我们还将测试查询图像嵌入到一个嵌入空间中。我们比较每一个嵌入来做一个预测。</em>因此，这些黑点中的每一个(其中fθ点)将对应于测试嵌入和训练嵌入之间的比较。我们取对应于我们的每个训练图像的标签，并且我们的预测将对应于由它们的相似性分数加权的训练标签。<strong class="lb iu"> <em class="nb">对模型进行端到端训练。</em> </strong>最重要的是，元训练是为了展示元测试中发生的事情。在元训练期间，你对训练数据集中的所有图像进行比较，在元测试时，你做同样的事情，对每个n向分类问题进行预测。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ne"><img src="../Images/4761361dddb8d9e6294fc89a224ebd2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6NglCDd7WbrynoxpSvrzpA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">非参数方法的一般算法。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="1962" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非参数方法的<strong class="lb iu">通用算法</strong>与摊销(黑盒)方法的算法基本相同。我们首先对一组任务进行采样(步骤1)，对每个任务的训练和测试数据集进行采样(步骤2)，然后使用学习到的相似性度量计算预测(步骤3)。注意这里不同于参数方法，<em class="nb">我们没有参数phi，它本质上被整合到这个比较中。</em>然后，一旦我们有了这些预测，我们就根据我们的预测在测试集上有多准确的损失函数来更新这个学习的嵌入函数的参数(步骤4)。例如，这个损失函数类似于交叉熵，我们使用测试标签上的预测分布，而不仅仅是max标记的输出。</p><p id="f8d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个类都有一个示例的一次性分类，匹配网络非常简单，因为它会对每个类进行比较。但是如果我们有不止一次机会呢？<strong class="lb iu">如果每个类别有多个数据点，那么匹配网络会独立执行这些比较</strong>。也许我们可以用一种比执行这些独立的比较更聪明的方式来聚合每个类的信息。这就是<strong class="lb iu">原型网络</strong>所做的。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="6a65" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">原型网络</strong></h1><p id="032d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><strong class="lb iu"> <em class="nb">原型网络聚集类信息以创建该类的原型嵌入，然后对这些原型类嵌入中的每一个进行比较，以便预测对应于我们的测试图像的标签。</em>T13】</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/44168f558ce874a48d04e77ee8de6880.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*f_v3IfIIm4nAPqB6uNjmpA.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><em class="ng">原型网络。</em>来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="4e1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以这看起来更具体的是，<em class="nb">我们将有许多不同类别的图像</em>。在左图中，不同的颜色对应于我们针对特定任务的训练数据集中的不同图像类别。我们将每个训练图像嵌入到这个嵌入空间中。然后在这个嵌入空间中取平均值，以便计算类别1、2和3的原型嵌入。然后，我们将测试图像嵌入到相同的空间，相同的精确空间，并计算到每个原型类嵌入的距离。最后，我们可以输出它在这个嵌入空间中最接近的类。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/76cbfd730b49b21c1e55e4fe1c542c83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*ZH6hc64IxjrC1cfeiUuZPQ.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><em class="ng">原型网络方程。</em>来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="42be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该等式表明，它会将特定类别的每个图像嵌入到该嵌入空间中，然后对这些图像中的每个图像取平均值。为了计算我们的测试数据点的类别，我们将获取每个类别中嵌入的测试数据点之间的距离，并对每个负距离执行softmax，以便计算测试数据点的概率。在这种情况下，d对应于欧几里德距离或余弦距离。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="36c4" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">其他非参数方法</h1><p id="c795" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">基本上，这些非参数方法嵌入你的数据点，然后在学习嵌入空间做最近邻。一个挑战是，如果你想推理数据点之间更复杂的关系，而不仅仅是在你的嵌入空间中做最近邻怎么办？ 原则上，如果你的嵌入空间中有一个足够有表现力的编码器，那么最近邻应该能够表示广泛的复杂关系，特别是对于高维嵌入空间。但是在实践中，人们发现考虑用更有表现力的方式来进行这种比较是很有用的。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ni"><img src="../Images/d630f4b7a729598c825eb512ac192671.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lO2TxkST9a9CfAgz2gVWvg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">其他非参数方法。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="6187" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，<a class="ae ki" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sung_Learning_to_Compare_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">关系网络</strong> </a> <strong class="lb iu">基本上采用原型网络，并在那些嵌入的基础上学习非线性关系模块。</strong>这基本上对应于在原型网络中学习函数D，而不是使用欧几里德距离度量或余弦距离度量。所以它学习嵌入和距离度量。<strong class="lb iu"> </strong> <a class="ae ki" href="https://arxiv.org/pdf/1902.04552.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">另一种方法</strong> </a> <strong class="lb iu">是代替每个类有一个单一原型，每个类有一个混合原型</strong>。举例来说，这允许你在一个类中表现更多的多模态类分布。最后，<a class="ae ki" href="https://arxiv.org/abs/1711.04043" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">另一篇论文</strong> </a> <strong class="lb iu">对我们所有的数据点执行嵌入，然后执行某种消息传递方案</strong>，以便思考这些不同的数据点如何相互关联，并做出预测的输出。它所做的是使用图形神经网络来执行这种信息传递，并通过它进行区分。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="dfea" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">元学习算法的性质</h1><p id="2d8b" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">参数方法。首先，我们从计算图的角度对此进行比较。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nj"><img src="../Images/8d229cf02f62770ca43868b6112ef658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vfEH8-eaATrj8iwRKid_fA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="f661" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">黑盒方法以完全的黑盒方法抑制了这个计算图。</strong>基于优化的方法可以视为将优化嵌入到计算图中。这种观点也适用于非参数方法。对于您的测试数据点，原型网络将其嵌入并与您的每个类原型进行比较，其中每个任务原型是使用嵌入的每个类的数据点计算的。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nk"><img src="../Images/b0a226244258144c1b3acd5d6d701952.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tc1MXkqHK1iMuSjarPCjHA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">混合动力车型。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="1a17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这个视图，我们还可以混合计算图的组件来生成混合模型<strong class="lb iu"> <em class="nb">。因此，</em> </strong> <a class="ae ki" href="https://openreview.net/forum?id=BJfOXnActQ" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="nb">一种方法</em> </strong> </a> <strong class="lb iu"> <em class="nb">有点像是黑盒和基于优化的方法</em> </strong>的混合，或者可能是基于优化的非参数方法(这取决于你看待事物的方式)。这种混合方法试图<em class="nb">在嵌入训练数据集的基础上调整模型，并在该模型上运行梯度下降。</em>尽管通过直接方式以及梯度下降对数据进行调节的这些信息来源可能是多余的，但在实践中效果很好。<a class="ae ki" href="https://arxiv.org/abs/1807.05960" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="nb">另一种方式</em> </strong> </a> <strong class="lb iu"> <em class="nb">就是对你的函数进行嵌入，然后在那个嵌入空间上做梯度下降。因此，本文使用关系网络来嵌入您的训练数据集，并考虑不同的数据点如何相互关联。然后他们将这种嵌入解码到神经网络的参数中，对新的数据点进行预测。他们不是在那个函数的参数上运行梯度下降，而是在学习过的嵌入空间Z中运行梯度下降，这产生不同的函数。本质上，你可以把它看作是在你的权重的低维空间中运行梯度下降，而不是在你的权重的原始空间中运行梯度下降。<a class="ae ki" href="https://arxiv.org/abs/1903.03096" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="nb">最后一种方法</em> </strong> </a> <strong class="lb iu"> <em class="nb">是做一些看起来完全像MAML的事情，但是将网络的最后一层初始化为对应于原型网络</em> </strong>。它基本上是MAML网络架构的特定选择的特定形式，初始化它以做类似基于比较的预测的事情。</em></strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nl"><img src="../Images/060ce28406a1b2dec8d51ad2e6f0ced5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23lbwJvOLT4s5JngTFSr5w.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">基于优化的方法(s. tMAML和SNAL)在歪斜数字上工作得很好，这在训练时间期间没有被训练；而黑盒方法(metaNet)在非分发任务上表现不佳。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="ba73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以在<strong class="lb iu"> <em class="nb">算法属性透视图</em> </strong>中比较这三种方法。<strong class="lb iu"> <em class="nb">一致性</em> </strong>和<strong class="lb iu"> <em class="nb">表现力</em> </strong>是对大多数应用都很重要的两个属性。E <em class="nb">表达能力是代表一系列学习过程的能力，它衡量一系列领域的可扩展性和适用性</em>。<em class="nb">一致性意味着学习的学习程序将解决具有足够数据的任务，而不管该任务的性质如何</em>。例如，梯度下降方法对应于一致的学习过程，因为如果我们只是在测试时运行梯度下降，你可以期望给定足够的测试任务数据，无论你的元训练数据是什么，你都能够解决一个任务。<em class="nb">一致性将减少对元训练任务的依赖，从而带来良好的分布外(ood)性能</em>。</p><p id="bdcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nb">对于监督学习，非常深度的模型通常是富有表现力的</em> </strong>(这对于一些RL算法不成立)。因此，黑盒方法具有完全的表达能力，而基于优化的方法对于非常深的模型具有表达能力，而非参数方法对于大多数架构具有表达能力。就一致性而言，<strong class="lb iu">黑盒根本不一致</strong>，而基于o <strong class="lb iu">优化的方法是一致的</strong>，因为它简化为梯度下降问题。<strong class="lb iu">非参数方法在这个意义上是一致的</strong> <em class="nb">如果你的嵌入没有丢失关于输入的信息，这对于做决策是很重要的，</em> <em class="nb">那么随着你积累越来越多的数据，它最终会渐近地到达一个任意接近你的测试数据点的数据点，然后能够对那个测试数据点做出正确的预测</em>。</p><p id="a055" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些方法还有其他一些优点(缺点)。<strong class="lb iu">由于黑盒方法容易与各种学习问题</strong>结合，如监督学习和RL；但是<strong class="lb iu">它通常是数据低效的</strong>，因为你需要从头开始训练一个神经网络，并且优化是具有挑战性的<strong class="lb iu">，因为在初始化<strong class="lb iu">时没有归纳偏差。</strong> <strong class="lb iu">基于优化的方法在肉学习</strong>开始时具有正归纳偏差，并且它<strong class="lb iu">很好地处理变化的k和大k。</strong>例如，如果你的数据比你训练的数据多，这些方法仍然可以很好地工作，因为它们是一致的。它是与模型无关的。</strong>但它包括<strong class="lb iu">二阶优化，</strong>这导致这种方法<strong class="lb iu">计算和内存密集型</strong>。非参数方法<strong class="lb iu">完全是前馈</strong>，它们不会在计算图中演化出任何反向传播，因此，它们往往在计算上非常<strong class="lb iu">快速</strong>并且<strong class="lb iu">易于优化。但是，如果你在比它们被训练的更多的k上测试它们，它们的表现往往不如其他算法能够达到的。因此，它们很难推广到变化的K值</strong>和<strong class="lb iu">很难扩展到非常大的K值</strong>。况且，到目前为止，他们<strong class="lb iu">仅限于分类</strong>。</p><p id="20db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一般来说，在现有的少数几个基准测试中，每个版本的性能都相当。<strong class="lb iu">使用哪种方法取决于您的用例</strong>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ef87" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考</h1><ol class=""><li id="31a2" class="nm nn it lb b lc mu lf mv li no lm np lq nq lu nr ns nt nu bi translated"><a class="ae ki" href="https://www.youtube.com/watch?v=bc-6tzTyYcM&amp;list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&amp;index=4" rel="noopener ugc nofollow" target="_blank">斯坦福CS330:多任务和元学习，2019 |讲座4——非参数元学习者</a></li><li id="af3b" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">T <a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_lecture4.pdf" rel="noopener ugc nofollow" target="_blank">何航向滑梯</a></li><li id="7749" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ki" href="https://arxiv.org/abs/1801.03924" rel="noopener ugc nofollow" target="_blank">深度特征作为感知度量的不合理有效性</a></li><li id="2273" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ki" href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" rel="noopener ugc nofollow" target="_blank">用于一次性图像识别的连体神经网络</a></li><li id="475b" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ki" href="https://arxiv.org/abs/1606.04080" rel="noopener ugc nofollow" target="_blank">用于一次性学习的匹配网络</a></li><li id="c0e2" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ki" href="https://arxiv.org/abs/1703.05175" rel="noopener ugc nofollow" target="_blank">少量学习的原型网络</a></li><li id="2c43" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ki" href="https://arxiv.org/pdf/1902.04552" rel="noopener ugc nofollow" target="_blank">用于少量学习的无限混合原型— arXiv </a></li><li id="8eca" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ki" href="https://arxiv.org/abs/1711.04043" rel="noopener ugc nofollow" target="_blank">用图形神经网络进行少量学习</a></li><li id="5495" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ki" href="https://arxiv.org/abs/1711.06025" rel="noopener ugc nofollow" target="_blank">学习比较:少投学习的关系网络</a></li><li id="68e6" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ki" href="https://arxiv.org/pdf/1806.02817" rel="noopener ugc nofollow" target="_blank">概率模型不可知元学习— arXiv </a></li><li id="0dc9" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><a class="ae ki" href="https://arxiv.org/abs/1807.05960" rel="noopener ugc nofollow" target="_blank">具有潜在嵌入优化的元学习</a></li></ol></div></div>    
</body>
</html>