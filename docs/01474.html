<html>
<head>
<title>Fine-tune Albert with Pre-training on Custom Corpus — Some notes and hiccups from real application training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用定制语料库的预训练来微调Albert来自真实应用程序训练的一些笔记和打嗝</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tune-albert-with-pre-training-on-custom-corpus-some-notes-and-hiccups-from-real-application-24e463d6ef01?source=collection_archive---------30-----------------------#2020-02-09">https://towardsdatascience.com/fine-tune-albert-with-pre-training-on-custom-corpus-some-notes-and-hiccups-from-real-application-24e463d6ef01?source=collection_archive---------30-----------------------#2020-02-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/89a59b7af892ed3c5cf5750cc0b1bf1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*sxoLYozt45uRGjtG8xYOgA.jpeg"/></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">图片来自互联网海底捞汽船</p></figure><p id="1360" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是我上一篇<a class="ae kw" rel="noopener" target="_blank" href="/fine-tune-albert-with-pre-training-on-custom-corpus-f56ea3cfdc82">文章</a>的后续文章，这篇文章展示了使用玩具数据集对Albert进行预训练的详细步骤。在这篇文章中，我将分享我自己在实际应用中应用这个模型的经验。我的应用程序是一个象征性的分类，Albert在包含2亿多句子的外语(非英语)的chats语料库上进行了预训练。</p><h1 id="4a4d" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">预培训</h1><p id="5bdc" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated"><strong class="ka ir">创建训练前文件</strong></p><p id="79b3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您需要遵循Albert的“创建预训练文件”脚本所要求的输入格式，即不同文档之间一行，文档中不同句子之间一行。</p><p id="9d7c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您直接在200+M的句子上运行创建预训练文件的脚本，将会花费很长时间。因此，我把文集分成200多个文件，每个文件有1百万行。对于每个1M行的文件，在CPU上运行大约需要2个小时，最终创建的文件大约需要4.5G空间。我在分布式机器上使用并行运行的批处理作业来创建所有文件(我公司现有的基础设施)</p><p id="3603" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">预训练设置</strong></p><p id="c7f3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我使用谷歌的免费300信用点进行预训练。继<a class="ae kw" href="https://cloud.google.com/tpu/docs/quickstart" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/tpu/docs/quickstart</a>之后，我设置了一辆v2–8 TPU。为了避免OOM，我将批量减少到512(默认为4096)。我已经将预训练的所有数据放入vocab文件，并将生成的预训练模型检查点也存储在GCS中。我已经将训练输出控制台保存到日志文件中，以便稍后我可以<a class="ae kw" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/misc/albert_pretrain_log_proc.ipynb" rel="noopener ugc nofollow" target="_blank">从日志文件中提取训练损失。</a></p><p id="69ff" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">预训练参数</strong></p><p id="ad1b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于聊天句子往往很短，因此我将最大长度设置为64(默认为512)。因为我已经减少了批量大小，所以我相应地线性增加了训练步骤(到1Mil，缺省值是125000)和预热步骤(到25000，缺省值是3125)。我已经线性地降低了学习率(到0.00022，缺省值是0.00176，虽然学习率和批量大小不是线性关系，但是0.00176这个很好的缺省值正好可以除以8，所以我就这样设置了，:P，但是反正对我来说是有效的)。我将save_checkpoints_steps设置为较小的最小值1000，希望减少内存使用。</p><p id="fcdf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">初始化</strong></p><p id="b4e3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我没有对Albert模型进行随机初始化，而是使用tf-hub的预训练Albert v2 base(英语，并且在撰写本文时，只有英语预训练模型可正式用于Albert)权重进行初始化。一些打嗝是:</p><ol class=""><li id="32a1" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">来自tf-hub的模型没有adam_m和adam_v变量，我们必须手动添加它们</li><li id="e9f4" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">tf-hub模型中的变量以前缀“module/”存储，我们必须删除它才能使用Albert repo的训练前脚本</li><li id="a30e" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">我们还需要手动添加glonal_step变量(要求是int64类型)</li></ol><p id="6b37" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">参考这个<a class="ae kw" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/misc/albert_ckpt_convert.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><p id="2b96" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">预训练结果</strong></p><p id="e8cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">训练损失从大约4+开始，在300k训练步数后稳定到大约1.8，此后在该范围内波动，直到700k步数，因此我在700k步数时提前停止(为了节省一些积分:P)。我尝试了另一个更低的学习率(0.00005)，损失减少到2.7左右，没有进一步发展。这确实因情况而异，我在网上看到一些训练损失稳定在0.3范围，而其他稳定在&gt; 3。</p><p id="9182" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">完成70万步需要大约2-3天的时间，几乎用光了300美元的积分。</p><h1 id="69d3" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">微调</h1><p id="2cff" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我在之前的<a class="ae kw" rel="noopener" target="_blank" href="/fine-tune-albert-with-pre-training-on-custom-corpus-f56ea3cfdc82">帖子</a>中已经提供了两个微调笔记本，一个用于tensorflow，一个用于pytorch(使用huggingface团队的变形金刚回购)，实际应用中使用的是pytorch版本。</p><p id="ffa2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">微调设置</strong></p><p id="fbfb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我用了8个CPU，30 GB内存+ 2个英伟达特斯拉T4 GPU。</p><p id="23e7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">微调参数</strong></p><p id="bd91" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我的标记分类微调任务(二元分类，要么标记是期望短语的一部分，要么不是)中，我用500k+的训练样本进行了训练，其中有50%的负样本(即对于负样本，整句中没有期望的标记，类比于餐馆评论玩具样本是，评论句子中没有菜名)。用5个epoches训练，学习率为0.000001，批量大小为32，具有完全微调的选项，即更新艾伯特模型以及最终微调FCL，使用来自所有样本的所有表征的平均交叉熵损失。</p><p id="dc74" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">微调结果</strong></p><p id="041a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我能够达到大约94%的验证准确率(所有样本中准确分类的令牌总数)，同样，这个数字没有意义，并且确实因情况而异。但总的来说，它对我的应用程序是有用的。</p></div></div>    
</body>
</html>