<html>
<head>
<title>TF-IDF Explained And Python Sklearn Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TF-IDF讲解和Python Sklearn实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275?source=collection_archive---------1-----------------------#2020-06-08">https://towardsdatascience.com/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275?source=collection_archive---------1-----------------------#2020-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="ff72" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">什么是TF-IDF，如何用Python和Scikit实现它-Learn。</em></p><p id="4de3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> TF-IDF </strong>是一个信息检索和信息抽取子任务，旨在表达一个词对一个文档的重要性，该文档是我们通常称为语料库的文档集合的一部分。它通常被一些搜索引擎用来帮助他们获得与特定查询更相关的更好的结果。在本文中，我们将讨论什么是TF-IDF，解释它背后的数学原理，然后我们将看到如何使用Scikit-Learn库在Python中实现它。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/e0d16683f5c2a578557e7bc8833ee0e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*t0jNKvVmznvOLvIc"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">照片由<a class="ae lf" href="https://unsplash.com/@prolabprints?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">罗恩·迪亚</a>在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="c5f8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">本文原载于</em> <a class="ae lf" href="https://programmerbackpack.com/tf-idf-explained-and-python-implementation/" rel="noopener ugc nofollow" target="_blank"> <em class="ko">程序员背包博客</em> </a> <em class="ko">。如果你想阅读更多这类的故事，一定要访问这个博客。</em></p><p id="2963" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">非常感谢您阅读本文！对更多这样的故事感兴趣？在Twitter上关注我，地址是<a class="ae lf" href="https://twitter.com/b_dmarius" rel="noopener ugc nofollow" target="_blank"><em class="ko">@ b _ dmarius</em></a><em class="ko">，我会在那里发布每一篇新文章。</em></p><h1 id="798f" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">文章概述</h1><ul class=""><li id="7fd7" class="me mf it js b jt mg jx mh kb mi kf mj kj mk kn ml mm mn mo bi translated">什么是TF-IDF</li><li id="af97" class="me mf it js b jt mp jx mq kb mr kf ms kj mt kn ml mm mn mo bi translated">TF-IDF公式解释</li><li id="02cf" class="me mf it js b jt mp jx mq kb mr kf ms kj mt kn ml mm mn mo bi translated">TF-IDF sklearn python实现</li><li id="c7e6" class="me mf it js b jt mp jx mq kb mr kf ms kj mt kn ml mm mn mo bi translated">TfIdfVectorizer与TfIdfTransformer —有何不同</li><li id="a066" class="me mf it js b jt mp jx mq kb mr kf ms kj mt kn ml mm mn mo bi translated">TF-IDF应用</li><li id="856e" class="me mf it js b jt mp jx mq kb mr kf ms kj mt kn ml mm mn mo bi translated">结论</li></ul><h1 id="10a1" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">什么是TF-IDF</h1><p id="3af1" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated"><strong class="js iu"> TF-IDF </strong>代表词频——逆文档频率，是一种统计数据，旨在更好地定义一个单词对一个文档的重要性，同时也考虑到与同一语料库中其他文档的关系。</p><p id="b207" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是通过查看一个单词在一个文档中出现了多少次，同时也注意相同的单词在语料库中的其他文档中出现了多少次来执行的。</p><p id="be38" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这背后的基本原理如下:</p><ul class=""><li id="538a" class="me mf it js b jt ju jx jy kb mx kf my kj mz kn ml mm mn mo bi translated">在文档中频繁出现的单词与该文档的相关性更高，这意味着该文档与该特定单词相关的概率更高</li><li id="b952" class="me mf it js b jt mp jx mq kb mr kf ms kj mt kn ml mm mn mo bi translated">在更多文档中频繁出现的单词可能会阻止我们在集合中找到正确的文档；这个词要么适用于所有文档，要么不适用。无论哪种方式，它都不能帮助我们从整个集合中过滤出单个文档或文档的一个子集。</li></ul><p id="201e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">那么<strong class="js iu"> TF-IDF是一个分数</strong>，它应用于我们数据集中每个文档中的每个单词。并且对于每个单词，TF-IDF值随着该单词在文档中的每次出现而增加，但是随着在其他文档中的每次出现而逐渐减少。数学会在下一节讲到。</p><h1 id="4f54" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">TF-IDF公式解释</h1><p id="a442" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">现在让我们看看TF-IDF统计指标背后的简单公式。首先让我们定义一些符号:</p><ul class=""><li id="32be" class="me mf it js b jt ju jx jy kb mx kf my kj mz kn ml mm mn mo bi translated"><em class="ko"> N </em>是我们数据集中的文档数量</li><li id="edee" class="me mf it js b jt mp jx mq kb mr kf ms kj mt kn ml mm mn mo bi translated"><em class="ko"> d </em>是我们数据集中的一个给定文档</li><li id="ab2f" class="me mf it js b jt mp jx mq kb mr kf ms kj mt kn ml mm mn mo bi translated"><em class="ko"> D </em>是所有文件的集合</li><li id="3ac9" class="me mf it js b jt mp jx mq kb mr kf ms kj mt kn ml mm mn mo bi translated">w是文档中的一个给定单词</li></ul><p id="3525" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一步是计算词频，我们的第一个衡量指标是得分。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi na"><img src="../Images/3f9a4672ce0d903194c213f05c4373d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*T-K5vPy1IROvrQe-Wh_CLA.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">词频公式</p></figure><p id="aedc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里<em class="ko"> f(w，d) </em>是<em class="ko"> w </em>在<em class="ko"> d. </em>文档中的出现频率</p><p id="d59f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第二步是计算逆项频率。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/ab6cc42ced37a8ec6f542e036c3a1e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/0*YcSQWBsLk9t5FaVJ.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">逆文档频率公式</p></figure><p id="d8e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于数据集中的<em class="ko"> N </em>个文档和整个数据集中的<em class="ko"> f(w，D) </em>单词<em class="ko"> w </em>的出现频率，这个数字会随着单词在整个数据集中的出现次数的增加而降低。</p><p id="cf75" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后一步是通过以下公式计算TF-IDF得分:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/bd62bf37e56acaecc8b748bbab966b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/0*rs8otIUS1Vu3nOMY.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">词频—逆文档频率—公式</p></figure><h1 id="c4f0" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">TF-IDF Sklearn Python实现</h1><p id="4d79" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">有了像<a class="ae lf" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>这样令人敬畏的库，实施TD-IDF就轻而易举了。首先，我们需要为我们的项目安装2个依赖项，所以现在就开始吧。</p><pre class="kq kr ks kt gt nd ne nf ng aw nh bi"><span id="7a2c" class="ni lh it ne b gy nj nk l nl nm">pip3 install scikit-learn<br/>pip3 install pandas</span></pre><p id="31c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了看到TF-IDF的全部威力，我们实际上需要一个适当的、更大的数据集。但是出于本文的目的，我们只想关注实现，所以让我们将依赖项导入到我们的项目中，并构建我们的迷你数据集。</p><pre class="kq kr ks kt gt nd ne nf ng aw nh bi"><span id="dfe1" class="ni lh it ne b gy nj nk l nl nm">import pandas as pd<br/>from sklearn.feature_extraction.text import TfidfTransformer</span><span id="e159" class="ni lh it ne b gy nn nk l nl nm">dataset = [<br/>    "I enjoy reading about Machine Learning and Machine Learning is my PhD subject",<br/>    "I would enjoy a walk in the park",<br/>    "I was reading in the library"<br/>]</span></pre><p id="1752" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们计算TF-IDF分数并打印出结果。</p><pre class="kq kr ks kt gt nd ne nf ng aw nh bi"><span id="e906" class="ni lh it ne b gy nj nk l nl nm">tfIdfVectorizer=TfidfVectorizer(use_idf=True)<br/>tfIdf = tfIdfVectorizer.fit_transform(dataset)<br/>df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=["TF-IDF"])<br/>df = df.sort_values('TF-IDF', ascending=False)<br/>print (df.head(25))</span></pre><p id="41c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们看看我们的结果。</p><pre class="kq kr ks kt gt nd ne nf ng aw nh bi"><span id="2aad" class="ni lh it ne b gy nj nk l nl nm">TF-IDF<br/>machine   0.513720<br/>learning  0.513720<br/>about     0.256860<br/>subject   0.256860<br/>phd       0.256860<br/>and       0.256860<br/>my        0.256860<br/>is        0.256860<br/>reading   0.195349<br/>enjoy     0.195349<br/>library   0.000000<br/>park      0.000000<br/>in        0.000000<br/>the       0.000000<br/>walk      0.000000<br/>was       0.000000<br/>would     0.000000</span></pre><h1 id="0d23" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">TfidfVectorizer与TfidfTransformer —有何不同</h1><p id="1941" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">如果您曾经看过TF-IDF的其他实现，您可能已经看到使用Scikit-Learn实现TF-IDF有两种不同的方式。一种是使用TfidfVectorizer类(就像我们刚才做的那样)，另一种是使用TfidfTransformer类。你可能想知道它们之间有什么不同，所以让我们来讨论一下。</p><p id="0942" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从理论上讲，这两种实现实际上没有区别。实际上，如果我们想使用TfidfTransformer，我们需要编写更多的代码。这两种实现的主要区别在于，TfidfVectorizer为您执行词频和逆文档频率，而使用TfidfTransformer需要您使用Scikit-Learn中的CountVectorizer类来执行词频。</p><p id="3366" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，让我们来看一个替代的TF-IDF实现，并验证结果是否相同。我们首先需要导入2个额外的依赖项到我们的项目中。</p><pre class="kq kr ks kt gt nd ne nf ng aw nh bi"><span id="69b8" class="ni lh it ne b gy nj nk l nl nm">from sklearn.feature_extraction.text import TfidfTransformer<br/>from sklearn.feature_extraction.text import CountVectorizer</span></pre><p id="850f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用与其他实现相同的小型数据集。让我们编写替代实现并打印出结果。</p><pre class="kq kr ks kt gt nd ne nf ng aw nh bi"><span id="25e3" class="ni lh it ne b gy nj nk l nl nm">tfIdfTransformer = TfidfTransformer(use_idf=True)<br/>countVectorizer = CountVectorizer()<br/>wordCount = countVectorizer.fit_transform(dataset)<br/>newTfIdf = tfIdfTransformer.fit_transform(wordCount)<br/>df = pd.DataFrame(newTfIdf[0].T.todense(), index=countVectorizer.get_feature_names(), columns=["TF-IDF"])<br/>df = df.sort_values('TF-IDF', ascending=False)<br/>print (df.head(25))</span></pre><p id="47e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以看看结果，看到和上面的一样。</p><pre class="kq kr ks kt gt nd ne nf ng aw nh bi"><span id="bd6c" class="ni lh it ne b gy nj nk l nl nm">TF-IDF<br/>machine   0.513720<br/>learning  0.513720<br/>about     0.256860<br/>subject   0.256860<br/>phd       0.256860<br/>and       0.256860<br/>my        0.256860<br/>is        0.256860<br/>reading   0.195349<br/>enjoy     0.195349<br/>library   0.000000<br/>park      0.000000<br/>in        0.000000<br/>the       0.000000<br/>walk      0.000000<br/>was       0.000000<br/>would     0.000000</span></pre><h1 id="494d" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">TF-IDF应用</h1><p id="a979" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">我们已经看到，使用正确的工具实现TF-IDF非常容易，但该算法的应用程序非常强大。TF-IDF最常见的两种使用情形是:</p><ul class=""><li id="b558" class="me mf it js b jt ju jx jy kb mx kf my kj mz kn ml mm mn mo bi translated"><strong class="js iu">信息检索:</strong>通过计算用户查询相对于整个文档集的TF-IDF得分，我们可以计算出一个文档与给定查询的相关程度。有传言说，大多数搜索引擎都使用某种TF-IDF实现，但我自己无法证实这一信息，所以对此要持保留态度。</li><li id="7997" class="me mf it js b jt mp jx mq kb mr kf ms kj mt kn ml mm mn mo bi translated"><a class="ae lf" href="https://programmerbackpack.com/machine-learning-project-series-part-2-python-keywords-extraction/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">关键词提取</strong> </a>:根据TF-IDF得分，文档中排名最高的词可以很好地代表该文档的关键词(因为它们使该文档从其他文档中脱颖而出)。因此，我们可以很容易地使用某种TF-IDF分数计算来从文本中提取关键字。</li></ul><h1 id="0485" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">结论</h1><p id="db70" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">因此，在本文中，我们看到了对什么是TF-IDF以及我们如何从数学上解释它的解释。然后，我们看到了使用Scikit-Learn Python库的两个替代实现。然后我们讨论了这个算法的一些可能的应用。我希望你喜欢这个！</p><p id="3ddb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">非常感谢您阅读这篇文章！有兴趣了解更多吗？在Twitter上关注我，地址是<a class="ae lf" href="https://twitter.com/b_dmarius" rel="noopener ugc nofollow" target="_blank"><em class="ko">@ b _ dmarius</em></a><em class="ko">，我会在那里发布每一篇新文章。</em></p></div></div>    
</body>
</html>