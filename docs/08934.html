<html>
<head>
<title>Building A Mental Model for Backpropagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">建立反向传播的心智模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-mental-model-for-backpropagation-987ac74d1821?source=collection_archive---------27-----------------------#2020-06-27">https://towardsdatascience.com/building-a-mental-model-for-backpropagation-987ac74d1821?source=collection_archive---------27-----------------------#2020-06-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4bf3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过用 Python 实现自己的深度学习框架</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4b36d127e8070005bd1be137a453b2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*u9z-qlAIRrQ4n5PW"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安德烈·韦莱在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="4f04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">作为深度学习的跳动的心脏，任何深度学习实践者都需要对反向传播有坚实的理解。虽然互联网上已经有很多解释反向传播的好资源，但大多数都是从非常不同的角度来解释的，并且每个都适合特定类型的受众。在这篇文章中，我将把直觉、动画图和代码结合在一起，供深度学习的初学者和中级水平的学生更容易使用。对任何算法的理解的一个好的评估是你是否能从头开始自己编码。看完这篇文章后，你应该知道如何用 Python 实现你自己版本的反向传播。</span></p><h1 id="adef" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">实值电路和校正力</h1><p id="e15b" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">从数学上讲，反向传播是通过应用<strong class="lb iu">链式法则</strong>来计算函数分量梯度的过程。在神经网络的情况下，感兴趣的函数是<strong class="lb iu">损失函数</strong>。我喜欢 Andrej Karpathy 在 CS231n 中的解释:把计算图看成是带有逻辑门的<strong class="lb iu">实值电路</strong>。门是函数中的运算，例如加、乘、取幂、矩阵乘法等。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/21c5915631a5d1b08616ba85eaf1ab04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pmkFkt7NRdg57k1J8rFOcQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://cs231n.github.io/optimization-2/" rel="noopener ugc nofollow" target="_blank">https://cs231n.github.io/optimization-2/</a></p></figure><p id="88b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个很好的心理模型，因为它意味着反向传播是一个局部过程。电路中的每个门都可以计算其输出和局部梯度，而无需了解全局。</p><p id="0602" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在反向传递(反向传播)期间，门应用链规则，即，在电路的最终输出上取其输出的梯度，并将其乘以关于其所有输入的局部梯度。可以使用<strong class="lb iu">递归方法</strong>从电路的输出返回到所有输入来实现反向传递。</p><p id="ceda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直观上，反向传播及其相关权重更新的最终效果是，电路<strong class="lb iu">“想要】</strong>输出一个更接近我们所拥有的任何目标值的值。以上图中加法门(-4)的梯度为例，意思是<strong class="lb iu">将 q 改变+1ε会导致 f </strong>中-4ε的变化。如果我们想要更高的 f 值，我们可以把 q 值降低。这就是梯度的本质。人们有时称之为“敏感”。另一个很好的类比是<strong class="lb iu">修正力</strong>。梯度的符号表示校正的<strong class="lb iu">方向</strong>，大小表示<strong class="lb iu">强度</strong>。</p><h1 id="a6e7" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">从函数到计算图</h1><p id="b21d" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">可视化反向投影的最好方法之一是绘制函数的计算图。让我们看看下面这个奇怪的函数，演示如何绘制它的计算图，然后手动反向投影。(<em class="nc"> σ()是 sigmoid 函数</em>)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/5b7a92266d2cd342f0a96326af5923fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*MmniJZBPLOW1mGrpMc0x6A.png"/></div></figure><p id="8f26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了计算其梯度，我们可以将其分解为加法、s 形、正方形门，如下面的动画步骤所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/98949d48978a938efbbe66020c23d3c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*CQ4qD5BY8GXW5RKrv4gNPQ.gif"/></div></div></figure><p id="51a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，该流程包括 3 个高级步骤</p><ol class=""><li id="940e" class="nf ng it lb b lc ld lf lg li nh lm ni lq nj lu nk nl nm nn bi translated">从操作(门)构建计算图</li><li id="0a12" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">在每个操作中运行正向传递</li><li id="6b93" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">基于(1)正向过程中计算的值和(2)每个门的反向函数运行反向过程，以计算其局部梯度</li></ol><p id="88af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以跟随并手动计算这些值。我将在最后一节展示如何实现它，但是现在让我们来看一个技巧，它将帮助我们简化这个过程。</p><h1 id="f8f2" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">分阶段计算</h1><p id="967b" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">任何一种可微函数都可以充当一个门，<strong class="lb iu">我们可以在方便的时候将多个门组合成一个门</strong>。</p><p id="58b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们不应该明确地解析求解梯度，所以这些函数分量的选择就成了一个需要考虑的问题。以 sigmoid 函数为例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ebf676c0130772d7bc3c18dca4798781.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*OeSnmmT_L9IIzswHjzE7IA.png"/></div></figure><p id="844c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将其分解为加法、乘法、求反、取幂和倒数门，如下图所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/876848b50d23a8413704ea252bf6c889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tWqWTMqcN66qQjQ9WMVtHQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">资料来源:https://cs231n.github.io/optimization-2/</p></figure><p id="eb64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个简单的乙状结肠已经有这么多的运算和梯度，这似乎是不必要的复杂。我们可以做的另一件事就是<strong class="lb iu">将一个 sigmoid gate </strong>与计算其梯度的函数一起应用于红框的输出。乙状结肠的坡度非常简单:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c0e1563b0499f1fa9c9c67329013e0a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*XzlkvyaVx_Ec-E61BfS5YQ.png"/></div></figure><p id="59d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样我们避免了许多不必要的计算。它节省了我们的时间、空间和能量，使代码更加模块化和易于阅读，并避免了数值问题。</p><p id="8336" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">乙状结肠门的代码可能类似于:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="e7a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它既有向前传递的输出，又有计算向后传递的局部梯度的功能，只需几行代码。在下一节中，我将把它放到更大的图片中，并展示如何用这样的组件编写一个迷你的<strong class="lb iu">亲笔签名的</strong>库。</p><h1 id="bbfc" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">亲笔签名:递归方法</h1><p id="9f37" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">要让计算机使用链规则计算任何用有向无环图(DAG)表示的函数的梯度，我们需要为前面提到的 3 个高级步骤编写代码。这样的程序通常被称为自动分化或自动签名。正如您接下来将看到的，我们可以将代码组织成一个定义数据和操作的<code class="fe ny nz oa ob b">Tensor</code>类，这样它不仅可以支持动态构建动态计算图，还可以递归地反向传播。</p><h1 id="ba0d" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">向前传递:构建图表</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卡帕西显微图中的代码</p></figure><p id="ac93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个张量对象有<code class="fe ny nz oa ob b">data</code>、<code class="fe ny nz oa ob b">grad</code>、一个<code class="fe ny nz oa ob b">_backward()</code>方法、一组<code class="fe ny nz oa ob b">_prev</code>张量节点和一个<code class="fe ny nz oa ob b">_op</code>操作。当我们执行一个表达式时，它会动态构建计算图，因为我们已经用定制的<em class="nc"> dunder 方法</em>覆盖了 Python 操作符，如<code class="fe ny nz oa ob b">+</code>、<code class="fe ny nz oa ob b">*</code>和<code class="fe ny nz oa ob b">**</code>。当前张量的<code class="fe ny nz oa ob b">_backward()</code>、<code class="fe ny nz oa ob b">_prev</code>和<code class="fe ny nz oa ob b">_op</code>由其父张量定义，即产生它的张量。例如<code class="fe ny nz oa ob b">c = a + b</code>中的<code class="fe ny nz oa ob b">c</code>有<code class="fe ny nz oa ob b">__add__</code>中定义的<code class="fe ny nz oa ob b">_backward()</code>，还有<code class="fe ny nz oa ob b">_prev = {a, b}</code>、<code class="fe ny nz oa ob b">_op = '+'</code>。这样我们就可以定义任何我们想要的操作，并让 Python 来构造图形。</p><p id="a36a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里说的是神经网络，所以我们关心的表达式是<strong class="lb iu">损失函数</strong>。以 MSE 损失为例(为简单起见，使用 1 个标量数据点)，<code class="fe ny nz oa ob b">MSELoss = (w*x - y)**2</code>其中<code class="fe ny nz oa ob b">w</code>、<code class="fe ny nz oa ob b">x</code>和<code class="fe ny nz oa ob b">y</code>为张量对象，分别初始化为 3、-4 和 2。然后，图形会自动构建为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/e07ee3a5f8f3095e15b67c72ba3f5b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kTN60q5fNhRu6_HLxFyHfQ.png"/></div></div></figure><p id="bee0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意减法其实就是否定和加法。我命名中间节点只是为了便于说明。有了图形，我们就可以实现反向投影了！</p><h1 id="12f4" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">向后传递:拓扑排序</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卡帕西显微图中的代码</p></figure><p id="397f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ny nz oa ob b">backward</code>应该从当前张量节点开始逐个计算梯度，并移动到其祖先。遍历的顺序需要进行拓扑排序，以确保每一步都计算了依赖关系。实现这种拓扑排序的一个简单方法是深度优先搜索。这里有一个动画来展示如何在 MSELoss 示例中执行它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/20f96ee42d19ccaa66a6192b2e06735b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*oSGPHsuUCXg3jisK2aslIg.gif"/></div></div></figure><p id="aef7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是图遍历 101:一个普通的老 DFS。如果你有一个大型复杂的神经网络，你只需用你的大图替换左上角的<code class="fe ny nz oa ob b">wx</code>，并且假设所有的操作和它们的<code class="fe ny nz oa ob b">_backward</code>都在<code class="fe ny nz oa ob b">Tensor</code>类中定义，那么 DFS 将会去那里计算梯度。这里我们需要的一些显而易见的操作包括<code class="fe ny nz oa ob b">max</code>、<code class="fe ny nz oa ob b">sum</code>、矩阵乘法、转置等。要使用渐变来更新权重，请执行以下操作:</p><pre class="kj kk kl km gt od ob oe of aw og bi"><span id="49ac" class="oh mf it ob b gy oi oj l ok ol">for p in &lt;your_parameters&gt;:<br/>    p.data -= learning_rate * p.grad</span></pre><p id="82a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就这样，几行代码中的亲笔签名算法。它是任何深度学习框架的支柱。现在，困难的部分已经过去了，要完成您的 mini 深度学习框架的实现，您只需要实现一个模型接口，其中包含层类型、损失函数和一些优化器的集合。</p><h1 id="86ef" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">推荐读物</h1><p id="7524" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">这篇文章的灵感很大程度上来源于 Andrej Karpathy 令人敬畏的<a class="ae ky" href="https://cs231n.github.io/" rel="noopener ugc nofollow" target="_blank"> CS231n 讲座</a>和漂亮的文字<a class="ae ky" href="https://github.com/karpathy/micrograd" rel="noopener ugc nofollow" target="_blank">微图:最小的亲笔签名引擎</a>。如果你想看看一个 DIY 深度学习框架的不同和更扩展的版本，它非常类似于 PyTorch，请查看 Andrew Trask 在<a class="ae ky" href="https://livebook.manning.com/book/grokking-deep-learning/chapter-13/1" rel="noopener ugc nofollow" target="_blank"> Grokking Deep Learning 中实现的那个。如果你更喜欢直接阅读 PyTorch 的亲笔签名，埃利奥特·韦特在 Youtube 上有一个</a><a class="ae ky" href="https://youtu.be/MswxJw-8PvE" rel="noopener ugc nofollow" target="_blank">很棒的视频</a>，它将为你节省大量钻研源代码的时间。坚持学习！</p><h1 id="6e2d" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">参考</h1><ul class=""><li id="ddc7" class="nf ng it lb b lc mw lf mx li om lm on lq oo lu op nl nm nn bi translated"><a class="ae ky" href="https://cs231n.github.io/" rel="noopener ugc nofollow" target="_blank"> CS231n 讲座</a></li><li id="faf7" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu op nl nm nn bi translated"><a class="ae ky" href="https://github.com/karpathy/micrograd" rel="noopener ugc nofollow" target="_blank">微克:最小的亲笔签名引擎</a></li></ul></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><p id="269e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nc">看我在</em> <a class="ae ky" href="https://medium.com/@loganyang" rel="noopener"> <em class="nc">上的其他帖子</em> </a> <em class="nc">，或者关注我在</em> <a class="ae ky" href="https://twitter.com/logancyang" rel="noopener ugc nofollow" target="_blank"> <em class="nc">上的推特</em> </a> <em class="nc">。</em></p></div></div>    
</body>
</html>