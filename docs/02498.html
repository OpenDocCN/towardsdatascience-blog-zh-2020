<html>
<head>
<title>Transposed Convolution Demystified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">转置卷积去神秘化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba?source=collection_archive---------0-----------------------#2020-03-10">https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba?source=collection_archive---------0-----------------------#2020-03-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="83bb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">转置卷积对于图像分割、超分辨率等应用来说是一个革命性的概念，但有时它会变得有点难以理解。在这篇文章中，我将试图揭开这个概念的神秘面纱，让它更容易理解。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/3bb11fcab18d698fe6ab5e6b1a057cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/0*ilAQO9B3hm5waqqq"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><a class="ae la" href="https://imgflip.com/memegenerator/7296870/Confused-Baby" rel="noopener ugc nofollow" target="_blank">(来源</a>)</p></figure><h1 id="7a2b" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">介绍</h1><p id="a355" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">自从卷积神经网络(CNN)流行以来，计算机视觉领域正在经历一个过渡阶段。这场革命始于Alexnet在2012年赢得ImageNet挑战赛，自那以来，CNN一直统治着图像分类、对象检测、图像分割和许多其他图像/视频相关任务的领域。</p><p id="cebc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随着我们深入网络，卷积运算降低了空间维度，并创建了输入图像的抽象表示。CNN的这一特性对于图像分类等任务非常有用，在这些任务中，您只需预测特定对象是否出现在输入图像中。但是该特征可能会给诸如目标定位、分割之类的任务带来问题，其中原始图像中的目标的空间维度是预测输出边界框或分割目标所必需的。</p><p id="cf4c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了解决这个问题，使用了各种技术，例如全卷积神经网络，其中我们使用“相同”填充来保留输入维度。虽然这种技术在很大程度上解决了这个问题，但是它也增加了计算成本，因为现在卷积运算必须应用于整个网络的原始输入维度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi me"><img src="../Images/f5e506625b110514bb60844c49206c63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jS2M_DNV6Z2YkhJ1.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图一。</strong>全卷积神经网络(<a class="ae la" href="https://arxiv.org/abs/1411.4038" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="66e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">用于图像分割的另一种方法是将网络分成两部分，即下采样网络和上采样网络。<br/>在下采样网络中，使用简单的CNN架构，并产生输入图像的抽象表示。<br/>在上采样网络中，使用各种技术对抽象图像表示进行上采样，以使其空间维度等于输入图像。这种架构以编码器-解码器网络而闻名。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mk"><img src="../Images/d0ace26eae6b09384b29a7f431fbe111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*t-FynrY2FJnaExY_.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图二</strong>。用于图像分割的下采样和上采样网络(<a class="ae la" href="https://arxiv.org/abs/1505.04366" rel="noopener ugc nofollow" target="_blank">来源</a>)。</p></figure><h1 id="483c" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">上采样技术</h1><p id="99bb" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">下采样网络直观且众所周知，但很少讨论用于上采样的各种技术。</p><p id="41a3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">编码器-解码器网络中最广泛使用的上采样技术是<strong class="js iu"> : </strong></p><ol class=""><li id="22e3" class="ml mm it js b jt ju jx jy kb mn kf mo kj mp kn mq mr ms mt bi translated"><strong class="js iu">最近邻居</strong>:顾名思义，在最近邻居中，我们取一个输入像素值，并将其复制到K个最近邻居，其中K取决于预期输出。</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/0cedbb70cc955a06245a7245f862e207.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/0*0EJ025oepLbyi-Zd.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图三</strong>。最近邻向上采样</p></figure><p id="8f9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.<strong class="js iu">双线性插值:</strong>在双线性插值中，我们取输入像素的4个最近像素值，并根据4个最近像元的距离进行加权平均，平滑输出。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/6f3aa1f053959086661ea2d1fafc47bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/0*tWSnVE_JhDSZq8HQ"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图4。</strong>双线性插值</p></figure><p id="3fb4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.<strong class="js iu">钉床:</strong>在钉床中，我们复制输出图像中相应位置的输入像素值，并在其余位置填充零。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/bfa6699fc62bc705af1a1fba37177ecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*LJAl2rkIfFTDRIQanIbfRQ.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图5。</strong>甲床上采样</p></figure><p id="528c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.<strong class="js iu">Max-un Pooling:</strong>CNN中的Max-Pooling层取内核中所有值中的最大值。要执行max-unpooling，首先，在编码步骤中为每个max-pooling层保存最大值的索引。然后在解码步骤中使用保存的索引，其中输入像素被映射到保存的索引，在其他地方填充零。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mx"><img src="../Images/5c05112fdca8abd511b8814a1a196dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mog6cmBG4XzLa0IFbjZIaA.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图6。</strong>最大-取消上采样</p></figure><p id="680c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所有上述技术都是预定义的，并且不依赖于数据，这使得它们是特定于任务的。它们不从数据中学习，因此不是一种通用的技术。</p><h1 id="3896" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">转置卷积</h1><p id="3a8b" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">转置卷积用于使用一些可学习的参数将输入特征映射上采样为期望的输出特征映射。<br/>转置卷积的基本操作解释如下:<br/> 1。考虑一个2×2编码的特征图，它需要被上采样为3×3的特征图。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/5b135980b7092a82d6cb7d532de9217a.png" data-original-src="https://miro.medium.com/v2/resize:fit:194/format:webp/1*BMJnnOKPhK8hoFP6sQ9edQ.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图7。</strong>输入特征地图</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/5523edeb052baf7fecbca809f5608bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*VxtMdM-DsGwIa51GyDx-XQ.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图8。</strong>输出特征地图</p></figure><p id="32ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.我们取一个大小为2x2的核，单位步幅，零填充。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi na"><img src="../Images/63a4528218da012178b58fd8cfab8f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*e6UnrcsFRaOidCq7mwJpTA.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图九。</strong>大小为2x2的果仁</p></figure><p id="f516" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.现在我们取输入特征图的左上元素，并将其与内核的每个元素相乘，如图10所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/d3f055f5ca2e74755f3a8aea9df02947.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*7hVid7EAqCPkG6sEjHMI5w.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图10。</strong></p></figure><p id="29c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.类似地，我们对输入特征图的所有剩余元素都这样做，如图11所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nc"><img src="../Images/a785cb5675527d4dcf13441da9448b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yxBd_pCiEVVwEQFmc-Heog.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图11。</strong></p></figure><p id="7be6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">5.如您所见，上采样后的要素地图中的某些元素相互重叠。为了解决这个问题，我们简单地添加重叠位置的元素。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nd"><img src="../Images/fa0470c89da50050f4b94a79fe3a0df4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*faRskFzI7GtvNCLNeCN8cg.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图12。</strong>完整的转置卷积运算</p></figure><p id="86aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">6.最终输出将是最终的上采样特征地图，具有所需的3×3空间维度。</p><p id="d975" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">转置卷积也称为反卷积，这是不恰当的，因为反卷积意味着消除卷积的影响，这不是我们的目标。</p><p id="132b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它也被称为上采样卷积，对于它用来执行的任务是直观的，即上采样输入特征图。</p><p id="637b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它也被称为分数步长卷积due，因为输出的步长相当于输入的分数步长。例如，输出上的步长2是输入上的1/2。</p><p id="ec52" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，它也被称为后向步长卷积，因为转置卷积中的前向传递相当于正常卷积的后向传递。</p><h1 id="ef0f" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">转置卷积的问题:</h1><p id="c467" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">如下所示，转置卷积会受到棋盘效应的影响。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ne"><img src="../Images/e661aacb04dc49f722b0f76a78d852a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Tsf3dlg7Wlhrt0D7k7osA.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图13。</strong>方格状文物(<a class="ae la" href="https://distill.pub/2016/deconv-checkerboard/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="c7b8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">造成这种情况的主要原因是在图像的某些部分出现不均匀的重叠，从而导致伪像。这可以通过使用可被步幅整除的内核大小来固定或减少，例如，当步幅为2时，采用2×2或4×4的内核大小。</p><h1 id="101e" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">转置卷积的应用；</h1><ol class=""><li id="f21a" class="ml mm it js b jt lz jx ma kb nf kf ng kj nh kn mq mr ms mt bi translated">超分辨率:</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="ab gu cl ni"><img src="../Images/f1ddecaaf1c60bb7047595dc62a14eb8.png" data-original-src="https://miro.medium.com/v2/format:webp/0*kIeyw3eMk-e1UchK.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图十四。</strong>使用转置卷积的超分辨率(<a class="ae la" href="http://openaccess.thecvf.com/content_ECCV_2018/html/Seong-Jin_Park_SRFeat_Single_Image_ECCV_2018_paper.html" rel="noopener ugc nofollow" target="_blank">信号源</a>)</p></figure><p id="4252" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.语义分割:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nj"><img src="../Images/c8d5c67de532938e1c02a3ba74117533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vk2xCr1r6ZaO7cYD.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd mj">图15。</strong>使用转置卷积实现语义分割(<a class="ae la" href="https://thegradient.pub/semantic-segmentation/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><h1 id="9fea" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">结论:</strong></h1><p id="f24c" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">转置卷积是现代分割和超分辨率算法的支柱。它们提供了抽象表示的最佳和最一般化的上采样。在这篇文章中，我们探讨了所使用的各种上采样技术，然后试图深入了解转置卷积的直观理解。<br/>我希望你喜欢这篇文章，如果你有任何疑问、疑问或评论，请随时通过<a class="ae la" href="https://twitter.com/Perceptron97" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或<a class="ae la" href="https://www.linkedin.com/in/divyanshu-mishra-ai/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>与我联系。</p><p id="3981" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">参考文献:</strong></p><ol class=""><li id="8184" class="ml mm it js b jt ju jx jy kb mn kf mo kj mp kn mq mr ms mt bi translated"><a class="ae la" href="https://www.youtube.com/watch?v=nDPWywWRIRo" rel="noopener ugc nofollow" target="_blank"> CS231n:视觉识别的卷积神经网络</a></li></ol><p id="58f8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.<a class="ae la" href="https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8" rel="noopener">转置卷积用… MS Excel解释！</a></p><p id="8481" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.<a class="ae la" href="http://d2l.ai/chapter_computer-vision/transposed-conv.html" rel="noopener ugc nofollow" target="_blank">深入钻研深度学习</a></p></div></div>    
</body>
</html>