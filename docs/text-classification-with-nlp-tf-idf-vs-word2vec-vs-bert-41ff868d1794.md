# åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡æœ¬åˆ†ç±»:Tf-Idf vs Word2Vec vs BERT

> åŸæ–‡ï¼š<https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794?source=collection_archive---------0----------------------->

![](img/bf7a59d41b19f964ce71b34753c515bf.png)

## é¢„å¤„ç†ã€æ¨¡å‹è®¾è®¡ã€è¯„ä¼°ã€è¯è¢‹çš„å¯è§£é‡Šæ€§ã€è¯åµŒå…¥ã€è¯­è¨€æ¨¡å‹

## æ‘˜è¦

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ NLP å’Œ Python è§£é‡Šæ–‡æœ¬å¤šç±»åˆ†ç±»çš„ 3 ç§ä¸åŒç­–ç•¥:è€å¼çš„*å•è¯è¢‹*(ä½¿ç”¨ Tf-Idf ) *ï¼Œ*è‘—åçš„*å•è¯åµŒå…¥(*ä½¿ç”¨ Word2Vec)ï¼Œä»¥åŠå°–ç«¯çš„*è¯­è¨€æ¨¡å‹*(ä½¿ç”¨ BERT)ã€‚

![](img/5c45d4e86f3251ebafa88ec4ce280e91.png)

[**ã€NLP(è‡ªç„¶è¯­è¨€å¤„ç†)**](https://en.wikipedia.org/wiki/Natural_language_processing) æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œç ”ç©¶è®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•ç»™è®¡ç®—æœºç¼–ç¨‹ï¼Œä»¥å¤„ç†å’Œåˆ†æå¤§é‡è‡ªç„¶è¯­è¨€æ•°æ®ã€‚NLP é€šå¸¸ç”¨äºæ–‡æœ¬æ•°æ®çš„åˆ†ç±»ã€‚**æ–‡æœ¬åˆ†ç±»**å°±æ˜¯æ ¹æ®æ–‡æœ¬æ•°æ®çš„å†…å®¹ç»™æ–‡æœ¬æ•°æ®åˆ†é…ç±»åˆ«çš„é—®é¢˜ã€‚

æœ‰ä¸åŒçš„æŠ€æœ¯å¯ä»¥ä»åŸå§‹æ–‡æœ¬æ•°æ®ä¸­æå–ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥è®­ç»ƒåˆ†ç±»æ¨¡å‹ã€‚æœ¬æ•™ç¨‹æ¯”è¾ƒäº†è€æ´¾çš„*å•è¯è¢‹*(ä¸ç®€å•çš„æœºå™¨å­¦ä¹ ç®—æ³•ä¸€èµ·ä½¿ç”¨)ã€æµè¡Œçš„*å•è¯åµŒå…¥*æ¨¡å‹(ä¸æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œä¸€èµ·ä½¿ç”¨)å’Œæœ€å…ˆè¿›çš„*è¯­è¨€æ¨¡å‹*(ä¸åŸºäºæ³¨æ„åŠ›çš„å˜å½¢é‡‘åˆšçš„è½¬ç§»å­¦ä¹ ä¸€èµ·ä½¿ç”¨)ï¼Œè¿™äº›æ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº† NLP çš„å‰æ™¯ã€‚

æˆ‘å°†å±•ç¤ºä¸€äº›æœ‰ç”¨çš„ Python ä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºå…¶ä»–ç±»ä¼¼çš„æƒ…å†µ(åªéœ€å¤åˆ¶ã€ç²˜è´´ã€è¿è¡Œ)ï¼Œå¹¶é€šè¿‡æ³¨é‡Šéå†æ¯ä¸€è¡Œä»£ç ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥å¤åˆ¶è¿™ä¸ªç¤ºä¾‹(ä¸‹é¢æ˜¯å®Œæ•´ä»£ç çš„é“¾æ¥)ã€‚

[](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb) [## mdipietro 09/data science _ äººå·¥æ™ºèƒ½ _ å®ç”¨å·¥å…·

### permalink dissolve GitHub æ˜¯è¶…è¿‡ 5000 ä¸‡å¼€å‘äººå‘˜çš„å®¶å›­ï¼Œä»–ä»¬ä¸€èµ·å·¥ä½œæ¥æ‰˜ç®¡å’Œå®¡æŸ¥ä»£ç ï¼Œç®¡ç†â€¦

github.com](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb) 

æˆ‘å°†ä½¿ç”¨â€œ**æ–°é—»ç±»åˆ«æ•°æ®é›†**â€ï¼Œå…¶ä¸­ä¸ºæ‚¨æä¾›äº†ä»*èµ«èŠ¬é¡¿é‚®æŠ¥*è·å¾—çš„ 2012 å¹´è‡³ 2018 å¹´çš„æ–°é—»æ ‡é¢˜ï¼Œå¹¶è¦æ±‚æ‚¨å°†å®ƒä»¬åˆ†ç±»åˆ°æ­£ç¡®çš„ç±»åˆ«ï¼Œå› æ­¤è¿™æ˜¯ä¸€ä¸ªå¤šç±»åˆ«åˆ†ç±»é—®é¢˜(ä¸‹é¢çš„é“¾æ¥)ã€‚

[](https://www.kaggle.com/rmisra/news-category-dataset) [## æ–°é—»ç±»åˆ«æ•°æ®é›†

### æ ¹æ®æ ‡é¢˜å’Œç®€çŸ­æè¿°è¯†åˆ«æ–°é—»çš„ç±»å‹

www.kaggle.com](https://www.kaggle.com/rmisra/news-category-dataset) 

ç‰¹åˆ«æ˜¯ï¼Œæˆ‘å°†ç»å†:

*   è®¾ç½®:å¯¼å…¥åŒ…ã€è¯»å–æ•°æ®ã€é¢„å¤„ç†ã€åˆ†åŒºã€‚
*   ç”¨ *scikit-learn* è¿›è¡Œç‰¹å¾å·¥ç¨‹&ç‰¹å¾é€‰æ‹©&æœºå™¨å­¦ä¹ ï¼Œç”¨ *lime* è¿›è¡Œæµ‹è¯•&è¯„ä¼°ï¼Œå¯è§£é‡Šæ€§ã€‚
*   å•è¯åµŒå…¥:ç”¨ *gensim* æ‹Ÿåˆä¸€ä¸ª Word2Vecï¼Œç”¨ *tensorflow/keras* è¿›è¡Œç‰¹å¾å·¥ç¨‹&æ·±åº¦å­¦ä¹ ï¼Œç”¨æ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•&è¯„ä»·ï¼Œå¯è§£é‡Šæ€§ã€‚
*   è¯­è¨€æ¨¡å‹:ç”¨*å˜å½¢é‡‘åˆš*è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼Œç”¨*å˜å½¢é‡‘åˆš*å’Œ*tensor flow/keras*æµ‹è¯•&è¯„ä¼°å‘é¢„å…ˆè®­ç»ƒå¥½çš„ BERT è½¬ç§»å­¦ä¹ ã€‚

## è®¾ç½®

é¦–å…ˆï¼Œæˆ‘éœ€è¦å¯¼å…¥ä»¥ä¸‹åº“:

```
**## for data** import **json** import **pandas** as pd
import **numpy** as np**## for plotting**
import **matplotlib**.pyplot as plt
import **seaborn** as sns**## for processing** import **re**
import **nltk****## for bag-of-words**
from **sklearn** import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing**## for explainer**
from **lime** import lime_text**## for word embedding**
import **gensim** import gensim.downloader as gensim_api**## for deep learning**
from **tensorflow**.keras import models, layers, preprocessing as kprocessing
from tensorflow.keras import backend as K**## for bert language model**
import **transformers**
```

æ•°æ®é›†åŒ…å«åœ¨ä¸€ä¸ª json æ–‡ä»¶ä¸­ï¼Œæ‰€ä»¥æˆ‘å°†é¦–å…ˆç”¨ *json* æŠŠå®ƒè¯»å…¥ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œç„¶åæŠŠå®ƒè½¬æ¢æˆä¸€ä¸ª *pandas* Dataframeã€‚

```
lst_dics = []
with **open**('data.json', mode='r', errors='ignore') as json_file:
    for dic in json_file:
        lst_dics.append( json**.loads**(dic) )**## print the first one**
lst_dics[0]
```

![](img/abad2559dc8105b8f4fda79c7a163672.png)

åŸå§‹æ•°æ®é›†åŒ…å«è¶…è¿‡ 30 ä¸ªç±»åˆ«ï¼Œä½†æ˜¯å‡ºäºæœ¬æ•™ç¨‹çš„ç›®çš„ï¼Œæˆ‘å°†ä½¿ç”¨ 3 ä¸ªç±»åˆ«çš„å­é›†:å¨±ä¹ã€æ”¿æ²»å’ŒæŠ€æœ¯ã€‚

```
**## create dtf**
dtf = pd.DataFrame(lst_dics)**## filter categories**
dtf = dtf[ dtf["category"].isin(['**ENTERTAINMENT**','**POLITICS**','**TECH**']) ][["category","headline"]]**## rename columns**
dtf = dtf.rename(columns={"category":"**y**", "headline":"**text**"})**## print 5 random rows**
dtf.sample(5)
```

![](img/dadf349b489276b644508140d4375a2c.png)

ä¸ºäº†ç†è§£æ•°æ®é›†çš„ç»„æˆï¼Œæˆ‘å°†é€šè¿‡ç”¨æ¡å½¢å›¾æ˜¾ç¤ºæ ‡ç­¾é¢‘ç‡æ¥æŸ¥çœ‹ç›®æ ‡çš„**å•å˜é‡åˆ†å¸ƒ**ã€‚

```
fig, ax = plt.subplots()
fig.suptitle(**"y"**, fontsize=12)
dtf[**"y"**].reset_index().groupby(**"y"**).count().sort_values(by= 
       "index").plot(kind="barh", legend=False, 
        ax=ax).grid(axis='x')
plt.show()
```

![](img/c34aa02e00de5b86b9f7cc6ab2d210c3.png)

æ•°æ®é›†æ˜¯ä¸å¹³è¡¡çš„:ä¸å…¶ä»–ç›¸æ¯”ï¼Œç§‘æŠ€æ–°é—»çš„æ¯”ä¾‹éå¸¸å°ï¼Œè¿™å°†ä½¿æ¨¡å‹è¯†åˆ«ç§‘æŠ€æ–°é—»ç›¸å½“å›°éš¾ã€‚

åœ¨è§£é‡Šå’Œæ„å»ºæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘å°†ç»™å‡ºä¸€ä¸ªé¢„å¤„ç†çš„ä¾‹å­ï¼Œé€šè¿‡æ¸…ç†æ–‡æœ¬ã€åˆ é™¤åœç”¨è¯å’Œåº”ç”¨è¯æ±‡åŒ–ã€‚æˆ‘å°†ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ã€‚

```
**'''
Preprocess a string.
:parameter
    :param text: string - name of column containing text
    :param lst_stopwords: list - list of stopwords to remove
    :param flg_stemm: bool - whether stemming is to be applied
    :param flg_lemm: bool - whether lemmitisation is to be applied
:return
    cleaned text
'''**
def **utils_preprocess_text**(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):
    **## clean (convert to lowercase and remove punctuations and   
    characters and then strip)**
    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())

    **## Tokenize (convert from string to list)**
    lst_text = text.split() **## remove Stopwords**
    if lst_stopwords is not None:
        lst_text = [word for word in lst_text if word not in 
                    lst_stopwords]

    **## Stemming (remove -ing, -ly, ...)**
    if flg_stemm == True:
        ps = nltk.stem.porter.PorterStemmer()
        lst_text = [ps.stem(word) for word in lst_text]

    **## Lemmatisation (convert the word into root word)**
    if flg_lemm == True:
        lem = nltk.stem.wordnet.WordNetLemmatizer()
        lst_text = [lem.lemmatize(word) for word in lst_text]

    **## back to string from list**
    text = " ".join(lst_text)
    return text
```

è¯¥å‡½æ•°ä»è¯­æ–™åº“ä¸­åˆ é™¤ä¸€ç»„ç»™å®šçš„å•è¯ã€‚æˆ‘å¯ä»¥ç”¨ *nltk* ä¸ºè‹±è¯­è¯æ±‡åˆ›å»ºä¸€ä¸ªé€šç”¨åœç”¨è¯åˆ—è¡¨(æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ æˆ–åˆ é™¤å•è¯æ¥ç¼–è¾‘è¿™ä¸ªåˆ—è¡¨)ã€‚

```
lst_stopwords = **nltk**.corpus.stopwords.words("**english**")
lst_stopwords
```

![](img/b10919fcef40946289f0de22bb3a3d5f.png)

ç°åœ¨ï¼Œæˆ‘å°†å¯¹æ•´ä¸ªæ•°æ®é›†åº”ç”¨æˆ‘ç¼–å†™çš„å‡½æ•°ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªåä¸ºâ€œ *text_clean* çš„æ–°åˆ—ä¸­ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥é€‰æ‹©å¤„ç†åŸå§‹è¯­æ–™åº“æˆ–é¢„å¤„ç†æ–‡æœ¬ã€‚

```
dtf["**text_clean**"] = dtf["text"].apply(lambda x: 
          **utils_preprocess_text**(x, flg_stemm=False, **flg_lemm=True**, 
          **lst_stopwords=lst_stopwords**))dtf.head()
```

![](img/06aef09197a0128cee33e71e25cf5cfd.png)

å¦‚æœä½ å¯¹æ›´æ·±å…¥çš„æ–‡æœ¬åˆ†æå’Œé¢„å¤„ç†æ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹[è¿™ç¯‡æ–‡ç« ](/text-analysis-feature-engineering-with-nlp-502d6ea9225d)ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘å°†æŠŠæ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†(70%)å’Œæµ‹è¯•é›†(30%)ï¼Œä»¥ä¾¿è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

```
**## split dataset**
dtf_train, dtf_test = model_selection.**train_test_split**(dtf, test_size=0.3)**## get target**
y_train = dtf_train[**"y"**].values
y_test = dtf_test[**"y"**].values
```

æˆ‘ä»¬å¼€å§‹å§ï¼Œå¥½å—ï¼Ÿ

## è¯æ±‡è¢‹

[*å•è¯è¢‹*](https://en.wikipedia.org/wiki/Bag-of-words_model) æ¨¡å‹å¾ˆç®€å•:å®ƒä»ä¸€ä¸ªæ–‡æ¡£è¯­æ–™åº“ä¸­æ„å»ºä¸€ä¸ªè¯æ±‡è¡¨ï¼Œå¹¶ç»Ÿè®¡è¿™äº›å•è¯åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚æ¢å¥è¯è¯´ï¼Œè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªå•è¯éƒ½æˆä¸ºä¸€ä¸ªç‰¹å¾ï¼Œä¸€ä¸ªæ–‡æ¡£ç”±ä¸€ä¸ªå…·æœ‰ç›¸åŒè¯æ±‡è¡¨é•¿åº¦çš„å‘é‡(ä¸€ä¸ªâ€œå•è¯åŒ…â€)æ¥è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ç”¨è¿™ç§æ–¹æ³•æ¥è¡¨è¾¾ 3 ä¸ªå¥å­:

![](img/06f565b9e55d35887acec308928b263f.png)

ç‰¹å¾çŸ©é˜µå½¢çŠ¶: ***æ–‡æ¡£æ•°é‡****x****è¯æ±‡é•¿åº¦*****

**å¯ä»¥æƒ³è±¡ï¼Œè¿™ç§æ–¹æ³•ä¼šå¯¼è‡´ä¸€ä¸ªæ˜¾è‘—çš„ç»´æ•°é—®é¢˜:æ–‡æ¡£è¶Šå¤šï¼Œè¯æ±‡è¡¨å°±è¶Šå¤§ï¼Œå› æ­¤ç‰¹å¾çŸ©é˜µå°†æ˜¯ä¸€ä¸ªå·¨å¤§çš„ç¨€ç–çŸ©é˜µã€‚å› æ­¤ï¼Œåœ¨å•è¯è¢‹æ¨¡å‹ä¹‹å‰é€šå¸¸è¦è¿›è¡Œé‡è¦çš„é¢„å¤„ç†(å•è¯æ¸…ç†ã€åœç”¨è¯å»é™¤ã€è¯å¹²åŒ–/è¯æ¡åŒ–)ï¼Œç›®çš„æ˜¯å‡å°‘ç»´æ•°é—®é¢˜ã€‚**

**æœ¯è¯­é¢‘ç‡ä¸ä¸€å®šæ˜¯æ–‡æœ¬çš„æœ€ä½³è¡¨ç¤ºã€‚äº‹å®ä¸Šï¼Œä½ å¯ä»¥åœ¨è¯­æ–™åº“ä¸­æ‰¾åˆ°å‡ºç°é¢‘ç‡æœ€é«˜ä½†å¯¹ç›®æ ‡å˜é‡å‡ ä¹æ²¡æœ‰é¢„æµ‹èƒ½åŠ›çš„å¸¸ç”¨è¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå•è¯åŒ…æœ‰ä¸€ä¸ªé«˜çº§å˜ä½“ï¼Œå®ƒä¸æ˜¯ç®€å•çš„è®¡æ•°ï¼Œè€Œæ˜¯ä½¿ç”¨**æœ¯è¯­é¢‘ç‡-é€†æ–‡æ¡£é¢‘ç‡**(æˆ– T[f-Idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf))**ã€‚**åŸºæœ¬ä¸Šï¼Œä¸€ä¸ªè¯çš„å€¼éšç€ count æˆæ­£æ¯”å¢åŠ ï¼Œä½†æ˜¯å’Œè¿™ä¸ªè¯åœ¨è¯­æ–™åº“ä¸­çš„å‡ºç°é¢‘ç‡æˆåæ¯”ã€‚**

**è®©æˆ‘ä»¬ä»**ç‰¹å¾å·¥ç¨‹å¼€å§‹ï¼Œ**é€šè¿‡ä»æ•°æ®ä¸­æå–ä¿¡æ¯æ¥åˆ›å»ºç‰¹å¾çš„è¿‡ç¨‹ã€‚æˆ‘å°†ä½¿ç”¨é™åˆ¶ä¸º 10ï¼Œ000 ä¸ªå•è¯çš„ Tf-Idf çŸ¢é‡å™¨(å› æ­¤æˆ‘çš„è¯æ±‡é•¿åº¦å°†ä¸º 10k)ï¼Œæ•è·å•å­—æ¯è¯(å³â€œ *new* â€å’Œâ€œ *york* â€)å’ŒåŒå­—æ¯è¯(å³â€œ *new york* â€)ã€‚æˆ‘è¿˜å°†æä¾›ç»å…¸è®¡æ•°çŸ¢é‡å™¨çš„ä»£ç :**

```
*****## Count (classic BoW)***
*vectorizer = feature_extraction.text.****CountVectorizer****(max_features=10000,* ngram_range=(1,2))

***## Tf-Idf (advanced variant of BoW)***
vectorizer = feature_extraction.text.**TfidfVectorizer**(max_features=10000, ngram_range=(1,2))**
```

**ç°åœ¨ï¼Œæˆ‘å°†åœ¨è®­ç»ƒé›†çš„é¢„å¤„ç†è¯­æ–™åº“ä¸Šä½¿ç”¨çŸ¢é‡å™¨æ¥æå–è¯æ±‡å¹¶åˆ›å»ºç‰¹å¾çŸ©é˜µã€‚**

```
**corpus = dtf_train["**text_clean**"]vectorizer.fit(corpus)
X_train = vectorizer.transform(corpus)
dic_vocabulary = vectorizer.vocabulary_**
```

**ç‰¹å¾çŸ©é˜µ *X_train* å…·æœ‰ 34ï¼Œ265(è®­ç»ƒä¸­çš„æ–‡æ¡£æ•°é‡)x 10ï¼Œ000(è¯æ±‡é•¿åº¦)çš„å½¢çŠ¶ï¼Œå¹¶ä¸”å®ƒéå¸¸ç¨€ç–:**

```
**sns.**heatmap**(X_train.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')**
```

**![](img/65b89a5de416006874c9be2bd115faac.png)**

**æ¥è‡ªç‰¹å¾çŸ©é˜µçš„éšæœºæ ·æœ¬(é»‘è‰²çš„éé›¶å€¼)**

**ä¸ºäº†çŸ¥é“æŸä¸ªå•è¯çš„ä½ç½®ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¯æ±‡è¡¨ä¸­æŸ¥æ‰¾å®ƒ:**

```
**word = "new york"dic_vocabulary[word]**
```

**å¦‚æœè¿™ä¸ªå•è¯å­˜åœ¨äºè¯æ±‡è¡¨ä¸­ï¼Œè¿™ä¸ªå‘½ä»¤æ‰“å°ä¸€ä¸ªæ•°å­— *N* ï¼Œæ„å‘³ç€çŸ©é˜µçš„ç¬¬ *N* ä¸ªç‰¹å¾å°±æ˜¯è¿™ä¸ªå•è¯ã€‚**

**ä¸ºäº†åˆ é™¤ä¸€äº›åˆ—å¹¶é™ä½çŸ©é˜µç»´æ•°ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä¸€äº›**ç‰¹å¾é€‰æ‹©**ï¼Œå³é€‰æ‹©ç›¸å…³å˜é‡å­é›†çš„è¿‡ç¨‹ã€‚æˆ‘å°†å¦‚ä¸‹è¿›è¡Œ:**

1.  **å°†æ¯ä¸ªç±»åˆ«è§†ä¸ºäºŒè¿›åˆ¶(ä¾‹å¦‚ï¼Œâ€œæŠ€æœ¯â€ç±»åˆ«å¯¹äºæŠ€æœ¯æ–°é—»ä¸º 1ï¼Œå¯¹äºå…¶ä»–ç±»åˆ«ä¸º 0)ï¼›**
2.  **æ‰§è¡Œ[å¡æ–¹æ£€éªŒ](https://en.wikipedia.org/wiki/Chi-squared_test)ä»¥ç¡®å®šç‰¹å¾å’Œ(äºŒå…ƒ)ç›®æ ‡æ˜¯å¦ç‹¬ç«‹ï¼›**
3.  **ä»…ä¿ç•™å¡æ–¹æ£€éªŒä¸­å…·æœ‰ç‰¹å®š p å€¼çš„è¦ç´ ã€‚**

```
**y = dtf_train["**y**"]
X_names = vectorizer.get_feature_names()
p_value_limit = 0.95dtf_features = pd.DataFrame()
for cat in np.unique(y):
    chi2, p = feature_selection.**chi2**(X_train, y==cat)
    dtf_features = dtf_features.append(pd.DataFrame(
                   {"feature":X_names, "score":1-p, "y":cat}))
    dtf_features = dtf_features.sort_values(["y","score"], 
                    ascending=[True,False])
    dtf_features = dtf_features[dtf_features["score"]>p_value_limit]X_names = dtf_features["feature"].unique().tolist()**
```

**æˆ‘é€šè¿‡ä¿ç•™ç»Ÿè®¡ä¸Šæœ€ç›¸å…³çš„ç‰¹å¾ï¼Œå°†ç‰¹å¾çš„æ•°é‡ä» 10ï¼Œ000 ä¸ªå‡å°‘åˆ° 3ï¼Œ152 ä¸ªã€‚è®©æˆ‘ä»¬æ‰“å°ä¸€äº›:**

```
**for cat in np.unique(y):
   print("# {}:".format(cat))
   print("  . selected features:",
         len(dtf_features[dtf_features["y"]==cat]))
   print("  . top features:", ",".join(
dtf_features[dtf_features["y"]==cat]["feature"].values[:10]))
   print(" ")**
```

**![](img/3737a49e1c9eaf5d037252cc9b67c052.png)**

**æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»™å®šè¿™ç»„æ–°å•è¯ä½œä¸ºè¾“å…¥ï¼Œåœ¨è¯­æ–™åº“ä¸Šé‡æ–°è£…é…çŸ¢é‡å™¨ã€‚è¿™å°†äº§ç”Ÿæ›´å°çš„ç‰¹å¾çŸ©é˜µå’Œæ›´çŸ­çš„è¯æ±‡è¡¨ã€‚**

```
**vectorizer = feature_extraction.text.**TfidfVectorizer**(vocabulary=X_names)vectorizer.fit(corpus)
X_train = vectorizer.transform(corpus)
dic_vocabulary = vectorizer.vocabulary_**
```

**æ–°çš„ç‰¹å¾çŸ©é˜µ *X_train* å…·æœ‰ is 34ï¼Œ265(è®­ç»ƒä¸­çš„æ–‡æ¡£æ•°é‡)x 3ï¼Œ152(ç»™å®šè¯æ±‡çš„é•¿åº¦)çš„å½¢çŠ¶ã€‚è®©æˆ‘ä»¬çœ‹çœ‹çŸ©é˜µæ˜¯å¦ä¸é‚£ä¹ˆç¨€ç–:**

**![](img/f683d14f6fcee76d0f2e9a684c8a5f45.png)**

**æ¥è‡ªæ–°ç‰¹å¾çŸ©é˜µçš„éšæœºæ ·æœ¬(é»‘è‰²éé›¶å€¼)**

**æ˜¯æ—¶å€™è®­ç»ƒä¸€ä¸ª**æœºå™¨å­¦ä¹ æ¨¡å‹**å¹¶æµ‹è¯•å®ƒäº†ã€‚æˆ‘æ¨èä½¿ç”¨æœ´ç´ è´å¶æ–¯ç®—æ³•:ä¸€ç§æ¦‚ç‡åˆ†ç±»å™¨ï¼Œå®ƒåˆ©ç”¨äº†[è´å¶æ–¯å®šç†](https://en.wikipedia.org/wiki/Bayes%27_theorem)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¯èƒ½ç›¸å…³çš„æ¡ä»¶çš„å…ˆéªŒçŸ¥è¯†ä½¿ç”¨æ¦‚ç‡è¿›è¡Œé¢„æµ‹çš„è§„åˆ™ã€‚è¯¥ç®—æ³•æœ€é€‚åˆè¿™ç§å¤§å‹æ•°æ®é›†ï¼Œå› ä¸ºå®ƒç‹¬ç«‹è€ƒè™‘æ¯ä¸ªç‰¹å¾ï¼Œè®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œç„¶åé¢„æµ‹æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ã€‚**

```
**classifier = naive_bayes.**MultinomialNB**()**
```

**æˆ‘å°†åœ¨ç‰¹å¾çŸ©é˜µä¸Šè®­ç»ƒè¿™ä¸ªåˆ†ç±»å™¨ï¼Œç„¶ååœ¨è½¬æ¢åçš„æµ‹è¯•é›†ä¸Šæµ‹è¯•å®ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘éœ€è¦æ„å»ºä¸€ä¸ª *scikit-learn* ç®¡é“:ä¸€ä¸ªè½¬æ¢åˆ—è¡¨å’Œä¸€ä¸ªæœ€ç»ˆä¼°è®¡å™¨çš„é¡ºåºåº”ç”¨ã€‚å°† Tf-Idf çŸ¢é‡å™¨å’Œæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨æ”¾åœ¨ä¸€ä¸ªç®¡é“ä¸­ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸€ä¸ªæ­¥éª¤ä¸­è½¬æ¢å’Œé¢„æµ‹æµ‹è¯•æ•°æ®ã€‚**

```
****## pipeline**
model = pipeline.**Pipeline**([("**vectorizer**", vectorizer),  
                           ("**classifier**", classifier)])**## train classifier** model["classifier"].fit(X_train, y_train)**## test** X_test = dtf_test["text_clean"].values
predicted = model.predict(X_test)
predicted_prob = model.predict_proba(X_test)**
```

**æˆ‘ä»¬ç°åœ¨å¯ä»¥**è¯„ä¼°å•è¯è¢‹æ¨¡å‹çš„æ€§èƒ½**ï¼Œæˆ‘å°†ä½¿ç”¨ä»¥ä¸‹æŒ‡æ ‡:**

*   **å‡†ç¡®æ€§:æ¨¡å‹é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ã€‚**
*   **æ··æ·†çŸ©é˜µ:æŒ‰ç±»åˆ«ç»†åˆ†æ­£ç¡®å’Œé”™è¯¯é¢„æµ‹æ•°é‡çš„æ±‡æ€»è¡¨ã€‚**
*   **ROC:è¯´æ˜åœ¨ä¸åŒé˜ˆå€¼è®¾ç½®ä¸‹çœŸé˜³æ€§ç‡ä¸å‡é˜³æ€§ç‡çš„å›¾è¡¨ã€‚æ›²çº¿ä¸‹çš„é¢ç§¯(AUC)è¡¨ç¤ºåˆ†ç±»å™¨å°†éšæœºé€‰æ‹©çš„é˜³æ€§è§‚å¯Ÿå€¼æ’åˆ—ä¸ºé«˜äºéšæœºé€‰æ‹©çš„é˜´æ€§è§‚å¯Ÿå€¼çš„æ¦‚ç‡ã€‚**
*   **Precision:ç›¸å…³å®ä¾‹åœ¨æ£€ç´¢åˆ°çš„å®ä¾‹ä¸­æ‰€å çš„æ¯”ä¾‹ã€‚**
*   **Recall:å®é™…æ£€ç´¢åˆ°çš„ç›¸å…³å®ä¾‹æ€»æ•°çš„ä¸€éƒ¨åˆ†ã€‚**

```
**classes = np.unique(y_test)
y_test_array = pd.get_dummies(y_test, drop_first=False).values **## Accuracy, Precision, Recall**
accuracy = metrics.accuracy_score(y_test, predicted)
auc = metrics.roc_auc_score(y_test, predicted_prob, 
                            multi_class="ovr")
print("Accuracy:",  round(accuracy,2))
print("Auc:", round(auc,2))
print("Detail:")
print(metrics.classification_report(y_test, predicted))

**## Plot confusion matrix**
cm = metrics.confusion_matrix(y_test, predicted)
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, 
            cbar=False)
ax.set(xlabel="Pred", ylabel="True", xticklabels=classes, 
       yticklabels=classes, title="Confusion matrix")
plt.yticks(rotation=0) fig, ax = plt.subplots(nrows=1, ncols=2)
**## Plot roc**
for i in range(len(classes)):
    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  
                           predicted_prob[:,i])
    ax[0].plot(fpr, tpr, lw=3, 
              label='{0} (area={1:0.2f})'.format(classes[i], 
                              metrics.auc(fpr, tpr))
               )
ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')
ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], 
          xlabel='False Positive Rate', 
          ylabel="True Positive Rate (Recall)", 
          title="Receiver operating characteristic")
ax[0].legend(loc="lower right")
ax[0].grid(True)

**## Plot precision-recall curve** for i in range(len(classes)):
    precision, recall, thresholds = metrics.precision_recall_curve(
                 y_test_array[:,i], predicted_prob[:,i])
    ax[1].plot(recall, precision, lw=3, 
               label='{0} (area={1:0.2f})'.format(classes[i], 
                                  metrics.auc(recall, precision))
              )
ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', 
          ylabel="Precision", title="Precision-Recall curve")
ax[1].legend(loc="best")
ax[1].grid(True)
plt.show()**
```

**![](img/70cd46763c53895ba93d7c08839e0148.png)**

**BoW æ¨¡å‹å¯¹æµ‹è¯•é›†çš„æ­£ç¡®ç‡ä¸º 85%(å‡†ç¡®ç‡ä¸º 0.85)ï¼Œä½†å¾ˆéš¾è¯†åˆ«ç§‘æŠ€æ–°é—»(åªæœ‰ 252 ä¸ªé¢„æµ‹æ­£ç¡®)ã€‚**

**è®©æˆ‘ä»¬è¯•ç€ç†è§£ä¸ºä»€ä¹ˆè¯¥æ¨¡å‹å°†æ–°é—»å½’å…¥æŸä¸€ç±»åˆ«ï¼Œå¹¶è¯„ä¼°è¿™äº›é¢„æµ‹çš„**è§£é‡Šèƒ½åŠ›**ã€‚lime åŒ…å¯ä»¥å¸®åŠ©æˆ‘ä»¬å»ºç«‹ä¸€ä¸ªè§£é‡Šå™¨ã€‚ä¸ºäº†ä¸¾ä¾‹è¯´æ˜ï¼Œæˆ‘å°†ä»æµ‹è¯•é›†ä¸­éšæœºè§‚å¯Ÿï¼Œçœ‹çœ‹æ¨¡å‹é¢„æµ‹äº†ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆã€‚**

```
****## select observation** i = 0
txt_instance = dtf_test["**text**"].iloc[i]**## check true value and predicted value**
print("True:", y_test[i], "--> Pred:", predicted[i], "| Prob:", round(np.max(predicted_prob[i]),2))**## show explanation**
explainer = lime_text.**LimeTextExplainer**(class_names=
             np.unique(y_train))
explained = explainer.explain_instance(txt_instance, 
             model.predict_proba, num_features=3)
explained.show_in_notebook(text=txt_instance, predict_proba=False)**
```

**![](img/4a949622b01ba00b6a67c146d6bca429.png)****![](img/aa1be6b6af7588a17b76ad91a4a06444.png)**

**è¿™æ˜¯æœ‰é“ç†çš„:å•è¯â€œ*å…‹æ—é¡¿*â€å’Œâ€œ*å…±å’Œå…š*â€ä¸ºè¿™ä¸ªæ¨¡å‹æŒ‡å‡ºäº†æ­£ç¡®çš„æ–¹å‘(æ”¿æ²»æ–°é—»)ï¼Œå³ä½¿å•è¯â€œ*èˆå°*â€åœ¨å¨±ä¹æ–°é—»ä¸­æ›´å¸¸è§ã€‚**

## **å•è¯åµŒå…¥**

**[*å•è¯åµŒå…¥*](https://en.wikipedia.org/wiki/Word_embedding) æ˜¯å°†è¯æ±‡ä¸­çš„å•è¯æ˜ å°„åˆ°å®æ•°å‘é‡çš„ç‰¹å¾å­¦ä¹ æŠ€æœ¯çš„ç»Ÿç§°ã€‚è¿™äº›å‘é‡æ˜¯æ ¹æ®å‡ºç°åœ¨å¦ä¸€ä¸ªå•è¯ä¹‹å‰æˆ–ä¹‹åçš„æ¯ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒæ¥è®¡ç®—çš„ã€‚æ¢å¥è¯è¯´ï¼Œç›¸åŒä¸Šä¸‹æ–‡çš„å•è¯é€šå¸¸ä¸€èµ·å‡ºç°åœ¨è¯­æ–™åº“ä¸­ï¼Œå› æ­¤å®ƒä»¬åœ¨å‘é‡ç©ºé—´ä¸­ä¹Ÿå°†æ˜¯æ¥è¿‘çš„ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸Šä¸€ä¸ªä¾‹å­ä¸­çš„ä¸‰ä¸ªå¥å­:**

**![](img/742e8b09d7c5128e2edd6dd30217c1cf.png)**

**åµŒå…¥ 2D å‘é‡ç©ºé—´çš„è¯**

**åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨è¿™ä¸ªå®¶æ—çš„ç¬¬ä¸€ä¸ªæ¨¡å‹:è°·æ­Œçš„[*word 2 vec*](https://en.wikipedia.org/wiki/Word2vec)*(2013)ã€‚å…¶ä»–æµè¡Œçš„å•è¯åµŒå…¥æ¨¡å‹è¿˜æœ‰æ–¯å¦ç¦çš„[*GloVe*](https://en.wikipedia.org/wiki/GloVe_(machine_learning))*(2014)*å’Œè„¸ä¹¦çš„[*fast text*](https://en.wikipedia.org/wiki/FastText)(2016)ã€‚*****

*****Word2Vec** åˆ©ç”¨è¯­æ–™åº“ä¸­çš„æ¯ä¸ªå”¯ä¸€å•è¯äº§ç”Ÿé€šå¸¸å…·æœ‰æ•°ç™¾ç»´çš„å‘é‡ç©ºé—´ï¼Œä½¿å¾—è¯­æ–™åº“ä¸­å…±äº«å…±åŒä¸Šä¸‹æ–‡çš„å•è¯åœ¨ç©ºé—´ä¸­å½¼æ­¤é è¿‘ã€‚è¿™å¯ä»¥ä½¿ç”¨ä¸¤ç§ä¸åŒçš„æ–¹æ³•æ¥å®Œæˆ:ä»å•ä¸ªå•è¯å¼€å§‹é¢„æµ‹å…¶ä¸Šä¸‹æ–‡( *Skip-gram* )æˆ–è€…ä»ä¸Šä¸‹æ–‡å¼€å§‹é¢„æµ‹å•è¯(*è¿ç»­å•è¯åŒ…*)ã€‚***

***åœ¨ Python ä¸­ï¼Œä½ å¯ä»¥åƒè¿™æ ·ä»[*genism-data*](https://github.com/RaRe-Technologies/gensim-data)*åŠ è½½ä¸€ä¸ªé¢„å…ˆè®­ç»ƒå¥½çš„å•è¯åµŒå…¥æ¨¡å‹:****

```
****nlp = gensim_api.load("**word2vec-google-news-300"**)****
```

****æˆ‘å°†ä½¿ç”¨ *gensim åœ¨è®­ç»ƒæ•°æ®è¯­æ–™åº“ä¸Šæ‹Ÿåˆæˆ‘è‡ªå·±çš„ Word2Vecï¼Œè€Œä¸æ˜¯ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ã€‚*åœ¨æ‹Ÿåˆæ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦å°†è¯­æ–™åº“è½¬æ¢æˆ n å…ƒæ–‡æ³•åˆ—è¡¨çš„åˆ—è¡¨ã€‚åœ¨è¿™ä¸ªç‰¹æ®Šçš„ä¾‹å­ä¸­ï¼Œæˆ‘å°†å°è¯•æ•è·å•å­—æ¯(" *york* ")ã€åŒå­—æ¯(" *new york* ")å’Œä¸‰å­—æ¯(" *new york city* ")ã€‚****

```
***corpus = dtf_train["**text_clean**"] **## create list of lists of unigrams**
lst_corpus = []
for string in corpus:
   lst_words = string.split()
   lst_grams = [" ".join(lst_words[i:i+1]) 
               for i in range(0, len(lst_words), 1)]
   lst_corpus.append(lst_grams) **## detect bigrams and trigrams**
bigrams_detector = gensim.models.phrases.**Phrases**(lst_corpus, 
                 delimiter=" ".encode(), min_count=5, threshold=10)
bigrams_detector = gensim.models.phrases.**Phraser**(bigrams_detector)trigrams_detector = gensim.models.phrases.**Phrases**(bigrams_detector[lst_corpus], 
            delimiter=" ".encode(), min_count=5, threshold=10)
trigrams_detector = gensim.models.phrases.**Phraser**(trigrams_detector)***
```

***å®‰è£… Word2Vec æ—¶ï¼Œéœ€è¦æŒ‡å®š:***

*   ***å•è¯å‘é‡çš„ç›®æ ‡å¤§å°ï¼Œæˆ‘ç”¨ 300ï¼›***
*   ***çª—å£ï¼Œæˆ–å¥å­ä¸­å½“å‰å•è¯å’Œé¢„æµ‹å•è¯ä¹‹é—´çš„æœ€å¤§è·ç¦»ï¼Œæˆ‘å°†ä½¿ç”¨è¯­æ–™åº“ä¸­æ–‡æœ¬çš„å¹³å‡é•¿åº¦ï¼›***
*   ***è®­ç»ƒç®—æ³•ï¼Œæˆ‘å°†ä½¿ç”¨ skip-grams (sg=1 ),å› ä¸ºé€šå¸¸å®ƒæœ‰æ›´å¥½çš„ç»“æœã€‚***

```
*****## fit w2v**
nlp = gensim.models.word2vec.**Word2Vec**(lst_corpus, size=300,   
            window=8, min_count=1, sg=1, iter=30)***
```

***æˆ‘ä»¬æœ‰è‡ªå·±çš„åµŒå…¥æ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä»è¯­æ–™åº“ä¸­é€‰æ‹©ä»»ä½•å•è¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå‘é‡ã€‚***

```
***word = "data"
nlp[word].shape***
```

***![](img/f77b6fe4c5b08cb3fada7872b0b3589e.png)***

***æˆ‘ä»¬ç”šè‡³å¯ä»¥é€šè¿‡åº”ç”¨ä»»ä½•é™ç»´ç®—æ³•(å³ [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) )æ¥ä½¿ç”¨å®ƒå°†ä¸€ä¸ªå•è¯åŠå…¶ä¸Šä¸‹æ–‡å¯è§†åŒ–åˆ°ä¸€ä¸ªæ›´å°çš„ç»´åº¦ç©ºé—´(2D æˆ– 3D)ä¸­ã€‚***

```
***word = "data"
fig = plt.figure()**## word embedding**
tot_words = [word] + [tupla[0] for tupla in 
                 nlp.most_similar(word, topn=20)]
X = nlp[tot_words]**## pca to reduce dimensionality from 300 to 3**
pca = manifold.**TSNE**(perplexity=40, n_components=3, init='pca')
X = pca.fit_transform(X)**## create dtf**
dtf_ = pd.DataFrame(X, index=tot_words, columns=["x","y","z"])
dtf_["input"] = 0
dtf_["input"].iloc[0:1] = 1**## plot 3d**
from mpl_toolkits.mplot3d import Axes3D
ax = fig.add_subplot(111, projection='3d')
ax.scatter(dtf_[dtf_["input"]==0]['x'], 
           dtf_[dtf_["input"]==0]['y'], 
           dtf_[dtf_["input"]==0]['z'], c="black")
ax.scatter(dtf_[dtf_["input"]==1]['x'], 
           dtf_[dtf_["input"]==1]['y'], 
           dtf_[dtf_["input"]==1]['z'], c="red")
ax.set(xlabel=None, ylabel=None, zlabel=None, xticklabels=[], 
       yticklabels=[], zticklabels=[])
for label, row in dtf_[["x","y","z"]].iterrows():
    x, y, z = row
    ax.text(x, y, z, s=label)***
```

***![](img/5c45d4e86f3251ebafa88ec4ce280e91.png)***

***è¿™å¾ˆé…·ï¼Œä½†æ˜¯åµŒå…¥è¿™ä¸ªè¯æ€ä¹ˆèƒ½ç”¨æ¥é¢„æµ‹æ–°é—»ç±»åˆ«å‘¢ï¼Ÿå—¯ï¼Œå•è¯å‘é‡å¯ä»¥åœ¨ç¥ç»ç½‘ç»œä¸­ç”¨ä½œæƒé‡ã€‚è¿™æ˜¯æ€ä¹ˆå›äº‹:***

*   ***é¦–å…ˆï¼Œå°†è¯­æ–™åº“è½¬æ¢æˆå•è¯ id çš„å¡«å……åºåˆ—ï¼Œä»¥è·å¾—ç‰¹å¾çŸ©é˜µã€‚***
*   ***ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªåµŒå…¥çŸ©é˜µï¼Œä½¿å¾— id ä¸º *N* çš„å•è¯çš„å‘é‡ä½äºç¬¬*N*è¡Œã€‚***
*   ***æœ€åï¼Œæ„å»ºä¸€ä¸ªå…·æœ‰åµŒå…¥å±‚çš„ç¥ç»ç½‘ç»œï¼Œè¯¥åµŒå…¥å±‚ç”¨ç›¸åº”çš„å‘é‡å¯¹åºåˆ—ä¸­çš„æ¯ä¸ªå•è¯è¿›è¡ŒåŠ æƒã€‚***

***è®©æˆ‘ä»¬ä»**ç‰¹å¾å·¥ç¨‹**å¼€å§‹ï¼Œé€šè¿‡ä½¿ç”¨ *tensorflow/keras* å°†ç»™äºˆ Word2Vec çš„ç›¸åŒé¢„å¤„ç†è¯­æ–™åº“(n å…ƒè¯­æ³•åˆ—è¡¨çš„åˆ—è¡¨)è½¬æ¢æˆåºåˆ—åˆ—è¡¨:***

```
*****## tokenize text**
tokenizer = kprocessing.text.**Tokenizer**(lower=True, split=' ', 
                     oov_token="NaN", 
                     filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n')
tokenizer.fit_on_texts(lst_corpus)
dic_vocabulary = tokenizer.word_index**## create sequence**
lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)**## padding sequence**
X_train = kprocessing.sequence.**pad_sequences**(lst_text2seq, 
                    maxlen=15, padding="post", truncating="post")***
```

***ç‰¹å¾çŸ©é˜µ *X_train* çš„å½¢çŠ¶ä¸º 34ï¼Œ265 x 15(åºåˆ—æ•° X åºåˆ—æœ€å¤§é•¿åº¦)ã€‚è®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸‹:***

```
***sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)
plt.show()***
```

***![](img/f9d1b04f3d2703846bfab65dde0abfb7.png)***

***ç‰¹å¾çŸ©é˜µ(34ï¼Œ265 x 15)***

***è¯­æ–™åº“ä¸­çš„æ¯ä¸ªæ–‡æœ¬ç°åœ¨éƒ½æ˜¯é•¿åº¦ä¸º 15 çš„ id åºåˆ—ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ–‡æœ¬ä¸­æœ‰ 10 ä¸ªæ ‡è®°ï¼Œé‚£ä¹ˆåºåˆ—ç”± 10 ä¸ª id+5 ä¸ª 0 ç»„æˆï¼Œè¿™æ˜¯å¡«å……å…ƒç´ (è€Œä¸åœ¨è¯æ±‡è¡¨ä¸­çš„å•è¯çš„ id æ˜¯ 1)ã€‚è®©æˆ‘ä»¬æ‰“å°ä¸€ä¸ªæ¥è‡ªè®­ç»ƒé›†çš„æ–‡æœ¬å¦‚ä½•è¢«è½¬æ¢æˆä¸€ä¸ªå¸¦æœ‰å¡«å……å’Œè¯æ±‡çš„åºåˆ—ã€‚***

```
***i = 0

**## list of text: ["I like this", ...]**
len_txt = len(dtf_train["text_clean"].iloc[i].split())
print("from: ", dtf_train["text_clean"].iloc[i], "| len:", len_txt)

**## sequence of token ids: [[1, 2, 3], ...]**
len_tokens = len(X_train[i])
print("to: ", X_train[i], "| len:", len(X_train[i]))

**## vocabulary: {"I":1, "like":2, "this":3, ...}**
print("check: ", dtf_train["text_clean"].iloc[i].split()[0], 
      " -- idx in vocabulary -->", 
      dic_vocabulary[dtf_train["text_clean"].iloc[i].split()[0]])

print("vocabulary: ", dict(list(dic_vocabulary.items())[0:5]), "... (padding element, 0)")***
```

***![](img/edd044da44c06749ed9f150e8015ef8b.png)***

***åœ¨ç»§ç»­ä¹‹å‰ï¼Œä¸è¦å¿˜è®°åœ¨æµ‹è¯•é›†ä¸ŠåšåŒæ ·çš„ç‰¹æ€§å·¥ç¨‹:***

```
***corpus = dtf_test["**text_clean**"] **## create list of n-grams**
lst_corpus = []
for string in corpus:
    lst_words = string.split()
    lst_grams = [" ".join(lst_words[i:i+1]) for i in range(0, 
                 len(lst_words), 1)]
    lst_corpus.append(lst_grams) **## detect common bigrams and trigrams using the fitted detectors**
lst_corpus = list(bigrams_detector[lst_corpus])
lst_corpus = list(trigrams_detector[lst_corpus]) **## text to sequence with the fitted tokenizer**
lst_text2seq = tokenizer.texts_to_sequences(lst_corpus) **## padding sequence**
X_test = kprocessing.sequence.**pad_sequences**(lst_text2seq, maxlen=15,
             padding="post", truncating="post")***
```

***![](img/30c51e6f881711789149c623f5126eed.png)***

***x _ æµ‹è¯•(14ï¼Œ697 x 15)***

***æˆ‘ä»¬å·²ç»å¾—åˆ°äº†æˆ‘ä»¬çš„ *X_train* å’Œ *X_test* ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦åˆ›å»ºåµŒå…¥çš„**çŸ©é˜µï¼Œå®ƒå°†è¢«ç”¨ä½œç¥ç»ç½‘ç»œåˆ†ç±»å™¨ä¸­çš„æƒé‡çŸ©é˜µã€‚*****

```
*****## start the matrix (length of vocabulary x vector size) with all 0s**
embeddings = np.zeros((len(dic_vocabulary)+1, 300))for word,idx in dic_vocabulary.items():
    **## update the row with vector**
    try:
        embeddings[idx] =  nlp[word]
    **## if word not in model then skip and the row stays all 0s**
    except:
        pass***
```

***è¯¥ä»£ç ç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º 22ï¼Œ338 x 300 çš„çŸ©é˜µ(ä»è¯­æ–™åº“ä¸­æå–çš„è¯æ±‡é•¿åº¦ x å‘é‡å¤§å°)ã€‚å®ƒå¯ä»¥é€šè¿‡å•è¯ id å¯¼èˆªï¼Œå¯ä»¥ä»è¯æ±‡è¡¨ä¸­è·å¾—ã€‚***

```
***word = "data"print("dic[word]:", dic_vocabulary[word], "|idx")
print("embeddings[idx]:", embeddings[dic_vocabulary[word]].shape, 
      "|vector")***
```

***![](img/dd16f543cb696f43146c079609e5d4da.png)***

***ç»ˆäºåˆ°äº†æ­å»º**æ·±åº¦å­¦ä¹ æ¨¡å‹**çš„æ—¶å€™äº†ã€‚æˆ‘å°†ä½¿ç”¨æˆ‘å°†æ„å»ºå’Œè®­ç»ƒçš„ç¥ç»ç½‘ç»œçš„ç¬¬ä¸€ä¸ªåµŒå…¥å±‚ä¸­çš„åµŒå…¥çŸ©é˜µæ¥å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»ã€‚è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ª id å°†è¢«ç”¨ä½œè®¿é—®åµŒå…¥çŸ©é˜µçš„ç´¢å¼•ã€‚è¿™ä¸ªåµŒå…¥å±‚çš„è¾“å‡ºå°†æ˜¯ä¸€ä¸ª 2D çŸ©é˜µï¼Œå¯¹äºè¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå•è¯ id æœ‰ä¸€ä¸ªå•è¯å‘é‡(åºåˆ—é•¿åº¦Ã—å‘é‡å¤§å°)ã€‚è®©æˆ‘ä»¬ç”¨å¥å­â€œ*æˆ‘å–œæ¬¢è¿™ç¯‡æ–‡ç« *â€ä½œä¸ºä¾‹å­:***

***![](img/092d46a775b0cf57dc4259c2e49c2b5f.png)***

***æˆ‘çš„ç¥ç»ç½‘ç»œç»“æ„å¦‚ä¸‹:***

*   ***å¦‚å‰æ‰€è¿°ï¼ŒåµŒå…¥å±‚å°†åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå°†å•è¯å‘é‡ä½œä¸ºæƒé‡ã€‚***
*   ***ä¸€ä¸ªç®€å•çš„æ³¨æ„å±‚ï¼Œä¸ä¼šå½±å“é¢„æµ‹ï¼Œä½†å®ƒå°†æ•è·æ¯ä¸ªå®ä¾‹çš„æƒé‡ï¼Œå¹¶å…è®¸æˆ‘ä»¬æ„å»ºä¸€ä¸ªå¥½çš„è§£é‡Šå™¨(å®ƒå¯¹äºé¢„æµ‹æ˜¯ä¸å¿…è¦çš„ï¼Œåªæ˜¯ä¸ºäº†è§£é‡Šï¼Œæ‰€ä»¥ä½ å¯ä»¥è·³è¿‡å®ƒ)ã€‚æ³¨æ„æœºåˆ¶åœ¨[æœ¬æ–‡](https://arxiv.org/abs/1409.0473) (2014)ä¸­æå‡ºï¼Œä½œä¸ºåºåˆ—æ¨¡å‹(å³ LSTM)é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥ç†è§£é•¿æ–‡æœ¬çš„å“ªäº›éƒ¨åˆ†å®é™…ä¸Šæ˜¯ç›¸å…³çš„ã€‚***
*   ***ä¸¤å±‚åŒå‘ LSTMï¼Œåœ¨ä¸¤ä¸ªæ–¹å‘ä¸Šå¯¹åºåˆ—ä¸­çš„å•è¯é¡ºåºè¿›è¡Œå»ºæ¨¡ã€‚***
*   ***ä¸¤ä¸ªæœ€ç»ˆçš„å¯†é›†å±‚å°†é¢„æµ‹æ¯ä¸ªæ–°é—»ç±»åˆ«çš„æ¦‚ç‡ã€‚***

```
*****## code attention layer**
def **attention_layer**(inputs, neurons):
    x = layers.**Permute**((2,1))(inputs)
    x = layers.**Dense**(neurons, activation="softmax")(x)
    x = layers.**Permute**((2,1), name="**attention**")(x)
    x = layers.**multiply**([inputs, x])
    return x **## input**
x_in = layers.**Input**(shape=(15,))**## embedding**
x = layers.**Embedding**(input_dim=embeddings.shape[0],  
                     output_dim=embeddings.shape[1], 
                     weights=[embeddings],
                     input_length=15, trainable=False)(x_in)**## apply attention**
x = attention_layer(x, neurons=15)**## 2 layers of bidirectional lstm**
x = layers.**Bidirectional**(layers.**LSTM**(units=15, dropout=0.2, 
                         return_sequences=True))(x)
x = layers.**Bidirectional**(layers.**LSTM**(units=15, dropout=0.2))(x)**## final dense layers**
x = layers.**Dense**(64, activation='relu')(x)
y_out = layers.**Dense**(3, activation='softmax')(x)**## compile**
model = models.**Model**(x_in, y_out)
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam', metrics=['accuracy'])

model.summary()***
```

***![](img/008d3348585bcfd23a1c581509298f2f.png)***

***ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨å®é™…æµ‹è¯•é›†ä¸Šæµ‹è¯•å®ƒä¹‹å‰ï¼Œæ£€æŸ¥ç”¨äºéªŒè¯çš„è®­ç»ƒé›†å­é›†çš„æ€§èƒ½ã€‚***

```
*****## encode y**
dic_y_mapping = {n:label for n,label in 
                 enumerate(np.unique(y_train))}
inverse_dic = {v:k for k,v in dic_y_mapping.items()}
y_train = np.array([inverse_dic[y] for y in y_train])**## train**
training = model.fit(x=X_train, y=y_train, batch_size=256, 
                     epochs=10, shuffle=True, verbose=0, 
                     validation_split=0.3)**## plot loss and accuracy**
metrics = [k for k in training.history.keys() if ("loss" not in k) and ("val" not in k)]
fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)ax[0].set(title="Training")
ax11 = ax[0].twinx()
ax[0].plot(training.history['loss'], color='black')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Loss', color='black')
for metric in metrics:
    ax11.plot(training.history[metric], label=metric)
ax11.set_ylabel("Score", color='steelblue')
ax11.legend()ax[1].set(title="Validation")
ax22 = ax[1].twinx()
ax[1].plot(training.history['val_loss'], color='black')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Loss', color='black')
for metric in metrics:
     ax22.plot(training.history['val_'+metric], label=metric)
ax22.set_ylabel("Score", color="steelblue")
plt.show()***
```

***![](img/d7a10cdb9f1d7072551abac81d5ce28d.png)***

***ä¸é”™ï¼åœ¨æŸäº›çºªå…ƒä¸­ï¼Œç²¾ç¡®åº¦è¾¾åˆ° 0.89ã€‚ä¸ºäº†å®Œæˆå•è¯åµŒå…¥æ¨¡å‹çš„**è¯„ä¼°**ï¼Œè®©æˆ‘ä»¬é¢„æµ‹æµ‹è¯•é›†å¹¶æ¯”è¾ƒä¹‹å‰ä½¿ç”¨çš„ç›¸åŒåº¦é‡(åº¦é‡çš„ä»£ç ä¸ä¹‹å‰ç›¸åŒ)ã€‚***

```
*****## test**
predicted_prob = model.predict(X_test)
predicted = [dic_y_mapping[np.argmax(pred)] for pred in 
             predicted_prob]***
```

***![](img/4ec9e658756f2ba8a9584f1c13b5064c.png)***

***è¯¥æ¨¡å‹çš„è¡¨ç°ä¸ä¸Šä¸€ä¸ªæ¨¡å‹ä¸€æ ·å¥½ï¼Œäº‹å®ä¸Šï¼Œå®ƒä¹Ÿå¾ˆéš¾å¯¹ç§‘æŠ€æ–°é—»è¿›è¡Œåˆ†ç±»ã€‚***

***ä½†æ˜¯è¿™ä¹Ÿæ˜¯**å¯ä»¥è§£é‡Šçš„å—ï¼Ÿæ˜¯çš„ï¼Œå®ƒæ˜¯ï¼æˆ‘åœ¨ç¥ç»ç½‘ç»œä¸­æ”¾ç½®äº†ä¸€ä¸ªæ³¨æ„åŠ›å±‚ï¼Œä»¥æå–æ¯ä¸ªå•è¯çš„æƒé‡ï¼Œå¹¶äº†è§£è¿™äº›æƒé‡å¯¹åˆ†ç±»ä¸€ä¸ªå®ä¾‹çš„è´¡çŒ®æœ‰å¤šå¤§ã€‚å› æ­¤ï¼Œæˆ‘å°†å°è¯•ä½¿ç”¨æ³¨æ„åŠ›æƒé‡æ¥æ„å»ºä¸€ä¸ªè§£é‡Šå™¨(ç±»ä¼¼äºä¸Šä¸€èŠ‚ä¸­çœ‹åˆ°çš„è§£é‡Šå™¨):*****

```
*****## select observation** i = 0
txt_instance = dtf_test["**text**"].iloc[i]**## check true value and predicted value**
print("True:", y_test[i], "--> Pred:", predicted[i], "| Prob:", round(np.max(predicted_prob[i]),2)) **## show explanation
### 1\. preprocess input** lst_corpus = []
for string in [re.sub(r'[^\w\s]','', txt_instance.lower().strip())]:
    lst_words = string.split()
    lst_grams = [" ".join(lst_words[i:i+1]) for i in range(0, 
                 len(lst_words), 1)]
    lst_corpus.append(lst_grams)
lst_corpus = list(bigrams_detector[lst_corpus])
lst_corpus = list(trigrams_detector[lst_corpus])
X_instance = kprocessing.sequence.pad_sequences(
              tokenizer.texts_to_sequences(corpus), maxlen=15, 
              padding="post", truncating="post")**### 2\. get attention weights**
layer = [layer for layer in model.layers if "**attention**" in 
         layer.name][0]
func = K.function([model.input], [layer.output])
weights = func(X_instance)[0]
weights = np.mean(weights, axis=2).flatten()**### 3\. rescale weights, remove null vector, map word-weight**
weights = preprocessing.MinMaxScaler(feature_range=(0,1)).fit_transform(np.array(weights).reshape(-1,1)).reshape(-1)
weights = [weights[n] for n,idx in enumerate(X_instance[0]) if idx 
           != 0]
dic_word_weigth = {word:weights[n] for n,word in 
                   enumerate(lst_corpus[0]) if word in 
                   tokenizer.word_index.keys()}**### 4\. barplot**
if len(dic_word_weigth) > 0:
   dtf = pd.DataFrame.from_dict(dic_word_weigth, orient='index', 
                                columns=["score"])
   dtf.sort_values(by="score", 
           ascending=True).tail(3).plot(kind="barh", 
           legend=False).grid(axis='x')
   plt.show()
else:
   print("--- No word recognized ---")**### 5\. produce html visualization**
text = []
for word in lst_corpus[0]:
    weight = dic_word_weigth.get(word)
    if weight is not None:
         text.append('<b><span style="background-color:rgba(100,149,237,' + str(weight) + ');">' + word + '</span></b>')
    else:
         text.append(word)
text = ' '.join(text)**### 6\. visualize on notebook** print("**\033**[1m"+"Text with highlighted words")
from IPython.core.display import display, HTML
display(HTML(text))***
```

***![](img/ef181ff814438c9b8eb1c2313724e68a.png)******![](img/f7c8a622fd9c5fd3bc0789735431bb13.png)***

***å°±åƒä»¥å‰ä¸€æ ·ï¼Œå•è¯â€œ*å…‹æ—é¡¿*â€å’Œâ€œ*å…±å’Œå…š*â€æ¿€æ´»äº†æ¨¡å‹çš„ç¥ç»å…ƒï¼Œä½†è¿™æ¬¡â€œ*é«˜*â€å’Œâ€œ*ç­åŠ è¥¿*â€ä¹Ÿè¢«è®¤ä¸ºä¸é¢„æµ‹ç•¥æœ‰å…³è”ã€‚***

## ***è¯­è¨€æ¨¡å‹***

***[è¯­è¨€æ¨¡å‹](https://en.wikipedia.org/wiki/Language_model)ï¼Œæˆ–è¯­å¢ƒåŒ–/åŠ¨æ€å•è¯åµŒå…¥**ï¼Œ**å…‹æœäº†ç»å…¸å•è¯åµŒå…¥æ–¹æ³•çš„æœ€å¤§é™åˆ¶:å¤šä¹‰è¯æ­§ä¹‰æ¶ˆé™¤ï¼Œä¸€ä¸ªå…·æœ‰ä¸åŒå«ä¹‰çš„å•è¯(ä¾‹å¦‚â€œ *bank* æˆ–â€œ *stick* â€)ä»…é€šè¿‡ä¸€ä¸ªå‘é‡æ¥è¯†åˆ«ã€‚æœ€å—æ¬¢è¿çš„æ–¹æ³•ä¹‹ä¸€æ˜¯ ELMO (2018)ï¼Œå®ƒæ²¡æœ‰åº”ç”¨å›ºå®šçš„åµŒå…¥ï¼Œè€Œæ˜¯ä½¿ç”¨åŒå‘ LSTMï¼ŒæŸ¥çœ‹æ•´ä¸ªå¥å­ï¼Œç„¶åä¸ºæ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ªåµŒå…¥ã€‚***

***è¿›å…¥å˜å½¢é‡‘åˆš:è°·æ­Œè®ºæ–‡ [*æå‡ºçš„ä¸€ç§æ–°çš„å»ºæ¨¡æŠ€æœ¯*](https://arxiv.org/abs/1706.03762)*(2017)*ä¸­å±•ç¤ºäº†é¡ºåºæ¨¡å‹(å¦‚ LSTM)å¯ä»¥å®Œå…¨è¢«æ³¨æ„åŠ›æœºåˆ¶å–ä»£ï¼Œç”šè‡³è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚*****

****è°·æ­Œçš„ [**ä¼¯ç‰¹**](https://en.wikipedia.org/wiki/BERT_(language_model)) (æ¥è‡ªå˜å½¢é‡‘åˆšçš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼Œ2018)ç»“åˆäº† ELMO ä¸Šä¸‹æ–‡åµŒå…¥å’Œå‡ ä¸ªå˜å½¢é‡‘åˆšï¼ŒåŠ ä¸Šå®ƒçš„åŒå‘(è¿™å¯¹å˜å½¢é‡‘åˆšæ¥è¯´æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„æ–°å¥‡)ã€‚BERT åˆ†é…ç»™å•è¯çš„å‘é‡æ˜¯æ•´ä¸ªå¥å­çš„å‡½æ•°ï¼Œå› æ­¤ï¼ŒåŸºäºä¸Šä¸‹æ–‡ï¼Œä¸€ä¸ªå•è¯å¯ä»¥æœ‰ä¸åŒçš„å‘é‡ã€‚è®©æˆ‘ä»¬ç”¨*å˜å‹å™¨*è¯•è¯•å§:****

```
***txt = **"bank river"****## bert tokenizer**
tokenizer = transformers.**BertTokenizer**.**from_pretrained**('bert-base-uncased', do_lower_case=True)**## bert model**
nlp = transformers.**TFBertModel**.**from_pretrained**('bert-base-uncased')**## return hidden layer with embeddings**
input_ids = np.array(tokenizer.encode(txt))[None,:]  
embedding = nlp(input_ids)embedding[0][0]***
```

***![](img/9f42c3aa067e46a5cf7eec8718ebe337.png)***

***å¦‚æœæˆ‘ä»¬å°†è¾“å…¥æ–‡æœ¬æ›´æ”¹ä¸ºâ€œ*é“¶è¡Œèµ„é‡‘*â€ï¼Œæˆ‘ä»¬å¾—åˆ°çš„ç»“æœæ˜¯:***

***![](img/bfea07a1c107b72a46ac20bb78cbbd2d.png)***

***ä¸ºäº†å®Œæˆæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œæ‚¨å¯ä»¥ä»¥ 3 ç§ä¸åŒçš„æ–¹å¼ä½¿ç”¨ BERT:***

*   ***è®­ç»ƒå®ƒä»åˆ’ç—•ï¼Œå¹¶ä½¿ç”¨å®ƒä½œä¸ºåˆ†ç±»å™¨ã€‚***
*   ***æå–å•è¯ embeddingï¼Œå¹¶åœ¨åµŒå…¥å±‚ä¸­ä½¿ç”¨å®ƒä»¬(å°±åƒæˆ‘å¯¹ Word2Vec æ‰€åšçš„é‚£æ ·)ã€‚***
*   ***å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹(è¿ç§»å­¦ä¹ )ã€‚***

***æˆ‘å°†é‡‡ç”¨åè€…ï¼Œå¹¶ä»ä¸€ä¸ªé¢„è®­ç»ƒçš„è¾ƒè½»ç‰ˆæœ¬çš„ BERT è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œç§°ä¸º[distilt-BERT](https://huggingface.co/transformers/model_doc/distilbert.html)(6600 ä¸‡ä¸ªå‚æ•°ï¼Œè€Œä¸æ˜¯ 1.1 äº¿ä¸ªï¼).***

```
*****## distil-bert tokenizer**
tokenizer = transformers.**AutoTokenizer**.**from_pretrained**('distilbert-base-uncased', do_lower_case=True)***
```

***åƒå¾€å¸¸ä¸€æ ·ï¼Œåœ¨è£…é…æ¨¡å‹ä¹‹å‰ï¼Œæœ‰ä¸€äº›ç‰¹å¾å·¥ç¨‹å·¥ä½œè¦åšï¼Œä½†æ˜¯è¿™æ¬¡ä¼šæœ‰ç‚¹æ£˜æ‰‹ã€‚ä¸ºäº†è¯´æ˜æˆ‘è¦åšçš„äº‹æƒ…ï¼Œè®©æˆ‘ä»¬ä»¥æˆ‘ä»¬æœ€å–œæ¬¢çš„å¥å­â€œ*æˆ‘å–œæ¬¢è¿™ç¯‡æ–‡ç« *â€ä¸ºä¾‹ï¼Œå®ƒå¿…é¡»è½¬æ¢ä¸º 3 ä¸ªå‘é‡(idã€æ©ç ã€æ®µ):***

***![](img/2f16d8673e765195c46a9def4ed73f67.png)***

***å½¢çŠ¶:3 x åºåˆ—é•¿åº¦***

***é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©åºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚è¿™æ¬¡æˆ‘å°†é€‰æ‹©ä¸€ä¸ªæ›´å¤§çš„æ•°å­—(å³ 50 ),å› ä¸º BERT ä¼šå°†æœªçŸ¥å•è¯æ‹†åˆ†æˆå­æ ‡è®°ï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªå·²çŸ¥çš„å•è¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç»™å®šä¸€ä¸ªåƒ" *zzdata* "è¿™æ ·çš„é€ è¯ï¼Œä¼¯ç‰¹ä¼šæŠŠå®ƒæ‹†åˆ†æˆ[" *z* "ï¼Œ" *##z* "ï¼Œ" *##data* "]ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨è¾“å…¥æ–‡æœ¬ä¸­æ’å…¥ç‰¹æ®Šæ ‡è®°ï¼Œç„¶åç”Ÿæˆæ©ç å’Œåˆ†æ®µã€‚æœ€åï¼Œå°†æ‰€æœ‰è¿™äº›æ”¾åœ¨ä¸€ä¸ªå¼ é‡ä¸­ä»¥è·å¾—ç‰¹å¾çŸ©é˜µï¼Œè¯¥çŸ©é˜µå°†å…·æœ‰ 3(idã€æ©ç ã€åˆ†æ®µ)x è¯­æ–™åº“ä¸­çš„æ–‡æ¡£æ•°é‡ x åºåˆ—é•¿åº¦çš„å½¢çŠ¶ã€‚***

***è¯·æ³¨æ„ï¼Œæˆ‘ä½¿ç”¨åŸå§‹æ–‡æœ¬ä½œä¸ºè¯­æ–™åº“(åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä¸€ç›´ä½¿ç”¨ *clean_text* åˆ—)ã€‚***

```
***corpus = dtf_train["**text**"]
maxlen = 50 **## add special tokens**
maxqnans = np.int((maxlen-20)/2)
corpus_tokenized = ["[CLS] "+
             " ".join(tokenizer.tokenize(re.sub(r'[^\w\s]+|\n', '', 
             str(txt).lower().strip()))[:maxqnans])+
             " [SEP] " for txt in corpus]

**## generate masks**
masks = [[1]*len(txt.split(" ")) + [0]*(maxlen - len(
           txt.split(" "))) for txt in corpus_tokenized]

**## padding**
txt2seq = [txt + " [PAD]"*(maxlen-len(txt.split(" "))) if len(txt.split(" ")) != maxlen else txt for txt in corpus_tokenized]

**## generate idx**
idx = [tokenizer.encode(seq.split(" ")) for seq in txt2seq]

**## generate segments**
segments = [] 
for seq in txt2seq:
    temp, i = [], 0
    for token in seq.split(" "):
        temp.append(i)
        if token == "[SEP]":
             i += 1
    segments.append(temp)**## feature matrix**
X_train = [np.asarray(idx, dtype='int32'), 
           np.asarray(masks, dtype='int32'), 
           np.asarray(segments, dtype='int32')]***
```

***ç‰¹å¾çŸ©é˜µ *X_train* çš„å½¢çŠ¶ä¸º 3Ã—34ï¼Œ265Ã—50ã€‚æˆ‘ä»¬å¯ä»¥ä»ç‰¹å¾çŸ©é˜µä¸­æ£€æŸ¥éšæœºè§‚å¯Ÿå€¼:***

```
***i = 0print("txt: ", dtf_train["text"].iloc[0])
print("tokenized:", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])
print("idx: ", X_train[0][i])
print("mask: ", X_train[1][i])
print("segment: ", X_train[2][i])***
```

***![](img/35417be842967bba45244f5d416b1bfe.png)***

***æ‚¨å¯ä»¥å°†ç›¸åŒçš„ä»£ç åº”ç”¨äº dtf_test["text"]ä»¥è·å¾— *X_test* ã€‚***

***ç°åœ¨ï¼Œæˆ‘è¦ä»é¢„å…ˆè®­ç»ƒå¥½çš„ BERT å¼€å§‹ï¼Œç”¨è¿ç§»å­¦ä¹ æ„å»º**æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘å°†ä½¿ç”¨å¹³å‡æ± å°† BERT çš„è¾“å‡ºæ€»ç»“ä¸ºä¸€ä¸ªå‘é‡ï¼Œç„¶åæ·»åŠ ä¸¤ä¸ªæœ€ç»ˆçš„å¯†é›†å±‚æ¥é¢„æµ‹æ¯ä¸ªæ–°é—»ç±»åˆ«çš„æ¦‚ç‡ã€‚*****

***å¦‚æœæ‚¨æƒ³ä½¿ç”¨ BERT çš„åŸå§‹ç‰ˆæœ¬ï¼Œä¸‹é¢æ˜¯ä»£ç (è®°å¾—ç”¨æ­£ç¡®çš„æ ‡è®°å™¨é‡åšç‰¹æ€§å·¥ç¨‹):***

```
*****## inputs**
idx = layers.**Input**((50), dtype="int32", name="input_idx")
masks = layers.**Input**((50), dtype="int32", name="input_masks")
segments = layers.Input((50), dtype="int32", name="input_segments")**## pre-trained bert**
nlp = transformers.**TFBertModel.from_pretrained**("bert-base-uncased")
bert_out, _ = nlp([idx, masks, segments])**## fine-tuning**
x = layers.**GlobalAveragePooling1D**()(bert_out)
x = layers.**Dense**(64, activation="relu")(x)
y_out = layers.**Dense**(len(np.unique(y_train)), 
                     activation='softmax')(x)**## compile**
model = models.**Model**([idx, masks, segments], y_out)for layer in model.layers[:4]:
    layer.trainable = Falsemodel.compile(loss='sparse_categorical_crossentropy', 
              optimizer='adam', metrics=['accuracy'])model.summary()***
```

***![](img/c98c8514c367551c81337b688a615c56.png)***

***æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œæˆ‘å°†ä½¿ç”¨æ›´ç®€å•çš„ç‰ˆæœ¬ï¼Œdistilt-BERT:***

```
*****## inputs**
idx = layers.**Input**((50), dtype="int32", name="input_idx")
masks = layers.**Input**((50), dtype="int32", name="input_masks")**## pre-trained bert with config**
config = transformers.DistilBertConfig(dropout=0.2, 
           attention_dropout=0.2)
config.output_hidden_states = Falsenlp = transformers.**TFDistilBertModel.from_pretrained**('distilbert-
                  base-uncased', config=config)
bert_out = nlp(idx, attention_mask=masks)[0]**## fine-tuning**
x = layers.**GlobalAveragePooling1D**()(bert_out)
x = layers.**Dense**(64, activation="relu")(x)
y_out = layers.**Dense**(len(np.unique(y_train)), 
                     activation='softmax')(x)**## compile**
model = models.**Model**([idx, masks], y_out)for layer in model.layers[:3]:
    layer.trainable = Falsemodel.compile(loss='sparse_categorical_crossentropy', 
              optimizer='adam', metrics=['accuracy'])model.summary()***
```

***![](img/aaeb3eec736e646710bfbda0a0eb20d6.png)***

***è®©æˆ‘ä»¬**è®­ç»ƒã€æµ‹è¯•ã€è¯„ä¼°**è¿™ä¸ªåå°å­(è¯„ä¼°çš„ä»£ç ç›¸åŒ):***

```
*****## encode y**
dic_y_mapping = {n:label for n,label in 
                 enumerate(np.unique(y_train))}
inverse_dic = {v:k for k,v in dic_y_mapping.items()}
y_train = np.array([inverse_dic[y] for y in y_train])**## train**
training = model.fit(x=X_train, y=y_train, batch_size=64, 
                     epochs=1, shuffle=True, verbose=1, 
                     validation_split=0.3)**## test**
predicted_prob = model.predict(X_test)
predicted = [dic_y_mapping[np.argmax(pred)] for pred in 
             predicted_prob]***
```

***![](img/a6d56582bddbbc0fe9e6c54ade57be57.png)******![](img/65be0ab75c86cda87d57d03c990a42a4.png)***

***BERT çš„æ€§èƒ½æ¯”ä»¥å‰çš„å‹å·ç•¥å¥½ï¼Œäº‹å®ä¸Šï¼Œå®ƒæ¯”å…¶ä»–å‹å·èƒ½è¯†åˆ«æ›´å¤šçš„ç§‘æŠ€æ–°é—»ã€‚***

## ***ç»“è®º***

***è¿™ç¯‡æ–‡ç« æ˜¯æ¼”ç¤º**å¦‚ä½•å°†ä¸åŒçš„ NLP æ¨¡å‹åº”ç”¨åˆ°å¤šç±»åˆ†ç±»ç”¨ä¾‹**çš„æ•™ç¨‹ã€‚æˆ‘æ¯”è¾ƒäº† 3 ç§æµè¡Œçš„æ–¹æ³•:ä½¿ç”¨ Tf-Idf çš„å•è¯åŒ…ã€ä½¿ç”¨ Word2Vec çš„å•è¯åµŒå…¥å’Œä½¿ç”¨ BERT çš„è¯­è¨€æ¨¡å‹ã€‚æˆ‘ç»å†äº†ç‰¹å¾å·¥ç¨‹&é€‰æ‹©ï¼Œæ¨¡å‹è®¾è®¡&æµ‹è¯•ï¼Œè¯„ä¼°&å¯è§£é‡Šæ€§ï¼Œæ¯”è¾ƒæ¯ä¸€æ­¥ä¸­çš„ 3 ä¸ªæ¨¡å‹(å¦‚æœå¯èƒ½çš„è¯)ã€‚***

***æˆ‘å¸Œæœ›ä½ å–œæ¬¢å®ƒï¼å¦‚æœ‰é—®é¢˜å’Œåé¦ˆï¼Œæˆ–è€…åªæ˜¯åˆ†äº«æ‚¨æ„Ÿå…´è¶£çš„é¡¹ç›®ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚***

> ***ğŸ‘‰[æˆ‘ä»¬æ¥è¿çº¿](https://linktr.ee/maurodp)ğŸ‘ˆ***

> ***æœ¬æ–‡æ˜¯ Python çš„ç³»åˆ—æ–‡ç«  **NLP çš„ä¸€éƒ¨åˆ†ï¼Œå‚è§:*****

***[](/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9) [## å¸¦ NLP çš„ AI èŠå¤©æœºå™¨äºº:è¯­éŸ³è¯†åˆ«+å˜å½¢é‡‘åˆš

### ç”¨ Python æ„å»ºä¸€ä¸ªä¼šè¯´è¯çš„èŠå¤©æœºå™¨äººï¼Œä¸ä½ çš„äººå·¥æ™ºèƒ½è¿›è¡Œå¯¹è¯

towardsdatascience.com](/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9) [](/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09) [## ä½¿ç”¨ NLP çš„æ–‡æœ¬æ‘˜è¦:TextRank vs Seq2Seq vs BART

### ä½¿ç”¨ Pythonã€Gensimã€Tensorflowã€Transformers è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†

towardsdatascience.com](/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09) [](/text-analysis-feature-engineering-with-nlp-502d6ea9225d) [## ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡æœ¬åˆ†æå’Œç‰¹å¾å·¥ç¨‹

### è¯­è¨€æ£€æµ‹ï¼Œæ–‡æœ¬æ¸…ç†ï¼Œé•¿åº¦ï¼Œæƒ…æ„Ÿï¼Œå‘½åå®ä½“è¯†åˆ«ï¼ŒN-grams é¢‘ç‡ï¼Œè¯å‘é‡ï¼Œä¸»é¢˜â€¦

towardsdatascience.com](/text-analysis-feature-engineering-with-nlp-502d6ea9225d) [](/text-classification-with-no-model-training-935fe0e42180) [## ç”¨äºæ— æ¨¡å‹è®­ç»ƒçš„æ–‡æœ¬åˆ†ç±»çš„ BERT

### å¦‚æœæ²¡æœ‰å¸¦æ ‡ç­¾çš„è®­ç»ƒé›†ï¼Œè¯·ä½¿ç”¨ BERTã€å•è¯åµŒå…¥å’Œå‘é‡ç›¸ä¼¼åº¦

towardsdatascience.com](/text-classification-with-no-model-training-935fe0e42180)***