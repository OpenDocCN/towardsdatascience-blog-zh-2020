<html>
<head>
<title>Building a suicidal tweet classifier using NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自然语言处理构建自杀推特分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-suicidal-tweet-classifier-using-nlp-ff6ccd77e971?source=collection_archive---------32-----------------------#2020-08-18">https://towardsdatascience.com/building-a-suicidal-tweet-classifier-using-nlp-ff6ccd77e971?source=collection_archive---------32-----------------------#2020-08-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="59fc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用自然语言处理来预测推特上的自杀意念。</h2></div><p id="6998" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">多年来，自杀一直是全球范围内的主要死亡原因之一，根据<a class="ae le" href="https://en.wikipedia.org/wiki/Suicide#:~:text=Suicides%20resulted%20in%20828%2C000%20global,of%20people%20die%20by%20suicide." rel="noopener ugc nofollow" target="_blank">维基百科</a>的数据，2015 年，自杀导致全球 82.8 万人死亡，比 1990 年的 71.2 万人有所增加。这使得自杀成为全球第十大死亡原因。越来越多的证据表明，互联网和社交媒体能够影响与自杀相关的行为。使用自然语言处理，机器学习的一个领域，我建立了一个非常简单的自杀意念分类器，它可以预测一个文本是否有自杀倾向。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/da3df62b1c8a1b9a63493c8765291d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WOtPfBqhMNgCSES7dXKw_Q.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">分析了 Tweet 的 WordCloud</p></figure><h1 id="a2e1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据</h1><p id="1144" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">我使用了一个我在 Github 上找到的 Twitter 爬虫，通过删除标签、链接、URL 和符号对代码做了一些修改。每当它从 Twitter 上抓取数据时，数据都是基于包含如下单词的查询参数抓取的:</p><blockquote class="ms mt mu"><p id="34e5" class="ki kj mv kk b kl km ju kn ko kp jx kq mw ks kt ku mx kw kx ky my la lb lc ld im bi translated">沮丧，绝望，答应照顾，我不属于这里，没有人值得我，我想死等等。</p></blockquote><p id="0d0b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然有些文本与自杀毫无关系，但我不得不手动标注了大约 8200 行推文的数据。我还获取了更多的 Twitter 数据，并且能够将我以前拥有的足够我训练的数据连接起来。</p><h1 id="ef61" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">构建模型</h1><h2 id="e1c5" class="mz lw it bd lx na nb dn mb nc nd dp mf kr ne nf mh kv ng nh mj kz ni nj ml nk bi translated">数据预处理</h2><p id="58b0" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">我导入了以下库:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="0136" class="mz lw it nm b gy nq nr l ns nt">import pickle<br/>import re<br/>import numpy as np<br/>import pandas as pd<br/>from tqdm import tqdm<br/>import nltk<br/>nltk.download('stopwords')</span></pre><p id="b688" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我编写了一个函数来清理文本数据，删除任何形式的 HTML 标记，保留表情字符，删除非单词字符，最后转换为小写。</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="b848" class="mz lw it nm b gy nq nr l ns nt">def preprocess_tweet(text):<br/>    text = re.sub('&lt;[^&gt;]*&gt;', '', text)<br/>    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text)<br/>    lowercase_text = re.sub('[\W]+', ' ', text.lower())<br/>    text = lowercase_text+' '.join(emoticons).replace('-', '') <br/>    return text</span></pre><p id="ca23" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之后，我对 tweet 数据集应用了 preprocess_tweet 函数来清理数据。</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="082f" class="mz lw it nm b gy nq nr l ns nt">tqdm.pandas()</span><span id="238e" class="mz lw it nm b gy nu nr l ns nt">df = pd.read_csv('data.csv')<br/>df['tweet'] = df['tweet'].progress_apply(preprocess_tweet)</span></pre><p id="16ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我使用。split()方法，并使用词干将文本转换为它们的根形式。</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="9265" class="mz lw it nm b gy nq nr l ns nt"><strong class="nm iu">from</strong> <strong class="nm iu">nltk.stem.porter</strong> <strong class="nm iu">import</strong> PorterStemmer<br/>porter = PorterStemmer()<br/><strong class="nm iu">def</strong> tokenizer_porter(text):<br/>    <strong class="nm iu">return</strong> [porter.stem(word) <strong class="nm iu">for</strong> word <strong class="nm iu">in</strong> text.split()]</span></pre><p id="9534" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我导入了停用词库来删除文本中的停用词。</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="5140" class="mz lw it nm b gy nq nr l ns nt"><strong class="nm iu">from</strong> <strong class="nm iu">nltk.corpus</strong> <strong class="nm iu">import</strong> stopwords<br/>stop = stopwords.words('english')</span></pre><p id="fece" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在单个文本上测试函数。</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="632e" class="mz lw it nm b gy nq nr l ns nt">[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]</span></pre><p id="6dc0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="e513" class="mz lw it nm b gy nq nr l ns nt">['runner', 'like', 'run', 'run', 'lot']</span></pre><h2 id="7d50" class="mz lw it bd lx na nb dn mb nc nd dp mf kr ne nf mh kv ng nh mj kz ni nj ml nk bi translated">矢量器</h2><p id="847c" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">对于这个项目，我使用了<strong class="kk iu">哈希矢量器</strong>，因为它与数据无关，这意味着它的内存非常低，可扩展到大型数据集，并且它不在内存中存储词汇字典。然后，我为哈希矢量器创建了一个记号赋予器函数</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="4d42" class="mz lw it nm b gy nq nr l ns nt">def tokenizer(text):<br/>    text = re.sub('&lt;[^&gt;]*&gt;', '', text)<br/>    emoticons = re.findall('(?::|;|=)(?:-)?(?:\(|D|P)',text.lower())<br/>    text = re.sub('[\W]+', ' ', text.lower())<br/>    text += ' '.join(emoticons).replace('-', '')<br/>    tokenized = [w for w in tokenizer_porter(text) if w not in stop]<br/>    return tokenized</span></pre><p id="62d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我创建了散列矢量器对象。</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="18a0" class="mz lw it nm b gy nq nr l ns nt">from sklearn.feature_extraction.text import HashingVectorizer</span><span id="c5fd" class="mz lw it nm b gy nu nr l ns nt">vect = HashingVectorizer(decode_error='ignore', n_features=2**21, <br/>                         preprocessor=None,tokenizer=<strong class="nm iu">tokenizer</strong>)</span></pre><h2 id="23b4" class="mz lw it bd lx na nb dn mb nc nd dp mf kr ne nf mh kv ng nh mj kz ni nj ml nk bi translated">模型</h2><p id="e1b9" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">对于该模型，我使用了随机梯度下降分类器算法。</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="5927" class="mz lw it nm b gy nq nr l ns nt"><strong class="nm iu">from</strong> <strong class="nm iu">sklearn.linear_model</strong> <strong class="nm iu">import</strong> SGDClassifier<br/>clf = SGDClassifier(loss='log', random_state=1)</span></pre><h2 id="c2c4" class="mz lw it bd lx na nb dn mb nc nd dp mf kr ne nf mh kv ng nh mj kz ni nj ml nk bi translated">培训和验证</h2><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="15b7" class="mz lw it nm b gy nq nr l ns nt">X = df["tweet"].to_list()<br/>y = df['label']</span></pre><p id="61a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于模型，我用了 80%用于训练，20%用于测试。</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="f582" class="mz lw it nm b gy nq nr l ns nt">from sklearn.model_selection import train_test_split<br/>X_train,X_test,y_train,y_test = train_test_split(X,<br/>                                                 y,<br/>                                                 test_size=0.20,<br/>                                                 random_state=0)</span></pre><p id="271b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我用我们之前创建的哈希矢量器将文本数据转换成矢量:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="2c00" class="mz lw it nm b gy nq nr l ns nt">X_train = vect.transform(X_train)<br/>X_test = vect.transform(X_test)</span></pre><p id="485c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我将数据与算法相匹配</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="905f" class="mz lw it nm b gy nq nr l ns nt">classes = np.array([0, 1])<br/>clf.partial_fit(X_train, y_train,classes=classes)</span></pre><p id="ab91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们测试一下测试数据的准确性:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="094a" class="mz lw it nm b gy nq nr l ns nt">print('Accuracy: %.3f' % clf.score(X_test, y_test))</span></pre><p id="0184" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="6d18" class="mz lw it nm b gy nq nr l ns nt">Accuracy: 0.912</span></pre><p id="927e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我有 91%的准确率，这很公平，之后，我用预测更新了模型</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="9c49" class="mz lw it nm b gy nq nr l ns nt">clf = clf.partial_fit(X_test, y_test)</span></pre><h1 id="1161" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">测试和预测</h1><p id="1e28" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">我在模型中添加了文本“我要自杀，我厌倦了沮丧和孤独的生活”。</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="845c" class="mz lw it nm b gy nq nr l ns nt">label = {0:'negative', 1:'positive'}<br/>example = ["I'll kill myself am tired of living depressed and alone"]<br/>X = vect.transform(example)<br/>print('Prediction: %s\nProbability: %.2f%%'<br/>      %(label[clf.predict(X)[0]],np.max(clf.predict_proba(X))*100))</span></pre><p id="5606" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我得到了输出:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="af0b" class="mz lw it nm b gy nq nr l ns nt">Prediction: positive<br/>Probability: 93.76%</span></pre><p id="5f73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">而当我用下面的文字“这么热的天，我想吃冰淇淋，逛公园”时，我得到了下面的预测:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="f538" class="mz lw it nm b gy nq nr l ns nt">Prediction: negative<br/>Probability: 97.91%</span></pre><p id="4d08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型能够准确预测这两种情况。这就是如何建立一个简单的自杀推特分类器。</p><p id="3ad7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在这里找到我写这篇文章用的笔记本<a class="ae le" href="https://github.com/AminuIsrael/Predicting-Suicide-Ideation" rel="noopener ugc nofollow" target="_blank"/></p><p id="84ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读😊</p></div></div>    
</body>
</html>