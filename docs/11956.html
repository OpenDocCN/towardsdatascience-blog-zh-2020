<html>
<head>
<title>Building a suicidal tweet classifier using NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æ„å»ºè‡ªæ€æ¨ç‰¹åˆ†ç±»å™¨</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/building-a-suicidal-tweet-classifier-using-nlp-ff6ccd77e971?source=collection_archive---------32-----------------------#2020-08-18">https://towardsdatascience.com/building-a-suicidal-tweet-classifier-using-nlp-ff6ccd77e971?source=collection_archive---------32-----------------------#2020-08-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="59fc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æ¥é¢„æµ‹æ¨ç‰¹ä¸Šçš„è‡ªæ€æ„å¿µã€‚</h2></div><p id="6998" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">å¤šå¹´æ¥ï¼Œè‡ªæ€ä¸€ç›´æ˜¯å…¨çƒèŒƒå›´å†…çš„ä¸»è¦æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œæ ¹æ®<a class="ae le" href="https://en.wikipedia.org/wiki/Suicide#:~:text=Suicides%20resulted%20in%20828%2C000%20global,of%20people%20die%20by%20suicide." rel="noopener ugc nofollow" target="_blank">ç»´åŸºç™¾ç§‘</a>çš„æ•°æ®ï¼Œ2015 å¹´ï¼Œè‡ªæ€å¯¼è‡´å…¨çƒ 82.8 ä¸‡äººæ­»äº¡ï¼Œæ¯” 1990 å¹´çš„ 71.2 ä¸‡äººæœ‰æ‰€å¢åŠ ã€‚è¿™ä½¿å¾—è‡ªæ€æˆä¸ºå…¨çƒç¬¬åå¤§æ­»äº¡åŸå› ã€‚è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼Œäº’è”ç½‘å’Œç¤¾äº¤åª’ä½“èƒ½å¤Ÿå½±å“ä¸è‡ªæ€ç›¸å…³çš„è¡Œä¸ºã€‚ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé¢†åŸŸï¼Œæˆ‘å»ºç«‹äº†ä¸€ä¸ªéå¸¸ç®€å•çš„è‡ªæ€æ„å¿µåˆ†ç±»å™¨ï¼Œå®ƒå¯ä»¥é¢„æµ‹ä¸€ä¸ªæ–‡æœ¬æ˜¯å¦æœ‰è‡ªæ€å€¾å‘ã€‚</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/da3df62b1c8a1b9a63493c8765291d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WOtPfBqhMNgCSES7dXKw_Q.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">åˆ†æäº† Tweet çš„ WordCloud</p></figure><h1 id="a2e1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">æ•°æ®</h1><p id="1144" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">æˆ‘ä½¿ç”¨äº†ä¸€ä¸ªæˆ‘åœ¨ Github ä¸Šæ‰¾åˆ°çš„ Twitter çˆ¬è™«ï¼Œé€šè¿‡åˆ é™¤æ ‡ç­¾ã€é“¾æ¥ã€URL å’Œç¬¦å·å¯¹ä»£ç åšäº†ä¸€äº›ä¿®æ”¹ã€‚æ¯å½“å®ƒä» Twitter ä¸ŠæŠ“å–æ•°æ®æ—¶ï¼Œæ•°æ®éƒ½æ˜¯åŸºäºåŒ…å«å¦‚ä¸‹å•è¯çš„æŸ¥è¯¢å‚æ•°æŠ“å–çš„:</p><blockquote class="ms mt mu"><p id="34e5" class="ki kj mv kk b kl km ju kn ko kp jx kq mw ks kt ku mx kw kx ky my la lb lc ld im bi translated">æ²®ä¸§ï¼Œç»æœ›ï¼Œç­”åº”ç…§é¡¾ï¼Œæˆ‘ä¸å±äºè¿™é‡Œï¼Œæ²¡æœ‰äººå€¼å¾—æˆ‘ï¼Œæˆ‘æƒ³æ­»ç­‰ç­‰ã€‚</p></blockquote><p id="0d0b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è™½ç„¶æœ‰äº›æ–‡æœ¬ä¸è‡ªæ€æ¯«æ— å…³ç³»ï¼Œä½†æˆ‘ä¸å¾—ä¸æ‰‹åŠ¨æ ‡æ³¨äº†å¤§çº¦ 8200 è¡Œæ¨æ–‡çš„æ•°æ®ã€‚æˆ‘è¿˜è·å–äº†æ›´å¤šçš„ Twitter æ•°æ®ï¼Œå¹¶ä¸”èƒ½å¤Ÿå°†æˆ‘ä»¥å‰æ‹¥æœ‰çš„è¶³å¤Ÿæˆ‘è®­ç»ƒçš„æ•°æ®è¿æ¥èµ·æ¥ã€‚</p><h1 id="ef61" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">æ„å»ºæ¨¡å‹</h1><h2 id="e1c5" class="mz lw it bd lx na nb dn mb nc nd dp mf kr ne nf mh kv ng nh mj kz ni nj ml nk bi translated">æ•°æ®é¢„å¤„ç†</h2><p id="58b0" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">æˆ‘å¯¼å…¥äº†ä»¥ä¸‹åº“:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="0136" class="mz lw it nm b gy nq nr l ns nt">import pickle<br/>import re<br/>import numpy as np<br/>import pandas as pd<br/>from tqdm import tqdm<br/>import nltk<br/>nltk.download('stopwords')</span></pre><p id="b688" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ç„¶åï¼Œæˆ‘ç¼–å†™äº†ä¸€ä¸ªå‡½æ•°æ¥æ¸…ç†æ–‡æœ¬æ•°æ®ï¼Œåˆ é™¤ä»»ä½•å½¢å¼çš„ HTML æ ‡è®°ï¼Œä¿ç•™è¡¨æƒ…å­—ç¬¦ï¼Œåˆ é™¤éå•è¯å­—ç¬¦ï¼Œæœ€åè½¬æ¢ä¸ºå°å†™ã€‚</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="b848" class="mz lw it nm b gy nq nr l ns nt">def preprocess_tweet(text):<br/>    text = re.sub('&lt;[^&gt;]*&gt;', '', text)<br/>    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text)<br/>    lowercase_text = re.sub('[\W]+', ' ', text.lower())<br/>    text = lowercase_text+' '.join(emoticons).replace('-', '') <br/>    return text</span></pre><p id="ca23" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ä¹‹åï¼Œæˆ‘å¯¹ tweet æ•°æ®é›†åº”ç”¨äº† preprocess_tweet å‡½æ•°æ¥æ¸…ç†æ•°æ®ã€‚</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="082f" class="mz lw it nm b gy nq nr l ns nt">tqdm.pandas()</span><span id="238e" class="mz lw it nm b gy nu nr l ns nt">df = pd.read_csv('data.csv')<br/>df['tweet'] = df['tweet'].progress_apply(preprocess_tweet)</span></pre><p id="16ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ç„¶åï¼Œæˆ‘ä½¿ç”¨ã€‚split()æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨è¯å¹²å°†æ–‡æœ¬è½¬æ¢ä¸ºå®ƒä»¬çš„æ ¹å½¢å¼ã€‚</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="9265" class="mz lw it nm b gy nq nr l ns nt"><strong class="nm iu">from</strong> <strong class="nm iu">nltk.stem.porter</strong> <strong class="nm iu">import</strong> PorterStemmer<br/>porter = PorterStemmer()<br/><strong class="nm iu">def</strong> tokenizer_porter(text):<br/>    <strong class="nm iu">return</strong> [porter.stem(word) <strong class="nm iu">for</strong> word <strong class="nm iu">in</strong> text.split()]</span></pre><p id="9534" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ç„¶åæˆ‘å¯¼å…¥äº†åœç”¨è¯åº“æ¥åˆ é™¤æ–‡æœ¬ä¸­çš„åœç”¨è¯ã€‚</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="5140" class="mz lw it nm b gy nq nr l ns nt"><strong class="nm iu">from</strong> <strong class="nm iu">nltk.corpus</strong> <strong class="nm iu">import</strong> stopwords<br/>stop = stopwords.words('english')</span></pre><p id="fece" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">åœ¨å•ä¸ªæ–‡æœ¬ä¸Šæµ‹è¯•å‡½æ•°ã€‚</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="632e" class="mz lw it nm b gy nq nr l ns nt">[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]</span></pre><p id="6dc0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è¾“å‡º:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="e513" class="mz lw it nm b gy nq nr l ns nt">['runner', 'like', 'run', 'run', 'lot']</span></pre><h2 id="7d50" class="mz lw it bd lx na nb dn mb nc nd dp mf kr ne nf mh kv ng nh mj kz ni nj ml nk bi translated">çŸ¢é‡å™¨</h2><p id="847c" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">å¯¹äºè¿™ä¸ªé¡¹ç›®ï¼Œæˆ‘ä½¿ç”¨äº†<strong class="kk iu">å“ˆå¸ŒçŸ¢é‡å™¨</strong>ï¼Œå› ä¸ºå®ƒä¸æ•°æ®æ— å…³ï¼Œè¿™æ„å‘³ç€å®ƒçš„å†…å­˜éå¸¸ä½ï¼Œå¯æ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ï¼Œå¹¶ä¸”å®ƒä¸åœ¨å†…å­˜ä¸­å­˜å‚¨è¯æ±‡å­—å…¸ã€‚ç„¶åï¼Œæˆ‘ä¸ºå“ˆå¸ŒçŸ¢é‡å™¨åˆ›å»ºäº†ä¸€ä¸ªè®°å·èµ‹äºˆå™¨å‡½æ•°</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="4d42" class="mz lw it nm b gy nq nr l ns nt">def tokenizer(text):<br/>    text = re.sub('&lt;[^&gt;]*&gt;', '', text)<br/>    emoticons = re.findall('(?::|;|=)(?:-)?(?:\(|D|P)',text.lower())<br/>    text = re.sub('[\W]+', ' ', text.lower())<br/>    text += ' '.join(emoticons).replace('-', '')<br/>    tokenized = [w for w in tokenizer_porter(text) if w not in stop]<br/>    return tokenized</span></pre><p id="62d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ç„¶åæˆ‘åˆ›å»ºäº†æ•£åˆ—çŸ¢é‡å™¨å¯¹è±¡ã€‚</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="18a0" class="mz lw it nm b gy nq nr l ns nt">from sklearn.feature_extraction.text import HashingVectorizer</span><span id="c5fd" class="mz lw it nm b gy nu nr l ns nt">vect = HashingVectorizer(decode_error='ignore', n_features=2**21, <br/>                         preprocessor=None,tokenizer=<strong class="nm iu">tokenizer</strong>)</span></pre><h2 id="23b4" class="mz lw it bd lx na nb dn mb nc nd dp mf kr ne nf mh kv ng nh mj kz ni nj ml nk bi translated">æ¨¡å‹</h2><p id="e1b9" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">å¯¹äºè¯¥æ¨¡å‹ï¼Œæˆ‘ä½¿ç”¨äº†éšæœºæ¢¯åº¦ä¸‹é™åˆ†ç±»å™¨ç®—æ³•ã€‚</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="5927" class="mz lw it nm b gy nq nr l ns nt"><strong class="nm iu">from</strong> <strong class="nm iu">sklearn.linear_model</strong> <strong class="nm iu">import</strong> SGDClassifier<br/>clf = SGDClassifier(loss='log', random_state=1)</span></pre><h2 id="c2c4" class="mz lw it bd lx na nb dn mb nc nd dp mf kr ne nf mh kv ng nh mj kz ni nj ml nk bi translated">åŸ¹è®­å’ŒéªŒè¯</h2><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="15b7" class="mz lw it nm b gy nq nr l ns nt">X = df["tweet"].to_list()<br/>y = df['label']</span></pre><p id="61a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">å¯¹äºæ¨¡å‹ï¼Œæˆ‘ç”¨äº† 80%ç”¨äºè®­ç»ƒï¼Œ20%ç”¨äºæµ‹è¯•ã€‚</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="f582" class="mz lw it nm b gy nq nr l ns nt">from sklearn.model_selection import train_test_split<br/>X_train,X_test,y_train,y_test = train_test_split(X,<br/>                                                 y,<br/>                                                 test_size=0.20,<br/>                                                 random_state=0)</span></pre><p id="271b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ç„¶åï¼Œæˆ‘ç”¨æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„å“ˆå¸ŒçŸ¢é‡å™¨å°†æ–‡æœ¬æ•°æ®è½¬æ¢æˆçŸ¢é‡:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="2c00" class="mz lw it nm b gy nq nr l ns nt">X_train = vect.transform(X_train)<br/>X_test = vect.transform(X_test)</span></pre><p id="485c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">æœ€åï¼Œæˆ‘å°†æ•°æ®ä¸ç®—æ³•ç›¸åŒ¹é…</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="905f" class="mz lw it nm b gy nq nr l ns nt">classes = np.array([0, 1])<br/>clf.partial_fit(X_train, y_train,classes=classes)</span></pre><p id="ab91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹æµ‹è¯•æ•°æ®çš„å‡†ç¡®æ€§:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="094a" class="mz lw it nm b gy nq nr l ns nt">print('Accuracy: %.3f' % clf.score(X_test, y_test))</span></pre><p id="0184" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è¾“å‡º:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="6d18" class="mz lw it nm b gy nq nr l ns nt">Accuracy: 0.912</span></pre><p id="927e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">æˆ‘æœ‰ 91%çš„å‡†ç¡®ç‡ï¼Œè¿™å¾ˆå…¬å¹³ï¼Œä¹‹åï¼Œæˆ‘ç”¨é¢„æµ‹æ›´æ–°äº†æ¨¡å‹</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="9c49" class="mz lw it nm b gy nq nr l ns nt">clf = clf.partial_fit(X_test, y_test)</span></pre><h1 id="1161" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">æµ‹è¯•å’Œé¢„æµ‹</h1><p id="1e28" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">æˆ‘åœ¨æ¨¡å‹ä¸­æ·»åŠ äº†æ–‡æœ¬â€œæˆ‘è¦è‡ªæ€ï¼Œæˆ‘åŒå€¦äº†æ²®ä¸§å’Œå­¤ç‹¬çš„ç”Ÿæ´»â€ã€‚</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="845c" class="mz lw it nm b gy nq nr l ns nt">label = {0:'negative', 1:'positive'}<br/>example = ["I'll kill myself am tired of living depressed and alone"]<br/>X = vect.transform(example)<br/>print('Prediction: %s\nProbability: %.2f%%'<br/>      %(label[clf.predict(X)[0]],np.max(clf.predict_proba(X))*100))</span></pre><p id="5606" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">æˆ‘å¾—åˆ°äº†è¾“å‡º:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="af0b" class="mz lw it nm b gy nq nr l ns nt">Prediction: positive<br/>Probability: 93.76%</span></pre><p id="5f73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è€Œå½“æˆ‘ç”¨ä¸‹é¢çš„æ–‡å­—â€œè¿™ä¹ˆçƒ­çš„å¤©ï¼Œæˆ‘æƒ³åƒå†°æ·‡æ·‹ï¼Œé€›å…¬å›­â€æ—¶ï¼Œæˆ‘å¾—åˆ°äº†ä¸‹é¢çš„é¢„æµ‹:</p><pre class="lg lh li lj gt nl nm nn no aw np bi"><span id="f538" class="mz lw it nm b gy nq nr l ns nt">Prediction: negative<br/>Probability: 97.91%</span></pre><p id="4d08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è¯¥æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹è¿™ä¸¤ç§æƒ…å†µã€‚è¿™å°±æ˜¯å¦‚ä½•å»ºç«‹ä¸€ä¸ªç®€å•çš„è‡ªæ€æ¨ç‰¹åˆ†ç±»å™¨ã€‚</p><p id="3ad7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°æˆ‘å†™è¿™ç¯‡æ–‡ç« ç”¨çš„ç¬”è®°æœ¬<a class="ae le" href="https://github.com/AminuIsrael/Predicting-Suicide-Ideation" rel="noopener ugc nofollow" target="_blank"/></p><p id="84ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">æ„Ÿè°¢é˜…è¯»ğŸ˜Š</p></div></div>    
</body>
</html>