<html>
<head>
<title>Day 105 of #NLP365: NLP Papers Summary — Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365 的第 105 天:NLP 论文摘要——用注意力超过注意力的神经网络进行方面级情感分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8?source=collection_archive---------43-----------------------#2020-04-14">https://towardsdatascience.com/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8?source=collection_archive---------43-----------------------#2020-04-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/61a393f717c1685f581a48076c22cd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDhaS8ed285Bb9L80C5U0g.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jg" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="d190" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="fbb6" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">NLP 论文摘要是我总结 NLP 研究论文要点的系列文章</h2></div><p id="83c1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">项目#NLP365 (+1)是我在 2020 年每天记录我的 NLP 学习旅程的地方。在这里，你可以随意查看我在过去的 100 天里学到了什么。</p><p id="ed3b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">今天的 NLP 论文是<strong class="lj jt"> <em class="md">方面级情感分类与注意力过度注意神经网络</em> </strong> s。下面是研究论文的关键要点。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="a61c" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">目标和贡献</h1><p id="10eb" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">介绍了一种用于基于方面的情感分析的 AOA 神经网络。AOA 模块联合学习方面和句子的表示，并且明确地捕捉方面和上下文句子之间的交互。在笔记本电脑和餐馆数据集上的结果优于以前基于 LSTM 的架构。</p><h1 id="f948" class="ml mm jj bd mn mo ni mq mr ms nj mu mv ky nk kz mx lb nl lc mz le nm lf nb nc bi translated">数据集</h1><p id="687a" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">使用来自 SemEval 2014 任务 4 的两个特定领域数据集进行实验:笔记本电脑和餐馆。准确性是评估标准。数据集汇总如下图所示:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/0455238560fefea05605d0daf8c82465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/0*40LRRFrtFq8Zcn9i.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">带有情感的数据集汇总统计数据[1]</p></figure><h1 id="aeed" class="ml mm jj bd mn mo ni mq mr ms nj mu mv ky nk kz mx lb nl lc mz le nm lf nb nc bi translated">方法学</h1><p id="f6b4" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">在这个任务中，我们被给定一个句子和一个体目标，我们的目标是对句子中体目标的情感极性进行分类。在下面显示的架构中有 4 个主要组件:单词嵌入、双 LSTM、注意力超过注意力(AOA)和最终预测。</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/58de68b37df346a83ea6285166779736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/0*Km2CPEnU1uxIUdRH.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">基于方面的情感分析的建议架构[1]</p></figure><h2 id="d2ed" class="nt mm jj bd mn nu nv dn mr nw nx dp mv lq ny nz mx lu oa ob mz ly oc od nb jp bi translated">单词嵌入和双 LSTM</h2><p id="1079" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">单词嵌入是一个标准化的步骤，我们将文本句子和方面目标转换成它的数字表示。这里没什么特别的。一旦我们得到了词向量，我们就把它们分别输入到两个双 LSTM 中，来学习词在句子和体目标中隐藏的语义。</p><h2 id="e859" class="nt mm jj bd mn nu nv dn mr nw nx dp mv lq ny nz mx lu oa ob mz ly oc od nb jp bi translated">注意力过度集中(AOA)</h2><p id="7ca8" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">下一步是使用 AOA 模块计算文本的注意力权重。下面是以下步骤:</p><ol class=""><li id="199b" class="oe of jj lj b lk ll ln lo lq og lu oh ly oi mc oj ok ol om bi translated">计算两个隐藏状态之间的成对交互矩阵，其中每个条目的值代表句子和目标之间的词对的相关性</li><li id="8705" class="oe of jj lj b lk on ln oo lq op lu oq ly or mc oj ok ol om bi translated">执行列级 softmax 以获得𝛼，目标到句子的关注</li><li id="83bc" class="oe of jj lj b lk on ln oo lq op lu oq ly or mc oj ok ol om bi translated">执行行方式 softmax 以获得𝛽，句子到目标的注意</li><li id="f050" class="oe of jj lj b lk on ln oo lq op lu oq ly or mc oj ok ol om bi translated">计算𝛽的列平均值，以获得目标级别的关注度𝛽，它告诉我们方面目标中的重要部分</li><li id="1b93" class="oe of jj lj b lk on ln oo lq op lu oq ly or mc oj ok ol om bi translated">最终句子级注意力𝛾是每个单独的目标到句子注意力𝛼的加权和，如下所示:𝛾=𝛼𝛽 𝑇</li></ol><h2 id="1497" class="nt mm jj bd mn nu nv dn mr nw nx dp mv lq ny nz mx lu oa ob mz ly oc od nb jp bi translated">最终预测</h2><p id="5b52" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">通过将来自 AOA 模块的句子注意力应用于句子隐藏状态来计算最终的句子表示，如下所示:𝑟=ℎ_𝑠^𝑇 *𝛾.这个最终句子表示被馈送到具有 softmax 函数的线性层中，以输出情感类别的概率。给定方面目标，具有最高概率的情感类是句子的预测标签。</p><h1 id="d8ff" class="ml mm jj bd mn mo ni mq mr ms nj mu mv ky nk kz mx lb nl lc mz le nm lf nb nc bi translated">实验和结果</h1><h2 id="2b4c" class="nt mm jj bd mn nu nv dn mr nw nx dp mv lq ny nz mx lu oa ob mz ly oc od nb jp bi translated">模型比较</h2><ul class=""><li id="50bd" class="oe of jj lj b lk nd ln ne lq os lu ot ly ou mc ov ok ol om bi translated"><strong class="lj jt"> <em class="md">多数</em> </strong>。简单基线，获取训练集中出现最频繁的情感，并将其分配给测试集中的样本</li><li id="3378" class="oe of jj lj b lk on ln oo lq op lu oq ly or mc ov ok ol om bi translated"><strong class="lj jt">T5【LSTM】T6</strong></li><li id="bb6c" class="oe of jj lj b lk on ln oo lq op lu oq ly or mc ov ok ol om bi translated"><strong class="lj jt"> <em class="md"> TD-LSTM </em> </strong>。使用 LSTMs 来捕获方面术语周围的上下文</li><li id="4ac7" class="oe of jj lj b lk on ln oo lq op lu oq ly or mc ov ok ol om bi translated"><strong class="lj jt"><em class="md">AT——LSTM</em></strong>。结合 LSTM 隐藏状态和方面嵌入计算注意向量</li><li id="75c3" class="oe of jj lj b lk on ln oo lq op lu oq ly or mc ov ok ol om bi translated"><strong class="lj jt"> <em class="md"> ATAE-LSTM </em> </strong>。AT-LSTM 的扩展，它将方面嵌入附加到每个单词嵌入中</li><li id="18bb" class="oe of jj lj b lk on ln oo lq op lu oq ly or mc ov ok ol om bi translated"><strong class="lj jt"> <em class="md">伊恩</em> </strong>。计算句子和体项的隐藏表示，并使用它来计算目标的注意力向量。句子和目标嵌入用于最终分类</li></ul><h2 id="ebfb" class="nt mm jj bd mn nu nv dn mr nw nx dp mv lq ny nz mx lu oa ob mz ly oc od nb jp bi translated">结果</h2><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/d60a8eac428269a0688669f063329d11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/0*uJKzcdMm64u042S8.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">跨两个数据集的模型比较(准确性)[1]</p></figure><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/0cfd59ebdbeeac0c3883169ba8d10034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/0*lEw7OFQbIZ4Wv5Bs.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">展示句子中的哪些标记对最终的情感分类贡献最大[1]</p></figure><ul class=""><li id="e12d" class="oe of jj lj b lk ll ln lo lq og lu oh ly oi mc ov ok ol om bi translated">根据结果表，AOA-LSTM 在与其他基线方法的比较中表现最佳</li><li id="aab2" class="oe of jj lj b lk on ln oo lq op lu oq ly or mc ov ok ol om bi translated">我们还包括了一个表格，通过可视化句子注意向量𝛾.来展示哪个单词对体情感极性贡献最大</li></ul><h1 id="677a" class="ml mm jj bd mn mo ni mq mr ms nj mu mv ky nk kz mx lb nl lc mz le nm lf nb nc bi translated">结论和未来工作</h1><p id="9502" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">在误差分析中，存在模型不能有效处理的情况。一是复杂的情感表达。另一个是生僻的成语。在未来的工作中，我们可以将句子的语法结构或先前的语言知识输入 AOA 神经网络。</p><h2 id="ebd5" class="nt mm jj bd mn nu nv dn mr nw nx dp mv lq ny nz mx lu oa ob mz ly oc od nb jp bi translated">来源:</h2><p id="e053" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">[1]黄、、、欧和凯瑟琳·m·卡利。"方面级情感分类与注意力-注意力神经网络."<em class="md">社会计算、行为文化建模和预测以及建模和仿真中的行为表现国际会议</em>。施普林格，查姆，2018。网址:【https://arxiv.org/pdf/1804.06536v1.pdf T2】</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="a10f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【https://ryanong.co.uk】原载于 2020 年 4 月 14 日<a class="ae jg" href="https://ryanong.co.uk/2020/04/14/day-105-nlp-research-papers-aspect-level-sentiment-classification-with-attention-over-attention-neural-networks/" rel="noopener ugc nofollow" target="_blank"><em class="md"/></a><em class="md">。</em></p></div></div>    
</body>
</html>