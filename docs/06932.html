<html>
<head>
<title>Analyzing K-Pop Using Machine Learning | Part 3— Model Building</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习分析韩国流行音乐|第 3 部分—建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/analyzing-k-pop-using-machine-learning-part-3-model-building-c19149964a22?source=collection_archive---------83-----------------------#2020-05-28">https://towardsdatascience.com/analyzing-k-pop-using-machine-learning-part-3-model-building-c19149964a22?source=collection_archive---------83-----------------------#2020-05-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1993" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/kpop-ml-tutorial" rel="noopener" target="_blank"> K-POP 机器学习教程系列</a></h2><div class=""/><div class=""><h2 id="6fd5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">这是本教程的第 3 部分，我构建了不同的预测模型并比较了结果。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/8f9a125d3bf15bc183f4935a5b9d8392.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ht05YUyI3m92fi3E-eF8Ew.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@dandycolor?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">萨维里·博博夫</a>在<a class="ae lh" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="li lj l"/></div></figure><p id="c51c" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">可以在这里 找到之前的教程<a class="ae lh" href="https://towardsdatascience.com/tagged/kpop-ml-tutorial" rel="noopener" target="_blank"> <strong class="lm jd">。</strong></a></p><p id="dc68" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated"><em class="mg">注意:你可以在这篇文章的底部找到我的全部代码的链接。</em></p><p id="5697" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">现在让我们做一些模型建设！</p><h1 id="f6a7" class="mh mi it bd mj mk ml mm mn mo mp mq mr ki ms kj mt kl mu km mv ko mw kp mx my bi translated">对数据框进行子集划分，并将分类变量转换为虚拟变量</h1><p id="54e7" class="pw-post-body-paragraph lk ll it lm b ln mz kd lp lq na kg ls lt nb lv lw lx nc lz ma mb nd md me mf im bi translated">对于模型构建，我删除了“fav_grp”列，因为我们在探索性数据分析中看到有太多的组，而 BTS 是主导组。</p><pre class="ks kt ku kv gt ne nf ng nh aw ni bi"><span id="1e7c" class="nj mi it nf b gy nk nl l nm nn">df_model = df[['popl_by_co_yn', 'reason', 'yr_listened',     'gender_pref','daily_music_hr', 'watch_MV_yn', 'daily_MV_hr', 'obsessed_yn','news_medium', 'pursuit', 'time_cons_yn', 'life_chg', 'pos_eff','yr_merch_spent', 'money_src', 'concert_yn', 'crazy_ev', 'age','country', 'job', 'gender', 'num_gr_like', 'bts_vs_others']]</span></pre><p id="c633" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">然后，我得到虚拟数据，将分类变量转换为回归模型的虚拟/指示变量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/05c7a2ed70be65e41c66f5711c0446d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPOV3D3K0VjrtRPgdofeZA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">获取虚拟数据以转换分类变量</p></figure><h1 id="0ef0" class="mh mi it bd mj mk ml mm mn mo mp mq mr ki ms kj mt kl mu km mv ko mw kp mx my bi translated">训练和测试分割</h1><p id="f347" class="pw-post-body-paragraph lk ll it lm b ln mz kd lp lq na kg ls lt nb lv lw lx nc lz ma mb nd md me mf im bi translated">主要目标是使用其他独立变量预测“每日音乐小时数”——K-pop 粉丝听 K-pop 的小时数。</p><p id="90c5" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">设 X 为除“每日 _ 音乐 _hr”之外的所有其他变量，设 Y 为“每日 _ 音乐 _hr”。然后我们用 80%作为训练集，剩下的 20%作为测试集。</p><h1 id="b119" class="mh mi it bd mj mk ml mm mn mo mp mq mr ki ms kj mt kl mu km mv ko mw kp mx my bi translated">多元线性回归</h1><p id="f5be" class="pw-post-body-paragraph lk ll it lm b ln mz kd lp lq na kg ls lt nb lv lw lx nc lz ma mb nd md me mf im bi translated">由于我们有一个小数据集(只有 240 行)，我们希望避免使用复杂的模型。所以我们从多元线性回归开始。</p><pre class="ks kt ku kv gt ne nf ng nh aw ni bi"><span id="841d" class="nj mi it nf b gy nk nl l nm nn">from sklearn.linear_model import LinearRegression</span><span id="0486" class="nj mi it nf b gy np nl l nm nn">from sklearn.metrics import mean_absolute_error</span><span id="d5df" class="nj mi it nf b gy np nl l nm nn"># initialize the linear regression model</span><span id="0f80" class="nj mi it nf b gy np nl l nm nn">lm = LinearRegression()</span><span id="fc49" class="nj mi it nf b gy np nl l nm nn"># train the model</span><span id="1903" class="nj mi it nf b gy np nl l nm nn">lm.fit(X_train, y_train)</span><span id="9d40" class="nj mi it nf b gy np nl l nm nn"># perform predicion on the test data</span><span id="ceac" class="nj mi it nf b gy np nl l nm nn">y_pred = lm.predict(X_test)</span><span id="fa21" class="nj mi it nf b gy np nl l nm nn"># performance metrics</span><span id="243a" class="nj mi it nf b gy np nl l nm nn">print('Coefficients:', lm.coef_)</span><span id="20de" class="nj mi it nf b gy np nl l nm nn">print('Intercept:', lm.intercept_)</span><span id="0ffd" class="nj mi it nf b gy np nl l nm nn">print('Mean absolute error (MAE): %.2f' % mean_absolute_error(y_test, y_pred))</span></pre><p id="2940" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">对于指标，我们将使用 MAE(平均绝对误差)来检查模型的准确性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/4d4495ba038667e25d704a9d295f7151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*7sxId4yEIVXL7smftJbR-A.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">多元线性回归的系数和平均误差</p></figure><p id="801e" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">多元线性回归(MLR)模型的 MAE 为 2.17。这意味着平均来说，我们的预测有 2.17 小时的误差。由于 K-pop 粉丝听 K-pop 的小时数从 0 到 10 不等，这是相当合理的。但是让我们看看我们是否能做得更好。</p><p id="1c9e" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">在同一个多元线性回归模型上，我们将应用 10 重交叉验证来概括数据。10 折交叉验证的工作原理是这样的——它在数据中创建了 10 个组，留下 1 个组进行验证，并使用剩余的 9 个组进行训练。最终，它创造了 10 种不同的 Mae。</p><p id="1c59" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">然后我们取它们的平均值，得到一个单一的 MAE-1.98。</p><p id="17d2" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">我们可以看到比上面的稍微好一点。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/b3b0d5911df755ed53586a026669fef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ez43sEqtFWHGRxD9c9By7A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用 10 重交叉验证的 MLR 的 MAE</p></figure><h1 id="bc63" class="mh mi it bd mj mk ml mm mn mo mp mq mr ki ms kj mt kl mu km mv ko mw kp mx my bi translated">套索回归</h1><p id="6d65" class="pw-post-body-paragraph lk ll it lm b ln mz kd lp lq na kg ls lt nb lv lw lx nc lz ma mb nd md me mf im bi translated">构建模型时处理小数据的另一种方法是使用正则化模型。Lasso ( <strong class="lm jd"> L </strong>东<strong class="lm jd">A</strong>b solute<strong class="lm jd">S</strong>hrinkage 和<strong class="lm jd">S</strong>election<strong class="lm jd">O</strong>operator)使用收缩(alpha)。收缩是指数据值向中心点收缩，如平均值。</p><p id="3326" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">它应用 L1 正则化，这增加了等于系数幅度绝对值的惩罚。</p><p id="77f4" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">套索的 MAE 是 1.58。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/29dfe017c43e11d4133ae46f13ca1a8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aM3Bm7w6zqQfBPYh2qls5Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">拉索回归的 MAE</p></figure><p id="36c8" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">我们也可以尝试寻找最优的 alpha 来找到最佳的套索模型。</p><p id="af4d" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">我们看到最佳 alpha 值是 0.09，MAE 现在稍微好一点，为 1.57。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/472b855dd244c5a1db63da29d79a2ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HEu1UgTyNHdRfosQgv8Y4Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">寻找套索回归的最佳α</p></figure><h1 id="27c4" class="mh mi it bd mj mk ml mm mn mo mp mq mr ki ms kj mt kl mu km mv ko mw kp mx my bi translated">里脊回归</h1><p id="391f" class="pw-post-body-paragraph lk ll it lm b ln mz kd lp lq na kg ls lt nb lv lw lx nc lz ma mb nd md me mf im bi translated">与 Lasso 类似，岭回归也增加了惩罚。它使用 L2 正则化。与 Lasso 回归的唯一区别是，它使用系数的平方大小，而不是绝对值。</p><p id="e049" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">岭回归的 MAE 为 1.85，与 lasso 相比并不算大。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/17381220866dde339b0cff86e0e10424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LLlJT8vgUkPEr1tulk4kOQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">岭回归的 MAE</p></figure><p id="9004" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">我们也可以尝试找到最佳收缩参数，但根据图，看起来我们已经有了最佳收缩参数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/35c1c9b6fca2c056f2ce819684fe7079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zBLePLs3Ubq2xu1_5DrRmw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">看我们是否能找到最佳收缩率</p></figure><h1 id="8cd7" class="mh mi it bd mj mk ml mm mn mo mp mq mr ki ms kj mt kl mu km mv ko mw kp mx my bi translated">随机森林回归量</h1><p id="d769" class="pw-post-body-paragraph lk ll it lm b ln mz kd lp lq na kg ls lt nb lv lw lx nc lz ma mb nd md me mf im bi translated">基于树的模型可以是好的模型，只要它们不太深。</p><p id="7896" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">我们可以看到 RF 的 MAE 为 1.61。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/10b12c88a2bb790b1bbb0fff10f7ae9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04qfeUMjknLVE7OZ0oUDoQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">随机森林回归的 MAE</p></figure><p id="1102" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">我们还可以尝试调整随机森林模型的超参数。使用 GridsearchCV 是调优参数的好方法。</p><p id="e574" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">下面是调整随机回归参数的一种方法。</p><pre class="ks kt ku kv gt ne nf ng nh aw ni bi"><span id="87f8" class="nj mi it nf b gy nk nl l nm nn">from sklearn.model_selection import GridSearchCV</span><span id="6c88" class="nj mi it nf b gy np nl l nm nn">params = {'n_estimators':range(10,100,10), <br/>          'criterion':('mse','mae'), <br/>          'max_features':('auto','sqrt','log2')}</span><span id="d091" class="nj mi it nf b gy np nl l nm nn">gs_rf = GridSearchCV(rf, params,<br/>                     scoring = 'neg_mean_absolute_error', cv = 10)</span><span id="f9b4" class="nj mi it nf b gy np nl l nm nn">gs_rf.fit(X_train, y_train)</span></pre><p id="797f" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">使用最佳估计量，最佳 MAE 为 1.51。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/63a4c2ffd9a7e1b6e6b5fbb8ea5889c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*x584ERHLTc9podKIpsTblQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">调谐随机森林</p></figure><h1 id="b9b0" class="mh mi it bd mj mk ml mm mn mo mp mq mr ki ms kj mt kl mu km mv ko mw kp mx my bi translated">XGBoost</h1><p id="9e57" class="pw-post-body-paragraph lk ll it lm b ln mz kd lp lq na kg ls lt nb lv lw lx nc lz ma mb nd md me mf im bi translated">另一个基于树的模型是 XGBoost。</p><p id="485e" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">XGBoost 的 MAE 为 1.54。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/fd0d5147c138d60903187a46f2bc35ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zBNaicPhNdmEJRVRf_0pDQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">XGBoost 的 MAE</p></figure><p id="a4f1" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">我们还可以尝试调整超参数，就像我们对随机森林模型所做的那样。</p><pre class="ks kt ku kv gt ne nf ng nh aw ni bi"><span id="55ca" class="nj mi it nf b gy nk nl l nm nn">params = {'min_child_weight': [3, 5, ], <br/>          'gamma': [0.5, 1], <br/>          'subsample': [0.8, 1.0],<br/>          'colsample_bytree': [0.6, 0.8], <br/>          'max_depth': [1,2]}</span><span id="ba72" class="nj mi it nf b gy np nl l nm nn">gs_xgb = GridSearchCV(xgb, params,<br/>                      scoring = 'neg_mean_absolute_error', cv = 10)</span><span id="512b" class="nj mi it nf b gy np nl l nm nn">gs_xgb.fit(X_train, y_train)</span></pre><p id="e221" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">调优后的 XGBoost 的 MAE 为 1.33。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b8efce1a8649083efff65c4332e8f6af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*Ib-FX0CTyD8Y3XVr6onYKQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">调优的 XGBoost</p></figure><h1 id="5215" class="mh mi it bd mj mk ml mm mn mo mp mq mr ki ms kj mt kl mu km mv ko mw kp mx my bi translated">比较所有型号的性能</h1><p id="2378" class="pw-post-body-paragraph lk ll it lm b ln mz kd lp lq na kg ls lt nb lv lw lx nc lz ma mb nd md me mf im bi translated">作为本教程的总结，我们将比较我们构建的所有模型的性能。</p><pre class="ks kt ku kv gt ne nf ng nh aw ni bi"><span id="7173" class="nj mi it nf b gy nk nl l nm nn">lm_pred = lm.predict(X_test)</span><span id="6dcd" class="nj mi it nf b gy np nl l nm nn">lm_las_pred = lm_las.predict(X_test)</span><span id="0575" class="nj mi it nf b gy np nl l nm nn">lm_rid_pred = lm_rid.predict(X_test)</span><span id="bffe" class="nj mi it nf b gy np nl l nm nn">rf_pred = gs_rf.best_estimator_.predict(X_test)</span><span id="1af3" class="nj mi it nf b gy np nl l nm nn">xgb_pred = gs_xgb.best_estimator_.predict(X_test)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/779fbe5ff4c8d43c260f360806d8dd19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*yJe3rTdUFzIjjGVCfx2AOQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">比较模型性能</p></figure><p id="0b51" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">我们看到 XGBoost 是最好的模型！平均而言，预测误差为 1.23 小时。</p><p id="7911" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">当然，你可以花几天时间去寻找“最好”的模型，但同时，我们也希望有效率。</p><p id="bc0a" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">感谢您的阅读！接下来的教程，我要讲的是模型制作！</p><p id="c696" class="pw-post-body-paragraph lk ll it lm b ln lo kd lp lq lr kg ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">我的<strong class="lm jd">全码</strong>就是这里的<a class="ae lh" href="https://github.com/importdata/kpop-analysis/blob/master/K_pop_Model_Building.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="lm jd"/></a>。</p></div></div>    
</body>
</html>