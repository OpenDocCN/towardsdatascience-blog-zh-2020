<html>
<head>
<title>Self-Supervised Tracking via Video Colorization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过视频彩色化的自我监督跟踪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-supervised-tracking-via-video-colorization-7b2b066359d5?source=collection_archive---------44-----------------------#2020-08-17">https://towardsdatascience.com/self-supervised-tracking-via-video-colorization-7b2b066359d5?source=collection_archive---------44-----------------------#2020-08-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="aea3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">作为代理任务的视频着色和作为下游任务的对象跟踪</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c590540678b3cb7d0dd091b4676b1c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mVrE3wGXACwYmA8Pc66IKA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">普里西拉·杜·普里兹在<a class="ae kv" href="https://unsplash.com/s/photos/night?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="de58" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>这篇文章中，我们将了解一种新颖的自监督目标跟踪方法。自我监督是模型自我学习的一种方法😎，这本身就让题目很有意思。在这里，我们将看到我们的模型如何学会自己跟踪对象。我们将从物体跟踪的基础开始，然后讨论什么是计算机视觉的自我监督学习，最后详细讨论这种方法。</p><blockquote class="mb mc md"><p id="7518" class="kw kx me ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">这个方法的实现可以在<a class="ae kv" href="https://github.com/hyperparameters/tracking_via_colorization" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p></blockquote><h1 id="3105" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">对象跟踪简介🎯</h1><p id="d7f4" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">用简单的语言来说，它可以理解为在整个视频序列中识别唯一的对象。要跟踪的对象通常被称为<em class="me">目标对象。</em>跟踪可以通过<a class="ae kv" href="https://d2l.ai/chapter_computer-vision/bounding-box.html" rel="noopener ugc nofollow" target="_blank"> <em class="me">包围盒</em> </a> <em class="me"> </em>或者<a class="ae kv" href="https://kharshit.github.io/blog/2019/08/23/quick-intro-to-instance-segmentation" rel="noopener ugc nofollow" target="_blank"> <em class="me">实例分割</em> </a> <em class="me">来完成。</em>有两种类型的公共对象跟踪挑战。</p><ol class=""><li id="77b4" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr nk nl nm nn bi translated">单个对象跟踪:在整个视频序列中跟踪感兴趣的对象。<em class="me"> </em>例如<a class="ae kv" href="https://www.votchallenge.net/" rel="noopener ugc nofollow" target="_blank"> VOT 挑战</a></li><li id="3b63" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">多目标跟踪:在整个视频序列中跟踪多个感兴趣的目标。例如<a class="ae kv" href="https://motchallenge.net/" rel="noopener ugc nofollow" target="_blank"> MOT 挑战</a></li></ol><h2 id="4ad6" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">研究趋势</h2><p id="6d42" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">用于解决对象跟踪的一些著名的经典 CV 算法是:</p><ol class=""><li id="58c4" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr nk nl nm nn bi translated"><a class="ae kv" href="https://docs.opencv.org/master/d7/d00/tutorial_meanshift.html" rel="noopener ugc nofollow" target="_blank">均值漂移</a></li><li id="15cc" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae kv" href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html" rel="noopener ugc nofollow" target="_blank">光流</a></li><li id="f238" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae kv" href="https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/" rel="noopener ugc nofollow" target="_blank">卡尔曼滤波器</a></li></ol><p id="ca9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中最著名的多目标跟踪算法<a class="ae kv" href="https://arxiv.org/abs/1602.00763" rel="noopener ugc nofollow" target="_blank"> SORT </a>以卡尔曼滤波器为核心，非常成功。</p><p id="88b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着深度学习时代的到来，非常创新的研究进入了社区，DL 在公共跟踪挑战方面成功地胜过了经典的 CV 方法。尽管 DL 在公共挑战上取得了巨大成功，但它仍在努力为现实世界的问题陈述提供通用的解决方案。</p><h1 id="e6a0" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">深度模型的挑战💭</h1><p id="9c19" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">当训练深度 CNN 模型时，我们面临的主要挑战之一是训练数据。</p><ul class=""><li id="4285" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr of nl nm nn bi translated"><strong class="ky ir">训练数据:</strong>深度学习方法需要大量数据，这几乎每次都会成为瓶颈。此外，像多目标跟踪这样的任务很难注释，并且这个过程变得不切实际且昂贵。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/dc0ffd4f29fe15ccd3899b71c82d00b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/1*oOAJFSCI-euruiYRKG8dgg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">DL 模型需要大量数据</p></figure><h1 id="8849" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">自我监督学习营救😯</h1><p id="8ca0" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">我们都知道<a class="ae kv" href="https://blogs.nvidia.com/blog/2018/08/02/supervised-unsupervised-learning/#:~:text=In%20a%20supervised%20learning%20model,and%20patterns%20on%20its%20own." rel="noopener ugc nofollow" target="_blank">监督和非监督学习</a>技术。这是一种相当新的类型，被称为自我监督学习。在这些类型的学习中，我们试图利用数据中已经存在的信息，而不是任何外部标签，或者有时我们说模型自己学习。在现实中，我们所做的是训练 CNN 模型来完成一些其他的任务，间接帮助我们实现我们的目标，模型会自我监督。这些任务被称为“<em class="me">代理任务</em>或“<em class="me">借口任务</em>”。<br/>代理任务的几个例子是:</p><ul class=""><li id="ab26" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr of nl nm nn bi translated"><strong class="ky ir">着色</strong></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/8b15ed00c6f952e511622020d13cd7be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IrkcjQEtLnMFMr1KJGHQnQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CNN 模型学习从灰度图像预测颜色。[ <a class="ae kv" href="https://arxiv.org/abs/1603.08511" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><ul class=""><li id="7946" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr of nl nm nn bi translated"><strong class="ky ir">将图像补丁放置在正确的位置</strong></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/0bd0111f6c3b243b6cda942716f625c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V9aYEoJyyU6wi04S7Xs-ww.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">这些小块是从图像中提取出来的，并被混洗。该模型学习解决这个拼图，并按照正确的顺序排列瓷砖，如图 3 所示。[ <a class="ae kv" href="https://arxiv.org/abs/1603.09246" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><ul class=""><li id="220b" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr of nl nm nn bi translated"><strong class="ky ir">按照正确的顺序放置框架</strong></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/a20854a163cc1654b8386e250059a47b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MRARnK-ZtcN-sChDcwbPUw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">该模型学习对视频序列中的混洗帧进行排序。[ <a class="ae kv" href="https://arxiv.org/abs/1708.01246" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="91b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">许多这样的任务可以用作计算机视觉问题的代理任务。这种培训的一个主要好处是，培训不需要手动注释数据，并且适合解决现实生活中的用例。</p><h1 id="7125" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">通过视频彩色化的自我监督跟踪</h1><p id="4be4" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">我们已经看到了什么是自我监督模型，你一定已经猜到了我们将使用着色作为我们的代理任务的名称。</p><div class="oi oj gp gr ok ol"><a href="https://arxiv.org/abs/1806.09594" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">通过给视频着色，跟踪出现了</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">我们使用大量未标记的视频来学习模型，以便在没有人工监督的情况下进行视觉跟踪。我们…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">arxiv.org</p></div></div></div></a></div><h2 id="13a9" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">介绍</h2><p id="4804" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">着色是我们的<em class="me">代理任务</em>或<em class="me">借口任务</em>，物体跟踪是<em class="me">主任务</em>或<em class="me">下游任务</em>。大规模无标签视频用于训练模型，没有任何单个像素被人类标注。视频的时间相干性用于使模型学习给灰度视频着色。这可能看起来令人困惑，但坚持下去，我会把事情弄清楚。</p><h2 id="3ed2" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">模型将如何学习跟踪？</h2><p id="fbce" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">我们将取两个帧一个目标帧(在时间 t)和一个参考帧(在时间 t-1)，并通过模型。期望该模型根据参考帧的颜色的先验知识来预测目标帧的颜色。这样，模型内部学习指向正确的区域，以便从参考帧复制颜色，如图所示。这种指向机制可以用作推断过程中的跟踪机制。我们将很快看到如何做到这一点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/7d208e6f15e3f56de0d2a9f2a050df52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*-KTE9-7G1LuAzGx4Ysenog.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">该模型接收一个彩色帧和一个灰度视频作为输入，并预测下一帧的颜色。该模型学习从参考帧复制颜色，这使得能够在没有人类监督的情况下学习跟踪机制[ <a class="ae kv" href="https://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="7ae6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们不复制网络中的颜色，而是训练我们的 CNN 网络来学习目标帧的像素和参考帧的像素之间的相似性<em class="me">(相似性是灰度像素之间的相似性)</em>，然后当与参考帧的真实颜色线性组合时，该相似性矩阵给出预测的颜色。数学上，让<em class="me"> Cᵢ </em>是参考帧中每个像素<em class="me"> i </em>的真实颜色，让<em class="me"> Cⱼ </em>是目标帧中每个像素<em class="me"> j </em>的真实颜色。该模型给出了目标帧和参考帧之间的相似性矩阵<em class="me"> Aᵢⱼ </em>。我们可以通过线性组合得到预测的颜色<em class="me"> yᵢ </em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/a18a4e1954633b10400c65228d72f292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XV4vsKMjrjQshyPN6JGsDQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">[ <a class="ae kv" href="https://arxiv.org/abs/1806.09594" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/9738f71fe402735c62086798ca90f056.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*dNIBw6xbOhcS0JB3EtCDKA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd ox">等式 1 </strong>:预测颜色与参考颜色的线性组合</p></figure><h2 id="fa70" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">相似矩阵怎么算？</h2><p id="f7be" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">当通过模型时，图像、参考帧和目标帧都学习对每个像素的低级嵌入，这里<em class="me"> fᵢ </em>是对参考帧中的像素<em class="me"> i </em>的嵌入，类似地，<em class="me"> fⱼ </em>是对目标帧中的像素<em class="me"> j </em>的嵌入。那么相似性矩阵可以通过下式计算:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/b6b718c6a895b9329ee513752823f464.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*SO70rzUXidNUn4wpqr_vBw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd ox">等式 2: </strong>内部<br/>产品相似度由 softmax 归一化</p></figure><p id="f882" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相似性矩阵中的每一行表示参考帧的所有像素<em class="me"> i </em>和目标帧的像素<em class="me"> j </em>之间的相似性，因此为了使总权重为 1，我们对每一行应用 softmax。</p><pre class="kg kh ki kj gt oz pa pb pc aw pd bi"><span id="e62b" class="nt mj iq pa b gy pe pf l pg ph">Lets look an example with dimension to make it clear,we try to find a similarity matrix of 1 pixel from target frame.<br/>An illustration of this example is shown below.</span><span id="c273" class="nt mj iq pa b gy pi pf l pg ph">Consider reference image and target image, size (5, 5) =&gt; (25,1)</span><span id="03a2" class="nt mj iq pa b gy pi pf l pg ph">for each pixel, cnn gives embedding of size (64, 1)<br/><em class="me">fᵢ</em>, embedding for reference frame, size (64, 25)<br/><em class="me">fⱼ</em>, embedding for target frame, size (64, 25)<br/><em class="me">at j=2 f₂</em>,  embedding for 3rd pixel in target frame, size (64, 1)</span><span id="3f8a" class="nt mj iq pa b gy pi pf l pg ph"><strong class="pa ir">Similarity Matrix,</strong> between reference frame and target pixel, j=2<br/> <em class="me">Aᵢ₂</em> =softmax <em class="me">(fᵢᵀ x f₂)</em> , size (25, 64) <em class="me">x</em> (64, 1) =&gt; (25,1) =&gt;   (5, 5)<br/>we get a similarity between all the ref pixels and a target pixel at j=2.</span><span id="f580" class="nt mj iq pa b gy pi pf l pg ph"><strong class="pa ir">Colorization, </strong>To copy the color (here, colours are not RGB but quantized colour of with 1 channel) from reference frame,<br/><em class="me">cᵢ</em>, Colors of reference frame size (5, 5) =&gt; (25, 1)<br/><em class="me">Aᵢ₂</em>, Similarity matrix, size (5, 5) =&gt; (1, 25)</span><span id="c174" class="nt mj iq pa b gy pi pf l pg ph"><strong class="pa ir">Predicted color</strong> at j=2, <br/><em class="me">y₂ = Aᵢ₂ x cᵢ</em>, size (1, 25) <em class="me">x </em>(25, 1) =&gt; (1, 1)</span><span id="e637" class="nt mj iq pa b gy pi pf l pg ph">From the similarity matrix in below figure, we can see reference color at i=1 is dominant(0.46), thus we have a color copied for target, j=2 from reference, i=1</span><span id="e252" class="nt mj iq pa b gy pi pf l pg ph"><strong class="pa ir">PS:<br/></strong>1. ᵀ denotes transpose<br/>2. matrix indices starts from 0</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/e2b2ad6e48e6099e18d1b9e9883df396.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9iY7DPZdhPqlqQu-wbUIPQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(a)示出了大小为(5，5)的 2 个帧，(b)参考帧嵌入和 j =2 处的目标像素嵌入的内积，(softmax 之后的相似性矩阵，(d)相似性矩阵和参考帧的真实颜色的线性组合[ <a class="ae kv" href="https://github.com/hyperparameters/tracking_via_colorization" rel="noopener ugc nofollow" target="_blank">源</a> ]</p></figure><p id="4eb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，对于目标帧中的每个目标像素(<em class="me"> (5，5)= &gt; 25 像素)，</em>我们将具有大小为(5，5)的相似性矩阵，即大小为(5，5，25) = (25，25)的完整相似性矩阵<em class="me"> Aᵢⱼ </em>。<br/>我们将在实现中使用(256 x 256)图像扩展相同的概念。</p><h2 id="e6bc" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">图像量化</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pk"><img src="../Images/51ffe926531a957fb9e00ba359f58ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MkTs6qM5qpwn6XQ4b8oJNw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">第一行显示原始帧，第二<br/>行显示来自 Lab space 的 ab 颜色通道。第三行将颜色空间量化为离散的面元，并扰动颜色以使效果更加明显。[ <a class="ae kv" href="https://arxiv.org/abs/1806.09594" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="7e99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">颜色是低空间频率，所以我们可以使用低分辨率的帧。我们不需要<em class="me"> C(255，3) </em>颜色组合，所以我们创建了 16 个聚类，并将颜色空间量化到这些聚类中。现在我们只有 16 种独特的颜色，(见上图第三栏)。聚类是使用 k-means 完成的。16 个簇会丢失一些颜色信息，但足以识别物体。我们可以增加聚类的数量来提高着色的精度，但代价是增加计算量。</p><h2 id="8295" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">为什么 LAB 色彩空间优于 RGB？</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/b05855c75b5cea4b688005a10ead3b30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*SEPc0kBGuUQ5kbqKKtOwFA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源</p></figure><p id="571d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了将图像量化成簇，我们将使用 LAB 色彩空间的 AB 通道，而不是 RGB 色彩空间。上图显示了 RGB 和 LAB 通道间相关性，我们可以从图中得出以下结论</p><ul class=""><li id="3478" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr of nl nm nn bi translated">RGB 往往比 LAB 有更多的相关性。</li><li id="1773" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr of nl nm nn bi translated">LAB 将迫使模型学习方差，它将迫使学习更健壮的表示，而不是依赖于局部颜色信息</li></ul><p id="4b7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以使用 sklearn 的<a class="ae kv" href="https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/" rel="noopener ugc nofollow" target="_blank"> KMeans </a>包来完成集群。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pm pn l"/></div></figure><p id="7ee8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个类将被用来制作颜色簇，我们将把它作为泡菜来保存。</p><h1 id="1b8e" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">履行💻</h1><blockquote class="mb mc md"><p id="d8bc" class="kw kx me ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated"><strong class="ky ir">注意</strong>:我用 pytorch 实现，它遵循(N，C，H，W)格式。在处理矩阵整形时，请记住这一点。如果你对形状有任何疑问，请联系我们。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/deea1b6d51d18480483377f1e4f7525c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*QgkHJFVN2xZNr9dzPhII7g.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/305123c07e616c346781f4d9a3614803.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*3CKC5jKEJb-IHcwVz7oQ5w.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">该模型从参考帧学习给视频帧着色。[ <a class="ae kv" href="https://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="ac9d" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">投入</h2><p id="e682" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">该模型的输入是下采样到<br/> 256 × 256 的四个灰度视频帧。三个参考帧和一个目标帧。</p><h2 id="4b77" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">预处理</h2><p id="4943" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">首先，我们将把所有的训练视频压缩到 6 帧/秒。然后预处理帧以创建两个不同的集合。一个用于 CNN 模型，另一个用于着色任务。</p><pre class="kg kh ki kj gt oz pa pb pc aw pd bi"><span id="93e1" class="nt mj iq pa b gy pe pf l pg ph">- Video fps is reduced to 6 fps</span><span id="8d78" class="nt mj iq pa b gy pi pf l pg ph"><strong class="pa ir">SET 1 - for CNN Model</strong></span><span id="5234" class="nt mj iq pa b gy pi pf l pg ph">- Down sampled to 256 x 256</span><span id="e61e" class="nt mj iq pa b gy pi pf l pg ph">- Normalise to have intensities between [-1, 1]</span><span id="0111" class="nt mj iq pa b gy pi pf l pg ph"><strong class="pa ir">SET 2 - for Colourization</strong></span><span id="c4b1" class="nt mj iq pa b gy pi pf l pg ph">- Convert to LAB colour space</span><span id="7294" class="nt mj iq pa b gy pi pf l pg ph">- Downsample to 32 x 32</span><span id="ea79" class="nt mj iq pa b gy pi pf l pg ph">- Quantize in 16 clusters using k-means</span><span id="262d" class="nt mj iq pa b gy pi pf l pg ph">- Create one-hot vector corresponding to the nearest cluster centroid</span></pre><h2 id="9ada" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">模型架构</h2><p id="b601" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">使用的主干是<em class="me"> ResNet-18 </em>，因此结果与其他方法相当。ResNet-18 的最后一层被更新以给出 32 x 32 x 256 的尺寸输出。ResNet-18 的输出然后被传递到 3D-Conv 网络，最终输出是 32 x 32 x 64。(<em class="me">下面的代码块显示了从 ResNet-18 网络</em>获取输入的 3D 网络)</p><h2 id="1544" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">培养</h2><p id="0950" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">培训可分为以下 3 个步骤:</p><ol class=""><li id="54a1" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr nk nl nm nn bi translated"><strong class="ky ir">网络通道</strong> <br/>我们将使用预处理帧的集合 1，即大小为(256 x 256)的 4 个灰度帧通过网络，以获得具有 64 个通道的(32 x 32)空间图。这可以解释为对(32 x 32)图像的每个像素进行 64 维嵌入。因此，我们有四个这样的像素级嵌入，三个用于参考图像，一个用于目标图像。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pm pn l"/></div></figure><ol class=""><li id="e61f" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr nk nl nm nn bi translated"><strong class="ky ir">相似性矩阵<br/> </strong>利用这五个嵌入，我们找到参考帧和目标帧之间的相似性矩阵。对于目标帧中的像素，我们将通过 softmax 将所有三个参考帧中的所有像素的相似性值归一化为 1。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pm pn l"/></div></figure><p id="627a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 3。着色<br/> </strong>我们将使用预处理帧的集合 2，即四个下采样为(32 x 32)并量化的帧用于着色。将三个参考帧与相似性矩阵组合以获得预测的量化帧。我们用预测的颜色<em class="me">找到<a class="ae kv" href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <em class="me">交叉熵损失</em> </a>(记得我们将帧量化为 16 个簇，现在我们有 16 个类别。我们在这些颜色上发现了多类别交叉熵损失。)</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pm pn l"/></div></figure><h2 id="0e20" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">推理</h2><div class="kg kh ki kj gt ab cb"><figure class="po kk pp pq pr ps pt paragraph-image"><img src="../Images/789bf969e808af6e7792b8f4d51c4ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*BIwajkeu78V1JzyhUMqKKw.gif"/></figure><figure class="po kk pp pq pr ps pt paragraph-image"><img src="../Images/6b083438f770b7edfc83d4b790a1dc22.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*H-elTOPjSOK3ukOXZ0WK6A.gif"/></figure><figure class="po kk pp pq pr ps pt paragraph-image"><img src="../Images/42d59420e4c23e7b474689f1277ab843.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*4_csmyYhrK6uA1jV78-g5g.gif"/><p class="kr ks gj gh gi kt ku bd b be z dk pu di pv pw translated">跟踪预测示例[ <a class="ae kv" href="https://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure></div><p id="84f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在学习了着色的任务之后，我们有了一个可以为一对目标和参考帧计算相似矩阵<em class="me"> Aᵢⱼ </em>的模型。现在对于实际的跟踪任务，我们利用了我们的模型在标签空间中是非参数的特性。我们简单地重复使用 e <em class="me">方程 1 </em>来传播，但是不是传播颜色，而是传播类别的分布。对于第一帧，我们有地面真实遮罩，我们将把所有实例遮罩安排为独热向量 c <em class="me"> ᵢ(这类似于训练期间使用的量化颜色的独热向量)</em>。将 c <em class="me"> ᵢ </em>与我们的相似性矩阵<em class="me"> Aᵢⱼ </em>结合起来，找到蒙版<em class="me">、</em>的新位置，但是记住预测 c <em class="me"> ⱼ </em>在随后的帧中会变软，表示模型的置信度。要做一个艰难的决定，我们可以简单地取最有信心的一类。推理的算法将是:</p><pre class="kg kh ki kj gt oz pa pb pc aw pd bi"><span id="c7be" class="nt mj iq pa b gy pe pf l pg ph"><strong class="pa ir">WHILE (</strong>target frame, reference frames) in the video</span><span id="4c76" class="nt mj iq pa b gy pi pf l pg ph">step 1. Pass the target and reference frames through CNN model</span><span id="eb15" class="nt mj iq pa b gy pi pf l pg ph">step 2. Find Similarity Matrix</span><span id="7edf" class="nt mj iq pa b gy pi pf l pg ph">step 3. Take ground truth object masks as one-hot encoding</span><span id="cddf" class="nt mj iq pa b gy pi pf l pg ph">step 4. Linear combine the object masks with similarity matrix</span><span id="91a7" class="nt mj iq pa b gy pi pf l pg ph">step 5. Update ground truth object masks by predicted masks</span></pre><h2 id="d77b" class="nt mj iq bd mk nu nv dn mo nw nx dp ms lf ny nz mu lj oa ob mw ln oc od my oe bi translated">故障模式</h2><p id="a3e5" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">让我们讨论一下在某些情况下模型什么时候会失败，这主要是着色失败的情况，这意味着着色与跟踪有很高的相关性。<br/>在以下情况下会发现一些故障:</p><ul class=""><li id="6b51" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr of nl nm nn bi translated">当视频中的光线剧烈或频繁变化时</li><li id="450b" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr of nl nm nn bi translated">该方法成功地跟踪具有轻微到中等遮挡物体，但是当物体经历较大遮挡时仍然失败</li><li id="edad" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr of nl nm nn bi translated">物体尺寸的突然变化</li></ul></div><div class="ab cl px py hu pz" role="separator"><span class="qa bw bk qb qc qd"/><span class="qa bw bk qb qc qd"/><span class="qa bw bk qb qc"/></div><div class="ij ik il im in"><h1 id="2cdd" class="mi mj iq bd mk ml qe mn mo mp qf mr ms jw qg jx mu jz qh ka mw kc qi kd my mz bi translated">结论</h1><p id="fb6c" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">在这里，我们看到了一个模型如何在没有任何手动注释数据的情况下从自己的模型中学习。我们学习了如何在一些代理任务上训练 CNN 模型，并利用这种学习来完成实际的任务。我们使用着色作为代理，但不限于此，各种新的方法作为新的<a class="ae kv" href="https://www.fast.ai/2020/01/13/self_supervised/" rel="noopener ugc nofollow" target="_blank">代理任务</a>出现。自我监督的方法是当前的需要，它们可以消除现实世界用例的昂贵数据收集的主要限制。这个模型还不能击败目前的 SOTA 监督模型，但优于许多其他模型。</p><p id="370c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该方法在方法和灵活性方面非常有前途。由于自身的优势，自监督模型将很快成为解决最大似然问题的首选。这篇文章是基于“<a class="ae kv" href="https://arxiv.org/abs/1806.09594" rel="noopener ugc nofollow" target="_blank"> Google Research </a>”的研究和他们所有的功劳。我试图根据我的知识和理解来解释这项研究。</p></div><div class="ab cl px py hu pz" role="separator"><span class="qa bw bk qb qc qd"/><span class="qa bw bk qb qc qd"/><span class="qa bw bk qb qc"/></div><div class="ij ik il im in"><h1 id="6bef" class="mi mj iq bd mk ml qe mn mo mp qf mr ms jw qg jx mu jz qh ka mw kc qi kd my mz bi translated">关于我</h1><p id="ec87" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">我是 Tushar Kolhe，在 Fynd 担任深度学习工程师。我的兴趣是建立解决现实世界问题的计算机视觉模型。<br/>通过<a class="ae kv" href="mailto:tushark.engg@outlook.com" rel="noopener ugc nofollow" target="_blank">电子邮件</a>寻求任何建议或帮助。</p><blockquote class="qj"><p id="215d" class="qk ql iq bd qm qn qo qp qq qr qs lr dk translated">我坚信，如果有足够的动力，任何人都可以学到任何东西，深度模型也是如此😛。</p></blockquote><h1 id="30f6" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw qt jx mu jz qu ka mw kc qv kd my mz bi translated">附录</h1><p id="e0d0" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">目标跟踪领域的一些有趣研究:</p><ol class=""><li id="0935" class="nf ng iq ky b kz la lc ld lf nh lj ni ln nj lr nk nl nm nn bi translated"><em class="me">简单的在线实时跟踪，具有深度关联度量。【</em> <a class="ae kv" href="https://arxiv.org/abs/1703.07402" rel="noopener ugc nofollow" target="_blank"> <em class="me">论文</em></a><em class="me">→扩展排序</em></li><li id="aeb9" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><em class="me">没有花里胡哨的跟踪。【</em> <a class="ae kv" href="https://arxiv.org/pdf/1903.05625.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="me">论文</em></a><em class="me">】</em></li><li id="d703" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><em class="me">单次多目标跟踪的简单基线- FairMOT </em>。<em class="me"/><a class="ae kv" href="http://arxiv.org/abs/2004.01888" rel="noopener ugc nofollow" target="_blank"><em class="me">论文</em></a><em class="me"/></li><li id="c3ed" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><em class="me">学习用于多目标跟踪的神经解算器。【</em> <a class="ae kv" href="https://arxiv.org/abs/1912.07515" rel="noopener ugc nofollow" target="_blank"> <em class="me">论文</em></a><em class="me">】</em></li></ol><p id="f80c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以找到更多有趣的研究，评论你觉得有趣的研究。</p></div><div class="ab cl px py hu pz" role="separator"><span class="qa bw bk qb qc qd"/><span class="qa bw bk qb qc qd"/><span class="qa bw bk qb qc"/></div><div class="ij ik il im in"><h1 id="ea36" class="mi mj iq bd mk ml qe mn mo mp qf mr ms jw qg jx mu jz qh ka mw kc qi kd my mz bi translated">参考</h1><ol class=""><li id="0e2c" class="nf ng iq ky b kz na lc nb lf qw lj qx ln qy lr nk nl nm nn bi translated">冯德里克、施里瓦斯塔瓦、法蒂、瓜达拉马和墨菲(2018 年)。跟踪是通过给视频着色而出现的。<em class="me"> ArXiv，abs/1806.09594 </em>。</li><li id="c3cb" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K" rel="noopener ugc nofollow" target="_blank">何</a>、<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+X" rel="noopener ugc nofollow" target="_blank">、</a>、<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ren%2C+S" rel="noopener ugc nofollow" target="_blank">、</a>、<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+J" rel="noopener ugc nofollow" target="_blank">、</a>。用于图像识别的深度残差学习。<em class="me">ArXiv</em><a class="ae kv" href="https://arxiv.org/abs/1512.03385v1" rel="noopener ugc nofollow" target="_blank"><em class="me">ABS/1512.03385</em>T49】</a></li><li id="2cec" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+R" rel="noopener ugc nofollow" target="_blank">张曦轲</a>，<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Isola%2C+P" rel="noopener ugc nofollow" target="_blank">菲利普·伊索拉</a>，<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Efros%2C+A+A" rel="noopener ugc nofollow" target="_blank">阿列克谢·阿·埃夫罗斯</a>。彩色图像彩色化。<em class="me">ArXiv</em>T58<em class="me">ABS/1603.08511</em>T61】</li><li id="6032" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated">迈赫迪·诺鲁齐，<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Favaro%2C+P" rel="noopener ugc nofollow" target="_blank">保罗·法瓦罗</a>。通过解决拼图游戏实现视觉表征的无监督学习。<em class="me"> ArXiv abs/1603.09246 </em></li><li id="24a1" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee%2C+H" rel="noopener ugc nofollow" target="_blank">李新英</a>，<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang%2C+J" rel="noopener ugc nofollow" target="_blank">贾-黄斌</a>，<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Singh%2C+M" rel="noopener ugc nofollow" target="_blank">辛格</a>，<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+M" rel="noopener ugc nofollow" target="_blank">明-杨玄</a>。通过排序序列的无监督表示学习。<em class="me"> ArXiv abs/1708.01246 </em></li><li id="4b8c" class="nf ng iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><a class="ae kv" href="https://www.fast.ai/2020/01/13/self_supervised/" rel="noopener ugc nofollow" target="_blank">https://www.fast.ai/2020/01/13/self_supervised/</a></li></ol></div></div>    
</body>
</html>