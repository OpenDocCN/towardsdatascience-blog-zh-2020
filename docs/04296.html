<html>
<head>
<title>How I Built a Simple Fake News Detector on Amazon SageMaker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我如何在Amazon SageMaker上建立一个简单的假新闻检测器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-i-built-a-simple-fake-news-detector-on-amazon-sagemaker-808bf4e0c490?source=collection_archive---------36-----------------------#2020-04-18">https://towardsdatascience.com/how-i-built-a-simple-fake-news-detector-on-amazon-sagemaker-808bf4e0c490?source=collection_archive---------36-----------------------#2020-04-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8201" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近，我决定注册一个Udacity纳米学位，因为这个想法在我脑海里盘旋了很久。</p><p id="acf4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在过去的两个月里，每天晚饭后和周末，我都跟着机器学习工程师Nanodegree课程，我遇到了亚马逊SageMaker。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/0d7bfa15a7f217dc5b569fedb04052c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*N1hCUa5KGx7xP6BN.jpg"/></div><p class="kt ku gj gh gi kv kw bd b be z dk translated">归功于aws.amazon.com</p></figure><p id="6a3e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Amazon SageMaker是一项完全托管的服务，允许数据科学家和开发人员大规模构建、训练和部署机器学习模型。</p><p id="02f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">令人惊讶的是，你真的可以在同一个平台上执行整个端到端的数据科学管道。<br/>事实上，通过Amazon SageMaker，您可以在一组不同的机器上创建Jupyter笔记本实例，这些机器基于计算(CPU/GPU)、RAM和网络功能而有所不同。</p><p id="e712" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以从导入数据、探索和清理数据开始，来训练模型并快速将其投入生产环境。</p><p id="d433" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与SageMaker共同的工作流程(至少从我的小经验中学到的是这样的):</p><ul class=""><li id="343e" class="kx ky iq jp b jq jr ju jv jy kz kc la kg lb kk lc ld le lf bi translated"><strong class="jp ir">数据整合与处理</strong></li><li id="9510" class="kx ky iq jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated">整合来自任何来源的数据集；</li><li id="28d7" class="kx ky iq jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated">探索它，做可视化和汇总统计，了解数据；</li><li id="f5d2" class="kx ky iq jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated">如有必要，清理必须清理的部分，预处理和设计您的特征；</li><li id="c468" class="kx ky iq jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated">将处理后的数据保存到S3存储桶中，该存储桶可以是默认的SageMaker实例存储桶，也可以是您选择的其他存储桶。</li><li id="6fef" class="kx ky iq jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated"><strong class="jp ir">模型搭建&amp;部署</strong></li><li id="3feb" class="kx ky iq jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated">要建立模型，Amazon SageMaker自带了一套有监督和无监督的模型，但是你也可以为你的定制模型提供一个你自己选择的框架(Scikit-learn，TensorFlow，MXNet…)连同一个训练脚本；</li><li id="8c4a" class="kx ky iq jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated">使用您在S3上保存的数据在一个或多个计算实例上训练模型</li><li id="ac24" class="kx ky iq jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated">在SageMaker端点上部署估计器以进行推断</li></ul><p id="7a61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我发现SageMaker对于数据科学项目来说是一个非常有价值的选择。从现在开始，我将与你分享我在SageMaker上进行Udacity Capstone项目的最后一次体验。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="5377" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个项目处理假/真新闻检测。可以毫无疑问地插入到自然语言处理问题的语境中。<br/>当我在Kaggle上导航时，我发现了这个有趣的<a class="ae ls" href="https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset/kernels" rel="noopener ugc nofollow" target="_blank">数据集</a>。<br/>该数据集由2个CSV文件(真实、虚假新闻)组成，其中存储了文章的标题、文章、日期和主题。</p><h1 id="2843" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">问题陈述</h1><p id="dd3d" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">所以，这个问题可以这样陈述:给定一篇文章的文本，我希望算法能够预测它指的是真的还是假的新闻。特别是，我将问题的解决方案组织如下:<br/>·来自不同来源(CSV)的数据将被标记和堆叠；<br/>“标题”和“文章”等文本特征被堆叠后，将被处理，以便生成有意义的词汇(没有标签、URL、奇怪的标点符号和停用词)<br/>从这里，可以遵循两条道路，这取决于算法的选择。<br/>如果使用机器学习算法，则有必要创建文本的单词包表示，或者通过使用单词计数，一种术语频率逆文档频率的热编码，可以与其他特征(例如，从日期中提取)一起使用来训练模型；<br/>相反，如果选择深度学习模型，如递归神经网络，人们可以想到只直接使用填充到相同长度的文本序列，并用word_to_integer词汇表进行映射。<br/>然后，可以训练神经网络来解决具有二元交叉熵损失的二元分类问题。</p><p id="88c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于我的报告长达10页，我将只报告主要步骤:</p><p id="814e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">预处理<br/> </strong>关于LSTM模型的预处理步骤:</p><ul class=""><li id="2ea9" class="kx ky iq jp b jq jr ju jv jy kz kc la kg lb kk lc ld le lf bi translated">我只考虑文章文本作为一个特征，过滤长度在20个单词以下和500个单词以上的文本，以避免空序列或太长的序列。这些文本已经被停用词、奇怪的标点符号过滤掉，并被转换成小写字母。</li><li id="9b4a" class="kx ky iq jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated">我使用来自Sklearn的train_test_split来拆分训练、验证和测试数据集中的数据</li><li id="c6af" class="kx ky iq jp b jq lg ju lh jy li kc lj kg lk kk lc ld le lf bi translated">我将keras的标记器应用于训练集，然后我使用它来转换验证和测试数据集(以避免数据泄漏)，然后将所有序列填充到max_len为500</li></ul><pre class="km kn ko kp gt mw mx my mz aw na bi"><span id="5447" class="nb lu iq mx b gy nc nd l ne nf">from tf.keras.preprocessing.text import Tokenizer<br/>from tf.keras.preprocessing import sequence</span><span id="914d" class="nb lu iq mx b gy ng nd l ne nf">tokenizer = Tokenizer(num_words=80000)<br/>text_tokenizer.fit_on_texts(X_train['article'].astype(str))</span><span id="b3f1" class="nb lu iq mx b gy ng nd l ne nf">X_train = text_tokenizer.texts_to_sequences(X_train)<br/>X_val = text_tokenizer.texts_to_sequences(X_val)<br/>X_test = text_tokenizer.texts_to_sequences(X_test)</span><span id="5356" class="nb lu iq mx b gy ng nd l ne nf">X_train = sequence.pad_sequences(X_train, maxlen=500, padding='post')<br/>X_val= sequence.pad_sequences(X_val, maxlen=500, padding='post')<br/>X_test= sequence.pad_sequences(X_test, maxlen=500, padding='post')</span></pre><p id="638a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">型号</strong></p><p id="4bc6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在培训脚本中(请记住我在SageMaker上),我定义了环境变量，您可以在这里定义模型结构，对其进行拟合，并在S3上保存其工件。这是我使用的网络结构(Keras)。</p><pre class="km kn ko kp gt mw mx my mz aw na bi"><span id="92ef" class="nb lu iq mx b gy nc nd l ne nf">from tf.keras.layers import Embedding, Bidirectional, LSTM,Dense,Activation<br/>from tf.keras.models import Sequential</span><span id="0ffd" class="nb lu iq mx b gy ng nd l ne nf">def RNN():</span><span id="8ca3" class="nb lu iq mx b gy ng nd l ne nf">model = Sequential()<br/>    layer = model.add(Embedding(80000, 128, input_length = 500))<br/>    layer = model.add(Bidirectional(LSTM(128))<br/>    layer = model.add(Dense(128))   <br/>    layer = model.add(Activation('relu'))<br/>    layer = model.add(Dense(1))<br/>    layer = model.add(Activation('sigmoid'))</span><span id="e4f3" class="nb lu iq mx b gy ng nd l ne nf">    return model</span></pre><p id="3cad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过在LSTM层添加双向功能，我将精确度提高了15%以上。</p><p id="aebd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后您添加代码以适应并保存模型；该代码将由SageMaker在培训工作中调用。</p><pre class="km kn ko kp gt mw mx my mz aw na bi"><span id="8c73" class="nb lu iq mx b gy nc nd l ne nf">model.fit(train_X,<br/>          train_y, <br/>          batch_size=256,<br/>          epochs=args.n_epochs,<br/>          validation_data=(val_X,val_y))</span><span id="ce6a" class="nb lu iq mx b gy ng nd l ne nf">model_path = '/opt/ml/model/'<br/>model.save(os.path.join(model_path,'bi_lstm/1'), save_format='tf')</span></pre><p id="001b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相反，在实例端，我实例化了一个TensorFlow对象，其中设置了训练脚本的路径、我要选择的实例的数量和类型、IAM角色和超参数:</p><pre class="km kn ko kp gt mw mx my mz aw na bi"><span id="420f" class="nb lu iq mx b gy nc nd l ne nf">input_channels = {"train":train_data,<br/>                  "validation":val_data}</span><span id="067c" class="nb lu iq mx b gy ng nd l ne nf">from sagemaker.tensorflow import TensorFlow</span><span id="2ee9" class="nb lu iq mx b gy ng nd l ne nf">estimator = TensorFlow(entry_point = 'source_train/train_keras_lstm.py',<br/>                       train_instance_type='ml.p2.xlarge',<br/>                       train_instance_count=1, <br/>                       role=role,<br/>                       framework_version='2.1.0',<br/>                       py_version='py3',<br/>                       hyperparameters={"n_epochs":3}</span></pre><p id="b62b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如你所看到的，我选择了一个' ml.p2.xlarge '实例，这是一个带有GPU访问的亚马逊入门级机器。</p><p id="9a92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用相同的策略，我在训练模型后部署了它:</p><pre class="km kn ko kp gt mw mx my mz aw na bi"><span id="d719" class="nb lu iq mx b gy nc nd l ne nf">predictor = estimator.deploy(initial_instance_count=1,<br/>                             instance_type='ml.c4.xlarge')</span></pre><p id="5ee2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并对测试集执行推断(这可以通过predict() API来完成，也可以通过在数据较大的情况下创建批处理转换作业来完成):</p><pre class="km kn ko kp gt mw mx my mz aw na bi"><span id="0965" class="nb lu iq mx b gy nc nd l ne nf">from sklearn.metrics import accuracy_score</span><span id="5b5f" class="nb lu iq mx b gy ng nd l ne nf">preds_df = pd.DataFrame(predictor.predict(X_test)<br/>target_preds = pd.concat([y_test,preds_df], axis=1)<br/>target_preds.columns=['targetClass','preds']</span><span id="607c" class="nb lu iq mx b gy ng nd l ne nf">print(accuracy_score(target_preds['targetClass'],<br/>                     target_preds['preds']))</span><span id="7aa8" class="nb lu iq mx b gy ng nd l ne nf">0.986639753940792</span></pre><p id="c2c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我在测试集上获得了98%的准确率。</p><p id="dca2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了模型本身，我希望我引起了你对SageMaker功能的注意。</p><p id="effa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想看完整个步骤，看报告或者看一下培训脚本，<a class="ae ls" href="https://github.com/guidiandrea/udacityCapstone-FakeNewsDetector" rel="noopener ugc nofollow" target="_blank">这是该项目的GitHub repo </a>。</p><p id="e7aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下次见，再见，感谢阅读！</p></div></div>    
</body>
</html>