<html>
<head>
<title>Are Deeper Networks Better? —A Case Study</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">网络越深越好吗？案例研究</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/are-deeper-networks-better-a-case-study-6ee9bcb0725b?source=collection_archive---------41-----------------------#2020-05-07">https://towardsdatascience.com/are-deeper-networks-better-a-case-study-6ee9bcb0725b?source=collection_archive---------41-----------------------#2020-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/4f75c958d2e34358763b10f7afb4e400.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qUc6pqsKGaevuuWH"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">约书亚·牛顿在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><h1 id="9d12" class="kn ko jj bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">介绍</h1><p id="35f4" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">你有没有想过建立更深层次的关系网会有什么影响，会带来什么变化？</p><p id="8b72" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我知道我有。</p><p id="3d7c" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我说的是在你的模型中增加额外的层，并仔细检查下游发生了什么变化。</p><p id="694e" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">其他条件不变:</p><ol class=""><li id="18c3" class="mo mp jj ln b lo mj ls mk lw mq ma mr me ms mi mt mu mv mw bi translated">额外的层对你的训练/验证损失曲线有什么影响(如果有的话)？</li><li id="aefc" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">一个额外的层在多大程度上影响你的下游预测结果？</li><li id="ce21" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">输入要素的数量与模型的复杂性之间有什么关系？</li><li id="fd92" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">这里面有什么取舍吗？</li></ol><blockquote class="nc"><p id="87dc" class="nd ne jj bd nf ng nh ni nj nk nl mi dk translated"><strong class="ak">在这篇文章中，我将分享一系列实验和它们各自的结果来回答这些问题。</strong></p></blockquote><p id="37b0" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated">我们言归正传吧！</p><h1 id="ab10" class="kn ko jj bd kp kq nr ks kt ku ns kw kx ky nt la lb lc nu le lf lg nv li lj lk bi translated">实验设置</h1><p id="1c7f" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">在我们开始之前，我需要解释一下所涉及的实验装置。</p><p id="f4c3" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">该设置由3部分组成:</p><ol class=""><li id="f6cb" class="mo mp jj ln b lo mj ls mk lw mq ma mr me ms mi mt mu mv mw bi translated">使用的训练数据集</li><li id="7220" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">为实验构建的模型架构</li><li id="4f5e" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">实验方法</li></ol><h2 id="31fa" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">训练数据</h2><p id="a7ff" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">实验所用的数据集来自一场卡格尔竞赛；<a class="ae jg" href="https://www.kaggle.com/c/tweet-sentiment-extraction/overview" rel="noopener ugc nofollow" target="_blank">推文情感提取</a>。</p><p id="3057" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">目标？</p><p id="a3f1" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">提取出支持特定观点的短语。</p><p id="b40a" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">以下是训练数据的一瞥:</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/dce9272c814c2a74c125632949db0d58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BqMqnSuFEnTI9OnHmYQ1DA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">推特情感提取训练数据(<a class="ae jg" href="https://www.kaggle.com/c/tweet-sentiment-extraction/overview" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="ad36" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我不会深入研究数据准备方法，因为我将在另一篇文章中讲述Kaggle的经历。</p><p id="8302" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">本质上，我们按照<a class="ae jg" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">高音手套方法</a>清理了文本，并为下游命名实体识别(NER)任务创建了自己的标签。</p><p id="c829" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">一旦我们有了建模就绪的数据集，我就开始构建一个基线seq2seq模型。</p><h2 id="5d34" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">模型架构</h2><p id="f9ed" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">模型架构由3个主要部分组成:</p><ol class=""><li id="94e5" class="mo mp jj ln b lo mj ls mk lw mq ma mr me ms mi mt mu mv mw bi translated">预先注意双LSTM</li><li id="cae0" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">多头自关注编码器堆栈</li><li id="0bec" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">关注LSTM</li></ol><p id="c0ea" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">该模型从前置注意双LSTM开始，其输出被馈入编码器堆栈。</p><p id="8de5" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">来自这些编码器堆栈的输出然后被馈送到后注意LSTM，以生成手头的NER任务的顺序预测。</p><p id="1277" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为什么选择编码器堆栈？</p><p id="e84f" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">使用编码器堆栈的灵感来自Vaswani等人的论文“<a class="ae jg" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>”。艾尔。</p><p id="7186" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">本文详细介绍了变压器模型的外观。</p><p id="62c2" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">简而言之，变压器模型由6个编码器堆栈和6个解码器堆栈组成。我不会深入这篇论文的细节，但如果你想知道《变形金刚》 (BERT)中著名的<a class="ae jg" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">双向编码器表示背后的架构。我推荐阅读。</a></p><p id="0166" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">总之，编码器堆栈是这样的:</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/f7325180a0176dba50e9c1d767b151a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*nnybtBNXrBQfDWTPkEiCqQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Vaswani等人的编码器堆栈示例。艾尔(<a class="ae jg" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="14f3" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">如您所见，每个编码器堆栈由两个子层组成。</p><p id="09cd" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">第一个是多头自关注机制，第二个是完全连接的前馈网络。</p><p id="0b7f" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在每个子层周围，使用残差连接，然后进行层归一化。这是有意义的，因为剩余连接将预先注意双LSTM输出与多头自我注意层学习的上下文相加。</p><p id="5dbd" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">那么我们得到的模型架构看起来是什么样的呢？</p><p id="289f" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">如果我把它画出来，它会是这样的:</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/993371b995a2935d8bcb2e5536fe40fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*noHwjVSB7VHmVKJ0jeiIdw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于实验的模型架构</p></figure><p id="37a6" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">作为一个<strong class="ln jk">基线</strong>，该架构仅从一个<strong class="ln jk">单编码器堆栈</strong>开始。</p><p id="bbb4" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">以下是实验中使用的保持不变的层参数:</p><pre class="oj ok ol om gt op oq or os aw ot bi"><span id="6742" class="nw ko jj oq b gy ou ov l ow ox"><strong class="oq jk">Pre-attention Bi-LSTM:<br/></strong>preatn_forward={type="recurrent", n=64, act='tanh', init='xavier', rnnType='LSTM',outputType='samelength', reversed=False, dropout=0.3}</span><span id="44ba" class="nw ko jj oq b gy oy ov l ow ox">preatn_backward={type="recurrent", n=64, act='tanh', init='xavier', rnnType='LSTM',outputType='samelength', reversed=True, dropout=0.3}</span><span id="8358" class="nw ko jj oq b gy oy ov l ow ox">concatenation_layer={type="concat"}</span></pre></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><pre class="op oq or os aw ot bi"><span id="cfb2" class="nw ko jj oq b gy oz pa pb pc pd ov l ow ox"><strong class="oq jk">Encoder Stack:</strong><br/>attention_layer={type="mhattention", n=128, act='gelu', init='xavier', nAttnHeads=16}</span><span id="edbf" class="nw ko jj oq b gy oy ov l ow ox">residual_layer1={type="residual"}<br/>norm_layer1=layer={type="layernorm"}</span><span id="a985" class="nw ko jj oq b gy oy ov l ow ox">fully_connected_layer={type="fc", n=128, act='relu', init='xavier', dropout=0.3}</span><span id="62f2" class="nw ko jj oq b gy oy ov l ow ox">residual_layer2={type="residual"}<br/>norm_layer2=layer={type="layernorm"}</span></pre></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><pre class="op oq or os aw ot bi"><span id="c36d" class="nw ko jj oq b gy oz pa pb pc pd ov l ow ox"><strong class="oq jk">Post-attention LSTM:</strong><br/>postatn_forward={type="recurrent", n=128, act='tanh', init='xavier', rnnType='LSTM',reversed=False, dropout=0.3};</span></pre><h2 id="1fa2" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">实验方法</h2><p id="2567" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">使用的方法如下。在实验案例中，我需要一种方法来:</p><ol class=""><li id="856e" class="mo mp jj ln b lo mj ls mk lw mq ma mr me ms mi mt mu mv mw bi translated">模拟深层网络的模型复杂性</li><li id="8c12" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">模拟不同的输入要素</li><li id="e041" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">使用标准评估指标来衡量预测结果</li></ol><p id="d082" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><strong class="ln jk">模拟模型复杂度</strong></p><p id="8344" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为了系统地增加模型的复杂性，我在模型架构中添加了一个编码器堆栈，同时保持每个实验的所有其他超参数不变。</p><p id="a90d" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">例如，高音扬声器手套25d的单个实验组将具有4种不同的架构。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/8e8ff764bfa1730008b704c2ec32c60a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ueaoj6AdeTJzd4n-kcmF_g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">实验中使用的不同布局配置</p></figure><blockquote class="pf pg ph"><p id="a2b3" class="ll lm pi ln b lo mj lq lr ls mk lu lv pj ml ly lz pk mm mc md pl mn mg mh mi im bi translated">请注意每次实验中编码器堆栈的数量是如何系统地增加的。</p></blockquote><p id="0134" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">使用的超参数也始终保持不变:</p><pre class="oj ok ol om gt op oq or os aw ot bi"><span id="428e" class="nw ko jj oq b gy ou ov l ow ox"><strong class="oq jk">Optimizer hyper-parameters:</strong><br/>miniBatchSize=32<br/>stagnation = 10<br/>algorithm = {method = ‘adam’, gamma = 0.9, beta1 = 0.9, beta2 = 0.999, learningRate = 0.001, clipGradMax = 100, clipGradMin = -100, lrPolicy = ‘step’, stepSize=10}<br/>regL2 = 0.0007<br/>maxEpochs = 10<br/>dropout=0.3</span></pre><p id="eb6e" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><strong class="ln jk">模拟输入特性</strong></p><p id="601b" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为了测试功能丰富的输入数据是否对结果有任何影响，我通过高音手套25d、50d、100d和200d运行了该模型。</p><p id="d827" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这意味着<strong class="ln jk">对于每个手套尺寸</strong>，我将运行4组实验并分别记录它们的结果。</p><ul class=""><li id="644b" class="mo mp jj ln b lo mj ls mk lw mq ma mr me ms mi pm mu mv mw bi translated">基线模型</li><li id="0009" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi pm mu mv mw bi translated">实验1:基线+ 1个附加编码器堆栈</li><li id="84f5" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi pm mu mv mw bi translated">实验2:基线+ 2个附加编码器堆栈</li><li id="01dc" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi pm mu mv mw bi translated">实验3:基线+ 3个附加编码器堆栈</li></ul><p id="764d" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">因此，总共运行了16种(4种架构* 4种不同的手套尺寸组)不同的模型配置。</p><p id="2bf9" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><strong class="ln jk">标准化评估指标</strong></p><p id="5e94" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">最后，我决定使用F1分数作为比较的标准，因为它是精确度和召回率之间的调和平均值。</p><blockquote class="pf pg ph"><p id="99c8" class="ll lm pi ln b lo mj lq lr ls mk lu lv pj ml ly lz pk mm mc md pl mn mg mh mi im bi translated">如果你需要复习精确度，回忆一下F1分数。我在上面找到了一个很好的视频。</p></blockquote><figure class="oj ok ol om gt iv"><div class="bz fp l di"><div class="pn po l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">解释精度、召回和F1的Youtube视频</p></figure><h1 id="3a1b" class="kn ko jj bd kp kq nr ks kt ku ns kw kx ky nt la lb lc nu le lf lg nv li lj lk bi translated">实验结果</h1><p id="4bf2" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">对于这一部分，我将首先分享在每个高音手套尺寸范围内的实验结果<strong class="ln jk">。然后，我将分享每个实验<strong class="ln jk">在</strong>高音手套尺寸上的结果。</strong></p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pp"><img src="../Images/969a74c5ff52aa0fe6ec7549ea4f3c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JDgsr_QrHmoCj-K6IAADzA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">维度内和跨维度的结果示例</p></figure><p id="a19f" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">从现在开始，我将以图像的形式显示一系列结果输出，并添加我认为合适的注释。</p><p id="4e05" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">对于每个结果输出，记下精确度、召回率、F1分数以及损失曲线的样子。</p><p id="9471" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><strong class="ln jk">记住，实验中唯一的变化是增加了一个编码器堆栈。</strong></p><blockquote class="nc"><p id="ee34" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated">在你进行的过程中，一定要花些时间自己解释这些结果。:)</p></blockquote><h2 id="43e1" class="nw ko jj bd kp nx pv dn kt nz pw dp kx lw px oc lb ma py oe lf me pz og lj oh bi translated">高音扬声器手套25D的结果(在范围内)</h2><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qa"><img src="../Images/27cd0fcda068bf55147262a3f7761a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KKyuN1U8AhNmZqOBYbS-YQ.png"/></div></div></figure><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qb"><img src="../Images/9fb1c99011b1799a829811b499d00342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lGRm-JrGQVkDxyIZ1CAYBA.png"/></div></div></figure><p id="148a" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">对于这个结果集，在实验中很难看到更深层次网络的影响。</p><p id="1276" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我确实对正在发生的事情有一个想法和感觉，但在模式变得更清晰之前，我不会发表评论。</p><blockquote class="pf pg ph"><p id="79d5" class="ll lm pi ln b lo mj lq lr ls mk lu lv pj ml ly lz pk mm mc md pl mn mg mh mi im bi translated">注意:在下一个结果集中会变得更清楚:)</p></blockquote><p id="25c7" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">话虽如此，在正标签(P)的F1-分数中有一个有趣的模式。</p><p id="ad52" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">随着模型变得越来越复杂，阳性标记(P)的F1分数似乎在每次实验中都在缓慢增加。</p><p id="f7a9" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我怀疑这与更深层次的网络学习和更复杂的关系有关。</p><p id="80de" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">例如共同引用、长期依赖等。</p><p id="1f90" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">现在让我们将特征的数量从25D增加到50D，以便更好地了解随着模型的深入会发生什么。</p><h2 id="ee9e" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">使用高音手套50D的结果(在范围内)</h2><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/37042d23962303351208bdb4a7f6a0e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H8HORZ1QhiGsQ7NedJsDdQ.png"/></div></div></figure><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qc"><img src="../Images/a6ceb84e4556cc7e347de77de1173c16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yt53erS1GBe20MiiUUa-ug.png"/></div></div></figure><p id="ddac" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">与上一个结果集相比，这无疑是一个更好的结果集。</p><p id="5a65" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">首先，注意随着模型的深入，验证损失是如何开始偏离训练损失曲线的？</p><blockquote class="nc"><p id="6e97" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated">你觉得为什么会这样？</p></blockquote><p id="62dc" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated">和前面的结果集类似(虽然<em class="pi">很</em>微妙)，但是<strong class="ln jk">这和机器学习中的</strong> <a class="ae jg" rel="noopener" target="_blank" href="/examples-of-bias-variance-tradeoff-in-deep-learning-6420476a20bd"> <strong class="ln jk">偏差-方差权衡</strong> </a> <strong class="ln jk">有关系。</strong></p><p id="6910" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">总之，随着模型复杂性的增加，模型开始更好地学习训练数据，导致偏差减少。</p><p id="51fe" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这从实验中训练损失(蓝线)略微下降的结果中可以明显看出。</p><blockquote class="pf pg ph"><p id="16b4" class="ll lm pi ln b lo mj lq lr ls mk lu lv pj ml ly lz pk mm mc md pl mn mg mh mi im bi translated">注意:训练更大的网络是降低偏差的标准缓解措施。这个结果清楚地显示了更大的网络是如何减少偏差的。</p></blockquote><p id="901b" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">然而…</p><p id="4543" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">随着模型开始过度拟合训练数据，它变得不可概括，导致方差增加。</p><p id="8ce9" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">注意验证损失曲线在不同实验中的变化。</p><p id="872c" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为了得到一个更具体的例子，只需看看实验2和实验3。</p><p id="a767" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">请注意，当验证误差开始出现峰值时，训练损失如何从0.1618下降到0.1595。即偏差减少，方差增加</p><p id="cfdf" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">现在开始下一个发现！</p><p id="f80d" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">下一个发现来自于所有实验的F1分数。</p><p id="b512" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><strong class="ln jk">与之前的结果集相比，F1的所有分数都有所提高。</strong></p><p id="9b24" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">f1-阳性标记(P)的平均分数约为32%,而阴性标记(N)的平均分数约为27%。</p><p id="276b" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">与之前的结果相比，阳性标签(P)的最大F1分数约为28%,而阴性标签(N)仅为18%。</p><p id="811b" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">记住，这里唯一的变化是使用了更丰富的输入集。即高音手套25D换成了50D。</p><p id="5e75" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">其他一切都保持不变。</p><blockquote class="nc"><p id="c7aa" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated"><strong class="ak">这说明了输入特征的数量与预测结果之间的关系。</strong></p></blockquote><p id="0cdd" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated"><strong class="ln jk">在这种情况下，使用更丰富的功能集提高了模型的整体性能。</strong></p><p id="6266" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">那么，将功能从50D增加到100D，我们会看到什么呢？</p><h2 id="7d81" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">使用高音手套100D的结果(在范围内)</h2><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qd"><img src="../Images/cfdd5c1a4439cc909978db3c15a963c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4W_rzI2rlhIk50g5tWOuA.png"/></div></div></figure><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qe"><img src="../Images/52fcd046401ca629c0fe58b9a11fe7b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FoS5SyvY2J7YZbS75pAwew.png"/></div></div></figure><p id="ac79" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">该结果集实际上验证了我们之前关于输入要素和预测结果之间关系的观点。</p><p id="4ee7" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><strong class="ln jk">但是..不是你想的那样。</strong></p><p id="3e75" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在这种情况下，该模型在预测<strong class="ln jk">阳性标记(P) </strong>方面略胜一筹；f1-实验得分比之前的结果高<strong class="ln jk"/>。</p><p id="19f5" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">然而，看起来这个模型在预测<strong class="ln jk">负标签(N) </strong>方面做得不是很好；与之前的结果相比，f1-实验得分<strong class="ln jk">下降</strong>。</p><p id="3d76" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">可能的是，添加的特征50D至100D可能允许模型更好地学习正面标签(P ),但是在学习负面标签(N)时却添加了噪声。</p><p id="a126" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这可能是我认为与之前的结果相比，我们在实验中看到F1分数下降的原因之一。</p><p id="9b16" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">另一个原因(可能更有道理)是，由于模型失去了普遍性，F1分数在波动。</p><p id="4768" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">注意验证损失曲线是多么不稳定。</p><p id="ae7e" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为了获得更好的结果，我们可能需要在再次查看F1分数之前调整其他超参数。</p><p id="ab77" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">然而，这个实验的目的只是通过增加更多的层来改变模型架构，并注意下游的变化，因此我不会在这里做任何超参数调整。</p><p id="f1f0" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">下一个发现很明显。</p><blockquote class="nc"><p id="4ebf" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated">提示:这与验证损失曲线有关。</p></blockquote><p id="9bd4" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated">请注意，验证损失通常低于实验中的训练损失。</p><p id="47a0" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这是一种不常见的模式，但确实会发生。</p><p id="f4b3" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">关于这个<strong class="ln jk"> </strong>点，下一个逻辑问题是……</p><blockquote class="nc"><p id="60ca" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated">为什么？</p></blockquote><p id="56d3" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated"><strong class="ln jk">为什么验证损失低于培训损失？</strong></p><p id="ee93" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">几个原因:</p><ol class=""><li id="1265" class="mo mp jj ln b lo mj ls mk lw mq ma mr me ms mi mt mu mv mw bi translated">这可能与培训过程中如何计算验证损失有关。对于每个时期，在训练损失之后<strong class="ln jk">计算验证损失。即，在模型更新了权重之后，计算验证损失。</strong></li><li id="b9b8" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">我在训练集中使用了辍学层。缺失图层具有正则化效果。这意味着它对权重进行了惩罚，以使预测更具普遍性。在验证计算期间，禁用辍学。</li></ol><p id="a56a" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">现在我们完成了100D，让我们看看200D会发生什么！</p><p id="4e9a" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">精神食粮。</p><blockquote class="nc"><p id="9458" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated">功能太多会是个问题吗？</p></blockquote><p id="37d7" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated">让我们找出答案，好吗？</p><h2 id="b1a9" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">使用高音手套200D的结果(在范围内)</h2><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qf"><img src="../Images/5b9309d75523c7ce4b3a154c0684b10c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fc-KjFipnvqdNjiSOFNVzg.png"/></div></div></figure><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qg"><img src="../Images/378072bece3e26e946bd64ad7c168ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KYzhmU4T5h-BRKgK53ZJQQ.png"/></div></div></figure><p id="00b2" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><strong class="ln jk">显然可以！</strong></p><p id="15dc" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">看起来200D导致模型在实验中过度拟合。</p><p id="8157" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">让我们退一步想一想。</p><blockquote class="nc"><p id="652e" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated">这里发生了什么？</p></blockquote><p id="87fe" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated">唯一的变化是使用了功能更加丰富的输入集。即100D到200D的特征。</p><blockquote class="nc"><p id="9e37" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated">拥有更多功能不应该给我们带来更好的结果吗？</p></blockquote><p id="9ac6" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated"><strong class="ln jk">不完全是。</strong></p><p id="02a9" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">拥有一组扩展的特性可能会不可避免地给模型带来更多的<strong class="ln jk">噪声</strong>。</p><p id="531a" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这从我们看到的过度拟合中可以明显看出。</p><p id="d8a5" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这是我认为正在发生的事情。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qh"><img src="../Images/aa3f0dc8653751b165275890c9037b3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HYRTnQDdqU6u6Gobkj-kug.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">单词嵌入示例</p></figure><p id="cccc" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">每个输入特征都可以映射到其他概念，如“男人”、“女人”、“皇冠”、“皇家”等。</p><p id="8ca4" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">每个特征(那些数字)的学习嵌入反映了输入术语是否满足那些特征的标准。这通常在[-1，1]的范围内。</p><p id="7de1" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">例如，对于输入术语“国王”。</p><p id="8dbd" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><em class="pi">国王是男人吗？</em></p><p id="ace0" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">是的。因此，所学习的嵌入对于特征“人”具有0.98的分数。</p><p id="c10d" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><em class="pi">国王是女人吗？</em></p><p id="dd2d" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">没有。因此，对于特征“女人”,学习嵌入的得分为0.01。</p><p id="4331" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">如此等等。</p><p id="5067" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我认为在200D，有太多的功能导致太多的噪音。</p><p id="d592" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">例如，像“红色”或“绿色”这样的特征可能只是模型的噪声。</p><p id="e6ed" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">那么我们能从中学到什么呢？</p><blockquote class="nc"><p id="8ca7" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated">添加更多输入要素并不能保证更好的模型或更好的结果。有时，拥有更多功能意味着你的模型开始学习噪音。</p></blockquote><p id="d735" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated">我还注意到，在所有16个实验中，该模型在负标签(N)上总是表现不佳。</p><p id="c7fb" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">看看25D、50D、100D和200D的F1分数。</p><p id="84da" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">负标签(N)的F1值总是低于正标签(P)的F1值。</p><blockquote class="nc"><p id="b718" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated">为什么会这样呢？</p></blockquote><p id="5217" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated">我认为正标签(P)训练示例可能比负标签(N)训练示例更容易学习。</p><p id="c2bd" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">也许学习否定或某些否定短语对这个模型来说很难。这就解释了为什么(N)看起来没有得到太多的性能提升。</p></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><p id="f4d2" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">好吧！我们已经到达每个输入维度内的结果<strong class="ln jk">的末尾。</strong></p><p id="facb" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在这篇文章的最后一部分，我想让你看一看个实验的结果。</p><p id="9aa2" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这会让你对结果有不同的看法。</p><p id="8cf1" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><strong class="ln jk">只要注意曲线是如何随着输入特征的增加而变化的。</strong></p><p id="ff31" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">记住，其他一切都是不变的。这里唯一的变化是输入特性。</p><blockquote class="nc"><p id="a6ee" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated">我会让你自己查看结果，没有任何评论。</p></blockquote><h2 id="c0eb" class="nw ko jj bd kp nx pv dn kt nz pw dp kx lw px oc lb ma py oe lf me pz og lj oh bi translated">跨输入要素的基线结果</h2><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qi"><img src="../Images/54dc3e3cd81cc12540501597c9f0703a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hVG1MNZ-Qa1t2fNmsf3aSw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">不同输入维度的基线结果</p></figure><h2 id="bb01" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">实验1跨输入特征的结果</h2><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qj"><img src="../Images/a96db88a4d579188399c8670e19b094b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YHDodDUnArreTi1G1FnwVA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">实验1不同输入维度的结果</p></figure><h2 id="d598" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">实验2跨输入特征的结果</h2><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qk"><img src="../Images/2c3d3145529789d5b58136f43f35718a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-53NBP_38pL415XESbNhZg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">实验2不同输入维度的结果</p></figure><h2 id="4a39" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">实验3跨输入特征的结果</h2><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ql"><img src="../Images/c28766a9a84f2fb0fc9cd44769587ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y4fXHjLB-uds6-XxIx1edg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">实验3不同输入维度的结果</p></figure><h1 id="c64a" class="kn ko jj bd kp kq nr ks kt ku ns kw kx ky nt la lb lc nu le lf lg nv li lj lk bi translated">摘要</h1><p id="95b7" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">在我结束这篇文章之前，我想我应该总结一下这个实验的一些收获。</p><ol class=""><li id="bd47" class="mo mp jj ln b lo mj ls mk lw mq ma mr me ms mi mt mu mv mw bi translated">深度学习中仍然存在<a class="ae jg" rel="noopener" target="_blank" href="/examples-of-bias-variance-tradeoff-in-deep-learning-6420476a20bd">偏差-方差权衡</a>。如果你积极地通过应用链接帖子中提到的缓解措施来减少偏差和差异，它是可以被缓解的。</li><li id="d135" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">一般来说，更深的网络会减少偏差，但如果其他因素保持不变，差异就会增加。随着网络构建得越深入，最好是迭代并应用正则化/添加更多的训练数据。</li><li id="daf0" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">有时，验证损失可能低于培训损失。不要被这个结果吓到。</li><li id="d1fb" class="mo mp jj ln b lo mx ls my lw mz ma na me nb mi mt mu mv mw bi translated">添加更多输入要素并不能保证模型或结果会更好。与100D的结果相比，200D在训练集中引入了更多的噪声，导致了更差的结果。</li></ol><p id="e9ba" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">所以在回顾了这个实验的结果之后…</p><blockquote class="nc"><p id="ea0b" class="nd ne jj bd nf ng pq pr ps pt pu mi dk translated">网络越深越好吗？</p></blockquote><p id="0def" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated">看情况。</p><p id="7a02" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">似乎有一个<strong class="ln jk">最佳深度</strong>，它与你必须开始的训练数据量相关联。</p><p id="2574" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在这个具有27，481行的tweeter数据集的情况下，看起来具有100D输入特征的<strong class="ln jk">实验1的模型架构是最佳的。</strong></p><p id="00b9" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为了便于参考，实验1的架构看起来像这样，有2个编码器堆栈。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qm"><img src="../Images/f825fdd1bab77c83ede492462f1c6bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rNrpzOdEy_TjDMwVABkkmA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">实验1模型架构</p></figure><h1 id="938c" class="kn ko jj bd kp kq nr ks kt ku ns kw kx ky nt la lb lc nu le lf lg nv li lj lk bi translated">结尾注释</h1><p id="d02c" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">嗯，那就这样吧！</p><p id="e679" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">写这篇文章确实花了一些时间来运行所有的实验。</p><p id="ff50" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">希望你觉得这篇文章很有见地！</p><p id="8642" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">如果是的话，让我知道，这样我就知道该继续写什么类型的文章了。:)</p><p id="6c75" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">直到下一个帖子…</p><p id="7159" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">再见了。</p><p id="c15c" class="pw-post-body-paragraph ll lm jj ln b lo mj lq lr ls mk lu lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">LinkedIn简介:<a class="ae jg" href="https://www.linkedin.com/in/timothy-tan-97587190/" rel="noopener ugc nofollow" target="_blank">蒂莫西·谭</a></p></div></div>    
</body>
</html>