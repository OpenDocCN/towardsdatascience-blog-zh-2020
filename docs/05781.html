<html>
<head>
<title>The Most Important Statistical Test</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最重要的统计测试</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-most-important-statistical-test-dee01f4d50cf?source=collection_archive---------28-----------------------#2020-05-13">https://towardsdatascience.com/the-most-important-statistical-test-dee01f4d50cf?source=collection_archive---------28-----------------------#2020-05-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1b32" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">剧透警告:可能性比率测试</h2></div><p id="dd67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">似然比检验(LRT)“统一”频数统计检验。诸如t检验、f检验、卡方检验等品牌检验是LRT的<em class="le">特定</em>案例(甚至是近似值)。</p><p id="42df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，令人惊讶的是许多人从未听说过LRT。当我帮助人们解决他们的统计问题时，大多数时候他们只需要表演LRT就可以了。但是之后他们会问<em class="le">这是什么</em>，问<em class="le">怎么做</em>。如果你必须记住一个统计测试，那应该是LRT。</p><p id="3d41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文将带您了解LRT的理论和应用。假设对数理统计有基本的了解。</p><p id="39b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管我学习贝叶斯理论，但任何应用统计学家也必须理解基本的频率主义方法。人们通常没有时间学习理论上的开销，上级要求报告中的p值。</p><h1 id="e2cb" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">什么是LRT？</h1><p id="fa13" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">顾名思义，我们采用两种可能性的比率，并对其进行转换:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/d1bd79ff67b00593248331ee7e745568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SSvKZkBm9dduZf_aKKWFTw.png"/></div></div></figure><p id="5857" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我为这个迟钝的符号道歉，但是它和维基百科<a class="ae mo" href="https://en.wikipedia.org/wiki/Likelihood-ratio_test" rel="noopener ugc nofollow" target="_blank">中的一致，所以请参考它来澄清。分子是零假设下的似然，分母是零<strong class="kk iu"> <em class="le">和</em> </strong>备选假设并集下的最大似然。我用这个量δD来表示零模型和松弛模型之间的偏差，-2对数似然+某个常数(从减法中抵消)。如果现在还不清楚，不要担心；通过下面的一些例子，LRT应该会变得更加清晰。</a></p><p id="4a10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那些曾经看过<a class="ae mo" href="https://en.wikipedia.org/wiki/Akaike_information_criterion" rel="noopener ugc nofollow" target="_blank"> AIC </a>或<a class="ae mo" href="https://en.wikipedia.org/wiki/Bayesian_information_criterion" rel="noopener ugc nofollow" target="_blank"> BIC </a>的人可能会对看似任意的-2感到困惑，但偏差是很重要的，因为<strong class="kk iu">δD是渐近χ分布的</strong>，其自由度(df)等于两个模型之间df的差异(估计参数数量的差异)。这个惊人的事实使得大多数假设检验成为可能。</p><p id="7bff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae mo" href="https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma" rel="noopener ugc nofollow" target="_blank">尼曼-皮尔逊引理</a>证明了LRT的使用是合理的，该引理指出LRT是比较两个简单假设的最强大的测试，例如H0: θ = 0和Ha: θ = 2。</p><p id="f0df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为备选假设通常不简单，比如θ ≠ 0，我们用极大似然估计(MLE)将其简化为简单的比如θ = 2。通常最大似然估计是通过(广义)线性回归完成的。直觉上，如果我们不能在最大似然估计处拒绝零假设，那么我们不可能在任何其他点拒绝，所以最大似然估计是唯一重要的点。</p><h1 id="3d67" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">将点连接起来:f检验</h1><p id="6dde" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">用人类的语言来说，我们在比较两个模型:一个有约束，另一个有约束的放松。你经常把它看作</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mp"><img src="../Images/367999cbf5b306ca6c03d966ea75d036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1fSKoV7OopN5AhTJFc9Kkg.png"/></div></div></figure><p id="6169" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在零模型中，我们将平均值限制为0。在另一个模型中，我们允许平均值不为零。然后我们需要指定一个似然函数。这里，为了保持熟悉，我们假设观察值来自方差σ = 1的正态分布。最大似然是样本平均值。因此:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mq"><img src="../Images/75c15ba99ff62ce2c891f15fab2438a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B_pU2pzQZfrnBSzBOab-ng.png"/></div></div></figure><p id="cae0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这看起来很粗糙，但是δD<strong class="kk iu"/>术语看起来很熟悉:回归平方和(SS)！(应该是缩放的，但是这里的scale参数是1。)它可以重新表示为总SS减去剩余SS，如第二行所示。而<a class="ae mo" href="https://en.wikipedia.org/wiki/Chi-squared_distribution" rel="noopener ugc nofollow" target="_blank"> χ分布</a>被定义为来自标准正态分布的SS，所以这一切都有意义。因为在替代模型中我们估计了一个额外的参数，所以δD遵循具有1 df的χ分布。</p><p id="5c1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果方差未知呢？那么零模型估计一个参数，而替代模型估计两个参数，因此df的差异仍然是1。在这种情况下，我们更熟悉地称之为ANOVA或f检验，在本例中相当于截距的t检验。</p><p id="95a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还记得我的观点吗，所有这些名字不同的统计测试实际上都是一样的(LRT)？是啊。这里有一些R代码来演示等价性:</p><pre class="md me mf mg gt mr ms mt mu aw mv bi"><span id="2fe5" class="mw lg it ms b gy mx my l mz na">set.seed(123)<br/>x &lt;- rnorm(50, 0.5, 1)<br/>model0 &lt;- lm(x~0)<br/>model1 &lt;- lm(x~1)</span><span id="105d" class="mw lg it ms b gy nb my l mz na">anova(model0, model1)<br/>summary(model1)<br/>t.test(x)</span></pre><p id="d9eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以把偏差看作是应用于非正态分布的SS的推广。虽然不完全准确，但直觉是存在的。作为练习，将二项式偏差与对数损失函数进行比较，后者有时会在分类模型中进行优化。</p><h1 id="3fbd" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">连接这些点:G测试</h1><p id="e992" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">除了测试样本均值之外，我们通常还想测试列联表的独立性。统计学充斥着许多过时的传统，这是其中之一。</p><p id="6cf4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大多数人会立即跳到皮尔逊卡方检验，但这仅仅是T2 G检验的近似值，是LRT的一个特例。在计算器出现之前，取对数很困难，所以统计学家使用卡方检验来简化计算。令人惊讶的是，我们现在有了计算器，但仍然没有默认切换到G测试。(另一个常见的替代方法，<a class="ae mo" href="https://en.wikipedia.org/wiki/Fisher%27s_exact_test" rel="noopener ugc nofollow" target="_blank"> Fisher精确检验</a>，提供精确的p值，而不是依赖于渐近值，但是阶乘使得它对于大样本量的计算不切实际。)</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/ce3623682e856e51015400a7d287d664.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*xAikrZxMgqmT2hsAPKZc4g.png"/></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">相依表</p></figure><p id="882d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可能性有多大？在独立情况下，我们可以认为数据是从多项式分布中生成的，其中落在每个像元中的概率是</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/f5fc91922fceabf6c4f60206dec7d97c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*lSg5QNCOikaZNXObBm1bNg.png"/></div></figure><p id="d512" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了依赖，每个单元格中的下降概率为</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e9ed40b8649ed689c193f618cae6e29f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*Y70nYQ_pXN9YPTu6v9e9Fg.png"/></div></figure><p id="d21a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以将δD计算为:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nj"><img src="../Images/08d0b9da30053c7ac5f541fc53721e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNyz52Xl1RiNvG_y8eYLmQ.png"/></div></div></figure><p id="8be3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这相当于G-test的Wikipedia页面中的测试统计。这个看起来很可疑<a class="ae mo" href="https://en.m.wikipedia.org/wiki/Kullback–Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> KL发散</a>，嗯？有趣的是，许多概念是如此的相互关联。</p><p id="a900" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用泊松回归很容易地执行这个测试:y~x1+x2对y~x1+x2+x1*x2。(相当于作为练习留下的多项式模型。哦不。我变成了我讨厌的东西。)</p><p id="8b46" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当所有πN都很大时，卡方检验和G检验得出相似的p值。G检验应该是更可取的，因为LRT是最有效的检验，并且检验统计量的分布更接近χ分布。</p><pre class="md me mf mg gt mr ms mt mu aw mv bi"><span id="81c3" class="mw lg it ms b gy mx my l mz na">library(data.table)</span><span id="81bd" class="mw lg it ms b gy nb my l mz na">N &lt;- 100 # play around with this<br/>factor1 &lt;- sample(<br/>  c('A', 'B'), <br/>  N, <br/>  replace = TRUE, <br/>  prob = c(0.4, 0.6)<br/>)<br/>factor2 &lt;- ifelse(<br/>  factor1 == 'A',<br/>  sample(c('C', 'D'), N, replace = TRUE, prob = c(0.3, 0.7)),<br/>  sample(c('C', 'D'), N, replace = TRUE, prob = c(0.7, 0.3))<br/>)<br/>dat &lt;- data.table(factor1, factor2)<br/>xtab &lt;- table(dat)<br/>dat_count &lt;- dat[,.N, by = .(factor1, factor2)]</span><span id="f5d2" class="mw lg it ms b gy nb my l mz na">model_indep &lt;- glm(<br/>  N~factor1+factor2, <br/>  data = dat_count, <br/>  family = 'poisson'<br/>)<br/>model_dep &lt;- glm(<br/>  N~factor1*factor2, <br/>  data = dat_count, <br/>  family = 'poisson'<br/>)<br/>summary(model_indep)<br/>summary(model_dep) # the fitted match the observed so deviance = 0</span><span id="85d9" class="mw lg it ms b gy nb my l mz na">pchisq(model_indep$deviance, df = 1, lower.tail = FALSE) # G-test<br/>chisq.test(xtab) # Chi-Squared test</span></pre><p id="d6d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以上是LRT的一般程序。我们拟合了一个简单的回归模型，并将偏差与一个更复杂的模型进行比较。</p><h1 id="d427" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">如何表演LRT？</h1><p id="8b77" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">从前面两个例子来看，很明显:</p><ol class=""><li id="42ea" class="nk nl it kk b kl km ko kp kr nm kv nn kz no ld np nq nr ns bi translated">计算约束零模型下的最大对数似然</li><li id="d1d3" class="nk nl it kk b kl nt ko nu kr nv kv nw kz nx ld np nq nr ns bi translated">当约束放松时，计算最大对数似然</li><li id="c07e" class="nk nl it kk b kl nt ko nu kr nv kv nw kz nx ld np nq nr ns bi translated">将δD与具有适当df的χ分布进行比较，以获得p值</li></ol><p id="75fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1)中的模型必须在2)下<em class="le">嵌套</em>。毕竟放松约束就是这个意思。否则，δD没有理论上的保证。</p><p id="88e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，y~x1对y~x1+x2的δD是公平的，因为在第一个模型中，我们约束b2 = 0，而在第二个模型中，我们放松了这个约束。</p><p id="dd16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是y~x1对y~x2的δD是没有意义的。在第一个模型中，我们没有约束b1 = 0，而在第二个模型中，我们<em class="le">有约束b1 = 0。对于b2来说，反之亦然。</em></p><p id="e76f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有时我们有一些假设，比如H0: θ ≤ 0，哈:θ &gt; 0。为了确保模型是简单的和嵌套的，当θ ≤ 0时，零模型应该是最大似然，而当θ是任意实数时，替代模型应该是最大似然。</p><p id="24b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">手工计算最大似然估计很麻烦，而且大多数时候最大似然估计没有封闭解。幸运的是，我们可以利用计算机的力量。</p><p id="94f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于线性模型，统计软件包有内置函数来计算GLMs的MLE，它涵盖了您想要测试的大多数假设。(小心！在Python中，sklearn的逻辑回归函数不<strong class="kk iu">也不</strong>计算最大似然估计，并且会产生不正确的偏差。请改用statsmodels。)我认为GLM是应用统计学家最重要的技能<em class="le">，因为如此多的推理任务都可以归结为GLM问题。</em></p><p id="6aeb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于更复杂的似然函数(如截尾和截断)，你需要写下函数并进行数值优化。</p><h1 id="749d" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">最后</h1><p id="2efe" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">你可以放弃几十种不同的统计测试，用LRT(主要通过GLM)取而代之，这应该是最强大的测试。学习如何将问题框定为正确的回归公式，选择最合理的分布，并诊断模型。</p><p id="c9cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">编写正确的回归公式是最棘手的部分。加入太多变量会导致非常不正确的推论。逐步回归是<em class="le">而不是</em>要做的事情的典型代表，而正则化方法是<em class="le">而不是</em> MLE，因此我们无法计算δd。参见<a class="ae mo" rel="noopener" target="_blank" href="/beyond-a-b-testing-primer-on-causal-inference-d8e462d90a0b?source=friends_link&amp;sk=73a0090c71e1c91e91e260b1f9794ff0">以前的文章</a>以了解如何选择变量。</p></div></div>    
</body>
</html>