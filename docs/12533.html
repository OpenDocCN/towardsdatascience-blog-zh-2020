<html>
<head>
<title>Classification in Imbalanced Data Sets.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不平衡数据集中的分类。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classification-framework-for-imbalanced-data-9a7961354033?source=collection_archive---------10-----------------------#2020-08-29">https://towardsdatascience.com/classification-framework-for-imbalanced-data-9a7961354033?source=collection_archive---------10-----------------------#2020-08-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c04a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解和利用不平衡数据。</h2></div><h1 id="90ca" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="fe4f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi lw translated"><span class="l lx ly lz bm ma mb mc md me di"> C </span> <strong class="lc iu">分类</strong>是机器学习中的一种监督学习，处理将数据分类成类。监督学习意味着模型将输入与其匹配的输出相结合来训练模型，该模型稍后可以在没有输出的情况下对一组新数据做出有用的预测。分类问题的一些例子包括:<em class="mf">邮件中的垃圾邮件检测、订阅分析、手写数字识别、生存预测</em>等。它们都涉及利用训练数据来理解输入变量如何与输出(目标)变量相关的分类器的使用。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mg"><img src="../Images/abadf95835fb8fb5835db2ed7685cea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WdfkquHnIe656geU"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">米切尔·施瓦茨在<a class="ae mw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="cdd6" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated"><strong class="lc iu">类别不平衡</strong>指的是分类中的一个问题，类别的分布是偏斜的。这种不平衡从轻微到极端都有。</p><p id="403f" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">这是一个问题，因为大多数分类算法对少数类的预测精度较低，因为它们是在假设类之间存在平衡的情况下运行的。</p><p id="8496" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">类别不平衡的一个例子是信用卡欺诈检测。在这种情况下，类别是<em class="mf">欺诈</em>和<em class="mf">非欺诈</em>。大多数交易都是<em class="mf">不欺诈，</em>因此<em class="mf">欺诈</em>阶层是少数阶层。少数类预测的准确性低是有问题的，因为它是最重要的类。</p><p id="73f6" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">这篇博客涵盖了处理不平衡数据集中的分类问题的步骤。包含所有代码的 Github 库可以在这里<a class="ae mw" href="https://github.com/ada-k/BankTermDepositPrediction" rel="noopener ugc nofollow" target="_blank">找到。</a></p><h1 id="a114" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">资料组</h1><p id="4a45" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">使用的数据来自 UCI 机器学习知识库。该数据与葡萄牙银行机构的营销活动相关。分类的目标是预测客户是否会订阅定期存款(变量<strong class="lc iu"> y </strong>)。</p><p id="8179" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">有效的模式有助于提高营销活动的效率，因为可以将更多的精力放在订阅机会高的客户身上。</p><p id="90d8" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">数据示例:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi nc"><img src="../Images/07f3c1eeb5457c2755c936184651334f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*khwYFIZqQsZ-WeZqannovw.png"/></div></div></figure><p id="8642" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">可视化目标变量(y ),以观察类别不平衡:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/f9fb0421d80615a14e2facaa8a8c9424.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*2bzNcEUxyayoPzaQFpcyfw.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">图片作者。</p></figure><p id="ccb3" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">圆圈的大小代表每个类的值计数。显然存在极端的阶级不平衡。这将在预处理部分处理。</p><h1 id="653a" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">预处理</h1><p id="9e8a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">预处理包括以下步骤:</p><h2 id="fd93" class="ne kj it bd kk nf ng dn ko nh ni dp ks lj nj nk ku ln nl nm kw lr nn no ky np bi translated"><em class="nq">输入空值</em></h2><p id="c86d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">需要处理缺失值，因为它们可能会导致错误的预测，还可能导致任何给定模型的高偏差。</p><p id="99f1" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">分类特征将用列<em class="mf">模式</em>估算，离散数字特征用列<em class="mf">中位数</em>估算，连续数字特征用列<em class="mf">平均数</em>估算。</p><h2 id="000c" class="ne kj it bd kk nf ng dn ko nh ni dp ks lj nj nk ku ln nl nm kw lr nn no ky np bi translated"><em class="nq">处理异常值</em></h2><p id="a14e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">离群值是许多机器学习算法的问题，因为它们会导致重要见解的丢失或扭曲真实结果，最终导致模型不太准确。</p><p id="4a0d" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">将使用第 10 个和第 90 个百分位数对异常值进行剪裁。</p><h2 id="9805" class="ne kj it bd kk nf ng dn ko nh ni dp ks lj nj nk ku ln nl nm kw lr nn no ky np bi translated"><em class="nq">特征生成</em></h2><p id="da35" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">从现有要素生成新要素增加了在模型训练期间可访问的新信息，因此提高了模型精度。</p><h2 id="b6a2" class="ne kj it bd kk nf ng dn ko nh ni dp ks lj nj nk ku ln nl nm kw lr nn no ky np bi translated"><em class="nq">缩放数值变量</em></h2><p id="e881" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">用<a class="ae mw" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">标准定标器</a>对数字特征进行标准化，以消除不同测量单位带来的差异。</p><h2 id="f998" class="ne kj it bd kk nf ng dn ko nh ni dp ks lj nj nk ku ln nl nm kw lr nn no ky np bi translated"><em class="nq">编码分类变量</em></h2><p id="8b80" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">大多数机器学习和神经网络算法需要数字输入，因此为了利用分类特征，我们必须重新映射它们。</p><p id="f561" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">应用一种热编码技术。它获取一个包含分类数据的列，然后将该列拆分成多个列。条目被 0 和 1 替换，这取决于哪一列有什么值。</p><h2 id="c3b9" class="ne kj it bd kk nf ng dn ko nh ni dp ks lj nj nk ku ln nl nm kw lr nn no ky np bi translated">对不平衡数据集进行重采样。</h2><p id="47ea" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">重采样包括创建不平衡数据集的新版本。</p><p id="17df" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">重采样有两种主要方法:</p><ul class=""><li id="2e5b" class="nr ns it lc b ld mx lg my lj nt ln nu lr nv lv nw nx ny nz bi translated"><em class="mf">过采样</em>:随机复制少数类中的条目。适用于小型数据集。</li><li id="8e13" class="nr ns it lc b ld oa lg ob lj oc ln od lr oe lv nw nx ny nz bi translated"><em class="mf">欠采样</em>:从多数类中随机删除条目。适用于大型数据集。</li></ul><p id="eddd" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">我们的数据集有 41188 行和 21 列，因此使用过采样是安全的。</p><p id="9911" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">预处理类:</p><figure class="mh mi mj mk gt ml"><div class="bz fp l di"><div class="of og l"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">预处理类代码。</p></figure><pre class="mh mi mj mk gt oh oi oj ok aw ol bi"><span id="d1dd" class="ne kj it oi b gy om on l oo op"># calling the class and its methods</span><span id="bf4f" class="ne kj it oi b gy oq on l oo op">d = DataPrep()<br/>path = '/content/bank-additional-full.csv'<br/>data = d.read_data(path)<br/>data = d.treat_null(data)<br/>data = d.outlier_correcter(data)<br/>data = d.generate_features(data)<br/>data = d.scaler(data)<br/>print('After scaling:', data.shape)<br/>data = d.encoder(data)<br/>data = d.over_sample(data)<br/>data.head()</span></pre><p id="4aed" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">输出:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi or"><img src="../Images/552fea71eadc76562a89845edc8158c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V9KJw1H2xe8wtXdaFYb-2w.png"/></div></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi os"><img src="../Images/7afed2c8dbe1f8bcb2328b9dfe4dc2c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NCr0sm3hQ4m7sk7L5Tn0Aw.png"/></div></div></figure><p id="e36a" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">请注意分类和数字列中的变化。</p><h1 id="06b4" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">建模</h1><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ot"><img src="../Images/c81b407d3483bab9b8d84295e7ab695b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l40xKccQqeNEWS3C"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">照片由<a class="ae mw" href="https://unsplash.com/@hharritt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亨特·哈里特</a>在<a class="ae mw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="9e05" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">从预处理到进行预测，我们首先使用训练和验证数据来训练和评估我们的模型。但首先，我们必须分离目标和预测变量，然后将它们分成训练集、验证集和测试集。测试集不是单独提供的，因此我们从数据集中检索它。</p><pre class="mh mi mj mk gt oh oi oj ok aw ol bi"><span id="d379" class="ne kj it oi b gy om on l oo op"># split the data to have the predictor and predicted variables<br/>x = data.drop(['y'], axis = 1)<br/>y = data[['y']]</span><span id="c42b" class="ne kj it oi b gy oq on l oo op"># Encode labels in target df.<br/>encoder = LabelEncoder()<br/>y = encoder.fit_transform(y)</span><span id="96e6" class="ne kj it oi b gy oq on l oo op"># get the sets<br/>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.10, random_state = 42)</span><span id="b3de" class="ne kj it oi b gy oq on l oo op">x_train, x_val, y_train, y_val = train_test_split(x_train ,y_train, test_size = 0.20, random_state = 42)</span></pre><p id="84c2" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">有待探索的算法:</p><ul class=""><li id="7d1e" class="nr ns it lc b ld mx lg my lj nt ln nu lr nv lv nw nx ny nz bi translated">XGBoost :</li></ul><p id="8695" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">XGBoost 分类器是一种基于树的集成学习算法，是梯度推进机器的一种实现。</p><p id="8a4b" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">它针对速度和性能进行了优化。</p><p id="f398" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">* <strong class="lc iu">多层感知器:</strong></p><p id="0099" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">多层感知器(MLP)是一类前馈人工神经网络。它至少由三层节点组成:输入层、隐藏层和输出层。</p><p id="2762" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">它区分不可线性分离的数据的能力是我们探索它的原因。</p><p id="83ff" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">* <strong class="lc iu">逻辑回归:</strong></p><p id="9c26" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">逻辑回归是一种简单但非常有效的分类算法，它使用对数比值比来预测组成员。</p><p id="bc08" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">它的简单性和使用对数比值比代替概率是我们探索它的原因。</p><h2 id="4227" class="ne kj it bd kk nf ng dn ko nh ni dp ks lj nj nk ku ln nl nm kw lr nn no ky np bi translated">交叉验证</h2><p id="6819" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="mf">交叉验证</em>是一种重采样程序，用于在有限的数据样本上评估机器学习模型。</p><p id="9a70" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">使用的技术:</p><ul class=""><li id="8ed6" class="nr ns it lc b ld mx lg my lj nt ln nu lr nv lv nw nx ny nz bi translated">K-fold  : K-Fold CV 是将给定的数据集分成 n 个折叠，其中每个折叠在某个点用作测试集，其余的用作训练集。</li><li id="609d" class="nr ns it lc b ld oa lg ob lj oc ln od lr oe lv nw nx ny nz bi translated"><strong class="lc iu">分层 K 折叠</strong>:分层 K 折叠将数据打乱，然后将其分成 n 个折叠，每个折叠都用作一个测试集。分层保持数据集目标之间的平衡(每个分层折叠保持目标类的相同比率)。这种策略在不平衡数据的情况下是最好的。</li></ul><p id="1697" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">在这两种技术中，默认的评分标准是<a class="ae mw" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">准确度分数</strong> </a> <strong class="lc iu"> </strong>(做出的正确预测数除以做出的预测总数)。</p><p id="782f" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">实施:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ou"><img src="../Images/f691dea8ef0e463222d4cd8141d4af94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NlhO2XJR2Ly9ZPQkSDffDg.png"/></div></div></figure><p id="b636" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">现在，我们使用这些技术在训练数据上观察我们的模型性能。</p><pre class="mh mi mj mk gt oh oi oj ok aw ol bi"><span id="943f" class="ne kj it oi b gy om on l oo op"># using grid search to find optimal parameters</span><span id="0fc6" class="ne kj it oi b gy oq on l oo op">regressor = LogisticRegression()<br/>grid_values = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}</span><span id="a5c9" class="ne kj it oi b gy oq on l oo op">model = GridSearchCV(regressor, param_grid=grid_values)<br/>model.fit(x_train,y_train)<br/>print(model.best_score_)<br/>print(model.best_params_)</span></pre><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/ca4a0cf6958717384dc503846cda4ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*IlAb5CNIkD6WZjDvm4T-0A.png"/></div></figure><pre class="mh mi mj mk gt oh oi oj ok aw ol bi"><span id="4579" class="ne kj it oi b gy om on l oo op"># using the optimal parametes printed out<br/>regressor = LogisticRegression(C = 1000, penalty= 'l2')<br/>regressor.fit(x_train, y_train)</span><span id="e6ed" class="ne kj it oi b gy oq on l oo op"># using kfolds<br/>print('Logistic Regression mean accuracy score using kfold:', overall_score(regressor, x_train))</span><span id="84e3" class="ne kj it oi b gy oq on l oo op"># stratified KFold<br/>print('Logistic Regression mean accuracy score using Stratifiedkfold :', overall__stratified_score(regressor, x_train))</span></pre><p id="9203" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">输出:</p><pre class="mh mi mj mk gt oh oi oj ok aw ol bi"><span id="bd35" class="ne kj it oi b gy om on l oo op">Logistic Regression mean accuracy score using kfold: 0.742437522099093<br/>Logistic Regression mean accuracy score using Stratifiedkfold : 0.7420879248958712</span></pre><p id="025d" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">对 XGBoost 和 MLP 重复相同的过程。</p><pre class="mh mi mj mk gt oh oi oj ok aw ol bi"><span id="0cf8" class="ne kj it oi b gy om on l oo op">xgb = XGBClassifier(silent = True,max_depth = 6, n_estimators = 200)<br/>xgb.fit(x_train, y_train)</span><span id="47c9" class="ne kj it oi b gy oq on l oo op"># using kfolds<br/>print('xgb mean score on the original dataset (kfold):', overall_score(xgb, x_train))</span><span id="8ac1" class="ne kj it oi b gy oq on l oo op"># stratified KFold<br/>print('xgb mean score on the original dataset (stratified kfold):', overall__stratified_score(xgb, x_train))</span></pre><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ow"><img src="../Images/7683d63411bbbbd9b73859bfbef97b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ILlsV6vg0N2MqhCcMQ6rA.png"/></div></div></figure><pre class="mh mi mj mk gt oh oi oj ok aw ol bi"><span id="84b4" class="ne kj it oi b gy om on l oo op">mlp = MLPClassifier() # working with default parameters<br/>mlp.fit(x_train, y_train)</span><span id="7071" class="ne kj it oi b gy oq on l oo op"># using kfolds<br/>print('mlp mean score on the original dataset (kfold):', overall_score(mlp, x_train))</span><span id="715f" class="ne kj it oi b gy oq on l oo op"># stratified KFold<br/>print('mlp mean score on the original dataset (stratified kfold):', overall__stratified_score(mlp, x_train))</span></pre><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ox"><img src="../Images/28ede39523599e47bda230534b17f27c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z3F4aKAi5Z94-_QwS91lOg.png"/></div></div></figure><p id="ee53" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">在所有的 3 个模型中，K-fold 产生了最高的准确性，尽管它随分层的变化是如此之小。如前所述，分层最适合不平衡的数据，但我们已经在预处理过程中对数据进行了重新采样，使其不再有用。</p><h1 id="8e46" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">进一步评估</strong></h1><p id="d778" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以通过验证集探索其他准确性指标。</p><ul class=""><li id="5bc4" class="nr ns it lc b ld mx lg my lj nt ln nu lr nv lv nw nx ny nz bi translated"><strong class="lc iu"> ROC </strong> — ROC 曲线是针对 0.0 和 1.0 之间的多个不同候选阈值的真阳性率(y 轴)与假阳性率(x 轴)的关系图。</li><li id="7cf1" class="nr ns it lc b ld oa lg ob lj oc ln od lr oe lv nw nx ny nz bi translated"><a class="ae mw" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">精度和召回</strong> </a> <strong class="lc iu"> — </strong>精度是正确鉴定的阳性结果的数量除以所有阳性结果的数量，包括那些没有正确鉴定的阳性结果，召回是正确鉴定的阳性结果的数量除以所有本应被鉴定为阳性的样本的数量。</li><li id="312f" class="nr ns it lc b ld oa lg ob lj oc ln od lr oe lv nw nx ny nz bi translated"><a class="ae mw" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu"> F1 分数</strong> </a> — F1 分数是对测试准确性的衡量。它是根据测试的精确度和召回率计算出来的。</li></ul><p id="81b8" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">当每类的观察值大致相等时，应使用 ROC 曲线。当存在中等到大的类别不平衡时，应该使用精确召回曲线。</p><p id="389c" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">由于我们对数据进行了重新采样以保持平衡，ROC 是我们的最佳选择。</p><p id="2a8f" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">ROC 图:实施:</p><p id="d421" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated"><em class="mf">这些图在其他模型上实现:RandomForest、CatBoost 和 LGBM，它们可以被编辑以适合任何模型。(代码可在</em><a class="ae mw" href="https://github.com/ada-k/BankTermDepositPrediction" rel="noopener ugc nofollow" target="_blank"><em class="mf">github</em></a><em class="mf">链接中共享)。</em></p><pre class="mh mi mj mk gt oh oi oj ok aw ol bi"><span id="0dbf" class="ne kj it oi b gy om on l oo op">from sklearn.datasets import make_classification<br/>from sklearn.metrics import roc_curve<br/>from sklearn.metrics import roc_auc_score</span><span id="e3c9" class="ne kj it oi b gy oq on l oo op">#predict probabilities<br/>ns_probs = [0 for _ in range(len(x_val))]  # no skill classifier<br/>f_prob = forest.predict_proba(x_val)<br/>c_prob = cat.predict_proba(x_val)<br/>l_prob = lgbm.predict_proba(x_val)</span><span id="a289" class="ne kj it oi b gy oq on l oo op"># keep probabilities for the positive outcome only<br/>f_prob = f_prob[:, 1]<br/>c_prob = c_prob[:, 1]<br/>l_prob = l_prob[:, 1]</span><span id="d598" class="ne kj it oi b gy oq on l oo op"># calculate scores then print them<br/>f_auc = roc_auc_score(y_val, f_prob)<br/>c_auc = roc_auc_score(y_val, c_prob)<br/>l_auc = roc_auc_score(y_val, l_prob)<br/>ns_auc = roc_auc_score(y_val, ns_probs)</span><span id="1942" class="ne kj it oi b gy oq on l oo op">print('RandomForest:', f_auc)<br/>print('CatBoost: ', c_auc)<br/>print('LGBM:', l_auc)</span><span id="57ec" class="ne kj it oi b gy oq on l oo op"># calculate roc curves<br/>f_fpr, f_tpr, _ = roc_curve(y_val, f_prob)<br/>c_fpr, c_tpr, _ = roc_curve(y_val, c_prob)<br/>l_fpr, l_tpr, _ = roc_curve(y_val, l_prob)<br/>ns_fpr, ns_tpr, _ = roc_curve(y_val, ns_probs)</span><span id="2987" class="ne kj it oi b gy oq on l oo op"># plot the roc curve for the model<br/>plt.figure(figsize = (12,7))<br/>plt.plot(f_fpr, f_tpr, marker='.', label='random forest')<br/>plt.plot(l_fpr, l_tpr, marker='.', label='lgbm')<br/>plt.plot(c_fpr, c_tpr, marker='.', label='catboost')<br/>plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')</span><span id="2b64" class="ne kj it oi b gy oq on l oo op">plt.legend()<br/>plt.title('ROC curves for different models')<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')</span></pre><p id="5b59" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">输出:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi oy"><img src="../Images/389b7e3158bc3db12d3f6e0eb19bfcb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iIahRTBd_YsKnQEzufYTVw.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">图片作者。</p></figure><p id="4f57" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">曲线下的面积越大，精确度越高。</p><p id="2fe9" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">无技能分类器是一种不能区分类别的分类器，它在所有情况下都会预测随机类别或恒定类别。(我们的基准)。</p><h1 id="e863" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">预言</h1><p id="5f70" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">使用 MLP，我们最初 3 个模型中表现最好的。</p><pre class="mh mi mj mk gt oh oi oj ok aw ol bi"><span id="423d" class="ne kj it oi b gy om on l oo op"># mlp<br/>pred = mlp.predict(x_test)<br/>pred_df = pd.DataFrame(pred)<br/>pred_df.columns = ['y']<br/>pred_df.to_csv('pred_df.csv') # export to a csv file</span></pre><h1 id="2c98" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="0120" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以继续研究我们的分类问题，探索不同的技术，比如降维，仅仅是为了获得一个性能更好的模型。但是现在，我们知道拥有不平衡的数据并不妨碍我们进行预测，它只是召唤我们使用适当的技术来避免对少数类的糟糕预测。</p><p id="1ab8" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">附言:向我在 10academy.org 的所有同学和导师表示敬意，感谢他们无尽的支持。</p><h1 id="949f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">参考</h1><ol class=""><li id="1888" class="nr ns it lc b ld le lg lh lj oz ln pa lr pb lv pc nx ny nz bi translated"><a class="ae mw" href="https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/" rel="noopener ugc nofollow" target="_blank">如何在 Python 中使用 ROC 曲线和精确召回曲线进行分类</a>。</li></ol><p id="cfe5" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">2.<a class="ae mw" rel="noopener" target="_blank" href="/understanding-auc-roc-curve-68b2303cc9c5">了解 AUC — ROC 曲线。</a></p><p id="5d72" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">3.XGBoost 算法:愿她统治长久！</p><p id="7ee8" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">4.<a class="ae mw" rel="noopener" target="_blank" href="/methods-for-dealing-with-imbalanced-data-5b761be45a18">处理不平衡数据。</a></p><p id="538a" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">5.<a class="ae mw" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> XGBoost 文档。</a></p><p id="1625" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">6.<a class="ae mw" href="https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/" rel="noopener ugc nofollow" target="_blank">不平衡数据:如何处理不平衡分类问题。</a></p><p id="0624" class="pw-post-body-paragraph la lb it lc b ld mx ju lf lg my jx li lj mz ll lm ln na lp lq lr nb lt lu lv im bi translated">7.<a class="ae mw" href="https://machinelearningmastery.com/imbalanced-classification-is-hard/" rel="noopener ugc nofollow" target="_blank">为什么不平衡分类很难？</a></p></div></div>    
</body>
</html>