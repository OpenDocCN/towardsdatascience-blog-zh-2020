<html>
<head>
<title>LightGBM Hyper Parameters Tuning in Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark 中的 LightGBM 超参数调谐</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lightgbm-hyper-parameters-tuning-in-spark-6b8880d98c85?source=collection_archive---------13-----------------------#2020-03-12">https://towardsdatascience.com/lightgbm-hyper-parameters-tuning-in-spark-6b8880d98c85?source=collection_archive---------13-----------------------#2020-03-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d9b4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">网格搜索，顺序搜索，远视…</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/72ffb7a5b61caaccf0382cacde95a689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A_QK7YUd_2dqv-SE"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯多夫·伯恩斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0b25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LightGBM 在各行各业的数据科学家中非常受欢迎。lightgbm 包用 Python 和 r 开发的很好，当数据越来越大的时候，人们希望在分布式数据框架的集群上运行模型。</p><p id="e263" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我最近在 Azure Databricks 上开发一个推荐系统。项目中使用了 LightGBM 模型。超参数调整部分不像在 Python 中那样平滑。在这篇博客中，我将分享我在调优时尝试过的 3 种方法。正在调整的参数有:</p><ul class=""><li id="1fa2" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">numLeaves</li><li id="c230" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">最大深度</li><li id="0b7a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">袋装馏分</li><li id="6326" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">特征分数</li><li id="2e1b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">明苏姆·谢宁利夫</li><li id="7cdf" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">lambdaL1</li><li id="901a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">lambdaL2</li></ul><p id="cb42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里用的 LightGBM 包是<a class="ae ky" href="https://github.com/Azure/mmlspark" rel="noopener ugc nofollow" target="_blank"> mmlspark </a>，微软机器学习 for Apache Spark。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="9f48" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">网格搜索</h1><p id="399d" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">网格搜索是一种蛮力方法。如果你有无限的计算能力，这种方法可以保证你的最佳超参数设置。以下代码显示了如何对 LightGBM 回归器进行网格搜索:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="cb5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们应该知道网格搜索有维度的诅咒。随着参数数量的增加，网格呈指数增长。在我的实践中，上面的网格设置永远不会在我的 exploring 集群上以下面的设置结束:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a44dc791b4a26de5c50a2f27fb0fa7f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*Ayx8fenqjuEdux2D3HbHpQ.png"/></div></figure><p id="c37b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实是，即使大幅缩小网格，计算也很可能失败(由于内存问题)。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="0704" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">顺序搜索</h1><p id="86af" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">降低计算压力的实质是降维。因此出现了按顺序进行调优的想法。在做了一些研究后，我发现这个由拜伦·吴写的中文博客非常有帮助。这个概念是一步一步地进行调整:</p><blockquote class="nr"><p id="cd3b" class="ns nt it bd nu nv nw nx ny nz oa lu dk translated"><em class="ob">第一步:设定一个相对较高的学习率，降低你的迭代次数。</em></p></blockquote><p id="a311" class="pw-post-body-paragraph kz la it lb b lc oc ju le lf od jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">这允许您在下面的步骤中更快地进行调优。完成调优后，您可以增加迭代次数，降低学习速度，以获得不错的性能。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><blockquote class="nr"><p id="90da" class="ns nt it bd nu nv oh oi oj ok ol lu dk translated">步骤 2:调整 numLeaves 和 maxDepth</p></blockquote><p id="efad" class="pw-post-body-paragraph kz la it lb b lc oc ju le lf od jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">这两个参数控制树模型的复杂性。值越高，模型越复杂。理论上，numLeaves ≤ 2^maxDepth.你可以检查你调好的值是否满足这个条件。你的模型可能会有高方差和低偏差。因此，在接下来的几个步骤中，我将尝试通过调整其他变量来减少过拟合</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="e565" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对了，CrossValidatorModel API 真的不友好。你可以输出 bestModel，但是你不能轻易地检查你的最终超参数。我写了一个函数来轻松地提取超参数信息进行检查。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><blockquote class="nr"><p id="7c95" class="ns nt it bd nu nv oh oi oj ok ol lu dk translated">步骤 3:调整 minSumHessianInLeaf</p></blockquote><p id="034e" class="pw-post-body-paragraph kz la it lb b lc oc ju le lf od jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">这个参数有很多别名，比如 min_sum_hessian_per_leaf，min_sum_hessian，min_hessian，min_child_weight。简而言之，它告诉你“一旦你在一个节点中达到一定程度的纯度，并且你的模型能够适应它，就不要试图分裂”。minSumHessianInLeaf 的正确值可以减少过度拟合。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><blockquote class="nr"><p id="6b2d" class="ns nt it bd nu nv oh oi oj ok ol lu dk translated">步骤 4:调整 bagging fraction &amp; feature fraction</p></blockquote><p id="09e0" class="pw-post-body-paragraph kz la it lb b lc oc ju le lf od jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">这两个参数必须同时调整。baggingFraction 控制实例子采样，featureFraction 控制要素子采样。它们都服务于减少过拟合的目的。(更小的分数也允许更快的计算)</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="ea49" class="or mr it on b gy os ot l ou ov"># similar code here</span></pre><blockquote class="nr"><p id="51a2" class="ns nt it bd nu nv oh oi oj ok ol lu dk translated">步骤 5:调整λ1 和λ2</p></blockquote><p id="19a9" class="pw-post-body-paragraph kz la it lb b lc oc ju le lf od jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">L1 和 L2 法规参数也有助于减少过度拟合</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="1d78" class="or mr it on b gy os ot l ou ov"># similar code here</span></pre><blockquote class="nr"><p id="b0fe" class="ns nt it bd nu nv oh oi oj ok ol lu dk translated">步骤 6:调整学习速度和迭代次数以适应模型</p></blockquote><figure class="ow ox oy oz pa kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="7ab2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这个连续的过程，尽管很繁琐，但您最终可以得到一个合理的调优结果。这种方法的主要问题是，首先需要初始化一个合理大小的网格。如果最优值位于您的网格之外，您需要重新分配一个合适的范围。一些尝试是不可避免的。你最好是一个有经验的数据科学家来做调整。因为每次尝试都要耗费大量的计算资源。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="6bf0" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">远视</h1><p id="b510" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated"><a class="ae ky" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank"> Hyperopt </a>是一个 Python 库，用于在笨拙的搜索空间上进行串行和并行优化，搜索空间可能包括实值、离散和条件维度</p><p id="b0f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“目前，hyperopt 中实施了三种算法:</p><ul class=""><li id="ac4f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">随机搜索</li><li id="154c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf" rel="noopener ugc nofollow" target="_blank">Parzen 估计器树(TPE) </a></li><li id="d3de" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://www.electricbrain.io/blog/learning-to-optimize" rel="noopener ugc nofollow" target="_blank">自适应 TPE </a></li></ul><p id="18ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Hyperopt 已经被设计为适应基于高斯过程和回归树的贝叶斯优化算法，但是这些算法目前还没有实现。"</p><p id="548f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Hyperopt 到目前为止还没有广泛使用，我发现一些帖子给出了指导性的 Python 实现:<a class="ae ky" rel="noopener" target="_blank" href="/hyperparameter-optimization-in-python-part-0-introduction-c4b66791614b"> 1。超参数调谐部分 0 </a>，<a class="ae ky" rel="noopener" target="_blank" href="/hyperparameter-optimization-in-python-part-2-hyperopt-5f661db91324"> 2。超参数调谐部分 2 </a>。然而，尽管 Azure Databricks 已经准备好使用 hyperopt，但在任何网站上都找不到 spark 实现。</p><p id="304a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，hyperopt 的实现非常简单。经过几次测试后，我能够在 spark 上运行调优。而且和我之前试过的所有方法相比真的很快。调音结果也很令人满意。下面是我如何在 PySpark 中实现的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="9e93" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">结论</h1><p id="d3b2" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">总之，据我所知，Hyperopt 可能是目前在 spark 数据框架上调整 LightGBM 的超参数的最佳选择。它比强力网格搜索快得多，比顺序搜索更可靠。但是它确实还没有被很好的记录。如果没有我在 Python 中找到的关于 hyperopt 的帖子，我可能无法在 spark 上实现它。</p><p id="4fc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我一直在 spark 上学习机器学习。然而，到目前为止，该材料仅限于在线使用；大部分的模型文件都不足以供外人使用。spark ml 实现的一些示例代码是共享的。希望越来越多的数据科学家可以分享他们的工作和故事，互相帮助。</p><p id="a48f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nq">喜欢讨论就上</em> <a class="ae ky" href="https://www.linkedin.com/in/yi-cao-data/" rel="noopener ugc nofollow" target="_blank"> <em class="nq"> LinkedIn </em> </a> <em class="nq">找我。</em></p></div></div>    
</body>
</html>