<html>
<head>
<title>Big Data Engineering — Best Practices</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大数据工程—最佳实践</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/big-data-engineering-best-practices-bfc7e112cf1a?source=collection_archive---------20-----------------------#2020-09-12">https://towardsdatascience.com/big-data-engineering-best-practices-bfc7e112cf1a?source=collection_archive---------20-----------------------#2020-09-12</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="8962" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="99e9" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">在实施大数据数据处理管道时，您应该记住什么。</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/f8f739a62214cab8eb7d994c69968889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qlHaqNUz3MxT29XK"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated"><a class="ae li" href="https://unsplash.com/@realaxer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">田宽</a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="c044" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这是大数据环境中的数据工程系列的第 1 部分。它将反映我个人的经验教训之旅，并在我创建的开源工具<a class="ae li" href="https://flowman.readthedocs.io" rel="noopener ugc nofollow" target="_blank"> Flowman </a>中达到高潮，以承担在几个项目中一遍又一遍地重新实现所有 boiler plate 代码的负担。</p><ul class=""><li id="9f68" class="mf mg iu ll b lm ln lp lq ls mh lw mi ma mj me mk ml mm mn bi translated">第 1 部分:大数据工程—最佳实践</li><li id="2ec3" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me mk ml mm mn bi translated"><a class="ae li" href="https://medium.com/@kupferk/big-data-engineering-apache-spark-d67be2d9b76f" rel="noopener">第 2 部分:大数据工程— Apache Spark </a></li><li id="d4a1" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me mk ml mm mn bi translated"><a class="ae li" rel="noopener" target="_blank" href="/big-data-engineering-declarative-data-flows-3a63d1802846">第 3 部分:大数据工程——声明性数据流</a></li><li id="1ce7" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me mk ml mm mn bi translated"><a class="ae li" rel="noopener" target="_blank" href="/big-data-engineering-flowman-up-and-running-cd234ac6c98e">第 4 部分:大数据工程— Flowman 启动并运行</a></li></ul><h1 id="cda0" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">期待什么</h1><p id="4922" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">本系列是关于用 Apache Spark 构建批处理数据管道的。但是有些方面对于其他框架或流处理也是有效的。最后，我将介绍<a class="ae li" href="https://flowman.readthedocs.io" rel="noopener ugc nofollow" target="_blank"> Flowman </a>，这是一个基于 Apache Spark 的应用程序，它简化了批处理数据管道的实现。</p></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><h1 id="b141" class="mt mu iu bd mv mw nx my mz na ny nc nd kj nz kk nf km oa kn nh kp ob kq nj nk bi translated">介绍</h1><p id="6955" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">越来越多的公司和项目使用 Apache Spark 作为中央数据处理框架来构建他们的数据处理管道。并且有很好的理由这样做(在本系列的另一部分中会有更多)。</p><p id="e134" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">Apache Spark 本身作为一个框架，在设计数据管道时并没有提供很多遵循最佳实践的指导，也没有考虑到许多细节，这些细节不是数据转换本身的直接部分，但从更广泛的角度来看是重要的，如模式管理和在出现故障或逻辑错误时重新处理数据的能力。</p><h1 id="137c" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">典型数据管道</h1><p id="ecd2" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">在讨论开发数据处理管道时需要记住的各个方面之前(无论使用何种技术，尽管我更喜欢 Apache Spark)，让我们先来看看典型的数据管道实际上是做什么的。</p><p id="3847" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我认为大多数数据管道本质上包含三个步骤:</p><ol class=""><li id="dbc8" class="mf mg iu ll b lm ln lp lq ls mh lw mi ma mj me oc ml mm mn bi translated"><strong class="ll je">提取。从某个源系统读取数据(可以是像 HDFS 这样的共享文件系统，也可以是像 S3 这样的对象存储，或者像 MySQL 或 MongoDB 这样的数据库)</strong></li><li id="edf1" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">改造。</strong>应用一些转换，如数据提取、过滤、连接甚至聚合。</li><li id="bfb1" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">装货。</strong>将结果再次存储到某个目标系统中。同样，这可以是共享文件系统、对象存储或某个数据库。</li></ol><p id="ef01" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">Apache Spark well 支持所有这些步骤，您可以用几行代码实现一个简单的数据管道。一旦将它投入生产，随着时间的推移，可能会出现一些问题，导致非功能性需求和最佳实践。这些都不是 Apache Spark 直接实现的，你得自己搞定这些。本系列文章将帮助您构建坚如磐石的数据管道，这些管道还会处理许多非功能性需求，但这些需求在生产中非常重要。</p></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><h1 id="9a2c" class="mt mu iu bd mv mw nx my mz na ny nc nd kj nz kk nf km oa kn nh kp ob kq nj nk bi translated">最佳实践</h1><p id="f55c" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">本系列的第一部分是关于最佳实践的。这指的是为实现稳定运营而采取的具体做法。实现它们的技术并不是革命性的，但是它们并没有被经常讨论(至少这是我的印象)。</p><h1 id="dc02" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">1.记录</h1><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj od"><img src="../Images/892d905a86b6114646d4ab433d0e4736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zgwU8Oc--WfpEHAN"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">由<a class="ae li" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马库斯·斯皮斯克</a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="51b9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">第一个要求是提供某种形式的日志记录。这并不令人兴奋，许多开发人员已经提供了这种功能。但是通常的问题是日志应该有多详细:应该将每个细节都记录到控制台还是只记录问题？</p><p id="dacc" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这个问题很难回答，但我更喜欢记录我的应用程序做出的所有重要决策，以及对应用程序有一定影响的所有重要变量和状态信息。您可能想问问自己:对于应用程序不能按预期工作的大多数事件，什么样的信息会有所帮助？</p><p id="6953" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我正在记录的内容的典型示例:</p><ul class=""><li id="c869" class="mf mg iu ll b lm ln lp lq ls mh lw mi ma mj me mk ml mm mn bi translated">当前运行的自定义设置是什么？例如，正在处理什么日期范围？正在处理什么客户？</li><li id="b16b" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me mk ml mm mn bi translated">读取的是什么数据？数据存储在哪里？多少文件？这些信息有助于验证应用程序是否试图从正确的位置读取。</li><li id="0d07" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me mk ml mm mn bi translated">应用程序试图将其结果写入哪里？同样，此信息有助于验证目标系统的所有设置是否正确。</li><li id="c2b8" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me mk ml mm mn bi translated">您还应该在最后记录某种成功消息——这对于在该消息长时间丢失时设置自动警报非常有用。</li></ul><p id="fda6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">正如上一条提到的，警报也是你应该想到的。大多数情况下，最好将所有日志存储在一个像 Graylog 这样的中央日志聚合器中，这样您就可以轻松地搜索特定问题并设置警报。</p><p id="ac43" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">与<em class="oe">日志</em>相关的还有主题<em class="oe">指标</em>。对于这个术语，我不仅仅指 Apache Spark 的内部技术指标，还指一些与业务更相关的指标。例如，提供关于读取和写入的记录数量的指标可能会很有趣——这两个指标在 Apache Spark 中都不直接可用，至少每个数据源和数据宿都不可用。</p><h1 id="c26d" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">2.重播</h1><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj of"><img src="../Images/85609d53d3501a9d06ab573b3abaa06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8fyTA15JuvnUYdUw"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">照片由<a class="ae li" href="https://unsplash.com/@jontyson?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔恩·泰森</a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="390c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">你应该从一开始就计划好的一个非常重要的特性就是所谓的<em class="oe">重播</em>。在许多批处理应用程序中，输入数据是按时间片(每天或每小时)提供的，而您只想处理新数据。</p><p id="851a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">但是如果出了问题会怎么样呢？例如，如果输入数据不完整或损坏，应该怎么办？或者您的应用程序包含一些逻辑错误并产生错误的结果？这种情况的简单答案是<em class="oe">重播</em>。这个术语指的是应用程序在出现任何问题时重新处理旧数据的能力。但是这种能力不会自动出现，您必须仔细考虑您的数据管理策略，如何组织您的输入和我们的输出数据以支持重新运行。</p><p id="fa07" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">重新运行需要几个方面，这将在接下来的主题中单独讨论。</p><h1 id="e1ee" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">3.每个分区一个编写器</h1><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj og"><img src="../Images/f2f17e6a5b65cde61ffc8d9bfde5e62a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MbfRlLRpq9sNYUHc"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated"><a class="ae li" href="https://unsplash.com/@nananadolgo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Nana Smirnova </a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="70dc" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">为了支持重新运行，您必须考虑数据组织。如果出现任何错误，理想情况下，您可以简单地删除特定批处理运行的输出，并用新的批处理运行的结果替换它。</p><p id="9c5f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果您使用一些简单的<em class="oe">分区</em>机制，这是非常容易实现的。使用<em class="oe">分区</em>我指的是使用子文件夹(用于基于文件的输出)或配置单元分区(当您使用配置单元时)来组织逻辑上属于同一输出的数据(就像单个配置单元表)。基本思想是每个批处理运行都应该写入一个单独的分区。</p><p id="9ef9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">例如，如果您的应用程序每小时处理一次新数据，只需使用数据和时间作为标识符来创建分区。在基于文件的工作流中，分区是一个目录，对于一些包含客户和交易数据的虚拟数据仓库，它可能如下所示</p><pre class="kt ku kv kw gu oh oi oj ok aw ol bi"><span id="93dc" class="om mu iu oi b gz on oo l op oq">/warehouse/shop_db/customer_transactions/hour=2020-09-12T08:00/</span></pre><p id="3f1d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在该目录中，存储了 2020 年 9 月 12 日 08:00 运行的特定作业的所有文件。如果批处理运行中出现任何错误，您可以简单地删除整个目录并重新启动作业。</p><p id="8045" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果您正在使用 Hive(如果您在一些共享文件系统或对象存储上使用 Spark，我强烈建议您这样做)，分区是 Hive 的核心特性。不幸的是，Spark 不太支持写入特定分区(但这种限制是可以解决的)。</p><h1 id="30a1" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">4.模式管理</h1><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj or"><img src="../Images/b4c65506b50199378009e545fe7022be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eHFDuIBM8F0aQ3ja"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">塞巴斯蒂安·赫尔曼在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4183" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">模式管理是一个非常重要的主题。这个术语指的是所有项目和开发任务有关的数据输入和输出格式。</p><p id="5bd9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">当您读入某个源系统的数据时，您期望数据的特定格式。这包括技术文件格式(如 CSV、JSON 或 Parquet)和用于存储数据的一组列和数据类型。我强烈推荐两件事:</p><ol class=""><li id="fbf4" class="mf mg iu ll b lm ln lp lq ls mh lw mi ma mj me oc ml mm mn bi translated">明确对输入模式的期望。这意味着不要简单地让应用程序推断正确的类型。</li><li id="2156" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated">为改变做好准备。我经常被告知“输入模式永远不会改变”——而这个假设最终总是被证明是错误的。</li></ol><p id="30fc" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">第一个建议提高了应用程序的健壮性，因为静默模式更改可以更早地被检测到，因为如果输入数据不再与您期望的模式匹配，应用程序应该报告一个错误(一些放宽是允许的，甚至是可取的，下面将详细介绍)。许多公司甚至有一些轻量级的组织过程来协商模式——这在我看来是正确的，因为由另一个应用程序获取的数据导出模式是一个技术契约。双方(交付方和消费方)都应该通过显式地使用模式进行写入和读取来意识到这一点。</p><p id="1666" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">第二个建议更难，尤其是与重新运行功能相结合。更改数据处理以反映新版本的输入模式可能会破坏重新处理旧数据的能力，因为旧数据可能是使用旧版本的模式存储的。这意味着您需要考虑您的数据管道如何可能与不同的模式版本一起工作。一个简单的解决方案是使用旧版本的数据管道来处理旧数据，但这通常不是一个好的选择，因为旧版本的应用程序缺少一些也应该应用于旧数据的重要功能。</p><p id="72ee" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">所以我建议用不同的模式版本重新运行，创建某种与所有版本的输入数据兼容的超级模式——至少在应用任何业务逻辑之前作为应用程序中的内部中间表示。我还建议尝试与源系统协商，只允许新模式版本<em class="oe">的<em class="oe">兼容变更</em>。</em>不幸的是，术语<em class="oe">兼容</em>高度依赖于所使用的技术，例如，Spring 对于像 Spark 一样改变 JSON 中的类型有其他限制。</p><h1 id="63b2" class="mt mu iu bd mv mw mx my mz na nb nc nd kj ne kk nf km ng kn nh kp ni kq nj nk bi translated">5.数据谱系关系</h1><p id="e766" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">最后，作为一个更广泛的概念，某种“数据所有权”应该在每个项目中可用。这个术语实际上指的是两个可能不同的方面，在这两个方面，如果出现任何需要注意的问题或事件，责任应该是明确的:</p><ol class=""><li id="744c" class="mf mg iu ll b lm ln lp lq ls mh lw mi ma mj me oc ml mm mn bi translated"><strong class="ll je">物理数据所有权。</strong>所有数据最终都存储在某个系统上，无论是在云中还是在本地数据中心，甚至是在我桌子下面的服务器上(不推荐)。需要有一个团队负责操作这个系统(是的，即使是云也需要一些操作)，包括备份、更新等。该角色通常由 IT 运营部门负责。</li><li id="03dc" class="mf mg iu ll b lm mo lp mp ls mq lw mr ma ms me oc ml mm mn bi translated"><strong class="ll je">业务数据所有权。</strong>除了物理所有权，每个数据源还需要一个业务所有者。该人员或团队负责存储的数据类型、数据模式以及与其他系统的接口。这个角色必须由定义存储数据的团队拥有。我总是推荐一个简单的规则:写数据的团队也拥有数据。这也意味着不允许其他团队写入自己的数据——除了一些用于数据交换的接口区域。</li></ol></div><div class="ab cl nq nr hy ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="in io ip iq ir"><h1 id="715a" class="mt mu iu bd mv mw nx my mz na ny nc nd kj nz kk nf km oa kn nh kp ob kq nj nk bi translated">最后的话</h1><p id="6d8f" class="pw-post-body-paragraph lj lk iu ll b lm nl ke lo lp nm kh lr ls nn lu lv lw no ly lz ma np mc md me in bi translated">这是关于使用 Apache Spark 构建健壮的数据管道的系列文章的第 1 部分。您可能会觉得有点被出卖了，因为它没有包含任何实际的代码。然而，我认为首先讨论一些概念是很重要的。下一部分将更多地关注 Apache Spark。</p></div></div>    
</body>
</html>