<html>
<head>
<title>DeepWalk: Its Behavior and How to Implement It</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepWalk:它的行为和如何实现它</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deepwalk-its-behavior-and-how-to-implement-it-b5aac0290a15?source=collection_archive---------18-----------------------#2020-09-11">https://towardsdatascience.com/deepwalk-its-behavior-and-how-to-implement-it-b5aac0290a15?source=collection_archive---------18-----------------------#2020-09-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/fa347a9e3a458b458dae6de9f98c77ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*N9MFSe6PSqhhAwlv"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@bracht?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">法比奥·布拉克特</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><div class=""/><div class=""><h2 id="b0be" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">使用 Python、Networkx 和 Gensim 快速分析和评估图网络关系的备忘单</h2></div><p id="03ef" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图形数据结构表示复杂交互的能力带来了新的方法来分析和分类由它们的共同交互定义的实体。虽然这些分析在发现社区内的不同结构方面是强大的，但是它们缺乏对图形的各个方面进行编码以输入到传统的机器学习算法中的能力。随着 DeepWalk 的提出，[1]图中的相互作用可以被简单的神经网络捕获并编码到可由上述 ML 算法使用的嵌入中。然而，虽然有文章简单介绍了 DeepWalk 算法，但我能找到的很少文章提供了代码并讨论了这些系统的实现细节。细节包括模型参数化、部署考虑和处理不可见数据。</p><p id="0273" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇短文中，我将提供对图网络、Word2Vec / Skip-Gram 和 DeepWalk 过程的高级概述。为了帮助解决这个问题，我将给出一个多类分类的例子来演示这个算法。之后，我将考虑不同的参数配置，并展示它们对算法性能的影响。最后，我将概述在系统中部署和处理不可见数据的一些注意事项。</p><h1 id="99be" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">图形网络</h1><p id="7f7e" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">图是有效地表示生态系统中两个或多个对象之间的交互的数据结构。该结构由两个对象定义，一个节点<em class="mr">或</em>顶点定义系统内的实体。在本文中，我将使用一个电子商务网站上的购物网络示例，图中的节点是正在销售的产品。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ebd28ea58e67919cf8a2f00ff758cc37.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*-N7-4Qei0HU9qJ57Bg-fIg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><em class="mx">作者图片</em></p></figure><p id="488f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图的另一个方面是连接它们的东西:一条<em class="mr">边</em>，它定义了连接两个节点的交互。一条边可以是有向的，显示出一种 to-from 关系——想象一个人 A 给人 b 发了一封电子邮件。一条边也可以有一个定义他们交互的<em class="mr">权重</em>。在我们的例子中，我们可以定义一个边缘权重，代表在我们的电子商务网站上购买了<em class="mr">和</em>两种产品的消费者的比例。</p><h1 id="183f" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated"><strong class="ak">深度行走算法:</strong></h1><p id="9ffd" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">DeepWalk 是一种图形神经网络<em class="mr">【1】</em>—一种直接在目标图形结构上操作的神经网络。它使用随机路径遍历技术来洞察网络中的局部结构。它通过利用这些随机路径作为序列来做到这一点，然后使用这些序列来训练 Skip-Gram 语言模型。为了简单起见，本文中我们将使用 Gensim 包 Word2Vec 来训练我们的 Skip-Gram 模型。</p><h2 id="a315" class="my lv jj bd lw mz na dn ma nb nc dp me lh nd ne mg ll nf ng mi lp nh ni mk nj bi translated"><strong class="ak"> Word2Vec </strong></h2><p id="de34" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">DeepWalk 算法的这个简单实现非常依赖于 Word2Vec 语言模型[2]。由谷歌在 2013 年推出的 Word2Vec 允许将单词嵌入到 n 维空间中，相似的单词彼此靠近。这意味着经常一起使用/在类似情况下使用的单词将具有较小的余弦距离。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/8de941d6740ba8d5bed6959224157aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*byYI20Y37rWdNSE9TVXHVg.jpeg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">单词嵌入行为的三维示例，<em class="mx">作者的图片</em></p></figure><p id="0d9b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Word2Vec 通过使用<strong class="la jk"> Skip-Gram </strong>算法将目标单词与其上下文进行比较来实现这一点。在高层次上，Skip-Gram 使用滑动窗口技术进行操作——在给定中间目标单词的情况下，它试图预测周围的单词。对于我们试图将图中的相似节点编码为在 n 维空间中彼此靠近的用例，这意味着我们有效地试图猜测我们网络中目标节点周围的邻居<em class="mr">。</em></p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/2876a1f7e3b3193d5050d23287f88a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ex7fqjCiw34CnCUBH4d2EQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">一个来自 S. Doshi [3]的 Skip-Gram 示例，修改了取自原作者的图像[2]</p></figure><h2 id="1ae5" class="my lv jj bd lw mz na dn ma nb nc dp me lh nd ne mg ll nf ng mi lp nh ni mk nj bi translated">深度行走</h2><p id="eb70" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">DeepWalk 通过图使用随机路径来揭示网络中的潜在模式，然后这些模式被神经网络学习和编码，以产生我们的最终嵌入。这些随机路径以极其简单的方式生成:从目标根开始，随机选择那个节点的一个<em class="mr">邻居，并将其添加到路径中，接下来，随机选择那个</em>节点的一个<em class="mr">邻居，并继续遍历，直到已经走了期望的步数。以电子商务为例，这种对网络路径的重复采样产生了一个产品 id 列表。然后，这些 ID 被视为句子中的标记，使用 Word2Vec 模型从这些 ID 中学习状态空间。更简洁地说，DeepWalk 过程遵循以下步骤:</em></p><h2 id="0f97" class="my lv jj bd lw mz na dn ma nb nc dp me lh nd ne mg ll nf ng mi lp nh ni mk nj bi translated"><strong class="ak">DeepWalk 过程分几个步骤进行:</strong></h2><ol class=""><li id="f1a9" class="nm nn jj la b lb mm le mn lh no ll np lp nq lt nr ns nt nu bi translated">对于每个节点，从该节点开始执行 N 个“随机步骤”</li><li id="6c08" class="nm nn jj la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">将每次遍历视为一系列节点 id 字符串</li><li id="151b" class="nm nn jj la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">给定这些序列的列表，使用 Skip-Gram 算法对这些字符串序列训练一个 word2vec 模型</li></ol><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/40fee62b113517bab57f99f6d7e39b0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*qT3x8XkY44iLbxhNcF3p2Q.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><em class="mx">作者图片</em></p></figure><h2 id="48b9" class="my lv jj bd lw mz na dn ma nb nc dp me lh nd ne mg ll nf ng mi lp nh ni mk nj bi translated">这在代码中看起来像什么？</h2><p id="f589" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我们从核心的 networkx 数据结构开始，它定义了一系列产品，这些产品由它们的 ID 给出。两个产品之间的顶点是在电子商务生态系统中共同购买的产品。首先，我们要定义一个函数<strong class="la jk"> get_random_walk </strong> (Graph，Node_Id):</p><pre class="mt mu mv mw gt ob oc od oe aw of bi"><span id="08d7" class="my lv jj oc b gy og oh l oi oj"># Instantiate a undirected Networkx graph<br/>G = <strong class="oc jk">nx.Graph</strong>()<br/>G.<strong class="oc jk">add_edges_from</strong>(list_of_product_copurchase_edges)</span><span id="d744" class="my lv jj oc b gy ok oh l oi oj"><strong class="oc jk">def get_random_walk</strong>(graph:nx.Graph, node:int, n_steps:int = 4)-&gt;List[str]:<br/>   """ Given a graph and a node, <br/>       return a random walk starting from the node <br/>   """</span><span id="5427" class="my lv jj oc b gy ok oh l oi oj">   local_path = [str(node),]<br/>   target_node = node</span><span id="e7dc" class="my lv jj oc b gy ok oh l oi oj">   <strong class="oc jk">for</strong> _ <strong class="oc jk">in</strong> <strong class="oc jk">range</strong>(n_steps):<br/>      neighbors = <strong class="oc jk">list</strong>(<strong class="oc jk">nx.all_neighbors</strong>(graph, target_node))<br/>      target_node = <strong class="oc jk">random.choice</strong>(neighbors)<br/>      <strong class="oc jk">local_path.append</strong>(<strong class="oc jk">str</strong>(target_node))</span><span id="99b8" class="my lv jj oc b gy ok oh l oi oj">   <strong class="oc jk">return</strong> local_path</span><span id="8df0" class="my lv jj oc b gy ok oh l oi oj">walk_paths = []</span><span id="be5a" class="my lv jj oc b gy ok oh l oi oj"><strong class="oc jk">for</strong> node <strong class="oc jk">in</strong> <strong class="oc jk">G.nodes</strong>():<br/>   <strong class="oc jk">for</strong> _ <strong class="oc jk">in</strong> <strong class="oc jk">range</strong>(10):<br/>      walk_paths.<strong class="oc jk">append</strong>(<strong class="oc jk">get_random_walk</strong>(G, node))<br/> <br/>walk_paths[0]<br/><strong class="oc jk"><em class="mr">&gt;&gt;&gt;</em></strong><em class="mr"> [‘10001’, ‘10205’, ‘11845’, ‘10205’, ‘10059’]</em></span></pre><p id="f671" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些随机漫步提供给我们的是一系列字符串，它们充当从起始节点开始的路径——沿着列表从一个节点随机漫步到下一个节点。我们接下来要做的是将这个字符串列表视为一个句子，然后利用这些字符串序列来训练一个 Word2Vec 模型</p><pre class="mt mu mv mw gt ob oc od oe aw of bi"><span id="2c78" class="my lv jj oc b gy og oh l oi oj"># Instantiate word2vec model<br/>embedder = <strong class="oc jk">Word2Vec</strong>(<br/>   window=4, sg=1, hs=0, negative=10, alpha=0.03, min_alpha=0.0001,    <br/>   seed=42<br/>)</span><span id="5d82" class="my lv jj oc b gy ok oh l oi oj"># Build Vocabulary<br/><strong class="oc jk">embedder.build_vocab</strong>(walk_paths, progress_per=2)</span><span id="ca40" class="my lv jj oc b gy ok oh l oi oj"># Train<br/><strong class="oc jk">embedder.train</strong>(<br/>   walk_paths, total_examples=embedder.corpus_count, epochs=20, <br/>   report_delay=1<br/>)</span></pre><h1 id="aedf" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">调谐和参数</h1><p id="b09c" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">现在我们已经有了 DeepWalk 的基本结构，在 Word2Vec 模型的一般模型参数的之外，还有许多方面是可参数化的<em class="mr">。这些可能包括:</em></p><ol class=""><li id="f85e" class="nm nn jj la b lb lc le lf lh ol ll om lp on lt nr ns nt nu bi translated">为 W2V 训练数据执行的随机行走的次数</li><li id="6c54" class="nm nn jj la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">从节点开始的每次行走的深度</li></ol><p id="3d68" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将利用一个样本数据集上的通用分类方法来展示这些参数如何影响模型的性能。在上面描述的图表中——利用一系列产品，用一个图表定义共同购买的产品——我们试图将产品分类到它们各自的 10 个类别。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/6427bfaddaa454a4d6c94dd9f9c8788f.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*CDbHdodmQFpU_prXgY7w8g.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="e8e0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面显示了在我们的 Word2Vec 模型的节点向量上训练的分类器的分类性能(精确度),在 y 轴上使用增加的随机行走次数，在 x 轴上使用增加的随机行走深度。<strong class="la jk">我们看到的是，准确性随着两个参数的增加而增加，但随着两个参数的增加，回报率逐渐降低</strong>。需要注意的一点是，随着行走次数的增加，训练时间<strong class="la jk">线性增加，</strong>所以训练时间会随着行走次数的增加而激增。例如，左上角的训练时间只相差 15 秒，而右下角的训练时间则相差一个多小时。</p><h1 id="9e88" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">部署系统:</h1><h2 id="d559" class="my lv jj bd lw mz na dn ma nb nc dp me lh nd ne mg ll nf ng mi lp nh ni mk nj bi translated"><em class="mx">热启动再培训</em></h2><p id="1166" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">既然我们知道了它的行为方式，我们如何让它变得实用呢？在大多数图形系统中，核心问题是更新和维护系统，而不必立刻重新训练整个模型。幸运的是，由于 DeepWalks 与 NLP 的关系，我们可以依赖类似的更新过程。使用 gensim 时，更新算法更加简单，其流程如下:</p><ol class=""><li id="25aa" class="nm nn jj la b lb lc le lf lh ol ll om lp on lt nr ns nt nu bi translated">将目标节点添加到图中</li><li id="d88f" class="nm nn jj la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">从该节点执行随机行走</li><li id="bd52" class="nm nn jj la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">使用这些序列来更新 Word2Vec 模型</li></ol><pre class="mt mu mv mw gt ob oc od oe aw of bi"><span id="e78d" class="my lv jj oc b gy og oh l oi oj"># Add new nodes to graph from their edges with current nodes<br/><strong class="oc jk">G.add_edges_from</strong>([new_edges])</span><span id="ff9e" class="my lv jj oc b gy ok oh l oi oj"># From a list of the new nodes' product_ids (unknown_nodes) get rw's<br/>sequences = [<strong class="oc jk">get_random_walks</strong>(G, node) <strong class="oc jk">for</strong> node <strong class="oc jk">in</strong> unknown_nodes]</span><span id="8757" class="my lv jj oc b gy ok oh l oi oj"><strong class="oc jk">model.build_vocab</strong>(new_nodes, update=True)<br/><strong class="oc jk">model.train</strong>(sequences,total_examples=model.corpus_count, epochs=model.epochs)</span></pre><h2 id="1f5a" class="my lv jj bd lw mz na dn ma nb nc dp me lh nd ne mg ll nf ng mi lp nh ni mk nj bi translated">不需要再培训</h2><p id="7521" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">或者，有一种更简单的方法来处理系统中的新节点。您可以利用模型的已知嵌入来推断未知节点的嵌入。遵循与前面的实现类似的模式:</p><ol class=""><li id="4359" class="nm nn jj la b lb lc le lf lh ol ll om lp on lt nr ns nt nu bi translated">将目标节点添加到图中</li><li id="e9b3" class="nm nn jj la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">从该节点执行随机行走</li><li id="9031" class="nm nn jj la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">聚集来自随机行走的嵌入，然后使用该聚集来代替未知节点嵌入</li></ol><pre class="mt mu mv mw gt ob oc od oe aw of bi"><span id="3b5f" class="my lv jj oc b gy og oh l oi oj"># Add new nodes to the graph from their edges with current nodes<br/><strong class="oc jk">G.add_edges_from</strong>([new_edges])</span><span id="9cda" class="my lv jj oc b gy ok oh l oi oj">sequences = [<strong class="oc jk">get_random_walk</strong>(G, node) <strong class="oc jk">for</strong> node <strong class="oc jk">in</strong> unknown_nodes]</span><span id="0097" class="my lv jj oc b gy ok oh l oi oj"><strong class="oc jk">for</strong> walk <strong class="oc jk">in</strong> sequences:<br/>   nodes_in_corpus = [<br/>        node <strong class="oc jk">for</strong> node <strong class="oc jk">in</strong> walk <strong class="oc jk">if</strong> node <strong class="oc jk">in</strong> word2vec<br/>   ]<br/>   node_embedding = [ # here we take the average of known embeddings<br/>        <strong class="oc jk">np.mean</strong>(embedder[nodes_in_corpus])<br/>   ]</span></pre><p id="ab32" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，我们的嵌入是从未知节点开始的随机行走中已知节点的平均值。这种方法的好处是它的速度，我们不需要重新训练任何东西，并且正在执行几个对 Gensim 中类似字典的数据结构的<em class="mr"> O </em> ( <em class="mr"> 1 </em>)调用。缺点是它的不精确性，在没有重新训练模型的情况下，新节点和它的邻居之间的交互是近似的，并且只和你的聚合函数一样好。要更深入地了解这些方法，您可以查阅讨论此类聚合功效的论文，如 TF-IDF 平均等。[4]</p><p id="5348" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在过去的 10 分钟里，我们走过了[<em class="mr">哈。</em>】DeepWalk 的核心组件，如何实现，以及在自己的工作中实现的一些考虑。对于图网络的评估，有许多可能性可以考虑，鉴于其简单性和可伸缩性，DeepWalk 当然应该在其他可用的算法中加以考虑。下面你可以找到一些上面概述的算法的参考，以及一些关于单词嵌入的进一步阅读。</p><h2 id="074e" class="my lv jj bd lw mz na dn ma nb nc dp me lh nd ne mg ll nf ng mi lp nh ni mk nj bi translated"><strong class="ak">来源:</strong></h2><p id="484e" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">[1] Perozzi 等人。<strong class="la jk">深度行走:在线学习社交表征</strong><a class="ae jg" href="https://arxiv.org/pdf/1403.6652.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1403.6652.pdf</a></p><p id="2d82" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]米科洛夫等人。<strong class="la jk">向量空间中单词表示的有效估计</strong><a class="ae jg" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1301.3781.pdf</a></p><p id="50f9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3] S .多希。<strong class="la jk"> Skip-Gram: NLP 上下文单词预测算法:</strong><a class="ae jg" rel="noopener" target="_blank" href="/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c?gi=8ff2aeada829">https://towardsdatascience . com/Skip-Gram-NLP-context-words-prediction-algorithm-5 bbf 34 f 84 e0c？gi = 8 ff 2 eada 829</a></p><p id="1dd0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4] Levy 等人。<strong class="la jk">利用从单词嵌入中吸取的经验教训改进分布相似性【https://www.aclweb.org/anthology/Q15-1016/</strong>T22</p></div></div>    
</body>
</html>