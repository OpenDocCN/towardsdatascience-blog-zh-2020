<html>
<head>
<title>An Introduction to Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维引论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-dimensionality-reduction-e873449c865?source=collection_archive---------32-----------------------#2020-04-04">https://towardsdatascience.com/an-introduction-to-dimensionality-reduction-e873449c865?source=collection_archive---------32-----------------------#2020-04-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="02c6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">以及如何应用它</h2></div><p id="1e5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">背景</strong></p><p id="9a89" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在统计学、机器学习和信息论中，降维是通过获得一组主变量来减少所考虑的随机变量的数量的过程。</p><p id="cf92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">高维数据集是具有大量列(或变量)的数据集。这样的数据集很容易迷失，并可能导致过度拟合等挑战。幸运的是，变量(或特征)通常是相关的，因为高维数据通常由少量简单变量主导。我们可以找到变量的子集来表示数据中相同级别的信息，或者将变量转换为一组新的变量，而不会丢失太多信息。这就是降维算法变得有用的时候。</p><p id="c3f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用的数据集检查台湾客户的信用卡违约付款。它来自加州大学欧文分校<a class="ae lb" href="https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients" rel="noopener ugc nofollow" target="_blank">机器学习库。</a></p><p id="a9b0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据集采用了一个二元变量，默认付款(是= 1，否= 0)，作为响应变量。它包含以下23个变量作为解释变量:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/2de3e4f8e9e34b24d7931f7ac0808fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-NhE1-SjAiKmFWiZmOHnDg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">提取和清理后的数据集</p></figure><p id="4104" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X1:给定信用的金额</p><p id="0b91" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X2:性别(1 =男性；2 =女性)。</p><p id="8fbe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X3:教育(1 =研究生院；2 =大学；3 =高中；4 =其他)。</p><p id="c39b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X4:婚姻状况(1 =已婚；2 =单身；3 =其他)。</p><p id="776c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X5:年龄(年)。</p><p id="dd9a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X6 — X11:以往付款历史(2005年4月至9月)。还款状态的衡量标准是:-1 =按时支付；1 =延迟一个月付款；2 =付款延迟两个月；。。。；9 =延迟付款九个月，以此类推。</p><p id="f12b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X12-X17:账单金额(2005年4月至9月)</p><p id="dc32" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X18-X23:上次付款金额(2005年4月至9月)</p><p id="6c5b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">摆弄违约(或另一个目标变量)的分布可能是一个有趣的练习。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ls"><img src="../Images/6a3c6d562141af5896485a4ae5a3f5d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0cwcKV3pztFiH_lnllp4xw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">如果绘制不当，多峰分布可能会被掩盖</p></figure><p id="3f48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">勘探和规模数据</strong></p><p id="94eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据集有3000多行和24列，其中23列是解释性的。没有空值(分数！)并且数据集似乎严重偏向(大约78%)没有默认实例。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/59bf54a0d6fc9421f01dc249f01112a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*S0Ej4NUdbrS5ueXd1uHIvQ.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">数据集严重偏向非违约案例</p></figure><p id="a771" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">特征缩放规范了数据列，是数据预处理的重要步骤。将特定范围内的数据标准化也有助于加快计算速度。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lu"><img src="../Images/f220f9cb3de961e4a2bf059b470a5153.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xT3bN8-2YAh9chGipZz2gg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">健壮的定标器不容易出现异常值</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lv"><img src="../Images/991d0ece257fb699226e38fc40d059d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k3bCq2M4X444jALp9zX5Pg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">新缩放的数据</p></figure><p id="cc15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">将数据分成测试和训练集</strong></p><p id="6762" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在机器学习中，构建了可以从数据中学习并对数据进行预测的算法。模型最初适合训练数据集，训练数据集是用于拟合参数的一组示例。使用特定的机器学习方法在训练数据集上训练该模型。然而，你不能做一个简单的训练测试分割，它是一个随机分割，忽略了类的分布或比例。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lw"><img src="../Images/29c37eca46c4f8ee996f4d75e8bf2c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XG7tQqxVvlT4QkjGx56e4Q.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">一个好的测试规模目标是大约20–25%</p></figure><p id="6f38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，您最终会得到数据分布完全不同的训练集和测试集。在与测试集完全不同的数据分布上训练的模型将在验证时表现不佳。</p><p id="b7ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">随机欠采样</strong></p><p id="f2fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了获得平衡的数据集并避免过度拟合，删除数据点非常重要。在这个数据集中，大约70–75%的条目或非默认实例。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lx"><img src="../Images/37f078cef5cfb0152a81423730ff7c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MI6CRFzmxgpCHGT5gEu_fQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">通过移动数据，您可以在每次运行脚本时查看您的模型是否保持其准确性</p></figure><p id="4d28" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们希望避免模型无法区分默认和非默认实例，而是“专注于”后者。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/502e0c558f86c3ac52a01e3388541ef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*nCfb2BYPp7Tg1B5PsNDAPg.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">新数据集不再不平衡</p></figure><p id="e3e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们删除条目，直到默认实例和非默认实例之间的比例达到50/50。其中一个问题是大量信息丢失。</p><p id="b3ce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">相关矩阵</strong></p><p id="49c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相关矩阵允许我们确定哪些特征对特定情况是否会成为违约的实例影响最大。重要的是，我们使用正确的子样本，以便我们看到哪些特征与违约实例高度相关。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lz"><img src="../Images/b2047ec1d7aa3a499433a722a0dad9a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NR--aejphrIid0QHiud7iw.png"/></div></div></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ma"><img src="../Images/709ef941f7918940075acb4b071c7902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lZ8WZM95lPEPhtWlQ-xDkw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">在这种情况下，不平衡数据集和平衡数据集之间没有区别</p></figure><p id="09c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当这些值越低，最终结果越有可能是违约交易时，就会出现负相关。当这些值越高，最终结果越有可能是违约交易时，就会出现正相关。在这种情况下，余额交易变量X12-X17似乎关联最大，因此这是我们将重点关注的内容。</p><p id="22cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">异常检测</strong></p><p id="3a3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从与我们的类高度相关的特征中移除异常值将会产生更准确的模型。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lz"><img src="../Images/b2047ec1d7aa3a499433a722a0dad9a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NR--aejphrIid0QHiud7iw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">应对异常情况进行评估，至少评估上一步中确定的所有特征</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mb"><img src="../Images/07d5a9cab93801fc180a0a49a7cbdfca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZprLcGuqsKxrklsA3eARg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">记录输出有助于比较多个范围</p></figure><p id="c1a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一种方法是四分位数范围法(IQR)，它可以消除落在第25个和第75个百分位数之间的实例。</p><p id="a485" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">降维</strong></p><p id="c034" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> PCA </strong></p><p id="d5e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">主成分分析(PCA)的思想是降低由大量相关变量组成的数据集的维度，同时尽可能多地保留数据中的方差。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mc"><img src="../Images/0c2d79703744083301c71f3bce20e4f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XJeWIyQrMJwxMUYVG1yXdw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">PCA允许部件的减少</p></figure><p id="eb03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">主成分分析帮助我们从现有的大量变量中提取一组新的变量。这些新提取的变量被称为主成分。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi md"><img src="../Images/9ee360140fc6177ca6b608076dc57ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JAe4H7605E81Y47HMw8Czw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">如果您愿意，可以删除剩余的组件</p></figure><p id="0e0a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">主成分是原始变量的线性组合。其他复杂因素包括:</p><ul class=""><li id="4009" class="me mf iq kh b ki kj kl km ko mg ks mh kw mi la mj mk ml mm bi translated">提取主成分，使得第一主成分解释数据集中的最大方差</li><li id="c5e0" class="me mf iq kh b ki mn kl mo ko mp ks mq kw mr la mj mk ml mm bi translated">第二主成分试图解释数据集中的剩余方差，并且与第一主成分不相关</li><li id="2766" class="me mf iq kh b ki mn kl mo ko mp ks mq kw mr la mj mk ml mm bi translated">第三个主成分试图解释前两个主成分无法解释的差异，依此类推</li></ul><p id="808a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">单变量分解</strong></p><p id="01cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">奇异值分解(SVD)可用于将我们的原始数据集分解成它的组成部分，这导致维数减少。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ms"><img src="../Images/3c16c6c476d8c1f359c88c6ba2bdab77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZh8GqPMsGuRXexscqhLaQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">奇异值分解识别出7个重要成分，比主成分分析少5个</p></figure><p id="6244" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它用于移除数据集中的冗余要素。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mt"><img src="../Images/a5aa6598e9f21da20990aea4f1efdb04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GKhL-usHp2XiCcPD5OdmfA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">奇异值分解图将揭示其矩阵分解技术</p></figure><p id="3be4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">奇异值分解将原始变量分解成三个组成矩阵。特征值和特征向量的概念用于确定这三个矩阵。</p><p id="ccf1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> t-SNE </strong></p><p id="7704" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">t-分布式随机近邻嵌入(t-SNE)是一种非线性降维技术，特别适合于在二维或三维的低维空间中可视化高维数据集</p><p id="c68a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基本上，t-SNE可以让你很容易地想象数据是如何在高维空间中排列的。</p><p id="5f2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">t-SNE算法计算高维空间和低维空间中的实例对之间的相似性度量。然后，它尝试使用成本函数来优化这两个相似性度量。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mu"><img src="../Images/6086c42c709676e77f7ca83b9de02f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PR5GOBBFepEZkbE2Qmlq-g.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">t-SNE的图受所选参数化的影响很大</p></figure><p id="5c3a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">t-SNE不同于PCA，它只保留小的成对距离或局部相似性，而PCA关心的是保留大的成对距离以最大化方差。</p><p id="c660" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">量词与超越</strong></p><p id="cf79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然这是对该数据集的采样降维方法的结束，但还可以做更多的事情！首先，一旦数据集的特征从标签中分离出来，就可以对数据使用分类器。这个特殊的数据集可能需要使用GridSearchCV或逻辑回归。</p><p id="f397" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还应该评估模型的欠拟合和过拟合，这可以分别通过高偏差和方差来确定。欠采样或过采样可能会影响模型检测默认情况的准确性。如您所见，在处理高维不平衡数据集时，需要考虑很多因素！</p></div></div>    
</body>
</html>