<html>
<head>
<title>Transformers for Multi-Label Classification made simple.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简化多标签分类的变压器。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1?source=collection_archive---------6-----------------------#2020-05-27">https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1?source=collection_archive---------6-----------------------#2020-05-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ec6c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">伯特、XLNet、罗伯塔等。对于多标签分类—逐步指南</h2></div><p id="1a5a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为一名数据科学家，我一直在学习文本分类的最新技术，我发现没有太多容易的例子来适应 transformers (BERT，XLNet 等。)进行<strong class="kh ir">多标签分类</strong>…所以我决定自己试试，这就是！</p><p id="6cd0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为对其他多标签文本分类博客帖子的敬意，我将使用<a class="ae lb" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data" rel="noopener ugc nofollow" target="_blank">有毒评论分类挑战</a>数据集。</p><p id="66cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章附有一个互动的<a class="ae lb" href="https://colab.research.google.com/github/rap12391/transformers_multilabel_toxic/blob/master/toxic_multilabel.ipynb" rel="noopener ugc nofollow" target="_blank">谷歌 Colab 笔记本</a>，所以你可以自己尝试一下。您所要做的就是将<em class="lc"> train.csv、test.csv 和 test_labels.csv </em>文件上传到实例中。让我们开始吧。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/d1576b3d4cdd46ff3be271893b627f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xHW9r3ijUtTCYDZ7QO5jrg.jpeg"/></div></div></figure><p id="7fec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本教程中，我将使用<a class="ae lb" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">拥抱脸的变形金刚库</a>以及<strong class="kh ir"> PyTorch(带 GPU) </strong>，尽管这可以很容易地适应 TensorFlow — <em class="lc">如果这与多类分类教程</em>一起获得牵引力，我可能稍后会为此编写一个单独的教程。下面我将训练一个 BERT 模型，但是<strong class="kh ir">我将向你展示在这个过程中为其他 transformer 模型改编这个代码是多么容易。</strong></p><h1 id="21f7" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">导入库</h1><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mh mi l"/></div></figure><h1 id="5261" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">加载和预处理训练数据</h1><p id="86cd" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">毒性数据集已经被清理并被分成训练集和测试集，因此我们可以加载训练集并直接使用它。</p><p id="cdd3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个 transformer 模型需要不同的标记化编码，这意味着句子的标记化方式和注意力屏蔽的使用可能会因您使用的 transformer 模型而异。令人欣慰的是，HuggingFace 的变形金刚库使得为每个模型实现变得极其容易。在下面的代码中，我们加载了一个预训练的 BERT 记号赋予器，并使用“batch_encode_plus”方法来获取记号、记号类型和注意掩码。随意加载适合您想要用于预测的模型的记号赋予器。例如，</p><pre class="le lf lg lh gt mo mp mq mr aw ms bi"><span id="f946" class="mt lq iq mp b gy mu mv l mw mx">BERT:<br/>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) </span><span id="6497" class="mt lq iq mp b gy my mv l mw mx">XLNet:<br/>tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=False) </span><span id="7982" class="mt lq iq mp b gy my mv l mw mx">RoBERTa:<br/>tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=False)</span></pre><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="d61e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们将使用 10%的训练输入作为验证集，这样我们就可以在训练时监控分类器的性能。在这里，我们希望确保我们利用了“分层”参数，这样就不会有看不见的标签出现在验证集中。为了进行适当的分层，我们将所有只在数据集中出现一次的标签，并强制它们进入训练集。我们还需要创建 PyTorch 数据加载器来加载用于训练/预测的数据。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mh mi l"/></div></figure><h1 id="e3f8" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">加载模型和设置参数</h1><p id="dca9" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">加载适当的模型可以如下所示，每个模型已经包含了一个单一的密集层用于分类。</p><pre class="le lf lg lh gt mo mp mq mr aw ms bi"><span id="e721" class="mt lq iq mp b gy mu mv l mw mx">BERT:<br/>model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)</span><span id="2e48" class="mt lq iq mp b gy my mv l mw mx">XLNet:<br/>model = XLNetForSequenceClassification.from_pretrained("xlnet-base-cased", num_labels=num_labels)</span><span id="08d0" class="mt lq iq mp b gy my mv l mw mx">RoBERTa:<br/>model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)</span></pre><p id="d37f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">优化器参数可以通过几种方式进行配置。这里我们使用了一个定制的优化参数(我在这方面做得更成功)，但是，您可以只传递注释中所示的“model.parameters()”。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mh mi l"/></div></figure><h1 id="3857" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">火车模型</h1><p id="32fd" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">使用<strong class="kh ir">“分类交叉熵”</strong>作为损失函数，将 HuggingFace 库配置为开箱即用的多类分类。因此，变压器模型的输出类似于:</p><pre class="le lf lg lh gt mo mp mq mr aw ms bi"><span id="52f1" class="mt lq iq mp b gy mu mv l mw mx">outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)</span><span id="3705" class="mt lq iq mp b gy my mv l mw mx"><strong class="mp ir">loss, logits = outputs[0], outputs[1]</strong></span></pre><p id="9af8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">但是，如果我们避免传入标签参数，模型将只输出逻辑值，我们可以使用这些逻辑值来计算多标签分类的损失。</strong></p><pre class="le lf lg lh gt mo mp mq mr aw ms bi"><span id="583e" class="mt lq iq mp b gy mu mv l mw mx">outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)</span><span id="0ec6" class="mt lq iq mp b gy my mv l mw mx"><strong class="mp ir">logits = outputs[0]</strong></span></pre><p id="0a29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是这样做的代码片段。这里我们使用<strong class="kh ir">“具有 Logits 的二元交叉熵”</strong>作为我们的损失函数。我们可以很容易地使用标准的“二元交叉熵”、“汉明损失”等。</p><p id="7e4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了验证，我们将使用微 F1 精度来监控跨时代的训练表现。为此，我们必须利用模型输出中的逻辑值，将它们传递给 sigmoid 函数(给出[0，1]之间的输出)，并对它们进行阈值处理(0.50)以生成预测。然后，这些预测可用于计算相对于真实标签的准确度。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="9695" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">维奥拉。我们已经准备好训练，现在运行它…我的训练时间在 20-40 分钟之间，取决于最大令牌长度和使用的 GPU。</p><h1 id="22e8" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">预测和指标</h1><p id="a12a" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">我们的测试集的预测类似于我们的验证集。在这里，我们将加载、预处理和预测测试数据。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mh mi l"/></div></figure><h1 id="4ca0" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">输出数据帧</h1><p id="8897" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">创建显示句子及其分类的输出数据框架。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mh mi l"/></div></figure><h1 id="ff7e" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">奖励—优化微 F1 精度的阈值</h1><p id="908d" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">迭代阈值以最大化微 F1 精度。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="152f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样！有问题请评论。这里是<a class="ae lb" href="https://colab.research.google.com/github/rap12391/transformers_multilabel_toxic/blob/master/toxic_multilabel.ipynb" rel="noopener ugc nofollow" target="_blank"> Google Colab 笔记本</a>的链接，以防你错过。如果您有任何私人问题，请随时通过 LinkedIn 或 Twitter 与我联系。</p><h1 id="dbed" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">参考资料:</h1><p id="63db" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated"><a class="ae lb" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">https://electricenjin . com/blog/look-out-Google-Bert-is-here-to-shake-up-search-queries</a></p><p id="4330" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">https://github.com/google-research/bert</a></p><p id="83ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/transformers</a></p><p id="aa75" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">【https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss T2】号</p><p id="d3bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">【https://arxiv.org/abs/1706.03762 T4】</p></div></div>    
</body>
</html>