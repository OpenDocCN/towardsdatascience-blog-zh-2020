<html>
<head>
<title>Latent Semantic Analysis — Deduce the hidden topic from the document</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">潜在语义分析—从文档中推断出隐藏的主题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/latent-semantic-analysis-deduce-the-hidden-topic-from-the-document-f360e8c0614b?source=collection_archive---------4-----------------------#2020-02-26">https://towardsdatascience.com/latent-semantic-analysis-deduce-the-hidden-topic-from-the-document-f360e8c0614b?source=collection_archive---------4-----------------------#2020-02-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="a8f2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让计算机学习和理解人类语言仍然是最困难的任务。语言包含巨大的词汇，每部作品根据上下文有不同的含义，让计算机学习上下文是一个悬而未决的问题。在此，我们将尝试推导出文本所代表的隐藏主题，并将这些知识用于文档聚类。</p><h1 id="17f4" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">主题模型</h1><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/0909746fd87a4a663b7158b71f48472f.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*EECZMH6ZpM8QjKl0joa0fw.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">分析vidhya —主题建模</p></figure><p id="4f45" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">主题模型是一种无监督的方式来推断隐藏的主题所代表的文本或文件。这个主题不是诸如体育、新闻或商业之类的实际主题，而是可以用来以最佳方式表示文本的词语。</p><p id="6f6c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种技术非常强大，可以在无人监督的情况下用于文档聚类。如果你使用过谷歌新闻，那么你会看到不同来源的新闻聚集在一起，如果这些新闻代表相似的主题。这是主题建模的应用之一。</p><h1 id="e5b6" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">潜在语义分析</h1><p id="77d1" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">潜在语义分析是一种通过理解文本的上下文来分析文本和发现隐藏主题的有效方法。</p><p id="b656" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">潜在语义分析(LSA)用于发现由文档或文本表示的隐藏主题。这个隐藏的主题然后被用于将相似的文档聚集在一起。LSA是一种无监督的算法，因此我们不知道文档的实际主题。</p><h1 id="5875" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">为什么是LSA？</h1><p id="5ec8" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">查找相似文档的最简单方法是使用文本的向量表示和余弦相似度。向量表示以向量的形式表示每个文档。这个向量被称为文档术语矩阵。</p><p id="56b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如:</p><pre class="ln lo lp lq gt md me mf mg aw mh bi"><span id="88b7" class="mi kp it me b gy mj mk l ml mm">a1 = "the petrol in this car is low"<br/>a2 = "the vehicle is short on fuel"</span></pre><p id="8f31" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">考虑以上两个字符串并形成上下文，我们可以理解这两个字符串是相似的。我们将尝试使用向量表示来找出这些字符串有多相似。</p><p id="c86f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上述示例的文档术语矩阵为:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/91f5753e5f55b03a3955ff4d0181296e.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*doGQQ1SHJZwCZpLYabt8pg.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">文档术语矩阵</p></figure><p id="bad2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">文档-术语矩阵的大小是(文档数量)*(词汇大小)。词汇量是所有文档中出现的唯一单词的总数。这里的词汇量是11，文档数是2。</p><p id="5099" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">文档之间的相似性是使用文档之间的余弦相似性矩阵来找出的。文档<code class="fe mo mp mq me b">a1</code>和<code class="fe mo mp mq me b">a2</code>之间的相似度是0.3086067，这太低了，因为文档在上下文中大多相似。这是文档术语矩阵的缺点，因此也是向量表示技术的缺点。另一个缺点是词汇量大，因为该语言具有巨大的词汇量，导致矩阵更大且计算成本高。</p><p id="272e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">向量表示法的这一缺点导致了寻找文档间相似性和隐藏主题的新技术的需求。该技术可以解决同义词的问题，并且计算上也不昂贵。建议的技术是潜在语义分析。</p><h1 id="a677" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">LSA的工作</h1><h2 id="9ccc" class="mi kp it bd kq mr ms dn ku mt mu dp ky kb mv mw lc kf mx my lg kj mz na lk nb bi translated">术语共现矩阵</h2><p id="9b75" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">这个矩阵的维数是(词汇大小)*(词汇大小)。它表示单词在数据集中出现的频率。矩阵帮助我们理解属于一起的单词。</p><p id="ea9d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于上面的例子，术语共生矩阵是:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/93047dbe2f7a60e35cfea9c8c790d2e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*tfY6S4RNPvf3O4coYyKIxw.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">术语共现矩阵</p></figure><p id="c84b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们所看到的，单词<code class="fe mo mp mq me b">the</code>和<code class="fe mo mp mq me b">is</code>是最常见的，但在句子的意思中不是很有用。我们将在本博客的后面看到如何使用这个矩阵及其好处。</p><h2 id="0b6f" class="mi kp it bd kq mr ms dn ku mt mu dp ky kb mv mw lc kf mx my lg kj mz na lk nb bi translated">概念</h2><p id="b0d4" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">LSA返回代表给定文档的概念而不是主题。概念是以最佳方式表示文档的单词列表。</p><p id="1694" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，在体育文档的数据集中，概念可以是</p><blockquote class="nd ne nf"><p id="4a54" class="jq jr ng js b jt ju jv jw jx jy jz ka nh kc kd ke ni kg kh ki nj kk kl km kn im bi translated">概念1:球、鞋、目标、胜利</p><p id="2d08" class="jq jr ng js b jt ju jv jw jx jy jz ka nh kc kd ke ni kg kh ki nj kk kl km kn im bi translated">概念2:球，球棒，得分，裁判</p></blockquote><p id="a06b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到两个概念——概念1代表足球，概念2代表板球。但是我们可以看到，这些概念可以有重叠的词，因此，整个词集共同代表一个概念，而不是单个的词。LSA试图使用术语共现矩阵来找到被称为概念的最佳单词集来表示文档。</p><blockquote class="nd ne nf"><p id="e2a6" class="jq jr ng js b jt ju jv jw jx jy jz ka nh kc kd ke ni kg kh ki nj kk kl km kn im bi translated">概念也是通过降维来表示文档的一种方式。</p></blockquote><h2 id="4c46" class="mi kp it bd kq mr ms dn ku mt mu dp ky kb mv mw lc kf mx my lg kj mz na lk nb bi translated">奇异值分解</h2><p id="bf00" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">我们可以看到文档-术语矩阵非常稀疏并且大小很大。在如此大的矩阵上的计算是昂贵的，并且没有非常显著的结果，并且矩阵中的许多值是零。为了降低计算复杂度并获得更相关和有用的结果，使用了SVD。</p><p id="6f31" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">SVD将矩阵分解成三个不同的矩阵:正交列矩阵、正交行矩阵和一个奇异矩阵。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/a5f52864e7eb55d42b1b4a7e11eef581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*sF__SqZtx-131YxX18nC9g.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">研究之门</p></figure><p id="b3b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">奇异值分解的主要优点是我们可以将矩阵的规模从数百万减少到100或1000。上图中的<code class="fe mo mp mq me b">K</code>是矩阵的秩。类似地，如果我们仅使用k列和k行，那么我们也可以近似地计算矩阵A，而没有任何大的损失。</p><p id="a41d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在SVD计算期间，我们计算代表项共生矩阵的<code class="fe mo mp mq me b">A*(A'T)</code>。这意味着上面矩阵中带有索引<code class="fe mo mp mq me b">(i,j)</code>的值表示术语(I)和术语(j)在文档数据集中同时存在的次数。点击了解SVD更多信息<a class="ae nl" href="https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="6d4e" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">履行</h1><p id="3184" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">实现是理解概念的最佳方式。我们将使用一个小例子来实现LSA，这将有助于我们理解LSA的工作和输出。</p><p id="62ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用的文档是</p><pre class="ln lo lp lq gt md me mf mg aw mh bi"><span id="5bb6" class="mi kp it me b gy mj mk l ml mm">a1 = "He is a good dog."<br/>a2 = "The dog is too lazy."<br/>a3 = "That is a brown cat."<br/>a4 = "The cat is very active."<br/>a5 = "I have brown cat and dog."</span></pre><p id="deff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里我们可以看到，必须生成两个概念，一个代表猫，另一个代表狗。</p><p id="5ece" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将此文档列表转换为数据帧:</p><pre class="ln lo lp lq gt md me mf mg aw mh bi"><span id="1c6d" class="mi kp it me b gy mj mk l ml mm">import pandas as pd<br/>df = pd.DataFrame()<br/>df["documents"] = [a1,a2,a3,a4,a5]<br/>df</span></pre><p id="75f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mo mp mq me b">df</code>应该是这样的:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f91fbfe6d58202137c1cbfcdd87788fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*wCjI8ji9SEDVB2hZmlkEmA.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">文档数据框架</p></figure><h2 id="1c78" class="mi kp it bd kq mr ms dn ku mt mu dp ky kb mv mw lc kf mx my lg kj mz na lk nb bi translated">预处理</h2><p id="2683" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">任何机器学习算法最重要的部分就是数据预处理。数据中存在的噪声越多，模型的准确性越低。</p><p id="dc2a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将对数据执行四种类型的处理:</p><ol class=""><li id="0e07" class="nn no it js b jt ju jx jy kb np kf nq kj nr kn ns nt nu nv bi translated">删除文本中的所有特殊字符。</li><li id="7d74" class="nn no it js b jt nw jx nx kb ny kf nz kj oa kn ns nt nu nv bi translated">删除所有少于3个字母的单词。</li><li id="89ad" class="nn no it js b jt nw jx nx kb ny kf nz kj oa kn ns nt nu nv bi translated">小写所有字符。</li><li id="fa6b" class="nn no it js b jt nw jx nx kb ny kf nz kj oa kn ns nt nu nv bi translated">删除停用词。</li></ol><pre class="ln lo lp lq gt md me mf mg aw mh bi"><span id="b3be" class="mi kp it me b gy mj mk l ml mm">#remove special characters<br/>df['clean_documents'] = df['documents'].str.replace("[^a-zA-Z#]", " ")</span><span id="058d" class="mi kp it me b gy ob mk l ml mm">#remove words have letters less than 3<br/>df['clean_documents'] = df['clean_documents'].fillna('').apply(lambda x: ' '.join([w for w in x.split() if len(w)&gt;2]))</span><span id="b8f5" class="mi kp it me b gy ob mk l ml mm">#lowercase all characters<br/>df['clean_documents'] = df['clean_documents'].fillna('').apply(lambda x: x.lower())</span></pre><p id="ad69" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了移除停用词，我们将对字符串进行标记，然后再次追加所有不是停用词的词。</p><pre class="ln lo lp lq gt md me mf mg aw mh bi"><span id="ba4f" class="mi kp it me b gy mj mk l ml mm">import nltk<br/>nltk.download('stopwords')<br/>from nltk.corpus import stopwords<br/>stop_words = stopwords.words('english')</span><span id="2494" class="mi kp it me b gy ob mk l ml mm"># tokenization<br/>tokenized_doc = df['clean_documents'].fillna('').apply(lambda x: x.split())</span><span id="2927" class="mi kp it me b gy ob mk l ml mm"># remove stop-words<br/>tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])</span><span id="1926" class="mi kp it me b gy ob mk l ml mm"># de-tokenization<br/>detokenized_doc = []<br/>for i in range(len(df)):<br/>    t = ' '.join(tokenized_doc[i])<br/>    detokenized_doc.append(t)</span><span id="f024" class="mi kp it me b gy ob mk l ml mm">df['clean_documents'] = detokenized_doc</span></pre><p id="be46" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">经过预处理后，我们的数据将如下所示:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/f3218713937f6e6c0b6d5c0e92d243e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*jBwyeEgCv3Y3qGh2i-Aw2A.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">清洗完文件后</p></figure><h2 id="7dcb" class="mi kp it bd kq mr ms dn ku mt mu dp ky kb mv mw lc kf mx my lg kj mz na lk nb bi translated">文档术语矩阵</h2><p id="1a34" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">我们将使用<code class="fe mo mp mq me b">sklearn</code>来生成文档术语矩阵。</p><pre class="ln lo lp lq gt md me mf mg aw mh bi"><span id="fbeb" class="mi kp it me b gy mj mk l ml mm">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="5692" class="mi kp it me b gy ob mk l ml mm">vectorizer = TfidfVectorizer(stop_words='english', smooth_idf=True)</span><span id="690c" class="mi kp it me b gy ob mk l ml mm">X = vectorizer.fit_transform(df['clean_documents'])</span></pre><p id="db36" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们用<code class="fe mo mp mq me b">TfidfVectorizer</code>代替<code class="fe mo mp mq me b">CountVectorizer</code>，因为tf-idf是更有效的矢量器。你可以在这里了解传递给<code class="fe mo mp mq me b">TfidfVectorizer</code> <a class="ae nl" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank">的各种参数，要了解tf-idf你可以</a><a class="ae nl" href="http://www.tfidf.com/" rel="noopener ugc nofollow" target="_blank">查看这个链接</a>。</p><p id="1c7c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mo mp mq me b">X</code>的形状将是<code class="fe mo mp mq me b">(5,6)</code>，其中行代表文档数5，列代表术语数6。</p><p id="aea6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要查看条款</p><pre class="ln lo lp lq gt md me mf mg aw mh bi"><span id="5a2e" class="mi kp it me b gy mj mk l ml mm">dictionary = vectorizer.get_feature_names()<br/>dictionary</span></pre><p id="8965" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这将给出一组单词</p><pre class="ln lo lp lq gt md me mf mg aw mh bi"><span id="02f3" class="mi kp it me b gy mj mk l ml mm">['active', 'brown', 'cat', 'dog', 'good', 'lazy']</span></pre><h2 id="7d58" class="mi kp it bd kq mr ms dn ku mt mu dp ky kb mv mw lc kf mx my lg kj mz na lk nb bi translated">奇异值分解</h2><pre class="ln lo lp lq gt md me mf mg aw mh bi"><span id="428d" class="mi kp it me b gy mj mk l ml mm">from sklearn.decomposition import TruncatedSVD</span><span id="f38d" class="mi kp it me b gy ob mk l ml mm"># SVD represent documents and terms in vectors <br/>svd_model = TruncatedSVD(n_components=2, algorithm='randomized', n_iter=100, random_state=122)</span><span id="e664" class="mi kp it me b gy ob mk l ml mm">lsa = svd_model.fit_transform(X)</span></pre><p id="c413" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mo mp mq me b">TruncatedSVD</code>对文档-术语矩阵执行SVD函数，并给出降维后的向量。如果你想要矩阵不降维，你应该使用<code class="fe mo mp mq me b">fit</code>而不是<code class="fe mo mp mq me b">fit_transform</code>。</p><p id="23b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mo mp mq me b">n_components</code>是输出数据的维度。<code class="fe mo mp mq me b">n_components</code>的值代表不同主题的数量。可以在这里了解更多<a class="ae nl" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html" rel="noopener ugc nofollow" target="_blank"> sklearn SVD。</a></p><p id="8ff5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们将检查分配给文档的主题</p><pre class="ln lo lp lq gt md me mf mg aw mh bi"><span id="6ace" class="mi kp it me b gy mj mk l ml mm">pd.options.display.float_format = '{:,.16f}'.format<br/>topic_encoded_df = pd.DataFrame(lsa, columns = ["topic_1", "topic_2"])<br/>topic_encoded_df["documents"] = df['clean_documents']<br/>display(topic_encoded_df[["documents", "topic_1", "topic_2"]])</span></pre><p id="0bd3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出如下所示</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/cb3ec1e3cc494f29095d0550b5563f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*7dr8QNm1Tt39L-C_53Vo6A.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">电平移动放大器(Level Shift Amplifier)</p></figure><p id="71a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到分配给每个文档的主题。关于狗的文档由topic_2表示，关于猫的文档由topic_1表示。最后一个既有猫又有狗的文档更多地由topic_1表示，但也属于topic_2。这与topic_1更相似，因为文档包含单词<code class="fe mo mp mq me b">brown</code>和<code class="fe mo mp mq me b">cat</code>，这两个单词在topic_1中的权重更高。</p><p id="2e93" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们还可以看到每个主题中术语的权重。</p><pre class="ln lo lp lq gt md me mf mg aw mh bi"><span id="969a" class="mi kp it me b gy mj mk l ml mm">encoding_matrix = pd.DataFrame(svd_model.components_, index = ["topic_1","topic_2"], columns = (dictionary)).T<br/>encoding_matrix</span></pre><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/e6a8cd12961feda3c625571bc1f0cdd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*IgufoQWrx0H-UkD3yqLzgA.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">术语-主题矩阵</p></figure><p id="944b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从上面我们可以看到，术语<code class="fe mo mp mq me b">brown</code>和<code class="fe mo mp mq me b">cat</code>在topic_1中的权重都高于topic_2。</p><p id="e07a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们已经看到了LSA的实施和运作。</p><h1 id="ac68" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">应用程序</h1><p id="3f6c" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">LSA是LSI和降维算法的先驱。</p><ol class=""><li id="802a" class="nn no it js b jt ju jx jy kb np kf nq kj nr kn ns nt nu nv bi translated">LSA用于降维。我们可以在不丢失任何上下文的情况下，将向量的大小从数百万急剧减少到数千。这将有助于我们减少计算能力和执行计算所需的时间。</li><li id="73dc" class="nn no it js b jt nw jx nx kb ny kf nz kj oa kn ns nt nu nv bi translated">LSA用于搜索引擎。潜在语义索引(LSI)是在LSA上开发的算法。使用从LSA开发的向量来找到匹配搜索查询的文档。</li><li id="9dad" class="nn no it js b jt nw jx nx kb ny kf nz kj oa kn ns nt nu nv bi translated">LSA也可以用于文档聚类。正如我们看到的，LSA为每个文档分配主题，基于分配的主题，我们可以对文档进行聚类。</li></ol><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="of og l"/></div></figure></div></div>    
</body>
</html>