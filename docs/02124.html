<html>
<head>
<title>PyTorch [Tabular] — Binary Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">py torch[表格]-二进制分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89?source=collection_archive---------1-----------------------#2020-02-29">https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89?source=collection_archive---------1-----------------------#2020-02-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/840b4e62b667c78ce2f768c8b11e4488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Dsdw-L4qVhT1WkyLvtsPg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">如何训练你的神经网络[图片[0]]</p></figure><h2 id="4d41" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/akshaj-wields-pytorch" rel="noopener">如何训练你的神经网络</a></h2><div class=""/><div class=""><h2 id="c2ac" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">这篇博文将带您了解使用PyTorch对表格数据进行二进制分类的实现。</h2></div><p id="93d8" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们将使用Kaggle上可用的下背部疼痛症状数据集<a class="ae lz" href="https://www.kaggle.com/sammy123/lower-back-pain-symptoms-dataset" rel="noopener ugc nofollow" target="_blank">。该数据集有13列，其中前12列是要素，最后一列是目标列。数据集有300行。</a></p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/7291afb44cc420fe23c3a54269cab8ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/0*ZMjHWzzdOB48jTSt"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">二元分类模因[图片[1]]</p></figure><h1 id="414b" class="mf mg jf bd mh mi mj mk ml mm mn mo mp ku mq kv mr kx ms ky mt la mu lb mv mw bi translated">导入库</h1><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="69fe" class="nc mg jf my b gy nd ne l nf ng">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/><br/>import torch<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>from torch.utils.data import Dataset, DataLoader<br/><br/>from sklearn.preprocessing import StandardScaler    <br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import confusion_matrix, classification_report</span></pre><h1 id="4c33" class="mf mg jf bd mh mi mj mk ml mm mn mo mp ku mq kv mr kx ms ky mt la mu lb mv mw bi translated">读出数据</h1><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="196b" class="nc mg jf my b gy nd ne l nf ng">df = pd.read_csv("data/tabular/classification/spine_dataset.csv")</span><span id="56cb" class="nc mg jf my b gy nh ne l nf ng">df.head()</span></pre><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/991392669bd8bdfb78c0a2dedd0c9ec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_oLQa4K4aRpuzWA5eXE88g.png"/></div></div></figure><h1 id="5ab0" class="mf mg jf bd mh mi mj mk ml mm mn mo mp ku mq kv mr kx ms ky mt la mu lb mv mw bi translated">EDA和预处理</h1><h2 id="58b7" class="nc mg jf bd mh nj nk dn ml nl nm dp mp lm nn no mr lq np nq mt lu nr ns mv jl bi translated">阶级分布</h2><p id="8dca" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">这里存在阶级不平衡。虽然有很多方法可以解决阶级不平衡问题，但这超出了本文的范围。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="fe14" class="nc mg jf my b gy nd ne l nf ng">sns.countplot(x = 'Class_att', data=df)</span></pre><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/615120436fc4d9e6527e2c42864169c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*kbq9c4GHpLaMtYowYkMj2g.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">等级不平衡条形图[图片[2]]</p></figure><h2 id="e64a" class="nc mg jf bd mh nj nk dn ml nl nm dp mp lm nn no mr lq np nq mt lu nr ns mv jl bi translated">编码输出类别</h2><p id="5ef2" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">PyTorch支持从0开始的标签。那就是<strong class="lf jp">【0，n】</strong>。我们需要从0开始重新映射我们的标签。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="58f6" class="nc mg jf my b gy nd ne l nf ng">df['Class_att'] = df['Class_att'].astype('category')</span><span id="f062" class="nc mg jf my b gy nh ne l nf ng">encode_map = {<br/>    'Abnormal': 1,<br/>    'Normal': 0<br/>}<br/><br/>df['Class_att'].replace(encode_map, inplace=True)</span></pre><h2 id="7166" class="nc mg jf bd mh nj nk dn ml nl nm dp mp lm nn no mr lq np nq mt lu nr ns mv jl bi translated">创建输入和输出数据</h2><p id="709c" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">最后一列是我们的输出。输入是除最后一列之外的所有列。这里我们使用Pandas库中的<code class="fe nz oa ob my b">.iloc</code>方法来选择我们的输入和输出列。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="1637" class="nc mg jf my b gy nd ne l nf ng">X = df.iloc[:, 0:-1]<br/>y = df.iloc[:, -1]</span></pre><h2 id="8d58" class="nc mg jf bd mh nj nk dn ml nl nm dp mp lm nn no mr lq np nq mt lu nr ns mv jl bi translated">列车测试分离</h2><p id="578a" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">我们现在将数据分为训练集和测试集。我们选择了33%的数据作为测试集。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="35d5" class="nc mg jf my b gy nd ne l nf ng">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)</span></pre><h2 id="58b2" class="nc mg jf bd mh nj nk dn ml nl nm dp mp lm nn no mr lq np nq mt lu nr ns mv jl bi translated">标准化输入</h2><p id="9bc4" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">为了正确训练神经网络，我们需要标准化输入值。我们通过去除平均值并缩放到单位方差来标准化特征。平均值为<code class="fe nz oa ob my b">u</code>且标准差为<code class="fe nz oa ob my b">s</code>的样本<code class="fe nz oa ob my b">x</code>的标准分数计算如下:</p><blockquote class="oc"><p id="587a" class="od oe jf bd of og oh oi oj ok ol ly dk translated"><em class="om"> z = (x — u) / s </em></p></blockquote><p id="5c6d" class="pw-post-body-paragraph ld le jf lf b lg on kp li lj oo ks ll lm op lo lp lq oq ls lt lu or lw lx ly ij bi translated">你可以在这里找到关于神经网络<a class="ae lz" href="https://www.jeremyjordan.me/batch-normalization/" rel="noopener ugc nofollow" target="_blank">标准化/规范化的更多信息。</a></p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="ebf0" class="nc mg jf my b gy nd ne l nf ng">scaler = StandardScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.transform(X_test)</span></pre><h1 id="486c" class="mf mg jf bd mh mi mj mk ml mm mn mo mp ku mq kv mr kx ms ky mt la mu lb mv mw bi translated">模型参数</h1><p id="0f89" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">为了训练我们的模型，我们需要设置一些超参数。请注意，这是一个非常简单的神经网络，因此，我们没有调整很多超参数。目标是了解PyTorch是如何工作的。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="23bc" class="nc mg jf my b gy nd ne l nf ng">EPOCHS = 50<br/>BATCH_SIZE = 64<br/>LEARNING_RATE = 0.001</span></pre><h1 id="9f4b" class="mf mg jf bd mh mi mj mk ml mm mn mo mp ku mq kv mr kx ms ky mt la mu lb mv mw bi translated">定义自定义数据加载器</h1><p id="a5f8" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">这里我们定义了一个数据加载器。如果这对您来说是新的，我建议您阅读下面关于数据加载器的博客文章，然后再回来。</p><div class="ip iq gp gr ir os"><a rel="noopener follow" target="_blank" href="/pytorch-basics-intro-to-dataloaders-and-loss-functions-868e86450047"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jp gy z fp ox fr fs oy fu fw jo bi translated">py torch[基础知识] —数据加载器和损失函数简介</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">这篇博文将带您了解PyTorch中的数据加载器和不同类型的损失函数。</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg ix os"/></div></div></a></div><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="5338" class="nc mg jf my b gy nd ne l nf ng">## train data</span><span id="ecf1" class="nc mg jf my b gy nh ne l nf ng">class TrainData(Dataset):<br/>    <br/>    def __init__(self, X_data, y_data):<br/>        self.X_data = X_data<br/>        self.y_data = y_data<br/>        <br/>    def __getitem__(self, index):<br/>        return self.X_data[index], self.y_data[index]<br/>        <br/>    def __len__ (self):<br/>        return len(self.X_data)<br/><br/><br/>train_data = TrainData(torch.FloatTensor(X_train), <br/>                       torch.FloatTensor(y_train))<br/></span><span id="092e" class="nc mg jf my b gy nh ne l nf ng">## test data    </span><span id="1405" class="nc mg jf my b gy nh ne l nf ng">class TestData(Dataset):<br/>    <br/>    def __init__(self, X_data):<br/>        self.X_data = X_data<br/>        <br/>    def __getitem__(self, index):<br/>        return self.X_data[index]<br/>        <br/>    def __len__ (self):<br/>        return len(self.X_data)<br/>    <br/><br/>test_data = TestData(torch.FloatTensor(X_test))</span></pre><p id="d1bd" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们初始化我们的数据加载器。我们将使用一个<code class="fe nz oa ob my b">batch_size = 1</code>作为测试数据加载器。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="b1b5" class="nc mg jf my b gy nd ne l nf ng">train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)</span><span id="1c34" class="nc mg jf my b gy nh ne l nf ng">test_loader = DataLoader(dataset=test_data, batch_size=1)</span></pre><h1 id="4aaf" class="mf mg jf bd mh mi mj mk ml mm mn mo mp ku mq kv mr kx ms ky mt la mu lb mv mw bi translated">定义神经网络架构</h1><p id="2018" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">这里，我们定义了一个具有批处理和丢失的2层前馈网络。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ph"><img src="../Images/95593aa58f570105c7726feb161577e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CLjAAd7s6o0yfEYZ.jpg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">使用前馈网络的二元分类示例[图像[3] <a class="ae lz" href="https://www.learnopencv.com/understanding-feedforward-neural-networks/" rel="noopener ugc nofollow" target="_blank">信用点</a></p></figure><p id="0ff3" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在我们的<code class="fe nz oa ob my b">__init__()</code>函数中，我们定义我们想要使用的层，而在<code class="fe nz oa ob my b">forward()</code>函数中，我们调用已定义的层。</p><p id="d23e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">由于数据集中输入要素的数量为12，因此第一个<code class="fe nz oa ob my b">nn.Linear</code>图层的输入将为12。输出可以是你想要的任何数字。</p><blockquote class="oc"><p id="3ddb" class="od oe jf bd of og oh oi oj ok ol ly dk translated"><strong class="ak">您唯一需要确保的是一个图层的输出要素数量应等于下一个图层的输入要素数量。</strong></p></blockquote><p id="c920" class="pw-post-body-paragraph ld le jf lf b lg on kp li lj oo ks ll lm op lo lp lq oq ls lt lu or lw lx ly ij bi translated">在<a class="ae lz" href="https://pytorch.org/docs/stable/nn.html#linear" rel="noopener ugc nofollow" target="_blank">文档</a>中阅读更多关于<code class="fe nz oa ob my b">nn.Linear</code>的信息。类似地，我们定义ReLU、Dropout和BatchNorm层。</p><p id="f7df" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">一旦我们定义了所有这些层，就该使用它们了。在<code class="fe nz oa ob my b">forward()</code>函数中，我们将变量<code class="fe nz oa ob my b">inputs</code>作为我们的输入。我们通过我们初始化的不同层传递这个输入。</p><p id="7d0b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe nz oa ob my b">forward()</code>函数的第一行接受输入，通过我们的第一个线性层，然后在其上应用ReLU激活。然后我们对输出应用BatchNorm。看看下面的代码可以更好地理解它。</p><blockquote class="pi pj pk"><p id="a8fe" class="ld le pl lf b lg lh kp li lj lk ks ll pm ln lo lp pn lr ls lt po lv lw lx ly ij bi translated">注意，我们在训练中没有在最后一层<strong class="lf jp"> </strong>中使用<strong class="lf jp">s形</strong>激活。这是因为，我们使用了自动应用Sigmoid激活的<code class="fe nz oa ob my b">nn.BCEWithLogitsLoss()</code>损失函数。</p></blockquote><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="0a61" class="nc mg jf my b gy nd ne l nf ng">class BinaryClassification(nn.Module):<br/>    def __init__(self):<br/>        super(BinaryClassification, self).__init__()</span><span id="d8eb" class="nc mg jf my b gy nh ne l nf ng">        # Number of input features is 12.<br/>        self.layer_1 = nn.Linear(12, 64) <br/>        self.layer_2 = nn.Linear(64, 64)<br/>        self.layer_out = nn.Linear(64, 1) <br/>        <br/>        self.relu = nn.ReLU()<br/>        self.dropout = nn.Dropout(p=0.1)<br/>        self.batchnorm1 = nn.BatchNorm1d(64)<br/>        self.batchnorm2 = nn.BatchNorm1d(64)<br/>        <br/>    def forward(self, inputs):<br/>        x = self.relu(self.layer_1(inputs))<br/>        x = self.batchnorm1(x)<br/>        x = self.relu(self.layer_2(x))<br/>        x = self.batchnorm2(x)<br/>        x = self.dropout(x)<br/>        x = self.layer_out(x)<br/>        <br/>        return x</span></pre><p id="5dfc" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">一旦我们定义了我们的架构，我们检查我们的GPU是否是活动的。PyTorch的神奇之处在于它使用GPU超级简单。</p><p id="c42e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">变量<code class="fe nz oa ob my b">device</code>要么会说<code class="fe nz oa ob my b">cuda:0</code>如果我们有GPU的话。如果没有，它会说<code class="fe nz oa ob my b">cpu</code>。即使您没有GPU，也可以在不修改代码的情况下遵循本教程。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="c8fc" class="nc mg jf my b gy nd ne l nf ng">device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")</span><span id="df5e" class="nc mg jf my b gy nh ne l nf ng">print(device)<br/></span><span id="6193" class="nc mg jf my b gy nh ne l nf ng">###################### OUTPUT ######################</span><span id="9cc5" class="nc mg jf my b gy nh ne l nf ng">cuda:0</span></pre><p id="dc6d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">接下来，我们需要初始化我们的模型。初始化后，我们将其移动到<code class="fe nz oa ob my b">device</code>。现在，这个设备是一个GPU，如果你有，或者它是CPU，如果你没有。我们使用的网络相当小。所以，在CPU上训练不会花很多时间。</p><p id="54c5" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在这之后，我们初始化优化器并决定使用哪个损失函数。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="7b51" class="nc mg jf my b gy nd ne l nf ng">model = BinaryClassification()<br/>model.to(device)</span><span id="a7e1" class="nc mg jf my b gy nh ne l nf ng">print(model)</span><span id="38f6" class="nc mg jf my b gy nh ne l nf ng">criterion = nn.BCEWithLogitsLoss()<br/>optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)<br/></span><span id="296e" class="nc mg jf my b gy nh ne l nf ng">###################### OUTPUT ######################</span><span id="d4a3" class="nc mg jf my b gy nh ne l nf ng">BinaryClassification(<br/>  (layer_1): Linear(in_features=12, out_features=64, bias=True)<br/>  (layer_2): Linear(in_features=64, out_features=64, bias=True)<br/>  (layer_out): Linear(in_features=64, out_features=1, bias=True)<br/>  (relu): ReLU()<br/>  (dropout): Dropout(p=0.1, inplace=False)<br/>  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>)</span></pre><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/34b884c4fa13f1434f66f5d87d5b3fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/0*mTIsR9oC3cOijxU8.jpg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">二元分类模因[图片[4]]</p></figure><h1 id="b5c2" class="mf mg jf bd mh mi mj mk ml mm mn mo mp ku mq kv mr kx ms ky mt la mu lb mv mw bi translated">训练模型</h1><p id="b3bf" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">在开始实际训练之前，让我们定义一个函数来计算精度。</p><p id="ba90" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在下面的函数中，我们将预测和实际输出作为输入。预测值(概率)被四舍五入，以将其转换为<strong class="lf jp"> 0 </strong>或<strong class="lf jp"> 1 </strong>。</p><p id="3571" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">一旦完成，我们只需将我们预测的1/0的数量与实际存在的1/0的数量进行比较，并计算准确度。</p><p id="c77c" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">请注意，输入<code class="fe nz oa ob my b">y_pred</code>和<code class="fe nz oa ob my b">y_test</code>是针对一个批次的。我们的<code class="fe nz oa ob my b">batch_size</code>是64。所以，这个精度是一次为64个预测(张量)计算的。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="f4e3" class="nc mg jf my b gy nd ne l nf ng">def binary_acc(y_pred, y_test):<br/>    y_pred_tag = torch.round(torch.sigmoid(y_pred))<br/><br/>    correct_results_sum = (y_pred_tag == y_test).sum().float()<br/>    acc = correct_results_sum/y_test.shape[0]<br/>    acc = torch.round(acc * 100)<br/>    <br/>    return acc</span></pre><p id="e9c9" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们期待已久的时刻到来了。让我们训练我们的模型。</p><p id="e096" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">你可以看到我们在循环之前放了一个<code class="fe nz oa ob my b">model.train()</code>。<code class="fe nz oa ob my b">model.train()</code>告诉PyTorch你正处于训练模式。</p><p id="698f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为什么我们需要这么做？如果您使用的层(如Dropout或BatchNorm)在训练和评估期间表现不同，您需要告诉PyTorch相应地采取行动。而PyTorch中的默认模式是<em class="pl">火车</em>，所以，您不必显式地编写它。但这是很好的练习。</p><p id="899a" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">同样，当我们测试我们的模型时，我们将调用<code class="fe nz oa ob my b">model.eval()</code>。我们将在下面看到。</p><p id="8a6e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">回到训练；我们开始一个循环。在这个for循环的顶部，我们将每个历元的损失和精度初始化为0。在每个时期之后，我们将打印出损失/精度并将其重置回0。</p><p id="8022" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然后我们有另一个for循环。这个for循环用于从<code class="fe nz oa ob my b">train_loader</code>中批量获取我们的数据。</p><p id="5163" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在我们做任何预测之前，我们先做<code class="fe nz oa ob my b">optimizer.zero_grad()</code>。由于<code class="fe nz oa ob my b">backward()</code>函数累加梯度，我们需要为每个小批量手动将其设置为0。</p><p id="56d9" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">从我们定义的模型中，我们获得一个预测，得到小批量的损失(和精度)，使用<code class="fe nz oa ob my b">loss.backward()</code>和<code class="fe nz oa ob my b">optimizer.step()</code>进行反向传播。最后，我们将所有小批量损失(和精度)相加，以获得该时期的平均损失(和精度)。</p><p id="efc5" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">该损耗和精度在外部<code class="fe nz oa ob my b">for</code>循环中打印出来。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="08d0" class="nc mg jf my b gy nd ne l nf ng">model.train()<br/>for e in range(1, EPOCHS+1):<br/>    epoch_loss = 0<br/>    epoch_acc = 0<br/>    for X_batch, y_batch in train_loader:<br/>        X_batch, y_batch = X_batch.to(device), y_batch.to(device)<br/>        optimizer.zero_grad()<br/>        <br/>        y_pred = model(X_batch)<br/>        <br/>        loss = criterion(y_pred, y_batch.unsqueeze(1))<br/>        acc = binary_acc(y_pred, y_batch.unsqueeze(1))<br/>        <br/>        loss.backward()<br/>        optimizer.step()<br/>        <br/>        epoch_loss += loss.item()<br/>        epoch_acc += acc.item()<br/>        <br/><br/>    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')<br/></span><span id="2184" class="nc mg jf my b gy nh ne l nf ng">###################### OUTPUT ######################</span><span id="9f93" class="nc mg jf my b gy nh ne l nf ng">Epoch 001: | Loss: 0.04027 | Acc: 98.250<br/>Epoch 002: | Loss: 0.12023 | Acc: 96.750<br/>Epoch 003: | Loss: 0.02067 | Acc: 99.500<br/>Epoch 004: | Loss: 0.07329 | Acc: 96.250<br/>Epoch 005: | Loss: 0.04676 | Acc: 99.250<br/>Epoch 006: | Loss: 0.03005 | Acc: 99.500<br/>Epoch 007: | Loss: 0.05777 | Acc: 98.250<br/>Epoch 008: | Loss: 0.03446 | Acc: 99.500<br/>Epoch 009: | Loss: 0.03443 | Acc: 100.000<br/>Epoch 010: | Loss: 0.03368 | Acc: 100.000<br/>Epoch 011: | Loss: 0.02395 | Acc: 100.000<br/>Epoch 012: | Loss: 0.05094 | Acc: 98.250<br/>Epoch 013: | Loss: 0.03618 | Acc: 98.250<br/>Epoch 014: | Loss: 0.02143 | Acc: 100.000<br/>Epoch 015: | Loss: 0.02730 | Acc: 99.500<br/>Epoch 016: | Loss: 0.02323 | Acc: 100.000<br/>Epoch 017: | Loss: 0.03395 | Acc: 98.250<br/>Epoch 018: | Loss: 0.08600 | Acc: 96.750<br/>Epoch 019: | Loss: 0.02394 | Acc: 100.000<br/>Epoch 020: | Loss: 0.02363 | Acc: 100.000<br/>Epoch 021: | Loss: 0.01660 | Acc: 100.000<br/>Epoch 022: | Loss: 0.05766 | Acc: 96.750<br/>Epoch 023: | Loss: 0.02115 | Acc: 100.000<br/>Epoch 024: | Loss: 0.01331 | Acc: 100.000<br/>Epoch 025: | Loss: 0.01504 | Acc: 100.000<br/>Epoch 026: | Loss: 0.01727 | Acc: 100.000<br/>Epoch 027: | Loss: 0.02128 | Acc: 100.000<br/>Epoch 028: | Loss: 0.01106 | Acc: 100.000<br/>Epoch 029: | Loss: 0.05802 | Acc: 98.250<br/>Epoch 030: | Loss: 0.01275 | Acc: 100.000<br/>Epoch 031: | Loss: 0.01272 | Acc: 100.000<br/>Epoch 032: | Loss: 0.01949 | Acc: 100.000<br/>Epoch 033: | Loss: 0.02848 | Acc: 100.000<br/>Epoch 034: | Loss: 0.01514 | Acc: 100.000<br/>Epoch 035: | Loss: 0.02949 | Acc: 100.000<br/>Epoch 036: | Loss: 0.00895 | Acc: 100.000<br/>Epoch 037: | Loss: 0.01692 | Acc: 100.000<br/>Epoch 038: | Loss: 0.01678 | Acc: 100.000<br/>Epoch 039: | Loss: 0.02755 | Acc: 100.000<br/>Epoch 040: | Loss: 0.02021 | Acc: 100.000<br/>Epoch 041: | Loss: 0.07972 | Acc: 98.250<br/>Epoch 042: | Loss: 0.01421 | Acc: 100.000<br/>Epoch 043: | Loss: 0.01558 | Acc: 100.000<br/>Epoch 044: | Loss: 0.01185 | Acc: 100.000<br/>Epoch 045: | Loss: 0.01830 | Acc: 100.000<br/>Epoch 046: | Loss: 0.01367 | Acc: 100.000<br/>Epoch 047: | Loss: 0.00880 | Acc: 100.000<br/>Epoch 048: | Loss: 0.01046 | Acc: 100.000<br/>Epoch 049: | Loss: 0.00933 | Acc: 100.000<br/>Epoch 050: | Loss: 0.11034 | Acc: 98.250</span></pre><h1 id="dcdd" class="mf mg jf bd mh mi mj mk ml mm mn mo mp ku mq kv mr kx ms ky mt la mu lb mv mw bi translated">测试模型</h1><p id="05f5" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">训练完成后，我们需要测试我们的模型进展如何。注意，在运行测试代码之前，我们已经使用了<code class="fe nz oa ob my b">model.eval()</code>。为了告诉PyTorch我们不希望在推断过程中执行反向传播，我们使用了<code class="fe nz oa ob my b">torch.no_grad()</code>来减少内存使用并加快计算速度。</p><p id="c1eb" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们首先定义一个包含我们预测的列表。然后我们使用<code class="fe nz oa ob my b">test_loader</code>循环遍历我们的批处理。对于每一批—</p><ul class=""><li id="9d7f" class="pq pr jf lf b lg lh lj lk lm ps lq pt lu pu ly pv pw px py bi translated">我们使用训练好的模型进行预测。</li><li id="e6ed" class="pq pr jf lf b lg pz lj qa lm qb lq qc lu qd ly pv pw px py bi translated">将概率四舍五入为1或0。</li><li id="4846" class="pq pr jf lf b lg pz lj qa lm qb lq qc lu qd ly pv pw px py bi translated">将批处理从CPU移动到GPU。</li><li id="b190" class="pq pr jf lf b lg pz lj qa lm qb lq qc lu qd ly pv pw px py bi translated">将张量转换为numpy对象，并将其添加到我们的列表中。</li><li id="383b" class="pq pr jf lf b lg pz lj qa lm qb lq qc lu qd ly pv pw px py bi translated">将列表展平，这样我们可以将它用作<code class="fe nz oa ob my b">confusion_matrix</code>和<code class="fe nz oa ob my b">classification_report</code>的输入。</li></ul><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="d977" class="nc mg jf my b gy nd ne l nf ng">y_pred_list = []</span><span id="a15a" class="nc mg jf my b gy nh ne l nf ng">model.eval()<br/>with torch.no_grad():<br/>    for X_batch in test_loader:<br/>        X_batch = X_batch.to(device)<br/>        y_test_pred = model(X_batch)<br/>        y_test_pred = torch.sigmoid(y_test_pred)<br/>        y_pred_tag = torch.round(y_test_pred)<br/>        y_pred_list.append(y_pred_tag.cpu().numpy())</span><span id="d28e" class="nc mg jf my b gy nh ne l nf ng"><br/>y_pred_list = [a.squeeze().tolist() for a in y_pred_list]</span></pre><h1 id="01f8" class="mf mg jf bd mh mi mj mk ml mm mn mo mp ku mq kv mr kx ms ky mt la mu lb mv mw bi translated">混淆矩阵</h1><p id="6a6e" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">一旦我们有了所有的预测，我们使用scikit-learn的<code class="fe nz oa ob my b">confusion_matrix()</code>函数来计算混淆矩阵。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="b87d" class="nc mg jf my b gy nd ne l nf ng">confusion_matrix(y_test, y_pred_list)</span><span id="d55c" class="nc mg jf my b gy nh ne l nf ng"><br/>###################### OUTPUT ######################</span><span id="cc02" class="nc mg jf my b gy nh ne l nf ng">array([[23,  8],<br/>       [12, 60]])</span></pre><h1 id="de24" class="mf mg jf bd mh mi mj mk ml mm mn mo mp ku mq kv mr kx ms ky mt la mu lb mv mw bi translated">分类报告</h1><p id="8150" class="pw-post-body-paragraph ld le jf lf b lg nt kp li lj nu ks ll lm nv lo lp lq nw ls lt lu nx lw lx ly ij bi translated">为了获得具有精确度、召回率和F1分数的分类报告，我们使用函数<code class="fe nz oa ob my b">classification_report</code>。</p><pre class="mb mc md me gt mx my mz na aw nb bi"><span id="4b74" class="nc mg jf my b gy nd ne l nf ng">print(classification_report(y_test, y_pred_list))</span><span id="b8fc" class="nc mg jf my b gy nh ne l nf ng"><br/>###################### OUTPUT ######################</span><span id="7435" class="nc mg jf my b gy nh ne l nf ng">precision    recall  f1-score   support</span><span id="7877" class="nc mg jf my b gy nh ne l nf ng">           0       0.66      0.74      0.70        31<br/>           1       0.88      0.83      0.86        72</span><span id="db3e" class="nc mg jf my b gy nh ne l nf ng">    accuracy                           0.81       103<br/>   macro avg       0.77      0.79      0.78       103<br/>weighted avg       0.81      0.81      0.81       103</span></pre></div><div class="ab cl qe qf hu qg" role="separator"><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj"/></div><div class="ij ik il im in"><p id="85f6" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">感谢您的阅读。欢迎提出建议和建设性的批评。:)</p><p id="6f85" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这篇博客是“如何训练你的神经网络”系列的一部分。你可以在这里找到<a class="ae lz" href="https://towardsdatascience.com/tagged/akshaj-wields-pytorch" rel="noopener" target="_blank">系列。</a></p><p id="2cbc" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">你可以在<a class="ae lz" href="https://www.linkedin.com/in/akshajverma7/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lz" href="https://twitter.com/theairbend3r" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上找到我。如果你喜欢这个，看看我的其他<a class="ae lz" href="https://medium.com/@theairbend3r" rel="noopener">博客</a>。</p><figure class="mb mc md me gt is gh gi paragraph-image"><a href="https://www.buymeacoffee.com/theairbend3r"><div class="gh gi ql"><img src="../Images/041a0c7464198414e6ce355f9235099e.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*SGCT6C60o4t58wRqeU2viQ.png"/></div></a></figure></div></div>    
</body>
</html>