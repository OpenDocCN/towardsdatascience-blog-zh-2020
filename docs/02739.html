<html>
<head>
<title>Getting Started with Text Preprocessing and Topic Modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本预处理和主题建模入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-hottest-topics-in-machine-learning-866ae21ba22d?source=collection_archive---------39-----------------------#2020-03-16">https://towardsdatascience.com/the-hottest-topics-in-machine-learning-866ae21ba22d?source=collection_archive---------39-----------------------#2020-03-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e497" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解自然语言处理(NLP)——Python中的文本预处理、主题建模和潜在狄利克雷分配(LDA)</h2></div><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="06e8" class="kr ks it kn b gy kt ku l kv kw">Feel free to follow me on Medium!</span></pre><p id="97dc" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">最近在DataCamp上完成了“<a class="ae lt" href="https://learn.datacamp.com/projects/158" rel="noopener ugc nofollow" target="_blank">机器学习最热门话题</a>”项目。该项目在NIPS论文上使用自然语言处理(NLP ),并实现主题建模和潜在狄利克雷分配(LDA ),以开发机器学习研究中最热门的主题。</p><figure class="ki kj kk kl gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lu"><img src="../Images/e3e0fe9443751e77b135bdc016638da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XGoy_cJDb3B1q0d5yyR9Xw.png"/></div></div></figure><p id="08b8" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">"<a class="ae lt" href="https://en.wikipedia.org/wiki/Topic_model" rel="noopener ugc nofollow" target="_blank"> <strong class="kz iu">主题建模</strong> </a>是一种统计模型，用于发现文档集合中出现的抽象主题。""<a class="ae lt" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank"> <strong class="kz iu">潜在狄利克雷分配</strong> ( <strong class="kz iu"> LDA </strong> ) </a>是主题建模的一个示例，是一个生成统计模型，它允许观察集由未观察到的组来解释，这解释了为什么数据的某些部分是相似的。"</p><blockquote class="mc md me"><p id="676d" class="kx ky mf kz b la lb ju lc ld le jx lf mg lh li lj mh ll lm ln mi lp lq lr ls im bi translated">"文档被认为是主题的概率分布，而主题被认为是单词的概率分布."</p></blockquote><h1 id="288a" class="mj ks it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">数据</h1><p id="e1d5" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">NIPS会议(神经信息处理系统)是机器学习社区最负盛名的年度活动之一。在每次NIPS会议上，都会发表大量的研究论文。该数据文件包含从1987年到2017年(30年)发表的不同NIPS论文的信息。这些论文讨论了机器学习中的各种主题，从神经网络到优化方法等等。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="eb51" class="kr ks it kn b gy kt ku l kv kw">import pandas as pd<br/>papers = pd.read_csv("datasets/papers.csv")<br/>papers.head()</span></pre><h1 id="194b" class="mj ks it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">EDA来理解数据</h1><p id="f6b2" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">对于论文的分析，我们只考虑与论文相关的文本数据和论文发表的年份，因此有必要删除所有不包含有用文本信息的列。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="ae12" class="kr ks it kn b gy kt ku l kv kw">papers.drop(columns=['id', 'event_type', 'pdf_name'], inplace=True)</span></pre><p id="3c3a" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了理解机器学习领域是如何随着时间的推移而发展的，我们将从可视化1987年以来每年的出版物数量开始。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="5b79" class="kr ks it kn b gy kt ku l kv kw">groups = papers.groupby('year')</span><span id="a6b0" class="kr ks it kn b gy nf ku l kv kw">counts = groups.size()</span><span id="98f9" class="kr ks it kn b gy nf ku l kv kw">import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>counts.plot(kind='bar')<br/>plt.xlabel('Year')<br/>plt.ylabel('Number of Papers per Group')<br/>plt.title('Machine Learning Publications since 1987')</span></pre><figure class="ki kj kk kl gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ng"><img src="../Images/c3c720e5b88867f466f5acea06f431a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DF11vz-QwvcKyq9OLHe_8Q.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">自1987年以来的机器学习出版物</p></figure><h1 id="2aa7" class="mj ks it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">数据预处理</h1><p id="c464" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">在我们开始之前，这里是文本预处理步骤的列表:</p><ul class=""><li id="1f2a" class="nl nm it kz b la lb ld le lg nn lk no lo np ls nq nr ns nt bi translated"><strong class="kz iu">正则表达式/规范化</strong> —小写单词，删除标点符号和数字</li><li id="1be5" class="nl nm it kz b la nu ld nv lg nw lk nx lo ny ls nq nr ns nt bi translated"><strong class="kz iu">记号化</strong> —将文本分割成称为记号的小块的过程</li><li id="15d5" class="nl nm it kz b la nu ld nv lg nw lk nx lo ny ls nq nr ns nt bi translated"><strong class="kz iu">停用词</strong>移除——任何语言中的一组常用词</li><li id="20bf" class="nl nm it kz b la nu ld nv lg nw lk nx lo ny ls nq nr ns nt bi translated"><strong class="kz iu">词汇化</strong> —将一个单词的词形变化组合在一起的过程，这样它们就可以作为一个单独的项目进行分析</li><li id="6c77" class="nl nm it kz b la nu ld nv lg nw lk nx lo ny ls nq nr ns nt bi translated"><strong class="kz iu">词干分析</strong> —将一个单词的词尾变化形式组合在一起的过程，这样它们就可以作为一个单独的项目进行分析</li></ul><p id="83b5" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">首先，我们将对标题进行一些简单的预处理，以便于分析。我们将使用正则表达式删除标题中的任何标点符号。然后我们将小写的单词。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="0a78" class="kr ks it kn b gy kt ku l kv kw">import re</span><span id="f2fd" class="kr ks it kn b gy nf ku l kv kw"># Remove punctuation<br/>papers['title_processed'] = papers['title'].map(lambda x: re.sub('[,\.!?]', '', x))</span><span id="18ed" class="kr ks it kn b gy nf ku l kv kw"># Lowercase the words<br/>papers['title_processed'] = papers['title_processed'].map(lambda x: x.lower())</span></pre><p id="672e" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了检查预处理是否正确，我们可以制作研究论文标题的单词云。Python有大量的开放库，但现在我们将使用Andreas Mueller的“<a class="ae lt" href="http://amueller.github.io/word_cloud/" rel="noopener ugc nofollow" target="_blank"> wordcloud库</a>”。这将为我们提供最常见单词的可视化表示。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="c403" class="kr ks it kn b gy kt ku l kv kw">import wordcloud</span><span id="48d0" class="kr ks it kn b gy nf ku l kv kw">long_string = " ".join(papers.title_processed)</span><span id="962b" class="kr ks it kn b gy nf ku l kv kw">wordcloud = wordcloud.WordCloud()<br/>wordcloud.generate(long_string)<br/>wordcloud.to_image()</span></pre><figure class="ki kj kk kl gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lu"><img src="../Images/e3e0fe9443751e77b135bdc016638da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XGoy_cJDb3B1q0d5yyR9Xw.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">词云</p></figure><h1 id="99c2" class="mj ks it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">LDA分析</h1><p id="3e89" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">我们将使用的主要文本分析方法是潜在狄利克雷分配(LDA)。LDA能够对大型文档集执行主题检测，确定大型未标记文本集中的主要主题。</p><p id="61b0" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">首先，需要将文档转换成简单的向量表示。文档向量的每个条目将对应于一个单词在文档中出现的次数。综上所述，我们将把一个标题列表转换成一个长度等于词汇量的向量列表。我们将根据文档向量列表绘制10个最常见的单词。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="78b2" class="kr ks it kn b gy kt ku l kv kw">from sklearn.feature_extraction.text import CountVectorizer<br/>import numpy as np</span><span id="7cfa" class="kr ks it kn b gy nf ku l kv kw">def plot_10_most_common_words(count_data, count_vectorizer):<br/>    import matplotlib.pyplot as plt<br/>    words = count_vectorizer.get_feature_names()<br/>    total_counts = np.zeros(len(words))<br/>    for t in count_data:<br/>        total_counts+=t.toarray()[0]<br/>    <br/>    count_dict = (zip(words, total_counts))<br/>    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]<br/>    words = [w[0] for w in count_dict]<br/>    counts = [w[1] for w in count_dict]<br/>    x_pos = np.arange(len(words))</span><span id="551c" class="kr ks it kn b gy nf ku l kv kw">plt.bar(x_pos, counts,align='center')<br/>    plt.xticks(x_pos, words, rotation=90) <br/>    plt.xlabel('words')<br/>    plt.ylabel('counts')<br/>    plt.title('10 most common words')<br/>    plt.show()</span><span id="00a6" class="kr ks it kn b gy nf ku l kv kw">count_vectorizer = CountVectorizer(stop_words='english')<br/>count_data = count_vectorizer.fit_transform(papers['title_processed'])<br/>plot_10_most_common_words(count_data, count_vectorizer)</span></pre><figure class="ki kj kk kl gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi nz"><img src="../Images/173c4d6fc71b779d4628ffd28a711c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gBgUM3xnGAYAcz1Yru7DFw.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">10个最常见的单词</p></figure><p id="8428" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">最后，将使用LDA分析研究标题。我们将改变的唯一参数是LDA算法中的主题数量，以便我们决定获得前10个主题。根据我们得到的结果，我们可以区分每个主题是关于什么的(“神经网络”、“学习”、“核方法”、“高斯过程”)。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="9e3b" class="kr ks it kn b gy kt ku l kv kw">import warnings<br/>warnings.simplefilter("ignore", DeprecationWarning)</span><span id="1a20" class="kr ks it kn b gy nf ku l kv kw">from sklearn.decomposition import LatentDirichletAllocation as LDA<br/><br/>def print_topics(model, count_vectorizer, n_top_words):<br/>    words = count_vectorizer.get_feature_names()<br/>    for topic_idx, topic in enumerate(model.components_):<br/>        print("\nTopic #%d:" % topic_idx)<br/>        print(" ".join([words[i]<br/>             for i in topic.argsort()[:-n_top_words - 1:-1]]))<br/>        <br/>number_topics = 10<br/>number_words = 6</span><span id="4487" class="kr ks it kn b gy nf ku l kv kw">lda = LDA(n_components=number_topics)<br/>lda.fit(count_data)</span><span id="4058" class="kr ks it kn b gy nf ku l kv kw">print("Topics found via LDA:")<br/>print_topics(lda, count_vectorizer, number_words)</span></pre><figure class="ki kj kk kl gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi oa"><img src="../Images/6c6385a1ad9c562d3fde6769f1fdc670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YsnWVNRKvHOmEVswPoz1Lg.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">通过LDA找到的10大主题</p></figure><h1 id="461d" class="mj ks it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">结论</h1><p id="555d" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">最后，根据我们之前创建的图表，从1987年到2017年，NIPS会议论文的数量呈指数级增长。我们还可以得出结论，机器学习在过去几年变得越来越流行。所以历史数据预测2018年会有更多的论文发表！</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="a6b6" class="kr ks it kn b gy kt ku l kv kw"># The historical data indicates that:<br/>more_papers_published_in_2018 = <strong class="kn iu">True</strong></span></pre><p id="541f" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">创建这篇文章的源代码可以在我的<a class="ae lt" href="https://github.com/shirley0823/The-Hottest-Topics-in-Machine-Learning" rel="noopener ugc nofollow" target="_blank"> Github </a>中找到。</p></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="f854" class="mj ks it bd mk ml oi mn mo mp oj mr ms jz ok ka mu kc ol kd mw kf om kg my mz bi translated">关于我</h1><p id="37d2" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">非常感谢您阅读我的文章！大家好，我是雪莉，目前在亚利桑那州立大学攻读商业分析硕士学位。如果您有任何问题，请随时联系我！</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="455a" class="kr ks it kn b gy kt ku l kv kw">Email me at <strong class="kn iu"><em class="mf">kchen122@asu.edu</em></strong><em class="mf"> </em>and feel free to connect me on <a class="ae lt" href="https://www.linkedin.com/in/kuanyinchen-shirley/" rel="noopener ugc nofollow" target="_blank"><strong class="kn iu">LinkedIn</strong></a>!</span></pre></div></div>    
</body>
</html>