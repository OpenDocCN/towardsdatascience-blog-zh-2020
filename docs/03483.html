<html>
<head>
<title>Predicting Trump’s Tweets With A Recurrent Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用递归神经网络预测特朗普的推文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-trump-tweets-with-a-rnn-95e7c398b18e?source=collection_archive---------22-----------------------#2020-04-02">https://towardsdatascience.com/predicting-trump-tweets-with-a-rnn-95e7c398b18e?source=collection_archive---------22-----------------------#2020-04-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ca8f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用LSTM RNN生成作者和特定任务的文本。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7e4df115e52617f03b4cbc38d4d4a0bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lYQm3PDJDB8Km_zHHA7jrg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">唐纳德·特朗普著名的“covfefe”推文。</p></figure><p id="2e87" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">唐纳德·特朗普独特的方言是每个美国人都能识别的；然而，我们能建立一个神经网络来模仿特朗普的‘cov fefe’一样的词汇吗？</p><div class="lr ls gp gr lt lu"><a href="https://github.com/mm909/Predicting-Trump" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd ir gy z fp lz fr fs ma fu fw ip bi translated">mm 909/预测-川普</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">文本预测只在特朗普的推文、演讲和采访上进行了训练。-mm 909/预测-川普</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">github.com</p></div></div><div class="md l"><div class="me l mf mg mh md mi kp lu"/></div></div></a></div><p id="8d35" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在Andrej Karpathy被高度引用的博文<a class="ae mj" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">递归神经网络</a>的不合理有效性中，Karpathy表明<a class="ae mj" href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank">递归神经网络(RNNs) </a>可以作为非常强大的生成模型。他给出了RNNs生成莎士比亚戏剧的例子，维基百科关于自然主义的文章，甚至带有真实用户评论的Linux源代码。</p><p id="9f18" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mk">我们将使用RNN来生成新的特朗普推文。</em></p><p id="c5ce" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这不是对自然语言处理(NLP)或RNNs的深入指导，而是对项目的概述；尽管如此，还是提供了所有的代码，并且有大量的链接可以帮助扩展本文中没有完全解释的主题。</p><h1 id="2e3a" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">数据</h1><p id="d853" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">我们首先收集特朗普词汇的例子，<a class="ae mj" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf" rel="noopener ugc nofollow" target="_blank">越多越好</a>。幸运的是，我们不缺乏数据。特朗普的思想几乎总是保存在推特上，在演讲中，或者在采访中。我们将使用这些推文、演讲和采访的组合来训练角色预测模型。</p><h2 id="3372" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">小鸟叫声</h2><p id="9206" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">特朗普的推文在几个地方存档。我从TrumpTwitterArchive下载了大约50，000条他的推文。然后，我通过从推文中删除所有表情符号和网址来清理数据，以帮助提高预测的准确性。以下节选自<a class="ae mj" href="https://github.com/mm909/Predicting-Trump/blob/master/data/trump/tweets/clean/cleanTweets.txt" rel="noopener ugc nofollow" target="_blank"> cleanTweets.txt </a>:</p><blockquote class="nu nv nw"><p id="a687" class="kv kw mk kx b ky kz jr la lb lc ju ld nx lf lg lh ny lj lk ll nz ln lo lp lq ij bi translated">感谢我们伟大的美国企业为保护我们最脆弱的公民的安全所做的一切！</p><p id="4d97" class="kv kw mk kx b ky kz jr la lb lc ju ld nx lf lg lh ny lj lk ll nz ln lo lp lq ij bi translated">经双方同意，我们将暂时关闭与加拿大的北部边境，禁止非必要的交通。贸易不会受到影响。详情随后！</p><p id="5418" class="kv kw mk kx b ky kz jr la lb lc ju ld nx lf lg lh ny lj lk ll nz ln lo lp lq ij bi translated">同样根据每日来电显示，在佛罗里达州，嗜睡的乔·拜登以48%对42%领先。</p></blockquote><h2 id="7e36" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">演讲:GitHub</h2><p id="e592" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">不幸的是，特朗普的所有演讲都没有一个中心位置，但有几个GitHub存储库存有他的演讲子集。下面是一些你可以从哪里得到数据的例子；然而，我只使用了第一个库。</p><div class="lr ls gp gr lt lu"><a href="https://github.com/unendin/Trump_Campaign_Corpus" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd ir gy z fp lz fr fs ma fu fw ip bi translated">unendin/Trump_Campaign_Corpus</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">特朗普竞选语料库包括唐纳德·特朗普的演讲、采访、辩论、市政厅、新闻发布会…</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">github.com</p></div></div><div class="md l"><div class="oa l mf mg mh md mi kp lu"/></div></div></a></div><div class="lr ls gp gr lt lu"><a href="https://github.com/PedramNavid/trump_speeches" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd ir gy z fp lz fr fs ma fu fw ip bi translated">PedramNavid/trump _演讲</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">特朗普2015年6月至2016年11月9日的所有演讲——PedramNavid/Trump _ speechs</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">github.com</p></div></div><div class="md l"><div class="ob l mf mg mh md mi kp lu"/></div></div></a></div><div class="lr ls gp gr lt lu"><a href="https://github.com/ryanmcdermott/trump-speeches" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd ir gy z fp lz fr fs ma fu fw ip bi translated">ryanmcdermott/trump-演讲</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">speechs . txt:1mb的文本数据摘自唐纳德·特朗普在2016年竞选活动中不同时间点的演讲…</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">github.com</p></div></div><div class="md l"><div class="oc l mf mg mh md mi kp lu"/></div></div></a></div><div class="lr ls gp gr lt lu"><a href="https://github.com/alexmill/trump_transcripts" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd ir gy z fp lz fr fs ma fu fw ip bi translated">Alex mill/trump _抄本</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">这是唐纳德·特朗普竞选演讲和辩论笔录的半结构化版本，因为…</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">github.com</p></div></div><div class="md l"><div class="od l mf mg mh md mi kp lu"/></div></div></a></div><p id="a76a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">同样，与tweet数据一样，这些数据也需要清理。对于来自<em class="mk"> unendin的</em>存储库的数据，我们需要删除非川普演讲者的所有文本。以下是唐纳德·川普和马克·霍尔珀林在<a class="ae mj" href="https://github.com/unendin/Trump_Campaign_Corpus/blob/master/text/2015-06-17%2017.10.00%20(Complete%2C%20As%20spoken)%20Interview%20with%20Mark%20Halperin%2C%20John%20Heilemann%2C%20'With%20All%20Due%20Respect'%2C%20Bloomberg.txt" rel="noopener ugc nofollow" target="_blank">接受采访的节选。</a></p><blockquote class="nu nv nw"><p id="224f" class="kv kw mk kx b ky kz jr la lb lc ju ld nx lf lg lh ny lj lk ll nz ln lo lp lq ij bi translated">马克·哈尔珀林:人们对你的个性非常感兴趣。很多人，我想你听过这句话，认为你自大或者太自信了。有些人说你极度缺乏安全感，这就是你如此行事的原因。你对任何事情都没有安全感吗？</p><p id="009c" class="kv kw mk kx b ky kz jr la lb lc ju ld nx lf lg lh ny lj lk ll nz ln lo lp lq ij bi translated">唐纳德·特朗普:我不知道。也许是两者的结合。</p><p id="edab" class="kv kw mk kx b ky kz jr la lb lc ju ld nx lf lg lh ny lj lk ll nz ln lo lp lq ij bi translated">马克·哈尔珀林:你缺乏安全感吗？</p><p id="2903" class="kv kw mk kx b ky kz jr la lb lc ju ld nx lf lg lh ny lj lk ll nz ln lo lp lq ij bi translated">唐纳德·特朗普:每个人都没有安全感。</p></blockquote><p id="18c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以通过编写一个正则表达式来匹配“SPEAKER: TEXT ”,然后删除除Donald Trump之外的所有发言者，从而删除采访中马克·霍尔珀林一方的所有内容。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="bdb0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我还选择删除所有的换行符，因为我觉得它们对文本预测并不重要。由此产生的文字记录只是特朗普在采访中的一面之词。</p><blockquote class="nu nv nw"><p id="f696" class="kv kw mk kx b ky kz jr la lb lc ju ld nx lf lg lh ny lj lk ll nz ln lo lp lq ij bi translated">我不知道。也许是两者的结合。每个人都没有安全感。</p></blockquote><p id="b7a7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后将所有的演讲连接起来，放入<a class="ae mj" href="https://github.com/mm909/Predicting-Trump/blob/master/data/trump/speeches/clean/cleanSpeech.txt" rel="noopener ugc nofollow" target="_blank"> cleanSpeech.txt </a></p><h2 id="7c05" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">演讲:<a class="ae mj" href="https://factba.se/transcripts" rel="noopener ugc nofollow" target="_blank">事实基础</a></h2><p id="79de" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">FactBase是唐纳德·特朗普的演讲和采访档案。可悲的是，FactBase没有简单的方法一键下载他所有的演讲；然而，这并不重要，因为这些数据很容易被删除。</p><p id="7972" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以使用<a class="ae mj" href="https://github.com/mm909/Predicting-Trump/blob/master/scraping/factbase.py" rel="noopener ugc nofollow" target="_blank"> Selenium从FactBase </a>中抓取数据。随着时间的推移，这将把特朗普的所有新演讲添加到我们的数据库中。</p><h1 id="8351" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">数据准备</h1><p id="d2f0" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">要开始训练我们的模型，我们需要加载和处理文本数据，为网络做好准备。通过这样做，我们将定义两个重要的<a class="ae mj" href="https://medium.com/@jrodthoughts/knowledge-tuning-hyperparameters-in-machine-learning-algorithms-part-i-67a31b1f7c88" rel="noopener">超参数</a>:序列长度和步长。</p><h2 id="51be" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">加载数据</h2><p id="3a16" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">让我们从把数据载入内存开始。我们将读入数据，并使所有的文本小写。这样，T和T被认为是同一个字母。语料库长度是我们数据库中的字符数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div></figure><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="eca5" class="ni mm iq oh b gy ol om l on oo">Corpus length: 23342237</span></pre><h2 id="3a6a" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">字符词典</h2><p id="f446" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">模型不会自然地理解文本和标点符号，比如“a”和“？”，但他们确实理解数字。因此，我们需要一种方法将我们所有的文本翻译成数字。</p><p id="9970" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有很多方法可以做到这一点，但我选择使用两本字典。一个把字母变成数字，另一个把数字变成字母。</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="5b4c" class="ni mm iq oh b gy ol om l on oo">char_indices['a'] -&gt; 42<br/>indices_char[42] -&gt; 'a'</span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div></figure><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="f6c1" class="ni mm iq oh b gy ol om l on oo">Unique Chars: 66</span></pre><h2 id="d7c7" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">序列长度</h2><p id="6cd5" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated"><em class="mk">序列长度是我们的模型得到的上下文窗口的大小。</em></p><p id="8a7d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">序列长度定义了有多少个字母，以及有多少上下文，我们要给模型预测下一个字符。序列长度越长，模型在进行预测时就有越多的上下文。序列长度为20的示例:</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="99f0" class="ni mm iq oh b gy ol om l on oo">"Today I will be addr"</span></pre><p id="33fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可能会猜到这个序列中的下一个字母是e，句子中的下一个单词是addressing。这就是我们希望我们的模型预测的。</p><p id="e439" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于我们的模型，我们将使用长度为80的序列。</p><h2 id="aff4" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">步长</h2><p id="b3cf" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">步长是我们每次迭代滑动上下文窗口的距离。</p><p id="33f7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">步长定义了在制作训练样本时，每次迭代要移动多少个字母。下面是一个序列长度为20，步长为2，句子为“今天我将向全国发表演讲”的例子：</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="1d34" class="ni mm iq oh b gy ol om l on oo">"Today I will be addr"<br/>"day I will be addres"<br/>"y I will be addressi"<br/>"will be addressing t"<br/>"ll be addressing the"<br/>" be addressing the n"<br/>"e addressing the nat"<br/>"addressing the natio"<br/>"dressing the nation."</span></pre><p id="8dd7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于我们的模型，我们将使用步长4。</p><h2 id="23d9" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">序列长度和步长的权衡</h2><p id="62b6" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">我们希望序列长度尽可能长，以便模型有更多的上下文来预测下一个字符，然而，更长的序列长度意味着更大的模型和更多的处理时间。我选择序列长度为80，因为我更关心网络拼凑长文本串的能力，而不是网络的处理时间。</p><p id="dde8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">步长的权衡更加片面。步长越小越好。较小的步长意味着更多的训练样本和更多的相关句子，然而，只有一定数量的样本可以被存储。我们希望最小化步长，同时仍然将所有的句子放入内存。</p><p id="7c56" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">***使用<a class="ae mj" href="https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly" rel="noopener ugc nofollow" target="_blank">数据发生器</a> ***消除了步长折衷</p><h2 id="4166" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">编码</h2><p id="8029" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">使用选择的序列长度和步长，我们现在创建一个句子数组来保存所有拆分的句子，创建一个下一个字符数组来保存序列中的下一个字母。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="151e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了更具可读性，我展示的例子是序列长度为20，步长为2。句子数组现在看起来像这样:</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="a802" class="ni mm iq oh b gy ol om l on oo">["Today I will be addr",<br/>"day I will be addres",<br/>"y I will be addressi",<br/>"will be addressing t",<br/>"ll be addressing the",<br/>" be addressing the n",<br/>"e addressing the nat",<br/>"addressing the natio",<br/>"dressing the nation."]</span></pre><p id="e34b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下一个字符数组保存每个句子中的下一个字母:</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="720e" class="ni mm iq oh b gy ol om l on oo">['e',<br/>'s',<br/>'n',<br/>'h',<br/>' ',<br/>'a',<br/>'i',<br/>'n',<br/>'']</span></pre><p id="e91b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这为我们提供了培训示例的总数:</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="7694" class="ni mm iq oh b gy ol om l on oo">Number of training examples len(sentences): 5835540</span></pre><p id="27e8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我们将<a class="ae mj" href="https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/" rel="noopener ugc nofollow" target="_blank">热编码</a>这些序列，使它们能够被神经网络处理。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="4165" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们现在有(<strong class="kx ir"> X </strong>，y)对供网络处理。提醒一下，5835540是训练样本的数量，80是序列长度，66是唯一字符的数量。</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="deda" class="ni mm iq oh b gy ol om l on oo">X.shape: (5835540, 80, 66)<br/>y.shape: (5835540, 66)</span></pre><h1 id="cfd3" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">模型</h1><p id="5758" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">我们的模型将是一个<a class="ae mj" rel="noopener" target="_blank" href="/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">长短期记忆(LSTM) RNN </a>。递归层之后是几个<a class="ae mj" href="https://keras.io/layers/core/" rel="noopener ugc nofollow" target="_blank">密集层</a>，用<a class="ae mj" href="https://keras.io/activations/" rel="noopener ugc nofollow" target="_blank">比例指数线性单元(SELU) </a>激活，用<a class="ae mj" href="https://keras.io/layers/normalization/" rel="noopener ugc nofollow" target="_blank">批量归一化</a>正则化。</p><p id="72db" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在探索了几种不同的选择之后，我做出了这些决定。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/d696b71b61d89681b82ec1490847d8fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*aoGLRD7rlYb7S3uXlkUnxw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型图</p></figure><h1 id="1902" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">结果</h1><p id="17e8" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">仅仅过了四个时期，我们就有了一个验证数据准确率为67.362%的模型。考虑到随机猜测会给我们1.51%的准确度，该模型表现良好。因此，我们的模型比猜测精确44倍。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/c17fcce3319d029dd92b0497f8367391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ck2HoMIecP-ceAk473AoCg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型统计</p></figure><p id="a71a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下表显示了对句子“Today I”的预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/c2a38eaaacc32a034e6842b25101650d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*zg9ZXVdnMsV5ZOGN00dHww.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">《今日我》预测表</p></figure><p id="bfc9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于第一个字符预测，模型选择“w ”,但它对其决策不是很有信心。这是因为在“今天的我”之后有大量合理的可能性。尽管如此，有20.53%的把握，这个模型把“今天的我”变成了“今天的我”。</p><p id="0062" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在第一次预测之后，我们继续向模型询问字符。随着可能选项的数量减少，该模型变得更有信心。“今天我要”变成了“今天我要”，然后是“今天我要”。最终，该模型99%确信最后一个字符是“l ”,从而得到字符串“Today I will”。</p><h1 id="b0be" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">预测的句子</h1><p id="de63" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">以下是不同模型生成的句子示例。所有的模型都在相同的数据集上训练，但是具有不同的架构。这些模型能够理解twitter标签、标点、连词、重要日期，甚至反问句。</p><h2 id="6681" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">模型1</h2><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="1dbb" class="ni mm iq oh b gy ol om l on oo">Seed: 'Welcome to America'<br/>Output: 'Welcome to America, right? Well, I would have said that we have to make America great again, but I want to talk about the problem now.'</span><span id="31be" class="ni mm iq oh b gy os om l on oo">Seed: 'It is really'<br/>Output: 'It is really sad but the fake news media don't want to see the Republican Party continue to prove what we have.'</span></pre><h2 id="989d" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">模型2</h2><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="f2fa" class="ni mm iq oh b gy ol om l on oo">Seed: 'I'<br/>Output: 'I mean, in the middle east, we are going to win. We are going to repeal and replace Obamacare. Hillary is a disaster. We are going to win.'</span><span id="86d0" class="ni mm iq oh b gy os om l on oo">Seed: 'America'<br/>Output: 'America first. Our jobs are going to win on November 8th.  Manufacturing jobs have got to do it.'</span></pre><h2 id="f537" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">模型3</h2><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="9176" class="ni mm iq oh b gy ol om l on oo">Seed: 'Today I will' <br/>Output: 'Today I will make America great again! @realdonaldtrump is a big thing that is the best in the country and the president is a big deal.'</span><span id="d031" class="ni mm iq oh b gy os om l on oo">Seed: 'I think that'<br/>Output: 'I think that is a big contract that comes out the world trade center and we have to take care of our country.'</span></pre><h2 id="91a1" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">模型4</h2><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="dfae" class="ni mm iq oh b gy ol om l on oo">Seed: 'I will always'<br/>Output: 'I will always stop the state of the state of Michigan in the state of Michigan'</span><span id="a256" class="ni mm iq oh b gy os om l on oo">Seed: 'The whitehouse' <br/>Output: 'The whitehouse is not for the state of Michigan'</span><span id="59c8" class="ni mm iq oh b gy os om l on oo">Seed: 'China is a' <br/>Output: 'China is a total crisis and the story in the same thing that I was a great and I will be interviewed by @seanhannity to the @whitehouse!'</span></pre><p id="0658" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">披露</strong>:我在输出中添加了所有的大写。</p><h1 id="9d2d" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">结论</h1><p id="e479" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">我们创造了非常现实的句子，听起来就像真的来自唐纳德·特朗普的推特。我自信地说，这个模型肯定是<a class="ae mj" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">循环神经网络</a>不合理有效性的另一个例子。在整个过程中，我做了一些重要的观察，并将它们列在下面。</p><h2 id="9a3a" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">选择性短语</h2><p id="8df4" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">似乎每个模型都选择了一些短语来保持重用。有时这些短语会在一个预测中出现多次。模型1不断重复使用短语，“但我现在想谈谈这个问题”和“假新媒体。”对于模型2，一切都发生在中东。Model 3总说东西是“全国最好的”。最后，Model 4讨厌密歇根州。尽管所有这些短语都是唐纳德·特朗普词汇的一部分，但它们在预测中出现得太频繁了，以至于不自然。</p><p id="321f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个有趣的观察是，在相同数据集上训练的模型可以专注于不同的短语。这意味着模型更频繁选择的短语与数据集无关。</p><h2 id="fcdd" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">影响</h2><p id="9b4a" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated">除了娱乐价值，rnn还有实际的现实价值。能够生成作者和特定任务的文本，比如tweets，非常有用。rnn能够通过给你文本建议来帮助你写更快的电子邮件，比如Gmail<a class="ae mj" href="https://www.blog.google/products/gmail/subject-write-emails-faster-smart-compose-gmail/" rel="noopener ugc nofollow" target="_blank">。它们还可以帮助演讲稿撰写人更准确地匹配演讲者词汇。RNNs和NLP之间的交叉非常广泛，应用领域非常广阔。</a></p><h2 id="910b" class="ni mm iq bd mn nj nk dn mr nl nm dp mv le nn no mx li np nq mz lm nr ns nb nt bi translated">丰富</h2><p id="e88f" class="pw-post-body-paragraph kv kw iq kx b ky nd jr la lb ne ju ld le nf lg lh li ng lk ll lm nh lo lp lq ij bi translated"><strong class="kx ir">训练时间:</strong>不幸的是，每个纪元需要大约一个小时才能完成，所以我将训练限制在4个纪元。虽然从第3代到第4代的改进很小，但我认为多训练几个小时会产生更精确的模型。</p><p id="cbf3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在写这篇文章的时候，我发现了<a class="ae mj" href="https://keras.io/layers/recurrent/" rel="noopener ugc nofollow" target="_blank"> CuDNNLSTM </a>。用CuDNNLSTM替换LSTM，可减少66%的培训时间。我还没有CuDNNLSTM如何影响模型准确性的结果；然而，我怀疑CuDNNLSTM和LSTM会表现相似。</p><p id="6c8f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">减少偏差:</strong>模型有大量可避免和不可避免的偏差，正如吴恩达在<a class="ae mj" href="https://d2wvfoqc9gyqzf.cloudfront.net/content/uploads/2018/09/Ng-MLY01-13.pdf" rel="noopener ugc nofollow" target="_blank">机器学习向往</a>中所定义的。即使对人类来说，文本预测也是一项艰巨的任务，正因为如此，我们可以预期会有相当高的不可避免的偏差。还有大量可以避免的偏见。这可以通过使用<a class="ae mj" href="https://github.com/minimaxir/textgenrnn" rel="noopener ugc nofollow" target="_blank">更复杂的模型</a>和添加<a class="ae mj" href="https://machinelearningmastery.com/what-are-word-embeddings/" rel="noopener ugc nofollow" target="_blank">字嵌入</a>来解决。</p><p id="f07a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">减少差异:</strong>唐纳德·特朗普(Donald Trump)喜欢的“covfefe”词汇的例子越多越好。我希望继续收到更多的推文、演讲和采访。虽然方差目前非常低，但随着模型变得更加复杂，方差将会增加。添加更多的数据将有助于控制差异。</p><p id="976e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> Keras dataGenerator: </strong>我还没有为文本数据实现<a class="ae mj" rel="noopener" target="_blank" href="/writing-custom-keras-generators-fe815d992c5a"> Keras的dataGenerator </a>类。实现这个类意味着我们的模型可以总是步长为1而不会溢出内存。这意味着我们可以显著增加训练数据库的大小，而不用担心它不适合内存。</p><p id="bdbe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所有这些改进将使模型更加精确。</p></div><div class="ab cl ot ou hu ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="ij ik il im in"><h1 id="24b3" class="ml mm iq bd mn mo pa mq mr ms pb mu mv jw pc jx mx jz pd ka mz kc pe kd nb nc bi translated">项目存储库</h1><div class="lr ls gp gr lt lu"><a href="https://github.com/mm909" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd ir gy z fp lz fr fs ma fu fw ip bi translated">mm909 -概述</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">在GitHub上注册你自己的个人资料，这是托管代码、管理项目和构建软件的最佳地方…</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">github.com</p></div></div><div class="md l"><div class="pf l mf mg mh md mi kp lu"/></div></div></a></div></div><div class="ab cl ot ou hu ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="ij ik il im in"><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="fa13" class="ni mm iq oh b gy ol om l on oo">Seed: 'Today I will'<br/>Output: 'Today I will be the world -- I think.'</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/24405825bbd3cf30724093bba93422d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*lXjF3oYFlvFeqefCthigRA.png"/></div></figure></div></div>    
</body>
</html>