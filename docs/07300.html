<html>
<head>
<title>AI: Taking A Peek Under The Hood. Part 2, Creating a Two-Layer Neural Network the Old-Fashioned Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">艾:往引擎盖下看一眼。第2部分，用传统的方法创建一个两层的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-taking-a-peek-under-the-hood-part-2-creating-a-two-layer-neural-network-the-old-fashioned-way-568ce2cf6d06?source=collection_archive---------75-----------------------#2020-06-02">https://towardsdatascience.com/ai-taking-a-peek-under-the-hood-part-2-creating-a-two-layer-neural-network-the-old-fashioned-way-568ce2cf6d06?source=collection_archive---------75-----------------------#2020-06-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a8932e65bfe6e2e8096fef3b312735ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3Kx2bPObYOoTyBlIB778w.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片来源:马太·亨利</p></figure><p id="8ae7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在深入本系列这一部分的新内容之前，我们先快速回顾一下本系列第1部分中所涉及的内容，以便让后续读者耳目一新，并为新读者提供背景知识。如前所述，人工智能只是数学。为我们今天看到的所有人工智能功能提供动力的数学算法是神经网络。</p><p id="dd14" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">神经网络有三种一般形式:</p><ol class=""><li id="cc7d" class="lb lc iq ke b kf kg kj kk kn ld kr le kv lf kz lg lh li lj bi translated">标准神经网络(通常用于二元分类和点估计或回归型问题)。</li><li id="f51f" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">卷积神经网络(一般用于图像识别)。</li><li id="2276" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">递归神经网络(通常应用于涉及序列数据的问题，如文本和语音识别、理解或预测)。</li></ol><p id="0e24" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">今天我们将构建一个标准的两层神经网络(一个隐藏层，一个输出层)。数学将被解释并且python代码将被提供。这将使你不仅能准确理解标准神经网络的工作原理，还能让你自己构建它们。</p><p id="64d3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">首先，在打开每个步骤之前，让我们快速看一下涉及的步骤。请注意，我们将不涉及数据争论、清理或转换，因为这在数据集和应用程序之间可能有很大差异。也就是说，值得指出的是，标准化输入数据对于关系识别以及最小化计算成本非常重要。</p><h1 id="f4c7" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">步骤:</h1><ol class=""><li id="4366" class="lb lc iq ke b kf mn kj mo kn mp kr mq kv mr kz lg lh li lj bi translated">定义模型结构(如输入要素的数量)</li><li id="b960" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">初始化模型的参数</li><li id="33fd" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">计算电流损耗(又名“正向传播”)</li><li id="a7fc" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">计算当前梯度(又名“反向传播”)</li><li id="853e" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">更新参数(梯度下降)</li><li id="d5be" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">根据保留数据进行预测</li></ol><p id="d618" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">步骤3-5在一个循环中执行，该循环取决于我们执行的迭代次数。我们执行的迭代次数是我们在步骤1中定义模型结构时做出的决定。下图说明了这一过程。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ms"><img src="../Images/4374298207eeb02b19f5df2681a7cd03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G9H0-4BgHhL06yezoNyjPA.png"/></div></div></figure><h1 id="b175" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">步骤1:定义模型结构</h1><p id="16b6" class="pw-post-body-paragraph kc kd iq ke b kf mn kh ki kj mo kl km kn mx kp kq kr my kt ku kv mz kx ky kz ij bi translated">定义您的模型结构包括您要构建的神经网络的类型，以及神经网络的超参数。超参数是层数、每层的节点数、每层中的激活函数、alpha(学习速率——这对梯度下降很重要，因为它决定了我们更新参数的速度)和迭代次数。这些超参数将影响我们的参数“w”和“b”(权重和偏差)，它们是在网络的每个节点内计算的。权重和偏差又是应用于网络每个节点的激活函数的输入。</p><p id="0682" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这部分既是艺术也是科学。在某个领域拥有丰富经验的数据科学家将获得直觉，知道哪组超参数最适合他们的业务问题，但这通常不能跨问题和领域转移，需要手动试错。你可以在谷歌上搜索大量的“经验法则”来给你一个关于起点的好主意，但这不会消除尝试不同组合来为你的业务问题提出最佳模型的需要。</p><p id="9321" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">需要记住的一点是，正如本系列第1部分中所讨论的，深度神经网络往往工作得更好，并且还可以减少网络中所需的节点总数。为了说明为什么深度神经网络通常会更好地工作，我们以面部识别为例。在建立一个执行面部识别的神经网络时，你需要几个层次来将任务分成易于管理的块。第一个隐藏层将关注更简单、更小的任务，例如检测边缘。第二个隐藏层将建立在这些边缘上，以检测面部部分(如鼻子、眼睛、嘴唇等。).第三层将把这些碎片放在一起以识别整张脸。最后，你的输出图层会告诉你这是不是你要找的脸(作为一个概率)。这就是为什么有时你的手机或电脑能识别你，有时却不能。如果算法以一定的概率确定实际上是你，该软件被编程为解锁你的设备。这个由网络输出的概率根据照明、你面对屏幕的角度、你是否戴着眼镜等等而变化。</p><p id="506a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">也就是说，让我们开始定义我们的神经网络的结构。</p><p id="8f4f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在下面的代码中，我们将定义3个变量:</p><ul class=""><li id="f34f" class="lb lc iq ke b kf kg kj kk kn ld kr le kv lf kz na lh li lj bi translated">n_x:输入层的大小</li><li id="b039" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz na lh li lj bi translated">n_h:隐藏层的大小(隐藏层中有多少个神经元——我们将设置为4个神经元)</li><li id="f018" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz na lh li lj bi translated">n_y:输出层的大小</li></ul><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="3d93" class="ng lq iq nc b gy nh ni l nj nk">def layer_sizes(X, Y):<br/>    """<br/>    Arguments:<br/>    X -- input dataset of shape (input size, number of examples)<br/>    Y -- labels of shape (output size, number of examples)<br/>    <br/>    Returns:<br/>    n_x -- the size of the input layer<br/>    n_h -- the size of the hidden layer<br/>    n_y -- the size of the output layer<br/>    """<br/><br/>    n_x = X.shape[0] # size of input layer<br/>    n_h = 4 #neurons in hidden layer<br/>    n_y = Y.shape[0] # size of output layer<br/>    <br/>    return (n_x, n_h, n_y)</span></pre><h1 id="3144" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">步骤2:初始化模型参数</h1><p id="470a" class="pw-post-body-paragraph kc kd iq ke b kf mn kh ki kj mo kl km kn mx kp kq kr my kt ku kv mz kx ky kz ij bi translated">初始化我们的模型参数意味着初始化我们的参数“w”和“b ”,在我们开始迭代每个节点内的计算和优化过程以识别我们的输入数据和预测(输出)变量之间的真实关系之前，以某个值开始。</p><p id="ca4e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">权重(“w”)将被初始化为非常小的随机值，而偏差(“b”)将被初始化为0。我们希望权重值较小，以便它们开始时接近双曲正切函数或sigmoid函数的中心，这样可以加快学习速度。如果我们将它们初始化为较大的值，那么我们将在曲线斜率很小的函数尾部附近开始优化，这将会减慢我们随后的梯度下降(优化)。它们需要随机的原因是它们不完全相同。节点必须计算不同的函数，以便它们有用。点击<a class="ae la" href="https://www.researchgate.net/figure/Nonlinear-function-a-Sigmoid-function-b-Tanh-function-c-ReLU-function-d-Leaky_fig3_323617663" rel="noopener ugc nofollow" target="_blank">此处</a>查看sigmoid、tanh、ReLU和Leaky ReLU函数的对比。</p><p id="3595" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">从这些图像中，您可以看到，靠近双曲正切函数或sigmoid函数的尾部(由大值产生)意味着几乎没有斜率。用小的值初始化并不总是能消除这个问题，这也是ReLU函数能提高处理时间的原因之一。</p><p id="3e3d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们看一下代码:</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="f52c" class="ng lq iq nc b gy nh ni l nj nk">def initialize_parameters(n_x, n_h, n_y):<br/>    """<br/>    Argument:<br/>    n_x -- size of the input layer<br/>    n_h -- size of the hidden layer<br/>    n_y -- size of the output layer<br/>    <br/>    Returns:<br/>    params -- python dictionary containing your parameters:<br/>                    W1 -- weight matrix of shape (n_h, n_x)<br/>                    b1 -- bias vector of shape (n_h, 1)<br/>                    W2 -- weight matrix of shape (n_y, n_h)<br/>                    b2 -- bias vector of shape (n_y, 1)<br/>    """<br/>    <br/>    np.random.seed(2) #set up a seed so that your output is consistent<br/><br/>    W1 = np.random.randn(n_h,n_x)* 0.01<br/>    b1 = np.zeros((n_h,1))<br/>    W2 = np.random.randn(n_y,n_h) * 0.01<br/>    b2 = np.zeros((n_y,1))<br/>    <br/>    assert (W1.shape == (n_h, n_x))<br/>    assert (b1.shape == (n_h, 1))<br/>    assert (W2.shape == (n_y, n_h))<br/>    assert (b2.shape == (n_y, 1))<br/>    <br/>    parameters = {"W1": W1,<br/>                  "b1": b1,<br/>                  "W2": W2,<br/>                  "b2": b2}<br/>    <br/>    return parameters</span></pre><p id="9c55" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">请注意，断言语句是为了确保我们的权重和偏差矩阵是符合我们的神经网络规范的正确维度(本文不涉及细节)。</p><h1 id="ef68" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">步骤3:正向传播</h1><p id="130f" class="pw-post-body-paragraph kc kd iq ke b kf mn kh ki kj mo kl km kn mx kp kq kr my kt ku kv mz kx ky kz ij bi translated">前向传播是指我们在网络中从输入层移动到输出层，在此过程中计算每个节点中的几个重要值。在每个节点中，我们将执行以下计算:</p><ul class=""><li id="9815" class="lb lc iq ke b kf kg kj kk kn ld kr le kv lf kz na lh li lj bi translated">Z = w^T x + b (w转置乘以x + b)</li><li id="1f41" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz na lh li lj bi translated">并且计算激活函数a = σ(Z)(在这个例子中使用sigmoid函数)</li></ul><p id="d10f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">重要的是将其矢量化以加快处理速度，而不是在Python中专门作为循环来执行这一步骤。为此，我们将通过以下方式将Z计算为矢量:</p><ol class=""><li id="fee2" class="lb lc iq ke b kf kg kj kk kn ld kr le kv lf kz lg lh li lj bi translated">把我们所有的W堆成一个矩阵(每个W是一个向量，但是我们转置它们，每个转置的向量变成一行。因此将它们堆叠在一起形成矩阵)，</li><li id="6b8a" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz lg lh li lj bi translated">然后简单地把b向量加到W矩阵上。</li></ol><p id="5858" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">就选择你的激活功能而言，这实际上取决于你试图解决的问题。但是，在隐藏层中，您可能希望选择一个加速处理的函数(例如tanh或ReLU)，而在输出层中，您需要选择一个与业务问题相关的激活函数。例如，如果您正在进行二进制分类，sigmoid函数是有意义的，因为它会给出一个介于0和1之间的值，然后您只需设置您的接受阈值(任何&gt; = 0.5的值是否等于1，或者是其他值？).</p><p id="1f3b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在查看代码之前，还有一点需要注意。在优化(梯度下降)部分，我们将需要能够计算我们的激活函数的导数(斜率)。因此，我们将在正向传播期间将这些值存储为“缓存”。</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="7e28" class="ng lq iq nc b gy nh ni l nj nk">def forward_propagation(X, parameters):<br/>    """<br/>    Argument:<br/>    X -- input data of size (n_x, m)<br/>    parameters -- python dictionary containing your parameters (output of initialization function)<br/>    <br/>    Returns:<br/>    A2 -- The sigmoid output of the second activation<br/>    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"<br/>    """<br/>    # Retrieve each parameter from the dictionary "parameters"<br/><br/>    W1 = parameters["W1"]<br/>    b1 = parameters["b1"]<br/>    W2 = parameters["W2"]<br/>    b2 = parameters["b2"]<br/>    <br/>    # Implement Forward Propagation to calculate A2 (probabilities)<br/><br/>    Z1 = np.dot(W1,X) + b1<br/>    A1 = np.tanh(Z1)<br/>    Z2 = np.dot(W2,A1) + b2<br/>    A2 = sigmoid(Z2)<br/>    <br/>    assert(A2.shape == (1, X.shape[1]))<br/>    <br/>    cache = {"Z1": Z1,<br/>             "A1": A1,<br/>             "Z2": Z2,<br/>             "A2": A2}<br/>    <br/>    return A2, cache</span></pre><p id="afc4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">通过一次正向传播计算了权重、偏差和激活值之后，我们需要计算成本(所有训练示例的平均损失)。请记住，损失本质上是误差(预测值和真实值之间的差异)。</p><p id="9edc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是我们将通过向前和向后迭代来最小化的，通过迭代生成新值，计算成本，然后更新我们网络中的值，直到模型收敛或我们满足我们指定的迭代次数。</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="7c2e" class="ng lq iq nc b gy nh ni l nj nk">def compute_cost(A2, Y, parameters):<br/>    """<br/>    <br/>    Arguments:<br/>    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)<br/>    Y -- "true" labels vector of shape (1, number of examples)<br/>    parameters -- python dictionary containing your parameters W1, b1, W2 and b2<br/>    <br/>    Returns:<br/>    cost -- cross-entropy cost <br/>    <br/>    """<br/>    <br/>    m = Y.shape[1] # number of example<br/><br/><br/>    # Compute the cross-entropy cost<br/><br/>    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1 - A2),1 - Y)<br/>    cost = - np.sum(logprobs) * (1 / m)<br/>    <br/>    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. <br/>                                  <br/>    assert(isinstance(cost, float))<br/>    <br/>    return cost</span></pre><h1 id="18c4" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">步骤4:反向传播</h1><p id="f958" class="pw-post-body-paragraph kc kd iq ke b kf mn kh ki kj mo kl km kn mx kp kq kr my kt ku kv mz kx ky kz ij bi translated">当进行反向传播时，我们需要能够计算激活函数的导数。对于任何给定的Z值，该函数将具有对应于该值的某个斜率。目标是使用梯度下降来寻找凸函数的全局最小值。点击<a class="ae la" href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">阅读更多关于梯度下降的信息。</a></p><p id="5741" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们将计算以下导数(斜率):</p><ul class=""><li id="0aee" class="lb lc iq ke b kf kg kj kk kn ld kr le kv lf kz na lh li lj bi translated">dZ[2] = A[2]-Y</li><li id="7059" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz na lh li lj bi translated">dW[2] = 1/m dZ[2] A[2]T</li><li id="544e" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz na lh li lj bi translated">db[2] = 1/m np.sum(dZ[2]，axis =1，keepdims = True)</li><li id="1fa5" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz na lh li lj bi translated">dZ[1] = W[2]T dZ[2] * g[1]' (Z[1])</li><li id="b713" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz na lh li lj bi translated">dW[1] = 1/m dZ[1] XT</li><li id="b12f" class="lb lc iq ke b kf lk kj ll kn lm kr ln kv lo kz na lh li lj bi translated">db[1] = 1/m np.sum(dZ[1]，axis =1，keepdims = True)</li></ul><p id="1fd7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">注意[2]指的是神经网络的第二层，而[1]指的是第一层(在这种情况下，这是我们的隐藏层)，' T '代表转置。如你所见，我们从神经网络的末端开始，然后回到起点(因此有术语“反向传播”)。</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="a31d" class="ng lq iq nc b gy nh ni l nj nk">def backward_propagation(parameters, cache, X, Y):<br/>    """<br/>    <br/>    Arguments:<br/>    parameters -- python dictionary containing our parameters <br/>    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".<br/>    X -- input data of shape (2, number of examples)<br/>    Y -- "true" labels vector of shape (1, number of examples)<br/>    <br/>    Returns:<br/>    grads -- python dictionary containing your gradients with respect to different parameters<br/>    """<br/>    m = X.shape[1]<br/>    <br/>    # First, retrieve W1 and W2 from the dictionary "parameters".<br/><br/>    W1 = parameters["W1"]<br/>    W2 = parameters["W2"]<br/>        <br/>    # Retrieve also A1 and A2 from dictionary "cache".<br/><br/>    A1 = cache["A1"]<br/>    A2 = cache["A2"]<br/>    <br/>    # Backward propagation: calculate dW1, db1, dW2, db2. <br/><br/>    dZ2 = A2 - Y<br/>    dW2 = 1 / m *(np.dot(dZ2,A1.T))<br/>    db2 = 1 / m *(np.sum(dZ2,axis = 1,keepdims = True))<br/>    dZ1 = np.dot(W2.T,dZ2) * (1 - np.power(A1, 2))<br/>    dW1 = 1 / m *(np.dot(dZ1,X.T))<br/>    db1 = 1 / m *(np.sum(dZ1,axis = 1,keepdims = True))<br/>    <br/>    grads = {"dW1": dW1,<br/>             "db1": db1,<br/>             "dW2": dW2,<br/>             "db2": db2}<br/>    <br/>    return grads</span></pre><p id="8d22" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们现在需要使用刚刚计算的导数来更新W1、b1、W2和b2的值。正如在本系列的<a class="ae la" href="https://www.linkedin.com/pulse/ai-taking-peek-under-hood-part-1-introduction-neural-simon-campbell/" rel="noopener ugc nofollow" target="_blank">第1部分</a>中提到的，这是深度学习的“学习”部分。这种更新是通过取向前传播中确定的值减去学习率(我们选择的超参数)乘以来自上面确定的当前迭代的线的梯度(或斜率)来完成的。</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="e4c1" class="ng lq iq nc b gy nh ni l nj nk">def update_parameters(parameters, grads, learning_rate = 1.2):<br/>    """<br/>    <br/>    Arguments:<br/>    parameters -- python dictionary containing your parameters <br/>    grads -- python dictionary containing your gradients <br/>    <br/>    Returns:<br/>    parameters -- python dictionary containing your updated parameters <br/>    """<br/>    # Retrieve each parameter from the dictionary "parameters"<br/><br/>    W1 = parameters["W1"]<br/>    b1 = parameters["b1"]<br/>    W2 = parameters["W2"]<br/>    b2 = parameters["b2"]<br/>    <br/>    # Retrieve each gradient from the dictionary "grads"<br/><br/>    dW1 = grads["dW1"]<br/>    db1 = grads["db1"]<br/>    dW2 = grads["dW2"]<br/>    db2 = grads["db2"]<br/>    <br/>    # Update rule for each parameter<br/><br/>    W1 = W1 - learning_rate * dW1<br/>    b1 = b1 - learning_rate * db1<br/>    W2 = W2 - learning_rate * dW2<br/>    b2 = b2 - learning_rate * db2<br/>    <br/>    parameters = {"W1": W1,<br/>                  "b1": b1,<br/>                  "W2": W2,<br/>                  "b2": b2}<br/>    <br/>    return parameters</span></pre><p id="36a6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在预测之前，让我们将这些函数放在一起，创建我们的标准神经网络模型。在本例中，我们将迭代次数设置为10，000次。</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="d816" class="ng lq iq nc b gy nh ni l nj nk">def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):<br/>    """<br/>    Arguments:<br/>    X -- dataset of shape (2, number of examples)<br/>    Y -- labels of shape (1, number of examples)<br/>    n_h -- size of the hidden layer<br/>    num_iterations -- Number of iterations in gradient descent loop<br/>    print_cost -- if True, print the cost every 1000 iterations<br/>    <br/>    Returns:<br/>    parameters -- parameters learnt by the model. They can then be used to predict.<br/>    """<br/>    <br/>    np.random.seed(3)<br/>    n_x = layer_sizes(X, Y)[0]<br/>    n_y = layer_sizes(X, Y)[2]<br/>    <br/>    # Initialize parameters<br/><br/>    parameters = initialize_parameters(n_x,n_h,n_y)<br/>    W1 = parameters["W1"]<br/>    b1 = parameters["b1"]<br/>    w2 = parameters["W2"]<br/>    b2 = parameters["b2"]<br/>    <br/>    # Loop (gradient descent)<br/><br/>    for i in range(0, num_iterations):<br/>         <br/>        # Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".<br/>        A2, cache = forward_propagation(X,parameters)<br/>        <br/>        # Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".<br/>        cost = compute_cost(A2, Y, parameters)<br/> <br/>        # Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".<br/>        grads = backward_propagation(parameters, cache, X, Y)<br/> <br/>        # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".<br/>        parameters = update_parameters(parameters, grads)<br/>        <br/>        # Print the cost every 1000 iterations<br/>        if print_cost and i % 1000 == 0:<br/>            print ("Cost after iteration %i: %f" %(i, cost))<br/><br/><br/>    return parameters</span></pre><h1 id="8cee" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">第六步:预测</h1><p id="cba6" class="pw-post-body-paragraph kc kd iq ke b kf mn kh ki kj mo kl km kn mx kp kq kr my kt ku kv mz kx ky kz ij bi translated">为了对保留数据进行预测，您将运行优化模型的正向传播。在本例中，我们预测了一个二进制分类，其中输出图层中激活返回的任何大于0.5的值都被分类为1，因此任何小于或等于0.5的值都被分类为0。</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="b618" class="ng lq iq nc b gy nh ni l nj nk">def predict(parameters, X):<br/>    """<br/>    Using the learned parameters, predicts a class for each example in X<br/>    <br/>    Arguments:<br/>    parameters -- python dictionary containing your parameters <br/>    X -- input data of size (n_x, m)<br/>    <br/>    Returns<br/>    predictions -- vector of predictions of our model (red: 0 / blue: 1)<br/>    """<br/>    <br/>    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.<br/><br/>    A2, cache = forward_propagation(X,parameters)<br/>    predictions = (A2 &gt; 0.5)<br/>    <br/>    <br/>    return predictions</span></pre><p id="7219" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最后，您可以按如下方式检查神经网络的准确性:</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="3a54" class="ng lq iq nc b gy nh ni l nj nk">predictions = predict(parameters, X)<br/><br/>print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')</span></pre><h1 id="dde0" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">结束语:</h1><p id="a24d" class="pw-post-body-paragraph kc kd iq ke b kf mn kh ki kj mo kl km kn mx kp kq kr my kt ku kv mz kx ky kz ij bi translated">在上面的内容中，我们介绍了构建两层标准神经网络的理论、数学和代码(一个隐藏层，一个输出层)。我希望这有助于增强您对日常遇到的人工智能应用程序的理解，同时为您提供一个强有力的起点，为二进制分类问题构建您自己的人工智能应用程序。网上有大量很棒的内容，你可以更深入地探讨其中的许多话题。我个人从吴恩达在Coursera上的神经网络和深度学习课程中学到了所有这些，你可以在这里找到<a class="ae la" href="https://www.coursera.org/learn/neural-networks-deep-learning?" rel="noopener ugc nofollow" target="_blank"/>。</p></div></div>    
</body>
</html>