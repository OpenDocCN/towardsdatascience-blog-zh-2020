<html>
<head>
<title>Improving the interpretability of the Random Forest classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">提高随机森林分类器的可解释性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/representing-a-random-forest-with-few-decision-trees-11283e433dbb?source=collection_archive---------19-----------------------#2020-02-26">https://towardsdatascience.com/representing-a-random-forest-with-few-decision-trees-11283e433dbb?source=collection_archive---------19-----------------------#2020-02-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1a80" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如果随机森林中的底层树形成集群，我们可以从每个集群中提取一棵树来表示随机森林</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6c5583102214541dd8950bba919e7f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hGCGEJKsl0JGwOkK"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@patrickian4?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">帕特里克·福尔</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="226e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于数据科学家来说，可解释性通常与结果同样重要。面对可解释性与准确性的权衡，我们经常避免部署深度学习模型和复杂的集成。基于决策树的随机森林分类器就是这样一种强大且常用的方法。这个想法是通过平均它们的结果来减少几个噪声决策树的预测中的方差。在可解释性方面，大多数人将其置于常规机器学习模型和深度学习之间。许多人认为这是一个黑箱。</p><p id="825e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管被广泛使用，随机森林通常仅用特征重要性和邻近图来解释。这些可视化非常有用，但还不够。许多研究人员正试图确定使用决策树来表示随机森林的方法。一个有趣的方法是近似一个单一的决策树，如周和胡克[1]和温伯格[2]所解释的。</p><p id="eaf4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我提出了一个替代的和新颖的解决方案。我证明了随机森林中的底层决策树可以形成集群，这使我们能够从每个集群中提取代表性的决策树，并使用它们进行解释。</p><h2 id="f7c6" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">底层决策树会形成聚类吗？</h2><p id="75bd" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">对于这个分析，我使用了随机森林分类器的 Scikit-learn 实现，对于二进制分类问题，n_estimators = 20，max_depth = 10。该数据有 75，800 个观察值和 372 个特征[3]。</p><p id="eda7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练模型后，第一步是提取底层决策树，并在所有深度级别(0 到 10)对其进行后期修剪。随机森林的 Scikit-learn 实现允许使用。评估者 _。然而，我们需要为后期修剪编码，利用决策树的二叉树结构。这个过程产生了 11 X 20 = 220 个独立的决策树。</p><p id="9107" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，使用每个决策树预测测试集的输出概率，以获得预测概率的[220 X(测试集大小)]矩阵，并对该矩阵执行相关性分析、t-SNE 和层次聚类，以识别聚类。</p><ol class=""><li id="f68a" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><strong class="ky ir">关联</strong></li></ol><p id="90e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图描绘了每棵树在不同深度级别的预测之间的相关性。在低深度，随机森林中的树木往往是相似的，所以我们看到了高度的正相关。随着深度的增加，相关性降低，因为树在不同的特征上开始分裂。重要的是，我们看到的模式表明，在每一个深度水平的集群的潜在形成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/b6977beb2be41415eddae614e114ac95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*C0mf3vJQfuap6yIxSRdaTQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基础决策树预测中的相关性。轴代表树的 ID。</p></figure><p id="2ae9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 2。t-SNE </strong></p><p id="fae9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">t-SNE 是一种非线性降维算法，对于可视化高维数据非常有用。下图显示了结果，其中每个数据点代表一个决策树，标签代表该树的 ID。结果还显示了在不同深度水平上的集群形成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/f941aaa86c516ac3269cb0479dda0fd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/1*9tQVhrCZw5vWw3YZ3K0Qfw.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">底层决策树预测的 t-SNE 表示。</p></figure><p id="04be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 3。层次聚类</strong></p><p id="ffdb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，让我们来看看凝聚集群。相关分析和 t-SNE 映射可以给我们提供关于最佳聚类数的有用信息。在这个特殊的例子中，我选择了两个集群。对于高维矩阵来说，在二维中可视化分层聚类是不可能的。然而，由于这是一个二进制分类问题，我们可以通过在其接收器操作特性(ROC)曲线上对每棵树进行颜色编码聚类分配来可视化结果。下面的结果也验证了簇的形成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/2a4488abfbe73c3a591c847e987125ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/1*u5Mcr8nO1onefQGXliDacA.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">聚类分配颜色编码的决策树 ROC 曲线</p></figure><p id="d88f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">观察这三个图中的树 id，您可以观察到这三种方法的聚类是相当一致的，并且树停留在不同深度级别的相同聚类中。</p><h1 id="2af2" class="nb lt iq bd lu nc nd ne lx nf ng nh ma jw ni jx md jz nj ka mg kc nk kd mj nl bi translated">结论</h1><p id="0b6b" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">总的来说，我想从上面的分析中展示的关键思想是随机森林中的底层决策树可以展示集群。一旦我们对集群有了信心，最后一步是从每个集群中挑选一棵有代表性的未修剪的树，并将其呈现给客户。代表树可以是最靠近聚类中心的树，也可以是聚类中 AUC 最高的树。需要注意的是，修剪只是为了演示集群。最后一步，你只需要未修剪的树。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><p id="7f00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nt"> [1]周逸尘和贾尔斯·胡克。2016.通过单树近似解释模型。arXiv 预印本 arXiv:1610.09036 (2016)。</em></p><p id="2aca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nt"> [2] Weinberg AI，Last M .从用于快速大数据分类的决策树模型集合中选择代表性决策树。j 大数据。2019;6(1):23 </em></p><p id="262f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]<a class="ae kv" href="https://www.kaggle.com/c/santander-customer-satisfaction/data" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/Santander-customer-satisfaction/data</a></p></div></div>    
</body>
</html>