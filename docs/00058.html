<html>
<head>
<title>Classifying Scientific Papers with Universal Sentence Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于通用句子嵌入的科技论文分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44?source=collection_archive---------29-----------------------#2020-01-02">https://towardsdatascience.com/classifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44?source=collection_archive---------29-----------------------#2020-01-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ecb5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何教计算机区分天体物理学和计算机科学？</h2></div><p id="589b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">几年来，由于计算机视觉的惊人进步，计算机已经获得了几乎像人类一样的识别建筑物、动物和其他物体的能力。使这成为可能的突破是理论直觉(<em class="le">即</em>上世纪80年代<a class="ae lf" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a>的引入)和硬件(得益于GPU卡的广泛和经济高效的可用性)的结合，这使得在合理的时间内执行复杂的算法成为可能。这一类别的缩影可能是VGG类的模型，即<a class="ae lf" href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" rel="noopener ugc nofollow" target="_blank">公开可用的</a>部署，因为大多数预先训练的模型。像它的大多数兄弟一样，<a class="ae lf" href="https://keras.io/applications/#vgg19" rel="noopener ugc nofollow" target="_blank"> VGG -19 </a>需要训练一个强大的计算机集群，但是一旦初始阶段完成，它可以部署在不太强大的机器上，在许多情况下甚至是移动设备上(例如参见<a class="ae lf" href="https://keras.io/applications/#mobilenet" rel="noopener ugc nofollow" target="_blank"> MobileNet </a>)。然而，虽然计算机视觉领域在相当长的一段时间内一直处于黄金时代，但对于另一项任务:书面文本的解释来说，情况并非如此。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/a56de0a8e220c4fee64690607f8aa24f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XV0QjOo7fdrOyLA2ON9uUA.jpeg"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">现在，计算机非常善于识别常见的物体。但是他们足够好去阅读一篇科学论文吗，更不用说去理解它的意思了？</p></figure><p id="f7c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以说，如果人类不能在为时已晚之前识别出狮子、鬣狗和其他食肉动物的威胁，他们就不会在生物进化的早期阶段幸存下来。另一方面，人类学家认为，随着我们语言能力的提高，人类社会变得越来越复杂。虽然语言最初是如何成为一种东西的<a class="ae lf" href="https://en.wikipedia.org/wiki/Origin_of_language" rel="noopener ugc nofollow" target="_blank">还没有被完全理解</a>，但是很明显，交流基本信息并不需要一种具有语法的完全成熟的语言。这个概念由<a class="ae lf" href="https://en.wikipedia.org/wiki/Noam_Chomsky" rel="noopener ugc nofollow" target="_blank">诺姆·乔姆斯基</a>在他的<a class="ae lf" href="https://en.wikipedia.org/wiki/Chomsky_hierarchy" rel="noopener ugc nofollow" target="_blank">语法等级理论</a>中进行了著名的阐述。因此，期望机器能够理解人类语言，或者至少通过口头交流的方式与我们互动，这并不太遥远，尽管在开始时是有限的。长话短说，我们正在努力，但人类语言的复杂性是一个非常难啃的骨头。最著名的问题可能是<a class="ae lf" href="https://nlp.stanford.edu/projects/coref.shtml" rel="noopener ugc nofollow" target="_blank">共指消解</a>、<em class="le">即</em>寻找文本中引用同一实体的所有表达式的任务。拿下面这句话来说:</p><blockquote class="lw lx ly"><p id="ea99" class="ki kj le kk b kl km ju kn ko kp jx kq lz ks kt ku ma kw kx ky mb la lb lc ld im bi translated">"我让一个铁球滚下斜坡，当它碰到一扇玻璃窗时，就碎了。"</p></blockquote><p id="10e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">任何人都可以看出，第一个代词<em class="le"> it </em>指的是<em class="le">球</em>，而第二个代词指的是<em class="le">玻璃</em>，反之亦然。从语法上来说，这两种选择都是完全正确的，但是物理世界的知识和一些直觉引导我们做出正确的匹配。这对机器来说绝非小事。当前的范例需要在大量文本(称为<em class="le">语料库</em>)上训练大型神经网络模型，从这些文本中估计单词之间关联的概率(<em class="le"> n </em> -grams)。一旦模型被训练，它随后被<em class="le">微调</em>到某个任务，<em class="le">例如</em> <a class="ae lf" rel="noopener" target="_blank" href="/building-a-question-answering-system-part-1-9388aadff507">问答</a>，<a class="ae lf" href="https://talktotransformer.com/" rel="noopener ugc nofollow" target="_blank">语言生成</a>，<a class="ae lf" rel="noopener" target="_blank" href="/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da">命名实体识别</a>。在下文中，我们将关注一个相对简单的任务:文档分类。我们将训练一台机器根据摘要来识别给定科学论文的类别。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="c788" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">文本嵌入</h2><p id="bec1" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">虽然用计算机可以处理的数字对图像进行编码非常简单(<em class="le">即</em>通过将其像素表示为矩阵)，但对文本来说却不是这样。其实不同的可能性都是存在的，最好的大多取决于任务本身。最基本的方法包括创建一个字典，<em class="le">，即</em>语料库中出现的所有<em class="le"/>单词的索引集合。然后，给定的文本由一个向量表示，其中每个元素都是相应单词的索引。</p><pre class="lh li lj lk gt nh ni nj nk aw nl bi"><span id="04b5" class="mj mk it ni b gy nm nn l no np">dictionary = ["apple", "banana", "zucchini"]</span><span id="145b" class="mj mk it ni b gy nq nn l no np">"banana banana apple" -&gt; [1, 1, 0]<br/>"zucchini apple apple" -&gt; [2, 1, 1]</span></pre><p id="e0de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种方法的问题是向量的长度是可变的(事实上相当于文本中的字数)，这对于数字处理来说不是一个理想的特性。另一种常见的选择是将文本表示为矩阵，其中每行代表一个单词。反过来，每个单词由一个固定长度的二进制向量表示，其中除了唯一指向字典中相应单词的一个元素之外，所有元素都为零，例如:</p><pre class="lh li lj lk gt nh ni nj nk aw nl bi"><span id="b767" class="mj mk it ni b gy nm nn l no np">[1,0,0] -&gt; "apple"<br/>[0,1,0] -&gt; "banana"<br/>[0,0,1] -&gt; "zucchini"</span></pre><p id="7df1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种方法叫做<em class="le">单热向量</em>，也不是特别适合，因为它需要大量内存，这基本上是浪费了，因为大多数元素都是零。这个问题的常见解决方案是使用<a class="ae lf" href="https://en.wikipedia.org/wiki/Sparse_matrix" rel="noopener ugc nofollow" target="_blank">稀疏矩阵</a>来代替。</p><p id="54c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">近年来，人们已经意识到，更好的解决方案是将每个单词表示为固定长度的<em class="le">密集</em>向量，例如:</p><pre class="lh li lj lk gt nh ni nj nk aw nl bi"><span id="4125" class="mj mk it ni b gy nm nn l no np">[0.1, -0.1] -&gt; "apple"<br/>[0.3, 0.1] -&gt; "banana"<br/>[-0.2, 0.1] -&gt; "zucchini"</span></pre><p id="45f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最明显的优势是编码每个单词需要更少的维度。字典基本上变成了单词向量的<em class="le">查找表</em>。然而，真正的交易是将语义意义<em class="le">附加到嵌入空间的每个维度的能力。最辉煌的例子如下:给定四个代表<em class="le">国王</em>、<em class="le">王后</em>、<em class="le">男性</em>、<em class="le">女性</em>的向量，人们发现:</em></p><pre class="lh li lj lk gt nh ni nj nk aw nl bi"><span id="3551" class="mj mk it ni b gy nm nn l no np">king - male + female = queen</span></pre><p id="86c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者类似地:</p><pre class="lh li lj lk gt nh ni nj nk aw nl bi"><span id="4741" class="mj mk it ni b gy nm nn l no np">france - paris + rome = italy</span></pre><p id="e2e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是如何实现的超出了本文的范围，但足以说明的是，一个神经网络(<a class="ae lf" href="https://becominghuman.ai/how-does-word2vecs-skip-gram-work-f92e0525def4" rel="noopener ugc nofollow" target="_blank"> word2vec或skip-gram </a>模型)是在一个语料库上训练的，以预测最可能的上下文，<em class="le">即</em>给定单词之后的单词，或者两个单词更有可能在给定单词之前和之后找到。</p><p id="8403" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在最近，这种和其他类似的技巧已经被用来通过爬行数百万网页(包括维基百科的完整转储)来训练巨大的网络，如<a class="ae lf" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank">伯特</a>和<a class="ae lf" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>。在这一点上，语言处理网络正在做着与计算机视觉网络相同的事情:如果不能访问庞大的样本数据库，它们的训练就不可能进行，而这反过来只有在互联网出现后才成为可能。</p><p id="59c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，对我们的目标来说最重要的是，一个类似的网络已经通过不仅考虑单个单词，而且考虑句子和短文段而得到训练。我们将部署由一个名为<a class="ae lf" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">通用句子编码器</a> (USE)的预训练网络创建的嵌入向量，用数字表示文档，并应用一些简单的分类技术。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="b7a6" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">文件分类:微调神经网络</h2><p id="956a" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">有了句子嵌入，我们现在可以把注意力转向实际的分类任务。对于这个例子，我们将通过下载arXiv服务器上出现的预印本摘要来创建一个用于训练/测试的小型数据库。关于如何下载这些文件的很好的介绍可以在<a class="ae lf" href="https://betatim.github.io/posts/analysing-the-arxiv/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="1a06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将摘要保存在数据帧中后，我们可以稍后加载该文件，并使用带标签的信息来训练一个简单的网络。</p><p id="1b46" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意代码的以下部分:</p><ul class=""><li id="d576" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">输入由作为文本存储的摘要序列组成。然后，输入被传递到第一层(UniversalEmbedding ),该层本质上是预训练的深度网络。其输出被传递到一个密集层，激活<em class="le"> softmax </em>来预测类别</li><li id="da61" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">为了支持多个类别，输出是一个向量，其中每个元素对应一个给定的类。激活<em class="le"> softmax </em>使输出标准化，以便可以根据输入属于给定类别的概率进行解释。为了一致性，损失函数是<em class="le">分类交叉熵</em>。</li></ul><pre class="lh li lj lk gt nh ni nj nk aw nl bi"><span id="3ecd" class="mj mk it ni b gy nm nn l no np">There are 2 known categories: ['cs.AI', 'astro-ph']<br/>Training set has 14550 samples<br/>Testing set has 4850 samples</span><span id="4292" class="mj mk it ni b gy nq nn l no np">Model: "AbstractClassifier"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #<br/>=================================================================<br/>text_in (InputLayer)         [(None, 1)]               0<br/>_________________________________________________________________<br/>lambda (Lambda)              (None, 512)               0<br/>_________________________________________________________________<br/>dense (Dense)                (None, 16)                8208<br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 2)                 34<br/>=================================================================</span><span id="48ba" class="mj mk it ni b gy nq nn l no np">Total params: 8,242<br/>Trainable params: 8,242<br/>Non-trainable params: 0<br/>_________________________________________________________________</span><span id="c604" class="mj mk it ni b gy nq nn l no np">Training...<br/>Train on 11640 samples, validate on 2910 samples<br/>.<br/>.<br/>.<br/>Epoch 19/20<br/>11640/11640 [==============================] - 11s 952us/sample - loss: 0.0143 - acc: 0.9962 - val_loss: 0.0255 - val_acc: 0.9897<br/>Epoch 20/20<br/>11640/11640 [==============================] - 11s 978us/sample - loss: 0.0139 - acc: 0.9964 - val_loss: 0.0254 - val_acc: 0.9911</span><span id="2246" class="mj mk it ni b gy nq nn l no np">Done training<br/>Testing...<br/>4850/1 - 4s - loss: 0.0097 - acc: 0.9938</span></pre><p id="30a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管相对简单，该代码能够区分astro-ph和cs之间的论文。具有99%准确度和非常少的过度训练的AI类别(验证损失稳定在比训练损失0.0139更高的值0.0254，训练损失0.0139持续下降更长时间)。事实上，这一壮举是可能的，由于使用嵌入做了繁重的工作。你刚刚发现了<em class="le">迁移学习</em>的乐趣:一旦一个模型已经在一个大型数据库上预先训练好，你所要做的就是<em class="le">根据你的目的对</em>进行微调！</p><h2 id="1540" class="mj mk it bd ml mm mn dn mo mp mq dp mr kr ms mt mu kv mv mw mx kz my mz na nb bi translated">信息太多！</h2><p id="b973" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">这些单词或句子嵌入的一个方面是嵌入空间的绝对大小，这里是512维，这对于实际应用来说是相当麻烦的。最有可能出现的问题是缺乏大型数据库，甚至无法执行微调步骤—将嵌入层连接到具有64个输出节点的密集层将需要拟合512*64+64 = 32，832个参数。根据经验，应该有至少10倍以上的训练示例，<em class="le">，即</em>大约300k，这对于许多应用来说可能很大。</p><p id="e58f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">人们很容易理解，并不是所有的维度都携带有用的信息来完成手头的任务。一种在保留信息的同时减小嵌入空间大小的简单而有效的方法是应用<a class="ae lf" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析</a> (PCA)。想法是查看所有嵌入向量的变化，并丢弃存在大变化的维度(代表噪声)，同时保留具有更强关联的维度(主分量)。</p><p id="bb1a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于实际上不可能将512维中发生的事情可视化，一个好的方法是计算这些向量之间以及不同类别之间的余弦相似性(<em class="le">即</em>归一化点积)。这种操作背后的思想是，属于相似类别的向量应该彼此靠近，因此具有小角度，而属于不同类别的向量应该远离，因此具有大角度。余弦运算将角度压缩到0到1之间，使比较更容易。如果PCA正在做它的工作，人们应该期望在由于PCA的维数减少之后不同类别之间的分离有所改善，<em class="le">，即</em>通过丢弃噪声应该出现“信号”。</p><p id="fea7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这正是下面的图所显示的。蓝色直方图显示了使用完整的512嵌入空间的余弦相似性的分布，而红色直方图表示PCA后的相同分布。在对角线上，属于同一类的文档应该有一个非常接近1的余弦，但似乎不是这样(它们大多分布在0.5左右)。此外，通过查看不同类别之间的乘积(非对角线直方图)，可以看到很少的分离，<em class="le">即</em>一些直方图峰值低于0.5，但效果并不显著。如果我们使用完整的嵌入空间，对文档进行分类的希望很小！</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi oh"><img src="../Images/8a1be7a7c01a2839e912010cc1a53762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yW6FFiPbPvDrSTwLit5QDg.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">PCA前后文档嵌入的余弦相似性。</p></figure><p id="4144" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，在PCA(红色直方图)之后，可以看到一些非常有趣的特征。首先，对角线上的直方图被推向1，这是一个好现象:同一类别中的论文由彼此接近的向量表示！此外，现在查看非对角线直方图，很明显直方图被推向0，<em class="le">，即</em>，类别之间的相似性相当低。最后，有趣的是，即使在PCA之后，两个类别仍然难以区分:hep-ex(实验高能物理)和astro-ph(天体物理)。这令人惊讶吗？数据基本上是在告诉我们，这两个科学分支比人工智能(cs)等其他分支有更强的相关性。AI)和数论(数学。NT)？</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="7d1c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总而言之，正如许多<a class="ae lf" href="https://medium.com/@thresholdvc/neurips-2019-entering-the-golden-age-of-nlp-c8f8e4116f9d" rel="noopener">人在温哥华NeurIPS2019大会上所争论的</a>，自然语言处理(NLP)领域正在进入一个黄金时代。NLP基本上正在经历与2010年计算机视觉相同的革命。迁移学习和预训练模型基本上为任何人提供了一个机会，为特定任务(如文档分类)微调简单模型。</p><p id="bfbe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">保持(微调)调谐！</p></div></div>    
</body>
</html>