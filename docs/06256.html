<html>
<head>
<title>Day 140 of #NLP365: NLP Papers Summary — Multimodal Machine Learning for Automated ICD Coding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365的第140天:NLP论文摘要——自动ICD编码的多模态机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-140-of-nlp365-nlp-papers-summary-multimodal-machine-learning-for-automated-icd-coding-b32e02997ea2?source=collection_archive---------63-----------------------#2020-05-19">https://towardsdatascience.com/day-140-of-nlp365-nlp-papers-summary-multimodal-machine-learning-for-automated-icd-coding-b32e02997ea2?source=collection_archive---------63-----------------------#2020-05-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="6263" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="0793" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">NLP论文摘要是我总结NLP研究论文要点的系列文章</h2></div><p id="160e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我在2020年每天记录我的NLP学习旅程的地方。在这里，你可以随意查看我在过去的273天里学到了什么。在本文的最后，你可以找到以前的论文摘要，按自然语言处理领域分类:)</p><p id="eab5" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的NLP论文是<strong class="lf jp"> <em class="lz">自动ICD编码的多模态机器学习</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="2ff9" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="a450" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">提出了一种新的预测ICD-10编码的多模态机器学习模型。这个模型是一个集合模型，它结合了三种不同的ML模型，这三种模型是为处理三种不同的数据类型而开发的:非结构化、半结构化和结构化数据。我们的模型优于所有的基线模型，并且具有离医生不远的高解释水平。</p><h1 id="a7b1" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">临床ICD景观</h1><p id="7624" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">ICD是诊断和程序代码的医学分类列表。这些代码广泛用于诊断信息的报销、存储和检索。分配ICD代码的过程非常耗时，因为临床编码员需要从电子病历(EMR)中提取关键信息并分配正确的代码。编码错误很常见，而且代价高昂。EMR通常以三种不同的形式存储数据:</p><ol class=""><li id="bbfb" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly no np nq nr bi translated"><em class="lz">非结构化文本</em>。护理记录、实验室报告、测试报告和出院总结</li><li id="7c72" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">半结构化文本</em>。描述医生所写诊断的结构化短语列表</li><li id="3421" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">结构化表格数据</em>。包含处方和临床测量，如数值测试结果</li></ol><h1 id="defb" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">资料组</h1><p id="cdf3" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">评估数据集是重症监护III (MIMIC-III)的医疗信息集市。总共有44，659人被录取。诊断代码从原来的ICD-9映射到ICD-10(一对一)。该数据集涵盖了32个ICD代码，它们是MIMIC-III和美国一家国立医院中的前50个频率。共有6张桌子:</p><ol class=""><li id="11f9" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly no np nq nr bi translated"><em class="lz">录取</em>。病人入院的所有信息</li><li id="e5f0" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">标签</em>。所有实验室测量</li><li id="1b8b" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">药方</em>。与订单条目相关的药物</li><li id="1f4d" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">微生物学</em>。微生物信息</li><li id="5440" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">图表事件</em>患者常规体征和其他相关健康信息的所有图表数据</li><li id="bea4" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">注意事件</em>。所有记录，包括护理和医生记录、出院总结和超声心动图报告</li></ol><h1 id="5759" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">方法学</h1><figure class="ny nz oa ob gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi nx"><img src="../Images/da6e58a59827b270ec09a21bdf7a8f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xs-Qk8-3pJcvLHkh.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">整体架构的集成模型的三个不同类型的模型，涵盖不同类型的数据集[1]</p></figure><p id="f685" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">上图是我们基于集成的模型，它结合了以下三种不同的ML模型:</p><ol class=""><li id="dd7e" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly no np nq nr bi translated"><em class="lz"> Text-CNN </em>。用于非结构化文本的多标签分类</li><li id="56c5" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz"> Char-CNN + BiLSTM </em>。用于分析诊断描述和ICD代码描述之间的语义相似性</li><li id="18bd" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">决策树</em>。将结构化数字特征转换为二进制特征以分类ICD码</li></ol><p id="4af9" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在推理过程中，我们的模型结合了三个最大似然模型进行预测，并从原始数据中提取关键证据进行检验，以提高可解释性。</p><h2 id="f1d2" class="og mi jf bd mj oh oi dn mn oj ok dp mr lm ol om mt lq on oo mv lu op oq mx jl bi translated">文本-CNN</h2><p id="4b89" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">对于非结构化数据，我们有Noteevents。这包括两个步骤:</p><ol class=""><li id="32c3" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly no np nq nr bi translated"><em class="lz">数据预处理</em></li><li id="3413" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">正文-CNN分类</em></li></ol><p id="5e02" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于数据预处理，对步骤2的输入进行简单的清理和标准化。对于步骤2，我们使用Text-CNN进行多标签分类。我们还修改了Text-CNN来开发TEXT-TF-IDF-CNN，如下所示。该模型包括从非结构化指南中提取的关键字和短语的TFIDF向量，以模拟临床指南经常用于指导诊断的真实世界情况。额外的TFIDF输入馈入Text-CNN的全连接层。</p><figure class="ny nz oa ob gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi or"><img src="../Images/c1f55870c803094fbb6a8cdc7f7fe593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dqXgzwOTEL8kvUrd.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">非结构化数据的模型架构[1]</p></figure><p id="1474" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">数据集中存在类别不平衡，这可能会降低我们的ML模型的性能，因此我们决定使用标签平滑正则化(LSR)，这可以防止我们的分类器在训练期间对标签过于确定。</p><h2 id="7893" class="og mi jf bd mj oh oi dn mn oj ok dp mr lm ol om mt lq on oo mv lu op oq mx jl bi translated">Char-CNN + BiLSTM</h2><p id="ba32" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">临床编码人员经常试图提取临床记录中的关键短语和句子，并将其分配给适当的ICD编码。最常见的是，编码描述和诊断描述之间存在紧密的语义相似性。我们将此过程公式化为基于诊断的排序(DR)问题，其中所有代码描述都在低维密集向量空间中表示。在推理过程中，诊断描述被映射到相同的向量空间，并且基于诊断向量和每个编码向量之间的距离对ICD码进行排序。因此，我们决定采用如下所示的架构。</p><figure class="ny nz oa ob gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi os"><img src="../Images/54a1830755f50598342d81466ef1bc29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*J860bW9tzVBwvVHo.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">半结构化数据的模型架构[1]</p></figure><p id="700b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们使用字符级CNN和预训练的单词嵌入将诊断和ICD编码描述编码到同一个空间。单词嵌入是在PubMed上预先训练的，PubMed包含超过550，000篇生物医学论文。然后，编码的嵌入被馈送到biLSTM和max-pooling层，以生成最终的特征向量。</p><p id="3065" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">损失函数通过最小化诊断实例和阳性实例(阳性对)之间的距离以及最大化诊断实例和阴性实例(阴性对)之间的距离来捕捉实例之间的相对相似性。距离是用欧几里得度量的。MIMIC-III数据集没有ICD代码和诊断的一对一映射，因此我们在网上爬行以提取ICD-10代码的同义词。每个ICD电码的所有同义词都是正面例子。使用类似于代码描述的n-grams来创建反例。</p><h2 id="84dd" class="og mi jf bd mj oh oi dn mn oj ok dp mr lm ol om mt lq on oo mv lu op oq mx jl bi translated">决策图表</h2><p id="0bf1" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">表2-5都是表格数据。我们的方法是对表中的二进制特征应用决策树，并利用一对多策略进行多标签分类。为了消除班级不平衡，来自少数民族班级的样本被赋予更高的权重。</p><h2 id="2b02" class="og mi jf bd mj oh oi dn mn oj ok dp mr lm ol om mt lq on oo mv lu op oq mx jl bi translated">模型集成</h2><p id="f861" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">在推理期间，我们的集合模型采用从三个单独模型预测的概率的加权和来计算每个类别的最终预测概率。</p><h2 id="c04e" class="og mi jf bd mj oh oi dn mn oj ok dp mr lm ol om mt lq on oo mv lu op oq mx jl bi translated">可解释性方法</h2><p id="1bcc" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">为了识别导致预测的ICD码的关键短语，我们试图捕捉单词w和ICD码y之间的关联强度。我们通过从我们的神经网络中提取连接w和y的所有路径并计算影响分数来实现这一点。然后将所有路径的分数相加，以测量关联强度。为了捕捉关键短语，我们组合了具有非零分数的连续单词，并按照最高分数对它们进行排序。排名靠前的短语被认为是确定特定ICD码预测的重要信号。</p><p id="d77f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于每个表格特征，我们使用局部可解释的模型不可知解释(LIME)来计算该特征对模型最终预测的重要性。</p><h1 id="4808" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">实验设置和结果</h1><p id="e6a5" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们使用两个评估指标来衡量模型的分类性能和可解释性:</p><ol class=""><li id="7b45" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly no np nq nr bi translated"><em class="lz">分类</em>。F1和AUC来测量精度和召回率，并总结不同阈值下的性能</li><li id="f01f" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">可解释性</em>。Jaccard相似性系数(JSC)来衡量提取的证据和医生的注释之间的重叠</li></ol><h2 id="570d" class="og mi jf bd mj oh oi dn mn oj ok dp mr lm ol om mt lq on oo mv lu op oq mx jl bi translated">结果</h2><figure class="ny nz oa ob gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ot"><img src="../Images/347d0e23c88d009bb4ecfdd36f8ffbb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XQ4XQu1wxbHdXSuB.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">模拟数据集的总体结果[1]</p></figure><p id="9f48" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们使用TFIDF的不同变体作为基线。大多数模型都是基于CNN的。vanilla Text-CNN和DenseNet在F1得分方面与基线模型表现相似，在AUC得分方面优于基线模型。如F1和AUC分数的显著改善所示，标签平滑在缓解类别不平衡问题方面是有效的。基于诊断的分级显示了类似的改善。</p><p id="fe28" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">随着我们转向不同的系综模型，我们的F1和AUC得分表现持续增长。Text-CNN + LS + DR + TD显示，与普通Text-CNN相比，macro-F1得分提高了7%,其他指标也有类似的提高。这展示了我们的集成方法的有效性。</p><p id="1fc0" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在最后一节中，我们展示了通过以TFIDF特征向量的形式合并临床指南，在性能方面的进一步强大改进。这表明在没有外部临床指南的情况下，宏观F1评分比表现最好的集合模型增加了7%。我们的Text-TFIDF-CNN + LS + DR + TD优于所有基线和集合模型，这告诉我们，将外部知识结合到分类任务中会显著提高模型的性能。</p><figure class="ny nz oa ob gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ou"><img src="../Images/79d6134fb1eaf62e09003c23c25bcf86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DS2rx8Xt6M5jojO4.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">AUC、F1和Jaccard相似性得分[1]</p></figure><p id="205e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在可解释性评估方面，我们从5个ICD-10编码中收集了25个样本的测试集，并由3名有经验的医生对它们进行了注释。我们将从我们的模型中提取的top-k短语与人类注释进行比较，并测量它们之间的重叠分数。结果如上表2所示。平均而言，我们的模型对于文本数据获得了0.1806的JSC。我们的模型能够捕捉与特定疾病直接相关的短语，或者为最终预测提供间接关系。</p><p id="ae36" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在表格数据方面，我们选择了LIME发现的k个最重要的特征作为模型预测的证据。同样，我们计算了这些特征和人工注释之间的重叠分数，结果显示在表2中。人类注释者之间的平均JSC是0.5，高于我们的模型和人类注释者之间的平均JSC 0.31。总的来说，我们的模型能够捕获比人类注释者更多的特征，其中一些特征对于诊断是有用的，而人类注释者没有发现。</p><h1 id="7ef7" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">结论和未来工作</h1><p id="d3a1" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">总的来说，我们的集成模型优于所有的基线方法，我们看到通过将人类知识融入模型中，性能得到了进一步的提高。此外，我们的模型的预测更容易解释。潜在的未来工作包括扩大编码列表，减少表格数据的特征维数，以及进一步研究添加人类知识的不同方法可以产生更好的结果。</p><h2 id="fb2f" class="og mi jf bd mj oh oi dn mn oj ok dp mr lm ol om mt lq on oo mv lu op oq mx jl bi translated">来源:</h2><p id="15f6" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">[1] Xu，k .，Lam，m .，Pang，j .，Gao，x .，Band，c .，Mathur，p .，Papay，f .，Khanna，A.K .，Cywinski，J.B .，Maheshwari，k .和Xie，p .，2019年10月。自动ICD编码的多模态机器学习。在<em class="lz">医疗保健机器学习会议</em>(第197-215页)。PMLR。</p><p id="8d5f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于2020年5月19日</em><a class="ae jc" href="https://ryanong.co.uk/2020/05/19/day-140-nlp-papers-summary-multimodal-machine-learning-for-automated-icd-coding/" rel="noopener ugc nofollow" target="_blank"><em class="lz"/></a><em class="lz">。</em></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="20f5" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="e370" class="nj nk jf lf b lg mz lj na lm ov lq ow lu ox ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="a6b3" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sense-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="8184" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitive-analysis-f 24 a2 EC 1 ca 32</a></li><li id="2d53" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="7d5d" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towards data science . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li><li id="e13e" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towards data science . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li><li id="a9ae" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5">https://towards data science . com/day-112-of-NLP 365-NLP-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b 7 a5 e 245 b5</a></li><li id="e2a9" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-123-of-nlp365-nlp-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f998d1131">https://towards data science . com/day-123-of-NLP 365-NLP-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f 998d 1131</a></li></ul><h1 id="51b1" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">总结</h1><ul class=""><li id="94a0" class="nj nk jf lf b lg mz lj na lm ov lq ow lu ox ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="bfbe" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1b 27</a></li><li id="b2dc" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-113-of-nlp365-nlp-papers-summary-on-extractive-and-abstractive-neural-document-87168b7e90bc">https://towards data science . com/day-113-of-NLP 365-NLP-papers-summary-on-extractive-and-abstract-neural-document-87168 b 7 e 90 BC</a></li><li id="e5c4" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-116-of-nlp365-nlp-papers-summary-data-driven-summarization-of-scientific-articles-3fba016c733b">https://towards data science . com/day-116-of-NLP 365-NLP-papers-summary-data-driven-summary-of-scientific-articles-3 FBA 016 c 733 b</a></li><li id="0696" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-117-of-nlp365-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge-61ae6cdf32f">https://towards data science . com/day-117-of-NLP 365-NLP-papers-summary-abstract-text-summary-a-low-resource-challenge-61a E6 CDF 32 f</a></li><li id="36f9" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-118-of-nlp365-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-aea118a5eb3f">https://towards data science . com/day-118-of-NLP 365-NLP-papers-summary-extractive-summary-of-long-documents-by-combining-AEA 118 a5 eb3f</a></li><li id="8711" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-120-of-nlp365-nlp-papers-summary-a-simple-theoretical-model-of-importance-for-summarization-843ddbbcb9b">https://towards data science . com/day-120-of-NLP 365-NLP-papers-summary-a-simple-theory-model-of-importance-for-summary-843 ddbcb 9b</a></li><li id="40bc" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-121-of-nlp365-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization-cd55e577f6de">https://towards data science . com/day-121-of-NLP 365-NLP-papers-summary-concept-pointer-network-for-abstract-summary-cd55e 577 F6 de</a></li><li id="1912" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-124-nlp-papers-summary-tldr-extreme-summarization-of-scientific-documents-106cd915f9a3">https://towards data science . com/day-124-NLP-papers-summary-tldr-extreme-summary-of-scientific-documents-106 CD 915 F9 a 3</a></li></ul><h1 id="d560" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">其他人</h1><ul class=""><li id="4227" class="nj nk jf lf b lg mz lj na lm ov lq ow lu ox ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li><li id="ea48" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-111-of-nlp365-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection-bff7f5f20ce5">https://towards data science . com/day-111-of-NLP 365-NLP-papers-summary-the-risk-of-race-of-bias-in-hate-speech-detection-BFF 7 F5 f 20 ce 5</a></li><li id="02a5" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-115-of-nlp365-nlp-papers-summary-scibert-a-pretrained-language-model-for-scientific-text-185785598e33">https://towards data science . com/day-115-of-NLP 365-NLP-papers-summary-scibert-a-pre trained-language-model-for-scientific-text-185785598 e33</a></li><li id="ab66" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-119-nlp-papers-summary-an-argument-annotated-corpus-of-scientific-publications-d7b9e2ea1097">https://towards data science . com/day-119-NLP-papers-summary-an-argument-annoted-corpus-of-scientific-publications-d 7 b 9 e 2e ea 1097</a></li><li id="a90e" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-122-of-nlp365-nlp-papers-summary-applying-bert-to-document-retrieval-with-birch-766eaeac17ab">https://towards data science . com/day-122-of-NLP 365-NLP-papers-summary-applying-Bert-to-document-retrieval-with-birch-766 EAC 17 ab</a></li><li id="3c06" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-125-of-nlp365-nlp-papers-summary-a2n-attending-to-neighbors-for-knowledge-graph-inference-87305c3aebe2">https://towards data science . com/day-125-of-NLP 365-NLP-papers-summary-a2n-attending-to-neighbors-for-knowledge-graph-inference-87305 C3 aebe 2</a></li><li id="c2b5" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly oy np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-126-of-nlp365-nlp-papers-summary-neural-news-recommendation-with-topic-aware-news-4eb9604330bb">https://towards data science . com/day-126-of-NLP 365-NLP-papers-summary-neural-news-recommendation-with-topic-aware-news-4eb 9604330 bb</a></li></ul></div></div>    
</body>
</html>