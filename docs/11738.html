<html>
<head>
<title>Fine Tuning XGBoost model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调 XGBoost 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-xgboost-model-257868cf4187?source=collection_archive---------12-----------------------#2020-08-14">https://towardsdatascience.com/fine-tuning-xgboost-model-257868cf4187?source=collection_archive---------12-----------------------#2020-08-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b537" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让你的模型更好的基本要素</h2></div><p id="19f9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">调整模型是增强模型性能的方法。让我们看一个例子，在这个例子中，根据 RMSE 分数，对未调优的 XGBoost 模型和调优的 XGBoost 模型进行了比较。稍后，您将了解 XGBoost 中对超参数的描述。</p><p id="bcd5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是 XGBoost 模型中未调整参数的代码示例:</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="lg lh l"/></div></figure><p id="0013" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">输出:34624.229980 </strong></p><p id="5781" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们看看当参数调整到一定程度时 RMSE 的值:</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="lg lh l"/></div></figure><p id="313f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">产量:29812.683594 </strong></p><p id="aeb3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以看出，当调整参数时，RMSE 分数降低了大约 15%。</p><h1 id="9618" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated">XGBoost 超参数</h1><p id="b9a2" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">对于 XGBoost 的每个基础学习者，都有不同的参数可以调整，以提高模型性能。</p><h2 id="6120" class="mf lj iq bd lk mg mh dn lo mi mj dp ls ko mk ml lu ks mm mn lw kw mo mp ly mq bi translated">通用树可调参数</h2><ul class=""><li id="fb76" class="mr ms iq kh b ki ma kl mb ko mt ks mu kw mv la mw mx my mz bi translated"><strong class="kh ir">学习率:</strong>使用额外的基本学习器会影响模型拟合残差的速度。低学习率将需要更多的提升回合来实现与具有高学习率的 XGBoost 模型相同的残差减少。它用 eta 表示。</li><li id="60c7" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated"><strong class="kh ir"> gamma: </strong>这是创建新树分裂的最小损失减少。为了使算法更加保守，最好使用高 gamma 值。</li><li id="5a03" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated"><strong class="kh ir"> lambda: </strong>这负责对叶权重进行 L2 正则化。</li><li id="a229" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated"><strong class="kh ir"> alpha: </strong>这负责叶子权重的 L1 正则化。</li><li id="4937" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated"><strong class="kh ir"> max_depth: </strong>它是一个正整数值，决定了每棵树在任何一轮提升中的生长深度。</li><li id="8dda" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated"><strong class="kh ir">子样本:</strong>范围从 0 到 1，是可用于任何给定增强回合的总训练集的分数。该参数的低值可能导致不匹配问题，而高值可能导致过度匹配。</li><li id="aeb5" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated"><strong class="kh ir"> colsample_bytree: </strong>该参数的取值范围也是从 0 到 1。它是在任何给定的提升回合期间可以选择的特征的一部分。</li></ul><h2 id="b6b4" class="mf lj iq bd lk mg mh dn lo mi mj dp ls ko mk ml lu ks mm mn lw kw mo mp ly mq bi translated">线性可调参数</h2><ul class=""><li id="9933" class="mr ms iq kh b ki ma kl mb ko mt ks mu kw mv la mw mx my mz bi translated"><strong class="kh ir"> lambda: </strong>这负责对叶权重进行 L2 正则化。</li><li id="be5c" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated"><strong class="kh ir">阿尔法:</strong>这负责对叶权重进行 L1 正则化。</li><li id="60b9" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated"><strong class="kh ir"> lambda_bias: </strong>可以应用于模型偏差的 L2 正则化项。</li></ul><p id="4ebb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是一些调整示例:</p><h2 id="ac77" class="mf lj iq bd lk mg mh dn lo mi mj dp ls ko mk ml lu ks mm mn lw kw mo mp ly mq bi translated"><strong class="ak">调谐 ETA </strong></h2><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="lg lh l"/></div></figure><p id="3bf1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">输出:</strong></p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8c32ea0b67a0b7d63b1f725470bcc654.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*gcF08fjvcBk8OAEHP7k_xQ.png"/></div></figure><p id="e142" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">可以看出，eta 值的增加给出了更好的模型。</strong></p><p id="5878" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">调整最大深度</strong></p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="lg lh l"/></div></figure><p id="7992" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">输出:</strong></p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/6224370004d7ce0ad53db50f8f5c0bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*6tklVlTbJdAKG1U8meb-iQ.png"/></div></figure><p id="635d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">可以看出，增加树深度的值给出了更好的模型。</strong></p><p id="e9e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">只有当超参数的值是最优的时，模型才会给出更好的性能。所以，问题是如何找到最优值以获得尽可能低的损失？</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nj"><img src="../Images/a571fcf9fa81e2aa30142d3e7033786e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Pbfe-i_uMUhodBWN"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated">艾米丽·莫特在<a class="ae ns" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="96a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同时选择几个超参数的两种常用策略是<strong class="kh ir">网格搜索</strong>和<strong class="kh ir">随机搜索</strong>。</p><p id="9688" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，我们来看看两者如何在 XGBoost 中使用？</p><h2 id="9b97" class="mf lj iq bd lk mg mh dn lo mi mj dp ls ko mk ml lu ks mm mn lw kw mo mp ly mq bi translated">网格搜索</h2><p id="6201" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">这是一种彻底搜索可能的参数值集合的方法。例如，如果您必须调整 3 个超参数，并且每个超参数有 4 个可能的值，那么在该参数空间上的网格搜索将尝试所有 64 个配置，并挑选为所使用的指标提供最佳值的配置(这里我们使用均方根误差)。让我们看一个例子。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="lg lh l"/></div></figure><p id="5b94" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出:最佳参数:{ '学习率':0.1，' n 个估计量':200，'子样本':0.5}最低 RMSE:2000。10001.686868686107</p><h2 id="10d5" class="mf lj iq bd lk mg mh dn lo mi mj dp ls ko mk ml lu ks mm mn lw kw mo mp ly mq bi translated">随机搜索</h2><p id="96f1" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">它与网格搜索有些不同。它为您想要搜索的每个超参数创建了一个超参数值范围。它还设置了继续随机搜索的迭代次数。在每次迭代中，在每个超参数的指定值范围内随机抽取一个值，用这些超参数搜索并训练/测试一个模型。在达到最大迭代次数后，它选择具有最佳分数的最佳超参数。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="lg lh l"/></div></figure><p id="377f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">输出:</strong>最佳参数:{ ' learning _ rate ':2.000000000000001，' n_estimators' : 200，'子样本':6.0000000000000000005 }最低 RMSE:2 . 2000005</p><p id="7162" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">eta best _ RMSE<br/>0 0.001 195736.406250<br/>1 0.010 179932.192708<br/>2 0.100 79797</p><h2 id="8d21" class="mf lj iq bd lk mg mh dn lo mi mj dp ls ko mk ml lu ks mm mn lw kw mo mp ly mq bi translated">结论</h2><p id="894b" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">现在，您知道了调优的含义以及它如何帮助提升模型。本文讨论了调整超参数 eta 和 max-depth，但是也可以将其他超参数调整到最佳值，从而为您的模型提供更好的性能，并且可以在网格搜索和随机搜索的帮助下选择最佳值。</p><p id="d9e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您想了解更多关于超参数调优的一般信息，请考虑以下链接:<a class="ae ns" href="https://neptune.ai/blog/hyperparameter-tuning-in-python-a-complete-guide-2020" rel="noopener ugc nofollow" target="_blank">https://Neptune . ai/blog/hyperparameter-tuning-in-python-a-complete-guide-2020</a></p></div></div>    
</body>
</html>