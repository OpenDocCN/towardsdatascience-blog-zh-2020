# 在数据科学中使用内存访问

> 原文：<https://towardsdatascience.com/using-in-memory-access-in-data-science-96130a5bcc33?source=collection_archive---------38----------------------->

## 基于磁盘的数据库和内存访问之间的简单性能比较

![](img/88f603946ce1002f7efdf0ae73ceac60.png)

图片来源:vectorgraphit.com。知识共享署名许可证。

在金融市场数据分析中，要分析的数据量和吞吐量可能令人望而生畏。这绝不是金融界特有的，因为它也发生在许多其他数据分析领域。这个特定行业的独特之处在于数据是高度结构化的(这在其他领域并不常见)。构成金融市场数据的是大量大小不一且大多互不相关的数据消息:最终很容易收到、存储、解码、解析和关联数亿条小消息。

分析如此大量的数据需要许多次迭代，并且在随机数据访问中节省时间的需要变得相关。

# 数据库与内存

在数据科学和数据工程中，我们面临许多次如前所述的处理数据集的需要。这可能是因为我们想要以大吞吐量注入数据——模拟环境——或者因为我们只是需要迭代大型数据集以进行统计分析或蒙特卡罗模拟——并非数据科学中的一切都可以简化为 ML 模型。

当存在这样的需求时，第一反应是使用数据库(可以是任何数据库)中的数据，并使用查询/访问策略来提取和使用数据。有时，这种方法根本无法避免，因为数据集太大，即使拆分也无法放入内存，或者因为使用数据的预期方式复杂多样。

但是在许多其他情况下，数据可以成功地简单地存储在 RAM 中，以便以后立即分配，或者在大块数据中，使用原始 I/O 直接从磁盘传输到内存。

后一种方法尽管简单，但通常不被采用(通过将数据访问减少到仅仅从磁盘进行内存复制，然后通过数组进行内存请求，从而减少了一层复杂性)。它肯定不适合所有的场景，但是它非常适合模拟、分析和回溯测试，正如在定量研究和分析中发现的那样。数据通常可以按年份或任何其他标准进行划分。

拆分可能被视为一个问题，但根据我的经验，它实际上简化了数据处理管道的后续阶段，即使使用数据库，也经常需要拆分。

在内存中存储要分配的数据的方式和将数据存储为可以查询的存储库的方式存在差异。如果您计划将所有内容都移入 RAM，那么理想情况下，您会希望存储一个将在编程语言中使用的结构的*字节内存*副本，而如果您计划使用数据库或存储库，那么它将是底层解决方案，即规定如何准备和存储数据的解决方案。

虽然您会发现许多人认为第一种方法违反了一些最佳实践(数据的字节顺序会引起可移植性问题，编译器不保证语言结构的一致性，等等)，但事实是它在受控环境中工作得非常好，在受控环境中，您可以定义平台、操作系统、编译器和分析的源代码。

这些反对意见/建议可能对打算发布的商业软件有效，或者可能是从理论角度进行讨论的基础，但是在现实生活的研究和分析任务中，它们的有效性可能会受到质疑，在这种情况下，平台和环境都是及时受控和稳定的。

此外，我们已经到达了一个点，在平台和编译器架构方面没有太多的选择，所以关于字节序和编译器差异的讨论更多的是学术而不是实践。对我来说，这些是 80 年代和 90 年代的回忆，那时我们在平台方面更加多样化(SPARC、PowerPC、英特尔、Alpha)。这同样适用于许多不同 UNIX 供应商的操作系统。这种多样性很久以前就消失了。

# RAM 比基于磁盘的数据库快多少？

为了回答这个问题，我们将测试两个场景。一个将模拟使用内存映射文件访问基于磁盘的存储系统(这被认为是大文件的快速访问方法)，另一个将模拟针对内存阵列的直接内存访问。

在这两种情况下，文件和内存数组都填充了随机数据，我们在整个数据集(512Mb)上迭代 10 次，进行简单的计算(以避免编译器在其优化功能中跳过指令)，我们将测量每种方法花费的时间。

用于测试基于磁盘的存储访问与内存访问的源代码

# 结果

在低端 FreeBSD 机器上运行的模拟结果清楚地显示了 RAM 如何显著提高访问性能:

```
durationTestFile
20318
durationTestArray
7312
```

即使我们用操作(每个检索字节的累积和与位屏蔽操作)来偏置测试，当比较内存和磁盘时，我们仍然得到 3:1 的速度性能比。可能内存与磁盘的隔离效应比这个比率还要高得多。

任何数据库或存储解决方案的性能都不可能超过内存映射文件的性能，因此我们可以得出结论，在内存上迭代可以大大减少计算时间。这不会让任何人感到惊讶，但是你仍然会发现有人声称两者是平等的，或者干脆否认直接内存访问方法是可行的，这是一个错误的假设。

作为对这个非正式基准的合理反对，有人可能会说我们没有考虑初始化数组的时间和将数据放入数组的时间。这些都是合理的反对意见，但是在对数据进行大量后续迭代(可能会持续几十分钟、几小时甚至几天)的情况下，使用内存而不是磁盘仍然有很大的好处，因为*它将大大减少运行分析的时间*。

> 处理时间越长，使用的内存就越多。

这种方法没有被广泛使用，因为它通常意味着能够压缩数据并处理底层语言的实际内存表示，而且它远远超出了在 *JSON* 和 *Jupyter 笔记本中处理数据的舒适区。*它可能更适合使用 C/C++ Java 或 Go 的低延迟开发环境。

还值得一提的是，整个方法只对高度结构化的数据有效，金融市场也是如此。处理非结构化数据或明确呈现关系依赖的数据需要不同的方法。

*免责声明:此处表达的所有观点和信息均为我个人观点，并不代表我曾经、现在或将隶属或关联的任何实体的观点。*