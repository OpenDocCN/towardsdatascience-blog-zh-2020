<html>
<head>
<title>Lovecraft with Natural Language Processing — Part 2: Tokenisation and Word Counts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理的Lovecraft第2部分:标记化和字数统计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690?source=collection_archive---------32-----------------------#2020-06-09">https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690?source=collection_archive---------32-----------------------#2020-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cb11" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Python NLP库spaCy分析H. P. Lovecraft小说中的词汇用法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/26e558c3cd7126c6ace7b3cf3229fa78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45kVvPQKBGs4KHjHINWs3A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/LUM3N-1066559/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1535201" rel="noopener ugc nofollow" target="_blank"> LUM3N </a>来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1535201" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="cdee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我从自然语言处理的角度分析H. P. Lovecraft作品系列的第二篇博文。在<a class="ae ky" rel="noopener" target="_blank" href="/lovecraft-with-natural-language-processing-part-1-rule-based-sentiment-analysis-5727e774e524">的第一个帖子</a>中，我们通过分析故事的整体情绪来开始。我们不能再推迟了，我们必须讨论所有NLP分析的基础:标记化。我觉得把这个问题和一个特定的问题联系起来会很有趣，所以在这篇文章中，我们将找出洛夫克拉夫特在每个故事中使用最多的词，以及在他的文学作品中使用最多的词。</p><p id="d44f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">阅读《Lovecraft》时，你记忆最深的一件事是奇怪的语言。他倾向于使用很多负面的词，恐怖，疯狂，沮丧，尤其是用形容词:不可名状，不可描述，不可描述的恐怖无处不在，对吗？《局外人》(1921)中有这样一段话:</p><blockquote class="lv lw lx"><p id="d188" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">上帝知道它不属于这个世界——或者不再属于这个世界——然而令我恐惧的是，我在它被侵蚀和露出骨头的轮廓中看到了对人类形体的恶意的、令人憎恶的嘲弄；在它发霉腐烂的衣服里，有一种说不出的特质，让我更加不寒而栗。</p></blockquote><p id="f00c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这让洛夫克拉夫特尖叫。在阿卡姆档案馆(我下载文章的网站)，有一个单独的页面<a class="ae ky" href="https://arkhamarchivist.com/wordcount-lovecraft-favorite-words/" rel="noopener ugc nofollow" target="_blank">专门用于字数统计，人们提交建议，收集故事的人统计它们。我很好奇，想知道这个概念是否可以被“科学地”观察到。这些故事有我们想象的那么消极吗？用的最多的形容词是什么，是“恐怖”“未知”“古老”吗？动词是关于知识和/或发疯的吗？他有没有用过“女人”这个词？好吧，让我们来看看！</a></p><p id="c983" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将从一个快速的理论背景开始，在文本上应用一些实际的准备工作，看一看<a class="ae ky" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">空间</a>中的几个例子，然后最后继续字数统计。</p><h2 id="ef63" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">理论</h2><p id="2fb0" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">首先，让我们看一下我们将在这个项目中使用的几个概念。</p><ul class=""><li id="1cac" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">标记化</strong> </a> <strong class="lb iu"> : </strong>一种文档分割技术，将非结构化(文本)数据分割成小块数据，这些数据可被视为离散元素。在我们的分析中，单个的记号将会是单词，但是不一定是这样，记号可以是一个段落，一个句子，单词的一部分，甚至是字符。</li><li id="6b22" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"/></a>:大量文本的无序聚合表示，可以是文档、章节、段落、句子等。原始文本中的语法、标点和词序都被忽略，唯一保留的是唯一的单词和与之相关的数字。这个数字可以是单词在文本中出现的频率，也可以是二进制的0或1，简单地衡量单词是否在文本中。</li><li id="1f3e" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Stemming" rel="noopener ugc nofollow" target="_blank">这种形式可以是单词的词根，例如，当我们在一篇文章中计算单词“house”时，我们可能还想包括“houses”和“House”，如果它是大写的，因为它位于句子的开头。有许多不同的方法，我们将使用的一种是引理化，例如词干化，甚至采用文本的小写版本也是规范化。</a></li><li id="80b5" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Lemmatisation" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">引理满足</strong> </a>:寻找单词的<a class="ae ky" href="https://en.wikipedia.org/wiki/Lemma_(morphology)" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">引理</strong> </a>的过程。词条基本上是单词的字典形式。比如“houses”的引理是“house”，“better”变成“good”，“thought”变成“think”等等。</li><li id="5e46" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Stop_words" rel="noopener ugc nofollow" target="_blank">比如“一”、“为什么”、“做”等。对于某些NLP任务(比如我们的)，忽略这些单词是有意义的。</a></li></ul><h2 id="8306" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">文本准备</h2><p id="e255" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">当我第一次为这个项目做<a class="ae ky" rel="noopener" target="_blank" href="/pdf-text-extraction-in-python-5b6ab9e92dd">文本准备工作</a>时，我怀疑我将不得不返回并做一些额外的调整，这被证明是正确的——当我开始使用标记化时，我遇到了问题。比如，你会不会猜到这两句话:</p><ul class=""><li id="45b3" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">”<em class="ly">为什么不呢？</em></li><li id="5c48" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">"<em class="ly">为什么不呢？</em>”</li></ul><p id="e5b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">会有不同的结果吗？原因是<a class="ae ky" href="https://en.wikipedia.org/wiki/Apostrophe" rel="noopener ugc nofollow" target="_blank">撇号</a>。一般来说，你希望单词的“<em class="ly"> n't </em>”部分与碱基分开。于是“<em class="ly">不是</em>”就变成了“<em class="ly">是</em>”和“<em class="ly">不是</em>”。但是它并不适用于所有的撇号。连字符、破折号、引号也有类似的问题——你无法想象有多少不同的字符用于相同的目的！</p><p id="2844" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ly">(注:洛夫克拉夫特</em> <a class="ae ky" href="https://howtowritelike.com/2019/02/06/how-to-write-like-h-p-lovecraft/" rel="noopener ugc nofollow" target="_blank"> <em class="ly">几乎从不使用缩写</em> </a> <em class="ly">)因为他认为它们是缺乏适当教育的明显标志。然而，他有时会在对话中使用它们，以显示——也告诉——那个人有多没文化。不过，他确实用了很多破折号。)</em></p><p id="130d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以在我们进行调查之前，我们需要追捕这些奇怪的人物。我认为谈论一下这些步骤是很重要的，即使这可能看起来有点无聊，因为(以我的个人经验)许多NLP指南完全跳过它，只从一个干净的文本开始。在大型NLP库中可能有这样的方法，比如NLTK，这可能是一个Unicode解码问题，但是对我来说，手动调整文本似乎是最合理的解决方案。</p><p id="0ba6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我开始分析一个故事中的人物，邓威奇恐怖(1928)，假设保存在<code class="fe no np nq nr b">text</code>变量中。排除空白，并使其小写:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="ae12" class="mc md it nr b gy nw nx l ny nz">text = text.lower()<br/>text = text.replace('\n','')<br/>text = text.replace('\x0c','')<br/>text = text.replace(' ','')</span></pre><p id="a9a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，您可以创建一串独特的字符，如下所示:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="1f1b" class="mc md it nr b gy nw nx l ny nz">unique_characters = set(text)</span></pre><p id="8eef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想把它们打印成一串，你可以简单地加入集合</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="bd8a" class="mc md it nr b gy nw nx l ny nz">unique_characters_in_string = ''.join(set(text))</span></pre><p id="9798" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Dunwich的案例中，这些是独特的角色:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="2a7d" class="mc md it nr b gy nw nx l ny nz">'-2wdj0i”m8uüosczävèr‖l)q,4b76e’n—a1(:é.ftpk?x‘hg!;93―y5'</span></pre><p id="5cb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我创建了一个名为<code class="fe no np nq nr b">regular_characters</code>的字符串，它包括英文字母表中的字母、数字、标点符号、括号、普通引号和撇号。我们遍历它们，从唯一的字符中去掉规则的字符，所以我们剩下的是不规则的字符。</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="1aa9" class="mc md it nr b gy nw nx l ny nz">for character in regular_characters:<br/>    try:<br/>        unique_characters.remove(character)<br/>    except:<br/>        pass</span></pre><p id="fa66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些是留下来的字符:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="f95b" class="mc md it nr b gy nw nx l ny nz">'ä', 'è', 'é', 'ü', '—', '―', '‖', '‘', '’', '”'</span></pre><p id="237a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是的，</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="66a5" class="mc md it nr b gy nw nx l ny nz">'—' == '―'</span></pre><p id="080f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于某种原因是假的，在这一点上，我们已经排除了我认为是常规的连字符和破折号！引号也有类似的问题，“”就是其中之一。(那不是大写的I，那是两条竖线，在文中是引号。)非英语拉丁字母是可以的，它们在文本中的用法是合法的，以防你想知道:</p><ul class=""><li id="d9c6" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">“我！舒布-尼格拉特！”</li><li id="2e30" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">“巴黎Bibliothèque国家公园”</li><li id="e38b" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">埃:“vigenère's·特拉伊特·德·希夫雷斯”</li><li id="313e" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">ü:“klüber's氪空间”</li></ul><p id="f6af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我为所有的故事重复了这个过程，收集了所有我想修改的奇怪角色。我最终得到了这本替换词典:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="b3ae" class="mc md it nr b gy nw nx l ny nz">replace_dict = {}<br/>replace_dict['*'] = ''<br/>replace_dict['—'] = '–'<br/>replace_dict['―'] = ''<br/>replace_dict['‖'] = '\"'<br/>replace_dict['‗'] = ''<br/>replace_dict['”'] = '"'<br/>replace_dict['‘'] = '\''<br/>replace_dict['’'] = '\''</span></pre><p id="a3ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">遍历所有文本文件，应用替换:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="06db" class="mc md it nr b gy nw nx l ny nz">for replace_char in replace_dict: <br/>    text = raw_text.replace(\<br/>        replace_char,  replace_dict[replace_char])</span></pre><p id="649d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并将它们保存为新的TXT文件。它们应该正常工作。好了，现在说正经事。</p><h2 id="6ef0" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">使用SpaCy</h2><p id="8949" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们首先安装将要使用的NLP库:<a class="ae ky" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> spaCy </a>。</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="f718" class="mc md it nr b gy nw nx l ny nz">pip install spacy</span></pre><p id="17d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还需要单独安装一个我们将要使用的语言模型。有多种语言的<a class="ae ky" href="https://spacy.io/usage/models" rel="noopener ugc nofollow" target="_blank">模型可用</a>，英语有三种不同的模型。我发现的大多数指南都推荐从<code class="fe no np nq nr b">en_core_web_sm</code>开始，我认为这对于一个简单的项目来说是可行的，但是经过一些尝试和错误之后，我决定使用<code class="fe no np nq nr b">en_core_web_lg</code>。(稍后我们将看到一个例子。)</p><p id="0f6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要安装模型，请执行以下操作:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="ebd3" class="mc md it nr b gy nw nx l ny nz">python -m spacy download en_core_web_lg</span></pre><p id="f3a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，下载和构建大模型可能需要一些时间。</p><p id="fa61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以设置一个<code class="fe no np nq nr b">nlp</code>对象作为我们的NLP模型，并创建一个<code class="fe no np nq nr b">doc</code>对象作为字符串输入的处理版本:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="4eeb" class="mc md it nr b gy nw nx l ny nz">nlp = spacy.load(“en_core_web_lg”)</span><span id="3b64" class="mc md it nr b gy oa nx l ny nz">doc = nlp(“At midnight the doorbell rang, startling him fearfully.”)</span></pre><p id="fdf6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe no np nq nr b">doc</code>是一个<a class="ae ky" href="https://spacy.io/api/doc#_title" rel="noopener ugc nofollow" target="_blank"> Doc </a>对象，它基本上是一个令牌容器。因此，在这一点上，我们已经有我们的文本标记！Doc保留了原始文本中的所有信息，比如空白，所以您可以从中重建文本。</p><p id="5c24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Doc对象中的令牌是<a class="ae ky" href="https://spacy.io/api/token#_title" rel="noopener ugc nofollow" target="_blank">令牌</a>对象，有一长串您可以请求的属性，请参见这里的完整列表<a class="ae ky" href="https://spacy.io/api/token#attributes" rel="noopener ugc nofollow" target="_blank"/>。现在，我们将使用三个属性:</p><ul class=""><li id="932b" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated"><code class="fe no np nq nr b">text</code>:文本中出现的令牌的简单文本</li><li id="84f8" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated"><code class="fe no np nq nr b">lemma_</code>:令牌的引理</li><li id="a1b3" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated"><code class="fe no np nq nr b">pos_</code>:标记的<a class="ae ky" href="https://universaldependencies.org/docs/u/pos/" rel="noopener ugc nofollow" target="_blank">词性</a>(例如名词、形容词、动词等……)</li></ul><p id="9ac7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了分析我们的句子，我们可以列出标记属性:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="26d4" class="mc md it nr b gy nw nx l ny nz">for token in doc: <br/>    print(token.text, token.lemma_, token.pos_)</span></pre><p id="d2b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它会打印出这个:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="a02c" class="mc md it nr b gy nw nx l ny nz">At at ADP<br/>midnight midnight NOUN<br/>the the DET<br/>doorbell doorbell PROPN<br/>rang ring VERB<br/>, , PUNCT<br/>startling startle VERB<br/>him -PRON- PRON<br/>fearfully fearfully ADV<br/>. . PUNCT</span></pre><p id="8936" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这似乎是正确的，错误地将“门铃”归类为专有名词，而不是名词。这是大模型。小模型表现更差，将“让”归类为专有名词，没有找到动词的现在时态。出于某种原因，这变成了一个棘手的句子。</p><p id="9d4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你也可以用显示法来形象化一个句子，尽管我认为这主要是在教育环境中有用。如果您正在Jupyter笔记本中工作，您已经打开了一个浏览器窗口，您可以使用<code class="fe no np nq nr b">render</code>方法:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="e7bd" class="mc md it nr b gy nw nx l ny nz">spacy.displacy.render(doc, style=”dep”)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/08e1ac96732df6a1cb032b64fb36575b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QnOQJjC4lJsPtkpE-h_9qQ.png"/></div></div></figure><p id="ed96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你也可以将这些可视化保存为SVG文件，你可以在<a class="ae ky" href="https://spacy.io/usage/visualizers" rel="noopener ugc nofollow" target="_blank">文档</a>中读到更多关于如何做的内容。</p><p id="42c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到了spaCy如何通过一个随机的句子很好地工作。像《老臭虫》(1919)中这样故意拼错的奇怪句子呢？</p><blockquote class="lv lw lx"><p id="eb6d" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">也是一个了不起的人——他的父亲是一个大公司的律师，他的母亲是一个文学天才。</p></blockquote><p id="b35d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嗯，不出所料，图书馆在挣扎。它或多或少地获得了标记的词性索引，但是没有识别出“‘n’”应该是“and”，“s”应该是“his”，等等…</p><p id="e4f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于spaCy库，我想谈的另一件事是它如何识别文本中的命名实体。令我惊讶的是，它甚至对虚构的作品也有效。</p><p id="bd7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">引用赫伯特·韦斯特《复活者》( 1922)的第一句话。</p><blockquote class="lv lw lx"><p id="ea63" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">赫伯特·韦斯特是我大学和晚年的朋友，说起他，我只能感到极度恐惧。</p></blockquote><p id="bb64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以遍历Doc对象中的<a class="ae ky" href="https://spacy.io/api/doc#ents" rel="noopener ugc nofollow" target="_blank">条目</a>来查找命名实体:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="bfc8" class="mc md it nr b gy nw nx l ny nz">doc = nlp(text)<br/>for ent in doc.ents:<br/>    print(ent.text, ent.label_)</span></pre><p id="f4f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将返回:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="a9ae" class="mc md it nr b gy nw nx l ny nz">Herbert West PERSON</span></pre><p id="4ce1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我发现这是一个令人印象深刻的特点。让我们看看同一个故事中的另一句话:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="2520" class="mc md it nr b gy nw nx l ny nz">We were in the third year of our course at the Miskatonic University Medical School in Arkham.</span></pre><p id="d2e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们以同样的方式列出实体后，我们得到:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="bcf9" class="mc md it nr b gy nw nx l ny nz">the third year DATE<br/>University Medical School ORG<br/>Arkham GPE</span></pre><p id="c06f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几乎完美，虽然出于某种原因，它错过了“米斯卡托尼克”部分。</p><p id="1521" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这也可以形象化，这一次我们应该将样式设置为“ent”:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="c643" class="mc md it nr b gy nw nx l ny nz">spacy.displacy.render(doc, style=”ent”)</span></pre><p id="d519" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">退货:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/f96b00186377d2c7239555426bad337e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2HPtFFl_h_W45Wn9O6lBWQ.png"/></div></div></figure><p id="6fc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是我们现在需要的。</p><h2 id="8c9d" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">赫伯特·韦斯特《复活者》( 1922)的词汇分析</h2><p id="55af" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">作为第一步，我们专注于一个故事。库中有许多内置的方法可以自动为你做单词包，我没有找到一个完全符合我需要的方法，幸运的是，我们自己编写代码真的很简单！让我们首先为这个项目创建一些助手函数。</p><p id="3a53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们构建一个<code class="fe no np nq nr b">create_word_counts_by_pos</code>函数，它接受一个原始文本文件、一个词性标签列表和一个可选的单词计数字典，以防我们想要以这种方式聚合多个故事。它将返回一个字典，其中的键是词性标签，值是更小的字典，对应标签的单词作为值，它们在文本中的计数作为值…所以word_count_dict的结果字典内部将有一个word_count_dict['名词']字典，它将由类似' man': 4，' house': 28，…，word_count_dict['ADJ']，word_count_dict['动词']等的键值对组成。</p><p id="ca7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个过程的一个重要部分是我们想要排除停用词，这对于spaCy来说非常简单，令牌有一个返回布尔值的<code class="fe no np nq nr b">is_stop</code>方法，我们只需要考虑这个属性为False的令牌。</p><p id="cb96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们有一个这种格式的<code class="fe no np nq nr b">word_count_dict</code>,我们可能想要过滤掉超过某个阈值的单词。这就是<code class="fe no np nq nr b">filter_word_count_dict_to_frequent</code>要做的，保持输入字典的结构，但是删除所有计数低于阈值的元素。</p><p id="a462" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们需要一些方法来排序单词，这样我们就可以知道哪些是最常用的。这就是<code class="fe no np nq nr b">collect_most_frequent_words</code>将要负责的，你输入一个带数字的单词计数词典，你会从每个词类类别中得到那么多出现频率最高的单词。</p><p id="cca4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是完成我们刚刚讨论的内容所需的所有代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="1f74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了这些函数，获取不同结构中的字数就非常容易了。目前，我们只对名词、专有名词、形容词和动词感兴趣:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="0a43" class="mc md it nr b gy nw nx l ny nz">list_of_pos = ['NOUN', 'PROPN', 'ADJ', 'VERB']</span></pre><p id="93ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们有了<code class="fe no np nq nr b">text</code>变量中的所有文本，要得到<code class="fe no np nq nr b">word_count_dict</code>:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="332c" class="mc md it nr b gy nw nx l ny nz">word_count_dict = \<br/>    word_count_func.create_word_counts_by_pos(text, list_of_pos)</span></pre><p id="4220" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe no np nq nr b">word_count_dict</code>将是一个很长的单词集合，任何在文本中出现过一次的名词、形容词或动词都将在那里出现。我们可以筛选出出现次数超过10次的单词:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="9779" class="mc md it nr b gy nw nx l ny nz">frequent_word_count_dict = \<br/>    word_count_func.filter_word_count_dict_to_frequent(<br/>        word_count_dict, 10)</span></pre><p id="dac9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe no np nq nr b">frequent_word_count_dict</code>足够简洁，在此简单复制并不过分:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="1bb2" class="mc md it nr b gy nw nx l ny nz">{'NOUN': {'friend': 13,<br/>  'college': 21,<br/>  'life': 39,<br/>  'work': 18,<br/>  'year': 19,<br/>  'experiment': 24,<br/>  'fear': 13,<br/>  'death': 16,<br/>  'solution': 22,<br/>  'time': 20,<br/>  'case': 11,<br/>  'reanimation': 13,<br/>  'corpse': 14,<br/>  'specimen': 34,<br/>  'body': 49,<br/>  'eye': 17,<br/>  'field': 15,<br/>  'thing': 53,<br/>  'room': 12,<br/>  'laboratory': 18,<br/>  'house': 16,<br/>  'horror': 15,<br/>  'man': 28,<br/>  'result': 15,<br/>  'night': 17,<br/>  'sound': 14,<br/>  'police': 13},<br/> 'PROPN': {'Herbert': 20,<br/>  'West': 132,<br/>  'Arkham': 18,<br/>  'Dr.': 15,<br/>  'Bolton': 12},<br/> 'ADJ': {'great': 20,<br/>  'hideous': 17,<br/>  'dead': 23,<br/>  'human': 21,<br/>  'fresh': 28,<br/>  'small': 12,<br/>  'new': 14,<br/>  'large': 12},<br/> 'VERB': {'see': 17,<br/>  'come': 24,<br/>  'hold': 11,<br/>  'know': 18,<br/>  'find': 14,<br/>  'restore': 11,<br/>  'inject': 15,<br/>  'think': 18,<br/>  'look': 14,<br/>  'tell': 11}}</span></pre><p id="42b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你读了这个故事/看了这部电影，你可能会发现很多这些令人满意。对于没有看过的人来说:这个故事基本上是关于一个疯狂的科学家创造僵尸，并永无止境地寻找新鲜尸体。</p><p id="a236" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看形容词，前三个形容词是:“新死去的人”可能是故事的工作标题。“标本”比“人”用得更多，显然与赫伯特·韦斯特是朋友的叙述者大多称他为“韦斯特”，而不是“赫伯特”，动词“注射”用得可疑。</p><p id="8d96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们对所有的故事都这样做！</p><h2 id="4527" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">所有故事的单词分析</h2><p id="0213" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我决定在分析中省略专有名词，专注于每个故事每个词类的前5个词。首先，我们需要设置一些变量:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="418d" class="mc md it nr b gy nw nx l ny nz">number_of_words_to_collect = 5<br/>list_of_pos = ['NOUN', 'ADJ', 'VERB']</span><span id="3db1" class="mc md it nr b gy oa nx l ny nz">words = {}<br/>word_counts = {}</span><span id="88b5" class="mc md it nr b gy oa nx l ny nz">for part_of_speech in list_of_pos:<br/>    words[part_of_speech] = {}<br/>    word_counts[part_of_speech] = {}<br/>    for number in range(number_of_words_to_collect):<br/>        words[part_of_speech][number] = []<br/>        word_counts[part_of_speech][number] = []</span></pre><p id="0559" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以做一个类似的过程，就像我们在上一章中对单个故事所做的那样。我们不使用常用词典，而是请求一个最常用单词的列表。假设<code class="fe no np nq nr b">filenames</code>包含我们要导入的文件的名称，并且它们保存在<code class="fe no np nq nr b">txt_folder</code>中:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="61c0" class="mc md it nr b gy nw nx l ny nz">for filename in filenames: <br/>    my_file = open(txt_adjusted_folder + filename + '.txt')<br/>    print("Currently processing: " + filename)<br/>    raw_text = my_file.read()<br/>    word_count_dict = \ <br/>        word_count_func.create_word_counts_by_pos(<br/>            raw_text, list_of_pos)<br/>    most_frequent_list_dict = \<br/>        word_count_func.collect_most_frequent_words(<br/>            word_count_dict, number_of_words_to_collect)<br/>    for part_of_speech in list_of_pos:<br/>        for number in range(number_of_words_to_collect):<br/>            words[part_of_speech][number].append(<br/>                most_frequent_list_dict[part_of_speech][number][0])<br/>            word_counts[part_of_speech][number].append(<br/>                most_frequent_list_dict[part_of_speech][number][1])</span></pre><p id="acff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完成后，我们应该在两个字典中有我们需要的一切，<code class="fe no np nq nr b">words</code>和<code class="fe no np nq nr b">word_counts</code>。我把它们和熊猫结合起来，对于‘名词’组，我们可以这样做:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="046b" class="mc md it nr b gy nw nx l ny nz">noun_counts = pd.DataFrame()<br/>noun_counts['filename'] = filenames<br/>for i in range(number_of_words_to_collect):<br/>    noun_counts['word_' + str(i+1)] = \<br/>        words['NOUN'][i]<br/>    noun_counts['word_' + str(i+1) + '_count'] = \ <br/>        word_counts['NOUN'][i]</span></pre><p id="b0c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对形容词和动词重复同样的操作，现在我们有了语料库中63个故事的前5个单词及其在3个词性组中的数量。把它们都复制到这里可能有点多，你可以在CSV文件中找到关于<a class="ae ky" href="https://github.com/MatePocs/lovecraft/blob/master/results/word_counts/most_frequent_words/most_frequent_nouns.csv" rel="noopener ugc nofollow" target="_blank">名词</a>、<a class="ae ky" href="https://github.com/MatePocs/lovecraft/blob/master/results/word_counts/most_frequent_words/most_frequent_adjs.csv" rel="noopener ugc nofollow" target="_blank">形容词</a>和<a class="ae ky" href="https://github.com/MatePocs/lovecraft/blob/master/results/word_counts/most_frequent_words/most_frequent_verbs.csv" rel="noopener ugc nofollow" target="_blank">动词</a>的数据。</p><p id="e4d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我举几个我觉得很有趣的例子。</p><p id="d3d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">名词:</p><ul class=""><li id="078e" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">我个人最喜欢的一首，埃里希·赞恩(Erich Zann，1921)的音乐是用这些名词来代表的:街道、音乐、夜晚、窗户、房间。</li><li id="61e8" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">潜伏的恐惧(1922):大厦，恐惧，夜晚，事物，眼睛。</li><li id="5989" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">《黑暗中的低语》(1930):事物、人、信、时间、声音</li></ul><p id="0832" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">形容词:</p><ul class=""><li id="34e0" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">Cele phas(1920):奇怪，白色，伟大，金色，古老</li><li id="d5ee" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">克苏鲁的召唤(1925):伟大，古老，陌生，年轻，死亡</li></ul><p id="4f96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">动词通常与故事不太匹配，毕竟有很多通用动词。再加上Lovecraft的风格很有描写性和距离感，几乎没有什么动作。</p><p id="38aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好的，到目前为止我很满意，我认为这样一个基本的分析是如何抓住这么多故事的本质的，这令人印象深刻。</p><h2 id="8d52" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">所有洛夫克拉夫特的故事加起来</h2><p id="190a" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">最后，回到大问题，他用得最多的词是什么？按照与上面类似的过程，但是在每一轮，我们保留<code class="fe no np nq nr b">word_count_dict</code>，所以它计算一个组合分数。(组合所有文本不起作用，这大约是spaCy中内存限制的2.5倍。)</p><p id="31dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，鼓点，这是洛夫克拉夫特最常用的20个名词、形容词和动词:</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="4f41" class="mc md it nr b gy nw nx l ny nz">{'NOUN': [('thing', 1152),<br/>  ('man', 1075),<br/>  ('time', 830),<br/>  ('night', 666),<br/>  ('place', 569),<br/>  ('house', 490),<br/>  ('day', 476),<br/>  ('city', 453),<br/>  ('year', 432),<br/>  ('stone', 407),<br/>  ('dream', 392),<br/>  ('room', 383),<br/>  ('world', 379),<br/>  ('door', 370),<br/>  ('way', 367),<br/>  ('horror', 352),<br/>  ('light', 350),<br/>  ('life', 340),<br/>  ('wall', 335),<br/>  ('eye', 333)],<br/> 'ADJ': [('old', 917),<br/>  ('great', 797),<br/>  ('strange', 488),<br/>  ('certain', 446),<br/>  ('black', 379),<br/>  ('little', 336),<br/>  ('ancient', 308),<br/>  ('high', 297),<br/>  ('small', 282),<br/>  ('dark', 277),<br/>  ('new', 268),<br/>  ('human', 267),<br/>  ('unknown', 259),<br/>  ('terrible', 256),<br/>  ('long', 244),<br/>  ('curious', 227),<br/>  ('low', 216),<br/>  ('hideous', 213),<br/>  ('young', 210),<br/>  ('good', 204)],<br/> 'VERB': [('come', 1100),<br/>  ('know', 1095),<br/>  ('see', 1079),<br/>  ('find', 784),<br/>  ('tell', 649),<br/>  ('think', 571),<br/>  ('hear', 528),<br/>  ('look', 474),<br/>  ('go', 439),<br/>  ('say', 425),<br/>  ('leave', 401),<br/>  ('begin', 400),<br/>  ('feel', 397),<br/>  ('take', 368),<br/>  ('grow', 310),<br/>  ('give', 306),<br/>  ('shew', 275),<br/>  ('bring', 267),<br/>  ('speak', 266),<br/>  ('try', 262)]}</span></pre><p id="f68d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我再次认为动词是最不令人兴奋的，尽管你可以看到大多数动词都是关于知识收集或分享的。如果你想知道，“shew”是show的老式拼法。</p><p id="5eb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">名词更好，“时间”，“夜晚”，“石头”，“梦”，“恐怖”——我会根据这五个词猜测作者是Lovecraft。我的意思是，恐怖出现在有史以来最常用的20个名词中，这确实令人放心！</p><p id="1494" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，形容词，至少有三分之二在列表中有这么高的位置通常会很奇怪。最常用的形容词是“老”，我的意思是，那有多牛逼？这个清单还包括“伟大”、“奇怪”、“古老”、“未知”、“可怕”等等。</p><p id="2acd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(你可能会注意到，我们有213次“可怕的”，但在我在介绍中提到的<a class="ae ky" href="https://arkhamarchivist.com/wordcount-lovecraft-favorite-words/" rel="noopener ugc nofollow" target="_blank">阿卡姆档案管理员</a>页面上，这个词有260次。这种差异是由于我们根据词类来区分单词的用法。他把这个词作为形容词用了213次，但这不包括名词“可怕”或副词“可怕地”。)</p><h2 id="e507" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">未来可能的分析</h2><p id="75a3" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我认为用这些简单的NLP工具可以做更多有趣的分析。我们把范围限制在名词、形容词和动词上，看看副词或专有名词也会很有趣。分析不同年份动词的相对比例也很有趣。这样，我们可以看到随着时间的推移，故事是否变得更加精彩，还是相反？还有一个非常重要的概念我们没有考虑到:<a class="ae ky" href="https://en.wikipedia.org/wiki/N-gram#:~:text=In%20the%20fields%20of%20computational,a%20text%20or%20speech%20corpus." rel="noopener ugc nofollow" target="_blank"> n-grams </a>。可以进行非常相似的分析来找出他的作品中最常用的表达。</p><p id="3c40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们从一个非常简单的问题开始:我想知道Lovecraft使用频率最高的词是什么？对于这样一个简单的问题，这比我想象的要复杂得多。看到这样一个简单的NLP表示工作得如此之好真是太酷了。</p><p id="830b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下一篇文章中，我将讲述如何进一步计算字数，并使用TF-IDF向量计算文档之间的差异。</p><h2 id="4cb9" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">参考</h2><p id="80e4" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">霍布森、科尔和汉尼斯(2019年)。自然语言处理实践:理解、分析和用Python生成文本。曼宁出版，2019。</p><div class="of og gp gr oh oi"><a href="https://spacy.io/usage/linguistic-features" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd iu gy z fp on fr fs oo fu fw is bi translated">语言特征空间使用文档</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">智能地处理原始文本是困难的:大多数单词是罕见的，对于看起来完全…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">空间. io</p></div></div><div class="or l"><div class="os l ot ou ov or ow ks oi"/></div></div></a></div><p id="5fee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">洛夫克拉夫特全集:</p><p id="a57e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arkhamarchivist.com/free-complete-lovecraft-ebook-nook-kindle/" rel="noopener ugc nofollow" target="_blank">https://arkhamarchivist . com/free-complete-love craft-ebook-nook-kindle/</a></p></div></div>    
</body>
</html>