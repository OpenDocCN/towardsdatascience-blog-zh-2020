<html>
<head>
<title>A hands on an end to end data science project using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python实践端到端数据科学项目</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-hands-on-an-end-to-end-data-science-project-using-python-4ed97842e27f?source=collection_archive---------32-----------------------#2020-05-14">https://towardsdatascience.com/a-hands-on-an-end-to-end-data-science-project-using-python-4ed97842e27f?source=collection_archive---------32-----------------------#2020-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f21d9f6e953461157aa51eec995e36e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zpaKgT77yS5VePEP.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:未知，来自<a class="ae jg" href="https://pagesandpaint.wordpress.com/" rel="noopener ugc nofollow" target="_blank"/></p></figure><h2 id="a58b" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">使用Python运行数据科学项目的6步指南</h2><div class=""/><p id="b0da" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jt">机器学习、数据科学、预测</strong>，这些词你可能听过很多，但它们到底是什么？在什么情况下有用？<br/>机器学习诞生于20世纪60年代，是人工智能的一个分支，它为系统提供了学习模型和进行预测的能力，例如根据房子的特征(位置、面积、房间数量……)预测房子的价格，或者预测肿瘤是否是良性的。在房子问题中，我们希望预测一个连续值，这类问题属于我们所说的<strong class="kr jt">回归问题</strong>。在肿瘤的情况下，结果是二进制值或更一般的离散值，这种类型的问题称为<strong class="kr jt">分类问题</strong>。在处理数据科学问题时，了解您正在寻找的结果的类型以了解您正在处理的是回归还是分类问题是极其重要的。</p><p id="4cd8" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这篇文章中，我写了一个进行端到端机器学习项目的指南。如下图所示，图中显示了数据科学项目问题的不同步骤，共有<strong class="kr jt"> 6个步骤</strong>。我们将在整篇文章中深入研究它们。</p><figure class="lo lp lq lr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ln"><img src="../Images/fc780d1769e9e8068b80ff62bcf39d7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VxmVQlXokt6_VVApdv76UQ.png"/></div></div></figure><p id="334e" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了说明数据科学项目的不同步骤，我将使用一个数据集来描述在高时间分辨率宇宙巡天期间收集的脉冲星候选样本。脉冲星是一种罕见的中子星，它产生的无线电辐射在地球上可以探测到。作为时空、星际介质和物质状态的探测器，它们具有相当大的科学价值。随着脉冲星的旋转，它们的发射光束扫过天空，当光束穿过我们的视线时，会产生一种可检测的宽带无线电发射模式。随着脉冲星快速旋转，这种模式周期性重复。因此，脉冲星涉及到用大型射电望远镜寻找周期性的无线电信号。每颗脉冲星产生的发射模式略有不同，每次旋转都会略有不同。因此，被称为“候选者”的潜在信号探测在脉冲星的多次旋转中被平均化，这由观测的长度决定。在缺乏额外信息的情况下，每个候选者都有可能描述一颗真正的脉冲星。然而在实践中，几乎所有的检测都是由射频干扰(RFI)和噪声引起的，使得合法信号很难被发现。</p><p id="0220" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这里，使用机器学习模型来<strong class="kr jt">预测一颗恒星是否是脉冲星</strong>是完全合法的，因为纯物理模型有很多障碍要克服。通过使用机器学习模型，我们可以获得准确的结果，从而降低问题的复杂性。让我们看看我们能做什么！<br/>你可以在这里找到数据集<a class="ae jg" href="https://www.kaggle.com/pavanraj159/predicting-a-pulsar-star" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="00b8" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">1.问题的定义</h1><p id="8a50" class="pw-post-body-paragraph kp kq jj kr b ks mq ku kv kw mr ky kz la ms lc ld le mt lg lh li mu lk ll lm im bi translated">在这个项目中，想法是预测一颗恒星是否是脉冲星。我们可以清楚地看到，我们的模型的结果将是二元的:<strong class="kr jt">如果恒星是脉冲星，则为1，否则为0</strong>。因此，我们将实现分类模型来回答这个问题。</p><h1 id="076a" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">2.提取数据并将其可视化</h1><p id="dcf0" class="pw-post-body-paragraph kp kq jj kr b ks mq ku kv kw mr ky kz la ms lc ld le mt lg lh li mu lk ll lm im bi translated">当我们使用Python时，我们将使用<strong class="kr jt"> Pandas </strong>库从csv文件中提取数据。对于数据可视化，库<strong class="kr jt"> seaborn </strong>是一个非常好的库，因为很多数据科学工具已经实现。<strong class="kr jt"> matplotlib.pylab </strong>库对于绘制图形也很有用。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="de0f" class="na lt jj mw b gy nb nc l nd ne"><strong class="mw jt">import</strong> <strong class="mw jt">numpy</strong> <strong class="mw jt">as</strong> <strong class="mw jt">np</strong> <br/><strong class="mw jt">import</strong> <strong class="mw jt">pandas</strong> <strong class="mw jt">as</strong> <strong class="mw jt">pd</strong> <br/><br/><em class="nf"># for data visualization</em><br/><strong class="mw jt">import</strong> <strong class="mw jt">seaborn</strong> <strong class="mw jt">as</strong> <strong class="mw jt">sns</strong> <br/><strong class="mw jt">import</strong> <strong class="mw jt">matplotlib.pylab</strong> <strong class="mw jt">as</strong> <strong class="mw jt">plt</strong></span></pre><p id="35eb" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，我们可以从csv文件中提取数据，并查看不同的特征。您应该经常做的一件事是检查每个特性中的元素数量。如果某些特征的元素数量不同，您必须应用某些操作来填补空白，或者如果缺少太多的值，您必须放弃该特征。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="b76d" class="na lt jj mw b gy nb nc l nd ne">pulsar_stars = pd.read_csv("pulsar_stars.csv") # read the data</span><span id="3f7b" class="na lt jj mw b gy ng nc l nd ne"><em class="nf"># to check if we have to clean the data </em><br/>print(pulsar_stars.apply(pd.Series.count))</span></pre><figure class="lo lp lq lr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nh"><img src="../Images/f821db7f4bd24dd297dffde9b37e1317.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*40mBwQeABY1vk7jif6JehA.png"/></div></div></figure><p id="f65a" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以看到，它们是定义恒星的<strong class="kr jt"> 8个特征</strong>，而<strong class="kr jt">目标类</strong>是一个二进制值(如果候选恒星是脉冲星，则为1，否则为0)。所有不同的特征具有相同数量的元素，因此没有必要进行填充间隙的操作。</p><p id="2e03" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下一步是<strong class="kr jt">将数据</strong>分割成我们将用来训练机器学习算法的数据和我们将用来测试它们各自性能的数据。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="9a31" class="na lt jj mw b gy nb nc l nd ne"><em class="nf"># we set the seed for the random cursor</em> <br/>random.seed(10)  </span><span id="6485" class="na lt jj mw b gy ng nc l nd ne"><em class="nf"># we split the data</em> <br/>X = pulsar_stars.drop(['target_class'], axis = 1) <br/>y = pulsar_stars['target_class'] <br/>X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.8, random_state = 10) </span><span id="e5f0" class="na lt jj mw b gy ng nc l nd ne">X_columns = X_train.columns</span></pre><p id="075d" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">接下来，我们可以查看不同特性之间的<strong class="kr jt">相关性</strong>以及一个特性和目标类之间的链接，以便了解对结果的影响。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="028f" class="na lt jj mw b gy nb nc l nd ne"><em class="nf">#correlation plot</em><br/>corr = pulsar_stars.corr()<br/>sns.heatmap(corr)</span></pre><figure class="lo lp lq lr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/87ba27986f3d28c196bb6f95b95f4d47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Lw2N-mo1tu1BypMD6G1yQ.png"/></div></div></figure><p id="3c3d" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们注意到，综合轮廓的过度峰度是与目标类更相关的特征。现在，我们将研究每个变量的<strong class="kr jt">分布</strong>，看看我们是否必须应用一个变换来使它更有价值。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="c439" class="na lt jj mw b gy nb nc l nd ne"><em class="nf">#create numeric plots</em><br/>num = [f <strong class="mw jt">for</strong> f <strong class="mw jt">in</strong> X_train.columns <strong class="mw jt">if</strong> X_train.dtypes[f] != 'object']<br/>nd = pd.melt(X_train, value_vars = num)<br/>n1 = sns.FacetGrid (nd, col='variable', col_wrap=3, height = 5.5, sharex=<strong class="mw jt">False</strong>, sharey = <strong class="mw jt">False</strong>)<br/>n1 = n1.map(sns.distplot, 'value')</span></pre><figure class="lo lp lq lr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/b9c4d45ea187f536601ed55f9a7643ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*STubMCcdL1ymmXwwTZtkbg.png"/></div></div></figure><p id="e8bf" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">大多数变量具有高斯分布(对于大多数特征，您可能认为数据遵循右偏分布，但这只是因为数据的数量不够大)。</p><p id="a76b" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们对数据有了更好的理解，我们可以对其进行预处理，以便为我们的机器学习算法提供信息。</p><h1 id="4589" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">3.数据的预处理</h1><p id="6116" class="pw-post-body-paragraph kp kq jj kr b ks mq ku kv kw mr ky kz la ms lc ld le mt lg lh li mu lk ll lm im bi translated">由于不同的特征有完全不同的尺度，我们需要将我们的数据标准化。当特征具有相同的尺度时，这个步骤是不必要的，除了诸如PCA的一些算法。</p><p id="91cf" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">它们是规范化数据的不同方法:</p><figure class="lo lp lq lr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/7c6f60eb37b8888c906b7db19566e206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jy-ql8jl3mKjswLwzYK4nA.png"/></div></div></figure><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="30bc" class="na lt jj mw b gy nb nc l nd ne">scaler = StandardScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.transform(X_test)</span></pre><h1 id="7fb7" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">4.机器学习模型的选择及其评估指标</h1><p id="e92c" class="pw-post-body-paragraph kp kq jj kr b ks mq ku kv kw mr ky kz la ms lc ld le mt lg lh li mu lk ll lm im bi translated">在这一部分中，我们实现了不同的机器学习模型来处理分类问题，并定义了评估其性能的措施。对于机器学习模型，我将使用决策树算法、逻辑回归算法、随机森林算法和K-最近邻算法。我将使用时间、精确度、回忆、加权f1分数和AUC分数来评估模型。我将更详细地解释我的选择和每个措施的意义。</p><h1 id="87af" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">5.训练机器学习模型并评估它们</h1><p id="3b03" class="pw-post-body-paragraph kp kq jj kr b ks mq ku kv kw mr ky kz la ms lc ld le mt lg lh li mu lk ll lm im bi translated">这些模型中的一些具有超参数，即其值在学习过程开始之前设定的参数。一种方法是建立超参数可以采用的值列表，并根据尽可能精确的度量通过网格进行搜索，以便找到最佳模型。另一种方法是使用贝叶斯优化(一种更有效的方法)。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="e306" class="na lt jj mw b gy nb nc l nd ne"><em class="nf"># we start by DecisionTreeClassifier</em><br/><br/>dc = DecisionTreeClassifier(max_depth = 4)<br/>dc.fit(X_train, y_train)<br/><br/>params = {'max_depth' : [2,4,8]}<br/>dcgrid = GridSearchCV(estimator = dc, param_grid = params, cv = KFold(5, random_state = 10) , scoring = 'accuracy')<br/>dcgrid.fit(X_train, y_train)<br/><br/><br/><em class="nf"># Then by LogisticRegression</em><br/><br/>lg = LogisticRegression(C=0.001, solver='liblinear')<br/>lg.fit(X_train, y_train)<br/><br/>params = {'C':[0.01,0.1,1,10]}<br/>lggrid = GridSearchCV(estimator = lg, param_grid = params, cv = KFold(5, random_state = 10), scoring = 'accuracy')<br/>lggrid.fit(X_train, y_train)<br/><br/><br/><em class="nf"># then by RandomForestClassifier</em><br/><br/>rf = RandomForestClassifier(n_estimators = 10, max_depth = 10)<br/>rf.fit(X_train, y_train)<br/><br/>params = {'n_estimators' : [10, 20, 50, 100], 'max_depth' : [10, 50]}<br/>rfgrid = GridSearchCV(estimator = rf, param_grid = params, cv = KFold(5, random_state = 10), scoring = 'accuracy')<br/>rfgrid.fit(X_train, y_train)<br/><br/><br/><em class="nf"># then KNeighborsClassifier</em><br/><br/>kn = KNeighborsClassifier(n_neighbors = 10, p = 2)<br/>kn.fit(X_train, y_train)<br/><br/>params = {'n_neighbors' : [2, 5, 10, 50], 'weights' : ['uniform', 'distance'], 'p' :[1,2]}<br/>kngrid = GridSearchCV(estimator = kn, param_grid = params, cv = KFold(5, random_state = 10), scoring = 'accuracy')<br/>kngrid.fit(X_train, y_train)</span></pre><p id="d9e8" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后我们找到最佳模型，我们还可以看到训练这些模型所需的<strong class="kr jt">时间</strong>，这可以作为选择最终模型的一个指标。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="c121" class="na lt jj mw b gy nb nc l nd ne"><em class="nf"># we define the best models </em><br/>dc_best = dcgrid.best_estimator_<br/>lg_best = lggrid.best_estimator_<br/>rf_best = rfgrid.best_estimator_<br/>kn_best = kngrid.best_estimator_<br/><br/><br/><br/><em class="nf"># now we will launch each model and see the time and the performance of each </em><br/>start = time.time()<br/><br/>dc = dc_best<br/>dc_best.fit(X_train, y_train)<br/><br/>end = time.time()<br/>print('time for Decision Tree Classifier = ', end - start, 's')<br/>performance_df['Decision_tree']['time(s)'] = end - start<br/><br/><em class="nf"># Then by LogisticRegression</em><br/>start = time.time()<br/><br/>lg = lg_best<br/>lg_best.fit(X_train, y_train)<br/><br/>end = time.time()<br/>print('time for Logisitic Regression= ', end - start, 's')<br/>performance_df['Logistic_regression']['time(s)'] = end - start</span><span id="f0ee" class="na lt jj mw b gy ng nc l nd ne"><br/><em class="nf"># then by RandomForestClassifier</em><br/>start = time.time()<br/><br/>rf = rf_best<br/>rf_best.fit(X_train, y_train)<br/><br/>end = time.time()<br/>print('time for Random Forst Classifier = ', end - start, 's')<br/>performance_df['Random_forest']['time(s)'] = end - start<br/><br/><br/><em class="nf"># then KNeighborsClassifier</em><br/>start = time.time()<br/><br/>kn = kn_best<br/>kn_best.fit(X_train, y_train)<br/><br/>end = time.time()<br/>print('time for K_Neighbors Classifier = ', end - start,'s')<br/>performance_df['K-NNeighbors']['time(s)'] = end - start</span></pre><p id="bbd5" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我们还可以根据另一个指标来评估模型，这个指标就是<strong class="kr jt">召回</strong>。回忆，也称为敏感度，衡量模型预测积极结果的强度，即1正确识别的比例。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="90f0" class="na lt jj mw b gy nb nc l nd ne"><em class="nf"># we will calculate the scores of each model </em><br/>y_predict_dc = dc_best.predict(X_test)<br/>accuracy = accuracy_score(y_test, y_predict_dc)<br/>recall = recall_score(y_test, y_predict_dc)<br/>performance_df['Decision_tree']['accuracy'] = accuracy<br/>performance_df['Decision_tree']['recall'] = recall<br/><br/>y_predict_lg = lg_best.predict(X_test)<br/>accuracy = accuracy_score(y_test, y_predict_lg)<br/>recall = recall_score(y_test, y_predict_lg)<br/>performance_df['Logistic_regression']['accuracy'] = accuracy<br/>performance_df['Logistic_regression']['recall'] = recall<br/><br/>y_predict_rf = dc_best.predict(X_test)<br/>accuracy = accuracy_score(y_test, y_predict_rf)<br/>recall = recall_score(y_test, y_predict_rf)<br/>performance_df['Random_forest']['accuracy'] = accuracy<br/>performance_df['Random_forest']['recall'] = recall<br/><br/>y_predict_kn = dc_best.predict(X_test)<br/>accuracy = accuracy_score(y_test, y_predict_kn)<br/>recall = recall_score(y_test, y_predict_kn)<br/>performance_df['K-NNeighbors']['accuracy'] = accuracy<br/>performance_df['K-NNeighbors']['recall'] = recall</span></pre><p id="2d2b" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，我们可以计算作为误差矩阵的<strong class="kr jt">混淆矩阵</strong>:</p><figure class="lo lp lq lr gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/6259534bf2de98efedbd30d5f529074e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*08tgK_b3yO8UDbpMFgIiYw.png"/></div></figure><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="3226" class="na lt jj mw b gy nb nc l nd ne"><em class="nf"># generate confusion matrix for Decision Tree classifier</em> <br/>conf_mat_dc = confusion_matrix(y_test, y_predict_dc) <em class="nf"># put it into a dataframe for seaborn plot function</em> </span><span id="a6f9" class="na lt jj mw b gy ng nc l nd ne">conf_math_dc_df = pd.DataFrame(conf_mat_dc)  <em class="nf"># Use a seaborn heatmap to plot confusion matrices</em> </span><span id="8a06" class="na lt jj mw b gy ng nc l nd ne"><em class="nf"># The dataframe is transposed to make Actual values on x-axis and predicted on y-axis</em> <br/><em class="nf"># annot = True includes the numbers in each box</em> <br/><em class="nf"># vmin and vmax just adjusts the color value</em> <br/>fig, ax = plt.subplots(figsize = (7,7)) sns.heatmap(conf_math_dc_df.T, annot=<strong class="mw jt">True</strong>, annot_kws={"size": 15}, cmap="Oranges", vmin=0, vmax=800, fmt='.0f',              linewidths=1, linecolor="white", cbar=<strong class="mw jt">False</strong>, xticklabels=["no pulsar star","pulsar star"],              yticklabels=["no pulsar star","pulsar star"]) </span><span id="59a6" class="na lt jj mw b gy ng nc l nd ne">plt.ylabel("Predicted", fontsize=15) <br/>plt.xlabel("Actual", fontsize=15) <br/>ax.set_xticklabels(["no pulsar star","pulsar star"], fontsize=13) ax.set_yticklabels(["no pulsar star","pulsar star"], fontsize=13) plt.title("Confusion Matrix for 'Decision Tree' Classifier", fontsize=15)  </span><span id="4a15" class="na lt jj mw b gy ng nc l nd ne">plt.show()<br/>print("")<br/>print(classification_report(y_test, y_predict_dc))</span></pre><figure class="lo lp lq lr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/318fd6d638af8204fcd9305bb8fbd290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WQ0vbPs87QQrmDOf8HZcfQ.png"/></div></div></figure><p id="b23a" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在对其他模型做了这些之后，我们计算最后的测量，即AUC曲线。AUC-ROC曲线是在不同阈值下对分类问题的性能测量。ROC是概率曲线，AUC是可分性的程度或度量。它表明模型区分不同类别的能力。AUC越高，模型就越能预测0是0，1是1。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="a4fd" class="na lt jj mw b gy nb nc l nd ne"><em class="nf">#Plotting the ROC curve</em><br/><br/><em class="nf">#Generating points to plot on ROC curve (logistic model)</em><br/>dc_best_prob = dc_best.predict_proba(X_test)<br/>fpr_logis, tpr_logis, thresholds_logis = roc_curve(y_test, dc_best_prob[:, 1])<br/><br/><br/>fig, ax = plt.subplots(figsize = (10,7))</span><span id="c0f4" class="na lt jj mw b gy ng nc l nd ne"><em class="nf">#plotting the "guessing" model<br/></em>plt.plot([0, 1], [0, 1], 'k--')</span><span id="91a6" class="na lt jj mw b gy ng nc l nd ne"><em class="nf">#plotting the logistic model</em><br/>plt.plot(fpr_logis, tpr_logis)<br/>plt.fill_between(fpr_logis, tpr_logis, alpha=0.2, color='b')<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>AUC = roc_auc_score(y_test, dc_best_prob[:, 1])<br/>plt.title('Decision Tree Classifier ROC curve: AUC=<strong class="mw jt">{0:0.3f}</strong>'.format(AUC))<br/>plt.show()<br/><br/>performance_df['Decision_tree']['AUC'] = AUC</span></pre><figure class="lo lp lq lr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/bf4ce31e8845dca6b4fcb470f958f0e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LKPkH8-mxdvrjngOX8Mgbg.png"/></div></div></figure><h1 id="bbf8" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">6.关于是否达到初始目标的结论，以及实现这些目标的最佳模式的选择</h1><p id="505f" class="pw-post-body-paragraph kp kq jj kr b ks mq ku kv kw mr ky kz la ms lc ld le mt lg lh li mu lk ll lm im bi translated">最后我们得到了下表:</p><figure class="lo lp lq lr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/f3530a9c4c84a72890f274c91e8e8299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yXDiptik_AptYMwf870LKg.png"/></div></div></figure><p id="b2b5" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">根据我们的问题，这些指标对我们的结果没有同等的权重。AUC分数是我们模型中最重要的指标，因为预测一颗恒星为非超级恒星，而犯错是我们想要避免的。因此，逻辑回归似乎是最有效的模型。</p><p id="485c" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">基于我们选择的模型，我们可以检查不同特征对结果的重要性。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="5994" class="na lt jj mw b gy nb nc l nd ne"><em class="nf"># thus the logistic regression is the best model : we will keep it </em><br/><em class="nf"># Now we want to see the importance of each feature on the result  </em><br/>dataframe_importance = pd.DataFrame()<br/>columns = X_train.columns<br/>importances = np.abs(lg_best.coef_[0])<br/><br/><strong class="mw jt">for</strong> i <strong class="mw jt">in</strong> range(len(columns)):<br/>    dataframe_importance[columns[i]] = [importances[i]]<br/><br/>dataframe_importance.insert(0, '', 'Importance features')<br/>dataframe_importance.head(10)</span></pre><figure class="lo lp lq lr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/fd7c535525ea2a2a2c717c24bbb024dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2RcbR__D9DY2HvfBo6PPgA.png"/></div></div></figure><h1 id="ef4a" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">如果模型不够有效</h1><p id="598a" class="pw-post-body-paragraph kp kq jj kr b ks mq ku kv kw mr ky kz la ms lc ld le mt lg lh li mu lk ll lm im bi translated">有时我们可能有时间限制或计算能力限制，我们需要使模型更简单。一种方法是进行<a class="ae jg" rel="noopener" target="_blank" href="/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2">特征选择</a>。我们可以通过使用统计方法，如<a class="ae jg" href="https://en.wikipedia.org/wiki/Chi-squared_test" rel="noopener ugc nofollow" target="_blank">卡方检验</a>，或使用维度缩减算法，如<a class="ae jg" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> PCA </a>。然后，我们重新评估模型，我们可以得出结论，是否考虑了我们在时间和精度方面的约束。</p><h1 id="dba6" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">为您介绍数据科学世界提供一些评论和提示</h1><p id="8a4f" class="pw-post-body-paragraph kp kq jj kr b ks mq ku kv kw mr ky kz la ms lc ld le mt lg lh li mu lk ll lm im bi translated">不要犹豫花很多时间来定义问题。一旦你习惯了数据科学项目的实现技术，你就会意识到这是你的项目中最容易的部分。另一个相当棘手的部分是数据提取和数据清理。在我在本文中向您展示的项目中，数据非常干净，但是情况几乎从来不是这样。最后，我希望你喜欢这篇文章，并且这是一门实用的科学，熟悉它的最好方法是实践它。选择一个你感兴趣的话题，把你学到的东西付诸实践！</p><p id="0cad" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可以在我的<a class="ae jg" href="https://github.com/Jleban/pulsar-stars/blob/master/Stars_Kaggle.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a>上找到作为神经网络的其他实现的整个项目。</p><p id="1177" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你喜欢阅读这样的故事，并想支持我成为一名作家，考虑注册成为一名灵媒成员。每月5美元，你可以无限制地阅读媒体上的故事。如果你注册使用<a class="ae jg" href="https://medium.com/@jonathan_leban/membership" rel="noopener">我的链接</a>，我会赚一小笔佣金，你仍然要支付5美元。谢谢大家！！</p><div class="is it gp gr iu nq"><a href="https://medium.com/@jonathan_leban/membership" rel="noopener follow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd jt gy z fp nv fr fs nw fu fw js bi translated">通过我的推荐链接加入媒体-乔纳森·莱班</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">阅读乔纳森·莱班的每一个故事(以及媒体上成千上万的其他作家)。您的会员费直接支持…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">medium.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe ja nq"/></div></div></a></div><p id="bb15" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="nf"> PS:我目前是伯克利的工程硕士，如果你想讨论这个话题，请随时联系我。</em><a class="ae jg" href="http://jonathan_leban@berkeley.edu/" rel="noopener ugc nofollow" target="_blank"><em class="nf"/></a><a class="ae jg" href="http://jonathan_leban@berkeley.edu" rel="noopener ugc nofollow" target="_blank"><em class="nf">这里的</em> </a> <em class="nf">是我的邮箱。</em></p></div></div>    
</body>
</html>