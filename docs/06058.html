<html>
<head>
<title>Instance Segmentation with Mask R-CNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于掩模 R-CNN 的实例分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/instance-segmentation-with-mask-r-cnn-6e5c4132030b?source=collection_archive---------28-----------------------#2020-05-17">https://towardsdatascience.com/instance-segmentation-with-mask-r-cnn-6e5c4132030b?source=collection_archive---------28-----------------------#2020-05-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="40ba" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用在 MS COCO 数据集上训练的 Mask R-CNN 的简要指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4ffd9b16e1852e22d8414438559c030f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCtbgBfPvkAomekPyDFVbA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对象检测和实例分割—输入图像源<a class="ae kv" href="https://sharedspace.work/the-value-of-independent-coworking-spaces/" rel="noopener ugc nofollow" target="_blank">共享空间</a></p></figure><p id="1a17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">诸如 YOLO、R-CNN 的目标检测模型帮助我们画出围绕目标的边界框，并且实例分割为我们提供了图像中每个目标的像素级掩模。一个问题可能会出现，为什么我们需要逐像素定位？</p><p id="d7fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们只是在自动驾驶汽车中使用对象检测，那么有可能多辆汽车的边界框重叠，自动驾驶汽车在这种情况下会变得混乱。实例分割可以避免这个缺陷。损伤检测和医学诊断是我想到的一些其他应用，因为知道损伤的程度或脑瘤的大小可能比仅仅检测存在更重要。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/5725c22b4876524490351d787845eec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C1JNNiP5xZ2oORD5DzTyWA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">相交边界框和非重叠遮罩</p></figure><p id="08f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的图像中，我们可以看到汽车的边界框是相交的，而类名为“汽车”的蒙版没有相交/重叠。</p><p id="3455" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们将介绍如何使用掩模 R-CNN(掩模区域 CNN)进行实例分割，然后使用掩模 R-CNN，我们可以获得图像中每个对象的逐个像素位置和边界框坐标。</p><p id="5dbb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">面具 R-CNN </strong></p><p id="d1d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">屏蔽 R-CNN 结合了更快的 R-CNN 和 FCN(全连接网络)以获得除类和盒输出之外的额外屏蔽输出。也就是说，Mask R-CNN 采用相同的两阶段过程，具有相同的第一阶段(即 RPN:区域提议网络)。第二阶段使用 RoIPool 从每个候选框中提取特征，并执行分类和包围盒回归。<a class="ae kv" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">阅读本文</a>以获得关于 R-CNN 面具的更详细的想法</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/09e0c5215049665b536e44e51d4ad333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tLHjgNXgxSLhWK-jegWEXw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">屏蔽 R-CNN 模型— <a class="ae kv" href="https://arxiv.org/pdf/1703.06870.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="0855" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我使用了基于 FPN 和 matterport 的 ResNet101 构建的 Mask R-CNN 进行实例分割。该模型在<a class="ae kv" href="http://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> MS COCO </a>上进行预训练，这是一个具有 80 个对象类的大规模对象检测、分割和字幕数据集。</p><p id="49b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在浏览代码之前，确保安装所有需要的包并屏蔽 R-CNN。</p><p id="ad5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">安装 Keras 和其他依赖项:</p><pre class="kg kh ki kj gt lu lv lw lx aw ly bi"><span id="c302" class="lz ma iq lv b gy mb mc l md me">$ pip install numpy scipy keras h5py tensorflow<br/>$ pip install pillow scikit-image matplotlib imutils<br/>$ pip install "IPython[all]"</span></pre><p id="a133" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">克隆 GitHub 存储库并安装 Mask R-CNN 的 matterplot 实现</p><pre class="kg kh ki kj gt lu lv lw lx aw ly bi"><span id="7d81" class="lz ma iq lv b gy mb mc l md me">$git clone <a class="ae kv" href="https://github.com/matterport/Mask_RCNN.git" rel="noopener ugc nofollow" target="_blank">https://github.com/matterport/Mask_RCNN.git</a><br/>$cd Mask_RCNN<br/>$python setup.py install</span></pre><p id="e66a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意:如果您已经安装或使用了<em class="mf"> tensorflow v2.0 </em>，那么您在执行脚本时可能会遇到一些回溯错误，因为 Mask R-CNN 使用的是<em class="mf"> tensorflow v1.3.0 </em></p><p id="45ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了避免这种情况，您可以降级 tensorflow 版本，或者在安装 Mask R-CNN 之前，通过替换以下函数来编辑文件 Mask_RCNN/rcnn/model.py:</p><ul class=""><li id="b1fa" class="mg mh iq ky b kz la lc ld lf mi lj mj ln mk lr ml mm mn mo bi translated"><code class="fe mp mq mr lv b">tf.log()</code>-&gt;-<code class="fe mp mq mr lv b">tf.math.log()</code></li><li id="2523" class="mg mh iq ky b kz ms lc mt lf mu lj mv ln mw lr ml mm mn mo bi translated"><code class="fe mp mq mr lv b">tf.sets.set_intersection()</code>-&gt;-T3】</li><li id="4d92" class="mg mh iq ky b kz ms lc mt lf mu lj mv ln mw lr ml mm mn mo bi translated"><code class="fe mp mq mr lv b">tf.sparse_tensor_to_dense()</code>-&gt;-<code class="fe mp mq mr lv b">tf.sparse.to_dense()</code></li><li id="8254" class="mg mh iq ky b kz ms lc mt lf mu lj mv ln mw lr ml mm mn mo bi translated"><code class="fe mp mq mr lv b">tf.to_float()</code>-&gt;-T7】</li></ul><p id="2ef5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们开始执行脚本:</p><p id="d4e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">第一步:导入所需的包</strong></p><pre class="kg kh ki kj gt lu lv lw lx aw ly bi"><span id="df2f" class="lz ma iq lv b gy mb mc l md me">from mrcnn.config import Config<br/>from mrcnn import model as modellib<br/>from mrcnn import visualize<br/>import cv2<br/>import colorsys<br/>import argparse<br/>import imutils<br/>import random<br/>import os<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import tensorflow as tf</span></pre><p id="65d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">第二步:为每个类标签生成随机颜色。</strong></p><p id="1d65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们创建一个配置，它定义了将在下一步中加载的模型的一些属性。</p><p id="5fbd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您的 GPU 可以处理变量 IMAGES_PER_GPU，您可以随意增加它的值，否则(在 CPU 的情况下)保持它为 1。</p><pre class="kg kh ki kj gt lu lv lw lx aw ly bi"><span id="1236" class="lz ma iq lv b gy mb mc l md me">class SimpleConfig(Config):<br/>    # give the configuration a recognizable name<br/>    NAME = "coco_inference"<br/>    # set the number of GPUs to use along with the number of images<br/>    # per GPU<br/>    GPU_COUNT = 1<br/>    IMAGES_PER_GPU = 1<br/>    # number of classes on COCO dataset<br/>    NUM_CLASSES = 81</span></pre><p id="32d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">第四步:创建一个配置类对象，加载模型权重。</strong></p><p id="6241" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以从<a class="ae kv" href="https://github.com/matterport/Mask_RCNN/releases/tag/v1.0" rel="noopener ugc nofollow" target="_blank">这里</a>下载重量。</p><pre class="kg kh ki kj gt lu lv lw lx aw ly bi"><span id="a6ee" class="lz ma iq lv b gy mb mc l md me">config = SimpleConfig()<br/>config.display()<br/>model = modellib.MaskRCNN(mode="inference", config=config, model_dir=os.getcwd())<br/>model.load_weights("mask_rcnn_coco.h5", by_name=True)</span></pre><p id="eadf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">步骤五:对任意图像进行正向传递，得到分割输出。</strong></p><p id="8a67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这一步中，我们通过加载的模型传递一个图像，以便获得带有类标签、边界框坐标和遮罩的输出变量。</p><pre class="kg kh ki kj gt lu lv lw lx aw ly bi"><span id="51d0" class="lz ma iq lv b gy mb mc l md me">image = cv2.imread("&lt;image_path&amp;name&gt;")<br/>image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/>image = imutils.resize(image, width=512)<br/># perform a forward pass of the network to obtain the results<br/>print("[INFO] making predictions with Mask R-CNN...")<br/>result = model.detect([image], verbose=1</span></pre><p id="3d28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">步骤六:可视化输出</strong></p><pre class="kg kh ki kj gt lu lv lw lx aw ly bi"><span id="ff71" class="lz ma iq lv b gy mb mc l md me">r1 = result[0]<br/>visualize.display_instances(image, r1['rois'], r1['masks'],   r1['class_ids'], CLASS_NAMES, r1['scores'])</span></pre><p id="73cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">样本输出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/b1300039db5548aaab7f8fe41714425e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ypGd9SaFNP6-URZWWWTl7A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在 COCO 上训练的 Mask R-CNN 模型创建了我同学的像素地图。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/d7df6ad8bdfe57e956ee522027dbfbf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1cYZ22FDqbyxWjRwSG6dRA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">面具 R-CNN 视角下的印度拥挤街道</p></figure><p id="1c1b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总结这篇文章，我会说实例分割是对象检测的一个进一步的步骤，因为它产生了图像的像素掩码。更快的 R-CNN 计算量很大，我们在 Mask R-CNN 的基础上引入了实例分割。因此，掩模 R-CNN 在计算上变得更加昂贵。这使得 Mask R-CNN 很难在 CPU 上实时运行。</p><p id="3bcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献</strong></p><p id="4667" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K" rel="noopener ugc nofollow" target="_blank">明凯·何</a>、<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gkioxari%2C+G" rel="noopener ugc nofollow" target="_blank">乔治亚·格基奥萨里</a>、<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Doll%C3%A1r%2C+P" rel="noopener ugc nofollow" target="_blank">彼得·多尔拉</a>、<a class="ae kv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Girshick%2C+R" rel="noopener ugc nofollow" target="_blank">罗斯·吉尔希克</a>、马斯克 R-CNN、<a class="ae kv" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">来源</a></p></div></div>    
</body>
</html>