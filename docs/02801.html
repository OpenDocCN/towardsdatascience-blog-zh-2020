<html>
<head>
<title>PyTorch [Tabular] —Multiclass Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">py torch[表格]-多类分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab?source=collection_archive---------1-----------------------#2020-03-18">https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab?source=collection_archive---------1-----------------------#2020-03-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/840b4e62b667c78ce2f768c8b11e4488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Dsdw-L4qVhT1WkyLvtsPg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">如何训练你的神经网络[图片[0]]</p></figure><h2 id="1b71" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/akshaj-wields-pytorch" rel="noopener">如何训练你的神经网络</a></h2><div class=""/><div class=""><h2 id="c2ac" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">这篇博文将带您了解使用 PyTorch 对表格数据进行多类分类的实现。</h2></div><p id="453b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们将使用 Kaggle 上的<a class="ae lz" href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009" rel="noopener ugc nofollow" target="_blank">葡萄酒数据集</a>。该数据集有 12 列，其中前 11 列是要素，最后一列是目标列。数据集有 1599 行。</p><h1 id="b3a9" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">导入库</h1><p id="48b9" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">我们使用<code class="fe mx my mz na b">tqdm</code>来启用训练和测试循环的进度条。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="153b" class="nj mb jf na b gy nk nl l nm nn">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from tqdm.notebook import tqdm<br/>import matplotlib.pyplot as plt<br/><br/>import torch<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler<br/><br/>from sklearn.preprocessing import MinMaxScaler    <br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import confusion_matrix, classification_report</span></pre><h1 id="414b" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">读出数据</h1><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="9294" class="nj mb jf na b gy nk nl l nm nn">df = pd.read_csv("data/tabular/classification/winequality-red.csv")</span><span id="a137" class="nj mb jf na b gy no nl l nm nn">df.head()</span></pre><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/d6076b9ef757739b113525f718219e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jM0yesrdrjHwEuuIKx0rFA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">输入数据[图像[2]]</p></figure><h1 id="26d1" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">EDA 和预处理</h1><p id="c890" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">为了使数据适合神经网络，我们需要对其进行一些调整。</p><h2 id="ceed" class="nj mb jf bd mc nq nr dn mg ns nt dp mk lm nu nv mm lq nw nx mo lu ny nz mq jl bi translated">阶级分布</h2><p id="6489" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">首先，我们绘制输出行来观察类分布。这里有很多不平衡。类别 3、4 和 8 的样本数量很少。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="6c15" class="nj mb jf na b gy nk nl l nm nn">sns.countplot(x = 'quality', data=df)</span></pre><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/519a35a132a4d93139865ba7322018ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*4KuhNm6wXTEDWSHW0t5wUQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">类别分布条形图[图像[3]]</p></figure><h2 id="9088" class="nj mb jf bd mc nq nr dn mg ns nt dp mk lm nu nv mm lq nw nx mo lu ny nz mq jl bi translated">编码输出类别</h2><p id="746b" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">接下来，我们看到输出标签从 3 到 8。这需要改变，因为 PyTorch 支持从 0 开始的标签。也就是<strong class="lf jp">【0，n】</strong>。我们需要从 0 开始重新映射我们的标签。</p><p id="a4e1" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为此，让我们创建一个名为<code class="fe mx my mz na b">class2idx</code>的字典，并使用 Pandas 库中的<code class="fe mx my mz na b">.replace()</code>方法来修改它。让我们也创建一个名为<code class="fe mx my mz na b">idx2class</code>的反向映射，它将 id 转换回它们原来的类。</p><p id="a5bd" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了创建反向映射，我们创建一个字典理解并简单地反转键和值。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="bbf3" class="nj mb jf na b gy nk nl l nm nn">class2idx = {<br/>    3:0,<br/>    4:1,<br/>    5:2,<br/>    6:3,<br/>    7:4,<br/>    8:5<br/>}<br/><br/>idx2class = {v: k for k, v in class2idx.items()}<br/><br/>df['quality'].replace(class2idx, inplace=True)</span></pre><h2 id="7166" class="nj mb jf bd mc nq nr dn mg ns nt dp mk lm nu nv mm lq nw nx mo lu ny nz mq jl bi translated">创建输入和输出数据</h2><p id="25ef" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">为了使用来自 Sklearn 的<code class="fe mx my mz na b">train_test_split</code>将我们的数据分成训练、验证和测试集，我们需要分离出我们的输入和输出。</p><p id="9128" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">输入<code class="fe mx my mz na b">X</code>是除最后一列之外的所有列。输出<code class="fe mx my mz na b">y</code>是最后一列。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="1637" class="nj mb jf na b gy nk nl l nm nn">X = df.iloc[:, 0:-1]<br/>y = df.iloc[:, -1]</span></pre><h2 id="9f2a" class="nj mb jf bd mc nq nr dn mg ns nt dp mk lm nu nv mm lq nw nx mo lu ny nz mq jl bi translated">培训-验证-测试</h2><p id="f577" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">为了创建 train-val-test 分割，我们将使用来自 Sklearn 的<code class="fe mx my mz na b">train_test_split()</code>。</p><p id="fc7f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">首先，我们将把我们的数据分成 train+val 和 test 集。然后，我们将进一步分割我们的 train+val 集合来创建我们的 train 和 val 集合。</p><p id="7dd7" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">因为存在类不平衡，所以我们希望在我们的训练、验证和测试集中，所有输出类的分布是均等的。为此，我们使用函数<code class="fe mx my mz na b">train_test_split()</code>中的<code class="fe mx my mz na b">stratify</code>选项。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="083a" class="nj mb jf na b gy nk nl l nm nn"># Split into train+val and test<br/>X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=69)<br/><br/># Split train into train-val<br/>X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=21)</span></pre><h2 id="58b2" class="nj mb jf bd mc nq nr dn mg ns nt dp mk lm nu nv mm lq nw nx mo lu ny nz mq jl bi translated">标准化输入</h2><p id="399e" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">神经网络需要介于(0，1)范围内的数据。网上有很多关于我们为什么需要这么做的资料。</p><p id="a805" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了衡量我们的价值，我们将使用 Sklearn 中的<code class="fe mx my mz na b">MinMaxScaler()</code>。<code class="fe mx my mz na b">MinMaxScaler</code>通过将每个特征缩放到给定的范围(在我们的例子中是(0，1 ))来转换特征。</p><blockquote class="ob"><p id="7d9e" class="oc od jf bd oe of og oh oi oj ok ly dk translated"><em class="ol">x _ scaled =(x-min(x))/(max(x)-min(x))</em></p></blockquote><p id="f334" class="pw-post-body-paragraph ld le jf lf b lg om kp li lj on ks ll lm oo lo lp lq op ls lt lu oq lw lx ly ij bi translated">注意，我们在<code class="fe mx my mz na b">X_train</code>上使用<code class="fe mx my mz na b">.fit_transform()</code>，而在<code class="fe mx my mz na b">X_val</code>和<code class="fe mx my mz na b">X_test</code>上使用<code class="fe mx my mz na b">.transform()</code>。</p><p id="1aa1" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们这样做是因为我们希望使用与训练集相同的参数来扩展验证和测试集，以避免数据泄漏。<code class="fe mx my mz na b">fit_transform</code>计算并应用缩放值，而<code class="fe mx my mz na b">.transform</code>仅应用计算值。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="ebf0" class="nj mb jf na b gy nk nl l nm nn">scaler = MinMaxScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_val = scaler.transform(X_val)<br/>X_test = scaler.transform(X_test)</span><span id="ea3a" class="nj mb jf na b gy no nl l nm nn">X_train, y_train = np.array(X_train), np.array(y_train)<br/>X_val, y_val = np.array(X_val), np.array(y_val)<br/>X_test, y_test = np.array(X_test), np.array(y_test)</span></pre><h2 id="2800" class="nj mb jf bd mc nq nr dn mg ns nt dp mk lm nu nv mm lq nw nx mo lu ny nz mq jl bi translated">可视化培训、评估和测试中的类别分布</h2><p id="42a9" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">一旦我们将数据分成训练集、验证集和测试集，让我们确保类在所有三个集中的分布是相等的。</p><p id="a9f0" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为此，让我们创建一个名为<code class="fe mx my mz na b">get_class_distribution()</code>的函数。该函数将 obj <code class="fe mx my mz na b">y</code>即。<code class="fe mx my mz na b">y_train</code>、<code class="fe mx my mz na b">y_val</code>或<code class="fe mx my mz na b">y_test</code>。在函数内部，我们初始化一个字典，其中包含作为键的输出类和作为值的输出类计数。计数都被初始化为 0。</p><p id="66c7" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然后我们循环遍历我们的<code class="fe mx my mz na b">y</code>对象并更新我们的字典。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="6d91" class="nj mb jf na b gy nk nl l nm nn">def get_class_distribution(obj):<br/>    count_dict = {<br/>        "rating_3": 0,<br/>        "rating_4": 0,<br/>        "rating_5": 0,<br/>        "rating_6": 0,<br/>        "rating_7": 0,<br/>        "rating_8": 0,<br/>    }<br/>    <br/>    for i in obj:<br/>        if i == 0: <br/>            count_dict['rating_3'] += 1<br/>        elif i == 1: <br/>            count_dict['rating_4'] += 1<br/>        elif i == 2: <br/>            count_dict['rating_5'] += 1<br/>        elif i == 3: <br/>            count_dict['rating_6'] += 1<br/>        elif i == 4: <br/>            count_dict['rating_7'] += 1  <br/>        elif i == 5: <br/>            count_dict['rating_8'] += 1              <br/>        else:<br/>            print("Check classes.")<br/>            <br/>    return count_dict</span></pre><p id="60a5" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">一旦我们有了字典计数，我们就使用 Seaborn 库来绘制条形图。为了进行绘图，我们首先使用<code class="fe mx my mz na b">pd.DataFrame.from_dict([get_class_distribution(y_train)])</code>将字典转换成数据帧。随后，我们<code class="fe mx my mz na b">.melt()</code>将数据帧转换成长格式，最后使用<code class="fe mx my mz na b">sns.barplot()</code>构建绘图。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="3429" class="nj mb jf na b gy nk nl l nm nn">fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(25,7))</span><span id="b3ce" class="nj mb jf na b gy no nl l nm nn"># Train<br/>sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(y_train)]).melt(), x = "variable", y="value", hue="variable",  ax=axes[0]).set_title('Class Distribution in Train Set')</span><span id="06dc" class="nj mb jf na b gy no nl l nm nn"># Validation<br/>sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(y_val)]).melt(), x = "variable", y="value", hue="variable",  ax=axes[1]).set_title('Class Distribution in Val Set')</span><span id="f73c" class="nj mb jf na b gy no nl l nm nn"># Test<br/>sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(y_test)]).melt(), x = "variable", y="value", hue="variable",  ax=axes[2]).set_title('Class Distribution in Test Set')</span></pre><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/093a902083c1d785aeca19bc2b54a5ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PC9mp_Q1OgXCrosCuKpG5g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">训练集、val 集和测试集中的类分布[图片[4]]</p></figure><h1 id="6c0e" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">神经网络</h1><p id="8c4a" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">我们现在达到了我们一直在等待的目标！</p><h2 id="70f0" class="nj mb jf bd mc nq nr dn mg ns nt dp mk lm nu nv mm lq nw nx mo lu ny nz mq jl bi translated">自定义数据集</h2><p id="09ac" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">首先，让我们定义一个自定义数据集。数据加载器将使用该数据集将我们的数据传递到模型中。</p><p id="35d4" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们通过传递 X 和 y 作为输入来初始化数据集。确保 X 是<code class="fe mx my mz na b">float</code>而 y 是<code class="fe mx my mz na b">long</code>。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="856c" class="nj mb jf na b gy nk nl l nm nn">class ClassifierDataset(Dataset):<br/>    <br/>    def __init__(self, X_data, y_data):<br/>        self.X_data = X_data<br/>        self.y_data = y_data<br/>        <br/>    def __getitem__(self, index):<br/>        return self.X_data[index], self.y_data[index]<br/>        <br/>    def __len__ (self):<br/>        return len(self.X_data)<br/><br/><br/>train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())</span><span id="e04e" class="nj mb jf na b gy no nl l nm nn">val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())</span><span id="2b29" class="nj mb jf na b gy no nl l nm nn">test_dataset = ClassifierDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())</span></pre><h2 id="b741" class="nj mb jf bd mc nq nr dn mg ns nt dp mk lm nu nv mm lq nw nx mo lu ny nz mq jl bi translated">加权抽样</h2><p id="baed" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">因为存在类别不平衡，所以我们使用分层分割来创建我们的训练、验证和测试集。</p><p id="9212" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">虽然它有所帮助，但它仍然不能确保我们模型的每个小批量都能看到我们的所有类。我们需要对值数量较少的类进行过采样。为此，我们使用<code class="fe mx my mz na b">WeightedRandomSampler</code>。</p><p id="5017" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">首先，我们获得一个名为<code class="fe mx my mz na b">target_list</code>的列表，其中包含我们所有的输出。这个列表然后被转换成张量。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="9289" class="nj mb jf na b gy nk nl l nm nn">target_list = []</span><span id="225c" class="nj mb jf na b gy no nl l nm nn">for _, t in train_dataset:<br/>    target_list.append(t)<br/>    <br/>target_list = torch.tensor(target_list)</span></pre><p id="39a0" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然后，我们获得训练集中所有类的计数。我们使用每个计数的倒数来获得它的重量。现在我们已经计算了每个类的权重，我们可以继续了。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="dacd" class="nj mb jf na b gy nk nl l nm nn">class_count = [i for i in get_class_distribution(y_train).values()]<br/>class_weights = 1./torch.tensor(class_count, dtype=torch.float) </span><span id="e9e1" class="nj mb jf na b gy no nl l nm nn">print(class_weights)<br/></span><span id="37e7" class="nj mb jf na b gy no nl l nm nn">###################### OUTPUT ######################</span><span id="ffd8" class="nj mb jf na b gy no nl l nm nn">tensor([0.1429, 0.0263, 0.0020, 0.0022, 0.0070, 0.0714])</span></pre><p id="23c1" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><code class="fe mx my mz na b">WeightedRandomSampler</code>期望每个<strong class="lf jp">样品</strong>有一个<strong class="lf jp">重量</strong>。我们使用下面的方法来实现。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="57f9" class="nj mb jf na b gy nk nl l nm nn">class_weights_all = class_weights[target_list]</span></pre><p id="005a" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最后，让我们初始化我们的<code class="fe mx my mz na b">WeightedRandomSampler</code>。在下面的数据加载器中，我们称之为。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="e8cd" class="nj mb jf na b gy nk nl l nm nn">weighted_sampler = WeightedRandomSampler(<br/>    weights=class_weights_all,<br/>    num_samples=len(class_weights_all),<br/>    replacement=True<br/>)</span></pre><h2 id="dbba" class="nj mb jf bd mc nq nr dn mg ns nt dp mk lm nu nv mm lq nw nx mo lu ny nz mq jl bi translated">模型参数</h2><p id="b184" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">在我们继续下一步之前，让我们定义几个我们将使用的参数。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="7b94" class="nj mb jf na b gy nk nl l nm nn">EPOCHS = 300<br/>BATCH_SIZE = 16<br/>LEARNING_RATE = 0.0007</span><span id="05c1" class="nj mb jf na b gy no nl l nm nn">NUM_FEATURES = len(X.columns)<br/>NUM_CLASSES = 6</span></pre><h2 id="b230" class="nj mb jf bd mc nq nr dn mg ns nt dp mk lm nu nv mm lq nw nx mo lu ny nz mq jl bi translated">数据加载器</h2><p id="cf2e" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">现在让我们初始化我们的数据加载器。</p><p id="c375" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">对于<code class="fe mx my mz na b">train_dataloader </code>,我们将使用<code class="fe mx my mz na b">batch_size = 64</code>,并将我们的采样器传递给它。注意，我们没有在<code class="fe mx my mz na b">train_dataloader</code>中使用<code class="fe mx my mz na b">shuffle=True</code>，因为我们已经使用了一个采样器。这两个是互斥的。</p><p id="234b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">对于<code class="fe mx my mz na b">test_dataloader</code>和<code class="fe mx my mz na b">val_dataloader</code>，我们将使用<code class="fe mx my mz na b">batch_size = 1</code>。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="e128" class="nj mb jf na b gy nk nl l nm nn">train_loader = DataLoader(dataset=train_dataset,<br/>                          batch_size=BATCH_SIZE,<br/>                          sampler=weighted_sampler<br/>)</span><span id="b86f" class="nj mb jf na b gy no nl l nm nn">val_loader = DataLoader(dataset=val_dataset, batch_size=1)</span><span id="7e95" class="nj mb jf na b gy no nl l nm nn">test_loader = DataLoader(dataset=test_dataset, batch_size=1)</span></pre><h2 id="6c2e" class="nj mb jf bd mc nq nr dn mg ns nt dp mk lm nu nv mm lq nw nx mo lu ny nz mq jl bi translated">定义神经网络架构</h2><p id="a11c" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">让我们定义一个简单的三层前馈网络，具有漏失和批量范数。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="134a" class="nj mb jf na b gy nk nl l nm nn">class MulticlassClassification(nn.Module):<br/>    def __init__(self, num_feature, num_class):<br/>        super(MulticlassClassification, self).__init__()<br/>        <br/>        self.layer_1 = nn.Linear(num_feature, 512)<br/>        self.layer_2 = nn.Linear(512, 128)<br/>        self.layer_3 = nn.Linear(128, 64)<br/>        self.layer_out = nn.Linear(64, num_class) <br/>        <br/>        self.relu = nn.ReLU()<br/>        self.dropout = nn.Dropout(p=0.2)<br/>        self.batchnorm1 = nn.BatchNorm1d(512)<br/>        self.batchnorm2 = nn.BatchNorm1d(128)<br/>        self.batchnorm3 = nn.BatchNorm1d(64)<br/>        <br/>    def forward(self, x):<br/>        x = self.layer_1(x)<br/>        x = self.batchnorm1(x)<br/>        x = self.relu(x)<br/>        <br/>        x = self.layer_2(x)<br/>        x = self.batchnorm2(x)<br/>        x = self.relu(x)<br/>        x = self.dropout(x)<br/>        <br/>        x = self.layer_3(x)<br/>        x = self.batchnorm3(x)<br/>        x = self.relu(x)<br/>        x = self.dropout(x)<br/>        <br/>        x = self.layer_out(x)<br/>        <br/>        return x</span></pre><p id="b9ea" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">检查 GPU 是否处于活动状态。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="c8fc" class="nj mb jf na b gy nk nl l nm nn">device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")</span><span id="7a59" class="nj mb jf na b gy no nl l nm nn">print(device)</span><span id="6193" class="nj mb jf na b gy no nl l nm nn"><br/>###################### OUTPUT ######################</span><span id="9cc5" class="nj mb jf na b gy no nl l nm nn">cuda:0</span></pre><p id="695f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">初始化模型、优化器和损失函数。将模型传输到 GPU。我们使用<code class="fe mx my mz na b">nn.CrossEntropyLoss</code>是因为这是一个多类分类问题。我们不需要在最后一层之后手动添加一个<code class="fe mx my mz na b">log_softmax</code>层，因为<code class="fe mx my mz na b">nn.CrossEntropyLoss</code>已经为我们做了。然而，我们需要应用<code class="fe mx my mz na b">log_softmax</code>进行验证和测试。</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi os"><img src="../Images/ab90bacbda712e3a083de63408b0821b.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*Pc0SWzi042_Hq2XaH0eSOQ.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">损失函数 meme [Image [5]]</p></figure><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="a386" class="nj mb jf na b gy nk nl l nm nn">model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)</span><span id="d1ac" class="nj mb jf na b gy no nl l nm nn">model.to(device)<br/><br/>criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))<br/>optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)</span><span id="afbe" class="nj mb jf na b gy no nl l nm nn">print(model)</span><span id="4a4d" class="nj mb jf na b gy no nl l nm nn"><br/>###################### OUTPUT ######################</span><span id="3771" class="nj mb jf na b gy no nl l nm nn">MulticlassClassification(<br/>  (layer_1): Linear(in_features=11, out_features=512, bias=True)<br/>  (layer_2): Linear(in_features=512, out_features=128, bias=True)<br/>  (layer_3): Linear(in_features=128, out_features=64, bias=True)<br/>  (layer_out): Linear(in_features=64, out_features=6, bias=True)<br/>  (relu): ReLU()<br/>  (dropout): Dropout(p=0.2, inplace=False)<br/>  (batchnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>)</span></pre><h1 id="94e9" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">训练模型</h1><p id="5119" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">在我们开始训练之前，让我们定义一个函数来计算每个历元的精度。</p><p id="de1a" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">该函数将<code class="fe mx my mz na b">y_pred</code>和<code class="fe mx my mz na b">y_test</code>作为输入参数。然后，我们将<code class="fe mx my mz na b">log_softmax</code>应用于<code class="fe mx my mz na b">y_pred</code>，并提取出具有较高概率的类别。</p><p id="ec52" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">之后，我们比较预测类别和实际类别来计算准确度。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="ed80" class="nj mb jf na b gy nk nl l nm nn">def multi_acc(y_pred, y_test):<br/>    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)<br/>    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    <br/>    <br/>    correct_pred = (y_pred_tags == y_test).float()<br/>    acc = correct_pred.sum() / len(correct_pred)<br/>    <br/>    acc = torch.round(acc * 100)<br/>    <br/>    return acc</span></pre><p id="c17f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们还将定义 2 个字典，用于存储训练集和验证集的准确度/时期和损失/时期。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="ccd4" class="nj mb jf na b gy nk nl l nm nn">accuracy_stats = {<br/>    'train': [],<br/>    "val": []<br/>}</span><span id="cf1d" class="nj mb jf na b gy no nl l nm nn">loss_stats = {<br/>    'train': [],<br/>    "val": []<br/>}</span></pre><p id="ee12" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们来看看我们的模型吧！</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ot"><img src="../Images/c7a59562bea9067b4a7ade0f84f43dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5hLWMX9u6_yAvFPg.jpg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">训练迷因[图片[6]]</p></figure><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="a13b" class="nj mb jf na b gy nk nl l nm nn">print("Begin training.")</span><span id="4f80" class="nj mb jf na b gy no nl l nm nn">for e in tqdm(range(1, EPOCHS+1)):<br/>    <br/>    # TRAINING<br/>    train_epoch_loss = 0<br/>    train_epoch_acc = 0</span><span id="8303" class="nj mb jf na b gy no nl l nm nn">model.train()<br/>    for X_train_batch, y_train_batch in train_loader:<br/>        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)<br/>        optimizer.zero_grad()<br/>        <br/>        y_train_pred = model(X_train_batch)<br/>        <br/>        train_loss = criterion(y_train_pred, y_train_batch)<br/>        train_acc = multi_acc(y_train_pred, y_train_batch)<br/>        <br/>        train_loss.backward()<br/>        optimizer.step()<br/>        <br/>        train_epoch_loss += train_loss.item()<br/>        train_epoch_acc += train_acc.item()<br/>        <br/>        <br/>    # VALIDATION    <br/>    with torch.no_grad():<br/>        <br/>        val_epoch_loss = 0<br/>        val_epoch_acc = 0<br/>        <br/>        model.eval()<br/>        for X_val_batch, y_val_batch in val_loader:<br/>            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)<br/>            <br/>            y_val_pred = model(X_val_batch)<br/>                        <br/>            val_loss = criterion(y_val_pred, y_val_batch)<br/>            val_acc = multi_acc(y_val_pred, y_val_batch)<br/>            <br/>            val_epoch_loss += val_loss.item()<br/>            val_epoch_acc += val_acc.item()</span><span id="bb3f" class="nj mb jf na b gy no nl l nm nn">loss_stats['train'].append(train_epoch_loss/len(train_loader))<br/>    loss_stats['val'].append(val_epoch_loss/len(val_loader))<br/>    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))<br/>    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))<br/>                              <br/>    <br/>    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')</span><span id="ad36" class="nj mb jf na b gy no nl l nm nn"><br/>###################### OUTPUT ######################</span><span id="db36" class="nj mb jf na b gy no nl l nm nn">Epoch 001: | Train Loss: 1.38551 | Val Loss: 1.42033 | Train Acc: 38.889| Val Acc: 43.750</span><span id="956b" class="nj mb jf na b gy no nl l nm nn">Epoch 002: | Train Loss: 1.19558 | Val Loss: 1.36613 | Train Acc: 59.722| Val Acc: 45.312</span><span id="fc65" class="nj mb jf na b gy no nl l nm nn">Epoch 003: | Train Loss: 1.12264 | Val Loss: 1.44156 | Train Acc: 79.167| Val Acc: 35.938</span><span id="fb8a" class="nj mb jf na b gy no nl l nm nn">.<br/>.<br/>.</span><span id="021a" class="nj mb jf na b gy no nl l nm nn">Epoch 299: | Train Loss: 0.29774 | Val Loss: 1.42116 | Train Acc: 100.000| Val Acc: 57.812</span><span id="3ed7" class="nj mb jf na b gy no nl l nm nn">Epoch 300: | Train Loss: 0.33134 | Val Loss: 1.38818 | Train Acc: 100.000| Val Acc: 57.812</span></pre><p id="4086" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">你可以看到我们已经在循环之前放置了一个<code class="fe mx my mz na b">model.train()</code>。<code class="fe mx my mz na b">model.train()</code>告诉 PyTorch 你正处于训练模式。</p><p id="9a2d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为什么我们需要这么做？如果您使用在训练和评估期间表现不同的层，如<code class="fe mx my mz na b">Dropout</code>或<code class="fe mx my mz na b">BatchNorm</code>(例如<em class="ou">；评估期间不使用 dropout</em>)，您需要告诉 PyTorch 采取相应的行动。</p><p id="3773" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">同样，当我们测试我们的模型时，我们将调用<code class="fe mx my mz na b">model.eval()</code>。我们将在下面看到。</p><p id="0e31" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">回到训练；我们开始一个循环。在这个<em class="ou"> for 循环</em>的顶部，我们将每个历元的损耗和精度初始化为 0。在每个时期之后，我们将打印出损失/精度并将其重置回 0。</p><p id="323b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然后我们有另一个<em class="ou"> for-loop </em>。这个<em class="ou"> for 循环</em>用于从<code class="fe mx my mz na b">train_loader</code>批量获取我们的数据。</p><p id="1e37" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在我们做任何预测之前，我们做<code class="fe mx my mz na b">optimizer.zero_grad()</code>。由于<code class="fe mx my mz na b">backward()</code>函数累加梯度，我们需要为每个小批量手动将其设置为 0。</p><p id="d3e9" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">从我们定义的模型中，我们获得一个预测，得到小批量的损失(和精度)，使用<code class="fe mx my mz na b">loss.backward()</code>和<code class="fe mx my mz na b">optimizer.step()</code>执行反向传播。</p><p id="13fc" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最后，我们将所有小批量损失(和精度)相加，以获得该时期的平均损失(和精度)。我们将每个小批量的所有损耗/精度相加，最后除以小批量的数量，即。<code class="fe mx my mz na b">train_loader</code>的长度，以获得每个历元的平均损失/精度。</p><p id="2923" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们遵循的训练程序与验证程序完全相同，除了我们用<code class="fe mx my mz na b">torch.no_grad</code>将它包装起来，并且不执行任何反向传播。<code class="fe mx my mz na b">torch.no_grad()</code>告诉 PyTorch 我们不想执行反向传播，这样可以减少内存使用并加快计算速度。</p><h1 id="9114" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">可视化损失和准确性</h1><p id="9ede" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">为了绘制损耗和精度线图，我们再次从<code class="fe mx my mz na b">accuracy_stats</code>和<code class="fe mx my mz na b">loss_stats</code>字典中创建一个数据帧。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="3a8b" class="nj mb jf na b gy nk nl l nm nn"># Create dataframes<br/>train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})</span><span id="0680" class="nj mb jf na b gy no nl l nm nn">train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})</span><span id="a322" class="nj mb jf na b gy no nl l nm nn"># Plot the dataframes<br/>fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))</span><span id="a7f1" class="nj mb jf na b gy no nl l nm nn">sns.lineplot(data=train_val_acc_df, x = "epochs", y="value", hue="variable",  ax=axes[0]).set_title('Train-Val Accuracy/Epoch')</span><span id="6aaf" class="nj mb jf na b gy no nl l nm nn">sns.lineplot(data=train_val_loss_df, x = "epochs", y="value", hue="variable", ax=axes[1]).set_title('Train-Val Loss/Epoch')</span></pre><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ov"><img src="../Images/8e6f7ce2af1d3708cfc45a878cf239af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7EJClqoyEabU7P7X_PGIkQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">损失和精度图[图 7]]</p></figure><h1 id="78db" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">测试模型</h1><p id="ee08" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">训练完成后，我们需要测试我们的模型进展如何。注意，在运行测试代码之前，我们已经使用了<code class="fe mx my mz na b">model.eval()</code>。为了告诉 PyTorch 我们不希望在推断过程中执行反向传播，我们使用了<code class="fe mx my mz na b">torch.no_grad()</code>，就像我们对上面的验证循环所做的那样。</p><p id="a194" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们首先定义一个包含我们预测的列表。然后我们使用<code class="fe mx my mz na b">test_loader</code>循环遍历我们的批处理。对于每一批—</p><ul class=""><li id="ac7c" class="ow ox jf lf b lg lh lj lk lm oy lq oz lu pa ly pb pc pd pe bi translated">我们将输入小批量数据转移到 GPU。</li><li id="a373" class="ow ox jf lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">我们使用训练好的模型进行预测。</li><li id="a4e6" class="ow ox jf lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">将<code class="fe mx my mz na b">log_softmax</code>激活应用于预测，并选择概率最高的指数。</li><li id="7fa1" class="ow ox jf lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">将批处理从 CPU 移动到 GPU。</li><li id="a4ff" class="ow ox jf lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">将张量转换为 numpy 对象，并将其添加到我们的列表中。</li><li id="522b" class="ow ox jf lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">将列表展平，这样我们可以将它用作<code class="fe mx my mz na b">confusion_matrix</code>和<code class="fe mx my mz na b">classification_report</code>的输入。</li></ul><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="1fbd" class="nj mb jf na b gy nk nl l nm nn">y_pred_list = []</span><span id="1cb5" class="nj mb jf na b gy no nl l nm nn">with torch.no_grad():<br/>    model.eval()<br/>    for X_batch, _ in test_loader:<br/>        X_batch = X_batch.to(device)<br/>        y_test_pred = model(X_batch)<br/>        _, y_pred_tags = torch.max(y_test_pred, dim = 1)<br/>        y_pred_list.append(y_pred_tags.cpu().numpy())</span><span id="0c5d" class="nj mb jf na b gy no nl l nm nn">y_pred_list = [a.squeeze().tolist() for a in y_pred_list]</span></pre><h1 id="dc41" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">混淆矩阵</h1><p id="38f6" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">我们从混淆矩阵中创建一个数据框架，并使用 seaborn 库将其绘制为热图。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="6942" class="nj mb jf na b gy nk nl l nm nn">confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, y_pred_list)).rename(columns=idx2class, index=idx2class)</span><span id="58eb" class="nj mb jf na b gy no nl l nm nn"><br/>sns.heatmap(confusion_matrix_df, annot=True)</span></pre><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pk"><img src="../Images/26d9e546bbe9f41821061660cb699528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3OnsbHIdaXe2h2SpjNKSIw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">混淆矩阵[图片[8]]</p></figure><h1 id="bed2" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">分类报告</h1><p id="9246" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">最后，我们打印出包含精确度、召回率和 F1 分数的分类报告。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="b1cd" class="nj mb jf na b gy nk nl l nm nn">print(classification_report(y_test, y_pred_list))</span><span id="7b8e" class="nj mb jf na b gy no nl l nm nn"><br/>###################### OUTPUT ######################</span><span id="e4f7" class="nj mb jf na b gy no nl l nm nn">precision    recall  f1-score   support<br/><br/>           0       0.00      0.00      0.00         2<br/>           1       0.14      0.27      0.19        11<br/>           2       0.70      0.65      0.67       136<br/>           3       0.63      0.57      0.60       128<br/>           4       0.49      0.60      0.54        40<br/>           5       0.00      0.00      0.00         3<br/><br/>    accuracy                           0.59       320<br/>   macro avg       0.33      0.35      0.33       320<br/>weighted avg       0.62      0.59      0.60       320</span></pre></div><div class="ab cl pl pm hu pn" role="separator"><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq"/></div><div class="ij ik il im in"><p id="9d7b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">感谢您的阅读。欢迎提出建议和建设性的批评。:)</p><p id="0d74" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这篇博客是“如何训练你的神经网络”系列的一部分。你可以在这里找到系列<a class="ae lz" href="https://towardsdatascience.com/tagged/akshaj-wields-pytorch" rel="noopener" target="_blank">。</a></p><p id="8834" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">你可以在<a class="ae lz" href="https://www.linkedin.com/in/akshajverma7/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lz" href="https://twitter.com/theairbend3r" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上找到我。如果你喜欢这个，看看我的其他<a class="ae lz" href="https://medium.com/@theairbend3r" rel="noopener">博客</a>。</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><a href="https://www.buymeacoffee.com/theairbend3r"><div class="gh gi ps"><img src="../Images/041a0c7464198414e6ce355f9235099e.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*SGCT6C60o4t58wRqeU2viQ.png"/></div></a></figure></div></div>    
</body>
</html>