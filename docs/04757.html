<html>
<head>
<title>Are all FLOPs created equal?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">所有的翻牌都是一样的吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/are-all-flops-created-equal-a-comparison-of-flops-vs-run-time-3cebb2dcb3da?source=collection_archive---------50-----------------------#2020-04-26">https://towardsdatascience.com/are-all-flops-created-equal-a-comparison-of-flops-vs-run-time-3cebb2dcb3da?source=collection_archive---------50-----------------------#2020-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="61f2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">FLOPs与运行时间的比较</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a9732e727e677b5b079e230aec2d959b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ms5sZ7R2rebj6FU9z3fUCQ.jpeg"/></div></div></figure><p id="5ba7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最初发布于<a class="ae lq" href="https://deci.ai/are-all-flops-created-equal-a-comparison-of-flops-vs-run-time/" rel="noopener ugc nofollow" target="_blank">https://deci . ai/are-all-flops-created-equal-a-comparison-of-flops-vs-run-time/</a></p><p id="7bec" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在深度学习时代的初期，我们的主要目标是达到新的准确性水平。事实上，深度学习的早期工作在各种任务和数据集上显示了显著的结果。然而，近年来，重点已经转移到最大限度地提高效率。如今，深度学习社区正在寻求推理效率提高的更准确的模型。深度学习推理中的效率概念取决于上下文。它可能指的是能耗、内存效率或运行时效率——这是本文的重点。</p><p id="0998" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对近年来开始出现的高效模型的分析通常假设浮点运算(FLOPs)可以作为任何类型的效率(包括运行时间)的准确代理。在串行与并行计算的环境中，这种假设显然是错误的；当N个工人可用时，挖壕沟(一个完全并行的任务)比挖井(一个固有的串行任务)快1/N。类似地，由于GPU的高并行化能力，可并行化的操作将比不可并行化的操作运行得更快。但是并行化考虑并不是一切，在这篇文章中，我们来看一个重要的例子，它表明FLOPS不足以衡量GPU计算环境中的运行时间。</p><p id="16c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我们的数值示例中，我们使用标准的CuPy和PyTorch库在GPU和CPU上执行矩阵乘法。我们证明了相同数量的FLOPs会导致不同的运行时间。我们的示例模拟了网络一层中的操作，其中我们研究了两个矩阵乘法。第一个乘积是一个NxN矩阵乘以一个Nx1向量。第二个是两个向量的外积，即一个Nx1矩阵与一个1xN矩阵的积，结果得到一个NxN矩阵。然后，为了在两种情况下获得相同数量的触发器，在执行外积之后，我们对每列使用N-1次加法来对矩阵的每列求和。图1中的图表示意性地描述了这两个操作。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lr"><img src="../Images/d023e7c068d026509664a549122bbda5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1ASZkB8nPUMdp47g"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">图1:两个矩阵/矢量产品消耗相同的FLOPs。左:矩阵*矢量积。右图:向量*向量外积，在外积之后，我们对每一列求和。</p></figure><p id="c981" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些操作中的每一个都消耗2 NxN-N个触发器，这些触发器被分解成NxN个浮点(FP)乘法和NxN-N个FP加法。</p><p id="ccaa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们现在考虑使用GPU加速这些计算(参见下面的Python代码)。图2中的图表描述了两种操作的运行时间与测得的触发器的关系。这些图是通过应用每种N值递增的产品，并在多次运行中求平均值而生成的。</p><div class="kj kk kl km gt ab cb"><figure class="lw kn lx ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/2183fae1eaebb04bf341708691512218.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/0*_zVLtUs-mOMpJM3l"/></div></figure><figure class="lw kn mc ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/c1ead2ba17711a9ef450a77816a5fb50.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/0*rEhlORZytRAY_YXv"/></div></figure><figure class="lw kn md ly lz ma mb paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/5938744c97d4f79f010e79db6e6ad581.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/0*n8whNgMpQekm0LFj"/></div><p class="ls lt gj gh gi lu lv bd b be z dk me di mf mg translated">图2:运行时间(单位为秒)与矩阵乘法的N(FLOPS = 2 nxn-N)和向量乘法及求和的函数关系。蓝色:矩阵*矢量；橙色:向量*向量(外积)。X轴(N)是矢量/矩阵的大小。从左到右:GPU上的CuPy，GPU上的PyTorch，CPU上的PyTorch。</p></figure></div><p id="e7c8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尽管每个场景中的FLOPs数量是相同的，但是它们的运行时间却有很大的不同。此外，随着N(矩阵中条目的数量)的增加，差异变得更大。例如，在CuPy曲线中很明显，当向量/矩阵大小为N=20，000时，外积大约比矩阵乘法长1.5倍。使用PyTorch，当N=20，000时，差异变得超过2倍长。</p><p id="af0e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于观察到的操作运行时间的巨大差异，以下是一些常见的解释。</p><h1 id="a92e" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">GPU计算的底层视图</h1><p id="7102" class="pw-post-body-paragraph ku kv it kw b kx mz ju kz la na jx lc ld nb lf lg lh nc lj lk ll nd ln lo lp im bi translated">为了理解什么可以区分FLOPs和运行时，我们需要首先理解GPU如何执行计算。在幕后，当计算深度神经网络中的层的值时，有两种操作:</p><p id="9324" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">乘法和累加(MAC) —此操作需要三个输入:内核(或关于内核的信息)、输入数据和从上一次迭代计算的部分和。这三个输入通过算术逻辑单元(ALU)传输，该单元执行乘法和加法的逐位运算。运算的输出是存储在单独的存储器单元中的部分和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/cbc4ae9571ca3166280609ee3c2f8ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/0*KCu7OLLOvzF9niQb"/></div></figure><p id="8dfc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">内存访问—此操作获取为每个MAC操作计算的部分和，并根据其大小将其存储在指定的内存单元中。粗略地说，有四种(有时更多)类型的存储单元:寄存器文件(RF)存储器，存储少量数据(最多1Kb)；处理引擎(PE ),存储更多的数据；缓冲单元；还有那杯酒。随着存储单元大小的增加，该单元的时间消耗也增加。在最糟糕的情况下，读取和写入DRAM的成本非常高，运行时间是存储在RF上的200多倍。</p><p id="976b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当我们计算失败次数时，我们实际上并不区分MAC和内存访问操作。然而，这些可能与运行时间有很大不同。一系列需要大量DRAM存储的MAC比同样数量的完全不使用DRAM的MAC花费的时间要多得多。</p><p id="13a8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">并行操作的能力是影响最终运行时间的另一个因素。与由几个为顺序串行处理而优化的内核组成的CPU相比，GPU由包含数千个较小内核的大规模并行架构组成。这为可以并行运行的网络结构提供了优势。例如，GPU的结构要求在一定的规模下，较宽较浅的网络在GPU上的运行速度要快于较深较细的网络。这是因为如果更广泛的网络的操作占用更多的GPU核心，则可以以并行方式进行计算，而更深入的网络的计算需要以更顺序的方式进行。操作的并行化取决于所使用的特定硬件和算法操作。</p><p id="e453" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">回到我们的数字示例，可以将运行时间的差异归因于两个因素:并行性和内存访问。也就是说，自定义实现可以减少运行时间的差异；因此，这种差异很大程度上取决于所用框架的具体实现。我们能够使用名为Nsight system的NVIDIA工具来识别特定的内存访问和并行性瓶颈。我们将在以后的文章中讨论这个话题。</p><h1 id="cef0" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">注重记忆的编程可以加快计算速度</h1><p id="8b06" class="pw-post-body-paragraph ku kv it kw b kx mz ju kz la na jx lc ld nb lf lg lh nc lj lk ll nd ln lo lp im bi translated">尽管我们在图2中观察到明显的差距，我们还是想举例说明该框架对结果的影响。使用高级框架时(例如CuPy、PyTorch、TensorFlow等。)，我们实际上是在使用一个预定义的算法。在不同的用例中，不同的算法表现出不同的性能。例如，2D数组的CuPy实现是通过重新索引1D数组的条目来实现的。这意味着当试图从2D数组中弹出元素时，一行中的相邻元素将比一列中的相邻元素更快地被检索到。由于这个原因，我们可以期待一个通用算法在行上比在列上更快地执行求和。回想一下，在图2的结果中，我们用CuPy对列进行了求和。我们现在表明，通过改变求和的维数，我们可以(几乎)使两个操作的运行时间相等。这可以从图3中看出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/629403c82640562da49b7a5434c773b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/0*KVnz6MAo1KPWSb0d"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">图3:矩阵乘法与向量乘法和求和的运行时间(秒)与触发器。蓝色:矩阵*矢量；橙色:向量*向量(外积)。X轴对应于矩阵中条目的数量。这个实现使用了CuPy，其中对行执行外积后的求和。</p></figure><p id="ae16" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尽管在这个具体的例子中，我们能够通过改变一行代码来改善结果，但是当使用更高级别的框架时，这并不总是可能的。例如，如果我们用PyTorch尝试同样的技巧，结果不会有明显的改善。此外，当涉及到真正的深度网络时，减少运行时间需要非常高的专业知识。</p><h1 id="d0a7" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">结论</h1><p id="6b62" class="pw-post-body-paragraph ku kv it kw b kx mz ju kz la na jx lc ld nb lf lg lh nc lj lk ll nd ln lo lp im bi translated">通过一个矩阵/向量乘积的具体例子，我们证明了在运行时，FLOPs不能作为效率的通用代理。这一事实经常被忽视，甚至在某些学术论文中也是如此。我们使用CuPy和PyTorch演示了两个矩阵/向量乘法，它们消耗完全相同的触发器数量，却可以有完全不同的运行时间。我们还简要讨论了这些差异的原因。</p><p id="b03b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> CuPy代码</strong></p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="fb16" class="nl mi it nh b gy nm nn l no np">import cupy as cp<br/>import time<br/>import os<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from cupy import cuda<br/>import pickle<br/><br/>def cp_Matmul(x, y, iterations):<br/>    for i in range(iterations): <br/>      q = cp.matmul(x, y, out=None)   <br/>    return q<br/><br/>def cp_Vecmul(x,y,iterations):<br/>    for i in range(iterations):<br/>        q = cp.matmul(x,y,out=None)<br/>        q = cp.sum(q,axis=1)      <br/>    return q<br/><br/>xmin = 1000<br/>xmax = 20000<br/>jump = 500<br/>lim = range(xmin,xmax,jump)<br/><br/>Matrix_result = []<br/>Vector_result = []<br/>iterations = 1000<br/><br/>for k in lim:<br/>    N = k<br/><br/>    vec_outer_1 = cp.random.rand(N,1)<br/>    vec_outer_2 = cp.random.rand(1,N)<br/><br/>    matrix = cp.random.rand(N,N)<br/>    vec_matrix = cp.random.rand(N,1)<br/><br/><br/>    start_event = cp.cuda.stream.Event()<br/>    stop_event = cp.cuda.stream.Event()<br/>    start_event.record()<br/>    q = cp_Vecmul(vec_outer_1,vec_outer_2,iterations)<br/>    stop_event.record()<br/>    stop_event.synchronize()<br/>    Vector_time = cp.cuda.get_elapsed_time(start_event, stop_event)/1000<br/>Matrix_result.append(Matrix_time)<br/>print('Total run time matrices', Matrix_time)</span></pre><p id="748c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> PyTorch代码</strong></p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="7429" class="nl mi it nh b gy nm nn l no np">import os<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import torch<br/>import pickle<br/><br/>def tr_Matmul(a, b, iterations):<br/>    for i in range(iterations):<br/>        q = torch.mm(a, b, out=None).to(device=cuda0)<br/>    return q<br/><br/>def tr_Vecmul(a,b,iterations):<br/>    for i in range(iterations):<br/>        q = torch.mm(a,b,out=None).to(device=cuda0)<br/>        q = torch.sum(q,dim=0).to(device=cuda0)<br/>    return q<br/><br/>xmin = 1000<br/>xmax = 20000<br/>jump = 500<br/>lim = range(xmin,xmax,jump)<br/><br/>Matrix_result = []<br/>Vector_result = []<br/>iterations = 1000<br/><br/>for k in lim:<br/>    N = k<br/>    cuda0 = torch.device(device='cuda')<br/><br/>    vec_outer_1 = torch.rand(N,1).to(device=cuda0)<br/>    vec_outer_2 = torch.rand(1,N).to(device=cuda0)<br/><br/>    matrix = torch.rand(N,N).to(device=cuda0)<br/>    vec_matrix = torch.rand(N,1).to(device=cuda0)<br/><br/><br/>    start_event = torch.cuda.Event(enable_timing=True)<br/>    stop_event = torch.cuda.Event(enable_timing=True)<br/>    start_event.record()<br/><br/>    q = tr_Vecmul(vec_outer_1,vec_outer_2,iterations)<br/>    stop_event.record()<br/>    torch.cuda.synchronize()<br/>    Vector_time = start_event.elapsed_time( stop_event)/1000<br/>    Vector_result.append(Vector_time)<br/>    print('Total run time vectors',Vector_time)<br/><br/>    start_event = torch.cuda.Event(enable_timing=True)<br/>    stop_event = torch.cuda.Event(enable_timing=True)<br/>    start_event.record()<br/>   <br/>    q = tr_Matmul(matrix, vec_matrix, iterations)<br/>    stop_event.record()<br/>    torch.cuda.synchronize()<br/>    Matrix_time =  start_event.elapsed_time( stop_event)/1000<br/>    Matrix_result.append(Matrix_time)<br/>    print('Total run time matrices', Matrix_time)</span></pre></div></div>    
</body>
</html>