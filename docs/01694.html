<html>
<head>
<title>What is Sparsemax?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是Sparsemax？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-sparsemax-f84c136624e4?source=collection_archive---------17-----------------------#2020-02-16">https://towardsdatascience.com/what-is-sparsemax-f84c136624e4?source=collection_archive---------17-----------------------#2020-02-16</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="d853" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">softmax的一个有用变体</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/bfb5b3d0087824fff61ec45b8969f411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KTHmsPzq5qk-YQuF"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">格雷格·罗森克在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="32d9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在机器学习中，有几个非常有用的函数，比如sigmoid，relu，softmax。后者作为神经网络的输出层广泛用于多类分类问题:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj lw"><img src="../Images/dd309e633027f2ee4587b7a8bc829fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*jwnBBjCjiXYEFHUjPxubrA.png"/></div></figure><p id="cbd4" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这个函数有一个有用的特性:它的元素之和是1，这使得它对概率建模非常有用。它也是处处可微的，并且导数决不为零，这使得它在反向传播算法中很有用。相比之下，调用softmax替换的argmax函数的导数始终为零。另一个有用的特性是，该函数保留了后验概率分布的支持，因为输出概率决不为零，无论它们可能是多么小的值。</p><p id="c33d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">但是有时候你想有一个稀疏的输出，只保留几个非零概率的值。对于这种情况，André F. T. Martins和Ramón F. Astudillo在他们的论文<a class="ae kz" href="http://proceedings.mlr.press/v48/martins16.pdf" rel="noopener ugc nofollow" target="_blank">从Softmax到sparsemax:注意力和多标签分类的稀疏模型</a>中提出了一个名为<em class="lx"> sparsemax </em>的新函数。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="ly lz l"/></div></figure><p id="f922" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">其思想是将z的最小值的概率设置为零，并且仅保持z的最高值的概率，但是仍然保持函数可微，以确保反向传播算法的成功应用。该功能定义为:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ma"><img src="../Images/2d89101dc3fc4209fea7c77e78df8a7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*VZ41y37jnMPwY3s_nLTnJg.png"/></div></figure><p id="f786" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这里<strong class="lc iv"> τ(z) </strong>称为<em class="lx">阈值函数</em>，它定义了包含<strong class="lc iv"> p </strong>的所有非零索引的<em class="lx">支持函数</em> <strong class="lc iv"> S(z) </strong>。sparsemax函数的python实现如下:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="mb lz l"/></div></figure><p id="fe45" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在相同的值上运行它和softmax，我们确实可以看到它将一些概率设置为零，而softmax将它们保持为非零:</p><pre class="kk kl km kn gu mc md me mf aw mg bi"><span id="de80" class="mh mi iu md b gz mj mk l ml mm">np.around(sparsemax([0.1,  1.1, 0.2, 0.3]), decimals=3)</span><span id="5593" class="mh mi iu md b gz mn mk l ml mm">array([0. , 0.9, 0. , 0.1])</span><span id="7284" class="mh mi iu md b gz mn mk l ml mm">np.around(softmax([0.1,  1.1, 0.2, 0.3]), decimals=3)</span><span id="48c1" class="mh mi iu md b gz mn mk l ml mm">array([0.165, 0.45 , 0.183, 0.202])</span></pre><p id="ede5" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">有趣的是，在二维情况下，这两个函数有什么不同。在这种情况下，softmax变成了一个<em class="lx"> sigmoid </em>函数，sparsemax可以用以下形式表示:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj mo"><img src="../Images/1681024548ee47ae81cc257f6121f389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*QhYvkUP8vLSLdhlB16Qt8g.png"/></div></figure><p id="f207" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">下图说明了它们的不同之处:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj mp"><img src="../Images/6b2361590092856ad88afb60286c4188.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*EK55_sEqmQDSXcIV2CGLKg.png"/></div></figure><p id="9262" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">注意，sparsemax函数并不是处处可微的(但<em class="lx"> relu </em>也不是)，但它在哪里是一个简单的计算:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj mq"><img src="../Images/1e33bbf066c0bcf3508adec1dcc65719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*bKGR5vYeB8Ei6GGcHUNyew.png"/></div></figure><p id="e761" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这里<strong class="lc iv"> |S(z)| </strong>是支撑S(z)中元素的个数。</p><h2 id="9772" class="mh mi iu bd mr ms mt dn mu mv mw dp mx lj my mz na ln nb nc nd lr ne nf ng nh bi translated">讨论</h2><p id="9396" class="pw-post-body-paragraph la lb iu lc b ld ni jv lf lg nj jy li lj nk ll lm ln nl lp lq lr nm lt lu lv in bi translated">sparsemax的明显问题是渐变消失。你可以看到，一旦z变大，导数变为零。作者承认这个问题，甚至提出了一个新的损失函数来代替交叉熵损失。然而，我认为sparsemax的主要优势不在输出层，而是在神经网络的中间，例如在<em class="lx">注意力</em>机制中。</p><p id="f47e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">你可以在<a class="ae kz" href="https://github.com/mlarionov/machine_learning_POC/blob/master/sparsemax/Sparsemax.ipynb" rel="noopener ugc nofollow" target="_blank">我的github库</a>中找到这篇文章的代码。</p></div></div>    
</body>
</html>