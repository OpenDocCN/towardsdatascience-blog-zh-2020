<html>
<head>
<title>How To Perform Feature Selection for Regression Problems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何对回归问题进行特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-perform-feature-selection-for-regression-problems-c928e527bbfa?source=collection_archive---------8-----------------------#2020-08-08">https://towardsdatascience.com/how-to-perform-feature-selection-for-regression-problems-c928e527bbfa?source=collection_archive---------8-----------------------#2020-08-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dce1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本文中，我将解释什么是特征选择，以及在用 Python 训练回归模型之前如何进行特征选择。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4707be7f7144ec4002db927d474153a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XgGa0BE3mTXBHdzGHHp62w.png"/></div></div></figure><h1 id="58ec" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">1.介绍</h1><h2 id="00ad" class="lm kv it bd kw ln lo dn la lp lq dp le lr ls lt lg lu lv lw li lx ly lz lk ma bi translated">什么是<strong class="ak">特征选择</strong>？</h2><blockquote class="mb"><p id="6aaa" class="mc md it bd me mf mg mh mi mj mk ml dk translated"><strong class="ak">特征选择</strong>是从<strong class="ak">输入</strong> <strong class="ak">变量</strong>中选择与目标变量(我们希望预测的)最相关的<strong class="ak">子集</strong>(所有可用变量中的一部分)的过程。</p></blockquote><p id="322e" class="pw-post-body-paragraph mm mn it mo b mp mq ju mr ms mt jx mu lr mv mw mx lu my mz na lx nb nc nd ml im bi translated"><strong class="mo iu">目标变量</strong>这里的<strong class="mo iu"> </strong>是指我们希望<strong class="mo iu">预测</strong>的<strong class="mo iu">变量</strong>。</p><p id="b63c" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated">对于本文，我们将假设我们只有数字输入变量和回归预测建模的数字目标。假设，我们可以很容易地估计每个<strong class="mo iu">输入</strong>变量和<strong class="mo iu">目标</strong>变量之间的<strong class="mo iu">关系</strong>。这种关系可以通过计算诸如相关值的度量来建立。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><p id="37e2" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated">如果你想在交互式路线图和活跃的学习社区的支持下自学数据科学，看看这个资源:<a class="ae nq" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h1 id="b3a4" class="ku kv it bd kw kx nr kz la lb ns ld le jz nt ka lg kc nu kd li kf nv kg lk ll bi translated">2.主要的数值特征选择方法</h1><p id="b505" class="pw-post-body-paragraph mm mn it mo b mp nw ju mr ms nx jx mu lr ny mw mx lu nz mz na lx oa nc nd ml im bi translated">可用于数字输入数据和数字目标变量的两种最著名的特征选择技术如下:</p><ul class=""><li id="eddf" class="ob oc it mo b mp ne ms nf lr od lu oe lx of ml og oh oi oj bi translated">相关性(皮尔森、斯皮尔曼)</li><li id="af06" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml og oh oi oj bi translated">互信息(MI，归一化 MI)</li></ul><p id="650c" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated">相关性是衡量两个变量如何一起变化的指标。最广泛使用的相关性度量是皮尔逊相关性，它假设每个变量为高斯分布，并检测数值变量之间的线性关系。</p><p id="f171" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated">这分两步完成:</p><ol class=""><li id="1af1" class="ob oc it mo b mp ne ms nf lr od lu oe lx of ml op oh oi oj bi translated">计算每个回归变量与目标之间的<strong class="mo iu">相关性</strong>，即((X[:，i] — mean(X[:，I)))*(y—mean _ y))/(STD(X[:，I))* STD(y))。</li><li id="4c96" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">它被转换成一个<strong class="mo iu"> F 值</strong>，然后被转换成一个<strong class="mo iu"> p 值</strong>。</li></ol></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><p id="7fbf" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated"><strong class="mo iu">互信息</strong>源于信息论领域。其思想是应用信息增益(通常在决策树的构造中使用)来执行特征选择。在给定一个变量的已知值的情况下，计算两个变量和测量之间的互信息，作为一个变量的不确定性的减少。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h1 id="9752" class="ku kv it bd kw kx nr kz la lb ns ld le jz nt ka lg kc nu kd li kf nv kg lk ll bi translated">3.数据集</h1><p id="5b8a" class="pw-post-body-paragraph mm mn it mo b mp nw ju mr ms nx jx mu lr ny mw mx lu nz mz na lx oa nc nd ml im bi translated">我们将使用<strong class="mo iu">波斯顿</strong> <strong class="mo iu">房屋</strong> - <strong class="mo iu">价格</strong> <strong class="mo iu">数据集</strong>。该数据集包含由美国人口普查局收集的有关马萨诸塞州波士顿地区住房的信息。数据集由以下变量组成:</p><ol class=""><li id="4885" class="ob oc it mo b mp ne ms nf lr od lu oe lx of ml op oh oi oj bi translated">CRIM——城镇人均犯罪率</li><li id="6d02" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">ZN——面积超过 25，000 平方英尺的住宅用地比例</li><li id="26bb" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">印度河——每个城镇非零售商业用地的比例。</li><li id="70f0" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">CHAS——查尔斯河虚拟变量(1 如果区域边界为河流；否则为 0)</li><li id="3cf3" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">NOX——氮氧化物浓度(百万分之一)</li><li id="5b2f" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">RM——每个住宅的平均房间数</li><li id="247a" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">年龄——1940 年之前建造的业主自用单元的比例</li><li id="525e" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">到五个波士顿就业中心的 DIS 加权距离</li><li id="7b9f" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">RAD——放射状公路可达性指数</li><li id="b9de" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">税收—每 10，000 美元的全价值财产税税率</li><li id="d5d9" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">pt ratio——按城镇分列的学生-教师比率</li><li id="79ce" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">B — 1000(Bk — 0.63)，其中 Bk 是按城镇划分的黑人比例</li><li id="a21e" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">LSTAT —人口中地位较低的百分比</li><li id="ebb8" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">MEDV——以千美元为单位的自有住房的中值</li></ol></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h1 id="1874" class="ku kv it bd kw kx nr kz la lb ns ld le jz nt ka lg kc nu kd li kf nv kg lk ll bi translated">4.Python 代码和工作示例</h1><p id="212e" class="pw-post-body-paragraph mm mn it mo b mp nw ju mr ms nx jx mu lr ny mw mx lu nz mz na lx oa nc nd ml im bi translated">让我们加载数据集并将其分为训练集(70%)和测试集(30%)。</p><pre class="kj kk kl km gt oq or os ot aw ou bi"><span id="3f2b" class="lm kv it or b gy ov ow l ox oy">from sklearn.datasets import load_boston<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.feature_selection import SelectKBest<br/>from sklearn.feature_selection import f_regression<br/>import matplotlib.pyplot as plt<br/>from sklearn.feature_selection import mutual_info_regression</span><span id="983a" class="lm kv it or b gy oz ow l ox oy"># load the data<br/>X, y = load_boston(return_X_y=True)</span><span id="8bdc" class="lm kv it or b gy oz ow l ox oy"># split into train and test sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)</span></pre><p id="4dad" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated">我们将使用众所周知的 scikit-learn 机器库。</p><h2 id="7280" class="lm kv it bd kw ln lo dn la lp lq dp le lr ls lt lg lu lv lw li lx ly lz lk ma bi translated">情况 1:使用相关性度量的特征选择</h2><p id="ebc3" class="pw-post-body-paragraph mm mn it mo b mp nw ju mr ms nx jx mu lr ny mw mx lu nz mz na lx oa nc nd ml im bi translated">对于<strong class="mo iu">相关性统计</strong>，我们将使用<a class="ae nq" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html" rel="noopener ugc nofollow" target="_blank"> f_regression()函数</a>。该功能可用于特征选择策略，如通过<a class="ae nq" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html" rel="noopener ugc nofollow" target="_blank">选择最佳类别</a>选择前 k 个最相关的特征(最大值)。</p><pre class="kj kk kl km gt oq or os ot aw ou bi"><span id="2b06" class="lm kv it or b gy ov ow l ox oy"># feature selection<br/>f_selector = SelectKBest(score_func=f_regression, k='all')</span><span id="e232" class="lm kv it or b gy oz ow l ox oy"># learn relationship from training data<br/>f_selector.fit(X_train, y_train)</span><span id="dd35" class="lm kv it or b gy oz ow l ox oy"># transform train input data<br/>X_train_fs = f_selector.transform(X_train)</span><span id="53cd" class="lm kv it or b gy oz ow l ox oy"># transform test input data<br/>X_test_fs = f_selector.transform(X_test)</span><span id="1e8b" class="lm kv it or b gy oz ow l ox oy"># Plot the scores for the features<br/>plt.bar([i for i in range(len(f_selector.scores_))], f_selector.scores_)</span><span id="3f42" class="lm kv it or b gy oz ow l ox oy">plt.xlabel("feature index")<br/>plt.ylabel("F-value (transformed from the correlation values)")<br/>plt.show()</span></pre><p id="c8c2" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated"><strong class="mo iu">提醒</strong>:对于关联统计情况:</p><ol class=""><li id="1c3a" class="ob oc it mo b mp ne ms nf lr od lu oe lx of ml op oh oi oj bi translated">计算每个回归变量与目标之间的相关性，即((X[:，i] — mean(X[:，I))*(y—mean _ y))/(STD(X[:，I))* STD(y))。</li><li id="61c3" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml op oh oi oj bi translated">它被转换成一个<strong class="mo iu"> F 分数</strong>，然后被转换成一个<strong class="mo iu"> p 值</strong>。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/1affe749232304bd639f4d9c3a5e5651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tp_-XnDug0ZNIBA9ywOm9w.png"/></div></div><p class="pb pc gj gh gi pd pe bd b be z dk translated">特征重要性图</p></figure><p id="7726" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated"><strong class="mo iu">上图显示，特性 6 和 13 比其他特性更重要。y 轴表示根据相关值估计的 F 值。</strong></p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h2 id="8b50" class="lm kv it bd kw ln lo dn la lp lq dp le lr ls lt lg lu lv lw li lx ly lz lk ma bi translated">情况 2:使用互信息度量的特征选择</h2><p id="f97b" class="pw-post-body-paragraph mm mn it mo b mp nw ju mr ms nx jx mu lr ny mw mx lu nz mz na lx oa nc nd ml im bi translated">scikit-learn 机器学习库通过<a class="ae nq" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html" rel="noopener ugc nofollow" target="_blank"> mutual_info_regression()函数</a>为带有数字输入和输出变量的特征选择提供了<strong class="mo iu">互信息</strong>的实现。</p><pre class="kj kk kl km gt oq or os ot aw ou bi"><span id="0851" class="lm kv it or b gy ov ow l ox oy"># feature selection<br/>f_selector = SelectKBest(score_func=mutual_info_regression, k='all')</span><span id="cb30" class="lm kv it or b gy oz ow l ox oy"># learn relationship from training data<br/>f_selector.fit(X_train, y_train)</span><span id="3b81" class="lm kv it or b gy oz ow l ox oy"># transform train input data<br/>X_train_fs = f_selector.transform(X_train)</span><span id="0b02" class="lm kv it or b gy oz ow l ox oy"># transform test input data<br/>X_test_fs = f_selector.transform(X_test)</span><span id="f32a" class="lm kv it or b gy oz ow l ox oy"># Plot the scores for the features<br/>plt.bar([i for i in range(len(f_selector.scores_))], f_selector.scores_)</span><span id="cf87" class="lm kv it or b gy oz ow l ox oy">plt.xlabel("feature index")<br/>plt.ylabel("Estimated MI value")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/52f1aeb9b813fc8dbe548f8d5dd7e423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d1L_O9q0TGS55EjPVvgEDA.png"/></div></div><p class="pb pc gj gh gi pd pe bd b be z dk translated">特征重要性图</p></figure><p id="4b3e" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated"><strong class="mo iu">y 轴表示每个特征和目标变量之间的估计互信息。与相关性特征选择方法相比，我们可以清楚地看到更多的特征被评分为相关。这可能是因为数据集中可能存在统计噪声。</strong></p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h1 id="ff08" class="ku kv it bd kw kx nr kz la lb ns ld le jz nt ka lg kc nu kd li kf nv kg lk ll bi translated">5.结论</h1><p id="e4a0" class="pw-post-body-paragraph mm mn it mo b mp nw ju mr ms nx jx mu lr ny mw mx lu nz mz na lx oa nc nd ml im bi translated">在这篇文章中，我提供了两种方法来执行特性选择。<strong class="mo iu">特征选择</strong>是选择与目标变量最相关的输入变量子集(所有可用变量中的一部分)的程序。<strong class="mo iu">目标变量</strong>这里的<strong class="mo iu"> </strong>是指我们希望<strong class="mo iu">预测</strong>的<strong class="mo iu">变量</strong>。</p><p id="2733" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated">使用<strong class="mo iu">相关性</strong>度量或者<strong class="mo iu">相互</strong> <strong class="mo iu">信息</strong>度量，我们可以很容易地估计每个<strong class="mo iu">输入</strong>变量和<strong class="mo iu">目标</strong>变量之间的<strong class="mo iu">关系</strong>。</p><p id="33db" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated"><strong class="mo iu">相关性</strong> <strong class="mo iu"> vs </strong> <strong class="mo iu">互</strong> <strong class="mo iu">信息:</strong>与相关性特征选择方法相比，我们可以清楚地看到更多的特征被评分为相关。这可能是因为数据集中可能存在统计噪声。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h2 id="fd98" class="lm kv it bd kw ln lo dn la lp lq dp le lr ls lt lg lu lv lw li lx ly lz lk ma bi translated">您可能还喜欢:</h2><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/lstm-time-series-forecasting-predicting-stock-prices-using-an-lstm-model-6223e9644a2f"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">LSTM 时间序列预测:使用 LSTM 模型预测股票价格</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在这篇文章中，我将向你展示如何使用预测 LSTM 模型来预测股票价格</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px ks pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/time-series-forecasting-predicting-stock-prices-using-an-arima-model-2e3b3080bd70"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">时间序列预测:使用 ARIMA 模型预测股票价格</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在这篇文章中，我将向你展示如何使用预测 ARIMA 模型来预测特斯拉的股票价格</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="py l pu pv pw ps px ks pj"/></div></div></a></div></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h1 id="a96f" class="ku kv it bd kw kx nr kz la lb ns ld le jz nt ka lg kc nu kd li kf nv kg lk ll bi translated">敬请关注并支持这一努力</h1><p id="db8b" class="pw-post-body-paragraph mm mn it mo b mp nw ju mr ms nx jx mu lr ny mw mx lu nz mz na lx oa nc nd ml im bi translated">如果你喜欢这篇文章，并且觉得它有用，那么<strong class="mo iu">关注</strong>我就可以看到我所有的新帖子。</p><p id="032c" class="pw-post-body-paragraph mm mn it mo b mp ne ju mr ms nf jx mu lr ng mw mx lu nh mz na lx ni nc nd ml im bi translated">有问题吗？把它们作为评论贴出来，我会尽快回复。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h1 id="98d4" class="ku kv it bd kw kx nr kz la lb ns ld le jz nt ka lg kc nu kd li kf nv kg lk ll bi translated">最新帖子</h1><div class="pg ph gp gr pi pj"><a href="https://medium.com/@seralouk/the-best-free-data-science-resources-free-books-online-courses-9c4a2df194e5" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">最佳免费数据科学资源:免费书籍和在线课程</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">最有用的免费书籍和在线课程，适合想了解更多数据科学知识的人。</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">medium.com</p></div></div><div class="ps l"><div class="pz l pu pv pw ps px ks pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/roc-curve-explained-using-a-covid-19-hypothetical-example-binary-multi-class-classification-bab188ea869c"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">用新冠肺炎假设的例子解释 ROC 曲线:二分类和多分类…</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在这篇文章中，我清楚地解释了什么是 ROC 曲线以及如何阅读它。我用一个新冠肺炎的例子来说明我的观点，我…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="qa l pu pv pw ps px ks pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">支持向量机(SVM)解释清楚:分类问题的 python 教程…</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在这篇文章中，我解释了支持向量机的核心，为什么以及如何使用它们。此外，我还展示了如何绘制支持…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="qb l pu pv pw ps px ks pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">PCA 清楚地解释了——如何、何时、为什么使用它以及特性的重要性:Python 指南</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在这篇文章中，我解释了什么是 PCA，何时以及为什么使用它，以及如何使用 scikit-learn 在 Python 中实现它。还有…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="qc l pu pv pw ps px ks pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">关于 Python 中的最小-最大规范化，您需要知道的一切</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在这篇文章中，我将解释什么是最小-最大缩放，什么时候使用它，以及如何使用 scikit 在 Python 中实现它</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="qd l pu pv pw ps px ks pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/how-and-why-to-standardize-your-data-996926c2c832"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">Scikit-Learn 的标准定标器如何工作</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在这篇文章中，我将解释为什么以及如何使用 scikit-learn 应用标准化</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="qe l pu pv pw ps px ks pj"/></div></div></a></div></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h1 id="7cae" class="ku kv it bd kw kx nr kz la lb ns ld le jz nt ka lg kc nu kd li kf nv kg lk ll bi translated">和我联系</h1><ul class=""><li id="45ab" class="ob oc it mo b mp nw ms nx lr qf lu qg lx qh ml og oh oi oj bi translated"><strong class="mo iu">LinkedIn</strong>:【https://www.linkedin.com/in/serafeim-loukas/ T2】</li><li id="5962" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml og oh oi oj bi translated">https://www.researchgate.net/profile/Serafeim_Loukas<strong class="mo iu">研究之门</strong>:<a class="ae nq" href="https://www.researchgate.net/profile/Serafeim_Loukas" rel="noopener ugc nofollow" target="_blank">T7】</a></li><li id="31d2" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml og oh oi oj bi translated"><strong class="mo iu">https://people.epfl.ch/serafeim.loukas</strong>EPFL<strong class="mo iu">简介</strong>:<a class="ae nq" href="https://people.epfl.ch/serafeim.loukas" rel="noopener ugc nofollow" target="_blank"/></li><li id="2a52" class="ob oc it mo b mp ok ms ol lr om lu on lx oo ml og oh oi oj bi translated"><strong class="mo iu">堆栈</strong> <strong class="mo iu">溢出</strong>:<a class="ae nq" href="https://stackoverflow.com/users/5025009/seralouk" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/users/5025009/seralouk</a></li></ul></div></div>    
</body>
</html>