<html>
<head>
<title>Multi-GPU Training in Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pytorch 中的多 GPU 培训</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-gpu-training-in-pytorch-dbdb3389fd4a?source=collection_archive---------40-----------------------#2020-03-04">https://towardsdatascience.com/multi-gpu-training-in-pytorch-dbdb3389fd4a?source=collection_archive---------40-----------------------#2020-03-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1281" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">数据和模型并行性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/aa044c18e05530394d7f3b2cdc925c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*29N3gECY-cAZHdUeK4o58Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安娜·安彻的《收割机》。链接:<a class="ae ky" href="https://en.wikipedia.org/wiki/File:Anna_Ancher_-_Harvesters_-_Google_Art_Project.jpg" rel="noopener ugc nofollow" target="_blank">维基百科</a>。</p></figure><p id="9cb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章将概述 Pytorch 中的多 GPU 培训，包括:</p><ul class=""><li id="7721" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">在一个 GPU 上训练；</li><li id="5b26" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">多 GPU 上的训练；</li><li id="a785" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">通过一次处理更多的例子，使用数据并行性来加速训练；</li><li id="733a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">使用模型并行性来支持需要比一个 GPU 上可用的内存更多的内存的训练模型；</li><li id="553c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">使用 num_workers &gt; 0 的数据加载器来支持多进程数据加载；</li><li id="0faf" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">仅在可用设备的子集上训练。</li></ul><h1 id="3da3" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">在一个 GPU 上训练</strong></h1><p id="7d5a" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">假设您有 3 个可用的 GPU，并且您想在其中一个上训练一个模型。您可以通过指定设备来告诉 Pytorch 使用哪个 GPU:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="da1d" class="nl mk it nh b gy nm nn l no np">device = torch.device('cuda:0') for GPU 0</span><span id="d101" class="nl mk it nh b gy nq nn l no np">device = torch.device('cuda:1') for GPU 1</span><span id="f9d9" class="nl mk it nh b gy nq nn l no np">device = torch.device('cuda:2') for GPU 2</span></pre><h1 id="7624" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">在多个 GPU 上训练</strong></h1><p id="5016" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">要允许 Pytorch“查看”所有可用的 GPU，请使用:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="dc48" class="nl mk it nh b gy nm nn l no np">device = torch.device('cuda')</span></pre><p id="048a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用多个 GPU 有几种不同的方式，包括数据并行和模型并行。</p><h1 id="ed1d" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">数据并行度</strong></h1><p id="bcb2" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">数据并行是指使用多个 GPU 来增加同时处理的实例数量。例如，如果 256 的批处理大小适合一个 GPU，您可以通过使用两个 GPU 使用数据并行性将批处理大小增加到 512，Pytorch 会自动将大约 256 个示例分配给一个 GPU，将大约 256 个示例分配给另一个 GPU。</p><p id="9296" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用数据并行可以通过数据并行轻松实现。例如，假设您有一个名为“custom_net”的模型，该模型当前初始化如下:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="afb3" class="nl mk it nh b gy nm nn l no np">import torch, torch.nn as nn</span><span id="1fd5" class="nl mk it nh b gy nq nn l no np">model = custom_net(**custom_net_args).to(device)</span></pre><p id="1f0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，使用数据并行性所要做的就是将 custom_net 包装在 DataParallel 中:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="220d" class="nl mk it nh b gy nm nn l no np">model = nn.DataParallel(custom_net(**custom_net_args)).to(device)</span></pre><p id="5fab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您还需要增加批量大小，以最大限度地利用所有可用设备。</p><p id="9470" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关数据并行性的更多信息，请参见本文<a class="ae ky" href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="44c8" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">模型并行度</strong></h1><p id="6462" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">您可以使用模型并行性来训练需要比一个 GPU 上可用的内存更多的内存的模型。模型并行性允许您将模型的不同部分分布在不同的设备上。</p><p id="1af1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用模型并行有两个步骤。第一步是在模型定义中指定模型的哪些部分应该在哪个设备上运行。这里有一个来自<a class="ae ky" href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html" rel="noopener ugc nofollow" target="_blank"> Pytorch 文档</a>的例子:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="5ade" class="nl mk it nh b gy nm nn l no np"><strong class="nh iu">import</strong> torch<br/><strong class="nh iu">import</strong> torch.nn <strong class="nh iu">as</strong> nn<br/><strong class="nh iu">import</strong> torch.optim <strong class="nh iu">as</strong> optim<br/><br/><br/><strong class="nh iu">class</strong> <strong class="nh iu">ToyModel</strong>(nn<strong class="nh iu">.</strong>Module):<br/>    <strong class="nh iu">def</strong> __init__(self):<br/>        super(ToyModel, self)<strong class="nh iu">.</strong>__init__()<br/>        self<strong class="nh iu">.</strong>net1 <strong class="nh iu">=</strong> torch<strong class="nh iu">.</strong>nn<strong class="nh iu">.</strong>Linear(10, 10)<strong class="nh iu">.</strong>to('cuda:0')<br/>        self<strong class="nh iu">.</strong>relu <strong class="nh iu">=</strong> torch<strong class="nh iu">.</strong>nn<strong class="nh iu">.</strong>ReLU()<br/>        self<strong class="nh iu">.</strong>net2 <strong class="nh iu">=</strong> torch<strong class="nh iu">.</strong>nn<strong class="nh iu">.</strong>Linear(10, 5)<strong class="nh iu">.</strong>to('cuda:1')<br/><br/>    <strong class="nh iu">def</strong> <strong class="nh iu">forward</strong>(self, x):<br/>        x <strong class="nh iu">=</strong> self<strong class="nh iu">.</strong>relu(self<strong class="nh iu">.</strong>net1(x<strong class="nh iu">.</strong>to('cuda:0')))<br/>        <strong class="nh iu">return</strong> self<strong class="nh iu">.</strong>net2(x<strong class="nh iu">.</strong>to('cuda:1'))</span></pre><p id="7c07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二步是确保在调用 loss 函数时，标签与模型的输出在同一个设备上。</p><p id="0aa7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，您可能希望从将标签移动到设备“cuda:1”并将数据移动到设备“cuda:0”开始。然后，您可以在“cuda:0”上用模型的一部分处理您的数据，然后将中间表示移动到“cuda:1”，并在“cuda:1”上生成最终预测。因为您的标签已经在“cuda:1”上，Pytorch 将能够计算损失并执行反向传播，而无需任何进一步的修改。</p><p id="d341" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关模型并行性的更多信息，请参见本文。</p><h1 id="f1d5" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">使用 Num_Workers 加快数据加载速度</strong></h1><p id="2e47" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">Pytorch 的数据加载器提供了一种自动加载和批处理数据的有效方法。您可以将它用于任何数据集，不管它有多复杂。您需要做的就是首先定义您自己的数据集，该数据集继承自 Pytorch 的 Dataset 类:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="d8d4" class="nl mk it nh b gy nm nn l no np">from torch.utils.data import DataLoader</span><span id="dfda" class="nl mk it nh b gy nq nn l no np">class MyComplicatedCustomDataset(Dataset): <br/>    def __init__(self, some_arg, some_other_arg):<br/>        """Documentation"""<br/>        self.some_arg = some_arg<br/>        self.some_other_arg = some_other_arg<br/> <br/>    # Pytorch Required Methods # — — — — — — — — — — — — — — —<br/>    def __len__(self):<br/>        """Return an integer representing the total number of <br/>        examples in your data set"""<br/>        return len(self.my_list_of_examples)<br/> <br/>    def __getitem__(self, idx):<br/>        """Return a single sample at index &lt;idx&gt;. The sample is any <br/>        kind of Python object you want. It could be a numpy array. <br/>        It could be a dictionary with strings for keys and <br/>        numpy arrays for values. It could be a list — really <br/>        whatever you want."""<br/>        return self._a_custom_method(self.my_list_of_examples[idx])<br/> <br/>    # Whatever Custom Stuff You Want # — — — — — — — — — — — -<br/>    def _a_custom_method(self, example_name):<br/>        #processing step 1<br/>        #processing step 2<br/>        #etc.<br/>        return processed_example</span></pre><p id="74d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对数据集的唯一要求是它定义了 __len__ 和 __getitem__ 方法。</p><p id="6338" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">__len__ 方法必须返回数据集中示例的总数。</p><p id="cfa5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">__getitem__ 方法必须返回基于整数索引的单个示例。</p><p id="bec2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你实际上如何准备例子和例子是什么完全取决于你。</p><p id="1a86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦创建了数据集，就需要将该数据集包装在 Pytorch 的数据加载器中，如下所示:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="84d8" class="nl mk it nh b gy nm nn l no np">from torch.utils.data import Dataset, DataLoader</span><span id="f68b" class="nl mk it nh b gy nq nn l no np">dataset_train = MyComplicatedCustomDataset(**dataset_args)</span><span id="e018" class="nl mk it nh b gy nq nn l no np">train_dataloader = DataLoader(dataset_train, batch_size=256, shuffle=True, num_workers = 4)</span></pre><p id="1f2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了获得批处理，您只需遍历数据加载器:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="8436" class="nl mk it nh b gy nm nn l no np">for batch_idx, batch in enumerate(train_dataloader):</span><span id="b1f1" class="nl mk it nh b gy nq nn l no np">    do stuff</span></pre><p id="b84e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果希望加快数据加载，可以使用多个工作线程。请注意，在对 DataLoader 的调用中，您指定了一些工作线程:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="b745" class="nl mk it nh b gy nm nn l no np">train_dataloader = DataLoader(dataset_train, batch_size=256, shuffle=True, <strong class="nh iu">num_workers = 4</strong>)</span></pre><p id="a26c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">默认情况下，num_workers 设置为 0。将 num_workers 设置为正整数将启用多进程数据加载，在这种情况下，将使用指定数量的加载器工作进程来加载数据。(注意，这并不是真正的多 GPU，因为这些加载器工作进程是 CPU 上的不同进程，但是因为它与加速模型训练有关，所以我决定将它放在同一篇文章中)。</p><p id="f73e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，工作进程越多并不总是越好。如果将 num_workers 设置得太高，实际上会降低数据加载速度。关于如何选择最佳工人数量，也没有很大的规则。网上有很多关于它的讨论(例如这里的<a class="ae ky" href="https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813" rel="noopener ugc nofollow" target="_blank"/>)，但没有结论性的答案。关于如何选择工作人员的数量，没有什么很好的规则，原因是最佳的工作人员数量取决于您正在使用的机器类型、您正在使用的数据集类型以及您的数据需要多少即时预处理。</p><p id="4b2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选择工人数量的一个好方法是在数据集上运行一些小实验，在这些实验中，您计算使用不同数量的工人加载固定数量的示例需要多长时间。随着 num_workers 从 0 开始增加，您将首先看到数据加载速度的增加，随后当您遇到“太多 workers”时，数据加载速度将会降低</p><p id="8b93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关更多信息，请参见本页的<a class="ae ky" href="https://pytorch.org/docs/stable/data.html" rel="noopener ugc nofollow" target="_blank">中的“多进程数据加载”。</a></p><h1 id="1e04" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">模型并行和数据并行同时进行</strong></h1><p id="278e" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">如果您想同时使用模型并行和数据并行，那么数据并行必须以稍微不同的方式实现，使用 DistributedDataParallel 而不是 DataParallel。更多信息，请参见<a class="ae ky" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" rel="noopener ugc nofollow" target="_blank">“分布式数据并行入门”</a></p><h1 id="0ac1" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">对可用设备子集的培训</strong></h1><p id="a74a" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">如果您想使用模型并行性或数据并行性，但不想占用单个模型的所有可用设备，该怎么办？在这种情况下，您可以限制 Pytorch 可以看到每个型号的哪些设备。在您的代码中，您将设置设备，好像您想要使用所有的 GPU(即使用 device = torch.device('cuda '))，但是当您运行代码时，您将限制哪些 GPU 可以被看到。</p><p id="c92f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设您有 6 个 GPU，您想在其中的 2 个上训练模型 A，在其中的 4 个上训练模型 B。您可以这样做:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="4421" class="nl mk it nh b gy nm nn l no np">CUDA_VISIBLE_DEVICES=0,1 python model_A.py</span><span id="8d2d" class="nl mk it nh b gy nq nn l no np">CUDA_VISIBLE_DEVICES=2,3,4,5 python model_B.py</span></pre><p id="c5b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者，如果您有 3 个 GPU，并且希望在其中一个上训练模型 A，在其中两个上训练模型 B，您可以这样做:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="b975" class="nl mk it nh b gy nm nn l no np">CUDA_VISIBLE_DEVICES=1 python model_A.py</span><span id="b4a5" class="nl mk it nh b gy nq nn l no np">CUDA_VISIBLE_DEVICES=0,2 python model_B.py</span></pre><p id="d572" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多 GPU 训练快乐！</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="8446" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ny">原载于 2020 年 3 月 4 日 http://glassboxmedicine.com</em><em class="ny">的</em> <a class="ae ky" href="https://glassboxmedicine.com/2020/03/04/multi-gpu-training-in-pytorch-data-and-model-parallelism/" rel="noopener ugc nofollow" target="_blank"> <em class="ny">。</em></a></p></div></div>    
</body>
</html>