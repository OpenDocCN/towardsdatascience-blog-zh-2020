<html>
<head>
<title>How to Properly Use the GPU within a Docker Container</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Docker容器中正确使用GPU</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-properly-use-the-gpu-within-a-docker-container-4c699c78c6d1?source=collection_archive---------1-----------------------#2020-05-19">https://towardsdatascience.com/how-to-properly-use-the-gpu-within-a-docker-container-4c699c78c6d1?source=collection_archive---------1-----------------------#2020-05-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5112" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">注意:我们也在博客上发布了<a class="ae ki" href="https://blog.roboflow.ai/use-the-gpu-in-docker/" rel="noopener ugc nofollow" target="_blank">如何在Docker </a>中使用GPU。在这篇文章中，我们将介绍在Docker容器中访问机器的GPU所需的步骤。</h2></div><p id="3b2d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在你的机器上配置GPU可能非常困难。配置步骤会根据您机器的操作系统和NVIDIA GPU的种类而有所不同。更困难的是，当Docker启动一个容器时，它几乎是从零开始的。某些东西，比如CPU驱动程序是为你预先配置的，但是<strong class="kl iu">当你运行docker容器</strong>时，GPU并没有被配置。幸运的是，您已经找到了这里解释的解决方案。它叫做<strong class="kl iu"> NVIDIA容器工具包</strong>！</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/65fe35032f250e997190c1da42dbafc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sNFYbKwwAN2TEBM8.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">Nvidia容器工具包(<a class="ae ki" href="https://github.com/NVIDIA/nvidia-docker" rel="noopener ugc nofollow" target="_blank">引文</a>)</p></figure><h1 id="fb1d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Docker中的潜在错误</h1><p id="3080" class="pw-post-body-paragraph kj kk it kl b km mn ju ko kp mo jx kr ks mp ku kv kw mq ky kz la mr lc ld le im bi translated">当您试图在Docker中运行需要GPU的容器时，您可能会收到以下任何错误。</p><p id="9cd5" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">错误:Docker找不到Nvidia驱动程序</p><pre class="lg lh li lj gt ms mt mu mv aw mw bi"><span id="9b61" class="mx lw it mt b gy my mz l na nb">docker: Error response from daemon: Container command 'nvidia-smi' not found or does not exist..</span></pre><p id="0ba7" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">错误:tensorflow无法访问Docker中的GPU</p><pre class="lg lh li lj gt ms mt mu mv aw mw bi"><span id="345b" class="mx lw it mt b gy my mz l na nb">I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.93<br/>I tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.</span></pre><p id="750e" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">错误:pytorch无法访问Docker中的GPU</p><pre class="lg lh li lj gt ms mt mu mv aw mw bi"><span id="dba3" class="mx lw it mt b gy my mz l na nb">RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50</span></pre><p id="581c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">错误:keras无法访问Docker中的GPU</p><pre class="lg lh li lj gt ms mt mu mv aw mw bi"><span id="131e" class="mx lw it mt b gy my mz l na nb">The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span></pre><p id="9393" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">您可能会收到许多其他错误，表明您的Docker容器无法访问机器的GPU。在任何情况下，如果您有任何类似上面的错误，您已经在这里找到了正确的位置。</p><h1 id="9991" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">首先，确保你的基本机器有GPU驱动程序</h1><p id="a238" class="pw-post-body-paragraph kj kk it kl b km mn ju ko kp mo jx kr ks mp ku kv kw mq ky kz la mr lc ld le im bi translated">您必须首先在您的基本计算机上安装NVIDIA GPU驱动程序，然后才能在Docker中使用GPU。如前所述，由于操作系统、NVIDIA GPU和NVIDIA GPU驱动程序的分布过多，这可能很困难。您将运行的确切命令会因这些参数而异。这里有一些资源，你可能会发现在你的基本机器上配置GPU有用。</p><ul class=""><li id="284f" class="nc nd it kl b km kn kp kq ks ne kw nf la ng le nh ni nj nk bi translated"><a class="ae ki" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html" rel="noopener ugc nofollow" target="_blank"> NVIDIA官方工具包文档</a></li><li id="4086" class="nc nd it kl b km nl kp nm ks nn kw no la np le nh ni nj nk bi translated"><a class="ae ki" href="https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-18-04-bionic-beaver-linux" rel="noopener ugc nofollow" target="_blank">在Ubuntu指南上安装NVIDIA驱动程序</a></li><li id="a3e2" class="nc nd it kl b km nl kp nm ks nn kw no la np le nh ni nj nk bi translated"><a class="ae ki" href="https://www.cyberciti.biz/faq/ubuntu-linux-install-nvidia-driver-latest-proprietary-driver/" rel="noopener ugc nofollow" target="_blank">从命令行安装NVIDIA驱动程序</a></li></ul><p id="7eb6" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">一旦你完成了这些步骤，通过运行<strong class="kl iu"> nvidia-smi </strong>命令并查看如下输出，你就会知道你成功了。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/117d0d1445c5f019978dfbe9da488a1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/0*Ed6oSNPhTGNhvgrj.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">我已经成功地在我的谷歌云实例上安装了GPU驱动程序</p></figure><p id="face" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，我们可以保证我们已经成功地保证NVIDIA GPU驱动程序安装在基础机器上，我们可以移动到Docker容器的更深一层。</p><h1 id="ec9b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">接下来，向Docker公开GPU驱动程序</h1><p id="0e4c" class="pw-post-body-paragraph kj kk it kl b km mn ju ko kp mo jx kr ks mp ku kv kw mq ky kz la mr lc ld le im bi translated">为了让Docker识别GPU，我们需要让它知道GPU驱动程序。我们在图像创建过程中这样做。Docker映像创建是一系列命令，用于配置Docker容器将要运行的环境。</p><p id="5a27" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">强力方法— </strong>强力方法包括您在基本机器上用来配置GPU的相同命令。当docker构建映像时，这些命令将会运行并在您的映像上安装GPU驱动程序，一切都会好的。在你的docker文件中，暴力方法看起来会像这样(代码归功于<a class="ae ki" href="https://stackoverflow.com/questions/25185405/using-gpu-from-a-docker-container" rel="noopener ugc nofollow" target="_blank">栈溢出</a>):</p><pre class="lg lh li lj gt ms mt mu mv aw mw bi"><span id="2b0b" class="mx lw it mt b gy my mz l na nb">FROM ubuntu:14.04<br/>MAINTAINER Regan &lt;<a class="ae ki" href="http://stackoverflow.com/questions/25185405/using-gpu-from-a-docker-container" rel="noopener ugc nofollow" target="_blank">http://stackoverflow.com/questions/25185405/using-gpu-from-a-docker-container</a>&gt;</span><span id="e40d" class="mx lw it mt b gy nr mz l na nb">RUN apt-get update &amp;&amp; apt-get install -y build-essential<br/>RUN apt-get --purge remove -y nvidia*</span><span id="f846" class="mx lw it mt b gy nr mz l na nb">ADD ./Downloads/nvidia_installers /tmp/nvidia                             &gt; Get the install files you used to install CUDA and the NVIDIA drivers on your host<br/>RUN /tmp/nvidia/NVIDIA-Linux-x86_64-331.62.run -s -N --no-kernel-module   &gt; Install the driver.<br/>RUN rm -rf /tmp/selfgz7                                                   &gt; For some reason the driver installer left temp files when used during a docker build (i don't have any explanation why) and the CUDA installer will fail if there still there so we delete them.<br/>RUN /tmp/nvidia/cuda-linux64-rel-6.0.37-18176142.run -noprompt            &gt; CUDA driver installer.<br/>RUN /tmp/nvidia/cuda-samples-linux-6.0.37-18176142.run -noprompt -cudaprefix=/usr/local/cuda-6.0   &gt; CUDA samples comment if you don't want them.<br/>RUN export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64         &gt; Add CUDA library into your PATH<br/>RUN touch /etc/ld.so.conf.d/cuda.conf                                     &gt; Update the ld.so.conf.d directory<br/>RUN rm -rf /temp/*  &gt; Delete installer files.</span></pre><p id="f6fd" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">强力方法的缺点— </strong>首先，每次你重新构建docker镜像时，你都必须重新安装镜像，这会减慢开发速度。第二，如果您决定将docker映像从当前机器中取出，放到一台具有不同GPU、操作系统的新机器上，或者您想要新的驱动程序，那么您将不得不每次都为每台机器重新编码这一步骤。这违背了建立码头工人形象的目的。第三，您可能不记得在您的本地机器上安装驱动程序的命令，并且在Docker中重新配置GPU。</p><p id="4f93" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">最佳方法— </strong>最佳方法是使用NVIDIA容器工具包。NVIDIA容器工具包是一个docker映像，它支持自动识别您的基本计算机上的GPU驱动程序，并在运行时将这些相同的驱动程序传递给您的Docker容器。因此，如果你能够在你的基本机器上运行<strong class="kl iu"> nvidia-smi </strong>，你也将能够在你的Docker容器中运行它(并且你的所有程序将能够引用GPU)。为了使用NVIDIA Container Toolkit，您只需将NVIDIA Container Toolkit映像放在Dockerfile的顶部，就像这样— <strong class="kl iu"> nano Dockerfile </strong>:</p><pre class="lg lh li lj gt ms mt mu mv aw mw bi"><span id="a451" class="mx lw it mt b gy my mz l na nb">FROM nvidia/cuda:10.2-base<br/>CMD nvidia-smi</span></pre><p id="c56e" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">这是你需要向Docker </strong>公开GPU驱动的所有代码。在该docker文件中，我们已经导入了10.2驱动程序的NVIDIA容器工具包映像，然后我们指定了一个在运行容器来检查驱动程序时要运行的命令。现在我们用<strong class="kl iu"> docker build构建图像。-t英伟达-测试</strong>:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/336af2a4c418a45c26355003f4a3f708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/0*xGIZYuk2xo-UUS9z.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">构建docker映像，并将其命名为“nvidia-test”</p></figure><p id="6829" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，我们通过使用命令<strong class="kl iu">docker run-GPU all NVIDIA-test从映像运行容器。</strong>请记住，我们需要<strong class="kl iu">—GPU all</strong>，否则GPU不会暴露给运行的容器。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/56a4267593be459d4e7340d9ca72c54d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/0*7tpf7VrZj7lmCuOt.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">成功！我们的docker容器看到了GPU驱动程序</p></figure><p id="8574" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">从这个基础状态开始，您可以相应地开发您的应用程序。在我的情况下，我使用英伟达容器工具包来支持实验性深度学习框架。完整构建的docker文件的布局可能如下所示(其中/app/包含所有python文件):</p><pre class="lg lh li lj gt ms mt mu mv aw mw bi"><span id="0158" class="mx lw it mt b gy my mz l na nb">FROM nvidia/cuda:10.2-base<br/>CMD nvidia-smi</span><span id="e986" class="mx lw it mt b gy nr mz l na nb">#set up environment<br/>RUN apt-get update &amp;&amp; apt-get install --no-install-recommends --no-install-suggests -y curl<br/>RUN apt-get install unzip<br/>RUN apt-get -y install python3<br/>RUN apt-get -y install python3-pip</span><span id="79a0" class="mx lw it mt b gy nr mz l na nb">COPY app/requirements_verbose.txt /app/requirements_verbose.txt</span><span id="ea1d" class="mx lw it mt b gy nr mz l na nb">RUN pip3 install -r /app/requirements_verbose.txt</span><span id="fc97" class="mx lw it mt b gy nr mz l na nb">#copies the applicaiton from local path to container path<br/>COPY app/ /app/<br/>WORKDIR /app</span><span id="5773" class="mx lw it mt b gy nr mz l na nb">ENV NUM_EPOCHS=10<br/>ENV MODEL_TYPE='EfficientDet'<br/>ENV DATASET_LINK='HIDDEN'<br/>ENV TRAIN_TIME_SEC=100</span><span id="e181" class="mx lw it mt b gy nr mz l na nb">CMD ["python3", "train_and_eval.py"]<br/></span></pre><p id="0f66" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">上述Docker容器使用基础机器GPU基于规范训练和评估深度学习模型。相当酷！</p><h1 id="9e77" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">在Dockerfile </strong>中需要不同的基础图像</h1><p id="0172" class="pw-post-body-paragraph kj kk it kl b km mn ju ko kp mo jx kr ks mp ku kv kw mq ky kz la mr lc ld le im bi translated">假设您一直依赖docker文件中的不同基础映像。然后，你可以<a class="ae ki" href="https://docs.docker.com/config/containers/resource_constraints/#gpu" rel="noopener ugc nofollow" target="_blank">安装NVIDIA容器运行时</a>。</p><pre class="lg lh li lj gt ms mt mu mv aw mw bi"><span id="29fb" class="mx lw it mt b gy my mz l na nb">apt-get install nvidia-container-runtime<br/>docker run -it --rm --gpus all ubuntu nvidia-smi</span></pre><p id="e230" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在你可以用nvidia容器运行时运行其他基础镜像(这里我们运行ubuntu基础)。</p><h1 id="4a2f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">NVIDIA容器工具包的威力</strong></h1><p id="32cd" class="pw-post-body-paragraph kj kk it kl b km mn ju ko kp mo jx kr ks mp ku kv kw mq ky kz la mr lc ld le im bi translated">现在，您已经编写了映像以通过基础机器的GPU驱动程序，您将能够从当前机器中提取映像，并将其部署到在您想要的任何实例上运行的容器中。</p><h1 id="8990" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">更新:需要Docker上的CuDNN和NVCC cuda工具包吗？</h1><p id="7d80" class="pw-post-body-paragraph kj kk it kl b km mn ju ko kp mo jx kr ks mp ku kv kw mq ky kz la mr lc ld le im bi translated">只有当你需要从其他NVIDIA Docker基础映像中获得<code class="fe nu nv nw mt b">cuDNN</code>或<code class="fe nu nv nw mt b">nvcc --version</code>时，<code class="fe nu nv nw mt b">nvidia/cuda:10.2-base</code>才会获得<code class="fe nu nv nw mt b">nvidia-smi.</code>，即:<code class="fe nu nv nw mt b">nvidia/cuda:10.2-devel-ubuntu18.0.</code>(获得nvcc cuda工具包)和<code class="fe nu nv nw mt b">nvidia/cuda:10.2-cudnn7-devel-ubuntu18.04.</code>(获得cuDNN)。</p><h1 id="c605" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="c1a8" class="pw-post-body-paragraph kj kk it kl b km mn ju ko kp mo jx kr ks mp ku kv kw mq ky kz la mr lc ld le im bi translated">恭喜你！现在，您知道如何使用NVIDIA容器工具包将GPU驱动程序暴露给正在运行的Docker容器。</p><p id="5777" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">想使用你的新Docker功能做一些很棒的事情吗？你可能会喜欢我们关于<a class="ae ki" href="https://blog.roboflow.ai/training-efficientdet-object-detection-model-with-a-custom-dataset/" rel="noopener ugc nofollow" target="_blank">训练一个最先进的物体检测模型</a>、<a class="ae ki" href="https://models.roboflow.ai/classification/resnet-32" rel="noopener ugc nofollow" target="_blank">训练一个最先进的图像分类模型</a>的其他帖子，或者只是通过查看一些<a class="ae ki" href="https://public.roboflow.ai/" rel="noopener ugc nofollow" target="_blank">免费的计算机视觉数据</a>！</p></div></div>    
</body>
</html>