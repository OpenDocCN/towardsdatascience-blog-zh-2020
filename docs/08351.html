<html>
<head>
<title>Python (Scikit-Learn): Logistic Regression Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python (Scikit-Learn):逻辑回归分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/python-scikit-learn-logistic-regression-classification-eb9c8de8938d?source=collection_archive---------18-----------------------#2020-06-18">https://towardsdatascience.com/python-scikit-learn-logistic-regression-classification-eb9c8de8938d?source=collection_archive---------18-----------------------#2020-06-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7e46" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过使用 Python 中的 scikit-learn 包，了解如何将逻辑回归应用于二元分类</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6452c6a59f75fabb7944ab1cf7e1213b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e5Vf07_3j_IfELV2mhiDRg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由 Pietro Jeng 在 Unsplash 上拍摄</p></figure><p id="7db2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用预测技术区分分类数据的过程称为<strong class="la iu">分类</strong>。最广泛使用的分类技术之一是<strong class="la iu">逻辑回归</strong>。关于逻辑回归的理论基础，请看我之前的<a class="ae lu" href="https://medium.com/@maurizio.s/logistic-regression-discover-the-powerful-classification-technique-d60c0ae4d3b1" rel="noopener">文章</a>。</p><p id="f990" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将利用 Python 编程语言中可用的<strong class="la iu"> scikit-learn (sklearn) </strong>包，将逻辑回归应用于一个<strong class="la iu">二元分类</strong>问题。</p><h1 id="47f2" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">泰坦尼克号数据集</h1><p id="d960" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我们将使用泰坦尼克号数据集(可在<a class="ae lu" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上获得)，目标是预测泰坦尼克号上的幸存者。也就是说，基于数据集中包含的特征(解释变量)，我们希望预测某个特定的人是否在泰坦尼克号沉船事故中幸存。</p><h1 id="2092" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">加载数据</h1><p id="6459" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我们从导入所需的包和加载 titanic 数据集开始。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="f5b8" class="mx lw it mt b gy my mz l na nb">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/>from sklearn import metrics<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import train_test_split<br/><br/>titanic = pd.read_csv(".../Titanic/train.csv")<br/><br/>titanic.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/7a98a951c960c6deac8413dfb6893c27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8mZ3th_ee8L2LZMbhYJHgA.png"/></div></div></figure><p id="d586" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我们可以看到数据集中包含的不同变量。让我们简要描述一下每个变量:</p><ul class=""><li id="f87c" class="nd ne it la b lb lc le lf lh nf ll ng lp nh lt ni nj nk nl bi translated"><strong class="la iu"> PassengerId </strong>:乘客的 ID；</li><li id="a298" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated"><strong class="la iu">幸存</strong>:目标变量；该人是在海难中幸存(1)还是没有在海难中幸存(0)；</li><li id="f800" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated"><strong class="la iu"> Pclass </strong>:乘客在船上所处的等级(1、2 或 3)；</li><li id="94d1" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated"><strong class="la iu">姓名</strong>:乘客姓名；</li><li id="eb2e" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated"><strong class="la iu">性别</strong>:乘客的性别；</li><li id="e835" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated"><strong class="la iu">年龄</strong>:乘客的年龄；</li><li id="0845" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated"><strong class="la iu"> SibSp </strong>:船上兄弟姐妹和配偶人数；</li><li id="2fb8" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated"><strong class="la iu"> Parch </strong>:船上父母/子女人数；</li><li id="d3e8" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated"><strong class="la iu">车票</strong>:车票号码；</li><li id="69b4" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated"><strong class="la iu">票价</strong>:车票的票价；</li><li id="fb83" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated"><strong class="la iu">舱室</strong>:舱室编号；</li><li id="84f6" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated"><strong class="la iu">登船</strong>:登船港(C =瑟堡，Q =皇后镇，S =南汉普顿)。</li></ul><h1 id="a13b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">预处理</h1><p id="a2e9" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">当应用任何预测算法时，我们可以<em class="nr">永远不要</em>在没有对数据进行任何<strong class="la iu">预处理</strong>的情况下立即使用它。这一步极其重要，绝不能忽视。对于该数据集，我们执行以下预处理步骤:</p><p id="c31f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 1。删除那些似乎不会给我们的模型增加任何价值的功能</strong></p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="4abd" class="mx lw it mt b gy my mz l na nb">titanic.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)</span></pre><p id="63a3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，我们从数据集中删除了 PassengerId、姓名、机票和客舱特性。其原因是，这些不能为我们的模型提供任何预测能力。</p><p id="4ec0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 2。为登船口创建分类模型</strong></p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="56ff" class="mx lw it mt b gy my mz l na nb">ports = pd.get_dummies(titanic.Embarked, prefix='Embarked')<br/>ports.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c9279b7349ac08d4d356b1de8888c377.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*yt3-WB3kXZo0pMbcSi09Xw.png"/></div></figure><p id="e68b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们从分类装载特征创建了三个二元特征，因为模型不能处理原始分类变量中的字符串名称。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="47ac" class="mx lw it mt b gy my mz l na nb">titanic = titanic.join(ports)<br/>titanic.drop(['Embarked'], axis=1, inplace=True)</span></pre><p id="c0d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 3。将性别名称转换为二进制</strong></p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="c79c" class="mx lw it mt b gy my mz l na nb">titanic.Sex = titanic.Sex.map({'male': 0, 'female': 1})</span></pre><p id="070b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过映射将“男性”和“女性”名称转换为二进制(0 和 1)。</p><p id="e619" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 4。替换丢失的值</strong></p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="7fbf" class="mx lw it mt b gy my mz l na nb">titanic[pd.isnull(titanic).any(axis=1)]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/980a8f67d5f548484796dd0312f54627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*shJ02-T8QvDvQyrb7mVXxA.png"/></div></div></figure><p id="9562" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，所有缺失值(nan)都出现在年龄要素中。为了解决这些缺失值，让我们用数据中的平均年龄来填充缺失值。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="da1b" class="mx lw it mt b gy my mz l na nb">titanic.Age.fillna(titanic.Age.mean(), inplace=True)</span></pre><h1 id="d920" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">训练和测试分割</h1><p id="4769" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">既然我们已经预处理了数据，我们可以提取 X 中的解释变量和 y 中的目标变量:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="0a1d" class="mx lw it mt b gy my mz l na nb">y = titanic.Survived.copy()<br/>X = titanic.drop(['Survived'], axis=1)</span></pre><p id="9529" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我们将数据分成训练集和测试集。<strong class="la iu">训练集</strong>用于训练逻辑回归模型。该模型从训练集中包括的特征中学习。<strong class="la iu">测试集</strong>用于验证逻辑回归模型的性能。对于测试集中的每个观察值，我们预测该人是否幸存，并将预测值与真实值进行比较。</p><p id="19a1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们拆分数据，使训练集包含 75%的数据，测试集包含 25%的数据。我们使用 scikit-learn 包的<code class="fe nu nv nw mt b">train_test_split</code>模块。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="4f09" class="mx lw it mt b gy my mz l na nb">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)</span></pre><h1 id="b587" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">逻辑回归模型</strong></h1><p id="e6e9" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">通过使用 scikit-learn 包中的<code class="fe nu nv nw mt b">LogisticRegression</code>模块，我们可以使用 X_train 中包含的特性将逻辑回归模型拟合到训练数据。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="5c06" class="mx lw it mt b gy my mz l na nb">model = LogisticRegression()<br/>model.fit(X_train, y_train)</span></pre><p id="7b29" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，现在我们已经根据训练数据训练了逻辑回归模型，我们能够使用该模型来预测测试集中包括的人是否在海难中幸存:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="76bb" class="mx lw it mt b gy my mz l na nb">y_pred = pd.Series(model.predict(X_test))</span><span id="82ef" class="mx lw it mt b gy nx mz l na nb">y_test = y_test.reset_index(drop=True)<br/>z = pd.concat([y_test, y_pred], axis=1)<br/>z.columns = ['True', 'Prediction']<br/>z.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/73993390523b4d66006047d8fd4618d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*4TXH6nk7ihZscnosM0Km3Q.png"/></div></figure><p id="d639" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，我们看到，对于测试集的前五个观察值，逻辑回归模型正确预测了 5 个中的 4 个。</p><p id="904b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了评估整个测试集，我们可以使用 scikit-learn 包中的<code class="fe nu nv nw mt b">metrics</code>模块。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="10cd" class="mx lw it mt b gy my mz l na nb">print("Accuracy:", metrics.accuracy_score(y_test, y_pred))<br/>print("Precision:", metrics.precision_score(y_test, y_pred))<br/>print("Recall:", metrics.recall_score(y_test, y_pred))</span></pre><p id="cb2c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="f72f" class="mx lw it mt b gy my mz l na nb">Accuracy: 0.8026905829596412<br/>Precision: 0.7631578947368421<br/>Recall: 0.6904761904761905</span></pre><p id="4e7a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也就是说，逻辑回归模型导致 80.3%的准确性。这么简单的车型绝对不差！</p><p id="fdae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，可以通过例如进行进一步的预处理、特征选择和特征提取来进一步提高模型性能。然而，这个模型形成了一个坚实的基线。</p><p id="3899" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可视化模型结果的一个好方法是利用<strong class="la iu">混淆矩阵</strong>。这里，我们使用<code class="fe nu nv nw mt b">matplotlib</code>和<code class="fe nu nv nw mt b">seaborn</code>来创建一个漂亮的混淆矩阵图。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="8fa2" class="mx lw it mt b gy my mz l na nb">cnf_matrix = metrics.confusion_matrix(y_test, y_pred)<br/><br/>labels = [0, 1]<br/>fig, ax = plt.subplots()<br/>tick_marks = np.arange(len(labels))<br/>plt.xticks(tick_marks, labels)<br/>plt.yticks(tick_marks, labels)<br/># create heatmap<br/>sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu", fmt='g')<br/>ax.xaxis.set_label_position("top")<br/>plt.title('Confusion matrix', y=1.1)<br/>plt.ylabel('True')<br/>plt.xlabel('Predicted')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/77f5af1e3532ae9ebc743634120ecf74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*WDuRkqVKcuIQ0lUErFVjEQ.png"/></div></figure><p id="5187" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在混淆矩阵中，我们看到 121 名海难幸存者被正确预测为未幸存，18 名海难幸存者被错误预测为海难幸存者，26 名海难幸存者被错误预测为海难幸存者，最后，58 名海难幸存者被正确预测。</p><h1 id="1478" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">逻辑回归的利与弊</h1><p id="6745" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated"><strong class="la iu">优点:</strong></p><ul class=""><li id="6f94" class="nd ne it la b lb lc le lf lh nf ll ng lp nh lt ni nj nk nl bi translated">不需要很高的计算能力；</li><li id="3c55" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated">易于实现；</li><li id="0e01" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated">简单易懂。</li></ul><p id="1a60" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">缺点:</strong></p><ul class=""><li id="5c1e" class="nd ne it la b lb lc le lf lh nf ll ng lp nh lt ni nj nk nl bi translated">容易过度拟合；</li><li id="a08d" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated">无法解决非线性问题；</li><li id="7cee" class="nd ne it la b lb nm le nn lh no ll np lp nq lt ni nj nk nl bi translated">无法处理大量的分类特征。</li></ul><h1 id="6ab6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">感谢阅读！</h1><p id="75cc" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">希望在阅读完本文后，您现在能够通过使用 Python 中的 scikit-learn 包来自己使用逻辑回归技术。祝你在自己的项目中使用它好运！</p></div></div>    
</body>
</html>