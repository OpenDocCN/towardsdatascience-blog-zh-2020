<html>
<head>
<title>Comparing Classification Models for Wine Quality Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">葡萄酒质量预测分类模型的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparing-classification-models-for-wine-quality-prediction-6c5f26669a4f?source=collection_archive---------12-----------------------#2020-05-21">https://towardsdatascience.com/comparing-classification-models-for-wine-quality-prediction-6c5f26669a4f?source=collection_archive---------12-----------------------#2020-05-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="770d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">选择监督机器学习模型预测葡萄酒质量的项目演练</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2e52cd0338b9e5479713e0e828ea969f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6YrgOeyNdBl7iuXN"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Matthieu Joannon 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="c95b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">目录</h1><ol class=""><li id="487f" class="lr ls it lt b lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><a class="ae ky" href="#c68e" rel="noopener ugc nofollow">简介</a></li><li id="3e03" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" href="#4fb5" rel="noopener ugc nofollow">探索性分析</a></li><li id="16a8" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" href="#79c6" rel="noopener ugc nofollow">数据预处理</a></li><li id="084c" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" href="#e4de" rel="noopener ugc nofollow">验证和模型选择</a></li><li id="0907" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" href="#51fc" rel="noopener ugc nofollow">结论</a></li></ol></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="c68e" class="kz la it bd lb lc mv le lf lg mw li lj jz mx ka ll kc my kd ln kf mz kg lp lq bi translated"><em class="na">简介</em></h1><p id="cace" class="pw-post-body-paragraph nb nc it lt b lu lv ju nd lw lx jx ne ly nf ng nh ma ni nj nk mc nl nm nn me im bi translated">这是我的第一篇博文！由于我决定将职业从化学工程转向数据科学，所以我将自己置于一个持续的、永无止境的学习过程中，我决定以博客帖子的形式分享我在项目中所学到和应用的东西。</p><p id="86ca" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">在这个项目中，我想比较几个分类算法来预测葡萄酒质量，得分在 0 到 10 之间。由于我更喜欢白葡萄酒而不是红葡萄酒，我决定通过使用来自<a class="ae ky" href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality" rel="noopener ugc nofollow" target="_blank"> UCI 机器学习库</a>的 winequality-white.csv 数据来比较和选择一种算法，以找出是什么造就了一种好酒。该数据集中的属性包括:</p><ul class=""><li id="076a" class="lr ls it lt b lu no lw np ly nt ma nu mc nv me nw mg mh mi bi translated">固定酸度</li><li id="c16a" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nw mg mh mi bi translated">挥发性酸度</li><li id="c900" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nw mg mh mi bi translated">柠檬酸</li><li id="ffbc" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nw mg mh mi bi translated">残糖</li><li id="0e08" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nw mg mh mi bi translated">氯化物</li><li id="f450" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nw mg mh mi bi translated">游离二氧化硫</li><li id="bcd2" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nw mg mh mi bi translated">二氧化硫总量</li><li id="d05d" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nw mg mh mi bi translated">密度</li><li id="4c4d" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nw mg mh mi bi translated">pH 值</li><li id="e24b" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nw mg mh mi bi translated">硫酸盐化</li><li id="38f5" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nw mg mh mi bi translated">酒精</li><li id="92c0" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nw mg mh mi bi translated">质量</li></ul></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="4fb5" class="kz la it bd lb lc mv le lf lg mw li lj jz mx ka ll kc my kd ln kf mz kg lp lq bi translated">探索性分析</h1><p id="37c5" class="pw-post-body-paragraph nb nc it lt b lu lv ju nd lw lx jx ne ly nf ng nh ma ni nj nk mc nl nm nn me im bi translated">探索性数据分析是一个非常重要的步骤，有助于了解数据集是什么样的，以及我们需要做什么样的修改。我开始导入库和模块，并将我将使用的数据读入 pandas dataframe。</p><h2 id="5396" class="nx la it bd lb ny nz dn lf oa ob dp lj ly oc od ll ma oe of ln mc og oh lp oi bi translated">导入库和模块</h2><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="5646" class="nx la it ok b gy oo op l oq or">import matplotlib.pyplot as plt<br/>from scipy import stats<br/>import seaborn as sns<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn import metrics</span></pre><h2 id="fba5" class="nx la it bd lb ny nz dn lf oa ob dp lj ly oc od ll ma oe of ln mc og oh lp oi bi translated"><strong class="ak">读取数据</strong></h2><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="17a4" class="nx la it ok b gy oo op l oq or">white_wines = pd.read_csv('winequality/winequality-white.csv')</span></pre><h2 id="2b7b" class="nx la it bd lb ny nz dn lf oa ob dp lj ly oc od ll ma oe of ln mc og oh lp oi bi translated">理解数据</h2><p id="985f" class="pw-post-body-paragraph nb nc it lt b lu lv ju nd lw lx jx ne ly nf ng nh ma ni nj nk mc nl nm nn me im bi translated">为了理解这些数据，我查看了前几行。它看起来很干净，但是它有 4898 行数据，仅仅通过查看来检查所有的行以确定数据是否有问题是不明智的。所以我将在下一步中立刻检查所有丢失的值。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="24aa" class="nx la it ok b gy oo op l oq or">print(white_wines.shape)<br/>white_wines.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/725ed0d2fe1370feaf7b5b7d8f5b1cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PpXPU-UnVo_cxHhhHPhr_A.png"/></div></div></figure><p id="d7c0" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu">检查缺失值</strong></p><p id="529f" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">数据中没有缺失值，因此可以进行分析，但在建模步骤之前，必须首先检测并消除异常值。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="10a8" class="nx la it ok b gy oo op l oq or">white_wines.isnull().any()<br/></span><span id="c788" class="nx la it ok b gy ot op l oq or">fixed acidity           False<br/>volatile acidity        False<br/>citric acid             False<br/>residual sugar          False<br/>chlorides               False<br/>free sulfur dioxide     False<br/>total sulfur dioxide    False<br/>density                 False<br/>pH                      False<br/>sulphates               False<br/>alcohol                 False<br/>quality                 False<br/>dtype: bool</span></pre><p id="86bc" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu">检测异常值</strong></p><p id="8ee3" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">我应用了 df.describe()方法来了解数据集的描述性统计信息。对数据集中的问题有一个直观的了解是一个非常好的方法。快速浏览一下，我可以看到有一些异常值。</p><p id="4680" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">让我们看看‘残糖’一栏。<em class="ou"> count </em>表示这一列有 4898 行。<em class="ou">表示</em>,<em class="ou">STD</em>分别表示该列的平均值和标准偏差值，其中<em class="ou"> 25% </em>小于 1.70，<em class="ou"> 75% </em>小于 9.90。有趣的是，平均值为 6.39，最小值为 0.60，最大值为 65.80。它看起来像一个离群值。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="bdc0" class="nx la it ok b gy oo op l oq or">white_wines.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/20a2c329397f001e9b15dd7b7204b35e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jKyZMeqrJiNWN5nH1UG96Q.png"/></div></div></figure><p id="b6a5" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">我使用箱线图来可视化“残余糖”列中的值的分布，以获得更好的洞察力。实际最大值约为 20，大于该值的值为异常值，因为它们不包括在观察范围内。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="ce48" class="nx la it ok b gy oo op l oq or">sns.boxplot(white_wines[‘residual sugar’])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/84e30c8ee9baa0d18704aa8c00af3ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*o3_chbO39I0mz0Ao3Y9cYA.png"/></div></figure><p id="31f4" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu">使用 Z 值消除异常值</strong></p><p id="7e3f" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">大多数时候，删除离群值很重要，因为它们很可能会影响机器学习模型的性能。但是假设你的数据集有 30%是异常值。那么将它们全部移除可能是不明智的，因为可能有更多的问题需要进一步检查。为了找到并去除异常值，我使用了 z-score。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/7a0cc07dde9c2a2f9c1be8e8d561e157.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*g1xhcuv3IHHm7EKUP5JkSA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">z 分数的数学公式</p></figure><p id="142f" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">它的解释是取数据点或观察值，减去总体的平均值，然后除以标准差。它表示一个数据点距离平均值有多少标准差。离平均值太远的数据点被认为是异常值。在大多数情况下，异常值检测的阈值是 z 得分&gt; 3 或 z 得分</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="a309" class="nx la it ok b gy oo op l oq or">z = np.abs(stats.zscore(white_wines))<br/>white_wines = white_wines[(z &lt; 3).all(axis=1)]<br/>white_wines.shape</span><span id="c4e7" class="nx la it ok b gy ot op l oq or">(4487, 12)</span></pre><p id="bd37" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu">检查属性之间的相关性</strong></p><p id="def2" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">当我检查列之间的相关性时，我可以看到一些特性与质量有很强的相关性，而另一些则没有。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="402b" class="nx la it ok b gy oo op l oq or">plt.subplots(figsize=(15, 10))<br/>sns.heatmap(white_wines.corr(), annot = True, cmap = ‘coolwarm’)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/336f7d726b463e27832802d03399979f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MvS4Bj_8raH82NFI33zCnA.png"/></div></div></figure><p id="9707" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu">检查班级不平衡</strong></p><p id="e5d3" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">作为探索性数据分析的最后一步，我想检查班级的不平衡。似乎存在着严重的阶级不平衡，少数阶级的代表性低于多数阶级。这将是后面步骤中建模的关键部分。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="7693" class="nx la it ok b gy oo op l oq or">white_wines[‘quality’].value_counts()</span><span id="5bdb" class="nx la it ok b gy ot op l oq or">6    2038<br/>5    1309<br/>7     855<br/>8     161<br/>4     124<br/>Name: quality, dtype: int64</span></pre></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="79c6" class="kz la it bd lb lc mv le lf lg mw li lj jz mx ka ll kc my kd ln kf mz kg lp lq bi translated">数据预处理</h1><p id="2e2c" class="pw-post-body-paragraph nb nc it lt b lu lv ju nd lw lx jx ne ly nf ng nh ma ni nj nk mc nl nm nn me im bi translated">在这一步的分析中，我定义了训练和测试机器学习模型的特征以及预测“质量”的目标。然后我对这些特征进行了标准化(也称为 z 分数标准化)，因为不同规模的特征可能会影响机器学习模型的性能。为此，我使用了 Scikit-learn 中定义的 StandardScaler()函数。最后，我将数据集分成 80%的训练集和 20%的测试集。</p><p id="a637" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu">定义特征和目标</strong></p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="580d" class="nx la it ok b gy oo op l oq or"># Define features X<br/>X = np.asarray(white_wines.iloc[:,:-1])</span><span id="480b" class="nx la it ok b gy ot op l oq or"># Define target y<br/>y = np.asarray(white_wines[‘quality’])</span></pre><p id="be6c" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu">标准化数据集</strong></p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="1ce0" class="nx la it ok b gy oo op l oq or">from sklearn import preprocessing</span><span id="6383" class="nx la it ok b gy ot op l oq or">X = preprocessing.StandardScaler().fit(X).transform(X)</span></pre><p id="5144" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu">训练和测试集分离</strong></p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="a8cb" class="nx la it ok b gy oo op l oq or">from sklearn.model_selection import train_test_split</span><span id="83a3" class="nx la it ok b gy ot op l oq or">X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=0)<br/>print (‘Train set:’, X_train.shape, y_train.shape)<br/>print (‘Test set:’, X_test.shape, y_test.shape)</span><span id="b4a1" class="nx la it ok b gy ot op l oq or">Train set: (3589, 11) (3589,)<br/>Test set: (898, 11) (898,)</span></pre></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="e4de" class="kz la it bd lb lc mv le lf lg mw li lj jz mx ka ll kc my kd ln kf mz kg lp lq bi translated">验证和模型选择</h1><p id="d169" class="pw-post-body-paragraph nb nc it lt b lu lv ju nd lw lx jx ne ly nf ng nh ma ni nj nk mc nl nm nn me im bi translated">在这一部分中，我训练了几种分类算法，以找到最适合我所使用的数据集的算法。</p><h2 id="1bab" class="nx la it bd lb ny nz dn lf oa ob dp lj ly oc od ll ma oe of ln mc og oh lp oi bi translated">k-最近邻</h2><p id="178a" class="pw-post-body-paragraph nb nc it lt b lu lv ju nd lw lx jx ne ly nf ng nh ma ni nj nk mc nl nm nn me im bi translated">我从 K-最近邻分类算法开始。该算法所做的是获取一个数据点，并在训练数据中选择 K 个最接近该数据点的观察值，然后根据来自 K 个最近邻居的最受欢迎的响应值来预测该数据点的响应。</p><p id="6c6e" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu"><em class="ou">KNN</em>精准剧情</strong></p><p id="cc55" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">我画了一个图，看看精度是如何随着 k 的数量而变化的。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="967a" class="nx la it ok b gy oo op l oq or">from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import cross_val_score</span><span id="8d46" class="nx la it ok b gy ot op l oq or"># Number of k from 1 to 26<br/>k_range = range(1, 26)</span><span id="5c6a" class="nx la it ok b gy ot op l oq or">k_scores = []</span><span id="a533" class="nx la it ok b gy ot op l oq or"># Calculate cross validation score for every k number from 1 to 26<br/>for k in k_range:<br/> knn = KNeighborsClassifier(n_neighbors=k)</span><span id="802e" class="nx la it ok b gy ot op l oq or"># It’s 10 fold cross validation with ‘accuracy’ scoring <br/>scores = cross_val_score(knn, X, y, cv=10, scoring=’accuracy’) <br/> k_scores.append(scores.mean())</span><span id="ce1e" class="nx la it ok b gy ot op l oq or">%matplotlib inline</span><span id="e35d" class="nx la it ok b gy ot op l oq or"># Plot accuracy for every k number between 1 and 26<br/>plt.plot(k_range, k_scores)<br/>plt.xlabel('Value of K for KNN')<br/>plt.ylabel('Cross-validated accuracy')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/a415b624635016c19a5ce1fe1e6674e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*EWLyR-f2pXCuSVGc_N8x6Q.png"/></div></figure><p id="f7c9" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu"> <em class="ou">交叉验证为 KNN </em> </strong></p><p id="b183" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">我决定用 k=19，因为用它可以获得最高的精度。并用不同的验证方法对模型进行训练并计算精度。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="eb3d" class="nx la it ok b gy oo op l oq or"># Train the model and predict for k=19</span><span id="6adf" class="nx la it ok b gy ot op l oq or">knn = KNeighborsClassifier(n_neighbors=19)<br/>knn.fit(X_train, y_train)<br/>y_pred = knn.predict(X_test)</span><span id="7ec9" class="nx la it ok b gy ot op l oq or">from sklearn.metrics import accuracy_score<br/>from sklearn.metrics import roc_auc_score</span><span id="46fe" class="nx la it ok b gy ot op l oq or"># classification report for test set<br/>print(metrics.classification_report(y_test, y_pred, digits=3, zero_division = 1))</span><span id="cffb" class="nx la it ok b gy ot op l oq or"># Calculate cv score with 'accuracy' scoring and 10 folds<br/>accuracy = cross_val_score(knn, X, y, scoring = 'accuracy',cv=10)<br/>print('cross validation score',accuracy.mean())</span><span id="b242" class="nx la it ok b gy ot op l oq or"># Calculate cv score with 'roc_auc_ovr' scoring and 10 folds<br/>accuracy = cross_val_score(knn, X, y, scoring = 'roc_auc_ovr',cv=10)<br/>print('cross validation score with roc_auc',accuracy.mean())</span><span id="0539" class="nx la it ok b gy ot op l oq or"># Calculate roc_auc score with multiclass parameter<br/>print('roc_auc_score',roc_auc_score(y_test,knn.predict_proba(X_test), multi_class='ovr'))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/00ebace058ded5c928cefe629fb93643.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*Jfn7JZmXZssyJfFeuOypoA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">KNN 的分类表和验证分数</p></figure><p id="68a5" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">当我查看分类报告时，我立即发现训练时没有考虑类别 4 和类别 8，因为它们的回忆结果为零。这意味着，在所有第 4 类和第 8 类成员中，它没有正确预测任何一个。所以，对于我们的数据集来说，这不是一个好的模型。</p><h2 id="6d07" class="nx la it bd lb ny nz dn lf oa ob dp lj ly oc od ll ma oe of ln mc og oh lp oi bi translated">逻辑回归</h2><p id="7550" class="pw-post-body-paragraph nb nc it lt b lu lv ju nd lw lx jx ne ly nf ng nh ma ni nj nk mc nl nm nn me im bi translated">逻辑回归实际上是一种二元分类算法，可用于是/否、真/假等问题。</p><p id="d3a5" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">在这种情况下，它允许我们将它用于多类分类问题，如我们的问题。因为在我们的数据集中，有 5 类质量被预测为。为了将其用作多类分类算法，我使用了 multi_class= '多项式'，solver = '牛顿-cg '参数。</p><p id="9303" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">考虑到这是一个多类分类问题，我在计算交叉验证分数时使用了“roc_auc_ovr”评分参数，而不是“准确性”。我还用 multi_class='ovr '参数计算了 roc_auc_score。我将在后面的结论中解释这些。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="b4e8" class="nx la it ok b gy oo op l oq or"># import module<br/>from sklearn.linear_model import LogisticRegression</span><span id="7b94" class="nx la it ok b gy ot op l oq or"># Train and fit model<br/>logreg = LogisticRegression(multi_class=’multinomial’,solver =’newton-cg’)<br/>logreg.fit(X_train, y_train)</span><span id="95b2" class="nx la it ok b gy ot op l oq or"># Predict out-of-sample test set<br/>y_pred = logreg.predict(X_test)</span><span id="e76e" class="nx la it ok b gy ot op l oq or"># classification report<br/>print(metrics.classification_report(y_test, y_pred, digits=3, zero_division = 1))</span><span id="4825" class="nx la it ok b gy ot op l oq or">print(‘accuracy’,accuracy_score(y_test, y_pred))</span><span id="f3e9" class="nx la it ok b gy ot op l oq or"># Calculate cv score with ‘roc_auc_ovr’ scoring and 10 folds<br/>accuracy = cross_val_score(logreg, X, y, scoring = ‘roc_auc_ovr’,cv=10)<br/>print(‘cross validation score with roc_auc’,accuracy.mean())</span><span id="7917" class="nx la it ok b gy ot op l oq or"># Calculate roc_auc score with multiclass parameter<br/>print(‘roc_auc_score’,roc_auc_score(y_test,logreg.predict_proba(X_test), multi_class=’ovr’))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/50c640a866839895e5ed71a9e132ff25.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*77OTB-Vc_qk9vt131hg_0g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">逻辑回归的分类表和验证分数</p></figure><p id="990b" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">即使交叉验证分数稍微高一点，一些召回结果仍然是零。让我们看看如果我们添加一些多项式特征会发生什么。</p><p id="d8df" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu">向逻辑回归添加多项式特征</strong></p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="abb2" class="nx la it ok b gy oo op l oq or">from sklearn.preprocessing import PolynomialFeatures<br/>from sklearn.pipeline import make_pipeline</span><span id="e0a5" class="nx la it ok b gy ot op l oq or"># Add polynomial features to the logistic regression model</span><span id="19fc" class="nx la it ok b gy ot op l oq or">def PolynomialRegression(degree=2, **kwargs):<br/> return make_pipeline(PolynomialFeatures(degree),<br/> LogisticRegression(multi_class=’multinomial’,solver =’newton-cg’, **kwargs))</span></pre><p id="f100" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">现在，我尝试向逻辑回归模型添加三次多项式特征。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="9eff" class="nx la it ok b gy oo op l oq or"># Train and fit the 3rd degree polynomial regression model<br/>poly = PolynomialRegression(3)<br/>poly.fit(X_train,y_train)</span><span id="7bc8" class="nx la it ok b gy ot op l oq or"># Test out-of-sample test set<br/>y_pred = poly.predict(X_test)</span><span id="5443" class="nx la it ok b gy ot op l oq or"># Classification report<br/>print(metrics.classification_report(y_test, y_pred, digits=3))</span><span id="6f12" class="nx la it ok b gy ot op l oq or"># Calculate cv score with 'roc_auc_ovr' scoring and 10 folds<br/>accuracy = cross_val_score(poly, X, y, scoring = 'roc_auc_ovr',cv=10)<br/>print('cross validation score with roc_auc_ovr scoring',accuracy.mean())</span><span id="a6a1" class="nx la it ok b gy ot op l oq or"># Calculate roc_auc score with multiclass parameter<br/>print('roc_auc_score',roc_auc_score(y_test,poly.predict_proba(X_test), multi_class='ovr'))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/f3576278911546d93ab1f2454b107eef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*j_qEhFnP4EdzJliFgfA-Lg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">三次多项式回归的分类表和验证分数</p></figure><p id="b306" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">最后，在预测时，我有一些少数类的代表，但它们非常低，交叉验证分数也比以前低。</p><h2 id="6acb" class="nx la it bd lb ny nz dn lf oa ob dp lj ly oc od ll ma oe of ln mc og oh lp oi bi translated">决策图表</h2><p id="6525" class="pw-post-body-paragraph nb nc it lt b lu lv ju nd lw lx jx ne ly nf ng nh ma ni nj nk mc nl nm nn me im bi translated">决策树是数据科学领域中最常用的分类算法之一。当我将其应用到我的数据集时，召回结果有所增加，但交叉验证分数下降了。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="f9e9" class="nx la it ok b gy oo op l oq or">from sklearn.tree import DecisionTreeClassifier</span><span id="83d6" class="nx la it ok b gy ot op l oq or"># Train and fit the Decision Tree Classification model<br/>tree = DecisionTreeClassifier(random_state=0)<br/>tree.fit(X_train, y_train)</span><span id="094f" class="nx la it ok b gy ot op l oq or"># Evaluate the model with out-of-sample test set<br/>y_pred = tree.predict(X_test)</span><span id="8385" class="nx la it ok b gy ot op l oq or"># Classification report<br/>print(metrics.classification_report(y_test, y_pred.round(), digits=3))</span><span id="b058" class="nx la it ok b gy ot op l oq or"># Calculate cv score with ‘roc_auc_ovr’ scoring and 10 folds<br/>accuracy = cross_val_score(tree, X, y,scoring = ‘roc_auc_ovr’,cv=10)<br/>print(‘cross validation score with roc_auc_ovr scoring’,accuracy.mean())</span><span id="e819" class="nx la it ok b gy ot op l oq or"># Calculate roc_auc score with multiclass parameter<br/>print(‘roc_auc_score’,roc_auc_score(y_test,tree.predict_proba(X_test), multi_class=’ovr’))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/bbd8f9da14f5a283960c548365e40f77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*5DYNYaV9WI2defllIvPeTA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树的分类表和验证分数</p></figure><h2 id="25da" class="nx la it bd lb ny nz dn lf oa ob dp lj ly oc od ll ma oe of ln mc og oh lp oi bi translated">随机森林</h2><p id="052c" class="pw-post-body-paragraph nb nc it lt b lu lv ju nd lw lx jx ne ly nf ng nh ma ni nj nk mc nl nm nn me im bi translated">随机森林是一种集成学习方法，它构建多个决策树，然后根据大多数决策树的预测结果进行预测。我喜欢把它想象成为了一个健康问题去看多个医生，然后根据大多数医生的意见决定你是否应该做手术。</p><p id="3cfd" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">所以让我们看看随机森林模型的结果。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="b0ed" class="nx la it ok b gy oo op l oq or">from sklearn.ensemble import RandomForestClassifier</span><span id="fe6e" class="nx la it ok b gy ot op l oq or"># Train and fit the Random Forest Classification model<br/>forest = RandomForestClassifier(n_estimators=100,random_state = 0)<br/>forest.fit(X_train, y_train)</span><span id="c504" class="nx la it ok b gy ot op l oq or"># Test out-of-sample test set<br/>y_pred = forest.predict(X_test)</span><span id="c248" class="nx la it ok b gy ot op l oq or"># Classification report<br/>print(metrics.classification_report(y_test, y_pred.round(), digits=3))</span><span id="b323" class="nx la it ok b gy ot op l oq or"># Calculate cv score with 'roc_auc_ovr' scoring and 10 folds<br/>accuracy = cross_val_score(forest, X, y,scoring = 'roc_auc_ovr',cv=10)<br/>print('cross validation score with roc_auc_ovr scoring',accuracy.mean())</span><span id="f039" class="nx la it ok b gy ot op l oq or"># Calculate roc_auc score with multiclass parameter<br/>print('roc_auc_score',roc_auc_score(y_test,forest.predict_proba(X_test), multi_class='ovr'))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/0953d2cc5cf110876688e3f01d942522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*4Us1sEr-Ghz40rHG77oy6A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树的分类表和验证分数</p></figure><p id="80b6" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">是目前为止最好的！roc_auc_score 非常好，交叉验证分数是目前为止最好的，甚至对于少数类也有一些召回结果。但这还不够。因此，可以提高召回率的一个方法是<em class="ou">过度采样</em>少数类。为此，我使用了随机森林算法和 SMOTE 算法实现。</p><p id="dfc7" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated"><strong class="lt iu">添加 SMOTE 算法</strong></p><p id="532c" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">SMOTE(合成少数过采样)算法创建合成少数类样本，以增加少数类的代表性。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="9108" class="nx la it ok b gy oo op l oq or"># Import SMOTE module<br/>from imblearn.over_sampling import SMOTE</span><span id="11d7" class="nx la it ok b gy ot op l oq or"># Create model and fit the training set to create a new training set<br/>sm = SMOTE(random_state = 2) <br/>X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())</span><span id="14ee" class="nx la it ok b gy ot op l oq or"># Create random forest model<br/>forest = RandomForestClassifier(n_estimators=100,random_state = 0)</span><span id="8415" class="nx la it ok b gy ot op l oq or"># Fit the model to the new train set<br/>forest.fit(X_train_res, y_train_res.ravel())</span><span id="3693" class="nx la it ok b gy ot op l oq or"># # Test out-of-sample test set<br/>y_pred = forest.predict(X_test)</span><span id="3700" class="nx la it ok b gy ot op l oq or"># Classification report<br/>print(metrics.classification_report(y_test, y_pred.round(), digits=3))</span><span id="a8f5" class="nx la it ok b gy ot op l oq or"># Calculate cv score with 'roc_auc_ovr' scoring and 10 folds<br/>accuracy = cross_val_score(forest, X, y,scoring = 'roc_auc_ovr',cv=10)</span><span id="667e" class="nx la it ok b gy ot op l oq or">print('cross validation score with roc_auc_ovr scoring',accuracy.mean())</span><span id="406e" class="nx la it ok b gy ot op l oq or"># Calculate roc_auc score with multiclass parameter<br/>print('roc_auc_score',roc_auc_score(y_test,forest.predict_proba(X_test), multi_class='ovr'))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/cc39d0df449c62ff54cdec1bd6d47eaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*NQmTg_XZXC-0ydKPxmIzSQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">SMOTE 随机森林的分类表和验证分数</p></figure><p id="0917" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">即使准确率几乎与前一个相同，少数民族班级的召回结果也显著增加。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="51fc" class="kz la it bd lb lc mv le lf lg mw li lj jz mx ka ll kc my kd ln kf mz kg lp lq bi translated">结论</h1><p id="51b3" class="pw-post-body-paragraph nb nc it lt b lu lv ju nd lw lx jx ne ly nf ng nh ma ni nj nk mc nl nm nn me im bi translated">在这个项目中，我使用了 K 近邻、多项式特征的逻辑回归、决策树和随机森林。利用 Scikit-learn 中的<code class="fe pg ph pi ok b">roc_auc_score</code>,我计算了每个模型的 AUC 分数。同样使用<code class="fe pg ph pi ok b">cross_val_score</code>方法，我通过传递<code class="fe pg ph pi ok b">roc_auc_ovr</code>参数使用交叉验证方法找到 AUC 分数。尽管它通常用于二进制分类中的性能评估，但通过一对多的方法，我将其应用于多类分类问题。当存在高等级不平衡时，用这种方法评估模型是有利的。也不需要设置分类阈值。</p><p id="016d" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">如果我们比较所有模型的交叉验证分数和召回结果，我们可以看到使用<strong class="lt iu">随机森林分类器</strong>和 SMOTE 方法得到的结果最好，交叉验证分数<strong class="lt iu">为 0.7565</strong>。由于我们的班级高度不平衡，SMOTE 创建了合成少数抽样来平衡样本。并训练模型，就好像我们的数据集中有平衡的类一样。</p><p id="bb90" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">尽管我们得到了最高的交叉验证分数，大约为 0.75，但它仍然不是一个理想的模型，因为召回结果对于任何类别都没有足够的代表性。在这种情况下，为了改进模型，我们需要更多的数据来训练它。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="4d2e" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">感谢您的阅读！</p><p id="6d92" class="pw-post-body-paragraph nb nc it lt b lu no ju nd lw np jx ne ly nq ng nh ma nr nj nk mc ns nm nn me im bi translated">如果您有任何反馈或更正，请随时通过我的<a class="ae ky" href="https://www.linkedin.com/in/guldenturgay/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我！</p></div></div>    
</body>
</html>