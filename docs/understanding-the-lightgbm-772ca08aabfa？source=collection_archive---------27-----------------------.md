# 了解光 GBM

> 原文：<https://towardsdatascience.com/understanding-the-lightgbm-772ca08aabfa?source=collection_archive---------27----------------------->

## 是什么让它更快更高效

![](img/15addbf4c78386625f42956e20fd497c.png)

[艾伦蔡](https://unsplash.com/@aycai?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/light?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

由微软研究人员创建的 LightGBM 是梯度增强决策树(GBDT)的一种实现，这是一种以串行方式(增强)组合决策树(作为弱学习器)的集成方法。

![](img/2fe3e38b8df9bab06132b5a87bc7ed14.png)

梯度增强决策树

决策树以这样一种方式组合，即每个新的学习者拟合来自前一个树的残差，从而改进模型。最终的模型汇总了每一步的结果，从而形成了一个强学习者。

GBDT 是如此准确，以至于它的实现一直主导着主要的机器学习竞赛。

# **灯光背后的动机 GBM**

决策树是通过基于特征值拆分观察值(即数据实例)来构建的。决策树就是这样“学习”的。该算法寻找导致最高信息增益的最佳分割。

**信息增益**基本就是分裂前后的熵差。熵是不确定性或随机性的度量。一个变量的随机性越大，熵就越大。因此，以减少随机性的方式进行分割。

寻找最佳分割是决策树学习过程中最耗时的部分。其他 GBDT 实现找到最佳分割的两种算法是:

*   预先排序:特征值预先排序，并评估所有可能的分割点。
*   基于直方图:连续特征被分成离散的箱，用于创建特征直方图。

“Sklearn GBDT”和“gbm in R”使用预排序算法，而“pGRT”使用基于直方图的算法。“xgboost”两者都支持。

基于直方图的算法在内存消耗和训练速度方面更有效。但是，随着实例或特征数量的增加，预排序和基于直方图的速度都会变慢。LightGBM 旨在解决这个效率问题，尤其是对于大型数据集。

# **是什么让 LightGBM 更加高效**

LightGBM 的起点是基于直方图的算法，因为它比预先排序的算法性能更好。

对于每个特征，扫描所有数据实例以找到关于信息增益的最佳分割。因此，基于直方图的算法的复杂性取决于数据实例和特征的数量。

为了解决这个问题，LightGBM 使用了两种技术:

*   梯度单侧采样
*   EFB(独家功能捆绑)

下面就来详细说说这些技术是做什么的，是如何让 LightGBM“轻”起来的。

# **高斯(梯度单侧采样)**

我们已经提到，一般的 GBDT 实现会扫描所有数据实例，以找到最佳分割。这绝对不是一个最优的方式。

如果我们能够根据信息增益对数据进行采样，算法将会更加有效。一种方法是根据权重对数据进行采样。但是，它不适用于 GBDT，因为在 GBDT 没有样品重量。

GOSS 通过使用梯度来解决这个问题，梯度给了我们对信息增益的有价值的洞察力。

*   小梯度:算法已经在这个实例上被训练，并且与它相关的误差很小。
*   大梯度:与该实例相关的误差很大，因此它将提供更多的信息增益。

我们可以排除梯度小的实例，只关注梯度大的实例。但是，在这种情况下，数据分布将会改变。我们不希望这样，因为这将对学习模型的准确性产生负面影响。

GOSS 提供了一种基于梯度的数据采样方法，同时考虑了数据分布。

它是这样工作的:

1.  数据实例根据其梯度的绝对值进行排序
2.  选择了前 ax100%个实例
3.  从剩余的实例中，选择大小为 bx100%的随机样本
4.  当计算信息增益时，小梯度的随机样本乘以等于(1-a) / b 的常数

GOSS 最终实现的是，模型的重点倾向于导致更多损失(即训练不足)的数据实例，而不会对数据分布产生太大影响。

# **EFB(独家功能捆绑)**

具有大量要素的数据集可能具有稀疏要素(即大量零值)。这些稀疏特征通常是互斥的，这意味着它们不会同时具有非零值。考虑独热编码文本数据的情况。在特定的行中，只有一列指示特定的单词是非零的，所有其他行都是零。

EFB 是一种使用贪婪算法将这些互斥组合(或捆绑)成单个特征(例如，互斥特征束)并因此降低维度的技术。EFB 减少了 GDBT 的训练时间，而不太影响准确性，因为创建特征直方图的复杂性现在与束的数量而不是特征的数量成比例(束的数量远小于特征的数量)。

EFB 面临的挑战之一是找到最佳捆绑包。微软的研究人员设计了一种算法，将捆绑问题转化为图形着色问题。

在图着色问题中，将特征作为顶点，在不互斥的特征之间添加边。然后，使用贪婪算法来产生束。

更进一步，该算法还允许捆绑很少同时具有非零值(即几乎互斥)的特征。

另一个挑战是以能够提取原始特征的值的方式将特征合并成束。考虑一组 3 个特性。我们需要能够使用捆绑特性的价值来确定这 3 个特性的价值。

回想一下，基于直方图的算法为连续值创建离散的仓。为了克服合并特征的挑战，将束中特征的唯一值放入不同的箱中，这可以通过向原始特征值添加偏移来实现。

# **结论**

LightGBM 背后的动机是解决在处理大型数据集时与 GBDTs 的传统实现相关的训练速度和内存消耗问题。

目标基本上是在尽可能保留信息的同时减少数据的大小(包括数据实例和特征)。戈斯和 EFB 技术是实现这一目标。

根据 LightGBM 创造者的[论文](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)，“LightGBM 将传统 GBDT 的训练过程加快了 20 倍以上，同时达到了几乎相同的精度”。

感谢您的阅读。如果您有任何反馈，请告诉我。

# **参考文献**

*   LightGBM:一个高效的梯度推进决策树