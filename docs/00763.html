<html>
<head>
<title>Intro to Segmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">细分简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/segmentation-u-net-mask-r-cnn-and-medical-applications-9a84bddf313e?source=collection_archive---------39-----------------------#2020-01-21">https://towardsdatascience.com/segmentation-u-net-mask-r-cnn-and-medical-applications-9a84bddf313e?source=collection_archive---------39-----------------------#2020-01-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="44d3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">U-Net、Mask R-CNN和医疗应用</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/65eef40372f7237079eb3d15c719431d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SegrvFeGYaL28SrphmCz5Q.png"/></div></div></figure><p id="97c7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">分割在医学成像(定位肿瘤、测量组织体积、研究解剖学、计划手术等)中有许多应用。)、自动驾驶汽车(定位行人、其他车辆、刹车灯等。)、卫星图像解译(建筑物、道路、森林、农作物)等等。</p><p id="70ff" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这篇文章将介绍分段任务。在第一部分中，我们将讨论语义分割和实例分割的区别。接下来，我们将深入研究用于语义分割的U-Net架构，并概述用于实例分割的Mask R-CNN架构。最后一节包括许多示例医学图像分割应用和视频分割应用。</p><h1 id="853d" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated"><strong class="ak">分割任务</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/f937dd2f3a176165675373bc8c665968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*PFbqsqNx12ZznXlL"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片来源:<a class="ae lq" href="https://arxiv.org/pdf/1405.0312.pdf" rel="noopener ugc nofollow" target="_blank">林等2015微软COCO:语境中的常见对象</a></p></figure><p id="5844" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">上图说明了四种常见的图像任务:</p><ul class=""><li id="b901" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated">(a)分类，其中模型输出图像中类别的名称(在这种情况下，人、羊和狗)；</li><li id="4c41" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">(b)目标定位，通常称为“目标检测”,其中模型输出图像中每个目标的边界框坐标；</li><li id="7235" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">(c)语义分割，其中模型为图像中的每个像素分配对象类别标签。在本例中，绵羊像素是蓝色的，狗像素是红色的，人像素是蓝绿色的，背景像素是绿色的。请注意，虽然图像中有多只绵羊，但它们都有相同的标签。</li><li id="302f" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">(d)实例分割，其中模型给图像中的每个像素分配一个“单个对象”标签。在这个例子中，每只羊的像素被分别标记。我们没有一个通用的“sheep”像素类，而是为五只绵羊创建了五个类:sheep1、sheep2、sheep3、sheep4和sheep5。</li></ul><p id="e2f7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是语义分割和实例分割之间差异的另一个说明，显示了在语义分割中所有“椅子”像素如何具有相同的标签，而在实例分割中，模型已经识别了特定的椅子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/8281df2f77d033d8752f672d31349283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oQyolp17m2yFAQ5b5gaWBQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片来源:<a class="ae lq" href="https://datascience.stackexchange.com/questions/52015/what-is-the-difference-between-semantic-segmentation-object-detection-and-insta" rel="noopener ugc nofollow" target="_blank"> StackExchange </a></p></figure><p id="a908" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下是对藻类图像的语义分割和实例分割的应用:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/242f2f1021403526f4f327b6e300a59b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*Y7jMr5K3xS2jeq9E"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片来源:<a class="ae lq" href="https://www.sciencedirect.com/science/article/pii/S0952197619302398?dgcid=rss_sd_all" rel="noopener ugc nofollow" target="_blank"> Ruiz-Santaquiteria等2020微观藻类检测中的语义与实例分割。</a></p></figure><p id="2677" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下图显示了实例分段的模型预测:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/6843b84e95ff0dc4759bf56607d71ee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/0*W3q8oveqIjxIVKDb"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片来源:<a class="ae lq" href="https://www.arxiv-vanity.com/papers/1712.04837/" rel="noopener ugc nofollow" target="_blank">陈等MaskLab:用语义和方向特征细化对象检测的实例分割。</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/f76fa0bc2eb7d34aff214b636e5288a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*uQp7XFy1F4RJGPYD"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片来源:<a class="ae lq" href="https://arxiv.org/pdf/1506.06204.pdf" rel="noopener ugc nofollow" target="_blank">皮涅罗等2015学习分割对象候选人。</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/c699e645eb1a77d7420894094348ae84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*yS6ltud-dr9WQz-g"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片来源:<a class="ae lq" href="https://github.com/matterport/Mask_RCNN" rel="noopener ugc nofollow" target="_blank"> matterport/Mask_RCNN </a></p></figure><h1 id="0884" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated"><strong class="ak"> U-Net:用于生物医学图像分割的卷积网络</strong></h1><p id="40e6" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">U-Net论文(可从这里获得:<a class="ae lq" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank"> Ronneberger et al. 2015 </a>)介绍了一种语义分割模型架构，这种架构已经变得非常受欢迎，被引用超过10，000次(在这个知识库中列出了五十篇不同的后续论文<a class="ae lq" href="https://github.com/ShawnBIT/UNet-family" rel="noopener ugc nofollow" target="_blank">)。它最初是在生物医学图像的背景下提出的，但后来也被应用于自然图像。</a></p><p id="7b34" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">U-Net的基本思想是执行以下任务:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/0ce6e033ce347b915f6ce363c28bf011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*n5Elm_Xfqoj33NzD"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">细胞/掩模图像来源:<a class="ae lq" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank"> Ronneberger等人2015 </a></p></figure><p id="30e6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">给定输入图像，在这种情况下，是细胞的灰度显微图像，U-Net模型产生1和0的二进制掩码，其中1表示细胞，0表示背景(包括细胞之间的边界)。请注意，这是一个语义分割任务，因为所有细胞都接收相同的“细胞”标签(即，我们没有不同的标签来区分不同的单个细胞，就像我们例如分割一样。)</p><p id="8b85" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该示意图显示了训练U-Net模型的设置:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/008399579a23ab246efdfd5c001faee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*o2RtjUG5X2zbUg0b"/></div></figure><p id="687a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在模型被完全训练之前，对于给定的输入图像，它将产生有问题的二进制分割掩模，例如上图所示的“预测的二进制分割掩模”，其中一些单元丢失或具有不正确的边界。U-Net loss函数将预测掩码与真实掩码进行比较，以实现参数更新，这将允许模型在下一个训练示例上执行更好的分割。</p><p id="833d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是U-Net架构:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/bacfca22344ad6186f9b98b0f1a9673f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*feAV-BtnlVUzqfSE"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图一。Ronneberger等人的2015年</p></figure><p id="b69e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从这个图中可以看出,“U-Net”这个名字是显而易见的，因为架构图显示了一个U形。U-Net的基本思想是首先通过传统的卷积神经网络获得图像的低维表示，然后对该低维表示进行上采样，以产生最终的输出分割图。</p><p id="dbd6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用右下角提供的架构图很有帮助，它解释了每种箭头类型对应的操作:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9ce1f8636fea2d96b1b05c5c0276438e.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/0*w0GUEsXCR30naaZL"/></div></figure><p id="b3fd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">U-Net由“收缩路径”和“扩张路径”组成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/b2fa60d9dab6e9405b2dda85d7b9fb1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*7ImkVEfCo3QF20WD"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">由图1修改而来。2015年</p></figure><p id="ff78" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">“收缩路径”是产生低维表示的传统CNN。“扩展路径”对表示进行上采样，以产生最终的输出分割图。灰色箭头表示复制操作，其中来自“收缩路径”的高分辨率特征地图被复制并连接到“扩展路径”中的特征地图，以使网络更容易学习高分辨率分割。</p><p id="ca90" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">U-Net没有任何完全连接的层，这意味着U-Net是一个完全卷积的网络。</p><h2 id="bf05" class="nk ls it bd lt nl nm dn lx nn no dp mb ld np nq md lh nr ns mf ll nt nu mh nv bi translated"><strong class="ak">生成预测分割图:1×1卷积和像素级Softmax </strong></h2><p id="7efd" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">在U-Net的最后一层，应用1×1卷积将每个64通道特征向量映射到期望数量的类别，在本文中被认为是两个类别(细胞/背景)。如果你想使用U-Net对几个类别进行语义分割，例如6个类别(狗、猫、鸟、龟、牛、背景)，那么64通道特征向量可以映射到6个类别(6个通道)。</p><p id="979b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是图1的特写，显示了“扩展路径”的最后一部分，其中通过[conv 3×3，ReLU]操作产生64通道特征向量，并最终使用代表1×1卷积的蓝绿色箭头映射到2通道特征向量(细胞与背景):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/200ccb7dd6fd2c8395a0cd4ea65f275a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*syexjIg--PUrDd3A"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">修改自<a class="ae lq" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank"> Ronneberger等人2015 </a>的图1</p></figure><p id="8d55" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将逐像素的softmax应用于最终的[2通道，388高度，388宽度]表示，以获得最终的输出，即预测的分割图。</p><p id="f8ad" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">像素级softmax函数为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/4ef0fe68e1c05290565851949736e3cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/0*G56hmboe9O_-q5g6"/></div></figure><p id="a11d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">关于softmax函数的更多细节，见<a class="ae lq" href="https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。</p><p id="d521" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">基于像素的softmax可以概念化如下。将输出地图想象成388 x 388的图像。该图像中的每个像素由K个值表示，K个通道各有一个值，其中K是感兴趣类别的数量。对于每个像素，我们在K个通道上取一个softmax，这样一个通道将“突出”为最高值；这个最高通道确定分配给该像素的类别。</p><h2 id="eed8" class="nk ls it bd lt nl nm dn lx nn no dp mb ld np nq md lh nr ns mf ll nt nu mh nv bi translated"><strong class="ak"> U-Net加权交叉熵损失</strong></h2><p id="a334" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">使用<a class="ae lq" href="https://glassboxmedicine.com/2019/12/07/connections-log-likelihood-cross-entropy-kl-divergence-logistic-regression-and-neural-networks/" rel="noopener ugc nofollow" target="_blank">交叉熵损失</a>来训练U-Net:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/fbe89cb2e05ceeb8d16d72e37ab276cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/0*7kRzmciMQSohyp0d"/></div></figure><p id="b3c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是一个典型的交叉熵损失，增加了权重<strong class="kw iu"> <em class="nz"> w </em> (x) </strong>，它提供了一个权重来告诉模型一些像素比其他像素更重要。</p><p id="d65b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">权重图<strong class="kw iu"> <em class="nz"> w </em> </strong>是使用传统的计算机视觉技术基于每个基础事实分割预先计算的。具体地，形态学图像处理被应用于基本事实分割，以识别分隔细胞的细边界，然后创建权重图，使得分隔细胞的这些细边界被赋予高权重。将这个权重图合并到交叉熵损失中意味着，如果U-Net遗漏了细胞之间的这些细边界，或者如果它将它们画在错误的位置，它将受到严重的惩罚。在损失中使用权重图的总体目标是“迫使网络学习接触细胞之间的小分离边界[……]”</p><h2 id="084b" class="nk ls it bd lt nl nm dn lx nn no dp mb ld np nq md lh nr ns mf ll nt nu mh nv bi translated"><strong class="ak"> U-Net数据增强</strong></h2><p id="d2da" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">构建数据集来训练分段模型是非常耗时的，因为需要手工绘制正确的基本事实分段。因此，最终数据集的大小可能很小。在U-Net论文中，作者采用数据扩充来增加训练数据的有效大小。他们对训练样本应用随机移位、旋转、灰度值变化和随机弹性变形。弹性变形在医学图像中特别有用，因为(通俗地说)生物样本通常是“粘糊糊的”，这意味着弹性变形的输出仍然是“真实的”</p><p id="fbcf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面是一些应用于大脑图像的数据增强的例子，来自Quantib博客:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/39bfcdf9aaf023562cd678734b0c0202.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*LvYl-S8-a7_7TjXK"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片来源:<a class="ae lq" href="https://www.quantib.com/blog/image-augmentation-how-to-overcome-small-radiology-datasets" rel="noopener ugc nofollow" target="_blank"> Quantib博客</a></p></figure><p id="1732" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">总的来说，在发表时，U-Net模型在细胞语义分割任务上取得了最先进的结果，并且随后被用于各种各样的自然图像和医学图像分割应用。</p><h1 id="8e79" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated"><strong class="ak">屏蔽R-CNN进行实例分割</strong></h1><p id="8a49" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">实例分段呢？回想一下，在实例分割中，我们不仅仅想要识别细胞与背景像素，我们还想要分离单个细胞。可以执行实例分割任务的一个模型是<a class="ae lq" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank"> Mask R-CNN </a>。Mask R-CNN是流行的<a class="ae lq" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">更快R-CNN </a>对象检测模型的扩展。</p><p id="5f28" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">掩模R-CNN的全部细节需要一整篇文章。这是对Mask R-CNN背后的思想的一个快速总结，为如何实现实例分割提供了一个思路。</p><p id="43e7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在掩模R-CNN的第一部分中，选择感兴趣区域(ROI)。RoI是输入图像的一部分，其中包含一个概率较高的对象。为每个输入图像识别多个ROI。</p><p id="51c0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在Mask R-CNN的第二部分，如下图所示，每个RoI用于获得三个模型输出:</p><ul class=""><li id="be30" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated">该RoI的最终预测类别(对象的类别，例如“人”)</li><li id="f865" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">从这个RoI获得的最终预测边界框(边界框角的坐标，它提供基本的对象定位)</li><li id="fae3" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">最终预测的分割(例如，提供非常详细的对象定位的人的轮廓)</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/31b5aa67fc05af8b4122fa97da7980f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*ILecaaEU8P0rgf3E"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片修改自<a class="ae lq" href="https://arxiv.org/pdf/1703.06870.pdf" rel="noopener ugc nofollow" target="_blank">何等2018口罩R-CNN </a></p></figure><p id="8966" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果RoI与真实边界框有足够的重叠，则被认为是“正的”。掩模R-CNN包括掩模损失，其量化预测的分割掩模与基本事实分割掩模的匹配程度。仅针对正RoI定义掩模损失，换句话说，仅当相关RoI与图像中的真实对象足够重叠时，才定义掩模损失。</p><p id="c07c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在被训练之后，掩模R-CNN可以为单个输入图像同时产生类别、边界框和分割掩模注释:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/6dae2bc8523314ac0d5a8101ab192a71.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/0*hmaULVRHRi2O_d3o"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片来源:<a class="ae lq" href="https://arxiv.org/pdf/1703.06870.pdf" rel="noopener ugc nofollow" target="_blank">何等2018口罩R-CNN </a></p></figure><p id="a95d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">掩模R-CNN也可以用于关键点检测。在下面的示例中，关键点显示为由线连接的点:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/30eb4f58d438781b7871fd443e04fe0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/0*LczPhkZBEBhuz8b1"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片来源:<a class="ae lq" href="https://arxiv.org/pdf/1703.06870.pdf" rel="noopener ugc nofollow" target="_blank">何等2018口罩R-CNN </a></p></figure><h1 id="b30c" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated"><strong class="ak">创建数据集以训练实例分割模型</strong></h1><p id="ecb3" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">有趣的是，要考虑创建适合训练实例分割模型的数据集需要多少工作。一个流行的实例分割数据集是MS COCO，它包括328，000个实例分割的图像。可可小姐的创作分三个阶段:</p><ol class=""><li id="4d97" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp oc mu mv mw bi translated"><em class="nz">类别标记</em>:为每张图片标记每个对象类别的一个实例。每个图像8个工人。91个可能的类别。20，000工时。</li><li id="195a" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp oc mu mv mw bi translated"><em class="nz">实例定位</em>:用“x”标记每个对象的每个实例，每张图片8个工人。10，000工时。</li><li id="5f81" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp oc mu mv mw bi translated"><em class="nz">实例分割</em>:对每个实例进行分割，即手动跟踪所有对象实例的轮廓。每幅图像由1名训练有素的工人分割，并由3-5名其他工人检查。55，000个工时。</li></ol><p id="2e1a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">总计85，000个工时，相当于一个人每周工作7天，每天工作12小时，工作时间超过19年！</p><p id="14dc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果您有兴趣进一步探索实例分段，可以从这里下载COCO数据集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/31320393e62bf1ee2b071ce8ac29ac6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*utUgfI3fwyw91qdG"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图片来源:<a class="ae lq" href="https://arxiv.org/pdf/1405.0312.pdf" rel="noopener ugc nofollow" target="_blank">林等2015微软COCO:语境中的常见对象。</a></p></figure><h1 id="cb8b" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated"><strong class="ak">医学图像分割</strong></h1><p id="f39b" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">下面是医学成像中一些很酷的分割应用的例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c6c416d9ea00ca19e716bc9f2590cdfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/0*EKMbepmu9QvsmQ0m"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">分割细胞核(深紫色。)来源:<a class="ae lq" href="https://github.com/matterport/Mask_RCNN" rel="noopener ugc nofollow" target="_blank"> matterport/Mask_RCNN </a>，<a class="ae lq" href="https://github.com/matterport/Mask_RCNN/blob/master/samples/nucleus" rel="noopener ugc nofollow" target="_blank">在显微图像中分割细胞核</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/a37eeab3cffbd96a9b30a219b1eb86d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*OQpgHIFgxC0q90qq"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">核磁共振成像扫描中的脑肿瘤分割。来源:<a class="ae lq" href="https://arxiv.org/pdf/1904.03355.pdf" rel="noopener ugc nofollow" target="_blank">陈等2019 3D扩张多纤维网络用于MRI实时脑肿瘤分割。</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6e9cbc53bff8c6b575aa241583988cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/0*w43hsf5AYYPdeEnb"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">在CTs中分割肝脏和肿瘤。来源:<a class="ae lq" href="https://github.com/ShawnBIT/Paper-Reading/blob/master/AHCNet.pdf" rel="noopener ugc nofollow" target="_blank">姜等2019 AHCNet:注意机制和混合连接在CT卷肝脏肿瘤分割中的应用</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/3c0963b132e33032b3aa937ad05e162a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*68HIFTSicVyr4RfV"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">分割肺的不同叶(右上叶、右中叶、右下叶、左上叶、舌部、左下叶)。来源:王等2019 <a class="ae lq" href="https://arxiv.org/pdf/1904.09106.pdf" rel="noopener ugc nofollow" target="_blank">利用协同引导的深度神经网络自动分割肺叶。</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/9914f92014fd064c7f8bb8289dfdc481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*xZJFOoNJw648TFpj"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">分割白内障手术器械。来源:<a class="ae lq" href="http://xxx.itp.ac.cn/pdf/1909.10360v1" rel="noopener ugc nofollow" target="_blank">倪等. RAUNet:用于白内障手术器械语义分割的剩余注意U-Net。</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/e1112a3274994cfc0ec5ca0dbc3d9131.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*Hp1duYwJ7yAfy7iY"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">腹腔镜手术图像的多器官分割。来源:<a class="ae lq" href="https://hal.archives-ouvertes.fr/hal-01314970/document" rel="noopener ugc nofollow" target="_blank"> Haouchine等人2016使用来自点云的结构对术中腹腔镜图像进行分割和标记。</a></p></figure><h1 id="3fed" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated"><strong class="ak">视频分割示例</strong></h1><p id="3784" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">还可以将分割算法应用到视频中！这里有两个简单的例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/26afad14ef97fee6bffb1b4416fc515b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/1*vd1ujv76AGBdbM8tctGeCQ.gif"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">GIF来源:<a class="ae lq" href="https://github.com/matterport/Mask_RCNN" rel="noopener ugc nofollow" target="_blank"> matterport/Mask_RCNN </a>。要观看完整的30分钟视频，请参见凯罗尔·马杰克的<a class="ae lq" href="https://www.youtube.com/watch?v=OOT3UIXZztE" rel="noopener ugc nofollow" target="_blank">Mask RCNN-COCO-instance segmentation。</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/b5afe67e487f0f8820098d58e554965a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Ji9v0IwNb-a66b5XT07QZA.gif"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">分割手术机器人。来源:<a class="ae lq" href="https://github.com/matterport/Mask_RCNN" rel="noopener ugc nofollow" target="_blank"> matterport/Mask_RCNN </a></p></figure><h1 id="189f" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated"><strong class="ak">总结</strong></h1><ul class=""><li id="5767" class="mo mp it kw b kx ne la nf ld oh lh oi ll oj lp mt mu mv mw bi translated">在语义分割中，每个像素被分配给一个对象类别；</li><li id="a97b" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">在实例分割中，每个像素被分配给一个单独的对象；</li><li id="8272" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">U-Net架构可以用于语义分割；</li><li id="16fb" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">掩模R-CNN架构可以用于实例分割。</li></ul><h2 id="5dc3" class="nk ls it bd lt nl nm dn lx nn no dp mb ld np nq md lh nr ns mf ll nt nu mh nv bi translated"><strong class="ak">关于特色图片</strong></h2><p id="93da" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">专题图片来自面膜R-CNN论文:<a class="ae lq" href="https://arxiv.org/pdf/1703.06870.pdf" rel="noopener ugc nofollow" target="_blank">何等2018面膜R-CNN </a>。</p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="5bb9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="nz">最初发表于2020年1月21日</em><a class="ae lq" href="https://glassboxmedicine.com/2020/01/21/segmentation-u-net-mask-r-cnn-and-medical-applications/" rel="noopener ugc nofollow" target="_blank"><em class="nz">http://glassboxmedicine.com</em></a>T22。</p></div></div>    
</body>
</html>