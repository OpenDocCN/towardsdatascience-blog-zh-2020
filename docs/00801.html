<html>
<head>
<title>TV shows are right to be scared about AI — for all the wrong reasons</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">电视节目对人工智能感到害怕是正确的——因为所有错误的原因</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tv-shows-are-right-to-be-scared-about-ai-for-all-the-wrong-reasons-81dde746f1a4?source=collection_archive---------26-----------------------#2020-01-22">https://towardsdatascience.com/tv-shows-are-right-to-be-scared-about-ai-for-all-the-wrong-reasons-81dde746f1a4?source=collection_archive---------26-----------------------#2020-01-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="73f8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们已经多次看到好莱坞对人工智能灾难的处理，但这个行业及其消费者实际上应该担心什么？</h2></div><p id="a52b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">去年夏天，当福克斯公司宣布他们的新“科幻小说”击中“下一个”时，该节目的描述听起来类似于黑镜或菲利普·k·迪克(Philip K. Dick)的《电梦》(Electric Dreams)，这是一个关于控制技术对其用户的侵蚀的警示故事:</p><p id="7ec5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“接下来是一部推进性的、基于事实的惊悚片，讲述了一种致命的、流氓式的人工智能的出现，它结合了脉冲冲击行动和对技术如何入侵我们的生活并以我们尚不了解的方式改变我们的分层检查。”</p><p id="6e75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，<a class="ae lf" href="https://www.youtube.com/watch?v=eXrE9dOc0DE" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">从它的预告片</strong> </a>来看，这部剧的故事情节离这个总结太远了。预告片以一个类似Alexa的东西开场，一切看起来都很好，直到它获得了自己的意志，以邦德超级反派的微妙和冗长肆虐并反抗其用户和人类创造者。这种试图用“入侵者”的形象来描绘社会恐惧的赫伯特·乔治·威尔斯方法可能对世界之战或哥斯拉有用。然而，在这种情况下使用人工智能会严重误导我们对人工智能和决策算法的现实担忧，其中许多都严重影响了我们生活在其中的非虚构现实中的人类生活。</p><h1 id="bd89" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">这不仅仅是数据隐私</h1><p id="991a" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">要明确的是，数据隐私是一个组成部分。你应该关心你的数据，人工智能的支柱，去哪里了。越来越不可避免的是，作为社会的积极参与者，你将被迫把你的品味和行为的很大一部分交给数据收集公司。这些公司的唯一目的是从数据中榨取尽可能多的价值，如果不成为数字隐士，我们对此几乎无能为力。</p><p id="6556" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你知道吗，不仅你可以通过点击和在线搜索被跟踪和识别，你的身体动作也可以被跟踪和识别。那些开始覆盖笔记本电脑网络摄像头的人可能在正确的道路上，但可能没有听说过行为生物测定学:通过这些技术，你在键盘上打字的节奏或你鼠标的移动模式等细节可以将你与你的身份联系起来(了解如何在这里停止<a class="ae lf" href="https://www.tripwire.com/state-of-security/security-awareness/how-to-bust-keyboard-biometrics-and-why-you-might-want-to/" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"/></a>)。</p><p id="ed16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我不是说你不应该尝试最小化你的数字数据足迹。此外，围绕数据隐私工作的对话也应该关注公众对更严格的安全和隐私政策的强烈要求。在Vox播客“Recode Decode  的最近一集<a class="ae lf" href="https://www.vox.com/recode/2019/5/27/18639284/duckduckgo-gabe-weinberg-do-not-track-privacy-legislation-kara-swisher-decode-podcast-interview" rel="noopener ugc nofollow" target="_blank">，DuckDuckGo的首席执行官Gabe Weinberg认为，选择加入数据跟踪应该是默认的，而不是许多数据公司实施的模糊的选择退出方法。这是许多欧洲国家通过政策认同的一个流行观点，需要在全球范围内采纳。</a></p><h1 id="ccf2" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">行为转变策略</h1><p id="3d6e" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">数据公司已经意识到，他们不仅可以使用他们的资产和工具进行数据收集和行为预测，而且他们现在可以部署他们的洞察力来<em class="le">转变</em>行为，有效地操纵我们未来的行动，为他们带来最佳利益。在《监视资本主义的时代》一书中，作者肖莎娜·祖博夫认为<a class="ae lf" href="https://www.theguardian.com/technology/2019/jan/20/shoshana-zuboff-age-of-surveillance-capitalism-google-facebook" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">资本主义行为分析的最终结果可能不仅仅是对我们隐私的威胁，而是对我们非常自由的意志的威胁</strong></a>——预设决策和自我实现预言的决定性景观。祖博夫认为:</p><p id="4ede" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">“监视资本主义单方面宣称人类经验是转化为行为数据的免费原材料。虽然这些数据中的一些被用于服务改进，但其余的被宣布为专有的行为盈余，被输入到被称为“机器智能”的先进制造流程中，并被制作成预测产品，预测您现在、不久和将来会做什么。最后，这些预测产品在一种新的市场中交易，我称之为行为期货市场。监视资本家从这些交易操作中变得非常富有，因为许多公司愿意对我们未来的行为下注”</em></p><p id="c86b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在资本主义思维控制下，不受监管的数据跟踪和操纵的影响应该是显而易见的。从对我们行为的威胁到民主本身，我们不需要一个电脑化的超级恶棍来想象一个真实存在的人工智能威胁。如果我们想在自主性完好无损的情况下走出这个新的数字黄金时代，一种稳健且基于道德的数据政策实施方法是根本。</p><h1 id="b01b" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">没有无偏见这回事</h1><p id="b120" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">偏见无处不在，算法于事无补，它们模糊了问题，加剧了问题。数据和算法不是我们世界的客观反映；他们带着来自创造他们的文化和组织的同样的偏见、成见和误解。除了它们隐藏在鼓励对这些偏见缺乏问责的包装中，使得更容易证明将决定论误认为客观性是正确的。通过实施决定谁优先接受医疗护理、谁涉嫌犯罪或向儿童推荐什么在线内容的系统，我们正在增加问责制的缺乏，而没有解决紧迫的问题。</p><p id="f566" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Rachel Thomas是旧金山大学数据研究所的教授，也是Fast.ai <a class="ae lf" href="https://www.fast.ai/2019/01/29/five-scary-things/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">的联合创始人，她曾就这个主题发表过长篇大论:</strong> </a></p><p id="c1ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">“我们的算法和产品影响着世界，是反馈回路的一部分。考虑一种预测犯罪并确定向哪里派遣警察的算法:向特定的街区派遣更多的警察不仅是一种效果，也是一种原因。在给定的街区，更多的警察可以导致更多的逮捕，这可能会导致算法向该街区派遣更多的警察(这种机制在关于失控反馈循环的论文</em> <a class="ae lf" href="https://arxiv.org/abs/1706.09847" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="le">中有所描述)。</em></strong></a></p><p id="edb2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与欧洲血统的人相比，计算机视觉算法<a class="ae lf" href="https://www.nytimes.com/2019/01/24/technology/amazon-facial-technology-study.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">在有色人种</strong> </a>上表现不佳，因为绝大多数面部数据来自欧洲血统的人。<strong class="kk iu"> M </strong> <a class="ae lf" href="https://arxiv.org/abs/1711.08536" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> ost收集的行为数据来自发达国家的富裕地区</strong> </a>，自动填充算法更有可能为男性建议STEM相关的职业术语，为女性建议家政术语。这些刻板印象不是恶意的结果，而是反映在数据广度和宽度上的社会和个人偏见。然而，不管来自何方，这些问题都需要解决。</p><p id="c4df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">算法和数据远非客观真理的仲裁者，而是我们偏见的载体，这种偏见非常丰富。缺乏对那些执行、部署、消费和管理它们的人的理解，导致我们被我们创造的有偏见的、可操纵的机器推下悬崖。自人工智能诞生以来，伦理考量就一直是其核心的重要组成部分。因此，等到损害已经造成，或者相信巨额罚款的威胁不仅仅是解决更大问题的权宜之计，都是错误的。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/0b03d12a9dc11c4f8314ae315bc2e195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mlURP6zVyDzLnT27fXu2Xw.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">认知偏差法典，设计:约翰·马诺吉安三世，类别和描述:巴斯特·本森，实现:蒂尔曼纳[CC BY-SA]</p></figure><h1 id="f626" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">我们能做些什么呢？</h1><p id="14bc" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">可悲的是，解决这些问题不会像“下一个”电视节目让我们相信的那样，像号召邋遢顽固的侦探打败Alexa的邪恶表弟那样容易。这里没有“其他人”，没有来自错误实验的流氓人工智能，只是对我们社会偏见和疑虑的认可，包装成闪亮的代码行。</p><p id="140a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经知道如何处理偏见。人类以前已经足够聪明，能够(在不同程度上)认识到并纠正它们。挑战在于我们是否有能力将这些解决方案应用到我们曾经认为公平或客观的系统中。这样做的下一步是为负责数据流程中每个步骤(数据的收集、存储、探索、建模和部署)的人员建立限制和问责制。此外，我们需要始终意识到偏见是多么容易被忽略，并理解算法可能会犯错误——频繁，而且比预期的影响更大。最终取得成功意味着准备好主动纠正问题，如果不能，在损害不可逆转之前放弃算法。</p><p id="481a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lf" href="https://www.infoq.com/presentations/unconscious-bias-machine-learning" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">在InfoQ的一次介绍会上</strong> </a>，Rachel Thomas概述了一些我们上面讨论的问题的解决方案，比如:</p><ul class=""><li id="c359" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated"><strong class="kk iu">不要最大化指标</strong> —想要数字增加很容易，但我们越是将数据简化为数字，就越容易遗漏错误并奖励我们提供的服务中存在的负面行为，或者启用不公正地歪曲所述服务的限制性反馈循环。</li><li id="151c" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated"><strong class="kk iu">雇佣多元化的团队</strong> —研究表明多元化的团队表现更好，从分析角度来看，他们有助于更好地理解数据集所包含的主题，带来更好的、可操作的见解，并确保偏见不是来自你的团队。</li><li id="0bac" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">“先进的技术不能代替好的政策，”托马斯说。偏见很难捕捉；不管你的算法或者管道有多好。需要制定法规和协议，以便用户可以对算法做出的决定提出上诉。</li><li id="294b" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated"><strong class="kk iu">永不放弃</strong> —我们永远不会达到人类和我们的算法没有偏见的地步。我们一开始就是这么过来的！我们有责任训练人们时刻警惕反馈循环、偏见和我们数据中隐含的偏见。</li></ul><p id="54c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，把黑白场景留给周末科幻电视节目。</p></div></div>    
</body>
</html>