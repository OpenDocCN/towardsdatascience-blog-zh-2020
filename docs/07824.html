<html>
<head>
<title>A Guide To Interpretable Machine Learning— Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释机器学习指南—第1部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-interpretable-machine-learning-1-139eae78113a?source=collection_archive---------47-----------------------#2020-06-10">https://towardsdatascience.com/a-guide-to-interpretable-machine-learning-1-139eae78113a?source=collection_archive---------47-----------------------#2020-06-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/978009b7e90fb91ee1d3be948a38b90f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XY781-Hf_BRCTlivSMlWiQ.jpeg"/></div></div></figure><div class=""/><h1 id="b80e" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">我们为什么要相信模型？</h1><p id="6afc" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">机器学习模型目前正以巨大的速度变得流行。它们现在被用来解决各种领域的各种问题。在这个越来越受欢迎并因此在我们日常生活中变得越来越重要的时刻，有一个潜在的危险，现在也是非常重要的:机器学习模型的可解释性。</p><p id="d7d9" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">最近，我们看到了图像处理如何用于安全部门和卫生部门，回归和分类模型如何用于股票市场和投资部门。如果我们仔细观察，我们会依赖机器学习来做出肯定会影响我们的重要决定。所以，问题来了，<strong class="ky jc">我们为什么要相信模型？</strong></p><p id="7091" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">换句话说，我们为什么要依赖一个智能算法做出的决定，甚至不知道它是如何操作的？你可以说有各种各样的算法，比如决策树和线性回归，它们非常容易理解和解释。但是，在当前世界中，研究人员和数据科学家正在不断尝试解决具有挑战性的问题，这些问题无法通过可解释的模型有效解决，因此需要非常复杂的算法或基于定制神经网络的算法。如果我们环顾四周，我们会发现大多数情况下，使用复杂的黑盒模型，我们很难回答这个问题<strong class="ky jc">“算法是如何做到的？”</strong></p><p id="dc3f" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">事实证明这是一个合理的问题。通常，在设计模型时，数据科学家会关注一些问题或关于某些特征的一些规定，他们希望模型能够学习并用于预测。比如我可能喂人口预测“是否在该地区开店”。原因是我可能认为，如果人口多，我的销售额就会高，所以这两者之间的相关性非常高，所以我的模型将学习这一特征，该特征的重要性将会很高。我训练我的模型，它给了我很高的准确性，但它可能没有按照计划的方式学习。所以，我们需要在几种情况下确定这些事情，以增加准确性，并确定我们的预测。</p><p id="00f5" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我举一个经典的例子，在一个“西伯利亚哈士奇”和“狼”的图像分类中，设计了一个模型。它给出了非常高的精确度。模型分析的时候发现，模型从来没有把哈士奇和狼分类。它划分了“下雪”或“不下雪”。雪被预言为哈士奇，无雪为狼。大多数哈士奇图像有雪，准确性非常好。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/1effc96150faafd9c95c577d4027c6bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*1W3OsilDcLRmkG_QxYz37A.png"/></div></figure><p id="867f" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">所以，从上面的例子我们可以理解，实际解释我们的模型有多重要。</p><h2 id="e81c" class="me jz jb bd ka mf mg dn ke mh mi dp ki lh mj mk km ll ml mm kq lp mn mo ku mp bi translated">问题的解决方法</h2><p id="7437" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">处理机器学习模型的解释的许多方法已经发展了多年。不同的方法使用不同的逻辑和理论来解释难以理解的“黑盒”算法，如神经网络、Xgboost和随机森林。两种最重要和最受欢迎的方法是石灰和SHAP。它们被用来解释局部的或观察到的预测。换句话说，它们给了我们一个清晰的直觉，让我们知道给定的特征如何影响我们模型中的预测，以及该特征对我们的模型有多重要。</p><p id="2032" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在这篇文章中，我们将详细而简要地谈论石灰和SHAP。我们将尝试使用著名的泰坦尼克号数据集来解释。</p><blockquote class="mr ms mt"><p id="c7c2" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">泰坦尼克号的沉没是历史上最臭名昭著的海难之一。</p><p id="5368" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">1912年4月15日，在她的处女航中，被广泛认为是“不沉”的皇家邮轮泰坦尼克号在与冰山相撞后沉没。不幸的是，没有足够的救生艇容纳船上的每个人，导致2224名乘客和船员中的1502人死亡。虽然幸存有一些运气成分，但似乎某些群体比其他群体更有可能幸存。</p><p id="4a75" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">在这个挑战中，我们要求你建立一个预测模型来回答这个问题:“什么样的人更有可能生存？”使用乘客数据(即姓名、年龄、性别、社会经济阶层等)。</p></blockquote><p id="2c6f" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">现在，让我们先谈一谈数据集，然后再谈我们的目标。</p><h2 id="2cbd" class="me jz jb bd ka mf mg dn ke mh mi dp ki lh mj mk km ll ml mm kq lp mn mo ku mp bi translated">数据预处理和准备</h2><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi my"><img src="../Images/f2233ee80b519bf224e629f7e74560e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_mrh1HlsQRJgHKlpUblkbg.png"/></div></div></figure><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mz"><img src="../Images/0e82ee9324a2dc154b9ca26b758dbe5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MshBCAARxr6awj1skGTG-Q.png"/></div></div></figure><p id="bf17" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">数据集看起来像这样，它有12列。“幸存”是我们的目标栏目，其他是专题栏目。现在，特性列需要一些预处理。</p><p id="32f5" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky jc">数据集提取片段:</strong></p><pre class="ma mb mc md gt na nb nc nd aw ne bi"><span id="b248" class="me jz jb nb b gy nf ng l nh ni">import pandas as pd<br/>df1=pd.read_csv('train.csv')<br/>df2=pd.read_csv('test.csv')<br/>df=pd.concat([df1,df2],axis=0)</span></pre><p id="ddac" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我们将删除PassengerID，因为它没有任何相关性。下一步，我们将采取“姓名”列，并根据他们的头衔将他们分配，类，像“夫人”，将类1，“先生”类2，等等。我们可以使用下面的代码片段做到这一点。</p><pre class="ma mb mc md gt na nb nc nd aw ne bi"><span id="d5be" class="me jz jb nb b gy nf ng l nh ni">i=0<br/>name=[]<br/>while i&lt;len(df):<br/>    #print(df.iloc[i]['Name'].type)<br/>    <br/>    if "Mrs." in df.iloc[i]['Name']:<br/>        name.append(str(1))<br/>        print("a")<br/>    elif 'Mr.' in df.iloc[i]['Name']:<br/>        name.append(str(2))<br/>        print("b")<br/>    elif "Miss." in df.iloc[i]['Name']:<br/>        name.append(str(3))<br/>        print("c")<br/>    elif "Master." in df.iloc[i]["Name"]:<br/>        name.append(str(4))<br/>        print("d")<br/>    else:<br/>        name.append(str(5))<br/>    i+=1<br/>df=df.drop(['Name'],axis=1)<br/>df['name']=name<br/></span></pre><p id="44bc" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">现在，我们转到SibSp，这里我们也将创建一个具有两个类的特征，具有用1表示的兄弟，没有用0表示的兄弟，并将其包含在特征集中，删除原始特征。我们需要注意的一件事是，这些特征中的大多数都有NaN，所以我们需要首先填充空值以避免错误。代码片段:</p><pre class="ma mb mc md gt na nb nc nd aw ne bi"><span id="de7f" class="me jz jb nb b gy nf ng l nh ni">i=0<br/>siblings=[]<br/>df.fillna(0)<br/>while i&lt;len(df):<br/>    if df.iloc[i]['SibSp']&gt;0:<br/>        siblings.append(str(1))<br/>    else:<br/>        siblings.append(str(0))<br/>        <br/>    i+=1<br/>df=df.drop(["SibSp"],axis=1)<br/>df["Siblings"]=siblings</span></pre><p id="2a68" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">接下来，我们对Parch特性做同样的事情，我们设计了两个类，用1表示父类，用0表示父类，并使用列作为特性。</p><pre class="ma mb mc md gt na nb nc nd aw ne bi"><span id="9b20" class="me jz jb nb b gy nf ng l nh ni">i=0<br/>parent=[]<br/>df.fillna(0)<br/>while i&lt;len(df):<br/>    if df.iloc[i]['Parch']&gt;0:<br/>        parent.append(str(1))<br/>    else:<br/>        parent.append(str(0))<br/>        <br/>    i+=1<br/>df=df.drop(["Parch"],axis=1)<br/>df["Parent"]=parent</span></pre><p id="a7e9" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我们选择“年龄”特征，也有一些NaNs，我们将使用年龄列的平均值来填充它们。</p><pre class="ma mb mc md gt na nb nc nd aw ne bi"><span id="8061" class="me jz jb nb b gy nf ng l nh ni">df['Age'].fillna(df['Age'].mean())</span></pre><p id="289f" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">然后，我们将创建一个具有5个类别的新特征列来分类和移除年龄的可变性质。我使用了给定的阶级界限:</p><blockquote class="mr ms mt"><p id="daaa" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">年龄&lt; 15岁:1级</p><p id="8929" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">15≤年龄&lt; 30:2级</p><p id="e479" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">30≤年龄&lt; 45:3级</p><p id="97c2" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">45≤年龄&lt; 60:4级</p><p id="8d53" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">年龄≥60岁:5级</p></blockquote><p id="6ca7" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">15岁以下的“年龄”归为1等。</p><p id="a0d9" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">代码:</p><pre class="ma mb mc md gt na nb nc nd aw ne bi"><span id="12db" class="me jz jb nb b gy nf ng l nh ni">i=0<br/>age=[]<br/>while i&lt;len(df):<br/>    if df.iloc[i]['Age']&lt;15:<br/>        age.append(str(1))<br/>    elif df.iloc[i]['Age']&lt;30:<br/>        age.append(str(2))<br/>    elif df.iloc[i]['Age']&lt;45:<br/>        age.append(str(3))<br/>    elif df.iloc[i]['Age']&lt;60:<br/>        age.append(str(4))<br/>    else:<br/>        age.append(str(5))<br/>    i+=1<br/>df['age']=age<br/>df=df.drop(['Age'],axis=1)</span></pre><p id="c7ca" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">对于“Fare”特性，我们将执行与“age”特性相同的操作。我们将用平均值填充NaN，并将其分类到一些固定的桶中。以下是我对桶边界的估计。</p><blockquote class="mr ms mt"><p id="c253" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">票价&lt;15: Class 1</p><p id="6ee4" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">15≤Fare&lt;55: Class 2</p><p id="fa46" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">55≤Fare&lt;120: Class 3</p><p id="5997" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">120≤Fare&lt;190: Class 4</p><p id="47cf" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">Fare≥190: Class 5</p></blockquote><p id="74d8" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">Next, if we check the “Tickets” column, we will see a few special tickets with letters, and others are numeric. I have labeled the completely numeric ones 1 else 0. Snippet:</p><pre class="ma mb mc md gt na nb nc nd aw ne bi"><span id="c94f" class="me jz jb nb b gy nf ng l nh ni">i=0<br/>ticket=[]<br/>while i&lt;len(df):<br/>    if df.iloc[i]['Ticket'].isnumeric():<br/>        ticket.append(str(1))<br/>    else:<br/>        ticket.append(str(0))<br/>    i+=1<br/>df['ticket']=ticket<br/>df=df.drop(['Ticket'],axis=1)</span></pre><p id="9902" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">Next, we do something similar to the cabin feature. Most of the entries in the “Cabin” feature are empty. We label the empty ones as 0 and others as 1.</p><pre class="ma mb mc md gt na nb nc nd aw ne bi"><span id="806a" class="me jz jb nb b gy nf ng l nh ni">z=0<br/>df['Cabin'].fillna("NA")<br/>cabin=[]<br/>while z&lt;len(df):<br/>    print(df.iloc[z]['Cabin'])<br/>    if 'NA' in str(df.iloc[z]['Cabin']):<br/>        cabin.append(str(0))<br/>    else:<br/>        cabin.append(str(1))<br/>    z+=1</span></pre><p id="418a" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">Now, our preprocessing is complete. After the preprocessing our data looks like this.</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/60abef800648ed0563cc2a4577cf6642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*kDrV34SBqDoH6Ywdv9wzUw.png"/></div></figure><p id="0a5a" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">We will now drop the Survived column and apply encoding on the categorical features. After application, we will obtain a Feature set X, of 31 columns i.e 31 features and a target set Y, of 1 column, Survived.</p><pre class="ma mb mc md gt na nb nc nd aw ne bi"><span id="e0ff" class="me jz jb nb b gy nf ng l nh ni">Index(['Survived', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female',<br/>       'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'name_1',<br/>       'name_2', 'name_3', 'name_4', 'name_5', 'Siblings_0', 'Siblings_1',<br/>       'Parent_0', 'Parent_1', 'age_1', 'age_2', 'age_3', 'age_4', 'age_5',<br/>       'fare_1', 'fare_2', 'fare_3', 'fare_4', 'fare_5', 'ticket_0',<br/>       'ticket_1', 'cabin_0', 'cabin_1'],<br/>      dtype='object')</span></pre><p id="fcd7" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">These are our 31 features.</p><p id="6513" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">We split our dataset in X_train, Y_train, X_test, and Y_test.</p><pre class="ma mb mc md gt na nb nc nd aw ne bi"><span id="f5d7" class="me jz jb nb b gy nf ng l nh ni">from sklearn.model_selection import train_test_split</span><span id="9853" class="me jz jb nb b gy nk ng l nh ni">X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3, random_state=42)</span></pre><p id="8a30" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">Thus we have processed our dataset and are ready for operations.</p><p id="cec3" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">We go back to our application.</p><h2 id="60e1" class="me jz jb bd ka mf mg dn ke mh mi dp ki lh mj mk km ll ml mm kq lp mn mo ku mp bi translated">LIME</h2><p id="7b16" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">LIME stands for Linear Model Agnostic Explanation. It is used for <strong class="ky jc">当地口译。</strong>它用于分析和解释底层黑盒模型的决策及其基于特定观察的预测，比如训练或测试数据集的特定行。现在，问题来了，它是怎么做到的？正如我们之前注意到的，有几个模型非常容易解释。LIME使用这些可解释的模型来预测黑盒模型。</p><p id="fb9b" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">LIME使用决策树和逻辑回归等模型来克隆黑盒模型给出的预测。它接收单个观察值，并通过对特征值进行一定程度的更改来创建一组观察值。因此，在尝试了所有的组合之后，LIME获得了一个全新的特性集。然后将该特征集传递给黑盒模型，并获得相应的预测。现在，这些预测成为可解释模型的Y训练集或目标集，而特征集成为X训练集。</p><p id="bc75" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">现在，LIME有几个模型，如决策树、线性回归、套索回归和逻辑回归。目标可解释模型是所有可解释模型中产生最小损失的模型。这是因为最小损失函数意味着具有最大准确性的模型是克隆黑盒模型最好的模型。现在，由于可解释模型的行为类似于黑盒模型，解释它将给出黑盒模型的结果。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/3f527559b08ae0863ef0068e90ef11a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*WfiNISvOOhhm9fWFWKAc7A.png"/></div></figure><p id="33d1" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">这是石灰的解释公式。这里，观察值x的解释由模型f或g中的一个示出，这取决于哪个将产生最小损失函数，并且sigma(g)描述了保持尽可能低的解释模型的复杂性。现在，由LIME产生的数据的变化也给了解释模型获得特征边界的自由，例如，如果特征的值小于值x，则它是A类，否则它是b类。</p><p id="abd3" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">也有一些缺点。LIME的问题在于它是一个局部算法，所以它的解释对整个数据集来说并不成立。</p><h2 id="021e" class="me jz jb bd ka mf mg dn ke mh mi dp ki lh mj mk km ll ml mm kq lp mn mo ku mp bi translated">解释算法是如何工作的</h2><p id="c225" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">一些最好的解释算法是决策树和逻辑回归。这里我们就来说说这两个算法。</p><p id="ed1e" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky jc">决策树:</strong>用于非线性数据集。现在，该树接收一个数据集，并根据不同要素的边界值继续分割数据集。它多次分割数据集，并创建几个数据子集。最后的子集是叶节点。他们是分类班。内部节点是拆分节点。训练有助于树判断每个特征的边界值及其对决策路径的影响。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/1a1185a8a74593df19f66eeddd754d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*P9AzZy7BNnb-JmMXyMH9vw.png"/></div><p class="nn no gj gh gi np nq bd b be z dk translated">照片由<a class="ae mq" href="https://christophm.github.io/" rel="noopener ugc nofollow" target="_blank">克里斯托弗</a>拍摄</p></figure><p id="51b8" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">因此，每个节点代表一个要素和一个与该要素相对应的值，该值充当边界。现在，每个决策树都想最小化<strong class="ky jc">基尼系数。</strong>基尼系数是实际值与预测值的差值。因此，在每个节点之后，如果我们试图判断与父节点相比系数减少了多少，我们将清楚地了解节点的权重。节点的权重描述了特征的重要性。</p><p id="4f0e" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky jc"> Logistic回归:</strong>是基于线性回归算法的分类。这不适用于非线性模型。在线性回归中，我们用一个方程表示一个n维超平面来预测值。</p><blockquote class="mr ms mt"><p id="c770" class="kw kx mu ky b kz lu lb lc ld lv lf lg mv lw lj lk mw lx ln lo mx ly lr ls lt ij bi translated">Y=b0+b1x1+b2x2+………+bnxn</p></blockquote><p id="f96c" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">这里b0是截距，特征‘I’的b{i}是回归中特征的系数或权重。它使用线性回归生成插值。</p><p id="0ee1" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在分类中，我们预测概率，因此我们使用逻辑函数将值压缩在0和1之间。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/b6aaf75d66649f328cc25513a66ae44e.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*M2PRGbCDzWCfmv51a706Pg.png"/></div></figure><p id="53a7" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">就像这样，</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d0f38219afb7f2a18e44300d8fc9a341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*6z4X_lTqudqqQR7AT4q_JQ.png"/></div><p class="nn no gj gh gi np nq bd b be z dk translated">照片由<a class="ae mq" href="https://christophm.github.io/" rel="noopener ugc nofollow" target="_blank">克里斯托弗</a>拍摄</p></figure><p id="ce71" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">因此，我们获得概率为</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/1d6bf852a71cbbd6d605ae70622ae839.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*YeT7puOi3UuHRzEm84PqGQ.png"/></div></figure><p id="f375" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">现在，我们处理事件发生的概率和事件不发生的概率之比。P(y=1)/P(y=0)。这个比率叫做<strong class="ky jc">赔率</strong>。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/567be065b875422299413389052e1694.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*zFMgZHvp-lAq3ZztN5He8w.png"/></div></div></figure><p id="6011" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">这是我们的方程式。这里p(y=1)/p(y=0) =n意味着事件发生的几率是事件不发生几率的n倍。</p><p id="5124" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">现在，如果我们将特性j (xj)的值增加1，则等式变为:</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/6881a24f2cfcf2d9c6ee7f263a46cf8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*7AVnmctuQlaoPVh2n3NPpg.png"/></div></figure><p id="225a" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">这导致了，</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/9302d7393781b6d71cd8b696f4276900.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*Sjxes1NtBLmybIlqCdrU0g.png"/></div></figure><p id="5e3d" class="pw-post-body-paragraph kw kx jb ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">因此，如果我们将一个特征增加1个单位，它对概率的影响将是其权重的指数。所以，它的贡献是它的重量的函数。因此，我们可以很容易地使用逻辑回归中的权重来获得特征贡献。</p><h2 id="c9cf" class="me jz jb bd ka mf mg dn ke mh mi dp ki lh mj mk km ll ml mm kq lp mn mo ku mp bi translated">结论</h2><p id="1b62" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在<a class="ae mq" href="https://medium.com/@myac.abhijit/a-guide-to-interpretable-machine-learning-2-fa3c4489fb53" rel="noopener">的下一部分</a>，我们将看到可解释模型的应用和实际实现。我们还将看到SHAP应用程序。</p></div></div>    
</body>
</html>