<html>
<head>
<title>Extract, Transform, Load (ETL) — AWS Glue</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">提取、转换、加载(ETL) — AWS 粘合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/extract-transform-load-etl-aws-glue-edd383218cfd?source=collection_archive---------15-----------------------#2020-05-17">https://towardsdatascience.com/extract-transform-load-etl-aws-glue-edd383218cfd?source=collection_archive---------15-----------------------#2020-05-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cc8e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何在 Spark 中对新的 Corona 病毒数据集使用 AWS Glue 进行 ETL 操作</h2></div><div class="ki kj kk kl gt ab cb"><figure class="km kn ko kp kq kr ks paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/d12de1812e0c366657a99dec09351e9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*b9m6jWSmn4oQNRvaBwztQQ.jpeg"/></div></figure><figure class="km kn ko kp kq kr ks paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/7f2edffe1f8ba52374738da90af6c3d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*dBIbG2oVPKg6hDaJnICeFg.jpeg"/></div></figure></div><p id="5930" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">AWS Glue 是一个完全托管的、无服务器的 ETL 服务，可用于为数据分析目的准备和加载数据。该服务可用于对数据进行编目、清理、丰富，并在不同的数据存储之间可靠地移动数据。在本文中，我将解释我们如何使用 AWS Glue 在 Spark 中对新的 Corona 病毒数据集执行 ETL 操作。</p><p id="1fcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将涵盖以下主题:</p><ul class=""><li id="784a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">粘合组件。</li><li id="a01b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">完成教程作者胶水火花工作。</li><li id="da45" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">从 S3 自动气象站提取数据。</li><li id="c924" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">使用 Spark 转换数据。</li><li id="13db" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">将转换后的数据以拼花格式存储回 S3。</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="a45b" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">粘合组件</h1><p id="7b0f" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">简而言之，AWS 胶水包含以下重要成分:</p><ul class=""><li id="5edf" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">数据源和数据目标:</strong>作为输入提供的数据存储称为数据源，存储转换数据的数据存储称为数据目标。</li><li id="120d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">数据目录:</strong>数据目录是 AWS Glue 的中央元数据存储库，在一个地区的所有服务之间共享。这个目录包含表定义、作业定义和其他控制信息，用于管理您的 AWS Glue 环境。</li><li id="c74e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">爬虫和分类器:</strong>爬虫是从数据存储中检索数据模式的程序(s3)。Crawler 使用自定义或内置分类器来识别数据格式，并填充目录中的元数据表。</li><li id="249d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">数据库和表:</strong>每次成功的爬虫运行都会在数据目录中填充一个数据库表。目录中的数据库是一组相关联的表。每个表只有数据的元数据信息，如列名、数据类型定义、分区信息，而实际数据保留在数据存储中。在 ETL 作业运行中，源和目标使用数据库中的一个或多个表。</li><li id="581d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">作业和触发器:</strong>执行 ETL 任务的实际业务逻辑。作业由转换脚本、数据源和数据目标组成。我们可以用 python 或 pyspark 来定义我们的工作。作业运行由触发器启动，这些触发器可以由事件计划或触发。</li></ul><p id="ea03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我留下了一些组件，因为它们不在本文的讨论范围之内。关于 AWS Glue 的详细研究，你可以访问官方的<a class="ae nn" href="https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html" rel="noopener ugc nofollow" target="_blank">开发者指南</a>。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="02b4" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">在 AWS Glue 上使用 PySpark 的 ETL</h1><p id="f625" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">现在我们已经了解了 Glue 的不同组成部分，我们现在可以开始讨论如何在 AWS 中创作 Glue 作业，并执行实际的提取、转换和加载(ETL)操作。</p><h2 id="5d8b" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated">新型冠状病毒数据集:</h2><p id="ec94" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">数据集是从<a class="ae nn" href="https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle 数据集</a>中获得的。我正在使用的版本最后更新于 2020 年 5 月 2 日。该数据集中的主文件是<code class="fe oa ob oc od b">covid_19_data.csv</code>,数据集的详细描述如下。</p><ul class=""><li id="8617" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">Sno —序列号</li><li id="4934" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">观察日期—观察的日期，以年/月/日为单位</li><li id="52c9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">省/州-观察的省或州(缺少时可以为空)</li><li id="35aa" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">国家/地区—观察国</li><li id="d144" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">上次更新时间—以 UTC 表示的给定省份或国家/地区的行更新时间。(未标准化，因此请在使用前清洁)</li><li id="104e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">已确认——截至该日期的累计已确认病例数</li><li id="1854" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">死亡——截至该日期的累计死亡人数</li><li id="7353" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">已恢复—截至该日期已恢复案例的累计数量</li></ul><h2 id="751b" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated">1)设置我们的数据存储:</h2><p id="38f7" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">作为开发端到端 ETL 工作的第一步，我们将首先设置我们的数据存储。转到 yout s3 控制台，并在那里创建一个存储桶。我们将使用以下分区方案在 bucket 中上传数据集文件:</p><pre class="ki kj kk kl gt oe od of og aw oh bi"><span id="18ca" class="no mr it od b gy oi oj l ok ol">s3://bucket-name/dataset/year=&lt;year&gt;/month=&lt;month&gt;/day=&lt;day&gt;/hour=&lt;hour&gt;/</span></pre><p id="2c87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们正在处理单个文件，所以你可以手动创建分区和上传文件，但如果处理大量文件，你可以使用我的<a class="ae nn" rel="noopener" target="_blank" href="/datalake-file-ingestion-from-ftp-to-aws-s3-253022ae54d4"> FTP 文件摄取代码</a>，我在我的上一篇文章中解释为你做这项工作。在使用 AWS Athena 时，以这种方式对数据进行分区有助于查询优化。</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi om"><img src="../Images/9fa85b5abe5e1e5abbc9c29c6479b121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xpGfnv9a1Jpho3BwjcOskQ.png"/></div></div></figure><h2 id="b7c2" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated">2)创建 AWS 粘合角色</h2><p id="d073" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">创建一个 Glue 角色，允许 Glue 访问不同的 AWS 资源，例如 s3。转到 IAM 控制台，添加一个新角色，并将<em class="on"> AWSGlueServiceRole </em>策略附加到该角色。此策略包含访问 Glue、CloudWatch、EC2、S3 和 IAM 的权限。有关如何为 Glue 设置 IAM 角色的更多细节，请考虑下面的<a class="ae nn" href="https://docs.aws.amazon.com/glue/latest/dg/getting-started-access.html" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><h2 id="c047" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated">3)设置爬虫来编目数据</h2><p id="6394" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">执行以下步骤来添加 crawler:</p><ul class=""><li id="6336" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">在左侧菜单中，单击数据库并添加一个数据库。</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi oo"><img src="../Images/b65c9d8732b9af871863a463c23f966a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*twdtI7-vMeOxv8LgtElNgA.png"/></div></div></figure><ul class=""><li id="2612" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">现在去爬虫和一个新的爬虫</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi op"><img src="../Images/5173b00e87e8d85415d0ae495b3b598e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVrK_rTLmC5ldkOQGEF15A.png"/></div></div></figure><ul class=""><li id="ff5c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">选择数据存储</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi oq"><img src="../Images/0d2e82756ccebc62c2df87628ae1b85d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwAfo3QYXIUZ3JFe9wCJlA.png"/></div></div></figure><ul class=""><li id="0bba" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">提供 s3 存储桶路径</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi or"><img src="../Images/9a0370d5f9c95e30a8dc05a58abf675e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g7pLsN22g8Ic93BC0fH7Fg.png"/></div></div></figure><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi os"><img src="../Images/9bae167a264cfe13ca98feab0f83edd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6SrJvlGZ3G7yPJFh2sSPeQ.png"/></div></div></figure><ul class=""><li id="b2e1" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">选择粘合角色</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ot"><img src="../Images/43188fde33ede784decf7dd894004123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-hZVbGPSv8vQ6R7aB9kmuA.png"/></div></div></figure><ul class=""><li id="9528" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">设置按需运行的频率</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ou"><img src="../Images/44b60d38613ff7060171b4c1ad8dcd86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fgw4a_W207PpoPFvTlL_RQ.png"/></div></div></figure><ul class=""><li id="b25f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">选择数据库</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ov"><img src="../Images/250f869301903fc0b67e749f76cbe1db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BQABUtEBWve6Wg_-eeB4XQ.png"/></div></div></figure><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ow"><img src="../Images/f32bbf5d9634e8075ddf0e15afd13ca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eRbuwb7D7he6P408JEZ3Bg.png"/></div></div></figure><ul class=""><li id="159b" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">最后检查并单击“完成”</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ox"><img src="../Images/428d4240479b6e04a1f988f730f8917f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_XSHaV2fOk-sMVyNI6hMGg.png"/></div></div></figure><ul class=""><li id="ebd3" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">现在，您的爬虫已经创建好了。单击“运行 Crawler”对数据集进行编目</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi oy"><img src="../Images/0b2eea95549a6927df6ef6212b2c22d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eVTF1rPMiUq3bdRs5zmBmA.png"/></div></div></figure><ul class=""><li id="4137" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">Crawler 可能需要一些时间来对数据进行分类。成功运行后，必须在指定的数据库中创建表</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi oz"><img src="../Images/089acbeeb420be7c8e57c860d7561a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nXSYcl5GOyIISHVXcGTjOg.png"/></div></div></figure><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi or"><img src="../Images/3da583a6630a77bccf2b0dc9a6906b1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5iRo3-vlIFGyveMHDj0PQ.png"/></div></div></figure><div class="ki kj kk kl gt ab cb"><figure class="km kn pa kp kq kr ks paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/7794011d2c402c04a07d19d90a2926ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*xe3qLZuwJWgdaeudnMbUhQ.png"/></div></figure><figure class="km kn pb kp kq kr ks paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/bbb3dd72c07caee800c9311959822e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*pgcWzKLBhNJPd7B8VmUXIg.png"/></div></figure></div><h2 id="7bdc" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated">4)为 ETL 工作添加胶合工作</h2><p id="1d7c" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">既然我们已经编目了我们的数据集，现在我们可以开始添加一个粘合工作，它将在我们的数据集上完成 ETL 工作。</p><ul class=""><li id="9362" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">在左侧菜单中，单击“作业”并添加一个新作业。Glue 可以自动生成一个 python 或 pyspark 脚本，我们可以用它来执行 ETL 操作。然而，在我们的例子中，我们将提供一个新的脚本。</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi pc"><img src="../Images/dd2e5d43789678cc7313f77c8bb2b807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wd9Sl5nK3VYZPRbgXOcWZA.png"/></div></div></figure><ul class=""><li id="bd28" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">按如下方式设置作业属性</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi pd"><img src="../Images/f5d68963b933fd4de7d1fc40d13c2805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QkPgtHQDGl0Pfs_nP4xt6g.png"/></div></div></figure><ul class=""><li id="1251" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">将以下内容保留为默认值</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi pe"><img src="../Images/ca33b71ca4b00ee2e06153e52dee1ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YabMWjZsF2AbrokrDnQnRg.png"/></div></div></figure><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi pf"><img src="../Images/e070750509b59b168a12654f42aa0498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RGPYZpVncLYV-hkHNpUOlQ.png"/></div></div></figure><ul class=""><li id="ab15" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">将最大容量设置为 2，作业超时设置为 40 分钟。您设置的 dpu 数量(最大容量)越高，您将承担的成本就越多。</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi pg"><img src="../Images/6b002ff0bbd31cb900f1b1993deb9b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gnms0qxp-77Dqfc_LZggUw.png"/></div></div></figure><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ph"><img src="../Images/0e0d3e7cedb3dc8d3a6c8364fa4d854f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yR9bLYjA5g21Z0w2bcRiHg.png"/></div></div></figure><ul class=""><li id="131c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">因为我们没有连接到任何 RDBMS，所以我们不必设置任何连接。单击“保存作业并编辑脚本”。</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi pi"><img src="../Images/094a0f9adc22bfa876a5413583a02d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QFWcyuA25AK2E7sCDcrGWA.png"/></div></div></figure><ul class=""><li id="fd9e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">我们将看到以下屏幕。将我的<a class="ae nn" href="https://github.com/furqanshahid85-python/AWS-Glue-Pyspark-ETL-Job" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>中的代码复制粘贴到下面的编辑器中，点击保存。现在，单击“运行作业”按钮。</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi pj"><img src="../Images/2c60a0395cd46582f8a754d2b723960f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QZVmuFQrGjQETCFKfSFhTQ.png"/></div></div></figure><ul class=""><li id="b66a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">根据工作的不同，执行作业可能需要一段时间(在本例中为 15 到 30 分钟)。到目前为止，胶合作业有至少 10 分钟的冷启动时间，之后作业开始执行。</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi pk"><img src="../Images/cada6b1c86ace7adeca67ddde5eabe37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u2cLO3eLoBqT63AS1I0wkw.png"/></div></div></figure><ul class=""><li id="5ebb" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">如果作业执行成功，您将在胶合作业中指定的目标桶中获得拼花格式的聚合结果。</li></ul><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi pl"><img src="../Images/aa7d5230e9758807425acce94bade141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rlVaJqR89n8tSnncxswhRA.png"/></div></div></figure><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi pm"><img src="../Images/12f42b8609d41ef05ac7a1946ab2a166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VKpOokk4qBYbYwmUw2VatQ.png"/></div></div></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="c551" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">了解 Spark 工作</h1><p id="f1ac" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">如果您遵循了上面提到的所有步骤，那么您应该可以通过 AWS Glue 成功执行 ETL 作业。在这一节中，我将深入研究执行实际 ETL 操作的 Spark 代码。</p><h2 id="d01d" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated">胶水和火花导入和参数</h2><p id="6451" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在最顶端，我们有必要的胶水和火花进口。</p><figure class="ki kj kk kl gt kn"><div class="bz fp l di"><div class="pn po l"/></div></figure><p id="96d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">导入后，我们有几个参数设置。这包括获取作业名、设置 spark 和 glue 上下文、初始化作业、定义目录数据库和表名以及 s3 输出路径。</p><figure class="ki kj kk kl gt kn"><div class="bz fp l di"><div class="pn po l"/></div></figure><h2 id="d376" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated">提取</h2><p id="14fa" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">ETL 操作的“提取”部分除了连接到某个数据存储并从中获取数据之外什么也不做。代码的提取部分执行以下操作:</p><ul class=""><li id="4b9f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">使用粘附目录中的创建粘附动态框架。已经提供了目录数据库和表名。</li><li id="f452" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">创建动态帧后，我们使用<strong class="lb iu"> toDF() </strong>方法将其转换为 Spark 数据帧。转换为 spark 数据帧将允许我们使用所有的 spark 转换和动作。</li></ul><figure class="ki kj kk kl gt kn"><div class="bz fp l di"><div class="pn po l"/></div></figure><h2 id="a3a6" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated">改变</h2><p id="0063" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在 ETL 操作的“转换”部分，我们对数据应用不同的转换。代码的转换部分执行以下操作:</p><ul class=""><li id="8332" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">首先，我们在 spark 中使用<strong class="lb iu"> drop() </strong>方法删除“最后更新”列(没有特殊原因)。</li><li id="e004" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">然后使用 spark 中的<strong class="lb iu"> dropna() </strong>方法删除任何包含 4 个以上空字段的行。</li><li id="41cc" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">然后，我们使用 spark 中的<strong class="lb iu"> fillna() </strong>方法，用自定义值“na_province_state”填充“province/state”列中缺少的值。</li><li id="c06f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">接下来，我们对数据集执行聚合。已经执行了 3 个不同的聚合。我们检查一个国家/地区的哪个省/州有最多的病例、最多的死亡和最多的康复。这是通过使用<strong class="lb iu"> groupBy() </strong>方法将记录按<strong class="lb iu">省/州和国家/地区</strong>列<strong class="lb iu"> </strong>分组，使用<strong class="lb iu"> max() </strong>方法将记录与<strong class="lb iu"> max(已确认)、max(死亡)和 max(已恢复)</strong>列聚合，然后使用<strong class="lb iu"> orderBy() </strong>方法将它们按降序排序来完成的。</li><li id="1c32" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">最后，我们使用<strong class="lb iu"> fromDF() </strong>方法将数据帧转换回 Glue DynamicFrame，并将结果保存在 S3 中。它接受三个参数 dataframe、glue 上下文和结果 DynamicFrame 的名称。</li></ul><figure class="ki kj kk kl gt kn"><div class="bz fp l di"><div class="pn po l"/></div></figure><h2 id="1ae7" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated"><strong class="ak">加载</strong></h2><p id="424f" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在 ETL 操作的加载部分，我们将转换后的数据存储到一些持久存储中，比如 s3。代码的加载部分执行以下操作:</p><p id="bcda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用 DynamicFrame()的<strong class="lb iu"> from_options() </strong>方法将结果保存到 s3。该方法采用以下参数:</p><ul class=""><li id="20c8" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu"> frame </strong>:我们要写的 DynamicFrame。</li><li id="cff3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu"> connection_type </strong>:我们正在写入的目标数据存储，在本例中是 s3。</li><li id="e43f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu"> connection_options </strong>:这里我们指定目标 s3 路径和数据格式(本例中为 parquet)来保存数据。</li><li id="b5d9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu"> transformation_ctx </strong>:可选的转换上下文。</li></ul><p id="dd09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有 3 个聚合结果都以拼花格式保存在目标路径上。</p><figure class="ki kj kk kl gt kn"><div class="bz fp l di"><div class="pn po l"/></div></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="2626" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">摘要</h1><p id="b0b9" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在本文中，我们学习了如何使用 AWS Glue 在 Spark 中进行 ETL 操作。我们学习了如何设置数据源和数据目标，创建爬虫来编目 s3 上的数据，并编写 Glue Spark 作业来执行提取、转换和加载(ETL)操作。</p><p id="b632" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整的 spark 代码可以在我的 GitHub 资源库中找到:</p><div class="pp pq gp gr pr ps"><a href="https://github.com/furqanshahid85-python/AWS-Glue-Pyspark-ETL-Job" rel="noopener  ugc nofollow" target="_blank"><div class="pt ab fo"><div class="pu ab pv cl cj pw"><h2 class="bd iu gy z fp px fr fs py fu fw is bi translated">furqanshahid 85-python/AWS-Glue-Pyspark-ETL-Job</h2><div class="pz l"><h3 class="bd b gy z fp px fr fs py fu fw dk translated">该模块对 noval corona 病毒数据集进行统计分析。具体实施如下…</h3></div><div class="qa l"><p class="bd b dl z fp px fr fs py fu fw dk translated">github.com</p></div></div><div class="qb l"><div class="qc l qd qe qf qb qg kx ps"/></div></div></a></div><p id="e32d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢你阅读❤.</p></div></div>    
</body>
</html>