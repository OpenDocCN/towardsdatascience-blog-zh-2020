<html>
<head>
<title>Batch Normalization — an intuitive explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批量标准化—直观的解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/batch-normalization-an-intuitive-explanation-42e473fa753f?source=collection_archive---------20-----------------------#2020-04-22">https://towardsdatascience.com/batch-normalization-an-intuitive-explanation-42e473fa753f?source=collection_archive---------20-----------------------#2020-04-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="28f7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为什么批量范数有助于训练深度学习模型？</h2></div><p id="bba9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章的目的是提供一个简单直观的理解批处理规范化(BN)以及它如何帮助训练更深更好的模型。我们开始吧，好吗？</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/b165f0eace8fef05013edc3f097c7df5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Xo6e2oxajheOHGD7"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">开始吧！伊恩·杜利在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="632e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">前奏:</strong></h1><p id="7fdf" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">一般来说，规范化是指将不同范围的数字压缩到一个固定的范围。例如，考虑简单双参数模型f(x)的输入<strong class="kk iu"> x1 </strong>和<strong class="kk iu"> x2 </strong></p><p id="22ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">T11】f(x)= w1 * x1+w2 * x2T13】</strong></p><p id="cd06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果x1和x2处于完全不同的范围，比如x1的范围是[1000–2000]而x2的范围是[0.1–0.5]，那么按原样使用它们就意味着优化我们的损失函数。直观地说，在这种情况下，模型参数没有一个公平的竞争环境，网络容易被w1压倒。通过将x1和x2的范围归一化为[0–1 ],可以使所有输入达到相似的范围，并帮助模型更快地学习。这是应用于输入的规范化的特例，称为<strong class="kk iu"> <em class="ms">输入规范化</em> </strong>(咄！).在计算机视觉中，通常使用训练数据集的平均值和标准差来标准化模型输入。如果你仍然困惑，我推荐<a class="ae lu" href="https://www.coursera.org/lecture/deep-neural-network/normalizing-inputs-lXv6U" rel="noopener ugc nofollow" target="_blank">这个来自吴恩达</a>的简短的视觉解释。</p><p id="3cdb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，有了上述直觉，让我们问这个问题:考虑到输入只是深层神经网络(DNN)的另一层，那么模型的其他层的输入呢？将这些中间输入标准化是否也有益？</p><p id="1cbf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就引出了我们的问题陈述。</p><h1 id="bf68" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">问题——或者说我们为什么需要批量定额:</strong></h1><p id="51c6" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">深度学习模型通常是一系列级联的层，每一层都接收一些输入，应用一些计算，然后将输出移交给下一层。本质上，每一层的输入构成了该层试图以某种方式“适应”的数据分布。现在，只要一个层的输入数据分布在数据的多个批处理/传递中保持相当一致，该层就可以轻松地完成拟合数据的“任务”。</p><p id="9c5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是模型/网络管道的本质使得这种一致的输入分布的期望变得困难。随着不同的小批量数据在网络中加载和传递，各层的输入分布会发生跳跃，使我们的各层更难完成工作。除了拟合基础分布之外，所讨论的图层现在还必须考虑图层输入分布中的漂移。这种转移输入分布的现象被称为<strong class="kk iu"> <em class="ms">内部同变量转移</em> </strong>。</p><p id="7842" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个问题是，当网络的不同参数的比例与梯度更新相关联时，它们可能导致<strong class="kk iu"> <em class="ms">爆炸和消失梯度</em> </strong>。一些激活可能会被推到它们的饱和区域，并且可能永远不会从那里恢复。直观上，模型参数不再具有公平的竞争环境(就像前面提到的f(x)中w1和w2的比例一样),并且一些参数的梯度可能不受控制地增加和减少。因此，从梯度更新中分离网络参数的比例可能有利于训练。 避免爆炸和消失梯度的变通方法通常包括使用更鲁棒的激活函数，如<strong class="kk iu"><em class="ms">【ReLU】，</em> </strong>选择较低的学习速率和小心初始化网络参数，但由于有许多旋钮要调节，核心问题仍然存在:训练我们的DNN是一个挑战。</p><h1 id="50e4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">解决方案:</strong></h1><p id="76c8" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">批处理规范化旨在解决我们上面描述的问题:</p><ol class=""><li id="e80e" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated">避免不稳定的梯度</li><li id="cd20" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">减少网络初始化对收敛的影响。</li><li id="17a9" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">允许更快的学习速率导致更快的收敛。</li></ol><p id="288b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所有这些都是通过一个相当简单的方法→ <strong class="kk iu"> <em class="ms">来实现的，即使用数据集均值和标准偏差</em> </strong>来归一化每个中间层的输入。理想情况下，与输入归一化一样，批处理归一化也应基于<em class="ms">整个数据集</em>归一化每个图层，但<em class="ms"> t </em>这并非微不足道，因此作者进行了简化:<strong class="kk iu"> <em class="ms">使用小型批处理统计数据而非</em> </strong>进行归一化，因此得名—批处理归一化。</p><p id="d792" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就是这样！</p><p id="cc43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不完全是，我还没有复制粘贴强制BN方程！</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/01f29903f12845ae01fbfdb982bf29ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*XPhikySyAytgJOBDw4jWaQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图片来自<a class="ae lu" href="http://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1502.03167</a></p></figure><p id="5659" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的片段中，前三个方程计算批次平均值和标准差，然后分别用这些矩对输入进行归一化。第三步中的ε是一个小数值，有助于数值稳定性。需要注意的关键点是，对批处理中的所有输入维度分别进行归一化(在卷积术语中，认为是通道)</p><p id="9969" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后一个等式引入了两个参数-&gt; <em class="ms">伽马(缩放)</em>和<em class="ms">贝塔(移位)</em>来进一步变换归一化的输入。原因是简单的标准化通过限制其范围降低了后续激活的表达能力(对于sigmoid，这将限制其输出到S曲线的线性范围)。为了克服这一点，BN允许网络学习gamma和beta参数，以便让层“调整”标准化的输入分布，使其更具表现力。与每个小批量计算的平均值和方差相反，伽马和贝塔参数是在整个数据集上学习的。</p><h1 id="848e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">BN有助于内部协变量转移-&gt;神话还是事实？</h1><p id="2679" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在介绍BN时，我说它有助于解决内部同变量转移的问题，这也是最初的作者声称的。但是，经过几年的辩论和研究，最近的证据指出情况可能并非如此。批次标准和内部协变量移位可能无关。事实上，在某些情况下，BN实际上可能<em class="ms">增加</em>内部同变移位！</p><p id="2c88" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个这样的实验训练了三个不同的模型:</p><p id="d833" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1.没有国阵的DNN <strong class="kk iu"/></p><p id="d8c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.一个DNN <strong class="kk iu">与</strong>和<strong class="kk iu"> BN </strong></p><p id="833c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.具有 <strong class="kk iu"> BN </strong>的DNN <strong class="kk iu">，但是现在具有添加噪声</strong>的<strong class="kk iu">(在应用BN之后并且在应用激活以创建同变量移位之前)。</strong></p><p id="205b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果表明，后两种模型更快地达到更好的精度，这是预期的，因为它们使用了BN。然而，第二种模式并不比第三种模式好多少。这表明，虽然BN确实有助于更好的训练制度，但它可能与内部协变量变化无关。</p><p id="3025" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么，BN到底为什么有效呢？T25】</p><p id="6809" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有些答案很直观。由于BN对每一层的输入进行归一化，这将梯度从网络参数的规模中解耦，从而防止不稳定的梯度。这也允许更高的学习率，因为网络现在有更少的机会卡住。所有这些都是稳定训练体制的关键因素。</p><p id="46fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">进一步观察发现，BN实际上将优化过程(准确地说是损失面)重新参数化为更平滑的状态。直观地说，这意味着BN采用了一个充满山丘和山谷的复杂损失表面，并使其变得更简单，山丘和山谷越来越少。优化过程现在可以朝着全局最优迈出更有信心和更大的步伐(具有更大的学习率),并且陷入可怕的局部最优的机会更少。用专业术语来说，这被描述为“平滑损失函数的<strong class="kk iu"><em class="ms"/></strong>】Lipschitz ness”。</p><p id="e775" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在参考资料部分包括了所有的资源，如果你想了解更多关于BN的细节，我强烈推荐你阅读前三篇文章。</p><p id="923e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">今天到此为止，各位！一如既往的感谢阅读！</p><h1 id="6891" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><ol class=""><li id="2ddc" class="mt mu it kk b kl mn ko mo kr ni kv nj kz nk ld my mz na nb bi translated">约夫和塞格迪(2015年)。<em class="ms">批量标准化:通过减少内部协变量偏移来加速深度网络训练</em>。<a class="ae lu" href="http://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1502.03167</a></li><li id="0271" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">比约克、戈麦斯、塞尔曼和温伯格。<em class="ms">了解批量标准化</em>。<br/><a class="ae lu" href="https://arxiv.org/abs/1806.02375" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1806.02375</a></li><li id="1229" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">桑图尔卡，齐普拉斯，伊利亚斯，麻省理工学院，˛。<em class="ms">批量规范化对优化有什么帮助？<br/></em><a class="ae lu" href="https://arxiv.org/abs/1805.11604" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1805.11604</a></li><li id="e076" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated"><a class="ae lu" href="https://www.jeremyjordan.me/batch-normalization/" rel="noopener ugc nofollow" target="_blank">https://www.jeremyjordan.me/batch-normalization/</a></li></ol></div></div>    
</body>
</html>