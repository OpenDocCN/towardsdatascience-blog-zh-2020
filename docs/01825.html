<html>
<head>
<title>When to use CPUs vs GPUs vs TPUs in a Kaggle Competition?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Kaggle比赛中什么时候用CPUs vs GPUs vs TPUs？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/when-to-use-cpus-vs-gpus-vs-tpus-in-a-kaggle-competition-9af708a8c3eb?source=collection_archive---------2-----------------------#2020-02-20">https://towardsdatascience.com/when-to-use-cpus-vs-gpus-vs-tpus-in-a-kaggle-competition-9af708a8c3eb?source=collection_archive---------2-----------------------#2020-02-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="282d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每一个机器学习算法的背后都是数千兆赫兹的硬件运算。在设置Kaggle笔记本时，您可能已经注意到了几个处理器选项，但哪一个最适合您呢？在这篇博文中，我们比较了使用CPU(<a class="ae ko" href="https://www.intel.com/content/www/us/en/products/processors/xeon.html" rel="noopener ugc nofollow" target="_blank">英特尔至强</a> *)与GPU(<a class="ae ko" href="https://www.nvidia.com/en-us/data-center/tesla-p100/" rel="noopener ugc nofollow" target="_blank">英伟达特斯拉P100 </a>)与TPUs ( <a class="ae ko" href="https://cloud.google.com/tpu/" rel="noopener ugc nofollow" target="_blank">谷歌TPU v3 </a>)来训练使用<a class="ae ko" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> tf.keras </a>编写的机器学习模型的相对优缺点(图1**)。我们希望这能帮助你理解这些选项，并为你的项目做出正确的选择。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/c1454778bbe51672c2b8dd7b7736c64d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*suXcuHEe29aKLPrQnXGBrg.png"/></div></div></figure><p id="3a29" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">我们如何准备考试</strong></p><p id="a410" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了比较CPU vs GPU vs TPUs在完成常见数据科学任务方面的性能，我们使用<a class="ae ko" href="https://www.tensorflow.org/datasets/catalog/tf_flowers" rel="noopener ugc nofollow" target="_blank"> tf_flowers数据集</a>来训练一个卷积神经网络，然后使用三个不同的后端运行完全相同的代码三次(CPU vs GPU vs TPUs；GPU是英伟达P100，配有英特尔至强2GHz(双核)CPU和13GB RAM。TPU为TPUv3 (8核)，配有英特尔至强2GHz (4核)CPU和16GB RAM。附带的<a class="ae ko" href="https://www.kaggle.com/mgornergoogle/flowers-with-keras-and-xception-fine-tuned-on-gpu" rel="noopener ugc nofollow" target="_blank">教程笔记本</a>展示了让您的TPU发挥最佳性能的一些最佳实践。</p><p id="03fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如:</p><ol class=""><li id="7dfb" class="lb lc it js b jt ju jx jy kb ld kf le kj lf kn lg lh li lj bi translated">使用分片文件的数据集(<a class="ae ko" href="https://www.kaggle.com/paultimothymooney/convert-kaggle-dataset-to-gcs-bucket-of-tfrecords" rel="noopener ugc nofollow" target="_blank">例如。TFRecord </a></li><li id="159f" class="lb lc it js b jt lk jx ll kb lm kf ln kj lo kn lg lh li lj bi translated">使用<a class="ae ko" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank"> tf.data </a> API将训练数据传递给TPU</li><li id="6451" class="lb lc it js b jt lk jx ll kb lm kf ln kj lo kn lg lh li lj bi translated">使用大批量(例如batch_size=128)</li></ol><p id="6df8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过在工作流中添加这些先行步骤，可以避免常见的I/O瓶颈，否则会阻止TPU充分发挥其潜力。你可以通过访问官方的<a class="ae ko" href="https://www.kaggle.com/docs/tpu" rel="noopener ugc nofollow" target="_blank"> Kaggle TPU文档</a>找到优化你的代码在TPUs上运行的额外技巧。</p><p id="b3f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">硬件表现如何</strong></p><p id="4bc1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们测试的三种硬件类型之间最显著的差异是使用<a class="ae ko" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> tf.keras </a>训练模型所用的速度。tf.keras库是最受欢迎的机器学习框架之一，因为tf.keras使快速试验新想法变得容易。如果您花更少的时间编写代码，那么您就有更多的时间来执行计算，如果您花更少的时间等待代码运行，那么您就有更多的时间来评估新的想法(图2)。tf.keras和TPUs在参加<a class="ae ko" href="https://kaggle.com/c/flower-classification-with-tpus" rel="noopener ugc nofollow" target="_blank">机器学习比赛</a>时是一个强大的组合！</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lp"><img src="../Images/b5ebd4e78e2b814cd42ba7a1374d44a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bqmG-YzgJzVeLbQ5Ym1iFg.png"/></div></div></figure><p id="9380" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们的第一个实验中，我们对所有三种硬件类型使用了相同的代码(<a class="ae ko" href="https://www.kaggle.com/mgornergoogle/flowers-with-keras-and-xception-fine-tuned-on-gpu" rel="noopener ugc nofollow" target="_blank">官方教程笔记本</a>的修改版本*** ),这需要使用非常小的批量16，以避免CPU和GPU的内存溢出错误。在这些条件下，我们观察到，在训练一个<a class="ae ko" href="https://keras.io/applications/#xception" rel="noopener ugc nofollow" target="_blank">异常</a>模型时，与CPU相比，TPU的速度提高了约100倍，与GPU相比，速度提高了约3.5倍(图3)。由于TPU在处理大批量时运行效率更高，我们还尝试将批量增加到128，这导致TPU的速度提高了约2倍，并且出现了GPU和CPU的内存不足错误。在这些条件下，TPU能够训练一个<a class="ae ko" href="https://keras.io/applications/#xception" rel="noopener ugc nofollow" target="_blank">exception</a>模型，速度比之前实验中的GPU快7倍以上****。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lp"><img src="../Images/d8f9d37c73a55a66fbf3e4107bb1b798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p2X9DQcq9K5Iu76Kk82vrg.png"/></div></div></figure><p id="1c74" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">观察到的模型训练加速因模型类型而异，Xception和Vgg16的性能优于ResNet50(图4)。<strong class="js iu"> </strong>模型训练是我们观察到TPU以如此大的优势胜过GPU的唯一任务类型。例如，我们观察到，在执行少量预测时，我们手中的TPU比CPU快大约3倍，比GPU慢大约3倍(TPU在某些情况下进行预测时表现异常，例如<a class="ae ko" href="https://docs.google.com/presentation/d/1O49AkNyYV48n0X4nWr7KE-5aask88pz9gBSQ26ZG-5o/edit#slide=id.g50ce3d3866_0_1590" rel="noopener ugc nofollow" target="_blank">对非常大的批次进行预测</a>，这在本实验中不存在)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/b786535c06049d7839131cb1c605f3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p7U2zlYn9O5Yvjluh2P-dg.png"/></div></div></figure><p id="4644" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了补充这些结果，我们注意到<a class="ae ko" href="https://arxiv.org/abs/1907.10701" rel="noopener ugc nofollow" target="_blank">王<em class="lq">等人。al </em> </a>开发了一个名为ParaDnn [1]的严格基准，可以用来比较不同硬件类型的性能，用于训练机器学习模型。王<em class="lq">等人用的就是这种方法。al </em>能够得出结论，当使用TPU而不是GPU时，参数化模型的性能优势从1倍到10倍不等，真实模型的性能优势从3倍到6.8倍不等(图5)。当与分片数据集、大批量和大模型相结合时，TPU表现最佳。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/57c125ebf89a3772451055b5ae21d523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QbP2CPDZH5BQWlnaTtW3oA.png"/></div></div></figure><p id="46f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">训练车型时的价格考虑</strong></p><p id="1346" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然我们的比较对硬件一视同仁，但在价格上有相当大的差异。<strong class="js iu"/>TPU的价格大约是GPU的5倍(Nvidia Tesla P100<a class="ae ko" href="https://www.nvidia.com/en-us/data-center/tesla-p100/" rel="noopener ugc nofollow" target="_blank">GPU的</a><a class="ae ko" href="https://cloud.google.com/compute/gpus-pricing" rel="noopener ugc nofollow" target="_blank">1.46美元/小时</a>对比<a class="ae ko" href="https://cloud.google.com/tpu/pricing" rel="noopener ugc nofollow" target="_blank">GPU的</a><a class="ae ko" href="https://cloud.google.com/tpu/" rel="noopener ugc nofollow" target="_blank">8.00美元/小时</a>谷歌TPU v3 对比<a class="ae ko" href="https://cloud.google.com/tpu/pricing" rel="noopener ugc nofollow" target="_blank">4.50美元/小时</a>在<a class="ae ko" href="https://cloud.google.com/pricing/" rel="noopener ugc nofollow" target="_blank"> GCP </a>具有“按需”访问功能的TPUv2)。如果您试图优化成本，那么使用TPU是有意义的，如果它训练您的模型的速度至少是使用GPU训练相同模型的5倍。</p><p id="c9a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当数据以<a class="ae ko" href="https://www.kaggle.com/paultimothymooney/convert-kaggle-dataset-to-gcs-bucket-of-tfrecords" rel="noopener ugc nofollow" target="_blank">分片格式</a>存储在<a class="ae ko" href="https://www.kaggle.com/paultimothymooney/how-to-move-data-from-kaggle-to-gcs-and-back" rel="noopener ugc nofollow" target="_blank"> GCS存储桶</a>中，然后以大批量传递给TPU时，我们始终观察到模型训练速度提高了约5倍，因此我们向熟悉<a class="ae ko" href="http://tf.data" rel="noopener ugc nofollow" target="_blank"> tf.data </a> API的注重成本的消费者推荐TPU。</p><p id="839e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一些机器学习实践者优先考虑减少模型训练时间，而不是优先考虑减少模型训练成本。对于那些只想尽可能快的训练他们的模型的人来说，TPU是最好的选择。如果你花更少的时间训练你的模型，那么你就有更多的时间迭代新的想法。但是不要相信我们的话——你可以通过在一个免费的<a class="ae ko" href="https://www.kaggle.com/docs/kernels#the-kernels-environment" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本</a>上运行你自己的代码来评估CPU、GPU和TPU的性能优势。Kaggle用户已经在尝试TPU和文本数据方面获得了很多乐趣和成功:查看<a class="ae ko" href="https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127333" rel="noopener ugc nofollow" target="_blank">这个论坛帖子</a>，它描述了TPU如何被用来训练一个BERT transformer模型，以在最近的<a class="ae ko" href="https://www.kaggle.com/c/tensorflow2-question-answering" rel="noopener ugc nofollow" target="_blank"> Kaggle比赛</a>中赢得8000美元(二等奖)。</p><p id="d67f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">您应该选择哪个硬件选项？</strong></p><p id="b7a3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总之，我们推荐CPU是因为它们的多功能性和大内存容量。当你想加快各种数据科学工作流的速度时，GPU是CPU的一个很好的替代选择，当你特别想尽可能快地训练一个机器学习模型时，TPU是最好的选择。</p><p id="6053" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过针对您正在使用的特定硬件优化您的代码，您可以获得更好的结果，我们认为将针对GPU优化的代码的运行时与针对TPU优化的代码的运行时进行比较是非常有趣的。例如，记录使用GPU加速库(如<a class="ae ko" href="https://rapids.ai/" rel="noopener ugc nofollow" target="_blank"> RAPIDS.ai </a>)训练梯度增强模型所需的时间，然后将该时间与使用TPU加速库(如<a class="ae ko" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> tf.keras </a>)训练深度学习模型所需的时间进行比较，这将非常有趣。</p><p id="6397" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个人训练一个准确的机器学习模型最少需要多少时间？你一天能评估多少不同的想法？当与tf.keras结合使用时，TPU允许机器学习实践者花更少的时间编写代码，花更少的时间等待他们的代码运行——留下更多的时间来评估新想法和提高自己在<a class="ae ko" href="http://kaggle.com/c/flower-classification-with-tpus" rel="noopener ugc nofollow" target="_blank"> Kaggle比赛</a>中的表现。</p><h1 id="1e3f" class="lr ls it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">引用的作品</h1><p id="0877" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">[1] Wang Y，Wei G，Brooks D. <a class="ae ko" href="https://arxiv.org/pdf/1907.10701.pdf" rel="noopener ugc nofollow" target="_blank">对深度学习的、GPU和CPU平台进行基准测试</a>。2019.<em class="lq"> arXiv: 1907.10701 </em>。</p><p id="5c32" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2] Kumar S，Bittorf V等。艾尔。<a class="ae ko" href="https://arxiv.org/pdf/1909.09756.pdf" rel="noopener ugc nofollow" target="_blank">在谷歌TPU-v3吊舱上缩放MLPerf-0.6模型</a>。2019.<em class="lq"> arXiv: 1909.09756 </em>。</p><p id="14f8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3]茹皮·恩·扬等。艾尔。<a class="ae ko" href="https://ieeexplore.ieee.org/abstract/document/8192463" rel="noopener ugc nofollow" target="_blank">张量处理单元的数据中心内性能分析</a>。2017.<em class="lq"> 2017年ACM/IEEE第44届计算机体系结构国际年会(ISCA) </em>。</p><h1 id="87d1" class="lr ls it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">脚注</h1><p id="0d67" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated"><strong class="js iu"> * </strong> CPU类型因可变性而异。除了英特尔至强处理器，您还可以分配到英特尔Skylake、英特尔Broadwell或英特尔Haswell处理器。GPU是英伟达P100，配有英特尔至强2GHz(双核)CPU和13GB RAM。TPU为TPUv3 (8核)，配有英特尔至强2GHz (4核)CPU和16GB RAM。</p><p id="408c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">**图1图片来自<a class="ae ko" href="https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-breaks-scalability-records-for-ai-inference" rel="noopener ugc nofollow" target="_blank">https://cloud . Google . com/blog/products/ai-machine-learning/cloud-TPU-breaks-scalability-records-for-ai-inference，</a>经许可。</p><p id="8542" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">***修改了教程笔记本，以保持三个不同后端之间的参数(如batch_size、learning_rate等)一致。</p><p id="aa04" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">**** CPU和GPU实验使用了16的批处理大小，因为它允许Kaggle笔记本从上到下运行，而没有内存错误或9小时超时错误。当批量增加到128时，只有支持TPU的笔记本电脑能够成功运行。</p></div></div>    
</body>
</html>