<html>
<head>
<title>Beyond BERT?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超越伯特？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1?source=collection_archive---------14-----------------------#2020-02-19">https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1?source=collection_archive---------14-----------------------#2020-02-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5c50" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">2020年的变形金刚。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a6875f66f80776f1ba2feabc33685ca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3Gg7SX4pMvGGXZFO"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">阿瑟尼·托古列夫在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="681a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/2019-the-year-of-bert-354e8106f7ba"> 2019年是伯特</a>年，关于它已经写了很多。说实话，很难高估变形金刚在NLP社区中的影响:LSTMs现在听起来已经过时了(<a class="ae kv" href="https://arxiv.org/abs/1911.11423" rel="noopener ugc nofollow" target="_blank">)，是吗？在2019年，最先进的论文一直在稳步前进，在谷歌，伯特以破纪录的时间</a>将其<a class="ae kv" href="https://www.blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">投入生产。以上全部同时启用</a><a class="ae kv" href="https://ruder.io/state-of-transfer-learning-in-nlp/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">转移学习</strong> </a>，现在是NLP-town最酷的小鬼。</p><p id="843a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，围绕这些模型的发展是显著的，但是变形金刚已经达到顶峰了吗？我们应该密切关注哪些研究领域？2020年，这些基于注意力的网络还有什么令人兴奋的地方？这些想法是最近在<a class="ae kv" href="https://www.zeta-alpha.com" rel="noopener ugc nofollow" target="_blank"> Zeta Alpha </a>举行的<a class="ae kv" href="https://www.zeta-alpha.com/deep-learning-nlp-transformers" rel="noopener ugc nofollow" target="_blank">变形金刚在工作</a>活动中讨论的焦点，会上考虑了许多关于该主题的有趣角度。</p><p id="3066" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我的看法。</p><h1 id="1f09" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">模型</h1><p id="0a65" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">2019年见证了变压器模型架构变体的爆炸，很难跟上(当然忘记了一些):大表亲(Transformer-XL，GPT-2，Ernie，XLNet，RoBERTa，CTRL)，小表亲(ALBERT，DistilBERT)或最近的侄子，如Reformer或Compressive Transformer。</p><p id="ffdf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在很清楚，增长模型仍然可以成功地提高许多任务的技术水平，但是我们应该这样做吗？它增加了多少价值？<strong class="ky ir">变小的型号</strong>但保持性能是我们在2019年开始看到的趋势，并希望在2020年保持稳定。也许除了模型修剪或<em class="mp">提炼</em>之外，还会出现一些创新的方法？无处不在的<a class="ae kv" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚库</a>的创造者<a class="ae kv" href="https://huggingface.co" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>的人们让我们谈论这种令人耳目一新的趋势，以及<a class="ae kv" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank"> DistilBERT </a> ⁰的培训方法，这自然与我的下一个观点有关。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/2f540d012615c55df6030d748f9cb75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X9gWZ5HdkuxpLydO"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">“学习信号”对人类开发智力至关重要。照片由<a class="ae kv" href="https://unsplash.com/@noguidebook?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">雷切尔</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="2e77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">闪亮的新架构获得了大量的关注(双关语)；但是在ML中，<strong class="ky ir"> <em class="mp">学习信号</em>在后台运行节目。</strong>广义来说，<strong class="ky ir">一个模型的表现受限于模型表现力和训练信号质量组合中最弱的因素</strong>(RL中的目标或回报，DL中的损失)。举个例子，DistilBERT在一个<a class="ae kv" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank"> <em class="mp">师生环境</em> </a> <em class="mp"> ⁰ </em>中接受训练，其中<em class="mp">学生网络</em>(规模较小)试图模仿<em class="mp">教师网络</em>(原创)的行为。通过增加这个术语，而不是仅在原始语言建模任务上训练，<strong class="ky ir">用于学生网络的损失函数更加丰富</strong>，允许网络更有表现力地学习。如果你还是不相信我，想想2014年<a class="ae kv" href="https://arxiv.org/pdf/1406.2661.pdf" rel="noopener ugc nofollow" target="_blank">甘斯</a>的遭遇:一个简单的网络耦合到一个有趣的损失函数(另一个网络)和…💥神奇！</p><p id="e3e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">自我监督和语言建模作为语言任务的通用训练信号</strong>应该像建筑革命一样归功于NLP的进步，所以2020年我希望看到这一领域的创新。</p><h1 id="01e2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">任务和数据集</h1><p id="1409" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">正如你可能听说过的，磁北极和地球的北极并不完全对齐；事实上，磁场年复一年地持续抖动。尽管如此，如果你在荷兰，想去真正的北极，传统的指南针将是一个很好的向导；至少比没有强。然而，随着你越来越接近你的目的地，你的指南针的偏差将变得越来越明显，使它不适合这项任务。</p><p id="9b21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在人工智能研究中，可以明显地得出一个类似的结论。客观测量是科学发展的基石，<strong class="ky ir">即使是有偏差的度量通常也比没有好</strong>。如何衡量进展是一个领域如何发展和最终完成什么研究的重要驱动力；这正是为什么我们需要彻底设计评估，使之与产生最佳发展的激励机制相一致。在过去几年中，标准的NLP任务一直是研究的一个令人惊叹的指南针，然而，<strong class="ky ir">我们越接近解决一个数据集，它作为进步的衡量标准就越糟糕，</strong>这就是为什么在2020年看到新的基准获得动力令人兴奋的原因。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/13774ef54fac0f2076b07e27f67b134e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BCBM_44shyafdecKNgtFlw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">截至2020年2月的GLUE基准排行榜。来源:<a class="ae kv" href="https://gluebenchmark.com" rel="noopener ugc nofollow" target="_blank">gluebenchmark.com</a></p></figure><p id="4339" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，在脸书研究中心，他们最近一直在研究一个新的长格式问题回答数据集和基准:<strong class="ky ir"> ELI5(像我5岁一样向我解释)——</strong>是的，它基于著名的同名<a class="ae kv" href="https://www.reddit.com/r/explainlikeimfive/" rel="noopener ugc nofollow" target="_blank">sub Reddit</a>——。这个新数据集的目的是推动开放领域问答的研究，推动变形金刚目前擅长的任务的边界。</p><blockquote class="ms mt mu"><p id="6231" class="kw kx mp ky b kz la jr lb lc ld ju le mv lg lh li mw lk ll lm mx lo lp lq lr ij bi translated">[……]一个长形式的问答数据集，强调在长源文档中分离相关信息和生成段落长度的解释以响应复杂多样的问题的双重挑战。</p></blockquote><p id="2eb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一个有趣的新数据集的例子是来自DeepMind的<a class="ae kv" href="https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory" rel="noopener ugc nofollow" target="_blank"> PG-19语言建模基准</a>:远程语言建模基准<em class="mp"/>(书规模！)，还有另一个名叫<a class="ae kv" href="https://arxiv.org/pdf/1911.05507.pdf" rel="noopener ugc nofollow" target="_blank">压缩变形金刚</a> ⁵.的变形金刚转世希望这项任务将有助于克服Seq2Seq模型目前在处理(非常)长期依赖性方面的局限性。</p><p id="0574" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就连无处不在的GLUE Benchmark也进行了急需的翻新。⁶作为一个强有力的竞争者在不久的将来成为事实上的通用语言理解基准。它包括——<strong class="ky ir">更具挑战性的任务和更全面的人类基线。</strong></p><p id="16ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果不提到我最近最喜欢的一篇论文，即弗朗索瓦·乔莱(Franç ois Chollet)关于<a class="ae kv" href="https://arxiv.org/abs/1911.01547" rel="noopener ugc nofollow" target="_blank">智力衡量</a>这一更广泛主题的论文，这一部分将是不完整的，该论文在这个问题上玩弄了<em class="mp">哲学旋转</em>，尽管如此，它还是提出了一个具体的建议:抽象推理文集<a class="ae kv" href="https://github.com/fchollet/ARC" rel="noopener ugc nofollow" target="_blank">及其具有挑战性的</a><a class="ae kv" href="https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview" rel="noopener ugc nofollow" target="_blank"> Kaggle竞赛</a>。让这些伟大的倡议不断涌现！</p><h1 id="b18c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">更好的理解<em class="my"/></h1><p id="9519" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">系统有一些吸引人的神秘之处，我们并不完全理解。通常，我们对算法智能的理解与我们对其机制的理解程度成反比。不久前，人们还认为掌握国际象棋比赛需要智力。然后深蓝在1996年打败了加里·卡斯帕罗夫，我们明白了它是如何做到的，所以这台机器不再需要智能。</p><p id="4047" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">围绕'<em class="mp"> why questions' </em>'建立坚实的理解对于取得进展至关重要，这就是为什么模型在任务排行榜上可能看起来很棒，但我们不应该在没有仔细调查他们的内部工作情况下就对他们的能力下过早的结论。将这个想法映射到变形金刚的空间中，<strong class="ky ir">很多工作致力于解开为什么这些模型工作得这么好</strong>；但是最近的文献还没有完全得出一个明确的结论。</p><p id="245d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，在研究伯特的预训练模型的行为时，“<a class="ae kv" href="https://arxiv.org/pdf/1906.04341.pdf" rel="noopener ugc nofollow" target="_blank">伯特在看什么？</a> ⁷“得出结论，某些注意力头负责检测语言现象；与许多直觉相反的是，“<a class="ae kv" href="https://arxiv.org/pdf/1902.10186.pdf" rel="noopener ugc nofollow" target="_blank">注意力不是一种解释</a> ⁸”断言注意力不是解释伯特所理解事物的可靠信号。"<a class="ae kv" href="https://www.aclweb.org/anthology/D19-1445.pdf" rel="noopener ugc nofollow" target="_blank">揭示伯特</a> ⁹的黑暗秘密"对<strong class="ky ir">微调期间发生的事情提供了有价值的见解，</strong>但他们的结论范围有限:注意力没有捕捉到清晰的语言现象，伯特被严重过度参数化(令人惊讶！🤯)，以及<strong class="ky ir"> BERT不需要<em class="mp">非常聪明</em>就能解决大部分任务</strong>的事实。这种定性的探索很容易被忽略，因为它不会在指标中显示出来，但我们应该始终关注它。</p><p id="5264" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总之，<strong class="ky ir">许多关于变形金刚工作原理的秘密仍有待揭开，这就是为什么等待2020年在这个领域出现新的研究令人兴奋。</strong></p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><p id="bfdd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些是我的首选，尽管许多其他主题也值得在这篇文章中关注，比如像<a class="ae kv" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">这样的框架🤗/transformers </a>将继续发展，以支持研究、迁移学习范围的扩大或有效结合符号推理和DL方法的新方法。</p><p id="397e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你有什么看法？<strong class="ky ir">2020年的变形金刚，你最激动的是什么？</strong></p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><p id="b966" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">引用:</p><p id="d221" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]《变形金刚在工作》，2020年1月17日。<a class="ae kv" href="https://www.zeta-alpha.com" rel="noopener ugc nofollow" target="_blank"> Zeta Alpha矢量</a>。</p><p id="f8a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]斯蒂芬·梅里蒂，2019。RNN:停止用你的脑袋思考</p><p id="e791" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]伊恩·古德费勒等人。艾尔。2014.<a class="ae kv" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">生成性对抗网络</a></p><p id="5b7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]安吉拉·范、亚辛·杰尼特、伊桑·佩雷斯等。艾尔。2019.<a class="ae kv" href="https://research.fb.com/wp-content/uploads/2019/07/ELI5-Long-Form-Question-Answering.pdf?" rel="noopener ugc nofollow" target="_blank"> <em class="mp"> ELI5:长格式问答</em> </a> <em class="mp">。</em></p><p id="c32f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5]杰克·w·雷等人。艾尔。2019.<a class="ae kv" href="https://arxiv.org/abs/1911.05507" rel="noopener ugc nofollow" target="_blank">用于长程序列建模的压缩变压器</a></p><p id="79fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[6]王敬实、亚大·普鲁克萨奇昆、尼基塔·南吉亚、阿曼普里特·辛格等。艾尔。2019.强力胶:通用语言理解系统的一个更棘手的基准</p><p id="1b15" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[7]凯文·克拉克，乌尔瓦希·汉德尔瓦尔，奥迈尔·利维，克里斯托弗·d·曼宁，2019年。<a class="ae kv" href="https://arxiv.org/pdf/1906.04341.pdf" rel="noopener ugc nofollow" target="_blank">伯特在看什么？伯特注意力分析。</a></p><p id="0d24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[8]萨尔萨克·贾恩，拜伦·华莱士，2019年。<a class="ae kv" href="https://arxiv.org/pdf/1902.10186.pdf" rel="noopener ugc nofollow" target="_blank">注意力不在解说</a></p><p id="f2cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[9]奥尔加·科瓦列娃，阿列克谢·罗马诺夫，安娜·罗杰斯，安娜·拉姆斯斯基，2019。<a class="ae kv" href="https://www.aclweb.org/anthology/D19-1445.pdf" rel="noopener ugc nofollow" target="_blank">揭露伯特的黑暗秘密</a></p><p id="345d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[10] V. Sanh，l .出道，J. Chaumond，T. Wolf，2019。<a class="ae kv" href="https://arxiv.org/pdf/1910.01108.pdf" rel="noopener ugc nofollow" target="_blank">蒸馏伯特，伯特的蒸馏版本:更小、更快、更便宜、更轻</a></p></div></div>    
</body>
</html>