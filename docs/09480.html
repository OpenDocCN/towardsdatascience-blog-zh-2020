<html>
<head>
<title>Diving into Deep Reinforcement Learning with Deep Q Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用深度 Q 学习潜入深度强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/diving-into-deep-reinforcement-learning-with-deep-q-learning-376e588bb803?source=collection_archive---------55-----------------------#2020-07-06">https://towardsdatascience.com/diving-into-deep-reinforcement-learning-with-deep-q-learning-376e588bb803?source=collection_archive---------55-----------------------#2020-07-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4f15" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从表格方法到函数逼近的过渡</h2></div><p id="2496" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将了解如何开始深度强化学习。经常可以看到，列表 RL 方法比函数逼近方法更容易理解。很多人发现很难从 q 学习过渡到深度 q 学习。所以在这篇文章中，我们将仔细研究深度 Q 学习背后的思想。</p><h2 id="6dea" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">什么是深度强化学习</h2><p id="fb0c" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">我们先来了解一下什么是深度强化学习。深度强化学习由基于函数逼近的算法组成，即有一些函数逼近动作的 Q 值。这里，我们不是维护一个 Q 值表，而是使用像神经网络这样的函数来预测特定状态下的动作的 Q 值。</p><h2 id="d703" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">为什么要深度强化学习</h2><p id="576d" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">你可能想知道，当我们可以简单地使用 Q 值的表格时，为什么我们需要神经网络。答案很简单，将每个 Q 值存储在一个表中会增加内存的使用，这对于具有大的状态空间和动作空间的环境是不可行的，我们的代理可能不会遇到一个状态甚至两次。使用神经网络可以让我们学习网络的权重，而不是学习单个的 Q 值。</p><div class="lz ma mb mc gt ab cb"><figure class="md me mf mg mh mi mj paragraph-image"><img src="../Images/907dd27a3e067b88a0326a8a863b1428.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*4-DM6spR3G_-rokIfaR6YQ.png"/></figure><figure class="md me mm mg mh mi mj paragraph-image"><img src="../Images/9db897114f68aaa1878b187d0c860204.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*KqHpc1kz5BNDeMou8RvMhw.png"/><p class="mn mo gj gh gi mp mq bd b be z dk mr di ms mt translated">表格法与函数逼近。图片由<a class="ae mu" href="https://auth.geeksforgeeks.org/user/AlindGupta/articles" rel="noopener ugc nofollow" target="_blank"> <strong class="bd mv">阿灵古塔</strong> </a> <strong class="bd mv"> </strong>经<a class="ae mu" href="https://www.geeksforgeeks.org/deep-q-learning/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/deep-q-learning/</a></p></figure></div></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h1 id="d64f" class="nd lc iq bd ld ne nf ng lg nh ni nj lj jw nk jx lm jz nl ka lp kc nm kd ls nn bi translated">深度 Q 学习理念</h1><p id="2e9c" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">让我们了解一下 Q 学习的概念和优化技巧。</p><ol class=""><li id="5123" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated">重放记忆:当我们的智能体在环境中行动并探索世界时，我们不会在旅途中训练神经网络。相反，我们将代理的所有经验存储在一个缓冲空间中，并通过随机抽取一批经验或优先化的经验来训练我们的模型。这背后的想法是，神经网络可以找到创建的 b/w 状态，并且可能倾向于过度拟合，并且在新状态下可能无法正常工作。</li><li id="afff" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">目标网络:深度 Q 学习使用的不是一个而是两个神经网络。这不是强制性的，但是具有两个网络可以优化性能，并且还可以解决移动目标的问题。当智能体使用相同的神经网络估计 Q 值和目标值时，出现移动目标。因此，为了避免移动目标，我们使用另一个具有固定参数的神经网络(目标网络),并且目标网络定期更新。</li><li id="5cf5" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">堆叠的帧:如果你使用一个图像作为一个状态，那么一堆多个图像可以用来代表一个状态。这种堆叠的图像给代理一种运动感，从而有助于采取正确的行动。</li><li id="e1cc" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">行动选择:许多行动选择技术可以用于探索，如 epsilon greedy，softmax。在这些技术中，Softmax 动作选择要好得多，因为它会在一段时间后自动停止探索。在本文中，我们将使用 greedy。</li></ol><h2 id="c924" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">算法:</h2><p id="4d2f" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">深度 Q 学习的步骤如下:</p><ol class=""><li id="66cb" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated">代理观察环境的当前状态。如果随机数小于或等于ε，则代理将采取随机行动，否则 DQN 预测 Q 值，并采取最大 Q 值的行动。</li><li id="0b81" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">下一个状态、奖励和一个终端变量存储在重放内存中。</li><li id="d139" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">经过一段时间，当记忆中有足够的例子时，代理通过抽样一批经验来训练 DQN。当前状态集被视为特征，标签是目标值，计算为[Traget = set _ of _ reward+gamma * numpy . max(target _ net . predict(set of next _ state))* set _ of _ done]。这里的 done 是这里使用的终端变量，因此终端状态的值为零。</li><li id="592e" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">目标网络定期更新主网络。</li></ol><figure class="lz ma mb mc gt me gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/faddf4528f505c58f3cb6961f37010d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*G2x9_EQxrwdT25Pw1q8ISg.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图片 via <a class="ae mu" href="https://dnddnjs.gitbooks.io/rl/content/deep_q_networks.html" rel="noopener ugc nofollow" target="_blank"> dnddnjs.gitbooks.io </a></p></figure><p id="47e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以在这里找到代码<a class="ae mu" href="https://github.com/abhisheksuran/Atari_DQN" rel="noopener ugc nofollow" target="_blank"/>。我将在下一篇文章中解释代码</p><p id="1def" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢你阅读我的文章，希望你喜欢并且能够理解我想要解释的东西。希望你阅读我即将发表的文章。哈里奥姆…🙏</p><h1 id="f050" class="nd lc iq bd ld ne od ng lg nh oe nj lj jw of jx lm jz og ka lp kc oh kd ls nn bi translated">参考资料:</h1><div class="oi oj gp gr ok ol"><a href="https://www.coursera.org/specializations/reinforcement-learning" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">强化学习</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">由阿尔伯塔大学提供。强化学习专业化包括 4 门课程探索权力…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">www.coursera.org</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz mk ol"/></div></div></a></div><p id="fdec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae mu" href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition" rel="noopener ugc nofollow" target="_blank">https://MIT press . MIT . edu/books/reinforcement-learning-second-edition</a></p></div></div>    
</body>
</html>