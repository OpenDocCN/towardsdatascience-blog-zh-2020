<html>
<head>
<title>Building Improved AI Agents for Doom using Double Deep Q-learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用双深度 Q 学习构建改进的末日智能体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4?source=collection_archive---------37-----------------------#2020-09-02">https://towardsdatascience.com/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4?source=collection_archive---------37-----------------------#2020-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="29e2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Pytorch 中强化学习的实现。</h2></div><h1 id="0267" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">简介</strong></h1><p id="50c1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi lw translated"><span class="l lx ly lz bm ma mb mc md me di"> O </span>在过去的几篇文章中，我们已经<a class="ae mf" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830" rel="noopener">讨论了</a>和<a class="ae mf" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">在<a class="ae mf" rel="noopener" target="_blank" href="/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2"> VizDoom 游戏环境</a>中实现了</a> <a class="ae mf" rel="noopener" target="_blank" href="/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc">深度 Q 学习</a> (DQN)并测试了它的性能。深度 Q-learning 是一种高度灵活且响应迅速的在线学习方法，它利用场景内的快速更新来估计环境中的状态-动作(Q)值，以便最大化回报。Q-learning 可以被认为是一种策略外的 TD 方法，其中该算法旨在选择独立于当前遵循的策略的最高值的状态-动作对，并且已经与 OpenAI Atari 健身房环境的许多原始突破相关联。</p><figure class="mg mh mi mj gt mk"><div class="bz fp l di"><div class="ml mm l"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">我们的<a class="ae mf" rel="noopener" target="_blank" href="/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2">香草 DQN 代理</a>的游戏性，训练超过 500 集。</p></figure><p id="0e03" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">然而，DQN 倾向于乐观地高估 Q 值，特别是在训练的初始阶段，导致次优动作选择的风险，因此收敛较慢。为了理解这个问题，回想一下 Q 学习更新方程，该方程利用当前状态奖励和最高值的状态-值对来估计当前状态的 Q 值，该 Q 值用于训练 DQN。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/56bbc8cba8c4d9d49efbbc7030d17b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*AsTJMiBH7ffOCwIaD9l6pA.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">q-学习更新。</p></figure><p id="34e2" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">请注意，误差项中存在一个 TD-target，它由当前奖励和最高值的状态-动作对的 Q 值之和组成，与代理的当前策略无关，因此，Q-学习通常被称为非策略 TD 学习。</p><p id="ef12" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">因此，Q-learning 依赖于为下一个状态选择具有最高值的动作的“远见”。但是我们怎么能确定下一个状态的最佳行动是具有最高 Q 值的行动呢？根据定义，Q 值的准确性取决于我们之前探索过的状态动作。因此，在训练开始时计算的 Q 值的准确性往往是不准确的，因为我们还没有完全探索状态-动作空间。在这种状态下取最大 Q 值可能会选择一个次优的行动，阻碍收敛。由于 TD 目标计算和行动选择是在同一个网络中进行的，这一事实加剧了这一问题，这可能会导致对次优行动选择的强化偏向。</p><p id="6976" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">为了解决这个问题，双深度 Q 学习(DDQN)首先由 Van Hasselt 等人<a class="ae mf" href="https://arxiv.org/abs/1509.06461" rel="noopener ugc nofollow" target="_blank">引入。a1 通过使用两个网络将动作选择与 Q 值目标计算步骤分离。我们可以通过将 TD 目标的 Q 值部分重写为两个部分来实现这一点，如下所示:</a></p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/689980160059361ddcb58521b9aa89a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*wAdiZN7pDamMZ1Ixlg6LxA.png"/></div></figure><p id="5c16" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">在特定的时间步长和状态，我们使用我们的 DQN 网络为我们的下一个状态选择具有最高 Q 值的动作。然后使用目标网络来计算在下一个状态采取该行动的 Q 值，然后将其与当前状态行动的奖励相结合，以形成 TD 目标。网络定期对称更新。</p><p id="903e" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">因此，DDQN 可以减少 Q 值高估，从而确保稳定的学习和更快的收敛。</p><p id="26c5" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">在我们的<a class="ae mf" rel="noopener" target="_blank" href="/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2">上一篇文章</a>中，我们探索了如何通过使用开源的 OpenAI gym 包装库<a class="ae mf" href="https://github.com/shakenes/vizdoomgym" rel="noopener ugc nofollow" target="_blank"> Vizdoomgym </a>，将深度 Q-learning 应用于训练代理在经典 FPS 游戏<strong class="lc iu"> Doom </strong>中扮演一个基本场景。我们将在那篇文章的基础上修改我们的方法，在 Pytorch 中加入 DDQN 架构。</p><h1 id="2b1c" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">实施</strong></h1><p id="f88e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">我们将在与上一篇文章相同的 VizDoomgym 场景中实现我们的方法，<em class="mz">保卫防线，</em>使用相同的多目标条件。</strong>环境的一些特征包括:</p><ul class=""><li id="fae3" class="na nb it lc b ld mr lg ms lj nc ln nd lr ne lv nf ng nh ni bi translated">一个 3 的动作空间:开火，左转，右转。不允许扫射。</li><li id="53a9" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">向玩家发射火球的棕色怪物，命中率为 100%。</li><li id="0e81" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">试图以之字形靠近来咬玩家的粉红色怪物。</li><li id="0880" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">重生的怪物可以承受更多伤害。</li><li id="460a" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">杀死一个怪物+1 点。</li><li id="4280" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">-死了得 1 分。</li></ul><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/03baab3591dd240768f6247caf2567ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/0*fHa56BL-v7pjRdDj.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">“防线方案”的初始状态</p></figure><p id="87f9" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">回想一下，在<a class="ae mf" rel="noopener" target="_blank" href="/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2">我们最初的 DQN 实现</a>中，我们已经利用了两个并发网络——一个用于行动选择的评估网络，以及一个定期更新的目标网络，以确保生成的 TD 目标是固定的。我们可以利用这个现有的设置来构建我们的 DDQN 架构，而无需初始化更多的网络。</p><p id="2071" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">请注意，由于两个网络定期更新彼此的权重，因此这两个模型仍然是部分耦合的，但重要的是，动作选择和 Q 值评估是由在特定时间步长不共享同一组 a 权重的独立网络完成的。</p><p id="4978" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">我们的 Google 协作实现是利用 Pytorch 用 Python 编写的，可以在<a class="ae mf" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> GradientCrescent Github 上找到。</a>我们的方法基于泰伯优秀强化学习<a class="ae mf" href="https://www.manning.com/livevideo/reinforcement-learning-in-motion" rel="noopener ugc nofollow" target="_blank">课程</a>中详述的方法。由于我们的 DDQN 实现类似于我们之前的普通 DQN 实现，所以整个高级工作流是共享的，这里不再重复<a class="ae mf" rel="noopener" target="_blank" href="/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2"/>。</p><p id="1969" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">让我们从导入所有必需的包开始，包括 OpenAI 和 Vizdoomgym 环境。我们还将安装火炬视觉所需的 AV 包，我们将使用它进行可视化。请注意，安装完成后必须重新启动运行时。</p><pre class="mg mh mi mj gt np nq nr ns aw nt bi"><span id="bea4" class="nu kj it nq b gy nv nw l nx ny">#Visualization cobe for running within Colab<br/>!sudo apt-get update<br/>!sudo apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev libopenal-dev timidity libwildmidi-dev unzip</span><span id="5898" class="nu kj it nq b gy nz nw l nx ny"># Boost libraries<br/>!sudo apt-get install libboost-all-dev</span><span id="ff5d" class="nu kj it nq b gy nz nw l nx ny"># Lua binding dependencies<br/>!apt-get install liblua5.1-dev<br/>!sudo apt-get install cmake libboost-all-dev libgtk2.0-dev libsdl2-dev python-numpy git<br/>!git clone <a class="ae mf" href="https://github.com/shakenes/vizdoomgym.git" rel="noopener ugc nofollow" target="_blank">https://github.com/shakenes/vizdoomgym.git</a><br/>!python3 -m pip install -e vizdoomgym/</span><span id="b90f" class="nu kj it nq b gy nz nw l nx ny">!pip install av</span></pre><p id="6eab" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">接下来，我们初始化我们的环境场景，检查观察空间和动作空间，并可视化我们的环境..</p><pre class="mg mh mi mj gt np nq nr ns aw nt bi"><span id="39e5" class="nu kj it nq b gy nv nw l nx ny">#Check the environment. You'll need to restart the runtime for it to work<br/>import gym<br/>import vizdoomgym<br/><br/>env = gym.make('VizdoomCorridor-v0')</span><span id="7d6d" class="nu kj it nq b gy nz nw l nx ny"># use like a normal Gym environment<br/>state = env.reset()<br/>state, reward, done, info = env.step(env.action_space.sample())<br/>print(state.shape)<br/># env.render()<br/>env.close()</span></pre><p id="8cb2" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">接下来，我们将定义预处理包装器。这些类继承自 OpenAI gym 基类，覆盖了它们的方法和变量，以便隐式地提供所有必要的预处理。我们将开始定义一个包装器来重复许多帧的每个动作，并执行元素方式的最大值以增加任何动作的强度。您会注意到一些三级参数，如<em class="mz"> fire_first </em>和<em class="mz">no _ ops</em>——这些是特定于环境的，在 Vizdoomgym 中对我们没有影响。</p><pre class="mg mh mi mj gt np nq nr ns aw nt bi"><span id="48ec" class="nu kj it nq b gy nv nw l nx ny">class RepeatActionAndMaxFrame(gym.Wrapper):<br/>  #input: environment, repeat<br/>  #init frame buffer as an array of zeros in shape 2 x the obs space<br/>    def __init__(self, env=None, repeat=4, clip_reward=False, no_ops=0,<br/>                 fire_first=False):<br/>        super(RepeatActionAndMaxFrame, self).__init__(env)<br/>        self.repeat = repeat<br/>        self.shape = env.observation_space.low.shape<br/>        self.frame_buffer = np.zeros_like((2, self.shape))<br/>        self.clip_reward = clip_reward<br/>        self.no_ops = no_ops<br/>        self.fire_first = fire_first</span><span id="72b1" class="nu kj it nq b gy nz nw l nx ny">  def step(self, action):<br/>        t_reward = 0.0<br/>        done = False<br/>        for i in range(self.repeat):<br/>            obs, reward, done, info = self.env.step(action)<br/>            if self.clip_reward:<br/>                reward = np.clip(np.array([reward]), -1, 1)[0]<br/>            t_reward += reward<br/>            idx = i % 2<br/>            self.frame_buffer[idx] = obs<br/>            if done:<br/>                break</span><span id="098b" class="nu kj it nq b gy nz nw l nx ny">        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])<br/>        return max_frame, t_reward, done, info</span><span id="c75f" class="nu kj it nq b gy nz nw l nx ny">  def reset(self):<br/>        obs = self.env.reset()<br/>        no_ops = np.random.randint(self.no_ops)+1 if self.no_ops &gt; 0    else 0<br/>        for _ in range(no_ops):<br/>            _, _, done, _ = self.env.step(0)<br/>            if done:<br/>                self.env.reset()<br/>        <br/>        if self.fire_first:<br/>            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'<br/>            obs, _, _, _ = self.env.step(1)</span><span id="2ca5" class="nu kj it nq b gy nz nw l nx ny">        self.frame_buffer = np.zeros_like((2,self.shape))<br/>        self.frame_buffer[0] = obs</span><span id="095a" class="nu kj it nq b gy nz nw l nx ny">    return obs</span></pre><p id="ac64" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">接下来，我们为我们的观察定义预处理函数。我们将使我们的环境对称，将它转换到盒子空间，将通道整数交换到张量的前面，并将其从原始(320，480)分辨率调整到(84，84)区域。我们也将我们的环境灰度化，并通过除以一个常数来归一化整个图像。</p><pre class="mg mh mi mj gt np nq nr ns aw nt bi"><span id="7897" class="nu kj it nq b gy nv nw l nx ny">class PreprocessFrame(gym.ObservationWrapper):<br/>  #set shape by swapping channels axis<br/> #set observation space to new shape using gym.spaces.Box (0 to 1.0)<br/>    def __init__(self, shape, env=None):<br/>        super(PreprocessFrame, self).__init__(env)<br/>        self.shape = (shape[2], shape[0], shape[1])<br/>        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,<br/>                                    shape=self.shape, dtype=np.float32)</span><span id="406d" class="nu kj it nq b gy nz nw l nx ny">   def observation(self, obs):<br/>        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)<br/>        resized_screen = cv2.resize(new_frame, self.shape[1:],<br/>                                    interpolation=cv2.INTER_AREA)<br/>        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)<br/>        new_obs = new_obs / 255.0</span><span id="891c" class="nu kj it nq b gy nz nw l nx ny">   return new_obs</span></pre><p id="bd6d" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">接下来，我们创建一个包装器来处理帧堆叠。这里的目标是通过将几个帧堆叠在一起作为单个批次，帮助从堆叠帧中捕捉运动和方向。这样，我们可以捕捉环境中元素的位置、平移、速度和加速度。通过堆叠，我们的输入采用(4，84，84，1)的形状。</p><pre class="mg mh mi mj gt np nq nr ns aw nt bi"><span id="e637" class="nu kj it nq b gy nv nw l nx ny">class StackFrames(gym.ObservationWrapper):<br/>  #init the new obs space (gym.spaces.Box) low &amp; high bounds as repeat of n_steps. These should have been defined for vizdooom<br/>  <br/>  #Create a return a stack of observations<br/>    def __init__(self, env, repeat):<br/>        super(StackFrames, self).__init__(env)<br/>        self.observation_space = gym.spaces.Box( env.observation_space.low.repeat(repeat, axis=0),<br/>                              env.observation_space.high.repeat(repeat, axis=0),<br/>                            dtype=np.float32)<br/>        self.stack = collections.deque(maxlen=repeat)</span><span id="b2ff" class="nu kj it nq b gy nz nw l nx ny">    def reset(self):<br/>        self.stack.clear()<br/>        observation = self.env.reset()<br/>        for _ in range(self.stack.maxlen):<br/>            self.stack.append(observation)</span><span id="9fb0" class="nu kj it nq b gy nz nw l nx ny">        return  np.array(self.stack).reshape(self.observation_space.low.shape)</span><span id="d845" class="nu kj it nq b gy nz nw l nx ny">    def observation(self, observation):<br/>        self.stack.append(observation)</span><span id="0758" class="nu kj it nq b gy nz nw l nx ny">    return np.array(self.stack).reshape(self.observation_space.low.shape)</span></pre><p id="37e8" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">最后，在返回最终环境供使用之前，我们将所有的包装器绑定到一个单独的<em class="mz"> make_env() </em>方法中。</p><pre class="mg mh mi mj gt np nq nr ns aw nt bi"><span id="7672" class="nu kj it nq b gy nv nw l nx ny">def make_env(env_name, shape=(84,84,1), repeat=4, clip_rewards=False,<br/>             no_ops=0, fire_first=False):<br/>    env = gym.make(env_name)<br/>    env = PreprocessFrame(shape, env)<br/>    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards, no_ops, fire_first)<br/>    <br/>    env = StackFrames(env, repeat)</span><span id="452c" class="nu kj it nq b gy nz nw l nx ny">    return env</span></pre><p id="e60a" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">接下来，让我们定义我们的模型，一个深度 Q 网络。这基本上是一个三层卷积网络，它采用预处理的输入观察值，将生成的展平输出馈送到一个全连接层，生成将游戏空间中的每个动作作为输出的概率。请注意，这里没有激活层，因为激活层的存在会导致二进制输出分布。我们的损失是我们当前状态-动作的估计 Q 值和我们预测的状态-动作值的平方差。我们将使用 RMSProp 优化器来最小化我们在训练期间的损失。</p><pre class="mg mh mi mj gt np nq nr ns aw nt bi"><span id="3579" class="nu kj it nq b gy nv nw l nx ny">import os<br/>import torch as T<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>import numpy as np</span><span id="3292" class="nu kj it nq b gy nz nw l nx ny">class DeepQNetwork(nn.Module):<br/>    def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):<br/>        super(DeepQNetwork, self).__init__()<br/>        self.checkpoint_dir = chkpt_dir<br/>        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)</span><span id="7c65" class="nu kj it nq b gy nz nw l nx ny">        self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)<br/>        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)<br/>        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)</span><span id="beb6" class="nu kj it nq b gy nz nw l nx ny">        fc_input_dims = self.calculate_conv_output_dims(input_dims)</span><span id="bc13" class="nu kj it nq b gy nz nw l nx ny">        self.fc1 = nn.Linear(fc_input_dims, 512)<br/>        self.fc2 = nn.Linear(512, n_actions)</span><span id="0a1f" class="nu kj it nq b gy nz nw l nx ny">        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)</span><span id="8ebb" class="nu kj it nq b gy nz nw l nx ny">        self.loss = nn.MSELoss()<br/>        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')<br/>        self.to(self.device)</span><span id="64d2" class="nu kj it nq b gy nz nw l nx ny">  def calculate_conv_output_dims(self, input_dims):<br/>        state = T.zeros(1, *input_dims)<br/>        dims = self.conv1(state)<br/>        dims = self.conv2(dims)<br/>        dims = self.conv3(dims)<br/>        return int(np.prod(dims.size()))</span><span id="8d46" class="nu kj it nq b gy nz nw l nx ny">  def forward(self, state):<br/>        conv1 = F.relu(self.conv1(state))<br/>        conv2 = F.relu(self.conv2(conv1))<br/>        conv3 = F.relu(self.conv3(conv2))<br/>        # conv3 shape is BS x n_filters x H x W<br/>        conv_state = conv3.view(conv3.size()[0], -1)<br/>        # conv_state shape is BS x (n_filters * H * W)<br/>        flat1 = F.relu(self.fc1(conv_state))<br/>        actions = self.fc2(flat1)</span><span id="b82e" class="nu kj it nq b gy nz nw l nx ny">        return actions</span><span id="0ab0" class="nu kj it nq b gy nz nw l nx ny">  def save_checkpoint(self):<br/>        print('... saving checkpoint ...')<br/>        T.save(self.state_dict(), self.checkpoint_file)</span><span id="c31f" class="nu kj it nq b gy nz nw l nx ny">  def load_checkpoint(self):<br/>        print('... loading checkpoint ...')<br/>        self.load_state_dict(T.load(self.checkpoint_file))</span></pre><p id="cea9" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">回想一下，Q-learning 的更新功能需要:</p><ul class=""><li id="bf32" class="na nb it lc b ld mr lg ms lj nc ln nd lr ne lv nf ng nh ni bi translated">当前状态<em class="mz"> s </em></li><li id="91a9" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">当前动作<em class="mz">一</em></li><li id="6744" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">当前动作后的奖励<em class="mz"> r </em></li><li id="a4ff" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">下一个状态<em class="mz">s’</em></li><li id="bec0" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">下一个动作<em class="mz">a’</em></li></ul><p id="e8a1" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">为了以有意义的数量提供这些参数，我们需要按照一组参数评估我们当前的策略，并将所有变量存储在一个缓冲区中，我们将在训练期间从该缓冲区中提取迷你批次中的数据。因此，我们需要一个重放内存缓冲区来存储和提取观察值。</p><pre class="mg mh mi mj gt np nq nr ns aw nt bi"><span id="f544" class="nu kj it nq b gy nv nw l nx ny">import numpy as np</span><span id="9c11" class="nu kj it nq b gy nz nw l nx ny">class ReplayBuffer(object):<br/>    def __init__(self, max_size, input_shape, n_actions):<br/>        self.mem_size = max_size<br/>        self.mem_cntr = 0<br/>        self.state_memory = np.zeros((self.mem_size, *input_shape),<br/>                                     dtype=np.float32)<br/>        self.new_state_memory = np.zeros((self.mem_size, *input_shape),<br/>                                         dtype=np.float32)</span><span id="bc74" class="nu kj it nq b gy nz nw l nx ny">        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)<br/>        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)<br/>        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)</span><span id="2939" class="nu kj it nq b gy nz nw l nx ny">#Identify index and store  the the current SARSA into batch memory<br/>    def store_transition(self, state, action, reward, state_, done):<br/>        index = self.mem_cntr % self.mem_size<br/>        self.state_memory[index] = state<br/>        self.new_state_memory[index] = state_<br/>        self.action_memory[index] = action<br/>        self.reward_memory[index] = reward<br/>        self.terminal_memory[index] = done<br/>        self.mem_cntr += 1</span><span id="f208" class="nu kj it nq b gy nz nw l nx ny">def sample_buffer(self, batch_size):<br/>        max_mem = min(self.mem_cntr, self.mem_size)<br/>        batch = np.random.choice(max_mem, batch_size, replace=False)<br/>        #I believe batch here is creating a list limit that is acquired through max_mem, whihch we use to subselect memory<br/>        states = self.state_memory[batch]<br/>        actions = self.action_memory[batch]<br/>        rewards = self.reward_memory[batch]<br/>        states_ = self.new_state_memory[batch]<br/>        terminal = self.terminal_memory[batch]</span><span id="a08b" class="nu kj it nq b gy nz nw l nx ny">     return states, actions, rewards, states_, terminal</span></pre><p id="e71e" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">接下来，我们将定义我们的代理，它不同于我们普通的 DQN 实现。我们的代理正在使用一个勘探率递减的ε贪婪策略，以便随着时间的推移最大化开发。为了学会预测使我们的累积奖励最大化的状态-行动-值，我们的代理人将使用通过抽样存储的记忆获得的贴现的未来奖励。</p><p id="8307" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">您会注意到，作为代理的一部分，我们初始化了 DQN 的两个副本，并使用方法将原始网络的权重参数复制到目标网络中。虽然我们的常规方法利用这种设置来生成固定的 TD 目标，但我们的 DDQN 方法将扩展到这一范围之外:</p><ul class=""><li id="c416" class="na nb it lc b ld mr lg ms lj nc ln nd lr ne lv nf ng nh ni bi translated">从重放存储器中检索状态、动作、奖励和下一状态(sar)。</li><li id="e920" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">评估网络用于生成当前状态的所有动作的 Q 值。</li><li id="5814" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">评估网络用于创建下一状态的 Q 值，最高 Q 值保存为<em class="mz"> max_actions。</em></li><li id="8017" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">目标网络还用于创建下一个状态的 Q 值。</li><li id="35b7" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">当前状态的 TD-target 通过将当前状态中的奖励与通过评估网络识别的<em class="mz"> max_actions </em>从下一个状态的目标网络导出的 Q 值相结合来计算。</li><li id="9f83" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">通过将 TD 目标与当前状态 Q 值进行比较来计算损失函数，然后将其用于训练网络。</li></ul><pre class="mg mh mi mj gt np nq nr ns aw nt bi"><span id="c5d8" class="nu kj it nq b gy nv nw l nx ny">import numpy as np<br/>import torch as T<br/>#from deep_q_network import DeepQNetwork<br/>#from replay_memory import ReplayBuffer</span><span id="c8a7" class="nu kj it nq b gy nz nw l nx ny">class DDQNAgent(object):<br/>    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,<br/>                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,<br/>                 replace=1000, algo=None, env_name=None, chkpt_dir='tmp/dqn'):<br/>        self.gamma = gamma<br/>        self.epsilon = epsilon<br/>        self.lr = lr<br/>        self.n_actions = n_actions<br/>        self.input_dims = input_dims<br/>        self.batch_size = batch_size<br/>        self.eps_min = eps_min<br/>        self.eps_dec = eps_dec<br/>        self.replace_target_cnt = replace<br/>        self.algo = algo<br/>        self.env_name = env_name<br/>        self.chkpt_dir = chkpt_dir<br/>        self.action_space = [i for i in range(n_actions)]<br/>        self.learn_step_counter = 0</span><span id="00ad" class="nu kj it nq b gy nz nw l nx ny">        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)</span><span id="c6a2" class="nu kj it nq b gy nz nw l nx ny">        self.q_eval = DeepQNetwork(self.lr, self.n_actions,<br/>                                    input_dims=self.input_dims,<br/>                                    name=self.env_name+'_'+self.algo+'_q_eval',<br/>                                    chkpt_dir=self.chkpt_dir)</span><span id="8c2c" class="nu kj it nq b gy nz nw l nx ny">        self.q_next = DeepQNetwork(self.lr, self.n_actions,<br/>                                    input_dims=self.input_dims,<br/>                                    name=self.env_name+'_'+self.algo+'_q_next',<br/>                                    chkpt_dir=self.chkpt_dir)</span><span id="76c2" class="nu kj it nq b gy nz nw l nx ny">    #Epsilon greedy action selection<br/>    def choose_action(self, observation):<br/>        if np.random.random() &gt; self.epsilon:<br/>          # Add dimension to observation to match input_dims x batch_size by placing in list, then converting to tensor<br/>            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)<br/>            actions = self.q_eval.forward(state)<br/>            #Return tensor, but use item() to reutnr integer<br/>            action = T.argmax(actions).item()<br/>        else:<br/>            action = np.random.choice(self.action_space)</span><span id="cd5c" class="nu kj it nq b gy nz nw l nx ny">        return action</span><span id="c238" class="nu kj it nq b gy nz nw l nx ny">def store_transition(self, state, action, reward, state_, done):<br/>        self.memory.store_transition(state, action, reward, state_, done)</span><span id="b80d" class="nu kj it nq b gy nz nw l nx ny">  def sample_memory(self):<br/>        state, action, reward, new_state, done = \<br/>                                      self.memory.sample_buffer(self.batch_size)</span><span id="f4b6" class="nu kj it nq b gy nz nw l nx ny">        states = T.tensor(state).to(self.q_eval.device)<br/>        rewards = T.tensor(reward).to(self.q_eval.device)<br/>        dones = T.tensor(done).to(self.q_eval.device)<br/>        actions = T.tensor(action).to(self.q_eval.device)<br/>        states_ = T.tensor(new_state).to(self.q_eval.device)</span><span id="587d" class="nu kj it nq b gy nz nw l nx ny">        return states, actions, rewards, states_, dones</span><span id="22e5" class="nu kj it nq b gy nz nw l nx ny">  def replace_target_network(self):<br/>        if self.learn_step_counter % self.replace_target_cnt == 0:<br/>            self.q_next.load_state_dict(self.q_eval.state_dict())</span><span id="76f9" class="nu kj it nq b gy nz nw l nx ny">  def decrement_epsilon(self):<br/>        self.epsilon = self.epsilon - self.eps_dec \<br/>                           if self.epsilon &gt; self.eps_min else  self.eps_min</span><span id="f527" class="nu kj it nq b gy nz nw l nx ny">  def save_models(self):<br/>        self.q_eval.save_checkpoint()<br/>        self.q_next.save_checkpoint()</span><span id="357d" class="nu kj it nq b gy nz nw l nx ny">  def load_models(self):<br/>        self.q_eval.load_checkpoint()<br/>        self.q_next.load_checkpoint()<br/>    #Main DDQN difference here<br/>    def learn(self):</span><span id="0abe" class="nu kj it nq b gy nz nw l nx ny">        #First check if memory is even big enough<br/>        if self.memory.mem_cntr &lt; self.batch_size:<br/>            return</span><span id="c7c4" class="nu kj it nq b gy nz nw l nx ny">        self.q_eval.optimizer.zero_grad()</span><span id="37d5" class="nu kj it nq b gy nz nw l nx ny">        #Replace target network if appropriate<br/>        self.replace_target_network()</span><span id="72e7" class="nu kj it nq b gy nz nw l nx ny">        states, actions, rewards, states_, dones = self.sample_memory()<br/>        #Fetch indices for  matrix multiplication for q_pred<br/>        indices = np.arange(self.batch_size)</span><span id="0dcc" class="nu kj it nq b gy nz nw l nx ny">        #Calculate the value of the states taken using the eval network<br/>        # We use the indices here to make sure our output q_pred is of shape batch_size instead of batch_size x action_size <br/>        q_pred = self.q_eval.forward(states)[indices, actions]<br/>        # calculate the state action value of the next state according to target network<br/>        q_next = self.q_next.forward(states_)<br/>        # calculate the state action value of the next state according to eval network<br/>        q_eval = self.q_eval.forward(states_)</span><span id="1452" class="nu kj it nq b gy nz nw l nx ny">        #Calculate the maximum action value for the new states according to the eval network<br/>        max_actions = T.argmax(q_eval, dim=1)<br/>        <br/>        #Set q_next to 0 for terminal states<br/>        q_next[dones] = 0.0<br/>        q_target = rewards + self.gamma*q_next[indices, max_actions]</span><span id="295e" class="nu kj it nq b gy nz nw l nx ny">        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)<br/>        loss.backward()<br/>        self.q_eval.optimizer.step()<br/>        self.learn_step_counter += 1</span><span id="72ba" class="nu kj it nq b gy nz nw l nx ny">        self.decrement_epsilon()</span></pre><p id="f91b" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">定义了所有支持代码后，让我们运行主训练循环。我们已经在最初的总结中定义了大部分，但是让我们为后代回忆一下。</p><ul class=""><li id="e673" class="na nb it lc b ld mr lg ms lj nc ln nd lr ne lv nf ng nh ni bi translated">对于训练集的每一步，在使用ε-贪婪策略选择下一个动作之前，我们将输入图像堆栈输入到我们的网络中，以生成可用动作的概率分布</li><li id="36cf" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">然后，我们将它输入到网络中，获取下一个状态和相应奖励的信息，并将其存储到我们的缓冲区中。我们更新我们的堆栈，并通过一些预定义的步骤重复这一过程。</li><li id="ce41" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">在一集的结尾，我们将下一个状态输入到我们的网络中，以便获得下一个动作。我们还通过对当前奖励进行贴现来计算下一个奖励。</li><li id="b1a0" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">我们通过 Q 学习更新函数生成我们的目标 y 值，并训练我们的网络。</li><li id="8116" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">通过最小化训练损失，我们更新网络权重参数，以便为下一个策略输出改进的状态-动作值。</li><li id="d44a" class="na nb it lc b ld nj lg nk lj nl ln nm lr nn lv nf ng nh ni bi translated">我们通过跟踪模型的平均得分(在 100 个训练步骤中测量)来评估模型。</li></ul><pre class="mg mh mi mj gt np nq nr ns aw nt bi"><span id="b434" class="nu kj it nq b gy nv nw l nx ny">env = make_env('VizdoomCorridor-v0')<br/>best_score = -np.inf<br/>load_checkpoint = False<br/>n_games = 5000<br/>agent = DDQNAgent(gamma=0.99, epsilon=1.0, lr=0.0001,input_dims=(env.observation_space.shape),n_actions=env.action_space.n, mem_size=5000, eps_min=0.1,batch_size=32, replace=1000, eps_dec=1e-5,chkpt_dir='/content/', algo='DDQNAgent',env_name='vizdoogym')</span><span id="1896" class="nu kj it nq b gy nz nw l nx ny">if load_checkpoint:<br/>  agent.load_models()</span><span id="a4a9" class="nu kj it nq b gy nz nw l nx ny">fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) +'_'+ str(n_games) + 'games'<br/>figure_file = 'plots/' + fname + '.png'</span><span id="fa56" class="nu kj it nq b gy nz nw l nx ny">n_steps = 0<br/>scores, eps_history, steps_array = [], [], []</span><span id="b03e" class="nu kj it nq b gy nz nw l nx ny">for i in range(n_games):<br/>  done = False<br/>  observation = env.reset()</span><span id="e0a1" class="nu kj it nq b gy nz nw l nx ny">score = 0<br/>  while not done:<br/>    action = agent.choose_action(observation)<br/>    observation_, reward, done, info = env.step(action)<br/>    score += reward</span><span id="30d7" class="nu kj it nq b gy nz nw l nx ny">if not load_checkpoint:<br/>      agent.store_transition(observation, action,reward, observation_, int(done))<br/>      agent.learn()<br/>    observation = observation_<br/>    n_steps += 1</span><span id="10af" class="nu kj it nq b gy nz nw l nx ny">scores.append(score)<br/>  steps_array.append(n_steps)</span><span id="4abb" class="nu kj it nq b gy nz nw l nx ny">avg_score = np.mean(scores[-100:])</span><span id="26f3" class="nu kj it nq b gy nz nw l nx ny">if avg_score &gt; best_score:<br/>    best_score = avg_score<br/>      <br/>    <br/>    print('Checkpoint saved at episode ', i)<br/>    agent.save_models()</span><span id="ab5a" class="nu kj it nq b gy nz nw l nx ny">print('Episode: ', i,'Score: ', score,' Average score: %.2f' % avg_score, 'Best average: %.2f' % best_score,'Epsilon: %.2f' % agent.epsilon, 'Steps:', n_steps)</span><span id="cd6a" class="nu kj it nq b gy nz nw l nx ny">eps_history.append(agent.epsilon)<br/>if load_checkpoint and n_steps &gt;= 18000:<br/>    break</span></pre><p id="5bfc" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">我们绘制了 500、1000 和 2000 集的代理商平均得分和 epsilon 比率。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/63dba64029784793fd166488c51052ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*9pHvlpAnL2s1DUXWatPIwQ.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">500 集后我们经纪人的奖励分配。</p></figure><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0ea62a6642ce379e54d0ef3b11b4adea.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*nnAmnxTeYoQrUwOWprQalQ.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">1000 集后我们经纪人的奖励分配。</p></figure><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/fbe05fda14330c531490f4f9aaf3be7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*nTwah1meeyOsLX6Ax-MBrQ.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">2000 集后我们经纪人的报酬分配。</p></figure><p id="e099" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">查看结果并将其与我们的普通 DQN <a class="ae mf" rel="noopener" target="_blank" href="/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2">实现</a>进行比较，您会立即注意到 500、1000 和 2000 集的评分分布有显著改善。此外，您会注意到振荡是如何被显著抑制的，这表明与普通实现相比，收敛性得到了改善。</p><p id="9003" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">我们可以想象我们的代理人在 500 集以下的表现。你可以将它与文章顶部的游戏视频进行比较，视频来自我们在相同的剧集持续时间内训练的香草 DQN 实现。</p><figure class="mg mh mi mj gt mk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="620d" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">但是在 1000 集以上，我们开始看到有趣的行为——代理人停止与怪物接触，只是不停地转圈。这一点之前在<a class="ae mf" href="https://databricks.com/session/real-time-in-flight-drone-route-optimization-with-apache-spark" rel="noopener ugc nofollow" target="_blank"> 2017 Spark AI 峰会</a>的 Vizdoom 环境中已经观察到了。</p><figure class="mg mh mi mj gt mk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="1356" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">虽然在峰会演示中，他们通过奖励功能工程解决了这个问题，但理解为什么会出现这种情况很重要。因为每一次重生都被证明可以显著增加怪物的生命值，所以有可能代理人发现棕色怪物的火球的高伤害在每一次重生中都更加可靠。然而，考虑到粉色怪物的移动模式有些随机，在多集游戏中依赖它们并不是一个可靠的策略。</p><p id="4773" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">解决这个问题要么需要改变环境(用扫射代替转弯可能是一种选择)，要么通过奖励工程——例如，根据生存的持续时间增加奖励。</p><p id="d5d5" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">这就结束了双深度 Q 学习的实现。在我们的下一篇文章中，我们将继续用更高级的 Q-learning 方法来检查我们的代理在这些环境中的性能。</p><p id="39db" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">我们希望你喜欢这篇文章，并希望你查看 GradientCrescent 上的许多其他文章，涵盖人工智能的应用和理论方面。为了保持对<a class="ae mf" href="https://medium.com/@adrianitsaxu" rel="noopener"> GradientCrescent </a>的最新更新，请考虑关注该出版物并关注我们的<a class="ae mf" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> Github </a>资源库。</p><h1 id="6005" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">来源</h1><p id="8e9a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">萨顿等人。al，“强化学习”</p><p id="e50d" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">塔博尔，“运动中的强化学习”</p><p id="40b3" class="pw-post-body-paragraph la lb it lc b ld mr ju lf lg ms jx li lj mt ll lm ln mu lp lq lr mv lt lu lv im bi translated">西蒙尼尼，<a class="ae mf" href="https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/" rel="noopener ugc nofollow" target="_blank">“深度 Q 学习的改进* </a></p></div></div>    
</body>
</html>