<html>
<head>
<title>Quantum machine learning: learning on neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">量子机器学习:神经网络学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quantum-machine-learning-learning-on-neural-networks-fdc03681aed3?source=collection_archive---------30-----------------------#2020-08-19">https://towardsdatascience.com/quantum-machine-learning-learning-on-neural-networks-fdc03681aed3?source=collection_archive---------30-----------------------#2020-08-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4df3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">分析梯度计算、Hadamard 测试等等</h2></div><p id="fb27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我的上一篇文章讨论了量子计算机上的贝叶斯网络(在这里阅读<a class="ae le" href="https://medium.com/analytics-vidhya/quantum-machine-learning-inference-on-bayesian-networks-351f242816e8" rel="noopener"/>！)，以及 k-means 聚类，我们进入量子机器学习这个怪异而奇妙的世界的第一步。</p><p id="91b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这一次，我们将深入兔子洞，看看如何在量子计算机上构建神经网络。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/3f7c696bc250c503a2a2e093ce7a2ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WCYlOskUZ3dXFbgbXC0ZHg@2x.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">作者图片</p></figure><p id="a4c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你不了解神经网络，不要担心——我们从神经网络 101 开始。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="078e" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">什么是(经典)神经网络？</h2><p id="f428" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">几乎每个人都听说过神经网络——它们被用来运行我们今天拥有的一些最酷的技术——自动驾驶汽车、语音助手，甚至是生成名人做可疑事情的超现实照片的软件。</p><p id="cf9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它们与常规算法的不同之处在于，我们不需要写下一套规则，而是需要向网络提供我们希望它解决的问题的例子。</p><p id="4fb8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以用 IRIS 数据集中的一些数据输入一个网络，这个数据集中包含了三种花的信息，它可能会猜出这是哪一种花:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi na"><img src="../Images/1d8f4f365ea8f9153b474bd76b0f4c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VUmCVvZdL1egmto2yydomQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">作者图片</p></figure><p id="2f1a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以现在我们知道了神经网络是做什么的——但是它们是如何做到的呢？</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="cf2a" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">重量、偏见和积木</h2><p id="fe90" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">神经网络由许多称为神经元的小单元组成，看起来像这样:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nb"><img src="../Images/c35ee9d918395469a30fc280bdf5eb37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hpo7_joGQ3Q-oV9R7Nxk-w@2x.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">作者图片</p></figure><p id="bb20" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大多数神经元接受多个数字输入(蓝色圆圈)，并将每个数字输入乘以一个权重(wᵢs)，该权重表示每个输入的重要性。权重的大小越大，相关的输入就越重要。</p><p id="3f9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">偏差被视为另一个权重，只是它所乘以的输入值始终为 1。当我们将所有加权输入相加，我们得到神经元的激活值，用上图中的紫色圆圈表示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/569ce2a831a18f20f8d66003f8447250.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/format:webp/0*pTE4Nq6jRxVuciYD.png"/></div></figure><p id="acc3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，激活值通过一个函数(蓝色矩形)传递，结果是神经元的输出:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/d16852fe2ed5c9b1aacccaad3fcce439.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/0*jNzz0LrNHYVGuZQb.png"/></div></figure><p id="743b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过改变神经元用来转换其激活值的函数来改变神经元的行为，例如，我们可以使用一个超级简单的转换，就像这样:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/a38b7fd983ad10985ea030f73d5173b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/0*b29hzBZHPZztMCvw.png"/></div></figure><p id="cc3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，在实践中，我们使用更复杂的函数，如 sigmoid 函数:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/82c8d15cea567de4ac012ad6a15488d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/0*MRo4qt_H6cSKtw3-.png"/></div></figure><p id="781c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经元有什么用？</p><p id="624b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">他们可以根据他们收到的输入做出决定——例如，我们可以使用神经元来预测我们下次在意大利餐馆吃饭时是吃披萨还是意大利面，方法是向它提供三个问题的答案:</p><ul class=""><li id="4f21" class="ng nh it kk b kl km ko kp kr ni kv nj kz nk ld nl nm nn no bi translated">我喜欢这家餐馆的意大利面吗？</li><li id="aafe" class="ng nh it kk b kl np ko nq kr nr kv ns kz nt ld nl nm nn no bi translated">餐馆有香蒜沙司吗？</li><li id="a9f5" class="ng nh it kk b kl np ko nq kr nr kv ns kz nt ld nl nm nn no bi translated">这家餐馆有三层奶酪比萨饼吗？</li></ul><p id="d370" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">抛开可能的健康问题，让我们看看神经元可能是什么样的——我们可以用 0 代表 no，1 代表 yes 来编码输入，并通过将 0 和 1 分别映射到 pasta 和 pizza 来对输出进行同样的处理:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nu"><img src="../Images/cf20038057920a728184028baf2be8f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GcvCIfy7BJ84TI4_TXD6Vg@2x.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">作者图片</p></figure><p id="1f71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用阶跃函数来转换神经元的激活值:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/aa0b19a4a962cb43dc8da92bfa315fb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/0*v8T7jIWuKwntPMTs.png"/></div></figure><p id="aa3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">仅仅使用一个神经元，我们就可以捕捉多种决策行为:</p><ul class=""><li id="6d23" class="ng nh it kk b kl km ko kp kr ni kv nj kz nk ld nl nm nn no bi translated">如果我们喜欢餐馆里的意大利面，我们会选择点意大利面，除非香蒜沙司卖完了，他们会提供三层奶酪披萨。</li><li id="3d26" class="ng nh it kk b kl np ko nq kr nr kv ns kz nt ld nl nm nn no bi translated">如果我们不喜欢餐馆里的意大利面，我们会点一份比萨饼，除非有香蒜沙司，而三层奶酪比萨饼没有。</li></ul><p id="1e65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们也可以用另一种方式来做事情——我们可以对神经元进行编程，使其对应于一组特定的偏好。</p><p id="f93c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们想做的只是预测我们下次外出时会吃什么，那么为一个神经元计算出一组权重和偏好可能会很容易，但如果我们不得不对一个完整规模的网络做同样的事情呢？</p><p id="3863" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可能需要一段时间。</p><p id="7f13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">幸运的是，我们可以创建算法来改变网络的参数——权重、偏差甚至结构，而不是猜测我们需要的权重值，这样它就可以学习我们想要解决的问题的解决方案。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="ad12" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">向下到达顶端</h2><p id="6bee" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">理想情况下，网络的预测应该与我们提供给它的输入相关联的标签相同——因此，预测和实际输出之间的差异越小，网络学习的权重集就越好。</p><p id="603b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用损失函数来量化这种差异，损失函数可以是我们想要的任何形式，就像这个，它被称为二次损失函数:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/8f24c7ddb02ff2092b39f8aca3b6b3c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/0*99O6zzu0YVnrIRYl.png"/></div></figure><p id="ef13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nx"> y(x) </em>是期望的输出，而<em class="nx"> o(x，θ) </em>是当输入带有参数θ <em class="nx"> — </em>的数据<em class="nx"> x </em>时网络的输出。由于损耗总是非负的，一旦它呈现接近于 0 的值，我们就知道网络已经学习了好的参数集。当然，还有其他可能出现的问题，比如过度拟合，但我们现在忽略这些。</p><p id="4c51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用损耗函数，我们可以计算出网络的最佳参数集:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b73bf599f40dfe9f774d16fa5a2df080.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/0*NWtBNwRLqN24yKd9.png"/></div></figure><p id="17ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们需要做的不是猜测权重，而是根据参数θ最小化<em class="nx"> C </em>,这可以使用一种称为梯度下降的技术来实现:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/307928f11afc9e478fd4c9ce8519b213.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/0*KHc8dpd7yuXGd77o.png"/></div></figure><p id="4faa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在这里所做的是，如果我们增加θᵢ的值，看看损失是如何变化的，然后更新θᵢ，使损失减少一点点。η是一个很小的数字，它控制着我们每次更新θᵢ时要改变多少。</p><p id="bf6f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为什么我们需要η变小？我们可以调整它，使电流<em class="nx"> x </em>的损耗在一次更新后接近于零——大多数情况下这不是一个好主意，因为虽然它会减少电流<em class="nx"> x </em>的损耗，但它通常会导致我们提供给网络的所有其他数据样本的性能更差。</p><p id="1fe8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">厉害！</p><p id="da79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经有了基本的东西，让我们弄清楚如何建立一个量子神经网络。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="76bd" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">进入量子宇宙</h2><p id="5f0b" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">我们将建立的量子神经网络与我们迄今为止研究的经典网络的工作方式并不完全相同——我们没有使用具有权重和偏见的神经元，而是将输入数据编码成一串量子位，应用一系列量子门，并改变门参数以最小化损失函数:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oa"><img src="../Images/d8c1397eb8b6f8a535d27b02a42ccb7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHQgrfKKWI6zcMUmr-BrFg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">作者图片</p></figure><p id="1023" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然这听起来很新鲜，但想法是一样的-更改参数集以最小化网络预测和输入标注之间的差异。</p><p id="abc4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了简单起见，我们将构建一个二元分类器——这意味着输入到网络的每个数据点都必须有一个 0 或 1 的关联标签。</p><p id="40a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它是如何工作的？</p><p id="9ebf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先将一些数据<em class="nx"> x </em>输入到网络，这些数据通过特征图传递，特征图是一种将输入数据转换成我们可以用来创建输入量子态的形式的功能:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/ca72bcc1929e190fced3088f641ecadd.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/format:webp/0*WMNJ3dBR4sPvj9cR.png"/></div></figure><p id="ee12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用的特征地图看起来几乎可以是任何东西——这里有一个二维向量<em class="nx"> x </em>，并给出一个角度:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/f7c25449796bf470ca8a29b75f405d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/0*cG0i9JRedyQ9kZHZ.png"/></div></figure><p id="d615" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦<em class="nx"> x </em>被编码成量子态，我们就应用一系列量子门:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/19c677d486e8cc5b2faaae288e727ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/0*5ZAs77qB6Eo1NUN_.png"/></div></figure><p id="3917" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">网络输出，我们称之为π( <em class="nx"> x </em>，<em class="nx"> </em> θ)，是在|1〉状态下被测量的最后一个量子位的概率，加上一个经典添加的偏置项:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/971cda71abe49008858aade925c263cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/0*ZgRdMYqCQqAU39Sj.png"/></div></figure><p id="2320" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="nx"> Zₙ ₋ ₁ </em> </strong>代表应用于最后一个量子位的<strong class="kk iu"> <em class="nx"> Z </em> </strong>门。</p><p id="83ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们获取与<em class="nx"> x </em>相关联的输出和标签，并使用它们来计算样本上的损耗——我们将使用上面相同的二次损耗。我们为网络提供的整个数据集的成本就变成了:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/5722f542fd68f61f9f7619eeee089041.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/0*O61VhZqMFbqRIamC.png"/></div></figure><p id="bbdf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">网络<em class="nx"> p </em>的预测可以从输出中获得:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/b619ed821d748298d2bcf74138f5610f.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/0*QSeswoUCPLA_gh4X.png"/></div></figure><p id="605e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们需要做的就是弄清楚如何计算损失函数<em class="nx"> l </em> ( <em class="nx"> x </em>，θ)的梯度。虽然我们可以经典地做到这一点，但那会很无聊——我们需要的是一种在量子计算机上计算它们的方法。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="af7c" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">计算梯度的新方法</h2><p id="f78c" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">让我们从关于参数θᵢ:的损失函数的微分开始</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1aa295a81fb453bf25a82845202f5a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*AsPVSfV6zaqOALsT.png"/></div></figure><p id="89db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们扩展最后一项:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7d8f49ecea34d179307578dcc443e58a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/0*OGMBcxAGiBJ-bvye.png"/></div></figure><p id="804b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以很快去掉常数项——在θᵢ = <em class="nx"> b </em>的情况下，我们知道梯度就是 1:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/d3410e237eddd212be28f70604efeeb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*hgLwMqDAFc6yLRMF.png"/></div></figure><p id="d164" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，使用乘积法则，我们可以进一步展开:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ok"><img src="../Images/6efe57e7da67b1ad748d0d144f203d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*t9lF4ArT2Zb6frob.png"/></div></div></figure><p id="de92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">读起来可能有点痛苦，但多亏了厄米共轭(+),它有一个简洁的表示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/522225f43d922717286d14250fd19c50.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/0*kpyFI04C9efUaSol.png"/></div></figure><p id="3d81" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于<strong class="kk iu"> <em class="nx"> U </em> </strong> (θ)由多个门组成，每个门由不同的参数(或参数集)控制，因此求<strong class="kk iu"><em class="nx"/></strong>u<strong class="kk iu"><em class="nx">【uᵢ</em></strong>【θᵢ】<strong class="kk iu"><em class="nx"/></strong>的偏导数只涉及对依赖于θᵢ:的门进行微分</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/2d9c6cb4a7a5dc736d0e874eed959bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*-UgVPK4R2UJbqrNq.png"/></div></figure><p id="4daa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是我们为每个<strong class="kk iu"><em class="nx"/></strong>uᵢ选择的形式变得重要的地方。我们将对每个<strong class="kk iu"><em class="nx"/></strong>使用相同的形式，我们称之为<strong class="kk iu"> <em class="nx"> G </em> </strong>门——形式的选择是任意的，因此您可以使用您能想到的任何其他形式来代替:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e0b1ec05b947313dceb3ca37246973a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/0*Jc-NX97YPrESlQHl.png"/></div></figure><p id="d6b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然我们知道了每一个<strong class="kk iu"><em class="nx"/></strong><strong class="kk iu"><em class="nx"/></strong>的样子，我们就可以找到它的派生物:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f049aa2bab0f0cc248c2ec64c6503211.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/0*4tMHHKLGXaZvl3wy.png"/></div></figure><p id="0377" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">幸运的是，我们可以用<strong class="kk iu"> <em class="nx"> G </em> </strong> <em class="nx"> </em>门来表达这一点:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/2a802f9041d05c2126d1014fb9115ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/0*UVBp8HiL8Uwdl6lJ.png"/></div></figure><p id="2912" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以剩下的就是找出如何创建一个电路，给我们提供我们需要的内在产品形式:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/2c678ccb08cce731e8ad894fb5b40716.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/0*-kncSBimDVAORSn2.png"/></div></figure><p id="89ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">获得与之成比例的可测量值的最简单方法是使用<a class="ae le" href="https://en.wikipedia.org/wiki/Hadamard_test_(quantum_computation)" rel="noopener ugc nofollow" target="_blank">哈达玛测试</a>——首先，我们准备输入量子态，并将一个辅助态推入叠加态:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/ba7111b77b4b0c43758f3a9c8aa77768.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/0*E8kpw-EZ00A3SeXp.png"/></div></figure><p id="012c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在将<strong class="kk iu"> <em class="nx"> Zₙ ₋ ₁B </em> </strong>应用到ψ上，条件是辅助处于 1 状态:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/0404f0a48cd5476f6f653ea87457171b.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/0*rM6NI4m_PcNBu3RP.png"/></div></figure><p id="6d5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后翻转安西拉，对<strong class="kk iu"> <em class="nx">和</em> </strong>做同样的操作:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/4ebb1c3ee2497207ec208f8c8e6738ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/0*tS2wejOImObdvCEc.png"/></div></figure><p id="d34e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，将另一个哈达玛门应用到辅助设备上:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/7420281cd96c38ce335097e2da2a6698.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/0*MezsQjz5F2xUg8FG.png"/></div></figure><p id="3328" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在将辅助设备测量为 0 的概率是</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/98e2bfeaeabb7ae5a85f4573879f8b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*7jbsm1w6RBvrYY1N.png"/></div></figure><p id="3895" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以如果我们用<strong class="kk iu"><em class="nx"/></strong>(θ)代替<strong class="kk iu"><em class="nx"/></strong>，并用<strong class="kk iu"><em class="nx"><strong class="kk iu"><em class="nx"/></strong>(θ)替换掉<strong class="kk iu"><em class="nx"/><strong class="kk iu"><em class="nx"/></strong>的一个副本，那么安西拉量子位的概率就会给我们梯度</strong></em></strong></p><p id="bff8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太好了！</p><p id="0565" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们找到了一种在量子计算机上分析计算梯度的方法——现在剩下的就是构建我们的量子神经网络了。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="12ef" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">构建量子神经网络</h2><p id="926f" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">让我们导入开始工作所需的所有模块:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="4751" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们来看看我们的一些数据(你可以在这里<a class="ae le" href="https://github.com/SashwatAnagolum/DoNew/blob/master/QNN/processedIRISData.csv" rel="noopener ugc nofollow" target="_blank">得到正确的数据</a>！)—这是 IRIS 数据集的处理版本，删除了一个类:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="60a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要将特征(前四列)与标签分开:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="455f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们构建一个函数，它将为我们进行特征映射。</p><p id="9072" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于输入向量是归一化的和 4 维的，因此有一个超级简单的映射选项——使用 2 个量子位来保存编码数据，并使用一个将输入向量重新创建为量子态的映射。</p><p id="b8a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，我们需要两个函数，一个用于从向量中提取角度:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="c12a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个是把我们得到的角度转换成量子态:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="51f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可能看起来有点令人困惑，但是理解它是如何工作的对于建造 QNN 来说并不重要——如果你喜欢，你可以在这里阅读它。</p><p id="bb4f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们可以编写实现<strong class="kk iu"><em class="nx"/></strong>(θ)所需的函数了，这些函数将采取<strong class="kk iu"> <em class="nx"> RY </em> </strong>和<strong class="kk iu"> <em class="nx"> CX </em> </strong> <em class="nx"> </em>门交替层叠的形式。</p><p id="9923" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为什么我们需要<strong class="kk iu"> <em class="nx"> CX </em> </strong>层？</p><p id="54f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们不包括它们，我们将无法执行任何纠缠操作，这将限制我们的网络可以到达的希尔伯特空间内的区域——使用<strong class="kk iu"><em class="nx"/></strong>门，网络可以捕获量子位之间的相互作用，没有它们它将无法实现。</p><p id="f674" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们先从<strong class="kk iu"> <em class="nx"> G </em> </strong>门开始:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="37c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们就做<strong class="kk iu"> <em class="nx"> CX </em> </strong>盖茨:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="5add" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们把它们放在一起得到<strong class="kk iu"> <em class="nx"> U </em> </strong> (θ):</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="3fca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们创建一个函数，它允许我们获得网络的输出，另一个函数将这些输出转换为类预测:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="9f17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们可以构建一个在网络上执行正向传递的函数——向其提供一些数据，对其进行处理，并向我们提供网络输出:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="828d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之后，我们可以编写测量梯度所需的所有函数—首先，我们需要能够应用<strong class="kk iu"><em class="nx"/></strong>(θ)的受控版本:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="4e77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用它，我们可以创建一个计算期望值的函数:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="3e72" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们可以算出损失函数的梯度——我们最后做的乘法是考虑梯度中的<strong class="kk iu"> π </strong> ( <em class="nx"> x 【T43，θ】)-y(<em class="nx">x</em>)项:</em></p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="f71b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们有了梯度，我们就可以使用梯度下降来更新网络参数，以及一种称为动量的技巧，这有助于加快训练时间:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="a5ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们可以构建成本和准确度函数，这样我们就可以看到我们的网络如何响应训练:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="45fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们创建训练网络的函数，并将其命名为:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="0dc2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们传递给 np.random.sample()方法的数字决定了参数集的大小——第一个数字(5)是我们想要的<strong class="kk iu"> <em class="nx"> G </em> </strong>层的数量。</p><p id="9431" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我对一个五层网络进行十五次迭代训练后得到的输出:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="2fb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看起来非常好——我们在验证集上达到了 100%的准确率，这意味着网络成功地推广到了看不见的例子！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="a185" class="mc md it bd me mf mg dn mh mi mj dp mk kr ml mm mn kv mo mp mq kz mr ms mt mu bi translated">包扎</h2><p id="013e" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">所以我们建立了一个量子神经网络——太棒了！</p><p id="4d27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有几种方法可以进一步降低损失——训练网络进行多次迭代，或者调整超参数，如批量大小和学习速率。</p><p id="974f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">向前推进的一个很酷的方法是为<strong class="kk iu"><em class="nx"/></strong>(θ)尝试不同的门选择——你可能会找到一个更好的！</p><p id="43ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在这里抓取整个项目<a class="ae le" href="https://github.com/SashwatAnagolum/DoNew/tree/master/QNN" rel="noopener ugc nofollow" target="_blank">。如果您有任何问题，请在此留言或联系我们——我很乐意帮助您！</a></p></div></div>    
</body>
</html>