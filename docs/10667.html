<html>
<head>
<title>Implement Logistic Regression with L2 Regularization from scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Python 中从头开始用 L2 正则化实现逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implement-logistic-regression-with-l2-regularization-from-scratch-in-python-20bd4ee88a59?source=collection_archive---------2-----------------------#2020-07-26">https://towardsdatascience.com/implement-logistic-regression-with-l2-regularization-from-scratch-in-python-20bd4ee88a59?source=collection_archive---------2-----------------------#2020-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="db4d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">构建自己的逻辑回归分类器的分步指南。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7da6d73c3591a6ee3dbeeb68c67fc7e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YhIMC577agp4dOMq"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马库斯·斯皮斯克</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="5559" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目录:</p><ol class=""><li id="c3e4" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><a class="ae ky" href="#f22f" rel="noopener ugc nofollow">简介</a></li><li id="6a3f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#91d4" rel="noopener ugc nofollow">先决条件</a></li><li id="5364" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#c7e8" rel="noopener ugc nofollow">幕后数学</a></li><li id="ed39" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#4077" rel="noopener ugc nofollow">正规化</a></li><li id="c1de" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#6611" rel="noopener ugc nofollow">代码</a></li><li id="6e8c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#976b" rel="noopener ugc nofollow">结果和演示</a></li><li id="8972" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#c25f" rel="noopener ugc nofollow">未来工作和结论</a></li><li id="e267" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="#aee5" rel="noopener ugc nofollow">参考文献</a></li></ol></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="f22f" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">1.简介:</h2><p id="af6b" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">逻辑回归是用于<em class="no">分类</em>的最常见机器学习算法之一。它是一种统计模型，使用逻辑函数来模拟二元因变量。本质上，它预测了某个观察值属于某个类别或标签的概率。例如，这是一张<em class="no">猫</em>的照片还是一张<em class="no">狗</em>的照片？</p><p id="8d2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">注意:虽然逻辑回归可以扩展到多类分类，但在本文中我们将只讨论二元分类设置。</em></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="7618" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">2.先决条件:</h2><p id="fd69" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">读者应理解以下内容:</p><ul class=""><li id="486f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu np mb mc md bi translated">什么是数据集？</li><li id="07a1" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated">什么是特性？</li><li id="71b0" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated">什么是多重共线性？</li><li id="5e05" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated">什么是 sigmoid 函数？</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="c7e8" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated"><strong class="ak"> 3。幕后数学</strong></h2><p id="a9c2" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated"><strong class="lb iu">假设:</strong>逻辑回归在开始其建模过程之前做出某些关键假设:</p><ol class=""><li id="7c3c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">标签几乎是线性可分的。</li><li id="739f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">观察结果必须是相互独立的。</li><li id="a508" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">独立变量之间的多重共线性很小或没有。</li><li id="d0b9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">自变量与对数概率呈线性相关。</li></ol><p id="035a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">假设:</strong>我们希望我们的模型预测某个观察值属于某个类别或标签的概率。因此，我们想要一个假设<strong class="lb iu"> <em class="no"> h </em> </strong>满足下面的条件<code class="fe nq nr ns nt b">0 &lt;= h(x) &lt;= 1</code>，其中<strong class="lb iu"> <em class="no"> x </em> </strong>是一个观察值。</p><p id="cede" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们定义<code class="fe nq nr ns nt b">h(x) = g(w</code> ᵀ <code class="fe nq nr ns nt b">* x)</code>，其中<strong class="lb iu"> <em class="no"> g </em> </strong>为 sigmoid 函数<strong class="lb iu"> <em class="no"> w </em> </strong>为可训练参数。因此，我们有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b8e71217a4251bffb18c6ce53f076c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/0*FkefjXYI0OF0zP0d"/></div></figure><p id="cb42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">观察的成本:</strong>既然我们可以预测观察的概率，我们希望结果具有最小的误差。如果类标签是<strong class="lb iu"> <em class="no"> y </em> </strong>，则与观察值<strong class="lb iu"> <em class="no"> x </em> </strong>相关联的成本(误差)由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a2791b03b9423a21ce8d152d5b52f765.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/1*vSGnYVz6I7sAObKuxuFAoQ.gif"/></div></figure><p id="d3a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">成本函数:</strong>因此，数据集中所有<strong class="lb iu"> <em class="no"> m </em> </strong>个观察值的总成本为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0cfbcdda3e7e8c98c1361821e39b511c.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/0*vZnp94vCoN0vMDAj"/></div></figure><p id="a349" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将成本函数<strong class="lb iu"> <em class="no"> J </em> </strong>改写为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/784605430b0c84d0ed053a85740747e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/0*o57ug0iMGDJVI1qo"/></div></figure><p id="ea75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逻辑回归的目标是找到参数<strong class="lb iu"> <em class="no"> w </em> </strong>，使得<strong class="lb iu"> <em class="no"> J </em> </strong>最小。但是，我们怎么做呢？</p><p id="560e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">梯度下降:</strong></p><blockquote class="ny nz oa"><p id="9953" class="kz la no lb b lc ld ju le lf lg jx lh ob lj lk ll oc ln lo lp od lr ls lt lu im bi translated"><strong class="lb iu">梯度下降</strong>是一种优化算法，用于通过在<strong class="lb iu">最陡下降</strong>方向迭代移动来最小化某个函数，该方向由<strong class="lb iu">梯度的负值定义。</strong></p></blockquote><p id="adbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用以下模板更新每个参数<strong class="lb iu"> <em class="no"> wᵢ </em> </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/de29d628577bcc8a30f82b746cd95892.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Q6ssvXABrvHUZrfy"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/7f4a1b36af833eac582fb2777f64ee33.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/0*7uVvuW-ZGauNWH_V"/></div></figure><p id="14a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上步骤将帮助我们找到一组参数<strong class="lb iu"><em class="no">【wᵢ】，</em> </strong>，然后这些参数将帮助我们提出<strong class="lb iu"><em class="no">【x】</em></strong>来解决我们的二元分类任务。</p><p id="5b1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是也存在与上述梯度下降步骤相关的不期望的结果。在寻找最佳 h(x)的尝试中，会发生以下情况:</p><pre class="kj kk kl km gt og nt oh oi aw oj bi"><span id="f46e" class="mq mr it nt b gy ok ol l om on">CASE I: For class label = 0<br/>h(x) will try to produce results as close 0 as possible<br/>As such, wT.x will be as small as possible<br/>=&gt; Wi will tend to -infinity</span><span id="7eff" class="mq mr it nt b gy oo ol l om on">CASE II: For class label = 1<br/>h(x) will try to produce results as close 1 as possible<br/>As such, wT.x will be as large as possible<br/>=&gt; Wi will tend to +infinity</span></pre><p id="2435" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这导致了一个称为过度拟合的问题，这意味着，模型将无法很好地概括，即，它将无法正确预测未知观察的类标签。所以，为了避免这种情况，我们需要控制 wᵢ.参数的增长 但是，我们怎么做呢？</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="1014" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated"><strong class="ak"> 4。正规化:</strong></h2><p id="4be8" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">正则化是一种通过惩罚成本函数来解决机器学习算法中<em class="no">过拟合</em>问题的技术。这是通过在成本函数中使用附加的惩罚项来实现的。有两种类型的正则化技术:</p><ol class=""><li id="da6c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">拉索或 L1 正则化</li><li id="67c2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">岭或 L2 正则化(我们将在本文中只讨论这一点)</li></ol><p id="29f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，L2 正则化如何帮助防止过度拟合呢？让我们先来看看我们新的成本函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/26b41e093b76ef11c217deac3637b40d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/0*Nc_ocecF0dHpUutK"/></div></figure><p id="bec3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="no"> λ </em> </strong>称为正则化参数。它控制两个目标之间的权衡:<em class="no">很好地拟合训练数据</em> <strong class="lb iu"> vs </strong> <em class="no">保持参数较小以避免过度拟合。</em></p><p id="3dba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">于是，<strong class="lb iu"> <em class="no"> J(w) </em> </strong>的梯度变成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/27c40ce614c85b6b30ae17efa909dd42.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/0*37PzYt9c80XZm8dK"/></div></figure><p id="25d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正则项将严重惩罚大的<strong class="lb iu"> <em class="no">、wᵢ </em> </strong>。对更小的<strong class="lb iu"><em class="no"/></strong>的影响会更小。这样，<strong class="lb iu"> <em class="no"> w </em> </strong>的增长受到控制。用这些受控参数<strong class="lb iu"> <em class="no"> w </em> </strong>得到的<strong class="lb iu"> <em class="no"> h(x) </em> </strong>将更具普适性。</p><p id="92d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:<strong class="lb iu"> <em class="no"> λ </em> </strong>是一个超参数值。我们必须通过交叉验证来找到它。</p><ul class=""><li id="56b0" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu np mb mc md bi translated">较大的值<strong class="lb iu"> <em class="no"> λ </em> </strong>会使<strong class="lb iu"> <em class="no"> wᵢ </em> </strong>收缩得更接近 0，这可能会导致欠拟合。</li><li id="525a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu np mb mc md bi translated"><strong class="lb iu"> <em class="no"> λ = 0，</em> </strong>将没有正则化效果。</li></ul><p id="74bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="no">选择λ时，我们必须妥善处理偏差与方差的权衡。</em> </strong></p><p id="c52e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里找到更多关于正规化<a class="ae ky" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)#:~:text=In%20mathematics%2C%20statistics%2C%20and%20computer,problem%20or%20to%20prevent%20overfitting." rel="noopener ugc nofollow" target="_blank">的信息。</a></p><p id="57ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们完成了所有的数学运算。让我们用 Python 实现代码。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="6611" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">5.代码:</h2><p id="d72c" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">注意:尽管我们在上面将正则化参数定义为<strong class="lb iu"> <em class="no"> λ </em> </strong>，我们在代码中使用了<strong class="lb iu"> <em class="no"> C = (1/λ) </em> </strong>以便与<em class="no"> sklearn </em>包相似。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="976b" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">5.结果和演示:</h2><p id="f01c" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">让我们在虚拟数据集上安装分类器，并观察结果:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="8cf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策边界图:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/6911488c675988b9aa303185e9b1ebe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*cg5u-0iKthH82o0d7yu8uA.jpeg"/></div></figure><p id="b7b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，我们的模型能够很好地对观察结果进行分类。界限就是决策线。</p><p id="b0ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里获得模型的沙盒体验:<a class="ae ky" href="https://play-with-lr.herokuapp.com/" rel="noopener ugc nofollow" target="_blank">现场预览</a>。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="c25f" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">6.未来工作和结论:</h2><p id="a75f" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">通过实现其他算法，如随机平均梯度、<strong class="lb iu"> </strong>有限记忆 BFGS，来解决优化问题，有提高分类器性能的余地。</p><p id="faea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以实现套索或 L1 正则化。</p><p id="9f6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅此而已。谢谢你阅读我的博客。如果你有任何想法，请留下评论、反馈和建议。</p><p id="139f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过我的<a class="ae ky" href="https://arya46.github.io" rel="noopener ugc nofollow" target="_blank">作品集</a>联系我，或者在<a class="ae ky" href="http://www.linkedin.com/in/tulrose" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上找到我。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="aee5" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">7.参考资料:</h2><p id="21fe" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">[1]<a class="ae ky" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/learn/machine-learning</a></p><p id="6643" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . linear _ model。LogisticRegression.html</a></p><p id="6444" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]<a class="ae ky" href="https://www.geeksforgeeks.org/understanding-logistic-regression/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/understanding-logistic-regression</a></p><p id="df5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="#5559" rel="noopener ugc nofollow">前往顶级^ </a></p></div></div>    
</body>
</html>