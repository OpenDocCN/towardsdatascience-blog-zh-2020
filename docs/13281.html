<html>
<head>
<title>Fine-Tuning Hugging Face Model with Custom Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自定义数据集微调拥抱人脸模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333?source=collection_archive---------9-----------------------#2020-09-12">https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333?source=collection_archive---------9-----------------------#2020-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1ba2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">端到端示例，解释如何使用 TensorFlow 和 Keras 通过自定义数据集微调拥抱人脸模型。我展示了如何保存/加载训练好的模型，并使用标记化的输入执行预测函数。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2d08d19688b604fc8bb204325b576efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LpnRH9y1gRn6UXh6O4QUPw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者:安德烈·巴拉诺夫斯基</p></figure><p id="8f06" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有很多文章是关于用自己的数据集进行拥抱人脸微调的。很多文章都是用 PyTorch，有些是用 TensorFlow。我有一项任务是基于自定义投诉数据集实现情感分类。我决定用拥抱脸变形金刚，因为 LSTM 的效果不太好。尽管有大量可用的文章，但我花了大量的时间将所有的信息整合在一起，并用 TensorFlow 训练的拥抱脸实现了我自己的模型。似乎大多数(如果不是全部的话)文章在解释训练的时候就停止了。我认为分享一个完整的场景并解释如何保存/加载训练好的模型并执行推理会很有用。这篇文章基于 TensorFlow 的拥抱脸 API。</p><p id="25ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你应该从拥抱脸文档开始。有一个非常有用的部分— <a class="ae lu" href="https://huggingface.co/transformers/master/custom_datasets.html" rel="noopener ugc nofollow" target="_blank">使用定制数据集进行微调</a>。为了了解如何使用自己的句子分类数据来微调拥抱人脸模型，我建议学习这一部分的代码— <em class="lv">与 IMDb 评论的序列分类</em>。拥抱脸文档为 PyTorch 和 TensorFlow 都提供了例子，非常方便。</p><p id="12e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我正在使用<a class="ae lu" href="https://huggingface.co/transformers/model_doc/distilbert.html#tfdistilbertforsequenceclassification" rel="noopener ugc nofollow" target="_blank">tfdistilbertfsequenceclassification</a>类来运行句子分类。关于<a class="ae lu" href="https://huggingface.co/transformers/model_doc/distilbert.html#" rel="noopener ugc nofollow" target="_blank">distil Bert</a>—<em class="lv">distil Bert 是由蒸馏 Bert base 训练出来的小型快速廉价轻便的变压器模型。根据 GLUE 语言理解基准测试</em>，它比 bert-base-uncased 少 40%的参数，运行速度快 60%，同时保留了超过 95%的 bert 性能。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="b60e" class="mb mc it lx b gy md me l mf mg">from transformers import DistilBertTokenizerFast<br/>from transformers import TFDistilBertForSequenceClassification</span><span id="2f80" class="mb mc it lx b gy mh me l mf mg">import tensorflow as tf</span></pre><ol class=""><li id="69f1" class="mi mj it la b lb lc le lf lh mk ll ml lp mm lt mn mo mp mq bi translated"><strong class="la iu">导入并准备数据</strong></li></ol><p id="d4c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个例子是基于使用报纸标题的讽刺分类。数据由劳伦斯·莫罗尼准备，作为他的<a class="ae lu" href="https://www.coursera.org/professional-certificates/tensorflow-in-practice" rel="noopener ugc nofollow" target="_blank"> Coursera </a>培训的一部分(源代码可在<a class="ae lu" href="https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%203%20-%20Lesson%202c.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得)。我直接从劳伦斯的博客中获取数据:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="bf99" class="mb mc it lx b gy md me l mf mg">!wget --no-check-certificate \<br/>    <a class="ae lu" href="https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json</a> \<br/>    -O /tmp/sarcasm.json</span></pre><p id="b9b1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后是数据处理步骤，读取数据，将其分成训练/验证步骤，并提取一组标签:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="4d85" class="mb mc it lx b gy md me l mf mg">training_size = 20000</span><span id="d4b5" class="mb mc it lx b gy mh me l mf mg">with open("/tmp/sarcasm.json", 'r') as f:<br/>    datastore = json.load(f)</span><span id="c93a" class="mb mc it lx b gy mh me l mf mg">sentences = []<br/>labels = []<br/>urls = []<br/>for item in datastore:<br/>    sentences.append(item['headline'])<br/>    labels.append(item['is_sarcastic'])</span><span id="2947" class="mb mc it lx b gy mh me l mf mg">training_sentences = sentences[0:training_size]<br/>validation_sentences = sentences[training_size:]<br/>training_labels = labels[0:training_size]<br/>validation_labels = labels[training_size:]</span></pre><p id="788f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有 20000 个条目用于训练，6709 个条目用于验证。</p><p id="24d5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.<strong class="la iu">设置 BERT 并运行训练</strong></p><p id="8b33" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我们将加载记号赋予器:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="a6ad" class="mb mc it lx b gy md me l mf mg">tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')</span></pre><p id="d0c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">标记培训和验证句子:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="6d86" class="mb mc it lx b gy md me l mf mg">train_encodings = tokenizer(training_sentences,<br/>                            truncation=True,<br/>                            padding=True)<br/>val_encodings = tokenizer(validation_sentences,<br/>                            truncation=True,<br/>                            padding=True)</span></pre><p id="7e7c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">创建 TensorFlow 数据集，我们可以将其输入 TensorFlow <em class="lv"> fit </em>函数进行训练。这里我们用标签映射句子，不需要单独将标签传入<em class="lv"> fit </em>函数:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="600e" class="mb mc it lx b gy md me l mf mg">train_dataset = tf.data.Dataset.from_tensor_slices((<br/>    dict(train_encodings),<br/>    training_labels<br/>))</span><span id="d9a6" class="mb mc it lx b gy mh me l mf mg">val_dataset = tf.data.Dataset.from_tensor_slices((<br/>    dict(val_encodings),<br/>    validation_labels<br/>))</span></pre><p id="100c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要一个预先训练好的拥抱脸模型，我们将根据我们的数据对其进行微调:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="d6aa" class="mb mc it lx b gy md me l mf mg"># We classify two labels in this example. In case of multiclass <br/># classification, adjust num_labels value</span><span id="ee54" class="mb mc it lx b gy mh me l mf mg">model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)</span></pre><p id="140b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过调用 TensorFlow <em class="lv"> fit </em>函数，用我们的数据微调模型。它来自<em class="lv">tfdistillbertforsequenceclassification</em>模型。您可以使用参数进行游戏和试验，但是所选择的选项已经产生了相当好的结果:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="99c3" class="mb mc it lx b gy md me l mf mg">optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)<br/>model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])<br/>model.fit(train_dataset.shuffle(100).batch(16),<br/>          epochs=3,<br/>          batch_size=16,<br/>          validation_data=val_dataset.shuffle(100).batch(16))</span></pre><p id="6d64" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 3 个时期中，它达到 0.0387 的损失和 0.9875 的精度，具有 0.3315 的验证损失和 0.9118 的验证精度。</p><p id="4fb8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用抱脸<em class="lv"> save_pretrained </em>功能保存微调后的模型。使用 Keras 保存函数<em class="lv"> model.save </em>进行保存确实有效，但是这种模型无法加载。这就是我使用<em class="lv"> save_pretrained </em>的原因:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="6370" class="mb mc it lx b gy md me l mf mg">model.save_pretrained("/tmp/sentiment_custom_model")</span></pre><p id="7dd2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">保存模型是基本步骤，运行模型微调需要时间，您应该在训练完成时保存结果。另一个选择是——你可以在云 GPU 上运行 fine-running 并保存模型，在本地运行它以进行推理。</p><p id="d2dd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.<strong class="la iu">加载保存的模型并运行预测功能</strong></p><p id="fdac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我使用 TFDistilBertForSequenceClassification 类来加载保存的模型，方法是从 _pretrained 调用拥抱脸函数<em class="lv">(指向保存模型的文件夹):</em></p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="42d1" class="mb mc it lx b gy md me l mf mg">loaded_model = TFDistilBertForSequenceClassification.from_pretrained("/tmp/sentiment_custom_model")</span></pre><p id="df9f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们想运行<em class="lv">预测</em>功能，并使用微调模型对输入进行分类。为了能够执行推理，我们需要像对训练/验证数据那样对输入句子进行标记。为了能够读取推理概率，将<em class="lv"> return_tensors="tf" </em>标志传递给 tokenizer。然后使用保存的模型调用<em class="lv">预测</em>:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="1752" class="mb mc it lx b gy md me l mf mg">test_sentence = "With their homes in ashes, residents share harrowing tales of survival after massive wildfires kill 15"<br/>test_sentence_sarcasm = "News anchor hits back at viewer who sent her snarky note about ‘showing too much cleavage’ during broadcast"</span><span id="cc03" class="mb mc it lx b gy mh me l mf mg"># replace to test_sentence_sarcasm variable, if you want to test <br/># sarcasm</span><span id="85d1" class="mb mc it lx b gy mh me l mf mg">predict_input = tokenizer.encode(test_sentence,<br/>                                 truncation=True,<br/>                                 padding=True,<br/>                                 return_tensors="tf")</span><span id="b403" class="mb mc it lx b gy mh me l mf mg">tf_output = loaded_model.predict(predict_input)[0]</span></pre><p id="b118" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">预测</em>在拥抱人脸模型上运行的函数返回 logit(soft max 之前的分数)。我们需要应用 SoftMax 函数来获得结果概率:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="6574" class="mb mc it lx b gy md me l mf mg">tf_prediction = tf.nn.softmax(tf_output, axis=1).numpy()[0]</span></pre><p id="9c6e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">结论</strong></p><p id="5ef0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章的目标是展示一个完整的场景，用自定义数据微调拥抱人脸模型——从数据处理、训练到模型保存/加载和推理执行。</p><p id="66a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">源代码</strong></p><ul class=""><li id="62fd" class="mi mj it la b lb lc le lf lh mk ll ml lp mm lt mr mo mp mq bi translated">GitHub 回购</li><li id="7ef1" class="mi mj it la b lb ms le mt lh mu ll mv lp mw lt mr mo mp mq bi translated">在<a class="ae lu" href="https://colab.research.google.com/drive/1yi9N-ZnQHtYfR3QDiwsPxYCYU6WyjwlQ" rel="noopener ugc nofollow" target="_blank"> Colab </a>笔记本中自己运行</li></ul></div></div>    
</body>
</html>