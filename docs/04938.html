<html>
<head>
<title>Hierarchical Reinforcement Learning: FeUdal Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分层强化学习:封建网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hierarchical-reinforcement-learning-feudal-networks-44e2657526d7?source=collection_archive---------37-----------------------#2020-04-29">https://towardsdatascience.com/hierarchical-reinforcement-learning-feudal-networks-44e2657526d7?source=collection_archive---------37-----------------------#2020-04-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="56e5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让电脑看到更大的画面</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b2de4b45906fe184481f24e7d0180124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kbHrvuB_LS7RYNr7"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约翰·西门子在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="d8b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个任务都可以自然地分成子任务。当我们准备晚餐时，我们不会微观管理我们手的每一个小动作。相反，我们将任务分成更小的部分(取出原料、切割、烹饪、上菜)，然后，我们专注于如何单独完成每一项任务。这种分解的概念启发了强化学习的分层方法。具体来说，封建网络(FuNs)通过使用模块化神经网络在经理和工人之间划分计算。经理给员工分配局部的、具体的目标，以达到最佳的学习效果。与此同时，经理学会了如何最优地<em class="lv">分配</em>这些子目标，以最好地完成一个“更大的”任务。</p><p id="fd97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们概述了封建网络背后的架构、直觉和数学。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi md"><img src="../Images/63f3e3215afeeea79582f8f626d289fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ihjZ43-Acs1cvzbM"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@jaysung?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jehyun Sung </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="a0c2" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">FuN的建筑</h1><p id="ca29" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">FuN是一个模块化的神经网络(MNN ),由两个独立的网络组成:经理和工人。这里，我们描述一个具有<strong class="lb iu">离散动作空间</strong>的环境。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/1fb0115b51343275550209d9184ada9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3eJ-1B-4C2yLAU6U4hcX_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有趣的模块化神经网络架构</p></figure><p id="4035" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该架构可能看起来很密集，因此下面是图表上每个节点所代表的内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/2a14defb47f49c8f6449bb7acf74c06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZf7WSR19EvwVJYVBkeb-g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">变量定义</p></figure><p id="7fbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">退一步仔细分析发生了什么是有用的。给定一个学习任务，我们在两个实体之间分配劳动:MNN的“经理”和“工人”部分。我们可以看到，变量<em class="lv"> z </em>只是我们观察的另一种表示，<em class="lv"> x. </em>换句话说，<em class="lv"> z </em>携带的信息与<em class="lv"> x，</em>相同，只是变量不同而已！我们将相同信息传递给员工和经理，但两者处理信息的方式略有不同。</p><h2 id="28b1" class="nd mf it bd mg ne nf dn mk ng nh dp mo li ni nj mq lm nk nl ms lq nm nn mu no bi translated">经理</h2><p id="580c" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">在接收到<em class="lv"> z之后，</em>管理器通过将<em class="lv"> z </em>传递给另一个函数来创建一个不同的潜在状态(<em class="lv"> s </em>)。这种潜在状态是环境的另一种表现形式，但是在更高的维度上。管理者在比工作者更高维的向量空间中操作，以编码管理者如何考虑更大的画面而不仅仅是局部信息。</p><p id="6ea5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，管理者将这种潜在状态(<em class="lv"> s </em>)推入一个递归神经网络(RNN)中，从而输出一个员工要实现的目标。此目标代表员工状态的<strong class="lb iu">相对变化。更正式地说:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/edee541d658bcca7213c232f52e92dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2SHdlFshTIcWwhWgP7k8A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">经理RNN转发功能</p></figure><p id="9684" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> h </em>代表RNN的隐藏州。目标正常化后，我们做一些特别的事情。我们<strong class="lb iu">在有限的时间范围内汇集所有的目标<em class="lv">c</em>T9】，然后将结果无偏差地传递到线性转换中。这有效地从经理的向量空间转换到员工的向量空间，并对经理分配的先前<em class="lv"> c </em>目标的表示进行编码。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/4e7810e30aabe650427443d0445232f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C7AohFMAUC0JxFGY7Yep7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性变换后的汇集</p></figure><p id="c007" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">向量<em class="lv"> w </em>有<em class="lv"> k </em>个维度和两个关键属性:</p><ul class=""><li id="7ba7" class="nr ns it lb b lc ld lf lg li nt lm nu lq nv lu nw nx ny nz bi translated">由于变换没有偏差，它永远不会产生一个不变的非零向量。因此，员工永远不会忽视经理的投入。总有一些“意义”需要工人去提取。</li><li id="b355" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">由于合用，经理的条件随着时间平滑地变化。这可以防止员工无法理解或处理的目标的任何不稳定变化。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/415aaa0210d5a8c80092b6dc913fa03b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_d-Jx87l3FRi_zk9"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约翰·施诺布里奇在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h2 id="b297" class="nd mf it bd mg ne nf dn mk ng nh dp mo li ni nj mq lm nk nl ms lq nm nn mu no bi translated">工人</h2><p id="1c64" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">一旦工作者接收到<em class="lv"> z，</em>它就将<em class="lv"> z </em>传递到不同的递归神经网络中。然而，工作者的RNN输出矩阵，而不是输出向量。该矩阵有几行和k列，行数等于可能的动作数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/c7e0799e770943d77b0e3c6b265b86f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qjX2d76K0tyS4f-9_K_ylA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">工人RNN前进函数</p></figure><p id="0312" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中每个<em class="lv"> h </em>代表RNN的隐藏状态。为了开发关于为什么我们输出矩阵而不是向量的直觉，我们看下面的等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/fdea14130a4f7f23225de2470f102a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*XMXRlbdqUGZGQmnkG80nDg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">工人的行动输出</p></figure><p id="27b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个输出是工人行动的概率分布。然而，让我们换一个稍微不同的视角。假设如果我们选择了相应的动作，那么<em class="lv"> U </em>的每一行都对结果状态进行编码。然后，如果我们查看向量<em class="lv"> Uw </em>，每个元素都是一行<em class="lv"> U </em>和编码目标<em class="lv"> w. </em>之间的点积，将点积视为相似性的度量，并且知道SoftMax保留相对排序，则该向量具有与实现经理目标的概率成比例的<strong class="lb iu">元素，前提是工人选择该行动。</strong>因此，根据这种分布对动作进行采样是有意义的。</p><p id="c3b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个转发过程如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/e4685f9df42e24d65a65f3b6412ce7a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ELiZkQfUB0UBavNmV5fTQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">趣味前进</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="d6c3" class="me mf it bd mg mh oj mj mk ml ok mn mo jz ol ka mq kc om kd ms kf on kg mu mv bi translated">它是如何学习的</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/48aa87fca588e9878fa851e23c9a10a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_5FqxLVfQa78RMId"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">金伯利农民在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="d626" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们考虑一下这个。在执行一个动作后，我们会得到一个奖励和另一组观察结果。然后，我们<em class="lv">可以</em>通过优化工人采取的行动，用TD-learning对整个MNN进行日常培训。之后，我们也将这些梯度传播给经理。然而，这违背了分层学习的目的，因为管理器的输出<em class="lv"> g </em>将<strong class="lb iu">失去所有语义</strong>。这将使乐趣与任何其他网络没有区别，因为g只是成为另一个内在的潜在变量。因此，我们改为独立培训经理和工人。</p><h2 id="9e79" class="nd mf it bd mg ne nf dn mk ng nh dp mo li ni nj mq lm nk nl ms lq nm nn mu no bi translated">经理</h2><p id="9502" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">直觉上，我们希望经理给员工的目标不仅是随着时间的推移回报最大化，而且是员工可以实现的<strong class="lb iu">。</strong>因此，我们最大化员工状态变化和经理设定的目标之间的相似性度量。</p><p id="dc8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据以下等式更新MNN的经理部分:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/58dc19690fc945b91ceab150bf67644e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFQox6nfIBRigRuHo06FRw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">经理政策更新方程式</p></figure><p id="2931" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中d_cos代表两个向量之间的余弦相似度，<em class="lv"> A </em>是管理者的优势函数，<em class="lv"> c </em>代表管理者的眼界。通过将相似性度量乘以优势函数，该更新规则<strong class="lb iu">有效地找到了可行性和回报之间的最佳平衡。</strong>使用经理的内部价值函数计算优势，并以类似于其他演员-评论家算法的方式进行更新。经理的奖励函数是根据手头的任务来定义的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/d1947632a5d05e27dd4179ca9e5bf5e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VOsqtpbR9GvBfVOI"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@solimonster?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> sol </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="07d9" class="nd mf it bd mg ne nf dn mk ng nh dp mo li ni nj mq lm nk nl ms lq nm nn mu no bi translated">工人</h2><p id="a388" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">我们希望鼓励员工遵循经理设定的目标。因此，让我们来定义一种内在奖励:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/b0bfe2d489f240f81c8b95a76418009f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jB5B9WVkA4Jsotr4CJxVOg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">工人内在报酬</p></figure><p id="ecd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种奖励是员工在有限的时间范围内对经理指示的平均遵循程度。该算法被训练成最大化由环境奖励和给定的内在奖励组成的加权和。利用这些，我们训练工人的价值函数，类似于管理者。然后，我们使用以下内容更新员工的策略:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/024ae7184dccc6e1cb3c049b6679389d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G972uWcZpfloN2xGIsXULQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">员工政策更新公式</p></figure><p id="33da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中我们最大化由优势函数缩放的对数概率。这类似于典型的演员-评论家算法。</p><p id="8eca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该算法指出，经理和员工可能有不同的折扣系数。因此，员工可以更关注眼前的、当地的回报，而经理则关注长期的事件。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="4b58" class="me mf it bd mg mh oj mj mk ml ok mn mo jz ol ka mq kc om kd ms kf on kg mu mv bi translated">结果呢</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/a2fe57cdd7912d2792f750c1ffb4c5d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PcDuw8Mjerif6MRtREiSFw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">DeepMind实验室迷宫的有趣表演</p></figure><p id="63eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">FuNs [1]上的论文使用了许多实验来展示算法的鲁棒学习能力，最著名的是在Montezuma的《复仇》和DeepMind Lab的游戏上。使用A3C训练的递归LSTM网络作为基线，在这两个实验中，FuN优于其他方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/68dd3d4f38429cf45d8ad0bee963bf13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*25cb9T-ouq-zPuS6Tp-2Og.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">关于蒙特祖马复仇的有趣表演</p></figure><p id="973f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更令人难以置信的是，乐趣从语义上学习有意义的子目标。在上面的可视化图中，高柱代表经理持续管理的目标，每个目标对应于游戏中的大“转折点”。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="84a3" class="me mf it bd mg mh oj mj mk ml ok mn mo jz ol ka mq kc om kd ms kf on kg mu mv bi translated">就是这样！</h1><p id="df26" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">封建网络为强化学习提供了一个巨大的垫脚石，让代理能够自主分解任务，产生语义上有意义的结果。下一次，我们将探索如何将这个算法扩展到各种多代理框架。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="4e27" class="nd mf it bd mg ne nf dn mk ng nh dp mo li ni nj mq lm nk nl ms lq nm nn mu no bi translated">参考</h2><p id="a864" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">[1] A. Vezhnevets，S. Osindero，T. Schaul，N. Heess，M. Jaderberg，D. Silver，K. Kavukcuoglu，<a class="ae ky" href="https://dl.acm.org/doi/10.5555/3305890.3306047" rel="noopener ugc nofollow" target="_blank">封建等级强化网络</a>学习(2017)，ICML‘17 .</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><blockquote class="ov ow ox"><p id="69ac" class="kz la lv lb b lc ld ju le lf lg jx lh oy lj lk ll oz ln lo lp pa lr ls lt lu im bi translated">从经典到最新，这里有讨论多代理和单代理强化学习的相关文章:</p></blockquote><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">OpenAI的MADDPG算法</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">多主体RL问题的行动者批评方法</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps ks pe"/></div></div></a></div><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/how-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">DeepMind的虚幻算法解释</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">最佳深度强化学习</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="pt l pp pq pr pn ps ks pe"/></div></div></a></div></div></div>    
</body>
</html>