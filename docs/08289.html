<html>
<head>
<title>Decision Trees: Parametric Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树:参数优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-parametric-optimization-344354bd3db5?source=collection_archive---------37-----------------------#2020-06-17">https://towardsdatascience.com/decision-trees-parametric-optimization-344354bd3db5?source=collection_archive---------37-----------------------#2020-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4853" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们开始处理数据时，我们(通常总是)观察到数据中几乎没有错误，比如缺失值、异常值、没有适当的格式等等。简而言之，我们称之为不一致性。这种一致性或多或少会扭曲数据，并妨碍机器学习算法进行正确预测。</p><p id="75b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">之前我们观察了异常值如何影响我们的分析。此外，我们观察到，k-NN 分类器在我们去除异常值并优化其参数后提高了准确性，而对我们来说，我们的决策树分类器表现不佳。一种猜测是，我们没有优化分类器的参数，所以在本文中，我们将看看分类器是否不适合这项任务，或者需要更多的考虑。</p><p id="b4df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您还没有阅读这篇文章，您可以在这里找到它:</p><div class="kl km gp gr kn ko"><a href="https://medium.com/@babandeepsingh411993/effect-of-outliers-in-classification-ed7e8b6d39f8" rel="noopener follow" target="_blank"><div class="kp ab fo"><div class="kq ab kr cl cj ks"><h2 class="bd ir gy z fp kt fr fs ku fu fw ip bi translated">分类中异常值的影响</h2><div class="kv l"><h3 class="bd b gy z fp kt fr fs ku fu fw dk translated">介绍</h3></div><div class="kw l"><p class="bd b dl z fp kt fr fs ku fu fw dk translated">medium.com</p></div></div><div class="kx l"><div class="ky l kz la lb kx lc ld ko"/></div></div></a></div><p id="3540" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看是否可以使用 DT 分类器的参数来提高我们的准确度。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="c85b" class="ln lo iq lj b gy lp lq l lr ls"><em class="lt">class </em>sklearn.tree.<strong class="lj ir">DecisionTreeClassifier</strong>(<em class="lt">*</em>, <em class="lt">criterion='gini'</em>, <em class="lt">splitter='best'</em>, <em class="lt">max_depth=None</em>, <em class="lt">min_samples_split=2</em>, <em class="lt">min_samples_leaf=1</em>, <em class="lt">min_weight_fraction_leaf=0.0</em>, <em class="lt">max_features=None</em>, <em class="lt">random_state=None</em>, <em class="lt">max_leaf_nodes=None</em>, <em class="lt">min_impurity_decrease=0.0</em>, <em class="lt">min_impurity_split=None</em>, <em class="lt">class_weight=None</em>, <em class="lt">presort='deprecated'</em>, <em class="lt">ccp_alpha=0.0</em>)</span></pre></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="fc0d" class="mb lo iq bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated"><em class="my">判断标准</em></h1><p id="5bf6" class="pw-post-body-paragraph jn jo iq jp b jq mz js jt ju na jw jx jy nb ka kb kc nc ke kf kg nd ki kj kk ij bi translated">衡量分割质量的函数。有两个最突出的标准是{ '基尼'，'熵' }。</p><p id="794d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">基尼指数</strong>是通过从 1 中减去每一类概率的平方和来计算的。它倾向于更大的分区。</p><figure class="le lf lg lh gt nf gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/08970455e4a567f7eb521ee7220864a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*sI-C3FwXYZhiZjuN"/></div></figure><p id="a360" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">熵或信息增益</strong>将类别概率乘以该类别概率的对数(基数=2)。</p><figure class="le lf lg lh gt nf gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/6a439a82e527e4bc9735e3743a5612e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/0*KFxJ-ZM0DrISBBI8"/></div></figure><p id="5d76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用这两个标准并检查它们与<em class="lt"> max_depth、minimum_samples_split、minimum_weight_fraction_leaf 的相互作用如何。</em></p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="f8ae" class="mb lo iq bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">最小重量分数叶</h1><p id="3cb9" class="pw-post-body-paragraph jn jo iq jp b jq mz js jt ju na jw jx jy nb ka kb kc nc ke kf kg nd ki kj kk ij bi translated">要求位于叶节点的权重总和(所有输入样本)的最小加权分数。当未提供 sample_weight 时，样本具有相等的权重。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="fcec" class="ln lo iq lj b gy lp lq l lr ls">fractions = [0,0.1,0.2,0.3,0.4,0.5]</span><span id="7348" class="ln lo iq lj b gy ni lq l lr ls">from sklearn.metrics import accuracy_score<br/>min_weight_fraction_leaf = []<br/>acc_gini = []<br/>acc_entropy = []</span><span id="359e" class="ln lo iq lj b gy ni lq l lr ls">for i in fractions:<br/> dtree = DecisionTreeClassifier(criterion='gini', min_weight_fraction_leaf=i )<br/> dtree.fit(X_train, y_train)<br/> pred = dtree.predict(X_test)<br/> acc_gini.append(accuracy_score(y_test, pred))<br/> ####<br/> dtree = DecisionTreeClassifier(criterion='entropy',min_weight_fraction_leaf=i)<br/> dtree.fit(X_train, y_train)<br/> pred = dtree.predict(X_test)<br/> acc_entropy.append(accuracy_score(y_test, pred))<br/> ####<br/> min_weight_fraction_leaf.append(i)<br/>d = pd.DataFrame({'acc_gini':pd.Series(acc_gini), <br/> 'acc_entropy':pd.Series(acc_entropy),<br/> 'min_weight_fraction_leaf':pd.Series(max_depth)})<br/># visualizing changes in parameters<br/>plt.plot('min_weight_fraction_leaf','acc_gini', data=d, label='gini')<br/>plt.plot('min_weight_fraction_leaf','acc_entropy', data=d, label='entropy')<br/>plt.xlabel('min_weight_fraction_leaf')<br/>plt.ylabel('accuracy')<br/>plt.legend()</span></pre><p id="ddf9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从下图中，我们可以观察到，对于这两个标准，精度都较高，为 0.3。</p><p id="8eb4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么？</p><p id="bca6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在向叶节点、终端节点中的样本提供权重之后，我们得到了相似的准确度、可能相似的信息增益以及两个标准之间共享的杂质。这意味着我们得到的是纯节点。</p><figure class="le lf lg lh gt nf gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/f909def18a202a5018cb01b84ce3e182.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*22YTuhVXm9zWpqrIoZ6sqw.png"/></div></figure></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="7249" class="mb lo iq bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">最小 _ 样本 _ 分割</h1><p id="e7fa" class="pw-post-body-paragraph jn jo iq jp b jq mz js jt ju na jw jx jy nb ka kb kc nc ke kf kg nd ki kj kk ij bi translated">分割内部节点所需的最小样本数:</p><ul class=""><li id="52ed" class="nk nl iq jp b jq jr ju jv jy nm kc nn kg no kk np nq nr ns bi translated">如果是 int，那么就把<code class="fe nt nu nv lj b">min_samples_split</code>当做最小数。</li><li id="0fe5" class="nk nl iq jp b jq nw ju nx jy ny kc nz kg oa kk np nq nr ns bi translated">如果是 float，那么<code class="fe nt nu nv lj b">min_samples_split</code>是一个分数，<code class="fe nt nu nv lj b">ceil(min_samples_split * n_samples)</code>是每次分割的最小样本数。</li></ul><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="7947" class="ln lo iq lj b gy lp lq l lr ls">min_samples_split = []<br/>acc_gini = []<br/>acc_entropy = []<br/>for i in range(2,20,1):<br/> dtree = DecisionTreeClassifier(criterion='gini', min_samples_split=i )<br/> dtree.fit(X_train, y_train)<br/> pred = dtree.predict(X_test)<br/> acc_gini.append(accuracy_score(y_test, pred))<br/> ####<br/> dtree = DecisionTreeClassifier(criterion='entropy',min_samples_split=i)<br/> dtree.fit(X_train, y_train)<br/> pred = dtree.predict(X_test)<br/> acc_entropy.append(accuracy_score(y_test, pred))<br/> ####<br/> min_samples_split.append(i)<br/>d = pd.DataFrame({'acc_gini':pd.Series(acc_gini), <br/> 'acc_entropy':pd.Series(acc_entropy),<br/> 'min_samples_split':pd.Series(max_depth)})<br/># visualizing changes in parameters<br/>plt.plot('min_samples_split','acc_gini', data=d, label='gini')<br/>plt.plot('min_samples_split','acc_entropy', data=d, label='entropy')<br/>plt.xlabel('min_samples_split')<br/>plt.ylabel('accuracy')<br/>plt.legend()</span></pre><figure class="le lf lg lh gt nf gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/7aa25e86d452fd21ddb6ae93a404e8ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*mXtNwcr3VKN5I4Fm-1SYTQ.png"/></div></figure><p id="26b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上面的图表中，我们可以看到熵在 min_samples_split 为 7 时工作得更好。</p><p id="bce6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么？</p><p id="d649" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从数学上来说，熵考虑了样本和它们的信息增益来建立一棵树。因此，在样本数量的最佳值下，我们可以获得更多关于进一步分割的信息。另一方面，虽然基尼系数在较大的分区中效果最好。我们拥有的数据包含杂质，这些杂质会导致精度随着 min_samples_split 的增加而降低。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="cef0" class="mb lo iq bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">最大深度</h1><p id="1b8b" class="pw-post-body-paragraph jn jo iq jp b jq mz js jt ju na jw jx jy nb ka kb kc nc ke kf kg nd ki kj kk ij bi translated">树的最大深度。如果没有，则扩展节点，直到所有叶子都是纯的，或者直到所有叶子包含少于 min_samples_split 样本。因此，让这两个参数协调工作是一个很好的做法。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="d805" class="ln lo iq lj b gy lp lq l lr ls">max_depth = []<br/>acc_gini = []<br/>acc_entropy = []<br/>for i in range(1,30):<br/> dtree = DecisionTreeClassifier(criterion='gini',max_depth=i )<br/> dtree.fit(X_train, y_train)<br/> pred = dtree.predict(X_test)<br/> acc_gini.append(accuracy_score(y_test, pred))<br/> ####<br/> dtree = DecisionTreeClassifier(criterion='entropy', max_depth=i)<br/> dtree.fit(X_train, y_train)<br/> pred = dtree.predict(X_test)<br/> acc_entropy.append(accuracy_score(y_test, pred))<br/> ####<br/> max_depth.append(i)<br/>d = pd.DataFrame({'acc_gini':pd.Series(acc_gini), <br/> 'acc_entropy':pd.Series(acc_entropy),<br/> 'max_depth':pd.Series(max_depth)})<br/># visualizing changes in parameters<br/>plt.plot('max_depth','acc_gini', data=d, label='gini')<br/>plt.plot('max_depth','acc_entropy', data=d, label='entropy')<br/>plt.xlabel('max_depth')<br/>plt.ylabel('accuracy')<br/>plt.legend()</span></pre><figure class="le lf lg lh gt nf gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/0b9f0a835bbcbe7b7e0eb4c597884c7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*YWaTMg-wCGffHo6Dod_w8g.png"/></div></figure><p id="6cb8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由此，我们可以清楚地看到，基尼指数跑赢了熵值。</p><p id="e2fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么？</p><p id="8416" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如我们所知，基尼系数考虑到了等级的概率，因此，基尼系数的程度在 0 和 1 之间变化，其中 0 表示所有元素都属于某个等级，或者如果只存在一个等级，1 表示元素随机分布在各个等级中。基尼指数为 0.5 表示某些阶层的人口分布均等。</p><p id="d938" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们的例子中，我们可以得出结论，在 max_depth = 6 时，我们观察到低杂质节点，从而增加了我们的模型的准确性。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="6c48" class="mb lo iq bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">结论</h1><p id="7554" class="pw-post-body-paragraph jn jo iq jp b jq mz js jt ju na jw jx jy nb ka kb kc nc ke kf kg nd ki kj kk ij bi translated">回顾我们上面的讨论，我们可以得出以下结论:</p><ul class=""><li id="55e9" class="nk nl iq jp b jq jr ju jv jy nm kc nn kg no kk np nq nr ns bi translated">基尼指数为我们提供了最高的精确度，最大深度= 6。</li><li id="a93e" class="nk nl iq jp b jq nw ju nx jy ny kc nz kg oa kk np nq nr ns bi translated">熵和基尼指数可以在适当选择最小权重分数叶的情况下表现相似。</li><li id="a635" class="nk nl iq jp b jq nw ju nx jy ny kc nz kg oa kk np nq nr ns bi translated">在 min_samples_split 为 7 的情况下，对于更多样本将提供更多信息增益的基本假设，熵的表现优于基尼系数，并且随着杂质的增加，熵倾向于扭曲基尼系数。</li></ul><p id="48eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，采用基尼和 max_depth = 6 作为标准，我们获得了 32%的精度，这比没有使用参数优化提高了 18%。因此，合理地优化参数，将增加模型精度并提供更好的结果。</p><h1 id="8d85" class="mb lo iq bd mc md oc mf mg mh od mj mk ml oe mn mo mp of mr ms mt og mv mw mx bi translated">参考</h1><ul class=""><li id="44e3" class="nk nl iq jp b jq mz ju na jy oh kc oi kg oj kk np nq nr ns bi translated"><a class="ae ok" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . tree . decision tree classifier . html</a></li><li id="40a1" class="nk nl iq jp b jq nw ju nx jy ny kc nz kg oa kk np nq nr ns bi translated">https://blog.quantinsti.com/gini-index/<a class="ae ok" href="https://blog.quantinsti.com/gini-index/" rel="noopener ugc nofollow" target="_blank"/></li></ul></div></div>    
</body>
</html>