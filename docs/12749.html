<html>
<head>
<title>A Framework For Contrastive Self-Supervised Learning And Designing A New Approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对比自我监督学习的框架和新方法的设计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619?source=collection_archive---------2-----------------------#2020-09-02">https://towardsdatascience.com/a-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619?source=collection_archive---------2-----------------------#2020-09-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8964" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在一篇新论文中，我们讨论了在自我监督学习中驱动性能的关键思想，并展示了什么是重要的。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d0cffa83f0d013f908fe8b52d3194159.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1q-7bfWVmP8jaizAtVx2rQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">成批输入。</p></figure><p id="0f44" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是与我们的新论文相匹配的合作伙伴博客:<a class="ae lr" href="https://arxiv.org/abs/2009.00104" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">对比自我监督学习的框架和设计新方法</strong> </a>(作者威廉·法尔孔和赵京云)。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="4b50" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在过去的一年里，一系列“<strong class="kx ir">小说</strong>”<em class="lz">自我监督学习</em>算法在人工智能研究领域创造了新的最新成果:AMDIM、CPC、SimCLR、BYOL、Swav 等</p><p id="2302" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们最近的论文中，我们提出了一个概念框架来描述对比自我监督学习方法。我们使用我们的框架分析了这些领先方法的三个例子，SimCLR、CPC、AMDIM，并表明尽管这些方法表面上看起来不同，但实际上它们都是彼此的细微调整。</p><p id="0a82" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇博客中，我们将:</p><ul class=""><li id="1286" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated">复习自我监督学习。</li><li id="990a" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">复习对比学习。</li><li id="a515" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">提出一个框架来比较最近的方法。</li><li id="71cb" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">使用我们的框架比较 CPC、AMDIM、MOCO、SimCLR 和 BYOL。</li><li id="f637" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">使用我们的框架制定一个新的方法— YADIM。</li><li id="0263" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">描述一下我们的一些结果。</li><li id="8a8e" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">描述实现这些结果的计算要求。</li></ul><p id="25a6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这项工作的大部分是在脸书人工智能研究所进行的。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="9ea2" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">履行</h1><p id="0c53" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">您可以发现我们在本文中描述的所有增强和方法都在<a class="ae lr" href="https://github.com/PyTorchLightning/pytorch-lightning" rel="noopener ugc nofollow" target="_blank"> PyTorch Lightning </a>中实现，这将允许您在任意硬件上进行训练，并使每种方法的并排比较更加容易。</p><p id="9512" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#amdim" rel="noopener ugc nofollow" target="_blank"> AMDIM </a></p><p id="f850" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#byol" rel="noopener ugc nofollow" target="_blank"> BYOL </a></p><p id="da50" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#cpc-v2" rel="noopener ugc nofollow" target="_blank"> CPC V2 </a>(据我们所知，仅验证了 DeepMind 之外的实现)。</p><p id="889b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">V2 莫科</p><p id="e045" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#simclr" rel="noopener ugc nofollow" target="_blank"> SimCLR </a></p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="49ba" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">自我监督学习</h1><p id="ccd5" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">回想一下在<em class="lz">监督学习</em>中，系统被给定输入(x)和标签(y)，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/e9e4eeba88a4d1ec0d796efdf7da4fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7azDYTXhM9v3yZJ7OszvxQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">监督学习:左边是输入，右边是标签。</p></figure><p id="bdb0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在<strong class="kx ir"> <em class="lz">自我</em></strong><em class="lz">——监督学习中，</em>系统只给出(x)。代替 a (y)，系统“学习从其输入的其他部分预测其输入的一部分”<a class="ae lr" href="https://twitter.com/ylecun/status/1123235709802905600?s=20" rel="noopener ugc nofollow" target="_blank">参考</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/edc680781342fd038b4bdcade3628d28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cv1qXwpfh_7JMULGIV3DOw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在自我监督学习中，输入被用作源和目标</p></figure><p id="ff55" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">事实上，这个公式是如此的通用，以至于你可以创造性地“分割”输入。这些策略被称为借口任务，研究人员已经尝试了各种方法。这里有三个例子:(1) <a class="ae lr" href="https://arxiv.org/abs/1505.05192" rel="noopener ugc nofollow" target="_blank">预测两个补丁的相对位置</a> , (2) <a class="ae lr" href="https://arxiv.org/abs/1603.09246" rel="noopener ugc nofollow" target="_blank">解决拼图游戏</a> , (3) <a class="ae lr" href="https://richzhang.github.io/colorization/" rel="noopener ugc nofollow" target="_blank">给图像着色</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/f482bf4b1b897de5d5a3a7710e44de8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mfx_cfrwJYlB8wHHMqVZVA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">借口任务的示例</p></figure><p id="fa05" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">尽管上面的方法充满了创造性，但它们在实践中并不奏效。然而，最近一系列使用<em class="lz">对比学习</em>的方法实际上已经开始显著缩小 ImageNet 上监督学习之间的差距。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/bddb311138098a8d09053ea0b9d5e68c.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*JW4M9x1CCxL6aLL06NuZQg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最新的方法(Swav)正在缩小在 ImageNet 上训练的监督变异的差距</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="7c13" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">对比学习</h1><p id="c311" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">大多数机器学习算法背后的一个基本思想是，相似的例子应该被分组在一起，并且远离其他相关例子的集群。</p><p id="f837" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种想法是关于对比学习的最早作品之一的背后，Chopra 等人在 2004 年<a class="ae lr" href="http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf" rel="noopener ugc nofollow" target="_blank">有区别地学习相似性度量，并应用于人脸验证</a>。</p><p id="7b5f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的动画说明了这一主要思想:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/270c6f68b08079483d7333e171622043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YXiAuTJvZyHgIpl8s_D6mg.gif"/></div></div></figure><p id="cde7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对比学习通过使用三个关键要素来实现这一点，即积极、锚定和消极(s)表征。为了创建一个正对，我们需要两个相似的例子，而对于一个负对，我们使用第三个不相似的例子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/1a2c3c7c93654f5f6da2ae851c133119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*pwIufoZNu2wanqtBQdIrQQ.gif"/></div></div></figure><p id="3d62" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是在自我监督学习中，<strong class="kx ir">我们不知道例子的标签</strong>。所以，没有办法知道两个图像是否相似。</p><p id="b074" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，如果我们假设<strong class="kx ir">每个图像是它自己的类别</strong>，那么我们可以想出各种方式来形成这些<em class="lz">三元组</em>(正负对)。这意味着在一个大小为 N 的数据集中，我们现在有 N 个标签！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/2c88bf2051d94e80dcfc6f5e62dd97b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7J-62ynCBrIu0mN5i4HN3g.png"/></div></div></figure><p id="66d3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们知道了每张图片的标签，我们可以使用数据扩充来生成这些三元组。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="2f88" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">特征 1:数据增强管道</h1><p id="db81" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">我们可以表征对比自我监督学习方法的第一种方式是通过定义数据扩充管道。</p><p id="f6c0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个<em class="lz">数据扩充管道</em> <strong class="kx ir"> A(x) </strong>将一系列随机变换应用于相同的输入。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/a3cf9a3dfbaa2ea4cf65faf1a7c4dd8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*cQ492d9IzTp6XFV4z-wR6g.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">应用于输入的随机数据扩充管道</p></figure><p id="f1e9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在深度学习中，数据扩充旨在构建对原始输入中的噪声不变的表示。例如，网络应该将上面的猪识别为猪，即使它被旋转，或者如果颜色消失，或者即使像素周围“抖动”。</p><p id="9d0a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在对比学习中，数据扩充管道有一个次要目标，即生成锚、正面和负面示例，这些示例将被提供给编码器，并用于提取表示。</p><p id="16c7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> CPC 管道</strong></p><p id="7d67" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">CPC 引入了一个应用变换的管道，如颜色抖动、随机灰度、随机翻转等，但它也引入了一个特殊的变换，将图像分割成重叠的子块。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f92ad67e8b9e3b05505c6cfa14d3e2ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*mUyVsDVGODuzQ3YZUClt0Q.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">关键 CPC 转换</p></figure><p id="ec21" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用此管道，CPC 可以生成多组正样本和负样本。在实践中，这一过程应用于一批样品，其中我们可以使用该批样品中的其余样品作为阴性样品。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/72c61db452550641b073ae59e46feacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*CK1JIe6XP_JWuz9fWHJ8Lw.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从一批图像中生成正对、锚对和负对。(批量= 3)。</p></figure><p id="2774" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> AMDIM 管道</strong></p><p id="ab91" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">AMDIM 采用了一种稍微不同的方法。在执行标准变换(抖动、翻转等)后，它通过对同一图像应用两次数据扩充管道来生成图像的两个版本。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/04322151a20f2c17553a0f256a31d1d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*BNeFdhczx9Nw9xYo2iSu_Q.gif"/></div></div></figure><p id="8c01" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个想法实际上是由 Dosovitski 等人在 2014 年通过<a class="ae lr" href="https://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">本文</a>提出的。这个想法是使用一个“种子”图像来生成同一图像的多个版本。</p><p id="47d1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">西姆 CLR、Moco、Swav、BYOL 管道</strong></p><p id="1bba" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">AMDIM 中的管道工作得如此之好，以至于随后的每种方法都使用相同的管道，但对之前发生的转换进行了细微的调整(有些添加了抖动，有些添加了高斯模糊，等等)。然而，与 AMDIM 中引入的主要思想相比，这些变换中的大多数都是无关紧要的。</p><p id="0c59" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的论文中，我们对这些变换的影响进行了评估，发现变换的选择对该方法的性能至关重要。事实上，我们认为这些方法的成功主要是由特定的转换选择所驱动的。</p><p id="c084" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些发现与西姆 CLR 和 BYOL 公布的类似结果一致。</p><p id="846c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的视频更详细地说明了 SimCLR 管道。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="5e9e" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated"><strong class="ak">特征二:编码器</strong></h1><p id="2615" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">我们表征这些方法的第二种方式是编码器的选择。上述大多数方法使用各种宽度和深度的网。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/4a1d6c8486513231556dcc169d92542a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hTK1Kj8_LOSkWWcH.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ResNet 架构(鸣谢:<a class="ae lr" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">原 ResNet 作者</a>)</p></figure><p id="aba5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当这些方法开始出现时，CPC 和 AMDIM 实际上设计了定制编码器。我们的消融发现，AMDIM 不能很好地概括，而 CPC 则较少受到编码器变化的影响。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/cde58264df78ab5689fc679ebf80d3cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*s-d9oQmljvQxlSx0W7NVBA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CIFAR-10 上的编码器鲁棒性</p></figure><p id="401b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">自 CPC 以来，每种方法都采用 ResNet-50。虽然我们可能还有更好的架构有待发明，但在 ResNet-50 上实现标准化意味着我们可以专注于改善其他特征，从而通过更好的培训方法而不是更好的架构来推动改进。</p><p id="2c21" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个发现确实适用于每一次消融，<strong class="kx ir">更宽的编码器</strong>在对比学习中表现得更好。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="3217" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated"><strong class="ak">特征 3:表示提取</strong></h1><p id="bb4c" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">表征这些方法的第三种方式是通过它们用来提取表示的策略。这可以说是所有这些方法中“神奇”的地方，也是它们最不同的地方。</p><p id="6472" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了理解为什么这很重要，让我们首先定义一下我们所说的表示是什么意思。一个表示是一组<strong class="kx ir">独特的特征</strong>，允许系统(和人类)理解是什么制造了那个对象，那个对象，而不是一个不同的对象。</p><p id="032d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://www.quora.com/What-is-representation-learning-in-deep-learning" rel="noopener ugc nofollow" target="_blank">这篇 Quora 帖子</a>使用了一个尝试对形状进行分类的例子。为了成功地对形状进行分类，一个好的表示可能是在该形状中检测到的角的数量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/0b304578c800709f26c16e292d981de4.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*H7_AsqYomhBj40qYnFG4rw.png"/></div></figure><p id="0a8a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个对比学习方法的集合中，这些表征以不同的方式被提取出来。</p><p id="5da4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> CPC </strong></p><p id="6361" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">CPC 引入了通过预测<em class="lz">潜在</em>空间中的“未来”来学习表征的思想。实际上，这意味着两件事:</p><p id="2b66" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">1)将一幅图像视为一条时间线，过去在左上方，未来在右下方。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/abdb1659f6e181144cbdada3129485f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*NJjen7TZnzPdFEQntgyiiw.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CPC“未来”预测任务</p></figure><p id="dc7d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2)预测不会发生在像素级别，而是使用编码器的输出(即:潜在空间)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/57d9622fb0186b73e4ccf253ccc9a4e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*IXPgYjFl28PUDq0JJFXjag.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从像素空间到潜在空间</p></figure><p id="d537" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，通过使用编码器(H)的输出作为投影头(作者称之为上下文编码器)生成的上下文向量的目标来制定预测任务，来进行表示提取。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/596e8406ad986e661af06a6fba1b3518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*sFCOp2-9yAQCu3D6L_JH6w.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CPC 表示提取</p></figure><p id="d957" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我们发现，只要数据扩充管道足够强大，这种预测任务是不必要的。虽然有很多关于什么是好的管道的假设，但我们认为，一个强大的管道会产生积极的配对，它们共享相似的全球结构，但具有不同的局部结构。</p><p id="4a00" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> AMDIM </strong></p><p id="e71c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一方面，AMDIM 使用从卷积神经网络(CNN)的中间层提取的特征图的视图间比较表示的思想。让我们把它分成两个部分，1)图像的多个视图，2)CNN 的中间层。</p><p id="43c0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">1)回想一下，AMDIM 的数据扩充管道生成同一图像的两个版本。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/ac549cfb64200a9acb47fe064c0422dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*LIta0cC8OC7anCJK1-fFpA.gif"/></div></figure><p id="567f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2)将每个版本传递到相同的编码器中，以提取每个图像的特征图。AMDIM 不会丢弃编码器生成的中间特征图，而是使用它们在<em class="lz">空间尺度</em>上进行比较。回想一下，当一个输入通过 CNN 的各层时，感受野对不同尺度的输入信息进行编码。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/e2b84a7740408388a502aa913692818a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tyUQ7rmxuY-MmEz7.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">参考文献<a class="ae lr" href="https://arxiv.org/pdf/1311.2901.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a><a class="ae lr" href="https://www.researchgate.net/figure/Hierarchical-representation-learning-by-a-Convolutional-Neural-Network-where-the-initial_fig4_317558591" rel="noopener ugc nofollow" target="_blank">【2】</a>。</p></figure><p id="6197" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">AMDIM 通过对 CNN 的中间输出进行比较来利用这些想法。下面的动画演示了如何对编码器生成的三个特征图进行比较。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/7edd860af0a1a64a1309f9e3e4cfaa64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*5FDDMTCYd2kr_pjCqg0K1Q.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oa"> AMDIM 表示提取</strong> : AMDIM 使用相同的编码器提取 3 组特征图。然后，它在特征地图之间进行比较。</p></figure><p id="ea41" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些方法的其余部分对 AMDIM 提出的想法进行了细微的调整。</p><p id="41a1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> SimCLR </strong></p><p id="c731" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">SimCLR 使用与 AMDIM 相同的思想，但是做了 2 处调整。</p><p id="13df" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">a)仅使用最新的特征地图</p><p id="2d53" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">b)通过投影头运行特征图，并比较两个向量(类似于 CPC 上下文投影)。</p><p id="e09c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> Moco </strong></p><p id="7db2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我们前面提到的，对比学习需要反面样本才能起作用。通常，这是通过将一批中的图像与一批中的其他图像进行比较来完成的。</p><p id="c346" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Moco 做了与 AMDIM 相同的事情(仅使用最近的特征图)，但是保留了它所看到的所有批次的历史，并且增加了阴性样本的数量。其效果是，用于提供对比信号的阴性样本的数量增加，超过了单个批次的大小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b371ccffe4c10e39fbb0355e33f55815.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/0*p-hW71amm3BCzbG-.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">鸣谢:Moco 原创作者。(<a class="ae lr" href="https://github.com/facebookresearch/moco" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="471f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> BYOL </strong></p><p id="0e82" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用与 AMDIM 相同的主要思想(但是只有最后的特性图)，但是有两个变化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/e441544abb0af5dc723200c0567be7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Vrek1qWENNxYfEmy.jpg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">鸣谢:Deepmind ( <a class="ae lr" href="https://twitter.com/deepmind/status/1272810643222126594" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><ol class=""><li id="9142" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq od mg mh mi bi translated">BYOL 使用两个编码器，而不是一个。第二个编码器实际上是第一个编码器的精确副本，但它不是在每次通过时更新权重，而是在滚动平均值上更新权重。</li><li id="7f57" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq od mg mh mi bi translated">BYOL 不使用阴性样本。而是依赖滚动权重更新作为向训练给出对比信号的方式。然而，<a class="ae lr" href="https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning.html" rel="noopener ugc nofollow" target="_blank">最近的一次消融</a>发现这可能是不必要的，事实上添加批量标准化是为了确保系统不会生成琐碎的解决方案。</li></ol><p id="ae7a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> Swav </strong></p><p id="acdf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将他们的表示提取任务框定为“在线聚类”之一，其中他们强制“来自同一图像的不同增强的代码之间的一致性”【<a class="ae lr" href="https://arxiv.org/pdf/2006.09882.pdf" rel="noopener ugc nofollow" target="_blank">参考</a>】。因此，这与 AMDIM 的方法相同(仅使用最后的特征图)，但他们不是直接比较向量，而是计算一组 K 个预计算代码的相似性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/31b9fb538447b46287902b8342507869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*9ArjSRD6c1oqg0qonEOYrQ.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">鸣谢:Swav 作者(<a class="ae lr" href="https://ai.facebook.com/blog/high-performance-self-supervised-image-classification-with-contrastive-clustering/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="735c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在实践中，这意味着 Swav 生成 K 个簇，并且对于每个编码的向量，它与这些簇进行比较以学习新的表示。这项工作可以被视为混合了 AMDIM 和<a class="ae lr" href="https://arxiv.org/abs/1704.05310" rel="noopener ugc nofollow" target="_blank"> Noise 作为目标的思想。</a></p><p id="f472" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">特点 3，外卖</strong></p><p id="2136" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些方法的不同之处在于表示提取策略。然而，这些变化是非常微妙的，没有严格的消融，很难判断到底是什么驱动了结果。</p><p id="19cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从我们的实验中，我们发现 CPC 和 AMDIM 策略对结果的影响可以忽略不计，反而增加了复杂性。使这些方法起作用的主要驱动力是数据扩充管道。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="0639" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated"><strong class="ak">特征 4:相似性度量</strong></h1><p id="3719" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">我们可以用来比较这些方法的第四个特征是它们使用的相似性度量。以上所有方法都使用点积或余弦相似度。虽然我们的论文没有列出这些消融，但我们的实验表明，相似性的选择在很大程度上是无关紧要的。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="59f7" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">特征 5:损失函数</h1><p id="d20d" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">我们用来比较这些方法的第五个特征是损失函数的选择。所有这些方法(除了 BYOL)都集中在使用 NCE 损失。NCE 损失有两部分，分子和分母。分子鼓励相似的向量靠近，分母推动所有其他向量远离。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/270c6f68b08079483d7333e171622043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YXiAuTJvZyHgIpl8s_D6mg.gif"/></div></div></figure><p id="d22a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果没有分母，损耗很可能变成常数，因此所学的表示法将没有用。</p><p id="8e52" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，BYOL 不再需要分母，而是依靠对第二编码器的加权更新来提供对比信号。然而，如前所述，<a class="ae lr" href="https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning.html" rel="noopener ugc nofollow" target="_blank">最近的消融</a>表明这实际上可能不是对比信号的驱动因素。</p><p id="8c4a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本视频中，我以 SimCLR 为例，对 NCE 损耗进行了全面解释。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="b83b" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">又一个 DIM(亚迪姆)</h1><p id="26ec" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">我们希望通过生成一种新的自我监督学习方法来展示我们框架的有用性，这种方法没有借口动机或涉及的表征提取策略。我们称这种新方法为另一种 DIM (YADIM)。</p><p id="962e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">YADIM 的特点如下:</p><p id="ba03" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">特点一:数据增强管道</strong></p><p id="b4b3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于 YADIM，我们合并了 CPC 和 AMDIM 的管道。</p><p id="0cda" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">特点二:编码器</strong></p><p id="70c1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们使用 AMDIM 的编码器，尽管任何编码器如 ResNet-50 也可以工作</p><p id="4bea" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">特征 3:表示提取</strong></p><p id="cfc0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">亚迪姆策略很简单。对图像的多个版本进行编码，并使用最后的特征图进行比较。没有投影头或其他复杂的比较策略</p><p id="b6e3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">特征 4:相似性度量</strong></p><p id="3827" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于 YADIM，我们坚持点积</p><p id="5052" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">特性 5:损失函数</strong></p><p id="b522" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们也使用 NCE 损失。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="8983" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">YADIM 结果</h1><p id="738e" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">尽管我们唯一有意义的选择是合并 AMDIM 和 CPC 的管道，但与其他方法相比，YADIM 仍然做得很好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/53399ffa8d548ba022777d9ffbf70c97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*GeHywGRLY3grE5X18A4qrw.png"/></div></figure><p id="9bff" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与所有相关的方法不同，我们通过实际实现每种方法来生成上面的结果。事实上，据我们所知，我们对 CPC V2 的实现是 DeepMind 之外的首次公开实现。</p><p id="3b70" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">更重要的是，我们使用 PyTorch Lightning 来标准化所有实现，这样我们就可以客观地提炼出上述结果的主要驱动因素。</p><p id="bece" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">计算效率</strong></p><p id="0109" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上述方法是使用大量计算资源训练的。高昂的成本意味着我们没有进行严格的超参数搜索，而是简单地使用 STL-10 的超参数在 ImageNet 上进行训练。</p><p id="4ebd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用 PyTorch Lightning 来有效地分配计算，我们能够通过 ImageNet 使用 16 位精度将一个时期降低到每个时期大约 3 分钟。</p><p id="581e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些是我们用于每种方法的计算资源</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/c28b5704925d0caf2752bdaead685b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*4ZvPYO96oLZVEzl4lsmsLg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基于每小时 31.212 美元的 23dn . 24x 大型实例</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/c3531bc3881315af6c3d30fdc027240c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*DhRYJGva8gKUIwrAqOhFdQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基于每小时 31.212 美元的 23dn . 24x 大型实例</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/5b0cdacb2b465f246216510e99200437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*BUSPadI6iCVopMfsGIrslA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基于每小时 31.212 美元的 23dn . 24x 大型实例</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/475dfeb2c04ef00377214fb5161ae06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*B9HwDA3fzdeDb8UO5VPkZA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基于每小时 31.212 美元的 23dn . 24x 大型实例</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="55ea" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">关键要点</h1><ol class=""><li id="0bbd" class="ma mb iq kx b ky ng lb nh le ok li ol lm om lq od mg mh mi bi translated">我们引入了一个概念框架来比较和更容易地设计对比学习方法。</li><li id="8232" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq od mg mh mi bi translated">AMDIM、CPC、SimCLR、Moco、BYOL 和 Swav 在细微方面彼此不同。主要的区别在于他们如何提取表示。</li><li id="2e60" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq od mg mh mi bi translated">AMDIM 和 CPC 介绍了其他方法使用的主要关键思想。SimCLR、Moco、BYOL 和 Swav 可以被视为 AMDIM 的变体。</li><li id="3c39" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq od mg mh mi bi translated">编码器的选择并不重要，只要它是宽的。</li><li id="a794" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq od mg mh mi bi translated">只要数据扩充管道生成良好的正面和负面输入，表示提取策略就无关紧要。</li><li id="b9a8" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq od mg mh mi bi translated">使用我们的框架，我们可以制定新的 CSL 方法。我们设计了 YADIM(另一个 DIM ),作为一个可以与竞争对手的方法相媲美的例子。</li><li id="c301" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq od mg mh mi bi translated">训练这些方法的成本意味着世界上只有少数研究小组能够继续取得进展。尽管如此，我们以一种标准化的方式发布所有这些算法至少缓解了实现这些算法和验证这些实现的问题。</li><li id="9fd4" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq od mg mh mi bi translated">由于大多数结果是由更广泛的网络和特定的数据增强管道驱动的，我们怀疑当前的研究路线可能有有限的改进空间。</li></ol></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="00f4" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">感谢</h1><p id="1336" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">正如我们在论文中提到的，我要感谢 CPC、AMDIM 和 BYOL 的一些作者进行了有益的讨论。</p><p id="cd6a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这项工作的大部分是在脸书人工智能研究所进行的。没有公平的计算资源，消融和长的训练时间是不可能的。</p><p id="e8d2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我还要感谢费尔和 NYU CILVR 的同事们，他们进行了有益的讨论，他们是 Stephen Roller、Margaret Li、Tullie、Cinjon Resnick、Ethan Perez、Shubho Sengupta 和 Soumith Chintala。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="a034" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">PyTorch 闪电</h1><p id="0391" class="pw-post-body-paragraph kv kw iq kx b ky ng jr la lb nh ju ld le ni lg lh li nj lk ll lm nk lo lp lq ij bi translated">此外，这恰好也是创建<a class="ae lr" href="https://github.com/PyTorchLightning/pytorch-lightning" rel="noopener ugc nofollow" target="_blank"> PyTorch Lightning </a>的主要原因之一，PyTorch Lightning 是一种利用大量计算资源快速迭代想法的技术，而不会陷入训练这种规模的模型所需的所有工程细节中。</p><p id="23a5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我要感谢我的顾问 Kyunghyun Cho 和 Yann LeCun 在平行建造 PyTorch Lightning 的同时进行这项研究的耐心。</p></div></div>    
</body>
</html>