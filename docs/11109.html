<html>
<head>
<title>Why Dropout is so effective in Deep Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么辍学在深度神经网络中如此有效</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-dropout-to-regularize-deep-neural-network-8e9d6b1d4386?source=collection_archive---------23-----------------------#2020-08-02">https://towardsdatascience.com/introduction-to-dropout-to-regularize-deep-neural-network-8e9d6b1d4386?source=collection_archive---------23-----------------------#2020-08-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8692" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">数据科学，深度学习</h2><div class=""/><div class=""><h2 id="8bdc" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">辍学是减少深层神经网络依赖性的一种简单方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/42b352e753f104a9e385cbeb8695836d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G7d49KM9DcxPDysK"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">艾莉娜·格鲁布尼亚克在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7efb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文中，您可以探索 dropout，正则化 vs dropout 的优缺点是什么，Dropout 方法在深度学习中如何工作，在深度学习中使用 Dropout 的有效方法是什么，以及如何在深度神经网络中实现 Dropout？</p><h1 id="98c8" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">什么是深度学习的辍学生？</strong></h1><p id="facb" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><strong class="lk jd"> Dropout </strong>意思是在神经网络中，去掉被掩盖的、明显的单元。在神经网络中，放弃是一种非常流行的克服过度拟合的方法。</p><p id="a7ed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">深度学习</strong>框架现在越来越深入。有了这些更大的网络，我们可以实现更好的预测准确性。然而，几年前却不是这样。深度学习有一个过度适应的问题。</p><p id="4ff0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那时，大约在 2012 年，Hinton 在他们的论文中提出了通过在训练过程的每次迭代中随机排除特征子集来退出的想法。这个概念彻底改变了深度学习。我们在深度学习方面取得的成就有很大一部分要归功于辍学。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/293691e3d293897e31ce9c9a08792478.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0L5S1nEeEmHj-u9i"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@herfrenchness?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Clarisse Croset </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="a08f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">转正 vs 退学的利弊？</strong></h1><p id="b732" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在<strong class="lk jd">辍学</strong>之前，一个重要的研究领域是<strong class="lk jd">正规化</strong>。在神经网络中引入正则化方法，例如 L1 和 L2 权重罚值，始于 2000 年代中期。尽管如此，这些调整并没有完全解决过度拟合的问题。</p><p id="1de3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Wager 等人在他们 2013 年的论文中指出，在学习特征权重方面，辍学正则化优于 L2 正则化。</p><h1 id="5c68" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">dropout 方法在深度学习中是如何工作的？</strong></h1><p id="6c63" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">Dropout 是一种在训练过程中丢弃随机选择的神经元的方法。他们被任意“退学”。这意味着它们对下游神经元激活的贡献在正向传递中暂时消失，任何权重刷新都不会应用于反向传递中的神经元。</p><p id="e615" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以想象，如果神经元在训练过程中偶然退出网络，另一个神经元将不得不介入并处理对缺失神经元进行预测所需的描述。这被认为会导致网络学习各种独立的内部表示。</p><p id="916f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管辍学已经成为一种非常成功的技术，但其成功的原因在理论层面上还没有得到很好的理解。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/95eae88e19132222ca46513cecf7da98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/0*VkwMmu7uPYQXn1Q9"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:自己的工作</p></figure><p id="022e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看到标准的前馈传递:权重乘以输入，加上偏差，并将其传递给激活函数。等式的第二种排列方式阐明了如果我们让学生退学会是什么样子:</p><ul class=""><li id="5afd" class="nd ne it lk b ll lm lo lp lr nf lv ng lz nh md ni nj nk nl bi translated">生成删除掩码:伯努利随机变量(示例 1.0 *(NP . random . random((size))&gt; p)</li><li id="6de0" class="nd ne it lk b ll nm lo nn lr no lv np lz nq md ni nj nk nl bi translated">使用屏蔽来断开一些神经元的输入。</li><li id="fae2" class="nd ne it lk b ll nm lo nn lr no lv np lz nq md ni nj nk nl bi translated">利用这个新的层来增加权重和偏差</li><li id="b9dc" class="nd ne it lk b ll nm lo nn lr no lv np lz nq md ni nj nk nl bi translated">最后，使用激活功能。</li></ul><p id="2c5e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所有的权重在潜在的指数数量的网络上共享，并且在反向传播期间，只有“稀疏网络”的权重将被刷新。</p><h1 id="3438" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">深度学习中使用 Dropout 的有效方法有哪些？</strong></h1><p id="3ff5" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">●根据(Srivastava，2013) Dropout，神经网络可以与随机梯度下降一起训练。对于每个迷你批次中的每个训练案例，独立完成退出。辍学可以利用任何激活功能，他们的实验与逻辑，双曲正切，矫正线性单位产生了可比的结果，但需要不同的训练时间，矫正线性单位是最快的训练。</p><p id="a9df" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">● Kingma 等人，2015 年推荐的辍学要求指出辍学率，即放弃一个神经元的概率。辍学率通常利用网格搜索进行优化。此外，作为贝叶斯正则化的一个特殊例子，变分丢失是高斯丢失的一个精致的翻译。这种方法允许我们调整辍学率，并且原则上可以用于为每一层、神经元甚至权重设置单独的辍学率。</p><p id="b5f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">●由(Ba 等人，2013)进行的另一个实验，增加深度学习算法中的隐藏单元的数量。辍学正则化的一个值得注意的事情是，它实现了大量隐藏单元的相当普遍的性能，因为所有单元都有相等的概率被排除。</p><h1 id="fd03" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">深度神经网络如何实现 dropout？</h1><p id="88a0" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">●通常，利用 20%-50%的神经元的小丢弃值，其中 20%提供了一个很好的起点。太低的概率具有不显著的影响，并且在系统的欠学习中价值太高的结果。</p><p id="7e34" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">●当 dropout 在一个更大的网络中使用时，你可能会显示出改进执行的迹象，允许模型有更多的机会学习免费的描述。</p><p id="6bbe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">●在接近(明显的)时使用叉头，就像隐蔽单元一样。在系统的每一层利用辍学已经证明了巨大的成果。</p><h1 id="189f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">参考</h1><p id="a46a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">* n . Srivastava，g . hint on，a . krijevsky，Sutskever，I .和 r . Salakhutdinov，2014 年。辍学:防止神经网络过度拟合的简单方法。《机器学习研究杂志》，第 15 卷第 1 期，第 1929-1958 页。</p><p id="e934" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">* hint on，G.E .，n . Srivastava，Krizhevsky，a .，Sutskever，I .和 Salakhutdinov，R.R .，2012 年。通过防止特征检测器的共同适应来改进神经网络。arXiv 预印本 arXiv:1207.0580。</p><p id="9047" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">●韦杰，王，梁，2013。作为适应性调整的辍学训练。神经信息处理系统的进展(第 351-359 页)。</p><p id="eb0e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">●北卡罗来纳州斯利瓦斯塔瓦，2013 年。用辍学改进神经网络。多伦多大学，182(566)，第 7 页</p><p id="631f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">●金玛博士、t .萨利曼斯和 m .韦林，2015 年。变分丢失和局部重新参数化技巧。神经信息处理系统进展(第 2575-2583 页)。</p><p id="f175" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">巴和弗雷分别于 2013 年出版。用于训练深度神经网络的自适应丢失。神经信息处理系统进展(第 3084-3092 页)。</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="02a3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，把你的想法放到 Twitter、Linkedin 和 Github 上吧！！</p><p id="e448" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">同意或不同意 Saurav Singla 的观点和例子？想告诉我们你的故事吗？</p><p id="687f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">推文<a class="ae lh" href="https://twitter.com/SAURAVSINGLA_08" rel="noopener ugc nofollow" target="_blank"> @SauravSingla_08 </a>，评论<a class="ae lh" href="http://www.linkedin.com/in/saurav-singla-5b412320" rel="noopener ugc nofollow" target="_blank"> Saurav_Singla </a>，明星<a class="ae lh" href="https://github.com/sauravsingla" rel="noopener ugc nofollow" target="_blank"> SauravSingla </a>马上！</p></div></div>    
</body>
</html>