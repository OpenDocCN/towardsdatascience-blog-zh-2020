<html>
<head>
<title>Get Your Decision Tree Model Moving by CART</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让您的决策树模型随车移动</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/get-your-decision-tree-model-moving-by-cart-82765d59ae09?source=collection_archive---------26-----------------------#2020-09-13">https://towardsdatascience.com/get-your-decision-tree-model-moving-by-cart-82765d59ae09?source=collection_archive---------26-----------------------#2020-09-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1b7114a3df968d5278e41468296a5622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b4a6zOj9aK0CuWXKay0gHg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由<a class="ae jg" href="https://pixabay.com/photos/shopping-toilet-paper-covid-19-4974313/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>上的<a class="ae jg" href="https://pixabay.com/users/Alexas_Fotos-686414/" rel="noopener ugc nofollow" target="_blank"> Alexas_Fotos </a>拍摄的照片</p></figure><div class=""/><div class=""><h2 id="d235" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">基尼杂质——另一种决策树节点划分标准</h2></div><p id="3c5e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作为最流行的经典机器学习算法之一，决策树在可解释性方面比其他算法更直观。在我以前的文章中，我介绍了用于构建决策树模型的 ID3 和 C4.5 算法。</p><ul class=""><li id="a699" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/go-out-for-exercise-or-not-let-data-science-decide-34f8f28ce7b4"> <strong class="la jk">决策树基础知识和 ID3 算法</strong> </a></li></ul><div class="is it gp gr iu md"><a rel="noopener follow" target="_blank" href="/go-out-for-exercise-or-not-let-data-science-decide-34f8f28ce7b4"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jk gy z fp mi fr fs mj fu fw ji bi translated">出去锻炼还是不锻炼？让数据科学来决定</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">决策树机器学习算法简介</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><ul class=""><li id="1625" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/do-not-use-decision-tree-like-this-369769d6104d"> <strong class="la jk"> ID3 的弊端和 C4.5 算法</strong> </a></li></ul><div class="is it gp gr iu md"><a rel="noopener follow" target="_blank" href="/do-not-use-decision-tree-like-this-369769d6104d"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jk gy z fp mi fr fs mj fu fw ji bi translated">不要像这样使用决策树</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">展示 ID3 中信息获取的局限性以及使用 C4.5 的优势</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="ms l mo mp mq mm mr ja md"/></div></div></a></div><p id="a584" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将介绍另一种叫做<strong class="la jk"> CART </strong>的算法，用于构建决策树模型，这可能也是最常用的。顺便说一句，当你使用 Scikit-Learn 库作为决策树分类器时，这是默认的算法。</p><h1 id="c4d0" class="mt mu jj bd mv mw mx my mz na nb nc nd kp ne kq nf ks ng kt nh kv ni kw nj nk bi translated">什么是 CART 算法？</h1><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1b355e0efc18a7646454d7ee3724892b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ikLQqm7-UQQ-hCYgeEZ_zQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://pixabay.com/users/683440-683440/" rel="noopener ugc nofollow" target="_blank"> 683440 </a>在<a class="ae jg" href="https://pixabay.com/photos/tree-tribe-forest-arrows-brown-746617/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>上拍摄的照片</p></figure><p id="394b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">CART 算法是<strong class="la jk"> C </strong>分类<strong class="la jk">A</strong>d<strong class="la jk">R</strong>回归<strong class="la jk"> T </strong> rees 的缩写。它是由 Breiman 等人在 1984 年发明的[1]。</p><p id="fb79" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它通常与 C4.5 非常相似，但具有以下主要特征:</p><ol class=""><li id="4c22" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt np ma mb mc bi translated">与可能有多个分支的一般树不同，CART 使用二叉树，每个节点只有两个分支。</li><li id="40b5" class="lu lv jj la b lb nq le nr lh ns ll nt lp nu lt np ma mb mc bi translated">CART 使用 Gini 杂质作为划分节点的标准，而不是信息增益。</li><li id="923a" class="lu lv jj la b lb nq le nr lh ns ll nt lp nu lt np ma mb mc bi translated">CART 支持数字目标变量，这使得它能够成为预测连续值的回归树。</li></ol><p id="772d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文将把重点放在 CART 作为分类树上。</p><h1 id="d16f" class="mt mu jj bd mv mw mx my mz na nb nc nd kp ne kq nf ks ng kt nh kv ni kw nj nk bi translated">基尼杂质</h1><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/a3e67b5f9bd3be470722ffac19dbd411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IW4QPkPOT6ommKWj4Fukiw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://pixabay.com/users/qimono-1962238/" rel="noopener ugc nofollow" target="_blank">奇莫诺</a>在<a class="ae jg" href="https://pixabay.com/photos/water-drop-liquid-splash-wet-1761027/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>上拍摄</p></figure><p id="5eed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就像 ID3 和 C4.5 算法依赖信息增益作为分割节点的标准一样，CART 算法使用另一个称为 Gini 的标准来分割节点。</p><p id="9872" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你学过经济学，你一定熟悉基尼指数，它表明一个国家或任何其他人群内部的收入不平等或财富不平等[2]。</p><p id="ff3c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 CART 算法中，出于类似的目的，直观地使用基尼系数。即基尼系数越大，意味着节点的杂质越大。类似于 ID3 和 C4.5 使用信息增益选择不确定性较大的节点，基尼系数会引导 CART 算法找到不确定性较大(即杂质)的节点，然后进行分裂。</p><p id="3e63" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基尼系数的公式比我们在其他两个决策树算法中使用的信息增益和信息增益比率相对简单。如下图所示。</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/f9c5e3b98b6c29bb714e448b70a0dcb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jRf8Rw3NZewXp3us7aPb8Q.png"/></div></div></figure><ul class=""><li id="2e30" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><em class="nw"> p(Ck|t) </em>是节点<em class="nw"> t </em>成为类别<em class="nw"> Ck </em>的概率。</li><li id="4607" class="lu lv jj la b lb nq le nr lh ns ll nt lp nu lt lz ma mb mc bi translated">节点<em class="nw"> t </em>的基尼系数为 1 减去所有类别的概率之和。</li></ul><p id="f4bf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不要被公式吓到。我们举个例子来演示一下。我保证不难理解。</p><p id="ca6d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面的例子在我写的所有关于决策树的文章中都被使用过。</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/05d03952d836e3f18d22a5d896c6a50a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrAjgSup8YQcdIBElHx31g.png"/></div></div></figure><p id="1c43" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我们想通过使用“天气”特性进行分割来启动决策树。然后，我们需要计算其条件下的基尼系数。让我们关注天气属性和结果，如下表所示。</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/e83e7ae2ef9636e60cc1c29cfa493421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L73Bv471rOrnHzWNRbuwRw.png"/></div></div></figure><p id="5a21" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">计算概率<em class="nw"> p(Ck|t)是相当容易的。</em>例如，如果我们考虑属性“天气=晴朗”,则总共 3 个样本中有 1 个“是”和 2 个“否”。因此，类别“是”的概率是 1/3，类别“否”的概率是 2/3。然后我们可以很容易地计算出基尼系数如下。</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/151426c41d187e2a3802e8e81c51e1dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*thSp4RAIOYqtGJF2_xz3sQ.png"/></div></div></figure><p id="89f6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们从上面 3 个等式中得到的直觉是</p><ul class=""><li id="32e9" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">当天气晴朗时，会有一些不确定性，因为我们可能会出去跑步，也可能不会。</li><li id="324d" class="lu lv jj la b lb nq le nr lh ns ll nt lp nu lt lz ma mb mc bi translated">当天气多云时，我们肯定要出去跑步。完全没有不确定性(100%纯度)</li><li id="7f0e" class="lu lv jj la b lb nq le nr lh ns ll nt lp nu lt lz ma mb mc bi translated">当天气下雨时，我们肯定不会出去跑步。也没有任何不确定性(100%纯度)</li></ul><p id="3a19" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些数字也反映了我们的直觉。天气晴朗的时候基尼系数是 0.444，因为不纯。当天气多云或下雨时，基尼系数为 0。因此，使用属性“天气=多云”和“天气=下雨”来分割节点没有任何意义，因为我们知道决策必须是全是或全否。</p><h1 id="fb4a" class="mt mu jj bd mv mw mx my mz na nb nc nd kp ne kq nf ks ng kt nh kv ni kw nj nk bi translated">CART 算法如何选择根/下一个节点</h1><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f4b9030681ffe7a9ce69aa67c9e55f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vrpr-aS8dImQPV7_htCHmw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由<a class="ae jg" href="https://pixabay.com/users/cocoparisienne-127419/" rel="noopener ugc nofollow" target="_blank"> cocoparisienne </a>在<a class="ae jg" href="https://pixabay.com/photos/root-tree-root-tree-nature-log-276636/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>上拍摄的照片</p></figure><p id="0975" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们如何决定根节点，从而使用相同的逻辑来决定如何分割内部节点？这与使用所有可能的子节点的基尼系数的加权和一样简单。</p><p id="077c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于 CART 算法是利用二叉树的 CART 算法，每次我们只需要计算两个节点的基尼系数的加权和。公式如下。</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/2f28b5ea6355d0032296064b096684a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhtYYzPNBlMLDI8rVOAN3w.png"/></div></div></figure><ul class=""><li id="71ec" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">|t_left|和|t_right|分别是左边和右边节点的样本大小。</li><li id="735d" class="lu lv jj la b lb nq le nr lh ns ll nt lp nu lt lz ma mb mc bi translated">|T|是候选父节点 T 的样本大小</li></ul><p id="59d3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还是那句话，不要被公式吓到。让我们用同样的例子，天气特征。</p><p id="ebb9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们使用天气作为根节点，有 3 种不同的情况:</p><ol class=""><li id="171a" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt np ma mb mc bi translated">除以“晴”和“不晴”，基尼系数= 0.476</li></ol><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/3931967c52df2200d523fb0c883df125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ff1y0oBdDI1bdR8KrDywgQ.png"/></div></div></figure><p id="473a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.除以“多云”和“不多云”，基尼系数= 0.229</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/0f580199faa13c3bfcf3c87374b19853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QVjhpSaPQy5ySeQeP6T3Lw.png"/></div></div></figure><p id="4e79" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.除以“多雨”和“不多雨”，基尼系数= 0.343</p><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/00bde0f7b80dbc752bffced78476b225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N0ugIES3YqJCAboArylhAg.png"/></div></div></figure><p id="63d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样的逻辑将应用于所有其他可能性，直到我们找到最大的基尼系数，这将是根节点。类似地，对于内部节点，应用相同的逻辑来确保每个分裂具有最大的基尼系数。</p><h1 id="4043" class="mt mu jj bd mv mw mx my mz na nb nc nd kp ne kq nf ks ng kt nh kv ni kw nj nk bi translated">摘要</h1><figure class="nl nm nn no gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/105c2642d4c685685e6905ac05d294d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JKEhEbm46nWGX9pJTEPvBQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://pixabay.com/users/geralt-9301/" rel="noopener ugc nofollow" target="_blank"> geralt </a>在<a class="ae jg" href="https://pixabay.com/photos/mark-marker-hand-write-training-589858/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>上拍摄</p></figure><p id="fd55" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我介绍了另一种决策树算法，称为 CART(分类和回归树)。它构造只有两个分支的二叉树。它还支持将连续数值作为目标。CART 最显著的特点是使用基尼系数作为划分节点的标准。</p><div class="is it gp gr iu md"><a href="https://medium.com/@qiuyujx/membership" rel="noopener follow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jk gy z fp mi fr fs mj fu fw ji bi translated">通过我的推荐链接加入 Medium 克里斯托弗·陶</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">medium.com</p></div></div><div class="mm l"><div class="oe l mo mp mq mm mr ja md"/></div></div></a></div><p id="948c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你觉得我的文章有帮助，请考虑加入 Medium 会员来支持我和成千上万的其他作者！(点击上面的链接)</p><h1 id="ed80" class="mt mu jj bd mv mw mx my mz na nb nc nd kp ne kq nf ks ng kt nh kv ni kw nj nk bi translated">参考</h1><p id="9e2b" class="pw-post-body-paragraph ky kz jj la b lb of kk ld le og kn lg lh oh lj lk ll oi ln lo lp oj lr ls lt im bi translated">[1] Breiman，Leo，等.分类和回归树.CRC 出版社，1984 年。</p><p id="7c71" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]基尼系数。维基百科。<a class="ae jg" href="https://en.wikipedia.org/wiki/Gini_coefficient" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Gini_coefficient</a></p></div></div>    
</body>
</html>