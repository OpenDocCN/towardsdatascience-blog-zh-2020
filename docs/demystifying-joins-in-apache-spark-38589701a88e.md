# 揭秘加入 Apache Spark

> 原文：<https://towardsdatascience.com/demystifying-joins-in-apache-spark-38589701a88e?source=collection_archive---------17----------------------->

![](img/5a978aa1f10e757fb7c6c4e60fc67ee6.png)

参赛:ixbay.com

## SPARK 执行指南

## 本文专门讨论 Apache Spark 中的 Join 操作，让您对 Spark Join 技术的基础有一个总体的了解。

在典型的数据分析流程中，经常使用连接操作来关联两个数据集。Apache Spark 作为一个统一的分析引擎，也为执行各种各样的连接场景提供了坚实的基础。

在非常高的级别上，Join 对两个输入数据集进行操作，并且该操作通过将属于一个输入数据集的每个数据记录与属于另一个输入数据集的每个其他数据记录进行匹配来工作。在发现匹配或不匹配(根据给定的条件)时，连接操作可以从两个数据集或连接的记录中输出匹配的单个记录。连接的记录基本上表示来自两个数据集的匹配的单个记录的组合。

**加入操作的重要方面:**

现在让我们了解影响 Apache Spark 中 Join 操作执行的三个重要方面。这些是:

**1)输入数据集的大小**:输入数据集的大小直接影响连接操作的执行效率和可靠性。此外，输入数据集的相对大小会影响连接机制的选择，从而进一步影响连接机制的效率和可靠性。

**2)连接条件**:连接输入数据集所依据的条件或子句称为连接条件。该条件通常涉及属于输入数据集的属性之间的逻辑比较。根据连接条件，连接分为两大类，等价连接和非等价连接。

相等联接涉及一个相等条件或需要同时满足的多个相等条件。每个等式条件应用于来自两个输入数据集的属性之间。例如，(A.x == B.x)或((A.x == B.x)和(A.y == B.y))是参与连接操作的两个输入数据集 A 和 B 的 x，y 属性上相等连接条件的两个示例。

非相等联接不涉及相等条件。但是，它们可能允许多个等式条件不能同时满足。例如，(A.x < B.x)或((A.x == B.x)或(A.y == B.y))是参与连接操作的两个输入数据集 A 和 B 的 x，y 属性上的非相等连接条件的两个示例。

**3)连接类型:**在输入数据集的记录之间应用连接条件后，连接类型会影响连接操作的结果。以下是各种连接类型的大致分类:

*内部连接*:内部连接只输出输入数据集中匹配的连接记录(在连接条件下)。

*外连接*:外连接输出除匹配的连接记录外，还输出不匹配的记录。根据输出不匹配记录的输入数据集的选择，外部连接进一步分为左、右和全外部连接。

*Semi Join* : Semi Join 输出只属于两个输入数据集之一的单个记录，无论是匹配的还是不匹配的实例。如果属于某个输入数据集的记录在不匹配的实例上输出，半连接也称为反连接。

*交叉连接*:交叉连接通过将一个输入数据集中的每条记录与另一个输入数据集中的每条记录相结合，输出所有可能的连接记录。

基于连接执行的上述三个重要方面，Apache Spark 选择正确的机制来执行连接。

**加入执行的各种机制**

在理解了执行连接操作的各个方面之后，现在让我们来理解执行连接操作的各种机制。

Apache Spark 总共提供了五种机制来执行连接操作。这些是:

*   无序散列连接
*   广播散列连接
*   排序合并联接
*   笛卡尔连接
*   广播嵌套循环连接

*广播散列连接*:在“广播散列连接”机制中，两个输入数据集之一(参与连接)被广播给所有执行器。在来自广播数据集的所有执行器上构建哈希表，之后，非广播输入数据集的每个分区独立地连接到作为本地哈希表可用的其他数据集。

> “广播散列连接”不涉及任何洗牌阶段，是最有效的。对其可靠性的唯一要求是执行器应该有足够的内存来容纳广播的数据集。因此，当两个输入数据集都大于可配置的阈值时，Spark 会避免这种机制。

*Shuffle Hash Join:* 在‘Shuffle Hash Join’机制中，首先，将两个输入数据集对齐到一个选择的输出分区方案(要了解更多关于选择的输出分区方案的信息，可以参考我最近出版的名为《[Spark 分区指南](https://www.amazon.com/Guide-Spark-Partitioning-Explained-Depth/dp/B08L25WHJ4)》的书)。如果一个或两个输入数据集不符合所选的分区方案，则在实际连接之前会执行一个 shuffle 操作来实现一致性。

在确保两个输入数据集符合所选的输出分区后，Shuffle Hash 使用标准哈希连接方法对每个输出分区执行连接操作。这意味着，对于每个输出分区，首先从较小输入数据集的相应分区构建哈希表，然后将较大输入数据集的相应分区与构建的哈希表连接。

> 与“广播散列连接”相比，在“混洗散列连接”的情况下，对执行器的存储器要求相对较少。这是因为哈希表只建立在较小输入数据集的某个分区上。因此，如果您提供了大量的输出分区，并且您有大量的具有适当内存配置的执行器，那么您可以通过“混洗散列连接”来实现更高的连接操作效率。然而，如果 Spark 需要在一个或两个输入数据集上执行额外的混洗操作以符合输出分区，那么效率将低于“广播散列连接”。

*排序合并连接*:‘排序合并连接’的初始部分类似于‘混洗哈希连接’。这里，首先，两个输入数据集与选择的输出划分方案对齐。如果一个或两个输入数据集不符合所选的分区方案，则在实际连接之前会执行一个 shuffle 操作来实现一致性。

在确保两个输入数据集符合所选输出分区后，排序合并使用标准排序合并连接方法对每个输出分区执行连接操作。

> 与“混洗散列连接”和“广播散列连接”相比，“排序合并连接”的计算效率较低，但是，用于执行“排序合并连接”的执行器的内存需求明显低于“混洗散列”和“广播散列”。此外，类似于“混洗散列连接”，如果输入数据集不符合期望的输出划分，则一个或两个输入数据集的混洗操作，视情况而定，增加了“排序合并连接”执行的开销。

*笛卡尔连接*:笛卡尔连接专门用于执行两个输入数据集之间的交叉连接。输出分区的数量总是等于输入数据集的分区数量的乘积。每个输出分区映射到一对唯一的分区，每对分区由一个输入数据集的一个分区和另一个输入数据集的另一个分区组成。对于输出数据集的每个输出分区，通过对来自映射到输出分区的两个输入分区的数据进行笛卡尔乘积来计算数据。

> 笛卡尔连接的缺点导致输出分区数量激增。但是如果你需要交叉连接，笛卡尔是唯一的机制。

*广播嵌套循环连接:*在‘广播嵌套循环连接’中，输入数据集之一被广播给所有的执行器。此后，使用标准嵌套循环连接过程将非广播输入数据集的每个分区连接到广播数据集，以产生输出连接数据。

> *“广播嵌套循环连接”*计算效率最低，因为执行嵌套循环是为了比较两个数据集。此外，它是内存密集型的，因为输入数据集之一需要广播给所有的执行器。

【Spark 如何选择加入机制？

在查看了连接操作的重要方面和连接执行的各种机制之后，现在让我们看看 Spark 是如何选择特定机制的:

Spark 基于以下因素选择执行连接操作的特定机制:

*   配置参数
*   加入提示
*   输入数据集的大小
*   连接类型
*   等价或非等价联接

Spark 在连接 API 中提供了灵活性，可以指定可选的连接提示来完成连接机制。诸如“广播”、“合并”、“混洗 _ 散列”和“混洗 _ 复制 _nl”的连接提示可以与参与连接的数据集一起提供。

以下是 Spark 如何根据上述因素选择各种连接机制的综合描述:

**‘广播散列连接’**

*强制条件*

*   仅适用于等同连接条件
*   不适用于“全外部”联接类型

除了强制条件之外，下列条件之一应成立:

*   在左侧输入数据集上提供了“广播”提示，连接类型为“右外”、“右半”或“内”。
*   未提供任何提示，但左侧输入数据集可根据配置'*spark . SQL . autobroadcastjointhreshold(缺省 10 MB)'* 进行广播，连接类型为“右外”、“右半”或“内”。
*   右侧输入数据集中提供了“广播”提示，连接类型为“左外”、“左半”或“内”。
*   没有提供任何提示，但是根据配置'*spark . SQL . autobroadcastjointhreshold(缺省 10 MB)'* ，右输入数据集是可广播的，并且联接类型是“左外”、“左半”或“内”。
*   在输入数据集和连接类型上提供的“广播”提示为“左外”、“左半”、“右外”、“右半”或“内”。
*   没有提供任何提示，但是两个输入数据集都是可根据配置'*spark . SQL . autobroadcastjointhreshold(缺省 10mb)'【T1]进行广播的，并且联接类型是'左外'、'左半'、'右外'、'右半'或'内'。*

**‘洗牌哈希加入’**

*强制条件*

*   仅适用于等同连接条件
*   不适用于“全外部”联接类型
*   配置'*spark . SQL . join . prefersortmergejoin(默认为真)*'被设置为假

除了强制条件之外，下列条件之一应成立:

*   在左侧输入数据集上提供了“shuffle_hash”提示，联接类型为“右外”、“右半”或“内”。
*   未提供任何提示，但左侧输入数据集比右侧输入数据集小得多，并且连接类型为“右外”、“右半”或“内”。
*   右输入数据集中提供了“shuffle_hash”提示，连接类型为“左外”、“左半”或“内”。
*   未提供任何提示，但右侧输入数据集比左侧数据集小得多，连接类型为“左外”、“左半”或“内”。
*   在输入数据集和连接类型上提供的“shuffle_hash”提示为“左外”、“左半”、“右外”、“右半”或“内”。
*   没有提供任何提示，但是两个数据集都相当小，并且联接类型为“左外”、“左半”、“右外”、“右半”或“内”。

**‘排序合并连接’**

*强制条件*

*   仅适用于等同连接条件
*   根据等价连接条件确定的连接键是可排序的
*   配置'*spark . SQL . join . prefersortmergejoin(默认为真)*'被设置为真

除强制条件外，下列条件之一应成立:

*   在任何输入数据集中提供了“merge”提示，并且联接类型可以是任何类型。
*   没有提供任何提示，联接类型可以是任意的。

**‘笛卡尔连接’**

*强制条件*

*   联接类型“内部”

除了强制条件之外，下列条件之一应成立:

*   在任何输入数据集中提供了“shuffle_replicate_nl”提示，连接条件可以是相等的或非相等的。
*   未提供任何提示，联接条件可以是相等的或非相等的。

**“广播嵌套循环连接”**

“广播嵌套循环连接”是默认的连接机制，当没有其他机制可以选择时，则选择“广播嵌套循环连接”作为最终机制，以针对任何连接条件执行任何连接类型。

> 如果有一个以上的连接机制适合执行，则按照“广播散列连接”优先于“排序合并连接”优先于“混洗散列连接”优先于“笛卡尔连接”的顺序选择优选的连接机制。
> 
> 在笛卡尔连接和广播嵌套循环连接中，当输入数据集之一可以广播时，广播嵌套循环比笛卡尔连接更适用于内部非对等连接。

最后但同样重要的是，分区对于给定连接机制的执行效率也起着非常重要的作用。要了解更多关于分区的信息，可以参考前面的链接。

希望这个故事能消除你对 Apache Spark 中 Join 执行的所有困惑和疑虑。如果还有任何疑问，请写在评论区或给我发消息。