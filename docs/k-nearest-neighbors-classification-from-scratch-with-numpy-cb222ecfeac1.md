# 用 NumPy 从零开始进行 k-最近邻分类

> 原文：<https://towardsdatascience.com/k-nearest-neighbors-classification-from-scratch-with-numpy-cb222ecfeac1?source=collection_archive---------6----------------------->

![](img/8e296977a24d25424160713982e3f1a6.png)

妮娜·斯特雷尔在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 拍摄的照片

W 欢迎来到另一个用 NumPy 从零开始实现机器学习算法的帖子。在这篇文章中，我将实现 K 近邻(KNN)，这是一种机器学习算法，可用于分类和回归的目的。它属于监督学习算法的范畴，为看不见的观察预测目标值。换句话说，它对带标签的数据集进行操作，并预测测试数据的类(分类)或数值(回归)。

正如我在其他帖子中提到的，我不会使用已经实现的 k-nearest neighborhood 算法，我将只使用 NumPy 实现它，并将使用其他库进行数据可视化，并创建和使用数据集。

考虑到有很多有用的资源可以解释 KNN 的基本原理，我想马上进入编码部分。我们继续吧！

我们从导入将在整个实现中使用的库和函数开始:

*   ***numpy*** :显然，它将用于多维数组的数值计算，因为我们大量处理高维向量。
*   ***make _ class ification***:我们将使用它来创建我们自己的分类数据集。我们可以决定需要多少个类和特征，是否需要对样本进行聚类，等等。

在 KNN 算法中，我们需要一个函数来计算训练数据点和我们想要分类的数据之间的距离。这里，我选择了欧几里德距离，因为它是机器学习应用中广泛使用的距离。可以尝试使用其他距离度量，如曼哈顿距离、切比雪夫距离等。

当我们考虑 KNN 的实际情况时，我们会尝试找到邻居，即最接近我们想要分类的数据的数据点。数据点是否接近是由我们上面实现的欧几里德距离函数决定的。这里，数据点之间距离的实际值并不重要，相反，我们感兴趣的是这些距离的顺序。

在训练过程之前，我们挑选一个数字作为我们的超参数(k ),并挑选 k 个与我们想要在训练期间分类的数据点最近的邻居。因此，例如，当我们说 5 个最近的邻居时，我们指的是前 5 个最近的数据点。

在上面的 ***kneighbors*** 函数中，我们找到了测试数据集中每个点(我们要分类的数据点)与数据集其余部分(即训练数据)之间的距离。我们将这些距离存储在 ***point_dist*** 中，其中每一行对应于一个测试数据点和所有训练数据之间的距离列表。因此，我们检查每一行，对其进行计数，然后根据距离进行排序。我们枚举每一行的原因是因为我们不想丢失用来计算距离的训练数据点的索引，因为我们稍后将引用它们。

因此， ***sorted_neigh*** 保存了我们的测试数据点的前 k 个最近邻，并且根据它们的欧几里德距离对它们进行排序。然后，我们从 ***sorted_neigh*** 中提取索引和距离值，并返回它们。

在找到 k-最近邻后，我们尝试预测我们的测试数据点所属的类。这里，我们有 k 个邻居，每个邻居都有一票来决定类别标签。然而，投票机制可以根据所选择的标准而变化。这里，在上面的**预测**函数中，如果**权重**被选择为**统一**，这意味着每个邻居在决定类标签时具有相等的投票(权重),而与它们的距离无关。

假设我们的测试数据点有 5 个最近邻，其中 3 个属于 A 类，2 个属于 b 类。我们不考虑邻居的距离，并得出测试数据点属于 A 类的结论，因为大多数邻居都属于 A 类。但是，如果选择**权重**作为**距离**，那么这意味着邻居的距离确实很重要。因此，最接近测试数据点的邻居具有与其距离的倒数成比例的最大权重(投票)。因此，关于前述示例，如果属于类别 A 的那 2 个点比其他 3 个点更接近测试数据点，则仅这一事实就可以在决定数据点的类别标签中起很大作用。

在**预测**功能中，如果**权重**是**均匀的**，那么预测数据点的标签是相当容易的。首先，我们获得邻居的索引，然后使用这些索引从训练数据集中获得它们对应的类别标签。 **neighbors** 中的每一行对应于每个测试数据点所具有的一组邻居。然后，我们使用 nump **y 的** **bincount** 函数找到类标签的出现，并获得对应于预测的类标签的最大出现的索引。

如果我们选择**权重**作为**距离**，事情会变得有点混乱。在这种情况下，我们找到相邻距离的平均倒数，并计算每个测试数据点的分类概率。

我实现了 **score** 函数，作为一个在分类问题中广泛使用的非常简单的准确性度量。我们只是返回正确分类标签的百分比。这很简单！

现在，是时候创建数据集了，我们将在其上测试我们的 knn 算法。我们利用 **sklearn.dataset** 的 **make_classification** 函数来填充数据集。之后，我们通过减去平均值然后除以标准偏差来标准化每个数据点。

在创建数据集之后，我们为我们的训练阶段实现随机分割，从而获得我们的训练和测试集。

```
In[104]: kneighbors(X_test)Out[104]: array([[ 85, 405, 370,  63, 694],
               [345,  64, 189, 136,  32],
               [554, 216, 690, 672,  51],
                ...,
               [560, 323,  99, 418, 295],
               [214, 180, 534, 190, 133],
               [ 20, 129, 564, 103, 679]])
```

是时候测试我们的 knn 实现代码了。我们得到了测试数据集的 k 个最近邻。请注意，每一行都与测试集中的每个数据点相关，并且每一行中的元素都对应于测试数据点的邻居的索引。

```
In[105]: predict(X_test)Out[105]: array([0, 1, 2, 1, 0, 2, 1, 1, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 1, 2, 2, ..., 2, 1, 0, 2, 1, 2, 1, 0, 0, 1, 1, 0, 0, 2, 0, 2, 1, 2, 0, 1, 2, 0])
```

如预期的那样，**预测**函数输出测试数据的预测类标签。

```
In [106]: score(X_test, y_test)Out[106]: 0.99
```

看起来我们的实现做得非常好，给出了很高的准确度分数。

# K-最近邻的类实现

在实现了所有必要的函数之后，创建 knn 的类实现就相当容易了。这里唯一新增加的函数是我们类的 **__init__** 函数。

# 将我们的实现与 Sklearn 的 KNeighborsClassifier 进行比较

```
Out[110]:   
           Our Implementation       Sklearn's Implementation                         Accuracy   0.955556                 0.955556
```

原来我们自己实现的精度和 **sklearn** 的实现看起来是一样的。这是好消息，对吗？我们在实现方面做得相当不错。

你也可以查看我的 [GitHub 简介](https://github.com/leventbass/k_nearest_neighbors)来沿着 *jupyter 笔记本*阅读代码或者简单地使用代码来实现。

将来，我一定会带来更多的实现。

编码快乐！

**疑问？评论？联系我在**[](http://leventbas92@gmail.com)****，通过**[**GitHub**](https://github.com/leventbass)**或**[**LinkedIn**](https://www.linkedin.com/in/levent-bas/)**。****