<html>
<head>
<title>Attention: Sequence 2 Sequence model with Attention Mechanism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意:具有注意机制的序列2序列模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a?source=collection_archive---------4-----------------------#2020-01-20">https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a?source=collection_archive---------4-----------------------#2020-01-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3b5c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Bahdanau和Luong提出的序列2序列模型中注意机制的详细解释</h2></div><p id="89ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，您将了解到:</p><ul class=""><li id="302c" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">为什么我们需要序列2序列模型的注意机制？</li><li id="e9e2" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">Bahdanua的注意力机制是如何工作的？</li><li id="b59d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">Luong的注意力机制是如何工作的？</li><li id="56af" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">什么是本地和全球关注？</li><li id="10a6" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">Bahdanau和Luong注意机制的主要区别</li></ul><p id="9aa3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">先决条件:</strong></p><p id="0e25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ls" href="https://medium.com/datadriveninvestor/recurrent-neural-network-rnn-52dd4f01b7e8" rel="noopener">像LSTM和GRU这样的循环神经网络(RNN)</a></p><p id="781c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ls" rel="noopener" target="_blank" href="/intuitive-explanation-of-neural-machine-translation-129789e3c59f"> Seq2Seq-神经机器翻译</a></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/c38dd2354c70695efef8d7a099bd8e5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wva1aVI-qyy1fGD0yELi5w.png"/></div></div></figure><h2 id="a9ce" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">什么是注意，为什么我们需要序列2序列模型的注意机制？</h2><p id="472c" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">让我们考虑两个场景，场景一，你正在阅读一篇与当前新闻相关的文章。第二种情况是你准备考试。两种情况下的注意力水平是相同还是不同？</p><p id="9ed8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与新闻文章相比，你在准备考试时会相当注意阅读。在准备考试的时候，你将会更加关注关键词来帮助你记住一个简单或复杂的概念。这同样适用于任何深度学习任务，我们希望专注于感兴趣的特定领域。</p><p id="a70d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">序列到序列(Seq2Seq)模型使用编码器-解码器架构。</strong></p><p id="bc8d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">seq2seq的几个用例</p><ul class=""><li id="fd3f" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">神经机器翻译(NMT)、</li><li id="4638" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">图像字幕，</li><li id="ddcd" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">聊天机器人</li><li id="a4b7" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">抽象文本摘要等。</li></ul><p id="8c89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Seq2Seq模型将源序列映射到目标序列。在神经机器翻译的情况下，源序列可以是英语，目标序列可以是印地语。</p><p id="edde" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将英语中的源句子传递给编码器；<strong class="kk iu">编码器将源序列的完整信息编码成单个实值向量，也称为上下文向量</strong>。这个上下文向量然后被传递给解码器，以产生目标语言(如印地语)的输出序列。上下文向量负责将整个输入序列总结成一个向量。</p><p id="266e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="nd">如果输入的句子很长，来自编码器的单个向量能否容纳所有相关信息提供给解码器？</em>T13】</strong></p><p id="93fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="nd">在预测目标词时，是否可以关注句子中的几个相关词，而不是保存整个句子信息的单个向量？</em> </strong></p><p id="3476" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意力机制有助于解决问题。</p><blockquote class="ne"><p id="508c" class="nf ng it bd nh ni nj nk nl nm nn ld dk translated">注意机制的基本思想是避免试图学习每个句子的单个向量表示，而是基于注意权重来注意输入序列的特定输入向量。</p></blockquote><p id="50cd" class="pw-post-body-paragraph ki kj it kk b kl no ju kn ko np jx kq kr nq kt ku kv nr kx ky kz ns lb lc ld im bi translated">在每一个解码步骤，解码器将被告知需要对每个输入单词给予多少“关注”，使用一组<strong class="kk iu"><em class="nd"/></strong><em class="nd">。</em>这些注意力权重向解码器提供上下文信息以进行翻译</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/acbafb3219e10ef8aa87c47fe05d00f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*ACuAqYeyR8tc__Eld098zQ.png"/></div></figure><h1 id="cdd2" class="nu mg it bd mh nv nw nx mk ny nz oa mn jz ob ka mq kc oc kd mt kf od kg mw oe bi translated">巴赫达瑙注意机制</h1><p id="6a96" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">Bahdanau等人提出了一种<strong class="kk iu">学习联合对齐和翻译</strong>的注意机制。它也被称为<strong class="kk iu">附加注意，因为它执行编码器状态和解码器状态的线性组合</strong>。</p><p id="531f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nd">让我们来理解巴赫达瑙提出的注意机制</em></p><ul class=""><li id="c007" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">编码器(前向和后向)和解码器的所有隐藏状态都用于生成上下文向量，不像seq2seq中只使用最后一个编码器隐藏状态而不加注意。</li><li id="c5be" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">注意机制利用由前馈网络参数化的比对分数来比对输入和输出序列。它有助于注意源序列中最相关的信息。</li><li id="92dd" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">该模型基于与源位置和先前生成的目标单词相关联的上下文向量来预测目标单词。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi of"><img src="../Images/e6aaf5e8a543949ce89ef996aca53b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*qhOlQHLdtfZORIXYuoCtaA.png"/></div><p class="og oh gj gh gi oi oj bd b be z dk translated">注意机制</p></figure><p id="6b5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">具有注意机制的Seq2Seq模型由编码器、解码器和注意层组成。</strong></p><p id="1c7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意力层包括</p><ul class=""><li id="cd66" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">对准层</li><li id="4699" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">注意力权重</li><li id="7412" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">上下文向量</li></ul><p id="7165" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">比对分数</strong></p><p id="2544" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">比对分数映射了位置<em class="nd">”</em><strong class="kk iu"><em class="nd">j”</em></strong>周围的输入和位置<strong class="kk iu"><em class="nd">I”</em></strong>处的输出匹配得如何。分数基于预测目标单词之前的前一解码器的隐藏状态<strong class="kk iu"> s₍ᵢ₋₁₎ </strong>和输入句子的隐藏状态hⱼ</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/a0bea1eadb901940e6ad79cd4ed147d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*u2YdTRPjN34Fpr-zxvoJsg.png"/></div></figure><p id="bcb1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">解码器决定需要关注源句子的哪一部分，而不是让编码器将源句子的所有信息编码成一个定长向量</strong>。</p><p id="f333" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与源序列具有相同长度的对齐向量，在解码器的每个时间步长进行计算</p><p id="f615" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nd">在我们的例子中，为了预测第二个目标词，</em><strong class="kk iu"><em class="nd"/></strong><em class="nd">，我们将为输入词</em> <strong class="kk iu"> <em class="nd">快速生成一个高分</em> </strong></p><p id="d465" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">注意力权重</strong></p><p id="5c28" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们<strong class="kk iu">将softmax激活函数应用于比对分数，以获得注意力权重</strong>。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/2ba2d55bdc7732c09f7d484aed8c1f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*3aCyU9aSVHvxzOwvQdExdQ.png"/></div></figure><p id="4b4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Softmax激活函数将获得总和等于1的概率，这将有助于表示每个输入序列的影响权重。输入序列的注意力权重越高，其对预测目标单词的影响就越大。</p><p id="8542" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nd">在我们的例子中，我们看到输入单词</em><strong class="kk iu"><em class="nd"/></strong><em class="nd"/><strong class="kk iu"><em class="nd">【तेज़ी】</em></strong>具有较高的注意力权重值</p><p id="9d7e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">上下文向量</strong></p><p id="881f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上下文向量用于计算解码器的最终输出。<strong class="kk iu">上下文向量𝒸ᵢ是注意力权重和编码器隐藏状态(h₁、h₂、…,hₜₓ)的加权和，其映射到输入句子。</strong></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi om"><img src="../Images/f9011c117b8e3044c05c23c818fa30fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*RXqgpEygsdTXh5Dz_Otrjg.png"/></div></figure><p id="8d88" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">预测目标词</strong></p><p id="5861" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了预测目标单词，解码器使用</p><ul class=""><li id="0be0" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">上下文vector(𝒸ᵢ)，</li><li id="d257" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">前一时间步的解码器输出(y <strong class="kk iu"> ᵢ₋₁ </strong>)和</li><li id="79f7" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">前一个解码器的隐藏状态(<strong class="kk iu"> sᵢ₋₁) </strong></li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi on"><img src="../Images/64c5ae10041fed58cbe043be7502ccf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*0czi26IzVMgf-i3eHgOnPw.png"/></div><p class="og oh gj gh gi oi oj bd b be z dk translated">解码器在时间步长I的隐藏状态</p></figure><h1 id="b3dd" class="nu mg it bd mh nv nw nx mk ny nz oa mn jz ob ka mq kc oc kd mt kf od kg mw oe bi translated">卢昂注意机制</h1><p id="f65a" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">Luong的注意力也被称为<strong class="kk iu">倍增注意力</strong>。<strong class="kk iu">通过简单的矩阵乘法将编码器状态和解码器状态转化为注意力分数。简单的矩阵乘法使它更快更节省空间。</strong></p><p id="c568" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Luong根据注意力在源序列中的位置提出了两种类型的注意机制</p><ol class=""><li id="de01" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld oo lk ll lm bi translated"><strong class="kk iu">全局关注</strong>关注所有源位置</li><li id="61f6" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld oo lk ll lm bi translated"><strong class="kk iu">局部注意力</strong>其中注意力仅放在每个目标单词的源位置的一个小子集上</li></ol><p id="cf4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">全局和局部注意力的共性</strong></p><ul class=""><li id="9ae3" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">在解码阶段的每个时间步长t，全局和局部注意这两种方法首先将堆叠LSTM顶层的隐藏状态hₜ作为输入。</li><li id="1322" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">这两种方法的目标都是导出一个上下文向量<strong class="kk iu"> 𝒸ₜ </strong>来捕捉相关的源端信息，以帮助预测当前的目标单词<strong class="kk iu"> yₜ </strong></li><li id="b037" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">注意力向量作为输入被馈送到下一个时间步骤，以通知模型关于过去的对齐决策。</li></ul><p id="45c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">全局和局部注意力模型在如何导出上下文向量𝒸ₜ方面有所不同</strong></p><p id="2298" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们讨论全局和局部注意之前，让我们先了解Luong的注意机制在任何给定时间t所使用的惯例</p><ul class=""><li id="acf8" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">𝒸ₜ:语境向量</li><li id="f66c" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">aₜ:对齐向量</li><li id="e2ba" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">hₜ:当前目标隐藏状态</li><li id="451e" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">hₛ:电流源隐藏状态</li><li id="e3ab" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">yₜ:预测当前目标词</li><li id="52b6" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">˜ₜ : 注意力向量</li></ul><h1 id="a6f9" class="nu mg it bd mh nv nw nx mk ny nz oa mn jz ob ka mq kc oc kd mt kf od kg mw oe bi translated">全球关注</h1><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi op"><img src="../Images/5911e3feea4b25cec4501c756754ef3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*LhEapXF1mtaB3rDgIjcceg.png"/></div><p class="og oh gj gh gi oi oj bd b be z dk translated">全球注意力来源:<a class="ae ls" href="https://arxiv.org/pdf/1508.04025.pdf" rel="noopener ugc nofollow" target="_blank">基于注意力的神经机器翻译的有效方法</a></p></figure><ul class=""><li id="ba21" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu">当计算上下文向量𝒸ₜ.时，全局注意力模型考虑编码器的所有隐藏状态</strong></li><li id="5f1f" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">通过将当前目标隐藏状态<strong class="kk iu">h<em class="nd">ₜ</em>T7】与源隐藏状态<strong class="kk iu"> hₛ </strong>中的每一个进行比较，得到等于源序列中时间步长数量大小的可变长度对齐向量<strong class="kk iu">t1】aₜT3】</strong></strong></li><li id="9d3f" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">比对分数被称为基于内容的函数，我们考虑了三种不同的备选方案</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oq"><img src="../Images/edabf58d41e9ee60bdd44e0f3b7395c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Ta67S8_lXTbVzJMztkxKg.png"/></div></div></figure><ul class=""><li id="0e30" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu">全局上下文向量𝒸ₜ是根据对齐向量<em class="nd"> aₜ </em>在所有源隐藏状态hₛ </strong>上计算的加权平均值</li></ul><p id="4712" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nd">当源序列是大段或者大文档时会怎样？</em></p><p id="c6a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当全局注意力模型考虑源序列的所有单词来预测目标单词时，它变得<strong class="kk iu">计算昂贵，并且翻译更长的句子可能具有挑战性</strong></p><p id="4c90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过使用局部注意来解决全局注意模型的这一缺陷</p><h1 id="d019" class="nu mg it bd mh nv nw nx mk ny nz oa mn jz ob ka mq kc oc kd mt kf od kg mw oe bi translated">当地的关注</h1><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi or"><img src="../Images/cc745763484e852894b41a3ef7c50553.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*YXjdGl3CnSfHfzYpQiObgg.png"/></div><p class="og oh gj gh gi oi oj bd b be z dk translated">局部注意力来源:<a class="ae ls" href="https://arxiv.org/pdf/1508.04025.pdf" rel="noopener ugc nofollow" target="_blank">基于注意力的神经机器翻译的有效方法</a></p></figure><ul class=""><li id="bd9e" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu">局部注意力只集中在每个目标单词的源位置的一个小的子集上，不像全局注意力那样集中在整个源序列上</strong></li><li id="55d1" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">计算成本低于全局注意力</strong></li><li id="60c0" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">局部注意力模型首先在时间t为每个目标单词生成对齐位置<strong class="kk iu"> Pₜ </strong></li><li id="793d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">上下文向量<strong class="kk iu"> 𝒸ₜ </strong>是作为所选窗口内的源隐藏状态集合的加权平均值导出的</li><li id="7dab" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">可单调或预测地选择对齐位置</strong></li></ul><h1 id="5ccc" class="nu mg it bd mh nv nw nx mk ny nz oa mn jz ob ka mq kc oc kd mt kf od kg mw oe bi translated">Bahdanau和Luong注意机制的主要区别</h1><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi os"><img src="../Images/787e083c64d3f462e1dabd460e2aaedb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BfwFEH4tgY-JwHxr-2D2KQ.png"/></div></div><p class="og oh gj gh gi oi oj bd b be z dk translated"><strong class="bd ot">双向编码器中前向和后向隐藏状态的Bahdanau级联。Luong attention在编码器和解码器的顶层都使用隐藏状态</strong></p></figure><h2 id="8a68" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">Bahdanau和Luong注意机制中注意的计算</h2><p id="3384" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">Bahdanau等人在双向编码器中使用前向和后向隐藏状态的连接，在他们的非堆叠单向解码器中使用先前目标的隐藏状态</p><p id="2a27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Loung等人的注意力在编码器和解码器中的顶部LSTM层使用隐藏状态</p><p id="0f7f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> Luong注意机制使用当前解码器的隐藏状态来计算对齐向量，而Bahdanau使用前一时间步的输出</strong></p><h2 id="fa06" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">对齐功能</h2><p id="206b" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated"><strong class="kk iu"> Bahdanau </strong>仅使用concat评分比对模型，而Luong使用dot、general和concat评分比对模型</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oq"><img src="../Images/edabf58d41e9ee60bdd44e0f3b7395c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Ta67S8_lXTbVzJMztkxKg.png"/></div></div></figure><p id="8349" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了注意力机制的知识，你现在可以构建强大的深度NLP算法。</p><h2 id="f644" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">参考资料:</h2><p id="4870" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">通过联合学习对齐和翻译Dzmitry Bahdanau的神经机器翻译</a></p><p id="d381" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ls" href="https://arxiv.org/pdf/1508.04025.pdf" rel="noopener ugc nofollow" target="_blank">基于注意力的神经机器翻译的有效方法:Minh-Thang Luong Hieu Pham Christopher d . Manning</a></p><div class="ou ov gp gr ow ox"><a href="https://devopedia.org/attention-mechanism-in-neural-networks" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">神经网络中的注意机制</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">在机器翻译中，编码器-解码器架构是常见的。编码器读取一个单词序列，然后…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">devopedia.org</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl md ox"/></div></div></a></div></div></div>    
</body>
</html>