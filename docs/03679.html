<html>
<head>
<title>NLP: Building a Text Summariser Using Global Vectors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP:使用全局向量构建文本摘要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-building-a-summariser-68e0c19e3a93?source=collection_archive---------40-----------------------#2020-04-06">https://towardsdatascience.com/nlp-building-a-summariser-68e0c19e3a93?source=collection_archive---------40-----------------------#2020-04-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1fff8992bc0132dd78a4a891770e979a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d2U0DnF4pAMaDz5c"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">蒂姆·莫斯霍尔德在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><p id="7dbe" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一篇文章的摘要是一个简短的版本，它包含了文章的关键点，目的是把文章浓缩成几个要点，并且用文章本身的语言来表达。主要地，只有那些我们认为最重要的元素/句子被提取出来，通常，这些元素/句子传达了主要思想，或者基本的支持点。</p><p id="6f70" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总结不是对文章的分析。两者是不同的东西。摘要在很多情况下是有用的，例如，获得一篇大文章的要点，用热门词汇介绍一个复杂的想法，从大文章中获得意义等等。</p><p id="ef53" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在学术上，提取摘要是一项有点挑战性的任务。谢天谢地，机器学习来拯救我们了。机器学习的自然语言处理(NLP)模块提供了许多可用于文本摘要的算法。有两种主要的文本摘要方法:</p><blockquote class="le lf lg"><p id="ac09" class="kg kh lh ki b kj kk kl km kn ko kp kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated"><strong class="ki jk">抽象摘要:</strong> <br/>这种技术使用先进的自然语言处理方法来产生一个摘要，它在所使用的单词/句子方面是全新的。也就是说，摘要是用文章中没有用到的词写成的。</p><p id="b1fd" class="kg kh lh ki b kj kk kl km kn ko kp kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">E <strong class="ki jk">提取摘要:</strong> <br/>在这种技术中，最重要的单词/句子被提取出来并组合在一起以创建摘要。因此，摘要中使用的单词/句子来自文章本身。</p></blockquote><p id="b7ee" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将使用抽取技术构建一个新闻摘要，从一篇大型新闻文章中抽取4-5个主要的重要句子。我们将检查一些流行和有效的策略来处理大量的文本，并从中提取4-5个有意义的句子。</p><p id="2385" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用全局向量，也称为手套算法，这是单词的向量表示。用外行人的话来说，我们将使用手套算法生成句子向量，并将选择每页排名最高的句子算法。事不宜迟，让我们深入研究代码。我在这个练习中使用了python。</p><h1 id="2dc4" class="ll lm jj bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">流程新闻RSS提要</strong></h1><p id="db58" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我选择在印度最受欢迎的新闻服务之一TimeOfIndia的RSS feed上工作。在这个练习中，我选择了新闻的“世界”部分。但是它足够灵活，可以处理各种新闻服务多个RSS提要。</p><p id="1bde" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们读取RSS提要，并将新闻条目链接传递给BeautifulSoup进行HTML解析。注意，这里我只取了一个RSS提要，并一步一步地进行解析。后来，我将这些步骤结合在一起，无缝地处理多个提要。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="1067" class="mx lm jj mt b gy my mz l na nb"># import basic required libraries<br/>import pandas as pd<br/>import numpy as np<br/>import os</span><span id="c5ed" class="mx lm jj mt b gy nc mz l na nb"># for web and HTML<br/>import requests<br/>from bs4 import BeautifulSoup</span><span id="3443" class="mx lm jj mt b gy nc mz l na nb"># create a dict of various rss feed link and their categories. Will iterate them one by one.<br/># Have mentioned only one feed for demo purposes<br/>timesofindia = {'world':'<a class="ae jg" href="http://timesofindia.indiatimes.com/rssfeeds/296589292.cms'" rel="noopener ugc nofollow" target="_blank">http://timesofindia.indiatimes.com/rssfeeds/296589292.cms'</a>}<br/>for category, rsslink in timesofindia.items():<br/>    print('Processing for category: {0}. \nRSS link: {1}'.format(category,rsslink))<br/>    # get the webpage URL and read the html<br/>    rssdata = requests.get(rsslink)<br/>    #print(rssdata.content)<br/>    soup = BeautifulSoup(rssdata.content)<br/>    print(soup.prettify())</span></pre><p id="2d6f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在BeautifulSoup解析之后，应该彻底检查网页HTML内容(如上所述，通过使用prettify函数)以找到标签/模式或标签序列，从而导航到所需的新闻标题、链接和发布日期。在我们的例子中，这些元素在“item”标签中。所以让我们提取它来遍历每个“item”标签并提取每个单独的元素。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="4dd1" class="mx lm jj mt b gy my mz l na nb"># get all news items. It has title, description, link, guid, pubdate for each news items. <br/># Lets call this items and we will iterate thru it<br/>allitems = soup.find_all('item')</span><span id="663d" class="mx lm jj mt b gy nc mz l na nb"># print one news item/healine to check<br/>for item in range(len(allitems)):<br/>    print('Processing news-item #:',item)<br/>    title = allitems[item].title.text<br/>    link = allitems[item].guid.text<br/>    pubdate = allitems[item].pubdate.text<br/>    print('TITLE:',title)<br/>    print('LINK:',link)<br/>    print('PUBDATE:',pubdate)</span></pre><p id="4bef" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="c6b1" class="mx lm jj mt b gy my mz l na nb">Total news items found: 20<br/>TITLE: Boris Johnson's pregnant fiancee says she is 'on the mend' from coronavirus<br/>LINK: <a class="ae jg" href="https://timesofindia.indiatimes.com/world/uk/boris-johnsons-pregnant-fiancee-says-she-is-on-the-mend-from-coronavirus/articleshow/74994200.cms" rel="noopener ugc nofollow" target="_blank">https://timesofindia.indiatimes.com/world/.</a>...<br/>PUBDATE: Sun, 05 Apr 2020 17:15:04 IST<br/>TITLE: US to airlift 22,000 Americans stranded overseas; many in India<br/>LINK: <a class="ae jg" href="https://timesofindia.indiatimes.com/world/us/us-to-airlift-22000-americans-stranded-overseas-many-in-india/articleshow/74992196.cms" rel="noopener ugc nofollow" target="_blank">https://timesofindia.indiatimes.com/world/.</a>...<br/>PUBDATE: Sun, 05 Apr 2020 14:08:04 IST<br/>TITLE: Ecuador VP apologizes after virus corpses left on streets<br/>LINK: <a class="ae jg" href="https://timesofindia.indiatimes.com/world/rest-of-world/ecuador-vice-president-apologizes-after-virus-corpses-left-on-streets/articleshow/74992101.cms" rel="noopener ugc nofollow" target="_blank">https://timesofindia.indiatimes.com/world/.</a>...<br/>PUBDATE: Sun, 05 Apr 2020 14:01:42 IST</span></pre><p id="52f4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要求的元素，即标题，链接，发布日期期待。让我们转到下一节，我们将创建一个从链接中获取新闻文章文本的基本函数。</p><h1 id="97cb" class="ll lm jj bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">摘录新闻文章</strong></h1><p id="e16a" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">在本节中，我们将通过解析网页的HTML链接来提取新闻文章文本。从RSS feed收到的链接中，我们将获取网页并使用BeautifulSoup解析它。</p><p id="9bee" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">应该对网页HTML进行彻底的分析，以识别其中包含所需新闻文本的标签。我创建了一个从链接中获取新闻文本的基本函数。使用BeautifulSoup，将提取特定html标签中可用的新闻文本。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="1fbb" class="mx lm jj mt b gy my mz l na nb"># Function to fetch each news link to get news essay <br/>def fetch_news_text(link):<br/>    # read the html webpage and parse it<br/>    soup = BeautifulSoup(requests.get(link).content, 'html.parser')</span><span id="bc46" class="mx lm jj mt b gy nc mz l na nb">    # fetch the news article text box<br/>    # these are with element &lt;div class="_3WlLe clearfix"&gt;<br/>    text_box = soup.find_all('div', attrs={'class':'_3WlLe clearfix'})<br/>    # extract text and combine<br/>    news_text = str(". ".join(t.text.strip() for t in text_box))<br/>    return news_text</span><span id="0e4d" class="mx lm jj mt b gy nc mz l na nb"># using the above function, process text<br/>news_articles = [{'Feed':'timesofindia',<br/>                  'Category':category, <br/>                  'Headline':allitems[item].title.text, <br/>                  'Link':allitems[item].guid.text, <br/>                  'Pubdate':allitems[item].pubdate.text,<br/>                  'NewsText': fetch_news_text(allitems[item].guid.text)} <br/>                     for item in range(len(allitems))]</span><span id="a1a2" class="mx lm jj mt b gy nc mz l na nb">news_articles = pd.DataFrame(news_articles)<br/>news_articles.head(3)</span></pre><figure class="mo mp mq mr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nd"><img src="../Images/ab023e630cf918af3d7524cf968f38d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-dD60rlFiiWuSMFU3HBbdQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">获取新闻文本后的新闻文章详细信息</p></figure><p id="d847" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以上数据看起来符合预期。注意，新闻文本的提取依赖于指定的HTML标签，所以如果新闻提供者碰巧改变/修改标签，上述提取可能不起作用。为了解决这个问题，我们可以在新闻文本为空时放置一个通知警报/标志，指示修改代码/标签的时间。</p><h1 id="969b" class="ll lm jj bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">文本预处理</strong></h1><p id="0431" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">对于文字清理，我使用了<a class="ae jg" rel="noopener" target="_blank" href="/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0?source=email-2e5d9ff5c77e-1586110771153-layerCake.autoLayerCakeWriterNotification-------------------------7c347700_4379_4710_9431_826fc777de45&amp;sk=646f1b75f932d40d9567767965ff8833."> <em class="lh">这篇</em> </a> <em class="lh"> </em>文章中详述的预处理步骤。这些步骤是删除HTML标签、特殊字符、数字、标点符号、停用词、处理重音字符、扩展缩写、词干化等。这些步骤在<a class="ae jg" rel="noopener" target="_blank" href="/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0?source=email-2e5d9ff5c77e-1586110771153-layerCake.autoLayerCakeWriterNotification-------------------------7c347700_4379_4710_9431_826fc777de45&amp;sk=646f1b75f932d40d9567767965ff8833."> <em class="lh">这篇</em> </a>文章中有详细阐述，所以一定要查看一下细节。</p><p id="108c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我将这些预处理步骤放在一个函数中，该函数将返回一个干净且规范化的语料库。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="9613" class="mx lm jj mt b gy my mz l na nb"># test normalize cleanup on one article<br/># clean_sentences = normalize_corpus([news_articles['NewsText'][0]])<br/>clean_sentences = normalize_corpus(news_articles['NewsText'])</span></pre><h1 id="a76c" class="ll lm jj bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">生成句子向量</h1><p id="e06d" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">使用手套单词嵌入，我们将生成句子的向量表示。对于这个练习，我使用的是预先训练好的维基百科2014 + Gigaword 5手套矢量可用<a class="ae jg" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <em class="lh">这里</em> </a>。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="619f" class="mx lm jj mt b gy my mz l na nb"># define dict to hold a word and its vector<br/>word_embeddings = {}</span><span id="4d8a" class="mx lm jj mt b gy nc mz l na nb"># read the word embeddings file ~820MB<br/>f = open('.\\GloVe\\glove.6B\\glove.6B.100d.txt', encoding='utf-8')<br/>for line in f:<br/>    values = line.split()<br/>    word = values[0]<br/>    coefs = np.asarray(values[1:], dtype='float32')<br/>    word_embeddings[word] = coefs<br/>f.close()</span><span id="205c" class="mx lm jj mt b gy nc mz l na nb"># check the length<br/>len(word_embeddings) # 400000</span></pre><p id="a16b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个集合中，我们有400K的单词嵌入。这些单词嵌入的大小是822 MB。大小可能因嵌入的令牌而异。嵌入越多，精确度越高。使用这些单词嵌入，让我们为规范化的句子创建向量。对于一个句子，我们将首先获取每个单词的向量。然后将取所有句子单词向量分数的平均分数，以得到一个句子的综合向量分数。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="95bb" class="mx lm jj mt b gy my mz l na nb"># create vector for each sentences<br/># list to hold vector <br/>sentence_vectors = []</span><span id="72d3" class="mx lm jj mt b gy nc mz l na nb"># create vector for each clean normalized sentence<br/>for i in clean_sentences:<br/>    if len(i) != 0:<br/>        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)<br/>    else:<br/>        v = np.zeros((100,))<br/>    sentence_vectors.append(v)</span><span id="4528" class="mx lm jj mt b gy nc mz l na nb">print('Total vectors created:',len(sentence_vectors))<br/></span></pre><h1 id="5d39" class="ll lm jj bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">取前N个句子</h1><p id="32d4" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">向量只是平面上的一个指针。使用余弦相似度方法，会发现句子之间的相似之处。向量将低余弦角将被计数器视为更相似。文章中每隔一个句子计算一个句子的余弦值。这里，也可以使用像欧几里德距离这样的其他方法，并且它们之间的距离越小的向量越相似。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="97f9" class="mx lm jj mt b gy my mz l na nb">from sklearn.metrics.pairwise import cosine_similarity</span><span id="d6f0" class="mx lm jj mt b gy nc mz l na nb"># define matrix with all zero values<br/>sim_mat = np.zeros([len(sentences),len(sentences)])</span><span id="24e9" class="mx lm jj mt b gy nc mz l na nb"># will populate it with cosine_similarity values <br/># for each sentences compared to other<br/>for i in range(len(sentences)):<br/>    for j in range(len(sentences)):<br/>        if i != j:<br/>            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]</span></pre><p id="d7c7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们将这个余弦相似性矩阵转换成一个图，其中节点代表句子，边代表句子之间的相似性得分。在这个图表上，将应用PageRank算法来得出每个句子的排名。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="08b5" class="mx lm jj mt b gy my mz l na nb">import networkx as nx</span><span id="c29a" class="mx lm jj mt b gy nc mz l na nb"># build graph and get pagerank<br/>nx_graph = nx.from_numpy_array(sim_mat)<br/>scores = nx.pagerank(nx_graph)</span><span id="cb76" class="mx lm jj mt b gy nc mz l na nb"># print final values of sentences<br/>scores</span></pre><p id="4521" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lh">输出:</em></p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="4406" class="mx lm jj mt b gy my mz l na nb">{0: 0.0651816121717921,<br/> 1: 0.0642861521750098,<br/> 2: 0.06399116048715114,<br/> 3: 0.06432009432128397,<br/> 4: 0.06385988469675835,<br/> 5: 0.06400525631019922,<br/> 6: 0.06520921510891638,<br/> 7: 0.06320537732857809,<br/> 8: 0.06298228524215846,<br/> 9: 0.06399491863786076,<br/> 10: 0.0640726538022174,<br/> 11: 0.06349704017361839,<br/> 12: 0.06357060319536506,<br/> 13: 0.057627597033478764,<br/> 14: 0.058463972076477785,<br/> 15: 0.05173217723913434}</span></pre><p id="4ffa" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们在最初的文章中有16个句子，所以我们得到了16个分数，即每个句子一个分数。是时候根据上面计算的排名选出前N个句子了。</p><h1 id="0898" class="ll lm jj bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">最后的步骤和结论</h1><p id="4ef1" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">如上所述，最终文本需要一些处理才能呈现。这些处理可以是大写每个句子的第一个字符，从每篇文章的开头删除位置名称，删除多余的空格/制表符/标点符号，纠正换行符等。</p><p id="b506" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们可以将所有这些步骤放在一起，创建一个摘要引擎/脚本。这个脚本可以被安排在每天早上选择的RSS提要上运行，并将新闻摘要发送到您的收件箱。这样你就不需要浏览所有文章来获取最新信息。或者您可以创建一个漂亮的HTML页面/小部件来显示主要出版物的新闻摘要。注意，在上面，我使用了单个RSS提要，但是在创建管道时，可以指定的RSS提要很少。此外，我使用了一些打印语句来显示中间值，可以删除这些中间值以获得无缝体验。</p><p id="9014" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望你喜欢这篇文章。如果你想分享关于任何方法的任何建议，请在评论中畅所欲言。读者的反馈/评论总是给作家带来灵感。</p></div></div>    
</body>
</html>