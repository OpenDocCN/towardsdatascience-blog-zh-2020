<html>
<head>
<title>Natural Language Processing with PySpark and Spark-NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PySpark和Spark-NLP进行自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-processing-with-pyspark-and-spark-nlp-b5b29f8faba?source=collection_archive---------7-----------------------#2020-02-05">https://towardsdatascience.com/natural-language-processing-with-pyspark-and-spark-nlp-b5b29f8faba?source=collection_archive---------7-----------------------#2020-02-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1511" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入阅读金融服务消费者投诉文本</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1872c12931655d7ca4cdbff882017792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*20yn0Csf8AqFXWfG"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">当我们深入挖掘客户投诉时，寻找宁静。汤姆·盖诺尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="kz"><p id="f182" class="la lb it bd lc ld le lf lg lh li lj dk translated">问题:哪些词(来自抱怨)明显是对等的？</p></blockquote><p id="ba76" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me lj im bi translated">今天，我们深入美国消费者金融保护局的<a class="ae ky" href="https://catalog.data.gov/dataset/consumer-complaint-database" rel="noopener ugc nofollow" target="_blank">金融服务消费者投诉数据库</a>，查看针对公司的投诉文本。问题:哪些词(来自抱怨)明显是对等的？我们将使用Spark-NLP研究文本清理、标记化和旅鼠，使用PySpark进行计数，以及tf-idf(术语频率-逆文档频率)分析。</p><p id="1772" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">我用了约翰·斯诺实验室的Spark-NLP库。你可以从<a class="ae ky" href="https://www.johnsnowlabs.com/spark-nlp/" rel="noopener ugc nofollow" target="_blank">他们</a>，或者<a class="ae ky" href="https://en.wikipedia.org/wiki/Spark_NLP" rel="noopener ugc nofollow" target="_blank">维基百科</a>(大概也是他们写的，只是风格不同)。</p><h1 id="5deb" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">安装Spark-NLP </strong></h1><p id="b39b" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">约翰·斯诺实验室提供了几个不同的快速入门指南——这里<a class="ae ky" href="https://nlp.johnsnowlabs.com/docs/en/quickstart" rel="noopener ugc nofollow" target="_blank">这里</a>和这里<a class="ae ky" href="https://github.com/JohnSnowLabs/spark-nlp" rel="noopener ugc nofollow" target="_blank">这里</a>——我发现一起使用很有用。</p><p id="4143" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">如果您尚未安装PySpark(注意:PySpark版本2.4.4是唯一受支持的版本):</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="e4b4" class="nm ml it ni b gy nn no l np nq">$ conda install pyspark==2.4.4<br/>$ conda install -c johnsnowlabs spark-nlp</span></pre><p id="069d" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">如果你已经有了PySpark，一定要把spark-nlp安装在和PySpark相同的通道里(你可以从conda列表里查看通道)。在我的例子中，PySpark安装在我的康达-福吉频道上，所以我使用</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="7a00" class="nm ml it ni b gy nn no l np nq">$ conda install -c johnsnowlabs spark-nlp — channel conda-forge</span></pre><p id="ad26" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">我已经安装了PySpark，并设置为与Jupyter笔记本一起使用，但是如果没有，您可能需要在终端中设置一些额外的环境变量(正如第二个快速入门指南中提到的，但不是第一个，所以…)</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="6431" class="nm ml it ni b gy nn no l np nq">$ export SPARK_HOME=/path/to/your/spark/folder<br/>$ export PYSPARK_PYTHON=python3<br/>$ export PYSPARK_DRIVER_PYTHON=jupyter<br/>$ export PYSPARK_DRIVER_PYTHON_OPTS=notebook</span></pre><h1 id="4a6f" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">Spark-NLP入门</strong></h1><p id="face" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">如果您希望使用预安装的数据集，因此不需要访问spark会话，可以从下面两行开始:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="8aa6" class="nm ml it ni b gy nn no l np nq">import sparknlp<br/>sparknlp.start()</span></pre><p id="44fd" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">在我的例子中，我需要SparkSession从parquet文件中加载我的数据，所以我将添加。config("spark.jars.packages "，" com . johnsnowlabs . NLP:spark-NLP _ 2.11:2 . 3 . 5 ")到我的SparkSession.builder</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="bfc1" class="nm ml it ni b gy nn no l np nq">from pyspark.sql import SparkSession</span><span id="8b55" class="nm ml it ni b gy nr no l np nq"># start spark session configured for spark nlp<br/>spark = SparkSession.builder \<br/>     .master('local[*]') \<br/>     .appName('Spark NLP') \<br/>     .config('spark.jars.packages', <br/>             'com.johnsnowlabs.nlp:spark-nlp_2.11:2.3.5') \<br/>     .getOrCreate()</span></pre><p id="1859" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">就是这样！你站起来了！</p><h1 id="7622" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">停止字</strong></h1><p id="89fd" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">Spark-NLP没有内置的停用词词典，所以我选择使用NLTK英语停用词，以及在我的数据集中找到的“xxxx”修订字符串。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="0efc" class="nm ml it ni b gy nn no l np nq">from nltk.corpus import stopwords</span><span id="9eba" class="nm ml it ni b gy nr no l np nq">eng_stopwords = stopwords.words('english')<br/>eng_stopwords.append('xxxx')</span></pre><h1 id="bcd2" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">设置您的文本操作管道</strong></h1><p id="7882" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">首先，导入您的工具:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="64be" class="nm ml it ni b gy nn no l np nq">from sparknlp.base import Finisher, DocumentAssembler<br/>from sparknlp.annotator import (Tokenizer, Normalizer,<br/>                                LemmatizerModel, StopWordsCleaner)<br/>from pyspark.ml import Pipeline</span></pre><p id="1846" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">大多数项目在开始时需要DocumentAssembler将文本转换成Spark-NLP注释器就绪的形式，在结束时需要Finisher将文本转换回人类可读的形式。您可以从注释器<a class="ae ky" href="https://nlp.johnsnowlabs.com/docs/en/annotators" rel="noopener ugc nofollow" target="_blank">文档</a>中选择您需要的注释器。</p><p id="f004" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">在设置管道之前，我们需要用适当的输入初始化注释器。我们将使用典型的Spark ML格式<code class="fe ns nt nu ni b">.setInputCols(<em class="nv">list of columns</em>)</code>和<code class="fe ns nt nu ni b">.setOutputCol(<em class="nv">output column name</em>)</code>，以及其他特定于注释器的函数(参见<a class="ae ky" href="https://nlp.johnsnowlabs.com/docs/en/annotators" rel="noopener ugc nofollow" target="_blank">文档</a>)。每个输出列都将是下面的注释器的输入列。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="fe1a" class="nm ml it ni b gy nn no l np nq">documentAssembler = DocumentAssembler() \<br/>     .setInputCol('consumer_complaint_narrative') \<br/>     .setOutputCol('document')</span><span id="fb9d" class="nm ml it ni b gy nr no l np nq">tokenizer = Tokenizer() \<br/>     .setInputCols(['document']) \<br/>     .setOutputCol('token')</span><span id="5daa" class="nm ml it ni b gy nr no l np nq"># note normalizer defaults to changing all words to lowercase.<br/># Use .setLowercase(False) to maintain input case.<br/>normalizer = Normalizer() \<br/>     .setInputCols(['token']) \<br/>     .setOutputCol('normalized') \<br/>     .setLowercase(True)</span><span id="f7e2" class="nm ml it ni b gy nr no l np nq"># note that lemmatizer needs a dictionary. So I used the pre-trained<br/># model (note that it defaults to english)<br/>lemmatizer = LemmatizerModel.pretrained() \<br/>     .setInputCols(['normalized']) \<br/>     .setOutputCol('lemma')</span><span id="643f" class="nm ml it ni b gy nr no l np nq">stopwords_cleaner = StopWordsCleaner() \<br/>     .setInputCols(['lemma']) \<br/>     .setOutputCol('clean_lemma') \<br/>     .setCaseSensitive(False) \<br/>     .setStopWords(eng_stopwords)</span><span id="bd83" class="nm ml it ni b gy nr no l np nq"># finisher converts tokens to human-readable output<br/>finisher = Finisher() \<br/>     .setInputCols(['clean_lemma']) \<br/>     .setCleanAnnotations(False)</span></pre><p id="5373" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">现在我们准备定义管道:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="d54d" class="nm ml it ni b gy nn no l np nq">pipeline = Pipeline() \<br/>     .setStages([<br/>           documentAssembler,<br/>           tokenizer,<br/>           normalizer,<br/>           lemmatizer,<br/>           stopwords_cleaner,<br/>           finisher<br/>     ])</span></pre><h1 id="c014" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">使用管道</strong></h1><p id="ac8b" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">我导入并选择我的数据，然后使用pipeline.fit(数据)。转换(数据)。例如:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="48f1" class="nm ml it ni b gy nn no l np nq"># import data<br/>df = spark.read.load('../data/consumer_complaints.parquet',<br/>                     inferSchema='true', header='true')</span><span id="84fa" class="nm ml it ni b gy nr no l np nq"># select equifax text data as test<br/>data = df.filter((df['company'] == 'EQUIFAX, INC.')<br/>           &amp; (df['consumer_complaint_narrative'].isNull() == False))<br/>data = data.select('consumer_complaint_narrative')</span><span id="fef1" class="nm ml it ni b gy nr no l np nq"># transform text with the pipeline<br/>equifax = pipeline.fit(data).transform(data)</span></pre><p id="8f1f" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">这将返回一个DataFrame，其中添加了管道注释器中指定的列。因此equifax.columns返回:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="8995" class="nm ml it ni b gy nn no l np nq">['consumer_complaint_narrative',<br/> 'document',<br/> 'token',<br/> 'normalized',<br/> 'lemma',<br/> 'clean_lemma',<br/> 'finished_clean_lemma']</span></pre><p id="49d5" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">当我们更仔细地观察整理器的输出时，在这个例子中是“finished_clean_lemma”，我们看到每个记录都是一个单词列表—例如<code class="fe ns nt nu ni b">[address, never, …]</code>、<code class="fe ns nt nu ni b">[pay, satisfied, …]</code>。</p><h1 id="72d6" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">对文本进行计数矢量化</strong></h1><p id="8769" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">为了让每个单词都在同一水平线上，我使用了<code class="fe ns nt nu ni b">pyspark.sql</code>分解功能。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="25ca" class="nm ml it ni b gy nn no l np nq">from pyspark.sql.functions import explode, col</span><span id="ecdf" class="nm ml it ni b gy nr no l np nq">equifax_words = equifax_words.withColumn('exploded_text', <br/>                               explode(col('finished_clean_lemma')))</span></pre><p id="7805" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">现在文本准备好<code class="fe ns nt nu ni b">.groupby().count()</code>来获得每个单词的计数。</p><p id="ab19" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">然后，我将结果转换为pandas，并使用字典理解将表转换为字典(这可能不是最优雅的策略)。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="340a" class="nm ml it ni b gy nn no l np nq">counts = equifax_words.groupby('exploded_text').count()<br/>counts_pd = counts.toPandas()<br/>equifax_dict = {counts_pd.loc[i, 'exploded_text']: <br/>                counts_pd.loc[i, 'count'] <br/>                for i in range(counts_pd.shape[0])}</span></pre><p id="b230" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">完全披露:即使使用在所有四个内核上运行的Spark，为前20名抱怨者做这件事也要花费大量的时间——这是大量的计算！</p><h1 id="bb61" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak"> Tf-idf </strong></h1><p id="5eaf" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">现在我已经将每个公司的投诉计数的文本矢量化(也就是转换成{word1: count1，word2: count2…})了，我已经准备好获取每个公司单词集中每个单词的tf-idf。</p><p id="28f4" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">助手功能:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="a9df" class="nm ml it ni b gy nn no l np nq">def term_frequency(BoW_dict):<br/>     tot_words = sum(BoW_dict.values())<br/>     freq_dict = {word: BoW_dict[word]/tot_words <br/>                  for word in BoW_dict.keys()}<br/>     return freq_dict</span><span id="6657" class="nm ml it ni b gy nr no l np nq">from math import log</span><span id="a8d8" class="nm ml it ni b gy nr no l np nq">def inverse_document_frequency(list_of_dicts):<br/>    tot_docs = len(list_of_dicts)<br/>    words = set([w for w_dict in list_of_dicts <br/>                   for w in w_dict.keys()])<br/>    idf_dict = {word: log(float(tot_docs)/<br/>                      (1.0 + sum([1 for w_dict in list_of_dicts <br/>                              if word in w_dict.keys()]))) <br/>                    for word in words}<br/>    return idf_dict</span><span id="5c41" class="nm ml it ni b gy nr no l np nq">def tf_idf(list_of_dicts):<br/>     words = set([w for w_dict in list_of_dicts <br/>                  for w in w_dict.keys()])<br/>     tf_idf_dicts = []<br/>     idfs = inverse_document_frequency(list_of_dicts)<br/>     for i, w_dict in enumerate(list_of_dicts):<br/>          w_dict.update({word: 0 for word in words <br/>                         if word not in w_dict.keys()})<br/>          tf = term_frequency(w_dict)<br/>          tf_idf_dicts.append({word: tf[word]*idfs[word] <br/>                               for word in words})<br/>     return tf_idf_dicts</span></pre><p id="1354" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">综合起来看:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="4e10" class="nm ml it ni b gy nn no l np nq">list_of_word_dicts = [company_complaint_word_counts_dict[company] <br/>                      for company in companies]<br/>tf_idf_by_company_list = tf_idf(list_of_word_dicts)<br/>tf_idf_by_company_dict = {c: tf_dict <br/>           for c, tf_dict in zip(companies, tf_idf_by_company_list)}</span></pre><h1 id="9933" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated"><strong class="ak">我们每个顶级公司的投诉有什么独特之处</strong></h1><p id="876c" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">为了找到让每家公司与众不同的词，我找到了我感兴趣的公司中tf-idf得分最高的词。</p><p id="b00e" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">不幸的是，这表明我没有做足够的工作来清理我的数据。所有tf-idf得分最高的单词都是错别字或组合在一起的单词(如“tobe”、“calledthem”等)。).有时候1000个字符串不带空格。所以在这里，我添加了一个基于<code class="fe ns nt nu ni b">nltk.corpus words.words()</code>列表的过滤器。不幸的是，tobe实际上是一个单词，所以它仍然出现在一些热门搜索结果中。鉴于数据集中缺少空间的问题，我怀疑人们是否在谈论“北非和中非一些地区的传统外衣，由一段布料缝制成宽松的长裙或披在身上并系在一个肩膀上”(<a class="ae ky" href="https://www.collinsdictionary.com/us/dictionary/english/tobe" rel="noopener ugc nofollow" target="_blank"> Collins Dictionary </a>)。</p><p id="2157" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">让我们来看看我们十大公司的一些顶级tf-idf评分词:</p><p id="ace6" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">Equifax:经销商，原告，重新插入，可证明，可证明，运行</p><p id="c685" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">益百利:经销商，原告，可证明的，可证明的，运行者，再插入</p><p id="e3b0" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">跨国联盟:经销商，原告，可证明的，可证明的，顺从的，重新插入</p><p id="51a7" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">美银:商户、止赎、柜员、白金、梅隆(？)，火器，mesne</p><p id="2df6" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">富国银行:保护，止赎，评估，保护主义者，出纳员，交易商</p><p id="4443" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">摩根大通:蓝宝石，西南，商人，航空公司，探险家，出纳员</p><p id="cdd7" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">花旗银行:仓库，促销，固特异，威望，商人，股息</p><p id="7535" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">资本一:科尔，水银，品味，果园，商人，风险，再比尔</p><p id="a533" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">Navient解决方案:先锋、mae、无补贴、文凭、再认证、毕业</p><p id="61af" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">奥克文金融:回家，取消抵押品赎回权，悬念，流氓，复式</p></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><p id="4522" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">我们确实看到了不同类别的金融机构，金融局从提供更多抵押贷款和店面银行业务的银行以及美国教育部贷款服务机构Navient得出了不同的结果。</p><p id="e43c" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">同时，这个结果也相当令人沮丧。这些最重要的话，感觉不是超级有见地。我在想:</p><ol class=""><li id="251b" class="od oe it lm b ln mf lq mg lt of lx og mb oh lj oi oj ok ol bi translated">也许我们需要看的不仅仅是这些热门词汇，比如前100名？</li><li id="ce99" class="od oe it lm b ln om lq on lt oo lx op mb oq lj oi oj ok ol bi translated">或者，tf-idf可能不是这项工作的合适工具。由于对这些公司的投诉如此之多，许多词在所有或几乎所有公司的投诉“文集”中至少出现一次。这个因素，逆文档频率，压倒了文本频率。我想知道如果我使用所有的公司，而不仅仅是前20名，结果会有什么不同。</li></ol></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><p id="9389" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">一如既往，在GitHub <a class="ae ky" href="https://github.com/allisonhonold/spark-blog-tfidf" rel="noopener ugc nofollow" target="_blank"> repo </a>中找到更多(代码)。</p><p id="9a88" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">编码快乐！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/1aa57793e64de13a6c874afd85b1bdc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zcj9nDxFJqlVazla"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@schmidy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Austin Schmid </a>拍摄的照片</p></figure></div></div>    
</body>
</html>