<html>
<head>
<title>End-to-end AWS Quantitative Analysis: Automatically Running a Script Using AWS and AWSCLI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">端到端AWS定量分析:使用AWS和AWSCLI自动运行脚本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/end-to-end-quantitative-trading-automating-ml-using-awscli-83035b4f6d03?source=collection_archive---------40-----------------------#2020-03-02">https://towardsdatascience.com/end-to-end-quantitative-trading-automating-ml-using-awscli-83035b4f6d03?source=collection_archive---------40-----------------------#2020-03-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6328579ec11a2dfe68e9e4e6984d9085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tlFVc7Jkfn3ThePhFPv9rQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">凯文·Ku从<a class="ae jg" href="https://www.pexels.com/photo/coding-computer-data-depth-of-field-577585/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a>拍摄的照片</p></figure><h2 id="d397" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/building-an-aws-pipeline" rel="noopener" target="_blank">建设AWS管道</a></h2><div class=""/><div class=""><h2 id="eb54" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">使用AWSCLI旋转、运行和拆除AWS服务器</h2></div><p id="58aa" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<a class="ae jg" rel="noopener" target="_blank" href="/end-to-end-quantitative-trading-part-1-798dcfeb165a">之前的一篇文章</a>中，我们探讨了如何使用Python从在线资源(在我们的例子中，是Yahoo！金融)。<br/>我们开发了两个Python脚本；一个为我们抓取数据，一个使用sklearn的决策树分类器处理数据。然后，我们将它们上传到AWS上的一个S3存储桶中，以便安全保管，也便于从其他AWS服务中访问。<br/>在本帖中，我们将探讨如何建立一个工作流，为我们创建一个处理服务器，在该服务器上训练我们的数据模型，然后在完成后移除该服务器。</p><h1 id="44de" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">在亚马逊网络服务(AWS)上运行一组模型</h1><p id="3b19" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">AWS的定价模式是“按使用付费”，这意味着我们可以创建一个服务器，在其上进行一些数据处理，然后拆除它，我们将只为服务器运行的时间付费。此外，我们将使用一个名为AWSCLI(AWS Client)的客户端库来启动服务器(创建服务器的时髦术语)和关闭它。我从Brent Lemieux 的一篇<a class="ae jg" rel="noopener" target="_blank" href="/production-data-processing-with-apache-spark-96a58dfd3fe7">帖子中开始了解我需要什么来启动服务器，我强烈建议你去看看那篇帖子。</a></p><h1 id="c2a1" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">配置我们的客户端应用程序</h1><p id="ea30" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">首先，我们将安装和配置AWSCLI。我将使用AWSCLI v2。一旦安装，我们可以测试它的工作</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="1418" class="nj me jj nf b gy nk nl l nm nn">aws2 --version<br/>&gt;&gt;&gt; aws-cli/2.0.0dev2 Python/3.7.5 Windows/10 botocore/2.0.0dev1</span></pre><p id="d10e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后我们配置它。在配置AWSCLI <a class="ae jg" href="https://docs.aws.amazon.com/general/latest/gr/rande.html" rel="noopener ugc nofollow" target="_blank">时，本页</a>可能对默认区域选择有用。</p><p id="c558" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们还应该通过运行</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="bec2" class="nj me jj nf b gy nk nl l nm nn">aws2 emr create-default-roles</span></pre><h1 id="2607" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">创建引导文件</h1><p id="745c" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">配置完成后，我们可以使用AWSCLI并启动我们的服务器。阅读Brent Lemieux的文章，他描述了如何创建一个引导文件来为我们初始化服务器。你会注意到我们需要在服务器上安装像pandas和matplotlib这样的包，因为它只附带了很少的python包。</p><p id="2f37" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这将是我们的引导文件:</p><figure class="na nb nc nd gt iv"><div class="bz fp l di"><div class="no np l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">自举_文件. sh</p></figure><ul class=""><li id="210b" class="nq nr jj lj b lk ll ln lo lq ns lu nt ly nu mc nv nw nx ny bi translated">第一行设置<code class="fe nz oa ob nf b">python</code>命令指向python 3(目前是3.6)而不是默认的2.7(为什么默认还是2.7？贝佐斯，这已经过去6年了。这样，我们漂亮的脚本将由python 3.x运行</li><li id="2127" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated">第二个命令使用python来安装我们需要的库。在这里，您可以随意添加任何其他对您有用的库。</li><li id="0de9" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated">第三个命令将我们的<code class="fe nz oa ob nf b">workflow-scripts</code> bucket的内容复制到集群中。<strong class="lj jt">你必须用你的桶名替换</strong> <code class="fe nz oa ob nf b"><strong class="lj jt">workflow-scripts</strong></code> <strong class="lj jt">。</strong></li><li id="f293" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated">第四行运行将为我们获取数据的脚本(您可以在这里获取<a class="ae jg" href="https://gist.github.com/bshabashFD/dc7cf0fa4eb20273c003cd55fc148fbe" rel="noopener ugc nofollow" target="_blank"/>)，并将数据存储在适当的S3桶中。<strong class="lj jt">你必须用你的桶名</strong>替换 <code class="fe nz oa ob nf b"><strong class="lj jt">data-files</strong></code> <strong class="lj jt">。</strong></li></ul><p id="32f4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个引导脚本将是我们的服务器建立后运行的第一个项目。</p><h1 id="e205" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">配置对S3的读/写权限</h1><p id="95b0" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">我们现在有了设置脚本和基本的数据处理脚本。我们差不多准备好运行我们的代码了。我们需要确保我们的脚本可以读取和写入S3桶。</p><p id="ce08" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将向默认角色添加一个策略来允许这一点。<br/>这是什么意思？嗯，当我们创建一个EMR集群(一组为我们执行处理的计算机)时，我们给该集群分配一个角色，这意味着我们给它一些权限，让它在集群中安装库。这就像是我们的集群能够做的动作的集合。希望我们按照指示创建了默认角色<strong class="lj jt">。</strong>但是，这些角色不包括与私有的S3存储桶进行交互的权限。有两种方法可以解决这个问题:</p><ul class=""><li id="9e09" class="nq nr jj lj b lk ll ln lo lq ns lu nt ly nu mc nv nw nx ny bi translated">让我们的桶中的文件完全公开(真的真的真的真的真的真的很糟糕的主意)</li><li id="47de" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated">在AWS配置上投入更多时间，并让一些角色能够与我们拥有的S3资源进行交互(好得多)</li></ul><p id="60a0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">导航到AWS中的IAM控制台，我们将找到所有可用角色的列表:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f3d6eed3a0b7fc5c061684409f59a5ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IY_mV-TKECf7WzHsEW5_tw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">EMR/EC2访问的默认角色</p></figure><p id="4635" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，我们将单击每个默认角色，并选择添加一个内联策略</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/c90be321a54e86d78bd345d72383aa8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-xhNFRqYCxUDvdYR-kfa6g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">添加内嵌策略</p></figure><p id="5fd7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，我们将向我们的角色添加完整的读写策略，如下所示:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9da3d49ba0b4df02ce0b1b89da616b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jfJbT972mvnb7czI-BM98Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">S3资源的读写策略</p></figure><p id="a125" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">实际上，我们可以更具体地说明我们的政策，以及我们的角色如何与我们的S3团队互动。现在，我们只需为所有四个资源组(访问点、存储桶、作业和对象)选择<strong class="lj jt"> Any </strong>，我们就可以检查并批准策略了。</p><p id="5494" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果有需要，应该选择更严格的条件。</p><h1 id="b2ae" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">纱线的配置文件</h1><p id="a3f4" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">我们要为这个演示做的事情将需要我们机器上的大量RAM，超过默认值所允许的。所以我们必须创建一个配置文件来帮助我们避开这个问题。</p><p id="9f0d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一般来说，大数据通常不会完全加载到RAM中，而是使用HIVE或Spark进行交互。然而，为了将Spark与我们经典的机器学习方法进行比较，我们将反叛并将大量数据加载到RAM中。</p><p id="1623" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们在本地机器上创建一个名为config.json的配置文件。</p><figure class="na nb nc nd gt iv"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="9682" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">它处理我们的资源管理器YARN(另一个资源协商器)，并告诉我们可以为整个应用程序分配多少内存，以及告诉YARN不要检查内存违规(整个内存中只有一小部分会分配给我们的Python脚本，因此我们需要停止检查这一小部分之外的违规)。</p><p id="6b36" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">同样，这是为了比较我们的基础<code class="fe nz oa ob nf b">sklearn</code>决策树，而不是处理大数据的方式。</p><h1 id="4a94" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">为我们的集群设置身份验证</h1><p id="489c" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">我们必须从EC2仪表板创建一个密钥对，并将其下载到我们的计算机上</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/7313e9064b53cffc361014913d2334b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HrzHqhCLYi5jfFCoDLe6zQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">EC2密钥对接口</p></figure><p id="06db" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦下载了密钥对，就必须通过运行命令使其可读</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="d336" class="nj me jj nf b gy nk nl l nm nn">chmod 400 my-key-pair.pem</span></pre><p id="70bb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在文件上使用终端(Mac)或Git-Bash (Windows)。这允许您稍后连接到启动的实例(如果它正在运行)。</p><h1 id="4bb0" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">通过AWS CLI启动EMR集群</h1><p id="2849" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">我们期待已久的时刻终于到来了。使用AWSCLI，我们可以在家中舒适地启动一个集群来为我们做一些工作。为此，我们将运行以下命令:</p><pre class="na nb nc nd gt ne nf ng nh aw ni bi"><span id="b0d1" class="nj me jj nf b gy nk nl l nm nn">aws2 emr create-cluster \<br/>--name "<strong class="nf jt">Spark cluster with step</strong>" \<br/>--release-label emr-5.29.0 \<br/>--applications Name=Spark \<br/>--log-uri <strong class="nf jt">s3://log-bucket/logs/</strong> \<br/>--ec2-attributes KeyName=<strong class="nf jt">my-key-pair</strong> \<br/>--instance-type m5.2xlarge \<br/>--instance-count 1 \<br/>--bootstrap-actions Path=<strong class="nf jt">s3://scripts-and-setup/bootstrap_file.sh</strong> \<br/>--configuration file://config.json \<br/>--steps Name="Command Runner",Jar="command-runner.jar",Args=["spark-submit","--deploy-mode=cluster","--executor-memory","20G","<strong class="nf jt">s3://workflow-script/process_data.py</strong>, <strong class="nf jt">s3://data-files/</strong>AAPL.csv"] \<br/>--use-default-roles \<br/>--auto-terminate</span></pre><p id="9bd3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个命令有很多东西，所以让我们打开重要的部分:</p><ul class=""><li id="9b13" class="nq nr jj lj b lk ll ln lo lq ns lu nt ly nu mc nv nw nx ny bi translated"><code class="fe nz oa ob nf b">aws2 emr create-cluster</code> —希望这是不言自明的</li><li id="c79c" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated"><code class="fe nz oa ob nf b">--name "<strong class="lj jt">Spark cluster with step</strong>"</code> —用您喜欢的任何名称替换此名称，但这将是集群的名称</li><li id="13b2" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated"><code class="fe nz oa ob nf b">--release-label emr-5.29.0</code> —要使用的集群版本。不同版本的集群包含各种其他应用程序的不同版本，如Hadoop、Spark、Hive等</li><li id="e3b4" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated"><code class="fe nz oa ob nf b">--applications Name=Spark</code> —我们将推出一款spark应用</li><li id="7276" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated"><code class="fe nz oa ob nf b">--log-uri <strong class="lj jt">s3://log-bucket/logs/</strong></code> <strong class="lj jt"> — </strong>这是我们将日志发送到的地方。您需要一个日志存储桶，以防您的集群出现故障，并且您可以查看出错原因的日志。你必须用你的桶替换它。</li><li id="8456" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated"><code class="fe nz oa ob nf b">--ec2-attributes <strong class="lj jt">KeyName=my-key-pair</strong></code> <strong class="lj jt"> — </strong>这是一个密钥对，如果需要，您可以使用它来建立到集群的ssh连接</li><li id="0fbb" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated"><code class="fe nz oa ob nf b">--instance-type m5.2xlarge</code> —这是我们将使用的实例类型和大小。AWSCLI不支持小于xlarge的</li><li id="58f0" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated"><code class="fe nz oa ob nf b">--instance-count 1</code> —我们将只启动一个实例(实际上并不是一个集群，但是我们将在下一篇文章中讨论)</li><li id="c255" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated"><code class="fe nz oa ob nf b">--bootstrap-actions Path=<strong class="lj jt">s3://scripts-and-setup/bootstrap_file.sh</strong></code> —这是命令的一部分，告诉集群使用哪个脚本来启动集群。您必须用您的脚本文件位置替换它</li><li id="a0bb" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated"><code class="fe nz oa ob nf b">--configuration file://config.json</code> —这告诉AWSCLI使用我们创建的config.json文件，前缀<code class="fe nz oa ob nf b">file://</code>告诉AWSCLI这是一个本地文件(尽管我们也可以将它存储在AWS S3上)</li><li id="4238" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated">这个很密集，所以让我们把它再分开一些。首先，我们将使用<code class="fe nz oa ob nf b">command-runner.jar</code> Jar文件来运行我们的python脚本。这是一个Java可执行文件，它将运行我们的python脚本并管理所有与Spark相关的工作。<br/>成那个样子。jar文件我们发几个参数<code class="fe nz oa ob nf b">Args</code>。它们是<code class="fe nz oa ob nf b">spark-submit</code>，它说这个任务被提交给Spark框架，<code class="fe nz oa ob nf b">--deploy-mode=cluster</code>，它说我们在集群内部启动我们的可执行文件(相对于从我们的家，非AWS机器)，我们要求Spark框架在运行我们的脚本和它的其他步骤时使用20GB的内存(实际上我们的脚本变得有点少，因为内存分配是如何在Spark和YARN之间工作的)用命令<code class="fe nz oa ob nf b">"--executor-memory"</code>和<code class="fe nz oa ob nf b">"20G"</code>， 最后是最后一个参数<br/> <code class="fe nz oa ob nf b"><strong class="lj jt">s3://workflow-scripts/process_data.py</strong>, <strong class="lj jt">s3://data-files/AAPL.csv</strong></code>，通过AWS的魔法，它被解析成<br/> <code class="fe nz oa ob nf b"><strong class="lj jt">s3://workflow-scripts/process_data.py</strong> <strong class="lj jt">s3://data-files/AAPL.csv</strong></code>(注意逗号被空格替换，形成一个完整的命令)。 这是Spark环境将运行的完整命令，这是我们的Python脚本(<a class="ae jg" href="https://gist.github.com/bshabashFD/80fec67dfc10c19e6eff38ed3fbb13f6" rel="noopener ugc nofollow" target="_blank">下载到这里</a>)及其所有参数。对于这一步，您必须用自己合适的名称替换存储桶名称。</li><li id="a884" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated"><code class="fe nz oa ob nf b">--use-default-roles</code> —告诉AWS使用我们之前设置的默认角色</li><li id="3dad" class="nq nr jj lj b lk oc ln od lq oe lu of ly og mc nv nw nx ny bi translated">这是一个真正美丽的论点。它告诉集群在所有处理完成后终止，因此我们不会为集群无所事事地坐在那里并收取我们的信用卡费用而付费。如果为了调试的目的，您想要保持集群运行，您可以省略它或者使用显式版本<code class="fe nz oa ob nf b">--no-auto-terminate</code>。<strong class="lj jt">但是，确保手动终止，然后</strong>。</li></ul><h1 id="77d1" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">我们运行的结果</h1><p id="4615" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">在我的机器上输入命令后，集群启动用了大约2秒钟，然后又用了12分钟来完成我要求的一切并终止。当检查S3存储桶的日志时，决策树分类器报告已经运行了8分钟进行拟合和预测。</p><p id="15b2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">到目前为止，我们已经启动了一个节点来为我们做一些处理，请注意，我们已经从使用比家用笔记本电脑更强大的机器中获益。我们使用sklearn进行分析的代码现在在一台功能强大的计算机上运行了大约12分钟。<br/>虽然这个集群在分布式计算领域还没有做太多工作，但我们已经做好了基础工作，可以看到Spark在发挥作用。我在这里提到过几次Spark，在下一篇文章中，我将最终展示这个框架在我们用决策树探索的相同数据上的实际应用。</p><h1 id="9652" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">迄今为止的总成本</h1><p id="1477" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">我想在这里停下来谈一谈成本。实践和磨练AWS技能的最大摩擦点之一是这样一个事实，即在大多数数据科学信息和技能都是免费的(来自Kaggle的免费数据集，Python和R等免费编程工具)的世界中，AWS需要花钱，而且直到第二天收到账单时才完全清楚要花多少钱。<br/>在学习和尝试正确设置的过程中，我已经旋转(设置)并拆除了100多个集群，到目前为止，我的总账单是注册AWS大约1美元，加上所有这些集群不到5美元。到目前为止，我便宜的账单很大程度上是因为我的每个“集群”中只有一个实例(<code class="fe nz oa ob nf b">--instance-count 1</code>)，并且我使用类型为<code class="fe nz oa ob nf b">m5.xlarge</code>的集群练习了大多数旋转和拆卸。我把数据帧的尺寸缩小到原型，最终只在<code class="fe nz oa ob nf b">m5.2xlarge</code>上做了原型。</p></div></div>    
</body>
</html>