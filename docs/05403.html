<html>
<head>
<title>Multi-class Sentiment Analysis using BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 BERT 的多类别情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-class-sentiment-analysis-using-bert-86657a2af156?source=collection_archive---------5-----------------------#2020-05-07">https://towardsdatascience.com/multi-class-sentiment-analysis-using-bert-86657a2af156?source=collection_archive---------5-----------------------#2020-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8965" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于 BERT 的 Yelp 评论多类别文本情感分析的简单快速实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8351174744d51affe7d3741c56dc7700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WyUZuP7MEXgJohK3nvQcLg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@tengyart?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">腾雅特</a>在<a class="ae ky" href="https://unsplash.com/s/photos/emotions?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="32b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">在本文中，我们将使用 BERT 对 Yelp 评论进行多类文本分类。</em></p><h2 id="bea8" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">先决条件:</h2><p id="4ff0" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/intuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e">变压器(BERT)双向编码器表示的直观解释</a></p><p id="466b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从<a class="ae ky" href="https://github.com/google-research/bert.git" rel="noopener ugc nofollow" target="_blank">这里</a>克隆或下载 BERT Github 库</p><p id="a1a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">点击从<a class="ae ky" href="https://github.com/google-research/bert#pre-trained-models" rel="noopener ugc nofollow" target="_blank">下载 BERT 预训练重量。</a></p><p id="8a44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">点击从<a class="ae ky" href="https://course.fast.ai/datasets#nlp" rel="noopener ugc nofollow" target="_blank">下载 Yelp 评论数据集</a></p><h2 id="1f95" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">BERT 概述</h2><p id="d23a" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">BERT 是一个<strong class="lb iu">深度双向表示模型，用于</strong>通用“语言理解”<strong class="lb iu">从左到右和从右到左学习信息。</strong> BERT 是根据从图书语料库(800 万字)<strong class="lb iu">和英语维基百科</strong>(2500 万字)中提取的未标注数据进行预训练的</p><p id="dcd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">伯特有两种型号</p><ol class=""><li id="91f8" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated"><strong class="lb iu"> BERT-base: </strong> <strong class="lb iu"> 12 个编码器，带 12 个双向自关注头</strong></li><li id="b774" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> BERT-large: </strong> <strong class="lb iu"> 24 个编码器，带 24 个双向自聚焦头</strong></li></ol><p id="10ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT-base 和 BERT-large 都有有壳和无壳版本。</p><ul class=""><li id="047e" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu ni na nb nc bi translated"><strong class="lb iu"> BERT 不区分大小写</strong>，其中文本在单词块标记化之前已经小写。例如，“詹姆斯·邦德”变成了“詹姆斯·邦德”。它还会删除重音标记。</li><li id="2d38" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ni na nb nc bi translated"><strong class="lb iu">带大小写的</strong>保留真实大小写和重音标记。</li></ul><p id="7e89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数情况下，我们将使用 BERT-Uncased，除非用例要求保留对于 NLP 任务至关重要的案例信息。</p><h2 id="1047" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">BERT 的基本设置</h2><p id="f481" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我们需要 Tensorflow 版本 1.11 或更高版本来运行 BERT 代码。</p><p id="cf27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下载预训练权重和数据集后，我们需要对多类分类进行一些更改。</p><p id="c17c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们克隆或下载的代码会有一个文件<strong class="lb iu"><em class="lv">run _ classifier . py</em></strong>。我们需要更新<strong class="lb iu"> <em class="lv">类 ColaProcessor </em> </strong>中的方法<strong class="lb iu"> <em class="lv"> get_labels </em> </strong>()，如下所示，用于多类文本分类</p><p id="5a1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">二元分类的原始代码</strong></p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="4622" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu">def get_labels(self):<br/> return [“0”, “1”]</strong></span></pre><p id="218a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">修改了多类文本分类的代码</strong></p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="15b8" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu">def get_labels(self):<br/> return [“0”, “1”, “2”, “3”, “4”]</strong></span></pre><h2 id="cd26" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">为 BERT 构建数据集</h2><p id="563d" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated"><strong class="lb iu">导入数据处理所需的库</strong></p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="9253" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu"><em class="lv">import numpy as np<br/>import pandas as pd<br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.model_selection import train_test_split</em></strong></span></pre><p id="a284" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">读取从文件中提取的 Yelp 审查训练和测试数据</strong></p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="d9ef" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu"><em class="lv">data_train= pd.read_csv(“yelp_review_full_csv\\train.csv”, header=None)</em></strong></span><span id="1b3f" class="lw lx it nk b gy ns np l nq nr"><strong class="nk iu"><em class="lv">data_test= pd.read_csv("yelp_review_full_csv\\test.csv", header=None)</em></strong></span><span id="8000" class="lw lx it nk b gy ns np l nq nr"><strong class="nk iu"><em class="lv">data_train.head(5)</em></strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7f61b62ac0fbc0f7b5b5a2d82ab411f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*lKNA0tCBejePPyWHwYaDzw.png"/></div></figure><p id="d62f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">查找类别标签的唯一值</strong></p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="8e1c" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu">data_train[0].unique()</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/6725e0ab7699d43fa3316f490e70c8cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*7NzyXUrwlSB1nbvS8FcT4g.png"/></div></figure><p id="3148" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值为 1 表示差评，值为 5 表示优评。</p><p id="19d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能希望类标签从 0 开始，所以我们从当前标签值中减去 1。现在，值为 0 表示差评，值为 4 表示差评</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="08ee" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu">data_train[0] = (data_train[0] -1)<br/>data_test[0] = (data_test[0] -1)<br/>data_train[0].unique()</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c632b830952dc121a630f58d73b63a22.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*ciQW_VQ0UbED4s8pwjRZZA.png"/></div></figure><p id="0d84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新多类文本分类的 BERT 码</strong></p><p id="3e77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在文件<strong class="lb iu"> <em class="lv"> run_classifier.py、</em> </strong>中修改<strong class="lb iu"> <em class="lv"> </em> </strong>方法<strong class="lb iu"><em class="lv">get _ labels</em></strong>()<strong class="lb iu"><em class="lv">类 ColaProcessor </em> </strong>并更新标签以匹配我们在训练数据中的内容</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="71b4" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu">def get_labels(self):<br/> return [“0”, “1”, “2”, “3”, “4”]</strong></span></pre><p id="ad3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 BERT 中处理不同 NLP 任务的类要求数据采用. tsv(制表符分隔值)文件格式的特定格式。</p><ul class=""><li id="6ad0" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu ni na nb nc bi translated"><strong class="lb iu">列 0: GUID: </strong>该行的 ID。训练和测试数据都需要</li><li id="fd3d" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ni na nb nc bi translated"><strong class="lb iu">第 1 列:第</strong>行的分类标签。该列的值应该与方法<strong class="lb iu"><em class="lv">get _ labels</em></strong>(<strong class="lb iu"><em class="lv">cola processor</em><strong class="lb iu"><em class="lv"/></strong>文件<strong class="lb iu"><em class="lv">run _ classifier . py</em></strong>中指定的值相匹配。仅训练数据需要。</strong></li><li id="6926" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ni na nb nc bi translated"><strong class="lb iu">列 2: alpha: </strong>这是用于文本分类的虚拟列，但是对于 BERT 的训练数据是需要的</li><li id="032b" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ni na nb nc bi translated"><strong class="lb iu">第 3 列:文本:这是要分类的文本</strong>。训练和测试数据都需要此列</li></ul><p id="b50e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">为培训、开发和测试构建 BERT 数据集</strong></p><p id="dd5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT 需要三个不同的数据集:用于训练的 train 和 dev 以及用于预测的 test</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="813f" class="lw lx it nk b gy no np l nq nr"># Creating dataframe according to BERT data requirements by adding the required columns in the right order</span><span id="aa2b" class="lw lx it nk b gy ns np l nq nr"><strong class="nk iu"><em class="lv">df_bert = pd.DataFrame({<br/> ‘id’:range(len(data_train)),<br/> ‘label’:data_train[0],<br/> ‘alpha’:[‘a’]*data_train.shape[0],<br/> ‘text’: data_train[1].replace(r’\n’, ‘ ‘, regex=True)<br/>})</em></strong></span><span id="0eba" class="lw lx it nk b gy ns np l nq nr"><strong class="nk iu"><em class="lv">df_bert.head()</em></strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/e36abe418e77e34e5adf09659038eb6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*UdXOq5R7pjaFQpbq5SWEPA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">符合 BERT 要求格式的数据</p></figure><p id="5beb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将数据分为训练和开发或评估数据集</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="24f6" class="lw lx it nk b gy no np l nq nr"># Splitting training data file into *train* and *test*<br/><strong class="nk iu"><em class="lv">df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)</em></strong></span></pre><p id="fb29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">构建测试数据集</strong></p><p id="87d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在测试数据集中只需要两列</p><ul class=""><li id="2e2b" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu ni na nb nc bi translated"><strong class="lb iu">列 0: GUID: </strong>行的 ID。</li><li id="2899" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ni na nb nc bi translated"><strong class="lb iu">第 1 列:文本:</strong>这是要分类的文本。</li></ul><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="475c" class="lw lx it nk b gy no np l nq nr"># Creating test dataframe according to BERT<br/><strong class="nk iu">df_bert_test = pd.DataFrame({<br/> ‘id’:range(len(data_test)),<br/> ‘text’: data_test[1].replace(r’\n’, ‘ ‘, regex=True)<br/>})</strong></span><span id="36c2" class="lw lx it nk b gy ns np l nq nr"><strong class="nk iu">df_bert_test.tail()</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/5971673cfea0725ea6bf91b97820e465.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*CfqP4iGnHSnivKxRr6M4Zg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">根据 BERT 测试数据集</p></figure><p id="dc66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将数据帧保存到不带标题的. tsv 文件，用于培训和开发，但文件 test.tsv 文件需要标题</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="66e6" class="lw lx it nk b gy no np l nq nr"># Saving dataframes to .tsv format as required by BERT<br/><strong class="nk iu">df_bert_train.to_csv(‘yelp_review_full_csv\\train.tsv’, sep=’\t’, index=False, header=False)</strong></span><span id="85df" class="lw lx it nk b gy ns np l nq nr"><strong class="nk iu">df_bert_dev.to_csv(‘yelp_review_full_csv\\dev.tsv’, sep=’\t’, index=False, header=False)</strong></span><span id="a070" class="lw lx it nk b gy ns np l nq nr"><strong class="nk iu">df_bert_test.to_csv(‘yelp_review_full_csv\\test.tsv’, sep=’\t’, index=False, header=True)</strong></span></pre><h2 id="28f9" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">训练 BERT 模型</h2><p id="bc4c" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我们将加载预训练的基于 BERT 的无案例模型权重，并在 Yelp 评论数据集上训练该模型。让我们了解一下我们需要为培训设置的参数。</p><p id="bd90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要训练多类文本分类，因此我们将使用 run_classifier.py 文件。</p><ol class=""><li id="f194" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated"><strong class="lb iu"> task_name </strong>:这是我们计划用于培训的任务。选项有</li></ol><ul class=""><li id="1f3f" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu ni na nb nc bi translated"><strong class="lb iu"> CoLA </strong>:语言可接受性的<strong class="lb iu">语料库是一个二元单句分类任务，是 ColaProcessor 类。目标是预测一个英语句子在语言学上是否“可接受”</strong></li><li id="bf7c" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ni na nb nc bi translated"><strong class="lb iu"> MNLI </strong> : <strong class="lb iu">多体裁自然语言推理</strong>是一项大规模、众包的蕴涵分类任务。给定一对句子，目标是预测第二个句子相对于第一个句子是蕴涵、矛盾还是中性的</li><li id="6b37" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ni na nb nc bi translated"><strong class="lb iu"> MRPC:微软研究院释义语料库</strong>由从在线新闻来源自动提取的句子对组成。目标是检查句子对中的句子是否语义等价</li><li id="544c" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ni na nb nc bi translated"><strong class="lb iu"> XNLI:跨语言自然语言推理</strong>使用了 15 种语言的跨语言句子分类。</li></ul><p id="3bde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.<strong class="lb iu"> do_train </strong>:设置为 True 进行训练。这将使用 train.tsv 文件进行训练</p><p id="a8e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.<strong class="lb iu"> do_eval </strong>:设置为 True 进行评估。这将使用 dev.tsv 文件进行评估</p><p id="5283" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.<strong class="lb iu"> data_dir </strong>:包含数据的目录。tsv 文件</p><p id="28b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.<strong class="lb iu"> vocab_file </strong>:指定 vocab.txt 文件。Vocab.txt 使用 BERT 提供的 Vocab 文件将单词映射到索引。词汇表有 119，547 个单词块标记</p><p id="8bcd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">6.<strong class="lb iu"> bert_config_file </strong>:包含 bert 模型的参数值。伯特预训练模型有这个文件</p><p id="8b22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">7.<strong class="lb iu"> init_checkpoint </strong>:初始检查点，通常从预先训练好的检查点开始。如果您是从中断的地方恢复训练过程，或者训练被中断，则提到最后一个检查点文件名。</p><p id="8f39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">8.<strong class="lb iu"> max_seq_length </strong>:已发布的模型使用最长 512 的序列长度进行训练，但是您可以使用更短的最大序列长度进行微调，以节省大量内存。更长的序列非常昂贵，因为注意力与序列长度成二次关系。短于指定长度的序列被填充</p><p id="2ffc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">9.<strong class="lb iu"> train_batch_size </strong>:训练的总批量。内存使用量也与批处理大小成正比。默认值为 32。仅为培训指定此参数。</p><p id="e164" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">10.<strong class="lb iu">learning _ rate</strong>:Adam 的初始学习速率。默认学习率为 0.00005。仅为培训指定此参数。</p><p id="bcc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">11.<strong class="lb iu"> num_train_epochs </strong>:要执行的训练总次数。仅为培训指定此参数。</p><p id="e230" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">12.<strong class="lb iu"> output_dir </strong>:输出目录，其中将写入模型检查点以及评估数据集的细节。您需要在为训练或推理指定该目录之前创建它。</p><p id="c1ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">13.<strong class="lb iu"> do_lower_case </strong>:是否对输入文本进行小写处理。对于无套管模型应为真，对于有套管模型应为假。指定仅用于培训。</p><p id="1367" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">14.<strong class="lb iu"> save_checkpoints_steps </strong>:指定保存模型检查点的频率。指定仅用于培训。</p><p id="da0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于培训，请在命令提示符下使用以下命令</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="2c6b" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu">python run_classifier.py</strong> <br/>    --<strong class="nk iu">task_name=cola</strong> <br/>    --<strong class="nk iu">do_train=true</strong> <br/>    --<strong class="nk iu">do_eval=true</strong> <br/>    --<strong class="nk iu">data_dir=\yelp_review_full_csv\ </strong><br/>    --<strong class="nk iu">vocab_file=\BERT\uncased_L-12_H-768_A-12\uncased_L-12_H-768_A-12\vocab.txt</strong> <br/>    --<strong class="nk iu">bert_config_file=\BERT\uncased_L-12_H-768_A-12\uncased_L-12_H-768_A-12\bert_config.json</strong><br/>    --<strong class="nk iu">init_checkpoint=\BERT\uncased_L-12_H-768_A-12\uncased_L-12_H-768_A-12\bert_model.ckpt</strong> <br/>    --<strong class="nk iu">max_seq_length=64</strong> <br/>    --<strong class="nk iu">train_batch_size=2 </strong><br/>    --<strong class="nk iu">learning_rate=2e-5</strong> <br/>    --<strong class="nk iu">num_train_epochs=3.0</strong> <br/>    --<strong class="nk iu">output_dir=\BERT\bert_output\</strong> <br/>    --<strong class="nk iu">do_lower_case=True</strong><br/>    --<strong class="nk iu">save_checkpoints_steps 10000</strong></span></pre><p id="5674" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练完成后，您可以看到包含开发数据集摘要的检查点文件和 eval_results.txt 文件</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="621f" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu">eval_accuracy = 0.71553844<br/>eval_loss = 1.035305<br/>global_step = 1287000<br/>loss = 1.0362284</strong></span></pre><h2 id="6099" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">预测情绪</h2><p id="7931" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">为了预测情绪，我们设置以下参数并将<strong class="lb iu"> — <em class="lv"> do_predict </em> </strong>参数设置为 True。</p><ol class=""><li id="91c2" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated"><strong class="lb iu">任务名称</strong>:应该与用于训练数据集的任务名称相匹配</li><li id="71e7" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> data_dir </strong>:指定包含 test.tsv 文件中数据的目录</li><li id="2844" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> vocab_file </strong>:指定 vocab.txt 文件。Vocab.txt 使用 BERT 提供的 Vocab 文件将单词映射到索引。</li><li id="0e3f" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> bert_config_file </strong>:包含 bert 模型的参数值。伯特预训练模型有这个文件</li><li id="fd3a" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> init_checkpoint </strong>:初始检查点。这是输出目录中模型检查点文件的最大数量。</li><li id="74f2" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu">最大序列长度</strong>:指定最大序列长度。</li><li id="8a5e" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> output_dir </strong>:将测试结果写入 test_results.tsv 文件的输出目录</li></ol><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="7a26" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu">python run_classifier.py</strong> <br/>    <strong class="nk iu">--task_name=cola <br/>    --do_predict=true <br/>    --data_dir=\yelp_review_full_csv\ <br/>    --vocab_file=\BERT\uncased_L-12_H-768_A-12\uncased_L-12_H-768_A-12\vocab.txt </strong> <br/>    <strong class="nk iu">--bert_config_file=\BERT\uncased_L-12_H-768_A-12\uncased_L-12_H-768_A-12\bert_config.json</strong> <br/>    <strong class="nk iu">--init_checkpoint=\BERT\bert_output\model.ckpt-1085250 <br/>    --max_seq_length=128 <br/>    --output_dir=\BERT\bert_output\</strong></span></pre><h2 id="ce84" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">评估测试结果</h2><p id="e90b" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">测试结果在文件 test_results.tsv 的输出目录中，您可以将它与测试标签进行比较，并评估我们的多类文本分类的性能</p><p id="7d8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">阅读并解释结果</strong></p><p id="fbb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个文本的结果在 test_results.tsv 文件中都有一行，其中包含五个标记类中每一个的概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/7bf2d46284e96ebcde7b3e1600690df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*QASpyy9__qUgn8_HUq9ZZg.png"/></div></figure><p id="4aa1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要找到概率最高的类，这将是 Yelp 评论的观点</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="8a0b" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu">import  csv<br/>label_results=[]<br/>pd.DataFrame()<br/>with open('\BERT\bert_outout\test_results.tsv') as file:<br/>    rd = csv.reader(file, delimiter="\t")<br/>    for row in rd:<br/>      data_1=[float(i) for i in row]<br/>      label_results.append(data_1.index(max(data_1)))<br/>df_results=pd.DataFrame()<br/>df_results=pd.DataFrame(label_results)<br/>df_results[0]</strong></span></pre><p id="3a9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们为从 0 到 4 的情感范围生成混淆矩阵</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="ca14" class="lw lx it nk b gy no np l nq nr"><strong class="nk iu">from sklearn.metrics import confusion_matrix<br/>confusion_matrix(data_test[0], df_results[0])</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b3ce70e44f179b86b95298dcf6bb7b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*bvn_Ymc0dEqYrsJxE-fw7Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">混淆矩阵</p></figure><p id="4fd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经在一个时期训练了 BERT 模型，你可以看到对于多类文本分类来说结果看起来不错</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="78a0" class="lw lx it nk b gy no np l nq nr">from sklearn import metrics<br/>print("Accuracy",metrics.accuracy_score(data_test[0], df_results[0]))<br/>print("F1-Score",metrics.f1_score(data_test[0], df_results[0],average='weighted'))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/388cafbacf5da0900ccd1ae30032e369.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*A0Hjg6vVeFLrxEUzI98m5g.png"/></div></figure><h2 id="0e49" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">参考资料:</h2><p id="20af" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a></p><div class="oa ob gp gr oc od"><a href="https://github.com/google-research/bert" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">谷歌研究/bert</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">这是一个 24 个较小的 BERT 模型的版本(只有英语，未封装，用单词屏蔽训练),参考…</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">github.com</p></div></div><div class="om l"><div class="on l oo op oq om or ks od"/></div></div></a></div><p id="67e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://mc.ai/a-guide-to-simple-text-classification-with-bert/" rel="noopener ugc nofollow" target="_blank">https://MC . ai/a-guide-to-simple-text-class ification-with-Bert/</a></p></div></div>    
</body>
</html>