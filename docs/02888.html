<html>
<head>
<title>Natural Language Processing: A beginner’s guide part-II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理:初学者指南第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-processing-a-beginners-guide-part-ii-54a1bf8c0497?source=collection_archive---------40-----------------------#2020-03-19">https://towardsdatascience.com/natural-language-processing-a-beginners-guide-part-ii-54a1bf8c0497?source=collection_archive---------40-----------------------#2020-03-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="af9d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让我们再深入一点…</h2></div><p id="c1da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我已经在NLP系列的第一部分讨论了NLP的基础知识。您会注意到，TF-IDF的概念没有传达任何语义或上下文。但是你可以从语法的角度来解释这些文章。</p><p id="f9b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们一起来探索语料库的隐藏之美吧！</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/f699a19b1658e7b24afaf05751a946e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kmKU8uhLnERIIwVF"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated"><a class="ae le" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="c3e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">位置标签</p><p id="68d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这指的是句子中的<em class="lv">词类</em>的概念，你们在高中已经学过了。使用nltk库，你可以给每个单词甚至短语添加词性标签。这是一个复杂的过程，有时单词在句子的形式上有模糊的意思。语料库有两种词性标注。</p><p id="75f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1.基于规则的位置标记:</p><p id="212f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当你遇到模棱两可的词时，你可以使用这个技巧。分析前人和后人。它也是特定于特定语言的。大写和标点词性标注是这种技术的一部分。</p><p id="07cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中一个例子是布里尔的标签。它是一个基于规则的标记器，在训练数据中实现，找出词性标记错误较少的规则集，对数据进行最佳定义。</p><p id="d570" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.随机位置标记:</p><p id="3f4c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果一个单词在训练句子中被标记了特定的标签，分析最高频率或概率，该单词将被赋予一个特殊的标签。</p><p id="ceb5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这也被称为<strong class="kk iu"> n元语法方法</strong>，指的是基于具有n个先前标签的概率来决定单词的事实。</p><pre class="lg lh li lj gt lw lx ly lz aw ma bi"><span id="1223" class="mb mc it lx b gy md me l mf mg">import nltk</span><span id="dfaa" class="mb mc it lx b gy mh me l mf mg">paragraph =’’’<br/>Ahead of U.S. President Donald Trump’s visit to India, some of the key deliverables from the trip, as well as the outcomes that may not be delivered after his meeting with Prime Minister Narendra Modi on Tuesday, are coming into view. The larger question remains as to whether the bonhomie between the two, who will be meeting for the fifth time in eight months, will also spur the bilateral relationship towards broader outcomes, with expectations centred at bilateral strategic ties, trade and energy relations as well as cooperation on India’s regional environment. On the strategic front, India and the U.S. are expected to take forward military cooperation and defence purchases totalling about $3 billion. Mr. Trump has cast a cloud over the possibility of a trade deal being announced, but is expected to bring U.S. Trade Representative Robert Lighthizer to give a last push towards the trade package being discussed for nearly two years. Both sides have lowered expectations of any major deal coming through, given that differences remain over a range of tariffs from both sides; market access for U.S. products; and India’s demand that the U.S. restore its GSP (Generalised System of Preferences) status. However, it would be a setback if some sort of announcement on trade is not made. A failure to do so would denote the second missed opportunity since Commerce Minister Piyush Goyal’s U.S. visit last September. Finally, much of the attention will be taken by India’s regional fault-lines: the Indo-Pacific strategy to the east and Afghanistan’s future to the west. India and the U.S. are expected to upgrade their 2015 joint vision statement on the Indo-Pacific to increase their cooperation on freedom of navigation, particularly with a view to containing China. Meanwhile, the U.S.-Taliban deal is expected to be finalised next week, and the two leaders will discuss India’s role in Afghanistan, given Pakistan’s influence over any future dispensation that includes the Taliban.Any high-level visit, particularly that of a U.S. President to India, is as much about the optics as it is about the outcomes. It is clear that both sides see the joint public rally at Ahmedabad’s Motera Stadium as the centrepiece of the visit, where the leaders hope to attract about 1.25 lakh people in the audience. Despite the Foreign Ministry’s statement to the contrary, the narrative will be political. Mr. Trump will pitch the Motera event as part of his election campaign back home. By choosing Gujarat as the venue, Mr. Modi too is scoring some political points with his home State. As they stand together, the two leaders, who have both been criticised in the last few months for not following democratic norms domestically, will hope to answer their critics with the message that they represent the world’s oldest democracy and the world’s largest one, respectively.<br/>‘’’</span><span id="1536" class="mb mc it lx b gy mh me l mf mg">#tokenizing sentences<br/>sentences = nltk.sent_tokenize(paragraph)</span><span id="3605" class="mb mc it lx b gy mh me l mf mg">#tokenizing words<br/>words = nltk.word_tokenize(paragraph)</span><span id="a1f2" class="mb mc it lx b gy mh me l mf mg">#pos tagging<br/>for word in words:<br/> print(nltk.pos_tag(words))</span></pre><p id="759f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">运行上面这段代码，你可以看到单词和相应的pos标签压缩在一个列表中。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mi"><img src="../Images/b496da2e659b6bfc751b36595cb9bea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HxtiqMsHQbfzW7eQ6upN9Q.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">仅代表目的。版权所有Somesh</p></figure><p id="eb9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里的缩写像NNP——专有名词，单数。</p><p id="ef59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">IN表示介词等。您也可以探索nltk库中关于缩写的更多信息。</p><p id="9d99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还可以进行组块操作，得到一组单词。组块用来给句子增加更多的结构。您还可以添加正则表达式技术来形成不同种类的标记组或名词短语组等。它也被称为浅层解析或轻量解析。</p><pre class="lg lh li lj gt lw lx ly lz aw ma bi"><span id="6bf3" class="mb mc it lx b gy md me l mf mg">pattern = ‘’’chunk_sample:{&lt;NNP.?&gt;*&lt;IN.?&gt;}’’’<br/>chunky = nltk.RegexpParser(pattern)<br/>print(chunky)</span><span id="db46" class="mb mc it lx b gy mh me l mf mg">chunk_op = chunky.parse(nltk.pos_tag(words))<br/>print(chunk_op)</span></pre><p id="d995" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里的模式告诉我们如何从单词标记中创建组块。</p><p id="dac9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于模式创建的一些有用信息:</p><pre class="lg lh li lj gt lw lx ly lz aw ma bi"><span id="58c1" class="mb mc it lx b gy md me l mf mg">Special Symbols       Meaning<br/>.                    Any character except newline</span><span id="f404" class="mb mc it lx b gy mh me l mf mg">*                    Match 0 or more repetitions</span><span id="b664" class="mb mc it lx b gy mh me l mf mg">?                    Match 0 or 1 repetitions</span><span id="1778" class="mb mc it lx b gy mh me l mf mg">pattern = '''chunk_sample:{&lt;NN.?&gt;*&lt;VBD.?&gt;*&lt;JJ.?&gt;}'''<br/>chunky = nltk.RegexpParser(pattern)<br/>print(chunky)</span><span id="e4f4" class="mb mc it lx b gy mh me l mf mg">#corpus_lem[5]=['however','would','setback','sort','announcement','trade made']</span><span id="56bf" class="mb mc it lx b gy mh me l mf mg">chunk_op = chunky.parse(nltk.pos_tag(corpus_lem[5].split(' ')))<br/>print(chunk_op)</span><span id="0644" class="mb mc it lx b gy mh me l mf mg">chunk_op.draw()</span></pre><p id="3fcb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您运行上面这段代码，您可以更好地理解分块技术，并将其可视化。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mj"><img src="../Images/3a9350dad60b809a80777c21b08dbb10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PKhzKvWp_EWs7iqw6wcP8Q.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">版权所有Somesh</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mk"><img src="../Images/408d4af39e169b20c01dd94c6017a60f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cc4Lc9dt5RwCoq9g0JzWQQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">版权所有Somesh</p></figure><p id="8bf0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里的图表描述了每个带有词性标签的单词。您可以更多地使用RegexpParser来从语法上理解标记，并可以制作漂亮的图形来更好地可视化。</p><pre class="lg lh li lj gt lw lx ly lz aw ma bi"><span id="f440" class="mb mc it lx b gy md me l mf mg">#Ngram in nltk <br/>from nltk import ngrams</span><span id="41d8" class="mb mc it lx b gy mh me l mf mg">bigram = list(ngrams(corpus_lem[8].split(),2))<br/>trigram = list(ngrams(corpus_lem[8].split(),3))</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/644635905716b7321e17b7f2667366cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*q45N3XhfffdnBiUBD3F_kg.jpeg"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">BIGRAM输出版权somesh</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/852f6774e59daaadc2bc5d9fbdae34c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*5PBGtTLdnNe1MfVP08EQrQ.jpeg"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">三元模型输出版权所有Somesh</p></figure><p id="7667" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以清楚地看到二元模型方法形成了两个单词的元组，三元模型方法形成了三个单词的元组。</p><p id="986e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">隐马尔可夫模型:</strong></p><p id="9ae3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些是概率模型，基于先前识别的单词和当前单词，帮助决定最可能的决定。</p><p id="90c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">HMM是具有单个状态变量<strong class="kk iu"> S </strong>和单个观察变量<strong class="kk iu"> O，</strong>的概率模型，因此该模型将具有两个概率部分。诸如</p><p id="7518" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(I)过渡模型——它告诉我们一个国家在一段时间内的过渡。</p><p id="e622" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(ii)观察模型——它告诉我们在给定的状态下，您获得不同观察结果的可能性有多大。(参考下图)</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/33ab269b72bcbf69da09373952e854ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*rqu2P6kagmJuzex0W00akg.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">版权所有Somesh</p></figure><p id="3628" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是HMM概念的图形表示，称为结构化转换模型。这些数值对应的是在给定时间状态到当前状态的每一次转移的概率。每个节点都是一个特定的指定状态变量。</p><p id="cb80" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，S1以0.7的概率转移到S2，并以0.2的概率停留在原来的状态。其他州也有类似的解释。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mo"><img src="../Images/f600c15b79a0793337616a0f4b3c42f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z7Jtx9z3lP0VVXcvZSRrxg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">版权所有Somesh</p></figure><p id="948d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是关于HMM的概念。它被广泛用于NLP项目中，如机器人定位、语音识别、生物序列分析、文本分析等。我将在接下来的故事中讨论详细的模型和编码部分。</p><p id="25a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，我们只分析了使用nltk库的单词。在这里，我将尝试使用wordcloud赋予文字艺术感。很迷人，不是吗？</p><p id="f33a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如下导入所需的库。将每个句子的词条转化成一个字符串，使用wordcloud你可以表达它。</p><pre class="lg lh li lj gt lw lx ly lz aw ma bi"><span id="e275" class="mb mc it lx b gy md me l mf mg">#to intsall wordcloud in anaconda type "pip install wordcloud" in #anaconda prompt</span><span id="69bb" class="mb mc it lx b gy mh me l mf mg">#import libraries</span><span id="76aa" class="mb mc it lx b gy mh me l mf mg">import pandas as pd<br/>from wordcloud import WordCloud</span><span id="821c" class="mb mc it lx b gy mh me l mf mg">#convert the corpus_lem to a dataframe<br/>df = pd.DataFrame(data=corpus_lem,columns=[‘words’])</span><span id="1462" class="mb mc it lx b gy mh me l mf mg">stop_words = set(stopwords.words(‘english’))<br/>words_test = ‘’</span><span id="0d79" class="mb mc it lx b gy mh me l mf mg">#convert all the lematized sentence token to a string<br/>for i in df.words:<br/> words_test = words_test+i+’ ‘<br/> <br/>#put it in a wordcloud<br/>word_cloud = WordCloud(width = 900, height = 900,stopwords = stop_words,<br/> background_color = ‘white’,min_font_size = 12).generate(words_test)</span><span id="00f6" class="mb mc it lx b gy mh me l mf mg">#show using matplotlib<br/>import matplotlib.pyplot as plt</span><span id="699e" class="mb mc it lx b gy mh me l mf mg">plt.figure(figsize = (10, 10), facecolor = None) <br/>plt.imshow(word_cloud) <br/>plt.axis(“off”) <br/>plt.tight_layout(pad = 0) <br/> <br/>plt.show()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mp"><img src="../Images/59d72f25dc01144e70d483cf3c5b7556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nv5RoJs0v_yIauuSM6nqYg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">版权所有Somesh</p></figure><p id="6e80" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，你可以发现作者试图将重点放在像<strong class="kk iu"> <em class="lv">美国、印度、贸易、合作、莫迪、特朗普等词上。</em>T9】</strong></p><p id="b479" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">稍后我将解释HMM，Word2Vec概念，以便更深入地理解NLP &amp;请务必阅读本系列的第1部分，以便更好地理解这个故事。</p><p id="ec5a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直到那时享受自然语言处理！！！</p><p id="c188" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢自然语言处理<a class="ae le" rel="noopener" target="_blank" href="/natural-language-processing-a-beginners-guide-part-i-1a5880cc3bdc">第一部分</a>。在<a class="ae le" href="https://medium.com/@somesh.routray11" rel="noopener"> <strong class="kk iu">媒体</strong> </a>上关注我，或者订阅我的博客来了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过Twitter @RoutraySomesh联系。</p></div></div>    
</body>
</html>