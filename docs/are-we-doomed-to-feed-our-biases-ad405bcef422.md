# 我们注定要满足我们的偏见吗？

> 原文：<https://towardsdatascience.com/are-we-doomed-to-feed-our-biases-ad405bcef422?source=collection_archive---------67----------------------->

![](img/6e4518f65e2d0126555c10b3a79ecfe4.png)

德鲁·格雷厄姆在 Unsplash 上拍摄的照片

*原载* [*发布于 2019 年 3 月 17 日领英*](https://www.linkedin.com/pulse/we-doomed-feed-our-biases-jennifer-redmon/?trackingId=Jt8Wu4xGRHCaJ6ZgYTAFTA%3D%3D) *。*

2016 年，脸书因偏向用户的新闻订阅而遭到猛烈抨击。作为回应，这家社交媒体巨头展示了我们的喜好、联系和活动如何成为决定我们获得哪些内容的算法的主要输入；结果，内容迎合了我们自己的偏见。从商业角度来看，这一切都非常合理。如果脸书和包括 Twitter 和 LinkedIn 在内的其他社交媒体玩家希望最大限度地提高我们对他们平台的参与度，而不是他们的竞争对手，为什么他们的算法会给我们提供可能让我们不愉快的内容？正如 Nelson Granados 言简意赅地指出的，“因为脸书根据你自己的行为来定制你的新闻提要，你无意中成为了你自己偏见的受害者。”

这些算法的潜在假设，以及脸书在获取我们的思想份额方面的巨大成功，是我们喜欢的人的确认偏差，“通过寻找或解释与一个人现有信念一致的信息来处理信息的倾向”(Brtiannica)。换句话说，当我们不同意美联储的内容时，我们更有可能脱离，而不是花时间探索这些令人不安的想法。如果后者是真的，我们的提要看起来会非常不同。

内特·西尔弗把这归因于人类的本能。在《信号与噪音》一书中，他指出“当我们拥有‘太多信息’时，我们本能的捷径是有选择地参与其中，挑选出我们喜欢的部分，忽略其余的部分，与做出相同选择的人结盟，与其余的人为敌。”

# 但是为什么呢？

一方面，我们普遍认为，向新的和外来的思想敞开心扉会拓宽我们的视野，并最终带来更大的智慧。我们比以往任何时候都更频繁地周游世界，部分原因是为了与狭隘的思想作斗争，这种思想是由于我们把自己限制在有限的智力和身体范围内而产生的。

另一方面，正如脸书的成功所证明的，我们通常不喜欢挑战我们信念的内容。

即使在几十年前，要像这些算法今天为我们做的那样整理我们收到的信息，即使不是不可能，也是难以置信的困难。当然，新闻媒体通常包含一些人的偏见——可能是基于每个媒体所在的国家和地区，以及其制作人、编辑和工作人员所持的主流观点。然而，存在一种社会联系，在这种联系中，我们同意新闻的客观性是最重要的。“偏见！”是我们对新闻提供者最严厉的批评之一。在反对这一契约的过程中，新闻来源经常在其观点部分有不同的观点——而我们作为读者面对的是两种或所有的观点。杂志/报纸中的广泛内容或诸如“60 分钟要点/对位法”等片段中的有意并列，通过提供整体“包”，如报纸或节目，有助于控制个人的无意识偏见，其中有大量个人偏见，但寻求客观性的专业人士做出了贡献。

截至 2018 年，大约 2/3 的人不再以这种方式接收信息。我们宁愿通过我们根据自己的偏好训练的社交媒体算法，一篇篇地、一段段地获取新闻和故事。结果，我们实际上收到了更多的报纸和节目——但整个版面都被剪掉了。我们不会收到一篇支持我们偏见的文章和许多不支持我们偏见的文章，而是会收到每一篇支持我们偏见的文章，没有一篇不支持我们偏见的。

# 那又怎样？

就像我们训练这些人工智能给我们提供有偏见的信息一样，我们也可以训练它们拓宽我们的视角。但首先，我们必须在情感上准备好质疑我们最根深蒂固的信念。正如戴尔·卡内基 83 年前教导我们的那样，“当与人打交道时，让我们记住我们不是在与逻辑动物打交道。我们面对的是感情丰富的生物，充满偏见、受骄傲和虚荣驱使的生物。”难怪强烈的情绪会降低我们的智商(对于那些倾向于量化的人来说，会降低 10-15 分),关于我们最关心的问题的辩论，比如政治，变得异常激烈。

用人工智能的术语来说，当用户忽略某些内容，或者——对一个以捕捉和保持你的注意力为目标的算法来说更糟糕——从平台上脱离时，它会学习。它不只是肤浅地了解这一点——我们输入这些算法的大量数据为强大的推荐引擎提供了燃料，这些引擎使用数据科学来“找到”你很有可能喜欢的内容，这些内容基于你可能从未见过的与你有相似偏好的人的活动。因此，我们和其他像我们一样的人不仅滋养了我们自己，也滋养了彼此的偏见。

# 现在怎么办？

虽然没有一个总开关来“提供挑战偏见的内容”，但如果我们改变习惯，这些算法会相应地重新训练自己。如果我们开始喜欢、关注和接触我们不喜欢的内容，我们会得到更多同样的东西。如果我们挑战自己，通过接触我们通常会忽略或反对的内容来质疑我们最亲近的和最具情感触发的偏见，我们将训练这些算法来满足我们的知识好奇心，而不是我们的偏见。

# 有什么意义？

我写这篇博客的要求说起来简单，做起来却很难:我们每个人都在引导思想开放和对抗自己的偏见方面发挥积极作用；在人工智能时代，我们需要付出额外的努力来做到这一点；我们每个人都为训练这些算法的活动承担个人责任；我们寻求迫使我们质疑自己偏见的想法，并在这样做的过程中变得更加舒适，不是因为拥有所有的答案，而是因为考虑到社交媒体和信息时代让我们触手可及的广泛视角。