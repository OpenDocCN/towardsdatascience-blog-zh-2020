<html>
<head>
<title>What is a Convolutional Neural Network?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是卷积神经网络？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-visualization-of-the-basic-elements-of-a-convolutional-neural-network-75fea30cd78d?source=collection_archive---------33-----------------------#2020-05-26">https://towardsdatascience.com/a-visualization-of-the-basic-elements-of-a-convolutional-neural-network-75fea30cd78d?source=collection_archive---------33-----------------------#2020-05-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/77a1021c2d4e60cb022bd720ba66baef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nWwSb30Q0CiEDPVPblS05g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">基本CNN的模板</p></figure><div class=""/><div class=""><h2 id="3884" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">卷积神经网络基本元素的可视化</h2></div><p id="f04a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">可视化是理解丰富概念的一个很好的工具，特别是对于该领域的初学者。在本文中，我们将使用视觉辅助工具来浏览卷积神经网络的基本元素。本文首先为一个具有不同构件的基本CNN提供一个模板(可视化的)，然后讨论每个构件最常用的元素。</p><h1 id="7756" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">基本CNN模板:</h1><p id="eb58" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">一个基本的CNN由三种层组成。输入、隐藏和输出如下所示。数据通过输入层进入CNN，并在到达输出层之前通过各种隐藏层。输出层是网络的预测。根据损失或误差，将网络的输出与实际标签进行比较。对于要学习的网络，计算该损失相对于可训练权重的偏导数，并且通过使用反向传播的各种方法之一来更新权重。</p><p id="afb3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">基本CNN的完整视觉模板可以在下面看到。</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/77a1021c2d4e60cb022bd720ba66baef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nWwSb30Q0CiEDPVPblS05g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">基本CNN的模板</p></figure><h1 id="e3aa" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">CNN的隐藏层</h1><p id="10dc" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">网络中的隐藏层提供了转换数据的基本构件(输入层或先前隐藏层的输出)。大多数常用的隐藏层(不是全部)都遵循一个模式。它首先将一个函数应用到它的输入，然后转移到池化、规范化，最后在它可以作为输入提供给下一层之前应用激活。因此，每一层可以分解成以下4个子功能</p><ol class=""><li id="c30b" class="mr ms jf kw b kx ky la lb ld mt lh mu ll mv lp mw mx my mz bi translated"><strong class="kw jg">层功能:</strong>卷积或全连接层等基本变换功能。</li><li id="f876" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated"><strong class="kw jg">池化:</strong>用于改变特征图的空间大小，增加(上采样)或减少(最常见的)它。例如最大池化、平均池化和取消池化。</li><li id="54f0" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated"><strong class="kw jg">归一化:</strong>该子函数将数据归一化，使其均值和单位方差为零。这有助于处理消失梯度、内部协变量移位等问题。<a class="ae nf" rel="noopener" target="_blank" href="/difference-between-local-response-normalization-and-batch-normalization-272308c034ac">(更多信息)</a>。最常用的两种标准化技术是局部响应标准化和批量标准化。</li><li id="b384" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated"><strong class="kw jg">激活:</strong>应用非线性并限制输出过高或过低。</li></ol><p id="0145" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将逐一介绍每个子功能，解释它们最常见的例子。</p><blockquote class="ng nh ni"><p id="82d1" class="ku kv nj kw b kx ky kg kz la lb kj lc nk le lf lg nl li lj lk nm lm ln lo lp ij bi translated">还有更复杂的CNN架构，有各种其他层和相当复杂的架构。不是所有的CNN架构都遵循这个模板。</p></blockquote><h1 id="78a9" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">1.层功能</h1><p id="3255" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">最常用的层函数是全连接、卷积和转置卷积(错误地称为解卷积)层。</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/18df2af2f54912fab0cefd259a0dcb4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Kg5cA0WNLjDnS3F6gbwFYQ.gif"/></div></div></figure><h2 id="8d0c" class="no lr jf bd ls np nq dn lw nr ns dp ma ld nt nu mc lh nv nw me ll nx ny mg nz bi translated">a.完全连接的层:</h2><p id="703d" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">这些层由输入和输出之间的线性函数组成。对于<em class="nj"> i </em>输入节点和<em class="nj"> j </em>输出节点，可训练权值为wij和bj。左图说明了3个输入节点和2个输出节点之间的全连接层是如何工作的。</p><h2 id="853f" class="no lr jf bd ls np nq dn lw nr ns dp ma ld nt nu mc lh nv nw me ll nx ny mg nz bi translated">b.卷积层:</h2><p id="fddf" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">这些图层应用于2D(和3D)输入要素地图。可训练权重是一个2D(或3D)核/过滤器，它在输入要素地图上移动，与输入要素地图的重叠区域生成点积。以下是用于定义卷积层的3个参数</p><ul class=""><li id="8be2" class="mr ms jf kw b kx ky la lb ld mt lh mu ll mv lp oa mx my mz bi translated"><strong class="kw jg">内核大小K: </strong>滑动内核或过滤器的大小。</li><li id="9381" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp oa mx my mz bi translated"><strong class="kw jg">步长S: </strong>定义在执行点积生成输出像素之前内核滑动了多少</li><li id="659d" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp oa mx my mz bi translated"><strong class="kw jg">填充P: </strong>输入特征图周围插入的零的帧大小。</li></ul><p id="b6ee" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">下面的4幅图直观地解释了大小为(<strong class="kw jg"> <em class="nj"> i </em> </strong> ) 5x5的输入上的卷积层，其中内核大小(<strong class="kw jg"> <em class="nj"> k </em> </strong>)为3x3，步长(<strong class="kw jg"> <em class="nj"> s </em> </strong>)和填充(<strong class="kw jg"> <em class="nj"> p </em> </strong>)</p><div class="mn mo mp mq gt ab cb"><figure class="ob is oc od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/33473439f79cbd9cdf01171d7ced6f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*YvlCSNzDEBGEWkZWNffPvw.gif"/></div></figure><figure class="ob is oc od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/ce5349debbeb800200d958ad93573ab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*gXAcHnbTxmPb8KjSryki-g.gif"/></div></figure></div><div class="ab cb"><figure class="ob is oc od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/c7c292034d619d40c16d23992a325587.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*34_365CJB5seboQDUrbI5A.gif"/></div></figure><figure class="ob is oc od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/3f38268fb9ae04fb574537c18e42b189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*WpOcRWlofm0Z0EDUTKefzg.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk oh di oi oj translated">动画卷积层(来源:Aqeel Anwar)</p></figure></div><p id="1192" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">步幅和填充与输入特征图一起控制输出特征图的大小。输出大小由下式给出</p><h2 id="8369" class="no lr jf bd ls np nq dn lw nr ns dp ma ld nt nu mc lh nv nw me ll nx ny mg nz bi translated">c.转置卷积(反卷积)层:</h2><p id="c81d" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">通常用于增加输出特征图的大小(上采样)。转置卷积层背后的想法是撤消(不完全)卷积层。正如卷积层一样，它也是由步长和填充定义的。如果我们对输出应用所提供的步幅和填充，并应用所提供大小的卷积核，它将生成输入。</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/0b546b8d7e7cca9a7bb53a79ceda597d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQoJO4cxEhJLxyGNVeqq_g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">转置卷积层(来源:Aqeel Anwar)</p></figure><p id="ab85" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">要生成输出，需要执行两件事情</p><ul class=""><li id="7e20" class="mr ms jf kw b kx ky la lb ld mt lh mu ll mv lp oa mx my mz bi translated">零插入(<strong class="kw jg"> <em class="nj"> z </em> </strong>):在原始输入的行和列之间插入的零的数量</li><li id="a2f0" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp oa mx my mz bi translated">padding(<strong class="kw jg"><em class="nj">p’</em></strong>):输入特征图周围插入的零的帧大小。</li></ul><p id="c9e6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">下面的4幅图直观地解释了在不同大小(<strong class="kw jg"> <em class="nj"> i </em> </strong>)的输入上的转置卷积层，对于3x3的内核大小(<strong class="kw jg"> <em class="nj"> k </em> </strong>)和不同步长(<strong class="kw jg"> <em class="nj"> s </em> </strong>)以及填充(<strong class="kw jg"> <em class="nj"> p </em> </strong>)而输出<strong class="kw jg"> <em class="nj"> (o) </em> </strong>固定为5×5</p><div class="mn mo mp mq gt ab cb"><figure class="ob is oc od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/c5473c7fd7cf70221d72e10ee2205fc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*SpxCUPzNfb9C8TiAcrRr5A.gif"/></div></figure><figure class="ob is oc od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/670154873acdb0b506090ad00e917dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*gff0oa2iPygyCEjj7Fb3yg.gif"/></div></figure></div><div class="ab cb"><figure class="ob is oc od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/bb25f8594f7472508c02b40b6bd9ed3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*WaBzh5OkmD-9EBLy5aXiug.gif"/></div></figure><figure class="ob is oc od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/724c36f0e93fd680065c3f01f8381023.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*L_hJRnywTpeTFJAaVZTRfQ.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk oh di oi oj translated">动画转置卷积层(来源:Aqeel Anwar)</p></figure></div><p id="7af2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">关于转置卷积层的深入细节可以在下面找到</p><div class="ip iq gp gr ir ol"><a rel="noopener follow" target="_blank" href="/what-is-transposed-convolutional-layer-40e5e6e31c11"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd jg gy z fp oq fr fs or fu fw je bi translated">什么是转置卷积层？</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">通过动画gif和python代码解释。</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ix ol"/></div></div></a></div><h1 id="7ae8" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">2.联营</h1><p id="62e1" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">最常用的池是最大、平均池和最大平均取消池。</p><h2 id="5221" class="no lr jf bd ls np nq dn lw nr ns dp ma ld nt nu mc lh nv nw me ll nx ny mg nz bi translated">最大/平均池:</h2><p id="c228" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">基于选择由内核定义的感受域中的最大值/平均值，不可训练层用于<strong class="kw jg">减小</strong>输入层的空间大小。核以给定的步幅滑过输入特征图。对于每个位置，输入特征图中与核重叠的部分的最大值/平均值就是相应的输出像素。</p><div class="mn mo mp mq gt ab cb"><figure class="ob is oc od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/fe5ae9a2286371ede7bbcb29144095a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*kW4HcS4zFxoKv6R4xtqFlg.gif"/></div></figure><figure class="ob is oc od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/327f1c289e6aea181dc85c79ae1fe94d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*LjXV6eQKTQcg-PJnBRE0VA.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk oh di oi oj translated">动画最大池层(来源:Aqeel Anwar)</p></figure></div><h2 id="25c2" class="no lr jf bd ls np nq dn lw nr ns dp ma ld nt nu mc lh nv nw me ll nx ny mg nz bi translated">取消轮询:</h2><p id="3642" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">基于将输入像素放置在由内核定义的输出的感受域中的某个索引处，不可训练层用于<strong class="kw jg">增加</strong>输入层的空间大小。对于一个非池层，在网络的早期需要有一个相应的池层。来自相应池层的最大/平均值的索引被保存并在非池层中使用。在非池化图层中，每个输入像素被放置在输出中池化图层中出现最大值/平均值的索引处，而其他像素被设置为零</p><h1 id="cd79" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">3.正常化</h1><p id="4e19" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">通常在激活函数之前使用归一化来限制无限制激活将输出图层值增加得过高。通常使用两种类型的标准化技术</p><h2 id="b31f" class="no lr jf bd ls np nq dn lw nr ns dp ma ld nt nu mc lh nv nw me ll nx ny mg nz bi translated">a.LRN当地反应正常化:</h2><p id="23c6" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">LRN是一个<strong class="kw jg">不可训练层</strong>，它在局部邻域内对特征图中的像素值进行平方归一化。基于邻域定义，有两种类型的LRN:通道间和通道内，如下图所示。</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pa"><img src="../Images/841308453b0785ec9799847d4205e583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFl0tPjwvc49HirAJZPhEA.png"/></div></div></figure><div class="mn mo mp mq gt ab cb"><figure class="ob is pb od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/f6556793074dad879269e7eaa5536f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*VoBDhNBaJPCxgcITvrQaXA.png"/></div></figure><figure class="ob is pc od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/d951dd78ac5f2d634924231b2dc69d4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*hSM8Prmr58B7GvnkjUj42w.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk pd di pe oj translated"><strong class="bd pf">左:</strong>通道内LRN … <strong class="bd pf">右</strong>:通道间LRN</p></figure></div><h2 id="8215" class="no lr jf bd ls np nq dn lw nr ns dp ma ld nt nu mc lh nv nw me ll nx ny mg nz bi translated">b.批量标准化BN:</h2><p id="608b" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">另一方面，BN是一种可训练的标准化数据的方法。在批量标准化中，隐藏神经元的输出在被馈送到激活函数之前以下面的方式被处理。</p><ol class=""><li id="2acc" class="mr ms jf kw b kx ky la lb ld mt lh mu ll mv lp mw mx my mz bi translated">将整批<em class="nj"> B </em>标准化为零均值和单位方差</li></ol><ul class=""><li id="699a" class="mr ms jf kw b kx ky la lb ld mt lh mu ll mv lp oa mx my mz bi translated">计算整个小批量产量的平均值:<em class="nj"> u_B </em></li><li id="d32e" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp oa mx my mz bi translated">计算整个小批量产量的方差:s <em class="nj"> igma_B </em></li><li id="8259" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp oa mx my mz bi translated">通过减去平均值并除以方差来标准化小批量</li></ul><p id="f34e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">2.引入两个可训练参数(<em class="nj"> Gamma: </em> scale_variable和<em class="nj"> Beta: </em> shift_variable)来缩放和移动标准化小批量输出</p><p id="45e8" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">3.将该缩放和移位的标准化小批量馈送到激活功能。</p><p id="8105" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">下面是这两种标准化技术的总结</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pg"><img src="../Images/9d2df25a8d828b432cc3bf386ce70992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7rxGz1f_2YWjdcsvqNCNA.png"/></div></div></figure><p id="8db0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">关于这些标准化技术的详细文章可以在<a class="ae nf" rel="noopener" target="_blank" href="/difference-between-local-response-normalization-and-batch-normalization-272308c034ac"> <strong class="kw jg">这里</strong> </a>找到</p><h1 id="a21d" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">4.激活</h1><p id="bfb9" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">激活函数的主要目的是引入非线性，因此CNN可以有效地映射输入和输出之间的非线性复映射。根据基本要求，可以使用多种激活功能。</p><ul class=""><li id="93a8" class="mr ms jf kw b kx ky la lb ld mt lh mu ll mv lp oa mx my mz bi translated"><strong class="kw jg">非参数/静态函数:</strong>线性，ReLU</li><li id="147c" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp oa mx my mz bi translated"><strong class="kw jg">参数函数:</strong> ELU，双曲正切，sigmoid，Leaky ReLU</li><li id="4bea" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp oa mx my mz bi translated"><strong class="kw jg">有界函数:</strong> tanh，sigmoid</li></ul><p id="c46d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">下面的gif形象地解释了最常用的激活函数的本质。</p><div class="mn mo mp mq gt ab cb"><figure class="ob is ph od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/5016b07c331de3cc526f642048f6d4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*EmTYifwsrA6YNPI2vYRf7g.gif"/></div></figure><figure class="ob is pi od oe of og paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/65e3541ee289fd5765e08f5d5a7865dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*HBvDu4Rl56AEz_jvF3BYBQ.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk pj di pk oj translated">动画激活功能(来源:Aqeel Anwar)</p></figure></div><p id="8935" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">最常用的激活函数是ReLU。当涉及到更深的神经网络时，诸如tanh和sigmoid的有界激活函数遭受消失梯度的问题，并且通常被避免。</p><h1 id="991e" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">5.损失计算:</h1><p id="d7dd" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">一旦定义了CNN，就需要选择一个损失函数来量化CNN预测与实际标签的差距。然后，在梯度下降法中使用这种损失来训练网络变量。像激活函数一样，损失函数有多种选择。</p><h2 id="527b" class="no lr jf bd ls np nq dn lw nr ns dp ma ld nt nu mc lh nv nw me ll nx ny mg nz bi translated">回归损失函数</h2><ul class=""><li id="d01f" class="mr ms jf kw b kx mi la mj ld pl lh pm ll pn lp oa mx my mz bi translated">平均绝对误差:估计值和标签是实数</li><li id="db17" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp oa mx my mz bi translated">均方差:估计值和标签是实数</li><li id="eb65" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp oa mx my mz bi translated">胡伯损失:估计值和标签是实数</li></ul><h2 id="12db" class="no lr jf bd ls np nq dn lw nr ns dp ma ld nt nu mc lh nv nw me ll nx ny mg nz bi translated">分类损失函数</h2><ul class=""><li id="7c1d" class="mr ms jf kw b kx mi la mj ld pl lh pm ll pn lp oa mx my mz bi translated">交叉熵:估计值和标签是概率(0，1)</li><li id="18d5" class="mr ms jf kw b kx na la nb ld nc lh nd ll ne lp oa mx my mz bi translated">铰链损耗:估计值和标签是实数</li></ul><p id="c60d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这些损失函数的细节可以在下图中看到</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi po"><img src="../Images/24de8872317532e2ca8fa617c849c996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ddQRcK9U-1Epmlfn6FT7ZQ.gif"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">动画ML损失函数(来源:Aqeel Anwar)</p></figure><h1 id="6a15" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">6.反向传播</h1><p id="5c45" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">反向传播不是CNN的结构元素，而是我们通过在梯度变化(梯度下降)的相反方向上更新权重来学习潜在问题的方法。关于不同梯度下降算法的深入细节可以在<a class="ae nf" href="https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="108d" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">总结:</h1><p id="1ffa" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">在这篇文章中，介绍了一个基本CNN的不同元素的动画可视化，这将有助于更好地理解它们的功能。</p><h1 id="ea78" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">奖金:</h1><p id="f2a1" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">可以在下面的链接中找到这个主题和机器学习中许多其他重要主题的紧凑备忘单</p><div class="ip iq gp gr ir ol"><a href="https://medium.com/swlh/cheat-sheets-for-machine-learning-interview-topics-51c2bc2bab4f" rel="noopener follow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd jg gy z fp oq fr fs or fu fw je bi translated">机器学习面试主题的备忘单</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">ML面试的视觉备忘单(www.cheatsheets.aqeel-anwar.com)</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">medium.com</p></div></div><div class="ou l"><div class="pp l ow ox oy ou oz ix ol"/></div></div></a></div></div><div class="ab cl pq pr hu ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="ij ik il im in"><p id="8a92" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">如果这篇文章对你有帮助，欢迎鼓掌、分享和回复。如果你想了解更多关于机器学习和数据科学的知识，请关注我@<a class="px py ep" href="https://medium.com/u/a7cc4f201fb5?source=post_page-----75fea30cd78d--------------------------------" rel="noopener" target="_blank"><strong class="kw jg">Aqeel an war</strong></a><strong class="kw jg">或者在</strong><a class="ae nf" href="https://www.linkedin.com/in/aqeelanwarmalik/" rel="noopener ugc nofollow" target="_blank"><strong class="kw jg"><em class="nj">LinkedIn</em></strong></a><strong class="kw jg"><em class="nj">上与我联系。</em> </strong></p></div></div>    
</body>
</html>