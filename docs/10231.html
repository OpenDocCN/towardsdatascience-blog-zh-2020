<html>
<head>
<title>Graph Laplacian and its application in Machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图拉普拉斯算子及其在机器学习中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-laplacian-and-its-application-in-machine-learning-7d9aab021d16?source=collection_archive---------18-----------------------#2020-07-19">https://towardsdatascience.com/graph-laplacian-and-its-application-in-machine-learning-7d9aab021d16?source=collection_archive---------18-----------------------#2020-07-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="aff4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">图拉普拉斯在谱聚类中的应用</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/396a40840b9600f74d8bf279c40ac412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ibv3jbiFZ-Lv3oR8"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@pietrozj?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Pietro Jeng </a>拍摄</p></figure><p id="4274" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文重点介绍了图形、其表示的属性及其在机器学习中执行谱聚类的应用。</p><h1 id="9137" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">介绍</h1><p id="ea0a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">图是一种数据结构，其中的节点通过有向或无向的边相互连接。例如，对于以城市为节点表示城市道路网络的图，边可以具有表示两个城市之间距离的权重。</p><h1 id="c057" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">内容</strong></h1><ul class=""><li id="abe5" class="ms mt it lb b lc mn lf mo li mu lm mv lq mw lu mx my mz na bi translated">加载数据</li><li id="55db" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">根据特征构建图形</li><li id="cabd" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">导出它的拉普拉斯表示</li><li id="6cc3" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">探索图拉普拉斯的性质</li><li id="d689" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">机器学习中的应用—谱聚类</li></ul><p id="02ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">加载数据</strong></p><p id="689e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将从 python 库 scikit-learn 中的玩具 make_moons 数据集开始。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="1453" class="nl lw it nh b gy nm nn l no np">import numpy as np<br/>from scipy import sparse</span><span id="4d5a" class="nl lw it nh b gy nq nn l no np">from sklearn.datasets import make_moons<br/>from sklearn.neighbors import kneighbors_graph<br/>from sklearn.cluster import KMeans<br/>from sklearn.metrics import homogeneity_score, completeness_score,v_measure_score</span><span id="a8e7" class="nl lw it nh b gy nq nn l no np">import networkx as nx<br/>import matplotlib.pyplot as plt</span><span id="e408" class="nl lw it nh b gy nq nn l no np">random_state = 213<br/>np.random.seed(random_state)</span><span id="304b" class="nl lw it nh b gy nq nn l no np">data_size = 150</span><span id="9be0" class="nl lw it nh b gy nq nn l no np">features,y = make_moons(n_samples=data_size, noise=0.07, random_state=213)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/49aab41f53fb21998552b803d2ea4c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*oUFvap6mMXctTyF5HanRnA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有两个聚类的数据集</p></figure><p id="705f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">构建 K-最近邻图</strong></p><p id="f2e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">k-最近邻图可以以两种模式构建——“距离”或“连通性”。</p><p id="2bb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在“距离”模式下，边代表两个节点之间的距离，而在“连通性”模式下，图的边权重为 1 或 0，表示两个节点之间是否存在边。我们将选择欧几里德距离度量来计算距离。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="760e" class="nl lw it nh b gy nm nn l no np">n_neighbors = 10<br/>knn_dist_graph = kneighbors_graph(X=features,<br/>                                 n_neighbors=n_neighbors,<br/>                                 mode='distance',<br/>                                 metric='euclidean',<br/>                                 n_jobs=6)</span></pre><p id="ca1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它返回一个稀疏图，图中的边表示数据点之间的距离。第一个数据点与其最近的 10 个邻居之间的距离如下所示。注意图的稀疏性，并且它将仅在矩阵中对应于其 k 个最近邻居的那些位置/索引处具有代表距离的连续值，其余的将为零。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="a5c1" class="nl lw it nh b gy nm nn l no np">knn_dist_graph.todense()[0]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/8dd89b007fa2b2f79ce78e005291b754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*fcnGBp8LwLdJXos4-Kka_g.png"/></div></figure><p id="e665" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该数据通常用于在数据点中寻找组，其中相似的数据点位于相同的类或簇中。</p><p id="aefd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，在其他这种情况下，当您想要捕捉数据点内的相似性而不是距离时，我们可以使用宽度为(sigma = 1)的高斯核将该图转换为基于相似性，距离 d(x1，x2)是上面稀疏图中非零位置的欧几里德距离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/8029801a549a962a9cc4d58dd656e008.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*6K9UIlCnRwsyLYH5AqHuqw.png"/></div></figure><p id="a66a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，在上面的数组中，距离= 0 的位置意味着该距离超出了 k 个最近邻的最大距离。当输入高斯核时，该值可以解释为最高相似度。因此，我们将只把这个内核应用于包含距离的索引。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="7321" class="nl lw it nh b gy nm nn l no np">sigma = 1<br/>similarity_graph = sparse.csr_matrix(knn_dist_graph.shape)<br/>nonzeroindices = knn_dist_graph.nonzero()</span><span id="5339" class="nl lw it nh b gy nq nn l no np">similarity_graph[nonzeroindices] = np.exp( -np.asarray(knn_dist_graph[nonzeroindices])**2 / 2.0 * sigma**2)</span><span id="e289" class="nl lw it nh b gy nq nn l no np">similarity_graph = 0.5 * (similarity_graph + similarity_graph.T)<br/>similarity_graph.todense()[0]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ca8b121db5d81b2ff4bef7047baadf4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*IA8klHglBuhIpJC9PU1gmQ.png"/></div></figure><p id="1215" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该图可能是不对称的，因为它是基于 k-最近邻的。我们需要使这个图对称，原因我们将在拉普拉斯的性质中了解。我们将它的图的转置加到它自身上，并将所有的值除以 2。</p><p id="c997" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看这个图形可视化后的样子</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/7c5f7a152855902e4604e23df4fcd131.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*5hc_zbzikrI5Cy8Y8gFCNQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用 networkx 实现可视化</p></figure><p id="d079" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">导出图的拉普拉斯表示法</strong></p><p id="feae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">T3】L = D-WT5】</strong></p><p id="72ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">度矩阵</p><p id="2901" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">w:相似图(用加权邻接矩阵表示)</p><p id="424a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将创建一个图的度矩阵，它是一个对角线矩阵，对角线上有节点度<strong class="lb iu"> <em class="nw"> d </em> </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ca7cf63a421eb4e26611ebe5be7a9d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*VDx83Dma_ONS8olhasTP6w.png"/></div></figure><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="2480" class="nl lw it nh b gy nm nn l no np">degree_matrix = similarity_graph.sum(axis=1)<br/>diagonal_matrix =         <br/>np.diag(np.asarray(degree_matrix).reshape(data_size,))</span><span id="f82e" class="nl lw it nh b gy nq nn l no np">L =  diagonal_matrix - similarity_graph</span></pre><p id="fabe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nw">注:</em> </strong>通过从度矩阵中减去相似度矩阵，图中的圈的影响被消除。最后，拉普拉斯算子包含对角线上的次数和矩阵其余部分中边权重的负值。</p><p id="d699" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">图的拉普拉斯性质</strong></p><ol class=""><li id="672a" class="ms mt it lb b lc ld lf lg li ny lm nz lq oa lu ob my mz na bi translated"><strong class="lb iu"> <em class="nw">实对称</em> </strong></li></ol><p id="6185" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为它是实的和对称的，所以它的特征值是实的，它的特征向量是正交的。</p><p id="eb09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.<strong class="lb iu"> <em class="nw">正半定</em> </strong></p><p id="9bed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拉普拉斯算子具有至少一个等于 0 的特征值。我们可以通过它的二次型来检验这一点。l 是实对称的，并且如果<strong class="lb iu"> <em class="nw"> x </em> </strong>是 n×1 列向量，则它的二次型 Q 由下式给出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/5ba29c50dcc438b036029170c2ddd062.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*mObExibBXRiQ9F2f_931Bg.png"/></div></figure><p id="0dba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">二次型是正半定的，如果-</p><ul class=""><li id="5cfb" class="ms mt it lb b lc ld lf lg li ny lm nz lq oa lu mx my mz na bi translated"><strong class="lb iu"> <em class="nw"> Q≥0 </em> </strong>为全部 x 而<strong class="lb iu"> <em class="nw"> Q = 0 </em> </strong>为部分<strong class="lb iu"> <em class="nw"> x≠0 </em> </strong></li></ul><p id="bb90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将 x 设为 1 的列向量。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="a829" class="nl lw it nh b gy nm nn l no np">x = np.ones(shape=(data_size,1))<br/>Q = np.dot(np.dot(x.T, L.todense()), x)<br/>Q.round(10)</span></pre><p id="db78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">任何包含相同值的列向量 x 将导致等于 0 的二次型。</p><p id="c860" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.<strong class="lb iu"> <em class="nw">拉普拉斯零特征值的个数等于图中连通分量的个数</em> </strong></p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="97b8" class="nl lw it nh b gy nm nn l no np"># Finding eigen values and eigen vectors<br/>e, evecs = np.linalg.eig(L.todense())<br/>e.shape, evecs.shape</span><span id="b640" class="nl lw it nh b gy nq nn l no np"># No. of eigen values equal to 0<br/>e_rounded_off = e.round(5)<br/>e_rounded_off[e_rounded_off == 0].shape</span><span id="001a" class="nl lw it nh b gy nq nn l no np"># No. of connected components<br/>nx.number_connected_components(nx_graph)</span></pre><p id="74d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">4<em class="nw">。具有高边缘权重的两个相似数据点在所得特征向量</em> </strong>中的相应索引处具有相似的值</p><p id="d0f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些特征向量之一是菲德勒向量——对应于最小非零特征值的特征向量。在下图中，数据点被符号很好地分开。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/bff4aa7fb2edda3b855fe91ce14d997d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*IpnlPnbYASjqDPQkn90H8Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">对应于最小非零特征值的特征向量图</p></figure><h2 id="f5dd" class="nl lw it bd lx oe of dn mb og oh dp mf li oi oj mh lm ok ol mj lq om on ml oo bi translated">图拉普拉斯算子的应用</h2><p id="46eb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">通过扩展上述所有属性，以及本征向量将数据点分组的事实，本征向量被用于聚类。这种方法叫做<strong class="lb iu">谱聚类</strong>。</p><p id="45ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是通过选择阈值将数据点从第一个最小的特征向量分成 2 个集群来执行的。对于 2 个以上的聚类，我们可以使用 Kmeans 算法直接从第一个最小的 K 个特征向量中获得 K 个聚类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/6ceb2ce33e8bc23b08cc1786f6c7ddee.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*EUyeiX7HCACPIewuL9guew.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征值对数据指数作图</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/a945f4bacefeb03ddcfba3b141c06b35.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*kHW7wvRTSSN7SHbiGdCheg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征值(排序)相对于数据指数绘制</p></figure><p id="b607" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nw">使用菲德勒矢量来划分数据点</em> </strong></p><p id="2650" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Graph_partition#:~:text=In%20mathematics%2C%20a%20graph%20partition,edges%20in%20the%20partitioned%20graph." rel="noopener ugc nofollow" target="_blank">图形分割</a> —在这种情况下，顶点被分割成分离的集合。该图被划分，使得组内的边具有高权重(聚类内的点是相似的)，而组间的边具有小权重(不同聚类中的点彼此不相似)。</p><p id="7380" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">费德勒向量是对应于最小非零特征值的特征向量。低于 0 的值的索引被分配给聚类 1，其余值的索引被分配给聚类 2。</p><p id="4c92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们能够以这种方式使用这个向量，因为这个特征向量是指示向量的缩放版本。指示向量与矩阵的每个非零特征值相关联。每个指示符向量也是彼此正交的，并且理想地包含二进制值 0 或 1 来指示集群成员资格。</p><p id="cab6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，我们使用带符号的连续值，而不是包含 0 和 1 的指示向量。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="4451" class="nl lw it nh b gy nm nn l no np"># Get smallest non-zero eigen value's index for obtaining partition to cluster<br/>fiedler_index = sorted_indices[1]</span><span id="0287" class="nl lw it nh b gy nq nn l no np"><br/># The eigen vector for smallest non-zero eigen value i.e plotting the Fiedler vector<br/></span><span id="c62e" class="nl lw it nh b gy nq nn l no np">plt.figure(figsize=(8,6))<br/>plt.scatter(np.arange(data_size), evecs[:,fiedler_index].tolist())<br/>plt.title("Eigen (Fiedler) vector plot")<br/>plt.show()</span><span id="819f" class="nl lw it nh b gy nq nn l no np">fiedler_vector = evecs[:,fiedler_index].copy()</span><span id="c4e5" class="nl lw it nh b gy nq nn l no np"><br/># Thresholding the values in this eigen vector at 0<br/>fiedler_vector[fiedler_vector &lt; 0.0] = 0<br/>fiedler_vector[fiedler_vector &gt; 0.0] = 1<br/>new_labels = np.asarray(fiedler_vector)[:,0]</span><span id="5f90" class="nl lw it nh b gy nq nn l no np"># Plot cluster result<br/>plt.scatter(features[:,0], features[:,1],         <br/>                          c=new_labels.astype(float))<br/>plt.title("Clusters plot")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/3835036e82bbf4c3c865c164a9d019fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*-9z0X63rFXWgbPSTOy2qtQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从谱聚类中获得的聚类</p></figure><p id="170c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">基于熵的外部聚类评价方法评价</strong></p><p id="ab47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness" rel="noopener ugc nofollow" target="_blank">同质性&amp;完整性评分和 v-measure </a>根据基本事实评估聚类结果，并考虑类和聚类标签的分布，以测量所获得的聚类的一致性。每项措施的最高分为 1.0。对于上述过程，我们每个指标的得分为 1.0。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="4a7a" class="nl lw it nh b gy nm nn l no np"># Evaluation of clustering result of the above procedure<br/>homogeneity_score(y, new_labels), completeness_score(y, new_labels), v_measure_score(y, new_labels)</span></pre><p id="7b37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">与 Kmeans 的比较</strong></p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="75e5" class="nl lw it nh b gy nm nn l no np">num_clusters = 2<br/>kmean_labels = KMeans(n_clusters=num_clusters, random_state=random_state, n_jobs=6).fit_predict(features)</span><span id="ed33" class="nl lw it nh b gy nq nn l no np">plt.scatter(features[:,0], features[:,1],                                                                           c=kmean_labels.astype(float))<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/e03f48837a428be2ddf001e67012d9a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*1sJKmyIEUafX5c-4slvY5w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从 KMeans 聚类中获得的聚类</p></figure><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="1c65" class="nl lw it nh b gy nm nn l no np"># Evaluation of clustering result of KMeans<br/>homogeneity_score(y, kmean_labels), completeness_score(y, kmean_labels), v_measure_score(y, kmean_labels)</span><span id="38ed" class="nl lw it nh b gy nq nn l no np"># Scores<br/># (0.1836464702880451, 0.1837407327840609, 0.18369358944333708)</span></pre><p id="8005" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设存在球状星团，Kmeans 不能很好地区分这两个星团。</p></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><p id="f240" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回顾拉普拉斯表示和求解特征值系统以获得聚类，一个问题是— <strong class="lb iu">为什么是特征值系统？</strong></p><p id="d271" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是因为特征值系统近似于图割。</p><p id="2bbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<strong class="lb iu"> <em class="nw">割(V1，</em> ) </strong>获得 2 个分区的 V1 &amp; V2 表示为——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e2d4b436099a698ce70af7535cf1df5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*UhfA9SnpMCXPS7jlGIvl0g.png"/></div></figure><p id="04ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上述方法中，特征值近似一个<a class="ae ky" href="https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf" rel="noopener ugc nofollow" target="_blank">归一化图割</a> — <strong class="lb iu"> <em class="nw"> NCut(V1，V2) </em> </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/93a0852381c59b6ed84182d027da4f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*td6BkWWQWz_4mwKKi1xzyA.png"/></div></figure><h2 id="dc3c" class="nl lw it bd lx oe of dn mb og oh dp mf li oi oj mh lm ok ol mj lq om on ml oo bi translated"><strong class="ak"> <em class="pc">怎么会这样？</em> </strong></h2><p id="229b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">NCut 的上述方程可以重新表示为<a class="ae ky" href="https://en.wikipedia.org/wiki/Rayleigh_quotient#:~:text=The%20Rayleigh%20quotient%20is%20used,approximation%20from%20an%20eigenvector%20approximation." rel="noopener ugc nofollow" target="_blank">瑞利商</a>，其最小值由一个广义特征值问题的最小特征值得到。设 x 为 N 维指示向量其中<strong class="lb iu"> <em class="nw"> xi </em> </strong> =1，若图节点<strong class="lb iu"> <em class="nw"> i </em> </strong>在 V1，否则为 0，则 Ncut 的最小化表示为如下瑞利商</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/f7d892124877e99f178ccb96b885129e.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*ejiaIy3NyiXu4kNeRKLsJw.png"/></div></figure><p id="a584" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在满足其中一个条件的情况下，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/b2d2f0d6ea7f15f98b52faaf8e3fb7e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/format:webp/1*HGicvbsoBvLQ2vRBaqmC8Q.png"/></div></figure><p id="3162" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> <em class="nw"> y </em> </strong>也像我们的指示向量 x 一样受到约束。</p><p id="18c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样，图划分问题就转化成了聚类问题。</p><h1 id="74da" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="ef23" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们从数据的特征开始，构建它的图和拉普拉斯算子，并使用它的谱嵌入(本征向量)，我们找到最佳分割以获得聚类。谱理论是处理图表示的特征值和特征向量的概念的来源，也用于机器学习的其他领域，如图像分割、谱图卷积神经网络以及工业和研究社区中的许多其他领域。</p><p id="a6cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文旨在总结机器学习和线性代数中的概念和应用，并希望它也能引发探索和了解更多的好奇心。</p><p id="38b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望有帮助。</p><p id="22a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请分享您的反馈，或者如果您在其他领域遇到这些概念的应用。</p><p id="93ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在 GitHub 上找到代码。</p><div class="pf pg gp gr ph pi"><a href="https://github.com/Taaniya/graph-analytics/blob/master/Graph_Laplacian_and_Spectral_Clustering.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd iu gy z fp pn fr fs po fu fw is bi translated">taaniya/图形分析</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">github.com</p></div></div><div class="pr l"><div class="ps l pt pu pv pr pw ks pi"/></div></div></a></div><h2 id="755b" class="nl lw it bd lx oe of dn mb og oh dp mf li oi oj mh lm ok ol mj lq om on ml oo bi translated">参考</h2><ul class=""><li id="9ac0" class="ms mt it lb b lc mn lf mo li mu lm mv lq mw lu mx my mz na bi translated">Ulrike von Luxburg，关于谱聚类的教程，2006 年。</li><li id="5f39" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated"><a class="ae ky" href="https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf" rel="noopener ugc nofollow" target="_blank">时剑波和吉滕德拉·马利克，</a>标准化切割和图像分割，2000 年</li><li id="f866" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">吉尔伯特·斯特朗教授，图中聚类，麻省理工学院开放式课程。</li><li id="0098" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated"><a class="ae ky" href="http://www2.econ.iastate.edu/classes/econ501/Hallam/documents/Quad_Forms_000.pdf" rel="noopener ugc nofollow" target="_blank">二次型和正定矩阵</a></li></ul></div></div>    
</body>
</html>