<html>
<head>
<title>Gradient Descent, clearly explained in Python, Part 1: The troubling theory</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降，在 Python，第 1 部分:麻烦的理论中有清楚的解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-clearly-explained-in-python-part-1-the-troubling-theory-49a7fa2c4c06?source=collection_archive---------40-----------------------#2020-08-07">https://towardsdatascience.com/gradient-descent-clearly-explained-in-python-part-1-the-troubling-theory-49a7fa2c4c06?source=collection_archive---------40-----------------------#2020-08-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="8100" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">当你第一次进入机器学习和数据科学领域时，你肯定会遇到的第一件事就是梯度下降。但问题是，到底什么是梯度下降<strong class="jz iu">？它是做什么的？是一回事，还是梯度下降有不同版本？到本文结束时，你将能够回答所有这些问题。</strong></p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi kv"><img src="../Images/81c5b4105263e035edff9ccc55c6ee7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*8igWexUP3iudvcpsoEcJxQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来自<a class="ae lh" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="bf2f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">到本文结束时，您将(希望)拥有:</p><ol class=""><li id="96a2" class="li lj it jz b ka kb ke kf ki lk km ll kq lm ku ln lo lp lq bi translated">了解什么是梯度下降实际上<strong class="jz iu">是</strong>和它做什么。</li><li id="aee6" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated">了解梯度下降的不同“味道”。</li></ol><p id="bf96" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在第二部分中，我们将从头开始编写我们在本文中学到的所有内容。所以，敬请关注！</p><p id="84aa" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu"> <em class="lw">事不宜迟，我们开始吧！</em>T9】</strong></p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi lx"><img src="../Images/dd322f9c997c3f89acafce9c6cc73fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8gFoTaT9yNrS_nALvkxTAw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@wanderlabs?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Wander Labs </a>在<a class="ae lh" href="https://unsplash.com/s/photos/begin?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="5d71" class="mc md it bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">直观理解梯度下降</h1></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="25e3" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在，在我开始抛出令人困惑的希腊符号之前(不幸的是我会在某个时候抛出，但请耐心等待)，我想让您从广义上直观地了解它到底是做什么的。</p><p id="0d74" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在，想象你在一座山顶上，你想找到最快的<strong class="jz iu"><em class="lw"/></strong>方式从山顶下来。因此，使用普通逻辑，可以如下处理该问题:</p><ol class=""><li id="0248" class="li lj it jz b ka kb ke kf ki lk km ll kq lm ku ln lo lp lq bi translated">环顾四周，看看下降的最佳方向是什么。</li><li id="253d" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated">朝那个方向迈出一步。</li><li id="cee2" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated">重复这个过程，直到你到达底部。</li></ol><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi na"><img src="../Images/358eaf07788efb60cba9a3714d3085d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mxR0AG1o75ssAg3EA5Rhaw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@rustyspear11?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Gavin Spear </a>在<a class="ae lh" href="https://unsplash.com/s/photos/standing-on-mountain?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="9333" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">注意</strong>:这个类比中需要记住的两个重要因素是:</p><ol class=""><li id="3294" class="li lj it jz b ka kb ke kf ki lk km ll kq lm ku ln lo lp lq bi translated"><strong class="jz iu">步长</strong> : <em class="lw">步长应该有多大/多小？</em></li><li id="61d6" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated"><strong class="jz iu">重复</strong> : <em class="lw">这个过程我要重复多少次？</em></li></ol></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="bfa1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在，记住这个类比，让我们来看看技术！T41】</p><h1 id="6ea0" class="mc md it bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">成本函数</h1><p id="c380" class="pw-post-body-paragraph jx jy it jz b ka nb kc kd ke nc kg kh ki nd kk kl km ne ko kp kq nf ks kt ku im bi translated">现在，在我们进入文章的主要数学之前，让我们简单地谈论一下成本函数。一个<em class="lw">成本函数</em>本质上是一个衡量你的模型表现如何的函数。我的意思是，线索就在名字里:它测量你的模型预测的损失。一些类型的成本函数包括:</p><ol class=""><li id="f6c4" class="li lj it jz b ka kb ke kf ki lk km ll kq lm ku ln lo lp lq bi translated">均方误差</li><li id="3ab6" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated">绝对平均误差</li><li id="25c7" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated">交叉熵</li><li id="43ec" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated">均方根误差</li></ol><p id="9963" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">例如，均方误差的算法如下:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/3ae63d7234e6ab5adb6ad918805b0984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/1*D-7K2fUG8Ev3iA_81svXjw.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由 Richie Ng 拍摄</p></figure></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="74a3" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如果你曾经参加过 Kaggle 比赛，这些通常被称为<em class="lw">评估指标。</em>通常，损失越低，您的型号性能越好。因此，举例来说，如果你预测房价并使用<em class="lw">均方差</em>，而你的成本是 25000 美元，这意味着你的模型表现不佳，因为它产生了 25000 美元的预测误差。</p><p id="bfac" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">回到我们的类比，如果你想象有一个 U 形曲线而不是一座山，有一个成本函数而不是一个人，初始成本值可能是 25，500。梯度下降的目的是将该成本最小化到 0( <em class="lw">全局最小值</em>)或更小的值(<em class="lw">局部最小值)。</em></p><h1 id="3e40" class="mc md it bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">数学上理解梯度下降</h1><p id="26a6" class="pw-post-body-paragraph jx jy it jz b ka nb kc kd ke nc kg kh ki nd kk kl km ne ko kp kq nf ks kt ku im bi translated"><em class="lw">快速提示:如果你想跳过这一部分，请便。但我向你保证，一旦你明白了，就不难理解了。我将给出一个非常高的梯度下降的数学概述，所以不要担心不能理解一切！</em></p><p id="91a3" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">梯度下降的正式定义如下:</p><p id="d225" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="lw">梯度下降是一种优化算法，用于寻找函数的最小值。它是通过得到一条线的负斜率的偏导数来计算的。</em></p><p id="faf6" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">老实说，那句话相当含糊，所以让我们用数学来分解它。梯度下降的数学表示如下:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/77b53f35c3553683026ea5d8f1b01d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*YiAkwR1uRIvoXV68RfdNkw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由 Richie Ng 拍摄</p></figure><p id="5ee8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">是的，我知道乍一看这看起来像一些复杂的爱因斯坦华夫饼，但请耐心听我说。现在，让我们理解一些符号:</p><p id="cb27" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="lw"> θj </em>:这是你的参数向量中的一个参数，或者系数。本质上是你的价值乘以什么，你在优化什么。</p><p id="6b78" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="lw"> α </em>这控制了步长。在机器学习行话中，它通常被称为<em class="lw">学习率。</em></p><p id="b441" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">m:数据集中训练样本的数量</p><p id="c8ca" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="lw"> h(x) </em>:您对数值的预测</p><p id="8ae3" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="lw"> y </em>:实际值</p><p id="ef1f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="lw"> x(i)j </em>:这是指第<em class="lw"> j </em>行第<em class="lw"> i </em>列的一个 your X 变量。我在这里不会讲太多技术，所以不要太担心，因为 Python 为我们完成了大部分困难的部分。</p><p id="4a3e" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">σ:这只是说我们在对将要计算的值求和。</p><p id="05e0" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">:=这意味着我们用将要计算的值替换当前值</p></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="2e5a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">本质上，这个算法陈述的是:</p><p id="2dc8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi">{</p><p id="6e7d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">对于我们试图优化的每个参数，按如下方式更改值:</p><ol class=""><li id="005e" class="li lj it jz b ka kb ke kf ki lk km ll kq lm ku ln lo lp lq bi translated">取我们参数的当前值</li><li id="78ad" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated">通过得到我们的预测值减去实际值的总和乘以我们的 X 特征并乘以我们的学习率来计算梯度</li><li id="30bf" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated">从上面计算的值中减去我们当前的参数值</li><li id="255c" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated">重复一定的次数。</li></ol><p id="6de7" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi">}</p><p id="4af4" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">现在，关于学习率和初始参数的一个重要注意事项:</strong></p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/bd7f2daa0accad59d9b216538331dd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*rcIXbShyVWcpADocTqztlg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片可在<a class="ae lh" href="https://stackoverflow.com/questions/42332058/gradient-descent-thetas-not-converging" rel="noopener ugc nofollow" target="_blank"> StackOverflow </a>上找到</p></figure><ol class=""><li id="bb32" class="li lj it jz b ka kb ke kf ki lk km ll kq lm ku ln lo lp lq bi translated"><em class="lw">如果学习率过大，梯度下降将错过局部最小值，并在不同点不受控制地波动</em></li><li id="80ec" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated"><em class="lw">如果学习率太小，算法达到最小值的时间太长，会耗尽你电脑的电量。</em></li><li id="0885" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated"><em class="lw">初始参数一般随机设置。这就是(yep) </em> <strong class="jz iu"> <em class="lw">随机初始化</em> </strong> <em class="lw">。不同的初始化可能导致算法只达到局部最小值而不是全局最小值。</em></li></ol><h1 id="5237" class="mc md it bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">梯度下降的类型</h1><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/dd56512274e45755a78b0c8f3717fa68.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*0wnEqeE6s5ihleWiUxgaFQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者照片</p></figure><p id="b265" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">上述算法被称为<strong class="jz iu">批量梯度下降。</strong>之所以取这个名字，是因为在每一步，它都会计算整个(<em class="lw">批次</em>)数据集的梯度。现在，你可能已经猜到了，但在大型数据集上，这是不可行的，因为这种工作所需的时间和计算能力太长了。这让我们想到了梯度下降的另外两种流行“味道”:</p><ol class=""><li id="7040" class="li lj it jz b ka kb ke kf ki lk km ll kq lm ku ln lo lp lq bi translated"><strong class="jz iu">随机梯度下降</strong></li><li id="0c4f" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated"><strong class="jz iu">小批量梯度下降</strong></li></ol><h1 id="c41d" class="mc md it bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">随机梯度下降</h1><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nk"><img src="../Images/724783cf43ec7247d3e37f4f743c8b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9FEwuZ1chjs1elm5VGUmsg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由 Richie Ng 拍摄</p></figure><p id="30a1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在这种方法中，不是在数据集的所有上计算梯度，而是在随机选择的训练样本上计算梯度(因此得名“随机”，意思是随机的)。这种方法比批量梯度下降快得多，因为它当时只计算一个示例的梯度，而不是整个数据集。</p><p id="7ea1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然而，由于其随机性，它并不倾向于遵循平滑的曲线，而是倾向于一直来回跳动。然而，在大型数据集上，它肯定会优于批量梯度下降</p><h1 id="dfd9" class="mc md it bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">小批量梯度下降</h1><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nl"><img src="../Images/59f3418a76709c183d6e790fab72cff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z1BQMkkFrFq5Mp9ND-_Bgg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由 Richie Ng 拍摄</p></figure><p id="c955" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在这种方法中，它不是计算整个数据集或数据集的随机样本的梯度，而是计算数据集的小子集，通常称为<em class="lw">小批量。这也是一个非常强大的算法，因为它提供了速度和准确性，并且不像随机梯度下降那样随机。</em></p><p id="4d5e" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">然而，根据参数的初始化，它有时可能会陷入局部最小值。因此，虽然它可能返回经过优化的良好参数，但它们不是最佳参数。</p></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="bfaf" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这实质上是梯度下降及其高级实现。现在，我确实把事情简单化了，但是这是一个基础，你可以学习并尝试自己实现它！我肯定会建议浏览互联网，以获得更好的理解！</p></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="a7e0" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">唷！这是很大的收获，坚持到最后做得很好。如果你想要一个更严谨和直观的解释，请查阅以下资源:</p><ol class=""><li id="9bac" class="li lj it jz b ka kb ke kf ki lk km ll kq lm ku ln lo lp lq bi translated"><a class="ae lh" href="https://www.youtube.com/watch?v=sDv4f4s2SB8" rel="noopener ugc nofollow" target="_blank">逐步梯度下降</a>通过 Statquest</li><li id="927d" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated"><a class="ae lh" href="https://www.youtube.com/watch?v=umAeJ7LMCfU&amp;t=245s" rel="noopener ugc nofollow" target="_blank">梯度下降:机器人的人工智能</a></li><li id="0ab3" class="li lj it jz b ka lr ke ls ki lt km lu kq lv ku ln lo lp lq bi translated"><a class="ae lh" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank"> Andrew NG 的 Coursera 机器学习课程</a></li></ol><p id="0789" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如果想对机器学习有一个全面的了解，我强烈建议考最后一门。安德鲁擅长用简单的术语直观地解释复杂的话题。</p><p id="b254" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在，这是一个很大的挑战。因此，与其拼命去想，不如试着休息一下。让你的大脑休息一下，让这个话题沉淀下来。然后，当你觉得神清气爽的时候，再回来试着更好地把握概念！</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nm"><img src="../Images/d8a4a67c05ff40057f01a9ee6d6e0090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UeG_fJ6o4qXiFJSQRwUGUw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@sincerelymedia?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">真诚媒体</a>在<a class="ae lh" href="https://unsplash.com/s/photos/rest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="9abc" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">非常感谢你阅读这篇文章，我将很快推出第二部分，所以请做好准备，因为真正的乐趣才刚刚开始！</p></div></div>    
</body>
</html>