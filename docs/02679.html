<html>
<head>
<title>The hidden linear algebra of reinforcement learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的隐藏线性代数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-hidden-linear-algebra-of-reinforcement-learning-406efdf066a?source=collection_archive---------17-----------------------#2020-03-15">https://towardsdatascience.com/the-hidden-linear-algebra-of-reinforcement-learning-406efdf066a?source=collection_archive---------17-----------------------#2020-03-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="7b2a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">线性代数的基础如何支持深度强化学习的顶点？答案就在求解马尔可夫决策过程时的迭代更新中。</p><p id="2044" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">强化学习(RL)是一组智能方法，用于<strong class="js iu">迭代</strong>学习一组任务。由于计算机科学是一个<strong class="js iu">计算</strong>领域，这种学习发生在状态、动作等向量上。以及动力学或跃迁矩阵。状态和向量可以采取不同的形式，但是我们怎么能看到算法的收敛成为技术社区的头条新闻呢？当我们想到将一个变量向量传递给某个线性系统，并得到类似的输出时，应该会想到<strong class="js iu">特征值</strong>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/260c790fc910f7a27112401afd69adde.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*_cikjl9_DwbuNV8WPAOuGw.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">特征值，特征向量方程。</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi la"><img src="../Images/dea4094d93bb405c60a3c9a91e4c8111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vec6r-NgsbCUxGrbOdnXPA.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">MDP中赝本征空间的示意图。MDP是由特征值决定的，但是它们在一个子空间里，可能有点复杂。</p></figure><p id="8bc5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文将引导您理解在RL环境中解决任务的迭代方法(收敛到最优策略)。这个基础将反映<em class="lf">环境</em>的特征向量和特征值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi lg"><img src="../Images/a0a6e2995b75d6bbd78d5f7c7a7c0da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BcismgmZz9O-tNlU1qjGhg.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">沉思的小狗——来源:作者的好朋友。</p></figure></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="79ad" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">回顾马尔可夫决策过程</h1><p id="ca2e" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">马尔可夫决策过程是支持强化学习的随机模型。如果你很熟悉，你可以跳过这一部分。</p><h2 id="fa30" class="mr lp it bd lq ms mt dn lu mu mv dp ly kb mw mx mc kf my mz mg kj na nb mk nc bi translated">定义</h2><ul class=""><li id="1ab1" class="nd ne it js b jt mm jx mn kb nf kf ng kj nh kn ni nj nk nl bi translated">状态集合<strong class="js iu"> s ∈ S </strong>。这些状态代表了世界上所有可能的构型。</li><li id="5905" class="nd ne it js b jt nm jx nn kb no kf np kj nq kn ni nj nk nl bi translated">一组动作<strong class="js iu"> a ∈ A </strong>。动作是代理可以采取的所有可能动作的集合。</li><li id="2a73" class="nd ne it js b jt nm jx nn kb no kf np kj nq kn ni nj nk nl bi translated">一个转移函数<strong class="js iu"> T(s，a，s’)</strong>。T(s，a，s’)持有MDP的<strong class="js iu">不确定性</strong>。给定当前位置和提供的动作，<strong class="js iu"> <em class="lf"> T </em> </strong>决定下一个状态跟随的频率。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nr"><img src="../Images/f5b301d7a555d85406c2c4dc4c87f4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sB0cYflezQERRESh.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">MDP就是一个例子。来源——我在CS188做的讲座。</p></figure><ul class=""><li id="5196" class="nd ne it js b jt ju jx jy kb nt kf nu kj nv kn ni nj nk nl bi translated">一个奖励函数<strong class="js iu"> R(s，a，s’)。任何代理人的目标都是回报总和最大化。</strong>这个函数说的是每一步获得多少奖励。一般来说，在每一步都会有一个小的负奖励(成本)来鼓励快速解决问题，在最终状态会有大的正(目标)或负(失败的任务)奖励。</li><li id="f2b4" class="nd ne it js b jt nm jx nn kb no kf np kj nq kn ni nj nk nl bi translated">[0，1]中的折扣因子<strong class="js iu"> γ (gamma) </strong>，将当前(下一步)的价值调整为未来奖励。这抓住了现在奖励相对于以后奖励的重要性。非常重要的是，在我们讨论如何解决MDP时，可以期待更多的信息。</li><li id="2af3" class="nd ne it js b jt nm jx nn kb no kf np kj nq kn ni nj nk nl bi translated">起始状态<strong class="js iu"> s0 </strong>，也可能是终止状态。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e72d316a744d46c5ba276b2fbf097fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*GgynckNep4B8Z5uRzis_2g.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">来源——我在<a class="ae ns" href="https://inst.eecs.berkeley.edu/~cs188/sp20/" rel="noopener ugc nofollow" target="_blank"> CS188 </a>做的讲座11。</p></figure><h2 id="c1c9" class="mr lp it bd lq ms mt dn lu mu mv dp ly kb mw mx mc kf my mz mg kj na nb mk nc bi translated">重要的价值观</h2><p id="0705" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">MDP有两个重要的特征效用-状态值和机会节点的q值。</p><ul class=""><li id="af10" class="nd ne it js b jt ju jx jy kb nt kf nu kj nv kn ni nj nk nl bi translated"><strong class="js iu">一个状态的价值</strong>:一个状态的价值是从一个状态出发的回报的最优递归和。<em class="lf">如果机器人在火坑里、宝石附近或沙发上，左边的状态值会有很大不同。</em></li><li id="ce63" class="nd ne it js b jt nm jx nn kb no kf np kj nq kn ni nj nk nl bi translated">状态动作对的Q值:Q值是与状态动作对相关的折扣奖励的最优和。<em class="lf">一个状态的q值由一个动作决定——因此，如果指向火焰内部或外部，壁架上的q值会有很大变化！</em></li></ul><p id="5ce2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这两个值通过<a class="ae ns" href="https://en.wikipedia.org/wiki/Mutual_recursion" rel="noopener ugc nofollow" target="_blank">相互递归</a>关联，贝尔曼更新。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="b010" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">行李员更新</h1><p id="85cb" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">理查德·e·贝尔曼是一位数学家，为现代控制和优化理论奠定了基础。通过一个递归一步方程，<strong class="js iu">一个贝尔曼更新方程，</strong>大型优化问题可以高效求解。通过递归贝尔曼更新，可以用<strong class="js iu">动态规划</strong>建立优化或控制问题，这是一个创建更小、更易于计算的问题的过程。这个过程从末端递归地进行<em class="lf">到</em>——一种滚动时域方法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/8ba9bb57626499c379c4247a92dd7378.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*VR-pIaWMPuxfuNejMFCq3w.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">理查德·e·贝尔曼(1920-1984)——维基百科。</p></figure><ol class=""><li id="a6d1" class="nd ne it js b jt ju jx jy kb nt kf nu kj nv kn ny nj nk nl bi translated"><strong class="js iu">贝尔曼方程</strong> : <em class="lf">公式化为动态规划</em> <strong class="js iu"> <em class="lf">的优化问题最优性的必要条件。</em>T25】</strong></li><li id="c68a" class="nd ne it js b jt nm jx nn kb no kf np kj nq kn ny nj nk nl bi translated"><strong class="js iu">动态编程</strong> : <em class="lf">通过将优化问题分解成最佳子结构来简化优化问题的过程。</em></li></ol><p id="8841" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在强化学习中，我们使用贝尔曼更新过程来求解状态-动作空间的最优值和q值。这最终形成了来自给定位置的<strong class="js iu">预期未来回报总额</strong>。</p><p id="68ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这里，我们可以看到来自审查交错的所有值。符号<strong class="js iu"> (*) </strong>表示最优，所以正确或收敛。我们有由最佳行为决定的状态值，一个q状态，然后是两个递归定义。递归值平衡了<strong class="js iu"><em class="lf">【T(s，a，s’)</em></strong><em class="lf"/>中访问任意状态的<em class="lf">概率和任意跃迁</em><strong class="js iu"><em class="lf">【R(s，a，s’)</em></strong><em class="lf"/>的<em class="lf">报酬，为状态-动作空间<em class="lf">的值创建一个全局映射。</em></em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nz"><img src="../Images/3554ed906a52d20aae7e93735819086a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yfq1rtWWSspNg86mnvmmOQ.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">最佳值与最佳动作条件q值相关。然后值和q值更新规则非常相似(加权转换、奖励和折扣)。顶部)值与q值的耦合；mid) Q值递归，bot)值迭代。来源<a class="ae ns" href="https://inst.eecs.berkeley.edu/~cs188/sp20/" rel="noopener ugc nofollow" target="_blank">加州大学伯克利分校的cs 188</a>。</p></figure><p id="db0b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">他们在这里的关键点是，我们是乘矩阵(<strong class="js iu"> <em class="lf"> R，T </em> </strong>)，乘向量(<strong class="js iu"> <em class="lf"> V，U </em> </strong>)，来迭代求解收敛。<em class="lf">由于一个状态的值如何由它们的邻居</em><strong class="js iu"><em class="lf">’</em></strong>确定，这些值将从任何初始状态收敛。有关MDP的更多信息，请参见<a class="ae ns" rel="noopener" target="_blank" href="/what-is-a-markov-decision-process-anyways-bdab65fd310c">我写的这篇简介</a>。</p><h2 id="3e19" class="mr lp it bd lq ms mt dn lu mu mv dp ly kb mw mx mc kf my mz mg kj na nb mk nc bi translated">强化学习？</h2><p id="9d7c" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">“有人告诉我会有RL，”—你，读者，4分钟后。这都是强化学习，我断言<strong class="js iu">理解算法所基于的假设和模型会让你比仅仅从OpenAI复制python教程准备得更好</strong>。之后再做。我已经指导了很多学生在RL中工作，那些做得更多的人总是那些了解正在发生什么，然后如何应用它的人。</p><p id="d56f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">也就是说，这是<strong class="js iu">离在线q学习</strong>的一小步，在在线q学习中，我们用T和R的样本来估计相同的贝尔曼更新，而不是在方程中显式地使用它们。所有相同的断言都适用于，但它是基于概率分布和期望。<a class="ae ns" href="https://daiwk.github.io/assets/dqn.pdf" rel="noopener ugc nofollow" target="_blank"> Q-learning是~2015年</a>解决雅达利游戏等著名算法。</p><h1 id="b240" class="lo lp it bd lq lr oa lt lu lv ob lx ly lz oc mb mc md od mf mg mh oe mj mk ml bi translated">隐藏数学</h1><h2 id="8772" class="mr lp it bd lq ms mt dn lu mu mv dp ly kb mw mx mc kf my mz mg kj na nb mk nc bi translated">特征值？哼。</h2><p id="06ef" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">回想一下系统<em class="lf">的特征值-特征向量对(<em class="lf"> λ，u</em>)A</em>是一个向量和标量，使得系统作用的向量返回原向量的标量倍数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/260c790fc910f7a27112401afd69adde.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*_cikjl9_DwbuNV8WPAOuGw.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">特征值，特征向量方程。</p></figure><p id="32ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">特征值和特征向量的美妙之处在于，当它们跨越状态空间时(对于大多数物理系统来说，广义特征向量保证了这一点)，每个向量都可以写成其他特征向量的组合。然后，在离散系统中，特征向量控制从任何初始状态的演化——任何初始向量将组合成特征向量的<strong class="js iu">线性组合。</strong></p><h2 id="cea8" class="mr lp it bd lq ms mt dn lu mu mv dp ly kb mw mx mc kf my mz mg kj na nb mk nc bi translated">随机矩阵和马尔可夫链</h2><p id="553f" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">MDP与马尔可夫链非常接近，但在结构上并不相同。马尔可夫链由转移矩阵<strong class="js iu"> <em class="lf"> P </em> </strong>决定。概率矩阵的作用类似于对动作求和的转移矩阵<strong class="js iu"> <em class="lf"> T(s，a，s’)</em></strong><em class="lf">。在马尔可夫链中，下一个状态由下式决定:</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/970c33a06ca48e3772334ca829ce30e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*uhVLwOgfclaVe3rnMMDVZw.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">一个<a class="ae ns" href="https://en.wikipedia.org/wiki/Stochastic_matrix" rel="noopener ugc nofollow" target="_blank">随机矩阵的演化。</a></p></figure><p id="99a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个矩阵<strong class="js iu"> P </strong>有一些特殊的值——你可以看到这是一个<em class="lf">特征值方程，所有特征值等于一个</em>(图a <em class="lf"> λ </em> =1预乘方程左侧)。为了使矩阵<strong class="js iu">保证</strong>的特征值等于1，所有列的总和必须等于1。</p><p id="d274" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在在RL中寻找的是，我们的解的演化如何与概率分布的收敛相关联？我们通过将用于<strong class="js iu"> <em class="lf"> V* </em> </strong>和<strong class="js iu"> <em class="lf"> Q* </em> </strong>的迭代算子公式化为线性算子(a矩阵)<strong class="js iu"> <em class="lf"> B </em> </strong>来做到这一点。收敛可能很棘手——我们使用的值和q值向量不是本征向量——它们收敛到本征向量，但这对于了解本征向量如何控制系统并不重要。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/d30402f037fa4556a71ca3757852bd7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*QUgsI30vgMVG-gqNJrX17A.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">贝尔曼算子，B，像一个带特征向量的线性变换，特征值对<em class="og"> λ=1。</em></p></figure><div class="kp kq kr ks gt ab cb"><figure class="oh kt oi oj ok ol om paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><img src="../Images/e7b6661b76826c52e3c7ec41f0b2d6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*RYyLnW9x6yaaYF_qRdI0xQ.png"/></div></figure><figure class="oh kt on oj ok ol om paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><img src="../Images/dcc015671c352c54d6dbcc4184cf1b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*L5fZ6H1m0Ma5pj20sVsuEg.png"/></div></figure><figure class="oh kt oo oj ok ol om paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><img src="../Images/6225530c9d53ad8d8ebe19620e2baf13.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*ks4I3r4zbfP0rjMHpcNpQQ.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk op di oq or translated">任何初始值分布将收敛到特征空间的形状。该图没有显示贝尔曼更新的确切特征值，而是显示了空间的形状如何随着值的递归更新而演变。最初，这些值将是完全未知的，但是随着学习的出现，已知的值将收敛到与环境完全匹配。</p></figure></div><h1 id="ffb4" class="lo lp it bd lq lr oa lt lu lv ob lx ly lz oc mb mc md od mf mg mh oe mj mk ml bi translated">贝尔曼矩阵更新</h1><p id="3f0b" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">到目前为止，我们知道，如果我们能够以一种更简单的形式制定我们的贝尔曼更新(线性更新)，一个方便的本征结构将出现。如何将我们的<strong class="js iu"> <em class="lf"> Q </em> </strong>更新公式化为一个简单的更新方程式？我们从一个Q迭代方程开始(最佳值代替右边的等价Q值方程)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi os"><img src="../Images/a61812be1ff516befa2b91c566b9ec61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e9eKHAC19-AGbjikO3AJQg.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">MDP的q迭代。</p></figure><p id="e45d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">实现这种转变需要几个小步骤(当然还有几个大的假设)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ot"><img src="../Images/431554f1c0a2d0ba8436bd7992c39509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hqRpZwu2YaWkKCpjAQmAlQ.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">将我们的系统移向线性算子(矩阵)</p></figure><h2 id="7500" class="mr lp it bd lq ms mt dn lu mu mv dp ly kb mw mx mc kf my mz mg kj na nb mk nc bi translated">I)让我们将几个术语重新表述为<strong class="ak">通用表格</strong></h2><p id="032d" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">前半段更新，求和超过<strong class="js iu"> <em class="lf"> R，T，</em> </strong>是显式奖励数；我们称之为<strong class="js iu"> R(s) </strong>。接下来，我们将转换的求和改为一个概率矩阵(方便地匹配一个马尔可夫矩阵)。同样，这导致了下一步——从<strong class="js iu"> <em class="lf"> Q </em> </strong>到实用程序的变化。(回想一下，你可以从一个总和中取出一个最大值(把它想象成一个更一般的上限)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/b9e93732beb88703a2a3d6770a976956.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*wAgCtVP01BRilminAC_zrg.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">接近我们想要的贝尔曼矩阵，其中P(s'|s，a)将决定我们矩阵的演化。</p></figure><h2 id="971c" class="mr lp it bd lq ms mt dn lu mu mv dp ly kb mw mx mc kf my mz mg kj na nb mk nc bi translated">ii) <strong class="ak">我们把这个做成向量方程</strong>。</h2><p id="47ce" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">我们最感兴趣的是效用，<strong class="js iu"><em class="lf"/></strong>【如何演化为MDP。效用意味着价值或q值。我们可以简单地将我们的<strong class="js iu"> <em class="lf"> Q </em> </strong>改写成<strong class="js iu"><em class="lf"/></strong>而没有太大的变化，但这意味着我们假设一个固定的策略。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/93dba2c24997706890948fd374dac2e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*z4mqd3mKlsqy0us0nPUWiQ.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">将q状态转换为特征值的通用效用向量。</p></figure><p id="3fef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">重要的是要记住，即使对于一个多维的物理系统来说，如果我们将所有测量的状态堆叠成一个长数组，那么状态的效用也是一个向量。一个固定的政策不会改变收敛——它只是意味着我们必须重新审视这一点，以学习如何迭代地获得一个政策。</p><h2 id="3bc7" class="mr lp it bd lq ms mt dn lu mu mv dp ly kb mw mx mc kf my mz mg kj na nb mk nc bi translated">三。采取固定政策</h2><p id="82a9" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">如果你假设一个固定的政策，最大化超过<strong class="js iu"> <em class="lf">一</em> </strong>就消失了。最大化算子明显是非线性的，但是线性代数中存在特征向量加上一个附加向量的形式(提示— <a class="ae ns" href="https://en.wikipedia.org/wiki/Generalized_eigenvector" rel="noopener ugc nofollow" target="_blank">广义特征向量</a>)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ow"><img src="../Images/5f5dfede5653898c752e4d6fbd38249d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*51WA8QGz0yTQizbzW_oDLQ.png"/></div></div></figure><p id="374b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面这个方程是贝尔曼效用更新的一般形式。我们想要一个线性算子，<strong class="js iu"> <em class="lf"> B </em> </strong>，然后我们可以看到这是一个特征值演化方程。它看起来有点不同，但这最终是我们想要的形式，减去几个线性代数断言，所以我们有我们的贝尔曼更新。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/d30402f037fa4556a71ca3757852bd7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*QUgsI30vgMVG-gqNJrX17A.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">贝尔曼算子，B，像一个带特征向量的线性变换，特征值对<em class="og"> λ=1。</em></p></figure><blockquote class="ox oy oz"><p id="295c" class="jq jr lf js b jt ju jv jw jx jy jz ka pa kc kd ke pb kg kh ki pc kk kl km kn im bi translated">从计算上来说，我们可以得到我们想要的特征向量，但是从分析上来说这样做是有挑战性的，因为在这个过程中要做很多假设。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi pd"><img src="../Images/9c4d72be611d94f0f15aac3149746ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h3loZeM-4FuV1nUQMapISA.jpeg"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">来源-作者，詹姆斯敦里。</p></figure><h1 id="f627" class="lo lp it bd lq lr oa lt lu lv ob lx ly lz oc mb mc md od mf mg mh oe mj mk ml bi translated">外卖食品</h1><p id="4687" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">线性运算符向您展示了某些离散的线性系统将如何演变，以及我们在强化学习中使用的环境遵循的结构。</p><blockquote class="pe"><p id="193a" class="pf pg it bd ph pi pj pk pl pm pn kn dk translated">我们收集的数据的特征值和特征向量可以表示RL问题的潜在值空间。</p></blockquote><p id="9c36" class="pw-post-body-paragraph jq jr it js b jt po jv jw jx pp jz ka kb pq kd ke kf pr kh ki kj ps kl km kn im bi translated">变量变化、线性变换、在线q-learning(而不是这里的q-iteration)以及更多的细节将在以后的文章中介绍。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><p id="63ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该系列的下一篇文章:</p><div class="pt pu gp gr pv pw"><a rel="noopener follow" target="_blank" href="/fundamental-iterative-methods-of-reinforcement-learning-df8ff078652a"><div class="px ab fo"><div class="py ab pz cl cj qa"><h2 class="bd iu gy z fp qb fr fs qc fu fw is bi translated">强化学习的基本迭代方法</h2><div class="qd l"><h3 class="bd b gy z fp qb fr fs qc fu fw dk translated">学习价值和策略迭代能掌握多少强化学习？很多。</h3></div><div class="qe l"><p class="bd b dl z fp qb fr fs qc fu fw dk translated">towardsdatascience.com</p></div></div><div class="qf l"><div class="qg l qh qi qj qf qk ku pw"/></div></div></a></div><div class="pt pu gp gr pv pw"><a rel="noopener follow" target="_blank" href="/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7"><div class="px ab fo"><div class="py ab pz cl cj qa"><h2 class="bd iu gy z fp qb fr fs qc fu fw is bi translated">强化学习算法的收敛性</h2><div class="qd l"><h3 class="bd b gy z fp qb fr fs qc fu fw dk translated">有什么简单的收敛界限吗？</h3></div><div class="qe l"><p class="bd b dl z fp qb fr fs qc fu fw dk translated">towardsdatascience.com</p></div></div><div class="qf l"><div class="ql l qh qi qj qf qk ku pw"/></div></div></a></div></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><p id="b621" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更多？订阅我关于机器人、人工智能和社会的时事通讯！</p><div class="pt pu gp gr pv pw"><a href="https://robotic.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="px ab fo"><div class="py ab pz cl cj qa"><h2 class="bd iu gy z fp qb fr fs qc fu fw is bi translated">自动化大众化</h2><div class="qd l"><h3 class="bd b gy z fp qb fr fs qc fu fw dk translated">一个关于机器人和人工智能的博客，让它们对每个人都有益，以及即将到来的自动化浪潮…</h3></div><div class="qe l"><p class="bd b dl z fp qb fr fs qc fu fw dk translated">robotic.substack.com</p></div></div><div class="qf l"><div class="qm l qh qi qj qf qk ku pw"/></div></div></a></div></div></div>    
</body>
</html>