<html>
<head>
<title>Fuzzy Matching and Deduplicating Hundreds of Millions of Records with Splink</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Splink对数亿条记录进行模糊匹配和重复数据删除</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fuzzy-matching-and-deduplicating-hundreds-of-millions-of-records-using-apache-spark-93d0f095001f?source=collection_archive---------22-----------------------#2020-04-16">https://towardsdatascience.com/fuzzy-matching-and-deduplicating-hundreds-of-millions-of-records-using-apache-spark-93d0f095001f?source=collection_archive---------22-----------------------#2020-04-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="137e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">支持Python、PySpark和AWS Athena的快速、准确和可扩展的记录链接</h2></div><h2 id="9038" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">摘要</h2><p id="a0ad" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">Splink 是一个用于概率记录链接(实体解析)的Python库。它支持使用<code class="fe lv lw lx ly b">Apache Spark</code>、<code class="fe lv lw lx ly b">AWS Athena</code>或<code class="fe lv lw lx ly b">DuckDB</code>后端运行记录链接工作负载。</p><p id="1b22" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">其主要特点是:</p><ul class=""><li id="e8fb" class="me mf iq ld b le lz lh ma ko mg ks mh kw mi lt mj mk ml mm bi translated">它非常快。使用<code class="fe lv lw lx ly b">DuckDB</code>后端，它能够在两分钟内连接现代笔记本电脑上的一百万条记录。</li><li id="74b7" class="me mf iq ld b le mn lh mo ko mp ks mq kw mr lt mj mk ml mm bi translated">它非常精确，支持词频调整和复杂的模糊匹配逻辑。</li><li id="c31d" class="me mf iq ld b le mn lh mo ko mp ks mq kw mr lt mj mk ml mm bi translated">它能够使用<code class="fe lv lw lx ly b">Spark</code>或<code class="fe lv lw lx ly b">AWS Athena</code>后端链接非常大的数据集(超过1亿条记录)。</li><li id="6d82" class="me mf iq ld b le mn lh mo ko mp ks mq kw mr lt mj mk ml mm bi translated">具有简单但高度可定制的界面，因此用户可以解决大多数记录链接和重复数据删除问题</li><li id="af03" class="me mf iq ld b le mn lh mo ko mp ks mq kw mr lt mj mk ml mm bi translated">不需要训练数据，因为可以使用无监督的方法来训练模型。</li><li id="6556" class="me mf iq ld b le mn lh mo ko mp ks mq kw mr lt mj mk ml mm bi translated">支持从探索性分析师到模型预测、诊断和质量保证的数据链接的完整工作流程。</li><li id="78ce" class="me mf iq ld b le mn lh mo ko mp ks mq kw mr lt mj mk ml mm bi translated">是健壮的，有一套自动化的单元和集成测试。</li></ul><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ms"><img src="../Images/17a22a23687bd6bed92eb049bdc617b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RIeQy9U_f3-Vh1tils_8bQ.png"/></div></div></figure><h2 id="ea6d" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">问题陈述</h2><p id="2ea1" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">一个常见的数据质量问题是有多个不同的记录引用同一个实体，但没有唯一的标识符将这些实体联系在一起。</p><p id="11cd" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">例如，客户数据可能已经被多次输入到多个不同的计算机系统中，具有不同的姓名拼写、不同的地址和其他打字错误。缺乏唯一的客户标识符给数据分析的所有阶段带来了挑战，从计算唯一客户的数量等基本问题，到用于机器学习目的的客户详细信息的特征工程。</p><p id="beb4" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">对这个问题有大量的理论和实证研究。该解决方案通常涉及使用统计估计、机器学习和/或基于规则的逻辑来计算新的唯一标识符列，该唯一标识符列允许实体被链接和分组。</p><p id="b740" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">然而，缺乏能够在数百万条记录的规模上解决这个问题的自由软件——这是大型组织中常见的规模。解决这个问题通常需要生成大量的记录比较，因此不适合R或Python中的内存解决方案。像Apache Spark这样的分布式计算框架，或者像DuckDB这样并行化且不受内存限制的后端，是更好的选择。</p><p id="e29d" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">我们发布了一个名为<code class="fe lv lw lx ly b">splink</code>的免费<a class="ae lu" href="https://github.com/moj-analytical-services/splink" rel="noopener ugc nofollow" target="_blank">开源</a>库，实现了fell egi-Sunter/期望最大化方法，这是数据链接文献的关键统计模型之一。这是一种无监督的学习算法，它为每对记录比较产生一个匹配分数。</p><h2 id="ac37" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">尝试一下</h2><p id="2eba" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">你可以使用我们的活页夹链接<a class="ae lu" href="https://mybinder.org/v2/gh/moj-analytical-services/splink_demos/master?urlpath=lab/tree/index.ipynb" rel="noopener ugc nofollow" target="_blank">在这里</a>在Jupyter笔记本上试试这个图书馆。</p><p id="3832" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">这些演示说明了如何使用这个库，但是请注意，它们是在免费服务器上以本地模式运行的，所以不要期望有很好的性能。</p><p id="4e87" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">您也可以访问我们的<a class="ae lu" href="https://moj-analytical-services.github.io/splink" rel="noopener ugc nofollow" target="_blank">文档网站</a>。</p><h2 id="5e6f" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">它是如何工作的</h2><p id="2d4a" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">Splink是Fellegi-Sunter模型的一个实现。该软件使用一种称为<a class="ae lu" href="https://en.wikipedia.org/wiki/Record_linkage#Probabilistic_record_linkage" rel="noopener ugc nofollow" target="_blank">阻塞</a>的方法生成成对记录比较，并为每对记录计算匹配分数，量化两个记录之间的相似性。</p><p id="5a35" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">匹配分数由称为部分匹配权重的参数确定。这些量化了比较的不同方面的重要性。</p><p id="47b9" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">例如，出生日期匹配比性别匹配更有利于两个记录匹配。邮政编码的不匹配可能提供不匹配的弱证据，因为人们搬家了，而出生日期的不匹配可能是不匹配记录的强有力证据。</p><p id="7f7c" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">这个简单的想法有很大的力量来构建高度微妙的模型。可以为任意数量的用户定义的场景计算部分匹配权重，而不仅仅是匹配或不匹配。例如，对于邮政编码不匹配但彼此相距在10英里以内的情况，可以估计部分匹配权重。</p><p id="c3cd" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">这些部分匹配权重被组合成一个总匹配分数，该分数表示两个记录匹配的证据的权重。</p><p id="198e" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">该库使用无监督学习(期望最大化算法)来估计这些匹配权重。你可以在我的互动培训材料中了解更多关于理论<a class="ae lu" href="https://www.robinlinacre.com/probabilistic_linkage/" rel="noopener ugc nofollow" target="_blank">的内容。</a></p><p id="67ba" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">关于这一切如何工作的更详细的视频描述可以在<a class="ae lu" href="https://youtu.be/msz3T741KQI?t=497" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ms"><img src="../Images/b1b255652e7914bda57b8558f2f9e77f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CLpqJCDPR5DQ5s6UEl-EPw.png"/></div></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">Splink的一些图形输出</p></figure><h2 id="6392" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">样本代码</strong></h2><p id="2463" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">我们试图设计一个简单的界面，但仍然可以适应大多数记录链接和重复数据删除问题。</p><p id="3bf1" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">在下面的代码中，我们:</p><ul class=""><li id="cb15" class="me mf iq ld b le lz lh ma ko mg ks mh kw mi lt mj mk ml mm bi translated">指定数据链接模型</li><li id="f06f" class="me mf iq ld b le mn lh mo ko mp ks mq kw mr lt mj mk ml mm bi translated">估计它的参数</li><li id="870f" class="me mf iq ld b le mn lh mo ko mp ks mq kw mr lt mj mk ml mm bi translated">使用该模型计算成对匹配分数</li><li id="2c43" class="me mf iq ld b le mn lh mo ko mp ks mq kw mr lt mj mk ml mm bi translated">将匹配分数分组以产生估计的唯一个人id</li></ul><pre class="mt mu mv mw gt ni ly nj nk aw nl bi"><span id="e17f" class="kf kg iq ly b gy nm nn l no np">from splink.duckdb.duckdb_linker import DuckDBLinker<br/>from splink.duckdb.duckdb_comparison_library import (<br/>    exact_match,<br/>    levenshtein_at_thresholds,<br/>)<br/><br/>import pandas as pd<br/><br/>df = pd.read_csv("./tests/datasets/fake_1000_from_splink_demos.csv")<br/><br/><em class="nq"># Specify a data linkage model</em><br/>settings = {<br/>    "link_type": "dedupe_only",<br/>    "blocking_rules_to_generate_predictions": [<br/>        "l.first_name = r.first_name",<br/>        "l.surname = r.surname",<br/>    ],<br/>    "comparisons": [<br/>        levenshtein_at_thresholds("first_name", 2),<br/>        exact_match("surname"),<br/>        exact_match("dob"),<br/>        exact_match("city", term_frequency_adjustments=True),<br/>        exact_match("email"),<br/>    ],<br/>}<br/><br/>linker = DuckDBLinker(df, settings)<br/><br/><em class="nq"># Estimate its parameters</em><br/>linker.estimate_u_using_random_sampling(target_rows=1e6)<br/><br/>blocking_rule_for_training = "l.first_name = r.first_name and l.surname = r.surname"<br/>linker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)<br/><br/>blocking_rule_for_training = "l.dob = r.dob"<br/>linker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)<br/><br/><em class="nq"># Use the model to compute pairwise match scores</em><br/>pairwise_predictions = linker.predict()<br/><br/><em class="nq"># Cluster the match scores into groups to produce an estimated unique person id</em><br/>clusters = linker.cluster_pairwise_predictions_at_threshold(pairwise_predictions, 0.95)<br/>clusters.as_pandas_dataframe(limit=5)</span></pre><h2 id="ff10" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">反馈</h2><p id="9889" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">我们感谢所有做出贡献并提供反馈的用户。请通过以下方式继续这样做:</p><ul class=""><li id="b21a" class="me mf iq ld b le lz lh ma ko mg ks mh kw mi lt mj mk ml mm bi translated">开始讨论如果你对如何做某事有疑问</li><li id="585f" class="me mf iq ld b le mn lh mo ko mp ks mq kw mr lt mj mk ml mm bi translated">提出<a class="ae lu" href="https://github.com/moj-analytical-services/splink/issues" rel="noopener ugc nofollow" target="_blank">一个问题</a>如果你发现了一个bug或者想要请求一个新特性</li><li id="27b8" class="me mf iq ld b le mn lh mo ko mp ks mq kw mr lt mj mk ml mm bi translated">如果你想修复一个错误或者增加一个特性，提出一个<a class="ae lu" href="https://github.com/moj-analytical-services/splink/pulls" rel="noopener ugc nofollow" target="_blank">拉请求</a></li></ul><p id="0517" class="pw-post-body-paragraph lb lc iq ld b le lz jr lg lh ma ju lj ko mb ll lm ks mc lo lp kw md lr ls lt ij bi translated">或者我是推特上的<a class="ae lu" href="https://twitter.com/RobinLinacre" rel="noopener ugc nofollow" target="_blank"> @robinlinacre </a>。</p></div></div>    
</body>
</html>