<html>
<head>
<title>Model deployment with Apache Beam and Dataflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Apache Beam和数据流进行模型部署</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-deployment-with-apache-beam-and-dataflow-be1175c96d1f?source=collection_archive---------13-----------------------#2020-02-29">https://towardsdatascience.com/model-deployment-with-apache-beam-and-dataflow-be1175c96d1f?source=collection_archive---------13-----------------------#2020-02-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1bac" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Apache Beam / Dataflow、BigQuery和Scikit-learn快速简单地操作分析模型的选项</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8d4d34e3c5a08570b9bd6a926d4aa6a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INjwNNZVdHU2iIn-kUpPKw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片来自Pixabay</p></figure><p id="bd0f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于一些数据科学家来说，操作数据科学模型有时可能会有压力。你的模型越复杂，生产时你面临的困难就越多。在开发客户流失分类器时，你是否后悔过将5个不同的模型集合在一起？别担心，阿帕奇光束来救援了。</p><p id="665a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在开始使用Apache Beam之前，让我们看看您有哪些选项来操作您的模型。第一个选项是在云上创建一个虚拟机(VM)来执行计算任务。很公平，但是设置和管理虚拟机将是一件令人头痛的事情，因为它需要大量的手动工作(您的云工程师不会喜欢它！).Cloud Dataproc有助于释放管理需求，并且是另一个值得考虑的好选择，因为它提供的计算资源只在一次运行期间有效。然而，您需要花一些时间将Python代码转换成PySpark或Scala，更不用说您可能无法完全复制您在Python中使用这些编程语言所做的事情。如果这些问题对你来说不是一个挑战，请查看我的同事发来的<a class="ae lu" href="https://medium.com/vinid/what-i-learned-about-deploying-machine-learning-application-c7bfd654f999" rel="noopener">这篇精彩的帖子</a>，但是如果是，数据流可能是一个不错的选择，因为:</p><ol class=""><li id="d6d9" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">您可以使用Apache Beam Python SDK编写Python3来创建运行在DataflowRunner后端的数据管道。那是什么意思？这意味着您可以编写Python3来构建您的管道，并直接操作您基于Python构建的ML模型，而无需将它们转换为Scala或PySpark。</li><li id="c186" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">Dataflow是一项完全托管的服务，通过自动扩展员工资源来最大限度地减少延迟、处理时间和成本。</li></ol><h1 id="7225" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">简介:什么是什么？</h1><p id="f066" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">你可能会对“Google Cloud Dataflow”和“Apache Beam”这两个术语感到困惑，就像我第一次知道它们一样，那么这些术语是什么意思呢？Apache Beam是一个开源的统一模型，它允许用户通过使用一个开源的Beam SDK(Python就是其中之一)来定义数据处理管道，从而构建一个程序。然后，管道由Beam管道运行器翻译，由分布式处理后端(如Google Cloud Dataflow)执行。越来越清晰了？好，我们开始吧！</p><h1 id="1c4d" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">用例</strong></h1><p id="eeb1" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">假设您是一家电信公司的数据科学家，您的工作是使用您的数据工程团队准备的客户数据来预测哪些客户将会流失。为了便于说明，我将在本教程中使用常见的电信客户流失数据集。这是一个非常小的公共数据集，形状为(7043，21)。你可以下载一下，在这里快速看一下专栏描述<a class="ae lu" href="https://www.kaggle.com/blastchar/telco-customer-churn" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="60f3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">理解数据后，让我们对其进行预处理，并使用Scikit-learn的RandomForestClassifier开发一个简单的模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预处理输入并开发分类器模型</p></figure><p id="c5b1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该模型在测试集上给出了81.17%的准确度和AUC = 0.7023，然而这不是理想的性能。让我们假设您的涉众对此有些满意，那么您将需要转储这个模型以将其部署到生产中。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="ac1b" class="nn mk it nj b gy no np l nq nr">from joblib import dump<br/>dump(clf, 'model_rf.joblib')</span></pre><h1 id="55b7" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">您的数据管道</h1><p id="cc87" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">图1展示了管道的组件。在高层次上，这是一个<em class="ns"> BigQuery-to-BigQuery流</em>，其中你的BigQuery (BQ)表形式的输入数据随后被云数据流处理，并被馈送到包含你的客户的<em class="ns"> ID和流失概率的输出BQ表</em>。在处理过程中，云数据流需要云存储来保存临时和暂存文件，以及加载您开发的随机森林模型。气流组件用于调度此作业，因为您的公司可能需要每周或每月运行客户流失预测。日程安排是一个完全不同的话题，不是这篇文章的重点(我可能会在以后写另一篇关于这个话题的文章)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/2dee5e5b90c4dd3b036d3df897891e4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*SZJ3KbTnSwXMVbDtfR2vaA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:通用数据管道</p></figure><h1 id="551b" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">循序渐进的指导</h1><h2 id="1c11" class="nn mk it bd ml nu nv dn mp nw nx dp mt lh ny nz mv ll oa ob mx lp oc od mz oe bi translated"><strong class="ak">设置GCP </strong></h2><p id="54ec" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">为了运行数据管道，您需要做一些初始设置。我假设你已经有了一个GCP账户，那么你需要遵循6个步骤来设置这里提到的数据流服务<a class="ae lu" href="https://cloud.google.com/dataflow/docs/quickstarts/quickstart-python" rel="noopener ugc nofollow" target="_blank"/>，否则你需要注册一个300美元的免费账户，<a class="ae lu" href="https://cloud.google.com/free/" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="12ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您还需要在云存储中创建一个bucket，其中包括您的模型(作为joblib或其他序列化形式)和两个文件夹:temp和staging。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/f28d438ca03b76717c004f86f4d7f39c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S6WCe2EAkcm3O3FZmNBULw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2: GCS存储桶结构</p></figure><p id="e83e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，您需要在BigQuery中创建一个数据集和一个表。为了便于说明，我将使用去掉标签的相同训练数据集(来自上面的Kaggle链接)，<em class="ns">，但在实践中</em>，您将对新的和未标记的数据运行预测。上传数据到BQ是非常简单的，你可以很容易地按照<a class="ae lu" href="https://cloud.google.com/bigquery/docs/loading-data" rel="noopener ugc nofollow" target="_blank">这个指令</a>去做。注意，在上传之前您需要清理一下您的数据，因为<code class="fe og oh oi nj b">TotalCharges</code>中有非数字值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">上传前的小数据清理</p></figure><h2 id="87e9" class="nn mk it bd ml nu nv dn mp nw nx dp mt lh ny nz mv ll oa ob mx lp oc od mz oe bi translated"><strong class="ak">对数据流进行编码</strong></h2><p id="e529" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">首先，每个波束管道中有4个关键术语:</p><ol class=""><li id="fc25" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated"><strong class="la iu">流水线:</strong>每个Beam程序的基础部分，一个<code class="fe og oh oi nj b">Pipeline</code>包含整个数据处理任务，从I/O到数据转换。下面的代码片段展示了一个4步管道图；三个关键元素<code class="fe og oh oi nj b">I/O transform</code>、<code class="fe og oh oi nj b">PCollection</code>、<code class="fe og oh oi nj b">PTransform</code>被包裹在<code class="fe og oh oi nj b">Pipeline</code>里面。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">波束管道示意图</p></figure><p id="d1a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.<strong class="la iu"> I/O转换:</strong>读取输入，写入输出。</p><p id="1753" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.<strong class="la iu">p集合:</strong>表示某种形式的数据，可能是分布式的。初始输入<code class="fe og oh oi nj b">PCollection</code>通常从某个数据集加载，通常是JSON格式(类似于带字典的),包括键和键值<code class="fe og oh oi nj b">{“key1”: “key_value1”, “key2”: “key_value2”,…, “keyn”: “key_valuen”}</code>。在上面的例子中，<code class="fe og oh oi nj b">p</code>是一个<code class="fe og oh oi nj b">PCollection</code>。</p><p id="69b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.<strong class="la iu"> PTransform: </strong> <code class="fe og oh oi nj b">PTransform</code>是通过管道运算符(<code class="fe og oh oi nj b">|</code>)应用于<code class="fe og oh oi nj b">PCollection</code>的每个元素的变换操作。束核变换有几种类型，包括:<code class="fe og oh oi nj b">ParDo</code>(并行do)<code class="fe og oh oi nj b">GroupByKey</code><code class="fe og oh oi nj b">CoGroupByKey</code><code class="fe og oh oi nj b">Combine</code><code class="fe og oh oi nj b">Flatten</code><code class="fe og oh oi nj b">Partition</code>。更多详情请参考官方<a class="ae lu" href="https://beam.apache.org/documentation/programming-guide/" rel="noopener ugc nofollow" target="_blank">梁文档</a>，本帖不一一介绍。</p><p id="9cf3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">回到我们的用例，类似于开发阶段，您需要实现两个转换步骤，包括输入格式化和预测。应用这些步骤需要由分布式处理功能以<code class="fe og oh oi nj b">DoFn</code>对象的形式定义的<code class="fe og oh oi nj b">ParDo</code>转换。下面的代码片段展示了与上面提到的两个步骤相对应的两个<code class="fe og oh oi nj b">DoFn</code>对象。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于ParDo变换的DoFn类</p></figure><p id="cd39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，输出数据通过WriteToBigQuery模块写入BigQuery。如果目标表不存在，可以通过设置<code class="fe og oh oi nj b">create_disposition</code> = 'CREATE_IF_NEEDED '来创建一个新表，如果目标表已经有一些数据，可以通过设置<code class="fe og oh oi nj b">write_disposition</code>参数来截断、追加或引发错误。</p><p id="dad5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有几个强制管道选项需要在代码中或运行时设置为参数(然后您需要在代码中用<code class="fe og oh oi nj b">ArgumentParser()</code>定义参数)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">管道选项</p></figure><h2 id="3d9f" class="nn mk it bd ml nu nv dn mp nw nx dp mt lh ny nz mv ll oa ob mx lp oc od mz oe bi translated">运行管道</h2><p id="1b77" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">有几种不同的方式来运行管道。您可以从Cloud Shell、本地终端或您的编程IDE运行它。这真的取决于你。在本教程中，我选择从本地终端运行作业。</p><p id="ec2d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，记得激活您用来编写管道代码的Python虚拟环境。我是通过Anaconda创建的，所以我需要运行:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="eb2a" class="nn mk it nj b gy no np l nq nr">conda activate /path/to/your/venv</span></pre><p id="acd0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您没有将环境变量<code class="fe og oh oi nj b">GOOGLE_APPLICATION_CREDENTIALS</code>设置为包含您的服务帐户密钥的JSON文件的路径。你需要现在就做。之后，您就可以进入包含您的作业的目录并运行它了。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="0d90" class="nn mk it nj b gy no np l nq nr">cd path/to/your/main.py<br/>python main.py</span></pre><p id="43b6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果作业正在运行，您可以转到Dataflow选项卡来查看您的管道，您将能够看到如图3所示的管道。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/8d0072a5bf1401ddd85c8727f3912eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*glz8DL4GLvdYeB5fbqSFfA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:云数据流管道</p></figure><p id="a3ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您在屏幕右侧查看您的工作信息，您将能够看到云数据流的自动缩放机制，它会根据数据量自动将员工数量从1增加到4。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/3d0c7b471c2fdcd1af9bfa04440848a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*LSbqccDtlTLRZQaFgcsnoQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:云数据流作业信息</p></figure><p id="3791" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这项工作通常在15分钟内完成(检查您的许可和启动worker也需要相当长的时间)。随后，您的输出就可以使用了。让我们尝试运行下面的查询，然后看看输出。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="720d" class="nn mk it nj b gy no np l nq nr">SELECT * FROM `your_project.your_dataset.your_tablename` LIMIT 1000</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/5b343290fcfcb5cef93ee4fe67a85efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mr2-JzUtVEYDzyM8-K6RPg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:对最终输出的查询</p></figure><h1 id="8123" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">关键要点</h1><p id="6eb3" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">部署您的数据科学模型有时很困难，但我希望本教程对那些选择云数据流进行操作的人有用。让我总结一下我在本教程中提到的一些要点。</p><ol class=""><li id="2c45" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">Apache Beam允许您在Python 3中开发数据管道，并作为后端运行程序在云数据流中执行它。</li><li id="15c4" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">云数据流是一种完全托管的服务，支持资源的自动扩展。</li><li id="bdc9" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">[最终输出p集合]=([初始输入p集合] | [第一p变换] | [第二p变换]</li><li id="f91b" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">您可以通过云外壳、本地终端或IDE(如PyCharm)运行数据流作业。</li></ol><p id="9b5d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本教程主要关注如何使用Apache Beam创建和运行数据管道。实际上，您可能还需要安排作业每天、每周或每月自动运行。此外，我还没有从您的服务帐户中提到Cloud IAM的作用。目前，我将它设置为项目的所有者，这已经足够了，我敢打赌你的Cloud DevOps团队在实践中很少让这种情况发生。我计划在接下来的文章中讨论这些话题。敬请期待！</p><p id="6c4a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读。快乐学习。</p><p id="71b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我的Github库:<a class="ae lu" href="https://github.com/nghiamab/dataflow-demo" rel="noopener ugc nofollow" target="_blank">https://github.com/nghiamab/dataflow-demo</a></p><h1 id="1fd5" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><p id="1b61" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">[1] Nike Nano，Apache Beam+Scikit learn(sk learn)(2019)，<a class="ae lu" href="https://medium.com/@niklas.sven.hansson/apache-beam-scikit-learn-19f8ad10d4d" rel="noopener">https://medium . com/@ niklas . sven . hansson/Apache-Beam-Scikit-learn-19 F8 ad 10d 4d</a></p><p id="0f6f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]官方光束编程指南:【https://beam.apache.org/documentation/programming-guide/ T4】</p></div></div>    
</body>
</html>