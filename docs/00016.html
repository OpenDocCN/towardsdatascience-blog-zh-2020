<html>
<head>
<title>Building My Neural Network From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始构建我的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-my-neural-network-from-scratch-cbb97321cb8f?source=collection_archive---------15-----------------------#2020-01-01">https://towardsdatascience.com/building-my-neural-network-from-scratch-cbb97321cb8f?source=collection_archive---------15-----------------------#2020-01-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4b59" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解 Python 中深度学习的具体细节</h2></div><h1 id="5148" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">动机</h1><p id="f2a0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">尽管已经有很多令人惊叹的深度学习框架可供使用，但我相信从头开始构建自己的神经网络有助于更好地理解神经网络的内部工作方式。</p><p id="5ea4" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">当我在 Coursera 上<a class="ae mb" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">斯坦福机器学习课程</a>时，有一个作业指导你完成神经网络反向传播的基本实现。所以我决定根据这个任务建立我的模型。但是有几件事我想修改/改进:</p><ul class=""><li id="f860" class="mc md it lc b ld lw lg lx lj me ln mf lr mg lv mh mi mj mk bi translated">基于用 Octave 编写的原始赋值派生一个 Python 实现。</li><li id="b5a4" class="mc md it lc b ld ml lg mm lj mn ln mo lr mp lv mh mi mj mk bi translated">不是建立一个简单的 3 层神经网络，而是让我们自己决定神经网络架构成为可能。</li><li id="0f0c" class="mc md it lc b ld ml lg mm lj mn ln mo lr mp lv mh mi mj mk bi translated">对算法实现矢量化。</li></ul><p id="76a5" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">事不宜迟，我们开始吧！</p><h1 id="3042" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">创建神经网络类</h1><p id="8eec" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">基本神经网络由以下部分组成:</p><ul class=""><li id="7c9f" class="mc md it lc b ld lw lg lx lj me ln mf lr mg lv mh mi mj mk bi translated">输入层</li><li id="d66e" class="mc md it lc b ld ml lg mm lj mn ln mo lr mp lv mh mi mj mk bi translated">输出层</li><li id="4be0" class="mc md it lc b ld ml lg mm lj mn ln mo lr mp lv mh mi mj mk bi translated">一个或多个隐藏层</li><li id="bc75" class="mc md it lc b ld ml lg mm lj mn ln mo lr mp lv mh mi mj mk bi translated">各层之间的权重</li><li id="d533" class="mc md it lc b ld ml lg mm lj mn ln mo lr mp lv mh mi mj mk bi translated">激活功能</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/97f13279ffda32dd2d7ab086c5e58fb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jO8vS1TJyVx-PRi9fRZYZg.png"/></div></div></figure><p id="9c66" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">请注意:</p><ul class=""><li id="7d0c" class="mc md it lc b ld lw lg lx lj me ln mf lr mg lv mh mi mj mk bi translated">输入层中的节点数与输入的要素数相同，输出的类数给出了输出层中的节点数。</li><li id="055f" class="mc md it lc b ld ml lg mm lj mn ln mo lr mp lv mh mi mj mk bi translated">除了输出层之外的每一层都增加了一个偏置单元。</li><li id="6e0d" class="mc md it lc b ld ml lg mm lj mn ln mo lr mp lv mh mi mj mk bi translated">有几种类型的激活功能。这里我为我的神经网络的每一层使用一个 sigmoid 函数。</li></ul><p id="d043" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">所以让我们用 python 创建一个神经网络类:</p><figure class="mr ms mt mu gt mv"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h1 id="815b" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">模型表示</h1><p id="b2d7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">给定 m 个训练例子<strong class="lc iu"> <em class="ne"> (X，Y)</em></strong><em class="ne"/>我们把它们写成矩阵:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nf"><img src="../Images/0031a70b679b87f6662e328303a447e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a3aQDj5lGj8fhui18avIxg.png"/></div></div></figure><p id="1ffb" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">其中，<strong class="lc iu"> <em class="ne"> X </em> </strong>的每一行是 n 个特征的一个训练示例，<strong class="lc iu"> <em class="ne"> Y </em> </strong>的每一行表示每个示例在<strong class="lc iu"> <em class="ne"> K </em> </strong>类中的哪一个。</p><p id="1654" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们将一个<strong class="lc iu"><em class="ne"/></strong>层神经网络的每一层<strong class="lc iu"> <em class="ne"> l </em> </strong>表示为如下:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nf"><img src="../Images/c08e7026bf31d030bdf85b9cc863a173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oGHlEhLkxQVF7Xk0da8DoQ.png"/></div></div></figure><p id="4e93" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">每个矩阵的第<strong class="lc iu"> <em class="ne"> j </em> </strong>行表示该层中第<strong class="lc iu"> <em class="ne"> j </em> </strong>个示例的值。这里<strong class="lc iu"> <em class="ne"> Sl </em> </strong> <em class="ne"> </em>表示第<strong class="lc iu"> <em class="ne"> l </em> </strong>层的节点数。</p><p id="9de3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">每层之间的权重也用矩阵表示:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nf"><img src="../Images/e1723730b3d163e985ab362a8cc02bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vHmZm7ZyKrS0hhFO6PKmBA.png"/></div></div></figure><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ng"><img src="../Images/689cac0743343764e97e6e205b2656e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfYpheJDFRYicIrDpsI9mA.png"/></div></div></figure><h1 id="fc1e" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">前馈计算</h1><p id="cea2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们使用前馈计算来计算给定输入和权重的预测输出。</p><p id="bc93" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">给定输入 X，我们首先计算第一个隐藏层的值。</p><p id="30b4" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">首先，向输入层添加一个偏置项。然后将输入层乘以权重矩阵得到</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nh"><img src="../Images/41033bb5c99aaafb375bdf02526d1f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sxKZCj3BazYYBIVqFqEI9g.png"/></div></div></figure><p id="9147" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">应用激活函数以获得第一层的矩阵 A:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nh"><img src="../Images/fb903c9a257da33005b74dc8b2a9c61d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vKZJPN4nHgB9JSN6Z5Br2Q.png"/></div></div></figure><p id="6b6f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在，我们只需重复此过程来计算每个图层的值，包括输出图层。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nh"><img src="../Images/4f59a484e90b48765e8535545c8c4d5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vC7K_lDt24upc6jS2HvvOg.png"/></div></div></figure><figure class="mr ms mt mu gt mv"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h1 id="6ba6" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">价值函数</h1><p id="d1c1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">正则化神经网络的成本函数由下式给出:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nf"><img src="../Images/d9cea26b72be2471a3ca460c38daa262.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q49WdJffWmGeTOco72rLyA.png"/></div></div></figure><p id="d77c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这里<strong class="lc iu"> <em class="ne"> a </em> </strong>是来自输出层的值，<strong class="lc iu"> λ </strong>表示正则化参数。</p><figure class="mr ms mt mu gt mv"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="d0e8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们的目标是找到一个好的设置权重参数来最小化代价函数。我们可以用几个优化算法比如<a class="ae mb" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>来最小化函数。为了应用该算法，我们必须计算成本函数的梯度。</p><h1 id="24c4" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">反向传播</h1><p id="d5f5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们实现反向传播算法来计算成本函数的梯度。</p><p id="a3d4" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我写了一篇关于<a class="ae mb" href="https://medium.com/@b06201018/how-backpropagation-works-d1bac48ca90" rel="noopener">反向传播的实现及其工作原理的文章</a>。如果你对反向传播的细节感兴趣，可以去看看。</p><p id="6f43" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这是代码:</p><figure class="mr ms mt mu gt mv"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h1 id="01c7" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">学习参数</h1><p id="f418" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在我们实现了成本函数和梯度计算之后，我们使用<a class="ae mb" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html" rel="noopener ugc nofollow" target="_blank"> SciPy 的最小化函数</a>来学习使成本函数最小化的良好参数集。</p><h1 id="52ee" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">训练神经网络</h1><p id="205b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们把所有东西放在一起训练我们的神经网络:</p><figure class="mr ms mt mu gt mv"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h1 id="ead0" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">预言；预测；预告</h1><p id="1dfc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，我们可以使用经过训练的神经网络来预测测试数据集的标签。为此，只需执行前馈计算来计算测试数据<strong class="lc iu"> <em class="ne"> X </em> </strong>的输出<strong class="lc iu"> <em class="ne"> Y </em> </strong>。</p><figure class="mr ms mt mu gt mv"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="f1dc" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="ne">这就是了！我们已经完成了整个神经网络的构建！</em> </strong></p><p id="c652" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">感谢你和我一起踏上这个从零开始构建神经网络的奇妙旅程。我确实从整个过程中获益良多，我希望你也有同样的感受！</p><p id="806b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">完整的代码可以在我的<a class="ae mb" href="https://github.com/1999ADEK/NN-from-scratch.git" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>中找到。欢迎给我留言！</p><h1 id="d500" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">参考</h1><ul class=""><li id="37d4" class="mc md it lc b ld le lg lh lj ni ln nj lr nk lv mh mi mj mk bi translated"><a class="ae mb" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">斯坦福机器学习课程</a></li><li id="7b30" class="mc md it lc b ld ml lg mm lj mn ln mo lr mp lv mh mi mj mk bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6">如何用 Python 从零开始构建自己的神经网络</a></li><li id="6777" class="mc md it lc b ld ml lg mm lj mn ln mo lr mp lv mh mi mj mk bi translated"><a class="ae mb" href="https://gist.github.com/andr0idsensei/92dd7e54a029242690555e84dca93efd" rel="noopener ugc nofollow" target="_blank">矢量化神经网络实现</a></li></ul></div></div>    
</body>
</html>