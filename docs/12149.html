<html>
<head>
<title>Dreaming over text!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">做梦超过文字！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dreaming-over-text-f6745c829cee?source=collection_archive---------40-----------------------#2020-08-21">https://towardsdatascience.com/dreaming-over-text-f6745c829cee?source=collection_archive---------40-----------------------#2020-08-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c537" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">把深梦的想法延伸到文字资料！</h2></div><blockquote class="ki kj kk"><p id="b504" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">“DeepDream 是一项将神经网络学习的模式可视化的实验。类似于当一个孩子观看云并试图解释随机形状时，DeepDream 过度解释并增强了它在图像中看到的模式。</p><p id="1555" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">它通过网络转发图像，然后计算图像相对于特定层激活的梯度。然后图像被修改以增加这些激活，增强网络看到的模式，并产生一个梦一样的图像。这个过程被称为“创意主义”(指的是<a class="ae li" href="https://arxiv.org/pdf/1409.4842.pdf" rel="noopener ugc nofollow" target="_blank">创意网</a>和<a class="ae li" href="https://en.wikipedia.org/wiki/Inception" rel="noopener ugc nofollow" target="_blank">电影</a>盗梦空间)。"</p><p id="9efa" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">【https://www.tensorflow.org/tutorials/generative/dee】T4 警梦</p></blockquote><p id="f2ae" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">让我给你分析一下。考虑一个卷积神经网络。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/5b20e1805a165b187aca3647238cd951.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1X8oCSkcGpBCxdkiQuvfEQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated"><a class="ae li" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" rel="noopener ugc nofollow" target="_blank"> LeNet-5 架构</a></p></figure><p id="8111" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">假设我们想要检查当我们增加突出显示的神经元激活<em class="kn"> h_{i，j} </em> <strong class="ko iu">，</strong>时会发生什么，并且我们想要在增加这些激活时将这些变化反映到输入图像上。</p><p id="42fc" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">换句话说，我们正在优化图像，以便神经元<em class="kn"> h_{i，j </em> }发出更多的信号。</p><p id="a8a1" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">我们可以把这个优化问题表述为:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/baa5091d953bdb8b78f85fcbc10d2d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*HCqUb1AU6mI8gyGoVizv8g.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者图片</p></figure><p id="7b3f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">也就是说，我们需要通过改变图像来最大化<em class="kn"> h_{i，j} </em>的平方范数(简单来说就是幅度)。</p><p id="c665" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">当我们按照上面所说的去做时，会发生这样的事情。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi md"><img src="../Images/e3fac51785683207e970a69514add380.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u-DGj2JQGchqpY6_psT-YQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">(<strong class="bd me">左</strong>图片由 Von.grzanka | ( <strong class="bd me">右</strong>)图片由<a class="ae li" href="https://www.tensorflow.org/tutorials/generative/deepdream" rel="noopener ugc nofollow" target="_blank"> Tensorflow 教程</a></p></figure><p id="f47f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">这里的原因是，当 CNN 被训练时，中间层的神经元学会了看到一些模式(这里是狗脸)。当我们增加这些激活时，输入图像开始包含越来越多的狗脸，以最大化激活。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="cbfc" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">在文本上实现深度梦背后的直觉。</h1><p id="7255" class="pw-post-body-paragraph kl km it ko b kp ne ju kr ks nf jx ku lj ng kx ky lk nh lb lc ll ni lf lg lh im bi translated">就像图像中的 deep dream，如果我们把任何一个隐藏层激活，并试图增加它的范数，文本输入会发生什么？<br/>为了回答这个问题，采用了文本分类模型，并设置了损失函数来增加隐藏层的激活程度。我们希望看到这个隐藏层学习到的模式/表现。</p><h2 id="6df6" class="nj mn it bd mo nk nl dn ms nm nn dp mw lj no np my lk nq nr na ll ns nt nc nu bi translated">嵌入层</h2><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/14d796956189f43fdc970820dc4448e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*XOiFFiafvMFijCYgucrbMw.gif"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者图像|计算句子嵌入的图像显示方法。</p></figure><p id="3984" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">神经网络模型不能理解单词。为了将单词作为输入，这些单词被转换成一个称为<strong class="ko iu">嵌入的 n 维数组。</strong>每个单词转换成特定的 n 维数组。为了得到句子嵌入，我们简单地取句子的平均值，然后这个数组被作为输入馈送给模型。这种方法解决了句子长度可变的问题，可以与香草<strong class="ko iu">人工</strong> <strong class="ko iu">神经网络</strong>协同工作。</p><h2 id="f66b" class="nj mn it bd mo nk nl dn ms nm nn dp mw lj no np my lk nq nr na ll ns nt nc nu bi translated">模型</h2><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nw"><img src="../Images/da06d0a1c07fb4e372b269fe87598f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_4IERshvMyvis4ihsmv2g.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者图片</p></figure><p id="be96" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">使用了一个经过训练的模型来对<strong class="ko iu"> IMDB 评论</strong>进行分类。模型验证准确率达到<strong class="ko iu"> 80.10% </strong>。</p><p id="a6f6" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">设置该实验是为了捕捉由<strong class="ko iu">全连接层 2 </strong>或<strong class="ko iu"> FC2 </strong>给出的表示，简而言之，具有<strong class="ko iu"> 512 </strong>尺寸。</p><p id="552a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">使用的成本函数是<strong class="ko iu"> fc2 输出</strong>的标准。</p><p id="eae6" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated"><strong class="ko iu">注意:</strong>因为“单词序列”是<em class="kn">长张量，</em>它们不能被反向传播优化。相反，句子的嵌入表示被优化。</p><h2 id="e5d7" class="nj mn it bd mo nk nl dn ms nm nn dp mw lj no np my lk nq nr na ll ns nt nc nu bi translated">程序概要</h2><p id="5d9b" class="pw-post-body-paragraph kl km it ko b kp ne ju kr ks nf jx ku lj ng kx ky lk nh lb lc ll ni lf lg lh im bi translated">步骤 1:将句子转换成张量。</p><p id="2842" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">第二步:获得<strong class="ko iu">句子嵌入</strong>。</p><p id="a59f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">第三步:通过<strong class="ko iu"> fc2 层</strong>，得到<strong class="ko iu"> fc2 输出</strong>。</p><p id="da86" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">第四步:优化<strong class="ko iu">语句嵌入</strong>到<strong class="ko iu">增加 fc2 层输出</strong>。</p><p id="e372" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">步骤 5:对于给定的迭代次数，用当前的句子嵌入重复步骤 2 到步骤 4。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nx"><img src="../Images/d00c2d63cbae9991a94471a103bb12f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_DBiH-C8Q9EH8dBanWSEIQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者图片</p></figure><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="4a90" class="mm mn it bd mo mp oa mr ms mt ob mv mw jz oc ka my kc od kd na kf oe kg nc nd bi translated">结果</h1><h2 id="9f23" class="nj mn it bd mo nk nl dn ms nm nn dp mw lj no np my lk nq nr na ll ns nt nc nu bi translated">实验 1</h2><p id="9d78" class="pw-post-body-paragraph kl km it ko b kp ne ju kr ks nf jx ku lj ng kx ky lk nh lb lc ll ni lf lg lh im bi translated"><strong class="ko iu">简单句</strong>用于获取我们保存的分类结果及其对应的句子嵌入。</p><p id="e5f7" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated"><strong class="ko iu">比如:</strong> <em class="kn">“我讨厌这样。”，“我爱这个节目。”</em>，我们用来分类。这些句子非常简单，分别传达了一种消极和积极的情绪。</p><p id="42b2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated"><strong class="ko iu">对这些嵌入进行了梦想或优化</strong>，并记录了迭代激活图。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/2f771116b7fd566a7515e0ce68351c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*kmK-XTxqJKXwWfGM_rBtxQ.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者图片</p></figure><p id="b222" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">这里可以观察到几件事情。</p><ol class=""><li id="0071" class="og oh it ko b kp kq ks kt lj oi lk oj ll ok lh ol om on oo bi translated">对于这两个句子，隐藏层表征的激活几乎是线性增加的</li><li id="8f6c" class="og oh it ko b kp op ks oq lj or lk os ll ot lh ol om on oo bi translated">这些句子的激活是不同的，这意味着模型可以很容易地区分这两个句子。</li></ol></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="bd62" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated"><strong class="ko iu">对于句子:<em class="kn">“我爱这个节目。”。</em> </strong>模型正确地将此预测为<strong class="ko iu"><em class="kn"/></strong><em class="kn">。</em></p><h2 id="7aee" class="nj mn it bd mo nk nl dn ms nm nn dp mw lj no np my lk nq nr na ll ns nt nc nu bi translated">相似词测验</h2><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ou"><img src="../Images/f29a618fba1504acffdd055092d67f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o6qiscRrM2HkKWU9ie3Swg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者图片|基于相似度的词云。大字体意味着更相似。</p></figure><p id="9df2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">最初，句子嵌入更类似于中性词，如“this，it，even same ”,但随着我们增加 fc2 激活的幅度，句子嵌入变得类似于积极的词，如“great，unique ”,这是有意义的，因为模型预测它是一个<strong class="ko iu">积极的</strong>句子。可视化迭代中的嵌入。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ov"><img src="../Images/296cd2ee806c5ae5e0889c3f31fe10ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*OjYVQvHuEO8ZCgDW3YP75g.gif"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图片作者|动画展示了嵌入是如何从负面词汇走向正面词汇的。</p></figure><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="ow nz l"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者视频|我们将肯定句的嵌入可视化。观察句子嵌入如何随步骤变化。</p></figure><p id="04ff" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated"><a class="ae li" href="https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/mythrex/02f9d3b8575910943b89d9964e7fde56/raw/85e1d59d26662d8166d1fd0edeb2fc7d409f674a/Deep%2520dream%2520text%2520embeddings" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu"> <em class="kn">试嵌入此处</em> </strong> </a> <strong class="ko iu"> <em class="kn">在 Tensorflow 投影仪上</em> </strong></p><p id="fdcb" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">观察句子<strong class="ko iu">嵌入如何从<strong class="ko iu"> step_1 </strong>开始</strong>并移动到<strong class="ko iu"> step_21。</strong>句子嵌入开始于<strong class="ko iu">肯定词</strong>和<strong class="ko iu">否定词</strong>之间，随着算法的梦想，嵌入向肯定词移动。</p><blockquote class="ki kj kk"><p id="1972" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">试试这些东西。</p><p id="f22a" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">观察 3d 中的嵌入。</p><p id="c541" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">找到类似<strong class="ko iu"> step_1 </strong>的词。</p><p id="ffe3" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">找到类似<strong class="ko iu"> step_21 </strong>的单词。</p></blockquote><p id="f38c" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated"><strong class="ko iu">对于句子:<em class="kn">“我讨厌这样”</em> </strong> <em class="kn">。</em>模型正确预测此为<strong class="ko iu"> <em class="kn">负</em> </strong> <em class="kn">。</em></p><h2 id="3479" class="nj mn it bd mo nk nl dn ms nm nn dp mw lj no np my lk nq nr na ll ns nt nc nu bi translated">相似词测验</h2><p id="8e1c" class="pw-post-body-paragraph kl km it ko b kp ne ju kr ks nf jx ku lj ng kx ky lk nh lb lc ll ni lf lg lh im bi translated">首先，我们观察做梦前后与句子嵌入相似(余弦相似)的词有哪些。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ox"><img src="../Images/071ea046d47b99e38c2b8bb74fcc4910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*szNXti7UGpkddL-eDtasDQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者图片|基于相似度的词云。大字体意味着更相似。</p></figure><p id="36a5" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">最初，句子嵌入更类似于中性词，如“<em class="kn"> this，it，even，same </em>”，但随着我们增加<strong class="ko iu"> fc2 激活</strong>的幅度，句子嵌入变得类似于单词“<em class="kn"> bad，nothing，bad</em>”，这传达了<strong class="ko iu">否定</strong>的意思，这是有意义的，因为模型预测它是<strong class="ko iu">否定</strong>句子。</p><h2 id="5bd0" class="nj mn it bd mo nk nl dn ms nm nn dp mw lj no np my lk nq nr na ll ns nt nc nu bi translated">可视化迭代中的嵌入。</h2><p id="a287" class="pw-post-body-paragraph kl km it ko b kp ne ju kr ks nf jx ku lj ng kx ky lk nh lb lc ll ni lf lg lh im bi translated">为了可视化迭代中的嵌入，使用<strong class="ko iu"> TSNE </strong>算法将嵌入维数从<strong class="ko iu"> 100 </strong>减少到<strong class="ko iu"> 2 </strong>。这些嵌入被绘制在一个 2d 地图上，用<strong class="ko iu">红点</strong>作为<strong class="ko iu">负面</strong>词(如一个坏的、更坏的、卑鄙的、错误的词语)，用<strong class="ko iu">绿点</strong>作为正面词(如伟大的、著名的、美妙的词语)。</p><p id="a4b1" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated"><strong class="ko iu">灰点</strong>是句子嵌入的中间位置，<strong class="ko iu">黑点</strong>是句子嵌入的最终位置。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oy"><img src="../Images/86236856fb28fca6929feca2c095192f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxqB31P4HMsjC4hgSAlYbQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者图片| 2d 空间上的 100 维嵌入</p></figure><blockquote class="ki kj kk"><p id="fbd0" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><a class="ae li" href="https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/mythrex/f91d5aafd9dd5147784704a8ff295655/raw/c8389eee91ae2e3a05ac113296b58ddb699b85a5/Deep%2520dream%2520text%2520embeddings%2520negative" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu"> <em class="it">试嵌入此处</em> </strong> </a> <strong class="ko iu"> <em class="it">在 Tensorflow 投影仪上</em> </strong></p></blockquote><p id="1a89" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">该图清楚地显示了嵌入远离正面单词并接近负面单词，这与模型预测一致。而且最后的句子嵌入现在更像<strong class="ko iu">红点(否定词)而不是绿点(肯定词)。</strong></p><p id="24b9" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">这里的关键观察是，最初，句子嵌入在正面和负面单词之间，但是随着做梦的进行，嵌入被推离负面单词。</p><h2 id="791c" class="nj mn it bd mo nk nl dn ms nm nn dp mw lj no np my lk nq nr na ll ns nt nc nu bi translated">结论</h2><p id="a8fa" class="pw-post-body-paragraph kl km it ko b kp ne ju kr ks nf jx ku lj ng kx ky lk nh lb lc ll ni lf lg lh im bi translated">做梦后的单词嵌入变得与“模型预测”中的单词相似，尽管如果我们观察初始嵌入的相似单词，它们对于两个句子或多或少是相同的，即使它们表达非常不同的意思，最终的句子嵌入显示出一些有趣的模式。</p><p id="ff7e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">例如</p><p id="8e18" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">1.<strong class="ko iu">否定</strong>预测被推近到像<strong class="ko iu"> <em class="kn">错误、肮脏、恶劣</em> </strong>这样的词语</p><p id="5112" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">2.<strong class="ko iu">正面的</strong>预言被推近到像<strong class="ko iu">的词语<em class="kn">独特的、伟大的、享有盛名的</em> </strong></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h2 id="c05d" class="nj mn it bd mo nk nl dn ms nm nn dp mw lj no np my lk nq nr na ll ns nt nc nu bi translated">实验二</h2><p id="c54d" class="pw-post-body-paragraph kl km it ko b kp ne ju kr ks nf jx ku lj ng kx ky lk nh lb lc ll ni lf lg lh im bi translated">我们现在将使用难句。前半句表达一种情感，后半句改变情感的句子。</p><p id="ec98" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">像这样的句子</p><ul class=""><li id="ba2d" class="og oh it ko b kp kq ks kt lj oi lk oj ll ok lh oz om on oo bi translated">这部剧很长很无聊，但是导演很棒。</li><li id="08b7" class="og oh it ko b kp op ks oq lj or lk os ll ot lh oz om on oo bi translated">我讨厌这部剧，因为里面有裸体，但表演很经典。</li></ul><p id="c6f7" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">人类很难判断这些句子表达了什么样的情感。</p><p id="acd2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">我们将再次优化句子嵌入，以最大化<strong class="ko iu"> <em class="kn"> fc2 层的激活。</em> </strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/3fdfa48fc1805222462f70f0d20c68a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*QlYubRZQLH8G-ucA3rFVBg.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者图片|作为迭代函数的 fc2 层激活标准</p></figure><p id="9d34" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">不像第一种情况。两个句子的激活差异不大，即这些句子的激活或多或少相似，这意味着模型对这些句子没有分类能力。<br/>我们来看看这些句子做梦前后的近义词。</p><p id="947d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">“这部戏很长很无聊，但是导演真的很棒。”，模型预测<strong class="ko iu">为正</strong></p><p id="b117" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated"><strong class="ko iu">相似词测试</strong></p><p id="e377" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">我们会发现类似于句首 vs 句尾嵌入的词。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pb"><img src="../Images/ebd61a992398f2f577b54b2a86e7160b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VpF1vXcKFlBAG4pTI7UrdQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者图片</p></figure><p id="8ee9" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">嗯，即使这个句子被归类为积极的，类似于最后一句嵌入的词并没有反映任何积极的情绪。</p><p id="b949" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated"> <em class="kn">“我讨厌这部剧，因为它的裸露，但它的表演真的很棒。”，</em>模型预测<strong class="ko iu">为负</strong></p><p id="7ca3" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated"><strong class="ko iu">相似词测试</strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pc"><img src="../Images/777d8a8b9ee54e6b839af9837d9bd5e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ah91NAcjeEZDlAY74Ow-Kw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">作者图片</p></figure><p id="dff2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">这个句子被归类为<strong class="ko iu">否定句，</strong>梦后的嵌入反映了消极情绪。</p><h2 id="f05d" class="nj mn it bd mo nk nl dn ms nm nn dp mw lj no np my lk nq nr na ll ns nt nc nu bi translated">结论</h2><p id="4965" class="pw-post-body-paragraph kl km it ko b kp ne ju kr ks nf jx ku lj ng kx ky lk nh lb lc ll ni lf lg lh im bi translated">由于模型对这些句子没有清晰的理解，所以做梦后这两个句子的句子嵌入几乎是相似的<em class="kn">(做梦后看相似的单词)。这是因为模型在其隐藏层中没有这些句子的丰富表示。</em></p><p id="6513" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">我们从研究图像上的深度梦如何工作开始，然后我们提出了如何在文本上实现深度梦。最后，我们展示了如何正确解释结果。这种方法可以用来理解语言模型已经学习了什么样的隐藏表示。</p><p id="3767" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">像这样的实验帮助我们更好地理解这些黑盒。</p><p id="3f99" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">您可以在本笔记本中尝试一个<a class="ae li" href="https://colab.research.google.com/github/mythrex/deep_dream_textual_data/blob/master/demo.ipynb#scrollTo=DiFimXLc-FK_" rel="noopener ugc nofollow" target="_blank">演示。</a></p><p id="9f83" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lj kw kx ky lk la lb lc ll le lf lg lh im bi translated">所有相关代码可在<a class="ae li" href="https://github.com/mythrex/deep_dream_textual_data" rel="noopener ugc nofollow" target="_blank"> my Github Repo </a>获得。</p><h1 id="90d0" class="mm mn it bd mo mp oa mr ms mt ob mv mw jz oc ka my kc od kd na kf oe kg nc nd bi translated">参考</h1><ol class=""><li id="d042" class="og oh it ko b kp ne ks nf lj pd lk pe ll pf lh ol om on oo bi translated"><a class="ae li" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" rel="noopener ugc nofollow" target="_blank">基于梯度的学习应用于文档<br/>识别</a></li><li id="279b" class="og oh it ko b kp op ks oq lj or lk os ll ot lh ol om on oo bi translated"><a class="ae li" href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2015/06/in ception ism-going-deep-into-neural . html</a></li><li id="bac1" class="og oh it ko b kp op ks oq lj or lk os ll ot lh ol om on oo bi translated">【https://www.tensorflow.org/tutorials/generative/deepdream】</li><li id="4011" class="og oh it ko b kp op ks oq lj or lk os ll ot lh ol om on oo bi translated"><a class="ae li" href="https://youtu.be/YHAIrwRVvWg?list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT" rel="noopener ugc nofollow" target="_blank">https://youtu.be/YHAIrwRVvWg?list = plyqspqzte 6m 9 gcgajvqbc 68 hk _ JKGBAYT</a></li><li id="004f" class="og oh it ko b kp op ks oq lj or lk os ll ot lh ol om on oo bi translated"><a class="ae li" href="https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/初学者/text _ 情操 _ ngrams _ 教程. html </a></li></ol></div></div>    
</body>
</html>