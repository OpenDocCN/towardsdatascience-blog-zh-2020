# 用 PyTorch 编写分布式数据并行应用程序

> 原文：<https://towardsdatascience.com/writing-distributed-applications-with-pytorch-f5de4567ed3b?source=collection_archive---------19----------------------->

## 多 GPU 节点的分布式数据并行训练

![](img/e83eee2b3a044f4d75cfa72395ac73f6.png)

照片由 Pexels 的 Manuel 拍摄

**概要**

本文的主要目的是用简单的步骤阐明如何利用 PyTorch 实现的分布式计算技术在多个 GPU 上训练深度学习模型。文章假设你熟悉训练深度学习网络。本教程首先介绍一些关于分布式计算的关键概念，然后使用 PyTorch 的分布式数据并行功能编写一个 python 脚本来训练一个具有 4 个 GPU 的模型

> 注意:此处的教程将仅涵盖与分布式培训相关的关键步骤。完整的代码可以在[这里](https://github.com/shreeraman96/distributed_training)找到

**分布式计算——什么是分布式计算？**

分布式系统是通过网络进行通信以实现共同目标的不同独立节点的集合。每台机器被称为一个节点，通过单个网络连接的一群节点形成一个集群。节点利用网络与其对等节点通信。

分布式计算与这样一种编写程序的风格相关联，这种编写程序的风格充分利用了分布在整个体系结构中的计算机的计算能力

**深度学习的分布式计算**

深度学习和分布式系统是最近越来越受欢迎的两个不断发展的系统。计算能力最近几乎呈指数级增长，通过深度神经网络(DNN)，研究爱好者正在世界各地创造奇迹。因此，从两者中获益是非常重要的。如果我们利用大规模分布式系统的计算潜力，DNNs 的训练时间可以大大减少，性能也会提高

**优化计算的要点:**

> 1.通过系统和一致的处理跨多个节点分布计算
> 
> 2.在对等体之间建立同步

这里的重点是用简单的步骤解释如何利用 PyTorch 分布式模块通过数据并行技术为 BERT 模型进行屏蔽语言建模训练。训练其他模型也可以遵循相同的步骤

**数据并行技术**

这里，数据分布在多个节点上，以实现更快的训练时间。每个节点都应该有自己专用的模型副本、优化器和其他要素。我们将使用 PyTorch 模块的分布式 API 来实现这一点。

**torch . distributed-基本概述**

PyTorch 通过在任何 PyTorch 模型周围提供一个包装类来支持同步分布式训练。

每个流程都有自己的 python 解释器、优化器和模型副本，并在每一步执行完整的优化，从而减少开销时间

PyTorch 还支持不同的通信后端来收集跨多个节点的迭代的每一步的向前和向后传递的结果。

现在让我们深入实际的实现

**步骤 1:** 获取所有可用 GPU 的列表，包括 device _ ids 和可用 GPU 的总数

**步骤 2:** 从任何来源加载用于掩蔽语言建模的句子，并清理和处理数据。

**第三步:**加载一个预先训练好的 BertTokenizer，用它编码所有的句子，创建目标句子。屏蔽输入句子的某些单词以创建屏蔽句子

**步骤 4:** 设置分布式训练所需的不同配置参数，并将其加载到单个实体中，最好是一个类

> **世界规模**:相当于可用 GPU 数量的进程总数
> 
> **主地址**:将托管等级为 0 的进程的机器的 IP 地址
> 
> **主端口**:节点间通信的空闲可用端口

**步骤 5:** 定义 train()函数:

这是将分布在多个节点上的功能。因此，确保每个节点都有自己的所需变量和设置的专用副本是非常重要的

将在单个实体上收集多个不同的参数和变量，并作为值参数传递给训练函数。

除此之外，训练函数还将接收参数 ***rank*** ，处理器通过该参数来决定它是工作者节点还是主节点。

等级为 0 的节点将是主节点。PyTorch 负责为每个 GPU 分配等级，所以我们不必为此担心

**步骤 5.1** :用合适的后端系统和模型建立流程组

***Init_process*** 允许进程通过共享它们的位置来相互通信和协调。PyTorch 给出了两种指定进程组配置和启动进程组的方法

> 1.指定世界大小、等级和商店(可选)
> 
> 2.用 URL 中编码的等级和世界大小或其他方式指定 URL 字符串，以指示在哪里/如何与对等方通信。默认情况下，它将是**“env://”**

init_process 接受 4 个参数:

> 1.**后端**:要使用的通信后端。可用选项:格洛，NCCL，MPI。NCCL 适合 GPU 训练，而 Gloo 更适合 CPU 训练
> 
> 2. **Init_method:** 字符串 URL(默认值:env://)
> 
> 3. **World_size** :作业中的进程数。通常相当于可用的 GPU 数量
> 
> 4.**等级**:当前流程的等级

**步骤 5.3** :加载模型

加载所需的模型。在我们的例子中，加载用于屏蔽语言建模的 BERT 模型(BertForMaskedLM)

**步骤 5.4:** set-GPU device &加载模型到设备中

**步骤 5.6** :用分布式数据并行类包装模型，将模型分布到各个节点

这个容器为我们的 PyTorch 模型提供了一个包装器，并将给定模块的应用并行化，并将输入拆分到指定的设备上。该模块在集群上的每台机器和每台设备上复制，每个副本处理一小部分输入。

> 1.**模块**:要并行化的模块。在我们的案例中，我们案例中使用的模型
> 
> 2.**设备标识** : CUDA 设备列表

**步骤 5.7** :为每个副本设置数据加载器

我们设置 *training_sampler* u *使用 DistributedDataSampler()* 包装类来采样和分发每个副本的输入数据。

参数:

> 1.**数据集**:输入数据集
> 
> 2.副本数量:在我们的例子中等于世界大小(4)

下一步是用我们定义的分布式采样器来设置 Dataloader。

**步骤 5.8** 定义培训流程的其余部分&保存模型

**第六步:**根据可用的 GPU 数量启动产卵过程。PyTorch 现在将在指定的世界大小和设备列表中生成进程

希望本文为您提供了一个关于如何利用 PyTorch 的分布式 API 在多个 GPU 上实现分布式训练的合理思路。完整的代码可以在这里的资源库中找到。如果您希望深入了解这些模块，我建议您访问以下链接。

 [## 分布式通信包- torch.distributed - PyTorch 主文档

### torch.distributed 支持三个后端，每个后端都有不同的功能。下表显示了哪些功能是…

pytorch.org](https://pytorch.org/docs/stable/distributed.html#basics) [](https://pytorch.org/tutorials/intermediate/dist_tuto.html) [## 使用 PyTorch 编写分布式应用程序- PyTorch 教程 1.5.1 文档

### 作者:Séb Arnold 在这个简短的教程中，我们将介绍 PyTorch 的分发包。我们将看到如何…

pytorch.org-](https://pytorch.org/tutorials/intermediate/dist_tuto.html)