<html>
<head>
<title>Dimensionality reduction with PCA: from basic ideas to full derivation.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA 降维:从基本思想到完全推导。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7?source=collection_archive---------11-----------------------#2020-04-13">https://towardsdatascience.com/dimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7?source=collection_archive---------11-----------------------#2020-04-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9090" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本文中，我们将在一个简单的例子上建立一些关于维数约简和 PCA 的直觉，然后整理其背后的数学，并推导出一般情况下的算法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/652367d2b81746de61a759a29313c54c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*azGFBZOTxUzcVtr8DsGfqQ.png"/></div></div></figure><p id="0677" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">主成分分析(PCA)是一种强大的算法，其思想是由卡尔·皮尔逊在 1901 年<a class="ae lq" href="https://zenodo.org/record/1430636#.XpGEef2A5hF" rel="noopener ugc nofollow" target="_blank">【1】</a>为一个数据拟合问题提出的。与最小二乘回归不同，它不依赖于哪个变量是独立的，反映了变量之间的普遍关系。主成分分析有许多应用，如噪声过滤、特征提取或高维数据可视化，但最基本的应用是数据降维。在下面的帖子中，我将从这个角度描述 PCA。</p><h1 id="af7a" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">内容</h1><p id="c3d6" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">在本文中，我们将:</p><ul class=""><li id="0fd7" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated">深入了解降维。</li><li id="bbbd" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">在<strong class="kw iu"> <em class="nc"> 2d </em> </strong>和<strong class="kw iu"> <em class="nc"> 3d </em> </strong>中几乎不用数学就能建立一些关于 PCA 的直觉。</li><li id="6ed6" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">推导一般情况下的主成分分解。</li><li id="478d" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">总结一下 PCA 降维算法。</li><li id="64af" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">看看它如何处理图像压缩。</li></ul><h1 id="dd8d" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated"><strong class="ak">降维</strong>的基本思想<strong class="ak">降维</strong></h1><p id="3c7c" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">一般意义上，降维是对原始<strong class="kw iu"><em class="nc">M</em></strong>-维度数据<strong class="kw iu"><em class="nc">N</em></strong>-维度子空间的一种表示，其中<strong class="kw iu"> <em class="nc"> N &lt; M </em> </strong>。为了建立一些直觉，让我们从一个简单的例子开始。假设我们的数据是从汽车中的 GPS 传感器获取的坐标。假设它是真实世界的数据，所以它包含一些噪声。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/59634f746e24080e147da4d89dc739c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*8rHY28MG5UPDzJ18rYoFvw.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">图一。汽车坐标的二维数据，生成为 y=2*x +噪声。</p></figure><p id="8a45" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，我们手头有二维数据，在这种情况下降低维度意味着找到数据的一维表示。这可以通过将每个点投影到一维子空间上来实现。不一定是正交投影。例如，让我们取由基向量<strong class="kw iu"><em class="nc">【b】</em></strong><em class="nc"/>跨越的子空间<strong class="kw iu"><em class="nc"/></strong><em class="nc"/>并使用某种投影矩阵<strong class="kw iu"> <em class="nc"> P </em> </strong>来投影我们的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/055752091dc175cc462f4ca4ba72e678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FE8E-D7th1Vh4XSPLwGCYw.png"/></div></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">图 2 我们以某种方式将数据投射到低维空间。低维坐标称为代码。然后，我们可以通过子空间基向量和编码将数据重构回原始维度。</p></figure><p id="d6c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，在子空间<strong class="kw iu"><em class="nc"/></strong><em class="nc"/>中，我们可以为每个数据点仅存储一个值——子空间<strong class="kw iu"> <em class="nc"> U </em> </strong>中对应的坐标λ。这些低维坐标称为<strong class="kw iu"> <em class="nc">代码</em> </strong>。使用这些坐标λ和一个基向量<strong class="kw iu"> b </strong>我们可以将数据重建回原始维度，结果称为<strong class="kw iu"> <em class="nc">重建</em> </strong>。</p><p id="3996" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可以在这里提到，重建的数据不同于原始数据，我们可以用原始点和重建点之间的欧几里德距离来衡量这种差异。我们可以看到降维是有一定代价的，我们要付出的代价是信息损失。但它仍然有一些意义:即使使用这样的随机投影，重建的数据也反映了原始数据的一些属性:我们仍然可以看到<strong class="kw iu"> <em class="nc"> X </em> </strong>和<strong class="kw iu"> <em class="nc"> Y </em> </strong>正相关，甚至重建数据的斜率也非常接近原始数据。</p><p id="0363" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们使用了一些随机的无动机的投射，所以问题是“我们能做得更好吗”？我们当然可以，这就引出了 PCA 算法。但首先，在一般情况下推导它之前，让我们对它建立一个更好的理解。</p><h1 id="38cb" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">主成分直觉</h1><p id="5a73" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">在我们开始之前，让我们进行一个小的数据准备，让我们把我们的数据转移到中心(从每个维度减去平均值)。正如我们所见，这种转变似乎很有帮助。它不改变变量之间的关系，很容易回到原始数据，我们只需要添加相应变量的平均值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/cae69e867711d4448c8fba2fff6a712f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pP21g2nka2tVd_-53Ii05Q.png"/></div></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">图 3。数据居中:X_c= X -E[X]，Y_c = Y -E[Y]。对于逆变换，我们保持 E[X]和 E[Y]。</p></figure><p id="f72d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以现在我们要进行降维，保存尽可能多的信息。我们可以用重建误差来衡量信息损失，重建误差定义为原始数据和重建点之间的平均平方距离:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5de1aa7d1984d0cbc8f5ab08603b1644.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*MU-rj3c9KAmOXiXp6A3ATQ.png"/></div></figure><p id="309f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里我们需要首先提到的是，我们应该使用正交投影，以便最小化原始点和投影点之间的距离。考虑图 4 的左图:正交投影(灰色)的距离总是小于非正交投影(橙色)的距离。这可以用三角形不等式来证明，稍后我们将从另一个角度再次看到它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/f3916d49d4522d8b3247ce1a3442dc73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H5y3XTwtdbtv9V1uUOTFdQ.png"/></div></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">图 4。正交投影可以最小化距离。现在我们需要找到最佳拟合向量 b。</p></figure><p id="9981" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以我们已经介绍了投影类型，现在我们需要找到这样一个向量<strong class="kw iu"><em class="nc">【b】</em></strong><em class="nc"/>，它最小化<strong class="kw iu"> <em class="nc"> L </em> </strong>。从图 4 中可以明显看出<strong class="kw iu"> <em class="nc"> b2 </em> </strong> <em class="nc"> </em>比<strong class="kw iu"> <em class="nc"> b1 </em> </strong>好，但是如何在所有的中找到最好的一个呢？</p><p id="344f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们稍微作弊一下，跳过大部分数学运算，通过旋转基向量<strong class="kw iu"><em class="nc"/></strong>并为每个向量<strong class="kw iu"><em class="nc">【b】</em></strong>计算相应的重建误差，在数值上优化<strong class="kw iu"> <em class="nc"> L </em> </strong> <em class="nc"> </em>。我还在右图中添加了投影点的方差，因此您可以注意到我们稍后将讨论的一个关键特性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/a4b3f914f90ceb47cad0e872ffca8a34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ep6hbK8rpOsMCtAbIHLCJQ.gif"/></div></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">图 5 .我们旋转基向量 b 并将数据投影到相应的子空间上，然后计算重建误差 L 和投影方差 Var(λ)。</p></figure><p id="70d5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">可以看出，通过这种方式，我们可以找到一个使重建误差最小化，同时使投影方差最大化的向量。这是一个重要的特性，所以让我们稍微关注一下:</p><ul class=""><li id="0aec" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated">跟随左边的动画:当我们向最优解移动时，投影(灰色)点试图尽可能远离原点。</li><li id="e85b" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">这是因为正交距离的最小化与投影和原点之间距离的最大化是相同的。从任何蓝点到原点的距离保持固定，我们最小化正交距离(虚线),因为根据勾股定理，这与最大化从原点到投影点的距离是相同的。</li><li id="f659" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">投影的期望值为零(因为我们在开始时将数据居中)，从原点到投影的距离最大化也会导致方差最大化。这是一个至关重要的特征，我们将在后面的一般推导中面对它。</li></ul><p id="1fbe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">还有一件事要讨论。让我们考虑子空间(<strong class="kw iu"><em class="nc">)U2</em></strong>)正交，因为我们刚刚在下图上找到(<strong class="kw iu"><em class="nc"/></strong>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/7b60d6fa5ab449e5f3d2c2f78c72275a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kOU7RRG8RycDr43O-Hla1A.png"/></div></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">图 6。我们找到的基本向量是我们数据的主要组成部分。主成分描述了大多数数据差异。</p></figure><p id="ee2d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些子空间上的投影是我们数据的主要组成部分。考虑图 6 中的投影方差:相应的子空间具有一个重要的性质，即第一投影包含大部分方差，而第二投影包含最小方差。它们一起描述了所有的数据差异。将总数据方差视为它们的总和，我们可以计算出子空间<strong class="kw iu"><em class="nc"/></strong>描述了 97.1%的数据方差，子空间<strong class="kw iu"> <em class="nc"> U2 </em> </strong>描述了 2.9%。因此，如果我们降低维数，只保留在<strong class="kw iu"> <em class="nc"> U1 </em> </strong>上的投影，信息损失将仅为 2.9%。</p><p id="7f48" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们从一个稍微不同的角度来看看我们到目前为止所做的事情:我们用一种新的基来执行数据的正交分解，其方式是基向量之一跨越一个具有最大方差的子空间。根据<a class="ae lq" href="https://mathworld.wolfram.com/OrthogonalDecomposition.html" rel="noopener ugc nofollow" target="_blank">正交分解定理</a>我们总是可以通过某个子空间及其正交补来分解任何向量，并且在 PCA 中，我们正在搜索包含大部分数据方差的子空间。</p><p id="8c3e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">比如在<strong class="kw iu"> <em class="nc"> 3d </em> </strong>中我们可以找到<strong class="kw iu"> <em class="nc"> 2d </em> </strong>(一个平面)<strong class="kw iu"> <em class="nc"> </em> </strong>或者<strong class="kw iu"> <em class="nc"> 1d </em> </strong>(一条线)数据方差最大的子空间。考虑跨越具有最大方差的平面的两个基向量和与其正交的第三个基向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/31f835a0e963c2f3c092e052b795adb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*au46kxikPzh9sAVGRoxIPw.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">图 7。红线代表数据主成分。大多数数据方差可以用基于前两个主成分的平面来描述。</p></figure><p id="594f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于更高维度，很难绘制或想象所有这些，但想法是相同的:找到一个子空间(线、平面、超平面),其中包含大部分数据方差及其正交补。跨越这些子空间的向量是主分量。</p><p id="75ec" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦我们得到这个，我们就可以进入一个普通的案例。</p><h1 id="d743" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">一般情况 PCA 推导</h1><p id="a0d5" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">假设我们有数据<strong class="kw iu"><em class="nc">×数据</em> </strong>由 N 个<strong class="kw iu"><em class="nc"/></strong>数据点组成，每个点都是一个<strong class="kw iu"> <em class="nc"> D </em> </strong>维向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/714e66c2dccd5d0c264aeafd5ec1f82c.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*XeevDpJcy0qfQWoLEURu4g.png"/></div></figure><p id="4ac8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">就像前面提到的例子一样，我们希望在另一个标准正交基中分解每个数据点，以使第一组分量包含大部分数据方差。我们可以这样写:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/950eefedac24df026dc6ebcd300a3e1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*sL_4AD_VTfwcV0jcMA0Xgg.png"/></div></figure><p id="0b7b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里<strong class="kw iu"> <em class="nc"> b </em> </strong>是我们新的基向量，β是对应的坐标。我们希望找到这样的<strong class="kw iu"> <em class="nc"> b </em> </strong>和β，使得第一个<strong class="kw iu"> <em class="nc"> M </em> </strong>基向量所跨越的子空间上的投影包含大部分数据方差。让我们回顾一下例子:</p><ul class=""><li id="d106" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated">在<strong class="kw iu"> <em class="nc"> 2d M=1 </em> </strong>中，我们找到了一个方差最大的子空间(一条直线)，一个正交子空间包含最小方差。</li><li id="71eb" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">在图 7 上的<strong class="kw iu"><em class="nc">3d</em></strong><strong class="kw iu"><em class="nc">M = 2</em></strong>中，我们找到了一个方差最大的子空间(一个平面)，它的正交补又包含最小的数据方差。</li></ul><p id="ef35" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了降维，我们去掉了具有最小方差的第二项，并用第一个<strong class="kw iu"> <em class="nc"> M </em> </strong>基向量来表示我们的数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/f15265ab6cc7e46a1458f1e7c41b2272.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*jXClTIH6E_kxIINVUkGN9g.png"/></div></figure><p id="de9e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="nc">注</em></strong><em class="nc">:</em><strong class="kw iu"><em class="nc">【x *】</em></strong><em class="nc">仍然是一个</em> <strong class="kw iu"> <em class="nc"> D </em> </strong> <em class="nc">维度向量，但现在它生活在一个更小的子空间中，可以用</em> <strong class="kw iu"> <em class="nc"> M </em> </strong> <em class="nc">坐标描述在新的</em><strong class="kw iu"/><em class="nc">基础上</em></p><p id="1fa9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们的目标保持不变:最小化我们定义为原始点和重建点之间的均方距离的重建误差:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5de1aa7d1984d0cbc8f5ab08603b1644.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*MU-rj3c9KAmOXiXp6A3ATQ.png"/></div></figure><p id="6f8e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是现在我们要推导出解析解。为此，让我们重写损失函数。首先，让我们用数据在相应基向量上的点积来表示投影坐标:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/94b71215b708e8dc50196ad1cc29cc62.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*NW0Ja5t0XeNWMATYVU78IQ.png"/></div></figure><p id="f695" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="nc">注:</em> </strong> <em class="nc">一个人可以证明正交投影是最优解，考虑 L 关于β的偏导数并设为零，得到同样的结果。</em></p><p id="f368" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用β的表示，我们可以将<strong class="kw iu"> <em class="nc"> x* </em> </strong>重写如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f2ed5216209f501ca81436badca08ee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*rJi1KvRoJQ_kIDnamz1gVA.png"/></div></figure><p id="add2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以基于同样的推理重写<strong class="kw iu"> <em class="nc"> x </em> </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f8373ecbbb4d1d7ab5652c66b8f28ea3.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*xT4gDkXwichJ0CdtxhiKWw.png"/></div></figure><p id="1d08" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以数据和重建的区别在于:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/98484df213ffcd7275d5cd99281046f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*KmfGPv_BZn-auT6N75OybA.png"/></div></figure><p id="2c9e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">损失可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d72e7758c072a6463b893deaab7b00d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*A1nIA1guDffLqY8X3EOOUg.png"/></div></figure><p id="5c61" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">已知<strong class="kw iu"> <em class="nc"> b </em> </strong>是正交基的组成部分，得到如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/5d7f46c6176adca04086854dde6e48a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*uS4_wXmuipHgDLG50GMqog.png"/></div></figure><p id="db56" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后我们可以重新排列总数，得到这个:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/ae8c5cfa0efcd048359602074cb349e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*-JaOvp6s480RkurSz6fdog.png"/></div></figure><p id="0577" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里有一件好事:假设我们的数据居中，括号中的一项是协方差矩阵<strong class="kw iu"> <em class="nc"> S=Cov(X) </em> </strong>，所以我们可以这样写:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/348c9d3a8aa48b3f215cbb51956b5921.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*qkui3ONPlaZYduNO7ymBUA.png"/></div></figure><p id="d018" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将最小化在<strong class="kw iu"> <em class="nc"> {M+1，D} </em> </strong>基础上预测的方差(这与在<strong class="kw iu"> <em class="nc"> {1，M} </em> </strong>基础上最大化的方差相同，在图 6 中概括结果)。</p><p id="a03a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">还有最后一步要走:我们要最小化<strong class="kw iu"> <em class="nc"> L </em> </strong>关于<strong class="kw iu"><em class="nc"/></strong>b<strong class="kw iu"><em class="nc">b</em></strong>假设<strong class="kw iu">b<em class="nc">b</em></strong>是一个标准正交基，它应该满足下面的等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/34f5b42bd7b44930d3562334638a64f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*8n499Gwo9qSjyGnHcr2krw.png"/></div></figure><p id="ac17" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，我们手头有一个约束优化问题。这可以用拉格朗日法解决(如果你不熟悉，这里有一个<a class="ae lq" href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives#lagrange-multipliers-and-constrained-optimization" rel="noopener ugc nofollow" target="_blank">伟大的来源</a>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/9951c59af62d6a9118a6cf42f051d933.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*AJ0qcGP9sQTWrB51pplcYQ.png"/></div></figure><p id="3666" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了找到<strong class="kw iu"> <em class="nc"> b </em> </strong>我们需要将拉格朗日的梯度设置为零，因此对于<strong class="kw iu"> <em class="nc"> b </em> </strong>的导数，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/9231f79db2148b6e64efa8bb6af84a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*Rgdb-g9McUOG4ErgCAlZzw.png"/></div></figure><p id="e4e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/35dd5f7e0ccdcb6f4ebfd644a64d851b.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/format:webp/1*emBQGQ4su43PtWhoDDOxcA.png"/></div></figure><p id="ebb1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是协方差矩阵的特征向量问题！这意味着协方差矩阵的特征向量是主分量<em class="nc">(注意，由于协方差矩阵的对称性，它们是正交的)</em>。</p><p id="1467" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们将它们代入损失函数的最终方程，我们也可以清楚地看到相应特征值的含义:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/ecbc316bd939ef1a774bccd937d7b567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*nEddky8J0EEEe7qvagOtGg.png"/></div></figure><p id="3596" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这意味着为了使<strong class="kw iu"> <em class="nc"> L </em> </strong>最小，我们应该取一个由具有最小特征值的<strong class="kw iu"> <em class="nc"> D-M </em> </strong>特征向量构成的子空间。换句话说，对应于第一个<strong class="kw iu"> <em class="nc"> M 个</em> </strong>最大特征值的特征向量跨越具有最大数据方差的子空间。因此，如果我们按特征值降序排列特征向量，第一个特征向量将代表最大数据方差的方向，第二个特征向量将代表来自其余正交方向的最大数据方差的方向，依此类推。</p><p id="c4a1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">设置<strong class="kw iu"> <em class="nc"> M=0，</em> </strong>我们可以计算最大损耗。它等于所有特征值的总和<em class="nc">(这是非常明显的，因为我们去掉了所有的主成分，即我们去掉了所有的信息)</em>。这意味着我们可以计算每个主成分中包含的方差(信息)的比率，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/da7920a51ca550dfa9b37390bed577aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*S2_j7Xp4KDP3OTAmLiXKxg.png"/></div></figure><p id="bdcc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就很容易决定我们应该为降维留下多少组件。为了方便起见，我们可以绘制解释方差的比率(scree plot)及其累积比率。考虑我们已经在图 7 中看到的数据示例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/f6dc63a5edca7e01d20b57496d98b67d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EL7o_lfMqYMFiceNMTfs2g.png"/></div></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">图 8 . 3D 数据示例的碎石图，第一主成分解释了约 65%的数据差异，第二主成分解释了约 25%，第三主成分解释了约 10%。</p></figure><p id="2d2d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，对于这个例子，如果我们减少 1 维，去掉第三个主成分，我们将丢失大约 10%的信息(数据方差)。</p><p id="cab6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们准备定义一个用主成分分析进行降维的最终算法。</p><h1 id="8038" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">主成分分析的降维算法</h1><p id="d5c5" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">最后是 PCA 算法，用于由<strong class="kw iu"><em class="nc"/></strong>数据点组成的数据<strong class="kw iu"> <em class="nc"> X </em> </strong>，其中每个点是一个<strong class="kw iu"> <em class="nc"> D </em> </strong>维向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/406065e5b9dd149308ded2267e9c3e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*pXUF_GM2DT3kXK-3ExcTHw.png"/></div></figure><ol class=""><li id="6b01" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp oh mu mv mw bi translated">首先，我们需要集中我们的数据:</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/affc5dff75f6091524c01e58c13df326.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*a8Z8fvL9utK3FgYKWucZqw.png"/></div></div></figure><p id="ddb4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">2.然后我们计算中心数据的协方差矩阵<strong class="kw iu"> <em class="nc"> S </em> </strong>并找到它的特征向量<strong class="kw iu"> <em class="nc"> b </em> </strong>和特征值λ。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/c717ecbd637643e0acd9f1b1c8304389.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*m34jfHslnEaOYXWTZuXmEQ.png"/></div></figure><p id="0c20" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">3.按对应的特征值降序排列特征向量。</p><p id="e576" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">4.然后我们从第一个<strong class="kw iu"><em class="nc"/></strong><em class="nc">(M&lt;【D】</em><strong class="kw iu"><em class="nc"/></strong>特征向量形成矩阵<strong class="kw iu"> <em class="nc"> B </em> </strong>并进行降维:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/a58bef191c77b044681407e6f2ad5139.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*OL25pSQvFoije8pHcaR5EA.png"/></div></figure><p id="23a6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">5.对于逆变换，我们需要回到原始空间(寻找投影)并将数据从中心移回，加上平均值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/ad6515da3aa30f550e103e17a9519a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*NR8X87duwZsMcSEwIYC5Tg.png"/></div></figure><h1 id="df4b" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">龟背竹压缩示例</h1><p id="a2f3" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">我们已经做了很多工作，所以让我们玩得开心点。还记得特色图片中的龟背竹吗？让我们用 PCA 压缩它。它是[225，255，3] RGB 图像，因此我们可以分别对每个通道应用 PCA，并探索不同数量的主分量的解释方差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/d5f132f1470b5d3e5e5010627dfb5568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*ogCW6vU_clwgpv-s00aziw.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">图 9 解释了不同颜色通道的差异。</p></figure><p id="fe2e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请注意，完全分解由 255 个分量组成，同时第一个分量解释了 40%以上的方差，前 40 个分量解释了大约 99%的方差。因此，我们可以使用大约 6 倍的较小表示来压缩图像，并且能够以接近零的损失来重建图像。让我们看看它是什么样子的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a774045be9acfce0ea26a52feebfdc01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*EFzy7yNqPLQ_WKUPTQm8aw.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">图 10。使用 PCA 压缩的图像重建示例。</p></figure><h1 id="d8b5" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">编后记</h1><p id="5998" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">到目前为止，我们已经想出了一个基本的 PCA 算法，并用其中一种可能的方法推导出了它。除了基本 PCA，还有其他更具体的实现，如稀疏 PCA、鲁棒 PCA、内核 PCA 等。其中一个扩展是从称为概率 PCA 的潜在变量模型的角度来看的 PCA，我很想写一篇关于它的帖子。敬请关注更新。感谢阅读！</p></div></div>    
</body>
</html>