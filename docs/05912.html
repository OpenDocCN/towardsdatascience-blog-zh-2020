<html>
<head>
<title>Quickly Compare Multiple Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">快速比较多个模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quickly-test-multiple-models-a98477476f0?source=collection_archive---------5-----------------------#2020-05-15">https://towardsdatascience.com/quickly-test-multiple-models-a98477476f0?source=collection_archive---------5-----------------------#2020-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d18c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何高效地训练和评估多个模型</h2></div><h1 id="d211" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="16d1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">所有数据科学家在从事任何机器学习项目时都必须面对的一个问题是…</p><blockquote class="lw lx ly"><p id="acc1" class="la lb lz lc b ld ma ju lf lg mb jx li mc md ll lm me mf lp lq mg mh lt lu lv im bi translated">哪种模型架构最适合我的数据？</p></blockquote><p id="af32" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">有几个理论问题需要考虑。例如，如果您的要素显示出与因变量(目标)的强线性关系，那么线性模型可能最适用。如果关系是非线性的，那么 SVM 或基于实例的分类器(如 K-最近邻)可能是最好的。如果可解释性是至关重要的，基于树的模型可能是正确的选择。此外，还有几个<a class="ae mi" rel="noopener" target="_blank" href="/do-you-know-how-to-choose-the-right-machine-learning-algorithm-among-7-different-types-295d0b0c7f60">你应该考虑的实际问题</a>。</p><p id="5ee2" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">出于本文的目的，我将假设您已经知道您是否正在处理一个<a class="ae mi" rel="noopener" target="_blank" href="/a-brief-introduction-to-supervised-learning-54a3e3932590">监督的</a>与<a class="ae mi" rel="noopener" target="_blank" href="/unsupervised-learning-and-data-clustering-eeecb78b422a">非监督的</a>问题，以及您是否需要<a class="ae mi" rel="noopener" target="_blank" href="/machine-learning-classifiers-a5cc4e1b0623">分类</a>或<a class="ae mi" rel="noopener" target="_blank" href="/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a">回归</a>。</p><p id="8905" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">不幸的是，对于哪种模式是最好的这个问题，没有明确的答案。当面对不确定性时，我心中的科学家总是诉诸人类最严谨、最可靠的知识发现方法:<em class="lz">实验！</em></p><p id="6007" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">在本文中，我将向您展示如何在您的数据集上快速测试多个模型，以找到哪些模型可能提供最佳性能，从而使您能够专注于微调和优化您的模型。</p><h1 id="9e0f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">数据准备</h1><p id="d6d8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在我们开始任何实验之前，我们需要一个数据集。我将假设我们的问题是一个<strong class="lc iu">监督二进制分类</strong>任务。让我们从<a class="ae mi" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> sklearn </a>载入乳腺癌数据集开始吧。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="09b3" class="ms kj it mo b gy mt mu l mv mw">from sklearn.datasets import load_breast_cancer<br/>X, y = data = load_breast_cancer(return_X_y=True)</span></pre><p id="db0b" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">接下来，我们需要将数据分成训练集和测试集。我选择了 75/25 的比例。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="e53a" class="ms kj it mo b gy mt mu l mv mw">from sklearn.model_selection import train_test_split</span><span id="41d1" class="ms kj it mo b gy mx mu l mv mw">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=8675309)</span></pre><p id="a849" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">我们现在准备运行一些实验！</p><h1 id="9bbc" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">进行实验</h1><p id="7775" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们将快速测试 6 个不同模型对该数据集的拟合度。我选择了测试:</p><ol class=""><li id="5bc9" class="my mz it lc b ld ma lg mb lj na ln nb lr nc lv nd ne nf ng bi translated">逻辑回归:基本线性分类器(好到基线)</li><li id="cbae" class="my mz it lc b ld nh lg ni lj nj ln nk lr nl lv nd ne nf ng bi translated">随机森林:集合装袋分类器</li><li id="c266" class="my mz it lc b ld nh lg ni lj nj ln nk lr nl lv nd ne nf ng bi translated">k 近邻:基于实例的分类器</li><li id="b81f" class="my mz it lc b ld nh lg ni lj nj ln nk lr nl lv nd ne nf ng bi translated">支持向量机:最大间隔分类器</li><li id="06ea" class="my mz it lc b ld nh lg ni lj nj ln nk lr nl lv nd ne nf ng bi translated">高斯朴素贝叶斯:概率分类器</li><li id="c3d7" class="my mz it lc b ld nh lg ni lj nj ln nk lr nl lv nd ne nf ng bi translated">XGBoost:合奏(极限！)提升分类器</li></ol><p id="434c" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">为了运行初始实验，我使用了每个模型的默认参数。为了更准确地表示每个模型的拟合程度，需要调整默认参数；然而，出于本文的目的，不调整每个模型会使总体思路更加清晰。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="ef91" class="ms kj it mo b gy mt mu l mv mw">from sklearn.linear_model import LogisticRegression<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.naive_bayes import GaussianNB<br/>from xgboost import XGBClassifier<br/>from sklearn import model_selection<br/>from sklearn.utils import class_weight<br/>from sklearn.metrics import classification_report<br/>from sklearn.metrics import confusion_matrix<br/>import numpy as np<br/>import pandas as pd</span><span id="cbb8" class="ms kj it mo b gy mx mu l mv mw">def run_exps(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -&gt; pd.DataFrame:<br/>    '''<br/>    Lightweight script to test many models and find winners</span><span id="d8ce" class="ms kj it mo b gy mx mu l mv mw">:param X_train: training split<br/>    :param y_train: training target vector<br/>    :param X_test: test split<br/>    :param y_test: test target vector<br/>    :return: DataFrame of predictions<br/>    '''<br/>    <br/>    dfs = []</span><span id="a5d1" class="ms kj it mo b gy mx mu l mv mw">models = [<br/>          ('LogReg', LogisticRegression()), <br/>          ('RF', RandomForestClassifier()),<br/>          ('KNN', KNeighborsClassifier()),<br/>          ('SVM', SVC()), <br/>          ('GNB', GaussianNB()),<br/>          ('XGB', XGBClassifier())<br/>        ]</span><span id="1e1f" class="ms kj it mo b gy mx mu l mv mw">results = []<br/>    names = []<br/>    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']<br/>    target_names = ['malignant', 'benign']</span><span id="2354" class="ms kj it mo b gy mx mu l mv mw">for name, model in models:<br/>        kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)<br/>        cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)<br/>        clf = model.fit(X_train, y_train)<br/>        y_pred = clf.predict(X_test)<br/>        print(name)<br/>        print(classification_report(y_test, y_pred, target_names=target_names))</span><span id="8938" class="ms kj it mo b gy mx mu l mv mw">results.append(cv_results)<br/>        names.append(name)</span><span id="2ca2" class="ms kj it mo b gy mx mu l mv mw">this_df = pd.DataFrame(cv_results)<br/>        this_df['model'] = name<br/>        dfs.append(this_df)</span><span id="bdcf" class="ms kj it mo b gy mx mu l mv mw">final = pd.concat(dfs, ignore_index=True)</span><span id="6d83" class="ms kj it mo b gy mx mu l mv mw">return final</span></pre><p id="020e" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">这个脚本中有很多东西需要解开。首先，我们创建一个变量<code class="fe nm nn no mo b">dfs</code>来保存所有数据集，这些数据集将通过对训练集应用 5 重交叉验证来创建。</p><p id="e848" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">接下来，<code class="fe nm nn no mo b">models</code>在一个元组列表中，保存每个要测试的分类器的名称和类。在这之后，我们循环遍历这个列表并运行 5 重交叉验证。每次运行的结果都记录在熊猫数据帧中，我们将其添加到<code class="fe nm nn no mo b">dfs</code>列表中。必须注意，这里记录的指标是两个类别的<em class="lz">加权平均值</em>指标。<strong class="lc iu">这对于任何不平衡的数据集都不适用</strong>，因为多数类的性能会盖过少数类。您可能希望调整下面的脚本，只记录感兴趣的类的指标！</p><p id="943f" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">为了进一步帮助评估，测试集上的分类报告被打印到屏幕上。最后，我们连接并返回所有结果。</p><figure class="mj mk ml mm gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi np"><img src="../Images/2b99ccffaa20c6b66bf087437fca47a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OBlMdg1alm3gWhC_V0izcA.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">脚本的示例输出</p></figure><h1 id="59af" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">评估结果</h1><p id="9081" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了总结我们的分析，我们将分析从<code class="fe nm nn no mo b">run_exps()</code>脚本返回的<code class="fe nm nn no mo b">final</code>数据帧中的数据。</p><p id="9ddb" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">为了从每个模型中获得更好的度量分布估计，我在 30 个样本上运行了经验自举。此外，我将数据分为两类:性能指标和适合时间指标。下面的代码块实现了这一点。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="a6a5" class="ms kj it mo b gy mt mu l mv mw">bootstraps = []<br/>for model in list(set(final.model.values)):<br/>    model_df = final.loc[final.model == model]<br/>    bootstrap = model_df.sample(n=30, replace=True)<br/>    bootstraps.append(bootstrap)<br/>        <br/>bootstrap_df = pd.concat(bootstraps, ignore_index=True)<br/>results_long = pd.melt(bootstrap_df,id_vars=['model'],var_name='metrics', value_name='values')</span><span id="c5af" class="ms kj it mo b gy mx mu l mv mw">time_metrics = ['fit_time','score_time'] # fit time metrics</span><span id="aa8f" class="ms kj it mo b gy mx mu l mv mw">## PERFORMANCE METRICS<br/>results_long_nofit = results_long.loc[~results_long['metrics'].isin(time_metrics)] # get df without fit data<br/>results_long_nofit = results_long_nofit.sort_values(by='values')</span><span id="ae8f" class="ms kj it mo b gy mx mu l mv mw">## TIME METRICS<br/>results_long_fit = results_long.loc[results_long['metrics'].isin(time_metrics)] # df with fit data<br/>results_long_fit = results_long_fit.sort_values(by='values')</span></pre><p id="af0e" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">我们现在有一些可靠的数据来绘制和分析。首先，让我们从五重交叉验证中绘制我们的性能指标。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="be7a" class="ms kj it mo b gy mt mu l mv mw">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>plt.figure(figsize=(20, 12))<br/>sns.set(font_scale=2.5)<br/>g = sns.boxplot(x="model", y="values", hue="metrics", data=results_long_nofit, palette="Set3")<br/>plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)<br/>plt.title('Comparison of Model by Classification Metric')<br/>plt.savefig('./benchmark_models_performance.png',dpi=300)</span></pre><figure class="mj mk ml mm gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi ob"><img src="../Images/d32e4e927efc4428ecdcae926037e80b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*THhvoW8w2DfAbleXc-oB5A.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">按模型和指标的箱线图</p></figure><p id="63a8" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">很明显，支持向量机在所有指标上对我们的数据拟合得很差，而集合决策树模型(Random Forest 和 XGBoost)对数据拟合得很好。</p><p id="5e6b" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">训练和得分时间怎么样？</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="a3bd" class="ms kj it mo b gy mt mu l mv mw">plt.figure(figsize=(20, 12))<br/>sns.set(font_scale=2.5)<br/>g = sns.boxplot(x="model", y="values", hue="metrics", data=results_long_fit, palette="Set3")<br/>plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)<br/>plt.title('Comparison of Model by Fit and Score Time')<br/>plt.savefig('./benchmark_models_time.png',dpi=300)</span></pre><figure class="mj mk ml mm gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi oc"><img src="../Images/3f3dead576e3ce207909cdf2cfe66dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OdCtOBI4K32uBIdJHq34cA.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">按型号划分的培训和评分时间</p></figure><p id="3a13" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">哇，SVM 就是不能休息一下！表现最差的车型<em class="lz">和</em>训练/评分慢！</p><p id="45d8" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">有趣的是，XGBoost 无疑是训练最慢的；然而，这是最好的表现。我们通常会在性能和训练时间方面进行权衡。考虑需要大量数据和时间来训练的神经网络，但是一般来说<em class="lz">表现得非常好。</em></p><p id="ac1f" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">RandomForest 和 GNB 的 KNN 相比速度相对较慢，而 LogReg 的表现排名第二。如果我继续改进模型，我可能会把大部分精力放在 RandomForest 上，因为它的表现几乎与 XGBoost ( <em class="lz">)相同，它们的 95%置信区间可能会重叠！</em>)但是训练速度快了差不多 4 倍！</p><p id="78b0" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">如果您希望对这些模型进行更多分析(例如，计算每个指标的置信区间)，您将需要访问每个指标的均值和标准差。你可以在这里找到这些信息。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="9dfd" class="ms kj it mo b gy mt mu l mv mw">metrics = list(set(results_long_nofit.metrics.values))<br/>bootstrap_df.groupby(['model'])[metrics].agg([np.std, np.mean])</span></pre><figure class="mj mk ml mm gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi od"><img src="../Images/f70fb8c5fb42e0c21b866f2fe79945c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*idbLzFtbY3W29-gdCdP7bw.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">每个指标的平均值和标准差</p></figure><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="a05a" class="ms kj it mo b gy mt mu l mv mw">time_metrics = list(set(results_long_fit.metrics.values))<br/>bootstrap_df.groupby(['model'])[time_metrics].agg([np.std, np.mean])</span></pre><figure class="mj mk ml mm gt nq gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/eca8134e0f574f327365c4972ef4e7ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*OBpKIombYe91kaHTQ3qufQ.png"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">时间的平均值和标准差</p></figure><h1 id="39c8" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="bd38" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，您已经有了工具，可以在您的数据上快速测试许多不同的模型，并查看哪些架构可能最适合您。</p><blockquote class="lw lx ly"><p id="a721" class="la lb lz lc b ld ma ju lf lg mb jx li mc md ll lm me mf lp lq mg mh lt lu lv im bi translated">这种比较并不详尽，这一点我怎么强调也不为过！</p></blockquote><p id="dad6" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">以上分析只考虑了<em class="lz">意味着</em>精度、召回率等。在你的实际问题中，你不太可能关心所有类的平均精度，相反，你可能特别关注一个类的精度！此外，必须调整每个模型的超参数，以真正评估它们与数据的拟合程度。</p><h1 id="dfad" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">参考</h1><p id="bba1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><a class="ae mi" href="https://conference.scipy.org/proceedings/scipy2010/pdfs/mckinney.pdf" rel="noopener ugc nofollow" target="_blank">python 中用于统计计算的数据结构</a>，McKinney，第 9 届科学中的 Python 会议录，第 445 卷，2010 年。</p><pre class="mj mk ml mm gt mn mo mp mq aw mr bi"><span id="3347" class="ms kj it mo b gy mt mu l mv mw">@software{reback2020pandas,<br/>    author       = {The pandas development team},<br/>    title        = {pandas-dev/pandas: Pandas},<br/>    month        = feb,<br/>    year         = 2020,<br/>    publisher    = {Zenodo},<br/>    version      = {latest},<br/>    doi          = {10.5281/zenodo.3509134},<br/>    url          = {https://doi.org/10.5281/zenodo.3509134}<br/>}</span></pre><p id="1ecf" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">Harris，C.R .，Millman，K.J .，van der Walt，S.J .等人<em class="lz">用 NumPy 进行数组编程</em>。自然 585，357–362(2020)。DOI:<a class="ae mi" href="https://doi.org/10.1038/s41586-020-2649-2" rel="noopener ugc nofollow" target="_blank">10.1038/s 41586–020–2649–2</a>。</p><p id="6a1e" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated"><a class="ae mi" href="http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html" rel="noopener ugc nofollow" target="_blank">sci kit-learn:Python 中的机器学习</a>，Pedregosa <em class="lz">等人</em>，JMLR 12，第 2825–2830 页，2011 年。</p><p id="5b2b" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated"><a class="ae mi" href="https://doi.org/10.1109/MCSE.2007.55" rel="noopener ugc nofollow" target="_blank"> J. D. Hunter，“Matplotlib:2D 图形环境”，科学计算&amp;工程，第 9 卷，第 3 期，第 90–95 页，2007 年</a>。</p><p id="363e" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">瓦斯科姆，法学硕士，(2021 年)。seaborn:统计数据可视化。《开源软件杂志》，6 卷(60 期)，3021 页，<a class="ae mi" href="https://doi.org/10.21105/joss.03021" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.21105/joss.03021</a></p><p id="d9ab" class="pw-post-body-paragraph la lb it lc b ld ma ju lf lg mb jx li lj md ll lm ln mf lp lq lr mh lt lu lv im bi translated">陈，t .，&amp; Guestrin，C. (2016)。XGBoost:一个可扩展的树提升系统。第 22 届 ACM SIGKDD 知识发现和数据挖掘国际会议论文集(第 785–794 页)。美国纽约州纽约市:ACM。<a class="ae mi" href="https://doi.org/10.1145/2939672.2939785" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1145/2939672.2939785</a></p></div></div>    
</body>
</html>