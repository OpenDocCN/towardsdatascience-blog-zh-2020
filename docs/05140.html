<html>
<head>
<title>Day 123 of #NLP365: NLP Papers Summary — Context-aware Embedding for Targeted Aspect-based Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365 的第 123 天:NLP 论文摘要——用于基于方面的目标情感分析的上下文感知嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-123-of-nlp365-nlp-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f998d1131?source=collection_archive---------64-----------------------#2020-05-02">https://towardsdatascience.com/day-123-of-nlp365-nlp-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f998d1131?source=collection_archive---------64-----------------------#2020-05-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="b32b" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="64a6" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">NLP 论文摘要是我总结 NLP 研究论文要点的系列文章</h2></div><p id="3048" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我在 2020 年每天记录我的 NLP 学习旅程的地方。在这里，你可以随意查看我在过去的 262 天里学到了什么。在本文的最后，你可以找到以前的论文摘要，按自然语言处理领域分类:)</p><p id="f42f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的 NLP 论文是<strong class="lf jp"> <em class="lz">针对有针对性的基于方面的情感分析的上下文感知嵌入</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="3d7b" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="dc3c" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">提出了上下文感知嵌入，使用高度相关的词来改进目标和方面的嵌入。大量先前的工作使用上下文无关向量来构造目标和方面嵌入，这导致语义信息的丢失，并且未能捕捉特定目标、其方面和其上下文之间的相互联系。这种方法导致了基于方面的目标情感分析(TABSA)的 SOTA 结果。TABSA 的目标是给定一个输入句子，我们想要提取属于目标的方面的情感。下图展示了 TABSA 任务:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/6a04aac7104b99421eb6563215b228eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/0*8efvjG8o_mL2uc9j.png"/></div></figure><p id="68c8" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这些贡献如下:</p><ol class=""><li id="a351" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly no np nq nr bi translated">通过使用稀疏系数向量来识别与目标高度相关的单词并相应地改进目标嵌入，来为目标构建上下文感知嵌入</li><li id="827c" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">微调方面嵌入以尽可能接近高度相关的目标嵌入</li><li id="11f4" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">在 SentiHood 和 SemEval 2015 上取得 SOTA 成果</li></ol><h1 id="2af6" class="mh mi jf bd mj mk nx mm mn mo ny mq mr ku nz kv mt kx oa ky mv la ob lb mx my bi translated">方法学</h1><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/68ab2177f1551f4bed1b343b5d50780b.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/0*J1j2BkHux7jqDeNJ.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">上下文感知嵌入的架构[1]</p></figure><p id="17f0" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">模型框架具有以下步骤:</p><ol class=""><li id="6e8e" class="nj nk jf lf b lg lh lj lk lm nl lq nm lu nn ly no np nq nr bi translated">句子嵌入矩阵 X 被馈入全连接层和阶跃函数以创建稀疏系数向量 u’。</li><li id="6123" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">u '的隐藏输出用于细化目标和方面嵌入</li><li id="78d3" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated">计算平方欧几里德函数并训练模型以最小化距离，从而获得目标和方面的最终精确嵌入</li></ol><h2 id="3af9" class="od mi jf bd mj oe of dn mn og oh dp mr lm oi oj mt lq ok ol mv lu om on mx jl bi translated">目标表示</h2><p id="8a14" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">可以通过将句子单词嵌入 X 乘以稀疏系数向量 u’来计算精确的目标嵌入。稀疏系数向量使用阶跃函数展示了上下文中不同单词的重要性。对于每个目标，我们通过迭代最小化目标和句子中高度相关单词之间的平方欧几里德距离来计算上下文感知目标嵌入。</p><h2 id="61b1" class="od mi jf bd mj oe of dn mn og oh dp mr lm oi oj mt lq ok ol mv lu om on mx jl bi translated">方面表示</h2><p id="3073" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们通过使用高度相关词的稀疏系数向量来细化方面嵌入。这背后的论点是，方面词通常包含重要的信息，上下文信息通常与方面有很高的联系。同样，对于每个方面，我们通过最小化方面嵌入、上下文感知目标嵌入和不相关嵌入之间的平方欧几里德距离来计算上下文感知方面嵌入。这将微调我们的方面嵌入，使其更接近高度相关的目标嵌入，并远离不相关的嵌入。</p><h1 id="0c0f" class="mh mi jf bd mj mk nx mm mn mo ny mq mr ku nz kv mt kx oa ky mv la ob lb mx my bi translated">实验和结果</h1><p id="a770" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">有两个评估数据集:SentiHood 和 SemEval 2015 Task 12。</p><h2 id="0665" class="od mi jf bd mj oe of dn mn og oh dp mr lm oi oj mt lq ok ol mv lu om on mx jl bi translated">模型比较</h2><ol class=""><li id="96dc" class="nj nk jf lf b lg mz lj na lm oo lq op lu oq ly no np nq nr bi translated"><em class="lz">LSTM-决赛</em>。只使用最终隐藏状态的 BiLSTM</li><li id="e9a4" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">LSTM-洛克</em>。使用位置目标所在的隐藏状态的 BiLSTM</li><li id="0b47" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz"> SenticLSTM </em>。使用外部知识的 BiLSTM</li><li id="4edc" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">延迟记忆</em>。延迟记忆机制</li><li id="72c1" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz"> RE+SenticLSTM </em>。我们的精致嵌入+ SenticLSTM</li><li id="5efb" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly no np nq nr bi translated"><em class="lz">RE+延时记忆</em>。我们的精细嵌入+延迟记忆</li></ol><h2 id="36af" class="od mi jf bd mj oe of dn mn og oh dp mr lm oi oj mt lq ok ol mv lu om on mx jl bi translated">结果</h2><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/89838b4d091b326858c09ef7d98e8e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/0*KPNFXMu5TSOkgDQR.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">关于 SentiHood 和 Semeval 2015 的结果[1]</p></figure><p id="d250" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于情感，我们提出的方法在 SenticLSTM 和延迟记忆的基础上，在方面检测和情感分类方面都取得了比原始模型更好的性能。我们的上下文感知嵌入允许模型更好地捕捉方面和情感信息，因为我们能够更好地对目标、其方面和上下文之间的互连进行建模。对于 SemEval 2015，我们展示了类似的结果，我们提出的方法优于原始模型。下图显示了我们提出的上下文感知嵌入与使用 TSNE 的原始方面嵌入的对比。如图所示，使用我们的上下文感知嵌入，不同方面之间有更多的分离，展示了它在上下文中区分不同方面的能力以及捕捉特定方面的共同特征的能力。</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ec39ccb15a6cce699fbc951550064c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/0*8zsdRdKwYXmtPg1u.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">通过不同的基于嵌入的模型学习的中间嵌入[1]</p></figure><h1 id="d8c1" class="mh mi jf bd mj mk nx mm mn mo ny mq mr ku nz kv mt kx oa ky mv la ob lb mx my bi translated">结论和未来工作</h1><p id="0129" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">通过选择和使用高度相关的词来提炼目标和方面嵌入，我们能够提取特定目标、其方面和其上下文之间的联系，以生成更好的有意义的嵌入。未来的工作包括为其他类似的 NLP 任务探索这种方法。</p><h2 id="f2cd" class="od mi jf bd mj oe of dn mn og oh dp mr lm oi oj mt lq ok ol mv lu om on mx jl bi translated">来源:</h2><p id="2594" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">[1]梁，b，杜，j，徐，r，李，b，黄，h，2019 .面向基于方面的情感分析的上下文感知嵌入。<em class="lz"> arXiv 预印本 arXiv:1906.06945 </em>。</p><p id="5b41" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于 2020 年 5 月 2 日</em><a class="ae jc" href="https://ryanong.co.uk/2020/05/02/day-123-nlp-papers-summary-context-aware-embedding-for-targeted-aspect-based-sentiment-analysis/" rel="noopener ugc nofollow" target="_blank"><em class="lz">【https://ryanong.co.uk】</em></a><em class="lz">。</em></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="c226" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="d60c" class="nj nk jf lf b lg mz lj na lm oo lq op lu oq ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="eef4" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sense-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="ddc5" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitive-analysis-f 24 a2 EC 1 ca 32</a></li><li id="7c15" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="8a96" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towards data science . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li><li id="961b" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towards data science . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li><li id="32e2" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5">https://towards data science . com/day-112-of-NLP 365-NLP-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b 7 a5 e 245 b5</a></li></ul><h1 id="7332" class="mh mi jf bd mj mk nx mm mn mo ny mq mr ku nz kv mt kx oa ky mv la ob lb mx my bi translated">总结</h1><ul class=""><li id="e1b0" class="nj nk jf lf b lg mz lj na lm oo lq op lu oq ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="c21e" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1b 27</a></li><li id="815c" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-113-of-nlp365-nlp-papers-summary-on-extractive-and-abstractive-neural-document-87168b7e90bc">https://towards data science . com/day-113-of-NLP 365-NLP-papers-summary-on-extractive-and-abstract-neural-document-87168 b 7 e 90 BC</a></li><li id="79ba" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-116-of-nlp365-nlp-papers-summary-data-driven-summarization-of-scientific-articles-3fba016c733b">https://towards data science . com/day-116-of-NLP 365-NLP-papers-summary-data-driven-summary-of-scientific-articles-3 FBA 016 c 733 b</a></li><li id="388a" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-117-of-nlp365-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge-61ae6cdf32f">https://towards data science . com/day-117-of-NLP 365-NLP-papers-summary-abstract-text-summary-a-low-resource-challenge-61 AE 6 CDF 32 f</a></li><li id="a03a" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-118-of-nlp365-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-aea118a5eb3f">https://towards data science . com/day-118-of-NLP 365-NLP-papers-summary-extractive-summary-of-long-documents-by-combining-AEA 118 a5 eb3f</a></li><li id="1088" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-120-of-nlp365-nlp-papers-summary-a-simple-theoretical-model-of-importance-for-summarization-843ddbbcb9b">https://towards data science . com/day-120-of-NLP 365-NLP-papers-summary-a-simple-theory-model-of-importance-for-summary-843 ddbcb 9b</a></li><li id="f5a7" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-121-of-nlp365-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization-cd55e577f6de">https://towards data science . com/day-121-of-NLP 365-NLP-papers-summary-concept-pointer-network-for-abstract-summary-cd55e 577 F6 de</a></li></ul><h1 id="249f" class="mh mi jf bd mj mk nx mm mn mo ny mq mr ku nz kv mt kx oa ky mv la ob lb mx my bi translated">其他人</h1><ul class=""><li id="eb3f" class="nj nk jf lf b lg mz lj na lm oo lq op lu oq ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li><li id="2d14" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-111-of-nlp365-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection-bff7f5f20ce5">https://towards data science . com/day-111-of-NLP 365-NLP-papers-summary-the-risk-of-race-of-bias-in-hate-speech-detection-BFF 7 F5 f 20 ce 5</a></li><li id="ac91" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-115-of-nlp365-nlp-papers-summary-scibert-a-pretrained-language-model-for-scientific-text-185785598e33">https://towards data science . com/day-115-of-NLP 365-NLP-papers-summary-scibert-a-pre trained-language-model-for-scientific-text-185785598 e33</a></li><li id="843e" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-119-nlp-papers-summary-an-argument-annotated-corpus-of-scientific-publications-d7b9e2ea1097">https://towards data science . com/day-119-NLP-papers-summary-an-argument-annoted-corpus-of-scientific-publications-d 7 b 9 e 2e ea 1097</a></li><li id="60d7" class="nj nk jf lf b lg ns lj nt lm nu lq nv lu nw ly os np nq nr bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-122-of-nlp365-nlp-papers-summary-applying-bert-to-document-retrieval-with-birch-766eaeac17ab">https://towards data science . com/day-122-of-NLP 365-NLP-papers-summary-applying-Bert-to-document-retrieval-with-birch-766 EAC 17 ab</a></li></ul></div></div>    
</body>
</html>