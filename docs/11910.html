<html>
<head>
<title>Principal Component Analysis - Visualized</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析-可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-visualized-17701e18f2fa?source=collection_archive---------54-----------------------#2020-08-17">https://towardsdatascience.com/principal-component-analysis-visualized-17701e18f2fa?source=collection_archive---------54-----------------------#2020-08-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b063" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用主成分分析(PCA)的数据压缩</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7136bb0c42fda2231a0ffbb1a99eaed7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*poXRRZdZAElrrP9C3ZQLoQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">约书亚·索蒂诺在<a class="ae kv" href="https://unsplash.com/s/photos/data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="159a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你曾经上过机器学习的在线课程，你一定遇到过用于降维的主成分分析，或者简单地说，用于数据压缩的主成分分析。你猜怎么着，我也上过这样的课程，但我从来没有真正理解 PCA 的图形意义，因为我看到的都是矩阵和方程。我花了相当多的时间从各种来源理解这个概念。所以，我决定在一个地方编译它。</p><p id="ada5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将采用一种可视化(图形化)的方法来理解 PCA 以及如何使用它来压缩数据。假设有线性代数和矩阵的基础知识。如果你对这个概念不熟悉，就跟着做吧，我已经尽了最大努力让它尽可能简单。</p><h1 id="1352" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">介绍</h1><p id="3c89" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如今，包含大量维度的数据集越来越常见，并且通常很难解释。一个例子可以是一个面部照片数据库，比如说，<strong class="ky ir"> 1，000，000 人</strong>。如果每张面部照片的尺寸为<strong class="ky ir"> 100x100，</strong>，则每张面部的数据为 10000 维(每张面部存储 100x100 = 10,000 个唯一值)。现在，如果需要 1 个字节来存储每个像素的信息，那么需要 10000 个字节来存储 1 张脸。由于数据库中有 1000 张人脸，因此需要 10，000 x 1，000，000 = 10 GB 来存储数据集。</p><p id="53cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">主成分分析(PCA)是一种用于降低这种数据集的维度的技术，利用了这些数据集中的图像具有共同点的事实。例如，在由脸部照片组成的数据集中，每张照片都有像眼睛、鼻子、嘴巴这样的面部特征。我们可以为每种类型的特征制作一个模板，然后将这些模板组合起来，生成数据集中的任何人脸，而不是逐个像素地对这些信息进行编码。在这种方法中，每个模板仍然是 100×100 = 1000 维，但是由于我们将重用这些模板(基函数)来生成数据集中的每个面，因此所需的模板数量将非常少。PCA 正是这样做的。</p><h1 id="6960" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">PCA 是如何工作的？</h1><p id="735d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这部分会有点技术性，所以请耐心听我说！我将用一个简单的例子来解释 PCA 的工作原理。让我们考虑下面显示的包含 100 个二维点的数据(需要 x &amp; y 坐标来表示每个点)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/5eb6062d8f2e709e81c607dd061a2fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*OQQaSdRrnnew3zZizJkJyw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9f5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">目前，我们使用 2 个值来表示每个点。让我们用更专业的方式来解释这种情况。我们目前使用 2 个基函数，<br/> x 为(1，0)，y 为(0，1)。数据集中的每个点都表示为这些基函数的加权和。例如，点(2，3)可以表示为 2(1，0) + 3(0，1) = (2，3)。如果我们忽略这些基函数中的任何一个，我们将无法准确地表示数据集中的点。因此，两个维度都是必要的，我们不能只去掉其中一个来降低存储需求。这组基函数实际上是二维的笛卡尔坐标。</p><p id="412f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们仔细观察，我们可以很好地看到数据近似于一条线，如下面的红线所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/bcb7a14bd3129440e77619dc597e492d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*YeGV5QXVmVpqQc9yZQ7Naw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="3c1d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们旋转坐标系，使 x 轴沿着红线。然后，y 轴(绿线)将垂直于这条红线。让我们把这些新的 x 轴和 y 轴分别称为<strong class="ky ir"> a 轴</strong>和<strong class="ky ir"> b 轴</strong>。如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/c63e8fb622e44be831bb9c173eb59f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XvyROjRTudsu14qCWYl06Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="8493" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果我们使用<strong class="ky ir"> a </strong>和<strong class="ky ir"> b </strong>作为这个数据集的新的集合基函数(而不是使用<strong class="ky ir"> x </strong>和<strong class="ky ir"> y </strong>)，那么说数据集中的大部分方差沿着<strong class="ky ir"> a 轴</strong>不会错。现在，如果我们去掉<strong class="ky ir"> b 轴，</strong>我们仍然可以使用<strong class="ky ir"> a 轴</strong>非常精确地表示数据集中的点。因此，我们现在只需要一半的存储空间来存储数据集并准确地重建它。这正是 PCA 的工作原理。</p><p id="98a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">五氯苯甲醚是一个 4 步流程。</strong>从包含<em class="mr"> n </em>维的数据集开始(需要表示<em class="mr"> n </em>轴):</p><ul class=""><li id="a7f7" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">找到一组新的基函数(<em class="mr">n</em>-轴)，其中一些轴对数据集中的方差贡献最大，而其他轴贡献很小。</li><li id="19df" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">按照方差贡献的降序排列这些轴。</li><li id="68e6" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">现在，选择要使用的顶部<em class="mr"> k </em>轴，放下剩余的<em class="mr"> n-k </em>轴。</li><li id="a9ba" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">现在，将数据集投影到这些<em class="mr"> k </em>轴上。</li></ul><p id="8ce7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这 4 个步骤之后，数据集将从<em class="mr"> n </em>维压缩到仅<em class="mr"> k </em>维(<em class="mr"> k </em> &lt; <em class="mr"> n </em>)。</p><h1 id="95c5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">步伐</h1><p id="d9e6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了简单起见，让我们取上面的数据集，并在其上应用主成分分析。所涉及的步骤将是技术和线性代数的基本知识是假设。您可以在此处查看 Colab 笔记本:</p><div class="ng nh gp gr ni nj"><a href="https://colab.research.google.com/drive/1QQcoE501NS9nPBAmlg12zQWHCT1IlI96" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">主成分分析</h2><div class="nq l"><p class="bd b dl z fp no fr fs np fu fw dk translated">colab.research.google.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw kp nj"/></div></div></a></div><h2 id="8dab" class="nx lt iq bd lu ny nz dn ly oa ob dp mc lf oc od me lj oe of mg ln og oh mi oi bi translated">第一步</h2><p id="c25e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">由于这是一个二维数据集，<em class="mr"> n </em> =2。第一步是找到新的一组基函数(<strong class="ky ir"> a </strong> &amp; <strong class="ky ir"> b </strong>)。在上面的解释中，我们看到数据集沿着一条线具有最大方差，我们手动选择这条线作为<strong class="ky ir"> a </strong>轴，垂直于它的线作为<strong class="ky ir"> b </strong>轴。实际上，我们希望这个步骤是自动化的。</p><p id="4b71" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为此，我们可以找到数据集协方差矩阵的特征值和特征向量。由于数据集是二维的，我们将得到 2 个特征值和它们相应的特征向量。然后，2 个特征向量是两个基函数(新轴),两个特征值告诉我们相应特征向量的方差贡献。特征值的大值意味着相应的特征向量(轴)对数据集的总方差的贡献更大。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/1748a727ac820a28754b60d45605c6ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*-3eRzfDdoFWyNxWzSifb9A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="c0f9" class="nx lt iq bd lu ny nz dn ly oa ob dp mc lf oc od me lj oe of mg ln og oh mi oi bi translated">第二步</h2><p id="07cf" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在，按照特征值递减对特征向量(轴)进行排序。这里，我们可以看到<strong class="ky ir"> a 轴</strong>的特征值比<br/> <strong class="ky ir"> b 轴</strong>的特征值大得多，这意味着 a 轴对数据集方差的贡献更大。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/82e4faf4d681d8c500c1fa8322c2c24b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SE0PUcNA4bofoOsafpMdiw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="113a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个轴对总数据集方差的百分比贡献可计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/75c6e712c68dd2153def93530cde4b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nazXtcaDZSctb2Rt"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/60740a2e37375a89eee58899b57f3729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*KAbKJzqtTa26DaDXJrasrA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="dad1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的数字证明了<strong class="ky ir"> a 轴</strong>对数据集方差的贡献为 99.7%，我们可以去掉<strong class="ky ir"> b 轴</strong>，仅损失 0.28%的方差。</p><h2 id="6dfd" class="nx lt iq bd lu ny nz dn ly oa ob dp mc lf oc od me lj oe of mg ln og oh mi oi bi translated">第三步</h2><p id="b9ab" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在，我们将去掉<strong class="ky ir"> b 轴</strong>，只保留<strong class="ky ir"> a 轴。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/08ab458d4f27b140feb8f6ccff8ffb81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*M_fJWTYK4JvvnWTa_dqnpg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="8dd6" class="nx lt iq bd lu ny nz dn ly oa ob dp mc lf oc od me lj oe of mg ln og oh mi oi bi translated">第四步</h2><p id="fdfd" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在，将第一个特征向量(a 轴)整形为一个 2x1 的矩阵，称为投影矩阵。它将用于将形状为<br/> (100，2)的原始数据集投影到新的基函数(a 轴)上，从而将其压缩到(100，1)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/bf18d8070dd2fabb0ffe0768eb1dccbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZwjwHRLkbVahbFRc"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/f7d4fcfed8d78055c3db9dbf78c3105f.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*Hdf3-2Cq22PiwB8r"/></div></figure><h1 id="a9c1" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">重建数据</h1><p id="983b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在，我们可以使用投影矩阵将数据扩展回其原始大小，当然会有很小的方差损失(0.28%)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/860985d7fbf663ff3ad9c06ebf53706a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9ZUBjcbRNOPc9S0s"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/2ae85b13a4cb242977271f1df51a716e.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*hyDH17-H9d3-VS2m"/></div></figure><p id="e0e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">重建的数据如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/255416911708b199ad9ba44b62a764b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*9bg0y68vImeOGjuTNM737A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="04f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，沿<strong class="ky ir"> b 轴</strong>的方差(0.28%)丢失，如上图所示。</p><h1 id="71c1" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">那都是乡亲们！</h1><p id="d938" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果你成功了，向你致敬！在本文中，我们采用了一种图形化的方法来理解主成分分析是如何工作的，以及如何将其用于数据压缩。在我的下一篇文章中，我将展示如何使用 PCA 来压缩<strong class="ky ir">在野外(LFW)标记的人脸，</strong>由 13233 个人脸图像组成的大规模数据集。</p><p id="1488" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有什么建议请留言评论。我定期写文章，所以你应该考虑关注我，在你的订阅中获得更多这样的文章。</p><p id="d3c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你喜欢这篇文章，你可能也会喜欢这些:</p><div class="ng nh gp gr ni nj"><a rel="noopener follow" target="_blank" href="/machine-learning-visualized-11965ecc645c"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">机器学习-可视化</h2><div class="or l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">理解机器学习的视觉方法</h3></div><div class="nq l"><p class="bd b dl z fp no fr fs np fu fw dk translated">towardsdatascience.com</p></div></div><div class="nr l"><div class="os l nt nu nv nr nw kp nj"/></div></div></a></div><div class="ng nh gp gr ni nj"><a rel="noopener follow" target="_blank" href="/face-landmarks-detection-with-pytorch-4b4852f5e9c4"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">用 Pytorch 检测人脸标志点</h2><div class="or l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">想知道 Snapchat 或 Instagram 如何将惊人的滤镜应用到你的脸上吗？该软件检测你的关键点…</h3></div><div class="nq l"><p class="bd b dl z fp no fr fs np fu fw dk translated">towardsdatascience.com</p></div></div><div class="nr l"><div class="ot l nt nu nv nr nw kp nj"/></div></div></a></div><p id="4d97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">访问我的<a class="ae kv" href="http://arkalim.github.io/" rel="noopener ugc nofollow" target="_blank">网站</a>了解更多关于我和我的工作。</p></div></div>    
</body>
</html>