# 强化学习算法的结构概述

> 原文：<https://towardsdatascience.com/an-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af?source=collection_archive---------17----------------------->

## 演员评论家、政策梯度、DQN、VFA、SARSA、Q-learning、基于模型和无模型蒙特卡罗、动态规划

![](img/076fbaa07f098f767b0e3dbaba15edfb.png)

作者照片

强化学习在过去的十年里获得了极大的普及，在机器人、游戏和许多其他领域都有一系列成功的现实应用。

在本文中，我将提供经典强化学习算法的高级结构概述。讨论将基于它们在算法复杂性方面的异同。

# RL 基础

让我们从一些基本概念的快速复习开始。如果您已经熟悉 RL 的所有术语，可以跳过这一部分。

强化学习模型是一种基于状态的模型，它利用了[马尔可夫决策过程(MDP)](https://en.wikipedia.org/wiki/Markov_decision_process) 。RL 的基本要素包括:

**插曲(展示)**:播放状态和动作的整个序列，直到达到终止状态；

**当前状态 s(或 s *t* )** :代理当前所在的位置；

**下一个状态 s’(或 s *t+1* )** :当前状态的下一个状态；

**动作 a** :状态 s 时要采取的动作；

**转移概率 P(s'|s，a)** :在状态 s *t* 采取行动时到达 s '的概率；

**Policy π(s，a)** :从每个状态到一个动作的映射，决定了代理在每个状态下如何动作。它可以是确定的，也可以是随机的

**奖励 R(或 R(s，a))** :奖励函数，生成在状态 s 采取行动 a 的奖励；

**Return G *t*** :状态 s *t 的未来总报酬；*

![](img/db7fb9a35cce80c97f2703ab640cf9a1.png)

**值 V(s)** :从状态 s 开始的期望收益；

![](img/17ec58fa333a7b52984d324f4e22b6d9.png)

**Q 值 Q(s，a)** :从状态 s 开始，采取行动 a 的期望收益；

![](img/54dd713acd8becba516ba7c8614a899e.png)

**贝尔曼方程**

![](img/0a2f927661280218bef6daca8a25b80d.png)

根据贝尔曼方程，现值等于当前报酬加上下一步的贴现(γ)值，遵循政策 **π** 。它也可以用 Q 值表示为:

![](img/ea3423f93f8d36218e14d402661802d1.png)

这是大多数强化学习算法中的理论核心。

# 预测与控制任务

强化学习有两个基本任务:预测和控制。

在预测任务中，我们被给定一个策略，我们的目标是通过估计遵循该策略采取行动的值或 Q 值来评估该策略。

在控制任务中，我们不知道策略，目标是找到让我们收集最多奖励的最优策略。在本文中，我们将只关注控制问题。

# RL 算法结构

下面是我做的一个图，用来可视化不同类型算法的高层结构。在接下来的几节中，我们将深入研究每种类型的复杂性。

![](img/be851bfadf3090c1445ae27bca208829.png)

# MDP 世界

在 MDP 世界中，我们有一个世界如何工作的心理模型，这意味着我们知道 MDP 动力学(跃迁 P(s'|s，a)和奖励函数 R(s，a))，所以我们可以直接使用贝尔曼方程建立一个模型。

同样，在控制任务中，我们的目标是找到一个给我们最大回报的策略。为了实现它，我们使用动态编程。

# 动态规划(迭代方法)

**1。策略迭代**

策略迭代本质上是重复执行两步直到收敛:策略评估和策略改进。

在策略评估步骤中，我们通过使用贝尔曼方程计算 Q 值来评估在状态 **s** 的策略 **π** :

![](img/6c1b55fe89398f8288448b8b90c03961.png)

在策略改进步骤中，我们通过贪婪地搜索在每个步骤中使 Q 值最大化的动作来更新策略。

![](img/238da3cddf80b951529c1c0fe34716a0.png)

让我们看看策略迭代是如何工作的。

![](img/314f6ff4f89039d066d23c63b9d77a4f.png)

**2。值迭代**

值迭代结合了策略迭代中的两个步骤，因此我们只需要更新 Q 值。我们可以将价值迭代解释为总是遵循贪婪的策略，因为在每一步，它总是试图找到并采取使价值最大化的行动。一旦价值收敛，就可以从价值函数中提取最优策略。

![](img/91b8072b498461d0d1ff56e5b17e59e6.png)

在大多数真实世界的情况下，我们不知道 MDP 动力学，所以迭代方法的应用是有限的。在下一节中，我们将转换话题，讨论可以处理未知世界的强化学习方法。

# 强化学习世界

在大多数情况下，MDP 动力学要么是未知的，要么在计算上无法直接使用，所以我们不是建立一个心理模型，而是从采样中学习。在下面所有的强化学习算法中，我们需要在环境中采取行动来收集奖励和估计我们的目标。

**勘探-开采困境**

在 MDP 模型中，我们可以在使用转移概率函数得出好的解决方案之前探索所有的潜在状态。然而，在过渡未知的强化学习中，如果我们继续贪婪地搜索最佳行动，我们可能会陷入几个状态，而无法探索整个环境。这就是探索-开发的困境。

为了走出次优状态，我们通常使用一种叫做**ε贪婪**的策略:当我们选择最佳行动时，有ε的概率我们可能会得到一个随机行动。

![](img/f6ad7b67e6d4be2a7a35c0945ac755d7.png)

# 基于模型的强化学习

估计 MDP 动力学的一种方法是抽样。遵循随机策略，我们对许多(s，a，r，s’)对进行采样，并使用蒙特卡罗(计算出现次数)从数据中显式地估计转移和奖励函数。如果数据足够大，我们的估计应该非常接近真实值。

![](img/0d62aef1dbde4f82895c6c8868686e63.png)

一旦我们有了估计，我们就可以使用迭代方法来寻找最优策略。

# 无模型强化学习(表格)

让我们后退一步。如果我们的目标只是找到好的政策，我们所需要的就是得到一个好的 q 估计。从这个角度来看，估计模型(过渡和奖励)只是达到目的的一种手段。为什么不直接切入正题，直接估算 Q 呢？

这就是所谓的无模型学习。

**1。无模型蒙特卡罗**

回想一下，当代理人从状态 s 采取行动 a 时，Q(s，a)是期望效用。

![](img/f5daf52f1e6c196accfc6199a8e73f41.png)

无模型蒙特卡罗的思路是对很多 rollouts 进行采样，用数据来估计 q，我们来看看算法。

![](img/52172deeffe6de95bc28213ae91364ea.png)

我们首先随机初始化所有东西，并使用 epsilon greedy 对一个动作进行采样，然后我们开始玩 rollouts。在每个卷展结束时，我们计算卷展中每个状态 St 的返回 Gt。为了得到 Q(st，at)，收益 Gt 的平均值，我们可以存储所有的收益，并在完成采样后更新 Q。然而，更有效的方法是在每个卷展结束时使用移动平均增量更新 Q，如下所示。

![](img/ccbaba2975aa6d93915436779ab1e8f4.png)

**2。萨尔萨**

SARSA 是一种时间差分(TD)方法，它结合了蒙特卡罗和动态规划方法。更新方程与蒙特卡洛的在线更新方程具有相似的形式，只是 SARSA 使用 rt + γQ(st+1，at+1)来替换来自数据的实际回报 Gt。N(s，a)也用参数α代替。

![](img/2279e699018f986e38b3a877530d84e6.png)

回想一下，在蒙特卡洛，我们需要等待该集结束，然后才能更新 Q 值。TD 方法的优点是，当我们移动一步并得到一个状态-动作对(st，at，rt，st+1，at+1)时，它们可以立即更新 Q 的估计。

![](img/fa9f454be2cfde8b047b6b34439da4c6.png)

**3。q-学习**

Q-learning 是另一种 TD 方法。SARSA 和 Q-learning 的区别在于，SARSA 是一个基于策略的模型，而 Q-learning 是基于策略的。在 SARSA 中，我们在状态 st 的返回是 rt + γQ(st+1，at+1)，其中 Q(st+1，at+1)是根据状态-动作对(st，at，rt，st+1，at+1)计算的，该状态-动作对是通过遵循策略π获得的。然而，在 Q-learning 中，Q(st+1，at+1)是通过采取最优行动获得的，这可能不一定与我们的策略相同。

![](img/8300599a7d77d394a791f008ce698885.png)

一般来说，策略上的方法更稳定，但是策略外的方法更可能找到全局最优。从下面我们可以看到，除了更新方程，算法的其他部分和 SARSA 一样。

![](img/4e60a33991deda8a77b8d48ab4f8db00.png)

# 价值函数近似(VFA)

到目前为止，我们一直假设我们可以将价值函数 V 或状态-动作价值函数 Q 表示为表格表示(向量或矩阵)。然而，许多现实世界的问题具有巨大的状态和/或动作空间，对于这些空间，表格表示是不够的。自然，我们可能想知道是否可以参数化值函数，这样我们就不必存储表了。

VFA 就是用参数化函数 Q hat 来表示 Q 值函数的方法。

![](img/d9167fd628f49b54ddced10c487c7126.png)

状态和动作被表示为特征向量 x(s，a ),并且估计的 Q hat 是线性预测器的分数。

![](img/08a0bc5ae5d2b2975afe0fe9645953c1.png)

目标是最小化估计 Q(预测)和实际 Q(目标)之间的损失，我们可以使用随机梯度下降来解决这个优化问题。

![](img/f0e51fcfd637ee65101c253257f45d7b.png)

我们如何得到我们的目标——目标函数中的真实 Q 值？

回想一下，Q 值是预期收益(Gt)，因此获得 Q 值的一种方法是使用蒙特卡罗:播放许多集并统计出现次数。

对于参数化蒙特卡罗，我们有以下目标函数和梯度:

![](img/134f4ce31446e54c77a5e47be1f11409.png)

另一种方法是利用 Q 值的递归表达式:Q(st，at) = rt + γQ(st+1，at+1)。正如我们前面讨论的，时间差分(TD)方法结合了蒙特卡罗和动态规划，并允许实时更新。因此，我们也可以使用 TD 方法获得目标 Q 值:SARSA 和 Q-learning。

萨尔萨:

![](img/d83e6708f700b0b990d85a45e2739d25.png)

q 学习

![](img/8ea7289aeb507095e9f51d8a22607572.png)

注意，在上面的 TD 方法中，我们实际上是使用模型预测来逼近真实的目标值 Q(st+1，at+1)，这种类型的优化称为半梯度。

# 深度 Q 网络(DQN)

如果我们有一套正确的功能，线性 VFA 通常工作得很好，这通常需要仔细的手工设计。一种替代方案是使用深度神经网络，该网络直接使用状态作为输入，而不需要特征的明确说明。

例如，在下图中，我们有一个神经网络，状态 s 作为输入层，2 个隐藏层，预测的 Q 值作为输出。这些参数可以通过反向传播来学习。

![](img/e51ffcd626e6616cf595925ae900bac0.png)

在 DQN 中有两个重要的概念:目标网和经验回放。

正如你可能已经意识到的，使用半梯度的一个问题是，模型更新可能非常不稳定，因为每次模型更新时，真实的目标都会改变。解决方案是创建一个**目标网络**，它以一定的频率复制训练模型，这样目标模型更新的频率就会降低。在下面的等式中，w-是目标网络的权重。

![](img/a4d508bfc82c04664186ce8920b2e017.png)

我们还创建了一个体验重放缓冲区，用于存储以前剧集中的(s，a，r，s’，a’)对。当我们更新权重时，我们从体验重放缓冲器中随机选择一个体验来运行随机梯度下降，而不是使用从剧集中生成的最新对。这将有助于避免过度拟合。

我以前实现了 DQN 与 Tensorflow 玩钢管舞游戏。如果您有兴趣了解更多关于实现的信息，请点击这里查看我的文章[。](/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998)

# 政策梯度

与之前对价值函数建模的算法不同，策略梯度方法通过将策略参数化为以下形式来直接学习策略:

![](img/aa8a426f5922122606f7ba6132e53948.png)

然而，当谈到优化时，我们仍然必须回到价值函数，因为策略函数本身不能用作目标函数。我们的目标可以表示为价值函数 V(θ)，它是我们从遵循随机策略π的轨迹τ中得到的期望总报酬。这里θ是策略的参数。注意不要将 V(θ)误解为参数化的值函数。

![](img/79d444a751fc692dff9a1b470a12455e.png)

其中τ是状态-动作轨迹:

![](img/7cd0bba43b18aaddf1ddcb4f51679a0e.png)

R(τ)是轨迹τ的奖励总和:

![](img/bbe90f8b7eddc6cd83e56781f6ef7a0b.png)

现在，目标是找到使值 V(θ)最大化的策略参数(θ)。为此，我们通过提升策略的梯度 w.r.t .参数θ来搜索 V(θ)中的最大值。

![](img/a9021ed8605d1743d9651e3c97f3be6d.png)

在如下所示的梯度中，策略ωθ通常使用 softmax、高斯或神经网络建模，以确保其可微分。

![](img/0b68f1a7d4b04c36be25b9502827ca15.png)

我们来看看政策梯度是什么样的。

![](img/e07f137518c4c5357bb59905143a58b2.png)

# 演员兼评论家

行动者-评论家方法不同于政策梯度方法，因为行动者-评论家方法估计政策和价值函数，并更新两者。在策略梯度方法中，我们使用 Gt 来更新θ，Gt 是单次部署中对 st 的价值函数的估计。虽然这个估计是无偏的，但它有很高的方差。

为了解决这个问题，行动者-批评家方法使用自举和函数逼近引入了偏差。我们用一个参数化的函数来估算价值，而不是使用展开中的 Gt，这就是批评家的用武之地。

这是“普通的”演员兼评论家政策梯度:

![](img/530a0b571790caa1569a760739e4eb1c.png)

在上述过程中，引入了两个新术语:优势(At)和基线(b(s))。

b(t)是状态 St 下的预期未来总报酬，相当于 V(St)，这是评论家估计的价值函数。

![](img/8504daa7f63942fd597c772109eca077.png)

在每一集之后，我们更新价值函数 b(s)和策略函数。与策略梯度中的不同，这里的策略函数用 at 而不是 Gt 更新，这有助于减少梯度的方差

除了普通的演员-评论家之外，还有两个流行的演员-评论家方法 A3C 和 A2C，它们用多个工作者更新策略和价值函数。他们的主要区别是 A3C 执行异步更新，而 A2C 是同步的。

# 结论和想法

在本文中，我们概述了许多经典和流行的强化学习算法，并讨论了它们在复杂性方面的异同。

值得一提的是，每个型号系列中都有很多我们没有涉及到的变体。例如，在 DQN 家族中，有决斗的 DQN 和双 DQN。在政策梯度上的影评世家，有 DDPG、宏碁、沈飞等。

此外，还有另一种 RL 方法:进化策略。受自然选择理论的启发，专家系统解决了目标函数没有精确解析形式的问题。因为它们超出了 MDP 的范围，所以我没有把它们包括在本文中，但是我可能会在以后的文章中讨论它们。敬请期待！:)

# 参考

斯坦福 CS234 课程笔记:[https://web.stanford.edu/class/cs234/slides/](https://web.stanford.edu/class/cs234/slides/)

莉莲的博客:【https://lilianweng.github.io/lil-log/】T2