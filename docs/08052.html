<html>
<head>
<title>Expectation-Maximization (EM) Algorithm: Solving a Chicken and Egg Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">期望最大化算法:解决一个先有鸡还是先有蛋的问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/solving-a-chicken-and-egg-problem-expectation-maximization-em-c717547c3be2?source=collection_archive---------17-----------------------#2020-06-14">https://towardsdatascience.com/solving-a-chicken-and-egg-problem-expectation-maximization-em-c717547c3be2?source=collection_archive---------17-----------------------#2020-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ebcf20515888ea8c6ffe0066863f1915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yftBrh4sfpFRo6ZHsIa3KQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者照片。</p></figure><h2 id="b8e9" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">流行的期望值最大化算法背后的直觉和示例代码</h2><p id="d58c" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">著名的 1977 年出版的期望最大化(EM)算法[1]是 20 世纪后期最重要的统计论文之一。EM 的目标不是简单地将分布模型拟合到数据，而是将模型拟合到数据的高级(即潜在)表示。本文的目的是描述 em 算法如何迭代计算两个相互依赖的未知数，这很像一个先有鸡还是先有蛋的问题。</p><p id="646f" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">EM 算法的主要优势是它能够在无监督的任务中处理未标记的数据，以及<a class="ae ly" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Proof_of_correctness" rel="noopener ugc nofollow" target="_blank">正确性证明</a>，这保证了最终将达到局部最大值或“足够好”的解决方案。它的实用性得到了各种应用的支持，包括无标签图像分割、无监督数据聚类、修复缺失数据(即插补)或发现原始数据的更高级(潜在)表示。</p><p id="de95" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">为了理解 EM 算法，我们将在无监督图像分割的环境中使用它。在这个例子中，我们的数据集是由像素集合组成的单个图像。每个基准点或像素都有三个特征，即 R、G 和 B 通道。例如，我们可以将<strong class="la ir">图 1 </strong>中的<em class="lz"> 321 x 481 x 3 </em>图像表示为<em class="lz"> 154401 x 3 </em>数据矩阵。我们的任务是聚集相关的像素。为了简单起见，让我们考虑一个聚类，其中<em class="lz"> k = 2 </em>，这意味着我们希望将每个像素分配到类 1 或类 2。我们的最终结果将看起来像图 1(右)的<strong class="la ir">。</strong></p><div class="ma mb mc md gt ab cb"><figure class="me jr mf mg mh mi mj paragraph-image"><img src="../Images/9f1974f65dd8bc45db8e433b567b454b.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*HqNpENlc624Z9H5Y2sg9GQ.png"/></figure><figure class="me jr mf mg mh mi mj paragraph-image"><img src="../Images/91651fcae281cc4a51e2795e73e3c89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*q2cTiRI8XWhzraPmIYdOqg.png"/><p class="jy jz gj gh gi ka kb bd b be z dk mk di ml mm translated">图一。来自 Berkeley 分割数据集[3]的原始图像(左)和使用基于 EM 算法的高斯混合模型(GMM)的聚类分配(右)。<strong class="bd mn">来源:</strong>作者照片。</p></figure></div><p id="3055" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">在这项任务中，EM 算法将用于拟合高斯混合模型(GMM ),以将图像分为两个部分。我们的 GMM 将使用两个(k=2)多元高斯分布的加权和来描述每个数据点，并将其分配给最可能的分布。本例中使用 EM 算法来计算多元高斯分布的参数以及混合权重。</p><h1 id="dc60" class="mo kd iq bd ke mp mq mr kh ms mt mu kk mv mw mx ko my mz na ks nb nc nd kw ne bi translated">第一节。期望最大化背后的核心思想——概率潜在表征</h1><p id="6af3" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">期望最大化(EM)算法的主要目标是计算数据的潜在表示，它捕捉数据的有用的、潜在的特征。使用概率方法，EM 算法计算数据的“软”或概率潜在空间表示。比如在<strong class="la ir">图 1 中。</strong>每个像素被分配一个属于类 0 和类 1 的概率。正如我们将在后面看到的，这些潜在空间表示反过来帮助我们提高对潜在统计模型的理解，这反过来帮助我们重新计算潜在空间表示，等等。</p><blockquote class="nf ng nh"><p id="2cb1" class="ky kz lz la b lb lt ld le lf lu lh li ni lv lk ll nj lw ln lo nk lx lq lr ls ij bi translated"><em class="iq">EM 算法的核心目标是在改进底层统计模型和更新数据的潜在表示之间交替进行，直到满足收敛标准。</em></p></blockquote><p id="cc07" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">通常，在描述 EM 算法和其他相关概率模型时，使用以下符号。</p><ul class=""><li id="f21f" class="nl nm iq la b lb lt lf lu kl nn kp no kt np ls nq nr ns nt bi translated"><strong class="la ir"> X: </strong>原始数据集其中| <strong class="la ir"> X </strong> | = <em class="lz"> (#数据点，#特征)</em>。x 可以是连续的或离散的(例如，原始 RGB 值在范围[0，255]内是连续的)。</li><li id="025a" class="nl nm iq la b lb nu lf nv kl nw kp nx kt ny ls nq nr ns nt bi translated"><strong class="la ir"> P(X|Z，θ): </strong>由<strong class="la ir"> Z </strong> where| <strong class="la ir"> P(X|Z，θ) </strong> | = <em class="lz"> (#数据点，#潜变量)指定的底层统计模型<strong class="la ir"> θ </strong>下的完全似然。</em>潜变量<strong class="la ir"> Z </strong>是离散的，规定了<strong class="la ir"> X </strong>属于哪个统计分布。在我们的示例中，<strong class="la ir"> X|Z，θ </strong>属于多元高斯模型，每个像素<em class="lz"> x </em>可以属于<strong class="la ir"> x|z=1，μ1，σ1</strong>或<strong class="la ir"> x|z=2，μ2，σ2</strong></li><li id="1d50" class="nl nm iq la b lb nu lf nv kl nw kp nx kt ny ls nq nr ns nt bi translated"><strong class="la ir"> P(Z=k): </strong>我们的潜在表示的先验概率<strong class="la ir"> Z </strong>或者随机采样数据点承担特定潜在分配的概率。Z 通常属于多项分布，或者在我们的例子中是更简单的二项分布。</li><li id="7a46" class="nl nm iq la b lb nu lf nv kl nw kp nx kt ny ls nq nr ns nt bi translated"><strong class="la ir"> θ: </strong>我们底层统计模型的参数。如果我们假设一个底层的多元高斯混合模型，那么<strong class="la ir"> θ </strong>既包括<strong class="la ir">σ</strong>和<strong class="la ir"> μ </strong>，也包括混合系数<strong class="la ir"> π </strong>，其中<strong class="la ir"> |π| </strong> =( <em class="lz"> #潜在变量)。</em>混合系数代表将潜在表示随机分配给数据点的概率，并且与<strong class="la ir"> P(Z=k) </strong>是相同的精确量。</li></ul><p id="e8a3" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">更清楚地说，在我们的例子中(假设一个 GMM 模型)<strong class="la ir"> X </strong>是<em class="lz">154401×3</em>像素矩阵，<strong class="la ir"> Z </strong>是<em class="lz">154401×1</em>聚类赋值，统计模型参数<strong class="la ir"> <em class="lz"> θ </em> </strong> <em class="lz"> = {μ1，σ1，μ2，σ2，π1，π2} </em>其中<em class="lz"> |μ_i| = 3</em></p><h1 id="8691" class="mo kd iq bd ke mp mq mr kh ms mt mu kk mv mw mx ko my mz na ks nb nc nd kw ne bi translated">第二节。理解期望最大化</h1><p id="5e90" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">现在我们有了一个具体的例子，让我们把 EM 算法的定义分解为<em class="lz">“更新未观察到的潜在空间变量以找到统计模型参数的局部最大似然估计的迭代方法”[2]。</em></p><h2 id="e3e3" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">第一部分。设置:“迭代方法…”</h2><p id="c58b" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">EM 算法有三个主要步骤:初始化步骤、期望值步骤和最大化步骤。在第一步中，统计模型参数<strong class="la ir"> θ </strong>被随机初始化或者通过使用 k 均值方法初始化。初始化后，EM 算法在 E 和 M 步之间迭代，直到收敛。</p><h2 id="2098" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">第二部分。<strong class="ak"><em class="nz">…</em></strong><em class="nz">即更新未观察到的潜在空间变量</em><strong class="ak"><em class="nz">…</em></strong></h2><p id="6985" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">E-step 用于更新未观察到的潜在空间变量<strong class="la ir"> Z </strong>并为更新统计模型的参数<strong class="la ir"> θ </strong>设置阶段。电子步骤可以分为两个部分。第一部分更新我们的条件分布<strong class="la ir"> P(Z|X，θ) </strong>，它代表我们数据的软潜在分配(计算<em class="lz">见<em class="lz">附录“软潜在分配】</em>)。</em>E 步的第二部分计算<strong class="la ir"> Q(θ，θ*) </strong>其中<strong class="la ir"> θ* </strong>代表统计模型的先前参数<strong class="la ir"> θ </strong>代表潜在的新参数。</p><blockquote class="nf ng nh"><p id="49b5" class="ky kz lz la b lb lt ld le lf lu lh li ni lv lk ll nj lw ln lo nk lx lq lr ls ij bi translated"><em class="iq">我们统计模型</em> <strong class="la ir"> <em class="iq"> θ* </em> </strong> <em class="iq">的当前值和数据</em> <strong class="la ir"> <em class="iq"> X </em> </strong> <em class="iq">用于计算软潜在赋值</em> <strong class="la ir"> <em class="iq"> P(Z|X，θ*)。</em>T77】</strong></p></blockquote><p id="a84e" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir"> Q(θ，θ*) </strong>背后的直觉大概是 EM 算法中最令人困惑的部分。它的包含最终导致计算时间和最优性之间的折衷。解释一下，EM 算法的缺点是它只能保证找到<strong class="la ir"> θ </strong>的估计，该估计找到了似然<strong class="la ir"> P(X|θ) </strong>的局部最大值，而不一定是绝对最大值。在理想情况下，真正的最大似然估计可以通过最大化似然<strong class="la ir"> P(X，Z|θ) </strong>来找到；然而，最大化这个值是困难的，因为我们需要对<strong class="la ir"> Z </strong>进行边缘化，以便获得可能性<strong class="la ir"> P(X|θ) </strong>(有关更多详细信息，请参见<em class="lz">附录“对数可能性的棘手性”</em>)。相反，EM 算法使<strong class="la ir"> Q(θ，θ*) </strong>最大化，这与<strong class="la ir"> P(X|θ) </strong>相关，但更容易优化。</p><blockquote class="nf ng nh"><p id="b45c" class="ky kz lz la b lb lt ld le lf lu lh li ni lv lk ll nj lw ln lo nk lx lq lr ls ij bi translated"><em class="iq">更具体地说</em> <strong class="la ir"> Q(θ，θ*) </strong> <em class="iq">是完全对数似然</em> <strong class="la ir"> <em class="iq"> log[ </em> P(X|Z，θ)】</strong><em class="iq">相对于</em> <strong class="la ir"> <em class="iq"> Z </em> </strong> <em class="iq">的当前分布给定</em> <strong class="la ir"> <em class="iq"> X </em> </strong> <em class="iq">和</em> <strong class="la ir">的当前估计</strong> <em class="iq">换句话说，它是相对于先前计算的软分配</em> <strong class="la ir"> <em class="iq"> Z|X，θ* </em> </strong>的完全对数似然的期望。</p></blockquote><p id="64fa" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">要理解我们为什么需要<strong class="la ir"> Q(θ，θ*) </strong>，想想这个。我们需要找到最佳的<strong class="la ir"> θ </strong>来最大化<strong class="la ir"> P(X，Z |θ)；</strong>然而，我们无法合理地对每个数据点的所有<strong class="la ir"> Z </strong>求和。然而，假设 Z 被神奇地知道了。那么这个问题可以完全避免，因为<strong class="la ir"> P(X，Z|θ) </strong>会变成<strong class="la ir"> P(X|Z，θ) </strong>。但是，明显的问题是<strong class="la ir"> Z </strong>一开始并不知道。为了解决这个问题，我们计算<strong class="la ir"> <em class="lz"> P(Z|X，θ*) </em> </strong>以提供对<strong class="la ir"> Z </strong>的软估计，并以<strong class="la ir"> <em class="lz"> Z|X，θ* </em> </strong>为条件，取完整对数似然的期望，填入<strong class="la ir">Z。</strong>换句话说，我们以<strong class="la ir"> P(X|Z，θ) </strong>为条件</p><p id="fefb" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir"> Q(θ，θ*) </strong>的简化版如下图所示(详见<em class="lz">附录“</em>计算 Q(θ，θ*)”)。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/d3a7bf8bce6c90e2fec446d4d79c57b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*ZCQVsT9Q8wMcCifT4wv8aQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">E-step 用于寻找 Q(θ，θ*)，这是在先前的统计模型参数<strong class="bd mn"> θ* </strong>和数据<strong class="bd mn"> X </strong>的条件下，相对于<strong class="bd mn"> Z </strong>的完全对数似然的期望。</p></figure><h2 id="24f3" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">第 3 部分:“… <em class="nz">求统计模型参数的局部极大似然估计(MLE)。</em></h2><p id="1af4" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">与 E-step 相比，M-step 非常简单，用于更新我们统计模型的参数<strong class="la ir"> θ </strong>。一旦我们计算出<strong class="la ir"> Q(θ，θ*) </strong>我们可以通过计算以下表达式来改进统计模型参数的 MLE:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/51c0e0e061df0276ef685a169c3afb9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*FHiYNkL5iP5JGBRN4nwJYg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">更新统计模型的 M 步操作。</p></figure><h2 id="817a" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">第四部分。收敛！</strong></h2><p id="4926" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">收敛标准很简单——将每个新计算的<strong class="la ir"> Q(θ，θ*) </strong>与之前的进行比较，如果差值小于某个阈值ϵ(例如，ϵ= 1e-4 ), em 算法终止。在实践中，您可能希望使用各种初始化<strong class="la ir"> θ </strong>来运行该算法几次，以找到最大化<strong class="la ir"> P(X|Z，θ) </strong>的参数，因为每次 EM 算法执行时，您只能保证找到局部最大似然估计。</p><h1 id="46c9" class="mo kd iq bd ke mp mq mr kh ms mt mu kk mv mw mx ko my mz na ks nb nc nd kw ne bi translated">第三节。使用 EM 参数化高斯混合模型的 Python 示例</h1><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="oc od l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="ak">代码片段:</strong>高斯混合模型(GMM)的一个例子，它使用期望最大化来聚类图像。</p></figure><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/93a314aa4adc85e1001972630f2719f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*XRu60TjjySVPfNwLk5W8zQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mn">图二。</strong>使用 EM 参数化高斯混合模型(k=2)对来自伯克利分割数据集[3]的图像进行示例分割。<strong class="bd mn">来源:</strong>作者照片。</p></figure><h1 id="fc81" class="mo kd iq bd ke mp mq mr kh ms mt mu kk mv mw mx ko my mz na ks nb nc nd kw ne bi translated">第四节。EM 的三个重要直觉</h1><p id="b24d" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">总之，在<strong class="la ir"> Q(θ，θ*) </strong>和最终的 EM 算法<strong class="la ir">背后有三个重要的直觉。</strong>首先，完全对数似然<strong class="la ir"> P(X|Z，θ) </strong>比对数似然<strong class="la ir"> P(X，Z|θ) </strong>更快最大化，因为没有超过<strong class="la ir"> Z </strong>的边缘化。但是，这就引入了一个问题，因为我们不知道<strong class="la ir"> Z </strong>。因此，第二个直觉是，我们可以改为最大化<strong class="la ir"> Q(θ，θ*) </strong>或<strong class="la ir"> P(X，|Z，θ) </strong>的对数的期望值，其中通过调节对<strong class="la ir"> Z|X，θ* </strong>的期望来填充<strong class="la ir"> Z </strong>。最后的直觉是，通过找到最大化<strong class="la ir"> Q(θ，θ*) </strong>的参数<strong class="la ir"> θ </strong>，我们将更接近最大化可能性<strong class="la ir"> P(X，Z|θ)的解决方案。</strong>虽然并非微不足道，但对这种正确性的<a class="ae ly" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Proof_of_correctness" rel="noopener ugc nofollow" target="_blank">证明</a>表明，提高<strong class="la ir"> Q(θ，θ*) </strong>会导致<strong class="la ir"> P(X，Z|θ) </strong>提高至少一样多，如果不是更多的话。因此，EM 算法将总是收敛到局部最大值。</p><h1 id="75d1" class="mo kd iq bd ke mp mq mr kh ms mt mu kk mv mw mx ko my mz na ks nb nc nd kw ne bi translated">第五节。最近的工作和结论</h1><p id="dd9e" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">最初由 Dempster、Laird 和 Rubin [1]描述的期望最大化(EM)算法提供了一种有保证的方法来计算依赖于未知或未观测数据的统计模型的局部最大似然估计(MLE)。尽管当数据集很大时，它的执行速度会很慢；收敛的保证和算法以无人监督的方式工作的能力使它在各种任务中有用。最近，人们开始使用神经网络，主要是编码器-解码器架构，其中编码器用于将 X 重新参数化为高级变量 X*，解码器封装统计参数θ*，产生每像素的软分配 P(Z|X*，θ*)。使用典型的编码器-解码器的损失函数来训练网络，但是用 P(Z|X*，θ*)来加权。这种方法被称为神经期望最大化(N-EM) [4]，虽然有用，但它失去了 EM 的收敛保证。此外，还不清楚这种方法是否从图像中提取了更多相似颜色的特征，为改进和进一步研究留下了足够的空间。</p><p id="075a" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">人工智能领域的一个令人兴奋的挑战将是开发从原始感觉数据中可靠地提取离散实体的方法，这是人类感知和组合概括的核心[5]。期望最大化，虽然不是什么新东西，但它提供了一个镜头，未来寻求解决这个问题的技术应该通过这个镜头来审视。</p><h1 id="896a" class="mo kd iq bd ke mp mq mr kh ms mt mu kk mv mw mx ko my mz na ks nb nc nd kw ne bi translated"><strong class="ak">附录</strong></h1><h2 id="f269" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">A.软潜在分配计算</h2><p id="55f5" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">在期望步骤(E-step)中计算软分配，以更新我们的潜在空间表示。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/d55136e0e26a2a0b5faa95baf61d277a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qzAwcLHTzdWT2Wr58aOUFg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mn">补充 1: </strong>使用贝叶斯法则和全概率法则导出的潜在空间变量计算的软赋值。</p></figure><h2 id="5b82" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak"> B .解释对数似然法的难解性</strong></h2><p id="1713" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">通常，统计模型的最佳参数通过找到使对数似然最大化的<strong class="la ir"> θ </strong>或<strong class="la ir"> log[P(X|θ)] </strong>来拟合数据。假设是独立的，这通常如下所示:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/827bffd3fd5658483a7f44f060981164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*9TC9lDBZ23hbr4h_UQ8o1w.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mn">等式 1: </strong>典型的最大似然估计(MLE)设置。</p></figure><p id="eba1" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">然而，由于对<strong class="la ir"> Z </strong>的依赖性，我们只能计算<strong class="la ir"> P(X，Z|θ) </strong>，因此，为了计算<strong class="la ir"> P(X|θ) </strong>，我们必须忽略<strong class="la ir"> Z </strong>并最大化以下内容:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/31f64cb30eb843a90b0fa2eb408bf225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*Mw9yEPfV2-sD5pmTTi9fMQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mn">方程式 2。</strong>对数似然法。</p></figure><p id="2620" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">这个数量更难以最大化，因为我们必须对所有的<em class="lz"> n </em>个数据点的潜在变量<strong class="la ir"> Z </strong>进行边缘化或求和。</p><h2 id="1007" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><em class="nz">计算 Q(θ，θ*) </em></h2><p id="69dd" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">不是最大化<strong class="la ir">等式 2 </strong>中的对数似然，而是最大化完整数据的对数似然，首先假设对于每个数据点<em class="lz"> x_i </em>，我们有一个已知的离散潜在分配<em class="lz"> z_i </em>。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/c36a4e7eeaf6a3d5218d854a1a523655.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*vlheBmJ3wkLsIRPuEPc5rg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mn">方程式 3。</strong>完全对数似然</p></figure><p id="c63d" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">好消息是，不像<strong class="la ir">等式 2。</strong>我们不再需要在<strong class="la ir">等式 3 中对 Z 求和。</strong>然而，坏消息是我们不知道<em class="lz"> z_i </em>。为了解决这个问题，我们尝试通过最大化<strong class="la ir"> Q(θ，θ*) </strong>或相对于<strong class="la ir"> Z|X，θ* </strong>的完全对数似然的期望来猜测<em class="lz"> z_i </em>，这允许我们填充<em class="lz"> z_i. </em>的值</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/58989908fc7602a8cb19e545a678f548.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*tucNXsQMma50Zs36DmDnXw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mn">方程式 4。</strong>关于当前条件分布的完全对数似然的期望值<strong class="bd mn"> Z|X，θ*。</strong></p></figure><p id="75ad" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">等式 4 </strong>可以简化为</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ok"><img src="../Images/2816ceaa504c94662581325d7eb28c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*IT5O43GoWcw9L2NGvq5U5w.png"/></div></div></figure><p id="aa46" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">其中<strong class="la ir"> I </strong>是<em class="lz">指标</em>函数，可以用来评估期望值，因为我们假设<em class="lz"> z_i </em>是离散的。最终，上面的等式可以简化为</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/f1cea7afcf21c18be6d7ce544d777a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*v7A9l0VjcE3c1IZz3HlQ9w.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mn">方程式 5。</strong>简化版的 Q(θ，θ*)</p></figure><p id="ff12" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">方程式 5。</strong>最后显示了<strong class="la ir"> Q(θ，θ*) </strong>的有用性，因为与<strong class="la ir">等式 3 不同，</strong>求和中没有任何一项以<strong class="la ir"> Z </strong>和<strong class="la ir"> θ为条件。</strong>在上面的等式中，最左边的项是软潜在分配，最右边的项是<strong class="la ir"> Z </strong>的先验和条件概率密度函数的对数乘积。最右边的项可以分成两项，以最大化混合权重(<strong class="la ir"> Z </strong>的先验)和概率密度函数的分布参数</p><p id="325e" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">回到具体的 GMM 例子，虽然在上面的<strong class="la ir">等式 5 中可能不明显。</strong>、<em class="lz"> {μ1，σ1 }、{μ2，σ2 }和{π1，π2} </em>出现在不同的项中，可以使用各自分布的已知最大似然估计独立地最大化。例如，当更新<em class="lz"> {μ1，σ1 }</em>和<em class="lz"> {μ2，σ2 }</em>时，可以使用高斯分布的最大似然估计，而对于<em class="lz"> {π1，π2} </em>可以使用二项式分布的最大似然估计。这些更新与经典 MLE 方程的唯一区别是包含了加权项<strong class="la ir"> P(Z|X，θ*) </strong>。</p><h1 id="6a92" class="mo kd iq bd ke mp mq mr kh ms mt mu kk mv mw mx ko my mz na ks nb nc nd kw ne bi translated"><strong class="ak">引文</strong></h1><p id="b44f" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">[1]登普斯特，A.P 新墨西哥州莱尔德；鲁宾博士(1977 年)。“通过 EM 算法不完整数据的最大可能性”。皇家统计学会杂志，B 辑。39: 1–38.</p><p id="ddd2" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">[2]“期望最大化算法”，维基百科文章，<a class="ae ly" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Expectation % E2 % 80% 93 最大化 _ 算法</a></p><p id="f650" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">[3]李辉，蔡剑飞，阮氏一青，.语义图像分割的基准。IEEE ICME 2013。</p><p id="ff05" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">[4]格雷夫、克劳斯、斯约尔德·范·斯廷基斯特和于尔根·施密德胡伯。“神经期望最大化。”<em class="lz">神经信息处理系统的进展</em>。2017.</p><p id="68b7" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">[5]巴塔格利亚、彼得·w 等，“关系归纳偏差、深度学习和图形网络”<em class="lz"> arXiv 预印本 arXiv:1806.01261 </em> (2018)。</p></div></div>    
</body>
</html>