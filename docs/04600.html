<html>
<head>
<title>PyTorch [Vision] — Binary Image Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">py torch[视觉] —二值图像分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-vision-binary-image-classification-d9a227705cf9?source=collection_archive---------8-----------------------#2020-04-24">https://towardsdatascience.com/pytorch-vision-binary-image-classification-d9a227705cf9?source=collection_archive---------8-----------------------#2020-04-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/840b4e62b667c78ce2f768c8b11e4488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Dsdw-L4qVhT1WkyLvtsPg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">如何训练你的神经网络[图片[0]]</p></figure><h2 id="10bb" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/akshaj-wields-pytorch" rel="noopener">如何训练你的神经网络</a></h2><div class=""/><div class=""><h2 id="96ba" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">本笔记本将带您使用 PyTorch 上的热狗/非热狗数据集，通过 CNN 实现二值图像分类。</h2></div><h1 id="26de" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">导入库</h1><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="44d2" class="me le jf ma b gy mf mg l mh mi">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from tqdm.notebook import tqdm<br/>import matplotlib.pyplot as plt<br/></span><span id="b791" class="me le jf ma b gy mj mg l mh mi">import torch<br/>import torchvision<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>import torch.nn.functional as F<br/>from torchvision import transforms, utils, datasets<br/>from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler<br/></span><span id="300c" class="me le jf ma b gy mj mg l mh mi">from sklearn.metrics import classification_report, confusion_matrix</span></pre><p id="c22e" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">设置随机种子。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="e51f" class="me le jf ma b gy mf mg l mh mi">np.random.seed(0)<br/>torch.manual_seed(0)</span></pre><p id="b817" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">设置<code class="fe ng nh ni ma b">Seaborn</code>样式。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="93b1" class="me le jf ma b gy mf mg l mh mi">%matplotlib inline<br/>sns.set_style('darkgrid')</span></pre><h1 id="b8cd" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">定义路径并设置 GPU</h1><p id="6e2e" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">让我们定义数据的路径。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="94dc" class="me le jf ma b gy mf mg l mh mi">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/>print("We're using =&gt;", device)</span><span id="214e" class="me le jf ma b gy mj mg l mh mi">root_dir = "../../../data/computer_vision/image_classification/hot-dog-not-hot-dog/"<br/>print("The data lies here =&gt;", root_dir)</span><span id="2023" class="me le jf ma b gy mj mg l mh mi"><br/>###################### OUTPUT ######################</span><span id="548f" class="me le jf ma b gy mj mg l mh mi">We're using =&gt; cuda<br/>The data lies here =&gt; ../../../data/computer_vision/image_classification/hot-dog-not-hot-dog/</span></pre><h1 id="b6d8" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">定义转换</h1><p id="a05c" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">让我们定义一个字典来保存训练/测试集的图像转换。我们将调整所有图像的大小为大小(224，224 ),并将图像转换为张量。</p><p id="6c0b" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">PyTorch 中的<code class="fe ng nh ni ma b">ToTensor</code>操作将所有张量转换到(0，1)之间。</p><blockquote class="no np nq"><p id="73b6" class="mk ml nr mm b mn mo kp mp mq mr ks ms ns mu mv mw nt my mz na nu nc nd ne nf ij bi translated"><code class="fe ng nh ni ma b"><em class="jf">ToTensor</em></code> <em class="jf">将范围[0，255]内的 PIL 图像或 numpy.ndarray (H x W x C)转换为火炬。形状(C x H x W)在[0.0，1.0] </em>范围内的浮点数</p></blockquote><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="fc43" class="me le jf ma b gy mf mg l mh mi">image_transforms = {<br/>    "train": transforms.Compose([<br/>        transforms.Resize((224, 224)),<br/>        transforms.ToTensor()<br/>    ]),<br/>    "test": transforms.Compose([<br/>        transforms.Resize((224, 224)),<br/>        transforms.ToTensor()<br/>}</span></pre><h1 id="8c46" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">初始化数据集</h1><h2 id="4f98" class="me le jf bd lf nv nw dn lj nx ny dp ln mt nz oa lp mx ob oc lr nb od oe lt jl bi translated">训练+验证数据集</h2><p id="d3a2" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">我们随身带着 2 个数据集文件夹— <strong class="mm jp">训练</strong>和<strong class="mm jp">测试</strong>。</p><p id="2b47" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">我们将进一步划分我们的<strong class="mm jp">火车</strong>集合为<strong class="mm jp">火车+ Val </strong>。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="7426" class="me le jf ma b gy mf mg l mh mi">hotdog_dataset = datasets.ImageFolder(root = root_dir + "train",<br/>                                      transform = image_transforms["train"]<br/>                                     )</span><span id="1d5b" class="me le jf ma b gy mj mg l mh mi">hotdog_dataset</span><span id="08a1" class="me le jf ma b gy mj mg l mh mi"><br/>###################### OUTPUT ######################</span><span id="4fbf" class="me le jf ma b gy mj mg l mh mi">Dataset ImageFolder<br/>    Number of datapoints: 498<br/>    Root location: ../../../data/computer_vision/image_classification/hot-dog-not-hot-dog/train<br/>    StandardTransform<br/>Transform: Compose(<br/>               Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)<br/>               ToTensor()<br/>           )</span></pre><h2 id="1a21" class="me le jf bd lf nv nw dn lj nx ny dp ln mt nz oa lp mx ob oc lr nb od oe lt jl bi translated">输出的类&lt;=&gt; ID 映射</h2><p id="f09e" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">PyTorch 内置了<code class="fe ng nh ni ma b">class_to_idx</code>功能。它返回数据集中的类 ID。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="a545" class="me le jf ma b gy mf mg l mh mi">hotdog_dataset.class_to_idx</span><span id="8f80" class="me le jf ma b gy mj mg l mh mi"><br/>###################### OUTPUT ######################</span><span id="a500" class="me le jf ma b gy mj mg l mh mi">{'hot_dog': 0, 'not_hot_dog': 1}</span></pre><p id="4840" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">我们现在将构建该字典的反向；ID 到类的映射。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="0c59" class="me le jf ma b gy mf mg l mh mi">idx2class = {v: k for k, v in hotdog_dataset.class_to_idx.items()}</span></pre><p id="6e1c" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">让我们还编写一个函数，它接受一个 dataset 对象并返回一个包含类样本计数的字典。我们将使用这个字典来构建图，并观察我们的数据中的类分布。</p><p id="963a" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated"><code class="fe ng nh ni ma b">get_class_distribution()</code>接受了一个名为<code class="fe ng nh ni ma b">dataset_obj</code>的论点。</p><ul class=""><li id="cd45" class="of og jf mm b mn mo mq mr mt oh mx oi nb oj nf ok ol om on bi translated">我们首先初始化一个<code class="fe ng nh ni ma b">count_dict</code>字典，其中所有类的计数都被初始化为 0。</li><li id="49ae" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">然后，让我们遍历数据集，并为循环中遇到的每个类标签将计数器加 1。</li></ul><p id="b65b" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated"><code class="fe ng nh ni ma b">plot_from_dict()</code>接受 3 个参数:一个名为<code class="fe ng nh ni ma b">dict_obj</code>、<code class="fe ng nh ni ma b">plot_title</code>和<code class="fe ng nh ni ma b">**kwargs</code>的字典。我们传入<code class="fe ng nh ni ma b">**kwargs</code>是因为稍后，我们将构建需要在 Seaborn 中传递<code class="fe ng nh ni ma b">ax</code>参数的支线剧情。</p><ul class=""><li id="6f7c" class="of og jf mm b mn mo mq mr mt oh mx oi nb oj nf ok ol om on bi translated">首先将字典转换为数据框。</li><li id="0097" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">融合数据框和绘图。</li></ul><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="0a27" class="me le jf ma b gy mf mg l mh mi">def get_class_distribution(dataset_obj):<br/>    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}</span><span id="2985" class="me le jf ma b gy mj mg l mh mi">    for _, label_id in dataset_obj:<br/>        label = idx2class[label_id]<br/>        count_dict[label] += 1<br/>    return count_dict<br/></span><span id="8832" class="me le jf ma b gy mj mg l mh mi">def plot_from_dict(dict_obj, plot_title, **kwargs):<br/>    return sns.barplot(data = pd.DataFrame.from_dict([dict_obj]).melt(), x = "variable", y="value", hue="variable", **kwargs).set_title(plot_title)</span><span id="b85a" class="me le jf ma b gy mj mg l mh mi">plt.figure(figsize=(15,8))<br/>plot_from_dict(get_class_distribution(hotdog_dataset), plot_title="Entire Dataset (before train/val/test split)")</span></pre><figure class="lv lw lx ly gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ot"><img src="../Images/62f158ac2efcdbc7a3864775ba705829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45U7wU6mWU2QYh3jLdbKxg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">整个数据集上的类分布[图像[1]]</p></figure><h1 id="4b76" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">获取训练和验证样本</h1><p id="9cfd" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">我们使用<code class="fe ng nh ni ma b">SubsetRandomSampler</code>来制作我们的训练和验证加载器。<code class="fe ng nh ni ma b">SubsetRandomSampler</code>用于使每批接收一个随机分布的类。</p><p id="82a7" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">我们也可以将我们的数据集分成两部分— train 和 val ie。制作 2 个<code class="fe ng nh ni ma b">Subsets</code>。但是这更简单，因为我们的数据加载器现在几乎可以处理所有的事情。</p><p id="8471" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated"><code class="fe ng nh ni ma b">SubsetRandomSampler(indices)</code>将数据的索引作为输入。</p><p id="2c81" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">我们首先创建我们的采样器，然后将它传递给我们的数据加载器。</p><ul class=""><li id="a279" class="of og jf mm b mn mo mq mr mt oh mx oi nb oj nf ok ol om on bi translated">创建索引列表。</li><li id="344b" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">打乱索引。</li><li id="23e2" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">根据列车价值百分比拆分指数。</li><li id="5b04" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">创造<code class="fe ng nh ni ma b">SubsetRandomSampler</code>。</li></ul><p id="a2d5" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">创建从 0 到数据集长度的索引列表。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="e0d6" class="me le jf ma b gy mf mg l mh mi">hotdog_dataset_size = len(hotdog_dataset)<br/>hotdog_dataset_indices = list(range(hotdog_dataset_size))</span></pre><p id="ace9" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">使用 np.shuffle 打乱索引列表。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="d975" class="me le jf ma b gy mf mg l mh mi">np.random.shuffle(hotdog_dataset_indices)</span></pre><p id="cb4e" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">创建拆分索引。我们选择拆分索引为数据集大小的 20% (0.2)。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="dfa7" class="me le jf ma b gy mf mg l mh mi">val_split_index = int(np.floor(0.2 * hotdog_dataset_size))</span></pre><p id="b178" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">将列表切片以获得 2 个索引列表，一个用于训练，另一个用于测试。</p><blockquote class="ou"><p id="205b" class="ov ow jf bd ox oy oz pa pb pc pd nf dk translated"><code class="fe ng nh ni ma b">0-----------val_split_index------------------------------n</code></p><p id="836d" class="ov ow jf bd ox oy oz pa pb pc pd nf dk translated">train = &gt;<strong class="ak">val _ split _ index</strong>至<strong class="ak"> n </strong></p><p id="f86b" class="ov ow jf bd ox oy oz pa pb pc pd nf dk translated">Val =&gt; <strong class="ak"> 0 </strong>至<strong class="ak"> val_split_index </strong></p></blockquote><pre class="pe pf pg ph pi lz ma mb mc aw md bi"><span id="ede6" class="me le jf ma b gy mf mg l mh mi">train_idx, val_idx = hotdog_dataset_indices[val_split_index:], hotdog_dataset_indices[:val_split_index]</span></pre><p id="1a00" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">最后，创建采样器。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="f7cc" class="me le jf ma b gy mf mg l mh mi">train_sampler = SubsetRandomSampler(train_idx)<br/>val_sampler = SubsetRandomSampler(val_idx)</span></pre><h1 id="4bf8" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">试验</h1><p id="8914" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">既然我们已经完成了训练和赋值数据，让我们加载测试数据集。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="5920" class="me le jf ma b gy mf mg l mh mi">hotdog_dataset_test = datasets.ImageFolder(root = root_dir + "test",<br/>                                            transform = image_transforms["test"]<br/>                                           )</span><span id="2f27" class="me le jf ma b gy mj mg l mh mi">hotdog_dataset_test</span><span id="84e1" class="me le jf ma b gy mj mg l mh mi"><br/>###################### OUTPUT ######################</span><span id="ab12" class="me le jf ma b gy mj mg l mh mi">Dataset ImageFolder<br/>    Number of datapoints: 500<br/>    Root location: ../../../data/computer_vision/image_classification/hot-dog-not-hot-dog/test<br/>    StandardTransform<br/>Transform: Compose(<br/>               Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)<br/>               ToTensor()<br/>           )</span></pre><h1 id="378b" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">培训、验证和测试数据加载器</h1><p id="2a22" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">现在，我们将把采样器传递给我们的数据加载器。请注意，当您使用<code class="fe ng nh ni ma b">SubsetRandomSampler</code>时，不能使用<code class="fe ng nh ni ma b">shuffle=True</code>。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="6027" class="me le jf ma b gy mf mg l mh mi">train_loader = DataLoader(dataset=hotdog_dataset, shuffle=False, batch_size=8, sampler=train_sampler)</span><span id="fa72" class="me le jf ma b gy mj mg l mh mi">val_loader = DataLoader(dataset=hotdog_dataset, shuffle=False, batch_size=1, sampler=val_sampler)</span><span id="5b4d" class="me le jf ma b gy mj mg l mh mi">test_loader = DataLoader(dataset=<!-- -->hotdog_dataset_test<!-- -->, shuffle=False, batch_size=1)</span></pre><h1 id="f67d" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">探索数据</h1><p id="7807" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">为了研究我们 train 和 val 数据加载器，让我们创建一个新函数，它接收一个数据加载器并返回一个包含类计数的字典。</p><ul class=""><li id="5564" class="of og jf mm b mn mo mq mr mt oh mx oi nb oj nf ok ol om on bi translated">将字典<code class="fe ng nh ni ma b">count_dict</code>初始化为全 0。</li><li id="3431" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">如果<code class="fe ng nh ni ma b">dataloader_obj</code>的 batch_size 为 1，则循环通过<code class="fe ng nh ni ma b">dataloader_obj</code>并更新计数器。</li><li id="8fd1" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">否则，如果<code class="fe ng nh ni ma b">dataloader_obj</code>的 batch_size 是<strong class="mm jp">而不是</strong> 1，则循环通过<code class="fe ng nh ni ma b">dataloader_obj</code>获得批次。循环遍历批以获得单个张量。现在，相应地更新了计数器。</li></ul><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="8569" class="me le jf ma b gy mf mg l mh mi">def get_class_distribution_loaders(dataloader_obj, dataset_obj):<br/>    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}</span><span id="df3c" class="me le jf ma b gy mj mg l mh mi">    if dataloader_obj.batch_size == 1:    <br/>        for _,label_id in dataloader_obj:<br/>            y_idx = label_id.item()<br/>            y_lbl = idx2class[y_idx]<br/>            count_dict[str(y_lbl)] += 1<br/>    else: <br/>        for _,label_id in dataloader_obj:<br/>            for idx in label_id:<br/>                y_idx = idx.item()<br/>                y_lbl = idx2class[y_idx]<br/>                count_dict[str(y_lbl)] += 1<br/></span><span id="693a" class="me le jf ma b gy mj mg l mh mi">    return count_dict</span></pre><p id="8cb7" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">为了绘制类分布，我们将使用前面定义的带有<code class="fe ng nh ni ma b">ax</code>参数的<code class="fe ng nh ni ma b">plot_from_dict()</code>函数。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="231b" class="me le jf ma b gy mf mg l mh mi">fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,7))</span><span id="6b6e" class="me le jf ma b gy mj mg l mh mi">plot_from_dict(get_class_distribution_loaders(train_loader, hotdog_dataset), plot_title="Train Set", ax=axes[0])</span><span id="7d36" class="me le jf ma b gy mj mg l mh mi">plot_from_dict(get_class_distribution_loaders(val_loader, hotdog_dataset), plot_title="Val Set", ax=axes[1])</span></pre><figure class="lv lw lx ly gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pj"><img src="../Images/321beb4b43928fce1b68fe975e4e575d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lgd_KszOPYtnGZSg03x_Lw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">训练和值集的类分布[图像[2]]</p></figure><p id="3417" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">既然我们已经看了类分布，现在让我们看一个单一的图像。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="a481" class="me le jf ma b gy mf mg l mh mi">single_batch = next(iter(train_loader))</span></pre><p id="93ac" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated"><code class="fe ng nh ni ma b">single_batch</code>是 2 个元素的列表。第一个元素(第 0 个索引)包含图像张量，而第二个元素(第 1 个索引)包含输出标签。</p><p id="e1cc" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">这是列表的第一个元素，它是一个张量。这个张量的形状是<code class="fe ng nh ni ma b">(batch, channels, height, width)</code>。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="30fc" class="me le jf ma b gy mf mg l mh mi">single_batch[0].shape</span><span id="fd0b" class="me le jf ma b gy mj mg l mh mi"><br/>###################### OUTPUT ######################</span><span id="2950" class="me le jf ma b gy mj mg l mh mi">torch.Size([8, 3, 224, 224])</span></pre><p id="f84e" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">这是该批次的输出标签。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="90fe" class="me le jf ma b gy mf mg l mh mi">print("Output label tensors: ", single_batch[1])<br/>print("\nOutput label tensor shape: ", single_batch[1].shape)</span><span id="f3e6" class="me le jf ma b gy mj mg l mh mi"><br/>###################### OUTPUT ######################</span><span id="8cb2" class="me le jf ma b gy mj mg l mh mi">Output label tensors:  tensor([1, 1, 1, 1, 1, 1, 1, 1])</span><span id="4fc5" class="me le jf ma b gy mj mg l mh mi">Output label tensor shape:  torch.Size([8])</span></pre><p id="187a" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">为了绘制图像，我们将使用 matloptlib 中的<code class="fe ng nh ni ma b">plt.imshow</code>。它期望图像尺寸为<code class="fe ng nh ni ma b">(height, width, channels)</code>。我们将<code class="fe ng nh ni ma b">.permute()</code>我们的单个图像张量来绘制它。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="1ecf" class="me le jf ma b gy mf mg l mh mi"># Selecting the first image tensor from the batch. <br/>single_image = single_batch[0][0]</span><span id="549c" class="me le jf ma b gy mj mg l mh mi">single_image.shape</span><span id="9518" class="me le jf ma b gy mj mg l mh mi"><br/>###################### OUTPUT ######################</span><span id="0af7" class="me le jf ma b gy mj mg l mh mi">torch.Size([3, 224, 224])</span></pre><p id="d43d" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">让我们来看图像。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="8fba" class="me le jf ma b gy mf mg l mh mi">plt.imshow(single_image.(1, 2, 0))</span></pre><figure class="lv lw lx ly gt is gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/ec2580449dd11736a9769578efc11a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*xZ_7p3FkVaGRidfAIJjiUQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">数据集[图像[3]]中的单个样本</p></figure><p id="963c" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">PyTorch 使我们更容易直接从批次中绘制网格图像。</p><p id="8530" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">我们首先从列表中提取图像张量(由我们的数据加载器返回)并设置<code class="fe ng nh ni ma b">nrow</code>。然后我们使用<code class="fe ng nh ni ma b">plt.imshow()</code>函数来绘制网格。记住<code class="fe ng nh ni ma b">.permute()</code>张量维度！</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="762f" class="me le jf ma b gy mf mg l mh mi"># We do single_batch[0] because each batch is a list <br/># where the 0th index is the image tensor and 1st index is the output label.<br/>single_batch_grid = utils.make_grid(single_batch[0], nrow=4)</span><span id="40a1" class="me le jf ma b gy mj mg l mh mi">plt.figure(figsize = (10,10))<br/>plt.imshow(single_batch_grid.permute(1, 2, 0))</span></pre><figure class="lv lw lx ly gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pl"><img src="../Images/4726f93ba8ef3b4bfe7778d27c270882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jg2aqjE7uLtrKLfrrEwYVQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来自数据集的多个样本[图像[4]]</p></figure><h1 id="a2d6" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">定义 CNN 架构</h1><p id="2ef8" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">我们的建筑很简单。我们使用 4 块 Conv 层。每个区块由<code class="fe ng nh ni ma b">Convolution</code> + <code class="fe ng nh ni ma b">BatchNorm</code> + <code class="fe ng nh ni ma b">ReLU</code> + <code class="fe ng nh ni ma b">Dropout</code>层组成。</p><p id="f159" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">我们不会在最后使用一个<code class="fe ng nh ni ma b">FC</code>层。我们将坚持使用<code class="fe ng nh ni ma b">Conv</code>层。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="8e10" class="me le jf ma b gy mf mg l mh mi">class HotDogClassifier(nn.Module):<br/>    def __init__(self):<br/>        super(HotDogClassifier, self).__init__()<br/></span><span id="d404" class="me le jf ma b gy mj mg l mh mi">        self.block1 = self.conv_block(c_in=3, c_out=256, dropout=0.1, kernel_size=5, stride=1, padding=2)<br/>        self.block2 = self.conv_block(c_in=256, c_out=128, dropout=0.1, kernel_size=3, stride=1, padding=1)<br/>        self.block3 = self.conv_block(c_in=128, c_out=64, dropout=0.1, kernel_size=3, stride=1, padding=1)<br/>        self.lastcnn = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=56, stride=1, padding=0)</span><span id="f20f" class="me le jf ma b gy mj mg l mh mi">        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)<br/></span><span id="6732" class="me le jf ma b gy mj mg l mh mi">    def forward(self, x):<br/>        x = self.block1(x)<br/>        x = self.maxpool(x)</span><span id="69b7" class="me le jf ma b gy mj mg l mh mi">        x = self.block2(x)</span><span id="28d1" class="me le jf ma b gy mj mg l mh mi">        x = self.block3(x)<br/>        x = self.maxpool(x)</span><span id="1c75" class="me le jf ma b gy mj mg l mh mi">        x = self.lastcnn(x)</span><span id="7913" class="me le jf ma b gy mj mg l mh mi">        return x<br/></span><span id="6594" class="me le jf ma b gy mj mg l mh mi">    def conv_block(self, c_in, c_out, dropout,  **kwargs):<br/>        seq_block = nn.Sequential(<br/>            nn.Conv2d(in_channels=c_in, out_channels=c_out, **kwargs),<br/>            nn.BatchNorm2d(num_features=c_out),<br/>            nn.ReLU(),<br/>            nn.Dropout2d(p=dropout)<br/>        )</span><span id="53ad" class="me le jf ma b gy mj mg l mh mi">        return seq_block</span></pre><p id="b4de" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">现在我们将初始化模型、优化器和损失函数。</p><p id="b7db" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">然后我们将模型传输到 GPU。</p><p id="d5ff" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">尽管这是一个二元分类问题，我们还是使用了<code class="fe ng nh ni ma b">nn.CrossEntropyLoss</code>。这意味着，我们将处理<code class="fe ng nh ni ma b">0 and 1</code>的返回 2 个值，而不是返回<code class="fe ng nh ni ma b">1/0</code>的单个输出。更具体地说，输出的概率是<code class="fe ng nh ni ma b">1</code>或<code class="fe ng nh ni ma b">0</code>。</p><p id="227c" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">我们不需要在最后一层之后手动添加一个<code class="fe ng nh ni ma b">log_softmax</code>层，因为<code class="fe ng nh ni ma b">nn.CrossEntropyLoss</code>已经为我们做了。</p><p id="6202" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">然而，我们需要应用<code class="fe ng nh ni ma b">log_softmax</code>进行验证和测试。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="ba06" class="me le jf ma b gy mf mg l mh mi">model = HotDogClassifier()<br/>model.to(device)<br/>print(model)</span><span id="3342" class="me le jf ma b gy mj mg l mh mi">criterion = nn.CrossEntropyLoss()<br/>optimizer = optim.Adam(model.parameters(), lr=0.008)</span><span id="2a9b" class="me le jf ma b gy mj mg l mh mi"><br/>###################### OUTPUT ######################</span><span id="26f3" class="me le jf ma b gy mj mg l mh mi">HotDogClassifier(<br/>  (block1): Sequential(<br/>    (0): Conv2d(3, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))<br/>    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (2): ReLU()<br/>    (3): Dropout2d(p=0.1, inplace=False)<br/>  )<br/>  (block2): Sequential(<br/>    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br/>    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (2): ReLU()<br/>    (3): Dropout2d(p=0.1, inplace=False)<br/>  )<br/>  (block3): Sequential(<br/>    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br/>    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (2): ReLU()<br/>    (3): Dropout2d(p=0.1, inplace=False)<br/>  )<br/>  (lastcnn): Conv2d(64, 2, kernel_size=(56, 56), stride=(1, 1))<br/>  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br/>)</span></pre><p id="6e2d" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">在我们开始训练之前，让我们定义一个函数来计算每个历元的精度。</p><p id="e058" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">该函数将<code class="fe ng nh ni ma b">y_pred</code>和<code class="fe ng nh ni ma b">y_test</code>作为输入参数。然后，我们将 softmax 应用于<code class="fe ng nh ni ma b">y_pred</code>并提取具有较高概率的类别。</p><p id="eec0" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">之后，我们比较预测类别和实际类别来计算准确度。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="cdff" class="me le jf ma b gy mf mg l mh mi">def binary_acc(y_pred, y_test):<br/>    y_pred_tag = torch.log_softmax(y_pred, dim = 1)<br/>    _, y_pred_tags = torch.max(y_pred_tag, dim = 1)</span><span id="fce2" class="me le jf ma b gy mj mg l mh mi">    correct_results_sum = (y_pred_tags == y_test).sum().float()</span><span id="6784" class="me le jf ma b gy mj mg l mh mi">    acc = correct_results_sum/y_test.shape[0]<br/>    acc = torch.round(acc * 100)</span><span id="f010" class="me le jf ma b gy mj mg l mh mi">    return acc</span></pre><p id="15de" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">我们还将定义 2 个字典，用于存储训练集和验证集的准确度/时期和损失/时期。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="3fb1" class="me le jf ma b gy mf mg l mh mi">accuracy_stats = {<br/>    'train': [],<br/>    "val": []<br/>}</span><span id="2504" class="me le jf ma b gy mj mg l mh mi">loss_stats = {<br/>    'train': [],<br/>    "val": []<br/>}</span></pre><p id="5aaa" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">让我们训练我们的模型！</p><p id="939b" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">你可以看到，我们在循环之前已经在。<code class="fe ng nh ni ma b">model.train()</code>告诉 PyTorch 你正处于训练模式。为什么我们需要这么做？如果你使用了像<code class="fe ng nh ni ma b">Dropout</code>或<code class="fe ng nh ni ma b">BatchNorm</code>这样在训练和评估过程中表现不同的层(例如；评估期间不使用<code class="fe ng nh ni ma b">dropout</code>，您需要告诉 PyTorch 采取相应的行动。而 PyTorch 中的默认模式是火车，因此，您不必显式地编写它。但这是很好的练习。</p><p id="67ef" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">同样，当我们测试我们的模型时，我们将调用<code class="fe ng nh ni ma b">model.eval()</code>。我们将在下面看到。回到训练；我们开始一个循环。在这个 for 循环的顶部，我们将每个历元的损失和精度初始化为 0。在每个时期之后，我们将打印出损失/精度并将其重置回 0。</p><p id="d442" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">然后我们有另一个 for 循环。这个 for 循环用于从<code class="fe ng nh ni ma b">train_loader</code>中批量获取我们的数据。</p><p id="5490" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">在我们做任何预测之前，我们先做<code class="fe ng nh ni ma b">optimizer.zero_grad()</code>。由于<code class="fe ng nh ni ma b">.backward()</code>函数累加梯度，我们需要为每个小批量手动将其设置为 0。然后，从我们定义的模型中，我们获得一个预测，获得该小批量的损失(和准确性)，使用 loss.backward()和 optimizer.step()执行反向传播。</p><p id="83f9" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">最后，我们将所有小批量损失(和精度)相加，以获得该时期的平均损失(和精度)。我们将每个小批次的所有损耗/精度相加，最后除以小批次的数量，即。为获得每个时期的平均损失/精确度，列车装载器的长度。</p><p id="ec7f" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">我们遵循的培训程序与验证程序完全相同，除了我们在<code class="fe ng nh ni ma b">torch.no_grad</code>中对其进行了包装，并且不执行任何反向传播。<code class="fe ng nh ni ma b">torch.no_grad()</code>告诉 PyTorch 我们不想执行反向传播，这样可以减少内存使用并加快计算速度。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="c3a5" class="me le jf ma b gy mf mg l mh mi">print("Begin training.")</span><span id="eace" class="me le jf ma b gy mj mg l mh mi">for e in tqdm(range(1, 21)):</span><span id="8370" class="me le jf ma b gy mj mg l mh mi">    # TRAINING</span><span id="b497" class="me le jf ma b gy mj mg l mh mi">    train_epoch_loss = 0<br/>    train_epoch_acc = 0</span><span id="e1c3" class="me le jf ma b gy mj mg l mh mi">    model.train()<br/>    for X_train_batch, y_train_batch in train_loader:<br/>        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)<br/>        optimizer.zero_grad()</span><span id="cc8f" class="me le jf ma b gy mj mg l mh mi">        y_train_pred = model(X_train_batch).squeeze()</span><span id="d456" class="me le jf ma b gy mj mg l mh mi">        train_loss = criterion(y_train_pred, y_train_batch)<br/>        train_acc = binary_acc(y_train_pred, y_train_batch)</span><span id="1369" class="me le jf ma b gy mj mg l mh mi">        train_loss.backward()<br/>        optimizer.step()</span><span id="8b7f" class="me le jf ma b gy mj mg l mh mi">        train_epoch_loss += train_loss.item()<br/>        train_epoch_acc += train_acc.item()<br/></span><span id="034f" class="me le jf ma b gy mj mg l mh mi">    # VALIDATION<br/>    with torch.no_grad():<br/>        model.eval()<br/>        val_epoch_loss = 0<br/>        val_epoch_acc = 0<br/>        for X_val_batch, y_val_batch in val_loader:<br/>            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)</span><span id="10c4" class="me le jf ma b gy mj mg l mh mi">            y_val_pred = model(X_val_batch).squeeze()<br/>            y_val_pred = torch.unsqueeze(y_val_pred, 0)</span><span id="7234" class="me le jf ma b gy mj mg l mh mi">            val_loss = criterion(y_val_pred, y_val_batch)<br/>            val_acc = binary_acc(y_val_pred, y_val_batch)</span><span id="6743" class="me le jf ma b gy mj mg l mh mi">            val_epoch_loss += val_loss.item()<br/>            val_epoch_acc += val_acc.item()</span><span id="8256" class="me le jf ma b gy mj mg l mh mi">    loss_stats['train'].append(train_epoch_loss/len(train_loader))<br/>    loss_stats['val'].append(val_epoch_loss/len(val_loader))<br/>    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))<br/>    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))<br/></span><span id="fb60" class="me le jf ma b gy mj mg l mh mi">    print(f'Epoch {e+0:02}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')</span><span id="875f" class="me le jf ma b gy mj mg l mh mi">###################### OUTPUT ######################</span><span id="dcbd" class="me le jf ma b gy mj mg l mh mi">Begin training.</span><span id="d7aa" class="me le jf ma b gy mj mg l mh mi">Epoch 01: | Train Loss: 113.08463 | Val Loss: 92.26063 | Train Acc: 51.120| Val Acc: 29.000<br/>Epoch 02: | Train Loss: 55.47888 | Val Loss: 50.39846 | Train Acc: 63.620| Val Acc: 57.000<br/>Epoch 03: | Train Loss: 33.44443 | Val Loss: 20.69457 | Train Acc: 70.500| Val Acc: 71.000<br/>Epoch 04: | Train Loss: 18.75201 | Val Loss: 1.50821 | Train Acc: 77.240| Val Acc: 71.000<br/>Epoch 05: | Train Loss: 12.88685 | Val Loss: 26.62685 | Train Acc: 75.480| Val Acc: 71.000<br/>Epoch 06: | Train Loss: 9.70507 | Val Loss: 3.25360 | Train Acc: 81.080| Val Acc: 86.000<br/>Epoch 07: | Train Loss: 11.04334 | Val Loss: 0.00000 | Train Acc: 79.320| Val Acc: 100.000<br/>Epoch 08: | Train Loss: 7.16636 | Val Loss: 10.48954 | Train Acc: 83.300| Val Acc: 71.000<br/>Epoch 09: | Train Loss: 4.32204 | Val Loss: 0.00001 | Train Acc: 86.400| Val Acc: 100.000<br/>Epoch 10: | Train Loss: 2.03338 | Val Loss: 0.00000 | Train Acc: 91.720| Val Acc: 100.000<br/>Epoch 11: | Train Loss: 1.68124 | Val Loss: 3.65754 | Train Acc: 92.320| Val Acc: 71.000<br/>Epoch 12: | Train Loss: 1.27145 | Val Loss: 5.52111 | Train Acc: 93.320| Val Acc: 86.000<br/>Epoch 13: | Train Loss: 0.42285 | Val Loss: 0.00000 | Train Acc: 97.600| Val Acc: 100.000<br/>Epoch 14: | Train Loss: 1.03441 | Val Loss: 0.00000 | Train Acc: 94.840| Val Acc: 100.000<br/>Epoch 15: | Train Loss: 0.76563 | Val Loss: 0.00000 | Train Acc: 96.340| Val Acc: 100.000<br/>Epoch 16: | Train Loss: 0.16889 | Val Loss: 0.00000 | Train Acc: 98.040| Val Acc: 100.000<br/>Epoch 17: | Train Loss: 0.42046 | Val Loss: 4.02560 | Train Acc: 96.560| Val Acc: 86.000<br/>Epoch 18: | Train Loss: 0.57535 | Val Loss: 0.00000 | Train Acc: 95.640| Val Acc: 100.000<br/>Epoch 19: | Train Loss: 0.40181 | Val Loss: 0.00000 | Train Acc: 96.620| Val Acc: 100.000<br/>Epoch 20: | Train Loss: 0.92207 | Val Loss: 0.00000 | Train Acc: 95.360| Val Acc: 100.000</span></pre><h1 id="c6ea" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">可视化损失和准确性</h1><p id="6b92" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">为了绘制损耗和精度线图，我们再次从<code class="fe ng nh ni ma b">accuracy_stats</code>和<code class="fe ng nh ni ma b">loss_stats</code>字典中创建一个数据帧。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="2199" class="me le jf ma b gy mf mg l mh mi">train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})</span><span id="6d97" class="me le jf ma b gy mj mg l mh mi">train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})</span><span id="b224" class="me le jf ma b gy mj mg l mh mi">fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))<br/>sns.lineplot(data=train_val_acc_df, x = "epochs", y="value", hue="variable",  ax=axes[0]).set_title('Train-Val Accuracy/Epoch')</span><span id="746c" class="me le jf ma b gy mj mg l mh mi">sns.lineplot(data=train_val_loss_df, x = "epochs", y="value", hue="variable", ax=axes[1]).set_title('Train-Val Loss/Epoch')</span></pre><figure class="lv lw lx ly gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pm"><img src="../Images/521c54f1ae0b8769abb28e72f1a2f9cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FzvGeEGN8UDNuCI0Rs8kmA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">列车和阀门组的 Acc/loss 曲线[图片[5]]</p></figure><h1 id="0b46" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">试验</h1><p id="6cf2" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">训练完成后，我们需要测试我们的模型进展如何。注意，在运行测试代码之前，我们已经使用了<code class="fe ng nh ni ma b">model.eval()</code>。为了告诉 PyTorch 我们不希望在推断过程中执行反向传播，我们使用了<code class="fe ng nh ni ma b">torch.no_grad()</code>，就像我们对上面的验证循环所做的那样。</p><ul class=""><li id="b1cb" class="of og jf mm b mn mo mq mr mt oh mx oi nb oj nf ok ol om on bi translated">我们首先定义一个包含我们预测的列表。然后我们使用<code class="fe ng nh ni ma b">test_loader</code>循环遍历我们的批处理。对于每一批-</li><li id="db5f" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">我们将输入小批量数据转移到 GPU。</li><li id="4eb2" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">我们使用训练好的模型进行预测。</li><li id="4e94" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">将 log_softmax 激活应用于预测，并选择概率最高的索引。</li><li id="4eb8" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">将批处理从 CPU 移动到 GPU。</li><li id="8659" class="of og jf mm b mn oo mq op mt oq mx or nb os nf ok ol om on bi translated">将张量转换为 numpy 对象，并将其添加到我们的列表中。</li></ul><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="2c69" class="me le jf ma b gy mf mg l mh mi">y_pred_list = []<br/>y_true_list = []<br/>with torch.no_grad():<br/>    for x_batch, y_batch in tqdm(test_loader):<br/>        x_batch, y_batch = x_batch.to(device), y_batch.to(device)</span><span id="f09d" class="me le jf ma b gy mj mg l mh mi">        y_test_pred = model(x_batch)<br/>        _, y_pred_tag = torch.max(y_test_pred, dim = 1)</span><span id="65b3" class="me le jf ma b gy mj mg l mh mi">        y_pred_list.append(y_pred_tag.cpu().numpy())<br/>        y_true_list.append(y_batch.cpu().numpy())</span></pre><p id="cbf3" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">我们将使列表变平，这样我们就可以将它用作<code class="fe ng nh ni ma b">confusion_matrix</code>和<code class="fe ng nh ni ma b">classification_report</code>的输入。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="23af" class="me le jf ma b gy mf mg l mh mi">y_pred_list = [i[0][0][0] for i in y_pred_list]</span><span id="0524" class="me le jf ma b gy mj mg l mh mi">y_true_list = [i[0] for i in y_true_list]</span></pre><h1 id="8a6e" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">分类报告</h1><p id="f643" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">最后，我们打印出包含精确度、召回率和 F1 分数的分类报告。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="7abe" class="me le jf ma b gy mf mg l mh mi">print(classification_report(y_true_list, y_pred_list))</span><span id="a6f1" class="me le jf ma b gy mj mg l mh mi"><br/>###################### OUTPUT ######################</span><span id="c8f2" class="me le jf ma b gy mj mg l mh mi">precision    recall  f1-score   support</span><span id="4be5" class="me le jf ma b gy mj mg l mh mi">           0       0.90      0.91      0.91       249<br/>           1       0.91      0.90      0.91       249</span><span id="6828" class="me le jf ma b gy mj mg l mh mi">    accuracy                           0.91       498<br/>   macro avg       0.91      0.91      0.91       498<br/>weighted avg       0.91      0.91      0.91       498</span></pre><h1 id="98dc" class="ld le jf bd lf lg lh li lj lk ll lm ln ku lo kv lp kx lq ky lr la ls lb lt lu bi translated">混淆矩阵</h1><p id="97d3" class="pw-post-body-paragraph mk ml jf mm b mn nj kp mp mq nk ks ms mt nl mv mw mx nm mz na nb nn nd ne nf ij bi translated">让我们使用<code class="fe ng nh ni ma b">confusion_matrix()</code>函数来制作一个混淆矩阵。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="b1b2" class="me le jf ma b gy mf mg l mh mi">print(confusion_matrix(y_true_list, y_pred_list))</span><span id="0633" class="me le jf ma b gy mj mg l mh mi"><br/>###################### OUTPUT ######################</span><span id="8d35" class="me le jf ma b gy mj mg l mh mi">[[226  23]<br/> [ 24 225]]</span></pre><p id="27ec" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">我们从混淆矩阵中创建一个数据框架，并使用 seaborn 库将其绘制为热图。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="2226" class="me le jf ma b gy mf mg l mh mi">confusion_matrix_df = pd.DataFrame(confusion_matrix(y_true_list, y_pred_list)).rename(columns=idx2class, index=idx2class)</span><span id="3a50" class="me le jf ma b gy mj mg l mh mi">fig, ax = plt.subplots(figsize=(7,5))         <br/>sns.heatmap(confusion_matrix_df, annot=True, ax=ax)</span></pre><figure class="lv lw lx ly gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pn"><img src="../Images/af0b0c5782c2f04cebfec817097caf85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NP4n4_iz-UsoJjc_5X7JpQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">混淆矩阵热图[图片[6]]</p></figure></div><div class="ab cl po pp hu pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="ij ik il im in"><p id="3861" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">感谢您的阅读。欢迎提出建议和建设性的批评。:)</p><p id="5979" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">这篇博文是专栏“如何训练你的神经网络”的一部分。你可以在这里找到<strong class="mm jp"> </strong> <a class="ae pv" href="https://towardsdatascience.com/tagged/akshaj-wields-pytorch" rel="noopener" target="_blank">系列</a> <strong class="mm jp">。</strong></p><p id="06e5" class="pw-post-body-paragraph mk ml jf mm b mn mo kp mp mq mr ks ms mt mu mv mw mx my mz na nb nc nd ne nf ij bi translated">你可以在<a class="ae pv" href="https://www.linkedin.com/in/akshajverma7/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae pv" href="https://twitter.com/theairbend3r" rel="noopener ugc nofollow" target="_blank"> Twitter </a>找到我。如果你喜欢这个，看看我的其他<a class="ae pv" href="https://medium.com/@theairbend3r" rel="noopener">博客</a>。</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><a href="https://www.buymeacoffee.com/theairbend3r"><div class="gh gi pw"><img src="../Images/041a0c7464198414e6ce355f9235099e.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*SGCT6C60o4t58wRqeU2viQ.png"/></div></a></figure></div></div>    
</body>
</html>