<html>
<head>
<title>Regularization: Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化:机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regularization-machine-learning-891e9a62c58d?source=collection_archive---------8-----------------------#2020-07-28">https://towardsdatascience.com/regularization-machine-learning-891e9a62c58d?source=collection_archive---------8-----------------------#2020-07-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e864972949fa7640c8e6f2255fe5a840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*N1c-N2BiI4Zg7QVg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@killerfvith?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Alex wong </a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><div class=""/><div class=""><h2 id="d8de" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">过度拟合模型、模型类型、数学和执行的解决方案。</h2></div><h1 id="0807" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">简介:</h1><p id="3000" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">为了理解正则化的概念及其与机器学习的联系，我们首先需要理解我们为什么需要正则化。我们都知道机器学习是用相关数据训练一个模型，并使用该模型预测未知数据。“未知”一词意味着模型尚未看到的数据。我们已经训练了这个模型，并且在使用训练数据时取得了不错的成绩。但是在预测的过程中，我们发现该模型与训练部分相比表现不佳。现在，这可能是一种过度拟合的情况(对此我将在下面解释),这导致了模型的不正确预测。在这种情况下，规范化模型有助于解决问题。</p><h1 id="aa0c" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">背景:</h1><p id="a094" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在本文中，我们将讨论以下主题:</p><ol class=""><li id="f7c8" class="mm mn jj ls b lt mo lw mp lz mq md mr mh ms ml mt mu mv mw bi translated">ML 中欠拟合和过拟合是什么意思？</li><li id="d317" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">如何解决不合身的问题？</li><li id="2087" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">正则化及其背后的数学介绍。</li><li id="248f" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">正规化的类型(L1 和 L2)。</li><li id="3d38" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">正规化和交叉验证。</li></ol><h1 id="1048" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">欠配合和过配合的问题:</h1><p id="af26" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这里是贯穿本文的 Jupyter 笔记本 的<a class="ae jg" href="https://github.com/Gokul-S-Kumar/Regularization.git" rel="noopener ugc nofollow" target="_blank"> <strong class="ls jk">链接。请并排浏览以彻底理解这篇文章。</strong></a></p><p id="071a" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">考虑线性回归的情况。我们将生成一些数据来处理。然后，数据将被分成训练和测试数据。</p><p id="d40a" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">生成的数据如下所示:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/31c20ae32a0832fa3fe54d94dee78073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*FA_8ZB42VWn2Gv8arLbAuw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图一。生成的数据</p></figure><p id="6f36" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们已经将生成的总数据的 20%分割成测试数据。真实拟合函数是我们生成数据所基于的函数。添加噪声是为了模拟真实世界的数据(因为实际上没有数据会完全符合函数)。</p><p id="1e29" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">了解欠拟合和过拟合涉及在具有不同程度的假设方程的模型上训练数据。让我们从一个具有 1 次的<strong class="ls jk">假设方程的模型开始，它将是:</strong></p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5b8b1703628938db2d1fcce773d71a4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*itveDAXxyiFmrm-cEzg7pQ.png"/></div></figure><p id="27a0" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">这是回归线的样子:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/c0c6614826f62b027d2d70123ea0ec6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUT36MKYMx89W-tUjVyMfg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图二。使用一次方程式。</p></figure><p id="605c" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">这是<strong class="ls jk">欠拟合</strong>的一个例子，因为我们可以看到，这条线对数据的拟合程度不足以让我们使用模型进行未来预测。训练和测试精度和误差值如下:</p><pre class="ng nh ni nj gt nm nn no np aw nq bi"><span id="4586" class="nr kz jj nn b gy ns nt l nu nv">train accuracy 0.9276025053551601<br/>test accuracy 0.9259414382942455<br/>Training MSE: 0.040338393489346015<br/>Test MSE: 0.04068556089245825</span></pre><p id="5668" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">欠拟合问题也称为<strong class="ls jk">‘高偏差’</strong>问题。这两者都意味着该模型不太适合训练数据。偏差一词指的是模型有一个强烈的先入为主的观念，即 y 随 x 线性变化，尽管事实并非如此。</p><p id="b7bf" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">现在，我们可以尝试用二次函数来拟合数据。假设是这样的:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c62ea7d4f8aa22cb87f406be0da9e9f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*JRLwHRErb53ht3YmQRbXFA.png"/></div></figure><p id="c120" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">该模型可以被可视化为:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/a764eb0a1d642209694cd5c722094252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M7qUw-GER9YXG9euGIUexQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图三。使用二次方程</p></figure><p id="54ec" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们可以看到，该模型在拟合训练数据方面工作得非常好。训练和测试精度和 MSE 值为:</p><pre class="ng nh ni nj gt nm nn no np aw nq bi"><span id="9db4" class="nr kz jj nn b gy ns nt l nu nv">Train accuracy: 0.9541558552139953<br/>test accuracy 0.9421659027644554<br/>Training MSE: 0.02554341363098832<br/>Test MSE: 0.031772324908036456</span></pre><p id="97ed" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">与以前的模型相比，精确度有所提高。这意味着更加合适。</p><p id="6189" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">由于增加阶数似乎会提高模型的性能，因此让我们尝试使用一个假设方程阶数为 20 的模型。这将是结果:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/7beff200f3c11f43576485c25b872699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CV0aHIDrIUEXuQAinXccvA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 4。使用 20 次方程</p></figure><p id="b8da" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">哎呀！模型函数似乎被扭曲了。这是因为高度使其更灵活，从而允许其在拟合时将噪声纳入数据中。模型函数试图通过所有点，因此看起来失真。</p><p id="75b5" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们把这个问题叫做<strong class="ls jk">【过拟合】</strong>，它有<strong class="ls jk">【高方差】</strong>。术语“高方差”背后的原因是，如果我们要拟合这样一个高阶多项式，那么该假设几乎可以拟合任何函数，即不同类型的可能假设非常大且变化太大，以至于我们没有足够的数据来约束它以得到一个完美的假设。</p><p id="0ff4" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们可以通过检查交叉验证分数或 MSE 值来决定过度拟合的问题。我绘制了上述数据的精确度和 MSE 值，这些数据的假设方程的度数从 1 度到 40 度。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/f2a8712e507896dcf6995040377035ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wU3f3NmDAVX3zzUiYxx-xg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图五。精确度和 MSE 值。</p></figure><p id="0c7f" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们可以看到，在一定程度(具体来说是 9)之后，测试 MSE 似乎增加了，而训练 MSE 继续减少。当模型由于过度拟合而无法预测看不见的数据时，就会发生这种情况。</p><p id="239e" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">使用 9 次方程，我们得到以下模型函数:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/383dc7d2b82c68f56ee31e001233f4bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-p7HmC6og7hLQPEEQ25EGg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图六。用一个 9 次方程。</p></figure><p id="de25" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">如果我们有太多的特征，过度拟合也会发生，这是当我们增加函数的次数时间接发生的。已学习的假设可能非常适合训练集(在某种程度上，成本值将为零)，但无法预测新的示例。同样的概念也适用于<strong class="ls jk">逻辑回归</strong>，这里考虑的是决策边界而不是回归线。</p><h1 id="9e92" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">解决过度拟合问题:</h1><p id="5bb5" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">大多数真实世界的数据集将具有大量的要素。如果我们有很多特征，但只有很少的训练数据，那么就会出现过度拟合。解决过度拟合有两种主要方法:</p><ol class=""><li id="712c" class="mm mn jj ls b lt mo lw mp lz mq md mr mh ms ml mt mu mv mw bi translated"><strong class="ls jk">减少特征数量</strong>:这可以通过手动减少特征数量来实现。我们可以在训练数据之前做一些 EDA 和特征选择，以知道哪些特征要保留，哪些要丢弃。我们还可以使用自动选择特征的模型选择算法。这种方法的缺点是，通过删除某些特征，我们删除了关于问题的信息。</li><li id="3f5a" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated"><strong class="ls jk">正则化</strong>:该方法保留了所有特征，但降低了假设参数的幅度。当所有参数都对标签的预测有贡献时，这种方法工作良好。</li></ol><h1 id="02ef" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">正规化介绍:</h1><p id="94b9" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">考虑一下我们上面看到的数据拟合 10 次函数的情况。假设是这样的:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c1afb6ec3e54ce450ec6cc3ca1b5a7b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*CUy73NoooLByJtZegykApw.png"/></div></figure><p id="5336" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">如果我们惩罚θ_ 10 并使其非常小(几乎等于零),那么假设将被简化为 9 次方程，这将是如上所示的数据的最佳拟合。</p><p id="5026" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">这是正则化背后的一般思想。我们不是减少一个参数，而是惩罚所有的参数。这将产生一个更简单的假设，它不容易过度拟合。</p><h1 id="7ce1" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">线性回归的正则化成本函数；</h1><p id="7113" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">对于任何不熟悉线性回归背后的数学的人，请参考下面我以前发表的故事。它还会让你对以后要用到的术语和变量有所了解。</p><div class="is it gp gr iu oa"><a rel="noopener follow" target="_blank" href="/univariate-linear-regression-theory-and-practice-99329845e85d"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd jk gy z fp of fr fs og fu fw ji bi translated">一元线性回归-理论与实践</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">简介:这篇文章解释了一元线性回归的数学和执行。这将包括数学…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ja oa"/></div></div></a></div><p id="f7ad" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">任何回归模型的成本函数由下式给出:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/ff3cc2c0e9555153935da5e280ed9cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*sUrAyNqW1wRuypqvOUNXjw.png"/></div></figure><p id="bb08" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">为了将正则化添加到模型中，成本函数被稍微修改为:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/198268b06fe452b6883840f7874f245c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*PI9FokYmO9Jy4cP8tyNgtw.png"/></div></figure><p id="3706" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">在哪里</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e60865c4c605473de1da1fa7defa5302.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*L1wBlFiEBJW9VgXbcVSpcg.png"/></div></figure><p id="b50d" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">Lambda 的目的是在限制参数值的同时为训练数据提供良好的拟合，从而保持假设相对简单，以避免过度捕捞。换句话说，我们正在给过度拟合的成本函数(趋于零)添加一个小的偏差，以防止它过度捕捞。通过增加少量的偏差，我们得到了方差的显著下降。</p><p id="ff7e" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">考虑 10 度假设的情况。如果我们把λ的值设得很高，比如说λ=⁰⁰.，会发生什么所有的参数 theta_1 到 theta_10 将被高度减少到假设将是:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/1bc946600788208da90a771cec1a8111.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*IYw7k2zVyZurGqa-VjAaPQ.png"/></div></figure><p id="df82" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">该曲线在绘制时将是一条水平直线，因此与我们的数据拟合不佳。因此，为了使正则化工作良好，还应该小心选择λ的值。</p><h1 id="eb58" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">线性回归的正则梯度下降；</h1><p id="09d5" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">没有正则化的线性回归的梯度下降由下式给出:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/24d2b5fbd4a5ae376480d205b7bf2f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*UpiLsPvSvBu1NF85d4OuCQ.png"/></div></figure><p id="d1bc" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">成本函数的偏导数将由于包含正则化而改变，正则化产生如下梯度下降方程:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/4c155b4a078cd7816adfb93586a4fab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*RGEkin1Q1fJolTwSfSOgLQ.png"/></div></figure><p id="87b3" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我已经单独写了θ_ 0，因为它不包括在正则化参数的总和中。这可以简化为:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/7101dfef0707966ab88ca3e402a30c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZom0Ze8lVhxXve2i3EY2g.png"/></div></div></figure><p id="240c" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">因为α、λ和 m 是正的:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/d3b5cedb96f5143ba38d05825e5b21d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*L00y55QqTdpFKKkORAlxAA.png"/></div></figure><p id="0099" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">因此，当应用梯度下降时，每次θ_ j 的值将从θ_ j 的较低值减小，比如 0.99 *θ_ j。这有助于在梯度下降的每次迭代之后收缩θ的值。</p><h1 id="68a2" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">正规化的类型:</h1><p id="1d94" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">主要有两种类型的正则化技术:</p><ol class=""><li id="46f7" class="mm mn jj ls b lt mo lw mp lz mq md mr mh ms ml mt mu mv mw bi translated">L1 正则化或套索回归。</li><li id="e0c7" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">L2 正则化或岭回归。</li></ol><h1 id="13bc" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">岭回归:</h1><p id="475c" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们上面讨论的所有数学都与岭回归有关。它的成本函数是</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/198268b06fe452b6883840f7874f245c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*PI9FokYmO9Jy4cP8tyNgtw.png"/></div></figure><p id="99ee" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">下面的图是通过用不同的λ值将 20 次方程拟合到我们上面使用的数据而得到的。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/2b64872614d3e3952fcc48d1427b06c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nuISH-zo8QpIW7GE9v_HYg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图七。λ= 0 的岭回归</p></figure><p id="33e0" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们可以看到，对于λ= 0，使用 20 次方程的过拟合线性回归没有变化。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/a3e9bc3eeffa82ff76797052f11a8741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Y7K-hPAdFnlK7EK0oz6aQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图八。λ= 0.5 的岭回归</p></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/caf0d306ccde6dbb2920a781da5333fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wEtPXCoKRCVeSqv44fHkfQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图九。λ= 1 的岭回归</p></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/73c7953089e2e9a6152e6661f86c9be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0RqaM0VF1-NiFWf676Kq9Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 10。λ= 10 的岭回归</p></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/46f153c4891a09388a1f9299f7986b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_5dVH5mXJuXWDSpR-npoJw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 11。λ= 100 的岭回归</p></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/a6289f7edba3d70529a167603e5a54be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PY0kjVtIm3ZSkiD_Erhrcw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 12。λ= 1000 的岭回归</p></figure><p id="1371" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们可以看到，随着λ值的增加，这条线逐渐平行于 x 轴。这是对上述论点的直观证明，其中λ的值很大，从而将假设减少到只有θ_ 0。获取 lambda = 10000 的参数值。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/85a2d4cf07c2501da877e3745c607c14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OIBEKN37269AAmCzCPoL-g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 13。λ= 10000 的岭回归</p></figure><pre class="ng nh ni nj gt nm nn no np aw nq bi"><span id="b0c8" class="nr kz jj nn b gy ns nt l nu nv">Parameters: [ 7.59298747e-05 -1.59864470e-03 -1.48732636e-03 -1.24979159e-03<br/> -1.04968203e-03 -8.94986923e-04 -7.75465974e-04 -6.81440074e-04<br/> -6.05849341e-04 -5.43793075e-04 -4.91877768e-04 -4.47723687e-04<br/> -4.09631866e-04 -3.76365596e-04 -3.47007135e-04 -3.20862795e-04<br/> -2.97399119e-04 -2.76199260e-04 -2.56932649e-04 -2.39333548e-04<br/> -2.23185647e-04]</span></pre><p id="bdb8" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们可以看到参数趋向于零，并且有不同的值。</p><p id="afce" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">岭回归用于训练样本的<strong class="ls jk">大小小于</strong>的情况，从而通过减少方差来防止模型过度拟合训练数据。</p><h1 id="4649" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">套索回归:</h1><p id="d5a6" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">套索回归与岭回归非常相似。在成本函数中有一个微小的修改，它将是:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/f08f67cb01cc439a46dcf4cddf1775b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*BTMCAdd4EeeTpxUsGXL8bQ.png"/></div></figure><p id="b77e" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们将使用参数的模之和，而不是正则项中参数的平方和。</p><p id="7921" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">下面的图是通过用不同的λ值将 20 次方程拟合到我们上面使用的数据而得到的。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/528c4eceb944b3c2af94868215167667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eQelAeIbYP4zu4AWLGxdKA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 14。lambda=1 的套索回归</p></figure><p id="438e" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们可以看到，对于λ= 0.1，模型是一条平行于 x 轴的直线，这与数据拟合得不好。因此，很明显，我们必须进一步降低λ的值。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/9d8fea9054f49989dee0c5afa687f770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xjh-yH8bL4Y5ymKbzqu7LQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 15。lambda=0.1 的套索回归</p></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/c2c61a43bc23db73bf83257235ad6ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5nskFl03blQE4wIPXB14yQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 16。lambda=0.01 的套索回归</p></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/b67c5d5dd5a593c9fffca2fbd3cad8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zJZTKoWCEECuvK3kBPCAjQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 17。lambda=0.001 的套索回归</p></figure><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/c5db37c941916180303c7cdbb4f962c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NN5fxN9u7trHtWu2Vb1SNg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 17。lambda=0.0005 的套索回归</p></figure><p id="f39a" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">随着λ值的增加，模型变成平行于 x 轴的直线。这里，对于较大的λ值，参数等于零。这是岭回归和套索回归的主要区别。</p><blockquote class="oy"><p id="222c" class="oz pa jj bd pb pc pd pe pf pg ph ml dk translated">在大λ值的岭回归中，参数趋向于零，而在拉索回归中，参数可以等于零。</p></blockquote><p id="76bd" class="pw-post-body-paragraph lq lr jj ls b lt pi kk lv lw pj kn ly lz pk mb mc md pl mf mg mh pm mj mk ml im bi translated">λ= 1 的参数为:</p><pre class="ng nh ni nj gt nm nn no np aw nq bi"><span id="67b3" class="nr kz jj nn b gy ns nt l nu nv">Parameters: [-0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563<br/> -0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563<br/> -0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563<br/> -0.00227563 -0.00227563 -0.00227563]</span></pre><p id="0e33" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们可以看到所有的值都相等，并且近似等于零。进一步增加 lambda 的值不会改变参数的值，如下所示。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/528c4eceb944b3c2af94868215167667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eQelAeIbYP4zu4AWLGxdKA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 18。λ= 10 的套索回归</p></figure><pre class="ng nh ni nj gt nm nn no np aw nq bi"><span id="b5c3" class="nr kz jj nn b gy ns nt l nu nv">Parameters: [-0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563<br/> -0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563<br/> -0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563 -0.00227563<br/> -0.00227563 -0.00227563 -0.00227563]</span></pre><p id="87a5" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">当参数减少到零时，与这些参数相关联的特征将不会对成本函数有任何影响。通过这样做，我们将执行一种类型的<strong class="ls jk">特征选择</strong>，其中我们省略了所有对模型没有太大贡献的无用特征。</p><h1 id="43dc" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">正则化系数的取值如何选择？</h1><p id="3cef" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">模型表现最佳的正则化系数或λ的值可以通过交叉验证获得。sklearn 的岭回归器中有内置的交叉验证技术。我们可以直接使用它，也可以执行单独的交叉验证过程。</p><p id="21c8" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们将使用与上述 20 次方程相同的数据。我们必须输入一个包含不同 lambda 值的数组来执行交叉验证。</p><p id="e4af" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">lambdas 的第一个输入是:</p><pre class="ng nh ni nj gt nm nn no np aw nq bi"><span id="ef79" class="nr kz jj nn b gy ns nt l nu nv">[0.0005, 0.001, 0.01, 0.1, 1, 10, 100]</span></pre><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/2052d53bf1f608af4acba6eb7f53c8d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UXIEig7LUKyn5VKP9OG-Zw.png"/></div></div></figure><p id="62fa" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">λ的最佳值及其对应的 CV 值为:</p><pre class="ng nh ni nj gt nm nn no np aw nq bi"><span id="6c56" class="nr kz jj nn b gy ns nt l nu nv">Best lambda: 0.0005<br/>CV score: 0.983912496818687</span></pre><p id="0daa" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们可以通过输入 0.0005 左右的值来进一步放大λ值。类似地，交叉验证的方法可以使用 sklearn 的 LassoCV 对象。</p><p id="988b" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">正则化方法也可用于防止<strong class="ls jk">逻辑回归</strong>中的过拟合。我们将使用 sklearn 的<strong class="ls jk">岭分类器</strong>在逻辑回归中实现 L2 正则化。</p><h1 id="c175" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">结论:</h1><p id="dff8" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们在本文中看到了以下内容:</p><ol class=""><li id="d17d" class="mm mn jj ls b lt mo lw mp lz mq md mr mh ms ml mt mu mv mw bi translated">欠拟合和过拟合如何影响模型。</li><li id="9eab" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">解决过拟合的方法。</li><li id="9b33" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">正规化的概念。</li><li id="6860" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">正则化代价函数和梯度下降。</li><li id="4e2f" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">L1 正则化或套索回归。</li><li id="74c6" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">L2 正则化或岭回归。</li><li id="33ff" class="mm mn jj ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">使用交叉验证来确定正则化系数。</li></ol><h1 id="7dfc" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">推论:</h1><ol class=""><li id="7bcb" class="mm mn jj ls b lt lu lw lx lz pn md po mh pp ml mt mu mv mw bi translated">关于机器学习概念背后的数学问题，你可以参考 Youtube 上的这个播放列表。</li></ol><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="pq pr l"/></div></figure><p id="3bb7" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">2.这是 Youtube 上的另一个播放列表，它简单地解释了所有的机器学习基础，没有涉及太多的术语。</p><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="pq pr l"/></div></figure><p id="8756" class="pw-post-body-paragraph lq lr jj ls b lt mo kk lv lw mp kn ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">请点击这里查看我关于数据科学和机器学习的其他文章<a class="ae jg" href="https://medium.com/@skumar.gokul" rel="noopener">。</a>欢迎在评论中和<a class="ae jg" href="https://www.linkedin.com/in/gokul-s-kumar" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上进行更深入的讨论。</p></div></div>    
</body>
</html>