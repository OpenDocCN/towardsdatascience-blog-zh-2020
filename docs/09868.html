<html>
<head>
<title>Exploring Classifiers with Python Scikit-learn — Iris Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python Scikit-learn-Iris 数据集探索分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-classifiers-with-python-scikit-learn-iris-dataset-2bcb490d2e1b?source=collection_archive---------3-----------------------#2020-07-13">https://towardsdatascience.com/exploring-classifiers-with-python-scikit-learn-iris-dataset-2bcb490d2e1b?source=collection_archive---------3-----------------------#2020-07-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c83c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何在 Python 中构建第一个分类器的分步指南。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ab72f5ded947c9ea7165d7ede918dc01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eZW9OBEUXqlThiwI"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">凯文·卡斯特尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="22d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一瞬间，想象你不是花卉专家(如果你是专家，那对你有好处！).你能区分三种不同的鸢尾吗——刚毛鸢尾、杂色鸢尾和海滨鸢尾？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/9387181036f5f62d70cbb2645ab8853b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bnLKsChXq94QjtAiRn40w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">鸣谢:基尚·马拉德卡(<a class="ae ky" href="https://analyticsindiamag.com/start-building-first-machine-learning-project-famous-dataset/" rel="noopener ugc nofollow" target="_blank">链接</a></p></figure><p id="e073" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我知道我不能…</p><p id="7779" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，如果我们有一个包含这些物种实例的数据集，并对它们的萼片和花瓣进行测量，会怎么样呢？</p><p id="1541" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，我们能从这个数据集中学到什么来帮助我们区分这三个物种吗？</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="2ab3" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">古玩目录</h1><ol class=""><li id="3776" class="mv mw it lb b lc mx lf my li mz lm na lq nb lu nc nd ne nf bi translated"><a class="ae ky" href="#c27c" rel="noopener ugc nofollow">我们为什么选择这个数据集？</a></li><li id="a467" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated">我们试图回答什么问题？</li><li id="3f8b" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated"><a class="ae ky" href="#75d2" rel="noopener ugc nofollow">我们能在这个数据集中找到什么？</a></li><li id="1942" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated"><a class="ae ky" href="#1599" rel="noopener ugc nofollow">我们正在构建哪些分类器？</a></li><li id="5134" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated"><a class="ae ky" href="#f4ab" rel="noopener ugc nofollow">接下来我们能做什么？</a></li></ol><h1 id="c27c" class="md me it bd mf mg nl mi mj mk nm mm mn jz nn ka mp kc no kd mr kf np kg mt mu bi translated">资料组</h1><p id="136e" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">在这篇博文中，我将探索来自 UCI 机器学习知识库的虹膜数据集。摘自其<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank">网站</a>，据说是“<em class="nt">也许是</em> <strong class="lb iu"> <em class="nt">模式识别文献</em>【1】中最知名的数据库 </strong> <em class="nt">。此外，创建了<a class="ae ky" href="https://machinelearningmastery.com/" rel="noopener ugc nofollow" target="_blank">机器学习大师</a>社区的杰森·布朗利(Jason Brownlee)称之为机器学习的“<strong class="lb iu">Hello World</strong>”[2]。</em></p><p id="8504" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我会把这个数据集推荐给任何一个初学数据科学并渴望构建他们的第一个 ML 模型的人。以下是该数据集的一些良好特征:</p><ol class=""><li id="b83d" class="mv mw it lb b lc ld lf lg li nu lm nv lq nw lu nc nd ne nf bi translated">150 个样本，4 个属性(相同单位，全是数字)</li><li id="8587" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated">均衡的类别分布(每个类别 50 个样本)</li><li id="f394" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated">没有丢失数据</li></ol><p id="3b2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您所看到的，这些特征有助于最大限度地减少您在数据准备过程中需要花费的时间，以便您可以专注于构建 ML 模型。并不是说准备阶段不重要。相反，这个过程非常重要，对于一些初学者来说，它可能太费时间了，以至于他们可能在进入模型开发阶段之前就不知所措了。</p><p id="34fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，来自 Kaggle 的流行数据集<a class="ae ky" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview" rel="noopener ugc nofollow" target="_blank">House Prices:Advanced Regression Techniques</a>有大约 80 个特征，其中超过 20%包含某种程度的缺失数据。在这种情况下，您可能需要花一些时间来理解属性并输入缺失的值。</p><p id="ec1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在希望你的信心水平(没有统计双关语)相对较高。这里有一些关于数据争论的资源，你可以在处理更复杂的数据集和任务时通读一下:<a class="ae ky" rel="noopener" target="_blank" href="/dimensionality-reduction-for-machine-learning-80a46c2ebb7e">降维</a>、<a class="ae ky" href="https://machinelearningmastery.com/what-is-imbalanced-classification/" rel="noopener ugc nofollow" target="_blank">不平衡分类</a>、<a class="ae ky" href="https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/" rel="noopener ugc nofollow" target="_blank">特征工程</a>和<a class="ae ky" href="https://machinelearningmastery.com/handle-missing-data-python/" rel="noopener ugc nofollow" target="_blank">插补</a>。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="e221" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">目标</h1><p id="2e20" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">探索该数据集后，我们希望能够回答两个问题，这两个问题在大多数分类问题中非常典型:</p><ol class=""><li id="94fe" class="mv mw it lb b lc ld lf lg li nu lm nv lq nw lu nc nd ne nf bi translated"><strong class="lb iu">预测</strong> —给定新的数据点，模型预测其类别(物种)的准确度如何？</li><li id="e243" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated"><strong class="lb iu">推断</strong> —哪个(些)预测器能有效地帮助预测？</li></ol></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="8729" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">简单说说分类</strong></h1><p id="400c" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">分类是一种类型的<a class="ae ky" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">监督机器学习</strong> </a>问题，其中目标(响应)变量是分类的。给定包含已知标签的训练数据，分类器逼近从输入变量(X)到输出变量(Y)的映射函数(f)。有关分类的更多资料，请参见<a class="ae ky" href="http://faculty.marshall.usc.edu/gareth-james/ISL/" rel="noopener ugc nofollow" target="_blank">统计学习介绍</a>、<a class="ae ky" href="https://www.coursera.org/learn/machine-learning#syllabus" rel="noopener ugc nofollow" target="_blank">吴恩达的机器学习课程</a>(第 3 周)和<a class="ae ky" href="https://www.simplilearn.com/classification-machine-learning-tutorial" rel="noopener ugc nofollow" target="_blank"> Simplilearn 的分类教程</a>中的第 3 章。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/831834e2a22e416a2610fdc0dfa688f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ftVWj_F37Gxmhd_R"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@florianolv?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Florian Olivo </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="bfde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在是时候写一些代码了！见我的<a class="ae ky" href="https://github.com/terryz1/explore-iris" rel="noopener ugc nofollow" target="_blank"> Github 页面</a>获取我的完整 Python 代码(写在 Jupyter 笔记本上)。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="379b" class="nx me it bd mf ny nz dn mj oa ob dp mn li oc od mp lm oe of mr lq og oh mt oi bi translated"><strong class="ak">导入库并加载数据集</strong></h2><p id="fd02" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">首先需要导入一些库:<em class="nt"> pandas </em>(加载数据集)<em class="nt"> numpy </em>(矩阵操作)<em class="nt"> matplotlib </em>和<em class="nt"> seaborn </em>(可视化)<em class="nt"> sklearn </em>(构建分类器)。确保在导入它们之前已经安装了它们(安装包指南<a class="ae ky" href="https://packaging.python.org/tutorials/installing-packages/" rel="noopener ugc nofollow" target="_blank">此处为</a>)。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="d987" class="nx me it ok b gy oo op l oq or">import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>from sklearn.model_selection import train_test_split<br/>from pandas.plotting import parallel_coordinates<br/>from sklearn.tree import DecisionTreeClassifier, plot_tree<br/>from sklearn import metrics<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.linear_model import LogisticRegression</span></pre><p id="e79e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要加载数据集，我们可以使用 pandas 的<em class="nt"> read_csv </em>函数(我的代码还包括通过 url 加载的选项)。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="17ab" class="nx me it ok b gy oo op l oq or">data = pd.read_csv('data.csv')</span></pre><p id="0495" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">加载完数据后，我们可以通过<em class="nt"> head </em>函数查看前几行:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="6002" class="nx me it ok b gy oo op l oq or">data.head(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/d048bcb7946a415d529ae8e9a7d3654a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YzENlMCHEktxCPALnL5ghA.png"/></div></div></figure><p id="c75d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:所有四个测量单位都是厘米。</p><h2 id="2c8f" class="nx me it bd mf ny nz dn mj oa ob dp mn li oc od mp lm oe of mr lq og oh mt oi bi translated">数字汇总</h2><p id="732b" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">首先，让我们通过<em class="nt">描述:</em>来看看每个属性的数字摘要</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="1e24" class="nx me it ok b gy oo op l oq or">data.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/e21f47bdcb1076786ff2dfca4123638c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qyQDOg-1FRO3rUdoSypO4A.png"/></div></div></figure><p id="79ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以使用<em class="nt"> groupby </em>和<em class="nt"> size </em>检查等级分布:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="88f2" class="nx me it ok b gy oo op l oq or">data.groupby('species').size()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/1dd93fc5d37bc4efa09049faf64e0683.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*d9_K5ux3hOkylw9ToUL61w.png"/></div></figure><p id="3820" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到每个类都有相同数量的实例。</p><h2 id="04de" class="nx me it bd mf ny nz dn mj oa ob dp mn li oc od mp lm oe of mr lq og oh mt oi bi translated">列车测试分离</h2><p id="08ff" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">现在，我们可以将数据集分为训练集和测试集。一般来说，我们还应该有一个<strong class="lb iu">验证集</strong>，用于评估每个分类器的性能，并微调模型参数，以便确定最佳模型。测试集主要用于报告目的。然而，由于这个数据集很小，我们可以通过使用测试集来服务于验证集的目的，从而简化这个过程。</p><p id="4e94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我使用分层的<a class="ae ky" href="https://medium.com/@eijaz/holdout-vs-cross-validation-in-machine-learning-7637112d3f8f#:~:text=its%20own%20post.-,Hold%2Dout,model%20performs%20on%20unseen%20data." rel="noopener">拒绝</a>方法来估计模型的准确性。另一种方法是做<a class="ae ky" href="https://medium.com/@eijaz/holdout-vs-cross-validation-in-machine-learning-7637112d3f8f#:~:text=its%20own%20post.-,Hold%2Dout,model%20performs%20on%20unseen%20data." rel="noopener">交叉验证</a>来减少偏差和方差。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="791f" class="nx me it ok b gy oo op l oq or">train, test = train_test_split(data, test_size = 0.4, stratify = data[‘species’], random_state = 42)</span></pre><p id="9181" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:一般的经验法则是将数据集的 20–30%作为测试集。由于这个数据集很小，我选择了 40%来确保有足够的数据点来测试模型性能。</p><h2 id="75d2" class="nx me it bd mf ny nz dn mj oa ob dp mn li oc od mp lm oe of mr lq og oh mt oi bi translated">探索性数据分析</h2><p id="99a7" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">分割数据集后，我们可以继续研究训练数据。无论是<a class="ae ky" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"> <em class="nt"> matplotlib </em> </a>还是<a class="ae ky" href="https://seaborn.pydata.org/" rel="noopener ugc nofollow" target="_blank"> <em class="nt"> seaborn </em> </a>都有很棒的绘图工具，我们可以用来进行可视化。</p><p id="630c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们首先通过每个特征的直方图创建一些单变量图:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="0c20" class="nx me it ok b gy oo op l oq or">n_bins = 10<br/>fig, axs = plt.subplots(2, 2)<br/>axs[0,0].hist(train['sepal_length'], bins = n_bins);<br/>axs[0,0].set_title('Sepal Length');<br/>axs[0,1].hist(train['sepal_width'], bins = n_bins);<br/>axs[0,1].set_title('Sepal Width');<br/>axs[1,0].hist(train['petal_length'], bins = n_bins);<br/>axs[1,0].set_title('Petal Length');<br/>axs[1,1].hist(train['petal_width'], bins = n_bins);<br/>axs[1,1].set_title('Petal Width');</span><span id="f74f" class="nx me it ok b gy ov op l oq or"># add some spacing between subplots<br/>fig.tight_layout(pad=1.0);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/7057e1b62f9460aa1078563306c74a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Mh5o7lKK4FhlegH7Q-K3uw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">四个特征的直方图</p></figure><p id="130f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，对于花瓣长度和花瓣宽度，似乎有一组数据点的值比其他数据点的值小，这表明该数据中可能有不同的组。</p><p id="9f9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们尝试一些并排的方框图:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="ee54" class="nx me it ok b gy oo op l oq or">fig, axs = plt.subplots(2, 2)<br/>fn = ["sepal_length", "sepal_width", "petal_length", "petal_width"]<br/>cn = ['setosa', 'versicolor', 'virginica']<br/>sns.boxplot(x = 'species', y = 'sepal_length', data = train, order = cn, ax = axs[0,0]);<br/>sns.boxplot(x = 'species', y = 'sepal_width', data = train, order = cn, ax = axs[0,1]);<br/>sns.boxplot(x = 'species', y = 'petal_length', data = train, order = cn, ax = axs[1,0]);<br/>sns.boxplot(x = 'species', y = 'petal_width', data = train,  order = cn, ax = axs[1,1]);<br/># add some spacing between subplots<br/>fig.tight_layout(pad=1.0);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/1535460a9e0d852659c22d329e48bddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*J7A4sYpqHTipO-LbfAPnSQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">并排箱形图</p></figure><p id="21fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">底部的两个图表明我们之前看到的那组数据点是<strong class="lb iu">setas。它们的花瓣尺寸比其他两种更小，也更不展开。比较其他两个物种，云芝的平均价值低于海滨锦鸡儿。</strong></p><p id="2488" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Violin plot 是另一种类型的可视化，它结合了直方图和箱线图的优点:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="3905" class="nx me it ok b gy oo op l oq or">sns.violinplot(x="species", y="petal_length", data=train, size=5, order = cn, palette = 'colorblind');</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/a8e2a0521832ea9455b70a961b0b703a.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*W84bN3qTOWXcsqicE9ZKbA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">花瓣长度的小提琴图</p></figure><p id="94d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以通过使用 seaborn 的<em class="nt"> pairplot </em>函数来制作所有成对属性的散点图:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="2bdb" class="nx me it ok b gy oo op l oq or">sns.pairplot(train, hue="species", height = 2, palette = 'colorblind');</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/a5773507ffd08ede2afd5d3bc5e5a8d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*NRn2BJRhAIKFNux3K5CoLg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">全配对属性散点图</p></figure><p id="9fbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，一些变量似乎是高度相关的，例如，花瓣长度和花瓣宽度。此外，花瓣的测量方法比萼片的测量方法更能区分不同的物种。</p><p id="0d42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们制作一个相关矩阵来定量考察变量之间的关系:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="e77a" class="nx me it ok b gy oo op l oq or">corrmat = train.corr()<br/>sns.heatmap(corrmat, annot = True, square = True);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/1a7977883a63eef0ace2ec3b6b1d67e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*zZF8RK1a8UK-1jKoortYQQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">相关矩阵</p></figure><p id="1603" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主要的收获是花瓣测量值具有<strong class="lb iu">高度正相关</strong>，而萼片测量值不相关。注意花瓣特征与萼片长度也有相对较高的相关性，但与萼片宽度没有相关性。</p><p id="d3fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个很酷的可视化工具是<a class="ae ky" href="https://docs.tibco.com/pub/spotfire/6.5.0/doc/html/para/para_what_is_a_parallel_coordinate_plot.htm#:~:text=A%20parallel%20coordinate%20plot%20maps,a%20plot%20is%20substantially%20different." rel="noopener ugc nofollow" target="_blank">平行坐标图</a>，它将每个样本表示为一条线。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="3f7c" class="nx me it ok b gy oo op l oq or">parallel_coordinates(train, "species", color = ['blue', 'red', 'green']);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/11af44b10e841f4acad313134881db1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*P931REPVBV07Q9d9za15oQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">平行坐标图</p></figure><p id="9669" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们之前看到的，花瓣测量比萼片测量更能区分物种。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="1599" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">构建分类器</strong></h1><p id="9539" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">现在我们准备建立一些分类器(呜-呼！)</p><p id="2a89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了使我们的生活更容易，让我们先把类标签和特性分离出来:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="130e" class="nx me it ok b gy oo op l oq or">X_train = train[['sepal_length','sepal_width','petal_length','petal_width']]<br/>y_train = train.species<br/>X_test = test[['sepal_length','sepal_width','petal_length','petal_width']]<br/>y_test = test.species</span></pre><p id="d46a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">分类树</strong></p><p id="f391" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我想到的第一个分类器是一个被称为分类树的区别性分类模型(在这里阅读更多<a class="ae ky" rel="noopener" target="_blank" href="/decision-trees-in-machine-learning-641b9c4e8052"/>)。原因是我们可以看到分类规则，并且很容易解释。</p><p id="988f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使用 sklearn ( <a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">文档</a>)构建一个，最大深度为 3，我们可以在测试数据上检查它的准确性:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="0748" class="nx me it ok b gy oo op l oq or">mod_dt = DecisionTreeClassifier(max_depth = 3, random_state = 1)<br/>mod_dt.fit(X_train,y_train)<br/>prediction=mod_dt.predict(X_test)<br/>print(‘The accuracy of the Decision Tree is’,”{:.3f}”.format(metrics.accuracy_score(prediction,y_test)))</span><span id="afaf" class="nx me it ok b gy ov op l oq or">--------------------------------------------------------------------<br/>The accuracy of the Decision Tree is 0.983.</span></pre><p id="0dee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该决策树正确预测了 98.3%的测试数据。该模型的一个优点是，您可以通过其<em class="nt"> feature_importances_ </em>属性看到每个预测值的重要性:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="290f" class="nx me it ok b gy oo op l oq or">mod_dt.feature_importances_</span><span id="0f21" class="nx me it ok b gy ov op l oq or">--------------------------------------------------------------------<br/>array([0.        , 0.        , 0.42430866, 0.57569134])</span></pre><p id="9f1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从输出和基于四个特征的索引，我们知道前两个特征(萼片测量)不重要，只有花瓣特征用于构建该树。</p><p id="037e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树的另一个好处是我们可以通过<em class="nt"> plot_tree </em>可视化分类规则:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="b014" class="nx me it ok b gy oo op l oq or">plt.figure(figsize = (10,8))<br/>plot_tree(mod_dt, feature_names = fn, class_names = cn, filled = True);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/fb7c77a966e1da54a577b11c2475374b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*PdiDCl_AvrqtDY7sPx0BwA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">此树中的分类规则(对于每个拆分，左-&gt;是，右-&gt;否)</p></figure><p id="8f1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了每个规则之外(例如，第一个标准是花瓣宽度≤ 0.7)，我们还可以看到在每个分割、指定类别等处的<a class="ae ky" href="http://www.learnbymarketing.com/481/decision-tree-flavors-gini-info-gain/#:~:text=Summary%3A%20The%20Gini%20Index%20is,partitions%20with%20many%20distinct%20values." rel="noopener ugc nofollow" target="_blank">基尼指数</a>(杂质测量)。注意，除了底部的两个“浅紫色”方框外，所有的终端节点都是纯的。对于这两类情况，我们可以不那么自信。</p><p id="476c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了演示对新数据点进行分类是多么容易，假设一个新实例的花瓣长度为 4.5 厘米，花瓣宽度为 1.5 厘米，那么我们可以根据规则预测它是杂色的。</p><p id="2bae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于只使用了花瓣特征，我们可以可视化决策边界并绘制 2D 的测试数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/c18a0150a6ce1752df92a898f64bbaf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*ZHeP-7m-FabK3G7j-a8dcQ.png"/></div></figure><p id="bb98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 60 个数据点中，59 个数据点被正确分类。显示预测结果的另一种方式是通过<a class="ae ky" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a>:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="20ad" class="nx me it ok b gy oo op l oq or">disp = metrics.plot_confusion_matrix(mod_dt, X_test, y_test,<br/>                                 display_labels=cn,<br/>                                 cmap=plt.cm.Blues,<br/>                                 normalize=None)<br/>disp.ax_.set_title('Decision Tree Confusion matrix, without normalization');</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/6e4d1ef22607672c537597f26c58317b.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*7nTD_E2B-NLi57eW0zSl4g.png"/></div></figure><p id="03c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这个矩阵，我们看到有一个我们预测是弗吉尼亚的云芝。</p><p id="7e6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个缺点是建造一个单独的树是它的<a class="ae ky" rel="noopener" target="_blank" href="/the-indecisive-decision-tree-story-of-an-emotional-algorithm-1-2-8611eea7e397"> <strong class="lb iu">不稳定</strong> </a>，这可以通过<a class="ae ky" href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="noopener ugc nofollow" target="_blank">集合</a>技术来改善，如随机森林，助推等。现在，让我们继续下一个模型。</p><p id="aa78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">高斯朴素贝叶斯分类器</strong></p><p id="31b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最流行的分类模型之一是朴素贝叶斯。它包含单词“Naive ”,因为它有一个关键的假设<strong class="lb iu">类-条件独立性</strong>,这意味着给定类，每个特性的值被假设为独立于任何其他特性的值(在这里阅读更多<a class="ae ky" rel="noopener" target="_blank" href="/naive-bayes-classifier-81d512f50a7c"/>)。</p><p id="2aa9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们知道，显然不是这种情况，花瓣特征之间的高度相关性证明了这一点。让我们用这个模型来检验测试的准确性，看看这个假设是否可靠:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="5927" class="nx me it ok b gy oo op l oq or">The accuracy of the Guassian Naive Bayes Classifier on test data is 0.933</span></pre><p id="4e9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们只使用花瓣特征，结果会怎样:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="d932" class="nx me it ok b gy oo op l oq or">The accuracy of the Guassian Naive Bayes Classifier with 2 predictors on test data is 0.950</span></pre><p id="8cd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有趣的是，仅使用两个特征会导致更正确的分类点，这表明在使用所有特征时可能会过度拟合。看来我们的朴素贝叶斯分类器做得不错。</p><p id="49d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">线性判别分析</strong></p><p id="aafb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们使用<strong class="lb iu">多变量高斯分布</strong>来计算类别条件密度，而不是采用单变量高斯分布的乘积(在朴素贝叶斯中使用)，那么我们将得到 LDA 模型(在此阅读更多<a class="ae ky" href="https://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">和</a>)。LDA 的关键假设是类之间的协方差相等。我们可以使用所有特征并且仅使用花瓣特征来检查测试准确性:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="1bfa" class="nx me it ok b gy oo op l oq or">The accuracy of the LDA Classifier on test data is 0.983<br/>The accuracy of the LDA Classifier with two predictors on test data is 0.933</span></pre><p id="edbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用所有特征提高了我们的 LDA 模型的测试精度。</p><p id="b093" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了可视化 2D 的决策边界，我们可以使用只有花瓣的 LDA 模型，并绘制测试数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/0be97183c8d60c6d14d1bce111cabfa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*QRSuB0ZVv2nVQjusQi4Cuw.png"/></div></figure><p id="c458" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">四个测试点被错误分类——三个 virginica 和一个 versicolor。</p><p id="cad4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在假设我们想用这个模型对新的数据点进行分类，我们可以在这个图上画出这个点，并根据它所属的彩色区域进行预测。</p><p id="c696" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">二次判别分析(QDA) </strong></p><p id="ee0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LDA 和 QDA 的区别在于，QDA 并不假设协方差在所有类中都是相等的，它被称为“二次的”,因为决策边界是二次函数。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="c974" class="nx me it ok b gy oo op l oq or">The accuracy of the QDA Classifier is 0.983<br/>The accuracy of the QDA Classifier with two predictors is 0.967</span></pre><p id="7975" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在所有特征的情况下，它与 LDA 具有相同的准确性，在仅使用花瓣时，它的性能略好。</p><p id="b711" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，让我们为 QDA(只有花瓣的模型)绘制决策边界:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/83c256499948243121eafa99dba04232.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*48Y6q9lfuE6entuv8c9UJw.png"/></div></figure><p id="9597" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> K 个最近邻居(K-NN) </strong></p><p id="6f20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们稍微转换一下话题，看看一个名为 KNN 的非参数生成模型(点击阅读更多<a class="ae ky" rel="noopener" target="_blank" href="/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761">)。这是一个流行的模型，因为它相对简单且易于实现。然而，当特征数量变大时，我们需要注意维度的</a><a class="ae ky" rel="noopener" target="_blank" href="/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d">诅咒</a>。</p><p id="7bef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们画出不同 K 选择的测试精度图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/38f173aaa420dd2bea811b49efa60ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*ElTbleTebsqVdGTkHW7zjA.png"/></div></figure><p id="0b52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，当 K 为 3，或者介于 7 和 10 之间时，精度最高(约为 0.965)。与之前的模型相比，对新数据点进行分类不太直接，因为我们需要查看四维空间中它的 K 个最近邻居。</p><p id="91b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">其他型号</strong></p><p id="9edc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我还探索了其他模型，如逻辑回归，支持向量机分类器等。详见我在 Github 上的代码。</p><p id="8717" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，<strong class="lb iu"> SVC(带线性内核)</strong>实现了 100%的测试准确率！</p><p id="e2ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在应该很有信心，因为我们的大多数模型的准确率都超过了 95%。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="f4ab" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">接下来的步骤</strong></h1><p id="dc90" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">以下是对未来研究的一些想法:</p><ol class=""><li id="c8e9" class="mv mw it lb b lc ld lf lg li nu lm nv lq nw lu nc nd ne nf bi translated">创建一个验证集并运行交叉验证，以获得准确的估计值，并比较它们之间的差值和平均准确度。</li><li id="5e57" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated">找到包括其他鸢尾属物种及其萼片/花瓣测量值的其他数据源(如果可能，也包括其他属性)，并检查新的分类准确性。</li><li id="587f" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated">制作一个交互式网络应用程序，根据用户输入的测量值预测物种(查看我的简单网络演示，Heroku 部署<a class="ae ky" href="https://dehao-iris-clf.herokuapp.com/" rel="noopener ugc nofollow" target="_blank">此处</a></li></ol></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="3e00" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">摘要</h1><p id="8f6a" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">让我们回顾一下。</p><p id="c1ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们研究了虹膜数据集，然后使用<em class="nt"> sklearn </em>构建了一些流行的分类器。我们看到花瓣的测量比萼片的测量更有助于实例的分类。此外，大多数模型的测试准确率都达到了 95%以上。</p><p id="5d16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢这篇博文，并请分享你的任何想法:)</p><p id="f6b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看我关于探索 Yelp 数据集的另一篇文章:</p><div class="pe pf gp gr pg ph"><a rel="noopener follow" target="_blank" href="/discover-your-next-favorite-restaurant-exploration-and-visualization-on-yelps-dataset-157d9799123c"><div class="pi ab fo"><div class="pj ab pk cl cj pl"><h2 class="bd iu gy z fp pm fr fs pn fu fw is bi translated">发现你下一个最喜欢的餐馆 Yelp 数据集上的探索和可视化</h2><div class="po l"><h3 class="bd b gy z fp pm fr fs pn fu fw dk translated">你用 Yelp 找好餐馆吗？这篇文章揭示了流行的 Yelp 数据集中的见解和模式。</h3></div><div class="pp l"><p class="bd b dl z fp pm fr fs pn fu fw dk translated">towardsdatascience.com</p></div></div><div class="pq l"><div class="pr l ps pt pu pq pv ks ph"/></div></div></a></div></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="0584" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">参考</h1><p id="7ef3" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">[1]<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/iris</a><br/>【2】<a class="ae ky" href="https://machinelearningmastery.com/machine-learning-in-python-step-by-step/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/machine-learning-in-python-step-by-step/</a></p></div></div>    
</body>
</html>