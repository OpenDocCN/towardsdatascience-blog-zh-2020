<html>
<head>
<title>Infinite Steps CartPole Problem With Variable Reward</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">奖励可变的无限步旋转木马问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/infinite-steps-cartpole-problem-with-variable-reward-7ad9a0dcf6d0?source=collection_archive---------44-----------------------#2020-06-03">https://towardsdatascience.com/infinite-steps-cartpole-problem-with-variable-reward-7ad9a0dcf6d0?source=collection_archive---------44-----------------------#2020-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c7c4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用继承修改撑杆跳开放式健身环境的步骤方法</h2></div><p id="a5fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae le" rel="noopener" target="_blank" href="/cartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7">上一篇博文</a>中，我们写了我们的第一个强化学习应用——cart pole 问题。我们使用深度 Q 网络来训练算法。正如我们在博客中看到的，固定奖励<em class="lf"> +1 </em>用于所有稳定状态，当翻筋斗失去平衡时，奖励<em class="lf"> 0 </em>。我们最后看到:当侧翻接近<em class="lf"> 200 </em>步时，往往会失去平衡。我们在博客结尾建议了一句话:最大步数(我们定义为 200)和固定奖励可能导致了这样的行为。今天，我们不限制步数和修改奖励，看看侧翻如何表现。</p><h1 id="7e66" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">电线杆问题定义</h1><p id="8d8d" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">当在<em class="lf"> 100 次</em>连续试验中平均奖励大于或等于<em class="lf"> 195.0 </em>时，认为横竿问题解决。这是考虑到<em class="lf"> 1.0 </em>的固定奖励。由于它的定义，对每个平衡状态保持固定的奖励<em class="lf"> 1.0 </em>并限制最大步数为<em class="lf"> 200 </em>是有意义的。很高兴知道这个问题在之前的<a class="ae le" rel="noopener" target="_blank" href="/cartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7">博客</a>中已经解决。</p><p id="cbf3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">横竿问题有以下终止发作的条件:</p><ol class=""><li id="4d91" class="md me it kk b kl km ko kp kr mf kv mg kz mh ld mi mj mk ml bi translated">磁极角度超过<em class="lf"> 12 </em>度。</li><li id="4d4c" class="md me it kk b kl mm ko mn kr mo kv mp kz mq ld mi mj mk ml bi translated">推车位置超过<em class="lf"> 2.4 </em> —推车中心到达显示屏边缘。</li></ol><h1 id="84df" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">可变奖励</h1><p id="0cb6" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">我们的目标是取消步数限制，给每个州一个可变的奖励。</p><p id="812c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果<em class="lf"> x </em>和<em class="lf"> θ </em>分别代表小车位置和杆角度，我们将奖励定义为:</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="f0b5" class="na lh it mw b gy nb nc l nd ne"><em class="lf">reward = (1 - (x ** 2) / 11.52 - (θ ** 2) / 288)</em></span></pre><p id="b089" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，小车位置和极角分量都被归一化到[0，1]区间，以给予它们相等的权重。让我们来看看<em class="lf"> 3D </em>图的<em class="lf"> 2D </em>视图的截图。</p><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nf"><img src="../Images/15dbcded16d7d467c7716539b7406c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwsyV1FfbdBr26Ch2WyCaA.png"/></div></div></figure><p id="7657" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在图中看到，当横竿完全平衡时(即<em class="lf"> x = 0 </em>和<em class="lf"> θ = 0 </em>)，奖励最大(即<em class="lf"> 1 </em>)。随着<em class="lf"> x </em>和<em class="lf"> θ </em>绝对值的增加，奖励减少，当<em class="lf"> |x| = 2.4 </em>和<em class="lf"> |θ| = 12 </em>时，奖励达到<em class="lf"> 0 </em>。</p><p id="e5d0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们将 CartPole 环境 gym 类(<em class="lf"> CartPoleEnv) </em>继承到我们的自定义类，<em class="lf"> CustomCartPoleEnv，</em>并覆盖<em class="lf">步骤</em>方法。在阶梯法中，我们写的是可变报酬，而不是固定报酬。</p><figure class="mr ms mt mu gt ng"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="0740" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过使用上面的代码块，制造了 TF-agent 的组件，并训练了深度 Q-网络。我们看到横翻在许多步后更加平衡和稳定。</p><h1 id="7f73" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">示范</h1><p id="46ac" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">让我们看看使用可变奖励后我们的横竿表现的视频。</p><figure class="mr ms mt mu gt ng"><div class="bz fp l di"><div class="np no l"/></div></figure><p id="a4cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一集平均持续<em class="lf"> 35.4 </em>秒。令人印象深刻，不是吗？</p><h1 id="bfd7" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">可能的改进</h1><p id="b4f2" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">这里，只有当两个表达式(杆角度和推车位置)都达到极值时，奖励才变为零。我们可以采用不同的回报函数，当达到一个极端条件时，回报为零。我期望这样的奖励函数做得更好。因此，鼓励读者尝试这样一个奖励功能，并评论一下横竿的表现。快乐的春天！</p></div></div>    
</body>
</html>