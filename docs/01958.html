<html>
<head>
<title>Principal Component Analysis — Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析—已解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-explained-d404c34d76e7?source=collection_archive---------14-----------------------#2020-02-24">https://towardsdatascience.com/principal-component-analysis-explained-d404c34d76e7?source=collection_archive---------14-----------------------#2020-02-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6549" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">详细的理论解释和 scikit-learn 示例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/b4e07214031871c9629b19a90a31915d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*wwjdwOwMDTR2uMFIe6abow.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">图源</a></p></figure><h1 id="885b" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated"><strong class="ak">主成分分析为什么重要？</strong></h1><p id="8653" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">随着数据科学的巨大进步，数据变得比以往任何时候都更有价值。现实生活中的数据集通常有许多要素(列)。一些特征可能是无信息的或者与其他特征相关。然而，我们可能事先不知道这一点，所以我们倾向于收集尽可能多的数据。在某些情况下，不使用所有功能也可以完成任务。由于计算和性能的原因，如果可能的话，希望用较少数量的特征来完成任务。无信息特征不提供任何预测能力，并且还导致计算负担。假设我们正在尝试预测篮球运动员的投篮命中率。数据集包括到篮筐的距离，方向的角度，防守者的位置，以前投篮的准确性和球的颜色。显而易见，球的颜色与投篮的准确性没有关系，所以我们可以把它去掉。现实生活中的情况不是那么明显，我们需要做一些预处理来确定无信息特征。使用软件包可以很容易地计算出特征之间或特征与目标变量之间的相关性。</p><p id="0942" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">也有一些案例具有大量的自然特征。例如，具有 8×8 像素图像的图像分类任务具有 64 个特征。我们可以找到一种方法，在不丢失大量信息的情况下，用较少的特征来表示这些图像。根据您工作的领域，您甚至可能会遇到包含一千多个要素的数据集。在这种情况下，减少功能的数量是一项具有挑战性但非常有益的任务。</p><p id="35a2" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">随着特征数量的增加，分类器的性能在某个点之后开始下降。更多的特征导致模型需要学习更多的组合，以便准确地预测目标。因此，对于相同数量的观测值(行),模型往往在要素数量较少的数据集上表现更好。此外，大量的特征增加了过度拟合的风险。</p><p id="4d3c" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">有两种主要的方法来减少特征的数量。第一个是<strong class="lp iu">特征选择</strong>，旨在找到最有信息的特征或消除无信息的特征。可以手动或使用软件工具选择功能。第二种方法是在保留尽可能多的信息的情况下，从现有的特性中派生出新的特性。这个过程叫做<strong class="lp iu">特征提取</strong>或者<strong class="lp iu">降维</strong>。</p><p id="8877" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我说的“保存尽可能多的信息”是什么意思？我们如何衡量信息量？答案是<strong class="lp iu">方差</strong>，这是一个变量被分散多少的度量。如果一个变量(特征)的方差很低，那么在建立模型时，它并不能告诉我们太多。下图显示了两个变量 x 和 y 的分布。正如您所看到的，x 的范围是从 1 到 6，而 y 的值介于 1 和 2 之间。在这种情况下，x 具有高方差。如果只有这两个特征来预测一个目标变量，那么 x 在预测中的作用远远高于 y。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/828dec2f89ce4fee20ea9c5cea2de6a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*UhS-zYFM1v5shKbnh6Tj6g.png"/></div></figure><p id="43cc" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在进行降维时，必须尽可能地保留当前数据集中的差异。降维的方法有很多。在这篇文章中，我将介绍一种最广泛使用的降维算法:<strong class="lp iu">【主成分分析】</strong>。</p><p id="76ea" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">PCA 是一种无监督学习算法，它在数据集中寻找特征之间的关系。它也被广泛用作监督学习算法的预处理步骤。</p><h1 id="e3e0" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated"><strong class="ak">PCA 是如何工作的？</strong></h1><p id="4c09" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated"><em class="mp">注意:PCA 是一种线性降维算法。也有非线性方法可用。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/ebe8d016ad9323d57e7c6872c364769c.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*IJdAhZMQ4Pi61FzoUAycJg.png"/></div></figure><p id="c339" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我们首先需要移动数据点，使数据中心位于原点。虽然单个数据点的位置会改变，但相对位置不会改变。例如，具有最高要素 1 值的点仍具有最高要素 1 值。然后，PCA 将直线拟合到数据，使数据点到直线的距离最小化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/39e1dd0e81a16e6137a41364a560f763.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*-Iyvlzq2BMoM1eBL4xkbtw.png"/></div></figure><p id="bd98" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这条红线是新的轴或第一个主分量(PC1)。数据集的大部分方差可以用 PC1 来解释。第二个主成分能够解释相对于 PC1 的垂直变化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/f563631ed38867b73cd0f0f14600a614.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*mwEgPihdhdITpc0f2Iy39w.png"/></div></figure><p id="91d3" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">排序红线是第二个主成分(PC2)。主成分的顺序根据它们所解释的原始数据集的方差分数来确定。很明显，PC1 比 PC2 能解释更多的差异。</p><p id="4165" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">然后旋转主分量和数据点，使得 PC1 成为新的 x 轴，PC2 成为新的 y 轴。数据点的相对位置不会改变。主分量彼此正交，因此线性无关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/72da37ba5aa237351baeb56c5c4cab28.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*18gZO7wOXKZXwWXy18QMVQ.png"/></div></figure><p id="1e39" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">主成分是原始数据集特征的线性组合。</p><p id="9bf0" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">PCA 的优点是使用比原始数据集少得多的特征保留了原始数据集的大量差异。主成分是根据它们所代表的方差来排序的。</p><p id="7b6f" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">让我们来看一个使用 scikit-learn 的例子。<a class="ae ku" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>是一个机器学习库，为预测数据分析提供简单高效的工具。</p><h1 id="e280" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated"><strong class="ak"> Scikit 学习实现</strong></h1><p id="73a6" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">为了保持一致，我将使用我从一开始就展示的数据点。这是一个非常简单的例子，但足以理解这个概念。</p><p id="07d6" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我们使用这些数据点创建一个数据帧，并为每个数据点分配一个类。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="b452" class="mz kw it mv b gy na nb l nc nd">import numpy as np<br/>import pandas as pd</span><span id="9ad9" class="mz kw it mv b gy ne nb l nc nd">df = pd.DataFrame({<br/>'feature_a':[2,1.5,2,2.5,3,2.5,3.7,2.8,1.8,3.3],<br/>'feature_b':[1,1.2,2,1.5,3,2.4,3.5,2.8,1.5,2.5],<br/>'target':['a','a','a','a','b','b','b','b','a','b']})</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/66639ac346a748767889982879c0fd3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*buYDDNwTfzSleqtaiijn-g.png"/></div></figure><p id="6a24" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">所以这是一个有两个独立变量的二元分类任务。</p><p id="c433" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在应用 PCA 之前，我们需要将数据标准化，使数据点的平均值为 0，方差为 1。Scikit-learn 提供了来自 sklearn 的<strong class="lp iu"> StandardScaler() </strong>.预处理导入 StandardScaler</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="158b" class="mz kw it mv b gy na nb l nc nd">from sklearn.preprocessing import StandardScaler</span><span id="c68a" class="mz kw it mv b gy ne nb l nc nd">df_features = df[['feature_a','feature_b']]<br/>df_features = StandardScaler().fit_transform(df_features)</span></pre><p id="1e32" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">然后，我们使用创建一个 PCA()对象，并拟合数据点。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="fb31" class="mz kw it mv b gy na nb l nc nd">from sklearn.decomposition import PCA</span><span id="d8af" class="mz kw it mv b gy ne nb l nc nd">pca = PCA(n_components=2)<br/>PCs = pca.fit_transform(df_features)</span></pre><p id="1089" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">然后，我们使用主成分创建一个新的数据帧:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="b2de" class="mz kw it mv b gy na nb l nc nd">#Data visualization libraries<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="3222" class="mz kw it mv b gy ne nb l nc nd">#Create DataFrame<br/>df_new = pd.DataFrame(data=PCs, columns={'PC1','PC2'})<br/>df_new['target'] = df['target'] #targets do not change</span></pre><p id="bd85" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我们可以绘制一个散点图来查看新的数据点:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="8df3" class="mz kw it mv b gy na nb l nc nd">fig = plt.figure(figsize = (8,4))<br/>ax = fig.add_subplot()<br/>ax.set_xlabel('PC1')<br/>ax.set_ylabel('PC2')</span><span id="7711" class="mz kw it mv b gy ne nb l nc nd">targets = ['a', 'b']<br/>colors = ['r', 'b']</span><span id="a30b" class="mz kw it mv b gy ne nb l nc nd">for target, color in zip(targets,colors):<br/>    rows = df_new['target'] == target<br/>    ax.scatter(df_new.loc[rows, 'PC1'],<br/>    df_new.loc[rows, 'PC2'],<br/>    ax.legend(targets)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/3f85a7c2175dd7e5b69e24a099c1b71d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*3kDrHDoXBQgo6WTsNI-KeA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">根据主成分的数据点</p></figure><p id="aad9" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">我们还可以绘制原始数据点的散点图，以便您可以清楚地看到数据点是如何转换的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/8cea4a582fa65ade429bef580d547c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*V7RuSnCNKKgaLJJ_lzI8bw.png"/></div></figure><p id="9863" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">正如您在主成分图中看到的，仅使用 PC1 就可以将两个类分开，而不用同时使用 feature_a 和 feature_b。因此，我们可以说 PC1 解释了大部分差异。确切地说，我们可以计算每个主成分对方差的解释程度。Scikit-learn 提供了<strong class="lp iu">解释 _ 方差 _ 比率 _ </strong>方法来计算这些金额:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="b15a" class="mz kw it mv b gy na nb l nc nd">pca.explained_variance_ratio_</span><span id="f668" class="mz kw it mv b gy ne nb l nc nd">array([0.93606831, 0.06393169])</span></pre><p id="a22f" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">PC1 解释了 93.6%的方差，PC2 解释了 6.4%。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/73d377b06d7a501d033223071780cd2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*mwrnp_QKHfu0pFuqyymlew.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">每个主成分解释的差异</p></figure><p id="a8ee" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><em class="mp">注:主成分是原始特征的线性组合。</em></p><p id="0510" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这个例子是一个非常简单的例子，但它解释了这个概念。当在具有更多特征的数据集上进行 PCA 时，我们只需遵循相同的步骤。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><p id="83f6" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="d51e" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">我的其他帖子</h1><p id="522b" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated"><strong class="lp iu">机器学习</strong></p><ul class=""><li id="5125" class="nq nr it lp b lq mj lt mk lw ns ma nt me nu mi nv nw nx ny bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/naive-bayes-classifier-explained-50f9723571ed">朴素贝叶斯分类器—解释</a></li><li id="025c" class="nq nr it lp b lq nz lt oa lw ob ma oc me od mi nv nw nx ny bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/logistic-regression-explained-593e9ddb7c6c">逻辑回归—已解释</a></li><li id="0745" class="nq nr it lp b lq nz lt oa lw ob ma oc me od mi nv nw nx ny bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/support-vector-machine-explained-8d75fe8738fd">支持向量机—解释</a></li><li id="9d16" class="nq nr it lp b lq nz lt oa lw ob ma oc me od mi nv nw nx ny bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/decision-tree-and-random-forest-explained-8d20ddabc9dd">决策树和随机森林—解释</a></li><li id="9fc8" class="nq nr it lp b lq nz lt oa lw ob ma oc me od mi nv nw nx ny bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/gradient-boosted-decision-trees-explained-9259bd8205af">梯度增强决策树—解释</a></li><li id="9a7f" class="nq nr it lp b lq nz lt oa lw ob ma oc me od mi nv nw nx ny bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/predicting-used-car-prices-with-machine-learning-fea53811b1ab">用机器学习预测二手车价格</a></li></ul><p id="6825" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><strong class="lp iu">数据分析</strong></p><ul class=""><li id="446c" class="nq nr it lp b lq mj lt mk lw ns ma nt me nu mi nv nw nx ny bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/the-most-underrated-tool-in-data-science-numpy-68d8fcbde524">数据科学中最被低估的工具:NumPy </a></li><li id="b477" class="nq nr it lp b lq nz lt oa lw ob ma oc me od mi nv nw nx ny bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/combining-dataframes-using-pandas-b9e2e83b9869">使用熊猫组合数据帧</a></li><li id="11e5" class="nq nr it lp b lq nz lt oa lw ob ma oc me od mi nv nw nx ny bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/handling-missing-values-with-pandas-b876bf6f008f">用熊猫处理缺失值</a></li><li id="28e1" class="nq nr it lp b lq nz lt oa lw ob ma oc me od mi nv nw nx ny bi translated"><a class="ae ku" href="https://medium.com/@soneryildirim1988/3-useful-functionalities-of-pandas-f4cb342a77ab" rel="noopener">熊猫的三大功能</a></li></ul></div></div>    
</body>
</html>