<html>
<head>
<title>Factorization Machines for Item Recommendation with Implicit Feedback Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于隐式反馈数据的项目推荐因式分解机</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/factorization-machines-for-item-recommendation-with-implicit-feedback-data-5655a7c749db?source=collection_archive---------1-----------------------#2020-06-28">https://towardsdatascience.com/factorization-machines-for-item-recommendation-with-implicit-feedback-data-5655a7c749db?source=collection_archive---------1-----------------------#2020-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="67a4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">超越经典的矩阵分解方法，包括用户/项目辅助特征，并直接优化项目排名顺序</h2></div><h2 id="0ed9" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">介绍</h2><p id="9de1" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">在本文中，我们将介绍因式分解机(FM ),它是一个灵活而强大的协同过滤推荐建模框架。然后，我们将描述直接优化项目排名顺序的专门损失函数如何使将 FM 模型应用于隐式反馈数据成为可能。我们将通过使用新的<a class="ae lu" href="https://github.com/etlundquist/rankfm" rel="noopener ugc nofollow" target="_blank">开源 FM 建模库</a>的<a class="ae lu" href="https://www.instacart.com/datasets/grocery-shopping-2017" rel="noopener ugc nofollow" target="_blank">真实世界隐式反馈数据集</a>来演示这些要点。向前！</p><h2 id="14c3" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">推荐系统</strong></h2><p id="e23f" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">随着数字经济规模的不断扩大和复杂性的不断增加，推荐系统为每个用户提供个性化的相关内容的作用比以往任何时候都更加重要。拥有越来越大的产品目录的电子商务网站可以同时向数百万用户呈现个性化的店面。数字内容提供商可以帮助用户浏览更多的书籍、文章、歌曲、电影等。比花一生的时间找到最符合每个用户特定兴趣和口味的小子集还要多。例如，网飞在 2015 年报告称，其推荐系统影响了网站上大约 80%的流媒体时间，并进一步估计该系统的价值每年超过 1B 美元。<a class="ae lu" href="#_edn1" rel="noopener ugc nofollow">【1】</a></p><p id="b9a2" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">推荐系统的两种广泛的高级方法是<strong class="ld ir">基于内容的过滤(CBF) </strong>和<strong class="ld ir">协同过滤(CF)。</strong> CBF 模型将用户和项目表示为<strong class="ld ir">属性</strong>或<strong class="ld ir">特征</strong>的向量(如用户年龄、状态、收入、活动水平；项目部门、类别、流派、价格)。相比之下，CF 方法仅依赖于<strong class="ld ir">过去的用户行为</strong>:该模型分析共现模式，以确定用户和/或项目的相似性，并试图仅使用用户记录的交互来推断用户对看不见的项目的偏好。基于 CF 的方法具有不受领域限制(即不需要特定的业务知识或特性工程)的优势，并且通常比 CBF 模型更准确和更具可扩展性。<a class="ae lu" href="#_edn2" rel="noopener ugc nofollow">【2】</a></p><h2 id="2dd4" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">矩阵分解</strong></h2><p id="0d32" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">许多最受欢迎和最成功的 CF 方法都是基于矩阵分解(MF)的，由于 2006-2009 Netflix 奖的举办，其发展速度迅速加快，获奖作品大量使用了 MF 技术，包括现在流行的 SVD++算法。<a class="ae lu" href="#_edn1" rel="noopener ugc nofollow">【3】</a>MF 模型试图学习用户和项目在共享潜在因素空间中的低维表示或嵌入。本质上，观察到的稀疏用户-项目交互矩阵被“分解”成包含用户和项目嵌入的两个低秩矩阵的近似乘积。在学习了这些潜在因素之后，可以计算用户/项目相似性，并且通过比较用户/项目潜在因素表示来推断未观察到的偏好。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/3def477b5ffd3bf718be3a8cd9c44fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8eGCNh7nYTvCUKViFp6DBQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者图片</p></figure><p id="b197" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">许多流行的 MF 算法通过最小化观察和预测评级之间的平方误差来学习这些用户/项目潜在因素，其中预测评级被计算为相关用户和项目潜在因素的内积。一些模型规范还包括全局用户/项目偏差和/或正则化项，以防止过度拟合。常见的 MF 损失函数可以表示为:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mq"><img src="../Images/a34c3f05d82b82431e555973f85f9a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0jFEjBlpkPvvf6tayeFnA.png"/></div></div></figure><h2 id="e1a8" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">隐性反馈</strong></h2><p id="2a2b" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">然而，当处理隐式反馈数据时，这种公式就失效了，在这种情况下，您不会直接观察到任何显式的数字评级或肯定/否定的响应，而只能观察到原始的用户行为(例如，观看、页面浏览、购买、点击)。隐式反馈数据在现实推荐环境中更为常见，事实上，仅使用显式反馈数据(即使存在)构建的推荐系统通常表现不佳，因为评级不是随机丢失的，而是与潜在的用户偏好高度相关。<a class="ae lu" href="#_edn1" rel="noopener ugc nofollow">【4】</a>为了使 MF 方法适应隐式反馈数据，胡等人<a class="ae lu" href="#_edn2" rel="noopener ugc nofollow">【5】</a>引入了评级的概念，作为二元隐含偏好(用户是否观察到该项目)，用数字置信权重表示该二元偏好的假定强度。这个模型公式是在<a class="ae lu" href="https://spark.apache.org/docs/latest/ml-collaborative-filtering.html" rel="noopener ugc nofollow" target="_blank"> SparkML </a>和<a class="ae lu" href="https://github.com/benfred/implicit" rel="noopener ugc nofollow" target="_blank">隐式</a> Python 库中实现的流行隐式反馈 ALS 算法的基础:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mr"><img src="../Images/5b23b484372cf310f722f9482d539930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AuJr2tSZ_-8k1lew49Jysw.png"/></div></div></figure><p id="4a58" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">虽然简单有效，但这种方法有几个主要缺点。首先，通过将数据表示为用户/项目交互的矩阵，不可能包括辅助特征，例如在 CBF 模型中使用的用户/项目属性和/或关于交互本身的其他上下文信息。当存在丰富的辅助功能时，这是一个重大的机会损失，也阻止了模型为新用户/项目生成信息性预测，通常称为<a class="ae lu" href="https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)" rel="noopener ugc nofollow" target="_blank">冷启动问题</a>。第二，将用户隐式反馈编码为二进制(0，1)评级并最小化模型的预测误差是用户偏好的非常间接的表示——你不能确定未被观察到的项目实际上对用户来说是负面的，当然一些正面(和负面)的项目比具有完全相同的编码(0，1)评级的其他项目更受欢迎。</p><p id="236f" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">考虑大多数推荐实际上是如何提供给用户的:作为一个或多个有序的项目列表。因此，直觉上，真正的目标应该是为每个用户导出正确的项目排序。这将允许我们生成推荐，其中所有项目都按照与每个用户的相关性排序，最相关的项目出现在每个用户的推荐项目列表的最顶端。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ms"><img src="../Images/8d47b252222b1d3c3248a519c2730d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mt7ff-dvb7agGmKk1DpyNA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者的亚马逊 Prime 视频主页</p></figure><p id="dce5" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">为了克服这些限制，我们需要一个更通用的模型框架，它可以扩展潜在因素方法，以纳入任意的辅助特征，以及使用隐式反馈数据直接优化项目排序的专用损失函数。进入<strong class="ld ir">因式分解机</strong>和<strong class="ld ir">学习排名</strong>。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="e790" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">因式分解机</strong></h2><p id="f407" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">因子分解机器(FM)是通用的监督学习模型，将任意实值特征映射到低维潜在因子空间，可以自然地应用于各种预测任务，包括回归、分类和排序。FMs 可以在非常稀疏的数据下准确估计模型参数，并以线性复杂度进行训练，允许它们扩展到非常大的数据集<a class="ae lu" href="#_edn1" rel="noopener ugc nofollow">【6】</a>——这些特性使 FMs 非常适合现实世界的推荐问题。与上面讨论的输入用户-项目交互矩阵的经典 MF 模型不同，FM 模型将用户-项目交互表示为实值特征向量和数字目标变量的元组-这种数据格式应该为任何训练过标准回归或分类模型的人所熟悉。</p><p id="a4ef" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">典型地，对于协同过滤，基本特征将是用户和项目指示符的二进制向量，使得每个训练样本恰好具有对应于给定用户/项目组合的两个非零条目。然而，这些用户/项目指示符可以用任意辅助特征来扩充，例如，用户或项目属性和/或与交互本身相关的上下文特征(例如，星期几、添加到购物车订单等)。).</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ms"><img src="../Images/f97095924622fb6eb1538c0e5d4b2bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-S4fQj-eEsAl1MWwzS_mGQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者图片</p></figure><p id="4c76" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">FM 模型方程由特征之间的 n 向相互作用组成。二阶模型(到目前为止最常见)包括每个基本特征的权重以及每个成对特征组合的交互项。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi na"><img src="../Images/d3073b57da3ef8107d625cd61b0acf26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*paJfgosykO7gKisZ9-f5zA.png"/></div></div></figure><p id="6bc3" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">这个模型公式可能看起来很熟悉——它只是一个二次线性回归。然而，与分别估计每个交互项的多项式线性模型不同，FMs 使用<strong class="ld ir">因式分解的交互参数</strong>:特征交互权重表示为两个特征的潜在因子空间嵌入的内积:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nb"><img src="../Images/b67874eedc9e27c1f475234a6fbaa246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ep6ZfFMna-e5V0Xz21yYA.png"/></div></div></figure><p id="ba3f" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">这极大地减少了要估计的参数数量，同时通过打破相互作用项之间严格的独立性标准来促进更精确的估计。考虑一个有 1，000，000 个用户和 10，000 个项目的现实推荐数据集。二次线性模型将需要估计 U+I+UI ~ 100 亿个参数。维度 F=10 的 FM 模型将只需要 U+I+F(U+I)~ 1100 万个参数。此外，许多常见的 MF 算法(包括 SVD++，ALS)可以重新制定为更通用/灵活的 FM 模型类的特例。<a class="ae lu" href="#_edn1" rel="noopener ugc nofollow">【7】</a></p><p id="01bd" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">然而，如何使 FM 模型适应隐式反馈数据并不是显而易见的。一种天真的方法是将所有观察到的用户-项目交互标记为(1)，将所有未观察到的交互标记为(-1)，并使用常见的分类损失函数(如铰链或对数损失)来训练模型。但是对于真实世界的推荐数据集，这将需要创建数十亿未观察到的用户项目训练样本，并且由于交互稀疏性而导致严重的类别不平衡。这种方法也具有与上面讨论的隐式反馈 MF 适应相同的概念问题:它仍然最小化评级预测误差，而不是直接优化项目排名顺序。</p><h2 id="e650" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">学习排名</strong></h2><p id="cb7f" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">直接学习排序顺序而不是最小化预测误差的优化技术被称为排序学习(LTR)。LTR 模型在成对或成列的训练样本上训练，而不是在单个观察值上训练。损失函数基于项目的相对顺序，而不是它们的原始分数。使用 LTR 的模型已经在搜索、信息检索和协同过滤方面产生了最先进的结果。这些技术是使 FM 模型适应隐式反馈推荐问题的关键。</p><p id="dfbe" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">用于项目推荐的最流行的 LTR 技术之一是<strong class="ld ir">贝叶斯个性化排名(BPR)。</strong> BPR 试图通过最大化模型参数的后验概率(MAP)来为每个用户学习项目的正确排序，给定观察到的用户项目偏好的数据集和选择的先验分布。假设每个用户观察到的项目(隐式反馈)优于未观察到的项目，并且假设所有成对的偏好是独立的。为了学习这些偏好，创建由[用户(u)，观察项目(I)，未观察项目(j)]元组组成的训练样本，并最大化关于模型参数的以下对数似然函数。<a class="ae lu" href="#_edn1" rel="noopener ugc nofollow">【第八期】</a></p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/5cb5a8100d432f6033927f01da3187db.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*9C6xqah8Mjevlgw-maAX0A.png"/></div></figure><p id="92cc" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">其中<strong class="ld ir"> ( &gt; u|theta) </strong>是模型对用户(u)的预测项目排名。这可以使用上述训练数据通过最大化用户观察到的项目优于他们未观察到的项目的联合概率来学习。我们可以使用 sigmoid 函数将该概率定义为映射到[0，1]的用户观察到的(I)和未观察到的(j)项目的预测效用分数之间的差异:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/303e43d74bfbab44237ea84ff3bb2bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*kWJGwt0vEJKZHPk1duc0KA.png"/></div></figure><p id="de0c" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">其中实值用户项目效用分数<strong class="ld ir"> f(u，i|theta) </strong>和<strong class="ld ir"> f(u，j|theta) </strong>是使用上面给出的主 FM 模型等式生成的。将所有这些放在一起并包括一个正则项，最大化的标准变成:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ne"><img src="../Images/c7c4ba95449b48a9ecbb4d7f321df12b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ah7hlqylEu7syan65Y98Lg.png"/></div></div></figure><p id="4447" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">更进一步，<strong class="ld ir">加权近似成对等级(WARP) </strong>不是简单地随机采样未观察到的项目(j)，而是对每个观察到的训练样本采样许多未观察到的项目，直到它为用户找到等级反转，从而产生更有信息的梯度更新。这在具有大量项目和高度倾斜的项目流行度(非常常见)的上下文中尤其重要。<a class="ae lu" href="#_edn1" rel="noopener ugc nofollow">【9】</a>基本程序是:</p><p id="2344" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">1.为用户随机抽取一个未观察到的项目，并计算其效用分数。如果未观察项目的分数超过观察项目的分数加上固定的余量，则进行梯度更新，否则继续对负项目进行采样</p><p id="f697" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">2.在发现边界违规之前，根据采样的负项数量调整渐变更新的幅度-如果采样的负项越多，更新幅度就越小，因为模型当前更有可能对用户偏好进行正确排序</p><p id="0115" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">事实上，如果您使用(0，1)乘数缩放梯度更新的幅度，BPR 可以被视为 WARP 的特殊情况，其中负样本的最大数量等于 1，导致梯度更新乘数恒定为 1。相对于 BPR，使用 WARP 会增加每个历元的训练时间，但通常会产生更快的收敛和更好的模型性能。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="7c3b" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">模型评估</strong></h2><p id="8a3b" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">现在，所有的理论都清楚了，让我们看看这些组件如何在一个众所周知的真实数据集上产生高质量的推荐。我们将使用作者的新<a class="ae lu" href="https://github.com/etlundquist/rankfm" rel="noopener ugc nofollow" target="_blank"> RankFM </a>包训练一个隐式反馈 FM 模型，该包实现了上述技术，并将其性能与之前讨论的流行的隐式反馈 ALS MF 算法进行比较(通过<a class="ae lu" href="https://github.com/benfred/implicit" rel="noopener ugc nofollow" target="_blank">隐式</a>包)。目标是表明，与类似指定的经典 MF 模型相比，包含辅助特征并使用 LTR 优化技术训练的 FM 模型产生更好的性能。</p><p id="c6ae" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">在本练习中，我们将使用<a class="ae lu" href="https://www.instacart.com/datasets/grocery-shopping-2017" rel="noopener ugc nofollow" target="_blank"> 2017 年 Instacart 订单数据</a>。它包含 200，000 个用户，50，000 个项目，340 万个订单，以及超过 3，200 万个记录的用户-项目交互。对数据的快速探索性研究揭示了高度的稀疏性:中间用户在 9 个订单中仅购买了 48 个项目，而中间项目在 60 个订单中仅被 35 个用户购买。总体交互稀疏率为 99.87%。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ms"><img src="../Images/adaebbe7fbe301107b54e4cddf4f500c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ncHqWsteLKxyoZLt_juhHQ.png"/></div></div></figure><p id="b3c5" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">我们将随机划分总体交互数据，用 75%来训练模型，剩下的 25%来评估验证指标和模型性能。为了反映重复购买携带更强偏好信号的想法，我们将结合使用每个用户商品购买计数的日志定义的样本权重。我们将用一组表示每个商品的超市部门和通道的商品特性来扩充主要的用户-商品交互数据。不幸的是，这个数据集没有用户辅助功能可以利用。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="7615" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">现在我们来训练 RankFM 模型。我们将使用 50 个潜在因素，最多 100 个负样本的偏差损失，以及一个逆比例学习计划，该计划将随着时间的推移降低学习速率，以帮助 SGD 收敛。RankFM 将打印每个训练时期的对数似然性，以便您可以逐个时期地监控训练时间和性能增益:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="d6b7" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">我们可以使用主 FM 模型方程和<strong class="ld ir"> predict() </strong>方法生成实值效用分数。没有出现在训练数据中的用户和项目可以被丢弃，或者在结果得分向量中将得分设置为<strong class="ld ir"> np.nan </strong>。我们可以使用<strong class="ld ir"> recommend() </strong>方法为验证集中的每个用户生成 TopN 个推荐项目。有一个有用的标志，让您选择是否包括以前观察到的训练项目或只生成新项目的建议。结果是一个 pandas 数据框架，其中包含 UserID 索引值和每个用户推荐的项目，这些项目按预期的偏好从左到右排列:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ms"><img src="../Images/0230742e1d6df7c9d40198ab54b57f0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*isdDKiDt5pep1qUF9vqCQQ.png"/></div></div></figure><p id="cc56" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">最后，我们将评估延迟验证数据的性能指标。RankFM 包括一个独立的<strong class="ld ir">评估</strong>模块，该模块实现了许多流行的推荐器指标，包括<strong class="ld ir">命中率</strong>、<strong class="ld ir">倒数排名</strong>、<strong class="ld ir">折扣累积收益</strong>和<strong class="ld ir">精度/召回</strong>。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="7b06" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">使用具有 50，000 个独特项目的数据集，其中中值用户购买少于 50 个项目，我们能够在很少的功能工程和所需的数据准备的情况下获得 80%以上的验证命中率。不算太寒酸。</p><h2 id="614f" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">基线模型对比</strong></h2><p id="fb92" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">现在，让我们将 RankFM 的性能与我们的基线 ALS 算法进行比较。为此，我们首先需要将用户-项目交互数据转换成一个稀疏的 CSR 矩阵。我们将使用完全相同的数据分割，并输入我们生成的样本权重作为置信度得分来训练 ALS 模型。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="969b" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">现在我们有了所需输入格式的数据，我们可以拟合 ALS 模型并为验证用户生成 TopN 建议。我们将使用与 RankFM 相同的潜在因子维度，否则使用默认值。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="6b7f" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">最后，我们将计算用于评估 RankFM 模型的同一组拒绝交互的模型的命中率、精确度和召回率。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="569b" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated">虽然完整的分析需要在各种超参数组合中适当调整两个模型，但我们可以看到，使用相同的相互作用数据和大致相等的模型规格(即潜在因素的数量)，FM 模型公式和 LTR 优化技术可以获得适度的性能增益(5–10%)。如果用户有更少的记录交互(在 Instacart 数据中，每个用户至少有 3 个订单)和/或我们有更多信息用户/商品辅助功能，我们可能会看到相对于基线 ALS MF 模型的更大性能增益。</p><h2 id="bfa2" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">总结</strong></h2><p id="bcea" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">在本文中，我们探索了因子分解机器(FM)作为一个通用而强大的模型框架，特别适合于协同过滤推荐问题。与传统的矩阵分解(MF)方法不同，FM 模型可以自然地扩展到包括用户、项目或上下文辅助特征，以增加主要交互数据。然后，我们展示了学习-排序(LTR)损失函数，如贝叶斯个性化排序(BPR)和加权近似成对排序(WARP)是如何成功地使 FM 模型适应隐式反馈数据的关键。为了证明这些观点，我们展示了一个隐式反馈 FM 模型，它在一个众所周知的开源隐式反馈推荐数据集上优于流行的 ALS MF 基线算法。最后，我们介绍了<a class="ae lu" href="https://github.com/etlundquist/rankfm" rel="noopener ugc nofollow" target="_blank"> RankFM </a>:一个新的 python 包，用于构建和评估带有隐式反馈数据的推荐问题的 FM 模型。</p><h2 id="d66e" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">参考文献</strong></h2><p id="7b70" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated"><a class="ae lu" href="#_ednref1" rel="noopener ugc nofollow">【1】</a>c .戈麦斯-乌里韦，n .亨特。<a class="ae lu" href="https://dl.acm.org/doi/10.1145/2843948" rel="noopener ugc nofollow" target="_blank">网飞推荐系统:算法、商业价值和创新</a> (2015)，美国计算机学会管理信息系统汇刊</p><p id="df31" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated"><a class="ae lu" href="#_ednref2" rel="noopener ugc nofollow">【2】</a>y .科伦，r .贝尔，c .沃林斯基。<a class="ae lu" href="https://ieeexplore.ieee.org/document/5197422" rel="noopener ugc nofollow" target="_blank">推荐系统的矩阵分解技术</a> (2009)，IEEE</p><p id="c037" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated"><a class="ae lu" href="#_ednref1" rel="noopener ugc nofollow">【3】</a>y .科伦，r .贝尔，c .沃林斯基。<a class="ae lu" href="https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf" rel="noopener ugc nofollow" target="_blank">奈飞奖的贝尔科尔解决方案</a> (2008 年)<a class="ae lu" href="http://www.netflixprize.com" rel="noopener ugc nofollow" target="_blank">www.netflixprize.com</a></p><p id="de7d" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated"><a class="ae lu" href="#_ednref1" rel="noopener ugc nofollow">【4】</a>h . Steck。<a class="ae lu" href="https://dl.acm.org/doi/10.1145/1835804.1835895" rel="noopener ugc nofollow" target="_blank">非随机缺失数据推荐系统的训练和测试</a> (2010)。KDD</p><p id="1226" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated"><a class="ae lu" href="#_ednref2" rel="noopener ugc nofollow"/>胡。<a class="ae lu" href="https://ieeexplore.ieee.org/document/4781121" rel="noopener ugc nofollow" target="_blank">隐式反馈数据集的协同过滤</a> (2008)。电气电子工程师学会</p><p id="be6b" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated"><a class="ae lu" href="#_ednref1" rel="noopener ugc nofollow"/><a class="ae lu" href="#_ednref1" rel="noopener ugc nofollow"/>s·伦德尔。<a class="ae lu" href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" rel="noopener ugc nofollow" target="_blank">因子分解机器</a> (2010)。ICDM 2010</p><p id="b040" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated"><a class="ae lu" href="#_ednref1" rel="noopener ugc nofollow">【7】</a>s·伦德尔。<a class="ae lu" href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" rel="noopener ugc nofollow" target="_blank">因子分解机器</a> (2010)。ICDM 2010</p><p id="336a" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated"><a class="ae lu" href="#_ednref1" rel="noopener ugc nofollow">【8】</a>s .伦德尔，c .弗赖登塔尔，z .甘特纳，l .施密特-蒂梅。<a class="ae lu" href="https://dl.acm.org/doi/10.5555/1795114.1795167" rel="noopener ugc nofollow" target="_blank"> BPR:来自隐式反馈的贝叶斯个性化排序</a> (2009)。UAI 2009</p><p id="18a9" class="pw-post-body-paragraph lb lc iq ld b le lv jr lg lh lw ju lj ko lx ll lm ks ly lo lp kw lz lr ls lt ij bi translated"><a class="ae lu" href="#_ednref1" rel="noopener ugc nofollow">【9】</a>s .伦德尔，c .科登泰勒。<a class="ae lu" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.587.3946&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">根据隐式反馈改进项目推荐的成对学习</a> (2014)。2014 年美国计算机学会</p></div></div>    
</body>
</html>