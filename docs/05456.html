<html>
<head>
<title>Exploring the Next Word Predictor!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索下一个单词预测器！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-the-next-word-predictor-5e22aeb85d8f?source=collection_archive---------5-----------------------#2020-05-08">https://towardsdatascience.com/exploring-the-next-word-predictor-5e22aeb85d8f?source=collection_archive---------5-----------------------#2020-05-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9785" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">深度学习|自然语言处理</h2><div class=""/><div class=""><h2 id="65e7" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">构建下一个单词预测器的不同方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e2129a86066ef4d1bd4a8d5d5b4f065a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GwlRNsLD4UWAm6Oh"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@freestocks?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">自由股票</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="99e7" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">语言预测导论</h1><p id="9fb1" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">你手机上的键盘怎么知道你下一步想输入什么？语言预测是一个自然语言处理NLP应用程序，涉及预测前面文本中给出的文本。自动完成或建议的回答是语言预测的流行类型。语言预测的第一步是选择语言模型。本文展示了在Whatsapp或任何其他消息应用程序中构建下一个单词预测器可以采用的不同方法。</p><p id="36c8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">通常有两种模型可以用来开发下一个单词的建议者/预测者:1) N-grams模型或2)长短期记忆(LSTM)。我们将仔细检查每种型号，并得出哪个型号更好的结论。</p><h1 id="40aa" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">n元语法方法</h1><p id="9989" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">如果你沿着<em class="nb"> n-grams </em>这条路走下去，你需要关注“马尔可夫链”来根据训练语料库预测每个后续单词或字符的可能性。下面是这种方法的代码片段。在这种方法中，一的序列长度被用来预测下一个字。这意味着我们将预测前一个单词中给出的下一个单词。</p><p id="0f8c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">导入必要的模块:<a class="ae lh" href="https://www.kite.com/python/docs/nltk.word_tokenize" rel="noopener ugc nofollow" target="_blank"><em class="nb">word _ tokenize</em></a><em class="nb">，</em><a class="ae lh" href="https://docs.python.org/2/library/collections.html#collections.defaultdict" rel="noopener ugc nofollow" target="_blank"><em class="nb">default dict</em></a><em class="nb">，</em> <a class="ae lh" href="https://docs.python.org/2/library/collections.html#collections.Counter" rel="noopener ugc nofollow" target="_blank"> <em class="nb">计数器</em> </a></p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="7c50" class="nh lj it nd b gy ni nj l nk nl">import re<br/>from nltk.tokenize import word_tokenize<br/>from collections import defaultdict, Counter</span></pre><p id="8f64" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">创建包含方法的类<em class="nb"> MarkovChain </em>:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="2ec9" class="nh lj it nd b gy ni nj l nk nl">class MarkovChain:<br/>  def __init__(self):<br/>    self.lookup_dict = defaultdict(list)</span><span id="e601" class="nh lj it nd b gy nm nj l nk nl">def _preprocess(self, string):<br/>    cleaned = re.sub(r’\W+’, ' ', string).lower()<br/>    tokenized = word_tokenize(cleaned)<br/>    return tokenized</span><span id="ffd6" class="nh lj it nd b gy nm nj l nk nl">def add_document(self, string):<br/>    preprocessed_list = self._preprocess(string)<br/>    pairs = self.__generate_tuple_keys(preprocessed_list)<br/>    for pair in pairs:<br/>      self.lookup_dict[pair[0]].append(pair[1])</span><span id="b7a1" class="nh lj it nd b gy nm nj l nk nl">def __generate_tuple_keys(self, data):<br/>    if len(data) &lt; 1:<br/>      return<br/>    for i in range(len(data) - 1):<br/>      yield [ data[i], data[i + 1] ]<br/>      <br/>  def generate_text(self, string):<br/>    if len(self.lookup_dict) &gt; 0:<br/>      print("Next word suggestions:", Counter(self.lookup_dict[string]).most_common()[:3])<br/>    return</span></pre><p id="eea9" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">当我们创建上述类的一个实例时，一个默认的字典被初始化。有一种方法可以对我们通过<em class="nb">添加的训练语料进行预处理。add_document() </em>方法。当我们在<em class="nb">的帮助下添加文档时。add_document() </em>方法，为每个唯一的单词创建对。让我们用一个例子来理解这一点:如果我们的训练语料库是“你好吗？从我们上次见面到现在有多少天了？你父母好吗？”在预处理和添加文档之后，我们的查找字典应该是:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="3f4d" class="nh lj it nd b gy ni nj l nk nl">{ 'how': ['are', 'many', 'are'], 'are': ['you', 'your'],<br/>  'you': ['how'], 'many': ['days'], 'days': ['since'],<br/>  'since': ['we'], 'we': ['last'], 'last': ['met'], 'met': ['how'],<br/>  'your': ['parents']}</span></pre><p id="739e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">每一个唯一的单词作为一个键和它后面的单词列表作为一个值被添加到我们的查找字典<em class="nb"> lookup_dict中。</em></p><p id="6973" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">当我们输入一个单词时，它会在字典中查找，并在下面的单词列表中给出最常用的单词。这里，单词建议的最大数量是三个，就像我们在键盘上一样。下面是序列长度为1的这种方法的运行示例。输出包含建议的单词及其在列表中各自的出现频率。当我们输入单词“how”时，会在字典中查找它，并从下面的单词列表中选择最常用的三个单词。这里，“许多”单词出现了1531次，这意味着单词序列“多少”在训练语料库中出现了1531次。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/28529a17e9ffa730630cbe9256c0245c.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/1*X0Uc185AXJkbESCiPhN4jA.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">n元模型的输出(序列长度为1)</p></figure><h1 id="c99f" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">在n元语法方法中:</h1><p id="2a65" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">此外，在上述方法中，我们可以具有2或3或更长的序列长度。为此，我们将不得不改变上面的一些代码。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="b11e" class="nh lj it nd b gy ni nj l nk nl">class MarkovChain:<br/>  """ <br/>  Previous code continued<br/>  """<br/>  def add_document(self, string):<br/>    preprocessed_list = self._preprocess(string)<br/>    pairs = self.__generate_tuple_keys(preprocessed_list)<br/>    for pair in pairs:<br/>      self.lookup_dict[pair[0]].append(pair[1])<br/>    pairs2 = self.__generate_2tuple_keys(preprocessed_list)<br/>    for pair in pairs2:<br/>      self.lookup_dict[tuple([pair[0], pair[1]])].append(pair[2])<br/>    pairs3 = self.__generate_3tuple_keys(preprocessed_list)<br/>    for pair in pairs3:<br/>      self.lookup_dict[tuple([pair[0], pair[1], pair[2]])].append(pair[3])<br/>  <br/>   def __generate_tuple_keys(self, data):<br/>    if len(data) &lt; 1:<br/>      return <br/>    for i in range(len(data) - 1):<br/>      yield [ data[i], data[i + 1] ]<br/>  <br/>  #to add two words tuple as key and the next word as value<br/>  def __generate_2tuple_keys(self, data):<br/>    if len(data) &lt; 2:<br/>      return<br/>    for i in range(len(data) - 2):<br/>      yield [ data[i], data[i + 1], data[i+2] ]<br/>  <br/>  #to add three words tuple as key and the next word as value <br/>  def __generate_3tuple_keys(self, data):<br/>    if len(data) &lt; 3:<br/>      return<br/>    for i in range(len(data) - 3):<br/>      yield [ data[i], data[i + 1], data[i+2], data[i+3] ]<br/>    <br/>  def oneword(self, string):<br/>    return Counter(self.lookup_dict[string]).most_common()[:3]</span><span id="35ee" class="nh lj it nd b gy nm nj l nk nl">def twowords(self, string):<br/>        suggest = Counter(self.lookup_dict[tuple(string)]).most_common()[:3]<br/>        if len(suggest)==0:<br/>            return self.oneword(string[-1])<br/>        return suggest</span><span id="acc6" class="nh lj it nd b gy nm nj l nk nl">def threewords(self, string):<br/>        suggest = Counter(self.lookup_dict[tuple(string)]).most_common()[:3]<br/>        if len(suggest)==0:<br/>            return self.twowords(string[-2:])<br/>        return suggest<br/>    <br/>  def morewords(self, string):<br/>        return self.threewords(string[-3:])</span><span id="0a88" class="nh lj it nd b gy nm nj l nk nl">def generate_text(self, string):<br/>    if len(self.lookup_dict) &gt; 0:<br/>        tokens = string.split(" ")<br/>        if len(tokens)==1:<br/>            print("Next word suggestions:", self.oneword(string))<br/>        elif len(tokens)==2:<br/>            print("Next word suggestions:", self.twowords(string.split(" ")))<br/>        elif len(tokens)==3:<br/>            print("Next word suggestions:", self.threewords(string.split(" ")))<br/>        elif len(tokens)&gt;3:<br/>            print("Next word suggestions:", self.morewords(string.split(" ")))<br/>    return</span></pre><p id="a645" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们来破解密码。方法<em class="nb">。__generate_2tuple_keys() </em>和<em class="nb">。__generate_3tuple_keys() </em>用于分别存储长度为2和3的序列及其后续单词列表。现在，我们的代码有能力根据之前的三个单词来预测单词。让我们看看我们新的查找字典<em class="nb"> lookup_dict </em>的例子:“你好吗？从我们上次见面到现在有多少天了？你父母好吗？”</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="3ca4" class="nh lj it nd b gy ni nj l nk nl">{ <br/>  """<br/>  Same as before<br/>  """<br/>  ('how', 'are'): ['you', 'your'],<br/>  ...<br/>  ('how', 'many'): ['days'],<br/>  ('many', 'days'): ['since'],<br/>  ...<br/>  ('how', 'are', 'you'): ['how'],<br/>  ...<br/>  ('how', 'many', 'days'): ['since'],<br/>  ...<br/>}</span></pre><p id="3c4f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">与前一对相比，新对被添加到字典中。我们上面创建的类<em class="nb"> MarkovChain </em>处理我们输入的任意长度的序列。如果我们输入一个单词，那么方法'<em class="nb"> oneword' </em>'将被调用，这将与前一个相同。对于长度为2或3的输入，将分别调用方法'<em class="nb">两个字</em>和'<em class="nb">三个字</em>'。这些方法的作用是在给定输入单词的情况下，从查找字典中查找最常见的三个单词。当输入单词多于四个时，将处理最后三个。当遇到未知单词时，该单词将被忽略，字符串的其余部分将被处理。看看下面的图来澄清任何疑问。这个数字是基于不同的训练语料库。左侧显示输入，右侧显示输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/a5d18334ef1fff8f275525677c9b0f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1GoGrEzYXYUg3TmcHOsjpA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们的n-grams模型的各种输入情况</p></figure><p id="6935" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">下面是这种方法的运行输出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/f0c470f06eababfb44102ba38fdcb4c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/1*p4aCoLc-qPPN_pg5Ea50WQ.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">n元模型的输出(序列长度超过1)</p></figure><p id="cc78" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">上面的输出是基于用于这种方法的一个不同的更大的数据集。GitHub对于这种方式的链接是<a class="ae lh" href="https://github.com/jackfrost1411/next_word_suggestor/tree/master/n-grams" rel="noopener ugc nofollow" target="_blank"> this </a>。你可以在那里找到上面的代码。</p><h2 id="112c" class="nh lj it bd lk nq nr dn lo ns nt dp ls mj nu nv lu mn nw nx lw mr ny nz ly iz bi translated">局限性:</h2><p id="7ab0" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">马尔可夫链没有记忆。采用这种方法有许多限制。举个例子，“我吃了这么多烤……”下一个单词“三明治”将基于“烤三明治”在训练数据中一起出现的次数来预测。因为我们得到的建议只是基于频率，所以在很多情况下这种方法可能会失败。</p><h1 id="e48c" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">长短期记忆(LSTM)方法:</h1><p id="5c0e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">使用神经语言模型的更高级的方法是使用长短期记忆(LSTM)。LSTM模型使用深度学习和管理记忆的人工“细胞”网络，使它们比传统的神经网络和其他模型更适合文本预测。</p><blockquote class="oa ob oc"><p id="5b1b" class="ma mb nb mc b md mw kd mf mg mx kg mi od my ml mm oe mz mp mq of na mt mu mv im bi translated">“草总是…”</p><p id="6fae" class="ma mb nb mc b md mw kd mf mg mx kg mi od my ml mm oe mz mp mq of na mt mu mv im bi translated">下一个词是简单的“绿色”，可以被大多数模型和网络预测。</p><p id="20ee" class="ma mb nb mc b md mw kd mf mg mx kg mi od my ml mm oe mz mp mq of na mt mu mv im bi translated">但是对于句子“现在是冬天，阳光很少，草总是……”，我们需要知道句子更后面的上下文来预测下一个单词“棕色”。</p></blockquote><p id="8467" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">当上下文和要预测的单词之间的差距增大时，标准RNNs和其他语言模型变得不太准确。这就是LSTM被用来解决长期依赖问题的时候，因为它有记忆细胞来记住以前的上下文。你可以在这里了解更多关于LSTM网络的信息。</p><p id="15ff" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们开始编码并定义我们的LSTM模型。在构建我们的模型时，首先，使用一个嵌入层，两个各有50个单位的堆叠LSTM层。</p><p id="1536" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">Keras提供了一个嵌入层，可用于文本数据上的神经网络。嵌入层用随机权重初始化，并学习训练数据集中所有单词的嵌入。它需要整数编码形式的输入数据。这个数据准备步骤可以在同样由Keras提供的<a class="ae lh" href="https://keras.io/preprocessing/text/#tokenizer" rel="noopener ugc nofollow" target="_blank"> Tokenizer API </a>的帮助下执行。点击了解更多关于嵌入层<a class="ae lh" href="https://keras.io/layers/embeddings/#embedding" rel="noopener ugc nofollow" target="_blank">的信息。</a></p><p id="0b71" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">两个LSTM层之后是两个完全连接的或密集的层。第一层有50个单元，第二个密集层是我们的输出(softmax)层，其单元数量等于词汇表大小。对于每个输入，模型将根据概率从我们的词汇中预测下一个单词。分类交叉熵被用作损失函数。</p><h2 id="e3dd" class="nh lj it bd lk nq nr dn lo ns nt dp ls mj nu nv lu mn nw nx lw mr ny nz ly iz bi translated">数据预处理:</h2><p id="3a59" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">对于嵌入层的输入，我们首先必须使用来自<em class="nb"> keras.processing.text </em>的Tokenizer来编码我们的输入字符串。我们在预处理中所做的很简单:我们首先创建特征字典的<em class="nb">序列。然后我们在记号赋予器的帮助下把它编码成整数形式。</em></p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="8250" class="nh lj it nd b gy ni nj l nk nl">from keras.preprocessing.text import Tokenizer<br/>import nltk<br/>from nltk.tokenize import word_tokenize<br/>import numpy as np<br/>import re<br/>from keras.utils import to_categorical<br/>from doc3 import training_doc3</span><span id="da50" class="nh lj it nd b gy nm nj l nk nl">cleaned = re.sub(r'\W+', ' ', training_doc3).lower()<br/>tokens = word_tokenize(cleaned)<br/>train_len = 4<br/>text_sequences = []</span><span id="09e2" class="nh lj it nd b gy nm nj l nk nl">for i in range(train_len,len(tokens)):<br/>  seq = tokens[i-train_len:i]<br/>  text_sequences.append(seq)</span><span id="cde2" class="nh lj it nd b gy nm nj l nk nl">sequences = {}<br/>count = 1</span><span id="7ddc" class="nh lj it nd b gy nm nj l nk nl">for i in range(len(tokens)):<br/>  if tokens[i] not in sequences:<br/>    sequences[tokens[i]] = count<br/>    count += 1</span><span id="5eb2" class="nh lj it nd b gy nm nj l nk nl">tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(text_sequences)<br/>sequences = tokenizer.texts_to_sequences(text_sequences)</span><span id="02e7" class="nh lj it nd b gy nm nj l nk nl">#vocabulary size increased by 1 for the cause of padding<br/>vocabulary_size = len(tokenizer.word_counts)+1<br/>n_sequences = np.empty([len(sequences),train_len], dtype='int32')</span><span id="fe6f" class="nh lj it nd b gy nm nj l nk nl">for i in range(len(sequences)):<br/>  n_sequences[i] = sequences[i]</span><span id="630b" class="nh lj it nd b gy nm nj l nk nl">train_inputs = n_sequences[:,:-1]<br/>train_targets = n_sequences[:,-1]<br/>train_targets = to_categorical(train_targets, num_classes=vocabulary_size)<br/>seq_len = train_inputs.shape[1]</span></pre><p id="a6a2" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们用一个例子来理解上面的代码中发生了什么:“你好吗？从我们上次见面到现在有多少天了？你父母好吗？”。我们首先清理我们的语料库，并借助nltk库中的<em class="nb">正则表达式、</em>和<em class="nb"> word_tokenize </em>对其进行标记。<em class="nb">【序列】</em>字典是做什么的？下面是使用记号赋予器之前的<em class="nb">‘sequences’</em>字典。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="ad79" class="nh lj it nd b gy ni nj l nk nl">{'how': 1, 'are': 2, 'you': 3, 'many': 4, 'days': 5, 'since': 6, 'we': 7, 'last': 8, 'met': 9, 'your': 10, 'parents': 11}</span></pre><p id="e19e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们的<em class="nb">‘text _ sequences’</em>列表保存了我们训练语料库中的所有序列，它将是:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="61c0" class="nh lj it nd b gy ni nj l nk nl">[['how', 'are', 'you', 'how'], ['are', 'you', 'how', 'many'], ['you', 'how', 'many', 'days'], ['how', 'many', 'days', 'since'], ['many', 'days', 'since', 'we'], ['days', 'since', 'we', 'last'], ['since', 'we', 'last', 'met'], ['we', 'last', 'met', 'how'], ['last', 'met', 'how', 'are'], ['met', 'how', 'are', 'your']]</span></pre><p id="111b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">使用tokenizer后，我们得到了编码形式的上述序列。这些数字只不过是重新分配前'<em class="nb">序列'</em>字典中相应单词的索引。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="e9a6" class="nh lj it nd b gy ni nj l nk nl">[[1, 2, 9, 1], [2, 9, 1, 3], [9, 1, 3, 4], [1, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7], [5, 6, 7, 8], [6, 7, 8, 1], [7, 8, 1, 2], [8, 1, 2, 10]]</span></pre><p id="0d3d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">一旦我们有了编码形式的序列，通过将序列分成输入和输出标签来定义训练数据和目标数据。对于此示例，我们将基于前三个单词预测下一个单词，因此在训练中，我们使用前三个单词作为输入，最后一个单词作为将由模型预测的标签。我们的'T12培训_输入'T13现在应该是:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="7d9f" class="nh lj it nd b gy ni nj l nk nl">[[1 2 9]  [2 9 1]  [9 1 3]  [1 3 4]  [3 4 5]  [4 5 6]  [5 6 7]  [6 7 8]  [7 8 1]  [8 1 2]]</span></pre><p id="d861" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">然后，我们将输出标签转换成一个热点向量，即0和1的组合。'<em class="nb"> train_targets' </em>中的单热点向量看起来像:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="fc1c" class="nh lj it nd b gy ni nj l nk nl">[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] <br/> ...<br/> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]</span></pre><p id="b695" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">对于第一个目标标签“how ”,序列字典中的索引是“1 ”,因此在编码形式中，您会在'<em class="nb"> train_targets' </em>的第一个独热向量中的索引1处看到“1”。</p><p id="0d61" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="nb">注意:以上代码是针对文本“你好吗？从我们上次见面到现在有多少天了？你父母好吗？”为了更简单的解释。但实际上，使用了更大的数据集。</em></p><h2 id="74be" class="nh lj it bd lk nq nr dn lo ns nt dp ls mj nu nv lu mn nw nx lw mr ny nz ly iz bi translated">构建模型:</h2><p id="4ac8" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">现在我们训练我们的<em class="nb">序列</em>模型，它有5层:一个嵌入层、两个LSTM层和两个密集层。在我们的模型的输入层，即嵌入层中，输入长度被设置为序列的大小，在这个例子中是3。<em class="nb">(注意:我们将训练输入和训练目标的数据分割为3比1，因此当我们为预测模型提供输入时，我们必须提供3个长度向量。)</em></p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="2daf" class="nh lj it nd b gy ni nj l nk nl">from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import LSTM<br/>from keras.layers import Embedding<br/>model = Sequential()<br/>model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))<br/>model.add(LSTM(50,return_sequences=True))<br/>model.add(LSTM(50))<br/>model.add(Dense(50,activation='relu'))<br/>model.add(Dense(vocabulary_size, activation='softmax'))</span><span id="18ea" class="nh lj it nd b gy nm nj l nk nl"># compiling the network<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>model.fit(train_inputs,train_targets,epochs=500,verbose=1)</span></pre><h2 id="e6ad" class="nh lj it bd lk nq nr dn lo ns nt dp ls mj nu nv lu mn nw nx lw mr ny nz ly iz bi translated">预测单词:</h2><p id="ff88" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在我们的模型被训练后，我们可以以编码的形式给出输入，并从<em class="nb"> softmax </em>函数中得到三个最可能的单词，如下所示。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="4f37" class="nh lj it nd b gy ni nj l nk nl">from keras.preprocessing.sequence import pad_sequences</span><span id="668f" class="nh lj it nd b gy nm nj l nk nl">input_text = input().strip().lower()<br/>encoded_text = tokenizer.texts_to_sequences([input_text])[0]<br/>pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')<br/>print(encoded_text, pad_encoded)</span><span id="618e" class="nh lj it nd b gy nm nj l nk nl">for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:<br/>  pred_word = tokenizer.index_word[i]<br/>  print("Next word suggestion:",pred_word)</span></pre><p id="995e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在上面的代码中，我们使用填充，因为我们在长度为3的序列上训练我们的模型，所以当我们输入5个单词时，填充将确保最后三个单词作为我们模型的输入。当我们输入少于3个单词时会发生什么？我们不会得到最好的结果！当我们输入一个未知的单词时，也会发生同样的情况，因为在这个单词的索引中，一键向量将包含0。我们将来可以做的是，将长度为2(输入)的序列添加到1(目标标签)中，将长度为1(输入)的序列添加到1(目标标签)中，就像我们在这里将长度为3(输入)的序列添加到1(目标标签)中一样，以获得最佳结果。</p><p id="ee7a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">下面是我们的模型根据前面的单词预测接下来的3个单词的最终输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a2b7c1c5acfb7bc755e9b4d29bb4ca41.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/1*IXsOp7LuVUzpLyNNCtrxGw.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">LSTM模型的输出</p></figure><p id="7b06" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">上面的输出显示了输入的向量形式以及建议的单词。[6，4，3]是<em class="nb">'编码_文本'</em>，[[6，4，3]]是'<em class="nb">填充_编码'</em>。</p><p id="0aec" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="nb">注意:这里我们将数据分为3(输入)比1(目标标签)。所以，一定要输入三个字。</em></p><p id="107f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">GitHub上面代码的链接是<a class="ae lh" href="https://github.com/jackfrost1411/next_word_suggestor/tree/master/LSTM" rel="noopener ugc nofollow" target="_blank">这个</a>。你可以在那里找到LSTM方法的代码。</p><h1 id="31bb" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">结论:</h1><p id="7aea" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">上面，我们看到n-grams方法不如LSTM方法，因为lstm具有从文本语料库中更远的地方记住上下文的记忆。在开始一个新项目时，您可能希望通过在互联网上寻找开源实现来考虑一个现有的预训练框架。这样，您就不必从头开始，也不需要担心训练过程或超参数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/23b83d18a174383256cbbb270bef5206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d0H7PQ6L6WWQ1Fai"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">乔希·里默尔在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="a7a8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">自然语言处理领域的许多最新工作包括开发和训练神经模型，以模拟人类大脑对语言的作用方式。这种深度学习方法使计算机能够以更有效的方式模仿人类语言。</p></div></div>    
</body>
</html>