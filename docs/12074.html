<html>
<head>
<title>Exploring AI at the Edge!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在边缘探索 AI！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-ia-at-the-edge-b30a550456db?source=collection_archive---------13-----------------------#2020-08-20">https://towardsdatascience.com/exploring-ia-at-the-edge-b30a550456db?source=collection_archive---------13-----------------------#2020-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bb36" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">树莓平台上基于 Tensorflow Lite 的图像识别、目标检测和姿态估计</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/3f36c1d808a2024eda95ddfe4d84bc7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*yVlHop7EmjLGfuByjgPNxw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者创作</p></figure><h1 id="997d" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">介绍</h1><p id="30fc" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo iu">什么是<em class="mi">边缘</em>(或者<em class="mi">雾</em>C<em class="mi">计算</em>？</strong></p><blockquote class="mj mk ml"><p id="1263" class="lm ln mi lo b lp mm ju lr ls mn jx lu mo mp lx ly mq mr mb mc ms mt mf mg mh im bi translated">Gartner 将边缘计算定义为:“分布式计算拓扑的一部分，其中信息处理位于靠近边缘的位置，即事物和人生产或消费信息的位置。”</p></blockquote><p id="3cc6" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">换句话说，边缘计算将计算(和一些数据存储)带到离生成或消费数据的设备更近的地方(尤其是实时)，而不是依赖于遥远的基于云的中央系统。使用这种方法，数据不会遇到延迟问题，从而降低了传输和处理的成本。在某种程度上，这是一种“回到最近的过去”，所有的计算工作都在本地的桌面上完成，而不是在云中。</p><p id="6651" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">边缘计算是由于连接到互联网的物联网设备的指数增长而发展起来的，这些设备要么从云中接收信息，要么将数据传送回云中。许多物联网(IoT)设备在运行过程中会产生大量数据。</p><p id="f2f2" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">边缘计算为物联网应用提供了新的可能性，特别是对于那些依赖机器学习(ML)来完成诸如对象和姿态检测、图像(和人脸)识别、语言处理和避障等任务的应用。图像数据是物联网的绝佳补充，但也是重要的资源消耗者(如电力、内存和处理)。图像处理“在边缘”，运行经典 AI/ML 模型，是一个巨大的飞跃！</p><h2 id="fa1e" class="mu kv it bd kw mv mw dn la mx my dp le lv mz na lg lz nb nc li md nd ne lk nf bi translated"><strong class="ak">tensor flow Lite——机器学习(ML)在边缘！！</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/2da4ab019163b99dd4b8dbd9470f953d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YVFETUuuOdHuHutIWnWYmA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:<a class="ae nl" href="https://blogs.gartner.com/paul-debeasi/2019/02/14/training-versus-inference/" rel="noopener ugc nofollow" target="_blank">机器学习训练对比推理</a> — Gartner</p></figure><p id="9986" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated"><strong class="lo iu">机器学习</strong>可以分为两个独立的过程:训练和推理，正如<a class="ae nl" href="https://blogs.gartner.com/paul-debeasi/2019/02/14/training-versus-inference/" rel="noopener ugc nofollow" target="_blank"> Gartner 博客</a>中所解释的:</p><ul class=""><li id="13ee" class="nm nn it lo b lp mm ls mn lv no lz np md nq mh nr ns nt nu bi translated"><strong class="lo iu">训练:</strong>训练是指创建机器学习算法的过程。训练涉及使用深度学习框架(例如 TensorFlow)和训练数据集(见上图左侧)。物联网数据提供了一个训练数据源，数据科学家和工程师可以使用它来训练各种情况下的机器学习模型，从故障检测到消费者智能。</li><li id="e4ef" class="nm nn it lo b lp nv ls nw lv nx lz ny md nz mh nr ns nt nu bi translated"><strong class="lo iu">推理:</strong>推理是指使用经过训练的机器学习算法进行预测的过程。物联网数据可以用作经过训练的机器学习模型的输入，实现预测，这些预测可以指导设备上、边缘网关上或物联网系统中其他地方的决策逻辑(见上图右侧)。</li></ul><p id="4012" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated"><a class="ae nl" href="https://www.tensorflow.org/lite" rel="noopener ugc nofollow" target="_blank"> <strong class="lo iu"> TensorFlow Lite </strong> </a>是一个开源的深度学习框架，支持<strong class="lo iu">在设备上的机器学习推理</strong>，具有低延迟和小二进制大小。它旨在使在网络“边缘”的设备上执行机器学习变得容易，而不是从服务器来回发送数据。</p><p id="7270" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">在设备上执行机器学习有助于改善:</p><ul class=""><li id="8b76" class="nm nn it lo b lp mm ls mn lv no lz np md nq mh nr ns nt nu bi translated"><em class="mi">延迟:</em>没有到服务器的往返</li><li id="0e58" class="nm nn it lo b lp nv ls nw lv nx lz ny md nz mh nr ns nt nu bi translated"><em class="mi">隐私:</em>没有数据需要离开设备</li><li id="f5e3" class="nm nn it lo b lp nv ls nw lv nx lz ny md nz mh nr ns nt nu bi translated"><em class="mi">连接:</em>不需要互联网连接</li><li id="78f5" class="nm nn it lo b lp nv ls nw lv nx lz ny md nz mh nr ns nt nu bi translated"><em class="mi">功耗:</em>网络连接非常耗电</li></ul><p id="e1ba" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">TensorFlow Lite (TFLite)由两个主要组件组成:</p><ul class=""><li id="8336" class="nm nn it lo b lp mm ls mn lv no lz np md nq mh nr ns nt nu bi translated"><strong class="lo iu"> TFLite 转换器</strong>，它将 TensorFlow 模型转换成高效的形式供解释器使用，并可以引入优化来提高二进制大小和性能。</li><li id="7a45" class="nm nn it lo b lp nv ls nw lv nx lz ny md nz mh nr ns nt nu bi translated">TFLite 解释器在许多不同的硬件类型上运行，包括手机、嵌入式 Linux 设备和微控制器。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/36fb7156e8dfd5f189c935d955b67020.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*qJ78tQhg1rKOh1xCcUeXvg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:TensorFlow Lite —在边缘设备部署模型</p></figure><p id="b20a" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">总之，一个经过训练和保存的 TensorFlow 模型(如<em class="mi"> model.h5 </em>)可以使用 TFLite FlatBuffer(如<em class="mi"> model.tflite </em>)中的<strong class="lo iu"> TFLite Converter </strong>进行转换，该转换器将由 Edge 设备内部的<strong class="lo iu"> TF Lite 解释器</strong>使用(作为一个 Raspberry Pi)，以对新数据进行推理。</p><p id="193e" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">例如，我在我的 Mac(上图中的“服务器”)中从头训练了一个简单的 CNN 图像分类模型。最终模型有 225，610 个参数要训练，使用 CIFAR10 数据集作为输入:60，000 个图像(形状:32，32，3)。经过训练的模型(<em class="mi"> cifar10_model.h5 </em>)的大小为 2.7Mb。使用 TFLite 转换器，在 Raspberry Pi 上使用的模型(<em class="mi"> model_cifar10.tflite </em>)的大小变为 905Kb(约为原始大小的 1/3)。用两个模型进行推断(h5 在 Mac 和。RPi 处的 tflite)留下相同的结果。两款笔记本都可以在<a class="ae nl" href="https://github.com/Mjrovai/TFLite_IA_at_the_Edge" rel="noopener ugc nofollow" target="_blank"> GitHub </a>找到。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ob"><img src="../Images/9617195cd67cfafab1fcc100faabceda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9_13w6PWlTDAqEa4fT3GJw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者笔记本</p></figure><h1 id="93e4" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">Raspberry Pi — TFLite 安装</h1><p id="91bf" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">也可以在 Raspberry Pi 从头开始训练模型，为此，需要完整的 TensorFlow 包。但是一旦我们要做的只是<strong class="lo iu">推理</strong>部分，我们将只安装 TensorFlow Lite 解释器。</p><blockquote class="mj mk ml"><p id="8649" class="lm ln mi lo b lp mm ju lr ls mn jx lu mo mp lx ly mq mr mb mc ms mt mf mg mh im bi translated">仅解释器包的大小是整个 TensorFlow 包的一小部分，并且包括使用 TensorFlow Lite 运行推理所需的最少代码。它只包含用于执行<code class="fe oc od oe of b">.tflite</code>模型的<code class="fe oc od oe of b"><a class="ae nl" href="https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter" rel="noopener ugc nofollow" target="_blank">tf.lite.Interpreter</a></code> Python 类。</p></blockquote><p id="ec51" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">让我们在 Raspberry Pi 打开终端，安装您的特定系统配置所需的<a class="ae nl" href="https://pythonwheels.com/" rel="noopener ugc nofollow" target="_blank"> Python wheel </a>。选项可以在这个链接上找到:<a class="ae nl" href="https://www.tensorflow.org/lite/guide/python" rel="noopener ugc nofollow" target="_blank"> Python 快速入门</a>。例如，在我的例子中，我运行的是 Linux arm 32(Raspbian Buster—Python 3.7)，所以命令行是:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="6f7e" class="mu kv it of b gy ok ol l om on">$ sudo pip3 install <a class="ae nl" href="https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl" rel="noopener ugc nofollow" target="_blank">https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl</a></span></pre><p id="0539" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">如果您想仔细检查 Raspberry Pi 中的操作系统版本，请运行以下命令:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="9213" class="mu kv it of b gy ok ol l om on">$ uname -</span></pre><p id="f057" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">如下图所示，如果你得到… <em class="mi"> arm7l… </em>，操作系统是 32 位 Linux。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi oo"><img src="../Images/2dd378d44cd1aed86aa85dc4587f6844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vHsX8_FHwa-re-HX0m3gsg.png"/></div></div></figure><p id="8f54" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">安装 Python wheel 是让 TFLite 解释器在 Raspberry Pi 中工作的唯一要求。可以在终端调用 TFLite 解释器来仔细检查安装是否正常，如下所示。如果没有错误出现，我们是好的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi op"><img src="../Images/65c99b815942cab76f7f0040c685cdfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gIDJpWAsTPB_KFz-c87R_A.png"/></div></div></figure><h1 id="fee1" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">图像分类</h1><h2 id="8e00" class="mu kv it bd kw mv mw dn la mx my dp le lv mz na lg lz nb nc li md nd ne lk nf bi translated">介绍</h2><p id="477d" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">应用于计算机视觉(CV)的人工智能的更经典的任务之一是图像分类。从 2012 年开始，当一个名为<a class="ae nl" href="https://en.wikipedia.org/wiki/AlexNet" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>的<a class="ae nl" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a> (CNN)(为了纪念其领先的开发者 Alex Krizhevsky)在 ImageNet 2012 挑战赛中取得了 15.3%的前 5 名误差时，信息架构和深度学习(DL)永远改变了。根据《经济学家》杂志的报道，“人们突然开始关注(人工智能)，不仅仅是在人工智能领域，而是在整个技术行业</p><p id="90be" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">这个项目，在 Alex Krizhevsk，一个更现代的架构(<a class="ae nl" href="https://arxiv.org/pdf/1704.04861.pdf" rel="noopener ugc nofollow" target="_blank"> MobileNet </a>)的近八年后，也使用相同的数据集<a class="ae nl" href="https://en.wikipedia.org/wiki/ImageNet" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>对数百万张图像进行了预训练，产生了 1000 个不同的类。这个预先训练和量化的模型是这样的，转换成一个. tflite 并在这里使用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi oq"><img src="../Images/fca2f0236065ac3cf4b7fdb9b57ee48a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D_YKkUUM3lD_kFFwBIzzWw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者创作</p></figure><p id="3ce4" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">首先，让我们在 Raspberry Pi 上移动到一个工作目录(例如，<em class="mi"> Image_Recognition </em>)。接下来，必须创建两个子目录，一个用于模型，另一个用于图像:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="e76f" class="mu kv it of b gy ok ol l om on">$ mkdir images<br/>$ mkdir models</span></pre><p id="f09c" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">一旦进入模型目录，让我们下载预先训练好的模型(在这个<a class="ae nl" href="https://www.tensorflow.org/lite/guide/hosted_models" rel="noopener ugc nofollow" target="_blank">链接</a>，可以下载几个不同的模型)。我们将使用量化的 Mobilenet V1 模型，用 224x224 像素的图像进行预训练。可以从<a class="ae nl" href="https://www.tensorflow.org/lite/models/image_classification/overview" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite 图片分类</a>下载的 zip 文件，使用<em class="mi"> wget </em>:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="e93e" class="mu kv it of b gy ok ol l om on">$ cd models<br/>$ wget <a class="ae nl" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip</a></span></pre><p id="ef6c" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">接下来，解压缩文件:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="a4e6" class="mu kv it of b gy ok ol l om on">$ unzip mobilenet_v1_1.0_224_quant_and_labels</span></pre><p id="87f6" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">下载了两个文件:</p><ul class=""><li id="d138" class="nm nn it lo b lp mm ls mn lv no lz np md nq mh nr ns nt nu bi translated"><strong class="lo iu">mobilenet _ v1 _ 1.0 _ 224 _ quant . TF Lite</strong>:tensor flow-Lite 转换模型</li><li id="ebf7" class="nm nn it lo b lp nv ls nw lv nx lz ny md nz mh nr ns nt nu bi translated"><strong class="lo iu">Labels _ mobilenet _ quant _ v1 _ 224 . txt</strong>:ImageNet 数据集 1000 个类标签</li></ul><p id="a7b7" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">现在，获取一些图像(例如。png，。jpg)并将它们保存在创建的图像子目录中。</p><blockquote class="mj mk ml"><p id="9ecd" class="lm ln mi lo b lp mm ju lr ls mn jx lu mo mp lx ly mq mr mb mc ms mt mf mg mh im bi translated">在<a class="ae nl" href="https://github.com/Mjrovai/TFLite_IA_at_the_Edge" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上，可以找到本教程使用的图片。</p></blockquote><h2 id="60b6" class="mu kv it bd kw mv mw dn la mx my dp le lv mz na lg lz nb nc li md nd ne lk nf bi translated">Raspberry Pi OpenCV 和 Jupyter 笔记本安装</h2><p id="f028" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">OpenCV(开源计算机视觉库)是一个开源的计算机视觉和机器学习软件库。在处理图像时，这是一种有益的支持。如果在 Mac 或 PC 上安装它非常简单，那么在 Raspberry Pi 上安装就有点“技巧”了，但是我推荐使用它。</p><p id="cac2" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">请按照 Q-Engineering 的这个很棒的教程在你的树莓 Pi 上安装 OpenCV:在树莓 Pi 4 上安装 OpenCV 4.4.0。尽管该指南是为 Raspberry Pi 4 编写的，但也可以不做任何修改地用于 Raspberry 3 或 2。</p><p id="6a18" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">接下来，安装 Jupyter 笔记本。它将成为我们的开发平台。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="3b6b" class="mu kv it of b gy ok ol l om on">$ sudo pip3 install jupyter<br/>$ jupyter notebook</span></pre><p id="9457" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">此外，在 OpenCV 安装过程中，应该已经安装了 NumPy，如果现在没有安装，与 MatPlotLib 相同。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="fbbc" class="mu kv it of b gy ok ol l om on">$ sudo pip3 install numpy<br/>$ sudo apt-get install python3-matplotlib</span></pre><p id="0d44" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">完成了！我们已经准备好开始我们的人工智能之旅了！</p><h2 id="8353" class="mu kv it bd kw mv mw dn la mx my dp le lv mz na lg lz nb nc li md nd ne lk nf bi translated">图像分类推理</h2><p id="4463" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">创建一个新的 Jupyter 笔记本并遵循以下步骤，或者从 GitHub 下载完整的笔记本。</p><p id="c07e" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">导入库:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="1f4d" class="mu kv it of b gy ok ol l om on">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import cv2<br/>import tflite_runtime.interpreter as tflite</span></pre><p id="f27e" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">加载 TFLite 模型并分配张量:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="211d" class="mu kv it of b gy ok ol l om on">interpreter = tflite.Interpreter(model_path=’./models/mobilenet_v1_1.0_224_quant.tflite’)<br/>interpreter.allocate_tensors()</span></pre><p id="e45b" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">获取输入和输出张量:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="22ae" class="mu kv it of b gy ok ol l om on">input_details = interpreter.get_input_details()<br/>output_details = interpreter.get_output_details()</span></pre><p id="7ef1" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated"><strong class="lo iu">输入细节</strong>将为您提供所需的信息，告诉您应该如何为模型输入图像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi or"><img src="../Images/13577da47516f65b1f44a60be8470ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pHyhvJgHg3x7RdB7pJWemw.png"/></div></div></figure><p id="7f06" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">(1，224x224x3)的形状通知一个尺寸为:(224x224x3)的图像应该一个一个输入(批量尺寸:1)。dtype uint8 表示这些值是 8 位整数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi os"><img src="../Images/5b14360345dbf3b3402296dac5c132ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fl2j1tezjJuFfxgJA7jojA.png"/></div></div></figure><p id="b27f" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated"><strong class="lo iu">输出细节</strong>显示推理将产生 1001 个整数值(8 位)的数组。这些值是图像分类的结果，其中每个值是特定标签与图像相关的概率。</p><p id="e786" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">例如，假设我们想要对形状为(1220，1200，3)的图像进行分类。首先，我们需要将其调整为(224，224，3)并添加一个批处理维度 1，如输入细节中所定义的:(1，224，224，3)。推断结果将是一个大小为 1001 的数组，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ot"><img src="../Images/4f5257451e93e1edfffef495456c8be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sv544LT6ErZ9piDpgB28dA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者创作—图片分类主要步骤</p></figure><p id="b24c" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">对这些操作进行编码的步骤是:</p><ol class=""><li id="3f3e" class="nm nn it lo b lp mm ls mn lv no lz np md nq mh ou ns nt nu bi translated">输入图像并将其转换为 RGB (OpenCV 将图像读取为 BGR 图像):</li></ol><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="a0c4" class="mu kv it of b gy ok ol l om on">image_path = './images/cat_2.jpg'<br/>image = cv2.imread(image_path)<br/>img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span></pre><p id="d6e1" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">2.图像预处理、整形和添加批量尺寸:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="79af" class="mu kv it of b gy ok ol l om on">img = cv2.resize(img, (224, 224))<br/>input_data = np.expand_dims(img, axis=0)</span></pre><p id="8e5e" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">3.指向用于测试的数据并运行解释器:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="5fbc" class="mu kv it of b gy ok ol l om on">interpreter.set_tensor(input_details[0]['index'], input_data)<br/>interpreter.invoke()</span></pre><p id="c25e" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">4.获取结果并将它们映射到类别:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="b029" class="mu kv it of b gy ok ol l om on">predictions = interpreter.get_tensor(output_details[0][‘index’])[0]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/7421d3b058b97055b14e718793f72ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*IIkNLsxm2JryrXkgjnlgEg.png"/></div></figure><p id="4dc8" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">输出值(预测值)从 0 到 255 不等(8 位整数的最大值)。要获得范围从 0 到 1 的预测，输出值应除以 255。与最高值相关的数组索引是这种图像最可能的分类。</p><p id="29a1" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">有了索引，我们必须找到它指定的类别(如汽车、猫或狗)。与模型一起下载的文本文件有一个与从 0 到 1，000 的每个索引相关联的标签。</p><p id="2023" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">让我们首先创建一个函数来加载。txt 文件作为字典:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="4593" class="mu kv it of b gy ok ol l om on">def load_labels(path):<br/>    with open(path, 'r') as f:<br/>        return {i: line.strip() for i, line in enumerate(f.readlines())}</span></pre><p id="d074" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">创建一个名为<em class="mi">标签</em>的字典，并检查其中的一些标签:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="5fc4" class="mu kv it of b gy ok ol l om on">labels = load_labels('./models/labels_mobilenet_quant_v1_224.txt')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/55583d17dd1d78c8ac243f35ca9aa5dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*RwCqJ7KD6x8ssIOq7eqqXA.png"/></div></figure><p id="8021" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">回到我们的例子，让我们得到前 3 个结果(最高概率):</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="b43f" class="mu kv it of b gy ok ol l om on">top_k_indices = 3<br/>top_k_indices = np.argsort(predictions)[::-1][:top_k_results]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/a234099c29a7458d209543f133985031.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*N6nsw9S4HHpMKNN4_4cAdQ.png"/></div></figure><p id="c347" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">我们可以看到，前 3 个指数都与猫有关。预测内容是与每个标签相关联的概率。如前所述，除以 255。，我们可以得到一个从 0 到 1 的值。让我们创建一个循环来检查顶部结果，打印标签和概率:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="eb04" class="mu kv it of b gy ok ol l om on">for i in range(top_k_results):<br/>    print("\t{:20}: {}%".format(<br/>        labels[top_k_indices[i]],<br/>        int((predictions[top_k_indices[i]] / 255.0) * 100)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f44d61687831777c7feb9a666a0c14c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*pqxaTpJ1SFRbABNRaoftmA.png"/></div></figure><p id="bf6a" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">让我们创建一个函数，来平滑地对不同的图像执行推理:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="2c4b" class="mu kv it of b gy ok ol l om on">def image_classification(image_path, labels, top_k_results=3):<br/>    image = cv2.imread(image_path)<br/>    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/>    plt.imshow(img)</span><span id="cfea" class="mu kv it of b gy oz ol l om on">img = cv2.resize(img, (w, h))<br/>    input_data = np.expand_dims(img, axis=0)</span><span id="8123" class="mu kv it of b gy oz ol l om on">interpreter.set_tensor(input_details[0]['index'], input_data)<br/>    interpreter.invoke()<br/>    predictions = interpreter.get_tensor(output_details[0]['index'])[0]</span><span id="8e9b" class="mu kv it of b gy oz ol l om on">top_k_indices = np.argsort(predictions)[::-1][:top_k_results]</span><span id="d5d4" class="mu kv it of b gy oz ol l om on">print("\n\t[PREDICTION]        [Prob]\n")<br/>    for i in range(top_k_results):<br/>        print("\t{:20}: {}%".format(<br/>            labels[top_k_indices[i]],<br/>            int((predictions[top_k_indices[i]] / 255.0) * 100)))</span></pre><p id="5e2d" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">下图显示了使用该函数的一些测试:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pa"><img src="../Images/b5a7d13a44617a2f05e60b5b55969fc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cWFAZQhbJvL_LXdBhOeB5A.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者根据公共数据集创作</p></figure><p id="2ace" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">整体表现惊人！从你输入内存卡中的图像路径开始，直到打印出结果，整个过程不到半秒钟，精度极高！</p><p id="f036" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">该功能可以很容易地应用于视频或现场摄像机的帧。本节讨论的笔记本和完整代码可以从<a class="ae nl" href="https://github.com/Mjrovai/TFLite_IA_at_the_Edge" rel="noopener ugc nofollow" target="_blank"> GitHub </a>下载。</p><h1 id="fca9" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">目标检测</h1><p id="aa7b" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">利用图像分类，我们可以检测这样的图像的主要主题是什么。但是，如果几个物体在同一个图像上是主要的和感兴趣的，会发生什么呢？要解决它，我们可以使用一个对象检测模型！</p><p id="7263" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">给定图像或视频流，对象检测模型可以识别已知对象集合中的哪一个可能存在，并提供关于它们在图像中的位置的信息。</p><p id="f408" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">对于此任务，我们将下载一个使用 COCO(上下文中的公共对象)数据集预训练的 Mobilenet V1 模型。这个数据集有超过 200，000 个带标签的图像，分属 91 个类别。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pb"><img src="../Images/b89f037079db292bd2113aafe1155f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PKcPBpp9OCe3iL-cJyiG0g.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者创作</p></figure><h2 id="4b90" class="mu kv it bd kw mv mw dn la mx my dp le lv mz na lg lz nb nc li md nd ne lk nf bi translated">下载模型和标签</h2><p id="6792" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在 Raspberry 终端上运行命令:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="4be7" class="mu kv it of b gy ok ol l om on">$ cd ./models <br/>$ curl -O <a class="ae nl" href="http://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip" rel="noopener ugc nofollow" target="_blank">http://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip</a><br/>$ unzip coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip<br/>$ curl -O  <a class="ae nl" href="https://dl.google.com/coral/canned_models/coco_labels.txt" rel="noopener ugc nofollow" target="_blank">https://dl.google.com/coral/canned_models/coco_labels.txt</a></span><span id="23e9" class="mu kv it of b gy oz ol l om on">$ rm coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip<br/>$ rm labelmap.txt</span></pre><p id="1106" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">在<em class="mi"> models </em>子目录中，我们应该以 2 个新文件结束:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="c31e" class="mu kv it of b gy ok ol l om on">coco_labels.txt  <br/>detect.tflite</span></pre><p id="d1e5" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">对新图像执行推断的步骤与图像分类非常相似，除了:</p><ul class=""><li id="4b3c" class="nm nn it lo b lp mm ls mn lv no lz np md nq mh nr ns nt nu bi translated">输入:图像的形状必须为 300x300 像素</li><li id="ddf9" class="nm nn it lo b lp nv ls nw lv nx lz ny md nz mh nr ns nt nu bi translated">输出:不仅包括标签和概率(“得分”)，还包括对象在图像上所处位置的相对窗口位置(“边界框”)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pc"><img src="../Images/ac7d30b8e01095afb493cd43eae78a7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*evVoVcFLu57086smS5Wazw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者创作</p></figure><p id="a284" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">现在，我们必须加载标签和模型，分配张量。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="ec13" class="mu kv it of b gy ok ol l om on">labels = load_labels('./models/coco_labels.txt')<br/>interpreter = Interpreter('./models/detect.tflite')<br/>interpreter.allocate_tensors()</span></pre><p id="0080" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">输入预处理和我们之前做的一样，但是输出应该被处理以获得更可读的输出。以下函数将对此有所帮助:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="d00f" class="mu kv it of b gy ok ol l om on">def set_input_tensor(interpreter, image):<br/>    """Sets the input tensor."""<br/>    tensor_index = interpreter.get_input_details()[0]['index']<br/>    input_tensor = interpreter.tensor(tensor_index)()[0]<br/>    input_tensor[:, :] = image</span><span id="9c9e" class="mu kv it of b gy oz ol l om on">def get_output_tensor(interpreter, index):<br/>    """Returns the output tensor at the given index."""<br/>    output_details = interpreter.get_output_details()[index]<br/>    tensor = np.squeeze(interpreter.get_tensor(output_details['index']))<br/>    return tensor</span></pre><p id="c926" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">借助上述函数，detect_objects()将返回推理结果:</p><ul class=""><li id="f739" class="nm nn it lo b lp mm ls mn lv no lz np md nq mh nr ns nt nu bi translated">对象标签 id</li><li id="dc15" class="nm nn it lo b lp nv ls nw lv nx lz ny md nz mh nr ns nt nu bi translated">得分</li><li id="fa83" class="nm nn it lo b lp nv ls nw lv nx lz ny md nz mh nr ns nt nu bi translated">边界框，它将显示对象的位置。</li></ul><p id="6e43" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">我们已经包括了一个“阈值”来避免正确概率低的对象。通常情况下，我们应该考虑 50%以上的分数。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="4255" class="mu kv it of b gy ok ol l om on">def detect_objects(interpreter, image, threshold):<br/>    set_input_tensor(interpreter, image)<br/>    interpreter.invoke()<br/>    <br/>    # Get all output details<br/>    boxes = get_output_tensor(interpreter, 0)<br/>    classes = get_output_tensor(interpreter, 1)<br/>    scores = get_output_tensor(interpreter, 2)<br/>    count = int(get_output_tensor(interpreter, 3))</span><span id="4eec" class="mu kv it of b gy oz ol l om on">    results = []<br/>    for i in range(count):<br/>        if scores[i] &gt;= threshold:<br/>            result = {<br/>                'bounding_box': boxes[i],<br/>                'class_id': classes[i],<br/>                'score': scores[i]<br/>            }<br/>            results.append(result)<br/>    return results</span></pre><p id="16de" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">如果我们将上述函数应用于一个整形的图像(与分类示例中使用的相同)，我们应该得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pd"><img src="../Images/1e603148aa1bfeed6458c65aafbd83f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4rV5O4XCWYvjAKElOYuXiA.png"/></div></div></figure><p id="65fa" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">太好了！在不到 200 毫秒的时间内，以 77%的概率，在由“边界框”界定的区域上检测到 id 为 16 的物体:(0.028011084，0.020121813，0.9886069，0.802299)。这四个数字分别与 ymin、xmin、ymax 和 xmax 相关。</p><p id="a061" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">考虑到 y 从顶部(ymin)到底部(ymax ), x 从左侧(xmin)到右侧(xmax ),如下图所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/d0ad420000b1ef610c902587d55a6858.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*eQ6Xxb3hmuZ00K04ZKDUXQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者创作</p></figure><p id="d87b" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">边界框有四个值，实际上，我们有左上角和右下角的坐标。有了两边，知道了图片的形状，就有可能画出物体周围的矩形。</p><p id="e2b6" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">接下来，我们应该找出 class_id 等于 16 意味着什么。打开文件<em class="mi"> coco_labels.txt，</em>作为一个字典，它的每个元素都有一个相关联的索引，检查索引 16，我们得到了预期的' cat '概率是从分数返回的值。</p><p id="87be" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">让我们创建一个通用函数来检测单个图片上的多个对象。第一个函数从图像路径开始，将执行推理，返回调整后的图像和结果(多个 id，每个 id 都有其分数和边界框:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="9502" class="mu kv it of b gy ok ol l om on">def detectObjImg_2(image_path, threshold = 0.51):</span><span id="4110" class="mu kv it of b gy oz ol l om on">img = cv2.imread(image_path)<br/>    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>    image = cv2.resize(img, (width, height),<br/>                       fx=0.5,<br/>                       fy=0.5,<br/>                       interpolation=cv2.INTER_AREA)<br/>    results = detect_objects(interpreter, image, threshold)</span><span id="0d93" class="mu kv it of b gy oz ol l om on">return img, results</span></pre><p id="28a4" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">有了整形后的图像和推断结果，下面的函数可用于在对象周围绘制一个矩形，为每个对象指定其标签和概率:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="41e4" class="mu kv it of b gy ok ol l om on">def detect_mult_object_picture(img, results):</span><span id="44d0" class="mu kv it of b gy oz ol l om on">    HEIGHT, WIDTH, _ = img.shape<br/>    aspect = WIDTH / HEIGHT<br/>    WIDTH = 640<br/>    HEIGHT = int(640 / aspect)<br/>    dim = (WIDTH, HEIGHT)</span><span id="7d25" class="mu kv it of b gy oz ol l om on">   img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)</span><span id="96b8" class="mu kv it of b gy oz ol l om on">   for i in range(len(results)):<br/>        id = int(results[i]['class_id'])<br/>        prob = int(round(results[i]['score'], 2) * 100)<br/>        <br/>        ymin, xmin, ymax, xmax = results[i]['bounding_box']<br/>        xmin = int(xmin * WIDTH)<br/>        xmax = int(xmax * WIDTH)<br/>        ymin = int(ymin * HEIGHT)<br/>        ymax = int(ymax * HEIGHT)</span><span id="8532" class="mu kv it of b gy oz ol l om on">        text = "{}: {}%".format(labels[id], prob)</span><span id="6db3" class="mu kv it of b gy oz ol l om on">        if ymin &gt; 10: ytxt = ymin - 10<br/>        else: ytxt = ymin + 15</span><span id="fe43" class="mu kv it of b gy oz ol l om on">        img = cv2.rectangle(img, (xmin, ymin), (xmax, ymax),<br/>                            COLORS[id],<br/>                            thickness=2)<br/>        img = cv2.putText(img, text, (xmin + 3, ytxt), FONT, 0.5, COLORS[id],<br/>                          2)</span><span id="5ee0" class="mu kv it of b gy oz ol l om on">   return img</span></pre><p id="03dd" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">下面是一些结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/1b171cfe42fe6feecb58aa8ce4c852a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*OYdoGiN2OQgZrd_8ZYHJ-w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者创作</p></figure><p id="0a4b" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">完整的代码可以在<a class="ae nl" href="https://github.com/Mjrovai/TFLite_IA_at_the_Edge" rel="noopener ugc nofollow" target="_blank"> GitHub </a>找到。</p><h2 id="be9a" class="mu kv it bd kw mv mw dn la mx my dp le lv mz na lg lz nb nc li md nd ne lk nf bi translated">使用摄像机的目标检测</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/10c9410f2167ca713356fe9b5e9cb642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*4jAnIvw-yC40fvAOkEZyuA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:树莓派基金会</p></figure><p id="6f52" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">如果您有一个连接到 Raspberry Pi 的 PiCam，则可以使用之前定义的相同功能，逐帧捕捉视频并执行对象识别。如果您的 Pi: <a class="ae nl" href="https://projects.raspberrypi.org/en/projects/getting-started-with-picamera" rel="noopener ugc nofollow" target="_blank">中没有可用的相机，请遵循本教程开始使用相机模块</a>。</p><p id="3c71" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">首先，必须定义相机要捕捉的帧的大小。我们将使用 640x480。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="60b9" class="mu kv it of b gy ok ol l om on">WIDTH = 640<br/>HEIGHT = 480</span></pre><p id="fec8" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">接下来，你必须校准摄像机:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="4252" class="mu kv it of b gy ok ol l om on">cap = cv2.VideoCapture(0)<br/>cap.set(3, WIDTH)<br/>cap.set(4, HEIGHT)</span></pre><p id="a3ad" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">并循环运行下面的代码。在按下“q”键之前，摄像机会一帧一帧地捕捉视频，并绘制带有相应标签和概率的边界框。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="40bd" class="mu kv it of b gy ok ol l om on">while True:</span><span id="f2e7" class="mu kv it of b gy oz ol l om on">    timer = cv2.getTickCount()<br/>    success, img = cap.read()<br/>    img = cv2.flip(img, 0)<br/>    img = cv2.flip(img, 1)</span><span id="3c43" class="mu kv it of b gy oz ol l om on">    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)<br/>    cv2.putText(img, "FPS: " + str(int(fps)), (10, 470),<br/>                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)</span><span id="7b6b" class="mu kv it of b gy oz ol l om on">    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>    image = cv2.resize(image, (width, height),<br/>                       fx=0.5,<br/>                       fy=0.5,<br/>                       interpolation=cv2.INTER_AREA)<br/>    start_time = time.time()<br/>    results = detect_objects(interpreter, image, 0.55)<br/>    elapsed_ms = (time.time() - start_time) * 1000</span><span id="30a9" class="mu kv it of b gy oz ol l om on">    img = detect_mult_object_picture(img, results)<br/>    cv2.imshow("Image Recognition ==&gt; Press [q] to Exit", img)</span><span id="879e" class="mu kv it of b gy oz ol l om on">if cv2.waitKey(1) &amp; 0xFF == ord('q'):<br/>        break</span><span id="df17" class="mu kv it of b gy oz ol l om on">cap.release()<br/>cv2.destroyAllWindows()</span></pre><p id="dc63" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">下面可以看到树莓 Pi 屏幕上实时运行的视频。注意视频运行在 60 FPS(每秒帧数)左右，相当不错！。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ph pi l"/></div></figure><p id="2bf8" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">下面是上述视频的一个截屏:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/3877ed60ac596d8e012b87f04cc1256a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*kVDSCN_0hhIKjrMcOZ_ybQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者创作</p></figure><p id="7754" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">完整的代码可以在<a class="ae nl" href="https://github.com/Mjrovai/TFLite_IA_at_the_Edge" rel="noopener ugc nofollow" target="_blank"> GitHub 上找到。</a></p><h1 id="0a74" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">姿态估计</h1><p id="566b" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">人工智能更令人兴奋和关键的领域之一是估计一个人的实时姿势，使机器能够理解人们在图像和视频中做什么。在我的文章<a class="ae nl" rel="noopener" target="_blank" href="/realtime-multiple-person-2d-pose-estimation-using-tensorflow2-x-93e4c156d45f">使用 TensorFlow2.x </a>进行实时多人 2D 姿势估计中深入探讨了姿势估计，但在这里的边缘，使用 Raspberry Pi 并在 TensorFlow Lite 的帮助下，可以轻松复制在 Mac 上完成的几乎相同的操作。</p><p id="a01b" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">我们将在这个项目中使用的型号是<a class="ae nl" href="https://github.com/tensorflow/tfjs-models/tree/master/posenet" rel="noopener ugc nofollow" target="_blank"> PoseNet </a>。我们将以与图像分类和对象检测相同的方式进行推理，其中图像通过预先训练的模型输入。PoseNet 带有几个不同版本的模型，对应于 MobileNet v1 架构和 ResNet50 架构的变化。在这个项目中，预训练的版本是 MobileNet V1，它比 ResNet 更小，更快，但不如 ResNet 准确。此外，有单独的模型用于单人和多人姿势检测。我们将探索为一个人训练的模型。</p><p id="8a11" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">在这个<a class="ae nl" href="https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html" rel="noopener ugc nofollow" target="_blank">站点</a>中，可以使用现场摄像机实时探索几个 PoseNet 模型和配置。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pk"><img src="../Images/a0bac86e8ce774084d17877cd78b6fa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lG5quurfPx-VwV9TKPc0qQ.png"/></div></div></figure><p id="ea3b" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">在 Raspberry Pi 上执行姿态估计的库与之前使用的相同。NumPy，MatPlotLib，OpenCV 和 TensorFlow Lite 解释器。</p><p id="a03c" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">预训练的模型是<a class="ae nl" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite" rel="noopener ugc nofollow" target="_blank">posenet _ mobilenet _ v1 _ 100 _ 257 x257 _ multi _ kpt _ stripped . TF Lite</a>，可以从上面的链接或者<a class="ae nl" href="https://www.tensorflow.org/lite/models/pose_estimation/overview" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite —姿态估计概述网站</a>下载。模型应该保存在 models 子目录中。</p><p id="fc69" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">开始加载 TFLite 模型并分配张量:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="684a" class="mu kv it of b gy ok ol l om on">interpreter = tflite.Interpreter(model_path='./models/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite')<br/>interpreter.allocate_tensors()</span></pre><p id="0fc3" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">获取输入和输出张量:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="7b8d" class="mu kv it of b gy ok ol l om on">input_details = interpreter.get_input_details()<br/>output_details = interpreter.get_output_details()</span></pre><p id="6351" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">与我们之前所做的一样，查看 input_details，可以看到用于姿态估计的图像应该是(1，257，257，3)，这意味着图像必须被整形为 257x257 像素。</p><p id="7c79" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">让我们以一个简单的人形作为输入，这将帮助我们分析它:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/500a7cee84ba5f06b3b156b9c27012ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*g21Mhu973iuZqEU3v7lPZA.png"/></div></figure><p id="4096" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">第一步是预处理图像。这个特殊的模型没有被量化，这意味着 dtype 是 float32。这些信息对于预处理输入图像至关重要，如下面的代码所示</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="ecf0" class="mu kv it of b gy ok ol l om on">image = cv2.resize(image, size) <br/>input_data = np.expand_dims(image, axis=0)<br/>input_data = input_data.astype(np.float32)<br/>input_data = (np.float32(input_data) - 127.5) / 127.5</span></pre><p id="f146" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">对图像进行预处理后，现在是执行推理的时候了，向张量输入图像并调用解释器:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="1977" class="mu kv it of b gy ok ol l om on">interpreter.set_tensor(input_details[0]['index'], input_data)<br/>interpreter.invoke()</span></pre><p id="7e48" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">一篇非常有助于理解如何使用 PoseNet 的文章是<a class="ae nl" href="https://medium.com/@ikunyankin?source=post_page-----ea2e9249abbd----------------------" rel="noopener"> Ivan Kunyakin </a>教程的<a class="ae nl" href="https://medium.com/roonyx/pose-estimation-and-matching-with-tensorflow-lite-posenet-model-ea2e9249abbd" rel="noopener">姿势估计和与 TensorFlow lite 的匹配</a>。Ivan 评论说，在输出向量中，找到关键点的关键是:</p><ul class=""><li id="9fd7" class="nm nn it lo b lp mm ls mn lv no lz np md nq mh nr ns nt nu bi translated"><strong class="lo iu">热图</strong>大小为(9，9，17)的 3D 张量，其对应于 17 个关键点(身体关节)中的每一个在图像的特定部分(9，9)中出现的概率。它用于定位关节的大致位置。</li><li id="5179" class="nm nn it lo b lp nv ls nw lv nx lz ny md nz mh nr ns nt nu bi translated"><strong class="lo iu">偏移向量:</strong>大小为(9，9，34)的 3D 张量，称为偏移向量。它用于更精确地计算关键点的位置。第三维的第一个 17 对应于 x 坐标，第二个 17 对应于 y 坐标。</li></ul><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="61a4" class="mu kv it of b gy ok ol l om on">output_details = interpreter.get_output_details()[0]<br/>heatmaps = np.squeeze(interpreter.get_tensor(output_details['index']))</span><span id="3daf" class="mu kv it of b gy oz ol l om on">output_details = interpreter.get_output_details()[1]<br/>offsets = np.squeeze(interpreter.get_tensor(output_details['index']))</span></pre><p id="ac37" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">让我们创建一个函数，该函数将基于热图和偏移量返回包含所有 17 个关键点(或人的关节)的数组。</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="74f3" class="mu kv it of b gy ok ol l om on">def get_keypoints(heatmaps, offsets):</span><span id="2e63" class="mu kv it of b gy oz ol l om on">    joint_num = heatmaps.shape[-1]<br/>    pose_kps = np.zeros((joint_num, 2), np.uint32)<br/>    max_prob = np.zeros((joint_num, 1))</span><span id="f1cc" class="mu kv it of b gy oz ol l om on">    for i in range(joint_num):<br/>        joint_heatmap = heatmaps[:,:,i]<br/>        max_val_pos = np.squeeze(<br/>            np.argwhere(joint_heatmap == np.max(joint_heatmap)))<br/>        remap_pos = np.array(max_val_pos / 8 * 257, dtype=np.int32)<br/>        pose_kps[i, 0] = int(remap_pos[0] +<br/>                             offsets[max_val_pos[0], max_val_pos[1], i])<br/>        pose_kps[i, 1] = int(remap_pos[1] +<br/>                             offsets[max_val_pos[0], max_val_pos[1],<br/>                                         i + joint_num])<br/>        max_prob[i] = np.amax(joint_heatmap)</span><span id="9212" class="mu kv it of b gy oz ol l om on">    return pose_kps, max_prob</span></pre><p id="73f1" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">使用上述函数以及从输出张量提取的热图和偏移向量，图像推断的结果，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/b0d8acf81951c3bff903461d5d08da26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*Cp5o3a1_3STpyIs41wErRw.png"/></div></figure><p id="f1eb" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">得到的数组显示了关于关节在 257×257 像素的图像上的位置的所有 17 个坐标(y，x)。使用下面的代码。可以在调整大小的图像上绘制每个关节。作为参考，数组索引已被标注，因此很容易识别每个关节:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="aa8e" class="mu kv it of b gy ok ol l om on">y,x = zip(*keypts_array)<br/>plt.figure(figsize=(10,10))<br/>plt.axis([0, image.shape[1], 0, image.shape[0]])  <br/>plt.scatter(x,y, s=300, color='orange', alpha=0.6)<br/>img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/>plt.imshow(img)<br/>ax=plt.gca() <br/>ax.set_ylim(ax.get_ylim()[::-1]) <br/>ax.xaxis.tick_top() <br/>plt.grid();</span><span id="c260" class="mu kv it of b gy oz ol l om on">for i, txt in enumerate(keypts_array):<br/>    ax.annotate(i, (keypts_array[i][1]-3, keypts_array[i][0]+1))</span></pre><p id="3a87" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">结果，我们得到了图片:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pn"><img src="../Images/dc210f1d85b4829e295d54c458cd0b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cBlCqbYjDQNv2dyCxAftdA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者创作</p></figure><p id="b85c" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">太好了，现在是时候创建一个通用函数来绘制“骨骼”，这是关节的连接。骨骼将被绘制为线条，这些线条是关键点 5 到 16 之间的连接，如上图所示。独立圆将用于关键点 0 至 4，与头部相关:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="3a81" class="mu kv it of b gy ok ol l om on">def join_point(img, kps, color='white', bone_size=1):<br/>    <br/>    if   color == 'blue'  : color=(255, 0, 0)<br/>    elif color == 'green': color=(0, 255, 0)<br/>    elif color == 'red':  color=(0, 0, 255)<br/>    elif color == 'white': color=(255, 255, 255)<br/>    else:                  color=(0, 0, 0)</span><span id="99b7" class="mu kv it of b gy oz ol l om on">    body_parts = [(5, 6), (5, 7), (6, 8), (7, 9), (8, 10), (11, 12), (5, 11),<br/>                  (6, 12), (11, 13), (12, 14), (13, 15), (14, 16)]</span><span id="ccd7" class="mu kv it of b gy oz ol l om on">    for part in body_parts:<br/>        cv2.line(img, (kps[part[0]][1], kps[part[0]][0]),<br/>                (kps[part[1]][1], kps[part[1]][0]),<br/>                color=color,<br/>                lineType=cv2.LINE_AA,<br/>                thickness=bone_size)<br/>    <br/>    for i in range(0,len(kps)):<br/>        cv2.circle(img,(kps[i,1],kps[i,0]),2,(255,0,0),-1)</span></pre><p id="f420" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">通过调用该函数，我们得到了图像中身体的估计姿势:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="628c" class="mu kv it of b gy ok ol l om on">join_point(img, keypts_array, bone_size=2)<br/>plt.figure(figsize=(10,10))<br/>plt.imshow(img);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi po"><img src="../Images/f5cec035462e42becbeab5367cc97e21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J4PonDKX46q9lEm8ddiElQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:作者创作</p></figure><p id="b1eb" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">最后但同样重要的是，让我们创建一个通用函数，以图像路径作为起点来估计姿态:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="097b" class="mu kv it of b gy ok ol l om on">def plot_pose(img, keypts_array, joint_color='red', bone_color='blue', bone_size=1):<br/>    join_point(img, keypts_array, bone_color, bone_size)<br/>    y,x = zip(*keypts_array)<br/>    plt.figure(figsize=(10,10))<br/>    plt.axis([0, img.shape[1], 0, img.shape[0]])  <br/>    plt.scatter(x,y, s=100, color=joint_color)<br/>    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><span id="8b7e" class="mu kv it of b gy oz ol l om on">    plt.imshow(img)<br/>    ax=plt.gca() <br/>    ax.set_ylim(ax.get_ylim()[::-1]) <br/>    ax.xaxis.tick_top() <br/>    plt.grid();<br/>    return img</span><span id="23bf" class="mu kv it of b gy oz ol l om on"><br/>def get_plot_pose(image_path, size, joint_color='red', bone_color='blue', bone_size=1):    <br/>    image_original = cv2.imread(image_path)<br/>    image = cv2.resize(image_original, size) <br/>    input_data = np.expand_dims(image, axis=0)<br/>    input_data = input_data.astype(np.float32)<br/>    input_data = (np.float32(input_data) - 127.5) / 127.5</span><span id="9034" class="mu kv it of b gy oz ol l om on">    interpreter.set_tensor(input_details[0]['index'], input_data)<br/>    interpreter.invoke()<br/>    <br/>    output_details = interpreter.get_output_details()[0]<br/>    heatmaps = np.squeeze(interpreter.get_tensor(output_details['index']))</span><span id="126a" class="mu kv it of b gy oz ol l om on">    output_details = interpreter.get_output_details()[1]<br/>    offsets = np.squeeze(interpreter.get_tensor(output_details['index']))</span><span id="7c25" class="mu kv it of b gy oz ol l om on">    keypts_array, max_prob = get_keypoints(heatmaps,offsets)<br/>    orig_kps = get_original_pose_keypoints(image_original, keypts_array, size)</span><span id="5dd6" class="mu kv it of b gy oz ol l om on">    img = plot_pose(image_original, orig_kps, joint_color, bone_color, bone_size)<br/>    <br/>    return orig_kps, max_prob, img</span></pre><p id="6d8a" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">此时，只需一行代码，就可以检测图像上的姿态:</p><pre class="kj kk kl km gt og of oh oi aw oj bi"><span id="225d" class="mu kv it of b gy ok ol l om on">keypts_array, max_prob, img  = get_plot_pose(image_path, size, bone_size=3)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi pp"><img src="../Images/4311d6ede62c656158d7e1fd6a26601f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O_xAQMLA8qZk753pl7tZ_g.png"/></div></div></figure><p id="cff4" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">本节开发的所有代码都可以在<a class="ae nl" href="https://github.com/Mjrovai/TFLite_IA_at_the_Edge" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。</p><blockquote class="mj mk ml"><p id="0286" class="lm ln mi lo b lp mm ju lr ls mn jx lu mo mp lx ly mq mr mb mc ms mt mf mg mh im bi translated">另一个简单的步骤是将该功能应用于来自视频和现场摄像机的帧。我会留给你的！；-)</p></blockquote><h1 id="d43d" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">结论</h1><p id="0586" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">TensorFlow Lite 是一个在边缘实现人工智能(更准确地说，是 ML)的伟大框架。在这里，我们探索了在 Raspberry Pi 上工作的 ML 模型，但 TFLite 现在越来越多地用于“边缘的边缘”，在非常小的微控制器上，在所谓的<em class="mi"> TinyML </em>中。</p><p id="7a6e" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">一如既往，我希望这篇文章可以激励其他人在 AI 的奇幻世界中找到自己的路！</p><p id="bd93" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">本文使用的所有代码都可以在 GitHub 项目上下载:<a class="ae nl" href="https://github.com/Mjrovai/TFLite_IA_at_the_Edge" rel="noopener ugc nofollow" target="_blank"> TFLite_IA_at_the_Edge。</a></p><p id="1ad4" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">来自世界南方的问候！</p><p id="ab3f" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">我的下一篇文章再见！</p><p id="3e14" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">谢谢你</p><p id="f21e" class="pw-post-body-paragraph lm ln it lo b lp mm ju lr ls mn jx lu lv mp lx ly lz mr mb mc md mt mf mg mh im bi translated">马塞洛</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/d3d180e33dddef27ab8ab4e153374f38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b0guuo1OVZ27LxFK86FrcQ.png"/></div></figure></div></div>    
</body>
</html>