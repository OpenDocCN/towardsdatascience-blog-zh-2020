<html>
<head>
<title>Performing Linear Regression Using the Normal Equation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用正态方程执行线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/performing-linear-regression-using-the-normal-equation-6372ed3c57?source=collection_archive---------7-----------------------#2020-04-13">https://towardsdatascience.com/performing-linear-regression-using-the-normal-equation-6372ed3c57?source=collection_archive---------7-----------------------#2020-04-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8212" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">并不总是需要运行优化算法来执行线性回归。你可以解一个特定的代数方程——正规方程——直接得到结果。尽管对于大数据集来说，它甚至还没有接近计算最优，但它仍然是值得注意的选项之一。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1ba3a74bec1b459e31c0e572329d8db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AC0Lz5n-cNbNDVfc"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安托万·道特里在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5c8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 1。简介</strong></p><p id="98a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归是数据分析中最重要和最流行的预测技术之一。它也是最古老的——著名的C.F .高斯在19世纪初用它在天文学中计算轨道。</p><p id="946e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其目标是通过计算最小化特定成本函数(误差)的回归函数参数，例如均方误差(MSE ),将最佳线(或超平面/平面)拟合到给定点(观察值)的集合。</p><p id="2bb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提醒一下，下面有一个扩展形式的线性回归方程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/01b86d17bc25ce83acb8c2edf7d21180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*sbsAvcXV9TOmWFIRnCFt-Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。1:线性回归方程</p></figure><p id="08a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在矢量化形式中，它看起来像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/250b977f74a60d9d62b924988bcf05c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*Lv9qHJf_Au33ikdAmB4x2Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。2:矢量化形式的线性回归方程</p></figure><p id="f7ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中θ是参数权重的向量。</p><p id="4282" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，通过运行某种优化算法(例如梯度下降)来最小化成本函数，从而找到最佳模型参数。然而，也可以通过求解称为标准方程的代数方程来获得这些参数的值(权重)。其定义如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/e02ec3480ba3f814398146d3c88b115d.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*aj3OqspbpwQXrkTZPdOe8g.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。3:正常方程</p></figure><p id="a7aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2。手工计算</strong></p><p id="538a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将对一个非常基本的情况进行线性回归，这样我们就可以避免冗长的手工计算。顺便说一下，如果你认为你需要刷新你的线性代数技能，互联网上有很多好的资源(例如<a class="ae ky" href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="noopener ugc nofollow" target="_blank"> YouTube系列</a>我推荐)。</p><p id="8a51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，只有三个只有一个变量(X₁).)的点(观测值)在图表上，它们看起来像下面。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ly"><img src="../Images/97744911ffe4eac4182e42622a3a7d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*77kT4qg4VKa9fizU5XZ1lg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">显示本例中使用的点的散点图</p></figure><p id="6488" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，线性回归方程具有以下形式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/75b59c636b3f58b314bf676144d32926.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*eZGoN6W9GjpJ_MJNzTby0A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。4:本例的线性回归方程</p></figure><p id="5c75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征(X)和标签(y)是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/539b0a0e003f8ec9e9b326c60e1c6138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*GW0oe_7V8lhmfgvt0XKNaQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">功能和标签矩阵</p></figure><p id="032c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我们添加了一个默认偏置项1，它将在计算过程中更新。不加这个项会导致一个错误的解。</p><p id="572f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第一步:</strong>矩阵X的转置</p><p id="2ab6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个相对简单的任务——行变成新列。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/a262a42651754ef1dd0f45453ad529c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*7ArcZ0OtY2lACsvmqgZEqA.jpeg"/></div></figure><p id="4de7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤2: </strong>转置矩阵与矩阵X相乘</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/64568f078d0b7b98cea48cb49cf6b657.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*BzWkyL929pbdZ3c8AQMPPA.jpeg"/></div></figure><p id="81ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤3: </strong>对合成矩阵求逆</p><p id="3c8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要对简单的2x2矩阵求逆，我们可以使用以下公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi md"><img src="../Images/6a69e36311ab2f3e06b3b2c895a00630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i31J6oZ9fXSwGJp08RZI0w.png"/></div></div></figure><p id="d3b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/6fead85a7eb41d23acc15f86f9cdb0ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*VtVznFGnEvqBHfu6UzAMmQ.png"/></div></figure><p id="9bef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mf">注意:对于更大的矩阵(大于3X3)，求逆变得更加麻烦，通常使用算法方法——比如高斯消去法。记住这一点很重要！</em></p><p id="5199" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤4:</strong>X转置的逆矩阵的乘法</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/5feaf787f7ed37e95d9a3ad0aa414cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*V-0bsMA1o9esfTDzpuqwYA.png"/></div></figure><p id="e433" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第五步:</strong>最终相乘得到最佳参数的向量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mh"><img src="../Images/1ed511ae430db3bcdbe881c1b32f6f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pE87db380l3XR2p4gvQhMw.png"/></div></div></figure><p id="35f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们的线性回归方程采用以下形式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/08d8e837128823fa1e9190b352b45fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*VJYsxuUvfT29DT28PazuDA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。5:具有最佳权重的线性回归方程</p></figure><p id="2877" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将这条线绘制到前面的图上，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/1527ec716c67fe3fb217f8a6b361b7dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*jc1iLLh6dp02f3qXWnHdDw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有原始点和回归线的散点图(红色)</p></figure><p id="0a8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3。用Python实现</strong></p><p id="69d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<a class="ae ky" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> Numpy </a>库可以在Python中实现相同的计算，该库包含了<a class="ae ky" href="https://numpy.org/doc/stable/reference/routines.linalg.html?highlight=linalg#module-numpy.linalg" rel="noopener ugc nofollow" target="_blank"> numpy.linalg </a>集合中的一组线性代数函数。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="2f8f" class="mp mq it ml b gy mr ms l mt mu">import numpy as np</span><span id="2808" class="mp mq it ml b gy mv ms l mt mu">X = np.c_[[1,1,1],[1,2,3]] # defining features<br/>y = np.c_[[1,3,2]] # defining labels<br/>theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) # normal equation<br/>print(theta)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/9de4eb82f1cfee0a182bf6882f5d5034.png" data-original-src="https://miro.medium.com/v2/resize:fit:158/format:webp/1*LNH4YMaike2kZkItFKDUmw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">运行上述代码的结果</p></figure><p id="31f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以定义我们想要预测其值的新要素。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="1ce7" class="mp mq it ml b gy mr ms l mt mu">X_new = np.c_[[1,1,1,1],[0, 0.5,1.5,4]]  # new features</span></pre><p id="b483" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过实施等式2，我们获得了预测值。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="b561" class="mp mq it ml b gy mr ms l mt mu">y_pred = X_new.dot(theta)  # making predictions<br/>print(y_pred)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/5d02000a153b2ad821b2a7d596f104b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/format:webp/1*ShJqq1RKbaNC_RPI-72AzA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预测值</p></figure><p id="6d43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 4。备注</strong></p><p id="246f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，使用法线方程并在Python中实现它非常容易——它只有一行代码。那么为什么不常用呢？</p><p id="f878" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问题在于它的<a class="ae ky" href="https://en.wikipedia.org/wiki/Computational_complexity" rel="noopener ugc nofollow" target="_blank">数值复杂度</a>。求解这个方程需要对一个矩阵求逆，这是一个计算量很大的操作——取决于实现方式，在<a class="ae ky" href="https://en.wikipedia.org/wiki/Big_O_notation" rel="noopener ugc nofollow" target="_blank">大O符号</a>中，它是O(n)或略小。这意味着规模会可怕地扩大，实际上意味着当要素数量增加一倍时，计算时间会增加2 = 8倍。也有可能第二步的结果根本不可逆——导致大麻烦。这就是这种方法在实践中不常见的原因。</p><p id="645e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从好的方面来看，这种方法只需一步就能计算出来，你不必选择学习率参数。此外，就内存使用而言，这种方法是线性的O(m ),这意味着它可以有效地存储大型数据集，如果它们只适合您的计算机内存的话。</p></div></div>    
</body>
</html>