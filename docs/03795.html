<html>
<head>
<title>Background Matting: The World is Your Green Screen</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">背景抠图:世界是你的绿屏</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635?source=collection_archive---------16-----------------------#2020-04-09">https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635?source=collection_archive---------16-----------------------#2020-04-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1bd4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用深度学习和 GANs 在您自己的家中实现专业质量的背景替换</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/097f9cdac04dc41e6848d5bedcd0c52b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JN3pK1gYoegcnPgRDs_V7w.png"/></div></div></figure><p id="31f0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你希望没有完整的工作室也能制作专业质量的视频吗？或者 Zoom 的虚拟背景功能在视频会议中表现更好？</p><p id="14e8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们最近在 CVPR 2020 上发表的论文【1】提供了一种新的简单的方法来为各种各样的应用替换你的背景。你可以在家里的日常环境中，用一个固定的或手持的相机来做这件事。我们的方法也是最先进的，其输出可与专业结果相媲美。在本文中，我们将介绍我们方法的动机、技术细节和使用技巧。</p><p id="fdfc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">您也可以查看我们的<a class="ae ln" href="https://grail.cs.washington.edu/projects/background-matting/" rel="noopener ugc nofollow" target="_blank">项目页面</a>和<a class="ae ln" href="https://github.com/senguptaumd/Background-Matting" rel="noopener ugc nofollow" target="_blank">代码库</a>。</p><h1 id="06de" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">什么是抠图？</h1><p id="3c9c" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">抠图是将图像分为前景和背景的过程，以便您可以将前景合成到新的背景上。这是绿屏效果背后的关键技术，它被广泛用于视频制作、图形和消费应用程序。为了模拟这个问题，我们将捕获图像中的每个像素表示为前景和背景的组合:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/2a84f21784be93950a73babb00521193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l5nl-HdhUTZ4-vVnj3f2cA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">抠图方程式</p></figure><p id="fbe8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们的问题是给定一幅捕获的图像(C ),求解每个像素的前景(F)、背景(B)和透明度(alpha)。显然，这是高度不确定的，并且由于图像具有 RGB 通道，这需要从 3 个观察值中求解 7 个未知数。</p><h2 id="c08b" class="mq lp iq bd lq mr ms dn lu mt mu dp ly la mv mw ma le mx my mc li mz na me nb bi translated">分割的问题是</h2><p id="f567" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">一种可能的方法是使用分割来分离前景以便合成。尽管分割在最近几年取得了巨大的进步，但它并没有解决全部的抠图问题。分割为每个像素分配一个二进制(0，1)标签，以表示前景和背景，而不是求解连续的 alpha 值。这种简化的效果在以下示例中显而易见:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/e4ecc9410896f4ac392d70f23ccdc08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n8L4b8_yukF9_uBIbU2DFA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">这个例子说明了为什么分段不能解决合成问题。使用 DeepLab v3+ [2]进行分割。</p></figure><p id="bee3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">边缘周围的区域，尤其是头发中的区域，具有介于 0 和 1 之间的真实 alpha 值<em class="nd">。因此，分割的二进制性质在前景周围产生了粗糙的边界，留下了可见的伪像。解决部分透明度和前景色允许在第二帧更好的合成。</em></p><h1 id="c7e4" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">使用随意捕捉的背景</h1><p id="0f75" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">因为抠图是一个比分割更难的问题，所以经常使用附加信息来解决这个无约束的问题，即使在使用深度学习时也是如此。</p><p id="74fc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">许多现有的方法[3][4][5]使用三分图，或已知前景、背景和未知区域的手绘地图。虽然这对于图像是可能的，但是注释视频是非常耗时的，并且对于这个问题来说不是一个可行的研究方向。</p><p id="cf5d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们选择使用捕获的背景作为真实背景的估计。这使得它更容易解决前景和阿尔法值。我们称之为“随意捕捉”的背景，因为它可以包含轻微的运动、颜色差异、轻微的阴影或与前景相似的颜色。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/8eddf0a751217991301eff8b4fbb3ae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ViQSdWYIRnJXifVCUtUj-g.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">我们的抓捕过程。当对象离开场景时，我们捕捉他们背后的背景以帮助算法。</p></figure><p id="cf31" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上图显示了我们如何轻松地提供真实背景的粗略估计。当这个人离开场景时，我们捕捉他们背后的背景。下图显示了这种情况:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/52fccb801d8bdc9a10b15743f0e96ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJ8OpK-jJ8ZTANJ_dYrQQg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">捕获的输入、捕获的背景和在新背景上合成的示例。</p></figure><p id="c2c7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">注意这张图片是多么具有挑战性，因为它有非常相似的背景和前景色(特别是在头发周围)。它也是用手持电话录制的，包含轻微的背景运动。</p><blockquote class="nf"><p id="cbc0" class="ng nh iq bd ni nj nk nl nm nn no lm dk translated">我们称之为随意捕捉的背景，因为它可以包含轻微的运动、颜色差异、轻微的阴影或与前景相似的颜色</p></blockquote><h2 id="7e36" class="mq lp iq bd lq mr nq dn lu mt nr dp ly la ns mw ma le nt my mc li nu na me nb bi translated">捕捉提示</h2><p id="8b30" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">尽管我们的方法在一些背景扰动下也能工作，但是当背景恒定时，它仍然是更好的，并且在室内环境中是最好的。例如，在存在由对象投射的非常明显的阴影、移动的背景(例如，水、汽车、树)或大的曝光变化的情况下，它不起作用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/3d8d7e4c60e532377e53f07148b9dc84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0gQhQqhPiVPZYhtX6CmtvA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">失败案例。这个人是在一个移动的喷泉前拍摄的。</p></figure><p id="3c82" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们还建议在视频结束时，通过让这个人离开场景来捕捉背景，并从连续的视频中提取该帧。当您从视频模式切换到照片模式时，许多手机都有不同的变焦和曝光设置。用手机拍摄时还应该启用自动曝光锁定。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/23a365db7902423110f826dd502d5dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I-MThxmb-WcD8ZOjcQLXmw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">理想的捕捉场景。背景在室内，不动，主体不投阴影</p></figure><p id="5968" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">捕获提示的摘要:</p><ol class=""><li id="27f7" class="nw nx iq kt b ku kv kx ky la ny le nz li oa lm ob oc od oe bi translated">选择你能找到的最稳定的背景。</li><li id="286e" class="nw nx iq kt b ku of kx og la oh le oi li oj lm ob oc od oe bi translated">不要站得离背景太近，这样你就不会投下阴影。</li><li id="ebae" class="nw nx iq kt b ku of kx og la oh le oi li oj lm ob oc od oe bi translated">在手机上启用自动曝光和自动对焦锁定。</li></ol><h2 id="5a58" class="mq lp iq bd lq mr ms dn lu mt mu dp ly la mv mw ma le mx my mc li mz na me nb bi translated">这种方法像不像背景减法？</h2><p id="0a61" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">另一个自然的问题是，这是否像背景减法。首先，如果很容易使用任何背景进行合成，电影业就不会这么多年来花费数千美元在绿色屏幕上。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/be7d2a15cac22f18a31f59d80f1270e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*bjy6W2QS--t8cwlSPskRww.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">背景减法对于随意捕捉的背景效果不佳</p></figure><p id="4b90" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，背景减法不解决部分 alpha 值，给出与分割相同的硬边缘。当有相似的前景和背景颜色或背景中有任何运动时，它也不能很好地工作。</p><h1 id="ae8e" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">网络详细信息</h1><p id="c138" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">该网络由监督步骤和非监督细化步骤组成。我们将在这里简要地总结一下，但是要了解全部的细节，你可以随时查阅这篇文章。</p><h2 id="5c06" class="mq lp iq bd lq mr ms dn lu mt mu dp ly la mv mw ma le mx my mc li mz na me nb bi translated">监督学习</h2><p id="54a1" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">为了首先训练网络，我们使用 Adobe Composition-1k 数据集，该数据集包含 450 个仔细注释的地面真相 alpha mattes。我们以完全监督的方式训练网络，输出上有每像素的损失。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/336ecc667fb96e83e70f37a4378ced08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rapPH4FfMZ2fFJJe5BAcw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">我们网络中受监管的部分。我们使用几个输入线索，然后输出一个阿尔法遮罩和预测的前景色。我们在 Adobe 1k 数据集上进行训练，并提供地面实况结果。</p></figure><p id="d974" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">注意，我们有几个输入，包括图像、背景、软分割和时间运动信息。我们新颖的<em class="nd">上下文切换模块</em>也确保了对不良输入的鲁棒性。</p><h2 id="616e" class="mq lp iq bd lq mr ms dn lu mt mu dp ly la mv mw ma le mx my mc li mz na me nb bi translated">使用 GANs 的无监督细化</h2><p id="c8a5" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">监督学习的问题是 adobe 数据集只包含 450 个地面真实输出，这远远不足以训练一个好的网络。获取更多的数据是极其困难的，因为它涉及到手工注释图像的 alpha 遮片。</p><p id="e6ff" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了解决这个问题，我们使用 GAN 细化步骤。我们从受监督的网络中获取输出阿尔法遮罩，并将其合成在新的背景上。然后鉴别器试图辨别这是真的还是假的图像。作为响应，发生器学习更新 alpha 遮罩，以使合成结果尽可能真实，从而欺骗鉴别器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/68430b63569abad1b85b5be796e91013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pypkRuzPYHlLbxOjQdCqqA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">我们的无监督 GAN 细化步骤。我们把前景放到一个新的背景上，然后阿甘试着分辨它是真是假。</p></figure><p id="ecfe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里重要的一点是，我们不需要任何带标签的训练数据。该鉴别器用数千幅很容易获得的真实图像进行训练。</p><h2 id="49f3" class="mq lp iq bd lq mr ms dn lu mt mu dp ly la mv mw ma le mx my mc li mz na me nb bi translated">根据您的数据训练 GAN</h2><p id="4c81" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">GAN 的另一个用处是，您可以根据自己的映像训练生成器，以提高测试时的结果。假设你跑网络，输出不是很好。你可以根据确切的数据更新生成器<em class="nd">的权重，以便更好地欺骗鉴别器。这将使您的数据过拟合，但会改善您提供的图像的结果。</em></p><h1 id="e6f2" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">未来的工作</h1><p id="81ea" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">虽然我们看到的结果相当不错，但我们仍在继续使这种方法更准确、更易于使用。</p><p id="c586" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">特别是，我们希望使这种方法对背景运动、摄像机运动、阴影等环境更加鲁棒。我们也在寻找方法使这种方法实时工作，并且使用更少的计算资源。这可以在视频流或移动应用程序等领域实现各种各样的用例。</p><p id="7d60" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果您有任何问题，欢迎<a class="ae ln" href="http://www.vivekjayaram.com" rel="noopener ugc nofollow" target="_blank">联系我</a>，Vivek Jayaram，或<a class="ae ln" href="https://homes.cs.washington.edu/~soumya91/" rel="noopener ugc nofollow" target="_blank"> Soumyadip Sengupta </a></p></div><div class="ab cl on oo hu op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="ij ik il im in"><h1 id="16f3" class="lo lp iq bd lq lr ou lt lu lv ov lx ly jw ow jx ma jz ox ka mc kc oy kd me mf bi translated">参考</h1><p id="a0a8" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">[1] S .森古普塔、v .贾亚拉姆、b .柯尔斯、s .塞茨和 I .凯梅尔马赫-什利泽曼，<a class="ae ln" href="https://grail.cs.washington.edu/projects/background-matting/" rel="noopener ugc nofollow" target="_blank">背景铺垫:世界是你的绿幕</a> (2020)，CVPR 2020</p><p id="b8a4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[2]陈立春，朱，帕潘德里欧，施若夫，亚当，<a class="ae ln" href="https://arxiv.org/abs/1802.02611" rel="noopener ugc nofollow" target="_blank"/>【2018】，ECCV 2018</p><p id="62fe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[3]庄玉英、柯尔斯、塞勒斯和塞利斯基，<a class="ae ln" href="https://grail.cs.washington.edu/projects/digital-matting/papers/cvpr2001.pdf" rel="noopener ugc nofollow" target="_blank"/>(2001)，CVPR，2001</p><p id="bd7e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[4]侯庆林和刘福林。<a class="ae ln" href="https://arxiv.org/abs/1909.09725" rel="noopener ugc nofollow" target="_blank">用于同步前景和阿尔法估计的上下文感知图像抠图</a> (2019)，ICCV 2019</p><p id="088c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[5] H. Lu，Y. Dai，C. Shen，和 S. Xu，<a class="ae ln" href="https://arxiv.org/abs/1908.00672" rel="noopener ugc nofollow" target="_blank">索引很重要:学习索引用于深度图像抠图</a> (2019)，2019</p></div></div>    
</body>
</html>