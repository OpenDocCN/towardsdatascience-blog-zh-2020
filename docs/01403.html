<html>
<head>
<title>An intuitive guide to information compression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">信息压缩直观指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-guide-to-information-compression-b735490c545e?source=collection_archive---------33-----------------------#2020-02-07">https://towardsdatascience.com/an-intuitive-guide-to-information-compression-b735490c545e?source=collection_archive---------33-----------------------#2020-02-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="598d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">信息:概率的负倒数值。</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi kj"><img src="../Images/7b1a8b3091311e0821f9ddc23fa029c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Z2OYiuncZfxlYa79xefHKw.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">瓶颈(自行点击图片)</p></figure><p id="19eb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">维数灾难指的是在高维空间中分析和组织数据时出现的各种现象，这些现象在低维环境中不会出现，例如日常经验的三维物理空间。因此，支持结果所需的数据量通常随着维数呈指数增长，而这并不总是可行的。此外，在处理更多的输入维度时也存在计算障碍。</p><p id="e9ae" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">Claude Shannon 的噪声信道编码定理</strong>表明，虽然从信源到接收机的信息传输过程中可能会添加噪声，并且对于通信信道的任何给定程度的噪声污染，都有可能通过信道以可计算的最大速率几乎无差错地传输离散数据。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/a1b35a0dfa822e50f12100c9b47cdad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8pz2xv16PCs_zob455vVJg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:维基媒体</p></figure><p id="1a35" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这激发了对基于编码器和解码器架构的更密集的低维输入表示的需求。其中除了解码器能够很好地有效估计消息之外，编码应该尽可能高效。有各种各样的降维技术和压缩方法可以找到解决方法。在这篇文章中，我将尝试给出一些这样的突出思想的直观概述。</p><h2 id="8f16" class="lw lx it bd ly lz ma dn mb mc md dp me le mf mg mh li mi mj mk lm ml mm mn mo bi translated">信息瓶颈法</h2><p id="2c3f" class="pw-post-body-paragraph kv kw it kx b ky mp ju la lb mq jx ld le mr lg lh li ms lk ll lm mt lo lp lq im bi translated">这种压缩方法的一个有趣的原理是来自信息论的信息瓶颈方法。如论文<em class="mu">中所述的信息瓶颈方法</em></p><blockquote class="mv mw mx"><p id="9747" class="kv kw mu kx b ky kz ju la lb lc jx ld my lf lg lh mz lj lk ll na ln lo lp lq im bi translated">理解信号 X 不仅需要预测 y，还需要指定 X 的哪些特征在预测中起作用。我们将问题形式化为寻找 X 的一个短码，该短码保留关于 Y 的最大信息。也就是说，我们通过有限的码字集 X \\'形成的“瓶颈”挤压 X 提供的关于 Y 的信息。</p></blockquote><p id="930a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">x 中关于 Y 的信息量由下式给出</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/fbfa4571066710a24017526176ae2204.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*kLc49ohFwSWH_TFcGVosuA.png"/></div></figure><p id="9214" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们可以通过最小化损失来找到最佳瓶颈表示 x</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/92d8d06479dfff837af88ab5f4ba91c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*H2DkQZiQz4vSBiB0tZInHg.png"/></div></figure><p id="834c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">β是用于改变压缩分辨率的拉格朗日乘数。这直观地意味着瓶颈表示应该编码更多关于目标变量的信息和更少的自变量。这为我们提供了信息压缩的理想目标。</p><h2 id="fedf" class="lw lx it bd ly lz ma dn mb mc md dp me le mf mg mh li mi mj mk lm ml mm mn mo bi translated">主成分分析</h2><p id="149f" class="pw-post-body-paragraph kv kw it kx b ky mp ju la lb mq jx ld le mr lg lh li ms lk ll lm mt lo lp lq im bi translated">PCA 是一种正交线性变换，它将数据变换到超椭球形式的新坐标系中，使得数据的最大方差点位于第一主分量轴上，第二大方差点位于第二坐标上，依此类推。如何将这一想法进一步用于降维，我们移除解释最小方差的轴。</p><p id="2e69" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这是通过计算数据的协方差矩阵，找出协方差矩阵的特征向量和特征值，然后最终按照特征值的降序排列特征向量来实现的。这些特征向量被称为主分量。</p><p id="bf5e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">PCA 用于一般的降维，也用于将多维点投影到更小的维度以便可视化。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/7014cee483d7ae031a180b4c2bd1734b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*GaI9TL5tp3jwdCP5mTcxAg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:维基媒体</p></figure><h2 id="7e22" class="lw lx it bd ly lz ma dn mb mc md dp me le mf mg mh li mi mj mk lm ml mm mn mo bi translated">奇异值分解</h2><p id="d676" class="pw-post-body-paragraph kv kw it kx b ky mp ju la lb mq jx ld le mr lg lh li ms lk ll lm mt lo lp lq im bi translated">这种思想可以用在许多应用中，其中很少是在推荐系统中，提取单词嵌入，执行 PCA，计算伪逆，伪逆大量用于找出封闭解，如在线性回归和一般矩阵分解的情况下。它是特征分解的推广，要求矩阵是正方形的，但这里也可以是矩形的。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/57491870f97cf990f85eeb4705798ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*XEK-zOJv2U1CWmczfqrS3g.png"/></div></figure><p id="f9d2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">其中 M 是分解成潜在矩阵 U 和 V*的矩阵，也称为左右奇异矩阵，<strong class="kx iu">σ</strong>等于<strong class="kx iu"> M </strong>的奇异值。M*M 的特征向量构成 V 的列，MM*的特征向量构成 U 的列，其中*对应于矩阵的转置。奇异值是 M*M 或 MM*的特征值的平方根。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/a42e335e223c5427a84818a68a95a954.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*-Xl32rTpROuue6IQ3qZ1MQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:维基媒体</p></figure><h2 id="a720" class="lw lx it bd ly lz ma dn mb mc md dp me le mf mg mh li mi mj mk lm ml mm mn mo bi translated">自动编码器</h2><p id="5b9a" class="pw-post-body-paragraph kv kw it kx b ky mp ju la lb mq jx ld le mr lg lh li ms lk ll lm mt lo lp lq im bi translated">自动编码器是最流行的信息压缩工具之一，它被扩展用于各种任务，例如嵌入学习、图像处理、维数减少和异常检测。</p><p id="157b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">自动编码器的目的是学习编码，然后从简化的编码中重建尽可能接近其原始输入的表示。为了迫使自动编码器学习更鲁棒的表示，使用了正则化版本。</p><p id="67d6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">稀疏自动编码器不一定需要较少数量的隐藏单元，而是只允许少量的隐藏单元同时活动。为了在隐藏单元的激活中实现稀疏性，由某个参数缩放的激活的 L1 或 L2 正则化被添加到损失函数中。</p><p id="0fc5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">去噪自动编码器采用部分受损的输入，并被训练以恢复原始的未失真输入，从而去除分离噪声。</p><p id="1819" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">收缩自动编码器试图实现类似于稀疏编码器的东西，不同之处在于它试图压缩编码器激活的雅可比矩阵相对于由某个参数缩放的输入的 Frobenius 范数。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/665ad2f814e509349cfa8851f0425d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*wKE69-fX180Q_gkzYzGbwg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:维基媒体</p></figure><h2 id="72c6" class="lw lx it bd ly lz ma dn mb mc md dp me le mf mg mh li mi mj mk lm ml mm mn mo bi translated">卷积神经网络</h2><p id="ceeb" class="pw-post-body-paragraph kv kw it kx b ky mp ju la lb mq jx ld le mr lg lh li ms lk ll lm mt lo lp lq im bi translated">从自然语言处理到计算机视觉，卷积神经网络组件找到了自己的位置。1x1 卷积、卷积 1D、卷积 2D 与池结合为我们提供了有效的压缩技术。它们主要用于计算机视觉任务，这表明它在处理如此高输入维度空间方面的有效性。</p><p id="b58c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在完全连接的神经网络中，每个神经元接收来自前一层的每个元素的输入。在卷积层中，神经元仅接收来自前一层的受限子区域的输入，该子区域称为感受野。在卷积层中，与全连接层相比，接收区域小于整个前一层。这些网络提供了各种优势，例如本地连接和参数共享。因此，它使我们能够有效地工作与压缩低维表示的图像。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/9173149055cb9331cac64f1016cda8ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*94AQWZXc2cmgeXSWZ9hVnQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:维基媒体</p></figure><h2 id="c277" class="lw lx it bd ly lz ma dn mb mc md dp me le mf mg mh li mi mj mk lm ml mm mn mo bi translated">t-SNE </h2><p id="2cd9" class="pw-post-body-paragraph kv kw it kx b ky mp ju la lb mq jx ld le mr lg lh li ms lk ll lm mt lo lp lq im bi translated">这是一种非线性降维技术，将数据嵌入二维或三维的低维空间，主要用于可视化目的。</p><p id="6fac" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">t-SNE 计算高维空间和低维空间中的实例对之间的相似性度量，然后使用使用梯度下降优化的 KL 散度最小化这些相似性在更高和更低维空间中的差异。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ni"><img src="../Images/917b2761e0a65f2979b78f82bc460781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*QCW6jkv_F7V5DDjGkaGxsg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出</p></figure><h2 id="2e72" class="lw lx it bd ly lz ma dn mb mc md dp me le mf mg mh li mi mj mk lm ml mm mn mo bi translated">结论</h2><blockquote class="mv mw mx"><p id="c3aa" class="kv kw mu kx b ky kz ju la lb lc jx ld my lf lg lh mz lj lk ll na ln lo lp lq im bi translated"><em class="it">“我喜欢密度，不喜欢体积。”</em></p></blockquote><p id="aa90" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">—莫林·霍华德</p></div></div>    
</body>
</html>