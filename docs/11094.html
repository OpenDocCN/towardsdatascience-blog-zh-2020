<html>
<head>
<title>Speeding Up the Conversion Between PySpark and Pandas DataFrames</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">加快 PySpark 和 Pandas 数据帧之间的转换</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-efficiently-convert-a-pyspark-dataframe-to-pandas-8bda2c3875c3?source=collection_archive---------8-----------------------#2020-08-02">https://towardsdatascience.com/how-to-efficiently-convert-a-pyspark-dataframe-to-pandas-8bda2c3875c3?source=collection_archive---------8-----------------------#2020-08-02</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="0337" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">将大型 Spark 数据帧转换为熊猫时节省时间</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/bf458e4414027c0ab3262d120a8fcbef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jEPLmqaM3KzSTrrhIeW4uA.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">诺亚·博加德在<a class="ae kz" href="https://unsplash.com/photos/mJFtH4FFl7o" rel="noopener ugc nofollow" target="_blank">unsplash.com</a>拍摄的照片</p></figure><p id="e9ea" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">由于使用了<code class="fe lw lx ly lz b">toPandas()</code>方法，将 PySpark 数据帧转换成 Pandas 非常简单。然而，这可能是成本最高的操作之一，必须谨慎使用，尤其是在处理大量数据时。</p><h1 id="3884" class="ma mb iu bd mc md me mf mg mh mi mj mk ka ml kb mm kd mn ke mo kg mp kh mq mr bi translated">为什么这么贵？</h1><p id="72bb" class="pw-post-body-paragraph la lb iu lc b ld ms jv lf lg mt jy li lj mu ll lm ln mv lp lq lr mw lt lu lv in bi translated">Pandas 数据帧存储在内存中，这意味着对它们的操作执行起来更快，然而，它们的大小受到单台机器内存的限制。</p><p id="d817" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">另一方面，Spark 数据帧分布在由至少一台机器组成的 Spark 集群的节点上，因此数据帧的大小受到集群大小的限制。当需要存储更多数据时，只需通过添加更多节点(和/或向节点添加更多内存)来扩展集群。</p><p id="3f68" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">重要的是要理解，当在 Spark 数据帧上执行<code class="fe lw lx ly lz b">toPandas()</code> <strong class="lc iv"> </strong>方法<strong class="lc iv"> </strong>时，分布在集群<strong class="lc iv">的节点上的所有行将被收集到驱动程序</strong>中，该驱动程序需要有足够的内存来容纳数据。</p><h1 id="d59f" class="ma mb iu bd mc md me mf mg mh mi mj mk ka ml kb mm kd mn ke mo kg mp kh mq mr bi translated">用 PyArrow 加速转换</h1><p id="e08f" class="pw-post-body-paragraph la lb iu lc b ld ms jv lf lg mt jy li lj mu ll lm ln mv lp lq lr mw lt lu lv in bi translated"><a class="ae kz" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Arrow </a>是一种独立于语言的内存列格式，当使用<code class="fe lw lx ly lz b">toPandas()</code>或<code class="fe lw lx ly lz b">createDataFrame()</code>时，可用于优化 Spark 和 Pandas 数据帧之间的转换。</p></div><div class="ab cl mx my hy mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="in io ip iq ir"><p id="91a1" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">首先，我们需要确保安装兼容的<code class="fe lw lx ly lz b">PyArrow</code>和<code class="fe lw lx ly lz b">pandas</code>版本。前者为<strong class="lc iv"> 0.15.1 </strong>，后者为<strong class="lc iv"> 0.24.2 </strong>。最近的版本也可能兼容，但目前 Spark 不提供任何保证，所以这很大程度上取决于用户测试和验证兼容性。有关所需版本和兼容性的更多详细信息，请参考<a class="ae kz" href="https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#recommended-pandas-and-pyarrow-versions" rel="noopener ugc nofollow" target="_blank"> Spark 的官方文档</a>。</p></div><div class="ab cl mx my hy mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="in io ip iq ir"><p id="7b6a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">默认情况下，基于箭头的列数据传输是禁用的，因此我们必须稍微修改我们的配置，以便利用这种优化。为此，应启用<code class="fe lw lx ly lz b">spark.sql.execution.arrow.pyspark.enabled</code> <strong class="lc iv"> </strong>。</p><pre class="kk kl km kn gu ne lz nf ng aw nh bi"><span id="ff4b" class="ni mb iu lz b gz nj nk l nl nm">import numpy as np<br/>import pandas as pd<br/><br/># Enable Arrow-based columnar data <!-- -->spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")</span><span id="7daa" class="ni mb iu lz b gz nn nk l nl nm"># Create a dummy Spark DataFrame<br/>test_sdf = spark.range(0, 1000000)</span><span id="a60e" class="ni mb iu lz b gz nn nk l nl nm"># Create a pandas DataFrame from the Spark DataFrame using Arrow<br/>pdf = test_sdf.toPandas()</span><span id="5f09" class="ni mb iu lz b gz nn nk l nl nm"># Convert the pandas DataFrame back to Spark DF using Arrow<br/>sdf = spark.createDataFrame(pdf)</span></pre><p id="d401" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">如果在实际计算之前出现错误，PyArrow 优化将被禁用。如果我们想避免潜在的非箭头优化，我们还需要启用以下配置:</p><pre class="kk kl km kn gu ne lz nf ng aw nh bi"><span id="e501" class="ni mb iu lz b gz nj nk l nl nm">spark.conf.set(<br/>    "spark.sql.execution.arrow.pyspark.fallback.enabled", "true"<br/>)</span></pre><h1 id="aef5" class="ma mb iu bd mc md me mf mg mh mi mj mk ka ml kb mm kd mn ke mo kg mp kh mq mr bi translated">PyArrow 的局限性</h1><p id="4fde" class="pw-post-body-paragraph la lb iu lc b ld ms jv lf lg mt jy li lj mu ll lm ln mv lp lq lr mw lt lu lv in bi translated">目前，基于箭头的优化转换不支持某些 Spark SQL 数据类型。这些类型是:</p><ul class=""><li id="36d9" class="no np iu lc b ld le lg lh lj nq ln nr lr ns lv nt nu nv nw bi translated"><code class="fe lw lx ly lz b">MapType</code></li><li id="b39a" class="no np iu lc b ld nx lg ny lj nz ln oa lr ob lv nt nu nv nw bi translated"><code class="fe lw lx ly lz b">TimestampType</code>中的<code class="fe lw lx ly lz b">ArrayType</code></li><li id="67ad" class="no np iu lc b ld nx lg ny lj nz ln oa lr ob lv nt nu nv nw bi translated">嵌套<code class="fe lw lx ly lz b">StructType</code></li></ul><h1 id="ea38" class="ma mb iu bd mc md me mf mg mh mi mj mk ka ml kb mm kd mn ke mo kg mp kh mq mr bi translated">结论</h1><p id="6fd0" class="pw-post-body-paragraph la lb iu lc b ld ms jv lf lg mt jy li lj mu ll lm ln mv lp lq lr mw lt lu lv in bi translated">正如我们已经讨论过的，<code class="fe lw lx ly lz b">toPandas()</code>是一个开销很大的操作，应该小心使用，以尽量减少对 Spark 应用程序的性能影响。在需要这样做的情况下，尤其是当数据帧相当大时，在将 Spark 转换为 Pandas 数据帧时，需要考虑 PyArrow 优化(反之亦然)。</p></div><div class="ab cl mx my hy mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="in io ip iq ir"><p id="e6db" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><a class="ae kz" href="https://gmyrianthous.medium.com/membership" rel="noopener"> <strong class="lc iv">成为会员</strong> </a> <strong class="lc iv">阅读介质上的每一个故事。你的会员费直接支持我和你看的其他作家。</strong></p></div></div>    
</body>
</html>