<html>
<head>
<title>Classifying Documents with Quantum-enhanced Transfer Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于量子增强迁移学习的文档分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classifying-documents-with-quantum-enhanced-transfer-learning-8ee6d04f3ccd?source=collection_archive---------21-----------------------#2020-01-16">https://towardsdatascience.com/classifying-documents-with-quantum-enhanced-transfer-learning-8ee6d04f3ccd?source=collection_archive---------21-----------------------#2020-01-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c179" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用变化的量子电路来补充预先训练的模型，将我们带到了量子增强的自然语言处理时代</h2></div><p id="00b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最近，一个趋势非常明显:需要大型的预训练模型来实现计算机视觉和语言理解的最先进的性能。特别是，随着基于 Transformer 架构的模型的出现，自然语言处理(NLP)领域的能力和网络规模都在呈指数级增长(例如<a class="ae le" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">伯特</a>、<a class="ae le" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>)。在本文中，我将通过使用通用语句编码器(USE)嵌入和一种称为<em class="lf">变分量子电路</em>的量子机器学习操作符，重点关注短文档(科学论文的预印本)的分类，粗略地说，这相当于经典机器学习领域中的全连接(密集)层。关于单词和句子嵌入如何用于分类的介绍，参见我之前的文章<a class="ae le" rel="noopener" target="_blank" href="/classifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44">用通用的句子嵌入对科学论文进行分类</a>。如果你不熟悉量子计算的概念，我希望你会喜欢阅读我的文章<a class="ae le" href="https://medium.com/@riccardo.disipio/spooky-computers-at-cern-c08f8756444" rel="noopener">CERN 的幽灵计算机</a>。</p><h2 id="55b6" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">量子机器学习</h2><p id="301e" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">在我们深入研究实际代码(你可以在<a class="ae le" href="https://github.com/rdisipio/qnlp" rel="noopener ugc nofollow" target="_blank">https://github.com/rdisipio/qnlp</a>上找到)之前，需要几句话来解释什么是量子机器学习，以及它如何对这种应用有用。</p><p id="4f16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，在量子计算中，人们处理称为<a class="ae le" href="https://en.wikipedia.org/wiki/Qubit" rel="noopener ugc nofollow" target="_blank">量子位</a>的等效位，由于量子系统的一种称为叠加的属性(有时据说它们可以同时代表 0 和 1)，量子位众所周知可以处理非二进制状态的信息。量子电路是应用于量子位的一系列操作，通过改变它们的相对相位来改变它们的状态<em class="lf">，例如</em>。量子位可以用所谓的<a class="ae le" href="https://en.wikipedia.org/wiki/Bloch_sphere" rel="noopener ugc nofollow" target="_blank">布洛赫球</a>进行几何表示，因此对量子位的操作对应于这个虚拟空间中量子状态向量的旋转。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi me"><img src="../Images/529e0c2cdabaabd9852f01973ddd0fe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q9B7zvFYPNo2PnXclA3xzQ.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">布洛赫球的图形表示。对量子位的量子操作对应于这个虚拟空间中量子状态向量的旋转。[鸣谢:维基百科]</p></figure><p id="c914" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据计算的目的，有许多可能的量子电路。一般来说，量子电路实现一个量子算法。关于什么样的计算是可能的，请参考<a class="ae le" href="https://www.microsoft.com/en-ca/quantum/" rel="noopener ugc nofollow" target="_blank">微软</a>的<a class="ae le" href="https://www.microsoft.com/en-us/research/people/stjorda/" rel="noopener ugc nofollow" target="_blank">斯蒂芬·乔丹</a>维护的<a class="ae le" href="https://quantumalgorithmzoo.org/" rel="noopener ugc nofollow" target="_blank">量子算法动物园</a>。最著名的大概就是<a class="ae le" href="https://en.wikipedia.org/wiki/Shor%27s_algorithm" rel="noopener ugc nofollow" target="_blank">彼得·肖尔的因式分解算法</a>。量子算法的一个关键概念是，由于固有的自然量子位，某些计算可以以比经典等价计算更小的复杂性来执行(这种特性被称为<em class="lf">超多项式加速</em>)。完全公平地说，也有很多正在进行的研究试图“去量子化”这种算法，并使经典版本和量子版本一样快——这是因为量子比特的初始状态必须在运行实际算法之前准备好，这降低了整体加速。这一方向的开创性论文是 Ewin Tang 的“<a class="ae le" href="https://arxiv.org/pdf/1811.00414.pdf" rel="noopener ugc nofollow" target="_blank">量子启发的经典主成分分析和监督聚类算法</a>”。</p><p id="8b96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事实上，实际应用更可能是经典和量子运算的混合。尤其是在机器学习领域，人们可能会想，这两个领域是否可以结合在一起，联手合作，好消息是，事实上他们可以，他们也确实这样做了。加拿大初创公司<a class="ae le" href="https://www.xanadu.ai/" rel="noopener ugc nofollow" target="_blank"> Xanadu </a>开发的一个名为<a class="ae le" href="https://pennylane.ai/" rel="noopener ugc nofollow" target="_blank"> PennyLane </a>的软件库提供了常见 ML 框架如<a class="ae le" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>和<a class="ae le" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>与量子计算生态系统之间的接口。主要的想法很简单，但非常强大:人们可以用量子对应物代替神经网络的一部分(比如说致密层)，就是这样！你创造了一个混合经典量子神经网络。事实上，由于经典机器学习并不仅限于神经网络，量子机器学习也在各种方法中找到了它的优势，如<a class="ae le" href="https://peterwittek.com/understanding-quantum-svms.html" rel="noopener ugc nofollow" target="_blank">量子支持向量机</a>。为了全面了解什么是可能的，我建议关注 edX 上的在线课程<a class="ae le" href="https://www.edx.org/course/quantum-machine-learning" rel="noopener ugc nofollow" target="_blank">，该课程由多伦多大学已故的彼得·魏特克教授。</a></p><p id="a6d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://qmlt.readthedocs.io/en/latest/variational.html" rel="noopener ugc nofollow" target="_blank">变分电路</a>(也称为<em class="lf">参数化量子电路</em>)是一系列混合量子经典算法，以类似于经典致密层的方式执行操作。该算法的核心是依赖于一组参数𝜃的<em class="lf">量子电路</em> 𝑈，以及定义待优化标量的<em class="lf">目标函数</em>。电路可以被认为是一个黑盒，优化通常是通过迭代方案实现的，例如通过<a class="ae le" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">基于梯度的优化</a>。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mu"><img src="../Images/1d8b73e2a4e1b6407b8d7bf2537db8a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UiDSatBJ7Rifel1ADlGw-g.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">变分量子电路示意图。[鸣谢:Xanadu.ai]</p></figure><h2 id="e6af" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">文件分类</h2><p id="8596" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">正如我在<a class="ae le" rel="noopener" target="_blank" href="/classifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44">用通用句子嵌入分类科学论文</a>中解释的，谷歌在<a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">tensorflohub</a>上发布了一个预训练版本的<a class="ae le" href="https://arxiv.org/abs/1803.11175" rel="noopener ugc nofollow" target="_blank">通用句子编码器</a>(使用)模型。要使用它:</p><pre class="mf mg mh mi gt mv mw mx my aw mz bi"><span id="a431" class="lg lh it mw b gy na nb l nc nd">import tensorflow_hub as hub<br/>embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")<br/>embeddings = embed([<br/>    "The quick brown fox jumps over the lazy dog.",<br/>    "I am a sentence for which I would like to get its embedding"])</span></pre><p id="2108" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出由一个 512 维的向量列表组成，每个向量对应于输入中的一个文本字符串(在上面的例子中<em class="lf">例如</em>“快速棕色狐狸…”)。按照迁移学习范式，这个模型可以作为一个更大的网络的一部分。人们可以直接使用它，其参数是固定的，或者通过允许优化者修改它们来微调它。反过来，使用模型的输出被传递到其他层，以便为给定的下游任务训练网络的其余部分。有几个来自官方文档的例子，但是文档分类可能是最直观的。在这个例子中，我们将把属于两个非常不同的类别(即<code class="fe ne nf ng mw b">astro-ph</code>和<code class="fe ne nf ng mw b">cs.AI</code>)的预印本的摘要转换成句子嵌入，并附加一个具有两个输出的密集层。每个输出节点对应一个类别，因此在将输出与实际标签进行比较之前，应用一个<a class="ae le" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax 激活</a>功能。这是一个简单模型的样子:</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="b676" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有趣的是，人们可以在几秒钟内只用 1000 个例子训练这样的模型，但却获得 99.6%的准确率。</p><p id="9fc2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了“量子化”我们的模型，我们必须用一个变分的量子电路来代替位于嵌入和输出之间的层。典型地，经典密集层具有<em class="lf"> N </em>个输入和<em class="lf"> M </em>个输出，因此在内部，它对应于矩阵乘法，随后是偏置的相加，以及激活函数的应用，<em class="lf">，例如</em>:</p><pre class="mf mg mh mi gt mv mw mx my aw mz bi"><span id="93e1" class="lg lh it mw b gy na nb l nc nd">def call(self, inputs):</span><span id="9d0e" class="lg lh it mw b gy nj nb l nc nd">   x = tf.matmul(inputs, self.W1) + self.b1<br/>   x = tf.math.tanh(x)</span><span id="c590" class="lg lh it mw b gy nj nb l nc nd">   return x</span></pre><p id="2a6d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不幸的是，量子层不能明确地做到这一点，而是需要<em class="lf">修饰</em>。这意味着量子变分层实际上由三个操作组成:</p><ul class=""><li id="aced" class="nk nl it kk b kl km ko kp kr nm kv nn kz no ld np nq nr ns bi translated">一个经典的密集层，将维度从 N_inputs 转换为 N _ qubits，并将输入缩放π/2(因此它可以表示围绕 Bloch 球的旋转)</li><li id="d19f" class="nk nl it kk b kl nt ko nu kr nv kv nw kz nx ld np nq nr ns bi translated">改变量子位状态的量子层</li><li id="7c5b" class="nk nl it kk b kl nt ko nu kr nv kv nw kz nx ld np nq nr ns bi translated">一个经典的稠密层，将维度从 N 个量子位转换成 N 个输出</li></ul><p id="1cfe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了用 PennyLane 库做到这一点，必须定义一个执行量子操作的设备(例如模拟器<code class="fe ne nf ng mw b">default.qubit</code>，或者一个真实的设备，比如由<a class="ae le" href="https://www.ibm.com/quantum-computing/?p1=Search&amp;p4=p50385922780&amp;p5=e&amp;cm_mmc=Search_Google-_-1S_1S-_-WW_NA-_-ibm%20quantum%20experience_e&amp;cm_mmca7=71700000061253574&amp;cm_mmca8=kwd-397733481108&amp;cm_mmca9=EAIaIQobChMI-bP58eSI5wIVGKSzCh13BACoEAAYASAAEgJE_PD_BwE&amp;cm_mmca10=409646905863&amp;cm_mmca11=e&amp;gclid=EAIaIQobChMI-bP58eSI5wIVGKSzCh13BACoEAAYASAAEgJE_PD_BwE&amp;gclsrc=aw.ds" rel="noopener ugc nofollow" target="_blank"> IBM </a> Q 或者<a class="ae le" href="https://www.rigetti.com/" rel="noopener ugc nofollow" target="_blank"> Rigetti </a>提供的设备)。然后，实际电路被编码到 python 函数<em class="lf">中，例如</em>:</p><pre class="mf mg mh mi gt mv mw mx my aw mz bi"><span id="f62c" class="lg lh it mw b gy na nb l nc nd">def _circuit(inputs, parameters):</span><span id="9201" class="lg lh it mw b gy nj nb l nc nd">   qml.templates.embeddings.AngleEmbedding(inputs, wires=list(range(self.n_qubits)))</span><span id="cd1f" class="lg lh it mw b gy nj nb l nc nd">   qml.templates.layers.StronglyEntanglingLayers(parameters, wires=list(range(self.n_qubits)))</span><span id="ff34" class="lg lh it mw b gy nj nb l nc nd">   return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]</span></pre><p id="dfae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以将许多致密层堆叠在一起，以增加网络的深度，量子变分层也可以做到这一点。在这个例子中，我们将保持简单，用四个量子位和两层快速运行。</p><p id="f95b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，有一个包装类<code class="fe ne nf ng mw b">QNode</code>负责将命令发送到后端设备，并得到结果。最值得注意的是，这个类通过简单地指定一个参数<em class="lf">，例如</em>，为 TensorFlow 和 PyTorch 提供了接口:</p><pre class="mf mg mh mi gt mv mw mx my aw mz bi"><span id="2b3c" class="lg lh it mw b gy na nb l nc nd">self.layer = qml.QNode(_circuit, self.dev, interface="tf")</span></pre><p id="4643" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">整个量子模型看起来像这样:</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="68f3" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">结果和讨论</h2><p id="f4a2" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">使用模拟器训练分类器的量子版本需要长得多的时间(每个时期大约一分钟，批量大小为 32)。这并不奇怪:如果我们有有效的量子过程模拟，我们首先就不需要量子硬件了！事实是，这种东西在原则上是不存在的，因为量子物理具有像纠缠这样的性质，而这在经典领域是没有对应的。因此，要传达的信息是，量子过程的模拟需要很长时间，并且随着量子比特数量的增加，变得越来越难以管理。另一方面，现实生活中的量子硬件是嘈杂的，因此约翰·普雷斯基尔定义我们目前生活在<a class="ae le" href="https://arxiv.org/abs/1801.00862" rel="noopener ugc nofollow" target="_blank">嘈杂的中等规模量子(NISQ)技术</a>时代。事实上，热噪声限制了在不引入误差的情况下进行长时间计算的能力，并且可能是今天实现创建大规模、可靠的量子硬件的目标的主要障碍。</p><p id="f727" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">也就是说，与纯经典模型相比，混合网络能够以 98.8%的准确度执行分类。这也不应该太令人惊讶，因为繁重的工作实际上是由使用嵌入层完成的。然而，这实际上是一个了不起的结果:它意味着我们可以部署迁移学习，这样我们就不必为了同样的目的训练一个纯粹的量子网络，而只是将其用于下游的任务。在 PennyLane 网站上有另一个关于图像分类的迁移学习的例子，我鼓励大家去看看。</p><p id="a3fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">量子增强的自然语言处理时代即将到来！</p></div></div>    
</body>
</html>