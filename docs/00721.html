<html>
<head>
<title>Part 1: Episodic Meta-RL and the Brain</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第一部分:情节性元学习和大脑</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/part-1-episodic-meta-rl-and-the-brain-909b9bccfc29?source=collection_archive---------41-----------------------#2020-01-20">https://towardsdatascience.com/part-1-episodic-meta-rl-and-the-brain-909b9bccfc29?source=collection_archive---------41-----------------------#2020-01-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="bc34" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="81cf" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">弥合机器学习和神经科学之间的差距</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/ed3c833f5298723fb9b06a0034278e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bswsV7x2x_PqvkbVgpdwLg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由摩根·豪斯尔在 Unsplash 上拍摄</p></figure><p id="8373" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这是我硕士学位论文系列博文的第一部分。它假设了一些神经科学和强化学习的初步知识，但不用担心，如果你有任何问题，我会在评论区。先简单介绍一下我:我获得了伦敦大学金史密斯学院的计算认知神经科学(CCN)硕士学位。因为我来自计算机科学/机器学习背景，所以我决定从事这两个领域交叉的研究项目。我很大程度上受到了 DeepMind 神经科学团队在<a class="ae ma" href="https://deepmind.com/blog/article/prefrontal-cortex-meta-reinforcement-learning-system" rel="noopener ugc nofollow" target="_blank">meta-RL</a>【20，21】上所做的工作以及萨姆·里特的博士论文【22】的启发，这篇文章在很大程度上是基于这些论文。这实际上是我决定去 CCN 攻读硕士学位的主要原因之一。这让我觉得，如果一个人想成为人工智能领域的主要研究人员，就有必要了解大脑。这篇文章讨论了情景元学习和大脑的关系，试图弥合机器学习和神经科学之间的差距。</p><h1 id="9b4a" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">动机</h1><p id="b354" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">近年来，深度强化学习(RL)一直处于人工智能(AI)研究的前沿，在一系列不同的领域取得了突破[1，2，3]。这导致认知科学家转向人工智能的发展，并询问它是否能给我们任何关于人类认知机制的见解[21]；考虑到 RL 最初的灵感来源于动物条件反射的心理学研究[4，5]。以伊凡·巴甫洛夫的开创性工作为例。在他的实验中，狗开始对蜂鸣器的声音垂涎三尺，将它与食物联系起来。因此，他们能够在实际观察之前就预测到奖励。使用这种预测信号，人们可以训练狗或任何动物来执行特定的任务。类似地，RL 算法可以通过正负奖励来指导人工智能体执行一组使累积长期奖励最大化的动作。</p><p id="1bc7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然而，deep-RL 的一个主要缺点是其样本效率低[21]，这自动使其失去了作为生物学习的合理模型的资格。换句话说，为了达到适当的专业水平，一个人工智能体所需要的经验比人类所需要的要多几个数量级。为了解决这个问题，Botvinick 等人(2019 年)确定了导致 deep-RL 缓慢的两个主要原因；这些是递增的参数调整，从弱感应偏置开始。前者是为了避免灾难性的推断，而后者允许神经网络适应广泛的问题。考虑到这一点，他们表明，这两个因素都可以通过使用情景记忆和学习任务分配的元学习来缓解。他们还指出，这两种解决方案都被证明对神经科学和心理学有进一步的影响。</p><h1 id="ab0f" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">与生物学的关系</h1><h1 id="5e49" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">RL 和纹状体多巴胺系统</h1><p id="c599" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">首先，让我们回溯一下，看看传统的 RL 算法为神经科学提供了什么。据观察，<a class="ae ma" href="https://en.wikipedia.org/wiki/Dopaminergic_pathways" rel="noopener ugc nofollow" target="_blank">中脑多巴胺神经元</a>的放电特性驱动纹状体中的突触可塑性，以巩固经验行为和奖励之间的联系[6]。RL 的研究后来引导神经科学家发展了多巴胺能功能的基于奖励的学习理论。具体来说，多巴胺释放编码了一个“惊喜”指数，反映了由<a class="ae ma" href="https://en.wikipedia.org/wiki/Temporal_difference_learning" rel="noopener ugc nofollow" target="_blank">时间差(TD) </a>学习算法预测的奖励预测误差(RPE)信号，或者换句话说，预期和实际奖励之间的差异【7】。对非人灵长类动物、人类和啮齿类动物的进一步研究集中在一个经典模型上，在该模型中，突触前和突触后神经元的同步放电在多巴胺存在时导致更强的突触连接，而在多巴胺不存在时导致更弱的连接[8]。换句话说，导致 RPE 信号增加的神经元放电通过所谓的“三因素规则”得到加强[9]。</p><h1 id="825c" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">Meta-RL 和前额叶皮层</h1><p id="657e" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">然而，这一经典模型受到了来自前额叶皮层(PFC)的一系列发现的挑战。已经表明，PFC 的扇区对 RL 的基本量进行编码，例如动作和状态的期望值[10，11]，以及奖励和动作的近期历史[12，13]。这表明 PFC 实现了基于奖励的学习机制，类似于以前归因于基于多巴胺的 RL 的机制。因此，一个问题自然产生了:这两个系统之间的关系是什么？Wang 等人(2018)通过提出一种新的理论解决了这一难题，在该理论中，PFC 和基于多巴胺的 RL 是两个独立的 RL 系统，实现了不同形式的学习。具体来说，他们表明 PFC 通过利用任务结构的表示来执行基于<em class="my">模型的</em> RL，而不是基于由多巴胺突触学习驱动的直接刺激-反应关联的<em class="my">无模型</em> RL。在这个理论中，前额叶网络中的突触权重是由基于多巴胺的 RL 在一系列相关任务中形成的。因此，无模型 RL 产生了能够快速适应新环境的第二种独立的基于模型的 RL，并且被称为“meta-RL”。Wang 等人(2018)通过大量模拟证明，meta-RL 可以解释大范围的行为和神经生理学发现，这些发现为基于多巴胺的标准模型带来了困难。</p><h1 id="3e1e" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">情景学习和海马体</h1><p id="a22f" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">Ritter 等人(2018)指出，虽然王等人(2018)提出的元强化学习框架提供了增量学习的完整描述，但它没有考虑情景学习过程。他们指出，利用过去的相关经验来制定新的决策是任何智能生物的一个基本特征。因此，代理人还必须能够将过去决策的结果提取到记忆中，并长时间存储，并且能够在以后遇到类似情况时检索相关结果，以评估未来行动的价值。最近的研究表明，在分析人类行为和 fMRI 数据时，考虑与<a class="ae ma" href="https://en.wikipedia.org/wiki/Hippocampus" rel="noopener ugc nofollow" target="_blank">海马</a>相关的情景学习，比只考虑之前讨论的增量学习算法更符合模型[14]。除此之外，meta-RL 代理已被证明遭受灾难性遗忘[15]；这意味着它必须重新学习以前掌握的策略，而不是利用从过去的经验中获得的知识。这些观察导致对利用情景记忆的计算模型的兴趣增加，并证明了情景学习过程在决策中的重要性[16]。受海马体相关功能属性的启发，Ritter 等人(2018)设计了扩展架构，使其包括模式完成、模式分离和皮层活动模式恢复等功能。这意味着，当代理人遇到一个类似于它以前见过的情境时，记忆应该被提取和恢复，而不完全干扰其工作记忆的当前状态。</p><h1 id="5cd0" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">作为计算模型的情景元强化学习</h1><p id="e6bd" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">继 Wang 等人(2018)的工作之后，meta-RL 模型将 PFC 及其连接的皮质下结构(即<a class="ae ma" href="https://en.wikipedia.org/wiki/Basal_ganglia" rel="noopener ugc nofollow" target="_blank">基底神经节</a>和<a class="ae ma" href="https://en.wikipedia.org/wiki/List_of_thalamic_nuclei" rel="noopener ugc nofollow" target="_blank">丘脑核</a>)概念化为形成同质的递归神经网络(RNN)，该网络遵循对这组区域进行建模的新兴传统，抽象出许多神经生理学和解剖学细节[17]。它使用一种<a class="ae ma" href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" rel="noopener ugc nofollow" target="_blank">演员-评论家算法</a>进行训练，这反映了基于多巴胺的学习系统。该网络在每个时间步接收感知数据的编码版本作为输入，以及表示在前一时间步它已经采取的奖励和行动的信号。RNN 学会将观察、奖励和行动的历史提取到它的隐藏状态中，这可以被视为一种工作记忆的形式，同时在一系列结构上相关的任务上进行训练。Ritter 等人(2018 年)扩展了这种架构，将非参数情景记忆作为可微分神经字典来实施[23]。它将工作记忆状态(即 RNN 隐藏状态)与作为密钥嵌入的感知上下文配对存储。当遇到类似的上下文时，通过使用一些学习到的门控机制，这可以稍后用于检索和恢复存储器激活到当前工作存储器中。这个模型与在神经科学研究中观察到的一致；检索情景记忆会触发一种活动模式，类似于最初在支持工作记忆的回路中经历的活动模式[18]。该模型中采用的恢复方法在功能上类似于 PFC 中运行的门控机制[19]。总之，当以最大化特定回报函数为目标，在包含情节和增量结构的任务分布上进行训练时，情节 meta-RL 模型能够提供对无模型和基于模型的策略的统一描述。</p><h1 id="3f60" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">下一步是什么</h1><p id="9458" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">在下面的文章中，我将使用这个模型来重现用于分离无模型系统和基于模型系统的情景(或上下文)两步任务的行为结果。同时，你可以在这里找到我的实现<a class="ae ma" href="https://github.com/BKHMSI/Meta-RL-TwoStep-Task" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="62ac" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">参考</h1><p id="f392" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">[1] Mnih，v .，Kavukcuoglu，k .，Silver，d .，鲁苏，a .，Veness，j .，Bellemare，M. G .，Graves，a .，Riedmiller，m .，Fidjeland，A. K .，Ostrovski，g .，Petersen，s .，Beattie，c .，Sadik，a .，Antonoglou，I .，King，h .，Kumaran，d .，Wierstra，d .，Legg，s .，Hassabis，D. (2015 年)。通过深度强化学习实现人类水平的控制。自然，518(7540):529–533。</p><p id="ed48" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[2] Silver，d .、Huang，a .、C. J .、Guez，a .、Sifre，l .、van den Driessche，g .、Schrittwieser，j .、Antonoglou，I .、Panneershelvam，v .、Lanctot，m .、Dieleman，s .、Grewe，d .、Nham，j .、Kalchbrenner，n .、Sutskever，I .、Lillicrap，t .、Leach，m .、Kavukcuoglu，k .、Graepel，t .和 Hassabis，D. (2016 年用深度神经网络和树搜索掌握围棋。自然，529:484–503。</p><p id="1f59" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[3] Vinyals，o .、Babuschkin，I .、Czarnecki，w .、Mathieu，m .、Dudzik，a .、Chung，j .、Choi，d .、Powell，r .、Ewalds，t .、p .、Oh，j .、Horgan，d .、Kroiss，m .、Danihelka，I .、Huang，a .、Sifre，l .、Cai，t .、Agapiou，j .、Jaderberg，m .和 Silver，D. (2019)。星际争霸 2 中使用多智能体强化学习的特级大师级别。自然，575。</p><p id="efc0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[4]巴甫洛夫，I. P. (1927 年)。条件反射:大脑皮层生理活动的研究。牛津大学出版社。</p><p id="2bf1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[5]雷斯科拉和瓦格纳(1972 年)。巴甫洛夫条件作用理论:加固和非加固有效性的变化，第 2 卷。</p><p id="3d4f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[6]舒尔茨、达扬和蒙塔古(1997 年)。预测和奖励的神经基础。科学，275(5306):1593–1599。</p><p id="641b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[7]萨顿和巴尔托(1998 年)。强化学习:导论。麻省理工学院出版社，美国马萨诸塞州剑桥。</p><p id="77f3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[8]n . Daw 和 Tobler，P. (2014 年)。通过强化学习的价值:多巴胺和强化学习的基础。神经经济学:决策和大脑，283-298 页。</p><p id="2f30" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[9]格里姆彻，P. W. (2011 年)。理解多巴胺和强化学习:多巴胺奖励预测误差假说。美国国家科学院院刊，108 期(增刊 3):15647–15654。</p><p id="3316" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[10]h .普拉斯曼、j .奥多尔蒂和 a .兰格尔(2007 年)。眶额皮层编码日常经济交易中的支付意愿。神经科学杂志，27(37):9984–9988。</p><p id="285e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[11]Padoa Schioppa c .和 Assad j .(2006 年)。眶额皮层神经元编码经济价值。自然 441，223–226。《自然》, 441:223–6。</p><p id="c225" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[12] Seo，m .，Lee，e .和 Averbeck，B. (2012 年)。额叶-纹状体回路中的动作选择和动作价值。神经元，74:947–60。</p><p id="75da" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[13] Tsutsui，K.-I .，Grabenhorst，f .，Kobayashi，s .，和 Schultz，W. (2016 年)。前额叶皮层神经元中经济对象估价的动态代码。自然通讯，7:12554。</p><p id="450c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[14]博恩施泰因、卡瓦、肖哈米和道(2017 年)。对过去选择的回忆会使人类对奖励产生偏见。bioRxiv。</p><p id="1318" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[15] Ritter，s .，Wang，J. X .，Kurth-Nelson，z .，Jayakumar，S. M .，Blundell，c .，Pascanu，r .，Botvinick，M. M. (2018 年)。经历过，做过:元学习与情景回忆。在 ICML。</p><p id="2ca8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[16] Gershman，S. J .和 Daw，N. D. (2017 年)。人类和动物的强化学习和情景记忆:一个整合框架。心理学年度评论，68(1):101–128。PMID: 27618944。</p><p id="434e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[17]宋海峰，杨广瑞，王晓军(2016)。用于认知和价值任务的递归神经网络的基于奖励的训练。bioRxiv。</p><p id="b929" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[18] Xiao，x .，Dong，q .，Gao，j .，Men，w .，Poldrack，R. A .，和薛，G. (2017 年)。情景记忆提取过程中的变形神经模式恢复。神经科学杂志，37(11):2986–2998。</p><p id="de26" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[19]c .查塔姆和 d .巴德雷(2015 年)。工作记忆的多重门。行为科学最新观点，1:23–31。</p><p id="80e3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[20] Wang，J. X .，Kurth-Nelson，z .，Kumaran，d .，Tirumala，d .，Soyer，h .，J. Z .，Hassabis，d .，Botvinick，M. (2018 年)。前额叶皮层作为一个元强化学习系统。bioRxiv。</p><p id="5bc7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[21] Botvinick，m .，Ritter，s .，Wang，j .，Kurth-Nelson，z .，Blundell，c .，和 Hassabis，D. (2019 年)。强化学习，有快有慢。认知科学趋势，23。</p><p id="b421" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[22]里特尔，S. (2019)。情景回忆的元强化学习:奖励驱动学习的整合理论。博士论文。</p><p id="aea2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[23] Pritzel，a .，Uria，b .，Srinivasan，s .，Badia，A. P .，Vinyals，o .，Hassabis，d .，Wierstra，d .，和 Blundell，C. (2017 年)。神经事件控制。更正，abs/1703.01988。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><p id="88a4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="my">原载于 2020 年 1 月 20 日</em><a class="ae ma" href="https://bkhmsi.github.io/blog/episodic-meta-rl-and-the-brain/" rel="noopener ugc nofollow" target="_blank"><em class="my">https://bkhmsi . github . io</em></a><em class="my">。</em></p></div></div>    
</body>
</html>