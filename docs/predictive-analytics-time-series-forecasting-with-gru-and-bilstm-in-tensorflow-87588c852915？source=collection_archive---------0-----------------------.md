# é¢„æµ‹åˆ†æ:ç”¨ TensorFlow ä¸­çš„ GRU å’Œæ¯”å°”æ–¯ç‰¹å§†è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹

> åŸæ–‡ï¼š<https://towardsdatascience.com/predictive-analytics-time-series-forecasting-with-gru-and-bilstm-in-tensorflow-87588c852915?source=collection_archive---------0----------------------->

![](img/6fa704cc47b7ff7f9e8a36437df4fc4d.png)

Enrique Alarcon åœ¨ [Unsplash](https://unsplash.com/) ä¸Šæ‹æ‘„çš„ç…§ç‰‡

## ä¸ºæ—¶åºé¢„æµ‹æ„å»º GRU å’ŒåŒå‘ LSTM çš„åˆ†æ­¥æ•™ç¨‹

R å½“å‰çš„ç¥ç»ç½‘ç»œè¢«è®¾è®¡ç”¨æ¥å¤„ç†æ—¶åºåˆ†æä¸­åºåˆ—ç›¸å…³æ€§çš„å¤æ‚æ€§ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä¸ºä¸€ä¸ªå•å˜é‡æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹å»ºç«‹ GRU å’Œæ¯”å°”æ–¯ç‰¹å§†ã€‚é—¨æ§å¾ªç¯å•å…ƒ(GRU)æ˜¯æ–°ä¸€ä»£çš„ç¥ç»ç½‘ç»œï¼Œéå¸¸ç±»ä¼¼äºé•¿çŸ­æœŸè®°å¿†(LSTM)ã€‚è€ŒåŒå‘ lstm(bil STM)çš„æ€æƒ³æ˜¯åœ¨ LSTM æ¨¡å‹ä¸­èšåˆç‰¹å®šæ—¶é—´æ­¥é•¿çš„è¿‡å»å’Œæœªæ¥çš„è¾“å…¥ä¿¡æ¯ã€‚

ä¸‹é¢è¿™ç¯‡æ–‡ç« å¾ˆå¥½åœ°ä»‹ç»äº† LSTMã€GRU å’Œæ¯”å°”æ–¯ç‰¹å§†ã€‚

[](/predictive-analysis-rnn-lstm-and-gru-to-predict-water-consumption-e6bb3c2b4b02) [## é¢„æµ‹åˆ†æ:TensorFlow ä¸­çš„ LSTMã€GRU å’ŒåŒå‘ LSTM

### å…³äºå¼€å‘ LSTMã€GRU å’ŒåŒå‘ LSTM æ¨¡å‹æ¥é¢„æµ‹ç”¨æ°´é‡çš„åˆ†æ­¥æŒ‡å—

towardsdatascience.com](/predictive-analysis-rnn-lstm-and-gru-to-predict-water-consumption-e6bb3c2b4b02) 

# ä»€ä¹ˆæ˜¯æ—¶é—´åºåˆ—åˆ†æï¼Ÿ

ä¸å›å½’åˆ†æä¸åŒï¼Œåœ¨æ—¶é—´åºåˆ—åˆ†æä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰å¼ºæœ‰åŠ›çš„è¯æ®è¡¨æ˜ä»€ä¹ˆå½±å“äº†æˆ‘ä»¬çš„ç›®æ ‡ã€‚æ—¶é—´åºåˆ—åˆ†æä½¿ç”¨æ—¶é—´ä½œä¸ºå˜é‡ä¹‹ä¸€ï¼Œä»¥è§‚å¯Ÿæ˜¯å¦éšæ—¶é—´å‘ç”Ÿå˜åŒ–ã€‚

# ä»€ä¹ˆæ˜¯æ—¶é—´åºåˆ—é¢„æµ‹ï¼Ÿ

æ—¶é—´åºåˆ—é¢„æµ‹çš„ç›®çš„æ˜¯æ ¹æ®å†å²æ•°æ®æ‹Ÿåˆä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥é¢„æµ‹æœªæ¥çš„è§‚æµ‹å€¼ã€‚è¿™ç¯‡æ–‡ç« è‡´åŠ›äºä½¿ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚å¦‚æœä½ æ„¿æ„å­¦ä¹ æ—¶é—´åºåˆ—é¢„æµ‹çš„ç»å…¸æ–¹æ³•ï¼Œæˆ‘å»ºè®®ä½ é˜…è¯»è¿™ä¸ª[ç½‘é¡µ](https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/)ã€‚

# â˜ºè®©æˆ‘ä»¬å» codeâ—

**ğŸ‘©â€ğŸ’»** [**ä¸Šçš„ Python ä»£ç  GitHub**](https://github.com/NioushaR/LSTM-TensorFlow-for-Timeseries-forecasting)

## èµ„æ–™ç»„

å¯¹äºæœ¬é¡¹ç›®ï¼Œæ•°æ®ä¸ºåŠ æ‹¿å¤§é­åŒ—å…‹çœ Brossard å¸‚ 2011 å¹´ 9 æœˆ 1 æ—¥è‡³ 2015 å¹´ 9 æœˆ 30 æ—¥çš„æ—¥ç”¨æ°´é‡ã€‚

## å¯¼å…¥åº“

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as pltfrom sklearn.preprocessing import MinMaxScaler, StandardScalerimport warnings
warnings.filterwarnings(â€˜ignoreâ€™)from scipy import stats
%matplotlib inlineimport tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, layers, callbacks
from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional
```

## è®¾ç½®éšæœºç§å­

```
tf.random.set_seed(1234)
```

# 1.è¯»å–å’Œæµè§ˆæ•°æ®

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘å¤„ç†çš„æ˜¯å•å˜é‡æ—¶é—´åºåˆ—æ•°æ®ã€‚å½“æˆ‘ä» CSV æ–‡ä»¶å¯¼å…¥æ•°æ®æ—¶ï¼Œæˆ‘é€šè¿‡ **parse_dates = ['Date']** ç¡®ä¿ **Date** åˆ—å…·æœ‰æ­£ç¡®çš„ *DateTime* æ ¼å¼ã€‚æ­¤å¤–ï¼Œå½“æˆ‘å¤„ç†æ—¥æœŸå’Œæ—¶é—´æ—¶ï¼Œå¦‚æœæˆ‘å°†**æ—¥æœŸ**åˆ—è®¾ç½®ä¸º dataframe ç´¢å¼•ï¼Œå°±ä¼šå˜å¾—å®¹æ˜“å¾—å¤šã€‚

```
df = pd.read_csv(â€˜Data.csvâ€™, parse_dates = [â€˜Dateâ€™])
```

![](img/26cd6f4b2796b25bafb6aadb61666a85.png)

## 1.1 æ—¶é—´åºåˆ—å›¾

ä¸ºäº†æ›´å¥½åœ°ç†è§£æ•°æ®ï¼Œæˆ‘ç»˜åˆ¶äº†æ¯æ—¥ã€æ¯æœˆå’Œæ¯å¹´çš„ç”¨æ°´é‡ã€‚

```
**# Define a function to draw time_series plot**def timeseries (x_axis, y_axis, x_label):
    plt.figure(figsize = (10, 6))
    plt.plot(x_axis, y_axis, color =â€™blackâ€™)
    plt.xlabel(x_label, {â€˜fontsizeâ€™: 12}) 
    plt.ylabel(â€˜Water consumption ($mÂ³$/capita.day)â€™, 
                                  {â€˜fontsizeâ€™: 12})
dataset = df.copy()
timeseries(df.index, dataset[â€˜WCâ€™], â€˜Time (day)â€™)dataset[â€˜monthâ€™] = dataset.index.month
dataset_by_month = dataset.resample(â€˜Mâ€™).sum()
timeseries(dataset_by_month.index, dataset_by_month[â€˜WCâ€™], 
           â€˜Time(month)â€™)dataset[â€˜yearâ€™] = dataset.index.year
dataset_by_year = dataset.resample(â€˜Yâ€™).sum()
timeseries(dataset_by_year.index, dataset_by_year[â€˜WCâ€™], 
           â€˜Time (month)â€™)
```

![](img/357c1fdbec21c3cfa02c14b8efc05de0.png)

æ—¥ç”¨æ°´é‡æ—¶é—´åºåˆ—

ä½ å¯ä»¥çœ‹åˆ°æ•°æ®æœ‰ä¸€ä¸ªå­£èŠ‚æ€§çš„æ¨¡å¼ã€‚

![](img/e89c29938e7adac5f972f8f83f002e94.png)

æœˆç”¨æ°´é‡æ—¶é—´åºåˆ—

![](img/06b69fc9bf20e019e905187fd19b29fd.png)

å¹´ç”¨æ°´é‡æ—¶é—´åºåˆ—

## 1.2 å¤„ç†ç¼ºå¤±å€¼

é¦–å…ˆï¼Œæˆ‘æƒ³æ£€æŸ¥ç¼ºå¤±å€¼çš„æ•°é‡ï¼Œå¹¶ç¡®å®šæ²¡æœ‰æ•°æ®å€¼å­˜å‚¨çš„æ—¥æœŸã€‚ç„¶åæˆ‘ä½¿ç”¨çº¿æ€§æ’å€¼æ¥æ›¿æ¢ä¸¢å¤±çš„å€¼ã€‚

```
**# Check for missing values**
print(â€˜Total num of missing values:â€™) 
print(df.WC.isna().sum())
print(â€˜â€™)
**# Locate the missing value**
df_missing_date = df.loc[df.WC.isna() == True]
print(â€˜The date of missing value:â€™)
print(df_missing_date.loc[:,[â€˜Dateâ€™]])**# Replcase missing value with interpolation**
df.WC.interpolate(inplace = True)**# Keep WC and drop Date**
df = df.drop('Date', axis = 1)
```

![](img/cd39ab89934381c9857edf20fc0892f0.png)

## 1.3 å°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®

åƒæˆ‘é€šå¸¸åšçš„é‚£æ ·ï¼Œæˆ‘å°†å‰ 80%çš„æ•°æ®è®¾ç½®ä¸ºè®­ç»ƒæ•°æ®ï¼Œå‰©ä¸‹çš„ 20%ä¸ºæµ‹è¯•æ•°æ®ã€‚æˆ‘ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ç”¨æµ‹è¯•æ•°æ®éªŒè¯å…¶æ€§èƒ½ã€‚

ğŸ’¡æé†’ä¸€ä¸‹ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨ *iloc* æ ¹æ®ç´¢å¼•ä½ç½®æ‰¾åˆ°æ•°æ®å¸§çš„å­é›†

```
**# Split train data and test data**
train_size = int(len(df)*0.8)

train_data = df.iloc[:train_size]
test_data = df.iloc[train_size:]
```

## 1.4 æ•°æ®è½¬æ¢

ä¸€ä¸ªå¾ˆå¥½çš„ç»éªŒæ³•åˆ™æ˜¯ï¼Œè§„èŒƒåŒ–çš„æ•°æ®ä¼šåœ¨ç¥ç»ç½‘ç»œä¸­äº§ç”Ÿæ›´å¥½çš„æ€§èƒ½ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä½¿ç”¨æ¥è‡ª [scikit-learn](https://scikit-learn.org/stable/) çš„ [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) ã€‚

æ‚¨éœ€è¦éµå¾ªä¸‰ä¸ªæ­¥éª¤æ¥æ‰§è¡Œæ•°æ®è½¬æ¢:

*   ä½¿ç”¨å¯ç”¨çš„è®­ç»ƒæ•°æ®æ‹Ÿåˆå®šæ ‡å™¨(MinMaxScaler)(è¿™æ„å‘³ç€ä½¿ç”¨è®­ç»ƒæ•°æ®ä¼°è®¡æœ€å°å’Œæœ€å¤§å¯è§‚æµ‹å€¼ã€‚)
*   å°†ç¼©æ”¾å™¨åº”ç”¨äºè®­ç»ƒæ•°æ®
*   å°†å®šæ ‡å™¨åº”ç”¨äºæµ‹è¯•æ•°æ®

ğŸ’¡éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ *MinMaxScaler()çš„è¾“å…¥ã€‚fit()* å¯ä»¥æ˜¯æ•°ç»„çŠ¶æˆ–æ•°æ®å¸§çŠ¶(n_samplesï¼Œn_features)ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­:

train_data.shap = (1192ï¼Œ1)

```
scaler = MinMaxScaler().fit(train_data)
train_scaled = scaler.transform(train_data)
test_scaled = scaler.transform(test_data)
```

## 1.5 åˆ›å»ºè¾“å…¥

GRU å’Œæ¯”å°”æ–¯ç‰¹å§†é‡‡ç”¨ä¸€ä¸ªä¸‰ç»´è¾“å…¥(æ•°é‡ _ æ ·æœ¬ï¼Œæ•°é‡ _ æ—¶é—´æ­¥é•¿ï¼Œæ•°é‡ _ ç‰¹å¾)ã€‚å› æ­¤ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªåŠ©æ‰‹å‡½æ•° *create_dataset* ï¼Œæ¥é‡å¡‘è¾“å…¥ã€‚

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘å®šä¹‰ look_back = 30ã€‚è¿™æ„å‘³ç€æ¨¡å‹åŸºäºæœ€è¿‘ 30 å¤©çš„æ•°æ®è¿›è¡Œé¢„æµ‹(åœ¨ for-loop çš„ç¬¬ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œè¾“å…¥æºå¸¦å‰ 30 å¤©ï¼Œè¾“å‡ºæ˜¯ç¬¬ 30 å¤©çš„ç”¨æ°´é‡)ã€‚

```
**# Create input dataset**
def create_dataset (X, look_back = 1):
    Xs, ys = [], []

    for i in range(len(X)-look_back):
        v = X[i:i+look_back]
        Xs.append(v)
        ys.append(X[i+look_back])

    return np.array(Xs), np.array(ys)LOOK_BACK = 30X_train, y_train = create_dataset(train_scaled,LOOK_BACK)
X_test, y_test = create_dataset(test_scaled,LOOK_BACK)**# Print data shape**
print(â€˜X_train.shape: â€˜, X_train.shape)
print(â€˜y_train.shape: â€˜, y_train.shape)
print(â€˜X_test.shape: â€˜, X_test.shape) 
print(â€˜y_test.shape: â€˜, y_test.shape)
```

![](img/838573368707a3e4061813b6b0f019b2.png)

# 2.åˆ›å»ºæ¨¡å‹

ç¬¬ä¸€ä¸ªå‡½æ•°ï¼Œ *create_bilstm* ï¼Œåˆ›å»ºä¸€ä¸ª BiDLSM å¹¶è·å–éšè—å±‚ä¸­çš„å•å…ƒ(ç¥ç»å…ƒ)æ•°é‡ã€‚ç¬¬äºŒä¸ªå‡½æ•°ï¼Œ *create_gru* ï¼Œæ„å»ºä¸€ä¸ª gru å¹¶è·å¾—éšè—å±‚ä¸­çš„å•å…ƒæ•°ã€‚

ä¸¤è€…åœ¨è¾“å…¥å±‚éƒ½æœ‰ 64 ä¸ªç¥ç»å…ƒï¼Œä¸€ä¸ªéšå±‚åŒ…æ‹¬ 64 ä¸ªç¥ç»å…ƒï¼Œåœ¨è¾“å‡ºå±‚æœ‰ 1 ä¸ªç¥ç»å…ƒã€‚ä¸¤ä¸ªæ¨¡å‹ä¸­çš„*ä¼˜åŒ–å™¨*éƒ½æ˜¯ [*äºšå½“*](https://keras.io/api/optimizers/adam/) ã€‚ä¸ºäº†ä½¿ GRU æ¨¡å‹å¯¹å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œä½¿ç”¨äº†**ä¸‹é™**å‡½æ•°ã€‚**æ‰çº¿(0.2)** éšæœºæ‰çº¿ 20%çš„å•ä½ã€‚

```
**# Create BiLSTM model**
def create_bilstm(units):
    model = Sequential()
    **# Input layer**
    model.add(Bidirectional(
              LSTM(units = units, return_sequences=True), 
              input_shape=(X_train.shape[1], X_train.shape[2])))
    **# Hidden layer**
    model.add(Bidirectional(LSTM(units = units)))
    model.add(Dense(1))
    **#Compile model**
    model.compile(optimizer=â€™adamâ€™,loss=â€™mseâ€™)
    return modelmodel_bilstm = create_bilstm(64)**# Create GRU model**
def create_gru(units):
    model = Sequential()
    **# Input layer**
    model.add(GRU (units = units, return_sequences = True, 
    input_shape = [X_train.shape[1], X_train.shape[2]]))
    model.add(Dropout(0.2)) 
    **# Hidden layer**
    model.add(GRU(units = units)) 
    model.add(Dropout(0.2))
    model.add(Dense(units = 1)) 
    **#Compile model**
    model.compile(optimizer=â€™adamâ€™,loss=â€™mseâ€™)
    return modelmodel_gru = create_gru(64)
```

## 2.1 æ‹Ÿåˆæ¨¡å‹

æˆ‘åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œ *fit_model* ï¼Œè·å–æ¨¡å‹å¹¶ç”¨ 100 ä¸ª**æ—¶æœŸ**å’Œ **batch_size** = 16 çš„è®­ç»ƒæ•°æ®è®­ç»ƒæ¨¡å‹ã€‚æˆ‘è®©æ¨¡å‹ä½¿ç”¨ 20%çš„è®­ç»ƒæ•°æ®ä½œä¸ºéªŒè¯æ•°æ®ã€‚æˆ‘è®¾ç½® **shuffle = False** æ˜¯å› ä¸ºå®ƒæä¾›äº†æ›´å¥½çš„æ€§èƒ½ã€‚

ä¸ºäº†é¿å…è¿‡åº¦æ‹Ÿåˆï¼Œæˆ‘è®¾ç½®äº†ä¸€ä¸ª*æå‰åœæ­¢*ï¼Œå½“*éªŒè¯æŸå¤±*åœ¨ 10 ä¸ªå‘¨æœŸåæ²¡æœ‰æ”¹å–„æ—¶(è€å¿ƒ= 10)åœæ­¢è®­ç»ƒã€‚

```
def fit_model(model):
    early_stop = keras.callbacks.EarlyStopping(monitor = â€˜val_lossâ€™,
                                               patience = 10)
    history = model.fit(X_train, y_train, epochs = 100,  
                        validation_split = 0.2,
                        batch_size = 16, shuffle = False, 
                        callbacks = [early_stop])
    return historyhistory_gru = fit_model(model_gru)
history_bilstm = fit_model(model_bilstm)
```

## 2.2 ç›®æ ‡å˜é‡çš„é€†å˜æ¢

å»ºç«‹æ¨¡å‹åï¼Œæˆ‘å¿…é¡»ä½¿ç”¨**scaler . inverse _ transform**å°†ç›®æ ‡å˜é‡è½¬æ¢å›è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„åŸå§‹æ•°æ®ç©ºé—´ã€‚

```
y_test = scaler.inverse_transform(y_test)
y_train = scaler.inverse_transform(y_train)
```

# 3.è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½

æˆ‘ä»¬å°†å¦‚ä½•è¯„ä»· GRU å’Œæ¯”å°”æ–¯ç‰¹å§†çš„è¡¨ç°ï¼Ÿ

## 1-ç»˜åˆ¶è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±

ä¸ºäº†è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ç»˜åˆ¶äº†è®­ç»ƒæŸå¤±ä¸éªŒè¯æŸå¤±çš„å…³ç³»å›¾ï¼Œæˆ‘é¢„è®¡éªŒè¯æŸå¤±ä½äºè®­ç»ƒæŸå¤±ğŸ˜‰

## 2-æ¯”è¾ƒé¢„æµ‹å’Œæµ‹è¯•æ•°æ®

é¦–å…ˆï¼Œæˆ‘ç”¨æ¯”å°”æ–¯ç‰¹å§†å’Œ GRU æ¨¡å‹é¢„æµ‹ WCã€‚ç„¶åï¼Œæˆ‘ç»˜åˆ¶äº†ä¸¤ä¸ªæ¨¡å‹çš„æµ‹è¯•æ•°æ®ä¸é¢„æµ‹ã€‚

## 3-è®¡ç®— RMSE å’Œæ¢…

æˆ‘ä½¿ç”¨ä¸¤ç§æ‹Ÿåˆä¼˜åº¦æ¥è¯„ä¼°æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

## 3.1 ç»˜åˆ¶åˆ—è½¦æŸå¤±å’ŒéªŒè¯æŸå¤±

```
def plot_loss (history, model_name):
    plt.figure(figsize = (10, 6))
    plt.plot(history.history[â€˜lossâ€™])
    plt.plot(history.history[â€˜val_lossâ€™])
    plt.title(â€˜Model Train vs Validation Loss for â€˜ + model_name)
    plt.ylabel(â€˜Lossâ€™)
    plt.xlabel(â€˜epochâ€™)
    plt.legend([â€˜Train lossâ€™, â€˜Validation lossâ€™], loc=â€™upper rightâ€™)

plot_loss (history_gru, â€˜GRUâ€™)
plot_loss (history_bilstm, â€˜Bidirectional LSTMâ€™)
```

![](img/b8b52804516a5f52b94fad9c647ba2bf.png)![](img/10849eb2dbfbb04668d63c73354c3fe9.png)

## 3.2 æ¯”è¾ƒé¢„æµ‹å’Œæµ‹è¯•æ•°æ®

```
**# Make prediction**
def prediction(model):
    prediction = model.predict(X_test)
    prediction = scaler.inverse_transform(prediction)
    return predictionprediction_gru = prediction(model_gru)
prediction_bilstm = prediction(model_bilstm)**# Plot test data vs prediction**
def plot_future(prediction, model_name, y_test):
    plt.figure(figsize=(10, 6))
    range_future = len(prediction)
    plt.plot(np.arange(range_future), np.array(y_test), 
             label=â€™Test   dataâ€™)
    plt.plot(np.arange(range_future), 
             np.array(prediction),label=â€™Predictionâ€™) plt.title(â€˜Test data vs prediction for â€˜ + model_name)
    plt.legend(loc=â€™upper leftâ€™)
    plt.xlabel(â€˜Time (day)â€™)
    plt.ylabel(â€˜Daily water consumption ($mÂ³$/capita.day)â€™)

plot_future(prediction_gru, â€˜GRUâ€™, y_test)
plot_future(prediction_bilstm, â€˜Bidirectional LSTMâ€™, y_test)
```

![](img/54d02bc93325f4f0f1bc0f67a2be0fe5.png)![](img/5c163fd5648f339abc651b973fc6451b.png)

## 3.3 è®¡ç®— RMSE å’Œæ¢…

```
def evaluate_prediction(predictions, actual, model_name):
    errors = predictions â€” actual
    mse = np.square(errors).mean()
    rmse = np.sqrt(mse)
    mae = np.abs(errors).mean()
    print(model_name + â€˜:â€™)
    print(â€˜Mean Absolute Error: {:.4f}â€™.format(mae))
    print(â€˜Root Mean Square Error: {:.4f}â€™.format(rmse))
    print(â€˜â€™)evaluate_prediction(prediction_gru, y_test, â€˜GRUâ€™)
evaluate_prediction(prediction_bilstm, y_test, â€˜Bidirectiona LSTMâ€™)
```

![](img/17208a94807d24d56d0776af7518a24d.png)

# 4.30 å¤©ç”¨æ°´é‡çš„å¤šæ­¥é¢„æµ‹

ä¸ºäº†ä½¿ç”¨ç»è¿‡è®­ç»ƒçš„ GRU å’Œæ¯”å°”æ–¯ç‰¹å§†æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œæˆ‘éœ€è¦è‡³å°‘ 60 å¤©çš„è§‚å¯Ÿæ•°æ®æ¥é¢„æµ‹æœªæ¥ 30 å¤©çš„æƒ…å†µã€‚ä¸ºäº†ä¾¿äºè¯´æ˜ï¼Œæˆ‘ä»è§‚æµ‹æ•°æ®ä¸­é€‰æ‹©äº† 60 å¤©çš„ç”¨æ°´é‡ï¼Œå¹¶ç”¨ GRU å’Œæ¯”å°”æ–¯ç‰¹å§†é¢„æµ‹äº†æœªæ¥ 30 å¤©çš„ç”¨æ°´é‡ã€‚

```
**# Make prediction for new data**
def prediction(model):
    prediction = model.predict(X_30)
    prediction = scaler.inverse_transform(prediction)
    return predictionprediction_gru = prediction(model_gru)
prediction_bilstm = prediction(model_bilstm)**# Plot history and future**
def plot_multi_step(history, prediction1, prediction2):

    plt.figure(figsize=(15, 6))

    range_history = len(history)
    range_future = list(range(range_history, range_history +
                        len(prediction1))) plt.plot(np.arange(range_history), np.array(history), 
             label='History')
    plt.plot(range_future, np.array(prediction1),
             label='Forecasted for GRU')
    plt.plot(range_future, np.array(prediction2),
             label='Forecasted for BiLSTM')

    plt.legend(loc='upper right')
    plt.xlabel('Time step (day)')
    plt.ylabel('Water demand (lit/day)')

plot_multi_step(new_data, prediction_gru, prediction_bilstm)
```

![](img/672e95b761b11dd27c44163df3a4b7b5.png)

# ç»“è®º

æ„Ÿè°¢æ‚¨é˜…è¯»è¿™ç¯‡æ–‡ç« ã€‚æˆ‘å¸Œæœ›å®ƒèƒ½å¸®åŠ©ä½ åœ¨ Tensorflow ä¸­å¼€å‘ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„ GRU å’Œæ¯”å°”æ–¯ç‰¹å§†æ¨¡å‹ğŸ˜Š

éå¸¸æ„Ÿè°¢æ‚¨çš„åé¦ˆã€‚ä½ å¯ä»¥åœ¨ LinkedIn ä¸Šæ‰¾åˆ°æˆ‘ã€‚