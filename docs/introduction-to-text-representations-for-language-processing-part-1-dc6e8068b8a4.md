# è¯­è¨€å¤„ç†çš„æ–‡æœ¬è¡¨ç¤ºä»‹ç»â€”ç¬¬ 1 éƒ¨åˆ†

> åŸæ–‡ï¼š<https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-1-dc6e8068b8a4?source=collection_archive---------13----------------------->

![](img/d4572fbe3ad47896132e8e9b3716c3ae.png)

åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šç”± [Jaredd Craig](https://unsplash.com/@jaredd_craig?utm_source=medium&utm_medium=referral) æ‹ç…§

## è®¡ç®—æœºæ˜¯å¦‚ä½•ç†è§£å’Œè§£é‡Šè¯­è¨€çš„ï¼Ÿ

è®¡ç®—æœºåœ¨å¤„ç†æ•°å­—æ—¶å¾ˆèªæ˜ã€‚å®ƒä»¬åœ¨è®¡ç®—å’Œè§£ç æ¨¡å¼æ–¹é¢æ¯”äººç±»å¿«å¾ˆå¤šä¸ªæ•°é‡çº§ã€‚ä½†æ˜¯å¦‚æœæ•°æ®ä¸æ˜¯æ•°å­—å‘¢ï¼Ÿå¦‚æœæ˜¯è¯­è¨€å‘¢ï¼Ÿå½“æ•°æ®æ˜¯å­—ç¬¦ã€å•è¯å’Œå¥å­æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿæˆ‘ä»¬å¦‚ä½•è®©è®¡ç®—æœºå¤„ç†æˆ‘ä»¬çš„è¯­è¨€ï¼ŸAlexaã€Google Home &å¾ˆå¤šå…¶ä»–æ™ºèƒ½åŠ©æ‰‹æ˜¯å¦‚ä½•ç†è§£&å›å¤æˆ‘ä»¬çš„å‘è¨€çš„ï¼Ÿå¦‚æœä½ æ­£åœ¨å¯»æ‰¾è¿™äº›é—®é¢˜çš„ç­”æ¡ˆï¼Œè¿™ç¯‡æ–‡ç« å°†æ˜¯ä½ èµ°å‘æ­£ç¡®æ–¹å‘çš„å«è„šçŸ³ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œè‡´åŠ›äºä½¿æœºå™¨ç†è§£å’Œå¤„ç†äººç±»è¯­è¨€ã€‚å¤§å¤šæ•°è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä»»åŠ¡çš„æœ€åŸºæœ¬æ­¥éª¤æ˜¯å°†å•è¯è½¬æ¢æˆæ•°å­—ï¼Œä»¥ä¾¿æœºå™¨ç†è§£å’Œè§£ç è¯­è¨€ä¸­çš„æ¨¡å¼ã€‚æˆ‘ä»¬ç§°è¿™ä¸€æ­¥ä¸ºæ–‡æœ¬è¡¨ç¤ºã€‚è¿™ä¸€æ­¥è™½ç„¶æ˜¯è¿­ä»£çš„ï¼Œä½†åœ¨å†³å®šæœºå™¨å­¦ä¹ æ¨¡å‹/ç®—æ³•çš„ç‰¹å¾æ–¹é¢èµ·ç€é‡è¦ä½œç”¨ã€‚

æ–‡æœ¬è¡¨ç¤ºå¯ä»¥å¤§è‡´åˆ†ä¸ºä¸¤éƒ¨åˆ†:

*   ç¦»æ•£æ–‡æœ¬è¡¨ç¤º
*   åˆ†å¸ƒå¼/è¿ç»­æ–‡æœ¬è¡¨ç¤º

æœ¬æ–‡å°†å…³æ³¨ç¦»æ•£æ–‡æœ¬è¡¨ç¤º&æˆ‘ä»¬å°†æ·±å…¥ç ”ç©¶ä¸€äº›åŸºæœ¬ Sklearn å®ç°ä¸­å¸¸ç”¨çš„è¡¨ç¤ºã€‚

# ç¦»æ•£æ–‡æœ¬è¡¨ç¤º:

è¿™äº›è¡¨ç¤ºä¸­ï¼Œå•è¯ç”±å®ƒä»¬åœ¨å­—å…¸ä¸­çš„ä½ç½®çš„å¯¹åº”ç´¢å¼•æ¥è¡¨ç¤ºï¼Œè¯¥ç´¢å¼•æ¥è‡ªæ›´å¤§çš„è¯­æ–™åº“ã€‚

å±äºè¿™ä¸€ç±»åˆ«çš„è‘—åä»£è¡¨æœ‰:

*   ä¸€é”®ç¼–ç 
*   è¯è¢‹è¡¨ç¤ºæ³•(BOW)
*   åŸºæœ¬ BOW è®¡æ•°çŸ¢é‡å™¨
*   å…ˆè¿›å¼“-TF-IDF

# ä¸€é”®ç¼–ç :

è¿™æ˜¯ä¸€ç§å°† 0 èµ‹ç»™å‘é‡ä¸­æ‰€æœ‰å…ƒç´ çš„è¡¨ç¤ºå½¢å¼ï¼Œåªæœ‰ä¸€ä¸ªå…ƒç´ çš„å€¼ä¸º 1ã€‚è¯¥å€¼è¡¨ç¤ºå…ƒç´ çš„ç±»åˆ«ã€‚

ä¾‹å¦‚:

å¦‚æœæˆ‘æœ‰ä¸€ä¸ªå¥å­ï¼Œâ€œæˆ‘çˆ±æˆ‘çš„ç‹—â€ï¼Œå¥å­ä¸­çš„æ¯ä¸ªå•è¯å°†è¡¨ç¤ºå¦‚ä¸‹:

```
I â†’ [1 0 0 0], love â†’ [0 1 0 0], my â†’ [0 0 1 0], dog â†’ [0 0 0 1]
```

ç„¶åï¼Œæ•´ä¸ªå¥å­è¡¨ç¤ºä¸º:

```
sentence = [ [1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1] ]
```

> *ä¸€é”®ç¼–ç èƒŒåçš„ç›´è§‰æ˜¯æ¯ä¸ªæ¯”ç‰¹ä»£è¡¨ä¸€ä¸ªå¯èƒ½çš„ç±»åˆ«&å¦‚æœä¸€ä¸ªç‰¹å®šçš„å˜é‡ä¸èƒ½å½’å…¥å¤šä¸ªç±»åˆ«ï¼Œé‚£ä¹ˆä¸€ä¸ªæ¯”ç‰¹å°±è¶³ä»¥ä»£è¡¨å®ƒ*

æ­£å¦‚æ‚¨å¯èƒ½å·²ç»ç†è§£çš„ï¼Œå•è¯æ•°ç»„çš„é•¿åº¦å–å†³äºè¯æ±‡é‡ã€‚è¿™å¯¹äºå¯èƒ½åŒ…å«å¤šè¾¾ 100ï¼Œ000 ä¸ªå”¯ä¸€å•è¯ç”šè‡³æ›´å¤šçš„éå¸¸å¤§çš„è¯­æ–™åº“æ¥è¯´æ˜¯ä¸å¯æ‰©å±•çš„ã€‚

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ Sklearn æ¥å®ç°å®ƒ:

```
from sklearn.preprocessing import OneHotEncoder
import itertools# two example documents
docs = ["cat","dog","bat","ate"]# split documents to tokens
tokens_docs = [doc.split(" ") for doc in docs]# convert list of of token-lists to one flat list of tokens
# and then create a dictionary that maps word to id of word,
all_tokens = itertools.chain.from_iterable(tokens_docs)
word_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}# convert token lists to token-id lists
token_ids = [[word_to_id[token] for token in tokens_doc] for tokens_doc in tokens_docs]# convert list of token-id lists to one-hot representation
vec = OneHotEncoder(categories="auto")
X = vec.fit_transform(token_ids)print(X.toarray())
```

# è¾“å‡º:

```
[[0\. 0\. 1\. 0.]
 [0\. 1\. 0\. 0.]
 [0\. 0\. 0\. 1.]
 [1\. 0\. 0\. 0.]]
```

# Sklearn æ–‡æ¡£:

[](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) [## sk learn . preprocessing . onehotencoder-sci kit-learn 0 . 23 . 1 æ–‡æ¡£

### å°†åˆ†ç±»ç‰¹å¾ç¼–ç ä¸ºä¸€ä¸ªç‹¬çƒ­æ•°å€¼æ•°ç»„ã€‚è¿™ä¸ªè½¬æ¢å™¨çš„è¾“å…¥åº”è¯¥æ˜¯ä¸€ä¸ªç±»ä¼¼æ•°ç»„çš„â€¦

scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) 

# ä¸€é”®ç¼–ç çš„ä¼˜åŠ¿:

*   æ˜“äºç†è§£å’Œå®æ–½

# ä¸€é”®ç¼–ç çš„ç¼ºç‚¹:

*   å¦‚æœç±»åˆ«æ•°é‡éå¸¸å¤šï¼Œç‰¹å¾ç©ºé—´ä¼šçˆ†ç‚¸
*   å•è¯çš„çŸ¢é‡è¡¨ç¤ºæ˜¯æ­£äº¤çš„ï¼Œå¹¶ä¸”ä¸èƒ½ç¡®å®šæˆ–æµ‹é‡ä¸åŒå•è¯ä¹‹é—´çš„å…³ç³»
*   æ— æ³•è¡¡é‡ä¸€ä¸ªå•è¯åœ¨å¥å­ä¸­çš„é‡è¦æ€§ï¼Œä½†å¯ä»¥ç†è§£ä¸€ä¸ªå•è¯åœ¨å¥å­ä¸­æ˜¯å¦å­˜åœ¨
*   é«˜ç»´ç¨€ç–çŸ©é˜µè¡¨ç¤ºå¯èƒ½æ˜¯å­˜å‚¨å™¨å’Œè®¡ç®—æ˜‚è´µçš„

# è¯è¢‹è¡¨ç¤ºæ³•

é¡¾åæ€ä¹‰ï¼Œå•è¯åŒ…è¡¨ç¤ºæ³•å°†å•è¯æ”¾åœ¨ä¸€ä¸ªâ€œåŒ…â€ä¸­ï¼Œå¹¶è®¡ç®—æ¯ä¸ªå•è¯çš„å‡ºç°é¢‘ç‡ã€‚å®ƒä¸è€ƒè™‘æ–‡æœ¬è¡¨ç¤ºçš„è¯åºæˆ–è¯æ±‡ä¿¡æ¯

> *BOW è¡¨ç¤ºèƒŒåçš„ç›´è§‰æ˜¯ï¼Œå…·æœ‰ç›¸ä¼¼å•è¯çš„æ–‡æ¡£æ˜¯ç›¸ä¼¼çš„ï¼Œè€Œä¸ç®¡å•è¯çš„ä½ç½®å¦‚ä½•*

# åŸºæœ¬ BOW è®¡æ•°çŸ¢é‡å™¨

CountVectorizer è®¡ç®—ä¸€ä¸ªå•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡ã€‚å®ƒå°†å¤šä¸ªå¥å­çš„è¯­æ–™åº“(æ¯”å¦‚äº§å“è¯„è®º)è½¬æ¢æˆè¯„è®ºå’Œå•è¯çš„çŸ©é˜µï¼Œå¹¶ç”¨æ¯ä¸ªå•è¯åœ¨å¥å­ä¸­çš„å‡ºç°é¢‘ç‡å¡«å……å®ƒ

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ Sklearn è®¡æ•°çŸ¢é‡å™¨:

```
from sklearn.feature_extraction.text import CountVectorizertext = ["i love nlp. nlp is so cool"]vectorizer = CountVectorizer()# tokenize and build vocab
vectorizer.fit(text)
print(vectorizer.vocabulary_)
# Output: {'love': 2, 'nlp': 3, 'is': 1, 'so': 4, 'cool': 0}# encode document
vector = vectorizer.transform(text)# summarize encoded vector
print(vector.shape) # Output: (1, 5)
print(vector.toarray())
```

# è¾“å‡º:

```
[[1 1 1 2 1]]
```

å¦‚ä½ æ‰€è§ï¼Œå•è¯â€œnlpâ€åœ¨å¥å­ä¸­å‡ºç°äº†ä¸¤æ¬¡&ä¹Ÿåœ¨ç´¢å¼• 3 ä¸­ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸ºæœ€ç»ˆæ‰“å°è¯­å¥çš„è¾“å‡º

> *ä¸€ä¸ªè¯åœ¨å¥å­ä¸­çš„â€œæƒé‡â€æ˜¯å®ƒçš„å‡ºç°é¢‘ç‡*

ä½œä¸º CountVectorizer çš„ä¸€éƒ¨åˆ†ï¼Œå¯ä»¥è°ƒæ•´å„ç§å‚æ•°æ¥è·å¾—æ‰€éœ€çš„ç»“æœï¼ŒåŒ…æ‹¬å°å†™ã€strp_accentsã€é¢„å¤„ç†å™¨ç­‰æ–‡æœ¬é¢„å¤„ç†å‚æ•°

å¯ä»¥åœ¨ä¸‹é¢çš„ Sklearn æ–‡æ¡£ä¸­æ‰¾åˆ°å®Œæ•´çš„å‚æ•°åˆ—è¡¨:

[](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) [## sk learn . feature _ extraction . text . count vectorizer-sci kit-learn 0 . 23 . 1 æ–‡æ¡£

### class sk learn . feature _ extraction . text . count vectorizer(*ï¼Œinput='content 'ï¼Œencoding='utf-8 'ï¼Œdecode_error='strict'â€¦

scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) 

# CountVectorizer çš„ä¼˜åŠ¿:

*   CountVectorizer è¿˜èƒ½ç»™å‡ºæ–‡æœ¬æ–‡æ¡£/å¥å­ä¸­å•è¯çš„é¢‘ç‡ï¼Œè¿™æ˜¯ä¸€é”®ç¼–ç æ‰€ä¸èƒ½æä¾›çš„
*   ç¼–ç å‘é‡çš„é•¿åº¦å°±æ˜¯å­—å…¸çš„é•¿åº¦

# CountVectorizer çš„ç¼ºç‚¹:

*   æ­¤æ–¹æ³•å¿½ç•¥å•è¯çš„ä½ç½®ä¿¡æ¯ã€‚ä»è¿™ç§è¡¨è¿°ä¸­ä¸å¯èƒ½é¢†ä¼šä¸€ä¸ªè¯çš„æ„æ€
*   å½“é‡åˆ°åƒâ€œisï¼ŒTheï¼Œanï¼ŒIâ€è¿™æ ·çš„åœç”¨è¯æ—¶&å½“è¯­æ–™åº“æ˜¯ç‰¹å®šäºä¸Šä¸‹æ–‡çš„æ—¶ï¼Œé«˜é¢‘è¯æ›´é‡è¦æˆ–æä¾›å…³äºå¥å­çš„æ›´å¤šä¿¡æ¯çš„ç›´è§‰å¤±æ•ˆäº†ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªå…³äºæ–°å† è‚ºç‚çš„è¯­æ–™åº“ä¸­ï¼Œå† çŠ¶ç—…æ¯’è¿™ä¸ªè¯å¯èƒ½ä¸ä¼šå¢åŠ å¾ˆå¤šä»·å€¼

# é«˜çº§å¼“

ä¸ºäº†æŠ‘åˆ¶éå¸¸é«˜é¢‘ç‡çš„å•è¯&å¿½ç•¥ä½é¢‘ç‡çš„å•è¯ï¼Œéœ€è¦ç›¸åº”åœ°æ ‡å‡†åŒ–å•è¯çš„â€œæƒé‡â€

**TF-IDF è¡¨ç¤º:**TF-IDF çš„å®Œæ•´å½¢å¼æ˜¯æœ¯è¯­é¢‘ç‡-é€†æ–‡æ¡£é¢‘ç‡æ˜¯ 2 ä¸ªå› å­çš„ä¹˜ç§¯

![](img/3fd6d7d83cd000c097f21a5e33997b7f.png)

å…¶ä¸­ï¼ŒTF(wï¼Œd)æ˜¯å•è¯â€œwâ€åœ¨æ–‡æ¡£â€œdâ€ä¸­çš„é¢‘ç‡

IDF(w)å¯ä»¥è¿›ä¸€æ­¥ç»†åˆ†ä¸º:

![](img/c1fa34d12c15a2e70abcbef99e390b0b.png)

å…¶ä¸­ï¼ŒN æ˜¯æ–‡æ¡£æ€»æ•°ï¼Œdf(w)æ˜¯åŒ…å«å•è¯â€œwâ€çš„æ–‡æ¡£çš„é¢‘ç‡

> TF-IDF èƒŒåçš„ç›´è§‰æ˜¯ï¼Œåˆ†é…ç»™æ¯ä¸ªå•è¯çš„æƒé‡ä¸ä»…å–å†³äºå•è¯é¢‘ç‡ï¼Œè¿˜å–å†³äºç‰¹å®šå•è¯åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„å‡ºç°é¢‘ç‡

å®ƒé‡‡ç”¨ä¸Šä¸€èŠ‚ä¸­è®¨è®ºçš„ CountVectorizer å¹¶å°†å…¶ä¹˜ä»¥ IDF åˆ†æ•°ã€‚å¯¹äºéå¸¸é«˜é¢‘ç‡çš„è¯(å¦‚åœç”¨è¯)å’Œéå¸¸ä½é¢‘ç‡çš„è¯(å™ªå£°é¡¹)ï¼Œä»è¯¥è¿‡ç¨‹å¾—åˆ°çš„è¯çš„è¾“å‡ºæƒé‡å¾ˆä½

ç°åœ¨è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ Sklearn æ¥å®ç°è¿™ä¸€ç‚¹

```
from sklearn.feature_extraction.text import TfidfVectorizertext1 = ['i love nlp', 'nlp is so cool', 
'nlp is all about helping machines process language', 
'this tutorial is on baisc nlp technique']tf = TfidfVectorizer()
txt_fitted = tf.fit(text1)
txt_transformed = txt_fitted.transform(text1)
print ("The text: ", text1)# Output: The text:  ['i love nlp', 'nlp is so cool', 
# 'nlp is all about helping machines process language', 
# 'this tutorial is on basic nlp technique']idf = tf.idf_
print(dict(zip(txt_fitted.get_feature_names(), idf)))
```

# è¾“å‡º:

```
{'about': 1.916290731874155, 'all': 1.916290731874155, 
'basic': 1.916290731874155, 'cool': 1.916290731874155, 
'helping': 1.916290731874155, 'is': 1.2231435513142097, 
'language': 1.916290731874155, 'love': 1.916290731874155, 
'machines': 1.916290731874155, 'nlp': 1.0, 'on': 1.916290731874155, 
'process': 1.916290731874155, 'so': 1.916290731874155, 
'technique': 1.916290731874155, 'this': 1.916290731874155, 
'tutorial': 1.916290731874155}
```

æ³¨æ„å•è¯â€œnlpâ€çš„æƒé‡ã€‚å› ä¸ºå®ƒå‡ºç°åœ¨æ‰€æœ‰çš„å¥å­ä¸­ï¼Œæ‰€ä»¥å®ƒè¢«èµ‹äºˆ 1.0 çš„ä½æƒé‡ã€‚åŒæ ·ï¼Œåœç”¨è¯â€œisâ€çš„æƒé‡ä¹Ÿç›¸å¯¹è¾ƒä½ï¼Œä¸º 1.22ï¼Œå› ä¸ºå®ƒåœ¨ç»™å‡ºçš„ 4 ä¸ªå¥å­ä¸­æœ‰ 3 ä¸ªå‡ºç°ã€‚

ä¸ CountVectorizer ç±»ä¼¼ï¼Œå¯ä»¥è°ƒæ•´å„ç§å‚æ•°æ¥è·å¾—æ‰€éœ€çš„ç»“æœã€‚ä¸€äº›é‡è¦çš„å‚æ•°(é™¤äº†åƒå°å†™ã€strip_accentã€stop_words ç­‰æ–‡æœ¬é¢„å¤„ç†å‚æ•°ã€‚)æ˜¯ max_dfï¼Œmin_dfï¼Œnormï¼Œngram_range & sublinear_tfã€‚è¿™äº›å‚æ•°å¯¹è¾“å‡ºæƒé‡çš„å½±å“è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ï¼Œå°†å•ç‹¬è®¨è®ºã€‚

æ‚¨å¯ä»¥åœ¨ä¸‹é¢æ‰¾åˆ° TF-IDF çŸ¢é‡å™¨çš„å®Œæ•´æ–‡æ¡£:

[](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) [## sk learn . feature _ extraction . text . tfidf vectorizer-sci kit-learn 0 . 23 . 1 æ–‡æ¡£

### class sk learn . feature _ extraction . text . tfidf vectorizer(*ï¼Œinput='content 'ï¼Œencoding='utf-8 'ï¼Œdecode_error='strict'â€¦

scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) 

# TF-IDF ä»£è¡¨çš„ä¼˜åŠ¿:

*   ç®€å•ã€æ˜“äºç†è§£å’Œè§£é‡Šçš„å®æ–½
*   æ„å»º CountVectorizer æ¥æƒ©ç½šè¯­æ–™åº“ä¸­çš„é«˜é¢‘è¯å’Œä½é¢‘è¯ã€‚åœ¨æŸç§ç¨‹åº¦ä¸Šï¼ŒIDF é™ä½äº†æˆ‘ä»¬çŸ©é˜µä¸­çš„å™ªå£°ã€‚

# TF-IDF ä»£è¡¨çš„ç¼ºç‚¹:

*   è¿™ä¸ªå•è¯çš„ä½ç½®ä¿¡æ¯ä»ç„¶æ²¡æœ‰åœ¨è¿™ä¸ªè¡¨ç¤ºä¸­è¢«æ•è·
*   TF-IDF é«˜åº¦ä¾èµ–è¯­æ–™åº“ã€‚ç”±æ¿çƒæ•°æ®ç”Ÿæˆçš„çŸ©é˜µè¡¨ç¤ºä¸èƒ½ç”¨äºè¶³çƒæˆ–æ’çƒã€‚å› æ­¤ï¼Œéœ€è¦æœ‰é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®

# è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹

ç¦»æ•£è¡¨ç¤ºæ˜¯æ¯ä¸ªå•è¯éƒ½è¢«è®¤ä¸ºæ˜¯å”¯ä¸€çš„&æ ¹æ®æˆ‘ä»¬ä¸Šé¢è®¨è®ºçš„å„ç§æŠ€æœ¯è½¬æ¢æˆæ•°å­—ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å„ç§ç¦»æ•£è¡¨ç¤ºçš„ä¸€äº›é‡å çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œè®©æˆ‘ä»¬å°†å…¶ä½œä¸ºä¸€ä¸ªæ•´ä½“æ¥æ€»ç»“

# ç¦»æ•£è¡¨ç¤ºçš„ä¼˜ç‚¹:

*   æ˜“äºç†è§£ã€å®æ–½å’Œè§£é‡Šçš„ç®€å•è¡¨ç¤º
*   åƒ TF-IDF è¿™æ ·çš„ç®—æ³•å¯ä»¥ç”¨æ¥è¿‡æ»¤æ‰ä¸å¸¸è§å’Œä¸ç›¸å…³çš„å•è¯ï¼Œä»è€Œå¸®åŠ©æ¨¡å‹æ›´å¿«åœ°è®­ç»ƒå’Œæ”¶æ•›

# ç¦»æ•£è¡¨ç¤ºçš„ç¼ºç‚¹:

*   è¿™ç§è¡¨ç°ä¸è¯æ±‡é‡æˆæ­£æ¯”ã€‚è¯æ±‡é‡å¤§ä¼šå¯¼è‡´è®°å¿†å—é™
*   å®ƒæ²¡æœ‰åˆ©ç”¨å•è¯ä¹‹é—´çš„å…±ç°ç»Ÿè®¡ã€‚å®ƒå‡è®¾æ‰€æœ‰çš„å•è¯éƒ½æ˜¯ç›¸äº’ç‹¬ç«‹çš„
*   è¿™å¯¼è‡´å…·æœ‰å¾ˆå°‘éé›¶å€¼çš„é«˜åº¦ç¨€ç–çš„å‘é‡
*   å®ƒä»¬æ²¡æœ‰æŠ“ä½å•è¯çš„ä¸Šä¸‹æ–‡æˆ–è¯­ä¹‰ã€‚å®ƒä¸è®¤ä¸ºå¹½çµå’Œææ€–æ˜¯ç›¸ä¼¼çš„ï¼Œè€Œæ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„æœ¯è¯­ï¼Œå®ƒä»¬ä¹‹é—´æ²¡æœ‰å…±æ€§

ç¦»æ•£è¡¨ç¤ºå¹¿æ³›ç”¨äºç»å…¸çš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åº”ç”¨ï¼Œä»¥è§£å†³å¤æ‚çš„ç”¨ä¾‹ï¼Œå¦‚æ–‡æ¡£ç›¸ä¼¼æ€§ã€æƒ…æ„Ÿåˆ†ç±»ã€åƒåœ¾é‚®ä»¶åˆ†ç±»å’Œä¸»é¢˜å»ºæ¨¡ç­‰ã€‚

åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºæ–‡æœ¬çš„åˆ†å¸ƒå¼æˆ–è¿ç»­æ–‡æœ¬è¡¨ç¤º&å®ƒå¦‚ä½•ä¼˜äº(æˆ–åŠ£äº)ç¦»æ•£è¡¨ç¤ºã€‚

å¸Œæœ›ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ã€‚å›å¤´è§ï¼

ç¼–è¾‘:è¯¥ç³»åˆ—çš„ç¬¬äºŒéƒ¨åˆ†:[https://medium . com/@ sundareshchandran/introduction-to-text-representations-for-language-processing-Part-2-54fe 6907868](https://medium.com/@sundareshchandran/introduction-to-text-representations-for-language-processing-part-2-54fe6907868)

# å›è´­é“¾æ¥

[](https://github.com/SundareshPrasanna/Introduction-to-text-representation-for-nlp/tree/master) [## SundareshPrasanna/è‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡æœ¬è¡¨ç¤ºä»‹ç»

### æ­¤æ—¶æ‚¨ä¸èƒ½æ‰§è¡Œè¯¥æ“ä½œã€‚æ‚¨å·²ä½¿ç”¨å¦ä¸€ä¸ªæ ‡ç­¾é¡µæˆ–çª—å£ç™»å½•ã€‚æ‚¨å·²åœ¨å¦ä¸€ä¸ªé€‰é¡¹å¡ä¸­æ³¨é”€ï¼Œæˆ–è€…â€¦

github.com](https://github.com/SundareshPrasanna/Introduction-to-text-representation-for-nlp/tree/master) 

å–œæ¬¢æˆ‘çš„æ–‡ç« ï¼Ÿç»™æˆ‘ä¹°æ¯å’–å•¡

 [## sundaresh æ­£åœ¨åˆ›ä½œä¸æ•°æ®ç§‘å­¦ç›¸å…³çš„æ–‡ç« ï¼Œå¹¶ä¸”çƒ­çˆ±æ•™å­¦

### å˜¿ğŸ‘‹æˆ‘åˆšåˆšåœ¨è¿™é‡Œåˆ›å»ºäº†ä¸€ä¸ªé¡µé¢ã€‚ä½ ç°åœ¨å¯ä»¥ç»™æˆ‘ä¹°æ¯å’–å•¡äº†ï¼

www.buymeacoffee.com](https://www.buymeacoffee.com/sundaresh)