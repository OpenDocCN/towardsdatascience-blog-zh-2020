<html>
<head>
<title>PyTorch: Switching to the GPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch:切换到GPU</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-switching-to-the-gpu-a7c0b21e8a99?source=collection_archive---------2-----------------------#2020-05-03">https://towardsdatascience.com/pytorch-switching-to-the-gpu-a7c0b21e8a99?source=collection_archive---------2-----------------------#2020-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9492" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何以及为什么在GPU上训练模型——包括代码。</h2></div><p id="ef9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与TensorFlow不同，PyTorch没有为GPU用户提供专用的库，作为开发人员，您需要在这里进行一些手工操作。但是最后，它会节省你很多时间。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/fe2c1eb9024b50b1f8268270629550fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0aYy7B_-Mos4AAgd"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">照片由<a class="ae lu" href="https://unsplash.com/@virussinside?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Artiom Vallat </a>在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="5b93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想知道，在你的机器上安装CUDA或者在Colab <strong class="kk iu">上切换到GPU运行时还不够。不要误解我的意思，这仍然是必要的第一步，但是仅仅这样做并不能充分利用GPU的能力。</strong></p><p id="0932" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，您将了解如何针对以下场景从CPU切换到GPU:</p><ol class=""><li id="900d" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">训练/测试分离方法</li><li id="9ab3" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">数据装载方法</li></ol><p id="6286" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一个最常用于<strong class="kk iu">表格数据</strong>，而你每次处理<strong class="kk iu">图像数据</strong>时都会用到第二个(至少根据我的经验)。</p><p id="4a24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这两种方法之间有相当多的差异，因此将对每一种方法进行深入解释。我还应该提到，我将在这篇文章中使用<strong class="kk iu"> Google Colab </strong>。如果你还没有，你可以在这里阅读我的观点和更多相关内容:</p><div class="mj mk gp gr ml mm"><a rel="noopener follow" target="_blank" href="/google-colab-how-does-it-compare-to-a-gpu-enabled-laptop-851c1e0a2ca9"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">Google Colab:它与支持GPU的笔记本电脑相比如何？</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">Colab简介、运行时、性能比较…以及疑难解答</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na lo mm"/></div></div></a></div><p id="e38a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章的结构如下:</p><ol class=""><li id="a70b" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">为什么要转GPU？</li><li id="7d77" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">训练/测试分离方法</li><li id="21da" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">数据加载器方法</li><li id="f1ed" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">结论</li></ol><p id="5ed2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以事不宜迟，让我们开始吧！</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="fb0e" class="ni nj it bd nk nl nm nn no np nq nr ns jz nt ka nu kc nv kd nw kf nx kg ny nz bi translated">为什么要转GPU？</h1><p id="f85c" class="pw-post-body-paragraph ki kj it kk b kl oa ju kn ko ob jx kq kr oc kt ku kv od kx ky kz oe lb lc ld im bi translated">在你使用真正深度神经网络的情况下——例如<strong class="kk iu">用<em class="of"> ResNet152 </em>进行迁移学习</strong>——对CPU的训练会持续很长时间。如果你是一个理智的人，你就不会试图那样做。</p><p id="38fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线性代数运算在GPU上并行完成，因此您可以将训练时间减少大约<strong class="kk iu">100倍</strong>。不用说，但它也是在多个GPU上执行训练的一个选项，这将再次减少训练时间。</p><p id="3f61" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你不必相信我的话。我已经决定基于<a class="ae lu" href="https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip" rel="noopener ugc nofollow" target="_blank">这个数据集</a>做一个<strong class="kk iu">猫和狗的分类器</strong>。该模型基于<em class="of"> ResNet50 </em>架构——首先在CPU上训练，然后在GPU上训练。</p><p id="7fa8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是训练时间:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi og"><img src="../Images/bf373afcc139dd61d2d4480773130829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7eIzzR5JIUa444kEqximdQ.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">GPU运行时间:00:11:57h；CPU运行时间:06:08:40小时</p></figure><p id="16cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自己判断吧，不过我还是坚持GPU运行时。它在Colab上是免费的，所以没有理由不这样做。好了，我们现在知道GPU应该用于模型训练，现在让我们看看如何进行切换。</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="df59" class="ni nj it bd nk nl nm nn no np nq nr ns jz nt ka nu kc nv kd nw kf nx kg ny nz bi translated">训练/测试分离方法</h1><p id="5c71" class="pw-post-body-paragraph ki kj it kk b kl oa ju kn ko ob jx kq kr oc kt ku kv od kx ky kz oe lb lc ld im bi translated">如果你在<strong class="kk iu"> Scikit-Learn </strong>中用Python做了一些机器学习，你肯定对训练/测试分割很熟悉。简而言之，这种想法是在数据集的一部分(假设80%)上训练模型，并在剩余部分(假设20%)上评估模型。</p><p id="2675" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练/测试分割仍然是深度学习中的一种有效方法——特别是对于表格数据。首先要做的是声明一个变量，它将保存我们正在训练的设备(CPU或GPU):</p><pre class="lf lg lh li gt oh oi oj ok aw ol bi"><span id="d69d" class="om nj it oi b gy on oo l op oq">device = torch.device(<strong class="oi iu">'</strong>cuda<strong class="oi iu">'</strong> if torch.cuda.is_available() else <strong class="oi iu">'</strong>cpu<strong class="oi iu">'</strong>)<br/>device</span><span id="cedc" class="om nj it oi b gy or oo l op oq"><strong class="oi iu">&gt;&gt;&gt; device(type='cuda')</strong></span></pre><p id="2860" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我将声明一些虚拟数据，它们将作为<strong class="kk iu"> X_train </strong>张量:</p><pre class="lf lg lh li gt oh oi oj ok aw ol bi"><span id="b106" class="om nj it oi b gy on oo l op oq">X_train = torch.FloatTensor([0., 1., 2.])<br/>X_train</span><span id="08a7" class="om nj it oi b gy or oo l op oq"><strong class="oi iu">&gt;&gt;&gt; tensor([0., 1., 2.])</strong></span></pre><p id="9b4d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">酷！我们现在可以检查张量是否存储在GPU上:</p><pre class="lf lg lh li gt oh oi oj ok aw ol bi"><span id="134b" class="om nj it oi b gy on oo l op oq">X_train.is_cuda</span><span id="72eb" class="om nj it oi b gy or oo l op oq"><strong class="oi iu">&gt;&gt;&gt; False</strong></span></pre><p id="ce2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如所料——默认情况下，数据不会存储在GPU上，但将数据移动到那里相当容易:</p><pre class="lf lg lh li gt oh oi oj ok aw ol bi"><span id="b164" class="om nj it oi b gy on oo l op oq">X_train = X_train.to(device)<br/>X_train</span><span id="338c" class="om nj it oi b gy or oo l op oq"><strong class="oi iu">&gt;&gt;&gt; tensor([0., 1., 2.], device='cuda:0')</strong></span></pre><p id="d8bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">干净利落。可以再次执行相同的健全性检查，这一次我们知道张量被移动到GPU:</p><pre class="lf lg lh li gt oh oi oj ok aw ol bi"><span id="1637" class="om nj it oi b gy on oo l op oq">X_train.is_cuda</span><span id="2722" class="om nj it oi b gy or oo l op oq"><strong class="oi iu">&gt;&gt;&gt; True</strong></span></pre><p id="83d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="of">很好，但是模型声明呢？</em> </strong></p><p id="a69f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我很高兴你问了。同样，这是一件非常简单的事情:</p><pre class="lf lg lh li gt oh oi oj ok aw ol bi"><span id="8fd3" class="om nj it oi b gy on oo l op oq">model = MyAwesomeNeuralNetwork()<br/>model.to(device)</span></pre><p id="48ca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就这样，你现在可以开始训练了。简单回顾一下，这里有一个代码应该如何构建的总结:</p><pre class="lf lg lh li gt oh oi oj ok aw ol bi"><span id="2b2f" class="om nj it oi b gy on oo l op oq">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)<br/><strong class="oi iu">X_train, X_test = X_train.to(device), X_test.to(device)<br/>y_train, y_test = y_train.to(device), y_test.to(device)</strong></span><span id="572a" class="om nj it oi b gy or oo l op oq">class MyAwesomeNeuralNetwork(nn.Module):<br/>    # your model here</span><span id="fc86" class="om nj it oi b gy or oo l op oq">model = MyAwesomeNeuralNetwork()<br/><strong class="oi iu">model.to(device)</strong></span><span id="ccab" class="om nj it oi b gy or oo l op oq"># training code here</span></pre><p id="3e9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们继续使用数据加载器方法。</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="36a0" class="ni nj it bd nk nl nm nn no np nq nr ns jz nt ka nu kc nv kd nw kf nx kg ny nz bi translated">数据加载器方法</h1><p id="e90c" class="pw-post-body-paragraph ki kj it kk b kl oa ju kn ko ob jx kq kr oc kt ku kv od kx ky kz oe lb lc ld im bi translated">数据加载器方法在CNN中更常见，在这一节中，我们将了解如何将数据(图像)放在GPU上。第一步保持不变，因此你必须声明一个变量来保存我们正在训练的设备(CPU或GPU):</p><pre class="lf lg lh li gt oh oi oj ok aw ol bi"><span id="b3c3" class="om nj it oi b gy on oo l op oq">device = torch.device(<strong class="oi iu">'</strong>cuda<strong class="oi iu">'</strong> if torch.cuda.is_available() else <strong class="oi iu">'</strong>cpu<strong class="oi iu">'</strong>)<br/>device</span><span id="708c" class="om nj it oi b gy or oo l op oq"><strong class="oi iu">&gt;&gt;&gt; device(type='cuda')</strong></span></pre><p id="cd87" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将声明我们的模型，并将其放在GPU上:</p><pre class="lf lg lh li gt oh oi oj ok aw ol bi"><span id="3d5c" class="om nj it oi b gy on oo l op oq">model = MyAwesomeNeuralNetwork()<br/>model.to(device)</span></pre><p id="bec1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能已经注意到我们还没有在GPU上放置数据。<strong class="kk iu">直接传输数据加载器是不可能的</strong>，所以我们在这里必须更聪明一点。我们将在培训过程中传输图像，如下所示:</p><pre class="lf lg lh li gt oh oi oj ok aw ol bi"><span id="c35b" class="om nj it oi b gy on oo l op oq">for epoch in range(epochs):<br/>    for inputs, labels in train_loader:<br/>        inputs, labels = inputs.to(device), labels.to(device)</span></pre><p id="3da5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，代码的整体结构应该如下所示:</p><pre class="lf lg lh li gt oh oi oj ok aw ol bi"><span id="3222" class="om nj it oi b gy on oo l op oq">class MyAwesomeNeuralNetwork(nn.Module):<br/>    # your model here</span><span id="ebfa" class="om nj it oi b gy or oo l op oq">model = MyAwesomeNeuralNetwork()<br/><strong class="oi iu">model.to(device)</strong></span><span id="2c4e" class="om nj it oi b gy or oo l op oq">epochs = 10<br/>for epoch in range(epochs):<br/>    for inputs, labels in train_loader:<br/>        <strong class="oi iu">inputs, labels = inputs.to(device), labels.to(device)</strong><br/>        # backpropagation code here</span><span id="155d" class="om nj it oi b gy or oo l op oq">        # evaluation<br/>        with torch.no_grad():<br/>            for inputs, labels in test_loader:<br/>                <strong class="oi iu">inputs, labels = inputs.to(device), labels.to(device)</strong><br/>        # ...</span></pre><p id="f9c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">而这就是你要做的一切——数据和模型都放在GPU上。</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="c644" class="ni nj it bd nk nl nm nn no np nq nr ns jz nt ka nu kc nv kd nw kf nx kg ny nz bi translated">结论</h1><p id="031f" class="pw-post-body-paragraph ki kj it kk b kl oa ju kn ko ob jx kq kr oc kt ku kv od kx ky kz oe lb lc ld im bi translated">这就是你想要的——两个步骤可以大大减少培训时间。起初，它看起来像是你需要执行的许多额外的步骤，但是一旦你掌握了它的要点，它就简单了。</p><p id="92e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我永远不会建议你在CPU上进行训练，感谢Google Colab，你不必这样做——因为你可以免费使用GPU运行时。</p><p id="87ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读。</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><p id="2dd2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="of">喜欢这篇文章吗？成为</em> <a class="ae lu" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="of">中等会员</em> </a> <em class="of">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="mj mk gp gr ml mm"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">通过我的推荐链接加入Medium-Dario rade ci</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">medium.com</p></div></div><div class="mv l"><div class="os l mx my mz mv na lo mm"/></div></div></a></div></div></div>    
</body>
</html>