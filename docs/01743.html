<html>
<head>
<title>The future of mapping is learned</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">地图的未来是可以学习的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-future-of-mapping-is-learned-e13e93c03e22?source=collection_archive---------30-----------------------#2020-02-17">https://towardsdatascience.com/the-future-of-mapping-is-learned-e13e93c03e22?source=collection_archive---------30-----------------------#2020-02-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2cdd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">探索鲁棒可视化地图构建的新方法</h2></div><p id="27b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据视频数据对3d世界进行推理是许多系统的一个重要方面；室内/室外导航、自主感知、持续增强现实(AR)等等。在这篇文章中，我们将探索使用传统和现代的方法将视频提取为对机器和人类都有用的信息。</p><p id="a2ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为一个激励人心的例子，考虑一下在黑鹰历史博物馆捕捉这头雄伟的美洲野牛的3d本质的任务:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/c295a29ede05f22c78ff291657294df6.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*7XJvo5sgsRxU5Y9fRHseYQ.gif"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">美国野牛展(黑鹰博物馆，圣拉蒙)</p></figure><p id="a585" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用现代的运动结构(SfM)管道，捕捉完全三维的纹理模型是可能的。在这种方法中，相机的姿态(其连续的位置和方向)和稀疏的、有区别的场景特征被联合估计。通过解决这个问题(相机在哪里，它在看什么),我们可以捕捉主体的简洁、准确的表现:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/92128a2df2a528a36dd72938c7ea44f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QxqeFh0S1cE4WHkSbiCj7A.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">从上面的视频中估计的摄像机位置，以及稀疏的有区别的场景点</p></figure><p id="719e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管该场景模型对于定位相机(在初始捕获期间以及任何后续的重新访问期间)是有用的，但是对于想要与场景本身进行交互的任何人来说，它的用途是有限的。为此，我们需要一个密集的网格。</p><p id="0dcc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了构建更引人注目的东西，我们需要捕捉更精细的细节。多视图立体(MVS)是一个广泛的算法家族的总称(见<a class="ae le" href="http://vision.lems.brown.edu/sites/default/files/Seitz-etal-CVPR2006.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>的概述)，一旦摄像机轨迹已知，该算法恢复一个明显更密集的场景表示(增加了计算负担)。下面的视频显示了使用估计的相机位置从上面的场景恢复的深度图(每像素范围，以米为单位):</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/0f8a7566d17ed7fb8d79ad524f1ce984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*0QTN-XcCSTvTr2YOC5FS0w.gif"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">场景的密集深度估计；原始图像(左)，稀疏采样的深度图(右)</p></figure><p id="7927" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大多数MVS方法利用光度一致性测量；给定一幅图像中的一小块图像，这与另一幅图像中的候选匹配有什么关系？通过关联补丁和三角测量结果，我们可以恢复一个更密集的近似。然而，因为基础面片是场景几何图形、照明和纹理的函数，所以经常有大面积区域缺乏精确的深度估计(这将在后面变得明显)。鉴于这种密集的点云，我们可以从各种<a class="ae le" href="https://hal.inria.fr/hal-01348404v1/document" rel="noopener ugc nofollow" target="_blank">表面重建方法中选择</a>来构建最终的模型，下面的动画展示了一个例子:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/0eda5fe142804211b056c75ba4e1f8f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*tqMKaRYrFWTSBhXmchrTfg.gif"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">相机位置和稀疏场景特征与估计的密集纹理模型的叠加</p></figure><p id="4ec8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种纹理网格表示非常容易处理，对于以对象为中心的贴图任务，这种方法表现很好。下面是一些其他的例子:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ly"><img src="../Images/01fda96a871360c372b22555dce1c8ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bj0Q0eQxIjn5lozk-VwoGg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">普通物体的纹理网格；组合沙发(左)、雕像(中)、跑鞋(右)</p></figure><p id="6b7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是这种方法如何推广呢？考虑一个不同的用例，即典型办公环境中的大规模室内制图。下面的视频显示了以下对比:a)原始视频采集，b)投影到图像中的恢复的稀疏场景结构，以及c)相应的恢复的深度图:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/09ab5f8c840be6589bcba932df1097a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GwKwNhCfFY0LfJJMH_nG5A.gif"/></div></div></figure><p id="4bf8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从大量丢失的深度帧中可以明显看出，无纹理区域现在是有问题的。虽然相机位置和方向的估计是准确的，但生成一个非常密集的场景是一项具有挑战性的任务。(这里，系统参数在以对象为中心和通用映射模式中保持不变；我们可以花时间调整它们以产生更好的上下文深度估计，但这不是本文的重点)。</p><p id="7989" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定规则的棱镜场景结构，更有效的方法是<strong class="kk iu">学习</strong>从外观到深度的映射。这样，我们可以针对无纹理区域建立鲁棒性。深度卷积神经网络的进步与“地面真实”室内数据集(例如[ <a class="ae le" href="http://rgbd.cs.princeton.edu/" rel="noopener ugc nofollow" target="_blank"> 1 </a>、[ <a class="ae le" href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" rel="noopener ugc nofollow" target="_blank"> 2 </a>、[ <a class="ae le" href="http://pr.cs.cornell.edu/sceneunderstanding/data/data.php" rel="noopener ugc nofollow" target="_blank"> 3 </a>)的爆炸结合在一起，使得高性能深度估计架构的开发成为可能。对于这个问题，有多种不同的方法来恢复场景结构:</p><ol class=""><li id="17ec" class="lz ma it kk b kl km ko kp kr mb kv mc kz md ld me mf mg mh bi translated">学习直接从单目图像到深度的映射，不考虑空间一致性(例如[ <a class="ae le" href="https://arxiv.org/pdf/1812.11941v2.pdf" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]，[ <a class="ae le" href="https://arxiv.org/pdf/1606.00373v2.pdf" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]，[ <a class="ae le" href="https://arxiv.org/abs/1806.02446" rel="noopener ugc nofollow" target="_blank"> 3 </a> ])</li><li id="0c6f" class="lz ma it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">通过使用多个图像及其估计的姿态(来自SfM步骤)来结合空间一致性，以学习更好的立体深度估计器(例如[ <a class="ae le" href="https://arxiv.org/abs/1807.08563v1" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]、[ <a class="ae le" href="https://arxiv.org/abs/1902.02166v2" rel="noopener ugc nofollow" target="_blank"> 2 </a>、[ <a class="ae le" href="https://arxiv.org/abs/1905.00538v1" rel="noopener ugc nofollow" target="_blank"> 3 </a> ])</li><li id="10ae" class="lz ma it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">直接回归输出目标点云，而不是深度帧(例如[ <a class="ae le" href="https://arxiv.org/abs/1908.04422" rel="noopener ugc nofollow" target="_blank"> 1 </a> ])</li></ol><p id="64c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了从<a class="ae le" href="https://github.com/ialhashim/DenseDepth" rel="noopener ugc nofollow" target="_blank">单目</a>密集深度实现和习得<a class="ae le" href="https://github.com/HKUST-Aerial-Robotics/MVDepthNet" rel="noopener ugc nofollow" target="_blank"> MVS </a>估计器获得的深度图的比较:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/2b5469473abd504c61ab67a17f54cfd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*gXZ3g4umPmqKqRBpuYwkmw.gif"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">单目深度估计(中)和多视图立体深度(右)与输入影像(左)的比较</p></figure><p id="df6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(注意，以上视频仅供定性评价；不同的训练集和SfM输出的质量极大地影响所得的深度图)。</p><p id="47e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">来自单目估计器的输出通常更一致，尽管给定模型的单次拍摄性质，聚合帧缺乏学习MVS方法的全局度量准确性。然而，依赖于运动来推断深度的方法在经历退化运动(仅旋转或小平移)时会受到影响。我们可以并排比较这些系统的指标输出:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mn"><img src="../Images/b30325c5f3e8595a6bc54d09c339b0e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M2N9LWyWVg6YChf6EZUddQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">使用上面显示的数据集对DenseDepth(左)与MVDepthNet(右)进行定性比较。</p></figure><p id="7d8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然令人信服，我们仍然需要将这些单独的估计融合成一个令人信服的整体，这提出了单独的挑战。如何将这种方法更进一步，在同一模型中联合估计相机运动<strong class="kk iu">和</strong>密集输出？通过这种方式，我们不需要在离散的步骤中估计相机运动和密集场景输出。</p><p id="24d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">几种方法已经考虑估计密集摄像机深度和运动(例如[<a class="ae le" href="https://arxiv.org/abs/1808.01900v2" rel="noopener ugc nofollow" target="_blank">1</a>][<a class="ae le" href="https://arxiv.org/abs/1612.02401v2" rel="noopener ugc nofollow" target="_blank">2</a>][<a class="ae le" href="https://arxiv.org/abs/1912.09697v1" rel="noopener ugc nofollow" target="_blank">3</a>])；这里我们考虑架构<a class="ae le" href="https://arxiv.org/abs/1812.04605" rel="noopener ugc nofollow" target="_blank"> DeepV2D </a>架构。DeepV2D使用“块”坐标下降来估计场景和相机几何；固定的相机姿态用于构建密集的场景表示，该场景表示随后用于改进相机估计。相机姿势示例(红色)和生成的场景如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mo"><img src="../Images/b2aa3f19407345e712ba5b06c9610604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IEBJbmGgEKCVI8hWuxRjkw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">DeepV2D相机和场景几何</p></figure><p id="1264" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据<a class="ae le" href="http://www.scan-net.org/" rel="noopener ugc nofollow" target="_blank">相似的室内数据</a>训练的模型的适用性从上面显示的输出中显而易见。虽然应用于不同领域(例如室外区域)时性能会下降，但在合理的时间范围内(推断时间与数据集采集时间成比例)从测试相关环境中推断相机运动和密集场景几何的能力令人印象深刻。</p><p id="cc9d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着<a class="ae le" href="https://heartbeat.fritz.ai/ios-12-core-ml-benchmarks-b7a79811aac1" rel="noopener ugc nofollow" target="_blank">不断提高</a>在设备上的性能，未来的方向是明确的；学习如何推理深度和运动来构建地图是一种构建大比例的鲁棒、精确地图的强大方法。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/240e0343847bd49b6063e5efd4ff2542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*-DCabocVpMeNeiyzrMwYjA.gif"/></div></div></figure></div></div>    
</body>
</html>