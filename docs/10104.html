<html>
<head>
<title>Interesting papers I read from ICML 2020</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我从 ICML 2020 中读到的有趣的论文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interesting-papers-i-read-from-icml-2020-part-1-df61ac397b94?source=collection_archive---------38-----------------------#2020-07-16">https://towardsdatascience.com/interesting-papers-i-read-from-icml-2020-part-1-df61ac397b94?source=collection_archive---------38-----------------------#2020-07-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/3cffcd2f7b446e8a2a8a9ac1fc906bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZBuHfhzIO9Vy3GZC"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">这是我从未去过的维也纳(图片由<a class="ae kf" href="https://unsplash.com/@dylu?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚采克·迪拉格</a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄)</p></figure><p id="3351" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">今年的机器学习国际会议(ICML)将在网上虚拟举行，这是一个很好的机会，让人们不用花太多钱就能参加，这对于不一定在 ML 的研究人员来说很好。所有的论文演示都是预先录制的，并且还提供了两个实时放大的问答部分。</p><p id="98c3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到第三天，我已经从各种论文、教程、小组讨论和指导会议中学到了很多。在这个系列中，我决定分享一些我觉得有趣的论文的笔记。这份名单绝不是在穷尽所有<strong class="ki iu">1086 篇</strong>被录取论文后的公正选择。论文是随机排序的，当然偏向于我感兴趣的话题。所以我们开始吧:</p><h1 id="1df9" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">1.通过超球面上的几何理解对比表征学习</h1><h2 id="6127" class="mc lf it bd lg md me dn lk mf mg dp lo kr mh mi ls kv mj mk lw kz ml mm ma mn bi translated">[ <a class="ae kf" href="https://proceedings.icml.cc/static/paper_files/icml/2020/5503-Paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae kf" href="https://icml.cc/virtual/2020/poster/6657" rel="noopener ugc nofollow" target="_blank">演示文稿</a> ][ <a class="ae kf" href="https://github.com/SsnL/align_uniform" rel="noopener ugc nofollow" target="_blank">代码</a></h2><p id="6808" class="pw-post-body-paragraph kg kh it ki b kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">对比表征学习是我最近最感兴趣的话题之一。我对对比损失进行了实验(见这里的<a class="ae kf" rel="noopener" target="_blank" href="/contrasting-contrastive-loss-functions-3c13ca5f055e">和这里的</a>和<a class="ae kf" rel="noopener" target="_blank" href="/contrastive-loss-for-supervised-classification-224ae35692e7">和</a>),发现它对于在没有监督的情况下学习其他任务的有用表征非常有效。</p><p id="8365" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，作者为对比目标提供了优雅的几何解释。他们将对比损失目标分解为两个量，用于评估学习表征空间的几何形状:</p><ol class=""><li id="fa30" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld my mz na nb bi translated">对齐(紧密度):来自正配对的嵌入彼此紧密吗？</li><li id="4631" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld my mz na nb bi translated">均匀性:投射到嵌入空间的样本是否均匀分散？</li></ol><p id="f17e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者发现，与使用监督目标学习的表征相比，通过优化对比目标学习的表征确实具有这两种性质。这两个量也可以用作神经网络的损失函数来进行显式优化，这与使用对比损失达到了类似的效果。他们还表明，对于学习监督任务的良好表示，对齐和一致性都是必需的。</p><h1 id="c910" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">2.连续图神经网络</h1><h2 id="4bf2" class="mc lf it bd lg md me dn lk mf mg dp lo kr mh mi ls kv mj mk lw kz ml mm ma mn bi translated">[ <a class="ae kf" href="https://proceedings.icml.cc/static/paper_files/icml/2020/4075-Paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>[<a class="ae kf" href="https://icml.cc/virtual/2020/poster/6441" rel="noopener ugc nofollow" target="_blank">简报</a></h2><p id="b49d" class="pw-post-body-paragraph kg kh it ki b kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">本文作者提出了一种方法来解决离散图神经网络(GNNs)在执行多个传播层的前向传递时性能下降的问题。众所周知，当 GNN 层数过多时，广义神经网络会出现过度平滑的问题。这是因为拉普拉斯平滑具有使来自具有相同程度的节点的传播信息彼此更加相似的趋势，从而掩盖了来自单个节点的独特特征。</p><p id="05c7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者设计了他们的方法连续 GNN (CGNN)，该方法受<a class="ae kf" href="https://arxiv.org/abs/1806.07366" rel="noopener ugc nofollow" target="_blank">神经节点</a>的启发，在节点表示上模拟连续动力学。他们的实证结果显示，CGNN 在标准基准(Cora、Citeseer、PubMed 等)上击败了图卷积网络(GCN)和<a class="ae kf" href="https://arxiv.org/abs/1710.10903" rel="noopener ugc nofollow" target="_blank">图注意力网络(GAT) </a>。)用于半监督节点分类任务。</p><p id="9272" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然作者如何发明 CGNN 的推导细节很难理解，但我确实发现这篇论文是 GNNs 的一个重要进步，因为它能够开发具有多个消息传递层的“更深层次”GNNs，而不会损失性能和图上节点之间的长期依赖性。更多关于 GCN 的信息，请阅读我之前的文章。</p><h1 id="d7ca" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">3.无监督文本可控表征的变分学习</h1><h2 id="bca2" class="mc lf it bd lg md me dn lk mf mg dp lo kr mh mi ls kv mj mk lw kz ml mm ma mn bi translated">[ <a class="ae kf" href="https://proceedings.icml.cc/static/paper_files/icml/2020/5816-Paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae kf" href="https://icml.cc/virtual/2020/poster/6704" rel="noopener ugc nofollow" target="_blank">演示文稿</a></h2><p id="cf77" class="pw-post-body-paragraph kg kh it ki b kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">为什么不能对文本进行风格转换？</p><p id="9d09" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，作者解释了为什么变分自动编码器(VAE)不能通过潜在空间操作进行文本生成。利用拓扑分析，他们发现 VAE 在文本数据上学习的潜在空间(<strong class="ki iu"> <em class="nh"> z </em> </strong>)比在图像数据上学习的潜在空间有更多的“漏洞”。</p><p id="994b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了减轻这种影响，他们开发了 CP-VAE(约束后验概率)，在 VAE 损失上增加了两项，以 1)鼓励学习的潜在空间具有正交基(我猜这是受 PCA 的启发？);以及 2)用类似于对比损失的结构重建损失来“填充”潜在空间。通过在损失函数中加入这些额外的术语，作者证明了合作原则-VAE 的潜在空间得到了更充分的“填充”,可以在语篇中进行“风格转移”。虽然，我认为作者提供的例子也稍微改变了输入句子的内容。</p><h1 id="a56e" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">4.CURL:用于强化学习的对比无监督表示学习</h1><h2 id="ccbd" class="mc lf it bd lg md me dn lk mf mg dp lo kr mh mi ls kv mj mk lw kz ml mm ma mn bi translated">[ <a class="ae kf" href="https://proceedings.icml.cc/static/paper_files/icml/2020/5951-Paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae kf" href="https://icml.cc/virtual/2020/poster/6729" rel="noopener ugc nofollow" target="_blank">简报</a> ][ <a class="ae kf" href="https://github.com/MishaLaskin/curl" rel="noopener ugc nofollow" target="_blank">代号</a></h2><p id="c9b7" class="pw-post-body-paragraph kg kh it ki b kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">对比学习的观察表征对学习有帮助吗？</p><p id="9413" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">答案是肯定的。在本文中，作者试图弥合 RL 代理与代理之间的差距，RL 代理可以访问底层状态，而代理只能在 DeepMind 控制环境(如 Walker)上通过像素看到观察结果。受对比表征学习最新进展的启发，作者开发了 CURL，它使用观察轨迹的重放缓冲区合并了对比表征学习，作者表明 RL 代理能够从像素观察的学习表征中注意相关区域，这有助于代理更快地学习。然而，这一结果的实现离不开三个绝妙的技巧:</p><ol class=""><li id="95da" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld my mz na nb bi translated">跨观测轨迹应用一致随机裁剪以保留时间结构</li><li id="ec41" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld my mz na nb bi translated">使用双线性内积代替余弦相似性作为编码观察序列的相似性度量</li><li id="4cbb" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld my mz na nb bi translated">使用指数移动平均(EMA)对观察值序列进行编码，以保留一些短期记忆</li></ol><p id="cfdb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">CURL 显著提高了基线 RL 算法的数据效率，在某些任务上达到最佳策略的速度提高了 5 倍。但是在 RL 代理访问真实状态方面仍然存在一些差距。作者也承认 CURL 在一些复杂动态的环境中失败了。这可能是由于来自像素的观察可能无法捕捉完整的状态信息，例如速度和接触力。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="90ba" class="le lf it bd lg lh np lj lk ll nq ln lo lp nr lr ls lt ns lv lw lx nt lz ma mb bi translated">5.感知生成自动编码器</h1><p id="19c0" class="pw-post-body-paragraph kg kh it ki b kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">[ <a class="ae kf" href="https://proceedings.icml.cc/static/paper_files/icml/2020/1042-Paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae kf" href="https://icml.cc/virtual/2020/poster/5921" rel="noopener ugc nofollow" target="_blank">简报</a> ][ <a class="ae kf" href="https://github.com/zj10/PGA" rel="noopener ugc nofollow" target="_blank">代号</a></p><p id="5234" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是在我们最喜欢的 VAE 以及深度生成模型的基础上开发的又一个杰出作品。作者认为，现代深度生成模型(GANs、基于流的模型和 VAE)的不完善源于未能解释数据的内在维度和环境维度之间的差异。为了解决这个问题，作者提出了感知生成自动编码器(PGA ),它可以最小化除数据重建误差之外的潜在重建误差。</p><p id="92ac" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从概念上讲，这就像为潜在空间和数据空间绑定了自动编码器。使用常规符号，让我们用<strong class="ki iu"> <em class="nh"> x </em> </strong>来表示一个高维数据点，<strong class="ki iu"> <em class="nh"> z </em> </strong>是它的潜在向量。我们有 encoder \ hat {<strong class="ki iu"><em class="nh">z</em></strong>} =<em class="nh">f</em>(<strong class="ki iu"><em class="nh">x</em></strong>)和 docoder \ hat {<strong class="ki iu"><em class="nh">x</em></strong>} =<em class="nh">g</em>(<strong class="ki iu"><em class="nh">z</em></strong>)。<strong class="ki iu"> <em class="nh"> x </em> </strong>的自动编码器将是\ hat {<strong class="ki iu"><em class="nh">x</em></strong>} =<em class="nh">g</em>(<em class="nh">f</em>(<strong class="ki iu"><em class="nh">x</em></strong>)。从<strong class="ki iu"> <em class="nh"> z </em> </strong>的角度看，编码器<em class="nh"> f </em>(。)实际上是解码器为<strong class="ki iu"> <em class="nh"> z </em> </strong>而解码器为<em class="nh"> g </em>(。)是编码器<strong class="ki iu"> <em class="nh"> z </em> </strong>。<strong class="ki iu"> <em class="nh"> z </em> </strong>的自动编码器将是\ hat {<strong class="ki iu"><em class="nh">z</em></strong>} =<em class="nh">f</em>(<em class="nh">g</em>(<strong class="ki iu"><em class="nh">z</em></strong>)。PGA 以这种方式看待自动编码器，并在来自数据空间的重建误差之上添加了两个潜在的重建损失项，这两个项都最小化了<strong class="ki iu"><em class="nh"/></strong>z<em class="nh">f</em>(<em class="nh">g</em>(<strong class="ki iu"><em class="nh">z</em></strong>)之间的 L2 距离。这里的<strong class="ki iu"> <em class="nh"> z </em> </strong>可以从预先定义的高斯先验或者后验<em class="nh">q</em>(<strong class="ki iu"><em class="nh">z</em></strong>|<strong class="ki iu"><em class="nh">x</em></strong>)中进行采样。</p><p id="ab94" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，作者展示了添加 PGA 损失项的 VAE 显著提高了通过<a class="ae kf" href="https://arxiv.org/abs/1706.08500" rel="noopener ugc nofollow" target="_blank"> FID 分数</a>测量的生成数据的质量，从而解决了 VAE 的样本模糊问题。</p><h1 id="38d3" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">6.NGBoost:用于概率预测的自然梯度推进</h1><p id="b063" class="pw-post-body-paragraph kg kh it ki b kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">[ <a class="ae kf" href="https://proceedings.icml.cc/static/paper_files/icml/2020/3337-Paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae kf" href="https://icml.cc/virtual/2020/poster/6336" rel="noopener ugc nofollow" target="_blank">演示</a> ][ <a class="ae kf" href="https://github.com/stanfordmlgroup/ngboost" rel="noopener ugc nofollow" target="_blank">代码</a></p><p id="2ea9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大多数监督 ML 回归模型仅给出给定特征向量的数据点的点估计。对于某些回归问题，预测的置信区间也是至关重要的。</p><p id="15a3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇论文中，作者提出了一个模块算法来进行概率预测。该算法由以下部分组成:</p><ul class=""><li id="5b55" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nu mz na nb bi translated">基础学员(<em class="nh"> f </em>)</li><li id="ad99" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nu mz na nb bi translated">目标 P_theta(Y|X=x)的参数化概率分布，可以是正态泊松分布</li><li id="f3d3" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nu mz na nb bi translated">评分规则<em class="nh"> S </em> (\theta，y)，可以是均方误差</li></ul><p id="e836" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者发现，对于普通梯度，即<em class="nh"> S </em> w.r.t .参数\theta 的梯度，该算法不起作用，因为梯度对于参数\theta 不是不变的。换句话说，通过遵循普通梯度来更新\theta 就像朝着移动目标进行优化。为了克服这个问题，作者对自然梯度进行了梯度下降，它位于分布空间而不是参数空间。自然梯度定义为<a class="ae kf" href="https://en.wikipedia.org/wiki/Riemannian_geometry" rel="noopener ugc nofollow" target="_blank">黎曼空间</a>中的最陡上升，它是\theta 的不变量。自然梯度也可以通过用 Reimannian 度量转换普通梯度来计算，这取决于评分规则的参数形式。</p><p id="8cd9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过使用自然梯度的梯度增强，NGBoost 算法能够正确地估计预测的均值和方差。作者还表明，在许多回归任务上，它与现有的更复杂的算法表现相当好。</p><p id="ecb8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我也会给 scikit-learn 兼容 API 加分。</p><h1 id="a9ea" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">7.从不规则采样的时间序列中学习:一个缺失数据的视角</h1><p id="ddb1" class="pw-post-body-paragraph kg kh it ki b kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">[ <a class="ae kf" href="https://proceedings.icml.cc/static/paper_files/icml/2020/3129-Paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae kf" href="https://icml.cc/virtual/2020/poster/6300" rel="noopener ugc nofollow" target="_blank">演示文稿</a></p><p id="09f6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">临床时间序列数据通常是不规则采样的:变量既不是以均匀的间隔测量的，也不是完全随机的。本文作者通过将不规则采样时间序列视为缺失数据问题，设计了一种生成过程。具体来说，他们使用了一个编解码器框架，该框架可以是 VAE 或双向 GAN(甘比)，以显式地对数据<strong class="ki iu"><em class="nh">【x】</em></strong>及其索引<strong class="ki iu"> <em class="nh"> t </em> </strong>进行建模。这个框架直接作用于离散时间序列。</p><p id="b9f2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者设计了一个有趣的实验，他们从 MNIST 和西里巴等数据集掩盖了大部分图像，以模拟不规则采样的离散时间序列。并且任务是让模型在给定可用像素(<strong class="ki iu"> <em class="nh"> x </em> </strong>)及其索引(<strong class="ki iu"> <em class="nh"> t </em> </strong>)的情况下，填充图像的缺失部分。他们的模型取得了相当好的性能。</p><p id="f5eb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了将这个框架扩展到具有缺失数据的连续时间序列，作者在解码器上覆盖了一个内核平滑器，在编码器上覆盖了一个卷积层。这里的巧妙之处在于利用互相关将不规则采样的数据点转换为均匀间隔的信号。作者接下来表明，该框架还能够从<a class="ae kf" href="https://mimic.physionet.org/" rel="noopener ugc nofollow" target="_blank"> MIMIC-III 数据集</a>中学习连续时间序列的有用表示，以帮助患者生理轨迹的下游分类任务(预测死亡率)。</p><h1 id="f200" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">8.使用疾病进展的深度预测聚类进行时间表型分析</h1><p id="1c75" class="pw-post-body-paragraph kg kh it ki b kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">[ <a class="ae kf" href="https://proceedings.icml.cc/static/paper_files/icml/2020/1742-Paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae kf" href="https://icml.cc/virtual/2020/poster/6046" rel="noopener ugc nofollow" target="_blank">简报</a> ][ <a class="ae kf" href="https://github.com/chl8856/AC_TPC" rel="noopener ugc nofollow" target="_blank">代号</a> ]</p><p id="713f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">沿着临床时间序列建模的思路，本研究旨在对时间序列数据进行监督聚类，以反映未来的标签。在我看来，这篇论文的问题表述非常不寻常:作者试图在给定时间点<em class="nh"> t </em>分类轨迹的同时学习分类表示。作者表示，这是一个具有挑战性的问题，因为它是 NP 难的，并且在选择聚类成员时涉及采样过程。</p><p id="1108" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">提出的算法 actor-critic 时序表型聚类(AC-TPC)由三个神经网络组成:</p><ul class=""><li id="940e" class="mt mu it ki b kj kk kn ko kr mv kv mw kz mx ld nu mz na nb bi translated">编码器:<strong class="ki iu"><em class="nh">z _ t</em></strong>=<em class="nh">f</em>(<strong class="ki iu"><em class="nh">x _ { 1:t }</em></strong>)</li><li id="fb0d" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nu mz na nb bi translated">选择器:提供集群分配<em class="nh">s _ t</em>=<em class="nh">h</em>(<strong class="ki iu">T5】z _ tT7)</strong></li><li id="a345" class="mt mu it ki b kj nc kn nd kr ne kv nf kz ng ld nu mz na nb bi translated">预测器:根据编码<strong class="ki iu"> <em class="nh"> z_t </em> </strong>或嵌入进行预测</li></ul><p id="1537" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于该算法旨在同时对数据进行聚类和分类，AC-TPC 的损失函数需要最小化真实标签和聚类分配之间的差异，同时确保每个轨迹仅分配给一个主导聚类，并防止聚类内的样本相互重叠。</p><p id="a040" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">他们在时态医学数据集上的实验表明，AC-TPC 具有更好的预测性能，并且比基线算法产生更好的聚类质量。通过诸如纯度、调整随机指数(ARI)和归一化互信息(NMI)的聚类度量来测量聚类质量。作者还表明，AC-TPC 允许聚类分配随着新的观察结果添加到轨迹中而改变，反映了患者表型可能因后续干预而改变的真实世界场景。</p></div></div>    
</body>
</html>