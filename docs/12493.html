<html>
<head>
<title>Generate Fresh Movie Stories for your Favorite Genre with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过深度学习为您最喜欢的类型生成新鲜的电影故事</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generate-fresh-movie-stories-for-your-favorite-genre-with-deep-learning-143da14b29d6?source=collection_archive---------17-----------------------#2020-08-28">https://towardsdatascience.com/generate-fresh-movie-stories-for-your-favorite-genre-with-deep-learning-143da14b29d6?source=collection_archive---------17-----------------------#2020-08-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="93f6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">微调 GPT 新协议，根据类型生成故事</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d4a0ad28f36490c2664fb718b4e0089b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gpg_j60icweJmNSf"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@tsaichinghsuan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">盛鹏鹏摄蔡</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><blockquote class="kz la lb"><p id="5791" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf iu">在发现时间旅行</strong>后，地球居民现在生活在由政府控制的未来派城市中，为期十年。政府计划向该市派遣两个精英科学家小组，以便调查这些机器的起源并发现“上帝”的存在。</p></blockquote><p id="4be4" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi mc translated">为你喜欢的类型创作故事不是很有趣吗？这就是我们今天要做的。我们将学习如何构建一个基于流派创建故事的故事生成器，就像上面创建科幻故事的生成器一样(在上面的故事中，用户提供的输入是粗体的)。</p><p id="91ac" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">你也可以把这篇文章作为开发你自己的文本生成器的起点。例如，您可以生成科技、科学和政治等主题的标题。或者生成你喜欢的艺术家的歌词。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="eea0" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">为什么是故事生成？</h1><p id="67b5" class="pw-post-body-paragraph lc ld it lf b lg nk ju li lj nl jx ll lz nm lo lp ma nn ls lt mb no lw lx ly im bi translated">作为一个狂热的电影和电视剧迷，我喜欢故事生成器的想法，它可以根据类型、输入提示甚至标题生成故事。在了解了 GPT 2 号之后，我想把这个想法变成现实。这就是我建造这个模型的原因。</p><p id="73d4" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">预期用途</strong>:寻找乐趣并测试融合讲故事的想法，我们可以通过混合我们的创造力(通过提供提示)和模型的创造力(通过使用提示生成故事的其余部分)来生成故事。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="0416" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">预告片时间:测试故事生成器！</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/c5b5e1cc5548b8536aab38af9121085d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OqcRq0rUgfLDqHIP"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Alex Litvin 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b8f9" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">在深入构建生成器之前，让我们先尝试一下故事生成。在这个<a class="ae ky" href="https://huggingface.co/pranavpsv/gpt2-genre-story-generator" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu"> Huggingface 链接</strong> </a> <strong class="lf iu"> </strong>查看我的故事生成器或者运行这个<a class="ae ky" href="https://colab.research.google.com/drive/17dEk6VJk_jnd7d5-jbaOrwsS367M2pWw?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu"> Colab 笔记本</strong>中的单元格来生成故事</a>。模型输入格式的形式如下:</p><p id="51fd" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu"> &lt; BOS &gt; &lt;流派&gt;可选提示……</strong></p><p id="7298" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">例如</strong>&lt;BOS&gt;&lt;sci _ fi&gt;发现时间旅行后，</p><p id="05a8" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">所属类型</strong>所属:超级英雄、科幻、动作、剧情、恐怖、惊悚</p><p id="c3e9" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">模型将使用这个提示生成它的故事。还有一种更直观的方式来生成故事:一个使用我的模型 的<a class="ae ky" href="http://54.173.99.218:8501" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu">网络应用(记住这个应用的生成比<a class="ae ky" href="https://huggingface.co/pranavpsv/gpt2-genre-story-generator" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu">hugging face link</strong></a>慢)。</strong></a></p><p id="d94c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在你已经完成了对模型的实验，让我们来探索这个想法:我们正在一个包含不同流派电影情节的数据集上微调 OpenAI GPT-2 模型。本演练遵循<strong class="lf iu">三幕结构:</strong></p><ul class=""><li id="3f9c" class="nq nr it lf b lg lh lj lk lz ns ma nt mb nu ly nv nw nx ny bi translated">第一幕:什么是新 GPT 协议？</li><li id="2bce" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu">第二幕:微调时间……</strong></li><li id="8139" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu">第三幕:生成时间！</strong></li></ul></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="e161" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">第一幕:什么是 GPT-2？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/cac5908c56459796acf9b99d0a4816de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YGJmtn2XXEf_nEap"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马特·波波维奇在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a4cb" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">如果你熟悉 GPT 2 的关键思想，你可以快进到下一幕。否则，请快速复习下面的 GPT 新协议。</p><p id="fb9f" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">最近，我们看到了<strong class="lf iu">文本生成模型 GPT-3 </strong>背后的巨大宣传，以及它在使用零触发学习生成代码<a class="ae ky" rel="noopener" target="_blank" href="/will-gpt-3-kill-coding-630e4518c04d"><strong class="lf iu"/></a>等任务中令人眼花缭乱的表现。GPT 2 号是 GPT 3 号的前身。</p><p id="05a2" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">等等，为什么我们用 GPT 2 号而不是 GPT 3 号？嗯，还在测试阶段。还有…</p><p id="bab0" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">正如其炒作一样，GPT-3 的尺寸也是<a class="ae ky" href="https://github.com/openai/gpt-3" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu">硕大</strong> </a> ( <a class="ae ky" href="https://github.com/openai/gpt-3/issues/1" rel="noopener ugc nofollow" target="_blank">传言要求<strong class="lf iu">350 GB RAM</strong></a>)。虽然体积仍然庞大，但 GPT-2 占用的空间少得多(最大的变体占用<a class="ae ky" href="https://github.com/openai/gpt-3/issues/1" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu"> 6GB 磁盘空间</strong> </a>)。它甚至有不同的尺寸<a class="ae ky" href="https://huggingface.co/transformers/model_doc/gpt2.html#:~:text=GPT%2D2%20is%20one%20of,code%20can%20be%20found%20here." rel="noopener ugc nofollow" target="_blank">和</a>。所以，我们可以在很多设备上使用它(包括<a class="ae ky" href="https://twitter.com/julien_c/status/1154771974213816321?lang=en" rel="noopener ugc nofollow" target="_blank">iphone</a>)。</p><p id="a69e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">如果你在寻找 GPT-2 的详细概述，你可以直接从<a class="ae ky" href="https://openai.com/blog/better-language-models/#fn1" rel="noopener ugc nofollow" target="_blank">马嘴</a>那里得到。</p><p id="1b56" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">但是，如果你只是想快速总结一下 GPT 2 号，这里有一个经过提炼的纲要:</p><ul class=""><li id="dd8a" class="nq nr it lf b lg lh lj lk lz ns ma nt mb nu ly nv nw nx ny bi translated">GPT-2 是一个<strong class="lf iu">文本生成</strong> <a class="ae ky" href="https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu">语言模型</strong> </a>使用一个<a class="ae ky" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">解码器专用变压器</a>(一个<a class="ae ky" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">变压器架构</a>的变体)。如果这看起来像是胡言乱语，只需知道<strong class="lf iu"> Transformer </strong>是用于<a class="ae ky" rel="noopener" target="_blank" href="/your-guide-to-natural-language-processing-nlp-48ea2511f6e1"> NLP </a>模型的最先进的架构。</li><li id="d052" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated">它被预先训练(<a class="ae ky" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">具有<strong class="lf iu"> 40GB 的文本数据</strong> </a>),任务是预测作为输入的前一文本在每个时间步长的下一个单词(更正式地说，令牌)。</li></ul></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="d11f" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">第二幕:微调时间…</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/5b9cfc3e21c9f285a6277006df27ccd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GD-0wR8qbpApOIRZ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@kushagrakevat?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Kushagra Kevat </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="e7ad" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们可以在选定的数据集上<a class="ae ky" href="https://www.youtube.com/watch?v=5T-iXNNiwIs" rel="noopener ugc nofollow" target="_blank">微调</a>(进一步训练)像 GPT-2 这样的预训练模型，以适应该数据集的性能。为了微调和使用 GPT-2 预训练模型，我们将使用<a class="ae ky" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu">拥抱脸/变形金刚</strong> </a>库。它为我们做了所有繁重的工作。</p><p id="31ea" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">出于这个想法，我通过清理、转换和组合<a class="ae ky" href="https://www.kaggle.com/jrobischon/wikipedia-movie-plots" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu"> Kaggle </strong> <strong class="lf iu">维基百科电影情节数据集</strong> </a>以及从维基百科搜集的超级英雄漫画情节来创建数据集文件。</p><p id="8d5c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><a class="ae ky" href="https://drive.google.com/file/d/11FgexOt7PWxFnn9TbkMaEYVEo7gkHwkl/view" rel="noopener ugc nofollow" target="_blank">培训文件</a>有 3 万多个故事。文件中的每一行都是这种格式的故事:</p><p id="10df" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu"> &lt;博斯&gt; &lt;流派&gt;故事到此为止……&lt;EOS&gt;</strong></p><p id="5056" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu"/><strong class="lf iu">类型</strong>:超级英雄、科幻、恐怖、动作、剧情、惊悚</p><p id="4a46" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">要为另一项任务创建自己的数据集，如基于主题生成研究论文摘要，每个示例的格式可以如下所示:</p><p id="280c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu"> &lt; BOS &gt; &lt;科目&gt;摘要此处……&lt;EOS&gt;</strong></p><p id="a639" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">科目:物理、化学、计算机科学等。</p><p id="56c6" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我推荐你在<a class="ae ky" rel="noopener" target="_blank" href="/getting-started-with-google-colab-f2fff97f594c"> <strong class="lf iu"> Google Colab </strong> </a>上训练模型(设置运行时为<strong class="lf iu"> GPU </strong>)。用于微调的 colab 笔记本的<strong class="lf iu">链接我们的型号是这里的<a class="ae ky" href="https://colab.research.google.com/drive/1l8rqAB4KMzAIDhqRN_DzHaxukDsEhrTX?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu"/></a>。您可以创建此<a class="ae ky" href="https://colab.research.google.com/drive/1l8rqAB4KMzAIDhqRN_DzHaxukDsEhrTX?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu">笔记本</strong> </a> <strong class="lf iu"> </strong>的副本，并为您自己的数据集进行修改。</strong></p><p id="e856" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">这里有<a class="ae ky" href="https://drive.google.com/file/d/11FgexOt7PWxFnn9TbkMaEYVEo7gkHwkl/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu">6 _ genre _ clean _ training _ data . txt</strong></a>(训练文件)和<a class="ae ky" href="https://drive.google.com/file/d/1aaXwQ_hkOfSDhqqTsKTvlfIySi_0kX2x/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu">6 _ genre _ eval _ data . txt</strong></a><strong class="lf iu"/>(评估文件)，你需要在运行代码之前上传到你的 Colab 笔记本的环境中。</p><p id="e681" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">在这篇文章中，我将介绍 Colab 笔记本中的一些核心代码。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="06ad" class="ol mt it oh b gy om on l oo op">!pip install transformers torch</span></pre><p id="bf14" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">上面，我们正在安装核心库。我们需要<strong class="lf iu">变形金刚</strong>库来加载预先训练好的 GPT-2 检查点并对其进行微调。我们需要<strong class="lf iu">手电筒</strong>，因为它是用来训练的。</p><p id="f995" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">注意</strong>:我从这个变形金刚示例<a class="ae ky" href="https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py" rel="noopener ugc nofollow" target="_blank">文件</a>中取出代码，并对其进行了修改。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="ebab" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">在上面的代码片段中，我们指定了模型的参数(<strong class="lf iu">模型参数</strong>)、数据参数(<strong class="lf iu">数据训练参数</strong>)和训练参数(<strong class="lf iu">训练参数</strong>)。让我们快速浏览一下这些参数的关键论点。要正确检查所有参数，查看<a class="ae ky" href="https://github.com/huggingface/transformers/blob/3dcb748e31be8c7c9e4f62926c5c144c62d07218/examples/language-modeling/run_language_modeling.py#L55" rel="noopener ugc nofollow" target="_blank">这里的</a>。</p><h2 id="de92" class="ol mt it bd mu os ot dn my ou ov dp nc lz ow ox ne ma oy oz ng mb pa pb ni pc bi translated"><code class="fe pd pe pf oh b">ModelArguments</code></h2><ul class=""><li id="e9e8" class="nq nr it lf b lg nk lj nl lz pg ma ph mb pi ly nv nw nx ny bi translated"><strong class="lf iu">型号名称或路径:</strong>用于指定型号名称或其路径。然后，模型被下载到我们的缓存目录中。查看<a class="ae ky" href="https://huggingface.co/models?search=gpt2" rel="noopener ugc nofollow" target="_blank">此处</a>的所有可用型号。这里，我们将<strong class="lf iu">模型名称或路径</strong>指定为<strong class="lf iu"> gpt2 </strong>。我们还有其他选项，如 gpt2-medium 或 gpt2-xl。</li><li id="f100" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu"> model_type </strong>:我们指定想要一个 gpt2 模型。这与上面的参数不同，因为，我们只指定型号类型，不指定名称(名称指 gpt2-xl、gpt2-medium 等。).</li></ul><h2 id="b517" class="ol mt it bd mu os ot dn my ou ov dp nc lz ow ox ne ma oy oz ng mb pa pb ni pc bi translated">数据训练参数</h2><ul class=""><li id="2833" class="nq nr it lf b lg nk lj nl lz pg ma ph mb pi ly nv nw nx ny bi translated"><strong class="lf iu"> train_data_file </strong>:我们提供培训文件。</li><li id="fc11" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu"> eval_data_file </strong>:我们提供一个数据文件来评估模型(使用困惑)。困惑衡量我们的语言模型从<em class="le"> eval_data_file </em>生成文本的可能性。困惑度越低越好。</li><li id="822b" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu">逐行</strong>:设置为 true，因为我们文件中的每一行都是一个新的故事。</li><li id="7870" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu"> block_size </strong> : <a class="ae ky" href="https://github.com/huggingface/transformers/blob/3dcb748e31be8c7c9e4f62926c5c144c62d07218/examples/language-modeling/run_language_modeling.py#L119" rel="noopener ugc nofollow" target="_blank">将每个训练示例缩短为最多只有<em class="le"> block_size </em>个令牌</a>。</li></ul><h2 id="f22e" class="ol mt it bd mu os ot dn my ou ov dp nc lz ow ox ne ma oy oz ng mb pa pb ni pc bi translated"><strong class="ak">训练参数</strong></h2><p id="cf8e" class="pw-post-body-paragraph lc ld it lf b lg nk ju li lj nl jx ll lz nm lo lp ma nn ls lt mb no lw lx ly im bi translated">下面，<em class="le"> n </em>是指这些参数的值。</p><ul class=""><li id="28e3" class="nq nr it lf b lg lh lj lk lz ns ma nt mb nu ly nv nw nx ny bi translated"><strong class="lf iu"> output_dir </strong>:微调后的最终模型检查点保存在哪里。</li><li id="e090" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu"> do_train，do_eval </strong>:设置为 true，因为我们正在训练和评估。</li><li id="0897" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu"> logging_steps </strong>:每经过<em class="le"> n </em>个优化步骤，记录模型的损耗。</li><li id="b8ba" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu">per _ device _ train _ batch _ size</strong>:每个训练优化步骤涉及<em class="le"> n </em>个训练样本。</li><li id="523e" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu"> num_train_epochs </strong>:训练数据集的完整遍数。</li><li id="0527" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu"> save_steps </strong>:在每个<em class="le"> n </em>优化步骤后保存中间检查点(如果 Colab 在几个小时后保持断开，建议使用)。</li><li id="03a9" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu"> save_total_limit </strong>:任意点存储的中间检查点数。</li></ul><p id="6969" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在，是时候加载模型、它的配置和标记器了。</p><h2 id="cd88" class="ol mt it bd mu os ot dn my ou ov dp nc lz ow ox ne ma oy oz ng mb pa pb ni pc bi translated">正在加载模型、标记器和配置</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="18b7" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">类<strong class="lf iu"> AutoConfig </strong>、<strong class="lf iu"> AutoTokenizer </strong>、<strong class="lf iu"> GPT2LMHeadModel </strong>根据 model_name 加载各自的配置(<strong class="lf iu"> GPT2Config </strong>)、标记器(<strong class="lf iu"> GPT2Tokenizer </strong>)和模型(<strong class="lf iu"> GPT2LMHeadModel </strong>)。</p><p id="04f2" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu"> GPT2Tokenizer </strong>对象<strong class="lf iu"> </strong>对文本进行标记(将文本转换成标记列表)并将这些标记编码成数字。注意，令牌可以是单词，甚至是子单词(GPT-2 使用<a class="ae ky" href="https://huggingface.co/transformers/_modules/transformers/tokenization_gpt2.html" rel="noopener ugc nofollow" target="_blank">字节对编码</a>来创建令牌)。下面是一个标记化的例子(没有编码)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="fe5d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">在此之后，我们必须将代币编码成数字，因为计算机只处理数字。</p><p id="2cd3" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们有<strong class="lf iu"> GPT2Config </strong>对象用于根据型号名称加载<strong class="lf iu"> GPT-2 型号</strong>的配置。</p><p id="a114" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">最后，我们有<a class="ae ky" href="https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu">GPT 2 lmheadmodel</strong></a><strong class="lf iu"/>对象，它根据模型名和 GPT2Config 加载模型。</p><p id="6e20" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">有些人可能想知道:到底什么是"<strong class="lf iu"> LMHeadModel </strong>？简单地说，对于文本生成，我们需要一个语言模型(一个为词汇中的标记分配概率分布的模型)。GPT2LMHeadModel 是一个语言模型(它为词汇表中的每个标记分配分数)。所以，我们可以用这个模型来生成文本。</p><p id="9ad6" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">下一个主要步骤是添加特殊的令牌来将模型与我们的数据集成在一起。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="59e0" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">数据集有一些特殊的标记，我们需要向模型指定这些标记是什么。我们可以传入一个<strong class="lf iu"> special_tokens_dict </strong>，它可以有一些类似“bos_token”的键。要查看哪些键是允许的，请访问此<a class="ae ky" href="https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.SpecialTokensMixin" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><ul class=""><li id="bec4" class="nq nr it lf b lg lh lj lk lz ns ma nt mb nu ly nv nw nx ny bi translated"><strong class="lf iu"> bos_token </strong> (" &lt; BOS &gt;")是出现在每个故事开头的令牌。</li><li id="95d1" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu">EOS _ token</strong>(“&lt;EOS&gt;”)是出现在每个故事结尾的 token。</li><li id="d4af" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu">PAD _ token</strong>(“&lt;PAD&gt;”)是指将较短的输出填充为固定长度的填充令牌。</li></ul><p id="936d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">然而，我们有一些<strong class="lf iu">额外的特殊记号</strong>(在本例中是我们的<strong class="lf iu">流派记号</strong>)在我们的 special_tokens_dict 中没有它们自己的键。这些额外令牌可以作为列表放在一个集合密钥“附加 _ 特殊 _ 令牌”下。</p><p id="cff1" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">如果您正在使用自己的数据集，请用自己的类别替换流派标记。</p><p id="aa8b" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们已经为训练做好了准备，所以让我们创建<strong class="lf iu">训练者</strong>对象。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="5d2e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">训练器</strong>对象可用于训练和评估我们的模型。</p><p id="fa9e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">为了创建<strong class="lf iu">教练</strong>对象，我们指定了模型、数据集以及训练参数(如上所示)。<strong class="lf iu"> data_collator 参数</strong>用于批量训练和评估示例。</p><p id="466d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">然后，我们调用训练者的训练方法，并在训练后将模型保存到我们的输出目录中。是时候放松一下，让机器训练模型了。</p><p id="a075" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">训练完模型后，将模型检查点文件夹从 Colab 下载到您的计算机上。你可以将其部署为<a class="ae ky" href="https://huggingface.co/transformers/model_sharing.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu"> Huggingface 社区模型</strong> </a>，开发一个<a class="ae ky" href="https://medium.com/datadriveninvestor/streamlit-framework-every-data-scientist-must-know-7fa0ae775d6a" rel="noopener"> <strong class="lf iu"> web app </strong> </a>，甚至可以使用该模型开发一个<a class="ae ky" rel="noopener" target="_blank" href="/on-device-machine-learning-text-generation-on-android-6ad940c00911"> <strong class="lf iu">手机 app </strong> </a>。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="444a" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">第三幕:世代时间！</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/32a9f1d53875bd1c4baaae142d3653b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*z9MYS7YO7jfdCT_B"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">埃里克·维特索在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="5a4a" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">激动人心的部分来了！如果你渴望创作故事，请在你的 Colab 笔记本中运行下面的单元格！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="47fc" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">这里，我们使用了<a class="ae ky" href="https://huggingface.co/transformers/main_classes/pipelines.html#textgenerationpipeline" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu">TextGenerationPipeline</strong></a>对象，它简化了我们模型的文本生成，因为它抽象了输入预处理和输出后处理步骤。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="096c" class="ol mt it oh b gy om on l oo op">text_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)</span></pre><p id="53d0" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu"> text_generator </strong>是<strong class="lf iu"> TextGenerationPipeline </strong>类的一个对象。我们如何将这个对象作为一个函数来使用？这个类有一个<a class="ae ky" href="https://www.geeksforgeeks.org/__call__-in-python/" rel="noopener ugc nofollow" target="_blank"> __call__ method </a>，允许它的对象像函数一样工作(当使用对象作为函数时调用这个方法)。这里，我们像使用函数一样使用这个对象，通过提供 input_prompt 来生成文本。这就是全部了。</p><p id="d4c1" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们还没有解释一些文本生成参数，比如 top_p。如果你想了解这些参数，请跟在片尾字幕后面。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="79c2" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">尾声和片尾字幕</h1><p id="55d0" class="pw-post-body-paragraph lc ld it lf b lg nk ju li lj nl jx ll lz nm lo lp ma nn ls lt mb no lw lx ly im bi translated">概括地说，我们采用了各种类型的电影情节数据集，并将其馈送给<strong class="lf iu"> GPT2LMHeadModel </strong>来微调我们的模型，以生成特定类型的故事。使用这些想法，您还可以创建其他数据集来基于这些数据集生成文本。</p><p id="12dd" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">要测试我的电影故事生成器，请单击此处的<a class="ae ky" href="https://huggingface.co/pranavpsv/gpt2-genre-story-generator" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu"/></a><strong class="lf iu"/>(或使用 web app <a class="ae ky" href="http://52.91.134.248:8501/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu">此处的</strong> </a>)。</p><p id="ae4e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">非常感谢<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu"> Huggingface </strong> <strong class="lf iu">团队</strong> </a>提供的变形金刚库和详细的例子，以及 Raymond Cheng 撰写的这篇<a class="ae ky" rel="noopener" target="_blank" href="/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7"> <strong class="lf iu">文章</strong> </a> <strong class="lf iu">帮助我创建了我的模型。</strong></p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="d078" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">可选:探索文本生成参数</h1><p id="40bf" class="pw-post-body-paragraph lc ld it lf b lg nk ju li lj nl jx ll lz nm lo lp ma nn ls lt mb no lw lx ly im bi translated">首先，让我们了解一下<strong class="lf iu"> GPT2LMHeadModel </strong>的输出是什么:</p><p id="1612" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">当预测下一个标记时，该模型为其词汇表中所有可能的标记生成逻辑。为了简化起见，可以将这些逻辑看作每个标记的分数。具有较高分数(logits)的记号意味着它们有较高的概率成为合适的下一个记号。然后，我们对所有令牌的 logits 应用 softmax 操作。我们现在得到每个令牌的 softmax 分数，该分数在 0 和 1 之间。所有令牌的 softmax 分数总和为 1。这些 softmax 分数可以被认为是在给定一些先前文本的情况下，某个标记成为合适的下一个标记的概率(尽管它们不是)。</p><p id="d038" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">以下是参数的基本大纲:</p><ul class=""><li id="98a7" class="nq nr it lf b lg lh lj lk lz ns ma nt mb nu ly nv nw nx ny bi translated"><strong class="lf iu"> max_length </strong> [int]:指定要生成的令牌的最大数量。</li><li id="49d8" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu">do _ sample</strong>【bool】:指定是否使用<a class="ae ky" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">采样</a>(比如 top_p 或者使用贪婪搜索)。贪婪搜索是在每个时间步选择最可能的单词(不推荐，因为文本会重复)。</li><li id="8f2e" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu">repetition _ penalty</strong>【float】:指定对重复的惩罚。增加<strong class="lf iu">重复 _ 惩罚</strong>参数以减少重复。</li><li id="6f10" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu">温度</strong>【浮动】:用于增加或减少对高逻辑令牌的依赖。增加温度值会减少只有少数令牌具有非常高的 softmax 分数的机会。</li><li id="2110" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu">top _ p</strong>【float】:指定仅考虑概率(形式上，softmax)得分之和不超过<strong class="lf iu"> top_p </strong>值的令牌</li><li id="48dc" class="nq nr it lf b lg nz lj oa lz ob ma oc mb od ly nv nw nx ny bi translated"><strong class="lf iu">top_k</strong>【int】:告诉我们在生成文本时只考虑 top _ k 个标记(按它们的 softmax 分数排序)。</li></ul><p id="beee" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">仅此而已。</p></div></div>    
</body>
</html>