<html>
<head>
<title>Model-Agnostic Methods for Interpreting any Machine Learning Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释任何机器学习模型的模型不可知方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504?source=collection_archive---------7-----------------------#2020-01-04">https://towardsdatascience.com/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504?source=collection_archive---------7-----------------------#2020-01-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c277" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解释方法概述:排列特征重要性，部分依赖图，石灰，SHAP等。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/659a5524b6f55299cd5cd41b5a8be523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*88xudeT-tL0kksY2"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Lukasz Szmigiel 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="31d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">越来越多的公司正在使用复杂的机器学习模型，比如神经网络和梯度推进机器。他们使用复杂模型的原因是因为它们优于传统模型，如决策树或逻辑回归。使用复杂模型的负面影响是你不能直接解释那些模型。你不想要一个有偏见的模型，或者一个根据陌生或不相关的知识做出选择的模型。过去的经验，如亚马逊或教师的经历，表明了解释复杂模型的重要性。了解模型做出决策的原因有积极的副作用，例如，您可以了解模型发现的新模式，并了解更多关于您的数据的信息。</strong></p><p id="0318" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于机器学习模型的可解释性已经做了很多研究。有不同的方法来解释机器学习模型。最容易的划分是在可解释模型和模型不可知方法之间。可解释的模型是解释它们自己的模型，例如从决策树中你可以很容易地提取决策规则。模型不可知方法是可以用于任何机器学习模型的方法，从支持向量机到神经网络。在本文中，重点将放在与模型无关的方法上。还有另一篇关于可解释模型的文章。</p><h2 id="3da1" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">资料组</h2><p id="395a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">您可以使用模型可解释性的一个领域是医疗保健。为了了解模型如何决定一个人是否患有心脏病，我们使用了克利夫兰数据库中具有以下特征的数据集:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/9a8af1703dbbf22e6806949e547e1c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2ipbSShp9W9q0PcNAMFEA.png"/></div></div></figure><p id="c34e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将尝试用随机森林来预测目标，并用模型不可知的方法来解释这个模型。你可以在Kaggle 上找到<a class="ae ky" href="https://www.kaggle.com/ronitf/heart-disease-uci" rel="noopener ugc nofollow" target="_blank">心脏病UCI数据集。</a></p><h2 id="7dec" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">密码</h2><p id="92f5" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这篇关于模型不可知方法的文章(以及关于可解释模型的文章)的代码可以在<a class="ae ky" href="https://github.com/henniedeharder/interpretability-heart/blob/master/Demo_Forest_SHAP_Heart.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="147f" class="nb lw it bd lx nc nd ne ma nf ng nh md jz ni ka mg kc nj kd mj kf nk kg mm nl bi translated">模型不可知的方法</h1><p id="fd16" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">不幸的是，直接解释大多数机器学习模型是不可能的。对于像随机森林、梯度增强机器和神经网络这样的流行模型，你需要与模型无关的方法。目前有一些有趣的方法可用，如排列特征重要性，部分依赖图(PDP)，个体条件期望(ICE)图，全局代理模型，局部可解释模型不可知解释(LIME)和Shapley加法解释(SHAP)。我们将深入研究这些方法，并讨论它们的优缺点。</p><h2 id="d5b2" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">置换特征重要性</h2><p id="0b32" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">你使用scikit-learn⁴的特征重要性吗？这些特征的重要性基于标准的平均减少，如基尼系数(对于决策树和随机森林)。最好使用排列特征重要性。使用这种方法，重要度是基于在置换要素值时测量预测误差的增加。因此，您需要计算两次预测误差，在特征置换之前和之后。预测误差之间的差异越大，该特征越重要。</p><p id="1feb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们将比较随机森林的scikit-learn特征重要性和置换特征重要性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/94fa2b912d0db9f430a6aefa6b3e3427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WOUww6k4aFgTmT9c6749tw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">sci kit-了解随机森林的要素重要性</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/147a77197535a345015e8462416bec3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*3GrMEV3RBcwk650Y3--i9A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机森林的置换特征重要性</p></figure><p id="aeef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">哇！如果您将scikit-learn特征重要性中的特征顺序与置换特征重要性进行比较，一切都会被打乱！根据最后一张图片，我们应该尝试排除chol和exang，因为当这些特征被置换时，模型执行得更好！对于特征fbs、trestbps和age，什么都不会发生(如果我们忽略方差)。</p><h2 id="77db" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><em class="no">部分相关图(PDP)</em></h2><p id="f882" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这些图有助于可视化预测目标和一个或多个特征之间的平均部分关系。通过强制所有实例具有相同的特征值来创建图。然后对这些实例进行预测，并对它们进行平均，这就给出了该特征值的平均预测值。由于可视化，大多数时候只研究一两个特征。</p><p id="a1ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了对PDP有一个概念，下面你可以在心脏病数据集中看到PDP的例子。前两个图像在x轴上有一个特征，在y轴上有心脏病的概率。在第三幅图中，您可以看到两个特征(一个在x轴上，一个在y轴上)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/7aa549ae15d97a7e3bc193ee8b70d897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CWAIWWfLw4V3cypmTA8z_g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一般来说，如果心率加快，患心脏病的可能性就会增加。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/665e54ef096a430fdfcc2d4bfac2dde2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RVRqi79Co1bbCi9q_EgaqA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">胸痛类型是一个分类变量。如果这个值等于1、2或3，那么当这个值等于0时，你患心脏病的可能性更大。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/de6eb27452f8995a33d159fa64f73b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*biLQgixivdC_z0yU8o8IVQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">该图显示了oldpeak和cp与平均预测值的关系。当胸痛类型等于1、2或3且oldpeak具有低值时，患心脏病的概率(&gt; 0.63)比cp等于0且oldpeak具有高值(&lt; 0.37).</p></figure><p id="b05e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">In PDPs, you force all the instances to have the same feature value. The plots can be misleading if you only have a small amount of instances who have a certain feature value. It’s better to include data distributions in your plot, so you can see if the data is equally distributed.</p><p id="4768" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Watch out with PDPs! It is assumed that features for which you compute the partial dependence are independent. So they shouldn’t be correlated with other features. You can also easily miss complexity of the model, because the predictions are averaged.</p><h2 id="568a" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">Individual Conditional Expectation (ICE)</h2><p id="4afc" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">A way to deal with the problem of missing complexity in the model with PDPs is to show them in combination with ICE plots⁵. ICE plots are more detailed and show how the prediction of each instance changes when a feature value is changed. In the following images, every blue line represents one instance.</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/29b124cd7b265abc6ccb20bbdbe42bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pE5vhCVb0yXFu_rREPy6OQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Combined PDP and ICE plot for the ‘sex’ variable.</p></figure><p id="8c5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">The ICE plot for the sex (female = 0, male =1) variable is shown above. The average is the thick line in the middle (same line as in the PDPs). You see that for some instances the prediction changes a lot when sex is changed to male, but for some instances the prediction almost stays the same, although it always has a negative effect to be female. On the bottom of the image you see the data distribution.</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/1949fef5d1b973d0b762444a7f6af338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gzGfNlCa5EQezpQeZYrM9A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Combined PDP and ICE plot for the ‘chol’ variable.</p></figure><p id="89bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">This is interesting! The cholesterol variable shows that the pattern is more complicated than you would expect from the PDP, because the instances are spread out all over the place and often they don’t follow the pattern of the thick line. Sometimes a higher cholesterol has a (small) positive effect and sometimes the effect is negative. There are not that many instances that have a cholesterol value above 400 (check out the distribution on the bottom) so we should be careful here!</p><p id="a59e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">With ICE plots we solved the problem of PDPs by showing more complexity, but what about the independent features problem? That problem isn’t solved with ICE plots, because the feature you plot still needs to be uncorrelated with the other features.</p><h2 id="a22f" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">Global Surrogate Models</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/a2accdb1eea455755ed6cba636b55ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*X2k8ms1L4xixRyCp_O-EOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Workflow for a global surrogate model.</p></figure><p id="2406" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Global surrogates are really easy to understand, that’s an advantage of this method. First you build a black box model on the training data with the real labels. Then you let the model predict the labels for the same data and you build an interpretable model on the data with the <em class="nv">预测</em>标记)时高得多。因为代理模型是可解释的，并且建立在黑盒模型的预测之上，所以您将了解黑盒模型是如何做出预测的。</p><p id="ccef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法很好，因为它很直观。有一些缺点。可解释模型的性能会比黑盒模型差(否则你应该替换黑盒模型)。您需要决定对于可解释模型的性能，什么是可接受的度量。除此之外，可解释模型得出关于黑盒模型的结论，而不是关于数据的结论。</p><h2 id="79d1" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">局部可解释的模型不可知解释(LIME)</h2><p id="26a1" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">当你想解释一个单独的预测时，你可以用石灰。用LIME训练一个局部代理模型。这个可解释的替代模型可以用来解释个体预测。您不仅可以在表格数据上使用石灰，还可以在图像或text⁶.上使用石灰</p><p id="8e55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图直观地展示了LIME的工作方式。红色加号和蓝色圆点是来自不同班级的样本。粉色和蓝色区域之间的边界是黑盒模型的决策边界。如果您想解释下图中的大红色加号，您可以创建其他实例，这些实例与可解释模型的局部决策边界(虚线)非常接近。这个局部决策边界比粉红色和蓝色区域之间的边界(黑盒模型的决策边界)更容易解释。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/849ec7be8c85dd8019931777222a58c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/0*sBN37fWxkE8m_gwr"/></div></figure><p id="3d12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从测试集中取一个新记录:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/11f8d8b72902a4a2e3d40a0175837e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yhTjCUDu7GLXRLaeMGUVBg.png"/></div></div></figure><p id="11e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而现在，让我们用石灰来解释这个记录的预言:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/0760d721acbde284dbde6ab0bb134145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ikmp3SzyorVCRXECDlccTg.png"/></div></div></figure><p id="101a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">千钧一发！我们看到，对于真(心脏病= 1)的预测概率略高于假(心脏病= 0)的预测概率。在右侧，我们可以看到哪些特征对预测贡献最大。</p><p id="7325" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像其他解释方法一样，你需要小心使用石灰。同一个记录解释两次，解释可以不一样！另一个缺点是你只能解释一个实例，所以不可能解释整个黑盒模型。</p><h2 id="95d9" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><em class="no">沙普利加法解释(SHAP) </em></h2><p id="b48e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">如果您想要一种非常好的方式来显示特征值对预测的贡献，您应该使用SHAP。对于这种方法，沙普利值(来自博弈论)和石灰是combined⁷.简而言之，shapley值使用联合来查看某个特征值对最终预测的贡献。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/c5a95856abb230af2b5f6a21b22e7e14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dGWF_9D9AQMee1YkET7Dmg.png"/></div></div></figure><p id="e1b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">被调查的记录和我们用于石灰的记录是一样的。预测概率等于0.53。该值略低于基准值0.5486。红色特征，如cp和oldpeak增加了患心脏病的可能性，而ca和thal降低了患心脏病的可能性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/7d2fb1165bcbee3fde9dbcce40f98fec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZ-Qs1CLz5oQKjG95NbOZQ.png"/></div></div></figure><p id="fee4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们将测试集中的所有样本旋转90度，将它们堆叠并按相似性排序，我们会得到上面的图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/8cc2c926a3bcddf366cd5dac624256b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5JSJ7i8CGNRxLKZMJNcgA.png"/></div></div></figure><p id="72b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的汇总图显示了高或低特征值的不同SHAP值。如果您查看ca特征，您会发现当该特征的值较低时，SHAP值较高，这意味着患心脏病的可能性较高。该图还向我们展示了最重要的特征到最不重要的特征(从上到下)。</p><p id="6d58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SHAP值有一些优势，因为我们可以用它们来进行局部和全局的解释。而且他们有很强的理论基础。开始时，它们不能处理相关的特性，但是研究表明，它们可以和相关的features⁸.一起使用SHAP值的一个问题是计算速度，当您拥有许多要素时，计算时间将显著增加。</p><p id="8488" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望您可以使用这些方法来调查您的数据和模型！</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="cec0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] J. Dastin，<a class="ae ky" href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G" rel="noopener ugc nofollow" target="_blank">亚马逊废除了对女性有偏见的秘密人工智能招聘工具</a> (2018)，路透社</p><p id="073b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]c·奥尼尔，<a class="ae ky" href="https://we.riseup.net/assets/404114/Weapons+of+Math+Destruction+Cathy+O%27Neil.pdf" rel="noopener ugc nofollow" target="_blank">数学毁灭的武器</a> (2016)，新冠</p><p id="f946" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] A. Altmann和L. Toloş，<a class="ae ky" href="https://academic.oup.com/bioinformatics/article/26/10/1340/193348" rel="noopener ugc nofollow" target="_blank">排列重要性:一个修正的特征重要性度量</a> (2010)，生物信息学</p><p id="58a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] T. Parr，K. Turgutlu，C. Csiszar和J. Howard，<a class="ae ky" href="https://explained.ai/rf-importance/index.html" rel="noopener ugc nofollow" target="_blank">当心默认随机森林重要性</a> (2018)，解释. ai</p><p id="1994" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] A. Goldstein，A. Kapelner，J. Bleich和E. Pitkin，<a class="ae ky" href="https://arxiv.org/pdf/1309.6392.pdf" rel="noopener ugc nofollow" target="_blank">窥视黑盒内部:用个体条件期望图可视化统计学习</a> (2014)，《计算和图形统计杂志》</p><p id="725c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] M. T .里贝罗，s .辛格和C. Guestrin，<a class="ae ky" href="https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf" rel="noopener ugc nofollow" target="_blank">“我为什么要相信你？”解释任何分类器的预测</a> (2016)，ResearchGate</p><p id="5cc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7] S. Lundberg和S. Lee，<a class="ae ky" href="https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf" rel="noopener ugc nofollow" target="_blank">解释模型预测的统一方法</a> (2017)，NIPS</p><p id="0b25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[8] K. Aas和M. Jullum，<a class="ae ky" href="https://arxiv.org/pdf/1903.10464.pdf" rel="noopener ugc nofollow" target="_blank">解释特征相关时的个体预测:更精确地逼近Shapley值</a> (2019)，ArXiv</p></div></div>    
</body>
</html>