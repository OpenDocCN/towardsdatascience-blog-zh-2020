<html>
<head>
<title>Decision Tree Classifier and Cost Computation Pruning using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 实现决策树分类器和成本计算剪枝</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-classifier-and-cost-computation-pruning-using-python-b93a0985ea77?source=collection_archive---------9-----------------------#2020-07-16">https://towardsdatascience.com/decision-tree-classifier-and-cost-computation-pruning-using-python-b93a0985ea77?source=collection_archive---------9-----------------------#2020-07-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="89b3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Python 中的成本计算修剪构建、可视化和微调决策树的完整实践指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/62d3f20a3b58375d9c503e26bc6e4d23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aON3WDxFgTz1X5A4"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@leliejens?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">延斯·勒列</a>拍摄</p></figure><h1 id="540e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="8bc0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">决策树分类器是监督学习模型，在我们关心可解释性时非常有用。想象一下，通过在每一层基于多个问题做出决策来分解数据。这是广泛使用的处理分类问题的算法之一。为了更好地理解它，让我们看看下面的例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/465b034d7bc37dbd7816482bb765bc75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NWiIMXV04253xh2iwC5p7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于决定特定一天的活动的示例决策树。改编自《Python 对机器学习的更深刻见解》(Raschka，Julian and Hearty，2016，第 83、88、89 页)</p></figure><p id="42da" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">决策树通常包括:</p><ul class=""><li id="1e51" class="mt mu it lt b lu mo lx mp ma mv me mw mi mx mm my mz na nb bi translated"><strong class="lt iu">根节点</strong> —代表被进一步分成同类组的样本或总体</li><li id="5e9e" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><strong class="lt iu">拆分</strong>——将节点分成两个子节点的过程</li><li id="dbc4" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><strong class="lt iu">决策节点</strong> —当一个子节点根据某种条件分裂成更多的子节点时，称为决策节点</li><li id="1e78" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><strong class="lt iu">叶节点或终端节点</strong> —不再进一步分裂的子节点</li><li id="1a9e" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><strong class="lt iu">信息增益</strong> —为了使用条件(比如最具信息性的特征)来分割节点，我们需要定义一个可以优化的目标函数。在决策树算法中，我们倾向于最大化每次分裂的信息增益。在测量信息增益时，通常使用三种杂质测量。它们是<strong class="lt iu">基尼</strong>杂质、<strong class="lt iu">熵、</strong>分类和误差</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/dea7dfbe60db909ef16cf70261214df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YFD1iTIh2LONT3Tlyn_gZA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带有树叶和树枝的决策树示例。参考—由作者使用 Lucid Chart 开发</p></figure><div class="ni nj gp gr nk nl"><a rel="noopener follow" target="_blank" href="/are-your-coding-skills-good-enough-for-a-data-science-job-49af101457aa"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">对于数据科学的工作，你的编码技能够好吗？</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">5 编码嗅探如果你在数据科学行业工作，你必须知道</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz ks nl"/></div></div></a></div><h1 id="2942" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">理解数学</h1><p id="425b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了理解决策树是如何开发的，我们需要更深入地理解如何在每一步使用一个杂质度量在每一步最大化信息增益。让我们举一个例子，我们有训练数据，包括学生信息，如性别、年级、因变量或分类变量，用于识别学生是否是美食家。我们有以下概述的信息。</p><ol class=""><li id="8f7f" class="mt mu it lt b lu mo lx mp ma mv me mw mi mx mm oa mz na nb bi translated">学生总数— 20 人</li><li id="2a19" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">被归类为美食家的学生总数— 10</li><li id="dd30" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">不属于美食家的学生总数— 10 人</li><li id="8ba0" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">p(吃货)，一个学生成为吃货的概率= (10/20) = 0.5</li><li id="2e23" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">q(不是美食家，或 1-P)，学生不是美食家的概率= (10/20) = 0.5</li></ol><p id="96d1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">让我们将基于性别的学生分为两个节点，重新计算上述指标。</p><p id="caf2" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">男同学(节点 A) </strong></p><ol class=""><li id="0698" class="mt mu it lt b lu mo lx mp ma mv me mw mi mx mm oa mz na nb bi translated">学生总数— 10 人</li><li id="11dc" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">被归类为美食家的学生总数— 8</li><li id="507c" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">不属于美食家的学生总数— 2</li><li id="8967" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">p(吃货)，一个学生成为吃货的概率= (8/10) = 0.8</li><li id="09af" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">q(不是美食家，或 1-P)，学生不是美食家的概率= (2/10) = 0.2</li></ol><p id="e5a3" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">女学生(节点 B) </strong></p><ol class=""><li id="84a2" class="mt mu it lt b lu mo lx mp ma mv me mw mi mx mm oa mz na nb bi translated">学生总数— 10 人</li><li id="9698" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">被归类为美食家的学生总数— 4</li><li id="2013" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">不属于美食家的学生总数— 6</li><li id="7f8e" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">p(吃货)，一个学生成为吃货的概率= (4/10) = 0.4</li><li id="f9e7" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">q(非美食家，或 1-P)，学生非美食家的概率= (6/10) = 0.6</li></ol><p id="7a33" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">基尼指数(GIn) </strong>对于节点 A 或男生= P + Q，其中 P 和 Q 分别是一个学生成为美食家和非美食家的概率。GIn(节点 A) = 0.8 + 0.2 = 0.68</p><p id="4162" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">基尼系数(GIp) </strong>对于节点 A = 1-基尼系数= 1–0.68 = 0.32</p><p id="98ea" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu"> Gini Index (GIn) </strong>对于 Node B 或女学生= P + Q，其中 P 和 Q 分别是学生成为美食家和非美食家的概率。GIn(节点 B) = 0.4 + 0.6 = 0.52</p><p id="1276" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">基尼系数(GIp) </strong>对于节点 B = 1-基尼系数= 1–0.52 = 0.48</p><p id="b73d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们在上面观察到的是，当我们根据学生的性别(男性和女性)将他们分别分成节点 A 和 B 时，我们分别有两个节点的基尼不纯度分数。现在，为了决定性别是否是将学生分为美食家和非美食家的正确变量，我们需要一个加权基尼杂质分数，该分数是使用下面列出的公式计算的。</p><p id="ee22" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">加权基尼杂质=(节点 A 总样本数/数据集中总样本数)*基尼杂质(节点 A) +(节点 B 总样本数/数据集中总样本数)*基尼杂质(节点 B) </strong></p><p id="17bf" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">使用此公式计算上述示例的加权基尼系数，根据性别划分学生时的加权基尼系数= (10/20)*0.32 + (10/20)*0.48 = 0.4</p><p id="1732" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">分类问题涉及多个独立变量。变量可以是分类的，也可以是连续的。决策树非常适合处理不同数据类型的变量。决策树算法在决定每个节点的分裂时考虑了所有可能的变量。使用<strong class="lt iu"> <em class="ob">的变量可以获得最大的加权杂质增益，被用作特定节点</em> </strong>的决策变量。</p><p id="ab44" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在上面的例子中，使用“性别”作为决策变量的加权杂质增益是 0.4，然而，假设使用“等级”作为决策变量，我们设法实现 0.56 的加权杂质增益，算法将使用“等级”作为决策变量来创建第一个分离。所有后续步骤都遵循类似的方法，直到每个节点都是同类的。</p><h1 id="1ec3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">关于决策树算法的快速事实</h1><ol class=""><li id="f347" class="mt mu it lt b lu lv lx ly ma oc me od mi oe mm oa mz na nb bi translated">随着算法继续将节点分割成子节点，直到每个节点变得同质，决策树易于过度拟合</li><li id="eb71" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">与测试集相比，训练数据的准确性要高得多，因此应该修剪决策树以防止模型过度拟合。修剪可以通过控制树的深度、每个节点中样本的最大/最小数量、要分裂的节点的最小杂质增益以及最大叶子节点来实现</li><li id="5fee" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">Python 允许用户使用基尼杂质或熵作为信息增益标准来开发决策树</li><li id="dea7" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">可以使用网格搜索或随机搜索 CV 对决策树进行微调。CV 代表交叉验证</li></ol><h1 id="d871" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">三种不同杂质标准的目视比较</h1><p id="ee5b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">下面概述的代码片段提供了不同杂质标准的直观比较，以及它们如何随着不同的概率值而变化。注意下面的代码改编自 2016 年 S.Raschka，D.Julian 和 J . heartful 的《Python:对机器学习的更深刻见解》。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="0d6c" class="ok la it og b gy ol om l on oo">import matplotlib.pyplot as plt</span><span id="0e30" class="ok la it og b gy op om l on oo">import numpy as np</span><span id="f09c" class="ok la it og b gy op om l on oo">#-----Calculating Gini Index<br/>def gini(p):<br/>    return (p)*(1 - (p)) + (1 - p)*(1 - (1-p))</span><span id="60da" class="ok la it og b gy op om l on oo">#-----Calculating Entropy<br/>def entropy(p):<br/>    return - p*np.log2(p) - (1 - p)*np.log2((1 - p))</span><span id="839c" class="ok la it og b gy op om l on oo">#-----Calculating Classification Error<br/>def classification_error(p):<br/>    return 1 - np.max([p, 1 - p])</span><span id="9e78" class="ok la it og b gy op om l on oo">#----Creating a Numpy Array of probability values from 0 to 1, with an increment of 0.01<br/>x = np.arange(0.0, 1.0, 0.01)</span><span id="84b9" class="ok la it og b gy op om l on oo">#---Obtaining Entropy for different values of p<br/>ent = [entropy(p) if p != 0 else None for p in x]</span><span id="a3cc" class="ok la it og b gy op om l on oo">#---Obtaining scaled entropy<br/>sc_ent = [e*0.5 if e else None for e in ent]</span><span id="558b" class="ok la it og b gy op om l on oo">#--Classification Error<br/>err = [classification_error(i) for i in x]</span><span id="0e6d" class="ok la it og b gy op om l on oo">#--Plotting</span><span id="b3cb" class="ok la it og b gy op om l on oo">fig = plt.figure();<br/>plt.figure(figsize=(10,8));<br/>ax = plt.subplot(111);</span><span id="eb13" class="ok la it og b gy op om l on oo">for i, lab, ls, c, in zip([ent, sc_ent, gini(x), err], ['Entropy', 'Entropy (scaled)','Gini Impurity',<br/>                                                        'Misclassification Error'],['-', '-', '--', '-.'],<br/>                          ['black', 'darkgray','blue', 'brown', 'cyan']):<br/>    line = ax.plot(x, i, label=lab,<br/>    linestyle=ls, lw=2, color=c)<br/>    <br/>ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3, fancybox=True, shadow=False)<br/>ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')<br/>ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')<br/>plt.ylim([0, 1.1])<br/>plt.xlabel('p(i=1)')<br/>plt.ylabel('Impurity Index')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/57e9e436d3f2275e6884ff73c886c10c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*9UNgb3BnnTBtJPxkyuoLCw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同概率值的杂质变化。参考—上述代码片段的输出</p></figure><h1 id="3e4f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">动手练习</h1><p id="fd94" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">问题陈述旨在开发一个分类模型来预测红酒的质量。关于问题陈述的细节可以在<a class="ae ky" href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009" rel="noopener ugc nofollow" target="_blank">这里</a>找到。这是一个多类分类问题的经典例子。请注意，所有机器学习模型对异常值都很敏感，因此在构建树之前，应该处理由异常值组成的特征/独立变量。</p><p id="f7c5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">不同特征/独立变量的一个重要方面是它们如何相互作用。皮尔逊相关可用于确定数据集中两个特征之间的关联程度。然而，对于像决策树这样基于决策的算法，我们不会丢弃高度相关的变量。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="d306" class="ok la it og b gy ol om l on oo">#---------------------------------------------Importing Required Libraries-----------------------------------<br/>%matplotlib inline</span><span id="bdfa" class="ok la it og b gy op om l on oo">import numpy as np<br/>import pandas as pd</span><span id="9bc2" class="ok la it og b gy op om l on oo">from sklearn.tree import DecisionTreeClassifier</span><span id="0906" class="ok la it og b gy op om l on oo">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns</span><span id="73f0" class="ok la it og b gy op om l on oo">sns.set(color_codes=True)</span><span id="7702" class="ok la it og b gy op om l on oo">from matplotlib import pyplot as plt<br/>from sklearn.model_selection import train_test_split #--------------splitting data into test and train<br/>from sklearn.tree import DecisionTreeClassifier #-----------Building decision tree model</span><span id="1dd0" class="ok la it og b gy op om l on oo">from sklearn import metrics<br/>from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix #-----model validation scores<br/>%matplotlib inline</span><span id="dba2" class="ok la it og b gy op om l on oo">from IPython.display import display #---------------------for displaying multiple data frames in one output</span><span id="ae63" class="ok la it og b gy op om l on oo">from sklearn.feature_extraction.text import CountVectorizer  #DT does not take strings as input for the model fit step</span><span id="9b59" class="ok la it og b gy op om l on oo">import missingno as msno_plot #--------------plotting missing values</span><span id="a3e4" class="ok la it og b gy op om l on oo">wine_df = pd.read_csv('winequality-red.csv',sep=';')</span></pre><p id="f87f" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">数据的快速描述性统计</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="68e5" class="ok la it og b gy ol om l on oo">wine_df.describe().transpose().round(2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/4660b0d599880d2cf5c6b2d05a67b497.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*XnZ05_ftQtS4_VcV6y0AJw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">的输出。describe()函数</p></figure><p id="050d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">检查缺失值</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="6a9b" class="ok la it og b gy ol om l on oo">#-------------------------------------------Barplot of non-missing values--------------------------------<br/>plt.title('#Non-missing Values by Columns')<br/>msno_plot.bar(wine_df);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/1ed91c9245a517247da80ca07b1857b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uywLCMKVxwUXG6mfVN6A1Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">说明数据集中非缺失值计数的图</p></figure><p id="72b0" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">异常值检查和处理</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="4ff9" class="ok la it og b gy ol om l on oo">#--Checking Outliers<br/>plt.figure(figsize=(15,15))<br/>pos = 1<br/>for i in wine_df.columns:<br/>    plt.subplot(3, 4, pos)<br/>    sns.boxplot(wine_df[i])<br/>    pos += 1</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/749117d96dd3b15984a5dea940d581d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PgLQFWGn3Ltiru6pyrgkQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">说明数据中存在异常值的箱线图</p></figure><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="07a6" class="ok la it og b gy ol om l on oo">col_names=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',<br/>       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',<br/>       'pH', 'sulphates', 'alcohol']</span><span id="bca7" class="ok la it og b gy op om l on oo">display(col_names)</span><span id="e745" class="ok la it og b gy op om l on oo">for i in col_names:<br/>    q1, q2, q3 = wine_df[i].quantile([0.25,0.5,0.75])<br/>    IQR = q3 - q1<br/>    lower_cap=q1-1.5*IQR<br/>    upper_cap=q3+1.5*IQR<br/>    wine_df[i]=wine_df[i].apply(lambda x: upper_cap if x&gt;(upper_cap) else (lower_cap if x&lt;(lower_cap) else x))</span></pre><p id="2ceb" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">上面的异常值使用 Q1-1.5 * IQR 和 Q3+1.5*IQR 值进行了 winsorized 化。Q1、Q3 和 IQR 分别代表四分位数 1、四分位数 3 和四分位数间范围。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="d0f0" class="ok la it og b gy ol om l on oo">sns.pairplot(wine_df);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/856ab2bfd5f12a90bae44faee843f105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YDi0lhS3FuGR1vjnhdzhyA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">说明不同变量相互作用的配对图</p></figure><p id="33e2" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">理解不同变量之间的关系。注意—在决策树中，我们不需要删除高度相关的变量，因为仅使用一个独立变量将节点划分为子节点，因此即使两个或更多变量高度相关，产生最高信息增益的变量也将用于分析。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="8430" class="ok la it og b gy ol om l on oo">plt.figure(figsize=(10,8))<br/>sns.heatmap(wine_df.corr(),<br/>            annot=True,<br/>            linewidths=.5,<br/>            center=0,<br/>            cbar=False,<br/>            cmap="YlGnBu")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/552b728e5452ead79d729d04df7b1209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*ZsdyMLkgc4FvFCM6mIpZRg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">说明不同属性相关性的热图</p></figure><p id="4b0c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">分类问题对阶级不平衡很敏感。类别不平衡是指相关属性中 1 的比例高于 0 的比例，反之亦然。在多类问题中，当其中一个类值的比例非常高时，就会出现类不平衡。类别平衡是通过组合属性“质量”的值来诱导的，该属性是本问题陈述中的因变量。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="3e34" class="ok la it og b gy ol om l on oo">plt.figure(figsize=(10,8))<br/>sns.countplot(wine_df['quality']);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/0bb9dcb1f13accb29f904531f7f01416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*Dd_vzcofCkcqK0DD-dr7Sg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">该图显示了不同葡萄酒质量的记录计数</p></figure><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="1455" class="ok la it og b gy ol om l on oo">wine_df['quality'] = wine_df['quality'].replace(8,7)<br/>wine_df['quality'] = wine_df['quality'].replace(3,5)<br/>wine_df['quality'] = wine_df['quality'].replace(4,5)<br/>wine_df['quality'].value_counts(normalize=True)</span></pre><p id="ad6b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">数据被分为训练集和测试集，以检查模型的准确性，并寻找过度拟合或欠拟合(如果有的话)。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="dd53" class="ok la it og b gy ol om l on oo"># splitting data into training and test set for independent attributes</span><span id="bf92" class="ok la it og b gy op om l on oo">from sklearn.model_selection import train_test_split</span><span id="115e" class="ok la it og b gy op om l on oo">X_train, X_test, y_train, y_test =train_test_split(wine_df.drop('quality',axis=1), wine_df['quality'], test_size=.3,<br/>                                                   random_state=22)<br/>X_train.shape,X_test.shape</span></pre><p id="4a0f" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">决策树模型是使用基尼标准开发的。注意，为了简单起见，我们将树修剪到最大深度为 3。这将有助于我们将树形象化，并将它与我们在最初部分中涉及的概念联系起来。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="d3dd" class="ok la it og b gy ol om l on oo">clf_pruned = DecisionTreeClassifier(criterion = "gini", random_state = 100,<br/>                               max_depth=3, min_samples_leaf=5)<br/>clf_pruned.fit(X_train, y_train)</span></pre><p id="a59f" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">请注意，可以调整以下参数来改善模型输出(Scikit Learn，2019)。</p><ol class=""><li id="e026" class="mt mu it lt b lu mo lx mp ma mv me mw mi mx mm oa mz na nb bi translated"><strong class="lt iu">标准</strong> —基尼系数用于决定变量，基于该变量，根节点和随后的决策节点应该被分割</li><li id="26e3" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated"><strong class="lt iu"> class_weight </strong> —无；所有类别都被赋予权重 1</li><li id="761c" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated"><strong class="lt iu">最大深度</strong>—3；修剪完成。当“无”时，表示节点将被扩展，直到所有叶子都是同类的</li><li id="ae2b" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated"><strong class="lt iu"> max_features </strong> —无；在决定一个节点的分裂时，考虑所有的特征或独立变量</li><li id="f8c7" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated"><strong class="lt iu"> max_leaf_nodes </strong> —无；</li><li id="8c0b" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated"><strong class="lt iu">最小杂质减少量</strong>—0.0；仅当分裂确保杂质减少大于或等于零时，才分裂节点</li><li id="0c29" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated"><strong class="lt iu">最小 _ 杂质 _ 分裂</strong> —无；</li><li id="bc53" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated"><strong class="lt iu">min _ samples _ leaf</strong>—1；叶子存在所需的最小样本数</li><li id="fd6d" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated"><strong class="lt iu">min _ samples _ split</strong>—2；如果 min_samples_leaf =1，则表示右节点和左节点应该各有 1 个样本，即父节点或根节点应该至少有两个样本</li><li id="7774" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated"><strong class="lt iu">分割器</strong>——‘最佳’；用于在每个节点选择分割的策略。最好确保在决定分割时考虑所有特征</li></ol><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="18f3" class="ok la it og b gy ol om l on oo">from sklearn.tree import export_graphviz<br/>from sklearn.externals.six import StringIO  <br/>from IPython.display import Image  <br/>import pydotplus<br/>import graphviz</span><span id="9cd0" class="ok la it og b gy op om l on oo">xvar = wine_df.drop('quality', axis=1)<br/>feature_cols = xvar.columns</span><span id="109f" class="ok la it og b gy op om l on oo">dot_data = StringIO()<br/>export_graphviz(clf_pruned, out_file=dot_data,  <br/>                filled=True, rounded=True,<br/>                special_characters=True,feature_names = feature_cols,class_names=['0','1','2'])</span><span id="8b7b" class="ok la it og b gy op om l on oo">from pydot import graph_from_dot_data<br/>(graph, ) = graph_from_dot_data(dot_data.getvalue())<br/>Image(graph.create_png())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/974a22a6ac67258c293b9db4c597f375.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bfIT0ygXnSzseE_AntIqg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图示为深度=3 时的决策树模型</p></figure><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="51c0" class="ok la it og b gy ol om l on oo">preds_pruned = clf_pruned.predict(X_test)<br/>preds_pruned_train = clf_pruned.predict(X_train)</span><span id="0781" class="ok la it og b gy op om l on oo">print(accuracy_score(y_test,preds_pruned))<br/>print(accuracy_score(y_train,preds_pruned_train))</span></pre><p id="fdda" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">该模型对训练和测试数据的准确率分别为 0.60 和 0.62。</p><p id="6215" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">特征重要性指的是为预测模型的输入特征分配分数的一类技术，这些分数指示了在进行预测时每个特征的相对重要性。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="7039" class="ok la it og b gy ol om l on oo">## Calculating feature importance</span><span id="d09f" class="ok la it og b gy op om l on oo">feat_importance = clf_pruned.tree_.compute_feature_importances(normalize=False)</span><span id="3b36" class="ok la it og b gy op om l on oo">feat_imp_dict = dict(zip(feature_cols, clf_pruned.feature_importances_))<br/>feat_imp = pd.DataFrame.from_dict(feat_imp_dict, orient='index')<br/>feat_imp.rename(columns = {0:'FeatureImportance'}, inplace = True)<br/>feat_imp.sort_values(by=['FeatureImportance'], ascending=False).head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/c9924906846075154d5b076e7389fb9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*2A-Cjx8cSmYOwmf2_2hsuA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">影响决策树拆分的五大特征</p></figure><p id="96d4" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">DecisionTreeClassifier()提供诸如 min_samples_leaf 和 max_depth 之类的参数来防止树过度拟合。可以把它想象成一个场景，我们明确定义了树的深度和最大叶子数。然而，最大的挑战是确定一棵树应该包含的最佳深度和叶子。在上面的例子中，我们使用 max_depth=3，min_samples_leaf=5。这些数字只是一个例子，用来观察树的行为。但是，如果在现实中我们被要求处理这个模型并得出模型参数的最佳值，这是具有挑战性的，但不是不可能的(决策树模型可以使用 GridSearchCV 算法进行微调)。</p><p id="e4a5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">另一种方法是使用成本复杂性修剪(CCP)。</p><p id="f693" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">成本复杂性修剪提供了另一种选择来控制树的大小。在决策树分类器中，这种修剪技术由成本复杂性参数 cost 来参数化。CCPα值越大，修剪的节点数越多。</p><p id="530c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">简单来说，成本复杂性是一个阈值。只有当模型的整体杂质被提高了大于该阈值的值时，模型才进一步将节点分裂成子节点，否则它停止。</p><p id="a01e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">CCP 越低，杂质越少。怎么会？</p><p id="eff8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">当 CCP 值较低时，即使杂质没有减少很多，模型也会将一个节点分割成子节点。随着树的深度增加，这一点很明显，也就是说，当我们沿着决策树往下走时，我们会发现分裂对模型的整体杂质的变化没有多大贡献。然而，较高的分割确保了类别被正确分类，即准确性更高。</p><p id="2629" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">当 CCP 值较低时，会创建更多的节点。节点越高，树的深度也越高。</p><p id="05e8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">下面的代码(Scikit Learn，n.d .)说明了如何调整 alpha 以获得具有改进的准确度分数的模型。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="e00c" class="ok la it og b gy ol om l on oo">path = model_gini.cost_complexity_pruning_path(X_train, y_train)<br/>ccp_alphas, impurities = path.ccp_alphas, path.impurities</span><span id="23f4" class="ok la it og b gy op om l on oo">fig, ax = plt.subplots(figsize=(16,8));<br/>ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post");<br/>ax.set_xlabel("effective alpha");<br/>ax.set_ylabel("total impurity of leaves");<br/>ax.set_title("Total Impurity vs effective alpha for training set");</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/9a65cdf70c73580f158a58f362cd4b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LbMrnXnpfOa2PzuMzfYi2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">显示杂质随α值变化的图像</p></figure><p id="33cf" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">让我们来理解深度和节点数随 alpha 的变化。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="43d3" class="ok la it og b gy ol om l on oo">clfs = clfs[:-1]</span><span id="5a15" class="ok la it og b gy op om l on oo">ccp_alphas = ccp_alphas[:-1]</span><span id="9003" class="ok la it og b gy op om l on oo">node_counts = [clf.tree_.node_count for clf in clfs]</span><span id="7d49" class="ok la it og b gy op om l on oo">depth = [clf.tree_.max_depth for clf in clfs]</span><span id="3036" class="ok la it og b gy op om l on oo">fig, ax = plt.subplots(2, 1,figsize=(16,8))</span><span id="950f" class="ok la it og b gy op om l on oo">ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")<br/>ax[0].set_xlabel("alpha")<br/>ax[0].set_ylabel("number of nodes")<br/>ax[0].set_title("Number of nodes vs alpha")<br/>ax[1].plot(ccp_alphas, depth, marker='o', drawstyle="steps-post")<br/>ax[1].set_xlabel("alpha")<br/>ax[1].set_ylabel("depth of tree")<br/>ax[1].set_title("Depth vs alpha")<br/>fig.tight_layout()<br/></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/b41886d5ba02221af84ac02d8ed4a20f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OaCjpwhHUfTh_1PcS6yuOA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">该图显示了深度和节点数随 alpha 值的变化</p></figure><p id="2ad8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">理解当α增加时精度的变化。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="2d16" class="ok la it og b gy ol om l on oo">fig, ax = plt.subplots(figsize=(16,8)); #-----------------Setting size of the canvas<br/>train_scores = [clf.score(X_train, y_train) for clf in clfs]<br/>test_scores = [clf.score(X_test, y_test) for clf in clfs]</span><span id="dbf4" class="ok la it og b gy op om l on oo">ax.set_xlabel("alpha")<br/>ax.set_ylabel("accuracy")<br/>ax.set_title("Accuracy vs alpha for training and testing sets")<br/>ax.plot(ccp_alphas, train_scores, marker='o', label="train",<br/>        drawstyle="steps-post")<br/>ax.plot(ccp_alphas, test_scores, marker='o', label="test",<br/>        drawstyle="steps-post")<br/>ax.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/9e78812dd2b59a46d5abae79056a7e86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2QTlPshuTt6oHwHflgWQLw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">该图显示了当 alpha 值增加时准确度分数的变化</p></figure><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="e3a3" class="ok la it og b gy ol om l on oo">i = np.arange(len(ccp_alphas))<br/>ccp = pd.DataFrame({'Depth': pd.Series(depth,index=i),'Node' : pd.Series(node_counts, index=i),\<br/>                    'ccp' : pd.Series(ccp_alphas, index = i),'train_scores' : pd.Series(train_scores, index = i),<br/>                   'test_scores' : pd.Series(test_scores, index = i)})</span><span id="8685" class="ok la it og b gy op om l on oo">ccp.tail()</span><span id="5631" class="ok la it og b gy op om l on oo">ccp[ccp['test_scores']==ccp['test_scores'].max()]</span></pre><p id="950c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">上面的代码提供了在测试数据中产生最高精确度的成本计算修剪值。</p><h1 id="2a90" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><ol class=""><li id="fe3f" class="mt mu it lt b lu lv lx ly ma oc me od mi oe mm oa mz na nb bi translated">s .拉什卡、d .朱利安和 j .哈特(2016 年)。<em class="ob"> Python:深入了解机器学习:利用 Python 的机器学习技术优势:三个模块的课程</em>。英国伯明翰:Packt 出版公司，第 83、88、89 页。</li><li id="0d54" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">‌<a class="ae ky" href="http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html" rel="noopener ugc nofollow" target="_blank">sci kit-learn:python 中的机器学习</a>，Pedregosa <em class="ob">等人</em>，JMLR 12，第 2825–2830 页，2011 年。</li><li id="3906" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">Scikit Learn (2019)。<em class="ob">sk learn . tree . decision tree classifier—sci kit-learn 0 . 22 . 1 文档</em>。[在线]Scikit-learn.org。可从以下网址获取:<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html." rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . tree . decision tree classifier . html</a></li><li id="50c2" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm oa mz na nb bi translated">Scikit Learn(未注明)。<em class="ob">后剪枝决策树与成本复杂度剪枝</em>。【在线】可在:<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py." rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/tree/plot _ cost _ complexity _ pruning . html # sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py。</a></li></ol></div><div class="ab cl pb pc hx pd" role="separator"><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg"/></div><div class="im in io ip iq"><p id="4c9d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><em class="ob">关于作者:高级分析专家和管理顾问，帮助公司通过对组织数据的商业、技术和数学的组合找到各种问题的解决方案。一个数据科学爱好者，在这里分享、学习、贡献；你可以和我在</em> <a class="ae ky" href="https://www.linkedin.com/in/angel-das-9532bb12a/" rel="noopener ugc nofollow" target="_blank"> <em class="ob">上联系</em> </a> <em class="ob">和</em> <a class="ae ky" href="https://twitter.com/dasangel07_andy" rel="noopener ugc nofollow" target="_blank"> <em class="ob">上推特</em></a><em class="ob">；</em></p></div></div>    
</body>
</html>