<html>
<head>
<title>Finetune a Facial Recognition Classifier to Recognize your Face using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorch微调面部识别分类器来识别您的面部</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/finetune-a-facial-recognition-classifier-to-recognize-your-face-using-pytorch-d00a639d9a79?source=collection_archive---------12-----------------------#2020-04-11">https://towardsdatascience.com/finetune-a-facial-recognition-classifier-to-recognize-your-face-using-pytorch-d00a639d9a79?source=collection_archive---------12-----------------------#2020-04-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a181" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用GANs的对抗性攻击来欺骗面部识别系统。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dc69c85a9c3364bf1d351a7978180f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1tgzXPZViS5uPqPCQDi3Cw.jpeg"/></div></div></figure><p id="c2d0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是我正在写的一个系列的一部分，关于利用GANs的对抗性攻击来欺骗面部识别系统。</p><p id="219c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，在我们欺骗面部识别分类器之前，我们需要建立一个来欺骗。我个人想造一个能识别自己脸的。我可以从一个预先训练好的网络开始，然后微调它来识别我的脸，而不是从头开始训练神经网络。微调是非常有益的，因为我们可以从已经在大规模人脸数据库上训练的模型权重开始，然后更新其中的一些权重，以反映我们希望它执行的新任务。这些权重已经知道如何识别人脸，但唯一的区别是它不知道我的脸。因此，让这个预训练的模型学习我自己的脸要容易得多，因为模型权重已经包含了执行任务所需的大部分信息。</p><h1 id="6879" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">目录结构</h1><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="7c77" class="mn lr it mj b gy mo mp l mq mr">project<br/>│   README.md<br/>│   AGN.ipynb  <br/>│<br/>└───data<br/>│   │   files_sample.csv<br/>│   └───eyeglasses<br/>│   │<br/>│   └───test_me<br/>│       └───train<br/>|           └───Adrien_Brody<br/>|           ...<br/>|           └───Michael_Chaykowsky<br/>|           ...<br/>|           └───Venus_Williams<br/>│       └───val<br/>|           └───Adrien_Brody<br/>|           ...<br/>|           └───Michael_Chaykowsky<br/>|           ...<br/>|           └───Venus_Williams<br/>│   <br/>└───models<br/>│   │   inception_resnet_v1.py<br/>│   │   mtcnn.py<br/>│   └───utils</span></pre><p id="4640" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ms mt mu mj b">models</code>目录来自基于上面链接的Tensorflow实现的<a class="ae mv" href="https://github.com/timesler/facenet-pytorch/tree/master/models" rel="noopener ugc nofollow" target="_blank"> PyTorch facenet实现</a>。</p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="b41b" class="mn lr it mj b gy mo mp l mq mr">└───models<br/>│   │   inception_resnet_v1.py<br/>│   │   mtcnn.py<br/>│   └───utils</span></pre><p id="4842" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个<code class="fe ms mt mu mj b">inception_resnet_v1.py</code>文件是我们将引入预训练模型的地方。Inception Resnet V1模型在<a class="ae mv" href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/" rel="noopener ugc nofollow" target="_blank"> VGGFace2 </a>上进行了预训练，其中VGGFace2是从谷歌图像搜索中开发的大规模人脸识别数据集，并且“在姿势、年龄、光照、种族和职业方面有很大的变化”</p><p id="88a3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">模型中每层的权重都有一个名为<code class="fe ms mt mu mj b">requires_grad</code>的属性，可以设置为<code class="fe ms mt mu mj b">True</code>或<code class="fe ms mt mu mj b">False</code>。当您在训练循环中运行<code class="fe ms mt mu mj b">loss.backward()</code>时，这些权重会更新，这包含了执行预测所需的所有信息。当微调网络时，我们<em class="mw">通过将<code class="fe ms mt mu mj b">requires_grad</code>属性设置为<code class="fe ms mt mu mj b">False</code>冻结</em>最后一个卷积块的所有层，然后只更新剩余层上的权重——直观地，您可以想象早期的层包含识别人脸属性和基本层特征所需的基本层信息，因此我们在更新最终层以包括另一张人脸(我的)的同时保持所有性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/297cee252bf850b651c4f4b738dc1db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iel-almpiiD3J7Dp8sWM-Q.png"/></div></div></figure><p id="2df1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所有的<code class="fe ms mt mu mj b">train</code>目录都有每个人的11或12个图像，所有的<code class="fe ms mt mu mj b">val</code>目录都有每个人的4或5个图像。<code class="fe ms mt mu mj b">Michael_Chaykowsky</code>是我的脸部目录，在那里我使用了各种姿势、灯光和角度。为了收集这些图像，我用标准的iPhone在不同的空间拍摄了视频，然后将这些视频转换为图像，并在每个视频上使用MTCNN进行面部对齐和裁剪到适当的大小(160 x 160像素)。</p><h1 id="0613" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">进口</h1><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="2bd2" class="mn lr it mj b gy mo mp l mq mr"><strong class="mj iu">from</strong> torch <strong class="mj iu">import</strong> nn, optim, as_tensor<br/><strong class="mj iu">from</strong> torch.utils.data <strong class="mj iu">import</strong> Dataset, DataLoader<br/><strong class="mj iu">import</strong> torch.nn.functional <strong class="mj iu">as</strong> F<br/><strong class="mj iu">from</strong> torch.optim <strong class="mj iu">import</strong> lr_scheduler<br/><strong class="mj iu">from</strong> torch.nn.init <strong class="mj iu">import</strong> *<br/><strong class="mj iu">from</strong> torchvision <strong class="mj iu">import</strong> transforms, utils, datasets, models<br/><strong class="mj iu">from</strong> models.inception_resnet_v1 <strong class="mj iu">import</strong> InceptionResnetV1</span><span id="733b" class="mn lr it mj b gy my mp l mq mr"><strong class="mj iu">import</strong> cv2<br/><strong class="mj iu">from</strong> PIL <strong class="mj iu">import</strong> Image<br/><strong class="mj iu">from</strong> pdb <strong class="mj iu">import</strong> set_trace<br/><strong class="mj iu">import</strong> time<br/><strong class="mj iu">import</strong> copy</span><span id="5671" class="mn lr it mj b gy my mp l mq mr"><strong class="mj iu">from</strong> pathlib <strong class="mj iu">import</strong> Path<br/><strong class="mj iu">import</strong> os<br/><strong class="mj iu">import</strong> sys<br/><strong class="mj iu">import</strong> matplotlib.pyplot <strong class="mj iu">as</strong> plt<br/><strong class="mj iu">import</strong> matplotlib.animation <strong class="mj iu">as</strong> animation<br/><strong class="mj iu">from</strong> skimage <strong class="mj iu">import</strong> io, transform<br/><strong class="mj iu">from</strong> tqdm <strong class="mj iu">import</strong> trange, tqdm<br/><strong class="mj iu">import</strong> csv<br/><strong class="mj iu">import</strong> glob<br/><strong class="mj iu">import</strong> dlib</span><span id="5323" class="mn lr it mj b gy my mp l mq mr"><strong class="mj iu">import</strong> pandas <strong class="mj iu">as</strong> pd<br/><strong class="mj iu">import</strong> numpy <strong class="mj iu">as</strong> np</span></pre><h1 id="f88d" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">用于人脸对齐的多任务级联卷积神经网络</h1><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="5a91" class="mn lr it mj b gy mo mp l mq mr"><strong class="mj iu">from</strong> IPython.display <strong class="mj iu">import</strong> Video</span><span id="c46a" class="mn lr it mj b gy my mp l mq mr">Video("data/IMG_2411.MOV", width=200, height=350)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/328add056c88c38cd6a2976219abe4ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*RshNl2bJiqCir-pAFUrp3A.png"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">我旋转脸的视频</p></figure><p id="8dab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将视频帧捕获为<code class="fe ms mt mu mj b">.png</code>文件，并旋转/裁剪/对齐。</p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="e851" class="mn lr it mj b gy mo mp l mq mr">vidcap = cv2.VideoCapture('IMG_2411.MOV')<br/>success,image = vidcap.read()<br/>count = 0<br/>success = <strong class="mj iu">True</strong><br/><strong class="mj iu">while</strong> success:<br/>    cv2.imwrite(f"./Michael_Chaykowsky/Michael_Chaykowsky_{<br/>                  format(count, '04d') }.png", image)<br/>    success,image = vidcap.read()<br/>    print('Read a new frame: ', success)<br/>    count += 1</span></pre><p id="8f89" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些图像是旋转的，所以我用<code class="fe ms mt mu mj b">imagemagick</code>让它们正面朝上。确保先进行<code class="fe ms mt mu mj b">brew install imagemagick</code>。我认为有另一种方法来安装这个库，但如果我回忆起来这是一个噩梦——所以一定要建议<code class="fe ms mt mu mj b">brew install</code>。</p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="a129" class="mn lr it mj b gy mo mp l mq mr">%%!<br/><strong class="mj iu">for</strong> szFile <strong class="mj iu">in</strong> ./Michael_Chaykowsky/*.png<br/><strong class="mj iu">do</strong> <br/>    magick mogrify -rotate 90 ./Michael_Chaykowsky/"$(basename "$szFile")" ; <br/><strong class="mj iu">done</strong></span></pre><p id="9c10" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ms mt mu mj b">! pip install autocrop</code></p><p id="68e3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Autocrop有一个很好的功能，他们可以调整人脸图像的大小，并且你可以指定人脸的百分比。如果你使用的是完整的MTCNN方法，你可以放弃这个方法(<em class="mw">preferred</em>)，但如果不是，你可以这样做，这要容易得多。</p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="4b5e" class="mn lr it mj b gy mo mp l mq mr">! autocrop -i ./me_orig/Michael_Chaykowsky -o ./me/Michael_Chaykowsky160 -w 720 -H 720 --facePercent 80</span></pre><p id="e7f6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ms mt mu mj b">! pip install tensorflow==1.13.0rc1</code></p><p id="6e71" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ms mt mu mj b">! pip install scipy==1.1.0</code></p><p id="c8ba" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在使用facenet的<a class="ae mv" href="https://github.com/davidsandberg/facenet" rel="noopener ugc nofollow" target="_blank">大卫·桑德伯格Tensorflow实现</a>中的<code class="fe ms mt mu mj b">align_dataset_mtcnn.py</code>脚本，我们可以将它应用到人脸图像目录中。</p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="8a39" class="mn lr it mj b gy mo mp l mq mr">%%!<br/><strong class="mj iu">for</strong> N <strong class="mj iu">in</strong> {1..4}; <strong class="mj iu">do</strong> \<br/>python ~/Adversarial/data/align/align_dataset_mtcnn.py \ # tensorflow script<br/>~/Adversarial/data/me/ \ # current directory<br/>~/Adversarial/data/me160/ \ # new directory<br/>--image_size 160 \<br/>--margin 32 \<br/>--random_order \<br/>--gpu_memory_fraction 0.25 \<br/>&amp; <strong class="mj iu">done</strong></span></pre><p id="e672" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在你已经有了一个目录，所有的面都被对齐并适当地裁剪以用于建模。</p><h1 id="bc9e" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">加载数据</h1><p id="e595" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">当我们加载数据时，我们将对图像进行一些随机变换，以改善训练。可以尝试不同的转换，您可以尝试不同的转换，例如</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/19ef3d8f39588ea3186d1f4677a14615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWyCPvaEDYwc_MS8lWd6-Q.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">随机颜色抖动</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/c2aef32fb3a87e766be6b6a58861b991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*7XChkQyTd8bJdgxT028S7Q.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">随机旋转+/- 5度</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/069c287d62656e5bcb6d958fe03dd2cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*IiM5YLTHZGwefLsSywKixw.png"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">随机水平翻转</p></figure><p id="dc89" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里我使用随机水平翻转。所有这些变换使模型更具普遍性，并防止过度拟合。</p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="3911" class="mn lr it mj b gy mo mp l mq mr">data_transforms = {<br/>    '<strong class="mj iu">train</strong>': transforms.Compose([<br/>        transforms.RandomHorizontalFlip(),<br/>        transforms.ToTensor(),<br/>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<br/>    ]),<br/>    '<strong class="mj iu">val</strong>': transforms.Compose([<br/>        transforms.ToTensor(),<br/>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<br/>    ]),<br/>}</span><span id="fd2b" class="mn lr it mj b gy my mp l mq mr">data_dir = 'data/test_me'<br/>image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),<br/>                                          data_transforms[x])<br/>                  <strong class="mj iu">for</strong> x <strong class="mj iu">in</strong> ['train', 'val']}<br/>dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],<br/>                                              batch_size=8, <br/>                                             shuffle=True)<br/>              <strong class="mj iu">for</strong> x <strong class="mj iu">in </strong>['train', 'val']}<br/>dataset_sizes = {x: <strong class="mj iu">len</strong>(image_datasets[x]) <strong class="mj iu">for</strong> x <strong class="mj iu">in </strong>['train','val']}<br/>class_names = image_datasets['train'].classes<br/>class_names</span></pre><p id="f037" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> Out[1]: </strong></p><p id="4b47" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ms mt mu mj b">['Adrien_Brody','Alejandro_Toledo','Angelina_Jolie','Arnold_Schwarzenegger','Carlos_Moya','Charles_Moose','James_Blake','Jennifer_Lopez','Michael_Chaykowsky','Roh_Moo-hyun','Venus_Williams']</code></p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="2242" class="mn lr it mj b gy mo mp l mq mr"><strong class="mj iu">def</strong> imshow(inp, title=<strong class="mj iu">None</strong>):<br/>    """Imshow for Tensor."""<br/>    inp = inp.numpy().transpose((1, 2, 0))<br/>    mean = np.array([0.485, 0.456, 0.406])<br/>    std = np.array([0.229, 0.224, 0.225])<br/>    inp = std * inp + mean<br/>    inp = np.clip(inp, 0, 1)<br/>    plt.imshow(inp)<br/>    <strong class="mj iu">if</strong> title <strong class="mj iu">is</strong> <strong class="mj iu">not</strong> <strong class="mj iu">None</strong>:<br/>        plt.title(title)<br/>    plt.pause(0.001)  # pause a bit so that plots are updated</span><span id="f3ba" class="mn lr it mj b gy my mp l mq mr"># Get a batch of training data<br/>inputs, classes = <strong class="mj iu">next</strong>(<strong class="mj iu">iter</strong>(dataloaders['train']))</span><span id="4d17" class="mn lr it mj b gy my mp l mq mr"># Make a grid from batch<br/>out = utils.make_grid(inputs)</span><span id="c997" class="mn lr it mj b gy my mp l mq mr">imshow(out, title=[class_names[x] <strong class="mj iu">for</strong> x <strong class="mj iu">in</strong> classes])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/685af6c201fcac093152dbf6c52e097d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YLvlFtUCdJI42VmJeEr_Cg.png"/></div></div></figure><h1 id="96db" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">获取VGGFace2数据集上的预训练ResNet</h1><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="cb2d" class="mn lr it mj b gy mo mp l mq mr"><strong class="mj iu">from</strong> models.inception_resnet_v1 <strong class="mj iu">import</strong> InceptionResnetV1</span><span id="322b" class="mn lr it mj b gy my mp l mq mr"><strong class="mj iu">print</strong>('Running on device: {}'.format(device))</span><span id="442c" class="mn lr it mj b gy my mp l mq mr">model_ft = InceptionResnetV1(pretrained='vggface2', classify=<strong class="mj iu">False</strong>, num_classes = <strong class="mj iu">len</strong>(class_names))</span></pre><h1 id="f306" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">冻结早期层</h1><p id="daf7" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">回想之前我提到过，我们将冻结穿过最后一个conv街区。为了找到它的位置，我们可以使用<code class="fe ms mt mu mj b">-n, -n-1, ...</code>遍历这个列表，直到找到这个块。</p><p id="036e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ms mt mu mj b">list(model_ft.children())[-6:]</code></p><p id="dfe3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> Out[2]: </strong></p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="6b89" class="mn lr it mj b gy mo mp l mq mr">[Block8(<br/>   (branch0): BasicConv2d(<br/>     (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>     (relu): ReLU()<br/>   )<br/>   (branch1): Sequential(<br/>     (0): BasicConv2d(<br/>       (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>       (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>       (relu): ReLU()<br/>     )<br/>     (1): BasicConv2d(<br/>       (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)<br/>       (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>       (relu): ReLU()<br/>     )<br/>     (2): BasicConv2d(<br/>       (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)<br/>       (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>       (relu): ReLU()<br/>     )<br/>   )<br/>   (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))<br/> ),<br/> AdaptiveAvgPool2d(output_size=1),<br/> Linear(in_features=1792, out_features=512, bias=False),<br/> BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),<br/> Linear(in_features=512, out_features=8631, bias=True),<br/> Softmax(dim=1)]</span></pre><p id="1fd2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">移除conv块后的最后几层，放入<code class="fe ms mt mu mj b">layer_list</code>。</p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="861c" class="mn lr it mj b gy mo mp l mq mr">layer_list = list(model_ft.children())[-5:] # all final layers<br/>layer_list</span></pre><p id="2409" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> Out[3]: </strong></p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="1bdc" class="mn lr it mj b gy mo mp l mq mr">[AdaptiveAvgPool2d(output_size=1),<br/> Linear(in_features=1792, out_features=512, bias=False),<br/> BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True),<br/> Linear(in_features=512, out_features=8631, bias=True),<br/> Softmax(dim=1)]</span></pre><p id="0749" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将所有开始的层放在一个<code class="fe ms mt mu mj b">nn.Sequential</code>中。<code class="fe ms mt mu mj b">model_ft</code>现在是一个火炬模型，但没有最终的线性，汇集，batchnorm和sigmoid层。</p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="4838" class="mn lr it mj b gy mo mp l mq mr">model_ft = nn.Sequential(*<strong class="mj iu">list</strong>(model_ft.children())[:-5])</span></pre><p id="9d9c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果训练只是最后一层:</p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="2fed" class="mn lr it mj b gy mo mp l mq mr"><strong class="mj iu">for</strong> param <strong class="mj iu">in</strong> model_ft.parameters():<br/>    param.requires_grad = <strong class="mj iu">False</strong></span></pre><p id="5c1a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">重新连接自动设置<code class="fe ms mt mu mj b">requires_grad = True</code>的最后5层。</p><p id="148d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个线性层<code class="fe ms mt mu mj b">Linear(in_features=1792, out_features=512, bias=False)</code>实际上需要编写两个自定义类，这看起来不太明显，但是如果你查看数据输入/输出，你可以看到在这个层中有一个扁平化和正常化类。<a class="ae mv" href="https://github.com/pytorch/vision/blob/3d5610391eaef38ae802ffe8b693ac17b13bd5d1/torchvision/models/resnet.py#L203" rel="noopener ugc nofollow" target="_blank">检查resnet实现</a>在<code class="fe ms mt mu mj b">last_linear</code>层整形的原因。</p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="be99" class="mn lr it mj b gy mo mp l mq mr"><strong class="mj iu">class</strong> Flatten(nn.Module):<br/>    <strong class="mj iu">def</strong> __init__(self):<br/>        <strong class="mj iu">super</strong>(Flatten, self).__init__()<br/>        <br/>    <strong class="mj iu">def</strong> forward(self, x):<br/>        x = x.view(x.size(0), -1)<br/>        <strong class="mj iu">return</strong> x</span><span id="7a34" class="mn lr it mj b gy my mp l mq mr"><strong class="mj iu">class</strong> normalize(nn.Module):<br/>    <strong class="mj iu">def</strong> __init__(self):<br/>        <strong class="mj iu">super</strong>(normalize, self).__init__()<br/>        <br/>    <strong class="mj iu">def</strong> forward(self, x):<br/>        x = F.normalize(x, p=2, dim=1)<br/>        <strong class="mj iu">return</strong> x</span></pre><p id="0b98" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，您可以将最后的层应用回新的顺序模型。</p><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="64f4" class="mn lr it mj b gy mo mp l mq mr">model_ft.avgpool_1a = nn.AdaptiveAvgPool2d(output_size=1)<br/>model_ft.last_linear = nn.Sequential(<br/>    Flatten(),<br/>    nn.Linear(in_features=1792, out_features=512, bias=False),<br/>    normalize()<br/>)<br/>model_ft.logits = nn.Linear(layer_list[3].in_features, len(class_names))<br/>model_ft.softmax = nn.Softmax(dim=1)</span><span id="dc30" class="mn lr it mj b gy my mp l mq mr">model_ft = model_ft.to(device)</span><span id="6a6d" class="mn lr it mj b gy my mp l mq mr">criterion = nn.CrossEntropyLoss()</span><span id="8d04" class="mn lr it mj b gy my mp l mq mr"># Observe that all parameters are being optimized<br/>optimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-2, momentum=0.9)</span><span id="55a0" class="mn lr it mj b gy my mp l mq mr"># Decay LR by a factor of *gamma* every *step_size* epochs<br/>exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)</span></pre><h1 id="7e81" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">火车</h1><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="8234" class="mn lr it mj b gy mo mp l mq mr"><strong class="mj iu">def</strong> train_model(model, criterion, optimizer, scheduler,<br/>                num_epochs=25):<br/>    since = time.time()<br/>    FT_losses = []<br/>    best_model_wts = copy.deepcopy(model.state_dict())<br/>    best_acc = 0.0</span><span id="e81a" class="mn lr it mj b gy my mp l mq mr">    <strong class="mj iu">for</strong> epoch <strong class="mj iu">in</strong> <strong class="mj iu">range</strong>(num_epochs):<br/>        <strong class="mj iu">print</strong>('Epoch {}/{}'.format(epoch, num_epochs - 1))<br/>        <strong class="mj iu">print</strong>('-' * 10)</span><span id="329f" class="mn lr it mj b gy my mp l mq mr">    # Each epoch has a training and validation phase<br/>        <strong class="mj iu">for</strong> phase <strong class="mj iu">in</strong> ['train', 'val']:<br/>            <strong class="mj iu">if</strong> phase <strong class="mj iu">==</strong> 'train':<br/>                model.train()  # Set model to training mode<br/>            <strong class="mj iu">else</strong>:<br/>                model.eval()   # Set model to evaluate mode</span><span id="5973" class="mn lr it mj b gy my mp l mq mr">            running_loss = 0.0<br/>            running_corrects = 0</span><span id="30e7" class="mn lr it mj b gy my mp l mq mr">            # Iterate over data.<br/>            <strong class="mj iu">for</strong> inputs, labels <strong class="mj iu">in</strong> dataloaders[phase]:<br/>                inputs = inputs.to(device)<br/>                labels = labels.to(device)</span><span id="1795" class="mn lr it mj b gy my mp l mq mr">                # zero the parameter gradients<br/>                optimizer.zero_grad()</span><span id="4dd1" class="mn lr it mj b gy my mp l mq mr">                # forward<br/>                # track history if only in train<br/>                <strong class="mj iu">with</strong> torch.set_grad_enabled(phase == 'train'):<br/>                    outputs = model(inputs)<br/>                    _, preds = torch.max(outputs, 1)<br/>                    loss = criterion(outputs, labels)</span><span id="21f8" class="mn lr it mj b gy my mp l mq mr">                    # backward + optimize only if in training phase<br/>                    <strong class="mj iu">if</strong> phase <strong class="mj iu">==</strong> 'train':<br/>                        loss.backward()<br/>                        optimizer.step()<br/>                        scheduler.step()<br/>                <br/>                FT_losses.append(loss.item())<br/>                # statistics<br/>                running_loss += loss.item() * inputs.size(0)<br/>                running_corrects += torch.sum(preds == labels.data)</span><span id="4eaf" class="mn lr it mj b gy my mp l mq mr">            epoch_loss = running_loss / dataset_sizes[phase]<br/>            epoch_acc = running_corrects.double() /<br/>                         dataset_sizes[phase]</span><span id="0e9c" class="mn lr it mj b gy my mp l mq mr">            <strong class="mj iu">print</strong>('{} Loss: {:.4f} Acc: {:.4f}'.format(<br/>                phase, epoch_loss, epoch_acc))</span><span id="ab99" class="mn lr it mj b gy my mp l mq mr">            # deep copy the model<br/>            <strong class="mj iu">if</strong> phase <strong class="mj iu">==</strong> 'val' <strong class="mj iu">and</strong> epoch_acc &gt; best_acc:<br/>                best_acc = epoch_acc<br/>                best_model_wts = copy.deepcopy(model.state_dict())</span><span id="f036" class="mn lr it mj b gy my mp l mq mr">    time_elapsed = time.time() - since<br/>    <strong class="mj iu">print</strong>('Training complete in {:.0f}m {:.0f}s'.format(<br/>        time_elapsed // 60, time_elapsed % 60))<br/>    <strong class="mj iu">print</strong>('Best val Acc: {:4f}'.format(best_acc))</span><span id="4d1c" class="mn lr it mj b gy my mp l mq mr">    # load best model weights<br/>    model.load_state_dict(best_model_wts)<br/>    <strong class="mj iu">return</strong> model, FT_losses</span></pre><h1 id="04be" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">评价</h1><pre class="kj kk kl km gt mi mj mk ml aw mm bi"><span id="0076" class="mn lr it mj b gy mo mp l mq mr">model_ft, FT_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=500)</span><span id="ec67" class="mn lr it mj b gy my mp l mq mr">plt.figure(figsize=(10,5))<br/>plt.title("FRT Loss During Training")<br/>plt.plot(FT_losses, label="FT loss")<br/>plt.xlabel("iterations")<br/>plt.ylabel("Loss")<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/a52fb06c049ad1b2538eb2a9e8e416ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-MPEmwuJSk8r_1biGML_Vw.png"/></div></div></figure><h1 id="243b" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">还会有更多</h1><p id="0e2a" class="pw-post-body-paragraph ku kv it kw b kx ne ju kz la nf jx lc ld ng lf lg lh nh lj lk ll ni ln lo lp im bi translated">请关注本系列的更多内容，我将描述如何使用GANs的对抗性攻击来欺骗这个分类器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/c3253b65aa0610e0c5ea3bf5652ee54f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fLVGQrtP-bozbCMRuaDHPA.png"/></div></div></figure></div></div>    
</body>
</html>