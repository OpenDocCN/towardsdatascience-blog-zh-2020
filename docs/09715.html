<html>
<head>
<title>What AlexNet Brought To The World Of Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AlexNet 给深度学习世界带来了什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc?source=collection_archive---------17-----------------------#2020-07-10">https://towardsdatascience.com/what-alexnet-brought-to-the-world-of-deep-learning-46c7974b46fc?source=collection_archive---------17-----------------------#2020-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6452c6a59f75fabb7944ab1cf7e1213b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e5Vf07_3j_IfELV2mhiDRg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由<a class="ae jg" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae jg" href="https://unsplash.com/@pietrozj?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Pietro Jeng </a>拍摄</p></figure><h2 id="5206" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">技术和解释</h2><div class=""/><div class=""><h2 id="fe62" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">花一分钟时间来理解彻底改变深度学习方法的琐碎技术和神经网络架构</h2></div><p id="8f50" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi md translated"><span class="l me mf mg bm mh mi mj mk ml di">T</span>Alex net 卷积神经网络(CNN)于 2012 年推出。从那时起，深度卷积神经网络的利用已经飙升到几个机器学习解决方案利用深度 CNN 的程度。</p><p id="d413" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">本文将介绍研究论文的基本发现和讨论要点，其中介绍了 AlexNet 架构。</p><h2 id="7f2c" class="mm mn jj bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd jp bi translated">在本文中，您可以找到以下内容:</h2><ul class=""><li id="036e" class="ne nf jj lj b lk ng ln nh lq ni lu nj ly nk mc nl nm nn no bi translated"><strong class="lj jt">介绍 AlexNet 的研究论文分解</strong></li><li id="453e" class="ne nf jj lj b lk np ln nq lq nr lu ns ly nt mc nl nm nn no bi translated"><strong class="lj jt">Alex net 架构的图示</strong></li><li id="22f6" class="ne nf jj lj b lk np ln nq lq nr lu ns ly nt mc nl nm nn no bi translated"><strong class="lj jt">Alex net</strong>内层成分表</li><li id="9e63" class="ne nf jj lj b lk np ln nq lq nr lu ns ly nt mc nl nm nn no bi translated"><strong class="lj jt">对各种技术的解释，如剔除、数据扩充、标准化等。</strong></li></ul><blockquote class="nu nv nw"><p id="faf7" class="lh li nx lj b lk ll kt lm ln lo kw lp ny lr ls lt nz lv lw lx oa lz ma mb mc im bi translated">所有级别的机器学习和深度学习实践者都可以遵循本文中介绍的内容。</p></blockquote></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="05cf" class="oi mn jj bd mo oj ok ol mr om on oo mu ky op kz mx lb oq lc na le or lf nd os bi translated">介绍</h1><p id="f57b" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">AlexNet 卷积神经网络架构在论文<a class="ae jg" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> ImageNet 分类与深度卷积神经网络</a>中提出。该论文由 Alex Krizhevsky、Ilya Sutskever 和深度学习教父 Geoffery Hinton 撰写。</p><div class="ow ox oy oz gt ab cb"><figure class="pa iv pb pc pd pe pf paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/55efed36b960d49a250f18cb820956eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/0*fxmmDliy1-ga9ACp"/></div></figure><figure class="pa iv pg pc pd pe pf paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/ad1375f1a793361d4f2558dd526b32b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/0*YxzvMUA3EDnVApGN.jpg"/></div></figure><figure class="pa iv ph pc pd pe pf paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/acf2aae8e27c977308964ad06f9b8763.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/0*_GfyI_nGPRITPIN7.jpg"/></div><p class="jc jd gj gh gi je jf bd b be z dk pi di pj pk translated">左:<a class="ae jg" href="https://qz.com/1307091/the-inside-story-of-how-ai-got-good-enough-to-dominate-silicon-valley/" rel="noopener ugc nofollow" target="_blank">亚历克斯·克里热夫斯基</a>，中:<a class="ae jg" href="https://www.bizjournals.com/sanjose/news/2016/11/30/techflash-q-a-reasons-to-be-hopeful-about-ourrobot.html" rel="noopener ugc nofollow" target="_blank">伊利亚·苏茨科夫</a>，右:<a class="ae jg" href="https://www.wired.com/story/googles-ai-guru-computers-think-more-like-brains/" rel="noopener ugc nofollow" target="_blank">杰弗里·辛顿</a></p></figure></div><p id="8aca" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该论文的作者旨在表明图像分类的琐碎任务可以通过使用深度卷积神经网络、有效的计算资源和常见的 CNN 实现技术来解决。</p><p id="99f6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">论文证明了一个由 5 个卷积层和 3 个全连接层组成的深度卷积神经网络可以高效、准确地对图像进行分类。</p><p id="de98" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一种深度卷积神经网络被称为 AlexNet，它被引入 ImageNet 大规模视觉识别挑战(ILSVRC 2012 竞赛)，在那里它为深度学习领域开创了先例。</p></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="508e" class="oi mn jj bd mo oj ok ol mr om on oo mu ky op kz mx lb oq lc na le or lf nd os bi translated">好时机</h1><p id="fa9b" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">在 AlexNet 推出之前，许多传统的神经网络和卷积神经网络在解决数据集上的图像分类方面表现良好，如<a class="ae jg" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>手写字符数据集。但是为了解决日常生活中物体的一般图像分类的问题，需要更大的数据集来说明图像中出现的物体的相当大的多样性。</p><figure class="ow ox oy oz gt iv gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/f46385d13454ed8fd684c650112ce05f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*tYUFQooqEp9iRJ2WrLOwFA.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://www.kaggle.com/shubham2306/cnn-on-mnist-using-keras-accuracy-0-993" rel="noopener ugc nofollow" target="_blank"> MNIST 数据集插图</a></p></figure><p id="bd03" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过引入大型数据集(如 ImageNet，包含 1500 万幅高分辨率图像中的 22，000 个类)解决了缺乏强大数据集的问题。</p><p id="4caa" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">引入 AlexNet 之前的另一个限制是计算机资源。</p><p id="d3c2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">增加网络的容量意味着增加网络的层数和神经元的数量。</p><p id="ad0d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当时，训练这样一个网络的计算资源是稀缺的。但是优化的 GPU 的引入使得训练深度常规神经网络成为可能。用于训练 AlexNet CNN 架构的特定 GPU 是<a class="ae jg" href="https://www.geforce.co.uk/hardware/desktop-gpus/geforce-gtx-580/specifications" rel="noopener ugc nofollow" target="_blank">英伟达 GTX 580 3GB GPU </a>。</p><figure class="ow ox oy oz gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/42db26169a4571e58303150555bf1260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*8En5Bw918pA06BHiE_q_tg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://www.techpowerup.com/gpu-specs/geforce-gtx-580.c270" rel="noopener ugc nofollow" target="_blank">英伟达 GTX 580 </a></p></figure></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="ab29" class="oi mn jj bd mo oj ok ol mr om on oo mu ky op kz mx lb oq lc na le or lf nd os bi translated">AlexNet 架构的独特特征</h1><h2 id="a18d" class="mm mn jj bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd jp bi translated">整流线性单位(ReLU)</h2><p id="2bac" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">为了训练神经网络内的神经元，标准做法是利用双曲正切或 sigmoid 非线性，这是 goto 激活函数，用于模拟 CNN 内的内部神经元激活。</p><p id="55cc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">AlexNet 继续使用校正线性单位，简称 ReLU。ReLU 是由 Vinod Nair 和 Geoffrey E. Hinton 于 2010 年在这篇论文中介绍的。</p><p id="fda8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">ReLu 可以被描述为对先前卷积层的输出执行的传递函数运算。ReLu 的使用确保了神经元内的正值被保持，但是对于负值，它们被箝位到零。</p><p id="bce5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用 ReLu 的好处是，与其他标准非线性技术相比，它能够加速训练过程，因为梯度下降优化以更快的速度发生。</p><p id="8f92" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">ReLu 层的另一个好处是它在网络中引入了非线性。它还消除了连续卷积的关联性。</p><h2 id="4c27" class="mm mn jj bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd jp bi translated">绘图处理器</h2><p id="27e5" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">在介绍 AlexNet 神经网络架构的原始研究论文中，模型的训练是利用两个具有 3GB 内存的 GTX 580 GPU 进行的。</p><p id="4ba2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">GPU 并行化和分布式训练是当今非常常用的技术。</p><p id="9eaf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">根据从研究论文中获得的信息，该模型在两个 GPU 上进行训练，其中一半的模型神经元在一个 GPU 上，另一半在第二个 GPU 的内存中。GPU 之间相互通信，不需要通过主机。GPU 之间的通信被限制在层的基础上；因此，只有特定的层可以相互通信。</p><p id="ce99" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如，AlexNet 网络第四层中的输入是从当前 GPU 上第三层的一半特征图中获得的，而另一半的剩余部分是从第二个 GPU 中获得的。这将在本文后面更好地说明。</p><h2 id="58ca" class="mm mn jj bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd jp bi translated">局部反应标准化</h2><p id="424a" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">标准化是将一组数据点放在一个可比较的基础或尺度上(<em class="nx">这是一个过于简单的描述</em>)。</p><p id="c916" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/batch-normalization-explained-algorithm-breakdown-23d2794511c">CNN 中的批量标准化(BN) </a>是一种通过将一批输入数据转换为均值为零、标准差为一的方式来标准化和规范化输入的技术。</p><p id="a02e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">许多人都熟悉批量标准化，但 AlexNet 架构在网络内使用了一种不同的标准化方法:本地响应标准化(LRN)。</p><p id="5878" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">LRN 是一种最大限度地激活邻近神经元的技术。相邻神经元描述了共享相同空间位置的多个特征图中的神经元。通过标准化神经元的激活，具有高激活的神经元被突出显示；这基本上模仿了神经生物学中发生的侧抑制。</p><p id="e100" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">LRN 在现代 CNN 架构中没有被广泛使用，因为还有其他更有效的归一化方法。虽然，LRN 的实现仍然可以在一些标准的<a class="ae jg" href="https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization" rel="noopener ugc nofollow" target="_blank">机器学习库和框架</a>中找到，所以请随意试验。</p><h2 id="57a6" class="mm mn jj bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd jp bi translated">重叠池</h2><p id="2395" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">CNN 中的池图层本质上封装了要素地图中一组像素或值内的信息，并将它们投影到较小尺寸的格网中，同时反映原始像素集中的一般信息。</p><p id="b8fd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图提供了一个池的示例，更具体地说是最大池。最大汇集是子采样的变体，其中落在汇集窗口的感受域内的像素的最大像素值。</p><figure class="ow ox oy oz gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/b4a4471fa731d5a134075415b97bbac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/1*h8InETzdC3PYQiZ0BNSqAA.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://www.oreilly.com/radar/visualizing-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">贾斯汀·弗朗西斯在奥里利的最大池插图</a></p></figure><p id="6082" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在介绍 AlexNet CNN 架构的文章中，介绍并利用了一种不同的池方法。重叠池。在传统的汇集技术中，从一个汇集窗口的一个中心到另一个中心的步幅被定位成确保来自一个汇集窗口的值不在随后的汇集窗口内。</p><p id="de39" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">与传统的池化方法相比，重叠池化利用的跨度小于池化窗口的尺寸。这意味着后续汇集窗口的输出封装了来自已经汇集了不止一次的像素/值的信息。很难看出这样做的好处，但根据这篇论文的发现，重叠池降低了模型在训练期间过度适应的能力。</p><div class="is it gp gr iu po"><a rel="noopener follow" target="_blank" href="/you-should-understand-sub-sampling-layers-within-deep-learning-b51016acd551"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd jt gy z fp pt fr fs pu fu fw js bi translated">(你应该)理解深度学习中的子采样层</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">平均池、最大池、子采样、下采样，这些都是你在深度学习中会遇到的短语…</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc ja po"/></div></div></a></div><h2 id="eabf" class="mm mn jj bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd jp bi translated">数据扩充</h2><p id="395e" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">另一种减少网络过度拟合机会的标准方法是通过数据扩充。通过人为扩充数据集，可以增加训练数据的数量，这反过来又会增加网络在训练阶段所暴露的数据量。</p><p id="176c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">图像的放大通常以变换、平移、缩放、裁剪、翻转等形式出现。</p><p id="7da6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">原始 AlexNet 论文中用于训练网络的图像在训练阶段被人工增强。所使用的增强技术是图像中像素强度的裁剪和改变。</p><p id="0a3f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">训练集中的图像从其 256×256 的尺寸被随机裁剪，以获得 224×224 的新裁剪图像。</p><p id="5aa3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> <em class="nx">增强为什么会起作用？</em> </strong></p><p id="d57e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">事实证明，对训练集进行随机扩充可以显著降低网络在训练过程中过度适应的可能性。</p><p id="6e95" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">增强图像仅仅是从原始训练图像的内容中得到的，那么为什么增强效果如此之好呢？</p><p id="f094" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">简单地说，数据扩充增加了数据集中的不变性，而不需要寻找新的数据。网络对看不见的数据集进行良好概括的能力也增加了。</p><p id="e621" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们举一个非常字面的例子；“生产”环境中的图像可能并不完美，有些可能会倾斜、模糊或只包含一些基本特征。因此，针对包括训练数据的更稳健变化的数据集来训练网络将使训练好的网络能够更成功地对生产环境中的图像进行分类。</p><h2 id="82a0" class="mm mn jj bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd jp bi translated">拒绝传统社会的人</h2><p id="bff8" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">辍学是许多深度学习从业者都很熟悉的一个术语。Dropout 是一种用于降低模型过度拟合可能性的技术。</p><p id="dfb9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Dropout 技术的工作原理是在 CNN 层内的神经元激活中增加一个概率因子。这个概率因子向神经元指示在电流前馈步骤期间以及在涉及反向传播过程期间被激活的机会。</p><p id="f7e8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">丢弃是有用的，因为它使神经元能够减少对相邻神经元的依赖性；这样一来，每个神经元都会学到更多有用的特征。</p><p id="3534" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在 AlexNet 架构中，在前两个完全连接的层中使用了 dropout 技术。</p><p id="0d72" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用丢弃技术的一个缺点是它增加了网络收敛的时间。</p><p id="312b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">虽然，利用辍学的优势远远超过其缺点。</p><div class="is it gp gr iu po"><a rel="noopener follow" target="_blank" href="/understanding-and-implementing-dropout-in-tensorflow-and-keras-a8a3a02c1bfa"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd jt gy z fp pt fr fs pu fu fw js bi translated">在 TensorFlow 和 Keras 中理解和实现辍学</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">辍学是一种常见的正规化技术，是杠杆在国家的艺术解决方案，以计算机视觉…</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qd l pz qa qb px qc ja po"/></div></div></a></div></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="ed3e" class="oi mn jj bd mo oj ok ol mr om on oo mu ky op kz mx lb oq lc na le or lf nd os bi translated">AlexNet 架构</h1><p id="cd73" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">在本节中，我们将了解 AlexNet 网络的内部组成。我们将关注与这些层相关的信息，并分解每个重要层的内部属性。</p><p id="edc9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">AlexNet CNN 架构由 8 层组成，其中包括 5 个 conv 层和 3 个全连接层。一些 conv 层由卷积层、池层和归一化层组成。</p><p id="0f86" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">AlexNet 是第一个采用连续卷积层(conv 第 3、4 和 5 层)架构的架构。</p><p id="ddfe" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">网络中的最终全连接层包含 softmax 激活函数，该函数提供表示 1000 个类的概率分布的向量。</p><h2 id="6609" class="mm mn jj bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd jp bi translated">Softmax 激活功能</h2><p id="d9ee" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">Softmax 激活用于导出输入向量中一组数字的概率分布。softmax 激活函数的输出是一个向量，其中它的一组值表示一个类或事件发生的概率。向量中的值加起来都是 1。</p><p id="823e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">除了最后完全连接的层之外，ReLU 激活功能被应用于网络中包括的其余层。</p><figure class="ow ox oy oz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qe"><img src="../Images/79a7fdcc3642e3000412b10fede94ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j4dH_AuL1PIVIg72Hi6KNg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">简化的<a class="ae jg" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet 神经网络多 GPU 架构</a></p></figure><p id="eeb0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上面 AlexNet 网络的图示被分成两个分区，因为该模型是在两个 GTX 580 GPU 上训练的。虽然网络跨两个 GPU 划分，但从图中我们可以看到 conv3、FC6、FC7 和 FC8 层中的一些跨 GPU 通信。</p><p id="5abb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下表列出了网络中各层的一些特征和属性。</p><figure class="ow ox oy oz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qf"><img src="../Images/fce6de182de2a59d6e1455000b8c6d77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nOFAKrHkVOcVoLlY5yVMJw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">AlexNet 架构属性表</p></figure><p id="db9b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在原始论文中，输入层的尺寸为 224 x 224 x 3，但在上表中，输入层的输入尺寸为 227 x 227 x 3，这种差异是由于在网络的实际训练过程中出现了一些未提及的填充，这些填充没有包括在已发表的论文中。</p></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="f302" class="oi mn jj bd mo oj ok ol mr om on oo mu ky op kz mx lb oq lc na le or lf nd os bi translated">结论</h1><p id="6305" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">AlexNet 的推出和成功改变了深度学习的格局。在其在 ILSVRC'12 竞赛中的胜利表现之后，接下来几年的获奖架构都是深度卷积神经网络。</p><p id="3ff8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">AlexNet 的一个变体以不同的超参数赢得了 ILSVRC'13 竞赛。2014 年、2015 年和 2016 年的获奖架构采用了更深的网络和更小的卷积内核/滤波器。</p><p id="3b73" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">理解 AlexNet 的架构很容易，甚至更容易实现，特别是使用像<a class="ae jg" href="https://pytorch.org/hub/pytorch_vision_alexnet/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>和 TensorFlow 这样的工具，这些工具在其库和框架中包含了一个架构模块。</p><p id="cff8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在以后的文章中，我将展示本文中介绍的 AlexNet 架构如何在 TensorFlow 中实现和使用。</p></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="45d0" class="oi mn jj bd mo oj ok ol mr om on oo mu ky op kz mx lb oq lc na le or lf nd os bi translated">我希望这篇文章对你有用。</h1><p id="0c88" class="pw-post-body-paragraph lh li jj lj b lk ng kt lm ln nh kw lp lq ot ls lt lu ou lw lx ly ov ma mb mc im bi translated">要联系我或找到更多类似本文的内容，请执行以下操作:</p><ol class=""><li id="e91e" class="ne nf jj lj b lk ll ln lo lq qg lu qh ly qi mc qj nm nn no bi translated">订阅我的<a class="ae jg" href="https://www.youtube.com/channel/UCNNYpuGCrihz_YsEpZjo8TA" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> YouTube 频道</strong> </a>视频内容即将上线<a class="ae jg" href="https://www.youtube.com/channel/UCNNYpuGCrihz_YsEpZjo8TA" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">这里</strong> </a></li><li id="b4eb" class="ne nf jj lj b lk np ln nq lq nr lu ns ly nt mc qj nm nn no bi translated">跟着我上<a class="ae jg" href="https://medium.com/@richmond.alake" rel="noopener"> <strong class="lj jt">中</strong> </a></li><li id="5577" class="ne nf jj lj b lk np ln nq lq nr lu ns ly nt mc qj nm nn no bi translated">通过<a class="ae jg" href="https://www.linkedin.com/in/richmondalake/" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> LinkedIn </strong> </a>联系我</li></ol></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><div class="ow ox oy oz gt po"><a rel="noopener follow" target="_blank" href="/how-you-should-read-research-papers-according-to-andrew-ng-stanford-deep-learning-lectures-98ecbd3ccfb3"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd jt gy z fp pt fr fs pu fu fw js bi translated">根据吴恩达(斯坦福深度学习讲座)，你应该如何阅读研究论文</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">关于如何通过知名人士发表的研究论文获取知识的指导。</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qk l pz qa qb px qc ja po"/></div></div></a></div><div class="is it gp gr iu po"><a rel="noopener follow" target="_blank" href="/algorithm-bias-in-artificial-intelligence-needs-to-be-discussed-and-addressed-8d369d675a70"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd jt gy z fp pt fr fs pu fu fw js bi translated">人工智能中的算法偏差需要讨论(和解决)</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">你在这件事上有责任…</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="ql l pz qa qb px qc ja po"/></div></div></a></div></div></div>    
</body>
</html>