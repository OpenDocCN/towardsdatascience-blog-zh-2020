<html>
<head>
<title>SPLENDID computer vision project makes your photo 3D!!!!!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">精彩的计算机视觉项目使你的照片三维！！！！！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/splendid-computer-vision-project-makes-your-photo-3d-fac10e334b2f?source=collection_archive---------43-----------------------#2020-04-24">https://towardsdatascience.com/splendid-computer-vision-project-makes-your-photo-3d-fac10e334b2f?source=collection_archive---------43-----------------------#2020-04-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="32e7" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">你的照片在和你说话！</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/902d65a8f7ceee950c9265a933d79a5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wibtMkDPQLqkCGEQ2ekheQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者照片，在通往班夫的路上，2018 年 11 月</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi le"><img src="../Images/ad45da0b65fca651bbd32e85eb049514.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*Inzb63-wcDzxmqYTgb2vWg.gif"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lf"><img src="../Images/9ecbe840d0ab7d04b67ce215e5ad52c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TEugkgXWy4uZpgfzpaIebQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者照片，班夫的 Yoho 国家公园，2018 年 11 月</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi le"><img src="../Images/bb6ec2ba3b70bf505dd7f57921273056.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*wKuUmoXjE4cD9znfrSbQuA.gif"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lg"><img src="../Images/efec7295bd2654c03b33b8f5ea0f4887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eVbdrKzrSpJxxkbwqdhQHA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者的照片，我在蒙特利尔的合作伙伴，2019 年 8 月</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/b86006be289f0f2d9777145f3c53a714.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*M0HrxIT2L1pZcZ6px47rAg.gif"/></div></figure></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><p id="74aa" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">太棒了，对吧？我和你们一样惊讶。这个神奇的<a class="ae mn" href="https://shihmengli.github.io/3D-Photo-Inpainting/" rel="noopener ugc nofollow" target="_blank">项目</a>是由四位伟大的研究人员<a class="ae mn" href="https://shihmengli.github.io/" rel="noopener ugc nofollow" target="_blank">施孟丽</a>、<a class="ae mn" href="https://lemonatsu.github.io/" rel="noopener ugc nofollow" target="_blank">施</a>、<a class="ae mn" href="https://johanneskopf.de/" rel="noopener ugc nofollow" target="_blank">约翰内斯·科普夫</a>和<a class="ae mn" href="https://filebox.ece.vt.edu/~jbhuang/" rel="noopener ugc nofollow" target="_blank">贾在</a><a class="ae mn" href="https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/3DPhoto_paper.pdf" rel="noopener ugc nofollow" target="_blank">2020 年 IEEE 计算机视觉与模式识别大会(CVPR)上完成的。</a></p><p id="db9f" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">想在你的照片上试试吗？如果你对计算机视觉和 CNN 完全没有概念也没关系，只要按照我在下面<strong class="lr iu"> <em class="mo">设置</em> </strong>部分的步骤运行这个<a class="ae mn" href="https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz" rel="noopener ugc nofollow" target="_blank">链接</a>中的所有代码就可以了！我建议在 Colab 中设置，因为它需要一定数量的计算机资源来训练，并且 Colab 会为您缓存。</p></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><h1 id="5ddc" class="jq jr it bd js jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn bi translated">设置:</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mu"><img src="../Images/6043bc0bf7aaeeb33173fedb4ba0b4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FqIB-OFSbxN1Gmbk-g3IQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">导入图像代码块</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/d9ad2c4449d9558925f378c0bb24374d.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*hpyCVm-BAYLkQxrH_OJ47Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">用于图像导入的文件夹</p></figure><ol class=""><li id="cd1c" class="mw mx it lr b ls lt lw lx ma my me mz mi na mm nb nc nd ne bi translated">运行该代码块之前的所有代码，将所有要制作 3D 的照片拖动到高亮显示的<em class="mo">图像</em>文件夹中，然后运行该代码块导入您上传的图像。</li><li id="92ee" class="mw mx it lr b ls nf lw ng ma nh me ni mi nj mm nb nc nd ne bi translated">然后只需运行下面的最后一个代码块:</li></ol><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="d4f6" class="np jr it nl b gy nq nr l ns nt">!python main.py --config argument.yml</span></pre><p id="b148" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">根据您的计算机规格和图片属性，每个训练批次需要等待 2-5 分钟。</p><p id="1c93" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">3.然后你得到结果！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b4ccbbf0797727f00084b78cce463ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*eamT3dhwsrU69I1rSVq2bQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">输出结果</p></figure><p id="1709" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">您可以在指定区域找到您的结果。它将输出五个输出视觉效果，其中包括由<a class="ae mn" href="https://github.com/intel-isl/MiDaS.git" rel="noopener ugc nofollow" target="_blank"> MiDaS </a>估计的深度图，修复的 3D 网格，以及在圆周、摆动和缩放运动中的 3D 视频演示。够简单了吧？想知道背后的逻辑就继续看下去吧！</p></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><h1 id="cc46" class="jq jr it bd js jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn bi translated">理论:</h1><p id="af4b" class="pw-post-body-paragraph lp lq it lr b ls nv lu lv lw nw ly lz ma nx mc md me ny mg mh mi nz mk ml mm im bi translated">一台机器如何从一张 2D 的照片中预测出 3D 视图？我的意思是，对于照片中的每个物体，如果你想“看到”它背后的东西，你必须以某种方式想象它是一个人。当人们看到一张照片时，他们不仅只是将它视为一个静态的图像，还会将它感知为一个有生命的 3D 物体，甚至会虚构一个想象的场景或回忆一些记忆。但是机器如何处理如此复杂的概念呢？它能不能“<strong class="lr iu"> <em class="mo">想象</em> </strong>”？？</p><blockquote class="oa ob oc"><p id="a5e9" class="lp lq mo lr b ls lt lu lv lw lx ly lz od mb mc md oe mf mg mh of mj mk ml mm im bi translated">嗯，一台机器无法想象，但它可以“<strong class="lr iu"> <em class="it">学习</em> </strong>”到“<strong class="lr iu"> <em class="it">想象</em> </strong>”，或者换句话说，它可以像人类一样处理数据和输出。基本上，机器只是做他们擅长的事情:计算。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/7edd8f2137973404dad5a223c1b1884a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4aONSQV2oQGBJ8PeYPUJww.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片来自<a class="ae mn" href="https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/3DPhoto_paper.pdf" rel="noopener ugc nofollow" target="_blank">使用上下文感知分层深度修复的 3D 摄影</a></p></figure><p id="58e4" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">通常人工智能学习 RGB-D 图像，其中 D 代表“深度”，以重温 3D 效果。目前，市场上大多数智能手机都有两个摄像头来分别捕捉颜色和深度。但是，没有深度的普通 RGB 图片怎么办？机器预测！通过一些标准的图像预处理步骤，我们可以很容易地找到深度图(a 到 d)</p><p id="f8a3" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">有了预测的深度，机器可以找到深度不连续的地方，然后分类，并分组到不同的颜色部分(e 到 f)。</p><p id="0bfd" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">有了所有的预处理准备，我们将从我们的 2D 照片修复三维视觉。我们正在使用的最重要的工具叫做<strong class="lr iu"><em class="mo">【LDI】</em></strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/e652963e8bd04c107e4ce2e9d3bf0469.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8zj8T28i84YLyBiSMjthA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片来自<a class="ae mn" href="https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/3DPhoto_paper.pdf" rel="noopener ugc nofollow" target="_blank">使用上下文感知分层深度修复的 3D 摄影</a></p></figure><p id="c483" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在边缘上，像素由两边通过一个锐降(a)连接。该程序首先将拖放连接切割成绿色和红色区域(b)，我们称它们为前景轮廓和背景轮廓，基于背景轮廓或上下文区域(c)生成一个合成区域，然后合并到模型中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/74504d7ab36b7ae1a9461a822c2e8542.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*nIJ-e5XUR_Pi4oIGHQaBYA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片来自<a class="ae mn" href="https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/3DPhoto_paper.pdf" rel="noopener ugc nofollow" target="_blank">使用上下文感知分层深度修复的 3D 摄影</a></p></figure><p id="9e80" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，由于我们已经分离了两个区域(上下文区域和合成区域)，科学家使用三个修复代理来完成修复任务:边缘修复网络、颜色修复网络和深度修复网络。你可以查看下面的参考资料，详细了解这些修复网络是如何工作的。</p><p id="e763" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">边缘修复网络修复上下文区域和合成区域之间的轮廓，以预测被阻挡的边缘。然后机器使用颜色修复网络和深度修复网络分别<em class="mo">想象</em>被遮挡的颜色和深度。在这之后，我们将结果反馈给 LDI 模型，就这样！我们有结果了！</p><blockquote class="oa ob oc"><p id="80fd" class="lp lq mo lr b ls lt lu lv lw lx ly lz od mb mc md oe mf mg mh of mj mk ml mm im bi translated">没有进一步的到期，玩模型，重温你的记忆！</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi le"><img src="../Images/4cb82115e5b98a501634ab68f180302b.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*Os_xXoFH6VyKZcJ9FJjbqQ.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae mn" href="https://pixabay.com/photos/toy-toy-story-childhood-little-2207781/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae mn" href="https://pixabay.com/users/coyot-2009089/" rel="noopener ugc nofollow" target="_blank"> Coyot </a>拍摄，作者加工</p></figure></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><h1 id="3678" class="jq jr it bd js jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn bi translated">参考和资源:</h1><h2 id="9d73" class="np jr it bd js oj ok dn jw ol om dp ka ma on oo ke me op oq ki mi or os km ot bi translated">相关项目:</h2><ul class=""><li id="ac69" class="mw mx it lr b ls nv lw nw ma ou me ov mi ow mm ox nc nd ne bi translated">迈达斯:<a class="ae mn" href="https://github.com/intel-isl/MiDaS" rel="noopener ugc nofollow" target="_blank">https://github.com/intel-isl/MiDaS</a></li><li id="6c8e" class="mw mx it lr b ls nf lw ng ma nh me ni mi nj mm ox nc nd ne bi translated"><a class="ae mn" href="https://github.com/LouisFoucard/StereoConvNet" rel="noopener ugc nofollow" target="_blank"> StereoConvNet </a></li></ul><h2 id="5183" class="np jr it bd js oj ok dn jw ol om dp ka ma on oo ke me op oq ki mi or os km ot bi translated">论文和参考文献:</h2><ul class=""><li id="e425" class="mw mx it lr b ls nv lw nw ma ou me ov mi ow mm ox nc nd ne bi translated"><a class="ae mn" href="https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/3DPhoto_paper.pdf" rel="noopener ugc nofollow" target="_blank">使用上下文感知分层深度修复的 3D 摄影</a>纸</li><li id="d3f0" class="mw mx it lr b ls nf lw ng ma nh me ni mi nj mm ox nc nd ne bi translated"><a class="ae mn" href="https://shihmengli.github.io/3D-Photo-Inpainting/" rel="noopener ugc nofollow" target="_blank">使用上下文感知分层深度修复的 3D 摄影</a>网站</li><li id="96ed" class="mw mx it lr b ls nf lw ng ma nh me ni mi nj mm ox nc nd ne bi translated"><a class="ae mn" href="https://arxiv.org/pdf/1901.00212.pdf" rel="noopener ugc nofollow" target="_blank">边缘修复网络</a>纸张</li><li id="865a" class="mw mx it lr b ls nf lw ng ma nh me ni mi nj mm ox nc nd ne bi translated"><a class="ae mn" href="https://arxiv.org/abs/1804.07723" rel="noopener ugc nofollow" target="_blank">彩色修复网</a>纸张</li><li id="ed43" class="mw mx it lr b ls nf lw ng ma nh me ni mi nj mm ox nc nd ne bi translated"><a class="ae mn" href="https://arxiv.org/abs/1901.05945" rel="noopener ugc nofollow" target="_blank">深度修复网络</a>论文</li></ul></div></div>    
</body>
</html>