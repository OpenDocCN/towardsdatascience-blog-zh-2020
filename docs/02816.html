<html>
<head>
<title>Autoregressive Models — PixelCNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自回归模型— PixelCNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/autoregressive-models-pixelcnn-e30734ede0c1?source=collection_archive---------16-----------------------#2020-03-18">https://towardsdatascience.com/autoregressive-models-pixelcnn-e30734ede0c1?source=collection_archive---------16-----------------------#2020-03-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="69a1" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">深度学习/生成模型</h2><div class=""/><div class=""><h2 id="1dd3" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用深度生成模型创建数字！</h2></div><p id="0e86" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">编剧<a class="ae lk" href="https://twitter.com/Warvito" rel="noopener ugc nofollow" target="_blank">沃尔特·雨果·洛佩兹·皮纳亚</a>、<a class="ae lk" href="https://twitter.com/thepfcosta" rel="noopener ugc nofollow" target="_blank">佩德罗·f·达·科斯塔</a>和<a class="ae lk" href="https://twitter.com/jessdafflon" rel="noopener ugc nofollow" target="_blank">杰西卡·达弗伦</a></p><p id="8aa7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">大家好！这是我们关于现代自回归模型系列的第一篇文章。以下是我们将在本系列中涉及的主题:</p><h2 id="e569" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated"><strong class="ak">总结</strong></h2><ol class=""><li id="7bea" class="md me iq kq b kr mf ku mg kx mh lb mi lf mj lj mk ml mm mn bi translated">自回归模型— PixelCNN</li><li id="daf2" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj mk ml mm mn bi translated"><a class="ae lk" href="https://medium.com/@pedro.hpf.costa/modelling-coloured-images-acd0ebde0102" rel="noopener">模拟彩色图像</a></li><li id="5136" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj mk ml mm mn bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/pixelcnns-blind-spot-84e19a3797b9"> PixelCNN的盲点及如何修复——门控PixelCNN </a></li><li id="5a9e" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj mk ml mm mn bi translated">使用门控像素CNN的条件生成</li><li id="1064" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj mk ml mm mn bi translated">带裁剪卷积的门控像素CNN</li><li id="d95e" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj mk ml mm mn bi translated">提高性能— PixelCNN++</li><li id="e2cf" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj mk ml mm mn bi translated">缩短采样时间—快速PixelCNN++</li><li id="c5ef" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj mk ml mm mn bi translated">使用注意机制——pixels nail</li><li id="538b" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj mk ml mm mn bi translated">生成多样化的高保真图像——VQ-VAE 2</li></ol><p id="64d0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">每个主题的实现都可以在<a class="ae lk" href="https://github.com/Mind-the-Pineapple/Autoregressive-models" rel="noopener ugc nofollow" target="_blank">这个库</a>中找到</p><p id="b098" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">开始吧！</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="3be0" class="na lm iq bd ln nb nc nd lq ne nf ng lt kf nh kg lw ki ni kj lz kl nj km mc nk bi translated">介绍</h1><p id="8ae2" class="pw-post-body-paragraph ko kp iq kq b kr mf ka kt ku mg kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">生成模型是来自无监督学习的一类重要的模型，在过去的几年中受到了很多关注。这些可以被定义为一类模型，其目标是学习如何生成看起来来自与训练数据相同的数据集的新样本。在训练阶段，生成模型试图解决<strong class="kq ja">密度估计的核心任务。</strong>在密度估计中，我们的模型学习构造一个估计量——<em class="no">p model(</em><strong class="kq ja"><em class="no">x</em></strong><em class="no">)</em>——尽可能的类似于不可观测的概率密度函数——<em class="no">pdata(</em><strong class="kq ja"><em class="no">x</em></strong><em class="no">)</em>。值得一提的是，创成式模型应该能够从分布中构造新的样本，而不仅仅是复制和粘贴现有的样本。一旦我们成功地训练了我们的模型，它就可以用于各种各样的应用，从图像修复、着色和超分辨率等重建形式到艺术作品的生成。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi np"><img src="../Images/50608e210bf0fb4c9c4068048fb88e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aHOrQWNk4iQeKg5RJfOYGg.png"/></div></div></figure><p id="c765" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有几种不同的方法可以用来进行这种概率密度估计，例如:</p><ol class=""><li id="3e56" class="md me iq kq b kr ks ku kv kx ob lb oc lf od lj mk ml mm mn bi translated"><strong class="kq ja">生成敌对网络(GANs) </strong>使用一种方法，其中模型执行<strong class="kq ja"> <em class="no">隐式密度估计</em> </strong>。在这种情况下，我们训练一个模型，它可以从<em class="no">p model(</em><strong class="kq ja"><em class="no">x</em></strong><em class="no">)</em>创建样本，而无需显式定义<em class="no">p model(</em><strong class="kq ja"><em class="no">x</em></strong><em class="no">)</em>；该模型学习产生数据的随机程序，但不提供观察概率的知识或指定条件对数似然函数；</li><li id="7a4e" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj mk ml mm mn bi translated"><strong class="kq ja">变分自动编码器(VAE) </strong>使用一个<strong class="kq ja"> <em class="no">显式密度估计</em> </strong>但是定义了一个难以处理的密度函数，其潜在变量不能被直接优化。所以，为了训练模型，我们改为推导并优化似然的下界(<strong class="kq ja"> <em class="no">近似密度</em></strong>)；我们通过最大化证据下限(ELBO)来优化数据的对数似然性(更多细节可在此处的<a class="ae lk" href="https://www.cs.ubc.ca/~lsigal/532S_2018W2/Lecture13b.pdf" rel="noopener ugc nofollow" target="_blank">和此处</a>的<a class="ae lk" href="https://arxiv.org/abs/1906.02691" rel="noopener ugc nofollow" target="_blank">中找到)；</a></li><li id="58ee" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj mk ml mm mn bi translated"><strong class="kq ja">自回归(AR)模型</strong>创建一个<strong class="kq ja"> <em class="no">显式密度</em> </strong>模型，该模型易于处理以最大化训练数据的可能性(<strong class="kq ja"> <em class="no">易于处理的密度</em> </strong>)。为此，使用这些方法，很容易计算数据观察的可能性，并获得生成模型的评估度量。</li></ol><p id="12f5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正如我们提到的，自回归是一种实用的方法，它提供了似然函数的显式建模。然而，为了对具有多个维度/特征的数据进行建模，自回归模型需要施加一些条件。首先，输入空间<em class="no"> X </em>需要有一个<strong class="kq ja">来决定其特征的</strong> <strong class="kq ja">排序</strong>。这就是为什么自回归模型通常用于具有内在时间步长序列的时间序列。然而，它们可以用于图像，例如，通过定义左边的像素在右边的像素之前，顶部的像素在底部的像素之前。第二，为了对数据观察中的特征的联合分布进行易处理的建模(<em class="no">p(</em><strong class="kq ja"><em class="no">x</em></strong><em class="no">)</em>，自回归方法将<em class="no">p(</em><strong class="kq ja"><em class="no">x</em></strong><em class="no">)</em>作为条件分布的<strong class="kq ja">产物。自回归模型在给定先前特征的值的情况下，对每个特征使用条件来定义联合分布。例如，来自图像的像素具有特定强度值的概率取决于所有先前像素的值；而一幅图像的概率(所有像素的联合分布)就是它所有像素的概率的组合。因此，自回归模型使用链式法则将数据样本<strong class="kq ja"> <em class="no"> x </em> </strong>的可能性分解为一维分布的乘积(下面的等式)。因式分解将联合建模问题转化为序列问题，在序列问题中，人们学习在给定所有先前生成的像素的情况下预测下一个像素。</strong></p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/e892eaa7f78bb2e6ce30ad9dc8bb0ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*b6AXt3LD7Gahuq9FZ7BlEA.png"/></div></figure><p id="2db1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这些条件(即确定排序和条件分布的乘积)是自回归模型的主要定义。</p><p id="acf8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，<strong class="kq ja">大挑战</strong>是计算这些条件可能性<em class="no"> p(x ᵢ| x₁，…，x ᵢ ₋ ₁) </em>。我们如何在一个易于处理和扩展的表达模型中定义这些复杂的分布呢？一种解决方案是使用通用近似器，如深度神经网络。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="6980" class="na lm iq bd ln nb nc nd lq ne nf ng lt kf nh kg lw ki ni kj lz kl nj km mc nk bi translated">像素CNN</h1><p id="7e8a" class="pw-post-body-paragraph ko kp iq kq b kr mf ka kt ku mg kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">DeepMind在2016年推出了pixel CNN(<a class="ae lk" href="https://arxiv.org/abs/1601.06759" rel="noopener ugc nofollow" target="_blank">Oord et al .，2016 </a>)，这个模型开启了自回归生成模型最有前途的家族之一。从那时起，它就被用于<a class="ae lk" href="https://deepmind.com/blog/article/wavenet-generative-model-raw-audio" rel="noopener ugc nofollow" target="_blank">生成语音</a>、<a class="ae lk" href="https://arxiv.org/abs/1610.00527" rel="noopener ugc nofollow" target="_blank">视频</a>和<a class="ae lk" href="https://arxiv.org/abs/1906.00446" rel="noopener ugc nofollow" target="_blank">高分辨率图片</a>。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/dc8d9ffe49db65398a6d4e763a3d1978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*X84aUh_3iiAuaA7_7iTA9A.png"/></div><p class="of og gj gh gi oh oi bd b be z dk translated">使用VQ-VAE-2生成图像(<a class="ae lk" href="https://arxiv.org/abs/1906.00446" rel="noopener ugc nofollow" target="_blank">链接</a>)</p></figure><p id="9552" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">PixelCNN是一个深度神经网络，可以捕捉其参数中像素之间的依赖关系分布。它沿着两个空间维度在图像中一次一个像素地顺序生成。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi oj"><img src="../Images/c1dc72d976acda723f071c7a79756833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Sm3JMqgAuQoP-_Aho-xOw.png"/></div></div><p class="of og gj gh gi oh oi bd b be z dk translated">感兴趣的像素I(红色)由所有先前的像素(蓝色)定义。PixelCNN可以沿着深度神经网络使用卷积层对它们的关联进行建模。</p></figure><p id="f94e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">使用卷积运算，PixelCNN可以并行学习图像中所有像素的分布。然而，在确定特定像素的概率时，标准卷积层的感受野违反了自回归模型的顺序预测。在处理一个中心像素的信息时，卷积滤波器考虑其周围的所有像素来计算输出特征图，而不仅仅是前面的像素。<strong class="kq ja">然后采用掩模</strong>来阻挡来自尚未预测的像素的信息流。</p><h1 id="0ab0" class="na lm iq bd ln nb ok nd lq ne ol ng lt kf om kg lw ki on kj lz kl oo km mc nk bi translated">屏蔽卷积层</h1><p id="f886" class="pw-post-body-paragraph ko kp iq kq b kr mf ka kt ku mg kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">可以通过将不应考虑的所有像素置零来进行掩蔽。在我们的实现中，创建了一个与卷积滤波器大小相同、值为1和0的掩码。在进行卷积运算之前，该遮罩与权重张量相乘。在PixelCNN中，有两种类型的遮罩:</p><ul class=""><li id="a6cd" class="md me iq kq b kr ks ku kv kx ob lb oc lf od lj op ml mm mn bi translated"><strong class="kq ja">掩码类型A </strong>:该掩码仅适用于第一个卷积层。它通过将遮罩中的中心像素置零来限制对感兴趣像素的访问。这样，我们保证模型不会访问它将要预测的像素(下图中的红色)。</li><li id="f2c7" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><strong class="kq ja">屏蔽类型B </strong>:该屏蔽应用于所有后续卷积层，并通过允许从一个像素到其自身的连接来放松屏蔽A的限制。为了说明第一层的像素预测，这是很重要的。</li></ul><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi oq"><img src="../Images/4195f6dec1bc6640914b4ed6754d923a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fWOza0sytZOXB2N1Mjb88Q.png"/></div></div><p class="of og gj gh gi oh oi bd b be z dk translated">掩码A仅用于第一卷积层。在所有其他层中使用掩码B，以允许以感兴趣的像素为中心的卷积运算的信息沿着网络传播。</p></figure><p id="66c3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这里，我们展示了一个片段，展示了使用Tensorflow 2.0框架实现的掩码。</p><figure class="nq nr ns nt gt nu"><div class="bz fp l di"><div class="or os l"/></div></figure><h1 id="bc33" class="na lm iq bd ln nb ok nd lq ne ol ng lt kf om kg lw ki on kj lz kl oo km mc nk bi translated">体系结构</h1><p id="0db0" class="pw-post-body-paragraph ko kp iq kq b kr mf ka kt ku mg kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">在Oord et al. 2016中，PixelCNN使用了以下架构:第一层是带有7x7滤波器的掩蔽卷积(A型)。然后，使用15个残差块。每个模块使用掩码类型为B的3×3卷积层和标准1×1卷积层的组合来处理数据。在每个卷积层之间，有一个非线性ReLU。最后，残余块还包括残余连接。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi ot"><img src="../Images/8a39a867c85117545a72aa2eb3fad034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Fq4u66LYCLZElYuF_OOmA.png"/></div></div><p class="of og gj gh gi oh oi bd b be z dk translated">像素CNN的架构。</p></figure><p id="766d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在块序列之后，网络具有RELU-CONV-RELU-CONV层链，使用具有1x1滤波器的标准卷积层。然后，输出层是预测像素的所有可能值中的值的softmax层。模型的输出具有与输入图像相同的空间格式(因为我们需要每个像素的输出值)乘以可能值的数量(例如，256个强度级别)。</p><p id="e64d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这里，我们展示了使用Tensorflow 2.0框架实现网络架构的一个片段。</p><figure class="nq nr ns nt gt nu"><div class="bz fp l di"><div class="or os l"/></div></figure><h1 id="e902" class="na lm iq bd ln nb ok nd lq ne ol ng lt kf om kg lw ki on kj lz kl oo km mc nk bi translated">预处理</h1><p id="3d17" class="pw-post-body-paragraph ko kp iq kq b kr mf ka kt ku mg kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">像素CNN的输入值被缩放到[0，1]的范围内。在这个预处理过程中，有可能以较低数量的强度等级量化像素值。在我们的实现中，我们首先呈现用两个强度级别训练的模型，然后用所有256个级别。我们注意到，由于问题复杂性较低(在像素的概率分布中要考虑的可能值较少)，该模型在具有较少级别的数据中表现更好。</p><p id="b761" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">目标数据对应于指示像素强度的分类(整数)值。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi ou"><img src="../Images/0ad9307751c41aa7c51bcaf689f991b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIeYsLm_ob2IMrm0RnTdGQ.png"/></div></div></figure><h1 id="016f" class="na lm iq bd ln nb ok nd lq ne ol ng lt kf om kg lw ki on kj lz kl oo km mc nk bi translated">模型评估</h1><p id="f5fa" class="pw-post-body-paragraph ko kp iq kq b kr mf ka kt ku mg kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">PixelCNN有一个简单的训练方法。该模型通过<strong class="kq ja">最大化训练数据</strong>的可能性来学习其参数。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/fbf5f8eb7eda497c2966302020933321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*TCVbG_tY3__Dk1xDSfk8Mw.png"/></div></figure><p id="5da1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由于大多数优化问题被定义为最小化问题，一个常用的技巧是将训练目标转化为最小化<strong class="kq ja">负对数似然</strong> (NLL)。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f0dd13cfd6c26328e037ff301f2c2fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*SIqNhj9XZRFH-V6Xp1OG5A.png"/></div></figure><p id="5acf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由于<em class="no"> p(xᵢ|θ) </em>对应于softmax层输出的概率，NLL等价于<strong class="kq ja">交叉熵损失</strong>函数——监督学习中常用的损失函数。此外，NLL是一种用于比较生成方法之间性能的指标(使用nats单位或每像素位数)。</p><h1 id="6b14" class="na lm iq bd ln nb ok nd lq ne ol ng lt kf om kg lw ki on kj lz kl oo km mc nk bi translated"><strong class="ak">推论</strong></h1><p id="7b23" class="pw-post-body-paragraph ko kp iq kq b kr mf ka kt ku mg kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">由于PixelCNN是一个自回归模型，推理恰好是顺序的——我们必须逐个像素地生成。首先，我们通过向模型传递零来生成图像。它不应该影响第一个像素，因为它的值被建模为独立于所有其他像素。因此，我们执行向前传递并获得它的分布。给定分布，我们从多项式概率分布中抽取一个值。然后，我们用采样的像素值更新我们的图像，并且我们重复这个过程，直到我们产生所有的像素值。这里使用PixelCNN使用MNIST数据集在150个时期后生成样本。每张生成的图像都有四级像素强度。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/85997ddf3b8e5195f35def07a70f55d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*-p-GZEfNKYDW9AjGEqCPIg.png"/></div></figure><p id="bb9c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">相同的采样过程可以用于部分遮挡的图像作为起点。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/dfb760f4f428251a57bcf428948d4d7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*TrQr--5qm8sf8DKtl2m8pg.png"/></div></figure><p id="017e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们还尝试训练或建模以产生具有256个像素强度级别的图像。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/506cec167ddc5f0c4be3f846c6e2e62b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*2E-FLtHLQ9h7KtRlTb0VhA.png"/></div><p class="of og gj gh gi oh oi bd b be z dk translated">生成了256个强度级别的数字。</p></figure><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/6fa17c659bb8e4e7f89b94a6114f9e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*w8TfkyzX3OSrNoZxdMEX3g.png"/></div><p class="of og gj gh gi oh oi bd b be z dk translated">部分遮挡的图像。</p></figure><p id="d768" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">与其他生成模型(VAE和甘斯)相比，这种采样过程相对较慢，在其他模型中，所有像素都是一次性生成的。然而，最近的进展已经使用缓存值来减少采样时间(快速PixelCNN++，在接下来的帖子中解决)</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="2d19" class="na lm iq bd ln nb nc nd lq ne nf ng lt kf nh kg lw ki ni kj lz kl nj km mc nk bi translated">结论</h1><p id="8c37" class="pw-post-body-paragraph ko kp iq kq b kr mf ka kt ku mg kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">PixelCNN模型的优点是联合概率学习技术是易处理的，并且可以使用梯度下降来学习。没有近似值；我们只是试图在给定所有先前像素值的情况下预测每个像素值。由于PixelCNN是通过最小化负对数似然来训练的，因此与替代方法(例如GANs需要找到纳什均衡)相比，它的训练更加稳定。然而，由于样本的生成是连续的(逐个像素)，原始的PixelCNN在可扩展性方面存在问题。在下一篇文章中，我们将在具有RGB通道的数据集中训练PixelCNN模型。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="92bb" class="na lm iq bd ln nb nc nd lq ne nf ng lt kf nh kg lw ki ni kj lz kl nj km mc nk bi translated">参考</h1><ul class=""><li id="0341" class="md me iq kq b kr mf ku mg kx mh lb mi lf mj lj op ml mm mn bi translated"><a class="ae lk" href="http://sergeiturukin.com/2017/02/22/pixelcnn.html" rel="noopener ugc nofollow" target="_blank">http://sergeiturukin.com/2017/02/22/pixelcnn.html</a></li><li id="0fa0" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173">https://towards data science . com/auto-regressive-generative-models-pixel rnn-pixel CNN-32d 192911173</a></li><li id="32c7" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated">【https://deepgenerativemodels.github.io/ T4】</li><li id="f3b1" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://eigenfoo.xyz/deep-autoregressive-models/" rel="noopener ugc nofollow" target="_blank">https://eigenfoo.xyz/deep-autoregressive-models/</a></li><li id="293b" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://wiki.math.uwaterloo.ca/statwiki/index.php?title=STAT946F17/Conditional_Image_Generation_with_PixelCNN_Decoders" rel="noopener ugc nofollow" target="_blank">https://wiki.math.uwaterloo.ca/statwiki/index.php?title = stat 946 f17/Conditional _ Image _ Generation _ with _ pixel CNN _ Decoders</a></li><li id="0b26" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://www.codeproject.com/Articles/5061271/PixelCNN-in-Autoregressive-Models" rel="noopener ugc nofollow" target="_blank">https://www . code project . com/Articles/5061271/pixel CNN-in-auto regressive-Models</a></li><li id="d0a5" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/blind-spot-problem-in-pixelcnn-8c71592a14a">https://towards data science . com/blind-spot-problem-in-pixel CNN-8c 71592 a 14 a</a></li><li id="de93" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated">https://www.youtube.com/watch?v=5WoItGTWV54&amp;t = 1165s</li><li id="fda8" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://www.youtube.com/watch?v=R8fx2b8Asg0" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=R8fx2b8Asg0</a></li><li id="7560" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://arxiv.org/pdf/1804.00779v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1804.00779v1.pdf</a></li><li id="4be9" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://blog.evjang.com/2019/07/likelihood-model-tips.html" rel="noopener ugc nofollow" target="_blank">https://blog.evjang.com/2019/07/likelihood-model-tips.html</a></li><li id="a68b" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://arxiv.org/abs/1810.01392" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1810.01392</a></li><li id="8b58" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="http://bjlkeng.github.io/posts/pixelcnn/" rel="noopener ugc nofollow" target="_blank">http://bjlkeng.github.io/posts/pixelcnn/</a></li><li id="0cfa" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://jrbtaylor.github.io/conditional-pixelcnn/" rel="noopener ugc nofollow" target="_blank">https://jrbtaylor.github.io/conditional-pixelcnn/</a></li><li id="19b6" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="http://www.gatsby.ucl.ac.uk/~balaji/Understanding-GANs.pdf" rel="noopener ugc nofollow" target="_blank">http://www.gatsby.ucl.ac.uk/~balaji/Understanding-GANs.pdf</a></li><li id="ee4e" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://www.cs.ubc.ca/~lsigal/532S_2018W2/Lecture13b.pdf" rel="noopener ugc nofollow" target="_blank">https://www.cs.ubc.ca/~lsigal/532S_2018W2/Lecture13b.pdf</a></li><li id="7e17" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://tinyclouds.org/residency/" rel="noopener ugc nofollow" target="_blank">https://tinyclouds.org/residency/</a></li><li id="8e04" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://tensorflow.blog/2016/11/29/pixelcnn-1601-06759-summary/" rel="noopener ugc nofollow" target="_blank">https://tensor flow . blog/2016/11/29/pixel CNN-1601-06759-summary/</a></li><li id="4e0e" class="md me iq kq b kr mo ku mp kx mq lb mr lf ms lj op ml mm mn bi translated"><a class="ae lk" href="https://web.cs.hacettepe.edu.tr/~aykut/classes/spring2018/cmp784/slides/lec10-deep_generative_models-part-I_2.pdf" rel="noopener ugc nofollow" target="_blank">https://web . cs . hacettepe . edu . tr/~ ay Kut/classes/spring 2018/CMP 784/slides/le C10-deep _ generative _ models-part-I _ 2 . pdf</a></li></ul></div></div>    
</body>
</html>