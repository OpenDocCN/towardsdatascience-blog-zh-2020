<html>
<head>
<title>Day 124: NLP Papers Summary — TLDR: Extreme Summarization of Scientific Documents</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第124天:NLP论文摘要——TLDR:科学文献的极端摘要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-124-nlp-papers-summary-tldr-extreme-summarization-of-scientific-documents-106cd915f9a3?source=collection_archive---------60-----------------------#2020-05-03">https://towardsdatascience.com/day-124-nlp-papers-summary-tldr-extreme-summarization-of-scientific-documents-106cd915f9a3?source=collection_archive---------60-----------------------#2020-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="eb25" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="8a87" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">NLP论文摘要是我总结NLP研究论文要点的系列文章</h2></div><p id="ee31" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我在2020年每天记录我的NLP学习旅程的地方。在这里，你可以随意查看我在过去的270天里学到了什么。在本文的最后，你可以找到以前的论文摘要，按自然语言处理领域分类:)</p><p id="e980" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的NLP论文是<strong class="lf jp"> <em class="lz"> TLDR:科学文献的极端摘要</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="cca2" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="ea6b" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">介绍了TLDR生成任务和SCITLDR，这是一个新的极端摘要数据集，研究人员可以使用它来训练模型，为科学论文生成TLDR。引入注释协议，使用同行评审意见创建不同的基本事实摘要，允许我们扩展数据集，并且第一次有多个摘要链接到单个源文档。最后，我们提出了一个基于TLDR和标题生成的多任务训练策略来适应我们的预训练语言模型BART。这已经显示出优于提取和抽象基线。</p><h1 id="5676" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">TLDR生成任务介绍</h1><p id="2d10" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">TLDR生成任务旨在生成忽略背景或方法细节的tldr，并更加关注关键方面，如论文的贡献。这要求模型具有背景知识以及理解特定领域语言的能力。下图展示了TLDR任务的一个示例，以及出现在TLDR的信息类别列表。</p><div class="nj nk nl nm gt ab cb"><figure class="nn iv no np nq nr ns paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><img src="../Images/15e8adeb8a3187c255c6673aa205dc30.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/0*nEmw3igz2QPrciD6.png"/></div></figure><figure class="nn iv nx np nq nr ns paragraph-image"><img src="../Images/c4699587ba9e0c0d4e332b0ff8e3df85.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/0*UN_S-QLk23J7pozi.png"/><p class="iy iz gj gh gi ja jb bd b be z dk ny di nz oa translated">TLDR极限汇总任务简介[1]</p></figure></div><h1 id="d096" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">SCITLDR数据集</h1><p id="0cb0" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">SCITLDR在计算机科学科学文献中有3935个TLDR。SCITLDR包括论文原作者和同行评议撰写的TLDR。然而，这里的关键区别在于，作者和同行评审是基于评审者的评论而不是原始研究论文来撰写TLDR的。这种方法假设读者有很好的背景知识来了解一般的研究领域，所以我们的TLDRs可以省略常见的概念。此外，审稿人的评论是由该领域的专家撰写的，因此它们是高质量的摘要。下图展示了注释过程的一个例子。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/f08ccaf0d4f1d93e629c3950469c3ead.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/0*vmjmxIhFke6Ai1b1.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">SCITLDR数据集的注释过程[1]</p></figure><p id="401d" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">SCITLDR的独特性之一是，测试集中的每篇论文都映射到多个基础事实TLDR，一个由原作者编写，其余由同行评审。这将a)允许我们更好地评估我们生成的摘要，因为现在有多个基本事实摘要来计算ROUGE分数，b)拥有作者和读者的TLDR允许我们基于读者的视角捕捉摘要中的变化。</p><h2 id="728b" class="oc mi jf bd mj od oe dn mn of og dp mr lm oh oi mt lq oj ok mv lu ol om mx jl bi translated">数据集分析</h2><p id="10a0" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">首先，SCITLDR是一个小得多的数据集，由于手动数据收集和注释，只有3.2K的论文。其次，与其他数据集相比，SCITLDR具有极高的压缩比。平均文档长度是5009，它被压缩成19的平均摘要长度。这使得总结非常具有挑战性。表3展示了这些汇总统计数据。SCITLDR对于测试集中的每篇论文至少有两个基础事实TLDR，因此我们研究不同基础事实tldr之间的ROUGE分数差异。作者生成的tldr和PR生成的tldr之间存在较低的ROUGEE-1重叠(27.40)。作者生成的TLDRs的ROUGE-1为34.1，标题为论文。PR生成的TLDRs只有24.7的ROUGE-1。这展示了多个基础事实TLDRs在总结中的重要性，因为一个源文件可能有多个相关的总结。</p><div class="nj nk nl nm gt ab cb"><figure class="nn iv on np nq nr ns paragraph-image"><img src="../Images/d850ddca127c1057cf1ac121ddc01a4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/0*gWC9mXx9jXxBdUqc.png"/></figure><figure class="nn iv oo np nq nr ns paragraph-image"><img src="../Images/0dd49d3c8cf6f0970c27f18aed31371c.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/0*qUh5EeRYj6TAFQFh.png"/><p class="iy iz gj gh gi ja jb bd b be z dk op di oq oa translated">左:数据集比较|右:摘要的新颖程度[1]</p></figure></div><h1 id="5907" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">实验设置和结果</h1><h2 id="d7a7" class="oc mi jf bd mj od oe dn mn of og dp mr lm oh oi mt lq oj ok mv lu ol om mx jl bi translated">模特培训</h2><p id="b873" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们微调了巴特模型来生成TLDR。但是，限制很少。首先，我们训练数据的大小。我们有一个小数据集来训练神经网络。这使我们从arXiv收集了额外的20K论文标题对，并对我们的SCITLDR进行了采样，以匹配新的卷。我们收集标题的原因是因为它通常包含关于论文的重要信息，我们相信如果我们训练模型也执行标题生成，它将学习如何从论文中选择重要信息。有了新的信息，我们就可以训练我们的模型了。首先，我们在XSUM数据集上训练BART-large模型，这是一个通用新闻领域的极端概括数据集。然后，我们将在SCITLDR和title数据集上微调我们的BART模型。</p><p id="99a8" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们面临的第二个限制是，BART对输入长度有限制，因此我们将BART放在两个设置下:BART_abstract (SCITLDR_Abst)和BART _ abstract _ intro _结论(SCITLDR_AIC)。这些是用于生成标题/TLDR的不同输入。现有的研究表明，研究论文中最重要的信息是摘要、引言和结论。</p><h2 id="ec37" class="oc mi jf bd mj od oe dn mn of og dp mr lm oh oi mt lq oj ok mv lu ol om mx jl bi translated">模型比较</h2><ol class=""><li id="2d37" class="or os jf lf b lg mz lj na lm ot lq ou lu ov ly ow ox oy oz bi translated"><em class="lz">提取模型</em>。PACSUM(text rank的非监督扩展)和BERTSUMEXT(监督扩展)</li><li id="09c4" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly ow ox oy oz bi translated"><em class="lz">抽象模型</em>。BART的不同变体</li></ol><p id="9f5f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们使用ROUGE指标进行评估。我们将计算每个地面实况TLDRs的胭脂分数，并选择最大值。</p><h2 id="d995" class="oc mi jf bd mj od oe dn mn of og dp mr lm oh oi mt lq oj ok mv lu ol om mx jl bi translated">结果</h2><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi pf"><img src="../Images/00df6f76fd8b707c8bed45bef79f7200.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XcUfKjpg-o_ittJR.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">SCITLDR的总体结果，包括AIC和Abst版本[1]</p></figure><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/69c7cce617efb0ff03bd81e61de63d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/0*Pzeab7HIlASr23F-.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">提取摘要的上限[1]</p></figure><p id="e04a" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">提取预言提供了一个上限性能。在表6中，我们可以看到随着输入空间的增加，ROUGE得分不断增加。具体来说，当包括介绍和结论作为输入时，有5个ROUGE分数的提高，展示了它们在生成有用的摘要中的重要性。虽然从《AIC》到《全文》的胭脂评分有所提高，但提高幅度并不大，说明论文其他部分的附加值没有《AIC》高。</p><p id="c1c2" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在表5中，我们可以看到BART在原始SCITLDR上的微调足以胜过其他提取和抽象基线。在XSUM上预训练BART时显示了进一步的改进，但是，这种改进只适用于SCITLDR_AIC。我们的多任务学习策略已经超越了所有基线模型，并在BART + XSUM的基础上实现了进一步的改进。这展示了为标题和TLDR一代培训模型的附加值。下图展示了由不同模型生成的摘要的定性示例。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi ph"><img src="../Images/a6e9fe1192ecb2c26e5dad1aa2816343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gqwzlNRwdMU6tVs3.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">BART tldr的定性示例[1]</p></figure><h1 id="c50a" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">结论和未来工作</h1><p id="55d8" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">潜在的未来工作可以利用整篇论文的信息，捕捉更多的背景。此外，我们可以明确地对读者的背景知识建模，根据读者是谁来创建TLDRs。最后，我们可以将我们的注释过程应用于其他数据集，并将任何同行评审意见转换为TLDRs摘要。</p><h2 id="0cc9" class="oc mi jf bd mj od oe dn mn of og dp mr lm oh oi mt lq oj ok mv lu ol om mx jl bi translated">来源:</h2><p id="cae8" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">[1]卡舒拉，I .，罗，k .，科汉，a .和韦尔德，D.S .，2020年。TLDR:科学文献的极端摘要。<em class="lz"> arXiv预印本arXiv:2004.15011 </em>。</p><p id="053b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于2020年5月3日https://ryanong.co.uk</em><em class="lz"/><a class="ae jc" href="https://ryanong.co.uk/2020/05/03/day-124-nlp-papers-summary-tldr-extreme-summarization-of-scientific-documents/" rel="noopener ugc nofollow" target="_blank"><em class="lz">。</em></a></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="f8c3" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="eecc" class="or os jf lf b lg mz lj na lm ot lq ou lu ov ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="3bf3" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sense-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="904e" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitive-analysis-f 24 a2 EC 1 ca 32</a></li><li id="94ee" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="ef74" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towards data science . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li><li id="8b6c" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towardsdatascience . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li><li id="0d55" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5">https://towards data science . com/day-112-of-NLP 365-NLP-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b 7 a5 e 245 b5</a></li><li id="226b" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-123-of-nlp365-nlp-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f998d1131">https://towards data science . com/day-123-of-NLP 365-NLP-papers-summary-context-aware-embedding-for-targeted-aspect-based-be9f 998d 1131</a></li></ul><h1 id="00a9" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">总结</h1><ul class=""><li id="74de" class="or os jf lf b lg mz lj na lm ot lq ou lu ov ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="3b45" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1 b 27</a></li><li id="ad4a" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-113-of-nlp365-nlp-papers-summary-on-extractive-and-abstractive-neural-document-87168b7e90bc">https://towards data science . com/day-113-of-NLP 365-NLP-papers-summary-on-extractive-and-abstract-neural-document-87168 b 7 e 90 BC</a></li><li id="88a8" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-116-of-nlp365-nlp-papers-summary-data-driven-summarization-of-scientific-articles-3fba016c733b">https://towards data science . com/day-116-of-NLP 365-NLP-papers-summary-data-driven-summary-of-scientific-articles-3 FBA 016 c 733 b</a></li><li id="7b1e" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-117-of-nlp365-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge-61ae6cdf32f">https://towards data science . com/day-117-of-NLP 365-NLP-papers-summary-abstract-text-summary-a-low-resource-challenge-61 AE 6 CDF 32 f</a></li><li id="349a" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-118-of-nlp365-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-aea118a5eb3f">https://towards data science . com/day-118-of-NLP 365-NLP-papers-summary-extractive-summary-of-long-documents-by-combining-AEA 118 a5 eb3f</a></li><li id="4d3a" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-120-of-nlp365-nlp-papers-summary-a-simple-theoretical-model-of-importance-for-summarization-843ddbbcb9b">https://towards data science . com/day-120-of-NLP 365-NLP-papers-summary-a-simple-theory-model-of-importance-for-summary-843 ddbcb 9b</a></li><li id="fc65" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-121-of-nlp365-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization-cd55e577f6de">https://towards data science . com/day-121-of-NLP 365-NLP-papers-summary-concept-pointer-network-for-abstract-summary-cd55e 577 F6 de</a></li></ul><h1 id="9ddf" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">其他人</h1><ul class=""><li id="279f" class="or os jf lf b lg mz lj na lm ot lq ou lu ov ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li><li id="e84d" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-111-of-nlp365-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection-bff7f5f20ce5">https://towards data science . com/day-111-of-NLP 365-NLP-papers-summary-the-risk-of-race-of-bias-in-hate-speech-detection-BFF 7 F5 f 20 ce 5</a></li><li id="a0bb" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-115-of-nlp365-nlp-papers-summary-scibert-a-pretrained-language-model-for-scientific-text-185785598e33">https://towards data science . com/day-115-of-NLP 365-NLP-papers-summary-scibert-a-pre trained-language-model-for-scientific-text-185785598 e33</a></li><li id="1c6b" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-119-nlp-papers-summary-an-argument-annotated-corpus-of-scientific-publications-d7b9e2ea1097">https://towards data science . com/day-119-NLP-papers-summary-an-argument-annoted-corpus-of-scientific-publications-d 7 b 9 e 2e ea 1097</a></li><li id="2d3e" class="or os jf lf b lg pa lj pb lm pc lq pd lu pe ly pi ox oy oz bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-122-of-nlp365-nlp-papers-summary-applying-bert-to-document-retrieval-with-birch-766eaeac17ab">https://towards data science . com/day-122-of-NLP 365-NLP-papers-summary-applying-Bert-to-document-retrieval-with-birch-766 EAC 17 ab</a></li></ul></div></div>    
</body>
</html>