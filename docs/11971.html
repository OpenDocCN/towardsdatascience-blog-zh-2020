<html>
<head>
<title>The Aha! Moments In 4 Popular Machine Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">啊哈！4 种流行的机器学习算法中的矩</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-aha-moments-in-4-popular-machine-learning-algorithms-f7e75ef5b317?source=collection_archive---------47-----------------------#2020-08-18">https://towardsdatascience.com/the-aha-moments-in-4-popular-machine-learning-algorithms-f7e75ef5b317?source=collection_archive---------47-----------------------#2020-08-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/2a50feb616e73882b46623b98000733f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTNfcUC6FE3dtbsLvlAQCg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://pixabay.com/illustrations/light-bulb-think-idea-solution-2010022/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><div class=""/><div class=""><h2 id="a1d6" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">凭直觉知道为什么，而不仅仅是如何</h2></div><p id="27cf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大多数人要么在两个阵营:</p><ul class=""><li id="7981" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">我不懂这些机器学习算法。</li><li id="13d2" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">我明白算法是如何工作的，但不明白<em class="mi">为什么</em>它们会工作。</li></ul><p id="1f35" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章不仅试图解释<em class="mi">的</em>算法是如何工作的，而且让人们直观地理解<em class="mi">为什么</em>会工作，让人们恍然大悟！瞬间。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="3c54" class="mq mr jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">决策树</h2><p id="ab47" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">决策树使用水平线和垂直线划分特征空间。例如，考虑下面一个非常简单的决策树，它有一个条件节点和两个类节点，指示一个条件以及满足该条件的训练点将属于哪个类别。</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/e238735fc032bd819343b3a4425b9f98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rllGKaKh_LPuDTaszhzh_g.png"/></div></div></figure><p id="b140" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，标记为每种颜色的字段和该区域内实际上是该颜色的数据点之间有很多重叠，或者说(粗略地说)<em class="mi">熵</em>。构造决策树以最小化熵。在这种情况下，我们可以增加一层复杂性。如果我们再增加一个条件。如果<em class="mi"> x </em>小于 6 <em class="mi">而</em>y<em class="mi">大于 6，我们可以将该区域的点指定为红色。这一举动降低了熵。</em></p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/55056a2edca44762c609d6a59ed291a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LaC4FKzf9Xzfcjah4-RGww.png"/></div></div></figure><p id="3eb4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每一步，决策树算法试图找到一种方法来建立树，使熵最小化。更正式地认为熵是某个分割器(条件)的“无序”或“混乱”的数量，它的反义词是“信息增益”——一个分割器<em class="mi">给模型增加了多少</em>信息和洞察力。具有最高信息增益(以及最低熵)的特征分割被放置在顶部。</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/889aa1fae221a374f743d1566836cdfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JSvd8bfjU-MJI5hx.png"/></div></div></figure><p id="325a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些条件可能会像这样分割它们的一维特征:</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/fb0654b10e0d6f350d095b559e812a62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qFSusuOf699Iu2JL.png"/></div></div></figure><p id="e982" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，条件 1 具有清晰的分离，因此具有低熵和高信息增益。条件 3 就不一样了，这就是为什么它被放在决策树的底部。这种采油树的结构确保了它尽可能的轻便。</p><p id="aaf5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以阅读更多关于熵及其在决策树和神经网络中的应用(交叉熵作为损失函数)<a class="ae jg" rel="noopener" target="_blank" href="/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3">这里</a>。</p><h2 id="e266" class="mq mr jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">随机森林</h2><p id="e307" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">随机森林是决策树的袋装(引导聚合)版本。主要的想法是，几个决策树分别在一个数据子集上进行训练。然后，一个输入通过每个模型，它们的输出通过一个函数(如 mean)聚合，产生一个最终输出。打包是集成学习的一种形式。</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/ea0ed35a7c68babe9611853c90759e71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*L3Ht0A5bMX1_M8rG.png"/></div></div></figure><p id="8433" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为什么随机森林运行良好，有许多相似之处。下面是一个常见的版本:</p><blockquote class="nx ny nz"><p id="3eb1" class="ky kz mi la b lb lc kk ld le lf kn lg oa li lj lk ob lm ln lo oc lq lr ls lt im bi translated">你需要决定下一步去哪家餐馆。要向某人寻求建议，你必须回答各种是/否的问题，这将引导他们决定你应该去哪家餐馆。</p><p id="7f84" class="ky kz mi la b lb lc kk ld le lf kn lg oa li lj lk ob lm ln lo oc lq lr ls lt im bi translated">你是宁愿只问一个朋友还是问几个朋友，然后找到模式还是普遍共识？</p></blockquote><p id="7759" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除非你只有一个朋友，否则大多数人会选择第二个。这个类比提供的见解是，每棵树都有某种“思想的多样性”，因为它们是在不同的数据上训练的，因此有不同的“经验”。</p><p id="2607" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个类比虽然简单明了，但我从未真正注意过。在现实世界中，单一朋友选项的经验少于所有朋友的总和，但在机器学习中，决策树和随机森林模型是在相同的数据上训练的，因此，有相同的经验。集合模型实际上没有接收任何新的信息。如果我可以请一位无所不知的朋友给我推荐，我不反对。</p><p id="8fae" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个在相同数据上训练的模型，随机抽取数据的子集来模拟人为的“多样性”,怎么能比一个在整个数据上训练的模型表现得更好呢？</p><p id="deb3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">取一个带有严重正态分布噪声的正弦波。这是你的单一决策树分类器，它自然是一个非常高方差的模型。</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/cc101fd466ae524c30b04e31ea0014a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gAcL-yWMMmNZpKhN.png"/></div></div></figure><p id="a64d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将选择 100 个“近似值”。这些近似器沿着正弦波随机选择点，并生成正弦曲线拟合，就像决策树在数据子集上进行训练一样。这些拟合然后被平均以形成袋装曲线。结果呢？—更平滑的曲线。</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oe"><img src="../Images/54c3fcabf96dc3e428489ccfe6553194.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S8AY1LbR6jBN_4qP.png"/></div></div></figure><p id="eaa8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">bagging 之所以有效，是因为它减少了模型的方差，并通过人为地使模型更加“自信”来帮助提高概括能力。这也是为什么 bagging 在像逻辑回归这样的低方差模型上效果不佳。</p><p id="c5ae" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在这里阅读更多关于直觉和更严格的证据来证明<a class="ae jg" rel="noopener" target="_blank" href="/how-injecting-randomness-can-improve-model-accuracy-11cdc04b3eeb">装袋</a>的成功。</p><h2 id="362d" class="mq mr jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">支持向量机</h2><p id="dabd" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">支持向量机试图找到一个可以最好地划分数据的超平面，依靠“支持向量”的概念来最大化两个类之间的划分。</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/6edbd5aab1e8297f5bfa2197fa06e550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PqV4e-y00WDQoOeckJ5Bfg.png"/></div></div></figure><p id="76bf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不幸的是，大多数数据集不那么容易分离，如果是这样，SVM 可能不是处理它的最佳算法。考虑这个一维分离任务；没有好的分割线，因为任何一个分割线都会导致两个独立的类被归入同一个类。</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi og"><img src="../Images/f994d5738de2812f9711e820a1782844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JvM8WZQZlGtmEeKR_AKxzA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">一个分裂的提议。</p></figure><p id="6c18" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">SVM 通过使用一种所谓的“内核技巧”在解决这类问题方面非常强大，这种技巧将数据投影到新的维度，使分离任务变得更容易。例如，让我们创建一个新的维度，它被简单地定义为<em class="mi"> x </em> ( <em class="mi"> x </em>是原始维度):</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oh"><img src="../Images/be4c0249b36f8a3b563a01ab5b181661.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TSXzZRLxeByCvhNZbBqieA.png"/></div></div></figure><p id="abcf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，在数据被投影到一个新的维度上之后，数据是完全可分离的(每个数据点在两个维度中表示为<code class="fe oi oj ok ol b">(<em class="mi">x</em>, <em class="mi">x</em>²)</code>)。</p><p id="d987" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用各种内核，最常见的是多项式、sigmoid 和 RBF 内核，内核技巧完成了创建变换空间的繁重工作，从而使分离任务变得简单。</p><h2 id="4f47" class="mq mr jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">神经网络</h2><p id="c6b0" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">神经网络是机器学习的巅峰。他们的发现，以及可以对其进行的无限变化和改进，保证了它成为自己领域的主题，即深度学习。诚然，神经网络的成功仍然是不完整的(“神经网络是没有人理解的矩阵乘法”)，但解释它们最简单的方法是通过通用逼近定理(UAT)。</p><p id="96f1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在它们的核心，每一个被监督的算法都试图模拟数据的一些潜在功能；通常这是回归平面或特征边界。考虑这个函数<em class="mi"> y </em> = <em class="mi"> x </em>，它可以用几个水平步长建模到任意精度。</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/0c498c3be71086620704765859bb282b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*esz_o4ONw0egM7p_.png"/></div></div></figure><p id="ed47" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这本质上是一个神经网络所能做的。也许它可以更复杂一点，并超越水平步骤(如下面的二次和线性线)来模拟关系，但在其核心，神经网络是一个分段函数逼近器。</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/c236a482bbf7c3a68542cac118b84c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JAg7DmTZzgH1-pvp.png"/></div></div></figure><p id="26a6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个节点被委托给分段函数的一部分，网络的目的是激活负责部分特征空间的某些神经元。例如，如果要对有胡子或没有胡子的人的图像进行分类，应该将几个节点专门分配给胡子经常出现的像素位置。在多维空间的某个地方，这些节点代表一个数值范围。</p><p id="ffdb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">再次注意，“神经网络为什么工作”这个问题仍然没有答案。UAT 没有回答这个问题，但指出，在某些人类解释下，神经网络可以模拟任何功能。可解释/可解释的人工智能领域正在兴起，通过像<a class="ae jg" href="https://medium.com/analytics-vidhya/every-ml-engineer-needs-to-know-neural-network-interpretability-afea2ac0824e" rel="noopener">激活最大化和灵敏度分析</a>这样的方法来回答这些问题。</p><p id="9bdf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以阅读更深入的解释，并查看通用近似定理的可视化<a class="ae jg" href="https://medium.com/analytics-vidhya/you-dont-understand-neural-networks-until-you-understand-the-universal-approximation-theorem-85b3e7677126" rel="noopener">在这里</a>。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="92c3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在所有这四种算法以及许多其他算法中，这些算法在低维度上看起来非常简单。机器学习的一个关键认识是，我们声称在人工智能中看到的许多“魔法”和“智能”实际上是隐藏在高维度伪装下的简单算法。</p><p id="d799" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">决策树把区域分割成正方形很简单，但是决策树把高维空间分割成超立方体就没那么简单了。SVM 执行一个内核技巧来提高一维到二维的可分性是可以理解的，但 SVM 在数百维的数据集上做同样的事情几乎是神奇的。</p><p id="6475" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们对机器学习的钦佩和困惑是基于我们对高维空间的缺乏理解。学习如何避开高维空间和理解自然空间中的算法有助于直观理解。</p><div class="is it gp gr iu oo"><a rel="noopener follow" target="_blank" href="/the-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jk gy z fp ot fr fs ou fu fw ji bi translated">神经网络优化的迷人的无梯度方法</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">忘记亚当，阿达格勒，SGD</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc ja oo"/></div></div></a></div><p id="2e7a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有图片均由作者创作。</p></div></div>    
</body>
</html>