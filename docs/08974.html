<html>
<head>
<title>SpaceX Falcon 9 Landing with RL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SpaceX 猎鹰 9 号与 RL 一起着陆</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spacex-falcon-9-landing-with-rl-7dde2374eb71?source=collection_archive---------18-----------------------#2020-06-28">https://towardsdatascience.com/spacex-falcon-9-landing-with-rl-7dde2374eb71?source=collection_archive---------18-----------------------#2020-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0b33" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">可重复使用的火箭</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/875c66b91f16d41e642ed30563d8ab3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gnVtfUTP_QMrlN4hnky68g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://cosmosmagazine.com/space/launch-land-repeat-reusable-rockets-explained/#:~:text=The%20Falcon%209%20first%2Dstage,Fuel%20tanks%3A&amp;text=These%20engines%20fire%20again%20as%20the%20rocket%20nears%20the%20landing%20platform." rel="noopener ugc nofollow" target="_blank">cosmosmagazine.com</a>了解更多关于火箭的信息</p></figure><p id="60cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由航天公司 SpaceX 开发的猎鹰 9 号意味着现在可以通过安全返回地球来重复使用火箭的第一级。</p><p id="18b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一项曾经看起来如此不可能的成就导致了多个“假 SpaceX 着陆视频-解释”的创建，现在人们普遍同意相关技术背后的惊人之处。</p><p id="8198" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然今天我不在这里，也不能给你们讲火箭工程课程，但我只想展示一下来自 SpaceX 的这个小图表，以便更好地理解。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/cf09e14f31ce9625b394925c4db686a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T-yIJQth30F3sL2QhTz3YA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://www.inverse.com/article/33904-how-spacex-lands-a-falcon-9-rocket-in-6-steps" rel="noopener ugc nofollow" target="_blank">你可以在这里查看更多关于 https://www.youtube.com/watch?v=Wn5HxXKQOjw</a><a class="ae kv" href="https://www.youtube.com/watch?v=Wn5HxXKQOjw" rel="noopener ugc nofollow" target="_blank"/>的视频</p></figure><p id="fb69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然没有任何人工智能技术被部署到任何一个<strong class="ky ir"> SpaceX </strong>技术管道中(他们正在使用经典的机器人/控制理论方式的<a class="ae kv" href="https://en.wikipedia.org/wiki/Motion_planning" rel="noopener ugc nofollow" target="_blank">路径规划</a>算法)，但如果我们试图用<em class="lt">最先进的</em> <a class="ae kv" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> <em class="lt">强化学习</em></strong></a><strong class="ky ir"><em class="lt"/></strong>算法来解决这个问题，那将会发生什么。</p><h1 id="c33d" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">环境</h1><p id="7200" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">如果你是 RL 新手，我建议你去看看其他的教程、书籍和资源，这些都是为初学者准备的，是关于学习更多关于 RL 的基础知识和数学知识的。</p><p id="268c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">亲爱的斯文·尼德伯格(<em class="lt"> @EmbersArc on GitHub </em>)前段时间创造了这个艰苦的强化学习环境。虽然在创作中，它的主要目的是利用 LunarLander envs 的想法创建一个漂亮的有吸引力/炒作的健身房一样的环境，但人们很快意识到，就 RL 而言，解决它比它的灵感要困难得多。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/f3388cb6787c668ba3b438dceeeca15d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*01PHi8ZCva2CKiErdThQ8g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">LunarLander 是如何工作的，<a class="ae kv" href="https://mc.ai/solving-lunar-lander-openaigym-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank"> ref mc.ai </a></p></figure><p id="1b83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">LunarLander 有两个版本。一个为<strong class="ky ir">设计的离散动作空间</strong></p><ul class=""><li id="67f8" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">0–什么都不做</li><li id="662d" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><code class="fe ng nh ni nj b">1</code>–点燃左侧发动机</li><li id="f84d" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><code class="fe ng nh ni nj b">2</code>–关闭发动机</li><li id="547b" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><code class="fe ng nh ni nj b">3</code>–右引擎点火</li></ul><p id="c70f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其他为<strong class="ky ir">连续动作空间</strong></p><p id="329e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作用是从-1 到+1 的两个实值向量。</p><ul class=""><li id="89ec" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">首先控制主发动机，-1..0 关闭，0..+1 从 50%到 100%功率节流。低于 50%的功率，发动机无法工作。</li><li id="5ec8" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">第二个值-1.0..-0.5 火左引擎，+0.5..+1.0 点火右发动机，-0.5..0.5 折。</li></ul><p id="bad1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然离散动作 1 可以很容易地用基于值的方法加上非线性函数近似来解决，如 DQN、彩虹 DQN 或大猩猩 DQN，但连续动作问题需要某种基于演员-评论家的算法才能工作，因为普通 DQN 方法的回报少、探索难且不稳定。</p><p id="c711" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> PPO+LSTM +平行培养</strong>(上策，集群式培养)<strong class="ky ir">软演员评论</strong> (SAC，关策)<strong class="ky ir"/><strong class="ky ir">D4PG</strong>(关策-混合演员评论法)可以用来解决这个问题。特别是如果你想了解政策方法，我强烈建议你看一看并行化和 LSTMs(或者成为 SOTA 的注意机制？)与 RL 的一般情况，但这是另一天的主题。</p><p id="d6e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的火箭健身房也使用了 LunarLander 设置中的大部分东西，并且它的结构高度可定制。它使用 Box2D 作为物理后端，openGL 用于环境的光照渲染(图像不用于观察，只是为了检查你的进度)</p><h2 id="72bf" class="nk lv iq bd lw nl nm dn ma nn no dp me lf np nq mg lj nr ns mi ln nt nu mk nv bi translated">状态变量</h2><p id="4578" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">状态由以下变量组成:</p><ul class=""><li id="a342" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">x 位置</li><li id="c8d0" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">y 位置</li><li id="f9ef" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">角</li><li id="e2e6" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">第一腿地面接触指示器</li><li id="c316" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">第二腿地面接触指示器</li><li id="3bef" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">喉咙</li><li id="95b0" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">发动机万向节</li></ul><p id="869d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果<strong class="ky ir"> VEL 状态</strong>(代码中)设置为真，速度包括:</p><ul class=""><li id="85fb" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">x 速度</li><li id="766e" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">y 速度</li><li id="8d86" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">角速度</li></ul><p id="47e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">观察</strong>非常有用<strong class="ky ir"> <em class="lt">从物理模拟中提取特征</em> </strong>这样你就不会浪费你大部分的计算能力在<em class="lt"> CNN 的</em>上来提取它们，因此我们的目的是创造一个有意义的<strong class="ky ir"> <em class="lt">机器人/控制算法替代</em> </strong>而不是一个完全成熟的系统。</p><p id="35c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我已经尝试了这两个选项，虽然没有很大的区别，我建议你打开速度状态以获得更多信息，这有助于代理学习，因此<strong class="ky ir"> <em class="lt">更多相关信息更好的心态</em> </strong>。请注意，为了训练过程的顺利进行，所有值都已标准化。</p><p id="a345" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">奖励计算</strong>在这一小段代码中有解释。基本上需要燃料成本和你与地面的相对位置以及火箭的重量来了解你的着陆在控制力学和物理学方面有多正确。因为你可能不会因为随机产卵而得到一个轻松的局面，但是如果你仍然从那个状态中恢复过来，你应该会得到一个不错的奖励信号</p><pre class="kg kh ki kj gt nw nj nx ny aw nz bi"><span id="4b75" class="nk lv iq nj b gy oa ob l oc od"># state variables for reward<br/> distance = np.linalg.norm((3 * x_distance, y_distance)) # weight x position more<br/> speed = np.linalg.norm(vel_l)<br/> groundcontact = self.legs[0].ground_contact or self.legs[1].ground_contact<br/> brokenleg = (self.legs[0].joint.angle &lt; 0 or self.legs[1].joint.angle &gt; -0) and groundcontact<br/> outside = abs(pos.x — W / 2) &gt; W / 2 or pos.y &gt; H<br/> fuelcost = 0.1 * (0 * self.power + abs(self.force_dir)) / FPS<br/> landed = self.legs[0].ground_contact and self.legs[1].ground_contact and speed &lt; 0.1<br/> done = False</span><span id="6394" class="nk lv iq nj b gy oe ob l oc od">reward = -fuelcost</span><span id="7b94" class="nk lv iq nj b gy oe ob l oc od">if outside or brokenleg:<br/> self.game_over = True</span><span id="aa7a" class="nk lv iq nj b gy oe ob l oc od">if self.game_over:<br/> done = True<br/> else:<br/> # reward shaping<br/> shaping = -0.5 * (distance + speed + abs(angle) ** 2)<br/> shaping += 0.1 * (self.legs[0].ground_contact + self.legs[1].ground_contact)<br/> if self.prev_shaping is not None:<br/> reward += shaping — self.prev_shaping<br/> self.prev_shaping = shaping</span><span id="1d70" class="nk lv iq nj b gy oe ob l oc od">if landed:<br/> self.landed_ticks += 1<br/> else:<br/> self.landed_ticks = 0<br/> if self.landed_ticks == FPS:<br/> reward = 1.0<br/> done = True</span><span id="6968" class="nk lv iq nj b gy oe ob l oc od">if done:<br/> reward += max(-1, 0–2 * (speed + distance + abs(angle) + abs(vel_a)))<br/> elif not groundcontact:<br/> reward -= 0.25 / FPS</span><span id="8cdb" class="nk lv iq nj b gy oe ob l oc od">reward = np.clip(reward, -1, 1)</span></pre><h2 id="aadc" class="nk lv iq bd lw nl nm dn ma nn no dp me lf np nq mg lj nr ns mi ln nt nu mk nv bi translated">控制输入</h2><p id="4aa8" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><em class="lt">离散控制输入</em>是:</p><ul class=""><li id="c30b" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">万向节向左</li><li id="dfe1" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">万向节向右</li><li id="510b" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">加大油门</li><li id="767b" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">节流</li><li id="7477" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">使用第一控制推进器</li><li id="e861" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">使用第二控制推进器</li><li id="2eba" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">没有行动</li></ul><p id="056f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lt">连续控制输入</em>为:</p><ul class=""><li id="4db4" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">万向节(左/右)</li><li id="20e2" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">油门(开/关)</li><li id="802d" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">控制推进器(左/右)</li></ul><h1 id="f3d1" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">代理的创建</h1><p id="9f5c" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">这个问题我试过 3 种算法。<strong class="ky ir"> D4PG、SAC 和 PPO </strong>做了一些修改。我将简要地谈谈每一次经历和它们的结果，你将能够在资源库中找到所有 3 个的完整代码。我的所有代理都完成了连续控制输入，也可以随时检查离散情况</p><p id="3fb8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于所有的代理，我使用了 PTAN py torch 的 RL 库，它简化了代理的设置处理过程，如日志，网络创建，设置和收集轨迹等等。我鼓励你去看看，因为大部分浪费在样板代码上的时间都是浪费时间，而且容易产生错误代码。虽然 PTan 不是即插即用，但培训有助于实现它的目标。</p><p id="39f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">https://github.com/Shmuma/ptan py torch agent net =<a class="ae kv" href="https://github.com/Shmuma/ptan" rel="noopener ugc nofollow" target="_blank"/></p><h2 id="458e" class="nk lv iq bd lw nl nm dn ma nn no dp me lf np nq mg lj nr ns mi ln nt nu mk nv bi translated">D4PG(分布式深度确定性策略梯度)</h2><p id="3dc0" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">DDPG 和 D4PG 的一个小区别来自于动作探索。我们不使用<strong class="ky ir">奥恩斯坦-乌伦贝克过程</strong>噪声，而是使用来自正态(高斯)分布<em class="lt">的更加<strong class="ky ir">简单的随机噪声。</strong></em>  <em class="lt"> </em>计算 OU 噪声似乎没有提供太多，但它补充了代码，所以我更喜欢 D4PG。</p><pre class="kg kh ki kj gt nw nj nx ny aw nz bi"><span id="bfb6" class="nk lv iq nj b gy oa ob l oc od">GAMMA = 0.99<br/>BATCH_SIZE = 64<br/>LEARNING_RATE = 1e-4<br/>REPLAY_SIZE = 100000<br/>REPLAY_INITIAL = 10000<br/>REWARD_STEPS = 5<br/>TEST_ITERS = 1000<br/>Vmax = 10<br/>Vmin = -10<br/>N_ATOMS = 51<br/>Hyper Parameters for D4PG</span></pre><p id="2cea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然我不小心删除了这次跑步的 TensorBoard 日志，但我实际上训练了 100 万步 D4PG，它似乎没有从失败中收敛，而是停留在一些局部最优值上。我还有一些视频。我仍然包括了它的代码，也许有不同的批量大小和随机种子，你可以更有效地训练。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">上尉，这是个陷阱！阿克巴上将</p></figure><p id="da4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">软演员评论家(SAC) </strong></p><p id="4dff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">SAC 仍然是无模型强化学习中的 SOTA 算法之一。简而言之，它的差异可以表述为</p><p id="1ccb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="lt">熵正则化</em> </strong> =在每个时间戳获得更多与当前步骤策略熵成比例的奖励</p><p id="a06c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="lt">双 Q 绝招</em> </strong> =你有 2 个预测 Q 值的网络，选择其中的最小值进行贝尔曼逼近会使你的训练过程更加平稳。</p><p id="6065" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该算法仍然建立在保守策略迭代(TRPO、PPO 等)的基础上。不仅仅是最大化终身奖励，它还试图最大化熵(外行人眼中的不确定性)。因此，您可以两全其美，能够从过去的经验中有效地学习(经验回放)和从基于策略的方法中建立更稳定的策略学习。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/75f184fb6b68571623796502e1a4c7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*YdfWJdCI9l6YeMHNIFaWGw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">随机化在训练开始时的低回报中扮演了重要角色。</p></figure><p id="02eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">火箭环境</strong>由于随机性，与真实世界的情况相比非常复杂，在每个时间点，火箭都可能处于即使在现实生活中也很难恢复的状态。因此，在评估这种环境时，您应该始终牢记这一点。你可以修改环境，使其更加“<strong class="ky ir"><em class="lt"/></strong>”也就是一个与现实生活相似的着陆过程，它不会紧急着陆，但我使用了默认着陆。过了一段时间，回报开始增加，代理人在大多数情况下都表现良好。这次训练用特斯拉 P100 走了 100 万步(16GB VRAM 和 21 TFlops 的 16 位性能)。</p><pre class="kg kh ki kj gt nw nj nx ny aw nz bi"><span id="c3c7" class="nk lv iq nj b gy oa ob l oc od">GAMMA = 0.99<br/>BATCH_SIZE = 64<br/>LR_ACTS = 1e-4<br/>LR_VALS = 1e-4<br/>REPLAY_SIZE = 100000<br/>REPLAY_INITIAL = 10000<br/>SAC_ENTROPY_ALPHA = 0.1<br/>TEST_ITERS = 10000<br/>Hyper parameters for Soft Actor Critic </span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="22fc" class="nk lv iq bd lw nl nm dn ma nn no dp me lf np nq mg lj nr ns mi ln nt nu mk nv bi translated">PPO —策略近似优化</h2><p id="a247" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">PPO 虽然被一些 SOTA(最先进的)超越，但仍然是非常有用的算法，用于尝试连续控制问题，因为与其他算法相比，它易于实现并且需要改变的超参数非常少。</p><pre class="kg kh ki kj gt nw nj nx ny aw nz bi"><span id="eb53" class="nk lv iq nj b gy oa ob l oc od">GAMMA = 0.99<br/>GAE_LAMBDA = 0.95<br/>TRAJECTORY_SIZE = 4097<br/>LEARNING_RATE_ACTOR = 1e-5<br/>LEARNING_RATE_CRITIC = 1e-4<br/>PPO_EPS = 0.2<br/>PPO_EPOCHES = 10<br/>PPO_BATCH_SIZE = 256<br/>TEST_ITERS = 100000<br/>Hyper Parameters for PPO</span></pre><p id="7297" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这一次我走了一条更大的轨迹，并希望尽可能多地提升我的计算能力。我训练了这个网络六百万步。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/f542a1b32d772b881bea5d8de1c4284c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*mGJ9VDp7p9b2ILGdoK3vjw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">它可以根据卵的轨迹做很好的着陆。</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div></figure><h1 id="63af" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">结论和想法</h1><p id="4e51" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我建议你熟悉一下<strong class="ky ir"> PTan </strong>以便在不同的算法上进行非常简单的实验，它允许你跟踪回报(即使在多个 env 设置上也相当容易地处理终端状态等),因为代码大量使用它们。当然，你也可以在我的 hyper params 中使用你自己的代码。</p><p id="bd22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个有趣的经历，这是一个很难的动态问题，加入了随机因素，你可以修改它，使其不那么随机，更容易解决，显然你也可以让它变得更难。我发现 PPO 在训练中最稳定，但 SAC 在 PPO 之前取得了很好的结果，而 D4PG 在我身上失败了，这可能是由于我的参数选择，请随意尝试您自己的版本并写下结果。</p><p id="ab2d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以访问我的存储库，其中包含准备训练谷歌 Colab 笔记本，本地训练代码和保存的模型/日志。我也强烈建议你去看看 Colab Pro，它可以让你在快速训练机上进行 24 小时训练。在 repository README 中，您可以找到有关如何运行代码的更多信息。</p><div class="oj ok gp gr ol om"><a href="https://github.com/ugurkanates/SpaceXReinforcementLearning" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ir gy z fp or fr fs os fu fw ip bi translated">ugurkanates/spacex 强化学习</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">由于这似乎得到了更多的关注，我已经把它更新到最新版本的健身房。我希望一切…</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">github.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa kp om"/></div></div></a></div></div></div>    
</body>
</html>