<html>
<head>
<title>How to Debug a ML Model: A Step-by-Step Case Study in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何调试ML模型:NLP中的分步案例研究</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-debug-an-ml-model-a-step-by-step-case-study-in-nlp-d79d384f7427?source=collection_archive---------45-----------------------#2020-06-21">https://towardsdatascience.com/how-to-debug-an-ml-model-a-step-by-step-case-study-in-nlp-d79d384f7427?source=collection_archive---------45-----------------------#2020-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="905f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">虽然有这么多文章介绍如何开始学习NLP或向您提供指导，但最难学习的课程之一是如何调试模型或任务实现。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7d4e61ec4546feb77023ed5095946706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XeTIb0-mdTivJKRuEnZ8Og.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型/任务实施前的精神状态，通过Pixabay (CC0)的<a class="ae ky" href="https://pixabay.com/users/StockSnap-894430/" rel="noopener ugc nofollow" target="_blank"> StockSnap </a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c5707cfef9a6aa30a3b7dd7e660d415f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Rj0dUgsY_xQ1KW2vg0RVA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型/任务实施后的精神状态，<a class="ae ky" href="https://pixabay.com/users/Free-Photos-242387/" rel="noopener ugc nofollow" target="_blank">Free-Photo</a>via pix abay(CC0)</p></figure><p id="6129" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不要担心！本文将介绍一系列相当微妙(和不那么微妙)的错误的调试过程，以及我们如何修复它们，并通过一个案例研究来引导您完成这些课程。<strong class="lb iu">如果你想看提示列表，向下滚动到最后！</strong></p><p id="00bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了做到这一点，让我带你回到几个月前，那时我们(我的研究合作者Phu和我)第一次将<a class="ae ky" rel="noopener" target="_blank" href="/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">屏蔽语言建模</a>实现到<a class="ae ky" href="http://jiant.info" rel="noopener ugc nofollow" target="_blank"> jiant </a>中，这是一个开源的NLP框架，目标是在RoBERTa模型上进行多任务训练。如果这听起来像是一种陌生的语言，我建议你先看看这篇关于<a class="ae ky" rel="noopener" target="_blank" href="/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a">迁移学习和多任务学习的文章，</a>和这篇关于<a class="ae ky" href="https://medium.com/towards-artificial-intelligence/a-robustly-optimized-bert-pretraining-approach-f6b6e537e6a6" rel="noopener">罗伯塔模型的文章。</a></p><h2 id="9ac7" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">设置场景</strong></h2><p id="2e71" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">屏蔽语言建模是BERT、RoBERTa和许多BERT风格变体的预训练目标之一。它由一个输入噪声目标组成，其中给定一个文本，该模型必须预测给定上下文的15%的标记。更困难的是，这些预测的标记有80%的时间被替换为“[MASK]”，10%被另一个随机标记替换，10%是正确的、未被替换的标记。</p><p id="f7e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，该模型如下所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/491ab0eecfa9e6e2c1677a239c40b4ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jIfgyuXwBMB7RDAcHst8pw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为MLM培训更改的文本示例。这里，模型将学习预测当前被“[MASK]”占据的令牌的“tail”</p></figure><h2 id="7c5c" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">设计初始实施</h2><p id="4787" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们首先查看了其他人以前是否实现过MLM，发现了Google的最初实现和AllenNLP的Pytorch实现。我们几乎使用了所有的<a class="ae ky" href="https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py" rel="noopener ugc nofollow" target="_blank"> Huggingface实现</a> n(它已经被移动了，因为它看起来像是以前在那里的文件已经不存在了)来实现转发功能。根据RoBERTa的论文，我们在每个时间步动态地屏蔽了批次。此外，Huggingface在这里暴露了预训练的MLM头<a class="ae ky" href="https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm" rel="noopener ugc nofollow" target="_blank">，我们使用如下。</a></p><p id="686b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们代码中的MLM流变成了下面这样:</p><blockquote class="mu mv mw"><p id="6bc3" class="kz la mx lb b lc ld ju le lf lg jx lh my lj lk ll mz ln lo lp na lr ls lt lu im bi translated">加载MLM数据-&gt;预处理和索引数据-&gt;加载模型-&gt;在模型训练的每个步骤中，我们:</p><p id="1be2" class="kz la mx lb b lc ld ju le lf lg jx lh my lj lk ll mz ln lo lp na lr ls lt lu im bi translated">1.动态屏蔽批处理</p><p id="3b2e" class="kz la mx lb b lc ld ju le lf lg jx lh my lj lk ll mz ln lo lp na lr ls lt lu im bi translated">2.计算每个屏蔽令牌的NLL损耗</p></blockquote><p id="9e9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">jiant框架主要使用AllenNLP进行词汇创建和索引，以及实例和数据集管理。</p><p id="65d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先用一个包含100个数据集示例<strong class="lb iu"> </strong>的玩具数据集进行测试，以确保AllenNLP的加载是正确的。在我们经历了一些非常明显的错误之后，比如一些标签类型与AllenNLP不匹配，我们遇到了一个更大的错误。</p><h2 id="8113" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">麻烦的最初迹象</h2><p id="f48a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在确保我们的预处理代码与AllenNLP一起工作后，我们发现了一个奇怪的bug。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="034b" class="lv lw it nc b gy ng nh l ni nj">TypeError: ~ (operator.invert) is only implemented on byte tensors. Traceback (most recent call last):</span></pre><p id="e473" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是因为我们从Huggingface复制粘贴的代码是用旧版本的Python编写的，并且是在您需要使用的Pytorch中。byte()而不是bool()。</p><p id="c2aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们简单地修改了一行，从</p><p id="2d4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nk nl nm nc b">indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() &amp; masked_indices</code></p><p id="c1d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="5a4e" class="lv lw it nc b gy ng nh l ni nj">bernoulli_mask = torch.bernoulli(torch.full(labels.shape, 0.8)).to( device=inputs.device, dtype=torch.uint8 )</span></pre><h2 id="2d1a" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">麻烦来了</h2><p id="e9a3" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在，我们终于能够运行一个正向函数而不出错了！庆祝了几分钟后，我们开始验证更微妙的错误。我们首先通过调用model.eval()并通过MLM转发函数运行模型来测试我们实现的正确性。因为这个模型，在这个例子中是RoBERTa-large，已经在MLM进行了预训练，我们希望它在MLM会有很好的表现。事实并非如此，我们损失惨重。</p><p id="0f01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">原因很明显:预测总是与黄金标签相差2%。例如，如果标记“tail”被分配了索引25，那么“狗看到零食时摆动它的[面具]”和“[面具]”的标签将是25，但是预测将是27。</p><p id="bd2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们是打了这个错误才发现的。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="a15d" class="lv lw it nc b gy ng nh l ni nj">`/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [19,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.`</span></pre><p id="5c9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个错误意味着预测空间大于类的数量。</p><p id="aa47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">经过大量的pdb跟踪，我们意识到我们没有使用AllenNLP标记/标签名称空间。在AllenNLP中，您可以使用名称空间(如标签名称空间和输入名称空间)跟踪AllenNLP词汇对象中需要的所有词汇。我们发现AllenNLP词汇对象a <a class="ae ky" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/vocabulary.py#L97" rel="noopener ugc nofollow" target="_blank">自动插入</a> @@PADDING@@和@ @ UNKOWN @ @标记来索引除标签名称空间(所有以“_tag”或“_labels”结尾的名称空间)之外的所有名称空间的0和1因为我们没有使用标签名称空间，所以我们的索引向前移动了两个，预测空间(由标签词汇大小定义)大了两个！发现这一点后，我们重新命名了标签索引，这种特殊的威胁得到了遏制。</p><h2 id="6b74" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">最后一个隐藏的Bug和一个支点</h2><p id="699a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">到目前为止，我们认为我们已经捕获了所有或者大部分的错误，并且MLM工作正常。然而，当模型现在变得越来越不困惑时，一周后，当与第三个人一起查看代码时，我们发现了另一个隐藏的bug。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="be12" class="lv lw it nc b gy ng nh l ni nj">if self._unk_id is not None: <br/>     ids = (ids — 2) * valid_mask + self._pad_id * pad_mask + self._unk_id * unk_mask <br/>else: <br/>     ids = (ids — 2) * valid_mask + self._pad_id * pad_mask</span></pre><p id="b6dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们几个月前编写的代码的某个单独部分，我们将任何基于Transformer的模型的输入向后移动了2，因为AllenNLP将其向前移动了2。因此，从本质上来说，模型看到的是胡言乱语，因为它看到的是距离正确输入索引两个索引的词汇标记。</p><p id="8165" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们是如何解决这个问题的？</p><p id="f366" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们结束了对以前的bug的修复，并且没有使用label名称空间作为输入，因为所有的东西都向后移动了2。并且简单地确保动态生成的mask_idx在被馈送到模型中之前向前移位2。为了修复之前预测和标签空间大小不匹配的错误，我们将标签的数量设为预训练模型的标记器的大小，因为这包括该模型预训练的所有词汇。</p><p id="d4df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">经过无数个小时的调试和初步实验，我们终于摆脱了bug。<em class="mx">唷！</em></p><p id="b6ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，作为对我们所做的事情的回顾，以确保代码没有错误，也是为了回顾我们所看到的错误类型，这里有一个漂亮的列表。</p><h2 id="6510" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">调试模型的关键要点</h2><ol class=""><li id="93b4" class="nn no it lb b lc mo lf mp li np lm nq lq nr lu ns nt nu nv bi translated">从玩具数据集开始测试。对于本案例研究，预处理整个数据集需要大约4个小时。</li><li id="900e" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">如果可能，使用已经创建的基础设施。</li><li id="86ba" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">请注意，如果您正在使用其他人的代码，请确保您确切地知道它如何适合您的代码，以及在将外部代码集成到您自己的代码中时可能出现的任何不兼容性，无论是微妙的还是不明显的。</li><li id="f521" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">如果您正在使用预训练的模型，并且如果有意义，请尝试加载一个训练过的模型，并确保它在任务中表现良好。</li><li id="cc4b" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">注意代码之间Pytorch版本(以及其他依赖版本)的差异。</li><li id="0c21" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">索引时要非常小心。有时勾画出索引的流程是非常混乱的，会导致很多令人头疼的问题，为什么你的模型执行得不好。</li><li id="c07d" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">让其他人也看看你的代码。</li><li id="7cb9" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">为了理解bug的来源，您可能需要更深入地了解这些杂草，并查看用于预处理的包的源代码(如AllenNLP)。</li><li id="c5de" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">创建单元测试来跟踪细微的错误，并确保您不会因为代码更改而依赖这些错误。</li></ol><p id="78b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在你有了它，一个调试案例研究和一些教训。我们都在一起进行更好的调试，我知道我远不是模型调试的专家，但是希望这篇文章对你有帮助！特别感谢Phu Mon Htut编辑这篇文章。</p><p id="3142" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想看最终的实现，点击这里查看！</p></div></div>    
</body>
</html>