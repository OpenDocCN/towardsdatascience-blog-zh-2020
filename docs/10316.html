<html>
<head>
<title>Predicting the price of used cars</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测二手车的价格</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-the-price-of-used-cars-891d13faf3fc?source=collection_archive---------44-----------------------#2020-07-20">https://towardsdatascience.com/predicting-the-price-of-used-cars-891d13faf3fc?source=collection_archive---------44-----------------------#2020-07-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/8670924475b5037ad0208da31e25facb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NwzJ9OtW3GyuRgJu"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">帕克·吉布斯在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="7f2b" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">具有特征选择和特征工程的 XGBoost 实现</h2></div><p id="0c9d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文旨在分析二手车数据集的属性，以便在使用 XGBoost 模型预测这些汽车的价格之前执行特征选择和特征工程。数据处理、清理、特征选择和建模的整个过程在下面的部分中示出。术语“属性”、“变量”和“特征”在本文中可互换使用，具有相同的含义，即它们指的是数据集的列。</p><p id="74c9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们先来了解一下特征选择和特征工程是什么意思。</p><h1 id="d9eb" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">特征选择</h1><p id="7f21" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">选择那些与输出变量预测最相关的输入特征的过程称为特征选择。这有助于减少数据的维度，并确保模型不会学习那些可能降低其准确性的不相关特征。</p><p id="b2ac" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有关功能选择<a class="ae jg" href="https://en.wikipedia.org/wiki/Feature_selection" rel="noopener ugc nofollow" target="_blank">的更多信息，请点击此处</a>。</p><h1 id="ed83" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">特征工程</h1><p id="7413" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">在使用或不使用现有要素的情况下，为给定数据集获取新要素以提高该数据集模型性能的过程称为要素工程。</p><p id="e3ee" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有关特征工程的更多信息<a class="ae jg" href="https://en.wikipedia.org/wiki/Feature_engineering" rel="noopener ugc nofollow" target="_blank">点击此处</a>。</p><p id="0277" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们深入到实现部分。</p><h1 id="288d" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">履行</h1><h2 id="f3f5" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">导入必要的库</h2><p id="b465" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我们导入了预测过程所需的以下库。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="ed16" class="mr lv jj ni b gy nm nn l no np">import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>from sklearn.preprocessing import StandardScaler, LabelEncoder<br/>from sklearn.model_selection import train_test_split<br/>import math<br/>import matplotlib<br/>import seaborn as sns</span><span id="0d5f" class="mr lv jj ni b gy nq nn l no np">%matplotlib inline</span></pre><h2 id="f3a4" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">加载数据集</h2><p id="4f89" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">这里使用的数据集是加载的<a class="ae jg" href="https://www.kaggle.com/orgesleka/used-cars-database" rel="noopener ugc nofollow" target="_blank">二手车</a>数据集。它拥有从易贝收集的 37 万行数据和描述每辆二手车细节的 28 个属性。但是使用了具有 50000 行的总数据集的子集。这些数据的内容是德语，然后翻译成英语。这个原始数据集的翻译子集可以在<a class="ae jg" href="https://drive.google.com/file/d/1xgRSyfaulUpSQIH4dcrXlrizSsJJPcMj/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>找到。数据集属性的描述如下所示:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="59a1" class="mr lv jj ni b gy nm nn l no np">df = pd.read_csv(‘../input/data.csv’, sep=’,’, header=0, encoding=’cp1252')</span></pre></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><pre class="nh ni nj nk aw nl bi"><span id="6624" class="mr lv jj ni b gy ny nz oa ob oc nn l no np">df.describe()</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/ffd954432cdaff17e226d1e96dab4d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*EbnUgXxgY3PHMdwD00v9Pg.png"/></div></figure><p id="2f20" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从数据描述中我们可以看到，有些<a class="ae jg" href="https://www.saedsayad.com/numerical_variables.htm#:~:text=A%20numerical%20or%20continuous%20variable,numerical%20variables%2C%20interval%20and%20ratio." rel="noopener ugc nofollow" target="_blank">数值变量</a>有不能存在的值。例如，<em class="oe"> yearOfRegistration </em>变量的最小值为 1000，最大值为 9999，而这两个值是不可能的。这些值将在接下来的代码块中处理。</p><h2 id="da7a" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">删除不必要的列和不允许的值</h2><p id="9a64" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">然后，我们删除数据集中对预测汽车成本没有意义的属性。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="833e" class="mr lv jj ni b gy nm nn l no np">df.drop([‘name’,’seller’, ‘offerType’, ‘dateCrawled’, ‘lastSeen’], axis=’columns’, inplace=True)</span></pre><p id="a1dc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后删除属性值不可能(不允许)的行。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="2e70" class="mr lv jj ni b gy nm nn l no np">df=df[(df.price&gt;=100) &amp; (df.price&lt;=200000) &amp; (df.yearOfRegistration&gt;=1950) &amp; (df.yearOfRegistration&lt;=2019) &amp; (df.monthOfRegistration&gt;0) &amp; (df.monthOfRegistration&lt;=12) &amp; (df.powerPS&gt;10) &amp; (df.powerPS&lt;1000)]</span><span id="1a3e" class="mr lv jj ni b gy nq nn l no np">df.describe()</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/ec7fe8cc6d68c3f691b5031292239436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*3nTAB9zI1L-Jgo3leuJ5uw.png"/></div></figure><p id="9ace" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上面的数据描述中我们可以看到，属性现在只有可能的值。</p><h2 id="bf51" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">NA 值的插补</h2><p id="6338" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我们首先需要找出哪些属性具有 NA 值，即空字段。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="f1d8" class="mr lv jj ni b gy nm nn l no np">df.isna().sum()</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/cb8a04d319cd2db9b0e53ca55a530717.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*gDWQtGLWwThRI9rvtg7BAg.png"/></div></div></figure><p id="c749" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以 NA 值只存在于<a class="ae jg" href="https://en.wikipedia.org/wiki/Categorical_variable" rel="noopener ugc nofollow" target="_blank">分类变量</a>中。为了填充每个空字段(<a class="ae jg" rel="noopener" target="_blank" href="/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779">插补</a>，使用该属性的模式值。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="2e34" class="mr lv jj ni b gy nm nn l no np">var=[‘vehicleType’,’gearbox’,’model’,’fuelType’,’notRepairedDamage’]<br/>for i in var:<br/>    df[i].fillna(df[i].mode()[0],inplace=True)<br/>df.isna().sum()</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/44922444333b707e5e7d02bf0518c792.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*nktLp_ueXqVYFXac5dlD-Q.png"/></div></figure><h2 id="dd0a" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">使用目标变量的特征可视化</h2><p id="94b4" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">根据<em class="oe">价格</em>变量绘制特征<em class="oe">注册年份</em>、<em class="oe">注册月份、</em>和<em class="oe">邮政编码</em>。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="5b6f" class="mr lv jj ni b gy nm nn l no np">for i in [‘yearOfRegistration’, ‘monthOfRegistration’,’postalCode’]:<br/>    sns.jointplot(x=i, y=’price’,data=df[[i,’price’]],size=7)</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/fe50e569e17c02874b0b6820c25a0fe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*1oSy3UipuCUq7M-SGtdoWA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">注册年份与价格</p></figure><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/45320074da8252f448f050cd94a00cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*0q2DOwcMR5AoRj0zJo38xA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">注册与价格</p></figure><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a81197a1e0bab5dfdb5a61aab358f7c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*qq146E-yRCYXIeJcH95OpQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">邮政编码与价格</p></figure><p id="25f5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为每对变量显示的图以及<a class="ae jg" href="https://dataschool.com/fundamentals-of-analysis/correlation-and-p-value/" rel="noopener ugc nofollow" target="_blank">皮尔逊相关性和 p 值</a>可用作特征选择度量。第二个图显示<em class="oe">月注册</em>与<em class="oe">价格的相关值非常低，而 p 值很高。</em>所以这个变量被删除了。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="6239" class="mr lv jj ni b gy nm nn l no np">df.drop([‘monthOfRegistration’],axis=’columns’,inplace=True)</span></pre><h2 id="f40e" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">特征工程——创造新的特征“ageOfCar”</h2><p id="44c9" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">这个数据集有一个属性<em class="oe"> dateCreated </em>，这是在易贝上创建汽车广告的日期。通过使用这个属性和<em class="oe"> yearOfRegistration </em>属性，我们可以创建一个告诉我们汽车年龄的新特性(<em class="oe"> ageOfCar </em>)。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="ff0b" class="mr lv jj ni b gy nm nn l no np">df[‘ageOfCar’]=pd.DatetimeIndex(df[‘dateCreated’]).year-df[‘yearOfRegistration’]<br/>sns.jointplot(x=’ageOfCar’,y=’price’,data=df[[‘ageOfCar’,’price’]],size=7)</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/cdd1c60ef0190aa70fe732132310a087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*kwj2Rfl_MzTBRCl3nTqumQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">车龄与价格</p></figure><p id="7319" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上面的图中我们可以看到，皮尔逊相关性的大小与<em class="oe">注册年份</em>和<em class="oe">价格</em>的皮尔逊相关性的大小相同，但是为负。这意味着随着车龄的增加，价格会下降，这在现实生活中也是如此。</p><p id="3a03" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们删除变量<em class="oe"> yearOfRegistration </em>和<em class="oe"> dateCreated。</em></p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="1e5e" class="mr lv jj ni b gy nm nn l no np">df.drop([‘yearOfRegistration’,’dateCreated’],axis=’columns’,inplace=True)</span></pre><h2 id="7f38" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">用箱线图识别和去除异常值</h2><blockquote class="oi oj ok"><p id="e5c9" class="ky kz oe la b lb lc kk ld le lf kn lg ol li lj lk om lm ln lo on lq lr ls lt im bi translated">在统计学中，<strong class="la jk">异常值</strong>是与其他观察值显著不同的数据点。异常值可能是由于测量中的可变性造成的，或者它可能表示实验误差；后者有时被排除在数据集之外。异常值会在统计分析中引起严重的问题。(来源:<a class="ae jg" href="https://en.wikipedia.org/wiki/Outlier" rel="noopener ugc nofollow" target="_blank">维基百科</a>)</p></blockquote><p id="1246" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以基本上离群点就是不符合一般数据分布的数据点。数据中存在的异常值会扭曲模型并扰乱其学习过程。因此，现在我们将检测数据集中的这些异常值，并使用<a class="ae jg" href="https://en.wikipedia.org/wiki/Box_plot" rel="noopener ugc nofollow" target="_blank">箱线图</a>(四分位数间距)方法移除它们。要了解不同的异常值检测方法<a class="ae jg" rel="noopener" target="_blank" href="/ways-to-detect-and-remove-the-outliers-404d16608dba">请点击这里</a>。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="ff66" class="mr lv jj ni b gy nm nn l no np">sns.boxplot(x=df[‘price’])</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/58247f9497b0df2bff0af9e5384d4c50.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*joTe_gVGD_pFjp48h4seSA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">价格箱线图</p></figure><p id="33c1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此<em class="oe">价格</em>变量的异常值位于值 25000 之后。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="b756" class="mr lv jj ni b gy nm nn l no np">sns.boxplot(x=df[‘ageOfCar’])</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/51a3cb6e4073f11fab4fee92a5b25c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*THgjdJI52SCHMdf4NZORrA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">ageOfCar 箱线图</p></figure><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="2478" class="mr lv jj ni b gy nm nn l no np">sns.boxplot(x=df['powerPS'])</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/84d46320eab8f1021001a540b1727913.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*8EWAx3oa562FAt-sVTYzXQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">powerPS 的箱线图</p></figure><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="ad48" class="mr lv jj ni b gy nm nn l no np">sns.boxplot(x=df['kilometer'])</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/075155a37dbf78a6af54f729fc7b5611.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*QrDumrgPxjp9VZ92AY7lNA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">公里箱线图</p></figure><p id="728b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，对于特征<em class="oe">而言，ageOfCar、powerPS、千米</em>值分别高于 30、高于 280 和低于 25000 都是异常值。这些异常值现在已被移除。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="74b1" class="mr lv jj ni b gy nm nn l no np">df=df[(df.price&lt;=25000) &amp; (df.ageOfCar&gt;=0) &amp; (df.ageOfCar&lt;=30) &amp; (df.powerPS&lt;=280) &amp; (df.kilometer&gt;=25000)]</span></pre><h2 id="4d87" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">编码分类变量</h2><p id="ec07" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">现在，我们需要对数据集的分类变量进行编码，以便模型可以使用它进行训练。分类变量有几种<a class="ae jg" rel="noopener" target="_blank" href="/all-about-categorical-variable-encoding-305f3361fd02">编码技术</a>。在这种情况下，我们将对具有两个类的变量使用标签编码，对具有两个以上类的变量使用目标编码。目标编码优于一键编码，因为后者会导致数据维数非常高。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="e1d8" class="mr lv jj ni b gy nm nn l no np">x=df.drop([‘price’],axis=’columns’,inplace=False)<br/>y=df[‘price’]</span><span id="7eaa" class="mr lv jj ni b gy nq nn l no np">le1=LabelEncoder().fit(x['gearbox'])<br/>x['gearbox'] =le1.transform(x['gearbox'])<br/>le2=LabelEncoder().fit(x['notRepairedDamage'])<br/>x['notRepairedDamage'] =le2.transform(x['notRepairedDamage'])<br/>le3=LabelEncoder().fit(x['abtest'])<br/>x['abtest']=le3.transform(x['abtest'])</span><span id="d43f" class="mr lv jj ni b gy nq nn l no np">from category_encoders import TargetEncoder</span><span id="a455" class="mr lv jj ni b gy nq nn l no np">te=TargetEncoder(cols=['brand','model','vehicleType','fuelType','postalCode']).fit(x,y)<br/>x=te.transform(x)</span></pre><h2 id="0c8f" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">使用相关热图</h2><p id="abf1" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">通过使用关联热图，我们可以了解变量之间的关系。从热图中可以得出两个主要观察结果(不包括对角线元素):</p><ul class=""><li id="3ec8" class="op oq jj la b lb lc le lf lh or ll os lp ot lt ou ov ow ox bi translated">与其他输入变量高度相关的输入变量。</li><li id="0609" class="op oq jj la b lb oy le oz lh pa ll pb lp pc lt ou ov ow ox bi translated">与输出变量相关性低的输入变量。</li></ul><p id="7d3f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们生成热图。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="378c" class="mr lv jj ni b gy nm nn l no np">f, ax = plt.subplots(figsize=(11, 9))<br/>cmap = sns.diverging_palette(220, 10, as_cmap=True)<br/>x[‘price’]=y<br/>sns.heatmap(x.corr(),cmap=cmap,square=True,annot=True)</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/8484e99e5c48ba741039599112302dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SMfbLCu_39_v7dh_8bd9kw.png"/></div></figure><p id="354e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从热图中，我们可以看到输入变量之间没有太大的相关性，但是，<em class="oe"> abtest </em>与<em class="oe"> price 的相关性很差。</em>因此<em class="oe"> abtest </em>被移除。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="3810" class="mr lv jj ni b gy nm nn l no np">x.drop([‘abtest’],axis=’columns’,inplace=True)</span><span id="196e" class="mr lv jj ni b gy nq nn l no np">y=x['price']<br/>x.drop(['price'],axis='columns',inplace=True)</span></pre><h2 id="5c9f" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">标准化，分为训练集和验证集</h2><p id="aa6d" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我们现在将变量标准化，并将其分为训练集(80%)和验证集(20%)。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="ce47" class="mr lv jj ni b gy nm nn l no np">sc=StandardScaler()<br/>x=sc.fit_transform(x)</span><span id="b2b6" class="mr lv jj ni b gy nq nn l no np">X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state = 42)</span></pre><h2 id="2ee7" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">使用 XGBoost 模型进行训练和测试</h2><p id="9159" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我们使用一个<a class="ae jg" rel="noopener" target="_blank" href="/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d"> XGBoost </a>模型来训练数据并预测输出。我们还使用 sklearn 模块的<a class="ae jg" href="https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998" rel="noopener"> GridSearchCV </a>来确定模型的最佳参数。使用 GridSearchCV 调优的参数有:</p><ul class=""><li id="786b" class="op oq jj la b lb lc le lf lh or ll os lp ot lt ou ov ow ox bi translated"><strong class="la jk"> n_estimators: </strong>定义 XGBoost 模型使用的估计器(树)的数量。</li><li id="a467" class="op oq jj la b lb oy le oz lh pa ll pb lp pc lt ou ov ow ox bi translated"><strong class="la jk"> max_depth: </strong>定义每棵树允许的最大深度，用于控制过拟合。</li><li id="e8e5" class="op oq jj la b lb oy le oz lh pa ll pb lp pc lt ou ov ow ox bi translated"><strong class="la jk"> eta: </strong>这是模型的学习率，它缩小了节点的权重，使其更加稳健。</li></ul><p id="de49" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了获得模型性能的无偏估计，我们使用了带有<code class="fe pe pf pg ni b">cv=5</code>的<a class="ae jg" href="https://machinelearningmastery.com/k-fold-cross-validation/" rel="noopener ugc nofollow" target="_blank"> K 倍交叉验证</a>方法。通过使用<a class="ae jg" href="https://en.wikipedia.org/wiki/Root-mean-square_deviation" rel="noopener ugc nofollow" target="_blank"> RMSE </a>作为误差度量来识别最佳参数。因为我们需要找到产生最小 RMSE 的模型，我们将 RMSE 的负数传递给<code class="fe pe pf pg ni b">scoring</code>参数。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="c324" class="mr lv jj ni b gy nm nn l no np">import xgboost as xgb<br/>from sklearn.model_selection import GridSearchCV</span><span id="ab72" class="mr lv jj ni b gy nq nn l no np">param={‘n_estimators’:[10,20,50,100,150,200],<br/> ‘max_depth’:range(3,11),<br/> ‘eta’:[0.05,0.1,0.15,0.2,0.25,0.3]}</span><span id="34b0" class="mr lv jj ni b gy nq nn l no np">xgr=xgb.XGBRegressor()<br/>gs=GridSearchCV(estimator=xgr,param_grid=param,scoring=’neg_root_mean_squared_error’,cv=5,verbose=3)<br/>gs.fit(X_train,y_train)</span></pre></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="8148" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">参数搜索完成后，我们现在可以查看确定的最佳参数及其相应的指标得分。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="eab1" class="mr lv jj ni b gy nm nn l no np">print(gs.best_params_)<br/>print(gs.best_score_)</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/3baaaaf4229cb3effa95cb30cb3cb701.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*PlfQbeAcs8R_NGsCmuJQ5Q.png"/></div></figure><p id="e43d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe pe pf pg ni b">eta=0.05</code>、<code class="fe pe pf pg ni b">n_estimators=200</code>、<code class="fe pe pf pg ni b">max_depth=7</code>被确定为产生最低 RMSE 分数的参数。使用 GridSearchCV 确定的这些参数，我们构建最终模型并将其用于预测。对于回归问题，比如这个问题，误差度量比如 RMSE、MSE、MAE、R 等。被使用。这里，我们使用 RMSE 和 T21(决定系数)作为误差度量。XGBoost 的<code class="fe pe pf pg ni b">score()</code>函数返回回归问题的 R 值。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="3028" class="mr lv jj ni b gy nm nn l no np">from sklearn.metrics import mean_squared_error</span><span id="3598" class="mr lv jj ni b gy nq nn l no np">xgr=xgb.XGBRegressor(eta=0.05,max_depth=7,n_estimators=200)<br/>xgr.fit(X_train,y_train)<br/>pred=xgr.predict(X_val)<br/>print('RMSE: ',mean_squared_error(y_val,pred,squared=False))<br/>print('R2Score: ',xgr.score(X_val,y_val))</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/e5cebca357c1989bee6c4ffd227c0231.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*ennRuK599oyGcnwLH9m8EQ.png"/></div></figure><p id="3188" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">RMSE 和 R 值表明模型的结果相当准确。现在，为了评估模型的性能，并了解数据分布的捕捉情况，我们从残差中生成图。</p><blockquote class="oi oj ok"><p id="c7b9" class="ky kz oe la b lb lc kk ld le lf kn lg ol li lj lk om lm ln lo on lq lr ls lt im bi translated">在回归分析中，因变量的观测值(y)与预测值(ŷ)之差称为<strong class="la jk">残差</strong> (e)。每个数据点有一个<strong class="la jk">残差</strong>。(来源:<a class="ae jg" href="https://stattrek.com/statistics/dictionary.aspx?definition=residual#:~:text=In%20regression%20analysis%2C%20the%20difference,residuals%20are%20equal%20to%20zero." rel="noopener ugc nofollow" target="_blank"> Stattrek </a>)</p></blockquote><p id="8fc8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们使用标准化残差(残差除以其标准差)来创建散点图和直方图。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="d9a9" class="mr lv jj ni b gy nm nn l no np">def residual_plot(y_test, y_pred):<br/>    res = y_test — y_pred<br/>    fig, axs = plt.subplots(1,2,figsize=(30,10))<br/>    std_res = res/np.std(res)<br/>    axs[0].title.set_text(‘Scatter Plot of residuals’)<br/>    axs[0].set_xlabel(‘True Output’)<br/>    axs[0].set_ylabel(‘Residuals’)<br/>    axs[0].scatter(y_test,std_res)<br/>    axs[1].title.set_text(‘Histogram of residuals’)<br/>    sns.distplot(std_res, ax = axs[1]);<br/>    plt.show()</span><span id="7472" class="mr lv jj ni b gy nq nn l no np">residual_plot(y_val,pred)</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pj"><img src="../Images/4d88883094378357da2091d4a579fd7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WcMJgag8IQYc5jy69AMJDQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">残差的散点图和直方图</p></figure><p id="a48f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">散点图显示存在一些异常值，并且残差不具有恒定的方差。因此，它们不遵循正态分布。直方图显示大部分残差集中在零附近。</p><h2 id="e5b9" class="mr lv jj bd lw ms mt dn ma mu mv dp me lh mw mx mg ll my mz mi lp na nb mk nc bi translated">特征重要性</h2><p id="4136" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">为了理解输出预测过程中每个特征的重要性并得出未来的推论，我们绘制了直接从 XGBoost 模型中获得的<a class="ae jg" rel="noopener" target="_blank" href="/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e">特征重要性</a>值。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="bed1" class="mr lv jj ni b gy nm nn l no np">feature_important = xgr.feature_importances_<br/>values = list(feature_important)</span><span id="dfd2" class="mr lv jj ni b gy nq nn l no np">data = pd.DataFrame(data=values, index=feat, columns=[“score”]).sort_values(by = “score”)<br/>ax=data.plot(kind=’barh’,title=’Feature Importance’,legend=None)<br/>ax.set_xlabel(‘Importance value’)<br/>ax.set_ylabel(‘Features’)<br/>plt.show()</span></pre><figure class="nd ne nf ng gt iv gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/c7563582e7cc4bdea19755a5305b830b.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*Qh9KlYag9TEQMiOxVHQkRg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">特征重要性</p></figure><p id="1c9b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="oe">未修复损坏</em>、<em class="oe">燃油类型、</em>和<em class="oe">变速箱</em>对<em class="oe">价格的预测作用不大。</em></p><h1 id="26b3" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">结论</h1><p id="be81" class="pw-post-body-paragraph ky kz jj la b lb mm kk ld le mn kn lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">总之，我们利用二手车数据集来理解数据清理、插补、特征选择和特征工程的概念。我们还学习了如何使用 XGBoost 模型，并通过绘制残差和特征重要性值对其进行回归分析。</p><p id="bb8c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要获得 IPython 笔记本的完整代码<a class="ae jg" href="https://drive.google.com/file/d/1qJhC6bqacrOvBMmNTfV-3-ELLatlZiu0/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">请点击这里</a>。</p></div></div>    
</body>
</html>