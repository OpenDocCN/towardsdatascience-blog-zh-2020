<html>
<head>
<title>Use GANs Beyond Generating Art?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用GANs超越生成艺术？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/can-we-use-gans-beyond-generating-art-c550cce0d467?source=collection_archive---------33-----------------------#2020-05-08">https://towardsdatascience.com/can-we-use-gans-beyond-generating-art-c550cce0d467?source=collection_archive---------33-----------------------#2020-05-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d52d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">gan相对较新，许多研究方向仍然开放。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c201f4b4527fe503a1df139d78e67266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NekSFkZvvptyhBKSKlWa_w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/Comfreak-51581/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2929646" rel="noopener ugc nofollow" target="_blank">com break</a>来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2929646" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="a1f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成对抗网络(GANs)取得了实质性的进展，它可以合成近乎完美的人脸[ <a class="ae ky" href="https://arxiv.org/abs/1912.04958" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]，恢复旧视频[ <a class="ae ky" href="http://iizuka.cs.tsukuba.ac.jp/projects/remastering/en/index.html" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]的颜色和质量，并生成逼真的<a class="ae ky" rel="noopener" target="_blank" href="/realistic-deepfakes-colab-e13ef7b2bba7?source=friends_link&amp;sk=a9d432c8b01ae6fd8ed57b501886ce63"> Deepfake </a>视频[ <a class="ae ky" href="https://aliaksandrsiarohin.github.io/first-order-model-website/" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]。</p><p id="5c4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GANs主要关注样本生成。GANs的基本思想包括一个生成器和一个鉴别器。发生器产生来自训练数据的样本，而鉴别器判断产生的样本以确定它们是真的还是假的。一个例子是:一个伪造者试图学会制造与真钱不同的货币，而警察则学会了捕捉假币。</p><p id="4117" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于GAN是最成功的，并且主要应用于图像合成，我们能在艺术创作之外使用GAN吗？</p><h1 id="491b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">图像到图像的翻译</h1><p id="def7" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">类似于通过学习句子/短语对映射将源语言翻译成目标语言的机器翻译，图像到图像翻译学习输入图像和输出图像对之间的映射。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/3ad9a5d172875d7aff9183c7ca4dda5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BOBm1HotWQMatdnO6Rx37g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动将图像从一张“翻译”到另一张[ <a class="ae ky" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank">纸张</a></p></figure><p id="773a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Berkeley [ <a class="ae ky" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]完成的一项研究中，他们的算法学习将一幅图像从一幅图像翻译成另一幅图像。比如从斑马到马，从夏天到冬天。在他们的工作中，他们提出了一种使用对抗损失从配对样本的缺乏中学习的方法，推动了无监督学习的边界。一定要看看他们的<a class="ae ky" href="https://junyanz.github.io/CycleGAN/" rel="noopener ugc nofollow" target="_blank">网站</a>，上面有各种使用他们作品的创意(比如<a class="ae ky" rel="noopener" target="_blank" href="/turning-fortnite-into-pubg-with-deep-learning-cyclegan-2f9d339dcdb0">把堡垒之夜变成PUBG </a>)应用。</p><h1 id="f8ba" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">文本生成</h1><p id="f5ea" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">文本的性质使得GAN很难生成离散记号的序列。因为(来自生成模型的)离散输出使得难以将梯度更新从判别模型传递到生成模型。</p><p id="dad6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，于等人<a class="ae ky" href="https://arxiv.org/pdf/1609.05473.pdf" rel="noopener ugc nofollow" target="_blank">等人</a>却提出了诗体生成汉诗的命题。使用<a class="ae ky" href="https://en.wikipedia.org/wiki/BLEU" rel="noopener ugc nofollow" target="_blank"> BLEU score </a>，一种评估文本质量的指标，SeqGAN生成了与人类诗歌创作相当的文本。此外，他们将真实的和生成的中国诗歌混合在一起，并邀请中国诗歌专家来判断每首诗歌是人类还是机器创作的。评委们无法区分真正的诗歌和机器生成的诗歌。</p><h1 id="bdbe" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">生成网络图</h1><p id="9e73" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">生成合成但真实的图形的能力对于异常检测很重要，异常检测就是将异常网络与正常网络区分开。图形生成技术已经应用于物理学、数学、社会学、生物学和计算机科学。</p><p id="33fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Bojchevski等人[ <a class="ae ky" href="https://arxiv.org/pdf/1803.00816.pdf" rel="noopener ugc nofollow" target="_blank"> 6 </a> ]提出了NetGAN，它生成的图形展示了使用Wasserstein GAN目标训练的众所周知的网络模式。像典型的GAN一样，NetGAN的生成器学习生成真实图形中似乎合理的随机行走，而鉴别器将图形与原始图形区分开来。</p><h1 id="1753" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">音频合成</h1><p id="0010" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">合成音频已经应用于音乐和电影的创造性声音设计中。这些声音剪辑存储在大型音效数据库中，但有时库中可能不存在理想的音效。如果我们可以通过微调一些输入参数来生成砾石路或沙地上脚步声的声音效果，会怎么样？</p><p id="76ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">加州大学圣地亚哥分校推出了WaveGAN [ <a class="ae ky" href="https://arxiv.org/pdf/1802.04208.pdf" rel="noopener ugc nofollow" target="_blank"> 7 </a> ]，学习合成音频以产生声音效果。它已经学会了产生踢鼓和小军鼓的声音效果，它也能产生各种不同的鸟叫。对于钢琴声音生成，它捕捉各种调号和节奏模式。他们实验的声音样本可以在他们的<a class="ae ky" href="https://chrisdonahue.com/wavegan_examples/" rel="noopener ugc nofollow" target="_blank">网站</a>上找到。</p><p id="ad77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">于等人[ <a class="ae ky" href="https://arxiv.org/pdf/1609.05473.pdf" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]也使用SeqGAN通过训练一组midi文件格式的民间曲调来生成音乐。在他们的工作中，他们将midi文件转换成从1到88的数字序列，以代表与钢琴上的键相对应的88个音高。他们的方法优于现有的算法，但他们没有提供人类判断的分析。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><blockquote class="na"><p id="ca8a" class="nb nc it bd nd ne nf ng nh ni nj lu dk translated">GANs是一种相对较新的方法，许多研究方向仍然开放。伊恩·古德菲勒</p></blockquote></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><div class="kj kk kl km gt ab cb"><figure class="nk kn nl nm nn no np paragraph-image"><a href="https://towardsdatascience.com/@jinglesnote"><img src="../Images/7c898af9285ccd6872db2ff2f21ce5d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*airGp_q6AXwaoL1LYXwYeQ.png"/></a></figure><figure class="nk kn nl nm nn no np paragraph-image"><a href="https://jinglescode.github.io/"><img src="../Images/e6191b77eb1b195de751fecf706289ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*fPTPd_WxZ4Ey7iOVElxwJQ.png"/></a></figure><figure class="nk kn nl nm nn no np paragraph-image"><a href="https://jingles.substack.com/subscribe"><img src="../Images/d370b96eace4b03cb3c36039b70735d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ESxUX6V6tAqj_2ZFSr-pUw.png"/></a></figure></div></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="4bbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]t . Karras，Laine，s .，Aittala，m .，Hellsten，j .，Lehtinen，j .和t . Aila，2019年。stylegan图像质量的分析与改进。<em class="nq"> arXiv预印本arXiv:1912.04958 </em>。[<a class="ae ky" href="https://arxiv.org/abs/1912.04958" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1912.04958</a></p><p id="7e4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] Iizuka，s .和Simo-Serra，e .，2019年。用于综合视频增强的时间源参考注意力网络。<em class="nq">《美国计算机学会图形汇刊(TOG)】</em>，<em class="nq"> 38 </em> (6)，第1–13页。[<a class="ae ky" href="http://iizuka.cs.tsukuba.ac.jp/projects/remastering/en/index.html" rel="noopener ugc nofollow" target="_blank">http://iizuka . cs . Tsukuba . AC . jp/projects/remastering/en/index . html</a>]</p><p id="1e5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] Siarohin，a .，Lathuilière，s .，Tulyakov，s .，Ricci，e .和Sebe，n .，2019年。图像动画的一阶运动模型。在<em class="nq">神经信息处理系统的进展</em>(第7135-7145页)。[<a class="ae ky" href="https://aliaksandrsiarohin.github.io/first-order-model-website/" rel="noopener ugc nofollow" target="_blank">https://aliaksandrsiarohin . github . io/first-order-model-website/</a>]</p><p id="4171" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4]朱，J.Y .，朴，t .，伊索拉，p .和埃夫罗斯，A.A .，2017年。使用循环一致对抗网络的不成对图像到图像翻译。IEEE计算机视觉国际会议论文集(第2223-2232页)。https://arxiv.org/pdf/1703.10593.pdf</p><p id="d330" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5]俞，李，张，王伟，王，于，2017年2月。Seqgan:具有策略梯度的序列生成对抗网。在第三十一届AAAI人工智能会议上。https://arxiv.org/pdf/1609.05473.pdf<a class="ae ky" href="https://arxiv.org/pdf/1609.05473.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="7968" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] Bojchevski，a .，Shchur，o .，Zügner，d .和Günnemann，s .，2018年。Netgan:通过随机漫步生成图形。<em class="nq"> arXiv预印本arXiv:1803.00816 </em>。https://arxiv.org/pdf/1803.00816.pdf</p><p id="4184" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7]c .多纳休，j .麦考利和m .帕克特，2018年。对抗性音频合成。<em class="nq"> arXiv预印本arXiv:1802.04208。</em><a class="ae ky" href="https://arxiv.org/pdf/1802.04208.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1802.04208.pdf</a></p></div></div>    
</body>
</html>