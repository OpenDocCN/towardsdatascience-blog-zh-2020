<html>
<head>
<title>Advanced monitoring of AWS glue jobs by enabling spark UI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过启用spark UI对AWS粘合作业进行高级监控</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/glue-spark-ui-6dab088929d9?source=collection_archive---------30-----------------------#2020-06-11">https://towardsdatascience.com/glue-spark-ui-6dab088929d9?source=collection_archive---------30-----------------------#2020-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ea0a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">启用spark历史服务器的Docker容器，可用于查看粘合作业的sparkUI</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1e8428bfeba5343c6649d278a948be67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gHx0_KXRoGIk-AJg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@kmuza?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Carlos Muza </a>拍摄的照片</p></figure><p id="8893" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Apache spark目前是处理大型数据集时不可或缺的框架。大多数处理大量数据的数据工程师和数据科学家在某些时候会利用spark功能。</p><blockquote class="lv lw lx"><p id="8ad3" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">说到对spark的理解，20%由<strong class="lb iu">代码语法</strong>组成，30%是<strong class="lb iu">数据工程概念</strong>，剩下的50%是<strong class="lb iu">优化</strong></p></blockquote><p id="2078" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">代码语法</strong>包括使用<em class="ly">函数和语法</em>，<strong class="lb iu">数据工程概念</strong>反映了<em class="ly"> SQL、高效查询构建和创建优化查询</em>和<strong class="lb iu">优化的知识</strong>包括集群管理的好坏和以<em class="ly">最小无序度运行代码、处理不可避免的小文件、优化连接、处理长时间运行的作业、优化倾斜数据集</em>以及利用<em class="ly">完整集群。</em></p><p id="5c31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">语法定义和数据工程概念可以随着时间的推移而学习，但是为了优化集群，您需要可视化您的集群如何响应在其上执行的查询。</p><p id="d1d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SparkUI用于识别<em class="ly">长时间运行作业背后的根本原因，可视化Dag</em>、<em class="ly"> OOM问题</em>、<em class="ly">磁盘溢出</em>、<em class="ly">过度洗牌</em>、<em class="ly">倾斜数据集、</em>以及集群的整体运行状况。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="f4b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要使用apache spark，我们需要大型集群，但有时，管理这些集群会带来额外的开销。因此，我们开始使用AWS EMR或GCP Dataproc在云上利用集群，但同样我们需要管理这些集群并充分利用它们。现在，我们需要能够运行spark工作负载的东西，并且只为我们使用的东西付费。这就是<strong class="lb iu"> AWS胶水</strong>出现的原因。</p><p id="eedd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Aws Glue是无服务器的，因此我们不需要管理集群或担心与之相关的运行成本。我们只为使用资源的时间付费，一旦我们的工作完成，资源就会被释放。</p><p id="6e57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每一件好事都是有代价的，所以这里的问题是我们不能在优化方面完全配置我们的胶合工作。此外，我们不能实时查看作业的spark UI，相反，我们需要运行spark历史服务器来查看胶合作业的Spark UI。</p><p id="e23b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要启用spark UI，我们需要遵循一些步骤:</p><ol class=""><li id="7c5e" class="mj mk it lb b lc ld lf lg li ml lm mm lq mn lu mo mp mq mr bi translated">在粘合作业中启用spark UI选项。</li><li id="9eb3" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">指定将生成日志的s3路径。</li><li id="3f86" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">使用docker和EC2启动Spark历史服务器。</li><li id="c194" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">访问历史服务器上的spark UI。</li></ol></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="055e" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">为日志生成启用spark UI</h2><p id="0686" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">创建一个新作业，并在monitoring部分启用spark UI选项，并为日志生成提供一个s3路径。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/1317df619bf8a33f363a997c1e1fecde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63IbdZwXRHdOD1S06HjZ7g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为粘合作业启用spark UI</p></figure><h2 id="3f3a" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">EC2上的Spark历史服务器设置</h2><p id="c237" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">我们现在将创建一个Spark历史服务器，从中我们可以看到粘合作业的DAG和任务详细信息。</p><p id="41ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相同的文档是:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Dockerfile文件</p></figure><p id="63b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我把阿尔卑斯山作为基本图像，因为我希望这个docker容器在尺寸上非常精简。</p><p id="27fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">安装Java 8、Maven和bash shell</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="8dce" class="mx my it nz b gy od oe l of og">RUN apk add --no-cache openjdk8<br/>RUN apk add --no-cache maven<br/>RUN apk add --no-cache bash</span></pre><p id="0606" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">获取spark 2.4并解压缩，因为它包含了启用历史服务器的类</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="b552" class="mx my it nz b gy od oe l of og">RUN wget <a class="ae ky" href="https://archive.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-without-hadoop.tgz" rel="noopener ugc nofollow" target="_blank">https://archive.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-without-hadoop.tgz</a><br/>RUN mv spark-2.4.3-bin-without-hadoop.tgz spark<br/>RUN tar -zxf spark<br/>RUN rm -rf spark<br/>RUN mv spark-2.4.3-bin-without-hadoop/ spark/</span></pre><p id="fcbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将<em class="ly"> pom.xml </em>复制到用于安装依赖项的容器中，并使用maven构建它</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="d326" class="mx my it nz b gy od oe l of og">COPY pom.xml .<br/>RUN mvn dependency:copy-dependencies -DoutputDirectory = /sparkui/spark/jars/</span></pre><p id="7800" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">移除maven作为构建现在是成功的，并且也移除了冲突的jar</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="a03b" class="mx my it nz b gy od oe l of og">RUN apk del maven<br/>RUN rm /sparkui/spark/jars/servlet-api-2.5.jar &amp;&amp; \<br/>    rm /sparkui/spark/jars/jsr305-1.3.9.jar &amp;&amp; \<br/>    rm /sparkui/spark/jars/jersey-*-1.9.jar</span></pre><p id="36c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">添加端口号来启用历史服务器，您可以根据您的要求更改它。我们需要将属性添加到<em class="ly"> spark-defaults.conf </em>文件中。</p><blockquote class="lv lw lx"><p id="0d71" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">您也可以在这里更改端口号。</p></blockquote><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="01ec" class="mx my it nz b gy od oe l of og">RUN echo $’\n\<br/>spark.eventLog.enabled true\n\<br/>spark.history.ui.port 18080\n\<br/>’ &gt; /sparkui/spark/conf/spark-defaults.conf </span></pre><p id="558d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将入口点添加到docker容器，该入口点将启用spark历史服务器。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="17a2" class="mx my it nz b gy od oe l of og">ENTRYPOINT [“/sparkui/spark/bin/spark-class”, “org.apache.spark.deploy.history.HistoryServer”]</span></pre><p id="df4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在从这个文件构建并运行docker容器。</p><h2 id="fa9f" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">从Dockerfile构建docker映像:</h2><p id="c11b" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">要从该Dockerfile文件构建映像，请运行以下命令:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="514a" class="mx my it nz b gy od oe l of og">docker build -t jnshubham/glue_sparkui .</span></pre><h2 id="4eaa" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">从DockerHub中提取现有图像</h2><p id="3df3" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">您还可以从docker hub上获取预构建映像。</p><p id="20b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要提取图像，请运行以下命令:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="25d8" class="mx my it nz b gy od oe l of og">docker pull jnshubham/glue_sparkui:latest</span></pre><p id="784d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过运行以下命令检查下载的图像</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="32f2" class="mx my it nz b gy od oe l of og">docker images</span></pre><h2 id="cce1" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">运行Docker容器</h2><p id="8c08" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">要运行容器并启动历史服务器，请运行以下命令:</p><p id="91e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用访问密钥和秘密密钥运行容器</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="5e09" class="mx my it nz b gy od oe l of og">$ LOG_DIR="s3a://path_to_eventlog/"<br/>$ AWS_ACCESS_KEY_ID="AKIAxxxxxxxxxxxx"<br/>$ AWS_SECRET_ACCESS_KEY="yyyyyyyyyyyyyyy"<br/><br/>$ docker run -itd -e SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=$LOG_DIR -Dspark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID -Dspark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY" -p 18080:18080 jnshubham/glue_sparkui</span></pre><p id="795a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行配置了IAM角色的容器来访问s3 bucket</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="6e5a" class="mx my it nz b gy od oe l of og">$ export LOG_DIR=s3a://bucket_name/logs_path/<br/>$ docker run -itd -e SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=$LOG_DIR" -p 18080:18080 jnshubham/glue_sparkui</span></pre><h2 id="dec0" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">从EC2访问历史服务器</h2><p id="18e5" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">要访问历史服务器，请启用安全组入站规则中的端口18080以允许流量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/7a203cbef196888d0dbe134312fc5082.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zxenO_feW-DFk1XEb2o3Aw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">启用端口18080的流量</p></figure><p id="d31d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦你有一些成功的日志，打开ip:18080打开历史服务器，它将显示粘合工作的spark用户界面:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/2354c4df8e03e614643ea112e614db13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8aM5C5MfGBx7u5p_XxqWGQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/aa9261ce6cb7806a8b0e67e5bc383300.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*yzn0_9fvxDLZeO-f0EHB1Q.png"/></div></figure><p id="72d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，您可以可视化生成的Dag和查询性能。您还可以跟踪每项任务所花费的时间，然后对其进行优化。</p><p id="ad93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望你们都喜欢这篇文章。</p><p id="9161" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更多信息，请访问我的<a class="ae ky" href="https://github.com/jnshubham/glue_sparkui_docker" rel="noopener ugc nofollow" target="_blank"> GitHub </a>或<a class="ae ky" href="https://hub.docker.com/repository/docker/jnshubham/glue_sparkui" rel="noopener ugc nofollow" target="_blank"> dockerhub </a>。</p></div></div>    
</body>
</html>