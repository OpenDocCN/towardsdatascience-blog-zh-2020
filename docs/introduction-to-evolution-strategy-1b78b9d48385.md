# è¿›åŒ–ç­–ç•¥ç®€ä»‹

> åŸæ–‡ï¼š<https://towardsdatascience.com/introduction-to-evolution-strategy-1b78b9d48385?source=collection_archive---------27----------------------->

## ç”¨è¿›åŒ–ç­–ç•¥è®­ç»ƒæ— åå‘ä¼ æ’­ç¥ç»ç½‘ç»œ

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ åœ¨ MNIST æ‰‹å†™æ•°å­—æ•°æ®é›†ä¸Šä½¿ç”¨ Python ä¸­çš„è¿›åŒ–ç­–ç•¥ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ— åå‘ä¼ æ’­çš„ç¥ç»ç½‘ç»œã€‚è¿™ä¸ªç®€å•çš„å®ç°å°†å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¿™ä¸ªæ¦‚å¿µï¼Œå¹¶å°†å…¶åº”ç”¨äºå…¶ä»–åˆé€‚çš„è®¾ç½®ã€‚æˆ‘ä»¬å¼€å§‹å§ï¼

**ç›®å½•**
1ã€‚æ•°å€¼ä¼˜åŒ–
2ã€‚è¿›åŒ–ç­–ç•¥
3ã€‚æ™®é€šå®ç°
4ã€‚Python ä»å¤´å®ç°
5ã€‚ç»“å°¾æ³¨é‡Š

## æ•°å€¼ä¼˜åŒ–

å‡ ä¹æ‰€æœ‰çš„æœºå™¨å­¦ä¹ ç®—æ³•éƒ½å¯ä»¥å½’ç»“ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ã€‚åœ¨ ML ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬æ›´æ–°æ¨¡å‹çš„å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±ã€‚ä¾‹å¦‚ï¼Œæ¯ä¸ªç›‘ç£å­¦ä¹ ç®—æ³•éƒ½å¯ä»¥å†™æˆï¼ŒÎ¸_ estimate = arg minğ”¼[l(y,f(x,Î¸))]ï¼Œå…¶ä¸­ x å’Œ y åˆ†åˆ«è¡¨ç¤ºç‰¹å¾å’Œç›®æ ‡ï¼ŒÎ¸è¡¨ç¤ºæ¨¡å‹å‚æ•°ï¼Œf è¡¨ç¤ºæˆ‘ä»¬è¯•å›¾å»ºæ¨¡çš„å‡½æ•°ï¼Œl è¡¨ç¤ºæŸå¤±å‡½æ•°ï¼Œå®ƒè¡¡é‡æˆ‘ä»¬çš„æ‹Ÿåˆç¨‹åº¦ã€‚æ¢¯åº¦ä¸‹é™ç®—æ³•ä¹Ÿç§°ä¸ºæœ€é€Ÿä¸‹é™æ³•ï¼Œå·²ç»è¯æ˜åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å¯ä»¥å¾ˆå¥½åœ°è§£å†³è¿™ç±»é—®é¢˜ã€‚è¿™æ˜¯ä¸€ç§æ±‚å¯å¾®å‡½æ•°å±€éƒ¨æå°å€¼çš„ä¸€é˜¶è¿­ä»£ç®—æ³•ã€‚æˆ‘ä»¬é‡‡å–ä¸å½“å‰ç‚¹çš„æŸå¤±å‡½æ•°çš„æ¢¯åº¦çš„è´Ÿå€¼æˆæ¯”ä¾‹çš„æ­¥é•¿ï¼Œå³Î¸_new = Î¸_old â€” Î±*âˆ‡ L(yï¼Œf(xï¼ŒÎ¸_old))ã€‚ç‰›é¡¿æ³•æ˜¯å¦ä¸€ç§äºŒé˜¶è¿­ä»£æ³•ï¼Œå®ƒä»¥è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°æ”¶æ•›ï¼Œä½†ç”±äºéœ€è¦è®¡ç®—æŸå¤±å‡½æ•°(æµ·æ£®çŸ©é˜µ)çš„äºŒé˜¶å¯¼æ•°çš„å€’æ•°ï¼Œè®¡ç®—é‡å¾ˆå¤§ï¼Œå³Î¸_new = Î¸_old â€” [âˆ‡ L(yï¼Œf(xï¼ŒÎ¸_old))]^(-1) * âˆ‡ L(yï¼Œf(xï¼ŒÎ¸_old))ã€‚æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨æ¢¯åº¦æœç´¢å‚æ•°ï¼Œå› ä¸ºæˆ‘ä»¬ç›¸ä¿¡å®ƒå°†å¼•å¯¼æˆ‘ä»¬æœç€å‡å°‘æŸå¤±çš„æ–¹å‘å‰è¿›ã€‚ä½†æ˜¯æˆ‘ä»¬èƒ½åœ¨ä¸è®¡ç®—ä»»ä½•æ¢¯åº¦çš„æƒ…å†µä¸‹æœç´¢æœ€ä¼˜å‚æ•°å—ï¼Ÿå…¶å®è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æœ‰å¾ˆå¤šï¼æœ‰è®¸å¤šä¸åŒçš„æ— å¯¼æ•°ä¼˜åŒ–ç®—æ³•(ä¹Ÿç§°ä¸ºé»‘ç›’ä¼˜åŒ–)ã€‚

![](img/f5c26854a5fc5956c9c1ffaa5450423c.png)

æ¥æº:è°·æ­Œå›¾ç‰‡

## è¿›åŒ–ç­–ç•¥

æ¢¯åº¦ä¸‹é™ä¸ä¸€å®šæ€»èƒ½è§£å†³æˆ‘ä»¬çš„é—®é¢˜ã€‚ä¸ºä»€ä¹ˆï¼Ÿç®€è€Œè¨€ä¹‹ï¼Œç­”æ¡ˆæ˜¯å±€éƒ¨æœ€ä¼˜ã€‚ä¾‹å¦‚ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ çš„ç¨€ç–å¥–åŠ±åœºæ™¯ä¸­ï¼Œä»£ç†åœ¨ä¸€é›†ç»“æŸæ—¶æ”¶åˆ°å¥–åŠ±ï¼Œå°±åƒåœ¨å›½é™…è±¡æ£‹ä¸­ï¼Œç»“æŸå¥–åŠ±åˆ†åˆ«ä¸ºèµ¢æˆ–è¾“æ¸¸æˆçš„+1 æˆ–-1ã€‚ä¸‡ä¸€æˆ‘ä»¬è¾“æ‰äº†æ¯”èµ›ï¼Œæˆ‘ä»¬å°±ä¸çŸ¥é“æˆ‘ä»¬æ˜¯ç©å¾—å¯æ€•è¿˜æ˜¯åªæ˜¯çŠ¯äº†ä¸€ä¸ªå°é”™è¯¯ã€‚å›æŠ¥æ¢¯åº¦ä¿¡å·å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯æ— ä¿¡æ¯çš„ï¼Œä¼šè®©æˆ‘ä»¬é™·å…¥å›°å¢ƒã€‚æˆ‘ä»¬å¯ä»¥æ±‚åŠ©äºè¯¸å¦‚è¿›åŒ–ç­–ç•¥(es)ä¹‹ç±»çš„æ— å¯¼æ•°æŠ€æœ¯ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å™ªå£°æ¢¯åº¦æ¥æ›´æ–°æˆ‘ä»¬çš„å‚æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»¥åŠåœ¨æˆ‘ä»¬ä¸çŸ¥é“ç›®æ ‡å‡½æ•°çš„ç²¾ç¡®è§£æå½¢å¼æˆ–ä¸èƒ½ç›´æ¥è®¡ç®—æ¢¯åº¦çš„æƒ…å†µä¸‹ï¼ŒES å·¥ä½œå¾—å¾ˆå¥½ã€‚

åœ¨ OpenAI çš„è¿™ç¯‡[è®ºæ–‡](https://arxiv.org/abs/1703.03864)ä¸­ï¼Œä»–ä»¬å±•ç¤ºäº† ES åœ¨åˆ†å¸ƒå¼è®¡ç®—ç¯å¢ƒä¸­æ›´å®¹æ˜“å®ç°å’Œæ‰©å±•ï¼Œå®ƒåœ¨ç¨€ç–å›æŠ¥çš„æƒ…å†µä¸‹ä¸å—å½±å“ï¼Œå¹¶ä¸”å…·æœ‰æ›´å°‘çš„è¶…å‚æ•°ã€‚æ­¤å¤–ï¼Œä»–ä»¬å‘ç°ï¼Œä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ç›¸æ¯”ï¼Œä¸“å®¶ç³»ç»Ÿå‘ç°äº†æ›´å¤šæ ·çš„ç­–ç•¥ã€‚

ES æ˜¯ä¸€ç§å—è‡ªç„¶å¯å‘çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒä½¿ç”¨éšæœºçªå˜ã€é‡ç»„å’Œé€‰æ‹©æ¥åº”ç”¨äºåŒ…å«å€™é€‰è§£çš„ä¸ªä½“ç¾¤ä½“ï¼Œä»¥ä¾¿è¿­ä»£åœ°è¿›åŒ–å‡ºæ›´å¥½çš„è§£ã€‚å®ƒå¯¹äºéçº¿æ€§æˆ–éå‡¸çš„è¿ç»­ä¼˜åŒ–é—®é¢˜éå¸¸æœ‰ç”¨ã€‚

åœ¨ä¸“å®¶ç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬ä¸å¤ªå…³å¿ƒå‡½æ•°åŠå…¶ä¸è¾“å…¥æˆ–å‚æ•°çš„å…³ç³»ã€‚æ•°ç™¾ä¸‡ä¸ªæ•°å­—(æ¨¡å‹çš„å‚æ•°)è¿›å…¥ç®—æ³•ï¼Œå¹¶ä¸”å®ƒåå‡º 1 ä¸ªå€¼(ä¾‹å¦‚ï¼Œç›‘ç£è®¾ç½®ä¸­çš„æŸå¤±ï¼›å¼ºåŒ–å­¦ä¹ æƒ…å†µä¸‹çš„å¥–åŠ±)ã€‚æˆ‘ä»¬è¯•å›¾æ‰¾åˆ°è¿™äº›æ•°å­—çš„æœ€ä½³é›†åˆï¼Œä¸ºæˆ‘ä»¬çš„ä¼˜åŒ–é—®é¢˜è¿”å›å¥½çš„å€¼ã€‚æˆ‘ä»¬æ­£åœ¨ä¼˜åŒ–ä¸€ä¸ªå…³äºå‚æ•°Î¸çš„å‡½æ•° J(Î¸)ï¼Œåªæ˜¯é€šè¿‡å¯¹å®ƒæ±‚å€¼ï¼Œè€Œä¸å¯¹ J çš„ç»“æ„åšä»»ä½•å‡è®¾ï¼Œå› æ­¤å‘½åä¸ºâ€œé»‘ç›’ä¼˜åŒ–â€ã€‚ä¸‹é¢å°±æ¥æ·±æŒ–ä¸€ä¸‹å®ç°ç»†èŠ‚å§ï¼

## æ™®é€šå®ç°

é¦–å…ˆï¼Œæˆ‘ä»¬éšæœºç”Ÿæˆå‚æ•°ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè°ƒæ•´ï¼Œä½¿å‚æ•°ç¨å¾®å·¥ä½œå¾—æ›´å¥½ã€‚æ•°å­¦ä¸Šï¼Œåœ¨æ¯ä¸€æ­¥æˆ‘ä»¬å–ä¸€ä¸ªå‚æ•°å‘é‡Î¸ï¼Œå¹¶é€šè¿‡ç”¨é«˜æ–¯å™ªå£°æŠ–åŠ¨Î¸æ¥äº§ç”Ÿä¸€ç¾¤ï¼Œæ¯”å¦‚è¯´ 100 ä¸ªç¨å¾®ä¸åŒçš„å‚æ•°å‘é‡Î¸â‚,Î¸â‚‚â€¦Î¸â‚â‚€â‚€ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡è¿è¡Œæ¨¡å‹ç‹¬ç«‹è¯„ä¼° 100 ä¸ªå€™é€‰ä¸­çš„æ¯ä¸€ä¸ªï¼Œå¹¶åŸºäºè¾“å‡ºå€¼è¯„ä¼°æŸå¤±æˆ–ç›®æ ‡å‡½æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬é€‰æ‹©å‰ N ä¸ªæ€§èƒ½æœ€ä½³çš„ç²¾è‹±å‚æ•°ï¼ŒN å¯ä»¥æ˜¯ 10ï¼Œå–è¿™äº›å‚æ•°çš„å¹³å‡å€¼ï¼Œç§°ä¹‹ä¸ºæˆ‘ä»¬è¿„ä»Šä¸ºæ­¢çš„æœ€ä½³å‚æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œé€šè¿‡å°†é«˜æ–¯å™ªå£°æ·»åŠ åˆ°è¿„ä»Šä¸ºæ­¢è·å¾—çš„æœ€ä½³å‚æ•°ä¸­ï¼Œå†æ¬¡ç”Ÿæˆ 100 ä¸ªä¸åŒçš„å‚æ•°ã€‚

ä»è‡ªç„¶é€‰æ‹©çš„è§’åº¦è€ƒè™‘ï¼Œæˆ‘ä»¬æ­£åœ¨éšæœºåˆ›å»ºä¸€ä¸ªå‚æ•°(ç‰©ç§)ç¾¤ä½“ï¼Œå¹¶æ ¹æ®æˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°(ä¹Ÿç§°ä¸ºé€‚åº”åº¦å‡½æ•°)é€‰æ‹©è¡¨ç°è‰¯å¥½çš„é¡¶çº§å‚æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å–å®ƒä»¬çš„å¹³å‡å€¼æ¥ç»“åˆè¿™äº›å‚æ•°çš„æœ€ä½³è´¨é‡(è¿™æ˜¯ä¸€ç§ç²—ç•¥çš„æ–¹æ³•ï¼Œä½†ä»ç„¶æœ‰æ•ˆï¼)å¹¶ç§°ä¹‹ä¸ºæˆ‘ä»¬çš„æœ€ä½³å‚æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡æ·»åŠ éšæœºå™ªå£°ä½¿è¯¥å‚æ•°çªå˜æ¥é‡å»ºç§ç¾¤ï¼Œå¹¶é‡å¤æ•´ä¸ªè¿‡ç¨‹ç›´åˆ°æ”¶æ•›ã€‚

![](img/2d43e471d5f067cafd970c9b4b674203.png)

æ¥æº:æ”¹ç¼–è‡ª Lur ä¸“é¢˜ç™¾ç§‘å…¨ä¹¦ï¼Œé€šè¿‡ç»´åŸºå…±äº«èµ„æº

**ä¼ªç **:

1.  ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒéšæœºåˆå§‹åŒ–æœ€ä½³å‚æ•°
2.  å¾ªç¯ç›´è‡³æ”¶æ•›:
    â€”é€šè¿‡å°†é«˜æ–¯å™ªå£°æ·»åŠ åˆ°æœ€ä½³å‚æ•°
    æ¥åˆ›å»ºå‚æ•°Î¸â‚,Î¸â‚‚â€¦Î¸â‚â‚€â‚€ç¾¤ä½“â€”â€”è¯„ä¼°æ‰€æœ‰å‚æ•°çš„ç›®æ ‡å‡½æ•°å¹¶é€‰æ‹©å‰ n ä¸ªæœ€ä½³æ€§èƒ½å‚æ•°(ç²¾è‹±å‚æ•°)
    â€”æœ€ä½³å‚æ•°=å¹³å‡å€¼(å‰ n ä¸ªç²¾è‹±å‚æ•°)
    â€”åœ¨æ¯æ¬¡è¿­ä»£ç»“æŸæ—¶ä»¥æŸä¸ªå› å­è¡°å‡å™ªå£°(åœ¨å¼€å§‹æ—¶ï¼Œæ›´å¤šçš„å™ªå£°å°†æœ‰åŠ©äºæˆ‘ä»¬æ›´å¥½åœ°æ¢ç´¢ï¼Œä½†æ˜¯å½“æˆ‘ä»¬åˆ°è¾¾æ”¶æ•›ç‚¹æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›å™ªå£°æœ€å°ä»¥ä¾¿ä¸åç¦»)

![](img/c3e6bbc8b584c4737acec879784de77d.png)

æ¥æº:https://en.wikipedia.org/wiki/CMA-ES[ã€‚å›¾ç‰‡åŸºäºå°¼å¤æ‹‰Â·æ±‰æ£®å’Œå…¶ä»–äººçš„å·¥ä½œã€‚
çƒå½¢ä¼˜åŒ–æ™¯è§‚ç”¨ f å€¼ç›¸ç­‰çš„å®çº¿æç»˜ã€‚åœ¨è¿™ä¸ªç®€å•çš„ä¾‹å­ä¸­ï¼Œç¾¤ä½“(ç‚¹)åœ¨å‡ æ¬¡è¿­ä»£åé›†ä¸­äºå…¨å±€æœ€ä¼˜ã€‚](https://en.wikipedia.org/wiki/CMA-ES)

## ä»å¤´å¼€å§‹ Python å®ç°

è®©æˆ‘ä»¬é€šè¿‡ Python ä¸­çš„ä¸€ä¸ªç®€å•ä¾‹å­æ¥æ›´å¥½åœ°ç†è§£ã€‚æˆ‘è¯•å›¾æ·»åŠ ä¸€äº›ä¸æ•°å€¼ç¨³å®šæ€§ç›¸å…³çš„ç»†èŠ‚ã€‚è¯·çœ‹è¯„è®ºï¼æˆ‘ä»¬å°†ä»åŠ è½½æ‰€éœ€çš„åº“å’Œ MNIST æ‰‹å†™æ•°å­—æ•°æ®é›†å¼€å§‹ã€‚

```
# Importing all the required libraries
import numpy as np
import matplotlib.pyplot as plt
import tqdm
import pickle
import warnings
warnings.filterwarnings(â€˜ignoreâ€™)
from keras.datasets import mnist# Machine Epsilon (needed to calculate logarithms)
eps = np.finfo(np.float64).eps# Loading MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()# x contains the images (features to our model)
# y contains the labels 0 to 9# Normalizing the inputs between 0 and 1
x_train = x_train/255.
x_test = x_test/255.# Flattening the image as we are using 
# dense neural networks
x_train = x_train.reshape( -1, x_train.shape[1]*x_train.shape[2])
x_test = x_test.reshape( -1, x_test.shape[1]*x_test.shape[2])# Converting to one-hot representation
identity_matrix = np.eye(10) 
y_train = identity_matrix[y_train]
y_test = identity_matrix[y_test]# Plotting the images
fig, ax = plt.subplots(2,5)
for i, ax in enumerate(ax.flatten()):
 im_idx = np.argwhere(y_train == i)[0]
 plottable_image = np.reshape(x_train[im_idx], (28, 28))
 ax.set_axis_off()
 ax.imshow(plottable_image, cmap=â€™grayâ€™)

plt.savefig(â€˜mnist.jpgâ€™)
```

è¿™æ˜¯å›¾åƒçš„æ ·å­ï¼Œ

![](img/5f16f73301716ce2f43db30944654828.png)

MNIST æ‰‹å†™æ•°å­—æ•°æ®é›†æ ·æœ¬å›¾åƒ

æˆ‘ä»¬å°†ä»å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹å¼€å§‹ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªåªæœ‰æ­£å‘ä¼ é€’çš„å•å±‚ç¥ç»ç½‘ç»œã€‚

```
def soft_max(x):'''
 Arguments: numpy array

 Returns: numpy array after applying 
 softmax function to each
 element '''

 # Subtracting max of x from each element of x for numerical
 # stability as this results in the largest argument to 
 # exp being 0, ruling out the possibility of overï¬‚ow
 # Read more about it at :
 # [https://www.deeplearningbook.org/contents/numerical.html](https://www.deeplearningbook.org/contents/numerical.html)

 e_x = np.exp(x â€” np.max(x))

 return e_x /e_x.sum()class Model():'''
 Single layer Neural Network

'''

 def __init__(self, input_shape, n_classes):

 # Number of output classes
 self.n_classes = n_classes

 # Parameters/Weights of our network which we will be updating
 self.weights = np.random.randn(input_shape, n_classes)

 def forward(self,x):

 '''
 Arguments: numpy array containing the features,
 expected shape of input array is
 (batch size, number of features)

 Returns: numpy array containing the probability,
 expected shape of output array is
 (batch size, number of classes)

 '''

 # Multiplying weights with inputs
 x = np.dot(x,self.weights)

 # Applying softmax function on each row
 x = np.apply_along_axis(soft_max, 1, x)

 return x

 def __call__(self,x):

 '''
 This dunder function
 enables your model to be callable

 When the model is called using model(x),
 forward method of the model is called internally

 '''

 return self.forward(x)

 def evaluate(self, x, y, weights = None):

 '''Arguments : x â€” numpy array of shape (batch size,number of features),
 y â€” numpy array of shape (batch size,number of classes),
 weights â€” numpy array containing the parameters of the model

 Returns : Scalar containing the mean of the categorical cross-entropy loss
 of the batch'''

 if weights is not None:

 self.weights = weights

 # Calculating the negative of cross-entropy loss (since
 # we are maximizing this score)
 # Adding a small value called epsilon 
 # to prevent -inf in the output

 log_predicted_y = np.log(self.forward(x) + eps)

 return (log_predicted_y*y).mean()
```

æˆ‘ä»¬ç°åœ¨å°†å®šä¹‰æˆ‘ä»¬çš„å‡½æ•°ï¼Œå®ƒå°†ä¸€ä¸ªæ¨¡å‹ä½œä¸ºè¾“å…¥å¹¶æ›´æ–°å®ƒçš„å‚æ•°ã€‚

```
def optimize(model,x,y,
 top_n = 5, n_pop = 20, n_iter = 10,
 sigma_error = 1, error_weight = 1, decay_rate = 0.95,
 min_error_weight = 0.01 ):

 '''
 Arguments : model â€” Model object(single layer neural network here),
 x â€” numpy array of shape (batch size, number of features),
 y â€” numpy array of shape (batch size, number of classes),
 top_n â€” Number of elite parameters to consider for calculating the
 best parameter by taking mean
 n_pop â€” Population size of the parameters
 n_iter â€” Number of iteration 
 sigma_error â€” The standard deviation of errors while creating 
 population from best parameter
 error_weight â€” Contribution of error for considering new population
 decay_rate â€” Rate at which the weight of the error will reduce after 
 each iteration, so that we donâ€™t deviate away at the 
 point of convergence. It controls the balance between 
 exploration and exploitation

 Returns : Model object with updated parameters/weights

 '''

 # Model weights have been randomly initialized at first
 best_weights = model.weights

 for i in range(n_iter):

 # Generating the population of parameters
 pop_weights = [best_weights + error_weight*sigma_error* \
 np.random.randn(*model.weights.shape)

 for i in range(n_pop)]

 # Evaluating the population of parameters
 evaluation_values = [model.evaluate(x,y,weight) for weight in pop_weights]

 # Sorting based on evaluation score
 weight_eval_list = zip(evaluation_values, pop_weights)

 weight_eval_list = sorted(weight_eval_list, key = lambda x: x[0], reverse = True)

 evaluation_values, pop_weights = zip(*weight_eval_list)

 # Taking the mean of the elite parameters
 best_weights = np.stack(pop_weights[:top_n], axis=0).mean(axis=0)

 #Decaying the weight
 error_weight = max(error_weight*decay_rate, min_error_weight)

 model.weights = best_weights

 return model# Instantiating our model object
model = Model(input_shape= x_train.shape[-1], n_classes= 10)print(â€œEvaluation on training dataâ€, model.evaluate(x_train, y_train))# Running it for 200 steps
for i in tqdm.tqdm(range(200)):

 model = optimize(model, 
 x_train,
 y_train, 
 top_n = 10, 
 n_pop = 100,
 n_iter = 1)

 print(â€œTest data cross-entropy loss: â€œ, -1*model.evaluate(x_test, y_test))
 print(â€œTest Accuracy: â€œ,(np.argmax(model(x_test),axis=1) == y_test).mean())

# Saving the model for later use
with open(â€˜model.pickleâ€™,â€™wbâ€™) as f:
 pickle.dump(model,f)
```

**ç»“æœ**:ç»è¿‡ 200 æ¬¡è¿­ä»£è®­ç»ƒåï¼Œæµ‹è¯•å‡†ç¡®ç‡çº¦ä¸º 85%ï¼Œäº¤å‰ç†µæŸå¤±çº¦ä¸º 0.28ã€‚è¿™ç›¸å½“äºç”¨åå‘ä¼ æ’­è®­ç»ƒçš„å•å±‚ç¥ç»ç½‘ç»œã€‚æ³¨æ„ï¼Œè¿™é‡Œæˆ‘ä»¬ç”šè‡³æ²¡æœ‰ä½¿ç”¨ decayï¼Œå› ä¸º n_iter è®¾ç½®ä¸º 1ã€‚

## **ç»“å°¾æ³¨é‡Š**

ES å®ç°èµ·æ¥éå¸¸ç®€å•ï¼Œä¸éœ€è¦æ¢¯åº¦ã€‚ä»…ä»…é€šè¿‡å°†å™ªå£°æ³¨å…¥åˆ°æˆ‘ä»¬çš„å‚æ•°ä¸­ï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿæœç´¢å‚æ•°ç©ºé—´ã€‚å°½ç®¡ä¸ºäº†ä¾¿äºç†è§£ï¼Œæˆ‘ä»¬å·²ç»è§£å†³äº†ä¸€ä¸ªç›‘ç£é—®é¢˜ï¼Œä½†å®ƒæ›´é€‚åˆå¼ºåŒ–å­¦ä¹ åœºæ™¯ï¼Œåœ¨è¿™ç§åœºæ™¯ä¸­ï¼Œäººä»¬å¿…é¡»é€šè¿‡é‡‡æ ·æ¥ä¼°è®¡é¢„æœŸå›æŠ¥çš„æ¢¯åº¦ã€‚

å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼

æ›´å¤šçš„æŠ€æœ¯åšå®¢ä¹Ÿå¯ä»¥æŸ¥çœ‹æˆ‘çš„ç½‘ç«™:[æ·±æŒ– ML](https://www.digdeepml.com)

**å‚è€ƒèµ„æ–™å’Œè¿›ä¸€æ­¥é˜…è¯»**:

[OpenAI åšæ–‡](https://openai.com/blog/evolution-strategies/)

[å¥¥æ‰˜ç½—çš„åšå®¢](https://blog.otoro.net/2017/10/29/visual-evolution-strategies/)

[è‰è²çš„åšå®¢](https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html)