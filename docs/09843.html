<html>
<head>
<title>Introduction to Style Transfer with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 风格转换简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-style-transfer-with-pytorch-339ba2219621?source=collection_archive---------42-----------------------#2020-07-12">https://towardsdatascience.com/introduction-to-style-transfer-with-pytorch-339ba2219621?source=collection_archive---------42-----------------------#2020-07-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f8b1" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/applied-neural-networks" rel="noopener">神经网络的应用</a></h2><div class=""/><div class=""><h2 id="afdb" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">卷积神经网络的新用途</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/8d4bcb1d7e71f33305a910650ca13ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*Whw6-zafnPIx7EiAjrxnHg.jpeg"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">我电脑旁的艺术。</p></figure><p id="feb2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你知道神经网络不相信左右脑分离吗？</p><p id="8bed" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">典型的例子:左边的图像不是由人手的笔触创建的，而是由卷积神经网络创建的！</p><p id="0467" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这表明神经网络并不局限于复杂的数学和统计学。稍加帮助，他们甚至可以创造艺术！这篇文章旨在解释风格转移的概念，正如加蒂丝等人在这篇<a class="ae lz" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">有趣的研究论文</a>中所定义的那样。</p><h2 id="6749" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated">什么是风格转移？</h2><p id="3b74" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">风格转移是由 Leon A. Gatys 等人开发的卷积神经网络的一种新颖应用。它允许对图像的“内容”和“风格”进行精确的数学定义。有了内容和风格，我们可以定义一种新的损失函数来描述两个图像之间的风格和内容的差异。然后，通过反向传播，我们可以更新一个图像的像素，以更紧密地匹配另一个图像的样式或内容。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/70f23acface0c87c41a786e3aad60215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fVNjuskht5pSnNwB4KOITA.jpeg"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated"><strong class="bd nc">左</strong>:梵高的星夜。<strong class="bd nc">右图</strong>:安德烈亚斯·普雷夫克拍摄</p></figure><p id="6676" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这些图像分别包含文章简介中图像的样式和内容。</p><h2 id="f154" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated">对 ConvNets 的一点回顾</h2><p id="1ab4" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">卷积神经网络是以这样一种方式独特设计的，即它们擅长识别和分离视觉输入中的模式。</p><p id="a971" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下面是它们工作原理的大概描述:</p><ol class=""><li id="06ca" class="nd ne it lf b lg lh lj lk lm nf lq ng lu nh ly ni nj nk nl bi translated">图像被传递到 ConvNet 的第一个卷积层。</li><li id="239b" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly ni nj nk nl bi translated">第一个卷积层让图像通过一组过滤器，这些过滤器检测像垂直线和水平线这样的简单图案。这些图案被提取并作为新的图像通道输出(每个滤镜一个)。</li><li id="4884" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly ni nj nk nl bi translated">然后，这些新的镜像通道被馈送到下一个卷积层，并重复该过程。本质上，网络是在检测模式中的模式！</li><li id="68f5" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly ni nj nk nl bi translated">有时，卷积层的输出可能会经过一个池层。池层本质上抛弃了“细节”,但保留了全局模式。这允许一个 ConvNet 将它所知道的推广到以前从未见过的图像。</li></ol><h2 id="aecd" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated">定义网络</h2><p id="c9f7" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">图像风格传输最初在卷积网络上执行，该网络包含由池层分隔的 5 个组中的 16 个卷积层。这个网络叫做 VGG19。每个组在输入中找到模式，然后将这些模式传递到池层，丢弃一些细节，但保留大图。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nr"><img src="../Images/dd0321bcd6d27ddfb0cee21c0104cd76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WLWfvz1HrYXSAZVAZybbaw.jpeg"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">最左边的黑色方块是原始图像。被网络过滤还原。</p></figure><p id="bcb3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将使用 PyTorch 预先培训的 VGG 网络开始:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h2 id="05bf" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated">定义图像内容</h2><p id="3ab9" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">根据我们对 ConvNets 的了解，它们保持一般模式，但开始丢弃网络深层的细节。记住这一点，就很容易理解为什么图像的“内容”被定义为网络深层的输出。</p><p id="3072" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将使用第 10 个卷积层的输出来定义图像的“内容”。我们将这一层命名为<strong class="lf jd"> conv4_2 </strong>，因为它是第四叠卷积层中的第二层。</p><p id="dd5f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下面是一个从网络中提取所选图层内容的函数。这些层是根据它们在链接的研究论文中的用途选择的。请注意，<strong class="lf jd"> conv4_2 </strong>也在其中。其余的将用于确定风格:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="eb5f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">使用<strong class="lf jd"> conv4_2 </strong>来描述“内容”给了我们想要模拟的一般结构。使用较浅的图层保留更多细节，使用较深的图层丢弃更多细节。您可以根据自己的艺术喜好选择使用不同的图层。</p><h2 id="e3f1" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated">定义风格</h2><p id="14ca" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">如果您知道如何计算样本空间的相关矩阵(通常被视为相关值的热图)，那么这将看起来很熟悉，而且相当容易。</p><p id="e0d2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回想一下，任何给定卷积层的输出都是一组新的镜像通道，每个通道描述输入中的一些模式或特征。这些通道中的每一个都包含输入图像的过滤版本，该版本突出显示某些特征或图案。然后，我们将风格定义为这些不同特征之间的相关性，并使用<a class="ae lz" href="https://en.wikipedia.org/wiki/Gramian_matrix" rel="noopener ugc nofollow" target="_blank"> Gramian 矩阵</a>计算相关性。</p><p id="424f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">还记得图像通道只是像素值的 2D 网格。然后，为了确定来自单个卷积层的不同模式信道之间的相关性，我们执行以下操作:</p><ol class=""><li id="b8c1" class="nd ne it lf b lg lh lj lk lm nf lq ng lu nh ly ni nj nk nl bi translated">对给定图层输出的每个通道进行矢量化。换句话说，分解像素值网格，从每个输出通道创建一个行向量。</li><li id="a776" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly ni nj nk nl bi translated">将所有这些行向量一个接一个地堆叠起来，创建一个二维矩阵。姑且称这个矩阵为<strong class="lf jd"> <em class="nu"> S </em> </strong> <em class="nu">。</em></li><li id="4d95" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly ni nj nk nl bi translated">将这个新的<strong class="lf jd"> <em class="nu"> S </em> </strong> <em class="nu"> </em>矩阵乘以它的转置。这个结果就是格拉米矩阵。</li></ol><p id="e89f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下面是一个小函数，它用两行代码为我们完成了这项工作:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="536a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了更好地感受图像的“风格”,我们为 5 个不同的卷积层分别创建了 5 个独立的格拉米矩阵。</p><pre class="ks kt ku kv gt nv nw nx ny aw nz bi"><span id="9905" class="ma mb it nw b gy oa ob l oc od">conv1_1<br/>conv2_1<br/>conv3_1<br/>conv4_1<br/>conv5_1</span></pre><p id="9b64" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能想知道这 5 层是如何被选择来代表输入图像的“风格”的。简而言之，我选择使用与作者相同的层。请随意尝试不同的层！</p><h1 id="06f9" class="oe mb it bd mc of og oh mf oi oj ok mi ki ol kj ml kl om km mo ko on kp mr oo bi translated">定义损失</h1><p id="2c99" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">因为我们希望创建一个包含一个父图像的样式和另一个父图像的内容的新图像，所以我们必须定义一个考虑样式和内容的损失函数。</p><h2 id="1aa0" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated">内容损失</h2><p id="49e5" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">内容流失很容易！内容损失被正式定义为两幅图像的内容之间的均方误差。图像的内容被定义为层<strong class="lf jd"> conv4_2 </strong>的输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/2d8671c7c93eb55461cbb53ecb204336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/0*u_csA5FIJLKHZ3SW.png"/></div></figure><p id="4879" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果上面的等式令人困惑，那也没关系。使用线性代数可以大大简化这个方程:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="cae1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回头看看<code class="fe oq or os nw b">get_features()</code>的代码片段，您会看到该函数返回了一个字典，其中包含以下各层的所有特征通道:</p><pre class="ks kt ku kv gt nv nw nx ny aw nz bi"><span id="a213" class="ma mb it nw b gy oa ob l oc od">conv1_1<br/>conv2_1<br/>conv3_1<br/>conv4_1<br/><strong class="nw jd">conv4_2</strong><br/>conv5_1</span></pre><p id="d97d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><code class="fe oq or os nw b">c_features</code>和<code class="fe oq or os nw b">t_features</code>仅仅是<code class="fe oq or os nw b">get_features()</code>应用于内容模板图像和目标图像时的输出。因此，将代码与等式匹配，我们看到张量<strong class="lf jd"> <em class="nu"> T </em> </strong>和<strong class="lf jd"> <em class="nu"> C </em> </strong>在代码中定义如下:</p><pre class="ks kt ku kv gt nv nw nx ny aw nz bi"><span id="33d0" class="ma mb it nw b gy oa ob l oc od">T = t_features['conv4_2']<br/>C = c_features['conv4_2']</span></pre><h2 id="5703" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated">风格丧失</h2><p id="983d" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">风格损失在数学上稍微复杂一点，但实现起来很简单。通过首先计算目标图像的格拉米矩阵和样式模板图像的格拉米矩阵的均方误差来找到样式损失:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/4ccee0f93e57503b573cb78adab78574.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/0*2o6gI0O5Z4oVxexC.png"/></div></figure><p id="989f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这里，<strong class="lf jd"> <em class="nu"> P </em> </strong>和<strong class="lf jd"> <em class="nu"> G </em> </strong>是目标图像和样式模板图像的格拉米矩阵。上标<strong class="lf jd"> <em class="nu"> l </em> </strong>表示计算 Gramians 的图层输出:</p><pre class="ks kt ku kv gt nv nw nx ny aw nz bi"><span id="fadb" class="ma mb it nw b gy oa ob l oc od"><strong class="nw jd">conv1_1 --&gt; E_1 <br/>conv2_1 --&gt; E_2<br/>conv3_1 --&gt; E_3<br/>conv4_1 --&gt; E_4</strong><br/>conv4_2 --&gt; N/A (used for content loss)<br/><strong class="nw jd">conv5_1 --&gt; E_5</strong></span></pre><p id="155b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">同样，这些层输出存储在由<code class="fe oq or os nw b">get_features()</code>返回的字典中。</p><p id="af73" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">接下来，我们求出每个误差项的加权和:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/b79c01e10f077498f6c2fe6780251d78.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/0*amYQQE-Il82KsbZ0.png"/></div></figure><p id="2ae4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">权重<strong class="lf jd"> <em class="nu"> w </em> </strong>只是用户出于艺术偏好而选择的。每一层的错误都会对最终艺术表现的结果产生不同的影响。</p><p id="da9e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下面是计算风格损失的代码:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h2 id="35a7" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated">全损</h2><p id="3428" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">总损失是风格和内容损失的线性组合:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/bd7b4bf280352d4bd74da063d071b9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/0*W8VRHgSmFQSSS5ig.png"/></div></figure><p id="3b17" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">其中，α和β是比例因子。α/β的比率将决定新目标图像中的样式/内容比率。在实践中，β会大得多，因为风格误差的尺度小得多。</p><h1 id="e8ab" class="oe mb it bd mc of og oh mf oi oj ok mi ki ol kj ml kl om km mo ko on kp mr oo bi translated">把所有的放在一起</h1><p id="5aa4" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">最后要做的事情是把前馈和反向传播结合起来。这一次，我们不更新网络参数！相反，我们正在更新我们的目标图像的像素值，以便它迭代地处理我们的样式图像的样式和我们的内容图像的内容:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="a46f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">不要害怕使用代码中的参数来实现您想要的艺术风格。这需要一些练习，并不是每一组图像都像你期望的那样。好好享受吧！</p><p id="bedc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果你想看我的关于风格转移的完整 Jupyter 笔记本(包括我遗漏的一些助手功能)，你可以在这里找到它。</p><p id="7ff3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，我将留给你这张结合了抽象艺术和积云的航拍照片:)</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/e7e72d5674c162651af04061cfbc7fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*Dni8gOVZS56OFY6EcjmSZw.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated"><strong class="bd nc">内容:</strong> <a class="ae lz" href="https://www.princeton.edu/news/2018/01/10/spotty-coverage-climate-models-underestimate-cooling-effect-daily-cloud-cycle" rel="noopener ugc nofollow" target="_blank">普林斯顿环境学院</a>。<strong class="bd nc">风格:</strong> <a class="ae lz" href="https://www.cianellistudios.com/" rel="noopener ugc nofollow" target="_blank">杰森·恰内利</a></p></figure></div></div>    
</body>
</html>