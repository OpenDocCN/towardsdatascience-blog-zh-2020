<html>
<head>
<title>Intuitive Understanding of Randomized Singular Value Decomposition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对随机化奇异值分解的直观理解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuitive-understanding-of-randomized-singular-value-decomposition-9389e27cb9de?source=collection_archive---------16-----------------------#2020-07-01">https://towardsdatascience.com/intuitive-understanding-of-randomized-singular-value-decomposition-9389e27cb9de?source=collection_archive---------16-----------------------#2020-07-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6119" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">随机线性代数奇异值分解的 Python 实现</h2></div><p id="1234" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">矩阵分解是许多机器学习问题的强大工具，它已经广泛用于数据压缩、降维和稀疏学习等。在许多情况下，为了用低秩结构逼近数据矩阵，奇异值分解(SVD)通常被证明是最佳选择。然而，大数据矩阵(例如，8k 乘 10k 矩阵)的准确且有效的 SVD 在计算上具有挑战性。为了解决这种情况下的奇异值分解，许多算法通过应用随机线性代数方法被开发出来。最重要的算法之一是随机化 SVD，它对于分解任何具有相对较低秩的大型矩阵具有竞争效率。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/6605c546ecbe5dc725a62786f171e0a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yzYQiMHDl3k-DUEejENqvQ.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图 SVD 主要发展的时间表。(图片来自[2])</p></figure><p id="d054" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章将介绍随机奇异值分解的基本思想。为了帮助读者更好地理解随机化 SVD，我们在本文中还提供了相应的 Python 实现。另外，这个帖子的 Jupyter 笔记本可以在<a class="ae lu" href="https://nbviewer.jupyter.org/github/xinychen/tensor-learning/blob/master/tutorial/randomized_svd.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="60b6" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">初步的</h1><p id="d612" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated"><strong class="kk iu">奇异值分解公式</strong></p><p id="fcca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先回顾奇异值分解的概念。你可能已经知道，SVD 是线性代数中最重要的分解公式之一。对于任意给定的矩阵<strong class="kk iu"> <em class="mz"> A </em> </strong>，SVD 具有如下形式</p><p id="5512" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="mz">a</em></strong>=<strong class="kk iu"><em class="mz">u</em>σ<em class="mz">v</em></strong>^t</p><p id="3f9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中矩阵<strong class="kk iu"><em class="mz"/></strong>和<strong class="kk iu"> <em class="mz"> V </em> </strong>分别由左和右奇异向量组成。<strong class="kk iu">σ</strong>的对角线项为奇异值。</p><p id="8a10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">一个小矩阵例子</strong></p><p id="a24d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以一个 3 乘 3 的矩阵为例，我们可以通过使用 Python 中的<code class="fe na nb nc nd b">numpy.linalg.svd()</code>来计算 SVD。让我们看一看:</p><pre class="lf lg lh li gt ne nd nf ng aw nh bi"><span id="90cb" class="ni md it nd b gy nj nk l nl nm">import numpy as np</span><span id="9678" class="ni md it nd b gy nn nk l nl nm">A = np.array([[1, 3, 2],<br/>              [5, 3, 1],<br/>              [3, 4, 5]])<br/>u, s, v = np.linalg.svd(A, full_matrices = 0)<br/>print('Left singular vectors:')<br/>print(u)<br/>print()<br/>print('Singular values:')<br/>print(s)<br/>print()<br/>print('Right singular vectors:')<br/>print(v)<br/>print()</span></pre><p id="8be4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，奇异值为 9.3427、3.2450 和 1.0885。</p><pre class="lf lg lh li gt ne nd nf ng aw nh bi"><span id="2b05" class="ni md it nd b gy nj nk l nl nm">Left singular vectors:<br/>[[-0.37421754 0.28475648 -0.88253894]<br/> [-0.56470638 -0.82485997 -0.02669705]<br/> [-0.7355732 0.48838486 0.46948087]]</span><span id="b113" class="ni md it nd b gy nn nk l nl nm">Singular values:<br/>[9.34265841 3.24497827 1.08850813]</span><span id="862b" class="ni md it nd b gy nn nk l nl nm">Right singular vectors:<br/>[[-0.57847229 -0.61642675 -0.53421706]<br/> [-0.73171177 0.10269066 0.67383419]<br/> [ 0.36051032 -0.78068732 0.51045041]]</span></pre><h1 id="e839" class="mc md it bd me mf no mh mi mj np ml mm jz nq ka mo kc nr kd mq kf ns kg ms mt bi translated">随机化奇异值分解</h1><p id="1e92" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated"><strong class="kk iu">基本理念</strong></p><p id="c27a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随机化 SVD 可以分为三个主要步骤。对于任意给定的<em class="mz"> m </em> -by- <em class="mz"> n </em>矩阵<strong class="kk iu"><em class="mz"/></strong>，如果我们强加一个目标秩<em class="mz"> k </em>与<em class="mz"> k </em> &lt; min( <em class="mz"> m </em>，<em class="mz"> n </em>)，那么如图 2 所示的第一步就是</p><ul class=""><li id="a159" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">1)通过<em class="mz"> -k </em>生成大小为<em class="mz"> n- </em>的高斯随机矩阵<strong class="kk iu">ω</strong>，</li><li id="adc6" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">2)计算新的<em class="mz"> m </em> -by- <em class="mz"> k </em>矩阵<strong class="kk iu"> <em class="mz"> Y </em> </strong>，</li><li id="c3f4" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">以及 3)对矩阵<strong class="kk iu"> <em class="mz"> Y </em> </strong>应用 QR 分解。</li></ul><p id="35b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，第一步需要返回<em class="mz"> m </em> -by- <em class="mz"> k </em>矩阵<strong class="kk iu"> <em class="mz"> Q </em> </strong>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oh"><img src="../Images/f381c93867dfc8e46104482b12345c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fj_FeePfJEyWaTF57jUGyg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图 2:随机化 SVD 的第一步。(图片来自[2])</p></figure><p id="000f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，如图 3 所示的第二步是</p><ul class=""><li id="6d18" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">4)将<strong class="kk iu"> <em class="mz"> Q </em> </strong>的转置矩阵与矩阵<strong class="kk iu"> <em class="mz"> A </em> </strong>相乘，得到一个<em class="mz"> k </em> -by- <em class="mz"> n </em>矩阵<strong class="kk iu"> <em class="mz"> B </em> </strong>，</li><li id="59bb" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">以及 5)计算矩阵<strong class="kk iu"> <em class="mz"> B. </em> </strong>的 SVD 这里，不是计算原矩阵<strong class="kk iu"> <em class="mz"> A </em> </strong>，<strong class="kk iu"> <em class="mz"> B </em> </strong>的 SVD，而是一个更小的矩阵来进行工作。</li></ul><p id="a814" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于矩阵<strong class="kk iu"><em class="mz"/></strong>的奇异值(即<strong class="kk iu">σ</strong>)和右奇异向量(即<strong class="kk iu"> <em class="mz"> V </em> </strong>)也是原矩阵<strong class="kk iu"><em class="mz"/></strong>A 的奇异值和右奇异向量，我们应该保留矩阵<strong class="kk iu"> <em class="mz"> B 计算的奇异值和右奇异向量</em></strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oi"><img src="../Images/4780d79bb6b915ea98f866d8aa4d52fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7vAta7Ytsrd9dlSafveT9g.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图 3:随机化 SVD 的第二步和第三步。(图片来自[2])</p></figure><p id="5550" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如图 3 所示，如果将第一步中导出的矩阵<strong class="kk iu"> <em class="mz"> Q </em> </strong>与<strong class="kk iu"> <em class="mz"> B </em> </strong>的左奇异向量组合，就可以得到第三步中矩阵<strong class="kk iu"><em class="mz">【A</em></strong>的左奇异向量(即<strong class="kk iu"> <em class="mz"> U </em> </strong>)。</p><p id="e997" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">一个小矩阵例子</strong></p><p id="dc5a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">即使我们在上面已经学习了随机化 SVD 的基本思想，如果没有直观的例子，它也不会真正清楚。为此，我们遵循前面提到的小矩阵奇异值分解。</p><p id="95ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们尝试编写随机化 SVD 的 Python 函数。这里，我们将使用两个 Numpy 函数，即<code class="fe na nb nc nd b">np.linalg.qr()</code>和<code class="fe na nb nc nd b">np.linalg.svd()</code>。</p><pre class="lf lg lh li gt ne nd nf ng aw nh bi"><span id="3523" class="ni md it nd b gy nj nk l nl nm">import numpy as np</span><span id="8d02" class="ni md it nd b gy nn nk l nl nm">def rsvd(A, Omega):<br/>    Y = A @ Omega<br/>    Q, _ = np.linalg.qr(Y)<br/>    B = Q.T @ A<br/>    u_tilde, s, v = np.linalg.svd(B, full_matrices = 0)<br/>    u = Q @ u_tilde<br/>    return u, s, v</span></pre><p id="95d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们用 3 乘 3 矩阵(<code class="fe na nb nc nd b">rank = 2</code>表示<em class="mz"> k </em>用<em class="mz">k</em>T95】min(<em class="mz">m</em>，<em class="mz"> n </em>)来测试一下:</p><pre class="lf lg lh li gt ne nd nf ng aw nh bi"><span id="61af" class="ni md it nd b gy nj nk l nl nm">np.random.seed(1000)</span><span id="17e4" class="ni md it nd b gy nn nk l nl nm">A = np.array([[1, 3, 2],<br/>              [5, 3, 1],<br/>              [3, 4, 5]])<br/>rank = 2<br/>Omega = np.random.randn(A.shape[1], rank)<br/>u, s, v = rsvd(A, Omega)<br/>print('Left singular vectors:')<br/>print(u)<br/>print()<br/>print('Singular values:')<br/>print(s)<br/>print()<br/>print('Right singular vectors:')<br/>print(v)<br/>print()</span></pre><p id="7b4a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个随机 SVD 示例的结果是:</p><pre class="lf lg lh li gt ne nd nf ng aw nh bi"><span id="2cfd" class="ni md it nd b gy nj nk l nl nm">Left singular vectors:<br/>[[ 0.38070859  0.60505354]<br/> [ 0.56830191 -0.74963644]<br/> [ 0.72944767  0.26824507]]</span><span id="e835" class="ni md it nd b gy nn nk l nl nm">Singular values:<br/>[9.34224023 3.02039888]</span><span id="af98" class="ni md it nd b gy nn nk l nl nm">Right singular vectors:<br/>[[ 0.57915029  0.61707064  0.53273704]<br/> [-0.77420021  0.21163814  0.59650929]]</span></pre><p id="b023" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回想一下，这个矩阵的奇异值是<strong class="kk iu"> 9.3427 </strong>、<strong class="kk iu"> 3.2450 </strong>和 1.0885。在这种情况下，随机化 SVD 的前两个奇异值为<strong class="kk iu"> 9.3422 </strong>和<strong class="kk iu"> 3.0204 </strong>。</p><p id="63ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，由这两种 SVD 算法计算的第一奇异值非常接近。但是，随机化 SVD 的第二奇异值略有偏差。有没有其他方法可以改善这个结果？又是怎么做到的？</p><p id="1f5a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">答案是肯定的！</p><h1 id="69d8" class="mc md it bd me mf no mh mi mj np ml mm jz nq ka mo kc nr kd mq kf ns kg ms mt bi translated">幂迭代随机奇异值分解</h1><p id="6683" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">为了提高随机奇异值分解的质量，可以直接使用幂迭代法。关于幂迭代的更多细节，请参见[1]第 39 页，第 40 页也有一个 Matlab 实现。</p><p id="bb09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的 Python 代码中，<code class="fe na nb nc nd b">power_iteration()</code>是迭代计算<em class="mz"> m </em> -by- <em class="mz"> k </em>矩阵<strong class="kk iu"> <em class="mz"> Y </em> </strong>然后通过 QR 分解导出<em class="mz"> m </em> -by- <em class="mz"> k </em>矩阵<strong class="kk iu"> <em class="mz"> Q </em> </strong>的函数。</p><pre class="lf lg lh li gt ne nd nf ng aw nh bi"><span id="9b1c" class="ni md it nd b gy nj nk l nl nm">import numpy as np</span><span id="84b0" class="ni md it nd b gy nn nk l nl nm">def power_iteration(A, Omega, power_iter = 3):<br/>    Y = A @ Omega<br/>    for q in range(power_iter):<br/>        Y = A @ (A.T @ Y)<br/>    Q, _ = np.linalg.qr(Y)<br/>    return Q</span><span id="3dc3" class="ni md it nd b gy nn nk l nl nm">def rsvd(A, Omega):<br/>    Q = power_iteration(A, Omega)<br/>    B = Q.T @ A<br/>    u_tilde, s, v = np.linalg.svd(B, full_matrices = 0)<br/>    u = Q @ u_tilde<br/>    return u, s, v</span></pre><p id="f2b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们测试一下新的<code class="fe na nb nc nd b">rsvd()</code>函数:</p><pre class="lf lg lh li gt ne nd nf ng aw nh bi"><span id="f0e6" class="ni md it nd b gy nj nk l nl nm">np.random.seed(1000)</span><span id="00cd" class="ni md it nd b gy nn nk l nl nm">A = np.array([[1, 3, 2],<br/>              [5, 3, 1],<br/>              [3, 4, 5]])<br/>rank = 2<br/>Omega = np.random.randn(A.shape[1], rank)<br/>u, s, v = rsvd(A, Omega)<br/>print('Left singular vectors:')<br/>print(u)<br/>print()<br/>print('Singular values:')<br/>print(s)<br/>print()<br/>print('Right singular vectors:')<br/>print(v)<br/>print()</span></pre><p id="2251" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果是:</p><pre class="lf lg lh li gt ne nd nf ng aw nh bi"><span id="079f" class="ni md it nd b gy nj nk l nl nm">Left singular vectors:<br/>[[ 0.37421757  0.28528579]<br/> [ 0.56470638 -0.82484381]<br/> [ 0.73557319  0.48810317]]</span><span id="7e48" class="ni md it nd b gy nn nk l nl nm">Singular values:<br/>[9.34265841 3.24497775]</span><span id="039b" class="ni md it nd b gy nn nk l nl nm">Right singular vectors:<br/>[[ 0.57847229  0.61642675  0.53421706]<br/> [-0.73178429  0.10284774  0.67373147]]</span></pre><p id="5d68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回想一下:</p><ul class=""><li id="1a0e" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">奇异值分解的奇异值有:<strong class="kk iu"> 9.3427 </strong>，<strong class="kk iu"> 3.2450 </strong>，1.0885。</li><li id="ef8c" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">无幂迭代的随机化 SVD 奇异值有:<strong class="kk iu"> 9.3422 </strong>和<strong class="kk iu"> 3.0204 </strong>。</li><li id="fa8f" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">幂迭代的随机化 SVD 奇异值为:<strong class="kk iu"> 9.3427 </strong>和<strong class="kk iu"> 3.2450 </strong>。</li></ul><p id="d023" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如您所见，幂迭代随机化 SVD 提供了极其精确的奇异值。</p><h1 id="3696" class="mc md it bd me mf no mh mi mj np ml mm jz nq ka mo kc nr kd mq kf ns kg ms mt bi translated">图像压缩</h1><p id="c2b1" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">如上所述，可以使用 SVD 或随机化 SVD 来压缩(低秩)信号矩阵。事实上，使用奇异值分解压缩图像的方法非常简单:直接对图像进行奇异值分解，只保留主奇异值和左/右奇异向量。在随机奇异值分解方面，我们可以先预先确定主奇异值的个数，然后通过随机奇异值分解得到奇异值和左/右奇异向量。</p><p id="1c47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于我们的评估，我们选择<strong class="kk iu">莉娜</strong>的彩色图像作为我们的数据。这张图片的尺寸是 256×256×3。在这里，我们通过只选择绿色通道来构建一个大小为 256×256 的矩阵<strong class="kk iu"><em class="mz"/></strong>。</p><ul class=""><li id="996e" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated"><strong class="kk iu">直接使用 SVD</strong></li></ul><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oj"><img src="../Images/988725bcc789585a2532d8ba90acbf2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwUoeb82A2l98c8bQRDPQw.png"/></div></div></figure><ul class=""><li id="0815" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated"><strong class="kk iu">使用随机奇异值分解代替</strong></li></ul><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ok"><img src="../Images/e89ea87b22e2835e5c982f19a9d4ff74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1dma6Pik1m1awKqh3PJCHw.png"/></div></div></figure><p id="61dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这个图像压缩实验中我们可以看出:</p><ul class=""><li id="5786" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">1)通过与 SVD 相比较，随机化的 SVD 也可以产生具有规定的低秩的精确压缩(这里，我们设置<code class="fe na nb nc nd b">rank = 50</code>)。</li><li id="e82c" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">2)随机化的 SVD 是计算友好的。通过指定<code class="fe na nb nc nd b">rank = 50</code>，随机化 SVD 的总 CPU 时间约为<strong class="kk iu"> 11.6 ms </strong>，而 SVD 的总 CPU 时间为<strong class="kk iu"> 31.5 ms </strong>。</li></ul><h1 id="a8de" class="mc md it bd me mf no mh mi mj np ml mm jz nq ka mo kc nr kd mq kf ns kg ms mt bi translated">摘要</h1><p id="7c4d" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">在本文中，您发现了 SVD 的随机化线性代数方法。具体来说，您学到了:</p><ul class=""><li id="fd93" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">随机化奇异值分解的基本思想。</li><li id="3f25" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">如何用 Python 实现随机化的 SVD？</li></ul><p id="ad6d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你有什么问题吗？</p><h1 id="86a1" class="mc md it bd me mf no mh mi mj np ml mm jz nq ka mo kc nr kd mq kf ns kg ms mt bi translated">参考</h1><p id="cb9c" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">[1]史蒂文·l·布伦顿，j·内森·库兹(2019)。数据驱动的科学与工程:机器学习、动力系统和控制。第 37–41 页。</p><p id="dc79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2] N .本杰明·埃里克森，谢尔盖·沃罗宁，史蒂文·l·布伦顿，j .内森·库兹(2016)。使用 R. arXiv 的随机矩阵分解:1608.02148。[ <a class="ae lu" href="https://arxiv.org/pdf/1608.02148.pdf" rel="noopener ugc nofollow" target="_blank"> PDF </a></p></div></div>    
</body>
</html>