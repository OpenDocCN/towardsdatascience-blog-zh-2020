# 用石灰解释机器学习分类器

> 原文：<https://towardsdatascience.com/explaining-machine-learning-classifiers-with-lime-def5adaacfea?source=collection_archive---------39----------------------->

## 石灰在实践中的效果如何？

机器学习算法可以在分类、预测、异常检测和许多其他难题中产生令人印象深刻的结果。理解结果的基础通常是复杂的，因为许多算法是黑盒，很难看到它们的内部工作。[可解释的 AI](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence) 是一个术语，指的是为 ML 算法输出提供人类可理解的解释的技术。

可解释的人工智能很有趣[因为](https://www.darpa.mil/attachments/XAIProgramUpdate.pdf) [许多](https://arxiv.org/abs/1909.06342) [原因](https://ucbrise.github.io/cs294-ai-sys-sp19/assets/lectures/lec26/xai.pdf)，[包括](https://www.h2o.ai/blog/h2o-world-explainable-machine-learning-discussions-recap/)能够推理所使用的算法，我们用来训练它们的数据，以及更好地理解如何使用这些算法测试系统。

[LIME](https://github.com/marcotcr/lime) 或[局部可解释的模型不可知解释](https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/)是最近似乎在该领域受到关注的一种技术。LIME 的想法是给它一个单独的数据点，以及要使用的 ML 算法，它将尝试为该特定数据点的 ML 算法的输出建立可理解的解释。如“因为发现此人打喷嚏咳嗽(datapoint 特征)，所以很大概率是得了流感(ML 输出)”。

[有](https://blog.dominodatalab.com/shap-lime-python-libraries-part-1-great-explainers-pros-cons/) [有](/understanding-model-predictions-with-lime-a582fdff3a3b) [有大量](https://medium.com/analytics-vidhya/explain-your-model-with-lime-5a1a5867b423)的[介绍性的](https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/) [文章](/explainable-machine-learning-for-healthcare-7e408f8e5130) [围绕](/explain-nlp-models-with-lime-shap-5c5a9f84d59b)为[石灰](https://christophm.github.io/interpretable-ml-book/lime.html)但我觉得我需要一些更具体的东西。所以我在几个分类器和数据集/数据点上尝试了一下。本文讨论了这些结果。

对于不耐烦的人，我可以总结石灰似乎很有趣，并在正确的方向上前进，但我仍然发现解释细节令人困惑。这并没有让我对解释很有信心。对于易于理解和高可信度的解释，似乎还有很多路要走。

![](img/be19655a6b98c6f1ead53d3330a0f9f3.png)

如果故事很枯燥，给自己拿一杯酸橙饮料，继续读下去。图片来自 [Pixabay](https://pixabay.com/photos/lime-club-soda-drink-cocktail-907124/) 的[Steve buiss NNE](https://pixabay.com/users/stevepb-282134/)。

# 实验设置

## 概观

本文的实验分为三个部分。首先，我尝试使用 LIME 来解释专门为表格数据设计的三种不同 ML 算法的输出。第二，我尝试解释一般神经网络架构的输出。第三，我尝试了一个与前两个问题相反的回归问题，前两个问题检查了一个分类问题。这三个部分都使用 LIME 来解释一些数据点，每个数据点都来自不同的数据集。

## 反转值

作为一个小实验，对于我实验中的每个 ML 算法，我选取了一个被 LIME 评为对数据点的解释有较高贡献的单一特征，并反转(或改变)了它们的值。然后，我在相同的数据点上重新运行了 ML 算法和 LIME，改变了单个值，并比较了解释。

在大多数情况下，反转特征是二元分类特征(在一种情况下，分类具有 4 个可能的值)，使得反转过程变得明显(例如，将性别从男性变为女性或者相反)。这样做的目的只是为了查看更改石灰权重较高的要素的值是否会导致 ML 算法输出和相关石灰权重变量重要性的较大变化。

## 数据集和要素

不同部分使用的数据集:

*   《泰坦尼克号》:一个特定的人有哪些特征可以被归类为幸存者？
*   [心脏病 UCI](https://www.kaggle.com/ronitf/heart-disease-uci) :什么特征导致一个特定的人被归类为有心脏病风险？
*   [艾姆斯房屋数据集](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview):哪些特征对预测房价有积极影响，哪些有消极影响？

# 树提升分类器

应用的算法:

*   泰坦尼克号:来自 LGBM，CatBoost，XGBoost 的分类器
*   心脏病 UCI: Keras 多层感知器神经网络架构
*   Ames 住房数据集:来自 XGBoost 的回归量

我在表格数据中看到的一些最流行的分类器是基于梯度增强决策树的分类器； [LGBM](https://github.com/Microsoft/LightGBM) 、 [Catboost](https://catboost.ai/) 和 [XGBoost](https://github.com/dmlc/xgboost) 。还有很多其他的方法，我有时也会用到，比如朴素贝叶斯、随机森林和逻辑回归。然而，LGBM、Catboost 和 XGBoost 是我最近经常首先尝试的表格数据。因此，在这一节中，我尝试使用 LIME 来解释这些 ML 算法的一些数据点。我希望对其他最大似然算法的类似评估应该遵循一个非常相似的过程。

对于这一部分，我使用的是 [Titanic](https://www.kaggle.com/c/titanic) 数据集。这个数据集的目标是预测谁能在海难中幸存，谁不能。其特点:

1.  *存活* : 0 =否，1 =是
2.  *pclass* :车票等级(1 =第一，2 =第二，3 =第三)
3.  *年龄*:以年为单位的年龄
4.  sibsp :泰坦尼克号上的兄弟姐妹/配偶数量
5.  泰坦尼克号上父母/孩子的数量
6.  *票*:票号
7.  *票价*:客运票价
8.  *舱室*:舱室编号
9.  *已装船*:装船港(C =瑟堡，Q =皇后镇，S =南安普顿)

实际的笔记本代码在我的 [Github](https://github.com/mukatee/ml-experiments/blob/master/lime/Explaining%20models%20with%20LIME.ipynb) 和 [Kaggle 笔记本](https://www.kaggle.com/donkeys/explaining-models-with-lime-et-al)上都有。

三个提升模型(LGBM、Catboost、XGBoost)中的每一个都以特征权重的形式提供对其内部统计数据的访问。详情查看部分[文章](https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/)和[文档](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)。与 LIME 试图解释的单一数据点相反，这些类型的模型特征权重在所有数据点上提供了模型工作的更全面的视图。在下文中，我将展示这些特征权重，以便进行比较。

然而，也有一些[非常好的批评](https://explained.ai/rf-importance/)使用这些类型的分类器内部统计进行特征重要性，注意到与其他技术如[排列重要性](https://scikit-learn.org/stable/modules/permutation_importance.html)和 drop-column 重要性进行比较可能也是有意义的。因此，我还计算了这里三个助推器中每一个的*排列重要性*，以及随后的 Keras NN 分类器。

# LGBM

## 来自分类器/排列的特征权重

下图说明了当我通过分类器`feature_importances_`属性在 Titanic 数据集上训练模型时，模型本身给出的权重。

![](img/c00ac41a1e75deaad6e90dbd1fe8c190.png)

LGBM 报告了特征重要性。图片作者。

而下图所示的是 SKLearn 的[置换重要性函数](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html)对同一分类器给出的。

![](img/4c77d4d75b0ff1b5dc2e16a48a5feba7.png)

排列重要性算法报告的特征重要性。图片作者。

比较上面的两种——基于模型统计的权重和基于排列的权重，它们的排名有很大的不同。接下来要记住一些有趣的事情。

## 数据点 1

下图说明了我在 Titanic 数据的测试集中选择的第一个数据点的 LIME 解释(图来自 LIME 本身):

![](img/19552b08db2ad64ca28aae4cc29641cf.png)

用原始值(左)和一个反转值(右)解释单个数据点的分类。图片作者。

该图显示了同一数据点的两个版本。左边的是数据集的原始数据。右边的那个把*性别*属性改成了异性。这是我之前提到的高排名石灰功能的反转。如图所示，LIME 将其列为该数据点的最高等级特性。

现在，将这两个数据点变量的这些时间可视化/解释与上面的全局特征重要性(来自模型内部统计和排列得分)进行比较。由 LIME 呈现的顶级特征与由作为顶级特征的全局置换重要性给出的那些特征非常匹配。事实上，这几乎是一个精确的匹配。

除此之外，图的左边说明了我对石灰的一个主要困惑。该数据点的分类器预测为:

*   未能幸存:71%的可能性
*   幸存几率:29%

我希望石灰特征权重显示出最高的贡献，然后是未幸存的*分类。但是它显示出*幸存*的权重更高。到目前为止,“性别=男性”似乎是 LIME 给出的任何变量中最重的权重，它显示为指向幸存的*。类似地，左图中的总体石灰特征权重为

*   未存活:0.17+0.09+0.03+0.00=0.29
*   幸存值:0.31+0.15+0.07+0.03+0.02+0.01 = 0.59

有趣的是*未幸存*的权重合计出了*幸存*的准确预测值。我可能认为我看问题的方式是错误的，但是我尝试用其他数据点做进一步的解释，似乎表明情况并非如此。从上图的右边开始。

上图的右侧，性别颠倒，也显示了作为最高 contibutor 的*性别*属性。但现在，这个头衔已经升得很高了。所以或许正说明了一个女主有更高的生存变化？我不知道，但可以肯定的是分类器的预测变成了:

*   未存活:43%
*   幸存率:57%

类似地，乘客等级( *Pclass* )值已经从生存加权跃升为非生存加权。在反向情况下，石灰要素权重的总和总体上看起来没有太大的不同，但是预测发生了相当大的变化。我本来希望有简单易懂的解释，但它似乎很复杂。

## 数据点 2

测试集中第二个数据点的时间解释:

![](img/80dca2978ff27684059a293ecca4ae5b.png)

用原始值(左)和一个反转值(右)解释单个数据点的分类。图片作者。

对于这一点，左侧原始数据点的 ML 预测似乎更强烈地表明预测的存活机会很低(*未存活*为 81%)，但是石灰特征权重甚至更强烈地指向相反的方向(石灰权重几乎所有特征值都有助于*存活*)。

另一方面，该图的右侧，性别值反转，提供了石灰重量与实际预测的相当好的匹配。石灰算法和最大似然算法的权重存活率都较高，比率相对接近。但与这里的原始数据点(性别未倒置，左侧)相比，无论如何都不一致。对于原始数据点，石灰权重与预测值不匹配，而对于改变后的数据点，预测值接近。

当然，右边的图说明了我的改变有多愚蠢(只颠倒了性别)。我不会期望*女*和*男*的结合发生在真实数据中。但是，不管某些值组合是否合理，我希望解释能同样好地反映预测。毕竟，LIME 旨在用给定的特征来解释给定的预测，不管这些特征有多疯狂。

有趣的一点是，在这两种情况下，性别似乎总是对生存有很大影响。也许这是由于其他特征值的组合学，但是考虑到时间权重与预测似乎在数据点之间有所不同，我不太确定。如果这个值在两种情况下都表示高存活率，为什么它会有很高的解释力呢？假设所有(两个)值都表示相同的结果(而其他值没有变化)？

# Catboost

## 来自分类器/排列的特征权重

基于模型内部的模型特征权重:

![](img/ac13c6c3d9237c738cb791b03e66a965.png)

Catboost 报告了功能重要性。图片作者。

基于排列:

![](img/dad8c7f86bc96964a11f0fa4ccab7c63.png)

排列重要性算法报告的特征重要性。图片作者。

有趣的是，*烤干*显示负贡献。

## 数据点 1

使用 Catboost 的第一个数据点:

![](img/38aab58b6441e0d7121679957d88b881.png)

用原始值(左)和一个反转值(右)解释单个数据点的分类。图片作者。

在这种情况下，LIME 似乎将雄性列为未存活，而雌性列为存活，两者的权重都非常高。当然，这是针对单个数据点的，因此也取决于其他变量的具体值。在这种情况下，存活的*对未存活的*对*的总权重对于石灰对 ML 分级机来说并不太远。然而，性别变化对毫升产量的影响很小，而对石灰产量的影响很大。同样，似乎不太一致。*

与上面的 LGBM 情况/部分相反，在这种情况下(对于 Catboost ),顶级石灰特征实际上似乎几乎完全遵循来自模型内部统计的全局特征权重。对于 LGBM 来说，情况正好相反，他们不遵循内部权重，而是遵循排列权重。令人困惑，尽管一个是特定于数据点的，另一个是全局的。

## 数据点 2

使用 Catboost 的第二个数据点:

![](img/98310102cf85cb89fe583c1cceaedb49.png)

用原始值(左)和一个反转值(右)解释单个数据点的分类。图片作者。

在这种情况下，LIME 为幸存于的*方的变量赋予了非常高的权重，而实际的分类器几乎完全预测了非幸存。并没有让我感到非常自信。*

# XGBoost

## 来自分类器/排列的特征权重

基于模型内部统计的模型特征权重:

![](img/806b1c2f42652aee0da86958bc4c3b3e.png)

XGBoost 报告了功能重要性。图片作者。

基于排列:

![](img/887d4cb2bd4684937b1478904eecbb9b.png)

排列重要性算法报告的特征重要性。图片作者。

## 数据点 1

为 XGBoost 解释的第一个数据点:

![](img/f50abb6fe678b8276ef882c0905c6d29.png)

用原始值(左)和一个反转值(右)解释单个数据点的分类。图片作者。

在这种情况下，左边的一个似乎表明*未能幸存*的权重相当大，但实际预测在*幸存*和*未能幸存*上相当平均。在右侧，权重与预测更符合石灰要素权重，似乎与预测相匹配。

至于来自模型内部和排列的时间权重和全局权重，在这种情况下，它们似乎是混合的。一些顶级特征与模型内部的顶级全局特征权重共享，一些与排列共享。

与前面的部分相比，石灰权重与模型和排列权重似乎无处不在。在内部特征权重的情况下，这可能与不同 ML 算法的一些属性有关，以及 LIME 如何考虑单个数据点与全局整个模型。然而，我希望 LIME 在排列权重方面更加一致，因为该算法在分类器之间是相同的。

## 数据点 2

第二个数据点:

![](img/bf64f899d299577cb2d37fbcca0e14ea.png)

用原始值(左)和一个反转值(右)解释单个数据点的分类。图片作者。

在这里，左边的石灰数字似乎表明更多的生存对重量，和非生存在实际的 XGBoost 预测。在右边，权重和预测似乎又更加一致了。不是很一致。

# 解释 Keras NN 分类器

本节使用不同的数据集[克利夫兰心脏病风险](https://www.kaggle.com/ronitf/heart-disease-uci)。在这种情况下，反转变量不是性别，而是 *cp* 变量，因为在我查看的数据点上，它似乎是石灰得分最高的分类变量。这个 *cp* 变量也有 4 个值，而不是 2 个，但是无论如何，我希望改变一个高分变量来显示一些影响。

特点:

1.  *年龄*:以年为单位的年龄
2.  *性别* : (1 =男；0 =女性)
3.  *cp* :胸痛类型(4 个值)
4.  *trestbps* :入院时的静息血压，单位为毫米汞柱
5.  *胆固醇*:血清胆固醇，单位为毫克/分升
6.  *空腹血糖*:空腹血糖> 120 毫克/分升
7.  *静息心电图*:静息心电图结果(数值 0，1，2)
8.  *达到最大心率
9.  *运动诱发心绞痛:(1 =是；0 =否)
10.  *oldpeak* :运动相对于休息诱发的 ST 段压低
11.  *斜率*:运动 ST 段峰值的斜率
12.  荧光镜染色的主要血管数量(0-3)
13.  *thal* : 3 =正常；6 =修复缺陷；7 =可逆转缺陷

## 排列的特征权重

作为一个通用的神经网络框架，Keras 不提供基于模型内部统计的特征权重，这与上面的 boosters 等特定算法相反。但是基于置换的特征加权总是一个选项:

![](img/93028d2d8ff98b4c38566d6da87d2177.png)

排列重要性算法报告的特征重要性。图片作者。

## 训练曲线

训练曲线总是很好看，所以开始吧:

![](img/3700d6f07331dc7a7a95b5f88ec8c42c.png)

模型精度与训练时期的损失。图片作者。

## 数据点 1

LIME 为 Keras 解释的第一个数据点:

![](img/0c71ac9b83a33765b124f4a53ab6c2ab.png)

用原始值(左)和一个改变的值(右)解释单个数据点的分类。图片作者。

这一个几乎完全预测了*两个数据点都没有风险*。然而，石灰重量似乎完全表明了患心脏病的风险。 *cp* (胸痛)值从 0 到 1 的变化已经将变量从顶级特性列表中完全删除。然而，石灰重量仍然严重显示在*心脏风险*侧，而 ML 算法预测几乎完全在*无风险*侧。非常不一致。

与全局排列权重相比，石灰权重共享相同的前 1-2 个特征，在较低等级的特征中有一些变化。

## 数据点 2

LIME 为 Keras 解释的第二个数据点:

![](img/26093f9cb5ac66301085312d37ec4fae.png)

用原始值(左)和一个改变的值(右)解释单个数据点的分类。图片作者。

在这种情况下，两边的预测和石灰重量更为混杂。右侧似乎在无风险的一侧比左侧有更多的权重，然而 ML 算法预测在相反的方向上有更多的偏移，朝向心脏风险的一侧。

在这种情况下，这些特征与第一个数据点完全不同，也与排列重要性给出的全局权重完全不同。这可能不是一个问题，因为 LIME 旨在解释单个数据点，而不是全局模型。然而，我确实看到了一个问题，那就是不能以任何合理的方式将石灰重量映射到预测。至少不是一贯如此。有时石灰重量与预测一致，通常不一致，有时则完全相反。

# 解释 XGBoost 回归变量

本节使用的[艾姆斯住宅数据集](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview)中的特征:

*   销售价格——以美元为单位的房产销售价格。这是你试图预测的目标变量。
*   实用程序:可用的实用程序类型
*   总体质量:整体材料和表面质量
*   GrLivArea:地面以上居住面积平方英尺
*   外部质量:外部材料质量
*   功能:家庭功能评级
*   厨房质量
*   壁炉质量
*   车库汽车:车库在汽车容量中的大小
*   YearRemodAdd:改造日期
*   车库面积:车库的面积，以平方英尺为单位

## 数据点 1

![](img/fe602eb7ed751ff272e065dd707dc8f3.png)

解释单个数据点回归预测值的时间。图片作者。

正如一篇 Python 数据文章所讨论的，LIME 结果对于分类的推理似乎比回归更直观。对于回归，它应该显示特征值如何影响预测回归值的一些相对值。在这种情况下，这将是预测特定特征值如何影响房价的方式。

但是，这其中的含义有点不清楚。例如，什么东西的权重为正意味着什么？还是消极？关于什么？什么是基线，或者什么是衡量标准？对解释回归模型进行更彻底的评估将是有趣的，但至少可以应用石灰。就像分类一样，结果可能会更直观一些。

## 数据分布

出于兴趣，这里描述了上面显示的特性的数据分布。

![](img/f51408404ba6378aeeaa6ece6cc2bd08.png)

不同变量的数据分布。图片作者。

也许可以对特征值分布如何与这些变量的石灰权重相关进行一些分析，并使用这些分析作为进一步分析与预测价格相关的石灰结果的手段。也许有一天有人会..🙂

# 结论

与模型内部统计给出的所有全局特征权重以及基于排列的权重相比，我在本文中展示的结果通常共享一些顶级特征。并且使用相同的算法比较不同数据点的解释，看起来有一些变化，其中特征时间在每个数据点中排名最高。总的来说，考虑到石灰应该是什么，这一切都是有意义的。解释单个数据点，其中全局重要特征可能经常(并且平均来说应该)排名靠前，但是单个数据点可能有所不同。

一般来说，时间可视化似乎是可视化数据点的特性重要性的好方法。我喜欢这些特性在一个方向相对于另一个方向的权重。尝试接近一个点的值来得出一个解释的想法似乎也有道理。然而，我在实验中看到的许多结果似乎不太合理。呈现的石灰重量似乎经常与 ML 算法的实际预测相反。

我试着在网上寻找这方面的见解，这本书的第章对石灰的局限性进行了很好的讨论，也许它解释了一些问题。这本书的章节最后说在使用石灰时要非常小心，以及石灰参数如何影响结果和给出的解释。这似乎与我在上面看到的一致。

我发现的许多文章(有些在开头有链接)提供了一个例子，或者一些来自 LIME papers 的图表，并且简单地掩盖了对结果的解释，很少提供对它如何在更大规模上表现的见解。对我来说，找到一些符合假设的例子并使技术看起来很好似乎很简单。这在学术界是很常见的，你想展示你研究的好结果。然而，如果我真的尝试使用它，这并不一定符合我的期望。在我的实验中，我得到了我在这里展示的结果，在这里，我无法在整个数据集中自信地将 LIME 结果与分类器输出进行匹配。

更有用的可能是理解其局限性，而不是期望 LIME(或任何其他技术)在每种情况下都能完美工作。当结果看起来与展示(广告)材料不同时，我有一种感觉，也许零件展示是精心挑选的。我自己也从事过研究，石灰是研究的成果，这当然是它的工作方式。你不会因为没有展示出好的结果，或者没有展示出你的方法中的问题而被接受论文或者获得资助和任期。但是不讨论它们会使我们更难找到实用的工作技术。

总的来说，有了这些结果，除了一些实验，我不会真的使用石灰。主要是因为我看不出自己有多信任这些类型的结果，不管销售[论点](https://arxiv.org/pdf/1602.04938.pdf)如何。但总的来说，这似乎是一项有趣的工作，我喜欢其中隐含的想法。也许这将有助于产生对我更好的其他新的可解释的人工智能技术。沿着这些思路，也很高兴看到这些类型的方法被[集成](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability)为 ML 平台产品和服务的一部分。

类似的方法还有其他有趣的方法。SHAP 是一个似乎很受欢迎的，Eli5 是另一个。[有人甚至说](https://blog.dominodatalab.com/shap-lime-python-libraries-part-1-great-explainers-pros-cons/) LIME 是 SHAP 的一个子集，它应该比 LIME 采用的采样方法更完整。也许有一天值得努力做一个比较..

如果你对我错过了什么有什么想法，或者可以做得更好，很高兴听到:)。

这次到此为止。干杯。

*原载于 2020 年 6 月 10 日 http://swenotes.wordpress.com**的* [*。*](https://swenotes.wordpress.com/2020/06/10/explaining-machine-learning-classifiers-with-lime/)