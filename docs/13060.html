<html>
<head>
<title>Your Guide to Linear Regression Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归模型指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/your-guide-to-linear-regression-models-df1d847185db?source=collection_archive---------14-----------------------#2020-09-08">https://towardsdatascience.com/your-guide-to-linear-regression-models-df1d847185db?source=collection_archive---------14-----------------------#2020-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0004" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用 Python 解释和编程线性回归模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/51caa826e5e0ee4c8e1d823275daec73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-rG6g8xIcKoWnncc"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@drew_beamer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">德鲁·比默</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="08bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可解释性是机器学习中最大的挑战之一。如果一个模型的决策更容易被人理解，那么这个模型就比另一个模型更具有可解释性。有些模型非常复杂，内部结构如此复杂，以至于几乎不可能理解它们是如何得出最终结果的。这些黑盒似乎打破了原始数据和最终输出之间的关联，因为在这之间发生了几个过程。</p><p id="be0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是在机器学习算法的宇宙中，有些模型比其他模型更透明。<a class="ae ky" rel="noopener" target="_blank" href="/modelling-classification-trees-3607ad43a123">决策树</a>肯定是其中之一，线性回归模型又是另外一个。它们的简单和直接的方法使它们成为解决不同问题的理想工具。让我们看看怎么做。</p><p id="daa0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用线性回归模型来分析给定地方的工资如何依赖于经验、教育水平、角色、工作城市等特征。同样，您可以分析房地产价格是否取决于诸如面积、卧室数量或到市中心的距离等因素。</p><p id="12bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将重点关注线性回归模型，这些模型检查一个<strong class="lb iu">因变量</strong>和一个(简单线性回归)或多个(多元线性回归)<strong class="lb iu">自变量</strong>之间的线性关系。</p><h1 id="9858" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">简单线性回归</h1><p id="1697" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">当输出变量(目标)只有一个输入变量(预测值)时，使用的是最简单的线性回归形式:</p><ul class=""><li id="b53d" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">输入</strong>或<strong class="lb iu">预测变量</strong>是帮助预测输出变量值的变量。俗称<strong class="lb iu"> <em class="nb"> X </em> </strong>。</li><li id="568a" class="ms mt it lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><strong class="lb iu">输出</strong>或<strong class="lb iu">目标变量</strong>是我们想要预测的变量。就是俗称的<strong class="lb iu"> <em class="nb"> y </em> </strong>。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/7298785540b61fddb1e864b944057273.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*zGkbD-yIkANDwn4i19VYdg.png"/></div></figure><p id="c865" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> β0 的值，也叫截距</strong>，表示估计回归线与<strong class="lb iu"> <em class="nb"> y </em> </strong>轴相交的点，而<strong class="lb iu"> β1 </strong> <strong class="lb iu">的值决定了估计回归线的斜率</strong>。<strong class="lb iu">随机误差</strong>描述的是因变量和自变量之间线性关系的随机成分(模型的扰动，<strong class="lb iu"> <em class="nb"> y </em> </strong>中<strong class="lb iu"> <em class="nb"> X </em> </strong>无法解释的部分)。真正的回归模型通常是未知的(因为我们无法捕捉影响因变量的所有效应)，因此对应于观察数据点的随机误差项的值仍然未知。然而，回归模型可以通过计算观察数据集的模型参数来估计。</p><p id="db7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回归背后的想法是从样本中估计参数<strong class="lb iu"> β0 </strong>和<strong class="lb iu"> β1 </strong>。如果我们能够确定这两个参数的最佳值，那么我们将有最佳拟合的<strong class="lb iu">线</strong>，我们可以用它来预测<strong class="lb iu"><em class="nb"/></strong>的值，给定<strong class="lb iu"> <em class="nb"> X </em> </strong>的值。换句话说，我们试图拟合一条线来观察输入和输出变量之间的关系，然后进一步用它来预测看不见的输入的输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/98a1adf644b0f75d63245461f3c95f3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IbiwnMrmD3Bwe5VwY4dFVg.png"/></div></div></figure><p id="5cd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们如何估算<strong class="lb iu"> β0 </strong> <em class="nb"> </em>和<strong class="lb iu"> β1 </strong>？我们可以使用一种叫做<strong class="lb iu">普通最小二乘法(OLS) </strong>的方法。<strong class="lb iu"> </strong>其背后的目标是将黑点到红线的距离尽可能地减小到接近零，这是通过最小化实际结果和预测结果之间的平方差来实现的。</p><p id="435b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际值和预测值之间的差异称为<strong class="lb iu">残差(e) </strong> <em class="nb"> </em>，可以是负数，也可以是正数，这取决于模型是否高估或低估了结果。因此，为了计算净误差，将所有残差直接相加会导致项的消除和净效应的减小。为了避免这种情况，我们取这些误差项的平方和，称为<strong class="lb iu"> <em class="nb">残差平方和(RSS)。</em>T51】</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/bfa7cc9bbb2f3777f79788c3278c9397.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/0*1m12le87BriQkEMF.png"/></div></figure><p id="89a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">普通最小二乘法(OLS)将残差平方和最小化</strong>，其目的是拟合一条回归线，使观测值与预测值(回归线)之间的距离(以二次值测量)最小化。</p><h1 id="4af3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">多元线性回归</h1><p id="3162" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">有两个或两个以上预测值或输入变量时，线性回归的形式<strong class="lb iu"> </strong>。与之前描述的 SLR 模型相似，它包括额外的预测因子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/caae19a54bca4e7c900034b8a2040a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ko7YDmTa_TctiL2Fkm-kGQ.png"/></div></div></figure><p id="7392" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，该方程只是简单线性回归方程的扩展，其中每个输入/预测值都有其对应的斜率系数<strong class="lb iu"> (β <em class="nb"> ) </em> </strong>。第一个<strong class="lb iu"> β </strong></p><p id="824b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着要素数量的增长，我们的模型的复杂性也在增加，并且变得更加难以可视化，甚至难以理解我们的数据。因为与单反相机相比，这些模型中的参数更多，所以在使用它们时需要更加小心。添加更多的术语将从本质上提高数据的拟合度，但新术语可能没有任何实际意义。这是危险的，因为它可能会导致一个模型符合这些数据，但实际上并不意味着任何有用的东西。</p><h1 id="ef7d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">一个例子</h1><p id="1f45" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">广告数据集包括一种产品在 200 个不同市场的销售额，以及三种不同媒体的广告预算:电视、广播和报纸。我们将根据电视、广播和报纸广告预算(自变量)，使用数据集来预测销售额(因变量)。</p><p id="673d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数学上，我们将尝试求解的公式是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/f9c2c849a4dba24a5433a297dc758bf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2OhUDPvd3Y7B4ZtgRUc7Ow.png"/></div></div></figure><p id="a2bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">找到这些常数的值<strong class="lb iu"> (β) </strong>就是回归模型通过最小化误差函数和拟合最佳直线或超平面(取决于输入变量的数量)所做的事情。我们编码吧。</p><h2 id="b301" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">加载数据并描述数据集</h2><p id="a69a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">你可以在<a class="ae ky" href="https://github.com/dlopezyse/Medium" rel="noopener ugc nofollow" target="_blank">这个链接</a>下下载数据集。在加载数据之前，我们将导入必要的库:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="11f0" class="nm lw it nz b gy od oe l of og">import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn import metrics<br/>from sklearn.metrics import r2_score<br/>import statsmodels.api as sm</span></pre><p id="964d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们加载数据集:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="3966" class="nm lw it nz b gy od oe l of og">df = pd.read_csv(“Advertising.csv”)</span></pre><p id="d68e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们了解数据集并对其进行描述:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="6d56" class="nm lw it nz b gy od oe l of og">df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/3736cad3dff0483f4b9b5086c3dcf767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fPwZ_gX20AtN-knnHGTIBg.png"/></div></div></figure><p id="6730" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将删除第一列(“未命名”)，因为我们不需要它:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="5714" class="nm lw it nz b gy od oe l of og">df = df.drop([‘Unnamed: 0’], axis=1)<br/>df.info()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/229a74e05ad4450aa0507038ccb6d65c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x6TRgBQLiHaucn9cE32U2A.png"/></div></div></figure><p id="bc32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的数据集现在包含 4 列(包括目标变量“sales”)、200 个寄存器，没有缺失值。让我们想象一下自变量和目标变量之间的关系。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="7d74" class="nm lw it nz b gy od oe l of og">sns.pairplot(df)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/a310079a01bd94a63ef3aee75075441b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OrnAQH8eXXq3oHcUp50ndQ.png"/></div></div></figure><p id="fb59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">电视和销售之间的关系似乎相当紧密，虽然广播和销售之间似乎有某种趋势，但报纸和销售之间的关系似乎并不存在。我们也可以通过相关图进行数字验证:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="90c8" class="nm lw it nz b gy od oe l of og">mask = np.tril(df.corr())<br/>sns.heatmap(df.corr(), fmt=’.1g’, annot=True, cmap= ‘cool’, mask=mask)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/5dfeabcc676f187d11ae37dff7167bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PL6X-0nllnafSun0TXgmNA.png"/></div></div></figure><p id="5418" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所料，最强的正相关发生在销售和电视之间，而销售和报纸之间的关系接近于 0。</p><h2 id="919e" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">选择特征和目标变量</h2><p id="6b67" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">接下来，我们将变量分成两组:因变量(或目标变量“y”)和自变量(或特征变量“X”)</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="95c4" class="nm lw it nz b gy od oe l of og">X = df.drop([‘sales’], axis=1)<br/>y = df[‘sales’]</span></pre><h2 id="f2c2" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">分割数据集</h2><p id="3f8a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">要了解模型性能，将数据集分为定型集和测试集是一个好策略。通过将数据集分成两个独立的集合，我们可以使用一个集合进行训练，并使用另一个集合中的未知数据来测试模型性能。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="fe0c" class="nm lw it nz b gy od oe l of og">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)</span></pre><p id="6627" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将数据集分成 70%训练和 30%测试。random_state 参数用于初始化内部随机数生成器，它将决定在您的情况下将数据分为训练和测试索引。我设置 random state = 0，这样您就可以使用相同的参数比较多次运行代码的输出。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="b5ef" class="nm lw it nz b gy od oe l of og">print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/09bb0f78c47be7f5b7da9218b0c4946d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O3wLPGQ1lZE6xgW56Mg3UQ.png"/></div></div></figure><p id="29a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过打印分割集合的形状，我们看到我们创建了:</p><ul class=""><li id="cd5f" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">2 个各有 140 个寄存器的数据集(占总寄存器的 70%)，一个有 3 个自变量，一个只有目标变量，将用于<strong class="lb iu">训练</strong>和生成线性回归模型。</li><li id="c3a8" class="ms mt it lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated">2 个数据集，每个包含 60 个寄存器(占总寄存器的 30%)，一个包含 3 个自变量，一个仅包含目标变量，将用于<strong class="lb iu">测试</strong>线性回归模型的性能。</li></ul><h2 id="fb22" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">建立模型</h2><p id="aad3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">构建模型非常简单:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="afc0" class="nm lw it nz b gy od oe l of og">mlr = LinearRegression()</span></pre><h2 id="0739" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">火车模型</h2><p id="6755" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">将模型拟合到训练数据代表了建模过程的训练部分。在模型定型后，可以使用预测方法调用来进行预测:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="ff73" class="nm lw it nz b gy od oe l of og">mlr.fit(X_train, y_train)</span></pre><p id="3bc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来看看模型训练后的输出，看看<strong class="lb iu"> β0 </strong>(截距)的值:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="1cdc" class="nm lw it nz b gy od oe l of og">mlr.intercept_</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/a66995a9a6d9993fdbd6b8a6c4ee3293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*feaLlmrnj92ADvZ3Frz21w.png"/></div></div></figure><p id="2abf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以打印系数的值<strong class="lb iu"> (β) </strong>:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="1477" class="nm lw it nz b gy od oe l of og">coeff_df = pd.DataFrame(mlr.coef_, X.columns, columns =[‘Coefficient’])<br/>coeff_df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/2b10cd3f1ab4513af10df66dbaf0eaa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5NqHYDLSlQ2qnrglIFKDCA.png"/></div></div></figure><p id="fdc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这种方式，我们现在可以根据电视、广播和报纸的不同预算值来估算“销售”值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/9b859f38c158c84da7ddd58538ad28ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*raNZs5Bd9ISvgfn3QA4ngQ.png"/></div></div></figure><p id="6b23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，如果我们确定电视预算值为 50，广播预算值为 30，报纸预算值为 10，则“销售额”的估计值为:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="dc27" class="nm lw it nz b gy od oe l of og">example = [50, 30, 10]<br/>output = mlr.intercept_ + sum(example*mlr.coef_)<br/>output</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/2298d48bf4ad4321c5cfcd282f3eb40d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vF4DFBq8IGEXaoamiiX3Hw.png"/></div></div></figure><h2 id="a5c3" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">试验模型</h2><p id="605b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">测试数据集是独立于训练数据集的数据集。该测试数据集是您的模型的未知数据集，它将帮助您更好地了解其归纳能力:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="bc9c" class="nm lw it nz b gy od oe l of og">y_pred = mlr.predict(X_test)</span></pre><h2 id="3da8" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated"><strong class="ak">评估绩效</strong></h2><p id="f394" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">模型的质量与其预测与测试数据集的实际值的匹配程度有关:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="519c" class="nm lw it nz b gy od oe l of og">print(‘Mean Absolute Error:’, metrics.mean_absolute_error(y_test, y_pred))<br/>print(‘Mean Squared Error:’, metrics.mean_squared_error(y_test, y_pred))<br/>print(‘Root Mean Squared Error:’, np.sqrt(metrics.mean_squared_error(y_test, y_pred)))<br/>print(‘R Squared Score is:’, r2_score(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/cfed58c920965ff67f26b768dda3548d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tteI8_puZtTefFmpokyNIQ.png"/></div></div></figure><p id="6baa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在根据测试集验证了我们的模型之后，我们得到了 0.86 的 R，这似乎是一个相当不错的性能分数。但是，尽管较高的 R 表明模型更适合，但高度量并不总是一件好事。我们将在下面看到一些解释和改进回归模型的方法。</p><h1 id="48e7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">如何解读和改进你的模型？</strong></h1><p id="3fd0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">好了，我们创建了模型，现在做什么？让我们看看训练数据的模型统计数据，以获得一些答案:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="f8e5" class="nm lw it nz b gy od oe l of og">X2 = sm.add_constant(X_train)<br/>model_stats = sm.OLS(y_train.values.reshape(-1,1), X2).fit()<br/>model_stats.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/953b3d4c8db1da7007febefa23b01ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5v33qvGfs5LEq0OVU7WLyw.png"/></div></div></figure><p id="c9dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面我们来看看这些数字是什么意思。</p><h2 id="2329" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated"><strong class="ak">假设检验</strong></h2><p id="3008" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">运行 MLR 模型时，您应该回答的一个基本问题是，<a class="ae ky" rel="noopener" target="_blank" href="/multiple-linear-regression-8cf3bee21d8b">是否至少有一个预测因子对预测输出</a>有用。如果自变量和目标之间的关系只是偶然的，并且没有任何预测因素对销售产生实际影响，那该怎么办？</p><p id="0b8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要进行假设检验来回答这个问题，并检查我们的假设。这一切都是从形成一个<strong class="lb iu">零假设(H0) </strong>开始的，该假设声明所有系数都等于零，并且预测值和目标值之间没有关系(这意味着没有独立变量的模型既适合数据，也适合您的模型):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/8733956a7212cf2c0446dc3d11956ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/0*zKszcsewUKm5IIWf.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多元线性回归。来源:<a class="ae ky" rel="noopener" target="_blank" href="/multiple-linear-regression-8cf3bee21d8b">走向数据科学</a></p></figure><p id="88e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，我们需要定义一个<strong class="lb iu">替代假设(Ha) </strong>，该假设声明至少有一个系数不为零，并且预测值和目标值之间存在关系(意味着您的模型比仅截距模型更好地拟合数据):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/c8320beff8e4cbb36882afc8d20cff97.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/0*3S8ecOREimbhlIRC.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多元线性回归。来源:<a class="ae ky" rel="noopener" target="_blank" href="/multiple-linear-regression-8cf3bee21d8b">走向数据科学</a></p></figure><p id="ecc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们想要拒绝零假设并对我们的回归模型有信心，我们需要找到强有力的统计证据。为此，我们进行假设检验，为此我们使用了<strong class="lb iu"> F 统计量</strong>。</p><blockquote class="ou ov ow"><p id="0a8b" class="kz la nb lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated"><em class="it">如果 F-statistic 的值等于或非常接近 1，则结果有利于零假设，我们无法拒绝它。</em></p></blockquote><p id="08e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在上表中看到的(用黄色标记)，F-统计量是 439.9，从而提供了反对零假设(所有系数都是零)的有力证据。接下来，我们还需要在假设零假设为真的情况下，检查 F 统计量(也用黄色标记)出现的<strong class="lb iu">概率，即 8.76e-70，一个极小的低于 1%的数字。这意味着在有效的零假设假设下，F 统计值 439.9 偶然出现的概率远低于 1%。</strong></p><p id="5e6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">话虽如此，我们可以拒绝零假设，并确信至少有一个预测因子在预测输出时是有用的。</p><h2 id="1d53" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">生成模型</h2><p id="51c8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">运行包含许多变量(包括不相关的变量)的线性回归模型会导致不必要的复杂模型。哪些预测因素是重要的？它们对我们的模型都有意义吗？为了找出答案，我们需要执行一个叫做<strong class="lb iu">特征选择</strong>的过程。<em class="nb"> </em>特征选择的两种主要方法是:</p><ol class=""><li id="a3b3" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu pa my mz na bi translated"><strong class="lb iu">正向选择:</strong>从与因变量相关性最高的预测值开始，一次添加一个预测值。然后，更大的理论重要性的变量被顺序地结合到模型中，直到达到停止规则。</li><li id="4908" class="ms mt it lb b lc nc lf nd li ne lm nf lq ng lu pa my mz na bi translated"><strong class="lb iu">逆向淘汰:</strong>从模型中的所有变量开始，去掉统计意义最小(p 值较大)的变量，直到达到停止规则。</li></ol><p id="dcc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然这两种方法都可以使用，除非预测值的数量大于样本大小(或事件数量)，否则通常最好使用向后排除法。</p><p id="701b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在<a class="ae ky" rel="noopener" target="_blank" href="/multiple-linear-regression-8cf3bee21d8b">链接</a>中找到这些方法的完整示例和实现。</p><h2 id="4914" class="nm lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated"><strong class="ak">对比车型</strong></h2><p id="42e3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">每当你在一个模型中加入一个独立变量，R 就会增加，即使这个独立变量微不足道。在我们的模型中，所有预测因素都有助于销售增长吗？如果是的话，他们做的程度一样吗？</p><p id="2ba9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与 R 相反，<strong class="lb iu">调整后的 R </strong>是一个仅在自变量显著并影响因变量时增加的度量。因此，如果随着您向模型中添加变量，您的 R 值增加了，但调整后的 R 值减少了，那么您就知道有些特性是没有用的，您应该删除它们。</p><p id="fc88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上表中一个有趣的发现是，报纸的<strong class="lb iu"> p 值</strong>超高(0.789，红色标注)。找到每个系数的 p 值将会知道该变量对于预测目标是否具有统计显著性。</p><blockquote class="ou ov ow"><p id="2b28" class="kz la nb lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated">根据一般经验，如果给定变量的 p 值小于 0.05，则该变量和目标之间有很强的关系。</p></blockquote><p id="7272" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方式，包括变量报纸似乎不适合达到一个稳健的模型，删除它可能会提高模型的性能和泛化能力。</p><p id="0364" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了调整后的 R 分数，您还可以使用其他标准来比较不同的回归模型:</p><ul class=""><li id="4e63" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">赤池信息准则(AIC): </strong>是一种用于估计模型的似然性以预测/估计未来值的技术。它奖励达到高拟合度分数的模型，如果它们变得过于复杂，则惩罚它们。一个好的模型是所有其他模型中 AIC 最小的模型。</li><li id="662d" class="ms mt it lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><strong class="lb iu">贝叶斯信息标准(BIC): </strong>是模型选择的另一个标准，衡量模型拟合度和复杂性之间的权衡，对过于复杂的模型的惩罚甚至超过 AIC。</li></ul><h1 id="d4b4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">假设</h1><p id="8c77" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">因为线性回归模型是任何事件的长期序列的近似值，所以它们需要对它们所代表的数据做出一些假设，以便保持适当。大多数统计测试依赖于对分析中使用的变量的某些假设，当这些假设不满足时，结果可能不可信(例如，导致 I 型或 II 型错误)。</p><p id="35ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从输出是输入变量的线性组合的意义上来说，线性回归模型是线性的，并且只适合于对可线性分离的数据进行建模。线性回归模型在各种假设下工作，这些假设必须存在，以便产生正确的估计，而不仅仅依赖于准确性分数:</p><ul class=""><li id="e5ed" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">线性</strong>:特征和目标之间的关系必须是线性的。检查线性关系的一种方法是目视检查散点图的线性。如果散点图中显示的关系不是线性的，那么我们需要运行非线性回归或转换数据。</li><li id="ed36" class="ms mt it lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><strong class="lb iu">同方差</strong>:对于 x 的任何值，残差的方差必须相同。多元线性回归假设残差中的误差量在线性模型的每个点都是相似的。这种情况称为同质性。散点图是检查数据是否同质的好方法，也有几种测试可以从数字上验证假设(如 Goldfeld-Quandt、Breusch-Pagan、White)</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/5a29f2cf546007fd8267523bf88318d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Pf8JAomWSiIAICyA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归算法的假设。来源:<a class="ae ky" rel="noopener" target="_blank" href="/assumptions-of-linear-regression-algorithm-ed9ea32224e1">走向数据科学</a></p></figure><ul class=""><li id="dd3f" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">无多重共线性:</strong>数据不应显示多重共线性，当独立变量(解释变量)彼此高度相关时会出现多重共线性。如果发生这种情况，在计算出造成因变量/目标变量差异的具体变量时就会出现问题。这种假设可以用方差膨胀因子(VIF)法或通过相关矩阵来检验。解决这个问题的替代方法可能是将数据居中(扣除平均分数)，或者进行因子分析并旋转因子以确保线性回归分析中因子的独立性。</li><li id="53d8" class="ms mt it lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><strong class="lb iu">无自相关</strong>:残差的值应该彼此独立。残差中相关性的存在大大降低了模型的准确性。如果误差项相关，估计的标准误差往往会低估真实的标准误差。为了检验这个假设，你可以使用 Durbin-Watson 统计。</li><li id="de8d" class="ms mt it lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><strong class="lb iu">残差的正态性</strong>:残差必须是正态分布的。正态性可以用拟合优度检验(如科尔莫戈罗夫-斯米尔诺夫或夏皮罗-维尔克检验)来检查，如果数据不是正态分布，非线性变换(如对数变换)可能会解决这个问题。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/b517433521fc1b6afc0197560d7280c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/0*febzA62009Qxvl51.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归算法的假设。来源:<a class="ae ky" rel="noopener" target="_blank" href="/assumptions-of-linear-regression-algorithm-ed9ea32224e1">走向数据科学</a></p></figure><p id="c6ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设是至关重要的，因为如果假设无效，那么分析过程就会被认为是不可靠的、不可预测的和失控的。无法满足假设可能导致得出无效或没有数据科学支持的结论。</p><p id="2ad7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在<a class="ae ky" href="https://www.kaggle.com/shrutimechlearn/step-by-step-assumptions-linear-regression" rel="noopener ugc nofollow" target="_blank">链接</a>中找到对假设的全面测试。</p><h1 id="c455" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">最后的想法</h1><p id="1cb3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">虽然 MLR 模型扩展了 SLR 模型的范围，但它们仍然是线性模型，这意味着模型中包含的术语不能显示彼此之间的任何非线性关系，也不能代表任何种类的非线性趋势。预测要素观测范围之外的点时也应小心，因为当您移出观测范围时，变量之间的关系可能会发生变化(由于没有数据，您无法知道这一事实)。</p><blockquote class="ou ov ow"><p id="f7b7" class="kz la nb lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated">观察到的关系可能是局部线性的，但在数据范围之外可能存在未观察到的非线性关系。</p></blockquote><p id="f37c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">线性模型还可以通过包含非线性变量(如多项式)和转换指数函数来模拟曲率</strong>。线性回归方程在 <a class="ae ky" href="https://statisticsbyjim.com/glossary/parameter/" rel="noopener ugc nofollow" target="_blank"> <em class="nb">参数</em> </a>中为<em class="nb">线性，意思是你可以将自变量提升一个指数来拟合一条曲线，仍然保持在“线性世界”中。线性回归模型可以包含对数项和反项，以遵循不同类型的曲线，但仍然保持参数的线性。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/4a4bd2604b9d2094bee246006f2e6fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N5viBqbRIUKGz5TjRQ3lOA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">当自变量被平方时，模型在参数上仍然是线性的</p></figure><p id="3ff1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像<a class="ae ky" rel="noopener" target="_blank" href="/machine-learning-with-python-easy-and-robust-method-to-fit-nonlinear-data-19e8a1ddbd49"> <strong class="lb iu">多项式回归</strong> </a>这样的回归可以模拟<em class="nb">非线性关系</em>，虽然线性方程有一种基本形式，但非线性方程可以采取许多不同的形式。您可能考虑使用<a class="ae ky" rel="noopener" target="_blank" href="/how-to-choose-between-a-linear-or-nonlinear-regression-for-your-dataset-e58a568e2a15"> <strong class="lb iu">非线性回归模型</strong> </a>的原因是，虽然线性回归可以模拟曲线，但它可能无法模拟数据中存在的特定曲线。</p><p id="2228" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您还应该知道，OLS 并不是拟合您的线性回归模型的唯一方法，其他优化方法，如<a class="ae ky" rel="noopener" target="_blank" href="/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76"> <strong class="lb iu">梯度下降</strong> </a>更适合大型数据集。将 OLS 应用于复杂的非线性算法可能不可扩展，梯度下降法在计算上可以更便宜(更快)地找到解决方案。<em class="nb">梯度下降是一种最小化函数的算法</em>，给定一个由一组参数定义的函数，该算法从一组初始参数值开始，迭代地向一组最小化该函数的参数值移动。这个<strong class="lb iu">迭代最小化</strong>是使用导数实现的，在函数梯度的负方向上采取步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/1fc74bcdab526c702c3da7703e15dfa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NrINywH3bS8-po_T.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用梯度下降的线性回归。来源:<a class="ae ky" rel="noopener" target="_blank" href="/linear-regression-using-gradient-descent-97a6c8700931">走向数据科学</a></p></figure><p id="827b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个需要考虑的关键问题是<strong class="lb iu">异常值会对回归线</strong>和相关系数产生巨大影响。为了识别它们，有必要执行<a class="ae ky" rel="noopener" target="_blank" href="/the-basics-of-data-prep-7bb5f3af77ac">探索性数据分析(EDA) </a>，检查数据以检测异常观察，因为它们会以激烈的方式影响我们的分析和统计建模的结果。如果您发现任何异常值，可以对其进行估算(例如，使用平均值/中值/众数)、封顶(替换某些限制之外的值)或替换缺失值并进行预测。</p><p id="3fa3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，线性回归模型的一些<a class="ae ky" href="https://www.imf.org/external/pubs/ft/fandd/2006/03/basics.htm" rel="noopener ugc nofollow" target="_blank">限制是:</a></p><ul class=""><li id="d741" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">省略变量</strong>。需要有一个好的理论模型来建议解释因变量的变量。在简单的双变量回归的情况下，人们必须考虑其他因素来解释因变量，因为可能有其他“未观察到的”变量来解释输出。</li><li id="cdcf" class="ms mt it lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><strong class="lb iu">反向因果关系</strong>。许多理论模型预测双向因果关系——也就是说，因变量可以引起一个或多个解释变量的变化。例如，更高的收入可以让人们在自己的教育上投入更多，这反过来会提高他们的收入。这使得回归估计的方式变得复杂，需要特殊的技术。</li><li id="aecd" class="ms mt it lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><strong class="lb iu">测量错误</strong>。因子可能测量不正确。例如，天资很难衡量，智商测试也存在众所周知的问题。因此，使用智商的回归可能无法正确控制资质，导致教育和收入等变量之间的不准确或有偏差的相关性。</li><li id="d42e" class="ms mt it lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><strong class="lb iu">太局限了一个焦点</strong>。回归系数仅提供一个变量的小变化(而非大变化)与另一个变量的变化之间的关系。它将显示教育中的一个小变化是如何影响收入的，但它不会让研究者概括大变化的影响。如果每个人同时接受大学教育，一个刚毕业的大学生不太可能挣得更多，因为大学毕业生的总供给会急剧增加。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><blockquote class="ou ov ow"><p id="cef4" class="kz la nb lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated">对这些话题感兴趣？在 Linkedin<a class="ae ky" href="https://www.linkedin.com/in/lopezyse/" rel="noopener ugc nofollow" target="_blank">或 Twitter </a>上关注我</p></blockquote></div></div>    
</body>
</html>