<html>
<head>
<title>Include Training Operations in Saved Models with Tensorflow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Tensorflow 2 在保存的模型中包含训练操作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/include-training-operations-in-saved-models-with-tensorflow-2-6494d304036d?source=collection_archive---------35-----------------------#2020-08-03">https://towardsdatascience.com/include-training-operations-in-saved-models-with-tensorflow-2-6494d304036d?source=collection_archive---------35-----------------------#2020-08-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b7df" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">向以 SavedModel 格式导出的模型中的推理操作添加训练操作，以及如何用低级操作调用它们。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5c408a20b5e0506a136a932654dd444a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q8j9ta2BhMs8482EqpXgyw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@iseeworld?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">马特·王</a>在<a class="ae ky" href="https://unsplash.com/s/photos/data-science?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="7ed3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数 Tensorflow 文档和教程都展示了如何用 python 训练一个模型，并将其保存为 SavedModel 格式，以便在另一个环境中进行预测。但是为<strong class="lb iu">培训</strong>保存一个模型需要更多的工作。本文介绍了一种方法。所有的代码都在这个<a class="ae ky" href="https://colab.research.google.com/drive/1VM0uolTFFxosakT5ox1Y9TUTjVVKLKzP?usp=sharing" rel="noopener ugc nofollow" target="_blank"> jupyter 笔记本</a>里(用 tensorflow 2.3 测试)</p><h2 id="93be" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">高级 API 的一些限制</h2><p id="b596" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">通常的场景是构建一个 Keras 模型并用<code class="fe mt mu mv mw b">model.save()</code>导出它。然而，这仅保存了预测图，并且以后不可能在另一个环境中加载该模型。</p><p id="5f89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，虽然简单的<code class="fe mt mu mv mw b">model.compile()</code> + <code class="fe mt mu mv mw b">model.fit()</code>场景可以涵盖许多场景，但它不允许设计复杂的损失函数，这些函数组合了对应于几个输出的标签，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/19c4af97e3823c6dc2e2723247e8f93f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kDdw2ISPXlMbHAdrQMeAqA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不能让一个函数使用多个标签来计算损失</p></figure><p id="83ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">复杂的情况通常用定制的训练循环来解决。节选自<a class="ae ky" href="https://keras.io/guides/writing_a_training_loop_from_scratch/" rel="noopener ugc nofollow" target="_blank">优秀 Keras 教程</a>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="b37b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，损失函数<code class="fe mt mu mv mw b">loss_fn</code>可以做任何事情，并且在多输出模型的情况下可以访问所有标签。但问题是，我们仍然只剩下一个只能导出服务图的模型。所有的训练循环都不会被导出。</p><p id="aae8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决方案是在<code class="fe mt mu mv mw b">tf.Module</code>中使用<code class="fe mt mu mv mw b">tf.function</code>来导出几乎任何可以用张量流图运算表达的代码。</p><h1 id="07f1" class="na lw it bd lx nb nc nd ma ne nf ng md jz nh ka mg kc ni kd mj kf nj kg mm nk bi translated">tf.function 中的自定义训练步骤</h1><p id="4909" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在下面的代码中，我们用函数式 API 定义了一个 Keras 模型。它包括一个定制层，不是完全无关紧要，但它可以是任何复杂性。我们还在包含在自定义<code class="fe mt mu mv mw b">tf.Module</code>中的<code class="fe mt mu mv mw b">tf.function</code>中定义了一个自定义训练步骤。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">tf.function 中的自定义训练步骤。组件</p></figure><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="4af1" class="lv lw it mw b gy np nq l nr ns">sample prediction:  [[0.20642705]]</span></pre><p id="e510" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们反复调用<code class="fe mt mu mv mw b">my_train</code>函数来训练模型</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="4ccf" class="lv lw it mw b gy np nq l nr ns">Mean squared error: step 100: 1.3914144039154053<br/>Mean squared error: step 200: 1.3325191736221313<br/>Mean squared error: step 300: 1.321561574935913<br/>Mean squared error: step 400: 1.3107181787490845<br/>Mean squared error: step 500: 1.3031591176986694</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d1d93c9f162068c4e500f00b59b0ca66.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*bDm_i10_jjGP3_QojDDV5Q.png"/></div></figure><p id="c30f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查一下我们使用的<code class="fe mt mu mv mw b">ADAM</code>优化器的状态。亚当学习与每个重量相关的两个变量<code class="fe mt mu mv mw b">m</code>和<code class="fe mt mu mv mw b">v</code>。</p><p id="cc59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">m</code>和<code class="fe mt mu mv mw b">v</code>分别是梯度的一阶矩(均值)和二阶矩(无中心方差)的估计。<br/>了解更多信息:<a class="ae ky" href="https://ruder.io/optimizing-gradient-descent/index.html#adam" rel="noopener ugc nofollow" target="_blank">https://ruder . io/optimizing-gradient-descent/index . html # Adam</a></p><p id="7798" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在张量流术语中，它们被称为<code class="fe mt mu mv mw b">slots</code>。有关如何跟踪它们的更多信息，请参见<a class="ae ky" href="https://www.tensorflow.org/guide/checkpoint#loading_mechanics" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/guide/check point # loading _ mechanics</a></p><p id="822e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查 ADAM 的<code class="fe mt mu mv mw b">m</code>槽的内容，看第一个密集层的偏差。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="5b44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们再训练一会儿。正如预期的那样，损失仍然很低(注意该图的比例)</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/6dcd6e7e1fb78b2cf80ea360c77e231d.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*nHFfGnsescSDwzoBJPcnvg.png"/></div></figure><p id="40ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以检查 ADAM 优化器的变量是否也发生了变化:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="ce67" class="na lw it bd lx nb nc nd ma ne nf ng md jz nh ka mg kc ni kd mj kf nj kg mm nk bi translated">坚持模型，继续训练</h1><p id="52ca" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">以上都是在记忆中完成的。同样，当我们保存一个模型时，不仅保存了层的权重，还保存了优化器的状态，这很好。然后，当我们继续训练一个重新加载的模型时，优化器不需要重新学习它的变量(ADAM 的<code class="fe mt mu mv mw b">m</code>和<code class="fe mt mu mv mw b">v</code>时刻)。</p><p id="312d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们以 SavedModel 格式保存模块。SavedModel 格式包含描述导出函数及其输入和输出的签名，这些签名在我们加载模型时可用。更多信息见<a class="ae ky" href="https://www.tensorflow.org/guide/saved_model" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/saved_model</a></p><p id="79bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们将检查与模型一起保存的检查点的内容。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="8130" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不仅可以看到层的权重，还可以看到前面讨论过的亚当槽变量(<code class="fe mt mu mv mw b">m</code>和<code class="fe mt mu mv mw b">v</code>)。</p><p id="45ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">节选自<a class="ae ky" href="https://www.tensorflow.org/guide/saved_model#saving_a_custom_model" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/guide/saved _ model # saving _ a _ custom _ model</a></p><blockquote class="nv nw nx"><p id="e03d" class="kz la ny lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated"><em class="it">保存一个</em> <code class="fe mt mu mv mw b"><em class="it">tf.Module</em></code> <em class="it">时，任何</em> <code class="fe mt mu mv mw b"><em class="it">tf.Variable</em></code> <em class="it">属性、</em><code class="fe mt mu mv mw b"><em class="it">tf.function</em></code><em class="it">-修饰方法，以及通过递归遍历找到的</em> <code class="fe mt mu mv mw b"><em class="it">tf.Modules</em></code> <em class="it">都被保存。(参见</em> <a class="ae ky" href="https://www.tensorflow.org/guide/checkpoint" rel="noopener ugc nofollow" target="_blank"> <em class="it">检查点教程</em> </a> <em class="it">了解更多关于递归遍历的信息。)</em></p></blockquote><p id="715d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这在<a class="ae ky" href="https://www.tensorflow.org/guide/checkpoint#loading_mechanics" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/guide/check point # loading _ mechanics</a>中有详细介绍</p><p id="1690" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查一下亚当的<code class="fe mt mu mv mw b">m</code>槽的内容，看看第一个密集层的偏差</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="774a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，这正是上面用<code class="fe mt mu mv mw b">module.opt.weights[2]</code>检查的那个槽的内存内容</p><p id="69ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这表明 ADAM 的状态确实保存在检查点中。让我们用 Tensorflow 捆绑的<code class="fe mt mu mv mw b">saved_model_cli</code>工具来看看导出的签名。</p><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="fa0f" class="lv lw it mw b gy np nq l nr ns">!saved_model_cli show --all --dir $model_dir</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="8eb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到预测(<code class="fe mt mu mv mw b">my_serve</code>)和训练(<code class="fe mt mu mv mw b">my_training</code>)输出函数的预期签名。稍后将详细介绍。<br/>让我们创建一个模块的新实例，不经训练保存它并重新加载它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="070e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里值得注意的是，加载的<code class="fe mt mu mv mw b">new_module</code>对象不是一个<code class="fe mt mu mv mw b">tf.Module</code>实例，而是另一种仍然提供我们导出的<code class="fe mt mu mv mw b">my_train</code>和<code class="fe mt mu mv mw b">__call__</code>函数的对象。让我们调用<code class="fe mt mu mv mw b">__call__()</code>方法来看看它是否工作(将产生任何东西，因为模型还没有被训练)</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="ad8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们仍然可以将<code class="fe mt mu mv mw b">my_train</code>函数与<code class="fe mt mu mv mw b">train_module</code>函数一起使用。让我们训练重新加载的模块，然后保存它</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/bc484cdb2ecceea053842f0337535dda.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*hdQ-B7TpvlQHAxrd7-9qjg.png"/></div></figure><p id="10b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面显示了我们可以加载一个模块并训练它，就像我们用<code class="fe mt mu mv mw b">CustomModule()</code>实例化它一样。让我们像上面一样检查一些优化器状态。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="a1d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重新加载模块，继续培训并保存</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/81c07c49c67eaaeb8af0c71e0f74a6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*XbzBlyJJqbgy2O_hkG-OBg.png"/></div></figure><p id="74db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像我们在内存中所做的一样——只是前面显示了重量已经被正确地重新加载，并且我们没有从头开始训练。亚当的变量呢？</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="b9ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">亚当的变量也发生了变化。随着训练的继续，它们的变化越来越小，这可以通过取训练前后那些槽的差的范数来显示，并且看到该范数随着时间而减小。</p><p id="b6f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这表明优化器状态也在保存的模型中被捕获，并且我们可以停止和恢复训练而不会丢失任何东西。</p><h1 id="954c" class="na lw it bd lx nb nc nd ma ne nf ng md jz nh ka mg kc ni kd mj kf nj kg mm nk bi translated">低级操作</h1><p id="b64e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">以上所有工作都是在重载模块时使用 python 对象和方法完成的。但是在只有图形和操作的情况下，我们如何用另一种语言来做呢？</p><p id="416a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们先来看看用 python 怎么做。这是<code class="fe mt mu mv mw b">my_train</code>签名的<code class="fe mt mu mv mw b">saved_model_cli</code>输出:</p><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="1c0c" class="lv lw it mw b gy np nq l nr ns">signature_def['my_train']:<br/>  The given SavedModel SignatureDef contains the following input(s):<br/>    inputs['X'] tensor_info:<br/>        dtype: DT_FLOAT<br/>        shape: (-1, 8)<br/>        name: my_train_X:0<br/>    inputs['y'] tensor_info:<br/>        dtype: DT_FLOAT<br/>        shape: (-1)<br/>        name: my_train_y:0<br/>  The given SavedModel SignatureDef contains the following output(s):<br/>    outputs['output_0'] tensor_info:<br/>        dtype: DT_FLOAT<br/>        shape: ()<br/>        name: StatefulPartitionedCall_1:0<br/>  Method name is: tensorflow/serving/predict</span></pre><p id="692a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实证明，我们可以通过这里显示的名字来访问输入和输出张量。例如:</p><ul class=""><li id="6eca" class="oe of it lb b lc ld lf lg li og lm oh lq oi lu oj ok ol om bi translated"><code class="fe mt mu mv mw b">inputs['X']</code>有名字<code class="fe mt mu mv mw b">my_train_x:0</code></li><li id="fa1d" class="oe of it lb b lc on lf oo li op lm oq lq or lu oj ok ol om bi translated"><code class="fe mt mu mv mw b">output['output_0']</code>(损失)有名称<code class="fe mt mu mv mw b">StatefulPartitionedCall_1:0</code></li></ul><p id="8301" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">隐藏的是用于保存模型的操作和张量</p><ul class=""><li id="b1a6" class="oe of it lb b lc ld lf lg li og lm oh lq oi lu oj ok ol om bi translated">检查点名称:<code class="fe mt mu mv mw b">saver_filename:0</code>:必须指向<code class="fe mt mu mv mw b">model_dir + '/variables/variables'</code></li><li id="03da" class="oe of it lb b lc on lf oo li op lm oq lq or lu oj ok ol om bi translated">保存操作:<code class="fe mt mu mv mw b">StatefulPartitionedCall_2:0</code>:本模块导出后的下一个<code class="fe mt mu mv mw b">StatefulPartitionedCall</code></li></ul><p id="e537" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于保存操作的信息当然没有被记录，这可能是有意的，因此这可能在未来的 tensorflow 版本中不起作用。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用低级操作训练和预测加载模型</p></figure><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="db98" class="lv lw it mw b gy np nq l nr ns">loss: 1.2404047<br/>prediction: [[1.9881454]]<br/>checkpoint saved</span></pre><p id="9668" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你不止一次执行<code class="fe mt mu mv mw b">train_predict_serve()</code>，你会得到不同的结果，因为模型的权重会随着训练和预测的变化而变化。</p><p id="ce35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上表明，我们可以仅通过低级操作来训练、保存模块并进行预测。<br/>这允许<strong class="lb iu">导出空白模型</strong>及其训练和服务图表，并让第三方组织对其进行训练和预测。导出的操作足以让该组织训练和监控损失减少，报告验证数据集的准确性并做出推断。</p><p id="4a7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ny">边注</em>:如果模块函数返回两个输出，<code class="fe mt mu mv mw b">saved_model_cli</code>会这样报告它们:</p><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="62be" class="lv lw it mw b gy np nq l nr ns">The given SavedModel SignatureDef contains the following output(s):<br/>    outputs['output_0'] tensor_info:<br/>        dtype: DT_FLOAT<br/>        shape: ()<br/>        name: StatefulPartitionedCall_1:0<br/>    outputs['output_1'] tensor_info:<br/>        dtype: DT_FLOAT<br/>        shape: ()<br/>        name: StatefulPartitionedCall_1:1</span></pre><p id="90cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们可以这样理解:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="863c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相同的导出模型可用于 java 中的训练和预测，代码如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="b339" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">打印这个:</p><pre class="kj kk kl km gt nl mw nm nn aw no bi"><span id="9118" class="lv lw it mw b gy np nq l nr ns">loss after training: 1.2554951<br/>prediction: 2.1101139</span></pre><p id="4240" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个 java 代码是用 tensorflow 2.3.0 测试的。更多信息:<br/><a class="ae ky" href="https://github.com/tensorflow/java" rel="noopener ugc nofollow" target="_blank">https://github.com/tensorflow/java</a><br/>T5】https://www . tensor flow . org/API _ docs/Java/reference/org/tensor flow/package-summary</p><p id="49a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了完整性，请注意，也可以通过使用<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#experimental_export_all_saved_models" rel="noopener ugc nofollow" target="_blank">experimental _ export _ all _ saved _ models</a>方法，将预测图和训练图导出到带有<code class="fe mt mu mv mw b">tf.Estimator</code>的单个 SavedModel 中。我们也可以用低级图形操作来调用它。这是一种不同的方法，需要函数来构建模型，并以明确定义的格式返回数据集。欲了解更多信息，请参见<a class="ae ky" href="https://www.tensorflow.org/guide/estimator" rel="noopener ugc nofollow" target="_blank">估算师指南</a>。</p><h1 id="4099" class="na lw it bd lx nb nc nd ma ne nf ng md jz nh ka mg kc ni kd mj kf nj kg mm nk bi translated">结论</h1><p id="e1a7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在本文中，我们展示了一种将训练步骤导出为 SavedModel 格式的方法，以及如何在 python 中的重载模型上调用它，以及在 python 或另一种语言(如 java)中使用低级操作调用它。</p><p id="9ba9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于 SavedModel 操作的其他有用链接(感谢 Drew Hodun！)<br/><a class="ae ky" rel="noopener" target="_blank" href="/how-to-extend-a-keras-model-5effc083265c">https://towards data science . com/how-to-extend-a-keras-model-5 effc 083265 c</a></p></div></div>    
</body>
</html>