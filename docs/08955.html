<html>
<head>
<title>Attention for Machine Translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意机器翻译</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-for-machine-translation-4a95db38ee34?source=collection_archive---------48-----------------------#2020-06-27">https://towardsdatascience.com/attention-for-machine-translation-4a95db38ee34?source=collection_archive---------48-----------------------#2020-06-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="008e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">神经机器翻译模型的注意机制的简要综述。</h2></div><p id="7e4d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的大脑并不具备一次处理大量信息的能力。然而，我们<em class="le">所擅长的是</em>专注于我们被给予的信息的一部分来理解它。当你被要求把一个句子从一种语言翻译成另一种语言时，你处理句子的方式是一边走一边挑选单个的单词，把它们串成短语，然后在心里为每一部分指定目标语言中相应的单词/短语。</p><p id="d57f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当一个书面单词呈现给你进行翻译时，它会在你的大脑中启动一系列的神经认知过程。你大脑中正常的语言系统会读出你母语中的单词，但对于翻译来说，这种通常的倾向必须被抑制，这样翻译出来的意思才能显现出来。大脑的一个特定区域(称为“尾状核”)协调这一活动，就像管弦乐队的指挥一样，产生令人震惊的复杂行为。本质上，单词或单词序列的句法、语音、词汇和语义方面被包含、吸收，然后被语境化，以将所述单词序列翻译成其在目标语言中的对等物。</p><p id="ed67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">翻译——无论是人工翻译还是机器翻译——在本质上都不是客观的，也就是说，对于任何给定的句子，都没有<em class="le">唯一的</em>翻译。人工翻译固有地带来将源语言单词、语法或句法引入目标语言表达的风险。两个人翻译一个相对较长的句子很少会完全相同。尽管最终的结果有所不同，但不变的是翻译的大致过程。对于翻译中出现的任何目标语言单词或短语，译者会更多地关注源句子的某些部分。</p><p id="cac4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直到 2015 年，Bahdanau、Cho 和 Bengio 引入了“注意力”机制，人类的这种与生俱来的品质才被赋予机器算法。提出这种机制是为了通过允许模型自动(软)搜索源句子中与预测目标单词相关的部分来最大化翻译性能，而不必将这些部分显式地形成为硬段。我承认这句话可能很难理解，但是我们将通过分解它并一次注意一部分来理解它。(很 meta，我知道。)这篇 2015 年的论文影响深远，它将继续成为几个最先进模型的构建模块。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/f24abd81d2eccd5c1969aa7c86a7dab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bPr_P2ZZcnt1Nhq3.png"/></div></div></figure><h2 id="0512" class="lr ls it bd lt lu lv dn lw lx ly dp lz kr ma mb mc kv md me mf kz mg mh mi mj bi translated">为什么我们需要关注？</h2><p id="4eac" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">在传统的神经机器翻译模型中，编码器-解码器组合与用于每种语言的编码器和解码器一起使用，或者将特定于语言的编码器应用于每个句子，然后比较其输出。编码器 RNN 读取源句子并将其编码成固定长度的向量。然后，解码器会根据编码器提供的矢量进行翻译。由语言对的编码器和解码器组成的整个编码器-解码器系统被联合训练以最大化给定源句子的正确翻译的概率。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mp"><img src="../Images/20dc988298cf2445d9a30bd53f1ad7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FWjiRK7J0J7VNiWZ.png"/></div></div></figure><h2 id="1e5c" class="lr ls it bd lt lu lv dn lw lx ly dp lz kr ma mb mc kv md me mf kz mg mh mi mj bi translated">瓶颈</h2><p id="f3c5" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">这种体系结构的一个问题是，它过度依赖于一个固定长度的向量来包含所有必要的信息，并且是源句子的高质量表示。这种在固定向量向量上压缩和捕获所有信息的压力是一个瓶颈，它使得编码器神经网络难以在长句上表现良好。前面已经表明，随着输入句子长度的增加，基本编码器-解码器的性能会迅速下降。</p><h2 id="a71a" class="lr ls it bd lt lu lv dn lw lx ly dp lz kr ma mb mc kv md me mf kz mg mh mi mj bi translated">核心理念</h2><p id="5042" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">为了解决这个问题，Bahdanau 等人引入了一个对编码器-解码器模型的扩展，该模型学习联合对齐和翻译。新的架构部署了一个双向 RNN 作为编码器和解码器，它将能够关注所有的隐藏状态，而不仅仅是最终的隐藏状态。这种修改所做的是为解码器提供决策的灵活性，并因此识别源句子中可能与目标句子中下一个单词的预测更相关的部分。这是注意力机制的直觉，它现在把我们引向实现这一点的数学。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/ec47e3afffcb45da942c3d05de89376c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/0*lJH9lr5jiMo6Ry9b.png"/></div></figure><p id="7002" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，这种新方法便于信息在注释序列中传播，解码器可以相应地选择性地检索该信息。</p><p id="896e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过语言这一媒介，我们能够在很长的空间和时间范围内交流思想，但是句子中词与词之间的句法联系的建立，无论它们是否彼此靠近，都是任何语言表达思想的基础。这就是注意力介入并帮助从源语言到目标语言的语法映射的地方。识别单词与同一个句子中可能很远的其他单词的关系——同时忽略对我们试图预测的单词没有太大影响的其他单词——这就是注意力的目的。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="7c8f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我很想解释一下注意力机制  <em class="le">的</em> <a class="ae my" href="https://karanpraharaj.github.io/post/attention/" rel="noopener ugc nofollow" target="_blank"> <em class="le">代数，但是我发现 Medium 的编辑对数学写作毫无帮助。你可以在本页</em> </a> <em class="le">底部的</em> <a class="ae my" href="https://karanpraharaj.github.io/post/attention/" rel="noopener ugc nofollow" target="_blank"> <em class="le">找到解释。</em></a></p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="3a8d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">您可以访问我的页面</em> </strong> <a class="ae my" href="https://karanpraharaj.github.io" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="le">这里</em> </strong> </a> <strong class="kk iu"> <em class="le">。你可以关注我的推特供稿</em> </strong> <a class="ae my" href="https://twitter.com/IntrepidIndian" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="le">这里</em> </strong> </a> <strong class="kk iu"> <em class="le">。</em> </strong></p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="0297" class="mz ls it bd lt na nb nc lw nd ne nf lz jz ng ka mc kc nh kd mf kf ni kg mi nj bi translated">参考</h1><p id="c3d7" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">[1] Bahdanau，d .，Cho，K. &amp; Bengio，Y. <a class="ae my" href="http://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">通过联合学习对齐和翻译的神经机器翻译。</a>在<em class="le">程序中。学习代表国际会议</em> (2015)</p><p id="a32f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]赵京贤、亚伦·库维尔和约舒阿·本吉奥。<a class="ae my" href="http://arxiv.org/abs/1507.01053" rel="noopener ugc nofollow" target="_blank">使用基于注意力的编码器-解码器网络描述多媒体内容。</a> (2015)</p><p id="2f99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]苏茨基弗，I .维尼亚尔斯，o .和勒。用神经网络进行序列对序列学习。在<em class="le">程序中。神经信息处理系统进展。</em> (2014)</p><p id="aef7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4]克里斯·奥拉赫的博文。<a class="ae my" href="https://distill.pub/2016/augmented-rnns/" rel="noopener ugc nofollow" target="_blank">“注意力和增强循环神经网络</a>”</p></div></div>    
</body>
</html>