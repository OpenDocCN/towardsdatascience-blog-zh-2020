<html>
<head>
<title>Feature selection in machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-in-machine-learning-d5af31f7276?source=collection_archive---------11-----------------------#2020-04-05">https://towardsdatascience.com/feature-selection-in-machine-learning-d5af31f7276?source=collection_archive---------11-----------------------#2020-04-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="30eb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Python 进行要素选择的方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1ba1930dc599341d056065e9e3d0be79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4PBvddHH6HCu4L4i"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者:<a class="ae ky" href="https://unsplash.com/@brune" rel="noopener ugc nofollow" target="_blank">凯布鲁恩</a>，来源:<a class="ae ky" href="https://unsplash.com/photos/Z-gTyL0dOH4" rel="noopener ugc nofollow" target="_blank">突发</a></p></figure><p id="a4ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">简介</strong></p><p id="9eb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度增强的决策树，如<em class="lv"> XGBoost </em>和<em class="lv">light GBM</em>【1–2】，成为表格数据和时间序列的分类和回归任务的流行选择。通常，首先提取代表数据的特征，然后将它们用作树的输入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/01df7225c9a411a3043262939a934bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hloMSdQBR2Krfx8L"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树集合示意图，作者:<a class="ae ky" href="https://medium.com/@mohtedibf" rel="noopener">莫赫塔迪·本·弗拉杰</a>，来源:<a class="ae ky" href="https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae" rel="noopener">媒体</a></p></figure><p id="77b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征是被观察现象的一个单独的可测量的属性或特征[3]——数据集中的一个属性。特征可以包括各种统计(平均值、标准差、中值、百分位数、最小值、最大值等)、趋势(上升和下降)、峰值分析(周期、平均峰值宽度、峰值数量、频率)、自相关和互相关等。有几个开源库可以帮助提取所有的基本特性，比如<em class="lv"> NumPy </em>、<em class="lv"> SciPy、sklearn、ts fresh</em>【4–7】等等。通常，根据领域知识和对问题的物理理解，为任务设计定制的特性也是有用的。</p><p id="6719" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦从数据中提取出特征，它们就被用作梯度推进决策树(GBDT) [8]的输入。然而，GBDT 容易过度拟合，对于相对较小的数据集，减少特征的数量，只保留那些有助于分类器的特征是很重要的。</p><p id="63e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树管道的一个重要部分是特征选择过程。特征选择有助于减少过度拟合，移除冗余特征，并避免混淆分类器。在这里，我描述了几种常用的方法来为任务选择最相关的特性。</p><p id="e103" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">自动递归特征消除</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/3138b7bbcb1fe8f79053968b36df4f93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QDo4DQbP7nWM_OiA.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者:<a class="ae ky" href="https://pixabay.com/users/qimono-1962238/" rel="noopener ugc nofollow" target="_blank">奇莫诺</a>，来源:<a class="ae ky" href="https://pixabay.com/illustrations/cubes-choice-one-yellow-light-2492010/" rel="noopener ugc nofollow" target="_blank"> pixabay </a></p></figure><p id="a9fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">移除额外特征的可能性之一是从<em class="lv"> sklearn </em>库【9】中移除递归特征的自动工具。带交叉验证的递归特征消除[10]比不带交叉验证的选项更常用。</p><p id="7b41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该工具的目标是通过递归考虑越来越小的要素集来选择要素。</p><ul class=""><li id="033f" class="ly lz it lb b lc ld lf lg li ma lm mb lq mc lu md me mf mg bi translated">首先，在初始特征集上训练估计器，并获得每个特征的重要性。</li><li id="3232" class="ly lz it lb b lc mh lf mi li mj lm mk lq ml lu md me mf mg bi translated">然后，从当前特征集中移除最不重要的特征，并再次检查分类度量。</li><li id="a85e" class="ly lz it lb b lc mh lf mi li mj lm mk lq ml lu md me mf mg bi translated">递归地重复该过程，直到最终达到要选择的特征的期望数量。</li></ul><p id="5367" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该工具提供了有用功能集的第一近似值。然而，自动特征消除并不总是最佳的，并且经常需要进一步的微调。在使用上述递归排除法选择了初始特征集之后，我使用排列重要性来选择特征。</p><p id="5b48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">排列重要性</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/00b6b475eebd5964367062d64d974da6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pmTqePhrO3hqAyRW.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://pixabay.com/illustrations/calculator-hand-robot-count-695084/" rel="noopener ugc nofollow" target="_blank"> pixabay </a></p></figure><p id="c204" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法被称为“排列重要性”或“平均降低精度”，并在 Breiman [11]中进行了描述。当特征值被置换(变成噪声)时，置换重要性被计算为分数的减少。</p><p id="0c4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测量特征重要性的方法之一是完全去除它，在没有该特征的情况下训练分类器，并观察这样做如何影响分数。然而，这种方法需要为每个特征重新训练分类器，这在计算上是昂贵的。解决方法是只从验证集中删除有问题的特征，并计算没有该特征的验证集的分数。由于经训练的分类器仍然期望该特征可用，因此可以用来自相同分布的随机噪声代替该特征作为初始特征值，而不是移除该特征。获得这种分布的最简单的方法是简单地混洗(或置换)原始特征值。这就是排列重要性是如何实现的。对于分类器来说，特征仍然存在，但是它不包含任何有用的信息。</p><p id="43b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">排列重要性</strong>可以使用 eli5 包来计算【12】。它提供了一个特性的排名，然后我删除了那些负面的或者不重要的特性。eli5 包提供了一种计算任何黑盒评估者的特性重要性的方法，通过测量当一个特性不可用时分数是如何降低的。</p><p id="edcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">示例:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="4c64" class="mr ms it mn b gy mt mu l mv mw">import eli5 <br/>from eli5.sklearn import PermutationImportancefrom lightgbm import LGBMClassifier</span><span id="887d" class="mr ms it mn b gy mx mu l mv mw"># set data and targets, split, and metric</span><span id="d135" class="mr ms it mn b gy mx mu l mv mw"># set a classifier<br/>clf = LGBMClassifier(**best_params)</span><span id="0da2" class="mr ms it mn b gy mx mu l mv mw"># fit the classifier model<br/>clf.fit(x_train, y_train, <br/>        eval_set=[(x_train, y_train), (x_valid, y_valid)],<br/>        eval_metric=lgbm_multi_weighted_logloss, <br/>        verbose=100,<br/>        early_stopping_rounds=400, <br/>        sample_weight=y_train.map(weights), )</span><span id="de7d" class="mr ms it mn b gy mx mu l mv mw"># calculate permitation importance for the classifier<br/>perm = PermutationImportance(<br/>           clf, scoring=permutation_scorer, <br/>           random_state=1).fit(x_valid, y_valid)        <br/>expl = eli5.explain_weights(<br/>           perm, feature_names=x_valid.columns.tolist(), top=None)        print(eli5.format_as_text(expl))        print(expl.feature_importances)</span><span id="7eed" class="mr ms it mn b gy mx mu l mv mw"># save importances to html <br/>text_file = open("importance.html", "w")        text_file.write(eli5.format_as_html(expl))        <br/>text_file.close()</span></pre><p id="3c53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该方法最适合于在特征数量不大时计算特征重要性；否则，它可能是资源密集型的。因此，我将它作为上述初始自动特征消除之后的第二步。</p><p id="7631" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我删除了置换重要性低或为负的特性，同时检查模型性能的改进。</p><p id="12e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些要素可能具有很高的排列重要性，但定义了非常相似的数据方面。在这种情况下，找到特征之间的相关性有助于识别冗余特征。</p><p id="89c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">去除多余特征</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c96b6f40cc614d7d25d074399731355a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/0*Y887jmSORutpQ7L2"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:knowyourmeme.com<a class="ae ky" href="https://knowyourmeme.com/memes/spider-man-pointing-at-spider-man" rel="noopener ugc nofollow" target="_blank"/>，知识共享署名-共享许可协议</p></figure><p id="7347" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">相关性</strong>是一种统计技术，可以显示变量对是否相关以及相关程度如何。</p><p id="d1f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当相关性等于 1 时，特征完全相关<br/>当相关性为零时，特征完全独立</p><p id="8688" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相关性高的特征更具线性相关性，因此对因变量的影响几乎相同。因此，当两个特性高度相关时，我们可以去掉其中一个。</p><p id="ddb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相关矩阵可以简单地计算为<em class="lv"> corr = X.corr()，</em>其中向量 X 包含具有所考虑特征的所有列。</p><p id="7255" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有特征之间的相关矩阵在对角线元素上具有 1，因为该特征与其自身 100%相关。具有高相关值的非对角线元素指示冗余特征。</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="b8ab" class="mr ms it mn b gy mt mu l mv mw">sns.heatmap(corr)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/334eb4aee625c7219279994d59bedfbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eSgBEV0fj9FeVSLp-wJhMg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者:a .胡宾，许可证:麻省理工学院</p></figure><p id="e432" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在检查验证度量时逐个手动移除冗余要素(高度相关的要素)有助于减少要素集并提高 GBDT 的性能。为此，只需比较不同特征(非对角线元素)之间的相关性，并尝试移除相关性高于某个阈值(例如 0.9)的两个特征之一</p><p id="7f7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">主成分分析</strong></p><p id="d9fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种减少特征数量的方法是使用<strong class="lb iu">主成分分析</strong>【13】。这种技术允许减少特征空间的维数，同时从特征的组合中找到最突出的成分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/1e951b3b563bc55515ad0656d1fd4bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*RDMSYYSfvi4o0eNt.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py" rel="noopener ugc nofollow" target="_blank">scikit-learn.org</a>，BSD 许可证</p></figure><p id="c369" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主成分分析的主要思想是降低由许多变量组成的数据集的维数，这些变量或多或少相互关联，同时最大程度地保留数据集中存在的差异。</p><p id="bdd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用 sklearn 库可以很容易地实现它[14]。PCA 对缩放敏感，并且在应用该算法之前需要对特征进行归一化。一个例子:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="d191" class="mr ms it mn b gy mt mu l mv mw">from sklearn.preprocessing import StandardScaler<br/>from sklearn.decomposition import PCA</span><span id="3c2c" class="mr ms it mn b gy mx mu l mv mw">features = [‘feat_a’, ‘feat_b’, ‘feat_c’, ‘feat_d’]</span><span id="7df4" class="mr ms it mn b gy mx mu l mv mw"># Scale the features<br/>x = StandardScaler().fit_transform(features)</span><span id="15dc" class="mr ms it mn b gy mx mu l mv mw"># decide on the number of components<br/>pca = PCA(n_components=2)<br/>principalComponents = pca.fit_transform(x)</span></pre><p id="0e87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多课程和博客文章，在那里你可以读到关于这种技术的细节，例如在[15]中。</p><p id="e160" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">遗言</strong></p><p id="e04a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，还有其他的特征选择方法，比如使用自动编码器、P 值、LightGBM 重要性等等。在这里，我描述了我个人选择的子集，这是我在 Kaggle 上的竞争机器学习期间开发的。</p><p id="b5a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我逐一执行步骤 1–2–3 来选择功能。这是在 Kaggle competition PLAsTiCC 天文分类[16]中应用特征选择技术的例子。首先，使用自动<strong class="lb iu">递归特征消除</strong>和交叉验证[10]来选择特征，给出 167 个特征。然后，使用在 eli5 中实现的<strong class="lb iu">排列重要性</strong>算法对剩余的特征进行排序，并选择顶部的特征。正如我们所看到的，50 个特征是不够的，100 个特征在这个数据集上表现得更好。最后，使用相关性从前 100 个特征中移除了<strong class="lb iu">冗余特征</strong>，给我们留下了所选的 82 个特征【17】。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/0afc8f66d82d864b517a5973b9572172.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KR58n7Pl3r_CqdDiJ3q5Ew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同特性集的模型结果，参见[17]中的完整文章</p></figure><p id="fa28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与自动特征选择相比，这种多步骤方法提高了模型性能，但并不显著。定义和设计最相关的特征仍然是使用 GBDT 分类器/回归器获得最佳模型的首要任务。</p><p id="f8ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> Tatiana Gabruseva 博士</em></p><p id="79da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献:</strong></p><p id="1f96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] LightGBM，<a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">https://lightgbm.readthedocs.io/en/latest/</a></p><p id="221f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] XGBoost，<a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/tutorials/index.html" rel="noopener ugc nofollow" target="_blank">https://XGBoost . readthedocs . io/en/latest/tutorials/index . html</a></p><p id="0774" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]克里斯托弗·毕晓普(2006 年)。模式识别和机器学习。柏林:施普林格。<a class="ae ky" href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" rel="noopener ugc nofollow" target="_blank">ISBN</a><a class="ae ky" href="https://en.wikipedia.org/wiki/Special:BookSources/0-387-31073-8" rel="noopener ugc nofollow" target="_blank">0–387–31073–8</a>。</p><p id="0f4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] NumPy，<a class="ae ky" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank">https://numpy.org/</a></p><p id="8379" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] SciPy，<a class="ae ky" href="https://www.scipy.org/" rel="noopener ugc nofollow" target="_blank">https://www.scipy.org/</a></p><p id="9c72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] sklearn，<a class="ae ky" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/classes . html # module-sk learn . feature _ extraction</a></p><p id="9983" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7] tsfresh，<a class="ae ky" href="https://tsfresh.readthedocs.io/en/latest/text/introduction.html" rel="noopener ugc nofollow" target="_blank">https://ts fresh . readthedocs . io/en/latest/text/introduction . html</a></p><p id="c815" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[8]<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Gradient_boosting</a></p><p id="193d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[9 ] RFE，<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ selection。RFE.html</a></p><p id="c903" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[10 ] RFECV，<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ selection。RFECV.html</a></p><p id="e408" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[11] Breiman，“随机森林”，机器学习，45(1)，5–32，2001 年(可在 https://www . stat . Berkeley . edu/% 7 ebreiman/randomforest2001 . pdf 上在线获得)。</p><p id="14c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[12]米哈伊尔·科罗博夫、康斯坦丁·洛普欣、埃利 5、<a class="ae ky" href="https://eli5.readthedocs.io/en/latest/overview.html" rel="noopener ugc nofollow" target="_blank">https://eli5.readthedocs.io/en/latest/overview.html</a></p><p id="318a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[13] PCA，<a class="ae ky" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Principal_component_analysis</a></p><p id="9aea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">【14】<a class="ae ky" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition" rel="noopener ugc nofollow" target="_blank">sk learn . decomposition</a>。PCA，<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . decomposition . PCA . html</a></p><p id="49ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[15] PCA:在机器学习中的应用，<a class="ae ky" href="https://medium.com/apprentice-journal/pca-application-in-machine-learning-4827c07a61db" rel="noopener">https://medium . com/apprentice-journal/PCA-Application-in-Machine-Learning-4827 c 07 a61db</a></p><p id="7dc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[16]塑料天文分类，<a class="ae ky" href="https://www.kaggle.com/c/PLAsTiCC-2018" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/PLAsTiCC-2018</a></p><p id="0907" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[17] T. Gabruseva，S. Zlobin 和 P. Wang，用机器学习进行光度学光变曲线分类，JAI，2020 (ArXiv: <a class="ae ky" href="https://arxiv.org/pdf/1909.05032.pdf" rel="noopener ugc nofollow" target="_blank">和)</a></p></div></div>    
</body>
</html>