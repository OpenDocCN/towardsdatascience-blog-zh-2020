# 解决数据不平衡的要点方法！

> 原文：<https://towardsdatascience.com/bullet-point-approach-to-data-imbalance-34c701c8e686?source=collection_archive---------60----------------------->

## 当绝大多数数据属于一个类(多数类)而少数类的样本很少时，就会出现数据不平衡问题或类不平衡问题。

![](img/49ba55bc4e8d40fc4b2712ce87b9560b.png)

由 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的 [Loic Leray](https://unsplash.com/@loicleray?utm_source=medium&utm_medium=referral) 拍摄的照片

**如果:**，就不会有数据不平衡的问题

*   你并不真正感兴趣的是，正确地划分少数阶级(很少发生！).
*   你有一个巨大的训练数据集，即使你的类样本比例非常不平衡，你仍然有很多样本在你的少数类。

## **精确不是你的最佳选择**

*   假设你有 1000 个样本(其中 990 个属于 A 类，10 个属于 b 类，你的智能算法决定把所有东西都归为 A 类！你的准确率，在这种情况下，是 99%！
    好看！除非你试图开发一个人工智能驱动的疾病识别工具，否则你将走向另一个 [Theranos](https://en.wikipedia.org/wiki/Theranos) 故事。
*   根据假阳性和假阴性的代价(和后果),你可以决定你应该更关注哪个指标。
*   例如，如果假阳性意味着您将损失 10 万美元，而假阴性意味着您将损失 10 美元，那么您知道您的指标应该更侧重于最小化假阳性！
    [了解更多！](https://en.wikipedia.org/wiki/False_positives_and_false_negatives)

# **如何处理问题？**

有 3 种方法:

*   **关注数据:**使用各种采样、增强技术等。重点是要有一个更平衡的数据集。
*   **关注算法:**这些方法大多关注于修改损失函数。
*   **混合方法:**抽样和算法变化的结合。

> 提示:根据假阳性和假阴性的代价(和后果),你可以决定你应该更关注哪个指标。

让我们从不同的方法开始

## **过采样和欠采样**

也许处理数据不平衡最常见的方法之一是对少数类进行过采样或对多数类进行欠采样。请确保仅对定型集进行过采样。

*   非常有用的关于 Python 中过采样的博客文章
*   [R 中的过采样和欠采样](https://www.rdocumentation.org/packages/mlr/versions/2.12.1/topics/oversample)

## **击杀**

过采样的一个有趣的概括是通过创建新的样本来合成少数类中的新数据点，而不仅仅是用替换进行过采样。该方法使用最近邻和随机数来生成新的合成(伪)数据点:

1.  随机选择一个数据点，称之为“主”。
2.  找出五(k)个最近的邻居。
3.  随机选择这 5 个相邻点中的一个。
4.  在你的“主要”点和随机选择的邻居之间画一条线。
5.  在你画的线上创建一个随机点，并对其进行分类。

[主要论文](https://arxiv.org/pdf/1106.1813.pdf)(有 5k+次引用)。如果你是视觉学习者，请看这里！
**Rose，**是另一种使用平滑自举方法的半类似方法。

## **成本敏感分类**

常见的分类算法都试图最小化分类误差(如果你把 A 误分类为 B，就会通过增加代价函数来惩罚算法)。但是对于一个代价敏感的分类器，并不是所有的误分类都是一样的！当算法错误分类少数类(少数类获得更高的权重)时，它们会对算法进行更多的惩罚。

![](img/f260e14f06b19f2d1ac31caa85488287.png)

[法比奥](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

# **卷积神经网络(CNN)怎么样？**

*   上述方法工作良好。但是一些有趣的细微差别出现了，对于“经典的”机器学习技术来说不一定是真的。这是一篇关于的快速阅读实验论文。

## **焦损失**

这种相对较新的方法是我的一个朋友让我注意到的。该方法主要针对一阶段和两阶段目标识别，通过引入聚焦因子对交叉熵损失函数进行了改进。焦点因子给予更“难”的例子更高的权重。读艾的论文。

# **结论**

还有很多，但实际上这些是处理不平衡数据的非常有用的技术。我还想指出，有时将问题重新框定为异常检测问题会更容易！在异常检测中，问题变成了如何定义(和标记)正常行为。在下一篇文章中，我将在几个问题上使用这些技术，看看它们在 python 中的表现。