<html>
<head>
<title>Overwriting reality</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">覆盖现实</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overwriting-reality-7a0631728997?source=collection_archive---------35-----------------------#2020-03-11">https://towardsdatascience.com/overwriting-reality-7a0631728997?source=collection_archive---------35-----------------------#2020-03-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="28b5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为引人注目的AR内容整合已有经验</h2></div><p id="adaf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">增强现实(AR)体验的一个关键部分是数字内容的质量和丰富程度。在<a class="ae le" rel="noopener" target="_blank" href="/the-future-of-mapping-is-learned-e13e93c03e22">之前的一篇</a>文章中，我们探索了构建以对象为中心的通用3d模型，使用了传统的基于运动的结构(<strong class="kk iu"> SfM </strong>)技术以及用于深度推断和姿态估计的已有架构。这里的重点是采用更好的方法来使用深度学习推断场景深度，以解决难以绘制地图的区域。</p><p id="406d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们考虑了从真实世界捕获构建高质量模型的一些其他挑战。例如，考虑从以下视频构建表示的任务:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/00d0015d1d881ed3f77b5ca99cf7db75.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/1*_xJay3dWuxaqO_6pDPUCew.gif"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">目标；建立一个精确的，视觉上吸引人的日产探路者模型</p></figure><p id="1cd2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里的目标显然是从一个短视频中创建一个吸引人的3d车辆模型。传统的流水线相对简单:</p><ol class=""><li id="a11d" class="lr ls it kk b kl km ko kp kr lt kv lu kz lv ld lw lx ly lz bi translated">估计摄像机的轨迹和它的内在参数</li><li id="e01d" class="lr ls it kk b kl ma ko mb kr mc kv md kz me ld lw lx ly lz bi translated">在多视图立体管道中使用该估计轨迹来提供密集的点云</li><li id="63db" class="lr ls it kk b kl ma ko mb kr mc kv md kz me ld lw lx ly lz bi translated">从估计的立体点构建平滑的网格，并使用视频数据对其进行纹理处理</li></ol><p id="f338" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，闪亮的金属表面往往对多视图立体方法反应不佳(尽管我们可以利用更复杂的深度恢复方法，如前<a class="ae le" rel="noopener" target="_blank" href="/the-future-of-mapping-is-learned-e13e93c03e22">所述</a>)。我们还遇到了另一个更难解决的问题:重建只能和场景覆盖一样好。下图说明了这一点:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mf"><img src="../Images/9063f1eadf3eeb80e79ce24dfa1d2325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*nF4aekYrUEP8F3CogR_Y6A.gif"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">从上面的视频轨迹恢复的场景结构。注意车辆后面未被观察到的大片区域。</p></figure><p id="3313" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的重建中可以清楚地看到，相机没有完全探索场景，因此场景是不完整的——几个大区域丢失了。这是我们寻求补救的典型问题；即使没有一个完全探索过的场景，我们也应该能够对目标的几何形状和外观做出有根据的猜测。</p><p id="2ffc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章的核心论点是，我们对正在讨论的对象的内在属性有一个很好的想法，一辆2000年初的日产探路者。如果我们有一个现有的3d模型，我们可以直接用我们的先验模型替换推断的模型，只要定位、比例和纹理与视频紧密匹配。</p><p id="1156" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的第一个任务是将场景分成前景/背景；使用现代<a class="ae le" href="https://arxiv.org/abs/1801.00868" rel="noopener ugc nofollow" target="_blank">全景分割</a>方法，这是微不足道的:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mf"><img src="../Images/8f47c3c6447212682e3e4f704a5ea570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*328HQb6OAMGMmEPzQkYfvw.gif"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">场景的每帧实例遮罩。</p></figure><p id="c6af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定上面的密集(每像素)每帧分割，我们可以轻松地将场景几何和纹理划分为前景和背景组件，如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mf"><img src="../Images/19e6c28ed04efb612e4763d694a0679d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*INSSLtfA95duZF1-sTforQ.gif"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">稀疏的相机点云，与重建的网格形成对比。(为清晰起见，省略了部分网格)</p></figure><p id="bb4d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的模型被渲染为背景网格和前景点云。虽然立体点不够密集，无法完全恢复汽车本身，但它们对于提供关于汽车的<strong class="kk iu">汇总统计数据</strong>很有用:位置、方向、比例。</p><p id="bfee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定背景分割和要插入的新内容的类别、位置和比例，我们如何着手生成实际的3d输入？一种方法是利用部分重建的网格并执行形状完成(例如，[ <a class="ae le" href="https://arxiv.org/pdf/1805.07290v2.pdf" rel="noopener ugc nofollow" target="_blank"> 1 </a>，[ <a class="ae le" href="https://arxiv.org/pdf/1808.00671v3.pdf" rel="noopener ugc nofollow" target="_blank"> 2 </a>，[ <a class="ae le" href="https://arxiv.org/pdf/1612.00101v2.pdf" rel="noopener ugc nofollow" target="_blank"> 3 </a>)。或者，我们可以从图像中合成完整的3d形状，例如[ <a class="ae le" href="https://arxiv.org/pdf/1901.05103.pdf" rel="noopener ugc nofollow" target="_blank"> 1 </a>、[ <a class="ae le" href="https://arxiv.org/pdf/1711.10669v1.pdf" rel="noopener ugc nofollow" target="_blank"> 2 </a>、[ <a class="ae le" href="https://arxiv.org/pdf/1812.03828v2.pdf" rel="noopener ugc nofollow" target="_blank"> 3 </a>、[ <a class="ae le" href="https://arxiv.org/pdf/1906.06543v3.pdf" rel="noopener ugc nofollow" target="_blank"> 4 </a>以获得清晰的概览]。在本文中，我们将探讨后者。</p><p id="f727" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，形状合成是从2d图像推断完全3d结构。作为一个例子，<a class="ae le" href="https://arxiv.org/abs/1812.02822" rel="noopener ugc nofollow" target="_blank"> IM-NET </a>是一种用于生成形状建模的学习隐式场方法，其对于单视图3d重建特别有用。下面的动画展示了形状生成模块IM-GAN的示例:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mf"><img src="../Images/3db2c05e3b409c40e8b5a4c855ab200c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*vKXcBHnH-OqqAKZHgakpMw.gif"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">IM-GAN的样车模型。注意多样性和质量。</p></figure><p id="eb6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过将估计的比例、平移和旋转应用于来自网络的采样3d输出，我们可以将合成的内容直接插入到场景中，如上所示。</p><p id="020d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用来自单视图重建(SVR)模块对来自我们数据集的分割图像的预测输出，我们可以产生精确平滑的合成模型(合成模型与估计位置和比例的融合),这是我们对象的近似。然后使用原始视频中的图像进行纹理处理:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mk"><img src="../Images/42f1e9a3abd65cc3cd5a72e1c031b080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Pgo8OfVva6E4ECMOLWldg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">IM-SVR的预测输出，使用原始视频图像进行纹理处理。未观察到/观察不到的组件以灰色呈现。</p></figure><p id="2de5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管上面有一些可见的伪像(车辆的车轮与估计的模型不完全匹配，并且由于可见性问题，一些区域没有纹理)，但结果是令人信服的——我们不需要为我们(先验地)非常了解的对象建立繁重的数据收集例程。对于以可扩展的方式为已知对象生成准确、相关的模型，这是一种很有前途的方法。</p></div></div>    
</body>
</html>