# å…ƒæ ‡ç­¾å’Œå †å 

> åŸæ–‡ï¼š<https://towardsdatascience.com/meta-labeling-and-stacking-f17a7f9804ec?source=collection_archive---------4----------------------->

## [ğŸ“ˆPython for finance ç³»åˆ—](/feature-engineering-feature-selection-8c1d57af18d2)

## å¦‚ä½•æé«˜ä½ çš„æœºå™¨å­¦ä¹ åˆ†æ•°

![](img/67dda5225a9c543276407d8aacc3fb34.png)

ç”±[æˆ´å¤«Â·ç”˜è¿ª](http://skuawk.com/)æ ¹æ®[å…¬å…±é¢†åŸŸä¸“ç”¨è®¸å¯](https://creativecommons.org/licenses/publicdomain/)æ‹æ‘„çš„ç…§ç‰‡

***æ¥è‡ªã€Šèµ°å‘æ•°æ®ç§‘å­¦ã€‹ç¼–è¾‘çš„æç¤º:*** *è™½ç„¶æˆ‘ä»¬å…è®¸ç‹¬ç«‹ä½œè€…æ ¹æ®æˆ‘ä»¬çš„* [*è§„åˆ™å’ŒæŒ‡å—*](/questions-96667b06af5) *å‘è¡¨æ–‡ç« ï¼Œä½†æˆ‘ä»¬å¹¶ä¸è®¤å¯æ¯ä¸ªä½œè€…çš„è´¡çŒ®ã€‚ä½ ä¸åº”è¯¥åœ¨æ²¡æœ‰å¯»æ±‚ä¸“ä¸šå»ºè®®çš„æƒ…å†µä¸‹ä¾èµ–ä¸€ä¸ªä½œè€…çš„ä½œå“ã€‚è¯¦è§æˆ‘ä»¬çš„* [*è¯»è€…æœ¯è¯­*](/readers-terms-b5d780a700a4) *ã€‚*

è­¦å‘Š:è¿™é‡Œæ²¡æœ‰ç¥å¥‡çš„å…¬å¼æˆ–åœ£æ¯ï¼Œå°½ç®¡ä¸€ä¸ªæ–°çš„ä¸–ç•Œå¯èƒ½ä¼šä¸ºä½ æ‰“å¼€å¤§é—¨ã€‚

**æ³¨ 1:** *å¦‚ä½•å®‰è£…*[*mlfinlab*](https://github.com/hudson-and-thames/mlfinlab)*åŒ…æ²¡æœ‰é”™è¯¯ä¿¡æ¯å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°*[](https://medium.com/@kegui/how-to-install-mlfinlab-without-error-messages-896e2fb43c2f)**ã€‚**

***æ³¨ 2:** *å¦‚æœä½ æ­£åœ¨è¯»é©¬ç§‘æ–¯Â·æ™®æ‹‰å¤šçš„* [*ã€é‡‘èæœºå™¨å­¦ä¹ è¿›å±•ã€‘*](https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089) *ã€‚* ***7ã€‚***[](https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55)***æ˜¯ç¬¬äº”ç« å…³äº* [*çš„ç»†åˆ†ç‰¹å¾*](https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55) *ã€‚* ***8ã€‚*** [***æ•°æ®æ ‡æ³¨***](/the-triple-barrier-method-251268419dcd) *æ˜¯ç¬¬ä¸‰ç« å…³äºä¸‰é‡å…³å¡çš„æ–¹æ³•ã€‚å’Œ* ***9ã€‚*** [***å…ƒæ ‡æ³¨***](/meta-labeling-and-stacking-f17a7f9804ec)**æ˜¯ç¬¬ 50 é¡µç¬¬ 3.6 ç« ã€‚*****

****æ³¨ 3** : *ç”±äºç®—æ³•æˆ–è¯„ä¼°ç¨‹åºçš„éšæœºæ€§æˆ–æ•°å€¼ç²¾åº¦çš„å·®å¼‚ï¼Œæ‚¨çš„ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚ä½†æ˜¯æˆ‘ç¡®å®å‘ç°å¾ˆå¤šäººå–å¾—äº†æ›´é«˜çš„åˆ†æ•°ï¼Œå› ä¸ºä»–ä»¬ä»¥* ***é”™è¯¯çš„*** *æ–¹å¼æ ‡å‡†åŒ–äº†ä»–ä»¬çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚åœ¨è¿™ç¯‡æ–‡ç« çš„æœ€åï¼Œæˆ‘å°†æ­ç¤ºé«˜åˆ†çš„å¤§ç§˜å¯†ã€‚***

## **ğŸ“ˆPython For Finance ç³»åˆ—**

1.  **[è¯†åˆ«å¼‚å¸¸å€¼](https://medium.com/python-in-plain-english/identifying-outliers-part-one-c0a31d9faefa)**
2.  **[è¯†åˆ«å¼‚å¸¸å€¼â€”ç¬¬äºŒéƒ¨åˆ†](https://medium.com/better-programming/identifying-outliers-part-two-4c00b2523362)**
3.  **[è¯†åˆ«å¼‚å¸¸å€¼â€”ç¬¬ä¸‰éƒ¨åˆ†](https://medium.com/swlh/identifying-outliers-part-three-257b09f5940b)**
4.  **[ç¨‹å¼åŒ–çš„äº‹å®](/data-whispering-eebb77a422da)**
5.  **[ç‰¹å¾å·¥ç¨‹&ç‰¹å¾é€‰æ‹©](https://medium.com/@kegui/feature-engineering-feature-selection-8c1d57af18d2)**
6.  **[æ•°æ®è½¬æ¢](/data-transformation-e7b3b4268151)**
7.  **[å¾®å°å·®å¼‚ç‰¹å¾](https://medium.com/swlh/fractionally-differentiated-features-9c1947ed2b55)**
8.  **[æ•°æ®æ ‡ç­¾](/the-triple-barrier-method-251268419dcd)**
9.  **[å…ƒæ ‡ç­¾å’Œå †å ](/meta-labeling-and-stacking-f17a7f9804ec)**

# **ä»‹ç»**

**å…ƒæ ‡ç­¾å·²ç»åœ¨æˆ‘çš„å†™ä½œæ¸…å•ä¸Šåäº†å¾ˆé•¿æ—¶é—´äº†ã€‚å®ƒæ˜¯ä¸€ä¸ªæœ‰ç”¨è€Œå¼ºå¤§çš„æœºå™¨å­¦ä¹ å·¥å…·ï¼Œå¯ä»¥æ”¶é›†åœ¨ä»»ä½•æ•°æ®ç§‘å­¦å®¶çš„å·¥å…·ç®±ä¸­ï¼Œä¸ç®¡ä½ ä½¿ç”¨çš„æ˜¯ä»€ä¹ˆæ¨¡å‹ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘å‡ ä¹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…³äºè¿™ä¸ªè¯é¢˜çš„åƒæ ·çš„æ•™ç¨‹ã€‚è€Œå †å æ˜¯ä¸€ç§æµè¡Œçš„ ***é›†æˆæ–¹æ³•*** ç”¨äºåŒ¹é…å­¦ä¹ ã€‚å †å åŒ…æ‹¬è®­ç»ƒä¸€ä¸ªå­¦ä¹ ç®—æ³•æ¥ç»„åˆå…¶ä»–å‡ ä¸ªå­¦ä¹ ç®—æ³•çš„é¢„æµ‹ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼Œé›†æˆå­¦ä¹ çš„åŸºæœ¬æ€æƒ³æ˜¯ä¿ƒè¿›æ¯”å•ç‹¬ä»ä»»ä½•ç»„æˆå­¦ä¹ ç®—æ³•è·å¾—çš„æ›´å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚è¿™æ˜¯ä¸€ç§â€œç¾¤ä½“æ™ºæ…§â€æ–¹æ³•ï¼Œä»å‡ ä¸ªæ¨¡å‹ä¸­æå–ä¿¡æ¯ï¼Œå½¢æˆä¸€ç»„é«˜åº¦å‡†ç¡®çš„ç»“æœã€‚æ ¹æ®è¿™ä¸ªå®šä¹‰ï¼Œå…ƒæ ‡è®°ä¹Ÿåº”è¯¥å±äºé›†æˆæ–¹æ³•ã€‚**

**è™½ç„¶å †å å’Œå…ƒæ ‡è®°æœ‰ä¸€äº›ç›¸ä¼¼ä¹‹å¤„ï¼Œä½†å®ƒä»¬æ˜¯æ ¹æœ¬ä¸åŒçš„ã€‚å †å åŸºæœ¬ä¸ŠåŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ã€‚é¦–å…ˆï¼Œä½¿ç”¨å¯ç”¨æ•°æ®è®­ç»ƒæ‰€æœ‰å…¶ä»–ç®—æ³•ï¼Œç„¶åä½¿ç”¨å…¶ä»–ç®—æ³•çš„æ‰€æœ‰é¢„æµ‹ä½œä¸ºé™„åŠ è¾“å…¥ï¼Œè®­ç»ƒç»„åˆå™¨ç®—æ³•ä»¥è¿›è¡Œæœ€ç»ˆé¢„æµ‹ã€‚å †å çš„è¿‡ç¨‹æ˜¯:**

1.  ***å»ºç«‹ç¬¬ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œå¾—åˆ°é¢„æµ‹***
2.  ***ç”¨é˜ˆå€¼è¿‡æ»¤é¢„æµ‹***
3.  ***å°†é¢„æµ‹ä¸è¾“å…¥åˆå¹¶ä¸ºæ–°çš„è¾“å…¥***
4.  ***å»ºç«‹ç¬¬äºŒä¸ªæ¨¡å‹ï¼Œå¹¶ç”¨æ–°çš„è¾“å…¥å¯¹å…¶è¿›è¡Œè®­ç»ƒ***
5.  ***ç”¨ç¬¬äºŒä¸ªæ¨¡å‹é¢„æµ‹***

**å¾ˆç®€å•ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯ç»™æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®å¢åŠ é¢å¤–çš„ç‰¹å¾ã€‚**

**è€Œå…ƒæ ‡è®°åˆ©ç”¨äº†ä¸¤å±‚æ¨¡å‹ï¼Œä½†ç›®çš„ä¸åŒã€‚æ ¹æ® Marcos Lopez de Prado åœ¨ä»–çš„ä¹¦ã€Šé‡‘èæœºå™¨å­¦ä¹ è¿›å±•ã€‹ç¬¬ä¸‰ç« ç¬¬ 50 é¡µ(é™¤äº†è¿™æœ¬ä¹¦ï¼Œç½‘ä¸Šæ²¡æœ‰å¤ªå¤šæœ‰ç”¨çš„ä¿¡æ¯)ã€‚**

> **å½“ä½ æƒ³è·å¾—æ›´é«˜çš„ F1 åˆ†æ•°æ—¶ï¼Œå…ƒæ ‡ç­¾å°¤å…¶æœ‰ç”¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å»ºç«‹ä¸€ä¸ªå®ç°é«˜å¬å›ç‡çš„æ¨¡å‹ï¼Œå³ä½¿ç²¾åº¦ä¸æ˜¯ç‰¹åˆ«é«˜ã€‚ç¬¬äºŒï¼Œæˆ‘ä»¬é€šè¿‡å°†å…ƒæ ‡è®°åº”ç”¨äºç”±ä¸»è¦æ¨¡å‹é¢„æµ‹çš„é˜³æ€§æ¥æ ¡æ­£ä½ç²¾åº¦ã€‚**

**ä¸­å¿ƒæ€æƒ³æ˜¯åˆ›å»ºä¸€ä¸ªå­¦ä¹ å¦‚ä½•ä½¿ç”¨ä¸»æ¨¡å‹çš„è¾…åŠ© ML æ¨¡å‹ã€‚è¿™å¯¼è‡´äº†æ”¹è¿›çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬:å‡†ç¡®åº¦ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°*ç­‰ã€‚*ã€‚**

**åœ¨é©¬ç§‘æ–¯çš„ä¹¦ä¸­ï¼Œå…ƒæ ‡ç­¾ä¹‹æ‰€ä»¥å¦‚æ­¤æœ‰æ•ˆçš„åŸå› æ˜¯:**

> **äºŒè¿›åˆ¶åˆ†ç±»é—®é¢˜æå‡ºäº† I å‹é”™è¯¯(å‡é˜³æ€§)å’Œ II å‹é”™è¯¯(å‡é˜´æ€§)ä¹‹é—´çš„æŠ˜è¡·ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¢åŠ äºŒå…ƒåˆ†ç±»å™¨çš„çœŸé˜³æ€§ç‡å°†å€¾å‘äºå¢åŠ å…¶å‡é˜³æ€§ç‡ã€‚äºŒå…ƒåˆ†ç±»å™¨çš„æ¥æ”¶å™¨æ“ä½œç‰¹å¾(ROC)æ›²çº¿æ ¹æ®æ¥å—æ›´é«˜çš„å‡é˜³æ€§ç‡æ¥æµ‹é‡å¢åŠ çœŸé˜³æ€§ç‡çš„æˆæœ¬ã€‚**

**ä¸€èˆ¬æ¥è¯´ï¼Œå…ƒæ ‡è®°çš„è¿‡ç¨‹æ˜¯è¿™æ ·çš„:**

1.  ***å»ºç«‹ç¬¬ä¸€ä¸ªåŸºæœ¬æ¨¡å‹ï¼Œè·å¾—é¢„æµ‹***
2.  ***ç”¨é˜ˆå€¼*è¿‡æ»¤é¢„æµ‹**
3.  ***å°†é¢„æµ‹ä¸ x_train åˆå¹¶ä½œä¸ºæ–°çš„è¾“å…¥***
4.  ***å°†é¢„æµ‹ä¸ y_train åˆå¹¶ä¸ºä¸€ä¸ªæ–°æ ‡ç­¾***
5.  ***å»ºç«‹ç¬¬äºŒä¸ªæ¨¡å‹ï¼Œå¹¶ç”¨æ–°çš„è¾“å…¥å’Œæ ‡ç­¾å¯¹å…¶è¿›è¡Œè®­ç»ƒ***
6.  ***ç”¨ç¬¬äºŒä¸ªæ¨¡å‹é¢„æµ‹***
7.  ***åŸºæœ¬æ¨¡å‹é¢„æµ‹å’Œå…ƒæ¨¡å‹é¢„æµ‹äº¤é›†çš„æœ€ç»ˆé¢„æµ‹ç»“æœã€‚***

**å…ƒæ ‡ç­¾æ˜¯æˆ‘èŠ±äº†å¤§é‡æ—¶é—´è¯•å›¾æ‰¾å‡ºåº”ç”¨è¿™ç§æ–¹æ³•çš„æœ€ä½³æ–¹å¼çš„ä¸»é¢˜ã€‚ä½†æ˜¯ä»ç„¶æœ‰å¤ªå¤šçš„æœªçŸ¥ï¼Œä¾‹å¦‚:**

1.  ****å…ƒæ ‡ç­¾æŒ‡æ ‡æ˜¯å¦ä¾èµ–äºï¼Ÿ****

***å½“æˆ‘ä»¬è¯„ä¼°ä¸€ç§æ–¹æ³•æ—¶ï¼Œè¿™å®é™…ä¸Šå–å†³äºæˆ‘ä»¬é€‰æ‹©å“ªäº›æŒ‡æ ‡ï¼Œä»¥åŠæ‚¨å¯¹æ¨¡å‹æ¶æ„ã€è¶…å‚æ•°ç­‰çš„ä¼˜åŒ–ç¨‹åº¦..***

**2.**èƒ½å¦é€‚ç”¨äºä¸åŒçš„è½¦å‹ï¼Ÿ****

**ç”±äºè¿™ç§æ–¹æ³•å°†ä½¿ç”¨ä¸¤ç§ä¸åŒçš„æ¨¡å‹ï¼Œæ¨¡å‹å·®å¼‚æ˜¯æˆ‘ä»¬éœ€è¦è€ƒè™‘çš„å¦ä¸€ä¸ªé—®é¢˜ã€‚**

**3.å…ƒæ ‡ç­¾åªé€‚ç”¨äºæŸäº›æ•°æ®å—ï¼Ÿ**

***å…³äºè¿™ä¸ªè¯é¢˜çš„å‚è€ƒæ–‡çŒ®å¤§å¤šæ¥è‡ªæ—¶åºæ•°æ®ï¼Œé‚£ä¹ˆéåºåˆ—æ•°æ®å‘¢ï¼Ÿ***

**è¯¸å¦‚æ­¤ç±»ï¼Œä½†æ˜¯è¿™äº›é—®é¢˜éƒ½æ˜¯å…³äºå…ƒæ ‡ç­¾çš„å±€é™æ€§ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘åœ¨æœ¬æ–‡ä¸­æƒ³è¦æ¢è®¨çš„ã€‚ä½†æ˜¯ç”±äºè¿™ç¯‡æ–‡ç« çš„é•¿åº¦é™åˆ¶ï¼Œæˆ‘å¯èƒ½æ— æ³•æ¶µç›–æ‰€æœ‰çš„å†…å®¹ã€‚**

**å…ƒæ ‡è®°ä¸é›†æˆæ–¹æ³•çš„åŒºåˆ«ï¼Œå°¤å…¶æ˜¯å †å ï¼Œåœ¨äºå…ƒæ ‡è®°å°†æ¥è‡ªä¸»è¦æ¨¡å‹çš„é¢„æµ‹æ·»åŠ åˆ°ç‰¹å¾å’Œæ ‡è®°ä¸­ï¼Œè€Œå †å ä»…å°†å…¶ç”¨ä½œæ–°ç‰¹å¾ã€‚æˆ‘å¯ä»¥ç†è§£é‚£äº›é¢å¤–çš„ç‰¹å¾(é¢„æµ‹)æ˜¯ç”±ç”¨æ¥åšé¢„æµ‹çš„æ¨¡å‹æ¥è¡¨ç¤ºçš„ã€‚ä½†æ˜¯ä¸ºä»€ä¹ˆè¦åœ¨æ ‡ç­¾ä¸­ä½¿ç”¨å‘¢ï¼Ÿå°½ç®¡è¿™å¯èƒ½å°±æ˜¯å…ƒæ ‡ç­¾è¿™ä¸ªåå­—çš„ç”±æ¥ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œåœ¨æ ‡ç­¾ä¸­åŠ å…¥é¢„æµ‹æ˜¯å¦æœ‰ä¿¡æ¯æ³„éœ²ï¼Ÿå—¯ï¼Œæˆ‘æƒ³è‚¯å®šä¼šæœ‰ä»åˆçº§æ¨¡å‹åˆ°æ¬¡çº§æ¨¡å‹çš„æ³„æ¼ï¼Œè¿™æ˜¯ä¸ºäº†å¾—åˆ°æ›´å¥½çš„åˆ†æ•°ã€‚**

**ç”±äºå¯ä»¥åœ¨ç½‘ä¸Šæ‰¾åˆ°è®¸å¤šå…³äºé›†æˆå­¦ä¹ çš„è®¨è®ºï¼Œç‰¹åˆ«æ˜¯å †å ï¼Œå…ƒæ ‡è®°å¾ˆå°‘å¾—åˆ°è¶³å¤Ÿçš„ç ”ç©¶ã€‚å› æ­¤ï¼Œæœ¬æ–‡å°†é›†ä¸­è®¨è®ºå…ƒæ ‡è®°åŠå…¶ä¸å †å çš„æ¯”è¾ƒã€‚**

# **å›¾ä¹¦é¦†**

**ä¸‹é¢æ˜¯æœ¬æ–‡ä¸­ä½¿ç”¨çš„åº“ã€‚**

```
**'''Main'''
import numpy as np
import pandas as pd'''Data Viz'''
import matplotlib.pyplot as plt
import seaborn as sns#plt.style.use('seaborn')
plt.rcParams['figure.figsize'] = [16, 9]
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.size'] = 20
plt.rcParams['axes.labelsize'] = 16
plt.rcParams['axes.titlesize'] = 18
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12
plt.rcParams['font.family'] = 'serif'
%matplotlib inline'''Data Prep'''
from sklearn import preprocessing as pp
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold'''Metrics'''
from sklearn.metrics import log_loss, accuracy_score, f1_score
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report'''Algos'''
from sklearn.linear_model import LogisticRegression
import lightgbm as lgbimport tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping**
```

# **æ•°æ®**

**ä¸ºäº†ç ”ç©¶å…ƒæ ‡è®°å’Œå †å ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡`sklearn.datasets.make_classification().`ç”Ÿæˆä¸€ä¸ªè™šæ‹Ÿçš„äºŒè¿›åˆ¶åˆ†ç±»æ•°æ®é›†ï¼Œä½†æ˜¯ä½¿ç”¨çœŸå®çš„æ•°æ®é›†æ›´æœ‰è¶£ã€‚æˆ‘ä»¬è¿™é‡Œä½¿ç”¨çš„æ•°æ®é›†æ˜¯ ULB æœºå™¨å­¦ä¹ å°ç»„çš„ ***ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹*** ã€‚æˆ‘ä»¬å…ˆå¿«é€Ÿçœ‹ä¸€ä¸‹æ•°æ®ã€‚**

```
**#pd.read_csv can read the url and unzip the zipped file at
#same time, but will take a few seconds, just be patient.url = "[https://clouda-datasets.s3.amazonaws.com/creditcard.csv.zip](https://clouda-datasets.s3.amazonaws.com/creditcard.csv.zip)"
data_original = pd.read_csv(url) 
data_original.head()**
```

**![](img/32f665eed1ac30fa14ba3b6dad9e8462.png)**

# **âœTipï¼**

**`pd.read_csv()` *å¯ä»¥åŒæ—¶è¯»å–ç½‘å€å’Œè§£å‹å‹ç¼©æ–‡ä»¶ã€‚æˆ‘ä»¬ä¸å†éœ€è¦ä»»ä½•å…¶ä»–å‡½æ•°æˆ–åº“æ¥å®Œæˆè¿™é¡¹å·¥ä½œã€‚***

**æ•°æ®é›†åŒ…å« 28 ä¸ªåŒ¿åè¦ç´ ã€1 ä¸ªâ€œæ•°é‡â€è¦ç´ ã€1 ä¸ªâ€œæ—¶é—´â€è¦ç´ å’Œ 1 ä¸ªç›®æ ‡å˜é‡ç±»ã€‚è¯¥æ•°æ®é›†æ˜¾ç¤ºäº†ä¸¤å¤©å†…å‘ç”Ÿçš„äº¤æ˜“ï¼Œå…¶ä¸­ 284ï¼Œ807 ç¬”äº¤æ˜“ä¸­æœ‰ 492 ç¬”æ¬ºè¯ˆã€‚è¿™äº›ç‰¹å¾è¢«åŒ¿ååŒ–ä»¥ä¿æŠ¤å®¢æˆ·çš„éšç§ï¼Œè¿™æ˜¯ PCA å˜æ¢çš„ç»“æœï¼Œå› ä¸ºæ•°æ®é›†åœ¨å…¬å…±åŸŸä¸­ã€‚å”¯ä¸€æ²¡æœ‰è¢« PCA è½¬æ¢çš„ç‰¹å¾æ˜¯â€œæ—¶é—´â€å’Œâ€œæ•°é‡â€ã€‚ç‰¹å¾â€œæ—¶é—´â€åŒ…å«æ•°æ®é›†ä¸­æ¯ä¸ªäº‹åŠ¡å’Œç¬¬ä¸€ä¸ªäº‹åŠ¡ä¹‹é—´ç»è¿‡çš„ç§’æ•°ã€‚ç‰¹å¾â€œé‡‘é¢â€æ˜¯äº¤æ˜“é‡‘é¢ï¼Œè¯¥ç‰¹å¾å¯ç”¨äºä¾èµ–äºç¤ºä¾‹çš„æˆæœ¬æ„ŸçŸ¥å­¦ä¹ ã€‚ç‰¹å¾â€œç±»â€æ˜¯å“åº”å˜é‡ï¼Œâ€œ0â€ä½œä¸ºç›®æ ‡å˜é‡å¯¹åº”äºéæ¬ºè¯ˆæƒ…å†µï¼Œè€Œç›®æ ‡å˜é‡ä¸­çš„â€œ1â€å¯¹åº”æ¬ºè¯ˆæƒ…å†µã€‚**

```
**data_original.info()**
```

**![](img/804348913fb4918859b2c385186c813d.png)**

**å˜é‡ä¹‹é—´ä¹Ÿæœ‰æœ€å°çš„ç›¸å…³æ€§â€”â€”è¿™å¯èƒ½æ˜¯ PCA å˜æ¢å˜é‡çš„ç»“æœã€‚**

```
**#see the cluster and corralation of features and classes
def plot_corr(data = data_original):

    ax1 = data.corrwith(data.Class).plot.bar(figsize = (20, 10),
         title = "Correlation with class",
         fontsize = 18, color='r',
         rot = 45, grid = True)
    ax1.title.set_size(28)

    sns.set(style="white")
    cmap = sns.diverging_palette(220, 20, as_cmap=True)

    corr =data.corr()
    sns.clustermap(corr,cmap=cmap,
                  linewidths=1,linecolor='w')

plot_corr();**
```

**![](img/e20847b97c93e684c39d7554f75697f3.png)****![](img/c8e7ba54c63ef876e13e91e58318ba09.png)**

**æˆ‘ä¹‹æ‰€ä»¥é€‰æ‹©è¿™ä¸ªæ•°æ®é›†ï¼Œæ˜¯å› ä¸ºä»è¿™ä¸ªæ•°æ®é›†æ— è®ºæ˜¯ç²¾åº¦è¿˜æ˜¯å¬å›ç‡éƒ½å¾ˆéš¾è¾¾åˆ°é«˜åˆ†ã€‚è¯¥æ•°æ®é›†éå¸¸ä¸å¹³è¡¡ï¼Œå› ä¸ºåœ¨ 284ï¼Œ807 ç¬”äº¤æ˜“ä¸­æœ‰ 492 ç¬”(0.17%)æ¬ºè¯ˆã€‚è¯¥æ•°æ®é›†ä¸­ 99.83%çš„äº¤æ˜“ä¸æ˜¯æ¬ºè¯ˆæ€§çš„ï¼Œè€Œåªæœ‰ 0.17%æ˜¯æ¬ºè¯ˆæ€§çš„ã€‚**

```
**val_counts = data_original[['Class']].value_counts()
ax = sns.barplot(x=val_counts.index,
                 y=val_counts/len(data_original))
ax.set(title=f'Frequency Percentage by {val_counts}',
       xlabel='Class',
       ylabel='Frequency Percentage');**
```

**![](img/8512bb4eeb91adcdfe55d40231aa1183.png)**

**é¢å¯¹å¦‚æ­¤å°‘é‡çš„æ¬ºè¯ˆæ€§æ•°æ®ï¼Œæˆ‘ä»¬å¿…é¡»å°å¿ƒæˆ‘ä»¬çš„æ•°æ®å¤„ç†å’Œæ¨¡å‹é€‰æ‹©ã€‚ç®—æ³•å¯ä»¥å¾ˆå®¹æ˜“åœ°é€šè¿‡é¢„æµ‹æ‰€æœ‰çš„æµ‹è¯•æ•°æ®æ˜¯æ¬ºè¯ˆæ€§çš„æ¥æ¬ºéª—æˆ‘ä»¬ã€‚éšç€ 99.9%çš„æ•°æ®é›†æ˜¯è´Ÿé¢çš„(éæ¬ºè¯ˆ)ï¼Œç½‘ç»œå°†å·§å¦™åœ°é¢„æµ‹æ‰€æœ‰æ˜¯è´Ÿé¢çš„ï¼Œå¯¼è‡´è¶…è¿‡ 99%çš„å‡†ç¡®æ€§ã€‚ç»“æœçœ‹èµ·æ¥å¾ˆå¥½ï¼Œä½†æ²¡æœ‰ç”¨ã€‚**

**è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé™¤äº†å‡†ç¡®æ€§ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ›´å¥½çš„æŒ‡æ ‡ã€‚**

# **è¡¡é‡æ ‡å‡†**

**ç»“æœè´¨é‡æ²¡æœ‰å•ä¸€çš„æœ€ä½³è¡¡é‡æ ‡å‡†ï¼Œé—®é¢˜åŸŸå’Œæ•°æ®å†³å®šäº†åˆé€‚çš„æ–¹æ³•ã€‚**

**å¤§å¤šæ•°æœºå™¨å­¦ä¹ ä½¿ç”¨å‡†ç¡®åº¦ä½œä¸ºé»˜è®¤åº¦é‡ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨å‡†ç¡®åº¦ä½œä¸ºåº¦é‡ï¼Œæ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼Œå‡†ç¡®åº¦æ˜¯çœŸé˜´æ€§å’ŒçœŸé˜³æ€§çš„æ€»å’Œé™¤ä»¥æ€»æ•°æ®é›†å¤§å°ã€‚è€ƒè™‘åˆ°çœŸå®çš„è´Ÿå€¼å‹å€’çœŸå®çš„æ­£å€¼ï¼Œå‡†ç¡®æ€§å¯èƒ½éå¸¸é«˜ï¼Œä½†ä¸ä¼šè¡¨æ˜æ‚¨çš„æ¨¡å‹å¯¹æ¬ºè¯ˆæ€§æ•°æ®è¿›è¡Œåˆ†ç±»çš„èƒ½åŠ›ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç”±äºæ¦‚å¿µç®€å•ã€æ˜“äºå®ç°å’Œç”¨é€”å¹¿æ³›ï¼Œæœ‰ä¸€äº›åº¦é‡æ–¹æ³•è¢«æ™®éé‡‡ç”¨ã€‚å› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“å“ªä¸€ä¸ªæœ€é€‚åˆè¿™ä¸ªæ•°æ®é›†ï¼Œæ‰€ä»¥æˆ‘å°†åœ¨æœ¬æ–‡ä¸­åˆ—å‡ºå®ƒä»¬ã€‚**

*   **å‡†ç¡®(æ€§)**
*   **å¹³å‡ç²¾åº¦**
*   **åœ°ä¸‹åŒºåŸŸ**
*   **ç²¾ç¡®**
*   **å¬å›**
*   **f1-åˆ†æ•°**
*   **å›°æƒ‘ _ çŸ©é˜µ**
*   **ç²¾ç¡®å›å¿†æ›²çº¿**
*   **æ›²çº¿ä¸‹é¢ç§¯**

**ä¸‹é¢å°†å¯¹æ¯ä¸ªæŒ‡æ ‡åšä¸€äº›ä»‹ç»ã€‚å½“ç„¶ï¼Œåœ¨ç½‘ä¸Šå¯ä»¥æ‰¾åˆ°å¤§é‡å…³äºåº¦é‡çš„èµ„æ–™ã€‚å¦‚æœä½ å·²ç»çŸ¥é“ä»–ä»¬ä¸­çš„å¤§éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥åœ¨è·³åˆ°ä¸‹ä¸€éƒ¨åˆ†ä¹‹å‰åˆ·æ–°æˆ‘ä»¬çš„è®°å¿†ã€‚**

## **1.å¬å›ç‡ã€ç²¾ç¡®åº¦å’Œ AUC ( **æ›²çº¿ä¸‹é¢ç§¯** ) ROC**

**æˆ‘æŠŠè¿™ä¸‰ä¸ªæŒ‡æ ‡æ”¾åœ¨ä¸€èµ·ï¼Œå› ä¸ºå®ƒä»¬æœ‰å¾ˆå¥½çš„ç›¸å…³æ€§ã€‚**

**æ ¹æ® Wikipediaï¼Œprecision æ˜¯æ­£ç¡®ç»“æœçš„æ•°é‡é™¤ä»¥æ‰€æœ‰è¿”å›ç»“æœçš„æ•°é‡ï¼Œè€Œ recall æ˜¯æ­£ç¡®ç»“æœçš„æ•°é‡é™¤ä»¥åº”è¯¥è¿”å›çš„ç»“æœçš„æ•°é‡ã€‚**

**è€Œä¸‹é¢è¿™å¼ å›¾æ¯” 1000 å­—æ›´å¥½çš„è§£é‡Šäº†è¿™ä¸¤ä¸ªæ¦‚å¿µã€‚**

**![](img/30fe03b5f942a195ce9fcc4c62ca78ae.png)**

**[ç²¾ç¡®å’Œå¬å›æ¥è‡ªç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Precision_and_recall)**

**æ¥æ”¶ç®—å­ç‰¹å¾(ROC)æ˜¯æ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œå…¶ä¸­ x æ˜¯å‡é˜³æ€§ç‡(FPR ), y æ˜¯çœŸé˜³æ€§ç‡(TPR ),é€šå¸¸ç”¨äºå‘ˆç°æœºå™¨å­¦ä¹ ä¸­äºŒå…ƒå†³ç­–é—®é¢˜çš„ç»“æœã€‚AUC æ˜¯ ROC æ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œä»£è¡¨å¬å›(TPR)å’Œç‰¹å¼‚æ€§(FPR)ä¹‹é—´çš„æƒè¡¡ã€‚ä¸å…¶ä»–æŒ‡æ ‡ä¸€æ ·ï¼ŒAUC ä»‹äº 0 å’Œ 1 ä¹‹é—´ï¼Œ0.5 æ˜¯éšæœºé¢„æµ‹çš„é¢„æœŸå€¼ã€‚**

**AUC-ROC æ›²çº¿æ˜¯åœ¨å„ç§é˜ˆå€¼è®¾ç½®ä¸‹å¯¹åˆ†ç±»é—®é¢˜çš„æ€§èƒ½æµ‹é‡ã€‚ROC æ˜¯æ¦‚ç‡æ›²çº¿ï¼ŒAUC ä»£è¡¨å¯åˆ†æ€§çš„ç¨‹åº¦æˆ–åº¦é‡ã€‚å®ƒå‘Šè¯‰æˆ‘ä»¬æ¨¡å‹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½å¤ŸåŒºåˆ†ä¸åŒçš„ç±»ã€‚**

> ****ä¸ç²¾ç¡®å¬å›æ›²çº¿ä¸åŒï¼ŒROC(æ¥æ”¶è€…æ“ä½œè€…ç‰¹å¾)æ›²çº¿æœ€é€‚ç”¨äºå¹³è¡¡æ•°æ®é›†ã€‚****

**![](img/3f6dc609ea4c07a9741df8886defa452.png)**

**[æ¥æº](https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/)**

**AUC æ¥è¿‘ 1ï¼Œè¿™æ„å‘³ç€å®ƒå…·æœ‰è‰¯å¥½çš„å¯åˆ†æ€§ã€‚å·®æ¨¡å‹çš„ AUC æ¥è¿‘ 0ï¼Œè¿™æ„å‘³ç€å®ƒå…·æœ‰æœ€å·®çš„å¯åˆ†æ€§åº¦é‡ã€‚äº‹å®ä¸Šï¼Œè¿™æ„å‘³ç€å®ƒåœ¨å¾€å¤ç»“æœã€‚**

**ä¸€ç¯‡å…³äºè¿™ä¸ªè¯é¢˜çš„æ–‡ç« å¯ä»¥åœ¨[è¿™é‡Œ](/understanding-auc-roc-curve-68b2303cc9c5)æ‰¾åˆ°ã€‚æ­¤å¤–ï¼Œä¸€ä¸ªå¾ˆå¥½çš„ ROC å’Œ AUC è§†é¢‘å¯ä»¥ä» Josh Starmer çš„ [StatQuest ä¸­æ‰¾åˆ°ã€‚](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)**

## **2.å¹³å‡ç²¾åº¦**

**å¹³å‡ç²¾åº¦æ˜¯ä¸€ä¸ªç”¨æ¥æ¦‚æ‹¬ç²¾åº¦-å¬å›æ›²çº¿(PR AUC)çš„å•ä¸€æ•°å­—ï¼Œå®ƒä½¿å¾—æ¯”è¾ƒä¸åŒçš„æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚PR AUC æ˜¯æ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œå…¶ä¸­ x æ˜¯å¬å›ç‡ï¼Œy æ˜¯ç²¾ç¡®åº¦ã€‚å¹³å‡ç²¾åº¦(AP)çš„ä¸€èˆ¬å®šä¹‰æ˜¯æ‰¾åˆ°ä¸Šè¿°ç²¾åº¦-å¬å›æ›²çº¿ä¸‹çš„é¢ç§¯ã€‚**

**![](img/8230d2e2fce4369e5a3183a15730d809.png)**

**ç²¾ç¡®åº¦å’Œå¬å›ç‡æ€»æ˜¯åœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚å› æ­¤ï¼ŒAP ä¹Ÿåœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚**

**[è¿™é‡Œ](https://sanchom.wordpress.com/tag/average-precision/)æ˜¯ä¸€ç¯‡å…³äºè¿™ä¸ªè¯é¢˜çš„å¥½æ–‡ç« ã€‚**

> ****ç„¶è€Œï¼Œå½“å¤„ç†é«˜åº¦å€¾æ–œçš„æ•°æ®é›†æ—¶ï¼Œç²¾ç¡®å¬å›(PR)æ›²çº¿ç»™å‡ºäº†ä¸€ä¸ªç®—æ³•æ€§èƒ½çš„** [**æ›´ä¸°å¯Œçš„**](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf) **ç”»é¢ã€‚** [å½“ AUC å’Œ AP éƒ½è¢«é‡æ–°è°ƒæ•´åˆ°ä½äº[0ï¼Œ1]æ—¶ï¼ŒAP å¤§çº¦æ˜¯ AUC ä¹˜ä»¥ç³»ç»Ÿçš„åˆå§‹ç²¾åº¦ã€‚](https://dl.acm.org/doi/10.1145/2808194.2809481)**

## ****3ã€‚F1 åˆ†æ•°****

**ä¼ ç»Ÿçš„ F å€¼æˆ–å¹³è¡¡ F å€¼( **F1 å€¼**)æ˜¯ç²¾åº¦å’Œå¬å›ç‡çš„ [**è°ƒå’Œå¹³å‡å€¼**](https://en.wikipedia.org/wiki/Harmonic_mean#Harmonic_mean_of_two_numbers) ã€‚æˆ‘ä»¬è®¡ç®— F1 åˆ†æ•°ä½œä¸ºç²¾åº¦çš„è°ƒå’Œå¹³å‡å€¼ï¼Œå¹¶å›æƒ³ä¸€ä¸‹å¦‚ä½•å®ç°è¿™ä¸€ç‚¹ã€‚è™½ç„¶æˆ‘ä»¬å¯ä»¥å–ä¸¤ä¸ªåˆ†æ•°çš„ç®€å•å¹³å‡å€¼ï¼Œä½†è°ƒå’Œå¹³å‡å€¼æ›´èƒ½æŠµæŠ—å¼‚å¸¸å€¼ã€‚å› æ­¤ï¼ŒF1 åˆ†æ•°æ˜¯ä¸€ä¸ªå¹³è¡¡çš„åº¦é‡ï¼Œå®ƒæ°å½“åœ°é‡åŒ–äº†è·¨è®¸å¤šé¢†åŸŸçš„æ¨¡å‹çš„æ­£ç¡®æ€§ã€‚**

**![](img/187f446e6fff0503eb30cb3219e756e3.png)**

****F1 å¾—åˆ†****

**F1 åˆ†æ•°é€‚ç”¨äº ROC æ›²çº¿çš„ä»»ä½•ç‰¹å®šç‚¹ã€‚è¯¥ç‚¹å¯ä»¥ä»£è¡¨ä¾‹å¦‚äºŒå…ƒåˆ†ç±»å™¨ä¸­çš„ç‰¹å®šé˜ˆå€¼ï¼Œå› æ­¤å¯¹åº”äºç‰¹å®šçš„ç²¾åº¦å’Œå¬å›å€¼ã€‚**

> **è¯·è®°ä½ï¼ŒF1 åˆ†æ•°æ˜¯ä¸€ç§æ—¢ä»£è¡¨å¬å›ç‡åˆä»£è¡¨å‡†ç¡®ç‡çš„èªæ˜æ–¹æ³•ã€‚å¯¹äºè¦é«˜çš„ F1 åˆ†æ•°ï¼Œç²¾ç¡®åº¦å’Œå¬å›ç‡éƒ½åº”è¯¥é«˜ã€‚**

**å› æ­¤ï¼ŒROC æ›²çº¿é’ˆå¯¹å„ç§ä¸åŒæ°´å¹³çš„é˜ˆå€¼ï¼Œå¹¶ä¸”å¯¹äºå…¶æ›²çº¿ä¸Šçš„å„ä¸ªç‚¹å…·æœ‰è®¸å¤š F1 åˆ†æ•°å€¼ã€‚**

## ****4ã€‚æ··ä¹±çŸ©é˜µ****

**å¯¹äºå¦‚æ­¤é«˜åº¦ä¸å¹³è¡¡çš„æ•°æ®é›†ï¼Œæ··æ·†çŸ©é˜µæ²¡æœ‰å¤šå¤§æ„ä¹‰ã€‚æˆ‘å°†å®ƒæ·»åŠ åˆ°åº¦é‡é›†åˆä¸­ï¼Œä»…ä¾›å‚è€ƒã€‚å®ƒå¯ä»¥é€šè¿‡ä¸‹è¡¨è‡ªæˆ‘è§£é‡Š:**

**![](img/5b15bfa386b3dd73663ccc6501f733ca.png)**

*   ****çœŸé˜³æ€§(TP):** è¿™äº›æ˜¯æˆ‘ä»¬é¢„æµ‹æ˜¯çš„ç—…ä¾‹(ä»–ä»¬æœ‰ç–¾ç—…)ï¼Œä»–ä»¬ç¡®å®æœ‰ç–¾ç—…ã€‚**
*   ****çœŸé˜´æ€§(TN):** æˆ‘ä»¬é¢„æµ‹æ²¡æœ‰ï¼Œä»–ä»¬æ²¡æœ‰è¿™ç§ç—…ã€‚**
*   **å‡é˜³æ€§(FP): æˆ‘ä»¬é¢„æµ‹æ˜¯çš„ï¼Œä½†ä»–ä»¬å®é™…ä¸Šå¹¶æ²¡æœ‰æ‚£ç—…ã€‚(ä¹Ÿç§°ä¸ºâ€œç¬¬ä¸€ç±»é”™è¯¯â€)**
*   ****å‡é˜´æ€§(FN):** æˆ‘ä»¬é¢„æµ‹æ²¡æœ‰ï¼Œä½†ä»–ä»¬ç¡®å®æœ‰ç–¾ç—…ã€‚(ä¹Ÿç§°ä¸ºâ€œç¬¬äºŒç±»é”™è¯¯â€)**

**æˆ‘åœ¨ä¸€ä¸ªå‡½æ•°ä¸­æ€»ç»“äº†æ‰€æœ‰çš„æŒ‡æ ‡ï¼Œä»¥ä¾¿ç¨åè°ƒç”¨ã€‚**

```
**def metrics_summary(true_label, prediction_prob, Threshold=0.5):

    #basically, slearn provides all the functions for metrics. average_precision = average_precision_score(true_label
    ,prediction_prob)
    fpr, tpr, thresholds = roc_curve(true_label, prediction_prob)
    areaUnderROC = auc(fpr, tpr)

    prediction_int = prediction_prob > Threshold

    accuracy = accuracy_score(true_label, prediction_int)

    print(f'accuracy: {accuracy}')
    print(f"average_precision: {average_precision}")
    print(f'areaUnderROC: {areaUnderROC } \n')
    print('*'*60)
    print(' '*20, 'classification_report')
    print('*'*60, "\n")
    print(classification_report(true_label, prediction_int))

    print('*'*60)
    print(' '*20, 'confusion_matrix \n')
    print('*'*60, "\n")
    display(confusion_matrix(true_label, prediction_int))
    print("\n")

    # precision_recall_curve and areaUnderROC 
    precision, recall, thresholds = precision_recall_curve( \
                                true_label, prediction_int)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,9))

    ax1.step(recall, precision, color='k', alpha=0.7, where='post')
    ax1.fill_between(recall, precision, step='post', 
    alpha=0.3,color='k') ax1.set_xlabel('Recall', fontname="Arial", fontsize=24)
    ax1.set_ylabel('Precision', fontname="Arial", fontsize=24) 
    ax1.tick_params(labelsize=20)

    ax1.set_title('Precision-Recall curve: Average Precision \
    = {0:0.2f}'.format(average_precision), fontsize=24,
    fontname="Arial")        

    ax2.plot(fpr, tpr, color='r', lw=2, label='ROC curve')
    ax2.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')
    ax2.tick_params(labelsize=20)
    ax2.set_xlabel('False Positive Rate', fontname="Arial",
    fontsize=24)
    ax2.set_ylabel('True Positive Rate', fontname="Arial",
    fontsize=24)
    ax2.set_title('areaUnderROC = {0:0.2f}'\
            .format(areaUnderROC), fontsize=24, fontname="Arial",)    
    ax2.legend(loc="lower right", fontsize=24, fancybox=True) 
    # Adjust the subplot layout, because the logit one may take 
      more space
    # than usual, due to y-tick labels like "1 - 10^{-3}"
    # plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10,
    # right=0.95, hspace=0.25,wspace=0.35)**
```

**è¿™äº›è¯„ä¼°æŒ‡æ ‡éå¸¸é‡è¦ã€‚è¿™æœ‰ç‚¹åƒæ•°æ®ç§‘å­¦å®¶å’Œå•†ä¸šäººå£«ä¹‹é—´çš„æ¥å£ã€‚å¯¹äºå¤§å¤šæ•°åªå¬è¯´è¿‡ AI ä½†ä»æœªåœ¨ä»»ä½•æ¨¡å‹ä¸Šè®­ç»ƒè¿‡çš„äººæ¥è¯´ï¼Œä»–ä»¬ä¸ä¼šå¤ªå…³æ³¨åƒ log lossã€äº¤å‰ç†µå’Œå…¶ä»–æˆæœ¬å‡½æ•°è¿™æ ·çš„ä¸œè¥¿ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æŒ‡æ ‡æ¥ç›´è§‚åœ°å‘ä¸šåŠ¡äººå‘˜è§£é‡Šç»“æœã€‚å°½å¯èƒ½ç®€å•åœ°å‘éæ•°æ®ç§‘å­¦å®¶ä¼ è¾¾å¤æ‚ç»“æœçš„èƒ½åŠ›æ˜¯åº”ç”¨æ•°æ®ç§‘å­¦å®¶éœ€è¦æŒæ¡çš„åŸºæœ¬æŠ€èƒ½ä¹‹ä¸€ã€‚**

# **æ¨¡å‹**

**ä»¥ä¸‹æ˜¯æˆ‘å°†è¦æ¢è®¨çš„ä¸‰ç§æ¨¡å¼:**

1.  **é€»è¾‘å›å½’**
2.  **lightBGM**
3.  **DNN**

**æˆ‘é€‰æ‹©è¿™ä¸‰ä¸ªæ¨¡å‹çš„åŸå› æ˜¯å®ƒä»¬é«˜åº¦ä¸ç›¸å…³ã€‚ç‹¬ç«‹è§£å†³æ–¹æ¡ˆéœ€è¦ç›¸å¯¹ä¸ç›¸å…³ã€‚å¦‚æœå®ƒä»¬éå¸¸ç›¸å…³ï¼Œä¸€æ—¦å°†å®ƒä»¬æ”¾å…¥ä¸€ä¸ªé›†åˆæ¨¡å‹ï¼Œå…¶ä¸­ä¸€ä¸ªçš„ä¼˜åŠ¿å°†åæ˜ å…¶ä½™çš„ä¼˜åŠ¿ï¼ŒåŠ£åŠ¿ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬çœ‹ä¸åˆ°é€šè¿‡åˆå¥å®ç°å¤šæ ·åŒ–çš„å¥½å¤„ã€‚**

**åœ¨æˆ‘ä»¬è¿›å…¥æ¨¡å‹ä¹‹å‰ï¼Œä»ç„¶æœ‰ä¸€äº›äº‹æƒ…éœ€è¦åšï¼Œå³ï¼ŒåŸºäºç‰¹å¾çš„æ ‡å‡†åŒ–ã€è¾“å…¥å’Œæ ‡ç­¾åˆ†ç¦»ä»¥åŠåˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚**

```
**#Normalize training and testing data
def scale_data(x_train, x_test=None):   
    features_to_scale = x_train.copy().columns
    scaler = pp.StandardScaler()
    print(scaler.fit(x_train[features_to_scale]))

    x_train.loc[:, features_to_scale] = \
    scaler.transform(x_train[features_to_scale])

    #normalize test dataset with the mean and std of train data set 
    x_test.loc[:, features_to_scale] = \
    scaler.transform(x_test[features_to_scale])

    return x_train, x_test#seperate input and labels    
def get_x_y(data=data_original):
    data_x = data.copy().drop(['Class', 'Time'], axis=1)
    data_y = data['Class'].copy()

    return data_x, data_y#split the train and test data
def data_split(data_x, data_y):
    x_train, x_test, y_train, y_test = \
          train_test_split(data_x,data_y,test_size=0.25,
          stratify=data_y,random_state=2020)      

     return  x_train, x_test, y_train, y_test#put all together
def data_process(data=data_original):
    data_x, data_y = get_x_y(data)

    x_train, x_test, y_train, y_test \
    = data_split(data_x, data_y)

    #do not touch the test data by any means!!!!
    x_train, x_test = scale_data(x_train, x_test)

    return  x_train, x_test, y_train, y_test**
```

# **âœTipï¼**

**ç”±äºæ•°æ®é›†é«˜åº¦ä¸å¹³è¡¡ï¼Œscikit-learn çš„`train_test_split()`å‡½æ•°ä¸­çš„å‚æ•°`stratify =data_y`æ¥å¾—éå¸¸æ–¹ä¾¿ã€‚æ•°æ®åœ¨ä¸€ç¬é—´ä»¥åˆ†å±‚çš„æ–¹å¼è¢«åˆ†å‰²ã€‚**

**ä½ å¯èƒ½æ³¨æ„åˆ°æˆ‘ç”¨ data_originalï¼Œx_test_orignalï¼Œy_test_orignal ä½œä¸ºå˜é‡åã€‚æˆ‘æƒ³æŠŠè¿™ä¸‰ä¸ªå­æ•°æ®é›†æ”¾åœ¨ä¸€è¾¹ï¼Œå› ä¸ºè¿™äº›åŸå§‹æ•°æ®ä¼šæœ‰å¾ˆå¤šè°ƒæ•´ï¼Œæˆ‘ä»¬ä¸æƒ³ä»¥ä»»ä½•æ–¹å¼å¼„ä¹±æµ‹è¯•æ•°æ®ã€‚**

```
**x_train, x_test_original, y_train, y_test_original \
= data_process(data_original)x_train.shape, x_test_original.shape, \
y_train.shape, y_test_original.shape**
```

**![](img/4970153f203e097b7bb22ab503d4c782.png)**

**è®­ç»ƒæ•°æ®é›†æœ‰ 213605 æ¡äº‹åŠ¡è®°å½•å’Œ 29 ä¸ªç‰¹å¾ï¼Œè€Œæµ‹è¯•æ•°æ®é›†æœ‰ 71202 æ¡äº‹åŠ¡è®°å½•å’Œ 29 ä¸ªç‰¹å¾ã€‚**

```
**print(f'No. of fraud in test dataset:\
      {x_test_original[y_test_original==1].shape[0]}')**
```

**![](img/db48157761baac6b30788bf12097efbe.png)**

**åœ¨æµ‹è¯•æ•°æ®é›†ä¸­çš„ 71202 æ¡äº¤æ˜“è®°å½•ä¸­ï¼Œåªæœ‰ 123 æ¡è®°å½•æ˜¯æ¬ºè¯ˆæ€§çš„ã€‚**

# **âœTipï¼**

***æ³¨æ„ï¼Œæµ‹è¯•æ•°æ®æ˜¯é€šè¿‡è®­ç»ƒæ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®æ¥æ ‡å‡†åŒ–çš„ã€‚ä½ ä¸åº”è¯¥åœ¨ä½ çš„å·¥ä½œæµç¨‹ä¸­ä½¿ç”¨ä»»ä½•æ ¹æ®æµ‹è¯•æ•°æ®è®¡ç®—çš„æ•°é‡ï¼Œå³ä½¿æ˜¯ç®€å•çš„æ•°æ®æ ‡å‡†åŒ–ã€‚* ***æ¢å¥è¯è¯´ï¼Œæ°¸è¿œä¸è¦ç¢°ä½ çš„æµ‹è¯•æ•°æ®ï¼*****

**æ•°æ®å‡†å¤‡å¥½äº†ï¼ŒæŒ‡æ ‡ç¡®å®šäº†ï¼Œæ¨¡å‹æ¥äº†:**

1.  ****æ¨¡å‹ 1** *(é€»è¾‘å›å½’)***

```
**def build_model_1(x_train, y_train):

    logitreg_parameters = {'C': np.power(10.0, np.arange(-9, 1)),
                           'solver' : ('lbfgs', 'liblinear') }

    model_1 = LogisticRegression(#solver='liblinear',
                                 class_weight='balanced', 
    #uses the values of y to automatically adjust weights

                                 warm_start=True,
    #reuse the solution of the previous call to fit
     as initialization
                                 max_iter = 300,
    #Maximum number of iterations taken for the solvers to converge.
                                 random_state=2020, 
    #so results can be reproduced
                                 ) logitreg_grid = GridSearchCV(model_1, param_grid = \
           logitreg_parameters,scoring = 'f1', n_jobs = 1, cv=5)          

    logitreg_grid.fit(x_train, y_train)

    return logitreg_gridmodel_1 = build_model_1(x_train, y_train)**
```

**å¼€å§‹æ—¶ï¼Œç”±äºæœ¬æ–‡ä¸æ˜¯å…³äºè·å¾—æœ€ä½³åˆ†æ•°ï¼Œæˆ‘ä½¿ç”¨äº†æ¥è‡ª ***sklearn*** çš„é€»è¾‘å›å½’çš„é»˜è®¤è®¾ç½®ã€‚ç²¾åº¦åˆ†å¾ˆä½ï¼Œ0.07 å·¦å³ã€‚ä½¿ç”¨ç›¸åŒçš„é»˜è®¤è®¾ç½®ï¼Œä¸€äº›åœ¨çº¿æ•™ç¨‹åœ¨ä½¿ç”¨é€»è¾‘å›å½’çš„ç›¸åŒæ•°æ®é›†ä¸Šæ˜¾ç¤ºäº†éå¸¸é«˜çš„ç²¾ç¡®åº¦ã€‚ç„¶è€Œï¼Œä»–ä»¬å¤„ç†æµ‹è¯•æ•°æ®çš„æ–¹å¼æ˜¯æœ‰é—®é¢˜çš„ã€‚ä¸ºäº†å¾—åˆ°æ›´å¥½çš„ç»“æœï¼Œä½¿ç”¨äº†`GridSearchCV`æ¥æœç´¢æœ€ä½³å‚æ•°ã€‚**

**![](img/f65b41c128240af2b167dd3d7f96af94.png)****![](img/0ddcf6427e9bf5cf4dd9189fe0498e04.png)**

**æˆ‘å¾—åˆ°çš„æœ€å¥½æˆç»©æ˜¯**

```
**model_1.best_score_**
```

**![](img/15c545403297e9d12133134e32d97a15.png)**

**ä½¿ç”¨ä»¥ä¸‹è®¾ç½®:**

```
**model_1.best_estimator_**
```

**![](img/6b74d2d1c92d80e6e403d6e1d7368cd9.png)**

**è®©æˆ‘ä»¬æ£€æŸ¥æµ‹è¯•æ•°æ®é›†çš„ç»“æœå’ŒæŒ‡æ ‡å¾—åˆ†:**

```
**# 0 and 1 two clasese
y_pred_prob_test_1 = model_1.predict_proba(x_test_original)[:,1]
# number of fraud is 123 in test dataset
y_pred_int_test_1 = y_pred_prob_test_1 > Threshold
pd.Series(y_pred_int_test_1).value_counts()**
```

**![](img/373d9640bc586dfa27147bc80f573d89.png)**

**ç›¸å½“ä¸é”™ï¼Œè®°ä½æ¬ºè¯ˆäº¤æ˜“çš„æ•°é‡æ˜¯ 123ã€‚**

```
**metrics_summary(y_test_original, y_pred_int_test_1)**
```

**![](img/48c93ab0ea14c42de566770b1e686aa8.png)****![](img/8f3a8d0cfc41e4ede81a0b8ab8650e56.png)**

****2ã€‚å‹å· 2** *(LightBGM)***

**å¯¹äº lightBGMï¼Œæˆ‘è¿›ä¸€æ­¥æŒ‡å®š 1/4 çš„è®­ç»ƒæ•°æ®ä½œä¸ºéªŒè¯æ•°æ®é›†ã€‚**

```
**#prepare data 
x_train_, x_cv, y_train_, y_cv = \
train_test_split(x_train, y_train,
                test_size=0.25,
                stratify=y_train,
                random_state=2020)def build_model_2(x_train, y_train, x_cv, y_cv ):
    #most of the parsmeters are default
    params_lightGB = {
    'task': 'train',
    'application':'binary',
    'num_class':1,
    'boosting': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'metric_freq':50,
    'is_training_metric':False,
    'max_depth':4,
    'num_leaves': 31,
    'learning_rate': 0.01,
    'feature_fraction': 1.0,
    'bagging_fraction': 1.0,
    'bagging_freq': 0,
    'bagging_seed': 2018,
    'verbose': 0,
    'num_threads':16
    } lgb_train = lgb.Dataset(x_train, y_train)
    lgb_eval = lgb.Dataset(x_cv, y_cv, reference=lgb_train) model_2 = lgb.train(params_lightGB, lgb_train,
                    num_boost_round=2000,
                    valid_sets=lgb_eval,
                    early_stopping_rounds=200,
                    verbose_eval=False)
    return model_2x_train_.shape, y_train_.shape, x_cv.shape, y_cv.shape**
```

**![](img/62b66d98552cda5e44403ffbe0775241.png)**

**ç»“æœè¿˜ä¸é”™ï¼Œæ²¡æœ‰è¿›è¡Œè¿›ä¸€æ­¥çš„è¶…å‚æ•°è°ƒæ•´ã€‚**

```
**model_2 = build_model_2(x_train_, y_train_, x_cv, y_cv)y_pred_prob_test_2 = model_2.predict(x_test_original)
y_pred_int_test_2 = y_pred_prob_test_2 > Threshold
pd.DataFrame(y_pred_int_test_2).value_counts()**
```

**![](img/fd50ba28daa9a734a2d6b4522e75e5ff.png)**

```
**metrics_summary(y_test_original, y_pred_int_test_2)**
```

**![](img/358c1983a82414465eeceb398bba1f6b.png)****![](img/746ffa62671dc9346a1eb74e5deb896e.png)**

**ä¸æ¨¡å‹ 1 ç›¸åŒï¼Œå‡†ç¡®ç‡éå¸¸é«˜(99.9%)ï¼Œç²¾ç¡®åº¦å¾ˆå¥½(93%)ï¼Œè€Œå¬å›ç‡é€‚ä¸­(77%)ã€‚**

****3ã€‚æ¨¡å‹ 3** *(æ·±åº¦ç¥ç»å…ƒç½‘ç»œ)***

**å¯¹äºæ·±åº¦ç¥ç»å…ƒç½‘ç»œ(DNN)ï¼Œæˆ‘ä½¿ç”¨äº†ä¸¤ä¸ªå›è°ƒï¼ŒEarlyStopping()å’Œ ReduceLROnPlateau()æ¥è·å¾—æ›´å¥½çš„ç»“æœã€‚åŒæ ·ï¼Œç”±äºç»“æœè¿˜ä¸é”™ï¼Œæ‰€ä»¥æ²¡æœ‰å®ç°è¶…å‚æ•°è°ƒä¼˜ã€‚( [Keras-tuner](https://medium.com/@kegui/a-few-pitfalls-for-kerastuner-beginner-users-13116759435b) ç¡®å®ä¸é”™ï¼Œå¯ä»¥æŸ¥ä¸€ä¸‹[æˆ‘ä»¥å‰çš„æ–‡ç« ](https://medium.com/@kegui/how-to-do-cross-validation-in-keras-tuner-db4b2dbe079a)æ›´å¥½çš„äº†è§£ã€‚)**

```
**callbacks = [EarlyStopping(monitor='loss', patience=3), \
                 ReduceLROnPlateau(monitor='val_loss', factor=0.2, \
                                   patience=3, min_lr=0.001)]def build_model_3(x_train, y_train, x_cv, y_cv, input_dim=29): 
    model_3 = Sequential([
                Dense(input_dim = input_dim, units = 32, 
                      activation  = 'relu'),
                Dense(units = 16, activation =  'relu'),
                #Dropout(0.5),
                Dense(units = 8, activation =  'relu'),
                Dense(units =1, activation = 'sigmoid'),]) model_3.compile(optimizer = 'adam', 
                 loss = 'binary_crossentropy', 
                 metrics = ['accuracy'])

    model_3.fit(x_train, y_train, 
              validation_data = (x_cv, y_cv),
              batch_size = 64, 
              epochs = 50,
              callbacks=callbacks)

    return model_3**
```

**è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ 3 å±‚ DNNï¼Œå…·æœ‰è¾ƒå°çš„å•å…ƒæ•°(32ï¼Œ16ï¼Œ8)ä»¥é¿å…è¿‡åº¦æ‹Ÿåˆã€‚**

# **âš ï¸Warning**

**DNN å¯¹è¾“å…¥ç‰¹å¾éå¸¸æ•æ„Ÿã€‚å¦‚æœå°†æ—¶é—´ç‰¹å¾æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­ï¼Œç»“æœä¼šæœ‰äº›å¥‡æ€ªã€‚ä½†æ˜¯ä¸€æ—¦ä½ å»æ‰æ—¶é—´ç‰¹æ€§ï¼Œå®ƒå°±æ¢å¤æ­£å¸¸äº†ã€‚**

**ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨æ¨¡å‹ 3 ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ•°æ®ã€‚**

```
**model_3 = build_model_3(x_train_, y_train_, \
x_cv, y_cv, input_dim=29)
y_pred_prob_test_3 = model_3.predict(x_test_original)y_pred_int_test_3 = y_pred_prob_test_3 > Threshold
y_pred_int_test_3.shape
pd.DataFrame(y_pred_int_test_3).value_counts()** 
```

**![](img/9f2e0d27598a921f9a0376857ec55b6c.png)**

```
**metrics_summary(y_test_original, y_pred_int_test_3)**
```

**![](img/22381e81f5ed68a411a6900ef9f1b7b0.png)****![](img/2849c8021848e8e2be1a90cf623279b1.png)**

**åœ¨æ²¡æœ‰ä»»ä½•è¶…å‚æ•°è°ƒæ•´çš„æƒ…å†µä¸‹ï¼Œç»“æœç±»ä¼¼äºä¼˜åŒ–çš„æ¨¡å‹ 1 é€»è¾‘å›å½’ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬åŒ…æ‹¬æ¥è‡ªä¸åŒæœºå™¨å­¦ä¹ å®¶æ—çš„ç±»ä¼¼å¼ºè§£(ä¾‹å¦‚ä¸€ä¸ªæ¥è‡ªéšæœºæ£®æ—ï¼Œä¸€ä¸ªæ¥è‡ªç¥ç»ç½‘ç»œ)ï¼Œè¿™äº›è§£çš„é›†åˆå°†å¯¼è‡´æ¯”ä»»ä½•ç‹¬ç«‹è§£æ›´å¥½çš„ç»“æœã€‚è¿™æ˜¯å› ä¸ºæ¯ä¸ªç‹¬ç«‹è§£å†³æ–¹æ¡ˆéƒ½æœ‰ä¸åŒçš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚é€šè¿‡å°†ç‹¬ç«‹çš„è§£å†³æ–¹æ¡ˆæ•´åˆåœ¨ä¸€èµ·ï¼Œä¸€äº›æ¨¡å‹çš„ä¼˜åŠ¿(T2)å¼¥è¡¥äº†å…¶ä»–æ¨¡å‹çš„åŠ£åŠ¿ï¼Œåä¹‹äº¦ç„¶ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ¥è‡ª 3 ä¸ªéå¸¸ä¸åŒçš„æ¨¡å‹çš„ç»“æœä¼¼ä¹æ»¡è¶³äº†æˆ‘ä»¬å¯¹å †å å’Œå…ƒæ ‡è®°çš„è¦æ±‚ã€‚**

**è®©æˆ‘ä»¬ç°åœ¨é›†åˆé‚£äº›æ¨¡å‹ã€‚**

# **å †å›**

**å› ä¸ºå åŠ æœ‰ç‚¹åƒå‘è¾“å…¥æ•°æ®æ·»åŠ æ–°ç‰¹å¾ï¼Œè€Œæ–°ç‰¹å¾æ¥è‡ªäºä¸»æ¨¡å‹çš„é¢„æµ‹ã€‚æˆ‘ä»¬é¦–å…ˆåšä¸€ä¸ªç‰¹å¾å·¥ç¨‹æ¥å †å æ‰€æœ‰çš„æ•°æ®ã€‚**

```
**def data_stack( x, y, m_1=model_1, m_2=model_2, m_3=model_3):
    #All required parameters must be placed before any 
     default arguments.
    '''
    x: features
    y: labels
    m_1, m_2, m_3: 3 models

    '''
    # build a container to hold all the prediction from 3 models
    pred_all = pd.DataFrame(data=[], index=y.index) pred_1 = m_1.predict_proba(x)[:,1]
    pred_1_df = pd.DataFrame(pred_1, index=y.index) pred_2 = m_2.predict(x,num_iteration=m_2.best_iteration)

    pred_2_df = pd.DataFrame(pred_2, index=y.index) pred_3 = m_3.predict(x).reshape(x.shape[0]) #to 1D shape
    pred_3_df = pd.DataFrame(pred_3, index=y.index) # join all the predictions together
    pred_all = pred_all.join(pred_1_df.astype(float),
                             how='left',rsuffix="0")\
                       .join(pred_2_df.astype(float),
                             how='left',rsuffix="1")\
                       .join(pred_3_df.astype(float),
                             how='left',rsuffix="2")
    pred_all.columns = ['pred_1', 'pred_2','pred_3']
    # final training data will be the merge of training data 
      and all the predictions
    x_pred = x.merge(pred_all, \
                    left_index=True, right_index=True)

    return x_pred**
```

**ç°åœ¨ï¼Œå°†æ–°ç‰¹å¾(æ¥è‡ª 3 ä¸ªæ¨¡å‹çš„é¢„æµ‹)æ·»åŠ åˆ°è®­ç»ƒæ•°æ®é›†ä¸­ã€‚**

```
**x_train_stack = data_stack(x_train, y_train)
x_train_stack.shape**
```

**![](img/474f9a836c8db11ac958c4f0ca6271f9.png)**

**ç„¶åï¼Œç›¸åŒçš„è¿‡ç¨‹åº”ç”¨äºæµ‹è¯•æ•°æ®é›†ã€‚**

```
**x_test_stack = data_stack(x_test_original, y_test_original)
x_test_stack.shape**
```

**![](img/a81c123f128253bc18a68ef7f9d13ba3.png)**

**ç”±äºæˆ‘ä»¬åœ¨ä¹‹å‰çš„æ•°æ®é›†ä¸­æ·»åŠ äº†æ–°çš„è¦ç´ ï¼Œå› æ­¤æŸ¥çœ‹è¿™äº›æ–°è¦ç´ ä¹‹é—´çš„ç›¸å…³æ€§ä¼šå¾ˆæœ‰æ„æ€ã€‚**

**![](img/d4776737c2afb5e81f7e3cbe3c5db9ba.png)****![](img/b9fa6cb02341d35b2f36cfc2bb257dd8.png)**

**æˆ‘ä»¬ç¡®å®çœ‹åˆ°åˆçº§æ¨¡å‹çš„é¢„æµ‹é«˜åº¦ç›¸å…³ï¼Œè¿™å¹¶ä¸å¥‡æ€ªã€‚**æ¥è‡ªç¬¬ä¸€æ¨¡å‹çš„ä¿¡æ¯æ³„æ¼åˆ°ç¬¬äºŒæ¨¡å‹ä¸­ï¼Œå› ä¸ºå®ƒä»¬å…±äº«ç›¸åŒçš„è®­ç»ƒæ•°æ®ã€‚åªè¦æµ‹è¯•æ•°æ®æ˜¯å®Œæ•´çš„ï¼Œæˆ‘ä»¬å°†æ›´å–œæ¬¢æ›´å¤šçš„ä¿¡æ¯æµå…¥ç¬¬äºŒä¸ªæ¨¡å‹ï¼Œä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚****

**ç°åœ¨æˆ‘ä»¬éœ€è¦ç»å†æ‰€æœ‰é‚£äº›ä¹å‘³ä½†å¿…è¦çš„æ•°æ®è¿‡ç¨‹ã€‚**

```
**#normalize training and testing data
x_train_stack, x_test_stack = scale_data(x_train_stack,  x_test_stack)#split the traning data to train and validation
x_train_stack_, x_cv_stack, y_train_, y_cv_ = \
train_test_split(x_train_stack, y_train,
                test_size=0.25,
                stratify=y_train,
                random_state=2020)
#stratify mean samplling with the ratio of each class percentage in #all data.x_train_stack_.shape, x_cv_stack.shape, y_train_.shape,  y_cv.shape**
```

**![](img/45265409673c7790b53c8a01bacbf33c.png)**

**ä¸è¿‡ï¼Œè¯´åˆ°å †å ï¼Œæœ‰ä¸€äº›é‡è¦çš„æ³¨æ„äº‹é¡¹ã€‚å¦‚æœç‹¬ç«‹çš„è§£å†³æ–¹æ¡ˆåŒæ ·å¼ºå¤§ï¼Œé›†åˆå°†æ¯”ä»»ä½•ç‹¬ç«‹çš„è§£å†³æ–¹æ¡ˆå…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚ä½†æ˜¯ï¼Œå¦‚æœå…¶ä¸­ä¸€ä¸ªè§£å†³æ–¹æ¡ˆæ¯”å…¶ä»–è§£å†³æ–¹æ¡ˆå¥½å¾—å¤šï¼Œé›†åˆçš„æ€§èƒ½å°†ç­‰äºæœ€ä½³ç‹¬ç«‹è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ï¼›ä¸åˆæ ¼çš„è§£å†³æ–¹æ¡ˆå¯¹æ•´ä½“æ€§èƒ½æ²¡æœ‰ä»»ä½•è´¡çŒ®ã€‚**

1.  ****å‹å· 2 (lightBGM)ä½œä¸ºäºŒçº§å‹å·****

**ç”±äºæ¨¡å‹ 2 (lightBGM)çš„ç²¾åº¦è¿„ä»Šä¸ºæ­¢æœ€é«˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¨¡å‹ 2 ä½œä¸ºç¬¬äºŒæ¨¡å‹ã€‚**

```
**model_2_stack = build_model_2(x_train_stack_, y_train_, x_cv_stack, y_cv_)
y_pred_prob_test_2_stack = model_2_stack.predict(x_test_stack)
y_pred_int_test_2_stack = y_pred_prob_test_2_stack > Threshold
pd.DataFrame(y_pred_int_test_2_stack).value_counts()**
```

**![](img/e970d3268e2e382b20d97171d009c031.png)**

**è¿™é‡Œï¼Œåœ¨æ²¡æœ‰å †å çš„æƒ…å†µä¸‹ï¼Œåœ¨ç›¸åŒæ¨¡å‹ä¸Šçš„ 102 ä¸ªç—…ä¾‹çš„æ¯”è¾ƒä¸­ç¡®å®šäº† 120 ä¸ªé˜³æ€§ç»“æœã€‚**

```
**metrics_summary(y_test_original, y_pred_int_test_2_stack)**
```

**![](img/595db27ba0b28b079e7fa27a7d554d32.png)****![](img/e23e2585256bc57831f51954ec726372.png)**

**å¬å›ç‡ä» 0.77 æé«˜åˆ° 0.85ï¼Œf1 åˆ†æ•°å’Œå¹³å‡å‡†ç¡®ç‡ä¹Ÿæœ‰é€‚åº¦çš„æé«˜ã€‚**

****2ã€‚ç¬¬ä¸‰æ¬¾ DNN ä½œä¸ºç¬¬äºŒæ¬¾****

**å¦‚æœæˆ‘ä»¬æŠŠ DNN ä½œä¸ºç¬¬äºŒä¸ªæ¨¡å‹ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**

```
**model_3_stack = build_model_3(x_train_stack_, y_train_, \
                        x_cv_stack, y_cv_, input_dim=32)y_pred_prob_test_3_stack = model_3_stack.predict(x_test_stack)
y_pred_int_test_3_stack = y_pred_prob_test_3_stack > Threshold
y_pred_int_test_3_stack.shape
pd.DataFrame(y_pred_int_test_3_stack).value_counts()**
```

**![](img/66c94396253ea54a39575dd18927f12a.png)**

**åŒæ ·çš„æ¨¡å‹ï¼Œ88 æ¯” 116ï¼Œç»“æœä¸å¤ªä¹è§‚ï¼Œè¿™è¡¨æ˜ç²¾ç¡®åº¦æ›´é«˜ï¼Œå¬å›ç‡æ›´ä½ã€‚**

```
**metrics_summary(y_test_original, y_pred_int_test_3_stack)**
```

**![](img/563547f87e6d1347553454597f188572.png)****![](img/a2880c0a00c29e9276949b6a4abafbf2.png)**

**ä¸å‡ºæ‰€æ–™ï¼Œç²¾åº¦æ›´é«˜(0.95)ï¼Œä½†å¬å›ç‡å¾ˆä½(0.68)ã€‚**

****3ã€‚æ¨¡å‹ 1 ä½œä¸ºäºŒçº§æ¨¡å‹çš„é€»è¾‘å›å½’****

**æ²¡æƒ³åˆ°ä¼šæœ‰æ›´å¥½çš„ç»“æœï¼Œçº¯ç²¹å¥½å¥‡ã€‚**

```
**model_1_stack = build_model_1(x_train_stack, y_train)model_1_stack.best_score_**
```

**![](img/4264326e909664a80d9cb96b09862034.png)**

**ç»“æœçœ‹èµ·æ¥å¾ˆæœ‰å¸Œæœ›ã€‚**

```
**y_pred_prob_test_1_stack = model_1_stack.predict_proba(x_test_stack)[:,1]# 0 and 1 two clases
y_pred_int_test_1_stack = y_pred_prob_test_1_stack > Threshold
pd.Series(y_pred_int_test_1_stack).value_counts()**
```

**![](img/1332d4e43e9a5eebdb26fb273c39b347.png)**

```
**metrics_summary(y_test_original, y_pred_int_test_1_stack)**
```

**![](img/5d2c95bde08e6fda97ae9addd0d45eb0.png)****![](img/073653af5093526d6658fd6931365797.png)**

**å‡ ä¹æ‰€æœ‰çš„æŒ‡æ ‡éƒ½æœ‰æˆ–å¤šæˆ–å°‘çš„æ”¹è¿›ï¼Œå®é™…ä¸Šï¼Œä¸ lightBGM æˆ– DNN ä½œä¸ºç¬¬äºŒä¸ªæ¨¡å‹ç›¸æ¯”ï¼Œç»“æœå¹¶ä¸å¤ªå·®ã€‚**

**ç°åœ¨ï¼Œè®©æˆ‘ä»¬å»å…ƒæ ‡ç­¾ã€‚**

# **å…ƒæ ‡è®°**

**å› ä¸ºå…ƒæ ‡è®°å°†éœ€è¦å‘è¾“å…¥å’Œæ ‡è®°æ·»åŠ æ–°çš„ç‰¹å¾ã€‚æˆ‘å†™äº†å¦ä¸€ä¸ªå‡½æ•°æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚**

```
**def data_meta(id, x, y, model):
    #get prediction from model 1
    pred_prob_meta = model.predict_proba(x)[:,1]
    pred_prob_meta = pd.Series(pred_prob_meta, \
                               index=x.index,
                               name=f'pred_{id}_meta')
    pred_int_meta = pred_prob_meta > Threshold
    y_meta = pd.Series(y & pred_int_meta, name=f'y_train_meta_{id}')
    x_meta = x.join(pred_int_meta)

    return x_meta, y_meta pred_prob_meta = model.predict_proba(x)[:,1]
    pred_prob_meta = pd.Series(pred_prob_meta, \
                               index=x.index,
                               name=f'pred_{id}_meta')
    pred_int_meta = pred_prob_meta > Threshold
    y_meta = pd.Series(y & pred_int_meta, name=f'y_train_meta_{id}')
    x_meta = x.join(pred_int_meta)

    return x_meta, y_meta**
```

1.  ****ç¬¬ä¸€ä¸ªå‹å·:logregï¼Œç¬¬äºŒä¸ªå‹å·:lightBGM****

**å‡†å¤‡å¥½æ•°æ®åï¼Œå¯¹äºç¬¬ä¸€ä¸ªå®éªŒï¼Œæˆ‘å°†ä½¿ç”¨é€»è¾‘å›å½’ä½œä¸ºä¸»è¦æ¨¡å‹ï¼ŒlightBGM ä½œä¸ºæ¬¡è¦æ¨¡å‹ã€‚**

```
**x_train_meta_1, y_train_meta_1 = \
data_meta(1, x_train, y_train, model_1)
x_train_meta_1.shape, y_train_meta_1.shape**
```

**![](img/73fe684e8a3bc6c3b019290f5e9b8cd8.png)**

**å†æ¬¡ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥å…ƒæ•°æ®çš„ç›¸å…³æ€§ã€‚**

```
**plot_corr_xy(x_train_meta_1, y_train_meta_1);**
```

**![](img/e68962fe2fffc31ae19a009a533a85f1.png)****![](img/8de86d1b22706fb267deb6eb9ca2bc22.png)**

**æ ‡ç­¾å’Œæ·»åŠ çš„åŠŸèƒ½ä¹‹é—´çš„ç›¸å…³æ€§éå¸¸å¼º(ä»æ¨¡å‹ 1 é¢„æµ‹)ã€‚é«˜çš®å°”é€Šç›¸å…³ç³»æ•°è¡¨æ˜æ›´å¤šçš„ä¿¡æ¯ä»ç¬¬ä¸€æ¨¡å‹æ³„æ¼åˆ°ç¬¬äºŒæ¨¡å‹ä¸­ã€‚**

**ç°åœ¨ï¼Œå†ä¸€æ¬¡ï¼Œæˆ‘ä»¬éœ€è¦ç»å†æ‰€æœ‰é‚£äº›ä¹å‘³ä½†å¿…è¦çš„æ•°æ®å¤„ç†ã€‚**

```
**# test data
x_test_meta_1, y_test_meta_1 = \
data_meta(1, x_test_original, y_test_original, model_1)
x_test_meta_1.shape, y_test_meta_1.shape**
```

**![](img/2d53dfa25370e4657ecda39b16bf3daf.png)**

**ç„¶åæ ‡å‡†åŒ–æµ‹è¯•å’Œè®­ç»ƒæ•°æ®é›†ã€‚**

```
**x_train_meta_1, x_test_meta_1 = scale_data( \
                                x_train_meta_1, x_test_meta_1)**
```

**å¹¶æ‹†åˆ†è®­ç»ƒæ•°æ®é›†ä»¥å†æ¬¡è·å¾—éªŒè¯æ•°æ®ã€‚**

```
**x_train_meta_1_, x_cv_meta_1, y_train_meta_1_, y_cv_meta_1 = \
train_test_split(x_train_meta_1, y_train_meta_1,
                test_size=0.25,
                stratify=y_train_meta_1,
                random_state=2020)
#stratify mean samplling with the ratio of each class percentage in #all data.x_train_meta_1_.shape, x_cv_meta_1.shape, y_train_meta_1_.shape,  y_cv_meta_1.shape**
```

**![](img/7f7fdb45d268e498beaf1cc0100056d1.png)**

**åšå®Œè¿™äº›ï¼Œæˆ‘ä»¬ç»ˆäºå¯ä»¥å»çœ‹æ¨¡ç‰¹äº†ã€‚**

```
**model_2_meta_1 = build_model_2( \
    x_train_meta_1_, y_train_meta_1_, x_cv_meta_1, y_cv_meta_1)y_pred_prob_test_2_meta_1 = model_2_meta_1.predict(x_test_meta_1)
y_pred_int_test_2_meta_1 = y_pred_prob_test_2_meta_1 > Threshold
pd.DataFrame(y_pred_int_test_2_meta_1).value_counts()**
```

**![](img/ca557b38d2fd306813a1feb563d50db9.png)**

**åœ¨æˆ‘ä»¬æœ‰äº†å…ƒæ¨¡å‹çš„é¢„æµ‹ä¹‹åï¼Œæˆ‘ä»¬å°†ç»“æœä¸ä¸»æ¨¡å‹çš„é¢„æµ‹ç»“åˆèµ·æ¥ã€‚**

```
**final_pred_2_meta_1 = y_pred_int_test_2_meta_1 &  y_pred_int_test_1
pd.DataFrame(final_pred_2_meta_1).value_counts()**
```

**![](img/b0bd0dd3ae25229d0efa490b4584f4ad.png)**

**çœ‹èµ·æ¥æ²¡ä»€ä¹ˆåŒºåˆ«ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æ‰€æœ‰çš„æŒ‡æ ‡ã€‚**

```
**metrics_summary(y_test_original, final_pred_2_meta_1)**
```

**![](img/d38733d60d9d9bc079197211340cb60e.png)****![](img/f41d718a69879d0d97659b64c245c66a.png)**

**ç°åœ¨çš„ç»“æœåœ¨ç²¾ç¡®åº¦å’Œå¬å›ç‡ä¹‹é—´æ›´åŠ å¹³è¡¡äº†ã€‚ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ f1 åˆ†æ•°ä»¥åŠå…¶ä»–æŒ‡æ ‡åˆ†åˆ«ä» 0.93ã€0.77ã€0.84 æé«˜åˆ° 0.96ã€0.81ã€0.88ã€‚**

****2ã€‚ç¬¬ä¸€æ¬¾è½¦å‹:logregï¼Œç¬¬äºŒæ¬¾è½¦å‹:DNN****

**ç”±äºç¬¬ä¸€ä¸ªæ¨¡å‹ä»ç„¶æ˜¯é€»è¾‘å›å½’ï¼Œæ‰€ä»¥ä¸éœ€è¦æ›´æ–°æ•°æ®ã€‚æˆ‘ä»¬ç›´æ¥å»çœ‹æ¨¡å‹å§ã€‚**

```
**#if you receive an error message, try to run the data process again.
model_3_meta_1 = build_model_3( \
    x_train_meta_1_, y_train_meta_1_, \
    x_cv_meta_1, y_cv_meta_1, input_dim=30)y_pred_prob_test_3_meta_1 = model_3_meta_1.predict(x_test_meta_1)
y_pred_int_test_3_meta_1 = y_pred_prob_test_3_meta_1 > Thresholdpd.DataFrame(y_pred_int_test_3_meta_1).value_counts()**
```

**![](img/f7158f0db83deadc30addba9c03d7c60.png)**

**åŒæ ·ï¼Œæœ€ç»ˆç»“æœå°†æ˜¯ä¸»è¦é¢„æµ‹å’Œæ¬¡è¦æ¨¡å‹é¢„æµ‹çš„äº¤é›†ã€‚**

```
**# combine the  meta prediction with primary prediction
final_pred_3_meta_1 = y_pred_int_test_3_meta_1.flatten() & y_pred_int_test_1final_pred_3_meta_1.shape**
```

**![](img/2d4216ec725bb1fc794064135e2fb076.png)**

**å—¯ï¼Œæˆ‘çœŸçš„å¼€å§‹æ€€ç–‘æœ€åä¸€æ­¥çš„å¿…è¦æ€§äº†ã€‚**

**![](img/e734d6f0b2656b0caf1afad236243615.png)****![](img/2700e263e3e93358a4d07288fcf8a149.png)**

**å¥½å§ï¼Œç²¾ç¡®åº¦ç¡®å®æé«˜äº†ï¼Œä½†ä»£ä»·æ˜¯å¬å›ç‡é™ä½äº†ã€‚å°±åƒ DNN ä½œä¸ºç¬¬äºŒä¸ªæ¨¡å‹çš„å †å æ–¹æ³•ä¸€æ ·ï¼Œä½†æ˜¯ç¨å¾®å¥½ä¸€ç‚¹ã€‚**

**![](img/20e20e8f8161dfa8d9d9aaca7f93e1bb.png)**

**ä»¥ DNN ä¸ºç¬¬äºŒæ¨¡å‹çš„å…ƒæ ‡ç­¾**

**![](img/371a6f8cd93f9561db0f9bd2f1511836.png)**

**å°† DNN ä½œä¸ºç¬¬äºŒä¸ªæ¨¡å‹è¿›è¡Œå †å **

****3ã€‚ç¬¬ä¸€æ¬¾:logreg + lightBGMï¼Œç¬¬äºŒæ¬¾:DNN****

**å †å æ–¹æ³•çš„å·¥ä½œæ–¹å¼è®©æˆ‘æƒ³çŸ¥é“ï¼Œå¦‚æœæˆ‘å°†æ¨¡å‹ 1 å’Œæ¨¡å‹ 2 éƒ½ä½œä¸ºä¸»è¦æ¨¡å‹ï¼Œè€Œå°†æ¨¡å‹ 3 ä½œä¸ºæœ€ç»ˆæ¨¡å‹ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿè¿™ä¼šæ”¹å–„æœ€ç»ˆç»“æœå—ï¼Ÿè®©æˆ‘ä»¬è¿™æ ·è¯•ä¸€è¯•ã€‚**

**å› ä¸ºè¿™ä¸€æ¬¡æˆ‘ä»¬å°†æœ‰é¢å¤–çš„åŠŸèƒ½éœ€è¦æ·»åŠ åˆ°è¾“å…¥ä¸­ï¼Œæ‰€ä»¥æˆ‘é‡æ–°ç¼–å†™äº†æ•°æ®å¤„ç†å‡½æ•°ã€‚**

```
**def data_meta_2(id, x, y, m_1, m_2):
    '''
    id: the id of new columns
    x: input features
    y: labels
    m_1: model 1, here logreg
    m_2: model 2
    '''
    pred_prob_meta_1 = m_1.predict_proba(x)[:,1]
    pred_prob_meta_1 = pd.Series(pred_prob_meta_1, \
                                 index=x.index,
                                 name=f'pred_{id}_meta')
    pred_int_meta_1 = pred_prob_meta_1 > Threshold

    pred_prob_meta_2 = m_2.predict(x)
    #as DNN give 2D prediction that needs to be flatten to 1D for
    #combination
    pred_prob_meta_2 = pd.Series(pred_prob_meta_2.flatten(), \
                                 index=x.index,
                                 name=f'pred_{id+1}_meta')
    pred_int_meta_2 = pred_prob_meta_2 > Threshold

    y_meta = pd.Series(y & pred_int_meta_1 & pred_int_meta_2, \
                       name=f'y_train_meta_{id}')
    x_meta = x.join(pred_int_meta_1).join(pred_int_meta_2)

    return x_meta, y_meta**
```

**ç„¶åï¼Œæˆ‘ä»¬å°†è¯¥å‡½æ•°åº”ç”¨äºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ã€‚**

```
**#meta_1_2: meta data from 1 model and 2 model
x_train_meta_1_2, y_train_meta_1_2 = \
data_meta_2(1, x_train, y_train, model_1, model_2)x_test_meta_1_2, y_test_meta_1_2 = \
data_meta_2(1, x_test_original, y_test_original, model_1, model_2)**
```

**å¹¶å†æ¬¡è¿›è¡Œå½’ä¸€åŒ–ã€‚**

```
**x_train_meta_1_2, x_test_meta_1_2 = \
scale_data(x_train_meta_1_2, x_test_meta_1_2)**
```

**å¹¶åˆ†å‰²è®­ç»ƒæ•°æ®ä»¥ç»™å‡ºéªŒè¯æ•°æ®é›†ã€‚**

```
**x_train_meta_1_2_, x_cv_meta_1_2, y_train_meta_1_2_, y_cv_meta_1_2 = \
train_test_split(x_train_meta_1_2, y_train_meta_1_2,
                test_size=0.25,
                stratify=y_train_meta_1_2,
                random_state=2020)
#stratify mean samplling with the ratio of each class percentage in #all data.x_train_meta_1_2_.shape, x_cv_meta_1_2.shape, \
y_train_meta_1_2_.shape,  y_cv_meta_1_2.shape**
```

**![](img/9760080c1773939d1d0026d123a81893.png)**

**å¥½ï¼Œè®©æˆ‘ä»¬åœ¨æ¨¡å‹ 3 ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ•°æ®ã€‚**

```
**model_3_meta_1_2 = build_model_3( \
    x_train_meta_1_2_, y_train_meta_1_2_, \
    x_cv_meta_1_2, y_cv_meta_1_2, input_dim=31)y_pred_prob_test_3_meta_1_2 = model_3_meta_1_2.predict(x_test_meta_1_2)
y_pred_int_test_3_meta_1_2 = y_pred_prob_test_3_meta_1_2 > Thresholdpd.DataFrame(y_pred_int_test_3_meta_1_2).value_counts()**
```

**![](img/7c6e368be04bcefccd278c452245956c.png)**

```
**# combine the  meta prediction with primary prediction
final_pred_3_meta_1_2 = \
y_pred_int_test_3_meta_1_2.flatten() & \
y_pred_int_test_1 & y_pred_int_test_2
pd.Series(final_pred_3_meta_1_2).value_counts()**
```

**![](img/7c6e368be04bcefccd278c452245956c.png)**

**å¾ˆæœ‰å¯èƒ½ï¼Œæœ€åä¸€æ­¥ä¸ä¼šæ”¹å˜è¿™ä¸ªé«˜åº¦ä¸å¹³è¡¡çš„æ•°æ®é›†ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æœ€åçš„åˆ†æ•°ã€‚**

```
**metrics_summary(y_test_original, y_pred_int_test_3_meta_1_2)**
```

**![](img/578835d9b28659ea86ce89f34d26c7fa.png)****![](img/a22529b43d94b712f073ff88909bf643.png)**

**å°† DNN ä½œä¸ºç¬¬äºŒä¸ªæ¨¡å‹æˆ–ç‹¬ç«‹çš„ DNN ä¸å åŠ æ³•ç›¸æ¯”ï¼Œæ‰€æœ‰æŒ‡æ ‡éƒ½æ›´å¥½ã€‚ä½†ä¸å¦‚ç¬¬äºŒä»£çš„ lightBGMã€‚**

****4ã€‚ç¬¬ä¸€æ¬¾:logreg + DNNï¼Œç¬¬äºŒæ¬¾:lightBGM****

**ç”±äº lightBGM ä½œä¸ºç¬¬äºŒä¸ªæ¨¡å‹ä¼¼ä¹æ¯”å…¶ä»–æ¨¡å‹æ›´å¥½ï¼Œè¿™æ˜¯æˆ‘æœ€ä¸æƒ³å°è¯•çš„ç»„åˆã€‚æˆ‘ä»¬éœ€è¦å†æ¬¡é‡æ–°å¤„ç†æ•°æ®é›†ã€‚**

```
**#meta_1_3: meta data from 1 model and 2 model 
#process the train dataset
x_train_meta_1_3, y_train_meta_1_3 = \
data_meta_2(1, x_train, y_train, model_1, model_3)#meta_1_3: meta data from 1st model and 3rd model 
#process the test dataset
x_test_meta_1_3, y_test_meta_1_3 = \
data_meta_2(1, x_test_original, y_test_original, model_1, model_3)#normalize the dataset
x_train_meta_1_3, x_test_meta_1_3 = \
scale_data(x_train_meta_1_3, x_test_meta_1_3)#do a train, validation split
x_train_meta_1_3_, x_cv_meta_1_3, y_train_meta_1_3_, y_cv_meta_1_3 = \
train_test_split(x_train_meta_1_3, y_train_meta_1_3,
                test_size=0.25,
                stratify=y_train_meta_1_3,
                random_state=2020)**
```

**ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚**

```
**model_2_meta_1_3 = build_model_2( \
    x_train_meta_1_3_, y_train_meta_1_3_, \
    x_cv_meta_1_3, y_cv_meta_1_3)**
```

**å¹¶é¢„æµ‹æµ‹è¯•æ•°æ®ã€‚**

```
**y_pred_prob_test_2_meta_1_3 = model_2_meta_1_3.predict(x_test_meta_1_3)
y_pred_int_test_2_meta_1_3 = y_pred_prob_test_2_meta_1_3 > Threshold
# combine the  meta prediction with primary prediction
final_pred_2_meta_1_3 = \
y_pred_int_test_2_meta_1_3 & \
y_pred_int_test_1 & y_pred_int_test_3.flatten()
pd.Series(final_pred_2_meta_1_3).value_counts(**
```

**![](img/f8402a9fb8e79ed090506eae4cd47ee3.png)**

```
**metrics_summary(y_test_original, final_pred_2_meta_1_3)**
```

**![](img/e5ae356dfcdc073ab53309ce8394b78f.png)****![](img/e18027097830b9a52b1969cdd230a7fa.png)**

**ç»“æœæ¯”ä¸Šä¸€ä¸ªå¥½ã€‚**

**![](img/72d0e118f927a3268460d6bfeb847aba.png)**

**ä½¿ç”¨ LightBGM ä½œä¸ºç¬¬äºŒä¸ªæ¨¡å‹ logreg + DNN ç¬¬ä¸€ä¸ªæ¨¡å‹çš„å…ƒæ ‡ç­¾**

**![](img/dac4ee99c9ace42e2475bb8a4dcce36a.png)**

**å°† DNN ä½œä¸ºç¬¬äºŒæ¨¡å‹ logreg + BGM ç¬¬ä¸€æ¨¡å‹çš„å…ƒæ ‡ç­¾**

# **ç‰¹å¾é‡è¦æ€§**

**æ­£å¦‚æˆ‘ä»¬ç°åœ¨æ‰€çŸ¥ï¼Œå †å å’Œå…ƒæ ‡è®°æœ‰ç‚¹åƒå‘è®­ç»ƒæ•°æ®æ·»åŠ é¢å¤–ç‰¹å¾çš„ç‰¹å¾å·¥ç¨‹æ–¹æ³•ã€‚ä½†æ˜¯è¿™äº›å¢åŠ çš„æ–°åŠŸèƒ½ä¸åŸæ¥çš„åŠŸèƒ½ç›¸æ¯”æœ‰å¤šé‡è¦ã€‚æ„Ÿè°¢ scikit-learn ä¸­çš„`feature_importance()`å‡½æ•°ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å®ç°è¿™ä¸ªå‡½æ•°æ¥äº†è§£è¿™äº›ç‰¹æ€§çš„é‡è¦æ€§ã€‚**

```
**def plot_feature_importance(model, X , importance_type = 'split'):
    feature_imp = pd.DataFrame({'Value':model.
                               feature_importance(importance_type),
                               'Feature':X.columns})
    f, ax = plt.subplots(figsize=(40, 30))
    ax.set_title(f'LightGBM Features Importance by \
                 {importance_type}', fontsize=75, fontname="Arial")     
    ax.set_xlabel('Features', fontname="Arial", fontsize=70)
    ax.set_ylabel('Importance', fontname="Arial", fontsize=70)  
    ax.tick_params(labelsize=50)

    sns.barplot(x="Value", y="Feature",
                data=feature_imp.sort_values(by="Value", 
                ascending=False), ax=ax)**
```

**æ ¹æ®å®šä¹‰ï¼Œæœ‰ä¸¤ç§é‡è¦ç±»å‹ï¼Œâ€œåˆ†å‰²â€å’Œâ€œè·å¾—â€ã€‚å¦‚æœä¸ºâ€œåˆ†å‰²â€ï¼Œç»“æœå°†åŒ…å«è¯¥ç‰¹å¾åœ¨æ¨¡å‹ä¸­ä½¿ç”¨çš„æ¬¡æ•°ã€‚å¦‚æœä¸ºâ€œå¢ç›Šâ€ï¼Œåˆ™ç»“æœåŒ…å«ä½¿ç”¨è¯¥åŠŸèƒ½çš„æ‹†åˆ†çš„æ€»å¢ç›Šã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä»–ä»¬ä¸¤ä¸ªã€‚**

```
**plot_feature_importance(model_2_meta_1_3, x_train_meta_1_3_)**
```

**![](img/0f81f52055263942e02ba85ab0faebd9.png)**

```
**plot_feature_importance(model_2_meta_1_3, x_train_meta_1_3_, 'gain')**
```

**![](img/0075b1aeb2b20dffaf9cb16a12d3c4cd.png)**

**ä¸¤ç§é‡è¦æ€§ç±»å‹éƒ½è¡¨æ˜å…ƒæ•°æ®è¿œæ¯”åŸå§‹ç‰¹å¾é‡è¦ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒåŒæ ·çš„ç»“æœä¹Ÿé€‚ç”¨äºæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹ã€‚**

```
**plot_feature_importance(model_2_meta_1, x_train_meta_1_)**
```

**![](img/bb43d4b056d90070f5315a84683f5bbf.png)**

```
**plot_feature_importance(model_2_meta_1, x_train_meta_1_, 'gain')**
```

**![](img/ca082ca104523e276722c4bef1652e09.png)**

# **é«˜åˆ†çš„å‡è±¡**

**æ ¹æ®ç‰¹å¾é‡è¦æ€§å€¼ï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œä»ç‰¹å¾åœ¨æ¨¡å‹ä¸­ä½¿ç”¨çš„æ¬¡æ•°(â€œåˆ†å‰²â€)å’Œä½¿ç”¨è¯¥ç‰¹å¾çš„åˆ†å‰²çš„æ€»å¢ç›Š(â€œå¢ç›Šâ€)çš„è§’åº¦æ¥çœ‹ï¼Œä¸»æ¨¡å‹çš„é¢„æµ‹å¯¹ç¬¬äºŒä¸ªæ¨¡å‹çš„ç»“æœå½±å“æœ€å¤§ã€‚æˆ‘ä»¬è¿˜çŸ¥é“ï¼Œä»ç›¸å…³å›¾æ¥çœ‹ï¼Œä¸»è¦æ¨¡å‹çš„é¢„æµ‹ä¸æ ‡ç­¾(â€œç±»åˆ«â€)å…·æœ‰éå¸¸é«˜çš„ç›¸å…³æ€§(~0.9)ã€‚**

**æ­¤å¤–ï¼Œæˆ‘ä»¬çŸ¥é“å°†ä¼šæœ‰ä¿¡æ¯ä»ä¸»æ¨¡å‹æ³„æ¼åˆ°ç¬¬äºŒæ¨¡å‹ï¼Œå°½ç®¡è¿™æ˜¯ä¼˜é€‰çš„ã€‚**

**å› æ­¤ï¼Œå¦‚æœä»æµ‹è¯•æ•°æ®åˆ°è®­ç»ƒæ•°æ®æœ‰è½»å¾®çš„æ³„æ¼ï¼Œæ³„æ¼çš„ä¿¡æ¯å°†é€šè¿‡ä¸Šè¿°æ–¹å¼è¢«æ”¾å¤§ã€‚åƒ DNN è¿™æ ·çš„æ¨¡ç‰¹çœŸçš„å¾ˆæ“…é•¿èµ°æ·å¾„ï¼ŒæŒ‘é€‰ä¿¡æ¯ï¼Œç»™é«˜åˆ†ã€‚**

**è®©æˆ‘ç”¨ä»£ç å±•ç¤ºç»™ä½ çœ‹ã€‚**

```
**#normalize all the data in one go.
scaler = pp.StandardScaler()
data_x.loc[:, features_to_scale] = scaler.fit_transform(data_x[features_to_scale])#split training and testing dataset afterwards.
x_train_cv, x_test, y_train_cv, y_test = train_test_split(data_x, data_y, test_size=0.25,stratify=data_y,random_state=2020)**
```

**å¦‚æœåœ¨å°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†ä¹‹å‰ä¸€æ¬¡æ€§å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼Œåˆ™ä½¿ç”¨çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®æ¥è‡ªè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ã€‚æ¥è‡ªæµ‹è¯•æ•°æ®é›†çš„ä¸€äº›ä¿¡æ¯å°†ä¸è®­ç»ƒæ•°æ®é›†å…±äº«ï¼Œç„¶åé€šè¿‡å…ƒæ ‡è®°æ”¾å¤§ã€‚**

**ä½¿ç”¨é€»è¾‘å›å½’å’Œ DNN ä½œä¸ºä¸»è¦æ¨¡å‹ï¼Œä½¿ç”¨ lightBGM ä½œä¸ºç¬¬äºŒæ¨¡å‹ï¼Œä»¥ä¸‹æ˜¯æµ‹è¯•æ•°æ®çš„å¾—åˆ†ã€‚**

**![](img/8b4718b59f118c598a2ce958aa54897c.png)****![](img/57fd2dc694b10026313f589f262391ff.png)**

**è¯·æ³¨æ„ï¼Œè¿™äº›åˆ†æ•°æ˜¯ç”±æ²¡æœ‰ä¼˜åŒ–å’Œé»˜è®¤è®¾ç½®çš„åŸºæœ¬æ¨¡å‹è·å¾—çš„ã€‚åœ¨æ•°å­—ä¸Šçœ‹èµ·æ¥ä¸é”™ï¼Œä½†å¯¹æœªçŸ¥æ•°æ®æ²¡ç”¨ã€‚**

# **æ‘˜è¦**

**æ€»ä¹‹ï¼Œæœ€å¥½çš„ç»“æœæ˜¯ä½¿ç”¨å…ƒæ ‡è®°ï¼ŒlightBGM ä½œä¸ºç¬¬äºŒæ¨¡å‹ï¼Œlogistic å›å½’ä½œä¸ºä¸»è¦æ¨¡å‹ã€‚**

**![](img/cc26351ae297195e9adf5774c2f4236e.png)**

**å°† BGM ä½œä¸ºç¬¬äºŒæ¨¡å‹ã€logreg ä½œä¸ºç¬¬ä¸€æ¨¡å‹çš„å…ƒæ ‡ç­¾**

**ç¨åŠ åŠªåŠ›ï¼Œæˆ‘ä»¬å¯ä»¥å–å¾—æ›´å¥½çš„æˆç»©ã€‚é¢å¤–çš„æé«˜å¯èƒ½çœ‹èµ·æ¥å¹¶ä¸å¼•äººæ³¨ç›®ï¼Œä½†åœ¨ä¸€äº›æ¯”èµ›ä¸­ï¼Œå½“ç¬¬ä¸€åå’Œç¬¬äºŒåçš„åˆ†æ•°å¦‚æ­¤æ¥è¿‘æ—¶ï¼Œè¿™å°±æ˜¯è¾“èµ¢çš„äº¤æ˜“ã€‚**

**æˆ‘æƒ³ç”¨æˆ‘æœ€å–œæ¬¢çš„å…³äº**æŒ‡æ ‡**çš„ä¸¤æ¡å®šå¾‹æ¥ç»“æŸè¿™ç¯‡æ–‡ç« :**

**"å½“ä¸€ä¸ªåº¦é‡æˆä¸ºç›®æ ‡æ—¶ï¼Œå®ƒå°±ä¸å†æ˜¯ä¸€ä¸ªå¥½çš„åº¦é‡."**

**â€”â€”[**å¤å¾·å“ˆç‰¹å®šå¾‹**](https://en.wikipedia.org/wiki/Goodhart%27s_law)**

**â€œç¤¾ä¼šå†³ç­–ä¸­ä½¿ç”¨çš„é‡åŒ–ç¤¾ä¼šæŒ‡æ ‡è¶Šå¤šï¼Œå°±è¶Šå®¹æ˜“å—åˆ°è…è´¥çš„å‹åŠ›â€**

**â€”â€”[**åè´å°”å®šå¾‹**](https://en.wikipedia.org/wiki/Campbell%27s_law)**

# **å‚è€ƒ**

1.  **[æ´›ä½©å…¹Â·å¾·Â·æ™®æ‹‰å¤šçš„ã€Šé‡‘èæœºå™¨å­¦ä¹ çš„è¿›å±•ã€‹ç®€ä»‹](https://www.quantopian.com/posts/introduction-to-advances-in-financial-machine-learning-by-lopez-de-prado)**

**2.[è¡¡é‡ä¸šç»©:AUC (AUROC)](https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/)**

**3.[ç›—ç‰ˆè€…çš„å‡†ç¡®åº¦ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œå…¶ä»–åˆ†æ•°æŒ‡å—](https://blog.floydhub.com/a-pirates-guide-to-accuracy-precision-recall-and-other-scores/)**

**4.[å¹³å‡ç²¾åº¦](https://sanchom.wordpress.com/tag/average-precision/)**

**5.[å¦‚ä½•ç”¨ Python è®¡ç®—ç‰¹å¾é‡è¦æ€§](https://machinelearningmastery.com/calculate-feature-importance-with-python/#:~:text=Feature%20importance%20refers%20to%20techniques,at%20predicting%20a%20target%20variable.&text=The%20role%20of%20feature%20importance%20in%20a%20predictive%20modeling%20problem.)**