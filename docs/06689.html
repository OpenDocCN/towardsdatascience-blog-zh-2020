<html>
<head>
<title>How and why to Standardize your data: A python tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何以及为什么标准化数据:python 教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-and-why-to-standardize-your-data-996926c2c832?source=collection_archive---------6-----------------------#2020-05-26">https://towardsdatascience.com/how-and-why-to-standardize-your-data-996926c2c832?source=collection_archive---------6-----------------------#2020-05-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bb3b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这篇文章中，我将解释为什么以及如何在 Python 中使用 scikit-learn 来应用标准化</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/92e991114e3246a8ce1025c782d09e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPLv3kNw9JTtNabr70dQDQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图摘自:<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/preprocessing/plot _ all _ scaling . html</a>。左侧子图:未缩放的数据。右侧子情节:转换后的数据。</p></figure><p id="174c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你好。</p><p id="27e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我的第一篇中帖。我是一名电子和计算机工程师，目前正在完成生物医学工程和计算神经科学领域的博士研究。在过去的 4 年里，我一直在研究机器学习问题。我在网络上看到的一个非常普遍的问题是，在拟合机器学习模型之前，如何标准化数据以及为什么要这样做。</p><blockquote class="lv"><p id="b3c9" class="lw lx it bd ly lz ma mb mc md me lu dk translated">scikit-learn 的<code class="fe mf mg mh mi b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">StandardScaler</a></code>是如何工作的？</p></blockquote><p id="10a3" class="pw-post-body-paragraph kz la it lb b lc mj ju le lf mk jx lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">一个人想到的第一个问题是:</p><blockquote class="lv"><p id="cb46" class="lw lx it bd ly lz ma mb mc md me lu dk translated"><strong class="ak"> <em class="mo">为什么首先要标准化？</em> </strong></p></blockquote><h1 id="b5a5" class="mp mq it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated"><strong class="ak">为什么要在拟合 ML 模型之前进行标准化？</strong></h1><p id="46bc" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">嗯，想法就是<strong class="lb iu"> <em class="nm">简单的</em> </strong>。在不同尺度下测量的变量对模型拟合&amp;模型学习函数的贡献不同，最终可能产生偏差。因此，为了处理这个潜在的问题，通常在模型拟合之前使用特征标准化(μ=0，σ=1)。</p><p id="def1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要使用<code class="fe mf mg mh mi b">scikit-learn</code>来实现这一点，我们首先需要构建一个包含特性和样本的输入数组<code class="fe mf mg mh mi b">X</code>，其中<code class="fe mf mg mh mi b">X.shape</code>为<code class="fe mf mg mh mi b">[number_of_samples, number_of_features]</code>。</p><p id="8302" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，所有的<code class="fe mf mg mh mi b">scikit-learn</code>机器学习(ML)功能都期望输入一个具有该形状的<code class="fe mf mg mh mi b">numpy</code>数组<code class="fe mf mg mh mi b">X</code>，即行是样本，列是特征/变量。说到这里，让我们假设我们有一个矩阵<code class="fe mf mg mh mi b">X</code>，其中每个<strong class="lb iu">行/线</strong>是一个<strong class="lb iu">样本/观察值</strong>，每个<strong class="lb iu">列</strong>是一个<strong class="lb iu">变量/特征。</strong></p><p id="6673" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注</strong> : <em class="nm">基于树的模型通常不依赖于缩放，但非树模型模型如 SVM、LDA 等。往往非常依赖它。</em></p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="6247" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想在交互式路线图和活跃的学习社区的支持下自学数据科学，看看这个资源:<a class="ae ky" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="512e" class="mp mq it bd mr ms nu mu mv mw nv my mz jz nw ka nb kc nx kd nd kf ny kg nf ng bi translated"><strong class="ak">方法核心</strong></h1><p id="c962" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">主要思想是在应用任何机器学习模型之前，<strong class="lb iu">将</strong>即<code class="fe mf mg mh mi b">μ = 0</code>和<code class="fe mf mg mh mi b">σ = 1</code>标准化/规范化<code class="fe mf mg mh mi b">X</code>、<em class="nm">的<strong class="lb iu">特征/变量/列</strong>、</em>、<strong class="lb iu">。因此，<code class="fe mf mg mh mi b">StandardScaler()</code>将<strong class="lb iu">归一化特征</strong>，即 X，<strong class="lb iu"> <em class="nm">的每一列单独</em> </strong>，这样每一列/特征/变量将有<code class="fe mf mg mh mi b">μ = 0</code>和<code class="fe mf mg mh mi b">σ = 1</code>。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/89aab6e76b81b372bf9431e215ff4889.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*YSAAU_v--I8OlHQzG5A1Sg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标准化程序的数学公式。图片由作者生成。</p></figure><h1 id="1106" class="mp mq it bd mr ms mt mu mv mw mx my mz jz oa ka nb kc ob kd nd kf oc kg nf ng bi translated"><strong class="ak">工作 Python 代码示例:</strong></h1><pre class="kj kk kl km gt od mi oe of aw og bi"><span id="a723" class="oh mq it mi b gy oi oj l ok ol">from sklearn.preprocessing import StandardScaler<br/>import numpy as np<br/><br/># 4 samples/observations and 2 variables/features<br/>X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])</span><span id="fd33" class="oh mq it mi b gy om oj l ok ol"># the scaler object (model)<br/>scaler = StandardScaler()</span><span id="effc" class="oh mq it mi b gy om oj l ok ol"># fit and transform the data<br/>scaled_data = scaler.fit_transform(X) <br/><br/>print(X)<br/>[[0, 0],<br/> [1, 0],<br/> [0, 1],<br/> [1, 1]])<br/><br/>print(scaled_data)<br/>[[-1. -1.]<br/> [ 1. -1.]<br/> [-1.  1.]<br/> [ 1.  1.]]</span></pre><p id="d7bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">验证每个特征(列)的平均值为 0: </strong></p><pre class="kj kk kl km gt od mi oe of aw og bi"><span id="880a" class="oh mq it mi b gy oi oj l ok ol">scaled_data.mean(axis = 0)<br/>array([0., 0.])</span></pre><p id="2d77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">验证每个特征(列)的标准差为 1: </strong></p><pre class="kj kk kl km gt od mi oe of aw og bi"><span id="8d72" class="oh mq it mi b gy oi oj l ok ol">scaled_data.std(axis = 0)<br/>array([1., 1.])</span></pre><h1 id="5462" class="mp mq it bd mr ms mt mu mv mw mx my mz jz oa ka nb kc ob kd nd kf oc kg nf ng bi translated"><strong class="ak">视觉示例中变换的效果</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/92e991114e3246a8ce1025c782d09e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPLv3kNw9JTtNabr70dQDQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图摘自:<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/preprocessing/plot _ all _ scaling . html</a>。左侧子图:未缩放的数据。右侧子情节:转换后的数据。</p></figure><h1 id="1d47" class="mp mq it bd mr ms mt mu mv mw mx my mz jz oa ka nb kc ob kd nd kf oc kg nf ng bi translated">摘要</h1><ul class=""><li id="8f68" class="on oo it lb b lc nh lf ni li op lm oq lq or lu os ot ou ov bi translated"><code class="fe mf mg mh mi b">StandardScaler</code>移除平均值，并将每个特征/变量缩放至单位方差。该操作以独立于<strong class="lb iu">的方式</strong>按特征<strong class="lb iu">执行。</strong></li><li id="0cc5" class="on oo it lb b lc ow lf ox li oy lm oz lq pa lu os ot ou ov bi translated"><code class="fe mf mg mh mi b">StandardScaler</code>可能受到<strong class="lb iu">异常值</strong>(如果它们存在于数据集中)的影响，因为它涉及每个特征的经验平均值和标准偏差的估计。</li></ul><h1 id="9cbe" class="mp mq it bd mr ms mt mu mv mw mx my mz jz oa ka nb kc ob kd nd kf oc kg nf ng bi translated">如何处理异常值</h1><ul class=""><li id="6b13" class="on oo it lb b lc nh lf ni li op lm oq lq or lu os ot ou ov bi translated">手动方式(不推荐):目视检查数据，并使用异常值剔除统计方法(如四分位距(IQR)阈值法)剔除异常值。</li><li id="ccc9" class="on oo it lb b lc ow lf ox li oy lm oz lq pa lu os ot ou ov bi translated">推荐方法:使用<code class="fe mf mg mh mi b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html" rel="noopener ugc nofollow" target="_blank">RobustScaler</a></code>来缩放特征，但是在这种情况下，使用对异常值稳健的<strong class="lb iu">统计。该缩放器根据<strong class="lb iu">分位数</strong> <strong class="lb iu">范围</strong>(默认为<strong class="lb iu"> IQR </strong>:四分位数间范围)移除<strong class="lb iu">中值</strong>和<strong class="lb iu">缩放</strong>数据。<em class="nm">IQR 是第一个四分位数(第 25 个四分位数)和第三个四分位数(第 75 个四分位数)之间的范围。</em></strong></li></ul><p id="3d62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天就到这里吧！希望你喜欢这第一个帖子！下一个故事下周开始。敬请关注&amp;注意安全。</p><h1 id="ec6b" class="mp mq it bd mr ms mt mu mv mw mx my mz jz oa ka nb kc ob kd nd kf oc kg nf ng bi translated">请继续关注并支持我</h1><p id="b10b" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">如果你喜欢这篇文章并觉得它有用，请关注我，为我的故事鼓掌支持我！</p><h2 id="55f0" class="oh mq it bd mr pb pc dn mv pd pe dp mz li pf pg nb lm ph pi nd lq pj pk nf pl bi translated">-我的邮件列表只需 5 秒:<a class="ae ky" href="https://seralouk.medium.com/subscribe" rel="noopener">https://seralouk.medium.com/subscribe</a></h2><h2 id="d38d" class="oh mq it bd mr pb pc dn mv pd pe dp mz li pf pg nb lm ph pi nd lq pj pk nf pl bi translated">-成为会员支持我:【https://seralouk.medium.com/membershipT21】</h2><h1 id="bc7e" class="mp mq it bd mr ms mt mu mv mw mx my mz jz oa ka nb kc ob kd nd kf oc kg nf ng bi translated"><strong class="ak">参考文献</strong></h1><p id="049f" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">[1]<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . preprocessing . standard scaler . html</a></p><p id="4f30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . preprocessing . robust scaler . html</a></p><h1 id="ed51" class="mp mq it bd mr ms mt mu mv mw mx my mz jz oa ka nb kc ob kd nd kf oc kg nf ng bi translated">和我联系</h1><ul class=""><li id="8982" class="on oo it lb b lc nh lf ni li op lm oq lq or lu os ot ou ov bi translated"><strong class="lb iu">领英</strong>:<a class="ae ky" href="https://www.linkedin.com/in/serafeim-loukas/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/serafeim-loukas/</a></li><li id="d18c" class="on oo it lb b lc ow lf ox li oy lm oz lq pa lu os ot ou ov bi translated"><strong class="lb iu">研究之门</strong>:<a class="ae ky" href="https://www.researchgate.net/profile/Serafeim_Loukas" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/profile/Serafeim_Loukas</a></li><li id="6193" class="on oo it lb b lc ow lf ox li oy lm oz lq pa lu os ot ou ov bi translated"><strong class="lb iu">https://people.epfl.ch/serafeim.loukasEPFL 简介</strong> : <a class="ae ky" href="https://people.epfl.ch/serafeim.loukas" rel="noopener ugc nofollow" target="_blank"/></li><li id="2df7" class="on oo it lb b lc ow lf ox li oy lm oz lq pa lu os ot ou ov bi translated"><strong class="lb iu">堆栈溢出【https://stackoverflow.com/users/5025009/seralouk】:<a class="ae ky" href="https://stackoverflow.com/users/5025009/seralouk" rel="noopener ugc nofollow" target="_blank"/></strong></li></ul></div></div>    
</body>
</html>