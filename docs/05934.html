<html>
<head>
<title>Understanding and using k-Nearest Neighbours aka kNN for classification of digits</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解和使用k-最近邻(kNN)进行数字分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-and-using-k-nearest-neighbours-aka-knn-for-classification-of-digits-a55e00cc746f?source=collection_archive---------27-----------------------#2020-05-15">https://towardsdatascience.com/understanding-and-using-k-nearest-neighbours-aka-knn-for-classification-of-digits-a55e00cc746f?source=collection_archive---------27-----------------------#2020-05-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="b188" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">什么是分类？</strong></h1><p id="8940" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在机器学习和统计学中，分类是使用新观察的特征/变量来发现新观察的类别的任务。这种分类是使用基于训练数据训练的分类器来完成的。训练数据是许多观察值的集合，这些观察值用适当的类名正确标记。监督学习涉及用正确标记的观察值进行训练，因此分类被认为是监督学习。</p><p id="64bb" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">分类示例:</p><ul class=""><li id="203a" class="lo lp iq kn b ko lj ks lk kw lq la lr le ls li lt lu lv lw bi translated">使用电子邮件的文字、图像和附件等电子邮件特征将电子邮件标记为“垃圾邮件”或非垃圾邮件。</li><li id="3c6b" class="lo lp iq kn b ko lx ks ly kw lz la ma le mb li lt lu lv lw bi translated">使用患者的特征，如性别、年龄、体重、血压和观察到的症状，将患者标记为“健康”或“患病”。</li></ul><p id="8cc8" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">有许多模型可以用于机器学习中的分类任务，但我们将使用k近邻，因为它使用简单但功能强大。</p><h1 id="5950" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak"> k近邻:</strong></h1><p id="0f52" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这是一种基于新数据点与其他数据点组的接近程度对其进行分类的算法。来自一个组的新数据点的接近度越高，它被分类到该组的可能性就越高。</p><p id="881d" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">数据点之间的距离通过距离度量来测量，如欧几里德距离、曼哈顿距离、闵可夫斯基距离、马哈拉诺比斯距离、切向距离、余弦距离等等。</p><p id="d141" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">对于具有n个特征的数据点X和Y:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/88f0cba114ea9455cd014316b6e2cff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*b9Wvqk2W5uW2elbekgyEcQ.png"/></div></figure><p id="5a9e" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">闵可夫斯基距离公式是:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/dc521dfa3e3e88dad1847caab2acd2b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*8sOkQMotWDXxBB9UwaBxog.png"/></div></figure><p id="3af1" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">闵可夫斯基距离当p = 1时是曼哈顿距离，当p =2时是欧几里德距离，当p = ∞时是切比雪夫距离。闵可夫斯基距离是欧几里得距离的推广形式。</p><p id="ad79" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">使用距离度量，我们创建新数据点的n个最近邻居的邻域。</p><p id="b532" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">为了获得新数据点的类别，我们查看在所创建的邻域中具有更多数据点的类别组，以及与邻域中的其他组相比更接近我们的新数据点的类别组。基于这两个因素，我们确定新数据点的类别。</p><p id="bea8" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">让我们通过一个例子来更好地理解这一点。</p><p id="34cd" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">举例:</strong></p><p id="1380" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">考虑将顾客分为快乐和不快乐两类的任务。您可能希望事先知道哪些客户不满意，以便通过提供折扣来防止他们转向竞争对手的服务。</p><p id="9c44" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">在图中，红色代表不满意的客户，他们已经转向另一家竞争对手，而蓝色代表对我们满意的客户，他们仍在使用我们的服务。</p><p id="6362" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">现在，我们有一个用绿色圆圈表示的新客户，我们想知道他对我们使用kNN算法的服务是满意还是不满意。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/fd5624c4c29dcd430e96f183f87cd281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*WaMQr5uCRehP9MaqInc0lA.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">蓝色:快乐的顾客，红色:悲伤的顾客，绿色:新顾客</p></figure><p id="7c52" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">如果我们使用3个邻居，并且我们对每个数据点使用相等的权重，那么我们在邻居中有2个红色点和1个蓝色点，绿色点，即新的数据点被分类为红色。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/71aa3ee3eb8ead037aff86c92c5e0cde.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*EhHDt-TI0p75-E7BJ5E6HQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">具有统一权重的3-最近邻示例</p></figure><p id="c895" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">如果我们使用5个邻居，并且我们对每个数据点使用相等的权重，那么我们在邻居中有3个蓝点和2个红点，并且绿点被分类为蓝色。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/6d33d51c8f899921b707583ce27cb0ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*jFFFj3q4P-Yp2128Z_nRDA.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">具有统一权重的3-最近邻示例</p></figure><p id="638e" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">如果我们使用5个邻居，并且我们使用欧几里德距离来计算每个数据点的权重，那么我们在邻居中有3个蓝点和2个红点。数据点之间的欧几里德距离用线表示。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/f446e8d2012de431bd9344e458e618d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*sJ_688uL1xFybueOSDlR-w.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">使用欧几里德距离度量的权重的5-最近邻示例</p></figure><p id="b34f" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">为了使用欧几里得距离计算权重，我们将取距离的倒数，以便更近的点具有更高的权重。对于每个类，我们将计算权重的总和，具有较高总和权重的类成为预测类。</p><p id="4d11" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">红色等级的权重总和:</p><p id="5db9" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi">1/3 + 1/4 = 0.5833</p><p id="2521" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">蓝色等级的权重总和:</p><p id="8167" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi">1/5 + 1/8 + 1/6 = 0.4912</p><p id="32c9" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">因为红色类别具有较高的权重，所以我们对新数据点的预测类别是红色类别。</p><h1 id="eae3" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">使用Scikit的kNN-learn:</h1><h2 id="09d4" class="mt jo iq bd jp mu mv dn jt mw mx dp jx kw my mz kb la na nb kf le nc nd kj ne bi translated">kNN超参数:</h2><p id="de22" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在机器学习中，在我们可以使用任何算法之前，我们需要为该模型选择超参数的值。在kNN的情况下，重要的超参数是:</p><ul class=""><li id="5072" class="lo lp iq kn b ko lj ks lk kw lq la lr le ls li lt lu lv lw bi translated">邻居的数量。</li><li id="5e1b" class="lo lp iq kn b ko lx ks ly kw lz la ma le mb li lt lu lv lw bi translated"><code class="fe nf ng nh ni b">weights</code>:如果设置为<code class="fe nf ng nh ni b">uniform</code>，每个邻域中的所有点对预测类的影响相等，即预测类是邻域中点数最高的类。如果设置为<code class="fe nf ng nh ni b">distance</code>，较近的邻居将比较远的邻居具有更大的影响，即具有更多靠近新数据点的点的类成为预测类，为此，我们在计算权重时取距离的倒数，以便较近的点具有更高的权重。</li><li id="0087" class="lo lp iq kn b ko lx ks ly kw lz la ma le mb li lt lu lv lw bi translated"><code class="fe nf ng nh ni b">metric</code>:当<code class="fe nf ng nh ni b">weights</code>设置为<code class="fe nf ng nh ni b">distance</code>时使用的距离度量。默认值为<code class="fe nf ng nh ni b">minkowski</code>，这是计算两个数据点之间距离的一种方法。我们可以更改默认值以使用其他<a class="ae nj" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html" rel="noopener ugc nofollow" target="_blank">距离度量</a>。</li><li id="507b" class="lo lp iq kn b ko lx ks ly kw lz la ma le mb li lt lu lv lw bi translated"><code class="fe nf ng nh ni b">p</code>:是<code class="fe nf ng nh ni b">minkowski</code>度量的功率参数。如果p=1，那么距离度量是<code class="fe nf ng nh ni b">manhattan_distance</code>。如果p=2，那么距离度量是<code class="fe nf ng nh ni b">euclidean_distance</code>。如果愿意，我们可以试验更高的p值。</li></ul><pre class="md me mf mg gt nk ni nl nm aw nn bi"><span id="fca4" class="mt jo iq ni b gy no np l nq nr"># kNN hyper-parametrs<br/>sklearn.neighbors.KNeighborsClassifier(n_neighbors, weights, metric, p)</span></pre><p id="1db1" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">通过交叉验证尝试不同的超参数值可以帮助您为最终模型选择正确的超参数。</p><h2 id="7693" class="mt jo iq bd jp mu mv dn jt mw mx dp jx kw my mz kb la na nb kf le nc nd kj ne bi translated"><strong class="ak"> kNN分类器:</strong></h2><p id="ca22" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们将建立一个分类器，将手写数字从0到9分类。我们将使用的数据来自MNIST数据库，这是一组60，000个28×28像素的黑白图像，包含0到9之间的手写数字。</p><p id="f5d1" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">导入库:</p><pre class="md me mf mg gt nk ni nl nm aw nn bi"><span id="b513" class="mt jo iq ni b gy no np l nq nr"># To load MNIST image data<br/>from sklearn.datasets import load_digits</span><span id="8abe" class="mt jo iq ni b gy ns np l nq nr"># kNN Classifier<br/>from sklearn.neighbors import KNeighborsClassifier</span><span id="d3aa" class="mt jo iq ni b gy ns np l nq nr"># Confusion matrix to check model performance<br/>from sklearn.metrics import confusion_matrix</span><span id="c117" class="mt jo iq ni b gy ns np l nq nr"># To split data into training and testing set<br/>from sklearn.model_selection import train_test_split</span><span id="8cb3" class="mt jo iq ni b gy ns np l nq nr"># For plotting digit<br/>import matplotlib.pyplot as plt</span></pre><p id="ec1d" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">正在加载数字的MNIST数据:</p><pre class="md me mf mg gt nk ni nl nm aw nn bi"><span id="a07b" class="mt jo iq ni b gy no np l nq nr">digits = load_digits()</span></pre><p id="b07f" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">转换数据以用于kNN分类器:</p><pre class="md me mf mg gt nk ni nl nm aw nn bi"><span id="4c97" class="mt jo iq ni b gy no np l nq nr"># Number of images<br/>n_samples = len(digits.images)</span><span id="884b" class="mt jo iq ni b gy ns np l nq nr"># Changing shape from 28x28 pixel values to a sequence of values<br/>X = digits.images.reshape((n_samples, -1))</span><span id="3a24" class="mt jo iq ni b gy ns np l nq nr"># Getting the already known targets for each image<br/>y = digits.target</span></pre><p id="73f3" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">创建我们的培训和测试集:</p><pre class="md me mf mg gt nk ni nl nm aw nn bi"><span id="84aa" class="mt jo iq ni b gy no np l nq nr"># Splitting data to train and test sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span></pre><p id="b712" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">创建和培训模型:</p><pre class="md me mf mg gt nk ni nl nm aw nn bi"><span id="5a2d" class="mt jo iq ni b gy no np l nq nr"># Creating model<br/>clf = KNeighborsClassifier(n_neighbors=3)</span><span id="2d8f" class="mt jo iq ni b gy ns np l nq nr"># Training model<br/>clf.fit(X_train, y_train)</span></pre><p id="d547" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">获得测试数据的预测:</p><pre class="md me mf mg gt nk ni nl nm aw nn bi"><span id="ad38" class="mt jo iq ni b gy no np l nq nr"># Predictions for test data<br/>predicted = clf.predict(X_test)</span></pre><p id="8a28" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">使用混淆矩阵比较实际目标值和预测目标值:</p><pre class="md me mf mg gt nk ni nl nm aw nn bi"><span id="51d2" class="mt jo iq ni b gy no np l nq nr"># Print confusion matrix<br/>confusion_matrix(y_test, predicted)</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/e3c37ff15f3b153a3faf1ac411fe485c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*zpOexniP-rfCiUBbPG7Y7A.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">分类数字混淆矩阵</p></figure><p id="959e" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">在矩阵中，行代表实际目标值，其中第一行代表0标签，第二行代表1标签，依此类推。同样，列表示预测，其中第一列表示0标签，第二列表示1标签，依此类推。</p><p id="5d24" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">以黄色突出显示的矩阵对角线上的值是预测正确的值。</p><p id="ba95" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">考虑在第4列第9行用蓝色突出显示的值。这是一个错误。我们的模型将<strong class="kn ir"> </strong> 3 <strong class="kn ir"> </strong>误分类为8。</p><p id="fbd2" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">总的来说，我们的模型在将数字分类为误分类方面做得很好，即除对角线以外的值大多为零或小于2。</p><p id="ff12" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">查看前10幅图像和预测:</p><pre class="md me mf mg gt nk ni nl nm aw nn bi"><span id="6ff4" class="mt jo iq ni b gy no np l nq nr"># Zip image with prediction<br/>image_with_prediction = list(zip(digits.images, clf.predict(X)))</span><span id="d1ca" class="mt jo iq ni b gy ns np l nq nr"># for first 10 images<br/>for pos, (image, prediction) in enumerate(image_with_prediction[:10]):<br/>    plt.subplot(3, 4, pos+1) # Create 3x4 grid<br/>    plt.axis('off') # no axis<br/>    plt.imshow(image, cmap=plt.cm.gray_r) # show image in gray scale<br/>    plt.title("Prediction: %i" % prediction) # set title to predicted value<br/>plt.show() # show plot</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d59790ab4aab8c24c66b131e1c4b48d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*rHlvt2CtKC8QufnojhGgOA.png"/></div></figure><p id="8e7e" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">所有的预测看起来都不错，除了第6张看起来更像5，但我们的模型认为它是9。</p><h1 id="1619" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">结论:</strong></h1><p id="1491" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">当在你自己的问题上使用kNN时，确保每个特征的距离根据该特征的重要性进行缩放。如果出现房价问题，我们的特征年龄和房价将具有非常不同的比例，您必须缩小房价特征的比例以提高模型性能。</p><p id="7a54" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">如果您的数据具有大量维度，则您可能希望通过使用要素缩减和要素工程技术来缩减要素，因为维度越高，数据的准确性越低。</p><p id="c955" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">kNN在MNIST数据集上工作得很好，因为它是一个受控的数据集，即数字的位置在所有图像上都是一致的。此外，所有图像的像素值具有相似的颜色梯度。当处理具有大量空间信息的图像的分类问题时，即图像中物体位置的变化和颜色梯度的变化，您可能希望使用专门为此类任务构建的卷积神经网络。</p><p id="03ae" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">kNN也可用作回归算法，即，代替预测离散类，它也可用于预测连续数字，如房价。</p><p id="0ba7" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">希望这有助于你理解kNN。</p><p id="233a" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">感谢阅读。</p></div></div>    
</body>
</html>