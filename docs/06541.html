<html>
<head>
<title>Entropy Regularization in Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习中的熵正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/entropy-regularization-in-reinforcement-learning-a6fa6d7598df?source=collection_archive---------13-----------------------#2020-05-24">https://towardsdatascience.com/entropy-regularization-in-reinforcement-learning-a6fa6d7598df?source=collection_archive---------13-----------------------#2020-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="764e" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">在这篇文章中，我假设你已经知道强化学习的基础。如果你是一个初学者，想了解更多关于RL的知识，你可以从我在这里写的<a class="ae ks" rel="noopener" target="_blank" href="/reinforcement-learning-for-everyone-9c1163f61440">一个关于RL的介绍性故事开始。</a></p></blockquote><p id="8cc9" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在我们的日常语言中，我们通常使用术语“<em class="jv">熵</em>”来指一个系统(例如，宇宙)缺乏秩序或可预测性。)在强化学习(RL)中，该术语以类似的方式使用:<strong class="jw iu">在RL中，<em class="jv">熵</em>指的是代理</strong>动作的可预测性。这与其政策的确定性密切相关，即什么行动将产生最高的长期累积回报:如果确定性高，熵就低，反之亦然。您可以在以下图像中看到这一点:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi kw"><img src="../Images/04b66e55cb64caa276ba9bf8685d2afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrR-5wnSOlq-aqsAozmxCg.jpeg"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated">图1:RL中Q值的高低熵分布；a_i代表行动【自制。]</p></figure><p id="5ecb" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">RL中熵的正式定义来自信息论，其中熵的计算如等式(1)所示，针对具有概率质量函数<em class="jv"> P(X) </em>的离散随机变量<em class="jv"> x </em>。在RL中，公式变成等式(2)，因为我们计算策略π(a|s_t)的熵，其中<em class="jv"> a </em>表示每个动作，<em class="jv"> s </em>表示状态，<em class="jv"> t </em>表示时间步长。注意，为了简单起见，这里我们使用离散动作空间，但是通过用积分代替和，该定义可以容易地应用于连续动作空间。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi lm"><img src="../Images/646ed93d6adc9c81548581f35a677cbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6PkrnQ9OopdVdFY7bHea9A.jpeg"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated">信息论中计算一个离散随机变量熵的方程(1) [ <a class="ae ks" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)#Definition" rel="noopener ugc nofollow" target="_blank">摘自维基百科</a>，]和RL (2)中计算一个策略π(a|s)熵的方程。</p></figure><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi ln"><img src="../Images/1e0f9ae3c047d67501ed2e061335155c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4mPj93YTeFmueHhgaLNbDw.png"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated">图2:图1所示q值分布的熵计算。</p></figure><p id="1cbe" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">如果我们计算第一张图中显示的分布的熵，我们可以看到这个公式是如何工作的。在q值的第一次分布中，所有的概率都类似地低，而在第二次分布中，a_2具有高概率，而其他动作具有低概率。这使得第一次分布的熵高于第二次分布的熵，正如你在左边看到的。</p><h1 id="5b53" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">我们如何在RL中使用熵</h1><p id="eb2c" class="pw-post-body-paragraph jt ju it jw b jx mm jz ka kb mn kd ke kt mo kh ki ku mp kl km kv mq kp kq kr im bi translated">当代理正在学习它的策略，并且一个动作为一个状态返回一个正奖励时，可能会发生代理在将来总是使用这个动作，因为它知道它产生了<em class="jv">一些</em>正奖励。可能存在另一个产生更高回报的行为，但代理人永远不会尝试，因为它只会利用它已经学到的东西。这意味着代理可能会陷入局部最优，因为没有探索其他行为的行为，永远不会找到全局最优。</p><p id="bf4d" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">这就是熵派上用场的地方:我们可以用熵来鼓励探索，避免陷入局部最优。为了使这一点正式化，我们用政策的熵来增加传统的RL目标，如<a class="ae ks" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.206.2460&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank"> Ziebart (2010) </a>。最大熵RL目标定义为:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi mr"><img src="../Images/bb5a6d1f9f5070ffe08ddba96b3ab24a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cyyLc0n9zPE4i7H7TFNzoA.jpeg"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated">最大熵RL目标【来自<a class="ae ks" href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/" rel="noopener ugc nofollow" target="_blank">唐&amp;哈诺贾(2017) </a>。]</p></figure><p id="804e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">学习这种<a class="ae ks" href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy" rel="noopener ugc nofollow" target="_blank">最大熵模型</a>的思想起源于统计建模，其目标是找到具有最高熵的概率分布，同时仍然满足观察到的统计量[<a class="ae ks" href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/" rel="noopener ugc nofollow" target="_blank">Tang&amp;Haar noja(2017)</a>]。最大熵的<strong class="jw iu">原理</strong>陈述了具有最大熵的概率分布是在精确陈述的先验数据(在我们的情况下，这些陈述的先验数据是代理的经验)的上下文中最好地代表当前知识状态的概率分布。)</p><p id="af21" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">我们现在使用一个<em class="jv">熵加成</em>来计算q值，这意味着我们现在将熵H[π(a|s_t)]加到我们的q值上。在软Q学习中，<a class="ae ks" href="https://dl.acm.org/ft_gateway.cfm?id=3305521&amp;type=pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jv"> Haarnoja等人(2017) </em> </a>将熵与以下等式合并:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi ms"><img src="../Images/0b90bf56a2d1913546934f83736ac5a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AWsBLd1K_DlOLYaJ333RhA.jpeg"/></div></div><p class="li lj gj gh gi lk ll bd b be z dk translated">使用熵计算软q值[ <a class="ae ks" href="https://dl.acm.org/ft_gateway.cfm?id=3305521&amp;type=pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mt">哈尔诺贾等人(2017) </em> </a>。]</p></figure><h1 id="eb96" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">为什么我们在RL中使用熵</h1><p id="e933" class="pw-post-body-paragraph jt ju it jw b jx mm jz ka kb mn kd ke kt mo kh ki ku mp kl km kv mq kp kq kr im bi translated">熵已经迅速成为RL中流行的正则化机制。事实上，许多当前最先进的RL方法，如软演员评论家、A3C和PPO，使用它有多种好处:</p><h2 id="633d" class="mu lp it bd lq mv mw dn lu mx my dp ly kt mz na mc ku nb nc mg kv nd ne mk nf bi translated">改进勘探</h2><p id="2c21" class="pw-post-body-paragraph jt ju it jw b jx mm jz ka kb mn kd ke kt mo kh ki ku mp kl km kv mq kp kq kr im bi translated">如前所述，熵鼓励探索，避免代理人陷入局部最优的情况。这对于奖励很少的任务非常重要，因为代理人不会经常收到对其行为的反馈，因此可能会“高估”收到的一些奖励，并总是重复导致该奖励的行为。</p><h2 id="1f38" class="mu lp it bd lq mv mw dn lu mx my dp ly kt mz na mc ku nb nc mg kv nd ne mk nf bi translated"><strong class="ak">微调政策</strong></h2><p id="b254" class="pw-post-body-paragraph jt ju it jw b jx mm jz ka kb mn kd ke kt mo kh ki ku mp kl km kv mq kp kq kr im bi translated">间接来说，鼓励探索也有助于将学习从已有的政策转移到新的政策。例如，如果我们训练一个机器人在一个区域行走，当我们将这个机器人放入迷宫时，机器人可以重新利用其行走知识来导航迷宫，而不是从零开始，没有任何知识。如果我们使用常规策略——不使用熵——智能体将需要更长的时间来适应新任务，因为它已经了解了以前产生回报的情况，不会像使用最大熵策略的智能体那样探索那么多。这在下面的视频中可以看到:</p><figure class="kx ky kz la gt lb"><div class="bz fp l di"><div class="ng nh l"/></div><p class="li lj gj gh gi lk ll bd b be z dk translated">一个软Q学习代理被预先训练行走，然后用于不同的任务。还将代理与随机初始化的代理和DDPG代理进行比较【视频来自<a class="ae ks" href="https://www.youtube.com/watch?v=7Nm1N6sUoVs&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">软学习</a>。]</p></figure><h2 id="1617" class="mu lp it bd lq mv mw dn lu mx my dp ly kt mz na mc ku nb nc mg kv nd ne mk nf bi translated">更加稳健</h2><p id="5c72" class="pw-post-body-paragraph jt ju it jw b jx mm jz ka kb mn kd ke kt mo kh ki ku mp kl km kv mq kp kq kr im bi translated">由于智能体在学习时会探索更多的状态，这要归功于其最大熵策略的鼓励性探索，智能体在开发任务时也将对异常或罕见事件更具鲁棒性。这使得代理更加健壮，因为它将知道如何在不同的情况下更好地处理。</p><h1 id="7c01" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">结论</h1><p id="757a" class="pw-post-body-paragraph jt ju it jw b jx mm jz ka kb mn kd ke kt mo kh ki ku mp kl km kv mq kp kq kr im bi translated">熵在RL中的应用带来了许多好处:它改进了代理的探索，它让我们微调以前用于不同任务的策略，并且对于环境的罕见状态也更加健壮。正因为如此，它在软演员评论家、A3C等RL方法的设计中变得非常流行。</p><p id="97ce" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">它的效果在它所应用的环境中会有很大的不同，因此有必要检查一下熵是否真的对你的RL设置有益。如果您想更深入地了解这个主题，我推荐这两个出版物，它们提供了RL中熵正则化的详细分析(我在撰写本文时使用了它们和其他材料):</p><ul class=""><li id="03b7" class="ni nj it jw b jx jy kb kc kt nk ku nl kv nm kr nn no np nq bi translated"><a class="ae ks" href="https://dl.acm.org/ft_gateway.cfm?id=3305521&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">t .哈尔诺贾，唐，h .，Abbeel，p .，&amp;莱文，S. (2017)。基于深层能量策略的强化学习。ICML。</a></li><li id="c151" class="ni nj it jw b jx nr kb ns kt nt ku nu kv nv kr nn no np nq bi translated"><a class="ae ks" href="https://arxiv.org/pdf/1811.11214.pdf" rel="noopener ugc nofollow" target="_blank">艾哈迈德，z .，鲁，N. L .，诺鲁齐，m .，&amp;舒尔曼斯，D. (2018)。理解熵对政策优化的影响。<em class="jv"> arXiv预印本arXiv:1811.11214 </em>。</a></li></ul><p id="1e8f" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">感谢阅读！:)</p></div></div>    
</body>
</html>