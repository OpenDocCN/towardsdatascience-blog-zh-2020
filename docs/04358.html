<html>
<head>
<title>Day 110 of #NLP365: NLP Papers Summary — Double Embeddings and CNN-based Sequence Labelling for Aspect Extraction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365 的第 110 天:NLP 论文摘要——方面提取的双重嵌入和基于 CNN 的序列标记</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd?source=collection_archive---------49-----------------------#2020-04-19">https://towardsdatascience.com/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd?source=collection_archive---------49-----------------------#2020-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/61a393f717c1685f581a48076c22cd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDhaS8ed285Bb9L80C5U0g.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jg" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="bcfb" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="1304" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">NLP 论文摘要是我总结 NLP 研究论文要点的系列文章</h2></div><p id="5347" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">项目#NLP365 (+1)是我在 2020 年每天记录我的 NLP 学习旅程的地方。在这里，你可以随意查看我在过去的 100 天里学到了什么。在本文的最后，你可以找到以前的论文摘要，按自然语言处理领域分类:)</p><p id="81bb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">今天的 NLP 论文是<strong class="lj jt"> <em class="md">双重嵌入和基于 CNN 的序列标注进行方面提取</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="afc8" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">目标和贡献</h1><p id="03ac" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">提出了一种新的双嵌入卷积神经网络(DE-CNN)模型用于有监督的特征提取。这两种预训练嵌入是通用嵌入和特定领域嵌入。本文首次将双重嵌入和 CNN 模型用于特征提取，性能优于 SOTA 方法。</p><h1 id="6f6f" class="ml mm jj bd mn mo ni mq mr ms nj mu mv ky nk kz mx lb nl lc mz le nm lf nb nc bi translated">数据集</h1><p id="21b4" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">有两个来自 SemEval 2014 和 2016 的评估数据集:笔记本电脑领域和餐馆领域数据集。请参见下面的数据集统计摘要。</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/9d1e7cd737b6ab8435f012bfe4ac2ada.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/0*P93rNqpTvRWUL6tN.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">笔记本电脑和餐厅数据集的汇总统计数据[1]</p></figure><h1 id="70a6" class="ml mm jj bd mn mo ni mq mr ms nj mu mv ky nk kz mx lb nl lc mz le nm lf nb nc bi translated">双重嵌入 CNN(去 CNN)</h1><p id="6092" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">当构建方面提取的模型时(或者一般而言)，除了实现 SOTA 结果之外，还有另外两个重要的方面:</p><ol class=""><li id="426b" class="ns nt jj lj b lk ll ln lo lq nu lu nv ly nw mc nx ny nz oa bi translated">自动化特征学习</li><li id="cb07" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated">由于推理速度和实用性，简化和轻量级模型总是首选</li></ol><p id="50f7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">DE-CNN(上图)就是针对这两个方面提出来的。第一个方面使用通用嵌入和特定领域嵌入的双重嵌入机制来解决。嵌入的质量至关重要，因为它们通常是 NLP 管道中的第一步。大多数方面提取只包括通用的嵌入，如 Word2Vec 或 GloVe。这可能不合适，因为不管上下文如何，对某些方面单词的通用嵌入保持不变。这就是为什么在处理细粒度含义时，特定于领域的嵌入在方面提取中非常有用。第二个方面是通过下述事实解决的，即 DE-CNN 是用于序列标记的纯 CNN 模型。下面是体系结构图和每个组件的简短描述:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/51d593ce033ba1f3e3c2f072c4e3b745.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/0*tPgHRUU-E8n3GR0m.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">DE-CNN 的模型架构[1]</p></figure><ol class=""><li id="3a64" class="ns nt jj lj b lk ll ln lo lq nu lu nv ly nw mc nx ny nz oa bi translated">输入单词序列经过两个独立的嵌入层，得到两个连续的表示:输入的一般嵌入和特定领域嵌入</li><li id="5b84" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated">将这两个嵌入连接在一起</li><li id="f944" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated">将级联嵌入馈送到 4 层 CNN，并获得所有单词的表示。注意，这些 CNN 层没有 max-pooling 层，因为我们需要模型很好地表示输入序列中的每个单词</li><li id="d646" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated">我们将 CNN 的输出输入到一个全连接的层，使用 softmax 函数计算输入序列中每个单词的标签概率分布。标签是{B，I，O}。b 和 I 代表体短语的开始词和非开始词，O 代表非体词</li></ol><h1 id="c5c6" class="ml mm jj bd mn mo ni mq mr ms nj mu mv ky nk kz mx lb nl lc mz le nm lf nb nc bi translated">实验</h1><h2 id="150a" class="oh mm jj bd mn oi oj dn mr ok ol dp mv lq om on mx lu oo op mz ly oq or nb jp bi translated">模型比较</h2><p id="cdfd" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">本文中的基准模型分为以下三组:</p><p id="d867" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">单一任务方法</p><ol class=""><li id="6ae8" class="ns nt jj lj b lk ll ln lo lq nu lu nv ly nw mc nx ny nz oa bi translated"><em class="md">条件随机场</em>。使用手套嵌入</li><li id="55ef" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated"><em class="md"> HIS_RD </em>。SemEval 任务中的最佳系统</li><li id="8da7" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated"><em class="md"> NLANGP </em>。SemEval 任务中的最佳系统</li><li id="e53f" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated"><em class="md"> WBEmb </em>。使用具有 3 种不同嵌入的 CRF</li><li id="4a79" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated"><em class="md"> BiLSTM。</em></li><li id="53f1" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated"><em class="md">比尔斯特姆-CNN-CRF </em>。命名实体识别的 SOTA 模型</li></ol><p id="d9d9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">多任务方法</p><ol class=""><li id="8afd" class="ns nt jj lj b lk ll ln lo lq nu lu nv ly nw mc nx ny nz oa bi translated"><em class="md"> RNCRF </em>依赖树神经网络和条件随机场的联合模型</li><li id="4fa0" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated">CMLA 。多层耦合注意网络</li><li id="ebcc" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated"><em class="md">最小</em>。由三个 LSTM 组成，其中两个 lstm 共同学习提取方面和观点，最后一个 LSTM 学习区分情感和非情感句子</li></ol><p id="5397" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">去 CNN 的变体</p><ol class=""><li id="11dd" class="ns nt jj lj b lk ll ln lo lq nu lu nv ly nw mc nx ny nz oa bi translated"><em class="md"> GloVe-CNN </em>。没有特定领域的嵌入</li><li id="2993" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated"><em class="md">域-CNN </em>。没有通用嵌入</li><li id="4b2a" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated"><em class="md"> MaxPool-DE-CNN </em>。包括最大池层</li><li id="ace2" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated"><em class="md"> DE-OOD-CNN </em>。用域外嵌入替换特定于域的嵌入</li><li id="e8b0" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated"><em class="md">去 Google-CNN </em>。使用 GoogleNews 嵌入代替手套</li><li id="9b43" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated"><em class="md">去 CNN-CRF </em>。用 CRF 层替换 softmax 函数</li></ol><h1 id="057f" class="ml mm jj bd mn mo ni mq mr ms nj mu mv ky nk kz mx lb nl lc mz le nm lf nb nc bi translated">结果</h1><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/0c2cd1fc4ec750a2c9ce2927177eda57.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/0*1awHjLL3h5GCgPPL.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">DE-CNN 和基线模型的变化之间的 F1 分数比较[1]</p></figure><p id="d0e1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上面的结果表显示，DE-CNN 的性能优于所有基准模型。结果表明，拥有一个独立的嵌入层，无论是通用的还是特定领域的，都不足以获得好的结果。不同领域的特定领域嵌入是不同的，笔记本电脑和餐馆数据集之间的结果差异向我们表明，DE-CNN 在具有许多特定领域方面的领域中将表现得更好。</p><h1 id="f532" class="ml mm jj bd mn mo ni mq mr ms nj mu mv ky nk kz mx lb nl lc mz le nm lf nb nc bi translated">结论和未来工作</h1><p id="d90d" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">通过错误分析，DE-CNN 似乎有两大错误:</p><ol class=""><li id="5cce" class="ns nt jj lj b lk ll ln lo lq nu lu nv ly nw mc nx ny nz oa bi translated">标签不一致。同一个方面有时被标记，有时不被标记</li><li id="767d" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc nx ny nz oa bi translated">看不见的方面。该模型未能提取彼此密切相关的一对方面。这可能是一个潜在的未来工作，包括连接词的语义</li></ol><h2 id="c23d" class="oh mm jj bd mn oi oj dn mr ok ol dp mv lq om on mx lu oo op mz ly oq or nb jp bi translated">来源:</h2><p id="a2fd" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">[1]徐，h，刘，b，舒，l 和余，p . s . 2018 .面向方面提取的双重嵌入和基于 cnn 的序列标记。<em class="md"> arXiv 预印本 arXiv:1805.04601 </em>。网址:【https://www.aclweb.org/anthology/P18-2094.pdf T2】</p><p id="3151" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【https://ryanong.co.uk】原载于 2020 年 4 月 19 日<a class="ae jg" href="https://ryanong.co.uk/2020/04/19/day-110-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-aspect-extraction/" rel="noopener ugc nofollow" target="_blank"><em class="md"/></a><em class="md">。</em></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="a745" class="oh mm jj bd mn oi oj dn mr ok ol dp mv lq om on mx lu oo op mz ly oq or nb jp bi translated">特征提取/基于特征的情感分析</h2><ul class=""><li id="0859" class="ns nt jj lj b lk nd ln ne lq ot lu ou ly ov mc ow ny nz oa bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="5f5b" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc ow ny nz oa bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sense-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="b2ec" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc ow ny nz oa bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitive-analysis-f 24 a2 EC 1 ca 32</a></li><li id="39e9" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc ow ny nz oa bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="17a4" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc ow ny nz oa bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towards data science . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li></ul><h2 id="0014" class="oh mm jj bd mn oi oj dn mr ok ol dp mv lq om on mx lu oo op mz ly oq or nb jp bi translated">总结</h2><ul class=""><li id="87e6" class="ns nt jj lj b lk nd ln ne lq ot lu ou ly ov mc ow ny nz oa bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="2b01" class="ns nt jj lj b lk ob ln oc lq od lu oe ly of mc ow ny nz oa bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1b 27</a></li></ul><h2 id="1da1" class="oh mm jj bd mn oi oj dn mr ok ol dp mv lq om on mx lu oo op mz ly oq or nb jp bi translated">其他人</h2><ul class=""><li id="edd3" class="ns nt jj lj b lk nd ln ne lq ot lu ou ly ov mc ow ny nz oa bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li></ul></div></div>    
</body>
</html>