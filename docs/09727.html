<html>
<head>
<title>Let’s make some Anime using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们用深度学习制作一些动漫</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lets-make-some-anime-using-deep-learning-d258806954f5?source=collection_archive---------29-----------------------#2020-07-10">https://towardsdatascience.com/lets-make-some-anime-using-deep-learning-d258806954f5?source=collection_archive---------29-----------------------#2020-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a958" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">文本生成方法比较:LSTM 与 GPT2</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/26caf84fc503e5b65dd3c2015ae08b86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GegtbQumzxzevbzS"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@brucetml?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布鲁斯唐</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="5618" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个项目的动机是看看在 NLP 领域，尤其是在生成创造性内容方面，技术在短短几年内取得了多大的进步。我通过生成动画概要探索了两种文本生成技术，第一种是相对古老的 LSTM 单位技术，第二种是微调过的 GPT2 转换器。</p><p id="218b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，你将会看到人工智能是如何从创造这些无意义的东西…</p><blockquote class="lv lw lx"><p id="1882" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">一个能干的年轻女人:一个被送回家的人类的神经劳力？打败一切成为他们的决心后，学校谁知道他们是否都使自己的能力。然而，那些叫她过去的学生焦油驳船在一起时，他们的神秘高艺术家是采取了计划，而吃饭打架！</p></blockquote><p id="9eef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">敬这件艺术品。</p><blockquote class="lv lw lx"><p id="7c73" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">一个名叫相户爱的女高中生暗恋一个名叫三木的神秘女孩。她是唯一一个能记住女孩名字的人，她决心要找出她到底是谁。</p></blockquote><p id="6209" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了充分利用这篇文章，你必须了解:</p><ul class=""><li id="edd6" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">Python 编程</li><li id="2cfb" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">Pytorch</li><li id="4291" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">RNNs 的工作</li><li id="de31" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">变形金刚(电影名)</li></ul><p id="a5e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，那么，让我们看看一些代码！</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="4301" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">数据描述</h1><p id="4253" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">这里使用的数据是从<a class="ae ky" href="http://imelist.net/anime.php" rel="noopener ugc nofollow" target="_blank"> myanimelist </a>上刮下来的，它最初包含了超过 16000 个数据点，是一个非常混乱的数据集。我已经采取了以下步骤来清理它:</p><ul class=""><li id="9097" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">删除了所有奇怪的动画类型(如果你是一个动漫迷，你会知道我在说什么)。</li><li id="e8b8" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">每个概要都在描述的最后包含了它的来源(例如:来源:myanimelist，来源:crunchyroll 等等。)所以我也把它去掉了。</li><li id="e6f3" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">基于视频游戏、衍生产品或一些改编的动画只有很少的摘要，所以我删除了所有单词少于 30 个的概要&amp;我还删除了所有包含“衍生产品”、“基于”、“音乐视频”、“改编”等单词的概要。这背后的逻辑是，这些类型的动画不会真正使我们的模型<strong class="lb iu">有创意</strong>。</li><li id="c18a" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">我也把剧情梗概词超过 300 的动漫给删了。这只是为了使培训更容易(查看 GPT2 部分了解更多详细信息)。</li><li id="11a9" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">移除符号。</li><li id="8c25" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">一些描述还包含日本字符，所以这些也被删除。</li></ul><p id="4dba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下函数负责所有这些</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h1 id="9d6c" class="mx my it bd mz na nw nc nd ne nx ng nh jz ny ka nj kc nz kd nl kf oa kg nn no bi translated">LSTM 之路</h1><p id="ee05" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">传统的文本生成方法使用循环 LSTM 单位。LSTM(或长短期记忆)专门设计用于捕获正常 rnn 无法捕获的序列数据中的长期相关性，它通过使用多个<em class="ly">门</em>来实现这一点，这些门控制从一个时间步骤传递到另一个时间步骤的信息。</p><p id="be3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直观地说，在一个时间步中，到达 LSTM 单元的信息通过这些门，它们决定信息是否需要被<em class="ly">更新，</em>如果它们被更新，那么旧的信息被忘记，然后这个新的更新值被发送到下一个时间步。为了更详细地了解 LSTMs，我建议你浏览这个<a class="ae ky" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">博客</a>。</p><h2 id="c49e" class="ob my it bd mz oc od dn nd oe of dp nh li og oh nj lm oi oj nl lq ok ol nn om bi translated">创建数据集</h2><p id="ff9f" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">因此，在我们构建模型架构之前，我们必须将概要符号化，并以模型能够接受的方式对其进行处理。</p><p id="6c16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在文本生成中，输入和输出是相同的，只是输出标记向右移动了一步。这基本上意味着模型接受输入的过去的单词并预测下一个单词。输入和输出令牌分批传递到模型中，每批都有固定的序列长度。我按照以下步骤创建了数据集:</p><ul class=""><li id="d74f" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">创建一个配置类。</li><li id="ddd1" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">把所有的大纲连接在一起。</li><li id="cb77" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">符号化概要。</li><li id="a9d1" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">定义批次数量。</li><li id="f125" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">创建词汇，索引词和索引词词典。</li><li id="e86a" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">通过向右移动输入记号来创建输出记号。</li><li id="3443" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">创建一个生成器函数，批量输出输入和输出序列。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">创建数据集</p></figure><h2 id="f88c" class="ob my it bd mz oc od dn nd oe of dp nh li og oh nj lm oi oj nl lq ok ol nn om bi translated">模型架构</h2><p id="baa6" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">我们的模型包括一个嵌入层，一堆 LSTM 层(我在这里用了 3 层)，丢弃层，最后是一个线性层，输出每个词汇标记的分数。我们还没有使用 softmax 层，你很快就会明白为什么。</p><p id="9364" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为 LSTM 单元也输出隐藏状态，所以模型也返回这些隐藏状态，以便它们可以在下一个时间步骤(下一批单词序列)中传递到模型上。此外，在每个时期之后，我们需要将隐藏状态重置为 0，因为我们在当前时期的第一时间步中不需要来自前一时期的最后时间步的信息，所以我们也有“零状态”函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型</p></figure><h2 id="c294" class="ob my it bd mz oc od dn nd oe of dp nh li og oh nj lm oi oj nl lq ok ol nn om bi translated">培养</h2><p id="c7df" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">然后，我们只需定义训练函数，存储每个时期的损失，并保存具有最佳损失的模型。我们还在每个时期之前调用零状态函数来重置隐藏状态。</p><p id="46f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用的损失函数是<strong class="lb iu">交叉熵损失</strong>，这就是我们不通过显式 softmax 层传递输出的原因，因为该损失函数是在内部计算的。</p><p id="61fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有培训都是在 GPU 上完成的，以下是正在使用的参数(在配置类中提供):</p><ul class=""><li id="a25a" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">批量= 32</li><li id="4395" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">最大序列长度= 30</li><li id="8442" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">嵌入维度= 100</li><li id="7c98" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">隐藏维度= 512</li><li id="9306" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">纪元= 15</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="0b73" class="ob my it bd mz oc od dn nd oe of dp nh li og oh nj lm oi oj nl lq ok ol nn om bi translated">制作动漫</h2><p id="a4ad" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">在文本生成步骤中，我们将一些输入文本输入到模型中，例如，“一个年轻的女人”，我们的函数将首先对此进行标记，然后将其传递到模型中。该函数还接受我们想要输出的概要的长度。</p><p id="bdc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型将输出每个词汇标记的分数。然后，我们将 softmax 应用于这些分数，将其转换为概率分布。</p><p id="28b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们使用 top-k 采样，即我们从 n 个词汇标记中选择概率最高的前 k 个标记，然后随机采样一个标记，我们返回该标记作为输出。</p><p id="654f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，这个输出被连接到初始输入字符串中。这个输出令牌成为下一个时间步的输入。假设输出是“有能力”，那么我们的连接文本是“一个年轻女子有能力”。我们一直这样做，直到输出最后一个令牌，然后打印输出。</p><p id="b452" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个很好的图表来理解这个模型在做什么</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/ef6412b052af5880918f53e1a48c9edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*WV4PZQ3mGbKTCWj0.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">推理步骤。来源:<a class="ae ky" href="https://machinetalk.org/2019/02/08/text-generation-with-pytorch/" rel="noopener ugc nofollow" target="_blank">机器对话</a></p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="26de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，我把最大长度设为 100，输入文本设为“In ”,这就是我们得到的输出</p><blockquote class="lv lw lx"><p id="a792" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">在这几天的尝试中。虽然它已经，但是！他们认为人类的这些问题。看来，如果真的会做出什么事来。因为她必须永远不克服津贴与 jousuke s，为了她的家在他没有这一切在世界上:在医院里，她使他从自己的恶魔和屠杀。一个成员和一个偶像队的权力，以任何方式，但这两个来到它的世界，如果这仍然是等待和不去！在一个</p></blockquote><p id="6670" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这似乎在语法上是正确的，但毫无意义。虽然 LSTM 比基本 RNN 更擅长捕捉长期依存关系，但如果我们使用双向 RNNs 来捕捉文本的上下文，它们只能看到向后或向前的几步，因此当生成很长的句子时，我们会看到它们没有意义。</p><h1 id="74b0" class="mx my it bd mz na nw nc nd ne nx ng nh jz ny ka nj kc nz kd nl kf oa kg nn no bi translated">GPT2 方式</h1><h2 id="3484" class="ob my it bd mz oc od dn nd oe of dp nh li og oh nj lm oi oj nl lq ok ol nn om bi translated">一点理论</h2><p id="fb16" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated"><strong class="lb iu">变形金刚</strong>在捕捉所提供文本的上下文方面做得更好。他们只使用注意力层(没有 rnn ),这使他们能够更好地理解文本的上下文，因为他们可以看到尽可能多的时间后退(和前进，取决于注意力)。注意力有不同的类型，但是<strong class="lb iu"> GPT2、</strong>使用的注意力是语言建模的最佳模型之一，叫做<strong class="lb iu">掩蔽自我注意力</strong>。如果你不熟悉变形金刚，请在继续之前浏览这个<a class="ae ky" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">博客</a>。</p><p id="a09e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GPT2 不使用转换器编码器和解码器堆栈，而是使用仅包含转换器解码器的高堆栈。根据堆叠的解码器数量，GPT2 变压器有 4 种变体。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/a7351664c5f73f2c5bb62deac9837d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YEkqj2DLXGyYRuRV.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">变体。来源:<a class="ae ky" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">贾勒马</a></p></figure><p id="ade9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个解码器单元主要由两层组成:</p><ul class=""><li id="de16" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">隐蔽的自我注意</li><li id="4bdd" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">前馈神经网络</li></ul><p id="085b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有一个图层规范化步骤，每个步骤之后还有一个残差连接。这就是我的意思…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/466ad5f59725c4f540a52ef226c358e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ohtRo9q8eCJPGusy7YSKw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单解码器单元</p></figure><p id="15ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你之前读过这篇博文，你一定知道自我关注是如何计算的。直观地说，自我关注分数给予我们当前时间步中的单词应该给予其他单词的<strong class="lb iu">重要性</strong>或<strong class="lb iu">关注度</strong>(过去的时间步或未来取决于关注度)。</p><p id="642c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在伪装的自我关注中，我们并不关心下一个或未来的单词。所以变压器解码器只允许<strong class="lb iu">参与</strong>到现在，过去的字和未来的字被<strong class="lb iu">屏蔽</strong>。</p><p id="609f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是这个想法的一个美丽的代表…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/6280c113d8295cd2a367fde6bf817d28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2fSxTHKV702koJrF.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">伪装的自我关注。来源:<a class="ae ky" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">贾勒马</a></p></figure><p id="a7aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，当前单词是“it ”,正如你所看到的，单词“a”和“robot”有很高的注意力分数。这是因为“它”被用来指代“机器人”，所以“a”也是。</p><p id="03d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您一定注意到了上面输入文本开头的<code class="fe oq or os ot b">&lt;s&gt;</code>标记。<code class="fe oq or os ot b">&lt;s&gt;</code>只是用来标记一个输入字符串的开始。传统上是用 T2 来代替 s。</p><p id="717b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一件事你一定注意到了，这类似于传统的语言建模，看到现在和过去的标记，下一个标记被预测。然后将这个预测的记号添加到输入中，然后再次预测下一个记号。</p><p id="0fce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经对 GPT2 做了非常直观和高层次的了解。尽管这足以深入研究代码，但阅读更多关于这个概念的内容以获得更深入的理解将是一个好主意。我推荐杰伊·阿拉玛的博客。</p><h2 id="d2d4" class="ob my it bd mz oc od dn nd oe of dp nh li og oh nj lm oi oj nl lq ok ol nn om bi translated">代码</h2><p id="df56" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">我已经使用 GPT2 和来自<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>库的线性模型头来生成文本。在 4 个变体中，我使用了具有 117M 参数的 GPT2 small。</p><p id="eb95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经在 Google Colab 上训练了该模型，训练中的主要问题是计算出批量大小和最大序列长度，以便我在 GPU 上训练时不会耗尽内存，批量大小为 10，最大序列长度为 300 最终为我工作。</p><p id="9767" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于这个原因，我也删除了超过 300 个单词的概要，这样当我们生成长度为 300 的概要时，它实际上是完整的。</p><h2 id="b297" class="ob my it bd mz oc od dn nd oe of dp nh li og oh nj lm oi oj nl lq ok ol nn om bi translated">创建数据集</h2><p id="86f4" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">对于微调，第一个任务是获得所需格式的数据，Pytorch 中的 dataloader 允许我们非常容易地做到这一点。</p><p id="268e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤:</p><ul class=""><li id="6cd2" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">使用上面定义的<strong class="lb iu"> clean_function </strong>清除数据。</li><li id="6227" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">在每个剧情简介后添加<code class="fe oq or os ot b">&lt;|endoftext|&gt;</code>标记。</li><li id="5e20" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">使用 HuggingFace 的 GPT2Tokenizer 对每个大纲进行标记。</li><li id="6de7" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">为标记化的单词创建一个掩码(<strong class="lb iu">注意</strong>:这个掩码与我们讨论过的掩蔽自我注意不同，这是为了掩蔽我们接下来将看到的填充标记)。</li><li id="cc19" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">使用<code class="fe oq or os ot b">&lt;|pad|&gt;</code>标记填充长度小于最大长度(此处为 300)的序列。</li><li id="294f" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">将令牌 id 和掩码转换为张量并返回它们。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">资料组</p></figure><h2 id="5c42" class="ob my it bd mz oc od dn nd oe of dp nh li og oh nj lm oi oj nl lq ok ol nn om bi translated">模型架构</h2><p id="28b5" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">在这里，我们不需要明确地创建一个模型架构，因为拥抱面部库已经为我们做好了准备。我们只是简单地用语言模型头导入预先训练好的 GPT2 模型。</p><p id="7945" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个 LM 头实际上只是一个线性层，输出每个词汇标记的分数(在 softmax 之前)。</p><p id="3a1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Hugging Face 提供的带有 LM head 的 GPT 2 模型的酷之处在于，我们可以在这里直接传递标签(我们的输入令牌),它们在内部向右移动一步，模型以及预测分数也返回损失。它实际上还会返回模型中每一层的隐藏状态以及注意力得分，但我们对此不感兴趣。</p><p id="4021" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以导入模型和标记器，并在一个配置类中定义所有的超参数，就像这样…</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="bc2a" class="ob my it bd mz oc od dn nd oe of dp nh li og oh nj lm oi oj nl lq ok ol nn om bi translated">培训功能</h2><p id="0899" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">步骤:</p><ul class=""><li id="214e" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">训练函数从数据加载器中获取 id 和掩码。</li><li id="28a2" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">通过模型传递 id 和掩码。</li></ul><p id="5f56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型输出一个元组:- <code class="fe oq or os ot b">(loss,predicted scores, list of key&amp;value pairs of every masked attention layer, list of hidden states of every layer,attention scores)</code>我们只对这个元组中的前两项感兴趣，所以我们访问它们。</p><ul class=""><li id="5a4d" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">执行反向传播并更新参数。</li><li id="3d99" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">返回该时期的平均损失。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="b110" class="ob my it bd mz oc od dn nd oe of dp nh li og oh nj lm oi oj nl lq ok ol nn om bi translated">运行列车功能</h2><p id="5cb9" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">步骤:</p><ul class=""><li id="21e1" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">读取数据。</li><li id="9f36" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">创建数据加载器对象。</li><li id="e4a4" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">定义优化器，我用的是 AdamW(权重衰减的 Adam)。学习率为 0.0001，重量衰减为 0.003。</li><li id="a32c" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">定义调度程序。我用的是线性时间表，从拥抱脸部开始热身。热身步骤有 10 步(这基本上意味着前 10 个训练步骤的学习率将线性增加，然后线性减少)。</li><li id="c2bd" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">运行培训功能。我已经训练了 5 个时代。</li><li id="dc58" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">保存损失最低的模型。</li><li id="a892" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">在每个时期后清空 GPU 缓存，以防止 OOM 错误。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="1083" class="ob my it bd mz oc od dn nd oe of dp nh li og oh nj lm oi oj nl lq ok ol nn om bi translated">制作动漫</h2><p id="c702" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">在生成步骤中，我使用了 top-k 抽样(如 LSTM 方式)和 top-p 抽样。在 top-p 采样中，我们提供一个累积概率，比如 p，那么被选择的顶级词汇标记必须具有 p 的和概率。</p><p id="28c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以组合 top-k 和 top-p 方法，首先选择具有最高概率得分的 top-k 个表征，然后为这些 k 个表征计算归一化得分。这使得 k 个表征的这些分数的总和是 1，我们也可以说概率质量只是在 k 个表征中重新分布。</p><p id="e190" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，对这 k 个分数进行 top-p 采样，然后从选定的标记中进行采样，最后我们使用概率进行采样，以获得最终的输出标记。</p><p id="49f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，我们不必编写所有这些代码，拥抱脸用它的生成方法来处理所有这些。</p><p id="7efe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤:</p><ul class=""><li id="618f" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">获取输入文本并对其进行编码(tokenize + pad)以获得 id。</li><li id="6eae" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">使用生成函数传递 id。在生成方法中传递编码后的<code class="fe oq or os ot b">&lt;|pad|&gt;</code>标记非常重要，这样就可以区分它。</li><li id="e3de" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">解码输出并返回。</li><li id="4ed9" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">将输出保存在文本文件中。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="201f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您一定注意到了 generate 方法有很多参数。可以对它们进行调整，以获得最佳输出。看看这篇详细解释这些参数的博客。</p><p id="b0ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于输入文本，“In the year”这是我们得到的输出…</p><blockquote class="lv lw lx"><p id="0cef" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">2060 年，人类已经殖民了太阳系，现在正处于殖民其他星球的边缘。为了抵御这种新的威胁，地球联邦建立了一个特殊的单位，称为行星防御部队，简称 PDF。该单位由精英地球防御部队组成，他们的任务是保护地球免受任何可能威胁地球安全的外星生命形式的威胁。然而，当一艘神秘的外星飞船在巡逻途中坠毁时，他们被迫使用他们特殊的机动战士来抵御外星人的威胁。</p></blockquote><p id="9b74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你猜怎么着我真的会看这个。</p><p id="23d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LSTMs 和 GPT2 生成的大纲之间的差异是巨大的！该模型不仅能够很好地捕捉长期相关性，而且上下文也始终得到维护。</p><p id="530d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里还有一个…</p><blockquote class="lv lw lx"><p id="8c9e" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">死神是传奇战士志贺美之父的后代，他被派往地球与被称为黑暗氏族的邪恶组织作战。然而，他的任务是去偷神圣的剑，光之剑，据说它可以授予使用它的人不死之身。</p></blockquote><p id="57b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">检查我的 github <a class="ae ky" href="https://github.com/Arpan-Mishra/Anime-Generation-using-Deep-Learning" rel="noopener ugc nofollow" target="_blank">库</a>，在那里你可以浏览代码并看到一些更酷的人工智能生成的动画。</p><p id="ae01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢这篇文章，并且能够很容易地关注它。请在评论中提供宝贵的反馈。我很想知道你将如何完成这项任务，以及你是否有更好的方法来做这件事和改进模型。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="98f5" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">参考</h1><p id="0581" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">克里斯托弗·奥拉的《了解 LSTM </p><p id="d6e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="http://lk.org/2019/02/08/text-generation-with-pytorch/" rel="noopener ugc nofollow" target="_blank">文本生成与 LSTM </a>由<a class="ae ky" href="https://machinetalk.org/author/chunml/" rel="noopener ugc nofollow" target="_blank"> Trung Tran </a></p><p id="885a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Martin Frolovs 的<a class="ae ky" rel="noopener" target="_blank" href="/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912">微调 GPT2 示例</a></p><p id="d076" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">杰伊·阿拉玛的《变形金刚》博客</p><p id="338a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">杰伊·阿拉玛的 GPT2 博客</p><p id="e600" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">生成抱脸方法</a></p></div></div>    
</body>
</html>