<html>
<head>
<title>Available hyperparameter-optimization techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可用的超参数优化技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/available-hyperparameter-optimization-techniques-dc60fb836264?source=collection_archive---------14-----------------------#2020-01-07">https://towardsdatascience.com/available-hyperparameter-optimization-techniques-dc60fb836264?source=collection_archive---------14-----------------------#2020-01-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2c66" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们在分类模型集上应用三种不同的超参数优化技术来比较它们的准确性。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fd4dc1ac45bf1c432c1a02690a91ca02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pxDkDWG3ipmIBrYW"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@vaniashows?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Vania拍摄的照片显示</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h1 id="86d8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">背景</h1><p id="ee9f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我经常收到关于自动搜索超参数优化的文章。一旦我理解了这些技术的意图和范围，我就想知道通过这种优化一个模型可以改进多少。<br/>本文的目的是调查不同的可用优化技术，并在一个简单的例子上测试它们，比较它们并查看所获得的改进的概述。</p><p id="c175" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">超参数是模型可调参数，必须对其进行调整以获得具有最佳性能的模型。然后，优化模型的超参数是提高所选算法性能的关键任务。</p><p id="b6f3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在某种程度上，我们需要知道每个超参数在每个算法中的含义及其可能的值。深入理解不同算法中每个超参数的含义是必要的，也是一项艰巨的任务，有时意味着了解算法内部的工作方式及其背后的数学原理。这篇文章的内容没有达到那个深度，虽然我们会在每一个里面选取一些超参数，用不同的算法来分析。</p><p id="45cb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用过任何算法的人可能已经对缺省值集进行了一些手动优化尝试。这种手动调整通常需要很长时间，并不总是严格地进行，并且很难将结果系统化</p><p id="94ea" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">第二，我们可以应用可用的自动化和相当简单的技术，例如网格搜索和随机搜索，它们通常给出更好的结果，但是时间和机器计算的成本很高。我们将应用这两种技术来比较它们的结果</p><p id="0e16" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，我们将应用贝叶斯优化，这是一种找到函数最小值的方法，在测试的最佳算法上使用Python的hyperopt库。这种技术的实现可能不那么容易，但它可以在性能或时间上给我们比以前更好的结果。</p><h1 id="2111" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">数据集</h1><p id="7d58" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将在练习中使用的数据集是<a class="ae ky" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank"> Kaggle的泰坦尼克号乘客数据集</a>，这是一个二元分类练习，用于预测哪些乘客幸存，哪些乘客没有。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="87fd" class="mx la it mt b gy my mz l na nb">import os<br/>import numpy as np  # linear algebra<br/>import pandas as pd  #<br/>from datetime import datetime<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import SGDClassifier<br/>from sklearn.linear_model import RidgeClassifier<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.ensemble import BaggingClassifier<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>import lightgbm as lgb<br/>from sklearn.metrics import confusion_matrix<br/>import scikitplot as skplt</span><span id="ecf0" class="mx la it mt b gy nc mz l na nb">%matplotlib inline</span><span id="8a99" class="mx la it mt b gy nc mz l na nb">np.random.seed(7)<br/>train = pd.read_csv('./data/train.csv', index_col=0)<br/>y = train.Survived  #.reset_index(drop=True)<br/>features = train.drop(['Survived'], axis=1)<br/>features.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/ee465f590c75f4054c727f0dd61d9ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x-xWJxIkRQLIaeikS8rLCw.png"/></div></div></figure><p id="3d5f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">选择的指标是准确性:正确预测的乘客百分比。<br/>准确率= (TP+TN) /(合计)</p><h1 id="dfda" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">特征工程和结果数据框架</h1><p id="b258" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将应用数据工程的基本技术，这将使我们能够无误地使用算法(记住，本文的目的不是获得好的结果，而只是比较应用优化技术时的性能)。</p><p id="8476" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们选择了八种算法来完成这项工作；这个决定是基于重用杰森·布朗利和T2·威尔·科尔森的部分工作，他们开发了这里使用的大部分代码。</p><p id="a0ed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这些模型中的每一个都将是我们结果表中的不同列:</p><ol class=""><li id="a6a5" class="ne nf it lt b lu mn lx mo ma ng me nh mi ni mm nj nk nl nm bi translated">随机梯度推进</li></ol><p id="82a5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.脊线分类器</p><p id="f1d5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.k-最近邻(KNN)</p><p id="09c6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.支持向量机(SVM)</p><p id="c540" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">5.袋装决策树</p><p id="f064" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">6.随机森林</p><p id="5308" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">7.逻辑回归</p><p id="5eca" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">8.LGBM</p><p id="d40d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于这些列中的每一列，我们将尝试应用以下优化技术:</p><ul class=""><li id="1994" class="ne nf it lt b lu mn lx mo ma ng me nh mi ni mm nn nk nl nm bi translated">默认超参数</li><li id="c5ac" class="ne nf it lt b lu no lx np ma nq me nr mi ns mm nn nk nl nm bi translated">Sklearn GridSearchCV</li><li id="4206" class="ne nf it lt b lu no lx np ma nq me nr mi ns mm nn nk nl nm bi translated">Sklearn随机搜索CV</li><li id="5713" class="ne nf it lt b lu no lx np ma nq me nr mi ns mm nn nk nl nm bi translated">Python的远视</li></ul><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="e61b" class="mx la it mt b gy my mz l na nb">features = features.drop(['Cabin'], axis=1)<br/>features = features.drop(['Name'], axis=1)<br/>objects = [col for col in features.columns if features[col].dtype == "object"]</span><span id="9e79" class="mx la it mt b gy nc mz l na nb">features.update(features[objects].fillna('None'))<br/>numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']<br/>numerics = [col for col in features.columns if features[col].dtype in numeric_dtypes]<br/>features.update(features[numerics].fillna(0))<br/>features.info()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f1d0291fc76980ef6a78ab8aaefa293f.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*gaoFsml1sgO7VjrYJyQkug.png"/></div></figure><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="6622" class="mx la it mt b gy my mz l na nb"># dummies and split<br/>X = pd.get_dummies(features)<br/>X_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size=0.70,test_size=0.30,random_state=0)</span><span id="03b4" class="mx la it mt b gy nc mz l na nb">#Results dataframe<br/>cols = ['Case','SGD','Ridge','KNN','SVM','Bagging','RndForest','LogReg','LGB']</span><span id="9308" class="mx la it mt b gy nc mz l na nb">resul = pd.DataFrame(columns=cols)<br/>resul.set_index("Case",inplace=True)<br/>resul.loc['Standard'] = [0,0,0,0,0,0,0,0]<br/>resul.loc['GridSearch'] = [0,0,0,0,0,0,0,0]<br/>resul.loc['RandomSearch'] = [0,0,0,0,0,0,0,0]<br/>resul.loc['Hyperopt'] = [0,0,0,0,0,0,0,0]</span><span id="46a4" class="mx la it mt b gy nc mz l na nb">resul.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/9dda2e53f766036f0684a6df8e483b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*eJoolmH_x-P5BnX4sMr8Xg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">结果的空数据框架</p></figure><h1 id="f43f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第1行—每个算法的默认超参数</h1><p id="5f80" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如上所述，我们的结果表的第一行是分析的起点，采用每个算法的超参数的默认值:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="090c" class="mx la it mt b gy my mz l na nb">#Models creation<br/>sgd   = SGDClassifier()<br/>ridge = RidgeClassifier()<br/>knn   = KNeighborsClassifier()<br/>svc   = SVC(gamma='auto')<br/>bag   = BaggingClassifier()<br/>rf    = RandomForestClassifier(n_estimators=10)<br/>lr    =  LogisticRegression(solver='liblinear')<br/>lgg   = lgb.LGBMClassifier()</span><span id="1e93" class="mx la it mt b gy nc mz l na nb">models = [sgd,ridge,knn,svc,bag,rf,lr,lgg]</span><span id="bda7" class="mx la it mt b gy nc mz l na nb">col = 0<br/>for model in models:<br/>    model.fit(X_train,y_train.values.ravel())<br/>    resul.iloc[0,col] = model.score(X_valid,y_valid)<br/>    col += 1</span><span id="9c57" class="mx la it mt b gy nc mz l na nb">resul.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/eb6f4efac9ddf85d5d0d1977166d1838.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*742QnZvFaJxVOcspuH9urg.png"/></div></figure><h1 id="7683" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第2行—应用GridSearchCV</h1><blockquote class="nw nx ny"><p id="8906" class="lr ls nz lt b lu mn ju lw lx mo jx lz oa mp mc md ob mq mg mh oc mr mk ml mm im bi translated">“网格搜索通过尝试您想要在模型中尝试的每个可能的参数组合来工作。这些参数中的每一个都在一系列交叉验证过程中被尝试。在过去的几年里，这种技术作为一种调整你的模型的方法已经流行起来了,“⁴.</p></blockquote><p id="4ec9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们必须为每个算法定义一个包含不同参数及其值的字典</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="e1bc" class="mx la it mt b gy my mz l na nb">from sklearn.model_selection import GridSearchCV<br/>from sklearn.model_selection import RepeatedStratifiedKFold</span><span id="e6b4" class="mx la it mt b gy nc mz l na nb">#SGD<br/>loss = ['hinge', 'modified_huber', 'log']<br/>penalty = ['l1','l2']<br/>alpha= [0.0001,0.001,0.01,0.1]<br/>l1_ratio= [0.15,0.05,.025]<br/>max_iter = [1,5,10,100,1000,10000]<br/>sgd_grid = dict(loss=loss,penalty=penalty,max_iter=max_iter,alpha=alpha,l1_ratio=l1_ratio)</span><span id="1b4e" class="mx la it mt b gy nc mz l na nb">#Ridge<br/>alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]<br/>ridge_grid = dict(alpha=alpha)</span><span id="da0b" class="mx la it mt b gy nc mz l na nb">#K-Nearest - Neighborg<br/>n_neighbors = range(1, 21, 2)<br/>weights = ['uniform', 'distance']<br/>metric = ['euclidean', 'manhattan', 'minkowski']<br/>knn_grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)</span><span id="0129" class="mx la it mt b gy nc mz l na nb">#Support Vector Classifier<br/>kernel = ['poly', 'rbf', 'sigmoid']<br/>C = [50, 10, 1.0, 0.1, 0.01]<br/>gamma = ['scale']<br/>svc_grid = dict(kernel=kernel,C=C,gamma=gamma)</span><span id="c95d" class="mx la it mt b gy nc mz l na nb">#Bagging Classifier<br/>n_estimators = [10, 100, 1000]<br/>bag_grid = dict(n_estimators=n_estimators)</span><span id="af82" class="mx la it mt b gy nc mz l na nb">#Random Forest<br/>n_estimators = [10, 100, 1000,10000]<br/>max_features = ['sqrt', 'log2']<br/>rf_grid = dict(n_estimators=n_estimators,max_features=max_features)</span><span id="1fa2" class="mx la it mt b gy nc mz l na nb">#Logistic Regrresion<br/>solvers = ['newton-cg', 'lbfgs', 'liblinear']<br/>penalty = ['l2']<br/>c_values = [100, 10, 1.0, 0.1, 0.01]<br/>lr_grid = dict(solver=solvers,penalty=penalty,C=c_values)</span><span id="08da" class="mx la it mt b gy nc mz l na nb">#LGB<br/>class_weight = [None,'balanced']<br/>boosting_type = ['gbdt', 'goss', 'dart']<br/>num_leaves = [30,50,100,150] #list(range(30, 150)),<br/>learning_rate = list(np.logspace(np.log(0.005), np.log(0.2), base = np.exp(1), num = 10)) #1000<br/>lgg_grid = dict(class_weight=class_weight, boosting_type=boosting_type, num_leaves=num_leaves, learning_rate =learning_rate)</span></pre><p id="3d28" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，对每个应用GridSearchCV:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="8ad3" class="mx la it mt b gy my mz l na nb">models = [sgd,ridge,knn,svc,bag,rf,lr,lgg]<br/>grids = [sgd_grid,ridge_grid,knn_grid,svc_grid,bag_grid,rf_grid,lr_grid,lgg_grid]</span><span id="dd05" class="mx la it mt b gy nc mz l na nb">col = 0<br/>for ind in range(0,len(models)):<br/>    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, <br/>                                 random_state=1)</span><span id="f17e" class="mx la it mt b gy nc mz l na nb">    grid_search = GridSearchCV(estimator=models[col], <br/>                  param_grid=grids[col], n_jobs=-1, cv=cv,  <br/>                  scoring='accuracy',error_score=0)</span><span id="bfdb" class="mx la it mt b gy nc mz l na nb">    grid_clf_acc = grid_search.fit(X_train, y_train)<br/>    resul.iloc[1,col] = grid_clf_acc.score(X_valid,y_valid)<br/>    col += 1</span><span id="c548" class="mx la it mt b gy nc mz l na nb">resul.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/d2a0ec3105fe53293b90473089c68e02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*jMqgGsK3UfqbhV_Yh5HthA.png"/></div></figure><p id="2d3b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">不同的算法有更多的参数，这里没有特意包括，您可以在每个超参数中强制搜索更多的值。<br/>这个缩减是根据我们愿意投入的时间和计算能力来发展的。</p><h1 id="402b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第3行—应用RandomSearchCV</h1><blockquote class="nw nx ny"><p id="48fb" class="lr ls nz lt b lu mn ju lw lx mo jx lz oa mp mc md ob mq mg mh oc mr mk ml mm im bi translated">“进入随机搜索。考虑尝试每一种可能的组合需要大量的蛮力计算。数据科学家是一群没有耐心的人，所以他们采用了一种更快的技术:从一系列参数中随机取样。这个想法是，你会比网格搜索更快地覆盖接近最优的参数集。然而，这种技术是幼稚的。它不知道也不记得以前运行过的任何东西。”⁴</p></blockquote><p id="2a6b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">出于实用目的，代码与GridSearchCV相同，只是修改了:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="dd45" class="mx la it mt b gy my mz l na nb">random_search = RandomizedSearchCV(models[col], <br/>                param_distributions=grids[col],n_iter=n_iter_search, <br/>                cv=cv)</span></pre><p id="e81c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">代替</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="f497" class="mx la it mt b gy my mz l na nb">grid_search = GridSearchCV(estimator=lr, param_grid=lr_grid, <br/>              n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)</span></pre><p id="21dd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">完整的代码如下:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="c07b" class="mx la it mt b gy my mz l na nb">from scipy.stats import randint as sp_randint<br/>from sklearn.model_selection import RandomizedSearchCV</span><span id="0092" class="mx la it mt b gy nc mz l na nb">col = 0<br/>for ind in range(0,len(models)):<br/>    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, <br/>                                 random_state=1)<br/>    n_iter_search = 3<br/>    random_search = RandomizedSearchCV(models[col],<br/>    param_distributions=grids[col],n_iter=n_iter_search, cv=cv)</span><span id="8c4b" class="mx la it mt b gy nc mz l na nb">    random_search.fit(X_train,y_train)<br/>    resul.iloc[2,col] = random_search.score(X_valid,y_valid)<br/>    col += 1</span><span id="69de" class="mx la it mt b gy nc mz l na nb">resul.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/4a99f7e51b5282b2404c78a0083c1447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*k2FDUwFHQjUC5_54dnLGSQ.png"/></div></figure><h1 id="c2ba" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">分析结果</h1><p id="e0d7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">到目前为止，如果我们检查结果网格，我们发现自动搜索技术的应用已经给出了良好的结果。<br/>在某些情况下，像SGD或SVM算法已经有了很大的改进，从67–69%的下限提高到78–76%。<br/>总的趋势是提高1、2或3个百分点，GridSearchCV比RandomSearchCV获得更好的结果，随机应用时间比网格应用时间好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/494a0ecfb43618320954a219a4d53afd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*HgM_lnb6Xb9zidc4IbJj1Q.png"/></div></figure><p id="f54e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们来分析一下他版本的GridSearchCV中的获胜算法(光照渐变提升):</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="623c" class="mx la it mt b gy my mz l na nb">cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3,  <br/>                             random_state=1)<br/>n_iter_search = 3<br/>grid_search = GridSearchCV(estimator=lgg, param_grid=lgg_grid, <br/>              n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)<br/>grid_win = grid_search.fit(X_train, y_train)</span><span id="2fde" class="mx la it mt b gy nc mz l na nb">#Predict values based on new parameters<br/>yv_pred = grid_win.predict(X_valid)</span><span id="b1ee" class="mx la it mt b gy nc mz l na nb">print(confusion_matrix(y_valid, yv_pred))<br/>skplt.metrics.plot_confusion_matrix(y_valid, yv_pred,figsize=(8,8))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1049d556bd83ba687733a31ac85156ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*ymkWp5K5oZJM7HZxFhBSgg.png"/></div></figure><p id="02fa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要了解我们使用的算法版本中的超参数:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a173" class="mx la it mt b gy my mz l na nb">print("Best: %f using %s" % (grid_win.best_score_, grid_win.best_params_))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/32ae3cb14efb6991fb1dcad46116c4fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*varZlJAmiuQftr6eOj4L8w.png"/></div></div></figure><h1 id="2d7e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第4行——使用hyperopt库应用贝叶斯超优化</h1><p id="6c1c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我必须承认，在我理解如何应用贝叶斯优化技术之前，我感到非常失落和困惑，几乎要放弃了。我可以参考的来自图书馆和其他文章的文档和例子都很模糊，或者太枯燥，或者非常过时。<br/>所有这一切，直到我发现这个<a class="ae ky" rel="noopener" target="_blank" href="/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a">优秀的图图利亚</a> l⁵和它的<a class="ae ky" href="https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb" rel="noopener ugc nofollow" target="_blank">代号</a> ⁶由<a class="ae ky" href="https://towardsdatascience.com/@williamkoehrsen?source=post_page-----dfda59b72f8a----------------------" rel="noopener" target="_blank">威尔·科尔森</a>所作，我建议你一步一步地回顾和尝试，因为它是清晰和详尽的。跟着他，我们要做的第一件事是定义我们的目标函数，它必须返回一个至少带有标签‘loss’和‘status’的字典。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="3198" class="mx la it mt b gy my mz l na nb">import csv<br/>from hyperopt import STATUS_OK<br/>from timeit import default_timer as timer</span><span id="e624" class="mx la it mt b gy nc mz l na nb">MAX_EVALS = 500<br/>N_FOLDS = 10</span><span id="eefb" class="mx la it mt b gy nc mz l na nb">def objective(params, n_folds = N_FOLDS):</span><span id="cf0d" class="mx la it mt b gy nc mz l na nb">   """Objective function for Gradient Boosting Machine Hyperparameter Optimization"""</span><span id="4324" class="mx la it mt b gy nc mz l na nb">   # Keep track of evals<br/>   global ITERATION<br/>   ITERATION += 1</span><span id="6fb3" class="mx la it mt b gy nc mz l na nb">   # Retrieve the subsample if present otherwise set to 1.0<br/>   subsample = params['boosting_type'].get('subsample', 1.0)</span><span id="e769" class="mx la it mt b gy nc mz l na nb">   # Extract the boosting type<br/>   params['boosting_type'] = params['boosting_type']<br/>         ['boosting_type']<br/>   params['subsample'] = subsample</span><span id="f048" class="mx la it mt b gy nc mz l na nb">   # Make sure parameters that need to be integers are integers<br/>   for parameter_name in ['num_leaves', 'subsample_for_bin', <br/>                          'min_child_samples']:<br/>       params[parameter_name] = int(params[parameter_name])</span><span id="65b1" class="mx la it mt b gy nc mz l na nb">   start = timer()<br/>   # Perform n_folds cross validation<br/>   cv_results = lgb.cv(params, train_set, num_boost_round = 10000, <br/>                       nfold = n_folds, early_stopping_rounds = 100, <br/>                       metrics = 'auc', seed = 50)<br/>   run_time = timer() - start</span><span id="89ad" class="mx la it mt b gy nc mz l na nb">   # Extract the best score<br/>   best_score = np.max(cv_results['auc-mean'])</span><span id="e4ac" class="mx la it mt b gy nc mz l na nb">   # Loss must be minimized<br/>   loss = 1 - best_score</span><span id="55ea" class="mx la it mt b gy nc mz l na nb">   # Boosting rounds that returned the highest cv score<br/>   n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)</span><span id="4b10" class="mx la it mt b gy nc mz l na nb">   # Write to the csv file ('a' means append)<br/>   of_connection = open(out_file, 'a')<br/>   writer = csv.writer(of_connection)<br/>   writer.writerow([loss, params, ITERATION, n_estimators, <br/>                   run_time])</span><span id="b5f3" class="mx la it mt b gy nc mz l na nb">   # Dictionary with information for evaluation<br/>   return {'loss': loss, 'params': params, 'iteration': ITERATION,<br/>           'estimators': n_estimators, 'train_time': run_time, <br/>           'status': STATUS_OK}</span></pre><blockquote class="nw nx ny"><p id="8a0d" class="lr ls nz lt b lu mn ju lw lx mo jx lz oa mp mc md ob mq mg mh oc mr mk ml mm im bi translated">域空间:域空间表示我们想要为每个超参数评估的值的范围。每次搜索迭代，贝叶斯优化算法将从域空间中为每个超参数选择一个值。当我们进行随机或网格搜索时，域空间是一个网格。在贝叶斯优化中，想法是相同的，除了这个空间对于每个超参数具有<em class="it">概率分布</em>而不是离散值。</p></blockquote><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="5988" class="mx la it mt b gy my mz l na nb">space = {<br/>'class_weight': hp.choice('class_weight', [None, 'balanced']),<br/>'boosting_type': hp.choice('boosting_type', [<br/>{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)},<br/>{'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},<br/>{'boosting_type': 'goss', 'subsample': 1.0}]),</span><span id="8116" class="mx la it mt b gy nc mz l na nb">'num_leaves': hp.quniform('num_leaves', 30, 150, 1),<br/>'learning_rate': hp.loguniform('learning_rate', np.log(0.01), <br/>                                                np.log(0.2)),<br/>'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, <br/>                                                      300000),<br/>'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),<br/>'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),<br/>'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),<br/>'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)}</span></pre><p id="b19a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里使用不同的域分发类型(在<a class="ae ky" href="https://github.com/hyperopt/hyperopt/wiki/FMin" rel="noopener ugc nofollow" target="_blank">hyperopt文档</a>中寻找完整的分发列表):</p><ul class=""><li id="a015" class="ne nf it lt b lu mn lx mo ma ng me nh mi ni mm nn nk nl nm bi translated"><code class="fe og oh oi mt b">choice</code>:分类变量</li><li id="f695" class="ne nf it lt b lu no lx np ma nq me nr mi ns mm nn nk nl nm bi translated"><code class="fe og oh oi mt b">quniform</code>:离散均匀(间隔均匀的整数)</li><li id="54c2" class="ne nf it lt b lu no lx np ma nq me nr mi ns mm nn nk nl nm bi translated"><code class="fe og oh oi mt b">uniform</code>:连续均匀(浮动间隔均匀)</li><li id="fc5c" class="ne nf it lt b lu no lx np ma nq me nr mi ns mm nn nk nl nm bi translated"><code class="fe og oh oi mt b">loguniform</code>:连续的均匀圆木(在圆木刻度上均匀分布的浮子)</li></ul><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="49ff" class="mx la it mt b gy my mz l na nb">from hyperopt import tpe<br/>from hyperopt import Trials</span><span id="588e" class="mx la it mt b gy nc mz l na nb"># optimization algorithm<br/>tpe_algorithm = tpe.suggest</span><span id="074b" class="mx la it mt b gy nc mz l na nb"># Keep track of results<br/><strong class="mt iu">bayes_trials = Trials()</strong></span><span id="d12e" class="mx la it mt b gy nc mz l na nb"># File to save first results<br/>out_file = './gbm_trials.csv'<br/>of_connection = open(out_file, 'w')<br/>writer = csv.writer(of_connection)</span><span id="b0af" class="mx la it mt b gy nc mz l na nb"># Write the headers to the file<br/>writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])<br/>of_connection.close()</span></pre><p id="1051" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，准备好所有代码后，我们通过fmin函数寻找参数的最佳组合:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="b236" class="mx la it mt b gy my mz l na nb">from hyperopt import fmin</span><span id="05ee" class="mx la it mt b gy nc mz l na nb"># Global variable<br/>global  ITERATION<br/>ITERATION = 0<br/>MAX_EVALS = 100</span><span id="0745" class="mx la it mt b gy nc mz l na nb"># Create a lgb dataset<br/>train_set = lgb.Dataset(X_train, label = y_train)</span><span id="ecf7" class="mx la it mt b gy nc mz l na nb"># Run optimization<br/>best = fmin(fn = objective, space = space, algo = tpe.suggest,<br/>            max_evals = MAX_EVALS, trials = bayes_trials, <br/>            rstate =np.random.RandomState(50))</span></pre><p id="68e5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它触发了寻找最佳组合的过程。在最初的例子中，变量MAX_EVALS被设置为500；由于本练习的性能问题，它被减少到100，这可能会影响最终结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/476f1ba4e4df04bd3c325ee268cb63de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g-FBaeZO__dffW1vg-6Cgg.png"/></div></div></figure><p id="997f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一旦流程结束，我们就可以获取Trials对象(在我们的例子中是bayes_trials)并分析其结果:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="2eb2" class="mx la it mt b gy my mz l na nb"># Sort the trials with lowest loss (highest AUC) first<br/>bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])<br/>bayes_trials_results[:2]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/0aa31379857b93a97718371fd201a186.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*hCgYD2e4ZKFQRrP30mjXOQ.png"/></div></figure><p id="942d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们还可以从CSV加载数据帧，并使用“ast”库将文本转换为字典，并为我们的最终模型提供最佳结果。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="6457" class="mx la it mt b gy my mz l na nb">results = pd.read_csv('./gbm_trials.csv')</span><span id="2b8d" class="mx la it mt b gy nc mz l na nb"># Sort with best scores on top and reset index for slicing<br/>results.sort_values('loss', ascending = True, inplace = True)<br/>results.reset_index(inplace = True, drop = True)<br/>results.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/2015825c42f943800aaace73fedd6b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*xgwaKPbD7i3ulmybobiKlA.png"/></div></figure><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="f48d" class="mx la it mt b gy my mz l na nb">import ast</span><span id="68a9" class="mx la it mt b gy nc mz l na nb"># Convert from a string to a dictionary<br/>ast.literal_eval(results.loc[0, 'params'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/e7df2c00e2569294b5cc471dd77ce04f.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*Bodl5ddN7G16qMxWGFnCaQ.png"/></div></figure><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="f343" class="mx la it mt b gy my mz l na nb"># Extract the ideal number of estimators and hyperparameters<br/>best_bayes_estimators = int(results.loc[0, 'estimators'])<br/>best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()</span><span id="9262" class="mx la it mt b gy nc mz l na nb"># Re-create the best model and train on the training data<br/>best_bayes_model = lgb.LGBMClassifier(n_estimators=best_bayes_estimators, n_jobs = -1,<br/>                   objective = 'binary', random_state = 7,<br/>                   **best_bayes_params)<br/>best_bayes_model.fit(X_train, y_train)</span><span id="24b3" class="mx la it mt b gy nc mz l na nb"># Evaluate on the testing data<br/>preds = best_bayes_model.predict(X_valid)<br/>print(confusion_matrix(y_valid, preds))<br/>print (best_bayes_model.score(X_valid,y_valid))<br/>skplt.metrics.plot_confusion_matrix(y_valid, preds,figsize=(8,8))</span><span id="e1d0" class="mx la it mt b gy nc mz l na nb">resul.iloc[3,7] = best_bayes_model.score(X_valid,y_valid)<br/>resul.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/8512269479eea8f44fa6d8bfe6bb4a16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bDjJenl6xxZXdjmqFdVO-A.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/d159c7dd363b998c82122edc2a61a4cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*fi7zpdGez-iJ67ljAoSgDA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">远视在LGB模型中的应用</p></figure><h1 id="0384" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><p id="5d89" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们开发了完整的代码来应用三种可用的优化技术，八种不同的分类算法。应用这些技术所需的时间和计算能力是一个需要考虑的问题；当所选模型的技术水平足够高时，似乎有必要执行这种优化。<br/>参数优化并不一定意味着训练模型的结果相对于测试数据的持续改进，因为参数的选择可能会产生过度拟合。<br/>最后，应该指出的是，在不同模型上观察到的改进通常具有相当大的幅度，这至少为通过这些技术中的任何一种来提高算法性能的可能性留下了余地。</p><h1 id="2cb0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">来源和参考</h1><p id="eda5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://en.wikipedia.org/w/index.php?title=Accuracy_and_precision&amp;action=edit&amp;section=4" rel="noopener ugc nofollow" target="_blank">[1]https://en.wikipedia.org/w/index.php?title = Accuracy _ and _ precision&amp;action =编辑&amp; section=4 </a></p><p id="1d5d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="nz">在二进制分类中，准确度也用作一种统计度量，用于衡量</em> <a class="ae ky" href="https://en.wikipedia.org/wiki/Binary_classification" rel="noopener ugc nofollow" target="_blank"> <em class="nz">二进制分类</em> </a> <em class="nz">测试正确识别或排除某个条件的程度。即准确率是真实结果(既有</em> <a class="ae ky" href="https://en.wikipedia.org/wiki/True_positive" rel="noopener ugc nofollow" target="_blank"> <em class="nz">真阳性</em> </a> <em class="nz">又有</em> <a class="ae ky" href="https://en.wikipedia.org/wiki/True_negative" rel="noopener ugc nofollow" target="_blank"> <em class="nz">真阴性</em> </a> <em class="nz">)占检查病例总数的比例……量化二进制准确率的公式为:</em></p><p id="3a32" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="nz">准确度= (TP+TN)/(TP+TN+FP+FN) </em></p><p id="2bf5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="nz">其中:TP =真阳性；FP =假阳性；TN =真阴性；FN =假阴性</em></p><p id="2464" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[2]<a class="ae ky" href="https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/hyperparameters-for-class ification-machine-learning-algorithms/</a></p><p id="bf9a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[3]<a class="ae ky" rel="noopener" target="_blank" href="/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a">https://towards data science . com/automated-machine-learning-hyperparameter-tuning-in-python-dfda 59 b 72 f 8 a</a></p><p id="4f8b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[4]<a class="ae ky" href="https://medium.com/apprentice-journal/hyper-parameter-optimization-c9b78372447b" rel="noopener">https://medium . com/apprentice-journal/hyper-parameter-optimization-c9b 78372447 b</a></p><p id="3d30" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[5]<a class="ae ky" rel="noopener" target="_blank" href="/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a">https://towards data science . com/automated-machine-learning-hyperparameter-tuning-in-python-dfda 59 b 72 f 8 a</a></p><p id="22f8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[6]<a class="ae ky" href="https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian % 20 hyperparameter % 20 optimization % 20 of % 20 gradient % 20 boosting % 20 machine . ipynb</a></p></div></div>    
</body>
</html>