<html>
<head>
<title>Cross-entropy method for Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的交叉熵方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cross-entropy-method-for-reinforcement-learning-2b6de2a4f3a0?source=collection_archive---------7-----------------------#2020-02-19">https://towardsdatascience.com/cross-entropy-method-for-reinforcement-learning-2b6de2a4f3a0?source=collection_archive---------7-----------------------#2020-02-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/3c5b9e79675b85a3823651ad1c9db27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Isi0Kz-fG8roySu8"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kf" href="https://unsplash.com/@franckinjapan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Franck V. </a>拍摄的照片</p></figure><p id="0e7d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你曾经鼓起勇气探索强化学习领域，很有可能你会发现自己迷失在花哨的词汇中。大词，复杂算法的名字，背后有更复杂的数学。但是，如果有更简单、更直观、效率更高的算法能够很好地工作呢？</p><p id="2b0b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">来看看交叉熵方法:一种用于参数化政策优化的进化算法，John Schulman 声称这种算法在复杂的 RL 问题上“令人尴尬地好”。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h2 id="13de" class="ll lm it bd ln lo lp dn lq lr ls dp lt kr lu lv lw kv lx ly lz kz ma mb mc md bi translated">什么是交叉熵方法？</h2><p id="599b" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">从生物学的角度来看，这是一种 T4 进化算法。一些个体是从群体中抽样出来的，只有最优秀的个体才能决定后代的特征。</p><p id="4e7b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在数学上，它可以被视为一种<em class="mj">无导数优化</em> (DFO)技术，即，它可以在没有计算导数的开销的情况下找到最优解(无反向传播！).</p><h2 id="9e71" class="ll lm it bd ln lo lp dn lq lr ls dp lt kr lu lv lw kv lx ly lz kz ma mb mc md bi translated">这种方法是如何工作的？</h2><p id="d3a4" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">假设您不知道什么是代理、环境和策略。你只是得到了一个“黑匣子”，它接受一些数字作为输入，输出一些其他的数字。您只能选择输入值并观察输出。<em class="mj">如何猜测输入，使输出成为您想要的值？</em></p><p id="0000" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一种简单的方法是获取一组输入，观察产生的输出，选择产生最佳输出的输入，并对其进行调整，直到您对看到的输出满意为止。这本质上就是交叉熵方法所做的。</p><h2 id="6af9" class="ll lm it bd ln lo lp dn lq lr ls dp lt kr lu lv lw kv lx ly lz kz ma mb mc md bi translated">那么，我该如何用它来解决我的 RL 问题呢？</h2><p id="519b" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">让我们通过一个例子来逐步理解 CEM 的工作原理。为了更好地理解实现，我在每个步骤中都添加了一些 python 代码片段。代码大量借用了 Udacity 关于深度强化学习的课程(惊人的 python RL 资源 btw，本文末尾 Github 链接)。</p><p id="e687" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑你的政策网络。你想要找到最佳的权重，它可以基于你的代理的状态采取正确的“有意义的”行动。用于找到这些权重的基于 CEM 的方法如下:</p><p id="809e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">第一步:</strong>从随机分布中抽取一串初始权重。尽管通常选择高斯分布，但您可以选择您认为权重来自的任何分布。假设我从具有均值<strong class="ki iu"> μ </strong>和方差<strong class="ki iu"> σ </strong>的高斯分布中抽取了 10 个权重候选值<strong class="ki iu"> w1 </strong>、<strong class="ki iu"> w2 </strong>、…、<strong class="ki iu"> w10 </strong>。</p><p id="c032" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑μ=0，σ=1，n_weights=10(候选数)，weights_dim 表示权重向量的维数。</p><pre class="mk ml mm mn gt mo mp mq mr aw ms bi"><span id="af25" class="ll lm it mp b gy mt mu l mv mw">mean = 0.0       <br/>std = 1.0<br/>n_weights = 10</span><span id="6a56" class="ll lm it mp b gy mx mu l mv mw">weights_pop = [mean + std*np.random.randn(weights_dim) for i_weight in range(n_weights)]</span></pre><p id="210d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">步骤 2: </strong>现在让代理根据这些权重从策略网络中选择行动，让代理运行一集并收集环境生成的奖励。对于我们的例子，比如说<strong class="ki iu"> w1 </strong>产生一个累积奖励<strong class="ki iu"> r1 </strong>，<strong class="ki iu"> w2 </strong>产生<strong class="ki iu"> r2 </strong>等等。</p><p id="21ca" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用于代理的评估方法将权重候选作为输入，播放一集并输出来自该集的累积奖励。</p><pre class="mk ml mm mn gt mo mp mq mr aw ms bi"><span id="43fb" class="ll lm it mp b gy mt mu l mv mw">rewards = [agent.evaluate(weights) for weights in weights_pop]</span></pre><p id="d27c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第三步:找出产生最佳回报的权重。假设最佳的 4 种重量是<strong class="ki iu"> w1 </strong>、<strong class="ki iu"> w2 </strong>、<strong class="ki iu"> w5 </strong>和<strong class="ki iu"> w6 </strong>(也称为“精英”重量)。这里 4 是我们选择的一个数字。一般来说，你考虑最好的<em class="mj"> n </em>权重，其中<em class="mj"> n </em>由你选择。</p><pre class="mk ml mm mn gt mo mp mq mr aw ms bi"><span id="607d" class="ll lm it mp b gy mt mu l mv mw">n_elite = 4<br/>elite_idxs = np.array(rewards).argsort()[-n_elite:]<br/>elite_weights = [weights_pop[idx] for idx in elite_idxs]</span></pre><p id="74a3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">步骤 4: </strong>从<em class="mj">精英</em>权重定义的分布中选择新权重。说<strong class="ki iu"> μ' </strong>是最佳权重的平均值(<strong class="ki iu"> w1 </strong>、<strong class="ki iu"> w2 </strong>、<strong class="ki iu"> w5 </strong>和<strong class="ki iu">w6</strong>)<strong class="ki iu">σ'</strong>是它们的方差。我们现在从具有平均值<strong class="ki iu">μ’</strong>和方差<strong class="ki iu">σ’</strong>的高斯分布中抽取 10 名候选人。</p><pre class="mk ml mm mn gt mo mp mq mr aw ms bi"><span id="7891" class="ll lm it mp b gy mt mu l mv mw">mean = np.array(elite_weights).mean()<br/>std = np.array(elite_weights).std()</span><span id="40ac" class="ll lm it mp b gy mx mu l mv mw">weights_pop = [mean + std*np.random.randn(weights_dim) for i_weight in range(n_weights)]</span></pre><p id="4a79" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第五步:重复第二步到第四步，直到你对得到的奖励感到满意。</p><p id="83b4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你不喜欢 python 代码，而你喜欢用数学术语阅读算法，这里有一些伪代码:</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi my"><img src="../Images/55a0d3496ece83c966efac30f13b7e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6FKxrEwGIqgEurmwA881Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">鸣谢:约翰·舒尔曼关于深度强化学习的 MLSS 2016</p></figure></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h2 id="72ac" class="ll lm it bd ln lo lp dn lq lr ls dp lt kr lu lv lw kv lx ly lz kz ma mb mc md bi translated">结论</h2><p id="d80a" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">交叉熵方法是一种简单的算法，可以用来训练 RL 代理。在包括 Tetris⁴.游戏在内的著名任务上，该方法已经胜过了几种 RL 技术在转向更复杂的 RL 算法(如 PPO、A3C 等)之前，您可以将此作为基线。CEM 有几种变体，但是，本文中定义的结构是所有变体的主干。</p><p id="e53f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇关于交叉熵强化学习方法的文章到此结束。我希望您喜欢您刚刚阅读的内容，并感谢您的宝贵时间。</p><h2 id="2a21" class="ll lm it bd ln lo lp dn lq lr ls dp lt kr lu lv lw kv lx ly lz kz ma mb mc md bi translated">参考</h2><p id="d657" class="pw-post-body-paragraph kg kh it ki b kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">[1]<a class="ae kf" href="https://github.com/udacity/deep-reinforcement-learning/tree/master/cross-entropy" rel="noopener ugc nofollow" target="_blank">https://github . com/uda city/deep-reinforcement-learning/tree/master/cross-entropy</a></p><p id="c7b8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2] MLSS 2016 关于深度强化学习由约翰·舒尔曼(<a class="ae kf" href="https://www.youtube.com/watch?v=aUrX-rP_ss4" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=aUrX-rP_ss4</a>)</p><p id="bfa5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]http://karpathy.github.io/2016/05/31/rl/<a class="ae kf" href="http://karpathy.github.io/2016/05/31/rl/" rel="noopener ugc nofollow" target="_blank"/></p><p id="ac9a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4] <em class="mj"> </em> I. Szita 和 A. Lorincz，<a class="ae kf" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">用有噪声的交叉熵方法学习俄罗斯方块</a> (2006)，神经计算</p></div></div>    
</body>
</html>