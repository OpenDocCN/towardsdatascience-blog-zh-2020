<html>
<head>
<title>How to detect and deal with Multicollinearity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何检测和处理多重共线性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-detect-and-deal-with-multicollinearity-9e02b18695f1?source=collection_archive---------5-----------------------#2020-06-06">https://towardsdatascience.com/how-to-detect-and-deal-with-multicollinearity-9e02b18695f1?source=collection_archive---------5-----------------------#2020-06-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="64f6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于识别和修复数值变量多重共线性的详细说明</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a453e9e6e8a129bf6ca7e194357626c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AY_sT3Bw9UVmhctBqTtUoA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由Jaxon Lott在Unsplash上拍摄</p></figure><p id="cfca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">多重共线性是需要排除的主要假设之一，以获得任何回归模型的更好估计</em> ✌️</p><p id="c6c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将通过一个示例数据集来介绍多重共线性的影响、如何识别以及何时解决这个问题。</p><p id="048a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">什么是相关性？</strong></p><p id="a9e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">两个变量之间的相关性可以用范围在-1到1之间的相关系数来衡量。如果值为0，则两个变量是独立的，没有相关性。如果测量值非常接近这些值中的一个，则表示线性关系，并且彼此高度相关。这意味着一个变量的变化与其他变量的显著变化相关联。<br/>相关性可以用<a class="ae lv" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">皮尔逊相关系数</a>和<a class="ae lv" href="https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">斯皮尔曼秩序系数</a>来计算。</p><p id="c64e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">什么是多重共线性？</strong></p><p id="640e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当回归分析中的独立变量之间存在高度相关性时，会发生多重共线性，这会影响对结果的整体解释。它降低了系数的功效，削弱了统计测量以信任p值来识别显著的独立变量。因此，我们无法检验自变量对因变量的个别解释。</p><p id="536e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">何时修复多重共线性？</strong></p><p id="59d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好消息是修复多重共线性并不总是强制性的。这完全取决于回归模型的主要目标。</p><p id="1d20" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">多重共线性的程度会极大地影响p值和系数，但不会影响预测和拟合优度检验。如果您的目标是执行预测，并且没有必要理解自变量的重要性，则没有必要解决多重共线性问题。</p><p id="5f43" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉<strong class="la iu">如何检验多重共线性？</strong></p><ol class=""><li id="97a7" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu">相关矩阵/相关图</strong></li><li id="02b4" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">变动通货膨胀系数(VIF) </strong></li></ol><p id="2ec0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">相关图可用于确定两个独立变量之间的相关性或双变量关系，而VIF用于确定一个独立变量与一组其他变量的相关性。因此，为了更好地理解，最好使用VIF。</em></p><p id="ce76" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">VIF = 1 →不相关<br/> VIF = 1比5 →中度相关<br/> VIF &gt; 10 →高度相关</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="eab7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们从代码开始…..</p><p id="568a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是数据集[ <a class="ae lv" href="https://github.com/SushmithaPulagam/Fixing-Multicollinearity" rel="noopener ugc nofollow" target="_blank">房屋销售</a> ]的链接</p><p id="67a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据集中的要素如下。销售价格是目标变量，其余为独立特征。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/09d77b31e696dd12de4f74175fe15d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*flGXkA9EEMwA_29-XsYTMA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集的前几行</p></figure><p id="8032" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">用数值变量识别多重共线性</strong></p><p id="0c8d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们将计算每个独立变量的VIF分数。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="103c" class="mx my it mt b gy mz na l nb nc">def vif_scores(df):<br/>    VIF_Scores = pd.DataFrame()<br/>    VIF_Scores["Independent Features"] = df.columns<br/>    VIF_Scores["VIF Scores"] = [variance_inflation_factor(df.values,i) for i in range(df.shape[1])]<br/>    return VIF_Scores</span><span id="728c" class="mx my it mt b gy nd na l nb nc">df1 = df.iloc[:,:-1]<br/>vif_scores(df1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/1256fe3d2feea60a487714c574b1bbae.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*hJGreWL1P5oV7hxouwOTLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个独立功能的VIF</p></figure><p id="a354" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大多数变量的VIF分数都高于10。如果我们使用此数据集构建回归模型，单个系数和p值将受到很大影响。我们将继续讨论如何解决这个问题。</p><p id="be34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">修复多重共线性—丢弃变量</strong></p><p id="9a1b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将考虑删除具有高VIF值的房间的特征<em class="lu">室内(平方英尺)</em>和<em class="lu"> #，因为相同的信息被其他变量捕获。此外，它还有助于减少数据集中的冗余。</em></p><p id="a290" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们比较一下去掉VIF值前后的VIF值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/76eebcbf3ca4828ce722ad98ec8f0d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*HmypHAiQ4n7jYI2TjumMTw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">突出显示的特征被移除并计算VIF分数</p></figure><p id="dfbb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上文中，我们可以注意到，在去掉高价值(<em class="lu">室内(平方英尺)</em>和<em class="lu"/>房间数量)【VIF特征】后，其他变量的VIF得分也降低了。</p><p id="de5a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">修复多重共线性—组合变量</strong></p><p id="74e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我们可以观察到床的数量和洗澡的数量可以合并为一个变量，这有助于我们从这两个变量中获取更多信息。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="2d9b" class="mx my it mt b gy mz na l nb nc">df5 = df4.copy()<br/>df5['Total Rooms'] = df4.apply(lambda x: x['# of Bed'] + x['# of Bath'],axis=1)<br/>X = df5.drop(['# of Bed','# of Bath'],axis=1)<br/>vif_scores(X)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/72c561b6a980599849a0c58a3299e5a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*GmjAaab6tgNfPJBFtThXXg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">床的数量和浴室的数量组合成一个单一的功能房间总数</p></figure><p id="9993" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上面，我们可以注意到所有三个变量(公寓费、税和总房间数)都达到了令人满意的VIF值，我们可以进一步建立回归模型。</p><p id="1064" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">总结</strong></p><p id="779d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们学习了如何识别和修复回归分析中数值的多重共线性问题。这是一个迭代过程，我们还需要领域知识来决定对哪些变量采取适当的行动。还有其他技术，如PCA和正则化方法也可以解决这个问题。</p><p id="9104" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请访问我的GitHub链接获取完整的[ <a class="ae lv" href="https://github.com/SushmithaPulagam/Fixing-Multicollinearity" rel="noopener ugc nofollow" target="_blank">代码</a></p><p id="8fdf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读，快乐学习！🙂</p></div></div>    
</body>
</html>