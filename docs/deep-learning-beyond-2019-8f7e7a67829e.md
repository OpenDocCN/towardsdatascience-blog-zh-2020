# 深度学习超越 2019

> 原文：<https://towardsdatascience.com/deep-learning-beyond-2019-8f7e7a67829e?source=collection_archive---------29----------------------->

*进步于* ***【缓慢】有意识任务解决*** *？*

![](img/8915d7c32a188f922084534b8b1a4c86.png)

这个图是由[yo shua beng io(neur IPS 2019 talk](https://youtu.be/FtUbMG3rlFs))、[*Yann le Cun*](https://youtu.be/A7AnCvYDQrU)*和*[*Leon Bottou*](https://youtu.be/lbZNQt0Q5HA)*最近的谈话综合而成。缩略词****【IID】****在图中展开为独立同分布的随机变量；****OOD****展开为不分配*

# TL；速度三角形定位法(dead reckoning)

1.  **自我监督学习——通过预测输入进行学习**
2.  **在分布式表示中利用组合的力量**
3.  **降 IID *(独立同分布随机变量)*假设**
4.  **自我监督表示学习方法**
5.  **注意力的作用**
6.  **多时间尺度的终身学习**
7.  **架构先验**

W 虽然深度学习 *(DL)* 模型在 2019 年继续创造新纪录，在各种各样的任务中表现出最先进的性能，特别是在自然语言处理方面，2019 年标志着不仅要审查问题 ***深度学习 1.0 之外是什么？*** 出现在公众话语中，对这个问题的研究也加快了步伐。

# 深度学习 1.0 —局限性的快速回顾

深度学习 1.0 *(* [*本吉奥教授称之为系统 1 深度学习*](https://youtu.be/T3sxeTgT4qc) *)* 在解决人类可以直观解决的任务方面取得了成功，通常是以快速无意识、非语言的方式。例子是直觉，一个特定的游戏走法是好的，或者一张图片包含一只狗——我们在不到一秒钟的时间内快速解决这些任务。我们习惯性解决的任务也属于这一类。

尽管 DL 1.0 模型的性能在某些任务基准中超越了人类，但即使在它们目前可以很好执行的任务中，也存在一些已知的缺陷

*   **DL 1.0 车型比美国**需要更多的训练数据/时间。例如，一个模型必须经历 200 年的等效实时训练才能掌握一款名为[星际争霸 2](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii)的策略游戏。我们平均 20 小时就能学会开车而不出事故。迄今为止，我们还没有能够完全自动驾驶的汽车，尽管在数小时的训练数据中暴露出比我们多几个订单。此外，对于许多任务，模型需要来自人类的标记数据来学习概念

![](img/f2814ee199f3254d7a675525b3895248.png)

图来自 Yann Lecun 最近的演讲(2019 年 11 月)[“基于能量的自我监督学习”。](https://youtu.be/A7AnCvYDQrU)加强了一些游戏的学习训练时间，在这些游戏中，模型的表现达到或超过了专业人类玩家。

*   **DL 1.0 车型会犯我们通常不会犯的错误**。例如，改变图像中的几个像素*(我们的眼睛甚至不会注意到)*会导致模型将其错误分类。站在电话附近的人可能会使模型认为[人正在打电话](https://youtu.be/lbZNQt0Q5HA)。这些错误似乎源于多种原因— (1)模型在某些情况下产生虚假关联(2)输入数据中的偏差导致模型输出被污染(3)模型对分布变化缺乏鲁棒性，或者在某些情况下无法处理属于训练分布一部分的罕见实例。

![](img/034f609e913a8def128f84ea858329c1.png)

DL 1.0 模型所犯的不同类型的错误。 **(a)** 在左上角的图中，注入了我们甚至没有察觉到的噪声，导致了模型的误分类。[图片来自 2015 年关于对抗性 e](https://arxiv.org/pdf/1412.6572.pdf) 例子的论文。 **(b)** 右图中的错误是因为该模型在很大程度上暴露给了在电话亭附近打电话的人——所以这是由于训练数据集中的选择偏差。图片来自 Leon Bottou 在 2019 年 10 月发表的关于[使用因果不变性学习表示法](https://youtu.be/lbZNQt0Q5HA) **(c)** 经常观察到的错误是模型无法将训练分布数据以外的数据推广到分布之外的数据，或者只是属于训练分布一部分的罕见事件。黑天鹅效应(下图)的一个具体例子是一辆无人驾驶汽车暴露在一个罕见的事件中(从训练数据分布的角度来看，这不太可能发生)。图片来自 [Yoshua Bengio 的 NeurIPS 2019 演讲](https://youtu.be/FtUbMG3rlFs)

# 我们如何更接近人类级别的 AI？

迄今为止，答案尚不明确。具体来说，我们如何解决 DL 1.0 现有的局限性，同时解决更困难的有意识的任务解决问题？一种看起来很有希望的方法是从人类那里获得灵感，这些人除了在无意识任务解决中没有 DL 1.0 *(样本效率低下，无法归纳出不符合分布的数据)*的限制之外，还擅长有意识任务解决 *(* [*)系统 2 任务*](https://youtu.be/T3sxeTgT4qc) *)* 如逻辑推理、规划等。

下面列出了一些研究方向*(合理的方法、假设和先验——其中一些已经在早期的小规模实施中实现)*，它们可能会让我们进入深度学习 2.0 *(有意识的任务解决)*。

1.  **自我监督学习——通过预测输入进行学习**
2.  **在分布式表示中利用组合的力量**
3.  **降 IID *(独立同分布随机变量)*假设**
4.  **自我监督表示学习的两种方法**
5.  **注意力的作用**
6.  **多时间尺度的终身学习**
7.  **架构优先**

下面详细讨论的这些研究方向表明，考虑到正在考虑的方法的本质，有意识解决任务的途径很可能同时克服上述 DL 1.0 的缺陷。

1.  **自我监督学习——通过预测输入进行学习**

自我监督学习本质上是通过对来自任何其他部分的输入数据的任何部分进行预测来进行学习。预测可以是关于输入数据序列*(在时间或空间上)*中的下一个是什么，或者一般来说是其中丢失了什么。输入数据可以是一种或多种类型——图像、音频、文本等。学习是通过重建输入中丢失的部分来进行的。

我们的大部分学习是自我监督的。几年前 Geoffrey Hinton 的《信封背面的推理》抓住了这一点(他称之为“无监督的”,尽管现在我们也称之为“自我监督的”,以表明我们通过重建输入数据来监督我们自己的学习)

![](img/dd1e7c2c909265afdc0f23090a3bcd09.png)

自我监督学习的感觉数据流的价值，除了其绝对数量*(就每秒训练数据而言)*的价值之外，还有

*   与典型的监督学习*(反馈是每个输入的类别值或几个数字)*或强化学习*(正如模型预测的那样，反馈是偶尔的标量奖励)相比，它向学习者提供了更多的反馈数据*(如果不是全部，至少是部分输入数据)*。*
*   来自环境的感觉数据流不是静止的。这迫使学习者，或者更具体地，嵌入学习者中的编码器，学习在变化的环境中基本不变的对象和概念的稳定表示。环境固有的不稳定性也提供了了解变化原因的机会。分布外泛化*(预测训练分布中未见的事件)*学习因果对于学习者做出生存所必需的预测是必不可少的。从本质上讲，环境的非静态性质提供了一个通过评估和提炼概念的学习表示以及它们之间的因果关系来持续学习的机会。
*   感觉流包括在学习中起关键作用的主体*(包括学习者)*。代理是环境的一个组成部分，并负责通过干预来改变它。在 DL 1.0 中，代理只是强化学习的一部分。为了让 DL 2.0 模型达到它的目标，让代理参与自我监督学习可能是至关重要的。即使是一个被动的学习者*(一个新生儿)*，在最初的几个月里，他很大程度上是在以最小的互动观察环境，从观察环境中其他代理的互动中学习。

![](img/b9fc8dcc1f3e41784c97ad267fb2cf2c.png)

图自最近 [Yann LeCun 的谈话](https://youtu.be/A7AnCvYDQrU)。新生的孩子凭直觉学习物理——例如，在 9 个月左右，通过观察他们周围的世界来学习重力——我们不会教他们重力。我们知道他们凭直觉理解重力，因为一个简单的汽车从桌子上推下来而不倒的实验(因为我们用一根他们看不见的细线吊着它来欺骗他们)不会让大约 9 个月大的孩子感到惊讶。大约 9 个月后，他们感到惊讶——因为他们的观察与他们 9 个月大的内部世界模型的预测输出不匹配，该模型预测它必须倒下。

*   学习捕捉因果关系的概念的稳定表示，使学习者能够根据其计算能力，通过内部模拟似是而非的行动序列来预测未来的几个时间步骤，并计划未来的奖励行动，避免有害的行动*(就像学习驾驶时避免驶下悬崖)*。

**DL 1.0 中的自我监督学习**

在 DL 1.0 中，自我监督学习已经被证明对于自然语言处理(NLP)任务非常有用和成功。我们有通过预测句子中的下一个单词或预测已经从句子中删除的单词来学习单词表示的模型([伯特](https://arxiv.org/pdf/1810.04805.pdf)——*这在 NLP 世界中被称为无监督预训练，但本质上是自我监督学习，其中模型通过重建输入的缺失部分进行学习*)。然而，DL 1.0 语言建模方法仅从文本输入中学习，而不是在其他感觉流也与代理交互一起考虑的环境中进行基础学习 [*(有一篇 2018 年的论文试图做到这一点*](https://arxiv.org/pdf/1810.08272.pdf) *)。*基于感官环境的语言学习赋予单词更多的上下文和意义，而不仅仅是统计它们的句子上下文*(句子中相对于其他单词的位置)*。由于这个原因，语言学习被限制为仅从文本进行自我监督学习，不仅承受着需要大量文本的负担，而且还将模型的理解限制为仅仅是单词序列的统计特性，而这些统计特性永远无法与从多感觉环境中进行的学习相匹配*(例如，模型永远不会获得空间理解——空间上适合于盒子，反之亦然， 当把“它”与句子中正确的物体联系起来时，需要理解的是:奖杯放不进盒子里，因为* ***它*** *太大了； 奖杯放不进盒子是因为***太小了；***“第一指奖杯，第二指盒子)*。**

**迄今为止，自我监督学习对于图像、视频和音频还没有像对于文本*那样成功(在文本中，模型必须预测单词在词汇上的分布；对于视频中的下一帧预测，在所有可能的下一帧上的分布输出是不可行的)*，尽管在具有 GANs 的[图像完成*【修补】*](https://arxiv.org/pdf/1808.07757.pdf) 、[视频下一帧预测模型](http://openaccess.thecvf.com/content_CVPR_2019/papers/Kwon_Predicting_Future_Frames_Using_Retrospective_Cycle_GAN_CVPR_2019_paper.pdf)中有一些有希望的结果。然而，从有意识的任务解决角度来看，直接在像素、视频和音频的输入空间中预测可能不是正确的方法*(我们不会有意识地在像素级别预测电影中接下来会发生什么——我们的预测是对象/概念的级别)*。与原始输入空间*(视频、音频等)相反，通过感觉模态的输入预测可能最好在抽象表示空间中完成。)*，尽管不同的感官输入对于理解世界有着重要的作用——语言理解是上面已经提到的许多原因之一*(结尾的附加注释考察了语言的特殊性质及其在 DL 2.0 调试中的潜在作用)*。**

****2。在分布式表示中利用组合的力量****

**组合性提供了从有限的元素集合中创建更大的*(指数)*元素组合的能力*。***

**DL 1.0 已经在以下方面利用了组合性的指数力量**

*   **[分布式表示](https://www.quora.com/Deep-Learning-What-is-meant-by-a-distributed-representation/answer/Ajit-Rajasekharan) —分布式表示中的每个特征都可以参与表示所有的概念，考虑到指数组合性。组成制图表达的特征是自动学习的。将分布式表示可视化为实值向量(浮点数/双精度数)使其具体化。向量可以是密集的*(大多数槽具有非零值)*或稀疏的*(在退化的情况下大多数槽为零是一个热码向量)*。**
*   **DL 模型中的每一层计算都允许额外的组合性——每一层的输出都是前一层输出的组合。这种组合性在 DL 1.0 模型中被利用来学习表示的层次*(例如，NLP 模型学习跨不同的不同层捕获不同比例的句法和语义相似性)***
*   **语言支持更高层次的组合性，这在 DL 1.0 中还没有得到充分利用。例如，语言提供了从训练分布中永远不可能得出的原创句子的能力——这并不是说它在训练分布中的概率低——它在训练分布中的概率可能是零。这是一种系统概括的形式，它超越了不符合分布(OOD)的概括。最近的语言模型可以生成连贯的新颖段落，这些段落表现出很大程度的原创性，尽管它们缺乏对潜在概念的理解，特别是当这些段落是由例如工程概念组成时。这种缺陷可能部分是由于前面提到的缺乏[基础语言理解](https://arxiv.org/pdf/1810.08272.pdf)，并且可能在 DL 2.0 中实现。**
*   **组合性不需要局限于仅仅创造新的句子，然而*(尽管语言在某种程度上仍然可以用来描述任何概念)* —它可以是来自先前概念的概念的原始组合，如下图所示。**

**![](img/4269652fa6db67ac4e8f06a03260bddd.png)**

**[从现有数据中组合出新的概念](https://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf)——DL 不能像我们这样成功地做到这一点**

****3。丢弃 IID *(独立同分布随机变量)*假设****

**大多数 DL 1.0 模型假设任何数据样本，无论它来自训练集还是测试集，都是相互独立的，并且来自同一个分布 *(IID 假设训练和测试分布都可以用同一套分布参数来描述)***

**从代理交互的非静态环境中的自我监督学习，由于其从变化的环境中学习的本质，需要放弃 IID 假设。**

**然而，即使在使用监督学习的问题中*(例如，自动驾驶汽车的图像/对象分类/识别)*，IID 假设也可能是一种负担，因为总会有模型在训练期间从未见过的真实生活场景，错误分类可能会被证明代价高昂*(在自动驾驶汽车的早期版本中已经有一些这样的实例*)。虽然为训练积累的大量驾驶时间可以减少错误，但在没有 IID 假设的情况下学习的模型比通过 IID 假设学习的模型更有可能更好地处理罕见和不符合分布的情况。**

**放弃 IID 假设的另一个原因是通过[“通过混洗数据使训练和测试数据同质”](https://arxiv.org/pdf/1907.02893.pdf)为训练模型创建数据集时的选择偏差。为了实现 IID 目标，从不同来源*(包含属性差异)*获得的数据都被混洗，然后分成训练集和测试集。这破坏了信息并产生虚假的相关性。例如，考虑将一幅图像分类为一头牛或一头骆驼。奶牛的照片可能都在绿色的牧场上，骆驼在沙漠里。在训练模型之后，它可能无法将沙滩上的奶牛图片分类，因为它进行了虚假的关联——将绿色景观分类为奶牛，将米色景观分类为骆驼。我们可以通过让模型学习跨不同环境的不变特征来避免这种情况。例如，我们可以在不同的绿色牧场上标记奶牛的照片，其中一个是 90%的绿色，另一个是 80%的绿色。然后，该模型可以了解到牧场和奶牛之间有着强烈但不同的相关性，应该被拒绝。但是奶牛自己允许模型识别它们，不管它们在什么环境中。因此，通过利用不同的分布来识别不变属性，而不是将它们混在一起，可以防止虚假的相关性。虽然这只是一个例子，但是，广泛地利用分布变化中的信息，并学习它们之间的不变表示，可能有助于[学习健壮的表示。](https://arxiv.org/pdf/1907.02893.pdf)顺便说一句，与直接确定因果关系变量相比，对分布变化不变的变量更容易确定，这可以用作识别因果变量的方法，尽管挑战在于找出那些不变变量是什么。**

**一个自然产生的问题是，如果我们放弃 IID 假设，我们如何在变化的环境中准确地学习表征？**

****4。两种自我监督表示学习方法****

1.  *****预测输入空间接下来会发生什么。*****
2.  *****在抽象空间中预测接下来会发生什么。*****

**![](img/1bd662134bf10bc3cc4b5f0caf8b5f75.png)**

**自我监督学习的两种方法。在左边，表征学习通过预测输入空间的缺失部分来进行。例如，对于来自视频流的自监督学习，通过使用时间 t-1 的图像来预测时间 t 的输入。预测器将时间 t-1 处的帧和潜在变量作为输入，以预测时间 t 处的帧。该模型输出由潜在变量使之成为可能的多个预测-挑选最低能量的预测对(y，y’)(在基于能量的模型中)。右边的预测发生在学习表示 c 和 h 的抽象空间中。训练目标 V 试图以某种方式将当前的 h 与过去的 c 相匹配，使得一些过去的意识状态与当前的 h 相一致。参考资料部分有这种方法的更多细节。**

**这两种方法并不相互排斥。一个模型有可能使用这两种方法来学习表示。**

*   ****预测输入空间接下来会发生什么。**这通常是通过一个潜在变量来完成的，该变量封装了所有关于环境的未知信息*(包括代理和代理交互)*，并训练一个模型来预测未来，或者等效地重建未来，并使用重建误差作为学习表示的手段。基于能量的模型是学习这种表示的一种方式。它们可以用于学习表示，使得输入(x)和输入(y)的预测/重构部分通过标量值能量函数映射到能量表面，在该能量表面，输入数据点 x 和 y 具有较低的能量。这是通过两大类方法实现的(1)第一类方法降低输入数据点 *(x 及其预测 y)* 的能量，同时提升所有其他点*的能量(例如，在基于* [*能量的 GAN*](https://arxiv.org/pdf/1609.03126.pdf) *中，生成器选择对比数据点——远离输入点的点)*和(2) 第二类方法将输入数据点的能量约束为低*(通过架构或通过某种正则化)*。 如前所述，环境中的未知因素通常由潜在变量(z)捕获，该潜在变量允许通过改变 z 并挑选具有最低能量的一个预测来对 y 进行多个预测。潜在变量的信息容量受到多种方法的限制，如使用正则化子使潜在变量稀疏，添加噪声等*(潜在变量的信息容量有*[](https://youtu.be/A7AnCvYDQrU)**)*。这些潜在变量通常在训练期间通过使用编码器来学习，该编码器被馈送输入(x)和被预测的实际数据(y’)。然后，解码器采用潜在变量和 x *(它的转换版本，其中转换由某个神经网络完成)*来进行预测。能量函数用作成本函数，其标量输出然后用于训练模型以学习正确的表示。仅通过利用接收输入数据和潜在变量的解码器*(编码器在实践中可用于如下所述的终身训练循环中)*来进行推断。 [Yann LeCun 最近的演讲](https://youtu.be/A7AnCvYDQrU)深入介绍了这种方法的更多细节，并展示了这种方法如何让汽车在模拟中学习驾驶*(训练数据是真实世界场景中汽车的视频记录，模型通过预测下一帧来学习，下一帧包括它自己与其他汽车在车道中的位置；成本函数考虑汽车和其他汽车之间的距离以及汽车是否停留在其车道上)。*这种方法本质上是将 DL 1.0 模型重新用于重建输入的自监督任务，其中反馈信号包含非常丰富的信息*(视频、音频等中的下一个图像帧)。)*相对于仅仅是标量*(强化学习)*或者标签*(自我监督学习)*。***
*   **预测抽象空间中接下来会发生什么。这种方法基于这样的假设，即环境的变化可以用几个因果变量*(捕捉为稀疏表示)*来解释，这些变量是从高维表示*(类似于 DL 1.0 表示的感知空间)*中获得的，这些高维表示是通过来自环境的感觉流输入来学习的。这些稀疏表示用于预测未来，不是在原始输入空间中，而是在与导出它们的感知空间一致的已学习稀疏表示的空间中。这类似于我们预测/计划我们下班回家的路程——我们在一个非常稀疏的*(低维)*水平上做这件事——而不是在开车旅行的实际感官体验的输入空间中。预测在远离感觉流的原始输入空间的抽象空间中发生的事情具有潜在的优势，不仅学习那些利用环境中的变化的输入流的更好的表示(*类似于 DL 1.0 表示)*，而且学习导致输入感觉流中的那些变化的表示。本质上，我们正在根据分布和 OOD 性能的变化对这些模型进行训练*(学习这些表示的训练目标仍未确定，如参考部分所述)*用作学习良好的低维因果表示的训练信号。假设环境的变化可以用一个低维表示法*(代表一个* [*稀疏因子图*](http://deepdive.stanford.edu/assets/factor_graph.pdf) *)，几个变量之间的无向图；在实现中，这可能不是一个显式的图，而只是一个低维表示，在下面的参考章节中进一步检查)*对编码器施加约束以学习这样的表示*(可能还需要附加的约束)*。一些早期工作使用 DL 方法寻找变量*(有向图)*之间的因果关系，该方法可用于在两个随机变量 A 和 B 的联合分布 P(A，B)的两个等价因子分解之间进行选择——P(A)P(B/A)和 P(B)P(A/B ),这两个因子分解最好地捕捉了 A 和 B 之间的因果关系。具有正确因果因子分解的模型，例如 P(A)P(B/A ),更快地适应[分布的变化【T25Yoshua Bengio 最近的演讲深入探讨了这种方法的更多细节。](https://arxiv.org/pdf/1901.10912.pdf)**

**虽然这两种方法非常不同，但它们有潜在的联系。一个是两种方法的稀疏性约束，即使通过不同的方式实现。另一个是因子图和能量函数之间的联系。变量之间的联合分布*(在右表示空间)*是对世界的粗略近似，可以帮助代理进行计划、推理、想象等。通过将联合分布划分为随机变量*(一个变量可以在多个子集中)*的小子集的函数，可以使用因子图来表示联合分布。正确的划分会造成能量函数的下降——否则它们就不值得放入[因子图中](https://arxiv.org/pdf/1709.08568.pdf)。**

****5。注意力的作用****

**虽然注意力本质上是一个加权和，但当权重本身在训练和推理过程中由内容驱动动态计算时，这个简单操作的威力就显而易见了。**

*   ****重点在哪里？**标准前馈神经网络中任何节点的输出本质上是该节点输入的加权和的非线性函数，其中权重是在训练时间学习的。相比之下，注意力允许那些权重本身被动态地计算，即使是在基于输入内容的推断期间。这使得在训练和推理期间，连接计算层的边上的静态权重能够被由注意力计算的动态权重所取代。这些动态权重是基于内容计算的。变压器架构(如 BERT)使用这种方法。例如，单词的向量表示被计算为其邻居的加权和，其中权重确定每个邻居对于计算单词的向量表示有多重要*(关注哪里)*—关键是这些权重是由注意头*(在 BERT 模型的每一层中有多个注意头)*使用句子中的所有单词动态计算的。**

**![](img/9937bf0c12174e01607c5541b419576f.png)**

****重点在哪里？**该图显示了模型层中使用关注度的动态边连接，并与模型中的层进行了比较，如推理期间层之间具有静态边权重的标准 FFN。左侧:节点 X 的输出是输入的加权和，其中权重 w1、w2、w3、w4、w5 在推理期间保持相同，而不管不同的输入(A1-A5、B1-B5)。右侧:注意力模型中节点 X 的输出也是输入的加权和，但是权重本身是作为输入的函数动态计算的(在训练和推断期间)。这使得权重本身在不同的输入(A1-A5，B1-B5)之间变化，如不同颜色的虚线边缘所示。**

*   ****什么时候聚焦？**在翻译中，给定一组由编码器计算的隐藏状态，注意力基于翻译的阶段*(解码器隐藏状态)*在每个时间步长*(何时聚焦)*中挑选不同数量的这些隐藏向量，以产生如下所示的翻译。**

**![](img/69ae925800a6948ce7f6fa9256912a95.png)**

****什么时候聚焦？**图改编自 [Jay Alammar 关于神经机器翻译的文章](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)。编码器的输出是三个隐藏状态向量，当翻译文本被输出时，在两个解码状态(时间步长 4 和 5)期间，使用注意力(A4 和 A5)来选择三个隐藏状态向量的不同比例。**

**注意力*(特别是“何时聚焦”的用法)*在前面描述的“抽象空间中的预测”方法中起着关键作用，以聚焦于一大组表征*(构成无意识空间的表征)*的选择方面，用于解决有意识的任务。因果推理、最优解的规划/图形搜索都可以被实现为时间上的顺序处理，其中在每个时间步，使用注意力挑选隐藏状态*(从无意识状态集合)*的正确子集。将噪声注入到图遍历中的下一步*(使用注意力)*的选择中为解决方案的搜索添加了探索性方面*(类似于 RL 中使用的蒙特卡罗树搜索)*。更重要的是，这种使用正确注意力屏蔽*(作为感知空间表示的函数动态计算)*的顺序处理可以像我们在 DL 1.0 中对翻译任务所做的那样进行学习。注意力不仅可以用于有意识的任务解决，还可能在任务过程中以自上而下的方式影响后续的感知。这种自上而下的影响从大脑中获得灵感，其中新皮层*的每个功能单元*(皮质柱)*(意识处理主要在这里进行)都有传入的感觉连接和传出的连接，其中一些连接到运动区。一旦输入中的某些东西吸引了我们的注意力，这些运动神经连接就有意识地将感知引导到输入流的某些部分。例如，大脑皮层处理音频输入的感觉区域与我们头部的肌肉有运动连接，一旦一个不寻常的声音吸引了我们的注意力，它就会将我们的头转向它的来源。***

**![](img/b08e6eb783f1bf25f49433f8e3bf31eb.png)**

**图来自 Yoshua Bengio 的演讲幻灯片。有意识的思想是以自下而上的方式选择的更大的无意识状态的方面，这反过来导致自上而下的对感官输入的关注**

****6。多时间尺度的终身学习****

**多时间尺度的学习和迭代优化促进了面向对象的泛化。例如，代理可以在不同的环境中学习快速适应，同时执行较慢的迭代以在它们之间进行推广。这种多时间尺度的方法是一种学习的方式。DL 1.0 的实践者通过人类完成学习来学习部分来实现同样的效果——他们围绕失败的案例不断训练监督学习模型，通过使用失败案例来扩展训练集，人类专家识别更多这样的边缘案例，然后将训练好的[模型部署到字段](https://youtu.be/0VH1Lim8gL8)。特斯拉是这种方法在实践中的一个例子——他们在空中无缝更新汽车，不断提高其自动驾驶能力。这种缓慢但肯定会侵蚀罕见事件分布尾部的方法是否能最终将黑天鹅事件概率降低到可以忽略不计的水平，使其不再被视为所有实际用途的危险，还有待观察。**

****7。架构优先****

**“抽象空间中的预测”方法，除了依赖于上述注意力之外，可能还需要将模型从 DL 1.0 中的向量处理机器转换到对可被称为*的向量集进行操作的机器(注意力中使用的关键字服务于这种间接功能，并且可被视为变量名)*并且由[动态重组的神经网络模块](https://arxiv.org/pdf/1909.10893.pdf)对其进行操作。**

**自我监督学习的输入空间方法中的预测似乎不需要新的架构-大量的现有模型可以大致分类为[基于能量的模型](https://youtu.be/A7AnCvYDQrU) *(例如像 BERT 这样的语言模型是基于能量的模型)*。自我监督学习在很大程度上利用了这些现有的架构。**

# ****接近人类 AI 水平的替代方法****

## ***混合方法***

**到目前为止，有许多混合方法的实现将 DL 1.0 与传统的符号处理和算法相结合。这些混合方法使得应用程序能够利用 DL 1.0 进行部署。因此，不能低估混合方法的重要性。所有这些混合方法，当用于决策用例*(判别模型；忽略生成用例，尽管它们也有应用——*[*生成真实感图像等。*](https://arxiv.org/pdf/1812.04948.pdf) *)* ，有一个共同的特性，它们对 DL 1.0 输出执行进一步的算法处理，通常是*(例外情况是图形嵌入)*将 DL 1.0 输出的分布式表示简化为符号，此时组合性*(符号不像我们可以用向量进行组合，我们只能将它们与更多的符号组合在一起，例如像语法树)*分布式表示中固有的相关性丢失。**

**如果一种混合方法将 DL 输出减少到符号，然后尝试 DL 2.0 任务，如对这些符号进行推理和规划，将使我们能够达到人类水平的人工智能，这仍有待观察。这些天来，所有关于混合方法达到人类人工智能水平的潜力与徒劳的公开辩论都归结为—*DL 2.0 任务可以只用符号来完成吗？或者，鉴于分布式表示为 DL 1.0 提供的优势，DL 2.0 任务一定需要分布式表示吗，即它们可以被学习，它们可以捕获相关性，并且它们允许组合性***

## **还有更多的先验来自自然智慧？**

**自然智能已经启发并继续以多种方式影响人工智能的创造，首先是智能的基本计算单元*(从硬件角度)*—神经元*(尽管人工神经元仅实现生物神经元的一个很小的，即使是关键的功能)*。深度学习继续从自然智能中汲取灵感，例如多层计算提供的组合性*(类似于视觉皮层处理视觉)*一直到有意识任务解决的先验*(*[*Yoshua beng io 的论文*](https://arxiv.org/pdf/1709.08568.pdf) *)* 。**

**[*克里斯托斯·帕帕迪米特里奥*](https://ccneuro.org/2019/proceedings/0000998.pdf) *的* 2019 年论文*(尽管其核心计算原语牢牢植根于生物学家实验验证的大脑计算方法，但人们可能会很快将其视为大脑的另一种计算模型* *)，但*凸显了上述问题的重要性——*在实现自然智能的过程中，我们至少还可以借鉴一些技巧(如果不是想法的话)*？**

**以下面概述的机制为例，一只苍蝇*(它的嗅觉系统硬件和功能是一般昆虫的代表)*如何学习识别气味 *(* [*)这在克里斯特斯的演讲*](https://www.youtube.com/watch?v=_sOgIwyjrOA&feature=youtu.be) *)* 中有描述，只有一两个样本*。称这种学习为“样本效率”是一种保守的说法——“类固醇学习”可能更恰当。***

****苍蝇如何学会识别气味****

**大约 50 个神经元感知气味，这些神经元随机投射到 2000 个神经元上，形成一个[随机图](http://snap.stanford.edu/class/cs224w-readings/erdos60random.pdf)，即[二分图](https://en.wikipedia.org/wiki/Bipartite_graph)。在向量术语中，由 50 维向量捕获的气味输入被随机投射到 2000 维向量，然后由抑制性神经元强制成为具有大约 10%非零值的稀疏向量*(非零值表示从超过某个阈值的放电模式活动的正态分布的尾部挑选的神经元)*。这个稀疏的 2000 维向量充当了苍蝇对特定气味的记忆。**

**![](img/f4d23638d049c98450b24995357db07e.png)**

**图摘自[克里斯特斯·帕迪米特里奥的演讲](https://youtu.be/_sOgIwyjrOA)。这是一个代表苍蝇如何识别气味的模型。他们可以记住一两次接触的气味，也有能力超越他们学到的东西进行归纳，因为他们只有大约 50 种不同的嗅觉传感器(我们有大约 500 种；老鼠大约有 1500 只)**

*   **这个随机投影*(在硬件中实现)*后面跟着一个 cap *(稀疏性的运行时实施)*看起来是大脑计算的一个非常基本的功能原语，我们也使用 *(* [*克里斯特斯的大脑模型*](https://ccneuro.org/2019/proceedings/0000998.pdf) *很大程度上是基于在自然智能的这个基本计算原语之上构建一些简单的算法运算)***
*   **随机投影和 cap 保持相似性*(给定一些超参数的正确选择)*。气味之间的相似性在它们的记忆表征中被捕获*(突触权重)。*记忆回忆唤起与所学重量的强度相称的点火活动。一只苍蝇有大约 50 种不同类型的嗅觉传感器*(我们有大约 500 个，老鼠有 1500 个)*。将气味世界映射到捕捉相似性的分布式表示的能力对于苍蝇的生存至关重要。**
*   **本质上，非常高的采样效率*(一两次尝试学习一种气味)*和非分布学习*(将新气味映射到现有气味)*通过这个简单的生物网络成为可能。**

**![](img/0d6c14dc036fa4dc24711c23e547e82c.png)**

**图摘自[克里斯特斯·帕迪米特里乌的讲话](https://youtu.be/_sOgIwyjrOA)，说明随机项目和 cap 保持相似性。大自然似乎已经找到了最佳的稀疏度——足够数量的神经元激活以捕捉语义相似性，同时活跃神经元的数量有限，足以区分不同的气味**

**苍蝇嗅觉系统设计的一个关键方面是在信息处理的所有阶段实施的稀疏表示。与 DL 模型相比，DL 模型的每个输入都会照亮整个模型，就像圣诞树上的灯一样，以不同的亮度活动*(当然这有点夸张，因为[1]我们的模型通常只解决一个任务[2]在培训期间，退出限制激活等。这样做也是有原因的——许多小而弱的特征加起来会在决策中变得重要。也许从输入开始一路实施稀疏性*(类似于随机投影和 cap 等操作原语)*将权重更新限制在部分启用 [*的几个参数内(2019 年发表的一篇论文解决了这个问题；参考章节有更多详情)*](https://arxiv.org/pdf/1803.03635.pdf) ，快速学习。此外，简单的权重更新*(学习)*“一起触发的细胞连接在一起”规则具有固有的记忆效率，当与随机投影和 cap 结合时，有助于随时间增加的泛化。DL 模型中的学习依赖于随机梯度下降和反向传播——迄今为止 DL 中学习的主干。也许我们还将对 DL 模型学习的效率进行基本的改进，超越 DL 2.0 目标从自我监督学习中获得的效果。***

# *最后的想法*

*一种全新的学习方法总有可能在未来出现，它更接近人类水平的人工智能，甚至比它更好。假设曾经发生过，深度学习的一些核心成功/想法很可能被新方法所包含/吸收——在正确的语义空间中捕获相关性(DL 1.0)和因果关系(DL 2.0 目标)的*分布式表示。**

## *确认*

*如今，在机器学习社区中，我们几乎认为理所当然的一件事是以关于 [arXiv](https://arxiv.org/) 和 [code](https://paperswithcode.com/) 的论文的形式免费获取想法。这使得像我这样的从业者能够理解并跟踪进展。我对这个领域的理解归功于机器学习社区、其研究人员和教师的开放性。*

***参考/附加注释***

*   *[意识在先(*此为最近更新—2019 年 12 月；原论文发表于 2017 年 10 月* )](https://arxiv.org/pdf/1709.08568.pdf) 。*

*![](img/46e544efdca0c0c86985f00d75b8f64f.png)*

*用于计算意识和无意识表征的[论文的逐字摘录](https://arxiv.org/pdf/1709.08568.pdf)*

*![](img/0f01db8a10c8415bebf8e51f3a2f8776.png)*

*从[的论文中逐字摘录了学习有意识和无意识表征的合理培训目标](https://arxiv.org/pdf/1709.08568.pdf)。我们可以通过学习映射将意识状态映射到语言，本质上是用语言调试意识状态。*

*   *语言在 DL 2.0 中对于“调试意识任务求解”的作用。语言*(忽略感知的感官形态——文本、音频等。)*的特殊之处在于“单词”——构成输入数据的单位*(不考虑它们是通过哪种感官形式提取的，尽管这些感官形式赋予单词丰富的上下文)*与用于操纵和表达有意识思维的单位非常接近*(然而，儿童可以在获得语言之前很早就执行有意识的任务解决，这表明语言是不必要的，即使它对表达思想是有效的)*。我们预测某人将要表达的内容，就像当前的机器学习语言模型预测序列中的下一个单词或序列中缺失的单词一样。从这个意义上说，语言可以成为有意识任务处理的一个很好的调试工具，因为单词序列是我们大脑中有意识处理的一个很好的代理，在那里我们直接用单词操纵和表达概念。*由于这个原因，DL 2.0 有意识任务解决可能不会受到 DL 1.0 任务解决的不透明性的影响，因为我们可以通过模型读取有意识任务解决，读取实现为从思想到话语的学习映射，即我们可以使用语言调试思想，假设 DL 2.0 有意识任务解决与我们进行有意识任务解决的方式一致(这个假设的有效性很大程度上依赖于我们在上面检查的有意识任务解决方法)。**
*   *[用于学习解开因果机制的元传递对象](https://arxiv.org/pdf/1901.10912.pdf)*
*   *[从未知干预中学习因果变量](https://arxiv.org/pdf/1910.01075.pdf)*
*   *[递归独立机制钢圈论文](https://arxiv.org/pdf/1909.10893.pdf)*
*   *[关于随机图的进化。鄂尔多斯和雷尼 1960](http://snap.stanford.edu/class/cs224w-readings/erdos60random.pdf) 。n 边的随机图可以通过从 v₂,…v₁的标记顶点之间的所有可能边的集合中挑选边来构建。当我们不断给随机图添加一条又一条*(下图)*的边时，本文检查了随机图的性质，其中每条被选取的边都不会被再次选取。*

*![](img/df0da2343a2624e446299d59edd64324.png)*

*图由来自[的数据组成，随机图论文](http://snap.stanford.edu/class/cs224w-readings/erdos60random.pdf)。当活跃突触连接的数量大于放电神经元数量的一半时，尚不清楚大自然是否利用了随机图结构中的突然变化(找不到这方面的任何工作)。*

*   *曼宁教授等人的一篇 [2019 论文](https://arxiv.org/pdf/1907.03950.pdf)。使用学习分布式表示*(来自图像输入)*的方法，然后将其映射到概念的预定离散空间*(对象和对象的属性)*以及它们之间的关系，以执行任务(*表示问题的文本输入也被学习为分布式表示，并映射到视觉场景上的概念的相同离散空间*。CNN 变体( *Mask R-CNN)* 用于检测场景中的对象。一旦检测到对象，就基于对象邻近度创建它们之间的有向图。使用图形注意网络来确定边之间的关系类型。由图像构成的图形代表了世界的模型。Q & A 是通过遍历这个结构化的世界模型来执行的。问题的输入单词被表示为手套嵌入。这些嵌入表示输入到 LSTM 的单词，后者输出表示句子的隐藏状态。然后通过 softmax 从有限的预定词汇表中挑选单词向量。这个向量序列用作指令序列来遍历从图像中提取的场景图。遍历*(模拟推理)*在给定指令向量的情况下，在每一步使用注意力来挑选感兴趣的图节点。这种监督学习模型*(纯粹基于 IID 假设的学习)*，使用分布式表示将推理空间限制在预先确定的有限概念集内的所有方式进行处理*(论文* *的第 7 部分* [*有更多细节)。这部作品的灵感部分来自于*](https://arxiv.org/pdf/1907.03950.pdf) [*意识先验论文*](https://arxiv.org/pdf/1709.08568.pdf)*

*![](img/91612429d859514b8758ffa499426897.png)*

*数字来自 al 的曼宁教授。2019 论文— [抽象学习—神经状态机](https://arxiv.org/pdf/1907.03950.pdf)。从图中逐字描述:NSM 推理过程的可视化:给定一幅图像和一个问题(左侧)，该模型首先构建一个概率场景图(蓝色方框和右侧的图像)，并将问题转化为一系列指令(绿色和紫色方框，其中对于每个指令，我们在向量空间中呈现其最接近的概念(或单词)(第 3.1 节)。然后，该模型在图形上执行顺序推理，在指令的引导下关注图像场景中的相关对象节点，以迭代地计算答案*

*   *[解释和利用对立的例子](https://arxiv.org/pdf/1412.6572.pdf)*
*   *[StyleGANs](https://arxiv.org/abs/1812.04948) 展示了在全球范围内控制图像生成的能力*(姿势、身份等)。)*使用一个潜在变量，并处于一个精细的层次*(头发、胡须等)。)*利用随机变异。这些方法是否可以用于自我监督学习，以预测不同抽象级别(像素级别和更高抽象级别)的输出，还有待观察。*
*   *[掌握星际争霸 2 的实时战略](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii)*
*   *[掌握雅达利等。通过使用学习模型进行规划](https://arxiv.org/abs/1911.08265)*
*   *[用于大脑计算的微积分](https://ccneuro.org/2019/proceedings/0000998.pdf)*
*   *规划未来几个步骤的能力并不局限于人类。[动物和鸟类为未来做计划](https://www.pnas.org/content/110/Supplement_2/10379.long)——例如回去找食物的地方，记住种子储存了多长时间，以便确定它们是否变质。*
*   *[生物学证据支持的意识功能观](https://www.quora.com/What-is-consciousness/answer/Ajit-Rajasekharan)。*
*   *[用注意力可视化模型](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)*
*   *[论文证明生物神经网络从一两个样本中学习，2019 年 12 月](https://arxiv.org/pdf/1808.08124.pdf)*
*   *[大脑如何学会嗅觉](https://www.quantamagazine.org/new-ai-strategy-mimics-how-brains-learn-to-smell-20180918/)*
*   *[嗅觉输入的随机收敛…](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4148081/) 本文提供了从苍蝇感觉神经元(50 维向量)到 2000 向量记忆层的投影的邻接矩阵是随机投影的证据。神经元的[连接](https://www.quora.com/What-is-a-connectome/answer/Ajit-Rajasekharan)并不在基因*中编码(考虑到这样做所需的信息量，一开始就在基因中编码连接是不可行的)，*到在发育过程中有一些机制充当神经元迁移的指导路径，特别是对于两个半球之间的主要连接纤维束等。其余的连接很大程度上是随机的。*
*   *[哺乳动物在感觉神经元投射到的层中具有循环连接](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3219421/)，这使得输入的后续投射能够用于进一步的下游处理。这种能力也许是达到哺乳动物智力水平的关键先决条件。苍蝇/昆虫似乎缺乏这种形式的多层加工，而这种加工是通过重复连接实现的。*
*   *由 Lex Fridman 撰写的关于 DL 1.0 迄今进展的综合概述—上传于 2020 年 1 月 10 日。[他演讲的幻灯片](https://lexfridman.com/files/slides/2020_01_06_deep_learning_state_of_the_art.pdf)*
*   *Lex 的演讲中提到的一篇论文解决了当前 DL 模型中从输入到输出一直缺乏稀疏性的问题。*

*![](img/cc146e9ea5d9d8485468ad6ff35ac8a1.png)*

*图[来自 Lex Fridman Jan 2020 年 1 月的演讲](http://Slides of his talk)*

*   *[2020 年 2 月 9 日图灵奖讲座](https://youtu.be/UX8OubxsY8w)由杰夫·辛顿、扬·勒昆和约舒阿·本吉奥主讲，他们在讲座中讨论了他们的最新作品。*

**这篇文章是从 Quora*[【https://qr.ae/TJg8ov】T21](https://qr.ae/TJg8ov)手动导入的*