<html>
<head>
<title>Deep Q-Network (DQN)-III</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度 Q-网络(DQN)-三</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-q-network-dqn-iii-c5a83b0338d2?source=collection_archive---------30-----------------------#2020-08-16">https://towardsdatascience.com/deep-q-network-dqn-iii-c5a83b0338d2?source=collection_archive---------30-----------------------#2020-08-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2ead" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/deep-r-l-explained" rel="noopener" target="_blank">深度强化学习讲解— 17 </a></h2><div class=""/><div class=""><h2 id="1f8e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">DQN 的性能和用途</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fa0f6596bcb8f7d913d0517d5a751d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDhcpkgdzkisARJU93i5xw.png"/></div></div></figure><p id="8351" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是第三篇专门讨论<strong class="lf jd">【深度 Q 网】(DQN)、<em class="lz"/><a class="ae ma" href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener ugc nofollow" target="_blank"><em class="lz">深度强化学习讲解</em></a><em class="lz"/>系列中的</strong>的帖子，其中我们展示了如何使用 TensorBoard 来获得模型性能的图形化视图。我们还提供了一种方法来查看我们训练有素的代理如何能够玩乒乓。</p><blockquote class="mb mc md"><p id="e6b9" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><a class="ae ma" href="https://medium.com/aprendizaje-por-refuerzo/9-m%C3%A9todos-value-based-deep-q-network-b52b5c3da0ba" rel="noopener">本出版物的西班牙语版本</a></p></blockquote><div class="mh mi gp gr mj mk"><a href="https://medium.com/aprendizaje-por-refuerzo/9-m%C3%A9todos-value-based-deep-q-network-b52b5c3da0ba" rel="noopener follow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">9.基于价值的营销:深度 Q 网络</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">请访问第 9 页的自由介绍</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">medium.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my lb mk"/></div></div></a></div><blockquote class="mb mc md"><p id="0444" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">这篇文章<a class="ae ma" href="https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb" rel="noopener ugc nofollow" target="_blank">的全部代码可以在 GitHub </a>上找到(而<a class="ae ma" href="https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb" rel="noopener ugc nofollow" target="_blank">可以使用这个链接</a>作为 Colab google 笔记本运行)。</p></blockquote><h1 id="d036" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">运行训练循环</h1><p id="3a69" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">当我们运行本例中的<a class="ae ma" href="https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>时，我们在控制台中获得以下输出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/6b5b6241828b0d3a1622b0d04e839dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KoW5z-T0O-Xp-grWuW8fwg.png"/></div></div></figure><p id="efdf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi">. . .</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/aa054c8a8f9ec8ae995479d1c735d9c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KK_FRu4Za4tQjVW1yDfn1A.png"/></div></div></figure><blockquote class="mb mc md"><p id="6c09" class="ld le lz lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">由于训练过程中的随机性，您的实际动态将与此处显示的不同。</p></blockquote><p id="9a64" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">但是，尽管信息丰富，却很难从一百万行中得出结论。读者应该记得，在之前的帖子 5 ( <a class="ae ma" rel="noopener" target="_blank" href="/pytorch-performance-analysis-with-tensorboard-7c61f91071aa"> <strong class="lf jd"> PyTorch 使用 tensor board</strong></a><strong class="lf jd">)</strong>中，我们介绍了工具<a class="ae ma" rel="noopener" target="_blank" href="/pytorch-performance-analysis-with-tensorboard-7c61f91071aa"><strong class="lf jd">tensor board</strong></a><strong class="lf jd"/>，该工具有助于跟踪不同参数的进度，并且可以为找到超参数的值提供出色的支持。</p><p id="3793" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们在这里不再重复解释应该插入什么代码，但是读者可以在<a class="ae ma" href="https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中找到这个例子的详细代码，它为 DQN 获得了一个在交互 TensorFlow 窗口中绘制的轨迹，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/37310e0b0d46d3ffd80219384d3bb90e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9jt529KGG1f1o9pIHxF3VQ.png"/></div></div></figure><p id="edb8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">例如，使用此代码，我们可以获得跟踪以监控 epsilon、奖励和平均奖励的行为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nz"><img src="../Images/fbb391d7b0e864e13c5194b7dfd5237e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3JnE3hwkBPjlEqido3YZHA.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/ace641e3151ec1435a6cf67029a77584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sOGpu3b7Ho6qrjxFhUbLgA.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/4a585a36e8e9757b22bdab9ea1c3bc03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H_1V_esN3ns-_fb5-q29iw.png"/></div></div></figure><p id="9393" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有了这个工具，我邀请用户找到更好的超参数。</p><h1 id="9012" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">使用模型</h1><p id="93a7" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">最后，在<a class="ae ma" href="https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中，读者可以找到允许我们看到我们训练有素的代理如何玩 Pong 的代码:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a36b248be610eab0812a97b1ea0fb792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/1*_9bX_XqWxT-BaYmFQB-ktw.gif"/></div></figure><p id="885c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们已经准备了代码，生成一个游戏的一集视频。视频存储在文件夹<code class="fe od oe of og b">video</code>中。</p><p id="406b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">代码几乎是代理类的方法<code class="fe od oe of og b">play_step()</code>的精确拷贝，没有ε-贪婪动作选择。我们只是将我们的观察传递给代理，并选择具有最大值的动作。这里唯一的新东西是环境中的<code class="fe od oe of og b">render()</code>方法，这是 Gym 中显示当前观察的标准方式。</p><p id="6204" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">代码中的主循环，将动作传递给环境，统计总奖励，当剧集结束时停止循环。在这一集之后，它会显示总奖励和代理执行该动作的次数。</p><p id="2a5b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因为它需要有一个图形用户界面(GUI ),并且我们是在 colab 环境中执行我们的代码，而不是在我们的个人计算机中，所以我们需要运行一组命令(从这个<a class="ae ma" rel="noopener" target="_blank" href="/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7">链接</a>中获得)。</p><h1 id="2aef" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">摘要</h1><p id="9cff" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">这是专门介绍<strong class="lf jd">深度 Q 网络(DQN)</strong>基础知识的三篇文章中的第三篇，在这三篇文章中，我们介绍了如何使用 TensorBoard 来帮助我们进行参数调整。我们还展示了如何可视化我们的代理的行为。</p><p id="68ae" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下期<a class="ae ma" rel="noopener" target="_blank" href="/policy-based-methods-8ae60927a78d">见</a>！</p></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><h1 id="f645" class="mz na it bd nb nc oo ne nf ng op ni nj ki oq kj nl kl or km nn ko os kp np nq bi translated">深度强化学习讲解系列</h1><p id="c09a" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated"><strong class="lf jd">由</strong> <a class="ae ma" href="https://www.upc.edu/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> UPC 巴塞罗那理工</strong> </a> <strong class="lf jd">和</strong> <a class="ae ma" href="https://www.bsc.es/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">巴塞罗那超级计算中心</strong> </a></p><p id="4cf4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一个轻松的介绍性<a class="ae ma" href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener ugc nofollow" target="_blank">系列</a>逐渐并以实用的方法向读者介绍这一令人兴奋的技术，这是人工智能领域最新突破性进展的真正推动者。</p><div class="mh mi gp gr mj mk"><a href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">深度强化学习解释-乔迪托雷斯。人工智能</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">本系列的内容</h3></div></div><div class="mt l"><div class="ot l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="89c0" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">关于这个系列</h1><p id="87a4" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">我是在五月份开始写这个系列的，那是在巴塞罗纳的<strong class="lf jd">封锁期。</strong>老实说，由于封锁，在业余时间写这些帖子帮助了我<a class="ae ma" href="https://twitter.com/hashtag/StayAtHome?src=hashtag_click" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> #StayAtHome </strong> </a>。感谢您当年阅读这份刊物；这证明了我所做的努力。</p><p id="0586" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">免责声明</strong> —这些帖子是在巴塞罗纳封锁期间写的，目的是分散个人注意力和传播科学知识，以防对某人有所帮助，但无意成为 DRL 地区的学术参考文献。如果读者需要更严谨的文档，本系列的最后一篇文章提供了大量的学术资源和书籍供读者参考。作者意识到这一系列的帖子可能包含一些错误，如果目的是一个学术文件，则需要对英文文本进行修订以改进它。但是，尽管作者想提高内容的数量和质量，他的职业承诺并没有留给他这样做的自由时间。然而，作者同意提炼所有那些读者可以尽快报告的错误。</p></div></div>    
</body>
</html>