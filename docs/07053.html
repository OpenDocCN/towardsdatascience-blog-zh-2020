<html>
<head>
<title>Don’t look backwards, LookAhead!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不要向后看，向前看！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dont-look-backwards-lookahead-6bcd7ff50f93?source=collection_archive---------38-----------------------#2020-05-30">https://towardsdatascience.com/dont-look-backwards-lookahead-6bcd7ff50f93?source=collection_archive---------38-----------------------#2020-05-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7c3f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">前瞻优化器使您的模型对超参数的选择不太敏感</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9ede9ddc63bdbd29ed0e69c855d8e2a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*89qJhcP5RM02P36MVRxFVQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/pl/users/rihaij-2145/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1366118" rel="noopener ugc nofollow" target="_blank"> rihaij </a> z <a class="ae ky" href="https://pixabay.com/pl/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1366118" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>提供</p></figure><p id="cd45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优化器的任务是寻找这样一组权重，使神经网络模型产生尽可能低的损失。如果你只有一个权重和一个如下图所示的损失函数，你不需要成为天才也能找到答案。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/caa1147cd206cb5777aa3ca612c065ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*uPlqVQ1djVIx-5rWakAzrw.png"/></div></figure><p id="8034" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，你通常有许多权重和一个很难简单的损失景观，更不用说不再适合2D绘画。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/6e7221d1149f06e728b65c1779fed3cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3UEMZSvyaxDXNRHaNsfHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用<a class="ae ky" href="https://arxiv.org/pdf/1712.09913.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1712.09913.pdf</a>中提出的方法可视化无跳跃连接的ResNet-56的损失表面。</p></figure><p id="68e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">找到这样一个函数的最小值不再是一个微不足道的任务。像Adam或SGD这样最常见的优化器需要非常耗时的超参数调整，并且可能陷入局部最小值。选择学习率这样的超参数的重要性可以用下图来概括:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/e08530592d6ebbda7bb18b445fcaba83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxG208ZG4hlzTzOeLlJS8g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">太大的学习率会引起最小值附近的振荡，而太小的学习率会使学习过程非常慢。</p></figure><p id="49a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近提出的<a class="ae ky" href="https://arxiv.org/pdf/1907.08610.pdf" rel="noopener ugc nofollow" target="_blank">前瞻优化器</a>进行优化过程</p><blockquote class="ly lz ma"><p id="3f04" class="kz la mb lb b lc ld ju le lf lg jx lh mc lj lk ll md ln lo lp me lr ls lt lu im bi translated">对次优超参数不太敏感，因此减少了对大范围超参数调整的需求。</p></blockquote><p id="6050" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这听起来像是值得探索的事情！</p><h1 id="2827" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">该算法</h1><blockquote class="ly lz ma"><p id="d23f" class="kz la mb lb b lc ld ju le lf lg jx lh mc lj lk ll md ln lo lp me lr ls lt lu im bi translated">直观地说，该算法通过<strong class="lb iu">在由另一个优化器生成的“快速权重”序列中前瞻</strong>来选择搜索方向。</p></blockquote><p id="39c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优化器保留两组权重:快速权重<code class="fe mx my mz na b">θ</code>和慢速权重<code class="fe mx my mz na b">ϕ</code>。它们都用相同的值初始化。具有一定学习速率<code class="fe mx my mz na b">η</code>的标准优化器(例如，Adam、SGD、…)被用于为定义数量的步骤<code class="fe mx my mz na b">k</code>更新快速权重<code class="fe mx my mz na b">θ</code>，从而产生一些新值<code class="fe mx my mz na b">θ’</code>。</p><p id="3b6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后关键的事情发生了:慢速权重<code class="fe mx my mz na b">ϕ</code>沿着权重向量 <code class="fe mx my mz na b">θ’- ϕ</code>的差定义的方向<strong class="lb iu">移动。这一步的长度由参数<code class="fe mx my mz na b">α</code>——慢速权重学习率控制。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/5166319782c784f516b577d888af39f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*3obqiUVCEmP40W_cVZ2MUQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">前瞻算法中的重要更新</p></figure><p id="0c54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，通过将快速权重值重新设置为新计算的慢速权重值<code class="fe mx my mz na b">ϕ’</code>，开始重复该过程。你可以看到下面的伪代码:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/47497f828d3ff01aca5be1ca44b58a4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*89KjBRzlLKIX5ROPCZaBBw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://arxiv.org/pdf/1907.08610.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1907.08610.pdf</a></p></figure><h1 id="bbf9" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">这有什么意义？</h1><p id="89f5" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">为了回答这个问题，我们将研究来自<a class="ae ky" href="https://arxiv.org/pdf/1907.08610.pdf" rel="noopener ugc nofollow" target="_blank">前瞻出版物</a>的(略微修改的)图片，但是作为介绍，让我们首先看另一张图片。如果我们的模型只有三个权重，损失函数可以很容易地可视化，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/a216743a1ff76b3983abd180bf527d09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*nF6umf93x4bGu5JBHOJb9A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在模型仅依赖于三个权重的情况下，权重空间中的损失函数可视化。提出了权重空间中损失到三个平面(“超平面”)的投影，其中一个权重具有恒定值。</p></figure><p id="3b83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，在现实生活的例子中，我们有三个以上的权重，导致权重空间具有更高的维度。然而，我们仍然可以通过把它投影到这样一个空间的超平面上来形象化这个损失。</p><p id="6731" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是在<a class="ae ky" href="https://arxiv.org/pdf/1907.08610.pdf" rel="noopener ugc nofollow" target="_blank">前瞻报告</a>中提出的内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/7566f827ee630174b856c72cbff3d57a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*ZV2rVwLUfDuOPElWlS0D1Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:https://arxiv.org/pdf/1907.08610.pdf<a class="ae ky" href="https://arxiv.org/pdf/1907.08610.pdf" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="b7aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到目标函数(在这种情况下是精度，但也可能是损失)到权重空间中的超平面的投影。不同的颜色对应不同的目标函数值:颜色越亮，值越优。前瞻优化器的行为如下所示:蓝色虚线表示快速权重<code class="fe mx my mz na b">θ</code>的轨迹(蓝色方块表示十个后续状态)，而紫色线表示快速权重更新<code class="fe mx my mz na b">θ’- ϕ</code>的方向。紫色三角形表示两个后续的慢速权重值<code class="fe mx my mz na b">ϕ</code>、<code class="fe mx my mz na b">ϕ’</code>。三角形之间的距离由慢速权重学习速率<code class="fe mx my mz na b">α</code>定义。</p><p id="1d5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，标准优化器(在本例中为SGD)遍历了一个次优绿色区域，而第二个慢权重状态已经非常接近最优。<a class="ae ky" href="https://arxiv.org/pdf/1907.08610.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>描述的更为优雅:</p><blockquote class="ly lz ma"><p id="0f84" class="kz la mb lb b lc ld ju le lf lg jx lh mc lj lk ll md ln lo lp me lr ls lt lu im bi translated">当在高曲率方向振荡时，快速权重更新沿着低曲率方向快速进行。慢速权重有助于通过参数插值消除振荡。快速权重和慢速权重的组合改善了在高曲率方向上的学习，减少了方差，并且使得前瞻在实践中能够快速收敛。</p></blockquote><h1 id="8f5e" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">在Keras怎么用？</h1><p id="f6a7" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">现在回到方法的实际方面；到目前为止只有<a class="ae ky" href="https://pypi.org/project/keras-lookahead/" rel="noopener ugc nofollow" target="_blank">一个非官方的Keras实现</a>;它可以很容易地与您当前的优化器一起使用；</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="a166" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，除了优化器本身之外，<code class="fe mx my mz na b">Lookahead</code>还需要两个参数:</p><ul class=""><li id="cc8e" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><code class="fe mx my mz na b">sync_period</code>对应前面介绍的<code class="fe mx my mz na b">k</code>——两组砝码同步后的步数，</li><li id="8e21" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated"><code class="fe mx my mz na b">slow_step</code>对应于<code class="fe mx my mz na b">α</code>慢速权重的学习速率。</li></ul><p id="a622" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了检查它是否按预期工作，您可以将<code class="fe mx my mz na b">slow_step</code>设置为<code class="fe mx my mz na b">1</code>，并将<code class="fe mx my mz na b">Lookahead</code>的行为与常规优化器的行为进行比较。</p><p id="cb52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<code class="fe mx my mz na b">1</code>的<code class="fe mx my mz na b">α</code>，前瞻更新步骤减少为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/6d1eabfa8ddcbb5af346b2dc5d73a9aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*oO_ElGwHWS5NPRYMkAi9GQ.png"/></div></figure><p id="3e91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着前瞻被简化为它的底层标准优化器。我们也可以在修改后的权重轨迹图上看到它:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/464b48caf4107d06c0228a00a14e9b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*G82h4LzizDjWy9zudH2bPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">改编自:<a class="ae ky" href="https://arxiv.org/pdf/1907.08610.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1907.08610.pdf</a></p></figure><p id="aab1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在慢速权重的结束状态与快速权重的结束状态相同。</p><p id="70b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用以下代码对其进行测试:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">测试证明具有Adam和慢学习率1的LookAhead等价于纯Adam。</p></figure><h1 id="dc0b" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">一锤定音</h1><p id="76fd" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">前瞻是一种有效的优化算法，它以可忽略的计算成本使寻找损失函数最小值的过程更加稳定。此外，需要较少的超参数调整。</p><p id="1079" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">据说当与<a class="ae ky" href="https://arxiv.org/pdf/1908.03265.pdf" rel="noopener ugc nofollow" target="_blank">修正的Adam优化器</a>结合使用时特别有效。我将在下一篇文章中讨论这个话题。</p></div></div>    
</body>
</html>