<html>
<head>
<title>Feature Scaling and Normalisation in a Nutshell</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简而言之，特征缩放和标准化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-scaling-and-normalisation-in-a-nutshell-5319af86f89b?source=collection_archive---------17-----------------------#2020-06-28">https://towardsdatascience.com/feature-scaling-and-normalisation-in-a-nutshell-5319af86f89b?source=collection_archive---------17-----------------------#2020-06-28</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="95f3" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">为什么、何时以及如何调整要素的比例</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/9f57720d84f6d0b1633e8e2d62285752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1LI9TzwDU1l6IyJFBRcULw.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">弗兰基·查马基在<a class="ae kz" href="https://unsplash.com/search/photos/data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0866" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">机器学习中最基本的步骤之一可能是特征工程，在此过程中，我们试图尽可能多地构建预测特征。一旦我们设法到达那里，我们可能会以一堆性质明显不同的特征而告终。那么这种不规则性对模型性能有什么影响，我们该如何处理呢？</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><p id="1be5" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">特征工程</strong>是创建预测特征的过程，这些预测特征可能有助于机器学习模型实现预期的性能。在大多数情况下，特征将是不同单位和数值范围的测量值。例如，您可能会考虑在您的特征空间中添加员工的年龄(理论上可以取 1 到 100 之间的值)以及他们的薪酬(可能在几千到几百万之间)。</p><p id="22ff" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在本文中，我将介绍<strong class="lc iv">特征缩放</strong>，这是一种预处理技术，用于处理 ML 模型需要缩放特征以获得最佳结果的情况。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h1 id="1738" class="md me iu bd mf mg mh mi mj mk ml mm mn ka mo kb mp kd mq ke mr kg ms kh mt mu bi translated">数据集中具有不同比例和值范围的要素有什么问题？</h1><p id="454e" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">当我们尝试构建的模型使用距离测量(如欧几里德距离)时，要素的比例和范围可能会有问题。这种模型可以是<strong class="lc iv">K-最近邻、K-均值聚类、学习矢量量化(LVQ) </strong>等。</p><p id="c006" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">主成分分析(PCA) </strong>也是一个很好的例子，说明了特征缩放的重要性，因为我们感兴趣的是使方差最大化的成分，因此我们需要确保我们是在进行比较。</p><p id="2924" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">此外，特征缩放还可以帮助使用梯度下降作为优化算法的模型，因为特征标准化有助于更快地达到收敛。</p><p id="5c13" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">另一方面，对于<em class="na">不</em>采用基于距离的方法的模型，不需要特征缩放(因此应用时无效)。这些包括基于树的模型，如决策树和随机森林。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h1 id="07bb" class="md me iu bd mf mg mh mi mj mk ml mm mn ka mo kb mp kd mq ke mr kg ms kh mt mu bi translated">什么是特征缩放？</h1><p id="ec74" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated"><strong class="lc iv">要素缩放</strong>是对数据集中的要素值进行缩放的过程，以使它们按比例参与距离计算。两种最常用的特征缩放技术是<em class="na">标准化(或 Z 分数标准化)</em>和<em class="na">最小-最大缩放</em>。</p><p id="2bba" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">标准化</strong>(也称为 Z 分数标准化/规范化)是重新调整特征<em class="na"> χ </em>的过程，以便它们具有<em class="na"> μ=0 </em>和<em class="na"> σ=1 </em>。从技术上讲，标准化通过减去平均值并除以标准差来集中和标准化数据。结果值被称为<strong class="lc iv">标准分数</strong>(或 z 分数)，计算如下:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nb"><img src="../Images/51e47af2f904dc737e5b0aeae6e6ca6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*Qs2Ll8QvZinR6BqHpt1PPQ.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">特征值的标准化</p></figure><p id="386a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在 Python 和 scikit 中——了解这可能会转化为</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="325a" class="nh me iu nd b gz ni nj l nk nl">from sklearn.preprocessing import StandardScaler</span><span id="d911" class="nh me iu nd b gz nm nj l nk nl">scaler = StandardScaler()<br/>train_X = scaler.fit_transform(train_X)<br/>test_X = scaler.transform(test_X)</span></pre><p id="325b" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">最小-最大缩放</strong>是将特征值重新缩放到特定范围(例如[0，1])的过程。下面给出了将值缩放到[a，b]之间的范围-σ内的公式+ -(m:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nn"><img src="../Images/867ed39810b1b22698380ef382c928a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*FPbB1UVTBctPf36F4VK7qw.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">将特征值缩放到范围[a，b]的公式</p></figure><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="428a" class="nh me iu nd b gz ni nj l nk nl">from sklearn.preprocessing import MinMaxScaler</span><span id="1c2b" class="nh me iu nd b gz nm nj l nk nl">scaler = MinMaxScaler()<br/>train_X = scaler.fit_transform(train_X)<br/>test_X = scaler.transform(test_X)</span></pre><h1 id="bb55" class="md me iu bd mf mg no mi mj mk np mm mn ka nq kb mp kd nr ke mr kg ns kh mt mu bi translated">我们在哪个建模步骤应用特征缩放？</h1><p id="53da" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">值得一提的是，在应用任何类型的数据标准化之前，<strong class="lc iv">我们首先需要将初始数据集分成训练集和测试集</strong>。不要忘记测试数据点代表真实世界的数据。</p><p id="0175" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">如前所述，在标准化我们的数据时，会考虑均值和标准差。如果我们取整个数据集的平均值和方差，那么我们将把未来的信息引入到训练解释变量中。因此，我们应该对训练数据执行特征缩放，然后对测试实例也执行归一化，但是这次使用训练解释变量的平均值和标准偏差。这样，我们可以测试和评估我们的模型是否可以很好地推广到新的、看不见的数据点。</p><h1 id="d952" class="md me iu bd mf mg no mi mj mk np mm mn ka nq kb mp kd nr ke mr kg ns kh mt mu bi translated">探索特征缩放对葡萄酒识别数据集的影响</h1><p id="de0e" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">现在让我们假设我们想要对<a class="ae kz" href="https://archive.ics.uci.edu/ml/machine-learning-databases/wine/" rel="noopener ugc nofollow" target="_blank"> UCI ML 葡萄酒识别数据集</a>执行主成分分析(PCA)。</p><blockquote class="nt nu nv"><p id="6524" class="la lb na lc b ld le jv lf lg lh jy li nw lk ll lm nx lo lp lq ny ls lt lu lv in bi translated">这些数据是对生长在意大利同一地区但来自三个不同品种的葡萄酒进行化学分析的结果。这项分析确定了三种葡萄酒中 13 种成分的含量。</p></blockquote><p id="3434" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">特征是<em class="na">酒精、苹果酸、灰分、灰分的碱度、镁、总酚、黄酮类化合物、非黄酮类化合物、原花青素、颜色强度、色调、稀释酒的 od 280/od 315</em>和<em class="na">脯氨酸</em>。目标是预测品种可能是<em class="na">类 _0 </em>、<em class="na">类 _1 </em>和<em class="na">类 _2 </em>中的一个。</p><p id="9927" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在这个例子中，我们将首先跳过特征缩放步骤，观察不进行预处理步骤时的结果。然后，我们将重复相同的程序，但这次使用特征缩放，最后比较结果。</p><p id="3cbd" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">第一步:加载数据</strong></p><p id="16c9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们加载数据，并将我们的特征从它们各自的目标变量中分离出来:</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="e2f4" class="nh me iu nd b gz ni nj l nk nl">from sklearn.datasets import load_wine</span><span id="8661" class="nh me iu nd b gz nm nj l nk nl">features, target = load_wine(return_X_y=True)</span></pre><p id="5d88" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">步骤 2:将初始数据集分成训练集和测试集</strong></p><p id="e5b4" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">如前所述，在采取预处理步骤之前，我们首先需要将数据集分成训练和测试。前者将用于模型训练，后者用于评估已训练模型的性能。</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="e107" class="nh me iu nd b gz ni nj l nk nl">from sklearn.model_selection import train_test_split</span><span id="a3d3" class="nh me iu nd b gz nm nj l nk nl">X_train, X_test, y_train, y_test = train_test_split(<br/>    <!-- -->features, target<!-- -->, test_size=0.3, random_state=42<br/>)</span></pre><p id="2b76" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">第三步:缩放数据</strong></p><p id="5ba1" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在，我们需要缩放数据，以便我们适合缩放器，并使用在观察训练示例后学习到的参数来转换训练集和测试集。</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="a147" class="nh me iu nd b gz ni nj l nk nl">from sklearn.preprocessing import StandardScaler<br/></span><span id="57e2" class="nh me iu nd b gz nm nj l nk nl">scaler = StandardScaler()<br/>X_train_scaled = scaler.fit_transform(X_train)<br/>X_test_scaled = scaler.transform(X_test)</span></pre><p id="2121" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">第四步:使用 PCA 进行降维</strong></p><p id="6c92" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在我们可以进行主成分分析。为了方便起见，我将使用两个组件，以便稍后在二维空间中更容易可视化结果:</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="d09e" class="nh me iu nd b gz ni nj l nk nl">pca = PCA(n_components=2)<br/>X_train_dim_red = pca.fit_transform(X_train_scaled)<br/>X_test_dim_red = pca.transform(X_test_scaled)</span></pre><p id="f4a8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在，我们可以在缩放和执行降维后快速可视化训练实例(下面给出了代码的链接):</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nz"><img src="../Images/73a65960d1b14750e0802183e81fd8db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hWspKj3NkTx3Mj6ue-4C1g.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">应用 StandardScaler 和 PCA 后的训练示例</p></figure><p id="3d9a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">下图生成了相同的图，但这次没有应用特征缩放:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nz"><img src="../Images/b09a84f7d316a3af6aaec67a8bd84209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VcMyxYX8dGxDZ_Jc9i6Nyg.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">应用 PCA 后的训练示例(但不是 StandardScaler)</p></figure><p id="6412" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">第五步:训练和评估模型</strong></p><p id="21a0" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">最后，我们可以拟合高斯朴素贝叶斯模型，并在测试实例上评估该模型的性能:</p><pre class="kk kl km kn gu nc nd ne nf aw ng bi"><span id="8641" class="nh me iu nd b gz ni nj l nk nl">from sklearn.naive_bayes import GaussianNB</span><span id="59e6" class="nh me iu nd b gz nm nj l nk nl">model = GaussianNB()<br/>model.fit(X_train_dim_red, y_train)<br/>predictions = model.predict(X_test_dim_red)<br/>print(f'Model Accuracy: {accuracy_score(y_test, predictions):.2f}')<br/>&gt;&gt;&gt; Model Accuracy: 0.98</span></pre><p id="638e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在测试实例上，模型准确率达到 98%。在没有应用缩放的情况下，测试精度下降到 0.81%。</p><p id="e8f2" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">完整的代码可以在</strong> <a class="ae kz" href="https://gist.github.com/gmyrianthous/59ce220c2785225d2efd4dd8cc57730f" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iv"> Github 上找到，作为一个要点。</strong>T13】</a></p><h1 id="0888" class="md me iu bd mf mg no mi mj mk np mm mn ka nq kb mp kd nr ke mr kg ns kh mt mu bi translated">结论</h1><p id="cc5c" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">特征缩放是我们在训练机器学习模型之前需要考虑的最基本的预处理步骤之一。正如我们已经讨论过的，我们需要理解是否需要特性缩放。这取决于我们要构建的模型(例如，基于树的模型不需要任何类型的特征缩放)和我们特征值的性质。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><p id="dfed" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><a class="ae kz" href="https://gmyrianthous.medium.com/membership" rel="noopener"> <strong class="lc iv">成为会员</strong> </a> <strong class="lc iv">阅读介质上的每一个故事。你的会员费直接支持我和你看的其他作家。</strong></p></div></div>    
</body>
</html>