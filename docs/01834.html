<html>
<head>
<title>Knowing What and Why? — Explaining Image Classifier Predictions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">知道什么和为什么？—解释图像分类器预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/knowing-what-and-why-explaining-image-classifier-predictions-680a15043bad?source=collection_archive---------11-----------------------#2020-02-20">https://towardsdatascience.com/knowing-what-and-why-explaining-image-classifier-predictions-680a15043bad?source=collection_archive---------11-----------------------#2020-02-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2172" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">数据科学工具包</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e681803258c98ee84c0391c7cf29bf7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Bli2ZcV4mP8bHFRMbbe9GA.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试库提供的解释的比较</p></figure><h1 id="f4b7" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="5a5b" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi mj translated">事实上，每年机器学习(ML)都在成为我们生活中越来越重要的一部分。ML帮助我们完成简单的日常任务，比如找到回家的最佳路线，用手机准确扫描文件或将文本翻译成不同的语言。然而，它也被用作许多高度负责的系统中的一个关键要素，在这些系统中，人的生命和福祉受到威胁。</p><ul class=""><li id="c7b6" class="ms mt iq lp b lq mu lt mv lw mw ma mx me my mi mz na nb nc bi translated">人工智能(AI)被用于司法系统，以评估被定罪的人再次违法的可能性有多大。</li><li id="de31" class="ms mt iq lp b lq nd lt ne lw nf ma ng me nh mi mz na nb nc bi translated">一些公司正在研发能够帮助甚至完全免除医生在医学图像上定位肿瘤的系统</li></ul><p id="090c" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated">随着我们实施类似于上面提到的那些系统，越来越清楚的是<strong class="lp ir">我们不仅要提供预测，还要提供解释</strong>什么影响了我们的决策。在这篇文章中，我将比较和测试最常用的库，用于解释<strong class="lp ir">图像分类</strong> — <strong class="lp ir"> Eli5、LIME和SHAP </strong>领域的模型预测。我们将调查他们利用的算法，以及比较所提供的解释的效率和质量。</p><p id="cfa0" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated">注意:因为我不想让你厌烦滚动巨大的代码片段，所以我只在文章中放了一小部分。然而，如果你想创建类似的可视化，或者学习如何解释你的分类器的预测，我鼓励你访问我的<a class="ae nl" href="https://github.com/SkalskiP/ILearnDeepLearning.py/tree/master/02_data_science_toolkit/03_explaining_image_classifier_predictions" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</p><h1 id="7b17" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">权衡取舍</h1><p id="da27" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi mj translated">在我们开始之前，让我们先回答一个最基本但同时也是最重要的问题。为什么我们甚至需要解释？难道我们不能解释模型的预测来了解是什么影响了它们吗？不幸的是，在很多情况下，答案是否定的</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/001c080a59c6c4b654688d81a5be5bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oqUZGGkAs0tZPcP_DBk4pw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型的潜在准确性和可解释性之间的权衡</p></figure><p id="7d01" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated">我们可以说在模型的复杂性和它的可解释性之间有一个权衡。由于一些问题——尤其是在计算机视觉领域——非常复杂，我们必须使用强大而复杂的模型来解决它们。这些类型的模型通常被称为黑盒，因为我们不知道或不理解它们内部发生了什么。因此，我们经常为了模型的准确性而牺牲可解释性。</p><h1 id="7bde" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">图像分类</h1><p id="3f9a" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi mj translated">正如我在第一段提到的，在这个项目中，我们将处理图像分类模型。这是最古老和最受认可的计算机视觉(CV)任务之一，它涉及到给照片分配一个带有对象类别名称的标签。尽管看起来很简单，但这是一项多年来给研究人员带来巨大问题的任务。突破性的想法是<a class="ae nl" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet </a> —卷积神经网络(CNN)，在2012年的ImageNet比赛中首次使用。AlexNet以大约低10个百分点的误差击败了其他参与者。这个解决方案彻底改变了计算机视觉。从那时起，CNN就成了这个研究分支中所有任务的默认答案。不幸的是，CNN是黑盒的一个例子——尽管我们知道黑盒内部发生的变化，但即使对于该领域的专家来说，对它们的全面解释也是一个问题。</p><p id="8ace" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated"><strong class="lp ir">注:</strong>如果你想更深入地探索CNN，我强烈建议你阅读我的另一篇文章——<a class="ae nl" rel="noopener" target="_blank" href="/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9">深入了解卷积神经网络背后的数学</a>。</p><h1 id="f262" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">ELI5</h1><p id="a464" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi mj translated">我们将研究的第一个库是<a class="ae nl" href="https://github.com/TeamHG-Memex/eli5" rel="noopener ugc nofollow" target="_blank">Eli 5</a>——这是一个简单但可靠的工具，旨在可视化、检查和调试ML模型。该库允许解释用Keras编写的图像分类器的预测。为此，Eli5利用了梯度加权类激活映射(Grad-CAM)算法。值得注意的是，这不是一个通用的方法，它仅适用于CNN解决方案。</p><h2 id="95c9" class="nm kw iq bd kx nn no dn lb np nq dp lf lw nr ns lh ma nt nu lj me nv nw ll nx bi translated">Grad-CAM</h2><p id="f5aa" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><a class="ae nl" href="https://arxiv.org/abs/1610.02391" rel="noopener ugc nofollow" target="_blank"> Grad-CAM </a>是<a class="ae nl" href="https://arxiv.org/abs/1512.04150" rel="noopener ugc nofollow" target="_blank"> CAM </a>的下一个迭代，这是将CNN的预测可视化的最初想法之一。它使用激活图的值以及反向传播来理解预测的来源。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e3f77b74335ae7a3cfed5e3c9df13c7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*-R_QfQHgoVB0w0F8kFPLwg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ResNet34激活在六个不同网络深度的可视化。随着层深度的增加，激活分辨率降低。</p></figure><p id="7ee3" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated">过滤图(或激活图)是将一个卷积核应用于层输入的结果。根据过滤器中的值，它们对不同的模式做出反应。重要的是，位于CNN不同深度的卷积层对它们所观察的物体的不同细节水平做出反应。浅层次对简单的形状有反应——不同角度的线条，相同亮度的表面。另一方面，深层的CNN会随着特定的复杂模式的出现而发光。此外，在大多数情况下，深层CNN层中的激活图的分辨率比图形开始处的低。</p><p id="803e" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated">Grad-CAM解释是通过执行前向激活图的线性加权组合，然后执行ReLU来获得的。相对于特征图激活，通过对给定类别(在Softmax之前)获得的分数梯度进行平均来计算线性方程的系数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/2ea6f7d28ee0fa6cee8e7c8febf2059d.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*Kjs3xdVk9dUfRbk39nzfjg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/533fadef0da42cf9458cdbd7036b4423.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*zKTjL6krd2RIviJEGU5l_g.png"/></div></figure><h2 id="2c3d" class="nm kw iq bd kx nn no dn lb np nq dp lf lw nr ns lh ma nt nu lj me nv nw ll nx bi translated">例子</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">允许使用Eli5解释任何基于Keras的CNN预测的代码片段— <a class="ae nl" href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/02_data_science_toolkit/03_explaining_image_classifier_predictions/01_coco_res_net/coco-resnet-keras-resnet-34-eli5.ipynb" rel="noopener ugc nofollow" target="_blank">完整示例</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/8141ec2dc7adf33b8930b3972626921f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RPQvZlzEx_-2weFdOwm4tQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用Eli5为ResNet34型号获得的解释</p></figure><h2 id="4dc5" class="nm kw iq bd kx nn no dn lb np nq dp lf lw nr ns lh ma nt nu lj me nv nw ll nx bi translated">摘要</h2><ul class=""><li id="8eea" class="ms mt iq lp b lq lr lt lu lw od ma oe me of mi mz na nb nc bi translated"><strong class="lp ir">[+]</strong>Eli 5背后的思想很简单，计算量也很小，因此，该算法既容易理解，又能很快执行<strong class="lp ir"/>。</li><li id="ff5b" class="ms mt iq lp b lq nd lt ne lw nf ma ng me nh mi mz na nb nc bi translated"><strong class="lp ir"> [―] </strong>该方法仅用于<strong class="lp ir">解释CNN预测</strong>。</li><li id="cadf" class="ms mt iq lp b lq nd lt ne lw nf ma ng me nh mi mz na nb nc bi translated"><strong class="lp ir"> [―] </strong>目前库<strong class="lp ir">只支持基于Keras的模型。</strong></li><li id="7dd5" class="ms mt iq lp b lq nd lt ne lw nf ma ng me nh mi mz na nb nc bi translated"><strong class="lp ir"> [―] </strong>该方法的精度有限——取决于<strong class="lp ir">激活图分辨率</strong>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/86d9002056398e8d863b3b7f9232ac29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fY2EE5d3q3CYNgFwHrZU4g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示CNN激活的分辨率如何影响解释的精确度的可视化</p></figure><h1 id="1a32" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">石灰</h1><p id="8982" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi mj translated">我为什么要相信你？:解释任何分类器的预测是一篇文章，是旨在解释ML模型的整个研究分支的基础。本文中包含的思想成为了最流行的解释库——本地可解释模型不可知解释(LIME)的基础。这种算法与Grad-CAM完全不同，它试图通过扰动输入数据来理解模型，并理解这些变化如何影响预测。</p><h2 id="f612" class="nm kw iq bd kx nn no dn lb np nq dp lf lw nr ns lh ma nt nu lj me nv nw ll nx bi translated">代理模型</h2><p id="abea" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi mj translated">这一次，我们的目标是用简单、可理解的线性模型取代复杂的黑盒模型。当然，新模型只是原模型的近似，但经过选择，它忠实地代表了当地的情况。这个方向的第一步是将图像分成超像素——颜色和亮度相似的相邻像素组。这种方法是有意义的，因为照片的分类可能是由许多像素决定的，所以单个像素的扰动对预测的影响很小。然后，我们创建一个人工照片集合，通过用灰色替换原始照片的随机超像素来创建。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fe2dca312d35e975f3bf509ffbd5feac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W4o2o84jUdHKJv0AklqtRA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">石灰算法</p></figure><p id="a38c" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated">我们还定义了一个函数<strong class="lp ir"><em class="og"/></strong>，它将允许我们确定原始图像和样本之间的相似程度，其中<strong class="lp ir"> <em class="og"> D </em> </strong>是图像之间的余弦距离，而<strong class="lp ir"><em class="og">【σ</em></strong>是图像的宽度。为了计算这个度量，我们将数码照片展平，并将其视为一个矢量。然后，我们计算代表被比较的两幅图像的向量之间的角度的余弦。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/66111ec61c48b4c58c197b6fbefbb788.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*AjxJFSHdsK7mPOzOQRmk1g.png"/></div></figure><p id="b1e8" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated">现在你需要做的就是找到原始模型对输入图像<strong class="lp ir"> <em class="og"> f(x) </em> </strong>的预测，以及线性模型对扰动图像<strong class="lp ir"><em class="og">g(z’)</em></strong>的预测，并求解由<strong class="lp ir"> <em class="og"> L </em> </strong>定义的加权回归。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/8ab33be6e377bac96a1b5b7ab7714d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*ZAHpFaP2UUbrK8CYwu2vFw.png"/></div></div></figure><h2 id="1dce" class="nm kw iq bd kx nn no dn lb np nq dp lf lw nr ns lh ma nt nu lj me nv nw ll nx bi translated">稳健性</h2><p id="1d00" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我们当然期望从梦的解释库中得到的一个关键特征是健壮性。我们想要两张几乎相同的图像来给出非常接近的解释。然而，事实证明<strong class="lp ir">石灰在这方面高度不稳定</strong>。</p><p id="7531" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated">为了验证这一点，我进行了一个实验——我解释了图像分类器对两幅几乎相同的图像的预测。第二张图片是通过在原始图片中添加少量高斯噪声创建的——如此之小，以至于从模型中获得的概率几乎不会改变，图像对于肉眼来说是无法区分的。然而，解释上的差异却是显著的。</p><h2 id="a89f" class="nm kw iq bd kx nn no dn lb np nq dp lf lw nr ns lh ma nt nu lj me nv nw ll nx bi translated">例子</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">允许使用LIME解释任何图像分类器预测的代码片段— <a class="ae nl" href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/02_data_science_toolkit/03_explaining_image_classifier_predictions/01_coco_res_net/coco-resnet-keras-resnet-34-lime.ipynb" rel="noopener ugc nofollow" target="_blank">完整示例</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/ee8159ac9bc6d15eb9a09b6b976129ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S443H3ybDFT0unz_kdZPTQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用ResNet34型号的石灰获得的解释</p></figure><h2 id="d456" class="nm kw iq bd kx nn no dn lb np nq dp lf lw nr ns lh ma nt nu lj me nv nw ll nx bi translated">摘要</h2><ul class=""><li id="3ec0" class="ms mt iq lp b lq lr lt lu lw od ma oe me of mi mz na nb nc bi translated"><strong class="lp ir"> [+]模型不可知方法</strong> —完全独立于ML模型的类或架构。</li><li id="46a5" class="ms mt iq lp b lq nd lt ne lw nf ma ng me nh mi mz na nb nc bi translated"><strong class="lp ir"> [+] </strong>库有现成的实现，可以让你解释任何图像分类器的预测。</li><li id="de60" class="ms mt iq lp b lq nd lt ne lw nf ma ng me nh mi mz na nb nc bi translated"><strong class="lp ir"> [―]耗时的</strong>计算，取决于所选的超参数，对于单个图像可能会持续几分钟。</li><li id="a382" class="ms mt iq lp b lq nd lt ne lw nf ma ng me nh mi mz na nb nc bi translated"><strong class="lp ir"> [―]稳定性问题</strong>——即使是图像的微小变化，也会导致截然不同的解释</li></ul><h1 id="3204" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">SHAP</h1><p id="9408" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi mj translated">Hapley加法解释(SHAP)和LIME非常相似——两者都是解释个体预测的加法和模型不可知的方法。然而，SHAP的目的是通过计算每个特征对预测的贡献来解释给定输入的模型预测。为了实现这个目标，SHAP使用了沙普利值，它最初来自博弈论。</p><h2 id="5e6f" class="nm kw iq bd kx nn no dn lb np nq dp lf lw nr ns lh ma nt nu lj me nv nw ll nx bi translated">沙普利值</h2><p id="dd44" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">首先，为了更好地理解所分析的算法，我们来解释一下什么是Shapley值。这是Shapley (1953)描述的一种方法，根据游戏玩家对总收益的贡献来分配奖励。这个想法已经被转移到SHAP，作为评估哪个特征对模型的最终预测贡献最大的一种方式。它被描述为一个特征值在所有可能的联合中的平均边际贡献。联盟向量<strong class="lp ir"><em class="og">z’</em></strong>为每个特征分配值0或1，定义它是否出现在联盟中。使用函数<strong class="lp ir"> <em class="og"> h </em> </strong>将这些向量映射到特征空间——对于图像，该函数填充灰色超像素，对应的向量值为0。</p><p id="ef5d" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated">KernelSHAP这是一个想法的组合，从LIME开始就知道了，但是使用了Shapley值。类似地，与石灰一样，我们使用超像素来限制特征的数量，将相似颜色和亮度的像素组合起来。我们还使用加权回归来建立线性模型，其中每个超像素代表我们模型的单个特征。</p><p id="151b" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated">关键区别在于回归模型权重的选择。对于LIME，它是原始图像和扰动图像之间的余弦度量，而对于KernelSHAP，使用以下公式来确定权重，其中<strong class="lp ir"> <em class="og"> M </em> </strong>是最大联合，<strong class="lp ir"><em class="og">| z’|</em></strong>是所考虑的联合中的特征数量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/290e116571a48830d16a235e2b88e663.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*KvkAH4Neh0QdXZ5Y2aC_Fg.png"/></div></figure><h2 id="0432" class="nm kw iq bd kx nn no dn lb np nq dp lf lw nr ns lh ma nt nu lj me nv nw ll nx bi translated">例子</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">允许使用SHAP解释任何图像分类器预测的代码片段— <a class="ae nl" href="https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/02_data_science_toolkit/03_explaining_image_classifier_predictions/01_coco_res_net/coco-resnet-keras-resnet-34-shap.ipynb" rel="noopener ugc nofollow" target="_blank">完整示例</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/52bd45b6485b69ef5e035c3c16054afe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4p5KfVCiAmZ-irK93UjtA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从ResNet34型号的SHAP获得的解释</p></figure><h2 id="16c4" class="nm kw iq bd kx nn no dn lb np nq dp lf lw nr ns lh ma nt nu lj me nv nw ll nx bi translated">摘要</h2><ul class=""><li id="d2f3" class="ms mt iq lp b lq lr lt lu lw od ma oe me of mi mz na nb nc bi translated"><strong class="lp ir"> [+]源于博弈论的坚实数学基础</strong>，使其具有很强的可解释性。</li><li id="ca78" class="ms mt iq lp b lq nd lt ne lw nf ma ng me nh mi mz na nb nc bi translated"><strong class="lp ir"> [+]模型不可知方法</strong></li><li id="5188" class="ms mt iq lp b lq nd lt ne lw nf ma ng me nh mi mz na nb nc bi translated"><strong class="lp ir"> [―]极其缓慢</strong>，因为它需要计算许多沙普利值</li><li id="dd17" class="ms mt iq lp b lq nd lt ne lw nf ma ng me nh mi mz na nb nc bi translated"><strong class="lp ir"> [―] </strong> KernelSHAP是一个通用算法，如果我们想用它来解释CV模型的预测，我们需要定义我们的函数<strong class="lp ir"> <em class="og"> h </em> </strong>将联盟映射到图像特征。</li></ul><h1 id="90c0" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">基准</h1><p id="30cc" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">我们</span>将使用执行模型解释所必需的墙时间，作为我们库性能的度量。为了避免与预测解释无关的因素的影响，我们将依赖于100个单独案例的平均时间。获得的值如下所示。测试是在没有GPU的计算机上进行的，该计算机配备了英特尔酷睿i7–7700 HQ CPU处理器(2.80GHz)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f0bfbec92ddb4edc29034c6cdae72485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C97VN7ESN9wAxO3yLc6_gw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试图书馆的平均讲解时间</p></figure><h1 id="e707" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结论</h1><p id="88bb" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi mj translated">如果你设法来到这里，祝贺你。非常感谢你花时间阅读这篇文章。如果你喜欢这篇文章，考虑把它分享给你的朋友，或者两个或五个朋友。我希望这篇文章是有帮助的，现在你知道如何解释你的图像分类器的预测。</p><p id="eb9b" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw ni ly lz ma nj mc md me nk mg mh mi ij bi translated">本文是“数据科学工具包”系列的另一部分，如果您还没有机会，请阅读<a class="ae nl" rel="noopener" target="_blank" href="/simple-method-of-creating-animated-graphs-127c11f58cc5">其他文章</a>。此外，如果你喜欢我目前的工作，请在<a class="ae nl" href="https://twitter.com/PiotrSkalski92" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae nl" href="https://medium.com/@piotr.skalski92" rel="noopener"> Medium </a>上关注我，并在<a class="ae nl" href="https://github.com/SkalskiP" rel="noopener ugc nofollow" target="_blank"> GitHub </a>和<a class="ae nl" href="https://www.kaggle.com/skalskip" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上查看我正在进行的其他项目。保持好奇！</p></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><div class="kg kh ki kj gt ab cb"><figure class="os kk ot ou ov ow ox paragraph-image"><a href="https://github.com/SkalskiP/ILearnDeepLearning.py"><img src="../Images/095cfbb7954f6fa410b1ea404383a0c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0ftx4YXKIvvHjGPovjkanw.png"/></a></figure><figure class="os kk ot ou ov ow ox paragraph-image"><a href="https://github.com/SkalskiP/make-sense"><img src="../Images/53811bbf6840c2d54f2e797035739a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*JGG-Es4txMrKcN6usNlgeg.png"/></a></figure><figure class="os kk ot ou ov ow ox paragraph-image"><a href="https://twitter.com/PiotrSkalski92"><img src="../Images/05564a6f663fe1a9638a456bf05da509.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*-ObBrMMfnVFCG5u--QOZ6g.png"/></a></figure></div></div></div>    
</body>
</html>