<html>
<head>
<title>Policy Iteration in RL: A step by step Illustration</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RL中的策略迭代:一步一步的说明</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/policy-iteration-in-rl-an-illustration-6d58bdcb87a7?source=collection_archive---------1-----------------------#2020-03-25">https://towardsdatascience.com/policy-iteration-in-rl-an-illustration-6d58bdcb87a7?source=collection_archive---------1-----------------------#2020-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e566" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">强化学习</h2><div class=""/><div class=""><h2 id="de03" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">策略迭代算法指南</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/df68be6cfd3c92ad8cc4130b0f48d046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rsfL_KxzQi9lJ3oW7FSlLA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:图片由来自<a class="ae lh" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3946618" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae lh" href="https://pixabay.com/users/AnnaliseArt-7089643/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3946618" rel="noopener ugc nofollow" target="_blank"> Annalise Batista </a>提供</p></figure><p id="d9c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">策略迭代是“强化学习”中的一种算法，它有助于学习使长期贴现回报最大化的最优策略。当有多个选项可供选择，并且每个选项都有自己的回报和风险时，这些技术通常是有用的。</p><p id="9d3f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文中，我们将把“策略迭代”算法应用到一个简单的游戏中，这个游戏涉及一些海盗，他们必须在冒险和有利的情况下到达目的地。</p><h1 id="4b25" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">游戏概述:</strong></h1><p id="1108" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">让我们考虑一艘海盗船，它现在停泊在一个岛上，必须安全到达它的祖国。有两条路线可供选择。</p><p id="ee48" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果它走向北的路线，那么它可以到达一个满是黄金的岛屿，它可以收集黄金，然后向南移动到达家园。然而，在黄金岛的北面有一个重力非常高的区域(像百慕大三角)。如果船误到了那里，那么它就会被吸进去，船就永远失去了。</p><p id="2d9e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果它走向南的路线，那么它可以到达一个满是白银的岛屿，它可以收集这些白银，然后向北移动到达家园。在银岛的南面有一个监狱岛。如果船误落在那里，那么船将被捕获，船员将被监禁。</p><p id="ad50" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">到目前为止，一切看起来都很好，然而，游戏中有一个转折。如果我们的生活是确定的，那就没有乐趣。让我们在游戏中引入随机性。</p><p id="771a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">还记得电影《加勒比海盗》中的杰克·斯派洛吗？！。假设我们的船长有一个破罗盘，类似于杰克的罗盘。所以，每次，船长向北移动，他向北移动的概率是0.8，然而，他可能会错过标记，以0.2的概率到达南方。同样，如果他往南走，有0.8的概率往南走，有0.2的概率往北走。</p><h2 id="5ffa" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated"><strong class="ak">奖励(正面和负面):</strong></h2><p id="fd50" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">让我们给每一次登陆的船只分配一些奖励。</p><ul class=""><li id="7edf" class="nm nn it lk b ll lm lo lp lr no lv np lz nq md nr ns nt nu bi translated">到达家园将允许海盗船收集+1点。</li><li id="cc1c" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated">登陆金岛将允许海盗船收集+2点积分。</li><li id="ff83" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated">登陆银岛将允许海盗船收集+1点。</li><li id="3ce3" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated">如果船被吸进百慕大三角，那么船得-2分。</li><li id="d55d" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated">如果这艘船是在监狱岛被捕获的，那么这艘船得-0.5分。</li></ul><p id="a106" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这种情况下，我们的目标是找到最优策略，使船只安全到达其祖国，并获得最大回报。</p><h1 id="cd80" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">快速回顾“策略迭代”算法:</strong></h1><p id="5eb0" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">策略迭代有三个步骤:</p><ol class=""><li id="a561" class="nm nn it lk b ll lm lo lp lr no lv np lz nq md oa ns nt nu bi translated">初始化随机策略</li><li id="ad2b" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md oa ns nt nu bi translated">政策评价</li><li id="94e2" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md oa ns nt nu bi translated">政策改进</li></ol><h2 id="bdf9" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated"><strong class="ak">什么是政策？</strong></h2><p id="6c8d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">策略是动作到系统中每个可能状态的映射。最优策略是使长期回报最大化的策略。因此，对于这个例子，我们可以有多个策略。即，在每个州(岛)有多组动作，然而，可能只有一个策略给我们最终的最大回报。我们的目标是找到最佳策略。</p><h2 id="71c7" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated"><strong class="ak">第一步:</strong></h2><p id="f6a5" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">随机初始化策略。在系统的每个状态下随机初始化动作。</p><h2 id="5129" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated"><strong class="ak">第二步:</strong></h2><p id="b78e" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">步骤2基于贝尔曼方程，该方程如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2ec284013747dff67646a96947cace6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/1*JjSpaM_PE2U3cey4xzYHMQ.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">政策评价</p></figure><p id="5aea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">获取策略中每个状态的动作，并使用上面的等式评估值函数。这里p是转移概率，也用t表示。</p><h2 id="d91d" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated"><strong class="ak">第三步:</strong></h2><p id="cc43" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">对于每个状态，使用以下公式从价值函数中获得最佳行动:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/88143395c166d9d7d80bf197594b36b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/1*fPtLjFeABf8vp24uJL9cqg.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">政策改进</p></figure><p id="a272" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果最佳操作优于当前策略操作，则用最佳操作替换当前操作。</p><h2 id="a7cd" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated"><strong class="ak">策略迭代:</strong></h2><p id="aa3f" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">重复步骤2和3，直到收敛。如果策略在整个迭代过程中没有改变，那么我们可以认为算法已经收敛。</p><h1 id="4394" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">状态转换图:</strong></h1><p id="a1e0" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">首先要做的是理解状态和动作，并构建一个状态转换图。在我们的例子中，每个岛屿是一个州，有两个动作，“北”和“南”。每个州的奖励如上图所示。基于这些事实，我们可以构建如下的状态转换图:</p><p id="3d13" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有六个州，包括起始州和目的地州，以及四个中间岛屿，他们可以在那里上车。让我们将从S1到S6的状态标记如下:</p><ol class=""><li id="05f4" class="nm nn it lk b ll lm lo lp lr no lv np lz nq md oa ns nt nu bi translated">S1:开始状态</li><li id="ee66" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md oa ns nt nu bi translated">S2:黄金岛登陆状态</li><li id="01cb" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md oa ns nt nu bi translated">S3:登陆银岛的状态</li><li id="f29c" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md oa ns nt nu bi translated">S4:登陆百慕达三角岛的状态</li><li id="58a8" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md oa ns nt nu bi translated">S5:目的岛的着陆状态</li><li id="a3f2" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md oa ns nt nu bi translated">S6:在监狱岛登陆的状态</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi od"><img src="../Images/9fe09fc5f62e4530a17fdb8dd9ecf812.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*-U31n7RLghA_3jgGZCuxIw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">状态转移图</p></figure><h1 id="2c4f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">转移概率矩阵:</strong></h1><p id="d65b" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">由于游戏是随机的，我们需要计算每个状态/动作对的转移概率。基于上面提供的概率和状态转移图，我们可以绘制如下图。注意，有两个动作，我们需要这两个动作的转移概率矩阵。注意，在应用贝尔曼方程时，T(S，a，S’)指的是在采取动作‘a’之后从状态S移动到状态S’的转移概率。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/198611a3f6804fe5494b11a8f8877618.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/1*X6_qII0qSB86ZGOBt8TQsw.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">北部行动的转移概率矩阵</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/bccabb4589ba00a5cd59d48259fc9264.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/1*kbOxwx9C60HIITcB8IZqyg.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">南方行动的转移概率矩阵</p></figure><h1 id="f2c7" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">策略迭代算法:</h1><p id="ab2d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">有了这些信息，让我们一步一步地应用上述算法。我们可以假设贴现因子(γ)为1。</p><h2 id="381a" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated"><strong class="ak">初始随机策略:</strong></h2><p id="6d4f" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">让我们随机初始化策略(状态到动作的映射),对于所有的状态都是向北移动。</p><p id="2ba5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">P = {N，N，N，N，N，N}</p><p id="dc2b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们观察状态转换图，状态S4、S5、S6在这些状态中没有任何支持的动作，因为这些是结束状态。因此，让我们缩减我们的政策，仅适用于我们可以采取行动的前三个州。</p><p id="5a58" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">P = {N，N，N}</p><h2 id="4360" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated"><strong class="ak">第一次迭代:</strong></h2><p id="7428" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">让我们假设所有状态的初始值V(s)为0。因此，贝尔曼方程将简化为V(s) = R(s)，其中R(s)是进入一种状态的回报。</p><p id="9415" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首次迭代的策略评估:</p><p id="1946" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">v[S1]= 0；V[S4] = -2</p><p id="4fc6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">v[S2]= 2；V[S5] = 1</p><p id="dfb8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">v[S3]= 1；V[S6] = -0.5</p><p id="e80e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首次迭代的策略改进:</p><p id="c8b7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们应用上面提供的公式来改进政策。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/c23fef3c4eedaaa11fafd60965c91f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wO6wbObzyaAVfcqpWSgiuQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">I迭代:政策改进</p></figure><p id="6b12" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据上表获得的策略如下:</p><p id="d836" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">P = {N，S，N}</p><h2 id="c2df" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated"><strong class="ak">第二次迭代:</strong></h2><p id="8ddd" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">第二次迭代的策略评估:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/21d29fe476ac4b164efe1d443079698c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1G3qaN3Izq6zrpR4fdQB_A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">II迭代:策略评估</p></figure><p id="4bc0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">每个状态的值可以总结如下:</p><p id="96c0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">v[S1]= 3；V[S4] = -2</p><p id="30fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">v[S2]= 1；V[S5] = 1</p><p id="a2ad" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">v[S3]= 1.5；V[S6] = -0.5</p><p id="4965" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第二次迭代的策略改进:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/6e6434d2f68898e0e5fc55034bee7906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F0vQdT9Nbgo0SvUVz2HGbA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">二次迭代:政策改进</p></figure><p id="a87f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">基于上表获得的策略如下:</p><p id="d6ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">P = {S，S，N}</p><h2 id="3113" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated"><strong class="ak">第三次迭代:</strong></h2><p id="4cb3" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">第三次迭代的策略评估:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/67bda309ac18259bd842b8b0624e9343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v4nRomi9eHUZDzY1NfRkRA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">III迭代:策略评估</p></figure><p id="f309" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">每个状态的值可以总结如下:</p><p id="6e23" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">v[S1]= 2.5；V[S4] = -2</p><p id="0a97" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">v[S2]= 1；V[S5] = 1</p><p id="1b2d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">v[S3]= 1.5；V[S6] = -0.5</p><p id="b578" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第三次迭代的策略改进:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/74a67df0ba01a81a2435c59df5dace45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yMQ-2Z5qsFVMf6ZKAsogEw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">III迭代:政策改进</p></figure><p id="87f7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">基于上表获得的策略如下:</p><p id="71d4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">P = {S，S，N}</p><p id="e682" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们将此策略与我们在第二次迭代中获得的策略进行比较，我们可以观察到策略没有改变，这意味着算法已经收敛，这是最优策略。</p><h1 id="79fc" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">结论:</strong></h1><p id="314e" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">通过应用该算法，我们得到了最优策略为{南，南，北}。如果我们观察这个例子，最初可能会很想去北方，因为金岛有更多的奖励，但它充满了风险，因为我们可能会失去2分，并结束游戏。因此，最好是牺牲短期回报，选择最终能让我们的长期回报最大化的南线。</p><p id="4755" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望这个例子有助于更好地理解这个算法。</p><h1 id="d156" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">参考文献:</strong></h1><ol class=""><li id="cb6a" class="nm nn it lk b ll mw lo mx lr ol lv om lz on md oa ns nt nu bi translated">萨顿和巴尔托(2018年)。强化学习:导论。麻省理工出版社。</li><li id="66a5" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md oa ns nt nu bi translated">翁，莉莲。(2018年2月19日)。对强化学习的(长)窥视。检索自<a class="ae lh" href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning . html</a></li></ol></div></div>    
</body>
</html>