<html>
<head>
<title>Batch Normalisation Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批量标准化说明</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/batch-normalisation-explained-5f4bd9de5feb?source=collection_archive---------16-----------------------#2020-05-12">https://towardsdatascience.com/batch-normalisation-explained-5f4bd9de5feb?source=collection_archive---------16-----------------------#2020-05-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7470" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">批量标准化</h2><div class=""/><div class=""><h2 id="ecfa" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">批量标准化工作原理及其解决的问题的详细指南</h2></div><p id="0a0a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这篇文章中，我详细介绍了批量规范化及其工作原理。Loffe 和 Szegedy 在 2015 年引入了批量规范化，并迅速成为几乎每个深度网络中实施的标准功能。</p><h1 id="35b5" class="ln lo it bd lp lq lr ls lt lu lv lw lx ki ly kj lz kl ma km mb ko mc kp md me bi translated">概述</h1><ol class=""><li id="1fe6" class="mf mg it kt b ku mh kx mi la mj le mk li ml lm mm mn mo mp bi translated">内部协变量移位</li><li id="3117" class="mf mg it kt b ku mq kx mr la ms le mt li mu lm mm mn mo mp bi translated">消失和爆炸渐变</li><li id="4f26" class="mf mg it kt b ku mq kx mr la ms le mt li mu lm mm mn mo mp bi translated">批量标准化是如何工作的？</li><li id="95e8" class="mf mg it kt b ku mq kx mr la ms le mt li mu lm mm mn mo mp bi translated">批量标准化的优势</li></ol></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="3383" class="ln lo it bd lp lq nc ls lt lu nd lw lx ki ne kj lz kl nf km mb ko ng kp md me bi translated">1.内部协变量移位</h1><p id="32a8" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la nh lc ld le ni lg lh li nj lk ll lm im bi translated">批量标准化处理的关键问题是<em class="nk">内部协变量移位</em>。由于神经网络的本质，会发生内部协变量移位。在训练的每一个时期，<strong class="kt jd">权重被更新，不同的数据被处理</strong>，这意味着神经元的输入每次都略有不同。当这些变化传递到下一个神经元时，它会产生一种情况，即每个神经元的输入分布在每个时期都是不同的。</p><p id="d3d6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通常，这没什么大不了的，但在深层网络中，输入分布的这些微小变化会快速累积，并在网络深处放大。最终，由最深层神经元接收的输入分布在每个时期之间变化很大。</p><p id="d1df" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，这些神经元需要不断适应不断变化的输入分布，这意味着它们的学习能力受到严重制约。这种不断变化的输入分布被称为内部协变量移位。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="ec1d" class="ln lo it bd lp lq nc ls lt lu nd lw lx ki ne kj lz kl nf km mb ko ng kp md me bi translated">2.消失和爆炸渐变</h1><p id="5ab9" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la nh lc ld le ni lg lh li nj lk ll lm im bi translated">批量规格化处理的另一个问题是消失或爆炸梯度。在整流线性单元(ReLUs)之前，使用饱和激活函数。饱和函数是具有朝向左右边界的“平坦”曲线的函数，例如 sigmoid 函数。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/e4ed932ce6b3391cd974ea7990bce018.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*r6UPzQ_rjbCljefhzpeRRA.png"/></div><p class="nt nu gj gh gi nv nw bd b be z dk translated">Sigmoid 曲线及其导数</p></figure><p id="c1fb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在 sigmoid 函数中，随着<em class="nk"> x </em>的值趋向于∞，梯度趋向于 0。随着神经网络被训练，权重可以被推向 s 形曲线的饱和端。这样，梯度变得越来越小，接近 0。</p><p id="ae8e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当这些小梯度在网络中更深地相乘时，它们会变得更小。当使用反向传播时，梯度以指数方式接近 0。这种“消失”梯度严重限制了网络的深度。</p><p id="fa1c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">虽然可以通过使用 ReLU 等非饱和激活函数轻松管理这种消失梯度，但批量归一化仍然有一席之地，因为它通过确保没有值变得过高或过低，防止权重被推到那些饱和区域。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="36a6" class="ln lo it bd lp lq nc ls lt lu nd lw lx ki ne kj lz kl nf km mb ko ng kp md me bi translated">3.批量标准化是如何工作的？</h1><p id="3817" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la nh lc ld le ni lg lh li nj lk ll lm im bi translated">批量标准化通过<strong class="kt jd">减去</strong>小批量<strong class="kt jd">平均值</strong>和<strong class="kt jd">除以</strong>小批量<strong class="kt jd">标准偏差</strong>来标准化层输入。小批量指的是为任何给定时期提供的一批数据，是整个训练数据的子集。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/05a468ab50a834bc471864e5881be434.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*TMJhyAQ3mL6Peupg_yjXhQ.png"/></div><p class="nt nu gj gh gi nv nw bd b be z dk translated">批量归一化的公式，其中 x̂指的是归一化的向量。</p></figure><p id="6427" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">标准化确保输入的平均值为 0，标准偏差为 1，这意味着每个神经元的输入分布是相同的，从而解决了内部协变量偏移的问题，并提供了正则化。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/38855da8c1b5c7482852db350350cafd.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*QQ2Q5rVBtLv7b3yGhO0flg.png"/></div></figure><p id="4d1d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，网络的代表性已经严重受损。如果对每一层进行归一化，则前一层所做的权重变化和数据间的噪声会部分丢失，因为在归一化过程中会丢失一些非线性关系。这可能导致传递次优权重。</p><p id="1cb9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了解决这个问题，批处理规格化增加了两个可训练的参数，gamma γ和 beta β，它们可以缩放和移动规格化的值。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/39fd5375b2e60478ead75efab1925eb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*1cG9JDWhrxE4EMQNvAaNrQ.png"/></div></figure><p id="6880" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随机梯度下降可以在标准反向传播过程中调整γ和β，以找到最佳分布，从而解决数据之间的噪声和权重变化的稀疏性。从本质上讲，这些参数缩放并改变了归一化的输入分布，以适应给定数据集的特性。</p><p id="c481" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">例如，假设未归一化的输入分布对于给定的数据集是最佳的，γ和β将收敛到√Var[x]和 E[x]，从而获得原始的未归一化的<em class="nk"> x </em>向量。因此，批量归一化确保归一化对于给定的数据集总是最优的。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="3166" class="oa lo it bd lp ob oc dn lt od oe dp lx la of og lz le oh oi mb li oj ok md iz bi translated">a.为什么要针对小批量进行标准化？</h2><p id="5d99" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la nh lc ld le ni lg lh li nj lk ll lm im bi translated">理想情况下，标准化应该针对整个训练数据集，因为这确保了不同批次之间的输入分布没有变化。然而，由于不在当前批次中的任何数据集都在反向传播的范围之外，随机梯度下降将不起作用，因为归一化中使用的统计数据来自该范围之外。</p><p id="0fef" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，标准化是针对小批量进行的，以确保可以进行标准反向传播。唯一的含义是，每一批应该在某种程度上代表整个训练集的分布，如果您的批大小不是太小，这是一个安全的假设。</p><h2 id="5a00" class="oa lo it bd lp ob oc dn lt od oe dp lx la of og lz le oh oi mb li oj ok md iz bi translated">b.测试阶段</h2><p id="6be1" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la nh lc ld le ni lg lh li nj lk ll lm im bi translated">在训练期间，使用小批量中的样本计算平均值和标准偏差。但是，在测试中，计算新值没有意义。因此，批量标准化使用在训练期间计算的运行平均值和运行方差。需要引入一个新的参数，动量或衰变。</p><pre class="nm nn no np gt ol om on oo aw op bi"><span id="144a" class="oa lo it om b gy oq or l os ot">running_mean = momentum * running_mean + (1-momentum) * new_mean<br/>running_var = momentum* running_var + (1-momentum) * new_var</span></pre><p id="7463" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">动量是对最后一次看到的小批量的重视，也被称为“滞后”。如果动量设置为 0，运行平均值和方差来自最后一次看到的小批量。然而，这可能是有偏见的，不是理想的测试。相反，如果动量设置为 1，它将使用第一个小批量的运行平均值和方差。本质上，动量控制每个新的小批量对运行平均值的贡献。</p><p id="6947" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">理想情况下，动量应设置为接近 1 (&gt;0.9)，以确保运行均值和方差的缓慢学习，从而忽略小批量中的噪声。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="d5c5" class="ln lo it bd lp lq nc ls lt lu nd lw lx ki ne kj lz kl nf km mb ko ng kp md me bi translated">4.批量标准化的优势</h1><h2 id="11a2" class="oa lo it bd lp ob oc dn lt od oe dp lx la of og lz le oh oi mb li oj ok md iz bi translated">a.更高的学习率</h2><p id="cb5d" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la nh lc ld le ni lg lh li nj lk ll lm im bi translated">通常，较大的学习速率会导致梯度消失/爆炸。然而，由于批量标准化处理了这一点，可以放心地使用更大的学习率。</p><h2 id="0b29" class="oa lo it bd lp ob oc dn lt od oe dp lx la of og lz le oh oi mb li oj ok md iz bi translated">b.减少过度拟合</h2><p id="dce7" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la nh lc ld le ni lg lh li nj lk ll lm im bi translated">批量归一化具有正则化效果，因为它向每一层的输入添加了噪声。这阻止了过度拟合，因为模型不再单独为给定的训练示例产生确定性的值。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="5d88" class="ln lo it bd lp lq nc ls lt lu nd lw lx ki ne kj lz kl nf km mb ko ng kp md me bi translated">结论</h1><p id="d25d" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la nh lc ld le ni lg lh li nj lk ll lm im bi translated">批量标准化的力量已经在机器学习的许多领域反复展示。这是一个简单的解决方案，将产生几乎任何型号的性能显着改善。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="9355" class="ln lo it bd lp lq nc ls lt lu nd lw lx ki ne kj lz kl nf km mb ko ng kp md me bi translated">参考资料:</h1><p id="0f58" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la nh lc ld le ni lg lh li nj lk ll lm im bi translated">[1] <a class="ae ou" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">批量归一化:通过减少内部协变量偏移加速深度网络训练</a></p></div></div>    
</body>
</html>