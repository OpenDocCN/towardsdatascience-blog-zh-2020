<html>
<head>
<title>PEX — The secret sauce for the perfect PySpark deployment of AWS EMR workloads</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PEX——AWS EMR 工作负载完美 PySpark 部署的秘方</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pex-the-secret-sauce-for-the-perfect-pyspark-deployment-of-aws-emr-workloads-9aef0d8fa3a5?source=collection_archive---------9-----------------------#2020-04-23">https://towardsdatascience.com/pex-the-secret-sauce-for-the-perfect-pyspark-deployment-of-aws-emr-workloads-9aef0d8fa3a5?source=collection_archive---------9-----------------------#2020-04-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2955" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用 PEX 加速 PySpark 应用程序在临时 AWS EMR 集群上的部署</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9c55574808a1b484b33874f526271d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FMUAYUwXD2fxV7Mbb_KGmA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[OC]</p></figure><p id="ee91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在大数据和数据科学领域，Spark 已经成为除深度学习之外的几乎所有事物的黄金标准:</p><ul class=""><li id="e5c4" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">用于数据湖的 ELT 取代了更传统的 ETL 范例</li><li id="1f09" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">用于高级分析的 Spark SQL</li><li id="2d0f" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">一个用于数据科学的分布式批量计算框架，包括 Spark ML、GraphX、GeoSpark 等。</li><li id="5bf4" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">具有小批量流的近实时应用</li></ul><p id="1d31" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然 Scala Spark API 在数据湖 ELT 工作负载和数据工程中发挥着重要作用，但数据科学和高级分析领域几乎完全是用 Python 编写的 PySpark 工作负载。大多数 Spark 工作负载都是在 AWS EMR 集群上执行的。一般的数据驱动型公司在数量同样惊人的短命 EMR 集群上轻松运行数百个每日 Spark 作业。</p><p id="a73b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">AWS Spot 实例具有吸引力的价格点已经建立了在非常短暂的 EMR 集群上运行 single Spark 作业的范例。EMR 提供短暂的计算资源，而 S3 为数据提供永久存储。使用由 100% Spot 实例组成的短暂 EMR 集群将这种模式推向极端是非常常见的，<strong class="la iu">神风特攻队风格</strong>。</p><p id="e9b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种短暂的 PySpark 集群模式只有<strong class="la iu">一大痛点</strong>:用 Python 包<strong class="la iu">引导 EMR 集群</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/89e7cf6786fb843c443dcf1d53861f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lwj51jA4VsGJlVsVSJyKuQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">https://www.flickr.com/photos/janpersiel/27706588173<a class="ae mj" href="https://www.flickr.com/photos/janpersiel/27706588173" rel="noopener ugc nofollow" target="_blank">(ɔ)</a></p></figure><h1 id="b597" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">PySpark 在电子病历上的应用:坏与丑</h1><p id="b15b" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在 EMR 上运行 PySpark 应用程序非常复杂。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/43a7e02f90392382594b2ae9fcc74d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MWFRoXcj_CmGFZeW-uwBJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[OC]</p></figure><p id="2ab1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于短暂的 EMR，我们必须在每次运行 PySpark 应用程序之前，根据我们的应用程序需求引导我们的集群。这应该是我们的 PySpark 应用程序包的一个简单的 pip 安装。但是现实远非如此简单。</p><ol class=""><li id="01f0" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt ni ma mb mc bi translated">我们需要在 EMR 上安装和更新软件包，因为许多版本后面缺少默认安装。</li><li id="ccbb" class="lu lv it la b lb md le me lh mf ll mg lp mh lt ni ma mb mc bi translated">我们应该为我们的应用程序创建一个虚拟的 Python 环境，因为全局 Python 包会以奇怪的方式干扰。我们都去过那里。</li><li id="31ad" class="lu lv it la b lb md le me lh mf ll mg lp mh lt ni ma mb mc bi translated">我们需要用 pip 安装我们的 PySpark 应用程序和需求</li><li id="921c" class="lu lv it la b lb md le me lh mf ll mg lp mh lt ni ma mb mc bi translated">我们需要从 S3 复制资产，例如 spark-submit 的 main.py 脚本</li><li id="7f16" class="lu lv it la b lb md le me lh mf ll mg lp mh lt ni ma mb mc bi translated">最后，我们使用我们的<strong class="la iu">特定于应用程序的 spark 配置</strong>调用 spark-submit 来最终运行我们的工作负载</li></ol><p id="9127" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">这种 EMR 方法有一些难点:</strong></p><ol class=""><li id="1985" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt ni ma mb mc bi translated">虽然我们实际上并不运行任何真正的工作负载，但是引导过程是为集群时间付费的。这对大型集群来说尤其重要！</li><li id="f792" class="lu lv it la b lb md le me lh mf ll mg lp mh lt ni ma mb mc bi translated">我们必须维护和下载不属于 Python 应用程序包的资产，例如 spark-submit main.py 脚本</li><li id="e7a5" class="lu lv it la b lb md le me lh mf ll mg lp mh lt ni ma mb mc bi translated">我们必须独立于 Python 应用程序包为我们的作业维护特定于应用程序的 Spark 配置，我们将 Python 应用程序包作为参数传递给 spark-submit 脚本。这些通常分散在 Jenkins 管道或气流 Dag 中，增加了维护 PySpark 应用程序的不必要的复杂性。</li><li id="eed5" class="lu lv it la b lb md le me lh mf ll mg lp mh lt ni ma mb mc bi translated">打包和安装具有复杂依赖关系的 Python 应用程序会直接将您引向臭名昭著的依赖地狱。</li></ol><p id="d699" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">考虑到 Python 的流行程度，Python 工具链远非理想，安装带有复杂依赖链和 pip 的包通常会导致众所周知的依赖地狱。不幸的是，pip 缺少一个强大的依赖解析器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/3c271e1ef1c18804146b2e892b627b97.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/0*z9zi9dmnuBvJ2Fku"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CC BY-NC 2.5，<a class="ae mj" href="https://xkcd.com/1987/" rel="noopener ugc nofollow" target="_blank">https://xkcd.com/1987/</a></p></figure><p id="cc42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在商业数据科学团队中，通常还有第二层复杂性，即专有代码的<strong class="la iu">私有包索引</strong>。我打赌你的私人包裹索引很慢。因此，您的 PySpark 应用程序的常见 EMR 引导脚本可能如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/fa8375eaa1591286bfb81b6fe9a635b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ERqCbLgE175Hxt-3eTcZWw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[OC]</p></figure><p id="0fb7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果运行 PySpark 应用程序像调用可执行文件一样简单<strong class="la iu">不是很好吗？无 pip 安装要求。没有主。py。没有火花-提交与火花内存配置搞乱詹金斯或气流。</strong></p><h1 id="21e5" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">解决方案:PEX</h1><p id="2d7b" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">PEX (Python 可执行文件)是一种文件格式和相关工具，用于创建类似于 virtualenv 的通用 Python 环境虚拟化解决方案。PEX 最初于 2011 年在 Twitter 上开发，用于将 Python 应用程序部署到生产中。PEX 文件是<strong class="la iu">自包含的</strong> <strong class="la iu">可执行的</strong> Python 虚拟环境。重点在于自包含和可执行性，这使得 PEX 文件非常适合应用程序部署到生产环境中。使用 PEX 文件，部署应用程序所需的唯一步骤是<strong class="la iu">复制文件</strong>。不需要安装 pip，也不需要修改路径。</p><p id="5106" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 PEX 的帮助下，在 EMR 上运行 PySpark 应用程序不再需要任何引导！</p><ul class=""><li id="e70b" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">PEX 大大简化了 PySpark 应用程序的运行</li><li id="03c3" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">通过利用集群更早地运行我们的实际应用工作负载，而无需任何集群引导，从而节省资金。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/0760d75fec1bfbbeb8ac0abd779105a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*56kmxAsq7ZnmNLDUaM4xaQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[OC]</p></figure><p id="5908" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要创建 pex 归档，您可以使用 PEX 实用程序。您可以简单地安装它</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="32ad" class="nr ml it nn b gy ns nt l nu nv">pip install pex</span></pre><p id="b733" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当您在新的 Python 虚拟环境中执行此操作时，您可以使用 pex 来打包自身。以下命令使用名为“pex”的控制台脚本创建包含 pex 和请求的 pex 文件。将创建的可执行文件保存到~/bin/pex，您可以在任何 virtualenv 内部或外部使用 pex，就像您的路径上的任何其他可执行文件一样。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="4a5d" class="nr ml it nn b gy ns nt l nu nv">pex pex requests -c pex -o ~/bin/pex</span></pre><p id="6576" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">PEX 有一个复杂之处:可执行文件包含一个自包含的 Python 虚拟环境，但不是 Python 解释器本身:PEX 可执行文件是依赖于平台的。目前，EMR 在 Linux 上运行 Python 3.6.10，你可以在 Mac 上开发。因此，通常最好使用 Docker 来创建可重复的结果。</p><p id="0cdc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">构建与 EMR 兼容的 docker 映像，在 docker 容器中创建您的 PEX 归档:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="d339" class="nr ml it nn b gy ns nt l nu nv">FROM python:3.6-slim-buster</span><span id="2aa7" class="nr ml it nn b gy nw nt l nu nv">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \<br/>  git \<br/> &amp;&amp; rm -rf /var/lib/apt/lists/*</span><span id="ea09" class="nr ml it nn b gy nw nt l nu nv">RUN pip3 install pex</span><span id="7b5b" class="nr ml it nn b gy nw nt l nu nv">RUN mkdir /app<br/>WORKDIR /app</span><span id="4e0c" class="nr ml it nn b gy nw nt l nu nv">ENV PATH=/root/.local/bin:$PATH</span></pre><p id="110f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您的组织使用私有包索引，例如 Artifactory，那么 PEX 显示了另一个弱点:在编写时，它没有通过 CLI 公开请求库的参数，这意味着当直接使用 PEX 解决包依赖关系时，我们不能为 pip 设置自定义网络超时。解决方法是使用舵手室。以下脚本可用于使用轮罩和带有自定义超时的私有包索引来构建 pex 归档:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="ccb9" class="nr ml it nn b gy ns nt l nu nv">#!/usr/bin/env bash</span><span id="6d43" class="nr ml it nn b gy nw nt l nu nv">pip3 download -r requirements.txt \<br/>    --dest ./build/wheelhouse \<br/>    --extra-index-url <a class="ae mj" href="https://private.registry.dev/pypi/simple" rel="noopener ugc nofollow" target="_blank">https://private.registry.dev/pypi/simple</a> \<br/>    --trusted-host private.registry.dev \<br/>    --timeout 120</span><span id="dd07" class="nr ml it nn b gy nw nt l nu nv">pex . -r requirements.txt \<br/>    -o ./dist/my_application.pex \<br/>    --platform manylinux2014-x86_64-cp-36-m \<br/>    --no-index \<br/>    -f ./build/wheelhouse</span></pre><h1 id="6e8e" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">打包 PySpark 应用程序</h1><p id="89d8" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">我们的目标是打包一个完全自包含的 PySpark 应用程序，并在不需要 spark-submit 的情况下运行它。因此，我们的 Python 主函数必须创建一个 SparkSession:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="f93d" class="nr ml it nn b gy ns nt l nu nv">if __name__ == "__main__":</span><span id="21e7" class="nr ml it nn b gy nw nt l nu nv">pex_file = os.path.basename([path for path in sys.path if path.endswith(".pex")][0])<br/>    os.environ["PYSPARK_PYTHON"] = "./" + pex_file</span><span id="b490" class="nr ml it nn b gy nw nt l nu nv">spark = (<br/>        SparkSession.builder<br/>        .master("yarn")<br/>        .appName("my_spark_application")<br/>        .config("spark.submit.deployMode", "client")<br/>        .config("spark.yarn.dist.files", pex_file)<br/>        .config("spark.executorEnv.PEX_ROOT", "./.pex")<br/>        .config("spark.sql.shuffle.partitions", 4000)<br/>        .config("spark.executor.memory", "1G")<br/>        .enableHiveSupport()<br/>        .getOrCreate()<br/>    )</span></pre><p id="48b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以将任何选项从您通常的 spark-submit 传递给 SparkSession builder。</p><p id="c608" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这允许您在 PEX 可执行文件中执行 PySpark 应用程序，例如:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="c2f0" class="nr ml it nn b gy ns nt l nu nv">./my_application.pex -m my_application.main</span></pre><h1 id="df50" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">执行 PEX 作为电子病历的一个步骤</h1><p id="24a8" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">最后一步是作为 EMR 步骤执行我们的 pex 应用程序。我们将使用脚本运行器和一个通用的瘦包装器作为一个步骤来执行 PEX。</p><p id="31c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">脚本运行程序调用我们的瘦包装器，该包装器从 S3 提取一个 PEX 文件，并使用我们可能需要的所有环境变量和命令行参数来执行它。下面的脚本是一个你可以使用的名为<strong class="la iu"> pex-executor.sh </strong>的瘦包装器。只需将它放在 S3 上，即可供您的 EMR 集群使用:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="1ae6" class="nr ml it nn b gy ns nt l nu nv">#!/bin/bash<br/># Author: Jan Teichmann<br/># Version: 2020-02-10<br/># Wrapper to execute a PEX archive via an EMR Step<br/># Step type: Custom JAR<br/># JAR location: s3://eu-west-1.elasticmapreduce/libs/script-runner/script-runner.jar<br/># Arguments:<br/>#      s3://.../pex-executer.sh <br/>#      s3://.../some-etl-job.pex <br/>#      HADOOP_HOME=/usr/lib/hadoop <br/>#      SPARK_HOME=/usr/lib/spark <br/>#      ./some-etl-job.pex -m package.module -fromdate=2020-04-20 -todate=2020-04-22</span><span id="ded7" class="nr ml it nn b gy nw nt l nu nv">aws s3 cp $1 .<br/>chmod +x $(basename -- $1);</span><span id="dced" class="nr ml it nn b gy nw nt l nu nv">shift;<br/>eval "$@"</span></pre><p id="be7d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，您可以提交 EMR 步骤，例如通过 Airflow 的 EmrAddStepsOperator:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="83de" class="nr ml it nn b gy ns nt l nu nv">EmrAddStepsOperator(<br/>    task_id="my_application",<br/>    job_flow_id="my_emr_cluster_id",<br/>    steps=[<br/>        {<br/>            "ActionOnFailure": "CONTINUE",<br/>            "Name": "Run my_application Step",<br/>            "HadoopJarStep": {<br/>                "Args": [<br/>                    "s3://.../pex-executer.sh",<br/>                    "s3://.../my_application.pex",<br/>                    "HADOOP_HOME=/usr/lib/hadoop",<br/>                    "SPARK_HOME=/usr/lib/spark",<br/>                    "./my_application.pex",<br/>                    "-m",<br/>                    "my_pyspark_application.main",<br/>                    "-parameter1",<br/>                    "value1" <br/>                ],<br/>                "Jar": "s3://eu-west-1.elasticmapreduce/libs/script-runner/script-runner.jar",<br/>            },<br/>        }<br/>    ],<br/>    aws_conn_id="aws_default",<br/>    dag=dag,<br/>)</span></pre><p id="b13d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如您在上面看到的，我们首先传递两条 s3 路径。第一个是我们的瘦包装器 pex-executor.sh，它将由 AWS script-runner.jar 执行。pex-executor 脚本将依次下载应用程序 pex 可执行文件。</p><p id="5998" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还定义了 EMR 需要的两个环境变量 HADOOP_HOME 和 SPARK_HOME。您可以根据需要添加任何额外的环境变量。</p><p id="c0b1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们传递可执行文件的名称，并传递 Python 应用程序的任何 CLI 参数。</p><p id="b077" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">或者通过 EMR 控制台:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/328b37b317f870b8ffbed6677051b673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0deI4x4ZE7_7onF0"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[OC]</p></figure><h1 id="55cf" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">摘要</h1><p id="a152" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">PEX 允许我们将 PySpark 应用程序作为完全自包含的可执行文件运行，就像我们使用 Scala API 时，带有 uber-JAR 或 fat-JAR 的 Spark 应用程序允许的那样。</p><p id="6e8d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这极大地<strong class="la iu">简化了</strong>使用 PySpark 和<strong class="la iu">的短暂 EMR 集群，节省了时间</strong>和<strong class="la iu">节省了资金</strong>，因为我们不必引导集群。</p><p id="422d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将 pex 可执行文件的创建打包到一个 Jenkins 管道中，您就拥有了一个强大的 DevOps 模式来构建包并将它们上传到 S3 进行部署。</p><p id="65dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，您可以安排您的 PySpark 应用程序，例如使用气流。因为您的 PEX 应用程序是完全自包含的，所以您将能够在 Airflow 中创建非常通用的 Dag，而不会导致任何应用程序逻辑和配置分散在多个平台和存储库中。</p><blockquote class="ny"><p id="9c49" class="nz oa it bd ob oc od oe of og oh lt dk translated">简单是最高级的复杂</p></blockquote></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/b40fa03f9762d1ec3c427365a4c45786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7HvEyV_ETBrk5E7jVL971A.png"/></div></div></figure><p id="43ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Jan 是公司数据转型方面的成功思想领袖和顾问，拥有将数据科学大规模应用于商业生产的记录。他最近被 dataIQ 评为英国 100 位最具影响力的数据和分析从业者之一。</p><p id="4db2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">在领英上连接:</strong><a class="ae mj" href="https://www.linkedin.com/in/janteichmann/" rel="noopener ugc nofollow" target="_blank">T5】https://www.linkedin.com/in/janteichmann/</a></p><p id="d479" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">阅读其他文章:</strong><a class="ae mj" href="https://medium.com/@jan.teichmann" rel="noopener"><strong class="la iu">https://medium.com/@jan.teichmann</strong></a></p></div></div>    
</body>
</html>