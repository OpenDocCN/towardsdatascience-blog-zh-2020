# PyTorch 中多任务学习的不平衡数据加载

> 原文：<https://towardsdatascience.com/unbalanced-data-loading-for-multi-task-learning-in-pytorch-e030ad5033b?source=collection_archive---------11----------------------->

## 在多个不平衡数据集上训练多任务模型的 PyTorch 实用指南

![](img/cacf674a42ca16c5918d914c272a3e20.png)

[由 Kjpargeter / Freepik](http://www.freepik.com) 设计

解决[多任务学习](https://ruder.io/multi-task/) (MTL)问题需要独特的培训设置，主要是在数据处理、模型架构和绩效评估指标方面。

在这篇文章中，我将回顾数据处理部分。具体来说，如何在多个数据集上训练多任务学习模型，以及如何处理高度不平衡数据集的任务。

我将分三步描述我的建议:

1.  将两个(或更多)数据集合并成一个 PyTorch *数据集。*该数据集将成为 PyTorch *数据加载器的输入。*
2.  修改批制备过程，以在每批中产生一个任务，或者在每批中混合来自两个任务的样品。
3.  通过使用*批量取样器*作为*数据加载器的一部分来处理高度不平衡的数据集。*

我只审查了*数据集*和*数据加载器*的相关代码，忽略了其他重要模块，如模型、优化器和指标定义。

为了简单起见，我使用一个通用的两个数据集的例子。然而，数据集的数量和数据的类型不应该影响主设置。我们甚至可以使用同一个数据集的几个实例，以防同一组样本有多组标签。例如，具有对象类别和空间位置的图像数据集，或者具有每个图像的面部情感和年龄标签的面部情感数据集。

一个 [PyTorch *数据集*](https://pytorch.org/docs/stable/data.html#dataset-types) 类需要实现`__getitem__()`函数。该函数处理给定索引的样本获取和准备。当使用两个数据集时，就可能有两种不同的方法来创建样本。因此，我们甚至可以使用单个数据集，获得具有不同标签的样本，并改变样本处理方案(输出样本应该具有相同的形状，因为我们将它们作为批量张量进行堆叠)。

首先，让我们定义两个数据集:

我们定义两个(二进制)数据集，一个有 10 个 1 的样本(平均分布)，第二个有 55 个样本，50 个数字 5 的样本和 5 个数字 5 的样本。这些数据集仅用于说明。在真实的数据集中，你应该既有样本又有标签，你可能会从数据库中读取数据或者从数据文件夹中解析数据，但是这些简单的数据集足以理解主要概念。

接下来，我们需要定义一个 [*数据加载器*](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) 。我们用我们的 *concat_dataset* 提供它，并设置加载器参数，比如批量大小，以及是否重排样本。

这一部分的输出如下所示:

```
tensor([ 5.,  5.,  5.,  5., -5.,  5., -5.,  5.])
tensor([5., 5., 5., 5., 5., 5., 5., 5.])
tensor([-1., -5.,  5.,  1.,  5., -1.,  5., -1.])
tensor([5., 5., 5., 5., 5., 5., 5., 5.])
tensor([ 5.,  5.,  5.,  5., -5.,  1.,  5.,  5.])
tensor([ 5.,  5.,  5.,  1.,  5.,  5.,  5., -1.])
tensor([ 5.,  5.,  5.,  5., -1.,  5.,  1.,  5.])
tensor([ 5., -5.,  1.,  5.,  5.,  5.,  5.,  5.])
tensor([5.])
```

每一批都是来自我们的 *concat_dataset* 的 8 个样本的张量。顺序是随机设置的，样本是从样本池中选择的。

直到现在，一切都相对简单。这些数据集被组合成一个数据集，并从两个原始数据集中随机选取样本来构建小批量。现在让我们试着控制和操作每批样品。我们希望在每个小批量中只从一个数据集获取样本，每隔一批在它们之间切换。

这是一个 *BatchSchedulerSampler* 类的定义，它创建了一个新的样本迭代器。首先，通过为每个内部数据集创建一个 *RandomSampler* 。第二种方法是从每个内部数据集迭代器中抽取样本(实际上是样本索引)。因此，构建一个新的样本索引列表。使用 8 的批量意味着我们需要从每个数据集获取 8 个样本。

现在让我们使用一个新的*数据加载器*运行并打印样本，它将我们的 *BatchSchedulerSampler* 作为输入采样器(使用采样器时，shuffle 不能设置为 *True* )。

输出现在看起来像这样:

```
tensor([-1., -1.,  1.,  1., -1.,  1.,  1., -1.])
tensor([5., 5., 5., 5., 5., 5., 5., 5.])
tensor([ 1., -1., -1., -1.,  1.,  1., -1.,  1.])
tensor([5., 5., 5., 5., 5., 5., 5., 5.])
tensor([-1., -1.,  1.,  1.,  1., -1.,  1., -1.])
tensor([ 5.,  5., -5.,  5.,  5., -5.,  5.,  5.])
tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  1.])
tensor([5., 5., 5., 5., 5., 5., 5., 5.])
tensor([-1., -1., -1., -1.,  1.,  1.,  1., -1.])
tensor([ 5., -5.,  5.,  5.,  5.,  5., -5.,  5.])
tensor([-1.,  1., -1.,  1., -1.,  1.,  1., -1.])
tensor([ 5.,  5.,  5.,  5.,  5., -5.,  5.,  5.])
tensor([ 1., -1., -1.,  1.,  1.,  1.,  1., -1.])
tensor([5., 5., 5., 5., 5., 5., 5.])
```

万岁！！！
对于每个小批量，我们现在只获得一个数据集样本。为了对更重要的任务进行降采样或升采样，我们可以尝试这种调度方式。

我们批次中剩下的问题来自第二个高度不平衡的数据集。这是 MTL 经常出现的情况，有一个主任务和一些其他的卫星子任务。一起训练主任务和子任务可能会提高性能，并有助于整体模型的[一般化](https://ruder.io/multi-task/)。问题是子任务的样本通常非常稀疏，只有几个正(或负)样本。让我们使用我们以前的逻辑，但是也强制在每个任务中关于样本分布的平衡批处理。

为了处理不平衡的问题，我们需要用一个*ImbalancedDatasetSampler*替换 *BatchSchedulerSampler* 类中的随机采样器(我使用了这个[资源库](https://github.com/ufoym/imbalanced-dataset-sampler)中的一个很好的实现)。这个类处理数据集的平衡。我们也可以混合使用 *RandomSampler* 用于一些任务，使用*不平衡数据采样器*用于其他任务。

我们首先创建*ExampleImbalancedDatasetSampler*，它继承了 *ImbalancedDatasetSampler* ，只修改了 *_get_label* 函数来适应我们的用例。

接下来，我们使用*BalancedBatchSchedulerSampler*，它类似于前面的 *BatchSchedulerSampler* 类，只是将 *RandomSampler* 用于不平衡任务的用法替换为*examplembanceddatasetsampler*。

让我们运行新的*数据加载器*:

输出如下所示:

```
tensor([-1.,  1.,  1., -1., -1., -1.,  1., -1.])
tensor([ 5.,  5.,  5.,  5., -5., -5., -5., -5.])
tensor([ 1.,  1.,  1., -1.,  1., -1.,  1.,  1.])
tensor([ 5., -5.,  5., -5., -5., -5.,  5.,  5.])
tensor([-1., -1.,  1., -1., -1., -1., -1.,  1.])
tensor([-5.,  5.,  5.,  5.,  5., -5.,  5., -5.])
tensor([-1., -1.,  1.,  1.,  1.,  1., -1., -1.])
tensor([-5.,  5.,  5.,  5.,  5., -5.,  5.,  5.])
tensor([ 1., -1.,  1.,  1.,  1., -1.,  1., -1.])
tensor([ 5.,  5.,  5., -5.,  5., -5.,  5.,  5.])
tensor([-1., -1., -1., -1.,  1.,  1.,  1.,  1.])
tensor([-5.,  5.,  5.,  5.,  5.,  5., -5.,  5.])
tensor([-1.,  1., -1.,  1.,  1.,  1.,  1.,  1.])
tensor([-5., -5.,  5.,  5., -5., -5.,  5.])
```

不平衡任务的小批量现在更加平衡了。

这个设置还有很大的发挥空间。我们可以以平衡的方式组合任务，通过将 *samples_to_grab* 设置为 4，这是批量大小的一半，我们可以获得一个混合的小批量，每个任务中有 4 个样本。为了对更重要的任务产生 1:2 的比率，我们可以为第一个任务设置 *samples_to_grab=2* ，为第二个任务设置 *samples_to_grab=6* 。

就是这样。完整的代码可以从我的[库](https://github.com/bomri/code-for-posts/tree/master/mtl-data-loading)下载。