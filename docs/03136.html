<html>
<head>
<title>What If Only Batch Normalization Layers Were Trained?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如果只训练了批量归一化图层呢？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c?source=collection_archive---------25-----------------------#2020-03-25">https://towardsdatascience.com/what-if-only-batch-normalization-layers-were-trained-1d13e2d3f35c?source=collection_archive---------25-----------------------#2020-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="60dc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">你可能会感到惊讶，这很有效。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b8faa0237da141536746250eb2d92661.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Yw_T8yMq6cNyULzO"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卡西·乔希在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e15e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拿我来说，我绝不会把我的钱押在这上面。</p><p id="b451" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近，我阅读了由 Jonathan Frankle、David J. Schwab 和 Ari S. Morcos 撰写的论文“<em class="lv">训练 BatchNorm 和 Only BatchNorm:关于 CNN 中随机特征的表达能力”</em>，该论文最近在<a class="ae ky" href="https://arxiv.org/abs/2003.00152" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> arXiv </em>平台</a>上发布。这个想法立刻引起了我的注意。到目前为止，我从未将<a class="ae ky" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">批量标准化</a> (BN)层视为学习过程本身的一部分，只是作为深度网络收敛和稳定的辅助手段。几个实验之后，我大错特错了。在下文中，我将介绍我在复制这篇论文的结果时所采取的措施，以及我从中学到的东西。</p><p id="321b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更详细地说，我使用 Tensorflow 2 Keras API 成功地再现了论文的主要实验，得出了类似的结论。也就是说，ResNets 可以通过仅训练批量归一化图层的伽马(γ)和贝塔(β)参数，在 CIFAR-10 数据集中获得令人满意的结果。从数字上看，我使用 ResNet-50、101 和 152 架构获得了 45%、52%和 50%的顶级精度，这远远算不上很好，但也远非随机。</p><p id="1e53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下文中，我概述了批处理规范化的概念、其好处背后的常见解释、我使用的代码以及获得的结果。最后，我对结果及其相关性进行了讨论。</p><p id="402b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近，这项工作被刊登在了 deeplearning.ai 的 Batch 新闻简报上。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="1b0f" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">批量标准化</h1><p id="8953" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">简而言之，批量归一化层估计其输入的均值(μ)和方差(σ)并产生标准化输出，<em class="lv">即，</em>输出具有零均值和单位方差。在实践中，这种技术有意义地提高了深度网络的收敛性和稳定性。此外，它使用两个参数(γ和β)来转换和调整输出。</p><p id="2bfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为该层的<em class="lv"> x </em>输入和<em class="lv"> z </em>输出，<em class="lv"> z </em>由以下公式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/efe2c58e620425716b0b782a6ff1054c.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/0*7zwbCVZ4ej_ZqpnJ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:批量标准化表达式</p></figure><p id="e626" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然μ和σ参数是根据输入数据估计的，但γ和β是可训练的。因此，反向传播算法可以利用它们来优化网络。</p><p id="95c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，这种操作可以显著提高网络训练的速度，并改善其对保留数据的性能。而且，它没有禁忌症。出于这个原因，大多数模型大量使用它，经常在所有 Conv-ReLU 操作之间，形成“Conv-BN-ReLU”三重奏(及其变体)。然而，尽管这是最常见的层之一，其好处背后的原因在文献中有很多争论。这里总结了三种主要给出的解释。</p><p id="36ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">编辑:</em>在某些情况下，批量定额确实有禁忌症。最近的工作提高了人们的认识，尽管提高了收敛性，批量规范严重影响训练速度。特别是，当跨多个 GPU 和设备进行训练时，在不影响并行性能的情况下计算批量统计数据是一个巨大的挑战。出于这个原因，几位作者提出了替代方案，如<a class="ae ky" href="https://arxiv.org/abs/1706.02515" rel="noopener ugc nofollow" target="_blank">自归一化网络</a>甚至<a class="ae ky" href="https://arxiv.org/abs/2102.06171" rel="noopener ugc nofollow" target="_blank">无归一化网络</a>。这些工作旨在保留批处理规范的好处，同时避免训练期间数据点之间的任何依赖。</p><p id="7293" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">内部共变移位:</strong>简单来说，如果输出具有零均值和单位方差，则下一层基于稳定输入进行训练。换句话说，它防止输出变化太大。虽然这是<a class="ae ky" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">最初的解释</a>，后来的著作发现<a class="ae ky" href="https://arxiv.org/abs/1805.11604" rel="noopener ugc nofollow" target="_blank">与证据</a>相矛盾，否定了这个假设。简而言之，如果你训练 VGG 网络(1)没有 BN，(2)有 BN 和(3)有 BN 加上人工协方差偏移，方法(2)和(3)仍然优于(1)，尽管人工协方差偏移被添加到网络中。</p><p id="c12d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">输出平滑:</strong> BN 也被认为是<a class="ae ky" href="https://arxiv.org/pdf/1805.11604.pdf" rel="noopener ugc nofollow" target="_blank">平滑优化景观</a>，减少损失函数的变化并限制其梯度。更平滑的目标训练更可预测，更不容易出现数字问题。</p><p id="bacf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">长度方向解耦:</strong>一些作者认为 BN 是优化问题的<a class="ae ky" href="https://arxiv.org/abs/1805.10694" rel="noopener ugc nofollow" target="_blank">改进公式</a>，因此可以扩展到更传统的优化设置。更详细地说，BN 框架允许独立地优化参数的长度和方向，从而提高收敛性。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="1afb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，这三种解释都集中在批处理规范化的规范化方面。相比之下，我们将着眼于 BN 的移动和缩放点，由γ和β参数实现。</p><h1 id="8e33" class="md me it bd mf mg nb mi mj mk nc mm mn jz nd ka mp kc ne kd mr kf nf kg mt mu bi translated">复制纸张</h1><p id="6653" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">如果一个想法是好的，它应该对实现和超参数的选择有弹性。在我的代码中，我使用 Tensorflow 2 和我自己选择的超参数尽可能简单地重新创建了主实验。更详细地说，我测试了以下命题:</p><blockquote class="ng nh ni"><p id="3ba7" class="kz la lv lb b lc ld ju le lf lg jx lh nj lj lk ll nk ln lo lp nl lr ls lt lu im bi translated"><strong class="lb iu"> ResNet </strong>型号在<strong class="lb iu"> CIFAR-10 </strong>上可以达到<strong class="lb iu">体面的结果</strong>除了批次归一化参数外<strong class="lb iu">所有重量都锁定。</strong></p></blockquote><p id="2a94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我将使用 Keras 的 CIFAR-10 和 ResNet 模块以及对 CIFAR-10 数据集的总体建议，即分类交叉熵损失和 Softmax 激活。我的代码下载数据集和随机初始化的 ResNet 模型，冻结不需要的层，并使用 1024 个图像的批量大小训练 50 个时期。您可以检查下面的代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6396" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码中应该注意一些事情:</p><ol class=""><li id="d954" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><a class="ae ky" href="https://keras.io/applications/#resnet" rel="noopener ugc nofollow" target="_blank"> Keras API </a>只有 ResNet-50、101 和 152 型号。为了简单起见，我只用过这些。如果您想深入了解，请参考本指南<a class="ae ky" href="https://keras.io/examples/cifar10_resnet/" rel="noopener ugc nofollow" target="_blank">了解整个 ResNet 架构的定制实现。</a></li><li id="b858" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">ResNet 模型对γ参数使用“一”初始化策略。在我们有限的训练场景中，这过于对称，无法通过梯度下降来训练。相反，如论文中所建议的，使用“he_normal”初始化。为此，我们在训练前手动重新初始化批量标准化权重。</li><li id="ae9e" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">作者使用 128 幅图像的批量大小和动量为 0.9 的 SGD 优化器训练了 160 个时期。学习率最初设置为 0.01，并在时期 80 和 120 预定为 0.001 和 0.0001。对于这样一个幼稚的想法，我发现这太具体了。取而代之的是，我使用了 50 个纪元、1024 的批量、普通的 Adam 和 0.01 的固定学习率。如果这个想法是好的，这应该不是一个问题。</li><li id="1857" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">作者也使用了数据扩充，而我没有。同样，如果这个想法是好的，这些改变都不会是一个大问题。</li></ol><h2 id="63fc" class="oc me it bd mf od oe dn mj of og dp mn li oh oi mp lm oj ok mr lq ol om mt on bi translated">结果</h2><p id="44c0" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">以下是我使用上述代码获得的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/df598f75729cb5bf57ea8a1b248e44fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KndXykkm3l2FJiK46uPEVw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ResNet 模型的训练精度仅训练批量归一化图层</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/e82111cf493464baa9ca7e9ee5e02543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KmeovEPkIHazwxcfGBdN0g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ResNet 模型的验证准确性仅训练批处理规范化图层</p></figure><p id="7737" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从数字上看，这三个模型分别达到了 50%、60%和 62%的训练精度以及 45%、52%和 50%的验证精度。</p><p id="1c2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更好地理解模型的表现，我们应该始终考虑随机猜测的表现。CIFAR-10 数据集有十个类。因此，随机地，我们有 10%的机会是正确的。以上方法比随机猜测要好五倍左右。因此，我们可以认为他们有<em class="lv">不错的</em>表现。</p><p id="1b67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有趣的是，验证准确性在十个时期后才开始增加，这是一个明显的迹象，即对于前十个时期，网络只是尽可能地过度拟合数据。随后，验证性能显著提高。然而，它每五个时期变化很大，这表明该模型不是很稳定。</p><p id="476c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在论文中，图 2 显示他们实现了大约 70%、大约 75%和大约 77%的验证准确性。考虑到作者做了一些调整，使用了定制的训练计划，并采用了数据扩充，这似乎很合理，与我的发现一致，证实了假设。</p><p id="6f8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用一个 866 层的 ResNet，作者达到了大约 85%的准确率，这仅比通过训练整个架构可达到的大约 91%低几个百分点。此外，他们测试了不同的初始化方案、架构，并测试了解冻最后一层和跳过连接，这导致了一些额外的性能增益。</p><p id="c985" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了准确性，作者还研究了γ和β参数的直方图，发现通过将γ设置为接近零的值，网络学会了抑制每个 BN 层中大约三分之一的所有激活。</p><h2 id="8ea1" class="oc me it bd mf od oe dn mj of og dp mn li oh oi mp lm oj ok mr lq ol om mt on bi translated">讨论</h2><p id="d002" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">此时，你可能会问:为什么会这样？首先，很好玩:)第二，BN 层是司空见惯的，但我们对它们的作用还是只有肤浅的了解。我们知道的是它们的好处。第三，这种调查让我们更深入地了解我们的模型是如何运作的。</p><p id="cdcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不相信这本身有实际应用。没有人会冻结他们的层，把这一切都留给 BNs。然而，这可能会激发不同的训练计划。也许像这样训练网络几个时期，然后训练所有权重可能会导致更好的性能。相反，这种技术对于微调预先训练的模型可能是有用的。我也可以看到这个想法被用来削减大型网络的权重。</p><p id="dce4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这项研究最让我困惑的是，我们都忽略了这两个参数。至少我从来不介意这两者。我记得只看到一个关于它的讨论，它认为在 ResNet 块上用“零”初始化γ是好的，这样可以迫使反向传播算法在早期使用更多的跳过连接。</p><p id="c882" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的第二个问题是关于<a class="ae ky" href="https://arxiv.org/abs/1706.02515" rel="noopener ugc nofollow" target="_blank"> SELU </a>和<a class="ae ky" href="https://arxiv.org/abs/1807.10117" rel="noopener ugc nofollow" target="_blank">塞卢</a>激活函数，它们具有自我规范化的特性。这两个函数都使批处理规范化层变得过时，因为它们在训练期间会自然地规范化它们的输出。现在，我问自己，这是否抓住了批处理规范化层的全部。</p><p id="e5e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，假设还是有点原始。它只考虑了 CIFAR-10 数据集和非常深的网络。如果这可以扩展到其他数据集或解决不同的任务，如仅 Batchnorm 的 GAN，则它是开放的。此外，我会很有兴趣看到一篇关于γ和β在完全训练好的网络中的作用的后续文章。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="32c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对这篇文章有任何问题，欢迎评论或<a class="ae ky" href="https://www.linkedin.com/in/ygorreboucas/" rel="noopener ugc nofollow" target="_blank">联系我</a>。如果你是新手，我强烈推荐<a class="ae ky" href="https://ygorserpa.medium.com/membership" rel="noopener">订阅</a>。对于数据和 IT 专业人员来说，中型文章是 StackOverflow 的完美组合，对于新手来说更是如此。注册时请考虑使用<a class="ae ky" href="https://ygorserpa.medium.com/membership" rel="noopener">我的会员链接。</a></p><p id="081e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读:)</p></div></div>    
</body>
</html>