# 理解语音识别系统的音频数据、傅立叶变换、FFT 和频谱图特征

> 原文：<https://towardsdatascience.com/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520?source=collection_archive---------0----------------------->

![](img/41f7dfaea4e44709b2da17f40599509f.png)

## 使用 python 进行音频数据分析(声音分析)的介绍

# 概观

几乎每个组织每天都会产生大量的音频数据。当数据科学家可以轻松访问音频数据以推动人工智能引擎和分析时，音频数据会产生大量的战略见解。已经意识到来自音频数据的信息的力量和重要性的组织正在利用 AI(人工智能)转录的对话来改善他们的员工培训、客户服务和增强整体客户体验。

另一方面，由于以下障碍，有些组织无法更好地利用他们的音频数据— 1。他们没有抓住它。2.数据质量很差。这些障碍会限制他们将要实施的机器学习解决方案(AI 引擎)的潜力。捕捉所有可能的数据并保证数据质量非常重要。

本文提供了从音频数据处理开始的分步指南。虽然这将有助于您开始基本分析，但在进入这一领域之前，对声波和基本信号处理技术有一个基本的了解也不失为一个好主意。你可以[点击这里](https://dropsofai.com/sound-wave-basics-every-data-scientist-must-know-before-starting-analysis-on-audio-data/)查看我关于声波的文章。那篇文章提供了对声波的基本理解，还解释了一些不同的音频编解码器。

在进一步讨论之前，我们先列出本文将要涉及的内容。让我们依次讨论以下每个主题—

1.  ***读取音频文件***
2.  ***【傅立叶变换】***
3.  ***【快速傅立叶变换】***
4.  ***谱图***
5.  ***利用声谱图特征进行语音识别***
6.  ***结论***

# 1.读取音频文件

## 利布罗萨

[*LibROSA*](https://librosa.github.io/librosa/)*是一个 python 库，它几乎拥有你在处理音频数据时需要的所有工具。这个丰富的库提供了大量不同的功能。这里有一个关于这些特征的快速展示—*

1.  **加载并显示音频文件的特征。**
2.  **光谱表示法**
3.  **特征提取和操作**
4.  **时间-频率转换**
5.  **时间分割**
6.  **顺序建模…等等**

*由于这个库很大，我们不打算讨论它的所有特性。为了便于理解，我们将只使用一些共同的特征。*

> *这里是你如何[快速安装这个库](https://librosa.github.io/librosa/install.html)*

```
***pypi**  : pip install librosa
**conda** : conda install -c conda-forge librosa*
```

## *将音频加载到 Python*

*Librosa 支持大量的[音频编解码器](https://en.wikipedia.org/wiki/Audio_codec)。虽然 [**。wav** (无损)](https://en.wikipedia.org/wiki/WAV)在涉及音频数据分析时应用广泛。一旦你成功地在你的 jupyter 笔记本上安装并导入了 libROSA。只需将 file_path 传递给`librosa.load()`函数，就可以读取给定的音频文件。*

*`**librosa.load()**` — >函数返回两个东西— 1。振幅数组。2.采样率。[采样率](https://en.wikipedia.org/wiki/Sampling_(signal_processing))是指录制音频文件时使用的*【采样频率】*。如果您保留参数`**sr = None**` ，它将以其原始采样率加载您的音频文件。(**注:**您可以根据自己的需求指定自定义采样率， *libROSA* 可以为您对信号进行上采样或下采样)。看下面这张图片—*

*![](img/87b3b96df49dd47d70d117ca573310fe.png)*

***sampling_rate = 16k** 表示这段音频是以 16k 的采样频率录制(采样)的。换句话说，在记录这个文件时，我们每秒钟捕获 **16000 个振幅**。因此，如果我们想知道音频的**持续时间**，我们可以简单地将样本数(振幅)除以采样率，如下所示*

*![](img/b5c8ddc1ab2d595ca983e1b96e0d4873.png)*

> *"是的，你可以播放你笔记本里的音频."*

> *IPython 为我们提供了一个通过笔记本播放音频文件的小部件。*

*![](img/1a67b4bc7d2a75c7e199a4331375afa7.png)*

## *可视化音频*

*我们从 librosa 得到了振幅和采样率。我们可以很容易地绘制出这些振幅随时间的变化曲线。 *LibROSA* 提供了如下所示的效用函数 *waveplot()**

*![](img/e40979182add13b99e9f3ffbb51b1dc3.png)**![](img/9785a4a2a8a1221dc48bbcd6013081fd.png)*

*这种可视化被称为给定信号的**时域**表示。这向我们展示了声波的响度(振幅)随时间的变化。这里**振幅= 0** 代表静音。(从声波的定义来看——这个振幅实际上是由于声音引起的大气压力变化而振荡的空气粒子的振幅)。*

*这些振幅**不太能提供信息**，因为它们只谈论录音的响度。为了更好地理解音频信号，有必要将其转换到**频域。信号的频域**表示告诉我们信号中存在哪些不同的频率。**傅立叶变换**是一个数学概念，可以将连续信号从时域转换到频域。让我们了解更多关于傅立叶变换的知识。*

# *2.傅立叶变换*

*音频信号是由多个“单频声波”组成的复杂信号，这些声波在介质中作为扰动(压力变化)一起传播。当声音被记录时，我们只捕捉那些多重波的 ***合成振幅*** 。[傅立叶变换](https://en.wikipedia.org/wiki/Fourier_transform)是一个数学概念，可以 ***将信号分解成其组成频率*** 。傅立叶变换不仅给出信号中存在的频率，还给出信号中存在的每个频率的幅度。*

*![](img/105985c27b51ce0adec44e1f9e289e5c.png)*

***傅立叶逆变换**与傅立叶变换正好相反。它将给定信号的频域表示作为输入，并对原始信号进行数学合成。*

*让我们看看如何利用傅立叶变换将音频信号转换成其频率成分*

# *3.快速傅立叶变换*

*快速傅立叶变换(FFT)是计算给定序列的**离散傅立叶变换**的数学算法。FT(傅立叶变换)和 FFT 的唯一区别是 FT 考虑连续信号，而 FFT 将离散信号作为输入。DFT 将一个序列(离散信号)转换成它的频率成分，就像 FT 对连续信号所做的一样。在我们的例子中，我们有一个从连续音频信号中采样的幅度序列。DFT 或 FFT 算法可以将这种时域离散信号转换到频域。*

*![](img/193b11e1e80219d3e632caf4e0b3ca37.png)*

*FFT 算法概述*

## *简单正弦波理解 FFT*

*为了理解 FFT 的输出，让我们创建一个简单的正弦波。下面这段代码创建了一个正弦波，其 ***采样率= 100，振幅= 1，频率= 3*** 。每隔***1/100 秒(采样率)*** 计算一次振幅值，并存储到一个名为 y1 的列表中。我们将传递这些离散幅度值，使用 FFT 算法计算该信号的 DFT。*

*![](img/f93f4037bcb9207678641706308903ba.png)*

*如果您绘制这些离散值(y1 ),保持 x 轴上的样本数和 y 轴上的振幅值，它会生成一个很好的正弦波图，如下图所示——*

*![](img/c4645ec279848fe7fd923a289a49975c.png)*

*现在我们有一个振幅序列存储在列表 y1 中。我们将把这个序列传递给由 [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.fftpack.fft.html) 实现的 FFT 算法。该算法返回在信号中找到的频率 的 ***复值振幅的列表 yf。该列表的前半部分返回正频率项，另一半返回与正频率项相似的负频率项。您可以选择任何一半，计算绝对值来表示信号中存在的频率。以下函数将样本作为输入，并绘制频率图—****

*![](img/41c5c85778d8ec6aa2bdd317e9dad88c.png)*

*在下图中，我们使用上述 fft_plot 函数绘制了正弦波的频率。可以看到，该图清楚地显示了正弦波中的单一频率值，即 3。此外，它还显示了与该频率相关的幅度，对于正弦波，该频率保持为 1。*

*![](img/2975c72834b6d88c14d38a3093c6986a.png)*

*为了检查一个信号的 FFT 输出，该信号具有超过一个频率的频率，让我们创建另一个正弦波。这次我们将保持 ***采样率= 100，幅度= 2，频率值= 11*** 。下面的代码生成这个信号，并绘制正弦波——*

*![](img/8b4c7c481fba8e37135230a6ed6a50a5.png)*

*生成的正弦波如下图所示。如果我们提高采样率的话，会更平滑。我们保持 ***采样率= 100*** ，因为稍后我们将把这个信号添加到我们的旧正弦波中。*

*![](img/5993de3c7f8005a785f430f645446e4c.png)*

*显然，对于该波，FFT 函数将显示频率为 11 的单个尖峰。但我们想看看，如果将这两个采样速率相同但频率和幅度值不同的信号相加，会发生什么情况。这里序列 ***y3 将代表合成信号。****

*![](img/a24ffb98fc31389417603b5322a3550b.png)*

*如果我们画出信号 y3，它看起来像这样—*

*![](img/296e5e4ebbb011cf37085ddacb73cbbc.png)*

*如果我们将这个序列(y3)传递给我们的 fft_plot 函数。它为我们生成了下面的频率图。它显示了合成信号中两个频率的两个尖峰。所以一个频率的存在不会影响信号中的另一个频率。另外，需要注意的一点是，频率的 ***幅度与我们生成的正弦波的*** 一致。*

*![](img/cc163c9bb3c233d997840047b455f898.png)*

## *对我们的音频信号进行 FFT*

*我们已经了解了 FFT 算法如何给出给定信号的所有频率。让我们尝试将原始音频信号传递到这个函数中。我们使用的是之前加载到 python 中的相同音频剪辑，采样率为 16000。*

*![](img/9dfa83365191075a7a21d520529852df.png)*

*现在，看下面的频率图。这种***【3 秒钟长】*** 的信号是由数千个不同的频率组成的。频率值> 2000 的幅度非常小，因为这些频率中的大部分可能是由噪声引起的。我们绘制的频率范围是 0 到 8kHz，因为我们的信号是以 16k 采样率采样的，根据[奈奎斯特采样定理](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem)，它应该只拥有≤ 8000Hz (16000/2)的频率。*

*强频率范围从 ***0 到 1kHz*** 只是因为这个音频片段是人的语音。我们知道，在典型的人类语言中，这个频率范围占主导地位。*

*![](img/656968792211c51f3e650047a7508695.png)*

> *我们得到了频率，但是时间信息在哪里？*

# *4.光谱图*

## *为什么是声谱图*

*假设您正在处理一项语音识别任务。您有一个音频文件，其中有人在说一个短语(例如:你好吗)。你的识别系统应该能够以同样的顺序预测这三个单词(1。怎么样，2。是，3。你’)。如果您还记得，在之前的练习中，我们将信号分解为频率值，这些值将作为我们识别系统的特征。但是，当我们对信号应用 FFT 时，它只给出了频率值，我们失去了时间信息。现在，如果我们使用这些频率作为特征，我们的系统将无法判断先说的是什么。我们需要找到一种不同的方法来计算我们的系统的特征，这样它就有了频率值以及它们被观察的时间。这里 [**光谱图**](https://en.wikipedia.org/wiki/Spectrogram) 进入画面。*

*给定信号的频率随时间变化的直观表示称为**频谱图**。在谱图表示图中，一个轴代表时间，第二个轴代表频率，颜色代表在特定时间观察到的频率的幅度(振幅)。下面的屏幕截图显示了我们之前讨论的同一音频信号的频谱图。明亮的颜色代表强频率。与之前的 FFT 图类似，较小的频率范围(0–1 khz)很强(明亮)。*

*![](img/dfa44dfd880ec13a50468447379086c9.png)*

## *创建和绘制光谱图*

*想法是将音频信号分成更小的帧(窗口)并计算每个窗口的 DFT(或 FFT)。这样，我们将获得每个窗口的频率，窗口号将代表时间。因为窗口 1 在前，窗口 2 在后…等等。保持这些窗口重叠是一个好习惯，否则我们可能会丢失一些频率。窗口大小取决于您正在解决的问题。*

*对于典型的语音识别任务，推荐使用 20 到 30 毫秒 长的 ***窗口。一个人不可能在这个时间窗口内说出一个以上的音素。所以保持窗口这么小，我们在分类时不会丢失任何音素。框架(窗口)重叠可以根据您的需要从 25%到 75%不等，通常为语音识别保留 50%。****

*在我们的谱图计算中，我们将保持窗口持续时间为 20 毫秒，窗口之间的重叠为 50%。因为我们的信号以 16k 频率采样，所以每个窗口将具有 **(16000 * 20 * 0.001) = 320** 振幅。对于 50%的重叠，我们需要前进 **(320/2) = 160** 个幅度值，以到达下一个窗口。因此，我们的步幅值是 160。*

*看看下图中的声谱图函数。在第 18 行中，我们制作了一个**加权窗口(Hanning )** ，并在将其传递给第 20 行中的 FFT 函数之前，将其与振幅相乘。这里使用加权窗口来处理这个小信号(来自单帧的小信号)的不连续性，然后将其传递给 DFT 算法。要了解为什么需要加权窗口的更多信息— [**点击此处**](https://www.tek.com/blog/window-functions-spectrum-analyzers) 。*

> *一个计算声谱图特征的 python 函数—*

*FFT 算法的输出是复数列表*(size = window _ size/2)***，表示窗口内不同频率的幅度。对于大小为 320 的窗口，我们将得到一个包含 160 个振幅的**频率仓**的列表，在我们的例子中，这些频率仓代表从 **0 Hz — 8kHz** (因为我们的采样率是 16k)的频率。***

***接下来，计算这些复数值幅度的绝对值并归一化。由此产生的 2D 矩阵就是你的声谱图。在这个矩阵中，行和列代表窗口帧号和频率仓，而值代表频率的强度。***

# ***5.利用声谱图特征的语音识别***

***我们现在知道如何生成频谱图，它是一个 2D 矩阵，表示给定信号的频率幅度和时间。现在把这个声谱图想象成一幅图像。您已经将音频文件转换为下图。***

***![](img/096ae287c6a681a00c22d8327f49a48e.png)***

***这就把它归结为一个 ***的图像分类问题*** 。这个图像适时地从左到右代表你所说的短语。或者把这个想象成一个图像，你的短语从左到右写，你需要做的就是识别那些隐藏的英文字符。***

***给定一个平行的英语文本语料库，我们可以训练一个深度学习模型，并建立一个我们自己的语音识别系统。这里有两个众所周知的开源数据集可以试用—***

```
***Popular open source datasets —
1\. [LibriSpeech](http://www.openslr.org/12/) ASR corpus
2\. [Common Voice](https://voice.mozilla.org/en) [Massively-Multilingual Speech Corpus](https://arxiv.org/abs/1912.06670)***
```

***深度学习架构的流行选择可以从以下优秀的研究论文中得到理解—***

1.  ***[wave 2 letter](https://arxiv.org/pdf/1609.03193.pdf)(脸书研究)***
2.  ***[深言](https://arxiv.org/pdf/1412.5567.pdf)、[深言 2](https://arxiv.org/pdf/1512.02595.pdf) 和[深言 3](https://arxiv.org/pdf/1707.07413.pdf) (百度研究)***
3.  ***[听、听、拼](https://arxiv.org/pdf/1508.01211.pdf)(谷歌大脑)***
4.  ***[碧玉](https://arxiv.org/pdf/1904.03288.pdf)(英伟达)***

# ***6.结论***

***这篇文章展示了如何从头开始处理音频数据和一些音频分析技术。此外，它还为构建语音识别系统提供了一个起点。尽管上述研究显示了识别系统非常有前途的结果，但是仍然有许多人不认为语音识别是一个已解决的问题，因为存在以下缺陷***

1.  ***研究人员提出的语音识别模型非常大(复杂)，这使得它们难以训练和部署。***
2.  ***当多人在交谈时，这些系统不能很好地工作。***
3.  ***当音频质量不好时，这些系统不能很好地工作。***
4.  ***他们对说话者的口音非常敏感，因此需要针对每一种不同的口音进行训练。***

***这个研究领域有巨大的机会。可以从数据准备的角度(通过创建更好的特征)进行改进，也可以从模型架构的角度(通过呈现更健壮和可扩展的深度学习架构)进行改进。***

***原刊[此处](https://dropsofai.com/understanding-audio-data-fourier-transform-fft-and-spectrogram-features-for-a-speech-recognition-system/)。***

***引用本文:[谷歌学术链接](https://scholar.google.com/scholar?hl=en&as_sdt=0,24&cluster=9680825736551595294)***

***感谢您的阅读，请告诉我您的意见/反馈。***