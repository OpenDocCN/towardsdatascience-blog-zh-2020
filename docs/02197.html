<html>
<head>
<title>GPU-as-a-Service on KubeFlow: Fast, Scalable and Efficient ML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">KubeFlow上的GPU即服务:快速、可扩展且高效的ML</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gpu-as-a-service-on-kubeflow-fast-scalable-and-efficient-ml-c5783b95d192?source=collection_archive---------17-----------------------#2020-03-02">https://towardsdatascience.com/gpu-as-a-service-on-kubeflow-fast-scalable-and-efficient-ml-c5783b95d192?source=collection_archive---------17-----------------------#2020-03-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="a44b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">机器学习(ML)和深度学习(DL)涉及计算和数据密集型任务。为了最大限度地提高我们的模型准确性，我们希望在更大的数据集上进行训练，评估各种算法，并为每个算法尝试不同的参数(超参数调整)。</p><p id="c7c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随着我们的数据集和模型复杂性的增加，我们等待工作完成所需的时间也在增加，导致我们的时间使用效率低下。我们最终运行更少的迭代和测试，或者在更小的数据集上工作。</p><p id="7921" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">NVIDIA GPUs是加速我们数据科学工作的伟大工具。当涉及深度学习工作负载时，它们是显而易见的选择，并且比CPU提供更好的ROI。随着像<a class="ae ko" href="https://developer.nvidia.com/rapids" rel="noopener ugc nofollow" target="_blank"> RAPIDS </a>这样的新发展，NVIDIA正在高效地处理数据分析和机器学习工作负载(如XGBoost)(阅读我之前帖子中的详细信息:<a class="ae ko" rel="noopener" target="_blank" href="/python-pandas-at-extreme-performance-912912b1047c">Python Pandas in Extreme Performance</a>)。例如，读取Json数据、聚合其指标并写回压缩(Parquet)文件的分析任务在GPU上运行1.4秒，而在CPU上运行43.4秒(快了30倍！).</p><h1 id="8d59" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">挑战:共享GPU</h1><p id="a15d" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">CPU长期以来一直支持虚拟化(虚拟机管理程序)、虚拟内存和IO管理等技术。我们可以在相同的CPU上运行许多不同的工作负载、虚拟机和容器，同时确保最大程度的隔离。我们可以使用各种集群技术在多个CPU和系统之间扩展计算，并使用调度程序将计算动态分配给任务。</p><p id="fe50" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一方面，GPU必须分配给特定的虚拟机或容器。这就导致了效率低下。当GPU密集型任务完成时，GPU将保持空闲。例如，如果我们将笔记本服务器与GPU相关联，每当任务不运行时，我们就会浪费GPU资源——例如当我们编写或调试代码时，或者当我们去吃午饭时。这是一个问题，尤其是当配备GPU的服务器更加昂贵的时候。</p><p id="41a4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">存在一些将单个GPU划分为更小的虚拟GPU的解决方案，但这并没有解决问题，因为我们获得的(大部分是空闲的)片段太小或内存太少，无法运行我们的任务，并且鉴于很难在同一GPU上的任务之间隔离内存，我们可能会遇到许多潜在的故障。</p><h1 id="b567" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">解决方案:动态GPU分配和横向扩展</h1><p id="3809" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">用户正在寻找的解决方案是，可以利用多个GPU来完成一项任务(因此可以更快地完成)，并在任务持续期间分配GPU。这可以通过将容器与编排、集群和共享数据层相结合来实现。</p><p id="a982" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设我们在Jupyter笔记本或IDE中编写了一些代码(例如PyCharm)。我们可以在本地执行它，但当我们需要扩展时，我们打开旋钮，它在分布式集群上的运行速度会快10到100倍。那不是很好吗？我们能实现这样的梦想吗？是的，我们可以，我将在本文的后面向您展示一个演示。</p><p id="d869" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了实现这一点，我们需要能够在运行时将代码和库打包并克隆到多个动态调度的容器中。我们需要所有这些容器共享相同的数据，并实现任务分配/并行机制，如下图所示。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/069cd11ded83513a55c573650af62432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vic9F51ZhM16D_cGFGZ8aA.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">动态GPU/CPU分配(图片由作者提供)</p></figure><p id="2762" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个名为<a class="ae ko" href="https://github.com/mlrun/mlrun" rel="noopener ugc nofollow" target="_blank"> MLRun </a>的新开源ML编排框架允许我们定义“无服务器”ML功能，这些功能由代码、配置、包和基础设施依赖项(如CPU、GPU、内存、存储等)组成。).这些“无服务器”功能可以在我们的笔记本电脑中本地运行，或者在一个或多个容器上运行，这些容器是在任务期间动态创建的(如果需要，也可以驻留更长时间)，客户端/笔记本电脑和容器可以通过低延迟共享数据平面(即虚拟化为一个逻辑系统)共享相同的代码和数据。</p><p id="9d76" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">MLRun建立在<a class="ae ko" href="https://kubernetes.io/" rel="noopener ugc nofollow" target="_blank"> Kubernetes </a>和<a class="ae ko" href="https://www.kubeflow.org/" rel="noopener ugc nofollow" target="_blank"> KubeFlow </a>之上，它使用Kubernetes API来创建和管理资源。它利用<a class="ae ko" href="https://www.kubeflow.org/" rel="noopener ugc nofollow" target="_blank"> KubeFlow </a>定制资源(CRD)无缝运行水平扩展工作负载(如<a class="ae ko" href="https://github.com/nuclio/nuclio" rel="noopener ugc nofollow" target="_blank"> Nuclio functions </a>、Spark、Dask、horo VOD……)，KubeFlow SDK将任务附加到存储卷和机密等资源，以及KubeFlow管道创建多步执行图(DAG)。</p><p id="56b0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过<a class="ae ko" href="https://github.com/mlrun/mlrun" rel="noopener ugc nofollow" target="_blank"> MLRun </a>执行的每个本地或远程任务都由MLRun服务控制器跟踪，所有输入、输出、日志和工件都存储在版本化数据库中，可以使用简单的UI、SDK或REST API调用浏览，即内置的作业和工件管理。MLRun函数可以链接起来形成一个管道，它们支持超参数和AutoML任务，Git集成，项目打包，但这些是不同帖子的主题，<a class="ae ko" href="https://github.com/mlrun/mlrun" rel="noopener ugc nofollow" target="_blank">在这里阅读更多内容</a>。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mi"><img src="../Images/ea8304c23dac35fc4b16935fb00401fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wnHqA38_pb30-g8xTE6Sqw.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">GPU即服务堆栈(图片由作者提供)</p></figure><h1 id="51ee" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">示例:使用Keras和TensorFlow的分布式影像分类</h1><p id="0593" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">在我们的示例中，我们有一个基于著名的猫和狗TensorFlow用例的4步管道:</p><ol class=""><li id="00f1" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">数据摄取功能—从S3自动气象站加载图像档案</li><li id="2372" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">数据标记功能—将图像标记为狗或猫</li><li id="9ce0" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">分布式训练功能—使用TensorFlow/Keras和Horovod来训练我们的模型</li><li id="51e2" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">部署交互式模型服务功能</li></ol><p id="3988" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以看到完整的MLRun项目和笔记本<a class="ae ko" href="https://github.com/mlrun/demos/tree/master/image_classification" rel="noopener ugc nofollow" target="_blank"> </a> <a class="ae ko" href="https://github.com/mlrun/demo-image-classification" rel="noopener ugc nofollow" target="_blank">来源这里</a>，我们将重点放在第3和第4步。要做到这一点，你需要使用Kubernetes，并在其上运行少量开源服务(Jupyter、KubeFlow Pipelines、KubeFlow MpiJob、Nuclio、MLRun)和共享文件系统访问，或者你可以要求使用那些预集成的<a class="ae ko" href="https://www.iguazio.com/" rel="noopener ugc nofollow" target="_blank"> Iguazio </a>云试用版。</p><p id="df9f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的代码可以在本地运行(参见笔记本中的第1步和第2步)，要在具有GPU的集群上运行分布式训练，我们只需将我们的函数定义为MPIJob类(使用MPI和Horovod来分发我们的TensorFlow逻辑)，我们指定代码的链接、容器映像(或者MLRun可以为我们构建映像)、所需的容器数量、GPU数量(每个容器)， 并将其附加到文件挂载(我们应用<a class="ae ko" href="https://www.iguazio.com/" rel="noopener ugc nofollow" target="_blank"> iguazio </a>低延迟v3io结构挂载，但其他K8s共享文件卷驱动程序或对象存储解决方案也可以)。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="aa05" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我们定义了函数对象，我们需要做的就是用一堆参数、输入(数据集或文件)运行它，并指定输出工件的默认位置(例如，训练好的模型文件)。</p><pre class="lt lu lv lw gt mz na nb nc aw nd bi"><span id="9c0d" class="ne kq it na b gy nf ng l nh ni">mprun = trainer.run(name='train', params=params, artifact_path='/User/mlrun/data', inputs=inputs, watch=<strong class="na iu">True</strong>)</span></pre><p id="a997" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意，在本例中，我们不需要移动代码或数据，我们在笔记本和worker容器中使用相同的低延迟共享文件系统挂载这一事实意味着我们可以在Jupyter中修改代码并重新运行我们的作业(所有作业容器都将看到新的Py文件更改)，并且所有作业结果都将在Jupyter文件浏览器或MLRun UI中即时查看。</p><p id="d1c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">除了交互查看作业进度(watch=True)之外，run对象(mprun)还保存运行的所有信息，包括指向输出工件、日志、状态等的指针。我们可以使用MLRun基于web的UI来跟踪我们的工作进度，比较实验结果或访问版本化的工件。</p><p id="7a06" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们用”。save()"为了将我们的函数对象序列化和版本化到数据库中，我们可以稍后在不同的笔记本或CI/CD管道中检索这个函数对象(不需要在笔记本和团队之间复制代码和配置)。</p><p id="6116" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们想要将生成的模型部署为一个交互式的无服务器函数，我们所需要的就是将“model”和“category_map”输出提供给一个“serving”函数，并将其部署到我们的测试集群中。</p><p id="4e37" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">MLRun编排自动扩展<a class="ae ko" href="https://github.com/nuclio/nuclio" rel="noopener ugc nofollow" target="_blank"> Nuclio功能</a>速度超快，可以是有状态的(支持GPU附件、共享文件挂载、状态缓存、流等。)，函数将自动伸缩以适应工作负载，如果请求在几分钟内没有到达，函数将伸缩到零(不消耗任何资源)。在这个例子中，我们使用“nuclio-serving”函数(承载标准<a class="ae ko" href="https://github.com/kubeflow/kfserving" rel="noopener ugc nofollow" target="_blank"> KFServing </a>模型类的nuclio函数)，正如您在下面看到的，只需要一个命令(deploy)就可以使它作为一个活动的无服务器函数运行。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nj"><img src="../Images/81882c002a6b4c5689306a3ce0c2c9ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vao52YzSp7-yomIbmjYDsA.png"/></div></div></figure><p id="516a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们有了一个正在运行的推理函数，我们可以使用简单的HTTP请求测试端点，在有效负载中有一个url，甚至是一个二进制图像。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nk"><img src="../Images/0a72b02ac5a7976c05524ae126dd912a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lSIOZH-npV47YdC3STnrjA.png"/></div></div></figure><h1 id="3a34" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">使用KubeFlow管道的端到端工作流程</h1><p id="0724" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">既然我们已经手动测试了管道的每一步，我们可能希望自动化这个过程，并可能按照给定的时间表运行它，或者由一个事件触发(例如Git推送)。下一步是定义一个<a class="ae ko" href="https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/" rel="noopener ugc nofollow" target="_blank"> KubeFlow管道</a>图(DAG ),它将4个步骤链接成一个序列并运行该图。</p><p id="4e08" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://github.com/mlrun/mlrun" rel="noopener ugc nofollow" target="_blank"> MLRun </a>函数可以用一个简单的方法转换成<a class="ae ko" href="https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/" rel="noopener ugc nofollow" target="_blank"> KubeFlow管道</a>步骤。as_step())，并指定如何将步骤输出馈入其他步骤输入，查看完整的笔记本<a class="ae ko" href="https://github.com/mlrun/demo-image-classification/blob/master/load_project.ipynb" rel="noopener ugc nofollow" target="_blank">示例在这里</a>，下面的代码演示了图形DSL。</p><p id="221e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">MLRun项目可以有多个工作流，它们可以通过单个命令启动，也可以由各种事件触发，如Git push或HTTP REST调用。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="5763" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我们运行我们的管道，我们可以使用KubeFlow跟踪它的进展，MLRun将自动在KubeFlow UI中注册指标、输入、输出和工件，而无需编写一行额外的代码(我想你应该先尝试在没有MLRun的情况下这样做，以便欣赏它😊).</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nl"><img src="../Images/b2988172ee1e27bdbfa3473048bf650b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fKBGHxJTq7Nik8C-TvcdGg.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">KubeFlow输出(图片由作者提供)</p></figure><p id="2696" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于更基本的项目示例，您可以查看MLRun <a class="ae ko" href="https://github.com/mlrun/demo-xgb-project" rel="noopener ugc nofollow" target="_blank"> Iris XGBoost项目</a>，其他演示可以在<a class="ae ko" href="https://github.com/mlrun/demos" rel="noopener ugc nofollow" target="_blank"> MLRun演示资源库</a>中找到，并且您可以查看<a class="ae ko" href="https://github.com/mlrun/mlrun/blob/master/README.md" rel="noopener ugc nofollow" target="_blank"> MLRun自述文件</a>和示例以获得教程和简单示例。</p><h1 id="a842" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">摘要</h1><p id="d6ae" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">本文展示了如何有效利用计算资源来大规模运行数据科学工作，但更重要的是，我展示了如何简化和自动化数据科学开发，从而实现更高的生产率和更快的上市时间。如果您有其他问题，请联系我。</p><p id="eeef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请查看11月KubeCon NA的“我的GPU即服务”演示和现场演示。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="nm my l"/></div></figure><p id="ec2c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">亚龙</p></div></div>    
</body>
</html>