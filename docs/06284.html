<html>
<head>
<title>Street Fighter II is Hard, So I Trained an AI To Beat It for Me</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">街头霸王2很难，所以我训练了一个人工智能来帮我打败它</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/street-fighter-ii-is-hard-so-i-trained-an-ai-to-beat-it-for-me-891dd5fc05be?source=collection_archive---------23-----------------------#2020-05-20">https://towardsdatascience.com/street-fighter-ii-is-hard-so-i-trained-an-ai-to-beat-it-for-me-891dd5fc05be?source=collection_archive---------23-----------------------#2020-05-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6ec6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">训练一个人工神经网络玩街霸II冠军版</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ad3d1a84d48f5e6c9a511c25054acaec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XArwAl7B4_fTDfKh"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">瑞安·昆塔尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="894f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我先说我不擅长打游戏。在成长过程中，我总是在大多数游戏中失败，直到我想出了扣杀的策略，直到我赢了。尽管如此，作为一个年轻的成年人，我决定接受街头霸王II的挑战，这是一款卡普空钟爱的游戏。这是90年代的一款游戏，它激发了一代人酷炫的战斗动作和惊人的关卡主题。</p><p id="83d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然砸按钮策略很有效，但我需要一个不同的策略来完成这个活动。然后，突然意识到，我想出了一个主意，训练一个强化剂来帮我打败它。所以，舞台设置好了，我决定开始训练我自己的模特来击败这场运动！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="b9b0" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">健身房复古和整合工具</h1><p id="5c0a" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">首先，我们需要一种方法将《街头霸王2》实际实现到Python中。有几种方法可以做到这一点，但为了节省时间，最简单的方法是健身房复古。Gym-retro是一个Python包，可以将我们的游戏数据转化为可用的环境。Gym-retro带有超过1000种不同游戏的预制环境。这个工具很棒，因为它使得在游戏中实现一个人工代理变得非常简单。这也使得提取每次跑步的行动和奖励变得非常简单。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/875dbdb4fefec4313f59b15ae647cebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RDJF9Lh9V42kggSSK5qZSw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://openai.com/blog/gym-retro/" rel="noopener ugc nofollow" target="_blank">开AI健身房复古</a></p></figure><p id="9f0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最初的包中唯一缺少的是每个级别的保存状态。但是，他们也发布了自己的集成工具，您可以在这种情况下使用！</p><p id="38c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你首先需要把它安装到你的健身房复古文件和游戏文件所在的文件夹中。开始后，你只需要把你的角色带到你想要保存状态的关卡开始，然后输出。短小精悍！</p><p id="4803" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，过了一段时间，我获得了我们将要用来训练代理人的所有等级。接下来，我们需要决定使用什么强化算法。</p><h1 id="3f90" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">稳定的基线和培训</h1><p id="d4e5" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我决定采用A2C(演员优势评论家)模型的算法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/7a9e9c610835a3c28b3f91c3f3ad315d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HYkW9nT7xFOwUKn7"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">A2C <a class="ae kv" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank">算法</a></p></figure><p id="57f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个模型背后的基本思想是，预测一个近乎无限量输入的行动几乎是不可能的。因此，我们将使用一个神经网络“演员”来探索当前环境并执行一个动作。另一个神经网络“批评家”获取当前状态和行为，并输出评估。“行动者”然后根据“批评者”的建议调整执行某些动作的概率。改变执行动作的方式是优势所在。它比较了执行某个动作的代理人与在该状态下采取的平均动作之间的回报。</p><p id="882d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">稳定基线是Open AI的初始基线Python包的延续，对原始包进行了改进。它包括许多强化学习模型，所以一定要去看看！他们的A2C模型是我们将使用的模型，有了它，我们就有了所有的运动部件来训练我们的模型！</p><div class="nd ne gp gr nf ng"><a href="https://github.com/hill-a/stable-baselines" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd ir gy z fp nl fr fs nm fu fw ip bi translated">丘陵/稳定基线</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">稳定基线是一组基于OpenAI基线的强化学习算法的改进实现…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">github.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu kp ng"/></div></div></a></div><p id="ac72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用Google Colab作为我们的Python环境。使用免费版的Colab，我们可以在谷歌的云平台上运行Python笔记本，并利用他们的Tesla K80 GPU进行我们的培训。安装完所有的包之后，是时候创建我们的模型了。首先，我们验证我们的Google Drive，并使用Gym-Retro包中的Python脚本导入我们的游戏文件。</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="2298" class="oa ma iq nw b gy ob oc l od oe">!pip install stable-baselines[mpi]==2.10.0<br/>!pip install gym-retro<br/>!pip install tqdm</span><span id="650b" class="oa ma iq nw b gy of oc l od oe">from tqdm import tqdm<br/>import time<br/>import retro</span><span id="efba" class="oa ma iq nw b gy of oc l od oe">from stable_baselines.common.policies import MlpPolicy,MlpLstmPolicy, MlpLnLstmPolicy, CnnLnLstmPolicy, CnnPolicy, CnnLstmPolicy</span><span id="9a88" class="oa ma iq nw b gy of oc l od oe">from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv</span><span id="bdfe" class="oa ma iq nw b gy of oc l od oe">from stable_baselines import PPO2, A2C</span><span id="fc11" class="oa ma iq nw b gy of oc l od oe">from google.colab import drive<br/>drive.mount('/gdrive')<br/>!python -m retro.import /gdrive/"My Drive"/"Where you Keep your Game Files"</span></pre><p id="83e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们将创建我们的模型。我们将使用我们之前在CNN政策中引入的A2C模型(最适合2D游戏)。我们还需要通过提供我们正在使用的游戏以及我们希望代理从什么状态开始来初始化我们的环境。</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="e6ad" class="oa ma iq nw b gy ob oc l od oe">#Create and Train Model on SFII Engine<br/>gamename = 'StreetFighterIISpecialChampionEdition-Genesis'<br/>modelname = 'Fighter_a2c_pt2' #whatever name you want to give it<br/>env = DummyVecEnv([lambda: retro.make(gamename ,state='Champion.Level1.RyuVsGuile')])</span><span id="3529" class="oa ma iq nw b gy of oc l od oe">model = A2C(CnnPolicy,env,n_steps=128, verbose=1)<br/>#model = A2C.load('/gdrive/My Drive/ROMS/Fighter_a2c_pt2.zip')<br/>model.set_env(env)<br/>model.learn(total_timesteps=1000)</span><span id="c512" class="oa ma iq nw b gy of oc l od oe">#Saves Model into<br/>model.save("/gdrive/My Drive/"#"Whatever Your File Name is/" + modelname)</span><span id="ec59" class="oa ma iq nw b gy of oc l od oe">env.close()</span></pre><p id="a93a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在是有趣的部分。我们将训练我们的模型来击败这场运动！街头霸王II战役中，Ryu是唯一可玩的角色，所以我们会用他来训练我们的模型。我们的模型将在每一关运行100，000次，并在每一关后保存。我们的最终结果基于10个训练周期(每个级别100万次迭代)。我很好奇的一件事是培训需要多长时间？通过利用Python中的时间包，我能够看到每个训练周期大约需要2个小时。</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="17e7" class="oa ma iq nw b gy ob oc l od oe">#Training and Saving Your Model</span><span id="030b" class="oa ma iq nw b gy of oc l od oe">#Use whatever you called your states without the .state extension<br/>sts = ['RyuVsGuile','RyuVsBlanka','RyuVsRyu','RyuVsKen','RyuVsChunLi','RyuVsZangief','RyuVsDhalsim','RyuVsHonda','RyuVsBalrog','RyuVsVega','RyuVsSagat','RyuVsBison']</span><span id="21a0" class="oa ma iq nw b gy of oc l od oe">start_time = time.time()<br/>for st in tqdm(sts, desc='Main Loop'):<br/>  print(st)<br/>  env = DummyVecEnv([lambda: retro.make('StreetFighterIISpecialChampionEdition-Genesis', state=st, scenario='scenario')])<br/>  model.set_env(env)<br/>  model.learn(total_timesteps=500000)<br/>  model.save(modelname)<br/>  env.close()<br/>end_time = time.time() - start_time<br/>print(f'\n The Training Took {end_time} seconds')</span></pre><p id="364d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练完我们的模型后，我们将需要计算代理获得的奖励金额。我们可以通过让代理人玩一个关卡并计算代理人获得的奖励金额(赢得一场比赛后获得的点数)来实现这一点。这个模型在一个比训练更容易的难度上被测试，仅仅是因为它产生了最好的结果。我们还将利用Gym-Retro中的一个伟大功能，它允许你记录你的代理人在游戏中的表现！录制的素材是BK2文件，可以使用Gym-Retro附带的另一个Python脚本将其转换为MP4。</p><pre class="kg kh ki kj gt nv nw nx ny aw nz bi"><span id="17b4" class="oa ma iq nw b gy ob oc l od oe">env = DummyVecEnv([lambda: retro.make('StreetFighterIISpecialChampionEdition-Genesis',state='RyuVsHonda-Easy', record='/gdrive/My Drive/'#"Wherever you put file")])</span><span id="d479" class="oa ma iq nw b gy of oc l od oe">model = A2C.load(modelname)<br/>model.set_env(env)<br/>obs = env.reset()<br/>done = False<br/>reward = 0</span><span id="733b" class="oa ma iq nw b gy of oc l od oe">while not done:<br/>  actions, _ = model.predict(obs)<br/>  obs, rew, done, info = env.step(actions)<br/>  reward += rew<br/>print(reward)</span><span id="d905" class="oa ma iq nw b gy of oc l od oe">### Convert BK2 to MP4 File</span><span id="5a83" class="oa ma iq nw b gy of oc l od oe">!python /usr/local/lib/python3.6/dist-packages/retro/scripts/playback_movie.py "/gdrive/My Drive/Level16.RyuVsHonda-Easy-000000.bk2"</span></pre><p id="d83a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">经过漫长的旅程，我们终于制造出了一个能够打败街霸II的人工智能！下面是Ryu_Bot打完战役的视频。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="og oh l"/></div></figure><h1 id="fa56" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">模仿和未来的设计</h1><p id="3234" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">虽然这个项目给出了令人满意的结果，但这个设计有一些限制。首先，这个模型只适用于Ryu。当切换到一个单独的角色时，比如Guile，模型的表现并不好，因为他们的特殊动作有单独的输入。对于未来的设计，我会考虑使用单独的角色来训练模型，或者比较和对比不同的强化学习算法，看看哪种表现最好。</p><p id="85ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在一天结束时，如果你对开始使用机器学习感到好奇，构建一个RL代理会非常有趣和有意义。除了视频游戏，强化学习还有许多用途，比如在优化问题或机器人方面。还有大量很棒的文章和视频系列可以帮助你开始。</p><h1 id="2aab" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">感谢您的阅读！</h1><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq oh l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://giphy.com/gifs/street-fighter-fist-bump-ryu-YwR8wm7KknBok" rel="noopener ugc nofollow" target="_blank">街头霸王拳头撞GIF </a></p></figure><p id="179d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完整代码可以通过Google Colab找到:</p><div class="nd ne gp gr nf ng"><a href="https://colab.research.google.com/drive/1cbyrrfqgc2mUu6ZyfYeB20A7-Khksq5-?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd ir gy z fp nl fr fs nm fu fw ip bi translated">谷歌联合实验室</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">龙_机器人</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">colab.research.google.com</p></div></div><div class="np l"><div class="or l nr ns nt np nu kp ng"/></div></div></a></div><h1 id="504c" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">参考</h1><ol class=""><li id="ca9d" class="os ot iq ky b kz mr lc ms lf ou lj ov ln ow lr ox oy oz pa bi translated">【https://retro.readthedocs.io/en/latest/getting_started.html T4】</li><li id="a7b5" class="os ot iq ky b kz pb lc pc lf pd lj pe ln pf lr ox oy oz pa bi translated"><a class="ae kv" href="https://stable-baselines.readthedocs.io/en/master/" rel="noopener ugc nofollow" target="_blank">https://stable-baselines.readthedocs.io/en/master/</a></li><li id="f71b" class="os ot iq ky b kz pb lc pc lf pd lj pe ln pf lr ox oy oz pa bi translated"><a class="ae kv" href="https://www.youtube.com/watch?v=NyNUYYI-Pdg&amp;t=547s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=NyNUYYI-Pdg&amp;t = 547s</a></li><li id="073d" class="os ot iq ky b kz pb lc pc lf pd lj pe ln pf lr ox oy oz pa bi translated"><a class="ae kv" href="https://www.youtube.com/channel/UCLA_tAh0hX9bjl6DfCe9OLw" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/channel/UCLA_tAh0hX9bjl6DfCe9OLw</a></li><li id="8d81" class="os ot iq ky b kz pb lc pc lf pd lj pe ln pf lr ox oy oz pa bi translated"><a class="ae kv" href="https://www.youtube.com/watch?v=O5BlozCJBSE" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=O5BlozCJBSE</a></li></ol></div></div>    
</body>
</html>