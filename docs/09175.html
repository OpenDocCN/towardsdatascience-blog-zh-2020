<html>
<head>
<title>An Intuitive Explanation of the Bayesian Information Criterion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯信息准则的直观解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-explanation-of-the-bayesian-information-criterion-71a7a3d3a5c5?source=collection_archive---------19-----------------------#2020-07-01">https://towardsdatascience.com/an-intuitive-explanation-of-the-bayesian-information-criterion-71a7a3d3a5c5?source=collection_archive---------19-----------------------#2020-07-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="82db" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何知道你是否选择了正确的模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a7dcbbc348b96508cd9b0f2c542914a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9CaS_gojCLtJRSoc"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">米歇尔·特雷瑟默在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="3714" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">在</span>机器学习中，当面对堆积如山的未标记数据时，数据科学家的第一个冲动是尝试对数据进行聚类。聚类为我们提供了一种描述数据、发现数据点之间的共性以及捕捉异常值的方法。</p><p id="f7fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是在没有任何先验知识的情况下，我们如何知道数据中存在多少个聚类呢？</p><p id="5f49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数聚类技术要求我们选择固定数量的聚类。像<strong class="lb iu"> <em class="me"> k-means </em> </strong>这样的算法然后会找到这些<strong class="lb iu"> <em class="me"> k </em> </strong>不同聚类的中心。有时目视检查会有所帮助。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mf"><img src="../Images/1f2c48dfdc033092883fe56ea7cef226.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LvEt5qAwrdKaEH9miGTAig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">该数据集中似乎有 3 个聚类。</p></figure><p id="3b49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目视检查可以是一个良好的开端，尤其是如果您的数据是二维或三维的。除此之外，可视化变得更加棘手。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="ac41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们停下来一分钟，问问自己，当我们进行视觉检查时，我们的大脑在做什么。</p><p id="a5c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的视觉系统非常善于分辨对比。我们寻找点密度的对比，然后挑选出高密度的空间分离区域。然后我们计算我们发现的这种区域的数量。对于这些致密区域之外的点，我们会问:“这代表另一个星团吗？”或者“这是原始集群之一的异常值吗？”</p><p id="847a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的大脑正在执行类似贝叶斯估计的东西。我们实际上是在问，“我们的数据被我们的模型解释的概率是多少？”以及“解释我们的数据的最简单的可能模型是什么？”</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/6016d6b5fb27c46a19749412474bb099.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/0*kBCt8JkEoiG9Higi.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型 m 的极大似然函数。</p></figure><p id="63f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个问题是关于似然函数，用<strong class="lb iu"> <em class="me"> L </em> </strong>表示。我们的数据<strong class="lb iu"> <em class="me"> x </em> </strong>用模型<strong class="lb iu"> <em class="me"> M </em> </strong>来解释的可能性有多大(比如一些聚类的方法)，这些模型有一定的模型参数<strong class="lb iu"><em class="me"/></strong>(比如聚类的中心和范围)。</p><p id="cbe4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们找到最大化这个概率的参数值时，我们用克拉(<strong class="lb iu"> ^ </strong>)来表示它们。这给出了“最大似然函数”，用一个克拉表示为<strong class="lb iu"> <em class="me"> L </em> </strong>。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="db6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回到我们的例子，您可以想象一个模型，它有多少个数据点就有多少个聚类。看，没有异常值！</p><p id="c143" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但这不是一个非常有用的模型。</p><blockquote class="mo"><p id="2249" class="mp mq it bd mr ms mt mu mv mw mx lu dk translated">所有的模型都是错的，但有些是有用的。</p></blockquote><p id="9b34" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">我们必须平衡我们模型的最大可能性，<strong class="lb iu"> <em class="me"> L </em> </strong>，与模型参数的数量，<strong class="lb iu"> <em class="me"> k </em> </strong>。我们寻求参数数量最少但仍能很好地解释数据的模型。因此，我们引入了模型参数数量的惩罚<em class="me">。</em></p><p id="5f27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在已经接近贝叶斯信息准则(BIC)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/461ff7a71d78a88554d6496a46a73be3.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/0*XnIxRHhbAhOjSbhh.png"/></div></figure><p id="7a23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BIC 针对最大似然函数平衡模型参数数量<strong class="lb iu"> <em class="me"> k </em> </strong> <em class="me"> </em>和数据点数量<strong class="lb iu"> <em class="me"> n </em> </strong>、<strong class="lb iu"> <em class="me"> L </em> </strong>。我们寻求找到最小化 BIC 的模型参数<strong class="lb iu"> <em class="me"> k </em> </strong>的数量。</p><p id="189d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种形式的 BIC 源自 Gideon Schwarz[1]1978 年的一篇论文。这种推导可能很难理解，所以我们在这里不做深入探讨。</p><p id="1d51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算最大似然函数是最难的部分，但对于大多数常见的模型，存在分析函数。例如，在线性回归中，对数似然就是均方误差。</p><p id="ae2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标准的机器学习库通常会为你计算似然函数，所以不要绝望。</p><p id="bbf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们完成数据聚类的例子。我想使用高斯混合模型对数据进行聚类，并确定要选择的最佳聚类数。在 Python 中，使用<em class="me"> scikit-learn </em>库，方法如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="7293" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制不同值的<strong class="lb iu"> <em class="me"> k </em> </strong>的 BIC，我们可以看到 3 个集群的 BIC 是如何最小化的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/8ff8048e7972a8fa62c0ec0b6956b53d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sz1RcC64FpVNroYJ7cF1HA.png"/></div></div></figure><p id="94be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BIC 同意我们最初的视觉估计。它还告诉我们，更大数量的聚类也可以很好地拟合数据，但代价是必须引入更多的参数。</p><h2 id="61e3" class="nh ni it bd nj nk nl dn nm nn no dp np li nq nr ns lm nt nu nv lq nw nx ny nz bi translated">结论</h2><p id="1b33" class="pw-post-body-paragraph kz la it lb b lc oa ju le lf ob jx lh li oc lk ll lm od lo lp lq oe ls lt lu im bi translated">您总是可以找到适合您的数据的模型，但这并不能使它成为一个伟大的模型。遵循奥卡姆剃刀原理，我们应该总是选择做出最少假设的模型。在机器学习中，过度拟合模型在野外表现不佳。</p><blockquote class="mo"><p id="0c75" class="mp mq it bd mr ms mt mu mv mw mx lu dk translated">有了四个参数，我就能适应一头大象，有了五个参数，我就能让它扭动鼻子。</p><p id="63bd" class="mp mq it bd mr ms mt mu mv mw mx lu dk translated">—约翰·冯·诺依曼</p></blockquote><p id="dac0" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">使用贝叶斯信息标准，您可以找到仍然工作良好的最简单的可能模型。希望这篇文章已经让您对它的工作原理有了直观的感受。</p><h2 id="0fe8" class="nh ni it bd nj nk nl dn nm nn no dp np li nq nr ns lm nt nu nv lq nw nx ny nz bi translated">参考</h2><p id="8e51" class="pw-post-body-paragraph kz la it lb b lc oa ju le lf ob jx lh li oc lk ll lm od lo lp lq oe ls lt lu im bi translated">[1] G. E .施瓦茨，<a class="ae ky" href="https://projecteuclid.org/euclid.aos/1176344136" rel="noopener ugc nofollow" target="_blank">估计模型的维数</a> (1978)，统计年鉴，<strong class="lb iu">6</strong>(2):461–464</p></div></div>    
</body>
</html>