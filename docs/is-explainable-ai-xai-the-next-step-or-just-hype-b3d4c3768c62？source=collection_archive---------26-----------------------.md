# 可解释的 AI (xAI)是下一步，还是只是炒作？

> 原文：<https://towardsdatascience.com/is-explainable-ai-xai-the-next-step-or-just-hype-b3d4c3768c62?source=collection_archive---------26----------------------->

![](img/548f94e9db1a27bff2631daab064a965.png)

近年来，人工智能已经扩展到一系列不同程度的行业。一旦一项地平线技术(或许类似于我们现在看待[量子计算](https://www.nytimes.com/2019/10/21/science/quantum-computer-physics-qubits.html)的方式)人工智能正式突破了日常生活，知情的观点不再是技术爱好者和精英数据科学家的专利。现在，利益相关者包括高管、投资者、经理、政府，最终是客户。

虽然关于可解释人工智能(xAI)的对话可以追溯到几十年前，但这个概念在 2019 年底重新焕发了活力，当时[谷歌宣布了](https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-ai-explanations-to-increase-fairness-responsibility-and-trust)其面向开发者的新 xAI 工具集。xAI 的概念相对简单:历史上，机器学习模型在“黑箱”内运行，结果由数量惊人的交织参数决定，这些参数如此复杂(以百万计)，以至于无法解释它们。xAI 的目标是将透明性和字面解释设计到模型中，最终允许最终结果配备上下文。例如，xAI 可能会确定一幅图像是一只狼，并给出解释:它是一种有着锋利牙齿和皮毛的动物，背景中有雪。

尽管 xAI 被认为是一种技术，但它同样可以被理解为一种最佳实践。人工智能在深陷伦理困境的领域表现优于人类，例如[医疗](https://hbr.org/2019/10/ai-can-outperform-doctors-so-why-dont-patients-trust-it)、[金融](https://fortune.com/2019/10/10/artificial-intelligence-disruptive-force-finance-even-for-fintechs/)和[法律](https://www.wired.com/story/can-ai-be-fair-judge-court-estonia-thinks-so/)。虽然实施技术来减少人类偏见和提高效率的承诺很诱人，但组织要为他们的决策(人类或机器人)负责，如果他们不能解释决策，他们就容易受到多重责任的影响。AI 也许能够以一种比法官更公平的方式来设定保释金；然而，即使是 AI 也可能被糟糕的数据或过度拟合所误导，当 AI 导致不公平的判决、拒绝抵押贷款申请或误诊癌症时，问题就不可避免地出现了。错误是不可避免的，但解释错误对于任何这些高风险的环境也是必要的。

在极端情况之外，xAI 为公司提供了一个扩展的功能类别来进行推广和销售。加内特预计，到 2022 年，全球人工智能经济将从 2019 年的 1.2 万亿美元扩大到[的 3.9 万亿美元，每家公司都应该期待定义超越承诺结果的竞争模式。提供一个保证改进的黑盒人工智能模型可能很有诱惑力，但识别特定的高级功能为组织提供了谈话要点，以增强他们自己的营销和客户的意识。](https://www.forbes.com/sites/alexknapp/2018/04/25/gartner-estimates-ai-business-value-to-reach-nearly-4-trillion-by-2022/#3eb979af33f9)

# 赛的崛起

这个时机或者说 xAI 的走红并不是巧合。近年来，公众对科技行业的看法在[直线下降](https://www.pewresearch.org/fact-tank/2019/07/29/americans-have-become-much-less-positive-about-tech-companies-impact-on-the-u-s/)，只有 50%的参与者认为科技公司在美国产生了积极影响，低于四年前超过 70%的比例。尽管许多公司适应这一趋势的速度很慢，但善于协调的领导者认识到了向问责和信任的转变。实施 xAI 让科技公司朝着这个方向前进，并在一个不可避免的问题上显示出主动性[政策](https://www.darpa.mil/attachments/XAIProgramUpdate.pdf)。

2017 年，谷歌宣布他们“人工智能优先”战略政策的决定似乎很大胆；然而，仅仅几年后，科技高管紧紧拥抱人工智能的概念似乎几乎在意料之中。自从世界上第一家公司开张以来(谷歌称它是 1602 年的荷兰东印度公司，至少是公开上市的)，领导者们一直依赖于财务信息灵通的决策。近年来，大数据和物联网的兴起为以前无法获得的见解打开了闸门；高管们调整了他们的语言，加入了“基于数据”的决策。接下来，自然进化是人工智能支持的决策。领导者应该向利益相关者、公众、媒体和法律讨论和捍卫自己的决策；这种期望不会随着复杂人工智能的引入而消失。

# 使 xAI 有效

为了让 xAI 变得可靠，它不能是临时添加的或者事后想到的。开发人员和工程师必须在他们构建的应用程序的设计和架构中实现 xAI。同样需要注意的是，并不是每个人工智能项目都需要解释；在视频游戏、娱乐或生产分析类型中，xAI 可能是笨重且成本过高的。

人工智能和机器学习中一个反复出现的现象是无法解释“黑盒”内的操作，从而产生某些理想的结果。开发者社区的成员对 xAI 的承诺表示怀疑，认为有些模型太复杂了，无法解释，被迫解释会阻碍创造性的进步。在某些情况下，这无疑是正确的。这种现实是由我们对技术的原始理解造成的，还是一个更普遍、不可避免的原因有待讨论。

走向 xAI 并不要求工程师或架构师停止生产黑盒模型；它只是提高了最关键的面向公众的技术的标准，这些技术在严重依赖健全的道德规范的领域中运行。任何人工智能的理想结果和期望都应该在早期会议中确定，xAI 应该是讨论的一部分。

一些项目可能需要专门为性能而设计的复杂的黑盒模型，而没有解释，而其他项目可能没有价值。每个项目都有独特的需求，xAI 提供了多一层可能性。

[*标题图片*](https://pixabay.com/photos/london-skyscraper-the-scalpel-sky-3833039/)