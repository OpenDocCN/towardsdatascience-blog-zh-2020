<html>
<head>
<title>Getting Started with Reinforcement Learning — Tic Tac Toe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习入门—井字游戏</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-to-reinforcement-learning-tic-tac-toe-f1d32d53acb4?source=collection_archive---------45-----------------------#2020-05-09">https://towardsdatascience.com/getting-started-to-reinforcement-learning-tic-tac-toe-f1d32d53acb4?source=collection_archive---------45-----------------------#2020-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a6a5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">实现一个简单的双代理强化学习模型</h2></div><p id="8826" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">想象你自己试图通过选择一系列活动(学习、疯狂观看、做白日梦)来最大化你的日常生产力。在这里，你是试图获得最大回报的代理人。生产力)通过选择一组合理的<em class="le">行动。</em>你选择的每一个行动都会把你带到一个新的<em class="le">状态</em>(例如，选择狂看而不是训练你的RL模型会影响你的情绪，这将进一步削弱你的生产力)。</p><p id="2d6d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">强化学习(RL)很好地抓住了这个想法。它关注的是代理人如何通过采取特定的行动来最大化他们的奖励功能。每一个动作及其对应的奖励都会产生一种新的状态，这种新的状态会影响后续的动作-奖励交互。当我们开始实现我们自己的RL模型时，这些概念将变得更加清晰，这个模型可以和我们一起玩井字游戏。我们开始吧！</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="2dfe" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated"><strong class="ak">在RL世界中构建井字游戏</strong></h1><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi me"><img src="../Images/cd636ed75a75b35eba233dfb114c0276.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*mIjIjWIUc45MQjLDVkOC-w.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图1:井字游戏板设置</p></figure><p id="1fa0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">井字游戏的目标是第一个在水平、垂直或对角线排列中放置标记(十字或十字)。现在，让我们根据前面提到的RL关键字来定义游戏:</p><ol class=""><li id="1a6e" class="mq mr it kk b kl km ko kp kr ms kv mt kz mu ld mv mw mx my bi translated"><em class="le">代理</em>涉及两个井字游戏玩家，他们试图通过轮流放置标记来智胜对方，</li><li id="021c" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated"><em class="le">奖励</em>指的是获胜代理人获得的任意值，</li><li id="fae5" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated"><em class="le">动作</em>规定每个代理只允许在一个空盒子中放置他们相应的标记，</li><li id="8437" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated"><em class="le">状态</em>是每回合后井字游戏棋盘的配置，直到游戏以赢或平结束。</li></ol><h1 id="832d" class="lm ln it bd lo lp ne lr ls lt nf lv lw jz ng ka ly kc nh kd ma kf ni kg mc md bi translated">国家</h1><p id="79db" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">我们将初始化<code class="fe no np nq nr b">State</code>类，它将监视每个代理如何交互、接收奖励和玩游戏。</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="0790" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里我们用0初始化一个3x3的棋盘，2个玩家和第一个开始游戏的玩家。<code class="fe no np nq nr b">isEnd</code>是一个标志，表示游戏是否还可以继续，或者已经确定了获胜者。每个玩家被分配一个特定的<code class="fe no np nq nr b">playerSymbol</code>，其中<code class="fe no np nq nr b">p1</code>用1和<code class="fe no np nq nr b">p2</code> -1表示。</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="6e5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe no np nq nr b">getHash</code>是一个方便的功能，可以使电路板配置变平，便于以后的分析。输出形状从(3，3)转换为(9)。<code class="fe no np nq nr b">getAvailablePosition</code>将获取棋盘上尚未被占据的箱子集合，因此可用于下一步棋。</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="fc88" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在为代理定义了可用的动作空间之后，我们需要定义如何确定获胜者(这将打破边玩边循环)。这里，每轮游戏后可以获得4种可能的<code class="fe no np nq nr b">break</code>结果:<em class="le">列赢、行赢、对角赢、和棋</em>。</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="6f49" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每次成功执行一个动作后，<code class="fe no np nq nr b">updateStates</code>将使用当前的<code class="fe no np nq nr b">playerSymbol</code>更新板卡配置。下一个玩家将会被选出来继续游戏。如果游戏结束(无论是赢还是平)，<code class="fe no np nq nr b">giveReward</code>将输入一个任意值(即。赢的玩家得1分，输的玩家得0分，平局的情况下两者都得0.1–0.5分)。</p><h1 id="cf63" class="lm ln it bd lo lp ne lr ls lt nf lv lw jz ng ka ly kc nh kd ma kf ni kg mc md bi translated">代理人</h1><p id="be39" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">现在让我们初始化<code class="fe no np nq nr b">Agent</code>类来指定它们的行为(选择一个最大化奖励的动作，计算奖励)</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="4139" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下超参数需要更多解释:</p><ol class=""><li id="fe6d" class="mq mr it kk b kl km ko kp kr ms kv mt kz mu ld mv mw mx my bi translated"><code class="fe no np nq nr b">lr (learning_rate)</code>每个代理完成学习的速度，</li><li id="465a" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated"><code class="fe no np nq nr b">decay_gamma</code>每回合奖励衰减的因子(即。迫使代理人以最少的可能移动赢得胜利，因为以更多的移动赢得胜利会导致<em class="le">衰减的</em>奖励，</li><li id="f3f0" class="mq mr it kk b kl mz ko na kr nb kv nc kz nd ld mv mw mx my bi translated"><code class="fe no np nq nr b">exp_rate</code>探索率，允许代理选择可用的随机位置，而不是贪婪地从当前学习的策略集合中寻找(利用)最佳的可能下一步行动。较高的值允许代理人的决策有更多的随机性。</li></ol><p id="95e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe no np nq nr b">states</code>将存储给定游戏中每个玩家的棋盘配置列表，而<code class="fe no np nq nr b">states_val</code>是一个键值字典，保存与特定棋盘配置/状态(‘key’)相关联的奖励(‘value’)。</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="f6b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们定义<code class="fe no np nq nr b">chooseAction</code>，它允许代理人或者<em class="le">探索</em>(选择随机动作)或者<em class="le">利用</em>(在给定当前棋盘配置/状态的情况下，选择具有最大回报的动作)。</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="1c90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代理采取行动后，<code class="fe no np nq nr b">addStates </code>会将结果板配置添加到<code class="fe no np nq nr b">states</code>。当游戏结束时，<code class="fe no np nq nr b">feedReward</code>会以一种(1)相反的方式给每个配置分配一个奖励:最新的移动/状态优先；以一种(2)折扣/衰减的方式:惩罚更多的移动。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/7778cdb3b0aa92047423b2e804ceb9cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*E21pvUvAj7PuAASoTwzKsw.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图2:值迭代(针对强化学习问题)</p></figure><p id="f819" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe no np nq nr b">feedReward</code>计算与特定状态相关的值(即states_val)通过<em class="le">累加</em> <em class="le">当前值和给定学习率下一个和当前状态之间的折扣/衰减差(图2)。</em></p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="c8f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于内务处理，我们将保存训练过的策略，并在以后使用训练过的RL机器人时加载它们！:)</p><h1 id="ee43" class="lm ln it bd lo lp ne lr ls lt nf lv lw jz ng ka ly kc nh kd ma kf ni kg mc md bi translated">让机器人玩吧！</h1><p id="df56" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">在定义了<code class="fe no np nq nr b">States</code>和<code class="fe no np nq nr b">Player</code>类之后，我们将实现游戏的实际运行！</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="4327" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe no np nq nr b">playBot</code>函数在<code class="fe no np nq nr b">State</code>类中定义，作为我们玩游戏的逻辑。两位玩家将轮流选择一个动作(随机或贪婪)，直到选出一个赢家。奖励将根据我们之前定义的价值分配给两个代理。</p><h1 id="b404" class="lm ln it bd lo lp ne lr ls lt nf lv lw jz ng ka ly kc nh kd ma kf ni kg mc md bi translated">让人类去玩吧！</h1><p id="9b8b" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">在策略被保存后，现在是我们和我们训练过的机器人比赛的时候了！我们首先需要稍微修改之前定义的<code class="fe no np nq nr b">playBot</code>函数来加载学习到的策略(让bot使用它之前学习到的智能！)</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="e40a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，我们还需要稍微修改一下<code class="fe no np nq nr b">HumanPlayer</code>的定义:通过用户输入选择下一组动作！</p><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="ns nt l"/></div></figure><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/f4004dd28f6318b135576c5347c98391.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*aOXTD4SJZDCrkyIAb1irWQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图3:我们试图击败超级机器人！</p></figure><p id="9598" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就这样，很酷吧？祝你训练愉快！你可以在这里查看完整代码和文档:<a class="ae nw" href="https://github.com/juannat95/reinforcement_learning_projects/tree/master/tic-tac-toe" rel="noopener ugc nofollow" target="_blank">代码</a>。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="0bfb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">做订阅我的邮件简讯:</em></strong><a class="ae nw" href="https://tinyurl.com/2npw2fnz" rel="noopener ugc nofollow" target="_blank"><em class="le">https://tinyurl.com/2npw2fnz</em></a><em class="le"/><strong class="kk iu"><em class="le">在这里我定期用通俗易懂的语言和漂亮的可视化方式总结AI研究论文。</em> </strong></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="9eca" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">参考</h1><p id="95b3" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">[1]<a class="ae nw" href="https://github.com/JaeDukSeo/reinforcement-learning-an-introduction" rel="noopener ugc nofollow" target="_blank">https://github . com/JaeDukSeo/reinforcement-learning-an-introduction</a></p></div></div>    
</body>
</html>