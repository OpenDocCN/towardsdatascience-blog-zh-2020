# è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åˆ†å—:è§£ç 

> åŸæ–‡ï¼š<https://towardsdatascience.com/chunking-in-nlp-decoded-b4a71b2b4e24?source=collection_archive---------8----------------------->

## å®ƒåœ¨æ–‡æœ¬å¤„ç†ä¸­èµ·ä»€ä¹ˆä½œç”¨

å½“æˆ‘å¼€å§‹å­¦ä¹ æ–‡æœ¬å¤„ç†æ—¶ï¼Œæˆ‘åšæŒäº†å¾ˆé•¿æ—¶é—´çš„ä¸€ä¸ªè¯é¢˜æ˜¯ç»„å—ã€‚(æˆ‘çŸ¥é“ï¼Œå¾ˆéš¾ç›¸ä¿¡ğŸ™†)é€šå¸¸ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç½‘ä¸Šæ‰¾åˆ°è®¸å¤šæ–‡ç« ï¼Œä»å®¹æ˜“åˆ°éš¾çš„ä¸»é¢˜ï¼Œä½†å½“è°ˆåˆ°è¿™ä¸ªç‰¹å®šçš„ä¸»é¢˜æ—¶ï¼Œæˆ‘è§‰å¾—æ²¡æœ‰ä¸€ç¯‡æ–‡ç« å¯ä»¥å¯¹ç»„å—æœ‰å…¨é¢çš„ç†è§£ï¼Œç„¶è€Œä¸‹é¢çš„ä¸€ç¯‡æ–‡ç« æ˜¯æˆ‘è¿„ä»Šä¸ºæ­¢ç ”ç©¶çš„æ‰€æœ‰ä¸è¯¥ä¸»é¢˜ç›¸å…³çš„æ–‡ç« æˆ–è§†é¢‘çš„åˆå¹¶ã€‚

æ‰€ä»¥ä¸‹é¢æ˜¯æˆ‘å¯¹ç»„å—çš„ç†è§£ã€‚

# é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯ç»„å—ï¼Ÿ

ç»„å—æ˜¯ä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æå–çŸ­è¯­çš„è¿‡ç¨‹ï¼Œè¿™æ„å‘³ç€åˆ†æå¥å­ä»¥è¯†åˆ«æˆåˆ†(åè¯ç»„ã€åŠ¨è¯ã€åŠ¨è¯ç»„ç­‰)ã€‚)ç„¶è€Œï¼Œå®ƒæ²¡æœ‰å…·ä½“è¯´æ˜å®ƒä»¬çš„å†…éƒ¨ç»“æ„ï¼Œä¹Ÿæ²¡æœ‰è¯´æ˜å®ƒä»¬åœ¨ä¸»å¥ä¸­çš„ä½œç”¨ã€‚

å®ƒå·¥ä½œåœ¨è¯æ€§æ ‡æ³¨ä¹‹ä¸Šã€‚å®ƒä½¿ç”¨è¯æ€§æ ‡ç­¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶æä¾›ç»„å—ä½œä¸ºè¾“å‡ºã€‚

> ç®€è€Œè¨€ä¹‹ï¼Œç»„å—æ„å‘³ç€å°†å•è¯/æ ‡è®°åˆ†ç»„ä¸ºç»„å—

# ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ

æˆ‘æ›¾ç»è®¤ä¸ºï¼Œåœ¨æˆ‘è¿›ä¸€æ­¥äº†è§£è¿™äº›ä¸»é¢˜ä¹‹å‰ï¼Œé€šå¸¸æ–‡æœ¬å¤„ç†æ˜¯é€šè¿‡ç®€å•åœ°å°†å¥å­åˆ†è§£æˆå•è¯æ¥å®Œæˆçš„ã€‚æ‰€ä»¥ç®€å•çš„æ‰“æ–­æ–‡å­—å¹¶æ²¡æœ‰å¤ªå¤§çš„å¸®åŠ©ã€‚çŸ¥é“è¿™ä¸ªå¥å­æ¶‰åŠåˆ°ä¸€ä¸ªäººã€ä¸€ä¸ªæ—¥æœŸã€ä¸€ä¸ªåœ°ç‚¹ç­‰ç­‰æ˜¯éå¸¸é‡è¦çš„..(ä¸åŒå®ä½“)ã€‚æ‰€ä»¥ä»–ä»¬å•ç‹¬æ˜¯æ²¡æœ‰ç”¨çš„ã€‚

> ç»„å—å¯ä»¥å°†å¥å­åˆ†è§£æˆæ¯”å•ä¸ªå•è¯æ›´æœ‰ç”¨çš„çŸ­è¯­ï¼Œå¹¶äº§ç”Ÿæœ‰æ„ä¹‰çš„ç»“æœã€‚
> 
> å½“ä½ æƒ³ä»æ–‡æœ¬ä¸­æå–ä¿¡æ¯æ—¶ï¼Œæ¯”å¦‚åœ°ç‚¹ã€äººåï¼Œç»„å—æ˜¯éå¸¸é‡è¦çš„ã€‚(å®ä½“æå–)

è®©æˆ‘ä»¬ä»å¤´å¼€å§‹ç†è§£å®ƒã€‚

ä¸€ä¸ªå¥å­é€šå¸¸éµå¾ªç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆçš„å±‚æ¬¡ç»“æ„ã€‚

## å¥å­â†’ä»å¥â†’çŸ­è¯­â†’å•è¯

è¯ç»„ç»„æˆçŸ­è¯­ï¼Œæœ‰äº”å¤§ç±»ã€‚

*   åè¯çŸ­è¯­
*   åŠ¨è¯çŸ­è¯­
*   å½¢å®¹è¯çŸ­è¯­
*   å‰¯è¯çŸ­è¯­
*   ä»‹è¯çŸ­è¯­

***çŸ­è¯­ç»“æ„è§„åˆ™:***

S -> NP VP

NP--> { Det Nï¼ŒProï¼ŒPN}

VP -> V (NP) (PP) (Adv)

PP -> P NP

AP -> A (PP)

![](img/f5ccaf773a98762c5db086df436f0c75.png)

ç»„å—:çŸ­è¯­è¢«åˆ†æˆç»„å—(æ¥æº:[https://www.nltk.org](https://www.nltk.org/book/ch07.html))

> åœ¨æ·±å…¥ç ”ç©¶ç»„å—ä¹‹å‰ï¼Œå…ˆç®€è¦äº†è§£ä¸€ä¸‹è¯­æ³•æ ‘å’Œè¯­æ³•è§„åˆ™æ˜¯æœ‰å¥½å¤„çš„ã€‚

æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¿™é‡Œæ•´ä¸ªå¥å­è¢«åˆ†æˆä¸¤ä¸ªä¸åŒçš„åè¯çŸ­è¯­ã€‚

ç°åœ¨è®©æˆ‘ä»¬ç”¨ python å®éªŒæ¥ç†è§£è¿™ä¸ªæ¦‚å¿µã€‚

1.  **åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„åˆ†å—**

åŸºäºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼çš„åˆ†å—å™¨ä»£ç æ®µ

![](img/a9210859eabc4735d84cf995bde138c6.png)

è§£ææ ‘(åè¯çŸ­è¯­åŸºäºç»™å®šçš„æ­£åˆ™è¡¨è¾¾å¼ç”Ÿæˆ)

è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªè¯­æ³•ã€‚
å…¶ä¸­ NP(åè¯çŸ­è¯­)ç”±
DT ç»„åˆè€Œæˆï¼Ÿâ†’ä¸€ä¸ªæˆ–é›¶ä¸ªé™å®šè¯
JJ* â†’é›¶ä¸ªæˆ–å¤šä¸ªå½¢å®¹è¯
NN â†’åè¯

æˆ‘ä»¬ç”¨ NLTK å®šä¹‰çš„æ­£åˆ™è¡¨è¾¾å¼è§£æå™¨æ¥è§£æè¿™ä¸ªè¯­æ³•ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæ•´ä¸ªå¥å­ S è¢«åˆ†æˆå‡ ä¸ªç»„å—ï¼Œå¹¶ç”¨æ ‘çŠ¶ç»“æ„è¡¨ç¤ºã€‚åŸºäºå®šä¹‰çš„è¯­æ³•ï¼Œåˆ›å»ºå†…éƒ¨æ ‘çŠ¶ç»“æ„ã€‚æ‰€ä»¥ä½ å¯ä»¥å®šä¹‰ä½ çš„è¯­æ³•ï¼ŒåŸºäºè¿™ä¸ªå¥å­å°†è¢«åˆ†å—ã€‚

**2ã€‚åŸºäºæ ‡ç­¾çš„è®­ç»ƒåˆ†å—å™¨**

æˆ‘ä½¿ç”¨äº†â€œconll2000â€è¯­æ–™åº“æ¥è®­ç»ƒ chunkerã€‚conll2000 è¯­æ–™åº“ä½¿ç”¨ IOB æ ‡ç­¾å®šä¹‰ç»„å—ã€‚

å®ƒæŒ‡å®šäº†å—çš„å¼€å§‹å’Œç»“æŸä½ç½®ï¼Œä»¥åŠå®ƒçš„ç±»å‹ã€‚
POS æ ‡ç­¾å‘˜å¯ä»¥æ¥å—è¿™äº› IOB æ ‡ç­¾çš„åŸ¹è®­

å—æ ‡ç­¾ä½¿ç”¨ IOB æ ¼å¼ã€‚
IOB : Insideï¼ŒOutsideï¼ŒBeginning
æ ‡ç­¾å‰çš„ B å‰ç¼€è¡¨ç¤ºï¼Œå®ƒæ˜¯å—çš„å¼€å§‹
I å‰ç¼€è¡¨ç¤ºå®ƒåœ¨å—å†…
O æ ‡ç­¾è¡¨ç¤ºä»¤ç‰Œä¸å±äºä»»ä½•å—

```
#Here conll2000 corpus for training shallow parser modelnltk.download('conll2000')
from nltk.corpus import conll2000data= conll2000.chunked_sents()
train_data=data[:10900]
test_data=data[10900:]print(len(train_data),len(test_data))
print(train_data[1])
```

![](img/756a40a8f87f7d679cf3c7273d1924ef.png)

â€œconll2000â€æ•°æ®é›†çš„è®°å½•

tree2conlltagsï¼Œconlltags2tree æ˜¯åˆ†å—å®ç”¨å‡½æ•°ã€‚

â†’`**tree 2 conltags**`ï¼Œè·å–ä¸‰å…ƒç»„(æ¯ä¸ªä»¤ç‰Œçš„å•è¯ã€æ ‡ç­¾ã€å—æ ‡ç­¾)ã€‚ç„¶åï¼Œè¿™äº›å…ƒç»„æœ€ç»ˆç”¨äºè®­ç»ƒæ ‡ç­¾å™¨ï¼Œå¹¶ä¸”å®ƒå­¦ä¹  POS æ ‡ç­¾çš„ IOB æ ‡ç­¾ã€‚

â†’ ` **conlltags2tree** `ä»è¿™äº›ä»¤ç‰Œä¸‰å…ƒç»„ç”Ÿæˆè§£ææ ‘
Conlltags2tree()æ˜¯ tree2conlltags()çš„åè½¬ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™äº›å‡½æ•°è®­ç»ƒæˆ‘ä»¬çš„è§£æå™¨

```
from nltk.chunk.util import tree2conlltags,conlltags2treewtc=tree2conlltags(train_data[1])
wtc
```

![](img/5938079f1b9ce854cc7cb50455d153d9.png)

```
tree=conlltags2tree(wtc)
print(tree)
```

![](img/216cb8c4be4c7437b04800d33dfcc66c.png)

```
def conll_tag_chunks(chunk_sents):
    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]
    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]def combined_tagger(train_data, taggers, backoff=None):
    for tagger in taggers:
        backoff = tagger(train_data, backoff=backoff)
    return backoff
```

# tagger æ˜¯ä»€ä¹ˆï¼Ÿ

å®ƒè¯»å–æ–‡æœ¬å¹¶ç»™æ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ª POS æ ‡ç­¾ã€‚(å•è¯ã€æ ‡ç­¾)

**Unigram tagger** :ç¡®å®š POS æ—¶ï¼Œåªä½¿ç”¨ä¸€ä¸ªå•è¯ã€‚(åŸºäºå•è¯ä¸Šä¸‹æ–‡çš„æ ‡è®°å™¨)

`UnigramTagger`ã€`BigramTagger`å’Œ`TrigramTagger`æ˜¯ç»§æ‰¿è‡ªåŸºç±»`NGramTagger`çš„ç±»ï¼ŒåŸºç±»æœ¬èº«ç»§æ‰¿è‡ª`ContextTagger`ç±»ï¼Œåè€…ç»§æ‰¿è‡ª`SequentialBackoffTagger`ç±»

æˆ‘ä»¬ç°åœ¨å°†å®šä¹‰ä¸€ä¸ªç±»`NGramTagChunker`ï¼Œå®ƒå°†æ¥å—å¸¦æ ‡ç­¾çš„å¥å­ä½œä¸ºè®­ç»ƒè¾“å…¥ï¼Œè·å–å®ƒä»¬çš„**(å•è¯ã€è¯æ€§æ ‡ç­¾ã€ç»„å—æ ‡ç­¾)WTC ä¸‰å…ƒç»„**å¹¶è®­ç»ƒä¸€ä¸ªå¸¦æœ‰`UnigramTagger`çš„`BigramTagger`ä½œä¸ºè¡¥å¿æ ‡ç­¾ã€‚

æˆ‘ä»¬è¿˜å°†å®šä¹‰ä¸€ä¸ª parse()å‡½æ•°æ¥å¯¹ä¸€ä¸ªæ–°å¥å­æ‰§è¡Œæµ…å±‚è§£æã€‚

```
from nltk.tag import UnigramTagger, BigramTagger
from nltk.chunk import ChunkParserI#Define the chunker class
class NGramTagChunker(ChunkParserI):
  def __init__(self,train_sentences,tagger_classes=[UnigramTagger,BigramTagger]):
    train_sent_tags=conll_tag_chunks(train_sentences)
    self.chunk_tagger=combined_tagger(train_sent_tags,tagger_classes)def parse(self,tagged_sentence):
    if not tagged_sentence:
      return None
    pos_tags=[tag for word, tag in tagged_sentence]
    chunk_pos_tags=self.chunk_tagger.tag(pos_tags)
    chunk_tags=[chunk_tag for (pos_tag,chunk_tag) in chunk_pos_tags]
    wpc_tags=[(word,pos_tag,chunk_tag) for ((word,pos_tag),chunk_tag) in zip(tagged_sentence,chunk_tags)]
    return conlltags2tree(wpc_tags)#train chunker model
ntc=NGramTagChunker(train_data)#evaluate chunker model performance
print(ntc.evaluate(test_data))
```

![](img/fb73436782eda9e096348c36fe842d63.png)

ç°åœ¨ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨è¿™ä¸ªæ¨¡å‹å¯¹æˆ‘ä»¬çš„æ–°é—»æ ‡é¢˜æ ·æœ¬è¿›è¡Œæµ…å±‚è§£æå’Œåˆ†å—ã€‚

```
import pandas as pd
sentence='No new emoji may be released in 2021 due to COVID-19 pandemic word'
nltk_pos_tagged=nltk.pos_tag(sentence.split())
pd.DataFrame(nltk_pos_tagged,columns=['word','POS tag'])
```

![](img/b7ff4e07bd2da0a58119aab3d8bc3b13.png)

```
chunk_tree=ntc.parse(nltk_pos_tagged)
print(chunk_tree)
```

![](img/0c0b965b0423780e637ff28525ba411d.png)

è¯­æ³•æ ‘

```
chunk_tree
```

![](img/f58941baa18a4ce2ecfd63d746be6dc5.png)

è§£ææ ‘

ä½ ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å®šä¹‰åŸºäºåˆ†ç±»å™¨çš„åˆ†å—å™¨ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œäº†è§£æ›´å¤šã€‚

[https://www . geeks forgeeks . org/NLP-classifier-based-chunking-set-1/ï¼Ÿref=rp](https://www.geeksforgeeks.org/nlp-classifier-based-chunking-set-1/?ref=rp)

## åˆ†å—çš„å¦ä¸€ä¸ªå­è¿‡ç¨‹å«åšâ€œåˆ†å—â€

æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä»¤ç‰Œåºåˆ—ï¼Œå®ƒä¸åŒ…å«åœ¨å—ä¸­ã€‚æ‰€ä»¥è¿™æ˜¯å¯»æ‰¾æ´å¯ŸåŠ›æˆ–èƒŒæ™¯ã€‚(æˆ‘ä¸åœ¨è¿™ä¸€éƒ¨åˆ†è®¨è®º)

æ„Ÿè°¢æ‚¨çš„é˜…è¯»ã€‚ğŸ™

æˆ‘è¯•å›¾æœ€å¤§ç¨‹åº¦åœ°æ¶µç›–è¿™ä¸ªè¯é¢˜ã€‚æ¬¢è¿å»ºè®®ã€‚

ä½œä¸ºè£èª‰å¥–ï¼Œæˆ‘è¦æ„Ÿè°¢[è¿ªæ½˜ç„¶(DJ)è¨å¡å°”](https://medium.com/u/6278d12b0682?source=post_page-----b4a71b2b4e24--------------------------------)ã€‚æˆ‘ä¸€ç›´è·Ÿç€ä»–çš„æ•™ç¨‹ä»é›¶å¼€å§‹å­¦ NLPã€‚

> é¡µï¼ˆpage çš„ç¼©å†™ï¼‰è¿™æ˜¯æˆ‘çš„ç¬¬ä¸€ç¯‡æŠ€æœ¯æ–‡ç« ã€‚å¸Œæœ›éšç€å­¦ä¹ çš„è¿›æ­¥ï¼Œæˆ‘ä¼šç»§ç»­å†™ä½œã€‚

## å‚è€ƒ

*   https://youtu.be/b4nbE-pG_TM
*   [https://towards data science . com/a-ä»ä¸šè€…-æŒ‡å—-è‡ªç„¶è¯­è¨€-å¤„ç†-ç¬¬ä¸€éƒ¨åˆ†-å¤„ç†-ç†è§£-æ–‡æœ¬-9f4abfd13e72](/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)
*   [https://www . geeks forgeeks . org/NLP-chunking-and-chinking-with-regex/](https://www.geeksforgeeks.org/nlp-chunking-and-chinking-with-regex/)
*   [https://www . geeks forgeeks . org/NLP-training-tagger-based-chunker-set-1/](https://www.geeksforgeeks.org/nlp-training-tagger-based-chunker-set-1/)