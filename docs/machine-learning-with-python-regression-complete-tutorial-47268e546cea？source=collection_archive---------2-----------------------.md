# Python æœºå™¨å­¦ä¹ :å›å½’(å®Œæ•´æ•™ç¨‹)

> åŸæ–‡ï¼š<https://towardsdatascience.com/machine-learning-with-python-regression-complete-tutorial-47268e546cea?source=collection_archive---------2----------------------->

![](img/bf7a59d41b19f964ce71b34753c515bf.png)

## æ•°æ®åˆ†æå’Œå¯è§†åŒ–ã€ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©ã€æ¨¡å‹è®¾è®¡å’Œæµ‹è¯•ã€è¯„ä¼°å’Œè§£é‡Š

## æ‘˜è¦

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨æ•°æ®ç§‘å­¦å’Œ Python è§£é‡Šå›å½’ç”¨ä¾‹çš„ä¸»è¦æ­¥éª¤ï¼Œä»æ•°æ®åˆ†æåˆ°ç†è§£æ¨¡å‹è¾“å‡ºã€‚

![](img/29f211b020ad5f116090ac99e99bf171.png)

æˆ‘å°†å±•ç¤ºä¸€äº›æœ‰ç”¨çš„ Python ä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥å¾ˆå®¹æ˜“åœ°ç”¨äºå…¶ä»–ç±»ä¼¼çš„æƒ…å†µ(åªéœ€å¤åˆ¶ã€ç²˜è´´ã€è¿è¡Œ)ï¼Œå¹¶é€šè¿‡æ³¨é‡Šéå†æ¯ä¸€è¡Œä»£ç ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥å¾ˆå®¹æ˜“åœ°å¤åˆ¶è¿™ä¸ªç¤ºä¾‹(ä¸‹é¢æ˜¯å®Œæ•´ä»£ç çš„é“¾æ¥)ã€‚

[](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/machine_learning/example_regression.ipynb) [## mdipietro 09/data science _ äººå·¥æ™ºèƒ½ _ å®ç”¨å·¥å…·

### permalink dissolve GitHub æ˜¯ 4000 å¤šä¸‡å¼€å‘äººå‘˜çš„å®¶å›­ï¼Œä»–ä»¬ä¸€èµ·å·¥ä½œæ¥æ‰˜ç®¡å’Œå®¡æŸ¥ä»£ç ï¼Œç®¡ç†â€¦

github.com](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/machine_learning/example_regression.ipynb) 

æˆ‘å°†ä½¿ç”¨â€œ**æˆ¿ä»·æ•°æ®é›†**â€(é“¾æ¥å¦‚ä¸‹)ï¼Œå…¶ä¸­ä¸ºæ‚¨æä¾›äº†æè¿°ä¸€äº›ä½å®…ä¸åŒæ–¹é¢çš„å¤šä¸ªè§£é‡Šå˜é‡ï¼Œä»»åŠ¡æ˜¯é¢„æµ‹æ¯å¥—ä½å®…çš„æœ€ç»ˆä»·æ ¼ã€‚

[](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) [## æˆ¿ä»·:é«˜çº§å›å½’æŠ€æœ¯

### é¢„æµ‹é”€å”®ä»·æ ¼å¹¶å®è·µç‰¹å¾å·¥ç¨‹ã€RFs å’Œæ¢¯åº¦æ¨è¿›

www.kaggle.com](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) 

ç‰¹åˆ«æ˜¯ï¼Œæˆ‘å°†ç»å†:

*   ç¯å¢ƒè®¾ç½®:å¯¼å…¥åº“å¹¶è¯»å–æ•°æ®
*   æ•°æ®åˆ†æ:ç†è§£å˜é‡çš„æ„ä¹‰å’Œé¢„æµ‹èƒ½åŠ›
*   ç‰¹å¾å·¥ç¨‹:ä»åŸå§‹æ•°æ®ä¸­æå–ç‰¹å¾
*   é¢„å¤„ç†:æ•°æ®åˆ’åˆ†ã€å¤„ç†ç¼ºå¤±å€¼ã€ç¼–ç åˆ†ç±»å˜é‡ã€ç¼©æ”¾
*   ç‰¹å¾é€‰æ‹©:åªä¿ç•™æœ€ç›¸å…³çš„å˜é‡
*   æ¨¡å‹è®¾è®¡:åŸºçº¿ã€è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•
*   ç»©æ•ˆè¯„ä¼°:é˜…è¯»æŒ‡æ ‡
*   å¯è§£é‡Šæ€§:ç†è§£æ¨¡å‹å¦‚ä½•åšå‡ºé¢„æµ‹

## è®¾ç½®

é¦–å…ˆï¼Œæˆ‘éœ€è¦å¯¼å…¥ä»¥ä¸‹åº“ã€‚

```
**## for data**
import **pandas** as pd
import **numpy** as np**## for plotting**
import **matplotlib**.pyplot as plt
import **seaborn** as sns**## for statistical tests**
import **scipy**
import **statsmodels**.formula.api as smf
import statsmodels.api as sm**## for machine learning**
from **sklearn** import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition**## for explainer**
from **lime** import lime_tabular
```

ç„¶åæˆ‘ä¼šæŠŠæ•°æ®è¯»å…¥ä¸€ä¸ª*ç†ŠçŒ«*æ•°æ®å¸§ã€‚åŸå§‹æ•°æ®é›†åŒ…å« 81 åˆ—ï¼Œä½†æ˜¯å‡ºäºæœ¬æ•™ç¨‹çš„ç›®çš„ï¼Œæˆ‘å°†ä½¿ç”¨ 12 åˆ—çš„å­é›†ã€‚

```
dtf = pd.read_csv("data_houses.csv")cols = ["OverallQual","GrLivArea","GarageCars", 
        "GarageArea","TotalBsmtSF","FullBath",
        "YearBuilt","YearRemodAdd",
        "LotFrontage","MSSubClass"]dtf = dtf[["Id"]+cols+["SalePrice"]]
dtf.head()
```

![](img/334b7aeb4f9175f7ec98e39a2132317e.png)

æœ‰å…³åˆ—çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨æ‰€æä¾›çš„æ•°æ®é›†é“¾æ¥ä¸­æ‰¾åˆ°ã€‚

è¯·æ³¨æ„ï¼Œè¡¨æ ¼çš„æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„æˆ¿å­(æˆ–è§‚å¯Ÿ)ã€‚å¦‚æœæ‚¨æ­£åœ¨å¤„ç†ä¸€ä¸ªä¸åŒçš„æ•°æ®é›†ï¼Œå®ƒæ²¡æœ‰è¿™æ ·çš„ç»“æ„ï¼Œå…¶ä¸­æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè§‚å¯Ÿï¼Œé‚£ä¹ˆæ‚¨éœ€è¦æ±‡æ€»æ•°æ®å¹¶è½¬æ¢å®ƒã€‚

ç°åœ¨ä¸€åˆ‡éƒ½è®¾ç½®å¥½äº†ï¼Œæˆ‘å°†ä»åˆ†ææ•°æ®å¼€å§‹ï¼Œç„¶åé€‰æ‹©ç‰¹å¾ï¼Œå»ºç«‹æœºå™¨å­¦ä¹ æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ã€‚

æˆ‘ä»¬å¼€å§‹å§ï¼Œå¥½å—ï¼Ÿ

## æ•°æ®åˆ†æ

åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œ[æ¢ç´¢æ€§æ•°æ®åˆ†æ](https://en.wikipedia.org/wiki/Exploratory_data_analysis)æ˜¯å¯¹æ•°æ®é›†çš„ä¸»è¦ç‰¹å¾è¿›è¡Œæ€»ç»“çš„è¿‡ç¨‹ï¼Œä»¥äº†è§£æ•°æ®åœ¨æ­£å¼å»ºæ¨¡æˆ–å‡è®¾æ£€éªŒä»»åŠ¡ä¹‹å¤–è¿˜èƒ½å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆã€‚

æˆ‘æ€»æ˜¯ä»è·å¾—æ•´ä¸ªæ•°æ®é›†çš„æ¦‚è¿°å¼€å§‹ï¼Œç‰¹åˆ«æ˜¯ï¼Œæˆ‘æƒ³çŸ¥é“æœ‰å¤šå°‘ä¸ª**åˆ†ç±»**å’Œ**æ•°å€¼**å˜é‡ä»¥åŠ**ç¼ºå¤±æ•°æ®**çš„æ¯”ä¾‹ã€‚è¯†åˆ«å˜é‡çš„ç±»å‹æœ‰æ—¶ä¼šå¾ˆæ£˜æ‰‹ï¼Œå› ä¸ºç±»åˆ«å¯ä»¥ç”¨æ•°å­—è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œæˆ‘å°†ç¼–å†™ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥å®Œæˆè¿™é¡¹å·¥ä½œ:

```
**'''
Recognize whether a column is numerical or categorical.
:parameter
    :param dtf: dataframe - input data
    :param col: str - name of the column to analyze
    :param max_cat: num - max number of unique values to recognize a column as categorical
:return
    "cat" if the column is categorical or "num" otherwise
'''**
def **utils_recognize_type**(dtf, col, max_cat=20):
    if (dtf[col].dtype == "O") | (dtf[col].nunique() < max_cat):
        return **"cat"**
    else:
        return **"num"**
```

è¿™ä¸ªåŠŸèƒ½éå¸¸æœ‰ç”¨ï¼Œå¯ä»¥ç”¨åœ¨å¾ˆå¤šåœºåˆã€‚ä¸ºäº†ä¸¾ä¾‹è¯´æ˜ï¼Œæˆ‘å°†ç»˜åˆ¶ dataframe çš„ [**çƒ­å›¾**](http://Heat map) ï¼Œå¹¶å¯è§†åŒ–åˆ—ç±»å‹å’Œç¼ºå¤±çš„æ•°æ®ã€‚

```
dic_cols = {col:**utils_recognize_type**(dtf, col, max_cat=20) for col in dtf.columns}heatmap = dtf.isnull()
for k,v in dic_cols.items():
 if v == "num":
   heatmap[k] = heatmap[k].apply(lambda x: 0.5 if x is False else 1)
 else:
   heatmap[k] = heatmap[k].apply(lambda x: 0 if x is False else 1)sns.**heatmap**(heatmap, cbar=False).set_title('Dataset Overview')
plt.show()print("\033[1;37;40m Categerocial ", "\033[1;30;41m Numeric ", "\033[1;30;47m NaN ")
```

![](img/40d6b603311fa00d7a35c32e0597d8df.png)

æœ‰ 1460 è¡Œå’Œ 12 åˆ—:

*   è¡¨ä¸­çš„æ¯ä¸€è¡Œéƒ½ä»£è¡¨ä¸€ä¸ªç”± *Id* æ ‡è¯†çš„ç‰¹å®šæˆ¿å±‹(æˆ–è§‚å¯Ÿ)ï¼Œæ‰€ä»¥æˆ‘å°†å®ƒè®¾ç½®ä¸ºç´¢å¼•(æˆ–è€…ä¸º SQL çˆ±å¥½è€…è®¾ç½®è¡¨çš„[ä¸»é”®](https://en.wikipedia.org/wiki/Primary_key))ã€‚
*   *SalePrice* æ˜¯æˆ‘ä»¬æƒ³è¦äº†è§£å’Œé¢„æµ‹çš„å› å˜é‡ï¼Œæ‰€ä»¥æˆ‘å°†è¯¥åˆ—é‡å‘½åä¸ºâ€œ*Yâ€*ã€‚
*   *æ€»ä½“è´¨é‡ã€è½¦åº“ã€å…¨æµ´*å’Œ*ms å­ç±»*æ˜¯åˆ†ç±»å˜é‡ï¼Œå…¶ä»–æ˜¯æ•°å€¼å˜é‡ã€‚
*   åªæœ‰ *LotFrontage* åŒ…å«ç¼ºå¤±æ•°æ®ã€‚

```
dtf = dtf.set_index("**Id**")dtf = dtf.rename(columns={"**SalePrice**":"**Y**"})
```

æˆ‘ç›¸ä¿¡å¯è§†åŒ–æ˜¯æ•°æ®åˆ†æçš„æœ€å¥½å·¥å…·ï¼Œä½†æ˜¯ä½ éœ€è¦çŸ¥é“ä»€ä¹ˆæ ·çš„å›¾æ›´é€‚åˆä¸åŒç±»å‹çš„å˜é‡ã€‚å› æ­¤ï¼Œæˆ‘å°†æä¾›ä»£ç æ¥ä¸ºä¸åŒçš„ç¤ºä¾‹ç»˜åˆ¶é€‚å½“çš„å¯è§†åŒ–ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å•å˜é‡åˆ†å¸ƒ(åªæœ‰ä¸€ä¸ªå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒ)ã€‚ä¸€ä¸ª [**ç›´æ–¹å›¾**](https://en.wikipedia.org/wiki/Histogram) å®Œç¾åœ°ç»™å‡ºäº†å•ä¸ª**æ•°å€¼**æ•°æ®çš„åº•å±‚åˆ†å¸ƒå¯†åº¦çš„ç²—ç•¥æ„Ÿè§‰ã€‚æˆ‘æ¨èä½¿ç”¨ä¸€ä¸ª [**æ–¹æ¡†å›¾**](https://en.wikipedia.org/wiki/Box_plot) æ¥å›¾å½¢åŒ–åœ°æç»˜æ•°æ®ç»„é€šè¿‡å®ƒä»¬çš„å››åˆ†ä½æ•°ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ç»˜åˆ¶ç›®æ ‡å˜é‡:

```
**x = "Y"**fig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)
fig.suptitle(x, fontsize=20)**### distribution**
ax[0].title.set_text('distribution')
variable = dtf[x].fillna(dtf[x].mean())
breaks = np.quantile(variable, q=np.linspace(0, 1, 11))
variable = variable[ (variable > breaks[0]) & (variable < 
                    breaks[10]) ]
sns.distplot(variable, hist=True, kde=True, kde_kws={"shade": True}, ax=ax[0])
des = dtf[x].describe()
ax[0].axvline(des["25%"], ls='--')
ax[0].axvline(des["mean"], ls='--')
ax[0].axvline(des["75%"], ls='--')
ax[0].grid(True)
des = round(des, 2).apply(lambda x: str(x))
box = '\n'.join(("min: "+des["min"], "25%: "+des["25%"], "mean: "+des["mean"], "75%: "+des["75%"], "max: "+des["max"]))
ax[0].text(0.95, 0.95, box, transform=ax[0].transAxes, fontsize=10, va='top', ha="right", bbox=dict(boxstyle='round', facecolor='white', alpha=1))**### boxplot** 
ax[1].title.set_text('outliers (log scale)')
tmp_dtf = pd.DataFrame(dtf[x])
tmp_dtf[x] = np.log(tmp_dtf[x])
tmp_dtf.boxplot(column=x, ax=ax[1])
plt.show()
```

![](img/f9aac63ecdb9248d46fc3e185b1d603c.png)

è¿™ä¸ªç¾¤ä½“çš„å¹³å‡æˆ¿ä»·æ˜¯ 181ï¼Œ000 ç¾å…ƒï¼Œåˆ†å¸ƒæ˜¯é«˜åº¦åæ–œçš„ï¼Œä¸¤è¾¹éƒ½æœ‰å¼‚å¸¸å€¼ã€‚

æ­¤å¤–ï¼Œä¸€ä¸ª [**æ¡å½¢å›¾**](https://en.wikipedia.org/wiki/Bar_chart) é€‚äºç†è§£å•ä¸ª**åˆ†ç±»**å˜é‡çš„æ ‡ç­¾é¢‘ç‡ã€‚è®©æˆ‘ä»¬ä»¥*å…¨æµ´å®¤*(æµ´å®¤æ•°é‡)å˜é‡ä¸ºä¾‹:å®ƒå…·æœ‰æ™®é€šæ€§(2 ä¸ªæµ´å®¤> 1 ä¸ªæµ´å®¤)ï¼Œä½†å®ƒä¸æ˜¯è¿ç»­çš„(ä¸€ä¸ªå®¶åº­ä¸å¯èƒ½æœ‰ 1.5 ä¸ªæµ´å®¤)ï¼Œæ‰€ä»¥å®ƒå¯ä»¥ä½œä¸ºä¸€ä¸ªèŒƒç•´æ¥åˆ†æã€‚

```
**x = "Y"**ax = dtf[x].value_counts().sort_values().plot(kind="barh")
totals= []
for i in ax.patches:
    totals.append(i.get_width())
total = sum(totals)
for i in ax.patches:
     ax.text(i.get_width()+.3, i.get_y()+.20, 
     str(round((i.get_width()/total)*100, 2))+'%', 
     fontsize=10, color='black')
ax.grid(axis="x")
plt.suptitle(x, fontsize=20)
plt.show()
```

![](img/45933eb55f8567a4f718ff33b65b8399.png)

å¤§å¤šæ•°æˆ¿å­æœ‰ 1 ä¸ªæˆ– 2 ä¸ªæµ´å®¤ï¼Œä¹Ÿæœ‰ä¸€äº›ç¦»ç¾¤å€¼æœ‰ 0 ä¸ªå’Œ 3 ä¸ªæµ´å®¤ã€‚

æˆ‘å°†æŠŠåˆ†æå¸¦åˆ°ä¸‹ä¸€ä¸ªå±‚æ¬¡ï¼Œå¹¶ç ”ç©¶äºŒå…ƒåˆ†å¸ƒï¼Œä»¥äº†è§£ *FullBath* æ˜¯å¦å…·æœ‰é¢„æµ‹ *Y* çš„é¢„æµ‹èƒ½åŠ›ã€‚è¿™å°±æ˜¯**åˆ†ç±»(*å…¨æµ´*)å¯¹æ•°å­—( *Y* )** çš„æƒ…å†µï¼Œå› æ­¤æˆ‘å°†è¿™æ ·è¿›è¡Œ:

*   å°†äººå£(æ•´å¥—è§‚å¯Ÿç»“æœ)åˆ†æˆ 4 ä¸ªæ ·æœ¬:æœ‰ 0 é—´æµ´å®¤(*å…¨æµ´= 0)ã€* 1 é—´æµ´å®¤(*å…¨æµ´= 1)* çš„æˆ¿å±‹éƒ¨åˆ†ï¼Œä»¥æ­¤ç±»æ¨â€¦
*   ç»˜åˆ¶å¹¶æ¯”è¾ƒ 4 ä¸ªæ ·æœ¬çš„å¯†åº¦ï¼Œå¦‚æœåˆ†å¸ƒä¸åŒï¼Œåˆ™å˜é‡æ˜¯å¯é¢„æµ‹çš„ï¼Œå› ä¸º 4 ç»„å…·æœ‰ä¸åŒçš„æ¨¡å¼ã€‚
*   å°†æ•°å€¼å˜é‡( *Y* )åˆ†ç»„åˆ°ç®±(å­æ ·æœ¬)ä¸­ï¼Œå¹¶ç»˜åˆ¶æ¯ä¸ªç®±çš„ç»„æˆï¼Œå¦‚æœæ‰€æœ‰ç®±ä¸­ç±»åˆ«çš„æ¯”ä¾‹ç›¸ä¼¼ï¼Œåˆ™è¯¥å˜é‡ä¸å…·æœ‰é¢„æµ‹æ€§ã€‚
*   ç»˜åˆ¶å¹¶æ¯”è¾ƒ 4 ä¸ªæ ·æœ¬çš„ç®±çº¿å›¾ï¼Œæ‰¾å‡ºå¼‚å¸¸å€¼çš„ä¸åŒè¡Œä¸ºã€‚

```
**cat, num = "FullBath", "Y"**fig, ax = plt.subplots(nrows=1, ncols=3,  sharex=False, sharey=False)
fig.suptitle(x+"   vs   "+y, fontsize=20)

**### distribution**
ax[0].title.set_text('density')
for i in dtf[cat].unique():
    sns.distplot(dtf[dtf[cat]==i][num], hist=False, label=i, ax=ax[0])
ax[0].grid(True)**### stacked**
ax[1].title.set_text('bins')
breaks = np.quantile(dtf[num], q=np.linspace(0,1,11))
tmp = dtf.groupby([cat, pd.cut(dtf[num], breaks, duplicates='drop')]).size().unstack().T
tmp = tmp[dtf[cat].unique()]
tmp["tot"] = tmp.sum(axis=1)
for col in tmp.drop("tot", axis=1).columns:
     tmp[col] = tmp[col] / tmp["tot"]
tmp.drop("tot", axis=1).plot(kind='bar', stacked=True, ax=ax[1], legend=False, grid=True)**### boxplot **  
ax[2].title.set_text('outliers')
sns.catplot(x=cat, y=num, data=dtf, kind="box", ax=ax[2])
ax[2].grid(True)
plt.show()
```

![](img/66c639c586c9d7e96f59be1341f867f4.png)

FullBath ä¼¼ä¹å…·æœ‰é¢„æµ‹æ€§ï¼Œå› ä¸º 4 ä¸ªæ ·æœ¬çš„åˆ†å¸ƒåœ¨ä»·æ ¼æ°´å¹³å’Œè§‚å¯Ÿæ¬¡æ•°ä¸Šæœ‰å¾ˆå¤§ä¸åŒã€‚æˆ¿å­é‡Œçš„æµ´å®¤è¶Šå¤šï¼Œä»·æ ¼å°±è¶Šé«˜ï¼Œä½†æˆ‘æƒ³çŸ¥é“ 0 æµ´å®¤æ ·æœ¬å’Œ 3 æµ´å®¤æ ·æœ¬ä¸­çš„è§‚å¯Ÿå€¼æ˜¯å¦æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ï¼Œå› ä¸ºå®ƒä»¬åŒ…å«çš„è§‚å¯Ÿå€¼å¾ˆå°‘ã€‚

å½“â€œè§†è§‰ç›´è§‰â€æ— æ³•è¯´æœä½ æ—¶ï¼Œä½ å¯ä»¥æ±‚åŠ©äºå¤è€çš„ç»Ÿè®¡æ•°æ®æ¥è¿›è¡Œæµ‹è¯•ã€‚åœ¨è¿™ç§åˆ†ç±»(*full bath*vs æ•°å€¼( *Y* )çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä¼šä½¿ç”¨ä¸€ä¸ª**o**[**n-way ANOVA æ£€éªŒ**](http://en.wikipedia.org/wiki/F_test#One-way_ANOVA_example) ã€‚åŸºæœ¬ä¸Šæ˜¯æ£€éªŒä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸Šç‹¬ç«‹æ ·æœ¬çš„å‡å€¼æ˜¯å¦æ˜¾è‘—ä¸åŒï¼Œæ‰€ä»¥å¦‚æœ p å€¼è¶³å¤Ÿå°(< 0.05)æ ·æœ¬çš„é›¶å‡è®¾æ„å‘³ç€ç›¸ç­‰å¯ä»¥è¢«æ‹’ç»ã€‚

```
**cat, num = "FullBath", "Y"**model = smf.**ols**(num+' ~ '+cat, data=dtf).fit()
table = sm.stats.**anova_lm**(model)
p = table["PR(>F)"][0]
coeff, p = None, round(p, 3)
conclusion = "Correlated" if p < 0.05 else "Non-Correlated"
print("Anova F: the variables are", conclusion, "(p-value: "+str(p)+")")
```

![](img/ce15c45655e68aa8ccd66c50e471675d.png)

æˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œæµ´å®¤çš„æ•°é‡å†³å®šäº†æˆ¿å­çš„ä»·æ ¼ã€‚è¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºæ›´å¤šçš„æµ´å®¤æ„å‘³ç€æ›´å¤§çš„æˆ¿å­ï¼Œè€Œæˆ¿å­çš„å¤§å°æ˜¯ä¸€ä¸ªé‡è¦çš„ä»·æ ¼å› ç´ ã€‚

ä¸ºäº†æ£€æŸ¥ç¬¬ä¸€ä¸ªç»“è®ºçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘å¿…é¡»åˆ†æç›®æ ‡å˜é‡ç›¸å¯¹äº *GrLivArea* (ä»¥å¹³æ–¹è‹±å°ºä¸ºå•ä½çš„åœ°ä¸Šå±…ä½é¢ç§¯)çš„è¡Œä¸ºã€‚è¿™æ˜¯ä¸€ä¸ª**æ•°å€¼( *GrLivArea* ) vs æ•°å€¼( *Y* )** çš„ä¾‹å­ï¼Œæ‰€ä»¥æˆ‘å°†ç”Ÿæˆä¸¤ä¸ªå›¾:

*   é¦–å…ˆï¼Œæˆ‘å°†æŠŠ *GrLivArea* å€¼åˆ†ç»„åˆ°ç®±ä¸­ï¼Œå¹¶æ¯”è¾ƒæ¯ä¸ªç®±ä¸­ *Y* çš„å¹³å‡å€¼(å’Œä¸­å€¼),å¦‚æœæ›²çº¿ä¸å¹³å¦ï¼Œåˆ™å˜é‡æ˜¯é¢„æµ‹æ€§çš„ï¼Œå› ä¸ºç®±å…·æœ‰ä¸åŒçš„æ¨¡å¼ã€‚
*   ç¬¬äºŒï¼Œæˆ‘å°†ä½¿ç”¨æ•£ç‚¹å›¾ï¼Œå›¾ä¸­ä¸¤è¾¹æ˜¯ä¸¤ä¸ªå˜é‡çš„åˆ†å¸ƒã€‚

```
**x, y = "GrLivArea", "Y"****### bin plot** dtf_noNan = dtf[dtf[x].notnull()]
breaks = np.quantile(dtf_noNan[x], q=np.linspace(0, 1, 11))
groups = dtf_noNan.groupby([pd.cut(dtf_noNan[x], bins=breaks, 
           duplicates='drop')])[y].agg(['mean','median','size'])
fig, ax = plt.subplots(figsize=figsize)
fig.suptitle(x+"   vs   "+y, fontsize=20)
groups[["mean", "median"]].plot(kind="line", ax=ax)
groups["size"].plot(kind="bar", ax=ax, rot=45, secondary_y=True,
                    color="grey", alpha=0.3, grid=True)
ax.set(ylabel=y)
ax.right_ax.set_ylabel("Observazions in each bin")
plt.show()**### scatter plot**
sns.jointplot(x=x, y=y, data=dtf, dropna=True, kind='reg', 
              height=int((figsize[0]+figsize[1])/2) )
plt.show()
```

![](img/4a5f81f0c2a71f05104378aa1b0bc5a6.png)

GrLivArea æ˜¯é¢„æµ‹æ€§çš„ï¼Œæœ‰ä¸€ä¸ªæ¸…æ™°çš„æ¨¡å¼:å¹³å‡æ¥è¯´ï¼Œæˆ¿å­è¶Šå¤§ï¼Œä»·æ ¼è¶Šé«˜ï¼Œå³ä½¿æœ‰ä¸€äº›è¶…å‡ºå¹³å‡æ°´å¹³çš„å¼‚å¸¸å€¼å’Œç›¸å¯¹è¾ƒä½çš„ä»·æ ¼ã€‚

å°±åƒä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹è¯•è¿™ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æ—¢ç„¶éƒ½æ˜¯æ•°å€¼ï¼Œæˆ‘å°± **t** [**est çš®å°”é€Šç›¸å…³ç³»æ•°**](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) :å‡è®¾ä¸¤ä¸ªå˜é‡æ˜¯ç‹¬ç«‹çš„(é›¶å‡è®¾)ï¼Œæ£€éªŒä¸¤ä¸ªæ ·æœ¬æ˜¯å¦æœ‰çº¿æ€§å…³ç³»ã€‚å¦‚æœ p å€¼è¶³å¤Ÿå°(< 0.05)ï¼Œå¯ä»¥æ‹’ç»é›¶å‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥è¯´è¿™ä¸¤ä¸ªå˜é‡å¯èƒ½æ˜¯ç›¸å…³çš„ã€‚

```
**x, y = "GrLivArea", "Y"**dtf_noNan = dtf[dtf[x].notnull()]
coeff, p = scipy.stats.**pearsonr**(dtf_noNan[x], dtf_noNan[y])
coeff, p = round(coeff, 3), round(p, 3)
conclusion = "Significant" if p < 0.05 else "Non-Significant"
print("Pearson Correlation:", coeff, conclusion, "(p-value: "+str(p)+")")
```

![](img/d03964f25d450aa83f37b3701b9a870a.png)

*FullBath* å’Œ *GrLivArea* æ˜¯é¢„æµ‹ç‰¹æ€§çš„ä¾‹å­ï¼Œå› æ­¤æˆ‘å°†ä¿ç•™å®ƒä»¬ç”¨äºå»ºæ¨¡ã€‚

åº”è¯¥å¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªå˜é‡è¿›è¡Œè¿™ç§åˆ†æï¼Œä»¥å†³å®šå“ªäº›åº”è¯¥ä½œä¸ºæ½œåœ¨ç‰¹å¾ä¿ç•™ï¼Œå“ªäº›å› ä¸ºä¸å…·æœ‰é¢„æµ‹æ€§è€Œå¯ä»¥æ”¾å¼ƒ(æŸ¥çœ‹å®Œæ•´ä»£ç çš„é“¾æ¥)ã€‚

## ç‰¹å¾å·¥ç¨‹

æ˜¯æ—¶å€™ä½¿ç”¨é¢†åŸŸçŸ¥è¯†ä»åŸå§‹æ•°æ®åˆ›å»ºæ–°è¦ç´ äº†ã€‚æˆ‘å°†æä¾›ä¸€ä¸ªä¾‹å­: *MSSubClass* åˆ—(building ç±»)åŒ…å« 15 ä¸ªç±»åˆ«ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„æ•°ç›®ï¼Œåœ¨å»ºæ¨¡è¿‡ç¨‹ä¸­ä¼šå¯¼è‡´ç»´åº¦é—®é¢˜ã€‚è®©æˆ‘ä»¬çœ‹çœ‹:

```
sns.**catplot**(x="MSSubClass", y="Y", data=dtf, kind="box")
```

![](img/07caee9f3dea139f9410f1158e28a237.png)

æœ‰è®¸å¤šç±»åˆ«ï¼Œå¾ˆéš¾ç†è§£æ¯ä¸ªç±»åˆ«ä¸­çš„åˆ†å¸ƒæƒ…å†µã€‚å› æ­¤ï¼Œæˆ‘å°†æŠŠè¿™äº›ç±»åˆ«åˆ†ç»„ä¸ºç°‡:å…·æœ‰è¾ƒé«˜ *Y* å€¼çš„ç±»(å¦‚ *MSSubClass 60 å’Œ 120* )å°†è¿›å…¥â€œæœ€å¤§â€ç°‡ï¼Œä»·æ ¼è¾ƒä½çš„ç±»(å¦‚ *MSSubClass 30ã€45ã€180* )å°†è¿›å…¥â€œæœ€å°â€ç°‡ï¼Œå…¶ä½™çš„å°†è¿›å…¥â€œå¹³å‡â€ç°‡ã€‚

```
**## define clusters**
MSSubClass_clusters = {"**min**":[30,45,180], "**max**":[60,120], "**mean**":[]}**## create new columns**
dic_flat = {v:k for k,lst in MSSubClass_clusters.items() for v in lst}
for k,v in MSSubClass_clusters.items():
    if len(v)==0:
        residual_class = k 
dtf[x+"_cluster"] = dtf[x].apply(lambda x: dic_flat[x] if x in 
                          dic_flat.keys() else residual_class)**## print**
dtf[["MSSubClass","MSSubClass_cluster","Y"]].head()
```

![](img/2e099d81b30f13b2f45c133be166b14c.png)

è¿™æ ·ï¼Œæˆ‘å°†ç±»åˆ«çš„æ•°é‡ä» 15 ä¸ªå‡å°‘åˆ° 3 ä¸ªï¼Œè¿™æ ·æ›´ä¾¿äºåˆ†æ:

![](img/e556fab1eaf8f886919f9649a88be2d0.png)

æ–°çš„åˆ†ç±»ç‰¹æ€§æ›´æ˜“äºé˜…è¯»ï¼Œå¹¶ä¸”ä¿æŒäº†åŸå§‹æ•°æ®ä¸­æ˜¾ç¤ºçš„æ¨¡å¼ï¼Œå› æ­¤æˆ‘å°†ä¿ç•™ *MSSubClass_cluster* è€Œä¸æ˜¯åˆ— *MSSubClass* ã€‚

## é¢„å¤„ç†

æ•°æ®é¢„å¤„ç†æ˜¯å‡†å¤‡åŸå§‹æ•°æ®ä»¥ä½¿å…¶é€‚åˆæœºå™¨å­¦ä¹ æ¨¡å‹çš„é˜¶æ®µã€‚ç‰¹åˆ«æ˜¯:

1.  æ¯ä¸ªè§‚å¯Ÿå¿…é¡»ç”¨ä¸€è¡Œæ¥è¡¨ç¤ºï¼Œæ¢å¥è¯è¯´ï¼Œä¸èƒ½ç”¨ä¸¤è¡Œæ¥æè¿°åŒä¸€ä¸ªä¹˜å®¢ï¼Œå› ä¸ºå®ƒä»¬å°†è¢«æ¨¡å‹åˆ†åˆ«å¤„ç†(æ•°æ®é›†å·²ç»æ˜¯è¿™æ ·çš„å½¢å¼ï¼Œæ‰€ä»¥âœ…).è€Œä¸”æ¯ä¸€åˆ—éƒ½åº”è¯¥æ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œæ‰€ä»¥ä½ ä¸åº”è¯¥ç”¨ *Id* ä½œä¸ºé¢„æµ‹å™¨ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿™ç§è¡¨å«åšâ€œ**ç‰¹å¾çŸ©é˜µ**â€ã€‚
2.  æ•°æ®é›†å¿…é¡»**åˆ†å‰²**æˆè‡³å°‘ä¸¤ä¸ªé›†åˆ:æ¨¡å‹åº”è¯¥åœ¨ä½ çš„æ•°æ®é›†çš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†ä¸Šè®­ç»ƒ(æ‰€è°“çš„â€œè®­ç»ƒé›†â€)ï¼Œåœ¨ä¸€ä¸ªè¾ƒå°çš„é›†åˆä¸Šæµ‹è¯•(â€œæµ‹è¯•é›†â€)ã€‚
3.  **ç¼ºå°‘çš„å€¼**åº”è¯¥ç”¨ä¸œè¥¿æ›¿æ¢ï¼Œå¦åˆ™ï¼Œä½ çš„æ¨¡å‹å¯èƒ½ä¼šå‡ºé—®é¢˜ã€‚
4.  **åˆ†ç±»æ•°æ®**å¿…é¡»ç¼–ç ï¼Œè¿™æ„å‘³ç€å°†æ ‡ç­¾è½¬æ¢ä¸ºæ•´æ•°ï¼Œå› ä¸ºæœºå™¨å­¦ä¹ æœŸæœ›çš„æ˜¯æ•°å­—ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²ã€‚
5.  å¯¹æ•°æ®è¿›è¡Œ**ç¼©æ”¾**æ˜¯ä¸€ç§å¾ˆå¥½çš„åšæ³•ï¼Œè¿™æœ‰åŠ©äºåœ¨ç‰¹å®šèŒƒå›´å†…å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¹¶åŠ å¿«ç®—æ³•ä¸­çš„è®¡ç®—é€Ÿåº¦ã€‚

å¥½çš„ï¼Œè®©æˆ‘ä»¬ä»**åˆ’åˆ†æ•°æ®é›†**å¼€å§‹ã€‚å½“æŠŠæ•°æ®åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶ï¼Œä½ å¿…é¡»éµå¾ªä¸€ä¸ªåŸºæœ¬è§„åˆ™:è®­ç»ƒé›†ä¸­çš„è¡Œä¸åº”è¯¥å‡ºç°åœ¨æµ‹è¯•é›†ä¸­ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šçœ‹åˆ°ç›®æ ‡å€¼ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥ç†è§£ç°è±¡ã€‚æ¢å¥è¯è¯´ï¼Œæ¨¡å‹å·²ç»çŸ¥é“è®­ç»ƒè§‚å¯Ÿçš„æ­£ç¡®ç­”æ¡ˆï¼Œåœ¨è¿™äº›åŸºç¡€ä¸Šæµ‹è¯•å°±åƒä½œå¼Šã€‚

```
**## split data**
dtf_train, dtf_test = **model_selection**.**train_test_split**(dtf, 
                      test_size=0.3)**## print info**
print("X_train shape:", dtf_train.drop("Y",axis=1).shape, "| X_test shape:", dtf_test.drop("Y",axis=1).shape)
print("y_train mean:", round(np.mean(dtf_train["Y"]),2), "| y_test mean:", round(np.mean(dtf_test["Y"]),2))
print(dtf_train.shape[1], "features:", dtf_train.drop("Y",axis=1).columns.to_list())
```

![](img/154543f6c0d5b567ea6e5cf868f78185.png)

ä¸‹ä¸€æ­¥: *LotFrontage* åˆ—åŒ…å«ä¸€äº›éœ€è¦å¤„ç†çš„**ç¼ºå¤±æ•°æ®** (17%)ã€‚ä»æœºå™¨å­¦ä¹ çš„è§’åº¦æ¥çœ‹ï¼Œé¦–å…ˆåˆ†æˆè®­ç»ƒå’Œæµ‹è¯•ï¼Œç„¶åç”¨è®­ç»ƒé›†çš„å¹³å‡å€¼æ›¿æ¢ *NAs* æ˜¯æ­£ç¡®çš„ã€‚

```
dtf_train["*LotFrontage*"] = dtf_train["*LotFrontage*"].**fillna**(dtf_train["*LotFrontage*"].**mean**())
```

æˆ‘åˆ›å»ºçš„æ–°åˆ— *MSSubClass_cluster* åŒ…å«åº”è¯¥è¢«ç¼–ç çš„**åˆ†ç±»æ•°æ®**ã€‚æˆ‘å°†ä½¿ç”¨ One-Hot-Encoding æ–¹æ³•ï¼Œå°† 1 ä¸ªå…·æœ‰ n ä¸ªå”¯ä¸€å€¼çš„åˆ†ç±»åˆ—è½¬æ¢ä¸º n-1 ä¸ªè™šæ‹Ÿåˆ—ã€‚

```
**## create dummy**
dummy = pd.**get_dummies**(dtf_train["*MSSubClass_cluster*"], 
                       prefix="*MSSubClass_cluster*",drop_first=True)
dtf_train= pd.concat([dtf_train, dummy], axis=1)
print( dtf_train.filter(like="*MSSubClass_cluster*",axis=1).head() )**## drop the original categorical column**
dtf_train = dtf_train.drop("*MSSubClass_cluster*", axis=1)
```

![](img/2c7b0fb052beeadeb07dffb538ca0f0a.png)

æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ï¼Œæˆ‘å°†**ç¼©æ”¾ç‰¹å¾**ã€‚å¯¹äºå›å½’é—®é¢˜ï¼Œé€šå¸¸éœ€è¦è½¬æ¢è¾“å…¥å˜é‡å’Œç›®æ ‡å˜é‡ã€‚æˆ‘å°†ä½¿ç”¨*é²æ£’å®šæ ‡å™¨*ï¼Œå®ƒé€šè¿‡å‡å»ä¸­å€¼ç„¶åé™¤ä»¥å››åˆ†ä½æ•°èŒƒå›´(75%å€¼-25%å€¼)æ¥è½¬æ¢ç‰¹å¾ã€‚è¿™ä¸ªå®šæ ‡å™¨çš„ä¼˜ç‚¹æ˜¯å®ƒå—å¼‚å¸¸å€¼çš„å½±å“è¾ƒå°ã€‚

```
**## scale X**
scalerX = preprocessing.**RobustScaler**(quantile_range=(25.0, 75.0))
X = scaler.fit_transform(dtf_train.drop("Y", axis=1))dtf_scaled= pd.DataFrame(X, columns=dtf_train.drop("Y", 
                        axis=1).columns, index=dtf_train.index)**## scale Y** scalerY = preprocessing.**RobustScaler**(quantile_range=(25.0, 75.0))
dtf_scaled[y] = scalerY.fit_transform(
                    dtf_train[y].values.reshape(-1,1))dtf_scaled.head()
```

![](img/9db538c9b0c54c5321e016701da473f1.png)

## ç‰¹å¾é€‰æ‹©

ç‰¹å¾é€‰æ‹©æ˜¯é€‰æ‹©ç›¸å…³å˜é‡çš„å­é›†æ¥æ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¿‡ç¨‹ã€‚å®ƒä½¿æ¨¡å‹æ›´å®¹æ˜“è§£é‡Šï¼Œå¹¶å‡å°‘è¿‡åº¦æ‹Ÿåˆ(å½“æ¨¡å‹é€‚åº”è®­ç»ƒæ•°æ®è¿‡å¤šï¼Œå¹¶ä¸”åœ¨è®­ç»ƒé›†ä¹‹å¤–è¡¨ç°ä¸ä½³æ—¶)ã€‚

åœ¨æ•°æ®åˆ†ææœŸé—´ï¼Œæˆ‘å·²ç»é€šè¿‡æ’é™¤ä¸ç›¸å…³çš„åˆ—è¿›è¡Œäº†ç¬¬ä¸€æ¬¡â€œæ‰‹åŠ¨â€ç‰¹å¾é€‰æ‹©ã€‚ç°åœ¨ä¼šæœ‰ä¸€ç‚¹ä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬å¿…é¡»å¤„ç†**å¤šé‡å…±çº¿æ€§**é—®é¢˜ï¼Œè¿™æ˜¯æŒ‡å¤šå…ƒå›å½’æ¨¡å‹ä¸­ä¸¤ä¸ªæˆ–æ›´å¤šè§£é‡Šå˜é‡é«˜åº¦çº¿æ€§ç›¸å…³çš„æƒ…å†µã€‚

æˆ‘ç”¨ä¸€ä¸ªä¾‹å­æ¥è§£é‡Š: *GarageCars* ä¸ *GarageArea* é«˜åº¦ç›¸å…³ï¼Œå› ä¸ºå®ƒä»¬ç»™å‡ºçš„ä¿¡æ¯éƒ½æ˜¯ä¸€æ ·çš„(è½¦åº“æœ‰å¤šå¤§ï¼Œä¸€ä¸ªæ˜¯æ ¹æ®èƒ½åœå¤šå°‘è½¦ï¼Œå¦ä¸€ä¸ªæ˜¯æ ¹æ®å¹³æ–¹è‹±å°º)ã€‚è®©æˆ‘ä»¬è®¡ç®—ç›¸å…³çŸ©é˜µæ¥çœ‹çœ‹:

```
corr_matrix = dtf_train.**corr**(method="pearson")
sns.heatmap(corr_matrix, vmin=-1., vmax=1., annot=True, fmt='.2f', cmap="YlGnBu", cbar=True, linewidths=0.5)
plt.title("pearson correlation")
```

![](img/6d7d72ef8b635a87a9a8eeda954e4d63.png)

*GarageCars* å’Œ *GarageArea* ä¸­çš„ä¸€ä¸ªå¯èƒ½æ˜¯ä¸å¿…è¦çš„ï¼Œæˆ‘ä»¬å¯ä»¥å†³å®šä¸¢å¼ƒå®ƒå¹¶ä¿ç•™æœ€æœ‰ç”¨çš„ä¸€ä¸ª(å³å…·æœ‰æœ€ä½ p å€¼çš„é‚£ä¸ªæˆ–æœ€èƒ½é™ä½ç†µçš„é‚£ä¸ª)ã€‚

[**çº¿æ€§å›å½’**](https://en.wikipedia.org/wiki/Linear_regression) æ˜¯ä¸€ç§å¯¹æ ‡é‡å“åº”å’Œä¸€ä¸ªæˆ–å¤šä¸ªè§£é‡Šå˜é‡ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡çš„çº¿æ€§æ–¹æ³•ã€‚å•å˜é‡çº¿æ€§å›å½’æµ‹è¯•å¹¿æ³›ç”¨äºæµ‹è¯•è®¸å¤šå›å½’å˜é‡ä¸­æ¯ä¸€ä¸ªçš„ä¸ªä½“æ•ˆåº”:é¦–å…ˆï¼Œè®¡ç®—æ¯ä¸ªå›å½’å˜é‡å’Œç›®æ ‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œç„¶åè¿›è¡Œ ANOVA F-æµ‹è¯•ã€‚

[**å²­æ­£åˆ™åŒ–**](https://en.wikipedia.org/wiki/Tikhonov_regularization) å¯¹äºç¼“è§£çº¿æ€§å›å½’ä¸­çš„å¤šé‡å…±çº¿æ€§é—®é¢˜ç‰¹åˆ«æœ‰ç”¨ï¼Œè¿™ç§é—®é¢˜é€šå¸¸å‡ºç°åœ¨å…·æœ‰å¤§é‡å‚æ•°çš„æ¨¡å‹ä¸­ã€‚

```
X = dtf_train.drop("Y", axis=1).values
y = dtf_train["Y"].values
feature_names = dtf_train.drop("Y", axis=1).columns**## p-value**
selector = **feature_selection.SelectKBest**(score_func=  
               feature_selection.f_regression, k=10).fit(X,y)
pvalue_selected_features = feature_names[selector.get_support()]

**## regularization**
selector = **feature_selection.SelectFromModel**(estimator= 
              linear_model.Ridge(alpha=1.0, fit_intercept=True), 
                                 max_features=10).fit(X,y)
regularization_selected_features = feature_names[selector.get_support()]

**## plot** dtf_features = pd.DataFrame({"features":feature_names})
dtf_features["p_value"] = dtf_features["features"].apply(lambda x: "p_value" if x in pvalue_selected_features else "")
dtf_features["num1"] = dtf_features["features"].apply(lambda x: 1 if x in pvalue_selected_features else 0)
dtf_features["regularization"] = dtf_features["features"].apply(lambda x: "regularization" if x in regularization_selected_features else "")
dtf_features["num2"] = dtf_features["features"].apply(lambda x: 1 if x in regularization_selected_features else 0)
dtf_features["method"] = dtf_features[["p_value","regularization"]].apply(lambda x: (x[0]+" "+x[1]).strip(), axis=1)
dtf_features["selection"] = dtf_features["num1"] + dtf_features["num2"]
dtf_features["method"] = dtf_features["method"].apply(lambda x: "both" if len(x.split()) == 2 else x)sns.barplot(y="features", x="selection", hue="method", data=dtf_features.sort_values("selection", ascending=False), dodge=False)
```

![](img/de6734a43cf8b4d0e35f99ea89a9d6a4.png)

è“è‰²çš„ç‰¹å¾æ˜¯ç”±æ–¹å·®åˆ†æå’Œå²­é€‰æ‹©çš„ï¼Œå…¶ä»–çš„æ˜¯ç”±ç¬¬ä¸€ç§ç»Ÿè®¡æ–¹æ³•é€‰æ‹©çš„ã€‚

æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨é›†æˆæ–¹æ³•æ¥è·å¾—ç‰¹å¾é‡è¦æ€§ã€‚ [**é›†æˆæ–¹æ³•**](https://en.wikipedia.org/wiki/Ensemble_learning) ä½¿ç”¨å¤šç§å­¦ä¹ ç®—æ³•æ¥è·å¾—æ¯”å•ç‹¬ä½¿ç”¨ä»»ä½•ä¸€ç§æˆåˆ†å­¦ä¹ ç®—æ³•æ‰€èƒ½è·å¾—çš„æ›´å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚æˆ‘å°†ç»™å‡ºä¸€ä¸ªä½¿ç”¨ [**æ¢¯åº¦æ¨è¿›**](https://en.wikipedia.org/wiki/Gradient_boosting) ç®—æ³•çš„ä¾‹å­:å®ƒä»¥å‘å‰é€çº§çš„æ–¹å¼å»ºç«‹ä¸€ä¸ªåŠ æ€§æ¨¡å‹ï¼Œå¹¶åœ¨æ¯ä¸ªé˜¶æ®µåœ¨ç»™å®šæŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦ä¸Šæ‹Ÿåˆä¸€ä¸ªå›å½’æ ‘ã€‚

```
X = dtf_train.drop("Y", axis=1).values
y = dtf_train["Y"].values
feature_names = dtf_train.drop("Y", axis=1).columns.tolist()**## call model**
model = ensemble.**GradientBoostingRegressor**()**## Importance**
model.fit(X,y)
importances = model.**feature_importances_****## Put in a pandas dtf**
dtf_importances = pd.DataFrame({"IMPORTANCE":importances, 
            "VARIABLE":feature_names}).sort_values("IMPORTANCE", 
            ascending=False)
dtf_importances['cumsum'] =  
            dtf_importances['IMPORTANCE'].cumsum(axis=0)
dtf_importances = dtf_importances.set_index("VARIABLE")

**##** **Plot**
fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False)
fig.suptitle("Features Importance", fontsize=20)
ax[0].title.set_text('variables')
    dtf_importances[["IMPORTANCE"]].sort_values(by="IMPORTANCE").plot(
                kind="barh", legend=False, ax=ax[0]).grid(axis="x")
ax[0].set(ylabel="")
ax[1].title.set_text('cumulative')
dtf_importances[["cumsum"]].plot(kind="line", linewidth=4, 
                                 legend=False, ax=ax[1])
ax[1].set(xlabel="", xticks=np.arange(len(dtf_importances)), 
          xticklabels=dtf_importances.index)
plt.xticks(rotation=70)
plt.grid(axis='both')
plt.show()
```

![](img/9697e956e25a8a62a7bc4d79b10c8592.png)

éå¸¸æœ‰è¶£çš„æ˜¯ï¼Œ *OverallQualã€GrLivArea* å’Œ *TotalBsmtSf* åœ¨æ‰€æœ‰å‘ˆç°çš„æ–¹æ³•ä¸­å ä¸»å¯¼åœ°ä½ã€‚

å°±æˆ‘ä¸ªäººè€Œè¨€ï¼Œæˆ‘æ€»æ˜¯è¯•å›¾ä½¿ç”¨å°½å¯èƒ½å°‘çš„åŠŸèƒ½ï¼Œå› æ­¤åœ¨è¿™é‡Œæˆ‘é€‰æ‹©ä»¥ä¸‹åŠŸèƒ½ï¼Œå¹¶ç»§ç»­è®¾è®¡ã€è®­ç»ƒã€æµ‹è¯•å’Œè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹:

```
X_names = ['OverallQual', 'GrLivArea', 'TotalBsmtSF', "GarageCars"]X_train = dtf_train[X_names].values
y_train = dtf_train["Y"].valuesX_test = dtf_test[X_names].values
y_test = dtf_test["Y"].values
```

è¯·æ³¨æ„ï¼Œåœ¨ä½¿ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹ä¹‹å‰ï¼Œæ‚¨å¿…é¡»å¯¹å…¶è¿›è¡Œé¢„å¤„ç†ï¼Œå°±åƒæˆ‘ä»¬å¯¹è®­ç»ƒæ•°æ®æ‰€åšçš„é‚£æ ·ã€‚

## æ¨¡å‹è®¾è®¡

æœ€åï¼Œæ˜¯æ—¶å€™å»ºç«‹æœºå™¨å­¦ä¹ æ¨¡å‹äº†ã€‚æˆ‘å°†é¦–å…ˆè¿è¡Œä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’ï¼Œå¹¶å°†å…¶ç”¨ä½œæ›´å¤æ‚æ¨¡å‹çš„åŸºçº¿ï¼Œå¦‚æ¢¯åº¦æ¨è¿›ç®—æ³•ã€‚

æˆ‘é€šå¸¸ä½¿ç”¨çš„ç¬¬ä¸€ä¸ªæŒ‡æ ‡æ˜¯ [**R çš„å¹³æ–¹**](https://en.wikipedia.org/wiki/Coefficient_of_determination) ï¼Œå®ƒè¡¨ç¤ºè‡ªå˜é‡ä¸­å¯é¢„æµ‹çš„å› å˜é‡æ–¹å·®çš„æ¯”ä¾‹ã€‚

æˆ‘å°†ä½¿ç”¨ **k å€äº¤å‰éªŒè¯**æ¥æ¯”è¾ƒçº¿æ€§å›å½’ R çš„å¹³æ–¹ä¸æ¢¯åº¦æ¨è¿›çš„å¹³æ–¹ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†æ•°æ®åˆ†æˆ k æ¬¡è®­ç»ƒå’ŒéªŒè¯é›†çš„è¿‡ç¨‹ï¼Œå¯¹äºæ¯æ¬¡åˆ†å‰²ï¼Œæ¨¡å‹éƒ½è¢«è®­ç»ƒå’Œæµ‹è¯•ã€‚å®ƒç”¨äºæ£€æŸ¥æ¨¡å‹é€šè¿‡ä¸€äº›æ•°æ®è¿›è¡Œè®­ç»ƒçš„èƒ½åŠ›ï¼Œä»¥åŠé¢„æµ‹æœªçŸ¥æ•°æ®çš„èƒ½åŠ›ã€‚

æˆ‘å°†é€šè¿‡ç»˜åˆ¶**é¢„æµ‹å€¼ä¸å®é™…å€¼ *Y*** *æ¥å¯è§†åŒ–éªŒè¯çš„ç»“æœã€‚*ç†æƒ³æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çš„ç‚¹éƒ½åº”æ¥è¿‘é¢„æµ‹å€¼=å®é™…å€¼çš„å¯¹è§’çº¿ã€‚

```
**## call model**
model = linear_model.**LinearRegression**()**## K fold validation**
scores = []
cv = model_selection.KFold(n_splits=5, shuffle=True)
fig = plt.figure()
i = 1
for train, test in cv.split(X_train, y_train):
    prediction = model.fit(X_train[train],
                 y_train[train]).predict(X_train[test])
    true = y_train[test]
    score = metrics.r2_score(true, prediction)
    scores.append(score)
    plt.scatter(prediction, true, lw=2, alpha=0.3, 
                label='Fold %d (R2 = %0.2f)' % (i,score))
    i = i+1
plt.plot([min(y_train),max(y_train)], [min(y_train),max(y_train)], 
         linestyle='--', lw=2, color='black')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('K-Fold Validation')
plt.legend()
plt.show()
```

![](img/dba4c2f4694ffd0e27b9a5aeeeea5bd4.png)

çº¿æ€§å›å½’çš„å¹³å‡ R å¹³æ–¹ä¸º 0.77ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æ¢¯åº¦æ¨è¿›éªŒè¯æ˜¯å¦‚ä½•è¿›è¡Œçš„:

![](img/080653e6de5764f9021f36bff3c0e42f.png)

æ¢¯åº¦æ¨è¿›æ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½(å¹³å‡ R å¹³æ–¹ä¸º 0.83)ï¼Œå› æ­¤æˆ‘å°†ä½¿ç”¨å®ƒæ¥é¢„æµ‹æµ‹è¯•æ•°æ®:

```
**## train**
model.**fit**(X_train, y_train)**## test**
predicted = model.**predict**(X_test)
```

è¯·è®°ä½ï¼Œæ•°æ®æ˜¯ç»è¿‡ç¼©æ”¾çš„ï¼Œå› æ­¤ä¸ºäº†å°†é¢„æµ‹å€¼ä¸æµ‹è¯•é›†ä¸­çš„å®é™…æˆ¿ä»·è¿›è¡Œæ¯”è¾ƒï¼Œå®ƒä»¬å¿…é¡»æ˜¯æœªç»ç¼©æ”¾çš„(ä½¿ç”¨é€†å˜æ¢å‡½æ•°):

```
predicted = scalerY.**inverse_transform**( 
                  predicted.reshape(-1,1) ).reshape(-1)
```

## ä¼°ä»·

å…³é”®æ—¶åˆ»åˆ°äº†ï¼Œæˆ‘ä»¬è¦çœ‹çœ‹æ‰€æœ‰è¿™äº›åŠªåŠ›æ˜¯å¦å€¼å¾—ã€‚é‡ç‚¹æ˜¯ç ”ç©¶æ¨¡å‹èƒ½è§£é‡Šå¤šå°‘ Y çš„æ–¹å·®ï¼Œä»¥åŠè¯¯å·®æ˜¯å¦‚ä½•åˆ†å¸ƒçš„ã€‚

æˆ‘å°†ä½¿ç”¨ä»¥ä¸‹å¸¸ç”¨åº¦é‡æ¥è¯„ä¼°è¯¥æ¨¡å‹:R å¹³æ–¹ã€**å¹³å‡ç»å¯¹è¯¯å·®(MAE)** å’Œ**å‡æ–¹æ ¹è¯¯å·®(RMSD)** ã€‚åä¸¤ä¸ªæ˜¯è¡¨è¾¾åŒä¸€ç°è±¡çš„æˆå¯¹è§‚å¯Ÿå€¼ä¹‹é—´çš„è¯¯å·®åº¦é‡ã€‚ç”±äºè¯¯å·®å¯èƒ½æ˜¯æ­£çš„(å®é™…>é¢„æµ‹)ä¹Ÿå¯èƒ½æ˜¯è´Ÿçš„(å®é™…<é¢„æµ‹)ï¼Œæ‚¨å¯ä»¥æµ‹é‡æ¯ä¸ªè¯¯å·®çš„ç»å¯¹å€¼å’Œå¹³æ–¹å€¼ã€‚

![](img/dd879484429b98719eff0235f7c4caf6.png)![](img/93bc620789b7332e503713494bef6ba2.png)

```
**## Kpi**
print("R2 (explained variance):", round(metrics.r2_score(y_test, predicted), 2))
print("Mean Absolute Perc Error (Î£(|y-pred|/y)/n):", round(np.mean(np.abs((y_test-predicted)/predicted)), 2))
print("Mean Absolute Error (Î£|y-pred|/n):", "{:,.0f}".format(metrics.mean_absolute_error(y_test, predicted)))
print("Root Mean Squared Error (sqrt(Î£(y-pred)^2/n)):", "{:,.0f}".format(np.sqrt(metrics.mean_squared_error(y_test, predicted))))**## residuals** residuals = y_test - predicted
max_error = max(residuals) if abs(max(residuals)) > abs(min(residuals)) else min(residuals)
max_idx = list(residuals).index(max(residuals)) if abs(max(residuals)) > abs(min(residuals)) else list(residuals).index(min(residuals))
max_true, max_pred = y_test[max_idx], predicted[max_idx]
print("Max Error:", "{:,.0f}".format(max_error))
```

![](img/a6e8b153f5afd27bc758fb1e483702ea.png)

è¯¥æ¨¡å‹è§£é‡Šäº†ç›®æ ‡å˜é‡ 86%çš„æ–¹å·®ã€‚å¹³å‡è€Œè¨€ï¼Œé¢„æµ‹è¯¯å·®ä¸º 2 ä¸‡ç¾å…ƒï¼Œæˆ–è€…è¯´è¯¯å·®ä¸º 11%ã€‚æµ‹è¯•é›†ä¸Šçš„æœ€å¤§è¯¯å·®è¶…è¿‡ 17 ä¸‡ç¾å…ƒã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»˜åˆ¶é¢„æµ‹å€¼ä¸å®é™…å€¼å’Œæ¯ä¸ªé¢„æµ‹å€¼çš„æ®‹å·®(è¯¯å·®)æ¥å¯è§†åŒ–è¯¯å·®ã€‚

```
**## Plot predicted vs true**
fig, ax = plt.subplots(nrows=1, ncols=2)
from statsmodels.graphics.api import abline_plot
ax[0].scatter(predicted, y_test, color="black")
abline_plot(intercept=0, slope=1, color="red", ax=ax[0])
ax[0].vlines(x=max_pred, ymin=max_true, ymax=max_true-max_error, color='red', linestyle='--', alpha=0.7, label="max error")
ax[0].grid(True)
ax[0].set(xlabel="Predicted", ylabel="True", title="Predicted vs True")
ax[0].legend()

**## Plot predicted vs residuals**
ax[1].scatter(predicted, residuals, color="red")
ax[1].vlines(x=max_pred, ymin=0, ymax=max_error, color='black', linestyle='--', alpha=0.7, label="max error")
ax[1].grid(True)
ax[1].set(xlabel="Predicted", ylabel="Residuals", title="Predicted vs Residuals")
ax[1].hlines(y=0, xmin=np.min(predicted), xmax=np.max(predicted))
ax[1].legend()
plt.show()
```

![](img/ac2daf957bc4739fee460f412bacd5d5.png)

è¿™å°±æ˜¯-170k çš„æœ€å¤§è¯¯å·®:æ¨¡å‹é¢„æµ‹çš„è¯¯å·®çº¦ä¸º 320kï¼Œè€Œå®é™…è§‚æµ‹å€¼çº¦ä¸º 150kã€‚ä¼¼ä¹å¤§å¤šæ•°è¯¯å·®ä½äº 50k å’Œ-50k ä¹‹é—´ï¼Œè®©æˆ‘ä»¬æ›´å¥½åœ°çœ‹çœ‹æ®‹å·®çš„åˆ†å¸ƒï¼Œçœ‹çœ‹å®ƒæ˜¯å¦è¿‘ä¼¼æ­£æ€:

```
fig, ax = plt.subplots()
sns.distplot(residuals, color="red", hist=True, kde=True, kde_kws={"shade":True}, ax=ax)
ax.grid(True)
ax.set(yticks=[], yticklabels=[], title="Residuals distribution")
plt.show()
```

![](img/5ed3be723bec1d7222150e837c5cd608.png)

## å¯è§£é‡Šæ€§

ä½ åˆ†æå¹¶ç†è§£äº†æ•°æ®ï¼Œä½ è®­ç»ƒäº†ä¸€ä¸ªæ¨¡å‹å¹¶æµ‹è¯•äº†å®ƒï¼Œä½ ç”šè‡³å¯¹æ€§èƒ½æ„Ÿåˆ°æ»¡æ„ã€‚ä½ å¯ä»¥å¤šèµ°ä¸€æ­¥ï¼Œè¯æ˜ä½ çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¸æ˜¯ä¸€ä¸ªé»‘ç›’ã€‚

è¿™ä¸ª*çŸ³ç°*åŒ…å¯ä»¥å¸®åŠ©æˆ‘ä»¬å»ºé€ ä¸€ä¸ª**è®²è§£å™¨**ã€‚ä¸ºäº†ä¸¾ä¾‹è¯´æ˜ï¼Œæˆ‘å°†ä»æµ‹è¯•é›†ä¸­éšæœºè§‚å¯Ÿï¼Œçœ‹çœ‹æ¨¡å‹é¢„æµ‹äº†ä»€ä¹ˆ:

```
print("True:", "{:,.0f}".format(y_test[1]), "--> Pred:", "{:,.0f}".format(predicted[1]))
```

![](img/7a4eeef01a8c6c930f529e8234301683.png)

è¯¥æ¨¡å‹é¢„æµ‹è¿™æ ‹æˆ¿å­çš„ä»·æ ¼ä¸º 194ï¼Œ870 ç¾å…ƒã€‚ä¸ºä»€ä¹ˆï¼Ÿè®©æˆ‘ä»¬ä½¿ç”¨è§£é‡Šå™¨:

```
explainer = lime_tabular.LimeTabularExplainer(training_data=X_train, feature_names=X_names, class_names="Y", mode="regression")
explained = explainer.explain_instance(X_test[1], model.predict, num_features=10)
explained.as_pyplot_figure()
```

![](img/0c90c13127259e7175f7e8bc8d39b71b.png)

è¿™ä¸ªç‰¹æ®Šé¢„æµ‹çš„ä¸»è¦å› ç´ æ˜¯æˆ¿å­æœ‰ä¸€ä¸ªå¤§çš„åœ°ä¸‹å®¤(TotalBsmft > 1.3k)ï¼Œå®ƒæ˜¯ç”¨é«˜è´¨é‡çš„ææ–™å»ºé€ çš„(æ€»ä½“è´¨é‡> 6)ï¼Œè€Œä¸”æ˜¯æœ€è¿‘å»ºé€ çš„(å»ºé€ å¹´ä»½> 2001)ã€‚

é¢„æµ‹å€¼ä¸å®é™…å€¼çš„å¯¹æ¯”å›¾æ˜¯æ˜¾ç¤ºæµ‹è¯•è¿›è¡Œæƒ…å†µçš„ä¸€ä¸ªå¾ˆå¥½çš„å·¥å…·ï¼Œä½†æ˜¯æˆ‘ä¹Ÿç»˜åˆ¶äº†**å›å½’å¹³é¢**æ¥ç›´è§‚åœ°å¸®åŠ©è§‚å¯Ÿæ¨¡å‹æ²¡æœ‰æ­£ç¡®é¢„æµ‹çš„å¼‚å¸¸å€¼ã€‚ç”±äºçº¿æ€§æ¨¡å‹æ•ˆæœæ›´å¥½ï¼Œæˆ‘å°†ä½¿ç”¨çº¿æ€§å›å½’æ¥æ‹ŸåˆäºŒç»´æ•°æ®ã€‚ä¸ºäº†ç»˜åˆ¶äºŒç»´æ•°æ®ï¼Œéœ€è¦è¿›è¡Œä¸€å®šç¨‹åº¦çš„é™ç»´(é€šè¿‡è·å¾—ä¸€ç»„ä¸»è¦å˜é‡æ¥å‡å°‘ç‰¹å¾æ•°é‡çš„è¿‡ç¨‹)ã€‚æˆ‘å°†ç»™å‡ºä¸€ä¸ªä¾‹å­ï¼Œä½¿ç”¨ [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) ç®—æ³•å°†æ•°æ®æ€»ç»“ä¸ºä¸¤ä¸ªå˜é‡ï¼Œè¿™äº›å˜é‡æ˜¯é€šè¿‡ç‰¹å¾çš„çº¿æ€§ç»„åˆè·å¾—çš„ã€‚

```
**## PCA**
pca = decomposition.PCA(n_components=2)
X_train_2d = pca.fit_transform(X_train)
X_test_2d = pca.transform(X_test)**## train 2d model**
model_2d = linear_model.LinearRegression()
model_2d.fit(X_train, y_train)**## plot regression plane**
from mpl_toolkits.mplot3d import Axes3D
ax = Axes3D(plt.figure())
ax.scatter(X_test[:,0], X_test[:,1], y_test, color="black")
X1 = np.array([[X_test.min(), X_test.min()], [X_test.max(), 
               X_test.max()]])
X2 = np.array([[X_test.min(), X_test.max()], [X_test.min(), 
               X_test.max()]])
Y = model_2d.predict(np.array([[X_test.min(), X_test.min(), 
                     X_test.max(), X_test.max()], 
                    [X_test.min(), X_test.max(), X_test.min(), 
                     X_test.max()]]).T).reshape((2,2))
Y = scalerY.inverse_transform(Y)
ax.plot_surface(X1, X2, Y, alpha=0.5)
ax.set(zlabel="Y", title="Regression plane", xticklabels=[], 
       yticklabels=[])
plt.show()
```

![](img/29f211b020ad5f116090ac99e99bf171.png)

## ç»“è®º

è¿™ç¯‡æ–‡ç« æ˜¯ä¸€ä¸ªæ•™ç¨‹ï¼Œå±•ç¤ºäº†**å¦‚ä½•ç”¨æ•°æ®ç§‘å­¦å¤„ç†å›å½’ç”¨ä¾‹**ã€‚æˆ‘ä»¥æˆ¿ä»·æ•°æ®é›†ä¸ºä¾‹ï¼Œç»å†äº†ä»æ•°æ®åˆ†æåˆ°æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¯ä¸ªæ­¥éª¤ã€‚

åœ¨æ¢ç´¢éƒ¨åˆ†ï¼Œæˆ‘åˆ†æäº†å•ä¸ªåˆ†ç±»å˜é‡ã€å•ä¸ªæ•°å€¼å˜é‡çš„æƒ…å†µï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•ç›¸äº’ä½œç”¨ã€‚æˆ‘ä¸¾äº†ä¸€ä¸ªä»åŸå§‹æ•°æ®ä¸­æå–ç‰¹å¾çš„ç‰¹å¾å·¥ç¨‹çš„ä¾‹å­ã€‚å…³äºé¢„å¤„ç†ï¼Œæˆ‘è§£é‡Šäº†å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼å’Œåˆ†ç±»æ•°æ®ã€‚æˆ‘å±•ç¤ºäº†é€‰æ‹©æ­£ç¡®ç‰¹æ€§çš„ä¸åŒæ–¹æ³•ï¼Œå¦‚ä½•ä½¿ç”¨å®ƒä»¬æ¥æ„å»ºå›å½’æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•è¯„ä¼°æ€§èƒ½ã€‚åœ¨æœ€åä¸€èŠ‚ï¼Œæˆ‘å°±å¦‚ä½•æé«˜ä½ çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯è§£é‡Šæ€§ç»™å‡ºäº†ä¸€äº›å»ºè®®ã€‚

ä¸€ä¸ªé‡è¦çš„æ³¨æ„äº‹é¡¹æ˜¯ï¼Œæˆ‘è¿˜æ²¡æœ‰ä»‹ç»åœ¨æ‚¨çš„æ¨¡å‹è¢«æ‰¹å‡†éƒ¨ç½²ä¹‹åä¼šå‘ç”Ÿä»€ä¹ˆã€‚è¯·è®°ä½ï¼Œæ‚¨éœ€è¦æ„å»ºä¸€ä¸ªç®¡é“æ¥è‡ªåŠ¨å¤„ç†æ‚¨å°†å®šæœŸè·å¾—çš„æ–°æ•°æ®ã€‚

ç°åœ¨ï¼Œæ‚¨å·²ç»çŸ¥é“å¦‚ä½•å¤„ç†æ•°æ®ç§‘å­¦ç”¨ä¾‹ï¼Œæ‚¨å¯ä»¥å°†è¿™äº›ä»£ç å’Œæ–¹æ³•åº”ç”¨äºä»»ä½•ç±»å‹çš„å›å½’é—®é¢˜ï¼Œæ‰§è¡Œæ‚¨è‡ªå·±çš„åˆ†æï¼Œæ„å»ºæ‚¨è‡ªå·±çš„æ¨¡å‹ï¼Œç”šè‡³è§£é‡Šå®ƒã€‚

æˆ‘å¸Œæœ›ä½ å–œæ¬¢å®ƒï¼å¦‚æœ‰é—®é¢˜å’Œåé¦ˆï¼Œæˆ–è€…åªæ˜¯åˆ†äº«æ‚¨æ„Ÿå…´è¶£çš„é¡¹ç›®ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚

> ğŸ‘‰[æˆ‘ä»¬æ¥è¿çº¿](https://linktr.ee/maurodp)ğŸ‘ˆ

> æœ¬æ–‡æ˜¯ç”¨ Python è¿›è¡Œæœºå™¨å­¦ä¹ ç³»åˆ—**çš„ä¸€éƒ¨åˆ†**ï¼Œå‚è§:

[](/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec) [## ç”¨ Python è¿›è¡Œæœºå™¨å­¦ä¹ :åˆ†ç±»(å®Œæ•´æ•™ç¨‹)

### æ•°æ®åˆ†æå’Œå¯è§†åŒ–ã€ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©ã€æ¨¡å‹è®¾è®¡å’Œæµ‹è¯•ã€è¯„ä¼°å’Œè§£é‡Š

towardsdatascience.com](/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec) [](/clustering-geospatial-data-f0584f0b04ec) [## èšç±»åœ°ç†ç©ºé—´æ•°æ®

### ä½¿ç”¨äº¤äº’å¼åœ°å›¾ç»˜åˆ¶æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ èšç±»

towardsdatascience.com](/clustering-geospatial-data-f0584f0b04ec) [](/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0) [## Python æ·±åº¦å­¦ä¹ :ç¥ç»ç½‘ç»œ(å®Œæ•´æ•™ç¨‹)

### ç”¨ TensorFlow å»ºç«‹ã€ç»˜åˆ¶å’Œè§£é‡Šäººå·¥ç¥ç»ç½‘ç»œ

towardsdatascience.com](/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0) [](/modern-recommendation-systems-with-neural-networks-3cc06a6ded2c) [## åŸºäºç¥ç»ç½‘ç»œçš„ç°ä»£æ¨èç³»ç»Ÿ

### ä½¿ç”¨ Python å’Œ TensorFlow æ„å»ºæ··åˆæ¨¡å‹

towardsdatascience.com](/modern-recommendation-systems-with-neural-networks-3cc06a6ded2c)