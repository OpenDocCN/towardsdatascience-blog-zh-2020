<html>
<head>
<title>Building A Recommendation System for Anime</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建动漫推荐系统</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-recommendation-system-for-anime-566f864acea8?source=collection_archive---------31-----------------------#2020-05-27">https://towardsdatascience.com/building-a-recommendation-system-for-anime-566f864acea8?source=collection_archive---------31-----------------------#2020-05-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="77e5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我已经决定做一个简单的动漫推荐系统。</h2></div><h2 id="7b7a" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">背景资料</h2><p id="9fa8" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">动漫是一种源自日本的手绘电脑动画，在世界各地吸引了大批追随者。动画产业由 430 多家公司组成。口袋妖怪和游戏王是西方电视上最受欢迎的动漫节目。由宫崎骏创作并由吉卜力工作室制作动画的《千与千寻》是动画电影中票房最高的一部。它在西方如此受欢迎的原因是宫崎骏的一个好朋友说服他将发行权卖给华特·迪士尼。像<em class="lx">千与千寻</em>，有数以千计的真正好的动画电影和节目是由同一家动画公司制作的。许多其他人可以以此为例，将其作为将此类艺术作品引入迪士尼+或西方任何流媒体网站的一种方式。这让我想到了我最近一直在做的事情:<strong class="lg iu">一个可以帮助任何人或任何公司查看/添加最高评级动漫的推荐系统。日本贸易振兴机构估计，2004 年该行业的海外销售额为 18 𝑏𝑖𝑙𝑙𝑖𝑜𝑛(仅美国就达 52 亿英镑)。这肯定已经增长，并有潜力进一步增长，尤其是在这个世界上的这段时间。像一些国家一样，日本正面临第一次长期衰退。</strong></p><p id="d009" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">下面，你可以看到我对一个动漫推荐系统的逐步指导。这将有助于解决上述问题，并可以创造更多的需求动漫。它还可以帮助任何不熟悉小众流派的人快速找到收视率最高的项目。(帮助发展市场)。</p><p id="5cda" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我已经从<a class="ae md" href="https://www.kaggle.com/CooperUnion/anime-recommendations-database" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/Cooper union/anime-recommendations-database</a>下载了动漫和用户评分。我将进行探索性的数据分析，让读者和我自己熟悉所呈现的数据。在那里，我使用奇异值分解(SVD)创建了一个基线模型。然后，我会做基于记忆的模型，将着眼于基于用户的 v 项目。我将使用 KNNBase、KNNBaseline 和 KNNWithMeans。然后我会选择表现最好的模型，评估其均方根误差(rmse)和平均绝对误差(mae)。</p><h2 id="fcb8" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">导入</h2><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="aaf8" class="ki kj it mj b gy mn mo l mp mq">import pandas as pd<br/>import numpy as np</span><span id="dc07" class="ki kj it mj b gy mr mo l mp mq">import random<br/>from random import randint</span><span id="1c98" class="ki kj it mj b gy mr mo l mp mq">import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import seaborn as sns</span><span id="1dcf" class="ki kj it mj b gy mr mo l mp mq">from scipy.sparse import csc_matrix<br/>from scipy.sparse.linalg import svds</span><span id="08af" class="ki kj it mj b gy mr mo l mp mq">from surprise.model_selection import train_test_split</span><span id="411c" class="ki kj it mj b gy mr mo l mp mq">from surprise.model_selection import GridSearchCV<br/>from surprise.model_selection import cross_validate</span><span id="41ab" class="ki kj it mj b gy mr mo l mp mq">from surprise.prediction_algorithms import KNNWithMeans, KNNBasic, KNNBaseline</span><span id="9b10" class="ki kj it mj b gy mr mo l mp mq">from surprise.prediction_algorithms import knns<br/>from surprise.prediction_algorithms import SVD</span><span id="acfe" class="ki kj it mj b gy mr mo l mp mq">from surprise.similarities import cosine, msd, pearson</span><span id="1b0f" class="ki kj it mj b gy mr mo l mp mq">from surprise import accuracy</span><span id="e7e4" class="ki kj it mj b gy mr mo l mp mq">from surprise import Reader<br/>from surprise import Dataset</span></pre><p id="2e40" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">在这个项目的大部分时间里，我决定坚持上面的方法，而且效果很好。我已经在 google colab 中尝试了下面的其他模型，我将在那里为导入库添加必要的代码。</p><h2 id="a9c5" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">擦洗/清洁</h2><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9e8f" class="ki kj it mj b gy mn mo l mp mq">anime_df = pd.read_csv('./anime.csv')<br/>anime_df.head()</span></pre><p id="1813" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">形状看起来像什么？这很重要，因为它有助于查看任何空值</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="8563" class="ki kj it mj b gy mn mo l mp mq">anime_df.shape</span></pre><p id="5ae0" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">删除空值(如果存在)</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="2425" class="ki kj it mj b gy mn mo l mp mq">anime_df.dropna(inplace=True)</span></pre><p id="be67" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我们将再次检查上述命令后的形状</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4d8e" class="ki kj it mj b gy mn mo l mp mq">anime_df.shape #this seemed to have reduced it down a bit</span></pre><p id="4bbb" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">这实际上减少了我们的动漫数据框架！</p><p id="45ae" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">在清理时，显示每一列代表什么是很重要的，这样我们就可以知道什么可以擦洗、转换，或者我们是否需要进行一些功能工程。</p><h2 id="3a0d" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">每列代表什么:</h2><p id="aede" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated"><strong class="lg iu"> <em class="lx"> anime_id:每部动漫的 id 号片名</em> </strong> <br/> <strong class="lg iu"> <em class="lx">名称:电影片名</em> </strong> <br/> <strong class="lg iu"> <em class="lx">类型:类别</em> </strong> <br/> <strong class="lg iu"> <em class="lx">类型:描述动漫分成电视、电影、OVA 等 3 个类别</em> </strong> <br/> <strong class="lg iu">集数:总集数</strong> <br/> <strong class="lg iu"> <em class="lx">评分:-1-10，最低到</em></strong></p><p id="06e6" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">接下来，它们和上面的匹配吗？</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="f732" class="ki kj it mj b gy mn mo l mp mq">anime_df.info() #having a look at all of the columns and types from the above cell and current to remove</span><span id="9638" class="ki kj it mj b gy mr mo l mp mq">#any unneccessary extraneous data</span></pre><p id="8718" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">让我们看看这个推荐项目的第二个 csv 文件。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5fd9" class="ki kj it mj b gy mn mo l mp mq">rating_df = pd.read_csv('./rating.csv')<br/>rating_df.head()</span></pre><p id="5d81" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">让我们看看他们代表了什么。</p><h2 id="1367" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">每列代表什么:</h2><p id="d8d5" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated"><strong class="lg iu"> user_id:不可识别的随机生成的 user-id </strong> <br/> <strong class="lg iu"> anime_id:用户已评级的动漫</strong> <br/> <strong class="lg iu">评级:该用户已分配的 10 分中的评级(如果用户观看了该动漫但未分配评级，则为-1)</strong></p><p id="4e9f" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">检查评级的形状</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="c800" class="ki kj it mj b gy mn mo l mp mq">rating_df.shape</span></pre><p id="fd95" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">都有 anime_id。让我们将这两者结合起来，使事情变得容易得多。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d087" class="ki kj it mj b gy mn mo l mp mq">df_merge = pd.merge(anime_df, rating_df, on = 'anime_id')<br/>df_merge.head()</span></pre><p id="3ff8" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">让我们再次检查形状。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0c86" class="ki kj it mj b gy mn mo l mp mq">df_merge.shape</span></pre><p id="3154" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我艰难地认识到奇异值分解(SVD)对如此大的数据集更敏感。我要看看最低评级。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4a42" class="ki kj it mj b gy mn mo l mp mq">df_merge.rating_x.min()</span></pre><p id="7a1a" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我已经决定去掉没有评级的列，它们表示为-1。在做推荐系统时，这取决于个人、任务和公司。我可能会在我的业余时间回去，把这个加回来，看看未完成的评级和它对推荐的影响。(确实起了巨大的作用)。我决定把它拿出来纯粹是为了娱乐。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="32f5" class="ki kj it mj b gy mn mo l mp mq">df_merge = df_merge[df_merge.rating_y != -1]<br/>df_merge.head()</span></pre><p id="4dd7" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">现在，我再次检查形状，看看这减少了多少 SVD 数据。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1a3d" class="ki kj it mj b gy mn mo l mp mq">df_merge.shape #have removed over 1 million rows</span></pre><p id="8283" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">这对于免费的 google colab 来说仍然不够小。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e0aa" class="ki kj it mj b gy mn mo l mp mq">sample = df_merge.sample(frac=.25)<br/>sample.shape # this is still too large</span></pre><p id="63e2" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我在我的 SVD 上运行了很多测试(这花了一天半的时间，我最终不得不满足于下面的样本大小)。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="c585" class="ki kj it mj b gy mn mo l mp mq">sample = df_merge.sample(n=5000)</span><span id="68a8" class="ki kj it mj b gy mr mo l mp mq">sample.shape #below I conduct SVD and it cannot handle anything larger than 5000 (i've tried)</span></pre><p id="9144" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">让我们看看数据类型</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="14ef" class="ki kj it mj b gy mn mo l mp mq">sample.dtypes #rating_x needs to be an int, for it to work in ALS</span></pre><p id="4129" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">让我们在这里转换它们</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="7a26" class="ki kj it mj b gy mn mo l mp mq">sample['rating_x'] = sample['rating_x'].astype(int)</span></pre><p id="aa0b" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">可以在这里检查数据类型。从一开始，我就发现检查我的代码是非常重要的。当有调试问题时，它对消除问题很有帮助。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="fffa" class="ki kj it mj b gy mn mo l mp mq">sample.dtypes</span></pre><p id="af27" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">看起来评级数据框架是基于每个用户和他们对每个动画 id 的个人评级，而动画数据框架是来自其所有观众的平均总体评级。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4163" class="ki kj it mj b gy mn mo l mp mq">#we are going to look at how many times each rating appears in a column</span><span id="0b6f" class="ki kj it mj b gy mr mo l mp mq">ratings_x = sample['rating_x'].value_counts() #continuous<br/>ratings_y = sample['rating_y'].value_counts() #discrete</span><span id="a3a6" class="ki kj it mj b gy mr mo l mp mq">print(ratings_x)<br/>print(ratings_y)</span></pre><p id="ac8a" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">另一件对 SVD 来说非常重要的事情是让变量变得谨慎，否则，它会占用你更多的时间。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="da62" class="ki kj it mj b gy mn mo l mp mq">sample.rating_x = sample.rating_x.apply(round) <br/>sample.head()</span></pre><p id="0ece" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">太好了，我想我已经洗完了。让我们进入有趣的部分！</p><h2 id="4596" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">探索性数据分析</h2><p id="a2b8" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">用户评级的分布</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5121" class="ki kj it mj b gy mn mo l mp mq"># plot distribution in matplotlib<br/>ratings_sorted = sorted(list(zip(ratings_y.index, ratings_y)))<br/>plt.bar([r[0] for r in ratings_sorted], [r[1] for r in ratings_sorted], color='cyan')<br/>plt.xlabel("Rating")<br/>plt.ylabel("# of Ratings")<br/>plt.title("Distribution of Ratings")<br/>plt.show()</span></pre><p id="2114" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我还没有把输出添加到清洗中，视觉效果非常重要，所以我会把它们添加进去。</p><figure class="me mf mg mh gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ms"><img src="../Images/7b0f895dd3da25ecaec3fc7d2f2e4045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYEOUjmDcJh1zxvc-MTpkA.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">这是我自己的图表，也可以在我的 github 上找到</p></figure><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="7d50" class="ki kj it mj b gy mn mo l mp mq">#number of users<br/>print("Number of Users:", df_merge.user_id.nunique()</span><span id="1515" class="ki kj it mj b gy mr mo l mp mq"># print("Average Number of Reviews per User:", df_merge.shape[0])/df_merge.user_id.nunique()</span><span id="a475" class="ki kj it mj b gy mr mo l mp mq">avg_rate_peruser = df_merge.shape[0]</span><span id="be0c" class="ki kj it mj b gy mr mo l mp mq">user = df_merge.user_id.nunique()<br/>avg_num_review_per_user = avg_rate_peruser/user</span><span id="94a1" class="ki kj it mj b gy mr mo l mp mq">print("Average Number of Reveiws per User:", avg_num_review_per_user)</span></pre><p id="7d5e" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">用户数量:15382</p><p id="8052" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">每个用户的平均评论数:88.69</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9f5b" class="ki kj it mj b gy mn mo l mp mq">sample[‘user_id’].value_counts()</span></pre><p id="b91d" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">5000 的样本量，给了我们总共 3，381 个做过评论的用户。以上是整套的。</p><p id="5748" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">每个用户的评论数量</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="38a2" class="ki kj it mj b gy mn mo l mp mq">ratings_per_user = sample['user_id'].value_counts()<br/>ratings_per_user = sorted(list(zip(ratings_per_user.index, ratings_per_user)))<br/>plt.bar([r[0] for r in ratings_per_user], [r[1] for r in ratings_per_user], color='pink')<br/>plt.xlabel('User IDs')<br/>plt.ylabel('# of Reviews')<br/>plt.title('Number of Reviews per User')<br/>plt.show()</span></pre><figure class="me mf mg mh gt mt gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/0994195a59e16e35f5311cdf353a593d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*q8aBj1VubNVuMn4kNHKA_Q.png"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">这是我自己的图表，也可以在我的 github 上找到</p></figure><p id="c679" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">从我们的样本集来看，给出的很多分数都是 1 分和 2-4 分。</p><p id="5741" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">不同类型的动漫</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="771d" class="ki kj it mj b gy mn mo l mp mq">print("Number of users:", sample.user_id.nunique())<br/>print("Number of types of different anime:", sample.type.nunique())<br/>print("Types of type:", sample.type.value_counts())</span></pre><p id="f556" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">由此，我们可以看到有 6 种类型。它们是什么？</p><p id="bfae" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">6 种类型:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4448" class="ki kj it mj b gy mn mo l mp mq">TV 3492 <br/>Movie 666<br/>OVA 461<br/>Special 314 <br/>ONA 46 <br/>Music 21 <br/>Name: type, dtype: int64</span></pre><p id="30e1" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">OVA 代表原创视频动画</p><p id="33c6" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">ONA 代表原创网络动画</p><p id="9dcd" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">特价是一次性的视频。</p><p id="43bb" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">音乐是以动漫为主题的，通常有一个动画与之搭配，但它通常是一个非常短的视频。</p><p id="3201" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">点击率最高的动漫</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="2158" class="ki kj it mj b gy mn mo l mp mq"># PLOT them<br/>fig = plt.figure(figsize=(12,10))<br/>sns.countplot(sample['type'], palette='gist_rainbow')<br/>plt.title("Most Viewed Anime", fontsize=20)<br/>plt.xlabel("Types", fontsize=20)<br/>plt.ylabel("Number of Views with Reviews", fontsize = 20)<br/>plt.legend(sample['type'])<br/>plt.show()</span></pre><figure class="me mf mg mh gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nf"><img src="../Images/e3cef5445c67c00ea5fe01597e58755a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*42d9Uk-qeNP-3NpZzlfImQ.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">这是我自己的图表，也可以在我的 github 上找到</p></figure><p id="71b4" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">让我们开始实施吧！</p><h2 id="7994" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">分析</h2><p id="43c2" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">最重要的是第一件事！做一个基础模型。我从转换我的数据开始。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="135b" class="ki kj it mj b gy mn mo l mp mq">#for surprise, it likes its data in a certain way and only that specific data</span><span id="dc60" class="ki kj it mj b gy mr mo l mp mq">data = sample[['user_id', 'anime_id', 'rating_x']] #may need to do rating_x rounded and then use rating_y</span><span id="b648" class="ki kj it mj b gy mr mo l mp mq">reader = Reader(line_format='user item rating', sep='')<br/>anime_loaded_data = Dataset.load_from_df(data, reader)</span><span id="40b3" class="ki kj it mj b gy mr mo l mp mq">#train_test_split<br/>trainset, testset = train_test_split(anime_loaded_data, test_size=.2)</span></pre><p id="a917" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">确保您的数据格式正确</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5bf3" class="ki kj it mj b gy mn mo l mp mq">anime_loaded_data</span></pre><p id="2e03" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">接下来，我们将实例化。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="959e" class="ki kj it mj b gy mn mo l mp mq">#INSTANTIATE the SVD and fit only the train set<br/>svd = SVD()</span><span id="9713" class="ki kj it mj b gy mr mo l mp mq">svd.fit(trainset)</span></pre><p id="e2f3" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">现在来看看预测和准确性。这对于比较您的学员模型非常重要。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1f2f" class="ki kj it mj b gy mn mo l mp mq">predictions = svd.test(testset) #<br/>accuracy.rmse(predictions)</span></pre><p id="9438" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">这是我的基线:RMSE: 2.3128，2.3127</p><p id="eae6" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">嗯，肯定不是 0 到 1 之间。这些分数实际上取决于领域。肯定不完美。我最初的反应是吓坏了，但事实证明 RMSE 超过 1 是没问题的。</p><p id="46fd" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我想看看我是否能减少这种情况。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="f1de" class="ki kj it mj b gy mn mo l mp mq">#perform a gridsearch CV<br/>params = {'n_factors': [20,50,100],<br/>'reg_all': [.02,.05, .10]}</span><span id="5719" class="ki kj it mj b gy mr mo l mp mq">gridsearch_svd1 = GridSearchCV(SVD, param_grid=params, n_jobs=-1, joblib_verbose=3)</span><span id="887b" class="ki kj it mj b gy mr mo l mp mq">gridsearch_svd1.fit(anime_loaded_data)</span><span id="fd4b" class="ki kj it mj b gy mr mo l mp mq">print(gridsearch_svd1.best_score)<br/>print(gridsearch_svd1.best_params)</span></pre><p id="557f" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">{'rmse': 2.3178，' Mae ':2.2080 } { ' RMSE ':{ ' n _ factors ':20，' reg_all': 0.02}，' mae': {'n_factors': 20，' reg_all': 0.02}}</p><p id="288e" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">它增加了。我现在将尝试其他模型，特别是基于内存的模型，然后是基于内容的模型。</p><p id="4788" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">k-最近邻(KNN)基本算法</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="fe82" class="ki kj it mj b gy mn mo l mp mq">#cross validate with KNNBasic<br/>knn_basic = KNNBasic(sim_options={'name':'pearson', 'user_based':True}, verbose=True)<br/>cv_knn_basic = cross_validate(knn_basic, anime_loaded_data, n_jobs=2)</span><span id="202e" class="ki kj it mj b gy mr mo l mp mq">for i in cv_knn_basic.items():</span><span id="c980" class="ki kj it mj b gy mr mo l mp mq">print(i)<br/>print('-----------------')<br/>print(np.mean(cv_knn_basic['test_rmse']))</span></pre><p id="2acd" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">将把你从其余的中拯救出来，这里是输出的一个简短剪辑</p><p id="97c5" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi">— — — — — — — — — 2.3178203641229667</p><p id="dca3" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">您可以对均方距离(msd)进行同样的操作。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="aad3" class="ki kj it mj b gy mn mo l mp mq">knn_basic_msd = KNNBasic(sim_options = {'name': 'msd', 'user-based':True})<br/>cv_knn_basic_msd = cross_validate(knn_basic_msd, anime_loaded_data, n_jobs=2)</span><span id="699d" class="ki kj it mj b gy mr mo l mp mq">for i in cv_knn_basic_msd.items():</span><span id="f59f" class="ki kj it mj b gy mr mo l mp mq">print(i)<br/>print('-----------------')<br/>print(np.mean(cv_knn_basic_msd['test_rmse']))</span></pre><p id="a1a1" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">— — — — — — 2.31787540672289，得分较高。让我们试试另一个模型</p><p id="1264" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">KNNBaseline</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="873d" class="ki kj it mj b gy mn mo l mp mq">#cross validate with KNN Baseline (pearson)<br/>knn_baseline = KNNBaseline(sim_options={'name': 'pearson', 'user_based':True})<br/>cv_knn_baseline = cross_validate(knn_baseline, anime_loaded_data, n_jobs=3)</span><span id="5e7d" class="ki kj it mj b gy mr mo l mp mq">for i in cv_knn_baseline.items():</span><span id="1527" class="ki kj it mj b gy mr mo l mp mq">print(i)<br/>print('-----------------')<br/>print(np.mean(cv_knn_baseline['test_rmse']))</span></pre><p id="309c" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi">— — — — — — — — — 2.317895626569356</p><p id="9957" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">同样，当我们希望它减少时，它却在增加。</p><p id="e599" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">KNN 基线与皮尔逊 _ 基线</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="48df" class="ki kj it mj b gy mn mo l mp mq">knn_pearson_baseline = KNNBaseline(sim_options={'name': 'pearson_baseline', 'user_based':True})</span><span id="e8d3" class="ki kj it mj b gy mr mo l mp mq">cv_knn_pearson_baseline = cross_validate(knn_pearson_baseline, anime_loaded_data, n_jobs=3)</span><span id="0901" class="ki kj it mj b gy mr mo l mp mq">for i in cv_knn_pearson_baseline.items():</span><span id="4801" class="ki kj it mj b gy mr mo l mp mq">print(i)<br/>print('-------------------')<br/>print(np.mean(cv_knn_pearson_baseline['test_rmse']))</span></pre><p id="8af7" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">这给了我们----- 2。46860 . 68868886861</p><p id="5e42" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">KNNWithMeans</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="8981" class="ki kj it mj b gy mn mo l mp mq">knn_means = KNNWithMeans(sim_options={'name': 'pearson', 'user_based': True})<br/>cv_knn_means = cross_validate(knn_means, anime_loaded_data, n_jobs=3)</span><span id="97e8" class="ki kj it mj b gy mr mo l mp mq">for i in cv_knn_means.items():</span><span id="9173" class="ki kj it mj b gy mr mo l mp mq">print(i)<br/>print('------------')<br/>print(np.mean(cv_knn_means['test_rmse']))</span></pre><p id="0a05" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi">— — — — — — 2.3185632763331805</p><p id="abfc" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">SVD 基线似乎具有最低的 RMSE。我会再次进行网格搜索</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="18dc" class="ki kj it mj b gy mn mo l mp mq">param_grid = {'n_factors': [5, 20, 100],<br/>'n_epochs': [5,10],<br/>'lr_all': [.002, .005],<br/>'reg_all': [.02, .05, .5]}</span><span id="baac" class="ki kj it mj b gy mr mo l mp mq">svd_gs = GridSearchCV(SVD, param_grid=param_grid, n_jobs=3, joblib_verbose=3)</span><span id="ed44" class="ki kj it mj b gy mr mo l mp mq">svd_gs.fit(anime_loaded_data)</span><span id="b2fe" class="ki kj it mj b gy mr mo l mp mq">print(svd_gs.best_score)<br/>print(svd_gs.best_params)</span></pre><p id="b291" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">然后</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1839" class="ki kj it mj b gy mn mo l mp mq">#Now use this to fit test set, initial gridsearch was 2.77096, so will use that gs here</span><span id="567e" class="ki kj it mj b gy mr mo l mp mq">highest_perf_algo = gridsearch_svd1.best_estimator['rmse']</span><span id="3884" class="ki kj it mj b gy mr mo l mp mq">#retrain the whole set</span><span id="93da" class="ki kj it mj b gy mr mo l mp mq">trainset = anime_loaded_data.build_full_trainset()<br/>highest_perf_algo.fit(trainset)</span><span id="3b56" class="ki kj it mj b gy mr mo l mp mq">#Biased Accuracy on trainset<br/>predictions = highest_perf_algo.test(trainset.build_testset())</span><span id="9eff" class="ki kj it mj b gy mr mo l mp mq">print('Biased accuracy on Trainset', end='')<br/>accuracy.rmse(predictions)</span><span id="6648" class="ki kj it mj b gy mr mo l mp mq">#UnBiased Accuracy on testset<br/>predictions = highest_perf_algo.test(testset)</span><span id="7d11" class="ki kj it mj b gy mr mo l mp mq">print('Unbiased Accuracy on test', end='')<br/>accuracy.rmse(predictions)</span></pre><p id="1c58" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我的结果是:训练集上的有偏精度 RMSE:2.3179301111112067，测试集上的无偏精度 RMSE:2.317938596</p><p id="4a5e" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我做了一些合作模型，列在我的<a class="ae md" href="https://github.com/anilaq/capstone/blob/master/latest.ipynb" rel="noopener ugc nofollow" target="_blank"> Github repo </a>上。我的协作模型采用余弦相似度和基于用户的 v 项目。</p><p id="37b0" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">基于内容</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1859" class="ki kj it mj b gy mn mo l mp mq">import nltk<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from nltk.corpus import stopwords<br/>import string<br/>from nltk import word_tokenize, FreqDist<br/>import re<br/>from sklearn.decomposition import TruncatedSVD<br/>from sklearn.metrics.pairwise import linear_kernel</span></pre><p id="9157" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">接下来，让我们提取必填字段。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e906" class="ki kj it mj b gy mn mo l mp mq">genre_tag = sample.loc[:, ['anime_id','name','genre']]</span></pre><p id="222d" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">让我们看看它是什么样子的</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="2b88" class="ki kj it mj b gy mn mo l mp mq">genre_tag.head()</span></pre><p id="ada8" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">目标是将每行的单词分开。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="cad1" class="ki kj it mj b gy mn mo l mp mq">tags = {}<br/>for col in ['genre']:<br/>  for a_id in sample['name'].unique(): <br/>    for i in sample[sample['name'] == a_id][col]:<br/>     if a_id in tags: <br/>       tags[a_id].append(' '.join(i.lower().split('|')))<br/>     else: <br/>         tags[a_id] = i.lower().split('|')</span></pre><p id="a9c4" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">将此转换成列表</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="ff0a" class="ki kj it mj b gy mn mo l mp mq">tags_list = list(tags.values())</span></pre><p id="c802" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">看看名单</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e84e" class="ki kj it mj b gy mn mo l mp mq">tags_list[:5]</span></pre><p id="f654" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">确保指定停用词为英语</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d8bc" class="ki kj it mj b gy mn mo l mp mq">stopwords_list = stopwords.words('english')<br/>stopwords_list += list(string.punctuation)</span></pre><p id="22f2" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">目的是减少产生的噪音。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="98d2" class="ki kj it mj b gy mn mo l mp mq">def process_article(article): <br/>  article = ' '. join(article)<br/>tokens = word_tokenize(article)<br/>tokens_2 = []<br/>for token in tokens: <br/>  if token.lower() not in stopwords_list: <br/>     tokens_2.append(token.lower())<br/>return tokens_2</span></pre><p id="6ea0" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">让我们来处理这篇文章。这将把测试分成基于空格或撇号分隔的字符串列表</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="dc4e" class="ki kj it mj b gy mn mo l mp mq">processed_tags = list(map(process_article, tags_list))</span></pre><p id="8536" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">现在让我们从体裁中获取总词汇量。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="8dde" class="ki kj it mj b gy mn mo l mp mq">articles_concat = []<br/>for doc in processed_tags: <br/>  articles_concat += doc</span></pre><p id="ddf8" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">每个单词出现多少次？</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="10aa" class="ki kj it mj b gy mn mo l mp mq">freqdist = FreqDist(articles_concat)<br/>freqdist</span></pre><p id="da56" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">让我们获得前 20 个单词，这将让我们看到最受欢迎的类型</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="c656" class="ki kj it mj b gy mn mo l mp mq">most_common_genre = freqdist.most_common(20)<br/>most_common_genre</span></pre><p id="7ddd" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">让我们画出来</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="50b7" class="ki kj it mj b gy mn mo l mp mq">plt.figure(figsize=(22,12))<br/>plt.bar([word[0] for word in most_common_genre], [word[1] for word in most_common_genre])<br/>plt.title("Most Popular Genre", fontsize= 20)<br/>plt.xlabel("Genre", fontsize=20)<br/>plt.ylabel("Total Number of Times Genre is Present in Anime Data", fontsize=20)<br/>plt.show()</span></pre><figure class="me mf mg mh gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ng"><img src="../Images/bcc4f12a87f5ce339d417de923f5ae60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nZBV_LTNPm57QDOmmQndNA.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">这是我自己的图表，也可以在我的 github 上找到</p></figure><p id="8156" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我们的推荐系统可能会推荐喜剧、动作片、爱情片等。</p><p id="9a2a" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">现在，把它转换成矢量器</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0bda" class="ki kj it mj b gy mn mo l mp mq">vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), stop_words=stopwords_list)</span><span id="77d8" class="ki kj it mj b gy mr mo l mp mq">tags_list_2 = [‘ ‘.join(x) for x in tags_list]<br/>tf_idf_data_train = vectorizer.fit_transform(tags_list_2)</span></pre><p id="e6d9" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">接下来，让我们看看它如何符合标准。因为 NLTK 是一个完全不同的库，所以最好看看它解释的方差比。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="0265" class="ki kj it mj b gy mn mo l mp mq">#instantiate SVD<br/>svd = TruncatedSVD(n_components=500, n_iter=7, random_state=42)</span><span id="9755" class="ki kj it mj b gy mr mo l mp mq">#fit and transform the vectorized tf-idf matrix<br/>tf_idf_data_train_svd = svd.fit_transform(tf_idf_data_train)</span></pre><p id="a3ec" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">这里我们通过以下方式得到最终结果:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="4744" class="ki kj it mj b gy mn mo l mp mq">print(svd.explained_variance_ratio_.sum())</span></pre><p id="fdb7" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi">0.9962558733287571</p><p id="11f5" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">在尝试了一堆模式之后，我觉得基于内容的模式是最好的。</p><p id="6e62" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">在我的笔记本上，我还看着计算动漫和 TF-IDF 的相似度。</p><h2 id="9bae" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">更多信息</h2><p id="0891" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">基于所做的建模，最好是对来自相同动画工作室、作家、导演和媒体公司的收视率最高和收视率最高的动画进行更多的统计分析。从这里，我们可以调整推荐系统，以包括他们的最新发布。</p><p id="6f97" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">另一个建议是看时间成分分析。电视节目是最受欢迎的动画类型，但是在一年中的什么时候呢？从这里开始，推荐系统可以帮助动画工作室填补财政年度内的空白。他们可以将日期分散开来，或者开辟一条新的渠道来推出更多特价商品。</p><p id="36a0" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">参考资料:</p><p id="94fb" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">[1]阿·库雷西，<a class="ae md" href="https://github.com/anilaq/capstone" rel="noopener ugc nofollow" target="_blank">卡普斯顿</a> e (2020)</p><p id="67b9" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">[2]苏珊·李，<a class="ae md" rel="noopener" target="_blank" href="/building-and-testing-recommender-systems-with-surprise-step-by-step-d4ba702ef80b">，</a> (2018)</p><p id="6f98" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">[3] Derrick Mwiti，<a class="ae md" rel="noopener" target="_blank" href="/how-to-build-a-simple-recommender-system-in-python-375093c3fb7d">如何用 Python 构建一个简单的推荐系统</a> (2018)</p><p id="0a99" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">[4] Robi56，<a class="ae md" href="https://github.com/robi56/Deep-Learning-for-Recommendation-Systems" rel="noopener ugc nofollow" target="_blank">深度学习推荐系统</a> (2020)</p><p id="e3cd" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">[5] JJ、梅和 RMSE — <a class="ae md" href="https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d" rel="noopener">哪个指标更好？</a> (2016)</p><p id="da23" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">[6]维基百科，<a class="ae md" href="https://en.wikipedia.org/w/index.php?title=Anime&amp;action=history" rel="noopener ugc nofollow" target="_blank">动漫</a> (2020)</p><p id="5e30" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">[7]维基百科，<a class="ae md" href="https://en.wikipedia.org/wiki/Spirited_Away" rel="noopener ugc nofollow" target="_blank">千与千寻</a> (2020)</p><p id="0232" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">[8]玛丽·马瓦德，<a class="ae md" href="https://sifted.eu/articles/streaming-startups-coronavirus/" rel="noopener ugc nofollow" target="_blank">冠状病毒:欧洲流媒体服务也在蓬勃发展</a> (2020)</p></div></div>    
</body>
</html>