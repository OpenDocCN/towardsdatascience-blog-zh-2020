<html>
<head>
<title>Logistic Regression for Classification Task</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类任务的逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-for-classification-task-f143a5a67785?source=collection_archive---------56-----------------------#2020-07-12">https://towardsdatascience.com/logistic-regression-for-classification-task-f143a5a67785?source=collection_archive---------56-----------------------#2020-07-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="deb9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">手写数字识别任务</h2></div><p id="2a27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于模式识别方法，有许多方法来实现手写数字识别任务。在我之前的<a class="ae le" rel="noopener" target="_blank" href="/handwritten-khmer-digit-recognition-860edf06cd57">故事</a>中，我已经介绍了基于高斯模型最大似然估计的线性判别分析。在这篇文章中，我将逻辑回归模型应用于英文数字手写数字识别任务。</p><h1 id="a3e0" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">逻辑模型</h1><p id="d362" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在逻辑回归模型中，事件的发生概率由逻辑函数表示。例如，在两类问题中，通常使用逻辑 sigmoid 函数。在多类问题中，已知 softmax 函数提供良好的性能。</p><p id="6301" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我们介绍了如何后验概率可以表示在逻辑函数。接下来，我们暗示每一个类别到输入模式的线性变换，并估计它们的后验概率。</p><h2 id="0b8f" class="mc lg it bd lh md me dn ll mf mg dp lp kr mh mi lr kv mj mk lt kz ml mm lv mn bi translated">用逻辑函数模拟后验概率</h2><p id="c35a" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">考虑类别数量为 c 的分类任务。使用最大后验概率规则，输入模式 x 的输出类别是类别 y，其中 p(y|x)最大。后验概率 p(y=i|x)为</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/37514c2ace2dcdb8e50514b3bb90294b.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*xyZsqqzoV_gHGtn4xjTXwg.png"/></div></figure><p id="6c80" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，让我们</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/c10ebb042a3f6d24c91356d5b6fb7c21.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*V7LzXgZt5BQV3l4eRLccwg.png"/></div></figure><p id="528d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/acb2a07bd36bc792a9a3998850872fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*e-ci4zZPMkNmrORfw3bd7g.png"/></div></figure><p id="48ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">softmax()被称为 softmax 函数。当 C=2 时，称为逻辑 sigmoid 函数。从(3)中，我们可以确认后验概率可以用逻辑函数来表示。</p><h2 id="fba9" class="mc lg it bd lh md me dn ll mf mg dp lp kr mh mi lr kv mj mk lt kz ml mm lv mn bi translated">后验概率估计</h2><p id="1f31" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在多类任务中，我们使用一个 softmax 函数，p(y=i|x)=softmax(a_i)。为了估计这个概率，我们通过将输入模式线性变换成第 I 类来估计 a_i。这意味着，</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/4f8e4eaeddd2701b0e6aa289889bcd3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*5wQLvcsx7GB2pUCE7BTKYg.png"/></div></figure><p id="4934" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中参数 w_i 由最大似然法估计。</p><p id="5164" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，后验概率 p(y=i|x)是</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/15872f8f5966b779ff91a201d8cd1ff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*3vH1A7Ql0u7fr98csH-e-w.png"/></div></figure><p id="33f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">设 C 为类别数，{(x_i，y_i)} (i=1，…，N)为样本数据集。似然函数是</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/133f203f5c41a98926d8188462fa3b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*i3Xrks4x9a2eqcGiyW7Jzg.png"/></div></figure><p id="40ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在哪里</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/700202bb8b920047cb5accafa92e8612.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*sPpOHuxsawmFNSJSiW2XnA.png"/></div></figure><p id="a699" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对数似然可以写成</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/beafd1e1c7350ccf070a7eb1d25b2e82.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*ko24OWf7lPRYt4VCNmWGjQ.png"/></div></figure><p id="1cbb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了定义最佳参数 w_j，我们计算对数似然函数的梯度。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/1a872a928bd3a13b3c2c6e5b67989271.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*7h0fylc3ig_X1SMbY3aTwA.png"/></div></figure><p id="2cd1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，解析解这个方程是不可能的。作为解决方案，我们应用梯度上升学习算法来最大化对数似然函数。学习算法是</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/3d2ae8b28721381c263f424b44262a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*U2uqdoDByvE22gsM8W2DDg.png"/></div></figure><h1 id="73fd" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">用 Python 实现</h1><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="83ed" class="mc lg it my b gy nc nd l ne nf">from scipy.special import softmax</span><span id="e4c7" class="mc lg it my b gy ng nd l ne nf"><strong class="my iu">class</strong> <strong class="my iu">LogisticRegression</strong>(object):<br/>    <em class="nh">"""logistic regression classifier with gradient ascent learning method</em><br/><em class="nh">    """</em><br/>    <strong class="my iu">def</strong> __init__(self, lr=0.01, iteration=100, C=10):<br/>        self.lr = lr<br/>        self.iteration=iteration<br/>        self.C = C<br/>    <br/>    <strong class="my iu">def</strong> fit(self, x, y):<br/>        self.w = np.zeros((self.C,1+x.shape[1]))<br/>        <strong class="my iu">for</strong> i <strong class="my iu">in</strong> range(self.C):<br/>            self.w[i,:] = np.random.normal(loc=0.0, scale=0.01, size=1+x.shape[1])<br/>        <em class="nh">#self.energy = []</em><br/>        <br/>        <strong class="my iu">for</strong> i <strong class="my iu">in</strong> range(self.iteration):<br/>            inp = self.getInput(x)<br/>            out = self.getOutput(inp)<br/>            error = y-out<br/>            self.w[:,1:] += self.lr*np.dot(error.T,x)<br/>            self.w[:,0] += self.lr*np.sum(error)<br/>        <strong class="my iu">return</strong> self<br/>    <br/>    <strong class="my iu">def</strong> softmax(self, a):<br/>        out = softmax(a,axis=1)<br/>        <strong class="my iu">return</strong> out<br/>    <br/>    <strong class="my iu">def</strong> getInput(self, x):<br/>        out = np.dot(x, self.w[:,1:].T)+self.w[:,0]<br/>        <strong class="my iu">return</strong> out<br/>    <br/>    <strong class="my iu">def</strong> getOutput(self, a):<br/>        <strong class="my iu">return</strong> self.softmax(a)<br/>    <br/>    <strong class="my iu">def</strong> predict(self,x):<br/>        <strong class="my iu">return</strong> self.getInput(x)</span><span id="0ca4" class="mc lg it my b gy ng nd l ne nf">lrgd = LogisticRegression(lr=0.0001, iteration=3000, C=10)<br/>lrgd.fit(X_train,y_train)</span><span id="1a43" class="mc lg it my b gy ng nd l ne nf">result = np.argmax(lrgd.predict(X_test),axis=1)<br/>print(result)<br/>confusion = np.zeros((C,C))<br/>for i in range(C):<br/>    for j in range(C):<br/>        confusion[i,j] = np.sum(np.array([1 for k in result[i*200:(i+1)*200] if k==j]))<br/>errors = []<br/>TFs = np.zeros((C,3))<br/>for i in range(C):<br/>    fn = 0<br/>    fp = 0<br/>    for j in range(C):<br/>        if j!=i:<br/>            fn = fn + confusion[i][j]<br/>            fp = fp + confusion[j][i]<br/>    #print(s)<br/>    TFs[i,0] = confusion[i,i]<br/>    TFs[i,1] = fn<br/>    TFs[i,2] = fp<br/>    fn = fn/np.sum(confusion,axis=1)[i]<br/>    errors.append(fn)<br/>print("Class Errors:\n",errors)<br/>print("Confusion Matrix:\n",confusion)<br/>print("Error Rate: ",np.mean(errors))<br/>print("Accuracy  : ",np.sum([[confusion[i][i] for i in range(C)]])/(NT*C))</span></pre><h1 id="fe59" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">实验结果</h1><h2 id="8a56" class="mc lg it bd lh md me dn ll mf mg dp lp kr mh mi lr kv mj mk lt kz ml mm lv mn bi translated">资料组</h2><p id="b3d8" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated"><strong class="kk iu">输入模式</strong> : 16x16 的英文手写数字图像(256 维向量)<br/>T5】输出类别 : 1，2，3，4，5，6，7，8，9，0</p><p id="659a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">训练数据</strong> : 5000 张图像<br/> <strong class="kk iu">测试数据</strong> : 2000 张图像</p><h2 id="8b20" class="mc lg it bd lh md me dn ll mf mg dp lp kr mh mi lr kv mj mk lt kz ml mm lv mn bi translated">学问</h2><p id="abd1" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated"><strong class="kk iu">学习率</strong> : 0.0001 <br/> <strong class="kk iu">迭代</strong> : 3000</p><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="63c4" class="mc lg it my b gy nc nd l ne nf">Class Errors:<br/> [0.005, 0.07, 0.045, 0.07, 0.115, 0.04, 0.075, 0.11, 0.05, 0.035]<br/>Confusion Matrix:<br/> [[199.   0.   0.   0.   1.   0.   0.   0.   0.   0.]<br/> [  0. 186.   0.   8.   0.   0.   1.   4.   1.   0.]<br/> [  0.   1. 191.   0.   4.   0.   2.   2.   0.   0.]<br/> [  1.   1.   0. 186.   1.   2.   0.   2.   7.   0.]<br/> [  0.   0.  11.   3. 177.   1.   1.   0.   3.   4.]<br/> [  0.   1.   0.   2.   1. 192.   0.   2.   0.   2.]<br/> [  0.   1.   3.   2.   0.   0. 185.   1.   8.   0.]<br/> [  0.   2.   6.   1.   7.   0.   0. 178.   2.   4.]<br/> [  2.   0.   0.   2.   0.   0.   3.   3. 190.   0.]<br/> [  0.   1.   0.   0.   4.   1.   0.   1.   0. 193.]]<br/>Error Rate:  0.062<br/>Accuracy  :  0.939</span></pre><h1 id="fd3f" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">与 Fisher 线性判别分析的比较</h1><p id="648f" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">现在让我们来看看与<a class="ae le" rel="noopener" target="_blank" href="/handwritten-khmer-digit-recognition-860edf06cd57"> Fisher 的线性判别分析</a>相比，识别准确率是如何提高的。</p><h2 id="aba8" class="mc lg it bd lh md me dn ll mf mg dp lp kr mh mi lr kv mj mk lt kz ml mm lv mn bi translated">费希尔线性判别分析[ <a class="ae le" rel="noopener" target="_blank" href="/handwritten-khmer-digit-recognition-860edf06cd57">转到上一个故事</a></h2><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="2f4a" class="mc lg it my b gy nc nd l ne nf">mean = np.zeros((D,C))<br/>cov = np.zeros((D, D))</span><span id="ad22" class="mc lg it my b gy ng nd l ne nf">for i in range(C):<br/>    mean[:,i] = np.mean(X[:,:,i],1)<br/>    cov = cov + np.cov(X[:,:,i])/C</span><span id="4c64" class="mc lg it my b gy ng nd l ne nf">invS = np.linalg.pinv(cov)</span><span id="005b" class="mc lg it my b gy ng nd l ne nf">p = np.zeros((C,NT,C))<br/>for i in range(C):<br/>    t = T[:,:,i]<br/>    for j in range(C):<br/>        m = mean[:,j]<br/>        p[i,:,j] = np.dot(t.T,np.dot(invS,m)) - np.dot(m.T,np.dot(invS,m))/2.</span><span id="7dff" class="mc lg it my b gy ng nd l ne nf">P = np.argmax(p,axis=2)<br/>confusion = np.zeros((C,C))<br/>for i in range(C):<br/>    for j in range(C):<br/>        confusion[i,j] = np.sum(np.array([1 for k in P[i,:] if k==j]))</span><span id="abad" class="mc lg it my b gy ng nd l ne nf">errors = []<br/>TFs = np.zeros((C,3))<br/>for i in range(C):<br/>    fn = 0<br/>    fp = 0<br/>    for j in range(C):<br/>        if j!=i:<br/>            fn = fn + confusion[i][j]<br/>            fp = fp + confusion[j][i]<br/>    #print(s)<br/>    TFs[i,0] = confusion[i,i]<br/>    TFs[i,1] = fn<br/>    TFs[i,2] = fp<br/>    fn = fn/np.sum(confusion,axis=1)[i]<br/>    errors.append(fn)</span><span id="1e7d" class="mc lg it my b gy ng nd l ne nf">print("Confusion Matrix:\n",confusion)<br/>print("Error Rate: ",np.mean(errors))<br/>print("Accuracy  : ",np.sum([[confusion[i][i] for i in range(C)]])/(NT*C))</span></pre><h2 id="3dce" class="mc lg it bd lh md me dn ll mf mg dp lp kr mh mi lr kv mj mk lt kz ml mm lv mn bi translated">Fisher 线性判别分析的结果</h2><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="eb5c" class="mc lg it my b gy nc nd l ne nf">Confusion Matrix:<br/> [[199.   0.   0.   0.   1.   0.   0.   0.   0.   0.]<br/> [  0. 169.   8.   8.   1.   2.   4.   8.   0.   0.]<br/> [  0.   0. 182.   1.   5.   0.   2.   8.   1.   1.]<br/> [  2.   2.   0. 182.   0.   1.   0.   3.  10.   0.]<br/> [  0.   0.  21.   4. 162.   1.   0.   4.   4.   4.]<br/> [  1.   2.   0.   1.   5. 185.   0.   3.   0.   3.]<br/> [  2.   0.   1.   5.   1.   0. 181.   0.   9.   1.]<br/> [  0.   1.  16.   6.   6.   0.   1. 164.   3.   3.]<br/> [  1.   0.   0.   8.   0.   0.   7.   2. 182.   0.]<br/> [  0.   0.   3.   0.   0.   4.   0.   1.   0. 192.]]<br/>Error Rate:  0.101<br/>Accuracy  :  0.899</span></pre><h2 id="caa5" class="mc lg it bd lh md me dn ll mf mg dp lp kr mh mi lr kv mj mk lt kz ml mm lv mn bi translated">比较</h2><pre class="mp mq mr ms gt mx my mz na aw nb bi"><span id="2269" class="mc lg it my b gy nc nd l ne nf"># Fisher’s Linear Discriminant Analysis<br/><strong class="my iu">Error Rate:  0.101<br/>Accuracy  :  0.899</strong></span><span id="6275" class="mc lg it my b gy ng nd l ne nf"># Logistic Regression Model<br/><strong class="my iu">Error Rate:  0.062<br/>Accuracy  :  0.939</strong></span></pre><h1 id="e82c" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">讨论和结论</h1><p id="16d2" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">从实验结果来看，logistic 回归模型提高了识别的准确率。</p><p id="789d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 Fisher 的 LDA 中，假设所有类别的方差-协方差相等。因此，缺乏来自每个类别特征的信息。然而，Fisher 的 LDA 是一个极其简单的模型，消耗较少的计算时间。</p><p id="20cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在逻辑回归模型中，输入到每个类别中的线性变换的概念是线性判别规则的一种形式，这在线性中也发现类似于 Fisher 的 LDA。逻辑回归模型的特点是应用逻辑函数来估计后验概率。这有利于增加数据特征表示的自由度。然而，由于我们不能用解析计算直接获得逻辑回归的线性变换的最佳参数，所以梯度上升学习中的时间计算是一个缺点。此外，该学习规则是数值计算，其中仅获得近似参数，结果往往会根据初始化而改变。</p></div></div>    
</body>
</html>