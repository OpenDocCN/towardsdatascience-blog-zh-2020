<html>
<head>
<title>Fastai - Disaster Prediction using ULMFiT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 ULMFiT 进行快速灾害预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fastai-disaster-prediction-using-ulmfit-4dd244d3889c?source=collection_archive---------66-----------------------#2020-08-31">https://towardsdatascience.com/fastai-disaster-prediction-using-ulmfit-4dd244d3889c?source=collection_archive---------66-----------------------#2020-08-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3f50" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">迁移学习在自然语言处理领域的应用</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/05191841f2c877c869ea39eab4da155a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5fw-IHgrfZHyj6pt854fAg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由来自<a class="ae ky" href="https://www.pexels.com/photo/person-holding-blue-ballpoint-pen-on-white-notebook-669610/" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae ky" href="https://www.pexels.com/@goumbik?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Lucas </a>拍摄</p></figure><p id="7cfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上个月，我启动了<a class="ae ky" href="http://course19.fast.ai" rel="noopener ugc nofollow" target="_blank">fastai</a>MOOC(2019)——面向程序员的深度学习。它涵盖了各种主题，如计算机视觉、自然语言处理、协同过滤等。我发现最吸引人的部分是迁移学习在自然语言处理领域的应用。<a class="ae ky" href="http://course.fast.ai" rel="noopener ugc nofollow" target="_blank"> fastai </a>使用的方法是<strong class="lb iu">通用语言模型微调(</strong><a class="ae ky" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">ulm fit</strong></a><strong class="lb iu">)</strong>，这是一种应用于自然语言处理领域的迁移学习方法论。</p><div class="lv lw gp gr lx ly"><a href="https://course.fast.ai/" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">面向编码人员的实用深度学习</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">如果你现在就准备好投入进去，下面是开始的方法。如果你想更多地了解本课程，请阅读…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">course.fast.ai</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><blockquote class="mn mo mp"><p id="f66a" class="kz la mq lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated"><strong class="lb iu">根据</strong> <a class="ae ky" href="https://en.wikipedia.org/wiki/Transfer_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">维基百科</strong> </a> <strong class="lb iu">的说法，迁移学习</strong> (TL)是机器学习<strong class="lb iu">中的一个研究问题</strong> (ML)侧重于存储在解决一个问题时获得的知识，并将其应用于另一个不同但相关的问题。例如，在<strong class="lb iu">学习</strong>识别汽车时学到的知识可以应用到识别卡车上。</p></blockquote><p id="2242" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了展示<strong class="lb iu"> ULMFiT </strong>的用例，我将同样应用在<a class="ae ky" href="https://www.kaggle.com/c/nlp-getting-started" rel="noopener ugc nofollow" target="_blank">上真实与否？NLP 与灾难推文</a>在<strong class="lb iu"> Kaggle </strong>上竞争。为了让您对数据有个大概的了解，数据集包含两个<strong class="lb iu"> CSV </strong>文件<code class="fe mu mv mw mx b">train.csv</code> <em class="mq"> </em>和<code class="fe mu mv mw mx b">test.csv</code>分别代表训练数据集和测试数据集。训练集包含<code class="fe mu mv mw mx b">text</code>列中的 tweet 数据和<code class="fe mu mv mw mx b">target</code>列中的 target 值，如果是真正的灾难，则 target 值为<code class="fe mu mv mw mx b">1</code>，如果不是真正的灾难，则 target 值为<code class="fe mu mv mw mx b">0</code>。测试集只包含 tweet 数据，没有目标值。任务是预测一条推文是否代表一场真正的灾难。</p><p id="8f83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下段落摘自<a class="ae ky" href="https://medium.com/mlreview/understanding-building-blocks-of-ulmfit-818d3775325b#:~:text=High%20level%20idea%20of%20ULMFIT,learning%20rates%20in%20multiple%20stages" rel="noopener">了解 ULMFiT 的构建模块</a>:</p><blockquote class="mn mo mp"><p id="571a" class="kz la mq lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated">ULMFiT 的高级思想是使用像 wiki text-103(1.03 亿个标记)这样的非常大的语料库来训练语言模型，然后采用这个预训练模型的编码器并将其与定制的头部模型相结合，例如用于分类，并在多个阶段中使用有区别的学习率仔细地进行良好的旧的微调。ULMFiT 用于其语言建模任务的架构是一个<a class="ae ky" href="https://arxiv.org/pdf/1708.02182.pdf" rel="noopener ugc nofollow" target="_blank"> AWD-LSTM </a>。这个名字是 ASGD 减肥 LSTM 的缩写。</p></blockquote><p id="a13f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解更多关于乌尔姆菲特的信息，请参考杰瑞米·霍华德和塞巴斯蒂安·鲁德写的这篇论文。</p><p id="9401" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在计算机视觉问题中，迁移学习用于直接帮助分类，但是在 NLP 的情况下，我们首先建立一个语言模型，该模型基本上预测句子的下一个单词(该模型必须理解书写文本的语言(例如，英语等)。))然后使用语言模型的编码器和词汇表构建我们的分类模型。</p><p id="23e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如在<a class="ae ky" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">的论文</a>中提到的，我们将使用在维基百科上预先训练好的<a class="ae ky" href="https://arxiv.org/pdf/1708.02182.pdf" rel="noopener ugc nofollow" target="_blank"> AWD-LSTM </a>架构。我们可以直接使用这个预先训练的语言模型来建立我们的灾难推文分类器，但这里的要点是维基百科的英语语言将不同于推文的英语语言。因此，我们将使用推文数据微调我们预先训练的语言模型，然后在此基础上建立我们的分类器。正如《Fastai 和 PyTorch 的程序员深度学习》一书中所解释的，他们在 IMDb 评论数据集上使用了相同的预训练架构来分类评论是正面还是负面。他们解释说，IMDb 评论英语更不正式，包含电影、导演、演员等的名字。比普通的维基百科英语更好，架构是在维基百科英语上预先训练的。</p><h2 id="2498" class="my mz it bd na nb nc dn nd ne nf dp ng li nh ni nj lm nk nl nm lq nn no np nq bi translated">我们开始吧！！</h2><p id="17f3" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">我们导入我们的<code class="fe mu mv mw mx b">test.csv</code>和<code class="fe mu mv mw mx b">train.csv</code>来获得我们的训练数据集和测试数据集。我不能分享数据集，因为它来自一个<a class="ae ky" href="https://www.kaggle.com/c/nlp-getting-started/data" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Kaggle </strong>竞赛</a>，你可以登录你的帐户并遵守竞赛规则自行下载。</p><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="6119" class="my mz it mx b gy oa ob l oc od"># Importing Pandas<br/>import pandas as pd</span><span id="7b82" class="my mz it mx b gy oe ob l oc od"># Importing fastai libraries for text and callbacks<br/>from fastai.text import *<br/>from fastai.callbacks import *</span><span id="0edf" class="my mz it mx b gy oe ob l oc od">train = pd.read_csv('train.csv')<br/>test = pd.read_csv('test.csv')</span></pre><p id="dbdf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不会详细介绍语言模型是如何构建的，你应该去看看<a class="ae ky" href="http://course.fast.ai" rel="noopener ugc nofollow" target="_blank"> MOOC </a>和<a class="ae ky" href="https://www.amazon.in/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527" rel="noopener ugc nofollow" target="_blank">Fastai 和 PyTorch </a>的《程序员深度学习》这本书，做进一步的阅读。基本思想是文本数据不能直接输入到模型中，它需要被转换成数字，这样我们就可以应用我们的数学函数。这通过使用<strong class="lb iu">记号化</strong>和<strong class="lb iu">数字化</strong>来完成。在<strong class="lb iu">记号化</strong>中，我们将文本转换成记号列表，在<strong class="lb iu">数字化</strong>中，我们根据它们的索引将它们转换成数字。如果你想潜得更深，你应该参考<a class="ae ky" href="https://www.amazon.in/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527" rel="noopener ugc nofollow" target="_blank">书</a>。</p><p id="dc80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用<strong class="lb iu">数据块 API </strong>在幕后为我们完成上述工作，然后我们将把创建的<strong class="lb iu">数据块</strong>输入到我们的语言模型中。</p><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="d425" class="my mz it mx b gy oa ob l oc od">data_lm = (TextList.from_df(pd.concat([train[['text']], test[['text']]], ignore_index=True, axis=0))<br/>           .split_by_rand_pct(0.15)<br/>           .label_for_lm()<br/>           .databunch(bs=128))<br/>data_lm.show_batch()</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文本列显示标记化和数值化的数据</p></figure><p id="1074" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们忽略标签，从训练和测试数据中提取文本语料库。请记住，我们现在正在制作一个语言模型，而不是一个分类模型。我们只是尽可能多地包含文本数据，以便我们的语言模型预测句子的下一个单词。接下来，我们使用我们的<code class="fe mu mv mw mx b">data_lm</code>数据束来制作我们的语言模型。</p><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="80ab" class="my mz it mx b gy oa ob l oc od">learn = language_model_learner(data_lm, AWD_LSTM, drop_mult = 0.5)<br/>learn.lr_find()<br/>learn.recorder.plot(suggestion = True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/04939623ab0dc29b6743872efa142d33.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*KyMVXqvzIvFcGnGsIkyxBA.png"/></div></figure><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="f095" class="my mz it mx b gy oa ob l oc od">learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="5571" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将解冻模型，我们将适应更多。我们将使用回调来选择最佳模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="2400" class="my mz it mx b gy oa ob l oc od">Better model found at epoch 0 with accuracy value: 0.4097544550895691.<br/>Better model found at epoch 1 with accuracy value: 0.4404464364051819.<br/>Better model found at epoch 2 with accuracy value: 0.4609375.<br/>Better model found at epoch 3 with accuracy value: 0.47495537996292114.<br/>Better model found at epoch 4 with accuracy value: 0.48810267448425293.<br/>Better model found at epoch 5 with accuracy value: 0.49515628814697266.<br/>Better model found at epoch 6 with accuracy value: 0.4975222945213318.<br/>Better model found at epoch 9 with accuracy value: 0.49756699800491333.</span></pre><p id="7eb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将选择 9 号纪元时的最佳精度。然后我们将保存语言模型和编码器。</p><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="99c4" class="my mz it mx b gy oa ob l oc od">learn.save('fine_tuned')<br/>learn.save_encoder('fine_tuned_enc')</span></pre><p id="4325" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将制作分类器。为此，我们需要创建一个新的数据中心。我们将验证集设为 10%,我们将保持我们的词汇表与语言数据库中的词汇表相同。我们还将在单独的<code class="fe mu mv mw mx b">add_test</code>参数中添加我们的测试数据。</p><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="0c0f" class="my mz it mx b gy oa ob l oc od">data_clas = (TextList.from_df(df, vocab=data_lm.vocab)<br/>             .split_by_rand_pct(0.1)<br/>             .label_from_df('target')<br/>             .add_test(TextList.from_df(test['text'], vocab=data_lm.vocab))<br/>             .databunch(bs=128))</span></pre><p id="e6bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用与语言模型相同的编码器来构建分类器。</p><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="a90b" class="my mz it mx b gy oa ob l oc od">learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, FBeta(beta=1)])<br/>learn.load_encoder('fine_tuned_enc')</span></pre><p id="df74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将进行<code class="fe mu mv mw mx b">lr_find()</code>检查，然后绘制图表。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/ed24c25e4aef57e3426a3b65d5bd98db.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*dIE_giqHnjXqdI-jxhtYkw.png"/></div></figure><p id="c78c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来拟合一个周期。我们看到我们得到了 77.66%的精度<strong class="lb iu"/>。</p><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="9189" class="my mz it mx b gy oa ob l oc od">learn.fit_one_cycle(1, 1e-3, moms=(0.8,0.7))</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="5f80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们解冻最后 2 层，训练一个周期。我们的准确度提高到<strong class="lb iu"> 79.5 </strong> %。</p><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="7646" class="my mz it mx b gy oa ob l oc od">learn.freeze_to(-2)<br/>learn.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-2), moms=(0.8,0.7))</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="4303" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将解冻最后 3 层，然后再训练一个周期。我们的准确率提高到<strong class="lb iu"> 81.73% </strong>！</p><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="34e0" class="my mz it mx b gy oa ob l oc od">learn.freeze_to(-3)<br/>learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="42de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里使用的是在 ULMFiT 中引入的区别学习率。正如文章<a class="ae ky" rel="noopener" target="_blank" href="/10-new-things-i-learnt-from-fast-ai-v3-4d79c1f07e33">中所解释的，我从 fast.ai v3 </a>中学到的 10 个新东西:</p><blockquote class="mn mo mp"><p id="6723" class="kz la mq lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated"><strong class="lb iu">预训练模型的判别学习率<br/> </strong>以超低学习率训练早期层，以较高学习率训练后期层。这个想法是不要剧烈地改变几乎完美的预训练权重，除了极少量的，并且更积极地教导输出附近的层。在 ULMFiT 中引入了判别学习率。</p></blockquote><p id="2cc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将解冻所有层，训练，并使用回调来选择我们的最佳模型。</p><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="490b" class="my mz it mx b gy oa ob l oc od">callbacks = SaveModelCallback(learn,monitor="accuracy", mode="max", name="best_classification_model")</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><pre class="kj kk kl km gt nw mx nx ny aw nz bi"><span id="8b3b" class="my mz it mx b gy oa ob l oc od">Better model found at epoch 0 with accuracy value: 0.8160315155982971.<br/>Better model found at epoch 1 with accuracy value: 0.8173456192016602.<br/>Better model found at epoch 2 with accuracy value: 0.822601854801178.<br/>Better model found at epoch 9 with accuracy value: 0.8239158987998962.</span></pre><p id="e517" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们得到的准确率为 82.39% ！！</p><h1 id="6146" class="oj mz it bd na ok ol om nd on oo op ng jz oq ka nj kc or kd nm kf os kg np ot bi translated">结论</h1><p id="1f9d" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">在迁移学习中，我们使用来源的知识，并将其应用于我们的目标。在自然语言处理领域中同样的实现提供了非常先进的结果，因为我们使用了预先训练的网络，所以只需要最少的训练。如果你想看代码的运行，你可以看看我在 Kaggle 上的笔记本:<a class="ae ky" href="https://www.kaggle.com/sachin93/nlp-disaster-prediction-ulmfit/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> NLP -灾难预测 ULMFiT </strong> </a></p><p id="2915" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:我在这里用的是法士泰 V1。fastai V2 和新的 MOOC 于 8 月 21 日发布。看看这里</strong></p><h1 id="1ea7" class="oj mz it bd na ok ol om nd on oo op ng jz oq ka nj kc or kd nm kf os kg np ot bi translated">参考</h1><div class="lv lw gp gr lx ly"><a href="https://arxiv.org/abs/1801.06146" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">用于文本分类的通用语言模型微调</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">归纳迁移学习极大地影响了计算机视觉，但现有的自然语言处理方法仍然需要…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="lv lw gp gr lx ly"><a href="https://www.oreilly.com/library/view/deep-learning-for/9781492045519/" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">利用 fastai 和 PyTorch 为编码人员提供深度学习</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">深度学习通常被视为数学博士和大型科技公司的专属领域。但是作为这个实践指南…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">www.oreilly.com</p></div></div><div class="mh l"><div class="ou l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a href="https://course.fast.ai/" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">面向编码人员的实用深度学习</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">如果你准备好现在就开始，这里有一些方法。如果你想更多地了解本课程，请阅读…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">course.fast.ai</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a href="https://fastai1.fast.ai/vision.html" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">视力</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">模块 fastai . vision . transform:get _ transforms(do _ flip:bool = True，flip_vert: bool…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">fastai1.fast.ai</p></div></div><div class="mh l"><div class="ov l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a href="https://medium.com/mlreview/understanding-building-blocks-of-ulmfit-818d3775325b" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">了解 ULMFIT 的构建模块</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">上周，我有时间处理了一个 Kaggle NLP 竞赛:Quora 虚假问题分类。因为很容易…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">medium.com</p></div></div><div class="mh l"><div class="ow l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/10-new-things-i-learnt-from-fast-ai-v3-4d79c1f07e33"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">我从 fast.ai v3 中学到的 10 件新事情</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">参加课程 3 周后的学习要点</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="ox l mj mk ml mh mm ks ly"/></div></div></a></div></div></div>    
</body>
</html>