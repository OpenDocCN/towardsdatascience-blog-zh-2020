<html>
<head>
<title>Understanding Reinforcement Learning Math, for Developers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解强化学习数学，面向开发人员</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-reinforcement-learning-math-for-developers-b538b6ef921a?source=collection_archive---------23-----------------------#2020-02-03">https://towardsdatascience.com/understanding-reinforcement-learning-math-for-developers-b538b6ef921a?source=collection_archive---------23-----------------------#2020-02-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2b02" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">开发人员破译强化学习数学公式的温和方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/72c0300f26c256a92f58996598756b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6xeqlYQyxL4koFwZ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@tine999?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tine ivani</a>拍摄的照片</p></figure><p id="4801" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新</strong>:学习和练习强化学习的最好方式是去<a class="ae ky" href="http://rl-lab.com/" rel="noopener ugc nofollow" target="_blank">http://rl-lab.com</a></p><p id="c617" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你是一个数学知识不足的开发人员，你可能很难掌握强化学习的基本公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/1ea8171f38346bb035a766a9b5e3252e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pMDsBLby3Rn7mW29.png"/></div></div></figure><p id="78b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于没有足够数学背景的人来说，理解这个等式可能很有挑战性。然而，除了神秘的符号之外，理解起来并不困难。</p><p id="c566" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">归结起来就是问一个问题:在 S 状态下有什么价值？<br/>更具体地说，假设你在一个电视节目中，放在两扇门前，一扇门后面有 5000 美元，另一扇门有 1000 美元，要求你选择一扇门打开。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/190ade0069cd664949d6dd9610d7a4b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MfHez_fE2l4xq2EX"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">罗伯特·阿纳施在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0973" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你不知道哪个是哪个，所以你有相同的概率打开其中任何一个。现在的问题是，出演这部电视剧值多少钱？答案相当简单。在最坏的情况下，它值 1000 美元(因为你可以赢得最少 1000 美元)，在最好的情况下，它值 5000 美元(因为你可以赢得最多 5000 美元)。为了只有一个值，我们计算平均值为 3000 美元。</p><p id="dc69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以这种情况或状态的价值，就是你可以期望得到的(平均而言)未来的奖励或回报。</p><p id="e93d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们稍微修改一下假设。你被告知左边的门包含 5000 美元，右边的门包含 1000 美元，但是要打开其中任何一扇门，你必须用球击中锁，有 20%的机会击中左边门的锁，有 80%的机会击中右边门的锁。</p><p id="e0d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种情况的价值是什么？</p><p id="6122" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将是未来的回报乘以获得它们的概率，所以:<br/> .2 * 5000 + .8 * 1000 = 1800 美元</p><p id="0c12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直观上，我们可以把状态 V(S)的价值想象成贴现的未来回报的总和，用得到它们的概率来加权。</p><p id="42b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要贴现因子(<strong class="lb iu"> γ) </strong>，来源于你在遥远的未来得到的一个奖励，比你在下一步得到的奖励更没有价值，所以会按照达到它所需的步数来贴现，例如:<strong class="lb iu"> γ*γ*…。*γ*r，</strong>其中<strong class="lb iu"> γ </strong>的乘法次数等于达到奖励的步数。</p><p id="cabe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数学细节可以在本文中找到<a class="ae ky" rel="noopener" target="_blank" href="/math-behind-reinforcement-learning-the-easy-way-1b7ed0c030f4">强化学习背后的数学，简单易行的方法</a></p><p id="9381" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图显示了状态、行动、奖励和目标状态之间的关系。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/6499f4406a11a35c3d3524c416ed2a2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHV-EdOmzrLS9UV6MugI6Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">状态 S 有 3 个可能的动作(a1，a2，a3)。每个行动将导致一个或多个目标状态(S1、S2、S3)和不同的奖励(r1、r2、r3、r4、r5、r6)。注意，从 s 到 S4 是不可达。每条边(或箭头)代表沿其路径的一种可能性。</p></figure><p id="580c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以看到状态 S 有 3 种可能的行动，每一种都导致一种或多种不同概率和不同回报的状态。值得注意的是，州 S4 从 S 是不可到达的，但是如果采取行动 a1，S1 是可到达的，并且将导致 2 个可能的奖励。如果采取了动作 a2 或 a3，则可以从 S 到达州 S2。实际上 a2 肯定会到达 S2，而 a3 可能以不同的概率到达 S2 或 S3。</p><p id="bef2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个状态<strong class="lb iu"> <em class="ly"> s </em> </strong>的值 V(s)，将是采取一个动作<strong class="lb iu"> <em class="ly"> a </em> </strong>，<strong class="lb iu"> <em class="ly"> </em> </strong>的概率乘以<strong class="lb iu"> <em class="ly"> </em> </strong>到达状态<strong class="lb iu"><em class="ly">‘s’</em></strong>，乘以获得一个奖励的概率<strong class="lb iu"> <em class="ly"> r </em> </strong>，乘以期限(t)<br/>但是因为在每个阶段我们可能有几个选项，意味着几个动作可供选择，这导致几个可能的状态<strong class="lb iu"><em class="ly">‘s’</em></strong>和几个可能的奖励<strong class="lb iu"> <em class="ly"> r </em> </strong>。我们需要总结所有这些不同的选择。</p><p id="d817" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">初始公式可以改写如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lz"><img src="../Images/5a24fce94918c1388c7cc135e1e5f2e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*18sDyBzzEdtDXCau3pPGDA.png"/></div></div></figure><h1 id="9a71" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">翻译成代码</h1><p id="83bd" class="pw-post-body-paragraph kz la it lb b lc ms ju le lf mt jx lh li mu lk ll lm mv lo lp lq mw ls lt lu im bi translated">这个公式如何转化成代码相当简单。<br/>每个 sum(σ)转换成一个循环，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/ca872854562e851325c8e429ae156ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YGwaSEK3WGBTONlDaC4DuA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">其中 A 是所有动作的集合，S 是所有状态的集合，R 是所有可能的奖励的集合，</p></figure><p id="ce1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代码显示的内容对开发人员来说可能很奇怪。它在动作集合 A 中的所有动作、状态集合 S 中的所有状态以及奖励集合 r 中的所有奖励上循环。然而，我们知道，有时并非所有动作在所有状态下都可用，并且并非所有状态都可以从当前状态到达，当然，并非所有类型的奖励对于每个动作和每个状态都可用。<br/>所以循环遍历所有这些集合纯粹是浪费精力和时间。<br/>有一些方法可以优化代码以提高性能。</p><h1 id="de47" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">最佳化</h1><p id="747b" class="pw-post-body-paragraph kz la it lb b lc ms ju le lf mt jx lh li mu lk ll lm mv lo lp lq mw ls lt lu im bi translated">看上面的图，很容易看出嵌套循环的数量。<br/>从编程角度来说，可以进行一些优化。没有必要循环所有动作、所有状态和所有奖励。但是我们将只遍历可能的选项，忽略不可用的选项。</p><p id="ffee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，并不是所有的动作集都出现在所有的状态上，所以我们不是在一个上循环，而是在该状态下可用的动作上循环，例如 state.getAvailableActions()。<br/>同样，代替整个状态集 S，我们循环遍历从当前状态可达的状态，state.getAccessibleStates()，代替所有奖励 R，我们循环遍历与当前状态和所选动作相关的可能奖励，getRewards(state，a)。</p><p id="9bce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的伪代码创建了一个描述状态行为的接口(IState)。这个接口将在下面的算法中使用。</p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="8645" class="nd mb it mz b gy ne nf l ng nh">interface IState {<br/> <br/>// returns list of states accessible from the current state<br/>function getAccessibleStates() </span><span id="cdc5" class="nd mb it mz b gy ni nf l ng nh">// returns list of actions that can be performed in current state<br/>function getAvailableActions()<br/>  <br/>// return probability of taking action 'a' in the current state<br/>function getActionProbability(IAction a)</span><span id="752e" class="nd mb it mz b gy ni nf l ng nh">// returns the probability of arriving at state s' when performing<br/>// action a<br/>function getStateProbability(IState s', IAction a)</span><span id="05fe" class="nd mb it mz b gy ni nf l ng nh">// get value of the state<br/>function getValue()</span><span id="af7f" class="nd mb it mz b gy ni nf l ng nh">// set value to the state<br/>function setValue(double v)</span><span id="9720" class="nd mb it mz b gy ni nf l ng nh">}</span></pre><p id="e206" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类 R 表示在给定状态<strong class="lb iu"> <em class="ly"> s </em> </strong>和动作<strong class="lb iu"> <em class="ly"> a </em> </strong>的情况下返回奖励的实用程序类。函数 getReward(s，a)只返回一个奖励，而 getPossibleRewards(s，a)返回一个奖励列表。<br/>函数 getrewardpability(r，s，a)返回在状态<strong class="lb iu"> <em class="ly"> s </em> </strong>和执行动作<strong class="lb iu"> <em class="ly"> a </em> </strong>时获得奖励<strong class="lb iu"> <em class="ly"> r </em> </strong>的概率。</p><p id="8b34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在许多情况下，奖励系统很简单，也就是说每个州和每个行动只有一个奖励。所以 getReward(s，a)和 getPossibleRewards(s，a)返回相同的奖励，getrewardpability(r，s，a)返回 1。</p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="76a8" class="nd mb it mz b gy ne nf l ng nh">class R {</span><span id="cafd" class="nd mb it mz b gy ni nf l ng nh">// returns the reward when at state s and performing action a static static function getReward(IState s, IAction a)</span><span id="53bf" class="nd mb it mz b gy ni nf l ng nh">// returns list of rewards when at state s and performing action a<br/>static function getPossibleRewards(IState s, IAction a)</span><span id="511f" class="nd mb it mz b gy ni nf l ng nh">// get the probability of getting reward r when at state s <br/>// and performing action a<br/>static function getRewardProbability(IReward r, IState s, IAction a)</span><span id="e261" class="nd mb it mz b gy ni nf l ng nh">}</span></pre><p id="3fd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数 computeStateValue(s)计算一个状态的值<strong class="lb iu"><em class="ly"/></strong>在最一般的情况下，我们假设奖励系统可能不简单，对于同一个状态/动作对可能有几种可能的奖励。</p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="171e" class="nd mb it mz b gy ne nf l ng nh">function computeStateValue(IState s){<br/> sum_s = 0<br/> <br/> for( a in ){<br/>  sum_s' = 0<br/>  <br/>  for(s' in s.getAccessibleStates()){<br/>   sum_r = 0<br/>   <br/>   for(r in R.getPossibleRewards(s, a)){<br/>    sum_r += R.getRewardProbability(r, s, a) * (r + gamma <br/>             * s.getValue())<br/>   }<br/>  <br/>   sum_s' += s.getStateProbability(s', a) * sum_r<br/>  }<br/>  <br/>  sum_s += s.getActionProbability(a) * sum_s'<br/> }<br/> <br/> return sum_s<br/>}</span></pre><p id="21c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设奖励系统很简单，函数 computeStateValueSinpleReward(s)计算状态 s 的值。所以不需要循环遍历可能的奖励，只是我们简单的调用 R.getReward(s，a)，得到的概率是 1。</p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="7641" class="nd mb it mz b gy ne nf l ng nh">function computeStateValueSinpleReward(IState s){<br/> sum_s = 0<br/> <br/> for( a in s.getAvailableActions()){<br/>  sum_s' = 0<br/>  <br/>  for(s' in s.getAccessibleStates()){<br/>   sum_s' += s.getStateProbability(s', a) * (R.getReward(s, a) <br/>             + gamma * s.getValue())<br/>  }<br/>  sum_s += s.getActionProbability(a) * sum_s'<br/> }<br/> <br/> return sum_s<br/>}</span></pre><p id="020a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不难注意到，上面的代码涉及一个状态的计算。为了计算所有状态的值，我们对所有状态集进行循环，如伪代码函数 computeValueForAllStates()所示</p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="6f75" class="nd mb it mz b gy ne nf l ng nh">function computeValueForAllStates(){<br/> for( s in S){<br/>  v = computeStateValue(s)<br/>  s.setValue(v)<br/> }<br/>}</span></pre><h1 id="77b7" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">结论</h1><p id="199a" class="pw-post-body-paragraph kz la it lb b lc ms ju le lf mt jx lh li mu lk ll lm mv lo lp lq mw ls lt lu im bi translated">该公式的实现非常简单。然而，这本身是不够的，因为许多参数在现实生活中是未知的。出于这个原因，有很多其他技术可以用来帮助估计这个公式的每个组成部分。</p><h1 id="8c3e" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">相关文章</h1><ul class=""><li id="3920" class="nj nk it lb b lc ms lf mt li nl lm nm lq nn lu no np nq nr bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/math-behind-reinforcement-learning-the-easy-way-1b7ed0c030f4">强化学习背后的数学，最简单的方法</a></li><li id="c9eb" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><a class="ae ky" href="https://medium.com/@zsalloum/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182" rel="noopener">开发者强化学习政策</a></li><li id="b50a" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><a class="ae ky" href="https://medium.com/p/9350e1523031" rel="noopener"> Q vs V 在强化学习中，最简单的方法</a></li><li id="d96a" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><a class="ae ky" href="https://medium.com/p/1b7ed0c030f4" rel="noopener">数学背后的强化学习，最简单的方法</a></li><li id="5768" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><a class="ae ky" href="https://medium.com/@zsalloum/dynamic-programming-in-reinforcement-learning-the-easy-way-359c7791d0ac" rel="noopener">动态编程在强化学习中的简便方法</a></li><li id="134c" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><a class="ae ky" href="https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511" rel="noopener">蒙特卡洛强化学习中的简单方法</a></li><li id="26bd" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce"> TD 在强化学习中，最简单的方法</a></li></ul></div></div>    
</body>
</html>