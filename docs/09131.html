<html>
<head>
<title>Word embeddings, what are they really?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入，它们到底是什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-embeddings-what-are-they-really-f106e1ff0874?source=collection_archive---------28-----------------------#2020-06-30">https://towardsdatascience.com/word-embeddings-what-are-they-really-f106e1ff0874?source=collection_archive---------28-----------------------#2020-06-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4556" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">令人惊奇的单词嵌入，对处理语言非常有用，它们到底是什么？我们仔细看看单词包和单词嵌入之间的联系，看看创建单词向量的 skip-gram 模型，并建立一些关于单词嵌入的直觉。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7f93be275a5426ce4c012b8e9e190cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2Q7DkLcmwYUm5mfHDCyUw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">帕特里克·托马索在<a class="ae kv" href="/s/photos/words?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a263" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你一直在从事自然语言处理(NLP)的工作，那么即使你曾经对 NLP 感到好奇，单词嵌入的概念也可能会出现。但是这些嵌入到底是什么呢？</p><p id="a4bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了找到答案，我们从头开始。NLP 是应用不同的方法从文本中计算出有用的东西。它首先把有问题的文本处理成我们可以用计算机进行计算并做出决定的东西。在这种情况下，决策是模型的输出，这可以是将请求分为不同的类别，如订单、反馈或投诉。它可以匹配项目的文本描述，以便找到类似的项目，或者找到与所描述的犯罪行为相关的法律文本。可能性的清单很长，而且多种多样。然而，在我们可以使用文本创建一个有意义的模型之前，我们需要将它解析成一种我们可以使用计算机处理的格式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/aa0b92359d09028e1e634732a2387a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*90f9Zzr6K3BUovn37w_V9A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">NLP 使用语言作为决策的输入，这是 NLP 模型的高级图表。</p></figure><h1 id="2b1c" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">为什么不用所有的字母呢？</h1><p id="69a4" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">什么事？英语只有 26 个，其他语言更多，(像我的 29 个)，何乐而不为呢？想象一个解决方案，将每个单词编码成一个数字，或者更好，一个热编码。然后我们用计算机算法可以理解的方式来表达字母。然而，这比它第一次出现时要复杂得多。你看，每个字母本身并没有意义。为了能够理解单独的字母，我们需要一个模型来查看每个字母与下一个字母之间的联系，这是一个有趣的情况，但还没有显示出实际有用的结果。很容易找到拼写相似的单词(甚至是变位词)的例子，而这些单词的意思肯定不一样。其结果是，任何使用单个字母作为输入的模型都必须考虑字母的顺序，这使得模型的开发具有挑战性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/47792e183618e8ba2606edcf24a4e931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9aiQQRXjHehPq7dokE_Tjw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">即使所有的字母都一样，单词的意思却不一样</p></figure><h1 id="4c28" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">一袋单词</h1><p id="4e8c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">单词包是一种从文档中创建矢量的方法。它的工作原理是统计文档中的所有单词，并将所选词汇表中的每个单词分配到向量表示中的一个位置。这是一个单词的热编码(词汇表中每个单词一个位置)，然后将向量相加得到文档的单词包表示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/123f11386f3596ea8271199ec0bbbc12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BsjyIcShUXorYo0yZvXRiw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">词汇量为 25 的单个单词的弓形表示。</p></figure><p id="826c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用单词包时要记住的一点是，向量的长度等同于词汇的大小，每个单词都表示为向量中的一个位置。从数学上来说，每个单词对结果向量贡献一个维度。这是一种稀疏编码，使用大量内存来存储关于文本的少量信息。</p><p id="d641" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">单词包已经成功地应用于诸如将文本分类的任务中。不同单词的频率实际上可以告诉你很多关于我们正在处理的文本类型的信息，这使得这种类型的向量对于例如分类任务非常有用。点击阅读更多关于单词袋<a class="ae kv" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">的信息。</a></p><p id="6d7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">词汇袋方法确实有用，可以用在很多情况下，但它有一定的局限性。单词袋的缺点:</p><ul class=""><li id="a2ab" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">不捕获单词和语法的顺序</li><li id="89b3" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">单词之间的关系丢失了，因为作品包生成的向量中的位置分配是任意的。</li><li id="f9e2" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">由于处理单词包所需的向量的长度与词汇量的大小直接相关，根据我的经验，与你在普通文本中遇到的词汇相比，词汇量通常也是有限的。testyourvocab.com<a class="ae kv" href="http://testyourvocab.com/blog.php" rel="noopener ugc nofollow" target="_blank">的一个在线词汇测试项目显示<em class="ng">“大多数成人母语考生的词汇量大约在 20，000-35，000 个单词之间”</em></a></li></ul><h1 id="9839" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">单词嵌入</h1><p id="b13e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">那么，有没有一种方法可以用数字来捕捉语言，以一种对计算机有意义的格式，最好不要用所有的存储空间来捕捉庞大的词汇？</p><p id="f3b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在数学上，我们可以将单词在单词类型向量包中的表示视为每个单词由多维空间中的一个方向表示，其中维度由我们包含在词汇表中的单词数量决定。单词的顺序是随机的，所有单词都在与所有其他单词正交的方向上起作用。这意味着，就我们从词语的表征中所能看出的来说，它们彼此之间都是同样不同的。如果我们的词汇中有“绵羊”、“羔羊”和“石英”，我们的模型必须将这些点连接起来，并找出“羔羊”和“绵羊”彼此更相似，而不是与“石英”相似。这不是对存储空间的非常紧凑的使用，并且没有对单词被分配向量的方式进行训练。</p><p id="a8ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些问题的答案是<strong class="ky ir">单词嵌入</strong>。它们是如何工作的？对于单词嵌入，我们为每个单词分配一个长度通常为 100-300 维的向量。在<a class="ae kv" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank"> glove </a>的论文中显示，嵌入的矢量大小的范围是具有最有用结果的范围。</p><p id="3484" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，这些随机初始化的向量被训练以实现向量组织，该向量组织根据由这些向量创建的向量空间中的含义来排列单词。为了实现这一点，我们必须使用单词在句子中出现的顺序来创建一个向量，以捕捉单词的一些含义。这个概念是训练一个神经网络来预测哪些单词一起出现。有两种方法可以做到这一点，要么我们向网络输入目标单词，并尝试预测周围的单词，即上下文单词，要么我们向网络输入上下文单词，并尝试预测目标单词。</p><p id="efa0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这方面的训练设置相当优雅。遍历语料库(大型自然文本数据集，例如维基百科转储)中的单词，并尝试预测周围的单词。这种方法被称为跳格法。还有一种替代的训练方法，即连续单词包，它将周围(上下文)的单词输入到模型中，然后对目标单词进行预测。在这里，我将集中讨论 skip-gram 版本，因为这两个版本在架构和结果上都非常相似。尽管如此，值得注意的是这些方法是不等价的，尽管它们是相似的。Skip-Gram 倾向于更好地处理不常用的单词，因为当它是目标单词时，不常用的单词将被同样对待，而连续的单词包倾向于给不常用的单词一个低概率，而它训练起来更快。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/744e806b79e39171acfb5bf9705bdac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8SVcWwWQRv5975XY0EHphA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用于训练神经网络以预测哪些单词同时出现的滑动窗口。</p></figure><p id="dd02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">词汇表的大小是在初始化时设置的参数。词汇表的大小等于模型中输入向量的大小。使用一种热编码来创建输入向量，即向量在向量中分配给目标单词的位置具有“1 ”,在所有其他位置具有“0”。输入向量乘以维度为 V×N 的权重矩阵，其中 N 是隐藏层的大小。然后，隐藏层乘以维数为 N×V 的矩阵 W ’,这产生了维数为 1×V 的向量。softmax 函数被应用于向量 U，以便产生每个单词的概率。输出向量是词汇表中所有单词成为目标/输入单词的上下文单词之一的概率列表。上下文窗口的大小是模型的一个参数，它对结果嵌入有影响。由于更多的可用训练样本，更大的窗口大小可以给出更高的准确度，但是也导致更长的训练时间。它还将使模型反映单词的更广泛的主题意义，而较小的窗口大小将倾向于给出反映关于目标单词的更集中的信息的结果。有关上下文窗口大小的更多信息，请参见论文“<a class="ae kv" href="https://www.aclweb.org/anthology/P14-2050/" rel="noopener ugc nofollow" target="_blank">基于依存关系的单词嵌入</a>”。</p><p id="2f23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个人工神经网络，我们可以应用一个误差函数，并使用反向传播来训练网络，最好是使用一个大型，多样化的文本语料库。这是一个简化的解释，在实现单词嵌入架构时，需要考虑很多调整和特殊的适应。如果你想继续读下去，了解培训的更多细节，我建议这篇<a class="ae kv" rel="noopener" target="_blank" href="/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b">文章</a>。还值得注意的是，几个预先训练的模型可用于单词嵌入，这对我们大多数人来说是明智的选择，因为它们是在非常大的语料库上训练的，并且产生非常好的单词嵌入。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/38275631b7ddb2691005d2525a7c69b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HQJXEdVJ6Q-gIgDlyScGvg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">跳过程序体系结构</p></figure><p id="aaea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在这几乎就像一个魔术:这个模型的重点从来不是预测上下文单词，我们所追求的是隐藏层，也就是单词嵌入的<strong class="ky ir">。如果你熟悉自动编码器，这是一个类似的概念。神经网络必须利用隐藏层大小的向量空间来捕获关于每个单词的信息，使其能够预测上下文单词。如果两个单词经常与相同的上下文单词一起出现，则模型必须为这些单词产生相似的向量，以便在预测上下文单词时实现高准确度。这也是我们在考察单词嵌入给出的一些向量时看到的。国家的名称和动物的种类聚集在一起，而服装和体育活动则在一个集群中。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/279451e920050f4b4e8e85a4eb834d63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n0yjo_OuKsnLo7OorpT59Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">单词嵌入清楚地显示相似的单词有相似的嵌入。(使用 Gensim 的模型生成的向量，该模型在谷歌新闻数据集上进行训练。使用 T-SNE 进行降维来实现可视化。</p></figure><p id="017a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.aclweb.org/anthology/N13-1090.pdf" rel="noopener ugc nofollow" target="_blank">连续空间单词表示的语言规则</a>的作者注意到的一个令人困惑的事情是，单词的单数和复数形式之间的关系具有几乎相同的单词嵌入差异:“<em class="ng"> x 苹果 x 苹果≈ <br/> x 汽车 x 汽车，x 家族 x 家族≈ x 汽车 x 汽车</em>”。同样，这同样适用于男性和女性，由此产生了著名的例子“<em class="ng">国王-男人+女人=王后</em>”。</p><p id="8c5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">单词嵌入的惊人之处在于，它们是在无人监督的情况下训练的，通过捕获如此多的关于单词如何使用的信息，它们也捕获了相当多的关于单词意思的信息。需要注意一些限制，其中之一是同音异义词，即拼写相同但含义完全不同的单词，该单词的两种含义属于向量空间中的不同位置，然而，这当然是不可能的，因为计算机无法区分这两种含义。当单词根据上下文具有不同的含义时，也会出现类似的问题。不考虑这些问题，单词嵌入是有用的，并且在无数的实际应用中被应用。</p></div></div>    
</body>
</html>