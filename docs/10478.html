<html>
<head>
<title>Depth Considered Harmful?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">被认为有害的深度？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/depth-considered-harmful-1a022f3abbac?source=collection_archive---------55-----------------------#2020-07-22">https://towardsdatascience.com/depth-considered-harmful-1a022f3abbac?source=collection_archive---------55-----------------------#2020-07-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/8c6af90589479e4d7e8036f6a47efb31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gwVdnMIQLoJbzxfnUDFGZw.png"/></div></figure><h2 id="9c62" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">我们需要深度图神经网络吗？</h2><p id="df2e" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb kd lc ld le kh lf lg lh kl li lj lk ll ij bi translated">由<a class="lm ln ep" href="https://medium.com/u/7b1129ddd572?source=post_page-----1a022f3abbac--------------------------------" rel="noopener" target="_blank">迈克尔布朗斯坦</a> — 9 分钟阅读</p><p id="c9b9" class="pw-post-body-paragraph kr ks iq kt b ku lo kw kx ky lp la lb kd lq ld le kh lr lg lh kl ls lj lk ll ij bi translated">深度学习的标志之一是使用数十层甚至数百层的神经网络。与之形成鲜明对比的是，图形深度学习中使用的大多数架构都很浅，只有少数几层。在这篇文章中，我提出了一个异端的问题:图神经网络架构中的深度带来任何优势吗？</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/294d1fcf38e3ba0c8d0171212847d7f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zcqiosZWcCDkvDyvzAOPzQ.jpeg"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">刘易斯·恩古吉在 Unsplash 上拍摄的照片</p></figure><h2 id="6767" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><a class="ae kq" rel="noopener" target="_blank" href="/5-typical-mindset-mistakes-of-aspiring-data-scientists-32eca8e9e0c4">有抱负的数据科学家的 5 个典型思维错误</a></h2><p id="774e" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb kd lc ld le kh lf lg lh kl li lj lk ll ij bi translated">由<a class="lm ln ep" href="https://medium.com/u/bc17bd299bf1?source=post_page-----1a022f3abbac--------------------------------" rel="noopener" target="_blank">孙铁麟迈斯特</a> — 6 分钟读取</p><p id="08b1" class="pw-post-body-paragraph kr ks iq kt b ku lo kw kx ky lp la lb kd lq ld le kh lr lg lh kl ls lj lk ll ij bi translated">在过去的几年里，我和 500 多名有抱负的数据科学家一起工作过，我看到了他们容易犯的一些典型的思维错误。在这篇文章中，我想分享其中的五个。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi jn"><img src="../Images/9b78f8b9548c2ba38d18d66733f41b56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lMB5rpzLcGwURibiUAH6ug.png"/></div></div></figure><h2 id="f09a" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><a class="ae kq" rel="noopener" target="_blank" href="/lyric-based-song-recommendation-with-doc2vec-embeddings-and-spotifys-api-5a61c39f1ce2">基于歌词的歌曲推荐，带有 Doc2Vec 嵌入和 Spotify 的 API </a></h2><p id="6b80" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb kd lc ld le kh lf lg lh kl li lj lk ll ij bi translated">由亚当·里维斯曼 — 5 分钟阅读</p><p id="1c87" class="pw-post-body-paragraph kr ks iq kt b ku lo kw kx ky lp la lb kd lq ld le kh lr lg lh kl ls lj lk ll ij bi translated">单词嵌入对于自然语言处理来说是一个非常有用的工具。它们通常作为神经网络的参数被学习，并允许我们将单词映射到数字。更具体地说，它们允许我们将单词映射到高维向量。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mm"><img src="../Images/c758c65ddb0e966e5f0bea7f5c2c8a17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FcBkK-sNUw8ahhWZFDrkPQ.jpeg"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">照片由 Riho Kroll 在 Unsplash 上拍摄</p></figure><h2 id="ceca" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><a class="ae kq" rel="noopener" target="_blank" href="/variance-infused-thinking-49f3e780890d">方差注入式思维</a></h2><p id="2a47" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb kd lc ld le kh lf lg lh kl li lj lk ll ij bi translated">通过<a class="lm ln ep" href="https://medium.com/u/840a3210fbe7?source=post_page-----1a022f3abbac--------------------------------" rel="noopener" target="_blank">饶彤彤</a> — 8 分钟读取</p><p id="da2e" class="pw-post-body-paragraph kr ks iq kt b ku lo kw kx ky lp la lb kd lq ld le kh lr lg lh kl ls lj lk ll ij bi translated">历史看起来是确定的，而且经常被这样解释。本世纪初的科技泡沫显然是一个泡沫，正如所有泡沫都会发生的那样，它最终破裂了。20 世纪 80 年代的日本房地产，2008 年的美国房地产，20000 美元的比特币，杂草股等等也是如此。在崩盘后的几年里，历史学家关注的是风险有多明显，最终的内爆有多不可避免。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mn"><img src="../Images/df5d33b1317babfd9d0f2eb385cde78c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_h9zmvm83Mnk4UMMl5TnzQ.png"/></div></div></figure><h2 id="62c2" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><a class="ae kq" rel="noopener" target="_blank" href="/a-journey-to-airflow-on-kubernetes-472df467f556">Kubernetes 上的气流之旅</a></h2><p id="eef8" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb kd lc ld le kh lf lg lh kl li lj lk ll ij bi translated">由马塞洛·拉韦略·罗西 — 12 分钟读完</p><p id="a026" class="pw-post-body-paragraph kr ks iq kt b ku lo kw kx ky lp la lb kd lq ld le kh lr lg lh kl ls lj lk ll ij bi translated">我对 Apache Airflow 的拙见:基本上，如果您有多个自动化任务要调度，并且您正在摆弄 cron 任务，即使它们的一些依赖项失败了，您也应该尝试一下。</p></div></div>    
</body>
</html>