<html>
<head>
<title>The Ultimate Beginner’s Guide To Implement A Neural Network From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始实现神经网络的最终初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-ultimate-beginners-guide-to-implement-a-neural-network-from-scratch-cf7d52d91e00?source=collection_archive---------26-----------------------#2020-05-14">https://towardsdatascience.com/the-ultimate-beginners-guide-to-implement-a-neural-network-from-scratch-cf7d52d91e00?source=collection_archive---------26-----------------------#2020-05-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="548a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学习神经网络背后的微积分和数学，并将方程式转换成代码。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/caeda94763ab4c5e9b73a32051695b69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*C99phV75NuJi_zJY"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">克林特·王茂林在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="1414" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">摘要</h1><p id="2f66" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">神经网络已经伴随我们很长时间了，但是由于计算机器的进步以及生活各个方面对技术需求的增长，神经网络近年来变得越来越重要。</p><p id="bcfc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从机器学习初学者的角度来看，神经网络被认为是一个黑匣子，它可以摄取一些数据，并根据手头的问题发出预测或分类类别。神经网络的美妙之处在于它们基于简单的微积分和线性代数或者两者的结合。当提供高数量和高质量的数据时，它们一起工作以得出接近准确的结果。</p><p id="f88b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所以，如果你有动力去学习神经网络实际上是如何工作的基础知识，那么你已经到了正确的地方。跟随我这篇文章，你将能够从头开始建立你自己的神经网络。</p><h1 id="a3ef" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">什么是神经网络？</h1><p id="343a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">神经网络的架构源自人脑的结构，而从数学的角度来看，它可以被理解为将一组输入映射到期望输出的函数。这篇文章的主要思想是详细理解这个函数并在 python 中实现它。</p><p id="565e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">神经网络由 7 部分组成:</p><ol class=""><li id="f1cc" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated"><strong class="lq ir">输入层(X) </strong>:该层包含与我们数据集中的特征相对应的值。</li><li id="8743" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><strong class="lq ir">一组权重和偏差(W₁,W₂、..等等)；(b₁,b₂，..etc) </strong>:这些权重和偏差以矩阵的形式表示，它们决定了数据集中每个特性/列的重要性。</li><li id="6983" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><strong class="lq ir">隐藏层</strong>:该层作为神经网络的大脑，也作为输入和输出层之间的接口。神经网络中可以有一个或多个隐藏层。</li><li id="77a2" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><strong class="lq ir">输出层(<em class="nd"> ŷ) </em> </strong>:输入层传输的值将通过隐藏层到达该层。</li><li id="b9e8" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><strong class="lq ir">一组激活函数(A) </strong>:这是向线性模型添加非线性味道的组件。这些功能应用于除输入层之外的每个层的输出，并激活/转换它们。</li><li id="6340" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><strong class="lq ir">损失函数(L) : </strong>该函数计算关于我们的猜测/预测有多好的度量，并且在通过网络反向传播时使用。</li><li id="9135" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><strong class="lq ir">优化器</strong>:这个优化器是一个根据反向传播过程中计算的梯度更新模型参数的函数(你很快就会知道)。</li></ol><h1 id="959a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">学习过程</h1><p id="bdb1" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">神经网络从训练数据中学习/训练，然后使用测试数据测试其性能。培训过程分为两个部分:</p><ol class=""><li id="fec8" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated">正向输送</li><li id="630f" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">反向传播</li></ol><p id="8b36" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">前馈基本上是通过预测一个值从输入层到输出层遍历神经网络。另一方面，反向传播通过计算梯度并通过网络将其推回并最终更新模型参数，使网络实际上进行学习。让我们先看看这两个过程是如何在纸上工作的，然后我们将把我们的方程转换成代码。</p><p id="465e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们首先设计我们的神经网络架构。我们将在本文中使用的架构如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/b6b8945b13b0b4a2c8bd117eeeb31096.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ktgUTrYq6TMUQyPvhnGbWw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">神经网络体系结构</p></figure><h2 id="6250" class="nf kx iq bd ky ng nh dn lc ni nj dp lg lx nk nl li mb nm nn lk mf no np lm nq bi translated">正向输送</h2><p id="7323" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">就我们的架构而言，我们只需要 2 个方程来执行前馈过程，为了将来的目的，您可能还记得:</p><p id="f583" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="nd">神经网络的层数=隐藏层数+ 1(输出层)</em> </strong></p><p id="47ab" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="nd">权重矩阵数=层数</em> </strong></p><p id="458b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们将在本帖中使用<strong class="lq ir"> ReLu </strong>和<strong class="lq ir">s 形</strong>激活函数，但也可以使用其他激活函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/4f3bb05c746e298738a3caa979b7587f.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*Cf1bEOsSYOBcJ329b-qPgw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">方程式-1</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2a7c9d86471209ce955c88b95c40a3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1a4gyfpR_BALhGA_VMBEpw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">方程式 2</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/01d216c63422726f2c7fd9005d54fa8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*HXsQBidbO6brYPnPRdmq0g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Sigmoid 函数定义</p></figure><p id="0471" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里，x、W₁、b₁、W₂和 b₂是矩阵，矩阵级计算在上述两个方程中完成。为了更好地理解，我们可以看看这些代表 x、W₁、b₁、W₂和 b₂.的矩阵在这篇文章中，我们忽略了 b₁和 b₂这两个带有偏见的术语，但是它们可以用类似的方式来处理。</p><p id="fcca" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们先定义几个值:</p><p id="8308" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> n₁ </strong> =输入层神经元数量，<strong class="lq ir"> n₂ </strong> =隐含层神经元数量，<strong class="lq ir"> n₃ </strong> =输出层神经元数量。</p><p id="2016" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，你可能在考虑如何决定输入层、隐藏层和输出层的神经元数量。下面是答案。</p><p id="c81f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">输入层(n₁)中神经元的数量</strong> =数据集中特征/列的数量，即独立变量的数量。</p><p id="b4c5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">隐含层中的神经元数量(n₂) </strong>是灵活的。</p><p id="4c61" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">输出层中神经元的数量(n₃) </strong> =</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/a45637fd78890f780f120fc03284bef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*65qu0lwpiUaRtl2KVxmGzw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输出神经元的数量</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/403a6506903518ce8d007f83746faede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DBhEViu71lDAjoJD79VNeQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代表 X、W1 和 W2 的矩阵</p></figure><p id="8d67" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">x 的尺寸:[米，n₁]；W₁: [n₁，n₂]的维度；</p><p id="7698" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">W₂: [n₂，n₃]的尺寸</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/a9e51df4697ae7ae373766bfd07c7b30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2ojwGyBIz2eWsWsN"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@jeshoots?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">JESHOOTS.COM</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h2 id="9ee9" class="nf kx iq bd ky ng nh dn lc ni nj dp lg lx nk nl li mb nm nn lk mf no np lm nq bi translated">反向传播</h2><p id="5e9c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">反向传播是训练神经网络的重要部分。这是一种基于梯度的学习方法，梯度是使用微积分中最漂亮的部分，即<strong class="lq ir">链规则计算出来的。</strong>这个<strong class="lq ir"> </strong>链规则被重复应用于计算整个网络的梯度。更清楚地说，梯度是损失函数 w.r.t .对模型参数(权重或偏差)的导数。</p><p id="d92c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如前所述，我们不会使用偏差项，只会将权重作为模型参数。因此，我们有两个模型参数，即 W₁和 W₂.我们将使用的损失函数是 MSE(均方误差)。现在让我们看看反向传播在数学上是如何工作的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/12fb6d88bded957da2878b1ef412bf40.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*eDwiHtEbftEM17tKe49pvg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">损失函数</p></figure><p id="673d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中 m=数据集中训练示例/行的数量</p><p id="f8b4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">y=数据集中的地面真实值/标注</p><p id="4b2d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们用链式法则计算 W₂损失梯度:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/f67bc0f4be64c25a9a46d5bf3131d0c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pmwxg0kfDRDEv3ypnFnkRw.png"/></div></div></figure><p id="8d45" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">“Sigmoid_derivative”术语的推导不在本文的上下文中，但在这里<a class="ae kv" href="https://beckernick.github.io/sigmoid-derivative-neural-network/" rel="noopener ugc nofollow" target="_blank">已经有了很好的解释</a>。</p><p id="9b9b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，我们的第一个梯度项就是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/dcbf502c66187623eeb6fd1467f75fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FNuiuYRVJOkVsAbJB1lbpA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">方程式 3</p></figure><p id="ed31" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，进一步，我们以类似的方式计算第二个梯度项，即损失对 W₁的导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/709ae7359f4ce4c8e85afdb963a93b56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9PDmLU_pV9BKTAuDzuX7WA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">方程式 4</p></figure><p id="b1f8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">也可以参考本帖，了解更多关于这些激活函数及其衍生函数的知识。</p><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/activation-functions-neural-networks-1cbd9f8d91d6"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd ir gy z fp oh fr fs oi fu fw ip bi translated">神经网络中的激活函数</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">Sigmoid，tanh，Softmax，ReLU，Leaky ReLU 解释！！！</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq kp oc"/></div></div></a></div><p id="7a2d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，我们已经计算了 W₁和 W₂的梯度，是时候执行优化步骤了。我们将使用<strong class="lq ir">梯度下降优化器</strong>，也称为<strong class="lq ir">普通梯度下降</strong>来更新 W₁和 W₂.你可能知道梯度下降算法有一个名为<strong class="lq ir">学习率</strong>的超参数，它在训练过程中非常重要。这个超参数的调整是非常必要的，并且通过验证测试来完成，这不在本文的范围之内。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/4acae354efd50b9e74415b9a7b1a20d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*pt_nys6B52YxdY5W7u3Xzg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">更新神经网络的权重</p></figure><p id="4394" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在我们已经学习了神经网络背后的数学概念，让我们将这些概念转换为代码，并在现实世界的机器学习问题上训练神经网络。我们将使用著名的 IRIS 数据集来训练我们的网络，然后预测花卉类别。你可以从<a class="ae kv" href="https://www.kaggle.com/uciml/iris" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据集。</p><h1 id="3cb1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">用 Python 实现神经网络</h1><h2 id="0b36" class="nf kx iq bd ky ng nh dn lc ni nj dp lg lx nk nl li mb nm nn lk mf no np lm nq bi translated">导入必要的库</h2><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="e8c5" class="nf kx iq os b gy ow ox l oy oz">import numpy as np<br/>import pandas as pd<br/>from sklearn import preprocessing<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.model_selection import train_test_split</span></pre><h2 id="8cb9" class="nf kx iq bd ky ng nh dn lc ni nj dp lg lx nk nl li mb nm nn lk mf no np lm nq bi translated">数据加载和预处理</h2><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="7180" class="nf kx iq os b gy ow ox l oy oz">data=pd.read_csv('IRIS.csv')<br/>X=data.iloc[:,:-1].values<br/>Y=data.iloc[:,-1].values<br/>StandardScaler=StandardScaler()<br/>X=StandardScaler.fit_transform(X)<br/>label_encoder=LabelEncoder()<br/>Y=label_encoder.fit_transform(Y)<br/>Y=Y.reshape(-1,1)<br/>enc=preprocessing.OneHotEncoder()<br/>enc.fit(Y)<br/>onehotlabels=enc.transform(Y).toarray()<br/>Y=onehotlabels  </span></pre><p id="5cb5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在对输入数据进行上述基本预处理后，我们继续创建我们的神经网络类，它包含前馈和反向传播功能。但是首先，让我们将数据分为训练和测试。</p><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="ca15" class="nf kx iq os b gy ow ox l oy oz">X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)</span></pre><p id="4218" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我也写代码来定义函数，可以计算函数值和函数导数值。我写了 3 个激活函数的代码，只使用了其中的两个(Relu 和 Sigmoid)。但是，您可以尝试这些激活函数的各种组合，也可以在代码中加入其他激活函数。</p><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="d905" class="nf kx iq os b gy ow ox l oy oz">def ReLU(x):<br/>   return (abs(x.astype(float))+x.astype(float))/2</span><span id="eb30" class="nf kx iq os b gy pa ox l oy oz">def ReLU_derivative(x):<br/>   y=x<br/>   np.piecewise(y,[ReLU(y)==0,ReLU(y)==y],[0,1])<br/>   return y</span><span id="27ac" class="nf kx iq os b gy pa ox l oy oz">def tanh(x):<br/>   return np.tanh(x.astype(float))</span><span id="25ac" class="nf kx iq os b gy pa ox l oy oz">def tanh_derivative(x):<br/>   return 1-np.square(tanh(x))</span><span id="7b30" class="nf kx iq os b gy pa ox l oy oz">def sigmoid(x):<br/>   return 1/(1+np.exp(-x.astype(float)))</span><span id="5054" class="nf kx iq os b gy pa ox l oy oz">def sigmoid_derivative(x):<br/>   return x*(1-x)</span></pre><p id="5ccb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">定义神经网络类:</p><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="17a8" class="nf kx iq os b gy ow ox l oy oz">class Neural_Network:<br/>   def __init__(self,x,y,h):<br/>      self.input=x<br/>      self.weights1=np.random.randn(self.input.shape[1],h)<br/>      self.weights2=np.random.randn(h,3)<br/>      self.y=y<br/>      self.output=np.zeros(y.shape)</span></pre><p id="a5c9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">“神经网络”类接受 3 个参数:X、Y 和 h，它们对应于 X 训练、Y 训练和隐藏层中神经元的数量。如前所述，隐藏层中神经元的数量是灵活的，因此在创建类的实例时，我们将把它的值与 X_train 和 Y_train 一起传递。</p><p id="4a2f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">“权重 1”和“权重 2”矩阵由随机分布初始化，并传递相应权重矩阵所需的维数。正如前面已经显示的 W₁和 W₂矩阵及其维数，我们将这些维数传递给这个随机函数来生成我们的初始权重。</p><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="0046" class="nf kx iq os b gy ow ox l oy oz">   def FeedForward(self):<br/>      self.layer1=ReLU(np.dot(self.input,self.weights1))<br/>      self.output=sigmoid(np.dot(self.layer1,self.weights2))</span></pre><p id="0c2a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们在课堂上的第一个功能是“前馈”,这是神经网络训练过程的第一步。代码遵循前面描述的“等式-1”和“等式-2”。</p><p id="464d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，我们进入最重要的部分，反向传播。</p><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="14a8" class="nf kx iq os b gy ow ox l oy oz">def BackPropogation(self):<br/>      m=len(self.input)<br/>      d_weights2=-(1/m)*np.dot(self.layer1.T,(self.y-self.output)       *sigmoid_derivative(self.output))</span><span id="79cc" class="nf kx iq os b gy pa ox l oy oz">d_weights1=-(1/m)*np.dot(self.input.T,(np.dot((self.y-    self.output)*sigmoid_derivative(self.output),self.weights2.T)*    ReLU_derivative(self.layer1)))</span><span id="413a" class="nf kx iq os b gy pa ox l oy oz">self.weights2=self.weights2 - lr*d_weights2<br/>      self.weights1=self.weights1 - lr*d_weights1</span></pre><p id="1ccc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">反向传播步骤计算模型参数的梯度，同时在网络中从输出层返回到输入层。这是由我们用链式法则得到的“方程 3”和“方程 4”决定的。您会发现术语“d_weights2”和“d_weights1”都包含两种运算，即点积和标准乘法。在 NumPy 中，np.dot(X，Y)表示 X 和 Y 之间的矩阵乘法，而 np.multiply(X，Y)或简称为 X*Y 表示 X 和 Y 之间的元素乘法，为了更深入地了解这段代码是如何工作的，我会给你一个小任务，这是我第一次从头开始实现这段代码时自己做的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/b6b8945b13b0b4a2c8bd117eeeb31096.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ktgUTrYq6TMUQyPvhnGbWw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">神经网络体系结构</p></figure><p id="5a32" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果你看隐藏层，<strong class="lq ir">对于每个神经元</strong>(蓝色)，有 3 个连接进入神经元。每一个这样的连接都是从输入层中一个不同的神经元出现的。神经网络的基础表明，隐藏层的蓝色神经元和通过连接接收信息的神经元集之间存在线性关系。这种关系可以用方程式的形式来表示。下面提到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/f33df4c7e6ddadf5e37ccdc16ffe504b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*zlzo1eAkduBcOT-sQpyQZw.png"/></div></figure><p id="cf56" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，将这个方程扩展到隐藏层的每个神经元(蓝色),我们得到 4 个这样的方程，因为我们在隐藏层中有 4 个神经元。到目前为止所做的工作仅代表 1 个训练示例/行。为了将我们的训练数据一次全部发送到前馈函数中，我们将不得不使用矩阵来执行该操作。但是真正的任务是满怀信心地验证(在纸上)上面这段代码(包括点积和元素乘法)是否与我们讨论的这 4 个线性方程匹配。但这一次，我们将同时对所有训练数据行应用这些等式。</p><p id="62e4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我希望你喜欢亲手解决矩阵和方程，但相信我，这是值得花时间的。</p><p id="2ebb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，我们定义一个函数，该函数将在模型被训练后在运行时预测输出类。</p><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="f759" class="nf kx iq os b gy ow ox l oy oz">def predict(self,X):<br/>   self.layert_1=ReLU(np.dot(X,self.weights1))<br/>   return sigmoid(np.dot(self.layert_1,self.weights2))</span></pre><p id="66e8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是“Neural_Network”类中的最后一个函数,“predict”函数只使用最终优化的权重进行正常的前馈。</p><p id="d6ac" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在我们已经定义了我们的类，是时候实际使用它了。</p><pre class="kg kh ki kj gt or os ot ou aw ov bi"><span id="ed99" class="nf kx iq os b gy ow ox l oy oz">#----Defining parameters and instantiating Neural_Network class----#</span><span id="bcd3" class="nf kx iq os b gy pa ox l oy oz">epochs=10000 #Number of training iterations<br/>lr=0.5 # learning rate used in Gradient Descent<br/>n=len(X_test)<br/>m=len(X)</span><span id="4c4d" class="nf kx iq os b gy pa ox l oy oz">nn1=Neural_Network(X_train,Y_train)<br/>#creating an object of Neural_Network class</span><span id="6de9" class="nf kx iq os b gy pa ox l oy oz">#----------------------Training Starts-----------------------------#<br/>for i in range(epochs):<br/>   nn1.FeedForward()<br/>   y_predict_train=enc.inverse_transform(nn1.output.round())<br/>   y_predict_test=enc.inverse_transform(nn1.predict(X_test).round())<br/>   y_train=enc.inverse_transform(Y_train)<br/>   y_test=enc.inverse_transform(Y_test)<br/>   train_accuracy=(m-np.count_nonzero(y_train-y_predict_train))/m<br/>   test_accuracy=(n-np.count_nonzero(y_test-y_predict_test))/n<br/>   nn1.BackPropogation()<br/>   cost=(1/m)*np.sum(np.square(nn1.y-nn1.output))<br/>   print("Epoch {}/{}  ==============================================================:- ".format(i+1,epochs))</span><span id="988c" class="nf kx iq os b gy pa ox l oy oz">#----------------Displaying Final Metrics--------------------------#<br/>print("MSE_Cost: {} , Train_Accuracy: {} , Test_Accuracy: {} ".format(cost,train_accuracy,test_accuracy))</span></pre><p id="ec8e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">经过 10000 个纪元的训练，结果如下:</p><p id="bad3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">训练精度:0.97777 </strong></p><p id="ab57" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">测试精度:0.9 </strong></p><p id="204a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当应用于手头的相同问题时，与其他机器学习模型相比，结果看起来令人惊讶。这个网络可以扩展到一个 3 层网络，它将有两个使用相同概念的隐藏层。你也可以尝试不同的优化程序，如 AdaGrad，Adam 等。</p><p id="c4d1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">您可以访问我在“成人收入预测”数据集上的额外工作，该数据集在我的 GitHub 存储库中使用了类似的神经网络，链接如下:</p><div class="nz oa gp gr ob oc"><a href="https://github.com/Ravsehajsinghpuri/Neural_Network_from_scratch" rel="noopener  ugc nofollow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd ir gy z fp oh fr fs oi fu fw ip bi translated">ravsehajsinghpuri/Neural _ Network _ 从头开始</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">嗨伙计们！这是我第一次尝试用简单的数学和 python 神奇的…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">github.com</p></div></div><div class="ol l"><div class="pc l on oo op ol oq kp oc"/></div></div></a></div><h1 id="a6d8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="2796" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在这篇文章中，我们开始学习神经网络的基本架构，并进一步研究神经网络架构的组件。我们还见证了其工作背后的数学，并详细解释了链规则和基本微积分在开发反向传播方程中的应用，这是训练过程的主要部分。最后，我们将我们的数学概念和方程转换成代码，并在 IRIS 数据集上实现了一个成熟的神经网络。</p></div><div class="ab cl pd pe hu pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="ij ik il im in"><p id="052d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">非常感谢您通读这篇文章。我希望你理解它的每一个方面。欢迎在回复部分提出任何问题。</p></div></div>    
</body>
</html>