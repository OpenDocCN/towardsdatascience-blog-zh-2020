<html>
<head>
<title>Can we perform NLP on unfamiliar (natural) languages?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我们可以在不熟悉的(自然)语言上执行NLP吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/can-we-perform-nlp-on-unfamiliar-natural-languages-138f6ea4af13?source=collection_archive---------14-----------------------#2020-03-01">https://towardsdatascience.com/can-we-perform-nlp-on-unfamiliar-natural-languages-138f6ea4af13?source=collection_archive---------14-----------------------#2020-03-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9a1c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用保加利亚语造句比真正理解它容易吗？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d748f1db94ba7b12b8793442988d1141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KWEkZHaZxfcWILzy"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">叶卡捷琳娜·诺维茨卡娅在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="5b7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇博文试图用一种我不熟悉的语言——保加利亚语来执行各种自然语言处理(NLP)。我将尝试探索这些数据，并在最后通过使用包含BiLSTM(双向长短期记忆)层的机器学习模型来生成一些保加利亚语句子。</p><p id="1d5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了让我享受这个实验，我将使用保加利亚最著名的歌手/说唱歌手之一Krisko写的歌词作为文本数据。这里你可以看到克里斯科最近的歌曲，名为“<a class="ae ky" href="https://www.youtube.com/watch?v=z_2xJOLOxTA" rel="noopener ugc nofollow" target="_blank">”даилине</a>”——意思是“是”或“不是”</p><p id="e21f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章的代码可以在我的GitHub页面上找到。</p><p id="d47f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">*所有歌词均来自<a class="ae ky" href="https://lyricstranslate.com/en/krisko-lyrics.html" rel="noopener ugc nofollow" target="_blank">歌词翻译</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/99d2d508e5369bf67b91b8e718af95fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*vX9W6rVfkYZRWdZgnZzpbA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这是克里斯科:来自他的<a class="ae ky" href="https://www.instagram.com/p/ByNlgYVFa8u/" rel="noopener ugc nofollow" target="_blank"> Instagram </a></p></figure><h2 id="d6f1" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">一点点保加利亚语言和音乐介绍</h2><blockquote class="mp mq mr"><p id="659e" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><em class="it">保加利亚语是南斯拉夫语的一种，主要在保加利亚使用。保加利亚语是用西里尔文写的(事实上是欧盟官方语言中唯一用西里尔文写的语言)。西里尔文是由圣徒西里尔和卫理公会于893年在第一个保加利亚帝国发明的。</em></p><p id="b847" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated">我个人喜欢保加利亚语，经常听保加利亚歌曲，尽管我并不真正理解它们的含义。保加利亚音乐非常有趣，当然音乐很好，但也受到了来自土耳其、希腊、塞尔维亚等邻国的深刻影响，尤其是一种叫做“Chalga”的民间流行音乐，你可以欣赏到这些不同音乐的完美融合。在这里你可以听一下艾米莉亚的一首名为<a class="ae ky" href="https://www.youtube.com/watch?v=pY_uq9zLSNo" rel="noopener ugc nofollow" target="_blank"><em class="it">акула</em></a><em class="it">的歌曲作为例子。</em></p></blockquote></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="573f" class="nd lx it bd ly ne nf ng mb nh ni nj me jz nk ka mh kc nl kd mk kf nm kg mn nn bi translated">目录</h1><ol class=""><li id="20bd" class="no np it lb b lc nq lf nr li ns lm nt lq nu lu nv nw nx ny bi translated"><strong class="lb iu">探索文本数据</strong></li><li id="bd7d" class="no np it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><strong class="lb iu">预处理文本数据</strong></li><li id="e16e" class="no np it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><strong class="lb iu">使用word2vec探索单词关系</strong></li><li id="39c6" class="no np it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><strong class="lb iu">构建BiLSTM模型以生成保加利亚语句子</strong></li><li id="e5cd" class="no np it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><strong class="lb iu">结论和后续步骤</strong></li></ol></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="69af" class="nd lx it bd ly ne nf ng mb nh ni nj me jz nk ka mh kc nl kd mk kf nm kg mn nn bi translated"><strong class="ak"> 1。探索文本数据</strong></h1><p id="1c2f" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">由于我不知道保加利亚语语法如何工作，甚至不知道许多基本单词，我将尝试在不做任何预处理的情况下探索数据集(克里斯科的歌词)。</p><p id="b505" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我会检查他的歌曲中经常使用的词是什么。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="b248" class="lw lx it oi b gy om on l oo op">import io</span><span id="2a32" class="lw lx it oi b gy oq on l oo op"># Read the data<br/>with io.open('/resources/data/krisko_lyrics.txt', encoding='utf-8') as f:</span><span id="bf19" class="lw lx it oi b gy oq on l oo op">krisko_text = f.read().lower().replace('\n', ' \n ')</span><span id="0a2d" class="lw lx it oi b gy oq on l oo op"># Split into words and make a dataframe<br/>words = [w for w in <br/>         krisko_text.split(' ') if w.strip() != '' or w == '\n']</span><span id="f2c5" class="lw lx it oi b gy oq on l oo op">print('Corpus length in words:', len(words))</span><span id="03e4" class="lw lx it oi b gy oq on l oo op">word_df = pd.DataFrame(words,columns=['word'])</span></pre><p id="c5de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在数据集中，有36首歌曲，16453个单词和3741个独特的单词。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="e8ab" class="lw lx it oi b gy om on l oo op">top = word_df.word.value_counts().head(30)</span><span id="0602" class="lw lx it oi b gy oq on l oo op">print(top)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/340ab754a17021c90d70c7e89decc39e.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*tGjpd45RmcbjBQqzPOCavQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最常用的30个单词</p></figure><p id="78b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以由此创造出单词cloud来可视化。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="1da4" class="lw lx it oi b gy om on l oo op"># For word cloud<br/>from wordcloud import WordCloud, ImageColorGenerator<br/>import matplotlib.pyplot as plt</span><span id="f0ea" class="lw lx it oi b gy oq on l oo op">wc = WordCloud(background_color="white", max_words=100,<br/>               max_font_size=50, random_state=42)</span><span id="a6cc" class="lw lx it oi b gy oq on l oo op">output = wc.generate(krisko_text)</span><span id="2c37" class="lw lx it oi b gy oq on l oo op">plt.figure()<br/>plt.imshow(output,interpolation='bilinear')<br/>plt.axis('off')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/877b1d93f30adb55f7f4341fff5f974d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zujUqF-kG8cN6yWsmz1H3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">常用词(？)用在克里斯科的歌里</p></figure><p id="2d38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这看起来不错，对不对？不幸的是，它们看起来都像停用词，如“是”、“和”、“否”等。为了理解数据集，我需要删除这些停用词。我们将会看到这个过程是如何完成的，会有什么不同。</p><h1 id="d12d" class="nd lx it bd ly ne ot ng mb nh ou nj me jz ov ka mh kc ow kd mk kf ox kg mn nn bi translated">2.<strong class="ak">预处理文本数据</strong></h1><p id="399d" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">我在<a class="ae ky" href="https://github.com/explosion/spaCy/blob/master/spacy/lang/bg/stop_words.py" rel="noopener ugc nofollow" target="_blank">空间</a>中找到了保加利亚语的停用词列表，让我们使用它并从这次探索中移除那些停用词。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="af1c" class="lw lx it oi b gy om on l oo op"># Import Bulgarian stop words<br/>from spacy.lang.bg.stop_words import STOP_WORDS as BG_STOPWORDS<br/></span><span id="2a79" class="lw lx it oi b gy oq on l oo op"># Take punctuations out<br/>krisko_cleaned = re.sub('\[,.*?“”…\]', '', krisko_text)</span><span id="7a29" class="lw lx it oi b gy oq on l oo op"># Tokenise the data<br/>TOKENS = word_tokenize(krisko_cleaned) </span><span id="57d8" class="lw lx it oi b gy oq on l oo op"># Add some more stop words<br/>EXTRA_STOPWORDS = {'теб','дон','кво','к\'во','бях','мене','нашият','ма','ше','yeah',           'недей','ей','ко','bang','ам','тебе','you','тука','мойта','тва',                    'але-але-алелуя','кат','tak','моа','оп','о','ся',<br/>'та','тез','дето','ја','aз','tik','i','ѝ','ток','твоя',                    'a','some','ideal','petroff','–','так','тия','ee','к'}</span><span id="564f" class="lw lx it oi b gy oq on l oo op">BG_STOPWORDS.update(EXTRA_STOPWORDS)</span><span id="51b8" class="lw lx it oi b gy oq on l oo op"># Filter those stop words out<br/>filtered_sentence = []<br/>  <br/>for w in TOKENS: <br/>    if w not in BG_STOPWORDS: <br/>        filtered_sentence.append(w)</span><span id="9e78" class="lw lx it oi b gy oq on l oo op">with open ('~/resources/data/krisko_cleaned.txt','w') as output:<br/>    output.write(str(filtered_sentence))</span><span id="3603" class="lw lx it oi b gy oq on l oo op">with io.open('~/resources/data/krisko_cleaned.txt',<br/>                 encoding='utf-8') as f:<br/>    krisko_cleaned = f.read()</span><span id="aac7" class="lw lx it oi b gy oq on l oo op">krisko_cleaned = re.sub('[%s]' % <br/>                 re.escape(string.punctuation), '', krisko_cleaned)</span></pre><p id="fea2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">删除这些单词后，我们可以创建热门单词列表。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/9639b6fe073e9406993c886d7a2b03f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*dV8sE-c631dLBIniFsxT0Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">删除停用词后的前30个词</p></figure><p id="ba33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，这个列表看起来比以前更好。似乎这里所有的单词都有一些含义。最常用的词是‘дай’(дам的第二人称单数祈使形式)，意思是‘给予，允许’。第二个是‘искам’(第一人称单数祈使句)，意思是‘想要’。</p><p id="441f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有趣的一点是，单词的结尾会根据主题而变化。正如我上面提到的，'我想要'是'искам'，'你想要'是'искаш'，她/他/它想要'是'иска'，等等……所以我们实际上可以看出'我想要'和'你想要'之间的区别，而不像在英语中那样不知道主语。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/66fabf5bf821bef06784150e72c16fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6uuz3arXH_EfsnggUg1PWw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">清理数据后的WordCloud</p></figure><p id="0142" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里是清理数据后的字云。看起来比上一个信息丰富多了。所以我在这幅图中看到的是‘искам’，'обичам'，'ръъм'，‘кииско’，…所以基本上，克里斯科唱的东西就像他想要并喜欢朗姆酒(？)——这完全说得通。</p><h1 id="4a8e" class="nd lx it bd ly ne ot ng mb nh ou nj me jz ov ka mh kc ow kd mk kf ox kg mn nn bi translated"><strong class="ak"> 3。使用word2vec探索单词关系</strong></h1><p id="be82" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">清理完数据后，我们可以通过word2vec算法看到单词之间的关系。Word2vec算法是在给定输入语料库的情况下产生单词嵌入(向量)的模型。利用这一点，我们可以看到，例如，相似的词，预测下一个词，词之间的距离。</p><p id="db58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面你可以找到我是如何建立我的模型的。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="3f75" class="lw lx it oi b gy om on l oo op"># Modules for word2vec<br/>from nltk.tokenize import sent_tokenize, word_tokenize <br/>import gensim <br/>from gensim.models import Word2Vec <br/>from gensim.models import Phrases<br/></span><span id="c5b4" class="lw lx it oi b gy oq on l oo op"># preparation<br/>input_words = []</span><span id="317f" class="lw lx it oi b gy oq on l oo op">for i in sent_tokenize(krisko_cleaned): <br/>    temp = [] <br/>      <br/>    # tokenize the sentence into words <br/>    for j in word_tokenize(i): <br/>        temp.append(j) <br/>  <br/>    input_words.append(temp)</span><span id="540e" class="lw lx it oi b gy oq on l oo op"># Create a word2vec model<br/>w2vModel = Word2Vec(input_words, min_count=1, size=1000, window=1)</span></pre><p id="349d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">word2vec中有几个参数；min_count是输入单词中单词的最小频率——在我的模型中，我将它设置为1，这样就包含了所有的单词。<em class="ms">大小</em>是维度的数量，而<em class="ms">窗口</em>是句子中当前单词和预测单词之间的最大距离。</p><p id="5b43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使用这个模型来探索数据集。首先，我们可以看到单词之间的相似性。作为一个例子，我可以用“女人”和“男人”这两个词来看它们的相似性。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="f2b8" class="lw lx it oi b gy om on l oo op"># Investigate the similarity between man and woman</span><span id="d1f4" class="lw lx it oi b gy oq on l oo op">print('Similarity between мъж and жена:<br/>                         '+str(w2vModel.similarity('мъж','жена')))</span><span id="6996" class="lw lx it oi b gy oq on l oo op">Similarity between мъж and жена: -0.026726926</span></pre><p id="37cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它们的相似度和-0.0267，也就是说那两个词并不像我们预期的那样彼此相似。现在我们可以看到男性和女性相似的词，看看他们之间是否也有一些差异，也可能是用法的上下文。下面你可以看到男人和女人相似词汇的列表。该列表包含三列，相似词，其翻译，以及-1和1之间的相似度范围。由于我不知道很多保加利亚语，我用谷歌翻译创建了一个翻译专栏。由于一些翻译看起来有点奇怪，我怀疑一些相似的单词可能不正确。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="66a5" class="lw lx it oi b gy om on l oo op"># Creating the most similar words list for man and woman</span><span id="607c" class="lw lx it oi b gy oq on l oo op">man_df = pd.DataFrame(w2vModel.most_similar('мъж'),columns=['similar_word','similarity'])<br/>translate = ['chief','household','tiger','goes away','we dream',<br/>             'lake','I ran','voodoo','haymans','stop']<br/>man_df['translation'] = translate<br/>man_df = man_df[['similar_word','translation','similarity']]</span><span id="0e87" class="lw lx it oi b gy oq on l oo op">woman_df = pd.DataFrame(w2vModel.most_similar('жена'),columns=['similar_word','similarity'])<br/>translate = ['bucket','noisy','I know','you come',<br/>             'polo','folk','ashamed','bon','bat','opportunity']<br/>woman_df['translation'] = translate<br/>woman_df = woman_df[['similar_word','translation','similarity']]</span></pre><div class="kj kk kl km gt ab cb"><figure class="pa kn pb pc pd pe pf paragraph-image"><img src="../Images/821464a766ff7a6d9eb5d1d1731bb04b.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*bEm8vDA0t9I_q8g2FzPjaQ.png"/></figure><figure class="pa kn pg pc pd pe pf paragraph-image"><img src="../Images/c1bb5d8a514d4ab60261b8a8438d70ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*XHbeK9uXRYHgCG6NL74eOw.png"/><p class="ku kv gj gh gi kw kx bd b be z dk ph di pi pj translated">与мъж(男)(左)和жена(女)(右)相似的词</p></figure></div><p id="6161" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嗯，由于我还没有消除性别刻板印象的偏见([1] Bolukbasi et al .，2016)，我们仍然可以看到它。但与此同时，这篇博文的主要目的是探索数据集并生成保加利亚语的Krisko-ish句子，所以我认为最好不要去偏向以获得更多的Krisko味道。</p><p id="ac76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以在克里斯科的歌词中,“男人”这个词经常用在“男性”的语境中，如“首领”、“家庭”和“老虎”,而“女人”这个词类似于一点点负面的词，如“吵闹”和“羞愧”。</p><p id="054d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在继续讨论“我想要”和“你想要”之间的相似之处。正如我上面提到的，我们可以在保加利亚语中看到这两个词的不同上下文。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="8a79" class="lw lx it oi b gy om on l oo op">print('Similarity between искам and искаш:<br/>        '<strong class="oi iu">+</strong>str(w2vModel.similarity('искам','искаш')))</span><span id="1dc4" class="lw lx it oi b gy oq on l oo op">Similarity between искам and искаш: 0.048260797</span></pre><p id="1e94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我所料，这两个词非常不同，尽管唯一的区别是动词的主语。现在我们可以探讨一下围绕那两个词用了什么样的词。因为这两个词的相似度很低，所以我认为它们周围的词也应该很不一样。</p><div class="kj kk kl km gt ab cb"><figure class="pa kn pk pc pd pe pf paragraph-image"><img src="../Images/12085b8e178520e5f42e993fe46a777b.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*N3SiMOVipgk9Qm8CmyNVBg.png"/></figure><figure class="pa kn pl pc pd pe pf paragraph-image"><img src="../Images/0489aecfc0e909abc2cd4877b555910d.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*O2adR8N7Pcjp4-Ccs7SGWg.png"/><p class="ku kv gj gh gi kw kx bd b be z dk pm di pn pj translated">下一个词预测:искам(我要)(左)和искаш(你要)(右)</p></figure></div><p id="ba75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与我的猜测相反，我们并没有真正观察到它们之间的巨大差异。公平地说，列表只有10个单词，概率是相同的，所以如果我扩展列表，我们可能会看到一些明显的差异。</p><p id="f05e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我能感觉到的一件事是‘我想要’这个词更男性化，比如‘我想要小鸡’、‘我想抽烟’(语法不正确)和‘我想做爱’。所以我猜克里斯科在用“我”的时候经常指的是他自己。而且我猜“你”通常指的是一个女孩，因为我在他的音乐视频中可以看到很多女孩(从词表上看不出来)。</p><p id="849d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以画出他的歌名的单词嵌入，来看它们之间的二维距离。我在这里使用的标题仅限于那些只包含一个单词的标题。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="23bc" class="lw lx it oi b gy om on l oo op"># Inspired by Stanford's <a class="ae ky" href="https://web.stanford.edu/class/cs224n/materials/Gensim%20word%20vector%20visualization.html" rel="noopener ugc nofollow" target="_blank">CS coursework</a></span><span id="1b05" class="lw lx it oi b gy oq on l oo op">def display_pca_scatterplot(model, words):<br/>    <br/>    word_vectors = np.array([model[w] for w in words])</span><span id="41cc" class="lw lx it oi b gy oq on l oo op">    twodim = PCA().fit_transform(word_vectors)[:,:2]<br/>    <br/>    plt.figure(figsize=(12,12))<br/>    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')<br/>    for word, (x,y) in zip(words, twodim):<br/>        plt.text(x+0.0003, y-0.0001, word)<br/></span><span id="c93e" class="lw lx it oi b gy oq on l oo op">display_pca_scatterplot(w2vModel,['базука','афтърмен',<br/>                                  'горила','антилопа',<br/>                                  'фотошоп','късай','герои',<br/>                                  'мърдай','наздраве',<br/>                                  'незаменим'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/8348705b4e1c9aa583419289cfd31180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p4SG5A43H5Qwb2O_3fLc2w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯科的歌词单词嵌入</p></figure><p id="3b11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本次歌名列表中有三首歌词不是克里斯科演唱的，分别是<a class="ae ky" href="https://www.youtube.com/watch?v=S37CvSvQN7o" rel="noopener ugc nofollow" target="_blank">антилопа</a>(antilope)<a class="ae ky" href="https://www.youtube.com/watch?v=uCOhwj7J9bo" rel="noopener ugc nofollow" target="_blank">фотошоп</a>(Photoshop)<a class="ae ky" href="https://www.youtube.com/watch?v=56b7a_FJzpQ" rel="noopener ugc nofollow" target="_blank">късай</a>(tear)。这些歌词是克里斯科写的，但由蒂塔演唱。令人惊讶的是，这三首歌彼此并不相似，尤其是《късай》与其他所有歌曲相去甚远。还有<a class="ae ky" href="https://www.youtube.com/watch?v=JLtaUvhTPsc" rel="noopener ugc nofollow" target="_blank">горила</a>(大猩猩)是一首由2BONA和Krisko演唱的歌曲，但似乎与其他歌曲没有太大区别。<a class="ae ky" href="https://www.youtube.com/watch?v=Ig5kvpoEH0I" rel="noopener ugc nofollow" target="_blank">мърдай</a>(摆动)在右手边，这很有意义，因为这首歌是由多名歌手演唱的(克里斯科、迪姆和боропърви).</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/3acb448e52f3d2c8f7798fb8691062d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d44SO03RP4pwVkH1azq05g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这是蒂塔:来自她的Instagram</p></figure><h1 id="be3a" class="nd lx it bd ly ne ot ng mb nh ou nj me jz ov ka mh kc ow kd mk kf ox kg mn nn bi translated"><strong class="ak"> 4。构建一个BiLSTM模型来生成保加利亚语句子</strong></h1><p id="4063" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">现在，我们将看看是否可以通过从Krisko的歌词数据建立一个模型来生成一些保加利亚语的句子。该模型将包括嵌入和双向LSTM (BiLSTM)层。</p><p id="2cbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LSTM是RNN(递归神经网络)体系结构之一。虽然正常的rnn在输入序列中仅具有长期依赖性，但是LSTM具有输入、遗忘和输出门，并且这些门使得能够具有随机的短期依赖性。与普通的LSTM(单向)不同，BiLSTM从前后两个方向运行输入数据。因此，它不仅有助于理解过去单词的上下文。</p><p id="473c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了建立一个模型，首先，我们需要准备输入序列。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="fe72" class="lw lx it oi b gy om on l oo op"># For BiLSTM model</span><span id="5c38" class="lw lx it oi b gy oq on l oo op">import feather<br/>import tensorflow as tf<br/>from tensorflow import keras<br/>from keras.models import Model, Sequential<br/>from keras.layers import Input, Dense, Embedding, Concatenate, Flatten, LSTM, Dropout, Reshape, Activation,Bidirectional,Layer<br/>from keras.losses import binary_crossentropy<br/>from keras import optimizers,activations,initializers<br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences</span><span id="4885" class="lw lx it oi b gy oq on l oo op">tf.compat.v1.set_random_seed(42)<br/>tokenizer = Tokenizer()</span><span id="7468" class="lw lx it oi b gy oq on l oo op"># Make the text data into input sequences</span><span id="1c8b" class="lw lx it oi b gy oq on l oo op">corpus = [w for w in krisko_cleaned).split('\n') <br/>          if w.strip() != '' or w == '\n']</span><span id="8078" class="lw lx it oi b gy oq on l oo op">tokenizer.fit_on_texts(corpus)</span><span id="a7ca" class="lw lx it oi b gy oq on l oo op">TOTAL_WORDS = len(tokenizer.word_index)+1</span><span id="f06a" class="lw lx it oi b gy oq on l oo op">input_seq = []</span><span id="1edf" class="lw lx it oi b gy oq on l oo op">for line in corpus:<br/>    token_list = tokenizer.texts_to_sequences([line])[0]<br/>    for i in range(1,len(token_list)):<br/>        n_gram_seq = token_list[:i+1]<br/>        input_seq.append(n_gram_seq)</span><span id="098b" class="lw lx it oi b gy oq on l oo op">MAX_SEQ_LEN = max([len(x) for x in input_seq])</span><span id="01d8" class="lw lx it oi b gy oq on l oo op">input_sequences = np.array(pad_sequences(input_seq,<br/>                           maxlen=MAX_SEQ_LEN, padding='pre'))</span><span id="7662" class="lw lx it oi b gy oq on l oo op">xs = input_sequences[:,:-1]<br/>labels = input_sequences[:,-1]<br/>ys = tf.keras.utils.to_categorical(labels, num_classes=TOTAL_WORDS)</span></pre><p id="f598" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我是这样建立模型的。下面这个模型是我做的第一个。这是一个单层的BiLSTM模型。首先，创建单词嵌入，然后将它们通过BiLSTM层，最后，使用softmax计算输出类的概率。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="0239" class="lw lx it oi b gy om on l oo op"># Build a model</span><span id="71da" class="lw lx it oi b gy oq on l oo op">BATCH_SIZE = 128</span><span id="d7ad" class="lw lx it oi b gy oq on l oo op">model = Sequential()<br/>model.add(Embedding(TOTAL_WORDS, 128, input_length=MAX_SEQ_LEN-1))<br/>model.add(Dropout(.5))<br/>model.add(Bidirectional(<br/>              LSTM(128,return_sequences=False,<br/>                   kernel_initializer='random_uniform')))<br/>model.add(Dropout(.5))<br/>model.add(Dense(TOTAL_WORDS, activation='softmax'))</span><span id="b822" class="lw lx it oi b gy oq on l oo op">model.compile(loss='categorical_crossentropy',<br/>              optimizer=optimizers.Adam(lr=0.1,decay=.0001),<br/>              metrics=['accuracy'])</span><span id="71eb" class="lw lx it oi b gy oq on l oo op">model.fit(xs,ys,epochs=500,verbose=1)</span></pre><p id="2590" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将使用我创建的模型生成句子。我们将生成四个句子，以“我想要(искам)”、“你想要(искаш)')、“我是(азсъм)')”和“你是(тиси)”开始。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="2104" class="lw lx it oi b gy om on l oo op">def textGenerator(seedText, textLength):</span><span id="7eef" class="lw lx it oi b gy oq on l oo op">    for _ in range(textLength):<br/>        token_list = tokenizer.texts_to_sequences([seedText])[0]<br/>        token_list = pad_sequences([token_list], <br/>                                   maxlen = MAX_SEQ_LEN - 1,<br/>                                   padding = 'pre')<br/>        pred = model.predict_classes(token_list,verbose=0)</span><span id="ddbb" class="lw lx it oi b gy oq on l oo op">output_word = ""</span><span id="8b7c" class="lw lx it oi b gy oq on l oo op">        for word, index in tokenizer.word_index.items():<br/>            if index == pred:<br/>                output_word = word<br/>                break<br/>        seedText += " " + output_word<br/>    return print(seedText)<br/></span><span id="2982" class="lw lx it oi b gy oq on l oo op">textGenerator('искам',MAX_SEQ_LEN-1)<br/>textGenerator('искаш',MAX_SEQ_LEN-1)<br/>textGenerator('Аз съм',MAX_SEQ_LEN-2)<br/>textGenerator('Ти си',MAX_SEQ_LEN-2)</span></pre><p id="d115" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些是模型的结果。为了理解这个句子，我用了谷歌翻译(有些翻译似乎很离谱)。</p><p id="30fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">具有128个单元- 500个时期的单个BiLSTM层</strong></p><blockquote class="mp mq mr"><p id="513f" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated">любовта——(我想给</p><p id="c873" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated">искашдаседаседасеасеаасеаасеасееаасееасееасеееаселюбовта——(你想给就给，给，给，做爱)</p><p id="efab" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Аз съм </strong> да се да се да се да се да се да се да се да любовта - (I am to give to give to give, to give to give to love)</p><p id="3508" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Ти си </strong> е да ме да ме да ме да ме да ме да ме да ме любовта - (You are to give me to give me to give me to give me to love me)</p></blockquote><p id="9128" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">This model didn’t learn much as we can see the predictions are all same for those seed text. Now increase the number of units.</p><p id="64dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Single BiLSTM layer with 256 units — 500 epochs </strong></p><blockquote class="mp mq mr"><p id="a81d" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Искам </strong> да източа не е важно е е е е е е е е е е или би - (I want to drain is not important is is is is is is is is or is)</p><p id="ee1e" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Искаш </strong> мандарините ли да те закарам в мойта душа и да се повозим и да се повозим от - (Do you want tangerines to drive you into my soul and to ride and ride from)</p><p id="9703" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Аз съм </strong> навсякъде където има ли има ли пожари да се оправя да се оправя да ме с - (I am everywhere where there are fires to handle to handle me with)</p><p id="8165" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Ти си </strong> моят замък да се качиш и аз съм до теб жега е до лягане да телефона - (You’re my castle to get on and I’m up to you heat is to bed to phone)</p></blockquote><p id="ff3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">We can see some improvements (at least the predictions are not the same for all the seed texts). But still not something we can say these are actual sentences. Continue to train until 1000 epochs.</p><p id="3afa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Single BiLSTM layer with 256 units — 1000 epochs </strong></p><blockquote class="mp mq mr"><p id="3b82" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Искам </strong> да избягаме е е е лесна работа ти си риба и аз бързо бързо бързо бързо бързо - (I want to escape is is easy job you are a fish and I fast fast fast fast fast)</p><p id="575a" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Искаш </strong> ли да се повозим в мойта кола да се повозим в мойта кола да се повозим на - (Do you want to drive in my car to drive in my car to drive in)</p><p id="3f82" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Аз съм </strong> slick да се гушка playmate трето поколение да се държи ма буден да се огледаш барна - (I’m slick to playmate third generation cuddle keep me awake to look around the bar)</p><p id="f8da" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Ти си </strong> моят замък е до теб жега е до теб жега е до теб жега е вързана - (You are my castle is to you heat is to you heat is to you heat is tied)</p></blockquote><p id="7a71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">I feel the model is getting better. Some sentences kinda make sense. Now put one more BiLSTM layer and train 1000 epochs.</p><p id="c91e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Double BiLSTM layer with 256 units — 1000 epochs </strong></p><blockquote class="mp mq mr"><p id="a242" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Искам </strong> един танц на не дължа да се разбивам да се да се моля бе на фитнес запали - (I want a dance I don’t owe to break to pray it was on fitness lights)</p><p id="1b66" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Искаш </strong> да се възгордяваш да се възгордяваш няма никой те боли и не мога да върна духат любовта - (You want to be proud to be proud no one hurts you and I can’t bring back love)</p><p id="2fc9" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated"><strong class="lb iu"> Аз съм </strong> купидон да се качиш да се моля да се фука напиеме теб ми бе се предавам - (I’m a cupid to get up to pray fuck you I was giving up)</p><p id="be53" class="kz la ms lb b lc ld ju le lf lg jx lh mt lj lk ll mu ln lo lp mv lr ls lt lu im bi translated">жена、моля、моля、моля和любовта</p></blockquote><p id="339f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很难说这些是否比以前的预测更好，但这个模型也产生了一些可以理解的句子。</p><p id="a990" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以这个实验的答案是:用一种不熟悉的语言生成句子是可能的，但是，你可能不明白这个模型生成的句子。由于不熟悉这门语言，很难使用n-gram这样的技术。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/d6c76577caf10279271a69ae8adcf2e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YvKULHyBdoYMPcHG"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@dougmaloney?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">道格·马洛尼</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="2297" class="nd lx it bd ly ne nf ng mb nh ni nj me jz nk ka mh kc nl kd mk kf nm kg mn nn bi translated">结论和后续步骤</h1><p id="58a8" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">这是一个非常有趣的实验。这个帖子通过了</p><ul class=""><li id="9140" class="no np it lb b lc ld lf lg li pr lm ps lq pt lu pu nw nx ny bi translated">数据探索</li><li id="3dfc" class="no np it lb b lc nz lf oa li ob lm oc lq od lu pu nw nx ny bi translated">创建单词云可视化</li><li id="adf4" class="no np it lb b lc nz lf oa li ob lm oc lq od lu pu nw nx ny bi translated">使用word2vec探索更多信息</li><li id="6af4" class="no np it lb b lc nz lf oa li ob lm oc lq od lu pu nw nx ny bi translated">使用BiLSTM层构建文本生成器模型</li></ul><p id="434f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来的步骤是</p><ul class=""><li id="e011" class="no np it lb b lc ld lf lg li pr lm ps lq pt lu pu nw nx ny bi translated">生成歌词(可能需要更多数据)</li><li id="8b5d" class="no np it lb b lc nz lf oa li ob lm oc lq od lu pu nw nx ny bi translated">尝试skipgram</li><li id="5443" class="no np it lb b lc nz lf oa li ob lm oc lq od lu pu nw nx ny bi translated">提高包括<eos>和<sos>的数据质量，而不是预测固定长度的句子</sos></eos></li><li id="9054" class="no np it lb b lc nz lf oa li ob lm oc lq od lu pu nw nx ny bi translated">创建文本生成器API</li></ul></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><p id="df11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望你对这篇文章感兴趣。如果你想和我联系，这是我的LinkedIn页面。</p><h1 id="f82c" class="nd lx it bd ly ne ot ng mb nh ou nj me jz ov ka mh kc ow kd mk kf ox kg mn nn bi translated">参考</h1><p id="adb1" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">[1]t . Bolukbasi，Chang，K. W .，Zou，J. Y .，Saligrama，v .，&amp; Kalai，A. T. (2016年)。男人对于电脑程序员就像女人对于家庭主妇一样？去偏置词嵌入。在<em class="ms">神经信息处理系统的进展</em>(第4349–4357页)。</p></div></div>    
</body>
</html>