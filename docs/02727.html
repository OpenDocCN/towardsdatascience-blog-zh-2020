<html>
<head>
<title>Experiments on Hyperparameter tuning in deep learning — Rules to follow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中超参数调整的实验——遵循的规则</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/experiments-on-hyperparameter-tuning-in-deep-learning-rules-to-follow-efe6a5bb60af?source=collection_archive---------27-----------------------#2020-03-16">https://towardsdatascience.com/experiments-on-hyperparameter-tuning-in-deep-learning-rules-to-follow-efe6a5bb60af?source=collection_archive---------27-----------------------#2020-03-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/8e7476edae45331bbf78f9b5b38ed361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/0*tNpcDH1e8wlhD0ma.jpg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">鸣谢—<a class="ae kc" href="https://image.freepik.com/free-photo/gear-cutting-machine_137573-2479.jpg" rel="noopener ugc nofollow" target="_blank">https://image . free pik . com/free-photo/gear-cutting-machine _ 137573-2479 . jpg</a></p></figure><p id="f702" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">任何深度学习模型都有一组参数和超参数。参数是模型的权重。这些是在每个反向传播步骤使用优化算法(如梯度下降)更新的。超参数是我们设定的。他们决定模型的结构和学习策略。例如，批量大小、学习速率、权重衰减系数(L2正则化)、隐藏层的数量和宽度等等。由于深度学习在模型创建中提供的灵活性，人们必须仔细挑选这些超参数以实现最佳性能。</p><p id="dba3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇博客中，我们讨论</p><p id="a560" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.调整这些超参数时要遵循的一般规则。<br/> 2。在数据集上的实验结果验证了这些规则。</p><h1 id="acfa" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">实验细节</h1><p id="8efc" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir">数据— <br/> </strong>实验在以下<a class="ae kc" href="https://www.kaggle.com/puneet6060/intel-image-classification" rel="noopener ugc nofollow" target="_blank">数据集</a>上进行。它包含6类图像——建筑物、森林、冰川、山脉、海洋、街道。每门课大约有2300个例子。测试集由每个类的大约500个例子组成。训练集和测试集都相当平衡。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/b53de69bd4c31e6138b9b6f8b797513c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*98UK4OsvIkOu166bTrSwMQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">资料组</p></figure><p id="40ad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">code—<br/><a class="ae kc" href="https://github.com/DhruvilKarani/HyperParameterTuning/blob/master/README.md" rel="noopener ugc nofollow" target="_blank">https://github . com/DhruvilKarani/hyperparameter tuning/blob/master/readme . MD</a></p><p id="fb81" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">硬件— </strong> <br/> NVIDIA 1060 6GB GPU。</p><p id="3524" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">使用的型号— <br/> </strong> 1。学习率实验— ResNet18 <br/> 2。对于批量大小、内核宽度、重量衰减的实验—自定义架构(见代码)。</p><p id="298b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">观测记录— <br/> </strong> 1。每个时期的训练和测试损失<br/> 2。每个历元的测试精度<br/> 3。每个时期的平均时间(测试集上的训练和推断)</p><h1 id="9ef8" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">超参数及其对模型训练的影响</h1><p id="5a11" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们以卷积神经网络(CNN)为例，将模型行为和超参数值联系起来。在这篇文章中，我们将讨论以下内容——学习速度、批量大小、内核宽度、权重衰减系数。但是在我们讨论这些通用规则之前，让我们回顾一下任何学习算法的目标。<em class="mj">我们的目标是减少训练误差以及训练误差和测试/验证误差之间的差距</em>。我们通过调整超参数来实现这一点</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/d4316d0a715ae6328690086e9c7b19aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vBeUVxt0xdNrPr8UyWa_EA.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">训练和验证误差通用图。原图。</p></figure><p id="2f98" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看深度学习文献如何描述改变超参数值的预期效果</p><h2 id="7f49" class="ml lc iq bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">学习率</h2><p id="96a7" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir">深度学习</strong>书上说—</p><blockquote class="mx"><p id="e0da" class="my mz iq bd na nb nc nd ne nf ng la dk translated">如果你有时间只调整一个超参数，调整学习率</p></blockquote><p id="4b77" class="pw-post-body-paragraph kd ke iq kf b kg nh ki kj kk ni km kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">在我的实验中，这当然成立。在尝试了三种学习速率后，我发现过低或过高的值都会严重降低算法的性能。对于学习率10^-5，我们实现了92%的测试准确性，对于其余两个率，我们几乎没有超过70%,所有其他超参数保持不变。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/56c16ecb1fbab3dbdd1160ce2b0d21c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uhDfs6w-8u2eRMtbgttJkw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">在ResNet上测试准确性</p></figure><p id="a54e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你看看误差图，会发现一些东西。例如在训练错误中—</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/1a1425bcc054bd13d1be0ec82504bd2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9DLAGOGGCT7kw_KqlUFm8Q.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">ResNet上的训练错误</p></figure><p id="471a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不出所料，三者都有所下降。但是在最低的学习速率下(粉色)，第10个时期的损失<em class="mj">大于第一个时期</em>的红色曲线的损失。由于学习率极低，你的模型学得非常慢。此外，在高学习速率下，我们期望模型学习得更快。但是如你所见，红色曲线上的最低训练误差<em class="mj">仍然大于橙色曲线上的误差</em>(中等学习率)。现在让我们看看测试误差—</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/ee8b92a50a90ed1918cf7c3668579c2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1584wv7hIKAkDRqDgA6-hQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">ResNet上的测试错误</p></figure><p id="2701" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一些观察。对于最低比率，测试损失稳步下降，似乎还没有达到最低点。这意味着模型有欠拟合，可能需要更多的训练。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/45de1da5eda82cdb5476c2c59dc09ba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*30UREcK5yRgbVl5N.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片鸣谢—<a class="ae kc" href="https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png" rel="noopener ugc nofollow" target="_blank">https://www . Jeremy Jordan . me/content/images/2018/02/Screen-Shot-2018-02-24-at-11 . 47 . 09-am . png</a></p></figure><p id="6c21" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">红色曲线显示异常行为。有人可能会怀疑，由于高学习率，优化器无法收敛到全局最小值，并一直在误差范围内跳动。</p><p id="d81c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于中等曲线(橙色)，测试误差在第一个时期后开始缓慢增加。这是<strong class="kf ir">过拟合</strong>的经典例子。</p><h2 id="6be4" class="ml lc iq bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">批量</h2><p id="7bac" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">如果你熟悉深度学习，你一定听说过随机梯度下降(SGD)和批量梯度下降。为了重新访问，SGD对每个数据点执行权重更新步骤。batch在对整个训练集中数据点的梯度进行平均后执行更新。根据<a class="ae kc" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" rel="noopener ugc nofollow" target="_blank"> Yann LeCun </a></p><p id="6c04" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SGD的优势— <br/> 1。学起来快多了<br/> 2。往往能获得更好的解决方案<br/> 3。用于跟踪更改。</p><p id="1561" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">批量学习的优势— <br/> 1。收敛的条件是众所周知的。<br/> 2。许多加速学习技术，如共轭梯度，在批量学习中很好理解。<br/> 3。重量动力学和收敛速度的理论分析要简单得多</p><p id="122a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用介于两者之间的方法，小批量梯度下降法很受欢迎。不是使用整个训练集来平均梯度，而是使用一小部分数据点的梯度平均。该批次的大小是一个超参数。</p><p id="7756" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">举例来说，考虑下图所示的损失情况</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5dac5548ace50142caabce1f9eb17ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*yKFtXyni9GPJbArC7DIHzQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">损失景观示例。原图</p></figure><p id="adab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面有数字的对角线是损失等值线。x轴和y轴是两个参数(比如说<em class="mj"> w1 </em>和<em class="mj"> w2 </em>)。沿着等高线，损耗是恒定的。例如，对于任何位于损耗为4.500的线上的<em class="mj"> w1 </em>和<em class="mj"> w2 </em>对，损耗为4.500。蓝色之字形线是SGD的行为方式。橙色线是您预期的小批量梯度下降的工作方式。红点代表参数的最佳值，此时损耗最小。</p><p id="ceae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">《深度学习》一书提供了一个很好的类比来理解为什么太大的批量效率不高。从<em class="mj"> n个</em>样本估计的标准误差为<strong class="kf ir"> σ/√ <em class="mj"> n. </em> </strong>考虑两种情况——一种有100个样本，另一种有10000个样本。后者需要100倍以上的计算。但是标准误差的预期降低只有10倍</p><p id="9492" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我的实验中，我改变了批量大小。我获得的测试准确度看起来像—</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/79e268ddb774184a25e178a9969a161b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u4HbWH8RO4y6oJOfiBmyJw.png"/></div></div></figure><h2 id="82b1" class="ml lc iq bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">内核宽度</h2><p id="cb90" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">CNN中的卷积操作包括从特征图中提取特征的核。内核由学习到的参数组成。在二维卷积中，内核是一个N*N的网格，其中N是内核宽度。</p><p id="0232" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">增加或减少内核宽度各有利弊。增加核宽度会增加模型中的参数，这是增加模型容量的明显方法。很难对内存消耗进行评论，因为参数数量的增加会增加内存使用量，但特征图的输出维度会变小，从而减少内存使用量。</p><h2 id="83b0" class="ml lc iq bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">重量衰减</h2><p id="d400" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">权重衰减是L2正则化的优势。它实质上惩罚了模型中较大的权重值。设置正确的强度可以提高模型的泛化能力并减少过度拟合。但是过高的值会导致严重的欠拟合。例如，我尝试了正常和极高的重量衰减值。如你所见，当这个系数设置不好时，学习能力几乎为零。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/7ed8897a97f535cf8ca3378128cb3582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nwCaqQVVH6K-KiwT3uGyIg.png"/></div></div></figure><h2 id="47f8" class="ml lc iq bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">结论</h2><p id="bbd5" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">这些实验完全证实了我们的大多数假设。像ResNet18这样调优不佳的复杂模型很容易比调优良好的简单架构表现更差。损耗曲线是研究超参数影响的良好起点。</p><p id="de09" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为你的模型找到最好的超参数是困难的。尤其是手动操作的时候。我建议看看HyperOpt和Optuna这样的超参数优化库。这篇由neptune.ai(一家为你的ML实验管理需求提供解决方案的公司)撰写的<a class="ae kc" href="https://neptune.ai/blog/optuna-vs-hyperopt" rel="noopener ugc nofollow" target="_blank">文章</a>很好地介绍了这些包。</p></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><p id="b636" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mj">如果你和我一样是ML爱好者，那就来连线一下</em> <a class="ae kc" href="https://www.linkedin.com/in/dhruvil-karani/" rel="noopener ugc nofollow" target="_blank"> <em class="mj"> LinkedIn </em> </a> <em class="mj">和</em><a class="ae kc" href="https://twitter.com/dhruvil_karani" rel="noopener ugc nofollow" target="_blank"><em class="mj">Twitter</em></a><em class="mj">。我非常乐意收到对这篇文章的任何评论。</em></p></div></div>    
</body>
</html>