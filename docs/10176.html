<html>
<head>
<title>Inter-Annotator Agreement (IAA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注释者间协议(IAA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/inter-annotator-agreement-2f46c6d37bf3?source=collection_archive---------4-----------------------#2020-07-18">https://towardsdatascience.com/inter-annotator-agreement-2f46c6d37bf3?source=collection_archive---------4-----------------------#2020-07-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/b250f5728b881d83e435029d902542a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*QpbEDaIj5sTL2Pkt9D3nOQ.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">作者图片</p></figure><div class=""/><div class=""><h2 id="eb78" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">分类标注的成对 Cohen kappa 和群 fleiss kappa(𝜅)系数</h2></div><p id="8bff" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这个故事中，我们将探索注释者间协议(IAA ),这是对多个<strong class="kv jf">注释者</strong>为某个类别做出相同的<strong class="kv jf">注释</strong>决定的一种度量。受监督的自然语言处理算法使用带标签的数据集，该数据集通常由人来注释。一个例子是我硕士论文的注释方案，其中推文被标记为辱骂性的<em class="lp"/>或非辱骂性的<em class="lp"/>。</p><p id="3090" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">IAA 向您展示了您的注释准则有多清晰，您的注释者对它的理解有多一致，以及注释任务的可重复性有多高。这是分类结果的验证和重现性的重要部分。</p><p id="6104" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><em class="lp">准确性</em>和<em class="lp"> F1 得分</em>没有考虑到<strong class="kv jf">人们注释实例时很可能出现的</strong>期望机会一致。考虑预期机会协议的措施:</p><ul class=""><li id="b2f3" class="lq lr je kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">科恩的𝜅:两个注释者用一个类别注释每个实例</li><li id="95af" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">弗莱斯的《𝜅:》每一个例子都用一个类别进行了注释《𝑛时报》</li></ul><p id="f042" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这个故事涵盖了注释的最佳实践，并探索了定性注释的两个 IAA 度量标准:Cohen 和 Fleiss 的 kappa。Cohen kappa 是在一对注释者之间计算的，而 Fleiss 的 kappa 是在一组多个注释者之间计算的。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi me"><img src="../Images/4e611d540cd922bd65cf745897aeed9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*OVSQpQ0fVDmc3ziMbGBIpw.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">Kappa 统计定义</p></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="c4a8" class="mq mr je bd ms mt mu mv mw mx my mz na kk nb kl nc kn nd ko ne kq nf kr ng nh bi translated">注释设置</h1><p id="925f" class="pw-post-body-paragraph kt ku je kv b kw ni kf ky kz nj ki lb lc nk le lf lg nl li lj lk nm lm ln lo im bi translated">监督学习是基于示例输入-输出对学习将输入映射到输出的函数的机器学习任务。它从由一组训练样本组成的带标签的训练数据中推断出该函数。</p><p id="f454" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在注释数据时，最好让多个注释者注释同一个训练实例来验证标签。当多个注释者注释数据的相同部分时，我们能够计算观察者之间的一致或 IAA。</p><p id="5375" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了我的硕士论文，我和 44 个本科生一起工作，分成 11 组。每个人都标注了 100 条独特的推文和 50 条重叠的推文，其他三个小组成员也标注了这些推文。这导致四个不同的注释者发布了 50 条带注释的 tweet，一个注释者发布了 400 条 tweet。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="a473" class="mq mr je bd ms mt mu mv mw mx my mz na kk nb kl nc kn nd ko ne kq nf kr ng nh bi translated">科恩卡帕</h1><p id="dad1" class="pw-post-body-paragraph kt ku je kv b kw ni kf ky kz nj ki lb lc nk le lf lg nl li lj lk nm lm ln lo im bi translated">Cohen 的 kappa 系数(<em class="lp"> κ </em>)是一个统计量，用于度量标注者之间对于定性(分类)项目的可靠性。这是一个比简单的百分比协议计算更稳健的衡量标准，因为<em class="lp"> κ </em>考虑了协议偶然发生的可能性。它是两个标注器之间成对的可靠性度量。</p><p id="05ee" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">科恩的卡帕统计量是两个评价人之间的一致，其中<em class="lp"> Po </em>是评价人之间相对观察到的一致(等同于准确度)，而<em class="lp"> Pe </em>是机会一致的假设概率。下面是该评估指标的编程实现。</p><figure class="mf mg mh mi gt iv"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="05a7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">注意</strong>注释列表必须按照相同的索引排序。<code class="fe np nq nr ns b">ann1[0]</code>和<code class="fe np nq nr ns b">ann2[0]</code>应该代表同一个带注释的实例(tweet)。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="bdff" class="mq mr je bd ms mt mu mv mw mx my mz na kk nb kl nc kn nd ko ne kq nf kr ng nh bi translated">弗莱斯卡帕</h1><p id="c409" class="pw-post-body-paragraph kt ku je kv b kw ni kf ky kz nj ki lb lc nk le lf lg nl li lj lk nm lm ln lo im bi translated">Fleiss' kappa 是一种统计方法，用于在给几个项目分配分类评级或对项目进行分类时，评估固定数量的评价人之间的一致性的可靠性。它是 Scott 的 pi (𝜋)评估度量的一种推广，将两个标注器扩展到多个标注器。斯科特的 pi 和科恩的 kappa 只适用于两位评分者，而弗莱斯的 kappa 适用于任何数量的评分者，他们对固定数量的项目进行分类评分。除此之外，并不是所有的评定者都需要对所有项目进行注释。</p><figure class="mf mg mh mi gt iv"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="d029" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">感谢<a class="ae nt" href="https://gist.github.com/skylander86/65c442356377367e27e79ef1fed4adee" rel="noopener ugc nofollow" target="_blank"> Skylander86 </a>对用 Python 实现 Fleiss kappa 的贡献。我只是复制了他的代码，并添加了注释来解释发生了什么。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="3fdc" class="mq mr je bd ms mt mu mv mw mx my mz na kk nb kl nc kn nd ko ne kq nf kr ng nh bi translated">Kappa 统计解释</h1><p id="b0d4" class="pw-post-body-paragraph kt ku je kv b kw ni kf ky kz nj ki lb lc nk le lf lg nl li lj lk nm lm ln lo im bi translated">如果评定者完全同意，那么<em class="lp"> κ = 1 </em>。如果评定者之间没有达成一致(除了偶然预期的情况)，那么<em class="lp"> κ ≤ 0 </em>。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi nu"><img src="../Images/338504d1aa8382062b195c62a3b2023c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mHB6Ciljb4OnOacNWgc0aw.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">卡帕诠释，<a class="ae nt" href="https://www.ncbi.nlm.nih.gov/pubmed/15883903" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="aaac" class="mq mr je bd ms mt mu mv mw mx my mz na kk nb kl nc kn nd ko ne kq nf kr ng nh bi translated">摘要</h1><ul class=""><li id="9141" class="lq lr je kv b kw ni kz nj lc nz lg oa lk ob lo lv lw lx ly bi translated">Cohen kappa 只有两个注释器，每个注释器注释每个项目。</li><li id="eed5" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">由于机会协议的计算方式，科恩的𝜅比斯科特的𝜋信息量更大:后者对每个评分者使用一种分布，而前者使用不同的分布。</li><li id="6b23" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">弗莱斯的卡帕是斯科特的𝜋对两位编码员的延伸(不是科恩的𝜅).</li><li id="2ce8" class="lq lr je kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">Fleiss 的 kappa 可以有任意数量的注释器，每个项目不一定都由每个注释器进行注释！</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="be19" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在下面的包中也有 Cohen 和 Fleiss 的 kappa 统计的实现，所以您不必为它们编写单独的函数(尽管这是一个很好的实践！).</p><pre class="mf mg mh mi gt oc ns od oe aw of bi"><span id="6a34" class="og mr je ns b gy oh oi l oj ok">import sklearn<br/>from sklearn.metrics import cohen_kappa_score</span><span id="9b8f" class="og mr je ns b gy ol oi l oj ok">import statsmodels<br/>from statsmodels.stats.inter_rater import fleiss_kappa</span></pre><p id="6608" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">希望这个故事向您展示了如何计算 IAA 来验证您的结果并提高您的实验的可重复性！完整的脚本可以在<a class="ae nt" href="https://github.com/LouisdeBruijn/Medium/tree/master/IAA" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div></div>    
</body>
</html>