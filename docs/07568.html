<html>
<head>
<title>NLP-Abstract Topic Modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">抽象主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-topic-modeling-to-identify-clusters-ca207244d04f?source=collection_archive---------37-----------------------#2020-06-07">https://towardsdatascience.com/nlp-topic-modeling-to-identify-clusters-ca207244d04f?source=collection_archive---------37-----------------------#2020-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eaed" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从长文本中引出主题</h2></div><p id="923a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是 4 部分文章的第 3 部分。到目前为止，我们一直在谈论:</p><ol class=""><li id="2680" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><a class="ae ln" href="https://medium.com/@tyagigaurika27/nlp-preprocessing-clinical-data-to-find-sections-461fdadbec77" rel="noopener">预处理和清洗</a></li><li id="b7e3" class="le lf it kk b kl lo ko lp kr lq kv lr kz ls ld lj lk ll lm bi translated"><a class="ae ln" href="https://medium.com/@tyagigaurika27/text-summarization-for-clustering-documents-2e074da6437a" rel="noopener">文本摘要</a></li><li id="6e94" class="le lf it kk b kl lo ko lp kr lq kv lr kz ls ld lj lk ll lm bi translated"><strong class="kk iu">使用潜在狄利克雷分配(LDA)的主题建模——我们在这里</strong></li><li id="6b61" class="le lf it kk b kl lo ko lp kr lq kv lr kz ls ld lj lk ll lm bi translated"><a class="ae ln" href="https://medium.com/@tyagigaurika27/identifying-relationships-in-clinical-text-nlp-clustering-929eb04b5942" rel="noopener">聚类</a></li></ol><p id="3717" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将使用长文本文档<strong class="kk iu"> <em class="lt">的摘要版本来查找构成每个文档</em> </strong>的主题。我们在主题建模之前对文本进行总结，因为在一些文档中可能会有额外的细节。然而，其他人可能只是抓住了要点。</p><p id="b652" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lt">等等，但是为什么要模型题目呢？这到底是什么意思？</em>T15】</strong></p><blockquote class="lu lv lw"><p id="be19" class="ki kj lt kk b kl km ju kn ko kp jx kq lx ks kt ku ly kw kx ky lz la lb lc ld im bi translated"><strong class="kk iu">主题建模</strong>用于发现文档集合中出现的抽象“主题”。它是一个常用的文本挖掘工具，用于发现文本中隐藏的语义结构。</p></blockquote><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/41819305195cbfdfcaf6aaee80150983.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*njHwI4RWEKNw_cRxZhizrA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者图片:原始文本文档</p></figure><p id="6808" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们希望保持简洁明了的信息，以便识别每个长文档的主题。所以，我们把这段文字总结成这样:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mq"><img src="../Images/ae00b05ce040f7611bcdb0ef596e9e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_6NtBJsgioInILLI.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图片 bu 作者:总结文字</p></figure><h1 id="fdd8" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">主题建模</h1><p id="56e2" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">我们将不会做任何进一步的预处理，因为我们已经基本上预处理时，清理文本最初，只有短语在总结中。</p><p id="58d0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">和前面的章节一样，我们首先证明了总结是否可行。让我们看看集群是否可行。</p><pre class="mb mc md me gt no np nq nr aw ns bi"><span id="1d6e" class="nt ms it np b gy nu nv l nw nx">from numpy import dot<br/>from numpy.linalg import norm</span><span id="7faa" class="nt ms it np b gy ny nv l nw nx">exam = nlp.parser.vocab[u"exam"]</span><span id="cafe" class="nt ms it np b gy ny nv l nw nx"># cosine similarity<br/>cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))</span><span id="9238" class="nt ms it np b gy ny nv l nw nx">allWords = list({w for w in nlp.parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != "exam"})</span><span id="3e9f" class="nt ms it np b gy ny nv l nw nx"># sort by similarity to Exam<br/>allWords.sort(key=lambda w: cosine(w.vector, exam.vector))<br/>allWords.reverse()<br/>print("Top 5 most similar words to exam:")<br/>for word in allWords[:5]:   <br/>    print(word.orth_)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nz"><img src="../Images/e66dd915ff9d8e5e9ba7c3dcad86e36a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cOAkXmSTOsG2rj7MCYR2VQ.png"/></div></div></figure><p id="7955" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">哇！！！那么这是否意味着我们甚至可以找到文档之间的相似之处呢？是啊！！因此，我们有可能找到这些不同文档的集群！！！！</p><h2 id="f97d" class="nt ms it bd mt oa ob dn mx oc od dp nb kr oe of nd kv og oh nf kz oi oj nh ok bi translated">寻找主题的最佳数量</h2><p id="c723" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated"><a class="ae ln" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank">潜在狄利克雷分配(LDA) </a>是文本文档的贝叶斯概率模型。它从未被观察的组中确定观察集。因此，解释了数据的相似部分。</p><blockquote class="ol"><p id="4c3e" class="om on it bd oo op oq or os ot ou ld dk translated">观察是来自文档的单词。每个文档都是少量主题的组合。每个单词的出现都归因于文档的一个主题。</p></blockquote><figure class="ov ow ox oy oz mf"><div class="bz fp l di"><div class="pa pb l"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">主题建模</p></figure><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pc"><img src="../Images/f60eb3ff32787b51bd0245fa8e9c0dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CaInS7UBQHECO2jIDLohvw.png"/></div></div></figure><p id="00e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你们中有多少人真正理解了上面部分发生的事情？如果你不知道，请告诉我，我很乐意再写一篇关于这本字典解释的文章！</p><p id="5f41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们能想象一下上面的话题吗？是的，我们可以。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pd"><img src="../Images/40246b7ae620b77fc07d7fab510bb14c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pm4DOerOXwN8ELZXqhVFgw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者图片:点击每个主题，了解它是如何由术语组成的</p></figure><p id="b3a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这表明话题是根据谈论的疾病/状况来分配的。太好了，让我们看看成绩单中的话题倾向。</p><p id="28d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当 LDA 找到上面的主题时，它本质上是试图在组中找到单词的出现。这确实意味着我们有一个与每个主题的每个图表相关联的概率分数。但是，我们并不是对它们都有信心。所以我们只提取 90%以上倾向的。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="pa pb l"/></div></figure><p id="fd71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果仔细观察，并不是所有的图表都有主题。这是因为该算法不符合我们的话题倾向性截止标准。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pe"><img src="../Images/42a49e788ef5e5d25404be4530e21f2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mKaDi7BuZnUgsawsvQnYKw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者提供的图片:带有最终主题和主题倾向的图表注释(范围为 0–1)</p></figure><p id="d60c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们想象一下我们已经确定的图表的主题/集群和频率</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pf"><img src="../Images/77b3af6d63a82947af5daee9206e4572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qEUQel8tOdvCv2x20wUPFg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者图片:话题频率</p></figure><h1 id="8a31" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">结论</h1><p id="69ed" class="pw-post-body-paragraph ki kj it kk b kl nj ju kn ko nk jx kq kr nl kt ku kv nm kx ky kz nn lb lc ld im bi translated">这里可以看到<em class="lt"> topic_21 </em>的图表最多。紧随其后的是<em class="lt"> topic_15 </em>和<em class="lt"> topic_14 </em>。</p><p id="f987" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们对这些文本文档进行聚类！</p><blockquote class="lu lv lw"><p id="3c3d" class="ki kj lt kk b kl km ju kn ko kp jx kq lx ks kt ku ly kw kx ky lz la lb lc ld im bi translated">如果你想<strong class="kk iu">自己尝试整个代码或跟随，请</strong>到我在 GitHub 上发布的 jupyter 笔记本:<a class="ae ln" href="https://github.com/gaurikatyagi/Natural-Language-Processing/blob/master/Introdution%20to%20NLP-Clustering%20Text.ipynb" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/gaurikatyagi/Natural-Language-Processing/blob/master/introduction % 20 to % 20 NLP-Clustering % 20 text . ipynb</a></p></blockquote></div></div>    
</body>
</html>