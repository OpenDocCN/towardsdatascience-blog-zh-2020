# 不，艾不是公正的。人工智能基于性别、种族和隶属关系进行歧视。

> 原文：<https://towardsdatascience.com/no-ai-isnt-impartial-ai-discriminates-based-on-gender-race-and-affiliations-6535fa7023f3?source=collection_archive---------32----------------------->

![](img/0e47fa6b9342d62c5853912c6a112879.png)

照片由 sebastiaan stam 拍摄。

## 一个常见的误解是机器是无偏的。没有什么比这更偏离事实了。

我这是一个常见的论点，我看到它一直在被使用。“我们应该实现人工智能(AI)，因为它是无偏见的。它不在乎你的性别、种族、民族和性取向——它不像我们人类那样歧视。”我甚至从大型科技公司那里听到过类似的观点。人们普遍认为人工智能是一个中立的实体，一个客观的灯塔，一个不偏不倚的圣人。

可悲的是，这是一个谎言。

## 性别歧视的招聘工具。

亚马逊精通人工智能。他们使用人工智能来推动他们业务的许多部分，他们甚至通过亚马逊网络服务向客户出售人工智能解决方案。亚马逊本质上是一家科技公司，存在于一个由男性主导的行业。像许多同行一样，亚马逊正试图变得更加多样化。《多元化使命》中的一个大问题是，人力招聘人员和经理存在偏见。研究表明，人类在招聘过程中会歧视他人。此外，对于拥有成千上万名申请人的公司来说，确定哪个申请人是任何给定职位的更好选择是非常具有挑战性的。

对于一家注入人工智能的公司来说，获得更加多样化的劳动力的自然步骤是在招聘过程中使用人工智能。毕竟，招聘人员是有偏见的，而众所周知机器不会。

[亚马逊就是这么做的。他们雇佣了一个人工智能来帮助审查申请人的简历。我的梦想是拥有一个机器人，它可以输入一百份简历，分析它们，并推荐最好的五份——不管他们的性别或种族。不幸的是，人工智能更喜欢男人而不是女人。机器学习模型已经研究了十年的简历。由于科技行业男性占主导地位，这些数据主要由男性简历组成。因为人工智能从它被喂养的数据中学习，人工智能学会了偏爱男人的特征。例如，机器学习算法对简历中使用的词汇具有重要意义，这些词汇在性别之间存在显著差异。亚马逊发现男性和女性在简历中使用不同的词汇。事实上，男性比女性更有可能使用动词，例如，执行的*和捕获*的*。由于人工智能从男性简历中学习，它学会了优先选择符合这些条件的申请人。*](https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine)

对于不在机器学习领域的人来说，简单地告诉人工智能在选择申请人的过程中忽略性别，听起来似乎是显而易见的。但是，正如你所看到的，这并不容易。

这只是众多关联之一。由于多年来的政治议程将人们引入刻板印象，男性和女性学习了不同的科目。不同的性别也有不同的业余爱好。发现这些相关性是困难的。非常难。纠正它们更难。对于你给人工智能的每一条规则和每一次引导它向特定方向的尝试，你都把你自己的偏见应用到人工智能上。无论你做什么，人工智能都会有一定程度的偏差。

你如何应对这种情况？你是否试图找到更多样的数据？你从哪里得到的？你会从竞争对手那里买简历吗？他们也没有。你不能伪造简历；没用的。你需要真实的数据。

在这一整节中，我只讨论了性别歧视。不同种族之间的歧视呢？如果你的公司成立于一个白人占主导地位的国家，并且碰巧有 99.8%的白人员工，那该怎么办？即使你有来自地球上各大洲的人的可靠混合数据，如果你有一个来自你没有数据的国家的申请人，并且申请人在他们的简历中有你的 AI 从未见过的 prio 经历，那该怎么办？为这些场景设计不考虑性别或种族等因素的公平算法不仅具有挑战性，甚至可能是不可能的。[平衡有利于多样性的限制是一种方式](https://www.ijcai.org/Proceedings/2019/0836.pdf)，然而，每增加一个限制，组合中也会增加一些偏差。

无论你最终做什么，人工智能都会有偏见。

![](img/98fa846afb7badd4971b5cf8776030e1.png)

通往人工智能的道路布满了堆积如山的有偏见的数据。由[亚历山大·米洛](https://unsplash.com/@lexmilo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄。

## 警方数据有种族歧视。因此，警察人工智能是种族主义者。

在我以前的一篇文章中，我讨论了世界各地的[警察部队如何使用歧视性的人工智能系统](https://medium.com/swlh/police-authorities-in-your-country-are-using-ai-surveillance-systems-22033f9c1581)。事实上，世界各地的警察当局现在都在使用人工智能来跟踪和识别公民。警察使用人工智能的最大问题之一是歧视，人工智能正在放大现有的偏见。

几十年来，警察系统地、不成比例地针对少数民族。有报道称，警察为了达到逮捕配额，在无辜的少数民族身上种植非法药物，也有记录称警察完全伪造报告。[这些可怕的数据现在被输入到世界各地警察部门使用的机器学习算法中](https://www.technologyreview.com/s/612957/predictive-policing-algorithms-ai-crime-dirty-data/)。该系统使用这些数据来学习在他们的犯罪预测中进行区分。在英国，[警察经常发现自己不同意机器学习人工智能做出的预测](https://www.bbc.com/news/technology-49717378)，表示人工智能本身创造了一个自我实现的预言。

然而，警察部队仍然欢迎人工智能系统，因为它们让他们的生活更轻松。人工智能提供了更快的决策和更轻的良心，使其成为打击犯罪的另一个工具。

## 有时候，保持中立是不可能的。

在许多情况下，不偏不倚是不可能的。即使你有世界上最广泛的数据库和最复杂的算法，一个必须在招聘过程中决定选择哪个申请人的人工智能也总是要做出主观选择。

认为人工智能是公正的误解可能源于这样一个事实，即传统的编程机器确实是公正的。无论你是谁，你的电子邮件客户端都会像对待其他人一样给你发送邮件。你的微软 Word 文档不关心你是谁；它会给你和其他人一样的编辑功能。

然而，机器学习的不同之处在于它从数据中学习。由于一些数据是人类努力的直接产物，并且由于人类不可能是无偏见的，基于这些数据教授的 AI 也不可能是无偏见的。

## 人工智能的例子:它仍然可以比人类中立得多。

尽管如此，人工智能仍然是对抗偏见的未来。我这篇文章的重点是强调人工智能是有偏见的——T4 而不是 T5，它比人类更有偏见。一个人工智能被提供了一组不同的数据，[被给予了公平的约束，并被赋予了适当的变量权重](https://www.ijcai.org/Proceedings/2019/0836.pdf)，尽管如此，它仍然比任何人都不偏不倚。人类永远是有偏见的。对我们的大脑进行重新编程以使其不偏不倚是不可能的(除非我们进入一些非常先进的基因工程领域)。计算机的大脑更容易编程。

一个训练有素的人工智能肯定不会像人类那样偏心。要做到这一点，人工智能的实现不能操之过急。他们需要接受不同数据的培训，并且需要仔细管理和测试。

此外，我想强调在实现机器学习人工智能时应该遵循的三条规则。

一个。AI 必须有一个可解释的日志。一定有某种机制告诉你为什么人工智能会做出这样的决定。通常情况下，人工智能会做出决定，但没有人有能力解释它为什么会做出这样的决定。

两个。必须减轻偏见。人工智能必须根据不同的数据进行训练。如果无法找到不同的数据，必须应用[约束和适当的权重](https://www.ijcai.org/Proceedings/2019/0836.pdf)。我一直强调约束和权重——虽然它们确实增加了某种程度的人为偏见，但它们对于减轻薄弱的数据集是必要的。

第三。不能让人类过度依赖人工智能。虽然人工智能可能经常比人类更少偏见，但它并非完美无缺。它会犯错误。质疑一下。

虽然使用人工智能完全自动化一个过程可能很诱人，但有时最好通过使用人工智能作为指导决策的*助手*来开始一个人的人工智能之旅。毫无疑问，未来在于人类与人工智能的合作，机器与人类平等相处，但我们还没有到那一步。例如，在今天，招聘人员不能被人工智能取代。然而，有了人工智能的陪伴，招聘人员的工作会变得容易得多。在阅读简历时，人工智能可以提供总结和见解，同时还可以清楚地展示其信念旁边的确定性。在对候选人进行面试时，人工智能可以实时推荐后续问题，并向招聘人员提供行为分析。

看一看[这篇文章](/why-ai-must-be-ethical-and-how-we-make-it-so-b52cdb1dd15f)中关于这些规则的更多细节，以及一些如果你选择忽视它们可能会发生什么的可怕故事。

[](/why-ai-must-be-ethical-and-how-we-make-it-so-b52cdb1dd15f) [## 为什么人工智能必须是道德的——以及我们如何做到这一点

### 当人工智能决定谁是有价值的，谁是正确的，谁是罪犯，你只能希望它使…

towardsdatascience.com](/why-ai-must-be-ethical-and-how-we-make-it-so-b52cdb1dd15f) 

和人类打交道的 AI 永远是有偏见的。

我们有责任减少这种情况。