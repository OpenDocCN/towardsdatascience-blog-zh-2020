<html>
<head>
<title>4 ways to Reduce Dimensionality of Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">4 种降低数据维度的方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/4-ways-to-reduce-dimensionality-of-data-8f82e6565a07?source=collection_archive---------15-----------------------#2020-08-08">https://towardsdatascience.com/4-ways-to-reduce-dimensionality-of-data-8f82e6565a07?source=collection_archive---------15-----------------------#2020-08-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b15a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">降维方法概述——相关、主成分分析、t-SNE、自动编码器及其在 python 中的实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2da65af43541fac4e783b348794f068e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QOl78F6FDeHZYrJhyyRL_g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@chuklanov?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Avel Chuklanov </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄，使用<a class="ae ky" href="https://pixlr.com/x/" rel="noopener ugc nofollow" target="_blank"> Pixlr </a>编辑</p></figure><p id="3dfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降维是减少数据集中特征或变量数量的过程。它是将数据从高维空间转换到低维空间，以便低维表示保留原始数据的一些有意义的属性。</p><h2 id="2988" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">为什么降维很重要？</h2><p id="cc6d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在现实世界的数据集中，数据中经常有太多的变量。要素数量越多，数据的可视化和处理就越困难。有时，这些特征中的大多数是相关的，因此是多余的。这就是降维算法发挥作用的地方。</p><p id="7116" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有多种方法可以降低数据的维数，在本文中，您可以了解其中的一些方法:</p><ol class=""><li id="0acb" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">特征选择方法:使用相关系数方法</li><li id="e266" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">矩阵分解</li><li id="7750" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">流形学习:SNE</li><li id="7950" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">自动编码器</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/ee8474f6e262288075b804659b188297.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*YLr11FiBTYHBKjLg6At4iw.png"/></div></figure><h1 id="9198" class="ni lw it bd lx nj nk nl ma nm nn no md jz np ka mg kc nq kd mj kf nr kg mm ns bi translated">特征选择方法:</h1><p id="6605" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">一些数据集具有大量的要素，而这些要素中只有一些与目标类标注相关。特征选择技术使用评分或统计方法来选择保留哪些特征和删除哪些特征。</p><p id="e063" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用于通过选择顶部特征来减小尺寸的技术或算法有:</p><ul class=""><li id="b213" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu nt mz na nb bi translated">皮尔逊相关系数(数值输入，数值输出)</li><li id="14b8" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nt mz na nb bi translated">Spearman 相关系数(数值输入，数值输出)</li><li id="4ee6" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nt mz na nb bi translated">卡方检验(分类输入，分类输出)</li><li id="2840" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nt mz na nb bi translated">肯德尔τ检验(数值输入，分类输出)</li></ul><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/pearson-and-spearman-rank-correlation-coefficient-explained-60811e61185a"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">皮尔逊和斯皮尔曼等级相关系数—解释</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">随机变量之间的关系。</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol ks nx"/></div></div></a></div><p id="a873" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据训练数据的特征是数字的还是分类的，以及目标类别标签是数字的还是分类的，可以从上述列表中选择不同的特征。</p><p id="c3e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">获取来自<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/wine" rel="noopener ugc nofollow" target="_blank"> UCI ML 知识库的葡萄酒样本数据集。</a>目标类别标签是分类的，其余的训练数据具有数字特征。因此，我们可以使用 Kendall Tau 测试找到与目标类别标签(“Customer_Segment”)高度相关的顶级特征。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/41051857a23f8ed543b655b563c698bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*m_-9_BtJmGF5C29rF4zE8g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，上面代码片段的第 4 行输出</p></figure><p id="65c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从具有目标类标签“Customer_Segment”的数据集中的每个特征的上述 Kendall 系数，可以确认特征“Flavanoids”、“OD280”和“Total_Phenols”是具有最大模值的前 3 个特征。您可以通过取系数值的模来挑选前 x 个特征。</p><h1 id="9d9b" class="ni lw it bd lx nj nk nl ma nm nn no md jz np ka mg kc nq kd mj kf nr kg mm ns bi translated">矩阵分解:</h1><p id="c64a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">矩阵分解方法可用于降维。主成分分析(PCA)是一种矩阵分解技术，用于将高维数据降维。PCA 保持方差最大的方向。</p><p id="36d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PCA 应遵循的步骤:</p><ul class=""><li id="9853" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu nt mz na nb bi translated">给定数据集 X 的形状(n 行，d 要素)</li><li id="6749" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nt mz na nb bi translated">标准化数据集 X</li><li id="531d" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nt mz na nb bi translated">计算协方差矩阵</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/0d2f48ab05a60ab4c2a469c3fb3334da.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*4bV5iBDiYvJE6CUlu4bqqQ.png"/></div></figure><ul class=""><li id="c416" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu nt mz na nb bi translated">从协方差矩阵中找出特征值和特征向量。</li><li id="1ccd" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nt mz na nb bi translated">为了挑选前 f 个特征，挑选具有相应的前 x 个最大特征值的特征向量。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)</p></figure><p id="f6d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于取自<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/wine" rel="noopener ugc nofollow" target="_blank"> UCI ML 知识库</a>的葡萄酒样本数据集，最初，数据集有 13 个特征，经过 PCA 算法后，维数减少到 2 维，可视化结果如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/66a3c9af96f0d3063e2a1932b3591d97.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*WfvonwgJN0LnHuSIS6KJGA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，来自 PCA 算法的二维数据的可视化</p></figure><h1 id="faae" class="ni lw it bd lx nj nk nl ma nm nn no md jz np ka mg kc nq kd mj kf nr kg mm ns bi translated">多方面学习:</h1><p id="1b87" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">高维度统计的技术也可以用于降维。使用流形学习并用于创建高维数据的低维投影。这通常用于数据可视化。</p><p id="4b45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">t-分布式随机邻居嵌入(<strong class="lb iu"> t-SNE </strong>)是一种流形学习技术，用于将高维数据投影到低维(主要是 2 维或 3 维)进行可视化。t-SNE 是一种邻域保持嵌入技术，它能最好地保持低维数据中的显著结构或关系。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)</p></figure><p id="f5f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于取自<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/wine" rel="noopener ugc nofollow" target="_blank"> UCI ML 知识库</a>的葡萄酒样本数据集，最初，数据集有 13 个特征，在应用 t-SNE 算法后，维数减少到 2 维，可视化结果如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/cf14b60842e680829236afbbb715c568.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*HfwCX5lPF7at1G_AICRvLA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，来自 t-SNE 算法的二维数据的可视化</p></figure><h1 id="3d19" class="ni lw it bd lx nj nk nl ma nm nn no md jz np ka mg kc nq kd mj kf nr kg mm ns bi translated">自动编码器:</h1><p id="5fce" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Auto Encoders 是一个人工神经网络模型，用于执行维度缩减。自动编码器有两个组成部分，压缩和扩展。形状的初始数据集(n 行，d 维)被传递到自动编码器神经网络模型，并被编码到较低维的隐藏层。然后，它试图从简化的编码中生成尽可能接近其原始输入的表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/9eb128964c066f53c310cf85cb0ee282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Eyzo-oDrvejuzu8TMP-f3Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png" rel="noopener ugc nofollow" target="_blank">图像源</a>)，自动编码器架构</p></figure><p id="351a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上是基于单层感知器的自动编码器，它参与了多层感知器(MLP)——具有输入层、输出层和连接它们的一个或多个隐藏层。输出层中的节点数量与输入层中的节点数量相同，并且通过最小化输入和输出之间的差异来重构其输入。</p><p id="fcb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入和输出层有 d 个神经元(d 是原始数据集的维数)。中间隐藏层有 f 个神经元(f 是降维后的维数)。</p></div><div class="ab cl os ot hx ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="im in io ip iq"><h1 id="94c6" class="ni lw it bd lx nj oz nl ma nm pa no md jz pb ka mg kc pc kd mj kf pd kg mm ns bi translated">结论:</h1><p id="a602" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在本文中，我们讨论了 4 种不同的降维技术，每种技术都有各自的优缺点。没有最好的降维技术。相反，最好的方法是使用系统的受控实验，并发现哪些降维技术与您选择的模型结合使用时，会在数据集上产生最佳性能。</p><blockquote class="pe"><p id="3907" class="pf pg it bd ph pi pj pk pl pm pn lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>