<html>
<head>
<title>Beginner’s Guide to Data Cleaning and Feature Extraction in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中数据清理和特征提取初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beginners-guide-for-data-cleaning-and-feature-extraction-in-nlp-756f311d8083?source=collection_archive---------24-----------------------#2020-05-12">https://towardsdatascience.com/beginners-guide-for-data-cleaning-and-feature-extraction-in-nlp-756f311d8083?source=collection_archive---------24-----------------------#2020-05-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/364e648afb51a17e16e5959c25fae0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jtQQrD1IeR9loU6iyGdgHQ.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:<a class="ae kf" href="https://www.shutterstock.com/g/ievgeniiya" rel="noopener ugc nofollow" target="_blank">莱弗吉尼亚州</a>途径:<a class="ae kf" href="https://www.shutterstock.com/image-illustration/painted-hand-shows-concept-hologram-data-1078657103" rel="noopener ugc nofollow" target="_blank">摄影记者</a></p></figure><p id="d66e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文将解释使用神经语言处理(NLP)进行文本分析的数据清理和未来提取的步骤。</p><p id="ec44" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在网上，有很多很棒的文字清理指南。一些指南在文本清理之后进行特征提取，而一些指南在文本清理之前进行特征提取。这两种方法都很好。然而，<strong class="ki iu">这里有一个很少被关注的问题</strong>:在数据清理过程中，我们丢失了一些可能的特征(变量)。在数据清洗之前，我们需要进行特征提取。另一方面，有些特征只有在数据清理后提取时才有意义。因此，我们还需要在数据清洗后进行特征提取。本研究关注这一点<strong class="ki iu">、</strong>和<strong class="ki iu">这是本研究的独特之处。</strong></p><p id="cb9b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解决上述问题，本研究依次遵循三个步骤:</p><ol class=""><li id="7027" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">特征提取—第一轮</li><li id="1936" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">数据清理</li><li id="6aec" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">特征提取—第二轮</li></ol><p id="e6fb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇研究文章是采用NLP方法的亚马逊综述分析的一部分。这里是主研究代码的Colab笔记本的<a class="ae kf" href="https://github.com/EnesGokceDS/Amazon_Reviews_NLP_Capstone_Project" rel="noopener ugc nofollow" target="_blank"> my GitHub repo </a>，以及本次研究的<a class="ae kf" href="https://github.com/EnesGokceDS/Amazon_Reviews_NLP_Capstone_Project/blob/master/1_Data_cleaning_and_feature_extraction.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu">代码</strong> </a>。</p><p id="cacf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">关于我使用的数据的简要信息:</strong>本项目使用的数据是从<a class="ae kf" href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载的。是斯坦福网络分析项目上传的。原始数据来自J. McAuley和J. Leskovec (2013)对“<a class="ae kf" href="http://i.stanford.edu/~julian/pdfs/www13.pdf" rel="noopener ugc nofollow" target="_blank">从业余爱好者到行家:通过在线评论</a>对用户专业知识的演变进行建模”的研究。这个数据集由来自亚马逊的美食评论组成。该数据包括了从1999年到2012年的所有568，454篇评论。评论包括产品和用户信息、评级和纯文本评论。</p></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="52dd" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated"><strong class="ak">特征提取—第一轮</strong></h1><p id="4d82" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">在这一部分中，将提取数据清洗后不可能获得的特征。</p><ol class=""><li id="d2b1" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="ki iu">停用词的数量:</strong>停用词是搜索引擎已经被编程忽略的常用词(例如“the”、“A”、“an”、“in”)，无论是在索引用于搜索的条目时还是在作为搜索查询的结果检索它们时。在Python的<strong class="ki iu"> <em class="nc"> nltk </em> </strong>包中，有127个英文停用词默认。通过应用停用词，这127个词被忽略。在删除停用词之前，让我们将“停用词数量”作为一个变量。</li></ol><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="f0bb" class="nm ma it ni b gy nn no l np nq">df['stopwords'] = df['Text'].apply(lambda x: len([x for x in x.split() if x in stop]))</span><span id="8039" class="nm ma it ni b gy nr no l np nq">df[['Text','stopwords']].head()</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/49cd80fb70b054fbc5286f98f36048c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*bgEWAmDwPgFBjPRqZ8efsg.png"/></div></figure><p id="9117" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.<strong class="ki iu">标点符号的数量:</strong>数据清洗后无法获得的另一个特征是因为发音会被删除。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="3fd6" class="nm ma it ni b gy nn no l np nq">def count_punct(text):<br/>    count = sum([1 for char in text if char in string.punctuation])<br/>    return count</span><span id="da54" class="nm ma it ni b gy nr no l np nq">#Apply the defined function on the text data<br/>df['punctuation'] = df['Text'].apply(lambda x: count_punct(x))</span><span id="27ae" class="nm ma it ni b gy nr no l np nq">#Let's check the dataset<br/>df[['Text','punctuation']].head()</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d9f0dd6028c99e97167d9e509c2536ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*ZDIB-w6VK1yh8vA7VIMLnQ.png"/></div></figure><p id="9fde" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.标签字符的数量:我们可以从文本数据中提取的一个更有趣的特征是标签或提及的数量。在数据清理期间，标签将被删除，我们将无法访问这些信息。因此，让我们在仍然可以访问它的时候提取这个特性。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="4db3" class="nm ma it ni b gy nn no l np nq">df['hastags'] = df['Text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))</span><span id="bb47" class="nm ma it ni b gy nr no l np nq">df[['Text','hastags']].head()</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/031aeeedcabbfc32aa83af7bb16bb193.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*aKlE_oLdNZI5cEJBKI3r1A.png"/></div></figure><p id="ff8e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.<strong class="ki iu">数字字符的数量:</strong>拥有评论中出现的数字字符的数量可能是有用的。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="faf3" class="nm ma it ni b gy nn no l np nq">df['numerics'] = df['Text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))</span><span id="7b93" class="nm ma it ni b gy nr no l np nq">df[['Text','numerics']].head()</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/5f6cdb4855c20ef0086e05bb143f74c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*caUXIqUDskBQsuqFfSNKoQ.png"/></div></figure><p id="236c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5.<strong class="ki iu">大写单词的数量:</strong>愤怒、愤怒等情绪经常通过大写单词来表达，这使得这成为识别这些单词的必要操作。在数据清理过程中，所有字母将被转换成小写。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="5c19" class="nm ma it ni b gy nn no l np nq">df['upper'] = df['Text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))</span><span id="338d" class="nm ma it ni b gy nr no l np nq">df[['Text','upper']].head()</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/fbb6d02d555c069f928d2a3ad126b606.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*rp_ctFhGURQUbraZviaAJA.png"/></div></figure><p id="09c5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们完成了只能在数据清理之前获得的特征。我们准备清除数据。</p><h1 id="93ae" class="lz ma it bd mb mc nx me mf mg ny mi mj mk nz mm mn mo oa mq mr ms ob mu mv mw bi translated"><strong class="ak">文字清理技巧</strong></h1><p id="e0e4" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">在对数据应用NLP技术之前，首先需要清理数据并为分析准备数据。如果这个过程做得不正确，它可能会完全破坏分析部分。以下是应用于数据的步骤:</p><ol class=""><li id="aa53" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="ki iu">将所有文本转换成小写:</strong>第一个预处理步骤是将评论转换成小写。这避免了相同单词的多个副本。例如，在计算字数时，如果我们忽略这种转换，则“狗”和“狗”将被视为不同的单词。</li></ol><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="e30f" class="nm ma it ni b gy nn no l np nq">df['Text'] = df['Text'].apply(lambda x: " ".join(x.lower() for x in x.split()))</span><span id="0277" class="nm ma it ni b gy nr no l np nq">df['Text'].head()</span></pre><p id="39f3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 2) </strong>目前，NLP方法还没有一个有意义的方法来分析标点符号。因此，它们被从文本数据中删除。通过这一步，这些字符被删除:[！" #$% &amp; '()*+，-。/:;= &gt;？@[\]^_`{|}~]</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="5bbe" class="nm ma it ni b gy nn no l np nq">df['Text'] = df['Text'].apply(lambda x: " ".join(x.lower() for x in df['Text'] = df['Text'].str.replace('[^\w\s]','')<br/>df['Text'].head()</span></pre><p id="84bd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 3) </strong> <strong class="ki iu">停用词的移除</strong>:通过这一步，我移除了<em class="nc"> nltk </em>包中所有默认的英文停用词。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="3628" class="nm ma it ni b gy nn no l np nq">from nltk.corpus import stopwords<br/>stop = stopwords.words('english')</span><span id="16bb" class="nm ma it ni b gy nr no l np nq">df['Text'] = df['Text'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))</span><span id="5d7d" class="nm ma it ni b gy nr no l np nq">df['Text'].sample(10)</span></pre><p id="fa72" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="nc">添加自己的停用词</em> </strong>:此时，你可能想添加自己的停用词。我这样做主要是在检查了最常用的单词之后。我们可以这样检查最常用的单词:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="6b6b" class="nm ma it ni b gy nn no l np nq">import pandas as pd<br/>freq = pd.Series(' '.join(df['Text']).split()).value_counts()[:20]<br/>freq</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/30649be09dfd53e9b2eb433d8cf254f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*uAnqQcLXgS1SFErSCJnHZw.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">最常见的20个单词</p></figure><p id="3319" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这几个词中，我想去掉' br '，' get '，' also '，因为它们没有太大意义。让我们将它们添加到停用词列表中:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="ac82" class="nm ma it ni b gy nn no l np nq"># Adding common words from our document to stop_words</span><span id="c953" class="nm ma it ni b gy nr no l np nq">add_words = ["br", "get", "also"]<br/>stop_words = set(stopwords.words("english"))<br/>stop_added = stop_words.union(add_words)</span><span id="fe07" class="nm ma it ni b gy nr no l np nq">df['Text'] = df['Text'].apply(lambda x: " ".join(x for x in x.split() if x not in stop_added))</span><span id="2f96" class="nm ma it ni b gy nr no l np nq">df['Text'].sample(10)</span></pre><p id="e909" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意:在其他指南中，您可能会遇到TF-IDF方法。TF-IDF是从文本数据中去除没有语义价值的单词的另一种方法。如果你用的是TF-IDF，就不需要应用停用词(但是两个都应用也无妨)。</p><p id="dfbe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 4) </strong> <strong class="ki iu">移除URL:</strong>URL是被移除的数据中的另一个噪音。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="802a" class="nm ma it ni b gy nn no l np nq">def remove_url(text): <br/>    url = re.compile(r'https?://\S+|www\.\S+')<br/>    return url.sub(r'', text)</span><span id="8c38" class="nm ma it ni b gy nr no l np nq"># remove all urls from df<br/>import re<br/>import string<br/>df['Text'] = df['Text'].apply(lambda x: remove_url(x))</span></pre><p id="533b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 5) </strong> <strong class="ki iu">去除html HTML标签:</strong> HTML在互联网上被广泛使用。但是HTML标签本身在处理文本时并没有什么帮助。因此，所有以url开头的文本都将被删除。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="36c9" class="nm ma it ni b gy nn no l np nq">def remove_html(text):<br/>    html=re.compile(r'&lt;.*?&gt;')<br/>    return html.sub(r'',text)</span><span id="07b4" class="nm ma it ni b gy nr no l np nq"># remove all html tags from df<br/>df['Text'] = df['Text'].apply(lambda x: remove_html(x))</span></pre><p id="3c70" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 6) </strong> <strong class="ki iu">删除表情符号:</strong>表情符号可以是与客户满意度相关的一些情绪的指示器。不幸的是，我们需要在我们的文本分析中删除表情符号，因为目前还不能用NLP分析表情符号。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="23a0" class="nm ma it ni b gy nn no l np nq"># Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b</span><span id="7636" class="nm ma it ni b gy nr no l np nq">def remove_emoji(text): <br/>    emoji_pattern = re.compile("["<br/>        u"\U0001F600-\U0001F64F"  # emoticons<br/>        u"\U0001F300-\U0001F5FF"  # symbols &amp; pictographs<br/>        u"\U0001F680-\U0001F6FF"  # transport &amp; map symbols<br/>        u"\U0001F1E0-\U0001F1FF"  # flags<br/>        u"\U00002702-\U000027B0"<br/>        u"\U000024C2-\U0001F251"<br/>        "]+", flags=re.UNICODE)<br/>    return emoji_pattern.sub(r'', text)</span><span id="a889" class="nm ma it ni b gy nr no l np nq">#Example<br/>remove_emoji("Omg another Earthquake 😔😔")</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/6be3573c11f6a2fae518b45f5a0b053c.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*KS2pWEPlhzWJp7ppI5HJeg.png"/></div></figure><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="1b40" class="nm ma it ni b gy nn no l np nq"># remove all emojis from df<br/>df['Text'] = df['Text'].apply(lambda x: remove_emoji(x))</span></pre><p id="0080" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 7) </strong> <strong class="ki iu">移除表情符号:</strong>表情符号和表情符号有什么区别？</p><p id="6e58" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">:-)是表情符号</p><p id="a0c2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">😜是一个→表情符号。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="3144" class="nm ma it ni b gy nn no l np nq">!pip install emot #This may be required for the Colab notebook</span><span id="a659" class="nm ma it ni b gy nr no l np nq">from emot.emo_unicode import UNICODE_EMO, EMOTICONS</span><span id="34b7" class="nm ma it ni b gy nr no l np nq"># Function for removing emoticons<br/>def remove_emoticons(text):<br/>    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in   EMOTICONS) + u')')<br/>    return emoticon_pattern.sub(r'', text)</span><span id="01b1" class="nm ma it ni b gy nr no l np nq">#Example<br/>remove_emoticons("Hello :-)")</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6a3c6d1109f2269823be55b9da133f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*iarP12yKlSZ9UEuzt4vb_g.png"/></div></figure><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="010e" class="nm ma it ni b gy nn no l np nq">df['Text'] = df['Text'].apply(lambda x: remove_emoticons(x))</span></pre><p id="d9ed" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 8) </strong> <strong class="ki iu">拼写纠正:</strong>亚马逊评论上，拼写错误多如牛毛。产品评论有时充满了匆忙发送的评论，有时几乎无法辨认。</p><p id="a608" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这方面，拼写纠正是一个有用的预处理步骤，因为这也将有助于我们减少单词的多个副本。例如，“分析”和“分析”将被视为不同的词，即使它们在同一意义上使用。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="0b76" class="nm ma it ni b gy nn no l np nq">from textblob import TextBlob<br/>df['Text'][:5].apply(lambda x: str(TextBlob(x).correct()))</span></pre><p id="d959" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">9.<strong class="ki iu">词条释义:</strong> <em class="nc">词条释义</em>是将一个单词转换成其基本形式的过程。词汇化考虑上下文，将单词转换成有意义的基本形式。例如:</p><p id="e028" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“关怀”-&gt;“词化”-&gt;“关怀”</p><p id="c8b9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Python NLTK提供了<strong class="ki iu"> WordNet词条整理器</strong>，它使用WordNet数据库来查找单词的词条。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="7e48" class="nm ma it ni b gy nn no l np nq">import nltk<br/>from nltk.stem import WordNetLemmatizer <br/><br/># Init the Wordnet Lemmatizer<br/>lemmatizer = WordNetLemmatizer()</span><span id="992a" class="nm ma it ni b gy nr no l np nq">df['Text'] = df['Text'].apply(lambda x: <!-- -->lemmatizer<!-- -->(x))</span></pre><p id="9f67" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于词汇化的更详细的背景，可以查看<a class="ae kf" href="https://www.datacamp.com/community/tutorials/stemming-lemmatization-python" rel="noopener ugc nofollow" target="_blank"> Datacamp </a>。</p><p id="b059" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我将停止清理数据。但是，作为一名研究人员，您可能需要根据您的数据进行更多的文本清理。例如，您可能希望使用:</p><p id="0bae" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">⚫对文本数据进行词干分析</p><p id="5e8e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">⚫拼写纠正的替代方法<em class="nc">:孤立词</em>纠正和<em class="nc">上下文相关</em>纠正方法</p><p id="e7d9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">⚫不同的包使用不同数量的停用词。你可以试试其他的NLP包。</p><h1 id="ea1a" class="lz ma it bd mb mc nx me mf mg ny mi mj mk nz mm mn mo oa mq mr ms ob mu mv mw bi translated"><strong class="ak">特征提取-第二轮</strong></h1><p id="a325" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">一些特征将在文本清理后提取，因为它们在此步骤获得更有意义。例如，如果我们在数据清理之前提取这个特征，字符的数量会受到URL链接的严重影响。此时，我们必须尝试提取尽可能多的特征，因为额外的特征有机会在文本分析期间提供有用的信息。我们不必担心这些功能将来是否真的有用。在最坏的情况下，我们不使用它们。</p><ol class=""><li id="7131" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="ki iu">字数:</strong>此功能告知评论中有多少字</li></ol><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="4c80" class="nm ma it ni b gy nn no l np nq">df['word_count'] = df['Text'].apply(lambda x: len(str(x).split(" ")))</span><span id="03fb" class="nm ma it ni b gy nr no l np nq">df[['Text','word_count']].head()</span></pre><p id="a879" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.<strong class="ki iu">字数:</strong>评论中包含多少个字母。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="d590" class="nm ma it ni b gy nn no l np nq">df['char_count'] = df['Text'].str.len() ## this also includes spaces</span><span id="ead5" class="nm ma it ni b gy nr no l np nq">df[['Text','char_count']].head()</span></pre><p id="7433" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.<strong class="ki iu">平均单词长度:</strong>评论中单词的平均字母数。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="4459" class="nm ma it ni b gy nn no l np nq">def avg_word(sentence):<br/>    words = sentence.split()<br/>    return (sum(len(word) for word in words)/(len(words)+0.000001))</span><span id="997e" class="nm ma it ni b gy nr no l np nq">df['avg_word'] = df['Text'].apply(lambda x: avg_word(x)).round(1)<br/>df[['Text','avg_word']].head()</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi of"><img src="../Images/d06207b74d0444454bf0aa959bb2ceb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*iz8BWW1Kzzn63PFvzZ7fuw.png"/></div></figure><p id="3045" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们检查提取的要素在数据框中的样子:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="6d9c" class="nm ma it ni b gy nn no l np nq">df.sample(5)</span></pre><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/129d820ebb7f101620d2e551d06ad149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zwa3g5Ei5vts3gKksTXxvg.png"/></div></div></figure><h1 id="8877" class="lz ma it bd mb mc nx me mf mg ny mi mj mk nz mm mn mo oa mq mr ms ob mu mv mw bi translated"><strong class="ak">结论</strong></h1><p id="18fa" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">这项研究解释了文本清洗的步骤。此外，<em class="nc"> </em> <strong class="ki iu"> <em class="nc">本指南的独特之处在于，在文本清理之前和文本清理之后，通过两轮</em> </strong> <em class="nc"> : </em> <strong class="ki iu"> <em class="nc">来完成特征提取。我们需要记住，对于实际的研究来说，文本清理是一个递归过程。一旦我们发现异常，我们会回来通过解决异常进行更多的清理。</em></strong></p><blockquote class="oh oi oj"><p id="db20" class="kg kh nc ki b kj kk kl km kn ko kp kq ok ks kt ku ol kw kx ky om la lb lc ld im bi translated">*特别感谢我的朋友塔比瑟·斯蒂克尔校对了这篇文章。</p></blockquote></div></div>    
</body>
</html>