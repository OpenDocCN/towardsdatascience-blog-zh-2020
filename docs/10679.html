<html>
<head>
<title>Sentiment Classification with Logistic Regression — Analyzing Yelp Reviews</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于逻辑回归的情感分类 Yelp 评论分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-classification-with-logistic-regression-analyzing-yelp-reviews-3981678c3b44?source=collection_archive---------14-----------------------#2020-07-26">https://towardsdatascience.com/sentiment-classification-with-logistic-regression-analyzing-yelp-reviews-3981678c3b44?source=collection_archive---------14-----------------------#2020-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/2cb73880f6c6fd14d4242cc1397055d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7LVoZs6rVS0upkf9"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">柴坦尼亚·皮拉拉在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7755" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">几周前你刚开了自己的公司。</p><p id="68ea" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你想了解顾客对你的产品和服务的感受，所以你去了社交媒体平台(推特、脸书等)。)来看看人家都说了些什么。</p><p id="7e6f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">哇哦。好像已经有几千条关于你的生意的帖子了。你很兴奋，但是你很快意识到自己一个人读完所有的书是不可行的。</p><p id="2e08" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你开始怀疑:有没有办法从文本信息中提取客户的情绪？</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="784b" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">古玩目录</h1><ol class=""><li id="6d89" class="mj mk it ki b kj ml kn mm kr mn kv mo kz mp ld mq mr ms mt bi translated"><a class="ae kf" href="#d046" rel="noopener ugc nofollow">为什么情感分析很重要？</a></li><li id="ab2a" class="mj mk it ki b kj mu kn mv kr mw kv mx kz my ld mq mr ms mt bi translated"><a class="ae kf" href="#c3b8" rel="noopener ugc nofollow">什么是逻辑回归？</a></li><li id="b57b" class="mj mk it ki b kj mu kn mv kr mw kv mx kz my ld mq mr ms mt bi translated"><a class="ae kf" href="#e4b4" rel="noopener ugc nofollow">我们应该使用哪些指标来评估模型？</a></li><li id="a4a1" class="mj mk it ki b kj mu kn mv kr mw kv mx kz my ld mq mr ms mt bi translated"><a class="ae kf" href="#0c67" rel="noopener ugc nofollow">模型如何理解文本输入？</a></li><li id="4cc4" class="mj mk it ki b kj mu kn mv kr mw kv mx kz my ld mq mr ms mt bi translated"><a class="ae kf" href="#1575" rel="noopener ugc nofollow">我们如何知道哪些文本特征是重要的？</a></li><li id="68d6" class="mj mk it ki b kj mu kn mv kr mw kv mx kz my ld mq mr ms mt bi translated">我们能进一步改进我们的模型吗？</li><li id="479d" class="mj mk it ki b kj mu kn mv kr mw kv mx kz my ld mq mr ms mt bi translated"><a class="ae kf" href="#7cf1" rel="noopener ugc nofollow">接下来我们能做什么？</a></li></ol></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="d046" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">动机</strong></h1><p id="3c11" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">情感分析是一种非常有效的工具，不仅可以让企业了解整体品牌认知，还可以评估客户对特定产品线或服务的态度和情感[1]。</p><p id="c4f3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种数据驱动的方法可以帮助企业更好地了解客户，发现他们观点的微妙变化，以满足不断变化的需求。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="c531" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">概观</h1><p id="86d2" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">这篇文章是我探索 Yelp 数据集的第二部分。有关数据集的更多信息，以及对其<strong class="ki iu">业务</strong>数据和<strong class="ki iu">提示</strong>数据的一些探索性数据分析，请参见我下面的帖子:</p><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/discover-your-next-favorite-restaurant-exploration-and-visualization-on-yelps-dataset-157d9799123c"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">发现你下一个最喜欢的餐馆 Yelp 数据集上的探索和可视化</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">你用 Yelp 找好餐馆吗？这篇文章揭示了流行的 Yelp 数据集中的见解和模式。</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt jz nf"/></div></div></a></div><p id="ae82" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章主要关注 Yelp 数据集中的<strong class="ki iu"> review.json </strong>文件，它包含了 Yelp 用户写的评论。此外，每个评论都包括一个相应的“星级”，或用户对企业的评级，可以作为情绪的代理。目标是建立一个模型，在给定文本数据的情况下，可以对评论的情绪进行分类(正面或负面)。此外，我们感兴趣的是哪些文本特征是对该分类任务最有帮助的预测器。</p><p id="c3b8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">逻辑回归</strong></p><p id="e961" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一般来说，有两种不同类型的分类模型:<strong class="ki iu">生成式</strong>模型<strong class="ki iu"> </strong>(朴素贝叶斯、隐马尔可夫模型等。)<strong class="ki iu"> </strong>和<strong class="ki iu">判别型</strong>模型(Logistic 回归、SVM 等)。).最终，两个模型都试图计算 p(类|要素)或 p(y|x)。关键区别在于，生成模型试图首先对联合概率分布 p(x，y)建模，然后使用<strong class="ki iu"> Baye 定理</strong>计算条件概率 p(y|x)，而判别模型直接对 p(y|x)建模。关于这两种模型之间的比较的详细讨论，参见吴恩达的论文<a class="ae kf" href="http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="ab3a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我将检验一个流行的判别模型——逻辑回归。关于它的数学基础(sigmoid 函数、成本函数、决策边界等)的更多细节，见<a class="ae kf" rel="noopener" target="_blank" href="/logistic-regression-detailed-overview-46c4da4303bc">这篇文章</a>。).</p><p id="9596" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要查看我的完整 Python 代码，请查看我的<a class="ae kf" href="https://www.kaggle.com/dehaozhang/sentiment-analysis-with-lr" rel="noopener ugc nofollow" target="_blank"> Kaggle 内核</a>或我的<a class="ae kf" href="https://github.com/terryz1/Yelp_Sentiment_Analysis" rel="noopener ugc nofollow" target="_blank"> Github 页面</a>。现在让我们开始吧！</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="3c97" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">偷看评论</strong></p><p id="7d4b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从评论数据集中取出<strong class="ki iu">1</strong>26】百万条记录来进行分析。“文本”列将是我们的模型输入。让我们来看看一个有积极情绪的随机评论(评分为 5.0):</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="a7e6" class="od lm it nz b gy oe of l og oh">"I love Deagan's. I do. I really do. The atmosphere is cozy and festive. The shrimp tacos and house fries are my standbys. The fries are sometimes good and sometimes great, and the spicy dipping sauce they come with is to die for. The beer list is amazing and the cocktails are great. The prices are mid-level, so it's not a cheap dive you can go to every week, but rather a treat when you do. Try it out. You won't be disappointed!"</span></pre><p id="e9f1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们考虑特征提取时，这里的几个线索可以帮助我们推断这是积极的情绪，如“爱”、“舒适”、“为...而死”、“令人惊叹”和“不会失望”。</p><p id="d695" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个负面评价(评分 1.0):</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="6e15" class="od lm it nz b gy oe of l og oh">"If I could give less than one star, that would have been my choice.  I rent a home and Per my lease agreement it is MY responsibility to pay their Pool Service company.  Within the last year they changed to PoolServ.  I have had  major issues with new techs every week, never checking PH balances, cleaning the filter, and not showing up at all 2 weeks in the past 2 months. I have had 4 different techs in the past 4 weeks.   I have emailed and called them and they never respond back nor even acknowledged my concerns or requests.  I cannot change companies but I'm required to still pay for lousy or no service.  Attached are a couple pictures of my pool recently due to one tech just didn't put any chlorine in it at all according to the tech who came the following week to attempt to clean it up.  Please think twice before working with these people.  No one wants to work with a business that doesn't return phone calls or emails."</span></pre><p id="e744" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管它很长，但我们仍然可以看到诸如“不到一星”、“糟糕”、“从不回复”、“没有服务”等线索是有用的预测因素。</p><p id="7d16" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们快速确认数据中没有缺失值:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="3e4b" class="od lm it nz b gy oe of l og oh">text     0.0<br/>stars    0.0<br/>dtype: float64</span></pre><p id="348c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">将评分转化为积极和消极情绪</strong></p><p id="83ee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们绘制收视率分布图:</p><figure class="nu nv nw nx gt ju gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a73871529dedbf3fe1fb15d9246ecef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*7e52mwoVHJgcZf7U7shfrQ.png"/></div></figure><p id="3df4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，在 100 万条评论中，几乎有一半包含 5.0 的评级。</p><p id="541a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个任务中，我们希望将所有的评论文本分为两类:积极情绪和消极情绪。因此，我们首先需要将“星星”值转换成两个类别。在这里，我们可以将‘4.0’和‘5.0’视为正面情绪，将‘1.0’和‘2.0’视为负面情绪。我们可以将“3.0”视为中性，甚至将每个明星视为其自己的情感类别，这将使其成为一个多类分类问题。但是，为了简单的二进制分类，我们可以将带有“3.0”的排除在外。</p><p id="a29f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们将积极情绪编码为 0 类，消极情绪编码为 1 类。因为我们知道类 0 中的样本比类 1 中的样本多，所以我们的基线模型可以是简单地将每个评论标记为类 0 的模型。让我们检查一下基线精度:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="7ef1" class="od lm it nz b gy oe of l og oh">0.74</span></pre><p id="b056" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们继续之前，让我们花点时间了解一下评估指标。</p><p id="e4b4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">评估指标</strong></p><p id="75c4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Jason Brownlee 在他的一篇文章中说，“分类器的好坏取决于用来评估它的度量标准”[2]。</p><p id="940e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在每个分类问题中，准确性可能不是合适的评估标准，特别是当类别分布不平衡时，以及当假阳性和假阴性的业务影响不相等时。例如，在信用卡欺诈检测问题中，预测每个交易是非欺诈的基线模型将具有超过 99.99%的准确度，但这并不意味着它是合适的模型，因为假阴性的百分比将是 100%，并且与假阳性相比，每个假阴性(未检测到欺诈交易)对企业和客户来说可能具有高得多的成本。</p><p id="66cc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此任务中，以下是一些合适的评估指标:</p><ol class=""><li id="68a9" class="mj mk it ki b kj kk kn ko kr oj kv ok kz ol ld mq mr ms mt bi translated"><strong class="ki iu">精度</strong> — TP/(TP+FP)，意思是模型归类为阳性的点实际上是阳性的比例。</li><li id="dc1d" class="mj mk it ki b kj mu kn mv kr mw kv mx kz my ld mq mr ms mt bi translated"><strong class="ki iu">回忆</strong> — TP/(TP+FN)，意思是被模型正确分类的实际阳性的比例。</li><li id="857d" class="mj mk it ki b kj mu kn mv kr mw kv mx kz my ld mq mr ms mt bi translated"><strong class="ki iu"> F1 得分</strong>—精确度和召回率的调和平均值。</li></ol><p id="9926" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看<a class="ae kf" rel="noopener" target="_blank" href="/beyond-accuracy-precision-and-recall-3da06bea9f6c">这篇文章</a>，了解关于这些指标的更多详细讨论。</p><p id="0083" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此任务中，我们将使用测试集上的<strong class="ki iu"> F1 分数作为关键评估指标。</strong></p><p id="22fa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">列车测试分割</strong></p><p id="32d6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们留出 30%的数据作为测试集，按类标签分层。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="b3c6" class="od lm it nz b gy oe of l og oh">train, test = train_test_split(df_reviews, test_size = 0.3, stratify = df_reviews['labels'], random_state = 42)</span></pre><p id="40f3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">文本预处理</strong></p><p id="46bf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在大多数文本挖掘或 NLP 相关的任务中，清理文本是至关重要的一步。让我们首先删除所有非字母字符，标点符号，并确保所有字母都是小写字母。稍后我们还将评估移除停用词和词干化/词条化的效果。</p><p id="0c67" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">矢量化</strong></p><p id="8190" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了让模型能够处理文本输入，我们需要将它们转换成向量。有几种不同的方法来表示这些文本特征，下面是最常见的几种:二进制，例如，单词“good”是否存在。2.计数，例如“好”这个词在这篇综述中出现了多少次，类似于朴素贝叶斯中的单词袋模型。3.TF-IDF，这是与文档相关的每个文本特征的加权重要性(在此阅读更多<a class="ae kf" rel="noopener" target="_blank" href="/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089"/>)。</p><p id="7c2f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先尝试使用所有单字的二进制表示。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="4cac" class="od lm it nz b gy oe of l og oh">cv= CountVectorizer(binary=True, analyzer = text_prep, min_df = 10, max_df = 0.95)<br/>cv.fit_transform(train['text'].values)<br/>train_feature_set=cv.transform(train['text'].values)<br/>test_feature_set=cv.transform(test['text'].values)</span></pre><p id="258b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我使用了 sklearn 的 CountVectorizer 对象来提取所有的单词特征，如果这个单词出现在少于 10 条评论或超过 95%的评论中，就会被排除。</p><p id="2e4f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们检查一下我们的字典里有多少独特的单词:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="3252" class="od lm it nz b gy oe of l og oh">train_feature_set.shape[1]<br/>--------------------------------------------------------------------<br/>40245</span></pre><p id="f6d0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大约有 40K 个独特的单词。例如，让我们检查单词“tasty”的索引:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="8cc5" class="od lm it nz b gy oe of l og oh">cv.vocabulary_['tasty']<br/>--------------------------------------------------------------------<br/>35283</span></pre><p id="6bcc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">适合 LR 车型</strong></p><p id="9d5a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们准备使用 sklearn 拟合我们的第一个逻辑回归模型:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="8e8f" class="od lm it nz b gy oe of l og oh">lr = LogisticRegression(solver = 'liblinear', random_state = 42, max_iter=1000)<br/>lr.fit(train_feature_set,y_train)<br/>y_pred = lr.predict(test_feature_set)<br/>print("Accuracy: ",round(metrics.accuracy_score(y_test,y_pred),3))<br/>print("F1: ",round(metrics.f1_score(y_test, y_pred),3))<br/>--------------------------------------------------------------------<br/>Accuracy:  0.955<br/>F1:  0.914</span></pre><p id="887f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们绘制一个混淆矩阵来可视化预测结果:</p><figure class="nu nv nw nx gt ju gh gi paragraph-image"><div class="gh gi om"><img src="../Images/36650e77e3a7b593add70b501f39c8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*YUn4OvxQwYIeconfxc8eEQ.png"/></div></figure><p id="1575" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">可视化特征重要性</strong></p><p id="057d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">逻辑回归的一个好处是，我们可以很容易地找到每个特征的重要性。让我们想象一下与消极情绪最相关的前 10 个词:</p><figure class="nu nv nw nx gt ju gh gi paragraph-image"><div class="gh gi on"><img src="../Images/34e5bba73e0a4ec45d783696d2fdf1f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*AYLtuGm9eUsj_rpSGD-HJw.png"/></div></figure><p id="02a6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些话在我们的预料之中。让我们检查所有包含“中毒”一词的评论，看看有多少属于负面情绪类别:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="7605" class="od lm it nz b gy oe of l og oh">0.904</span></pre><p id="10d0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，让我们来看看积极情绪的前 10 个相关词:</p><figure class="nu nv nw nx gt ju gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/1d6cffe7a619709101b721032dc6a948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*1uTHZqqD_po40sMF5KmNvQ.png"/></div></figure><p id="937c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，高肯定特征重要性与类 1 的高可能性相关，低否定(高绝对值)特征重要性与类 0 的高可能性相关。</p><p id="f907" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在检查了积极的评论后，我意识到“对接”这个词在顶部，因为大多数给“4.0”星的评论都提到“我对接一颗星是因为……”。</p><p id="2778" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">改进策略</strong></p><p id="70c9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们建立了第一个模型之后，让我们检查几个想法，看看我们的模型是否可以进一步改进。</p><blockquote class="op oq or"><p id="c735" class="kg kh os ki b kj kk kl km kn ko kp kq ot ks kt ku ou kw kx ky ov la lb lc ld im bi translated">想法 1:降低概率截止阈值</p></blockquote><p id="5d20" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了减少假阴性，一种直觉是降低截止阈值(默认为 0.5)。这将提高召回率，但也会降低准确率。因此，我们需要检查这是否会提高 F1 的整体得分:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="7e53" class="od lm it nz b gy oe of l og oh">******** For i = 0.3 ******<br/>F1: 0.91<br/><br/>******** For i = 0.4 ******<br/>F1: 0.915<br/><br/>******** For i = 0.45 ******<br/>F1: 0.915<br/><br/>******** For i = 0.5 ******<br/>F1: 0.914</span></pre><p id="c96c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，F1 分数对该阈值的变化相对稳健。</p><blockquote class="op oq or"><p id="df74" class="kg kh os ki b kj kk kl km kn ko kp kq ot ks kt ku ou kw kx ky ov la lb lc ld im bi translated">想法 2:过采样类别 1 或欠采样类别 0</p></blockquote><p id="0f35" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">过采样少数类和欠采样多数类是处理不平衡分类的常用方法(此处<a class="ae kf" href="https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/" rel="noopener ugc nofollow" target="_blank">阅读更多</a>)。然而，在这种情况下，F1 分数并没有提高。</p><p id="68a5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">过采样等级 1 的性能:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="5bf7" class="od lm it nz b gy oe of l og oh">Accuracy:  0.95<br/>F1:  0.908</span></pre><p id="f883" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">欠采样等级 0 的性能:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="07ff" class="od lm it nz b gy oe of l og oh">Accuracy:  0.947<br/>F1:  0.904</span></pre><blockquote class="op oq or"><p id="922a" class="kg kh os ki b kj kk kl km kn ko kp kq ot ks kt ku ou kw kx ky ov la lb lc ld im bi translated">想法 3:去掉停用词和词干</p></blockquote><p id="489e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">去除停用词和词干可以去除噪音，从而减少词汇量。然而，准确度和 F1 分数<strong class="ki iu">都略微降低</strong>。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="7311" class="od lm it nz b gy oe of l og oh">Accuracy:  0.949<br/>F1:  0.902</span></pre><blockquote class="op oq or"><p id="399a" class="kg kh os ki b kj kk kl km kn ko kp kq ot ks kt ku ou kw kx ky ov la lb lc ld im bi translated">想法 4:用 TF-IDF 代替二进制表示</p></blockquote><p id="420c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这一次，F1 分数略有增加，但增幅不大。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="432f" class="od lm it nz b gy oe of l og oh">Accuracy:  0.958<br/>F1:  0.919</span></pre><blockquote class="op oq or"><p id="688c" class="kg kh os ki b kj kk kl km kn ko kp kq ot ks kt ku ou kw kx ky ov la lb lc ld im bi translated">想法 5:将单词和双词作为特征包含进来</p></blockquote><p id="6074" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">动机可以用这个例子来说明:</p><p id="7e73" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以我们最初开发的 LR 模型为例，然后在这篇评论中预测——“我不喜欢食物或服务”:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="e4ed" class="od lm it nz b gy oe of l og oh">test_review = cv.transform(["I did not enjoy the food or the service"])<br/>lr.predict_proba(test_review)<br/>--------------------------------------------------------------------<br/>array([[0.50069323, 0.49930677]])</span></pre><p id="088a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型认为这一评论是积极的，因为该模型只接受与积极情绪相关联的单词，如“enjoy”，而不考虑否定“enjoy”含义的“did not”。</p><p id="eb80" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们把单词和双词(两个单词的序列)都考虑进去之后，我们首先看到词汇量的增加:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="1fa9" class="od lm it nz b gy oe of l og oh">488683</span></pre><p id="36f2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">拟合该模型后，我们看到两个指标都有所改善，尤其是 F1 得分。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="0fcc" class="od lm it nz b gy oe of l og oh">Accuracy:  0.969<br/>F1:  0.942</span></pre><p id="465a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们再次检查它对同一个句子的预测:</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="5395" class="od lm it nz b gy oe of l og oh">test_review = cv.transform(["I did not enjoy the food or the service"])<br/>lr.predict_proba(test_review)<br/>--------------------------------------------------------------------<br/>array([[0.2678198, 0.7321802]])</span></pre><p id="339f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，它以相对较高的可信度做出正确的预测。</p><p id="739b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以再次看到正面和负面情绪的 10 大新特征:</p><figure class="nu nv nw nx gt ju gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/4fb0bdf6dc1d191c6ef28a0ff6b76c34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*nR17i1jy5ZanDjKaa8Q1yQ.png"/></div></figure><p id="da18" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们开始看到诸如“两颗星”、“不值得”、“不谢谢”、“不推荐”之类的二元词出现在热门功能中。</p><figure class="nu nv nw nx gt ju gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/839e1e3934aedda37cebfce8b5d3866c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*Xiqeu8piHPKq7r5MKwsjFA.png"/></div></figure><p id="5b04" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意,“失望”可能看起来是一种消极的情绪，但这可能是因为它是“不要失望”短语的一部分。</p><p id="52be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们应该包括三元模型(三个单词的序列)或者更高阶的 N 元模型吗？请注意，随着我们包含越来越高阶的 N 元文法，我们的特征大小变得越来越大，这消耗了更多的内存空间，并且还存在极度稀疏的问题(在此阅读更多<a class="ae kf" href="http://louistiao.me/posts/serious-shortcomings-of-n-gram-feature-spaces-in-text-classification/#:~:text=The%20major%20drawback%20of%20feature,respect%20to%20learned%20training%20data." rel="noopener ugc nofollow" target="_blank"/>)。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="7cf1" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">后续步骤</h1><p id="8bd8" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">以下是对后续步骤的一些想法:</p><ol class=""><li id="5eec" class="mj mk it ki b kj kk kn ko kr oj kv ok kz ol ld mq mr ms mt bi translated">LR 模型中有几个超参数我们可以调整，调整它们可以得到更优化的模型。此外，为了避免过度拟合，尝试交叉验证以获得每个模型更准确的指标(点击这里查看 grid search<a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"/>)。</li><li id="47cb" class="mj mk it ki b kj mu kn mv kr mw kv mx kz my ld mq mr ms mt bi translated">将“3.0”视为中性，或将每个类别视为其自己的类别，并重新制定这些模型。</li><li id="754e" class="mj mk it ki b kj mu kn mv kr mw kv mx kz my ld mq mr ms mt bi translated">建立一个交互式情感分析器，允许用户输入评论，并给出对其情感的预测。具有增量学习的内置功能，用户可以帮助模型在做出错误预测时进行学习。</li></ol></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="e4ff" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">摘要</h1><p id="f06a" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">让我们回顾一下。</p><p id="bf67" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用逻辑回归建立了一个情感分类模型，并尝试了不同的策略来改进这个简单的模型。在这些想法中，包含二元模型作为特征对 F1 分数的提高最大。对于简单模型和改进模型，我们还分析了其最重要的文本特征。</p><p id="b5d1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这篇文章，并请分享你的想法:)</p><p id="5e3c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DS/ML 初学者？查看我的另一篇关于如何使用经典 Iris 数据集构建第一个 Python 分类器的帖子:</p><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/exploring-classifiers-with-python-scikit-learn-iris-dataset-2bcb490d2e1b"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">使用 Python Scikit-learn-Iris 数据集探索分类器</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">如何用 Python 构建第一个分类器的分步指南。</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="ox l nq nr ns no nt jz nf"/></div></div></a></div></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="e016" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">参考</h1><p id="7e45" class="pw-post-body-paragraph kg kh it ki b kj ml kl km kn mm kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">[1]<a class="ae kf" href="http://coppertaurus.com/insights/sentiment-analysis-product-management/#:~:text=Product%20reviews%20provide%20sentiment%20analysis,subsequent%20refinement%20of%20their%20offering." rel="noopener ugc nofollow" target="_blank">http://copper Taurus . com/insights/opinion-analysis-Product-management/#:~:text = Product % 20 reviews % 20 provide % 20 opinion % 20 analysis，successive % 20 refinement % 20 of % 20 their % 20 offering。</a>【2】<a class="ae kf" href="https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/tour-of-evaluation-metrics-for-unbalanced-class ification/</a></p></div></div>    
</body>
</html>