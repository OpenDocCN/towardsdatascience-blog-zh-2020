<html>
<head>
<title>4 Simple Hacks Every Data Scientist should know.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每个数据科学家都应该知道的 4 个简单技巧。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/4-simple-hacks-every-data-scientist-should-know-45928b3dda37?source=collection_archive---------52-----------------------#2020-04-21">https://towardsdatascience.com/4-simple-hacks-every-data-scientist-should-know-45928b3dda37?source=collection_archive---------52-----------------------#2020-04-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e4f9" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习技巧/窍门</h2><div class=""/><div class=""><h2 id="1514" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">这些技巧肯定会在任何项目中节省你的时间，并提高你的工作效率。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3dce3b065edb152426487b824a99deff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oKHf8Q2fNItKxhHw"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@itfeelslikefilm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">🇸🇮·扬科·菲利</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4940" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> T </span> <strong class="lk jd"> <em class="mn">通过这篇文章，我们将了解一些简单的技巧，它们可以真正提高您的机器学习、人工智能或数据科学项目的生产率。📔</em>T9】</strong></p><p id="8c3f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mn">希望你喜欢！！😊</em></p><h1 id="ea7f" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated"><strong class="ak"> 1。树木的可视化</strong></h1><p id="01be" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">在使用任何机器学习模型时，我们只知道哪个模型将适合数据集来解决特定问题，并且基于数据集，我们通过一些数学/逻辑推理来设置模型的参数。该模型工作完全正常，符合目的。但是没人关心黑盒里有什么。听起来没那么吸引人，对吧？对于任何一个树模型来说都不是。让我们看看如何可视化黑盒。</p><p id="229f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将导入一些必要的库来设置我们自己的任务！</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="a8cc" class="nq mp it nm b gy nr ns l nt nu">from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier,plot_tree<br/>from sklearn.datasets import load_wine<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import pandas as pd</span></pre><p id="46bf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为此，我们将使用著名的葡萄酒数据集。将数据集加载到 Pandas 数据框中，并分离为预测变量和响应变量。</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="1edb" class="nq mp it nm b gy nr ns l nt nu">wine=load_wine()</span><span id="fb83" class="nq mp it nm b gy nv ns l nt nu">df = pd.DataFrame(data= np.c_[wine['data'], wine['target']],<br/>                   columns= wine['feature_names'] + ['target'])</span><span id="ded6" class="nq mp it nm b gy nv ns l nt nu">X = df.drop('target',axis=1)<br/>y = df["target"]</span><span id="8f73" class="nq mp it nm b gy nv ns l nt nu">features = df.columns[:-1].values.tolist()</span></pre><p id="f7bb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将数据分成训练集和测试集，选择适当的模型并拟合该模型。</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="a138" class="nq mp it nm b gy nr ns l nt nu">X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 2020)</span><span id="0c4b" class="nq mp it nm b gy nv ns l nt nu">model = DecisionTreeClassifier()distinguish between two things</span><span id="0641" class="nq mp it nm b gy nv ns l nt nu">model.fit(X_train, y_train)</span></pre><p id="2f07" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将使用 sklearn 的库。(<em class="mn">导入 sklearn.tree.plot_tree </em>)</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="3c72" class="nq mp it nm b gy nr ns l nt nu">plt.figure(figsize = (20, 10))<br/>plot_tree(model, feature_names = features, filled = True)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/57d47db1d4c309d498a4fb39c0dabe38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wP3Fmkgj0GpOVtkQwZQfjQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:我的谷歌 Colab 笔记本</p></figure><p id="f5e5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它告诉我们关于样本(元组的数量)、每个节点中的特征、节点的数量、我们制作树的标准(这里，默认情况下，我们使用基尼指数)。</p><p id="9ffd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">更多细节和示例请参考<a class="ae lh" href="https://scikit-learn.org/stable/modules/tree.html#tree" rel="noopener ugc nofollow" target="_blank">文件</a>。</p><h1 id="44a1" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated"><strong class="ak"> 2。</strong>在单个图中绘制 ROC 曲线</h1><p id="c38a" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">ROC ( <em class="mn">受试者操作特征</em>)曲线告诉我们该模型在将班级分成两个或更多个班级方面有多好。更好的模型可以准确地区分两者或更多。然而，一个差的模型将很难在两者之间进行分离。更多细节请参考<a class="ae lh" rel="noopener" target="_blank" href="/understanding-the-roc-and-auc-curves-a05b68550b69">文章</a>。</p><p id="fb05" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">导入一些必需的库</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="4726" class="nq mp it nm b gy nr ns l nt nu">from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import plot_roc_curve<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.datasets import load_breast_cancer</span></pre><p id="537e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为此，我们将使用乳腺癌数据集。将数据集加载到 Pandas 数据框中，并分离为预测变量和响应变量。</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="52d1" class="nq mp it nm b gy nr ns l nt nu">db=load_breast_cancer()<br/>df = pd.DataFrame(data= np.c_[db['data'], db['target']])</span><span id="6770" class="nq mp it nm b gy nv ns l nt nu">df=df.rename(columns={30:'target'})</span><span id="4259" class="nq mp it nm b gy nv ns l nt nu">X = df.drop('target',axis=1)<br/>y = df['target']</span></pre><p id="0f23" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将数据分成训练集和测试集，选择适当的模型并拟合该模型。</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="7f72" class="nq mp it nm b gy nr ns l nt nu">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)</span><span id="ddaf" class="nq mp it nm b gy nv ns l nt nu">dt = DecisionTreeClassifier()<br/>rf = RandomForestClassifier()</span><span id="26df" class="nq mp it nm b gy nv ns l nt nu">dt.fit(X_train, y_train)<br/>rf.fit(X_train, y_train)</span></pre><p id="f231" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用<em class="mn">sk learn . metrics . plot _ roc _ curve</em>库显示图形。</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="0d57" class="nq mp it nm b gy nr ns l nt nu">disp = plot_roc_curve(dt, X_test, y_test)<br/>plot_roc_curve(rf, X_test, y_test, ax = disp.ax_)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/2e5aa51bb591ed1b6a76cee506c990f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*CWEYUTzmpc9LN_U3szzhTg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:我的谷歌 Colab 笔记本</p></figure><p id="f8f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看到两个分类器之间的 ROC 曲线以及 AUC 分数。随机森林分类器的 AUC 大于决策树分类器，因此随机森林分类器比决策树分类器更合适。</p><p id="4d31" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">更多细节和示例请参考<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html?highlight=sklearn%20metric%20plot_roc_curve#sklearn-metrics-plot-roc-curve" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><h1 id="333e" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated"><strong class="ak"> 3。修剪决策树</strong></h1><p id="249f" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">当在某些数据集上训练决策树时，我们经常过度拟合训练集，因此我们的模型在测试集上的精度较低。这个问题可以通过修剪树的方法来解决。这种修剪过的树比没有修剪过的树表现更好。有不同的方法可以用来修剪树木。它们是后剪枝、前剪枝、成本复杂性剪枝，我们将对我们的模型使用成本复杂性剪枝。</p><p id="f366" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">DecisionTreeClassifier 提供了各种参数来防止树过度拟合。其中一个是成本复杂度参数，<strong class="lk jd"><em class="mn">CCP _ 阿尔法</em> </strong>。<strong class="lk jd"><em class="mn">CCP _ 阿尔法</em> </strong>的值越大，修剪的节点数越多。这里我们只展示了<strong class="lk jd"><em class="mn">CCP _ 阿尔法</em> </strong>对正则化树的影响，以及如何根据验证分数选择一个<strong class="lk jd"><em class="mn">CCP _ 阿尔法</em> </strong>。</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="df52" class="nq mp it nm b gy nr ns l nt nu">from sklearn.model_selection import cross_val_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.datasets import load_breast_cancer</span><span id="8f99" class="nq mp it nm b gy nv ns l nt nu">db=load_breast_cancer()</span><span id="967b" class="nq mp it nm b gy nv ns l nt nu">df = pd.DataFrame(data= np.c_[db['data'], db['target']])</span><span id="0721" class="nq mp it nm b gy nv ns l nt nu">df=df.rename(columns={30:'target'})</span><span id="f179" class="nq mp it nm b gy nv ns l nt nu">X = df.drop('target',axis=1)<br/>y = df['target']</span></pre><p id="95b8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">型号 1。(<em class="mn">此处 Random_state 是为了再现性</em>)</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="01c7" class="nq mp it nm b gy nr ns l nt nu">model = DecisionTreeClassifier(random_state = 2020)</span><span id="2e64" class="nq mp it nm b gy nv ns l nt nu">model.fit(X, y)</span><span id="252d" class="nq mp it nm b gy nv ns l nt nu">score = cross_val_score(model, X, y, scoring = "accuracy")<br/>print("Model 1 has {} nodes and score is {}".format(model.tree_.node_count, score.mean()))</span></pre><p id="f4b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">型号 2 的<strong class="lk jd"><em class="mn">CCP _ 阿尔法</em> </strong>值等于 0.01</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="d145" class="nq mp it nm b gy nr ns l nt nu">model = DecisionTreeClassifier(ccp_alpha = 0.01, random_state = 2020)</span><span id="8f63" class="nq mp it nm b gy nv ns l nt nu">model.fit(X, y)</span><span id="bc62" class="nq mp it nm b gy nv ns l nt nu">score = cross_val_score(model, X, y, scoring = "accuracy")<br/>print("Model 2 has {} nodes and score is {}".format(model.tree_.node_count, score.mean()))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/52a81ec22c4b21ebcaee8f92a7776050.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*c5I94gAjaUQZusepDL4sDA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:我的谷歌 Colab 笔记本</p></figure><p id="f1a9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，分数上升了。修剪树木有很多好处，主要的好处是减少过度修剪。</p><p id="3193" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">更多细节和示例，请参考<a class="ae lh" href="https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><h1 id="1cef" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">4.情节混乱矩阵</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nz"><img src="../Images/162f97342d6c2e4ff1c028942694c95e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g92gP48BAvQzJ8oeZtugEw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781838555078/6/ch06lvl1sec34/confusion-matrix" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="1bcb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">混淆矩阵，名字本身就包含了混淆在里面。为什么会这样呢？第一次理解时，这个问题的答案似乎令人困惑，但一旦你理解了，它肯定会帮助你对你的模型做出一个富有成效的决定。更多细节请参考<a class="ae lh" rel="noopener" target="_blank" href="/understanding-confusion-matrix-a9ad42dcfd62">文章</a>。</p><p id="e9a8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mn">注意:-在任何其他来源中，混淆矩阵的表示可以不同。</em></p><p id="c239" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">导入一些必要的库</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="a4db" class="nq mp it nm b gy nr ns l nt nu">from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import plot_confusion_matrix<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.datasets import load_digits</span></pre><p id="6fc4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为此，我们将使用数字数据集。将数据集加载到 Pandas 数据框中，并分离为预测变量和响应变量。</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="177c" class="nq mp it nm b gy nr ns l nt nu">db=load_digits()</span><span id="4bd0" class="nq mp it nm b gy nv ns l nt nu">df = pd.DataFrame(data= np.c_[db['data'], db['target']])</span><span id="3f1f" class="nq mp it nm b gy nv ns l nt nu">df=df.rename(columns={64:'target'})</span><span id="c540" class="nq mp it nm b gy nv ns l nt nu">X = df.drop('target',axis=1)</span><span id="57cb" class="nq mp it nm b gy nv ns l nt nu">y = df['target']</span></pre><p id="b0f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将数据分成训练集和测试集，选择适当的模型并拟合该模型。</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="7aa1" class="nq mp it nm b gy nr ns l nt nu">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)</span><span id="bf37" class="nq mp it nm b gy nv ns l nt nu">model = LogisticRegression(random_state = 2020)</span><span id="e20f" class="nq mp it nm b gy nv ns l nt nu">model.fit(X_train, y_train)</span><span id="7462" class="nq mp it nm b gy nv ns l nt nu">disp = plot_confusion_matrix(model, X_test, y_test, cmap = "Blues")</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/e3f4be841ef93396ecb87a88e7defc42.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*eDOFsj7RwiSil9xP75AwYA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:我的谷歌 Colab 笔记本</p></figure><p id="75f5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">更多细节和示例请参考<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html?highlight=confusion%20metrics#sklearn-metrics-plot-confusion-matrix" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="e2d8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我相信这个技巧对你会有用，你会从这篇文章中有所收获。直到那时快乐编码！。</p></div></div>    
</body>
</html>