<html>
<head>
<title>Next-Generation Sequencing Data Analysis With PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PySpark进行新一代测序数据分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/next-generation-sequencing-data-analysis-with-pyspark-888a1e0a079?source=collection_archive---------14-----------------------#2020-01-24">https://towardsdatascience.com/next-generation-sequencing-data-analysis-with-pyspark-888a1e0a079?source=collection_archive---------14-----------------------#2020-01-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e5d2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用PySpark在Google Colab中分析基因组数据</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/745414b3eb93bb148978853c329de6e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s35X18Nnztomf4EC9xAesQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">信用:pix abay/monar</p></figure><blockquote class="ky kz la"><p id="901e" class="lb lc ld le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">有了DNA，你必须能够分辨哪些基因是打开的，哪些是关闭的。目前的DNA测序无法做到这一点。下一代DNA测序需要能够做到这一点。如果有人发明了这个，那么我们就可以开始非常精确地确定疾病的治疗方法。——埃隆·马斯克</p></blockquote><p id="c60c" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">单细胞RNA测序(scRNA-seq)等下一代测序技术的快速发展要求对大数据进行高效的并行处理和分析。Hadoop和Spark是存储和处理海量数据集的首选开源框架。Spark最显著的优势是其迭代分析能力与内存计算架构相结合。打电话。弹性分布式数据集(RDD)上的cache()有效地将其保存在内存中，并使其立即可用于计算；因此，后续的过滤、映射和归约任务变得即时。Spark有自己的查询语言Spark SQL，它的MLlib库非常适合机器学习任务。</p><p id="5c41" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">scRNA-seq可以做马斯克指出的事情——以细胞分辨率计算出哪些基因被打开/关闭。但这不是灵丹妙药——因为基因表达的调控并不停留在转录水平。mRNA或信使RNA——基因蛋白质编码区的代表——也包含决定核糖体蛋白质合成水平的序列。表达的蛋白质也可以经历一系列激活或抑制它的翻译后修饰。</p><h1 id="43c4" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">设置笔记本</h1><p id="7a0b" class="pw-post-body-paragraph lb lc it le b lf mt ju lh li mu jx lk ly mv ln lo lz mw lr ls ma mx lv lw lx im bi translated">这些年来，我一直喜欢用Google Colab做我所有快速而肮脏的项目原型。使用Colab，您可以跳过启动项目所需的所有初始步骤。无需设置虚拟环境、依赖关系等。此外，Colab还配有免费的GPU/TPU，可满足您所有的机器学习需求。</p><p id="2213" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">首先，去Colab，按照上面的链接，启动一个<em class="ld">新的Python 3笔记本</em>。PySpark需要Java (Java 1.8)和Scala，所以我们接下来会安装它们。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="3e39" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">现在安装PySpark。这里我们将安装pyspark[sql],因为它将允许我们处理。gtf，。床，。砰然后。萨姆稍后归档。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="6713" class="ng mc it nc b gy nh ni l nj nk">!pip install pyspark[sql]</span></pre><h1 id="c4a1" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">从NCBI序列读取档案(SRA)下载数据</h1><p id="1915" class="pw-post-body-paragraph lb lc it le b lf mt ju lh li mu jx lk ly mv ln lo lz mw lr ls ma mx lv lw lx im bi translated">已发表研究的序列数据可从<a class="ae my" href="https://www.ncbi.nlm.nih.gov/sra" rel="noopener ugc nofollow" target="_blank"> SRA </a>下载。在使用sra文件之前，您需要解压缩。使用<a class="ae my" href="https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software#header-global" rel="noopener ugc nofollow" target="_blank"> SRA工具包</a>中的fastq-dump工具。除了SRA文件，你可以直接下载。fastq、. fa.gz或. fastq.gz格式；现在可以用PySpark读取这些文件。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="3f68" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">注意文件的大小。</p><h1 id="5330" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">PySpark的魔力</h1><p id="42f4" class="pw-post-body-paragraph lb lc it le b lf mt ju lh li mu jx lk ly mv ln lo lz mw lr ls ma mx lv lw lx im bi translated">PySpark的核心是弹性分布式数据集(RDD)；它代表了可以并行操作的不可变的数据分区集合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/9ada462ca95b17eeefee7a376ecac275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*pvETiCVDPz85aYe_.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">信用:【https://cwiki.apache.org/】T4</p></figure><p id="3a4d" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">首先初始化一个spark上下文。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="cc11" class="ng mc it nc b gy nh ni l nj nk">import pyspark as spark<br/>from pyspark import SparkConf</span><span id="87d9" class="ng mc it nc b gy nm ni l nj nk">sc = spark.SparkContext.getOrCreate(conf=set_conf())</span></pre><p id="0f7c" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">为您的工作流传递一个最佳配置也是一个很好的实践。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="f587" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">数据分析</h1><p id="3416" class="pw-post-body-paragraph lb lc it le b lf mt ju lh li mu jx lk ly mv ln lo lz mw lr ls ma mx lv lw lx im bi translated">现在是时候读取我们的数据并将其转换到RDD了。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="6c2e" class="ng mc it nc b gy nh ni l nj nk">data = sc.textFile(path/to/fastq)<br/># in case you have a list of sequences<br/>data = sc.parallelize(your_list)</span><span id="b6b9" class="ng mc it nc b gy nm ni l nj nk"># lets take a look at the first read<br/># each read in fastq is represented by 4lines <br/>data.take(4)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/d99de7fda6381ba6767006701caef837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*YDMA5ifszBZk_o3wMhvn5g.png"/></div></figure><p id="3863" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">让我们只从数据中提取序列。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="8910" class="ng mc it nc b gy nh ni l nj nk">sequences = data.filter(lambda x: x.isalpha())<br/>sequences.count() # outputs the size of RDD - the number of reads<br/># =&gt; 1843156</span></pre><p id="3483" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">看一下前四段。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="149c" class="ng mc it nc b gy nh ni l nj nk">sequences.take(4)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/0f4a4a78c6b771c72e22a6f52dccc20f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y0xN8YpFYAnNVAfBSCwLVA.png"/></div></div></figure><p id="7d3b" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">找出读数的长度怎么样？这只需要一行代码！</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="6a02" class="ng mc it nc b gy nh ni l nj nk">read_lengths = sequences.map(lambda seq: len(seq))<br/>read_lengths.take(10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3bc98422b14393fdb6fd4ad13144e781.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*UZ6c5l2Ie36_n98vIxYoJQ.png"/></div></figure><p id="b5ee" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">接下来，计算读取的平均长度。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="87f6" class="ng mc it nc b gy nh ni l nj nk">len_sum = read_lengths.reduce(lambda a, b: a+b)<br/>len_sum//read_lengths.count()<br/># =&gt; 564</span></pre><p id="532b" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">最后，我们来算一下基数。首先，我们将使用<em class="ld"> list() </em>将序列分成单独的碱基，然后使用<em class="ld"> flatMap </em>将列表合并成一个。然后，我们遍历各自的基并创建一个元组，将第一个元素作为基，将“1”作为其值。然后我们使用<em class="ld"> reduceByKey </em>通过元组的第一个元素——key——来聚合值。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="6e0a" class="ng mc it nc b gy nh ni l nj nk">base_count = sequences.flatMap(lambda seq: list(seq))\<br/>                       .map(lambda c: (c, 1)) \<br/>                       .reduceByKey(lambda a, b: a+b)<br/>base_count</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/708ac53d1a86768a9bccd1637e4b2222.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*xUvcQjGMDT-vJxkH1P-3tg.png"/></div></figure><p id="283c" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">我们这里只关心A，T，G，C；剩下的都是神器。如你所见，这些序列富含GC。</p><p id="a11c" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">我在这里只触及了皮毛，您可以使用PySpark做许多很酷的事情。我会带着另一篇文章回来。</p><p id="4385" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated"><strong class="le iu">你可以在</strong> <a class="ae my" href="https://colab.research.google.com/drive/1OZdFuMRKg85gumJejdmGrgDH1NSz0yAT" rel="noopener ugc nofollow" target="_blank"> <strong class="le iu">这个colab笔记本</strong> </a>里找到代码示例。</p><blockquote class="ky kz la"><p id="d2ef" class="lb lc ld le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我希望我已经介绍了足够的基础知识，可以帮助您开始学习。下次再聊。</p></blockquote></div></div>    
</body>
</html>