# 发展型 BERTology 的奇特案例

> 原文：<https://towardsdatascience.com/the-curious-case-of-developmental-bertology-d601ec52f69d?source=collection_archive---------57----------------------->

## 稀疏性、迁移学习、概括和大脑

这篇文章是为机器学习研究人员和神经科学家写的(将使用这两个领域的一些术语)。虽然这不是一个全面的文献综述，我们将通过一系列主题的经典作品和新成果的选择，试图发展以下论点:

> 就像表征学习和感知/认知神经生理学之间富有成效的相互作用一样，迁移/持续学习、高效深度学习和发展神经生物学之间也存在类似的协同作用。

希望它能以一种或两种方式激励读者，或者至少在全球疫情期间消除一些无聊。

![](img/010e4e8c6eb9b01a8f50f52c75da7a74.png)

由[弗瑞德·卡尼](https://unsplash.com/@fredasem?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

我们将通过大型语言模型的镜头触及以下主题:

*   过度参数化的深度神经网络如何泛化？
*   迁移学习如何帮助概括？
*   我们如何让深度学习在实践中计算高效？
*   在解决这些问题的过程中，深度学习研究*会如何使*和*受益于对发育和老化大脑的*科学研究？

# 哲学序言

在我们开始之前，谨慎的做法是说几句关于*大脑隐喻*的话，以澄清作者在这个问题上的立场，因为它经常在辩论中出现。

深度学习和神经科学的融合可以说早在人工神经网络的概念出现时就已经发生了，因为人工神经元抽象了生物神经元的特征行为。然而，截然不同的学习机制和智能功能种类的差异在这两个屹立了几十年的人之间竖起了一道难以逾越的障碍。近年来现代深度学习的成功重新点燃了另一股融合的潮流，结出了新的果实。除了设计受大脑启发的 AI 系统(例如[ [2](https://www.cell.com/neuron/fulltext/S0896-6273(17)30509-3) ])，深度神经网络最近也被提议作为一个有用的模型系统来理解大脑如何工作(例如[ [3](https://www.nature.com/articles/s41593-019-0520-2) ])。好处是相互的。学习机制的协调正在取得进展，但在不止一个重要方面，智力差距仍然顽固存在。

现在，对于一个深度学习研究人员或从业者来说，看看今天这个复杂的景观，大脑类比*是有益的*还是误导的？基于信仰给出答案当然简单，双方都有大量信徒。但是现在，让我们不要凭信念选择立场。相反，让我们完全通过其实际分支来评估其独特上下文中的每个类比:**，它只有在做出实验上可验证/可证伪的预测时才是有用的，而* ***对于工程*** *来说，它只有在生成可以经受坚实基准测试的候选特征时才是有用的*。因此，对于我们将在本文其余部分提出的所有大脑类比，无论它们看起来多么合适或牵强，我们都将超越任何先前的原则，努力阐明可以在实践中指导未来科学和工程工作的假设，无论是在这些页面的限制之内还是之外。*

# *工作类比*

*当把深度神经网络比作大脑时，我们通常会想到什么？*

*对大多数人来说，网络结构映射到大脑区域的大体解剖结构(如感觉通路)和它们的相互连接，即连接体，单元映射到神经元或细胞组件，连接权重映射到突触强度。因此，神经生理学执行模型推理的计算。*

*深度神经网络的学习通常以在训练数据集上优化目标函数的形式，在给定预定义的网络架构的情况下进行。(一个主要的困难在于人工学习算法的生物学合理性，这是我们在本文中没有触及的主题——在这里，尽管机制不同，我们只是接受功能的相似性。)因此，通过优化的数据驱动学习类似于基于经验的神经发展，即*培育*，而网络架构以及很大程度上的初始化和一些超参数是作为进化的结果而被遗传编程的，即*本性*。*

> ****备注*** *:需要注意的是，现代的深度网络架构，无论是手工隐式设计的，还是通过神经架构搜索(NAS)显式优化的[*[*7*](http://arxiv.org/abs/1905.01392)*]，都是数据驱动优化的结果，产生了*归纳偏差*——免费的午餐由所有未能在自然选择中幸存的不适合者买单。**

*由于数据和计算能力的快速增长，2010 年的十年见证了深度神经网络物种的寒武纪大爆发，在机器学习领域迅速传播。*

# *伯特学*

*随着现代深度学习的进化在过去两年里产生了一群新物种，情况变得更加复杂。他们在自然语言理解的大陆(NLU)，在承载着巨大计算能力的大河肥沃的三角洲上茁壮成长，比如谷歌和微软。这些非凡的生物有一些关键的共性:它们都有一个被称为*变压器* [ [8](http://arxiv.org/abs/1706.03762) ]的典型皮质微电路，拥有快速增加的脑容量，创造了历史记录(例如[ [9](http://arxiv.org/abs/1909.08053) ， [10](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/) ， [11](http://arxiv.org/abs/2005.14165) )，并且经常以其中一个布偶的科学名称命名。但是这些物种对其进化成功至关重要的最突出的共同特征是转移学习的能力。*

*这是什么意思？嗯，这些生物有两个阶段的神经发展:一个漫长的，自我监督的幼虫阶段，称为*预训练*，随后是一个快速的，受监督的成熟阶段，称为*微调*。在自我监督的预训练过程中，大量未标记文本的语料库被呈现给受试者，受试者通过优化某些目标来自娱自乐，这些目标非常类似于解决给人类孩子的语言测验，例如完成句子，填写缺失的单词，说出句子的逻辑顺序，以及找出语法错误。然后，在微调过程中，经过良好预训练的受试者可以通过监督训练快速学会执行特定的语言理解任务。*

*transformers (BERT) [ [12](http://arxiv.org/abs/1810.04805) ]的双向编码器表示的出现标志着迁移学习对 NLU 土地的彻底征服。BERT 和它的变体已经在相当大的范围内推进了最先进的技术。他们非凡的成功激起了人们对这些模型内部运作的极大兴趣，创造了对“机器人学”的研究。与神经生物学家不同，BERTologists 将电极插入模型大脑，以记录解释神经代码的活动(即激活和注意模式)，对大脑区域(即编码层和注意头)进行有针对性的损伤，以了解它们的功能，并研究早期发展的经验(即训练前的目标)如何有助于成熟的行为(即在 NLU 任务中的良好表现)。*

# *网络压缩*

*与此同时，在深度学习的世界中，多阶段发展(如迁移学习)发生在不止一个动物王国。特别是，在生产中，人们经常需要将一个训练好的庞大神经网络压缩成一个紧凑的网络，以便有效地部署。*

*网络压缩的实践源自深度神经网络的一个非常令人困惑的特性:*过度参数化不仅有助于泛化，还有助于优化*。也就是说，训练一个小网络往往不仅不如训练一个大网络(如果一个人当然有能力这样做的话)[ [14](https://www.pnas.org/content/116/32/15849) ]，而且不如将一个训练好的大网络压缩到同样小的规模。在实践中，压缩可以通过稀疏化(剪枝)、蒸馏等来实现。*

> ****备注:*** *值得注意的是，对密集网络进行优化再压缩所产生的最佳稀疏网络的现象(见例如【* [*15*](http://arxiv.org/abs/1710.01878) *，* [*16*](http://arxiv.org/abs/1902.09574) *)很像正在发育的大脑，其中过度产生的连接被逐渐修剪掉【* [*17*](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(17)30200-0)*

*然而，模型压缩中的多阶段开发类型与迁移学习非常不同。迁移学习的两个阶段看到相同的模型针对不同的目标进行优化，而在模型压缩中，原始模型变形为不同的模型，以保持相同目标的最优性。如果说前者类似于获得新技能的成熟，那么后者更像是优雅的衰老，而不会失去已经学到的技能。*

# *学习权重与学习结构:二元性？*

*当一个网络被压缩时，它的*结构*经常会发生变化。这可能意味着*网络架构*(例如在提取的情况下)或者*参数稀疏性*(例如在修剪的情况下)。这些结构上的改变通常是由启发法或正则化方法强加的，这些方法限制了原本已经有效的优化。*

*但是*结构*能超越仅仅是一个效率限制，成为一个有效的学习手段吗？越来越多的新兴研究似乎表明了这一点。*

*一个有趣的例子是权重不可知网络。这些类似水母的生物在一生中不需要学习，但仍然非常适应它们的生态位，因为进化为它们选择了有效的大脑结构。*

*即使是自然选择的固定架构，*学习稀疏结构仍然可以和学习突触权重*一样有效。最近，Ramanujan 等人[ [19](http://arxiv.org/abs/1911.13299) ]设法找到了初始化卷积网络的稀疏化版本，如果足够宽和足够深，其泛化能力不会比经历权重训练的密集网络差。理论研究还表明，如果模型被充分过度参数化，随机权重的稀疏化与优化参数一样有效[ [20](http://arxiv.org/abs/2002.00585) ， [21](http://arxiv.org/abs/2003.01794) ]。*

*因此，在现代深度学习严重过度参数化的制度中，我们有一把双刃剑:权重的优化和结构的优化*。这让人想起作为生物学习和记忆基础机制的*突触*和*结构可塑性*(例如，参见[ [22](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3181802/) 、 [23](https://www.cell.com/current-biology/pdf/S0960-9822(06)02620-0.pdf) )。**

> *****备注*** *:描述参数稀疏性的一种正式方式是通过参数掩码的公式化(图 1)。学习可以通过优化固定结构中的连续权重来实现，也可以通过优化给定一组固定权重的离散结构来实现(图 2)。***

**![](img/d67f1e1fdb6316bcfc25b8f3e5524918.png)**

****图一。模型参数结构稀疏性的参数掩模公式。****

**![](img/44826410ff251831608161ff0da89f36.png)**

****图二。学习权重与学习结构。****

# **通过稀疏化进行微调**

**既然结构就像权重一样可以优化学习，那么这个机制是否可以用来让迁移学习变得更好？**

**是的，确实可以。最近，Radiya-Dixit & Wang[[24](http://arxiv.org/abs/2004.14129)]让 BERT 捡起这个新基因，进化出新的东西。他们表明，可以通过稀疏化预训练的权重来有效地微调 BERT，而不改变它们的值，正如通用语言理解评估(GLUE)任务所系统展示的那样[ [25](https://arxiv.org/abs/1804.07461) ]。**

**![](img/55db132bfaf5131efabad8d3f9ce65e6.png)**

****图 3。通过稀疏化对 BERT 进行微调** [ [24](http://arxiv.org/abs/2004.14129) ]。**

> *****备注*** *:注意，类似的稀疏化微调已经成功应用于计算机视觉，例如【* [*26*](http://arxiv.org/abs/1801.06519) *】。还要注意预训练期间稀疏化 BERT 的现有工作[*[*27*](http://arxiv.org/abs/2002.08307)*]。***

**通过稀疏化进行的微调具有*有利的实际意义*。一方面，预训练的参数值在学习多个任务时保持不变，将特定于任务的参数存储减少到只有一个二进制掩码；另一方面，稀疏化压缩了模型，通过适当的硬件加速，潜在地避免了许多“乘零累加”操作。一石二鸟。**

**然而，除了实际的好处，通过稀疏化进行微调的可能性带来了一些新的机会，有助于更深入地理解语言预训练及其与生物大脑的潜在联系。让我们在接下来的部分中看看它们。**

# **不同彩票的中奖彩票**

**首先我们从最优化的角度研究语言预训练的本质。**

**似乎语言预训练元学习是学习下游 NLU 任务的良好初始化。正如 Hao 等人[ [28](http://arxiv.org/abs/1908.05620) ]最近所表明的，预训练的 BERT 权重具有良好的特定于任务的最优值，在损失情况下更接近且更平坦。这意味着预训练使微调更容易，微调后的解决方案更通用。**

**类似地，预训练也使得发现微调的稀疏子网络更容易[ [24](http://arxiv.org/abs/2004.14129) ]。因此，有趣的是，预先训练的语言模型具有弗兰克尔和卡宾[ [29](http://arxiv.org/abs/1803.03635) ]制定的“中奖彩票”的所有关键属性，但鉴于优化权重与结构的二重性，它们恰恰是*互补*类型(图 3、4):**

*   ****Frankle-Carbin 中奖票**是*一种特定的稀疏结构*，使得*便于权重优化*。对重量初始化 [ [29](http://arxiv.org/abs/1803.03635) 敏感*。这是*跨视觉任务 [30](http://arxiv.org/abs/1906.02773) 的潜在转移*。***
*   *一个**预训练的语言模型**就是*一组特定的权重*使得*便于结构优化*。对结构初始化 [ [24](http://arxiv.org/abs/2004.14129) 敏感*。它是*跨 NLU 任务*转移 [24](http://arxiv.org/abs/2004.14129) 。**

*![](img/e377ca2f7ecb57e6624b1b0b690878c5.png)*

***图 4。弗兰克尔-卡宾赢票** [ [29](http://arxiv.org/abs/1803.03635) ，*比照*通过稀疏化进行微调(图 3)。*

> ****备注*** *:注意预训练 BERT 的“中奖票”属性与[19]中的宽深政权不同。基于 transformer 的大型语言模型，如果做得足够宽和足够深(如果它们已经很大，那么一定会非常大)，是否可以在没有预训练的情况下，从随机初始化中有效地进行微调，这仍然是一个悬而未决的问题。**

*虽然学习中奖彩票的权重和在预训练的权重内搜索子网会导致相同的结果——一个紧凑、稀疏的网络，可以很好地推广，但这两种方法的生物学合理性截然不同:找到一张弗兰克尔-卡宾彩票需要重复的时间倒带和重新训练，如果早期的状态可以被遗传编码，然后在下一代中复制以实现倒带，这一过程只有在多个生物代中才有可能。但是在结构稀疏化之后的重量预训练类似于发育和老化，都发生在一代人的时间内。因此，密集预训练和稀疏微调可能是神经发育的有用模型。*

# *健壮性:不同结构的相同功能*

*伯特和大脑的另一个不可思议的相似之处是其结构的坚固性。*

*在各种稀疏水平下，似乎有大量良好的预训练 BERT 子网络[ [24](http://arxiv.org/abs/2004.14129) ]:一个典型的胶合任务可以通过消除预训练权重的百分之几到一半以上来学习，良好的稀疏解决方案存在于其间的任何地方(图 5，左)。这让人想起了在成熟和衰老的大脑中发挥作用的结构可塑性——其获得的功能保持不变，而底层结构随着时间的推移不断发生变化。这与易碎的*点方案*乘的传统工程截然不同。*

*![](img/ffebd5ad8c31f5602427b1e6725a275e.png)*

***图五。通过稀疏化进行微调的语言模型的结构鲁棒性**。(左)存在许多预训练 BERT 的良好子网络，这些子网络跨越大范围的稀疏性(从百分之几到一半以上)[ [24](http://arxiv.org/abs/2004.14129) ]。(右)持续稀疏化过程中损失景观的漫画视图。密集训练(实心洋红色和橙色箭头)找到位于连续流形上的低损耗解决方案(类似于[ [31](http://arxiv.org/abs/1803.00885) ]的图 1 的黄色虚线框)。只要通过重量消除的任何结构扰动(紫色虚线箭头和圆圈)没有偏离低损失歧管太远，快速的结构微调(洋红色虚线箭头和圆圈)可以连续地恢复最佳性。蓝色网格表示稀疏参数的离散集合。*

*这种现象主要源于深度神经网络的过度参数化。在总体过度参数化的现代体制中，损失景观中的最优值通常是高维连续非凸流形[ [31](http://arxiv.org/abs/1803.00885) ， [32](http://arxiv.org/abs/1906.04724) ]。这与生物学有着奇怪的相似之处，在生物学中，完全相同的网络行为可以从非常不同的底层参数配置中产生，在参数空间中形成非凸集，例如参见[ [33](https://www.pnas.org/content/108/Supplement_3/15542) ]。*

*现在有趣的部分来了。就像生物学中的终身体内平衡调整一样，类似的机制可能支持过度参数化的深度网络中的持续学习(如图 5 所示):密集连接的早期学习发现了一个好的解流形，沿着它存在大量好的稀疏解；随着网络的老化，网络的持续和逐渐稀疏化可以通过结构可塑性快速微调(就像保持终身可塑性的大脑)。*

*从神经生物学的角度来看，如果接受***优化假说*** [ [3](https://www.nature.com/articles/s41593-019-0520-2) ]，那么终生可塑性必然在整个生命周期中不断进行某种功能优化。按照这个逻辑，这个过程出现偏差而导致的神经发育障碍，本质上应该是 ***优化疾病*** ，具有初始化不良、优化器动态不稳定等病因学特征。*

*上述假设是否对深层神经网络普遍适用，是否足以作为神经发育和病理生理学的良好模型，这是未来研究的开放问题。*

# *伯特学到了多少？*

*最后，让我们将一些神经科学的思维应用于 BERTology。*

*我们问这样一个问题:在*预先训练的* BERT 参数中存储了多少与解决 NLU 任务相关的信息？这不是一个容易回答的问题，因为在预训练和微调期间参数值的连续变化会相互混淆。*

*在通过稀疏化进行微调的 BERT 的情况下，这种限制不再存在，其中预训练仅学习权重值，微调仅学习结构。对生物学家来说，如果两个发展阶段涉及完全不同的生理过程，这总是好消息，在这种情况下，其中一个可以用来研究另一个。*

*现在让我们这样做。让我们扰动预先训练的权重值，并研究下游结果。在这个实验中，我们不进行生理扰动(例如破坏注意力头)，而是进行药理学扰动:全身应用一种影响整个大脑中每一个突触的物质。这种药物是量子化的。表 1 总结了一些初步的剂量反应:尽管 BERT 和相关物种已经发展出了巨大的大脑，但似乎在语言预训练期间学到的知识可能只用每个突触的几个比特来描述。*

*在实践中，这意味着，由于预训练的权重在通过稀疏化进行微调的过程中不会改变值，因此可能只需要存储所有 BERT 参数的低精度整数版本，而不会产生任何不利后果——显著的压缩。结果:**你所需要的是所有任务共享的预训练参数的量化整数版本，以及为每个任务微调的二进制掩码**。*

> ****备注*** *:注意现有的关于 BERT 权重的量化的工作是量化微调的权重(例如 Q-BERT[*[*34*](http://arxiv.org/abs/1909.05840)*])而不是预训练的权重。**

*![](img/1fdb7022cdbebf28ae09a718f3807522.png)*

***表 1。MRPC 微调 BERT 和相关模型的 F1 分数。**多亏了[抱紧脸的变形金刚](https://github.com/huggingface/transformers)，像这样的实验轻而易举。*

# *收场白*

*深度神经网络和大脑有明显的区别:在最底层，在学习算法上，在最高层，在一般智力上。然而，中级水平的深刻相似性已被证明有利于深度学习和神经科学的进步。*

*例如，感知和认知*神经生理学*已经启发了*有效的深度网络架构*，这反过来又为理解大脑提供了一个有用的模型。在这篇文章中，我们提出了另一个交叉点:生物*神经发育*可能会激发*高效和稳健的优化程序*，这反过来又会成为大脑成熟和老化的有用模型。*

> ****备注*** *:需要注意的是，传统联结主义语境下的神经发展是在 20 世纪 90 年代提出的(例如参见[*[*35*](https://mitpress.mit.edu/books/rethinking-innateness)*)*。*

*具体来说，我们回顾了最近关于权重学习和结构学习作为优化的补充手段的一些结果，以及它们如何结合起来，在大型语言模型中实现有效的迁移学习。*

*随着结构学习在深度学习中变得越来越重要，我们将看到相应的硬件加速器出现(例如，Nvidia 的 Ampère 架构支持稀疏权重[ [36](https://blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/) ])。这可能会带来专用硬件架构多样化的新浪潮——加速结构学习需要适应特定计算的智能数据移动，这是一个新的探索前沿。*

## *参考*

*[1] W. S .麦卡洛克和 w .皮茨，“[神经活动中固有观念的逻辑演算](https://link.springer.com/article/10.1007/BF02478259)”，1943 年。
[2] D .哈萨比斯等，“[神经科学启发的人工智能](https://www.cell.com/neuron/fulltext/S0896-6273(17)30509-3)”，2017。
[3] B. A. Richards 等人，“[神经科学的深度学习框架](https://www.nature.com/articles/s41593-019-0520-2)”，2019。
[4] T. P. Lillicrap 等人，“[反向传播与大脑](https://www.nature.com/articles/s41583-020-0277-3)”，2020。
【5】g . Marcus，《深度学习:[一个批判性的鉴定](http://arxiv.org/abs/1801.00631)，2018。
[6] G. Marcus，“人工智能的下一个十年:迈向强大人工智能的四个步骤”，2020 年。
[7] M. Wistuba 等著《[关于神经架构搜索的调查](http://arxiv.org/abs/1905.01392)》，2019。
[8] A. Vaswani 等人，“[注意力是你所需要的全部](http://arxiv.org/abs/1706.03762)”，2017。
[9] M. Shoeybi 等，“[威震天-LM:利用模型并行性训练数十亿参数语言模型](http://arxiv.org/abs/1909.08053)”，2019。
【10】micro sift Research，“[图灵-NLG:微软](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)的 170 亿参数语言模型”，2020 年。
【11】t . b . Brown 等.[语言模型是很少出手的学习者](http://arxiv.org/abs/2005.14165)，2020。
【12】j . Devlin 等人，“ [BERT:用于语言理解的深度双向变形器的预训练](http://arxiv.org/abs/1810.04805)”，2018。
【13】A . Rogers 等著《[伯特学入门:我们所知道的伯特如何工作](http://arxiv.org/abs/2002.12327)》，2020 年。
[14] M. Belkin 等人，“[调和现代机器学习实践与古典-方差权衡](https://www.pnas.org/content/116/32/15849)”，2019。
【15】m . Zhu，S. Gupta，“[修剪，还是不修剪:探索模型压缩中修剪的功效](http://arxiv.org/abs/1710.01878)”，2017。
【16】t . Gale 等，“[深度神经网络中的稀疏状态](http://arxiv.org/abs/1902.09574)”，2019。
【17】s . Navlakha 等人，“[网络设计与大脑](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(17)30200-0)”，2018。
【18】a . Gaier 和 D. Ha，“[权重不可知神经网络](http://arxiv.org/abs/1906.04358)”，2019。
【19】v . Ramanujan 等人《[随机加权的神经网络中隐藏着什么？](http://arxiv.org/abs/1911.13299)》，2019。
【20】e .马拉奇等著《[证明彩票假说:修剪是你所需要的全部](http://arxiv.org/abs/2002.00585)》，2020。
【21】m .叶等，“[良好子网络可证明存在:通过贪婪正向选择进行剪枝](http://arxiv.org/abs/2003.01794)”，2020。
【22】f . h . Gage，“成人大脑的[结构可塑性](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3181802/)”，2004。
【23】h . Johansen-Berg，“[结构可塑性:重新连接大脑](https://www.cell.com/current-biology/pdf/S0960-9822(06)02620-0.pdf)”，2007。
【24】e . Radiya-Dixit 和 X. Wang，“[微调能有多细？学习高效的语言模型](http://arxiv.org/abs/2004.14129)”，2020。
【25】A .王等，“ [GLUE:自然语言理解的多任务基准与分析平台](https://arxiv.org/abs/1804.07461)”，2019。
【26】a . Mallya 等人，“[捎带:通过学习任务权重](http://arxiv.org/abs/1801.06519)使单个网络适应多项任务”，2018。
【27】m . a . Gordon 等，“[压缩 BERT:研究权重剪枝对迁移学习的影响](http://arxiv.org/abs/2002.08307)”，2020。
【28】y .郝等《[可视化与理解 BERT](http://arxiv.org/abs/1908.05620) 的有效性》，2020。
【29】j . Frankle 和 M. Carbin，“[彩票假说:寻找小的、可训练的神经网络](http://arxiv.org/abs/1803.03635)”，2018 年。
[30] A. S. Morcos 等人，“[一票全赢:跨数据集和优化器的一般化彩票初始化](http://arxiv.org/abs/1906.02773)”，2019。
[31] F. Draxler 等人，“[神经网络能源格局中本质上没有壁垒](http://arxiv.org/abs/1803.00885)”，2018。
[32] S. Fort 和 S. Jastrzebski，“神经网络损失景观的[大尺度结构](http://arxiv.org/abs/1906.04724)”，2019。
【33】e . Marder，“神经元和电路中的[可变性、补偿和调制](https://www.pnas.org/content/108/Supplement_3/15542)”，2011 年。
[34] S .沈等，“ [Q-BERT:基于 Hessian 的超低精度量化的 BERT](http://arxiv.org/abs/1909.05840) ”，2019。
[35] J. Elman 等.[反思天赋:发展的联结主义观点](https://mitpress.mit.edu/books/rethinking-innateness)，1996 年(ISBN 978–0–262–55030–7)。
【36】英伟达博客，“[什么是 AI 推理中的稀疏性？](https://blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/)”，2020。*