<html>
<head>
<title>Visualising LSTM Activations in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Keras中可视化LSTM激活</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualising-lstm-activations-in-keras-b50206da96ff?source=collection_archive---------7-----------------------#2020-01-26">https://towardsdatascience.com/visualising-lstm-activations-in-keras-b50206da96ff?source=collection_archive---------7-----------------------#2020-01-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a5eb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">看看每个LSTM细胞学到了什么</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8957e6addf063c2b4dba32208e2efc9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-bkxd-sw58WAY2VA0zR1sw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">弗兰基·查马基在<a class="ae kv" href="https://unsplash.com/s/photos/artificial-intelligence?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="23a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你有没有想过LSTM层学到了什么？有没有想过是否有可能看到每个细胞对最终输出的贡献。我很想尝试想象一下。在满足我好奇的神经元时，我偶然发现了安德烈·卡帕西的博客，名为<a class="ae kv" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">循环神经网络的不合理有效性</a>。如果你想得到更深入的解释，我建议你去看看他的博客。</p><p id="f735" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们不仅要在<a class="ae kv" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>中构建一个<strong class="ky ir">文本生成</strong>模型，还要可视化一些单元格在生成文本时所看到的内容。与CNN的情况一样，它学习图像的一般特征，如水平和垂直边缘、线条、补丁等。类似地，在文本生成中，LSTMs学习诸如空格、大写字母、标点符号等特征。我们将会看到LSTM层中的每个细胞正在学习什么特征。</p><p id="a8e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用刘易斯·卡罗尔的《爱丽丝漫游奇境记》这本书作为训练数据。该模型架构将是一个简单的架构，由两块<a class="ae kv" href="https://keras.io/layers/recurrent/#lstm" rel="noopener ugc nofollow" target="_blank"> LSTM </a>和<a class="ae kv" href="https://keras.io/layers/core/#dropout" rel="noopener ugc nofollow" target="_blank">脱落</a>层以及最后的<a class="ae kv" href="https://keras.io/layers/core/#dense" rel="noopener ugc nofollow" target="_blank">密集</a>层组成。</p><blockquote class="lt lu lv"><p id="8d93" class="kw kx ls ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">您可以在此下载训练数据和训练模型权重<a class="ae kv" href="https://github.com/Praneet9/Visualising-LSTM-Activations" rel="noopener ugc nofollow" target="_blank"/></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lz"><img src="../Images/4e699ea4e6bd3b53de3ff893657dae82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tKEDjqtbYPDT5Sb2WHUvlg.png"/></div></div></figure><p id="87af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们激活单细胞的样子。我希望你能辨认出上图中的图案。如果不能，你会在文末找到。</p><p id="288b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们深入研究代码。</p><h2 id="cf89" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">步骤1:导入所需的库</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><blockquote class="lt lu lv"><p id="c3d2" class="kw kx ls ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">注意:我用CuDNNLSTM代替了LSTM，因为它训练速度快15倍。CuDNNLSTM由CuDNN支持，只能在GPU上运行。</p></blockquote><h2 id="c499" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">第二步:读取训练数据并进行预处理</h2><p id="4305" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">我们将使用正则表达式删除一个以上的空格。<code class="fe na nb nc nd b">char_to_int</code>和<code class="fe na nb nc nd b">int_to_char</code>只是数字到字符和字符到数字的映射。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><h2 id="214a" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">步骤3:为培训准备数据</h2><p id="31a5" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">重要的是准备好我们的数据，使每个输入都是一个字符序列，输出是后面的字符。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><h2 id="4c4c" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">步骤4:构建模型架构</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/c45061a442397a60a8bcb5b1b4016d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*3w0JYyNRTRbYUEzNPPqflA.png"/></div></figure><h2 id="fdf9" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">第五步:训练模型</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="1878" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我无法一次训练我的模型300个纪元，因为我使用了<a class="ae kv" href="https://colab.research.google.com/notebooks/intro.ipynb#recent=true" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>来训练我的模型。我必须在3天内训练它，每天100个周期，方法是保存重量，然后从我结束训练的同一点重新加载。</p><p id="299b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你有一个强大的GPU，你可以一口气训练300个纪元的模型。如果你没有，我会建议你使用Colab，因为它是免费的。</p><p id="17c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以使用下面的代码加载模型，并从最后一点开始训练。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="ef66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在是文章最重要的部分——可视化LSTM激活。我们需要一些函数来让这些可视化变得可以理解。让我们开始吧。</p><h2 id="aa84" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">步骤6:获取中间层输出的后端函数</h2><p id="14dc" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">正如我们在上面的<strong class="ky ir">步骤4 </strong>中看到的，第一层和第三层是LSTM层。我们的目标是可视化第二LSTM层的输出，即整个架构中的第三层。</p><p id="8360" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Keras Backend 帮助我们创建一个函数，它接收输入，并从中间层向我们提供输出。我们可以用它来创建我们自己的管道函数。这里<code class="fe na nb nc nd b">attn_func</code>将返回一个大小为512的隐藏状态向量。这些将是512个单位的LSTM层的激活。我们可以想象这些细胞的每一次激活，以理解它们试图解释什么。要做到这一点，我们必须将它转换成一个可以表示其重要性的范围。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><h2 id="567a" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">步骤7:助手功能</h2><p id="543b" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">这些辅助函数将帮助我们可视化字符序列及其每个激活值。我们通过<code class="fe na nb nc nd b">sigmoid</code>函数传递激活，因为我们需要一个可以表示它们对整个输出的重要性的值。<code class="fe na nb nc nd b">get_clr</code>功能有助于为给定值获得合适的颜色。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="268f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图显示了每个值是如何用各自的颜色表示的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/015946905f704edc3786bf0dbd2c63ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*EEzHWObtbBIbFkfd4J8EQA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从0到1的激活颜色级别</p></figure><h2 id="5b37" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">第八步:获得预测</h2><p id="0ed4" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated"><code class="fe na nb nc nd b">get_predictions</code>函数随机选择一个输入种子序列，并获得该种子序列的预测序列。<code class="fe na nb nc nd b">visualize</code>函数将预测序列、序列中每个字符的sigmoid值以及要可视化的单元格编号作为输入。根据输出的值，用适当的背景颜色打印字符。</p><p id="21f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对层输出应用sigmoid后，值在0到1的范围内。数字越接近1，重要性越高。如果该数字更接近0，则意味着对最终预测没有任何重大影响。这些单元格的重要性由颜色表示，其中<strong class="ky ir">蓝色</strong>表示较低的重要性，<strong class="ky ir">红色</strong>表示较高的重要性。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><h2 id="ce67" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">步骤9:可视化激活</h2><p id="bb4f" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">超过90%的细胞没有显示任何可理解的模式。我手动可视化了所有的512个细胞，注意到其中的三个(189，435，463)显示了一些可以理解的模式。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="8903" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如您在下面看到的，189号单元格是为引号内的文本激活的。这表明预测时单元格在寻找什么。如下所示，该单元格对引号之间的文本贡献很大。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/c4a5d0ac83e68c3e945238b380b64e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HofxWQXYvkj5pFLqqVzLgQ.png"/></div></div></figure><p id="a42e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">435号单元格是为引号中的句子后的几个单词激活的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/52cd8c8c1856fe6137103ffaf7ee6d80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*itsyU8CRRKRmHXJzRqQGOQ.png"/></div></div></figure><p id="a630" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个单词的第一个字符激活单元号463。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/69b49636ecaf595e5a1480045e427385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jegbdzv4jQ9_Xurhf-Rf-A.png"/></div></div></figure><p id="cb61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过更多的训练或更多的数据，可以进一步改善结果。这恰恰证明了一点，深度学习毕竟不是一个完全的黑箱。</p><p id="a3ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在我的<a class="ae kv" href="https://github.com/Praneet9/Visualising-LSTM-Activations" rel="noopener ugc nofollow" target="_blank"> Github简介</a>上查看全部代码。</p><p id="20b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我第一次尝试写博客。我希望你能从中学到一些东西。</p></div></div>    
</body>
</html>