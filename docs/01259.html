<html>
<head>
<title>Neural Networks: Everything you Wanted to Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:你想知道的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-everything-you-wanted-to-know-327b78b730ab?source=collection_archive---------23-----------------------#2020-02-04">https://towardsdatascience.com/neural-networks-everything-you-wanted-to-know-327b78b730ab?source=collection_archive---------23-----------------------#2020-02-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="024a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习深度学习模型核心背后的数学！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8f14a6fc00f390933f872e171f962483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AxOpo27lzG_0cFFL"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/photos/so1L3jsdD3Y" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h1 id="1055" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="8fa5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这篇文章的目标是:</p><ul class=""><li id="0359" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">简要介绍什么是神经网络以及它们是如何工作的</li><li id="946d" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">对他们如何学习做出详细的数学解释</li></ul><h1 id="28de" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">神经网络</h1><p id="d563" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一般来说，神经网络是一种逼近函数的方法。遵循通用近似定理:</p><blockquote class="nd ne nf"><p id="916b" class="lr ls ng lt b lu mp ju lw lx mq jx lz nh ni mc md nj nk mg mh nl nm mk ml mm im bi translated">具有单个隐藏层的神经网络足以表示给定值范围内的任何函数，尽管隐藏层可能太大以至于不可能实现或者不可能找到适当的权重。</p></blockquote><p id="621e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">如果我们考虑到正常情况下你能想到的任何问题都可以以数学函数的形式分解和建模，根据这个定理，神经网络应该可以解决任何问题，对吗？</p><p id="a869" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">神经网络有两个主要限制:</p><ul class=""><li id="a17e" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">它们只能逼近连续函数</li><li id="b3ab" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">并且在一定的数值范围内</li></ul><p id="21c1" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">粗略地说，有了神经网络，我们可以检测模式，并用它们来解决问题。此外，通过深度神经网络，我们可以转换数据空间，直到我们找到一种有助于实现所需任务的表示。</p><p id="c69c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">他们将执行类似于下图所示的操作:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/989377fe51a9b8cdd1c805b5a532ac8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*eRcLHR5320kfy1Qr9NKovw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="cbd0" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">基本上，它们将输入数据转换为其他空间表示，以便能够以更简单的方式区分它。</p><p id="1479" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">下面的例子很有代表性:</p><p id="18ef" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">如果我们试图用线性分类器来解决以下数据集的分类问题，这将非常困难，因为数据点不是线性可分的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/de1b831d6f6f2701374376d08c8eb97c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lBseWr0ZwcVhg1raCdjq-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="e4f9" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">我们将获得类似于以下内容的内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/08384875ca0a89bee947014b3e1de9ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vrQF4LDoCleYf1M2act7Rw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="de1f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">但正如我们所看到的，它并不太好。你想看看神经网络的能力吗？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/4858c67f6a50792a6e8921145f1731f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DHz4-mbyoL1ZvIOE4YxmMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分列的数字</p></figure><p id="6790" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">这看起来好多了。)</p><h2 id="d961" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">神经网络不能做的事情</h2><p id="9bbd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">神经网络不能给出问题的精确解。例如，神经网络很难实现简单的乘法运算。</p><ul class=""><li id="0b3d" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">首先，因为我们需要它的精确值。</li><li id="3802" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">第二，因为我们之前说过，它们能够在给定的范围内逼近函数。乘法需要[-inf，+inf]的范围。</li></ul><p id="6cbc" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">此外，他们也不能“思考”。他们只是非常强大的模式检测器，给人以智能的感觉，但他们没有。我们必须自己提供情报。</p><p id="0701" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">我们还必须考虑到，虽然它们非常有用，因为它们解决了迄今为止对计算机来说非常复杂的问题，例如检测不同类型的狗品种或交通标志，但很难从中提取知识。</p><p id="e2b2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">换句话说，他们有能力做我们要求他们做的事情，但是很难发现他们到底是怎么做的。</p><h1 id="fbba" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">神经网络是如何学习的？</h1><p id="6cf2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">基本上，神经网络的学习过程类似于人类的学习。他们通过重复学习。</p><p id="a462" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">他们看到一些东西，猜测是什么，其他代理告诉他们是否正确。如果他们错了，他们会调整自己的知识，避免重蹈覆辙。</p><h2 id="43ef" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">让我们看一个具体的例子</h2><p id="b711" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">你可能知道，神经网络是由层组成的，而层又是由神经元组成的。</p><p id="5a32" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">在这个链接中可以看到完整的例子:<a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p><p id="fe78" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">在下图中，我们可以看到一个前馈型网络，其 iput 层由 2 个神经元组成，隐藏层由 2 个神经元组成，输出层由另外 2 个神经元组成。另外，隐藏层和输出层也有偏差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c8dd238071ec65bf96a26cc0b86db97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*9I-SdmJwH0h1idzxWo1cgw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="6d34" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">还有很多架构，但这是最常用的一种。在这种类型的架构中，I 层的每个神经元都与 i+1 层的所有神经元相连接。</p><p id="51b8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">也就是说，一层的神经元只允许连接到下一层的神经元。这些类型的层被称为密集层或全连接层。</p><p id="9222" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">为了简单起见，让我们假设我们的训练集只由 1 个元素组成:0.05，0.1，这是 1 类(如果神经元 o1 的概率大于神经元 o2 的概率，则可能是 0 类，或者如果相反，则可能是 1 类)。</p><p id="5ffa" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">在下图中，您可以看到具有随机初始化的权重、输入的训练集元素和所需输出的网络:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/0cae13805ca5429df05f53a9fd4b98d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*NkESTN-No0BdBpG5K3EneQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="0c58" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">一旦我们的神经网络被定义并准备好接受训练，我们就来关注它们是如何学习的。</p><p id="9790" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">两种最常见的学习方法是:</p><ul class=""><li id="49d6" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">前进传球</li><li id="4382" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">反向传播</li></ul><h2 id="4d17" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">前进传球</h2><p id="2eb7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">它包括用当前权重值计算网络的输出。为此，我们向网络输入我们的训练元素。</p><p id="61ce" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">但是，首先，让我们看看神经元到底是怎样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/bdbb52e6e458e570c562127fdca02ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*DP6WFuBw192AFQ5v6yn0ZQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" rel="noopener" target="_blank" href="/complete-guide-of-activation-functions-34076e95d044">https://towards data science . com/complete-guide-of-activation-functions-34076 e95d 044</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f301b0e7c578a85c064033259436a602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*SwOIOM5kSRhvP0qJqkEuIw.png"/></div></figure><p id="3ccf" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">其中<strong class="lt iu"> fn_activation </strong>是选择的 activation 函数。一些最流行的激活功能有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/9fd39bb1ea84d5bbb7c4d8efe29943d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nKkk6pMYSmmnm8QNukvD9w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" rel="noopener" target="_blank" href="/complete-guide-of-activation-functions-34076e95d044">https://towards data science . com/complete-guide-of-activation-functions-34076 e95d 044</a></p></figure><h2 id="3fb6" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">人工神经网络和真正的神经网络相似吗？</h2><p id="f4f9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">人工神经网络是受人脑的启发。事实上，我们有大约 100 亿个神经元，每个神经元都与其他 10，000 个神经元相互连接。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/524d7dc7d6760bfcbcd0e0ef7ef2206b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*wTg9pkmdpSxJh4544sRWfQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://training.seer.cancer.gov/anatomy/nervous/tissue.html" rel="noopener ugc nofollow" target="_blank">https://training . seer . cancer . gov/anatomy/neural/tissue . html</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/202bca75e45814b2b23a852a1f59b4ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yhhSCc7agfu3WT5SvghtkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="bd38" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">神经元的身体称为<strong class="lt iu">胞体</strong>，入口称为<strong class="lt iu">树突</strong>，出口称为<strong class="lt iu">轴突</strong>。大脑的神奇之处在于一些树突与其他神经元轴突的成千上万个连接。</p><p id="1daa" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">其工作方式是，每个神经元通过其轴突(输出)和受体的树突(输入)接收来自其他神经元的电化学脉冲。</p><p id="fc58" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">如果这些脉冲强到足以激活神经元，那么这个神经元就会将脉冲传递给它的连接。通过这样做，每个连接的神经元再次检查从树突到达细胞体的冲动(在我们之前的例子中为<strong class="lt iu"> inh1 </strong>)是否足够强，以激活神经元(<strong class="lt iu"> fn_activation </strong>负责检查这一点)并扩展更多的神经元。</p><p id="b107" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">考虑到这种操作模式，我们意识到神经元真的就像一个开关:要么传递信息，要么不传递。</p><blockquote class="nd ne nf"><p id="d356" class="lr ls ng lt b lu mp ju lw lx mq jx lz nh ni mc md nj nk mg mh nl nm mk ml mm im bi translated">神经网络不是基于它们的生物伙伴，而是受到它们的启发。</p></blockquote><h2 id="0084" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">正向和反向传播</h2><p id="2235" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，我们将了解网络如何计算输出<strong class="lt iu"> outh1。</strong></p><p id="43c8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">再看一下网络:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/039f520a0720271f95196996a84e6735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*3NSeEktNlBl7nMAhRxbs5w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/e05f2d1518c8b6bc8af6b5e80f0bd8fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*d4fUWucO2D_ZeVNcRYgJAw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="cdb4" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">图像</p><p id="eaed" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">为了计算 o1 和 o2，我们需要:</p><p id="7552" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">对于第一层，即隐藏层:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/6aaa666d18466e8703fd31a98b61f794.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*zi-ZrgERWmsXDkorpCC_5A.png"/></div></figure><p id="9862" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">对于最后一层，输出层:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/6e108951bd88fb9c22fe07a382d8565b.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/1*ZwlcNirSaGLeUh023qjAbA.png"/></div></figure><p id="7995" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">其中:<strong class="lt iu"> o1 = outo1 </strong>和<strong class="lt iu"> o2 = outo2。</strong></p><p id="04d5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">所以我们需要计算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/6ad2577cd7974769307e1bfec202cba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*yp-MUFhZDLryIBGwieX4cg.png"/></div></figure><p id="ac70" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">现在，要获得 outh1 和 outh2，我们需要应用 fn_activation。在这种情况下，我们选择物流功能作为激活功能:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/519f9fff622e48e2a8de3be2903c1114.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*1a_YyM59xxzF4tmh9AaU1g.png"/></div></figure><p id="5725" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">所以:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/29a714d155b199b9c6dfe8d0b23022a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*cBLBOviffoLvzkCBRx7Pww.png"/></div></figure><p id="2436" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">让我们看看我们在输出计算过程中的情况:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/55ef77def292edd68497815ae228c90b.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*a5f-_MAvo1y3FeiODVNm6w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="4dd3" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">我们现在只需要计算 o1 和 o2:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/8588233d7ae59632a9dc693946583a2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*g9bBefEYpdNeGZiL-dc9Zw.png"/></div></figure><p id="ac96" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">因此，应用激活函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/498550991a7813bd350f4be70415aec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*xh2VyLtgCuO534iLtGxvtw.png"/></div></figure><p id="62e9" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">终于！我们已经计算出我们的神经网络预测！但是坚持住，我们的预测太可怕了！他们不像 0.01 和 0.99 期望！我们如何解决这个问题？</p><p id="696b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">不如我们计算一下总误差，尽量把它减到最小？事实上，这正是反向传播算法所做的:它根据每个权重对总误差的影响程度来更新权重，以便最小化总误差</p><p id="cbf4" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">让我们计算总误差:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/a7ba64334cbf67c7f7e468b98ce9eaf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*rJX8M4hmFZOLByl2BVwOgQ.png"/></div></figure><p id="78a3" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">请注意，outo2 误差比 outo1 误差小得多。这是因为 0.7729 比 0.01 的 0.7514 更接近 0.99，所以参与 outo1 计算的神经元的变化应该比 outo2 更大。</p><p id="c875" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">我们如何根据影响总误差的因素来更新权重？很简单，只需计算给定重量的变化对总误差的影响，并根据这种关系进行更新。</p><p id="0585" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">例如，你能想出一种方法来计算权重 w5 对总误差的影响有多大吗？换句话说，w5 权重的变化对总误差的影响有多大？这听起来不耳熟吗？</p><p id="01b5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">我们说的是衍生品！看，我们可以把每个神经元理解为一个函数，应用链式法则，从总误差得到 w5 权重。但首先，你还记得链式法则是如何运作的吗？</p><p id="268c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">让我们看一个例子:假设我们想对函数求导:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/55a884a3d048e61bd64e63dd51bd650c.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*h962I8uHP-5gvK2qn0G2JA.png"/></div></figure><p id="0151" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">换句话说，我们想知道<strong class="lt iu"> x </strong>变量的变化对<strong class="lt iu"> y </strong>变量的影响有多大。</p><p id="583e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">我们可以把这个函数理解为其他两个函数的组合，其中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/1446597232d8984a405df71c1980dad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*VEusAAu5Bs4rMUFIeXO3YA.png"/></div></figure><p id="139e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">现在，我们需要分别对<strong class="lt iu"> x </strong>求导<strong class="lt iu"> y </strong>。为此，我们需要首先相对于<strong class="lt iu"> u </strong>对<strong class="lt iu"> y </strong>求导，然后相对于<strong class="lt iu"> x </strong>对<strong class="lt iu"> u </strong>求导。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/52662311d8ad803cecd304309e8db6a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*tMo6TolyrHUIhlakdvGtAg.png"/></div></figure><p id="49f4" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">在我们的例子中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/271245facb9e12ec98c3890456732bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*Hijyr9NWRKoPsKc4p_ctJw.png"/></div></div></figure><p id="d7a6" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">这就是应用的链式法则。现在让我们把它看成一个图表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/97c8018c21a3b6019a4fdfb0effd7c3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bqpsnNEjiANKXF58EaKNTQ.png"/></div></div></figure><p id="5adb" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">想象每个圆都是函数，每个箭头都是乘法。然后，利用链式法则，我们可以写出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/dd8f44a52ef7ec8d8a86562127118abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*ZYtPn9c2q2hoZrZziJH4yw.png"/></div></figure><p id="d8ab" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">现在，让我们看看如何写出计算 Etotal 相对于权重 w5 如何变化的公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/06ede08f98cad59dbe17fb0a9b269480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*pOliiuI0jhVGudb_K4sr8w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/f05aaaf1605650d163183ca3f27ca507.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*fwWO3JxlywTNF6LVoC5oLg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/92baf5906ddc84b03c527eeb06f29e5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-QVTJ_oMMMdNH7DglqTFag.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="989b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">正如开膛手杰克所说:让我们去寻找零件。总误差定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/68c030f92313c9815d6d764ed6920389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*dq8JIbTKD9ym3o1sy8v-fA.png"/></div></figure><p id="6671" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">正如我们所见，w5 只影响 o1 神经元，所以我们只关心使我们的<strong class="lt iu"> Etotal = Eo1 的<strong class="lt iu"> Eo1 </strong>。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/73c4bcfa243ff1f7b62a3fdd87e1f68a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xx01LSaOJuJBqiRA9wrHjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="30c1" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">因此，我们可以将 w5 权重与 resepct to <strong class="lt iu"> Etotal </strong>的方差定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/b7098805c81a4d3aaad39d4732ae239f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*CUHkhPhyfQGjVe6tPZ3cFg.png"/></div></figure><p id="4a4b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">太好了！让我们看下一个术语:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/096f15b08723e4de167fa2bc2934f53d.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*WhtVusznC5PrR7CYn8FcvA.png"/></div></figure><p id="7838" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">这可以表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/a7c17640767a453ab0dd01f0b9edf4e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*3IrE21Hq7iu-Dm9rJjCv-A.png"/></div></figure><p id="0d3e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">及其衍生物:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/31a64ff8c752092f7cee00c569daf52e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2faXCClhnIpfxg8jMPXT-w.png"/></div></div></figure><p id="aca1" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">最后两步是可能的，因为逻辑函数的导数是一个偶函数，即<strong class="lt iu"> f(x)=f(-x) </strong>。</p><p id="4d51" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">好吧，让我们回到正题。我们已经有了导数，所以现在我们要计算它的值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/562fc89c760ca435b0362c0d42d380b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*fGptG81mqc4pD-QwI6XRLw.png"/></div></figure><p id="1d79" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">我们已经有了第一项和第二项，我们只需要计算突出显示的项就可以得到<strong class="lt iu"> w5 </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/ad6d08bd150c399438c766bbebc6f431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*sicquT4lOYmtZQi9-UKvIA.png"/></div></figure><p id="9e0d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">如果我们回忆一下，01 年的<strong class="lt iu">是:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/654a07fc013aab1c76e817f03b203e5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*nLyK3cCmMPD1nps0pibF1g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/b3f27aea8a03ac7f62948238f0a028bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*jJPcOUI40WUNgwe9C75GKQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="9e57" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">如果我们看一下<strong class="lt iu"> o1 </strong>神经元的输入，我们可以直接计算 in01 公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/6794a4a68e34b95dec170427c2daa98f.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*GR8ZUfxJlFwJiSLQetz8Ww.png"/></div></figure><p id="eb53" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">现在，我们能够计算我们的期限；</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/e0405736e16fb372270fc2692b1a84f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*NTZCveDD360sfCsWdBoxJA.png"/></div></figure><p id="dee9" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">终于！我们已经有了所有必要的术语来了解 w5 对 Etotal 的影响程度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/924005128b40c369941297e7c838d3f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*Fw61HU8cPHi6WYn6mnl6MQ.png"/></div></figure><p id="0a8d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">我们已经知道如何计算第二层和 terera(隐藏和退出)影响之间的权重。但是…我们如何发现第一层和第二层(入口和出口)之间的权重有什么影响呢？</p><p id="620f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">很简单，和我们之前做的一样！</p><p id="e078" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">我们首先需要定义的是什么？</p><p id="db08" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated"><strong class="lt iu"> Etotal </strong>相对于重量<strong class="lt iu"> w1 </strong>的导数。这是怎么定义的？</p><p id="34dc" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">这里是网络:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/0fff6ae62db42460886acc13ee75aa22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*Ablp00_8al9H-P8qGF_LZA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="3e90" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">让我们将神经网络解构为神经元:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/4d8019c3982e7740b92c90f8bae76f33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3S1eaNzPSTeH4-yVHZ98wg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="a7d5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">现在，我们有两条路可走:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/1164bfcab129d3ca66d392964f57808a.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*vvRX19NAJMYsaoxkuAVuBA.png"/></div></figure><p id="9950" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">因此，我们需要计算对应于 1 和 2 神经元的误差:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/8a5924dc80ebceed918cd3f285bbe8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*mMVoUHFZj6ZPtiMkApKpqg.png"/></div></figure><p id="6a7f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">这意味着我们现在有两个影响权重 w1 的误差源，即 Eo1 和 Eo2。各自是如何定义的？很简单，如果我们在网络图中从 Eo1 走到权重 w1，我们会发现的第一个元素是什么？outo1，对吧？</p><p id="9a19" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">有了它，我们已经不得不:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/26e1b9087e30e97182442db0a5e9a76d.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*HUfaeKLqCrVqAGcGJqjdqQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/4bed4c117318b5cd524e5f6a299296ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*ECauPxx6tjpGlDK1n0ck4w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="360d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">让我们求解到<strong class="lt iu"> Eo1: </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/0a46e5ff6bada81356cada4ee6ccb8ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*Bx9TU1pTkPKs500RkeacNQ.png"/></div></figure><p id="2435" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">现在我们有了<strong class="lt iu"> Eo1 </strong>相对于<strong class="lt iu"> w1 </strong>的变化量。但是我们缺少一些东西，如果我们再看看神经网络:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/17fcc3f06cde3b1563e80fa7b00d8f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nnx-U_SpPh4kRE9lopob_g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="4818" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">我们还需要对应于第二个神经元的误差，<strong class="lt iu"> Eo2 </strong>。有了这个误差，我们就可以计算出重量 w1 对总量<strong class="lt iu">的影响有多大，这就是我们感兴趣的:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/cbce2cfa52f72585d634442467b9d00a.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*iQGff3oajtm8LC-BqurlrQ.png"/></div></figure><p id="aebd" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">对每个权重重复这一过程，我们就可以对权重应用梯度下降，从而更新权重，参见:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pz"><img src="../Images/3f07a77fde11370f8fc539e09e6973e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o7mk7bJ8dfLZFWH-54p6tw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">https://mattmazur . com/2015/03/17/a-逐步-反向传播-示例/ </a></p></figure><p id="c2e5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated">其中<strong class="lt iu"> η </strong>是学习率，表示我们在梯度的反方向要走多大的一步。</p><h1 id="6876" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最后的话</h1><p id="838e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一如既往，我希望你喜欢这篇文章，你已经了解了神经网络以及它们如何利用数学来学习自己！</p><p id="43db" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated"><em class="ng">如果你喜欢这篇文章，那么你可以看看我关于数据科学和机器学习的其他文章</em> <a class="ae ky" href="https://medium.com/@rromanss23" rel="noopener"> <em class="ng">这里</em> </a> <em class="ng">。</em></p><p id="c0cb" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma ni mc md me nk mg mh mi nm mk ml mm im bi translated"><em class="ng">如果你想了解更多关于机器学习、数据科学和人工智能的知识</em> <strong class="lt iu"> <em class="ng">请在 Medium </em> </strong> <em class="ng">上关注我，敬请关注我的下一篇帖子！</em></p></div></div>    
</body>
</html>