<html>
<head>
<title>How To Evaluate Unsupervised Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何评价无监督学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-evaluate-unsupervised-learning-models-3aa85bd98aa2?source=collection_archive---------7-----------------------#2020-05-15">https://towardsdatascience.com/how-to-evaluate-unsupervised-learning-models-3aa85bd98aa2?source=collection_archive---------7-----------------------#2020-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="38aa" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/music-by-numbers" rel="noopener" target="_blank">数字音乐</a></h2><div class=""/><div class=""><h2 id="c7ed" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">韩国流行音乐和古典音乐到底有多大的不同？</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e32f991688992687947731c25f992747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*7t-dW-uL0us-mz-6cY59-A.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">一个音乐集群。视频:<a class="ae lh" href="https://www.pexels.com/video/musical-instruments-856945/" rel="noopener ugc nofollow" target="_blank">像素</a></p></figure><p id="3a77" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di">所以</span>到目前为止，在<a class="ae lh" href="https://towardsdatascience.com/tagged/music-by-numbers" rel="noopener" target="_blank">这个博客系列</a>中，我们已经研究了如何通过聚集一组曲目来创建自动的歌曲播放列表，完全基于它们的<a class="ae lh" href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/" rel="noopener ugc nofollow" target="_blank">音频特征</a>。之前，我们制作了一个包含32首歌曲的<a class="ae lh" rel="noopener" target="_blank" href="/generating-spotify-playlists-with-unsupervised-learning-abac60182022">玩具示例</a>，并展示了<a class="ae lh" href="https://en.wikipedia.org/wiki/Hierarchical_clustering#Agglomerative_clustering_example" rel="noopener ugc nofollow" target="_blank">分层凝聚聚类</a> (HAC)如何自动创建相似歌曲的子组。我们能够通过我们现有的歌曲知识来验证这一聚类练习的结果(我们的算法确认<a class="ae lh" href="https://www.youtube.com/watch?v=pWB5JZRGl0U" rel="noopener ugc nofollow" target="_blank">motrhead</a>和<a class="ae lh" href="https://www.youtube.com/watch?v=0qanF-91aJo" rel="noopener ugc nofollow" target="_blank">黑色安息日</a>在音乐上是相似的——请看图)。</p><p id="d00e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是如果我们没有这些先验知识呢？如果数据甚至没有被标记(许多现实生活中的聚类情况就是如此)，那会怎么样呢？即使是，如果这些标签最初对我们来说毫无意义呢？有很多我从未听说过的艺术家，如果我们试图将数千首曲目分组，那么手动验证每一组显然是不切实际的。在这些情况下，我们需要某种数学方法来衡量我们的聚类有多“成功”。</p><p id="b042" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了探索我们可能如何去做这件事，我们再次求助于Spotify的API。假设我们从四个非常不同的播放列表中选取歌曲:</p><ul class=""><li id="0dbf" class="mn mo it lk b ll lm lo lp lr mp lv mq lz mr md ms mt mu mv bi translated"><a class="ae lh" href="https://open.spotify.com/playlist/37i9dQZF1DX6PKX5dyBKeq?si=troGGkf6SLSP5JhYBP3dZQ" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">说唱英国</strong> </a></li><li id="5ebe" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated"><a class="ae lh" href="https://open.spotify.com/playlist/37i9dQZF1DXdwTUxmGKrdN?si=hR5UZpPmSdy2l4-JKgNDcw" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">流畅爵士</strong> </a></li><li id="71a2" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated"><a class="ae lh" href="https://open.spotify.com/playlist/37i9dQZF1DWWEJlAGA9gs0?si=LCS9EwdhTCi_Lx4Cyu_QfQ" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">古典要领</strong> </a></li><li id="856a" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated"><a class="ae lh" href="https://open.spotify.com/playlist/37i9dQZF1DX14fiWYoe7Oh?si=lczQeag8T2SP0NOVpVUImw" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">精华K-Pop </strong> </a></li></ul><p id="2c58" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们将这些合并成一个数据集，一个无监督的机器学习算法应该能够将他们的歌曲分成四个集群，这四个集群有点类似于最初的四个播放列表。</p><p id="fb1e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们开始之前，我们可能确实想要检查我们的假设，即来自这些不同播放列表的歌曲确实是“不同的”。当然，在一张图表上同时显示两个以上的特征是具有挑战性的。然而，如果我们绘制一个所有特征的散点图，按播放列表进行颜色编码，我们可以看到有大量的度量组合来展示每个流派的独特特征。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/492103fb088ec3e9d5f095d8151b027a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Vnw-Zn6HhqN2SMJFM_A0A.png"/></div></div></figure><p id="bb25" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们在<a class="ae lh" rel="noopener" target="_blank" href="/generating-spotify-playlists-with-unsupervised-learning-abac60182022">之前的博客</a>中看到，在Scikit-Learn中运行聚类算法非常简单:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="99b6" class="nh ni it nd b gy nj nk l nl nm"><strong class="nd jd">#We scale the data to ensure that<br/>#feature units don't impact distances<br/>from</strong> sklearn.preprocessing <strong class="nd jd">import</strong> StandardScaler<br/>sclaer = StandardScaler()<br/>X_scaled = scaler.fit_transform(X)</span><span id="4260" class="nh ni it nd b gy nn nk l nl nm"><strong class="nd jd"><em class="no">#This scaled data can then be fed into the HAC algorithm<br/></em>from</strong> sklearn.cluster <strong class="nd jd">import</strong> AgglomerativeClustering<br/><strong class="nd jd"><em class="no">#We can tell it how many clusters we're aiming for</em></strong><br/>agg_clust = AgglomerativeClustering(n_clusters=4)<br/>assigned_clusters = agg_clust.fit_predict(X_scaled)</span></pre><p id="b98d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们还看到HAC有三种不同的“链接标准”——算法通过这种方法将集群连接在一起:</p><ul class=""><li id="f25a" class="mn mo it lk b ll lm lo lp lr mp lv mq lz mr md ms mt mu mv bi translated"><strong class="lk jd"> ward </strong>(默认):选择两个聚类进行合并，使所有聚类内的方差增加最小。一般来说，这导致集群的大小相当一致。</li><li id="7be3" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated"><strong class="lk jd">完全</strong>(或最大链接):合并两个点之间具有最小<strong class="lk jd"> <em class="no">最大</em> </strong>距离的簇。</li><li id="4c10" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated"><strong class="lk jd">平均</strong>:合并所有点之间<strong class="lk jd"> <em class="no">平均</em> </strong>距离最小的两个聚类。</li></ul><p id="9379" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们看看这三个链接标准是如何处理流派播放列表数据集的。我们用一个矩阵表示我们的结果，显示每个播放列表中的歌曲在每个集群中的百分比(集群被随意命名为“A”、“B”、“C”和“D”)。</p><p id="16bf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当然，如果聚类是完美的，我们希望矩阵的每一行和每一列都包含一个100%的条目(当然，它不必在对角线上，因为聚类名称的分配是任意的)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c00a236d57e474eef34f6fd0a42880d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*DRqVDgcTHXC3hxQEIjLDDw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">默认的“ward”链接试图最小化集群内的差异，在所有四种类型中都做得很好，尽管有一些泄漏到集群b中。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/49963b23939a87984ad28f0e442b5c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*ya8NGRX2SuKXNZZ9uPuJaQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">“完全”联动显然效果不佳。它已将大量数据集放入聚类a。聚类C由一首说唱歌曲组成。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/0e869640592169378721c10c07f57396.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*tj0HcUfpmEeKktzZdFHKfQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">“平均”关联与“完全”关联有相似的问题。许多数据点被放入一个集群中，两个集群由一首歌曲组成。</p></figure><p id="dfc8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">值得一提的是，还有另一种常见的聚类类型，K-Means，它的工作方式略有不同。HAC通过将聚类合并在一起来迭代地减少聚类的数量，而K-Means聚类保持固定数量的聚类(名义上的<em class="no"> k </em>)，但是迭代地改变每个聚类的成员。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/68e8b2771b55a98cab1478fdb9446e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/1*Nx6IyGfRAV1ly6uDGnVCxQ.gif"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">每个聚类的“中心”由带有黑色“+”号的“大”标记表示。所有的点都被分配到离其中心最近的聚类中。在这个分配步骤之后，重新计算聚类中心以包括新成员，并且发生另一个重新分配步骤。如果在重新分配步骤中没有点改变聚类，则算法结束(动画:<a class="ae lh" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank">维基百科</a>)。</p></figure><p id="706e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">K-Means在Python中实现起来很简单:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="09f0" class="nh ni it nd b gy nj nk l nl nm"><strong class="nd jd">#We scale the data to ensure that<br/>#feature units don't impact distances<br/>from</strong> sklearn.preprocessing <strong class="nd jd">import</strong> StandardScaler<br/>sclaer = StandardScaler()<br/>X_scaled = scaler.fit_transform(X)</span><span id="60af" class="nh ni it nd b gy nn nk l nl nm"><strong class="nd jd"><em class="no">#This scaled data can then be fed into the K-Means alorithm<br/></em>from</strong> sklearn.cluster <strong class="nd jd">import</strong> KMeans<br/><strong class="nd jd"><em class="no">#We can tell it how many clusters we're aiming for</em></strong><br/>km_clust = KMeans(n_clusters=4)<br/>assigned_clusters = km_clust.fit_predict(X_scaled)</span></pre><p id="0b7d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们对播放列表数据集应用K-Means聚类，我们会得到以下结果:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/b02dde400b75d891e03dbf3b378b50a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*crE4Rf4-1FeZqOZ3oRe2kA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">与使用“ward”链接的HAC算法一样，K-Means聚类在大多数算法中都做得很好，一些爵士乐和说唱歌曲被“误认为”K-Pop。</p></figure><p id="3710" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然这些矩阵有利于“目测”我们的结果，但它们远非数学上的严格。让我们考虑一些指标，以实际帮助我们为集群质量分配一个数字。</p><h1 id="c918" class="nr ni it bd ns nt nu nv nw nx ny nz oa ki ob kj oc kl od km oe ko of kp og oh bi translated">调整后的兰德指数</h1><p id="db36" class="pw-post-body-paragraph li lj it lk b ll oi kd ln lo oj kg lq lr ok lt lu lv ol lx ly lz om mb mc md im bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index" rel="noopener ugc nofollow" target="_blank">调整后的Rand指数</a>是经典Rand指数的变体，并试图表达多少比例的聚类分配是“正确的”。它通过考虑所有样本对，计算两个不同聚类之间的相似性度量，并针对真实聚类对分配到相同或不同预测聚类中的样本对进行计数，针对随机机会进行调整。</p><p id="0ef6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这一点(以及我们将考虑的其他指标)可以使用Scikit-Learn进行评估。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="e910" class="nh ni it nd b gy nj nk l nl nm"><strong class="nd jd">from</strong> sklearn <strong class="nd jd">import</strong> metrics</span><span id="aea3" class="nh ni it nd b gy nn nk l nl nm">metrics.adjusted_rand_score(predicted_labels, actual)</span></pre><p id="f247" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">调整后的Rand指数介于-1和1之间。越接近1越好，越接近-1越不好。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/b9f6de82474beb47dc851a00fa167691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*I6thrhfbUoTHhjI0DT9UlQ.png"/></div></figure><p id="da05" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们看到K-Means和Ward Linkage得分很高。根据我们之前观察到的矩阵，我们预料到了这一点。</p><h1 id="e770" class="nr ni it bd ns nt nu nv nw nx ny nz oa ki ob kj oc kl od km oe ko of kp og oh bi translated">福克斯·马洛得分</h1><p id="7d5e" class="pw-post-body-paragraph li lj it lk b ll oi kd ln lo oj kg lq lr ok lt lu lv ol lx ly lz om mb mc md im bi translated">Fowlkes Mallow评分也是类似的，因为它告诉你聚类分配的“正确”程度。特别地，它计算精确度和召回率之间的几何平均值。它介于0和1之间，值越高越好。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="f602" class="nh ni it nd b gy nj nk l nl nm">metrics.fowlkes_mallows_score(predicted_labels, actual)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/4ac13bffba2990e759dee554bda91bf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*hXsnpYbimn6aIefj_STO2A.png"/></div></figure><p id="a15b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的排名类似于调整后的兰德指数——这是我们所期望的，因为它们是试图回答同一个问题的两种方法。</p><p id="3a1e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">值得注意的是，为了计算这些指标，我们需要知道原始标签。考虑到处理未标记数据是无监督学习的主要用例之一，我们需要一些其他度量来评估聚类结果，而不需要参考“真实”标签。</p><p id="79ec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">假设我们从三个独立的聚类分析中得到以下结果。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/508848d249f8cd54d62a7b85130b3c83.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*zAQHaae6WQff9bJCubyNqg.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/c3613a1a5df9e1d9780dc6f31636eed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*8NZS-b55AdIK7kVhcSbVvA.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/ff5aed9dd621d37f21eee8245e6f4a8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*hxA9TWt6WQdIZCOHFbodiw.png"/></div></figure><p id="f4f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">显然，我们能使集群越“紧密”越好。有什么方法可以把这个“紧密”的概念归结为一个数字吗？</p><h1 id="6aae" class="nr ni it bd ns nt nu nv nw nx ny nz oa ki ob kj oc kl od km oe ko of kp og oh bi translated">剪影分数</h1><p id="c429" class="pw-post-body-paragraph li lj it lk b ll oi kd ln lo oj kg lq lr ok lt lu lv ol lx ly lz om mb mc md im bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Silhouette_(clustering)" rel="noopener ugc nofollow" target="_blank">剪影分数</a>试图描述一个数据点与其聚类中的其他数据点的相似程度，相对于其聚类中的数据点<em class="no">而非</em>(这是对所有数据点的汇总，以获得整体聚类的分数)。换句话说，它考虑的是星团在空间中的“独特性”——事实上，人们可以使用任何“距离”来计算分数。</p><p id="b78e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它介于-1和1之间。更接近-1表示聚类不正确，而更接近+1表示每个聚类都非常密集。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="fc26" class="nh ni it nd b gy nj nk l nl nm">metrics.silhouette_score(scaled_feature_data, cluster_labels)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/9d439736ed7f429b091c18cef863dcfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*g8tS8b0_OOxwS9XwQW812Q.png"/></div></figure><p id="e80b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们看到没有一个集群有超高的轮廓分数。有趣的是，我们看到平均连锁群得分最高。但是，请记住，这种算法产生了两个集群，每个集群只包含一个数据点，这在现实世界中不太可能是一个理想的结果(这是一个教训，您通常不能依赖单个指标来决定算法的质量！)</p><h1 id="7c90" class="nr ni it bd ns nt nu nv nw nx ny nz oa ki ob kj oc kl od km oe ko of kp og oh bi translated">卡林斯基哈拉巴兹指数</h1><p id="4ddd" class="pw-post-body-paragraph li lj it lk b ll oi kd ln lo oj kg lq lr ok lt lu lv ol lx ly lz om mb mc md im bi translated">卡林斯基哈拉巴兹指数是一个数据点相对于其他聚类中的点的方差与相对于其聚类内的点<em class="no">的方差的比率。因为我们希望第一部分高，第二部分低，所以高CH指数是理想的。与我们看到的其他指标不同，这个分数是没有界限的。</em></p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="1eab" class="nh ni it nd b gy nj nk l nl nm">metrics.calinski_harabasz_score(scaled_feature_data, cluster_labels)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/1bdb4f5ba396999dc97c0395bb1a6138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*Qe_gnN6bIRmWbnfjewqKFg.png"/></div></figure><p id="7eb8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里我们看到我们的K-Means和Ward连锁算法得分很高。完整和平均连锁算法因具有一个或两个较大的聚类而受到惩罚，这将具有较高水平的内部方差。</p></div><div class="ab cl op oq hx or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="im in io ip iq"><blockquote class="ow ox oy"><p id="af49" class="li lj no lk b ll lm kd ln lo lp kg lq oz ls lt lu pa lw lx ly pb ma mb mc md im bi translated">这是我的“<a class="ae lh" href="https://towardsdatascience.com/tagged/music-by-numbers" rel="noopener" target="_blank">数字音乐</a>”专栏中的最新博客，它使用数据讲述关于音乐的故事。我很乐意听到对上述分析的任何评论——欢迎在下面留言，或通过<a class="ae lh" href="https://www.linkedin.com/in/callum-ballard/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我！</p></blockquote></div></div>    
</body>
</html>