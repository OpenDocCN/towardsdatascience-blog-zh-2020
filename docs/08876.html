<html>
<head>
<title>Most Common Loss Functions in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中最常见的损失函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0?source=collection_archive---------17-----------------------#2020-06-26">https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0?source=collection_archive---------17-----------------------#2020-06-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a84a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">现实世界中的 DS</h2><div class=""/><div class=""><h2 id="bec5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">每个机器学习工程师都应该了解机器学习中这些常见的损失函数，以及何时使用它们。</h2></div><blockquote class="kr"><p id="850f" class="ks kt it bd ku kv kw kx ky kz la lb dk translated">在<a class="ae lc" href="https://en.wikipedia.org/wiki/Mathematical_optimization" rel="noopener ugc nofollow" target="_blank">数学优化</a>和<a class="ae lc" href="https://en.wikipedia.org/wiki/Decision_theory" rel="noopener ugc nofollow" target="_blank">决策理论</a>中，损失函数或成本函数是将<a class="ae lc" href="https://en.wikipedia.org/wiki/Event_(probability_theory)" rel="noopener ugc nofollow" target="_blank">事件</a>或一个或多个变量的值映射到<a class="ae lc" href="https://en.wikipedia.org/wiki/Real_number" rel="noopener ugc nofollow" target="_blank">实数</a>上的函数，该实数直观地表示与该事件相关的一些“成本”。<br/>——<a class="ae lc" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></blockquote><figure class="le lf lg lh li lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ld"><img src="../Images/d97bcee98a515aa9bb9675ce1727a9c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l3kNYfW54bLC2b1VW4whmA.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><a class="ae lc" href="https://unsplash.com/@joshsrose?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">约什·罗斯</a>在<a class="ae lc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="ca8b" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated">作为一个核心元素，损失函数是一种评估机器学习算法的方法，它可以很好地模拟您的特征数据集。它被定义为<strong class="lw jd">衡量你的模型在预测预期结果方面有多好。</strong></p><p id="0039" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated"><strong class="lw jd"> <em class="mp">成本函数</em> </strong>和<strong class="lw jd"> <em class="mp">损失函数</em> </strong>指同一上下文。成本函数是作为所有损失函数值的平均值计算的函数。然而，损失函数是针对每个样本输出与其实际值的比较来计算的。</p><p id="99ef" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated"><strong class="lw jd"> <em class="mp">损失函数与你已经建立的模型的预测直接相关。因此，如果您的损失函数值较小，您的模型将提供良好的结果。损失函数，或者更确切地说，用于评估模型性能的成本函数，需要最小化以提高其性能。</em> </strong></p><p id="84f2" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated">现在让我们深入研究损失函数。</p><p id="8843" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated">广泛地说，损失函数可以根据我们在现实世界中遇到的问题类型分为两大类—<a class="ae lc" href="https://en.wikipedia.org/wiki/Loss_functions_for_classification" rel="noopener ugc nofollow" target="_blank"><strong class="lw jd"/></a>分类和<strong class="lw jd">回归</strong>。在分类中，任务是预测问题所处理的所有类别各自的概率。相反，在回归中，任务是预测关于学习算法的一组给定独立特征的连续值。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="44ed" class="mz na it mv b gy nb nc l nd ne"><strong class="mv jd">Assumptions:</strong><br/>    n/m — Number of training samples.<br/>    i — ith training sample in a dataset.<br/>    y(i) — Actual value for the ith training sample.<br/>    y_hat(i) — Predicted value for the ith training sample.</span></pre><h1 id="96e6" class="nf na it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">分类损失</h1><h2 id="f8a1" class="mz na it bd ng nw nx dn nk ny nz dp no md oa ob nq mh oc od ns ml oe of nu iz bi translated">1.二元交叉熵损失/对数损失</h2><p id="eaa6" class="pw-post-body-paragraph lu lv it lw b lx og kd lz ma oh kg mc md oi mf mg mh oj mj mk ml ok mn mo lb im bi translated">这是分类问题中最常用的损失函数。交叉熵损失随着预测概率收敛到实际标签而减少。它测量分类模型的性能，该模型的预测输出是介于 0 和 1 之间的概率值。</p><p id="9f5e" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated"><strong class="lw jd">当类别数为 2 时，<em class="mp">二元分类</em> </strong></p><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ol"><img src="../Images/17061658ec554151f795222e0d92b5e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-nJTj5mXtAAz9F107398sQ@2x.png"/></div></div></figure><p id="3c5f" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated"><strong class="lw jd">当类别数大于 2 时，<em class="mp">多类别分类</em> </strong></p><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b8d780bb64fe7ebaa6c86b99ea005c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*wcTVsYh3d4IQnYcUGdu66Q@2x.png"/></div></figure><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi on"><img src="../Images/b2222151ef3ac4767cee1b46eee9b070.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*3l1R-sWJVpDb1URHy5XFVA.png"/></div></figure><p id="b3f4" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated">交叉熵损失公式是从正则似然函数中推导出来的，但是加入了对数。</p><h2 id="0af7" class="mz na it bd ng nw nx dn nk ny nz dp no md oa ob nq mh oc od ns ml oe of nu iz bi translated">2.铰链损耗</h2><p id="9461" class="pw-post-body-paragraph lu lv it lw b lx og kd lz ma oh kg mc md oi mf mg mh oj mj mk ml ok mn mo lb im bi translated">用于分类问题的第二常见损失函数和交叉熵损失函数的替代函数是铰链损失，主要用于支持向量机(SVM)模型评估。</p><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/62c58841808e8c33ea3262e481d555a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*z9PRqqd_feIwV2EeJliIEA.jpeg"/></div></figure><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi op"><img src="../Images/adc53276a54fcb43e2d18bf77ac6212f.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*axK_lLrWa20u8_tM0V7uBQ.png"/></div></figure><blockquote class="oq or os"><p id="2871" class="lu lv mp lw b lx ly kd lz ma mb kg mc ot me mf mg ou mi mj mk ov mm mn mo lb im bi translated"><em class="it">铰链损失不仅惩罚错误的预测，也惩罚不自信的正确预测。它主要用于类别标签为-1 和 1 的 SVM 分类器。确保将恶性分类标签从 0 更改为-1。</em></p></blockquote><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ow"><img src="../Images/16d28840de25c623ff766e4eaf468a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4ZxpnNHNK9ehInmM"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">照片由<a class="ae lc" href="https://unsplash.com/@jentheodore?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jen Theodore </a>在<a class="ae lc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="ffd4" class="nf na it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">回归损失</h1><h2 id="f8d1" class="mz na it bd ng nw nx dn nk ny nz dp no md oa ob nq mh oc od ns ml oe of nu iz bi translated">1.均方误差/二次损耗/ L2 损耗</h2><p id="372c" class="pw-post-body-paragraph lu lv it lw b lx og kd lz ma oh kg mc md oi mf mg mh oj mj mk ml ok mn mo lb im bi translated">MSE 损失函数被定义为实际值和预测值之间的平方差的平均值。它是最常用的回归损失函数。</p><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/8554425e7ceda1beb1c20886c5f74349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*SRKwqe7YM2-cV3PMMN7VTg.png"/></div></figure><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/c9132a1bc37272922980a83b62d5666f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*H1mkqlq7buDZ7rDWD_4qIw.png"/></div></figure><p id="5652" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated">对应的代价函数是这些<strong class="lw jd">平方误差</strong>的<strong class="lw jd">均值</strong>。MSE 损失函数通过平方它们来惩罚产生大误差的模型，并且该属性使得 MSE 成本函数对异常值不太稳健。因此，<strong class="lw jd"> <em class="mp">如果数据容易出现很多离群值，就不应该使用。</em> </strong></p><h2 id="e1bd" class="mz na it bd ng nw nx dn nk ny nz dp no md oa ob nq mh oc od ns ml oe of nu iz bi translated">2.平均绝对误差/ L1 损耗</h2><p id="8c6c" class="pw-post-body-paragraph lu lv it lw b lx og kd lz ma oh kg mc md oi mf mg mh oj mj mk ml ok mn mo lb im bi translated">MSE 损失函数被定义为实际值和预测值之间的绝对差值的平均值。这是第二个最常用的回归损失函数。它测量一组预测中误差的平均大小，不考虑它们的方向。</p><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/bd8348c28f43455c8ce8f2be610881e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*5A-4HIx11hquyDaOImEFkA.png"/></div></figure><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/9e5f098c21be00a0fb14df83b993a567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*ID63kMgh8F0fcgmsrmjS5A.png"/></div></figure><p id="a3a4" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated">对应的代价函数是这些<strong class="lw jd">绝对误差(MAE) </strong>的<strong class="lw jd">均值</strong>。与 MSE 损失函数相比，MAE 损失函数对异常值更稳健。因此，<strong class="lw jd"> <em class="mp">在数据容易出现很多离群值的情况下应该使用。</em>T12】</strong></p><h2 id="606e" class="mz na it bd ng nw nx dn nk ny nz dp no md oa ob nq mh oc od ns ml oe of nu iz bi translated">3.Huber 损失/平滑平均绝对误差</h2><p id="d385" class="pw-post-body-paragraph lu lv it lw b lx og kd lz ma oh kg mc md oi mf mg mh oj mj mk ml ok mn mo lb im bi translated">Huber 损失函数被定义为当𝛿 ~ 0 时 MSE 和当𝛿 ~ ∞(大数)时 MAE 接近<strong class="lw jd">时 MSE 和 MAE 损失函数的组合。它是平均绝对误差，当误差很小时，它变成二次误差。使误差为二次型取决于误差有多小，这是由一个可以调节的超参数𝛿(δ)控制的。</strong></p><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/ae2b32a66feeac088062959b4f1c5568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*Vx7otH8Vzkkw_Gxzt9xpgg.png"/></div></figure><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/50b5cb58e6a4da4a78c5041cebb9d70a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*kBXeqZvMpfVMsYry0Chs5g.png"/></div></figure><p id="2418" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated">增量值的选择至关重要，因为它决定了您愿意将什么视为异常值。因此，取决于超参数值，与 MSE 损失函数相比，Huber 损失函数对异常值不太敏感。因此，<strong class="lw jd"> <em class="mp">如果数据容易出现异常值，可以使用它，并且</em>我们可能需要训练超参数 delta，这是一个迭代过程。</strong></p><h2 id="6b98" class="mz na it bd ng nw nx dn nk ny nz dp no md oa ob nq mh oc od ns ml oe of nu iz bi translated">4.对数损失</h2><p id="73ff" class="pw-post-body-paragraph lu lv it lw b lx og kd lz ma oh kg mc md oi mf mg mh oj mj mk ml ok mn mo lb im bi translated">对数余弦损失函数被定义为预测误差的双曲余弦的对数。这是回归任务中使用的另一个函数，比 MSE 损失平滑得多。它具有 Huber 损失的所有优点，并且它在任何地方都是两次可微的，不像 Huber 损失，因为像 XGBoost 这样的一些学习算法使用牛顿法来寻找最优值，因此需要二阶导数(<em class="mp"> Hessian </em>)。</p><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/139282adfe2a1985be9ca7e3e6266832.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*GrNtSStzBEwM343vuZoIIQ.png"/></div></figure><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/eccf1cadf1be41b0178377d97a398f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*VQf8ToK0Td-XQjfuWrD-ew.png"/></div></figure><blockquote class="oq or os"><p id="74bd" class="lu lv mp lw b lx ly kd lz ma mb kg mc ot me mf mg ou mi mj mk ov mm mn mo lb im bi translated"><code class="fe pd pe pf mv b"><em class="it">log(cosh(x))</em></code> <em class="it">约等于</em> <code class="fe pd pe pf mv b"><em class="it">(x ** 2) / 2</em></code> <em class="it">为小</em> <code class="fe pd pe pf mv b"><em class="it">x</em></code> <em class="it">，以</em> <code class="fe pd pe pf mv b"><em class="it">abs(x) - log(2)</em></code> <em class="it">为大</em> <code class="fe pd pe pf mv b"><em class="it">x</em></code> <em class="it">。这意味着“logcosh”的工作方式很大程度上类似于均方误差，但不会受到偶尔出现的严重错误预测的强烈影响。<br/> — </em> <a class="ae lc" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/logcosh" rel="noopener ugc nofollow" target="_blank"> <em class="it"> Tensorflow 文档</em> </a></p></blockquote><h2 id="b129" class="mz na it bd ng nw nx dn nk ny nz dp no md oa ob nq mh oc od ns ml oe of nu iz bi translated"><strong class="ak"> 5。分位数损失</strong></h2><p id="2f66" class="pw-post-body-paragraph lu lv it lw b lx og kd lz ma oh kg mc md oi mf mg mh oj mj mk ml ok mn mo lb im bi translated">分位数是一个值，低于这个值，一个组中的一部分样本就会下降。机器学习模型通过最小化(或最大化)目标函数来工作。顾名思义，分位数回归损失函数用于预测分位数。对于一组预测，损失将是其平均值。</p><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi pg"><img src="../Images/3d744c02c693f84fd142f732c32cb38c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S4LuBEEuOrPHv55YBisPNQ.png"/></div></div></figure><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/a9ec9fda72373ed86f80ea982b0bff4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*G9ei3IOtTOz2N8QCTj-37Q.png"/></div></figure><p id="0ebc" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated"><a class="ae lc" rel="noopener" target="_blank" href="/deep-quantile-regression-c85481548b5a">分位数损失函数</a>在我们对预测区间而不仅仅是点预测感兴趣时非常有用。</p></div><div class="ab cl ph pi hx pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="im in io ip iq"><p id="e19b" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo lb im bi translated">感谢您的阅读！希望这篇帖子有用。我感谢反馈和建设性的批评。如果你想谈论这篇文章或其他相关话题，你可以在这里或我的<a class="ae lc" href="https://www.linkedin.com/in/imsparsh/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>账户上给我发短信。</p><figure class="mq mr ms mt gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi po"><img src="../Images/463b4304d8622ab8bd9075b5656497bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NgtElaMk5jG3trBElXPG8A.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">照片由<a class="ae lc" href="https://unsplash.com/@crawford?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克劳福德乔利</a>在<a class="ae lc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure></div></div>    
</body>
</html>