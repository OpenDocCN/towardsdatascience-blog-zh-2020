# AI 算法会有偏差吗？

> 原文：<https://towardsdatascience.com/can-ai-algorithms-be-biased-6ab05f499ed6?source=collection_archive---------26----------------------->

## 定义、检测和避免偏见

人工智能算法越来越多地被用于广泛的领域，以做出影响我们日常生活的决策。例如，招聘、医疗保健、刑事司法、信用风险评分等。它不仅被私营企业使用，也被政府使用。

一般来说，使用人工智能或机器来做决定的一个好处是——它们可能是公正、客观的，可能不会像人类一样带有偏见，因此可能更“公平”。最近的一些研究表明，人工智能系统也可能有偏差。

公共图像数据库 Imagenet 最近删除了 60 万张图片，因为一个艺术项目[揭露了其图片中的种族偏见。所有这些都导致了对人工智能偏见的认识增加，并提出了与许多人工智能系统的信任有关的基本问题。](https://hyperallergic.com/518822/600000-images-removed-from-ai-database-after-art-project-exposes-racist-bias/)

## 什么是偏见？

在我们深入这个话题之前，定义偏见是很重要的。这是维基百科的一个定义

> **偏见**是指对一个观点或事物的或*不相称的重视*，通常以一种狭隘、偏见或不公平的方式。**

已经有很多关于人类遭受的偏见的研究，诺贝尔奖获得者丹尼尔·卡内曼在他的优秀著作[思考快与慢](https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow)中谈到了人类直觉中的各种偏见

人类通常遭受的偏差的一个例子是— **确认偏差。**它被定义为以一种确认或加强一个人已有信念的方式搜索、解释和回忆信息的倾向。虽然有人可能认为互联网的出现和我们可获得的信息(数据)的爆炸会让我们找到“真相”，但确认偏差是让人们选择性地解释信息并与他们已经相信的东西挂钩的因素。

![](img/3409818a0d82a1ab0fe4d16fa4795082.png)

**确认偏差**

## 人工智能偏差的案例研究

人工智能偏见的一个著名案例研究是美国法院用来评估被告成为惯犯的可能性的系统。对该软件的一项调查发现，虽然该系统是为最大化整体准确性而设计的，但非裔美国人的假阳性率是白种人的两倍。

另一个流行的案例研究是——亚马逊的人工智能招聘工具。亚马逊开发了一个工具来帮助招聘，帮助筛选简历。它是根据亚马逊内部招聘决策的 10 年数据进行训练的。当发现这个工具更有可能选择男人而不是女人时，亚马逊后来决定废弃这个工具

## 偏差的数学定义

虽然到目前为止我们所回顾的都是好的，但是偏见和公平的定义在法律、社会科学和哲学中有着长期的争论。很难就这些的确切定义达成共识。在人工智能模型的背景下，更棘手的是我们需要用数学术语来定义它，因为人工智能系统只理解数字和数学运算。

区分偏差和随机误差很重要。人工智能系统犯错并不罕见，这是因为它们是复杂现实世界的简化。但偏差是系统误差，它以某种可预测的方式发生。偏见可能会导致一个人对个人或团体做出不公平的行为。

一些人试图通过平衡不同群体(如性别、种族等)预测中的假阳性和假阴性率来定义人工智能系统中不存在偏差。但是这个定义不够普遍，因为偏见可能以各种形式表现出来。坦率地说，这仍然是一个开放的研究领域。

## 偏见是如何蔓延到人工智能系统中的？

一个自然的问题是，人工智能系统是如何产生偏差的？即使它们没有被明确地编纂成有偏见的，它们能有偏见吗？以下是人工智能系统可能出现偏差的一些可能原因:

*   **属性选择:**如果在算法中使用年龄、性别、种族等属性，算法可以学习这些属性与目标之间的关系，这可能会导致算法出现偏差。
*   **数据收集/采样:**很多时候，数据收集或采样的方式可能会导致某个群体代表性过高或过低，从而导致偏见。波士顿市政当局有一个关于[街道颠簸](http://streetbump.org/)应用的著名案例研究，该研究旨在检测路面坑洼，表明较富裕的社区有更多颠簸，只是因为生活在富裕社区的人更有可能使用该应用。
*   **数据中的隐含偏差:**大多数人工智能系统从提供给它们训练的数据中学习模式。很多时候，这些数据可能是由带有固有偏见的人生成的。如果是这种情况，最终的人工智能系统也会反映这些偏差，因为人工智能系统的好坏取决于训练它们的数据。*我之前说的 Imagenet* 事件就是这种类型的一个很好的例子。
*   **优化指标**:人工智能系统通常会训练最大化/最小化某个指标，例如整个数据的错误率。当人工智能系统忙于做这些事情时，不能保证它不会做任何我们人类认为“不公平”的事情。

## 怎样才能避免 AI 偏见？

虽然这仍然是一个开放的研究领域，但你可以采取以下步骤来减轻你的人工智能应用中的偏见:

*   **使用有代表性的数据**:尝试积极寻找数据中的偏差来源和/或数据收集和采样的方式。仔细研究数据是如何被注释的，以及人们注释数据的动机。
*   **对模型进行审核:**仔细检查你的模型的预测，比较和对比你的预测在不同子群体中的假阳性和假阴性率。受法律保护的子群体可能是一个很好的起点。
*   **关注模型的可解释性**:许多人工智能系统可能类似于黑匣子，对它们为什么做出某种预测的洞察力有限。有很多关于模型可解释性的研究工作正在进行——更好地理解为什么模型会做出某种预测可能有助于发现和消除偏差
*   **第三方工具**:有各种第三方工具可以帮助你在 AI 生命周期的不同阶段评估偏见。看看 [IBM 的 AI Fairness 360 开源工具包](http://aif360.mybluemix.net/)的例子
*   **雇佣多元化的团队:**团队的多元化可能有助于在寻找偏见的途径时引入多元化的观点。

## 摘要

人工智能应用最近出现了惊人的增长。有一些真实的例子对人工智能系统做出的决定的公平性提出了质疑。重要的是处理偏见问题，以便人工智能系统继续享有组织和群众的信任。在这个领域有很多有前途的工作正在进行，让我们希望所有这些将使人工智能应用更加公平，让这个世界成为每个人生活的更好的地方。