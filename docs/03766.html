<html>
<head>
<title>Control What You Can: Reinforcement Learning with Task Planning!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">控制你所能控制的:通过任务规划强化学习！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/control-what-you-can-reinforcement-learning-with-task-planning-b1fc9959b773?source=collection_archive---------25-----------------------#2020-04-08">https://towardsdatascience.com/control-what-you-can-reinforcement-learning-with-task-planning-b1fc9959b773?source=collection_archive---------25-----------------------#2020-04-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/1762957259f3f5752d5e5938b896c1b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GLelFy98C2usyaHNury2RA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk">[2]</p></figure><div class=""/><div class=""><h2 id="6d31" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">在这里我谈谈我们NeurIPS 2019的论文，将规划与强化学习代理agraphnd内在动机结合起来。</h2></div><p id="083a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现实世界中的许多控制问题都有某种层次结构。当我们谈论真正的自主机器人时，理想情况下，我们希望它们获得对环境的最大控制。我们希望这发生在通过奖励功能很少或没有监督的基础上。此外，我们希望代理在环境中利用<strong class="kw jg">这些固有的任务层次</strong>，以使学习更加有效。通俗地说，就是想让机器人自己一个人呆着，没有任何规范，让它自己想明白一切。大多数方法只处理这些问题中的一个，因此我们[1]提出了一种同时处理所有这些问题的方法。</p><p id="96fd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在开始使用这种方法之前，我们必须弄清楚我们所做的假设和某些术语。首先，我们假设环境有明确定义的任务空间，并且对我们可用。在我们的情况下，任务空间被定义为观察空间的子空间，例如物体的坐标，尽管机器人不知道语义。这意味着自我强加的目标对应于达到任务空间中的目标，例如，将对象移动到某个位置。</p><p id="1109" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">自然地，任务可以相互依赖，然而，现在仅仅学习这种依赖性就足够了，因为我们还需要知道在子任务中需要达到哪个确切的目标。举个例子，考虑一个仓库里的机器人。仓库中有一个重物太重，无法直接提起，因此机器人需要使用叉车。这个场景自然分解为3个任务(简化):机器人位置、叉车位置、重物位置。我们希望机器人自己想出应该移动到叉车，让叉车靠近重物，然后移动物体。这包括分别瞄准每个子任务的正确目标。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/db21c1a91c09ee14a54fd3a29a934ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*YnXO0w1JBnCpKmBs7z2uMQ.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">有机器人的仓库场景。我们提出的方法学习重物、叉车和机器人之间的正确任务依赖性。(1)</p></figure><p id="06ee" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">记住所有这些，在我们提议的方法中，这些是我们试图解决的主要挑战:</p><ol class=""><li id="30dc" class="lv lw jf kw b kx ky la lb ld lx lh ly ll lz lp ma mb mc md bi translated">在使用代理执行首次展示之前，我们如何选择要尝试的任务？</li><li id="73a2" class="lv lw jf kw b kx me la mf ld mg lh mh ll mi lp ma mb mc md bi translated">假设任务之间有相互依赖关系，我们如何找到正确的任务依赖关系？</li><li id="e2da" class="lv lw jf kw b kx me la mf ld mg lh mh ll mi lp ma mb mc md bi translated">我们如何根据学习到的任务依赖关系生成子目标？</li></ol><h2 id="adfc" class="mj mk jf bd ml mm mn dn mo mp mq dp mr ld ms mt mu lh mv mw mx ll my mz na nb bi translated">你能控制的框架</h2><p id="96c5" class="pw-post-body-paragraph ku kv jf kw b kx nc kg kz la nd kj lc ld ne lf lg lh nf lj lk ll ng ln lo lp ij bi translated">在《控制你所能》( CWYC)中，我们有多个部分共同发挥作用，以实现学习中的样本效率。但是在进入单个组件之前，我们需要一种方法来指导缺乏外在奖励的学习。人们可以依靠前向模型的预测误差作为替代奖励，但预测误差是不够的。它不够用的原因是在嘈杂的环境中。如果我们在环境中有不可预测的因素，这将产生持续的预测误差。因此，我们在以下所有组成部分<strong class="kw jg">中严重依赖<strong class="kw jg">惊喜</strong>对<strong class="kw jg">内在动机</strong>的衡量。</strong>在没有成功信号的情况下(如果任务被解决，则为1)，该算法更依赖<strong class="kw jg">惊喜作为替代奖励。如果一个事件/转变在一定的置信区间之外引起相当大的误差，我们说它是令人惊讶的，这使我们能够处理环境中的噪声。</strong></p><p id="b8fd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">任务选择器</strong>决定代理要尝试哪个任务。任务选择器实际上是一个多臂强盗，我们需要一种方法来适应它的臂分布。应该尝试的任务是可以改进最多的任务，因此我们引入<strong class="kw jg">改进/学习进度</strong>(成功的时间导数)作为更新分布的手段。</p><p id="9e65" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">关于任务依赖性的信息包含在<strong class="kw jg">任务规划器</strong>中，它实际上是一个对任务图或上下文多臂bandit进行编码的矩阵(其中上下文是要解决的任务，动作是之前的任务)。我们根据单元格条目从任务规划器中抽取任务序列。作为示例，从任务A到任务B的转移概率与求解B所需的时间量成比例，给定求解任务A和在B的目标空间中在求解任务B之前求解任务A时看到的惊奇</p><p id="c72b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">子目标生成器</strong>使我们能够<strong class="kw jg">给定任务链中的任务转移</strong>来设置目标。即，这意味着假设我们知道我们想要稍后解决任务B，生成器输出任务a的目标。子目标生成器可以被视为任务的目标空间上的潜在函数。同样，在没有成功信号的情况下，这种奖励被<strong class="kw jg">惊喜所主导。</strong></p><p id="57d3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">下图总结了上述所有组件:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/61b00098ba5a6ff247d2806a2a7cc891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0JJKV3ypq1KqqJ_rgCtV5w.png"/></div></div></figure><h2 id="5b2a" class="mj mk jf bd ml mm mn dn mo mp mq dp mr ld ms mt mu lh mv mw mx ll my mz na nb bi translated">实验</h2><p id="40e4" class="pw-post-body-paragraph ku kv jf kw b kx nc kg kz la nd kj lc ld ne lf lg lh nf lj lk ll ng ln lo lp ij bi translated">我们在两个连续的控制任务上评估了我们的方法，一个合成工具使用任务和一个挑战性机器人工具使用任务，对照内在动机和分级RL基线。我们注意到的是，在缺乏一个良好的奖励的情况下，基线无法学习如何解决这些层次化的任务。这与这样一个事实紧密相关，即状态空间中只有很小一部分与解决任务相关。在一个形状良好的奖励面前，其他方法也能够解决任务。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/1dc6a4c4a2dd3ef2503964dc44227e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LQ2N8xa_hcqS-boTwFnxUA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">我们在评估中使用的环境</p></figure><p id="2d43" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">下图是工具使用任务的性能图。我们的方法(CWYC)在层次结构中稍微复杂一点的任务上一直优于基线，比如拿起一个重物。详见[1]。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/5fb5267da0f563ace09a0dd61dc7ee8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IuZM0TVC23O6bQNa86-TRA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">实验结果。HIRO是另一种分层RL算法，ICM是我们的内在动机基线，SAC(软行动者-批评家)是标准RL算法。</p></figure><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/ebca8785a37e459b7fa1aee4a5828beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*jOVKVJ69pZ2fcEKGzyYNaQ.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk">(2)</p></figure><p id="84a3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在动画(2)中，我们可以看到算法在机械臂任务中的训练进度。盒子太远，机器人无法自己够到，所以它需要意识到它应该先抓住钩子，才能成功地将盒子移动到目标位置(红色)。</p><p id="433a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">此外，我们测量了算法的“资源分配”,即算法为每个单独的任务贡献了多少时间。在下图中，我们可以很好地说明我们的观点。基于我们的方法，通过使用<strong class="kw jg">意外信号</strong>及其与<strong class="kw jg">成功信号</strong>的组合，我们实现了有效的资源分配。无法解决的任务，如移动一个物体，随机出现在不同的地方，很快得不到关注。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/e57092a05ece06021bc57f90c7ec24d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*806dU_2PEreUfRQo7r8XRQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图(a)示出了代理随时间的资源分配，图(b)示出了任务依赖图的学习编码，较高的数字意味着在展开任务链时任务转移发生的概率较高。图(c)是学习任务图的图形表示，注意，进入任务的箭头意味着该任务是箭头所源自的任务的依赖项。例如，loco(运动)是工具任务的依赖项。</p></figure><p id="4eb5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">然而，问题仍然存在，我们能否进一步简化我们的假设，就像在任务空间的情况下一样。对于一个导致高效学习的问题，我们能学习到高效的任务空间划分吗？假设我们不知道任务空间的哪一部分是可达的，我们如何学习为任务取样可行的目标？这个我们留待以后研究。</p><h2 id="77b1" class="mj mk jf bd ml mm mn dn mo mp mq dp mr ld ms mt mu lh mv mw mx ll my mz na nb bi translated">参考</h2><p id="f1eb" class="pw-post-body-paragraph ku kv jf kw b kx nc kg kz la nd kj lc ld ne lf lg lh nf lj lk ll ng ln lo lp ij bi translated">[1] Blaes，Sebastian等人<a class="ae nl" href="https://bit.ly/3aSzEsZ" rel="noopener ugc nofollow" target="_blank">控制你所能控制的:内在激励的任务规划代理</a>，NeurIPS 2019</p><p id="a582" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">[2]图片取自Pixabay</p><p id="5460" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">确认</strong></p><p id="60b6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这是马克斯·普朗克智能系统研究所<a class="ae nl" href="https://al.is.tuebingen.mpg.de/" rel="noopener ugc nofollow" target="_blank">自主学习小组</a>的工作。</p></div></div>    
</body>
</html>