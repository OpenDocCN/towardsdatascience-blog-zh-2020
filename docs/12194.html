<html>
<head>
<title>Learn to Bet — Use Bayesian Bandits for Decision-Making</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学会下注——使用贝叶斯决策</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learn-to-bet-use-bayesian-bandits-for-decision-making-2e2e489087a5?source=collection_archive---------36-----------------------#2020-08-22">https://towardsdatascience.com/learn-to-bet-use-bayesian-bandits-for-decision-making-2e2e489087a5?source=collection_archive---------36-----------------------#2020-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="578b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">贝叶斯思维如何影响我们在概率世界中的决策</h2></div><p id="4dc8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们生活在不确定性中，有时我们不得不在给定的几个选项中做出选择，而我们对这些选项都知之甚少。随着时间的推移，如果我们一次又一次面对同样的选择，我们可能会知道哪个选择会给我们最大的回报，哪个是第二好的，等等。一般来说，这可以说是人类的学习过程，由我们的目标引导，最大化总回报(或最小化总损失或遗憾)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/d5ae2973abe18367497310d842cb4a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aMERssCHt9Fhy5Fe"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">马库斯·斯皮斯克在<a class="ae lr" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e215" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，一些重要的业务应用程序也可以用这种方式建模。思考以下情况:</p><blockquote class="ls lt lu"><p id="2cb0" class="kf kg lv kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">(1)给定一堆股票代码，每一个都有不同的回报，你如何理性地选择一个能使你的回报最大化的代码？</p><p id="c7d5" class="kf kg lv kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">(2)有三种网站登录页面设计可供您尝试，您会如何选择一种最大化您的指标(如转化率)的设计？</p><p id="2646" class="kf kg lv kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">(3)假设您想要推广您的业务，并且有三个不同的广告场地，您将如何选择一个最符合您预算的场地？</p></blockquote><p id="d173" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些现实的业务问题都可以方便地抽象为以下场景:</p><p id="ab6e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设你有 N 个吃角子老虎机(或强盗)，每个都有自己的概率来给你奖励 r。你如何计算出随着时间的推移选择哪个吃角子老虎机，以便尽可能多地获得奖励？</p><p id="2ea1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">这种说法看似简单，但实际上，决策过程要复杂得多</strong>,因为你要不断处理两难的选择，是坚持当前相对较好的选择，还是尝试其他可能更好的选择，即开发还是勘探。我们需要一个合理的框架来处理这种情况。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lz"><img src="../Images/febcc713624558da9c349a9091d4d017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IDjT6fPdGqvCd4Ww"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">照片由<a class="ae lr" href="https://unsplash.com/@gcalebjones?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯勒·琼斯</a>在<a class="ae lr" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="4dd7" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">贝叶斯强盗</h1><p id="a606" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">贝叶斯土匪提供了一个直观的解决问题的方法。一般来说，它遵循以下步骤:</p><ol class=""><li id="e6e3" class="mx my iq kh b ki kj kl km ko mz ks na kw nb la nc nd ne nf bi translated">对每个强盗给予奖励的可能性进行初步猜测。用统计学的语言来说，这就是假设每个土匪的行为(给不给奖励)有一定的先验分布。由于我们试图模拟概率，一个方便的先验分布将是贝塔分布，它通常用于模拟 0 到 1 之间的随机变量。请注意，如果您对强盗行为的潜在分布一无所知，您的理性猜测将是平等对待每个强盗，即假设均匀分布，这是 beta 分布的特例。</li><li id="fa51" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la nc nd ne nf bi translated"><strong class="kh ir">从每一个土匪</strong>身上抽取一个样本，根据其目前的分布情况。</li><li id="337f" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la nc nd ne nf bi translated"><strong class="kh ir">关注样本最高的土匪</strong>；通过选择来决定你获得的奖励(后悔)的数量，并<strong class="kh ir">相应地更新强盗的分布。</strong>在统计学的语言中，你有一个随机变量的特定参数的先验分布，现在你有了随机变量的值的新观察，你能够更新你关于随机变量的先验信念，通过应用贝叶斯规则得出后验分布；最终结果是有一组新的分布参数来解释新的观察结果:</li></ol><blockquote class="ls lt lu"><p id="90df" class="kf kg lv kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">先前分配:P(A)</p><p id="d147" class="kf kg lv kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">新观察:x</p><p id="9734" class="kf kg lv kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">后验分布:P(A|x) ~ P(x|A)*P(A)</p><p id="2952" class="kf kg lv kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">即<strong class="kh ir">后验分布与似然*先验分布</strong>成正比</p></blockquote><p id="1f97" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4.回到步骤 2 并重复。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nl"><img src="../Images/98010894cb32100372464db02a353b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JRMXWyAfb3qzt1ov"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">照片由<a class="ae lr" href="https://unsplash.com/@tine999?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tine ivani</a>在<a class="ae lr" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="d9d6" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">模拟</h1><p id="58d4" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">让我们看一个带有一些代码的具体例子。</p><p id="120e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，让我们导入必要的包:</p><pre class="lc ld le lf gt nm nn no np aw nq bi"><span id="f434" class="nr mb iq nn b gy ns nt l nu nv">import numpy as np</span><span id="718b" class="nr mb iq nn b gy nw nt l nu nv">from scipy.stats import beta</span><span id="b279" class="nr mb iq nn b gy nw nt l nu nv">import matplotlib.pyplot as plt</span></pre><p id="8109" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们定义我们的强盗；这些数字表示各自的强盗提供奖励的概率，你可以随意修改这些数字并运行不同的模拟:</p><pre class="lc ld le lf gt nm nn no np aw nq bi"><span id="a83e" class="nr mb iq nn b gy ns nt l nu nv">bandits = np.array([0.75, 0.5, 0.6])</span></pre><p id="c16f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着基础知识的清除，现在下面的函数说明了贝叶斯强盗的核心:</p><pre class="lc ld le lf gt nm nn no np aw nq bi"><span id="f402" class="nr mb iq nn b gy ns nt l nu nv">def experiment(accumulated_rewards_per_bandit, times_chosen_per_bandit, bandits, num_draws=1):<br/>    <br/>    for _ in range(num_draws):<br/>        bandit_chosen =    np.argmax(np.random.beta(1+accumulated_rewards_per_bandit, 1+times_chosen_per_bandit-accumulated_rewards_per_bandit))</span><span id="50ea" class="nr mb iq nn b gy nw nt l nu nv">        reward = get_reward_from_bandit_i(bandits, bandit_chosen)<br/>        <br/>        accumulated_rewards_per_bandit[bandit_chosen] += reward</span><span id="0fd4" class="nr mb iq nn b gy nw nt l nu nv">        times_chosen_per_bandit[bandit_chosen] += 1</span><span id="b4d7" class="nr mb iq nn b gy nw nt l nu nv">    a = 1+accumulated_rewards_per_bandit<br/>    b = 1+times_chosen_per_bandit-accumulated_rewards_per_bandit<br/>    return a, b</span></pre><p id="9bf3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们一行一行地分解这个函数。首先，让我们指定输入参数:</p><blockquote class="ls lt lu"><p id="4f7c" class="kf kg lv kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">每个强盗累积的奖励:在一个实验中，每个强盗累积的总奖励，它的大小等于强盗的数量。</p><p id="0192" class="kf kg lv kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">times _ chosen _ per _ bandit:在一个实验中，每个强盗被选中的次数，其大小等于强盗的数量。</p><p id="78b0" class="kf kg lv kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">强盗:我们各种奖励概率的强盗。</p><p id="537f" class="kf kg lv kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">num_draws:一个实验中的试验次数</p></blockquote><p id="9012" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回想一下，从根本上说，我们的目标是更好地了解不同强盗的回报概率，这意味着我们试图获得对基础分布越来越准确的估计，在我们的情况下，这些分布都是贝塔分布。</p><p id="ad6d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，beta 分布由两个参数<strong class="kh ir"> a </strong>和<strong class="kh ir"> b，</strong>唯一定义，在我们的示例中，这两个参数分别代表强盗获得奖励的尝试次数(在我们的示例中等于累积奖励)和强盗未获得奖励的尝试次数。</p><p id="77a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">因此，我们的目标是在试验中保持这两个量的精确记录。</strong></p><p id="68c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在应该清楚我们在上面的函数中试图实现什么了。循环查看试验次数，我们首先找出给出最大样本的强盗，然后通过在下面的行中选择这个强盗来确定我们是否得到任何奖励:</p><pre class="lc ld le lf gt nm nn no np aw nq bi"><span id="e69c" class="nr mb iq nn b gy ns nt l nu nv">reward = get_reward_from_bandit_i(bandits, bandit_chosen)</span></pre><p id="a191" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我使用下面的简单实现来确定奖励:</p><pre class="lc ld le lf gt nm nn no np aw nq bi"><span id="8088" class="nr mb iq nn b gy ns nt l nu nv">def get_reward_from_bandit_i(bandits, i):<br/>    return np.random.rand() &lt; bandits[i]</span></pre><p id="ba4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以看到这个实现导致奖励为 1 或 0。在实践中，你可能想设计自己的奖励机制来反映你的问题的性质。</p><p id="b83f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在获得奖励(或不获得)后，我们可以更新累积的奖励和为我们的采样 bandit 选择的时间，这将在下一次迭代中作为更新的 beta 分布参数 a 和 b。</p><p id="f8dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，在循环所有试验之后，我们获得完全更新的分布参数并返回它们。</p><p id="2dd4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们已经配备了贝叶斯强盗的核心组件，让我们使用下面的代码片段开始运行模拟:</p><pre class="lc ld le lf gt nm nn no np aw nq bi"><span id="f389" class="nr mb iq nn b gy ns nt l nu nv">if __name__ == "__main__":<br/>    bandits = np.array([0.75, 0.5, 0.6])</span><span id="4020" class="nr mb iq nn b gy nw nt l nu nv">    num_draws_per_experiment = [300]</span><span id="c165" class="nr mb iq nn b gy nw nt l nu nv">    accumulated_rewards_per_bandit = np.zeros(bandits.size)</span><span id="2e3f" class="nr mb iq nn b gy nw nt l nu nv">    times_chosen_per_bandit = np.zeros(bandits.size)</span><span id="6c34" class="nr mb iq nn b gy nw nt l nu nv">    for i in num_draws_per_experiment:<br/>        a, b = experiment(accumulated_rewards_per_bandit, times_chosen_per_bandit, bandits, i)</span></pre><h1 id="4e48" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">结果</h1><p id="8dec" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">似乎我们需要一种方法来确定我们做得有多好。在本文中，让我们尝试绘制每组 beta 分布参数的概率密度函数，看看它们是否显示出任何可辨别的模式。</p><p id="88c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们发挥这一功能:</p><pre class="lc ld le lf gt nm nn no np aw nq bi"><span id="9337" class="nr mb iq nn b gy ns nt l nu nv">def plot_beta(x_range, a, b, filename):<br/>    x = x_range<br/>    y0 = beta.pdf(x, a[0], b[0])<br/>    y1 = beta.pdf(x, a[1], b[1])<br/>    y2 = beta.pdf(x, a[2], b[2])</span><span id="a888" class="nr mb iq nn b gy nw nt l nu nv">    plt.plot(x, y0, color='red', lw=2, ls='-', alpha=0.5, label='pdf: 1st bandit')<br/>    plt.plot(x, y1, color='green', lw=2, ls='-', alpha=0.5, label='pdf: 2nd bandit')<br/>    plt.plot(x, y2, color='blue', lw=2, ls='-', alpha=0.5, label='pdf: 3rd bandit')</span><span id="1766" class="nr mb iq nn b gy nw nt l nu nv">    plt.legend()</span><span id="745b" class="nr mb iq nn b gy nw nt l nu nv">    plt.savefig(filename)</span></pre><p id="c947" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对不同数量的试验运行绘图功能，我们得到以下 pdf 图:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nx"><img src="../Images/b97bad90ec25089822ed07c47985e80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IgO2oXecxD7JWKzvblaikw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">每个强盗的 pdf 图，试验次数=1，10，50，100</p></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nx"><img src="../Images/963d6dd821d657539508383833f597b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vl-6ZnrAXe7Z60dIW9ytKQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">每个强盗的 pdf 图，试验次数=200，500，1000，2000</p></figure><p id="95c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，第一个强盗(红色)的回报概率最高，为 0.75，在 100 次尝试后，随着其 beta 分布的形状缩小并在真实值的 0.75 附近达到峰值，它逐渐胜出，而其他两个强盗的分布形状仍然相对平坦且分布广泛，这暗示了不确定性。</p><p id="8325" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管通过足够多的试验，每个强盗的分布将趋向于在奖励概率的真实值附近达到峰值，但在这个问题中，我们更感兴趣的是找到最好的强盗，而不是精确地推断隐藏的奖励概率。可以对我们的模拟进行一些改进:例如，可以将学习率(&gt; 1 或&lt; 1)应用于累积奖励的更新，以微调我们是倾向于保持当前的好选项还是更多地探索其他选项。</p><p id="9ffb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在后续文章中，我将展示贝叶斯土匪如何帮助构建股票的动态投资组合，这可以被建模为具有随时间变化的潜在回报概率的土匪。因此，请关注我，继续关注更多内容。</p></div></div>    
</body>
</html>