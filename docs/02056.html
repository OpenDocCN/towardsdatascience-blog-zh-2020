<html>
<head>
<title>Machine learning for stock prediction. A quantitative approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于股票预测的机器学习。定量方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-for-stock-prediction-a-quantitative-approach-4ca98c0bfb8c?source=collection_archive---------4-----------------------#2020-02-27">https://towardsdatascience.com/machine-learning-for-stock-prediction-a-quantitative-approach-4ca98c0bfb8c?source=collection_archive---------4-----------------------#2020-02-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e911" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">训练用于股票价格预测的多个模型并分析它们的结果</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a65a8c779debee4e0734a09a30306327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*W0vYxAn10YAdAYkW"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·斯皮斯克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a45a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用机器学习来预测<strong class="lb iu">股价</strong>可能具有挑战性，而<strong class="lb iu">可能很难</strong>。模拟股票价格的动态可能很难，在某些情况下，甚至是不可能的。</p><p id="6b60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将介绍一些使用机器学习预测股票价格的技术。我们将看到一些模型的运行，它们的性能以及如何<strong class="lb iu">改进</strong>它们。所有的计算都将使用著名的Pandas、NumPy和Scikit-learn库在Python中完成。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="e3f9" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">数据准备</h1><p id="3a75" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">对于这个例子，我将使用<strong class="lb iu">微软</strong>过去10年的每日股票价格，从<strong class="lb iu">雅虎财经</strong>下载CSV格式。除了交易量之外，该数据集还包括最高价、最低价、开盘价和调整后的收盘价。</p><p id="7e71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们加载数据集。为了简单起见，我将用“调整后收盘价”来代替“收盘价”。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="56c4" class="ne md it na b gy nf ng l nh ni">df = pd.read_csv("MSFT.csv",index_col=0)</span><span id="9137" class="ne md it na b gy nj ng l nh ni">df['Close'] = df['Adj Close']<br/>df = df.drop("Adj Close",axis=1)</span></pre><h1 id="c3b9" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">生成输入要素</h1><p id="226c" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">价格本身不足以产生有用的预测。对于本例，我将添加<strong class="lb iu">以下指示器</strong>，它们将用作我们模型的<strong class="lb iu">输入</strong>:</p><ul class=""><li id="abb1" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated">移动平均线(最近<em class="ny"> N </em>天的平均价格)</li><li id="0320" class="np nq it lb b lc nz lf oa li ob lm oc lq od lu nu nv nw nx bi translated">布林线(移动平均线加上或减去一定量的标准差)</li><li id="94e2" class="np nq it lb b lc nz lf oa li ob lm oc lq od lu nu nv nw nx bi translated">唐奇安通道(过去<em class="ny"> N </em>天的滚动最高价和最低价)</li></ul><p id="55e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们再加上<strong class="lb iu">周期为5，10，20，50，100 200的简单均线</strong>。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="f675" class="ne md it na b gy nf ng l nh ni">for sma_period in [5,10,20,50,100,200]:<br/>    indicator_name = "SMA_%d" % (sma_period)<br/>    df[indicator_name] = df['Close'].rolling(sma_period).mean()</span></pre><p id="5bee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们添加<strong class="lb iu">布林线</strong>，设置如下:</p><ul class=""><li id="860b" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated">20个周期，2个标准差</li><li id="1356" class="np nq it lb b lc nz lf oa li ob lm oc lq od lu nu nv nw nx bi translated">20个周期，1个标准差</li><li id="3faf" class="np nq it lb b lc nz lf oa li ob lm oc lq od lu nu nv nw nx bi translated">10个周期，1个标准差</li><li id="7be3" class="np nq it lb b lc nz lf oa li ob lm oc lq od lu nu nv nw nx bi translated">10个周期，2个标准差</li></ul><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="f4de" class="ne md it na b gy nf ng l nh ni">df['BollingerBand_Up_20_2'] = df['Close'].rolling(20).mean() + 2*df['Close'].rolling(20).std()</span><span id="c531" class="ne md it na b gy nj ng l nh ni">df['BollingerBand_Down_20_2'] = df['Close'].rolling(20).mean() - 2*df['Close'].rolling(20).std()</span><span id="51e2" class="ne md it na b gy nj ng l nh ni">df['BollingerBand_Up_20_1'] = df['Close'].rolling(20).mean() + df['Close'].rolling(20).std()</span><span id="0eab" class="ne md it na b gy nj ng l nh ni">df['BollingerBand_Down_20_1'] = df['Close'].rolling(20).mean() - df['Close'].rolling(20).std()</span><span id="b36f" class="ne md it na b gy nj ng l nh ni">df['BollingerBand_Up_10_1'] = df['Close'].rolling(10).mean() + df['Close'].rolling(10).std()</span><span id="5e17" class="ne md it na b gy nj ng l nh ni">df['BollingerBand_Down_10_1'] = df['Close'].rolling(10).mean() - df['Close'].rolling(10).std()</span><span id="dac2" class="ne md it na b gy nj ng l nh ni">df['BollingerBand_Up_10_2'] = df['Close'].rolling(10).mean() + 2*df['Close'].rolling(10).std()</span><span id="c170" class="ne md it na b gy nj ng l nh ni">df['BollingerBand_Down_10_2'] = df['Close'].rolling(10).mean() - 2*df['Close'].rolling(10).std()</span></pre><p id="ca6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，让我们加上均线周期相同的唐契安通道。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="9683" class="ne md it na b gy nf ng l nh ni">for channel_period in [5,10,20,50,100,200]:<br/>    up_name = "Donchian_Channel_Up_%d" % (channel_period)<br/>    down_name = "Donchian_Channel_Down_%d" % (channel_period)<br/>    <br/>    df[up_name] = df['High'].rolling(channel_period).max()<br/>    df[down_name] = df['Low'].rolling(channel_period).min()</span></pre><p id="da3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们想在<strong class="lb iu">的几个时间滞后</strong>中使用所有这些功能(即最后一天、最后两天等等)，所以我们需要<strong class="lb iu">将</strong>全部延迟一些时间。对于本例，我们将把它们从1天改为10天。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="64be" class="ne md it na b gy nf ng l nh ni">newdata = df['Close'].to_frame()<br/>for lag in [1,2,3,4,5,6,7,8,9,10]:<br/>    shift = lag<br/>    shifted = df.shift(shift)<br/>    shifted.columns = [str.format("%s_shifted_by_%d" % (column ,shift)) for column in shifted.columns]<br/>    newdata = pd.concat((newdata,shifted),axis=1)</span></pre><p id="c627" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看一下数据集，我们现在会注意到像SMA _ 5 _ shifted _ by _ 3这样的特征。意思是“前3天计算的5期移动平均线”。</p><p id="2eb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将创建<strong class="lb iu">目标变量</strong>，这是我们希望<strong class="lb iu">预测</strong>的数字。对于这个例子，假设我们想预测未来5天的收盘价。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="3b41" class="ne md it na b gy nf ng l nh ni">forward_lag = 5</span><span id="3386" class="ne md it na b gy nj ng l nh ni">newdata['target'] = newdata['Close'].shift(-forward_lag)<br/>newdata = newdata.drop('Close',axis=1)</span><span id="4cea" class="ne md it na b gy nj ng l nh ni">newdata = newdata.dropna()</span></pre><p id="45ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在有了<strong class="lb iu">大量的</strong>数据(2302行× 311列)，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/ad6d7abcc0d63168fdcff8570501886c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6H-TU5i9HQELhM_v7yBNVg.png"/></div></div></figure><h1 id="67b4" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">在训练集和测试集中拆分</h1><p id="22db" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">像往常一样，我们将把数据集分成<strong class="lb iu">训练</strong>和<strong class="lb iu">测试</strong>集。训练集将用于训练我们的模型并调整<strong class="lb iu">超参数</strong>，而测试集将仅用于<strong class="lb iu">性能</strong>计算。</p><p id="19b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机执行分割<strong class="lb iu"/>。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="92d5" class="ne md it na b gy nf ng l nh ni">X = newdata.drop("target",axis=1)<br/>Y = newdata['target']</span><span id="cf71" class="ne md it na b gy nj ng l nh ni">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)</span></pre><h1 id="ea78" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">特征选择</h1><p id="421f" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">310个特性是一个很大的数字。只拿<strong class="lb iu">最重要的</strong>非常有用。对于本例，我们将根据每个特征和目标的<strong class="lb iu">皮尔逊相关系数</strong>的绝对值对特征进行排序。这个想法是只选择与目标最相关的50个特征。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="0b3f" class="ne md it na b gy nf ng l nh ni">correlations = np.abs(X_train.corrwith(y_train))</span><span id="8a5d" class="ne md it na b gy nj ng l nh ni">features =  list(correlations.sort_values(ascending=False)[0:50].index)</span><span id="80a8" class="ne md it na b gy nj ng l nh ni">X_train = X_train[features]<br/>X_test = X_test[features]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/03a814459a0cfdfd234a134d3ee2278f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qBXuN-BOYxKG6BSBaoMTlQ.png"/></div></div></figure><p id="1420" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以从模型开始。</p><h1 id="a08c" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">线性回归</h1><p id="bc27" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">先说有史以来最简单的回归模型，那就是<strong class="lb iu">线性回归</strong>。</p><p id="cbe7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将训练模特。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="d38f" class="ne md it na b gy nf ng l nh ni">lr = LinearRegression()</span><span id="c621" class="ne md it na b gy nj ng l nh ni">lr.fit(X_train,y_train)</span></pre><p id="beae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们计算模型的<strong class="lb iu">预测</strong>。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="c3f5" class="ne md it na b gy nf ng l nh ni">y_pred = lr.predict(X_test)</span></pre><p id="7fb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预测可以与真实测试集数据进行比较，以计算<strong class="lb iu">预测误差</strong>。对于这个例子，我将使用<strong class="lb iu">平均绝对误差</strong>。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="be36" class="ne md it na b gy nf ng l nh ni">mean_absolute_error(y_test,y_pred)</span></pre><p id="592b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">误差为1.325</p><p id="a367" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们最终可以绘制测试集的<strong class="lb iu">真实值</strong>和<strong class="lb iu">预测值</strong>来检查它们是否位于穿过0的45°线上</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="c13c" class="ne md it na b gy nf ng l nh ni">plt.scatter(y_test,y_pred)<br/>plt.xlabel("Real")<br/>plt.ylabel("Predicted")<br/>plt.title("Linear regression")</span><span id="b578" class="ne md it na b gy nj ng l nh ni">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/7afa1b7d404e0c3790169575ec933a3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*-PlwT6jnEPmrHwzOhqTN9g.png"/></div></figure><p id="3353" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，真实值和预测值非常相似。</p><h1 id="fdfe" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">随机森林</h1><p id="38fe" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">下一个模型是来自sklearn库的<strong class="lb iu">随机森林回归器</strong>。</p><p id="33ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于随机森林模型有许多<strong class="lb iu">超参数</strong>，通过<strong class="lb iu">随机搜索</strong>微调它们是有用的。我们要调整的超参数是<strong class="lb iu">树的数量</strong>和<strong class="lb iu">特征的数量</strong>。</p><p id="eac1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用<strong class="lb iu"> 5重交叉验证</strong>执行20步随机搜索，计算每重的平均绝对误差，然后在5重中取平均值。最小化误差(即最大化带负号的误差)的超参数值是<strong class="lb iu">最佳</strong>值。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="a306" class="ne md it na b gy nf ng l nh ni">rf = RandomizedSearchCV(RandomForestRegressor(),<br/>param_distributions =  {<br/>                  'n_estimators':np.arange(10,500,5),<br/>                  'max_features':np.arange(1,10,1)<br/>               },<br/>                  cv=5, n_iter = 20,<br/>                  iid=False,random_state=0,refit=True,<br/>                  scoring="neg_mean_absolute_error")</span><span id="ac0c" class="ne md it na b gy nj ng l nh ni">rf.fit(X_train,y_train)</span></pre><p id="8433" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数的最佳值是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/f3c05911ba674380eb940585cab060f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*uw9jN287jkMSJHB_cPh8zA.png"/></div></figure><p id="7f55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用用于线性回归的相同代码计算的平均绝对误差是0.868</p><p id="9549" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是散点图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/bb4c9599969405b5febcee09c4bf4e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*gAp54tDUikDttBNO9eR1Yw.png"/></div></figure><p id="88fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林显示出比线性回归更低的误差。</p><h1 id="6eb2" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">梯度推进树回归器</h1><p id="2cb4" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们现在可以尝试使用<strong class="lb iu">梯度推进树回归器</strong>，它使用<strong class="lb iu">推进</strong>技术来提高模型精度。</p><p id="d36a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们仍然有<strong class="lb iu">两个超参数</strong>，它们是在提升序列中使用的树的数量和特征的最大数量。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="b6dc" class="ne md it na b gy nf ng l nh ni">gb = RandomizedSearchCV(GradientBoostingRegressor(),<br/>param_distributions =  {<br/>               'n_estimators':np.arange(10,500,5),<br/>               'max_features':np.arange(1,10,1)<br/>            },<br/>          cv=5, n_iter = 20,<br/>          iid=False,random_state=0,refit=True,<br/>          scoring="neg_mean_absolute_error")</span><span id="d93f" class="ne md it na b gy nj ng l nh ni">gb.fit(X_train,y_train)</span></pre><p id="a1f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数的最佳值是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/58aeb837acf6301adb55772c3518e9e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*KQqi0PO6rcHfTLJW9maAtA.png"/></div></figure><p id="f354" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平均绝对误差为0.916，下面是散点图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/e5e4043798fdedc7e447c86277eb5f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*NerqbHMictz0NVcxePNGhw.png"/></div></figure><p id="0be6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GBR是一个很好的模型，但有一个<strong class="lb iu">略高于</strong>随机森林的误差。</p><h1 id="7013" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">k个最近邻居</h1><p id="be42" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><strong class="lb iu"> KNN </strong>在许多应用程序中是一个非常强大模型，我们将在这个例子中使用它。我们将最近邻从1更改为20，并将评分公式从“统一”(每个邻居都具有相同的权重)更改为“距离”(每个邻居都由它与输入向量之间的距离的倒数进行加权)。</p><p id="9d36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于这个超参数空间非常小，我们可以很容易地使用<strong class="lb iu">网格搜索</strong>来代替随机搜索。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="2138" class="ne md it na b gy nf ng l nh ni">knn = GridSearchCV(KNeighborsRegressor(),<br/>param_grid =  {<br/>            'n_neighbors':np.arange(1,20,1),<br/>            'weights':['distance','uniform']<br/>            },<br/>          cv=5, <br/>          iid=False,refit=True,<br/>          scoring="neg_mean_absolute_error")</span><span id="c4cc" class="ne md it na b gy nj ng l nh ni">knn.fit(X_train,y_train)</span></pre><p id="44e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最佳值是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/6f64be2dbe9d25a305073f3949b79d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*lhlrl52G_CwOCMiRjCpyHw.png"/></div></figure><p id="8d8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平均绝对误差为0.830，这是散点图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/01275155ca1180a844b2d41d59ffbd41.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*2MzeRTpc92q_pyCnDrb7Ow.png"/></div></figure><p id="595f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，这是产生<strong class="lb iu">最低误差</strong>的模型。</p><h1 id="69dc" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">神经网络</h1><p id="f0f8" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们将使用单层<strong class="lb iu">人工神经网络</strong>完成我们的模型搜索，该网络具有5000个最大历元、自适应学习速率，并使用随机梯度下降优化算法。</p><p id="2195" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的超参数是隐藏层的神经元的数量(我们将从1到50跨越)和它们的激活函数(T21)的数量(将跨越逻辑和逻辑)。</p><p id="8543" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在将训练数据交给人工神经网络之前，我们必须<strong class="lb iu">对它们进行缩放</strong>。我们将使用<strong class="lb iu">最小最大缩放器</strong>将每个特征缩放到<strong class="lb iu">0–1区间</strong>。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="6047" class="ne md it na b gy nf ng l nh ni">scaler = MinMaxScaler()</span><span id="9adf" class="ne md it na b gy nj ng l nh ni">scaler.fit(X_train)</span><span id="8179" class="ne md it na b gy nj ng l nh ni">nnet = RandomizedSearchCV(MLPRegressor(max_iter=5000,learning_rate = 'adaptive',solver='sgd'),<br/>param_distributions =  {<br/>     'hidden_layer_sizes':[(x,) for x in np.arange(1,50,1)],<br/>     'activation':['logistic','relu']<br/>},<br/>cv=5, n_iter = 20,<br/>iid=False,random_state=0,refit=True,<br/>scoring="neg_mean_absolute_error")</span></pre><p id="a256" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些是最好的价值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/4f70b042ef33acd20a26cd3ff712e61b.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*XWRrvij9LKKjeepGbqiR4w.png"/></div></figure><p id="3c40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平均绝对误差为1.407，这是散点图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/fbb5f201fbf5d186b2569d5ea3694008.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*AcKwu0IbQh82hc8MLUpPTw.png"/></div></figure><h1 id="ca4d" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">KNN是量子交易的圣杯吗？不全是</h1><p id="8124" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">到目前为止，一切似乎都在告诉我们，KNN是最强大的模特。事实上，并非如此。</p><p id="b1c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一切都取决于我们如何执行<strong class="lb iu">培训/测试分割</strong>。如果我们随机分割记录，测试集中的一些记录可能<strong class="lb iu">非常接近</strong>训练集中的其他记录。这就是为什么KNN和树工作得很好。</p><p id="a944" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们把<strong class="lb iu">前70% </strong>的数据作为训练，把<strong class="lb iu">后30% </strong>的数据作为测试，会怎么样？测试记录现在与训练记录不同，所以事情会发生巨大的变化。</p><p id="0f50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过下面的代码来执行这种分割，它替换了前面的train_test_split代码</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="5ff2" class="ne md it na b gy nf ng l nh ni">X = newdata.drop("target",axis=1)<br/>Y = newdata['target']</span><span id="bca4" class="ne md it na b gy nj ng l nh ni">train_size = int(X.shape[0]*0.7)</span><span id="56d1" class="ne md it na b gy nj ng l nh ni">X_train = X[0:train_size]<br/>y_train = Y[0:train_size]</span><span id="fb4a" class="ne md it na b gy nj ng l nh ni">X_test = X[train_size:]<br/>y_test = Y[train_size:]</span></pre><p id="6f28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以<strong class="lb iu">重复</strong>每个型号的培训程序。在计算误差之前，让我们看一下散点图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/557cce715680b987ecb0072eb41e808d.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*1IuNsx7ALeFd3CTtSuSSKg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/95d2af5f4ad39e329dd698058b5b92f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*JDhR8r_jaBO2IONcPJqiMw.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/82503783e67f5a6e9859e56dddf73475.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*pqfR51GkYo4jpGCQ1WGK0A.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/5985cb52fabc0484c882edcfc95d86da.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*9o5i_3lt1neXUYQSmszB0w.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/eef27eabfc2380eb607c8d998721ea82.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*4LWdhKmdNg8_fyzTsu7YYQ.png"/></div></figure><p id="3e83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很明显，KNN和树现在表现很差<strong class="lb iu"/>。线性回归和神经网络<strong class="lb iu">仍然抵制</strong>。</p><p id="64c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是每个模型的平均绝对误差:</p><ul class=""><li id="acee" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated">线性回归:2.379</li><li id="2e0a" class="np nq it lb b lc nz lf oa li ob lm oc lq od lu nu nv nw nx bi translated">随机森林:42.591</li><li id="07b6" class="np nq it lb b lc nz lf oa li ob lm oc lq od lu nu nv nw nx bi translated">梯度推进回归量:42.594</li><li id="3464" class="np nq it lb b lc nz lf oa li ob lm oc lq od lu nu nv nw nx bi translated">k最近邻:42.885</li><li id="9eff" class="np nq it lb b lc nz lf oa li ob lm oc lq od lu nu nv nw nx bi translated">神经网络:2.366</li></ul><p id="ad18" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在最好的模型是<strong class="lb iu">神经网络</strong>。</p><p id="84ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">发生了什么事？</p><p id="e2df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，我们正在使用经典的机器学习方法来模拟一种现象，这实际上是一个<strong class="lb iu">时间序列</strong>。记录并非完全互不相关，因此<strong class="lb iu">随机</strong>训练/测试分割<strong class="lb iu">可能是错误的</strong>。这就是为什么交易系统通常是以历史数据的第一部分作为训练集，只有最近的数据作为测试集。</p><p id="5557" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，KNN和树只能预测与它们被训练的输出相似的输出。线性回归和神经网络是<strong class="lb iu">参数公式</strong>，因此一旦<strong class="lb iu">参数</strong>已经通过训练过程<strong class="lb iu">固定</strong>，它们可以无限制地预测任何可能的值。</p><p id="acbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">股票价格可以从0变到无穷大，这就是为什么我们需要一个数学模型而不是拓扑或几何模型。</p><h1 id="1daf" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">提高线性回归的性能</h1><p id="19b1" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们都喜欢神经网络，但让我们面对一个简单的事实:在这个例子中，ANN的表现比线性回归略好。这不是一个戏剧性的性能改进。</p><p id="a4da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我认为当模型是简单的时候，机器学习会更有效。这就是为什么，为了这个目的，我宁愿选择<strong class="lb iu">线性回归</strong>。</p><p id="4c21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有没有办法让<strong class="lb iu">提升</strong>的性能？当然可以。</p><p id="5ea3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还记得随机森林和梯度推进回归器吗？它们的预测能力依赖于<strong class="lb iu">装袋</strong>和<strong class="lb iu">助推</strong>技术。前者创建<strong class="lb iu">多个训练数据集</strong>重新采样原始记录并随机选择特征子集，而后者创建一系列模型，<strong class="lb iu">从之前的错误中学习</strong>。</p><p id="bf10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用sklearn的<strong class="lb iu"> BaggingRegressor </strong>和<strong class="lb iu"> AdaBoostRegressor </strong>模块，这些技术可适用于任何型号。</p><p id="5d16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了将Bagging应用于线性回归，我们将使用下面的代码。它对模型的数量和特征的数量执行超参数优化。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="941d" class="ne md it na b gy nf ng l nh ni">lr_bag = RandomizedSearchCV(BaggingRegressor(LinearRegression()),<br/>param_distributions =  {<br/>               'n_estimators':np.arange(10,500,5),<br/>               'max_features':np.arange(1,10,1)<br/>          },<br/>          cv=5, n_iter = 20,<br/>          iid=False,random_state=0,refit=True,<br/>          scoring="neg_mean_absolute_error")</span><span id="ee4e" class="ne md it na b gy nj ng l nh ni">lr_bag.fit(X_train,y_train)</span></pre><p id="d161" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最佳值是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/24bb5221df46474da263484e346a0249.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*0RKksyenkdMhp47iTkJGUQ.png"/></div></figure><p id="5c01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平均绝对误差为2.343，下面是散点图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/fdfac6e362d3b2688c60c774cfa486ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*omiUlSqSjC_6zPWYyFhw4g.png"/></div></figure><p id="8c51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们可以使用Boosting技术优化模型的数量(我们可以使用简单的网格搜索，因为我们只有一个超参数)。</p><p id="fd2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代码如下:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="d13c" class="ne md it na b gy nf ng l nh ni">lr_boost = GridSearchCV(AdaBoostRegressor(LinearRegression()),<br/>param_grid =  {<br/>               'n_estimators':np.arange(20,500,5)<br/>      },<br/>      cv=5, <br/>      iid=False,refit=True,<br/>      scoring="neg_mean_absolute_error")</span><span id="1725" class="ne md it na b gy nj ng l nh ni">lr_boost.fit(X_train,y_train)</span></pre><p id="7114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最佳值是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/d3a4b64496c8d223995727f17596692d.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*n6DxsK47conez0yNSPp_4A.png"/></div></figure><p id="3006" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平均绝对误差为2.580，散点图为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/c6393de42beb82bf5f9e70a27c61c826.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*zfj-GGvxaHPNd_Denb8fjQ.png"/></div></figure><p id="ef40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，通过使用bagging 的<strong class="lb iu">线性回归获得了最佳结果。</strong></p><h1 id="9a43" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">可能的后续步骤</h1><p id="5e24" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">预测股票价格很难，而且非常困难。应该经常<strong class="lb iu"/>这样做，以便从最近的价格波动中吸取教训，并试图更好地<strong class="lb iu">预测未来的</strong>。</p><p id="5a06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归在Bagging技术的帮助下表现最佳，Bagging技术<strong class="lb iu">减少过度拟合</strong>并试图<strong class="lb iu">减少输入特征之间的共线性</strong>。</p><p id="6669" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步可能是使用递归神经网络，特别是长短期记忆(<strong class="lb iu"> LSTM </strong>)模型。在过去的几年里，它们越来越受欢迎，并证明了它们模拟非线性问题的能力。</p><p id="62ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ny">来自《走向数据科学》编辑的提示:</em> </strong> <em class="ny">虽然我们允许独立作者根据我们的</em> <a class="ae ky" rel="noopener" target="_blank" href="/questions-96667b06af5"> <em class="ny">规则和指导方针</em> </a> <em class="ny">发表文章，但我们并不认可每个作者的贡献。你不应该在没有寻求专业建议的情况下依赖一个作者的作品。详见我们的</em> <a class="ae ky" rel="noopener" target="_blank" href="/readers-terms-b5d780a700a4"> <em class="ny">读者术语</em> </a> <em class="ny">。</em></p></div></div>    
</body>
</html>