# 揭开自动完成的神秘面纱—第 1 部分

> 原文：<https://towardsdatascience.com/index-48563e4c1572?source=collection_archive---------31----------------------->

![](img/9d930743471ad062863473313bb0549e.png)

照片由 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的 [Dmitry Ratushny](https://unsplash.com/@ratushny?utm_source=medium&utm_medium=referral) 拍摄

## 都是概率局长！

## 自动完成

你手机键盘应用程序的自动完成功能保存了多少次(或者有时毁掉了，取决于你想输入什么)你的对话？从我们每天发送的短信和电子邮件的数量来看，这个数字将是惊人的，你甚至可能不认为这是什么重要的事情，因为你已经习惯了这种经常被忽视和低估的技术。你甚至可能会继续说，“嗯，它有什么特别的，它很简单，我写一个单词，它就能预测接下来的事情。”

![](img/8aeff394fa5d2465215da389bcb4b462.png)

作者笔下的 autocomplete.jpeg

相信我，这看起来很简单。其实不是。在这个看似简单的事情背后有很多工作要做，以给你带来在网上大吵大闹时节省一些按键的便利。让我们揭开这个自动完成功能看似简单但并不简单的本质。

## 它只是预测…

**预测**是一个复杂的词。因为它有很多包装。我们在谈论什么样的预测？根据什么来预测？这个预测会如何进行呢？很多问题。因此，让我们为所有这些问题选择一个简单的答案。预测是基于某件事情发生的可能性。如果天气预报说明天会下雨，他们是在说明明天下雨的可能性。他们是如何做到的？根据过去几年的降雨记录。或者可能很多年。你明白了。

所以，当你在手机上写下一个单词时，你有没有注意到与这个单词最匹配的单词是如何立即出现的？这是因为底层软件试图预测你下一个可能写的单词。现在，它不会总是正确的，但它会尽最大努力列出一个单词列表，最好地补充您之前所写的内容。

可以这么说，自动完成就是预测你接下来可能会写什么！或者，我们可以说 autocomplete 会生成下一个最有可能的单词，不管我们已经写了什么。说到基于概率生成文本——在 **NLP** 中有一个特殊的主题叫做**语言模型。**

## 什么是语言模型？

维基百科对语言模型的定义对于理解这个主题的重要性来说并不那么鼓舞人心，但是不管怎样，让我们从这个开始吧。

> 统计语言模型是单词序列的概率分布。给定这样一个长度为 m 的序列，它分配一个概率 P(w1…wm)应用于整个序列。

是的……概率。好的。让我们开始吧。语言模型所做的是，对于一些文本(可以是一行或一段或任意长度的文本)，它为单词或字符分配概率分数。但是这些分数是基于什么呢？好问题。答案是，它们可以基于任何标准，但语言模型最可能的用例是找出一个单词在一种语言中出现在另一个单词之后的可能性。让我们以下面的句子为例。

```
I ate burger.
```

你好奇的心可能会问，为什么是汉堡而不是比里亚尼？也许我可以吃炸肉排。或者可能是多纳·凯巴普。或者其他太多的事情。你的大脑几乎立刻就在试图挑选可以用来代替`burger`的词。因为，一个人可以吃的东西很多吧？庞大的自然选择和进化强化的大脑数据库可以很容易地将`ate`与可能的食物列表联系起来。

好吧，我们简化一下。让我们忘记我们只有这句话

```
ate ___________
```

在单词 eat 后填空。根据你的知识，你能想出多少单词。现在，问问你自己，为什么在吃这个词后面应该只有食物或者消耗品？为什么不能写汽车，山，电视等。？因为你大脑中的知识库说——“伙计，吃车没道理。”**或者，你的大脑分配给某人吃车的概率非常低。**如此之低以至于不再合理或重要。

所以回到手边的语言模型这个话题。它为一些文本分配概率。语言模型可以用于从翻译(是的，谷歌翻译使用一种高级语言模型)到问答到自动完成——任何你需要生成单词或文本或预测它们的地方。

对于 autocomplete，正如您已经理解的，我们正在查看一个单词，并试图预测下一个单词。所以，如果我把第一句话`I ate burger.`拿来，试着根据每个单词基于前一个单词的概率来建模，它应该变成这样一个等式:

在我们写方程之前，让我们给句子添加一些符号。为什么这样马上就要谈到这一点了！

现在对于这个等式-

## 等等，那不是条件概率吗？

是的。每个单词的存在都基于前一个单词存在的先验。事实上，这种基于条件概率的语言建模有一个名字。这些被称为**马尔可夫模型。**

> 虽然基于左上下文对单个单词进行建模的任务似乎比给整个句子分配概率分数更容易管理，但等式中的最后一项仍然需要对 n-1 个单词进行调节，这与对整个句子进行建模一样困难。由于这个原因，语言模型利用了马尔可夫假设，即假设现在，未来独立于过去。

[2](#fn2-15899)

马尔可夫模型表明序列中的下一个元素是基于它之前的元素。这就引出了一个问题，我们只考虑了前一个词。如果有更多的话呢？为了得到先验概率，我们可以选择多少个词，有没有一个标准的定义？答案是否定的，对此没有标准的定义。你问的这个字数，叫做**窗口大小**。窗口大小可以根据许多因素而变化，比如你的文本量等等。窗口的值会直接影响您的预测结果。此外，除了窗口大小还有一个因素。

## n-grams

如果你看看上面的等式，我们只考虑了两个单词的**组合。比如**汉堡|吃了**。像这样的两个词的组合被称为**二元词**。如果我们考虑三个词，它将被称为一个**三元模型**。四个？四方图等等。因此这种方法被称为 **n 元语法**。像窗口大小一样，如果你使用马尔可夫模型，克数也会对概率产生影响。我在句子中添加了 BEGIN 和 END，这样句子开头和结尾的单词可以在 n-gram 中对照这些标记进行测试。**

## 其他句子呢？

假设我们坚持我们的方法，使用二元模型，只检查前一个单词。那么对于任何一个句子， **s** 和 **n** 个单词的语言模型的等式就是-

现在，如果我们想要一个三元模型呢？那么我们会检查前面的两个单词，而不是一个。

## 等等，怎么得到那个概率的值？

我很高兴你问了。我们使用一种被证明稳定的防弹统计方法，叫做**最大似然估计** [3](#fn3-15899) 。它只有一个任务，计算出某件事情发生的可能性，这是一件已经发生的相关事情。既然我们在谈论单词，我们就这样做:

假设我们有一个由单词 **a** 和 **b** 组成的二元模型，我们需要概率 **P(b | a)**

当我们说计数时，我们实际上是指它们在某个文本数据集或语料库中出现的次数。

## 文集

对于外行来说，语料库是一个结构化的、定义良好的、带注释的包含文本的数据集。如果你对机器学习感兴趣，你应该知道这意味着什么。我们人类学习数百个单词，我们的语言中有数千甚至数百万个单词。我们可以很容易地选择它们，因为我们的大脑已经经历了数千年的自我训练，并通过基因的共享将训练经验传递下去。我们的计算机是孤独的，没有无限的内存。他们一次只能看到所有文本的一小部分。

## 我在社交网络上使用的非常规词汇呢？

是的关于那个。你可能已经注意到，你在手机上使用的键盘会记住你最常用的单词类型。当你越来越多地使用键盘时，它会存储你用得更多的单词，并给它们分配一个特定的重要性权重，这样它们总是比其他单词获得更高的概率。

## 但是马科夫不是卡拉什尼科夫冲锋枪的头儿！

他们有他们的局限性。虽然马尔可夫模型对于小句和文本数据集或语料库的表现令人钦佩，但当句子和语料库开始变大时，它们就开始打嗝了。这就是为什么对于高级的现实世界任务，我们倾向于使用**神经语言模型**。在这里，您可以选择更多的参数，而不仅仅是一些窗口大小和 n-gram。

同样，正如你从上面的 MLE 方程中看到的，我们使用了单词的**频率。虽然使用频率很好，但有时可能会发生这样的情况，您的系统在一个语料库上训练，但缺少一个在实际训练语料库或系统附带的初始知识库中不存在的单词。该单词的 Count()将为 0，并且您的系统将为该单词分配一个 0 概率，这并没有真正的帮助，因为系统不会像预期的那样工作。相信我，没有人喜欢不能工作的软件。您可以对这类系统应用一些常见的补救措施，如平滑、对新单词使用特殊标记，或者将新单词添加到您的知识库等。**

对于单独的平滑，您可以应用拉普拉斯平滑、alpha 平滑 [5](#fn5-15899) [6](#fn6-15899) 、Jelinek Mercer 插值平滑 [7](#fn7-15899) [8](#fn8-15899) 、Kneser Ney 平滑 [9](#fn9-15899) 等。

另一件事，正如亚夫·戈德堡提到的-

> 基于 MLE 的语言模型缺乏跨上下文的通用性。观察到黑色汽车和蓝色汽车并不影响我们对红色汽车事件的估计，如果我们以前没有看到它的话。
> 
> [10](#fn10-15899)

## 这篇文章越来越长了

是的是的。我知道。我将在这篇文章的下一部分讨论神经语言模型。在那之前，继续使用你的手机键盘，让它们努力预测下一个单词！

## 参考资料和资源

*   乔里斯·佩勒曼斯，诺姆·沙泽尔和西普里安·切尔巴。稀疏非负矩阵语言建模。计算语言学协会汇刊，4(1):329–342，2016。http://aclweb.org/anthology/Q16–1024
*   戈德堡，你好。自然语言处理的神经网络方法(人类语言技术综合讲座)。摩根&克莱普出版社。
*   [StatQuest:最大似然，解释清楚！！！](https://www.youtube.com/watch?v=XepXtl9YKwc)
*   斯坦利. f .陈和约书亚.古德曼。语言建模平滑技术的实证研究。1996 年计算语言学协会第 34 届年会。http://aclweb.org/anthology/P96–1041·多伊:10.1006/csla
*   [维基百科关于加法平滑的文章](https://en.wikipedia.org/wiki/Additive_smoothing)
*   马丁·茹拉夫斯基。语音和语言处理(第三版。草稿)
*   [曼宁，克里斯托弗。统计自然语言处理基础(麻省理工学院出版社)](https://nlp.stanford.edu/fsnlp/)

[***也发表在我的博客上***](https://shawonashraf.github.io/demystify-autocomplete-1/)

1.  [https://en.wikipedia.org/wiki/Language_model](https://en.wikipedia.org/wiki/Language_model)t18】↩︎
2.  戈德堡，你好。自然语言处理的神经网络方法(人类语言技术综合讲座)(第 105-106 页)。摩根&克莱普出版社。Kindle 版。 [↩︎](#fnr2-15899)
3.  [https://en.wikipedia.org/wiki/Maximum_likelihood_estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)t24】↩︎
4.  [https://en.wikipedia.org/wiki/Markov_model](https://en.wikipedia.org/wiki/Markov_model)t28】↩︎
5.  斯坦利. f .陈和约书亚.古德曼。语言建模平滑技术的实证研究。计算机语音和语言，13(4):359–394，1999。↩︎
6.  约书亚·古德曼。语言建模的一点进步。更正，cs.CL/0108005, 2001 年。[http://arxiv.org/abs/cs.CL/0108005](http://arxiv.org/abs/cs.CL/0108005)doi:10.1006/csla . 2001.0174 .[↩︎](#fnr6-15899)
7.  弗雷德里克·耶利内克和罗伯特·默瑟。稀疏数据中马尔可夫源参数的插值估计。模式识别实践研讨会，1980 年。 [↩︎](#fnr7-15899)
8.  斯坦利. f .陈和约书亚.古德曼。语言建模平滑技术的实证研究。计算机语音和语言，13(4):359–394，1999。↩︎
9.  赖因哈德·科内瑟和赫尔曼·内伊。改进的 m-gram 语言建模的后退。声学、语音和信号处理，ICASSP-95，国际会议，第 1 卷，181-184 页，1995 年 5 月。doi:10.1109/icassp . 1995.479394 .[↩︎](#fnr9-15899)
10.  戈德堡，你好。自然语言处理的神经网络方法(人类语言技术综合讲座)(第 108 页)。摩根&克莱普出版社。Kindle 版。 [↩︎](#fnr10-15899)