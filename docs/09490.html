<html>
<head>
<title>Creating My Own Dataset using BeautifulSoup</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 BeautifulSoup 创建我自己的数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-my-own-dataset-using-beautifulsoup-43c4284210d4?source=collection_archive---------65-----------------------#2020-07-06">https://towardsdatascience.com/creating-my-own-dataset-using-beautifulsoup-43c4284210d4?source=collection_archive---------65-----------------------#2020-07-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b7e0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何使用 BeautifulSoup 库应用 Web 报废来构建自定义数据集以供将来分析。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f81af960e1b37571d99871ad31e1d3e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dFuaWNM0T7TTCF2GPGyUsw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://pixabay.com/service/terms/#license" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>授权</p></figure><p id="cd18" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个项目的目标是使用网络抓取创建我自己的数据集，从<a class="ae kv" href="http://www.renthop.com/" rel="noopener ugc nofollow" target="_blank"> RentHop 网站</a>提取波士顿公寓租赁数据，并将其保存到 CSV 文件中。</p><p id="4ec2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将遵循 4 个简单的步骤:</p><ol class=""><li id="e991" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">访问网页</li><li id="11e9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">查找特定信息</li><li id="e723" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">检索数据</li><li id="f8bd" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">保存数据</li></ol><h1 id="95a0" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">加载库</h1><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="7965" class="nd mh iq mz b gy ne nf l ng nh">import pandas as pd<br/>import requests<br/>from bs4 import BeautifulSoup</span></pre><p id="cef2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">验证<em class="ni">请求</em>和<em class="ni"> BeautifulSoup </em>库是否已经安装。如果没有，则需要接下来的步骤来运行项目代码。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="56a8" class="nd mh iq mz b gy ne nf l ng nh">pip install beautifulsoup4<br/>pip install requests</span></pre><h1 id="077c" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">访问网页</h1><p id="01a8" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">我将使用库<em class="ni">请求</em>来访问 RentHop 站点。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="7d2b" class="nd mh iq mz b gy ne nf l ng nh">r = requests.get('https://www.renthop.com/boston-ma/apartments-for-rent')<br/>r.content</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b7454706fbf652e1202b8fd720243c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*rwPRrx-etYKHdwAh"/></div></figure><p id="6563" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将使用<em class="ni"> BeautifulSoup </em>来进行 HTML 解析。我将创建一个<em class="ni"> BeautifulSoup </em>对象，并应用一个过滤器来获取代码中的&lt; div &gt;标签。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="80dd" class="nd mh iq mz b gy ne nf l ng nh"># Creating an instance of BeautifulSoup<br/>soup = BeautifulSoup(r.content, "html5lib")</span><span id="4f5f" class="nd mh iq mz b gy np nf l ng nh">listing_divs = soup.select('div[class*=search-info]')<br/>print(listing_divs)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/5d80dee4dc32bf45b0ace4295680f036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/0*IQSK6mVPqIsDE83a"/></div></figure><h1 id="852f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">查找特定信息</h1><p id="79b6" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">之所以把所有的&lt;<strong class="ky ir"> div &gt; </strong>标签都放到我的 list <em class="ni"> listing_divs </em>中，是因为包含了 20 套公寓的房源数据。现在我应该找出每个公寓的单独数据点。</p><p id="d9ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些是我想要锁定的信息:</p><ul class=""><li id="e609" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nr ly lz ma bi translated">列表的 URL</li><li id="3ce7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nr ly lz ma bi translated">公寓的地址</li><li id="8dc8" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nr ly lz ma bi translated">附近</li><li id="03ea" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nr ly lz ma bi translated">卧室数量</li><li id="69bf" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nr ly lz ma bi translated">浴室数量</li></ul><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="053b" class="nd mh iq mz b gy ne nf l ng nh"># Retrieving data from one record</span><span id="f3a5" class="nd mh iq mz b gy np nf l ng nh">url = listing_divs[0].select('a[id*=title]')[0]['href']<br/>address = listing_divs[0].select('a[id*=title]')[0].string<br/>neighborhood = listing_divs[0].select('div[id*=hood]')[0].string.replace('\n','')</span><span id="71ee" class="nd mh iq mz b gy np nf l ng nh">print(url)<br/>print(address)<br/>print(neighborhood)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/fe6a8c1412f0a850aae0b79a190815af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*qOvFf_EjuMjMC7Za"/></div></figure><h1 id="6553" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">检索数据</h1><p id="2559" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">要获得超过 20 条记录的数据，需要在几个页面上迭代。使用网站上的<em class="ni">搜索</em>选项，我得到了下面的 URL，它将有助于通过最后一个参数<em class="ni">页面</em>在不同的页面导航。</p><p id="6714" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">https://www.renthop.com/search/boston-ma?min _ price = 0 &amp; max _ price = 50000 &amp; q = &amp; sort = hop score &amp; search = 0 &amp;<strong class="ky ir">page = 0</strong></p><p id="77d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们创建一个简单的代码来生成第 1 页到第 4 页的 URL:</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="85c8" class="nd mh iq mz b gy ne nf l ng nh">url_prefix = "https://www.renthop.com/search/boston-ma?min_price=0&amp;max_price=50000&amp;q=&amp;sort=hopscore&amp;search=0&amp;page="<br/>page_number = 0</span><span id="525e" class="nd mh iq mz b gy np nf l ng nh">for url in <em class="ni">range</em>(4):<br/>    target_page = url_prefix + <em class="ni">str</em>(page_number)<br/>    print(target_page + '\n')<br/>    page_number += 1</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/5496489427b8c81769aea6ef1fdf3f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*tsDFevR6E1SUk73R"/></div></figure><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="9a7b" class="nd mh iq mz b gy ne nf l ng nh"># Creating a function to retrieve data from a list of &lt;div&gt;s</span><span id="bb09" class="nd mh iq mz b gy np nf l ng nh"><em class="ni">def</em> retrieve_data(listing_divs):    <br/>    listing_list = []<br/>    for index in <em class="ni">range</em>(<em class="ni">len</em>(listing_divs)):<br/>        each_listing = []<br/>        current_listing = listing_divs[index]</span><span id="308b" class="nd mh iq mz b gy np nf l ng nh">        url = current_listing.select('a[id*=title]')[0]['href']<br/>        address = current_listing.select('a[id*=title]')[0].string<br/>        neighborhood = current_listing.select('div[id*=hood]')[0].string.replace('\n','')</span><span id="42a3" class="nd mh iq mz b gy np nf l ng nh">        each_listing.append(url)<br/>        each_listing.append(address)<br/>        each_listing.append(neighborhood)</span><span id="bce7" class="nd mh iq mz b gy np nf l ng nh">        listing_specs = current_listing.select('table[id*=info] tr') </span><span id="d0fb" class="nd mh iq mz b gy np nf l ng nh">        for spec in listing_specs:<br/>            try:<br/>                each_listing.extend(spec.text.strip().replace(' ','_').split())<br/>            except:<br/>                each_listing.extend(np.nan)<br/>        listing_list.append(each_listing)</span><span id="7b63" class="nd mh iq mz b gy np nf l ng nh">    return listing_list</span><span id="1f9a" class="nd mh iq mz b gy np nf l ng nh"># Looping and getting data from 350 pages (part of the result of searching)</span><span id="400f" class="nd mh iq mz b gy np nf l ng nh">url_prefix = "https://www.renthop.com/search/boston-ma?min_price=0&amp;max_price=50000&amp;q=&amp;sort=hopscore&amp;search=0&amp;page="<br/>page_number = 1</span><span id="5d77" class="nd mh iq mz b gy np nf l ng nh">all_pages_parsed = []<br/>pages = 350</span><span id="075b" class="nd mh iq mz b gy np nf l ng nh">for url in <em class="ni">range</em>(pages):<br/>    target_page = url_prefix + <em class="ni">str</em>(page_number)<br/>    page_number += 1</span><span id="2780" class="nd mh iq mz b gy np nf l ng nh">    r = requests.get(target_page)<br/>    <br/>    # Getting a BeautifulSoup instance to be able to retrieve data<br/>    soup = BeautifulSoup(r.content, "html5lib")</span><span id="164b" class="nd mh iq mz b gy np nf l ng nh">    listing_divs = soup.select('div[class*=search-info]')<br/>    <br/>    one_page_parsed = retrieve_data(listing_divs)<br/>    all_pages_parsed.extend(one_page_parsed)</span></pre><h1 id="4886" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">将数据保存在 CSV 文件中</h1><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="f25e" class="nd mh iq mz b gy ne nf l ng nh">df = pd.DataFrame(all_pages_parsed, columns=['url','address','neighborhood','price','rooms','baths','none'])<br/>df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/09ab4a50f6ebba430fec83189aa063ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*ZJ0a7dSnS2ekAfNn"/></div></figure><p id="2ec6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，最后一步！</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="530b" class="nd mh iq mz b gy ne nf l ng nh"># Writing a comma-separated values (CSV) file</span><span id="9c36" class="nd mh iq mz b gy np nf l ng nh">df.to_csv('apartments_leasing.csv', index=False)</span></pre><h1 id="c9c2" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论:</h1><ul class=""><li id="624e" class="ls lt iq ky b kz nj lc nk lf ns lj nt ln nu lr nr ly lz ma bi translated">应用网络抓取允许我们为将来的分析创建我们自己的数据集。这只是一个例子，但是互联网上有很多网页</li><li id="3a70" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nr ly lz ma bi translated">为了方便提取和定位我们需要的数据，了解要废弃的网页代码是至关重要的</li><li id="4d8c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nr ly lz ma bi translated"><em class="ni"> BeautifulSoup </em>是一个有用且强大的网页抓取工具，它很容易学习，并且有非常好的文档，你可以在这个<a class="ae kv" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start" rel="noopener ugc nofollow" target="_blank">链接</a>上查看</li><li id="854b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nr ly lz ma bi translated"><em class="ni"> BeautifulSoup </em>需要一个外部库向网站发出请求，在这种情况下，我使用了<em class="ni"> Requests </em>，这种依赖对于这个特定的项目来说并不代表任何缺点</li><li id="beae" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nr ly lz ma bi translated">我邀请你在我的<a class="ae kv" href="https://github.com/mwpnava/Data-Science-Projects/tree/master/Apartments_Leasing" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>上查看这个项目的完整代码</li></ul></div></div>    
</body>
</html>