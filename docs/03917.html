<html>
<head>
<title>3 skills to master before reinforcement learning (RL)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习(RL)前需要掌握的3项技能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-skills-to-master-before-reinforcement-learning-rl-4176508aa324?source=collection_archive---------30-----------------------#2020-04-11">https://towardsdatascience.com/3-skills-to-master-before-reinforcement-learning-rl-4176508aa324?source=collection_archive---------30-----------------------#2020-04-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7ab4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">你需要能够训练神经网络，将搜索视为规划，并理解学术论文。</h2></div><p id="8aad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我在搜索强化学习的掌握时，通过所有的教程和课程，有一些他们认为你知道的常用工具。最终，你要应用的领域中的小弱点会变成大错误。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/4e1de42c9bd549a193cea493b03be423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3qmUx0Pg6IdVbHmh2msu7A.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">来源——作者在俄勒冈州本德的冒险经历。</p></figure><h1 id="8e93" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">1.监督学习</h1><p id="68d8" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">现代强化学习几乎完全专注于<strong class="kk iu">深度强化学习</strong>。短语<em class="mr">深度强化学习</em>中的单词<strong class="kk iu"> <em class="mr">深度</em> </strong>暗示了在算法的核心方面使用神经网络。神经网络在学习过程中进行一些高维近似。也就是说，模型不需要有许多层和特性，这是一个常见的误解，即深度意味着许多层。</p><p id="f380" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">几乎所有的课程和教程都会假设你<strong class="kk iu">能够微调简单的神经网络来逼近状态值或者创建最终的策略</strong>。这些模型在历史上对以下所有训练参数高度敏感:学习率、批量大小、模型参数、数据标准化等等。再加上难以解决的任务，调试RL可能会非常困难，就像一个二元的<em class="mr">它工作</em>或<em class="mr">它不工作</em>。通过了解所有的次近似都达到标准，消除混淆的尾巴。最好的方法是学习监督学习，然后让一个<a class="ae ms" href="https://www.automl.org/automl/" rel="noopener ugc nofollow" target="_blank"> AutoML </a>工具为你完成这项工作。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="41c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从一些<a class="ae ms" href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" rel="noopener ugc nofollow" target="_blank">代码</a>开始，破坏一切，然后重新构建。这样做几次，你应该很好去。<strong class="kk iu">监督学习</strong>也是一项技能，在未来十年里，它将转化到计算机科学的几乎所有领域，所以，努力追求它吧。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi na"><img src="../Images/a6218b23e2a606edcd17f5109dc7d288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vU7zMR6l3PThLvaQx9zETw.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">在使用现代人工智能系统时，你会经常看到这个草图。</p></figure><h1 id="43e3" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">2.人工智能中的搜索方法</h1><p id="5e2e" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">RL是策略空间中的搜索问题。近年来最具影响力的报纸一直从过去的搜索问题中获得严肃的灵感。让我们来看看最近RL最有影响力的三篇论文:</p><ol class=""><li id="cb9d" class="nb nc it kk b kl km ko kp kr nd kv ne kz nf ld ng nh ni nj bi translated">迄今为止最主流的强化学习成果:<a class="ae ms" href="https://deepmind.com/research/publications/investigation-model-free-planning" rel="noopener ugc nofollow" target="_blank"> Deepmind用at-scale强化学习对很多游戏的掌握</a>。最近的结果是使用规划网络来探索未来行动的长链。</li><li id="4d60" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">基于模型的RL的最新发展水平:<a class="ae ms" href="https://arxiv.org/abs/1906.08253" rel="noopener ugc nofollow" target="_blank">基于模型的策略优化</a> (MBPO)。MBPO正在状态-动作空间的附近区域进行搜索，以获得系统动力学的更完整的知识。这种模拟的知识就像一种肤浅的、面包优先的搜索。</li><li id="04ac" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">无模型RL中的艺术状态:软演员-评论家(SAC)。SAC以结合高效探索和高峰性能而闻名。它通过最大化策略上的熵项来做到这一点。直接策略搜索是RL作为搜索问题的最关键的方面。</li></ol><blockquote class="np nq nr"><p id="c789" class="ki kj mr kk b kl km ju kn ko kp jx kq ns ks kt ku nt kw kx ky nu la lb lc ld im bi translated">RL中的规划对于基于模型的<strong class="kk iu"><em class="it"/></strong>强化学习的子领域(相对较小，但正在增长)来说是一个巨大的胜利，该子领域试图学习离线规划的环境模型。</p></blockquote></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="58ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你问:我在哪里可以重温一下<strong class="kk iu">搜索的概念？</strong>这里是我学习的地方:<a class="ae ms" href="https://inst.eecs.berkeley.edu/~cs188/" rel="noopener ugc nofollow" target="_blank">加州大学伯克利分校的</a>和<a class="ae ms" href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/" rel="noopener ugc nofollow" target="_blank">麻省理工学院的</a>人工智能课程简介。然后做一两个代码项目，挑战自己。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nv"><img src="../Images/1ef79066e07c5c435ff8341dbe762444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3czEv5ZCSe8ZA0Mvs0oNGw.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">来源——我们在<a class="ae ms" href="https://inst.eecs.berkeley.edu/~cs188/sp20/assets/lecture/lec-4.pdf" rel="noopener ugc nofollow" target="_blank"> CS188 </a>的一次讲座。</p></figure><h1 id="208a" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">3.理解学术论文</h1><p id="f581" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">请注意，我没有说<em class="mr">阅读</em>学术论文。关键是要能够理解他们。</p><p id="7c7a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">学术领域变化很快，活跃的Twitter社区每天都有论文发布(说真的，关注几个著名的研究人员——这对让你跟上进度很有帮助)。你需要学习的是区分<strong class="kk iu">有影响力的论文</strong>和<em class="mr">噪音</em>以及<strong class="kk iu">进度</strong>和<em class="mr">炒作</em>。</p><p id="9a8c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在不是转发或引用的问题——尽管我承认引用是一个很好的替代——而是实质性的实验。机器学习的出版周期是粘性的，研究人员预计一年出版多次。你应该看的是实验:它们是遵循一个问题、实验、然后解决的弧线，还是只是逐步改进另一个人的方法？最好的论文是在寻找真理，而不是数字。</p><p id="e544" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最好的研究寻找最重要的问题，不管学术讨论的浪潮如何。学会辨别这些信号，你就能跟上这个领域，成为下一个十年有价值的人力资源。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nw"><img src="../Images/e21f4256d0d587e783f864c341a1072d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_B291Gip2-VQN6kZ-zqNWg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">来源，我在机器人学和ML方面的3篇论文</p></figure></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="bcd2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我已经写了很多在强化学习中建立基本技能的教程，见下面的大纲。</p><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/what-is-a-markov-decision-process-anyways-bdab65fd310c"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">到底什么是马尔可夫决策过程？</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">了解大多数强化学习问题中使用的模型。</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo lo oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/the-hidden-linear-algebra-of-reinforcement-learning-406efdf066a"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">强化学习的隐藏线性代数</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">线性代数的基础如何支持深度强化学习的顶点？</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="op l ol om on oj oo lo oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/fundamental-iterative-methods-of-reinforcement-learning-df8ff078652a"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">强化学习的基本迭代方法</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">学习价值和策略迭代能掌握多少强化学习？很多。</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="oq l ol om on oj oo lo oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">强化学习算法的收敛性</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">有什么简单的收敛界限吗？</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="or l ol om on oj oo lo oa"/></div></div></a></div><p id="9bc4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">继续学习！</p><p id="b0c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更多？订阅我关于机器人、人工智能和社会的时事通讯！</p><div class="nx ny gp gr nz oa"><a href="https://robotic.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">自动化大众化</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">一个关于机器人和人工智能的博客，让它们对每个人都有益，以及即将到来的自动化浪潮…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">robotic.substack.com</p></div></div><div class="oj l"><div class="os l ol om on oj oo lo oa"/></div></div></a></div></div></div>    
</body>
</html>