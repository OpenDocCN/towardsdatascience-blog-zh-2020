<html>
<head>
<title>Principal Component Analysis algorithm in Real-Life: Discovering patterns in a real-estate dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">现实生活中的主成分分析算法:发现房地产数据集中的模式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-algorithm-in-real-life-discovering-patterns-in-a-real-estate-dataset-18134c57ffe7?source=collection_archive---------4-----------------------#2020-06-03">https://towardsdatascience.com/principal-component-analysis-algorithm-in-real-life-discovering-patterns-in-a-real-estate-dataset-18134c57ffe7?source=collection_archive---------4-----------------------#2020-06-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7344" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在Python中使用PCA进行降维和模式可视化</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/154fff44e4d027f5c3b42852a189ec5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMKU2fCs8-qUwRfFYKiXwQ.png"/></div></div></figure><p id="6855" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">主成分分析，简称PCA，是一种无监督的学习技术，用于显示数据中的核心模式。</p><p id="2362" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本文中，我们将介绍PCA如何处理真实生活中的一个房地产经纪人的例子，他想了解为什么他们的一些房源需要很长时间才能关闭，以及我们如何使用PCA来编码一个更小的数据集。</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><p id="13ce" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你的朋友Maggie是一名房地产经纪人，她很困惑为什么她公司管理的一些房产六个多月都没有卖出去。她知道你是一名数据科学家，并问你是否能帮助她了解正在发生的事情。</p><p id="fa41" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你从做一些探索性的分析开始。您对数据进行全面检查，看看是否有缺失或不一致的值，然后就该在数据中寻找模式了。</p><p id="3a2a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Maggie想知道为什么这些房产这么长时间都卖不出去。她想知道数据中的特征和模式是什么，使得这些房产需要这么长时间才能售出。</p><p id="2711" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是，在一个庞大且具有大量属性(通常称为特征)的数据集中寻找模式是不可能手工完成的。</p><p id="c939" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">除此之外，您的数据集可能有几个冗余要素。您可能会怀疑有些功能是其他功能的细微变化。在某些情况下，一些特征可能是纯粹的噪声，不能揭示数据中的核心模式。</p><p id="2467" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">例如，您注意到在Maggie的数据集中有以下特征:</p><ul class=""><li id="a01b" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">一所房子的门的总数，</li><li id="c4f9" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">卧室总数，</li><li id="43ae" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">浴室总数。</li></ul><p id="db9c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">拥有一个关于门总数的特征似乎并不能揭示关于数据的一些关键信息，因为我们拥有另外两个特征。从统计学的角度来看，一所房子的门的总数很可能是一个多余的特征，并不能解释数据中的差异。</p><p id="bcb3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">好吧，没什么大不了的！如果一所房子的门的总数是一个多余的特征，我们可以从数据集中删除它。这在我们的情况下是有意义的，但是在具有数百甚至数千个特征的数据集中，我们需要更加谨慎。</p><p id="2ace" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在包含大量要素的数据集中，我们需要在移除某个要素之前确保它是冗余的或有噪声的。我们可能会在不知情的情况下删除关于数据的重要信息。</p><blockquote class="mi"><p id="9c80" class="mj mk iq bd ml mm mn mo mp mq mr lm dk translated">我们应该只移除我们确信不会影响数据模式或预测结果的特征。</p></blockquote><p id="f208" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated">要在这个数据集中找到模式，您会立即想到<a class="ae mx" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析</a>。</p><p id="a57b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">主成分分析(PCA)是一种无监督的学习方法，这意味着当我们在数据集中没有每个观察值的标签或目标时，可以使用它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/4ef115f2e4723d3db2b5d7fe8507cd3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AbO21312s6gygGdhistqJw.jpeg"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">监督与非监督学习的目标。</p></figure><p id="9427" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在有监督的学习方法中，我们必须为每个观察值设置一个标签，因为目标是预测一个类别或一个数字，而在无监督的学习中，目标是在数据中寻找模式。</p><p id="92a1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">所以，你可以帮助我们的朋友Maggie，使用主成分分析来找出需要更长时间才能出售的资产的核心特征。</p><h1 id="8ce6" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">PCA是如何工作的</h1><p id="09df" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">主成分分析的目标是对数据进行线性变换，从而最大限度地减少噪音和冗余。结果是数据集揭示了数据的最重要的特征。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/4261e861bd1170b719bb28f20e636243.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*IUlqdft1JKN-UqUtZS5_QQ.jpeg"/></div></figure><p id="ca12" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">应用PCA后，您将拥有一组主成分，按照它们对描述数据模式的贡献大小降序排列。用统计学的术语来说，它们是根据它们解释了多少差异来排名的。</p><p id="3e12" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">第一个主成分在描述数据的变化方面是最重要的。其余的主成分在表达数据模式的可变性方面不太重要。</p><p id="246c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在幕后，主成分分析使用统计工具来识别数据集中的噪声和冗余。它使用协方差矩阵来分析:</p><ul class=""><li id="283d" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">每个特征的方差。它会显示某个特征是相关的还是纯粹的噪声，我们可以在对角线单元格中读取。</li><li id="b06d" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">特征对之间线性关系的强度。这有助于发现冗余特征，并且它被读入所有非对角线值。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/301d210670e6f34db8c294429a9506f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DkRLlyL28g-Mn63OvGMu0Q.jpeg"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">解释协方差矩阵。</p></figure><p id="4b69" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">协方差矩阵突出显示了妨碍观察数据模式的东西。那么，在一天结束时，什么样的主成分分析将产生一组主成分:</p><ul class=""><li id="7462" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">通过最大化特征方差来减少噪声。</li><li id="1976" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">通过最小化要素对之间的协方差来减少冗余。</li></ul><p id="92af" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">PCA的基础是协方差矩阵，并且在实践中，有两种方法来识别主分量:</p><ul class=""><li id="d0bb" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">计算协方差矩阵的<a class="ae mx" href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" rel="noopener ugc nofollow" target="_blank">特征向量</a>。</li><li id="fb4d" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">计算协方差矩阵的<a class="ae mx" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">奇异值分解</a>。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><p id="2725" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在研究Maggie的数据之前，PCA要求我们准备好数据集。原始数据集如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/90a522b677c38203038c74058c67aaf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YPsMIL-Oz8q5EvkLpe6aZw.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">我们虚拟数据集的一部分😀</p></figure><p id="bb85" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在探索性分析阶段，我们还想创建一个<a class="ae mx" href="https://seaborn.pydata.org/generated/seaborn.pairplot.html" rel="noopener ugc nofollow" target="_blank"> pairplot </a>，并找出特征之间任何明显的相关性。但是，即使在这个只有17个要素的小虚拟数据集上，生成的pairplot也是巨大的，并且计算起来很慢。这是有道理的，因为我们必须选择2 = 136个单独的地块。</p><h1 id="ce84" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">特征标准化和缩放</h1><p id="df87" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">PCA的第一步是归一化数据，并确保所有特征呈正态分布。然后，数据将遵循标准正态分布，平均值等于零，标准偏差等于一。</p><p id="2d0a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对数据进行规范化会将每个要素转换为一个通用的表示形式，并使要素之间的所有值具有可比性。当要素具有不同的比例时，对其进行归一化尤为重要。例如，当一个要素以英里为单位，而另一个以分钟为单位时。</p><p id="f9d2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">标准化还保证主要成分是:</p><ul class=""><li id="b290" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">线性无关。每个主成分都是一个线性组合，不是由其他主成分组成的。</li><li id="0569" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">正交。这意味着所有的主分量彼此成90度角。</li></ul><p id="a12f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有几种方法可以标准化您的特征，通常称为<a class="ae mx" href="https://en.wikipedia.org/wiki/Feature_scaling" rel="noopener ugc nofollow" target="_blank">特征缩放</a>。其中之一是Z分数标准化，也称为标准化。</p><p id="04ab" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要对数据集应用Z值归一化，您需要更新每个数据点的值，以便:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/fcb579ce9d854076ce7deb3dc0787603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZuEgn8Kp0ZVsgjfhXm29ZA.png"/></div></div></figure><p id="bf4e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们走了这条弯路来讨论规范化，但是大多数统计库会为您做这件事。有了像<a class="ae mx" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> ScikitLearn </a>或<a class="ae mx" href="https://www.statsmodels.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> statsmodels </a>这样的Python库，你只需要设置一些参数。</p><p id="20de" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这个过程的最后，PCA会将你的特征编码成主成分。但是需要注意的是，主成分不一定与特征一一对应。它们是数据集的新表示形式，可以一次对多个要素进行编码。</p><h1 id="8888" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">查看协方差矩阵</h1><p id="e032" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">我们探索性分析的另一步是看一下协方差矩阵。PCA对数据进行归一化并创建协方差矩阵，但它对于可视化要素之间的线性关系非常有用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/39f1186078c2c6fcce8b52851c6ad03f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*67Gjo2_aedbBuaEBFbXCOw.jpeg"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">数据集的协方差矩阵。</p></figure><p id="85e2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要阅读这个矩阵，我们应该首先关注较暗的区域:</p><ul class=""><li id="5146" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">在非对角线上，成对的冗余特征。证实我们最初的预感<em class="of">卧室</em>和<em class="of">号_门</em>有97%的协方差。这意味着这两个特征之间有很强的线性关系。随着<em class="of">号门</em>增加一个单位，<em class="of">间卧室</em>增加一个单位，反之亦然。这对特征编码了相似的模式，所以我们可以认为它们是多余的。</li><li id="169c" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">在对角线上，我们看到所有的特征都有很高的方差。看起来所有的特征都是相关的，没有一个是纯粹的噪音。</li></ul><p id="ac99" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是如何绘制协方差矩阵。</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="ad9c" class="ol ne iq oh b gy om on l oo op">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="34bb" class="ol ne iq oh b gy oq on l oo op"># Load dataset<br/>dataset = pd.read_csv('src/dataset.csv')</span><span id="5c5a" class="ol ne iq oh b gy oq on l oo op">pca = PCA(dataset, standardize=True, method='eig')<br/>normalized_dataset = pca.transformed_data</span><span id="d493" class="ol ne iq oh b gy oq on l oo op"># Covariance Matrix<br/># bias =True, so dataset is normalized<br/># rowvar = False, each column represents a variable, i.e., a feature. This way we compute the covariance of features as whole instead of the covariance of each row</span><span id="fa26" class="ol ne iq oh b gy oq on l oo op">covariance_df = pd.DataFrame(data=np.cov(normalized_dataset, bias=True, rowvar=False), columns=dataset.columns)</span><span id="d74d" class="ol ne iq oh b gy oq on l oo op"># Plot Covariance Matrix<br/>plt.subplots(figsize=(20, 20))</span><span id="3c8c" class="ol ne iq oh b gy oq on l oo op">sns.heatmap(covariance_df, cmap='Blues', linewidths=.7, annot=True, fmt='.2f', yticklabels=dataset.columns)</span><span id="6284" class="ol ne iq oh b gy oq on l oo op">plt.show()</span></pre><h1 id="4269" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">Python中的PCA</h1><p id="53bb" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">我们将使用Python库<a class="ae mx" href="https://www.statsmodels.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> statsmodels </a>来执行PCA。你也可以使用<a class="ae mx" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> ScikitLearn </a>，但是<em class="of"> statsmodels </em>提供了更多的灵活性。例如，你可以选择是使用特征向量还是奇异值分解。</p><p id="0e40" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，在本例中，我们将在<a class="ae mx" href="https://www.statsmodels.org/stable/generated/statsmodels.multivariate.pca.PCA.html" rel="noopener ugc nofollow" target="_blank"> statsmodel PCA方法</a>中设置以下标志:</p><ul class=""><li id="455e" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated"><em class="of">标准化</em>设置为True，所以我们用均值0和方差1来标准化数据。</li><li id="9e24" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated"><em class="of">方法</em>设置为<em class="of"> eig </em>，那么协方差矩阵的特征向量就成为我们的主成分。</li></ul><p id="a454" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将让参数<em class="of"> ncomp </em>不设置，因此PCA返回所有主成分，即与特征的数量一样多。</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="67fd" class="ol ne iq oh b gy om on l oo op">import pandas as pd<br/>from sklearn import preprocessing</span><span id="a776" class="ol ne iq oh b gy oq on l oo op"># Load dataset<br/>dataset = pd.read_csv('dataset.csv')</span><span id="df33" class="ol ne iq oh b gy oq on l oo op"># Run PCA<br/>pca = PCA(dataset, standardize=True, method='eig')</span><span id="a3b9" class="ol ne iq oh b gy oq on l oo op">components_df = pca.factors</span></pre><h1 id="75b4" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">解释PCA的结果</h1><p id="4628" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">所以，<em class="of"> components_df </em>是一个包含所有主分量的数组，即协方差矩阵的特征向量。</p><p id="01af" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是这本身并不能告诉我们太多的数据。我们需要找到一种方法来解释这些结果，并将其与我们的原始数据集联系起来。</p><p id="0735" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将首先计算原始数据集中每个主成分和每个要素之间的相关性。这里的关键是将主成分和特征作为一个整体进行成对关联，而不是在单个值之间进行关联。</p><p id="eeee" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果我们把相关矩阵可视化就更容易了。在颜色渐变的顶部，我们将稍微调整一下图形:</p><ul class=""><li id="0891" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">在单元格之间创建一些空间，为了更好的可读性和美观，线宽= 0.7。</li><li id="68db" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">用<em class="of"> annot=True将相关值添加到每个单元格。</em>这样更容易发现单元格数值之间的差异。</li><li id="8223" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">用<em class="of"> fmt='.2f' </em>截断每个单元格中的值，使其具有两位有效数字，以提高可读性。</li></ul><p id="e0c3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们有一个相关矩阵😀</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/77364b92d02f0b96205cc3eb03b72652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5CVUyQoG4ct6UDM7dmy2jQ.jpeg"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">主成分和原始数据集特征的相关矩阵。</p></figure><p id="aa01" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以立即看到与第一主成分<em class="of"> comp_00相关的最高相关值。</em></p><p id="6536" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">主成分是根据它们编码的数据集中的可变性大小来排序的。因此，第一主成分与数据集特征具有更高的相关性是有意义的。</p><p id="a0e3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">与第一主成分相关联的这些特征与确定数据中的模式最相关。</p><p id="37f9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们还看到特征和主成分之间没有一对一的关系。第一主成分与几个特征有很强的正相关性。</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="51ba" class="ol ne iq oh b gy om on l oo op">import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="a843" class="ol ne iq oh b gy oq on l oo op"># Load dataset<br/>dataset = pd.read_csv('dataset.csv')</span><span id="1b9c" class="ol ne iq oh b gy oq on l oo op"># Run PCA<br/>pca = PCA(dataset, standardize=True, method='eig')<br/>components_df = pca.factors</span><span id="c9fc" class="ol ne iq oh b gy oq on l oo op">combined_df = pd.concat([dataset, components_df], axis=1)<br/>correlation = combined_df.corr()<br/></span><span id="cc41" class="ol ne iq oh b gy oq on l oo op"># This matrix will have the correlation between:<br/># 1. feature vs features<br/># 2. feature vs principal component<br/># 3. principal component vs principal component<br/># We're removing part of the output to keep only the correlation between features and principal components</span><span id="2c29" class="ol ne iq oh b gy oq on l oo op">correlation_plot_data = correlation[:-len(components_df.columns)].loc[:, 'comp_00':]</span><span id="ab9e" class="ol ne iq oh b gy oq on l oo op"><br/># plot correlation matrix<br/>fig, ax = plt.subplots(figsize=(20, 7))<br/>sns.heatmap(correlation, cmap='YlGnBu', linewidths=.7, annot=True, fmt='.2f')<br/>plt.show()</span></pre><h2 id="bccc" class="ol ne iq bd nf os ot dn nj ou ov dp nn la ow ox np le oy oz nr li pa pb nt pc bi translated">PCA如何帮助Maggie</h2><p id="5e0e" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">相关矩阵显示了哪些特征与第一主成分相关，因此哪些特征将一起改变。一个很好的例子是，由于第一主成分和<em class="of">卧室</em>和<em class="of">平方英尺</em>之间存在正相关，我们可以说，当你增加卧室或浴室的数量时，平方英尺往往会增加。</p><p id="7f1e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是也有一些与第一主成分负相关，例如与<em class="of">交通可达性、靠近市中心</em>和<em class="of">超市靠近度</em>负相关。这表明，更大的房子，有更大的<em class="of">面积</em>和更多的<em class="of">卧室</em>、<em class="of">、</em>将倾向于远离市中心、超市和更低的交通可达性。</p><p id="3518" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由于Maggie正在分析超过6个月仍未售出的房屋的特征，她可以研究这些负相关性，以找到这些房屋为何长期未售出的线索。</p><h1 id="3fb0" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">权衡:失去可解释性</h1><p id="2efa" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">第一主成分与数据集中17个特征中的11个具有强正相关。这意味着它编码或代表了这些特征的模式。</p><p id="b563" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这很棒，因为我们现在可以用更少的数据表达更多的信息。这里的权衡是，我们失去了可解释性。</p><p id="f58f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当我们绘制彼此相对的特征时，更容易理解这些模式。但是由于主成分编码了几个特征，当我们绘制它们时，就不清楚了:</p><ul class=""><li id="1eec" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">每个主成分编码什么特征，</li><li id="ef6c" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">每个特征的比例对我们看到的模式有更大的影响。</li></ul><p id="aaa3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了理解每个特征对主成分的贡献，我们需要查看每个主成分的<em class="of">负载</em>。</p><p id="432e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">负载是应用于线性组合中的每个特征的权重，其导致主成分的得分。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/70e2dd3a2da6eb933e2c4de074533442.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*s9fp3yt9-MIDszPpLo6UDg.jpeg"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">前两个主要成分的载荷。</p></figure><p id="c92b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">只看前两个主成分，我们可以看到:</p><ul class=""><li id="92ef" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated"><em class="of">卧室</em>和<em class="of">数量_门</em>在第一主成分中捕获更多信息。</li><li id="a8ad" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated"><em class="of">硬木地板</em>和<em class="of">城市中心</em>在第二主成分中捕获了更多的信息。</li></ul><p id="1893" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">以下是检查装载的方法。</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="869d" class="ol ne iq oh b gy om on l oo op">import pandas as pd<br/>from statsmodels.multivariate.pca import PCA</span><span id="d258" class="ol ne iq oh b gy oq on l oo op"># Load dataset<br/>dataset = pd.read_csv('dataset.csv')</span><span id="ce6c" class="ol ne iq oh b gy oq on l oo op">pca = PCA(dataset, standardize=True, method='eig')<br/>loadings = pca.loadings<br/>print(loadings)</span></pre><h1 id="985f" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">降维</h1><p id="3c6e" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">PCA最受欢迎的应用之一是降维。这是一种创建数据集新表示的方式，即:</p><ul class=""><li id="cfd2" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">小于原始数据集，</li><li id="dcbc" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">仅保留其核心模式和特征。</li></ul><p id="4e66" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">选择主成分来表示较小版本的数据集本身就是一个复杂的主题。之所以叫<a class="ae mx" href="https://en.wikipedia.org/wiki/Factor_analysis" rel="noopener ugc nofollow" target="_blank">因子分析</a>，是因为我们也可以把组件称为因子。</p><p id="12f0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">挑选因素的传统方法包括:</p><ul class=""><li id="ecb7" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">凯泽准则</li><li id="6608" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">解释方差</li><li id="fcd6" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">碎石图</li></ul><h2 id="e478" class="ol ne iq bd nf os ot dn nj ou ov dp nn la ow ox np le oy oz nr li pa pb nt pc bi translated">凯泽准则</h2><p id="1f99" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">根据Kaiser准则，我们只选择特征值大于1的主成分。每个特征值与一个特征向量相关联，该特征向量实际上是一个主分量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/10b78995529edf0352b6a7bbbdd63c29.png" data-original-src="https://miro.medium.com/v2/resize:fit:252/format:webp/1*fLDME1tBBfdOoVTmbVd9vw.jpeg"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">凯泽标准适用于我们的结果。</p></figure><p id="984b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果我们使用这种方法，我们将排除最后三个主要成分。</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="b8ce" class="ol ne iq oh b gy om on l oo op">import pandas as pd<br/>from statsmodels.multivariate.pca import PCA<br/></span><span id="921d" class="ol ne iq oh b gy oq on l oo op"># Load dataset<br/>dataset = pd.read_csv('src/dataset.csv')</span><span id="2f99" class="ol ne iq oh b gy oq on l oo op">pca = PCA(dataset, standardize=True, method='eig')<br/>eigen_values = pd.DataFrame(data=pca.eigenvals.values, columns=['eigenvalue'])</span><span id="5cfb" class="ol ne iq oh b gy oq on l oo op">print(eigen_values)</span></pre><h1 id="8ed3" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">解释方差</h1><p id="38d9" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">使用这种方法，您可以选择想要主成分编码的数据集中的总方差。通常这些临界值是80%或90%。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/962240d34e8f2f686748b004fb119aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*mcKaNOYXEGy_G6Ab-M6IpA.jpeg"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">主成分解释的累积方差。</p></figure><p id="41ed" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果我们使用这种方法，我们有:</p><ul class=""><li id="6f1e" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">前六个主成分解释了80%多一点的方差，</li><li id="04cf" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">前九个主成分可以大致解释我们数据集中90%的差异。</li></ul><p id="bc32" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们会使用前六个或前九个主成分，这取决于我们建立的临界值。</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="4d13" class="ol ne iq oh b gy om on l oo op">import pandas as pd<br/>from statsmodels.multivariate.pca import PCA</span><span id="be69" class="ol ne iq oh b gy oq on l oo op"># Load dataset<br/>dataset = pd.read_csv('src/dataset.csv')</span><span id="9c41" class="ol ne iq oh b gy oq on l oo op">pca = PCA(dataset, standardize=True, method='eig')</span><span id="9e0c" class="ol ne iq oh b gy oq on l oo op"># Cumulative Variance Explained</span><span id="92bc" class="ol ne iq oh b gy oq on l oo op">cumulative_variance_explained = pd.DataFrame(data=pca.rsquare.values, columns=['cumulative_var'])</span><span id="1265" class="ol ne iq oh b gy oq on l oo op">print(cumulative_variance_explained)</span></pre><p id="4633" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在统计学中，<a class="ae mx" href="https://en.wikipedia.org/wiki/Coefficient_of_determination" rel="noopener ugc nofollow" target="_blank"> R平方</a>是用于确定一个变量的方差有多少是由另一个变量解释的度量。因此，PCA模型的这个属性的名称。</p><h1 id="6d4a" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">碎石图</h1><p id="180b" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">有了<a class="ae mx" href="https://en.wikipedia.org/wiki/Scree_plot" rel="noopener ugc nofollow" target="_blank"> scree plot </a>，我们可以更直观的方式挑选组件。我们绘制每个组件的特征值，并使用<a class="ae mx" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank">肘方法</a>来确定我们的分界点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/9b7fd44da3968a2d147b0b83946d3fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*89oukoEGXrwUNlfooFJekg.jpeg"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">在碎石块中寻找弯头。</p></figure><p id="9e88" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这个想法是沿着减少的特征值，看看这些值在哪里形成一个弯头。我们只包括导致拐点的主分量。</p><p id="6ce2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是一个不太精确的方法，因为它取决于每个人看手肘的角度。</p><pre class="kg kh ki kj gt og oh oi oj aw ok bi"><span id="bad9" class="ol ne iq oh b gy om on l oo op">import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from statsmodels.multivariate.pca import PCA</span><span id="6545" class="ol ne iq oh b gy oq on l oo op"># Load dataset<br/>dataset = pd.read_csv('src/dataset.csv')</span><span id="6f5d" class="ol ne iq oh b gy oq on l oo op">pca = PCA(dataset, standardize=True, method='eig')</span><span id="d5af" class="ol ne iq oh b gy oq on l oo op">plt.subplots(figsize=(10, 10))<br/>pca.plot_scree()<br/>plt.show()</span></pre><h1 id="f14d" class="nd ne iq bd nf ng nh ni nj nk nl nm nn jw no jx np jz nq ka nr kc ns kd nt nu bi translated">结论</h1><p id="ecbe" class="pw-post-body-paragraph kr ks iq kt b ku nv jr kw kx nw ju kz la nx lc ld le ny lg lh li nz lk ll lm ij bi translated">希望你喜欢这个例子。借助这个随机生成的数据集，我们经历了PCA的每个步骤，涵盖了:</p><ul class=""><li id="e485" class="lu lv iq kt b ku kv kx ky la lw le lx li ly lm lz ma mb mc bi translated">PCA的工作原理。</li><li id="da28" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">什么是主成分以及它们与原始数据集的关系。</li><li id="ed65" class="lu lv iq kt b ku md kx me la mf le mg li mh lm lz ma mb mc bi translated">如何决定保留哪些主成分，以防我们想要<em class="of">缩减</em>我们的数据集。</li></ul><p id="80e5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="of">感谢阅读！</em></p></div></div>    
</body>
</html>