<html>
<head>
<title>Ensembling HuggingFaceTransformer models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">组装 HuggingFaceTransformer 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ensembling-huggingfacetransformer-models-f21c260dbb09?source=collection_archive---------23-----------------------#2020-07-06">https://towardsdatascience.com/ensembling-huggingfacetransformer-models-f21c260dbb09?source=collection_archive---------23-----------------------#2020-07-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9f5dcbfc8939016f01bb2c4ee7c651de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sj62K8B_7rdEKlqkyfWP8Q.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@jeetdhanoa" rel="noopener ugc nofollow" target="_blank"> @jeetdhanoa </a>在<a class="ae jg" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> unsplash </a>上拍摄</p></figure><div class=""/><div class=""><h2 id="c8c7" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">使用一个简单的线性层将两个或更多的变形金刚组合在一起。</h2></div><p id="0846" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最近，当我使用 BERT 进行问答研究时，有人建议我集成两个 BERT 模型。我选择了一条显而易见的路线——谷歌搜索。但令我惊讶的是，什么都没有出现。有很多关于变形金刚的文章，但是没有关于如何组合变形金刚模型的。本文讨论的正是这个问题——如何集成两个 PyTorch 拥抱脸变压器模型。</p><p id="224b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">什么是模型组装？</strong> <br/>在很多情况下，一个单一的模型可能不会给出最好的结果。然而，如果我们将“几个弱”分类器组合在一起，并以一种有意义的方式组合这些弱分类器的结果，我们可能会得到更好的结果。</p><p id="0748" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，假设我们正在使用 BERT 进行问答—如果我们将两个句子<em class="lu">send _ 1</em>和<em class="lu">send _ 2</em>传递给 BERT 模型，该模型应该预测这两个句子是否形成问答对，即<em class="lu">send _ 2</em>answers<em class="lu">send _ 1</em>。</p><p id="8bc1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通常的技术是以两种方式向模型提供问答对—<br/><em class="lu">【CLS】+sent _ 1+【SEP】+sent _ 2+【SEP】</em>和<br/><em class="lu">【CLS】+sent _ 2+【SEP】+sent _ 1+【SEP】</em><br/>到<em class="lu">一个单一模型</em>，并训练该模型。</p><p id="8641" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们宁愿<em class="lu">集合</em> 2 个模型——以如下方式训练 2 个模型，而不是这样做:<br/> <strong class="la jk">模型 1 </strong>获得输入，<br/><em class="lu">【CLS】+sent _ 1+[SEP]+sent _ 2+[SEP]</em>，<br/> <strong class="la jk">模型 2 </strong>获得输入，<br/><em class="lu">【CLS】+sent _ 2+[SEP]+sent _ 1+[SEP]</em></p><p id="ee11" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，使用一个简单的前馈网络，我们可以结合模型的结果(不要在如何做上敲你的头，只是假设它现在可以做。我将展示如何做到这一点，这是本文的关键)。</p><ol class=""><li id="f7c4" class="lv lw jj la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated"><strong class="la jk">为什么组装更好？<br/> </strong> <em class="lu">嗯，这不！至少不总是这样。</em>在某些情况下，集成的结果可能优于单个大模型。这是可行的，因为手头的任务可能太复杂，一个单一的模型无法理解。<br/>这与一个有 100 个神经元的深度 CNN 比一个有 1 层和 100 个神经元的 CNN 工作得更好的原因是一样的——每一层都学到不同的东西，使整个模型变得更好。这和一个人比一头狮子弱是一个道理，但是作为一个社会，我们是地球上最优越的物种。这和 5 个我比 1 个梅西强是一个道理(至少我昨天是这么想的😆).这和悟吉塔比布罗利强是一个道理。但是最主要的原因是他们没有过度适应。</li></ol><h1 id="645b" class="me mf jj bd mg mh mi mj mk ml mm mn mo kp mp kq mq ks mr kt ms kv mt kw mu mv bi translated">单一模型方法</h1><p id="d924" class="pw-post-body-paragraph ky kz jj la b lb mw kk ld le mx kn lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">让我们回到问答的例子。让我们考虑 3 个句子，</p><p id="6b90" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">句子 _1 </em> =谁杀了弗里扎？<br/> <em class="lu">句子 _2 </em> =弗里扎被小悟空杀死<br/> <em class="lu">句子 _3 </em> =弗里扎消灭了玛雅人却放过了玛雅人。</p><p id="255d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们馈入模型，<br/> <strong class="la jk">输入</strong>:<em class="lu">【CLS】+句子 _ 1+【SEP】+句子 _ 2+【SEP】</em><br/><strong class="la jk">输出</strong> : 1 <br/> <strong class="la jk">输入</strong>:<em class="lu">【CLS】+句子 _ 1+【SEP】+句子 _ 3+【SEP】</em><br/><strong class="la jk">输出</strong> : 0</p><p id="0491" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如前所述，如果我们有一个单一的模型，那么训练模型的正常方式是，<br/><em class="lu">【CLS】+问题+【SEP】+答案+【SEP】<br/>【CLS】+答案+【SEP】+问题+【SEP】</em><br/>具体到我们的情况，<br/><em class="lu">【CLS】+句子 _ 1+【SEP】+句子 _ 2+【SEP】<br/>【CLS】+句子 _ 2+【SEP】+句子 _ 1+【SEP】<br/></em></p><h1 id="f497" class="me mf jj bd mg mh mi mj mk ml mm mn mo kp mp kq mq ks mr kt ms kv mt kw mu mv bi translated">变形金刚合奏</h1><p id="85f5" class="pw-post-body-paragraph ky kz jj la b lb mw kk ld le mx kn lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">在这种方法中，我们有两种模式</p><ol class=""><li id="1f42" class="lv lw jj la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">模型 1 <br/> <strong class="la jk">输入</strong>:<em class="lu">【CLS】+提问+【SEP】+回答+【SEP】</em></li><li id="0324" class="lv lw jj la b lb nb le nc lh nd ll ne lp nf lt ma mb mc md bi translated">模型 2 <br/> <strong class="la jk">输入</strong>:<em class="lu">【CLS】+回答+【九月】+提问+【九月】</em></li></ol><p id="7eb8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在的问题变成了如何将两个模型的输出合并成一个，即系综？</p><p id="4243" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">嗯，空谈是廉价的，所以让我们编码。由于代码并不便宜，我将尝试解释它的大部分。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9dfd3d336ad8822df365e74a592b1bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-1MJVZ0C4igUwHW2L28Eg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><em class="nk">作者提供的图片(使用 Paint S 创建)</em></p></figure><h1 id="15d2" class="me mf jj bd mg mh mi mj mk ml mm mn mo kp mp kq mq ks mr kt ms kv mt kw mu mv bi translated">代码</h1><figure class="ng nh ni nj gt iv"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="fa9f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我创建了一个名为<em class="lu">BertEnsembleForNextSentencePrediction 的新模型。</em></p><pre class="ng nh ni nj gt nn no np nq aw nr bi"><span id="3e44" class="ns mf jj no b gy nt nu l nv nw">self.bert_model_1 = BertModel(config)        <br/>self.bert_model_2 = BertModel(config)         <br/>self.cls = nn.Linear(self.n_models * self.config.hidden_size, 2)</span></pre><p id="c5cb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">BertEnsembleForNextSentencePrediction</em>以 2 个<em class="lu"> BertModel </em>作为输入(在<em class="lu"> __init__ </em>中可以看到)并增加一个<em class="lu"> nn。线性</em>在它们上面。<em class="lu"> nn。线性，</em>如前面提到的<em class="lu"> </em> <a class="ae jg" href="https://pytorch.org/docs/master/generated/torch.nn.Linear.html" rel="noopener ugc nofollow" target="_blank"> <em class="lu">这里的</em> </a> <em class="lu">，</em>应用线性变换就像:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6326bf94e9188f4af2622fb5b2b75c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*XRY1HoQpRT_FxnYwihez8A.png"/></div></figure><p id="b4ed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在输入端。我来解释一下为什么<em class="lu"> nn。使用了线性</em>(尽管任何看过一点变形金刚代码的人应该能够立即看到许多<em class="lu"> nn。线性</em>如<em class="lu">bertonlynshead，BertForSequenceClassification，</em>等。).</p><pre class="ng nh ni nj gt nn no np nq aw nr bi"><span id="9949" class="ns mf jj no b gy nt nu l nv nw">input_ids_1 = input_ids[0]        <br/>attention_mask_1 = attention_mask[0]        <br/>token_type_ids_1 = token_type_ids[0]</span><span id="cb2c" class="ns mf jj no b gy ny nu l nv nw">input_ids_2 = input_ids[1]        <br/>attention_mask_2 = attention_mask[1]        <br/>token_type_ids_2 = token_type_ids[1]</span></pre><p id="42b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来是主<em class="lu">前进</em>功能。转发函数的参数<em class="lu"> input_ids、attention_mask、token_type_ids </em>为元组，第 0 个索引用于第一个模型，第 1 个索引用于第二个模型。因此，第一个模型将 T28 输入 _ 标识[0]，注意 _ 屏蔽[0]，令牌 _ 类型 _ 标识[0]作为输入。(我不会详细说明这些术语的含义，因为它们是标准的 BERT 术语)。以上 6 行正是这么做的。</p><pre class="ng nh ni nj gt nn no np nq aw nr bi"><span id="840d" class="ns mf jj no b gy nt nu l nv nw">outputs.append(self.bert_model_1(input_ids_1, attention_mask=attention_mask_1, token_type_ids=token_type_ids_1))</span><span id="24c9" class="ns mf jj no b gy ny nu l nv nw">outputs.append(self.bert_model_2(input_ids_2, attention_mask=attention_mask_2, token_type_ids=token_type_ids_2))</span></pre><p id="c5db" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们将上面的左侧变量，即<em class="lu">输入 _ 标识 _1、注意 _ 屏蔽 _1、令牌 _ 类型 _ 标识 _1、输入 _ 标识 _2、注意 _ 屏蔽 _2、令牌 _ 类型 _ 标识 _2 </em>传递给 BERT 模型— <em class="lu"> BertModel </em>。</p><p id="93c8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如<a class="ae jg" href="https://huggingface.co/transformers/model_doc/bert.html#bertmodel" rel="noopener ugc nofollow" target="_blank">此处</a>所示，<em class="lu"> BertModel </em>返回<em class="lu"> last_hidden_state </em>和<em class="lu"> pooler_output </em>作为前两个输出。我们对这里的<em class="lu">池 _ 输出</em>感兴趣。这里所说的<a class="ae jg" href="https://huggingface.co/transformers/model_doc/bert.html#bertmodel" rel="noopener ugc nofollow" target="_blank">和</a>中的<em class="lu"> pooler_output </em>是</p><blockquote class="nz oa ob"><p id="8757" class="ky kz lu la b lb lc kk ld le lf kn lg oc li lj lk od lm ln lo oe lq lr ls lt im bi translated">L <!-- -->由线性层和双曲正切激活函数进一步处理的序列的第一个标记(分类标记)的 ast 层隐藏状态。在预训练期间，根据下一句预测(分类)目标来训练线性层权重。</p></blockquote><p id="db93" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于给定的输入，<em class="lu"> pooler_output </em>的大小为(<em class="lu"> batch_size，hidden_size </em>)。默认情况下<em class="lu"> hidden_size = 768。</em></p><pre class="ng nh ni nj gt nn no np nq aw nr bi"><span id="9018" class="ns mf jj no b gy nt nu l nv nw">last_hidden_states = torch.cat([output[1] for output in outputs], dim=1) <br/>logits = self.cls(last_hidden_states)</span></pre><p id="6081" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在进入使用<em class="lu"> nn.Linear. </em>的主要组合部分，如何组合模型 1 和模型 2 的输出？很简单——每个模型中的<em class="lu">【CLS】</em>令牌都是大小(<em class="lu"> batch_size * 768 </em>)。所以基本上对于每个问答配对，我们都有一个大小为 768 的向量。因此，对于每个给定的问答配对，将有分别从 2 个模型中的每一个生成的每个大小为 768 的 2 个向量。比如<br/><em class="lu">【cls 1】+谁杀了 Freeza？+ [SEP] +弗里扎被悟空+[SEP]</em><br/><em class="lu">【cls 2】+弗里扎被悟空+ [SEP] +谁杀了弗里扎？+ [SEP] </em></p><p id="97e6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu"> nn。线性</em>接受<em class="lu">输出</em>数组元素的串联，即展平<em class="lu">输出</em>数组并执行线性运算<em class="lu">。</em>因此，线性层将大小为(2 * 768)的向量作为输入，并输出 0 或 1 的概率，即<em class="lu">逻辑值</em>(逻辑值并不完全是概率，但如果我们对逻辑值应用 softmax，我们会得到概率，所以它足够接近)。</p><p id="5aef" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，线性层将大小为(2 * 768)的向量作为输入，并输出 0 或 1。</p><p id="7f28" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个完整的工作示例。<br/><a class="ae jg" href="https://colab.research.google.com/drive/1SyRrBAudJHiKjHnxXaZT5w_ukA0BmK9X?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/1 syrrbaudjhikjhnxxazt 5 w _ uka 0 bmk 9 x？usp =分享</a></p><p id="b1c9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，代码中使用的数据集非常小，代码的编写方式对于这样小的数据集来说是一个巨大的破坏。但是我编写代码的方式使得任何更大的数据集都可以使用。此外，PyTorch 的通用模式已用于编写代码。我发现保存这个模板并根据用例稍微调整一下代码很有用。</p><p id="dbb9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就是这样！</p><h1 id="3dea" class="me mf jj bd mg mh mi mj mk ml mm mn mo kp mp kq mq ks mr kt ms kv mt kw mu mv bi translated">参考和链接:</h1><ol class=""><li id="110f" class="lv lw jj la b lb mw le mx lh of ll og lp oh lt ma mb mc md bi translated">为什么集合更好:<a class="ae jg" href="https://www.quora.com/How-do-ensemble-methods-work-and-why-are-they-superior-to-individual-models" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/How-do-ensemble-methods-work-and-why-be-superior-to-individual-models</a></li><li id="bc10" class="lv lw jj la b lb nb le nc lh nd ll ne lp nf lt ma mb mc md bi translated">PyTorch <em class="lu"> nn。线性</em>:<a class="ae jg" href="https://pytorch.org/docs/master/generated/torch.nn.Linear.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/docs/master/generated/torch . nn . linear . html</a></li></ol></div></div>    
</body>
</html>