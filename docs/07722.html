<html>
<head>
<title>The Simple Approach to Word Embedding for Natural Language Processing using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现自然语言处理中单词嵌入的简单方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-simple-approach-to-word-embedding-for-natural-language-processing-using-python-ae028c8dbfd2?source=collection_archive---------30-----------------------#2020-06-09">https://towardsdatascience.com/the-simple-approach-to-word-embedding-for-natural-language-processing-using-python-ae028c8dbfd2?source=collection_archive---------30-----------------------#2020-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dc64" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Gensim 和 Plotly 探索单词嵌入</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7a3c7fcabc8943da577ff5a0ee28aa6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0UCSgw-eLn9mdickV9XkrQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.needpix.com/photo/336813/word-cloud-words-tag-cloud-tagcloud-wordcloud-happy-optimistic-satisfied" rel="noopener ugc nofollow" target="_blank">画面一</a>、<a class="ae ky" href="https://pixabay.com/photos/model-posture-legs-bodysuit-cube-4689824/" rel="noopener ugc nofollow" target="_blank">画面二</a></p></figure><h1 id="00ea" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">学习 NLP 的下一步？</h1><p id="34ea" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当头脑风暴研究新的数据科学主题时，我总是倾向于自然语言处理(NLP)。这是一个快速发展的数据科学领域，不断有创新需要探索，我喜欢分析写作和修辞。NLP 自然符合我的兴趣！之前，我写了一篇关于使用“单词袋”模型开始 NLP 的<a class="ae ky" rel="noopener" target="_blank" href="/3-super-simple-projects-to-learn-natural-language-processing-using-python-8ef74c757cd9">简单项目的文章。本文旨在通过探索使用 Word2Vec 通过 Python Gensim 库生成单词嵌入的快捷方式，从而超越简单的“单词包”方法。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/18eb8183745dfe49b7aa7e6efb45d71e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*Xz6333CoNEjNXn8NvjD0Xg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank">https://radimrehurek.com/gensim/</a></p></figure><h1 id="0baf" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">传统单词包 vs Word2Vec</h1><p id="1f09" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当我开始探索自然语言处理(NLP)时，我了解的第一个模型是简单的单词袋模型。尽管它们非常有效，但也有局限性。</p><h2 id="abd1" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">传统的词汇袋方法</h2><p id="e317" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">单词包(BoW) </strong>是描述文本语料库中<strong class="lt iu">单词</strong>出现的文本表示，但不解释单词的顺序。这意味着它对待所有的单词都是相互独立的，因此得名单词包。</p><p id="61a8" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">BoW 由一组单词(词汇)和一个类似于<em class="nf">频率</em>或<em class="nf"> TF-IDF </em>的指标组成，用来描述每个单词在语料库<em class="nf">中的值。</em>这意味着如果词汇表非常大，BoW 会导致稀疏矩阵和高维向量，这会消耗大量计算机资源。</p><p id="0e3b" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">为了简化弓向量化的概念，假设您有两个句子:</p><blockquote class="ng nh ni"><p id="f47f" class="lr ls nf lt b lu na ju lw lx nb jx lz nj nc mc md nk nd mg mh nl ne mk ml mm im bi translated"><em class="it">狗是白色的<br/>猫是黑色的</em></p></blockquote><p id="412b" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">将句子转换为向量空间模型会以这样的方式转换它们，即查看所有句子中的单词，然后用数字表示句子中的单词。如果句子是一次性编码的:</p><blockquote class="ng nh ni"><p id="6a61" class="lr ls nf lt b lu na ju lw lx nb jx lz nj nc mc md nk nd mg mh nl ne mk ml mm im bi translated"><em class="it">狗是白色的猫是黑色的<br/>狗是白色的= [1，1，0，1，1，0] <br/>猫是黑色的= [1，0，1，1，0，1] </em></p></blockquote><p id="6aa3" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">BoW 方法有效地将文本转换成用于机器学习的固定长度向量。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="3263" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">Word2Vec 方法</h2><p id="8a56" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">由谷歌的一组研究人员开发的 Word2Vec 试图解决 BoW 方法的几个问题:</p><ul class=""><li id="7ffe" class="nt nu it lt b lu na lx nb ma nv me nw mi nx mm ny nz oa ob bi translated">高维向量</li><li id="141e" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ny nz oa ob bi translated">假设彼此完全独立的词</li></ul><p id="82e4" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">使用只有几层的神经网络，Word2Vec 试图学习单词之间的关系，并将它们嵌入到低维向量空间中。为此，Word2vec 根据输入语料库中与其相邻的其他单词来训练单词，从而捕获单词序列中的一些含义。研究人员设计了两种新方法:</p><ul class=""><li id="0df9" class="nt nu it lt b lu na lx nb ma nv me nw mi nx mm ny nz oa ob bi translated">连续的单词袋</li><li id="73be" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ny nz oa ob bi translated">跳跃图</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/618f519b0c650d78799a9b48266b53c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*I47s74ecqoX9nSJFh2-KYQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">CBOW 架构基于上下文预测当前单词，而 Skip-gram 根据当前单词预测周围的单词。</a></p></figure><p id="0d8c" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这两种方法都会产生一个向量空间，该向量空间基于上下文含义将单词向量紧密地映射在一起。这意味着，如果两个单词向量很接近，那么基于它们在语料库中的上下文，这些单词应该具有相似的含义。例如，使用余弦相似性来分析他们的数据产生的向量，研究人员能够构建类似于<strong class="lt iu">国王</strong>减去<strong class="lt iu">男人</strong>加上<strong class="lt iu">女人</strong> =？</p><p id="8058" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">最匹配<strong class="lt iu">皇后</strong>的输出向量。</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="e01d" class="mo la it oj b gy on oo l op oq">king - man + woman = queen</span></pre><p id="5a4f" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">如果这看起来令人困惑，不要担心。应用和探索 Word2Vec 很简单，随着我对示例的深入，它会变得更有意义！</p><h1 id="3619" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">依赖性和数据</h1><p id="4e89" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Python 库<a class="ae ky" href="https://radimrehurek.com/gensim/index.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>使得应用 Word2Vec 以及其他几种算法来进行主题建模变得很容易。Gensim 是免费的，可以使用 Pip 或 Conda 安装:</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="df12" class="mo la it oj b gy on oo l op oq">pip install --upgrade gensim</span><span id="c2ad" class="mo la it oj b gy or oo l op oq">OR</span><span id="8392" class="mo la it oj b gy or oo l op oq">conda install -c conda-forge gensim</span></pre><p id="9858" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><strong class="lt iu">数据和所有代码可以在我的 GitHub 中找到，并且是与前一篇文章中使用的垃圾邮件数据集相同的 repo。</strong></p><div class="os ot gp gr ou ov"><a href="https://github.com/bendgame/nlpBeginnerProjects" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">bendgame/nlpBeginnerProjects</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">NLP 中级入门项目。通过创建帐户，为 bendgame/nlpBeginnerProjects 的开发做出贡献…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">github.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj ks ov"/></div></div></a></div><p id="833e" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">导入依赖项</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="4bda" class="mo la it oj b gy on oo l op oq">from gensim.models import Word2Vec, FastText<br/>import pandas as pd<br/>import re</span><span id="44ed" class="mo la it oj b gy or oo l op oq">from sklearn.decomposition import PCA</span><span id="e081" class="mo la it oj b gy or oo l op oq">from matplotlib import pyplot as plt<br/>import plotly.graph_objects as go</span><span id="a425" class="mo la it oj b gy or oo l op oq">import numpy as np</span><span id="925a" class="mo la it oj b gy or oo l op oq">import warnings<br/>warnings.filterwarnings('ignore')</span><span id="0a6c" class="mo la it oj b gy or oo l op oq">df = pd.read_csv('emails.csv')</span></pre><p id="86aa" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">我首先使用 Pandas 加载库并读取 csv 文件。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h1 id="0e89" class="kz la it bd lb lc pk le lf lg pl li lj jz pm ka ll kc pn kd ln kf po kg lp lq bi translated">探索 Word2Vec</h1><p id="714c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在处理电子邮件数据之前，我想用一个简单的例子来探索 Word2Vec，这个例子使用了几个句子的小词汇表:</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="f626" class="mo la it oj b gy on oo l op oq">sentences = [['i', 'like', 'apple', 'pie', 'for', 'dessert'],<br/>            ['i', 'dont', 'drive', 'fast', 'cars'],<br/>            ['data', 'science', 'is', 'fun'],<br/>            ['chocolate', 'is', 'my', 'favorite'],<br/>            ['my', 'favorite', 'movie', 'is', 'predator']]</span></pre><h2 id="0889" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">生成嵌入</h2><p id="5621" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">注意，句子已经被标记化了，因为我想在单词级而不是句子级生成嵌入。通过 Word2Vec 模型运行句子。</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="f5a0" class="mo la it oj b gy on oo l op oq"># train word2vec model<br/>w2v = Word2Vec(sentences, min_count=1, size = 5)</span><span id="b0c3" class="mo la it oj b gy or oo l op oq">print(w2v)<br/>#Word2Vec(vocab=19, size=5, alpha=0.025)</span></pre><p id="7a3a" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">注意，在构建模型时，我传入了<strong class="lt iu"> min_count =1 </strong>和<strong class="lt iu"> size = 5 </strong>。也就是说，它将包含<em class="nf">出现≥ 1 </em>次的所有单词，并生成一个<em class="nf">固定长度为 5 </em>的向量。<br/>请注意，打印时，模型会显示唯一 vocab 单词的计数、数组大小和学习率(默认为. 025)</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="3a95" class="mo la it oj b gy on oo l op oq"># access vector for one word<br/>print(w2v['chocolate'])</span><span id="be9e" class="mo la it oj b gy or oo l op oq">#[-0.04609262 -0.04943436 -0.08968851 -0.08428907  0.01970964]</span><span id="e562" class="mo la it oj b gy or oo l op oq">#list the vocabulary words<br/>words = list(w2v.wv.vocab)</span><span id="b74c" class="mo la it oj b gy or oo l op oq">print(words)</span><span id="df8f" class="mo la it oj b gy or oo l op oq">#or show the dictionary of vocab words<br/>w2v.wv.vocab</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/d1123a149aae84197768b934f40419ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kxNjAdrxLwIVLKeHzJ-sUQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">考察词汇的两种不同方法</p></figure><p id="ac07" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">注意，一次可以访问一个单词的嵌入。<br/>注意，可以使用<strong class="lt iu"><em class="nf">w2v . wv . vocab .</em></strong>以几种不同的方式复习词汇中的单词</p><h2 id="5c80" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">可视化嵌入</h2><p id="f33f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">既然已经使用 Word2Vec 创建了单词嵌入，那么可以使用一种方法来可视化它们，以在展平的空间中表示矢量。我使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"> Sci-Kit Learn 的主成分分析</a> (PCA)功能将单词向量展平到 2D 空间，然后使用<a class="ae ky" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"> matplitlib </a>将结果可视化。</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="65a3" class="mo la it oj b gy on oo l op oq">X = w2v[w2v.wv.vocab]<br/>pca = PCA(n_components=2)</span><span id="5a58" class="mo la it oj b gy or oo l op oq">result = pca.fit_transform(X)</span><span id="933f" class="mo la it oj b gy or oo l op oq"># create a scatter plot of the projection<br/>plt.scatter(result[:, 0], result[:, 1])<br/>words = list(w2v.wv.vocab)</span><span id="90bd" class="mo la it oj b gy or oo l op oq">for i, word in enumerate(words):<br/>    plt.annotate(word, xy=(result[i, 0], result[i, 1]))</span><span id="7c03" class="mo la it oj b gy or oo l op oq">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/6c024a9d3d8a649fd1fe0ddb5b9ffd44.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*h-ulnd1To-RCmle9phhz1A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单词嵌入图</p></figure><p id="5809" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">幸运的是，语料库很小，所以很容易可视化；然而，很难从标绘的点中解读出任何意义，因为该模型可以学习的信息如此之少。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h1 id="e929" class="kz la it bd lb lc pk le lf lg pl li lj jz pm ka ll kc pn kd ln kf po kg lp lq bi translated">可视化电子邮件单词嵌入</h1><p id="3678" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在我已经完成了一个简单的例子，是时候将这些技巧应用到更大的数据中了。通过调用 dataframe <strong class="lt iu"> <em class="nf"> head()检查邮件数据。</em> </strong></p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="df89" class="mo la it oj b gy on oo l op oq">df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/7b1be7800e4432e64326f55a773d5041.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*4Y0NyEq4gycaTzcZkj3oIQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">df.head()</p></figure><h2 id="7e12" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">清理数据</h2><p id="4d8f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">请注意，文本根本没有经过预处理！使用一个简单的函数和一些正则表达式，清除文本中的标点符号和特殊字符，并将其全部设置为小写很简单。</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="a0ac" class="mo la it oj b gy on oo l op oq">clean_txt = []<br/>for w in range(len(df.text)):<br/>    desc = df['text'][w].lower()<br/>    <br/>    #remove punctuation<br/>    desc = re.sub('[^a-zA-Z]', ' ', desc)<br/>    <br/>    #remove tags<br/>    desc=re.sub("&amp;lt;/?.*?&amp;gt;"," &amp;lt;&amp;gt; ",desc)<br/>    <br/>    #remove digits and special chars<br/>    desc=re.sub("(\\d|\\W)+"," ",desc)<br/>    clean_txt.append(desc)</span><span id="2e9d" class="mo la it oj b gy or oo l op oq">df['clean'] = clean_txt<br/>df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/c12548703fb933beca07b19a947eceb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ybilAMnUQ4lMZIg3GHelw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">df.head()显示干净的文本</p></figure><p id="f4c3" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">请注意，数据帧中添加了 clean 列，文本中的标点符号和大写字母已被清除。</p><h2 id="89c2" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">创建语料库和向量</h2><p id="2581" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">因为我想要单词嵌入，所以文本需要标记化。使用循环的<em class="nf">,我遍历标记每个干净行的数据帧。创建语料库后，我通过将语料库传递给 Word2vec 来生成单词向量。</em></p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="3d78" class="mo la it oj b gy on oo l op oq">corpus = []<br/>for col in df.clean:<br/>    word_list = col.split(" ")<br/>    corpus.append(word_list)</span><span id="3f53" class="mo la it oj b gy or oo l op oq">#show first value<br/>corpus[0:1]</span><span id="c122" class="mo la it oj b gy or oo l op oq">#generate vectors from corpus<br/>model = Word2Vec(corpus, min_count=1, size = 56)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/8356d59b6a8e20effa8d52a018fc3d4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*hyzk2eQx-XiQNWgJ9BTrYg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文集</p></figure><p id="688d" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">请注意，数据已经进行了标记化，可以进行矢量化了！</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h1 id="3387" class="kz la it bd lb lc pk le lf lg pl li lj jz pm ka ll kc pn kd ln kf po kg lp lq bi translated">可视化电子邮件单词向量</h1><p id="e0a0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">电子邮件数据的语料库比简单示例大得多。由于包含了很多单词，我不能像使用 matplotlib 那样绘制它们。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/3d50c4c332026808212779c4bb94f29e.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*KXFpzx-SAJNw0YgyJ67DTA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用 matplotlib 可视化电子邮件单词</p></figure><p id="15ca" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">祝你理解这一点！是时候使用不同的工具了。代替<em class="nf"> matplotlib </em>，我将使用<strong class="lt iu"> plotly </strong>生成一个可以缩放的交互式可视化。这将使探索数据点变得更加容易。</p><p id="1390" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">我再次使用主成分分析技术。然后我把结果和单词放入一个数据框中。这将使 plotly 中的图形和注释更容易。</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="c98c" class="mo la it oj b gy on oo l op oq">#pass the embeddings to PCA<br/>X = model[model.wv.vocab]<br/>pca = PCA(n_components=2)<br/>result = pca.fit_transform(X)</span><span id="9710" class="mo la it oj b gy or oo l op oq">#create df from the pca results<br/>pca_df = pd.DataFrame(result, columns = ['x','y'])</span><span id="e36e" class="mo la it oj b gy or oo l op oq">#add the words for the hover effect<br/>pca_df['word'] = words<br/>pca_df.head()</span></pre><p id="2bbb" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">请注意，我将单词 column 添加到了 dataframe 中，因此当鼠标悬停在图表上的点上时，就会显示该单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/2dc474adad326adc9a5c8ebfd32053b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*dF2iGmShDDaDyUQ6V98eKA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PCA_df.head()</p></figure><p id="337a" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><a class="ae ky" href="https://plotly.com/python/line-and-scatter/" rel="noopener ugc nofollow" target="_blank">接下来，使用 ploty Scattergl 构建散点图，以在大型数据集上获得最佳性能</a>。有关不同散点图选项的更多信息，请参考文档。</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="eead" class="mo la it oj b gy on oo l op oq">N = 1000000<br/>words = list(model.wv.vocab)<br/>fig = go.Figure(data=go.Scattergl(<br/>    x = pca_df['x'],<br/>    y = pca_df['y'],<br/>    mode='markers',<br/>    marker=dict(<br/>        color=np.random.randn(N),<br/>        colorscale='Viridis',<br/>        line_width=1<br/>    ),<br/>    text=pca_df['word'],<br/>    textposition="bottom center"<br/>))</span><span id="8f28" class="mo la it oj b gy or oo l op oq">fig.show()</span></pre><p id="8fc3" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">注意，我使用<em class="nf"> Numpy </em>为图形颜色生成随机数。这使它在视觉上更有吸引力！<br/>注意，我将文本设置到 dataframe 的<strong class="lt iu">单词</strong>列。当鼠标悬停在数据点上时，会出现该单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/8ca3fa9213faef844f6f1cf53cebd8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bd_GOhV3vGjdjyk0600sBg.png"/></div></div></figure><p id="8706" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">Plotly 很棒，因为它可以生成交互式图形，并且允许我放大图形，更近距离地检查点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/7096cfe7089c0f127c400e0bc3ac734c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JuFxVWd7N_-9WWXdwPy8XA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">放大</p></figure></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h1 id="7f2f" class="kz la it bd lb lc pk le lf lg pl li lj jz pm ka ll kc pn kd ln kf po kg lp lq bi translated">使用单词嵌入进行分析和预测</h1><p id="414c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">除了可视化嵌入之外，还可以用一些代码来探索它们。此外，模型可以保存为文本文件，以便在将来的建模中使用。查看 Gensim 文档，了解完整的功能列表。</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="47a9" class="mo la it oj b gy on oo l op oq">#explore embeddings using cosine similarity<br/>model.wv.most_similar('eric')</span><span id="5a70" class="mo la it oj b gy or oo l op oq">model.wv.most_similar_cosmul(positive = ['phone', 'number'], negative = ['call'])</span><span id="ef97" class="mo la it oj b gy or oo l op oq">model.wv.doesnt_match("phone number prison cell".split())</span><span id="310c" class="mo la it oj b gy or oo l op oq">#save embeddings<br/>file = 'email_embd.txt'<br/>model.wv.save_word2vec_format(filename, binary = False)</span></pre><p id="2cba" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">Gensim 使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>来查找最相似的单词。<br/>请注意，评估类比并找出与其他单词最不相似或不匹配的单词也是可能的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/765cc5365fc83d78111465bddde965f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VEtNScNr1y9Stta1l2Kglg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">查找相似单词的输出</p></figure><h2 id="ce1f" class="mo la it bd lb mp mq dn lf mr ms dp lj ma mt mu ll me mv mw ln mi mx my lp mz bi translated">使用嵌入</h2><p id="e3f4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我不会在本文中涵盖完整的示例，但是在预测建模中使用这些向量是可能的。为了使用嵌入，需要映射单词向量。为了使用训练好的模型将多个单词的文档转换成单个向量，通常取文档中所有单词的 word2vec，然后取其平均值。</p><pre class="kj kk kl km gt oi oj ok ol aw om bi"><span id="7a50" class="mo la it oj b gy on oo l op oq">mean_embedding_vectorizer = MeanEmbeddingVectorizer(model)<br/>mean_embedded = mean_embedding_vectorizer.fit_transform(df['clean'])</span></pre><p id="2084" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">要了解更多关于在预测建模中使用 word2vec 嵌入的信息，请查看 kaggle.com 的笔记本。</p><div class="os ot gp gr ou ov"><a href="https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">基本自然语言处理:单词包，TF-IDF，Word2Vec，LSTM</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">使用 Kaggle 笔记本探索和运行机器学习代码|使用来自个性化医疗的数据:重新定义癌症…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">www.kaggle.com</p></div></div><div class="pe l"><div class="pz l pg ph pi pe pj ks ov"/></div></div></a></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/65eada0ea63c8d8c35dc6f2aefd2fa6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nYOOSbtXN30Yi5v68sHJbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">添加了数组列的数据帧</p></figure><h1 id="57b0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最后的想法</h1><p id="14af" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">使用 Word2Vec 模型可用的新方法，很容易训练非常大的词汇表，同时在机器学习任务上获得准确的结果。自然语言处理是一个复杂的领域，但是有许多 Python 的库和工具可以使入门变得简单。如果您有兴趣了解更多关于 NLP 或数据科学的知识，请查看我的其他文章:</p><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/ultimate-beginners-guide-to-collecting-text-for-natural-language-processing-nlp-with-python-256d113e6184"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">用 Python 收集自然语言处理(NLP)文本的入门指南</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">通过 API 和 Web 抓取收集文本</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="qb l pg ph pi pe pj ks ov"/></div></div></a></div><h1 id="2e6d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">谢谢大家！</h1><ul class=""><li id="e580" class="nt nu it lt b lu lv lx ly ma qc me qd mi qe mm ny nz oa ob bi translated"><em class="nf">如果你喜欢这个，</em> <a class="ae ky" href="https://medium.com/@erickleppen" rel="noopener"> <em class="nf">在 Medium 上关注我</em> </a> <em class="nf">获取更多</em></li><li id="a870" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ny nz oa ob bi translated"><a class="ae ky" href="https://erickleppen.medium.com/membership" rel="noopener"> <em class="nf">通过订阅</em> </a>获得对我的内容的完全访问和帮助支持</li><li id="f2ba" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ny nz oa ob bi translated"><em class="nf">我们来连线一下</em><a class="ae ky" href="https://www.linkedin.com/in/erickleppen01/" rel="noopener ugc nofollow" target="_blank"><em class="nf">LinkedIn</em></a></li><li id="9e15" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ny nz oa ob bi translated"><em class="nf">用 Python 分析数据？查看我的</em> <a class="ae ky" href="https://pythondashboards.com/" rel="noopener ugc nofollow" target="_blank"> <em class="nf">网站</em> </a></li></ul><p id="e5bd" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><a class="ae ky" href="http://pythondashboards.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> —埃里克·克莱本</strong> </a></p></div></div>    
</body>
</html>