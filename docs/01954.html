<html>
<head>
<title>Classical Neural Network: What really are Nodes and Layers?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">经典神经网络:什么是真正的节点和层？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classical-neural-network-what-really-are-nodes-and-layers-ec51c6122e09?source=collection_archive---------10-----------------------#2020-02-24">https://towardsdatascience.com/classical-neural-network-what-really-are-nodes-and-layers-ec51c6122e09?source=collection_archive---------10-----------------------#2020-02-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ea511733c93501190546cad6c5a5c684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YgJ6SYO7byjfCmt5uV0PmA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">权利:<a class="ae jg" href="https://webkid.io/blog/neural-networks-in-javascript/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><div class=""/><div class=""><h2 id="6010" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">一个节点和一个层在数学上代表什么？简单易懂的幕后概念介绍。</h2></div><p id="58c5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们之前已经讨论过激活函数，正如承诺的那样，我们将解释它与神经网络架构中的层和节点的联系。</p><p id="5561" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，这是对经典神经网络的解释，而不是对专门神经网络的解释。尽管如此，这些知识在研究特定的神经网络时还是有用的。</p><p id="cf4e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好吧，说了这么多，我们开始吧。</p><p id="49d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们将以下面非常简单的神经网络(NN)结构为例。(图 1)</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lu"><img src="../Images/b2ed9ce35c10d6d19679818593712ead.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eVuJqbLIf-_izCUg_ne9xQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图一(右图:<a class="ae jg" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><ul class=""><li id="d5be" class="lz ma jj la b lb lc le lf lh mb ll mc lp md lt me mf mg mh bi translated">输入层:节点 1 → X |激活:sigmoid</li><li id="8d9d" class="lz ma jj la b lb mi le mj lh mk ll ml lp mm lt me mf mg mh bi translated">隐藏层:节点 1 →N1 和节点 2 → N2(从上到下)|激活:sigmoid</li><li id="49bc" class="lz ma jj la b lb mi le mj lh mk ll ml lp mm lt me mf mg mh bi translated">输出层:节点 1 → M |激活</li></ul><pre class="lv lw lx ly gt mn mo mp mq aw mr bi"><span id="06b5" class="ms mt jj mo b gy mu mv l mw mx">#This code is the keras implementation of the above described NNdef simple_nn():<br/>    model = Sequential()<br/>    model.add(Dense(2, input_dim=1, activation='sigmoid'))<br/>    model.add(Dense(1, activation='sigmoid'))<br/>    model.compile(loss='mean_squared_error', optimizer='sgd')<br/>    return model</span></pre><h2 id="4786" class="ms mt jj bd my mz na dn nb nc nd dp ne lh nf ng nh ll ni nj nk lp nl nm nn no bi translated">这个神经网络代表哪种功能？</h2><p id="aab7" class="pw-post-body-paragraph ky kz jj la b lb np kk ld le nq kn lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">给定上面的符号，我们得到下面的函数(图 2):</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/e23602d72119ebfa72b4b7be78c77669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sP9EkMExkD3LxXQ74NhuVA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 2(权利:自己的形象)</p></figure><p id="e3ec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里需要注意多种情况:</p><ul class=""><li id="9358" class="lz ma jj la b lb lc le lf lh mb ll mc lp md lt me mf mg mh bi translated">神经网络的输出将总是属于[0，1]。正如我们在激活功能文章中提到的，输出层激活功能非常重要，它定义了你想要实现的模型类型(例如分类/回归等)</li><li id="d5fc" class="lz ma jj la b lb mi le mj lh mk ll ml lp mm lt me mf mg mh bi translated">由于只有一个由两个节点组成的隐藏层，我们最终得到一个维数为 7 的权重向量。这使得当节点数量增加时，训练变得困难。</li><li id="7b2a" class="lz ma jj la b lb mi le mj lh mk ll ml lp mm lt me mf mg mh bi translated">除了激活函数，操作都是线性组合。再次，激活函数引入了非线性。</li></ul><h2 id="110a" class="ms mt jj bd my mz na dn nb nc nd dp ne lh nf ng nh ll ni nj nk lp nl nm nn no bi translated">为什么我们要使用线性组合和这些类型的激活函数？</h2><p id="3200" class="pw-post-body-paragraph ky kz jj la b lb np kk ld le nq kn lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">首先，尽管深度学习(多层神经网络的研究)本身是一种研究，但它与经典的机器学习有着相同的目标:“从数据点(大部分时间)接近特定的底层模型/分布”。因此，神经网络的目标也是逼近一个分布，即一个函数，但如何实现呢？这里介入一些关于分析的基础知识，振作起来！</p><p id="f9de" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了简单起见(如果你有兴趣知道更一般的解释，可以联系我)，我们将改为下面的架构。下面是它的功能(图 3):</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/bfc82fc302bbeec60ad072a747b8413b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*msAUU3TIb6wv1nVX7N2Nxw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 3(权利:自己的形象)</p></figure><p id="4f7c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们先从一个从实数到实数的连续函数开始。让我们为自己设定一个接近这样一个函数的目标。开始的标准方法是首先画出我们的神经网络所代表的函数(图 3)。由于我们不需要任何具体的数据来解释这个想法，我们将不训练神经网络，而只是简单地任意分配权重(图 4)。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/304f06b10be676d974da0852bbafcf6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qy_THUlEQaaYcUmU51uUg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 4(权利:自己的形象)</p></figure><p id="8e4b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是曲线图(图 5):</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/fa1d50ec0fd0ec984aac5a40f51e1826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*qPkQ8LTzT-WrLXjylNbiYg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图 5(权利:自己的形象)</p></figure><p id="d2a3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">惊喜！这是什么形状？一个长方形！！！</p><p id="004e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以我们的神经网络实际上是在模仿一个矩形的分布(或多或少)。对于一些人来说，这似乎并不特别，但对于其他一些听说过黎曼积分和阶梯函数的人来说，他们或多或少会明白我的意思。确切地说，是像神经网络这样的阶跃函数对连续函数的近似(不完全是阶跃函数，但是求和完成了这项工作)。</p><p id="20cc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有几件事需要注意:</p><ul class=""><li id="f29f" class="lz ma jj la b lb lc le lf lh mb ll mc lp md lt me mf mg mh bi translated">矩形边缘的锐度由 X 前面的标量定义，高值导数的位置由加到乘积上的标量定义。</li><li id="a9e7" class="lz ma jj la b lb mi le mj lh mk ll ml lp mm lt me mf mg mh bi translated">两个节点足以构成一个矩形，因此要实现连续函数，我们只需添加节点！(在奇数个节点的情况下，我们将简单地使用矩形和阶梯函数)</li></ul><p id="d33c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管这很难描述，但在更高维度中，连续函数的近似值几乎是一样的(除了我们重新调整值的那一步)。</p><p id="0588" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">既然我们已经看到了这样的近似，我们可以对神经网络逼近分布(即使是非连续的)的能力充满信心。但这种解释仍然缺乏一些东西，我们任意地给我们的神经网络一些准确的权重，但不幸的是，我们无法在一般的数据集这样做，因为我们忽略了分布。这里介入了优化技术，如著名的 SGD(随机梯度下降)或批处理 GD(等等)。假设这些优化确实让我们得到了一个足够接近的解决方案，这将意味着我们超出了我们给出的原始权重(对于矩形)。显示上面的例子(矩形)在某种程度上给出了精确度的下限，即使提到的技术看起来是最佳的，权重的优化可能不一定收敛于该技术的最佳值，但会再次超过它。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="8962" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您的阅读，敬请关注后续文章。此外，点击这个<a class="ae jg" href="https://direct-link.net/91830/aitechfordummies" rel="noopener ugc nofollow" target="_blank">链接</a>(指向联盟计划)真的会帮我解决问题！您只需完成一些快速任务(只需等待和激活通知)，所有这些将真正帮助我了解更多未来的硬件相关内容！</p></div></div>    
</body>
</html>