<html>
<head>
<title>Writing distributed data parallel applications with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 PyTorch 编写分布式数据并行应用程序</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/writing-distributed-applications-with-pytorch-f5de4567ed3b?source=collection_archive---------19-----------------------#2020-07-10">https://towardsdatascience.com/writing-distributed-applications-with-pytorch-f5de4567ed3b?source=collection_archive---------19-----------------------#2020-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4c43" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">多 GPU 节点的分布式数据并行训练</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e83eee2b3a044f4d75cfa72395ac73f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywd_QK_ptvoPt5Tk0PAYBA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由 Pexels 的 Manuel 拍摄</p></figure><p id="3235" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">概要</strong></p><p id="3572" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文的主要目的是用简单的步骤阐明如何利用 PyTorch 实现的分布式计算技术在多个 GPU 上训练深度学习模型。文章假设你熟悉训练深度学习网络。本教程首先介绍一些关于分布式计算的关键概念，然后使用 PyTorch 的分布式数据并行功能编写一个 python 脚本来训练一个具有 4 个 GPU 的模型</p><blockquote class="lu lv lw"><p id="dfd5" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">注意:此处的教程将仅涵盖与分布式培训相关的关键步骤。完整的代码可以在<a class="ae mb" href="https://github.com/shreeraman96/distributed_training" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p></blockquote><p id="0d58" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">分布式计算——什么是分布式计算？</strong></p><p id="45e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分布式系统是通过网络进行通信以实现共同目标的不同独立节点的集合。每台机器被称为一个节点，通过单个网络连接的一群节点形成一个集群。节点利用网络与其对等节点通信。</p><p id="1b84" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分布式计算与这样一种编写程序的风格相关联，这种编写程序的风格充分利用了分布在整个体系结构中的计算机的计算能力</p><p id="c771" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">深度学习的分布式计算</strong></p><p id="034c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">深度学习和分布式系统是最近越来越受欢迎的两个不断发展的系统。计算能力最近几乎呈指数级增长，通过深度神经网络(DNN)，研究爱好者正在世界各地创造奇迹。因此，从两者中获益是非常重要的。如果我们利用大规模分布式系统的计算潜力，DNNs 的训练时间可以大大减少，性能也会提高</p><p id="70cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">优化计算的要点:</strong></p><blockquote class="lu lv lw"><p id="703f" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">1.通过系统和一致的处理跨多个节点分布计算</p><p id="cb60" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">2.在对等体之间建立同步</p></blockquote><p id="246b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里的重点是用简单的步骤解释如何利用 PyTorch 分布式模块通过数据并行技术为 BERT 模型进行屏蔽语言建模训练。训练其他模型也可以遵循相同的步骤</p><p id="f634" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">数据并行技术</strong></p><p id="ac2f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，数据分布在多个节点上，以实现更快的训练时间。每个节点都应该有自己专用的模型副本、优化器和其他要素。我们将使用 PyTorch 模块的分布式 API 来实现这一点。</p><p id="c095" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">torch . distributed-基本概述</strong></p><p id="7cea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">PyTorch 通过在任何 PyTorch 模型周围提供一个包装类来支持同步分布式训练。</p><p id="5993" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个流程都有自己的 python 解释器、优化器和模型副本，并在每一步执行完整的优化，从而减少开销时间</p><p id="783e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">PyTorch 还支持不同的通信后端来收集跨多个节点的迭代的每一步的向前和向后传递的结果。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="b9ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们深入实际的实现</p><p id="7817" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤 1: </strong>获取所有可用 GPU 的列表，包括 device _ ids 和可用 GPU 的总数</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="c262" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤 2: </strong>从任何来源加载用于掩蔽语言建模的句子，并清理和处理数据。</p><p id="5062" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">第三步:</strong>加载一个预先训练好的 BertTokenizer，用它编码所有的句子，创建目标句子。屏蔽输入句子的某些单词以创建屏蔽句子</p><p id="98f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤 4: </strong>设置分布式训练所需的不同配置参数，并将其加载到单个实体中，最好是一个类</p><blockquote class="lu lv lw"><p id="6c3c" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">世界规模</strong>:相当于可用 GPU 数量的进程总数</p><p id="581f" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">主地址</strong>:将托管等级为 0 的进程的机器的 IP 地址</p><p id="b19e" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">主端口</strong>:节点间通信的空闲可用端口</p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="20d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤 5: </strong>定义 train()函数:</p><p id="1326" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是将分布在多个节点上的功能。因此，确保每个节点都有自己的所需变量和设置的专用副本是非常重要的</p><p id="c23a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将在单个实体上收集多个不同的参数和变量，并作为值参数传递给训练函数。</p><p id="b7d5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除此之外，训练函数还将接收参数<strong class="la iu"> <em class="lx"> rank </em> </strong>，处理器通过该参数来决定它是工作者节点还是主节点。</p><p id="7e4f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">等级为 0 的节点将是主节点。PyTorch 负责为每个 GPU 分配等级，所以我们不必为此担心</p><p id="1f44" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤 5.1 </strong>:用合适的后端系统和模型建立流程组</p><p id="0e03" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lx"> Init_process </em> </strong>允许进程通过共享它们的位置来相互通信和协调。PyTorch 给出了两种指定进程组配置和启动进程组的方法</p><blockquote class="lu lv lw"><p id="984e" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">1.指定世界大小、等级和商店(可选)</p><p id="403d" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">2.用 URL 中编码的等级和世界大小或其他方式指定 URL 字符串，以指示在哪里/如何与对等方通信。默认情况下，它将是<strong class="la iu">“env://”</strong></p></blockquote><p id="f0f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">init_process 接受 4 个参数:</p><blockquote class="lu lv lw"><p id="3e72" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">1.<strong class="la iu">后端</strong>:要使用的通信后端。可用选项:格洛，NCCL，MPI。NCCL 适合 GPU 训练，而 Gloo 更适合 CPU 训练</p><p id="9563" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">2.<strong class="la iu"> Init_method: </strong>字符串 URL(默认值:env://)</p><p id="37c8" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">3.<strong class="la iu"> World_size </strong>:作业中的进程数。通常相当于可用的 GPU 数量</p><p id="b38f" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">4.<strong class="la iu">等级</strong>:当前流程的等级</p></blockquote><p id="c426" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤 5.3 </strong>:加载模型</p><p id="d188" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">加载所需的模型。在我们的例子中，加载用于屏蔽语言建模的 BERT 模型(BertForMaskedLM)</p><p id="f560" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤 5.4: </strong> set-GPU device &amp;加载模型到设备中</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="b8f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤 5.6 </strong>:用分布式数据并行类包装模型，将模型分布到各个节点</p><p id="50ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个容器为我们的 PyTorch 模型提供了一个包装器，并将给定模块的应用并行化，并将输入拆分到指定的设备上。该模块在集群上的每台机器和每台设备上复制，每个副本处理一小部分输入。</p><blockquote class="lu lv lw"><p id="9c21" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">1.<strong class="la iu">模块</strong>:要并行化的模块。在我们的案例中，我们案例中使用的模型</p><p id="b4d2" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">2.<strong class="la iu">设备标识</strong> : CUDA 设备列表</p></blockquote><p id="fc96" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤 5.7 </strong>:为每个副本设置数据加载器</p><p id="5fcb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们设置<em class="lx"> training_sampler </em> u <em class="lx">使用 DistributedDataSampler() </em>包装类来采样和分发每个副本的输入数据。</p><p id="2067" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">参数:</p><blockquote class="lu lv lw"><p id="7b74" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">1.<strong class="la iu">数据集</strong>:输入数据集</p><p id="55d7" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">2.副本数量:在我们的例子中等于世界大小(4)</p></blockquote><p id="9904" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一步是用我们定义的分布式采样器来设置 Dataloader。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="e978" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">步骤 5.8 </strong>定义培训流程的其余部分&amp;保存模型</p><p id="fe09" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">第六步:</strong>根据可用的 GPU 数量启动产卵过程。PyTorch 现在将在指定的世界大小和设备列表中生成进程</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="79f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">希望本文为您提供了一个关于如何利用 PyTorch 的分布式 API 在多个 GPU 上实现分布式训练的合理思路。完整的代码可以在这里的资源库中找到。如果您希望深入了解这些模块，我建议您访问以下链接。</p><div class="ml mm gp gr mn mo"><a href="https://pytorch.org/docs/stable/distributed.html#basics" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd iu gy z fp mt fr fs mu fu fw is bi translated">分布式通信包- torch.distributed - PyTorch 主文档</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">torch.distributed 支持三个后端，每个后端都有不同的功能。下表显示了哪些功能是…</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">pytorch.org</p></div></div></div></a></div><div class="ml mm gp gr mn mo"><a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd iu gy z fp mt fr fs mu fu fw is bi translated">使用 PyTorch 编写分布式应用程序- PyTorch 教程 1.5.1 文档</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">作者:Séb Arnold 在这个简短的教程中，我们将介绍 PyTorch 的分发包。我们将看到如何…</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">pytorch.org-</p></div></div><div class="mx l"><div class="my l mz na nb mx nc ks mo"/></div></div></a></div></div></div>    
</body>
</html>