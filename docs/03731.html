<html>
<head>
<title>Statistical Decision Theory for Predictive Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测模型的统计决策理论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-math-you-need-to-develop-your-own-predictive-models-fdb771cc1ddf?source=collection_archive---------29-----------------------#2020-04-07">https://towardsdatascience.com/the-math-you-need-to-develop-your-own-predictive-models-fdb771cc1ddf?source=collection_archive---------29-----------------------#2020-04-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn"><p id="445a" class="jo jp iq bd jq jr js jt ju jv jw jx dk translated">沉默的统计学家改变了我们的世界；不是通过发现新的事实或技术发展，而是通过改变我们推理、实验和形成观点的方式。伊恩·哈金</p></blockquote><p id="977e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku jx ij bi translated">你能从观察值中写出一个预测函数来最小化误差偏差吗？还是平均误差？你能找到能最小化任何给定损失函数的最佳理论预测值吗？</p><p id="bd3f" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">这篇文章将通过快速介绍统计决策理论来回答这些问题。<strong class="ka ir">这不是另一个机器学习的线性代数基础教程</strong>。我假设读者有一点数学背景。一年级的Stems本科生所知道的就足够了。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi la"><img src="../Images/94a17425f6c7c0711349dd5fd1cea1c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnc6XQfNbXR6QIX3tNVK8g.jpeg"/></div></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">照片由<a class="ae lq" href="https://unsplash.com/@chrisliverani?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">克里斯·利维拉尼</a>在<a class="ae lq" href="https://unsplash.com/s/photos/stats?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="e779" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">统计决策理论</h1><p id="1a7b" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku jx ij bi translated">我们的模型是根据几个假设定义的:</p><ul class=""><li id="4b2f" class="mu mv iq ka b kb kv kf kw kj mw kn mx kr my jx mz na nb nc bi translated">我们有一个输入向量<em class="nd"> X </em>的<em class="nd"> p </em>随机参数。</li><li id="a753" class="mu mv iq ka b kb ne kf nf kj ng kn nh kr ni jx mz na nb nc bi translated">我们有一个期望的输出变量<em class="nd"> Y </em>。</li><li id="6d78" class="mu mv iq ka b kb ne kf nf kj ng kn nh kr ni jx mz na nb nc bi translated">目标是找到一个预测函数<em class="nd"> f </em>来预测给定<em class="nd"> X </em>的<em class="nd"> Y </em>。</li><li id="8560" class="mu mv iq ka b kb ne kf nf kj ng kn nh kr ni jx mz na nb nc bi translated">为了搜索给定标准的最优<em class="nd"> f </em>，我们将根据该标准选择损失函数<em class="nd"> Lf </em>。</li><li id="71c4" class="mu mv iq ka b kb ne kf nf kj ng kn nh kr ni jx mz na nb nc bi translated">根据我们的标准，如果预测与现实相差甚远，我们的损失函数将返回最大值。</li></ul><p id="e3d5" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">由于预测器的质量是由Lf的损失函数来测量的，所以最佳预测器f仅在Lf规定的意义上是最佳的。损失函数的不同选择导致不同的最优解。</p><p id="3dc1" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">一般来说，不可能想出一个在所有可以想到的方面都是最佳的预测器。因此，为了在一组预测因子中进行选择，我们将比较它们损失函数的平均值E。</p><h2 id="3e3d" class="nj ls iq bd lt nk nl dn lx nm nn dp mb kj no np mf kn nq nr mj kr ns nt mn nu bi translated">最佳预测器的搜索</h2><p id="18e8" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku jx ij bi translated">我们将任何概率分布表示为<em class="nd"> ℙ </em>，期望值表示为<em class="nd"> E </em>。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/885dbbbcdff7b449ecfd786c1a0b426b.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*bk1A5OFz8ia5lz-QQVooHg.png"/></div></figure><p id="a540" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">从贝叶斯定理，</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ca01aea2a27920798b3a136477f6cc8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*RggEMvnIZUJKpgU58VvLoQ.png"/></div></figure><p id="c5ff" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">然后通过分解二元积分，</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/87b312b79b66242c925287154c337148.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*dFaeAtUSacW-t_rRq9f5kw.png"/></div></figure><p id="2ae2" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">现在可以通过逐点最小化期望误差来求出f。<br/>设c ∈ ℝᵖ，</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ce088d603ad8bfd8b0d29ddd47181b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*TUtiPIdR4oikrJGtdiLQog.png"/></div></figure><p id="0376" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">用<em class="nd"> L_Id </em>将损失函数应用于身份函数。通过最小化上述期望，无论是解析地还是数值地，我们可以找到最佳匹配预测器<em class="nd"> f </em>。</p><p id="6e08" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">案例研究将有助于阐明如何在给定的损失函数上找到f。</p></div><div class="ab cl nz oa hu ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="ij ik il im in"><h2 id="ddc3" class="nj ls iq bd lt nk nl dn lx nm nn dp mb kj no np mf kn nq nr mj kr ns nt mn nu bi translated">案例研究:均方误差</h2><p id="bf1d" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku jx ij bi translated">ML/DL中最常用的损失函数通常是均方误差:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f45e1d3edb9a715969386d1384ce52b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*GK1P_BfvHcvmIrYCYS6yHw.png"/></div></figure><p id="9367" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">从上一节来看，</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/776c8720471a65755a08c1509ee0409b.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*5AJA6GBMePiwcUbaLnDG_g.png"/></div></figure><p id="2e53" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">因此，</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/74604e945e8ddca9197249b099386df5.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*n1ZWPahFSxaVY_stpFOXTQ.png"/></div></figure><p id="8813" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">解是f(x) = E(Y|X=x)，因为:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/e7b7e1058b99c4a6f595af1eade9a120.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*CoJcKf8gwfa2_YP6YYjD_Q.png"/></div></figure><p id="bb6d" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">我们刚刚证明，当损失函数是均方误差时，Y在任一点X=x的最佳预测值是条件均值:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/d7e0ef9f45d9026e1c5c7db7b4f285fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*qLBzsYYQUNsEoqthCN5QAg.png"/></div></figure><p id="17c2" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">这个预测值通常被称为线性回归。通过将f(x)注入多元积分并使关于β的矢量导数等于零，可以求解β。</p><p id="f672" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">损失函数的另一种可能选择是偏差的绝对值:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/47565a57349910e766677ba18172b069.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*c6IGq-kYafUcLYwLCk857w.png"/></div></figure><p id="9c79" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">可以证明最佳预测值是条件中位数:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi om"><img src="../Images/17c8bff26e27f789599f7e4381baa73b.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*DkNQJ9IHYsTcPYCJDfbtPA.png"/></div></figure><p id="f72a" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">这是一种不同的位置度量，它的估计比条件均值的估计更稳健。不幸是，偏差的绝对值在其导数中具有不连续性，这阻碍了它们在损失函数中的广泛应用。</p><h1 id="e342" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">分类变量的预测值</h1><p id="6d1b" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku jx ij bi translated">当我们预测一个变量<em class="nd"> Y </em>取离散值<em class="nd"> c₁，c₂，…，cₖ </em>可视为某种群体标签时，我们经常会谈到分类。</p><p id="e84f" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">我们的损失函数可以用一个<em class="nd"> K×K </em>矩阵<em class="nd"> Lf </em>来表示，其中<em class="nd"> K </em>为类的数量。<em class="nd"> Lf(ci，cj) </em>是对类别<em class="nd"> j </em>而不是类别<em class="nd"> i </em>的错误分类的损失。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi on"><img src="../Images/cabc8d346e854c9a92651eaa44a00339.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*EunxfTcEoQVzDlwvPwNUhA.png"/></div></figure><p id="7e0f" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">然后，我们计算损失函数的平均值:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi oo"><img src="../Images/93753bda9f0ed15953fecdc88e5c3885.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*NHFTZrS8z8NkJc1iAWjMEQ.png"/></div></div></figure><p id="9bc5" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">现在可以最小化逐点损失的期望值，得到f的表达式:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi op"><img src="../Images/19e3a69f7a70b309439b91a782715085.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*38pCvJb5szNVQcDu-OkdZg.png"/></div></figure></div><div class="ab cl nz oa hu ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="ij ik il im in"><h2 id="3f67" class="nj ls iq bd lt nk nl dn lx nm nn dp mb kj no np mf kn nq nr mj kr ns nt mn nu bi translated">案例研究:贝叶斯分类器</h2><p id="b827" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku jx ij bi translated">损失函数的一个非常常见的例子是零一损失。如果类谓词是正确的，那么loss是0。如果是错的，损失是1:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/0f054c6a96c54e157f50234eb0922abf.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*hdjyAQtaHA5xDQ-BeaPiZg.png"/></div></figure><p id="ae99" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">然后，用<em class="nd"> δ </em>作为克罗内克δ，期望损失为:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi or"><img src="../Images/0467dd125754c4042573297b45a49c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*gC3M5WYG2Pe59ndMNa17lQ.png"/></div></div></figure><p id="7ea2" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">因此，对于每个<em class="nd"> x </em>，最小化<em class="nd"> 1−ℙ(f(X)|X) </em>的<em class="nd"> f </em>的最佳选择为:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi os"><img src="../Images/0d9a79ef71c0905b34897b386a898825.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*qcFBIKSDYxuMc_Y2HF0hSw.png"/></div></figure><p id="c122" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">这个预测器被称为贝叶斯分类器。由于这些“真实”的概率本质上是永远不知道的，因为在实践中很难计算出<em class="nd"> ℙ </em> (c|X=x)，这更多的是一个理论概念，而不是你可以实际使用的东西。通过对该量应用贝叶斯定理，然后假设所有参数相互独立:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/717887f96ec16c4997e50efcc20df8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*8sfX9aNYnbC5Ctoj6yITgw.png"/></div></figure><p id="9a61" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">这就是<a class="ae lq" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank"> <em class="nd">朴素贝叶斯分类器</em> </a>，然后可以在实践中实现。</p><h1 id="a2f8" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">一个问题，两种解决方案</h1><p id="6088" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku jx ij bi translated">我们在前面几节中研究的所有预测器都有明显的缺点。如果在输入数据<em class="nd"> X </em>中存在特殊结构，回归不能减少估计的偏差和方差。此外，如果<em class="nd"> X </em>的<a class="ae lq" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">尺寸较高</a>，回归会导致较大的误差。</p><p id="e5f4" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">我们的目标是找到对函数<em class="nd"> f(x) </em>的有用近似<em class="nd">f(x)</em>，该函数是具有大的和高维状态空间的输入和输出之间的预测和潜在结构化关系的基础。</p><p id="c86f" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">这个问题可以通过两种不同的方法来解决。</p><h2 id="2702" class="nj ls iq bd lt nk nl dn lx nm nn dp mb kj no np mf kn nq nr mj kr ns nt mn nu bi translated">统计估计量</h2><p id="d572" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku jx ij bi translated">应用数学和统计学采用的方法是从函数逼近和估计的角度。</p><p id="5a59" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">输入和输出被视为欧几里得空间中的点，并且预测估计器将这些点对映射到该空间的超平面中。估计量现在表示为:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/ed42beaa1b66649cbb5e1ffd0d2dea0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*v_RcGJalR0Sr1B2bvqGuNQ.png"/></div></figure><p id="b1d0" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">通过对拟合的模型类别施加一些严格的限制来选择估值器:<strong class="ka ir">粗糙度惩罚、核方法和基函数</strong>就是其中的一些。</p><h2 id="422a" class="nj ls iq bd lt nk nl dn lx nm nn dp mb kj no np mf kn nq nr mj kr ns nt mn nu bi translated">通过实例学习</h2><p id="8c58" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku jx ij bi translated">这种方法试图通过旨在最小化损失函数的教师来学习f。教师的目标是根据输入产生输出<em class="nd">f(Xi)</em>。在学习过程完成后，我们希望预测输出和实际输出足够接近，以便对实践中可能遇到的所有输入集都有用。</p><p id="70e1" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">这种方法通常被称为监督学习。<strong class="ka ir">据我们所知，机器学习旨在从技术上解决这个问题。</strong></p><p id="1be1" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated"><a class="ae lq" href="http://antoinechampion.com" rel="noopener ugc nofollow" target="_blank"> <em class="nd">安托万冠军2020年4月6日</em> </a></p></div><div class="ab cl nz oa hu ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="ij ik il im in"><h1 id="6b86" class="lr ls iq bd lt lu ov lw lx ly ow ma mb mc ox me mf mg oy mi mj mk oz mm mn mo bi translated">参考</h1><p id="edb0" class="pw-post-body-paragraph jy jz iq ka b kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr mt kt ku jx ij bi translated">哈斯蒂，特雷弗，蒂布拉尼，罗伯特和弗里德曼，杰罗姆。<em class="nd">统计学习的要素</em>。美国纽约州纽约市:斯普林格纽约公司，2001年。</p><p id="62a9" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">尼尔斯·理查德，<em class="nd">预测和分类</em>，哥本哈根大学，2006年</p><p id="acda" class="pw-post-body-paragraph jy jz iq ka b kb kv kd ke kf kw kh ki kj kx kl km kn ky kp kq kr kz kt ku jx ij bi translated">John L. Weatherwax，David Epstein，<em class="nd">解决方案手册和注释:统计学习的要素</em>，第2章，2020年</p></div></div>    
</body>
</html>