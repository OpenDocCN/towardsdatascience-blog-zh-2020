<html>
<head>
<title>Number of Parameters in a Feed-Forward Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">前馈神经网络中的参数数量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/number-of-parameters-in-a-feed-forward-neural-network-4e4e33a53655?source=collection_archive---------3-----------------------#2020-09-15">https://towardsdatascience.com/number-of-parameters-in-a-feed-forward-neural-network-4e4e33a53655?source=collection_archive---------3-----------------------#2020-09-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cd36" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">手动计算前馈神经网络中可训练参数的总数</h2></div><p id="eb82" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">今天，机器学习正在解决如此大量的复杂问题，看起来就像魔术一样。但是机器学习没有任何魔力，相反它有很强的数学和统计基础。</p><p id="094f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在试图理解机器学习的重要且有些困难的概念时，我们有时甚至不会考虑一些琐碎的概念。也许你会想到这些，但是我知道我经常忽略很多简单的事情。原因是令人惊叹的机器学习和深度学习库，它们具有为我们快速做到这一点的功能和方法。😍</p><p id="28fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个这样的小问题是手动找出前馈神经网络中可训练参数的总数。我在一次考试中遇到的一个问题，它让我对所提供的选项感到困惑。这个问题也被很多机器学习的从业者在很多不同的论坛上问过。🙋🏻</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/2f0fc75f90b56d94282bd5dd96f8fd21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4eP-lfGBMqQQlvznmqZVsQ.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">照片由<a class="ae lu" href="https://unsplash.com/@priscilladupreez" rel="noopener ugc nofollow" target="_blank">普里西拉</a>在<a class="ae lu" href="https://unsplash.com/photos/vDzeKnPBPLM" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d59c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lv">本帖讨论的问题是:</em></p><h1 id="1c68" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">如何求一个前馈神经网络中可训练参数的总数？</h1><p id="97d8" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">你一定想知道为什么这是一个需要讨论的重要问题。的确是！训练一个模型所需的时间取决于要训练的参数的数量，因此这些知识有时真的可以帮助我们。</p><p id="90f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过查看一个简单的网络，您可以很容易地计算并说出参数的数量。最坏的情况，你可以画出图，说出参数个数。但是当你遇到一个问题，一个神经网络有 7 层，每层的神经元数量不同，比如说 8，10，12，15，15，12，6，会怎么样呢？你怎么知道总共有多少个参数？</p><p id="c18b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们一起找到一个数学公式来得到计数。但是在开始计算之前，让我们先了解一下什么是前馈神经网络，它具有什么特征。这将帮助我们找到参数的总数。</p><blockquote class="mt mu mv"><p id="cd62" class="ki kj lv kk b kl km ju kn ko kp jx kq mw ks kt ku mx kw kx ky my la lb lc ld im bi translated"><strong class="kk iu">前馈神经网络是最简单的人工神经网络，其中感知机之间的连接不形成循环。</strong></p></blockquote><p id="0a77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管是最简单的神经网络，但它们对机器学习实践者来说极其重要，因为它们构成了当今使用的许多重要和高级应用的基础。🤘</p><p id="af37" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">前馈神经网络的特性；</p><ol class=""><li id="b564" class="mz na it kk b kl km ko kp kr nb kv nc kz nd ld ne nf ng nh bi translated">感知器是分层排列的。第一层接收输入，最后一层给出输出。中间层被称为隐藏层，因为它们对外部世界是隐藏的。</li><li id="1c19" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">一层中的每个感知器都连接到下一层的每个感知器。这就是信息不断从一层流向下一层的原因，因此得名前馈神经网络。</li><li id="8f40" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">同一层的感知器之间没有联系。</li><li id="9a9f" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">从当前层到前一层没有向后连接(称为反馈连接)。</li></ol><p id="ec26" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv">注:</em> </strong> <em class="lv">感知器是计算输入值加权和的神经网络的基本单元。</em></p><p id="d237" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数学上，前馈神经网络定义了映射<strong class="kk iu">y = f(x；θ) </strong>并学习有助于找到最佳函数近似的参数<strong class="kk iu"> <em class="lv"> θ </em> </strong>的值。</p><p id="9782" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv">注:</em> </strong> <em class="lv">前馈神经网络中除输出层外的所有层中也有一个偏置单元。通过将激活功能向左或向右移动，偏差对成功学习非常有帮助。迷茫？</em>🤔<em class="lv">简单来说，bias 类似于直线的线性方程 y = mx + c 中的截距(常数)，有助于预测线更好地拟合数据，而不是一条总是经过原点(0，0)的线(y = mx 的情况)。</em></p><p id="db7e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们利用这一知识来找出参数的个数。</p><p id="0255" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv">场景一:</em> </strong>一个只有一个隐层的前馈神经网络。输入层、隐藏层和输出层中的单元数分别为 3、4 和 2。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/4174ae7285bd26302e655514cc492888.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*osFo5MLmWLXE5rCOvwIiIA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">一个前馈神经网络(图片由作者提供)</p></figure><p id="b182" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">假设:</strong></p><p id="c023" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv"> i </em> </strong> =输入层的神经元数量</p><p id="18e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv"> h </em> </strong> =隐含层神经元的数量</p><p id="3e06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv"> o </em> </strong> =输出层神经元数</p><p id="02af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从图中，我们有<em class="lv"> i </em> = 3，<em class="lv"> h </em> = 4，<em class="lv"> o </em> = 2。请注意，红色神经元是该层的偏差。一层的每一个偏置都连接到下一层除了下一层偏置以外的所有神经元。</p><p id="e3c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数学上:</p><ol class=""><li id="b27c" class="mz na it kk b kl km ko kp kr nb kv nc kz nd ld ne nf ng nh bi translated">第一层和第二层的连接数:3 × 4 = 12，无非是<em class="lv"> i </em>和<em class="lv"> h </em>的乘积。</li><li id="afbf" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">第二层和第三层的连接数:4 × 2 = 8，无非是<em class="lv"> h </em>和<em class="lv"> o </em>的乘积。</li><li id="3f8a" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">层与层之间也有通过偏置的连接。第一层的 bias 和第二层的神经元之间的连接数(第二层的 bias 除外):1 × 4，无非就是<em class="lv"> h </em>。</li><li id="73fc" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">第二层偏置与第三层神经元的连接个数:1 × 2，无非是<em class="lv"> o </em>。</li></ol><p id="e5c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总结所有:</p><p id="ad0c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">3 × 4 + 4 × 2 + 1 × 4 + 1 × 2</p><p id="7a63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">= 12 + 8 + 4 + 2</p><p id="e614" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">= 26</p><p id="85ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，这个前馈神经网络总共有 26 个连接，因此将有 26 个可训练参数。</p><p id="e1df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们试着用这个等式来概括，找出一个公式。</p><p id="6714" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">3 × 4 + 4 × 2 + 1 × 4 + 1 × 2</p><p id="37dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">= 3 × 4 + 4 × 2 + 4 + 2</p><p id="595b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">=<em class="lv">I</em>×<em class="lv">h</em>+<em class="lv">h</em>×<em class="lv">o</em>+<em class="lv">h</em>+<em class="lv">o</em></p><p id="dcd1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">因此，具有一个隐藏层的前馈神经网络中的参数总数由下式给出:</strong></p><p id="3a2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">(<em class="lv">I</em>×<em class="lv">h</em>+<em class="lv">h</em>×<em class="lv">o</em>)<em class="lv"/>+<em class="lv">h</em>+<em class="lv">o</em></strong></p><p id="7cbf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于该网络是一个小型网络，因此也可以通过计算图中的连接数来得出总数。但是，如果层数更多呢？让我们再研究一个场景，看看这个公式是否有效，或者我们需要对它进行扩展。</p><p id="6800" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv">场景一:</em> </strong>一个具有三个隐层的前馈神经网络。输入层、第一隐藏层、第二隐藏层、第三隐藏层和输出层中的单元数分别为 3、5、6、4 和 2。</p><p id="5926" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">假设:</strong></p><p id="6a68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv"> i </em> </strong> =输入层的神经元数</p><p id="cadd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv"> h1 </em> </strong> =第一个隐藏层的神经元数量</p><p id="fd43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv"> h2 </em> </strong> =第二个隐藏层的神经元数量</p><p id="83f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv"> h3 </em> </strong> =第三隐藏层的神经元数量</p><p id="06a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv"> o </em> </strong> =输出层的神经元数量</p><ol class=""><li id="f307" class="mz na it kk b kl km ko kp kr nb kv nc kz nd ld ne nf ng nh bi translated">第一层和第二层的连接数:3 × 5 = 15，无非是<em class="lv"> i </em>和<em class="lv"> h1 </em>的乘积。</li><li id="cf7c" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">第二层和第三层的连接数:5 × 6 = 30，无非是<em class="lv"> h1 </em>和<em class="lv"> h2 </em>的乘积。</li><li id="ef56" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">第三层和第四层的连接数:6 × 4 = 24，无非是<em class="lv"> h2 </em>和 h3 的乘积。</li><li id="fcbd" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">第四层和第五层的连接数:4 × 2= 8，无非是<em class="lv"> h3 </em>和<em class="lv"> o </em>的乘积。</li><li id="e1bc" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">第一层的 bias 和第二层的神经元(第二层的 bias 除外)的连接个数:1 × 5 = 5，无非就是<em class="lv"> h1 </em>。</li><li id="500b" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">第二层偏置与第三层神经元的连接个数:1 × 6 = 6，无非是<em class="lv"> h2 </em>。</li><li id="107e" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">第三层偏置与第四层神经元的连接个数:1 × 4 = 4，无非是<em class="lv"> h3 </em>。</li><li id="21b4" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">第四层偏置与第五层神经元的连接个数:1 × 2 = 2，无非是<em class="lv"> o </em>。</li></ol><p id="df02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总结所有:</p><p id="7358" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">3 × 5 + 5 × 6 + 6 × 4 + 4 × 2 + 1 × 5 + 1 × 6 + 1 × 4 + 1 × 2</p><p id="6f0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">= 15 + 30 + 24 + 8 + 5 + 6 + 4 + 2</p><p id="7a7f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">= 94</p><p id="eeb2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，这个前馈神经网络总共有 94 个连接，因此有 94 个可训练参数。</p><p id="eb35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们试着用这个等式来概括，找出一个公式。</p><p id="268f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">3 × 5 + 5 × 6 + 6 × 4 + 4 × 2 + 1 × 5 + 1 × 6 + 1 × 4 + 1 × 2</p><p id="aa64" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">= 3 × 5 + 5 × 6 + 6 × 4 + 4 × 2 + 5 + 6 + 4 + 2</p><p id="bdf7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lv">= I</em>×<em class="lv">h1</em>+<em class="lv">h1</em>×<em class="lv">H2+H2</em>×<em class="lv">H3</em>+<em class="lv">H3</em>×<em class="lv">o</em>+<em class="lv">h1+H2+H3</em>+<em class="lv">o</em></p><p id="1b93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">因此，具有三个隐藏层的前馈神经网络中的参数总数由下式给出:</strong></p><p id="4c5e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">(<em class="lv">I</em>×<em class="lv">h1</em>+<em class="lv">h1</em>×<em class="lv">H2+H2</em>×<em class="lv">H3</em>+<em class="lv">H3</em>×<em class="lv">o</em>+<em class="lv">h1+H2+H3</em>+<em class="lv">o</em></strong></p><h2 id="2046" class="no lx it bd ly np nq dn mc nr ns dp mg kr nt nu mi kv nv nw mk kz nx ny mm nz bi translated"><span class="l oa ob oc bm od oe of og oh di"> T </span>因此，找到具有<em class="oi"> n </em>个隐藏层的前馈神经网络中可训练参数总数的公式由下式给出:</h2><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/c5f651dc50a849e13149729af8e79a81.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*U2ig8PVRqTtSES6sdv0ssA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">计算前馈神经网络中参数总数的公式(图片由作者提供)</p></figure><p id="965e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果这个公式听起来有点让人不知所措😳，别担心，没必要背这个公式🙅。请记住，为了找到参数的总数，我们需要总结以下内容:</p><ol class=""><li id="722a" class="mz na it kk b kl km ko kp kr nb kv nc kz nd ld ne nf ng nh bi translated">输入层和第一个隐藏层中神经元数量的乘积</li><li id="88a2" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">两个连续隐藏层之间神经元数量的乘积之和</li><li id="c0c8" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">最后一个隐藏层和输出层中神经元数量的乘积</li><li id="17e4" class="mz na it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">所有隐藏层和输出层中神经元数量的总和</li></ol><p id="137b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我希望你可以应用这种方法，即使不使用库，也可以找到具有任意数量的隐藏层和神经元的前馈神经网络中的参数总数。🙃</p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="e2ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">参考:</strong></p><p id="9716" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lu" href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/feedforward.html" rel="noopener ugc nofollow" target="_blank">https://cs . Stanford . edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/前馈. html </a></p><p id="7c99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谢谢大家！如果你对这篇文章有任何反馈或建议，请留下你的评论！</p><p id="27e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lu" href="https://www.linkedin.com/in/chetna-khanna/" rel="noopener ugc nofollow" target="_blank">领英</a></p></div></div>    
</body>
</html>