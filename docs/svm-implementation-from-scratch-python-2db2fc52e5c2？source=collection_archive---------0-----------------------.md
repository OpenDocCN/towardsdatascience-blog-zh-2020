# ä»é›¶å¼€å§‹çš„ SVMâ€”â€”Python

> åŸæ–‡ï¼š<https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2?source=collection_archive---------0----------------------->

## é‡è¦æ¦‚å¿µæ€»ç»“

![](img/ecfee175a47b8e5fbaaa5b44bdf67c1a.png)

å¦‚æœä½ æ˜ç™½äº†ï¼Œå¹²æ¯ğŸ˜‰

## æœ¬åšå®¢å°†æ¶µç›–çš„å†…å®¹:

1. [SVM ç®€ä»‹](#c22e)T2 2ã€‚[è¯»å–æ•°æ®é›†](#d7d8)
3ã€‚[ç‰¹è‰²å·¥ç¨‹](#9b54)
4ã€‚[æ‹†åˆ†æ•°æ®é›†](#1001)
5ã€‚[æˆæœ¬å‡½æ•°](#66a2)
6ã€‚[æˆæœ¬å‡½æ•°çš„æ¢¯åº¦](#72a3)
7ã€‚[åˆ—è½¦å‹å·ä½¿ç”¨ SGD](#163f)8ã€‚[åœæœºåˆ¤æ®ä¸º SGD](#80a3)9
ã€‚[æµ‹è¯•å‹å·](#b46d)
10ã€‚[å…·æœ‰ç›¸å…³æ€§çš„ç‰¹å¾é€‰æ‹©& P å€¼](#6a0c)
11ã€‚ç»™æˆ‘ä»£ç 

åœ¨æ·±å…¥ä»£ç æˆ–æŠ€æœ¯ç»†èŠ‚ä¹‹å‰ï¼Œæˆ‘æƒ³æä¸€ä¸‹ï¼Œè™½ç„¶æœ‰è®¸å¤šåº“/æ¡†æ¶å¯ç”¨äºå®ç° SVM(æ”¯æŒå‘é‡æœº)ç®—æ³•ï¼Œè€Œæ— éœ€ç¼–å†™å¤§é‡ä»£ç ï¼Œä½†æˆ‘å†³å®šç”¨å°½å¯èƒ½å°‘çš„é«˜çº§åº“æ¥ç¼–å†™ä»£ç ï¼Œä»¥ä¾¿æ‚¨å’Œæˆ‘å¯ä»¥å¾ˆå¥½åœ°æŒæ¡è®­ç»ƒ SVM æ¨¡å‹(å‡†ç¡®ç‡ä¸º 99%ï¼Œå¬å›ç‡ä¸º 0.98%ï¼Œç²¾ç¡®åº¦ä¸º 0.98%)æ‰€æ¶‰åŠçš„é‡è¦ç»„ä»¶ã€‚å¦‚æœä½ æ­£åœ¨å¯»æ‰¾ SVM çš„å¿«é€Ÿå®ç°ï¼Œé‚£ä¹ˆä½ æœ€å¥½ä½¿ç”¨åƒ [scikit-learn](https://scikit-learn.org/) ã€ [cvxopt](https://cvxopt.org/) ç­‰è¿™æ ·çš„åŒ…ã€‚è¦ä¸ï¼Œæˆ‘ä»¬å¼€å§‹å§ï¼

# SVM ç®€ä»‹

![](img/adfc6f10cc218c5b8e01d7c1f57d0610.png)

**å›¾ 1:** SVM ç”¨ä¸€å¼ å›¾è¡¨æ¦‚æ‹¬â€” [Ireneli.eu](http://ireneli.eu)

**SVM** (æ”¯æŒå‘é‡æœº)æ˜¯ä¸€ç§**ç›‘ç£çš„** **æœºå™¨å­¦ä¹ **ç®—æ³•ï¼Œé€šå¸¸ç”¨äº**äºŒå…ƒåˆ†ç±»é—®é¢˜**ã€‚å®ƒé€šè¿‡è¾“å…¥å¸¦æœ‰*æ ‡ç­¾ç¤ºä¾‹çš„*æ•°æ®é›†*æ¥è®­ç»ƒ(xáµ¢ï¼Œyáµ¢).*ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨çš„ç¤ºä¾‹æ˜¯ç”µå­é‚®ä»¶ï¼Œè€Œæ‚¨çš„é—®é¢˜æ˜¯åƒåœ¾é‚®ä»¶æ£€æµ‹ï¼Œé‚£ä¹ˆ:

*   ç¤ºä¾‹ç”µå­é‚®ä»¶æ¶ˆæ¯ ***xáµ¢*** è¢«å®šä¹‰ä¸ºèƒ½å¤Ÿåœ¨ n ç»´ç©ºé—´ä¸Šç»˜åˆ¶çš„ *n ç»´ç‰¹å¾å‘é‡*ã€‚
*   ç‰¹å¾å‘é‡ï¼Œé¡¾åæ€ä¹‰ï¼ŒåŒ…å«ç‰¹å¾(å¦‚å­—æ•°ã€é“¾æ¥æ•°ç­‰ã€‚)ä»¥æ•°å­—å½¢å¼æ˜¾ç¤º
*   æ¯ä¸ªç‰¹å¾å‘é‡éƒ½æ ‡æœ‰ä¸€ä¸ªç±»åˆ«
*   *ç±»åˆ«*å¯ä»¥æ˜¯+ve æˆ–-ve(ä¾‹å¦‚ï¼Œspam=1ï¼Œè€Œé-spam=-1)**

**ä½¿ç”¨è¿™ä¸ª*æ•°æ®é›†*ï¼Œç®—æ³•*æ‰¾åˆ°ä¸€ä¸ªè¶…å¹³é¢*(æˆ–å†³ç­–è¾¹ç•Œ)ï¼Œè¯¥è¶…å¹³é¢ç†æƒ³åœ°åº”è¯¥å…·æœ‰ä»¥ä¸‹å±æ€§:**

*   **å®ƒä»¥æœ€å¤§çš„è£•åº¦åœ¨ä¸¤ä¸ªç±»çš„ç¤ºä¾‹ä¹‹é—´åˆ›å»ºäº†åˆ†ç¦»**
*   **å®ƒçš„ç­‰å¼ *(w.x + b = 0)* å¯¹äºæ¥è‡ª+ve ç±»çš„ä¾‹å­äº§ç”Ÿâ‰¥ 1 çš„å€¼ï¼Œå¯¹äºæ¥è‡ª-ve ç±»çš„ä¾‹å­äº§ç”Ÿâ‰¤-1 çš„å€¼**

****å®ƒæ˜¯æ€ä¹ˆæ‰¾åˆ°è¿™ä¸ªè¶…å¹³é¢çš„ï¼Ÿ**é€šè¿‡æ‰¾åˆ°å®šä¹‰è¯¥è¶…å¹³é¢çš„æœ€ä¼˜å€¼ *w*(æƒé‡/æ³•çº¿)*å’Œ*b ****(æˆªè·)*ã€‚é€šè¿‡*æœ€å°åŒ–æˆæœ¬å‡½æ•°æ‰¾åˆ°æœ€ä½³å€¼ã€‚*ä¸€æ—¦ç®—æ³•è¯†åˆ«å‡ºè¿™äº›æœ€ä¼˜å€¼ï¼Œåˆ™ *SVM æ¨¡å‹ f(x)* è¢«å®šä¹‰å¦‚ä¸‹:***

**![](img/a30db037ba57829d938f259e47a0a9ce.png)**

**æ•°å­¦è¡¨è¾¾çš„ SVM æ¨¡å‹**

**åœ¨æˆ‘ä»¬ç»§ç»­ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å¯¼å…¥æœ¬æ•™ç¨‹æ‰€éœ€çš„åŒ…ï¼Œå¹¶åˆ›å»ºæˆ‘ä»¬ç¨‹åºçš„æ¡†æ¶ *svm.py* :**

```
*****# svm.py***
import numpy as np  *# for handling multi-dimensional array operation*
import pandas as pd  *# for reading data from csv* 
import statsmodels.api as sm  *# for finding the p-value*
from sklearn.preprocessing import MinMaxScaler  *# for normalization*
from sklearn.model_selection import train_test_split as tts
from sklearn.metrics import accuracy_score 
from sklearn.utils import shuffle*# >> FEATURE SELECTION << #*
def remove_correlated_features(X):
def remove_less_significant_features(X, Y):*# >> MODEL TRAINING << #*
def compute_cost(W, X, Y):
def calculate_cost_gradient(W, X_batch, Y_batch):
def sgd(features, outputs):def init():**
```

# **è¯»å–æ•°æ®é›†**

**æˆ‘ä»¬å°†ä½¿ç”¨ [Kaggle](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data) ä¸Šçš„ä¹³è…ºç™Œæ•°æ®é›†ã€‚æ•°æ®é›†ä¸­çš„ç‰¹å¾æ˜¯æ ¹æ®ä¹³è…ºè‚¿å—ç»†é’ˆæŠ½å¸(FNA)çš„æ•°å­—åŒ–å›¾åƒè®¡ç®—å¾—å‡ºçš„ã€‚å®ƒä»¬æè¿°äº†å›¾åƒä¸­å‡ºç°çš„ç»†èƒæ ¸çš„ç‰¹å¾ã€‚åŸºäºè¿™äº›ç‰¹å¾ï¼Œæˆ‘ä»¬å°†è®­ç»ƒæˆ‘ä»¬çš„ SVM æ¨¡å‹æ¥æ£€æµ‹è‚¿å—æ˜¯è‰¯æ€§çš„ **B** (ä¸€èˆ¬æ— å®³)è¿˜æ˜¯æ¶æ€§çš„ **M** (ç™Œå˜)ã€‚**

**ä¸‹è½½æ•°æ®é›†å¹¶å°† *data.csv* æ–‡ä»¶æ”¾åœ¨ä¸ *svm.py* ç›¸åŒçš„æ–‡ä»¶å¤¹ä¸­ã€‚ç„¶åå°†è¿™æ®µä»£ç æ·»åŠ åˆ° *init()* å‡½æ•°ä¸­:**

```
****def init():
**    data = pd.read_csv('./data.csv') *# SVM only accepts numerical values. 
    # Therefore, we will transform the categories M and B into
    # values 1 and -1 (or -1 and 1), respectively.*
    diagnosis_map = {'M':1, 'B':-1}
    data['diagnosis'] = data['diagnosis'].map(diagnosis_map) # drop last column (extra column added by pd)
    # and unnecessary first column (id)
    data.drop(data.columns[[-1, 0]], axis=1, inplace=True)**
```

**[*read _ CSV()*](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)Pandas[åŒ…çš„å‡½æ•°ä»ã€‚csv æ–‡ä»¶å¹¶å°†å…¶å­˜å‚¨åœ¨*æ•°æ®å¸§*å’Œ**ä¸­ã€‚**æŠŠDataFrame æƒ³è±¡æˆä¸€ä¸ªæ•°æ®ç»“æ„çš„å®ç°ï¼Œå®ƒçœ‹èµ·æ¥åƒä¸€ä¸ªå¸¦æœ‰æ ‡è®°çš„åˆ—å’Œè¡Œçš„è¡¨æ ¼ã€‚ä¸‹é¢æ˜¯ä» *data.csv* ä¸­è¯»å–çš„æ•°æ®åœ¨ DataFrame ä¸­çš„æ ·å­:](https://pandas.pydata.org/)**

**![](img/4c97672ab2dc0a3b5e8949253a7346e1.png)**

****å›¾ 2:** åœ¨ PyCharm IDE ä¸­æŸ¥çœ‹çš„æ•°æ®å¸§ä¸­ä¹³è…ºç™Œæ•°æ®é›†çš„ä¸€éƒ¨åˆ†**

**å¦‚æ‚¨æ‰€è§ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†çš„æ ‡ç­¾å’Œç»“æ„å¾—ä»¥ä¿ç•™ï¼Œè¿™ä½¿å¾— DataFrame å˜å¾—ç›´è§‚ã€‚**

# **ç‰¹å¾å·¥ç¨‹**

**æœºå™¨å­¦ä¹ ç®—æ³•åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šæ“ä½œï¼Œè¯¥æ•°æ®é›†æ˜¯ä¸€ä¸ªç”±[ç‰¹å¾](https://developers.google.com/machine-learning/crash-course/framing/ml-terminology#features)å’Œ[æ ‡ç­¾](https://developers.google.com/machine-learning/crash-course/framing/ml-terminology#labels)ç»„æˆçš„å¸¦æ ‡ç­¾çš„ä¾‹å­çš„é›†åˆï¼Œå³åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹*è¯Šæ–­*æ˜¯ä¸€ä¸ªæ ‡ç­¾ï¼Œã€*åŠå¾„ _ å¹³å‡*ï¼Œ*ç»“æ„ _ å¹³å‡*ï¼Œ*çº¹ç† _ å¹³å‡â€¦â€¦*ç‰¹å¾ï¼Œå¹¶ä¸”æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªä¾‹å­ã€‚**

**åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ‚¨æœ€åˆæ”¶é›†çš„æ•°æ®å¯èƒ½æ˜¯åŸå§‹çš„ï¼›å®ƒè¦ä¹ˆä¸ä½ çš„æ¨¡å‹ä¸å…¼å®¹ï¼Œè¦ä¹ˆå¦¨ç¢å®ƒçš„æ€§èƒ½ã€‚è¿™æ—¶**ç‰¹å¾å·¥ç¨‹**æ¥æ‹¯æ•‘ã€‚å®ƒåŒ…æ‹¬é¢„å¤„ç†æŠ€æœ¯ï¼Œé€šè¿‡ä»åŸå§‹æ•°æ®ä¸­æå–ç‰¹å¾æ¥ç¼–è¯‘æ•°æ®é›†ã€‚è¿™äº›æŠ€æœ¯æœ‰ä¸¤ä¸ªå…±åŒçš„ç‰¹ç‚¹:**

*   **å‡†å¤‡ä¸æ¨¡å‹å…¼å®¹çš„æ•°æ®**
*   **æé«˜æœºå™¨å­¦ä¹ ç®—æ³•çš„æ€§èƒ½**

****æ ‡å‡†åŒ–**æ˜¯æˆ‘ä»¬å°†è¦ä½¿ç”¨çš„ä¼—å¤šç‰¹å¾å·¥ç¨‹æŠ€æœ¯ä¹‹ä¸€ã€‚å½’ä¸€åŒ–æ˜¯å°†æ•°å€¼èŒƒå›´è½¬æ¢ä¸ºæ ‡å‡†æ•°å€¼èŒƒå›´çš„è¿‡ç¨‹ï¼Œé€šå¸¸åœ¨åŒºé—´[1ï¼Œ1]æˆ–[0ï¼Œ1]å†…ã€‚è¿™ä¸æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¦æ±‚ï¼Œä½†å®ƒæé«˜äº†å­¦ä¹ çš„é€Ÿåº¦(ä¾‹å¦‚ï¼Œåœ¨æ¢¯åº¦ä¸‹é™ä¸­æ›´å¿«çš„æ”¶æ•›)å¹¶é˜²æ­¢æ•°å€¼æº¢å‡ºã€‚åœ¨ *init()* å‡½æ•°ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç ï¼Œä»¥è§„èŒƒåŒ–æ‚¨çš„æ‰€æœ‰åŠŸèƒ½:**

```
*****# inside init()*** *# put features & outputs in different DataFrames for convenience* Y = data.loc[:, 'diagnosis']  # all rows of 'diagnosis' 
X = data.iloc[:, 1:]  # all rows of column 1 and ahead (features)*# normalize the features using MinMaxScalar from
# sklearn.preprocessing* X_normalized = MinMaxScaler().fit_transform(X.values)
X = pd.DataFrame(X_normalized)**
```

# **åˆ†å‰²æ•°æ®é›†**

**æˆ‘ä»¬å°†ä½¿ç”¨*sk learn . model _ selection*ä¸­çš„ *train_test_split()* å‡½æ•°å°†æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå•ç‹¬çš„æ•°æ®é›†æ¥è¿›è¡Œæµ‹è¯•ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦çœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹åœ¨çœ‹ä¸è§çš„è§‚å¯Ÿä¸Šè¡¨ç°å¦‚ä½•ã€‚å°†æ­¤ä»£ç æ·»åŠ åˆ° *init()* ä¸­:**

```
*****# inside init()****# first insert 1 in every row for intercept b*
X.insert(loc=len(X.columns), column='intercept', value=1)# test_size is the portion of data that will go into test set
# random_state is the seed used by the random number generator
print("splitting dataset into train and test sets...")
X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)**
```

**å¦‚æœä½ å¯¹æˆ‘ä»¬ä¸ºä»€ä¹ˆåœ¨æ¯ä¸€è¡ŒåŠ  1 æ„Ÿåˆ°å›°æƒ‘ï¼Œé‚£ä¹ˆä¸è¦æ‹…å¿ƒã€‚ä½ ä¼šåœ¨ä¸‹ä¸€èŠ‚å¾—åˆ°ç­”æ¡ˆã€‚**

# **ä»·å€¼å‡½æ•°**

**![](img/405d429b161a65d712f8e376f33d1c3a.png)**

****å›¾ 3:** H1 å¹¶æ²¡æœ‰æŠŠé˜¶çº§åˆ†å¼€ã€‚H2 æœ‰ï¼Œä½†å¹…åº¦å¾ˆå°ã€‚H3 ä»¥æœ€å¤§çš„å·®è·å°†ä»–ä»¬åˆ†å¼€â€” [ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Support-vector_machine)**

**ä¹Ÿç§°ä¸ºç›®æ ‡å‡½æ•°ã€‚æ¯ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•çš„æ„å»ºæ¨¡å—ä¹‹ä¸€ï¼Œå®ƒæ˜¯æˆ‘ä»¬è¯•å›¾*æœ€å°åŒ–æˆ–æœ€å¤§åŒ–*ä»¥å®ç°æˆ‘ä»¬ç›®æ ‡çš„å‡½æ•°ã€‚**

**æˆ‘ä»¬åœ¨ SVM çš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªè¶…å¹³é¢ï¼Œå®ƒä»¥æœ€å¤§çš„è£•åº¦åˆ†éš”+ve å’Œ-ve ç¤ºä¾‹ï¼ŒåŒæ—¶ä¿æŒå°½å¯èƒ½ä½çš„é”™è¯¯åˆ†ç±»(è§å›¾ 3)ã€‚**

**æˆ‘ä»¬å¦‚ä½•å®ç°è¿™ä¸ªç›®æ ‡ï¼Ÿæˆ‘ä»¬å°†æœ€å°åŒ–æˆæœ¬/ç›®æ ‡å‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤º:**

**![](img/f744e7b912c07d83bcfb5011098ab593.png)**

**åœ¨è®­ç»ƒé˜¶æ®µï¼Œè¾ƒå¤§çš„ C å¯¼è‡´è¾ƒçª„çš„è£•åº¦(å¯¹äºæ— é™å¤§çš„ Cï¼ŒSVM å˜æˆç¡¬è£•åº¦)ï¼Œè€Œè¾ƒå°çš„ C å¯¼è‡´è¾ƒå®½çš„è£•åº¦ã€‚**

**æ‚¨å¯èƒ½è§è¿‡å¦ä¸€ä¸ªç‰ˆæœ¬çš„æˆæœ¬å‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤º:**

**![](img/a0971dbbe8abf3283c75bff8386bfd8a.png)**

**è¾ƒå¤§çš„Î»ç»™å‡ºè¾ƒå®½çš„ä½™é‡ï¼Œè¾ƒå°çš„Î»å¯¼è‡´è¾ƒçª„çš„ä½™é‡(å¯¹äºæ— é™å°çš„Î»ï¼ŒSVM å˜æˆç¡¬ä½™é‡)ã€‚**

**åœ¨è¿™ä¸ªæˆæœ¬å‡½æ•°ä¸­ï¼ŒÎ»åŸºæœ¬ä¸Šç­‰äº *1/C* ï¼Œå¹¶ä¸”å…·æœ‰ç›¸åçš„æ•ˆæœï¼Œå³è¾ƒå¤§çš„Î»ç»™å‡ºè¾ƒå®½çš„ä½™é‡ï¼Œåä¹‹äº¦ç„¶ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•ä¸Šè¿°æˆæœ¬å‡½æ•°ï¼Œè®°ä½æ¯ä¸ªæ­£åˆ™åŒ–å‚æ•°(C å’ŒÎ»)çš„ä½œç”¨ï¼Œç„¶åç›¸åº”åœ°è°ƒæ•´å®ƒä»¬ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è®¡ç®—(1)ä¸­ç»™å‡ºçš„æ€»æˆæœ¬ï¼Œç„¶åæˆ‘ä»¬å°†ç»§ç»­è®¨è®ºå…¶æ¢¯åº¦ï¼Œè¯¥æ¢¯åº¦å°†ç”¨äºè®­ç»ƒé˜¶æ®µä»¥ä½¿å…¶æœ€å°åŒ–:**

```
****def compute_cost(W, X, Y):**
    # calculate hinge loss
    N = X.shape[0]
    distances = 1 - Y * (np.dot(X, W))
    distances[distances < 0] = 0  # equivalent to max(0, distance)
    hinge_loss = reg_strength * (np.sum(distances) / N)

    # calculate cost
    cost = 1 / 2 * np.dot(W, W) + hinge_loss
    return cost**
```

**æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œæˆªè·æœ¯è¯­ *b* ä¸è§äº†ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬æŠŠå®ƒæ¨è¿›äº†é‡é‡å‘é‡ï¼Œå°±åƒè¿™æ ·:**

**![](img/df8e5e192f3f9a2d865ec830fd501363.png)**

**å°†æˆªè·é¡¹æ¨å…¥æƒé‡å‘é‡ã€‚**

**è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨æ‹†åˆ†æ•°æ®é›†ä¹‹å‰æ·»åŠ äº†ä¸€ä¸ªå…¨ 1 çš„é¢å¤–åˆ—ã€‚åœ¨æœ¬æ•™ç¨‹çš„å‰©ä½™éƒ¨åˆ†ï¼Œè¯·è®°ä½è¿™ä¸€ç‚¹ã€‚**

# **æˆæœ¬å‡½æ•°çš„æ¢¯åº¦**

**![](img/17b422535e4b39f9e56e6342379f78ca.png)**

**æˆæœ¬å‡½æ•°çš„æ¢¯åº¦**

**æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œç­‰å¼(4)ä¸­çš„æˆæœ¬å‡½æ•°æœ‰ä¸€äº›å˜åŒ–ã€‚ä¸è¦æ‹…å¿ƒï¼Œå¦‚æœä½ è§£æåœ°è§£å†³å®ƒï¼Œå®ƒæ˜¯ç›¸åŒçš„ã€‚ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ç­‰å¼(5)å®ç°*calculate _ cost _ gradient()*å‡½æ•°:**

```
**# I haven't tested it but this same function should work for
# vanilla and mini-batch gradient descent as well
**def calculate_cost_gradient(W, X_batch, Y_batch):**
    # if only one example is passed (eg. in case of SGD)
    if type(Y_batch) == np.float64:
        Y_batch = np.array([Y_batch])
        X_batch = np.array([X_batch]) distance = 1 - (Y_batch * np.dot(X_batch, W))
    dw = np.zeros(len(W)) for ind, d in enumerate(distance):
        if max(0, d) == 0:
            di = W
        else:
            di = W - (reg_strength * Y_batch[ind] * X_batch[ind])
        dw += di dw = dw/len(Y_batch)  # average
    return dw**
```

# **ä½¿ç”¨ SGD è®­ç»ƒæ¨¡å‹**

**è®°å¾—æˆ‘åœ¨ä¸Šé¢è¯´è¿‡ï¼Œâ€œä¸ºäº†å®ç°æˆ‘ä»¬çš„ç›®æ ‡ï¼Œæˆ‘ä»¬è¯•å›¾*æœ€å°åŒ–æˆ–æœ€å¤§åŒ–æˆæœ¬å‡½æ•°â€*ã€‚åœ¨ SVM ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬æœ€å°åŒ–æˆæœ¬å‡½æ•°ã€‚**

****æˆ‘ä»¬ä¸ºä»€ä¹ˆè¦æœ€å°åŒ–æˆæœ¬å‡½æ•°ï¼Ÿå› ä¸ºæˆæœ¬å‡½æ•°æœ¬è´¨ä¸Šæ˜¯è¡¡é‡æˆ‘ä»¬çš„æ¨¡å‹åœ¨å®ç°ç›®æ ‡æ–¹é¢åšå¾—æœ‰å¤šå·®çš„æŒ‡æ ‡ã€‚å¦‚æœä½ ä»”ç»†è§‚å¯Ÿ J(w)ï¼Œä¸ºäº†æ‰¾åˆ°å®ƒçš„æœ€å°å€¼ï¼Œæˆ‘ä»¬å¿…é¡»:****

1.  ***æœ€å°åŒ–*âˆ£âˆ£wâˆ£âˆ£*t13ã€‘å“ªä¸ª*æœ€å¤§åŒ–è¾¹è·* (2/âˆ£âˆ£wâˆ£âˆ£)***
2.  **æœ€å°åŒ–é“°é“¾æŸè€—çš„æ€»å’Œ*ï¼Œå…¶ä¸­*æœ€å°åŒ–é”™è¯¯åˆ†ç±»*ã€‚*

*![](img/5ef4983416d2c16d5a71decf97f7ef1c.png)*

*é“°é“¾æŸå¤±å‡½æ•°*

*å› ä¸ºæˆ‘ä»¬çš„ä¸¤ä¸ª SVM ç›®æ ‡éƒ½æ˜¯é€šè¿‡æœ€å°åŒ–æˆæœ¬å‡½æ•°æ¥å®ç°çš„ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦æœ€å°åŒ–å®ƒã€‚*

*æˆ‘ä»¬å¦‚ä½•å°†å®ƒæœ€å°åŒ–ï¼Ÿå—¯ï¼Œæœ‰å¤šç§æ–¹æ³•ï¼Œä½†æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§å«åšéšæœºæ¢¯åº¦ä¸‹é™æˆ– SGD çš„æ–¹æ³•ã€‚åœ¨æ·±å…¥ SGD ä¹‹å‰ï¼Œæˆ‘é¦–å…ˆç®€å•è§£é‡Šä¸€ä¸‹æ¢¯åº¦ä¸‹é™æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚*

*![](img/3261cd0d993540a48da243bc3b8162e5.png)*

***å›¾ 4:** æ¢¯åº¦ä¸‹é™*

*æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å·¥ä½œåŸç†å¦‚ä¸‹:*

1.  *æ‰¾åˆ°æˆæœ¬å‡½æ•°çš„æ¢¯åº¦ï¼Œå³âˆ‡J(w')*
2.  *ä»¥ä¸€å®šçš„é€Ÿç‡å‘æ¢¯åº¦çš„ç›¸åæ–¹å‘ç§»åŠ¨ï¼Œå³ w' = w' â€” âˆ(âˆ‡J(w'))*
3.  *é‡å¤æ­¥éª¤ 1-3ï¼Œç›´åˆ°æ”¶æ•›ï¼Œå³æˆ‘ä»¬å‘ç° w 'å…¶ä¸­ J(w)æœ€å°*

*ä¸ºä»€ä¹ˆå®ƒçš„è¿åŠ¨æ–¹å‘ä¸æ¢¯åº¦æ–¹å‘ç›¸åï¼Ÿå› ä¸ºæ¢¯åº¦æ˜¯å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘ã€‚æˆ‘ä»¬éœ€è¦å‘ç›¸åçš„æ–¹å‘ç§»åŠ¨ï¼Œä½¿å‡½æ•° J(w)æœ€å°ã€‚å› æ­¤ï¼Œåœ¨æ¢¯åº¦ä¸‹é™ä¸­ä½¿ç”¨â€œä¸‹é™â€ä¸€è¯ã€‚*

*åœ¨å…¸å‹çš„æ¢¯åº¦ä¸‹é™(åˆåæ™®é€šæ¢¯åº¦ä¸‹é™)ä¸­ï¼Œä½¿ç”¨æ‰€æœ‰ç¤ºä¾‹(1â€¦N)è®¡ç®—ä¸Šè¿°æ­¥éª¤ 1ã€‚ç„¶è€Œï¼Œåœ¨ SGD ä¸­ï¼Œä¸€æ¬¡åªä½¿ç”¨ä¸€ä¸ªç¤ºä¾‹ã€‚æˆ‘ä¸ä¼šåœ¨è¿™é‡Œè®¨è®º SGD çš„å¥½å¤„ï¼Œä½†æ˜¯ä½ å¯ä»¥åœ¨è¿™ä¸ªåšå®¢çš„æœ«å°¾æ‰¾åˆ°ä¸€äº›æœ‰ç”¨çš„é“¾æ¥ã€‚ä¸‹é¢æ˜¯å¦‚ä½•ç”¨ä»£ç å®ç° SGD:*

```
***def sgd(features, outputs):
**    max_epochs = 5000
    weights = np.zeros(features.shape[1])
    *# stochastic gradient descent*
    for epoch in range(1, max_epochs): 
        *# shuffle to prevent repeating update cycles*
        X, Y = shuffle(features, outputs)
        for ind, x in enumerate(X):
            ascent = calculate_cost_gradient(weights, x, Y[ind])
            weights = weights - (learning_rate * ascent)

    return weights*
```

*è®©æˆ‘ä»¬é€šè¿‡æ·»åŠ ä»¥ä¸‹ä»£ç åœ¨ *init()* å‡½æ•°å†…éƒ¨è°ƒç”¨å®ƒ:*

```
****# inside init()****# train the model*
print("training started...")
W = sgd(X_train.to_numpy(), y_train.to_numpy())
print("training finished.")
print("weights are: {}".format(W))*
```

# *SGD çš„åœæ­¢æ ‡å‡†*

*åœ¨ä¸Šé¢çš„ *sgd()* å®ç°ä¸­ï¼Œæˆ‘ä»¬è¿è¡Œäº† 5000 æ¬¡å¾ªç¯(å¯èƒ½æ˜¯ä»»ä½•æ¬¡æ•°)ã€‚æ¯æ¬¡è¿­ä»£éƒ½èŠ±è´¹æˆ‘ä»¬æ—¶é—´å’Œé¢å¤–çš„è®¡ç®—ã€‚æˆ‘ä»¬ä¸éœ€è¦å®Œæˆæ‰€æœ‰çš„è¿­ä»£ã€‚å½“æ»¡è¶³ä¸­æ–­æ ‡å‡†æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç»ˆæ­¢å¾ªç¯ã€‚*

***åœå·¥æ ‡å‡†åº”è¯¥æ˜¯ä»€ä¹ˆï¼Ÿ**æœ‰å¤šä¸ªé€‰é¡¹ï¼Œä½†æˆ‘ä»¬å°†ä½¿ç”¨æœ€ç®€å•çš„ä¸€ä¸ªã€‚ä¸ä¹‹å‰çš„æˆæœ¬ç›¸æ¯”ï¼Œå½“å½“å‰çš„æˆæœ¬æ²¡æœ‰é™ä½å¾ˆå¤šæ—¶ï¼Œæˆ‘ä»¬å°†åœæ­¢åŸ¹è®­ã€‚ä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•ç”¨åœæ­¢æ ‡å‡†å®šä¹‰ *sgd()* :*

```
***def sgd(features, outputs):**
    max_epochs = 5000
    weights = np.zeros(features.shape[1])
    nth = 0
    prev_cost = float("inf")
    cost_threshold = 0.01  # in percent
    # stochastic gradient descent
    for epoch in range(1, max_epochs):
        # shuffle to prevent repeating update cycles
        X, Y = shuffle(features, outputs)
        for ind, x in enumerate(X):
            ascent = calculate_cost_gradient(weights, x, Y[ind])
            weights = weights - (learning_rate * ascent) # convergence check on 2^nth epoch
        if epoch == 2 ** nth or epoch == max_epochs - 1:
            cost = compute_cost(weights, features, outputs)
            print("Epoch is:{} and Cost is: {}".format(epoch, cost))
            # stoppage criterion
            if abs(prev_cost - cost) < cost_threshold * prev_cost:
                return weights
            prev_cost = cost
            nth += 1
    return weights*
```

# *æµ‹è¯•æ¨¡å‹*

*åœ¨ä½¿ç”¨ SGD è®­ç»ƒæ¨¡å‹ä¹‹åï¼Œæˆ‘ä»¬æœ€ç»ˆè·å¾—äº†æœ€ä¼˜æƒé‡ **w*** ï¼Œå…¶å®šä¹‰äº†åˆ†ç¦»ä¸¤ä¸ªç±»çš„æœ€ä½³å¯èƒ½è¶…å¹³é¢ã€‚è®©æˆ‘ä»¬ç”¨è¿™ä¸ªè¶…å¹³é¢æ¥æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹ã€‚å°†æ­¤ä»£ç æ·»åŠ åˆ° *init()* å‡½æ•°ä¸­:*

```
****# inside init()****# testing the model on test set*
y_test_predicted = np.array([])
for i in range(X_test.shape[0]):
    yp = np.sign(np.dot(W, X_test.to_numpy()[i])) #model
    y_test_predicted = np.append(y_test_predicted, yp)print("accuracy on test dataset: {}".format(accuracy_score(y_test.to_numpy(), y_test_predicted)))
print("recall on test dataset: {}".format(recall_score(y_test.to_numpy(), y_test_predicted)))
print("precision on test dataset: {}".format(recall_score(y_test.to_numpy(), y_test_predicted)))*
```

*ç°åœ¨è®©æˆ‘ä»¬è°ƒç”¨ *init()* å‡½æ•°:*

```
**# set hyper-parameters and call init
# hyper-parameters are normally tuned using cross-validation
# but following work good enough*
reg_strength = 10000 # regularization strength
learning_rate = 0.000001
init()*
```

*ä»¥ä¸‹æ˜¯è¾“å‡ºç»“æœ:*

```
***# OUTPUT**
reading dataset...
applying feature engineering...
splitting dataset into train and test sets...
training started...
Epoch is: 1 and Cost is: 5333.266133501857
Epoch is: 2 and Cost is: 3421.9128432834573
Epoch is: 4 and Cost is: 2437.2790231100216
Epoch is: 8 and Cost is: 1880.2998267933792
Epoch is: 16 and Cost is: 1519.5578612139725
Epoch is: 32 and Cost is: 1234.642324549297
Epoch is: 64 and Cost is: 977.3285621274708
Epoch is: 128 and Cost is: 804.8893546235923
Epoch is: 256 and Cost is: 703.407799431284
Epoch is: 512 and Cost is: 645.8275191300031
Epoch is: 1024 and Cost is: 631.6024252740094
Epoch is: 2048 and Cost is: 615.8378582171482
Epoch is: 4096 and Cost is: 605.0990964730645
Epoch is: 4999 and Cost is: 606.8186618758745
training finished.
weights are: [ 1.32516553  0.83500639  1.12489803  2.16072054 -1.24845441 -3.24246498
  3.27876342  6.83028706 -0.46562238  0.10063844  5.68881254 -1.93421932
  3.27394523  3.77331751  1.67333278 -2.43170464 -1.7683188   0.84065607
 -1.9612766  -1.84996828  2.69876618  5.32541102  1.0380137   3.0787769
  2.2140083  -0.61998182  2.66514199  0.02348447  4.64797917  2.17249278
 -9.27401088]
testing the model...
accuracy on test dataset: 0.9736842105263158
recall on test dataset: 0.9534883720930233
precision on test dataset: 0.9534883720930233*
```

*è¯·æ³¨æ„æ¨¡å‹çš„å‡†ç¡®æ€§ã€å¬å›ç‡å’Œç²¾ç¡®åº¦åˆ†æ•°ï¼Œä»¥åŠå®Œæˆæ¨¡å‹æ‰€éœ€çš„æ—¶æœŸã€‚ç°åœ¨è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ç‰¹å¾é€‰æ‹©æ¥æ”¹è¿›å®ƒã€‚*

# *å…·æœ‰ç›¸å…³æ€§å’Œ P å€¼çš„ç‰¹å¾é€‰æ‹©*

*ç‰¹å¾é€‰æ‹©åŒ…æ‹¬å¸®åŠ©è¿‡æ»¤ä¸ç›¸å…³æˆ–å†—ä½™ç‰¹å¾çš„ç»Ÿè®¡æŠ€æœ¯ã€‚ç›¸å…³æ€§å’Œ p å€¼å°±æ˜¯è¿™äº›ç»Ÿè®¡æŠ€æœ¯ä¸­çš„ä¸€ç§ã€‚ä½¿ç”¨å®ƒä»¬ï¼Œæˆ‘ä»¬å°†ä»æˆ‘ä»¬çš„åŸå§‹ç‰¹å¾é›†ä¸­é€‰æ‹©ä¸€ä¸ªç›¸å…³å’Œé‡è¦çš„ç‰¹å¾å­é›†*

***ä»€ä¹ˆæ˜¯ç›¸å…³æ€§ï¼Ÿ**ç›¸å…³æ€§æ˜¯ä¸¤ä¸ªå˜é‡ä¹‹é—´çº¿æ€§ä¾èµ–(æˆ–çº¿æ€§å…³ç³»)çš„ç¨‹åº¦ã€‚å¦‚æœä¸€ä¸ªç‰¹å¾çš„å€¼å¯ä»¥ç”¨ç¬¬äºŒä¸ªç‰¹å¾çš„æŸç§çº¿æ€§å…³ç³»æ¥è§£é‡Šï¼Œåˆ™ç§°ä¸¤ä¸ªç‰¹å¾ç›¸å…³ã€‚è¿™ç§å…³ç³»çš„ç¨‹åº¦ç”±**ç›¸å…³ç³»æ•°**(æˆ–â€œrâ€)ç»™å‡ºã€‚èŒƒå›´ä»-1.0 åˆ°+1.0ã€‚r è¶Šæ¥è¿‘+1 æˆ–-1ï¼Œè¿™ä¸¤ä¸ªå˜é‡çš„å…³ç³»å°±è¶Šå¯†åˆ‡ã€‚*

*ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦åˆ é™¤å…¶ä¸­ä¸€ä¸ªç›¸å…³ç‰¹å¾ï¼Ÿæœ‰å¤šç§åŸå› ï¼Œä½†æœ€ç®€å•çš„åŸå› æ˜¯ç›¸å…³ç‰¹å¾å¯¹å› å˜é‡çš„å½±å“å‡ ä¹ç›¸åŒã€‚æ­¤å¤–ï¼Œç›¸å…³ç‰¹å¾ä¸ä¼šæ”¹å–„æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè€Œä¸”å¾ˆå¯èƒ½ä¼šæ¶åŒ–å®ƒï¼Œå› æ­¤æˆ‘ä»¬æœ€å¥½åªä½¿ç”¨å…¶ä¸­çš„ä¸€ä¸ªã€‚æ¯•ç«Ÿç‰¹å¾è¶Šå°‘ï¼Œå­¦ä¹ é€Ÿåº¦è¶Šå¿«ï¼Œæ¨¡å‹è¶Šç®€å•(ç‰¹å¾è¶Šå°‘çš„æ¨¡å‹)ã€‚*

***ä»€ä¹ˆæ˜¯ p å€¼ï¼Ÿ**è¿™ä¸ªåšå®¢æ¶‰åŠçš„èŒƒå›´å¤ªå¹¿äº†ã€‚ä½†æ˜¯ï¼Œåœ¨ç‰¹å¾é€‰æ‹©çš„èƒŒæ™¯ä¸‹ï¼Œp å€¼å¸®åŠ©æˆ‘ä»¬*æ‰¾åˆ°åœ¨è§£é‡Šå› å˜é‡(y)* çš„å˜åŒ–ä¸­æœ€é‡è¦çš„ç‰¹å¾ã€‚*

*åœ¨è§£é‡Šè¿™ç§å˜åŒ–æ—¶ï¼Œp å€¼ä½çš„è¦ç´ æ„ä¹‰æ›´å¤§ï¼Œè€Œ p å€¼é«˜çš„è¦ç´ æ„ä¹‰è¾ƒå°ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬è®¾ç½®ä¸€ä¸ª**æ˜¾è‘—æ€§æ°´å¹³ SL** (é˜ˆå€¼)ï¼Œå¦‚æœä¸€ä¸ªç‰¹å¾çš„ p å€¼é«˜äºè¿™ä¸ªæ°´å¹³ï¼Œå®ƒå°†è¢«ä¸¢å¼ƒã€‚æˆ‘ä¼šåœ¨è¿™ç¯‡åšå®¢çš„æœ€åç•™ä¸‹ä¸€äº›é“¾æ¥ï¼Œå¯¹ p å€¼è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚*

***ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦ç§»é™¤é«˜ p å€¼çš„ç‰¹å¾ï¼Ÿå› ä¸ºå®ƒä»¬ä¸èƒ½è¯´æ˜å› å˜é‡çš„è¡Œä¸ºã€‚å› æ­¤ï¼Œå½“å®ƒä»¬ä¸èƒ½å¸®åŠ©æˆ‘ä»¬é¢„æµ‹ç»“æœæ—¶ï¼Œä¸ºä»€ä¹ˆè¦ä¿ç•™å®ƒä»¬ï¼Œå¹¶ä¸å¿…è¦åœ°å¢åŠ æˆ‘ä»¬æ¨¡å‹çš„å¤æ‚æ€§ã€‚***

*æˆ‘ä»¬æœ‰ä¸¤ä¸ªåä¸º*remove _ correlated _ features()remove _ less _ significant _ features()*çš„å‡½æ•°ï¼Œåˆ†åˆ«ç”¨äºç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾å’Œä¸å¤ªé‡è¦çš„ç‰¹å¾(ä½¿ç”¨ p å€¼å’Œ[å‘åæ¶ˆé™¤](https://www.javatpoint.com/backward-elimination-in-machine-learning)):*

```
*# >> FEATURE SELECTION << #
**def remove_correlated_features(X):**
    corr_threshold = 0.9
    corr = X.corr()
    drop_columns = np.full(corr.shape[0], False, dtype=bool)
    for i in range(corr.shape[0]):
        for j in range(i + 1, corr.shape[0]):
            if corr.iloc[i, j] >= corr_threshold:
                drop_columns[j] = True
    columns_dropped = X.columns[drop_columns]
    X.drop(columns_dropped, axis=1, inplace=True)
    return columns_dropped**def remove_less_significant_features(X, Y):**
    sl = 0.05
    regression_ols = None
    columns_dropped = np.array([])
    for itr in range(0, len(X.columns)):
        regression_ols = sm.OLS(Y, X).fit()
        max_col = regression_ols.pvalues.idxmax()
        max_val = regression_ols.pvalues.max()
        if max_val > sl:
            X.drop(max_col, axis='columns', inplace=True)
            columns_dropped = np.append(columns_dropped, [max_col])
        else:
            break
    regression_ols.summary()
    return columns_dropped*
```

*åœ¨åº”ç”¨*è§„èŒƒåŒ–*ä¹‹å‰ï¼Œè®©æˆ‘ä»¬åœ¨ *init()* ä¸­è°ƒç”¨è¿™äº›å‡½æ•°:*

```
****# inside init()***# filter features
remove_correlated_features(X)
remove_less_significant_features(X, Y)*
```

*é‡æ–°è¿è¡Œä»£ç å¹¶æ£€æŸ¥è¾“å‡º:*

```
***# OUTPUT WITH FEATURE SELECTION** reading dataset...
applying feature engineering...
splitting dataset into train and test sets...
training started...
Epoch is: 1 and Cost is: 7198.889722245353
Epoch is: 2 and Cost is: 6546.424590270085
Epoch is: 4 and Cost is: 5448.724593530262
Epoch is: 8 and Cost is: 3839.8660601754004
Epoch is: 16 and Cost is: 2643.2493061396613
Epoch is: 32 and Cost is: 2003.9830891013514
Epoch is: 64 and Cost is: 1595.2499320295813
Epoch is: 128 and Cost is: 1325.7502330505054
Epoch is: 256 and Cost is: 1159.7928936478063
Epoch is: 512 and Cost is: 1077.5846940303365
Epoch is: 1024 and Cost is: 1047.208390340501
Epoch is: 2048 and Cost is: 1040.2241600540974
training finished.
weights are: [ 3.53520254 11.03169318 -2.31444264 -7.89186867 10.14516174 -1.28905488
 -6.4397589   2.26113987 -3.87318997  3.23075732  4.94607957  4.81819288
 -4.72111236]
testing the model...
accuracy on test dataset: 0.9912280701754386
recall on test dataset: 0.9767441860465116
precision on test dataset: 0.9767441860465116*
```

*å¯ä»¥çœ‹åˆ°*å‡†ç¡®ç‡(99%)* ï¼Œ*ç²¾åº¦(0.98)* ï¼Œä»¥åŠ*å¬å›(0.98)* çš„åˆ†æ•°éƒ½æœ‰æ‰€æé«˜ã€‚è€Œä¸”ï¼Œæ–°å¸æ”¶æ•›å¾—æ›´å¿«ï¼›è®­ç»ƒåœ¨ 2048 ä¸ªçºªå…ƒå†…ç»“æŸï¼Œè¿™ä¸ä¹‹å‰çš„ä¸€æ¬¡(5000 ä¸ªçºªå…ƒ)ç›¸æ¯”è¦å°‘å¾—å¤š*

# *å®Œå…¨ç *

*æ‚¨å¯ä»¥åœ¨è¿™ä¸ª Github èµ„æºåº“ä¸­è·å¾—å®Œæ•´çš„ä»£ç :*

*[](https://github.com/qandeelabbassi/python-svm-sgd) [## qandeelabbassi/python-svm-sgd

### SVM éšæœºæ¬¡æ¢¯åº¦ä¸‹é™ç®—æ³•çš„ Python å®ç°- qandeelabbassi/python-svm-sgd

github.com](https://github.com/qandeelabbassi/python-svm-sgd)* 

***æœ‰ç”¨é“¾æ¥***

*   *[SGD ä¸ºä»€ä¹ˆä¼šèµ·ä½œç”¨ï¼Ÿ](/https-towardsdatascience-com-why-stochastic-gradient-descent-works-9af5b9de09b8)*
*   *[æ–°å¸çš„å¥½å¤„](https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent)*
*   *[æ•°æ®ç§‘å­¦å®¶è§£é‡Šçš„ P å€¼](/p-values-explained-by-data-scientist-f40a746cfc8)*
*   *[çº¿æ€§å›å½’å’Œ p å€¼](https://www.youtube.com/watch?v=nk2CQITm_eo)*
*   *[è½åæ·˜æ±°](https://www.javatpoint.com/backward-elimination-in-machine-learning)*