<html>
<head>
<title>tl;dr: How having more SWAG can make for safer AI drivers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TL；博士:拥有更多的赃物如何能让人工智能司机更安全</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tl-dr-how-having-more-swag-can-make-for-safer-ai-drivers-d23457ecdb93?source=collection_archive---------37-----------------------#2020-01-28">https://towardsdatascience.com/tl-dr-how-having-more-swag-can-make-for-safer-ai-drivers-d23457ecdb93?source=collection_archive---------37-----------------------#2020-01-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="97d9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">贝叶斯深度学习可以帮助捕捉传统方法通常忽略的不确定性属性。通过使用 SWAG 进行学习，我们表明可以训练自动驾驶员以更高的准确性做出更可靠的决策。</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/eefce8df7bb50222812463baf78644e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GgmYWHi0rqzRxtscRlLRaA.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">KITT OG 自动驾驶汽车</p></figure><p id="a3f8" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">自动驾驶汽车曾经只是《霹雳游侠》等 20 世纪娱乐经典的主题。如今，谷歌有了<a class="ae lz" href="https://waymo.com/" rel="noopener ugc nofollow" target="_blank"> Waymo </a>，优步有了<a class="ae lz" href="https://www.theverge.com/2019/6/12/18662626/uber-volvo-self-driving-car-safety-autonomous-factory-level" rel="noopener ugc nofollow" target="_blank"> XC90 SUV </a>，每辆现代特斯拉都配备了自己的、不那么啰嗦的 KITT 版本，名为<a class="ae lz" href="https://www.tesla.com/en_GB/autopilot" rel="noopener ugc nofollow" target="_blank"> autopilot </a>。和普通司机一样，AI 司机的大脑也要有经验，反应快，最重要的是，<em class="ly">自信</em>。虽然我们可以通过更多的数据来改善体验，通过更强大的机器或更快的算法来提高速度，但我们如何衡量和调节我们的信心呢？</p><p id="b9b1" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">注意我说的<em class="ly">调节</em>并不一定是<em class="ly">提高</em>。这是因为自动驾驶司机的决定通常由某种深度学习模型驱动，而这种模型已经<em class="ly">过于自信</em>。驾驶并不是一项让过度自信有回报的活动；罕见的错误会带来毁灭性的后果。没有人说深度学习模型自大，它们只是大多数确定性函数，不能代表不确定性。这就是贝叶斯方法胜出的地方。</p><p id="675c" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">随机梯度下降</strong></p><p id="a2dc" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">DNNs 非常强大的模型，在从语言翻译到图像分类的一系列预测任务中表现出卓越的性能。它们通常包含成千上万的参数，可以调整这些参数以从数据中学习复杂的模式。在监督学习中，目标是学习<em class="ly"> p </em> ( <strong class="le ir"> y </strong> | <strong class="le ir"> θ，X</strong>)；我们搜索模型参数<strong class="le ir"> θ </strong>，连同我们的特征数据<strong class="le ir"> X，</strong>可用于区分或预测我们的目标变量<strong class="le ir"> y </strong>中的观察值。对于任何<strong class="le ir"> θ </strong>，我们可以通过使用适当的损失度量<strong class="le ir"><em class="ly"/></strong><em class="ly"/>来计算我们的模型代表数据的程度，例如在分类任务的负对数似然回归模型的情况下的均方误差(MSE)。</p><p id="a4de" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">如果我们绘制<strong class="le ir"><em class="ly">【L】</em></strong>对<strong class="le ir"> θ </strong>的曲线，我们可以看到损耗幅度如何在参数空间内变化:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/7232366f9ea89a0745d063c8c17c102a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-5O3TzO96v83oeg-oGtpQ.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">CIFAR10 数据集的 RESNET-20 损失面的可视化。鸣谢:铁木尔·加里波夫和同事们。</p></figure><p id="253a" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这张漂亮的插图放大了在 CIFAR10 数据集上训练的 RESNET-20 DNN 的高度非凸损失表面。(x，y)轴上的每个<strong class="le ir"> θ </strong>实质上代表一个新的模型，其误差由 z 轴上的<strong class="le ir"> <em class="ly"> L </em> </strong>的高度跟踪；理想情况下，我们希望挑选<strong class="le ir"> <em class="ly"> L </em> </strong>值最小的<strong class="le ir"> θ </strong>。随机梯度下降(SGD)就像一种全球定位系统，引导我们从最初的起始位置沿着最大下降的路径前进，直到我们到达许多局部最小值中的一个。对于那些不熟悉 SGD 的人，我强烈推荐从 Andrew NG 的精彩概述<a class="ae lz" href="https://www.coursera.org/lecture/machine-learning/stochastic-gradient-descent-DoRHJ" rel="noopener ugc nofollow" target="_blank">开始。</a></p><p id="e86c" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这种方法并非没有缺点:</p><ul class=""><li id="62d7" class="ma mb iq le b lf lg li lj ll mc lp md lt me lx mf mg mh mi bi translated"><a class="ae lz" href="https://arxiv.org/abs/1803.05407" rel="noopener ugc nofollow" target="_blank">有人认为</a>传统的 SGD 只收敛到一组高性能网络的边界。小样本变化会导致训练和测试数据集之间的<em class="ly">损失表面移动</em>，从而将<strong class="le ir"> θ </strong>推离该边界，导致泛化能力差。</li><li id="2e10" class="ma mb iq le b lf mj li mk ll ml lp mm lt mn lx mf mg mh mi bi translated">SGD 通常与一个<em class="ly">衰减学习率</em>一起使用，以便收敛到一个最优<strong class="le ir"> θ </strong>。虽然这加快了推理的速度，但它并没有覆盖所有可能的解决方案。</li><li id="a344" class="ma mb iq le b lf mj li mk ll ml lp mm lt mn lx mf mg mh mi bi translated">当我们做出导致过度自信估计的预测时，我们忽略了与<strong class="le ir"> θ </strong>相关的不确定性。</li></ul><p id="6de1" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我们怎样才能在不增加额外计算开销的情况下解决这些问题呢？</p><p id="124d" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">贝叶斯网络的 SWAG</strong></p><p id="aa33" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">SWAG 中的 SWA 代表<a class="ae lz" href="https://arxiv.org/abs/1803.05407" rel="noopener ugc nofollow" target="_blank"> <strong class="le ir">随机加权平均</strong> </a> <strong class="le ir"> g </strong>，这是一种专门解决前两个缺点的方法。其思路是从一个预先训练好的解<strong class="le ir">θ_ {</strong><em class="ly">pre</em><strong class="le ir">}</strong>开始，然后把学习率推上去，利用 SGD 探索局部几何。在每一步<em class="ly"> i </em>中，我们以恒定或循环的学习速率移动到权重空间<strong class="le ir"> θ </strong> <em class="ly"> _{i} </em>中的新位置。恒定的学习速率产生一个新的解，我们连续地将它与我们的预训练解进行平均，以给出 SWA 解<strong class="le ir">θ_ {</strong><em class="ly">SWA</em><strong class="le ir">}</strong>。对于循环学习率，求解器“跳出”一个局部极小值，收敛到附近的另一个解；学习周期结束时的收敛解与训练解进行平均。作者表明，这种新的解决方案更有可能集中在一组广泛的高性能网络中，并强调了跨许多残差和图像分类网络的预测准确性的提高。</p><p id="db45" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><a class="ae lz" href="https://arxiv.org/pdf/1902.02476.pdf" rel="noopener ugc nofollow" target="_blank"> Maddox 等人</a>将该方法包装在贝叶斯魔术中，通过使用形式为<em class="ly">N</em>(<strong class="le ir">θ_ {</strong><em class="ly">SWA</em>}，<strong class="le ir">σ</strong>)的多元高斯模型来近似局部误差表面，将 SWA 扩展到 SWA( <strong class="le ir"> G </strong>)。</p><p id="401b" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">σ</strong>是一个协方差矩阵，它跟踪沿局部几何形状的变化，并使用沿 SWA 算法路径移动的每个新点<strong class="le ir"> θ </strong> <em class="ly"> _{i} </em>进行更新。不幸的是，如果我们包含每个迭代<em class="ly"> i，那么计算<strong class="le ir">σ</strong>是很昂贵的；</em>dnn 包含数百万个参数。作为速度黑客，作者使用全套迭代来仅更新<strong class="le ir">σ</strong>中的对角线元素，使用 SGD 迭代中的最后<strong class="le ir"> T </strong>项来计算非对角线。</p><p id="21f8" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">现在来看最精彩的部分:我们不再只有对<strong class="le ir"> θ、</strong>的<em class="ly">一个</em>选择，我们有了由<em class="ly">N</em>(<strong class="le ir">θ_ {</strong><em class="ly">swa</em><strong class="le ir">}</strong>、<strong class="le ir">σ</strong>)给出的整个<em class="ly">分布</em>，它描述了围绕<strong class="le ir">θ_ {</strong><em class="ly">swa</em><strong class="le ir">}</strong>的几何特征。这让我们可以使用<a class="ae lz" href="https://www.jstor.org/stable/2676803?seq=1" rel="noopener ugc nofollow" target="_blank"> <strong class="le ir">贝叶斯模型平均</strong> </a>将不确定性直接建模到任何给定的预测中。原则上，这包括将<em class="ly"> p </em> ( <strong class="le ir"> y </strong> | <strong class="le ir"> θ，X </strong>)中的<strong class="le ir"> θ </strong>积分，得到<em class="ly">p</em>(<strong class="le ir">y</strong>|<strong class="le ir">X</strong>)。在实践中，这在计算上是难以处理的，所以我们通过<a class="ae lz" href="https://machinelearningmastery.com/monte-carlo-sampling-for-probability/" rel="noopener ugc nofollow" target="_blank">蒙特卡罗采样</a>来解决近似:采样<strong class="le ir">θ_ {<em class="ly">K</em>} ~</strong><em class="ly">N</em>(<strong class="le ir">θ_ { swa }</strong>，<strong class="le ir">σ</strong>)<strong class="le ir">K</strong>次，并平均 1/<strong class="le ir">K(</strong>σ<em class="ly">p</em>(<strong class="le ir">y<strong class="le ir"/></strong></p><p id="ef33" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">SWAG 在图像分类、分割和回归任务上表现出了令人印象深刻的性能，但它在自动驾驶上又会如何呢？</p><p id="4bd1" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="le ir">带 SWAG 的自动驾驶</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mo"><img src="../Images/9b491b0a14902d7ac2898e9f722cd153.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2u3zy6GRNBKb5CAVNqkk9Q.png"/></div></div></figure><p id="c788" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">Udemy 早在 2016 年就推出了一款很棒的<a class="ae lz" href="https://github.com/udacity/self-driving-car-sim" rel="noopener ugc nofollow" target="_blank">驾驶模拟器</a>，可以用来手动收集训练数据和处理来自 python API 的 I/O。本实验中使用的网络架构和数据预处理步骤来自于<a class="ae lz" rel="noopener" target="_blank" href="/deep-learning-for-self-driving-cars-7f198ef4cfa2"> Manajit Pal </a>的一个很棒的教程，其中还包括一个链接，指向用于两个轨道之一的<a class="ae lz" href="https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip" rel="noopener ugc nofollow" target="_blank">数据集</a>。</p><p id="8edd" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">本质上，该网络是一种改进的卷积神经网络，具有预测单一转向角的线性层。该网络将汽车驾驶员视角的右视图、中视图和左视图作为输入。因为这是一项回归任务，所以使用均方误差作为损失标准。所有代码都是用 python 编写的，使用 pytorch 和 Maddox 等人发布的<a class="ae lz" href="https://github.com/wjmaddox/swa_gaussian/" rel="noopener ugc nofollow" target="_blank"> SWAG 训练报告</a>进行推理。</p><p id="9065" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">使用 80:20 的种子随机分配将数据分成训练集和验证集。首先，使用传统的 SGD 在 60 个时期内训练初始网络，线性衰减的学习速率最初被设置为 0.1，持续 30 个时期，线性衰减至 0.00001，并在最后 6 个时期保持固定。在整个训练过程中，动量被设置为常数 0.9，其中 L2 正则化使用 0.0001 重量衰减来实现。通常在 40 个时期后观察到收敛。</p><p id="5e68" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">然后 SWAG 再运行 60 个时期，更新每个时期 I 的循环学习率，如<a class="ae lz" href="https://arxiv.org/abs/1803.05407" rel="noopener ugc nofollow" target="_blank">伊兹迈洛夫等人</a>所述:</p><ol class=""><li id="4771" class="ma mb iq le b lf lg li lj ll mc lp md lt me lx mp mg mh mi bi translated">t(I)=(mod(i1，c) + 1)/c，</li><li id="9dc7" class="ma mb iq le b lf mj li mk ll ml lp mm lt mn lx mp mg mh mi bi translated">α(I)=(1t(I))α1+t(I)α2</li></ol><p id="b5e7" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">使用基本学习率参数(α1，α2)= {(101，103)，c 是设置为 5 个历元的周期长度。我记录了训练/验证 MSE，并将其转换为负对数可能性，以便于解释。我们可以对获得的协方差矩阵执行奇异值分解(SVD ),以将我们的验证集损失表面投影到由特征向量定义的二维平面上:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/538da4c27627e90a0f04eb1a091dd516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*6ngwcP2ACKRycic99_zcLA.png"/></div></figure><p id="b410" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这里，v1、v2 表示在由前两个 SVD 特征向量跨越的低维平面中距 SWA 解的距离。这是一个非常低维的表示，但是非常好地捕捉了损失表面的几何形状！！我们看到，尽管我们的 SGD 解在 SWAG 3σ区域内(绿色虚线)，但它与恰好位于宽损失边界正中心的 SWAG 平均解相差甚远。</p><p id="c984" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">最终的结论是，我们的 SGD 模型实现了 0.91±0.0115 的验证集 NLL，而 SWAG 构造具有更低的损失和更紧密的围绕 0.88±0.0094 的解决方案的置信界限。</p><p id="1af6" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">由于该框架非常灵活，因此可以扩展到更复杂的自动驾驶网络，这些网络具有类似记忆的组件，如 LSTMs。但是现在，我希望我已经让你相信贝叶斯网络真的很神奇，并且对未来有很大的希望。</p><p id="054e" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">谷歌的协作环境为这个项目提供了动力，他们免费提供 GPU 时间，因此任何人都可以训练他们的模型！也感谢<a class="ae lz" href="https://www.linkedin.com/in/marcantoniomawada/" rel="noopener ugc nofollow" target="_blank">Marco Antonio Awada</a>博士对信息几何及其在深度学习中的应用的大力指导。</p></div></div>    
</body>
</html>