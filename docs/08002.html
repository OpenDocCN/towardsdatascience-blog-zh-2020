<html>
<head>
<title>Intuitively, How Do Neural Networks Work?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">直观来看，神经网络是如何工作的？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuitively-how-do-neural-networks-work-d7710b602e51?source=collection_archive---------27-----------------------#2020-06-13">https://towardsdatascience.com/intuitively-how-do-neural-networks-work-d7710b602e51?source=collection_archive---------27-----------------------#2020-06-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="5bf0" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">直观地</h2><div class=""/><div class=""><h2 id="3a02" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">“神经网络”这个术语可能看起来很神秘，为什么一个算法叫做神经网络？它真的能模仿真正的神经元吗，又是如何模仿的呢？</h2></div><p id="1410" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我上一篇关于<a class="ae ln" rel="noopener" target="_blank" href="/intuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3"> <em class="lo">直观上，我们如何理解不同的分类算法</em> </a>的文章中，我介绍了分类算法的主要原理。然而，我使用的玩具数据非常简单，几乎是线性可分的数据；在现实生活中，数据几乎总是<strong class="kt jd">非线性</strong>，所以我们应该让我们的算法能够处理非线性可分数据。</p><h1 id="3a86" class="lp lq it bd lr ls lt lu lv lw lx ly lz ki ma kj mb kl mc km md ko me kp mf mg bi translated">简单逻辑回归和非线性数据</h1><p id="8cd9" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la mj lc ld le mk lg lh li ml lk ll lm im bi translated">让我们比较一下逻辑回归对于几乎线性可分数据和非线性可分数据的表现。</p><p id="8eec" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过下面的两个玩具数据，我们可以看到，当数据几乎是线性可分时，逻辑回归有助于我们找到决策边界，但当数据不是线性可分数据时，逻辑回归无法找到明确的决策边界。这是可以理解的，因为逻辑回归只能将数据分成两部分。</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/6901f7f60886de5afd63b81bcfe007e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*W2WGtiymsH9e0tP_0AEaGw.png"/></div></figure><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/b200f7712121bc9c9620ea4285728f18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*HLNFj5YLKnF53ow_FwLL6Q.png"/></div></figure></div><p id="52bc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本文中，我们将尝试以不同的方式重用逻辑回归，并尝试使其适用于非线性数据。</p><h1 id="0617" class="lp lq it bd lr ls lt lu lv lw lx ly lz ki ma kj mb kl mc km md ko me kp mf mg bi translated">组合逻辑回归</h1><p id="f772" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la mj lc ld le mk lg lh li ml lk ll lm im bi translated">当观察蓝点和红点时，数据是非线性的，因为我们可以看到有 3 个部分，我们应该找到两个决策边界。那么，如果…如果我们试着结合两个逻辑回归会怎么样？</p><p id="47c3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">例如，我们有这两个基于逻辑回归的模型。</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/b24a151d77bebc4e8ae5013493074504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*2IG4o5VPeUT_Li_6Dd6OhA.png"/></div></figure><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/976eb29e3546afe4ad007778c8de0464.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uIe8fS0jWNa_C_NsN8hIpw.png"/></div></figure></div><p id="041e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们称它们为 h1 和 h2。</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr nd mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/5305936bcc5bf8dfa22be7d76069f4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*O9dmHvolSYhQ1sG_9BZmSw@2x.png"/></div></figure><figure class="mq mr ne mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/176807d486126a92126a6a23cdce2e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*iu7JZc6vfH6lcOn5G69-2g@2x.png"/></div></figure></div><p id="5796" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，我们可以将 h1 和 h2 结合起来，用另一个逻辑回归进行最终预测。</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nf"><img src="../Images/f66c994fd5750c9f4a2612a971b22aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ATWq4sZr6pwkoWcANSh8QQ@2x.png"/></div></div></figure><p id="cdec" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，您可以用 h1 和 h2 各自的逻辑函数表达式来替换它们:</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ng"><img src="../Images/9851b0e7a3d137e16b88cd5bb50a83ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-rO8V0YdFd_bW4s2KSMWw@2x.png"/></div></div></figure><p id="6d70" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后我们可以通过找到不同的系数来解决问题，你猜怎么着，最终的结果是相当令人满意的:我们可以看到，两个组合的 Logistic 回归给了我们两个决策边界。</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/ee35feed546f3be2326581ec79f0b3cc.png" data-original-src="https://miro.medium.com/v2/format:webp/1*YYt7FWTJtOIxF20nPTzsVw.png"/></div></figure><h1 id="372c" class="lp lq it bd lr ls lt lu lv lw lx ly lz ki ma kj mb kl mc km md ko me kp mf mg bi translated">更好的表现</h1><p id="acfb" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la mj lc ld le mk lg lh li ml lk ll lm im bi translated">让我们回到我们模型的最终表达式:</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ng"><img src="../Images/9851b0e7a3d137e16b88cd5bb50a83ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-rO8V0YdFd_bW4s2KSMWw@2x.png"/></div></div></figure><p id="dd5c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">这个表情可能看起来挺吓人的。让我们给它加点艺术感。</strong></p><p id="1060" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们画一些东西来以更易读的方式表示这些函数:</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ni"><img src="../Images/2682a71ba0952985dbe72ead41248a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n6C-eH21MA7iBhIk8YAgCw.png"/></div></div></figure><p id="fd78" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">横向来看，甚至会更好:</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nj"><img src="../Images/b043fe6196d21f91748b6baca365f090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zhM-4MwJwhXt5KiQLnYpFQ.png"/></div></div></figure><p id="a796" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们指定所有要确定的系数，我们可以将它们放在线上，这样更容易理解:</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nk"><img src="../Images/67841e33b568bbf44a57395d08b7763c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o1LqDbnsCA42Kwtn_FyrxA.png"/></div></div></figure><p id="12d8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以将线性函数和 sigmoid 函数分开，以更好地提高可读性。</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nl"><img src="../Images/20eaa8de2ea6d86fa53dab1418785060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vBNJkmpFB9Q0XaGdvMTAkw.png"/></div></div></figure><p id="f4a5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">长方形？不，圆圈更好。等等，你看到我看到的了吗？</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nm"><img src="../Images/8a2b3b66e49185e9e03fdcb42f872357.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FdXe9croUkSy94v4FB0dLg.png"/></div></div></figure><p id="683d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">哇，它们就像…神经元！</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nn"><img src="../Images/ac8ef6055cf870ccf73615e25b04758d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-e2rmm9miYpOMlFRvAd1jQ.jpeg"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated">图片来自 123rf</p></figure><p id="155d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以添加更多的颜色使它更好。连接代表重量，我们可以让它们变粗来代表更大的数字。用一种颜色，我们可以代表这个符号。</p><ul class=""><li id="59ed" class="ns nt it kt b ku kv kx ky la nu le nv li nw lm nx ny nz oa bi translated">这里，绿色是正的，红色是负的。</li></ul><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ob"><img src="../Images/95e0dbce360592426a4bb56d16a25d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LBlNCYT8VFX4UzDOl3e4Tg.png"/></div></div></figure><p id="dcc4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">数学家是艺术家！</p><h1 id="c7e9" class="lp lq it bd lr ls lt lu lv lw lx ly lz ki ma kj mb kl mc km md ko me kp mf mg bi translated">神经网络词汇</h1><p id="1e20" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la mj lc ld le mk lg lh li ml lk ll lm im bi translated">现在让我们围绕这个神经网络创造一个全新的世界。</p><p id="83c9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们创建不同的层:输入、输出和隐藏层。</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oc"><img src="../Images/de2988e270184275312542df65b13047.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e-XkV3YYOf4cnN_gyxyq7Q.png"/></div></div></figure><p id="0fcf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当计算给定输入值的输出值时，必须遍历所有隐藏层。所以你会从左边走到右边。姑且称之为“<strong class="kt jd">正向传播</strong>”。</p><p id="be23" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最后，您将计算出一个估计的输出，并且您必须将它与实际输出进行比较。这个误差将帮助我们微调权重。为此，我们可以使用梯度下降，我们必须计算不同的导数。</p><p id="8be4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">由于最终会计算误差，因此我们会在输出层之前调整神经元的权重。我们总是会从右向左调整。所以姑且称之为“<strong class="kt jd">反向传播</strong>”。</p><h1 id="2704" class="lp lq it bd lr ls lt lu lv lw lx ly lz ki ma kj mb kl mc km md ko me kp mf mg bi translated">二维空间会发生什么</h1><p id="4eac" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la mj lc ld le mk lg lh li ml lk ll lm im bi translated">现在让我们考虑两个输入变量，这是玩具数据。</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi od"><img src="../Images/53141deef434e45bd3975ba374b41e92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FiCqQKMuH3WHLrp4BsbCCA.png"/></div></div></figure><p id="e895" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">直观上，我们可以看到两个决策边界就足够了。所以让我们应用两个隐藏的神经元:</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr oe mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/220aa4d63f85c0bc6726e010a1242fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*5aUXVhiVz2a9YAISxprjSg.png"/></div></figure><figure class="mq mr of mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/dfc5c50bc3c3c326e825910864fbb0c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*aj3ww4NUnY63ldapagCzjQ.png"/></div></figure><figure class="mq mr og mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/be0445ab3421f265bf5851373d9cf9c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*jtRF82rO1Hz8GDVUJq-yhw.png"/></div></figure></div><p id="be58" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，我们可以通过用周围区域数据测试神经网络来可视化最终结果。我们可以看到模型做得相当好。</p><p id="0fe2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，既然我们知道在神经网络内部只有逻辑回归，我们可以尝试将转换的不同步骤可视化。</p><h2 id="6cba" class="oh lq it bd lr oi oj dn lv ok ol dp lz la om on mb le oo op md li oq or mf iz bi translated">隐藏层转换</h2><p id="060e" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la mj lc ld le mk lg lh li ml lk ll lm im bi translated">让我们看看 H1:输入数据是原始的蓝点和红点。H1 是一个有两个输入变量的逻辑回归，所以结果是一个表面。</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr os mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/6fcf91f577a5c83e58a7d4d25f0fe87d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*bosRVL_3jwoAY4PxR7dVMQ.png"/></div></figure><figure class="mq mr ot mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/611b9f792bb75f8a920bfed67c3fcbee.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*i6chKh7HmZEn_rfvr9jzEg.png"/></div></figure></div><p id="aa5f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后第二个隐藏神经元 H2 是类似的。</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr ou mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/e6e84ed768a4e50f3e64264381a6d2d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*V-RzARYCRLOqgLxNnaHM-g.png"/></div></figure><figure class="mq mr ov mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/2c94f0aa5746f3dbd5448cf6c78a3060.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*2K34rnU6lQWNr3FBl-BUog.png"/></div></figure></div><p id="7157" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">输出图层 O1 是一个逻辑回归，也接受两个输入，因此我们也有一个表面来表示结果。而 O1 的输入是数值在 0 和 1 之间的两个数据系列，因为它们是两个逻辑回归的结果。然后我们可以看到，如果我们取值 0.5 到表面，最初的蓝点和红点可以线性分成两部分。</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr ow mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/c1730c8e5ef0e95cf3e01f03dcdb2197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*VF3_iEoyVQen88MV18A-0w.png"/></div></figure><figure class="mq mr ox mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/2cab418253e21d12e3a1a70d7a54cacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*0STP8l_Ha30oesNy5smOog.png"/></div></figure></div><p id="38ee" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">那么在这个神经网络中发生了什么呢？在某种意义上，我们可以说<strong class="kt jd">神经网络将非线性可分离的数据转化为几乎线性可分离的数据。</strong></p><h2 id="886b" class="oh lq it bd lr oi oj dn lv ok ol dp lz la om on mb le oo op md li oq or mf iz bi translated">另一个可视化</h2><p id="b1db" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la mj lc ld le mk lg lh li ml lk ll lm im bi translated">隐藏层有两个神经元，所以是二维的。因此，如果我们想看到这种转变，我们希望看到点是如何在平面上移动的。我们知道这些点的最终位置在单位正方形(0，1)x(0，1)中。</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oy"><img src="../Images/228e247ace2125f02b20976932453f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TuJTz3wvP7BmuG3uHiUStw.png"/></div></div></figure><p id="7ec4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以分别看到蓝点和红点。<strong class="kt jd">注意，箭头指向最终位置。</strong></p><div class="mm mn mo mp gt ab cb"><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/3a838478b69e8f5ef6e69e711843e041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*11JWkkjANirdUhd98y_NPA.png"/></div></figure><figure class="mq mr ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/04c3bf44455914d02b01cfd637c65570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*QQd47Mpdnh8sZY9-ekzLaQ.png"/></div></figure></div><p id="b31b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后我们可以看到最后的逻辑回归。</p><p id="40dd" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">原始数据不是线性可分的，但是经过变换后，它们被移动到单位正方形中，在那里它们变得几乎线性可分。</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr oz mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/9fa32ac2cc7aee08d066c0a8aa6b0aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*vKv8Z6zjrxT-dbDdHo5UqA.png"/></div></figure><figure class="mq mr pa mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/0df8d88a75ba068d0cda4b70126aad09.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*A94SohgsOUzE5RGN8bfmfg.png"/></div></figure></div><h1 id="f5c0" class="lp lq it bd lr ls lt lu lv lw lx ly lz ki ma kj mb kl mc km md ko me kp mf mg bi translated">另一个更复杂数据的可视化</h1><p id="eb58" class="pw-post-body-paragraph kr ks it kt b ku mh kd kw kx mi kg kz la mj lc ld le mk lg lh li ml lk ll lm im bi translated">现在让我们考虑另一个玩具数据。凭直觉，你会应用多少个隐藏神经元？</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/3d677b32865f28fe1654cde74d0c04b1.png" data-original-src="https://miro.medium.com/v2/format:webp/1*5j3Lc0Pz53cEMFMHgs7_EQ.png"/></div></figure><p id="3cdf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">嗯，3 个神经元就足够了，如下图所示:</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr pb mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/db247f0f9619cfb69ce595d482d9bf4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*aXRwqHe9n_h5vR17SvbScA.png"/></div></figure><figure class="mq mr pc mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/775106b9907269f1c2bfc669788eedac.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*Q6TSYvbIqiiT07lIHaugFw.png"/></div></figure><figure class="mq mr pd mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/f27e742205aa9067994567790dad8cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*HBJ3wwAQTMB2nOSFAKvIsA.png"/></div></figure></div><p id="47d9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以看到每个神经元的逻辑回归曲面。</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr pe mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/254b5db3a944f64e88c5b018e98eb31f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*o599T-SFTFlH08nY4f4R7g.png"/></div></figure><figure class="mq mr pf mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/97aa7edc6005efecc9c991d8546ee690.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*39EZGpzPCc_3UYcCqLdKkg.png"/></div></figure></div><p id="e79d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">隐藏神经元 2</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr pg mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/41b0e8850aa3e6332227ada7a79050a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*fyK3Wiy3VMFpeAO0pfGc4g.png"/></div></figure><figure class="mq mr ph mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/c528d0223b38d127674c64586220dfb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*I0g742f50JndQ5aG7SVewA.png"/></div></figure></div><p id="538b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">隐藏神经元 3</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr pi mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/b2699c840f0944b9820e209e585adac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*KB0HHCPbCAZibqor7oi9Kw.png"/></div></figure><figure class="mq mr pj mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/d846011f31f8b30009b33b6603067094.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*JgUhnmOs-xTZmZ9GJXMe4g.png"/></div></figure></div><p id="f83b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于输出神经元，它有 3 个输入(因为它们是逻辑回归的结果，值在 0 和 1 之间)，我们可以在一个单位立方体中可视化它们。</p><div class="mm mn mo mp gt ab cb"><figure class="mq mr pk mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/1ac834552da68e6f0c905aae53160075.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*7Z2tmH129uEgiV-u-BCtTg.png"/></div></figure><figure class="mq mr pl mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/d47af0a9038efde909afb3a7eb31005b.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*aRs_lJGSRroRT-0rr11qoA.png"/></div></figure><figure class="mq mr pm mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><img src="../Images/23ea03f9dbd6262e71b7ab54da20719e.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*sVyY0NUCop1pDmMWdcqSyQ.png"/></div></figure></div><p id="8d1d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以创建一个动画来更好地可视化</p><figure class="mm mn mo mp gt mr gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/c45534202a071bf6caca3d510cc32849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*OckvjmM3doCijaNot-AbQw.gif"/></div></figure><p id="04ae" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们回顾一下:</p><ul class=""><li id="4af4" class="ns nt it kt b ku kv kx ky la nu le nv li nw lm nx ny nz oa bi translated">初始 2D 数据不是线性可分的。</li><li id="c958" class="ns nt it kt b ku po kx pp la pq le pr li ps lm nx ny nz oa bi translated">它们被转换成 3D 空间(在一个隐藏层中有 3 个神经元)。</li><li id="5440" class="ns nt it kt b ku po kx pp la pq le pr li ps lm nx ny nz oa bi translated">经过变换后，它们变得几乎线性可分。</li><li id="94c8" class="ns nt it kt b ku po kx pp la pq le pr li ps lm nx ny nz oa bi translated">最后，简单的逻辑回归允许我们对转换后的数据进行线性分类。</li><li id="231b" class="ns nt it kt b ku po kx pp la pq le pr li ps lm nx ny nz oa bi translated">最终的逻辑回归有 3 个输入变量。因此该模型可以预测三维空间中每个点的概率。这就是为什么我们可以看到立方体充满了彩色点。决策边界是一个曲面。</li></ul></div><div class="ab cl pt pu hx pv" role="separator"><span class="pw bw bk px py pz"/><span class="pw bw bk px py pz"/><span class="pw bw bk px py"/></div><div class="im in io ip iq"><p id="f5f8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">更复杂的神经网络可以捕捉甚至更复杂的输入数据关系。但我们可以看到原理相当简单:<strong class="kt jd">隐藏神经元的目的是将非线性可分离的数据转换到一个可以线性分离的空间。</strong></p><p id="d711" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你对其他分类器如何处理非线性可分离数据感兴趣，你可以看看下面的文章:</p><div class="qa qb gp gr qc qd"><a rel="noopener follow" target="_blank" href="/intuitively-how-can-we-build-non-linear-classifiers-10c381ed633e"><div class="qe ab fo"><div class="qf ab qg cl cj qh"><h2 class="bd jd gy z fp qi fr fs qj fu fw jc bi translated">凭直觉，我们如何建立非线性分类器</h2><div class="qk l"><h3 class="bd b gy z fp qi fr fs qj fu fw dk translated">现实生活中的问题通常是非线性的，所以让我们看看算法是如何处理非线性可分数据的。</h3></div><div class="ql l"><p class="bd b dl z fp qi fr fs qj fu fw dk translated">towardsdatascience.com</p></div></div><div class="qm l"><div class="qn l qo qp qq qm qr nb qd"/></div></div></a></div></div></div>    
</body>
</html>