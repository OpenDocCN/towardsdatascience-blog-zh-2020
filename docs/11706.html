<html>
<head>
<title>Origins of AutoML: Best Subset Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AutoML 的起源:最佳子集选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/origins-of-automl-best-subset-selection-1c40144d86df?source=collection_archive---------43-----------------------#2020-08-13">https://towardsdatascience.com/origins-of-automl-best-subset-selection-1c40144d86df?source=collection_archive---------43-----------------------#2020-08-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1343d7122fe3954dae47c29fbd334928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pG8oRKgmiKAEN1bfuOXPDA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">所有图片均由作者生成</p></figure><div class=""/><div class=""><h2 id="b335" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">以及选择后推理的危险</h2></div><p id="0776" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">由于有很多关于<a class="ae lt" href="https://en.wikipedia.org/wiki/Automated_machine_learning" rel="noopener ugc nofollow" target="_blank"> AutoML </a>的传言，我决定写一下最初的 AutoML；逐步回归和最佳子集选择。然后，我决定忽略逐步回归，因为它是<a class="ae lt" href="https://www.stata.com/support/faqs/statistics/stepwise-regression-problems/" rel="noopener ugc nofollow" target="_blank">坏</a>，可能应该停止教授。这就留下了最佳子集选择来讨论。</p><p id="80eb" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">最佳子集选择背后的想法是选择变量的“最佳”子集以包括在模型中，一起查看变量组，而不是一次比较一个变量的逐步回归。我们通过评估哪个子模型最符合数据来确定哪组变量是“最佳”的，同时对模型中的独立变量的数量进行惩罚以避免过度拟合。评估模型拟合程度的指标有多种:调整后的𝑅-squared、阿凯克信息标准(AIC)、贝叶斯信息标准(BIC)和马洛的<strong class="kz jj"> 𝐶𝑝 </strong> <em class="lu"> </em>可能是最著名的。</p><p id="7fa2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">每一个的公式如下。</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lv"><img src="../Images/f02654860c7e5bc72f2c05e9819da7c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ruiqmbGeonHumUqhNflaw.png"/></div></div></figure><p id="65d2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">对于校正后的 R 平方，您希望找到具有最大校正后 R 平方的模型，因为它解释了因变量中的最大方差，从而降低了模型的复杂性。对于其他模型，您希望找到具有最小信息标准的模型，因为它是因变量中具有最小未解释方差的模型，这会降低模型的复杂性。它们是同一个概念，即最大化好的东西与最小化坏的东西。</p><p id="2c05" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">AIC 和 Mallow 的<strong class="kz jj"> 𝐶𝑝 </strong> <em class="lu"> </em>都倾向于给出更好的预测模型，而 BIC 则倾向于给出自变量更少的模型，因为它对复杂模型的惩罚比其他两个更重。</p><p id="b19e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">像生活中的大多数事情一样，自动化模型选择是有代价的。如果你用你的数据选择一个线性模型，所选变量的系数将<strong class="kz jj"><em class="lu"/></strong>偏离零！每个系数的单个<strong class="kz jj"> t 检验</strong>和整体模型显著性的<strong class="kz jj"> F 检验</strong>的零假设都基于每个系数的均值为 0 的正态分布假设。由于我们在系数中引入了偏差，这些测试的<strong class="kz jj">I 型</strong>误差水平会增加！如果您只需要一个预测模型，这可能不是一个问题，但是它会使使用所选模型做出的任何统计推断完全无效。AutoML 可能能够生成体面的预测模型，但推理仍然需要一个人仔细思考问题，并遵循科学的方法。</p><h1 id="454c" class="ma mb ji bd mc md me mf mg mh mi mj mk ko ml kp mm kr mn ks mo ku mp kv mq mr bi translated">证明最佳子集选择的偏差</h1><p id="30df" class="pw-post-body-paragraph kx ky ji kz b la ms kj lc ld mt km lf lg mu li lj lk mv lm ln lo mw lq lr ls im bi translated">我进行了一项模拟研究来证明由最佳子集选择引起的偏差。我们不看系数中的偏差，而是看模型中误差项的估计标准差中的偏差</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/ab231e12f725f0d0cc9d9ae3df578039.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*DLyjCXKkpy64qyhn6GPRWQ.png"/></div></figure><p id="61cc" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">其中误差项是同分布且独立分布的<strong class="kz jj"><em class="lu">【𝑁(0,𝜎】</em></strong>随机变量。</p><p id="2724" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在每一轮模拟中，从相同的分布中产生 100 个观察值的样本。真实模型，它只包含真正重要的变量，以及 AIC 和 BIC 选择的最佳子集模型也被估计。从每个模型中，我用公式估算了<strong class="kz jj"><em class="lu"/></strong><em class="lu"/>的误差项</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/ed37b8c7b411740497ac65fee7358833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gyqOtKfdRRYOxGkH7dEfOg.png"/></div></div></figure><p id="1bc4" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这被执行 500 次。</p><p id="d0d5" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我模拟的具体参数如下:<strong class="kz jj"><em class="lu"/></strong><em class="lu"/>= 100，#个自变量= 6，<strong class="kz jj"><em class="lu"/></strong>= 1，有效自变量个数为 2。截距也很重要，所以 3 个系数是非零的。使用<strong class="kz jj"><em class="lu">【𝑁(5,1】</em></strong>随机数来选择非零系数，因为我懒得定义固定的数字，但它们在所有轮次的模拟中都保持固定。</p><p id="b0f4" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我首先定义了自己的函数，使用 AIC 或 BIC，通过查看变量的每个组合，使用一种简单的方法来执行最佳子集选择。它只对少量变量有效，因为它必须考虑的模型数量随着变量数量的增加而增加。考虑的模型数量为</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/bfb8ab46451f371330371d346f9dee96.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/format:webp/1*_wCluR6ixhn_1797Rxc8Cw.png"/></div></figure><p id="a652" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">但是最佳子集选择的更聪明的实现使用树搜索来减少考虑的模型数量。</p><p id="8f2c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">感兴趣的图形位于最佳子集选择函数和模拟代码块的下方。</p><figure class="lw lx ly lz gt iv"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="lw lx ly lz gt iv"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="0852" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">红线是 y 轴等于 x 轴的线，是真实模型对<strong class="kz jj"> <em class="lu"> 𝜎 </em> </strong>的无偏估计。正如你在下面的图表中看到的，𝜎<strong class="kz jj"><em class="lu"/></strong>的估计值偏离了 best AIC 和 BIC 选择的模型。事实上它们将永远小于或等于来自真实模型的<strong class="kz jj"><em class="lu"/></strong>的无偏估计。这说明了为什么通过最佳子集选择选择的模型对于推理是无效的。</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1343d7122fe3954dae47c29fbd334928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pG8oRKgmiKAEN1bfuOXPDA.png"/></div></div></figure><h1 id="84ff" class="ma mb ji bd mc md me mf mg mh mi mj mk ko ml kp mm kr mn ks mo ku mp kv mq mr bi translated">奖励部分:调查 LASSO 和岭回归中误差项的估计标准差的偏差</h1><p id="2dbe" class="pw-post-body-paragraph kx ky ji kz b la ms kj lc ld mt km lf lg mu li lj lk mv lm ln lo mw lq lr ls im bi translated">在进行上述模拟研究时，我对正则化方法在线性模型中估计误差项的标准偏差时的潜在偏差产生了兴趣，尽管人们不会使用正则化模型来估计参数以进行推断。众所周知，LASSO 和岭回归有意地将估计系数偏向零，以减少模型中的方差(来自同一总体的样本之间估计系数的变化量)。套索可以设置系数等于零，执行变量选择。岭回归使系数偏向零，但不会将它们设置为零，因此它不是像最佳子集选择或套索那样的可变选择工具。</p><p id="fd29" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我使用与之前相同的设置，但是将样本大小从 100 增加到 200，自变量的数量从 6 增加到 100，重要自变量的数量从 2 增加到 50。使用三重交叉验证在 0.01、0.1、1.0 和 10.0 之间选择套索和脊模型中的收缩参数。为了计算<strong class="kz jj"><em class="lu"/></strong>𝜎̂，我计算了 LASSO 模型中非零系数的数量，并使用了所有 100 个系数，加上 1 个用于岭模型的截距，因为它将系数偏向于零，但没有将它们设置为零。</p><p id="e4f8" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">显然，正则化的线性模型对于推断的目的是无效的，因为它们使系数的估计有偏差。我仍然认为，调查误差项的估计标准差中的任何偏差都值得编写一点代码。</p><p id="4d67" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这些图位于这个代码块的下方，用于模拟。</p><figure class="lw lx ly lz gt iv"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="acba" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">通过目测，<strong class="kz jj"><em class="lu"/></strong>在 LASSO 模型中出现向下的无偏估计，但是无偏估计并不像在最好的 AIC 和 BIC 模型中那样形成一个上界。岭模型在估计这个参数时没有表现出明显的偏差。让我们用配对 t 检验来研究一下，因为估计值是在每次迭代中从同一个样本中得到的。我使用标准的 p 值截止值 0.05，因为我懒得决定我想要的测试功效。</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/5a3270cbcbbe62fedb341c6b4ffdf50a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-gSC_YisuicGri3l5zCdJA.png"/></div></div></figure><figure class="lw lx ly lz gt iv"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/510814d58c84b4e60c93a9441131c3f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*6dZnwCtPVayGtqbvGlksSA.png"/></div></figure><p id="adb5" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">正如目测所猜测的，没有足够的证据表明来自真模型和岭模型的<strong class="kz jj"><em class="lu"/></strong>的估计值之间存在均值差异。然而，在 0.05 的显著性水平上有足够的证据可以得出结论，LASSO 模型倾向于对<strong class="kz jj"><em class="lu"/></strong>做出向下有偏的估计。这是否是一个普遍的事实还不知道。做出结论需要正式的证据。</p><p id="7b28" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">谢谢你坚持到最后。虽然使用数据选择模型会使经典的推断假设失效，但是<a class="ae lt" href="http://statweb.stanford.edu/~tibs/ftp/nips2015.pdf" rel="noopener ugc nofollow" target="_blank">选择后的推断</a>是统计研究的一个热门领域。也许几年后我们会谈到自体感染。</p><p id="ebd9" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我这个项目的所有代码都可以在这里找到<a class="ae lt" href="https://github.com/jkclem/Basics-of-Best-Subset-Selection" rel="noopener ugc nofollow" target="_blank"/>。</p></div></div>    
</body>
</html>