<html>
<head>
<title>Paper Weekly: Image reconstruction without data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">纸周刊:没有数据的图像重建</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-tuesday-image-reconstruction-without-data-c2acdba1aa53?source=collection_archive---------29-----------------------#2020-02-17">https://towardsdatascience.com/paper-tuesday-image-reconstruction-without-data-c2acdba1aa53?source=collection_archive---------29-----------------------#2020-02-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8049" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每周二，我都会强调我在研究或工作中遇到的一篇有趣的论文。希望我的评论能帮助你在2分钟内获得论文中最多汁的部分！</p><h1 id="44be" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">基本思想</h1><p id="5942" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">图像重建是一项具有挑战性的学习任务，因为没有人知道原始图像是什么样子的。因此，似乎唯一实用且合乎逻辑的方法是开发关于图像的一些先验知识，并挑选具有最大概率(最大先验估计)的重建。例如，我们希望在MNIST数据集上训练的模型能够开发一些关于手写数字的先验知识，可以用来对模糊的数字进行降噪。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/769ae0d495f71c469dfb76d9ecfa247e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eC2yJdb_g5wS-UwrpmBcbA.png"/></div></div></figure><p id="9c44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我偶然看到了这篇名为<em class="ma"> Deep Image Prior </em>的论文，该论文由Ulynov、Veldadi和Lempitsky于2017年发表。下面是链接:<a class="ae mb" href="https://sites.skoltech.ru/app/data/uploads/sites/25/2018/04/deep_image_prior.pdf" rel="noopener ugc nofollow" target="_blank">https://sites . skol tech . ru/app/data/uploads/sites/25/2018/04/deep _ image _ prior . pdf</a></p><p id="cb3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">研究人员发现了深度CNN的一个有趣特性——随机初始化的网络比纯噪声更快地拟合自然图像。换句话说，CNN对自然图像有天然的“优先”偏好，可以利用这种偏好在没有任何数据的情况下去除图像中的伪影！</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mc"><img src="../Images/46042398a0dc6c2b1c4bb1d37a8328aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ze5RPQmMdbLa5EQ-CJ3MNg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">来自<a class="ae mb" href="https://sites.skoltech.ru/app/data/uploads/sites/25/2018/04/deep_image_prior.pdf" rel="noopener ugc nofollow" target="_blank">的论文</a></p></figure><p id="c6bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了对图像去噪(去除水印、去除修补等)，随机初始化CNN并将图像馈送到模型中(输入=图像，输出=图像，就像自动编码器一样)。不出所料，模型逐渐实现零训练损失(参数&gt; &gt;图像中的像素)。然而，当适当地提前停止训练时，网络产生去噪图像。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mh"><img src="../Images/e73485c9e2176659bd646ac48719c108.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_GHiphNX_6zg5HVnnYLaFw.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">培训目标，来自<a class="ae mb" href="https://sites.skoltech.ru/app/data/uploads/sites/25/2018/04/deep_image_prior.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h1 id="7ca4" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">结果</h1><p id="fe03" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">图像重建可以采取多种形式——去噪、修复恢复、超分辨率等。研究人员证明，神经网络架构在免训练图像重建中起着重要作用</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mi"><img src="../Images/56519a40de684a0149039a145f0f653d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q6Plh1NapXYGWOlGVO0njg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">摘自<a class="ae mb" href="https://sites.skoltech.ru/app/data/uploads/sites/25/2018/04/deep_image_prior.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="7313" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当选择合适的网络架构时，DIP会产生惊人的重建性能，与监督模型相当，甚至超过监督模型:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mj"><img src="../Images/80927c3334bf317a91871569c0ca692f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SGOvYvqPMs86gkXDIjIF-Q.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">来自<a class="ae mb" href="https://sites.skoltech.ru/app/data/uploads/sites/25/2018/04/deep_image_prior.pdf" rel="noopener ugc nofollow" target="_blank">的论文</a></p></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mk"><img src="../Images/c1c525f94a3c47fa17bbb0ade02366e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_hFFdZ4MWmSQ9mQ30Yyy-Q.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">来自<a class="ae mb" href="https://sites.skoltech.ru/app/data/uploads/sites/25/2018/04/deep_image_prior.pdf" rel="noopener ugc nofollow" target="_blank">的论文</a></p></figure><h1 id="1860" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">一些想法</strong></h1><p id="2ee3" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">DIP是过度参数化(权重&gt; &gt;数据)的力量被完全低估的另一个证明。在我的ML课上，我的教授警告我们不要在“小”数据集上使用巨型模型，因为可能会过度拟合。然而，超参数化网络有许多有趣的特性可以利用！如果我们可以用一个<strong class="jp ir">未训练的</strong>模型对图像去噪，谁知道一个有无数参数的训练有素的模型能做什么？</p></div></div>    
</body>
</html>