# 了解线性回归

> 原文：<https://towardsdatascience.com/understanding-linear-regression-eaaaed2d983e?source=collection_archive---------13----------------------->

## 线性回归背后的数学详细解释

![](img/81138d249163b51e15ccd933d05238c3.png)

作者图片

假设你想从网上商店买一台新电脑(你最感兴趣的是它有多少内存)，你在首页上看到一些 4GB 的电脑售价 100 美元，还有一些 16 GB 的售价 1000 美元。你的预算是 500 美元。因此，你在脑子里估计，根据你目前看到的价格，一台 8 GB 内存的电脑应该在 400 美元左右。这将符合您的预算，并决定购买一台 8 GB 内存的电脑。

这种估计几乎可以在你的大脑中自动发生，而不知道它被称为线性回归，也不需要在你的大脑中明确计算回归方程(在我们的例子中:y = 75x - 200)。

那么，线性回归是什么*？*

我将尝试简单地回答这个问题:

> 线性回归就是根据一些已知的量来估计一个未知量的过程(这是回归部分)，条件是未知量可以通过只使用标量乘法和加法(这是线性部分)这两种运算从已知量中获得。我们将每个已知量乘以某个数，然后将所有这些项相加，得到未知量的估计值。

当它以正式的数学方式或代码描述时，它可能看起来有点复杂，但事实上，如上所述的简单估计过程，你可能在听到机器学习之前就已经知道了。只是你不知道这叫线性回归。

现在，让我们深入线性回归背后的数学。

在线性回归中，我们获得未知变量的估计值(用 y 表示；我们的模型的输出)通过计算我们的已知变量(用 xᵢ表示；输入)，我们向其添加了偏置项。

![](img/325e6eb6d8b4acb33b4c372e3a85e01f.png)

其中 **n** 是我们拥有的数据点的数量。

添加偏差与想象我们有一个始终为 1 的额外输入变量并且只使用权重是一样的。我们将考虑这种情况，使数学符号变得简单一点。

![](img/7fd0d056bf4ed3f26799e32fab62d2c0.png)

其中 **x₀** 永远是 1， **w₀** 是我们之前的 b

为了使记法简单一点，我们将从上面的和记法过渡到矩阵记法。上述等式中的加权和相当于所有输入变量的行向量与所有权重的列向量的乘积。那就是:

![](img/85de3b179d55e1d08414e7284b84dd13.png)

上面的等式只针对一个数据点。如果我们想一次计算更多数据点的输出，我们可以将输入行连接成一个矩阵，用 **X** 表示。对于所有这些不同的输入行，权重向量将保持不变，我们将用 **w** 表示。现在 **y** 将用于表示具有所有输出的列向量，而不仅仅是单个值。这个新方程的矩阵形式如下:

![](img/c02aabf43ab0bd410a76316aeaec75e8.png)

给定一个输入矩阵 **X** 和一个权重向量 **w** ，我们可以使用上面的公式很容易地得到 **y** 的值。假设输入值是已知的，或者至少是容易获得的。

但问题是:我们如何获得权重向量？

我们将从例子中学习它们。为了学习权重，我们需要一个数据集，其中我们知道 x 和 y 值，基于这些我们将找到权重向量。

如果我们的数据点是定义我们的回归线所需的最小值(比输入数多 1)，那么我们可以简单地求解方程(1)得到 **w** :

![](img/85da2b5ec9b0bc9ad17f550c6e487ffc.png)

我们称这个东西为回归线，但实际上，它是一条只针对 1 个输入的线。对于 2 个输入，它将是一个平面，对于 3 个输入，它将是某种“3D 平面”，等等。

大多数情况下，上述解决方案的要求并不成立。大多数时候，我们的数据点不会完全符合一条直线。我们的回归线周围会有一些随机噪声，我们将无法获得 **w** 的精确解。然而，我们将尝试为 **w** 获得*的最佳可能解*，以使误差最小。

如果等式(1)无解，这意味着 **y** 不属于 **X** 的列空间。因此，我们将使用 **y** 在 **X** 的列空间上的投影，而不是 **y** 。这是离 **y** 最近的向量，也属于 **X** 的列空间。如果我们将等式两边相乘。(1)通过 **X** 的转置，我们将得到一个考虑了这个投影的方程。你可以在麻省理工学院的吉尔伯特·斯特朗的讲座中找到更多关于解决这个问题的线性代数方法。

![](img/227cc4733c6083060f2416416e9b6fc7.png)

虽然这个解决方案对 **X** 的限制比我们之前的方案要少，但是在某些情况下还是不行；我们将在下面看到更多关于这个问题的内容。

另一种获得 w 的方法是使用微积分。主要思想是定义一个误差函数，然后用微积分找到最小化这个误差函数的权重。

我们将定义一个函数 **f** ,它将一个权重向量作为输入，并给出这些权重将在我们的线性回归问题上产生的平方误差。该函数只是查看数据集的每个真实 y 值与回归模型的估计 y 值之间的差异。然后将所有这些差值平方并相加。在矩阵符号中，这个函数可以写成:

![](img/6b18cdd7a43f164c76ac41020df44a4e.png)

如果这个函数有最小值，它应该在临界点之一(梯度 **∇f** 为 0 的点)。所以，我们来找临界点。如果你不熟悉矩阵微分，可以看看[这篇](https://en.wikipedia.org/wiki/Matrix_calculus)维基百科的文章。

我们从计算梯度开始:

![](img/e5d67245d510a257b0fb1852720dcc89.png)

然后我们将其设为 0，并求解 **w** :

![](img/afe776368c891799fffd4a67f725546a.png)

我们有一个关键点。现在我们应该弄清楚它是最小值还是最大值。为此，我们将计算 *Hessian 矩阵*并建立函数 **f** 的凸凹性。

![](img/fe9d109c3bcba7ec45d7b45f9a6516f3.png)

现在，我们可以观察到关于 **H** 的什么？如果我们取任意实值向量 **z** 并将其乘以 **H** 的两边，我们将得到:

![](img/44576e6ddcc38336f996d6b7e6a0c930.png)

因为 **f** 是一个凸函数，这意味着我们上面找到的 **w** 的解是一个极小点，这正是我们要找的。

正如你可能注意到的，我们通过使用前面的线性代数方法和这种寻找权重的微积分方法得到了相同的解决方案。我们可以认为它要么是我们用 **y** 到 **X** 的列空间上的投影代替 **y** 时矩阵方程的解，要么是使误差平方和最小的点。

这种解决方案总是有效吗？号码

比平凡解限制少: **w = X** ⁻ **y** 其中我们需要 **X** 为方阵非奇异矩阵，但还是需要一些条件才能成立。我们需要 **Xᵀ X** 是可逆的，为此 ***X*** *需要有完整的列秩*；也就是说，它的所有列都是线性独立的。如果我们的行数比列数多，通常就满足了这个条件。但是如果我们的数据例子比输入变量少，这个条件就不可能成立。

这个 **X** 具有满列秩的要求与 **f** 的凸性密切相关。如果你看上面那个关于 **f** 是凸的小证明，你会注意到，如果 **X** 具有满列秩，那么 **X z** 不可能是零向量(假设 **z ≠ 0** )，这意味着 **H** 是正定的，因此 **f** 是严格*凸的。如果 **f** 是严格凸的，那么它只能有一个最小点，这就解释了为什么在这种情况下我们可以有一个封闭形式的解。*

另一方面，如果 **X** 没有满列秩，那么会有一些 **z ≠ 0** ，对于这些 **X z = 0** ，因此 **f** 是非严格凸的。这意味着 **f** 可能没有单一的最小点，但是有一个同样好的最小点的山谷，我们的封闭形式的解决方案不能捕获所有的最小点。视觉上，未满列秩的情况在 3D 中看起来像这样:

![](img/f8863b749be305f9c9b2fabcb8e22429.png)

来源: [GeoGebra](https://www.geogebra.org/3d)

一种即使在这种情况下也能给我们解决方案的方法是*随机梯度下降* (SGD)。这是一种迭代方法，从误差函数 **f** 表面上的随机点开始，然后在每次迭代中，沿着梯度 **∇f** 的负方向向谷底前进。

这个方法总会给我们一个结果(即使有时候需要大量迭代才能水落石出)；在 **X** 上不需要任何条件。

此外，为了提高计算效率，它不会一次使用所有数据。我们的数据矩阵 **X** 被垂直分割成批。在每次迭代中，仅基于一个这样的批次进行更新。

在列秩 **X** 不全的情况下，解不会唯一；在“最小谷”中的所有点中，SGD 将只给出一个依赖于随机初始化和批次随机化的点。

SGD 是一种更通用的方法，它不仅仅局限于线性回归；它还用于更复杂的机器学习算法，如神经网络。但在最小二乘线性回归的情况下，我们的优势在于，由于误差函数的凸性，SGD 不会陷入局部最小值，而这在神经网络中是常见的情况。当这种方法将达到最小值时，它将是一种全球性的方法。下面是该算法的简要概述:

![](img/803006022f56db7fc7f8cddfcbc20828.png)

其中 **α** 是一个常数，称为*学习率*。

现在，如果我们插入本文中上面计算的梯度，我们会得到下面专门针对最小二乘线性回归的结果:

![](img/10ce771fffe67668a5488a8a04325a4e.png)

暂时就这样了。在接下来的两篇文章中，我还将展示如何使用一些数值库(如 NumPy、TensorFlow 和 PyTorch)实现线性回归。

[](/how-to-implement-linear-regression-with-numpy-172790d2f1bc) [## 如何用 NumPy 实现线性回归

### 更好地理解线性回归并提高您的数字技能

towardsdatascience.com](/how-to-implement-linear-regression-with-numpy-172790d2f1bc) [](https://medium.com/towards-artificial-intelligence/how-to-implement-linear-regression-with-tensorflow-406b2cff1ffa) [## 如何用 TensorFlow 实现线性回归

### 通过实施线性回归学习张量流基础知识

medium.com](https://medium.com/towards-artificial-intelligence/how-to-implement-linear-regression-with-tensorflow-406b2cff1ffa) [](/how-to-implement-linear-regression-with-pytorch-5737339296a6) [## 如何用 PyTorch 实现线性回归

### 通过实现线性回归学习 PyTorch 基础知识

towardsdatascience.com](/how-to-implement-linear-regression-with-pytorch-5737339296a6) 

*我希望这些信息对你有用，感谢你的阅读！*

这篇文章也贴在我自己的网站[这里](https://www.nablasquared.com/understanding-linear-regression/)。随便看看吧！