<html>
<head>
<title>The Holy Trinity of Topological Machine Learning: Gudhi, Scikit-Learn and Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">拓扑机器学习的神圣三位一体:Gudhi、Scikit-Learn和Tensorflow</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-holy-trinity-of-topological-machine-learning-gudhi-scikit-learn-and-tensorflow-pytorch-3cda2aa249b5?source=collection_archive---------21-----------------------#2020-02-16">https://towardsdatascience.com/the-holy-trinity-of-topological-machine-learning-gudhi-scikit-learn-and-tensorflow-pytorch-3cda2aa249b5?source=collection_archive---------21-----------------------#2020-02-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/776ca5a53ade4ea4279d8a193878e5f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*NMdAgH3-5j3pfOAGmJmxbQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">库斯科学校，“加冕的三位一体”(约1730)，利马艺术博物馆(公共领域)</p></figure><div class=""/><p id="e236" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">大家好！今天，我想强调拓扑数据分析在机器学习中的作用，并演示如何结合使用三个Python库来实际使用它:Gudhi、Scikit-Learn和Tensorflow。</p><h1 id="8fe3" class="ld le ji bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">拓扑数据分析？</h1><p id="23c8" class="pw-post-body-paragraph kf kg ji kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">先说第一件事，先说<a class="ae mg" href="https://en.wikipedia.org/wiki/Topological_data_analysis" rel="noopener ugc nofollow" target="_blank">拓扑数据分析</a> (TDA)。这是一个相对较小的数据科学领域(特别是与机器学习和深度学习相比)，但它正在积极发展，并引起了数据科学家的关注。许多初创企业和公司实际上正在将这些技术集成到他们的工具箱中(我们正在谈论的是<a class="ae mg" href="https://researcher.watson.ibm.com/researcher/view_group.php?id=6585" rel="noopener ugc nofollow" target="_blank"> IBM </a>、<a class="ae mg" href="https://www.fujitsu.com/global/about/resources/news/press-releases/2016/0216-01.html" rel="noopener ugc nofollow" target="_blank">富士通</a>、<a class="ae mg" href="https://www.ayasdi.com/platform/technology/" rel="noopener ugc nofollow" target="_blank"> Ayasdi </a> …),由于它最近在各种应用中取得的成功，例如<a class="ae mg" href="https://www.ncbi.nlm.nih.gov/pubmed/28459448" rel="noopener ugc nofollow" target="_blank">生物学</a>、<a class="ae mg" href="https://www.ams.org/journals/notices/201905/rnoti-p686.pdf" rel="noopener ugc nofollow" target="_blank">时间序列</a>、<a class="ae mg" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2931836" rel="noopener ugc nofollow" target="_blank">金融</a>、<a class="ae mg" href="https://topology-tool-kit.github.io/" rel="noopener ugc nofollow" target="_blank">科学可视化</a>、<a class="ae mg" href="http://www.lix.polytechnique.fr/~maks/papers/top_opt_SGP18.pdf" rel="noopener ugc nofollow" target="_blank">计算机图形学</a>……我可能会写一篇关于这方面的帖子-)</p><p id="1a05" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">TDA的目标是计算和编码数据的拓扑，这意味着记录数据集中各种连接的组件、环路、空腔和高维结构。这可能非常有用，主要是因为这种类型的信息不能由其他描述符计算，所以TDA真正存储了一组独特的数据特征，这些特征在其他任何地方都找不到。事实证明，这些功能经常被证明对增强机器学习预测是有用的，所以如果你以前没有见过或听说过这些功能，我会让你跟上速度；-)</p><p id="5981" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我已经写了几篇关于这个主题的<a class="ae mg" rel="noopener" target="_blank" href="/a-concrete-application-of-topological-data-analysis-86b89aa27586">文章</a>中的<a class="ae mg" rel="noopener" target="_blank" href="/mixing-topology-and-deep-learning-with-perslay-2e60af69c321">两篇</a>，你可以在Medium上找到<a class="ae mg" rel="noopener" target="_blank" href="/applied-topological-data-analysis-to-deep-learning-hands-on-arrhythmia-classification-48993d78f9e6">许多</a> <a class="ae mg" rel="noopener" target="_blank" href="/persistent-homology-with-examples-1974d4b9c3d0">其他</a> <a class="ae mg" rel="noopener" target="_blank" href="/topological-data-analysis-unpacking-the-buzzword-2fab3bb63120">关于TDA的帖子</a>，所以我不会花时间在数学定义上，而是通过分解TDA文献的一个经典例子来说明如何在你的数据集上应用TDA。</p><h1 id="458b" class="ld le ji bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">TDA的参考范例:点云分类</h1><p id="05bf" class="pw-post-body-paragraph kf kg ji kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">这个数据集是在TDA的一篇开创性文章<a class="ae mg" href="http://jmlr.org/papers/v18/16-337.html" rel="noopener ugc nofollow" target="_blank">中介绍的。它由通过生成以下动力系统的轨道而获得的一组点云组成:</a></p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mh"><img src="../Images/6bad6b6649a574b4c71eadd6825a210a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lkAcZaozqZ_2uu7NQTJtzA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">动力系统方程。</p></figure><p id="731e" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这意味着我们将在单位正方形中随机选取一个初始点，并用上面的等式生成一系列点。这将给出一个单点云。现在我们可以随心所欲地重复操作，最终得到一堆点云。这些云的一个有趣的特性是，根据您用来生成点序列的<em class="mm"> r </em>参数的值，点云最终会有非常不同和有趣的结构。例如，如果<em class="mm"> r = 3.5 </em>，产生的点云似乎覆盖了整个单位正方形，而如果<em class="mm"> r = 4.1 </em>，单位正方形中的一些区域是空的:换句话说，你的点云中有洞。这对我们来说是个好消息:TDA可以直接计算出这些结构的存在。</p><div class="mi mj mk ml gt ab cb"><figure class="mn iv mo mp mq mr ms paragraph-image"><img src="../Images/17ba729d74e6b5e2a8855a9d03325800.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*KeyArwABBgfurO78VDJjQA.png"/></figure><figure class="mn iv mt mp mq mr ms paragraph-image"><img src="../Images/550df80c715f91e8cba1553db6c3feb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*e-3dmLuvB4pokkrNU9Eosw.png"/><p class="jc jd gj gh gi je jf bd b be z dk mu di mv mw translated">用r = 3.5(左)和r = 4.1(右)计算的点云。很明显，后者有一个洞，而前者没有。</p></figure></div><p id="cd99" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">TDA追踪这些黑洞的方法其实很简单。想象给定半径的球以你的点云的每个点为中心。如果<em class="mm"> R = 0 </em>，这些球的并集就是点云本身。如果<em class="mm"> R =无穷大</em>，球的并集就是全单位正方形。但是如果仔细选择，球的结合可能包含许多拓扑结构，比如洞。</p><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/ad98d188012fed1c1fd7f404577f7fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3HP6GAezUloAkaPev3hbcQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">球联合的例子。对于中间的union，这个union显然形成了一个洞。这张图片是无耻地从我以前的一篇文章中重复使用的。</p></figure><p id="5a5c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">因此，为了避免不得不为<em class="mm"> R </em>手动选择这个“好”值，TDA将为0到无穷大之间的<em class="mm"> R </em>的所有可能值计算这个球的联合，并记录每个球洞出现和消失的半径，并将这些值用作一些点的2D坐标。TDA的输出是另一个点云，其中每个点代表一个洞:它被称为<em class="mm"> Rips持久性图</em>。假设点云存储在一个形状为(n x 2)的numpy数组<em class="mm"> X </em>中，可以用下面这段代码用<a class="ae mg" href="http://gudhi.gforge.inria.fr/python/latest/" rel="noopener ugc nofollow" target="_blank"> Gudhi </a>计算两行图表:</p><pre class="mi mj mk ml gt my mz na nb aw nc bi"><span id="8e4a" class="nd le ji mz b gy ne nf l ng nh">import gudhi</span><span id="baf2" class="nd le ji mz b gy ni nf l ng nh">rips = gudhi.RipsComplex(points=X).create_simplex_tree()<br/>dgm = rips.persistence()</span></pre><figure class="mi mj mk ml gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/cd3008c52e42776f0b164d62c4e5c6ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*XK82qjIpXuPqg7JxbAf2Hw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">根据对应于r = 4.1的点云计算出的漂亮的持久性图。红色的点代表连接的组件，蓝色的点代表孔。</p></figure><p id="c3a7" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">给定点云，我们要解决的任务是预测<em class="mm"> r </em>的值。</p><h1 id="c3be" class="ld le ji bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">用Gudhi + Scikit-Learn进行拓扑机器学习</h1><p id="44c8" class="pw-post-body-paragraph kf kg ji kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">持久性图很简洁，对吗？但是它们的问题是，从不同的点云中计算的持久性图可能有不同的点数(因为点云可能有不同数量的洞)。因此，如果您想用Scikit-Learn从持久性图中预测<em class="mm"> r </em>，不幸的是，没有直接的方法，因为这些库期望将结构化向量作为输入。这就是为什么有大量的工作将持久性图转换成固定长度的欧几里得向量，甚至开发持久性图的内核。那太好了，但是你应该使用哪一个呢？</p><p id="89c1" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">不用再担心了！<a class="ae mg" href="http://gudhi.gforge.inria.fr/python/latest/" rel="noopener ugc nofollow" target="_blank">古迪</a>又来找你了。使用其<a class="ae mg" href="https://gudhi.inria.fr/python/latest/representations.html" rel="noopener ugc nofollow" target="_blank"> <em class="mm">表示</em> </a>模块，您不仅可以计算所有这些矢量化和内核，还可以完全使用Scikit-Learn来交叉验证和/或选择最佳的。就像这样简单:</p><pre class="mi mj mk ml gt my mz na nb aw nc bi"><span id="5615" class="nd le ji mz b gy ne nf l ng nh">import gudhi.representations as tda<br/><strong class="mz jj">from</strong> <strong class="mz jj">sklearn.pipeline</strong> <strong class="mz jj">import</strong> Pipeline<br/><strong class="mz jj">from</strong> <strong class="mz jj">sklearn.svm</strong> <strong class="mz jj">import</strong> SVC<br/><strong class="mz jj">from</strong> <strong class="mz jj">sklearn.ensemble</strong> <strong class="mz jj">import</strong> RandomForestClassifier as RF<br/><strong class="mz jj">from</strong> <strong class="mz jj">sklearn.neighbors</strong> <strong class="mz jj">import</strong> KNeighborsClassifier as kNN<br/><strong class="mz jj">from</strong> <strong class="mz jj">sklearn.model_selection</strong> <strong class="mz jj">import</strong> GridSearchCV</span><span id="a225" class="nd le ji mz b gy ni nf l ng nh">pipe = Pipeline([("TDA",       tda.PersistenceImage()),<br/>                 ("Estimator", SVC())])<br/><br/>param =    [{"TDA": [tda.SlicedWassersteinKernel()], <br/>             "TDA__bandwidth": [0.1, 1.0],<br/>             "TDA__num_directions": [20],<br/>             "Estimator": [SVC(kernel="precomputed")]},<br/>            <br/>            {"TDA": [tda.PersistenceWeightedGaussianKernel()], <br/>             "TDA__bandwidth": [0.1, 0.01],<br/>             "TDA__weight": [<strong class="mz jj">lambda</strong> x: np.arctan(x[1]-x[0])], <br/>             "Estimator": [SVC(kernel="precomputed")]},<br/>            <br/>            {"TDA": [tda.PersistenceImage()], <br/>             "TDA__resolution": [ [5,5], [6,6] ],<br/>             "TDA__bandwidth": [0.01, 0.1, 1.0, 10.0],<br/>             "Estimator": [SVC()]},<br/>            <br/>            {"TDA": [tda.Landscape()], <br/>             "TDA__resolution": [100],<br/>             "Estimator": [RF()]},<br/>           <br/>            {"TDA": [tda.BottleneckDistance()], <br/>             "TDA__epsilon": [0.1], <br/>             "Estimator: [kNN(metric="precomputed")]}<br/>           ]</span><span id="d0de" class="nd le ji mz b gy ni nf l ng nh">model = GridSearchCV(pipe, param, cv=3)<br/>model = model.fit(diagrams, labels)</span></pre><p id="20a3" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在前面的代码中，我基本上是用<a class="ae mg" href="http://proceedings.mlr.press/v70/carriere17a/carriere17a.pdf" rel="noopener ugc nofollow" target="_blank">切片Wasserstein内核</a>和<a class="ae mg" href="http://proceedings.mlr.press/v48/kusano16.html" rel="noopener ugc nofollow" target="_blank">持久性加权高斯内核</a>来尝试内核SVM，用<a class="ae mg" href="http://jmlr.org/papers/v18/16-337.html" rel="noopener ugc nofollow" target="_blank">持久性图像</a>来尝试C-SVM，用<a class="ae mg" href="http://www.jmlr.org/papers/volume16/bubenik15a/bubenik15a.pdf" rel="noopener ugc nofollow" target="_blank">持久性景观</a>来尝试随机森林，以及用所谓的持久性图之间的瓶颈距离来尝试简单的k-NN。在<a class="ae mg" href="http://gudhi.gforge.inria.fr/python/latest/" rel="noopener ugc nofollow" target="_blank"> Gudhi </a>里还有很多其他的可能性，你一定要去看看！如果你想了解更多的细节，你也可以看看<a class="ae mg" href="https://github.com/GUDHI/TDA-tutorial/blob/master/Tuto-GUDHI-representations.ipynb" rel="noopener ugc nofollow" target="_blank">这个古迪教程</a>。</p><h1 id="7c1f" class="ld le ji bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">Gudhi + Tensorflow拓扑优化</h1><p id="ee5f" class="pw-post-body-paragraph kf kg ji kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">我很确定你现在已经对TDA上瘾了。如果你还是不相信，我有别的东西给你，是从<a class="ae mg" href="https://arxiv.org/abs/1905.12200" rel="noopener ugc nofollow" target="_blank">这篇论文</a>得到的启发。假设你现在想解决一个更难的问题:我想让你给我一个点云，它的持久性图有尽可能多的点。换句话说，你要生成一个有很多洞的点云。</p><p id="d6b6" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我能看到你额头上的汗水。但我是慈悲为怀的那种人，让你知道<a class="ae mg" href="http://gudhi.gforge.inria.fr/python/latest/" rel="noopener ugc nofollow" target="_blank"> Gudhi </a>眨眼之间就能做到这一点。想一想:当你生成一个持久性图时，这个图中不同点的坐标并不依赖于完整的原始点云，对吗？对于该图中的给定点<em class="mm">p</em>,<em class="mm">p</em>的坐标仅以简单的方式取决于原始点云中形成洞的点的位置(对应于<em class="mm"> p </em>):这些坐标仅仅是球的联合使这个洞出现或消失的半径——或者等价地，这些点内的最大成对距离。原来<a class="ae mg" href="https://gudhi.inria.fr/python/latest/" rel="noopener ugc nofollow" target="_blank"> Gudhi </a>可以用它的<em class="mm"> persistence_pairs() </em>函数吐出这些依赖关系。然后，梯度被简单地定义为欧几里德距离函数的导数(正式定义见<a class="ae mg" href="https://sites.google.com/view/hiraoka-lab-en/research/mathematical-research/continuation-of-point-cloud-data-via-persistence-diagram" rel="noopener ugc nofollow" target="_blank">本文</a>)。</p><p id="dfcd" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">因此，让我们编写几个函数，一个从点云中计算Rips持久性图，另一个计算持久性图点的导数。为了可读性，我稍微简化了代码，实际的代码可以在<a class="ae mg" href="https://github.com/GUDHI/TDA-tutorial/blob/master/Tuto-GUDHI-optimization.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="mi mj mk ml gt my mz na nb aw nc bi"><span id="0902" class="nd le ji mz b gy ne nf l ng nh"><strong class="mz jj">def</strong> Rips(DX, mel, dim):<br/>    rc = gd.RipsComplex(distance_matrix=DX, max_edge_length=mel)<br/>    st = rc.create_simplex_tree(max_dimension=dim+1)<br/>    dgm = st.persistence()<br/>    indices = st.persistence_pairs()<br/>    <strong class="mz jj">return</strong> indices</span><span id="9975" class="nd le ji mz b gy ni nf l ng nh"><strong class="mz jj">class</strong> RipsModel(tf.keras.Model):<br/>    <strong class="mz jj">def</strong> __init__(self, X, mel=12, dim=1, card=50):<br/>        super(RipsModel, self).__init__()<br/>        self.X = X<br/>        self.mel = mel<br/>        self.dim = dim<br/>        self.card = card<br/>        <br/>    <strong class="mz jj">def</strong> call(self):<br/>        m, d, c = self.mel, self.dim, self.card<br/>        <br/>        # Compute distance matrix<br/>        DX = tfa.losses.metric_learning.pairwise_distance(self.X)<br/>        DXX = tf.reshape(DX, [1, DX.shape[0], DX.shape[1]])<br/>        <br/>        # Turn numpy function into tensorflow function<br/>        RipsTF = lambda DX: tf.numpy_function(Rips, [DX, m, d, c], [tf.int32 for _ in range(4*c)])<br/>        <br/>        # Compute vertices associated to positive and negative simplices <br/>        # Don't compute gradient for this operation<br/>        ids = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(RipsTF,DXX,dtype=[tf.int32 for _ in range(4*c)]))<br/>        <br/>        # Get persistence diagram by simply picking the corresponding entries in the distance matrix<br/>        dgm = tf.reshape(tf.gather_nd(DX, tf.reshape(ids, [2*c,2])), [c,2])<br/>        <strong class="mz jj">return</strong> dgm</span></pre><p id="217f" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，让我们定义一个损耗，它与余辉图中指向对角线的距离相反。这将迫使图表有许多点，纵坐标比横坐标大得多，相当于一个有许多大孔的点云。</p><pre class="mi mj mk ml gt my mz na nb aw nc bi"><span id="ebd1" class="nd le ji mz b gy ne nf l ng nh">model = RipsModel()<br/>optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)</span><span id="4017" class="nd le ji mz b gy ni nf l ng nh"><strong class="mz jj">for</strong> epoch <strong class="mz jj">in</strong> range(100):<br/>    <br/>    <strong class="mz jj">with</strong> tf.GradientTape() <strong class="mz jj">as</strong> tape:<br/>        <br/>        dgm = model.call()        <br/>        loss = -tf.math.reduce_sum(tf.square(.5*(dgm[:,1]-dgm[:,0])))<br/>        gradients = tape.gradient(loss, model.trainable_variables)<br/>        optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span></pre><p id="66ef" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在我们来优化一下！以下是第0、20和90个时期的结果:</p><div class="mi mj mk ml gt ab cb"><figure class="mn iv nk mp mq mr ms paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/27d2850de9a41934721d1b5b1b5c3cc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*CsI_ZRuj7_tNT5FaqeHu9g.png"/></div></figure><figure class="mn iv nk mp mq mr ms paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/080ba8ad132102b372cc539dc36a1260.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*hv6NF5r8DtzV6gN1Mymn5Q.png"/></div></figure><figure class="mn iv nk mp mq mr ms paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/1f013b1f0de61c5670752fb09a643ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*kayI95C8gzsYRmM9_nMKBg.png"/></div></figure></div><p id="60b7" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如此多的洞，如此美丽…我们活在梦里。如果你想继续玩其他损失，检查<a class="ae mg" href="https://github.com/GUDHI/TDA-tutorial/blob/master/Tuto-GUDHI-optimization.ipynb" rel="noopener ugc nofollow" target="_blank">这个Gudhi教程</a>，它也包含了另一个关于图像优化的例子。</p><h1 id="509e" class="ld le ji bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">最后一句话…</h1><p id="38f0" class="pw-post-body-paragraph kf kg ji kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">这篇文章只是结合Gudhi、Scikit-Learn和Tensorflow所提供的许多可能性的一瞥。我希望我说服了你，将TDA整合到你的管道中已经变得非常简单了。尽管TDA的许多应用已经出现在文献中，但肯定还有更多需要发现！</p></div></div>    
</body>
</html>