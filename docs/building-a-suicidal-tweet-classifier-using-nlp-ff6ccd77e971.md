# ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æ„å»ºè‡ªæ€æ¨ç‰¹åˆ†ç±»å™¨

> åŸæ–‡ï¼š<https://towardsdatascience.com/building-a-suicidal-tweet-classifier-using-nlp-ff6ccd77e971?source=collection_archive---------32----------------------->

## ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æ¥é¢„æµ‹æ¨ç‰¹ä¸Šçš„è‡ªæ€æ„å¿µã€‚

å¤šå¹´æ¥ï¼Œè‡ªæ€ä¸€ç›´æ˜¯å…¨çƒèŒƒå›´å†…çš„ä¸»è¦æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œæ ¹æ®[ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Suicide#:~:text=Suicides%20resulted%20in%20828%2C000%20global,of%20people%20die%20by%20suicide.)çš„æ•°æ®ï¼Œ2015 å¹´ï¼Œè‡ªæ€å¯¼è‡´å…¨çƒ 82.8 ä¸‡äººæ­»äº¡ï¼Œæ¯” 1990 å¹´çš„ 71.2 ä¸‡äººæœ‰æ‰€å¢åŠ ã€‚è¿™ä½¿å¾—è‡ªæ€æˆä¸ºå…¨çƒç¬¬åå¤§æ­»äº¡åŸå› ã€‚è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼Œäº’è”ç½‘å’Œç¤¾äº¤åª’ä½“èƒ½å¤Ÿå½±å“ä¸è‡ªæ€ç›¸å…³çš„è¡Œä¸ºã€‚ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé¢†åŸŸï¼Œæˆ‘å»ºç«‹äº†ä¸€ä¸ªéå¸¸ç®€å•çš„è‡ªæ€æ„å¿µåˆ†ç±»å™¨ï¼Œå®ƒå¯ä»¥é¢„æµ‹ä¸€ä¸ªæ–‡æœ¬æ˜¯å¦æœ‰è‡ªæ€å€¾å‘ã€‚

![](img/da3df62b1c8a1b9a63493c8765291d8c.png)

åˆ†æäº† Tweet çš„ WordCloud

# æ•°æ®

æˆ‘ä½¿ç”¨äº†ä¸€ä¸ªæˆ‘åœ¨ Github ä¸Šæ‰¾åˆ°çš„ Twitter çˆ¬è™«ï¼Œé€šè¿‡åˆ é™¤æ ‡ç­¾ã€é“¾æ¥ã€URL å’Œç¬¦å·å¯¹ä»£ç åšäº†ä¸€äº›ä¿®æ”¹ã€‚æ¯å½“å®ƒä» Twitter ä¸ŠæŠ“å–æ•°æ®æ—¶ï¼Œæ•°æ®éƒ½æ˜¯åŸºäºåŒ…å«å¦‚ä¸‹å•è¯çš„æŸ¥è¯¢å‚æ•°æŠ“å–çš„:

> æ²®ä¸§ï¼Œç»æœ›ï¼Œç­”åº”ç…§é¡¾ï¼Œæˆ‘ä¸å±äºè¿™é‡Œï¼Œæ²¡æœ‰äººå€¼å¾—æˆ‘ï¼Œæˆ‘æƒ³æ­»ç­‰ç­‰ã€‚

è™½ç„¶æœ‰äº›æ–‡æœ¬ä¸è‡ªæ€æ¯«æ— å…³ç³»ï¼Œä½†æˆ‘ä¸å¾—ä¸æ‰‹åŠ¨æ ‡æ³¨äº†å¤§çº¦ 8200 è¡Œæ¨æ–‡çš„æ•°æ®ã€‚æˆ‘è¿˜è·å–äº†æ›´å¤šçš„ Twitter æ•°æ®ï¼Œå¹¶ä¸”èƒ½å¤Ÿå°†æˆ‘ä»¥å‰æ‹¥æœ‰çš„è¶³å¤Ÿæˆ‘è®­ç»ƒçš„æ•°æ®è¿æ¥èµ·æ¥ã€‚

# æ„å»ºæ¨¡å‹

## æ•°æ®é¢„å¤„ç†

æˆ‘å¯¼å…¥äº†ä»¥ä¸‹åº“:

```
import pickle
import re
import numpy as np
import pandas as pd
from tqdm import tqdm
import nltk
nltk.download('stopwords')
```

ç„¶åï¼Œæˆ‘ç¼–å†™äº†ä¸€ä¸ªå‡½æ•°æ¥æ¸…ç†æ–‡æœ¬æ•°æ®ï¼Œåˆ é™¤ä»»ä½•å½¢å¼çš„ HTML æ ‡è®°ï¼Œä¿ç•™è¡¨æƒ…å­—ç¬¦ï¼Œåˆ é™¤éå•è¯å­—ç¬¦ï¼Œæœ€åè½¬æ¢ä¸ºå°å†™ã€‚

```
def preprocess_tweet(text):
    text = re.sub('<[^>]*>', '', text)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text)
    lowercase_text = re.sub('[\W]+', ' ', text.lower())
    text = lowercase_text+' '.join(emoticons).replace('-', '') 
    return text
```

ä¹‹åï¼Œæˆ‘å¯¹ tweet æ•°æ®é›†åº”ç”¨äº† preprocess_tweet å‡½æ•°æ¥æ¸…ç†æ•°æ®ã€‚

```
tqdm.pandas()df = pd.read_csv('data.csv')
df['tweet'] = df['tweet'].progress_apply(preprocess_tweet)
```

ç„¶åï¼Œæˆ‘ä½¿ç”¨ã€‚split()æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨è¯å¹²å°†æ–‡æœ¬è½¬æ¢ä¸ºå®ƒä»¬çš„æ ¹å½¢å¼ã€‚

```
**from** **nltk.stem.porter** **import** PorterStemmer
porter = PorterStemmer()
**def** tokenizer_porter(text):
    **return** [porter.stem(word) **for** word **in** text.split()]
```

ç„¶åæˆ‘å¯¼å…¥äº†åœç”¨è¯åº“æ¥åˆ é™¤æ–‡æœ¬ä¸­çš„åœç”¨è¯ã€‚

```
**from** **nltk.corpus** **import** stopwords
stop = stopwords.words('english')
```

åœ¨å•ä¸ªæ–‡æœ¬ä¸Šæµ‹è¯•å‡½æ•°ã€‚

```
[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]
```

è¾“å‡º:

```
['runner', 'like', 'run', 'run', 'lot']
```

## çŸ¢é‡å™¨

å¯¹äºè¿™ä¸ªé¡¹ç›®ï¼Œæˆ‘ä½¿ç”¨äº†**å“ˆå¸ŒçŸ¢é‡å™¨**ï¼Œå› ä¸ºå®ƒä¸æ•°æ®æ— å…³ï¼Œè¿™æ„å‘³ç€å®ƒçš„å†…å­˜éå¸¸ä½ï¼Œå¯æ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ï¼Œå¹¶ä¸”å®ƒä¸åœ¨å†…å­˜ä¸­å­˜å‚¨è¯æ±‡å­—å…¸ã€‚ç„¶åï¼Œæˆ‘ä¸ºå“ˆå¸ŒçŸ¢é‡å™¨åˆ›å»ºäº†ä¸€ä¸ªè®°å·èµ‹äºˆå™¨å‡½æ•°

```
def tokenizer(text):
    text = re.sub('<[^>]*>', '', text)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\(|D|P)',text.lower())
    text = re.sub('[\W]+', ' ', text.lower())
    text += ' '.join(emoticons).replace('-', '')
    tokenized = [w for w in tokenizer_porter(text) if w not in stop]
    return tokenized
```

ç„¶åæˆ‘åˆ›å»ºäº†æ•£åˆ—çŸ¢é‡å™¨å¯¹è±¡ã€‚

```
from sklearn.feature_extraction.text import HashingVectorizervect = HashingVectorizer(decode_error='ignore', n_features=2**21, 
                         preprocessor=None,tokenizer=**tokenizer**)
```

## æ¨¡å‹

å¯¹äºè¯¥æ¨¡å‹ï¼Œæˆ‘ä½¿ç”¨äº†éšæœºæ¢¯åº¦ä¸‹é™åˆ†ç±»å™¨ç®—æ³•ã€‚

```
**from** **sklearn.linear_model** **import** SGDClassifier
clf = SGDClassifier(loss='log', random_state=1)
```

## åŸ¹è®­å’ŒéªŒè¯

```
X = df["tweet"].to_list()
y = df['label']
```

å¯¹äºæ¨¡å‹ï¼Œæˆ‘ç”¨äº† 80%ç”¨äºè®­ç»ƒï¼Œ20%ç”¨äºæµ‹è¯•ã€‚

```
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,
                                                 y,
                                                 test_size=0.20,
                                                 random_state=0)
```

ç„¶åï¼Œæˆ‘ç”¨æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„å“ˆå¸ŒçŸ¢é‡å™¨å°†æ–‡æœ¬æ•°æ®è½¬æ¢æˆçŸ¢é‡:

```
X_train = vect.transform(X_train)
X_test = vect.transform(X_test)
```

æœ€åï¼Œæˆ‘å°†æ•°æ®ä¸ç®—æ³•ç›¸åŒ¹é…

```
classes = np.array([0, 1])
clf.partial_fit(X_train, y_train,classes=classes)
```

è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹æµ‹è¯•æ•°æ®çš„å‡†ç¡®æ€§:

```
print('Accuracy: %.3f' % clf.score(X_test, y_test))
```

è¾“å‡º:

```
Accuracy: 0.912
```

æˆ‘æœ‰ 91%çš„å‡†ç¡®ç‡ï¼Œè¿™å¾ˆå…¬å¹³ï¼Œä¹‹åï¼Œæˆ‘ç”¨é¢„æµ‹æ›´æ–°äº†æ¨¡å‹

```
clf = clf.partial_fit(X_test, y_test)
```

# æµ‹è¯•å’Œé¢„æµ‹

æˆ‘åœ¨æ¨¡å‹ä¸­æ·»åŠ äº†æ–‡æœ¬â€œæˆ‘è¦è‡ªæ€ï¼Œæˆ‘åŒå€¦äº†æ²®ä¸§å’Œå­¤ç‹¬çš„ç”Ÿæ´»â€ã€‚

```
label = {0:'negative', 1:'positive'}
example = ["I'll kill myself am tired of living depressed and alone"]
X = vect.transform(example)
print('Prediction: %s\nProbability: %.2f%%'
      %(label[clf.predict(X)[0]],np.max(clf.predict_proba(X))*100))
```

æˆ‘å¾—åˆ°äº†è¾“å‡º:

```
Prediction: positive
Probability: 93.76%
```

è€Œå½“æˆ‘ç”¨ä¸‹é¢çš„æ–‡å­—â€œè¿™ä¹ˆçƒ­çš„å¤©ï¼Œæˆ‘æƒ³åƒå†°æ·‡æ·‹ï¼Œé€›å…¬å›­â€æ—¶ï¼Œæˆ‘å¾—åˆ°äº†ä¸‹é¢çš„é¢„æµ‹:

```
Prediction: negative
Probability: 97.91%
```

è¯¥æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹è¿™ä¸¤ç§æƒ…å†µã€‚è¿™å°±æ˜¯å¦‚ä½•å»ºç«‹ä¸€ä¸ªç®€å•çš„è‡ªæ€æ¨ç‰¹åˆ†ç±»å™¨ã€‚

ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°æˆ‘å†™è¿™ç¯‡æ–‡ç« ç”¨çš„ç¬”è®°æœ¬

æ„Ÿè°¢é˜…è¯»ğŸ˜Š