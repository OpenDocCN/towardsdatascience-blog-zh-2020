# Python 文本分析入门

> 原文：<https://towardsdatascience.com/getting-started-with-text-analysis-in-python-ca13590eb4f7?source=collection_archive---------5----------------------->

## 这是一个实用的循序渐进的教程，面向那些坚持使用 Excel 进行文本分析的数据分析师

![](img/4d0b3fd63d21146ce0669410a086ae16.png)

[来源](https://pixabay.com)

因此，显然使用 MS Excel 处理文本数据是一件好事，因为你可以安装[插件](https://text2data.com/Excel)来创建字数统计和单词云，甚至可以进行情感分析。然而，我真的不知道为什么有人会这样做，如果存在免费的和不那么笨拙的工具——比如 Python。

我在[之前的一篇文章](/how-to-get-started-with-data-analysis-and-data-science-in-python-and-r-a-pragmatic-approach-a6ff498dec61)中概述过，许多人不愿意学习编码，因为他们认为这很难，并且需要深入的数学知识。两者都不对。如果您可以在 Excel 中编写冗长而笨拙的函数，那么让我向您保证，Python 要简单得多，也更直观。此外，还有各种用于自然语言处理(NLP)的 Python 库，其中有大量的内置函数可以帮你完成繁重的工作。

在本文中，我想从 Python 中文本分析的基础开始。一些 Python 知识是必要的，所以我建议你看看我的前一篇文章，在这篇文章中，我给出了如何开始使用 Python 或 R 进行数据分析的技巧。这篇文章将是一个类似的格式。有大量关于如何开始使用 Python 学习 NLP 的教程和文章，其中一些我将在本文中链接到。但是，我想举一个实用的例子，说明如何处理您在日常工作中可能会遇到的真实文本数据。我想展示如何应用一些简单但强大的文本分析技术，以及如何解决您可能会遇到的问题。

我将从一个笔记本中给出一些代码示例，这个笔记本是我为一个想开始用 Python 进行文本分析的朋友创建的。整本笔记本在[这里](https://github.com/lisanka93/text_analysis_python_101/blob/master/Railroad_incidents_USA2019.ipynb)都有，如果你想翻一遍的话。这不是最令人兴奋的数据集，因为我试图找到一个公共数据集，类似于他在工作中必须分析的数据类型。该数据集包含 2019 年以来的美国铁路事故。

# 问正确的问题

与你在教程中分析的经过精心清理和格式化的数据集不同，真实世界的数据通常是杂乱和嘈杂的。例如，当导入美国铁路数据时，应该在一列中的事件文本不知何故出现在 15 列中。我不太清楚 MS Excel(已经很多年没用了)，但在 libre office Calc(Ubuntu 自带的开源版本)中，你必须逐行合并列。这对于 20 行已经很繁琐了，更不用说 2000 行了。当然，在 Python 中，你可以将其自动化，最好的数据操作库叫做 *pandas* (查看[这篇中型文章](/a-quick-introduction-to-the-pandas-python-library-f1b678f34673)中的介绍)。

在我看来，学习新东西的最好方法就是把手弄脏。当然，您应该花一些时间熟悉基础知识，但是没有人有时间去做所有有用的 Python 库的教程。如果你不知道如何执行某个操作(比如用*熊猫*合并栏目)——谷歌(或者百度，或者 Yandex……)是你的朋友。我可以向你保证，以前有人问过这个问题，并在 [*stackoverflow*](https://stackoverflow.com/) 上得到答案。

对于每个编程新手(或者实际上是任何东西)来说，最难的部分是当他们遇到困难时，为了找到他们正在寻找的答案，制定正确的问题。问正确的问题和谷歌搜索是一项技能，需要练习。因此，在开始的时候，你可能要花更长的时间才能在网上找到答案。但是不要担心——你很快就会掌握必要的词汇和经验。

> **问题:**本应在一列中的数据被拆分成多列
> **最终目标:**拥有一个包含所有列中数据的新列，并删除多余的列
> 
> **解决方案:**合并列
> 
> **谷歌查询:**合并多列熊猫

![](img/f72e9b0cce1553c94a88a36f1d6b7cd2.png)

谷歌搜索结果

谷歌推荐了几个 stackoverflow 帖子。主要的一个不是最理想的，因为我们有两个以上的列。但是还有一个问题是关于[几个栏目](https://stackoverflow.com/questions/33098383/merge-multiple-column-values-into-one-column-in-python-pandas)的。

这是一个非常简洁的解决方案。为什么？因为它是通用的，并且考虑到了以后可能遇到的其他问题:比如 NaN 值(空单元格)和不同列中的不同数据类型(NaN 的类型是 *float* )。我在[我的虚拟笔记本](https://github.com/lisanka93/text_analysis_python_101/blob/master/Dummy%20movie%20dataset.ipynb)中一步一步地讲解这个功能。

# 非文本预处理

鉴于我想把重点放在文本数据上，我将不讨论如何处理丢失的数据。我正在给出一些如何在[这个笔记本](https://github.com/lisanka93/DataScience_summer_school_2019/blob/master/day9/titanic-logisticreg_and_NB.ipynb)中估算缺失数值数据的例子(注意:是用 R，不是 Python)。对于铁路事故数据集，我只是检查重复的行并删除它们。我建议你用你的数据做一切必要的事情，让你的生活更轻松。如果您仅对某些列感兴趣，请仅选择这些列并将其存储在数据框中。如果您只对数据的某一部分感兴趣，请选择该部分(例如，只对发生在堪萨斯城的事件感兴趣)。

# 文本预处理

在这一节中，我想回顾一些重要的 NLP 概念，并展示如何将它们应用于文本数据的代码示例。

**停用词移除**
停用词是可能不携带任何有价值信息的词，如冠词(*、*)、连词(*、*)或命题(*、*)。你为什么要删除它们？因为发现 *"the"* 和 *"a"* 是数据集中最常见的词并不能告诉你太多关于数据的信息。然而，如果最常见的词包括*【损害】*和*【脱轨】*你已经可以对事故的类型做出某些推断(在美国铁路数据集中)。

NLP Python 库，如 [NLTK](https://www.nltk.org/) 通常带有一个内置的停用词表，您可以很容易地导入它。

然而，NLTK 停用词列表只有大约 200 个停用词。我用于文本分析的停用词表包含将近 600 个单词。我把它保存在一个文件中，在那里我删除了撇号(*不是* → *不是*)，因为我在删除停用词之前删除了标点符号并对文本进行了标记。

请注意，在某些情况下，停用词很重要。例如，识别负面评论或建议。人们会在差评中使用*【不】**【不是】*这样的停用词:“*我会* ***不是*** *再买这个商品。我看到了* ***没有*** *使用它的好处”。*我在[笔记本](https://github.com/lisanka93/text_analysis_python_101/blob/master/Railroad_incidents_USA2019.ipynb)里寻址否定。

# 转换成 n-grams

正如您已经在上面的代码片段中看到的，我们需要将文本标记为单个单词，并将它们存储在一个列表中，以便遍历列表并删除停用的单词。单个词被称为一元词，两个词的二元词，三个词的三元词。

当处理一个新的数据集时，我发现提取最常用的词来了解数据是关于什么的很有帮助。您通常希望首先提取最常见的单字，但是提取具有较大 n 的 n 个单字来识别模式也是有用的。然而，你仍然应该至少阅读一些数据点，以便发现潜在的问题。例如，在铁路事故文本中，我注意到一些报告明确指出没有发生*T21 损害或有害物质泄漏。然后，我使用二元语法和三元语法来识别否定，并区分损害和有害物质释放发生的事件和没有发生的事件。*

NLTK 有内置的二元模型、三元模型和 n 元模型函数。

# 使用正则表达式删除标点符号

标点在预测文章的意思时并不总是有用的，所以它们经常和停用词一起被删除。去掉标点符号后，剩下的只有字母数字字符。我不会在这里讨论正则表达式——互联网上有很多关于它们的教程。下面是两个正则表达式—一个删除所有标点和数字，另一个在文本中保留数字。

注意，删除标点符号是否是个好主意，完全取决于你的数据。如果您想要提取特定的信息，例如，损害的美元金额，您想要保留美元符号，以便您可以提取随后的金额。另一个例子，保持标点符号是有益的是训练一个垃圾邮件-火腿分类器。垃圾邮件或文本通常包含更多的标点符号，因此可能是一个有用的功能。

# 引理满足和词干

[词干化和词干化](/stemming-vs-lemmatization-2daddabcb221)都是指将一个单词还原到其词根的过程。区别在于**词干**可能不是一个实际的单词，而一个词条是一个实际的单词。如果你想避免将同一个单词的不同形式视为不同的单词，例如*出轨，出轨，出轨*和*出轨*，这是一个方便的工具。

> **词干:**考虑过，正在考虑，考虑→“考虑”
> 词干:考虑过，正在考虑，考虑→“考虑”

在训练分类器时，我个人从未注意到引理匹配和词干匹配之间的显著差异。不过，建议你自己试试。NLTK 带有许多不同的内置 lemmatisers 和 stemmers，所以只需即插即用。

# 把所有的放在一起

现在，您应该对 Python 中的文本分析有了足够的了解。让我们通过使用我用 5 个电影评论创建的虚拟数据集来把它们放在一起。笔记本可以在这里找到下载[。](https://github.com/lisanka93/text_analysis_python_101/blob/master/Dummy%20movie%20dataset.ipynb)

Jupyter 笔记本，演示文章中涉及的所有概念

笔记本中的预处理函数是我在博士刚开始时写的，并且在不同的项目中复制粘贴过。如果您在工作中总是处理相同类型的数据，那么很容易编写一些自动化预处理、探索和分析的函数，您可以在每次处理新数据集时复制粘贴这些函数。

我希望这篇文章对您有用，它将帮助您开始使用 Python pandas、NLTK 和任何其他库，这将使您的工作更容易。我将在下一篇文章中介绍一些更高级的 NLP 主题，所以如果您对我应该介绍的内容有任何建议，请告诉我！

[1] Lewis，David D .，et al .(2004)[RC v1:文本分类研究的新基准集。](http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf)*机器学习研究杂志*5:361–397。