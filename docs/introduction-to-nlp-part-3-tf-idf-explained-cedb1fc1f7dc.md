# NLP ä»‹ç»-ç¬¬ 3 éƒ¨åˆ†:TF-IDF è®²è§£

> åŸæ–‡ï¼š<https://towardsdatascience.com/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc?source=collection_archive---------30----------------------->

è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ï¼Œä¹Ÿç§°ä¸º tf-idfâ€¦ğŸ’¤

ä½ è§‰å¾—è¿™å¬èµ·æ¥åƒèƒ¡è¨€ä¹±è¯­å—ï¼Ÿä½†ä½ å¸Œæœ›ä¸ä¼šï¼Ÿåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†é¦–å…ˆæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ *sklearn* å°†æ–‡æœ¬æ•°æ®çŸ¢é‡åŒ–åˆ° tf-idfï¼Œç„¶åå±•ç¤ºå¦‚ä½•åœ¨æ²¡æœ‰ä»»ä½•è½¯ä»¶çš„æƒ…å†µä¸‹è‡ªå·±å®Œæˆçš„ä¸€æ­¥ä¸€æ­¥çš„è¿‡ç¨‹ã€‚å¸Œæœ› *tf-idf* åœ¨è¿™ç¯‡å¸–å­ç»“æŸåï¼Œä½ ä¼šæ›´æ¸…æ¥šï¼ğŸ“

![](img/fe46cf3d552e1c646476997b5e265e28.png)

JESHOOTS.COM åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Š[æ‹ç…§](https://unsplash.com/@jeshoots?utm_source=medium&utm_medium=referral)

# **1ã€‚å®šä¹‰ğŸ“—**

é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç†Ÿæ‚‰ä¸€äº›å®šä¹‰ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å¯¹æ¯ä¸ªæ¦‚å¿µçš„å«ä¹‰ä¿æŒä¸€è‡´:

![](img/5614013ce97cc2a26ccb5699f28e6d4c.png)

> * tf å’Œ tf-idf è¢«åˆ†æˆä¸¤ä¸ªå˜ä½“:ä¸€ä¸ªåŸºäº count (_raw)ï¼Œå¦ä¸€ä¸ªåŸºäº percentage ä»¥ä½¿äº‹æƒ…æ›´æ¸…æ¥šã€‚
> 
> * * IDF çš„ä¸€ä¸ªæ›´é€šç”¨çš„å®šä¹‰æ˜¯ä¸€ä¸ªæƒé‡ï¼Œå®ƒæé«˜ä¸å¤ªé¢‘ç¹çš„æœ¯è¯­çš„æƒé‡ï¼Œé™ä½è¾ƒé¢‘ç¹çš„æœ¯è¯­çš„æƒé‡ã€‚ç„¶è€Œï¼Œæˆ‘é€‰æ‹©äº†ä¸€ä¸ªç®€å•çš„å®šä¹‰ï¼Œå› ä¸ºæ ¹æ®æ‰€ä½¿ç”¨çš„å…¬å¼ï¼Œä¸€ä¸ªæœ¯è¯­çš„æœ€ä½æƒé‡å¯ä»¥æ˜¯ 1ã€‚æ— è®ºå“ªç§æ–¹å¼ï¼Œåœ¨ tf-idf ä¸­ï¼Œä¸ tf ç›¸æ¯”ï¼Œé¢‘ç‡è¾ƒä½çš„æœ¯è¯­æƒé‡è¾ƒé«˜ï¼Œé¢‘ç‡è¾ƒé«˜çš„æœ¯è¯­æƒé‡è¾ƒä½ã€‚
> 
> æ‹¬å·ä¸­çš„ä¾‹å­æ˜¾ç¤ºäº†æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ•°æ®é›†çš„å‚è€ƒã€‚

ä¸ºäº†ä¾¿äºç®¡ç†ï¼Œæˆ‘ä»¬å°†ä»è¿™äº›å¥å­ä¸­åˆ›å»ºä¸¤ä¸ªå°æ–‡æ¡£ï¼Œè¿™å°†å…è®¸æˆ‘ä»¬ç›‘æ§æ¯ä¸ªæ­¥éª¤çš„è¾“å…¥å’Œè¾“å‡º:

```
d1 = 'I thought, I thought of thinking of thanking you for the gift'
d2 = 'She was thinking of going to go and get you a GIFT!'
```

å¦‚æœä½ æƒ³çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘é€‰æ‹©äº†è¿™ä¸¤ä¸ªå¥å­ï¼Œæˆ‘å¿…é¡»æƒ³å‡ºä¸€ä¸ªæœ€å°çš„ä¾‹å­ï¼Œåœ¨å®ƒä»¬è¢«é¢„å¤„ç†åï¼Œè½¬æ¢æˆæˆ‘ç†æƒ³çš„æœ«ç«¯å‘é‡ï¼Œç„¶åå‘åæƒ³ã€‚ä¸€ä¸ªç»•å£ä»¤æ´¾ä¸Šäº†ç”¨åœºï¼

# 2.å¸¦ sklearn çš„ TF-IDFğŸ’»

æœ¬èŠ‚å‡è®¾æ‚¨å·²ç»è®¿é—®å¹¶ç†Ÿæ‚‰ Pythonï¼ŒåŒ…æ‹¬å®‰è£…åŒ…ã€å®šä¹‰å‡½æ•°å’Œå…¶ä»–åŸºæœ¬ä»»åŠ¡ã€‚å¦‚æœä½ æ˜¯ Python çš„æ–°æ‰‹ï¼Œ[è¿™ä¸ª](https://www.python.org/about/gettingstarted/)æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å…¥é—¨åœ°æ–¹ã€‚

## 2.0.Python è®¾ç½®ğŸ”§

æˆ‘å·²ç»ä½¿ç”¨å¹¶æµ‹è¯•äº† Python 3.7.1 ä¸­çš„è„šæœ¬ã€‚åœ¨ä½¿ç”¨ä»£ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ç¡®ä¿æ‚¨æœ‰åˆé€‚çš„å·¥å…·ã€‚

## â¬œï¸ç¡®ä¿å®‰è£…äº†æ‰€éœ€çš„è½¯ä»¶åŒ…:ç†ŠçŒ«å’Œ nltk

æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å¼ºå¤§çš„ç¬¬ä¸‰æ–¹è½¯ä»¶åŒ…:

*   *ç†ŠçŒ«*:æ•°æ®åˆ†æåº“ï¼Œ
*   *nltk:* è‡ªç„¶è¯­è¨€å·¥å…·åŒ…åº“å’Œ
*   *sklearn:* æœºå™¨å­¦ä¹ åº“ã€‚

## â¬œï¸ä» nltk ä¸‹è½½â€œåœç”¨è¯â€å’Œâ€œwordnetâ€è¯­æ–™åº“

ä¸‹é¢çš„è„šæœ¬å¯ä»¥å¸®åŠ©ä½ ä¸‹è½½è¿™äº›è¯­æ–™åº“ã€‚å¦‚æœæ‚¨å·²ç»ä¸‹è½½äº†ï¼Œè¿è¡Œå®ƒä¼šé€šçŸ¥æ‚¨å®ƒæ˜¯æœ€æ–°çš„:

```
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
```

## 2.1.å®šä¹‰æ–‡æœ¬å¤„ç†åŠŸèƒ½

é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç”¨åŒ…å’Œæ•°æ®å‡†å¤‡ç¯å¢ƒ:

```
# Import packages and modules
import pandas as pd
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer# Create a dataframe
X_train = pd.DataFrame({'text': [d1, d2]})
```

ä»ç°åœ¨å¼€å§‹ï¼Œæˆ‘ä»¬å°†æŠŠâ€˜X _ trainâ€™ä½œä¸ºæˆ‘ä»¬çš„è¯­æ–™åº“(ä¸ç®¡å®ƒæœ‰å¤šå¤§)ï¼ŒæŠŠä¸¤ä¸ªå¥å­ä½œä¸ºæ–‡æ¡£ã€‚ç¬¬äºŒï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–‡æœ¬å¤„ç†å‡½æ•°æ¥å°†å®ƒä¼ é€’ç»™ *TfidfVectorizer* :

```
def preprocess_text(text):
    # Tokenise words while ignoring punctuation
    tokeniser = RegexpTokenizer(r'\w+')
    tokens = tokeniser.tokenize(text)

    # Lowercase and lemmatise 
    lemmatiser = WordNetLemmatizer()
    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]

    # Remove stopwords
    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]
    return keywords
```

ğŸ”—å¦‚æœä½ éœ€è¦è§£é‡Šï¼Œæˆ‘å·²ç»åœ¨[ç³»åˆ—çš„ç¬¬ä¸€éƒ¨åˆ†](https://medium.com/@zluvsand/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96)ä¸­è¯¦ç»†è§£é‡Šäº†è¿™ä¸ªåŠŸèƒ½ã€‚è¿™ä¸ªé¢„å¤„ç†ç¨‹åºå°†æŠŠæ–‡æ¡£è½¬æ¢æˆ:

```
d1 = [â€˜thinkâ€™, â€˜thinkâ€™, â€˜thinkâ€™, â€˜thankâ€™, â€˜giftâ€™]
d2 = [â€˜thinkâ€™, â€˜goâ€™, â€˜goâ€™, â€˜getâ€™, â€˜giftâ€™]
```

## 2.2.ä½¿ç”¨ TfidfVectorizer å‘ tf-idf çŸ¢é‡åŒ–

æœ€åï¼Œå¯¹è¯­æ–™è¿›è¡Œé¢„å¤„ç†:

```
# Create an instance of TfidfVectorizer
vectoriser = TfidfVectorizer(analyzer=preprocess_text)# Fit to the data and transform to feature matrix
X_train = vectoriser.fit_transform(X_train['text'])# Convert sparse matrix to dataframe
X_train = pd.DataFrame.sparse.from_spmatrix(X_train)# Save mapping on which index refers to which words
col_map = {v:k for k, v in vectoriser.vocabulary_.items()}# Rename each column using the mapping
for col in X_train.columns:
    X_train.rename(columns={col: col_map[col]}, inplace=True)
X_train
```

ä¸€æ—¦è„šæœ¬è¿è¡Œï¼Œæ‚¨å°†å¾—åˆ°ä»¥ä¸‹è¾“å‡º:

![](img/4adb054820259e47405b848d052d5f86.png)

tf-idf çŸ©é˜µ

Tadaâ•æˆ‘ä»¬å·²ç»å°†è¯­æ–™åº“çŸ¢é‡åŒ–åˆ° tf-idfï¼æ•°æ®ç°åœ¨æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹å¯æ¥å—çš„æ ¼å¼ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹æœ‰æˆ–æ²¡æœ‰çœ‹ä¸è§çš„æœ¯è¯­çš„æµ‹è¯•æ–‡æ¡£æ˜¯å¦‚ä½•è½¬æ¢çš„:

```
d3 = â€œHe thinks he will go!â€
d4 = â€œThey donâ€™t know what to buy!â€# Create dataframe
X_test = pd.DataFrame({â€˜textâ€™: [d3, d4]})# Transform to feature matrix
X_test = vectoriser.transform(X_test['text'])# Convert sparse matrix to dataframe
X_test = pd.DataFrame.sparse.from_spmatrix(X_test)# Add column names to make it more readible
for col in X_test.columns:
    X_test.rename(columns={col: col_map[col]}, inplace=True)
X_test
```

å½“`preprocess_text`è¢«åº”ç”¨æ—¶ï¼Œæµ‹è¯•æ–‡æ¡£å°†è½¬æ¢æˆ:

```
d3 = [â€˜thinkâ€™, 'go'] # *vectoritiser is familiar with these terms*
d4 = [â€˜knowâ€™, â€˜buyâ€™] # *vectoritiser is not familiar with these terms*
```

![](img/191c8b97086f20a1d7ad045df826af6f.png)

tf-idf çŸ©é˜µ

è¿™æ˜¯æ‚¨åœ¨ X_test è½¬æ¢æ—¶æ‰€æœŸæœ›çœ‹åˆ°çš„å—ï¼Ÿè™½ç„¶æˆ‘ä»¬åœ¨ *d3* ä¸­å‡ºç°äº†ä¸€æ¬¡â€œgoâ€å’Œâ€œthink â€,ä½†ä½ æœ‰æ²¡æœ‰æ³¨æ„åˆ°â€œgoâ€ç›¸å¯¹äºâ€œthinkâ€çš„æƒé‡æ˜¯å¦‚ä½•å¢åŠ çš„ï¼Ÿ *d4* å–å…¨ 0 å€¼è¿™ä¸ªäº‹å®å¯¹ä½ æœ‰æ„ä¹‰å—ï¼Ÿä½ æ˜¯å¦æ³¨æ„åˆ°çŸ©é˜µä¸­çš„é¡¹æ•°å–å†³äºè®­ç»ƒæ•°æ®ï¼Œå°±åƒä»»ä½•å…¶ä»–çš„ *sklearn* å˜å½¢é‡‘åˆšä¸€æ ·ï¼Ÿ

# 3.TF-IDF -è‡ªå·±åŠ¨æ‰‹ğŸ“

æˆ‘è®¤ä¸ºå½“æˆ‘ä»¬å¼€å§‹å¯»æ‰¾å¼•æ“ç›–ä¸‹çš„ä¸œè¥¿æ—¶ï¼Œäº‹æƒ…ä¼šå˜å¾—æ›´æœ‰è¶£ã€‚åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ‰‹åŠ¨è¿›è¡Œè½¬æ¢ï¼Œè¿™ä¸æ˜¯å¾ˆæœ‰è¶£å—ï¼ŸğŸ˜

å¦‚æœä½ å–œæ¬¢æ•°å­¦ï¼Œæˆ‘é¼“åŠ±ä½ æŒ‰ç…§è¿™ä¸ªæŒ‡å—æ‰‹åŠ¨è®¡ç®—é—®é¢˜ï¼Œæˆ–è€…æ›´å¥½çš„æ˜¯ï¼Œåœ¨ç»§ç»­ä¸‹é¢çš„ç­”æ¡ˆä¹‹å‰å°è¯•è‡ªå·±è®¡ç®—ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œè¡Œä»£è¡¨æ–‡æ¡£æˆ–è¯­æ–™åº“ï¼Œåˆ—ä»£è¡¨æœ¯è¯­ã€‚

## 3.1.åŸå§‹æœ¯è¯­é¢‘ç‡è¡¨

**ğŸ”’é—®é¢˜:**æŒ‰æ–‡æ¡£ç»Ÿè®¡æ¯ä¸ªæœ¯è¯­çš„*åŸå§‹æœ¯è¯­é¢‘ç‡*ã€‚

**ğŸ’­æç¤º:**çœ‹é¢„å¤„ç†åçš„ *d1* å’Œ *d2* ä»¥åŠ *tf_raw* çš„å®šä¹‰ã€‚

**ğŸ”‘ç­”æ¡ˆ:**

![](img/d7c0f3cf2b19515da247d2d2fc65c0dd.png)

ä¸‹è¡¨æ€»ç»“äº†æ¯ä¸ªæœ¯è¯­åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚ä¾‹å¦‚:æˆ‘ä»¬åœ¨æ–‡æ¡£ 1 ä¸­çœ‹åˆ°â€œthinkâ€å‡ºç°äº† 3 æ¬¡ï¼Œä½†åœ¨æ–‡æ¡£ 2 ä¸­åªå‡ºç°äº†ä¸€æ¬¡ã€‚åˆ—çš„æ•°é‡ç”±è¯­æ–™åº“ä¸­å”¯ä¸€æœ¯è¯­çš„æ•°é‡å†³å®šã€‚è¿™ä¸€æ­¥å…¶å®å°±æ˜¯ *sklearn çš„ count vector ser*åšçš„äº‹æƒ…ã€‚

## 3.2.æœ¯è¯­é¢‘ç‡

*(æ­¤æ­¥éª¤ä»…ç”¨äºæ¯”è¾ƒæœ€ç»ˆè¾“å‡ºã€‚)*

**ğŸ”’é—®é¢˜:**æŒ‰æ–‡æ¡£è®¡ç®—æ¯ä¸ªè¯çš„*è¯é¢‘*ã€‚

**ğŸ’­æç¤º:**ä¸€ä¸ªæœ¯è¯­åœ¨æ–‡æ¡£ä¸­å å¤šå¤§æ¯”ä¾‹ï¼Ÿ(ç¬¬%è¡Œ)

**ğŸ”‘ç­”æ¡ˆ:**

![](img/c0fbf3ddfa9938c260906c47cbc0d369.png)

æœ¯è¯­â€œæ€è€ƒâ€å äº†æ–‡çŒ® 1 ä¸­ 60%çš„æœ¯è¯­ã€‚

## 3.3.æ–‡æ¡£é¢‘ç‡

**ğŸ”’é—®é¢˜:**ç»Ÿè®¡*æ¯æœŸçš„æ–‡æ¡£é¢‘ç‡*ã€‚

**ğŸ’­æç¤º:**æœ‰å¤šå°‘ä¸ªæ–‡æ¡£åŒ…å«ç‰¹å®šçš„æœ¯è¯­ï¼Ÿ

**ğŸ”‘å›ç­”:**

![](img/5d9b5ce4a82e6c29e5a2e2710738dde9.png)

æœ¯è¯­â€œgetâ€ä»…åœ¨æ–‡æ¡£ 1 ä¸­å‡ºç°ï¼Œè€Œæœ¯è¯­â€œthinkâ€åœ¨ä¸¤ä¸ªæ–‡æ¡£ä¸­éƒ½å‡ºç°ã€‚

## 3.4.é€†æ–‡æ¡£é¢‘ç‡

è¿™æ˜¯ç¬¬ä¸€æ¬¡è®¡ç®—çš„åœ°æ–¹ã€‚
**â—å…¬å¼:**åœ¨è¯¥å…¬å¼ä¸­ï¼Œ *n* ä»£è¡¨æ–‡æ¡£æ•°ã€‚

![](img/a6ff4576eeb5db28363291c809cf12a9.png)

**ğŸ”’é—®é¢˜:**è®¡ç®—æ¯é¡¹çš„ *idf* ã€‚

**ğŸ”‘ç­”:***IDF*çœ‹èµ·æ¥æ˜¯ä¸æ˜¯å’Œ *df* æœ‰ç‚¹åï¼Ÿ

![](img/961035054e29313371a00243a88c2fcf.png)

åœ¨ *sklearn* ä¸­ï¼Œå¯ä»¥ä»æ‹Ÿåˆçš„*tfidf çŸ¢é‡å™¨*ä¸­è®¿é—® *idf_* å±æ€§ã€‚

ğŸ”**ä¾‹é¢˜è®¡ç®—:**ä¸‹é¢ï¼Œæˆ‘å·²ç»æä¾›äº†ä¸€ä¸ªä¾‹é¢˜è®¡ç®—ã€‚é€šè¿‡å¤åˆ¶ç›¸åŒçš„é€»è¾‘ï¼Œæ‚¨å¯ä»¥å°†å®ƒä½œä¸ºå…¶ä½™æœ¯è¯­çš„æŒ‡å—:

![](img/74bf3fbf0abd58d7f5347386da87fb37.png)

## 3.5.åŸå§‹æœ¯è¯­é¢‘ç‡ä¸æ–‡æ¡£é¢‘ç‡ç›¸å

**â—å…¬å¼:**

![](img/81ec23c2f7e5e840ecb3938790a6278f.png)

**ğŸ”’é—®é¢˜:**æŒ‰æ–‡æ¡£è®¡ç®—æ¯ä¸ªæœ¯è¯­çš„ *raw tf-idf* (å³åŠ æƒè®¡æ•°)ã€‚

**ğŸ”‘ç­”æ¡ˆ:**

![](img/ceeb70faf5df678a9bc6b2eecff6c3d3.png)

**ğŸ”ç¤ºä¾‹è®¡ç®—:**

![](img/7f7bfc9a1a71c698da7fa0f0c31221f1.png)

## 3.6.æœ¯è¯­é¢‘ç‡ä¸æ–‡æ¡£é¢‘ç‡æˆåæ¯”

**â—å…¬å¼:**

![](img/2fb582152aa934399e43bf72534dc9d9.png)

**ğŸ”’é—®é¢˜:**é€šè¿‡æ–‡æ¡£è®¡ç®—æ¯é¡¹çš„ *tf-idf* ã€‚

**ğŸ”‘ç­”æ¡ˆ:**

![](img/85e32de54782a1b64dd4a5adb095e43f.png)

**ğŸ”** **ç¤ºä¾‹è®¡ç®—:**

![](img/48c2f17d7be114f6f7f14fa7ff83324b.png)![](img/1aa960a1d06dd9e0b022bcac896400b6.png)

Yayâ•æˆ‘ä»¬å·²ç»è·å¾—äº†å®Œå…¨ç›¸åŒçš„ç»“æœï¼å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœæ‚¨åœ¨ä¸­é—´æ­¥éª¤çš„ä»»ä½•åœ°æ–¹è¿›è¡Œäº†èˆå…¥ï¼Œç”±äºèˆå…¥è¯¯å·®ï¼Œæ‚¨çš„æœ€ç»ˆç­”æ¡ˆå¯èƒ½ä¸ä¸Šé¢æä¾›çš„ç­”æ¡ˆä¸å®Œå…¨åŒ¹é…ã€‚

**ğŸ“Œ** **ç»ƒä¹ :**çœ‹çœ‹èƒ½ä¸èƒ½è®¡ç®—å‡º *d3* å’Œ *d4* çš„ *tf-idf* ï¼Œå¹¶ä¸ä¸Šä¸€èŠ‚ *sklearn* çš„è¾“å‡ºç›¸åŒ¹é…ã€‚

**ğŸ’­** **æç¤º:** (1) Count tf_raw - terms æŒ‡çš„æ˜¯æ¥è‡ªè®­ç»ƒæ•°æ®çš„æœ¯è¯­ï¼Œ(2)ä½¿ç”¨æˆ‘ä»¬æ„å»ºçš„ idf è®¡ç®— tf-idf_rawï¼Œ(3)è®¡ç®— tf-idfã€‚ä»…å¯¹åŸ¹è®­ä¸­çš„æœ¯è¯­æ‰§è¡Œè¿™äº›æ­¥éª¤ã€‚

å½“ *sklearn* ä¸­çš„*tfidf vector*æˆ– *TfidfTransformer* çš„ *smooth_idf=True* æ—¶ï¼Œè¯¥æ–¹æ³•å¤åˆ¶è¾“å‡ºã€‚å¦‚æœæ‚¨å°†è¯¥å‚æ•°æ›´æ”¹ä¸º *False* ï¼Œæ‚¨å°†ä¸å¾—ä¸é€šè¿‡ä»åˆ†å­å’Œåˆ†æ¯ä¸­å–å‡º+1 æ¥ç¨å¾®è°ƒæ•´ *idf* å…¬å¼ã€‚

åœ¨æˆ‘ä»¬æ€»ç»“ä¹‹å‰ï¼Œè®©æˆ‘ä»¬æ¯”è¾ƒä¸€ä¸‹æ–‡æ¡£ 1 çš„ *tf* ä¸ *tf-idf* :

![](img/f5175d80ec1913d05cd012f76007e196.png)

å› ä¸ºâ€œgiftâ€å’Œâ€œthinkâ€å‡ºç°åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­ï¼Œæ‰€ä»¥å®ƒä»¬åœ¨ä¸¤ä¸ªè¡¨ä¸­çš„ç›¸å¯¹æƒé‡æ˜¯ç›¸åŒçš„ã€‚ç„¶è€Œï¼Œâ€œæ„Ÿè°¢â€åªå‡ºç°åœ¨æ–‡æ¡£ 1 ä¸­ï¼Œå› æ­¤ä¸â€œç¤¼ç‰©â€æˆ–â€œæ€è€ƒâ€ç›¸æ¯”ï¼Œå®ƒåœ¨ tf-idf ä¸­çš„ç›¸å¯¹é¢‘ç‡æ›´é«˜ã€‚å› æ­¤ï¼Œè¿™å±•ç¤ºäº† *tf-idf* å¦‚ä½•æé«˜è¾ƒå°‘æ–‡æ¡£ä¸­çš„æœ¯è¯­çš„æƒé‡ï¼Œé™ä½è¾ƒå¤šæ–‡æ¡£ä¸­çš„æœ¯è¯­çš„æƒé‡ã€‚

**ğŸ“Œ** **ç»ƒä¹ :**è‡ªå·±åˆ†ææ–‡çŒ® 2ã€‚

æ˜¾ç„¶ï¼Œæ‰‹åŠ¨è®¡ç®—æ›´å®¹æ˜“å‡ºé”™ï¼Œå¹¶ä¸”ä¸å¤ªå¯èƒ½é€‚ç”¨äºå…·æœ‰æ•°ç™¾ã€æ•°åƒç”šè‡³æ•°ç™¾ä¸‡æ–‡æ¡£çš„çœŸå®è¯­æ–™åº“ã€‚éå¸¸æ„Ÿè°¢ *sklearn* è´¡çŒ®è€…æä¾›äº†è¿™æ ·ä¸€ç§æœ‰æ•ˆçš„æ–¹å¼ï¼Œç”¨çŸ­çŸ­å‡ è¡Œä»£ç å°†æ–‡æœ¬è½¬æ¢æˆ *tf-idf* ã€‚æ­¤å¤„æ˜¾ç¤ºçš„æ‰‹åŠ¨è®¡ç®—ä»…ç”¨äºä¸¾ä¾‹è¯´æ˜ä½¿ç”¨è½¯ä»¶æ—¶çš„åº•å±‚å®ç°ã€‚å¦‚æœæ‚¨æƒ³è‡ªå·±ä»å·¥å…·è€Œä¸æ˜¯ä» *sklearn* å¤åˆ¶è¾“å‡ºï¼Œå¯èƒ½éœ€è¦å¯¹å…¬å¼è¿›è¡Œè°ƒæ•´ï¼Œä½†æ€»ä½“æ€è·¯åº”è¯¥æ˜¯ç›¸ä¼¼çš„ã€‚æˆ‘å¸Œæœ›è¿™äº›è§£é‡Šæ˜¯æœ‰ç”¨å’Œæœ‰è§åœ°çš„ã€‚

![](img/f5a48ac1b6a85495bcf202352506d41f.png)

[Leone Venter](https://unsplash.com/@fempreneurstyledstock?utm_source=medium&utm_medium=referral) åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹ç…§

*æ‚¨æƒ³è¦è®¿é—®æ›´å¤šè¿™æ ·çš„å†…å®¹å—ï¼Ÿåª’ä½“ä¼šå‘˜å¯ä»¥æ— é™åˆ¶åœ°è®¿é—®åª’ä½“ä¸Šçš„ä»»ä½•æ–‡ç« ã€‚å¦‚æœæ‚¨ä½¿ç”¨* [*æˆ‘çš„æ¨èé“¾æ¥*](https://zluvsand.medium.com/membership)*æˆä¸ºä¼šå‘˜ï¼Œæ‚¨çš„ä¸€éƒ¨åˆ†ä¼šè´¹å°†ç›´æ¥ç”¨äºæ”¯æŒæˆ‘ã€‚*

æ„Ÿè°¢æ‚¨èŠ±æ—¶é—´é˜…è¯»è¿™ç¯‡æ–‡ç« ã€‚æˆ‘å¸Œæœ›ä½ ä»é˜…è¯»å®ƒä¸­å­¦åˆ°ä¸€äº›ä¸œè¥¿ã€‚å…¶ä½™å¸–å­çš„é“¾æ¥æ•´ç†å¦‚ä¸‹:
â—¼ï¸ [ç¬¬ä¸€éƒ¨åˆ†:Python ä¸­çš„æ–‡æœ¬é¢„å¤„ç†](https://medium.com/@zluvsand/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96)
â—¼ï¸ [ç¬¬äºŒéƒ¨åˆ†:è¯æ³•åˆ†æå’Œè¯å¹²åˆ†æçš„åŒºåˆ«](https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc)
â—¼ï¸ **ç¬¬ä¸‰éƒ¨åˆ†:TF-IDF è§£é‡Š**
â—¼ï¸ [ç¬¬å››éƒ¨åˆ†:Python ä¸­çš„æœ‰ç›‘ç£æ–‡æœ¬åˆ†ç±»æ¨¡å‹](https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267)
â—¼ï¸ [ç¬¬äº”éƒ¨åˆ†:Python ä¸­çš„æ— ç›‘ç£ä¸»é¢˜æ¨¡å‹(sklearn)](/introduction-to-nlp-part-5a-unsupervised-topic-model-in-python-733f76b3dc2d)
â—¼ï¸ [ç¬¬äº”éƒ¨åˆ†](/introduction-to-nlp-part-5b-unsupervised-topic-model-in-python-ab04c186f295)

å¿«ä¹å˜èº«ï¼å†è§ğŸƒğŸ’¨

# 4.å‚è€ƒğŸ“

*   [ä¼¯å¾·ã€å²è’‚æ–‡ã€çˆ±å¾·åÂ·æ´›ç€å’Œä¼Šä¸‡Â·å…‹è±æ©ï¼Œ*ç”¨ Python è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†*ã€‚å¥¥è±åˆ©åª’ä½“å…¬å¸ï¼Œ2009 å¹´](http://www.nltk.org/book/)
*   [*ç‰¹å¾æå–*ï¼Œsklearn æ–‡æ¡£](https://scikit-learn.org/stable/modules/feature_extraction.html)