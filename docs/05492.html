<html>
<head>
<title>Weight Decay == L2 Regularization?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">权重衰减== L2正则化？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd?source=collection_archive---------2-----------------------#2020-05-09">https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd?source=collection_archive---------2-----------------------#2020-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="3838" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">神经网络是很好的函数逼近器和特征提取器，但有时它们的权重变得过于专业，导致过度拟合。这就是正则化概念出现的地方，我们将讨论这两种被误认为相同的主要权重正则化技术之间的细微差别。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/dad96adc117582dad71d33d67170462c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EpvYBgEcFRm8O_5x9RZWWw.jpeg"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><a class="ae lf" href="https://unsplash.com/photos/P8lU5CgYybM" rel="noopener ugc nofollow" target="_blank">(来源</a>)</p></figure><h1 id="fc37" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">简介:</h1><p id="d510" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">神经网络由沃伦麦卡洛克和沃尔特皮茨于1943年首次推出，但当时还不够流行，因为它们需要大量的数据和计算能力，这在当时是不可行的。但是，随着上述约束以及其他训练进步(如参数初始化和更好的激活功能)变得可行，它们再次开始主导各种比赛，并在各种人类辅助技术中找到应用。<br/>今天，神经网络形成了许多著名应用的主干，如自动驾驶汽车、谷歌翻译、面部识别系统等，并应用于进化人类使用的几乎所有技术中。</p><p id="bece" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">神经网络非常擅长逼近线性或非线性函数，在从输入数据中提取特征时也非常出色。这种能力使他们能够在大范围的任务中创造奇迹，无论是计算机视觉领域还是语言建模。但是我们都听过这句名言:<br/> <em class="ko">“权力越大，责任越大”。<br/> </em>这句话同样适用于无所不能的神经网络。他们作为强大的函数逼近器的能力有时会导致他们通过逼近一个函数来过度拟合数据集，该函数在它被训练的数据上表现得非常好，但在它从未见过的数据上测试时却悲惨地失败了。更专业地说，神经网络学习更专门针对给定数据的权重，而不学习可以概括的特征。<br/>为了解决过度拟合的问题，应用了一种称为正则化的技术来降低模型和约束权重的复杂性，其方式是迫使神经网络学习可概括的特征。</p><h1 id="5bfd" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">正规化:</h1><p id="2ca1" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">正则化可以被定义为我们对训练算法进行的任何改变，以便减少泛化误差，而不是训练误差。正则化策略有很多。一些对模型施加额外的约束，例如对参数值添加约束，而一些对目标函数添加额外的项，这可以被认为是对参数值添加间接或软约束。如果我们小心使用这些技术，这可以提高测试集的性能。在深度学习的背景下，大多数正则化技术都基于正则化估计器。当正则化一个估计量时，有一个权衡，我们必须选择一个增加偏差和减少方差的模型。一个有效的规则化是一个有利可图的交易，显著减少方差，同时不过度增加偏差。</p><p id="d024" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">实践中使用的主要正则化技术有:</p><ol class=""><li id="0f22" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">L2正则化</li><li id="c37b" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">L1正则化</li><li id="bc27" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">数据扩充</li><li id="4dfa" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">拒绝传统社会的人</li><li id="e8ee" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">提前停止</li></ol><p id="232b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇文章中，我们主要关注L2正则化，并讨论我们是否可以将L2正则化和权重衰减视为一枚硬币的两面。</p><h2 id="d353" class="mx lh it bd li my mz dn lm na nb dp lq kb nc nd lu kf ne nf ly kj ng nh mc ni bi translated">L2正规化:</h2><p id="ba52" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">L2正则化属于被称为参数范数罚的正则化技术类别。之所以这样说，是因为在这类技术中，特定参数(主要是权重)的范数被添加到被优化的目标函数中。在L2规范中，通常称为<strong class="js iu">正则化项</strong>的额外项被添加到网络的成本函数中。<br/>例如:</p><p id="284e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们考虑交叉熵成本函数，其定义如下。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/a62c0ac6f208adddbda149e1fdabaf38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*OX5MdShek00nPSS12ID6Cw.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图一。</strong>交叉熵损失函数</p></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/9421fd17d7d717489eb153c1b2af3d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*zioOmN56xnwMvurKv5tMzA.png"/></div></figure><p id="5159" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了将L2正则化应用于任何有交叉熵损失的网络，我们将正则化项添加到成本函数中，正则化项如图2<strong class="js iu">所示。</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/fdf0a0f2dd03e297ee2706b063435bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*zQfGEhkSUaqFkcN1q4BH-A.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图二。</strong> L2范数或欧几里德范数</p></figure><p id="0cfb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<strong class="js iu">图2 </strong>中，λ是正则化参数，与应用的正则化量成正比。如果λ =0，则不应用正则化，当λ为1时，将最大正则化应用于网络。<br/> λ是一个超参数，这意味着它不是在训练期间学习的，而是由用户手动调整或使用一些超参数调整技术，如随机搜索。</p><p id="387b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们把这些放在一起，形成应用于图3给出的交叉熵损失函数的L2正则化的最终方程。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5945dfd4ead844852b59ad6e1c91c6ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*eSVD1BDcpYR9_JqkMoCoxg.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图3。</strong>最终L2正则化成本函数</p></figure><p id="2951" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的例子示出了应用于交叉熵损失函数的L2正则化，但是这个概念可以推广到所有可用的成本函数。下面在图4 中给出了L2正则化的更一般的公式，其中Co是未正则化的成本函数，C是添加了正则化项的正则化成本函数。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi no"><img src="../Images/315ecbbb802943e97f408471f6a0c164.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*HKc9-xYg1564Pa-P37j7vA.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图四。</strong>任意代价函数的L2正则化的一般形式</p></figure><p id="cb8e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">注意</strong> : <em class="ko">我们在正则化网络时不考虑网络的偏差，原因如下:</em> <br/> 1 .与权重相比，偏差通常需要较少数据来精确拟合。每个权重规定了两个变量(w和x)如何相互作用，因此很好地拟合权重需要在各种条件下观察两个变量，而每个偏差仅控制一个变量(b)。因此，我们不引入太多的偏差，让偏差不规范。<br/> 2。调整偏差会引入大量的不匹配。</p><h2 id="fb4f" class="mx lh it bd li my mz dn lm na nb dp lq kb nc nd lu kf ne nf ly kj ng nh mc ni bi translated">为什么L2正规化有效？？</h2><p id="7e25" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated"><strong class="js iu">实际原因:</strong></p><p id="afa5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们试着理解基于成本函数梯度的L2正则化的工作原理。<br/>如果我们采用<strong class="js iu">图4 </strong>中所示方程的偏导数或梯度，即网络中所有权重和偏差的∂C/∂w和∂C/∂b。<br/>取偏导数我们得到:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi np"><img src="../Images/923a134aa5a6b4f24ed5a58a80a3da0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*kzhTyXJsbBXXhuSR-k7Wvw.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图5。</strong>成本函数关于权重和偏差的梯度。</p></figure><p id="e834" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以使用反向传播算法计算上述方程中提到的∂C0/∂w和∂C0/∂b项。<br/>偏差参数的偏导数将保持不变，因为没有正则化项应用于它，而权重参数将包含额外的((λ/n)*w)正则化项。</p><p id="c0d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">偏差和权重的学习规则因此变成:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/9ebbba6260007fb6822b1352eae7d232.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*GvClCSnUtbztMO9HAAbRzw.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图6。</strong>偏差参数的梯度下降学习规则</p></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/bbf4fbb194517dee08d53a2ede2f6eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*Pd8lkZqhuENgxcIXUfkKiQ.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图七。</strong>权重参数的梯度下降学习规则</p></figure><p id="5425" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的权重公式类似于通常的梯度下降学习规则，除了现在我们首先通过<strong class="js iu">(</strong>1(η*λ)/n)重新调整权重<strong class="js iu"> w </strong>。这一项是L2正则化经常被称为<strong class="js iu">权重衰减</strong>的原因，因为它使权重变小。因此，你可以看到为什么正则化工作，它使网络的权重更小。权重的小意味着，如果我们在这里或那里改变一些随机输入，网络行为不会改变太多，这反过来使得正则化网络难以学习数据中的局部噪声。这迫使网络只学习那些在训练集中经常看到的特征。</p><p id="33b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">个人直觉:<br/> </strong>从优化成本函数的角度来简单考虑L2正则化，当我们将正则项添加到成本函数中时，我们实际上增加了成本函数的值。因此，如果权重较大，也会导致成本上升，训练算法会通过惩罚权重来降低权重，迫使权重取较小的值，从而调整网络。</p><h2 id="8b6c" class="mx lh it bd li my mz dn lm na nb dp lq kb nc nd lu kf ne nf ly kj ng nh mc ni bi translated">L2正则化和权重衰减是一回事吗？</h2><p id="d23f" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">没有L2正则化和权重衰减是不同的东西，但可以通过基于学习速率的权重衰减因子的重新参数化来使SGD等效。迷茫？我来给你详细解释一下。</p><p id="8528" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">重量衰减方程如下所示，λ为衰减系数。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/862466d57584f7a0cb599752a3544e78.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*JngjOItGKmMLTjVwl0gMFA.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图8 </strong>:神经网络中的权重衰减</p></figure><p id="c62f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在SGD的情况下，L2正则化可以在以下证明中被证明为等价于权重衰减:</p><ol class=""><li id="4bed" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">让我们首先考虑下面图9 中给出的L2正则化方程。我们的目标是对其进行重新参数化，使其等效于图8<strong class="js iu">中给出的重量衰减方程。</strong></li></ol><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ee8687b2fa614b0e7b967c89a7b0d5a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*4ECVuRUD0-z8rN9VzXVPlw.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图九。</strong>神经网络中的L2正则化</p></figure><p id="95a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.首先，我们找到L2正则化成本函数相对于参数<strong class="js iu"> w </strong>的偏导数(梯度),如图<strong class="js iu">图10 </strong>所示。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/0e82148b8214df9a31bb87ec804d1005.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*1utF-1Ys8Nf7j_fxX1D6Ng.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图十</strong>。损失函数C关于w的偏导数</p></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/74e636cc5b652f813663e733737a8c02.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*23dApDYymZGuVEjB9gIVDw.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">注:</strong>图中两个符号的意思相同。</p></figure><p id="1519" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.在我们获得成本函数的偏导数的结果后(<strong class="js iu">图10 </strong>)，我们将该结果代入<strong class="js iu">图11 </strong>所示的梯度下降学习规则中。代入后，我们打开括号并重新排列术语，使其在某些假设下等同于重量衰减方程(<strong class="js iu">图8 </strong>)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/aeecd7e7ca052616217c6c49e352f823.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*Rv-fgKgn9pjSk7U25ym_ig.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图11 </strong>。替换梯度下降规则中成本函数的梯度并重新排列术语。</p></figure><p id="d0ab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.正如你所注意到的，最终重新排列的L2正则化方程(<strong class="js iu">图11 </strong>)和权重衰减方程(<strong class="js iu">图8 </strong>)之间的唯一区别是α(学习速率)乘以λ(正则化项)。为了得到两个方程，我们通过用λ′/α代替λ <br/>来重新参数化L2正则化方程，如图<strong class="js iu">图12 </strong>所示。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/0570cd218f2f457db703fb0fae9bb0bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/format:webp/1*6_zfpDvA8MBU9_jJyOsn_A.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图12。</strong>L2正则化与权重衰减等价的条件</p></figure><p id="6154" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">5.在用λ代替λ′后，L2正则化方程被重新参数化，现在等价于重量衰减方程(<strong class="js iu">图8 </strong>)，如图<strong class="js iu">图13 </strong>所示。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/332c9092f5abcd3accdc95479768a23d.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*6TTjOVZ_A06i2k1IqDI73w.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd nk">图十三。</strong>重新参数化的L2正则化方程</p></figure><p id="7477" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从上面的证明中，你一定已经理解了为什么在SGD的情况下，L2正则化被认为等同于权重衰减，但是对于其他优化算法，如Adam、AdaGrad等，情况并非如此，它们是基于自适应梯度的。特别地，当与自适应梯度结合时，L2正则化导致具有大历史参数和/或梯度幅度的权重被正则化得比使用权重衰减时更少。与SGD相比，当使用L2正则化时，这导致adam表现不佳。另一方面，体重下降在SGD和Adam上的表现是一样的。</p><p id="20dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">令人震惊的结果是，具有动量的SGD优于Adam等自适应梯度方法，因为常见的深度学习库实现了L2正则化，而不是原始的权重衰减。因此，在使用L2正则化对SGD有利的任务上，Adam导致比带动量的SGD差的结果。</p><h1 id="7a33" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">结论</strong></h1><p id="5104" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">因此，我们得出结论，尽管权重衰减和L2正则化在某些条件下可能达到等价，但仍然是稍微不同的概念，并且应该被不同地对待，否则会导致无法解释的性能下降或其他实际问题。</p><p id="3876" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我希望你喜欢这篇文章，并学到一些新的东西。如果您有任何疑问或想进一步讨论，请随时通过<a class="ae lf" href="https://twitter.com/Perceptron97" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或<a class="ae lf" href="https://www.linkedin.com/in/divyanshu-mishra-ai/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>与我联系。</p><h2 id="40df" class="mx lh it bd li my mz dn lm na nb dp lq kb nc nd lu kf ne nf ly kj ng nh mc ni bi translated"><strong class="ak">进一步阅读<em class="nz"> g: </em> </strong></h2><ol class=""><li id="9123" class="mj mk it js b jt me jx mf kb oa kf ob kj oc kn mo mp mq mr bi translated"><a class="ae lf" href="https://arxiv.org/abs/1711.05101" rel="noopener ugc nofollow" target="_blank"> <em class="ko">解耦权重衰减正则化</em> </a></li></ol><h1 id="98fe" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">参考资料:</h1><ol class=""><li id="17d4" class="mj mk it js b jt me jx mf kb oa kf ob kj oc kn mo mp mq mr bi translated"><a class="ae lf" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习</a>。</li><li id="9c5b" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">伊恩·古德菲勒、约舒阿·本吉奥和亚伦·库维尔的《深度学习》。</li></ol><p id="3f30" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.<a class="ae lf" href="https://jamesmccaffrey.wordpress.com/2019/05/09/the-difference-between-neural-network-l2-regularization-and-weight-decay/" rel="noopener ugc nofollow" target="_blank">神经网络L2正则化和权重衰减的区别</a></p></div></div>    
</body>
</html>