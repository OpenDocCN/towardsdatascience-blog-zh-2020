<html>
<head>
<title>How your model is optimized | Know your Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何优化您的模型|了解您的优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-your-model-is-optimized-know-your-optimization-d2809c7a912c?source=collection_archive---------69-----------------------#2020-06-15">https://towardsdatascience.com/how-your-model-is-optimized-know-your-optimization-d2809c7a912c?source=collection_archive---------69-----------------------#2020-06-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="04f1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">对“为什么我的模型永远在运行？”还是那句经典的:“我觉得它可能已经收敛了？”</em></h2></div><p id="ed24" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">普通数据科学家或 ML 工程师日常使用的许多模型背后的驱动因素依赖于数值优化方法。研究不同功能的优化和性能有助于更好地理解流程是如何工作的。<br/>我们每天面临的挑战是，有人给了我们一个他们如何看待世界或他们的问题的模型。现在，<strong class="ki ir">你</strong>作为一名数据科学家<strong class="ki ir">必须找到问题的最优解</strong>。例如，你看着一个能量函数，想找到一个绝对的全局最小值，使你的工具工作或你的蛋白质稳定。也许你已经建立了用户数据模型，你想找到一个给定所有输入特征的理想客户——希望是连续的输入特征。这里全面总结了优化器是如何工作的。</p><h1 id="dbc5" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">基准函数——曲线函数、弯曲函数和非对称函数</h1><p id="395b" class="pw-post-body-paragraph kg kh iq ki b kj lu jr kl km lv ju ko kp lw kr ks kt lx kv kw kx ly kz la lb ij bi translated">在我们遇到我们的优化器之前，我们必须熟悉我们想要优化的函数。我个人最喜欢的三个是:</p><ul class=""><li id="3b58" class="lz ma iq ki b kj kk km kn kp mb kt mc kx md lb me mf mg mh bi translated">好好先生就是<strong class="ki ir">椭球函数</strong>。它总体上表现良好，是凸的，并且有一个全局最小值。<br/>它的定义是:</li></ul><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/338df2aa435c9db9381416c4826b06b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*X0nl035vl7h2GZIbQsjAMA.png"/></div></figure><ul class=""><li id="766f" class="lz ma iq ki b kj kk km kn kp mb kt mc kx md lb me mf mg mh bi translated">Rosenbrock 或<strong class="ki ir">香蕉功能</strong>。它不像我们的椭球那样好，因为它的死亡之谷对优化算法来说是一个挑战。它被定义为:</li></ul><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/0ac40dcee09867dd58e4a928c456d0a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*5D9AaRAjDWXoKGKsyYOzvw.png"/></div></figure><ul class=""><li id="066a" class="lz ma iq ki b kj kk km kn kp mb kt mc kx md lb me mf mg mh bi translated">扭曲的是我们的吸引扇形函数。这是最不寻常的选择，因为它是非凸的，并且在最小值周围不对称。它被定义为</li></ul><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b98b694459a06d22c79c60fb2f121aad.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*nweaWHBNhQzoWQUeAaYymg.png"/></div></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/b42f7a9acb3d911b2bd9ec3790e53bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*0gg_Li8bLgv2uAsqUPkaow.png"/></div></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/110b3541e2a5492e5c0ba39b4ea5efd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Kk1eIjsivmKZnuXP_Yzs7A.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">图 1a)R 中的椭球函数。较低的函数值用蓝色表示，全局最小值在(0，0)。</p></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/5e05adbc70123ef01d90c6da547b51c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*fPmshyLIEmPaH2r92HgSBQ.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">图 1b)R 中的 Rosenbrock 函数。较低的函数值用蓝色表示，全局最小值在(1，1)处，位于谷中。</p></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/9ecdd97f61b199590f45fc82d7f17bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*HerFo9vqXT5BEZIyP2s1Lw.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">图 1c)R 中的吸引扇区函数。较低的函数值用蓝色表示，全局最小值在(0，0)附近。</p></figure><p id="52de" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这是一个易于理解的函数选择，并且存在大量可用的基准函数来运行您的优化器。从<a class="ae my" href="https://commons.wikimedia.org/wiki/File:Styblinski-Tang_function.pdf" rel="noopener ugc nofollow" target="_blank"> Styblinski-Tang </a>功能的深度到<a class="ae my" href="https://en.wikipedia.org/wiki/Rastrigin_function" rel="noopener ugc nofollow" target="_blank"> Rastrigin </a>的美丽窥视。其中一些是现成的，如[1]中所示。</p><pre class="mj mk ml mm gt mz na nb nc aw nd bi"><span id="c22e" class="ne ld iq na b gy nf ng l nh ni">As you might have noticed the functions work on R^n and we provide an input vector <strong class="na ir">x</strong> ∈ R^n. We will only be looking at two dimensions R^2 and three dimensions R^3 to make the display easier.</span></pre><p id="e088" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">现在我们已经建立了一个平台，我们可以加载我们选择的工具，比如带有<a class="ae my" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html" rel="noopener ugc nofollow" target="_blank">优化库</a>的<a class="ae my" href="https://docs.scipy.org/doc/scipy/reference/index.html" rel="noopener ugc nofollow" target="_blank"> scipy </a>。我们的目标是捕捉优化器如何在给定不同起点的基准函数上找到最小值。我们只是挑选任何优化程序，如简单的梯度下降“CG”或更复杂的 BFGS。事情是这样的:</p><pre class="mj mk ml mm gt mz na nb nc aw nd bi"><span id="5834" class="ne ld iq na b gy nf ng l nh ni">from scipy.optimize import minimize</span><span id="88a0" class="ne ld iq na b gy nj ng l nh ni"># import benchmark function with its derivatives<br/>from scipy.optimize import rosen<br/>from scipy.optimize import rosen_der<br/>from scipy.optimize import rosen_hess</span><span id="24b4" class="ne ld iq na b gy nj ng l nh ni">import numpy as np</span><span id="ef29" class="ne ld iq na b gy nj ng l nh ni">def banana(x, y):<br/>“”” <br/>a nice 2D way to look at Rosenbrock <br/>“””<br/> return (1 — x)**2 + (100 * (y — (x**2))**2)</span><span id="618c" class="ne ld iq na b gy nj ng l nh ni"># we select some interesting optimizers:<br/>OPTIMIZERS = [‘CG’, ‘BFGS’, ‘Newton-CG’, ‘dogleg’]<br/>FUNCTIONS = {‘rosen’: [rosen, rosen_der, rosen_hess, banana]<br/># feel free to add or implement ellipsoid or other benchmark functions here with their derivatives<br/>}</span><span id="636d" class="ne ld iq na b gy nj ng l nh ni">start_values = np.random.uniform(low=x_min, high=x_max, size=(10, 2))</span><span id="7448" class="ne ld iq na b gy nj ng l nh ni">start_iter = 1<br/>max_iter = 50<br/>step = 1<br/>x_min, x_max = -5, 5<br/>y_min, y_max = -5, 5</span><span id="cb1e" class="ne ld iq na b gy nj ng l nh ni">def optimize_funct(fname, funct, x0, derivative, hess, optimizer, iteration):<br/>“””<br/>Just a wrapper around the scipy minimize function<br/>“””<br/> fitted_min = minimize(funct, x0=x0, method=optimizer, <br/> jac=derivative, hess=hess,<br/> options={‘maxiter’:iteration})<br/> return fitted_min</span><span id="bddf" class="ne ld iq na b gy nj ng l nh ni"># run actual optimization<br/>fitted_optimizers = {}<br/>for opt in OPTIMIZERS:<br/> fitted_optimizers[opt] = {}<br/> for name, functs in FUNCTIONS.items():<br/> fitted_optimizers[opt][name] = []<br/> f = functs[0]<br/> fd = functs[1]<br/> fdd = functs[2]<br/> f_2d = functs[3]<br/> for vals in start_values:<br/> computed_values = []<br/> x, y = vals<br/> z = f_2d(x,y)<br/> # include start values before optimization<br/> computed_values.append(np.array([x,y,z]))<br/> for i in range(start_iter, max_iter, step):<br/> out = optimize_funct(fname=name, funct=f, x0=vals, derivative=fd, hess=fdd,<br/> optimizer=opt, iteration=i)<br/> # only save the output values (stored under x)<br/> x, y = out.x<br/> z = f_2d(x, y)<br/> computed_values.append(np.array([x, y, z]))<br/> fitted_optimizers[opt][name].append(np.array(computed_values))</span></pre><p id="1756" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">并非每个优化器都是平等的。有些需要一阶导数，有些需要二阶导数作为我们的优化函数，也就是 Jakobian 函数和 Hessian 函数。要额外简要回顾这些是什么以及它们是如何工作的，你可以看这里的<a class="ae my" href="https://www.value-at-risk.net/functions/#:~:text=The%20gradient%20f%20and%20Hessian,of%20its%20first%20partial%20derivatives." rel="noopener ugc nofollow" target="_blank"/>。需要记住的一件事是，我们从不同的起始位置开始，让优化器自由漫游——我们现在还没有进行约束优化。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/e5b7365285e564ad896cac3e652d19ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*mPOLh4KBGuA86QObdcD4AA.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated"><em class="kf">图 2——使用标准梯度下降法从不同起点绘制的椭球函数等高线图。每一步都是“+”。优化器在几个步骤后找到全局最小值。</em></p></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/c4d39a8b1a3f38b284deafd7a9e160c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*88_gVcrmgcMPFGGRXPXR2A.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated"><em class="kf">图 3——罗森布罗克-(香蕉-)函数的等值线图。梯度下降首先走进正确的方向(见黑色“+”)，但它寻找全局最小值无济于事。它所穿越的山谷的功能价值已经很低，而且关于下一步该去哪里的信息也不多。</em></p></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/95fef6eafabcacf7c9a0051d9c020c4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*ngN7LdQMGYMtXh7HzFBu1A.png"/></div></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/a1018b02de4c39685aa09550383f7d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*EusgCbjwzUPi_vw50A2TJQ.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated"><em class="kf">图 4。属性。函数的扇形等高线图。函数空间上的 BFGS 优化。给定三个起点(三种颜色)，每一步用一个黑色十字表示。最小值很快被发现，我们可以看到线性方法(CG)与 BFGS(属于拟牛顿方法家族)相比表现如何不同。</em></p></figure><p id="e65a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">就我个人而言，它对我很有帮助，让我看到优化过程是如何在不同的函数空间中移动的。在上图中，你可以看到 CG 和 BFGS 的表现。现在 CG 是 scipy 实现<a class="ae my" href="https://youtu.be/IHZwWFHWa-w?t=325" rel="noopener ugc nofollow" target="_blank"> <strong class="ki ir">渐变下降</strong> </a>的名字。这是简单明了的优化过程，也是经典的<strong class="ki ir">线搜索法</strong>。另一方面，BFGS 使用近似的高阶信息。因此，BFGS 把它归类为一种准牛顿法。</p><pre class="mj mk ml mm gt mz na nb nc aw nd bi"><span id="09e6" class="ne ld iq na b gy nf ng l nh ni">A quasi-Newton method approximates higher order information like the Hessian Matrix (second order derivatives) and makes improvements per each step taken. [2]</span></pre><h1 id="9223" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">线搜索|回溯一切。</h1><p id="de04" class="pw-post-body-paragraph kg kh iq ki b kj lu jr kl km lv ju ko kp lw kr ks kt lx kv kw kx ly kz la lb ij bi translated">一种优化方法是简单地遵循给定基础函数的一条线(见图 2 和图 3)。我们通过回溯保留了一些信息，但仅此而已。为了优化，我们可以利用函数的一阶导数信息。具有回溯的线搜索优化的直观公式是:</p><ol class=""><li id="8256" class="lz ma iq ki b kj kk km kn kp mb kt mc kx md lb nk mf mg mh bi translated">计算你的点的梯度</li><li id="ba4f" class="lz ma iq ki b kj nl km nm kp nn kt no kx np lb nk mf mg mh bi translated">根据你的梯度和步长计算步长</li><li id="c6c3" class="lz ma iq ki b kj nl km nm kp nn kt no kx np lb nk mf mg mh bi translated">向优化方向迈出一步</li><li id="e8bf" class="lz ma iq ki b kj nl km nm kp nn kt no kx np lb nk mf mg mh bi translated">通过预先定义的因子(如α)调整步长</li><li id="c12c" class="lz ma iq ki b kj nl km nm kp nn kt no kx np lb nk mf mg mh bi translated">重复，直到找到最小值，或者您的步骤之间的值差异非常(非常)小，例如 0.00000001</li></ol><pre class="mj mk ml mm gt mz na nb nc aw nd bi"><span id="26fa" class="ne ld iq na b gy nf ng l nh ni">Gradients give us information on how the slope of a function behaves. We take the gradient as the first-order-derivative or partial derivative of a function. It is depicted as a vector and defined as:</span></pre><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nq"><img src="../Images/cc2727b3ba6667f3bf95c091dc7d572f.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*zczonhOgUO1rvdGk8QO8fQ.png"/></div></div></figure><h1 id="d6b0" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">信任区域优化器|循环思考</h1><p id="3438" class="pw-post-body-paragraph kg kh iq ki b kj lu jr kl km lv ju ko kp lw kr ks kt lx kv kw kx ly kz la lb ij bi translated">除了直视前方和后方，我们还可以 360 度全方位地环顾四周。我们查看我们的功能信息，如雅可比和海森，并朝着正确的方向迈出最佳的一步。为了实现这一点，我们考虑二次模型函数:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/8f4647c25b2c6bd22784471a34dbb9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*_R70e42G1DkrVO8VytSWHw.png"/></div></figure><p id="2ede" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">，其中 g 是 f 和 B 的梯度。<br/>当我们进行优化时，我们针对 p 进行优化，并适当调整我们周围的半径。这看起来像:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/8a7f02af2cd30501cb97cd11ae5de5b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Dd7jKn2BdZsptudEXe3CQw.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated"><em class="kf">图 attr 轮廓的信赖域优化。-部门职能。每一步都用“+”表示，每一步的周围是一个区域，从该区域中选择下一步作为一个圆。</em></p></figure><p id="8d29" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">程序的直观表述可以是这样的<br/>(参见附录中的正式表述):</p><ol class=""><li id="fbbc" class="lz ma iq ki b kj kk km kn kp mb kt mc kx md lb nk mf mg mh bi translated">计算函数的 Hessian 和梯度</li><li id="d4ae" class="lz ma iq ki b kj nl km nm kp nn kt no kx np lb nk mf mg mh bi translated">在您位置周围的半径范围内选择一个最佳点</li><li id="bd9e" class="lz ma iq ki b kj nl km nm kp nn kt no kx np lb nk mf mg mh bi translated">根据计算出的信息采取措施</li><li id="7f9a" class="lz ma iq ki b kj nl km nm kp nn kt no kx np lb nk mf mg mh bi translated">根据你进步的多少来调整半径的大小——越接近最小值越小，否则就越大</li><li id="2d95" class="lz ma iq ki b kj nl km nm kp nn kt no kx np lb nk mf mg mh bi translated">重复直到达到收敛或公差</li></ol><p id="e7b6" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">当然，这些简短的步骤并不十分复杂，你需要更多的参数和更多的数学知识来找到你半径中的正确点，调整步长和半径。如果你感兴趣的话，你可以去看看 Alg。附录中的 1 和 2。我们可以在实践中观察算法，看看半径如何随时间变化:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/431c1bf076ba627ff2123d98675f2c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*IdEL_fIn78HgrAecxSBI7A.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated"><em class="kf">图 6 —采取的步骤(x 轴)中函数值的幅度变化(y 轴)。半径δ随着函数值的提高而变化。</em></p></figure><p id="8936" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们看到，如果函数值随着时间的推移而提高，那么我们的半径会变小(见图 5 和图 6)，而如果一开始函数值不好，我们会增加半径。参见图 6 中对吸引扇区函数进行优化的第一步。</p><h1 id="eec4" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">我们起始问题的答案</h1><p id="509a" class="pw-post-body-paragraph kg kh iq ki b kj lu jr kl km lv ju ko kp lw kr ks kt lx kv kw kx ly kz la lb ij bi translated">来回答这个问题:“我的模型收敛了吗？”。我建议如下</p><ol class=""><li id="7bfe" class="lz ma iq ki b kj kk km kn kp mb kt mc kx md lb nk mf mg mh bi translated">测试不同的优化算法，比较性能和确定的最小值——它们是全局的吗？</li><li id="6127" class="lz ma iq ki b kj nl km nm kp nn kt no kx np lb nk mf mg mh bi translated">如果你的问题表面是平坦的，也就是说在优化过程中只有很小的变化:运行更多的迭代，增加容差(收敛-差异的值非常小)，也许尝试跳到函数的不同区域</li><li id="33ef" class="lz ma iq ki b kj nl km nm kp nn kt no kx np lb nk mf mg mh bi translated">从多个不同的起点开始:选择均值或中值来衡量绩效。</li></ol><h1 id="6388" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结论|兔子洞</h1><p id="f956" class="pw-post-body-paragraph kg kh iq ki b kj lu jr kl km lv ju ko kp lw kr ks kt lx kv kw kx ly kz la lb ij bi translated">我们已经看到了两类优化方法是如何工作的:直线线性优化器和信赖域优化器。这只是兔子洞的入口。对于我们看到的每个函数和问题，我们都能发现新的东西。我们可以分析我们的 Jacobian 或 Hessian 矩阵如何运行，以使我们的优化器完成工作。我们必须看看函数的形状和性质。显然，存在着大量可用的证明和定理，它们是我们刚刚看到的内容的数学支柱。如果你想深入了解原始数学，我推荐你去读一读《T2 数值优化》这本书。)作者 j·诺切达尔和 s·j·莱特[3]。</p></div><div class="ab cl nx ny hu nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="ij ik il im in"><h1 id="bd6b" class="lc ld iq bd le lf oe lh li lj of ll lm jw og jx lo jz oh ka lq kc oi kd ls lt bi translated">承认</h1><p id="5743" class="pw-post-body-paragraph kg kh iq ki b kj lu jr kl km lv ju ko kp lw kr ks kt lx kv kw kx ly kz la lb ij bi translated">我在这里展示的大部分工作都是我在参加<a class="ae my" href="https://di.ku.dk/english/" rel="noopener ugc nofollow" target="_blank">大学——DIKU</a>的数值优化课程时完成的。没有课程官员的指导，这是不可能的。</p><h1 id="70d4" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">参考</h1><p id="ed80" class="pw-post-body-paragraph kg kh iq ki b kj lu jr kl km lv ju ko kp lw kr ks kt lx kv kw kx ly kz la lb ij bi translated">[1] MathWorks <a class="ae my" href="https://www.mathworks.com/matlabcentral/fileexchange/23147-test-functions-for-global-optimization-algorithms" rel="noopener ugc nofollow" target="_blank"> <em class="nw">全局优化算法的测试函数</em></a><br/>【2】c . Geiger 和 C. Kanzow。<em class="nw">拟牛顿-弗法伦</em> <a class="ae my" href="https://link.springer.com/chapter/10.1007/978-3-642-58582-1_11" rel="noopener ugc nofollow" target="_blank">斯普林格</a><a class="ae my" href="https://link.springer.com/chapter/10.1007/978-3-642-58582-1_11" rel="noopener ugc nofollow" target="_blank">1999</a><br/>【3】j .诺切达尔和 S. J .莱特<em class="nw">数值优化</em>(第二版。斯普林格 2006 年</p><h1 id="f5b8" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">附录</h1><h2 id="5268" class="ne ld iq bd le oj ok dn li ol om dp lm kp on oo lo kt op oq lq kx or os ls ot bi translated">信赖域算法</h2><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi ou"><img src="../Images/b8924bed8de6ca11467e738fee5e5709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0YRfXPoR4hL9eKGm4DIT2w.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated"><em class="kf">图 A1——图 5 中信赖域优化的算法</em></p></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi ov"><img src="../Images/6cb6ed1bfd3013905eeeaa004dc1d149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDoU1GVQP-DnF-2kf6EewQ.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated"><em class="kf">图 A2 — Alg。求解 p——求解λ是另一个子问题。</em></p></figure></div></div>    
</body>
</html>