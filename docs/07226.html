<html>
<head>
<title>Lidar 3d Object Detection Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">激光雷达三维目标检测方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lidar-3d-object-detection-methods-f34cf3227aea?source=collection_archive---------1-----------------------#2020-06-02">https://towardsdatascience.com/lidar-3d-object-detection-methods-f34cf3227aea?source=collection_archive---------1-----------------------#2020-06-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a8f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇博文最适合那些对基于图像的2d物体检测网络有基本了解，并且有兴趣了解如何使用2d物体检测网络中使用的标准方法并针对点云3d物体检测任务进行调整的人。</p><p id="b5b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇博客文章中，首先，我们回顾了KITTI数据集中表示的激光雷达点云的数据格式。然后，我们正式定义了三维目标检测任务，并提出了常用的回归和分类损失来衡量模型处理三维目标检测任务的性能。接下来，我们将激光雷达3d对象检测网络分为两类，一类是具有输入排列不变性的网络，其展示了直接处理原始点云的对称性，另一类是具有依赖于点云的有序结构化表示的点云网格表示的网络。我们将详细讨论这两类网络的优缺点。</p><p id="b769" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于这篇博文的更详细的概要，请阅读下面的介绍部分。</p><h1 id="bb41" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">介绍</h1><p id="a8f2" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">这篇博客分为三个主要部分:激光雷达点云，三维物体检测背景和三维物体检测神经网络。</p><p id="4435" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在激光雷达点云<em class="lo"> </em>部分，首先，我们回顾一下已经成为自动驾驶感知任务标准基准的KITTI数据集。然后，我们正式定义激光雷达坐标框架，该坐标框架用作表示返回的激光雷达点的坐标以及检测网络输出处的预测定向3d框的坐标框架。接下来，描述返回的激光雷达点的数据格式。</p><p id="ad15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们从3d对象检测背景部分开始，正式定义3d对象检测任务，以及回顾编码每个预测的定向3d框的6个自由度。然后，我们提出焦点损失和硬负挖掘作为解决三维目标检测网络分类损失中背景类别不平衡的常用方法，以及平滑L1作为针对离群点的稳健回归损失。此外，我们比较了具有和不具有锚盒的3d对象检测网络的回归目标。此外，讨论了数据扩充作为3d对象检测网络的训练管道的必要部分，以确保更好的推广，并且提供了两种基于搜索的方法，其使用RL和进化算法来解决寻找最优数据扩充策略。</p><p id="2bce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在3d对象检测神经网络<em class="lo"> </em>部分，首先，我们讨论由点云作为无序点集的排列不变性引起的通过神经网络处理激光雷达点的挑战。然后，我们将三维目标检测网络分为两类:具有输入排列不变性的网络和具有点云有序网格表示的网络。关于具有输入方式排列不变性的3d对象检测网络，讨论了用于基于lidar的分类和分割的排列不变性架构<a class="ae lp" href="https://arxiv.org/abs/1612.00593" rel="noopener ugc nofollow" target="_blank">点网</a>。此外，我们回顾了<a class="ae lp" href="https://arxiv.org/abs/1711.08488" rel="noopener ugc nofollow" target="_blank">平截头体点网</a>，其依赖于基于图像的2d对象检测网络来扩充点网以用于3d对象检测任务。</p><p id="2ea4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们切换我们的齿轮集中在三维物体检测网络与点云有序网格表示。由于这些网络属于CNN目标检测网络的范畴，我们首先回顾CNN目标检测网络的两种主要类型:单镜头和基于区域提议的。此外，包括<a class="ae lp" href="https://arxiv.org/abs/1612.03144" rel="noopener ugc nofollow" target="_blank"> FPN </a>和<a class="ae lp" href="https://arxiv.org/abs/1803.01534" rel="noopener ugc nofollow" target="_blank"> PANet </a>的骨干网络和特征金字塔网络被讨论为CNN对象检测网络的主要构建块。此外，非最大值抑制作为后处理步骤被提出，以过滤由密集对象检测网络做出的预测。接下来，我们解释距离图像表示的优缺点，距离图像表示将点云解释为由激光雷达传感器拍摄的3d环境中的360度照片。引入3d体素化表示作为围绕激光雷达传感器的3d立方体子空间的量化。讨论了3d体素化表示的存储和计算效率低的问题。此外，3d卷积被呈现为自然卷积层，以处理3d体素化张量。</p><p id="0b1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们关注2d体素化表示作为3d体素化表示的替代，其通过使用2d卷积层而不是3d卷积层展示了更好的计算效率。此外，我们比较了作为3d对象检测网络一部分的手工设计和机器学习的特征编码器，3d对象检测网络依赖于2d体素化表示作为其输入。</p><h1 id="5966" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">激光雷达点云</h1><p id="81cf" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在本节中，首先，我们回顾作为激光雷达3d对象检测任务的最流行基准的<a class="ae lp" href="http://www.cvlibs.net/datasets/kitti/" rel="noopener ugc nofollow" target="_blank"> KITTI </a>数据集。然后，我们解释了KITTI数据集中使用的激光雷达坐标框架。最后，讨论了返回激光雷达点的数据格式。</p><h2 id="4b57" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">Kitti数据集</h2><p id="c25b" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir"/><a class="ae lp" href="http://www.cvlibs.net/datasets/kitti/" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">KITTI</strong></a><strong class="jp ir">数据集已经成为自动驾驶感知任务的标准基准数据集，包括基于图像的单目和立体深度估计、光流、语义和实例分割以及2d和3d对象检测。</strong>该数据集由所示的多传感器记录平台(一辆配备有激光雷达传感器和两个前向摄像头的汽车)通过在德国<a class="ae lp" href="https://www.google.com/maps/place/Karlsruhe,+Germany/@49.0157643,8.2694498,11z/data=!3m1!4b1!4m5!3m4!1s0x47970648a2e07809:0xb6fc55734cb7ee7f!8m2!3d49.0068901!4d8.4036527" rel="noopener ugc nofollow" target="_blank">卡尔斯鲁厄</a>街道上衍生生成。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/c47e1e281cd99bdbc4f3f5a329630480.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*n4iWzXS7i5rgsLVER-hhag.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="http://www.cvlibs.net/publications/Geiger2013IJRR.pdf" rel="noopener ugc nofollow" target="_blank">KITTI多模态传感器套件。</a></p></figure><p id="1eff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">KITTI数据集是一个多模态数据集，每个训练示例都是通过两个前置摄像头生成的两个摄像头图像和安装在车顶的威力登HDL-64E激光雷达传感器生成的点云捕获的带标签的3d场景。</strong>在KITTI数据集中存在7481个训练场景和7581个测试场景。由于激光雷达传感器的100毫秒360度扫描时间，该多模式传感器套件从外部3d世界的采样频率为10HZ。</p><p id="9d8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">因此，每个训练示例都是汽车周围3d世界的100毫秒快照，并由激光雷达点云(您可以将激光雷达点云视为在100毫秒内捕获的360度照片)和与扫描激光雷达传感器同步的两个相机图像形成。</strong>对于依赖于相机图像和激光雷达点云融合的感知方法，两个相机与激光雷达传感器的同步是必不可少的。这种同步要求相机在激光雷达扫描处于其视野中心时捕捉图像。</p><p id="b7d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">KITTI多模态传感器套件中的激光雷达传感器是威力登HDL-64E，如下所示。<strong class="jp ir">该激光雷达传感器提供3d场景的时空离散化扫描，其中其空间离散化由仰角(垂直)和方位角分辨率表征，其时间离散化过程由100毫秒的扫描时间表征。</strong>它的仰角分辨率为0.4度，根据它的64个激光束转换成26.9度的垂直视场。此外，它的方位分辨率等于0.08度。因此，给定64个通道(64个激光束)和0.08度的方位分辨率，由威力登HDL-64E生成的3d点云照片是具有64行和4500列的图像。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/a9b1786ba171369fd15fdf5af34b966a.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*Q2bf9b8XPk0Z1wWQB7RKRw.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://velodynelidar.com/products/hdl-64e/" rel="noopener ugc nofollow" target="_blank">威力登HDL-64E </a></p></figure><h2 id="df19" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">激光雷达坐标框架</h2><p id="ca36" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">在KITTI传感器套件中，安装在车顶顶部的激光雷达传感器的坐标框架的重要性在于，不仅返回的激光雷达点会显示在激光雷达坐标框架中，预测的3d边界框也会显示在该坐标框架中。</strong>换句话说，3d物体检测模型的输入和输出都呈现在激光雷达坐标框架中。在下图中，您可以看到激光雷达坐标框架，其中汽车被描绘为灰色框。特别地，这个坐标框架的原点是激光雷达传感器的中心；它的x轴指向车头；它的y轴指向驾驶座的左侧，z轴指向天空。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/7726832307b6e3b3711f02d23b6d820d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*WDgby8Gb7ei9N7Hw27bPwA.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">激光雷达坐标框架:笛卡尔坐标和球面坐标。</p></figure><p id="5672" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所描述的激光雷达坐标框架中的点可以用其笛卡尔坐标(x，y，z)或球坐标(θ，ϕ，r)来表示。在球坐标中，θ称为仰角，是相对于z轴正方向的角度，ϕ称为方位角，是在x-y平面中相对于x轴正方向的角度，r是该点到原点的距离。笛卡尔坐标和球坐标之间存在如下一对一映射:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/71a1376a98784fcce06e011c4ea94dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*NmULyYwzKqNuAlUb5fTUsw.png"/></div></figure><h2 id="2f7a" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">点云数据格式</h2><p id="2fe7" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">每个点云都是返回的激光雷达点的无序集合。每个返回的激光雷达点的数据格式是由其相对于激光雷达坐标框架的坐标及其强度ρ形成的4元组。在KITTI数据集中，ρ是介于0和1之间的归一化值，它取决于激光雷达光束反射的表面特征。返回的激光雷达点可由其笛卡尔坐标(x，y，z)或球面坐标(θ，ϕ，r)表示。KITTI数据集使用激光雷达坐标系中的笛卡尔坐标及其强度来表示返回的激光雷达点，如下所示:(x，y，z，ρ)。下面，你可以看到一个点云的插图。KITTI数据集中的每个场景点云平均有大约100K个点。请注意，每个场景返回的激光雷达点的数量根据场景的特征(如场景中行人或汽车的数量)而有所不同。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/c7035cf695319100aee1682fb56730c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*cRf5TwITW5QuJoZQqz29TA.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://arxiv.org/abs/1907.03670" rel="noopener ugc nofollow" target="_blank">旁边儿的云</a></p></figure><h1 id="1ad6" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">3d对象检测背景</h1><p id="3b42" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在本节中，我们将重点介绍制定3d对象检测任务所需的背景。我们首先从正式定义点云三维物体检测任务开始。然后，我们介绍了如何使用回归和分类损失来测量这项任务的性能。最后，我们回顾了数据扩充作为一种基本方法，以提高处理三维物体检测任务的ML模型的泛化能力。</p><h2 id="7bf0" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">3d对象检测任务</h2><p id="47f4" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">这里，我们将基于lidar的3d对象检测任务正式定义如下:给定由返回的lidar点(在lidar坐标框架中表示)形成的场景的点云，预测与场景中的目标演员相对应的定向3d边界框(在lidar坐标框架中表示)。</strong>对于自动驾驶应用，你可以假设最本质的这类目标行为者是汽车、自行车和行人。此外，定向3d框是相对于3d框的主体坐标框架增加了航向角的3d框。</p><p id="601d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">务必充分理解在激光雷达坐标框架中表示定向3d边界框的含义。通常用由盒子尺寸、盒子中心坐标和航向角组成的6个自由度来编码定向的3d边界盒子。<strong class="jp ir">注意，仅编码航向角而不编码俯仰角和横滚角的原因是航向角对自动驾驶系统的跟踪、预测和规划堆栈的逻辑的影响，而俯仰角和横滚角在这些堆栈中做出的决策中不起主要作用。</strong></p><p id="6228" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">盒子的尺寸仅仅是它的宽度、长度和高度(w，l，h ),同时考虑到长度总是大于宽度的不变性。盒子中心的坐标(x，y，z)相对于激光雷达坐标框架来表示。盒子的航向角ϕ在盒子的主体坐标系中测量，该坐标系是激光雷达坐标系到盒子中心的平移版本。特别地，盒子的航向角θ指的是其长度尺寸(x-y平面上的较长边)平行于其主体坐标框架的x轴的情况。</p><h1 id="5df3" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">分类损失和回归损失</h1><p id="347d" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">无论使用什么ML模型来实现3d对象检测任务，我们都期望模型输出对每个预测框的6个自由度及其类别进行编码。这些分类和回归预测分别被合并到分类和回归损失中，以便为3d对象检测模型提供训练信号。</p><h1 id="94cf" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">分类损失</h1><p id="fd0e" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">目标检测网络的默认分类损失是交叉熵，其等于地面真实类的负对数似然。目标检测网络的分类损失的主要复杂性是解决背景(负面)类别和正面类别之间的类别不平衡。可以观察到，对于给定的图像或场景，大多数潜在的2d和3d边界框候选包含背景场景，而不是目标演员和对象。因此，在目标检测训练数据集中存在显著的类别不平衡。解决这种等级不平衡的两种标准方法是<a class="ae lp" href="https://arxiv.org/abs/1504.08083" rel="noopener ugc nofollow" target="_blank">硬负开采</a>和/或<a class="ae lp" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank">焦损</a>。<strong class="jp ir">在硬否定挖掘方法中，目标是对每个图像/场景的否定边界框进行子采样，使得否定框的数量最多是肯定框数量的3倍，同时为每个图像/场景选择最硬的否定框。</strong>用于量化负盒硬度的度量是其交叉熵损失，其中较大的交叉熵损失指定较难的示例。</p><p id="e912" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">另一方面，</strong> <a class="ae lp" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">焦点损失</strong> </a> <strong class="jp ir">通过在训练过程中自适应地调整每个样本在分类损失中的贡献权重来解决背景类和正面类的类不平衡，而不是显式地对负面样本进行子采样</strong>。焦点损失主要基于这样的想法，即大多数背景框对于网络来说非常容易识别，使得它们的基础真实类概率(背景类概率)在训练过程的早期变得非常接近1.0。因此，自适应地调整样本的贡献权重，使其与它们的当前基本真实概率和实数1.0之间的绝对差成比例，导致大多数负样本对分类损失的贡献可以忽略，这减轻了背景类和正类之间的类不平衡。</p><h1 id="e052" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">回归目标和损失</h1><p id="698e" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">回归损失根据对应于盒子的6个自由度的回归目标来定义。<strong class="jp ir">通常的做法是预测这6个自由度的转换版本(称为回归目标)，而不是直接预测它们，以增加模型预测的动态范围，并通过提供具有潜在更高熵的梯度来帮助训练过程。</strong></p><h2 id="027c" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">带有定位框的回归目标</h2><p id="59cf" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">首先，我们回顾依赖于锚盒的对象检测网络的回归目标。<strong class="jp ir">如果输入遭受像自然图像那样的比例模糊，其中图像中出现的对象的尺寸取决于它们到摄像机的距离，或者如果网络需要检测具有不同典型尺寸的不同类别对象，如旨在检测汽车、自行车和行人的激光雷达3d对象检测网络，则使用锚定框变得至关重要。</strong>在这种情况下，锚框允许网络学习关于一组具有不同比例和纵横比的预定义锚框的回归偏移，这些锚框被设计为模型的超参数，以最佳匹配具有不同维度的类对象。设计锚定框的目标是，在训练过程结束时，每种类型的锚定框都将专用于一个与锚定框的比例和纵横比最匹配的类对象。</p><p id="7111" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面，你可以看到带有定位框的回归目标。在这些公式中，下标<em class="lo"> gt </em>指地面真相箱，下标<em class="lo"> a </em>指锚箱。<strong class="jp ir">标准做法是使用<em class="lo"> log </em>刻度来表示盒子的宽度、长度和高度，以增加预测尺寸的动态范围。</strong>此外，根据锚盒的尺寸的划分确保预测目标是相对于匹配的锚盒的偏移校正的形式。中心框坐标是根据它们相对于锚框中心坐标的偏移来预测的，并通过它们相应的锚框尺寸来进一步归一化。<strong class="jp ir">最后，航向角目标被编码为地面真实航向角和锚箱航向角之差的<em class="lo"> sin </em>和<em class="lo"> cos </em>。这种编码方案消除了明确实现预测角度必须在0和2π范围内的约束的要求。</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mp"><img src="../Images/0e601ebb2924803d673d49259e81bcc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IkoSUuhkNHY_mESi_WG9Tg.png"/></div></div></figure><h2 id="6dc7" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">没有定位框的回归目标</h2><p id="eb71" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在没有锚盒的对象检测模型的情况下，回归目标如下。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mu"><img src="../Images/88845eb47bea24e30d416e109a903ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yYDQjhLWmLuWWcZkE7KHQg.png"/></div></div></figure><p id="a299" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，对于中心框坐标目标，我们仍然将它们预测为相对于参考点的偏移，这取决于用于该预测的特征地图条目的空间坐标。不需要锚定框的对象检测任务的一个例子是3d lidar对象检测任务，其仅关注于检测汽车并且不会遭受比例模糊。</p><h2 id="e93c" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">回归损失</h2><p id="8b38" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><a class="ae lp" href="https://arxiv.org/abs/1504.08083" rel="noopener ugc nofollow" target="_blank">平滑L1损失</a>已经成为目标检测回归目标的标准损失，可以写成</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/fbbc87b0a82c9cd5887458a5a8652668.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*4A7oHSSSyTlpSWpbrciTfA.png"/></div></figure><p id="cc94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<em class="lo"> x </em>表示网络预测值和回归目标值之间的差值。因此，我们将有6个平滑L1项，对应于每个盒子的6个自由度。<strong class="jp ir">平滑L1损失优于L2损失，主要是因为它对异常值更稳健。离群值导致具有大绝对值的误差，而与误差幅度成比例的L2损失导数导致SGD训练过程变得不稳定并被离群值支配。另一方面，对于<em class="lo"> |x| &gt; 1 </em>的平滑L1损失的导数是常数，并且与误差幅度不成比例，这使得它对于异常值是鲁棒的。</strong></p><h1 id="8390" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">数据扩充</h1><p id="78be" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">数据扩充是每个对象分类和检测模型的训练管道的基本部分。<strong class="jp ir">数据增强的目标是提高泛化能力，使分类和检测网络相对于图像像素值和点云激光雷达点的旋转、平移和自然变化保持不变</strong>。在本节中，我们首先回顾图像的数据扩充技术，因为它们将为讨论点云的数据扩充方法提供相关背景。</p><h2 id="986a" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">图像数据增强</h2><p id="bc73" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">展示数据增强对于感知任务的重要性的突出作品是<a class="ae lp" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>。在AlexNet之后，用于图像分类和检测模型的数据扩充方法成为训练管道的标准步骤。这种增强技术的例子是随机裁剪、缩放、旋转、水平翻转和光度变换。<strong class="jp ir">注意，这些数据扩充方法的随机性对提高网络的泛化能力起着重要作用。</strong>特别地，对于训练的每个时期和对于每个图像，除了是否应用给定的数据扩充方法之外，这些数据扩充方法的参数(像旋转数据扩充的旋转角度)在训练期间被随机绘制。另一种最近流行的数据扩充方法是<a class="ae lp" href="https://arxiv.org/abs/1708.04552" rel="noopener ugc nofollow" target="_blank"> Cutout </a>，它在训练过程中随机屏蔽图像的正方形区域。</p><p id="c1df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所有上述数据扩充方法都是图像级数据扩充方法，并且最初是针对图像分类任务提出的，但是已经显示出对于对象检测任务也是有用的。<strong class="jp ir">最近，将这些数据增强方法单独应用于对象的边界框也变得很常见，这被称为对象级数据增强方法。</strong>换句话说，除了影响整个图像的图像级变换之外，我们还可以将这些变换分别应用于每个对象的边界框。例如，通过旋转单个对象的相应边界框并用值0填充堤岸区域，可以将随机旋转应用于单个对象。</p><p id="c282" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">与为每个输入图像即时随机创建数据增强方法的组合相反，有理由假设存在数据增强方法的特定组合，其更好地模拟图像和我们周围世界中自然发生的变换。因此，学习这样的作文将潜在地导致更好的概括。遵循这一思想，<a class="ae lp" href="https://arxiv.org/pdf/1805.09501.pdf" rel="noopener ugc nofollow" target="_blank">自动增强</a>将数据增强方法的哪些组合应用于图像以用于图像分类任务的顺序离散决策问题公式化为增强学习(RL)问题，奖励信号是测试数据集上训练模型的准确性。</strong></p><p id="d9a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae lp" href="https://arxiv.org/abs/1906.11172" rel="noopener ugc nofollow" target="_blank"> <em class="lo">学习用于对象检测的数据增强策略</em> </a> <em class="lo">中，将上述策略梯度RL方法进一步应用于对象检测任务。</em> <strong class="jp ir">在提议的RL框架中，RL代理被建模为RNN网络，其中在每个时间步，它通过其输出softmax层做出离散决策，以选择数据扩充方法或为已经选择的数据扩充方法选择超参数值。</strong>RL代理被要求生成五个数据增强子策略，其中每个子策略是两种数据增强方法的组合。在训练期间，对于每个输入图像，随机选择五个子策略中的一个并应用于该图像。结果表明，使用RL代理生成的数据扩充策略优于随机数据扩充方法。</p><h2 id="42d4" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">点云数据扩充</h2><p id="a4b6" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">应用于激光雷达点云的数据增强方法主要受最初为图像设计的增强方法的启发</strong>。<strong class="jp ir">点云数据扩充方法必须符合控制激光束传播的物理定律。</strong>例如，我们希望点云随着远离激光雷达传感器而变得越来越稀疏。因此，如果数据扩充方法不符合此约束，则不应在训练期间使用，因为它不模拟任何真实世界的场景。</p><p id="02db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一些常见的点云数据增强是围绕激光雷达坐标框架的z轴的旋转、相对于激光雷达坐标框架的x-z和y-z平面的翻转、激光雷达点的稀疏化、应用于激光雷达点的加性高斯噪声、平截头体缺失和具有对象点云的场景增强。这些数据扩充方法中的大多数既可以应用于场景级，也可以应用于对象级。在实时训练期间，对于每个给定的点云场景，我们做出是否应用每个场景级数据扩充方法的随机二元决策，以及场景中每个对象的随机二元决策，以便确定是否应用每个对象级数据扩充方法。下面，您可以看到应用于点云的潜在数据增强方法的示例，这些示例来自论文<a class="ae lp" href="https://arxiv.org/abs/2004.00831" rel="noopener ugc nofollow" target="_blank">通过基于渐进人群的增强改进3D对象检测</a>。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mv"><img src="../Images/bc3990be65ceb7e366f84e5112836fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9uD6v1G0ksSdYmtdQXSpQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://arxiv.org/abs/2004.00831" rel="noopener ugc nofollow" target="_blank">应用于点云的数据增强方法。</a></p></figure><p id="b117" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">受为图像开发的</strong> <a class="ae lp" href="https://arxiv.org/pdf/1805.09501.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">自动增强</strong> </a> <strong class="jp ir">框架的启发，开发了用于点云的类似的</strong> <a class="ae lp" href="https://arxiv.org/abs/2004.00831" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">框架</strong> </a> <strong class="jp ir">，其将寻找点云数据增强方法的最基本组合的问题公式化为搜索问题，其中搜索空间跨越所有潜在的这种组合。所采用的搜索方法是一种进化算法，称为基于渐进人群的增强(PPBA)。</strong>有人认为，选择这种进化搜索方法而不是基于RL的搜索方法是因为其效率更高，这一点至关重要，因为点云数据增强方法的搜索空间大于其图像对应物。</p><h1 id="5a6c" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">三维物体检测神经网络</h1><p id="74b5" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在本节中，我们将重点关注3d对象检测神经网络及其挑战。在回顾了3d对象检测神经网络所面临的挑战之后，我们将点云3d对象检测网络分为两个主要类别:具有输入方式排列不变性的网络和具有点云有序网格表示的网络。</p><p id="39b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">点云的主要特征阻止我们容易地采用CNN对象检测神经网络，这是因为它们的排列不变性，而CNN对象检测网络假定它们的输入是以网格形式表示的有序数据结构。网格有序数据结构的例子是图像，这意味着改变图像的像素顺序会修改图像的内容。</p><p id="82ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">点云是无序的点集，意味着改变点的顺序不会改变由点云表示的3d对象。这一特性要求处理点云的神经网络相对于它们的输入是置换不变性的，如果它们打算直接消耗点云的话。特别是，神经网络预测的3d盒子不应受到输入激光雷达点顺序变化的影响。<strong class="jp ir">存在一类激光雷达3d物体检测神经网络，其是输入方式排列不变性的，并且直接消耗点云作为其输入，而不依赖于点云的任何中间表示形式。</strong></p><p id="541b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，存在另一类点云3d对象检测神经网络，其依赖于将点云变换为类似于图像的有序网格表示，使得它们不再受点云的排列不变性的限制，并且结果，它们可以将CNN对象检测网络开箱即用地应用于点云的网格表示。</p><h1 id="6560" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">具有输入方式排列不变性的3d对象检测网络</h1><p id="09c2" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">输入方式排列不变性神经网络通过直接处理原始点云来执行点云3d对象检测任务，而不依赖于它们的有序网格表示</strong>。因此，为了尊重点云是激光雷达点的无序集合的属性，要求它们是输入方式的置换不变性。考虑到这一特性，更改网络输入处激光雷达点的顺序不会更改网络的输出预测。</p><p id="fc22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lp" href="https://arxiv.org/abs/1612.00593" rel="noopener ugc nofollow" target="_blank"> PointNet </a>是一种输入方式排列不变性神经网络，设计用于点云分类和语义分割任务(不用于3d对象检测任务)。<strong class="jp ir">point net背后的主要思想是以下通用近似定理:任何连续的输入方式排列不变性函数<em class="lo"> f </em>可以通过两个函数<em class="lo"> h </em>和<em class="lo"> g </em>的组合来近似，其中<em class="lo"> g </em>必须是对称函数，以确保<em class="lo">f</em></strong>的排列不变性。特别地，首先，函数<em class="lo"> h </em>被单独地(逐点地)应用于每个激光雷达点，该函数将每个激光雷达点变换为然后，函数<em class="lo"> g </em>获取由函数<em class="lo"> h </em>生成的lidar点嵌入，并生成对应于输入点云的维度<em class="lo"> d </em>的单个全局特征嵌入。这种对称函数g的例子是基于元素的最大池和平均池。如果嵌入维数<em class="lo"> d </em>足够大，以保证函数f通过函数h和g的合成相对于给定的逼近边界误差的逼近，则通用逼近定理是有效的。下图显示了对应于四个激光雷达点的函数<em class="lo"> h </em>和对称函数<em class="lo"> g </em>的组成。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mw"><img src="../Images/a52c7ebc3f3add5419ec46ae5f48df97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hc4YQVyPYHtJtXbVxxfyZA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">通过将点网络应用于4个激光雷达点来说明建议的架构。首先，函数h分别应用于每个激光雷达点，并将它们转换为维数为d的嵌入。然后，对称函数g采用这4个嵌入，并输出单个全局特征向量。</p></figure><p id="d750" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图显示了为点云分类和语义分割任务提议的点网络架构。顶部的蓝色路径指定分类网络，而底部的黄色路径是语义分割网络。在分类网络中，从输入层到最大池层的层形成函数<em class="lo"> h </em>，而最大池层被选为对称函数<em class="lo"> g </em>。嵌入维数<em class="lo"> d </em>选择为1024。在PointNet中，以逐点方式应用于每个激光雷达点的函数<em class="lo"> h </em>由全连接层(MLP)和<a class="ae lp" href="https://arxiv.org/abs/1506.02025" rel="noopener ugc nofollow" target="_blank">空间变换网络</a>形成。PointNet中有两个空间转换器网络，在下图中称为T-Net。空间变换网络在其输入点上执行数据相关的仿射变换。依赖于数据导致应用于输入点的仿射变换被即时确定为输入点本身的函数。<strong class="jp ir">空间变换网络的主要目标是确保分类网络对于应用于点云的仿射变换的不变性。</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mx"><img src="../Images/2967baaa2e256cf73c1486317f1c3d08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z1af6WTIURm9jv5o2SW8JA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://arxiv.org/abs/1612.00593" rel="noopener ugc nofollow" target="_blank">点网</a></p></figure><p id="20b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上图所示，虽然PoinNet分类网络的最终输出代表输入点云的单个分类决策(k个输出得分),但语义分割网络依赖于点云的全局特征(维度为1024)和局部特征(维度为64)的串联，以针对每个点云做出单独的分类决策(m个输出得分)。</p><p id="c9d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">点网无法执行3d对象检测任务，因为点网的分类网络假设输入点云的所有激光雷达点都属于单个对象。<strong class="jp ir"> </strong> <a class="ae lp" href="https://arxiv.org/abs/1711.08488" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">平截头体点网</strong> </a> <strong class="jp ir">用3d区域提议网络来扩充点网，以便将3d输入场景划分为子空间，使得每个子空间潜在地仅包含单个对象的lidar点。</strong>作为这种划分的结果，类似点网的网络可以用于聚焦于对应于特定3d提议区域的子空间的3d对象检测任务。下图显示了平截头体点网的建议架构。3d区域提议网络是左边的块，它被称为平截头体提议。<strong class="jp ir">该建议网络基于由基于2d图像的对象检测网络预测的2d边界框的3d投影。因此，平截头体点网络是一个多模态(图像和激光雷达)3d物体检测网络</strong>。中间的块被称为3d实例分割，其获取3D提议区域内的激光雷达点，并执行逐点二元分类以确定每个给定的激光雷达点是否属于感兴趣的对象。最后的块是模型3d框估计，其将通过3d实例分割预测的那些lidar点视为属于感兴趣的对象，并输出对应于3D提议区域的3D定向框的尺寸、中心坐标和航向角。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi my"><img src="../Images/b10839d93e47e632b748a8c3579bbb66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r5_H5x1BBp_hipEyzzDSFQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://arxiv.org/abs/1711.08488" rel="noopener ugc nofollow" target="_blank">锥台点网</a></p></figure><h1 id="0f6b" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">具有点云有序网格表示的3d对象检测网络</h1><p id="e051" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在本节中，我们关注一类3d对象检测网络，其依赖于有序网格张量来表示点云，以便去除它们的置换不变性约束。特别地，这些检测网络不是直接消耗原始点云，而是以类似图像的有序网格的形式获取点云的中间表示作为它们的输入。<strong class="jp ir">这类3d物体检测网络的主要特点是将类似于图像的点云表示为结构化网格，以便它们可以受益于现有的基于图像的CNN物体检测网络。</strong></p><p id="0ccd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过将点云表示为有序网格，这些有序网格表示可以直接传递给CNN对象检测网络，如<a class="ae lp" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">fast-RCNN</a>和<a class="ae lp" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank"> SSD </a>。也就是说，这些网络的回归头需要进行一些改变，以适应3d对象检测任务的回归目标。例如，需要在激光雷达坐标框架中而不是在2d图像平面坐标框架中进行盒子尺寸和坐标的预测。此外，回归头需要预测箱子的航向角。具有有序网格表示输入的点云3d对象检测网络主要采用了2d对象检测网络的现有架构，并进行了较小的修改。因此，我们首先回顾2d对象检测网络的两个主要类别:单触发和基于区域提议的网络，以及作为这些2d对象检测网络的主要构建块的骨干网络和特征金字塔网络。</p><h2 id="fcf1" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">单发探测网络</h2><p id="cc04" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><a class="ae lp" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank">单发探测网络</a>通常被称为实时目标探测网络。<strong class="jp ir">这些检测网络的快速响应时间是基于它们预测箱子的单阶段过程。通过由卷积层作为回归和分类头实现的密集预测范例，这种单级预测机制是可能的。</strong>该密集预测过程为每个输入图像生成大约100K个盒子，这些盒子需要通过非最大抑制(<a class="ae lp" href="https://arxiv.org/abs/1704.04503" rel="noopener ugc nofollow" target="_blank"> NMS </a>)后处理步骤进行滤波。这些100K预测框中的大多数是背景框，因此容易过滤掉。另一方面，具有不同于背景类别的类别的预测的盒子需要被传递到NMS，从而在重叠的盒子中仅选择具有最高置信度得分的盒子。NMS将在下面的章节中详细讨论。</p><p id="c17d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图展示了名为<a class="ae lp" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank"> RetinaNet </a>的单次检测网络的架构。每个单镜头检测网络由以下三个主要的构建模块组成:(1)主干网络(2)特征金字塔网络(3)分类和回归头。在接下来的章节中，我们将详细介绍主干网络和特征金字塔网络。</p><p id="0466" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">分类和回归头负责单发检测网络的密集预测。他们将特征图作为输入，并对特征图的每个条目预测一个盒子，这实现了单触发网络的密集预测范式</strong>。分类和回归头是完全卷积网络，以便执行高分辨率密集预测。这些全卷积网络将特征图作为它们的输入，并为每个特征图条目生成表示类逻辑和回归目标的输出特征图。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mz"><img src="../Images/415f8ea5fccfa4f1173f8d8e8a373224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gw9Nk55FtGhI86IcKDDe4g.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank">视网膜网</a></p></figure><h2 id="47e7" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">基于区域提议的检测网络</h2><p id="72e2" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">基于区域提议的检测网络是两级检测网络，其不依赖于在单次检测网络中使用的密集预测范例，而是采用显式区域提议网络，该网络输出将被传递到分类和回归头的区域候选。</strong>基于区域提议的检测网络的代表网络是<a class="ae lp" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">更快的R-CNN </a>，如下图所示。与针对特征图的每个条目预测一个框的单次检测网络不同，在基于区域提议的检测网络中，区域提议网络生成大约2000个区域候选，其中，首先，它们的特征图使用ROI(感兴趣区域)汇集层来汇集。然后，候选区域的汇集特征图被分别传递到分类和回归头，以便对每个候选区域预测一个盒子。显式区域提议网络和单独处理区域候选使得基于区域提议的检测网络不如单次检测网络那样快。然而，类似于单次检测网络，基于区域提议的检测网络依赖于主干网络和特征金字塔网络作为特征编码器和特征地图增强模块。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/15289a5913328d5265865ca75fea5ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*7qRWrCuvBnkiNYQaF4k-CQ.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">更快的R-CNN </a></p></figure><h2 id="319d" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">主干网络</h2><p id="78ec" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">主干网络是输入图像或输入点云有序网格表示的特征生成器(编码器)。通常使用为<a class="ae lp" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>分类任务(具有1000个类别和大约1M训练图像的图像分类任务)设计的CNN网络作为对象检测网络的骨干网络。特别是，ImageNet分类网络在其最终的1000路softmax层之前被切割，并用作主干网络。最知名和常用的骨干网有:<a class="ae lp" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>、<a class="ae lp" href="https://arxiv.org/pdf/1409.1556.pdf" rel="noopener ugc nofollow" target="_blank"> VGG </a>、<a class="ae lp" href="https://arxiv.org/abs/1512.00567" rel="noopener ugc nofollow" target="_blank"> Inception </a>、<a class="ae lp" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet </a>、<a class="ae lp" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNet </a>、<a class="ae lp" href="https://arxiv.org/abs/1801.04381" rel="noopener ugc nofollow" target="_blank"> MobileNetV2 </a>、<a class="ae lp" href="https://arxiv.org/abs/1707.07012" rel="noopener ugc nofollow" target="_blank"> NasNet </a>、<a class="ae lp" href="https://arxiv.org/abs/1802.01548" rel="noopener ugc nofollow" target="_blank"> AmoebaNet </a>、<a class="ae lp" href="https://arxiv.org/abs/1807.11626" rel="noopener ugc nofollow" target="_blank"> MnasNet </a>和<a class="ae lp" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank">efficent net</a>。对于检测网络，骨干网络的选择取决于感知系统的延迟要求以及可用的存储器和计算资源。高效的骨干网有Inception、MobileNetV2、MnasNet、EfficientNet。</p><p id="9bd4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当使用骨干网络作为对象检测网络的特征生成器时，另一个设计选择是选择由骨干网络生成的哪些特征图将被传递到检测网络内的下游模块。<strong class="jp ir">不仅处理主干网络的最终特征图，而是传递在主干网络的不同阶段生成的若干特征图背后的主要动机是处理输入传感器模态(如自然图像)的比例模糊问题，或/和检测具有不同类别的对象，这些对象表现出显著不同的维度。</strong>后一种情况的一个例子是自动驾驶应用，其中3d对象检测网络旨在检测具有不同尺寸的汽车、卡车、自行车和行人。</p><p id="dbdd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">特别是，主干网络不同阶段的特征地图具有不同的空间感受域，这使它们成为解决传感器模态(如图像或不同维度对象的检测)的尺度模糊性的自然解决方案。这种方法在感知系统中被称为多尺度特征地图。要了解更多关于多尺度特征图的特征以及不同类型的卷积层如何影响特征图的感受野，可以回顾我们以前的博客文章，名为<a class="ae lp" rel="noopener" target="_blank" href="/analysis-and-applications-of-multi-scale-cnn-feature-maps-a6804bbac8">多尺度CNN特征图的分析和应用</a>。</p><h2 id="5d78" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">特征金字塔网络</h2><p id="1c54" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">特征金字塔网络用于主干网络之上，以丰富和扩充主干网络生成的特征地图。该增强过程的主要目标是增强特征图的表示能力，针对2d和3d对象检测回归和分类任务而定制。</strong>特征金字塔网络采用在骨干网络的<em class="lo"> C </em>不同阶段(可能具有<em class="lo"> C </em>不同的空间分辨率)生成的<em class="lo"> C </em>特征地图，并输出通常具有与输入特征地图空间分辨率相同的空间分辨率的<em class="lo"> C </em>增强特征地图。</p><p id="7ab5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">提出的突出特征金字塔网络是下图所示的<a class="ae lp" href="https://arxiv.org/abs/1612.03144" rel="noopener ugc nofollow" target="_blank"> FPN </a>。左边的路径是自下而上的路径，是主干网络的一部分。在主干网络的不同阶段，具有不同空间分辨率的三个特征地图被选择并传递到特征金字塔网络，该网络由右侧自上而下的路径示出。通常选择空间分辨率为<em class="lo"> H/4 </em> x <em class="lo"> W/4，H/8 </em> x <em class="lo"> W/8 </em>和<em class="lo"> H/16 </em> x <em class="lo"> W/16 </em>的骨干特征图作为特征金字塔网络的输入，其中<em class="lo"> H </em>和<em class="lo"> W </em>表示输入图像的高度和宽度。</p><p id="5009" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更深的特征地图在语义上更强，FPN的目标是使具有更高空间分辨率的更浅的特征地图在语义上与更深的特征地图一样强。这是通过FPN自上而下的上采样路径将编码的语义信息从较深的特征图转移到较浅的特征图来实现的。然而，FPN自上而下的上采样路径缺乏对目标定位至关重要的粒度空间信息。通过使用横向连接将自顶向下路径中的上采样特征图与自底向上路径中的特征图融合，这种空间信息的缺乏得以缓解。</p><p id="7985" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在特征金字塔网络的设计过程中，存在着如何对自顶向下路径中的特征地图进行上采样以及如何将自底向上路径的特征地图与自顶向下路径的特征地图相结合的自由度。虽然FPN使用最近邻插值作为上采样方法，但上采样的其他选项有<a class="ae lp" rel="noopener" target="_blank" href="/transposed-convolution-demystified-84ca81b4baba">钉床、双线性插值、最大解卷积和转置卷积(解卷积)</a>。<strong class="jp ir">在这些上采样方法中，唯一依靠训练数据来学习定制的上采样操作的方法是转置卷积。因此，它是最有前途的上采样层类型，并在最近的神经网络架构中受到欢迎</strong>。然而，由于其可学参数，如果模型大小是一个问题，应避免。</p><p id="a654" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个自由度是如何将由横向逐像素卷积层从自底向上路径转换的特征图与自顶向下路径的上采样特征图融合。两种常见的方法是:(1)元素相加，它要求特征映射具有相同数量的通道(2)通道相连接。逐通道连接方法具有更高的表示能力，但计算成本更高。FPN使用元素相加作为融合方法，并进一步依靠3×3卷积层来平滑融合的特征图，以减轻混叠效应。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/e5dfa89ae77427d5549ef41c9805d582.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*qBUFmwQqFM88xhf9Zodx5g.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://arxiv.org/abs/1612.03144" rel="noopener ugc nofollow" target="_blank"> FPN </a></p></figure><p id="1bdf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个最近提出的特征金字塔网络是下图所示的<a class="ae lp" href="https://arxiv.org/abs/1803.01534" rel="noopener ugc nofollow" target="_blank">面板</a>。它是作为FPN的延伸而建造的。特别是，它为FPN增加了一条新的自下而上的道路。在PANet框图中，块<em class="lo"> a </em>是原来的FPN，块<em class="lo"> b </em>是新增加的自底向上路径。这个新的自下而上的路径以FPN自上而下的路径生成的特征地图(P2，P3，P4，P5)作为输入，并生成一组新的特征地图(N2，N3，N4，N5)作为输出。<strong class="jp ir">这种新的自下而上路径背后的动机是确保具有较低空间分辨率的较深特征地图包含基本语义信息，例如由较浅特征地图编码的边缘、拐角和斑点。在主干网络中，从较浅的层到较深的层获取此类基本信息的可能性较小，因为它必须经过许多层，而在PANet的自下而上路径中，它只需要经过3层，如下图中绿色定向路径所示。</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi na"><img src="../Images/5b7124eec6bc74820c4c216bf3a8f75c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*81AnwYbwuiqAh7NiNS3OyA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://arxiv.org/abs/1803.01534" rel="noopener ugc nofollow" target="_blank">面板</a></p></figure><p id="455b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另外<a class="ae lp" href="https://arxiv.org/abs/1904.07392" rel="noopener ugc nofollow" target="_blank"> NAS-FPN </a>将寻找最佳特征金字塔网络公式化为神经架构搜索问题。所发现的架构由自顶向下和自底向上的连接组合而成。最近，<a class="ae lp" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank"> BiFPN </a>提出了一种加权双向(自顶向下和自底向上路径)特征金字塔网络块，该网络块可以根据计算资源和所需精度重复。</p><h2 id="6d7e" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">非最大抑制</h2><p id="a2b5" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><a class="ae lp" href="https://arxiv.org/abs/1704.04503" rel="noopener ugc nofollow" target="_blank">非极大值抑制</a> (NMS)是大多数目标检测网络的后处理步骤。对象检测网络为场景中的每个对象生成几个肯定的预测。<strong class="jp ir">因此，要求NMS只选择重叠边界框中具有最高置信度得分的那些肯定预测边界框。对象检测网络为每个对象生成若干预测的这一特征源于训练范例，该范例为每个对象分配若干肯定目标。</strong>例如，在具有锚框的<a class="ae lp" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank">对象检测网络的情况下，与基础事实边界框具有大于0.5的交集(IOU)的每个锚框与该基础事实边界框匹配，并被认为是正面示例。因此，在这个过程的最后，每个基础事实框潜在地存在几个正锚框。此外，在没有锚框</a>的<a class="ae lp" href="https://arxiv.org/abs/1902.06326" rel="noopener ugc nofollow" target="_blank">对象检测网络的情况下，地面真实边界框内的每个参考特征地图条目被认为是旨在预测对应于目标对象的边界框的正面示例。</a></p><h1 id="f093" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">距离图像表示</h1><p id="7b17" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在回顾了单镜头和基于区域提议的目标检测网络类别及其主干和特征金字塔网络形式的构建块之后，我们将注意力转向点云的网格表示。<strong class="jp ir">在本节中，我们重点关注点云的距离图像表示，它将点云解释为激光雷达传感器拍摄的3d环境的360度照片。</strong></p><p id="0b6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在下图所示的网格表示中，点云被转换为一个图像张量，其中行维度表示激光束的仰角θ，列维度表示激光雷达传感器的方位角ϕ。对于威力登HDL-64E，由于其64个激光束，其相应的距离图像具有64行，由于其0.08度的方位分辨率，其相应的距离图像具有4500列。此影像的每个条目的像素值等于其对应的返回激光雷达点的范围。然而，对于此图像中的大量条目，没有返回的激光雷达点，这主要是因为激光束射向天空，没有击中任何障碍物。此外，标准做法是为没有返回激光雷达点的条目分配零值来表示缺失的观测值。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nb"><img src="../Images/6aee9dbc6f8976bab1a85ff2b5d54460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3pGTFA4JFB7bn-N0KqUhpA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">距离图像表示，其中行表示仰角θ，列表示方位角ϕ.</p></figure><p id="662a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">距离图像表示的主要优点有两个:(1)距离图像可以直接馈入CNN目标检测网络(2)距离图像导致在将点云转换为网格表示时添加最少的冗余信息。特别地，添加的冗余信息是指没有返回的激光雷达点的距离图像表示的那些条目。这种信息冗余是最小的，因为距离图像的大小至多与发射的激光束的数量一样大。另一方面，原始点云表示是最有效的表示，因为它只对返回的激光雷达点进行编码。</p><p id="3ebc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">深度图像表示的主要缺点是自然图像普遍存在的尺度模糊和遮挡。</strong>比例模糊问题源于这样一个事实，即距离激光雷达传感器较近的物体在距离图像中比距离较远的物体显得更大。这使得检测网络很难在不同距离图像上以不同尺寸出现的一类物体之间进行归纳。此外，比例模糊增加了3d盒的尺寸推断的复杂性。<strong class="jp ir">理论上，人们可能会认为使用距离图像预测盒子尺寸应该是简单明了的，因为每个像素的深度都可以作为距离信息。然而，即使对象检测网络可以访问像素深度，卷积层也没有被设计成明确地使用该深度信息来消除比例模糊。特别地，CNN对象检测网络将深度图像的深度信息视为特征通道。</strong></p><p id="7f5a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">距离图像也会受到遮挡的影响。通常，遮挡降低了CNN目标检测网络的性能，主要是因为卷积滤波器的矩形结构。<strong class="jp ir">卷积滤波器的矩形结构允许信息从遮挡物体泄漏到将用于检测其他物体的特征图条目中。</strong></p><p id="fd17" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在使用全卷积网络的3D激光雷达的<a class="ae lp" href="https://arxiv.org/abs/1608.07916" rel="noopener ugc nofollow" target="_blank">车辆检测中，提出了基于距离图像表示的检测网络</a>。如下图所示，该网络将点云的距离图像表示作为输入，由左上角的点地图表示。然后，它通过将输入的距离图像通过主干网络进行处理。最后，由主干网络生成的特征图被分类和回归头使用。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nc"><img src="../Images/2cd0c2910f5b3f84b04d2979f067fbd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qQdXtMv_ziPJauRWS43irg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://arxiv.org/abs/1608.07916" rel="noopener ugc nofollow" target="_blank">使用全卷积网络从3D激光雷达检测车辆</a></p></figure><h1 id="a8db" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">3d体素化网格表示</h1><p id="75fa" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">点云的另一种网格表示形式是3d体素化，通过量化激光雷达传感器周围的3d </strong> <a class="ae lp" href="https://en.wikipedia.org/wiki/Cuboid" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">长方体</strong> </a> <strong class="jp ir">子空间来实现。选择3d长方体是因为它们的几何形状与张量的立方体形状兼容。为了实现3d体素化，首先，对于给定的lidar传感器，根据lidar传感器的范围以及给定应用的目标范围，选择围绕lidar传感器的3d </strong> <a class="ae lp" href="https://en.wikipedia.org/wiki/Cuboid" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">长方体</strong> </a> <strong class="jp ir">。</strong>激光雷达传感器范围的含义是避免激光雷达超范围空间的量化，以便节省存储器和计算。特别地，沿着激光雷达坐标框架的x和y轴的3d立方体的尺寸主要根据激光雷达传感器的范围来选择。例如，假设威力登HDL-64E的范围是120米，假设激光雷达传感器在立方体的中心，则其相应的3d立方体体素化的x和y维度不应大于240米。</p><p id="85dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，影响3d长方体尺寸的另一个因素是3d对象检测网络所设计的应用的目标范围。例如，在自动驾驶应用的情况下，高度大于激光雷达传感器(安装在自动驾驶汽车车顶上的激光雷达传感器)高度2米的子空间并不重要，因为自动驾驶汽车路径的规划是在激光雷达坐标系的x-y平面中执行的，并且在我们发明飞行自动驾驶汽车之前，自动驾驶汽车不会沿着激光雷达坐标系的z轴移动:)。在激光雷达坐标系中，3d长方体的z尺寸的典型范围是从-2.5米到1.5米，其中考虑到激光雷达传感器安装在自动驾驶汽车的车顶上，并且需要对地面上的物体进行检测，因此选择了-2.5米的较低范围。</p><p id="6c6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在选择围绕激光雷达传感器的3d立方体子空间的尺寸之后，下一步是量化3d立方体，以便将它们转换成张量，用作3d对象检测网络的输入。下图说明了这种量化过程，其中3d立方体在x、y和z维度上被量化。每个量化的子立方体称为一个体素单元。量化分辨率通常在三个维度上是相同的。这种分辨率的一个<a class="ae lp" href="https://arxiv.org/abs/1902.06326" rel="noopener ugc nofollow" target="_blank">典型值是0.1米</a>，这导致每个体素单元是一个0.1米×0.1米×0.1米的立方体。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/246fa3ec5626322e0680050232898ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*gvVlQByXcxGsRRM-7q-7aw.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">3d体素化。</p></figure><p id="d375" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">量化后的最后一步是为每个体素单元选择一个表示。</strong>最常见的表示形式是给每个体素单元分配一个二进制值，如果体素单元包含至少一个激光雷达点，则分配值为1，否则为0。这种表示将把每个场景转换成3d张量。体素细胞的另一类表示是特征向量，其可以是手工设计的特征或机器学习的特征。为体素单元选择特征向量表示导致场景被编码为4d张量。</p><p id="e592" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3d体素化表示的主要优点如下。(1)因为3d体素化表示将原始点云转换成结构化网格表示，所以它们可以直接用作CNN对象检测网络的输入。这将允许3d体素化表示方法受益于CNN对象检测网络的最新进展。(2)与距离图像表示不同，3d体素化表示不会遭受比例模糊和遮挡，因为它们不将激光雷达点投影到2d视点平面上。</p><p id="84f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，这种表示法的主要缺点是量化误差以及计算和存储效率低。 3d体素化表示是一种类似于任何其他量化方法的有损变换。具体而言，给定体素单元内的所有激光雷达点将由标量或固定大小的矢量表示，该矢量可能不传达体素单元激光雷达点中编码的所有信息。<strong class="jp ir">此外，3d体素化生成的张量大小比原始点云表示大几个数量级。</strong>例如，在KITTI数据集中，每个点云场景平均包含100k个激光雷达点，而其对应的具有80m×80m×3m尺寸的立方体和0.1m量化分辨率的3d体素化表示将是大小为800x800x30的3d张量，其具有大约19M个条目，比原点云表示大约大200倍。这种表示的计算和存储效率低下是由于这些体素单元的大部分(97%)是空的，这消耗了存储器并将被卷积滤波器处理。</p><p id="7da2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">与3d体素化表示相关联的额外计算成本的另一个来源是在3d对象检测网络中使用3d卷积层，3d对象检测网络将这些表示作为输入。</strong>下图显示了一个3d 3x3x3卷积滤波器。类似于依赖于x-y平面中局部邻域的空间相关性的沿x和y轴的2d卷积运算，这种局部邻域相关性也存在于3d体素化表示的z维中，这证明了沿所有x、y和z维使用3d卷积的合理性。尽管事实上3d卷积是3d体素化表示的自然选择，但是与2d卷积相比，3d卷积的问题在于其额外的计算成本以及比2d卷积更多的参数。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/58edebf35d6dc795fc31133e54d9c350.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*nTh9BNgMrO9rr5ooU8qmlw.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">3d卷积。</p></figure><h1 id="6f7b" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">2d体素化网格表示</h1><p id="b8f0" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir"> 2d体素化网格表示类似于3d体素化网格表示，主要区别在于，在基于传感器的范围和目标范围选择围绕激光雷达传感器的3d立方体子空间之后，我们仅沿着3d立方体的x和y轴而不是z轴对其进行体素化</strong>。下图显示了这样的2d体素化过程。作为2d体素化的结果，立方体子空间被分割成x-y平面中的2d体素单元。请注意，这些2d体素单元实际上是跨越立方体子空间的高度维度的高3d体素单元，并且被称为2d体素单元仅仅是因为在2d x-y平面上执行的量化过程。</p><p id="e140" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些2d体素单元的大部分是空的，这使得2d体素化的计算和存储效率低于原始的原始点云表示。标准做法是用固定大小的特征向量来表示每个2d体素单元。这样做从2d体素化过程得到的张量将是3d张量。量化误差是2d体素化的另一个问题，它是通过固定大小的特征向量来表示2d体素单元内的点云的预期结果。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/7fdcaa5eb81333b5a9f74495498acd25.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*GAGUJX07dYWLipCZA3tTLg.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">二维体素化。</p></figure><p id="b38c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似于3d体素化的2d体素化是将由CNN对象检测网络处理的结构化网格形式的自然表示。<strong class="jp ir">处理2d体素化网格的CNN对象检测网络比那些将3d体素化网格作为输入的检测网络在计算上更高效。</strong>是因为依赖2d卷积层而不是检测网络处理3d体素化网格所使用的3d卷积层。另一方面，类似于3d体素化，2d体素化表示不会遭受尺度模糊和遮挡。</p><p id="8fa2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个2d体素单元被称为柱，其是与3d立方体体素表示具有相同高度的体素单元。<strong class="jp ir">与对应于2d体素化表示的最终3d张量中的每个支柱相关联的表示是固定大小的向量，其可以由手工设计的支柱编码器或机器学习的支柱编码器生成。</strong>在接下来的章节中，我们将提供手工设计和机器学习的特征向量的优缺点。</p><h1 id="b493" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">柱式编码器</h1><p id="be32" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">柱子编码器是映射，其获取给定柱子内部的激光雷达点，并生成对应于柱子的固定大小的特征向量。由柱子编码器生成的柱子特征向量形成点云的3d张量网格表示。下图显示了一个带有相关激光雷达点的柱子。请注意，与每个矿柱相关联的激光雷达点的数量不是固定的，而是因矿柱而异。<strong class="jp ir">柱子编码器必须具有输入排列不变性，因为改变柱子内激光雷达点的顺序不会影响这些激光雷达点所代表的几何形状和语义类别。</strong></p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/7bc828bf2bc21689c075cf88ff0d7dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*Nl63WlZQhSTd7FlUhTKg7w.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">标有紫色点的柱子及其关联的激光雷达点。</p></figure><p id="f0db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">支柱编码器的两个类别是手工设计的和机器学习的编码器，这将在下面的章节中进一步讨论。</p><h2 id="dbdc" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">手工设计的柱式编码器</h2><p id="f0a4" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">手工设计的柱子编码器是由实践者和研究人员设计的输入方式排列不变性编码器，并且不是从数据中学习的，以便表示柱子的激光雷达点的统计。</strong>手工设计的支柱编码器的设计需要专家的工程努力，并不保证能产生支柱的最佳表现。依赖手工设计的编码器的另一个挑战是，它们可能无法在不同的数据集和任务之间转移。这意味着切换到新的数据集将需要全新的编码器，这需要额外的工程努力。</p><p id="834d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图所示的<a class="ae lp" href="https://arxiv.org/abs/1902.06326" rel="noopener ugc nofollow" target="_blank"> PIXOR </a>是一个带有手工设计的柱状编码器的著名激光雷达3d物体探测网络。该网络依赖于具有x = [0，70]，y = [-40，40]和z = [-2.5，1]的尺寸(以米为单位)的3d立方体的2d体素化，量化分辨率为0.1m。柱特征向量的尺寸为38，其转化为对应于800×700×38的2d体素化表示的3d张量的尺寸。首先，这个3d张量表示由残余块形成的主干网络处理。然后，由该主干网络生成的特征地图被特征金字塔网络扩充。具有200×175×96尺寸的特征金字塔网络的最终特征地图输出将被传递到分类和回归头。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ne"><img src="../Images/64ba34c1227edabaa8d429fecd7a9f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2GSZPOeLe6QlZext1F72Fw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://arxiv.org/abs/1902.06326" rel="noopener ugc nofollow" target="_blank"> PIXOR </a>。注意输入张量36的z维不正确，应该是38。</p></figure><p id="4daa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们把重点放在<a class="ae lp" href="https://arxiv.org/abs/1902.06326" rel="noopener ugc nofollow" target="_blank"> PIXOR </a>手工设计的立柱编码器上。PIXOR柱子编码器是输入方式置换不变性映射，其将柱子的lidar点变换为具有维度38的固定大小的特征向量。<strong class="jp ir">这些特征向量中的大多数条目编码了在不同高度的柱子上的激光雷达点的存在。</strong>具体来说，每个支柱沿z轴被分割成高度为0.1米的分段。因此，假设每个支柱的高度为3.5米，则每个支柱将有35个这样的分段。每个线段都表示为一个二元变量，如果线段中至少存在一个激光雷达点，则该变量的值为1，否则为0。除了这35个与高度相关的特征之外，另一个特征条目被设置为等于矿柱中激光雷达点的平均强度。此外，还有两个特征条目，它们表示与沿z轴的每个支柱相对应的越界点。如果在其x-y边界内的支柱上方(下方)至少存在一个激光雷达点，则顶部(底部)超出范围要素条目将设置为1，否则设置为0。</p><h2 id="2f43" class="lq km iq bd kn lr ls dn kr lt lu dp kv jy lv lw kz kc lx ly ld kg lz ma lh mb bi translated">机器学习支柱编码器</h2><p id="2803" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated"><strong class="jp ir">机器学习支柱编码器是以端到端的方式学习的，作为激光雷达3d对象检测网络的一部分，依赖于标记的训练数据。</strong>机器学习的支柱编码器必须具有输入排列不变性，因此类似于已经讨论过的<a class="ae lp" href="https://arxiv.org/abs/1612.00593" rel="noopener ugc nofollow" target="_blank">点网</a>架构。机器学习的立柱编码器相对于手工设计的立柱编码器的优势有两方面，如下所述。(1) <strong class="jp ir">机器学习的支柱编码器映射作为3d对象检测网络的一部分直接从数据中学习，它们表示针对3d对象检测网络的回归和分类任务精确定制的特征。</strong> (2)改变训练数据和目标任务不需要额外的工程努力来重新设计支柱特征向量，因为支柱特征编码器是以自动方式从数据中学习的。</p><p id="cc89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lp" href="https://arxiv.org/abs/1812.05784" rel="noopener ugc nofollow" target="_blank"> PointPillars </a>是机器学习支柱编码器中的一个突出模型，如下图所示。机器学习的柱子编码器(简称柱子特征网)是<a class="ae lp" href="https://arxiv.org/abs/1612.00593" rel="noopener ugc nofollow" target="_blank">点网</a>的简化版本，其中点态函数<em class="lo"> h </em>是一个64个神经元的全连接层，后面是BatchNorm和ReLU。此外，在这个类似PointNet的结构中，对称函数<em class="lo"> g </em>是一个基于元素的max-pooling层。首先，映射<em class="lo"> h </em>被单独应用于给定支柱中的每个激光雷达点，这为每个激光雷达点生成维度64的嵌入。然后，基于元素的最大池层<em class="lo"> g </em>获取矿柱的所有激光雷达点的嵌入，并生成维数为64的单个矿柱特征向量。基于2d体素化和所描述的柱编码生成的3d张量被传递到3d对象检测网络的主干网络。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nf"><img src="../Images/00876e96f34219fbb1d1011d4e2d9cb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QHxIxKvpKgkd_8mRGePmxg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae lp" href="https://arxiv.org/abs/1812.05784" rel="noopener ugc nofollow" target="_blank">点柱</a>。</p></figure></div></div>    
</body>
</html>