<html>
<head>
<title>NLP: From Watermelon Boxes to Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理:从西瓜盒子到单词嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a?source=collection_archive---------37-----------------------#2020-04-23">https://towardsdatascience.com/natural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a?source=collection_archive---------37-----------------------#2020-04-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9119" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用单词嵌入的自然语言处理的强大发展</h2></div><p id="54cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">众所周知，人类不擅长随机应变。不管我们是否意识到这一点，我们做的每件事都有一定的模式。</p><p id="a9fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们用来交流的语言也不例外。所有的语言都遵循某种被称为句法的规则模式。</p><p id="f49b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是这有什么关系呢？</p><p id="3dec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">深度学习行业的很大一部分专注于一种名为<strong class="kk iu">的自然语言处理(NLP) <em class="le">的东西。</em> </strong> NLP 是人工智能的一个子领域，专注于训练计算机处理人类语言。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/63e2f88c5b48f1ab1788eccfafb66497.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*36AkFCTR-hbclQGa.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">信用:<a class="ae lv" href="https://medium.com/dair-ai/deep-learning-for-nlp-an-overview-of-recent-trends-d0d8f40a776d" rel="noopener">猫王</a></p></figure><p id="a0ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自然语言处理始于 20 世纪 50 年代，当时哈佛的一名研究人员创造了一种叫做“西瓜盒”的机器。</p><p id="d417" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它做了一件简单的事情…当一个人说“西瓜”这个词时，它可以探测到。</p><p id="5b24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">创造它的人认为 NLP 的未来是创造数千个盒子，每个盒子检测一个单词…</p><p id="a982" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这在现在听起来很可笑，但在当时却是一项非常重要的发明。</p><p id="bf3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从那以后，我们走过了漫长的道路，西瓜盒子现在已经被遗忘了很久。事实上，如果没有数以千计的方形西瓜出现，很难在网上找到任何关于它们的信息:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/7f73ff191d06690717d86d842a5dad58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*FlkDCkrwiC2Q9fdN-3X-rw.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">信用:<a class="ae lv" href="http://www.melonmold.com/product/square-watermelon-box" rel="noopener ugc nofollow" target="_blank">melonmold.com</a></p></figure><p id="65d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，NLP 到处都在用。无论你使用的是谷歌翻译、Siri，还是语法——都有一个人工智能在后台使用 NLP 来理解你用什么语言交流。</p><p id="3d36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以最大的问题是:<strong class="kk iu">神经网络是如何变得如此擅长理解人类语言的？</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lx"><img src="../Images/5f37cd6a1624ebb3a07594a3b75dc264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QaKqILfr1GP8tSF_EBsXEA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">信用:<a class="ae lv" href="https://www.igeeksblog.com/how-to-activate-siri-on-iphone-x/" rel="noopener ugc nofollow" target="_blank">igeeksblog.com</a></p></figure><p id="e780" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从经典的全连接神经网络如何处理这个问题开始。</p><blockquote class="ly lz ma"><p id="9751" class="ki kj le kk b kl km ju kn ko kp jx kq mb ks kt ku mc kw kx ky md la lb lc ld im bi translated">如果你想了解神经网络是如何工作的，可以看看<a class="ae lv" href="https://medium.com/datadriveninvestor/the-basics-of-neural-networks-304364b712dc" rel="noopener"> <strong class="kk iu"> <em class="it">这篇文章</em> </strong> </a> <strong class="kk iu"> <em class="it">。</em>T29】</strong></p></blockquote><p id="5332" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，神经网络只能接受数字作为输入。所以你不能只给它一堆文本来处理。你得把它转换成一个数值。</p><p id="5dab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最初解决这个问题的方法是通过索引。基本上每个单词都会用它自己唯一的数字来表示。这个数字然后被一键编码。</p><p id="523f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这相当于给一个对化学一无所知的人一个基本周期表的单个单元格。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi me"><img src="../Images/199f8b79cae8b394caf7520ada7f8507.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uhuqjd96oK8enMo9JR-K-g.jpeg"/></div></div></figure><p id="f116" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就像桌子上的每个元素都有唯一的原子序数一样，神经网络看到的每个单词都有唯一的索引。</p><p id="1e83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">元素在周期表上的位置提供了很多信息。很明显，只有表格中的一个单元格，您会丢失那些有价值的信息。</p><p id="6e11" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，<strong class="kk iu">简单的全连接神经网络的问题是，人工智能没有这个词的上下文。</strong></p><p id="0b81" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它不知道在它所查看的单词之前出现了什么单词，所以你不能指望它用那个单词做任何有价值的事情。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="b35c" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">递归神经网络</h1><p id="eeb3" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">NLP 的很多改进都是从递归神经网络(RNNs)开始的。它们非常擅长处理一系列数据(比如句子)，因为它们使用内存来使用上下文。</p><blockquote class="ly lz ma"><p id="920f" class="ki kj le kk b kl km ju kn ko kp jx kq mb ks kt ku mc kw kx ky md la lb lc ld im bi translated">你可以在这里了解递归神经网络如何工作<a class="ae lv" href="https://blog.goodaudience.com/writing-fairy-tales-using-ai-a64d0ecb1ddc" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"><em class="it"/></strong></a><strong class="kk iu"><em class="it">。</em>T13】</strong></p></blockquote><p id="7304" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用我们的化学类比，这和给你整个周期表是一样的。现在，您有更好的机会根据元素所在的行和列来理解趋势。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nj"><img src="../Images/c0bcbdc3a126cdfe23503f32bb89d60e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DXOR5u18x-L4mGnof51XnA.jpeg"/></div></div></figure><p id="84df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，RNN 可以使用它正在分析的词周围的词来理解上下文并做出更好的预测。</p><p id="cebd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RNNs 是一个重大的进步，可以更好地生成和分析文本。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="5fcf" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">Word2Vec</h1><p id="dbc7" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">从那以后，NLP 有了更多的进展，出现了各种关于新的训练优化器、网络架构等的研究论文。</p><p id="216a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是在所有这些中，<strong class="kk iu">NLP 最大的胜利之一是使用了像 Word2Vec 这样的嵌入层。</strong></p><p id="5009" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用 RNNs，算法可以识别单词的上下文，但它不会真正理解每个单词。<strong class="kk iu">它只是看到一个随机的索引号。</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3b5ceedb9b82683b6e21aedb93d4bdb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*UbR3sY_-vGt67-dCaKEOtA.jpeg"/></div></figure><p id="f697" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经网络遗漏了关于每个单词的如此多的信息。</p><p id="5758" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">任何语言中的每个单词都有几个我们在理解该语言时下意识分析的属性。一个词可以是复数，可以是名词，可以有特定的动词时态，等等。</p><p id="57b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Word2Vec 确实如其名。<strong class="kk iu">它获取一个单词，并将其转换为向量表示:</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/e9033d885dd9cb599b8a8786659ee9c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*hu-HAuQL3HwJo7WREYcSIg.jpeg"/></div></figure><p id="e7e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将忠于我的周期表类比，展示使用 Word2Vec 这样的嵌入层处理 Word 的整个过程。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nl"><img src="../Images/b9dd07dafec03b2d1126757f8e2fcac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bzd05RSbEfbMA4yPkMhYCg.jpeg"/></div></div></figure><p id="0e9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">索引氦给出了它独特的数字表示:它的原子序数。</p><p id="677f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将这个数字传递给经过训练的 Word2Vec 层，我们就可以得到氦的矢量表示。这将是原子量，电子配置，物理状态等。</p><p id="66f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一个我创建的可视化，用来总结 Word2Vec 和 RNNs 对自然语言处理的影响的周期表类比:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nm"><img src="../Images/e0070bd79690c2ef8707726243853e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7IiSxjMP41mjGEN-3WfXUg.png"/></div></div></figure><p id="34eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过嵌入 Word2Vec 这样的层产生的矢量中最酷的部分之一是<strong class="kk iu">你可以可视化和量化单词之间的关系。</strong></p><p id="4822" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嵌入向量的维数取决于嵌入的大小(由程序员决定)。</p><p id="deef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当任何单词被转换成向量时，它的维数等于嵌入的大小。</p><p id="c5a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，您可以在图表上绘制这些向量，并可视化各种单词之间的关系。</p><p id="a4fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们以嵌入大小为 3 为例。它将生成 3D 矢量，我们可以在笛卡尔平面上绘制这些矢量:</p><p id="7c83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">在实践中，嵌入层比这大得多，但是你不能可视化超过 3 维的空间。</em></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nn"><img src="../Images/803a4afc356edf973eb96a646b7a54b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mctC4BYry9EnM4c3IH_57A.jpeg"/></div></div></figure><p id="fbaf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以立即看到这些向量位置的一些趋势。<strong class="kk iu">相似的词在向量空间中位置相近。</strong>我对矢量头部进行了颜色编码，以阐明示例中的聚类:</p><blockquote class="ly lz ma"><p id="7d24" class="ki kj le kk b kl km ju kn ko kp jx kq mb ks kt ku mc kw kx ky md la lb lc ld im bi translated"><strong class="kk iu">红色→ </strong>地方</p><p id="f83c" class="ki kj le kk b kl km ju kn ko kp jx kq mb ks kt ku mc kw kx ky md la lb lc ld im bi translated"><strong class="kk iu">橙色→ </strong>动词</p><p id="e39c" class="ki kj le kk b kl km ju kn ko kp jx kq mb ks kt ku mc kw kx ky md la lb lc ld im bi translated"><strong class="kk iu">黄色→ </strong>人</p></blockquote><p id="7317" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更有趣的是，<strong class="kk iu">单词之间精确的矢量转换背后也有意义。</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nn"><img src="../Images/eb5323f8111968464ca751059c0a663b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3DKEy9uWLbqesM7tlH8HPQ.jpeg"/></div></div></figure><p id="5f64" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">每个箭头颜色都是某种矢量变换，代表单词之间的关系。</em></p><p id="d8d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是本例中矢量变换及其显示的关系的摘要:</p><blockquote class="ly lz ma"><p id="0899" class="ki kj le kk b kl km ju kn ko kp jx kq mb ks kt ku mc kw kx ky md la lb lc ld im bi translated"><strong class="kk iu">绿色:</strong>国家→首都</p><p id="a33f" class="ki kj le kk b kl km ju kn ko kp jx kq mb ks kt ku mc kw kx ky md la lb lc ld im bi translated"><strong class="kk iu">粉色:</strong>不定式→过去式</p><p id="e415" class="ki kj le kk b kl km ju kn ko kp jx kq mb ks kt ku mc kw kx ky md la lb lc ld im bi translated">蓝色:不定式→现在式</p><p id="4859" class="ki kj le kk b kl km ju kn ko kp jx kq mb ks kt ku mc kw kx ky md la lb lc ld im bi translated"><strong class="kk iu">紫色:</strong>男性→女性</p><p id="165d" class="ki kj le kk b kl km ju kn ko kp jx kq mb ks kt ku mc kw kx ky md la lb lc ld im bi translated"><strong class="kk iu">棕色:</strong>人→皇室</p></blockquote><h2 id="4973" class="no mn it bd mo np nq dn ms nr ns dp mw kr nt nu my kv nv nw na kz nx ny nc nz bi translated">Word2Vec 向量是如何计算的</h2><p id="47e3" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">Word2Vec 通过学习从训练句子中预测单词来嵌入单词。</p><p id="b337" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个想法是，神经网络将学习为以类似方式使用的单词分配类似的权重。</p><p id="4f25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">拿下面这句话来说:</p><blockquote class="oa"><p id="5324" class="ob oc it bd od oe of og oh oi oj ld dk translated">The ____ 正在跳跃。</p></blockquote><p id="f3a2" class="pw-post-body-paragraph ki kj it kk b kl ok ju kn ko ol jx kq kr om kt ku kv on kx ky kz oo lb lc ld im bi translated">空白单词可以是任何名词，如青蛙、人、狗等。</p><p id="514e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，相似的单词将具有相似的权重值，将它们连接到嵌入层中的每个节点。</p><p id="cc16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以用余弦距离来衡量单词之间的相似度。相关单词将具有小的余弦距离。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi op"><img src="../Images/15fde9f00cb3bb24507e27c3017f5306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KKEexAlgIsnaozl5.png"/></div></div></figure><p id="2cd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于单词是一次性编码的，嵌入层的权重变成了一个查找表，我们可以在其中找到该单词的矢量表示。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oq"><img src="../Images/8754e3c790ac311d7392313ac0f00541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*-kNmlTRG47Bmv5CFY2YunA.gif"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">信用:<a class="ae lv" href="https://www.udacity.com/" rel="noopener ugc nofollow" target="_blank"> Udacity </a></p></figure><p id="46a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就像一本字典，你提供你想要嵌入的单词的索引和它在权重矩阵中相应行的外观。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="08dc" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">Word2Vec 的类型</h1><p id="22c6" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">即使在 Word2Vec 中，也有多种类型:连续单词包(CBOW)和 Skip-gram。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi or"><img src="../Images/34d7bfc5d75fa1eb85833ea9f90adc66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XPBpxReUA4LaUky9.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">信用:<a class="ae lv" href="https://machinelearningmastery.com/what-are-word-embeddings/" rel="noopener ugc nofollow" target="_blank">machinelearningmastery.com</a></p></figure><p id="da70" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 CBOW 中，你取一个句子中某个窗口的单词，试着填空。<strong class="kk iu"> CBOW 获取上下文并生成单词。</strong></p><p id="8062" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Skip-gram 做的正好相反——它取一个单词，然后猜测它周围出现的单词。<strong class="kk iu"> Skip-gram 获取一个单词并生成其上下文。</strong></p><p id="8972" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直觉上，你会认为 CBOW 比 Skip-gram 更容易实现，因为两者之间的比率给了你更多的信息，而你需要猜测的更少。</p><p id="439c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有趣的是 Skip-gram 是实现 Word2Vec 的首选方式。只是看起来效果更好。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="662a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除了 Word2Vec，在嵌入层方面还有一些更有趣的进步。</p><p id="4175" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一种称为 Doc2Vec 的嵌入类型创建整个段落的矢量表示，顾名思义就是文档。</p><h2 id="a175" class="no mn it bd mo np nq dn ms nr ns dp mw kr nt nu my kv nv nw na kz nx ny nc nz bi translated">原帖:</h2><div class="os ot gp gr ou ov"><a href="https://mayankjain.ca/blog-posts/17" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">Mayank Jain - NLP:从西瓜盒子到单词嵌入</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">众所周知，人类不擅长随机应变。不管我们是否意识到，我们所做的一切都倾向于…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">mayankjain.ca</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj lp ov"/></div></div></a></div><h1 id="dfae" class="mm mn it bd mo mp pk mr ms mt pl mv mw jz pm ka my kc pn kd na kf po kg nc nd bi translated">如果你喜欢这篇文章:</h1><ul class=""><li id="afa0" class="pp pq it kk b kl ne ko nf kr pr kv ps kz pt ld pu pv pw px bi translated">在<a class="ae lv" href="https://medium.com/@mayankj2112" rel="noopener"> Medium </a>上关注我，在<a class="ae lv" href="https://www.linkedin.com/in/mayankj2112/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系，了解我的最新工作进展！</li><li id="d78f" class="pp pq it kk b kl py ko pz kr qa kv qb kz qc ld pu pv pw px bi translated">订阅我的时事通讯，跟随我的旅程！：</li></ul><div class="os ot gp gr ou ov"><a href="https://www.subscribepage.com/mayank" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">注册我的每月更新！</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">注册以跟上我每月的进度，包括项目、会议、文章等等...</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">www.subscribepage.com</p></div></div><div class="pe l"><div class="qd l pg ph pi pe pj lp ov"/></div></div></a></div></div></div>    
</body>
</html>