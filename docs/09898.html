<html>
<head>
<title>But what is Entropy?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">但是熵是什么呢？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/but-what-is-entropy-ae9b2e7c2137?source=collection_archive---------33-----------------------#2020-07-13">https://towardsdatascience.com/but-what-is-entropy-ae9b2e7c2137?source=collection_archive---------33-----------------------#2020-07-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/8bbce6e0fe403efb00fb93b0f029d28d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sTYLDRDOG7iSj_3KkKVONg.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:https://pixabay.com/users/tumisu-148124/</p></figure><p id="6f76" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章从不同的角度重新介绍了熵的概念，重点是它在机器学习、概率编程和信息论中的重要性。</p><p id="7a32" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是字典根据谷歌快速搜索对它的定义-</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/e0b498ecd6346dca5bd2fa0b44c7575c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tODkepcMxDWPZAYhMQLyKw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:谷歌搜索截图</p></figure><p id="4633" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基于这个结果，你可以注意到这里有<strong class="ki iu">两个核心想法</strong>，起初，它们之间的相关性似乎不是很明显-</p><ul class=""><li id="a48a" class="lj lk it ki b kj kk kn ko kr ll kv lm kz ln ld lo lp lq lr bi translated">熵是热力学中做功所缺少的(或需要的)能量</li><li id="e09a" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">熵是无序或随机(不确定性)的量度</li></ul><p id="47a4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么到底是什么——能量缺失，还是一种度量，或者两者都有？让我提供一些观点，希望能帮助你理解这些定义。</p><h1 id="f803" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">倒楣的事情发生了！</h1><p id="5954" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">把这个令人讨厌的标题换成更容易接受的说法</p><blockquote class="na nb nc"><p id="6113" class="kg kh nd ki b kj kk kl km kn ko kp kq ne ks kt ku nf kw kx ky ng la lb lc ld im bi translated"><strong class="ki iu">任何可能出错的事情都会出错——墨菲定律</strong></p></blockquote><p id="a520" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们都已经接受了这条定律，因为我们一直在观察和体验这条定律，而这背后的罪魁祸首正是这篇文章的主题——是的，你说对了，这就是<strong class="ki iu">熵！</strong></p><p id="dc90" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以现在我让你们更加困惑了——熵不仅仅是缺失的能量和无序的量度，它也是无序的原因。太好了！</p><p id="ee2d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就定义而言，我们在这里下不了决心。然而，事实是，在适当的背景下，上述三种观点都是正确的。为了理解这些背景，让我们先看看无序及其与熵的关系。</p><h1 id="3a3c" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">无序是主导力量</h1><p id="6102" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">我借助 James Clear(《原子习惯》的作者)的一篇文章中的例子来解释这一点。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/ed757c9d2ebdaaf8a19a64a4d4015c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uivJWaibO0VuzpEUS3Ffpg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:左图(<a class="ae kf" href="https://pixabay.com/illustrations/puzzle-puzzle-piece-puzzles-3303412/" rel="noopener ugc nofollow" target="_blank">https://pix abay . com/illustrations/puzzle-puzzle-piece-puzzles-3303412/</a>)右图(图片由<a class="ae kf" href="https://unsplash.com/@picsbyjameslee?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">李中清</a>在<a class="ae kf" href="https://unsplash.com/s/photos/sand-castle?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄)+作者注明</p></figure><p id="3f18" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">理论上，这两种情况都有可能发生，但发生的几率微乎其微。好吧，好吧，称之为不可能🤐！。这里的主要信息如下:</p><blockquote class="na nb nc"><p id="1498" class="kg kh nd ki b kj kk kl km kn ko kp kq ne ks kt ku nf kw kx ky ng la lb lc ld im bi translated"><strong class="ki iu">无序的变化永远比有序的多得多！</strong></p></blockquote><p id="e989" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">借用伟大的史蒂芬·平克的智慧:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/23920853c61db947d7555d65526a2c5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QBZRgFpRVNYpVB0JvdeyEQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><strong class="bd nj">史蒂芬·平克— </strong>哈佛大学心理学系约翰斯通家族教授(图片由作者创建)</p></figure><p id="2a59" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">我们如何击退熵的浪潮？</strong></p><p id="3843" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">付出必要的努力。努力意味着我们应该花精力去对抗熵产生的无序。</p><p id="b7c1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在热力学中，熵的观点占主导地位，也就是说，你将必要的(缺失的)能量注入你的系统，使其达到平衡。</p><p id="c547" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于这是一本机器学习杂志，我不会在这方面(即统计热力学)停留太久，而是提供了一篇精彩文章的链接，这篇文章以一种我永远无法解释的方式进行了解释。</p><div class="nk nl gp gr nm nn"><a href="https://plus.maths.org/content/satanic-science" rel="noopener  ugc nofollow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">撒旦科学</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">在伦敦北部这个悲惨的二月天，当我打开暖气时，我非常感激一个基本的自然事实…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">plus.maths.org</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob jz nn"/></div></div></a></div><p id="3198" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你要注意的是，作为无序度量度的熵的公式是什么样子的，就像它在玻尔兹曼的墓碑上显示的那样。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/4abbda48a7d002df13386a1fd8530062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bwgQ7Z9rq66N_mcNi8Q4Iw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:【https://www.atlasobscura.com/places/boltzmanns-grave<a class="ae kf" href="https://www.atlasobscura.com/places/boltzmanns-grave" rel="noopener ugc nofollow" target="_blank">+作者标注</a></p></figure><p id="0e0f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们从信息的角度探索熵，这是机器学习打算从数据中提取的内容。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="e3e9" class="lx ly it bd lz ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu bi translated">熵是信息的一种度量</h1><p id="db4a" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">如果你在思考——早先他说熵是无序或随机(不确定性)的量度，现在它已经演变成信息的量度——那么这意味着你在注意。干得好！:)</p><p id="73b1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">信息和不确定性确实是相互关联的。耐心点，你很快就会看到的。</p><p id="03d0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“信息”这个词意味着一个人没有的知识。如果你已经知道了，那就不是信息了！</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi op"><img src="../Images/ebc519ee56675be87baab51626dc3a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*ZbFpxVYiX47U4PvfS5vBPA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:谷歌搜索截图</p></figure><p id="7337" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在信息也可以被看作是一个“惊喜”,尽管你感到惊讶的程度会有所不同。例如，如果我告诉你明天太阳会升起，你会说——哼！毫不奇怪，也就是说，这份声明中没有任何信息，但如果我告诉你明天世界将会毁灭，你会非常惊讶(…至少非常悲伤，我希望！😩)</p><p id="51c4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从惊讶的角度思考的好处是，大多数时候我们对“信息”的思考是二元的——要么我有信息，要么没有，而“惊讶”有助于带来可变性程度的概念。你的惊讶与某个事件发生的概率(几率)成反比。事件越罕见，你越惊讶！概率是什么？….这是对不确定性的衡量！</p><p id="99df" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">借助亚里士多德的逻辑，你可以理解我们已经确立了信息(惊奇)和不确定性(概率)之间的关系。接下来，让我们试着用数学公式表示它。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oq"><img src="../Images/795f3a2ff9d01973a87df618d72702fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n8biylpN9aXfNgfBkqEQwA@2x.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:作者</p></figure><p id="9dd9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述数学公式有些奇怪。我不知道你怎么想，但我肯定不开心！</p><p id="46b7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我得到信息(让我吃惊..咄！)大概两个(😲&amp;🤔)<strong class="ki iu">独立的</strong>事件，总的惊喜将会是<strong class="ki iu">附加的</strong>(😲+🤔)在性质上与<strong class="ki iu">相乘</strong>(😲x🤔).</p><p id="376f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们解决这个问题。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi or"><img src="../Images/4ba3999d27956a580f0e8f2c7691dc8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OPgH-tWnzKoJ-nVENpVG8g.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:作者</p></figure><p id="43da" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，惊喜与事件概率(随机变量)的对数成反比。<strong class="ki iu">对数函数帮助我们把惊喜的附加方面带回家</strong>。换句话说，我们可以通过认识到乘法可以通过对相关概率的对数进行处理而被视为加法，来提高我们对计算惊喜函数的直觉——我们的常识会说，惊喜函数必须是事件概率的加法而不是乘积(乘法)。</p><p id="cde0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对我们来说，下一个问题是制定“从长远来看，我会有多惊讶？”</p><p id="1091" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们为什么对上面的问题感兴趣(这里的关键字是<strong class="ki iu">“长期”</strong>)是因为我们在这里的组合中引入了随机变量(因此也带来了不确定性)，因此我们需要测量我们的惊喜的<strong class="ki iu">中心趋势</strong>。</p><blockquote class="na nb nc"><p id="e589" class="kg kh nd ki b kj kk kl km kn ko kp kq ne ks kt ku nf kw kx ky ng la lb lc ld im bi translated">前一段时间我写了一篇文章叫做“<a class="ae kf" rel="noopener" target="_blank" href="/but-what-is-a-random-variable-4265d84cb7e5">但是什么是随机变量呢？</a>”。如果你对随机变量&amp;分布感到不舒服，不妨去看看。</p></blockquote><div class="nk nl gp gr nm nn"><a rel="noopener follow" target="_blank" href="/but-what-is-a-random-variable-4265d84cb7e5"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">但是什么是随机变量呢？</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">对什么是随机变量及其与概率论的联系的清晰而简单的解释。</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="os l ny nz oa nw ob jz nn"/></div></div></a></div><h2 id="7fd3" class="ot ly it bd lz ou ov dn md ow ox dp mh kr oy oz ml kv pa pb mp kz pc pd mt pe bi translated">随机变量的期望值(或中心趋势)</h2><p id="2bd0" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">当我们处理常规数字时，我们经常使用平均数、众数和中位数来统计这些数字。让我们在这里花一分钟的时间只关注均值。平均值是你的数字集中趋势的一个指标，当你计算它的时候，你给你所有的数字以相同的权重。数学很简单——取所有的数字，然后除以数字的个数。</p><p id="df3e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在随机变量的情况下，在给定的时刻，我们没有属于这个随机变量的所有数字(样本),更重要的是，随机变量都是关于公平的；贡献对他们很重要，因此与传统的数字平均不同，他们不会对所有的可能性给予同等的权重！</p><p id="2d53" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过考虑<strong class="ki iu">“长期平均值”</strong>(也称为随机变量的<strong class="ki iu">期望值)来解决上述挑战。</strong>这是你计算平均值的方法(抱歉..期望值)。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pf"><img src="../Images/bc7bce2e1181f28a174db9307ab9a82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQQ191q-O0Euavc_wRfINw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:作者</p></figure><p id="22dd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每一种可能性对中心趋势(期望值)的贡献(x_i)由等于其发生的可能性(概率)的权重给出。</p><h2 id="8a06" class="ot ly it bd lz ou ov dn md ow ox dp mh kr oy oz ml kv pa pb mp kz pc pd mt pe bi translated">惊喜的期望值(或集中趋势)</h2><p id="6030" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">如果你理解随机变量的期望值，你就不会对此感到惊讶(双关语😎！)</p><p id="03c0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">平均或预期惊喜也应该是所有惊喜的加权平均之和！</p><p id="312a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">运用我们之前学到的知识:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pg"><img src="../Images/ce70f09358adffb12984a4c488a8b82d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K3ESC42FHCrdH3LCMax9JA.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:作者</p></figure><p id="2dc3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你所看到的，在简化了上图中的各个步骤后，我们最终得到了预期的惊喜，看起来像熵的公式(来自热力学)。看我标注的玻尔兹曼墓碑的图像。我真的希望他的鬼魂不会因此困扰我！</p><p id="fd7b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">克劳德·香农在 1948 年的开创性论文《沟通的数学理论》中提出了信息(惊喜)期望值的公式</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/fc24b72d2ab176b23b2156ae53027e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*ujws3WfBSbCuDYubgOfKsw.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">克劳德·香农:信息论之父(图片来源:<a class="ae kf" href="https://en.wikipedia.org/wiki/Claude_Shannon" rel="noopener ugc nofollow" target="_blank">维基百科</a>)</p></figure><p id="3ccf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">克劳德·香农的探索是用尽可能少的比特数传输和接收信息。我之前借助惊奇概念推导出的公式，也可以用来告诉你一条消息的最小比特数。为此，您可以使用基数为 2 的对数。让我们也做几个例子。这些例子摘自《信息论的<a class="ae kf" href="https://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954" rel="noopener ugc nofollow" target="_blank">元素</a>一书，并使用张量流概率求解。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pi"><img src="../Images/4fd2cd8a361c8fc1f40a44deb630fa0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zKFfCveNH2oeS5GbM-F92A.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:作者</p></figure><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pj"><img src="../Images/c43e6376ca56e49dea4a5bc78f00aae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BgLVfD7LOHqUGJPbnyZcIQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:作者</p></figure><p id="4b8c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为什么在信息论的上下文中它被称为熵是因为度量(公式)看起来像热力学的公式。它被称为<strong class="ki iu">香农熵或信息熵和熵。真的只是熵！</strong></p><p id="b9e6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">香农的工作也为文件压缩奠定了基础，其核心思想是在不扭曲原始信息的情况下，用更少的比特来表示信息。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="61e5" class="lx ly it bd lz ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu bi translated">使用熵来选择概率分布</h1><p id="9ee9" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">首先，习惯性地提及贝叶斯定理。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pk"><img src="../Images/f65ee47c08ed9f0353e808e9cd079364.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eGWO45MpvsPhfDAwXcBOlw@2x.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:<a class="ae kf" href="https://en.wikipedia.org/wiki/Thomas_Bayes" rel="noopener ugc nofollow" target="_blank">图片</a>来自维基百科+作者标注</p></figure><p id="0b6e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你有没有想过应该如何选择先验分布和概率分布？许多统计通常是通过使用正态(高斯)分布来完成的，但通常常规选择并不是最佳选择。</p><blockquote class="na nb nc"><p id="6282" class="kg kh nd ki b kj kk kl km kn ko kp kq ne ks kt ku nf kw kx ky ng la lb lc ld im bi translated">无知比错误更好，不相信任何东西的人比相信错误的人更接近真理——托马斯·杰斐逊</p></blockquote><p id="7928" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">托马斯·杰斐逊的智慧可以成为我们做出这一选择的指导力量。把它转化成我们的问题域，看起来会像这样-</p><p id="4b88" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具有最大概率分布的分布是具有最大熵的分布。这也意味着这也是你能做出的信息量最少、最保守的选择。这是最大熵分布原理背后的核心思想。遵循它有助于在我们的计算中不引入任何额外的偏差或假设。</p><p id="5d07" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于无处不在的正态分布选择，一个经常被引用的理由是它遵循<strong class="ki iu">中心极限定理</strong>。最大熵原理也可以用来证明它的使用。可以解析地证明，如果分布的方差已知，那么正态分布就是具有最大熵的分布。如果你对证明的严谨性感兴趣，我会推荐你阅读这些笔记——<a class="ae kf" href="https://mtlsites.mit.edu/Courses/6.050/2003/notes/chapter10.pdf" rel="noopener ugc nofollow" target="_blank">https://MTL sites . MIT . edu/Courses/6.050/2003/notes/chapter 10 . pdf</a></p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="a54d" class="lx ly it bd lz ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu bi translated">相对熵(KL 散度)</h1><p id="bc24" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">到目前为止，我们一直使用熵作为概率分布信息含量的指标。我们应该问自己的下一个问题是— <strong class="ki iu">我们可以用它来比较两个分布吗？</strong></p><p id="a1ea" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">分布之间的比较可以被视为距离(在某个空间中),然而对于分布来说，该距离本质上是不对称的，即给定 p &amp; q 分布，从 p 到 q 的距离不会与从 q 到 p 的距离相同，当然，除非它们相同。在这种情况下，它是零。这就是为什么我们用另一个术语来描述这种不同，称之为差异。</p><p id="3018" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有几种不同类型的散度，最广泛使用和已知的是由 Solomon Kullback 和 Richard Leibler 创建的一种，用于测量分布之间的相对熵</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pl"><img src="../Images/95011c4513393f3bb1ac4a71de9d83b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KJX9jwCDX5e7CTHz4cZbQQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Solomon_Kullback" rel="noopener ugc nofollow" target="_blank">所罗门·库尔巴克</a> &amp; <a class="ae kf" href="https://en.wikipedia.org/wiki/Richard_Leibler" rel="noopener ugc nofollow" target="_blank">理查德·莱布勒</a>KL 发散的发明者(图片来自维基百科)</p></figure><p id="3450" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">很明显，熵在计算这种差异时扮演了一个角色，但你必须等一会儿才能看到它。</p><p id="f85c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们不想把 KL 散度的公式扔给你，我们想要发展一种关于它的起源的直觉，为此，我们从可能性的概念中得到帮助。</p><p id="c974" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设通过进行一项实验(例如投掷骰子数千次)，你观察到了真实的分布，我们称之为<strong class="ki iu"> p </strong>。现在假设我们有另一个候选分布(模型)叫做<strong class="ki iu"> q </strong>，它可能也适合于描述我们的随机变量。我们比较这两种分布(模型)的一种方法是通过查看它们的可能性比率。这是一种被称为<a class="ae kf" href="https://en.wikipedia.org/wiki/Likelihood-ratio_test" rel="noopener ugc nofollow" target="_blank">似然比测试</a>的假设测试形式(参见维基百科；解释得真好)。在实践中，我们取似然比的自然对数。</p><p id="8011" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用这个对数似然比，并使用在<strong class="ki iu"> p </strong>中出现的数据点对其进行加权，这将为我们提供两个分布之间的差异(距离)的期望值。是时候用数学方法来看待它了:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pm"><img src="../Images/ca38dd8168a44faba2dbe26fa2a1bce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GEB5GpMGKK-1cnU_y4to6w@2x.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:作者</p></figure><p id="7778" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你看到的，我们的熵公式再次出现，这就是为什么 KL 散度也叫做相对熵。</p><p id="dd5f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们也可以借助惊奇来理解 KL 发散。你本质上要问的问题是—</p><blockquote class="na nb nc"><p id="a072" class="kg kh nd ki b kj kk kl km kn ko kp kq ne ks kt ku nf kw kx ky ng la lb lc ld im bi translated">对于给定的随机变量 X，如果你选择分布<strong class="ki iu"> q </strong>而不是参考分布<strong class="ki iu"> p </strong>惊喜的相对变化是多少？</p></blockquote><h2 id="1ed3" class="ot ly it bd lz ou ov dn md ow ox dp mh kr oy oz ml kv pa pb mp kz pc pd mt pe bi translated"><strong class="ak">使用参考分布近似分布</strong></h2><p id="9ac2" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">在机器学习中，KL 散度应用的一个主要领域是在进行变分推理时将其用作损失函数。</p><p id="9743" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">变分推断包括使用另一个(联合)分布来近似一个难以处理的后验分布(来自贝叶斯定理),我们有一个可用的分析形式。随着你的神经网络学习最小化 KL 发散损失，你的参考(p)和目标(q)分布看到相对熵的减少。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="3bba" class="lx ly it bd lz ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu bi translated">交叉熵</h1><p id="4156" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">我也想涵盖熵的这一方面，因为它被广泛用于机器学习，尤其是分类任务。在某些方面，我们已经看到它隐藏在 KL 背离中。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pn"><img src="../Images/188219aeee2a6996937672e60fa14509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pHXPK_sJgC2oy7mdxU9mjA@2x.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:作者</p></figure><p id="e5ad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以 H(p)是分布 p 的熵(自我信息), H(p，q)是分布 p &amp; q 之间的交叉熵。</p><p id="2ec7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以用另一种方式来解读它，更重要的是理解相对熵(KL 散度)和交叉熵之间的区别。我们将尝试从香农对熵的使用来看信息所需的比特数。</p><p id="ac36" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">比方说，对于编码消息，我们希望使用 q 作为候选(目标)分布，因为它的分析形式而不是真正的分布 p(我们在前面的变分推理部分讨论过)。然而，现实情况是我们的近似并不完美，因此 KL 散度可以被视为 q 编码所需的额外比特的平均数量<strong class="ki iu">。然而，交叉熵可以被视为使用 q 表示信息所需的比特</strong>的平均数量<strong class="ki iu"/></p><p id="d623" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是显示交叉熵和 KL 散度关系的示例代码片段。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi po"><img src="../Images/b60eac278c50ee6d3c87d13935f7dddc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zsBHNmCfI0FH8-TNTLWT1Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来源:作者</p></figure><h1 id="b0dd" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">结束语</h1><p id="21d9" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">熵可能会吓到你，因为许多不同的公式和在不同科学领域的广泛使用，但正如我们在这篇文章中看到的，它们确实是相互联系的。</p><p id="aa84" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">需要记住的要点是:</p><ul class=""><li id="9f5d" class="lj lk it ki b kj kk kn ko kr ll kv lm kz ln ld lo lp lq lr bi translated">熵是信息的一种度量</li><li id="9978" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">信息令人惊讶</li><li id="1cbd" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">熵帮助你为你的领域问题选择合适的分布(给定约束)</li><li id="3f98" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">使用一个分布来近似另一个分布依赖于这些分布之间的相对熵</li></ul><p id="202d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你有问题，需要更多的澄清，请写在评论中，我会很乐意尝试回答他们，如果需要的话，更新文章。</p><h1 id="7727" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">参考资料和进一步阅读</h1><p id="30a3" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">【1】<a class="ae kf" href="https://jamesclear.com/entropy" rel="noopener ugc nofollow" target="_blank">熵:为什么生活似乎总是变得更加复杂？</a></p><p id="3d01" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]威尔·库尔特关于 KL 发散的博文<a class="ae kf" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" rel="noopener ugc nofollow" target="_blank">https://www . countbayesie . com/Blog/2017/5/9/kull back-lei bler-Divergence-explained</a></p><p id="1c0b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]https://plus.maths.org/content/satanic-science<a class="ae kf" href="https://plus.maths.org/content/satanic-science" rel="noopener ugc nofollow" target="_blank">的 M. Freiberger 著《撒旦的科学》</a></p><p id="f83f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4]信息论要素—<a class="ae kf" href="https://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954" rel="noopener ugc nofollow" target="_blank">https://www . Amazon . com/Elements-Information-Theory-telecom munications-Processing/DP/0471241954</a></p><p id="c7c4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[5]似然比检验—<a class="ae kf" href="https://en.wikipedia.org/wiki/Likelihood-ratio_test" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Likelihood-ratio_test</a></p></div></div>    
</body>
</html>