<html>
<head>
<title>Opening the black-box — beyond gradient descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">打开黑匣子——超越梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/opening-the-black-box-beyond-gradient-descent-439596fce294?source=collection_archive---------44-----------------------#2020-01-02">https://towardsdatascience.com/opening-the-black-box-beyond-gradient-descent-439596fce294?source=collection_archive---------44-----------------------#2020-01-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="88a8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">梯度下降近似视图介绍</h2></div><p id="8e1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当训练模型时，我们试图最小化训练样本上模型损失函数的平均值:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/b03d7c05414a6e32e3c85cb88d532fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*4Z9D0spbo5d2uLzKmKTq7w.png"/></div></figure><p id="80b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于以上总和中的每个<em class="lm"> i </em>，<em class="lm"> f_i </em>，是iᵗʰ训练样本所遭受的损失，作为模型的参数向量<em class="lm"> w </em>的函数。对于损失最小化任务，我们采用随机梯度方法的变体:在步骤<em class="lm"> k </em>，我们从{ <em class="lm"> f_1 </em>，…，<em class="lm"> f_n </em> }中随机选择一个函数<em class="lm"> f </em>，并计算:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/9a14d479e2d75363eca41704ce1cac45.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*-ZmxttQZ2epLMZTxKtwvmA.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">梯度步长</p></figure><p id="3471" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该方法存在许多变体，如mini-batching、Ada-Grad和Adam，但有一点是所有这些方法共有的:每个损失<em class="lm"> f </em>都作为“黑箱”给出——除了计算其梯度的能力之外，没有任何假设。</p><p id="afb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">黑盒方法有其优点，例如，我们可以很容易地改变损失函数，而不改变训练算法。但是这种方法有一个主要缺点——我们没有利用任何关于<em class="lm"> f </em>的知识，如果利用这些知识，可能会导致更有效的算法，并大大减少训练我们的模型所需的计算资源。</p><h1 id="b9c4" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">热身——近视图</h1><p id="3fa7" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">梯度步骤通常被教导为“在负梯度方向上迈出一小步”。但是还有一种替代观点，即<em class="lm">近端</em>观点:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mp"><img src="../Images/130367b9fd509d6b82afa73dd13f8895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JAJGnC-gFOJzh2fYB1Pamg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">梯度法的近视图。蓝色——在<em class="mu"> w_k处的切线。红色——靠近</em> w_k <em class="mu">。</em></p></figure><p id="3808" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们简化上面的“多毛”公式。蓝色项是切线，或在<em class="lm"> w_k </em>处的一阶近似，而红色项是对<em class="lm"> w_k. </em>的近似度量。因此，根据近似视图，</p><blockquote class="mv mw mx"><p id="cb72" class="ki kj lm kk b kl km ju kn ko kp jx kq my ks kt ku mz kw kx ky na la lb lc ld im bi translated">在每次迭代中，我们最小化一个在沿切线下降和保持接近w_k <em class="it">之间平衡的函数。</em></p></blockquote><p id="9e0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两个冲突力之间的平衡由1 <em class="lm"> /η — </em>邻近项的权重决定。正切和邻近项之和在<em class="lm"> w_k </em>处产生一条‘正切抛物线’，其最小值是下一次迭代。</p><p id="3943" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，直观地说，近端视图将梯度步长解释为正切抛物线的最小值，其局部近似我们的损失函数，如下所示。<em class="lm"> η </em>越大，抛物线越“平”，因此我们离当前迭代<em class="lm"> w_k </em>越远。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nb"><img src="../Images/ed833cdd7431d27b9cfaa651103b1874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AugcKb34UUOuHfBFsHilHA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">黑色曲线——损失函数f .橙色曲线——局部近似正切抛物线。我们计算梯度步长，作为正切抛物线的最小值。</p></figure><p id="88cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">形式上，要说服自己上面的公式确实是变相的梯度步长，我们来解决最小化问题。取<em class="lm"> argmin </em>内的项的梯度w.r.t <em class="lm"> w </em>并使其等于零，得到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/a609848175d3682b8b3aac30aecdb28b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*4Xf5z91z5S8RIJlXCMTUpg.png"/></div></figure><p id="bde9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两边乘以<em class="lm"> η </em>并提取<em class="lm"> w </em>，恢复梯度步长。</p><p id="47d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">备注</strong>:梯度法的近似观点在优化领域已经广为人知很久了，可以在Boris Polyak 1987年出版的《优化导论》一书中找到。</p><h1 id="811f" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">变暖—正规化</h1><p id="ed42" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">切线当然是我们一无所知的函数的合理近似。但是如果我们<strong class="kk iu">知道些什么呢？我们可以用更好的近似值代替蓝色部分(切线)吗？</strong></p><p id="3aba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来看一个具体的例子。假设每个损失都被<em class="lm">正则化</em>，即，</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/fc945637ffc9b9ce5b570561b65e86ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*-ZRj-6LA3fL7sIYBBtP7wA.png"/></div></figure><p id="1ca8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="lm"> lᵢ </em>是(可能加权的)‘原始’损失，我们在其上添加一个正则化项。在这种情况下，我们可以用切线来近似<em class="lm"> lᵢ </em>，同时保持正则项不变。为什么我们要？因为以下直觉的经验法则:</p><blockquote class="mv mw mx"><p id="20f9" class="ki kj lm kk b kl km ju kn ko kp jx kq my ks kt ku mz kw kx ky na la lb lc ld im bi translated">我们逼近得越少，关于损失函数的信息被保留和利用得越多，从而获得更好的算法。</p></blockquote><p id="bb40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的例子中，原来的梯度步骤被替换为</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ne"><img src="../Images/65864243da7ac972c964f68ea302d36c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dVWw-6lXTiVttTYnIaeRyw.png"/></div></div></figure><p id="e7aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看看能否得到一个<em class="lm"> w_{k+1} </em>的显式公式。取梯度w.r.t <em class="lm"> w </em>，等于零，结果为:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/b3d976eb2e2d7456d5fb1287f674e692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*57FdYyrlhpl9Y9mW646asA.png"/></div></figure><p id="50f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">提取<em class="lm"> w </em>，我们得到</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/13225ef0382b0ab374e14b8976213743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*yX3xSACj4W9tBVhgY1KC5w.png"/></div></figure><p id="7f07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到了一个全新的算法，它与直接应用于损失的随机梯度法有本质的不同。根据我们的新算法，在每次迭代中，我们将随机梯度步骤应用于“原始”损失<em class="lm"> l </em>，然后将结果除以(2 <em class="lm"> η </em> +1)。</p><h1 id="5607" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">超越L2正则化</h1><p id="022b" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">如果正则项不是平方欧几里德范数，而是其他函数<em class="lm"> g </em>会发生什么？明确地写，损失是</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/9348e2f3a52c4283480e21142ee6f195.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*d8u7c3ePotmFshMsEjWzrA.png"/></div></figure><p id="204d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最近的问题变成了</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ni"><img src="../Images/b8a7c28463aa9a0ec041385fe274a258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uof8iWX1ZVTZfhKhNy6bKQ.png"/></div></div></figure><p id="2f4c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们获得了众所周知的<a class="ae nj" href="https://en.wikipedia.org/wiki/Proximal_gradient_method" rel="noopener ugc nofollow" target="_blank">近端梯度</a>阶梯。为了在实践中实现，我们需要一个显式的公式来进行下一次迭代<em class="lm"> w_{k+1}。</em></p><p id="120a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在前面的例子中，<em class="lm"> g </em>是平方欧几里德范数，在这种情况下，我们可以很容易地获得一个显式公式。一般来说，推导这样一个公式并不总是可能的，当它是可能的时候，它取决于函数<em class="lm"> g </em>。每个<em class="lm"> g </em>导致不同的定制更新步骤，因此该方法不再是“黑盒”——我们<em class="lm">为每个<em class="lm"> g </em>定制</em>它。</p><p id="c585" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，<em class="lm"> g </em>应该足够“简单”,这样就可以得到一个简单的显式公式，因此这个方案只适用于这种“简单”的函数<em class="lm"> g </em>。</p><h1 id="3228" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">更多主题</h1><ul class=""><li id="36dd" class="nk nl it kk b kl mk ko ml kr nm kv nn kz no ld np nq nr ns bi translated">不同的近似值而不是正切值导致<em class="lm">稳定的</em>学习——它对步长的选择不太敏感，为我们节省了昂贵的步长调整。参见[1]中的示例。</li><li id="2e08" class="nk nl it kk b kl nt ko nu kr nv kv nw kz nx ld np nq nr ns bi translated">超越切线抛物线—考虑非二次邻近项。例子包括众所周知的镜像下降算法[2]的随机变体，这对于许多问题类别是有利的。</li></ul><h1 id="a21b" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">参考</h1><p id="76fb" class="pw-post-body-paragraph ki kj it kk b kl mk ju kn ko ml jx kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">[1]:希拉勒·阿西，约翰·杜奇(2019年7月)。随机(近似)邻近点方法:收敛性、最优性和适应性。<a class="ae nj" href="https://arxiv.org/abs/1810.05633" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1810.05633</a></p><p id="0064" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]:阿米尔·贝克、马克·特布尔(2003)。<em class="lm">凸优化的镜像下降和非线性投影次梯度方法。</em><a class="ae nj" href="https://www.sciencedirect.com/science/article/abs/pii/S0167637702002316" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/science/article/ABS/pii/s 0167637702002316</a></p></div></div>    
</body>
</html>