<html>
<head>
<title>Build your first CNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">建立你的第一个CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-your-first-cnn-fb3aaad77038?source=collection_archive---------21-----------------------#2020-03-11">https://towardsdatascience.com/build-your-first-cnn-fb3aaad77038?source=collection_archive---------21-----------------------#2020-03-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b059" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种构建卷积神经网络的更好方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/0381a2aee85a4c7e53eeb11ef18726db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*gMzDDP1bjMYrzVBBml8Vnw.png"/></div></figure><h1 id="ec1f" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">定义</h1><p id="aa47" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">卷积神经网络(CNN)是一种深度神经网络，在图像识别和分类中是有效的。它们接受图像作为输入，通过内核从中提取特征，并预测所需的输出。在CNN中，数据的预处理是最少的，并且它们可以自学识别来自不同图像的重要特征。对象检测、对象识别、面部识别、自动驾驶等领域的几乎所有先进网络。以某种方式利用CNN。</p><h1 id="54ef" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">介绍</h1><p id="fb83" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">网上有很多关于从头开始构建CNN的教程。尽管它们中的大多数确实提供了高精度，但是它们忽略了内存和参数的大量使用。当谈到在行业中部署模型时，构建网络不仅要达到目标精度，还要优化内存消耗。在本文中，我们将学习如何为物体识别构建一个优化的CNN。<br/>为了保持正确的期望值，让我们设定一个目标:</p><p id="db3d" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><strong class="lk iu">目标:<strong class="lk iu">上的</strong>MNIST数据集</strong></p><blockquote class="mj mk ml"><p id="d1a1" class="li lj mm lk b ll me ju ln lo mf jx lq mn mg lt lu mo mh lx ly mp mi mb mc md im bi translated"><em class="it"> 1。达到99.4 %的验证准确率<br/> 2。模型的总参数数应小于15k <br/> 3。最大历元数= 20 </em></p></blockquote><p id="d67c" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">为了实现这个目标，我们将把我们的行动计划分成4个迭代。首先，我们将重点构建一个达到目标精度的模型。然后，我们将大幅降低参数，同时保持架构。这种参数的减少会导致精度的下降。为了弥补这一下降，我们将优化模型以满足我们的目标。<br/>为了确保我们的模型在每次迭代中都是一致的，让我们添加一个条件，将每次迭代之间的修改限制为三次。<br/>在我们开始设计网络之前，有几个术语我们应该熟悉——</p><h2 id="f864" class="mq kr it bd ks mr ms dn kw mt mu dp la lr mv mw lc lv mx my le lz mz na lg nb bi translated">核心</h2><p id="d4b1" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">核是随机分配的权重矩阵，其在图像上被遍历和卷积。一个内核从图像中提取单个特征。</p><h2 id="ac7e" class="mq kr it bd ks mr ms dn kw mt mu dp la lr mv mw lc lv mx my le lz mz na lg nb bi translated">频道</h2><p id="62ab" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">卷积层的输出可以视为一组通道。<br/>例如，1个灰度图像(NxN) →与64个核的卷积(3x3) → 64个通道((N-2)x(N-2))</p><h2 id="87a8" class="mq kr it bd ks mr ms dn kw mt mu dp la lr mv mw lc lv mx my le lz mz na lg nb bi translated">感受野</h2><p id="983d" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">感受野是感觉周围的一个区域，在这个区域内刺激可以影响感觉细胞的电活动。在CNN中，这是由通道/内核的单个细胞感知的图像部分。</p><blockquote class="mj mk ml"><p id="311d" class="li lj mm lk b ll me ju ln lo mf jx lq mn mg lt lu mo mh lx ly mp mi mb mc md im bi translated">CNN的模型结构是通过将焦点保持在感受野上来设计的。通常，网络末端的模型的感受野与对象的大小相似。</p></blockquote><h2 id="d7b5" class="mq kr it bd ks mr ms dn kw mt mu dp la lr mv mw lc lv mx my le lz mz na lg nb bi translated">卷积块</h2><p id="acb8" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">包含多个<em class="mm">卷积层(3x3)的块。</em>在每个后续层中，通常增加内核/文件的数量，以确保从图像中提取所有重要特征。</p><p id="b1ec" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">每个卷积层都有一个激活函数(最好是ReLU，因为它对参数要求很低，这使得反向传播很容易校正权重),它给网络增加了非线性。</p><h2 id="13e6" class="mq kr it bd ks mr ms dn kw mt mu dp la lr mv mw lc lv mx my le lz mz na lg nb bi translated">过渡块</h2><p id="1383" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该块具有逐点(1x1) <em class="mm">卷积</em>以减少通道数量，以及<em class="mm">最大池</em>层以减少空间维度或通道大小。这个块的关键作用是过滤掉不相关的特征，限制参数的数量。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><p id="330f" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">现在让我们开始设计网络</p><h1 id="933d" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">目标1:普通网络</h1><p id="5224" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">布局架构时首先要考虑的是<em class="mm">感受域</em>。在模型的末尾，感受野应该等于物体的大小。完整的架构是围绕这个概念设计的。添加成对的卷积块和过渡块以获得所需的感受野。</p><blockquote class="mj mk ml"><p id="a704" class="li lj mm lk b ll me ju ln lo mf jx lq mn mg lt lu mo mh lx ly mp mi mb mc md im bi translated">注意:过渡块有一个池层，这会导致信息丢失。在网络的末端，信道的大小很小，信息很重要。因此，避免了朝向网络末端的过渡块。</p></blockquote><p id="0182" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">在最后一个卷积块之后，我们使输出变平，并使用softmax激活来提供类似概率的分布。</p><p id="f403" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><em class="mm">在这个特定的问题中，第一个过渡块在网络中添加得更快，因为背景主要是白色的，不包含太多信息，并且大部分信息位于中心。所以在网络早期使用第一个maxPool比较方便。</em></p><blockquote class="mj mk ml"><p id="2dda" class="li lj mm lk b ll me ju ln lo mf jx lq mn mg lt lu mo mh lx ly mp mi mb mc md im bi translated"><em class="it">参数个数:1595316<br/>最佳精度:99.93%(train)<br/></em><a class="ae nj" href="https://colab.research.google.com/drive/1IJwulhX4WNWKUXVArwIJk_fuCXv-2zUN" rel="noopener ugc nofollow" target="_blank"><em class="it">Colab笔记本</em> </a></p></blockquote><h1 id="9399" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">目标2:减少参数</h1><p id="a7ee" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的目标是在少于15k的参数中实现99.40%的精度。在本节中，主要重点是减少每个卷积中内核/文件的数量，以便总参数小于15k，而不会对精度造成太大影响(这将在后面的步骤中通过优化技术来恢复)。</p><p id="7fbe" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">对以前型号的更新:</p><p id="742f" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><strong class="lk iu"> 1。减少3×3卷积层中的滤镜</strong> <br/> <strong class="lk iu"> 2。减少1x1卷积层中的滤波器</strong> <br/>即使大幅减少所有层中的参数，总参数仍然超过15k。<br/> <strong class="lk iu"> 3。恰好在第一过渡块和输出层的中间添加部分过渡块。</strong>该部分过渡块将仅包括1x1卷积，但不包括最大池。<br/>第三步确保只有重要的功能在网络中继续进行，并避免因最大池而可能发生的信息丢失。</p><blockquote class="mj mk ml"><p id="f67f" class="li lj mm lk b ll me ju ln lo mf jx lq mn mg lt lu mo mh lx ly mp mi mb mc md im bi translated"><em class="it">参数个数:12490<br/>最佳精度:99.71%(train)<br/></em><a class="ae nj" href="https://colab.research.google.com/drive/1vS2UPSF6kIxHjGcJnqlTqltn3uD5X6jm" rel="noopener ugc nofollow" target="_blank"><em class="it">Colab笔记本</em> </a></p></blockquote><h1 id="243f" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">目标3:优化</h1><p id="a179" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于我们已经丢弃了一大块参数，我们需要采用一些优化技术来确保我们的模型不会损害准确性。</p><p id="9cf3" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">对以前型号的更新:</p><p id="5fbe" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><strong class="lk iu"> 1。添加批量规范化图层。</strong> <br/>这有助于标准化数据，并限制网络稍后的规模变化。这固定了层输入的均值和方差，有助于更快的收敛和改进梯度流。<br/> <strong class="lk iu"> 2。选择合适的学习速率</strong> <br/>选择最适合网络的LR(基于实验)<br/> <strong class="lk iu"> 3 .一个验证检查点</strong> <br/>精度与历元图并没有严格递增。精度不断波动，因此不能依赖最后一个历元来给出最佳结果。因此，添加了一个验证检查点，用于在每个时期后计算验证数据的准确性。</p><blockquote class="mj mk ml"><p id="c5c9" class="li lj mm lk b ll me ju ln lo mf jx lq mn mg lt lu mo mh lx ly mp mi mb mc md im bi translated"><em class="it">参数个数:12874<br/>最佳精度:99.28% (val)，99.74%(train)<br/></em><a class="ae nj" href="https://colab.research.google.com/drive/1Vuj54Tx26tBIJXyiDpa3Mrm2ORzVpUMw" rel="noopener ugc nofollow" target="_blank"><em class="it">Colab笔记本</em> </a></p></blockquote><h1 id="4c00" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">目标4:过度拟合和进一步优化</h1><p id="73ca" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，训练和验证准确性之间的差距随着随后的时代而扩大。这说明模型过拟合。为了解决这个问题，我们将在每个3x3卷积后添加一个丢弃层。</p><p id="7392" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">对以前型号的更新:</p><p id="e79d" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><strong class="lk iu"> 1。Add Dropout </strong> <br/> Dropout随机跳过一些连接，强制网络学习其他参数。<br/> <strong class="lk iu"> 2。增加一个LR调度器</strong> <br/>一旦精度饱和，就需要降低学习率，这样模型才能收敛到更优的最小值，最终导致精度提高。<br/> <strong class="lk iu"> 3。增加批量</strong> <br/>大批量有助于更快地训练模型。<br/>注意:批量不能过大。这可能导致损失函数陷入次优局部最小值。</p><blockquote class="mj mk ml"><p id="45fc" class="li lj mm lk b ll me ju ln lo mf jx lq mn mg lt lu mo mh lx ly mp mi mb mc md im bi translated"><em class="it">参数个数:12874<br/>最佳精度:99.36% (train)，99.41% (val)，第19历元<br/> </em> <a class="ae nj" href="https://colab.research.google.com/drive/165lEq_jXsBIaOZnYHbNPreH0fBaJu8yB" rel="noopener ugc nofollow" target="_blank"> <em class="it"> Colab笔记本</em> </a></p></blockquote><h1 id="39e5" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">超出我们的目标</h1><p id="fd14" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们已经实现了我们的目标。现在，让我们通过进一步减少每层中的内核数量，在不影响精度的情况下，尝试将我们的模型参数减少到8k以下。</p><blockquote class="mj mk ml"><p id="44ff" class="li lj mm lk b ll me ju ln lo mf jx lq mn mg lt lu mo mh lx ly mp mi mb mc md im bi translated"><em class="it">参数个数:7602<br/>最佳精度:98.93% (train)，99.44% (val)，第22历元<br/> </em> <a class="ae nj" href="https://colab.research.google.com/drive/1TeL7ueHTRB34ofw41dehvHA_xkdX0Cpj" rel="noopener ugc nofollow" target="_blank"> <em class="it"> Colab笔记本</em> </a></p></blockquote><h1 id="0f86" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">进一步地</h1><ol class=""><li id="69aa" class="nk nl it lk b ll lm lo lp lr nm lv nn lz no md np nq nr ns bi translated">在稍微硬一点的数据集上实施这种方法，例如CIFAR10、CIFAR100</li><li id="525d" class="nk nl it lk b ll nt lo nu lr nv lv nw lz nx md np nq nr ns bi translated">使用图像增强:最有效的方法之一规范你的网络</li><li id="00c2" class="nk nl it lk b ll nt lo nu lr nv lv nw lz nx md np nq nr ns bi translated">添加跳过连接:这有助于模型拓宽其感受野范围，学习不同大小的对象。</li><li id="0503" class="nk nl it lk b ll nt lo nu lr nv lv nw lz nx md np nq nr ns bi translated">使用循环LR加快收敛速度</li></ol><h2 id="69a8" class="mq kr it bd ks mr ms dn kw mt mu dp la lr mv mw lc lv mx my le lz mz na lg nb bi translated">参考资料:</h2><ol class=""><li id="94b4" class="nk nl it lk b ll lm lo lp lr nm lv nn lz no md np nq nr ns bi translated">MNIST:勒村、扬恩；科琳娜·科尔特斯；克里斯托弗J.C伯吉斯。<a class="ae nj" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">“MNIST手写数字数据库，Yann LeCun，Corinna Cortes和Chris Burges”</a>。检索于2013年8月17日</li><li id="635e" class="nk nl it lk b ll nt lo nu lr nv lv nw lz nx md np nq nr ns bi translated">斯利瓦斯塔瓦，尼蒂什&amp;辛顿，杰弗里&amp;克里热夫斯基，亚历克斯&amp;苏茨基弗，伊利亚&amp;萨拉胡季诺夫，鲁斯兰。(2014).<a class="ae nj" href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank">退出:防止神经网络过度拟合的简单方法。机器学习研究杂志</a>。15.1929–1958.</li></ol></div></div>    
</body>
</html>