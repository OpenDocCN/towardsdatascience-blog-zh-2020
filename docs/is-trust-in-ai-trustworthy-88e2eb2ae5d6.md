# 对 AI 的信任值得信任吗？

> 原文：<https://towardsdatascience.com/is-trust-in-ai-trustworthy-88e2eb2ae5d6?source=collection_archive---------28----------------------->

## *科技世界和人工智能有一个信任问题。但是在我们开始用制造问题的同样的蛮力来解决它之前，让我们停下来问一下，我们值得信任吗？*

![](img/04e7614a6382e8414428e3ca8c470c01.png)

由[法兰克诉](https://unsplash.com/@franckinjapan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

在我上一篇关于 [*信任人工智能*](https://medium.com/swlh/trust-in-ai-eff46f0b36c4) 的文章中，我写了如何在人工智能中建立信任需要包括 1)技术和人工智能背后的人和机构(那些销售、制造和使用它的人)以及 2)人工智能系统和解决方案的技术。但是，在我们跑去集体开一家“信任”商店或规划蓝图并开始将信任编码到我们的行为或技术中之前，让我们花点时间来理解信任。

第一个群体*是人民，他们应该而且总是对信任负有主要责任。为什么？因为无论我们在技术中直接或间接地构建了什么工具和方法，它们总是我们目标的产物。以微软最近的衍生产品 [Xiaoice](https://www.cnbc.com/2020/07/13/microsoft-spins-off-xiaoice-chatbot-for-chinese-users.html) (或者它的早期版本， [Zo](https://qz.com/1340990/microsofts-politically-correct-chat-bot-is-even-worse-than-its-racist-one/) )为例，这是一个有问题的聊天机器人，有一个十几岁的女孩角色。开发这个聊天机器人的几个版本花了五年多的时间。为什么在这个生态系统中领导可信人工智能的任何人都不能提出足够的关注，即在十几岁的女孩之后建模一个聊天机器人？这些产品不仅让微软，也让整个聊天机器人行业——以及人类——离信任越来越远。难道他们不能创新，展示他们的才华，甚至用一种不同的、问题更少的、投机取巧的角色来赚钱吗？*

*为什么我们总是弄错呢？是因为我们不懂信任吗？这是因为做出技术和商业决策的人被困在他们孤立的世界里，除了闪亮的可能性之外，无法感知任何东西吗？因为法律领域之外的责任和义务不是技术建设生态系统的一部分？即使当我们插入每个城市、每个家庭、每个招聘决定、刑事司法系统，与人工智能进行越来越多的互动和交易时，我们也不会花时间去“倾听”我们想要服务的人？*

*![](img/baf3b51fa1027a7d88ad92dedf995546.png)*

*照片由[阿里·帕扎尼](https://www.pexels.com/@alipazani?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)从[派克斯](https://www.pexels.com/photo/woman-in-brown-button-up-shirt-standing-near-white-and-red-wall-4863557/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)拍摄*

*根据[爱德曼的 2020 年信任晴雨表](https://www.edelman.com/trustbarometer)全球调查:*

*   *61%的人认为技术变化的速度太快，政府对新技术的理解不足以有效地管理它们。*
*   *66%的人担心技术会使人们无法知道他们看到或听到的是不是真实的。*

*通过人工智能，我们正在创新现实本身。最大的风险是，我们这些创新者将永远失去公众的信任——到某个时候，我们将没有办法纠正方向。如果我们现在不重新考虑信任技术的必要性，我们可能不会有第二次机会。*

# *信任的复杂本质*

*信任就是满足我们设定的期望。所以这是关于意图，沟通，清晰，纪律，文化，习惯。很多东西很难精确定位或定义。信任也像倾听或理解。我们想要的比我们愿意付出的多，所以需要努力。*

*信任是一种有意识和潜意识的计算:**当感知的信任成本小于感知的不信任成本时，我们就会信任。当信任的感知价值大于不信任的感知收益或价值时，我们就会信任。这是一种自我利益一致的信念。***

*我们如何准确地计算出这些感知的成本和价值？试试这个练习。想想你非常信任的人或事。到底是什么让你信任他们？什么会让你失去信任？随着时间的推移，你的信任有所改变或演变吗？下面是我从观察和分析中收集到的一系列特征，这些特征让信任变得令人兴奋、有价值且难以驾驭:*

1.  *信任是一场赌博。它需要我们去猜测，去相信，去相信。*
2.  *参差不齐。我们想要拥有的比我们想要给予的多。*
3.  *信任需要时间和注意力。不能操之过急。*
4.  *这需要努力。深思熟虑的工作。*
5.  *信任随着时间和互动而发展。它随着我们的改变或学习而改变。*
6.  *它是易碎的。更容易坏，更难修复。*
7.  *这不是完全由你控制的。这是相互关联和相互依赖的。其他各方必须愿意并准备好。*
8.  *信任对未来和社会都有影响——包括收益和成本。*
9.  *信任不是一切。有时候，兴奋、一笔好交易、奖励或生存比信任更重要*
10.  *信任不同于关心。你可以关心某人却不信任他们，反之亦然。*
11.  *信任更多的是关于真实性(某人的价值观)而不是诚实。信任是关于理解；如果需要，让你保守秘密。*
12.  *信任是难以捉摸的。我们对它越严厉，它就越躲避我们。*

*我对这些特征有一个单独的更深入的分析，但是对于这次讨论，我们可以将它们分成三个关键点。信任包括:*

1.  *缺乏确定性和高度可变性。*
2.  ***对自我意识的需要，以及对他人和未来影响的认识。***
3.  ***灵活自律，愿意付出努力，但放弃控制的欲望。***

*那是棘手的工作。为什么我们还要担心信任？因为信任是无价的。它能够更快、更低风险地做出决策。它使具有不同自我利益和目标的不同群体能够合作实现更大的集体价值和机会。信任可以产生新的想法，并形成生态系统，将想法付诸行动并形成规模。*

***什么时候信任很重要？当我们在不确定的情况下或与我们不确定的人群或机构一起做决定时，这很重要。想想你从哪里获得关于冠状病毒、工作适应力或在家上学的信息和指导。我们根据已知的东西来导航“未知”的元素。基本上，我们对未来的决定是基于我们从现在和过去所能预测或推断的。信任有助于我们做一个精心计算的赌注，以驾驭风险和回报。这就是信任如此珍贵、令人害怕和令人兴奋的原因。***

*完全理解信任的关键是“感知的”价值或成本。还记得信任特征列表上的最后两行吗？对我们来说，很难对复杂的相互联系和一个情况的每个角度有一个完整的了解。我们的信任程度基于我们对现实的了解和理解，以及我们的局限和偏见。我们的感知。这就是为什么我们说后知之明是 20/20。这就是为什么我们在一次大采购后会后悔，或者后来意识到我们所认为的一笔好交易、一份好工作或一个好伙伴并不是这样。*

****我们不想被人耍，更不想让大家发现我们自己耍了。这种对我们脆弱性的社会认知让我们更加烦恼。我们对自己的信任能力失去了信任。*** 这就是为什么失信的成本如此之高且难以修复。*

# *狡猾的骗子*

*我与技术决策者的大多数讨论似乎都集中在:*

1.  *我如何区分真实和炒作？也就是说，哪种技术(5G、对话式人工智能、差分隐私)已准备好采用，适用于哪种用例？*
2.  *我应该购买或投资什么公司和工具？*
3.  *什么策略可以“让”消费者和企业信任我的产品，“让我们远离麻烦？”*
4.  *我如何在不减缓成长的情况下把自己从“坏演员”或“错误”中分离出来？*

*所有实际和公平的问题。我们需要回答这些问题来做出日常决策。但是另一组重要的问题没有人问过我:**我值得信任吗？用户或公众应该信任我吗？他们什么时候应该或者不应该信任我？***

*让我们打开这个。我们什么时候值得信任？基本上，个人或集体的利己主义是信任和值得信任的最大动力。这种自我利益可以是无形的，如我们的价值观、社会地位、品牌、声誉，也可以是有形的，如工作、财产、定居点、商业股份。但是成本和价值都是可以感知的。信任还取决于我们如何以及与谁一起填充这个公式。是习惯性自动的，一切照旧吗？它是否对多个利益相关者进行了深思熟虑和反思，是否考虑了长期影响和短期影响？现在，人工智能创新完全是关于自动化、采用率和估值。能让我们产生信任的变量在哪里？*

*我们需要停止将[图灵测试](https://en.wikipedia.org/wiki/Turing_test)作为人工智能的目标和获取新闻头条的方式。对人工智能的终极渴望不应该是它欺骗我们的能力。我们应该关注它帮助我们、理解我们和回应我们需求的潜力。在 [Xiaoice](https://www.cnbc.com/2020/07/13/microsoft-spins-off-xiaoice-chatbot-for-chinese-users.html) 和 [Zo](https://qz.com/1340990/microsofts-politically-correct-chat-bot-is-even-worse-than-its-racist-one/) 青少年聊天机器人的例子中，重点似乎是展示一个看起来像人的商业上可行的聊天机器人。一个写诗，办画展，很“时髦”，穿校服，不介意成年男人向她表白的聊天机器人。在技术的巧妙中，长期的成本迷失了。想一想在一个高度资助和宣传的产品中滥用少女角色及其固有的性别偏见，这个产品的制作历时五年多。有这么多的时间来改变或纠正航向。我们想知道为什么没有更多的女性对科技感兴趣。*

*我们如何增加破坏这种基本信任的成本，降低针对弱势群体或走捷径的产品、技术和企业的价值？目前，它们之所以繁荣，是因为有一个群体或人群愿意支付高价来使用这些产品。在我们解决这个问题之前，我们怎么能开始信任人工智能背后的人呢？在这些人的手中,“值得信赖的人工智能”的标签会成为方便的幌子并制造混乱吗？它会一点都不值得信赖吗？*

# *我们对信任的态度是不是完全错误的？*

*![](img/aa195c6c4bae42fd5116481cddc89f53.png)*

*照片由[伯纳德·赫尔曼](https://unsplash.com/@bernardhermant?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/@bernardhermant?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄*

*我们都知道，给一个聪明人一份没有约束的说服蓝图，就像给他们一份如何操作的指南。[你认为我们最初是如何“迷上”](https://axbom.blog/nir-eyal-habit-danger/)并“沉迷”于技术的？对信任的担忧已经进入市场。产品、咨询服务、技术工具和领导力教练正在进入信任的 ***业务*** 和 ***战略*** 。*

*事实是，没有关心和同情的信任会导致对自我利益的信任，这意味着我们可以相信人们会受自我保护本能的引导，即使以他人为代价。没有尊重的信任导致傲慢和操纵。没有意识的信任可能是危险的。没有喜悦的信任，无聊。*

*这里是对 NPS 设计者和一些多样性和包容性项目的一些集体反馈——大多数人都明白。他们得到了在调查和培训中需要说的话(显性文化)和他们能够或应该如何表现(隐性文化)之间的差异。见鬼，我们教孩子们这个。我们说“不要撒谎”,然后当着别人的面撒谎，说我们的年龄、薪水以及为什么迟到。在这样的环境下，信任是如何发挥作用的？*

*员工信任隐性文化——通过观察领导在做什么。如果接受采访或在公共场合演讲，什么时候鼓掌，什么时候保持在他们的公关指导点上。但是爱德曼的调查结果显示普通大众也知道这一点。最终，我们都明白了。问题是我们还有什么其他选择，以及我们在多大程度上摆脱了我们的习惯模式去做一些事情。*

***记住，信任是关于我们或对方的感知价值和感知成本。我们所能理解和看到的，作为我们的得失。**有时候随波逐流，随大流也是应该的。我们不能假设我们得到了他们的信任。你有没有听到有人说“它不知从哪里冒出来的”？但你比他们早看到了？很少有事情是凭空出现的——这取决于我们在追踪谁和什么。*

# *这一切意味着什么？*

*如果有人出卖你的信任，赶紧跑。或者慢下来足够理解利益的对齐。因为他们可能会有利益冲突。如果有人向你展示如何骗取别人的信任，赶紧跑。除非你是一个游牧民族，并计划赚快钱和隐藏起来。那样的话，我不是给你建议的人。但如果你真的在深思熟虑地思考信任，那么首先从这个问题开始:我信任自己吗？为什么或什么时候我不值得信任？谁或什么可以帮助找出丢失了什么？练习你对自己的信任度。*

*问题和答案一开始可能会感觉不舒服。但是根据我的经验，我可以告诉你，它们也会给你一种解脱感。或者至少是清晰的。你不必去忏悔。肯定的，不要发微博。但是要知道自己的目标。如果你从事制造、购买或使用技术的行业，尤其是人工智能，在构思或决策阶段，问这些重要的问题。你有一个选择。人工智能系统既复杂又可靠；并使用经过彻底测试的、符合伦理道德的、准确的数据。也就是说，如果它们不是为了标记偏见或匿名或对数据源透明而设计的，系统就不会自动指出这些问题。当然，这并不是终点。人工智能系统正在发展。即使是用大量资源和对细节的关注构建的东西，也可能被其他拥有不同系统和不同目标的人重新构建、破解或打乱。*

*我们有科学和建筑评论，它们是很好的反思模型。但往往连认知多样性都没有。我让一些严肃、严厉的评论家评论了这篇文章。我还让一个不是来自技术领域的人，为了清晰起见，对它进行了审查。够了吗？*

*信任是结果。这是一个决定，一个措施，一个标准。与其试图建立信任，不如我们用责任和信任来设计？设计“具有信任的人工智能”是关于一个一致的、负责任的决策框架，该框架牢记关于受影响的人的考虑。*

*而不是“我们如何让人们信任 5G 或 AI，以便他们更快地采用它？”，如果我们问:*

*   *为什么人们不信任某种技术的使用——5G、人工智能、数据分析或神经技术？*
*   *哪些人或者哪些群体不信任？*
*   *他们为什么不信任它？*
*   *他们什么时候不应该相信它？*
*   *他们会失去什么？他们的担忧是什么？*
*   *是否咨询了该团体或该团体的支持者？*
*   *我们如何创新来解决他们的担忧？*

# *我们能做什么？*

*我经常与积极而关切的人工智能专家、商业领袖、研究人员、教育工作者、工程师和产品经理交谈，他们问:但我能做什么？他们雄心勃勃，他们想在经济上和职业上做得更好。但他们厌倦了不得不妥协自己的价值观，或以他们认为存在根本缺陷的方式做事。他们问我，为什么我们一直让世界变得更糟。他们正在寻找替代品。他们希望重新审视我们的技术制造框架，而不影响他们的愿望。他们希望他们的领导人将这一转变作为优先事项。他们希望指标有所改变。他们希望别人改变。我知道，从很多方面来说，我过去和现在都是这样的人。*

*这是我告诉自己的:[到 2030 年，人工智能预计将为全球经济增加超过 13 万亿美元](https://blogs.wsj.com/cio/2018/11/16/the-impact-of-artificial-intelligence-on-the-world-economy/)。这意味着它将触及人类系统和环境的每个部分，将产生持久的影响，并将产生足够的收入，我们没有借口*而不是*带着责任和信任投资建设。监管监督会阻碍创新。我们甚至还没有提到资金的关系，这是许多利益冲突的根源。不要对推动问责制感到内疚，好像我们在某种程度上背叛了公司或经济。我们通过回归基本面——我们的**价值观**来帮助**他们和我们**。这一切到底是为了什么——人民。*

*这就是我告诉技术和商业领袖的:对我有用的是意识到缺少什么，并重新构建问题陈述。我们正走向认知超载。我们需要信任来帮助我们驾驭世界。无论是人工智能还是其他技术，如果我们没有办法看到或理解对所有利益相关者的影响，我们将会做出可怕的事情。我告诉他们不要低估或高估他们拥有的资源、权力和技能。不管他们知道什么或者有什么，利用它。向他人学习或交流想法。加入一个有着不同想法和共同价值观的人的社区。关心是可以的。关心很重要。让我们让关心变得可以接受。平衡它与我们的需求。找出如何满足我们成功的专业和智力驱动力，并创新平衡的饮食。让我们成为我们想成为的人，为我们想创造的未来做出贡献。问，我错过了什么？什么能让我更值得信任？谁能帮我弄明白？*

*这激励我启动了[负责任的创新项目](https://responsibleproject.com)，并为产品和技术的构思、开发和评估创建了一个框架。但这仅仅是一个开始。我们不能单独或孤立地做到这一点。从生存到繁荣的转变必须成为科技文化的准则。如果我们要制造值得信赖的技术或人工智能，我们必须将信任整合到我们的流程中，并以值得信赖的方式制造它。但在此之前，我们必须花时间去理解信任，并问问我们缺少什么。为此承担责任。这是我们唯一可以相信的转变。*