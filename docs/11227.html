<html>
<head>
<title>Learn Web Scraping in 15 minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">15 分钟学会网络抓取</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learn-web-scraping-in-15-minutes-27e5ebb1c28e?source=collection_archive---------20-----------------------#2020-08-04">https://towardsdatascience.com/learn-web-scraping-in-15-minutes-27e5ebb1c28e?source=collection_archive---------20-----------------------#2020-08-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8f1b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从 Flipkart 网站抓取笔记本电脑数据</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/79554460d535ad8c0e7f15036a1b3d6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Q_OFLv6PQiCmE0pw"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Emile Perron 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="f3f5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是网页抓取？</h1><p id="c972" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Web 抓取，也称为 web 数据提取，是从网站检索或“抓取”数据的过程。收集这些信息，然后导出为对用户更有用的格式，可以是电子表格或 API。虽然<a class="ae ky" href="https://www.upwork.com/search/profiles/?nbs=1&amp;q=web%20scraping" rel="noopener ugc nofollow" target="_blank">网页抓取可以手动完成</a>，但在大多数情况下，自动工具在抓取网页数据时更受青睐，因为它们成本更低，工作速度更快。</p><h1 id="9ae5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">网络抓取合法吗？</h1><p id="70f0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最简单的方法就是查看网站的 robots.txt 文件。您可以通过将“/robots.txt”附加到您想要抓取的 URL 来找到该文件。它通常位于网站域/robots.txt。如果所有由“用户代理:*”指示的机器人在 robots.txt 文件中被阻止/禁止，则不允许您抓取。为了这篇文章，我正在浏览 Flipkart 网站。所以，要看“robots.txt”文件，网址是<a class="ae ky" href="http://www.flipkart.com/robots.txt" rel="noopener ugc nofollow" target="_blank">www.flipkart.com/robots.txt.</a></p><h1 id="9faf" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">用于网页抓取的库</h1><p id="65df" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">BeautifulSoup: BeautifulSoup 是一个 Python 库，用于从 HTML 和 XML 文件中提取数据。它与您喜欢的解析器一起工作，提供导航、搜索和修改解析树的惯用方式。</p><p id="77a4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Pandas: Pandas 是一个快速、强大、灵活且易于使用的开源数据分析和操作工具，构建于 Python 编程语言之上。</p><h1 id="398c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">为什么是 BeautifulSoup？</h1><p id="67bc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这是一个从网页中提取信息的不可思议的工具。你可以用它来提取表格、列表、段落，也可以用过滤器从网页中提取信息。更多信息，你可以参考 BeautifulSoup <a class="ae ky" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank">文档</a></p><h1 id="9b8e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">抓取 Flipkart 网站</h1><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="49b4" class="mx la it mt b gy my mz l na nb">from bs4 import BeautifulSoup <br/>import requests <br/>import csv<br/>import pandas as pd</span></pre><p id="a8fc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">首先，我们导入了 BeautifulSoup 和 requests 库，它们对于 web 抓取来说是非常重要的库。</p><p id="efa1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">requests: requests 是 Python 中使语言变得有趣的包之一。requests 基于 Python 的 urllib2 模块。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="ec49" class="mx la it mt b gy my mz l na nb">req = requests.get("<a class="ae ky" href="https://www.flipkart.com/search?q=laptops&amp;otracker=search&amp;otracker1=search&amp;marketplace=FLIPKART&amp;as-show=on&amp;as=off&amp;page=1" rel="noopener ugc nofollow" target="_blank">https://www.flipkart.com/search?q=laptops&amp;otracker=search&amp;otracker1=search&amp;marketplace=FLIPKART&amp;as-show=on&amp;as=off&amp;page=1</a>")  # URL of the website which you want to scrape<br/>content = req.content # Get the content</span></pre><p id="bb48" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要获取指定 URL 的内容，请使用请求库提交请求。这是包含笔记本电脑的 Flipkart 网站的 URL。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/19e5e2cbdcf4ac46b9426541ffea3fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OGZqW8KjNJzbsRFv0_h1hw.png"/></div></div></figure><p id="0808" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是由不同笔记本电脑组成的 Flipkart 网站。该页面包含 24 台笔记本电脑的详细信息。现在，我们尝试提取笔记本电脑的不同特征，例如笔记本电脑的描述(型号名称以及笔记本电脑的规格)、处理器(英特尔/AMD、i3/i5/i7/Ryzen3Ryzen5/Ryzen7)、RAM (4/8/16 GB)、操作系统(Windows/Mac)、磁盘驱动器存储(SSD/HDD、256/512/1TB 存储)、显示屏(13.3/14/15.6 英寸)、保修(现场/有限硬件/国际)、评级(4.1</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="edbf" class="mx la it mt b gy my mz l na nb">soup = BeautifulSoup(content,'html.parser')<br/>print(soup.prettify())</span><span id="0854" class="mx la it mt b gy nd mz l na nb">&lt;!DOCTYPE html&gt;<br/>&lt;html lang="en"&gt;<br/> &lt;head&gt;<br/>  &lt;link href="https://rukminim1.flixcart.com" rel="dns-prefetch"/&gt;<br/>  &lt;link href="https://img1a.flixcart.com" rel="dns-prefetch"/&gt;<br/>  &lt;link href="//img1a.flixcart.com/www/linchpin/fk-cp-zion/css/app.chunk.21be2e.css" rel="stylesheet"/&gt;<br/>  &lt;link as="image" href="//img1a.flixcart.com/www/linchpin/fk-cp-zion/img/fk-logo_9fddff.png" rel="preload"/&gt;<br/>  &lt;meta content="text/html; charset=utf-8" http-equiv="Content-type"/&gt;<br/>  &lt;meta content="IE=Edge" http-equiv="X-UA-Compatible"/&gt;<br/>  &lt;meta content="102988293558" property="fb:page_id"/&gt;<br/>  &lt;meta content="658873552,624500995,100000233612389" property="fb:admins"/&gt;<br/>  &lt;meta content="noodp" name="robots"/&gt;<br/>  &lt;link href="https://img1a.flixcart.com/www/promos/new/20150528-140547-favicon-retina.ico" rel="shortcut icon"&gt;<br/>....<br/>....<br/>&lt;/script&gt;<br/>  &lt;script async="" defer="" id="omni_script" nonce="7596241618870897262" src="//img1a.flixcart.com/www/linchpin/batman-returns/omni/omni16.js"&gt;<br/>  &lt;/script&gt;<br/> &lt;/body&gt;<br/>&lt;/html&gt;</span></pre><p id="af0d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里我们需要指定内容变量和解析器，也就是 HTML 解析器。所以现在 soup 是我们解析的 HTML 的 BeautifulSoup 对象的一个变量。soup.prettify()显示网页的全部代码。</p><p id="6ff0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">提取描述</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/72e1f2f8ae5d45862d5042646d97b4de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VdZ_KOin2rLEPyeYOICD7g.png"/></div></div></figure><p id="d267" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">当你点击“检查”标签，你会看到一个“浏览器检查框”打开。我们观察到描述的类名是' _3wU53n '，因此我们使用 find 方法提取笔记本电脑的描述。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a37a" class="mx la it mt b gy my mz l na nb">desc = soup.find_all('div' , class_='_3wU53n')</span><span id="71c4" class="mx la it mt b gy nd mz l na nb">[&lt;div class="_3wU53n"&gt;HP 14s Core i5 10th Gen - (8 GB/512 GB SSD/Windows 10 Home) 14s-cs3010TU Laptop&lt;/div&gt;,<br/> &lt;div class="_3wU53n"&gt;HP 14q Core i3 8th Gen - (8 GB/256 GB SSD/Windows 10 Home) 14q-cs0029TU Thin and Light Laptop&lt;/div&gt;,<br/> &lt;div class="_3wU53n"&gt;Asus VivoBook 15 Ryzen 3 Dual Core - (4 GB/1 TB HDD/Windows 10 Home) M509DA-EJ741T Laptop&lt;/div&gt;,<br/> &lt;div class="_3wU53n"&gt;Acer Aspire 7 Core i5 9th Gen - (8 GB/512 GB SSD/Windows 10 Home/4 GB Graphics/NVIDIA Geforce GTX 1650...&lt;/div&gt;,<br/>....<br/>....<br/>&lt;div class="_3wU53n"&gt;MSI GP65 Leopard Core i7 10th Gen - (32 GB/1 TB HDD/512 GB SSD/Windows 10 Home/8 GB Graphics/NVIDIA Ge...&lt;/div&gt;,<br/> &lt;div class="_3wU53n"&gt;Asus Core i5 10th Gen - (8 GB/512 GB SSD/Windows 10 Home/2 GB Graphics) X509JB-EJ591T Laptop&lt;/div&gt;]</span></pre><p id="909b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用<em class="nf"> find </em>方法从网站中提取描述——抓取类名为“_3wU53n”的 div 标签。这将返回类名为' _3wU53n '的所有 div 标记。由于 class 在 python 中是一个特殊的关键字，我们必须使用 class_ keyword 并在这里传递参数。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="0e40" class="mx la it mt b gy my mz l na nb">descriptions = [] # Create a list to store the descriptions<br/>for i in range(len(desc)):<br/>    descriptions.append(desc[i].text)<br/>len(descriptions)</span><span id="fe9b" class="mx la it mt b gy nd mz l na nb">24 # Number of laptops<br/>['HP 14s Core i5 10th Gen - (8 GB/512 GB SSD/Windows 10 Home) 14s-cs3010TU Laptop',<br/> 'HP 14q Core i3 8th Gen - (8 GB/256 GB SSD/Windows 10 Home) 14q-cs0029TU Thin and Light Laptop',<br/> 'Asus VivoBook 15 Ryzen 3 Dual Core - (4 GB/1 TB HDD/Windows 10 Home) M509DA-EJ741T Laptop',<br/> 'Acer Aspire 7 Core i5 9th Gen - (8 GB/512 GB SSD/Windows 10 Home/4 GB Graphics/NVIDIA Geforce GTX 1650...',<br/>....<br/>....<br/>'MSI GP65 Leopard Core i7 10th Gen - (32 GB/1 TB HDD/512 GB SSD/Windows 10 Home/8 GB Graphics/NVIDIA Ge...',<br/> 'Asus Core i5 10th Gen - (8 GB/512 GB SSD/Windows 10 Home/2 GB Graphics) X509JB-EJ591T Laptop']</span></pre><p id="dce0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">创建一个空列表来存储所有笔记本电脑的描述。我们甚至可以通过点访问来访问子标签。所以现在遍历所有的标签，然后使用。方法只从标记中提取文本内容。在每次迭代中，将文本添加到描述列表中。因此，在遍历所有标签后，descriptions 列表将包含所有笔记本电脑的文本内容(笔记本电脑型号名称和规格的描述)。</p><p id="d672" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">类似地，我们应用相同的方法来提取所有其他特征。</p><p id="19e0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">提取规格</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/4b3a013f046653d7d3c8185ed2026b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFSJwGzgO6VJ7fmQ0OUkSw.png"/></div></div></figure><p id="68d5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们注意到，不同的规格都在同一个 div 下，所有这 5 个特性(处理器、RAM、磁盘驱动器、显示器、保修)的类名都是相同的。</p><p id="4fed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所有的特性都在“li”标签中，所有的类名都是相同的，都是“tVe95H ”,所以我们需要应用一些技术来提取不同的特性。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="3420" class="mx la it mt b gy my mz l na nb"># Create empty lists for the features<br/>processors=[]<br/>ram=[]<br/>os=[]<br/>storage=[]<br/>inches=[]<br/>warranty=[]</span><span id="f68c" class="mx la it mt b gy nd mz l na nb">for i in range(0,len(commonclass)):<br/>    p=commonclass[i].text # Extracting the text from the tags<br/>    if("Core" in p): <br/>        processors.append(p)<br/>    elif("RAM" in p): <br/>        ram.append(p)<br/># If RAM is present in the text then append it to the ram list. Similarly do this for the other features as well</span><span id="145d" class="mx la it mt b gy nd mz l na nb">    elif("HDD" in p or "SSD" in p):<br/>        storage.append(p)<br/>    elif("Operating" in p):<br/>        os.append(p)<br/>    elif("Display" in p):<br/>        inches.append(p)<br/>    elif("Warranty" in p):<br/>        warranty.append(p)</span></pre><p id="2116" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">的。文本方法用于从标签中提取文本信息，这样我们就可以得到处理器、RAM、磁盘驱动器、显示器和保修的值。因此，以同样的方式，我们也将这种方法应用于其余的功能。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="25bf" class="mx la it mt b gy my mz l na nb">print(len(processors))<br/>print(len(warranty))<br/>print(len(os))<br/>print(len(ram))<br/>print(len(inches))</span><span id="cfc7" class="mx la it mt b gy nd mz l na nb">24<br/>24<br/>24<br/>24<br/>24</span></pre><p id="60cd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">提取价格</strong></p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="80a9" class="mx la it mt b gy my mz l na nb">price = soup.find_all(‘div’,class_=’_1vC4OE _2rQ-NK’) <br/># Extracting price of each laptop from the website<br/>prices = []<br/>for i in range(len(price)):<br/> prices.append(price[i].text)<br/>len(prices)<br/>prices</span><span id="dee5" class="mx la it mt b gy nd mz l na nb">24<br/>['₹52,990',<br/> '₹34,990',<br/> '₹29,990',<br/> '₹56,990',<br/> '₹54,990',<br/>....<br/>....<br/>'₹78,990',<br/> '₹1,59,990',<br/> '₹52,990']</span></pre><p id="3cde" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">同样，我们提取每台笔记本电脑的价格，并将所有价格添加到价格列表中。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="850c" class="mx la it mt b gy my mz l na nb">rating = soup.find_all('div',class_='hGSR34') <br/>Extracting the ratings of each laptop from the website<br/>ratings = []<br/>for i in range(len(rating)):<br/>    ratings.append(rating[i].text)<br/>len(ratings)<br/>ratings</span><span id="4236" class="mx la it mt b gy nd mz l na nb">37<br/>['4.4',<br/> '4.5',<br/> '4.4',<br/> '4.4',<br/> '4.2',<br/> '4.5',<br/> '4.4',<br/> '4.5',<br/> '4.4',<br/> '4.2',<br/>....<br/>....<br/>'₹1,59,990',<br/> '₹52,990']</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/0ef8173db8c782264d3ef9cc5d97b08e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7x9fqbArTihtbzdmmG3G7w.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/3e4d0277d3c86b49e0213909120bf7e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i6mNyvm4DL_Z9EScwsEDYw.png"/></div></div></figure><p id="49f5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里我们得到的评级长度为 37。但是背后的原因是什么呢？</p><p id="eae6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们观察到推荐的笔记本电脑的类名也与特色笔记本电脑的类名相同，这就是为什么它也提取推荐的笔记本电脑的评级。这导致了收视率的上升。应该是 24 岁，但现在是 37 岁！</p><p id="b413" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后但同样重要的是，将所有要素合并到一个数据框中，并以所需的格式存储数据！</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="ea67" class="mx la it mt b gy my mz l na nb">df = {'Description':descriptions,'Processor':processors,'RAM':ram,'Operating System':os,'Storage':storage,'Display':inches,'Warranty':warranty,'Price':prices}<br/>dataset = pd.DataFrame(data = d) </span></pre><p id="955b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">最终数据集</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/bd1e5014f58ab5fe3909b4f395de12ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*40qS476DfEVbsgDteaf6rA.png"/></div></div></figure><p id="e309" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">将数据集保存到 CSV 文件</strong></p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="b0a9" class="mx la it mt b gy my mz l na nb">dataset.to_csv('laptops.csv')</span></pre><p id="dec7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在我们将整个数据集放入一个 CSV 文件中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/8f3e26420fb77a463e85b4ac233e5440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EJdP5wiz-EkpIFb1AAj8WA.png"/></div></div></figure><p id="2f45" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了再次验证，我们阅读了 Jupyter 笔记本中下载的 CSV 文件。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="7ee4" class="mx la it mt b gy my mz l na nb">df = pd.read_csv('laptops.csv')<br/>df.shape</span><span id="8039" class="mx la it mt b gy nd mz l na nb">(24, 9)</span></pre><p id="c94d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于这是一个动态的网站，内容不断变化！</p><p id="be70" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">完整代码可以随时参考我的<a class="ae ky" href="https://github.com/mathangpeddi/Flipkart-Web-Scraping" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> GitHub 库</strong> </a>。</p><p id="3cf6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在 LinkedIn 上联系我<a class="ae ky" href="https://www.linkedin.com/in/mathang-peddi-23763317b/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">这里</strong> </a></p><blockquote class="nj"><p id="2e0f" class="nk nl it bd nm nn no np nq nr ns mm dk translated">“你在网络分析工具上每花 20 美元，你就应该在大脑上花 80 美元来理解数据。”—杰夫·绍尔</p></blockquote><p id="4316" class="pw-post-body-paragraph lr ls it lt b lu nt ju lw lx nu jx lz ma nv mc md me nw mg mh mi nx mk ml mm im bi translated">我希望你觉得这篇文章很有见地。我很乐意听到反馈，以便即兴创作，并带来更好的内容。</p><p id="7bda" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">非常感谢您的阅读！</p></div></div>    
</body>
</html>