<html>
<head>
<title>Introduction to NLP - Part 3: TF-IDF explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 介绍-第 3 部分:TF-IDF 讲解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc?source=collection_archive---------30-----------------------#2020-06-07">https://towardsdatascience.com/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc?source=collection_archive---------30-----------------------#2020-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="8c33" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">词频-逆文档频率，也称为 tf-idf…💤</p><p id="9194" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你觉得这听起来像胡言乱语吗？但你希望不会？在这篇文章中，我将首先演示如何使用<em class="ko"> sklearn </em>将文本数据矢量化到 tf-idf，然后展示如何在没有任何软件的情况下自己完成的一步一步的过程。希望<em class="ko"> tf-idf </em>在这篇帖子结束后，你会更清楚！🎓</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/fe46cf3d552e1c646476997b5e265e28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*z3JPgRkHoaAGe6-o"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">JESHOOTS.COM 在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae lf" href="https://unsplash.com/@jeshoots?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">拍照</a></p></figure></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="6253" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated"><strong class="ak"> 1。定义📗</strong></h1><p id="ceb0" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">首先，让我们熟悉一些定义，以确保我们对每个概念的含义保持一致:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mq"><img src="../Images/5614013ce97cc2a26ccb5699f28e6d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7s5Ho3rZwk3i7b_5wSSWcA.png"/></div></div></figure><blockquote class="mr ms mt"><p id="1840" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">* tf 和 tf-idf 被分成两个变体:一个基于 count (_raw)，另一个基于 percentage 以使事情更清楚。</p><p id="47da" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">* * IDF 的一个更通用的定义是一个权重，它提高不太频繁的术语的权重，降低较频繁的术语的权重。然而，我选择了一个简单的定义，因为根据所使用的公式，一个术语的最低权重可以是 1。无论哪种方式，在 tf-idf 中，与 tf 相比，频率较低的术语权重较高，频率较高的术语权重较低。</p><p id="7410" class="jq jr ko js b jt ju jv jw jx jy jz ka mu kc kd ke mv kg kh ki mw kk kl km kn im bi translated">括号中的例子显示了本文中使用的数据集的参考。</p></blockquote><p id="af75" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了便于管理，我们将从这些句子中创建两个小文档，这将允许我们监控每个步骤的输入和输出:</p><pre class="kq kr ks kt gt mx my mz na aw nb bi"><span id="b7fb" class="nc lo it my b gy nd ne l nf ng">d1 = 'I thought, I thought of thinking of thanking you for the gift'<br/>d2 = 'She was thinking of going to go and get you a GIFT!'</span></pre><p id="df07" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你想知道为什么我选择了这两个句子，我必须想出一个最小的例子，在它们被预处理后，转换成我理想的末端向量，然后向后想。一个绕口令派上了用场！</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="bf25" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">2.带 sklearn 的 TF-IDF💻</h1><p id="4c72" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">本节假设您已经访问并熟悉 Python，包括安装包、定义函数和其他基本任务。如果你是 Python 的新手，<a class="ae lf" href="https://www.python.org/about/gettingstarted/" rel="noopener ugc nofollow" target="_blank">这个</a>是一个很好的入门地方。</p><h2 id="519a" class="nc lo it bd lp nh ni dn lt nj nk dp lx kb nl nm mb kf nn no mf kj np nq mj nr bi translated">2.0.Python 设置🔧</h2><p id="50b7" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">我已经使用并测试了 Python 3.7.1 中的脚本。在使用代码之前，让我们确保您有合适的工具。</p><h2 id="4359" class="nc lo it bd lp nh ni dn lt nj nk dp lx kb nl nm mb kf nn no mf kj np nq mj nr bi translated">⬜️确保安装了所需的软件包:熊猫和 nltk</h2><p id="29d4" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">我们将使用以下强大的第三方软件包:</p><ul class=""><li id="251a" class="ns nt it js b jt ju jx jy kb nu kf nv kj nw kn nx ny nz oa bi translated"><em class="ko">熊猫</em>:数据分析库，</li><li id="9bbc" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated"><em class="ko"> nltk: </em>自然语言工具包库和</li><li id="01f3" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated"><em class="ko"> sklearn: </em>机器学习库。</li></ul><h2 id="e569" class="nc lo it bd lp nh ni dn lt nj nk dp lx kb nl nm mb kf nn no mf kj np nq mj nr bi translated">⬜️从 nltk 下载“停用词”和“wordnet”语料库</h2><p id="dedf" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">下面的脚本可以帮助你下载这些语料库。如果您已经下载了，运行它会通知您它是最新的:</p><pre class="kq kr ks kt gt mx my mz na aw nb bi"><span id="76be" class="nc lo it my b gy nd ne l nf ng">import nltk<br/>nltk.download('stopwords')<br/>nltk.download('wordnet')</span></pre><h2 id="3cd2" class="nc lo it bd lp nh ni dn lt nj nk dp lx kb nl nm mb kf nn no mf kj np nq mj nr bi translated">2.1.定义文本处理功能</h2><p id="b1cc" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">首先，让我们用包和数据准备环境:</p><pre class="kq kr ks kt gt mx my mz na aw nb bi"><span id="f4a6" class="nc lo it my b gy nd ne l nf ng"># Import packages and modules<br/>import pandas as pd<br/>from nltk.stem import WordNetLemmatizer<br/>from nltk.tokenize import RegexpTokenizer<br/>from nltk.corpus import stopwords<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="64a7" class="nc lo it my b gy og ne l nf ng"># Create a dataframe<br/>X_train = pd.DataFrame({'text': [d1, d2]})</span></pre><p id="be02" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从现在开始，我们将把‘X _ train’作为我们的语料库(不管它有多大)，把两个句子作为文档。第二，我们需要一个文本处理函数来将它传递给<em class="ko"> TfidfVectorizer </em>:</p><pre class="kq kr ks kt gt mx my mz na aw nb bi"><span id="9383" class="nc lo it my b gy nd ne l nf ng">def preprocess_text(text):<br/>    # Tokenise words while ignoring punctuation<br/>    tokeniser = RegexpTokenizer(r'\w+')<br/>    tokens = tokeniser.tokenize(text)<br/>    <br/>    # Lowercase and lemmatise <br/>    lemmatiser = WordNetLemmatizer()<br/>    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]<br/>    <br/>    # Remove stopwords<br/>    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]<br/>    return keywords</span></pre><p id="5bee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">🔗如果你需要解释，我已经在<a class="ae lf" href="https://medium.com/@zluvsand/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96" rel="noopener">系列的第一部分</a>中详细解释了这个功能。这个预处理程序将把文档转换成:</p><pre class="kq kr ks kt gt mx my mz na aw nb bi"><span id="6943" class="nc lo it my b gy nd ne l nf ng">d1 = [‘think’, ‘think’, ‘think’, ‘thank’, ‘gift’]<br/>d2 = [‘think’, ‘go’, ‘go’, ‘get’, ‘gift’]</span></pre><h2 id="be94" class="nc lo it bd lp nh ni dn lt nj nk dp lx kb nl nm mb kf nn no mf kj np nq mj nr bi translated">2.2.使用 TfidfVectorizer 向 tf-idf 矢量化</h2><p id="18b4" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">最后，对语料进行预处理:</p><pre class="kq kr ks kt gt mx my mz na aw nb bi"><span id="3233" class="nc lo it my b gy nd ne l nf ng"># Create an instance of TfidfVectorizer<br/>vectoriser = TfidfVectorizer(analyzer=preprocess_text)</span><span id="8f4e" class="nc lo it my b gy og ne l nf ng"># Fit to the data and transform to feature matrix<br/>X_train = vectoriser.fit_transform(X_train['text'])</span><span id="c6bb" class="nc lo it my b gy og ne l nf ng"># Convert sparse matrix to dataframe<br/>X_train = pd.DataFrame.sparse.from_spmatrix(X_train)</span><span id="c52b" class="nc lo it my b gy og ne l nf ng"># Save mapping on which index refers to which words<br/>col_map = {v:k for k, v in vectoriser.vocabulary_.items()}</span><span id="63b1" class="nc lo it my b gy og ne l nf ng"># Rename each column using the mapping<br/>for col in X_train.columns:<br/>    X_train.rename(columns={col: col_map[col]}, inplace=True)<br/>X_train</span></pre><p id="689b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦脚本运行，您将得到以下输出:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/4adb054820259e47405b848d052d5f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*DTESJXX_5VvMqM2P8GiG5g.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">tf-idf 矩阵</p></figure><p id="39a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Tada❕我们已经将语料库矢量化到 tf-idf！数据现在是机器学习模型可接受的格式。</p><p id="42be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看有或没有看不见的术语的测试文档是如何转换的:</p><pre class="kq kr ks kt gt mx my mz na aw nb bi"><span id="70f5" class="nc lo it my b gy nd ne l nf ng">d3 = “He thinks he will go!”<br/>d4 = “They don’t know what to buy!”</span><span id="d463" class="nc lo it my b gy og ne l nf ng"># Create dataframe<br/>X_test = pd.DataFrame({‘text’: [d3, d4]})</span><span id="1944" class="nc lo it my b gy og ne l nf ng"># Transform to feature matrix<br/>X_test = vectoriser.transform(X_test['text'])</span><span id="ee4c" class="nc lo it my b gy og ne l nf ng"># Convert sparse matrix to dataframe<br/>X_test = pd.DataFrame.sparse.from_spmatrix(X_test)</span><span id="58e2" class="nc lo it my b gy og ne l nf ng"># Add column names to make it more readible<br/>for col in X_test.columns:<br/>    X_test.rename(columns={col: col_map[col]}, inplace=True)<br/>X_test</span></pre><p id="f05e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当<code class="fe oi oj ok my b">preprocess_text</code>被应用时，测试文档将转换成:</p><pre class="kq kr ks kt gt mx my mz na aw nb bi"><span id="d9fc" class="nc lo it my b gy nd ne l nf ng">d3 = [‘think’, 'go'] # <em class="ko">vectoritiser is familiar with these terms</em><br/>d4 = [‘know’, ‘buy’] # <em class="ko">vectoritiser is not familiar with these terms</em></span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/191c8b97086f20a1d7ad045df826af6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*aOx5Q3QL-db6m09wKbmjHg.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">tf-idf 矩阵</p></figure><p id="dba6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是您在 X_test 转换时所期望看到的吗？虽然我们在<em class="ko"> d3 </em>中出现了一次“go”和“think ”,但你有没有注意到“go”相对于“think”的权重是如何增加的？<em class="ko"> d4 </em>取全 0 值这个事实对你有意义吗？你是否注意到矩阵中的项数取决于训练数据，就像任何其他的<em class="ko"> sklearn </em>变形金刚一样？</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="4009" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">3.TF-IDF -自己动手📝</h1><p id="c97e" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">我认为当我们开始寻找引擎盖下的东西时，事情会变得更有趣。在这一节中，我们将手动进行转换，这不是很有趣吗？😍</p><p id="d9fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你喜欢数学，我鼓励你按照这个指南手动计算问题，或者更好的是，在继续下面的答案之前尝试自己计算。在下面的例子中，行代表文档或语料库，列代表术语。</p><h2 id="462f" class="nc lo it bd lp nh ni dn lt nj nk dp lx kb nl nm mb kf nn no mf kj np nq mj nr bi translated">3.1.原始术语频率表</h2><p id="9d6e" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated"><strong class="js iu">🔒问题:</strong>按文档统计每个术语的<em class="ko">原始术语频率</em>。</p><p id="e66b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">💭提示:</strong>看预处理后的<em class="ko"> d1 </em>和<em class="ko"> d2 </em>以及<em class="ko"> tf_raw </em>的定义。</p><p id="664f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔑答案:</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi om"><img src="../Images/d7c0f3cf2b19515da247d2d2fc65c0dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*DVNUD7zHpxzZVwh_VfxU5A.png"/></div></figure><p id="3b41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下表总结了每个术语在文档中出现的次数。例如:我们在文档 1 中看到“think”出现了 3 次，但在文档 2 中只出现了一次。列的数量由语料库中唯一术语的数量决定。这一步其实就是<em class="ko"> sklearn 的 count vector ser</em>做的事情。</p><h2 id="e6d9" class="nc lo it bd lp nh ni dn lt nj nk dp lx kb nl nm mb kf nn no mf kj np nq mj nr bi translated">3.2.术语频率</h2><p id="925a" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated"><em class="ko">(此步骤仅用于比较最终输出。)</em></p><p id="9fae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔒问题:</strong>按文档计算每个词的<em class="ko">词频</em>。</p><p id="2150" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">💭提示:</strong>一个术语在文档中占多大比例？(第%行)</p><p id="f04f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔑答案:</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi on"><img src="../Images/c0fbf3ddfa9938c260906c47cbc0d369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*5mhHP91A6H3ibiXerCh-rA.png"/></div></figure><p id="995e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">术语“思考”占了文献 1 中 60%的术语。</p><h2 id="49dd" class="nc lo it bd lp nh ni dn lt nj nk dp lx kb nl nm mb kf nn no mf kj np nq mj nr bi translated">3.3.文档频率</h2><p id="c536" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated"><strong class="js iu">🔒问题:</strong>统计<em class="ko">每期的文档频率</em>。</p><p id="7548" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">💭提示:</strong>有多少个文档包含特定的术语？</p><p id="5a2a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔑回答:</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/5d9b5ce4a82e6c29e5a2e2710738dde9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*1RtIhRPohlYfUmy1mJj7Qw.png"/></div></figure><p id="69ab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">术语“get”仅在文档 1 中出现，而术语“think”在两个文档中都出现。</p><h2 id="2749" class="nc lo it bd lp nh ni dn lt nj nk dp lx kb nl nm mb kf nn no mf kj np nq mj nr bi translated">3.4.逆文档频率</h2><p id="60a3" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated">这是第一次计算的地方。<br/> <strong class="js iu"> ➗公式:</strong>在该公式中，<em class="ko"> n </em>代表文档数。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi op"><img src="../Images/a6ff4576eeb5db28363291c809cf12a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*byrQcIBFebwdttxa0rY9_Q.png"/></div></figure><p id="8ad8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔒问题:</strong>计算每项的<em class="ko"> idf </em>。</p><p id="427e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔑答:</strong><em class="ko">IDF</em>看起来是不是和<em class="ko"> df </em>有点反？</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi om"><img src="../Images/961035054e29313371a00243a88c2fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*dghINsg0PjyWdC2DS072sQ.png"/></div></figure><p id="697f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<em class="ko"> sklearn </em>中，可以从拟合的<em class="ko">tfidf 矢量器</em>中访问<em class="ko"> idf_ </em>属性。</p><p id="8257" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">🔍<strong class="js iu">例题计算:</strong>下面，我已经提供了一个例题计算。通过复制相同的逻辑，您可以将它作为其余术语的指南:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/74bf3fbf0abd58d7f5347386da87fb37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*bMMz0anQtzM94GoUpyr9Vg.png"/></div></figure><h2 id="2563" class="nc lo it bd lp nh ni dn lt nj nk dp lx kb nl nm mb kf nn no mf kj np nq mj nr bi translated">3.5.原始术语频率与文档频率相反</h2><p id="0354" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated"><strong class="js iu"> ➗公式:</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi or"><img src="../Images/81ec23c2f7e5e840ecb3938790a6278f.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*lSYirsp9zqfv1Gw3q5QX5g.png"/></div></figure><p id="5f2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔒问题:</strong>按文档计算每个术语的<em class="ko"> raw tf-idf </em>(即加权计数)。</p><p id="b388" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔑答案:</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi os"><img src="../Images/ceeb70faf5df678a9bc6b2eecff6c3d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*2kxNoOxEE7axt_Ls8h8ZNw.png"/></div></figure><p id="2089" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔍示例计算:</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/7f7bfc9a1a71c698da7fa0f0c31221f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*IIryNKlrDkUaE9fiLLDFZg.png"/></div></figure><h2 id="75b4" class="nc lo it bd lp nh ni dn lt nj nk dp lx kb nl nm mb kf nn no mf kj np nq mj nr bi translated">3.6.术语频率与文档频率成反比</h2><p id="b95a" class="pw-post-body-paragraph jq jr it js b jt ml jv jw jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn im bi translated"><strong class="js iu"> ➗公式:</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/2fb582152aa934399e43bf72534dc9d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*EgKaWiPiNnggeQoaU5yHgA.png"/></div></figure><p id="26d3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔒问题:</strong>通过文档计算每项的<em class="ko"> tf-idf </em>。</p><p id="9f42" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔑答案:</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/85e32de54782a1b64dd4a5adb095e43f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*c_alm6AKf06l1FCIUGVNkA.png"/></div></figure><p id="f6e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">🔍</strong> <strong class="js iu">示例计算:</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ow"><img src="../Images/48c2f17d7be114f6f7f14fa7ff83324b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CyYtBGcm3YxOa2EiCLI-lg.png"/></div></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/1aa960a1d06dd9e0b022bcac896400b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*8WZXd0V0Jka34ddP_PB7Jw.png"/></div></figure><p id="1d20" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Yay❕我们已经获得了完全相同的结果！值得注意的是，如果您在中间步骤的任何地方进行了舍入，由于舍入误差，您的最终答案可能与上面提供的答案不完全匹配。</p><p id="f9d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">📌</strong> <strong class="js iu">练习:</strong>看看能不能计算出<em class="ko"> d3 </em>和<em class="ko"> d4 </em>的<em class="ko"> tf-idf </em>，并与上一节<em class="ko"> sklearn </em>的输出相匹配。</p><p id="d328" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">💭</strong> <strong class="js iu">提示:</strong> (1) Count tf_raw - terms 指的是来自训练数据的术语，(2)使用我们构建的 idf 计算 tf-idf_raw，(3)计算 tf-idf。仅对培训中的术语执行这些步骤。</p><p id="6899" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当<em class="ko"> sklearn </em>中的<em class="ko">tfidf vector</em>或<em class="ko"> TfidfTransformer </em>的<em class="ko"> smooth_idf=True </em>时，该方法复制输出。如果您将该参数更改为<em class="ko"> False </em>，您将不得不通过从分子和分母中取出+1 来稍微调整<em class="ko"> idf </em>公式。</p><p id="0291" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们总结之前，让我们比较一下文档 1 的<em class="ko"> tf </em>与<em class="ko"> tf-idf </em>:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f5175d80ec1913d05cd012f76007e196.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*IMPrwWoVfyvQdjpat_A38Q.png"/></div></figure><p id="3b78" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为“gift”和“think”出现在所有文档中，所以它们在两个表中的相对权重是相同的。然而，“感谢”只出现在文档 1 中，因此与“礼物”或“思考”相比，它在 tf-idf 中的相对频率更高。因此，这展示了<em class="ko"> tf-idf </em>如何提高较少文档中的术语的权重，降低较多文档中的术语的权重。</p><p id="3060" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">📌</strong> <strong class="js iu">练习:</strong>自己分析文献 2。</p><p id="1f3c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">显然，手动计算更容易出错，并且不太可能适用于具有数百、数千甚至数百万文档的真实语料库。非常感谢<em class="ko"> sklearn </em>贡献者提供了这样一种有效的方式，用短短几行代码将文本转换成<em class="ko"> tf-idf </em>。此处显示的手动计算仅用于举例说明使用软件时的底层实现。如果您想自己从工具而不是从<em class="ko"> sklearn </em>复制输出，可能需要对公式进行调整，但总体思路应该是相似的。我希望这些解释是有用和有见地的。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oz"><img src="../Images/f5a48ac1b6a85495bcf202352506d41f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3LiM3NWnA941Ypy9"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><a class="ae lf" href="https://unsplash.com/@fempreneurstyledstock?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Leone Venter </a>在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="37ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">您想要访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果您使用</em> <a class="ae lf" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="ko">我的推荐链接</em></a><em class="ko">成为会员，您的一部分会费将直接用于支持我。</em></p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><p id="adc5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢您花时间阅读这篇文章。我希望你从阅读它中学到一些东西。其余帖子的链接整理如下:<br/> ◼️ <a class="ae lf" href="https://medium.com/@zluvsand/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96" rel="noopener">第一部分:Python 中的文本预处理</a> <br/> ◼️ <a class="ae lf" href="https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc" rel="noopener">第二部分:词法分析和词干分析的区别</a> <br/> ◼️ <strong class="js iu">第三部分:TF-IDF 解释</strong> <br/> ◼️ <a class="ae lf" href="https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267" rel="noopener">第四部分:Python 中的有监督文本分类模型</a> <br/> ◼️ <strong class="js iu"> </strong> <a class="ae lf" rel="noopener" target="_blank" href="/introduction-to-nlp-part-5a-unsupervised-topic-model-in-python-733f76b3dc2d">第五部分:Python 中的无监督主题模型(sklearn) </a> <br/> ◼️ <a class="ae lf" rel="noopener" target="_blank" href="/introduction-to-nlp-part-5b-unsupervised-topic-model-in-python-ab04c186f295">第五部分</a></p><p id="2cfb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">快乐变身！再见🏃💨</p><h1 id="8cd5" class="ln lo it bd lp lq pa ls lt lu pb lw lx ly pc ma mb mc pd me mf mg pe mi mj mk bi translated">4.参考📁</h1><ul class=""><li id="6793" class="ns nt it js b jt ml jx mm kb pf kf pg kj ph kn nx ny nz oa bi translated"><a class="ae lf" href="http://www.nltk.org/book/" rel="noopener ugc nofollow" target="_blank">伯德、史蒂文、爱德华·洛珀和伊万·克莱恩，<em class="ko">用 Python 进行自然语言处理</em>。奥莱利媒体公司，2009 年</a></li><li id="9986" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated"><a class="ae lf" href="https://scikit-learn.org/stable/modules/feature_extraction.html" rel="noopener ugc nofollow" target="_blank"> <em class="ko">特征提取</em>，sklearn 文档</a></li></ul></div></div>    
</body>
</html>