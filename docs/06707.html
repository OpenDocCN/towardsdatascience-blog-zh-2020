<html>
<head>
<title>OpenAI’s MADDPG Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAI的MADDPG算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82?source=collection_archive---------24-----------------------#2020-05-26">https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82?source=collection_archive---------24-----------------------#2020-05-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="50b8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">多主体RL问题的行动者批评方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c3fcded180ea583e69f532400f64b41b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wKTtW_Cf5_SO5OL_"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">萨法尔·萨法罗夫在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="9860" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">新方法</h1><p id="24a6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">多主体强化学习是一个正在进行的、内容丰富的研究领域。然而，在多智能体环境中天真地应用单智能体算法“使我们陷入困境”由于许多原因，学习变得困难，尤其是由于:</p><ul class=""><li id="d164" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">独立主体间的非平稳性</li><li id="3a6b" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">状态和动作空间的指数增长</li></ul><p id="aaf6" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">研究人员提出了许多方法来减轻这些挑战的影响。这些方法的一个大子集属于“集中计划，分散执行”的范畴</p><h2 id="adab" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">集中规划</h2><p id="8338" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">每个代理只能直接访问本地观测。这些观察可以是许多事情:环境的图像、与地标的相对位置、或者甚至其他代理的相对位置。此外，在学习过程中，所有代理都由一个集中的模块或评论家指导<strong class="lt iu">。</strong></p><p id="4f2b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">尽管每个代理只有本地信息和本地策略需要训练，但是有一个实体可以忽略整个代理系统，建议他们如何更新策略。这降低了非平稳性的影响。所有代理都在具有全局信息的模块的帮助下学习。</p><h2 id="a61a" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">分散执行</h2><p id="8186" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">然后，在测试期间，集中的模块被移除，只留下代理、它们的策略和本地观察。这减少了增加状态和动作空间的不利影响，因为<em class="ns">联合</em>策略从未被明确学习。相反，我们希望中央模块已经给出了足够的信息来指导<em class="ns">本地</em>策略培训，以便一旦测试时间到来，它对于整个系统是最优的。</p><h2 id="3efd" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">OpenAI</h2><p id="46cd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">OpenAI、加州大学伯克利分校和麦吉尔大学的研究人员介绍了一种使用多代理深度确定性策略梯度的多代理设置的新方法。受其单一代理对手DDPG的启发，这种方法使用演员-评论家风格的学习，并显示出有希望的结果。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/560eb9276d245222007586c911727269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q1MNNYGkJR_ANiar"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@alinnnaaaa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Alina Grubnyak </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="f3eb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">体系结构</h1><p id="b98c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们假设熟悉单代理版本的MADDPG:深度确定性政策梯度(DDPG)。快速回顾一下，<a class="ob oc ep" href="https://medium.com/u/b24112d01863?source=post_page-----9d2dad34c82--------------------------------" rel="noopener" target="_blank"> Chris Yoon </a>有一篇精彩的文章对此进行了概述:</p><div class="od oe gp gr of og"><a rel="noopener follow" target="_blank" href="/deep-deterministic-policy-gradients-explained-2d94655a9b7b"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">解释了深层确定性策略梯度</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">连续动作空间中的强化学习</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou ks og"/></div></div></a></div><p id="12de" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">每个智能体都有一个观察空间和<strong class="lt iu">连续行动空间</strong>。此外，每个代理都有三个组件:</p><ul class=""><li id="f553" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">一个演员网络，使用<em class="ns">本地</em>观察来进行<em class="ns">确定性</em>动作</li><li id="dec3" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">用于训练稳定性的具有相同功能的目标演员网络</li><li id="9d9f" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">一个使用联合状态动作对来估计Q值的批判网络</li></ul><p id="76f5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">随着时间的推移，评论家学习<em class="ns">联合</em> Q值函数，它向演员发送适当的Q值近似值以帮助训练。我们将在下一节看到对这种交互的更深入的研究。</p><blockquote class="ov ow ox"><p id="dd17" class="lr ls ns lt b lu mp ju lw lx mq jx lz oy nd mc md oz ne mg mh pa nf mk ml mm im bi translated">请记住，批评家可以是所有N个代理之间的共享网络。换句话说，不是训练N个估计相同价值的网络，而是简单地训练一个网络，并用它来帮助所有参与者学习。如果代理是同质的，这同样适用于行动者网络。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/b1a7d9ae57fac9fd54d5468b859c5e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4aPqpDYFe3ibl0JIO8AeFg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MADDPG架构(Lowe，2018年)</p></figure><h1 id="0f18" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">学问</h1><p id="16cd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">首先，MADDPG使用经验回放进行高效的政策外培训。在每个时间步长，代理存储以下转换:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/247ede15eda6cc2aef1f0e2e0f98df8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5tVjJUYJxrw4AP87q3J-Yg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">体验回放过渡</p></figure><p id="2331" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">其中存储了联合状态、下一个联合状态、联合行动以及每个代理收到的奖励。然后，我们从体验回放中抽取一批这样的转换来训练我们的代理。</p><h2 id="8d18" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">评论家更新</h2><p id="5bcf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了更新代理的集中评论，我们使用一步前瞻TD-error:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/fdbf7dd6b9dd6292b3f8ecf1770fcf56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nP3DsEdLX6mPrGbfY3vezw.png"/></div></div></figure><p id="10a5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">其中mu表示演员。请记住，这是一个<em class="ns">集中式评论器，</em>意味着它使用联合信息来更新其参数。主要动机是<strong class="lt iu">知道所有代理采取的行动使得环境静止，即使当策略改变时。</strong></p><p id="d3cf" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">请注意右侧目标Q值的计算。尽管我们从未明确存储<em class="ns">下一个联合动作，</em>我们在更新期间使用每个代理的<em class="ns">目标角色</em>来计算这个下一个动作，以帮助训练稳定性。目标参与者的参数会定期更新，以匹配代理的参与者参数。</p><h2 id="affc" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">演员更新</h2><p id="0070" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">类似于单代理DDPG，我们使用确定性策略梯度来更新每个代理的参与者参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/a1702a129f6146e50191f478ca66c232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nwSsCBD8Gq5kFLVLf2VyfA.png"/></div></div></figure><p id="02ad" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">其中mu表示代理的演员。</p><p id="0e32" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">让我们稍微深入一下这个更新等式。我们使用一个<em class="ns">中心评论家</em>来指导我们，相对于<em class="ns">演员的参数</em>进行渐变。要注意的最重要的事情是，即使演员只有局部的观察和动作，我们在训练期间使用一个集中的评论家，为整个系统提供关于其动作的最优性的信息。这减少了非平稳性的影响，同时将策略学习保持在较低的状态空间！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/f1576ea1725dc053db8ea17f9fa58502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4OQ2aud1Sa2_JURd"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@sanderweeteling?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">拍摄的桑德韦特林</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="1d5e" class="kz la it bd lb lc pg le lf lg ph li lj jz pi ka ll kc pj kd ln kf pk kg lp lq bi translated">政策推理和政策集合</h1><p id="6e34" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们可以进一步下放权力。在早期的critic更新中，我们假设每个代理<em class="ns">自动</em>知道其他代理的行动。然而，MADDPG建议<strong class="lt iu">推断其他代理的政策</strong>以使学习更加独立。实际上，每个代理增加了<em class="ns"> N-1 </em>个网络来估计每个其他代理的真实策略。我们使用一个概率网络，并最大化输出另一个代理的观察行为的对数概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/bc769c27d869f8d82fe495c641ffd796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2tz8pckGzxDxn7ZKIGiXBQ.png"/></div></div></figure><p id="c769" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">其中我们示出了第I个代理的损失函数，用熵正则化器估计第j个代理的策略。结果，当我们用我们的预测行为替换代理行为时，我们的Q值目标变成了一个稍微不同的值！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/84f4302753116cfbdcee9ccb8d30631f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_b5Du6MMI3sy39jexQaq6A.png"/></div></div></figure><p id="7b7f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">那么，我们到底做了什么？我们已经排除了所有特工都知道彼此政策的假设。相反，我们试图训练代理人通过一系列观察来正确预测其他政策。实际上，每个代理都是通过<em class="ns">从环境中提取全局信息</em>来独立训练的，而不是自动将它放在手边。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/13f582038a4fa652033a5b336a1861ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X5v0xr9AAqAvU_RT"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@timmossholder?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">蒂姆·莫斯霍尔德</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="e54a" class="kz la it bd lb lc pg le lf lg ph li lj jz pi ka ll kc pj kd ln kf pk kg lp lq bi translated">政策组合</h1><p id="1bad" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">上述方法有一个大问题。在许多多智能体环境中，特别是在竞争环境中，智能体可以制定策略来适应其他智能体的行为。这使得策略变得脆弱、不稳定，并且通常是次优的。为了弥补这一点，MADDPG为每个代理训练了一组子策略。在每个时间步，代理随机选择一个子策略来选择操作。然后，执行。</p><p id="532e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">政策梯度略有改变。我们对<em class="ns"> K </em>子策略进行平均，使用线性期望值，并通过Q值函数传播更新。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/b22e3ee73681da439081bb93c0c70a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o1fmfHhPmNMc9ZGrzDRQHQ.png"/></div></div></figure><h1 id="3ccb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">后退一步</h1><p id="57cb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这概括了整个算法！在这一点上，重要的是后退一步，内化我们到底做了什么，并直观地理解它为什么有效。本质上，我们做了以下事情:</p><ul class=""><li id="15d1" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><strong class="lt iu">为仅使用本地观察的代理定义参与者</strong>。这有助于抑制呈指数增长的状态和动作空间的影响。</li><li id="74b5" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu">为每个使用联合信息的代理定义了一个集中的评论家</strong>。这有助于减少非平稳性的影响，并引导参与者使其对全球系统最优</li><li id="58b5" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu">定义策略推理网络</strong>以评估其他代理的策略。这有助于限制代理的相互依赖性，并消除代理对完美信息的需求。</li><li id="ba12" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu">已定义的策略集合</strong>，以减少过度适应其他代理策略的影响和可能性。</li></ul><p id="8c19" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">该算法的每个组件都服务于一个特定的委托目的。这就是MADDPG成为强大算法的原因:它的各种组件都经过精心设计，可以克服多智能体系统通常会遇到的巨大障碍。接下来，我们来看看算法的性能。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/5ecb140c39eac397d745c5a9d1758882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7XSGUAEq4bhkxd2o"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@retrosupply?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">反推</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">去飞溅</a>上拍摄</p></figure><h1 id="6dc4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结果</h1><p id="3e55" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">MADDPG在许多环境中进行了测试。有关其性能的完整概述，请随意查阅本文[1]。这里我们只讨论<em class="ns">协同通信</em>任务。</p><h2 id="5aaa" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">环境概述</h2><p id="50c8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这里有两个代理:一个说话者和一个听者。在每一次迭代中，听者会得到一个彩色的路标，并得到与路标距离成反比的奖励。这里有个问题:听者只知道它的相对位置和所有地标的颜色。它不知道应该去哪个地标。另一方面，说话者知道这一集的正确地标的颜色。因此，两个代理必须进行通信和协作来解决任务。</p><h2 id="e2f6" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">比较</h2><p id="d262" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这项任务中，该论文将MADDPG与最先进的单代理方法进行了对比。通过使用MADDPG，我们可以看到显著的改进。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/6901a5d4a6dda7bd60b4752b1e561660.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gdL3zpuEXOilTR28JY4BUQ.png"/></div></div></figure><p id="f3d4" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">还表明，即使策略并不完美，策略推断也能获得与使用真实策略观察相同的成功率。更好的是，收敛速度没有明显放缓。</p><p id="4f35" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">最后，政策组合显示了有希望的结果。论文[1]测试了竞争环境中的群体效应，并证明了比只有一种策略的代理人明显更好的性能。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="9f52" class="kz la it bd lb lc pg le lf lg ph li lj jz pi ka ll kc pj kd ln kf pk kg lp lq bi translated">结束语</h1><p id="af38" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">就是这样！在这里，我们综述了一种新的方法来解决多代理强化学习问题。当然，在“MARL保护伞”下有无穷无尽的方法，但是MADDPG为解决多代理系统的最大问题的方法提供了一个强有力的起点。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="acad" class="kz la it bd lb lc pg le lf lg ph li lj jz pi ka ll kc pj kd ln kf pk kg lp lq bi translated">参考</h1><p id="745e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1] R. Lowe，Y. Wu，A. Tamar，J. Harb，P. Abbeel，I. Mordatch，<a class="ae ky" href="https://arxiv.org/pdf/1706.02275.pdf" rel="noopener ugc nofollow" target="_blank">混合合作竞争环境下的多主体行动者-批评家</a> (2018)。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><blockquote class="ov ow ox"><p id="5e08" class="lr ls ns lt b lu mp ju lw lx mq jx lz oy nd mc md oz ne mg mh pa nf mk ml mm im bi translated">从经典到最新，这里有讨论多代理和单代理强化学习的相关文章:</p></blockquote><div class="od oe gp gr of og"><a rel="noopener follow" target="_blank" href="/how-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">DeepMind的虚幻算法解释</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">最佳深度强化学习</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="pr l or os ot op ou ks og"/></div></div></a></div><div class="od oe gp gr of og"><a rel="noopener follow" target="_blank" href="/hierarchical-reinforcement-learning-feudal-networks-44e2657526d7"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">分层强化学习:封建网络</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">让电脑看到更大的画面</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="ps l or os ot op ou ks og"/></div></div></a></div></div></div>    
</body>
</html>