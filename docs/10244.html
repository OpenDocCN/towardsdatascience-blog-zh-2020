<html>
<head>
<title>Learn AI Today 01: Getting started with Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">今日学习人工智能 Pytorch 入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learn-ai-today-01-getting-started-with-pytorch-2e3ba25a518?source=collection_archive---------31-----------------------#2020-07-19">https://towardsdatascience.com/learn-ai-today-01-getting-started-with-pytorch-2e3ba25a518?source=collection_archive---------31-----------------------#2020-07-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="f526" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="http://towardsdatascience.com/tagged/learn-ai-today" rel="noopener" target="_blank">今天学 AI</a></h2><div class=""/><div class=""><h2 id="ea5f" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">定义和训练 Pytorch 模型并动态可视化结果</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/f2c0ce505a88bd39480d0200a127aca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oyEEAx1t-pdzP-HEl9cXVQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@tateisimikito?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Jukan Tateisi </a>在<a class="ae le" href="https://unsplash.com/s/photos/stairs?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><p id="c476" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是今天我创作的<a class="ae le" href="http://learn-ai-today.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">学艾</strong> </a> <strong class="lh ja"> </strong>系列中的第一个故事！这些故事，或者至少是前几个，是基于我在研究/学习<strong class="lh ja"> PyTorch </strong>和<strong class="lh ja">深度学习</strong>时创作的一系列<strong class="lh ja"> Jupyter 笔记本</strong>。我希望你和我一样觉得它们很有用！</p><h2 id="fbb3" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">你将从这个故事中学到什么</h2><ul class=""><li id="b255" class="mt mu iq lh b li mv ll mw lo mx ls my lw mz ma na nb nc nd bi translated">如何创建 PyTorch 模型</li><li id="0c71" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">如何训练你的模型</li><li id="bc94" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">动态可视化培训进度</li><li id="4798" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">学习速度如何影响训练</li></ul><h1 id="a638" class="nj mc iq bd md nk nl nm mg nn no np mj kf nq kg mm ki nr kj mp kl ns km ms nt bi translated">1.PyTorch 中的线性回归</h1><p id="e52a" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">线性回归是一个你可能很熟悉的问题。最基本的形式就是用一条线来拟合一组点。</p><h2 id="5d8c" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">1.1 介绍概念</h2><p id="0ffe" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">考虑一条线的数学表达式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/cdcea67c656023d46bfe12ca4ae50edb.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*tv1dmTHMY5MSSF9Xc3pQ7A.png"/></div></figure><p id="5ebb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe ny nz oa ob b">w</code>和<code class="fe ny nz oa ob b">b</code>是该线性模型的两个<strong class="lh ja">参数</strong>或<strong class="lh ja">权重</strong>。在<strong class="lh ja">机器学习</strong>中，通常使用<code class="fe ny nz oa ob b">w</code>指<strong class="lh ja">重量</strong>和<code class="fe ny nz oa ob b">b</code>指<strong class="lh ja">偏差</strong>参数。</p><p id="3c45" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在机器学习中，当我们<strong class="lh ja">训练一个模型</strong>时，我们基本上是在为一组给定的输入/目标<code class="fe ny nz oa ob b">(x,y)</code>对寻找<strong class="lh ja">最佳参数</strong> <code class="fe ny nz oa ob b">w</code>和<code class="fe ny nz oa ob b">b</code>。在模型被训练之后，我们可以计算模型估计。该表达式现在将看起来</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/bf63a7317b4eb4bffc3dd2ca5c0ab337.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*CzGMs5QSgPB6a41UUwVzMQ.png"/></div></figure><p id="45fc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里我把<code class="fe ny nz oa ob b">y</code>的名字改成<code class="fe ny nz oa ob b">ye</code> (y 估计)，因为这个解不精确。</p><p id="4963" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">均方差(MSE) </strong>就是<code class="fe ny nz oa ob b">mean((ye-y)²)</code>——目标值和估计值之间的均方差。对于一个回归问题，你确实可以最小化<strong class="lh ja"> MSE </strong>以便找到最好的<code class="fe ny nz oa ob b">w</code>和<code class="fe ny nz oa ob b">b</code>。</p><p id="3bcf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">线性回归的思想可以用代数矩阵符号来概括，以允许多个输入和目标。如果你想了解更多关于回归问题的数学精确解，你可以搜索<a class="ae le" href="https://mathworld.wolfram.com/NormalEquation.html" rel="noopener ugc nofollow" target="_blank">正规方程</a>。</p><h2 id="4b49" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">1.2 定义模型</h2><p id="0f97" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">PyTorch <code class="fe ny nz oa ob b">nn.Linear</code>类是定义具有任意数量输入和输出的线性模型所需要的全部。对于将直线拟合到一组点的基本示例，考虑以下模型:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="769f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="of">注:</em> </strong> <em class="of">我使用的</em> <code class="fe ny nz oa ob b">Module</code> <em class="of">来自</em><a class="ae le" href="https://github.com/fastai/fastai" rel="noopener ugc nofollow" target="_blank"><em class="of">fastai</em></a><em class="of">库，因为它使代码更干净。如果你想使用纯 PyTorch，你应该使用</em> <code class="fe ny nz oa ob b">nn.Module</code> <em class="of">来代替，你需要在</em> <code class="fe ny nz oa ob b">__init__</code> <em class="of">方法中添加</em> <code class="fe ny nz oa ob b">super().__init__()</code> <em class="of">。fastai </em> <code class="fe ny nz oa ob b">Module</code> <em class="of">为你做到了。</em></p><p id="7ff2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你熟悉<strong class="lh ja"> Python 类</strong>，代码是不言自明的。如果没有，考虑在深入 PyTorch 之前做一些研究。有许多在线教程和课程涵盖了这一主题。</p><p id="6820" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">回到代码。在<code class="fe ny nz oa ob b">__init__</code>方法中，您定义了模型的层。在这种情况下，它只是一个线性层。然后，<code class="fe ny nz oa ob b">forward</code>方法就是当你调用模型时被调用的方法。类似于普通 Python 类中的<code class="fe ny nz oa ob b">__call__</code>方法。</p><p id="8996" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，您可以将 LinearRegression 模型的一个实例定义为<code class="fe ny nz oa ob b">model = LinearRegression(1, 1)</code>，表示输入和输出的数量。</p><p id="54e8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">也许你现在在问为什么我不简单地做<code class="fe ny nz oa ob b">model = nn.Linear(1, 1)</code>你是绝对正确的。我在定义<code class="fe ny nz oa ob b">LinearRegression</code>类时遇到这么多麻烦的原因只是为了给以后的改进提供一个模板，稍后你会发现。</p><h2 id="a83d" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">1.3 如何训练你的模型</h2><p id="7850" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">训练过程基于一系列 4 个步骤，反复重复:</p><ul class=""><li id="12ac" class="mt mu iq lh b li lj ll lm lo og ls oh lw oi ma na nb nc nd bi translated"><strong class="lh ja">正向传递:</strong>将输入数据提供给模型，并获得模型输出— <code class="fe ny nz oa ob b">outputs = model(inputs)</code></li><li id="5fab" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated"><strong class="lh ja">计算损失函数:</strong>对于线性回归问题，我们使用的损失函数是均方误差(MSE)。我们经常把这个函数称为标准— <code class="fe ny nz oa ob b">loss = criterion(outputs, targets)</code></li><li id="fe2f" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated"><strong class="lh ja">反向传递:</strong>计算损失函数相对于每个可学习参数的梯度。记住，我们要减少损失函数，使输出接近目标。梯度告诉我们，如果你增加或减少每个参数，损耗会如何变化— <code class="fe ny nz oa ob b">loss.backwards()</code></li><li id="0d45" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated"><strong class="lh ja">更新参数:</strong>在减少损失的方向少量更新参数值。更新参数的方法可以简单到减去乘以一个小数值的梯度值。这个数字被称为<strong class="lh ja">学习率</strong>，我刚刚描述的<strong class="lh ja">优化器</strong>是<strong class="lh ja">随机梯度下降(SGD) </strong> — <code class="fe ny nz oa ob b">optimizer.step()</code></li></ul><p id="17db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我还没有确切地定义<code class="fe ny nz oa ob b">criterion</code>和<code class="fe ny nz oa ob b">optimizer</code>，但我会在一分钟。这只是给你一个训练迭代步骤的总体概述和理解，或者通常称为<strong class="lh ja">训练时期</strong>。</p><p id="d321" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们定义我们的<code class="fe ny nz oa ob b">fit</code>函数，它将完成所有需要的步骤。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="3d4e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请注意，在 — <code class="fe ny nz oa ob b">optimizer.zero_grad()</code>之前，还有一个额外的步骤我没有提到。这是因为默认情况下，在 PyTorch 中，当您调用<code class="fe ny nz oa ob b">loss.backwards()</code>时，优化器会将梯度值相加。如果你不在每个时期将它们设置为零，那么它们将会一直累加，这是不可取的。除非你在做梯度积累——但那是一个更高级的话题。除此之外，正如你在上面的代码中看到的，我保存了每个时期的损失值。我们应该预计它会稳步下降——这意味着该模型在预测目标方面变得越来越好。</p><p id="09ae" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如我上面提到的，对于线性回归，通常使用的标准是<strong class="lh ja"> MSE </strong>。至于优化器，现在我总是把<strong class="lh ja"> Adam </strong>作为首选。它速度很快，应该可以很好地解决大多数问题。我不会详细说明 Adam 现在是如何工作的，但我们的想法总是在最短的时间内找到最佳解决方案。</p><p id="c7aa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在让我们继续创建我们的线性回归模型的实例，定义我们的<strong class="lh ja">标准</strong>和我们的<strong class="lh ja">优化器</strong>:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="4ba8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe ny nz oa ob b">model.parameters()</code>是向优化器提供可训练参数列表的方式，而<code class="fe ny nz oa ob b">lr</code>是学习率。</p><p id="bfff" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">现在让我们创建一些数据并训练模型！</strong></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="541a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">数据只是一组遵循模型<code class="fe ny nz oa ob b">y = 2x + 1 + noise</code>的点。为了让它更有趣一点，我让 x 的值越大噪声越大。第 4 行和第 5 行中的<code class="fe ny nz oa ob b">unsqueeze(-1)</code>只是在末尾给张量增加了一个额外的维度(从<code class="fe ny nz oa ob b">[10000]</code>到<code class="fe ny nz oa ob b">[10000,1]</code>)。数据是相同的，但张量需要有这样的形状，这意味着我们有 10000 个样本，每个样本有一个特征。</p><p id="f248" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">绘制数据，结果就是下图，可以看到真实的模型和输入数据+噪声。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/147f854e9cb9d01da8a508e05e120073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*xtfEJtNSGGtKfwiTIEFmPg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">线性回归模型的输入数据。图片由作者提供。</p></figure><p id="dafa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">现在为了训练模型我们只需运行我们的</strong> <code class="fe ny nz oa ob b"><strong class="lh ja">fit</strong></code> <strong class="lh ja">函数！</strong></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="11fb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">经过训练后，我们可以绘制 100 个时期内损失的演变。正如您在下图中看到的，最初的损失约为 2.0，然后急剧下降到接近零。<strong class="lh ja">这是意料之中的</strong>，因为当我们开始时，模型参数是<strong class="lh ja">随机初始化的</strong>，并且随着训练的进行，它们收敛到解决方案。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/bc3abb28b1e0acda4760b7695b852430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*fT24JbXmBMr-5Dn6GsFAqg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">100 个训练时期的损失演变(MSE)。图片由作者提供。</p></figure><p id="d039" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">注:</strong>试试玩学习率值，看看对训练有什么影响！</p><p id="5830" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">要检查训练好的模型的参数，可以在训练完模型后运行<code class="fe ny nz oa ob b">list(model.parameters())</code>。您将会看到，在这个示例中，它们非常接近 2.0 和 1.0，因为真正的模型是<code class="fe ny nz oa ob b">y = 2x + 1</code>。</p><p id="4c65" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">您现在可以计算模型估计值— <code class="fe ny nz oa ob b">ye = model(x_train)</code>。(请注意，在计算评估之前，您应该始终运行<code class="fe ny nz oa ob b">model.eval()</code>来将模型设置为评估模式。这对于这个简单的模型来说不会有什么不同，但是当我们开始使用批处理规范化和删除时，就会有所不同。)</p><p id="88ac" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">绘制预测图，您可以看到它几乎完美地匹配了真实数据，尽管事实上模型只能看到有噪声的数据。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/d6f37c32bdcbd59eae2dd8375fb6016b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*YVrpf4W3JsHBnN09fIVHJA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">可视化模型评估。图片由作者提供。</p></figure></div><div class="ab cl ok ol hu om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="ij ik il im in"><h1 id="e138" class="nj mc iq bd md nk or nm mg nn os np mj kf ot kg mm ki ou kj mp kl ov km ms nt bi translated">2.逐步走向多项式回归</h1><p id="e96b" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">既然我们已经使它适用于简单的情况，那么转移到更复杂的线性模型就非常简单了。第一步当然是生成这样的输入数据。对于这个例子，我认为模型<code class="fe ny nz oa ob b">y = 3x² + 2x + 1 + noise</code>如下:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/832ff9cb0e21dcc80bd11b6811c177e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*vvxEt_Kqq_ZWqL91LFf_0Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">多项式模型的输入数据。图片由作者提供。</p></figure><p id="af7c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请注意，这次输入形状是<code class="fe ny nz oa ob b">[1000, 2]</code>，因为我们有两个对应于<code class="fe ny nz oa ob b">x</code>和<code class="fe ny nz oa ob b">x²</code>的特征。这就是使用线性回归拟合多项式的方法！</p><p id="329e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">与前面的例子相比，现在唯一的区别是模型需要两个输入— <code class="fe ny nz oa ob b">model = LinearRegression(2,1)</code>。就是这样！现在，您可以按照完全相同的步骤来训练模型。</p><p id="0ec1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，让我们用一些动态的可视化来让事情变得更有趣一点吧！</p><h2 id="9e5a" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">2.1 动态可视化培训进度</h2><p id="e075" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">为了动画化训练的发展，我们需要更新拟合函数，以便也存储每一步的模型估计值。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="de4c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你可能注意到了一个‘新词’——<code class="fe ny nz oa ob b">detach()</code>(代码第 17 行)。这是告诉 PyTorch 从梯度计算图中分离变量(它将不再计算分离变量的梯度)。如果在分离之前尝试将张量转换为 NumPy，将会出现错误。</p><p id="70a0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">继续，您可以像以前一样重复相同的过程来训练模型。唯一的区别是<code class="fe ny nz oa ob b">fit2</code>函数也将返回每个训练时期的模型估计值。</p><p id="4c9d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">要创建培训的视频/gif，请看下面的代码:</strong></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="4e27" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe ny nz oa ob b">%%capture</code>告诉 Jupyter 抑制单元格的输出，因为我们将在下一个单元格中显示视频。然后，从第 3 行到第 10 行，我照常设置情节。不同之处在于模型预测。我将其初始化为空，然后使用<code class="fe ny nz oa ob b">matplotlib.animation</code>迭代更新图形以生成动画。最后，可以使用来自<code class="fe ny nz oa ob b">IPython.display</code>的<code class="fe ny nz oa ob b">HTML</code>渲染视频。看看下面的结果！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/b1904f0c6ebfb958e8788a9e910d7a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*mky-IXWxsNQ-sbr0mk0MBQ.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在训练期间可视化模型预测。作者的动画。</p></figure><p id="f3e1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有趣的是，蓝线最初非常快地弯曲成正确的形状，然后为了最终的解决方案收敛得更慢！</p><p id="8cb1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">注:</strong>尝试使用<strong class="lh ja">学习率</strong>、不同的<strong class="lh ja">优化器</strong>以及任何您能想到的东西，看看对优化的影响。<strong class="lh ja">这是一个直观了解优化工作原理的好方法！</strong></p></div><div class="ab cl ok ol hu om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="ij ik il im in"><h1 id="3b53" class="nj mc iq bd md nk or nm mg nn os np mj kf ot kg mm ki ou kj mp kl ov km ms nt bi translated">3.神经网络模型</h1><p id="8a62" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">上面的例子对于学习和实验来说很有趣。然而，在实践中，你的数据通常不是由多项式生成的，或者至少你不知道多项式的项是什么。关于<strong class="lh ja">神经网络</strong>的一个好处是你不需要担心它！</p><p id="974b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们从定义我命名为<code class="fe ny nz oa ob b">GeneralFit</code>的模型开始:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="667e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个模型中有一些新的方面需要考虑。有 3 个线性层，正如你在正向方法中看到的，在前两个线性层之后，使用了一个<strong class="lh ja"> ReLU </strong> <strong class="lh ja">激活函数</strong> — <code class="fe ny nz oa ob b">F.relu()</code> —。<strong class="lh ja"> ReLU </strong>代表<strong class="lh ja">整流线性单元</strong>，简单来说就是<strong class="lh ja">将所有负值归零</strong>。然而，这个看似琐碎的操作足以<strong class="lh ja">使模型非线性。</strong></p><p id="25bc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">注意，线性层只是矩阵乘法。</strong>如果你有一个接一个的 100 个线性图层，线性代数会告诉你有一个线性图层执行相同的操作。这个线性层是 100 个矩阵的简单乘积。然而，当你引入非线性激活函数时，这就完全改变了。现在，您可以继续添加更多的线性层与非线性激活交错，如 ReLU(在最近的模型中最常见)。</p><p id="80ca" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一个<strong class="lh ja">深度神经网络</strong>只不过是一个具有几个“隐藏”层的神经网络。回头看看上面的代码，例如，你可以尝试添加更多的“隐藏”层并训练模型。事实上，你可以称之为深度学习。(请注意，隐藏层只是输入层和输出层之间任何层的传统名称。)</p><p id="5f88" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用上述模型和一组新生成的数据，我获得了以下训练动画:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/c01e472cea99b5ff9b15cffc3ee21958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*SSKbY_B5lsAxFqGe7uCnvg.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">以 0.01 的学习率在训练期间可视化 GeneralFit 模型的模型预测。作者的动画。</p></figure><p id="33a1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于这个例子，我训练了 200 个纪元，学习率为 0.01。让我们试着将学习率设置为 1。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/1f986a7f57b06583110013cda92cfd05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*Aofhzgy8p8D-FGrIv6rLnA.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在学习率为 1 的训练期间可视化 GeneralFit 模型的模型预测。作者的动画。</p></figure><p id="1a4a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">显然这样不好！</strong>当<strong class="lh ja">学习率过高</strong>时，模型可能无法正确收敛到一个好的解，甚至可能发散。如果你把学习率设定为 10 或 100，它不会有任何进展。</p></div><div class="ab cl ok ol hu om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="ij ik il im in"><h1 id="2b11" class="nj mc iq bd md nk or nm mg nn os np mj kf ot kg mm ki ou kj mp kl ov km ms nt bi translated">家庭作业</h1><p id="29ea" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">我可以给你看一千个例子，但如果你能自己做一两个实验，你会学到更多！我给你们展示的这些实验的完整代码可以在这个笔记本上找到。</p><ul class=""><li id="2349" class="mt mu iq lh b li lj ll lm lo og ls oh lw oi ma na nb nc nd bi translated">试着玩一下<strong class="lh ja">学习率</strong>，历元数，隐藏层数，隐藏层数的大小；</li><li id="d73c" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">也试试 SGD optimizer，玩玩学习率，也许还玩玩动量(我在这个故事中没有涉及到，但现在你知道了，你可以做一些研究)；</li></ul><p id="4c3c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你通过实验创造了有趣的动画笔记本，那就在 GitHub、Kaggle 上分享吧，或者写一个关于它的故事！</p></div><div class="ab cl ok ol hu om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="ij ik il im in"><h1 id="e12a" class="nj mc iq bd md nk or nm mg nn os np mj kf ot kg mm ki ou kj mp kl ov km ms nt bi translated">结束语</h1><p id="b39e" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated"><strong class="lh ja">今日学 AI</strong>系列第一个故事到此结束！</p><ul class=""><li id="cf01" class="mt mu iq lh b li lj ll lm lo og ls oh lw oi ma na nb nc nd bi translated">请考虑<a class="ae le" href="https://docs.google.com/forms/d/e/1FAIpQLSc0IBzdCn7osIjvGno1GjBakI-DfXHE8gDLZ--jNzWsXtRW0g/viewform" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">在此链接加入我的邮件列表</strong> </a> <strong class="lh ja"> </strong>获取更新，以便您不会错过以下任何故事或重要更新！</li><li id="e8f5" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">我还会在 learn-ai-today.com<a class="ae le" href="http://learn-ai-today.com/" rel="noopener ugc nofollow" target="_blank"><strong class="lh ja"/></a>列出新的故事，这是我为这次学习之旅创建的页面！</li><li id="e983" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">如果你以前错过了，<a class="ae le" href="https://www.kaggle.com/mnpinto/learn-ai-today-01-getting-started-with-pytorch/" rel="noopener ugc nofollow" target="_blank">这是 Kaggle 笔记本的链接，上面有这个故事的代码</a>！</li></ul><p id="e00b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">欢迎在评论中给我一些反馈。你觉得什么最有用，或者什么可以解释得更好？让我知道！</p><p id="3dd3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本系列的下一个故事:</p><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/learn-ai-today-02-introduction-to-classification-problems-using-pytorch-b710918cba63"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd ja gy z fp pg fr fs ph fu fw iz bi translated">今日学习人工智能:02 —使用 PyTorch 解决分类问题简介</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">用神经网络对花卉进行分类，可视化决策边界和理解过度拟合。</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pl l pm pn po pk pp ky pb"/></div></div></a></div><p id="3ad5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你可以在下面的故事中了解更多关于我的旅程！</p><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/my-3-year-journey-from-zero-python-to-deep-learning-competition-master-6605c188eec7"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd ja gy z fp pg fr fs ph fu fw iz bi translated">我的 3 年历程:从零 Python 到深度学习竞赛高手</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">自从 2017 年开始学习 Python 以来，我一直遵循的道路是成为一名独自参加 Kaggle 比赛的大师…</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pq l pm pn po pk pp ky pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/my-2-year-journey-on-kaggle-how-i-became-a-competition-master-ef0f0955c35d"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd ja gy z fp pg fr fs ph fu fw iz bi translated">我在 Kaggle 上的两年旅程:我如何成为竞赛大师</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">描述我的旅程和策略，我遵循成为一个竞赛大师与个人金牌</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pr l pm pn po pk pp ky pb"/></div></div></a></div><p id="cfd1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="of">感谢阅读！祝您愉快！</em></p></div></div>    
</body>
</html>