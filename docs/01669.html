<html>
<head>
<title>Self-Paced Learning for Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于机器学习的自定进度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-paced-learning-for-machine-learning-f1c489316c61?source=collection_archive---------14-----------------------#2020-02-15">https://towardsdatascience.com/self-paced-learning-for-machine-learning-f1c489316c61?source=collection_archive---------14-----------------------#2020-02-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6dcd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">提高神经网络收敛的聪明方法(并发现异常…)</h2></div><p id="d575" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你通过无情地向你的神经网络灌输数据来折磨它！通常，当用随机梯度下降(SGD)训练机器学习模型时，训练数据集会被打乱。通过这种方式，我们可以确保模型以无特定顺序看到不同的数据点，并且可以均衡地学习任务，而不会陷入局部最优。然而，早在2009年，Bengio等人就证明了某种排序是有益的。他们将他们的方法称为<em class="lb">课程学习</em>，并表明如果机器学习模型的训练数据按照特定的顺序，它会达到更高的准确性。更准确地说，开头的例子比较容易，结尾的例子比较难。在他们的实验中，他们使用了一个已经训练好的模型，并让这个模型决定什么样的数据点容易或难。然后，一个新的模型在正确排序的数据集上进行训练，并收敛到比随机排序或相反课程训练的模型更高的精度。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="8ad1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我目前的项目中，我遇到了一种叫做<em class="lb">的自定进度学习</em> (SPL)的技术。这不是一个新的想法，相应的论文发表于大约10年前。无论如何，这种技术非常有趣而且仍然重要，因为它有助于随机梯度下降(SGD)更快地收敛，甚至更高的精度。它会跳过某些被认为太难的数据点。它基于课程学习，但在训练时对数据进行分类。不需要额外的预训练模型来决定排序。因此得名<em class="lb">自学</em>。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/17fdf05b140251cbb5cd0a3e2aa1f87d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*r1rTGQHEq_wApmD1"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">苏珊·d·威廉姆斯在<a class="ae lz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="b9a0" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">SPL背后的直觉</h1><p id="72d5" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">自定进度学习这个术语起源于人类使用的一种学习技术。它允许你定义自己的速度，以适应你的学习模式。SPL可以被看作是在学习或训练一项特殊技能，例如数学。当我们开始学习数学时，我们从数数开始，然后是加法、减法等等。我们直到某个年龄才听说矩阵乘法或者导数。同样，机器学习的SPL从非常简单的例子开始，一旦学会，就从已经学会的“基础”中受益，继续学习更难的例子。我把SPL想象成随着时间的推移缩小任务范围的一种方式。考虑一个二维空间中的简单分类任务和一个需要在正确点分割两个点云的模型。较容易的样本远离相交区域。较硬的样品靠近相交区域。模型的初始状态是这个空间某处的一条线。如果我们只从简单的数据点开始，模型会得到告诉它向某个方向前进的梯度。如果我们只从硬点开始，我们的模型会知道它是错误的，会得到一个方向，但可能会远离另一边。将点缩小到正确的区域有助于模型避免超调并更平滑地收敛，正如我在下面创建的动画中看到的那样。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/a3e43d5ee250d2e6ca2bd1559a626621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*LSHmNrrIRy2VttfICz9Y9Q.gif"/></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">简单数据集上的自定进度学习</p></figure><h1 id="d88f" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">该算法</h1><p id="c48d" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">诀窍很简单。它使用一个阈值，我们称之为<strong class="kh ir">λ</strong>。它的存在是为了与训练集中数据点的损失值进行比较。通常，<strong class="kh ir">λ</strong>从一个接近0的数开始非常低。对于每个时期，<strong class="kh ir">λ</strong>被乘以一个大于1的固定因子。被训练的模型必须计算其训练点的损失值以执行SGD。通常，这些损失值随着进一步的训练迭代而变得更小，因为模型在训练任务方面变得更好，并且犯的错误更少。阈值<strong class="kh ir">λ</strong>现在确定数据点是被认为<em class="lb">容易</em>还是<em class="lb">困难</em>。每当一个数据点的损失低于<strong class="kh ir">λ</strong>时，它就是一个容易的数据点。如果是上面的，那就算辛苦了。在训练期间，反向传播步骤仅在容易的数据点上执行，而困难的数据点被跳过。因此，该模型在训练过程中每当其进展足够大时就增加训练实例的难度。当然，在开始时，模型可能认为没有数据点是容易的，根本不会训练。因此，SPL的作者引入了一个热身阶段，在这个阶段不允许跳跃，只使用训练集的一小部分。</p><h1 id="0641" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">数学</h1><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi my"><img src="../Images/67ef4c588ef7bbb2d3da322451a4761e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tvd-fE2UTcXhnWAq.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">损失函数最小化</p></figure><p id="928f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SPL的论文介绍了一种两步学习法。这里，损失函数被训练两次，保持一些变量对于每一步是固定的。给定模型权重“w”和变量“v”，损失函数需要最小化。在损失函数中，我们看到几项。第一项“r(w)”是一个常用的正则项，有助于模型避免过度拟合。这也是其他与SPL无关的损失函数的一部分。第二项是我们模型的数据点损失“f(x，y，w)”乘以变量“v”的总和。这个变量“v”稍后将决定当前数据点“(x，y)”是否足够容易进行训练。第三项是所有“v”的总和乘以阈值“lambda ”,我们在前面的章节中已经提到过。变量“v”是整数，只能取值“0”或“1”。在第一学习步骤中，变量“w”是固定的，只有变量“v”根据优化而改变。如果你仔细观察损失函数，我们会发现“λ”确实起到了阈值的作用。如果“f(x，y，w)”小于λ，并且“v”是1，我们将从正则项中减去一些。因此，在“v=0”的情况下，我们不会减去任何东西，这比减去一些东西要大。如果“f(x，y，w)”大于λ，并且“v=1”，“f(x，y，w)-λ”将为正，我们将添加一些东西。因此，在“v=0”的情况下，我们不会添加任何东西，这比添加一些东西要小。总之，每当“f(x，y，w)”小于“λ”时，第一步通过将“v”设置为“1”来优化“L”，否则设置为“0”。第二步固定之前计算的“v”并优化“w”。如果“v”是“1”，则执行通常的模型更新，例如反向传播。如果“v”是“0”，则“f(x，y，w)”的梯度也将是0，并且不执行更新(除了正则化项，但是为了更好地理解，现在可以忽略它)。在开始时，将阈值设置为非常低的数字将不会产生任何结果，因为所有的“v”都将是“0 ”,因为没有数据点丢失会低于阈值。因此，SPL的作者建议在没有SPL的情况下进行一定次数的迭代，然后从SPL开始。</p><h1 id="96af" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">PyTorch实现</h1><p id="0b98" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">在下面的代码示例中，我展示了如何在虚拟数据集上用PyTorch实现SPL。特别是，我们将为我在这篇博文开头展示的动画进行训练。首先，我们将定义一个非常简单的模型，接受2个特征并输出两个数字，这两个数字定义了每个类别的概率。输出告诉我们模型认为它看到的是什么类。为了将输出转换成概率，我们使用了softmax函数。不过，在代码中，我使用了log_softmax函数。这是由于我稍后使用的损失函数。最后，模型以同样的方式训练。</p><pre class="lk ll lm ln gt mz na nb nc aw nd bi"><span id="3e6f" class="ne mb iq na b gy nf ng l nh ni"><strong class="na ir">import </strong>torch<br/><strong class="na ir">import </strong>torch.nn <strong class="na ir">as </strong>nn<br/><br/><br/><strong class="na ir">class </strong>Model(nn.Module):<br/>    <strong class="na ir">def </strong>__init__(self, input_size, output_size):<br/>        super(Model, self).__init__()<br/>        self.input_layer = nn.Linear(input_size, output_size)<br/><br/>    <strong class="na ir">def </strong>forward(self, x):<br/>        x = self.input_layer(x)<br/>        <strong class="na ir">return </strong>torch.log_softmax(x, dim=1)</span></pre><p id="2044" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">损失函数代码可以在下一节看到。这里我们计算每个点的损耗，也就是NLL损耗。如果损失小于阈值，我们将损失乘以1，否则乘以0。因此，零乘损失对训练没有任何影响。</p><pre class="lk ll lm ln gt mz na nb nc aw nd bi"><span id="5dcb" class="ne mb iq na b gy nf ng l nh ni"><strong class="na ir">import </strong>torch<br/><strong class="na ir">from </strong>torch <strong class="na ir">import </strong>Tensor<br/><strong class="na ir">import </strong>torch.nn <strong class="na ir">as </strong>nn<br/><br/><br/><strong class="na ir">class </strong>SPLLoss(nn.NLLLoss):<br/>    <strong class="na ir">def </strong>__init__(self, *args, n_samples=0, **kwargs):<br/>        super(SPLLoss, self).__init__(*args, **kwargs)<br/>        self.threshold = 0.1<br/>        self.growing_factor = 1.3<br/>        self.v = torch.zeros(n_samples).int()<br/><br/>    <strong class="na ir">def </strong>forward(self, input: Tensor, target: Tensor, index: Tensor) -&gt; Tensor:<br/>        super_loss = nn.functional.nll_loss(input, target, reduction=<strong class="na ir">"none"</strong>)<br/>        v = self.spl_loss(super_loss)<br/>        self.v[index] = v<br/>        <strong class="na ir">return </strong>(super_loss * v).mean()<br/><br/>    <strong class="na ir">def </strong>increase_threshold(self):<br/>        self.threshold *= self.growing_factor<br/><br/>    <strong class="na ir">def </strong>spl_loss(self, super_loss):<br/>        v = super_loss &lt; self.threshold<br/>        <strong class="na ir">return </strong>v.int()</span></pre><p id="ad92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最终，训练函数看起来和往常一样。我们加载一个数据加载器，初始化模型和优化器，并开始多次遍历数据集。为了简单起见，我省略了动画的绘图功能。</p><pre class="lk ll lm ln gt mz na nb nc aw nd bi"><span id="b447" class="ne mb iq na b gy nf ng l nh ni"><strong class="na ir">import </strong>torch.optim <strong class="na ir">as </strong>optim<br/><br/><strong class="na ir">from </strong>model <strong class="na ir">import </strong>Model<br/><strong class="na ir">from </strong>dataset <strong class="na ir">import </strong>get_dataloader<br/><strong class="na ir">from </strong>loss <strong class="na ir">import </strong>SPLLoss<br/><br/><br/><strong class="na ir">def </strong>train():<br/>    model = Model(2, 2, 2, 0)<br/>    dataloader = get_dataloader()<br/>    criterion = SPLLoss(n_samples=len(dataloader.dataset))<br/>    optimizer = optim.Adam(model.parameters())<br/><br/>    <strong class="na ir">for </strong>epoch <strong class="na ir">in </strong>range(10):<br/>        <strong class="na ir">for </strong>index, data, target <strong class="na ir">in </strong>dataloader:<br/>            optimizer.zero_grad()<br/>            output = model(data)<br/>            loss = criterion(output, target, index)<br/>            loss.backward()<br/>            optimizer.step()<br/>        criterion.increase_threshold()<br/>    <strong class="na ir">return</strong> model</span></pre><p id="faf4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">整个项目可以在我的<a class="ae lz" href="https://github.com/wenig/spl" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中找到。你可以随意摆弄它。此外，绘图功能可以在那里找到。</p><h1 id="f46b" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">异常检测</h1><p id="db25" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">由于SPL方法是以某种方式根据基于损失的硬度对数据点进行排序，所以我有了用它进行异常检测的想法。异常是指与数据集中的任何其他数据点都不相似的数据点，距离很远，可能是错误输入或系统错误的结果。如果数据集中出现异常，其损失应该高于正常点的损失，因为机器学习模型不能概括错误，如果它们很少出现。SPL的方法最终应该会越过异常点。这样，通过观察数据点的“激活”顺序，我们可以很容易地将它们归类为异常，即认为它们很容易。</p><p id="7e61" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这个实验，我使用了前面提到的代码，并且没有运行固定数量的时期。相反，只要有超过阈值的5个数据点，我就运行训练，因此被认为是困难的。一旦我有5个或更少，我就停止训练，把它们标为红点。正如你在下面的动画中看到的，算法在蓝云的左下部分发现了异常。我添加了这个异常，从橙色质心取最远的点，并将其类改为“橙色”。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/0eb7232e362a5c2472f19949d9a8fb38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*oEx7WKbMAn8qedxQv1dTfg.gif"/></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">最后用红点表示异常</p></figure><p id="6764" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，这个例子并不难，但它说明了异常检测面临的问题。如果维度多于2或3，任务会变得更加复杂，并且会出现明显的异常，就像我们的例子中发现的那样不容易。</p><h1 id="f5db" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">结论</h1><p id="cc0e" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">为什么不是每个人都用SPL？找到阈值的正确起始因子值和增长因子值需要一些时间，因为这不是通用的，会因模型、损失函数和数据集而异。对于我在这篇文章中使用的例子，我不得不多次尝试，最终找到正确的配置。假设你有一个非常大的数据集和一个非常大的模型。在你开始实际训练之前，多次检查整个进度基本上是不可行的。然而，还有多种适合不同培训设置的其他课程学习技巧。尽管有这些观点，目前的想法是一个非常直观的想法，很容易掌握，工作起来也很有趣。它基本上只是你需要优化的另一组超参数；-)</p><h1 id="56b9" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">资源</h1><h2 id="e5d8" class="ne mb iq bd mc nj nk dn mg nl nm dp mk ko nn no mm ks np nq mo kw nr ns mq nt bi translated">链接</h2><ul class=""><li id="e777" class="nu nv iq kh b ki ms kl mt ko nw ks nx kw ny la nz oa ob oc bi translated"><a class="ae lz" href="https://github.com/wenig/spl" rel="noopener ugc nofollow" target="_blank">带有示例的GitHub库</a></li></ul><h2 id="a6ee" class="ne mb iq bd mc nj nk dn mg nl nm dp mk ko nn no mm ks np nq mo kw nr ns mq nt bi translated">报纸</h2><ul class=""><li id="5af4" class="nu nv iq kh b ki ms kl mt ko nw ks nx kw ny la nz oa ob oc bi translated"><a class="ae lz" href="https://dl.acm.org/doi/abs/10.1145/1553374.1553380?casa_token=ep1KmLpw4FIAAAAA:_-S1hEHrOehjJkn164s4y1RAkDyNUmL5PG1YvpdwtunO06HQ7L53kGLc9TgIVvPBTOouvTY3zMsW" rel="noopener ugc nofollow" target="_blank">课程学习</a></li><li id="f418" class="nu nv iq kh b ki od kl oe ko of ks og kw oh la nz oa ob oc bi translated"><a class="ae lz" href="https://papers.nips.cc/paper/3923-self-paced-learning-for-latent-variable-models" rel="noopener ugc nofollow" target="_blank">潜在变量模型的自定步调学习</a></li></ul></div></div>    
</body>
</html>