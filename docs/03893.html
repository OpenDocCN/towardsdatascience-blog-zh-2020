<html>
<head>
<title>Linear Regression by Hand</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">手工线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-by-hand-ee7fe5a751bf?source=collection_archive---------6-----------------------#2020-04-11">https://towardsdatascience.com/linear-regression-by-hand-ee7fe5a751bf?source=collection_archive---------6-----------------------#2020-04-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c78c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">线性回归是数据科学家最基本也是最强大的工具。让我们仔细看看最小二乘直线和相关系数。</h2></div><h1 id="518f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">线性回归的发明</h1><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi la"><img src="../Images/422f34fa738584ecb6075a74af345fd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PZlezFfELTQbIz8Xoq-prA.jpeg"/></div></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">Johannes Plenio 在<a class="ae lq" href="https://unsplash.com/s/photos/sailing-ships?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="521c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">线性回归是线性代数的一种形式，据称是由卡尔·弗里德里希·高斯(1777-1855)发明的，但最早发表在阿德里安·玛丽·勒让德(1752-1833)的一篇科学论文中。高斯用最小二乘法猜测谷神星小行星何时何地会出现在夜空中(统计回归的发现，2015)。这不是一个爱好项目，这是一个资金充足的研究项目，目的是为了海洋导航，这是一个高度竞争的领域，对技术中断非常敏感。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="01be" class="ki kj it bd kk kl mu kn ko kp mv kr ks jz mw ka ku kc mx kd kw kf my kg ky kz bi translated">线性回归原理</h1><p id="c87d" class="pw-post-body-paragraph lr ls it lt b lu mz ju lw lx na jx lz ma nb mc md me nc mg mh mi nd mk ml mm im bi translated">线性回归是一种从x预测<strong class="lt iu"> <em class="ne"> y的方法</em> </strong>在我们的例子中，<strong class="lt iu"> <em class="ne"> y是因变量，x是自变量。</em> </strong>我们想要预测给定的<strong class="lt iu"/>x值的<strong class="lt iu"> y </strong>值。现在，如果数据是完全线性的，我们可以简单地根据<strong class="lt iu"> <em class="ne"> y = mx+ b </em> </strong>计算直线的斜率截距形式。要预测<strong class="lt iu"> <em class="ne"> y </em> </strong>，我们只需插入给定的<strong class="lt iu"> <em class="ne"> x和b的值。</em> </strong>在现实世界中，我们的数据不会是完全线性的。它很可能以<strong class="lt iu">散点图</strong>上的数据点簇的形式出现。从散点图中，我们将确定，<strong class="lt iu"> <em class="ne">描述数据线性质量的最佳拟合线</em> </strong>是什么，以及<strong class="lt iu"> <em class="ne">该线与点群的拟合程度如何？</em> </strong></p><blockquote class="nf ng nh"><p id="8d6d" class="lr ls ne lt b lu lv ju lw lx ly jx lz ni mb mc md nj mf mg mh nk mj mk ml mm im bi translated">线性回归试图通过将线性方程拟合到观察到的数据来模拟两个变量之间的关系(<a class="ae lq" href="http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm" rel="noopener ugc nofollow" target="_blank"> <em class="it">线性回归</em> </a>，n.d .)。</p></blockquote><h1 id="8bbd" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">散点图</h1><p id="da6d" class="pw-post-body-paragraph lr ls it lt b lu mz ju lw lx na jx lz ma nb mc md me nc mg mh mi nd mk ml mm im bi translated">让我们编造一些数据作为例子。黑猩猩狩猎团体的规模和成功狩猎的百分比之间的关系已经被很好的记录了。(Busse，1978)我将从Busse获取一些数据点用于本文，并使用seaborn散点图绘制数据。注意到我在数据中画的线并不完全符合它，但是这些点近似于一个线性模式吗？我通过数据画出的线是<strong class="lt iu">最小二乘法线</strong>，用来预测给定x值 的<strong class="lt iu"> <em class="ne"> y值。仅使用手工绘制的基本最小二乘法线，我们可以预测4只黑猩猩的狩猎队将有大约52%的成功率。我们不是100%准确，但随着更多的数据，我们可能会提高我们的准确性。数据与<strong class="lt iu">最小二乘法直线</strong>的拟合程度就是<strong class="lt iu">相关系数</strong>。</em></strong></p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi nl"><img src="../Images/df5d27744e55a116f5f442f5392232f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zPrANde5QS28vb30USMg6A.png"/></div></div></figure><h1 id="5664" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">最小平方线</h1><p id="3c4a" class="pw-post-body-paragraph lr ls it lt b lu mz ju lw lx na jx lz ma nb mc md me nc mg mh mi nd mk ml mm im bi translated">在上面的图表中，我只是通过我判断为最佳拟合的数据手工画了一条线。我们要用斜率截距形式<strong class="lt iu"> <em class="ne"> y = mx + b </em> </strong>来计算这条线，才能做出真正的预测。我们寻求的是一条直线，在这条直线上，直线和每个点之间的差异尽可能小。这是最佳拟合线。</p><blockquote class="nf ng nh"><p id="a516" class="lr ls ne lt b lu lv ju lw lx ly jx lz ni mb mc md nj mf mg mh nk mj mk ml mm im bi translated">最小二乘直线被定义为从数据点到直线的垂直距离的平方和尽可能小的直线(Lial，格伦威尔和Ritchey，2016)。</p></blockquote><p id="27ca" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最小二乘法直线有两个分量:斜率<strong class="lt iu"> <em class="ne"> m、</em> </strong>和y截距<strong class="lt iu"> <em class="ne"> b. </em> </strong>我们将首先求解<strong class="lt iu"> <em class="ne"> m </em> </strong>，然后求解<strong class="lt iu"><em class="ne">b .</em></strong><strong class="lt iu"><em class="ne">m</em></strong>和<strong class="lt iu"> <em class="ne"> b </em> </strong>的方程为:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi nm"><img src="../Images/e9cb109deb86237c2297d576eb4d7f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a_LOmWEm2KHW4cpDho3A_w.png"/></div></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">在MS Word公式编辑器中创建</p></figure><p id="4b4d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">那可是好多Sigmas)！。不过不用担心，适马只是表示“的总和”，比如“x的总和”，用∑x来象征，也就是x列的总和，“黑猩猩的数量”我们需要计算∑x、∑y、∑xy、∑x和∑y。然后，将每一个部分输入到等式中，用于计算<strong class="lt iu"><em class="ne"/></strong>和<strong class="lt iu"><em class="ne">b。</em></strong>根据我们的原始数据集创建下表。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi nn"><img src="../Images/42801ded17b553a1e05a5d26b4b08650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EwvjzWR9YC_51QfrZHRArA.png"/></div></div></figure><p id="7bbd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在很简单，将我们的适马值插入到m和b的公式中。n是数据集中值的数量，在我们的例子中是8。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi no"><img src="../Images/8fbe085344fbbce9f2cbef3a70baf403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h4zYPIEs2GLHlw8pBSWTdw.png"/></div></div></figure><p id="af3f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">你有它！你可以根据给定的<strong class="lt iu"> <em class="ne"> x </em> </strong>的值来预测<strong class="lt iu"> <em class="ne"> y </em> </strong>，使用你的等式:<strong class="lt iu"> <em class="ne"> y = 5.4405x + 31.6429。</em> </strong> <em class="ne"> </em>这意味着我们的线从<strong class="lt iu"> 31.6429 </strong>开始，每有一只黑猩猩加入狩猎队，y值就会增加<strong class="lt iu"> 5.4405 </strong>个百分点。为了验证这一点，让我们来预测4只黑猩猩的狩猎成功率。</p><pre class="lb lc ld le gt np nq nr ns aw nt bi"><span id="e205" class="nu kj it nq b gy nv nw l nx ny"><strong class="nq iu"><em class="ne">y = 5.4405(4)+31.6429</em></strong>, which results in <strong class="nq iu">y=53.4</strong></span></pre><p id="1169" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们刚刚预测了黑猩猩狩猎队狩猎成功的百分比，这仅仅是基于对他们群体大小的了解，这是相当惊人的！</p><p id="b83d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们使用python在之前的散点图上绘制最小二乘直线，以展示它如何拟合数据。<code class="fe nz oa ob nq b">Seaborn.regplot()</code>是在这种情况下使用的一个很好的图表，但是出于演示的目的，我将手动创建<strong class="lt iu"> <em class="ne"> y=mx+b线</em> </strong>并将其放置在seaborn图表上。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi oe"><img src="../Images/af0d0944c7210649e5ba250b7a5dd3c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*010BzoCEYQCPYkAv7_SNUw.png"/></div></div></figure><p id="3bb7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">然而，现在您可以进行预测了，您需要用<strong class="lt iu">相关系数</strong>来限定您的预测，相关系数描述了数据与您的计算线的吻合程度。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="2fd2" class="ki kj it bd kk kl mu kn ko kp mv kr ks jz mw ka ku kc mx kd kw kf my kg ky kz bi translated">相关系数</h1><p id="3777" class="pw-post-body-paragraph lr ls it lt b lu mz ju lw lx na jx lz ma nb mc md me nc mg mh mi nd mk ml mm im bi translated">我们使用相关系数来确定最小二乘法是否是我们数据的好模型。如果数据点不是线性的，那么直线就不是正确的预测模型。<strong class="lt iu">卡尔·皮尔逊</strong>发明了相关系数<strong class="lt iu"> <em class="ne"> r </em> </strong>，介于1和-1之间，衡量两个变量之间线性关系的强度(Lial，格伦威尔和Ritchey，2016)。如果<strong class="lt iu"> <em class="ne"> r </em> </strong>正好是-1或1，则表示数据正好符合<em class="ne">、</em>线，没有偏离线。<strong class="lt iu"> <em class="ne"> r=0 </em> </strong>表示没有线性相关。当<strong class="lt iu"> <em class="ne"> r </em> </strong> <strong class="lt iu"> <em class="ne">值</em> </strong>接近零时，意味着关联度也降低。</p><p id="af68" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">相关系数由以下公式描述</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi of"><img src="../Images/335d5086356a3203042a33fcfe98e883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Bqrl8wpIdUQXT4l1iC87Q.png"/></div></div></figure><p id="7965" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">幸运的是，这些适马值已经在前面的表格中计算过了。我们简单地把它们代入我们的方程。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi of"><img src="../Images/8a776cf08c183c17cd70b2b18b54ff9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jGDq9KCLaAgNfE4h8iDObQ.png"/></div></div></figure><p id="ab45" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们的值接近正1，这意味着数据是高度相关的，并且是正的。你可以通过观察散点图上的最小二乘方线来确定这一点，但是相关系数给了你科学的证据！</p><h1 id="0fa2" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="92d8" class="pw-post-body-paragraph lr ls it lt b lu mz ju lw lx na jx lz ma nb mc md me nc mg mh mi nd mk ml mm im bi translated">线性回归是数据科学家或统计学家可用的最佳机器学习方法之一。有许多方法可以使用您的编程技能来创建机器学习模型，但让您自己熟悉模型使用的数学绝对是一个好主意。</p><h2 id="5183" class="nu kj it bd kk og oh dn ko oi oj dp ks ma ok ol ku me om on kw mi oo op ky oq bi translated">参考</h2><p id="fb1d" class="pw-post-body-paragraph lr ls it lt b lu mz ju lw lx na jx lz ma nb mc md me nc mg mh mi nd mk ml mm im bi translated">布塞博士(1978年)。黑猩猩会合作捕猎吗？<em class="ne">美国博物学家</em>，<em class="ne"> 112 </em> (986)，767–770。【https://doi.org/10.1086/283318 T4】</p><p id="f838" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">利亚尔，格伦威尔和里奇(2016)。<em class="ne">有限数学与微积分及应用，第10版</em>。纽约州纽约市:皮尔森[ISBN-13 9780133981070]。</p><p id="273e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><em class="ne">线性回归</em>。(未注明)。检索于2020年4月11日，来自<a class="ae lq" href="http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm" rel="noopener ugc nofollow" target="_blank">http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm</a></p><p id="75cc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">统计回归的发现。(2015年11月6日)。价格经济学。<a class="ae lq" href="http://priceonomics.com/the-discovery-of-statistical-regression/" rel="noopener ugc nofollow" target="_blank">http://priceonomics . com/the-discovery-of-statistical-regression/</a></p></div></div>    
</body>
</html>