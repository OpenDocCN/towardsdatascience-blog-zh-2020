<html>
<head>
<title>Python: Implementing Matrix Factorization from Scratch!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python:从头开始实现矩阵分解！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recommender-systems-in-python-from-scratch-643c8fc4f704?source=collection_archive---------2-----------------------#2020-02-19">https://towardsdatascience.com/recommender-systems-in-python-from-scratch-643c8fc4f704?source=collection_archive---------2-----------------------#2020-02-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ebc5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于梯度下降的矩阵分解模型在推荐系统中的应用</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5fa8ad2f8e6a16c3c804abc79abc1877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tkjwMtt_aarEOFmyT1RgJg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">信用:Pixabay</p></figure><p id="e8aa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你有没有想过网飞是如何确定你可能喜欢什么内容，或者亚马逊是如何让你注意到你可能会购买的产品的？或者，如果你正在阅读这篇文章，你可能非常清楚这些系统的存在，但不确定它们是如何运作的。</p><h2 id="68ab" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">基于内容的过滤与协同过滤</h2><p id="df54" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">一般来说，推荐系统有两种主要的方法:基于内容的和协作的。当然，有混合的方法，并且据称，也存在基于知识的系统。见下图:<a class="ae ms" href="https://pdfs.semanticscholar.org/7e41/3d5661f185b4f19825da9220535cc04388ae.pdf" rel="noopener ugc nofollow" target="_blank">https://pdfs . semantic scholar . org/7e 41/3d 5661 f 185 B4 f 19825 da 9220535 cc 04388 AE . pdf</a></p><p id="3f0c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">没有任何不必要的麻烦，基于内容意味着根据内容与其他内容的相似性向你推荐内容<em class="mt">，而不考虑其他用户的偏好</em>。协作是相反的，它不关心项目的非抽象特征，而是利用群体的智慧来产生推荐。前者的一个例子就是潘多拉的推荐系统。他们(煞费苦心地)精心制作了大约 400 个特征的音乐基因组。并且根据歌曲展现这些特征的程度向用户推荐歌曲。(想想流派，歌曲长度，艺人年龄，艺人种族等。)协作过滤发现用户项目偏好中的模式，并利用这些模式来生成推荐。对于本文，我们将只关注协同过滤。</p><p id="e502" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在协作过滤生态系统中，有许多选项可用。例如，奇异值分解(SVD)是第一种这样的技术，它(顾名思义)将用户项目偏好矩阵分解为三个元素:用户特征特征向量、特征项目特征向量和特征值的对角矩阵。将用户条目矩阵分解成潜在特征是非常有用的。用户被理解为他们<em class="mt">更喜欢</em>的潜在特征，而项目被理解为潜在特征和它们<em class="mt">呈现的程度</em>。换句话说，距离度量，例如余弦相似性，可以用于将用户直接与项目进行比较，并生成最佳匹配。当然，您可以将用户与用户进行比较，将项目与项目进行比较，但是 SVD 要强大得多！</p><p id="f01f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一种方法是基于梯度下降的矩阵分解，我们将在本文的剩余部分重点介绍这种方法。这里的核心思想是创建参数并迭代更新它们，以最小化某个成本函数。如果这听起来像是神经网络——你就在正确的轨道上；我们朝那个方向去了！<strong class="la iu">注意</strong>:基于深度学习的方法<em class="mt">确实</em>存在；然而，我们不会使用神经元或非线性，我们肯定不会将多层神经元链接在一起(<em class="mt">又名深度学习。</em>)</p><p id="ae45" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们开始考虑我们将如何处理这个问题，从理解我们的参数、矩阵乘法以及我们如何迭代地更新这些参数开始。让我们考虑下面的情况:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/23d127571c0a5b534738c435450445b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*Uie-PTrPudRo64yjmZr6ZA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用户特征行和特征项目列的点积是单个用户项目评级</p></figure><p id="0260" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，用户特征矩阵和特征项目矩阵中的每个单元都是一个参数。(在神经网络行话中称为权重)我们需要通过一些成本函数迭代更新这些参数。但是在我们深入研究之前，我们需要知道任何给定的用户特征单元和特征项单元所贡献的<em class="mt">用户项</em>单元。比如<code class="fe mv mw mx my b">U1F1</code>、<code class="fe mv mw mx my b">U1F2</code>贡献<code class="fe mv mw mx my b">U1I1</code>、<code class="fe mv mw mx my b">U1I2</code>、<code class="fe mv mw mx my b">U1I3</code>。矩阵乘法被执行为左矩阵的<em class="mt">行</em>与右矩阵的<em class="mt">列</em>的点积。正如您将看到的，用户条目矩阵的行和列索引决定了从分解的矩阵中选择的整个<em class="mt">行和列。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/2d28d3eb68e193fe001a4afb91eb4a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*YI7qvEMsEZ07ujMG6l1feA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用户特征单元和它们参与的用户项目单元</p></figure><p id="091c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们考虑另一个例子。请注意，我已经提醒您注意的用户项单元格形成了一个单独的列，<em class="mt">而不是一行</em>。这是因为单元格<code class="fe mv mw mx my b">F1I1</code>和<code class="fe mv mw mx my b">F2I2</code>构成了用户项矩阵中的三个不同的单元格。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/e30c3a8c14e9c706c06ccd98e5579f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*C6vyQBK15HPp3MMKzBtqlw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">要素项单元及其所在的用户项单元</p></figure><p id="beab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果这还不明显，我们需要更新我们的用户特征和特征项参数，但重要的是要注意— <em class="mt">每个参数将由三个不同梯度的平均值更新，每个梯度都与给定的参数相关。</em></p><p id="298c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面的例子显示了更新<code class="fe mv mw mx my b">U1F1</code>所需的三个梯度中的一个；注意另外两个梯度<code class="fe mv mw mx my b">U1I2</code>和<code class="fe mv mw mx my b">U1I3</code>，每个(当然)相对于<code class="fe mv mw mx my b">U1F1</code>。当我们更新<code class="fe mv mw mx my b">U1F1</code>时，我们将对这三个梯度进行平均。这里的直觉是，如果一次追逐一个梯度，我们<em class="mt">可能</em>永远不会收敛到最佳参数值。通过平均梯度，我们可以同时采取三个步骤来达到最佳决策。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/28052bec1e4e67a5e09c409112e598f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*MWwtm6S1CXUu-6X5WGX_Hw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">U1I1 的 MSE 和相对于 U1F1 的梯度</p></figure><p id="280b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">信不信由你——我们现在已经拥有了用 python 编写矩阵分解求解器所需的所有构件！让我们看看代码。我在下面附上了一个 Google Colab 笔记本，你可以在那里看到并运行代码来试验结果！我们将在类架构中逐个方法地剖析代码。</p><div class="nb nc gp gr nd ne"><a href="https://colab.research.google.com/drive/1X0cFGeY2y8D2OLK4VB8frHkVxCMaBS1s?authuser=1#scrollTo=H5mPYAKPIyfu" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">谷歌联合实验室</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">矩阵分解从零开始 colab.research.google.com</h3></div></div><div class="nm l"><div class="nn l no np nq nm nr ks ne"/></div></div></a></div><p id="20f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，让我们创建一个类对象并初始化它的特性。我已经做出了设计决定，将参数初始化为 0.1 到 0.9 之间的默认值。此外，我们将如前所述定义 MSE 函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/db54233b9077d59c4993217bd0273ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QhwOJBiGmvr_jgdvNv1Mdg.png"/></div></div></figure><p id="9b35" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二，让我们找到一个<em class="mt">单一的</em>梯度。如前所述，这是相对于一个用户特征单元(或一个特征项单元)的一个用户项单元。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/0706353d75ac05f18ee306dd23166fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dwWB1dTjUBUkMszFg3ZlvA.png"/></div></div></figure><p id="35cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是正如我们已经讨论过的，仅仅找到一个梯度是不够的。在更新给定参数时，我们必须考虑整行(或整列)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/c1efeb153f68116f1d6e75ca16f15c52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qH2Ypnz_YgUJhiMRg2WO7g.png"/></div></div></figure><p id="0ff8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我们需要定义一个训练模型的方法。默认情况下，我们将学习率设置为 0.1，迭代次数设置为 1000 次。这可以在上面提到的 Google Colab 中修改。您会注意到，在每次迭代中，项目特征值都增加了<em class="mt"/>。梯度不是负的吗？<em class="mt">是的！但在梯度下降中，使用符号</em> <code class="fe mv mw mx my b">w -= learning_rate * gradient</code>是标准的。在我们的例子中，双重否定完全抵消了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/d35f7ec6660158334b14baa30e1ca267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P9_OQv3IlgWvdGKspMy2yQ.png"/></div></div></figure><p id="f24e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们验证我们的代码是否有效！正如你所看到的，只有两个潜在的特征，我们离最优解只有这么近。误差不会减小到超过 3.54。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/901aa869331234efa5df75946136431b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XSyhNC1LiyD3hDU3i_sdpA.png"/></div></div></figure><p id="ba4e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，如果我们再增加一个潜在的特征，我们可以非常接近 0 误差。事实上，当我们将矩阵用户特征和特征项相乘时，我们得到了我们开始时的矩阵。是不是很美！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/d1f9dbaabb46611b7cb15e50db1dbb5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hLVexH90kwqimN6u07F43w.png"/></div></div></figure><p id="d9e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢今天的文章。如果你想看到更多这样的内容，请在下面评论！</p></div></div>    
</body>
</html>