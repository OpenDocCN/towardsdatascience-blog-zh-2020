<html>
<head>
<title>BiDirectional Attention Flow Model for Machine Comprehension</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器理解的双向注意流模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bidirectional-attention-flow-model-for-machine-comprehension-d533f5600007?source=collection_archive---------34-----------------------#2020-02-08">https://towardsdatascience.com/bidirectional-attention-flow-model-for-machine-comprehension-d533f5600007?source=collection_archive---------34-----------------------#2020-02-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/8f74865b0e990e7ef9d2eb7d3462cd1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SY2x0j0DJ1-6SvWkJ4EE9w.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">使用BiDAF回答问题</p></figure><div class=""/><div class=""><h2 id="a21e" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">深入研究BiDAF模型</h2></div><p id="a95f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">问题回答一直是自然语言处理的主要工作领域。我将讨论并实施一篇在质量保证相关问题上表现出色的研究论文的关键要素。那么，我们在问答中做什么呢？给我们一个上下文，并根据该上下文进行查询。模型的任务是找到问题的准确答案。答案可能在上下文中，也可能不在上下文中。如果它存在，那么这个任务可以被公式化为一个分类问题，如果它不存在，那么我们就转向一个更加困难的文本生成问题。但是对于所有这些，我们需要一个好的特征向量，它包含来自上下文和查询的信息以及它们之间的关系。</p><p id="d864" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我将要谈论的论文是Minjoon Seo等人的<a class="ae lq" href="https://arxiv.org/pdf/1611.01603.pdf" rel="noopener ugc nofollow" target="_blank">机器理解的双向注意力流</a>。我们将主要讨论架构的技术部分，并将按顺序实现这些部分。总的来说，这里主要是用较少的文本编写代码。下面给出的是BiDAF的架构。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/d55b529c2c98fb42693f15b5dcf706ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OaUzxYS_YzTiqrWD"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来源:https://allenai.github.io/bi-att-flow/BiDAF.png<a class="ae lq" href="https://allenai.github.io/bi-att-flow/BiDAF.png" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="d0eb" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">如图所示，模型中的文本表示首先使用字符级嵌入层，然后使用单词级嵌入，如Glove或Word2vec。最后，将两种表示连接在一起，得到最终的表示。为了简单起见，我们只能在代码中使用单词级的手套嵌入。</p><p id="f70d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一旦我们获得文本序列中每个单词的向量表示，我们将在双向LSTM层中馈送该序列，以获得良好的上下文表示。图中没有显示的一个重要的东西是高速公路网。因为我在以前的博客中没有提到过这个术语，所以在进入实现部分之前，我们将简单讨论一下。</p><h2 id="4202" class="lw lx jf bd ly lz ma dn mb mc md dp me ld mf mg mh lh mi mj mk ll ml mm mn mo bi translated">公路网络</h2><p id="95ed" class="pw-post-body-paragraph ku kv jf kw b kx mp kg kz la mq kj lc ld mr lf lg lh ms lj lk ll mt ln lo lp ij bi translated">想象一个具有非常深的结构的网络，包括NN层的多个堆叠。使用梯度下降很难优化深度较大的模型。此外，如果使用多个堆栈，由于绝对值小于1的变量相乘过多，会出现信息丢失。因此，将模型的深度增加到某一点之后，并不会使之前的结果受益。</p><p id="1792" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">受LSTMs的启发，提出了高速公路网络，其中使用门控机制将信息直接传播到下一层(因此出现了术语高速公路)。其结构如下所示:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/2997f320d731cb3348eaa9d3a64fbd57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/0*1b53Ld47SHeKc3s6.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来源:<a class="ae lq" href="https://miro.medium.com/max/1120/1*qHf_AHv8yJJsKQok4KS4Jw.png" rel="noopener">https://miro . medium . com/max/1120/1 * qHf _ ahv 8 yjjskqok4k S4 jw . png</a></p></figure><p id="7b6f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">引入了一个变换门T，它只是一个神经网络，后面跟着一个sigmoid激活。这意味着变换门将产生一个概率，该概率与当前层的输出相乘，并传播到下一层。线性门，C无非是1-T，是与当前层的输入相乘，传入下一层的概率。高速公路网络的一种变体，残差网络，其中C和T都等于1，用于微软著名的图像分类模型ResNet。结果表明，对于复杂问题，现在可以使用高速公路网络建立一个包含数百个图层的模型。</p><p id="4e36" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在这篇博客中，我们还将为每个双向LSTM使用高速公路网络，以实现强大的信息流。</p><h2 id="b56d" class="lw lx jf bd ly lz ma dn mb mc md dp me ld mf mg mh lh mi mj mk ll ml mm mn mo bi translated">相似矩阵</h2><p id="fef8" class="pw-post-body-paragraph ku kv jf kw b kx mp kg kz la mq kj lc ld mr lf lg lh ms lj lk ll mt ln lo lp ij bi translated">通常，注意力机制用于概括查询的上下文向量。但是在这里，通过使用上下文和查询表示来计算共享的相似性矩阵，而不是为查询计算单个关注，计算两个方向上的关注，即，上下文2查询和查询2上下文，以最大化信息增益。相似矩阵是形状为TxJ的矩阵，其中T是上下文的序列长度，J是查询的序列长度。这两种关注度都可以通过共享的相似度矩阵来计算。整个计算机制如下图所示:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mv"><img src="../Images/af26351277dc12a1fc953b90438d0405.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HlPQjYjP2_ZeHQ64"/></div></div></figure><p id="34c1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">可以看出，为了计算S_ij，输入为C_i和Q_j，计算公式如下:</p><p id="bf6f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">其中[；]是跨行的连接操作，[o]是逐元素的乘法操作，W_ij是大小为[1×3 * dim]的可训练权重向量。</p><h1 id="eed4" class="mw lx jf bd ly mx my mz mb na nb nc me kl nd km mh ko ne kp mk kr nf ks mn ng bi translated">上下文2查询和查询2上下文注意</h1><p id="2025" class="pw-post-body-paragraph ku kv jf kw b kx mp kg kz la mq kj lc ld mr lf lg lh ms lj lk ll mt ln lo lp ij bi translated">Context2Query Attention表示每个上下文单词在查询句子中的重要单词。这意味着Context2Query的形状应该是[TxJ]，这可以通过按行取相似性矩阵的softmax来实现:</p><p id="3260" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">C2Q = Softmax(S，axis=-1)</p><p id="6fd1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">有人参与的查询，AQ = C2Q。查询，shape=[Txdim]，[。=矩阵乘法]</p><p id="f411" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Query2Context Attention表示每个查询单词在上下文句子中最相似的单词。这是通过首先从相似性矩阵中取出最大元素，然后对其应用softmax来获得的。因此，最终输出是形状= [Tx1]的概率向量。</p><p id="5731" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">参与上下文= Q2C t上下文，shape=[1xdim]</p><p id="dfa6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">最终参与上下文，AC = tile(参与上下文，T)，shape = [Txdim]</p><h1 id="8d0b" class="mw lx jf bd ly mx my mz mb na nb nc me kl nd km mh ko ne kp mk kr nf ks mn ng bi translated">合并操作</h1><p id="c68a" class="pw-post-body-paragraph ku kv jf kw b kx mp kg kz la mq kj lc ld mr lf lg lh ms lj lk ll mt ln lo lp ij bi translated">该操作用于组合由注意力C2Q和Q2C获得的信息。合并操作将原始上下文(OC)、参与查询和参与上下文作为输入，并给出如下所示的最终表示:</p><p id="eeaf" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Merge(OC，AQ，AC)=[OC；AQ；AQ主管；OC o AC]，其中[；]是按行连接，[o]是按元素乘法。</p><p id="4ecd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">合并层为我们提供了一个shape = [T x 4 * dim]的输出，它可以进一步用于输入另一组双向LSTMs，后跟一个softmax，以获得答案的开始和结束概率。开始和结束概率是给定段落中答案的开始和结束索引的概率。正如前面所讨论的，只有答案在段落中，开始和结束概率的概念才会起作用。如果没有，我们已经将最终的表示提供给一个解码器，使它成为一个序列生成问题。</p><p id="b964" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">喔！我没有遵守我的承诺，保持讨论简短，主要集中在编码部分😀。无论如何，让我们现在做一些PYTHONING化的工作(Google的Meena告诉了我这个单词)。</p><h1 id="6f32" class="mw lx jf bd ly mx my mz mb na nb nc me kl nd km mh ko ne kp mk kr nf ks mn ng bi translated">相似矩阵</h1><pre class="ls lt lu lv gt nh ni nj nk aw nl bi"><span id="70bd" class="lw lx jf ni b gy nm nn l no np"><strong class="ni jg">class</strong> <strong class="ni jg">SimilarityMatrix</strong>(keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>Layer):<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">__init__</strong>(self,dims, <strong class="ni jg">**</strong>kwargs):<br/>        self<strong class="ni jg">.</strong>dims <strong class="ni jg">=</strong> dims<br/>        super(SimilarityMatrix, self)<strong class="ni jg">.</strong>__init__(<strong class="ni jg">**</strong>kwargs)<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">similarity</strong>(self, context, query):<br/>        e <strong class="ni jg">=</strong> context<strong class="ni jg">*</strong>query<br/>        c <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>concatenate([context, query, e], axis<strong class="ni jg">=-</strong>1)<br/>        dot <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>squeeze(K<strong class="ni jg">.</strong>dot(c, self<strong class="ni jg">.</strong>W), axis<strong class="ni jg">=-</strong>1)<br/>        <strong class="ni jg">return</strong> keras<strong class="ni jg">.</strong>activations<strong class="ni jg">.</strong>linear(dot <strong class="ni jg">+</strong> self<strong class="ni jg">.</strong>b)<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">build</strong>(self, input_shape):<br/>        dimension <strong class="ni jg">=</strong> 3<strong class="ni jg">*</strong>self<strong class="ni jg">.</strong>dims<br/>        self<strong class="ni jg">.</strong>W <strong class="ni jg">=</strong> self<strong class="ni jg">.</strong>add_weight(name<strong class="ni jg">=</strong>'Weights',<br/>                                shape<strong class="ni jg">=</strong>(dimension,1),<br/>                                initializer<strong class="ni jg">=</strong>'uniform',<br/>                                trainable<strong class="ni jg">=</strong>True)<br/>        <br/>        self<strong class="ni jg">.</strong>b <strong class="ni jg">=</strong> self<strong class="ni jg">.</strong>add_weight(name<strong class="ni jg">=</strong>'Biases',<br/>                                shape<strong class="ni jg">=</strong>(),<br/>                                initializer<strong class="ni jg">=</strong>'ones',<br/>                                trainable <strong class="ni jg">=</strong>True)<br/>        <br/>        super(SimilarityMatrix, self)<strong class="ni jg">.</strong>build(input_shape)<br/>        <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">call</strong>(self, inputs):<br/>        C, Q <strong class="ni jg">=</strong> inputs<br/>        C_len <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>shape(C)[1]<br/>        Q_len <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>shape(Q)[1]<br/>        C_rep <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>concatenate([[1,1],[Q_len],[1]], 0)<br/>        Q_rep <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>concatenate([[1],[C_len],[1,1]],0)<br/>        C_repv <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>tile(K<strong class="ni jg">.</strong>expand_dims(C, axis<strong class="ni jg">=</strong>2),C_rep)<br/>        Q_repv <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>tile(K<strong class="ni jg">.</strong>expand_dims(Q, axis<strong class="ni jg">=</strong>1), Q_rep)<br/>        <br/>        <strong class="ni jg">return</strong> self<strong class="ni jg">.</strong>similarity(C_repv, Q_repv)<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">compute_output_shape</strong>(self, input_shape):<br/>        batch_size <strong class="ni jg">=</strong> input_shape[0][0]<br/>        C_len <strong class="ni jg">=</strong> input_shape[0][1]<br/>        Q_len <strong class="ni jg">=</strong> input_shape[1][1]<br/>        <strong class="ni jg">return</strong> (batch_size, C_len, Q_len)<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">get_config</strong>(self):<br/>        cofig <strong class="ni jg">=</strong> super()<strong class="ni jg">.</strong>get_config()<br/>        <strong class="ni jg">return</strong> config</span></pre><h1 id="f835" class="mw lx jf bd ly mx my mz mb na nb nc me kl nd km mh ko ne kp mk kr nf ks mn ng bi translated">Context2Query查询注意</h1><pre class="ls lt lu lv gt nh ni nj nk aw nl bi"><span id="a2ba" class="lw lx jf ni b gy nm nn l no np"><strong class="ni jg">class</strong> <strong class="ni jg">Context2QueryAttention</strong>(keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>Layer):<br/>    <strong class="ni jg">def</strong> <strong class="ni jg">__init__</strong>(self, <strong class="ni jg">**</strong>kwargs):<br/>        super(Context2QueryAttention, self)<strong class="ni jg">.</strong>__init__(<strong class="ni jg">**</strong>kwargs)<br/>        <br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">build</strong>(self, input_shape):<br/>        super(Context2QueryAttention, self)<strong class="ni jg">.</strong>build(input_shape)<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">call</strong>(self, inputs):<br/>        mat,query <strong class="ni jg">=</strong> inputs<br/>        attention <strong class="ni jg">=</strong> keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>Softmax()(mat)<br/>        <strong class="ni jg">return</strong> K<strong class="ni jg">.</strong>sum(K<strong class="ni jg">.</strong>dot(attention, query), <strong class="ni jg">-</strong>2)<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">compute_output_shape</strong>(self,input_shape):<br/>        mat_shape, query_shape <strong class="ni jg">=</strong> input_shape<br/>        <strong class="ni jg">return</strong> K<strong class="ni jg">.</strong>concatenate([mat_shape[:<strong class="ni jg">-</strong>1],query_shape[<strong class="ni jg">-</strong>1:]])<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">get_config</strong>(self):<br/>        config <strong class="ni jg">=</strong> super()<strong class="ni jg">.</strong>get_config()<br/>        <strong class="ni jg">return</strong> config</span></pre><h1 id="d2c6" class="mw lx jf bd ly mx my mz mb na nb nc me kl nd km mh ko ne kp mk kr nf ks mn ng bi translated">查询2上下文</h1><pre class="ls lt lu lv gt nh ni nj nk aw nl bi"><span id="9f67" class="lw lx jf ni b gy nm nn l no np"><strong class="ni jg">class</strong> <strong class="ni jg">Query2ContextAttention</strong>(keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>Layer):<br/>    <strong class="ni jg">def</strong> <strong class="ni jg">__init__</strong>(self, <strong class="ni jg">**</strong>kwargs):<br/>        super(Query2ContextAttention, self)<strong class="ni jg">.</strong>__init__(<strong class="ni jg">**</strong>kwargs)<br/>        <br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">build</strong>(self, input_shape):<br/>        super(Query2ContextAttention, self)<strong class="ni jg">.</strong>build(input_shape)<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">call</strong>(self, inputs):<br/>        mat,context <strong class="ni jg">=</strong> inputs<br/>        attention <strong class="ni jg">=</strong> keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>Softmax()(K<strong class="ni jg">.</strong>max(mat, axis<strong class="ni jg">=-</strong>1))<br/>        prot <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>expand_dims(K<strong class="ni jg">.</strong>sum(K<strong class="ni jg">.</strong>dot(attention,context),<strong class="ni jg">-</strong>2),1)<br/>        final <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>tile(prot, [1,K<strong class="ni jg">.</strong>shape(mat)[1],1])<br/>        <strong class="ni jg">return</strong> final<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">compute_output_shape</strong>(self,input_shape):<br/>        mat_shape, cont_shape <strong class="ni jg">=</strong> input_shape<br/>        <strong class="ni jg">return</strong> K<strong class="ni jg">.</strong>concatenate([mat_shape[:<strong class="ni jg">-</strong>1],cont_shape[<strong class="ni jg">-</strong>1:]])<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">get_config</strong>(self):<br/>        config <strong class="ni jg">=</strong> super()<strong class="ni jg">.</strong>get_config()<br/>        <strong class="ni jg">return</strong> config</span></pre><h1 id="eaf4" class="mw lx jf bd ly mx my mz mb na nb nc me kl nd km mh ko ne kp mk kr nf ks mn ng bi translated">大合并</h1><pre class="ls lt lu lv gt nh ni nj nk aw nl bi"><span id="9ede" class="lw lx jf ni b gy nm nn l no np"><strong class="ni jg">class</strong> <strong class="ni jg">MegaMerge</strong>(keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>Layer):<br/>    <strong class="ni jg">def</strong> <strong class="ni jg">__init__</strong>(self, <strong class="ni jg">**</strong>kwargs):<br/>        super(MegaMerge, self)<strong class="ni jg">.</strong>__init__(<strong class="ni jg">**</strong>kwargs)<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">build</strong>(self, input_shape):<br/>        super(MegaMerge, self)<strong class="ni jg">.</strong>build(input_shape)<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">call</strong>(self, inputs):<br/>        context, C2Q, Q2C <strong class="ni jg">=</strong> inputs<br/>        CC2Q <strong class="ni jg">=</strong> context<strong class="ni jg">*</strong>C2Q<br/>        CQ2C <strong class="ni jg">=</strong> context<strong class="ni jg">*</strong>Q2C<br/>        final <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>concatenate([context, C2Q, CC2Q, CQ2C], axis<strong class="ni jg">=-</strong>1)<br/>        <strong class="ni jg">return</strong> final<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">compute_output_shape</strong>(self, input_shape):<br/>        C_shape,_,_ <strong class="ni jg">=</strong> input_shape<br/>        <strong class="ni jg">return</strong> K<strong class="ni jg">.</strong>concatenate([C_shape[:<strong class="ni jg">-</strong>1], 4<strong class="ni jg">*</strong>C_shape[<strong class="ni jg">-</strong>1:]])<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">get_config</strong>(self):<br/>        config <strong class="ni jg">=</strong> super()<strong class="ni jg">.</strong>get_config()<br/>        <strong class="ni jg">return</strong> config</span></pre><h1 id="24f4" class="mw lx jf bd ly mx my mz mb na nb nc me kl nd km mh ko ne kp mk kr nf ks mn ng bi translated">高速公路</h1><pre class="ls lt lu lv gt nh ni nj nk aw nl bi"><span id="96ab" class="lw lx jf ni b gy nm nn l no np"><strong class="ni jg">class</strong> <strong class="ni jg">HighwayLSTMs</strong>(keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>Layer):<br/>    <strong class="ni jg">def</strong> <strong class="ni jg">__init__</strong>(self, dims, <strong class="ni jg">**</strong>kwargs):<br/>        self<strong class="ni jg">.</strong>dims <strong class="ni jg">=</strong> dims<br/>        super(HighwayLSTMs, self)<strong class="ni jg">.</strong>__init__(<strong class="ni jg">**</strong>kwargs)<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">build</strong>(self, input_shape):<br/>        self<strong class="ni jg">.</strong>LSTM <strong class="ni jg">=</strong> keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>Bidirectional(keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>LSTM(self<strong class="ni jg">.</strong>dims, return_sequences<strong class="ni jg">=</strong>True))<br/>        super(HighwayLSTMs, self)<strong class="ni jg">.</strong>build(input_shape)<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">call</strong>(self, inputs):<br/>        h <strong class="ni jg">=</strong> self<strong class="ni jg">.</strong>LSTM(inputs)<br/>        flat_inp <strong class="ni jg">=</strong> keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>Flatten()(inputs)<br/>        trans_prob <strong class="ni jg">=</strong> keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>Dense(1, activation<strong class="ni jg">=</strong>'softmax')(flat_inp)<br/>        trans_prob <strong class="ni jg">=</strong> K<strong class="ni jg">.</strong>tile(trans_prob, [1,2<strong class="ni jg">*</strong>self<strong class="ni jg">.</strong>dims])<br/>        trans_prob <strong class="ni jg">=</strong> keras<strong class="ni jg">.</strong>layers<strong class="ni jg">.</strong>RepeatVector(K<strong class="ni jg">.</strong>shape(inputs)[<strong class="ni jg">-</strong>2])(trans_prob)<br/>        out <strong class="ni jg">=</strong> h <strong class="ni jg">+</strong> trans_prob<strong class="ni jg">*</strong>inputs<br/>        <strong class="ni jg">return</strong> out<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">compute_output_shape</strong>(self, input_shape):<br/>        <strong class="ni jg">return</strong> input_shape<br/>    <br/>    <strong class="ni jg">def</strong> <strong class="ni jg">get_config</strong>(self):<br/>        config <strong class="ni jg">=</strong> super()<strong class="ni jg">.</strong>get_config()<br/>        <strong class="ni jg">return</strong> config</span></pre><p id="5f73" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在唯一剩下的事情就是将这些片段应用到一个人的用例中。我希望我没有让你厌烦(当然没有，如果你正在读这一行的话)。</p><h1 id="c04e" class="mw lx jf bd ly mx my mz mb na nb nc me kl nd km mh ko ne kp mk kr nf ks mn ng bi translated">结果和结论</h1><p id="d125" class="pw-post-body-paragraph ku kv jf kw b kx mp kg kz la mq kj lc ld mr lf lg lh ms lj lk ll mt ln lo lp ij bi translated">我们现在已经讨论了BiDAF模型的各个部分的技术方面和实现。现在，让我们通过一个例子来说明这种注意力机制是如何为一个特定的问题找到答案的。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/08f8957621ea17f69e9a6cadc95e02fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MwOC4Ljdx3xBa7uBuipisQ.png"/></div></div></figure><p id="2ae9" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在上图中，给出的方块是两个问题的注意力矩阵可视化。矩阵的每一列表示段落中的上下文单词，而每一行表示问题向量中的单词。块越粗，它的注意力权重就越大。在块1中，可以清楚地看到，对于问题中的单词“Where”，给予单词“at，the，stadium，Levi，In，Santa，Ana”更多的权重。甚至它可以把焦点放在问号“？”上这其中涉及到更多的“倡议”二字。</p><p id="af83" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">从结果的角度来看，BiDAF在SQUAD、CNN/DailyMail数据集上进行了测试，结果如下:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/4189c88fdcc840ba99d3ecf014215501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fbVWhQUvGOYy11l2Gut9uA.png"/></div></div></figure><h1 id="4c44" class="mw lx jf bd ly mx my mz mb na nb nc me kl nd km mh ko ne kp mk kr nf ks mn ng bi translated"><strong class="ak">参考文献</strong></h1><p id="41a8" class="pw-post-body-paragraph ku kv jf kw b kx mp kg kz la mq kj lc ld mr lf lg lh ms lj lk ll mt ln lo lp ij bi translated">如果你想阅读更多的主题，没有什么比研究论文本身更好的了。</p><ol class=""><li id="41bf" class="ns nt jf kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated"><strong class="kw jg"> <em class="ob">比达夫:</em></strong><a class="ae lq" href="https://arxiv.org/pdf/1611.01603.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1611.01603.pdf</a></li><li id="03f0" class="ns nt jf kw b kx oc la od ld oe lh of ll og lp nx ny nz oa bi translated"><strong class="kw jg"> <em class="ob">公路网:</em></strong><a class="ae lq" href="https://arxiv.org/abs/1505.00387" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1505.00387</a></li></ol><p id="8ff3" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我希望你喜欢这个博客，如果你对我有任何建议或者你想联系，你可以点击下面的链接:</p><p id="d676" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="ob">领英:</em></strong><a class="ae lq" href="https://www.linkedin.com/in/spraphul555/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/spraphul555/</a></p><p id="7fda" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在是签名的时候了，</p><h1 id="22cd" class="mw lx jf bd ly mx my mz mb na nb nc me kl nd km mh ko ne kp mk kr nf ks mn ng bi translated">不断学习，不断分享</h1></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><p id="37e8" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><em class="ob">原载于2020年2月8日</em><a class="ae lq" href="https://spraphul.github.io/blog/bidaf" rel="noopener ugc nofollow" target="_blank"><em class="ob">https://spraphul . github . io</em></a><em class="ob">。</em></p></div></div>    
</body>
</html>