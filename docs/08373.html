<html>
<head>
<title>Fake news detector with deep learning approach (Part-II) Modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于深度学习方法(第二部分)建模的假新闻检测器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fake-news-detector-with-deep-learning-approach-part-ii-modeling-42b9f901b12b?source=collection_archive---------40-----------------------#2020-06-18">https://towardsdatascience.com/fake-news-detector-with-deep-learning-approach-part-ii-modeling-42b9f901b12b?source=collection_archive---------40-----------------------#2020-06-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7cdf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">创建用于假新闻检测的深度学习神经网络。</h2></div><p id="53f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这一系列文章中，我将展示我们如何使用深度学习算法来检测假新闻，并比较一些神经网络架构。</p><p id="3dcf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是这个系列的第二部分，我想用Keras和Tensorflow创建几个深度学习模型。在本系列的<a class="ae lc" rel="noopener" target="_blank" href="/fake-news-detector-with-deep-learning-approach-part-i-eda-757f5c052?source=collection_category---4------0-----------------------">前</a>部分，我对假新闻和非假新闻做了探索性的数据分析。我用不同的分析技术来比较假的和非假的新闻，让我们把这项工作交给神经网络吧。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/6300f6db597e3b470dc9c859897d65a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nscZsOO3y1to0iAW"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">照片由<a class="ae lc" href="https://unsplash.com/@alinnnaaaa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿丽娜·格鲁布尼亚</a>在<a class="ae lc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="cde2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要开始建模，我们需要进行数据预处理。让我们检查列车数据框中的NA值:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/490312f505a1f21df181cb7eb739047b.png" data-original-src="https://miro.medium.com/v2/resize:fit:244/format:webp/1*4N5HV8GQJlq77cg-4llqlg.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">NA数据检查</p></figure><p id="d093" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所看到的，我们的数据集中有很多NA值。在我的模型中，我计划只使用标题和文本列。为了解决NA值问题，我决定用文本替换标题中的NA值，反之亦然。这种方法帮助我为训练保存更多的数据。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/dd2657d79eb3054f50f5b27e104488d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*y9V5qHau4BvbxdLcYZoKPQ.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">NA替换结果</p></figure><p id="7c3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我的火车专栏里没有任何NA。</p><p id="c739" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一步是文本预处理。我想进行与我在EDA中相同的预处理步骤:</p><ul class=""><li id="bb0d" class="lv lw iq kh b ki kj kl km ko lx ks ly kw lz la ma mb mc md bi translated">替换标点符号；</li><li id="69ef" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">小写字母盘</li><li id="6e03" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">按单词拆分</li><li id="b96d" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">堵塞物</li><li id="afee" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">删除停用词</li></ul><p id="cdf9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来的步骤是一个热词表示和序列创建，对于<strong class="kh ir">标题</strong>列，最大句子长度为20，对于<strong class="kh ir">文本</strong>列，最大句子长度为100。这一步对于创建正确形式的数据集以用于神经网络非常重要。</p><p id="fa4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我的第一个模型将只用于带有二进制目标假/假的<strong class="kh ir">标题</strong>列。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/cd00e2330736a437cbc99fb75a0b2e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*CzQhL058hx-zGj1aARYl5g.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">标题模型的模型拱门</p></figure><p id="a8ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一层是嵌入。单词嵌入是对文本的学习表示，其中具有相同含义的单词具有相似的表示。单词嵌入实际上是一类技术，其中单个单词被表示为预定义向量空间中的实值向量。每个单词都被映射到一个向量，向量值以类似神经网络的方式学习，因此该技术通常被归入深度学习领域。这种方法的关键是对每个单词使用密集分布的表示法。</p><p id="9fef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个单词用一个实值向量来表示，往往是几十维或者几百维。这与稀疏单词表示所需的数千或数百万维形成对比，例如一键编码。</p><p id="672a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分布式表示是基于单词的使用来学习的。这允许以相似方式使用的单词产生相似的表示，自然地捕捉它们的意思。这与单词包模型中清晰但脆弱的表示形成对比，在单词包模型中，除非明确管理，否则不同的单词具有不同的表示，不管它们如何使用。</p><p id="eec0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Keras提供了一个嵌入层，可用于文本数据上的神经网络。</p><p id="172f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它要求输入数据是整数编码的，因此每个字都由一个唯一的整数表示。嵌入层用随机权重初始化，并将学习训练数据集中所有单词的嵌入。</p><p id="8c2a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二层是LSTM。这一层的主要概念是:</p><ul class=""><li id="f178" class="lv lw iq kh b ki kj kl km ko lx ks ly kw lz la ma mb mc md bi translated">它使用顺序信息。</li><li id="a23b" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">它有一个记忆，可以记录到目前为止已经计算过的内容。</li></ul><p id="0e0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我说过我们试图解决二进制分类问题，所以:</p><ul class=""><li id="6ad4" class="lv lw iq kh b ki kj kl km ko lx ks ly kw lz la ma mb mc md bi translated">我们输入每个单词，单词在某些方面相互关联。</li><li id="eab0" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">当我们看到那篇文章中的所有单词时，我们在标题/正文的末尾进行预测。</li><li id="7621" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">rnn从最后一个输出绕过输入，能够保留信息，并能够在最后利用所有信息进行预测。</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mk"><img src="../Images/fbcb777fa5e3d3f9feff541165540fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S6R-O6qelhs14611.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated"><a class="ae lc" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p></figure><p id="f3e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我的网络的输出是用于二进制分类的具有1个输出sigmoid激活函数的密集层。为了编译我的模型，我使用二进制交叉熵作为损失函数和准确性度量。对于优化器，我像往常一样使用Adam。</p><p id="94ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们运行模型并检查结果:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ml"><img src="../Images/d48446dc072f50233958500991bc3e62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2EknCk7in99f8E5sPLN23A.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">模型训练过程</p></figure><p id="4abe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所见，该模型有点过度拟合，这在ащк LSTM架构和LSTM层中非常常见。为了避免这种情况，我们可以使用脱落层。我想在Kaggle排行榜上查看我的成绩。比赛已经结束，但我可以查看我的结果。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mm"><img src="../Images/3eacc9a095b616de924aeb9db9500c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gycgjh4LLjbZ3MUf9ewIhQ.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">我的第一次提交</p></figure><p id="b6be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们创建相同的模型(仅将最大句子长度更改为1000，并将嵌入向量特征更改为100)，但仅针对<strong class="kh ir">文本</strong>列，并查看结果。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mn"><img src="../Images/9dd7312942befaf45167af4bd59362fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gwYVeFg7jRzeMqTdkH7ifA.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">模型训练过程</p></figure><p id="ce45" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个结果还算不错，但比仅仅是<strong class="kh ir">冠名的</strong>车型还要糟糕。为了增加这些结果，我可以增加这个(文本)数据的历元数，或者稍微改变一下我的架构。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/fa1aa895a6f3dc248413c90dc2447bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*sL18SYPSIEOrC-1Ks5peaA.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">文本模型第二版的模型拱门</p></figure><p id="7e5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如您所见，我添加了GlobalMaxPool1D来对输入制图表达进行缩减采样，添加了密集图层来生成更多要素，添加了Dropout来避免模型过度拟合。</p><p id="cbdb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GlobalMaxPool1D的工作原理:</p><ul class=""><li id="bf51" class="lv lw iq kh b ki kj kl km ko lx ks ly kw lz la ma mb mc md bi translated">通过获取时间维度上的最大值对输入制图表达进行缩减采样。</li><li id="edd4" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">全局最大池=池大小等于输入大小的普通最大池层(精确地说，减去过滤器大小+ 1)。</li><li id="7ede" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">全局池层可用于多种情况。首先，它可用于降低某些卷积层输出的特征图的维度，以取代分类器中的平坦层，有时甚至是致密层(Christlein等人，2019)。此外，它还可以用于例如单词识别(Sudholt &amp; Fink，2016)。这是由于它允许检测<em class="lb">噪声</em>，从而“大输出”(例如上面例子中的值9)。然而，这也是全局最大池的缺点之一，和常规一样，我们接下来将讨论全局平均池。</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/7070a6d2ed5a02cdc07feda57e24e7ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*8Gi5TEWOS2owqWV0.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated"><a class="ae lc" href="https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/" rel="noopener ugc nofollow" target="_blank">全局池层(矩阵示例)</a></p></figure><p id="4dee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者换个角度来看:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/228614485f0893fdcfb4c0c45c7beffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/0*UNov6gkzfpaNANCA.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated"><a class="ae lc" href="https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/" rel="noopener ugc nofollow" target="_blank">全局池层(可视化示例)</a></p></figure><p id="ed1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们开始训练:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mr"><img src="../Images/5836cf2d4837e0be80d2ded3a48cc9a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B2NtAqlkxwtpMZJNn4ZkOg.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">模型训练过程</p></figure><p id="97a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型比前一个版本执行得更好，但对我来说，我们需要更多的纪元才能获得更好的结果。让我们在Kaggle上检查一下这个模型:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ms"><img src="../Images/0cb2a1822c0deea7d1548364de1d6366.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CV1k3aB6WP1zpSh2rY8VRA.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">我的第二次提交</p></figure><p id="eff2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我可以将精度提高到与以前的模型相同的水平，但让我们想象一下，我们需要一个好的模型，它将在10个时期内给出好的结果。所以我创造了第四个模型:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mt"><img src="../Images/f4edfa51e04252a4288bdcdbfcf35312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ObCuqhP7j0FTy4dO031Z4w.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">多输入模型arch</p></figure><p id="b357" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，您可以看到，我创建了一个多输入模型，将两个以前的架构合并为一个。我使用的函数式API是一种创建模型的方法，比tf.keras.Sequential API更灵活。功能API可以处理具有非线性拓扑的模型、具有共享层的模型以及具有多个输入或输出的模型。使用这种架构，我可以从以前的两种架构中为两个独立的功能获得最好的东西，并将其合并到一个模型中。</p><p id="917f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好吧，让我们来看看训练结果:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mu"><img src="../Images/4c07c130f9cdb94b9727bac92150f27d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p57R4cw2OYBTPinVWh44Kg.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">模型训练过程</p></figure><p id="32c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是之前所有车型的最好成绩。让我们在Kaggle上检查一下。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mv"><img src="../Images/f2f07f97cde0a382660d59a499b80ac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RgghpOfszl8a_QvPWWeHqA.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">我的第三次提交</p></figure><p id="d139" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，结果看起来很棒。这将是这场激烈竞争中的第三名。</p><h1 id="0777" class="mw mx iq bd my mz na nb nc nd ne nf ng jw nh jx ni jz nj ka nk kc nl kd nm nn bi translated">结论</h1><p id="2660" class="pw-post-body-paragraph kf kg iq kh b ki no jr kk kl np ju kn ko nq kq kr ks nr ku kv kw ns ky kz la ij bi translated">作为这个分析的结果，我们可以看到不同的数据量(标题/正文)，不同的神经网络架构带来不同的结果。有时，最好将两个模型合并为一个，而不是试图调整其中一个。</p><p id="53bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这些故事的第三部分，我将尝试部署这个模型，并为假新闻检测建立实时管道。</p><p id="1736" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以阅读的第一部分—“<a class="ae lc" href="https://medium.com/@andriishchur/fake-news-detector-with-deep-learning-approach-part-i-eda-757f5c052" rel="noopener">采用深度学习方法的假新闻检测器(第一部分)EDA </a>”</p><p id="96a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你能在Git仓库中找到的所有代码— <a class="ae lc" href="https://github.com/AndriiShchur/Fake_news" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="a5d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">附言</p><p id="8852" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有建模都通过Azure Data Science虚拟机进行了训练和评分:</p><ul class=""><li id="d4d4" class="lv lw iq kh b ki kj kl km ko lx ks ly kw lz la ma mb mc md bi translated">NC12型(2个NVIDIA K80)</li></ul></div></div>    
</body>
</html>