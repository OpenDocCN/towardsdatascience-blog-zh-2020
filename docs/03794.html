<html>
<head>
<title>Convergence of Reinforcement Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习算法的收敛性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7?source=collection_archive---------15-----------------------#2020-04-09">https://towardsdatascience.com/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7?source=collection_archive---------15-----------------------#2020-04-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="04a3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学习一项任务的速度有什么简单的界限吗？Q-learning背景下的研究。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/cc06024d8e68fe5bb3c1a672d8836882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AYlaziI4fhMbJmAeCA6Dkw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">激动人心的时刻即将到来。照片由<a class="ae kv" href="https://www.pexels.com/@agk42?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">亚历山大·奈特</a>从<a class="ae kv" href="https://www.pexels.com/photo/high-angle-photo-of-robot-2599244/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a>拍摄</p></figure><p id="fa0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> D </span> eep强化学习算法可能是最近机器学习发展中最难对其性能设定<strong class="ky ir">数值界限</strong>的算法(在那些起作用的算法中)。理由是双重的:</p><ol class=""><li id="1b98" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">深层神经网络是模糊的黑匣子，没有人真正理解它们如何或为什么如此好地融合。</li><li id="96ab" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">强化学习任务<strong class="ky ir">收敛是</strong> <strong class="ky ir">历史上不稳定</strong>因为从环境中观察到的稀疏奖励(以及底层任务的难度— <strong class="ky ir"> <em class="mp">从零开始学习！</em> </strong>)。</li></ol><p id="0258" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我将向您介绍一种启发式算法，我们可以用它来描述RL算法如何收敛，并解释如何将其推广到更多的场景。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/4137ccc90a5de352f4d99873de2fcfc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xY-vP5Zzd-YLw-usXTV1gg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源——作者，哥斯达黎加。</p></figure><p id="852c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些相关的文章，可以在这之前看，或者在这之后看:</p><ol class=""><li id="9f07" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">什么是马尔可夫决策过程？</li><li id="e42a" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/the-hidden-linear-algebra-of-reinforcement-learning-406efdf066a">强化学习的隐藏线性代数。</a></li><li id="9962" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/fundamental-iterative-methods-of-reinforcement-learning-df8ff078652a">强化学习的基础迭代方法。</a></li></ol><p id="f2f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章解决了<strong class="ky ir"> <em class="mp">的问题:像值迭代、q学习和高级方法这样的迭代方法在训练时是如何收敛的？</em> </strong>在这里，我们将简要回顾马尔可夫决策过程(更多细节请参见上面的链接1和3)，解释我们如何将贝尔曼更新视为一个<strong class="ky ir">特征向量</strong>方程(更多信息请参见链接2)，并展示<strong class="ky ir">的数学运算，即值如何收敛到最优量</strong>。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="b591" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">回顾马尔可夫决策过程</h1><p id="ca2e" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">马尔可夫决策过程是支持强化学习的随机模型。如果你很熟悉，你可以跳过这一部分。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/f5b301d7a555d85406c2c4dc4c87f4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sB0cYflezQERRESh.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MDP就是一个例子。来源——我在CS188做的一个<a class="ae kv" href="https://inst.eecs.berkeley.edu/~cs188/sp20/assets/lecture/lec10.pdf" rel="noopener ugc nofollow" target="_blank">讲座</a>。</p></figure><h2 id="fa30" class="nw mz iq bd na nx ny dn ne nz oa dp ni lf ob oc nk lj od oe nm ln of og no oh bi translated">定义</h2><ul class=""><li id="1ab1" class="mb mc iq ky b kz nq lc nr lf oi lj oj ln ok lr ol mh mi mj bi translated">一组状态<strong class="ky ir"> s ∈ S </strong>，动作<strong class="ky ir"> a ∈ A </strong>。</li><li id="2a73" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr ol mh mi mj bi translated">一个转移函数<strong class="ky ir"> T(s，a，s’)</strong>。</li><li id="4ecb" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr ol mh mi mj bi translated">一个奖励函数<strong class="ky ir"> R(s，a，s’)。</strong>这个函数r的任何样本都在区间[-Rmax，+Rmax]内。</li><li id="f2b4" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr ol mh mi mj bi translated">区间[0，1]中的折扣因子<strong class="ky ir"> γ (gamma) </strong>。</li><li id="2af3" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr ol mh mi mj bi translated">开始状态<strong class="ky ir"> s0 </strong>，也可能是结束状态。</li></ul><h2 id="c1c9" class="nw mz iq bd na nx ny dn ne nz oa dp ni lf ob oc nk lj od oe nm ln of og no oh bi translated">重要的价值观</h2><p id="0705" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">MDP有两个重要的特征效用-状态值和机会节点的q值。任何MDP或RL值中的<strong class="ky ir"> * </strong>表示一个<strong class="ky ir"> <em class="mp">最优量</em> </strong>。效用是1)一个状态的v值<strong class="ky ir">和2)一个状态、动作对的Q值<strong class="ky ir">T21。</strong></strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/3554ed906a52d20aae7e93735819086a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yfq1rtWWSspNg86mnvmmOQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最佳值与最佳动作条件q值相关。然后值和q值更新规则非常相似(加权转换、奖励和折扣)。顶部)值与q值的耦合；mid) Q值递归，bot)值迭代。我在加州大学伯克利分校cs 188的讲座。</p></figure><blockquote class="on oo op"><p id="9bd1" class="kw kx mp ky b kz la jr lb lc ld ju le oq lg lh li or lk ll lm os lo lp lq lr ij bi translated">我们试图做的是<strong class="ky ir">估计这些价值、q值和效用</strong>，以便我们的代理人能够<strong class="ky ir">计划最大化回报的行动</strong>。</p><p id="fd5e" class="kw kx mp ky b kz la jr lb lc ld ju le oq lg lh li or lk ll lm os lo lp lq lr ij bi translated">真实估计=最优策略。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/13b1c8aaa8f3a39526ad9505bd1a359a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5gWWZsikLYfye-505_WtA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源，作者。美国印第安纳州詹姆斯敦。</p></figure><h1 id="4a1f" class="my mz iq bd na nb ou nd ne nf ov nh ni jw ow jx nk jz ox ka nm kc oy kd no np bi translated">概括迭代更新</h1><p id="1785" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">在新内容的第一部分中，我将回忆我正在使用的RL概念，并强调获得一个方程组所需的<strong class="ky ir">数学变换</strong>，该方程组的<strong class="ky ir">以离散的步骤</strong>发展，并且<strong class="ky ir">具有收敛界限</strong>。</p><h2 id="dfbf" class="nw mz iq bd na nx ny dn ne nz oa dp ni lf ob oc nk lj od oe nm ln of og no oh bi translated">强化学习</h2><p id="5d72" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">RL是我们试图“解决”和MDP的范例，但我们不知道底层环境。简单的RL解决方案是基本MDP求解算法(价值和策略迭代)的基于采样的变体。回想一下Q值迭代，这是我将重点关注的贝尔曼更新:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/a61812be1ff516befa2b91c566b9ec61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e9eKHAC19-AGbjikO3AJQg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MDP的q值迭代。</p></figure><p id="84d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看看精确值迭代或策略迭代如何提炼为在上述等式中的每次赋值(←)后比较一个值向量，这是一轮递归更新。这些方法的收敛产生了与强化学习算法如何收敛成比例的度量，因为<strong class="ky ir">强化学习算法是价值和策略迭代的基于采样的版本，具有一些更多的移动部分</strong>。</p><p id="6884" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mp">回忆:Q-learning是与Q-value迭代相同的更新规则，但是将转移函数替换为采样的动作，将奖励函数替换为实际样本，</em> <strong class="ky ir"> <em class="mp"> r </em> </strong> <em class="mp">，从环境中接收。</em></p><h2 id="4a9f" class="nw mz iq bd na nx ny dn ne nz oa dp ni lf ob oc nk lj od oe nm ln of og no oh bi translated">线性算子和特征向量</h2><p id="594a" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">我们需要将我们的Bellman更新公式化为一个线性操作符，<strong class="ky ir"> <em class="mp"> B </em> </strong>，(矩阵是线性操作符的子集)，看看我们是否可以让它表现为一个<a class="ae kv" href="https://en.wikipedia.org/wiki/Stochastic_matrix" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">随机矩阵</strong> </a>。随机矩阵<a class="ae kv" href="https://math.stackexchange.com/questions/40320/proof-that-the-largest-eigenvalue-of-a-stochastic-matrix-is-1" rel="noopener ugc nofollow" target="_blank">保证</a>有一个<strong class="ky ir">特征向量</strong>与<strong class="ky ir">特征值1 </strong>配对。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/d30402f037fa4556a71ca3757852bd7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*QUgsI30vgMVG-gqNJrX17A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Lambda#Lower-case_letter_%CE%BB" rel="noopener ugc nofollow" target="_blank"> λ </a> x =Ax，其中<code class="fe pb pc pd pe b"> lambda=1.</code></p></figure><blockquote class="on oo op"><p id="2d55" class="kw kx mp ky b kz la jr lb lc ld ju le oq lg lh li or lk ll lm os lo lp lq lr ij bi translated">也就是说，我们可以像研究特征空间的演化一样研究RL中的迭代更新。</p></blockquote><p id="c542" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们需要做一些符号上的改变，从矩阵中关于<em class="mp"> Q值的公式过渡到向量中作用于<strong class="ky ir">效用的公式(效用概括为值和Q值，因为它被定义为奖励的贴现和)。</strong></em></p><ol class=""><li id="02cb" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">我们知道效用与q值成正比；<strong class="ky ir">改变符号</strong>。我们将使用<strong class="ky ir"><em class="mp"/></strong>。</li><li id="18c3" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">将实用程序重新排列为一个<strong class="ky ir">向量</strong>类似于许多编码库的<code class="fe pb pc pd pe b">flatten()</code>功能(例如，将XY状态空间转换为跨越状态的1d向量索引)。我们得到<strong class="ky ir"><em class="mp"/></strong>。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/93dba2c24997706890948fd374dac2e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*z4mqd3mKlsqy0us0nPUWiQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">将q状态转换为特征值的通用效用向量。</p></figure><p id="65a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些变化对于将问题公式化为特征向量是至关重要的。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h2 id="6e45" class="nw mz iq bd na nx ny dn ne nz oa dp ni lf ob oc nk lj od oe nm ln of og no oh bi translated">了解Q-learning的各个部分</h2><p id="f782" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">从下面Q值迭代的基本方程开始，我们如何将它推广到线性系统？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/a61812be1ff516befa2b91c566b9ec61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e9eKHAC19-AGbjikO3AJQg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MDP的q值迭代。</p></figure><p id="bafa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，我们需要更改右侧，使其如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/431554f1c0a2d0ba8436bd7992c39509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hqRpZwu2YaWkKCpjAQmAlQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">将我们的系统移向线性算子(矩阵)</p></figure><p id="2798" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这给我们留下了最后一种形式(合并本节的方程，以及本征值部分的结尾)。我们可以用这个作为我们需要的线性算子吗？考虑一下，如果我们使用的是最优策略(不失一般性)，那么棘手的最大over actions就会消失，留给我们的是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ph"><img src="../Images/f4346d9325324baddead98c81af001b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*msEgcClfazwSoFuJDRRo1Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">这是我们能得到的最接近的，使它成为一个纯特征向量方程。它非常接近所谓的<a class="ae kv" href="https://en.wikipedia.org/wiki/Generalized_eigenvector" rel="noopener ugc nofollow" target="_blank">广义特征向量</a>。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pi"><img src="../Images/a83a3aab55e134de2fb1a2d8b35bc115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*snUOn2q1D2zvMoyjFdxM3A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源——作者，香港维多利亚的山顶。</p></figure><h1 id="75f8" class="my mz iq bd na nb ou nd ne nf ov nh ni jw ow jx nk jz ox ka nm kc oy kd no np bi translated">研究趋同</h1><p id="21b3" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">在本节中，我将推导出一个关系式，该关系式保证N 步后ε的<strong class="ky ir">最小误差，并展示其含义。</strong></p><h2 id="3f41" class="nw mz iq bd na nx ny dn ne nz oa dp ni lf ob oc nk lj od oe nm ln of og no oh bi translated">我们可以研究的系统</h2><p id="8268" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">我们在上面看到，我们可以用一种非常接近特征向量的方式来制定效用更新规则，但是我们偏离了一个常数向量<strong class="ky ir"> <em class="mp"> r </em> </strong>来表示MDP的基本回报面。如果我们取两个效用向量的差会发生什么？常数项消失了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/a60afcb53467d5e4ef5fbfb90dec1f1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*q6eTJEJShQHnBiXcOiRwUg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">取两个效用向量之间的差，我们看到,<strong class="bd pk"> r向量</strong>可以抵消。</p></figure><p id="060f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在有我们的系统，我们可以像特征向量的动态系统一样研究，但我们在效用之间的差异空间中工作，用矩阵<strong class="ky ir"><em class="mp">B</em></strong><em class="mp">—</em><strong class="ky ir">加权转移概率的总和</strong>。因为效用向量是相互定义的，所以我们可以神奇地重新排列这个(代入递归的贝尔曼方程，取范数)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/2d8f8bedfa33a2b73919dd1de3e9bb65.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*ExiqqXm6nirdff32AVOjkw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">贝尔曼更新后的误差减少了折扣因子。</p></figure><blockquote class="on oo op"><p id="e985" class="kw kx mp ky b kz la jr lb lc ld ju le oq lg lh li or lk ll lm os lo lp lq lr ij bi translated">研究任何效用估计之间的差异是巧妙的，因为它显示了a)估计如何不同于真实值，或者b)仅来自递归更新的数据如何演变(而不是小向量<strong class="ky ir"> <em class="iq"> r </em> </strong>)。</p></blockquote><h2 id="0e8a" class="nw mz iq bd na nx ny dn ne nz oa dp ni lf ob oc nk lj od oe nm ln of og no oh bi translated">ε-N关系</h2><p id="f388" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">任何收敛证明都将寻找<strong class="ky ir">误差界限</strong>，ε，<strong class="ky ir"> </strong>和<strong class="ky ir">步数</strong>，<em class="mp"> N </em>，(迭代)之间的关系。这种关系将使我们有机会用一个<strong class="ky ir">分析方程</strong>来限制性能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/5e2c368af18222323dc5de1d6c545fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*Jh_mvLh5eS2iw5yGl9apWA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们希望在步骤N-b(N)-处效用误差的界限小于ε。该界限将是一个解析表达式，ε是一个标量，表示估计的和真实的效用向量之间的差的范数。</p></figure><p id="bf04" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，我们在寻找ε的一个界限，这是n的函数</p><p id="d2d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们知道(根据MDP的定义)每一步的回报，<strong class="ky ir"><em class="mp">【r】</em></strong>，<strong class="ky ir"> <em class="mp"> </em> </strong>有界在区间[-Rmax，+Rmax]内。那么，把效用(报酬的贴现和)的定义看做一个几何级数，我们就可以把任意向量的差绑定到真实向量上。<em class="mp"> ( </em> <a class="ae kv" href="https://en.wikipedia.org/wiki/Geometric_series#Proof_of_convergence" rel="noopener ugc nofollow" target="_blank"> <em class="mp">等比数列收敛的证明</em> </a> <em class="mp">)。</em></p><div class="kg kh ki kj gt ab cb"><figure class="pn kk po pp pq pr ps paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/0d3436cae5608fe77041bb911e3e3f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*dNpM5Vd2udRI2nhPNc5MTg.png"/></div></figure><figure class="pn kk pt pp pq pr ps paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/42de696c6adffdc3b11ec382f0ebcac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*B5tQ0662lPWtXPTCTsc4oQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk pu di pv pw translated">左)效用的定义:贴现报酬的期望总和。右)我们收敛的起点。从一个<a class="ae kv" href="https://en.wikipedia.org/wiki/Geometric_series" rel="noopener ugc nofollow" target="_blank">几何级数</a>的收敛来考虑真实效用U和初始效用U(0)之间的差异。</p></figure></div><p id="3173" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">界限来自最坏情况的估计——其中每一步的真实回报是+Rmax，但我们将估计值初始化为-Rmax。唉，我们对我们的实用程序的误差有一个<strong class="ky ir">初始界限！</strong>回忆一下U0是我们初始化效用向量的值，然后每当我们运行贝尔曼更新时，索引就会增加。</p><p id="7711" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">贝尔曼更新— <strong class="ky ir">初始化时的界限如何随着每一步</strong>而演变？上面，我们看到<strong class="ky ir">在每一步</strong>误差都减少了折扣因子(从递归更新中的总和总是预先加上一个伽玛)。随着每次迭代，这将演变成一系列递减的误差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi px"><img src="../Images/fcaa485177de16d3257916bab5506777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ah7FhXjLNg_D-Y94X4kj5g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">当前效用更新(U_i)与真实值<strong class="bd pk"> U </strong>之差的收敛。</p></figure><p id="e6a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">剩下的就是宣告界限，<strong class="ky ir"><em class="mp">ε</em></strong>，相对于步数，<strong class="ky ir"> <em class="mp"> N </em> </strong>。</p><h2 id="9def" class="nw mz iq bd na nx ny dn ne nz oa dp ni lf ob oc nk lj od oe nm ln of og no oh bi translated">结果——可视化融合</h2><p id="2249" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">在上面等式的右边，我们有一个效用估计的精确度界限。</p><blockquote class="py"><p id="17a9" class="pz qa iq bd qb qc qd qe qf qg qh lr dk translated">从逻辑上讲，对于任何ε，我们知道误差将在N步中小于ε。</p></blockquote><figure class="qj qk ql qm qn kk gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/cb5798b10118da7898958d23e2922fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*-u_cWvim7cBMfLk_g8nvJg.png"/></div></figure><p id="306e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还可以用数学技巧在ε和N之间来回变换——因此，如果我们知道<em class="mp">我们希望估计值有多精确，我们就可以解决让我们的算法运行多长时间的问题</em>！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/521f758cd055fdb679cc594d73401414.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*WVVwdYp08mQdoaWSJ6M0xw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">通过取两边的对数，我们可以解出达到所述界限的迭代次数！</p></figure><p id="c548" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们发现的界限是状态空间上累积值误差的界限(下面的实线)。精明的读者会想知道的是，政策错误与此相比如何？从概念上来说，这意味着，<em class="mp">‘在多少个州，当前政策会与最优政策不同？’</em>事实证明，随着数值的标准化(因此它们在数值上相似)，政策误差收敛得更快，并且更快地达到零(没有渐近线！).</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/1d3da76e305a61bacafc7fed425f127b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*vNufYKncMc1gXVHl1y13fg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">政策迷失空间中价值迭代与政策迭代的收敛性。策略的离散性使得策略迭代收敛更快<strong class="bd pk">在许多情况下</strong> <em class="qq">。</em> <a class="ae kv" href="http://aima.cs.berkeley.edu/" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="28c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这代表了在某些情况下使用策略迭代优于值迭代的优势。它会带来更多的算法吗？</p><h2 id="f6cc" class="nw mz iq bd na nx ny dn ne nz oa dp ni lf ob oc nk lj od oe nm ln of og no oh bi translated">影响-限制最近的深度增强算法</h2><p id="85ef" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">绑定深度RL算法是每个人都想要的。近年来我们已经看到了令人印象深刻的成果，机器人可以<a class="ae kv" href="https://arxiv.org/pdf/1812.11103.pdf" rel="noopener ugc nofollow" target="_blank">跑</a>、<a class="ae kv" href="https://bair.berkeley.edu/blog/2018/11/30/visual-rl/" rel="noopener ugc nofollow" target="_blank">叠毛巾</a>、<a class="ae kv" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16669/16677" rel="noopener ugc nofollow" target="_blank">玩</a>游戏。如果我们在性能上有限制，那就太好了。</p><p id="7bb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们所能做的，必然是我们对世界的表述将如何趋同。我们已经证明<strong class="ky ir">效用函数将会收敛</strong>。有两个持久的挑战:</p><ol class=""><li id="e21b" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><em class="mp">我们没有被赋予</em> <strong class="ky ir"> <em class="mp">奖励功能</em> </strong> <em class="mp">对于现实生活中的任务，我们必须设计它。</em></li><li id="d8fb" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">运行这些迭代算法目前是不安全的。机器人探索涉及大量的力量、相互作用和(实际上)损害。</li></ol><blockquote class="py"><p id="f727" class="pz qa iq bd qb qc qr qs qt qu qv lr dk translated">我们可以研究算法的收敛性，但是限制深度强化学习应用于现实世界任务的大多数工程问题是奖励工程和安全学习。</p></blockquote><p id="7970" class="pw-post-body-paragraph kw kx iq ky b kz qw jr lb lc qx ju le lf qy lh li lj qz ll lm ln ra lp lq lr ij bi translated">这就是我留给你们的——一个帮助我们设计更好系统的行动号召，这样我们就可以展示更多管理它的基础数学。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><p id="8cd1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是我对人们在深度RL研究中使用的方法的总结。</p><div class="rb rc gp gr rd re"><a rel="noopener follow" target="_blank" href="/getting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec"><div class="rf ab fo"><div class="rg ab rh cl cj ri"><h2 class="bd ir gy z fp rj fr fs rk fu fw ip bi translated">最新深度增强算法概述</h2><div class="rl l"><h3 class="bd b gy z fp rj fr fs rk fu fw dk translated">获得RL算法要点的资源，无需浏览大量文档或方程。</h3></div><div class="rm l"><p class="bd b dl z fp rj fr fs rk fu fw dk translated">towardsdatascience.com</p></div></div><div class="rn l"><div class="ro l rp rq rr rn rs kp re"/></div></div></a></div></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><p id="d724" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更多？订阅我关于机器人、人工智能和社会的时事通讯！</p><div class="rb rc gp gr rd re"><a href="https://robotic.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="rf ab fo"><div class="rg ab rh cl cj ri"><h2 class="bd ir gy z fp rj fr fs rk fu fw ip bi translated">自动化大众化</h2><div class="rl l"><h3 class="bd b gy z fp rj fr fs rk fu fw dk translated">一个关于机器人和人工智能的博客，让它们对每个人都有益，以及即将到来的自动化浪潮…</h3></div><div class="rm l"><p class="bd b dl z fp rj fr fs rk fu fw dk translated">robotic.substack.com</p></div></div><div class="rn l"><div class="rt l rp rq rr rn rs kp re"/></div></div></a></div></div></div>    
</body>
</html>