<html>
<head>
<title>Mining Twitter Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">挖掘 Twitter 数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mining-twitter-data-ba4e44e6aecc?source=collection_archive---------10-----------------------#2020-01-29">https://towardsdatascience.com/mining-twitter-data-ba4e44e6aecc?source=collection_archive---------10-----------------------#2020-01-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f577" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">社交媒体内部</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi kg"><img src="../Images/04128b9a412ee661a805f05a56ff678f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*UUgFyQojAfSfDTweifRl5g.png"/></div><p class="ko kp gj gh gi kq kr bd b be z dk translated">来自 cnbc.com<a class="ae ks" href="https://www.cnbc.com/2019/06/07/how-trolls-use-twitter-lists-to-target-and-harass-other-users.html" rel="noopener ugc nofollow" target="_blank">的劳伦·费纳的照片</a></p></figure><p id="62b7" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><em class="lp"> Twitter 是一种微博和社交网络服务，用户在这里发布内容并与帖子互动，即所谓的“推文”。今天，Twitter 已经普及了标签的使用，作为一种分组对话的方式，并允许用户关注特定主题的对话。</em></p><h1 id="def9" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated"><strong class="ak">目标</strong></h1><p id="ee64" class="pw-post-body-paragraph kt ku iq kv b kw mi jr ky kz mj ju lb lc mk le lf lg ml li lj lk mm lm ln lo ij bi translated">每天有超过 5 亿条推文，你可以想象这个平台的信息有多丰富。这个项目的目标是收集和分析 twitter 数据，以发现有趣的信息和隐藏的模式。</p><p id="ce61" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">这篇文章是我完成这项任务的经验的“技术反映”。我将展示并简要解释相关的代码片段以及我的分析结果。以下是我认为值得讨论的关键步骤的概述。</p><h1 id="0be4" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated"><strong class="ak">概述</strong></h1><ol class=""><li id="1aad" class="mn mo iq kv b kw mi kz mj lc mp lg mq lk mr lo ms mt mu mv bi translated">证明</li><li id="319f" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo ms mt mu mv bi translated">数据收集</li><li id="b286" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo ms mt mu mv bi translated">数据清理和预处理</li><li id="d6f9" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo ms mt mu mv bi translated">建模和分析</li><li id="6a84" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo ms mt mu mv bi translated">结论</li></ol><h1 id="dfe6" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated"><strong class="ak">认证</strong></h1><p id="445d" class="pw-post-body-paragraph kt ku iq kv b kw mi jr ky kz mj ju lb lc mk le lf lg ml li lj lk mm lm ln lo ij bi translated">在这个应用程序中，使用行业标准 OAuth 流程执行身份验证步骤，该流程涉及用户、消费者(应用程序)和资源提供者(Twitter)。这里的关键是，用户名和密码等凭证的交换只发生在用户和资源提供者之间。所有其他交易所都是由代币驱动的。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/25802855e9268c1c1564378181124324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*ST-T86FmDR3wfRdcjuHEOQ.png"/></div><p class="ko kp gj gh gi kq kr bd b be z dk translated">甲骨文<a class="ae ks" href="https://docs.oracle.com/cd/E50612_01/doc.11122/oauth_guide/content/oauth_intro.html" rel="noopener ugc nofollow" target="_blank">OAuth 程序照片</a></p></figure><ul class=""><li id="a5ee" class="mn mo iq kv b kw kx kz la lc nc lg nd lk ne lo nf mt mu mv bi translated">创建一个<a class="ae ks" href="https://developer.twitter.com" rel="noopener ugc nofollow" target="_blank"> Twitter 开发者账户</a>。您需要一个开发者帐户来访问 twitter API。一旦您<a class="ae ks" href="http://apps.twitter.com" rel="noopener ugc nofollow" target="_blank">创建了一个应用</a>，您就可以生成<em class="lp">消费者密钥</em>、<em class="lp">消费者秘密</em>、<em class="lp">访问令牌</em>和<em class="lp">访问秘密</em></li><li id="1ae5" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">我选择 tweepy 作为我的 Twitter API 客户端，并且我选择为我的令牌创建环境变量，因为它提高了安全性并且是语言不可知的。以下是我如何设置我的 twitter 客户端。</li></ul><pre class="kh ki kj kk gt ng nh ni nj aw nk bi"><span id="44df" class="nl lr iq nh b gy nm nn l no np">import os<br/>import sys<br/>from tweepy import API<br/>from tweepy import OAuthHandler</span><span id="a3c4" class="nl lr iq nh b gy nq nn l no np">def get_twitter_auth():<br/><strong class="nh ir">    #set up twitter authentication</strong></span><span id="3386" class="nl lr iq nh b gy nq nn l no np"># Return: tweepy.OAuthHandler object</span><span id="0c52" class="nl lr iq nh b gy nq nn l no np">try:<br/>        consumer_key = os.environ['TWITTER_CONSUMER_KEY']<br/>        consumer_secret = os.environ['TWITTER_CONSUMER_SECRET']<br/>        access_token = os.environ['TWITTER_ACCESS_TOKEN']<br/>        access_secret= os.environ['TWITTER_ACCESS_SECRET']<br/>    except KeyError:<br/>        sys.stderr.write("TWITTER_* environment variables not set\n")<br/>        sys.exit(1)<br/>    auth = OAuthHandler(consumer_key, consumer_secret)<br/>    auth.set_access_token(access_token, access_secret)<br/>    return auth</span><span id="7f5d" class="nl lr iq nh b gy nq nn l no np">def get_twitter_client():<br/><strong class="nh ir">    #Setup twitter API client.</strong></span><span id="fdd4" class="nl lr iq nh b gy nq nn l no np"># Return tweepy.API object</span><span id="3829" class="nl lr iq nh b gy nq nn l no np">auth = get_twitter_auth()<br/>    client = API(auth)<br/>    return client</span></pre><h1 id="3dad" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated"><strong class="ak">数据收集</strong></h1><p id="ed6b" class="pw-post-body-paragraph kt ku iq kv b kw mi jr ky kz mj ju lb lc mk le lf lg ml li lj lk mm lm ln lo ij bi translated">初步设置后。我可以立即开始收集数据并与 API 交互。</p><p id="4a48" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">与 twitter 的 REST API 交互</strong></p><ul class=""><li id="c259" class="mn mo iq kv b kw kx kz la lc nc lg nd lk ne lo nf mt mu mv bi translated">所有 REST 端点都允许你“回到过去”，这意味着我们能够搜索已经发布的推文。</li></ul><p id="3233" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">与 twitter 的流媒体应用编程接口互动</strong></p><ul class=""><li id="9738" class="mn mo iq kv b kw kx kz la lc nc lg nd lk ne lo nf mt mu mv bi translated">流式 API 展望未来。一旦你打开了一个连接，你就可以让它保持开放并及时向前。通过保持 HTTP 连接打开，我们可以在发布时检索所有符合过滤标准的推文。</li><li id="5674" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">这通常是下载大量推文的首选方式。</li><li id="6281" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">就我而言，我想调查 2019 年 12 月美伊紧张局势期间用户之间的实时对话，所以我决定播放所有带有趋势标签的推文，如#USIran #USvsIran #Iranattacks</li></ul><p id="1515" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">下面是我如何实现一个定制的流监听器。</p><pre class="kh ki kj kk gt ng nh ni nj aw nk bi"><span id="fde2" class="nl lr iq nh b gy nm nn l no np">class CustomListener(StreamListener):<br/><strong class="nh ir">    """ Custom StreamListener for streaming twitter data."""</strong></span><span id="a804" class="nl lr iq nh b gy nq nn l no np">def __init__(self, fname):<br/>        safe_fname = format_filename(fname)<br/>        self.outfile = "stream_%s.jsonl" % safe_fname</span><span id="3a12" class="nl lr iq nh b gy nq nn l no np">def on_data(self, data):</span><span id="766d" class="nl lr iq nh b gy nq nn l no np">        #called when data is coming through. This method simply stores data as it is received in a .jsonl file. Each line in this file will contain a single tweet in json format</span><span id="efc3" class="nl lr iq nh b gy nq nn l no np">        try:<br/>            with open(self.outfile, 'a') as f:<br/>                f.write(data)<br/>                return True<br/>        except BaseException as e:<br/>            sys.stderr.write("Error on_data: {}\n".format(e))<br/>            time.sleep(5)<br/>        return True<br/>                                <strong class="nh ir">...</strong></span><span id="8216" class="nl lr iq nh b gy nq nn l no np">if __name__ == '__main__':<br/>    query = sys.argv[1:] # list of CLI argumentsquery_fname<br/>    query_fname = ' '.join(query) # string<br/>    auth = get_twitter_auth()<br/>    twitter_stream = Stream(auth, CustomListener(query_fname))<br/>    twitter_stream.filter(track=query, is_async=True)</span></pre><h1 id="d3c5" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated"><strong class="ak">数据清理和预处理</strong></h1><p id="0587" class="pw-post-body-paragraph kt ku iq kv b kw mi jr ky kz mj ju lb lc mk le lf lg ml li lj lk mm lm ln lo ij bi translated">因此，在流了大约一个小时后，我设法收集了来自用户的 13，908 条谈论美伊紧张局势的推文。我将这些推文存储在一个. jsonl 文件中，现在是预处理文本的时候了。以下是我执行的一些预处理任务:</p><p id="69ec" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">使用 nltk 库进行标记化</strong></p><p id="dfec" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">记号化是指将文本流分解成单词、短语、符号等记号。tweet 的内容包括表情符号、用户提及、hasthags、URL 等。因此，在使用 nltk 库时，我将展示如何使用 TweetTokenizer 类作为工具来适当地标记 twitter 内容。</p><p id="5022" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">停止字清除</strong></p><p id="b4d3" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">停用词没有内容承载，所以我们需要删除它们。这类词包括冠词、副词、符号、标点符号等。频率分析显示，这些字符串在文档中出现的频率最高。</p><p id="03ab" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">正常化</strong></p><p id="28ec" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">规范化用于聚合同一单位中的不同术语。执行大小写规范化将有助于自动将这些字符串与最初不同的大小写进行匹配，以便它们可以聚合在同一术语下。</p><pre class="kh ki kj kk gt ng nh ni nj aw nk bi"><span id="265f" class="nl lr iq nh b gy nm nn l no np">import sys<br/>import string<br/>import json<br/>from collections import Counter<br/>from nltk.tokenize import TweetTokenizer<br/>from nltk.corpus import stopwords<br/>import matplotlib.pyplot as plt</span><span id="f3cf" class="nl lr iq nh b gy nq nn l no np">def process(text, tokenizer=TweetTokenizer(), stopwords=[]):<br/>    """ Process tweet text:<br/>        - Lowercase<br/>        - tokenize<br/>        - stopword removal<br/>        - digits removal<br/>        Return: list of strings<br/>    """</span><span id="bbba" class="nl lr iq nh b gy nq nn l no np">text = text.lower()<br/>    tokens = tokenizer.tokenize(text)<br/>    return [tok for tok in tokens if tok not in stopwords and not tok.isdigit()]</span><span id="b6d4" class="nl lr iq nh b gy nq nn l no np">if __name__ == '__main__':<br/>    fname = sys.argv[1]<br/>    tweet_tokenizer = TweetTokenizer()<br/>    punct = list(string.punctuation)<br/>    stopword_list = stopwords.words('english') + punct + ['rt', 'via', '...']</span><span id="26e8" class="nl lr iq nh b gy nq nn l no np">tf = Counter()<br/>    with open(fname, 'r') as f:<br/>        for line in f:<br/>            tweet = json.loads(line)<br/>            tokens = process(text=tweet['text'], tokenizer=tweet_tokenizer, stopwords=stopword_list)<br/>            tf.update(tokens)<br/>        for tag, count in tf.most_common(20):</span><span id="bd26" class="nl lr iq nh b gy nq nn l no np">print("{}: {}".format(tag, count))</span><span id="a0ce" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir">#plot results</strong></span><span id="9d1d" class="nl lr iq nh b gy nq nn l no np">y = [count for tag, count in tf.most_common(20)]<br/>    x = range(1, len(y)+1)</span><span id="ee94" class="nl lr iq nh b gy nq nn l no np">plt.bar(x, y)<br/>    plt.title("Term frequencies used in US-Iran Stream Data")<br/>    plt.ylabel("Frequency")<br/>    plt.savefig('us-iran-term-distn.png')</span></pre><h1 id="3171" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated"><strong class="ak">建模和分析</strong></h1><p id="fd32" class="pw-post-body-paragraph kt ku iq kv b kw mi jr ky kz mj ju lb lc mk le lf lg ml li lj lk mm lm ln lo ij bi translated">在上面的片段中，我通过创建我收集的 tweets 中最常用的 20 个术语的术语分布的可视化表示来开始分析。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nr"><img src="../Images/f6e5b61986da57925f81fca1ac416e5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eHwHgqBnAorseUTE58AfOQ.png"/></div></div><p class="ko kp gj gh gi kq kr bd b be z dk translated">前 20 个常用术语和术语分布</p></figure><p id="72aa" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">继续分析，我在数据集中搜索每个人都在谈论的关键人物，找到他们被提及的频率。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nw"><img src="../Images/46b4a2b0d1355d472cb2eb4c90e74b48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Jijc18Vz10je9XkxReE9A.png"/></div></div><p class="ko kp gj gh gi kq kr bd b be z dk translated">提及频率结果</p></figure><p id="4bf0" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">有了这些信息，我就确切地知道我可以开始调查哪些概要文件来接收更相关的提要。然而，令人震惊的是，realdonaldtrump 在与此问题相关的热门用户提及中排名第八。</p><p id="6332" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">时间序列分析</strong></p><ul class=""><li id="605a" class="mn mo iq kv b kw kx kz la lc nc lg nd lk ne lo nf mt mu mv bi translated">时间序列是由给定时间间隔内的连续观察值组成的数据点序列</li><li id="12bc" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">它可以用来重新排序推文，并跟踪用户对实时事件(如体育、政治选举、新闻)的反应</li></ul><p id="a0a1" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">在这个应用程序中，我执行了一个时间序列分析，以监控随着更多关于这个话题的新闻发布，用户是如何发 tweet 的。在我看来，在我上传视频的一个小时内，用户们一直在积极地发关于这个话题的推文，在上午 10:45 左右收集了 200 条推文</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/fbdc11adff323cf1f73bf46cb9fc2a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*bdQ02yfzavtPMEDIWi_ctA.png"/></div></figure><p id="7120" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">用户、关注者和朋友</strong></p><p id="8f5d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">我决定探索一些顶级用户的个人资料、关注者名单和“关注者”(或朋友)，以进行进一步分析。以下是我如何从 twitter 上获取这些信息的。</p><pre class="kh ki kj kk gt ng nh ni nj aw nk bi"><span id="0926" class="nl lr iq nh b gy nm nn l no np"><strong class="nh ir">#get followers for a given user</strong></span><span id="e525" class="nl lr iq nh b gy nq nn l no np">fname = "Twitter_Profiles/{}/followers.jsonl".format(screen_name)<br/>    with open(fname, 'w') as f:<br/>        for followers in Cursor(client.followers_ids, screen_name=screen_name).pages(max_pages):<br/>            for chunk in paginate(followers, 100):<br/>                users = client.lookup_users(user_ids=chunk)<br/>                for user in users:<br/>                    f.write(json.dumps(user._json)+"\n")<br/>            if len(followers) == 5000:<br/>                print("More results available. Sleeping for 60 seconds to avoid rate limit")<br/>                time.sleep(60)</span><span id="6608" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir">#get friends for a given user</strong></span><span id="3062" class="nl lr iq nh b gy nq nn l no np">fname = "Twitter_Profiles/{}/friends.jsonl".format(screen_name)<br/>    with open(fname, 'w') as f:<br/>        for friends in Cursor(client.friends_ids, screen_name=screen_name).pages(max_pages):<br/>            for chunk in paginate(friends, 100):<br/>                users = client.lookup_users(user_ids=chunk)<br/>                for user in users:<br/>                    f.write(json.dumps(user._json)+"\n")<br/>            if len(friends) == 5000:<br/>                print("More results available. Sleeping for 60 seconds to avoid rate limit")<br/>                time.sleep(60)</span><span id="d433" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir"># get user's profile</strong><br/>    fname = "Twitter_Profiles/{}/user_profile.json".format(screen_name)<br/>    with open(fname, 'w') as f:<br/>        profile = client.get_user(screen_name=screen_name)<br/>        f.write(json.dumps(profile._json, indent=4))</span></pre><p id="8bf3" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">衡量影响力和参与度</strong></p><ul class=""><li id="4208" class="mn mo iq kv b kw kx kz la lc nc lg nd lk ne lo nf mt mu mv bi translated">参与度指标提供了对用户对推文和其他内容的反应的评估，通常是为了增加流量而创建的。在 twitter 上，用户通过点赞或转发的方式参与，这为推文提供了更多的可见性</li><li id="2a68" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">了解用户的影响力包括结合用户的推文参与度统计数据分析用户的覆盖范围。</li></ul><pre class="kh ki kj kk gt ng nh ni nj aw nk bi"><span id="f914" class="nl lr iq nh b gy nm nn l no np"><strong class="nh ir">#Build up a list of followers</strong><br/>    followers_file1 = 'Twitter_Profiles/{}/followers.jsonl'.format(screen_name1)<br/>    followers_file2 = 'Twitter_Profiles/{}/followers.jsonl'.format(screen_name2)</span><span id="a0fd" class="nl lr iq nh b gy nq nn l no np">with open(followers_file1) as f1, open(followers_file2) as f2:<br/>        reach1 = []<br/>        reach2 = []<br/>        for line in f1:<br/>            profile = json.loads(line)<br/>            reach1.append((profile['screen_name'], profile['followers_count']))<br/>        for line in f2:<br/>            profile = json.loads(line)<br/>            reach2.append((profile['screen_name'], profile['followers_count']))</span><span id="e471" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir">#Load basic statistics</strong><br/>    profile_file1 = 'Twitter_Profiles/{}/user_profile.json'.format(screen_name1)<br/>    profile_file2 = 'Twitter_Profiles/{}/user_profile.json'.format(screen_name2)<br/>    with open(profile_file1) as f1, open(profile_file2) as f2:<br/>        profile1 = json.load(f1)<br/>        profile2 = json.load(f2)</span><span id="ce99" class="nl lr iq nh b gy nq nn l no np">followers1 = profile1['followers_count']<br/>        followers2 = profile2['followers_count']</span><span id="9b14" class="nl lr iq nh b gy nq nn l no np">tweets1 = profile['statuses_count']<br/>        tweets2 = profile['statuses_count']</span><span id="6fdf" class="nl lr iq nh b gy nq nn l no np">sum_reach1 = sum([x[1] for x in reach1]) #sum up all of a user's followers, followers<br/>    sum_reach2 = sum([x[1] for x in reach2])<br/>    avg_followers1 = round(sum_reach1/ followers1, 2)<br/>    avg_followers2 = round(sum_reach2/ followers2, 2)</span><span id="041a" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir">#Load the timelines for two users to observe the number of times their tweets have been favorited</strong><br/>    timeline_file1 = 'user_timeline_{}.jsonl'.format(screen_name1)<br/>    timeline_file2 = 'user_timeline_{}.jsonl'.format(screen_name2)<br/>    with open(timeline_file1) as f1, open(timeline_file2) as f2:<br/>        favorited_count1, retweet_count1 = [], []<br/>        favorited_count2, retweet_count2 = [], []<br/>        for line in f1:<br/>            tweet = json.loads(line)<br/>            favorited_count1.append(tweet['favorite_count'])<br/>            retweet_count1.append(tweet['retweet_count'])</span><span id="0e62" class="nl lr iq nh b gy nq nn l no np">for line in f2:<br/>            tweet = json.loads(line)<br/>            favorited_count2.append(tweet['favorite_count'])<br/>            retweet_count2.append(tweet['retweet_count'])</span></pre><ul class=""><li id="1bfb" class="mn mo iq kv b kw kx kz la lc nc lg nd lk ne lo nf mt mu mv bi translated">完整的脚本可以在我的<a class="ae ks" href="https://github.com/owecodes/mining-twitter" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到</li></ul><p id="07ee" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">挖掘社交社区</strong></p><ul class=""><li id="66c4" class="mn mo iq kv b kw kx kz la lc nc lg nd lk ne lo nf mt mu mv bi translated">一个社会群体是一群有着共同条件的人，比如地理区域，相同的宗教信仰，或者有着相同兴趣的人。</li><li id="6012" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated"><strong class="kv ir">明确的社区</strong> —有共同兴趣的人明确知道他们是否属于某个社区，并且通常了解所述社区中的其他成员是谁。</li><li id="9a50" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated"><strong class="kv ir">隐性社区</strong>——当成员拥有共同的兴趣，但彼此之间没有明确而紧密的联系时，隐性社区就出现了</li></ul><p id="b2f1" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">在下一节中，我将演示我如何分析用户数据，以便<strong class="kv ir">将一堆用户资料</strong>分成组，目的是发现用户的隐含社区。</p><p id="21af" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">聚类分析</strong></p><p id="0763" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">聚类分析是一种机器学习技术，用于以这样一种方式对项目进行分组，即同一聚类中的对象彼此相似，而与其他聚类中的对象不相似。以下是该脚本的一些关键要点。</p><ul class=""><li id="7571" class="mn mo iq kv b kw kx kz la lc nc lg nd lk ne lo nf mt mu mv bi translated">我用了 K-means 算法来达到这个目的。</li><li id="7114" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">每个用户简档被表示为一个向量。基于用户描述，我使用 TF-IDF 方法来衡量一个词在给定上下文中的权重/重要性</li><li id="dfe8" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">我们可以限制 tfidf 矢量器提取的要素数量(通过显式指定 max_features 或指定 min_df 和 max_df 的范围)。基于文档频率排除特征的想法是为了避免不具有代表性的特征。限制它们也可以用来加速计算。</li><li id="bad4" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">ngram 是 n 个项目的连续序列，在这个上下文中，它指的是被标记为单词序列的文本。使用 ngrams 的好处是它能够捕捉短语。</li><li id="c6ee" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">下面是我如何根据用户 bios 进行聚类分析的。</li></ul><pre class="kh ki kj kk gt ng nh ni nj aw nk bi"><span id="6eb9" class="nl lr iq nh b gy nm nn l no np">from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.cluster import KMeans</span><span id="5d03" class="nl lr iq nh b gy nq nn l no np">def get_parser():<br/>    parser = ArgumentParser("Clustering for followers")<br/>    parser.add_argument('--filename') <br/>    parser.add_argument('--k', type=int) <br/>    parser.add_argument('--min-df', type=int, default=2) <br/>    parser.add_argument('--max-df', type=float, default=0.8) <br/>    parser.add_argument('--max-features', type=int, default=None) <br/>    parser.add_argument('--no-idf', dest='user_idf', default=True, action='store_false') <br/>    parser.add_argument('--min-ngram', type=int, default=1) <br/>    parser.add_argument('--max-ngram', type=int, default=1) <br/>    return parser</span><span id="461f" class="nl lr iq nh b gy nq nn l no np">if __name__ == '__main__':<br/>    parser = get_parser()<br/>    args = parser.parse_args()<br/>    if args.min_ngram &gt; args.max_ngram:<br/>        print("Error: incorrect value for --min--ngram ({}): it cant be higher than \<br/>        --max--value ({})".format(args.min_ngram, args.max_ngram))<br/>        sys.exit(1)<br/>    with open(args.filename) as f:<br/><strong class="nh ir">        #load data</strong></span><span id="9dcd" class="nl lr iq nh b gy nq nn l no np">users = []<br/>        for line in f:<br/>            profile = json.loads(line)<br/>            users.append(profile['description'])<br/><strong class="nh ir">        #create vectorizer</strong><br/>        vectorizer = TfidfVectorizer(max_df=args.max_df,<br/>                                    min_df=args.min_df,<br/>                                    max_features=args.max_features,<br/>                                    stop_words='english',<br/>                                    ngram_range=(args.min_ngram, args.max_ngram),<br/>                                    use_idf=args.user_idf)</span><span id="0a75" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir">#fit data</strong><br/>        X = vectorizer.fit_transform(users)<br/>        print("Data dimensions: {}".format(X.shape))</span><span id="d15c" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir">#perform clustering</strong><br/>        km = KMeans(n_clusters=args.k)<br/>        km.fit(X)<br/>        clusters = defaultdict(list)<br/>        for i, label in enumerate(km.labels_):<br/>            clusters[label].append(users[i])</span><span id="af0a" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir">#print 10 user description of this cluster</strong></span><span id="f316" class="nl lr iq nh b gy nq nn l no np">for label, description in clusters.items():<br/>            print("--------- Cluster {}".format(label+i))<br/>            for desc in description[:10]:<br/>                print(desc)</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi ny"><img src="../Images/081a136fe64f5a1785e0dc781efc84d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FWdtsBWIwhsTKRqRZUnK1Q.png"/></div></div><p class="ko kp gj gh gi kq kr bd b be z dk translated">聚类分析的结果</p></figure><p id="02de" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">因此，您可以看出，该脚本在根据用户个人资料中的简历对用户进行细分方面表现得相当不错。我在这个例子中使用了我的 twitter 个人资料，你可以看出我的社区中有一些投资者、技术爱好者、企业家和宗教人士。这让我想起了一句流行的话——“给我看你的朋友，我就知道你是谁” 。该脚本还可以用来更深入地研究其他用户的个人资料，以发现他们所属的隐含社区。</p><p id="f205" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">图形挖掘和会话分析</strong></p><p id="cb78" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">当许多用户参与对话时，一个“回复”网络就出现了。下图展示了 twitter 上的一个对话模型，节点是 tweets，边是“回复”关系。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nz"><img src="../Images/696a162f6ae3b4fa45d4194a499486d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ridjdlwjYvSllr6gtPEH6A.png"/></div></div><p class="ko kp gj gh gi kq kr bd b be z dk translated">对话模型</p></figure><p id="be00" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">自然，这种结构可以很容易地映射到图形数据结构。该模型的一些关键要点:</p><ul class=""><li id="ca54" class="mn mo iq kv b kw kx kz la lc nc lg nd lk ne lo nf mt mu mv bi translated">一对多基数</li><li id="f2c2" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">没有周期</li></ul><p id="67f8" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">出于这些原因，我建立了一个<strong class="kv ir">有向无环图</strong>的模型，并使用这个图的属性和图论中的算法来挖掘对话。下面是我使用的图形挖掘技术的代码片段。</p><pre class="kh ki kj kk gt ng nh ni nj aw nk bi"><span id="40e4" class="nl lr iq nh b gy nm nn l no np">import sys<br/>import json<br/>from operator import itemgetter<br/>import networkx as nx</span><span id="611e" class="nl lr iq nh b gy nq nn l no np">def usage():<br/>    print("Usage")<br/>    print("python {} &lt;filename&gt;".format(sys.argv[0]))</span><span id="0e7a" class="nl lr iq nh b gy nq nn l no np">if __name__ == "__main__":<br/>    if len(sys.argv) != 2:<br/>        usage()<br/>        sys.exit(1)</span><span id="4454" class="nl lr iq nh b gy nq nn l no np">fname = sys.argv[1]<br/>    with open(fname) as f: #takes in a jsonl file of tweets as input<br/>        graph = nx.DiGraph()<br/>        for line in f:<br/>            tweet = json.loads(line)<br/>            if 'id' in tweet:<br/>                graph.add_node(tweet['id'],<br/>                                tweet=tweet['text'],<br/>                                author=tweet['user']['screen_name'],<br/>                                created_at=tweet['created_at'])<br/>                if tweet['in_reply_to_status_id']:<br/>                    reply_to = tweet['in_reply_to_status_id']<br/>                    if reply_to in graph and tweet['user']['screen_name'] != graph.node[reply_to]['author']:  #if the user is not replying to themselves<br/>                        graph.add_edge(tweet['in_reply_to_status_id'], tweet['id'])<br/>        <br/><strong class="nh ir">#Print some basic stats</strong></span><span id="3ea6" class="nl lr iq nh b gy nq nn l no np">print(nx.info(graph))</span><span id="1bde" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir">#Find most replied tweet </strong><br/>        sorted_replied = sorted(graph.degree(), key=itemgetter(1), reverse=True)</span><span id="0446" class="nl lr iq nh b gy nq nn l no np">most_replied_id, replies = sorted_replied[0]<br/>        print("Most replied tweet ({} replies:".format(replies))<br/>        print(graph.node[most_replied_id])</span><span id="d344" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir">#Find longest conversation (longest path)</strong><br/>        print("Longest discussion:")<br/>        longest_path = nx.dag_longest_path(graph)<br/>        for tweet_id in longest_path:<br/>            node = graph.node[tweet_id]<br/>            print("{} (by {} at {})".format(node['tweet'], node['author'], node['created_at']))</span></pre><ul class=""><li id="b7d0" class="mn mo iq kv b kw kx kz la lc nc lg nd lk ne lo nf mt mu mv bi translated">节点的入度:给定 tweet 的回复数量</li><li id="0045" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">根:对话的开始</li><li id="d9a9" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">叶子:谈话的结束</li><li id="3686" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">最长路径算法:用于查找最长对话的 tweet。</li><li id="430b" class="mn mo iq kv b kw mw kz mx lc my lg mz lk na lo nf mt mu mv bi translated">我使用 networkX 库，因为它提供了图形结构的高效计算</li></ul><p id="e550" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">动态地图</strong></p><p id="eb89" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">最后，为了总结我的分析，我通过在地图上绘制它们，创建了这些推文来自哪里的可视化表示。</p><p id="024d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">地理数据的提取</strong></p><p id="8d4f" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">使用 GeoJSON 库，我从一个流文件中提取了每条 tweet 的几何数据。我用这个地理数据的 geo.json 扩展名创建了一个输出文件</p><pre class="kh ki kj kk gt ng nh ni nj aw nk bi"><span id="5335" class="nl lr iq nh b gy nm nn l no np"><strong class="nh ir">#read dataset of tweets in jsonl file and produce a geoson file </strong>associted with geographical data<br/>import json<br/>from argparse import ArgumentParser</span><span id="56c5" class="nl lr iq nh b gy nq nn l no np">def get_parser():<br/>    parser = ArgumentParser()<br/>    parser.add_argument('--tweets')<br/>    parser.add_argument('--geojson')<br/>    return parser</span><span id="9d23" class="nl lr iq nh b gy nq nn l no np">if __name__ == "__main__":<br/>    parser = get_parser()<br/>    args = parser.parse_args()</span><span id="c032" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir">#Read tweet collection and build geo data structure.</strong><br/>    with open(args.tweets, 'r') as f: #read dataset of tweets<br/>        geo_data = {<br/>        'type' : "FeatureCollection",<br/>        'features' : []<br/>        }</span><span id="4173" class="nl lr iq nh b gy nq nn l no np">for line in f:<br/>            tweet = json.loads(line)<br/>            try:<br/>                if tweet['coordinates']:<br/>                    geo_json_feature = {<br/>                        "type" : "Feature",<br/>                        "geometry": {<br/>                        "type" : "Point",<br/>                        "coordinates" : tweet['coordinates']['coordinates']<br/>                        },<br/>                        "properties": {<br/>                        "text" : tweet['text'],<br/>                        "created_at" : tweet['created_at']<br/>                        }<br/>                    }<br/>                    geo_data['features'].append(geo_json_feature)<br/>            except KeyError:<br/>                #Skip if json doc is not a tweet (errors, etc)<br/>                continue<br/><strong class="nh ir">    #Save geo data</strong><br/>    with open(args.geojson, 'w') as fout:<br/>        fout.write(json.dumps(geo_data, indent=4))</span></pre><p id="c5f4" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">创建动态地图</strong></p><p id="5166" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">我使用了 leav 库来生成一个交互式地图。优点是，HTML 无缝地处理 python 数据结构与 Javascript、HTML 和 CSS 组件之间的转换。</p><pre class="kh ki kj kk gt ng nh ni nj aw nk bi"><span id="f64b" class="nl lr iq nh b gy nm nn l no np">from argparse import ArgumentParser<br/>import folium<br/>from folium.plugins import MarkerCluster<br/>import json</span><span id="8856" class="nl lr iq nh b gy nq nn l no np">def get_parser():<br/>    parser = ArgumentParser()<br/>    parser.add_argument('--geojson')<br/>    parser.add_argument('--map')<br/>    return parser</span><span id="c8fa" class="nl lr iq nh b gy nq nn l no np">def make_map(geojson_file, map_file):</span><span id="b2d8" class="nl lr iq nh b gy nq nn l no np">tweet_map = folium.Map(Location=[50, 5], max_zoom=20)</span><span id="f203" class="nl lr iq nh b gy nq nn l no np">marker_cluster = MarkerCluster().add_to(tweet_map)</span><span id="58ec" class="nl lr iq nh b gy nq nn l no np">geodata= json.load(open(geojson_file))</span><span id="2e1e" class="nl lr iq nh b gy nq nn l no np">for tweet in geodata['features']:<br/>        tweet['geometry']['coordinates'].reverse()<br/>        marker = folium.Marker(tweet['geometry']['coordinates'], popup=tweet['properties']['text'])<br/>        marker.add_to(marker_cluster)</span><span id="9ff9" class="nl lr iq nh b gy nq nn l no np"><strong class="nh ir">#Save to HTML map file</strong><br/>    tweet_map.save(map_file)</span><span id="c1a8" class="nl lr iq nh b gy nq nn l no np">if __name__ == '__main__':<br/>    parser = get_parser()<br/>    args = parser.parse_args()<br/>    make_map(args.geojson, args.map)</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi oa"><img src="../Images/d26219068b2ae5061ffb609d420cec34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Cps6_DcWgZCgwBt82iLdQ.png"/></div></div><p class="ko kp gj gh gi kq kr bd b be z dk translated">动态地图(缩小)</p></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi gj"><img src="../Images/de2fe21bbb73f3d9e431e35ed5384123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DXqBuENaMRwIpVKnwlz0yw.png"/></div></div><p class="ko kp gj gh gi kq kr bd b be z dk translated">动态地图(放大)</p></figure><p id="ab78" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">正如你所看到的，这个脚本可视化了人们在 NBA 传奇人物科比·布莱恩特去世时发微博的不同地点。该脚本将来自特定区域的用户推文聚集在一起，并精确定位每条推文发布的确切位置。</p><p id="dd46" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">这些结果是在事件公开后几个小时，从 15 分钟的推特风暴流中产生的。您可以在这里 查看完整的<a class="ae ks" href="http://www.paulowe.com/tweet_map.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir">地图并与之互动</strong></a></p><h1 id="57a2" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">结论</h1><p id="f85c" class="pw-post-body-paragraph kt ku iq kv b kw mi jr ky kz mj ju lb lc mk le lf lg ml li lj lk mm lm ln lo ij bi translated">这都是为了挖掘 Twitter 数据。我真的很喜欢参与这个项目。我的 GitHub 上有完整的源代码，可以提供给任何对实现这个项目感兴趣的人。</p></div></div>    
</body>
</html>