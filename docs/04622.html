<html>
<head>
<title>Boosting performance with XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 XGBoost 提升性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/boosting-performance-with-xgboost-b4a8deadede7?source=collection_archive---------30-----------------------#2020-04-24">https://towardsdatascience.com/boosting-performance-with-xgboost-b4a8deadede7?source=collection_archive---------30-----------------------#2020-04-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="99cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇博客中，我们将借助一个例子来看看 XGBoost 是如何工作的，以及 XGBoost 的一些重要特性。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/c551eb4acddf1d16b974722958f4cbfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*OUSzhiIb3-kUibQ3"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">来源:<a class="ae la" href="https://www.positive.news/environment/people-share-their-love-for-trees-and-forests-on-international-day-of-forests/" rel="noopener ugc nofollow" target="_blank">正面消息</a></p></figure><p id="3adb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以，我们很多人都听说过树模型和助推技术。让我们将这些概念放在一起，讨论 XGBoost，这是目前最强大的机器学习算法。</p><p id="a967" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">XGboost 呼吁极端梯度助推树木。</p><blockquote class="lb lc ld"><p id="e0c9" class="jq jr le js b jt ju jv jw jx jy jz ka lf kc kd ke lg kg kh ki lh kk kl km kn im bi translated">【XGBoost 这个名字实际上是指提升树算法的计算资源极限的工程目标。这也是很多人使用 XGBoost 的原因。</p></blockquote><p id="62bb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">自 2014 年推出以来，XGBoost 具有很高的预测能力，比其他梯度增强技术快近 10 倍。它还包括各种正则化，减少过度拟合，提高整体性能。因此也被称为“<strong class="js iu">正则化增强</strong>技术。XGBoost 在性能和速度方面证明了自己的实力。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi li"><img src="../Images/2476feca852fd54cfaf439b3fbd61412.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/0*40rrWwr28SFOAq_K"/></div></figure><p id="f866" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">好吧！！！别急，我们来说说助推</p><h1 id="903e" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">什么是助推树？</h1><p id="b0e9" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">我们遇到过许多基于树的算法，如决策树，在这些算法中，我们用来在特定数据集上训练我们的单一模型，可能需要一些参数调整。同样在集合模型中，我们习惯于单独训练所有的模型。</p><p id="de26" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Boosting 也是一种集成技术，它将许多模型结合起来给出最终的模型，但不是单独评估所有模型，而是依次训练模型。这意味着，每一个新模型都被训练来纠正前一个模型的错误，当没有进一步的改进时，该序列被停止。这就是为什么它更准确。</p><h1 id="d087" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">安装 XGBoost</h1><p id="f2c2" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">在<a class="ae la" href="http://xgboost.readthedocs.io/en/latest/build.html" rel="noopener ugc nofollow" target="_blank"> XGBoost 文档网站</a>上有全面的安装指南。</p><p id="85f6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它涵盖了 Linux、Mac OS X 和 Windows 的安装。</p><p id="1dad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它还涵盖了在 R 和 Python 等平台上的安装。</p><h1 id="b35f" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">设置我们的数据</h1><p id="b7fe" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">所以，第一件事就是为我们的模型准备数据。我们将使用来自 Scikit Learn 的 iris flower 数据集。</p><p id="9941" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里，我们用 python 从 Sklearn 加载了数据集，还导入了 XGBoost 库</p><pre class="kp kq kr ks gt mm mn mo mp aw mq bi"><span id="eabe" class="mr lk it mn b gy ms mt l mu mv">from sklearn import datasets<br/>import xgboost as xgb</span><span id="7826" class="mr lk it mn b gy mw mt l mu mv">iris = datasets.load_iris()<br/>X = iris.data<br/>y = iris.target</span></pre><p id="e976" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们必须将数据集分成两部分:训练和测试数据。这是检验我们的模型表现如何的重要一步。因此，我们将把数据分成 80%-20%的部分。</p><pre class="kp kq kr ks gt mm mn mo mp aw mq bi"><span id="519a" class="mr lk it mn b gy ms mt l mu mv">from sklearn.model_selection import train_test_split</span><span id="fa88" class="mr lk it mn b gy mw mt l mu mv">X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)</span></pre><p id="d6c4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与其他算法不同，XGBoost 需要将我们的数据转换成特定的格式，即 DMatrix。</p><p id="81e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> DMatrix </strong>是由<strong class="js iu"> XGBoost </strong>使用的内部数据结构，针对内存效率和训练速度进行了优化。</p><pre class="kp kq kr ks gt mm mn mo mp aw mq bi"><span id="a3dc" class="mr lk it mn b gy ms mt l mu mv">D_train = xgb.DMatrix(X_train, label=Y_train)<br/>D_test = xgb.DMatrix(X_test, label=Y_test)</span></pre><p id="ce1b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们将 NumPy 数组数据转换成 DMatix 格式，以提供给我们的模型。但在此之前，我们需要定义我们的模型。</p><h1 id="9fef" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">定义 XGBoost 模型</h1><p id="43b4" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">我们要做的第一件事是定义梯度下降系综的参数。我们有 N 个参数可用于我们的模型，但现在，我们将把重点放在一些重要的。可能参数的完整列表可在官方<a class="ae la" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> XGBoost 网站</a>上获得。</p><pre class="kp kq kr ks gt mm mn mo mp aw mq bi"><span id="33d0" class="mr lk it mn b gy ms mt l mu mv">param = {<br/>    'eta': 0.2, <br/>    'max_depth': 4,  <br/>    'objective': 'multi:softprob',  <br/>    'num_class': 4<br/>    }</span><span id="2908" class="mr lk it mn b gy mw mt l mu mv">epochs = 20</span></pre><p id="2037" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我们的参数:</p><ul class=""><li id="0477" class="mx my it js b jt ju jx jy kb mz kf na kj nb kn nc nd ne nf bi translated"><strong class="js iu"> max_depth </strong>:被训练决策树的最大深度</li><li id="5855" class="mx my it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><strong class="js iu">目标</strong>:使用损失函数</li><li id="9db5" class="mx my it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><strong class="js iu"> num_class </strong>:数据集中类的数量</li><li id="22d0" class="mx my it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><strong class="js iu"> eta </strong>:学习率</li></ul><p id="8f95" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们已经知道的，这种模型是按顺序工作的，这使得它更加复杂。这种技术很容易过度拟合。</p><p id="d8cd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">eta 参数/学习率帮助我们的算法防止过度拟合，这不仅是通过将新树的预测添加到具有全权重的集合中，而且 eta 将乘以正在添加的残差以减少它们的权重。</p><p id="60af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">注</strong>:建议在 0.1 到 0.3 的范围内使用较小的 eta 值</p><p id="25da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在已经定义了模型，让我们来训练它</p><h2 id="f277" class="mr lk it bd ll nl nm dn lp nn no dp lt kb np nq lx kf nr ns mb kj nt nu mf nv bi translated">培训和测试</h2><pre class="kp kq kr ks gt mm mn mo mp aw mq bi"><span id="97a1" class="mr lk it mn b gy ms mt l mu mv">model = xgb.train(param, D_train, steps)</span></pre><p id="bf72" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一个与 Scikit Learn 非常相似的过程，运行评估也非常熟悉。</p><pre class="kp kq kr ks gt mm mn mo mp aw mq bi"><span id="3d23" class="mr lk it mn b gy ms mt l mu mv">import numpy as np<br/>from sklearn.metrics import precision_score, recall_score, accuracy_score</span><span id="64c0" class="mr lk it mn b gy mw mt l mu mv">preds = model.predict(D_test)<br/>best_preds = np.asarray([np.argmax(line) for line in preds])</span><span id="6302" class="mr lk it mn b gy mw mt l mu mv">print("Precision = {}".format(precision_score(Y_test, best_preds, average='macro')))<br/>print("Recall = {}".format(recall_score(Y_test, best_preds, average='macro')))<br/>print("Accuracy = {}".format(accuracy_score(Y_test, best_preds)))</span></pre><p id="fe13" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/a49b1a9d1a24fa1f95e2438d55c0b96a.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/0*dOYkgz9tG1ITeXdz"/></div></figure><p id="e676" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">太棒了，我们达到了 90%以上的准确率</p><p id="2cd6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如上所述，我们有很多参数，选择错误的参数，可能会影响你的模型性能很多。</p><p id="6e59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以这里的问题是:<strong class="js iu">如何选择合适的参数？</strong></p><p id="906a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">嗯，用不同的值来比较模型性能太容易了。让我们看看</p><h1 id="677d" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">寻找最佳参数</h1><p id="d032" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">设置任何 ML 模型的最佳超参数都是一项挑战。那么为什么不让 Scikit Learn 帮你做呢？我们可以很容易地将 Scikit Learn 的网格搜索与 XGBoost 分类器结合起来:</p><pre class="kp kq kr ks gt mm mn mo mp aw mq bi"><span id="effb" class="mr lk it mn b gy ms mt l mu mv">from sklearn.model_selection import GridSearchCV</span><span id="5790" class="mr lk it mn b gy mw mt l mu mv">clf = xgb.XGBClassifier()<br/>parameters = {<br/>     "eta"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,<br/>     "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],<br/>     "min_child_weight" : [ 1, 3, 5, 7 ],<br/>     "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],<br/>     "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]<br/>     }</span><span id="b0e7" class="mr lk it mn b gy mw mt l mu mv">grid = GridSearchCV(clf,<br/>                    parameters, n_jobs=4,<br/>                    scoring="neg_log_loss",<br/>                    cv=3)</span><span id="7072" class="mr lk it mn b gy mw mt l mu mv">grid.fit(X_train, Y_train)</span></pre><p id="5941" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">输出</strong>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/c8c753ce71a29813ec06c31348e613fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/0*LdgpuNP0u41Cimpg"/></div></figure><p id="9c6d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你有时间的话，只在大数据集上这样做——进行网格搜索本质上是多次训练决策树的集合！</p><p id="a6f3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦您的 XGBoost 模型经过训练，您就可以将它的可读描述转储到一个文本文件中:</p><pre class="kp kq kr ks gt mm mn mo mp aw mq bi"><span id="c4a8" class="mr lk it mn b gy ms mt l mu mv">model.dump_model('dump.raw.txt')</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/f5b17071b0ba0ec53484268e776b419c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/0*A8aucHUBZI-RFqFE"/></div></figure><p id="fd32" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是我们如何创建 XGBoost 模型，并为其选择理想的超参数。</p><p id="2e85" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">保持曲调，快乐学习</p><p id="1524" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最初发布于:<a class="ae la" href="https://blog.knoldus.com/machinex-boosting-performance-with-xgboost/" rel="noopener ugc nofollow" target="_blank">https://blog . knol dus . com/machinex-boosting-performance-with-xgboost/</a></p><p id="f419" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 LinkedIn 和 Twitter 上关注我，了解更多信息:</p><ul class=""><li id="024a" class="mx my it js b jt ju jx jy kb mz kf na kj nb kn nc nd ne nf bi translated"><a class="ae la" href="https://www.linkedin.com/in/shubham-goyal-0946b7127" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="abf6" class="mx my it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae la" href="https://twitter.com/Shubham99142929" rel="noopener ugc nofollow" target="_blank">推特</a></li></ul></div></div>    
</body>
</html>