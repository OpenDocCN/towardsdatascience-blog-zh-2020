<html>
<head>
<title>How to Differentiate Gradient Descent Objective Function in 3 simple steps</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用三个简单的步骤区分梯度下降目标函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-differentiate-gradient-descent-objective-function-in-3-simple-steps-b9d58567d387?source=collection_archive---------26-----------------------#2020-08-14">https://towardsdatascience.com/how-to-differentiate-gradient-descent-objective-function-in-3-simple-steps-b9d58567d387?source=collection_archive---------26-----------------------#2020-08-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fda2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">拿起笔和纸</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e85487f1e24f3b72da31caccdbcc6d9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VJdv4cH0Xl_mQz7v"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的<a class="ae ky" href="https://unsplash.com/@thisisengineering?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> ThisisEngineering RAEng </a></p></figure><p id="2f6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如今，我们可以了解通常为学术团体保留的领域。从<em class="lv">人工智能</em>到<em class="lv">量子物理学</em>，我们可以浏览互联网上的大量信息并从中受益。</p><p id="ff86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，信息的可用性有一些缺点。我们需要意识到大量未经证实的来源，充满了事实错误(这是一个完全不同的讨论主题)。更重要的是，我们可以习惯于通过谷歌搜索来轻松获得答案。因此，我们经常认为它们是理所当然的，并在没有更好理解的情况下使用它们。</p><p id="61ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们自己发现事物的过程是学习的一个重要部分。让我们参加这样一个实验，计算一个<strong class="lb iu">线性回归</strong>的<strong class="lb iu">梯度下降</strong>算法背后的导数。</p><h1 id="5600" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">一点点介绍</h1><p id="1361" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">线性回归是一种统计方法，可以用来模拟变量之间的关系[1，2]。它由一个线性方程描述:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/362f32365d5f5d71e7aa0a9a5b60d3a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*yWQdH8hDoBc9n7TmOUIVIA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线方程(图片由作者提供)。</p></figure><p id="d2fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有两个参数<em class="lv">θ₀</em>和<em class="lv">θ₁</em>和一个<em class="lv">变量</em>x。有了数据点，我们可以找到最佳的参数来拟合我们的数据集线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/68485b32da134b8913dcdcc4e0bf56b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4yrAK5rH-k1mmIDjjHR_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将直线拟合到数据集(图片由作者提供)。</p></figure><p id="8282" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好，现在梯度下降[2，3]。它是一种迭代算法，广泛用于机器学习(有许多不同的风格)。我们可以用它来自动找到我们生产线的最佳参数。</p><p id="e5cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们需要优化由以下公式定义的目标函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/bf54c8fa5f1713bd40e6434ef10b5427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NwmUF1mJnQuTIlZgSUqRbw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归目标函数(图片由作者提供)。</p></figure><p id="121c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个函数中，我们迭代数据集中的每个点<em class="lv"> (xʲ，yʲ) </em>。然后我们为<em class="lv"> xʲ </em>计算一个函数<em class="lv"> f </em>的值，以及当前的θ参数(<em class="lv">θ₀</em>、<em class="lv">θ₁</em>)。我们取一个结果，然后减去 yʲ。最后，我们将它平方并加到总和中。</p><p id="7822" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后在梯度下降公式(每次迭代更新<em class="lv">θ₀</em>和<em class="lv">θ₁</em>)中，我们可以在方程的右边找到这些神秘的导数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/1930e64c498b1116a80b6968b3eb41f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VKuW_BhmxdJ5dPxhq6zmlA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度下降公式(图片由作者提供)。</p></figure><p id="01df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些是目标函数<em class="lv">Q(θ)的导数。</em>有两个参数，需要计算两个导数，每个<em class="lv">θ</em>一个。让我们继续，用 3 个简单的步骤来计算它们。</p><h1 id="2e25" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">第一步。链式法则</h1><p id="34fc" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们的目标函数是一个复合函数。我们可以认为它有一个<em class="lv">【外层】</em>功能和一个<em class="lv">【内层】</em>功能[1]。为了计算复合函数的导数，我们将遵循链式法则:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/79ef9cf0314dbf975f8ca8320b6ae6a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMWZS5kpE-WCMz8Wu6Rd9w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">链式法则公式(图片由作者提供)。</p></figure><p id="5029" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的例子中，<em class="lv"> "outer" </em>部分是关于将括号内的所有内容(<em class="lv"> "inner function" </em>)提升到 2 次方。根据规则，我们需要将<em class="lv">“外部函数”</em>的导数乘以<em class="lv">“内部函数”</em>的导数。看起来是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/58c10b830180dca82b102a7494f22e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fDvs5cwwXV7icux8oYVCqQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将链式法则应用于目标函数(图片由作者提供)。</p></figure><h1 id="6713" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">第二步。权力规则</h1><p id="1aa5" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">下一步是计算幂函数的导数[1]。让我们回忆一下一个导数幂律公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/5a79c8661019bbc22c20ee0eccfd40a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xA1xusSNdaBuOuxb3ZzWnA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">幂律公式(图片由作者提供)。</p></figure><p id="66d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的<em class="lv">【外函数】</em>只是一个二次幂的表达式。所以我们把<em class="lv"> 2 </em>放在整个公式之前，剩下的就不做了(<em class="lv"> 2 -1 = 1 </em>，表达式的一次幂就是那个表达式)。</p><p id="3315" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第二步之后，我们有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/924ae392d4711d8f58d49e33986a544b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDBfURdjPSjpasvfMU2BSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将幂法则应用于目标函数(图片由作者提供)。</p></figure><p id="6432" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还需要计算一个<em class="lv">【内函数】</em>(公式右侧)的导数。让我们进入第三步。</p><h1 id="fa84" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">第三步。常数的导数</h1><p id="7588" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">最后一条规则是最简单的。它用于确定常数的导数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/8c2fc643ff53abfe5f0b37b56c04e080.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*1lZFh4vKjCEca-oRvrC5Hg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">常数的导数(图片由作者提供)。</p></figure><p id="943b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为常数意味着，没有变化，常数的导数等于零[1]。例如<em class="lv">f’(4)= 0</em>。</p><p id="6110" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">记住这三条规则，让我们来分解一下<em class="lv">“内在功能”</em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/fbeb0a52c855a152781bc60642db51b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EipHj8qThOmiNbgQ4rJ8Jg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">内函数导数(图片由作者提供)。</p></figure><p id="b2cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的梯度下降目标函数的棘手之处在于<em class="lv"> x </em>不是一个变量。<em class="lv"> x </em>和<em class="lv"> y </em>是来自数据设定点的常数。当我们寻找我们生产线的最佳参数时，<em class="lv">θ₀</em>和<em class="lv">θ₁</em>是变量。这就是为什么我们计算两个导数，一个关于θ₀，一个关于θ₁.</p><p id="44c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们开始计算相对于<em class="lv">θ₀</em>的导数。这意味着<em class="lv">θ₁</em>将被视为常量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/8d54751f0ba4cd50467f2f43920292b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V9GSfMEJi4XSoDXeh7X1Bw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">相对于<em class="ne">θ₀的内部函数导数(图片由作者提供)。</em></p></figure><p id="d2cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以看到常数部分被设置为零。θ₀怎么了？因为它是一个一次幂的变量(<em class="lv"> a =a </em>)，所以我们应用了幂法则。这导致<em class="lv">θ₀</em>被提升到零的幂。当我们将一个数提升到零的幂时，它等于 1 ( <em class="lv"> a⁰=1 </em>)。就是这样！我们对θ₀的导数等于 1。</p><p id="556e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们得到了关于θ₀:的整体导数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/08cff965c6a5a935ba81894b72acd34a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3Ik7WNKNOhvNP4Qk0dlow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">关于<em class="ne">θ₀的目标函数导数(图片由作者提供)。</em></p></figure><p id="1ffc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在是时候计算关于θ₁的导数了。这意味着我们把θ₀视为一个常数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/4c3096f0a6b5298cddefe33ec95dad47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uGi2nV6R6qNLwq1NNxy0kg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">关于<em class="ne">θ₁</em>的内部函数导数</p></figure><p id="f3ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与前面的例子类似，<em class="lv">θ₁</em>被视为一个一次幂的变量。然后，我们应用了一个幂法则，它将<em class="lv">θ₁</em>减少到 1。然而<em class="lv">θ₁</em>乘以<em class="lv"> x </em>，所以我们最终得到的导数等于<em class="lv">x</em></p><p id="5f65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于<em class="lv">θ₁</em>的导数的最终形式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/c6e63ed8282139b7226a8626d21c6d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GyRMvQI50cyuqw4az0WpCQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">关于<em class="ne">θ₁的目标函数导数(图片由作者提供)。</em></p></figure><h1 id="ccc1" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">完全梯度下降配方</h1><p id="4570" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们计算了梯度下降算法所需的导数！让我们把它们放在它们该在的地方:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/1d160df9612d81f603f0b975eb1fc668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ShUM1iI9RmfMVofd-DzKNg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">包含目标函数导数的梯度下降公式(图片由作者提供)。</p></figure><p id="b034" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过做这个练习，我们对公式的起源有了更深的理解。我们并不把它当作我们在旧书中找到的魔咒，而是积极地经历分析它的过程。我们把这个方法分解成更小的部分，我们意识到我们可以自己完成计算，然后把它们放在一起。</p><p id="9781" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">时不时抓起笔和纸，解决一个问题。你可以找到一个你已经成功使用的公式或方法，并试图通过分解它来获得更深层次的洞察力。它会给你很大的满足感，激发你的创造力。</p><h2 id="002b" class="nj lx it bd ly nk nl dn mc nm nn dp mg li no np mi lm nq nr mk lq ns nt mm nu bi translated">参考书目:</h2><ol class=""><li id="0afa" class="nv nw it lb b lc mo lf mp li nx lm ny lq nz lu oa ob oc od bi translated">K.a .斯特劳德，德克斯特 j .布斯，<em class="lv">工程数学</em>，ISBN:978–0831133276。</li><li id="72ca" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">Joel Grus，<em class="lv">从头开始的数据科学，第二版</em>，ISBN:978–1492041139</li><li id="906b" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">乔希·帕特森，亚当·吉布森，<em class="lv">深度学习</em>，ISBN:978–1491914250</li></ol></div></div>    
</body>
</html>