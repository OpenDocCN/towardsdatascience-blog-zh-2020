<html>
<head>
<title>Faster RCNN [1506.01497]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更快的RCNN [1506.01497]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/faster-rcnn-1506-01497-5c8991b0b6d3?source=collection_archive---------16-----------------------#2020-05-05">https://towardsdatascience.com/faster-rcnn-1506-01497-5c8991b0b6d3?source=collection_archive---------16-----------------------#2020-05-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="025e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">面向实时对象检测</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dda729562db92a572316f7033c022b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gmKoyPT_vT6flqDP.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://en.wikipedia.org/wiki/Object_detection" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Object_detection</a>作者:<a class="ae ky" href="https://commons.wikimedia.org/wiki/User:MTheiler" rel="noopener ugc nofollow" target="_blank"> Mtheiler </a></p></figure><p id="49a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经计划阅读主要的对象检测论文(虽然我已经浏览了大部分，但现在我将详细阅读它们，好到足以写一篇关于它们的博客)。这些论文与基于深度学习的对象检测相关。随时给建议或询问疑惑会尽我所能帮助大家。任何从这个领域开始的人都可以跳过许多这样的论文。当我看完所有的论文后，我也会写下它们的优先顺序/重要性。<br/>我写这篇博客是考虑到和我相似并且仍在学习的读者。虽然我会通过从各种来源(包括博客、代码和视频)深入理解论文来尽力写出论文的关键，但如果您发现任何错误，请随时在博客上指出或添加评论。我已经提到了我将在博客末尾涉及的论文列表。</p><p id="4ef8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始吧:)</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="add8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">老实说，我大约在两年前开始研究对象检测，并从更快的RCNN、Yolo和SSD等论文开始(我们将在接下来的博客中讨论这两篇论文)。这些题目让我很为难，很长一段时间都无法理解。我是深度学习的新手，没有耐心，也没有经验去研究完整的论文，也找不到任何博客能足够好地解释这些话题让我理解。在这篇博客中，我将尽可能地简化这篇文章，因此你可能会发现它有点大而且充满了理论。</p><p id="c6b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快速RCNN和RCNN使用诸如选择性搜索的区域提议算法来提议图像中对象的估计位置。这些算法是当前目标检测系统的测试时间瓶颈。很直观，由CNN提取的特征最终用于分类并给出图像的包围盒。因此，这些提取的特征具有检测图像中的对象所需的信息。更快的RCNN架构基于这一观察。区域提议算法由区域提议网络代替，该网络给出具有对象的区域的估计。该区域提议网络基于CNN，并从CNN提取的特征中给出区域提议。</p><p id="cec5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，更快的RCNN架构是提出区域的RPN和使用这些提出的区域给出对象的最终边界框的快速RCNN检测器的组合。因为快速RCNN和RPN都需要基于CNN的特征提取器来执行几乎相似的任务(RPN的最终任务是仅给出对象区域),所以我们可以使用单个特征提取器，而不是使用具有几乎相同权重的两个独立的模型。下图显示了Fast RCNN和RPN的统一结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/58923e98f6770911d1525ed05fc4519f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*SU3REQM_Q4kVsfxrnL4D2w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">快速RCNN网络(RPN+快速RCNN)来源:<a class="ae ky" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">快速RCNN论文</a>作者:任</p></figure><h2 id="62cc" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">什么是锚盒</h2><p id="be15" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">锚盒是现代物体探测器的主要部分。虽然在本文后面会讨论，但我觉得你应该在进入RPN之前了解一下。在对象检测中，我们试图为图像中的每个对象获得一个矩形框，因此在每个图像中有多个不同形状和大小的框。</p><p id="b4a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，忘记早期的架构如快速RCNN、RCNN如何解决这个问题，而是将其视为正常的深度学习问题，类似于图像分类，我们可以使用一个带有回归头的层来预测对象的边界框。在图像分类中，我们为每个图像输出一个标签，但是在对象检测中，我们可以在每个图像中有多个对象，因此每个图像需要多个输出。我们可以通过获得多个预测而不是一个来解决这个问题，但是模型如何知道哪个边界框是针对哪个对象的呢？</p><p id="a3e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快速RCNN和RCNN通过使用区域提议方法(如前所述，其具有缺点)解决了这个问题。在更快的RCNN中，我们使用深度神经网络来获得这些建议，因此我们可以说在某种程度上我们正在尝试使用该网络来解决对象检测的问题。因此，上面讨论的问题现在就有了眉目。</p><p id="06b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">锚箱是救世主。锚定框只不过是放置在图像中不同位置的一些参考框。在我们的特征图(CNN的输出)中为每个像素生成k个锚框。因此，锚盒的总数是h*w*k(h*w是特征图的输出大小)。这里是一个超参数。这些k锚盒有不同的大小和长宽比，这有助于覆盖各种形状和大小的对象。锚框是特定于数据集和该数据集中的对象类型的超参数(例如，在一些医学数据中，如果对象只能是单一尺寸，我们将只需要3个不同纵横比的锚框)。我们可以在下面看到这些锚盒。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/2bdeae074a621419955d6b629f14e6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/0*bjcSaSMT50oNLRPJ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">锚箱来源:<a class="ae ky" href="http://www.programmersought.com/article/8369776535/" rel="noopener ugc nofollow" target="_blank">程序员搜索</a></p></figure><p id="5181" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不是获得一个对象的原始回归输出，而是作为锚框的偏移来计算(我们将在后面看到如何训练使用锚框的模型)。在大多数情况下，这种偏移是锚定框的轻微移动，因为这些锚定框被放置在整个图像上。检测到的对象将被多个锚定框重叠，而不仅仅是1个。这些冗余预测随后使用非最大抑制移除。我们的模型的输出是4*k*h*w维度(对于每个锚框有一个框预测，对于每个锚框也预测分类分数，给出它包含对象的概率)。这在理论上将CNN能够探测到的物体数量限制在4*k*h*w，但实际上，这个数量已经足够大了。</p><p id="3f0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">锚盒可以通过使用不同大小的锚盒(上图中的红、绿、蓝盒子)来解决测试时使用多尺度的问题。</p><h2 id="0868" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">RPN</h2><p id="a4c8" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">区域提议网络以特征地图作为输入，并输出一组矩形对象提议，每个提议具有一个对象性分数。对象性测量对象和背景之间的分数(因此，背景的分数较低，具有对象的区域的分数较高)。选择性搜索的区域建议时间为每幅图像2秒，而RPN仅为10ms。</p><p id="0382" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更快的RCNN使用3种纵横比和3种比例的锚盒。因此，对于特征图中的每个像素，有9个锚框。</p><p id="809f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该架构是一个简单的卷积层，其内核大小为3*3，后面是两个完全连接的层(一个用于客观评分(分类)，另一个用于建议的回归)。这个全连接层是使用1*1卷积层实现的。分类图层的输出大小应为2*9(前景和背景)，回归图层的输出大小应为4*9(此处9是每个像素的锚点数量)。预测的总数现在是直观的，并且将是(4+2)*9*(H*W)(对于特征图中的每个像素)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/9ee2308c90149ad961820c972d15d60e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oaQIpEtYpN1ZQbLhZhs47g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">快RCNN论文</a>作者:任</p></figure><h2 id="2228" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">损失函数</h2><p id="bebf" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">在快速R-CNN中使用的损失函数类似于快速R-CNN，即多任务损失。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/ac42655e74fe8b46b5a1a1c5f18ad0de.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*FxZojbriYwv5hMwO9Gc08w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多任务损失函数求更快R-CNN来源:<a class="ae ky" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">更快RCNN论文</a>作者:任</p></figure><p id="44a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个等式中，pᵢ是预测概率(从cls输出)，pᵢ*是基本事实。类似地，tᵢ是预测边界框，tᵢ*是基本事实边界框。Lcₗs是分类损失(对数损失)，Lreg是平滑L₁损失。</p><p id="0c13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，回归偏移是从最近的锚盒计算的。为了与区域提议技术相关联，锚盒现在充当区域提议。一张40*60大小的特征地图，总共有40*60*9 ~ 20000个锚盒。所有锚箱在训练时都不会造成损失。IOU与地面真实值最高的锚和IOU重叠高于0.7的锚被分配正标签。IOU低于0.3的锚标为负。既不积极也不消极的锚对训练目标没有贡献。跨界锚点也将被忽略。</p><h2 id="7856" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">培养</h2><p id="34d0" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">快速RCNN架构是由RPN和快速R-CNN组成的统一网络，CNN层由两种架构共享。我们不能单独训练RPN和快速RCNN(它将给出不同的权重，因此我们需要为它们中的每一个通过CNN两次)。作者使用了下面讨论的4步训练算法(也讨论了其他一些算法，你可以在论文中看到):</p><ol class=""><li id="be79" class="ne nf it lb b lc ld lf lg li ng lm nh lq ni lu nj nk nl nm bi translated">使用初始训练的RPN和来自图像网络的预训练权重。</li><li id="e478" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">使用由步骤1 RPN生成的建议来训练快速RCNN。(此时两个网络不共享卷积层)。</li><li id="1e15" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">使用来自步骤2的卷积层训练RPN，并且仅更新RPN特有的层(卷积层的权重不更新)。</li><li id="b16e" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">保持这个卷积层，对快速R-CNN特有的层进行微调。</li></ol><p id="218b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练时，作为RPN输出的20000个锚(建议)首先通过移除跨界锚(给出6000个锚)来减少。应用非最大抑制(NMS)来移除阈值为0.7的冗余预测(给出2000个锚)。在NMS之后，使用排名前N位(分类分数)的建议区域。</p><h2 id="749a" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">测试</h2><p id="f5ae" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">更快的RCNN是一种完全基于深度学习的方法，具有统一的网络，不依赖于像选择性搜索提议这样的算法。因此，图像被直接传送到网络，作为输出给出预测。RPN(20k建议)的输出使用上一节最后一段中讨论的类似步骤进行二次抽样。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="0bc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是最重要的文章之一，也是理解未来大部分文章所必需的。如果我有不清楚的地方，请在评论中提出来，我会更新的。</p><p id="6c02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和平…</p><h2 id="552d" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">参考</h2><ol class=""><li id="f67a" class="ne nf it lb b lc mw lf mx li ns lm nt lq nu lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.01497.pdf</a></li><li id="81c6" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" rel="noopener ugc nofollow" target="_blank">https://tryolabs . com/blog/2018/01/18/faster-r-CNN-down-the-rabbit-hole-of-modern-object-detection/</a></li><li id="37c6" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">【https://d2l.ai/chapter_computer-vision/anchor.html T4】</li><li id="06f1" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="http://machinethink.net/blog/object-detection/" rel="noopener ugc nofollow" target="_blank">http://machinethink.net/blog/object-detection/</a></li><li id="5760" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/faster-r-cnn-for-object-detection-a-technical-summary-474c5b857b46">https://towards data science . com/faster-r-CNN-for-object-detection-a-technical-summary-474 C5 b 857 b 46</a></li></ol></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="d82d" class="nv me it bd mf nw nx ny mi nz oa ob ml jz oc ka mo kc od kd mr kf oe kg mu of bi translated">论文列表:</h1><ol class=""><li id="b08c" class="ne nf it lb b lc mw lf mx li ns lm nt lq nu lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1312.6229.pdf" rel="noopener ugc nofollow" target="_blank"> OverFeat:使用卷积网络的综合识别、定位和检测</a>。[ <a class="ae ky" rel="noopener" target="_blank" href="/overfeat-review-1312-6229-4fd925f3739f">链接到博客</a></li><li id="95fe" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">丰富的特征层次，用于精确的对象检测和语义分割(RCNN)。</a> [ <a class="ae ky" href="https://medium.com/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a" rel="noopener">链接到博客</a></li><li id="30f4" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络中的空间金字塔池(SPPNet)。</a> [ <a class="ae ky" href="https://medium.com/@sanchittanwar75/review-spatial-pyramid-pooling-1406-4729-bfc142988dd2" rel="noopener">链接到博客</a></li><li id="5d84" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">快速R-CNN</a>↓[<a class="ae ky" href="https://medium.com/@sanchittanwar75/fast-rcnn-1504-08083-d9a968a82a70" rel="noopener">链接到博客</a> ]</li><li id="6a81" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">更快的R-CNN:用区域提议网络实现实时目标检测。 ←你完成了这篇博客。</li><li id="e0a7" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的，实时的物体检测。</a>【博客链接】</li><li id="fdf3" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank"> SSD:单次多盒探测器</a>。[博客链接]</li><li id="36b8" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">R-FCN:通过基于区域的完全卷积网络的目标检测。【博客链接】</li><li id="3845" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1612.03144.pdf" rel="noopener ugc nofollow" target="_blank">用于目标检测的特征金字塔网络。</a>【博客链接】</li><li id="dbcf" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1701.06659.pdf" rel="noopener ugc nofollow" target="_blank"> DSSD:解卷积单粒子探测器</a>。[博客链接]</li><li id="6866" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank">密集物体检测的焦点丢失(视网膜网)。</a>【博客链接】</li><li id="0afd" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1804.02767.pdf" rel="noopener ugc nofollow" target="_blank"> YOLOv3:增量改进</a>。[博客链接]</li><li id="00ca" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1805.09300v3.pdf" rel="noopener ugc nofollow" target="_blank">狙击手:高效多尺度训练</a>。[博客链接]</li><li id="3a5d" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1904.04514.pdf" rel="noopener ugc nofollow" target="_blank">标注像素和区域的高分辨率表示。</a>【博客链接】</li><li id="2d1a" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1904.01355v5.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS:全卷积一级目标检测</a>。[博客链接]</li><li id="8074" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1904.07850.pdf" rel="noopener ugc nofollow" target="_blank">以物为点</a>。[博客链接]</li><li id="d39a" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">CornerNet-Lite:高效的基于关键点的对象检测。【博客链接】</li><li id="b226" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1904.08189v3.pdf" rel="noopener ugc nofollow" target="_blank"> CenterNet:用于对象检测的关键点三元组</a>。[博客链接]</li><li id="d84e" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1909.00700.pdf" rel="noopener ugc nofollow" target="_blank">用于实时对象检测的训练时间友好网络。</a>【博客链接】</li><li id="e790" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1909.03625v1.pdf" rel="noopener ugc nofollow" target="_blank"> CBNet:一种用于目标检测的新型复合主干网络架构。</a>【博客链接】</li><li id="4263" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1911.09070v2.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet:可扩展且高效的对象检测</a>。[博客链接]</li></ol></div></div>    
</body>
</html>