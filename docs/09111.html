<html>
<head>
<title>How to choose your loss when designing a Siamese Neural Network ? Contrastive, Triplet or Quadruplet ?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">设计连体神经网络时如何选择自己的损失？对比，三胞胎还是四胞胎？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-choose-your-loss-when-designing-a-siamese-neural-net-contrastive-triplet-or-quadruplet-ecba11944ec?source=collection_archive---------8-----------------------#2020-06-30">https://towardsdatascience.com/how-to-choose-your-loss-when-designing-a-siamese-neural-net-contrastive-triplet-or-quadruplet-ecba11944ec?source=collection_archive---------8-----------------------#2020-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="3287" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">彻底的比较</h2><div class=""/><div class=""><h2 id="682a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用于在<a class="ae kr" href="https://www.kaggle.com/c/quora-question-pairs" rel="noopener ugc nofollow" target="_blank"> Quora 数据集</a> [1]上训练相似性学习算法的三种流行技术(对比、三重和四重损失)的性能比较</h2></div><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ks"><img src="../Images/c11b4edeb74a39df35b8cac14028851a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZLCmA_ii9D9dAgt1YffUvA.jpeg"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">由<a class="ae kr" href="https://unsplash.com/@axelbonniot?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">阿克塞尔·邦尼奥特</a>在<a class="ae kr" href="https://medium.com/t/wallpapers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener"> Unsplash </a>上拍摄的照片</p></figure><p id="3195" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">通过这篇文章，我将评估和比较深度相似性学习任务的三种不同损失。如果您仍然不能完全理解这个主题，我已经写了一篇文章，介绍了主要的概念和代码示例，以及一个完整的 GitHub 库，您可以查看:</em></p><div class="mf mg gp gr mh mi"><a rel="noopener follow" target="_blank" href="/introduction-to-deep-similarity-learning-for-sequences-89d9c26f8392"><div class="mj ab fo"><div class="mk ab ml cl cj mm"><h2 class="bd jd gy z fp mn fr fs mo fu fw jc bi translated">序列深度相似性学习简介</h2><div class="mp l"><h3 class="bd b gy z fp mn fr fs mo fu fw dk translated">对用于相似性分类任务的深度学习技术的一系列深入评论的第一部分。</h3></div><div class="mq l"><p class="bd b dl z fp mn fr fs mo fu fw dk translated">towardsdatascience.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw lc mi"/></div></div></a></div><h1 id="8d39" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">目录</h1><p id="30ed" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated"><strong class="lk jd">一、任务概述</strong></p><p id="928a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">二。暹罗循环网络:序列的相似性学习</strong></p><p id="731c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">三。深度相似性学习的损失</strong></p><p id="b211" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">四。</strong> <strong class="lk jd">具体应用:问题对检测</strong></p><h1 id="1dfb" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">一.任务概述</h1><p id="80bc" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">我在这个任务中使用了著名的 Quora 问题对数据集，其主要目标是预测两个问题对是否有相同的意图。例如:</p><ul class=""><li id="514b" class="nu nv it lk b ll lm lo lp lr nw lv nx lz ny md nz oa ob oc bi translated"><em class="me">什么能让物理变得简单易学？/怎样才能让物理变得简单易学？</em>有<strong class="lk jd">相似的意图</strong></li><li id="90b2" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated">网上赚钱最好的方法是什么？/网上要钱最好的方式是什么？有<strong class="lk jd">不同的意图</strong></li></ul><p id="8aab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于这项任务，可以使用不同的解决方案，但我们今天将看到的是:<strong class="lk jd">单词嵌入+暹罗循环网络</strong>。单词嵌入算法不是这里的重点(将使用 Word2Vec)，但我们将专注于训练暹罗递归网络。因此，在谈论培训之前，我们将快速概述一下什么是暹罗循环网络(更多细节可以在我上面的另一篇文章中找到……)。</p><h1 id="6d37" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">二。暹罗循环网络:序列的相似性学习</h1><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi oi"><img src="../Images/3976efaf0b872d026bdd7fb888d34fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKCLCg7mKEdMlA4oF3NJjg.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">暹罗猫的形象</p></figure><p id="8a15" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如上所述，连体递归神经网络是这样一种神经网络，其将两个数据序列和<strong class="lk jd">作为<strong class="lk jd">输入</strong>、<strong class="lk jd">并将它们分类为<strong class="lk jd">相似</strong>或<strong class="lk jd">不相似</strong>。</strong></strong></p><h1 id="4d68" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">编码器</h1><p id="ca10" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">为此，它使用一个<strong class="lk jd">编码器</strong>，其工作是<strong class="lk jd">将输入数据<strong class="lk jd">转换为特征</strong> s 的<strong class="lk jd">向量</strong>，然后为每个输入创建一个向量，并将其传递给<strong class="lk jd">分类器。</strong>处理图像时，这个<em class="me">编码器</em>往往会是一堆<em class="me">卷积层</em>，而处理<em class="me">序列</em>时，往往会是一堆<em class="me">rnn</em>。在我们的例子中，我们使用了 3 个双向 LSTMs 的堆栈。</strong></p><h1 id="9a64" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">分类器</h1><p id="0aac" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated"><strong class="lk jd">分类器</strong>然后<strong class="lk jd">根据这两个输入计算</strong>、距离<strong class="lk jd">值</strong>(距离函数可以是任何距离:L1、L2……)。这个<strong class="lk jd">距离</strong>然后被<strong class="lk jd">分类为</strong>相似或<strong class="lk jd">不相似</strong>数据实例的距离:这个过程就类似于<strong class="lk jd">找到正确的距离值阈值</strong>，超过该阈值两个数据对象被认为是不相似的。</p><h1 id="d138" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">训练一个连体神经网络</h1><p id="4df8" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">给定编码器和分类器的定义，人们可以认识到使用暹罗神经网络的所有困难在于特征向量的创建过程。事实上，这个向量需要以下特性:</p><ul class=""><li id="ed6c" class="nu nv it lk b ll lm lo lp lr nw lv nx lz ny md nz oa ob oc bi translated"><strong class="lk jd">足够恰当地描述</strong>，使得两个<strong class="lk jd">相似的</strong>数据(具有可变性)将<strong class="lk jd">具有相似的向量</strong>(因此，距离小)</li><li id="7315" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated"><strong class="lk jd">具有足够的辨别力</strong>，使得两个<strong class="lk jd">不相似的</strong>条<strong class="lk jd">数据</strong>将<strong class="lk jd">具有不相似的矢量</strong></li></ul><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi oj"><img src="../Images/05bda4a378397a00309d4dc029e6d3f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*eKDuAVqPKldzvzDGaXLepg.gif"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">数据比较过程的动画</p></figure><p id="5152" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，我们看到，<strong class="lk jd">训练</strong>这个<strong class="lk jd">网络</strong>就是训练它，一方面，<strong class="lk jd">识别相似的事物</strong>，<strong class="lk jd"> </strong>，另一方面，<strong class="lk jd">识别不相似的事物</strong>:两者都有<strong class="lk jd">良好的信心</strong>。仅仅是<strong class="lk jd">向<strong class="lk jd">教授</strong>一个模型什么是<strong class="lk jd">两个相似的数据</strong>是不够的，它会过度适应训练数据，并倾向于发现所有的数据都是相似的(<em class="me">高召回率但低精度</em>):这也是关于<strong class="lk jd">训练</strong>它去<strong class="lk jd">识别不相似的数据</strong> ( <em class="me">因此，平衡它的召回率和精度</em>)以及最终是什么产生了两个数据</strong></p><p id="ab42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了训练一个连体神经网络，最常用的损失函数是<strong class="lk jd">对比损失</strong>【2】<strong class="lk jd"/>(在我之前的文章中有更详细的介绍，你可以在上面找到)。然而，它并不是唯一存在的。我将通过详细描述这些损失背后的主要思想以及它们的 PyTorch 实现，将其与另外两个损失进行比较。</p><h1 id="8bc6" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">三。深度相似性学习的损失</h1><h1 id="16c6" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">对比损失</h1><p id="666e" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">当训练具有对比损失[2]的连体网络时，在每个时间步将使用<strong class="lk jd">两个输入数据</strong>来比较<strong class="lk jd">。这两个输入数据可能相似，也可能不相似。这由二进制类变量 Y 建模，其值为:</strong></p><ul class=""><li id="39bb" class="nu nv it lk b ll lm lo lp lr nw lv nx lz ny md nz oa ob oc bi translated">如果不同，则为 0；</li><li id="55b6" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated">1 如果相似。</li></ul><p id="1b42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些类别显然可以改变，以适应损失函数的条件。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/c53ae4d2f8255678f2ffc720fe44b4f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*g2I-W-iIQuCNsczuGPN0ZQ.png"/></div><p class="le lf gj gh gi lg lh bd b be z dk translated">对比损失详情说明</p></figure><p id="46b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以在下面找到对比损失的 PyTorch 代码:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="ol om l"/></div></figure><h1 id="e42c" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">三重损失</h1><p id="478e" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">当训练具有三重损失[3]的连体网络时，在每个时间步需要<strong class="lk jd">三个输入数据</strong>来比较<strong class="lk jd">。与对比损失相反，<strong class="lk jd">输入</strong>被<strong class="lk jd">有意采样</strong>关于它们的<strong class="lk jd">类</strong>:</strong></p><ul class=""><li id="c5f0" class="nu nv it lk b ll lm lo lp lr nw lv nx lz ny md nz oa ob oc bi translated">我们对一个<strong class="lk jd">锚对象</strong>进行采样，用作其他两个数据对象的比较点；</li><li id="62da" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated">我们采样一个<strong class="lk jd">阳性物体</strong>，已知是<strong class="lk jd">类似于<strong class="lk jd">锚</strong>物体的</strong>；</li><li id="ce80" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated">然后，我们对一个<strong class="lk jd">负对象</strong>进行采样，已知它与<strong class="lk jd">锚</strong>对象<strong class="lk jd">不同</strong>。</li></ul><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a15d0b10833fabc724273715740244ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*lLLhrGXxYtRhIeooMzvMbw.png"/></div><p class="le lf gj gh gi lg lh bd b be z dk translated">三重损失细节图</p></figure><p id="c913" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以在下面找到三联体丢失的 PyTorch 代码:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="ol om l"/></div></figure><h1 id="c21a" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">四联缺失</h1><p id="71c4" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">当训练一个有四个损失的连体网络[3]时，将需要<strong class="lk jd">个输入数据</strong>在每个时间步比较<strong class="lk jd">。就像三重丢失一样，<strong class="lk jd">输入</strong>再次被<strong class="lk jd">有意采样</strong>关于它们的<strong class="lk jd">类</strong>:</strong></p><ul class=""><li id="af2e" class="nu nv it lk b ll lm lo lp lr nw lv nx lz ny md nz oa ob oc bi translated">我们对一个<strong class="lk jd">锚对象</strong>进行采样，用作其他两个数据对象的比较点；</li><li id="7b8e" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated">我们对一个<strong class="lk jd">阳性对象</strong>进行采样，已知其<strong class="lk jd">与<strong class="lk jd">锚</strong>对象</strong>相似；</li><li id="9469" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated">我们采样一个<strong class="lk jd">否定对象</strong>，已知是<strong class="lk jd">与<strong class="lk jd">锚</strong>对象不相似的</strong>；</li><li id="2ba2" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated">然后，我们对另一个否定对象进行采样<strong class="lk jd">，已知它是 3 个<strong class="lk jd">数据对象</strong>的<strong class="lk jd">相异</strong>到<strong class="lk jd">。</strong></strong></li></ul><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/9d4104db8fced92747a9a8da00476d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*8UEcgRfWYCos_jidT3TkzQ.png"/></div><p class="le lf gj gh gi lg lh bd b be z dk translated">四联丢失详细信息的图示</p></figure><p id="66e9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以在下面找到四胞胎丢失的代码:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="ol om l"/></div></figure><h1 id="83b7" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">直观比较损失及其对网络的影响</h1><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi op"><img src="../Images/6f6ea03d79866dbad41c93f2edba1377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GjKENc84w-7uL-1bPNEBJQ.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">三种损失的比较及其对暹罗网络架构的影响</p></figure><p id="c87b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在此图中，<strong class="lk jd"> 3 损失</strong>与<strong class="lk jd">并排比较。我们可以很容易地看到<strong class="lk jd">的差异</strong>在<strong class="lk jd">的输入数</strong>中，取决于所使用的损耗。</strong></p><p id="8778" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在每个<em class="me">编码器</em>的右侧是计算损耗的<strong class="lk jd">图形表示。它可以提供关于如何使用编码器的输出来训练网络的更多见解，还可以显示损失之间的不同复杂程度。每个损失的输出是紫色的计算节点。</strong></p><h1 id="cd9b" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">四。具体应用</h1><h1 id="37a5" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">架构和损失定义(PyTorch)</h1><p id="e105" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">我训练了三个不同的模型，每个模型对应一次损失。他们都使用相同的编码器来处理输入，他们之间的唯一区别是输入的数量:</p><ul class=""><li id="7395" class="nu nv it lk b ll lm lo lp lr nw lv nx lz ny md nz oa ob oc bi translated">对比损失模型的 2 个输入；</li><li id="64a3" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated">三重态损耗模型的 3 个输入；</li><li id="39c8" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated">四联丢失模型的 4 个输入。</li></ul><p id="4e9c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该编码器具有以下架构(内置于 PyTorch 中):</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="3ec3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，每个模型将有一个单一版本的编码器，他们将使用它来为他们的输入生成特征向量。例如，对于四联丢失模型，我们有:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="ol om l"/></div></figure><h1 id="ce0b" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">培训详情和结果</h1><p id="99a7" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">我使用以下超参数并行训练我的网络(使用相同的 for-loop ):</p><ul class=""><li id="8195" class="nu nv it lk b ll lm lo lp lr nw lv nx lz ny md nz oa ob oc bi translated">25 个时代</li><li id="51a5" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated"><em class="me"> 1e-3 </em>的学习率</li><li id="8c2c" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated">批量为 64 件</li><li id="b70b" class="nu nv it lk b ll od lo oe lr of lv og lz oh md nz oa ob oc bi translated">嵌入大小(Word2Vec 建模)为 40</li></ul><p id="1b13" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我的三个算法的性能是在每个时期结束时使用 AUC 分数来测量的，在验证阶段之后，使用训练、验证和测试集来计算它们的相互 AUC 分数。总体而言，它们都遵循相同的进度，因此我将只显示测试结果:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi oq"><img src="../Images/851d4501dec04d738381d879bb07623b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K39GW74_8ASagMglM8059A.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">每个模型的测试集上的 AUC 分数作为纪元编号的函数的图</p></figure><p id="fa34" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们在这里看到，在整个训练中，用四重损失训练的模型明显优于对比损失模型。虽然它们最终似乎都趋于一致，但这两种模型之间仍有 0.01 的 AUC 差异。这证明了四联缺失训练模型将数据转换为特征密集向量是多么有效。</p><p id="79f3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我的 github 仓库里有这个项目的全部代码，在这里:</p><div class="mf mg gp gr mh mi"><a href="https://github.com/dimartinot/Text-Semantic-Similarity" rel="noopener  ugc nofollow" target="_blank"><div class="mj ab fo"><div class="mk ab ml cl cj mm"><h2 class="bd jd gy z fp mn fr fs mo fu fw jc bi translated">文本语义相似度</h2><div class="mp l"><h3 class="bd b gy z fp mn fr fs mo fu fw dk translated">这是运行以下文章中介绍的实验的代码的存储库</h3></div><div class="mq l"><p class="bd b dl z fp mn fr fs mo fu fw dk translated">github.com</p></div></div><div class="mr l"><div class="or l mt mu mv mr mw lc mi"/></div></div></a></div><blockquote class="os ot ou"><p id="ede7" class="li lj me lk b ll lm kd ln lo lp kg lq ov ls lt lu ow lw lx ly ox ma mb mc md im bi translated">d<strong class="lk jd">is claimer</strong>:Quora 数据集为了这个实验的目的被稍微修改了一下。虽然它最初包含相似和不相似的问题示例，但只有相似的问题被保留用于我的计算。然后，我会从数据集中随机抽取任何其他问题来创建不同的示例。原始数据集的全部困难在于，一些问题在意义上非常接近，但实际上是不同的。为这一挑战而构建的解决方案的关键不仅在于构建深度相似性网络，还在于手工创建将被使用的<em class="it">神奇特征</em>，合并到向量中，用于分类。</p></blockquote><h1 id="72ca" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">额外资源</h1><p id="b82f" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">关于暹罗神经网络的另一个具体应用，我向你推荐这篇由<a class="ae kr" href="https://www.linkedin.com/in/raulgomezbruballa/" rel="noopener ugc nofollow" target="_blank">劳尔·戈麦斯·布鲁巴拉</a>发表在 Neptune.ai 博客部分的关于这个主题的广泛文章。它提出了通过相似嵌入内容的匹配进行图像检索的概念。</p><div class="mf mg gp gr mh mi"><a href="https://neptune.ai/blog/content-based-image-retrieval-with-siamese-networks" rel="noopener  ugc nofollow" target="_blank"><div class="mj ab fo"><div class="mk ab ml cl cj mm"><h2 class="bd jd gy z fp mn fr fs mo fu fw jc bi translated">用 PyTorch - neptune.ai 中的连体网络实现基于内容的图像检索</h2><div class="mp l"><h3 class="bd b gy z fp mn fr fs mo fu fw dk translated">图像检索是寻找与给定查询相关的图像的任务。对于基于内容的图像检索，我们指的是…</h3></div><div class="mq l"><p class="bd b dl z fp mn fr fs mo fu fw dk translated">海王星. ai</p></div></div><div class="mr l"><div class="oy l mt mu mv mr mw lc mi"/></div></div></a></div><h1 id="fc19" class="mx my it bd mz na nb nc nd ne nf ng nh ki ni kj nj kl nk km nl ko nm kp nn no bi translated">参考</h1><p id="b308" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">[1] Quora。2017.<em class="me"> Quora 问题对</em>，<a class="ae kr" href="https://www.kaggle.com/c/quora-question-pairs/overview" rel="noopener ugc nofollow" target="_blank"> Kaggle </a></p><p id="36e4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2] R .哈德塞尔，s .乔普拉，y .勒昆。通过学习不变映射进行降维。2006.</p><p id="99d6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3] F .施罗夫、d .卡列尼琴科、j .菲尔宾。FaceNet:人脸识别和聚类的统一嵌入。2015.</p><p id="9f2b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[4]陈文伟，陈晓霞，张军，黄国光.超越三重缺失:用于个人再识别的深层四重网络。2017.</p></div></div>    
</body>
</html>