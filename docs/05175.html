<html>
<head>
<title>Neural Networks for Absolute Beginners with Numpy from scratch — Part 3: Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">完全初学者用的神经网络从零开始第 3 部分:逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-part-3-logistic-regression-18b474096a4e?source=collection_archive---------32-----------------------#2020-05-03">https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-part-3-logistic-regression-18b474096a4e?source=collection_archive---------32-----------------------#2020-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b810" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">sigmoid 激活函数是神经网络中最基本的概念。在本教程中，您将学习实现使用 sigmoid 激活函数和 Numpy 进行分类的逻辑回归。</h2></div><p id="ca48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">无论是机器学习还是深度学习，都会遇到两种问题，分别是<strong class="kk iu"><em class="le"/></strong>和<strong class="kk iu"> <em class="le">分类</em> </strong>。</p><p id="f10a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<strong class="kk iu"> <em class="le">回归问题</em> </strong>中，你预测一个连续的实数值，而在<strong class="kk iu"> <em class="le">分类问题</em> </strong>中，你预测不同类别的对象。</p><p id="eae8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在前面的教程中，我们深入探讨了什么是感知器，后来又学习了它如何使用线性回归来学习预测。如果你是这个领域的绝对初学者，我推荐你去看看我以前的简短教程。</p><div class="lf lg gp gr lh li"><a rel="noopener follow" target="_blank" href="/neural-networks-with-numpy-for-absolute-beginners-introduction-c1394639edb2"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">神经网络与 Numpy 的绝对初学者:简介</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">在本教程中，您将简要了解什么是神经网络以及它们是如何发展起来的。在…</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">towardsdatascience.com</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw lx li"/></div></div></a></div><div class="lf lg gp gr lh li"><a rel="noopener follow" target="_blank" href="/neural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">绝对初学者用的带 Numpy 的神经网络第 2 部分:线性回归</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">在本教程中，您将详细学习使用 Numpy 实现预测的线性回归，并可视化…</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">towardsdatascience.com</p></div></div><div class="lr l"><div class="ly l lt lu lv lr lw lx li"/></div></div></a></div><p id="5c90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本教程中，您将了解用于分类问题的逻辑回归。</p><p id="3d1c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">像以前一样，会涉及到一些数学知识，但我会确保从基础开始，这样你就可以很容易地理解。</p><p id="9207" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你不确定使用哪种环境来实现本教程中的代码，我推荐<a class="ae lz" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>。该环境已经安装了许多重要的软件包。安装新的包以及导入和导出数据非常简单。最重要的是，它还带有 GPU 支持。所以，开始编码吧！</p><p id="9f76" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我建议您在开始之前先浏览前两个教程。然而，如果你已经对线性回归和神经网络有所了解，或者只是想学习逻辑回归，你可以马上开始！</p><h1 id="35a9" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">Sigmoid 激活函数</h1><p id="5cf5" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">使用最早和最普遍的激活功能之一是<em class="le">s 形函数</em>。</p><p id="8860" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">sigmoid 函数的等式如下:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/83d55d84f1d106c30735a10675237b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/0*n9Z0tMemP4DQhiLg"/></div></figure><p id="b4f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该函数的图形如下:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/347a38503b166868218200f1a80e3377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*We0RrcxFuNNlDZ0c.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">Sigmoid 函数</p></figure><p id="a158" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，具有 sigmoid 激活函数的感知器对给定的数据集进行二元分类。这个二元分类的过程俗称<strong class="kk iu"> <em class="le">逻辑回归</em> </strong>。在下一节中，我们将深入研究<em class="le">逻辑回归</em>，并了解模型是如何被训练的。</p><h1 id="5028" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">逻辑回归</h1><blockquote class="nn"><p id="b833" class="no np it bd nq nr ns nt nu nv nw ld dk translated">那么，什么是逻辑回归呢？</p></blockquote><p id="eeb4" class="pw-post-body-paragraph ki kj it kk b kl nx ju kn ko ny jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">逻辑回归是一种用于二元分类的技术。它在数据点之间创建一个决策边界，以便将它们归类到两个类别中的任何一个。下图就是这样一个例子。我们将通过实际操作来深入理解和训练一个<em class="le">逻辑回归</em>模型。理解逻辑回归将为理解神经网络模型提供基础。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/62def8027e509efa2593fbe15a4cc425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/0*YicluSk3M7E6cmjj.gif"/></div></figure><p id="0199" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要实施的逻辑回归的计算图如下图所示。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/ae0ada70d58340b231fcd039c1a98206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HitsR-9jpyrqGy6u.png"/></div></div></figure><p id="08ca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们有两个输入<strong class="kk iu"> <em class="le"> x₁ </em> </strong>和<strong class="kk iu"> <em class="le"> x₂ </em> </strong>，它们分别乘以权重<strong class="kk iu"> <em class="le"> w₁ </em> </strong>和<strong class="kk iu"> <em class="le"> w₂ </em> </strong>。将附加偏置<strong class="kk iu"> <em class="le"> b </em> </strong>加到它们的和上，得到<strong class="kk iu"> <em class="le"> z </em> </strong>。这些参数(<strong class="kk iu"> <em class="le"> w₁ </em> </strong>、<strong class="kk iu"> <em class="le"> w₂ </em> </strong>、<strong class="kk iu"> <em class="le"> b </em> </strong>)是在梯度下降过程中学习到的。</p><blockquote class="nn"><p id="8493" class="no np it bd nq nr ns nt nu nv nw ld dk translated">现在让我们开始编码吧！</p></blockquote><p id="2781" class="pw-post-body-paragraph ki kj it kk b kl nx ju kn ko ny jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">第一步是导入所需的包。</p><p id="c7e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您将使用 sklearn 包来执行两项任务:</p><ol class=""><li id="34b2" class="od oe it kk b kl km ko kp kr of kv og kz oh ld oi oj ok ol bi translated">生成 blobs 数据集</li><li id="c92c" class="od oe it kk b kl om ko on kr oo kv op kz oq ld oi oj ok ol bi translated">将数据分成训练集和测试集。</li></ol><p id="535c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您将使用<strong class="kk iu"> <em class="le"> matplotlib </em> </strong>来可视化结果。</p><p id="97c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在你开始定义代码之前，在任何机器学习项目中，你的第一个任务将是定义超参数。超参数可以是数据集大小、学习率、时期数等。你一定想知道为什么这些变量被称为超参数！您稍后会发现，参数是在模型中学习到的，我们使用 hyperparameter 来微调模型，以实现更高的准确性。</p><p id="7635" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们还定义了创建数据集所需的输入要素数和聚类数。</p><p id="2166" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">继续定义它们吧！</p><p id="8ec9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们用 make_blobs 函数从<strong class="kk iu"> <em class="le"> scikit-learn </em> </strong>导入数据集，该函数创建类的 blob。我们也将同样想象。</p><p id="2de5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们绘制一个图形来可视化生成的数据。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi or"><img src="../Images/31e0823cf807cd931a18a2c816b2392d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*Dg0Ba7dMakE8eoYi.png"/></div></figure><p id="6071" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这两组数据点属于这两类。我们的目标是找到一个区分这两类的最优决策边界。</p><p id="6a61" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下一步将把数据分成训练集和测试集。我们这样做是为了验证所学算法的准确性。</p><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="303a" class="ox mb it ot b gy oy oz l pa pb">Shape of X_train (400, 2) <br/>Shape of y_train (400, 1) <br/>Shape of X_test (100, 2) <br/>Shape of y_test (100, 1)</span></pre><p id="989f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，您将随机初始化参数 W 和 b，它们是在训练过程中学习到的。</p><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="a8f3" class="ox mb it ot b gy oy oz l pa pb">Initializing weights... <br/>W: [[-0.59134456] [-0.31427067]] <br/>b: -1.6078558751252707</span></pre><p id="7551" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们随机初始化参数后，下一步是执行正向传播，并查看网络如何预测。</p><p id="a568" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，您将利用<code class="fe pc pd pe ot b">generate_mesh_grid</code>函数绘制数据集和等高线。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/d749508b2d52d62ae245a940afe720a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/0*OpR2S1Q860DLAWTr.png"/></div></figure><p id="f0a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在是时候定义损失函数了！！</p><h1 id="f481" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">成本/损失函数</h1><p id="105a" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">与线性回归的 MSE 不同，这里我们使用逻辑损失函数。这是因为当我们通过 sigmoid 激活函数传递总和<strong class="kk iu"> <em class="le"> z </em> </strong>时，输出是非线性的(因为 sigmoid 函数是非线性的)。这会导致非凸误差。</p><blockquote class="nn"><p id="d0ca" class="no np it bd nq nr ns nt nu nv nw ld dk translated">但是你说的非凸是什么意思？</p></blockquote><p id="d952" class="pw-post-body-paragraph ki kj it kk b kl nx ju kn ko ny jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">简单来说，非凸函数看起来就像右图。</p><p id="bc44" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，虽然我们的目标是找到最小点，但由于波谷和波峰，找到一个点变得极其困难。相反，如果我们有一个简单的函数(就像左边的那个)，我们可以很容易地找到最小点，我们就可以使我们的问题变得简单。这是左边的凸函数。</p><p id="c814" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们使用一个对数函数，它实际上将非线性函数转换回线性函数，从而产生一个凸函数！</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi pg"><img src="../Images/1c0f5759ce08f70fbdcb25ccb5a1e523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4AUrjLjQjCiUq88h.jpg"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">凸优化和非凸优化。来源:<a class="ae lz" href="https://bit.ly/3d9ESRL" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3d9ESRL</a></p></figure><p id="5616" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">损失可以定义为:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/e28d4a3926518b62f05b14440ed9f57a.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/0*UveoqxomOX8Pqv8O"/></div></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/8f2132dab4c5bb11b72240df5eea1fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CC2V6CIYBSdCVq94.png"/></div></div></figure><p id="fdc0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ￚ <strong class="kk iu">测井曲线(<em class="le"> x </em> ) </strong>如上图所示。可以推断给定<strong class="kk iu"> <em class="le"> y = 1 </em> </strong>如果<strong class="kk iu"><em class="le">y→</em><em class="le">0</em></strong>，那么<strong class="kk iu"> <em class="le"> loss → 0 </em> </strong>而当<strong class="kk iu"> <em class="le"> y' → 0 </em> </strong>时，那么<strong class="kk iu"> <em class="le"> loss → </em> </strong> ∞。同样，如果<strong class="kk iu"><em class="le">y’→0</em></strong>，那么<strong class="kk iu"> <em class="le"> loss → 0 </em> </strong>而当<strong class="kk iu"><em class="le">y’→1</em></strong>，那么<strong class="kk iu"> <em class="le"> loss → </em> </strong> ∞给定<strong class="kk iu"> <em class="le"> y = 0 </em> </strong>。这意味着当预测错误时，参数会受到严重惩罚，而当预测正确时，参数不会受到惩罚。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/221f593577ca4b14c064605be3fa1958.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/0*dKdxBH_rTCBGnadM"/></div></figure><p id="3b10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，函数简化为ￚ <strong class="kk iu"> log( <em class="le"> y' </em> ) </strong>当<strong class="kk iu"> <em class="le"> y = 1 </em> </strong>和<strong class="kk iu">log(<em class="le">1</em></strong><em class="le">ￚ</em><strong class="kk iu"><em class="le">y '</em>)</strong>当<strong class="kk iu"> <em class="le"> y = 0 </em> </strong>正如我们</p><p id="9af2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然以上只是一个例子(数据点)的错误，我们需要考虑所有的例子。因此，我们将所有误差相加，然后除以示例数量，得出误差的平均值，如下所示。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/ea2b107a9bed3ffe270168db5f384402.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/0*o54gmXByH4f3B84Q"/></div></figure><p id="5313" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这方面的代码如下:</p><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="c423" class="ox mb it ot b gy oy oz l pa pb">1.0579769986979133</span></pre><p id="c5ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们画出损失图。您将跟踪所有迭代(时期)的损失，之后您将能够可视化误差的减少。</p><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="2531" class="ox mb it ot b gy oy oz l pa pb">[1.0579769986979133]</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/48b2ce8f22c26c571958c90a45ffd188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/0*iXeoEgPH4-ZeIy7Z.png"/></div></figure><h1 id="4fdf" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">梯度下降</h1><p id="98b1" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">这通常是最难理解的部分，但是我用了非常简单的术语。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/f0b9895e420379cf0c1b3618e118ad0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*4LjuBu6QFAb0uo_N.gif"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">来源:<a class="ae lz" href="https://gph.is/1SuKcy4" rel="noopener ugc nofollow" target="_blank">吉菲</a></p></figure><p id="b7e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用外行人的话说梯度下降就像走下一座小山。这座山指的是我们的错误。误差越大，山越高！！所以当你滚下山的时候，误差(<strong class="kk iu"> <em class="le">损失</em> </strong>)就下降了。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/ea90147cf9ef1f3beb550a78c4ceecc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*OFWOHnIMP3PqlGUl.gif"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">来源:<a class="ae lz" href="https://gph.is/1NiQzpi" rel="noopener ugc nofollow" target="_blank">吉菲</a></p></figure><p id="b4b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们的目标是滚到误差最小的点。</p><blockquote class="nn"><p id="b334" class="no np it bd nq nr ns nt nu nv nw ld dk translated">嘿，但是等一下！我该如何改变损失？🤔</p></blockquote><p id="fcd5" class="pw-post-body-paragraph ki kj it kk b kl nx ju kn ko ny jx kq kr nz kt ku kv oa kx ky kz ob lb lc ld im bi translated">现在你必须观察到，如果你调整参数<strong class="kk iu"> <em class="le"> W </em> </strong>和<strong class="kk iu"><em class="le">b</em></strong><strong class="kk iu"><em class="le">损耗</em> </strong>也会改变！！</p><p id="d1ca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们找到关于参数<strong class="kk iu"><em class="le">W</em></strong>&amp;<strong class="kk iu"><em class="le">b</em></strong>的导数(即变化率),并相应地更新它们。</p><p id="34ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯，那就简单了！只要找到它的导数或者斜率。数学上，斜率在极小点(minima)处为<strong class="kk iu"> <em class="le">零</em> </strong>【如果你不了解导数和斜率，可以参考这个精彩的可汗学院视频。]而如果你观察敏锐的话，斜率在极小值左边是负的，在极小值右边是正的。当<strong class="kk iu"> <em class="le"> p </em> </strong>在最小值的左边时，我们需要增加一些值，以便它向最优值<strong class="kk iu"> <em class="le"> p </em> </strong>移动，当<strong class="kk iu"> <em class="le"> p </em> </strong>在右边时，减去一些值。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi pn"><img src="../Images/c5cb628af2a6f411834b449dd0ab01f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sFpYRyBdlid6U0ow.gif"/></div></div></figure><p id="52db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的例子中，我们找到了<strong class="kk iu"> <em class="le">损失</em> </strong>相对于<strong class="kk iu"> <em class="le"> W </em> </strong>和<strong class="kk iu"> <em class="le"> b </em> </strong>的导数。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi po"><img src="../Images/0e4e10f706cb55d81474687984898d8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:186/0*mWxPsNcoJSOGvqQV"/></div></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/e86347f12808088724d6c342717cd45f.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/0*lOk60HvaQCgJEBuR"/></div></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/f8fe0a05898af871f652fe4f9fc93170.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/0*liinFUC59c4rGXba"/></div></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/adf3c95c9f33df01b877fa4af4841e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/0*CvWgVEDsBg5Wqkaq"/></div></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/db3e6330f4b9c1e4f49b93ff3ea11d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/0*Kazq9oud5V_WTqcF"/></div></figure><p id="d028" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我跳过了几个不必要的步骤，但是如果感兴趣，你可以参考下面的视频。</p><h2 id="6f0e" class="ox mb it bd mc pt pu dn mg pv pw dp mk kr px py mm kv pz qa mo kz qb qc mq qd bi translated">更新参数</h2><p id="d4d3" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">至于更新参数的最后部分，利用斜率，我们将斜率乘以阻尼因子<em class="le"> α </em>，使得参数在具有非常高的误差时不会过冲。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/746144f86529c60ab90a1d5766b145e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/0*kL64KFe9ts2WwayO"/></div></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/b8aad387ba57affd4aff8047919b67ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:194/0*LQWs0A2fIa-Y4bDt"/></div></figure><p id="cda9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您现在将实现与下面相同的<code class="fe pc pd pe ot b">gradient_descent</code>算法。</p><p id="941d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在还有最后一件事要做，即将两个绘图功能合并为一个。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi qg"><img src="../Images/664024b763b8c8b5e3fa9d13ddebab74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wqRSSA4Vx6gK87SM.png"/></div></div></figure><p id="13ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们获得了一个美丽的决定边界以及损失的情节。</p><p id="55b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，作为你们的最后一项任务，让我们把你们到目前为止所做的一切汇总起来，自己看看结果。</p><p id="107a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，你首先要定义超参数。您可以摆弄这些值，尤其是学习率<code class="fe pc pd pe ot b">l_r</code>和<code class="fe pc pd pe ot b">epoch</code>，以观察模型如何学习预测。</p><p id="c7e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，您可以一次性编写从创建数据集到训练逻辑回归模型的代码，如下所示。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi or"><img src="../Images/8589dd06ead983f819fe8d9cb1f38c05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*Cep_JYrR__xco5YU.png"/></div></figure><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="09e1" class="ox mb it ot b gy oy oz l pa pb">Shape of X_train (400, 2) <br/>Shape of y_train (400, 1) <br/>Shape of X_test (100, 2) <br/>Shape of y_test (100, 1) <br/>Initializing weights... <br/>W: [[ 0.43753829] [-1.70958537]] <br/>b: -0.8793505127088026<br/>--------------------------------------------------------------------<br/>Iteration: 0 <br/>W = [[ 0.43753829] [-1.70958537]] <br/>b = -0.8793505127088026 <br/>Loss = 0.3991070405361789</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi qh"><img src="../Images/483cf73253a584499818c79d79a573d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YFdNp5fPyI6LhrR0.png"/></div></div></figure><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="38b5" class="ox mb it ot b gy oy oz l pa pb">-------------------------------------------------------------------- Iteration: 5 <br/>W = [[ 0.09634223] [-1.73455902]] <br/>b = [[-0.87943027]] <br/>Loss = 0.10346899489337041</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi qh"><img src="../Images/f782f7444a73a0565cde31c61d79eb92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CA7AvQ5_W8DSCfW_.png"/></div></div></figure><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="5b33" class="ox mb it ot b gy oy oz l pa pb">--------------------------------------------------------------------Iteration: 10 <br/>W = [[-0.02440044] [-1.73555684]] <br/>b = [[-0.87945069]] <br/>Loss = 0.06810528539660123</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi qh"><img src="../Images/ae4a18be4ab07ae479003fe943d7fce8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZeP1_fMF1RmV2v9T.png"/></div></div></figure><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="416a" class="ox mb it ot b gy oy oz l pa pb">--------------------------------------------------------------------Iteration: 15 <br/>W = [[-0.09215724] [-1.73617853]] <br/>b = [[-0.87945614]] <br/>Loss = 0.0569509026646708</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi qh"><img src="../Images/af6f1127bde5de0e990f7c5447d1157b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NLoP7N2ri6c6lZvm.png"/></div></div></figure><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="50f0" class="ox mb it ot b gy oy oz l pa pb">--------------------------------------------------------------------Iteration: 20 <br/>W = [[-0.13607178] [-1.737859 ]] <br/>b = [[-0.87945457]] <br/>Loss = 0.05224655857924122</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi qh"><img src="../Images/e3bd9052470c4f01888130f04543495d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZNXQd8uOutXmgT6z.png"/></div></div></figure><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="c19c" class="ox mb it ot b gy oy oz l pa pb">--------------------------------------------------------------------Iteration: 25 <br/>W = [[-0.16649094] [-1.74055496]] <br/>b = [[-0.87944893]] <br/>Loss = 0.04996979153280091</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi qh"><img src="../Images/43fab7655e38a076f6cff63f7aeee1ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Dw8fthwkXTzlEltH.png"/></div></div></figure><h1 id="ed5d" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">预言；预测；预告</h1><p id="b131" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">我们之前已经创建了一个测试数据集，现在您将根据它测试您的回归模型并确定其准确性。</p><p id="a2f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">精确度的公式为:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/64ef1fced4da7a28bc3ce9d346fcc40b.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/0*9hlqUwtcmHzwRHlQ"/></div></figure><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="f5cb" class="ox mb it ot b gy oy oz l pa pb">Prediction: Loss = 0.2929627097697204 Accuracy = 95.0%</span></pre><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/a21c9bcd50f2bd595d4bcc9d1a7a2a8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*0F1f3zKmUbG9l1Xb.png"/></div></figure><pre class="my mz na nb gt os ot ou ov aw ow bi"><span id="a318" class="ox mb it ot b gy oy oz l pa pb">Hence W = [[ 3.9538485] [-3.9152981]] b = [[-0.68703193]]</span></pre><p id="9ea8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经获得了 95%的准确率，这是相当不错的！</p><h1 id="6f9d" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">结论</h1><p id="4378" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">在本教程中，您学习了</p><ol class=""><li id="f618" class="od oe it kk b kl km ko kp kr of kv og kz oh ld oi oj ok ol bi translated">Sigmoid 激活函数</li><li id="6139" class="od oe it kk b kl om ko on kr oo kv op kz oq ld oi oj ok ol bi translated">创建逻辑回归模型</li><li id="e531" class="od oe it kk b kl om ko on kr oo kv op kz oq ld oi oj ok ol bi translated">训练逻辑回归模型</li></ol><p id="df2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下一篇教程中，您将学习仅使用 Numpy 从头实现神经网络！</p></div></div>    
</body>
</html>