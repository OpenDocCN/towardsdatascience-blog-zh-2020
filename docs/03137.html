<html>
<head>
<title>A Practical Suggestion in Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归中的一个实用建议</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-practical-suggestion-in-linear-regression-cb639fd5ccdb?source=collection_archive---------26-----------------------#2020-03-25">https://towardsdatascience.com/a-practical-suggestion-in-linear-regression-cb639fd5ccdb?source=collection_archive---------26-----------------------#2020-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7864" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="89fb" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从弹性网开始，记得调好定义 l1 范数之比的超参数。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/43ea264f8c1953e7b6cc1bb21c916c9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*i3SobXGYr7ZWm-2G"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">贾斯汀·科布利克在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="6e0d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di">不</span>无论你是一名经验丰富的数据科学家还是机器学习的初学者，线性回归仍然是你需要掌握的最基本的模型之一。</p><p id="f106" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">简单而有用的线性回归长期以来一直受到多个领域研究人员的青睐，比如生物学和金融学。原因是线性回归的优点和它的局限性一样明显。</p><p id="1a6d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，即使它依赖于 Xs 和 Y 之间的线性关系的假设，它仍然比复杂的网络更容易实现，比如深度神经网络。而且模型本身更容易解读。</p><p id="4878" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，在一个真实的项目中，实现线性回归就像输入下面几行代码一样简单吗？</p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="f459" class="mt mu it mp b gy mv mw l mx my"><strong class="mp jd">from</strong> <strong class="mp jd">sklearn.linear_model</strong> <strong class="mp jd">import</strong> LinearRegression<br/>my_model = LinearRegression()<br/>my_model.fit(X,y)</span></pre><p id="fa10" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">不，不是真的。</p><p id="6fee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的代码很容易遇到<strong class="lk jd"> <em class="mn">过拟合问题</em> </strong>。过度拟合意味着与测试数据相比，您的模型在训练数据中表现得更好。</p><p id="2140" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">导致过度拟合问题的原因通常是模型的高度复杂性。在上面的代码中，将 X 中的所有特征汇集到线性回归模型中，将很难对未来的数据点做出预测。</p><p id="fdb9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通常可以通过两种方式降低线性模型的复杂度，<strong class="lk jd"> <em class="mn">特征工程、</em> </strong>和<strong class="lk jd"> <em class="mn">正则化</em> </strong>。</p><p id="4b52" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">特征工程意味着从原始 X 表中手动选择一个特征子集，然而，该过程还需要数据集的先验知识来指导特征过滤。我们不会谈论太多。</p><p id="9195" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如我所说，降低线性模型复杂性的第二种方法是通过正则化。<em class="mn">脊</em>回归、<em class="mn">套索</em>回归，以及<em class="mn">弹性网</em>是三种实现方式。</p><p id="e1de" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我不打算在这里涉及任何数学方程，所以对于那些感兴趣的人，请参考<a class="ae lh" href="https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net" rel="noopener ugc nofollow" target="_blank">这个不错的帖子</a>。</p><h2 id="05ed" class="mt mu it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">里脊回归</h2><p id="194b" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">Ridge 在线性回归的原始损失函数中添加了一个“l2 范数”惩罚项，该惩罚项倾向于将所有变量的系数收缩到某一水平(基于正则化的强度)。代码如下。</p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="ad91" class="mt mu it mp b gy mv mw l mx my"><strong class="mp jd">from</strong> <strong class="mp jd">sklearn.linear_model</strong> <strong class="mp jd">import</strong> Ridge</span><span id="5f26" class="mt mu it mp b gy nu mw l mx my">my_alpha = 0.1<br/>my_model = Ridge(alpha = my_alpha)<br/>my_model.fit(X, y)</span></pre><p id="ea8e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里“my_alpha”是提到的强度。my_alpha 越高，正则化越强，模型的方差越低。</p><h2 id="eec9" class="mt mu it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">套索回归</h2><p id="0631" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">套索回归的思想类似于岭回归的思想，但是在罚函数中用‘L1 范数’代替‘L2 范数’。</p><p id="754d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">套索和脊之间的这种差异会导致套索可以将系数缩小到 0，而脊则不能。这就是为什么人们也说 LASSO 有做特征选择的能力。代码如下。</p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="4aba" class="mt mu it mp b gy mv mw l mx my"><strong class="mp jd">from</strong> <strong class="mp jd">sklearn.linear_model</strong> <strong class="mp jd">import</strong> Lasso</span><span id="6a37" class="mt mu it mp b gy nu mw l mx my">my_alpha = 0.1<br/>my_model = Lasso(alpha = my_alpha)<br/>my_model.fit(X, y)</span></pre><p id="b536" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果 my_alpha 足够大，它会将所有系数收缩为零，这就产生了最有偏差的线性模型。</p><h2 id="d8fb" class="mt mu it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">弹性网</h2><p id="d767" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated">创建弹性网来混合套索和脊，其中惩罚项是 l1 和 l2 范数正则化项的组合。它引入了另一个参数 l1_ratio，以将两个不同的权重分配给 l1 和 l2 范数，其总和为 1。代码如下。</p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="7653" class="mt mu it mp b gy mv mw l mx my"><strong class="mp jd">from</strong> <strong class="mp jd">sklearn.linear_model</strong> <strong class="mp jd">import</strong> ElasticNet</span><span id="d22c" class="mt mu it mp b gy nu mw l mx my">my_alpha = 0.1<br/>my_l1ratio = 0.5<br/>my_model = ElasticNet(alpha = my_alpha, l1_ratio = my_l1ratio)<br/>my_model.fit(X, y)</span></pre><p id="7858" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在实际操作中，最好的 l1_ratio 总是通过交叉验证来调整。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="4890" class="mt mu it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">选哪个？脊，套索，还是弹力网？</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/9121f7acbf0ed471e15e179646ada590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ArhQ5BOxGd8Jd_4N"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">莎伦·麦卡琴在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4cb4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我的实际建议是<strong class="lk jd">总是从弹性网</strong>开始。</p><p id="5dba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我先解释一下<em class="mn">弹力网</em>相对于其他方法的缺点。前面提到过，<em class="mn">弹性网</em>归纳出一个新的参数<strong class="lk jd"> <em class="mn"> l1_ratio </em> </strong>，无法高效手动赋值。</p><p id="849d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">交叉验证是调整最佳<strong class="lk jd"> <em class="mn"> l1_ratio </em> </strong>最常用的方法。因此，与<em class="mn">脊</em>和<em class="mn">套索</em>相比，弹性网的计算成本更高。</p><p id="1f85" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，我想忽略这种计算成本的增加，因为我正在工作的大多数平台都可以轻松处理更复杂的模型，如<em class="mn">随机森林</em>和<em class="mn">深度神经网络</em>。</p><p id="37a5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当我对数据集有一些预先了解时，我只使用<em class="mn">脊</em>或<em class="mn">套索</em>。举个例子，如果我知道 X 中的所有特征对预测 Y 都是有用的，那么 Ridge 是首选，因为我不想丢失任何一个变量。如果我知道只有一小部分特征是有用的，我想把它们选出来，那么 LASSO 当然是更好的选择。</p><p id="a595" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，根据我的经验，在建模步骤之前，我对数据一无所知。我更关心的是选择一个基于错误假设的模型，而不是花更长的时间来调整我的模型。这就是为什么<em class="mn">弹性网</em>总是我在分析中使用的第一个线性模型。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="b587" class="mt mu it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">弹性网实现的真实例子</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/5385284ef24c85542bce664bce5335f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ltB96iU-VLxuLQey"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">奥雷连·罗曼在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="25fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是我的示例代码，用于分析时间序列数据和驱动变化的因素之间的关系。</p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="75fe" class="mt mu it mp b gy mv mw l mx my">from sklearn.impute import SimpleImputer<br/>from sklearn.linear_model import ElasticNet<br/>from sklearn.model_selection import TimeSeriesSplit<br/>from sklearn.pipeline import Pipeline</span><span id="050c" class="mt mu it mp b gy nu mw l mx my">def train_EN(X,y):<br/>    # X are features<br/>    # y is output<br/>    # set cross-validation<br/>    tscv = TimeSeriesSplit(n_splits=10)<br/>    data_split = tscv.split(X)<br/>    <br/>    # build a pipeline of pre-processing and elasticNet regression model<br/>    my_pipe = Pipeline([<br/>        ('imputer', SimpleImputer(strategy="median")), <br/>        ('std_scaler', StandardScaler()),<br/>        ('en_model', ElasticNet(random_state=42))<br/>    ])</span><span id="f66b" class="mt mu it mp b gy nu mw l mx my">    # parameter space<br/>    param_grid_en = {<br/>        'en_model__alpha' : [1e-2,1e-1,1,10,100,1000,10000,],<br/>        'en_model__l1_ratio' : [0,0.25,0.5,0.75,1]<br/>    }</span><span id="45cc" class="mt mu it mp b gy nu mw l mx my">    # gridsearch for best hyper-parameters<br/>    gs_en = GridSearchCV(my_pipe, param_grid=param_grid_en, cv=data_split, scoring='neg_mean_squared_error', n_jobs=-1)<br/>    # fit dataset<br/>    gs_en.fit(X, y)</span></pre><p id="e523" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">预处理(pre_pipeline)分两步完成，缺失值输入和数据规范化。为了超参数的调整(<strong class="lk jd"> <em class="mn"> GridSearchCV </em> </strong>)，整个训练数据集被分成 10 倍(<strong class="lk jd"><em class="mn">TimeSeriesSplit(n _ splits = 10)</em></strong>)。所选择的最佳模型将是来自超参数空间(<strong class="lk jd"><em class="mn">param _ grid _ en</em></strong>)的一个 alpha 和一个 l1_ratio 以及特征系数的组合。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="aea5" class="mt mu it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">外卖</h2><ol class=""><li id="e648" class="oe of it lk b ll np lo nq lr og lv oh lz oi md oj ok ol om bi translated">当你想对数据进行线性回归时，别忘了应用正则化。</li><li id="8178" class="oe of it lk b ll on lo oo lr op lv oq lz or md oj ok ol om bi translated"><strong class="lk jd">始终从弹性网开始。</strong></li><li id="6d21" class="oe of it lk b ll on lo oo lr op lv oq lz or md oj ok ol om bi translated">不要忘记调整你的训练数据集中的超参数。</li></ol><p id="c022" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望这篇文章对你也有帮助。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/39fbcb4e1a98987ebf2b16349135dd8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8pYIswlkOOwz2Dv9"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">格伦·杰克逊在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="8a24" class="mt mu it bd mz na nb dn nc nd ne dp nf lr ng nh ni lv nj nk nl lz nm nn no iz bi translated">参考资料:</h2><p id="fadb" class="pw-post-body-paragraph li lj it lk b ll np kd ln lo nq kg lq lr nr lt lu lv ns lx ly lz nt mb mc md im bi translated"><a class="ae lh" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/classes . html # module-sk learn . linear _ model</a></p></div></div>    
</body>
</html>