<html>
<head>
<title>Getting Started with Text Vectorization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本矢量化入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-text-vectorization-2f2efbec6685?source=collection_archive---------2-----------------------#2020-04-01">https://towardsdatascience.com/getting-started-with-text-vectorization-2f2efbec6685?source=collection_archive---------2-----------------------#2020-04-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2dc4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解自然语言处理(NLP)-Python 中的文本矢量化</h2></div><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="a3e9" class="kr ks it kn b gy kt ku l kv kw">Feel free to follow me on Medium :)</span></pre><p id="536a" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">最近，我在 DataCamp 上完成了“<a class="ae lt" href="https://learn.datacamp.com/projects/607" rel="noopener ugc nofollow" target="_blank">查尔斯·达尔文的书籍推荐</a>”案例研究。在这个项目中，我们将学习如何实现文本预处理和文本矢量化，如何使用自然语言处理(NLP)构建图书推荐系统，以及检测查尔斯·达尔文的书籍彼此之间的相关程度。</p><figure class="ki kj kk kl gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lu"><img src="../Images/bb0405fe2f467fa5a42f5e798496e792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HpKh-MGZIq-GyRAiqEUnSw.png"/></div></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="8b79" class="mj ks it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">文本矢量化</h1><p id="510f" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated"><strong class="kz iu"> <em class="nf">文本矢量化</em> </strong>是将文本转换成数值表示的过程。以下是一些实现文本矢量化的常用方法:</p><ul class=""><li id="0d8b" class="ng nh it kz b la lb ld le lg ni lk nj lo nk ls nl nm nn no bi translated">二进制术语频率</li><li id="d5c3" class="ng nh it kz b la np ld nq lg nr lk ns lo nt ls nl nm nn no bi translated">单词袋(BoW)术语频率</li><li id="79a7" class="ng nh it kz b la np ld nq lg nr lk ns lo nt ls nl nm nn no bi translated">(L1)标准化术语频率</li><li id="784a" class="ng nh it kz b la np ld nq lg nr lk ns lo nt ls nl nm nn no bi translated">(L2)标准化的 TF-IDF</li><li id="96c4" class="ng nh it kz b la np ld nq lg nr lk ns lo nt ls nl nm nn no bi translated">Word2Vec</li></ul><p id="a3ab" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在这一节中，我们将使用下面的语料库来介绍文本矢量化中的 5 种流行方法。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="b2d0" class="kr ks it kn b gy kt ku l kv kw">corpus = ["This is a brown house. This house is big. The street number is 1.",<br/>          "This is a small house. This house has 1 bedroom. The street number is 12.",<br/>          "This dog is brown. This dog likes to play.",<br/>          "The dog is in the bedroom."]</span></pre><h2 id="f395" class="kr ks it bd mk nu nv dn mo nw nx dp ms lg ny nz mu lk oa ob mw lo oc od my oe bi translated">二进制术语频率</h2><p id="e927" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">二进制术语频率捕获文档中术语的存在(1)或不存在(0)。对于这一部分，在 TfidfVectorizer 下，我们将二进制参数设置为 true，以便它可以只显示存在(1)或不存在(0)以及范数参数等于 false。</p><div class="ki kj kk kl gt ab cb"><figure class="of lv og oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><img src="../Images/89474c3483e4aa2d2a4f410c065722e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*s6orANF73jXejQoLCuQ8GA.png"/></div></figure><figure class="of lv ol oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><img src="../Images/08469544fdd3fdf2c2775d522546d356.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*eIBMUNiHRHmD6mqMkepWuw.png"/></div></figure></div><h2 id="402b" class="kr ks it bd mk nu nv dn mo nw nx dp ms lg ny nz mu lk oa ob mw lo oc od my oe bi translated">单词袋(BoW)术语频率</h2><p id="33d0" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">单词袋(BoW)术语频率捕获文档中术语的频率。在 TfidfVectorizer 下，我们将二进制参数设置为 false，以便它可以显示项的实际频率，而范数参数等于 none。</p><div class="ki kj kk kl gt ab cb"><figure class="of lv om oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><img src="../Images/a2d583ffbaafa8327f9094d9e16108f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*_zfzlmCzjFCVcimdJ-CiXg.png"/></div></figure><figure class="of lv on oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><img src="../Images/e4efbde1c7b29d95df593c8c6330ac60.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*td6OGTHYPKKUOwV1gsxxrw.png"/></div></figure></div><h2 id="eed2" class="kr ks it bd mk nu nv dn mo nw nx dp ms lg ny nz mu lk oa ob mw lo oc od my oe bi translated">(L1)标准化术语频率</h2><p id="8360" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">(L1)规范化术语频率获取文档中规范化的 BoW 术语频率。在 TfidfVectorizer 下，我们将二进制参数设置为等于 false，这样它就可以显示项的实际频率，范数参数等于<strong class="kz iu"> <em class="nf"> l1 </em> </strong>。</p><div class="ki kj kk kl gt ab cb"><figure class="of lv oo oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><img src="../Images/7e52159ad100bd6ea046febf9b51aab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*8IUfXkhGaFkIqsrGo_3Elg.png"/></div></figure><figure class="of lv op oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><img src="../Images/87fdf12be8abd76b232074d1113f4eb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*N_ILjCtGXc6lxVci2R9oGQ.png"/></div></figure></div><h2 id="051f" class="kr ks it bd mk nu nv dn mo nw nx dp ms lg ny nz mu lk oa ob mw lo oc od my oe bi translated">(L2)标准化的 TF-IDF</h2><p id="493d" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">(L2)规范化 TFIDF(术语频率-文档频率倒数)捕获文档中的规范化 TFIDF。以下是计算 TFIDF 的公式。</p><figure class="ki kj kk kl gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi oq"><img src="../Images/2423476f69bfed37d34c6eb1be80889b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FAwUef0viFgvqmpTclGgrA.png"/></div></div><p class="or os gj gh gi ot ou bd b be z dk translated">TFIDF 的公式</p></figure><p id="1394" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在 TfidfVectorizer 下，我们将 binary 参数设置为等于 false，这样它就可以显示项的实际频率，norm 参数等于<strong class="kz iu"> <em class="nf"> l2 </em> </strong>。</p><div class="ki kj kk kl gt ab cb"><figure class="of lv ov oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><img src="../Images/91b7a010f0865bdd3722a4e4eb615f11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*emD5qij_P4xftS98YgHWdQ.png"/></div></figure><figure class="of lv ow oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><img src="../Images/f7009f1e1f1dcdf1e9834d3972257744.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*RYkkNq1JQjJj3mVRp91prg.png"/></div></figure></div><h2 id="c233" class="kr ks it bd mk nu nv dn mo nw nx dp ms lg ny nz mu lk oa ob mw lo oc od my oe bi translated">Word2Vec</h2><p id="d582" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">Word2Vec 提供了单词的嵌入式表示。Word2Vec 从语料库中所有单词的一种表示开始，并在非常大的数据语料库上训练 NN(具有 1 个隐藏层)。以下是通常用于训练神经网络的两种方法:</p><ul class=""><li id="07a3" class="ng nh it kz b la lb ld le lg ni lk nj lo nk ls nl nm nn no bi translated"><strong class="kz iu">连续单词包(CBOW) </strong> —基于<em class="nf">上下文单词窗口</em>预测中心/目标单词的矢量表示</li><li id="bd2d" class="ng nh it kz b la np ld nq lg nr lk ns lo nt ls nl nm nn no bi translated"><strong class="kz iu"> Skip-Gram (SG) </strong> —基于<em class="nf">中心/目标词</em>预测上下文词窗口的向量表示</li></ul><p id="6fce" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">一旦我们有了每个单词的嵌入向量并将其用于 NLP:</p><ul class=""><li id="0c3c" class="ng nh it kz b la lb ld le lg ni lk nj lo nk ls nl nm nn no bi translated">计算单词向量之间的<strong class="kz iu">余弦相似度</strong>，使用单词向量的加权平均值创建更高阶的表示，并提供给分类任务</li></ul><p id="e745" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">Python 的<strong class="kz iu"> <em class="nf"> spacy </em> </strong>包提供了预先训练好的模型，我们可以用它来看看 w2v 是如何工作的。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="06c9" class="kr ks it kn b gy kt ku l kv kw">import spacy<br/>nlp = spacy.load("en_core_web_md", disable=['parser', 'ner'])</span><span id="fac8" class="kr ks it kn b gy ox ku l kv kw"># Get w2v representation of the word 'breakfast'<br/>print (nlp('breakfast').vector.size)<br/>nlp('breakfast').vector[:10]</span><span id="0095" class="kr ks it kn b gy ox ku l kv kw"># Find cosine similarity between w2v representations of breakfast and universe<br/>nlp('breakfast').similarity(nlp('universe'))   # <strong class="kn iu">0.044292555</strong></span><span id="2eca" class="kr ks it kn b gy ox ku l kv kw">doc1 = nlp("I like oranges that are sweet.")<br/>doc2 = nlp("I like apples that are sour.")<br/>doc1.similarity(doc2)     # <strong class="kn iu">0.962154245</strong></span></pre></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="5f4b" class="mj ks it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">案例研究:查尔斯·达尔文的书籍推荐</h1><h2 id="5d52" class="kr ks it bd mk nu nv dn mo nw nx dp ms lg ny nz mu lk oa ob mw lo oc od my oe bi translated">数据</h2><p id="b2a0" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">查尔斯·达尔文是世界上最著名的科学家。他还写了许多其他主题广泛的书籍，包括地质学、植物或他的个人生活。在这个项目中，我们将开发<strong class="kz iu"> </strong>一个<strong class="kz iu">基于内容的图书推荐系统</strong>，它将根据所讨论主题的相似程度来确定哪些图书彼此接近。我们先来看看后面要用的书。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="1328" class="kr ks it kn b gy kt ku l kv kw">import glob  # <strong class="kn iu">glob</strong> is a general term used to define techniques to match specified patterns according to rules related to Unix shell.</span><span id="7778" class="kr ks it kn b gy ox ku l kv kw">folder = "datasets/"<br/>files = glob.glob(folder + "*.txt")<br/>files.sort()</span></pre><h2 id="5f13" class="kr ks it bd mk nu nv dn mo nw nx dp ms lg ny nz mu lk oa ob mw lo oc od my oe bi translated">文本预处理</h2><p id="ae8f" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">作为第一步，我们需要加载每本书的内容，并检查<strong class="kz iu">正则表达式</strong>，通过删除所有非字母数字字符来简化这个过程。我们称这样的文本集合为语料库。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="2d82" class="kr ks it kn b gy kt ku l kv kw">import re, os</span><span id="589e" class="kr ks it kn b gy ox ku l kv kw">txts = []<br/>titles = []</span><span id="2511" class="kr ks it kn b gy ox ku l kv kw">for n in files:<br/>    f = open(n, encoding='utf-8-sig')<br/>    data = re.sub('<strong class="kn iu">[\W_]+</strong>', ' ', f.read()) <br/>    txts.append(data)<br/>    titles.append(os.path.basename(n).replace('.txt', ''))<br/>[len(t) for t in txts]</span></pre><p id="fb3f" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然后，为了一致性，我们将参考达尔文最著名的关于物种起源的书<em class="nf"/>来检查其他书的结果。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="51c8" class="kr ks it kn b gy kt ku l kv kw">for i in range(len(titles)):<br/>    if titles[i] == 'OriginofSpecies':<br/>        ori = i<br/>print(ori)      <strong class="kn iu"># Index = 15</strong></span></pre><p id="bb43" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">下一步，我们通过做<strong class="kz iu">标记化</strong>将语料库转换成一种格式。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="7c99" class="kr ks it kn b gy kt ku l kv kw">stoplist = set('for a of the and to in to be which some is at that we i who whom show via may my our might as well'.split())</span><span id="e462" class="kr ks it kn b gy ox ku l kv kw">txts_lower_case = [i.lower() for i in txts]<br/>txts_split = [i.split() for i in txts]</span><span id="e9c1" class="kr ks it kn b gy ox ku l kv kw">texts = [[word for word in txt if word not in stoplist] for txt in txts_split]<br/>texts[15][0:20]</span></pre><p id="99f2" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">对于文本预处理的下一部分，我们使用一个<strong class="kz iu">词干化</strong>过程，该过程将一个单词的变形形式组合在一起，这样它们就可以作为一个单独的项目进行分析:<em class="nf">词干</em>。为了使过程更快，我们将直接从 pickle 文件中加载最终结果，并回顾用于生成它的方法。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="6be3" class="kr ks it kn b gy kt ku l kv kw">import pickle</span><span id="ed1a" class="kr ks it kn b gy ox ku l kv kw">texts_stem = pickle.load(open('datasets/texts_stem.p', 'rb'))<br/>texts_stem[15][0:20]</span></pre><h2 id="f0f3" class="kr ks it bd mk nu nv dn mo nw nx dp ms lg ny nz mu lk oa ob mw lo oc od my oe bi translated">文本矢量化</h2><p id="baef" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated"><strong class="kz iu">词袋模型(BoW) </strong></p><p id="22c0" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">首先，我们需要创建一个包含查尔斯·达尔文著作语料库中所有单词的宇宙，我们称之为字典。然后，使用词干标记和字典，我们将创建词袋模型(BoW ),将我们的书表示为它们包含的与它们各自的出现次数相关联的所有唯一标记的列表。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="5b02" class="kr ks it kn b gy kt ku l kv kw">from gensim import corpora</span><span id="33b7" class="kr ks it kn b gy ox ku l kv kw">dictionary = corpora.Dictionary(texts_stem)<br/>bows = [dictionary.doc2bow(text) for text in texts_stem]<br/>print(bows[15][:5])</span></pre><p id="fd85" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了更好地理解这个模型，我们将把它转换成一个数据框架，并显示出“<em class="nf">物种起源</em>”这本书最常见的 10 个词干。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="af8a" class="kr ks it kn b gy kt ku l kv kw">df_bow_origin = pd.DataFrame()</span><span id="fa91" class="kr ks it kn b gy ox ku l kv kw">df_bow_origin['index'] = [i[0] for i in bows[15] if i]<br/>df_bow_origin['occurrences'] = [i[1] for i in bows[15] if i]<br/>df_bow_origin['token'] = [dictionary[index] for index in df_bow_origin['index']]<br/>df_bow_origin.occurrences.sort_values(ascending=False).head(10)</span></pre><p id="3f71" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz iu"> TF-IDF 型号</strong></p><p id="fdf6" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">接下来，我们将使用一个<strong class="kz iu"> TF-IDF 模型</strong>根据每个单词在文本中的出现频率来定义它的重要性。因此，一个单词的高 TF-IDF 分数将指示该单词特定于该文本。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="1648" class="kr ks it kn b gy kt ku l kv kw">from gensim.models import TfidfModel</span><span id="6467" class="kr ks it kn b gy ox ku l kv kw">model = TfidfModel(bows)<br/>model[bows[15]]</span></pre><p id="7c94" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">同样，为了更好地理解这个模型，我们将把它转换成一个数据框架，并显示“<em class="nf">物种起源</em>”这本书的 10 个最具体的单词。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="4a39" class="kr ks it kn b gy kt ku l kv kw">df_tfidf = pd.DataFrame()</span><span id="2ce1" class="kr ks it kn b gy ox ku l kv kw">df_tfidf['id'] = [i[0] for i in model[bows[15]]]<br/>df_tfidf['score'] = [i[1] for i in model[bows[15]]]<br/>df_tfidf['token'] = [dictionary[index] for index in df_tfidf['id']]<br/>df_tfidf.score.sort_values(ascending=False).head(10)</span></pre><h2 id="3145" class="kr ks it bd mk nu nv dn mo nw nx dp ms lg ny nz mu lk oa ob mw lo oc od my oe bi translated">建议</h2><p id="295c" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">现在我们有了一个 TF-IDF 模型来衡量它们与每本书的具体程度，我们可以衡量书籍之间的相关程度。因此，我们将使用<strong class="kz iu">余弦相似度</strong>，并将结果可视化为距离矩阵。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="f058" class="kr ks it kn b gy kt ku l kv kw">from gensim import similarities</span><span id="d44f" class="kr ks it kn b gy ox ku l kv kw">sims = similarities.MatrixSimilarity(model[bows])<br/>sim_df = pd.DataFrame(list(sims))<br/>sim_df.columns = titles <br/>sim_df.index = titles<br/>sim_df</span></pre><h2 id="006b" class="kr ks it bd mk nu nv dn mo nw nx dp ms lg ny nz mu lk oa ob mw lo oc od my oe bi translated"><strong class="ak">结论</strong></h2><p id="0419" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">我们现在有了一个矩阵，其中包含了查尔斯·达尔文的任何两本书之间的所有相似性度量！我们可以使用 barh()来显示一个横条图，其中的书籍与《物种起源》中的<em class="nf">最为相似。</em></p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="7d38" class="kr ks it kn b gy kt ku l kv kw">%matplotlib inline<br/>import matplotlib.pyplot as plt</span><span id="d1a0" class="kr ks it kn b gy ox ku l kv kw">v = sim_df['OriginofSpecies']<br/>v_sorted = v.sort_values()<br/>v_sorted.plot.barh()<br/>plt.xlabel('Similarity')</span></pre><figure class="ki kj kk kl gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi oy"><img src="../Images/073084cee7f3275f9b3206e53219a9eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_d4sW_b0UXbavv3HuLxSEg.png"/></div></div></figure><p id="1cbf" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然而，我们想更好地了解大局，看看达尔文的书通常是如何相互关联的。为此，我们将把整个相似性矩阵表示为一个树状图，这是显示此类数据的标准工具。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="cc47" class="kr ks it kn b gy kt ku l kv kw">from scipy.cluster import hierarchy</span><span id="9dd8" class="kr ks it kn b gy ox ku l kv kw">Z = hierarchy.linkage(sim_df, 'ward')<br/>chart = hierarchy.dendrogram(Z, leaf_font_size=8, labels=sim_df.index, orientation="left")</span></pre><figure class="ki kj kk kl gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi oz"><img src="../Images/111c44fd3e63fbd3cbd09dcc30b9a316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dgOzmSrFFQB6a6mntSragA.png"/></div></div></figure><p id="1c83" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">最后，基于我们之前创建的图表，我们可以得出结论:在物种起源上，“<em class="nf">被驯化的动植物的变异</em>与“<em class="nf">最相关。</em>”</p><p id="74fe" class="pw-post-body-paragraph kx ky it kz b la lb ju lc ld le jx lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">创建这篇文章的源代码可以在我的<a class="ae lt" href="https://github.com/shirley0823/Book-Recommendations-from-Charles-Darwin" rel="noopener ugc nofollow" target="_blank"> Github </a>中找到。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="fee2" class="mj ks it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">关于我</h1><p id="37d2" class="pw-post-body-paragraph kx ky it kz b la na ju lc ld nb jx lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">非常感谢您阅读我的文章！大家好，我是雪莉，目前在亚利桑那州立大学攻读商业分析硕士学位。如果您有任何问题，请随时联系我！</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="8584" class="kr ks it kn b gy kt ku l kv kw">Email me at <strong class="kn iu"><em class="nf">kchen122@asu.edu</em></strong><em class="nf"> </em>and feel free to connect me on <a class="ae lt" href="https://www.linkedin.com/in/kuanyinchen-shirley/" rel="noopener ugc nofollow" target="_blank"><strong class="kn iu">LinkedIn</strong></a>!</span></pre></div></div>    
</body>
</html>