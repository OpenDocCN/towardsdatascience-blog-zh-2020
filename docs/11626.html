<html>
<head>
<title>Understanding gradient boosting from scratch with a small dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用小数据集从头理解梯度增强</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-gradient-boosting-from-scratch-with-small-dataset-587592cc871f?source=collection_archive---------20-----------------------#2020-08-12">https://towardsdatascience.com/understanding-gradient-boosting-from-scratch-with-small-dataset-587592cc871f?source=collection_archive---------20-----------------------#2020-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="1ea0" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">什么是助推？</h1><p id="b81e" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">Boosting 是一种非常流行的集成技术，在这种技术中，我们将许多弱学习者组合起来，将他们转化为强学习者。增强是一种顺序操作，其中我们以渐进的方式建立相互依赖的串联弱学习器，即弱学习器 m 依赖于弱学习器 m-1 的输出。用于增强的弱学习者具有高偏差和低方差。简而言之，boosting 可以解释为 boosting =弱学习者+加法梳理。</p><h1 id="f467" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">梯度推进算法</h1><p id="f4ca" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">梯度推进是一种用于回归和分类问题的机器学习技术，它以弱预测模型的集合的形式产生预测模型，通常是决策树(<a class="ae lm" href="https://en.wikipedia.org/wiki/Gradient_boosting#:~:text=Gradient%20boosting%20is%20a%20machine,prediction%20models%2C%20typically%20decision%20trees." rel="noopener ugc nofollow" target="_blank">维基百科定义</a>)</p><h1 id="3922" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">算法步骤:</h1><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/decacf794b263189334f655dfb9dd29b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J6tdM41_B4afb5axBhkC5A.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">作者图片</p></figure><p id="0d65" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">现在让我们在一个小数据集(D)上应用所有这些步骤。数据集的大小保持非常小，以避免冗长的计算。这是一个预测公寓价格(以 10 万卢比计)的数据集，使用的参数包括地毯面积、位置(位于城市黄金地段或城外)和停车场(我们有公寓停车场吗)数据集:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mi"><img src="../Images/8606bf7f2e627cd949658a9c6164ee3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FmIxo4R8V-PqYf-PTfheog.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">作者图片</p></figure><h1 id="8623" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">第一步:</h1><p id="e11c" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">梯度增强的第一步是用常数值初始化模型 F0(x)。也就是说，我们必须找到γ(γ)的最佳值，以使损耗值降低。现在让我们为数据集(D)求解步骤 1 中的方程</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mj"><img src="../Images/8f6077c3760b72115438ce39fea228d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jtTnnMSJlgP1HJ4mhv8_5w.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">作者图片</p></figure><h1 id="321a" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">第二步:</h1><p id="1b25" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这一步是我们建立所有弱学习者的循环。这里 m=1 表示我们正在构建第一个弱学习器，并且我们已经在步骤 1 中构建了基础学习器 F0(x)</p><h1 id="c4b2" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">第 2.1 步</h1><p id="8eb5" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这里我们首先计算伪残差，也称为假残差。与残差相比，使用伪残差的最大优点是，他们让我们有任何损失函数。也就是说，它们帮助我们最小化我们选择的任何损失函数，直到损失函数是可微的。大多数伪残差几乎与残差相似，或者与残差成一定比例。</p><p id="3200" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">以下等式显示了伪残差与实际残差是如何相似的。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/f743e544cbcd9bbd9f879ad38edfb321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*FbsDmDE0MgiqYtUki9y3iA.png"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">作者图片</p></figure><p id="185b" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">对于所使用的损失函数，伪残差完全等于实际残差，但这并不总是正确的，因为随着损失函数的改变，方程和值也改变。在上面的等式中，Fm(Xi)表示前一阶段来自弱学习者的预测值</p><p id="51af" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">算法步骤 2.1 中给出的方程与上述方程的 L.H.S .相同，用简单的英语来说，这两个方程都可以解释为损失函数相对于阶段 m-1 的模型构建的负导数</p><p id="51ae" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">让我们计算数据集(D)的伪残差</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/6cac9b6f279c0e424e56861d217fb6a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*GWqxLsb6CjgO7aN-Eb_1kQ.png"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">作者图片</p></figure><p id="17f8" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">在上表中，rim 是实际值(yi)和模型预测值(F0(x))的差值，r 表示残差，m 表示我们正在制作的弱学习器的数量，I 表示实例的数量，因此这里 r11 表示实例 1 的弱学习器 1 的残差。</p><h1 id="86a6" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">第 2.2 步</h1><p id="450e" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在这一步，我们训练下一个弱学习者 F1(X)。让本周的学习者成为决策树。</p><p id="56d0" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">在梯度推进中，如果我们使用决策树作为弱学习器，那么决策树的深度应该在 2 到 9 之间，默认情况下设置为 3。此外，请注意，深度必须在 2 到 9 之间并不是一条经验法则，这在很大程度上取决于我们的数据集。</p><p id="c8ae" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">在梯度推进中，我们在 Xi 和伪残差上训练我们的弱学习者，即我们将使用地毯面积、停车场和位置来预测我们在步骤 2.1 中计算的伪残差。我们数据集的决策树</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/edaf066a5f9616c3b80b71a87aa2cfe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*IaalxYGN6YO13VJcI8GO-Q.jpeg"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">作者图片</p></figure><p id="1951" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">决策树的叶子被称为终端区域(Rjm)。其中 m 是我们刚刚制作的树的索引，因为这是第一棵树，m=1，j 是树中每片叶子的索引。</p><h1 id="d16c" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">第 2.3 步</h1><p id="002a" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在这一步中，我们确定每个叶子的输出值。在上面的树中，有一个具有 2 个条目的叶子，所以不清楚它的输出值应该是什么，所以叶子的输出值可以通过求解以下方程来计算</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/0dd07eeed0a4eb11d44f172c861c25d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*V97E1n39aovT7YcO3cVUoA.png"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">作者图片</p></figure><p id="9edb" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">注意:给定任何损失函数，叶的输出值总是该叶的残差的平均值。</p><h1 id="247c" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">第 2.4 步</h1><p id="60cc" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在这一步，我们更新我们的模型。步骤 2.4 的等式有两个部分，第一部分显示我们对伽马(γ)的初始预测，即 F0(x)，第二部分代表我们在步骤 2.3 之前刚刚构建的树。因此，该等式可以表示为</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/8cadae89ba37d48e4765d1ce052e998d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*Leg_sGJ6oHRaicHnNs0bUw.png"/></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">作者图片</p></figure><p id="856e" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">在上面的等式中，alpha(α)代表学习率。α的值在 0 到 1 之间变化，设α= 0.1。梯度提升中的学习率参数用于控制每个弱学习器的贡献，这些弱学习器被串联添加。</p><p id="4ec2" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated"><strong class="kq iu"> Record1 </strong>:新预测= 45+0.1 *(10)= 44，与模型 F0(x)所做的早期预测相比，更接近真实值。这意味着我们朝着正确的方向迈出了一小步。</p><p id="b394" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated"><strong class="kq iu">记录 2 </strong>:新的预测= 45+0.1*(15)=46.5，与之前模型 F0(x)的预测相比，更接近真实值。这意味着我们朝着正确的方向迈出了一小步</p><p id="d36b" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated"><strong class="kq iu">记录 3 </strong>:新的预测= 45+0.1 *(20)= 43，与模型 F0(x)的早期预测相比，更接近真实值。这意味着我们朝着正确的方向迈出了一小步。</p><p id="8d1f" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated"><strong class="kq iu"> Record4 </strong>:新预测= 45+0.1*(15)= 46.5，与模型 F0(x)做出的早期预测相比，更接近真实值。这意味着我们朝着正确的方向迈出了一小步。</p><p id="47e5" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">现在我们已经完成了步骤 2 的第一次迭代。我们将继续迭代，直到 M 的值达到 M 的值，这里 M 是构建一个算法所需的弱学习者总数。实际上，对于梯度提升，M = 100。但是为了简单起见，让我们保持 M=2</p><p id="3575" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">现在我们将再次重复步骤 2。这意味着对于 m=2，我们将计算伪残差，拟合下一个弱学习器 F2(x)，计算叶子的输出，并将更新模型。这里我们完成第 2 步得到的 m 的值达到了 m 的值</p><h1 id="2833" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">第三步</h1><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mp"><img src="../Images/97bf7e541404a493b27e952f47434246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jkkZ5eO5grHJfmLgV-bB2g.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">作者图片</p></figure><p id="104e" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">在步骤 3 中，我们计算梯度推进算法的输出。上面的等式代表了我们算法的最终输出。当 m = 2 时，上述等式的第 3 部分表示步骤 2 中的树构建。这里 M = 2，这意味着 F2(x)是梯度增强算法的输出。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><p id="ed82" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated">希望这篇文章能帮助你增加对梯度推进的理解。我们知道梯度推进是一种强大的机器学习算法，但这不应该阻止我们了解它是如何工作的。我们对算法了解得越多，我们就能更好地有效使用它，并解释它是如何做出预测的。</p><p id="3635" class="pw-post-body-paragraph ko kp it kq b kr md kt ku kv me kx ky kz mf lb lc ld mg lf lg lh mh lj lk ll im bi translated"><strong class="kq iu">感谢阅读。</strong></p></div></div>    
</body>
</html>