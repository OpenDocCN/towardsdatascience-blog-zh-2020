<html>
<head>
<title>Multi-Armed Bandits: UCB Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多武装匪徒:UCB算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-armed-bandits-ucb-algorithm-fa7861417d8c?source=collection_archive---------7-----------------------#2020-01-08">https://towardsdatascience.com/multi-armed-bandits-ucb-algorithm-fa7861417d8c?source=collection_archive---------7-----------------------#2020-01-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3a66" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于置信界限优化动作</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2c73d8b2dcb9542e8ca6225f832fb1d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Gqzt_YAPZA4QdHX8"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔纳森·克洛克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="14a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想象一下，你在一个赌场，在一堆(k个)有不同回报概率的独臂强盗(也叫吃角子老虎机)中进行选择，并且想要选择最好的一个。你使用什么策略来帮助你走出最大数量的季度？</p><p id="e815" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是多兵种土匪问题的本质，是一个<a class="ae ky" href="https://www.datahubbs.com/multi_armed_bandits_reinforcement_learning_1/" rel="noopener ugc nofollow" target="_blank">简化强化学习</a>任务。当你在网站上寻求最大化的参与度，或者搞清楚临床试验，或者试图优化你的电脑性能时，这些都可能发生。</p><p id="4538" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它们不仅仅是一个有用的框架，解决起来也很有趣！</p><h1 id="bbee" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="230d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们用Python中的例子来教授置信上限bandit算法，以便让您快速熟悉这种方法。</p><h1 id="6f5f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">你的第一个策略</h1><p id="298d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">强盗问题需要在探索和开发之间取得平衡。因为问题始于对奖励的无先验知识，一旦缩小了选择范围，它需要探索(尝试许多吃角子老虎机)然后利用(反复拉动最佳杠杆)。</p><p id="1496" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最简单的强盗采用ϵ-greedy策略，这意味着它会选择它认为大多数时候最好的老虎机，但有一个小的ϵ%概率，它会选择随机的老虎机。这是一个简单的策略，可以产生良好的结果(<a class="ae ky" href="https://www.datahubbs.com/multi_armed_bandits_reinforcement_learning_1/" rel="noopener ugc nofollow" target="_blank">如果你想看到这种方法被实施</a>，请看这篇文章)，但是我们可以做得更好。</p><h1 id="07eb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">置信上限Bandit</h1><p id="742b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">ϵ-greedy可能会花很长时间来决定选择合适的独臂强盗，因为这是基于小概率的探索。<strong class="lb iu">置信上限</strong> (UCB)方法有所不同，因为我们基于对给定选择的不确定性来做出选择。</p><p id="2172" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我会向你展示数学，然后解释:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/3cf0823943b6fb6fa37248cc26d7b19d.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*xL_lDbRdg4FdLQLOb4ThOg.png"/></div></figure><p id="71a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述等式为我们的模型提供了选择标准。Q_n(a)是我们对给定老虎机<em class="mt"> a. </em>的当前估计值，平方根下的值是我们尝试过的老虎机总数的对数，<em class="mt"> n </em>除以我们尝试每台老虎机的次数<em class="mt"> a </em> ( <em class="mt"> k_n </em>)，而<em class="mt"> c </em>只是一个常数。我们通过选择在每一步<em class="mt"> n </em>中给出最大值的强盗来选择下一台吃角子老虎机。</p><p id="b783" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平方根项是对每个动作<em class="mt">和</em>的方差的估计。如果我们还没有选择一个土匪，方差是无限的(我们将除以0)，这意味着这将是我们的下一个选择。这迫使算法快速探索那些未知值。一旦它选择了它，在其他条件相同的情况下，它的价值就会下降，相对于其他选择来说，可能性变得更小。因为log(n)在分子中，每次我们不选择一个动作，它的值变得更有可能，尽管分子增长得更慢(即对数)，所以你收集的数据越多，这种影响变得越小。最终结果是一种算法，在锁定最有利可图的机器之前，可以非常快速地采样，以减少未知的不确定性。</p><h2 id="dd01" class="mu lw it bd lx mv mw dn mb mx my dp mf li mz na mh lm nb nc mj lq nd ne ml nf bi translated">一些实际的警告</h2><p id="099f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在我们深入实现之前，我们将初始化我们的值，k_n(a) = 1而不是0，以确保我们不会在Python中弹出任何<code class="fe ng nh ni nj b">nan</code>值。此外，我们将模拟很多强盗拉，所以我们需要有效率地计算我们的一些值。Q_n(a)只是该动作的平均值，我们可以使用一个方便的公式来计算平均值，而不是为每个动作保留数千个值，只需为每个动作存储两条信息，即我们为该给定动作做出了多少次选择(k_n(a))和我们的当前平均值(m_n)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b01b3d0d8e4df782d439eb23c066b292.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*kCsYAbE94jyUUtkHAbF6Uw.png"/></div></figure><p id="20cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中R_n是我们刚刚从采取一项行动中获得的回报(为简洁起见，我省略了a)。</p><h1 id="4e9d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">UCB电码</h1><pre class="kj kk kl km gt nl nj nm nn aw no bi"><span id="5ea6" class="mu lw it nj b gy np nq l nr ns"># import modules <br/>import numpy as np <br/>import matplotlib.pyplot as plt <br/>import pandas as pd</span><span id="4aba" class="mu lw it nj b gy nt nq l nr ns">class ucb_bandit:<br/>    '''<br/>    Upper Confidence Bound Bandit<br/>    <br/>    Inputs <br/>    ============================================<br/>    k: number of arms (int)<br/>    c:<br/>    iters: number of steps (int)<br/>    mu: set the average rewards for each of the k-arms.<br/>        Set to "random" for the rewards to be selected from<br/>        a normal distribution with mean = 0. <br/>        Set to "sequence" for the means to be ordered from <br/>        0 to k-1.<br/>        Pass a list or array of length = k for user-defined<br/>        values.<br/>    '''<br/>    def __init__(self, k, c, iters, mu='random'):<br/>        # Number of arms<br/>        self.k = k<br/>        # Exploration parameter<br/>        self.c = c<br/>        # Number of iterations<br/>        self.iters = iters<br/>        # Step count<br/>        self.n = 1<br/>        # Step count for each arm<br/>        self.k_n = np.ones(k)<br/>        # Total mean reward<br/>        self.mean_reward = 0<br/>        self.reward = np.zeros(iters)<br/>        # Mean reward for each arm<br/>        self.k_reward = np.zeros(k)<br/>        <br/>        if type(mu) == list or type(mu).__module__ == np.__name__:<br/>            # User-defined averages            <br/>            self.mu = np.array(mu)<br/>        elif mu == 'random':<br/>            # Draw means from probability distribution<br/>            self.mu = np.random.normal(0, 1, k)<br/>        elif mu == 'sequence':<br/>            # Increase the mean for each arm by one<br/>            self.mu = np.linspace(0, k-1, k)<br/>        <br/>    def pull(self):<br/>        # Select action according to UCB Criteria<br/>        a = np.argmax(self.k_reward + self.c * np.sqrt(<br/>                (np.log(self.n)) / self.k_n))<br/>            <br/>        reward = np.random.normal(self.mu[a], 1)<br/>        <br/>        # Update counts<br/>        self.n += 1<br/>        self.k_n[a] += 1<br/>        <br/>        # Update total<br/>        self.mean_reward = self.mean_reward + (<br/>            reward - self.mean_reward) / self.n<br/>        <br/>        # Update results for a_k<br/>        self.k_reward[a] = self.k_reward[a] + (<br/>            reward - self.k_reward[a]) / self.k_n[a]<br/>        <br/>    def run(self):<br/>        for i in range(self.iters):<br/>            self.pull()<br/>            self.reward[i] = self.mean_reward<br/>            <br/>    def reset(self, mu=None):<br/>        # Resets results while keeping settings<br/>        self.n = 1<br/>        self.k_n = np.ones(self.k)<br/>        self.mean_reward = 0<br/>        self.reward = np.zeros(iters)<br/>        self.k_reward = np.zeros(self.k)<br/>        if mu == 'random':<br/>            self.mu = np.random.normal(0, 1, self.k)</span></pre><p id="dca3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的代码定义了我们的<code class="fe ng nh ni nj b">ucb_bandit</code>类，使我们能够模拟这个问题。运行它需要三个参数:要拉的臂数(<code class="fe ng nh ni nj b">k</code>)、探索参数(<code class="fe ng nh ni nj b">c</code>)和迭代次数(<code class="fe ng nh ni nj b">iters</code>)。</p><p id="79dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过设置<code class="fe ng nh ni nj b">mu</code>来定义奖励(默认情况下，奖励均值来自正态分布)。我们对此运行1000集，并平均每集1000步的回报，以了解该算法的执行情况。</p><pre class="kj kk kl km gt nl nj nm nn aw no bi"><span id="5574" class="mu lw it nj b gy np nq l nr ns">k = 10 # number of arms<br/>iters = 1000</span><span id="c956" class="mu lw it nj b gy nt nq l nr ns">ucb_rewards = np.zeros(iters)<br/># Initialize bandits<br/>ucb = ucb_bandit(k, 2, iters)</span><span id="7bac" class="mu lw it nj b gy nt nq l nr ns">episodes = 1000<br/># Run experiments<br/>for i in range(episodes): <br/>    ucb.reset('random')<br/>    # Run experiments<br/>    ucb.run()<br/>    <br/>    # Update long-term averages<br/>    ucb_rewards = ucb_rewards + (<br/>        ucb.reward - ucb_rewards) / (i + 1)<br/>    <br/>plt.figure(figsize=(12,8))<br/>plt.plot(ucb_rewards, label="UCB")<br/>plt.legend(bbox_to_anchor=(1.2, 0.5))<br/>plt.xlabel("Iterations")<br/>plt.ylabel("Average Reward")<br/>plt.title("Average UCB Rewards after " <br/>          + str(episodes) + " Episodes")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/731edbbe44f76afa7809f5a65731b2c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Te7D4wk3fpkmHQbtQd0_zA.png"/></div></div></figure><p id="adbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目前看来还不错！强盗通过稳步增长的平均奖励来学习。为了了解它的工作情况，我们将它与标准的ϵ-greedy方法进行比较(<a class="ae ky" rel="noopener" target="_blank" href="/multi-armed-bandits-and-reinforcement-learning-dc9001dcb8da">代码可以在这里找到</a>)。</p><pre class="kj kk kl km gt nl nj nm nn aw no bi"><span id="e597" class="mu lw it nj b gy np nq l nr ns">k = 10<br/>iters = 1000</span><span id="e81b" class="mu lw it nj b gy nt nq l nr ns">eps_rewards = np.zeros(iters)<br/>ucb_rewards = np.zeros(iters)</span><span id="f84e" class="mu lw it nj b gy nt nq l nr ns"># Initialize bandits<br/>ucb = ucb_bandit(k, 2, iters)<br/>eps_greedy = eps_bandit(k, 0.1, iters, ucb.mu.copy())</span><span id="6a00" class="mu lw it nj b gy nt nq l nr ns">episodes = 1000<br/># Run experiments<br/>for i in range(episodes):<br/>    ucb.reset()<br/>    eps_greedy.reset()</span><span id="98d1" class="mu lw it nj b gy nt nq l nr ns"># Run experiments<br/>    ucb.run()<br/>    eps_greedy.run()<br/>    <br/>    # Update long-term averages<br/>    ucb_rewards += (ucb.reward  - ucb_rewards) / (i + 1)<br/>    eps_rewards += (eps_greedy.reward - eps_rewards) / (i + 1)<br/>    <br/>plt.figure(figsize=(12,8))<br/>plt.plot(ucb_rewards, label='UCB Bandit')<br/>plt.plot(eps_rewards, label="$\epsilon={}$".format(<br/>    eps_greedy.eps))<br/>plt.legend(bbox_to_anchor=(1.3, 0.5))<br/>plt.xlabel("Iterations")<br/>plt.ylabel("Average Reward")<br/>plt.title("Average rewards for UCB and $\epsilon$-Greedy Bandits")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/e69bcc95ee19c28c4f4bdf945419132c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UcUxiQmclLQZtEOD-ow0AA.png"/></div></div></figure><p id="2ea7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">UCB方法快速找到最佳动作，并在大部分剧集中利用它，而贪婪算法尽管也能相对快速地找到最佳动作，但随机性太大。我们也可以通过观察动作选择和比较每个算法选择的最优动作来观察这一点。</p><pre class="kj kk kl km gt nl nj nm nn aw no bi"><span id="4cae" class="mu lw it nj b gy np nq l nr ns">width = 0.45<br/>bins = np.linspace(0, k-1, k) - width/2</span><span id="090d" class="mu lw it nj b gy nt nq l nr ns">plt.figure(figsize=(12,8))<br/>plt.bar(bins, eps_greedy.k_n,<br/>        width=width, <br/>        label="$\epsilon$={}".format(eps_greedy.eps))<br/>plt.bar(bins+0.45, ucb.k_n,<br/>        width=width, <br/>        label="UCB")<br/>plt.legend(bbox_to_anchor=(1.3, 0.5))<br/>plt.title("Number of Actions Selected by Each Algorithm")<br/>plt.xlabel("Action")<br/>plt.ylabel("Number of Actions Taken")<br/>plt.show()</span><span id="a86c" class="mu lw it nj b gy nt nq l nr ns">opt_per = np.array([eps_greedy.k_n, ucb.k_n]) / iters * 100<br/>df = pd.DataFrame(np.vstack(<br/>    [opt_per.round(1), <br/>    eps_greedy.mu.reshape(-1, 1).T.round(2)]), <br/>        index=["Greedy", "UCB", "Expected Reward"],<br/>        columns=["a = " + str(x) for x in range(0, k)])<br/>print("Percentage of actions selected:")<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/30b51c2cc541c2af35df00c7acf992f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wDQSrsWwx9NWTh_7pUUlLw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/f2d36d26e8f4a7fd90a778c8b43ece4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vpk4v9ANUuWEs-1rmqroRQ.png"/></div></div></figure><p id="b256" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，我们的最优行动是a=6，平均回报为1.54。ϵ-greedy算法选择它的概率是83.4%，而UCB算法选择它的概率是89.7%。此外，您将看到贪婪算法选择负值的频率比UCB算法高得多，这也是因为每次选择都有10%的机会选择随机行动。</p><p id="01fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有几十个<a class="ae ky" href="https://www.datahubbs.com/multi-armed-bandits-reinforcement-learning-2/" rel="noopener ugc nofollow" target="_blank">其他多臂土匪方法</a>在那里学习。这是一个简单的优化框架，但是功能非常强大，玩起来也很有趣！</p></div></div>    
</body>
</html>