<html>
<head>
<title>Squeeze These Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">挤压这些神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/squeeze-that-neural-network-juice-3ce28a2cca75?source=collection_archive---------42-----------------------#2020-04-24">https://towardsdatascience.com/squeeze-that-neural-network-juice-3ce28a2cca75?source=collection_archive---------42-----------------------#2020-04-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a02c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">制作神经网络(和更有效的网络)的技巧</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/23ca0b97976e0ab9e471b15d5d0c4ec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-reFUqDEI7hC2YLL"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">杰西卡·路易斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7e0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">我们</span>都知道神经网络有多贵。训练是一项乏味的任务，需要多次迭代(每次迭代需要几个时期(每个时期需要几个步骤)(您调整过超参数吗？(也许还有一层？(和…)))))((())))))))))))))</p><p id="92ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">…如果您已经开始部署任何东西，那么您也知道云推理不是免费的。模型越大，神经网络从你身上榨取的A̶m̶a̶z̶o̶n就越多。</p><p id="1b7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，i̶̶t̶e̶a̶c̶h̶̶h̶o̶w̶̶t̶o̶̶m̶a̶k̶e̶̶c̶a̶i̶p̶i̶r̶i̶n̶h̶a̶s我概述了一套技术，你可以探索，使训练更快，生产更小的模型，并最终节省金钱和时间。所有的技巧都是实用的，是你今天可以应用，最多明天就能得到结果的东西。</p><p id="6e8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不要让神经网络压榨你的钱包。把那些重物挤出来。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ml"><img src="../Images/a1c35722cac26b540861822f19dd5e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QAxKjgEm6NwhURel"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@mariadasdores?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Maria das Dores </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="321e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顺便说一句，如果你是为了caipirinhas来的，我很抱歉。它需要一个切成八个半月形的酸橙和一汤匙(或三汤匙)糖，放在一个大玻璃杯里碾碎。不多也不少。倒入一杯<em class="mm">cachaa</em>，剩下的装满冰块。伏特加也行，但用caipivodka代替。</p><p id="ec36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络。聚焦聚焦</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="cd03" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">你能抽出2%吗？</h1><p id="bbc5" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">假设你有一家公司，它有一个92%准确的神经网络产品。然而，它使用了大约一亿个参数，并需要两天时间在多GPU基础设施上进行训练。进一步改进模型的成本特别高，因为大多数改变要么使它变得更糟，要么根本没有提供任何改进。</p><p id="6797" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">如果您可以拥有一个90%准确的模型，并且占用的资源减少20-50倍，这会扼杀您的业务还是让它蓬勃发展？</strong></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="bb6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">达到94%是很难的。这需要大量资源、更大的网络、时间…</p><p id="4563" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">达到90%很容易。如果能做到92%，做的少就是小事。</p><p id="1ceb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">目标是:在不低于90%的情况下，您能简化多少？</strong></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="68e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">当前的文献和实践表明，在保持大部分原始精度不变的情况下，降级模型是相当容易的。有些技术是纯技术性的，而其他技术需要对网络架构稍加改动。在下文中，我首先概述了可以应用于预训练模型的技术，以便您今天就可以获得结果，然后是需要一些再训练的技术，这可能明天就可以完成。</p><h1 id="ef5b" class="mn mo it bd mp mq nk ms mt mu nl mw mx jz nm ka mz kc nn kd nb kf no kg nd ne bi translated">适用于预训练模型的技术</h1><h2 id="271f" class="np mo it bd mp nq nr dn mt ns nt dp mx li nu nv mz lm nw nx nb lq ny nz nd oa bi translated">使用半精度:</h2><p id="0724" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">通常，使用浮点数来训练模型。大多数编码人员都知道“double”类型，这是一种更精确的浮点数，但是知道“half”类型(16位浮点数)的人却不多。</p><p id="af97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用“半”类型可以将内存消耗和计算时间减半。使用半精度训练的神经网络可以用两倍于普通网络的批量来训练，基本上每个时期的时间减半。在推理过程中，半精度模型更容易转移，执行速度更快。此外，许多公司为半精确模型提供特殊的云硬件。</p><p id="4e70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">好消息是:</strong>你可以将一个预先训练好的模型量化到半精度，它确实有效。<a class="ae ky" href="https://www.tensorflow.org/lite/performance/post_training_float16_quant" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite包可以帮你做这件事</a>。如果您正在从头开始训练您的模型，您可以通过<a class="ae ky" href="https://www.tensorflow.org/guide/keras/mixed_precision" rel="noopener ugc nofollow" target="_blank">启用半精度模式</a>来从中受益(不要忘记将批量加倍！).</p><h2 id="9ac6" class="np mo it bd mp nq nr dn mt ns nt dp mx li nu nv mz lm nw nx nb lq ny nz nd oa bi translated">权重修剪</h2><p id="d9ad" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在数百万个参数中，有些很大，有些很小。有许多技术可以发现这些“微小”的权重，并将其从网络中移除。仅修剪就可以去除10%到90%的权重，而对总准确度的影响不到2%。</p><p id="848d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">修剪过的网络压缩得更好，推理时间也更短。你可能会感到惊讶，甚至MobileNet可以缩小到原来的一半大小。大型非优化模型，如InceptionNet，可能会去除高达85%的原始重量。</p><p id="8bf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">好消息是:</strong> TensorFlow有几项权重修剪技术<a class="ae ky" href="https://www.tensorflow.org/model_optimization/guide/pruning" rel="noopener ugc nofollow" target="_blank">易于使用</a>，也与<a class="ae ky" href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras" rel="noopener ugc nofollow" target="_blank"> Keras后端</a>兼容。一个社区解决方案是<a class="ae ky" href="https://github.com/BenWhetton/keras-surgeon" rel="noopener ugc nofollow" target="_blank"> Keras Surgeon </a>，它作为一个<a class="ae ky" href="https://github.com/BenWhetton/keras-surgeon/blob/master/src/kerassurgeon/examples/lenet_mnist.py" rel="noopener ugc nofollow" target="_blank">很好的例子</a>展示了如何在修剪和调整之间交替。</p><p id="0948" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更进一步:</strong>大多数修剪工具都去掉了权重，但保持了架构的完整性。更先进的技术可以从卷积中去除整个过滤器，从密集层中去除神经元，从而提高训练速度。Keras外科医生可以做到这一点。两个有趣的阅读是<a class="ae ky" href="https://arxiv.org/abs/1608.08710" rel="noopener ugc nofollow" target="_blank">这个关于修剪过滤器的工作</a>和<a class="ae ky" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank">彩票假设</a>。</p><h2 id="033e" class="np mo it bd mp nq nr dn mt ns nt dp mx li nu nv mz lm nw nx nb lq ny nz nd oa bi translated">考虑使用优化的“现成”模型</h2><p id="a240" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">许多公司在他们的管道中使用预先训练的模型，不仅仅是作为主干架构。通常，他们使用“大型”模型，如VGG-19、InceptionNet和ResNet。请考虑改用优化模型。</p><p id="553d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">例如，</strong>如果您正在使用ResNet-151，您可以考虑迁移到ResNet-50，或者向下迁移到MobileNetV3架构。如果你依赖于更快的R-CNN架构来进行物体检测，考虑切换它的主干(或者整个东西)。YOLO和YOLO快速是伟大的优化替代品。这可能会为你节省数千美元的推理时间。</p><p id="b893" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">明智一点:这些选择中的一些会让你对我们2%的协议不满。有些会让你下跌3%，或者10%。他们会省下多少钱？你会失去多少顾客？改变可能还是有利可图的。</p><h2 id="2fe6" class="np mo it bd mp nq nr dn mt ns nt dp mx li nu nv mz lm nw nx nb lq ny nz nd oa bi translated">服无期徒刑</h2><p id="6d69" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">快速提醒一下，你可以用YOLO或MobileNet，修剪它，量化到半精度。这些技术并不相互排斥。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="65a8" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">需要再培训的技术</h1><h2 id="e652" class="np mo it bd mp nq nr dn mt ns nt dp mx li nu nv mz lm nw nx nb lq ny nz nd oa bi translated">改变脊柱</h2><p id="dbec" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">继续最后一个技巧，如果您使用现成的模型作为主干，这是一个时髦的做法，请考虑切换到较小的ResNet主干或MobileNetV3。这对迁移学习很有帮助。</p><p id="91fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">摆脱腰痛。<a class="ae ky" href="https://keras.io/applications/" rel="noopener ugc nofollow" target="_blank"> Keras </a> / <a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/applications" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>捆绑了很多预先训练好的模型，可以很方便的作为骨干使用。</p><h2 id="29b6" class="np mo it bd mp nq nr dn mt ns nt dp mx li nu nv mz lm nw nx nb lq ny nz nd oa bi translated">像MobileNet一样</h2><p id="ebc5" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">阅读论文<a class="ae ky" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank">的好处是</a>知道其他作者是如何做出惊人之举的。MobileNet性能的关键是<strong class="lb iu">深度方向可分离卷积</strong>。</p><p id="dff6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，这个想法是将具有128个滤波器的3乘3卷积分成两个连续的步骤:“空间”卷积，完成“3乘3”部分，以及“深度”卷积，完成“128个滤波器”部分。<strong class="lb iu">平均而言，这要快9倍，并且使用的参数数量级更少。</strong></p><p id="2940" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">好消息是:</strong>这和修改一行代码一样简单。对于Keras用户，只需将“Conv2D”改为“<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D" rel="noopener ugc nofollow" target="_blank">depthwisecov2d</a>”。所有参数都一样。以我的经验来看，这并没有训练得更快，但是推理和模型大小都得到了极大的改进。</p><h2 id="d6e8" class="np mo it bd mp nq nr dn mt ns nt dp mx li nu nv mz lm nw nx nb lq ny nz nd oa bi translated">知识蒸馏</h2><p id="9fd1" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">如果你已经有了一个工作模型，一个非常有效的修剪技术就是知识提炼。也被称为<strong class="lb iu">师生模式。</strong></p><p id="c8e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对整个数据集运行良好的旧模型并保存其预测。这些是“软标签”它们不是典型的一次性编码标签；他们有一些不确定性。现在，在软标签上训练较小的模型，而不是地面事实。这将教会“学生”复制“老师”，而不是试图自己学习。</p><p id="ee0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">这看起来很复杂，但其实很简单:</strong>你只需要像平常一样运行旧模型，然后处理结果。然后，使用您拥有的相同代码，丢弃数据集并从pickle文件中读取基本事实标签。为了避免将“数据集”和“教师”作为两个独立的事物，您也可以将数据集剥离。原来的老师。</p><p id="8f23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">多做一点:</strong>更全面的方法可能会将真实标签与教师标签进行插值，或者创建一个混合损失函数。除了输出层之外，您还可以“教授”内部层的输出。然而，这让事情变得有点复杂。</p><h2 id="d3a7" class="np mo it bd mp nq nr dn mt ns nt dp mx li nu nv mz lm nw nx nb lq ny nz nd oa bi translated">对输入进行缩减采样</h2><p id="d3a1" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">大多数成功的网络使用224x224或256x256输入。如果您使用的数量超过这个数量，您可能会浪费资源。<em class="mm">少往往就是多</em>。</p><p id="7880" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图像是高度相关的。在像素(23，23)处发现的数据与在(24，23)或(23，22)处发现的数据几乎相同。随着分辨率的提高，相关性占主导地位。一张4K的脸并不比一张256x256的脸更有面子。它的所有毛孔都清晰可见。</p><p id="abb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除非您正在处理大海捞针的问题，否则请考虑将输入降采样到较低的分辨率。只要你，人类，还能认识到有什么要认识的，模型就没问题。</p><p id="9e9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">下采样可以无需训练:</strong>如果你的模型是完全卷积的，或者在变成完全连接的网络之前使用全局平均池，你可能会获得下采样的好处，而不必重新训练它。</p><h2 id="2d9d" class="np mo it bd mp nq nr dn mt ns nt dp mx li nu nv mz lm nw nx nb lq ny nz nd oa bi translated">对输出进行缩减采样</h2><p id="f66d" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">如果您正在处理图像到图像的问题，尤其是图像分割，分割很可能在较低的分辨率下完成。</p><p id="6a74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设您正在制作256x256分割图。如果你重新训练你的模型做128x128，它的性能可能会上升。然后，你只需要将其放大到256x256，并重新评估实际的影响。除了保留初始卷积和最终反卷积，您很有可能保持大约2%的原始精度。</p><p id="6d48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里要注意的是，尽管整体形状是正确的，但分割边界通常是嘈杂和错误的。以较低的分辨率+传统的放大是获得更平滑结果的简单方法，同时也节省了计算时间。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/c7f52a1045f7abd0579d6ea7e8141acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lJ9NgNnm8xZRfcea"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@kaizennguyen?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">改善Nguyễn </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="97e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">这篇文章的关键要点是我们都在浪费资源。神经网络不需要很大。如果你用力挤压，所有的低效都会消失。你所需要的就是剩下的多汁的网络。</p><p id="5012" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">抛开其他一切。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="9ea9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读:)</p></div></div>    
</body>
</html>