<html>
<head>
<title>Multivariate Linear Regression in Python Step by Step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中多元线性回归的逐步实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multivariate-linear-regression-in-python-step-by-step-128c2b127171?source=collection_archive---------1-----------------------#2020-06-25">https://towardsdatascience.com/multivariate-linear-regression-in-python-step-by-step-128c2b127171?source=collection_archive---------1-----------------------#2020-06-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d4ad43511230d30168a1b71da6f0ca17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X1fAggweW3_c91mX"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">迈克尔·泽兹奇在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="0e23" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">学习用 Python 从头开始开发任意数量变量的多元线性回归。</h2></div><p id="f553" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">线性回归可能是最简单的机器学习算法。它对初学者来说非常好，因为它使用简单的公式。所以，这对学习机器学习概念是有好处的。在这篇文章中，我将尝试一步一步地解释多元线性回归。</p><h2 id="9c51" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">概念和公式</h2><p id="acc6" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">线性回归使用我们在学校都学过的简单公式:</p><p id="cc63" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Y = C + AX</p><p id="5faf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">提醒一下，Y 是输出或因变量，X 是输入或自变量，A 是斜率，C 是截距。</p><p id="7edd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于线性回归，我们遵循相同公式的这些符号:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ms"><img src="../Images/145bb26141069e593a643392388e906a.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*BTWn8ybQLzPuyKNY5UXAuQ.png"/></div></div></figure><p id="5c87" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们有多个独立变量，线性回归的公式将如下所示:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/bb1b357500d666cd0238b264b3a5a018.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*Utt3IMq92MAvQvCPsc6pVQ.png"/></div></figure><p id="a712" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，‘h’被称为假设。这是预测的输出变量。<strong class="la jk">θ0 是偏置项</strong>，所有其他θ值是系数。它们在开始时是随机初始化的，然后用算法进行优化，使该公式能更接近地预测因变量。</p><h2 id="464a" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">成本函数和梯度下降</h2><p id="55dd" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">当θ值在开始时被初始化时，该公式不被训练来预测因变量。假设与原始输出变量“Y”相差甚远。这是估计所有训练数据的累积距离的公式:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/57230793ae1ea48dfb46190568e40759.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*IVsprhqnH6fOk8NDPH1rjQ.png"/></div></figure><p id="e34e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这被称为<strong class="la jk">成本函数</strong>。如果你注意到了，它从假设(预测输出)中减去 y(原始输出)，取平方省略负数，求和除以 2 乘以 m，这里 m 是训练数据的个数。你可能会看到，成本函数是原始产量和预测产量之间的差异。机器学习算法的思想是最小化成本函数，使得原始输出和预测输出之间的差异更接近。为此，我们需要优化θ值。</p><p id="a164" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是我们更新θ值的方法。我们取成本函数相对于每个θ值的偏导数，并从现有的θ值中减去该值，</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/bd55802759c8fe0ebbd8802995d81634.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*b2J7eo5-5cho3ry-OXKTaw.png"/></div></figure><p id="3035" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，α是学习率，它是一个常数。我没有给出所有θ值的相同公式。但是对于所有的θ值，公式都是一样的。微分后，公式为:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/e3b56e17100e53f3fe812ce2640b79b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*5yXQ1vCWOdjyhcsM-RXoPg.png"/></div></figure><p id="d92e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这叫做梯度下降。</p><h2 id="0b2f" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">逐步实现算法</h2><p id="7b20" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我要用的数据集来自 Andre Ng 在 Coursera 上的机器学习课程。我会在本页底部提供链接。请随意下载数据集并使用本教程进行练习。如果对你来说是新的，我鼓励你在阅读的时候用数据集来练习。这是理解它的唯一方法。</p><p id="582d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个数据集中，只有两个变量。但是我为任意数量的变量开发了算法。如果对 10 个变量或 20 个变量使用相同的算法，应该也可以。我会用 Python 中的 Numpy 和 Pandas 库。所有这些丰富的 Python 库使得机器学习算法变得更加容易。导入包和数据集:</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="202c" class="lu lv jj nc b gy ng nh l ni nj">import pandas as pd<br/>import numpy as np</span><span id="50fa" class="lu lv jj nc b gy nk nh l ni nj">df = pd.read_csv('ex1data2.txt', header = None)<br/>df.head()</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/ba4cba97b27b2185e414874a1160a6e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*o1vZyQqKYIoObynZWYDlZQ.png"/></div></figure><ol class=""><li id="43ac" class="nm nn jj la b lb lc le lf lh no ll np lp nq lt nr ns nt nu bi translated">为偏置项添加一列 1。我选择 1 是因为如果你把 1 乘以任何值，那个值都不会改变。</li></ol><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="5321" class="lu lv jj nc b gy ng nh l ni nj">df = pd.concat([pd.Series(1, index=df.index, name='00'), df], axis=1)<br/>df.head()</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/d8333c81edbfd7bc403471fc3ffaf0b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*szmJ14rtWyEmneBaNO4MgA.png"/></div></figure><p id="2f6f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.定义输入变量或自变量 X 和输出变量或因变量 y。在该数据集中，列 0 和 1 是输入变量，列 2 是输出变量。</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="8484" class="lu lv jj nc b gy ng nh l ni nj">X = df.drop(columns=2)<br/>y = df.iloc[:, 3]</span></pre><p id="223a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.通过将每列除以该列的最大值来规范化输入变量。这样，每列的值将在 0 到 1 之间。这一步不是必需的。但它使算法更快地达到最优。另外，如果您注意到数据集，与列 1 的元素相比，列 0 的元素太大了。如果对数据集进行规范化，就可以防止第一列在算法中过于突出。</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="679c" class="lu lv jj nc b gy ng nh l ni nj">for i in range(1, len(X.columns)):<br/>    X[i-1] = X[i-1]/np.max(X[i-1])<br/>X.head()</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/81e643f1c872be3c99ef05aba2ca8646.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*BKemwEiG7u0bLK5SEfrkZg.png"/></div></figure><p id="040e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.初始化θ值。我把它们初始化为 0。但是任何其他数字都可以。</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="2390" class="lu lv jj nc b gy ng nh l ni nj">theta = np.array([0]*len(X.columns))</span><span id="9d5e" class="lu lv jj nc b gy nk nh l ni nj">#Output: array([0, 0, 0])</span></pre><p id="df65" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">5.计算训练数据的数量，在上式中表示为 m:</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="3f7f" class="lu lv jj nc b gy ng nh l ni nj">m = len(df)</span></pre><p id="42cd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">6.定义假设函数</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="306a" class="lu lv jj nc b gy ng nh l ni nj">def hypothesis(theta, X):<br/>    return theta*X</span></pre><p id="7eba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">7.使用上面解释的成本函数的公式来定义成本函数</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="b191" class="lu lv jj nc b gy ng nh l ni nj">def computeCost(X, y, theta):<br/>    y1 = hypothesis(theta, X)<br/>    y1=np.sum(y1, axis=1)<br/>    return sum(np.sqrt((y1-y)**2))/(2*47)</span></pre><p id="24ae" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">8.写出梯度下降的函数。该函数将 X，y，theta，学习率(公式中的 alpha)和时期(或迭代次数)作为输入。我们需要不断更新θ值，直到成本函数达到最小值。</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="1d4e" class="lu lv jj nc b gy ng nh l ni nj">def gradientDescent(X, y, theta, alpha, i):<br/>    J = []  #cost function in each iterations<br/>    k = 0<br/>    while k &lt; i:        <br/>        y1 = hypothesis(theta, X)<br/>        y1 = np.sum(y1, axis=1)<br/>        for c in range(0, len(X.columns)):<br/>            theta[c] = theta[c] - alpha*(sum((y1-y)*X.iloc[:,c])/len(X))<br/>        j = computeCost(X, y, theta)<br/>        J.append(j)<br/>        k += 1<br/>    return J, j, theta</span></pre><p id="cd1d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">9.使用梯度下降函数获得最终成本、每次迭代中的成本列表以及优化的参数θ。我选择α为 0.05。但是你可以尝试使用其他值，比如 0.1，0.01，0.03，0.3，看看会发生什么。我反复运行了 10000 次。请尝试更多或更少的迭代，看看有什么不同。</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="33c9" class="lu lv jj nc b gy ng nh l ni nj">J, j, theta = gradientDescent(X, y, theta, 0.05, 10000)</span></pre><p id="37c8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">10.使用优化的 theta 预测输出</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="9440" class="lu lv jj nc b gy ng nh l ni nj">y_hat = hypothesis(theta, X)<br/>y_hat = np.sum(y_hat, axis=1)</span></pre><p id="2184" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">11.绘制原始 y 和预测输出“y_hat”</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="df7a" class="lu lv jj nc b gy ng nh l ni nj">%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>plt.figure()<br/>plt.scatter(x=list(range(0, 47)),y= y, color='blue')         <br/>plt.scatter(x=list(range(0, 47)), y=y_hat, color='black')<br/>plt.show()</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/ec743c0317058671bdc4a79668f6984b.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*FKNN4feCO7KqH0tGssKnQQ.png"/></div></figure><p id="6309" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一些输出点几乎与预测输出重叠。有些接近但不重叠。</p><p id="af74" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">12.绘制每次迭代的成本，以观察行为</p><pre class="mt mu mv mw gt nb nc nd ne aw nf bi"><span id="770c" class="lu lv jj nc b gy ng nh l ni nj">plt.figure()<br/>plt.scatter(x=list(range(0, 10000)), y=J)<br/>plt.show()</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f00e92e7ce3007e6ff1d67505682fc1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*XzxF57W0A3OLrXJ6G7kYFQ.png"/></div></figure><p id="da46" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">成本随着每次迭代而不断下降。这表明该算法运行良好。</p><p id="9b71" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望，这是有帮助的，你也在为自己尝试。如果你正在阅读这篇文章以学习机器学习概念，我鼓励你下载数据集并尝试自己运行所有代码。以下是数据集的链接:</p><div class="is it gp gr iu ny"><a href="https://github.com/rashida048/Machine-Learning-With-Python/blob/master/ex1data2.txt" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd jk gy z fp od fr fs oe fu fw ji bi translated">rashida 048/用 Python 进行机器学习</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">通过在 GitHub 上创建一个帐户，为 rashida 048/用 Python 进行机器学习开发做出贡献。</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">github.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om ja ny"/></div></div></a></div><p id="7ba7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望它有帮助。请随时在<a class="ae jg" href="https://twitter.com/rashida048" rel="noopener ugc nofollow" target="_blank">推特</a>上关注我，并喜欢我的<a class="ae jg" href="https://www.facebook.com/Regenerative-149425692134498/" rel="noopener ugc nofollow" target="_blank">脸书</a>页面。</p></div></div>    
</body>
</html>