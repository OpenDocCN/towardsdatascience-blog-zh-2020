<html>
<head>
<title>Word2Vec to Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec至变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word2vec-to-transformers-caf5a3daa08a?source=collection_archive---------3-----------------------#2020-01-07">https://towardsdatascience.com/word2vec-to-transformers-caf5a3daa08a?source=collection_archive---------3-----------------------#2020-01-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4243" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">单词嵌入的演变，来自CS224n的注释。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4e3dda49b21e139ee37acf44fb2c86e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DCJadgelhkx7L28Qb648PQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">你的论点是合理的，完全合理</p></figure><h2 id="fb00" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">介绍</h2><p id="327c" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">开发有意义的单词表示是NLP成立以来的主要目标之一。在2010年代，这一基础性任务一直是该领域进步和创新的主要驱动力之一。在这篇文章中，我将介绍完成这项任务的一些主要方法，以及大大提高我们在不同上下文中捕捉词义和相似性的能力的主要观点。</p><h2 id="6b85" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">一袋单词</h2><p id="462f" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">解决这个问题最简单的方法叫做词汇袋。这种方法为文本中出现的每个单词分配一个唯一的标记，通常是一个数字。因此，例如短语“你的论点是<em class="mn">声音</em>，除了<em class="mn">声音</em>什么都没有”将被表示为<em class="mn">“1-2-3-4-5-6-4”。</em>通过这种基线方法，我们可以捕捉到单词之间同一性的概念，也就是说，当一个单词被多次使用时，我们可以识别出来。此外，使用像Tf-Idf这样的使用单词包的技术，我们可以在一定程度上成功地测量文档之间的相似性，只要基于哪些单词在文档中使用以及使用的频率。使用像WordNet这样的资源，一个支持的字典，我们也可以发现文本中单词的多种含义，并将字典中列为同义词的那些连接起来。但是这种表示本身并没有捕捉到单词的相似性，也没有捕捉到单词在我们的文本中被使用的特定意义。</p><h2 id="67c0" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">Word2Vec (CBOW或Skip-Gram)</h2><p id="fa45" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">可以说，NLP在2010年代早期的最大发展是Word2Vec，这是一种无监督的学习技术，用于学习单词的连续表示。在许多方面，Word2Vec建立在BoW的基础上，但它不是为单词分配离散的标记，而是为训练语料库中的每个单词学习连续的多维向量表示。更具体地说，它通过学习预测给定一个中心单词在其周围固定大小的窗口中最有可能出现的单词来做到这一点<em class="mn"> (Skip-Gram) </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/c83b7f825875e7c23045bed44b6101f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1zorACun3FI0lJwoYCAYmA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Word2Vec示例</p></figure><p id="da16" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated">或者通过学习根据上下文单词<em class="mn">(连续单词袋)</em>预测中心词。像许多其他机器学习技术一样，Word2Vec使用梯度下降来最小化整个语料库的交叉熵损失，即预测错误单词的概率。实际上，基本思想是在中心词和外部词的任何给定配置下，最大化给定中心词时预测外部词的条件概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/f819c64822a5ab32a53b52f682e9d1a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tyxL-bX6Vyl67ZELg8toPQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表达式最大化，其中u_o是中心词上下文中外部词之一的向量表示，v_c是中心词的表示。直观上，向量积是线性代数中相似性的度量，因此我们想要最大化当前外部单词和中心单词之间的相似性，该相似性基于中心单词相对于语料库中所有单词的相似性的总和来归一化。指数是用来使一切都为正的。</p></figure><p id="3cb6" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated">现在，如果我们想在所有语料库中最大化这种表达，我们需要逐步学习更好地捕捉单词之间相似性的向量。因此，Word2Vec通过设计来捕捉我们语料库中单词的相似性，这要归功于一个单词在向量空间中与其他单词有多远或多近的概念，即词义的概念。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/f6814dab973d9ef047b9d1e0277ed0e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*sZ17cYbnzFt8nR-aITZI5w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">经典的国王-男人+女人=女王Word2Vec如何捕捉相似性的直觉。</p></figure><p id="c1f9" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated">然而，这种相似性的概念只能带我们到此为止。Word2Vec的主要问题是，它为一个词提供了一种单一的表示，无论上下文如何，这个词都是相同的。因此，像“<em class="mn">bank”</em>这样有几个不同含义的词，例如river bank和investment bank，最终会有一个表示，它是没有很好地表示任何一个含义的平均表示。</p><h2 id="9dac" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">上下文单词嵌入(ELMo)</h2><p id="898e" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">基于单词出现的上下文来证明每个单词有多个表示是上下文单词嵌入背后的核心思想。这个想法是使用RNN语言模型来实现的，该模型以无监督的方式类似于Word2Vec来训练。更具体地，我们使用RNNs来预测，给定句子中的当前单词，下一个单词。我们使用这些网络是因为它们有能力在隐藏状态下捕获和维护长期依赖关系。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/6f951a54ff7aabd2cba980d910a01836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QA9_84dKqOfYIKKl9buULQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">隐藏状态(红色)可以保持关于句子中前一个单词的信息</p></figure><p id="398c" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated">其思想是，在输入当前单词后，我们可以将隐藏状态连接到通常的Word2Vec表示，以维护当前单词和过去上下文的信息。</p><p id="0793" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated">第一个实现这个想法的系统是Peters和co .的TagLM。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/7b4c8e7a0d47f350358ffe2e922e753d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieq-QLfyWVwX1tA384dwCw.png"/></div></div></figure><p id="7650" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated">TagLM使用预训练的双LSTM语言模型来产生单词嵌入的“上下文部分”,该单词嵌入连接到Word2Vec矢量或更复杂的字符级CNN/RNN生成的单词表示。这种表示现在是新的嵌入，有效地取代了NLP管道中的Word2Vec或GloVe向量。</p><p id="13ba" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated">ELMo嵌入的工作方式非常相似，主要区别在于ELMo对预训练语言模型使用两层双LSTM，而嵌入连接是一种可学习的，在微调期间，两层的组合将针对特定任务进行优化。ELMo还完全抛弃了Word2Vec，只依赖字符级的CNN/RNNs作为单词表示的第一部分。</p><h2 id="7776" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">变形金刚(伯特，GPT)</h2><p id="5b4b" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">训练单独的语言模型以产生更好的上下文单词表示的想法已经被证明在许多NLP任务中非常成功地改善了SOTA，但是RNN语言模型由于其循环、顺序的性质而倾向于训练缓慢并且非常难以并行化。因此，在2017年，Aswani和他的同事开发了一种非递归替代RNNs的方法，其核心是变压器块。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/92d7e89b4da7ff0199be3de0cdef553f.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*KiKLNyV8DIb2dKuVHZMTfg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">变压器架构的编码器</p></figure><p id="8e72" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated">Transformer的主要特点是它使用注意力(seq2seq架构中有助于翻译对齐的概念)来捕捉句子中单词之间的关系，类似于卷积的方式。就像卷积一样，我们可以让多个注意力头来计算每个单词应该将注意力集中在哪里，以及注意力所代表的关系。然而，注意力不同于卷积，因为它捕捉空间中单词之间的相似性，在该空间中，不同头部的权重矩阵投射它们的表示。为了捕捉更远的依赖性，类似于卷积，我们可以堆叠多个变压器块。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/43cfd2a077507c32c4c81f22b76f86c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*y1G8PBVsGFtJi4V-gpO29Q.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/48913bfa6bd2948ec22b07a6eb821356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJ4BHZWs25-LPwGgg-zzcg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/01199519ace129658beb195ff88fc361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*haWnpdbV4TqoTFpwNbVL9A.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/2611ffd279d33a8dbf7d72f055575473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*6kyv2XajOX1kouVqYYWtMg.png"/></div></figure><p id="ae82" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated">解码器模块的工作方式有些不同，但是你所需要的是引起<em class="mn">注意的图像</em>纸张让一切变得更加清晰。</p><p id="ae1b" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated">BERT使用transformer block来训练一个使用屏蔽技术的语言模型，其中系统的任务不是猜测下一个单词，而是猜测句子中屏蔽掉的一个单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/348826a4e6c20234c3b06f2beff5ada5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qu5lB7mqUyejj9msDZQl9g.png"/></div></div></figure><p id="1804" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated">这样，它能够使用整个上下文进行预测，而不仅仅是左上下文。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="9576" class="pw-post-body-paragraph lu lv it lw b lx mp ju lz ma mq jx mc lh mr me mf ll ms mh mi lp mt mk ml mm im bi translated"><em class="mn">这些笔记是对CS224n Stanford课堂第13和14课的最基本的总结。这些材料的学分属于克里斯·曼宁教授和这门课的助教。<br/>更多资料可在</em> <a class="ae nl" href="http://web.stanford.edu/class/cs224n/" rel="noopener ugc nofollow" target="_blank"> <em class="mn">课程网站</em> </a> <em class="mn">和</em><a class="ae nl" href="https://www.youtube.com/watch?v=5vcj8kSwBCY&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank"><em class="mn">YouTube</em></a><em class="mn">上获得。</em></p></div></div>    
</body>
</html>