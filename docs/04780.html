<html>
<head>
<title>Day 117 of #NLP365: NLP Papers Summary — Abstract Text Summarization: A Low Resource Challenge</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365 的第 117 天:NLP 论文摘要-摘要文本摘要:低资源挑战</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-117-of-nlp365-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge-61ae6cdf32f?source=collection_archive---------73-----------------------#2020-04-26">https://towardsdatascience.com/day-117-of-nlp365-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge-61ae6cdf32f?source=collection_archive---------73-----------------------#2020-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="2e67" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="932a" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">NLP 论文摘要是我总结 NLP 研究论文要点的系列文章</h2></div><p id="87d3" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我在 2020 年每天记录我的 NLP 学习旅程的地方。在这里，你可以随意查看我在过去的 257 天里学到了什么。在本文的最后，你可以找到以前的论文摘要，按自然语言处理领域分类:)</p><p id="50bf" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的 NLP 论文是<strong class="lf jp"> <em class="lz">摘要文本摘要:低资源挑战</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="fb07" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="642e" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">提出了一种迭代数据扩充技术，使用真实的德语汇总数据生成合成数据。随后，他们使用 Transformer 为德语建立了一个抽象概括模型。本文解决了其他非英语语言的 NLP 任务中存在的低资源挑战。当与没有数据扩充而训练的模型相比时，数据扩充提高了抽象概括模型的性能。</p><h1 id="27f6" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">方法学</h1><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/f005a9ece5ad1d5ac8c96fedeefdd781.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/0*sNUlCTyWerkvbPtT.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">数据集的统计数据[1]</p></figure><p id="b57c" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">转换器模型是使用 OpenNMT-py 实现的。上图展示了我们数据集汇总统计数据。我们使用了两个德国维基数据集，即 SwissText 2019 和 Common Crawl。SwissText 2019 用作真实数据，而 Common Crawl 用作合成数据。真实数据 SwissText 2019 (100K 数据)分为 train、val、test 比例为 90:5:5。对于通用爬网，以下是生成合成数据的以下步骤:</p><ol class=""><li id="608f" class="no np jf lf b lg lh lj lk lm nq lq nr lu ns ly nt nu nv nw bi translated">使用 SwissText 数据集构建最常用德语词汇</li><li id="1719" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">基于词汇和阈值的通用爬行数据集中的句子选择。例如，一个句子有 20 个单词，阈值是 10%，只有当它在我们的词汇表中至少有 20 个单词时，才会被选择</li><li id="fda9" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">从步骤 2 中随机选择句子</li><li id="0d61" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">100K 个选择的句子被用作摘要，我们需要建立一个模型来生成相应的输入文本。因此，我们使用摘要作为输入，目标是文本。这是反向训练的模型，如下所示。最终的总数据集为 190K</li></ol><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e932ce7199a3a4ade568f1ffba3e6034.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/0*xWkBUsIomm1jFPkN.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">使用逆向系统生成合成数据[1]</p></figure><p id="f9b5" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后，为了提高合成数据的质量，我们使用了如下所示的迭代方法。我们将首先使用真实的和合成的数据来训练我们的变压器模型。然后，我们将使用经过训练的变压器模型来重新生成我们的合成数据，以训练我们的最终变压器模型。</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/8d1f19657002bec923cffebe518c0521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/0*wOlCeb01WFwtRh0p.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">合成数据再生的最终过程[1]</p></figure><h1 id="e75e" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">实验设置和结果</h1><p id="5d5f" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">有如下三种实验设置:</p><ol class=""><li id="1d44" class="no np jf lf b lg lh lj lk lm nq lq nr lu ns ly nt nu nv nw bi translated"><em class="lz"> S1 </em>。仅使用真实数据(90K)来训练我们的变压器模型。这是基线</li><li id="dd72" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><em class="lz"> S2 </em>。使用真实和合成数据(190K)训练我们的变压器模型</li><li id="ef8d" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><em class="lz"> S3 </em>。使用真实的和再生的合成数据来训练我们的变压器模型</li></ol><h2 id="1c52" class="oe mi jf bd mj of og dn mn oh oi dp mr lm oj ok mt lq ol om mv lu on oo mx jl bi translated">结果</h2><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/b6dfd52c3073fd394b4ab0a509d46439.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/0*DTA26uX8wPRFzyFY.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">开发和测试集的结果[1]</p></figure><p id="120c" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">尽管努力提高 S3 合成数据的质量，S2 模型表现最好。如下图所示，ROUGE score 的开发在早期迭代中达到了顶峰，展示了通过更快的训练过程可以达到良好的结果。与 S1 相比，S2 模型在单词和短语生成以及摘要长度方面往往有更多的差异。S2 的平均摘要长度是 41.42，而 S1 是 39.81。</p><figure class="nk nl nm nn gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/799a45d842d189268771f68d38ee245e.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/0*x2ZKrOQgwpps_pWU.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">ROUGE-1 学习曲线[1]</p></figure><h1 id="a268" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">结论和未来工作</h1><p id="74fb" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">潜在的未来工作可能涉及对合成摘要数据的进一步调查，并利用迁移学习对具有低资源数据的非英语语言进行文本摘要。</p><h2 id="951a" class="oe mi jf bd mj of og dn mn oh oi dp mr lm oj ok mt lq ol om mv lu on oo mx jl bi translated">来源:</h2><p id="c3ef" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">[1]帕里达，s .和莫特利切克，p . 2019，11 月。低资源挑战。在<em class="lz">2019 自然语言处理经验方法会议暨第九届国际自然语言处理联合会议(EMNLP-IJCNLP) </em>(第 5996–6000 页)。</p><p id="8b79" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于 2020 年 4 月 26 日 https://ryanong.co.uk</em><em class="lz">的</em> <a class="ae jc" href="https://ryanong.co.uk/2020/04/26/day-117-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge/" rel="noopener ugc nofollow" target="_blank"> <em class="lz">。</em></a></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="6cc2" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="955e" class="no np jf lf b lg mz lj na lm oq lq or lu os ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="75ae" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sensation-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="277e" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitive-analysis-f 24 a2 EC 1 ca 32</a></li><li id="b4a6" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="3b6b" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towardsdatascience . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li><li id="ebbc" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towardsdatascience . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li><li id="9c8a" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5">https://towards data science . com/day-112-of-NLP 365-NLP-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b 7 a5 e 245 b5</a></li></ul><h1 id="4df8" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">总结</h1><ul class=""><li id="8b01" class="no np jf lf b lg mz lj na lm oq lq or lu os ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="38eb" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1 b 27</a></li><li id="70cf" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-113-of-nlp365-nlp-papers-summary-on-extractive-and-abstractive-neural-document-87168b7e90bc">https://towards data science . com/day-113-of-NLP 365-NLP-papers-summary-on-extractive-and-abstract-neural-document-87168 b 7 e 90 BC</a></li><li id="4480" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-116-of-nlp365-nlp-papers-summary-data-driven-summarization-of-scientific-articles-3fba016c733b">https://towards data science . com/day-116-of-NLP 365-NLP-papers-summary-data-driven-summary-of-scientific-articles-3 FBA 016 c 733 b</a></li></ul><h1 id="310d" class="mh mi jf bd mj mk ne mm mn mo nf mq mr ku ng kv mt kx nh ky mv la ni lb mx my bi translated">其他人</h1><ul class=""><li id="ee00" class="no np jf lf b lg mz lj na lm oq lq or lu os ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li><li id="3187" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-111-of-nlp365-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection-bff7f5f20ce5">https://towards data science . com/day-111-of-NLP 365-NLP-papers-summary-the-risk-of-race-of-bias-in-hate-speech-detection-BFF 7 F5 f 20 ce 5</a></li><li id="a5ec" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly ot nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-115-of-nlp365-nlp-papers-summary-scibert-a-pretrained-language-model-for-scientific-text-185785598e33">https://towards data science . com/day-115-of-NLP 365-NLP-papers-summary-scibert-a-pre trained-language-model-for-scientific-text-185785598 e33</a></li></ul></div></div>    
</body>
</html>