<html>
<head>
<title>Transfer Learning with Fruit Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">水果分类的迁移学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transfer-learning-with-fruit-classification-a7a38e3f93b4?source=collection_archive---------23-----------------------#2020-05-08">https://towardsdatascience.com/transfer-learning-with-fruit-classification-a7a38e3f93b4?source=collection_archive---------23-----------------------#2020-05-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fa8e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">迁移学习的简要概述，以及如何利用预训练模型通过InceptionV3进行深度学习的示例。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/1891ed58b240e16afe43746c0fcf42c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*AWHmuP2UTHHc3_1_x4W9Hw.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来自Freerange的Jack Moreh的“一个假想的人工智能大脑的图像”</p></figure><p id="47b4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi ln translated">或者你们当中不知道什么是深度学习的人，它是机器学习中的一种类型，属于<em class="lw">人工智能的范畴。这种类型的技术不是我<a class="ae lx" href="https://en.wikipedia.org/wiki/I,_Robot_(film)" rel="noopener ugc nofollow" target="_blank">，机器人</a>人工智能，我们作为一个物种，离真正开发这样的东西还很远。然而，当我们谈论像人类一样行动的可编程计算机时，深度学习和许多其他类型的机器学习方法非常接近。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/67260151f06fe349314f1cd29dd9fd07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*ZWrGQKNGBrb-EY5l8lWMHg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">“神经网络图”</p></figure><h2 id="799b" class="lz ma iq bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq mr bi translated">深度学习</h2><p id="715f" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated">简而言之，深度学习是一种机器学习方法，它超越了可能只需要一两层学习的更小的学习方法，这可能被称为<em class="lw">浅层学习。</em>但是，深度学习是一层层的学习。代表这些层的模型被称为<em class="lw">神经网络</em>，其名称源于对<em class="lw">神经生物学</em>的研究，但不要混淆，因为它实际上不是一个模拟大脑功能的网络。神经网络有许多不同的形式，但我们将只关注我们的迁移学习中的一种，即<em class="lw">卷积神经网络。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/97b0afcdda14b34ea6c0f7c0ace7b854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrHUGAbr-UH7tCVydpDFZg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">“卷积过程”</p></figure><blockquote class="nc"><p id="16e5" class="nd ne iq bd nf ng nh ni nj nk nl lm dk translated">那么什么是卷积神经网络呢？</p></blockquote><p id="13ef" class="pw-post-body-paragraph kr ks iq kt b ku nm jr kw kx nn ju kz la no lc ld le np lg lh li nq lk ll lm ij bi translated">卷积神经网络只是训练网络(模型)以给出精确分类的另一种方法。在所有其他神经网络中，卷积神经网络在计算机视觉学习方面表现出色。CNN或covnets的惊人之处在于，它们从图像中学习的模式是平移不变的，这意味着如果它们在图像的角落中找到一个模式，它们就会在任何不同的图像角落中识别出相同的模式，而常规网络必须一次又一次地重新学习它。Covnet还能够学习模式的空间层次，这意味着cov net的每一层都将学习不同的东西。第一层可以学习小模式，下一层可以学习作为第一层特征的较大模式。这些特征是如何通过<em class="lw">卷积函数获得的。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/a9e83c181d4c25c4af2100de135d4b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/1*BKEsVKwrVt1yqmXDfb6BbA.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">“卷积运动的gif图”，Narges Khatami，来自<a class="ae lx" href="https://commons.wikimedia.org/wiki/File:Valid-padding-convolution.gif" rel="noopener ugc nofollow" target="_blank">维基共享资源</a></p></figure><h2 id="305c" class="lz ma iq bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq mr bi translated">卷积</h2><p id="0eab" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated">卷积函数用于获得卷积层的特征图(特征矩阵)。在基线上，covnets已经配置了由一个<em class="lw">内核</em>组成的权重。核用于从输入图像(输入层)中获得独特的特征，就像它可以用于收集输入图像的锐度、边缘或收集关于如何检测边缘的信息一样。这个函数可以表示为n*n，这是一个包含许多唯一值的矩阵。内核<em class="lw">在输入图像的顶部进行</em>(滑动和相乘)，假设输入图像是(10，10)，内核是(3，3)。第一张幻灯片(stride)将在输入图像的左上角乘以9个像素，以在称为特征图的新矩阵的左上角产生单个像素的输出。</p><p id="b6c9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">更新:第二次乘法应该是4 * 2 = 8，用8代替1的乘积，得到所有乘积的和。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ns"><img src="../Images/ea2ac1c4e9c121ebc3cbe0411df8623f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jkyvlYuvjSThOd8yJ0R2Fg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">由<a class="ae lx" rel="noopener" target="_blank" href="/convolution-neural-networks-a-beginners-guide-implementing-a-mnist-hand-written-digit-8aa60330d022"> Krut Patel </a>从<a class="ae lx" href="https://towardsdatascience.com/" rel="noopener" target="_blank">走向数据科学</a>2019；詹姆斯·纳尔逊注释，2020年。</p></figure><p id="5b69" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当内核像这样滑过输入图像时，这种乘法继续进行。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nt"><img src="../Images/e0c66add20cd31afcb98c4e6919d7584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_9O4RI6m_Vz3npYxUAiNVA.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">由<a class="ae lx" rel="noopener" target="_blank" href="/convolution-neural-networks-a-beginners-guide-implementing-a-mnist-hand-written-digit-8aa60330d022"> Krut Patel </a>从<a class="ae lx" href="https://towardsdatascience.com/" rel="noopener" target="_blank">走向数据科学</a>2019；詹姆斯·纳尔逊注释，2020年。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nu"><img src="../Images/03ececed56b9560248a285a5fddf8dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6jAdckfqTCU0je6eqlR_qQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">由<a class="ae lx" rel="noopener" target="_blank" href="/convolution-neural-networks-a-beginners-guide-implementing-a-mnist-hand-written-digit-8aa60330d022"> Krut Patel </a>从<a class="ae lx" href="https://towardsdatascience.com/" rel="noopener" target="_blank">走向数据科学</a>2019；詹姆斯·纳尔逊注释，2020年。</p></figure><p id="a1e2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这个<em class="lw">连续的</em>过程不会停止，直到整个特征矩阵已经被这些卷积值填充，并且一旦特征矩阵完成，它就被堆叠在卷积层内。如果网络被设计成这样，则另一个内核将产生具有相同输入图像的另一个特征矩阵，以将下一个特征矩阵存储在相同的卷积层中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/bb61536af18aaeabd249800e28592d3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*qXPmORzdoMnCQSmT4xRl1g.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">“制作特征矩阵时会发生什么”</p></figure><blockquote class="nw nx ny"><p id="76f3" class="kr ks lw kt b ku kv jr kw kx ky ju kz nz lb lc ld oa lf lg lh ob lj lk ll lm ij bi translated">简而言之，卷积神经网络的训练是关于定位每个核的所有正确值，以便当输入图像通过各层时，它将激活最后一个输出层上的不同神经元，以预测和准确分类图像。</p></blockquote><h2 id="8a46" class="lz ma iq bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq mr bi translated">什么是迁移学习，它有什么帮助？</h2><p id="81c4" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated">迁移学习让每个人的生活变得更轻松、更美好。尽管从头开始创建卷积神经网络很有趣，但它们可能有点贵并且耗费大量<em class="lw">计算</em>T4】能力。因此，为了减少网络所需的功率量，我们使用转移学习，这是已经在另一个图像上经过训练的<em class="lw">预训练</em>权重，以便提高我们网络的性能。使用预训练模型是一个最佳选择，因为它们已经在数以百万计的其他图像上进行配置和训练，这些图像由数千个类别组成，每次持续多天，以提供我们所需的高性能预训练权重，以便轻松训练我们自己的网络(Aditya Ananthram，2018)。</p><h1 id="daba" class="oc ma iq bd mb od oe of me og oh oi mh jw oj jx mk jz ok ka mn kc ol kd mq om bi translated">实际应用</h1><p id="94c5" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated">现在，为了展示迁移学习能力的实际应用，我将介绍所使用的数据、选择的预训练模型、模型架构，然后是代码。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/83520e40e7f2d95eee6a640b4162cf89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*TycUEfvvb7qjCgARaX3L_w.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">“意大利的一个农贸市场替身”，作者是散养农场的梅里泽</p></figure><h2 id="4589" class="lz ma iq bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq mr bi translated">数据的描述</h2><p id="c304" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated"><a class="ae lx" href="https://www.kaggle.com/moltean/fruits" rel="noopener ugc nofollow" target="_blank">数据集</a>包含81，104张不同水果和蔬菜的图像，由每张水果和蔬菜图像的120个唯一分类组成。图像的总数被分成训练和测试数据集。训练数据集包含60，486幅图像，测试数据集是20，618幅图像。</p><p id="9d6f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">所有图像的大小都是100x100像素，是用罗技C920相机收集的，该相机用于拍摄水果/蔬菜(Mihai Oltean，2019)。所有的水果和蔬菜都被种植在一个装有低速马达的轴内，在那里它们被记录下来，每一个持续时间为20秒。水果和蔬菜测试图像是用5X智能手机拍摄的。</p><h2 id="1e4a" class="lz ma iq bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq mr bi translated">模型</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi on"><img src="../Images/a875c97249aba9b3c9b4b62575ffe1d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QV1SXvBI_aaY3Uzyyx_OeQ.jpeg"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">“一个‘图腾’旋转的图像。你还在做梦吗？”照片由<a class="ae lx" href="https://unsplash.com/@modernafflatusphotography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">现代灵感的灰烬</a>在<a class="ae lx" href="https://unsplash.com/@modernafflatusphotography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="1217" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">选择的迁移学习模型叫做<a class="ae lx" href="https://www.mathworks.com/help/deeplearning/ref/inceptionv3.html" rel="noopener ugc nofollow" target="_blank"> InceptionV3 </a>。该模型是一个卷积神经网络，在架构上设计为48层深度，对299×299的图像形状进行训练。最初的Inception架构网络被称为“GoogLeNet”，这是一个27层深度卷积神经网络，早在2014年就已建立(Shaikh，2018)。该模型的名称来自克里斯托弗·诺兰导演的电影“盗梦空间”，基于深入梦境的概念“梦中的梦”，转化为卷积神经网络中的卷积神经网络。</p><p id="2bff" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">GoogLeNet设计背后的想法是消除在处理更深层次的神经网络时经常发现的过度拟合问题。当数据集太小并且正在大型神经网络中进行训练时，通常会发生过度拟合，过度拟合带来的问题是对模型的验证准确性(测试准确性)的错误陈述。测试准确性是衡量训练好的网络准确预测它没有看到的图像的精确度。设计一个巨大的网络来产生这种准确性的解决方案是创建一个稀疏连接的神经网络来代替完全连接的神经网络(Shaikh，2018)，这就是为什么GoogLeNet模型在2014年以80%以上的预测准确性赢得了ImageNet视觉识别挑战。</p><h2 id="cfa9" class="lz ma iq bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq mr bi translated">模型架构</h2><p id="a320" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated">InceptionV3模型连接到底部的两个完全连接的层，但在此连接之前，其维度从3D减少到1D，具有<a class="ae lx" href="https://tensorspace.org/html/docs/layerGlobalPooling2d.html" rel="noopener ugc nofollow" target="_blank">全球平均池2D </a>。汇集还将为每个特征矩阵输出一个响应。在汇集之后，架构的下一层是具有512个单元(神经元)的第一密集隐藏层，其将连接到具有10个神经元的最终输出层，以匹配水果和蔬菜类的数量。这就是InceptionV3架构的样子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi oo"><img src="../Images/5ee07528e1fdb985dfc34bbe37f5c256.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*81Oic-X6ZFZqxye6Yj4ocg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">英特尔<a class="ae lx" href="https://www.intel.com/content/www/us/en/homepage.html" rel="noopener ugc nofollow" target="_blank">的</a><a class="ae lx" href="https://software.intel.com/en-us/articles/inception-v3-deep-convolutional-architecture-for-classifying-acute-myeloidlymphoblastic" rel="noopener ugc nofollow" target="_blank"> Milton-Barker Adam </a>的“预训练InceptionV3模型的架构设计”</p></figure><p id="eb59" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是连接到架构的底层全连接层的样子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi op"><img src="../Images/6345e04235ba0e5d0b70c35370c0feb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MWK73hgOwxkGwRo420PcYg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">“附加到完全连接的层的InceptionV3模型的底层架构布局”</p></figure><p id="d2db" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">还值得一提的是，微调这些预先训练的模型以及与它们相关联的权重。我可以从模型中选择我想要使用的权重，它可以是上半部分、下半部分、中间部分，或者我可以冻结所有权重。这样做意味着我冻结的预训练模型的任何部分都不会是可以为我正在制作的模型更新的可训练权重。我还可以选择模型被训练的图像的权重，但是在这个例子中，通过反复试验，我选择不冻结权重。我对InceptionV3的实现将使用在<a class="ae lx" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>上预先训练的权重。“ImageNet是一个根据<a class="ae lx" href="http://wordnet.princeton.edu/" rel="noopener ugc nofollow" target="_blank"> WordNet </a>层次结构(目前只有名词)组织的图像数据库，其中层次结构的每个节点都由成百上千的图像描述。目前，我们平均每个节点有超过500张图像。”(《影像网》，2017年)</p><h1 id="769a" class="oc ma iq bd mb od oe of me og oh oi mh jw oj jx mk jz ok ka mn kc ol kd mq om bi translated">最后是代码</h1><p id="d196" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated">现在，您已经对数据集的外观和模型架构有了一个概念，是时候执行了。</p><h2 id="f47e" class="lz ma iq bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq mr bi translated">准备数据和训练网络</h2><p id="702f" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated"><strong class="kt ir">加载库。</strong></p><p id="a390" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">先做最重要的事情；我们必须加载必要的库。加载库时，确保导入所有必需的模块，以便我们可以准备数据和训练模型。</p><pre class="kg kh ki kj gt oq or os ot aw ou bi"><span id="6665" class="lz ma iq or b gy ov ow l ox oy"><em class="lw"># read in libraries</em><br/>import tensorflow as tf<br/>from tensorflow.keras import backend, models, layers, optimizers<br/>import numpy as np<br/>from tensorflow.keras.layers import GlobalAveragePooling2D<br/>from tensorflow.keras.callbacks import EarlyStopping<br/>from sklearn.model_selection import train_test_split<br/>from tensorflow.keras.utils import plot_model<br/>from IPython.display import display<br/>from PIL import Image<br/>from tensorflow.keras.preprocessing.image import ImageDataGenerator<br/>import os, shutil<br/>from tensorflow.keras.models import Model<br/>np.random.seed(42)</span></pre><p id="9735" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">加载数据并准备好。</strong></p><p id="60ea" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">接下来，为了准备数据，我们需要用ImageDataGenerator设置一个train_datagen和test_datagen。然后，使用这些生成器调整训练数据和测试数据的图像大小，以匹配预训练模型的像素图像输入。以确保神经网络不会学习不相关的模式，并反过来提高整体性能。</p><pre class="kg kh ki kj gt oq or os ot aw ou bi"><span id="173d" class="lz ma iq or b gy ov ow l ox oy"><em class="lw"># Specify the base directory where images are located.</em><br/>base_dir = '/kaggle/input/fruits/fruits-360/'<br/></span><span id="bece" class="lz ma iq or b gy oz ow l ox oy"><em class="lw"># Specify the traning, validation, and test dirrectories.  </em><br/>train_dir = os.path.join(base_dir, 'Training')<br/>test_dir = os.path.join(base_dir, 'Test')</span><span id="fa9c" class="lz ma iq or b gy oz ow l ox oy"><em class="lw"># Normalize the pixels in the train data images, resize and augment the data.</em><br/>train_datagen = ImageDataGenerator(<br/>    rescale=1./255,<em class="lw"># The image augmentaion function in Keras</em><br/>    shear_range=0.2,<br/>    zoom_range=0.2, <em class="lw"># Zoom in on image by 20%</em><br/>    horizontal_flip=True) <em class="lw"># Flip image horizontally </em></span><span id="0705" class="lz ma iq or b gy oz ow l ox oy"><em class="lw"># Normalize the test data imagees, resize them but don't augment them</em><br/>test_datagen = ImageDataGenerator(rescale=1./255) <br/></span><span id="b57a" class="lz ma iq or b gy oz ow l ox oy">train_generator = train_datagen.flow_from_directory(<br/>    train_dir,<br/>    target_size=(299, 299),<br/>    batch_size=16,<br/>    class_mode='categorical')</span><span id="bfb7" class="lz ma iq or b gy oz ow l ox oy">test_generator = test_datagen.flow_from_directory(<br/>    test_dir,<br/>    target_size=(299, 299),<br/>    batch_size=16,<br/>    class_mode='categorical')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pa"><img src="../Images/0df485489719effd2565f6dcc28c3d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xKJ2D857SjzbYCT6ON7kPg.png"/></div></div></figure><p id="9b64" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">准备InceptionV3模型</strong></p><p id="b0b8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">既然已经准备好了图像，现在是时候导入并设置迁移学习的预训练InceptionV3模型了。</p><pre class="kg kh ki kj gt oq or os ot aw ou bi"><span id="ca7a" class="lz ma iq or b gy ov ow l ox oy"><em class="lw"># Load InceptionV3 library</em><br/>from tensorflow.keras.applications.inception_v3 import InceptionV3</span><span id="ea7c" class="lz ma iq or b gy oz ow l ox oy"><em class="lw"># Always clear the backend before training a model</em><br/>backend.clear_session()</span><span id="a295" class="lz ma iq or b gy oz ow l ox oy"><em class="lw"># InceptionV3 model and use the weights from imagenet</em><br/>conv_base = InceptionV3(weights = 'imagenet', <em class="lw">#Useing the inception_v3 CNN that was trained on ImageNet data.  </em><br/>                  include_top = False)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pb"><img src="../Images/80da965ff6bbb8d28c0d9c103005a839.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PnGxKw2xBZCIawY4d92dQw.png"/></div></div></figure><p id="c0c7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">创建一个功能API模型。</strong></p><p id="7733" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，让我们将预训练的InceptionV3模型权重与密集层(完全连接的层)相结合，并减少两者之间的模型维度。</p><pre class="kg kh ki kj gt oq or os ot aw ou bi"><span id="3975" class="lz ma iq or b gy ov ow l ox oy"><em class="lw"># Connect the InceptionV3 output to the fully connected layers</em><br/>InceptionV3_model = conv_base.output<br/>pool = GlobalAveragePooling2D()(InceptionV3_model)<br/>dense_1 = layers.Dense(512, activation = 'relu')(pool)<br/>output = layers.Dense(120, activation = 'softmax')(dense_1)</span></pre><p id="91c7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">显示功能API模型。</strong></p><p id="36a0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了了解模型架构，我们可以将功能API模型作为一个整体来显示，以直观地看到网络的深度。</p><pre class="kg kh ki kj gt oq or os ot aw ou bi"><span id="f272" class="lz ma iq or b gy ov ow l ox oy"><em class="lw"># Create an example of the Archictecture to plot on a graph</em><br/>model_example = models.Model(inputs=conv_base.input, outputs=output)<br/><em class="lw"># plot graph</em><br/>plot_model(model_example)</span></pre><p id="926f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">(模型太大，无法在介质上显示，请点击此<a class="ae lx" href="https://www.kaggleusercontent.com/kf/33118903/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mgX558CwYG9r4t0UjFitlg.esQoE_Dfd7XoiQf77hP6ehN4l09q32dN7OKxnBEwLgFp3oTzbZFBOFgaIg-tcaEQpmmNM2ohDUmULXanHFDntIvPpj4tVjfL5YN1Njj2RqGmMEnORKhAKuRxci7hU73jdjzqmXW7Tcbclf4ML3NgZLCxrj84LNNP-ss8-vHiK-ZW018mzpughC89AdIuv2l8gd2_KisAAvsscU5EAb31E9v73Sm634BfyHmoc88k6DGJieDR4POQ0iODF-GISqnnayFbnKqv6Ph_DJa9_8Ck6Ky4KBYjCiCHNV_pdn6oIz35VCxNbvZgZjFlGQvqAEUffKxeuUslcSfQIwOtbw1OqMidPv6mWxSYe1E82aHIwdItGHdbkN_ITV891orlGhS0g4wkbaY4wqP7IxZyNXhGZz_59Mzh9-F3f3XoDmrGBQVQ-0JV1b_HM_S4TMmGqkpga-LGF8P2tfG1uZUdgl8Fei2spvlpNVTr4F4-HHfBFPO_dX3BDdA0JAGhmXF91jmYOorvdQG89JgqwoW-5bwCOjGU-XiBogu0B2qhmBup_v-aP76ob_NQBy4fWwcl39YnG_nPgOuvmLZVJBPmmvQFSyB0y06rbl4lZkT0B9E2lKAEaX_wgf7V9k1i19EM8ndPrHaHfSoljCpEEhqf9fwztvqg5VprUrCOG7YVHrAqie8.nNrlLJTieo9wt9JBoUHMog/__results___files/__results___15_0.png" rel="noopener ugc nofollow" target="_blank">链接</a>查看)</p><p id="b286" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">定义模型并编译它。</strong></p><p id="6fe6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了让我们训练模型，我们需要定义函数式API模型，并使用类别交叉熵作为损失函数以及具有学习速率和动量参数的随机梯度下降来编译该模型。</p><pre class="kg kh ki kj gt oq or os ot aw ou bi"><span id="4819" class="lz ma iq or b gy ov ow l ox oy"><em class="lw"># Define/Create the model for training</em><br/>model_InceptionV3 = models.Model(inputs=conv_base.input, outputs=output)</span><span id="257a" class="lz ma iq or b gy oz ow l ox oy"><em class="lw"># Compile the model with categorical crossentropy for the loss function and SGD for the optimizer with the learning</em><br/><em class="lw"># rate at 1e-4 and momentum at 0.9</em><br/>model_InceptionV3.compile(loss='categorical_crossentropy',<br/>              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),<br/>              metrics=['accuracy'])</span></pre><p id="5397" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">查看GPU要使用的设备列表。</strong></p><p id="3d42" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在我建议使用GPU来训练这个模型，因为InceptionV3模型有超过2100万个参数，在CPU上训练可能需要几天才能完成。如果你有一个GPU，你可以使用你自己的，但我使用了Kaggle的GPU提供给他们的笔记本电脑，这花了我大约20-25分钟来完成培训。找到可用的GPU设备，以便加快训练过程。</p><pre class="kg kh ki kj gt oq or os ot aw ou bi"><span id="18e7" class="lz ma iq or b gy ov ow l ox oy"><em class="lw"># Import from tensorflow the module to read the GPU device and then print</em></span><span id="202e" class="lz ma iq or b gy oz ow l ox oy">from tensorflow.python.client import device_lib<br/>print(device_lib.list_local_devices())</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pc"><img src="../Images/43a2b09e6fd78423c95752a56eef5880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjTgohQnYbO9fBp0wGzAaw.png"/></div></div></figure><p id="4b02" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">训练模型。</strong></p><p id="0bc0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在找到要使用的GPU后，我们将把它合并到我们的代码中，最终使用train_generator为训练数据训练模型，并将validation_data参数设置为test_generator。</p><pre class="kg kh ki kj gt oq or os ot aw ou bi"><span id="caae" class="lz ma iq or b gy ov ow l ox oy"><em class="lw"># Execute the model with fit_generator within the while loop utilizing the discovered GPU</em><br/>import tensorflow as tf<br/>with tf.device("/device:GPU:0"):<br/>    history = model_InceptionV3.fit_generator(<br/>        train_generator,<br/>        epochs=5,<br/>        validation_data=test_generator,<br/>        verbose = 1,<br/>        callbacks=[EarlyStopping(monitor='val_accuracy', patience = 5, restore_best_weights = True)])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pd"><img src="../Images/a3263ea802054d3383b9dec3ec478f9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yEECujSmc83faZ_9d0qDdQ.png"/></div></div></figure><p id="11e6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">99%的验证准确性和0.0187的损失是非常好的。</p><p id="f7e1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">显示模型的测试精度和测试损失值</strong></p><p id="6dba" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，让我们通过绘制各个时期的训练精度/验证精度和训练损失/验证损失来看看我们的模型是什么样子，然后打印最终的测试精度和测试损失。</p><pre class="kg kh ki kj gt oq or os ot aw ou bi"><span id="c31a" class="lz ma iq or b gy ov ow l ox oy"><em class="lw"># Create a dictionary of the model history </em><br/>import matplotlib.pyplot as plt<br/>history_dict = history.history<br/>loss_values = history_dict['loss']<br/>val_loss_values = history_dict['val_loss']<br/>acc_values = history_dict['accuracy']<br/>val_acc_values = history_dict['val_accuracy']<br/>epochs = range(1, len(history_dict['accuracy']) + 1)</span><span id="cd7e" class="lz ma iq or b gy oz ow l ox oy"><em class="lw"># Plot the training/validation loss</em><br/>plt.plot(epochs, loss_values, 'bo', label = 'Training loss')<br/>plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')<br/>plt.title('Training and validation loss')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show()</span><span id="fc1e" class="lz ma iq or b gy oz ow l ox oy"><em class="lw"># Plot the training/validation accuracy</em><br/>plt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')<br/>plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')<br/>plt.title('Training and validation accuracy')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Accuracy')<br/>plt.legend()<br/>plt.show()<br/></span><span id="d99f" class="lz ma iq or b gy oz ow l ox oy"><em class="lw"># Evaluate the test accuracy and test loss of the model</em><br/>test_loss, test_acc = model_InceptionV3.evaluate_generator(test_generator)</span><span id="4a7b" class="lz ma iq or b gy oz ow l ox oy">print('Model testing accuracy/testing loss:', test_acc, " ", test_loss)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/cbe5998da6fec183a6386a60e2cf33f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*90XLoM1P3Ct4AyuN.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/ea1ffe370740ab1698828564e7cef1e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/0*-D9pWw2NZk3nm6r2.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pg"><img src="../Images/db895a071f060ad949c0f262e46b20ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1bMcKnCsw-EeucRxElFbYw.png"/></div></div></figure><h2 id="4f7a" class="lz ma iq bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq mr bi translated">结果分析</h2><p id="13d3" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated">对120类果蔬图像的准确预测结果表明，测试准确率为99%，损失值为1.8%。损失价值是衡量我们的产出与我们的期望之间的距离。训练分5个阶段进行，每个阶段大约需要20-25分钟，在ka ggle GPU的帮助下可以达到这一精度，从而加快了整个过程。训练数据是一个3781步的过程(迭代)，每16个样本取一批数据，向前和向后传播给我们一遍。一遍等于一次迭代。</p><h2 id="d9f6" class="lz ma iq bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq mr bi translated">结论</h2><p id="b861" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated">总之，迁移学习是训练数据集识别和分类图像的一种非常有效的方法。它允许快速设置，而无需从头开始详细设计卷积神经网络架构，并且它利用预训练模型的先前训练来提供高精度。你可以去我项目的Kaggle <a class="ae lx" href="https://www.kaggle.com/jnelson790612/fruit-360-transfer-learning" rel="noopener ugc nofollow" target="_blank">笔记本</a>了解更多信息。</p><h1 id="1aaa" class="oc ma iq bd mb od oe of me og oh oi mh jw oj jx mk jz ok ka mn kc ol kd mq om bi translated">参考</h1><p id="d1e0" class="pw-post-body-paragraph kr ks iq kt b ku ms jr kw kx mt ju kz la mu lc ld le mv lg lh li mw lk ll lm ij bi translated">阿迪蒂亚·阿南瑟拉姆。(2018年10月17日)。Keras中使用迁移学习的初学者深度学习。2020年4月24日检索，来自Medium网站:<a class="ae lx" rel="noopener" target="_blank" href="/keras-transfer-learning-for-beginners-6c9b8b7143e">https://towards data science . com/keras-transfer-learning-for-初学者-6c9b8b7143e </a></p><p id="06dd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">f .谢赫(2018 . 10 . 18)。从头开始理解Inception网络(带Python代码)。2020年5月7日检索，来自Analytics Vidhya网站:<a class="ae lx" href="https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/" rel="noopener ugc nofollow" target="_blank">https://www . Analytics vid hya . com/blog/2018/10/understanding-inception-network-from-scratch/</a></p><p id="dc72" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">f . chollet(2018)。<em class="lw">用Python进行深度学习</em>。庇护岛(纽约，州联合):曼宁，警察。</p><p id="574d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">文件:Valid-padding-convolution.gif —维基共享。(2018年7月6日)。2020年5月6日检索，来自wikimedia.org网站:<a class="ae lx" href="https://commons.wikimedia.org/wiki/File:Valid-padding-convolution.gif" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:Valid-padding-convolution . gif</a></p><p id="bb2f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">获取蓝色背景的人脑智能概念的免费库存照片在线|下载最新免费图片和免费插图。(2020).2020年5月9日检索，来自freerangestock.com网站:<a class="ae lx" href="https://freerangestock.com/photos/65677/concept-of-intelligence-with-human-brain-on-blue-background.html" rel="noopener ugc nofollow" target="_blank">https://freerangestock . com/photos/65677/concept-of-intelligence-with-human-brain-on-blue-background . html</a></p><p id="1cc1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在线获取意大利水果和蔬菜供应商的免费图片|下载最新免费图片和免费插图。(2020).检索于2020年5月9日，来自freerangestock.com网站:<a class="ae lx" href="https://freerangestock.com/photos/37652/fruit-and-vegetables-vendor-italy.html" rel="noopener ugc nofollow" target="_blank">https://freerangestock . com/photos/37652/fruit-and-vegetables-vendor-Italy . html</a></p><p id="d1ef" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">https://github.com/syt123450,的syt123450。(2020).图层GlobalPooling2d。2020年5月6日检索，来自Tensorspace.org网站:【https://tensorspace.org/html/docs/layerGlobalPooling2d.html T2】</p><p id="aefe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">ImageNet。(2017).2020年5月7日检索，来自Image-net.org网站:【http://www.image-net.org/ T4】</p><p id="cb65" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">克鲁特·帕特尔。(2019年9月8日)。卷积神经网络-数据科学初学者指南。2020年4月24日检索，来自Medium网站:<a class="ae lx" rel="noopener" target="_blank" href="/convolution-neural-networks-a-beginners-guide-implementing-a-mnist-hand-written-digit-8aa60330d022">https://towards data science . com/convolution-neural-networks-a-beginners-guide-implementing-a-m NIST-hand-written-digit-8aa 60330d 022</a></p><p id="f7d7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">米哈伊·奥尔特安。(2020).水果360。检索于2020年5月6日，来自Kaggle.com网站:<a class="ae lx" href="https://www.kaggle.com/moltean/fruits" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/moltean/fruits</a></p><p id="5159" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">米尔顿-巴克，A. (2019年2月17日)。用于急性髓细胞/淋巴细胞白血病分类的Inception V3深度卷积架构。2020年5月6日检索，来自intel.com网站:<a class="ae lx" href="https://software.intel.com/en-us/articles/inception-v3-deep-convolutional-architecture-for-classifying-acute-myeloidlymphoblastic" rel="noopener ugc nofollow" target="_blank">https://software . Intel . com/en-us/articles/inception-v3-deep-convolutional-architecture-for-classification-acute-myeloid-lymphoblastic</a></p><p id="d481" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">普拉哈尔·甘尼什。(2019年10月18日)。卷积核的类型:简化—面向数据科学。2020年4月24日检索，来自Medium网站:<a class="ae lx" rel="noopener" target="_blank" href="/types-of-convolution-kernels-simplified-f040cb307c37">https://towardsdatascience . com/types-of-convolution-kernels-simplified-f 040 CB 307 c 37</a></p><p id="de46" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Unsplash。(2020).现代灵感的灰烬。检索于2020年5月9日，来自Unsplash.com网站:<a class="ae lx" href="https://unsplash.com/@modernafflatusphotography" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/@modernafflatusphotography</a></p><p id="ffc1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">可视面包屑。(2016).2020年4月27日检索，来自mathworks.com网站:<a class="ae lx" href="https://www.mathworks.com/help/deeplearning/ref/inceptionv3.html" rel="noopener ugc nofollow" target="_blank">https://www . mathworks . com/help/deep learning/ref/inceptionv3 . html</a></p><p id="b01a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">维基百科贡献者。(2020年5月2日)。我，机器人(电影)。2020年5月5日检索，来自维基百科网站:<a class="ae lx" href="https://en.wikipedia.org/wiki/I,_Robot_(film)" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/I，_Robot_(film) </a></p></div></div>    
</body>
</html>