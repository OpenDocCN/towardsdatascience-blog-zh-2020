<html>
<head>
<title>Knowing Spark and Kafka: A 100 Million Events Use-Case</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解火花和卡夫卡:一亿个事件用例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/knowing-pyspark-and-kafka-a-100-million-events-use-case-5910159d08d7?source=collection_archive---------14-----------------------#2020-03-15">https://towardsdatascience.com/knowing-pyspark-and-kafka-a-100-million-events-use-case-5910159d08d7?source=collection_archive---------14-----------------------#2020-03-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="9811" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">电子商务市场中典型的一天，每分钟处理10，000个事件，并选择正确的工具来处理这些事件</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/860c03f765ad516044d3d6242a1e1bf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-4kntYpo86zHPWws"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@flaunter?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Flaunter.com</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="4867" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文将帮助您重新创建一个场景，其中有大量数据流入，您不仅要存储这些数据，还要执行一些实时分析！</p><p id="fd0b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这只是<strong class="js iu">系统设计</strong>的一个例子，在这里你必须开发一个高度可用和可伸缩的数据管道。虽然对于像电子商务这样的用例，可能有许多其他因素需要考虑，但为了本文的目的，我们将把它分成3个主要部分:</p><ol class=""><li id="7787" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated">摄取</li><li id="9ba2" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">处理</li><li id="77f6" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">输出</li></ol><p id="8e0e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">简而言之，这是几乎所有系统设计的鸟瞰图，也是可能出错的地方。</p><h2 id="5544" class="lt lu it bd lv lw lx dn ly lz ma dp mb kb mc md me kf mf mg mh kj mi mj mk ml bi translated">摄入层</h2><p id="0e7a" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">在开始使用我们的工具之前，让我们后退一步，看看我们在这里试图解决什么样的用例或问题。要了解我们的<strong class="js iu">输入或摄取</strong>层，首先要了解我们的<strong class="js iu">输出</strong>层。一般来说，你可以通过两种方式展示你的发现:</p><ol class=""><li id="1a49" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated"><strong class="js iu">批处理:</strong>如果您的分析只是一次性的，或者可能只是每日报告更新，或者只是团队中的随机演示，您可以选择批量接收数据。这可能意味着从您的数据库中取出一个小的数据转储，并用它进行一些分析。</li><li id="fb80" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu">实时处理:</strong>也称为<strong class="js iu">流数据</strong>是一种在新数据分析至关重要时可以采用的方法。最常见于B2C场景，在这种场景中，您可以即时执行操作。</li></ol><p id="dba9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">批处理的好处是，它消除了构建实时管道的开销，并且您永远不会处理完整的数据集。不过，这并不适用于B2C环境，尤其是电子商务，在电子商务中，你必须推荐新产品、跟踪用户行程或设计实时仪表板。</p><p id="1f3f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">既然我们知道我们的<strong class="js iu">输出层</strong>将是实时的，我们将相应地选择我们的<strong class="js iu">摄取工具</strong>。当然，有成千上万的工具可供你从中获取数据，但根据受欢迎程度、社区实力和各种用例的实现情况，我们将挑选<a class="ae le" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">【卡夫卡】</strong> </a> <strong class="js iu">和</strong><a class="ae le" href="https://spark.apache.org/streaming/" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">Spark Streaming</strong></a><strong class="js iu">。</strong></p><p id="2aca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里再次强调，了解您的业务需求以决定执行相同工作的几个工具是很重要的。在像e-comm这样的场景中，我们已经知道我们想要实时输出，但是我们在这里谈论的<strong class="js iu">数字</strong>是什么？</p><p id="c91e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">1-2秒相当实时！是的，但对一个电子商务网站来说不是这样，在那里你的用户不会等待超过一秒来执行下一次点击。这让我们想到了<strong class="js iu">延迟</strong>的概念。这是我们用来选择摄取工具的指标。这两个工具有很多不同之处，但是卡夫卡提供了毫秒级的延迟！</p><h2 id="4f09" class="lt lu it bd lv lw lx dn ly lz ma dp mb kb mc md me kf mf mg mh kj mi mj mk ml bi translated">处理层</h2><p id="1df2" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">在我们的用例中，我们将分别检查Spark和Kafka的处理机制。我们将看到spark如何使底层硬件实际上不应该保存的数据处理成为可能。另一方面，我们将看到使用Kafka消费数据是多么容易，以及它如何在数百万的规模上实现这一点。</p><p id="5523" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我将使用下面来自<a class="ae le" href="https://kaggle.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> Kaggle </strong> </a>的数据集，它有超过1亿行</p><blockquote class="mr ms mt"><p id="ee33" class="jq jr mu js b jt ju jv jw jx jy jz ka mv kc kd ke mw kg kh ki mx kk kl km kn im bi translated"><a class="ae le" href="https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/mkechinov/ecommerce-behavior-data-from-multi-category-store</a></p></blockquote><p id="ab78" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">除非您拥有一台非常高端的机器，否则不可能将整个数据集加载到您本地机器的内存中，甚至不可能将它拆分成批处理，当然除非您对每个传入的批处理执行处理，但这就是为什么我们使用类似于<a class="ae le" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> Spark </strong> </a> <strong class="js iu">的东西。</strong></p><h2 id="30b8" class="lt lu it bd lv lw lx dn ly lz ma dp mb kb mc md me kf mf mg mh kj mi mj mk ml bi translated">基础设施</h2><p id="3419" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">设置spark有其复杂性，因此，为了加快速度，我们将在<a class="ae le" href="http://databricks.com/try-databricks" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">数据块</strong> </a> <strong class="js iu">，</strong>上启动Spark集群，让您可以在数据驻留的<a class="ae le" href="https://aws.amazon.com/s3/" rel="noopener ugc nofollow" target="_blank"> AWS S3 </a>的数据支持下快速启动集群。</p><p id="af60" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Spark遵循典型的主从架构，概括来说，这意味着<strong class="js iu">主服务器</strong>负责所有的作业调度工作以及其他一些事情，另一方面，<strong class="js iu">从服务器</strong>负责执行实际操作或将数据保存在内存中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi my"><img src="../Images/150e2f002d74f50bea2b10cd9f45c238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xGMpCZJ6vVKhg773UeHwQQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Spark架构，1个主节点+ 2个工作/从节点</p></figure><p id="7f3f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我们在数据上实现Spark时，我们将对其进行更详细的介绍。目前，我已经在databricks上构建了一个1 <strong class="js iu">工作节点+ 1个主节点</strong>集群，总共有2个内核和8 GB内存，尽管完整的配置应该是4个内核和16 GB内存。</p><p id="f32c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它是2和8，因为我们所有的spark操作都只发生在worker节点上，而我们只有1个worker节点。内核的数量(即2个)将在这里发挥关键作用，因为所有并行化工作都将在这里进行。</p><p id="d7ee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">读取8 GB内存上的14 GB数据</strong></p><p id="6ff0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在比内存本身更大的内存中存储数据是不切实际的，因此，spark所做的是，只有当你想对数据进行一些操作时，它才会将数据加载到内存中。例如，以下代码行将并行读取我们的数据集，即利用2个内核读取我们的数据。</p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="ae8c" class="lt lu it na b gy ne nf l ng nh">ecomm_df = sparkSession.read.csv("/mnt/%s/ecomm.csv" % MOUNT_NAME, header=True</span></pre><p id="98e8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的14 GB文件将被分成大约112个小块，由于我们有2个内核，因此将被分成2个块，每次128 MB</p><p id="4f43" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然，spark不会在你提交这个命令的时候开始读取文件，因为有一个完全不同的概念<a class="ae le" href="https://bit.ly/2xxt2Br" rel="noopener ugc nofollow" target="_blank">懒惰求值</a>，这使得它不会以传统的pythonic方式读取它！但是我们仍然可以通过快速转换成RDD来检查这个文件的分区/块的数量</p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="faaa" class="lt lu it na b gy ne nf l ng nh">ecomm_df.rdd.getNumPartitions()<br/>OUTPUT: <strong class="na iu">110 #Number of partitions</strong></span></pre><p id="8a61" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这与我们的计算非常接近。<a class="ae le" href="https://bit.ly/2IR77Yh" rel="noopener ugc nofollow" target="_blank">看看这个</a>知道我是如何从14 GB的文件大小计算出112个分区的。</p><p id="76d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，不要太专业，让我们快速浏览一下我们的数据</p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="4410" class="lt lu it na b gy ne nf l ng nh"><strong class="na iu"># A SAMPLE RECORD OF OUR DATA</strong></span><span id="d48b" class="lt lu it na b gy ni nf l ng nh">Row(event_time='2019-11-01 00:00:00 UTC', event_type='view', product_id='1003461', category_id='2053013555631882655', category_code='electronics.smartphone', brand='xiaomi', price='489.07', user_id='520088904', user_session='4d3b30da-a5e4-49df-b1a8-ba5943f1dd33')</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nj"><img src="../Images/30839f3114175db7e3fd80b2f547da31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gEJ-wj0B2a7vjdlsnurk3g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">过滤掉只购买了Xiamoi智能手机的人，然后执行左连接。查看如何将每个命令分解为110个任务，并且2个任务总是并行运行</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nk"><img src="../Images/5083c48051987567e67c3e8919104099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IH8s6FldhVlxJOKSNDcnWg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">按品牌分析有多少百分比的用户只是查看和添加到购物车，以及购买特定商品</p></figure><p id="7730" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在您已经了解了spark的功能，它是一种非常可扩展的方法，可以在有限的资源集上训练/分析几乎任何规模的数据</p><h2 id="9ed4" class="lt lu it bd lv lw lx dn ly lz ma dp mb kb mc md me kf mf mg mh kj mi mj mk ml bi translated">模拟实时数据摄取</h2><p id="47fb" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">我们之前已经讨论了很多关于Kafka的要点，所以不需要太深入，让我们看看我们在真实场景中摄取这种数据的Kafka管道是什么样子的！</p><p id="f198" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我们谈论任何给定时间的1亿个事件这样的数字时，可伸缩性成为优先考虑的问题，对<strong class="js iu">分区和消费者群体</strong>的理解也是如此。在这种摄入水平下，这两种成分可以成就或破坏我们的系统。看看这个架构，对Kafka系统有个大概的了解</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/922ded234ff1bb339f1e15b52098a69f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*HdrdFds-0W6KW9Hw0UP96w.png"/></div></figure><p id="2924" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个模型的真实世界复制品将是你的邮箱。</p><ol class=""><li id="a354" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated"><strong class="js iu">邮递员:</strong>这个人是制作人，他的工作只是收集数据，然后把它放进你的邮箱</li><li id="14e5" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu">邮箱/信箱:</strong>这是你的经纪人，如果没人来收，信件会一直堆积。</li><li id="2a12" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu">你的地址:</strong>这是你的题目，邮递员怎么知道要把这些数据发到哪里？</li><li id="3c88" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu">你:</strong>你是消费者，收集这些数据并进一步处理是你的责任</li></ol><p id="5e92" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是Kafka数据流机制的一个非常简单的解释，足以进一步理解本文，而分区和消费者组的概念也将帮助您理解代码片段</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/9d6b61e6cd3fcdbbef011d99ebf8d29d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zTRQR9H_Glv1xbPWl_yY6w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">主题是保存您的数据的内容，可以划分为<strong class="bd nn"> n </strong>个分区供并行使用</p></figure><p id="0b11" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这种规模下，您希望并行化数据消费。为此，可以将传入的数据分成不同的<strong class="js iu">分区</strong>，当这种情况发生时，我们可以建立<strong class="js iu">消费者组</strong>，这意味着多个消费者希望从同一个源读取数据。</p><p id="8517" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参考上面的体系结构，两个使用者从同一个源读取数据，因此，同时读取更多的数据，但读取的数据不同。如果<strong class="js iu">消费者1 </strong>已经读取了行1和行2，<strong class="js iu">消费者2 </strong>将永远看不到这些数据，因为这种隔离已经在分区级别发生了！</p><p id="fa74" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我在使用分区和消费者组大规模接收这种数据时做的一个小实现</p><pre class="kp kq kr ks gt mz na nb nc aw nd bi"><span id="f5f0" class="lt lu it na b gy ne nf l ng nh"><strong class="na iu"># 4 Partitions Made<br/># Topic Name : ecomm_test</strong></span><span id="cbe4" class="lt lu it na b gy ni nf l ng nh">./kafka_2.11-2.3.1/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 4 --topic ecomm_test</span><span id="a677" class="lt lu it na b gy ni nf l ng nh"><strong class="na iu"># Send data to ecomm_test topic</strong><br/>producer.send(topic='ecomm_test', value=line)</span><span id="77f1" class="lt lu it na b gy ni nf l ng nh"><strong class="na iu"># Start 2 consumers and assign it to the group "ecommGroup"<br/></strong>consumer = KafkaConsumer('ecomm_test', group_id='ecommGroup')<br/>consumer = KafkaConsumer('ecomm_test', group_id='ecommGroup')</span><span id="30b5" class="lt lu it na b gy ni nf l ng nh"><strong class="na iu"># Output of how consumer 1 reads data, only reading from 2 partitions i.e. 0 &amp; 1</strong></span><span id="46af" class="lt lu it na b gy ni nf l ng nh">ConsumerRecord(topic=u'ecomm_test', partition=1, value='2019-11-01 00:00:01 UTC,view,17302664,2053013553853497655,,creed,28.31,561587266,755422e7-9040-477b-9bd2-6a6e8fd97387\n')</span><span id="6cec" class="lt lu it na b gy ni nf l ng nh">ConsumerRecord(topic=u'ecomm_test', partition=0, value='2019-11-01 00:00:01 UTC,view,3601530,2053013563810775923,appliances.kitchen.washer,lg,712.87,518085591,3bfb58cd-7892-48cc-8020-2f17e6de6e7f\n')</span><span id="431c" class="lt lu it na b gy ni nf l ng nh"><strong class="na iu"># Output of how consumer 2 reads data, only reading from 2 partitions i.e. 2 &amp; 3</strong></span><span id="8403" class="lt lu it na b gy ni nf l ng nh">ConsumerRecord(topic=u'ecomm_test', partition=3, value='2019-11-01 00:00:05 UTC,view,4600658,2053013563944993659,appliances.kitchen.dishwasher,samsung,411.83,526595547,aab33a9a-29c3-4d50-84c1-8a2bc9256104\n')</span><span id="c9cd" class="lt lu it na b gy ni nf l ng nh">ConsumerRecord(topic=u'ecomm_test', partition=2, value='2019-11-01 00:00:01 UTC,view,1306421,2053013558920217191,computers.notebook,hp,514.56,514028527,df8184cc-3694-4549-8c8c-6b5171877376\n')</span></pre></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><h2 id="fd14" class="lt lu it bd lv lw lx dn ly lz ma dp mb kb mc md me kf mf mg mh kj mi mj mk ml bi translated"><strong class="ak">一吻制作架构:输出</strong></h2><p id="faee" class="pw-post-body-paragraph jq jr it js b jt mm jv jw jx mn jz ka kb mo kd ke kf mp kh ki kj mq kl km kn im bi translated">我们只需要确保我们与以下概念保持一致:</p><ol class=""><li id="2272" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated"><strong class="js iu">吻:保持简单愚蠢:</strong>尽可能保持架构简单</li><li id="34ea" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu">微服务:</strong>分离组件以避免一连串的故障</li><li id="d607" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu"> CAP定理:</strong>一致性，可用性，划分容差。选择两个对你来说最重要的</li></ol><p id="e788" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们将介绍可以在生产系统中实施的最终体系结构，尽管还涉及许多其他组件，如可用性区域、存储系统、故障切换计划，但这只是生产中最终处理层的概述</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/3871dc58a1015add219974f0961c98c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MwEC5C61aemlSnn6gFxTHw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">包含数据流的完整架构图</p></figure><p id="f34e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如你所看到的，图表是不言自明的，没有一个正确的架构/系统设计适合所有的用例，你只需要在给定的资源下构建可以工作的东西。</p><blockquote class="mr ms mt"><p id="ffd2" class="jq jr mu js b jt ju jv jw jx jy jz ka mv kc kd ke mw kg kh ki mx kk kl km kn im bi translated">欢迎<a class="ae le" href="https://www.linkedin.com/in/jspuri/" rel="noopener ugc nofollow" target="_blank">联系</a>或在此发布您的问题/反馈</p></blockquote><p id="5ee3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">干杯！</p></div></div>    
</body>
</html>