<html>
<head>
<title>Actor-Critic With TensorFlow 2.x [Part 1 of 2]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 2.x 的演员兼评论家[第 1 部分，共 2 部分]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/actor-critic-with-tensorflow-2-x-part-1-of-2-d1e26a54ce97?source=collection_archive---------12-----------------------#2020-08-01">https://towardsdatascience.com/actor-critic-with-tensorflow-2-x-part-1-of-2-d1e26a54ce97?source=collection_archive---------12-----------------------#2020-08-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7aa1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用 Tensorflow 2.x 以不同方式实现参与者-批评家方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/941d8a1c645693ca0fdc35a9862d9c2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cyRyqYRC3kwOjGAryQtNAw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">戴维·维克斯列尔在<a class="ae kv" href="https://unsplash.com/s/photos/reinforcement-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4021" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这一系列文章中，我们将试图理解演员-评论家方法，并以 3 种方式实现它，即天真的 AC，没有多个工人的 A2C，和有多个工人的 A2C。</p><p id="f9ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是系列的第一部分，我们将使用 TensorFlow 2.2 实现天真的演员评论家。我们先来了解一下演员-评论家法是什么，是如何运作的？了解加强政策梯度方法将是有益的，你可以在这里找到它<a class="ae kv" rel="noopener" target="_blank" href="/reinforce-policy-gradient-with-tensorflow2-x-be1dea695f24">。</a></p><h2 id="2d01" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">概述:</h2><p id="3b67" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">如果你读过加强政策梯度法，你就会知道它的更新规则是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/0da8448049e044d579747dd7065be3b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5nFL3yqr91kzl5EqgBIzcg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">更新加固规则</p></figure><p id="7588" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在演员-评论家方法中，我们从折扣奖励中减去基线。这些方法的常用基线是状态值函数。所以我们的演员-评论家更新规则将如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/844694445f700cbf6ae32dd5ce1c7991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*aC1OGZTWs-YVbwQiOR9hiw.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/82826aeb18becc177d79f4d89a326aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*p_wzZfT1R-dTXudNOiyoGQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">演员-评论家更新规则</p></figure><p id="59bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在演员-评论家方法中，我们有两个神经网络，即演员和评论家。actor 用于动作选择，Critic 用于计算状态值。如果您查看更新等式，您会注意到状态值被用作基线。有了基线有助于确定所采取的行动是坏的/好的，还是状态是坏的/好的。您可以在参考资料部分找到非常好的理论资源。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="c946" class="na lt iq bd lu nb nc nd lx ne nf ng ma jw nh jx md jz ni ka mg kc nj kd mj nk bi translated">天真的演员兼评论家:</h1><p id="1b0e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在这个实现中，我们将在每个时间戳更新我们的神经网络。这种实现不同于 A2C，在那里我们在每 n 个时间戳之后更新我们的网络。我们将在本系列的下一部分实现 A2C。</p><h2 id="b847" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">神经网络:</h2><p id="b80a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">神经网络基本上可以用两种方式实现。</p><ol class=""><li id="6642" class="nl nm iq ky b kz la lc ld lf nn lj no ln np lr nq nr ns nt bi translated">一个网络用于演员和评论家功能，即一个网络具有两个输出层，一个用于状态值，另一个用于动作概率。</li><li id="9671" class="nl nm iq ky b kz nu lc nv lf nw lj nx ln ny lr nq nr ns nt bi translated">独立的网络，一个是演员的，另一个是评论家的。</li></ol><p id="c723" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将对演员和评论家使用不同的网络，因为我发现这个可以快速学习。</p><h2 id="444f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">代码:</h2><p id="22b5" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">演员和评论家网络:</p><ol class=""><li id="ed0e" class="nl nm iq ky b kz la lc ld lf nn lj no ln np lr nq nr ns nt bi translated">批评家网络输出每个状态的一个值，而行动者网络输出该状态中每个单个动作的概率。</li><li id="06e2" class="nl nm iq ky b kz nu lc nv lf nw lj nx ln ny lr nq nr ns nt bi translated">这里，演员网络中的 4 个神经元是动作的数量。</li><li id="8068" class="nl nm iq ky b kz nu lc nv lf nw lj nx ln ny lr nq nr ns nt bi translated">注意，Actor 在外层有一个 softmax 函数，它输出每个动作的动作概率。</li></ol><blockquote class="nz oa ob"><p id="3ba1" class="kw kx oc ky b kz la jr lb lc ld ju le od lg lh li oe lk ll lm of lo lp lq lr ij bi translated">注意:隐藏层中的神经元数量对于代理学习非常重要，并且因环境而异。</p></blockquote><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="9ce6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代理类的初始化方法:</p><ol class=""><li id="bcd4" class="nl nm iq ky b kz la lc ld lf nn lj no ln np lr nq nr ns nt bi translated">这里，我们为我们的网络初始化优化器。<strong class="ky ir">请</strong> <strong class="ky ir">注意</strong>学习的<strong class="ky ir"/><strong class="ky ir">速度</strong>也很重要，并且会因使用的环境和方法而异。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="96cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">动作选择:</p><ol class=""><li id="6675" class="nl nm iq ky b kz la lc ld lf nn lj no ln np lr nq nr ns nt bi translated">这种方法利用了张量流概率库。</li><li id="17e5" class="nl nm iq ky b kz nu lc nv lf nw lj nx ln ny lr nq nr ns nt bi translated">首先，Actor 给出概率，然后使用 TensorFlow 概率库将概率转换成分布，然后从分布中抽取动作。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="d0bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">学习功能和损失:</p><ol class=""><li id="34b4" class="nl nm iq ky b kz la lc ld lf nn lj no ln np lr nq nr ns nt bi translated">我们将利用梯度胶带技术进行定制培训。</li><li id="cb4d" class="nl nm iq ky b kz nu lc nv lf nw lj nx ln ny lr nq nr ns nt bi translated">行动者损失是采取行动的对数概率乘以 q 学习中使用的时间差的负数。</li><li id="bee9" class="nl nm iq ky b kz nu lc nv lf nw lj nx ln ny lr nq nr ns nt bi translated">对于临界损失，我们采取了一种简单的方法，只计算时间差的平方。如果你愿意，你可以使用 tf2 的均方误差函数，但是你需要对时差计算做一些修改。我们将在本系列的下一部分中使用 MSE，所以不要担心。</li><li id="8766" class="nl nm iq ky b kz nu lc nv lf nw lj nx ln ny lr nq nr ns nt bi translated">你可以在 TensorFlow 官网找到更多关于定制训练循环的内容。</li></ol><blockquote class="nz oa ob"><p id="e37c" class="kw kx oc ky b kz la jr lb lc ld ju le od lg lh li oe lk ll lm of lo lp lq lr ij bi translated">注意:请确保使用语句(上下文管理器)调用 networks inside，并且只使用张量进行网络预测，否则您将得到一个关于没有提供梯度的错误。</p></blockquote><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="421d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Trining 循环:</p><ol class=""><li id="0aee" class="nl nm iq ky b kz la lc ld lf nn lj no ln np lr nq nr ns nt bi translated">代理在环境中采取行动，然后 bot 网络更新。</li><li id="1610" class="nl nm iq ky b kz nu lc nv lf nw lj nx ln ny lr nq nr ns nt bi translated">对于月球着陆器环境，这种实现表现良好。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="og oh l"/></div></figure><blockquote class="nz oa ob"><p id="4b15" class="kw kx oc ky b kz la jr lb lc ld ju le od lg lh li oe lk ll lm of lo lp lq lr ij bi translated">注意:在实现这些方法时，我注意到学习速率和隐藏层中的神经元对学习有很大的影响。</p></blockquote><p id="b1a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在这里找到这篇文章的完整代码。请继续关注即将发布的文章，我们将在有多个工人和没有多个工人的情况下实施 A2C。</p><p id="e971" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个系列的第二部分可以在这里访问<a class="ae kv" rel="noopener" target="_blank" href="/actor-critic-with-tensorflow-2-x-part-2of-2-b8ceb7e059db">。</a></p><p id="06ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，本文到此结束。谢谢你的阅读，希望你喜欢并且能够理解我想要解释的东西。希望你阅读我即将发表的文章。哈里奥姆…🙏</p><h1 id="f301" class="na lt iq bd lu nb oi nd lx ne oj ng ma jw ok jx md jz ol ka mg kc om kd mj nk bi translated">参考资料:</h1><div class="on oo gp gr op oq"><a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">强化学习，第二版</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">显着扩大和更新的广泛使用的文本强化学习的新版本，最…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">mitpress.mit.edu</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe kp oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">直觉 RL:优势介绍-演员-评论家(A2C)</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">强化学习(RL)实践者已经产生了许多优秀的教程。然而，大多数描述 RL 在…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">hackernoon.com</p></div></div><div class="oz l"><div class="pf l pb pc pd oz pe kp oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">优势演员评论方法介绍:让我们玩刺猬索尼克！</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">托马斯西蒙尼尼介绍优势演员评论家的方法:让我们玩刺猬索尼克！从…开始</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">www.freecodecamp.org</p></div></div><div class="oz l"><div class="pg l pb pc pd oz pe kp oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://www.youtube.com/channel/UC58v9cLitc8VaCjrcKyAbrw" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">菲尔的机器学习</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">你好。在 Neuralnet.ai，我们涵盖了各种主题的人工智能教程，从强化…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">www.youtube.com</p></div></div><div class="oz l"><div class="ph l pb pc pd oz pe kp oq"/></div></div></a></div></div></div>    
</body>
</html>