<html>
<head>
<title>A Primer on Generalized Linear Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">广义线性模型入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-primer-on-generalized-linear-models-ab1769a03100?source=collection_archive---------29-----------------------#2020-05-29">https://towardsdatascience.com/a-primer-on-generalized-linear-models-ab1769a03100?source=collection_archive---------29-----------------------#2020-05-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ccce" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">应用统计学的支柱</h2></div><p id="00d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性模型非常非常重要。而其他线性模型存在(分级、比例风险等。)，GLMs提供了一个很好的起点。</p><p id="a3b0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一，业务方面。对于提出建议和交流结果，可解释性是关键。我想不出比GLM更具解释性的机器学习(ML)模型了。频率主义者可以使用GLM进行<a class="ae lb" rel="noopener" target="_blank" href="/the-most-important-statistical-test-dee01f4d50cf">似然比测试</a>来测试假设，而贝叶斯主义者可以通过使用先验拟合GLM来获得后验概率。</p><p id="e79e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二，技术方面。即使当可解释性不是一个问题，正则化GLM可能是最常用的ML模型。对于数据科学项目来说，它通常是“最不可行的产品”,因为它易于培训和部署，同时性能相当好。Booking.com在<a class="ae lb" href="https://booking.ai/https-booking-ai-machine-learning-production-3ee8fe943c70" rel="noopener ugc nofollow" target="_blank">的这篇文章</a>中详细介绍了他们如何快速大规模生产模型(剧透:从GLMs开始)。如果您有许多可能的项目要做，那么更值得的是坚持使用GLM，只有在您完成了所有高优先级的项目之后，才返回来改进模型。</p><p id="a66e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">几年前有一条病毒式的推特:</p><blockquote class="lc ld le"><p id="b200" class="kf kg lf kh b ki kj jr kk kl km ju kn lg kp kq kr lh kt ku kv li kx ky kz la ij bi translated">当你在筹款时，它是人工智能。</p><p id="e9ad" class="kf kg lf kh b ki kj jr kk kl km ju kn lg kp kq kr lh kt ku kv li kx ky kz la ij bi translated">当你在招人的时候，就是ML。</p><p id="bb8e" class="kf kg lf kh b ki kj jr kk kl km ju kn lg kp kq kr lh kt ku kv li kx ky kz la ij bi translated">当你实施时，它是线性回归。</p><p id="9ec8" class="kf kg lf kh b ki kj jr kk kl km ju kn lg kp kq kr lh kt ku kv li kx ky kz la ij bi translated">当你在调试时，它是printf()。</p></blockquote><p id="dde2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还有人讽刺地开玩笑说:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/3f4806c69c7debfbf53939f4cd398e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*006COpEBcYuSuwUJFgqXKA.png"/></div></figure><p id="e9ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然这些说法并不完全属实，但其中有一点是真实的。当然，许多工作需要的是相反的极端:深度学习。尝试使用线性回归做NLP或计算机视觉，祝你好运。但是深度学习不就是一堆逻辑回归堆在一起吗？(我开玩笑。)</p><p id="cab3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">希望你相信GLM是分析和数据科学中需要掌握的一项重要技能。我想把GLMs的精髓提炼到这篇短文中，这样你只需要花20分钟就可以很好地理解，而不是阅读整本教科书。</p><p id="85ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文将首先讨论逻辑回归，因为它是最常见和最熟悉的类型。然后，它将作为一个例子来解释总体GLM概念。然后我们将深入研究其他类型的GLMs。假定了解普通最小二乘(OLS)回归。</p><h1 id="dcab" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">二元结果:逻辑回归</h1><p id="b95c" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">逻辑回归用于编码为1或0、真或假的二进制响应。当汇总时，我们可能将成功的比例或计数作为响应变量。我们假设数据是像掷硬币一样产生的(<a class="ae lb" href="https://en.wikipedia.org/wiki/Binomial_distribution" rel="noopener ugc nofollow" target="_blank">二项式分布</a>)，我们希望根据预测因素来预测成功的概率。</p><p id="35b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用<a class="ae lb" href="https://en.wikipedia.org/wiki/Space_Shuttle_Challenger" rel="noopener ugc nofollow" target="_blank">号航天飞机挑战者</a> <a class="ae lb" href="https://en.wikipedia.org/wiki/O-ring" rel="noopener ugc nofollow" target="_blank">号O型圈</a>数据集进行演示。1986年，航天飞机<a class="ae lb" href="https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster" rel="noopener ugc nofollow" target="_blank">在发射</a>后不久解体，所有乘客遇难，原因是O型环故障。</p><p id="c1d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一事件是可以避免的。工程师们表达了担忧，并强烈建议因天气原因推迟发射，但警告被美国宇航局管理层忽视。发射的前一天晚上，工程师艾柏林告诉他的妻子“它要爆炸了。”</p><p id="01dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">工程师是怎么知道的？以下是他们所掌握的数据(要点略显紧张):</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/e20302122bc7c2409f13ec99be4fa31e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*-FwoNSjOo9lOrPoYK9T_eQ.jpeg"/></div></figure><p id="9c81" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">发射时，外部温度为31华氏度。即使通过目测，我们也应该警惕。工程师们要求等到温度上升到53华氏度以上。美国宇航局认为发射失败的几率为0.001%。这次发射的风险到底有多大？</p><p id="62d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回答这个问题最天真的方法是使用OLS回归来预测概率:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/59ba28dba695044286d981abbc207b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*45T4cxNQrlGups7OsnAhiQ.jpeg"/></div></figure><p id="0178" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，这种模式没有意义。首先，它可以预测[0，1]之外的概率，或者在这种情况下，预测[0，6]之外的预期计数。与OLS拟合直线不同，逻辑回归拟合的是逻辑函数<a class="ae lb" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank"/>，一种S形函数:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/e1d9c383bb0c14c104fa316ddf1f0e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*PaEjawWyMpwVttNjwswDnA.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><a class="ae lb" href="https://commons.wikimedia.org/wiki/File:Logistic-curve.svg" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="7b06" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以为我们的预测设定一个下限和上限。事实证明，预测对数概率(logit)并将其输入逻辑(逆logit)函数可以实现我们的目标。我们的回归模型是:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/027e1a9ba051cf2310fcab71a9c9e57e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hrR9BGgfLU7uuzU3Wx80Qg.png"/></div></div></figure><p id="a13b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意没有ε。不可约误差来自二项式分布，它是p的函数。</p><p id="c0d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在R中，我们可以用一个函数来拟合模型:</p><pre class="lk ll lm ln gt mz na nb nc aw nd bi"><span id="7cc1" class="ne ls iq na b gy nf ng l nh ni">model &lt;- glm(<br/>  damage/6 ~ temp, <br/>  data = faraway::orings, <br/>  family = 'binomial', <br/>  weights = rep(6, nrow(faraway::orings))<br/>)</span></pre><p id="9616" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有两种方法可以将分组二项数据输入到R的glm中。我更喜欢这个，这里的响应是比例，权重是试验次数。在95%置信区间的情况下，预测的平均值为:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/76204a197bfecfcb5474a3dc0295774e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*uOQe3XU6ipXYtTrqVBczzg.jpeg"/></div></figure><p id="1b68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据模型假设，我们预计在发射条件下，平均有5或6个O型圈会失效。发射注定要失败。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="1823" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经看到了我们的第一个例子，我们将在讨论其他发行版之前讨论GLM的一般方面。</p><h2 id="7871" class="ne ls iq bd lt nq nr dn lx ns nt dp mb ko nu nv md ks nw nx mf kw ny nz mh oa bi translated">理论:链接功能</h2><p id="884d" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">GLMs预测链接函数而不是原始观察值。把它们看作是把你的观察和你的模型联系起来的一种方式。</p><p id="b192" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解释链接函数的最好方法是将其与转换函数进行对比。对数变换常用于处理OLS回归中的右偏残差，以预测函数的条件期望E[ln(y)|X]。使用具有对数链路的高斯GLM代替预测ln(E[y|X])，<strong class="kh ir">作为条件期望</strong>的函数。</p><p id="a01f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们看到假设ε服从正态分布的回归方程时，这种差异就变得更加明显:</p><ul class=""><li id="af95" class="ob oc iq kh b ki kj kl km ko od ks oe kw of la og oh oi oj bi translated">对数变换:ln(y) = Xβ + ε。y的条件分布是<em class="lf">对数正态</em>。</li><li id="969e" class="ob oc iq kh b ki ok kl ol ko om ks on kw oo la og oh oi oj bi translated">日志链接:ln(y + ε) = Xβ。y的条件分布是<em class="lf">正态</em>。</li></ul><div class="lk ll lm ln gt ab cb"><figure class="op lo oq or os ot ou paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/d0bfdc4ad1344f0c86071cc52dc1f7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*r_io6XQhUlRFOHjuMVXE7Q.jpeg"/></div></figure><figure class="op lo oq or os ot ou paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/3dd1e15936f4359b98dc1ce3e8215c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*AKCPAbTcSdU-Hc-waW9D-w.jpeg"/></div></figure></div><p id="db42" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在对数变换中，exp(fitted)不是预测的平均值，而是众数。没有一个比另一个更好:他们做了不同的假设。</p><p id="8fdf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">链接函数在计数和二进制观测中显示了它们的惊人之处。我们经常使用log link函数来模拟计数。您多久会因为无法记录日志(0)而感到沮丧？在泊松假设下，即使条件均值非零，你也能观察到0。取条件平均值的对数没有问题。</p><p id="b3f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">逻辑回归中的典型关联函数是logit(对数优势)函数</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/38e8ed84f6c0f111bc11f1d8fa54e7cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*7q_97Z9Cuf2JjO6zRoIHaw.png"/></div></figure><p id="d63f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们说典范是因为它是我们从二项分布本身得到的。logit函数是转换(0，1) → R的一种方便的方法，但是有很多函数可供选择。为什么是这个？</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ow"><img src="../Images/345dba3690218cc6adeaac5bb24f0cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2zR7matMEQ8T9XKQHwFeA.png"/></div></div></figure><p id="35cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看到什么熟悉的东西了吗？我们称logit(p)为二项式分布的自然参数，因为它是与y相互作用的参数。练习:对泊松做同样的操作，以说明为什么ln(λ)是规范链接。</p><p id="2ff4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，没有人强迫你使用规范链接。例如,<a class="ae lb" href="https://en.wikipedia.org/wiki/Probit" rel="noopener ugc nofollow" target="_blank"> probit </a>链接通常用于对罕见事件建模。</p><h2 id="7e99" class="ne ls iq bd lt nq nr dn lx ns nt dp mb ko nu nv md ks nw nx mf kw ny nz mh oa bi translated">理论:GLMs如何工作</h2><p id="ac33" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">有几种算法可以拟合GLM，但本文将只讨论迭代加权最小二乘法(IRLS)，因为它提供了最佳的直觉。透过渐变的镜头来看太抽象了。一些数学问题将被抛到一边，这样我们就可以专注于直觉。我们从OLS的封闭解开始:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/38826ab425e2d728ab01f520f0281b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*nIFNvpAwmlIreUL1n2gPSQ.png"/></div></figure><p id="2228" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在OLS，所有的观察值都被假定为独立的，并且误差来自具有共同方差的同一个正态分布。如果观察值的权重不相等，则解决方案看起来像:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/5b120c7035fd736e19f1a4b0fd9301f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*ud4GrkdfZF5DQLyvIotohQ.png"/></div></figure><p id="e324" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在OLS，W是单位矩阵，因此被省略。但是如果不是恒等式呢，比如在<a class="ae lb" href="https://en.wikipedia.org/wiki/Weighted_least_squares" rel="noopener ugc nofollow" target="_blank">加权最小二乘</a>中？我们保持独立性假设，所以W是对角矩阵。对角线上的每个条目应该是精度，或者1/方差。</p><p id="b2a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这最后一点是统计学中一个常见且重要的主题。统计学中如此多的东西可以被视为一个<strong class="kh ir">精确加权平均值</strong>。当我们确实对某件事有把握时(方差很小)，我们希望在计算我们的估计值时给它一个很高的权重——精度告诉我们每个观察值包含多少信息。</p><p id="6146" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为一个更具体的例子，我们知道样本均值 s的<a class="ae lb" href="https://en.wikipedia.org/wiki/Standard_error" rel="noopener ugc nofollow" target="_blank">标准误差可以计算为</a></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/36af433a4aecc5df123d5173d4a58f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*qulE0F8S-ZGCkqy7b3Ws7Q.png"/></div></figure><p id="5848" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们对数据进行分组，并假设每个单独的观察值都来自共享相同方差的特定于组的正态分布，会怎么样？如果我们有三个组，我们可以计算全局平均值为</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/d5ac6e0e8b87278c3e31567248439d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*8UPqgSif2wo0QCMZiYXuJw.png"/></div></figure><p id="67a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，精确加权平均值与常规加权平均值完全相同。记住这种直觉。非正态分布的GLM中的W变得更复杂，但条目仍然是精确的。</p><p id="d96c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一步是用工作因变量z替换y。我们并不试图直接预测y，所以我们需要修改它以与链接函数一起工作。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi pb"><img src="../Images/eb7d43c8b6e59d1412f8b8f5c7feecfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWqAS5vRmumWbdZ-ou0_7A.png"/></div></div></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/c3a72c451abb0438db621ef3abf8abbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*6oGItXZ5cDadiBNPej8BFw.png"/></div></figure><p id="7730" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的e是实际预测误差。在我们的逻辑回归例子中，如果Xb = 1，那么我们的预测概率是inv.logit(1) = 0.73。如果我们观察到一个成功，那么e = 1 - 0.73 = 0.27。σ来自我们的选择分布。由于logistic回归使用的是二项式，所以σ = p(1 - p)。</p><p id="c400" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们假设正态分布有恒等环节g(x) = x呢？那么最后一项是</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/19c9920592cf2c4ee49c57eea91b4b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*58uHA-awbauneTQqyoRUrQ.png"/></div></figure><p id="d9d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也就是刚刚的<a class="ae lb" href="https://en.wikipedia.org/wiki/Standard_score" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">标准化公式</strong> </a>！所以我们的工作因变量是我们的预测值加上多少标准差就是我们的实际值。</p><p id="c3bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们配备了1)权重以获取加权平均值，以及2)一个“标准化”变量以获取平均值。</p><p id="7f10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一步是认识到<strong class="kh ir"> W和σ取决于预测值，反之亦然</strong>。上面两个方程看起来是递归的吧？事实上，当且仅当样本来自正态分布时，样本均值和样本方差才是独立的。这是一个深刻的陈述——除了正态分布，没有其他分布有这个性质，这就是为什么我们可以一步解决OLS。</p><p id="de0b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以把IRLS看作是一种期望最大化算法。我们保持预测值不变，并计算W和z。然后，保持W和z不变，以计算新的预测值。迭代直到收敛。</p><p id="55f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们有了。直观地说，GLM是一种以巧妙的方式进行精确加权平均的算法。</p><h2 id="d72d" class="ne ls iq bd lt nq nr dn lx ns nt dp mb ko nu nv md ks nw nx mf kw ny nz mh oa bi translated">诊断:异常</h2><p id="28a8" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">如果模型非常适合，则偏差，-2对数似然性(+一些被忽略的常数)应遵循χ分布。根据经验，如果偏差远远高于模型自由度，那么模型可能不太符合数据。处理分组数据时要小心，因为R会输出不正确的偏差，您应该手动计算。简单演示:</p><pre class="lk ll lm ln gt mz na nb nc aw nd bi"><span id="72bb" class="ne ls iq na b gy nf ng l nh ni">dat &lt;- data.frame(<br/>  y = 5:7 / 10,<br/>  n = rep(10, 3),<br/>  x = 1:3<br/>)<br/>model &lt;- glm(<br/>  y~x, <br/>  data = dat, <br/>  weights = n, <br/>  family = 'binomial'<br/>)<br/>summary(model)</span></pre><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi pa"><img src="../Images/4ca3149fc3d308e44df3549892abb857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*FJ42grp2Y2FbbWd64-Dudg.png"/></div></div></figure><p id="bc88" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">r错误地认为3行= 3个观察值，尽管我们有30个观察值。剩余偏差应该有28个自由度。</p><pre class="lk ll lm ln gt mz na nb nc aw nd bi"><span id="63ee" class="ne ls iq na b gy nf ng l nh ni">binom_deviance &lt;- -2 * sum(<br/>  log(choose(dat$n, dat$y * dat$n)) + <br/>    log(fitted(model)) * dat$y * dat$n + <br/>    log(1-fitted(model)) * (1-dat$y) * dat$n <br/>)<br/>pchisq(<br/>  q = binom_deviance, <br/>  df = sum(dat$n) - length(model$coefficients),<br/>  lower.tail = FALSE<br/>)</span></pre><p id="cfe4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为p值很大，为0.9999，所以我们无法拒绝模型非常适合的零假设。</p><h2 id="28bd" class="ne ls iq bd lt nq nr dn lx ns nt dp mb ko nu nv md ks nw nx mf kw ny nz mh oa bi translated">诊断:皮尔逊残差</h2><p id="344b" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">为了直观地检查GLM，我建议查看皮尔逊残差，即残差除以理论标准差，就像二项式的sqrt(p × (1-p))和泊松的sqrt(λ) <strong class="kh ir"> </strong>。因为方差取决于拟合值，所以这样做可以获得相同单位尺度上的所有残差，并且您可以使用标准正态的经验规则来观察。只有少数皮尔逊残差应该落在(-2，2)之外，极少情况下应该落在(-3，3)之外。</p><pre class="lk ll lm ln gt mz na nb nc aw nd bi"><span id="5f3a" class="ne ls iq na b gy nf ng l nh ni">set.seed(123)<br/>x_val &lt;- rnorm(100)<br/>y_poisson &lt;- sapply(<br/>  x_val, <br/>  function(x) rpois(n = 1, lambda = exp(x))<br/>)<br/>y_negbin &lt;- sapply(<br/>  x_val, <br/>  function(x) rnbinom(n = 1, size = 0.8, mu = exp(x))<br/>)</span><span id="e42e" class="ne ls iq na b gy pe ng l nh ni">model_correct &lt;- glm(y_poisson~x_val, family = 'poisson')<br/>model_wrong &lt;- glm(y_negbin~x_val, family = 'poisson')</span><span id="cd22" class="ne ls iq na b gy pe ng l nh ni">plot(<br/>  fitted(model_correct), <br/>  residuals(model_correct, type = 'pearson'),<br/>  main = 'Good fit',<br/>  ylab = 'Pearson residuals',<br/>  xlab = 'Fitted values'<br/>)</span></pre><div class="lk ll lm ln gt ab cb"><figure class="op lo oq or os ot ou paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/9cf52e605b094f337d47a3cf6ea07fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*M3dlD8Lr8JHseapf1JqPmg.jpeg"/></div></figure><figure class="op lo oq or os ot ou paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/ae7bc49524c1d3e25253388587a3b7ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*QdvVacnU0qKnt6jN9SGJMA.jpeg"/></div></figure></div><p id="d7ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一般来说，我们不希望皮尔逊残差遵循正态分布，因为误差项的条件分布通常是偏斜的，计数不能低于0，概率介于0和1之间。因此，一个正常的QQ图从表面上看是没有帮助的。</p><p id="4f5a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，如果你取皮尔逊残差的绝对值，如果模型非常适合，它们应该类似于半正态分布。一个快速而简单的方法是复制第二份皮尔逊残差，然后翻转符号，做一个普通的QQ图。</p><pre class="lk ll lm ln gt mz na nb nc aw nd bi"><span id="cae5" class="ne ls iq na b gy nf ng l nh ni">double_resid &lt;- c(<br/>  residuals(model_correct, type = 'pearson'),<br/>  -residuals(model_correct, type = 'pearson')<br/>)<br/>qqnorm(double_resid, main = 'Good fit')<br/>qqline(double_resid)</span></pre><div class="lk ll lm ln gt ab cb"><figure class="op lo oq or os ot ou paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/1c8f27b96d299a1690e4020105484e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*VfWY2xwITXUjlSPYOTQUtg.jpeg"/></div></figure><figure class="op lo oq or os ot ou paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/343b9130db2456ae986f500494324116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*I6enx6rJVT2OyGCAYvhysg.jpeg"/></div></figure></div><h2 id="25e0" class="ne ls iq bd lt nq nr dn lx ns nt dp mb ko nu nv md ks nw nx mf kw ny nz mh oa bi translated">假设检验:似然比检验</h2><p id="dae1" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">GLMs测试可以通过LRT完成。查看我的<a class="ae lb" rel="noopener" target="_blank" href="/the-most-important-statistical-test-dee01f4d50cf">上一篇文章</a>详细解释。基本思想是我们想要比较两个模型，一个嵌套在另一个中。较小模型的预测值是较大模型预测值的子集。系列、数据、链接功能等。必须都一样。我们将偏差差异与χ分布进行比较，df =估计的附加参数数量:</p><pre class="lk ll lm ln gt mz na nb nc aw nd bi"><span id="5445" class="ne ls iq na b gy nf ng l nh ni">dat &lt;- data.frame(<br/>  x1 = rnorm(10),<br/>  x2 = rnorm(10)<br/>)<br/>dat$y &lt;- dat$x1 + dat$x2 + rnorm(10)<br/>model_small &lt;- glm(y~x1, data = dat)<br/>model_large &lt;- glm(y~x1+x2, data = dat)</span><span id="0768" class="ne ls iq na b gy pe ng l nh ni">pchisq(<br/>  q = model_small$deviance - model_large$deviance,<br/>  df = model_small$df.residual - model_large$df.residual,<br/>  lower.tail = FALSE<br/>) # Likelihood Ratio Test</span><span id="71bd" class="ne ls iq na b gy pe ng l nh ni">summary(model_large)$coefficients # Wald test</span></pre><p id="aa4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，LRT计算H0的p值:coef(x2) = 0。它优于回归总结中包含的p值，后者是通过Wald测试得到的一个<em class="lf">方便的近似值</em>(根据回归公式，甚至可能无法回答正确的问题)。当二次近似法失败时，贝叶斯主义者可以进行类比。</p><h2 id="907d" class="ne ls iq bd lt nq nr dn lx ns nt dp mb ko nu nv md ks nw nx mf kw ny nz mh oa bi translated">实践中:正规化和交叉验证</h2><p id="0490" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">当你关心p值和无偏估计(在模型假设下)时，你使用香草GLM。但是如果你想让<strong class="kh ir">更好的预测</strong>，你可以使用<a class="ae lb" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">正则化</a> GLM。当你有比观测值更多的预测值(p &gt; n)时，你无论如何都不得不使用正则化，因为MLE不存在。</p><p id="20d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">谈论正规化是困难的，因为不同的软件包有不同的公式。虽然最终结果是等效的，但参数的定义是不同的(C或λ)。我将坚持使用R的glmnet文档。</p><p id="9b83" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GLM最大限度地减少偏差。正则化的GLM将偏差最小化(乘以一个常数，我将挥去)+一个惩罚项:</p><ul class=""><li id="a0e4" class="ob oc iq kh b ki kj kl km ko od ks oe kw of la og oh oi oj bi translated">拉索增加了L1规范作为惩罚。将系数的绝对值(截距除外)相加，并乘以常数λ。</li><li id="81d9" class="ob oc iq kh b ki ok kl ol ko om ks on kw oo la og oh oi oj bi translated">里奇增加了L2标准作为惩罚。你将系数的平方和(截距除外)乘以一个常数λ。</li><li id="8e25" class="ob oc iq kh b ki ok kl ol ko om ks on kw oo la og oh oi oj bi translated">elasticnet对这两种惩罚进行加权平均。你选一个常数α，罚项设为α × L1 + (1-α)/2 × L2。</li></ul><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi pf"><img src="../Images/71b6e449a5946ae5d9b41badf0c378b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fLSk3T3IuUiqNniJ-X5DNg.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">正则化的几何解释(<a class="ae lb" href="https://commons.wikimedia.org/wiki/File:L1_and_L2_balls.svg" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="22cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">L1范数鼓励稀疏性，可以将许多系数设置为0，而岭将系数向0收缩，但永远不会达到0。把λ项想象成设定一个“预算”，这样每个系数都不会花费太多(数量巨大)。</p><p id="ff09" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你需要标准化预测器(glmnet默认这样做)来公平地分配预算。如果一个预测值是g，而另一个是kg，那么给kg更大的预算可能更有意义。(我们能生产多少蛋糕？也许每公斤面粉+1，但每克糖+0.001。)标准化把所有的预测器放在同一个尺度上。</p><p id="e0d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有一点值得注意，很多人可能会忽略:通过正则化，您应该对所有级别的预测值进行编码。如果您有一个具有k个可能值的分类预测值，传统的哑编码将丢弃一个作为参考级别，然后创建k-1个指示列。通过正则化，<strong class="kh ir">你应该创建k个指示器列</strong>。拦截不会受到处罚。因为下降的电平被吸收到截距中，系数将根据下降的电平而变化，这是不希望的。</p><p id="9eb1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们仍然需要设置超参数λ和α。最简单的方法是对α使用网格搜索(因为它在0和1之间，所以很容易做到均匀间隔)，对λ使用k倍<a class="ae lb" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" rel="noopener ugc nofollow" target="_blank">交叉验证</a>。关于CV的具体细节可以在别处找到。</p><p id="d97b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我注意到统计学和计算机科学之间的一个主要区别是他们如何从CV中选择λ(或C)。Python中的实现返回产生最小目标函数(lambda.min)的λ，您没有选择。在R中，您可以在λ. min和最大λ之间进行选择，最大λ在最小值(λ. 1s)的一个标准误差内。</p><p id="7844" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据简约法则(<a class="ae lb" href="https://en.wikipedia.org/wiki/Occam%27s_razor" rel="noopener ugc nofollow" target="_blank">奥卡姆剃刀</a>)，一些人更喜欢λ. 1se，因为它产生一个更简单的模型，其性能与λ. min差不多。此外，λ. 1se往往更稳定。将数据重新随机化成k倍可以产生非常不同的λmin，但是更相似的λ1s</p><p id="e163" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有的谈话都会变得令人困惑，所以让我们装一个套索来看看它的作用。r代码:</p><pre class="lk ll lm ln gt mz na nb nc aw nd bi"><span id="941c" class="ne ls iq na b gy nf ng l nh ni">library(glmnet)</span><span id="aa63" class="ne ls iq na b gy pe ng l nh ni"># The 0+ makes Species not drop a reference level<br/>X &lt;- model.matrix(Sepal.Length~0+., data = iris)<br/>y &lt;- iris$Sepal.Length</span><span id="2449" class="ne ls iq na b gy pe ng l nh ni">set.seed(123)<br/>train &lt;- sample(1:nrow(X), floor(nrow(X) * 2/3))<br/>model_cv &lt;- cv.glmnet(X[train,], y[train])<br/>plot(model_cv)</span><span id="45cf" class="ne ls iq na b gy pe ng l nh ni">model_1se &lt;- glmnet(<br/>  X[train,], <br/>  y[train], <br/>  lambda = model_cv$lambda.1se<br/>)<br/>model_min &lt;- glmnet(<br/>  X[train,], <br/>  y[train], <br/>  lambda = model_cv$lambda.min<br/>)</span><span id="e14b" class="ne ls iq na b gy pe ng l nh ni">mse &lt;- function(pred, actual){<br/>  mean((actual - pred)^2)<br/>}</span><span id="146d" class="ne ls iq na b gy pe ng l nh ni"># while sometimes model_min yields lower MSE<br/># generally model_1se performs better in this example</span><span id="4bf8" class="ne ls iq na b gy pe ng l nh ni">mse(predict(model_1se, X[-train,]), y[-train]) # 0.09685109<br/>mse(predict(model_min, X[-train,]), y[-train]) # 0.1022131</span></pre><p id="83a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是的，你可以抱怨，因为我们用了虹膜数据集。我们想预测萼片。长度使用所有其他变量。plot()函数返回</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/d9a6000f886c812c2450031909514e94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*JCEU-4QHD2_KcmX4-BCmjg.jpeg"/></div></figure><p id="054c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里log(λmin)大约为-7，而log(λ1s)大约为-5，如虚线所示。随意尝试不同的种子值来比较MSE，但通常lambda.1se表现更好。(不知道为什么Python里这不是默认而不是lambda.min。手动编写交叉验证来获得lambda.1se是一件痛苦的事情。)</p><p id="36c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将数据分为训练集和测试集，以说明保留数据的性能，但在实践中，您应该<em class="lf">而不是</em>进行这种划分。来自CV的误差比来自测试集的误差更可信。对整个数据进行CV以选择λ，然后对整个数据集进行改装。</p><p id="43fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你们注意估计的系数:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/09de9af6d2fe978bc1e7e7b1746c859e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*EF7bvsuWLmD0EfZD17IgPQ.png"/></div></figure><p id="68e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">versicolor的系数被设置为0，因为它是中间值。拉索学会了吸收云芝，而不是其他物种，进入拦截。</p><p id="1239" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">直观地说，如果{virginica，versicolor，setosa}是{1，2，3}并且我们去掉了versicolor，那么MLE系数将是{-1，/，1}，并且它们收缩相等。但是如果我们去掉setosa，因为我们按字母顺序，那么MLE系数将是{-2，-1，/}，并且-2将比-1收缩得更多。这没有意义，因为数据和现实世界都没有改变。我们不希望预测依赖于我们手动降低的水平。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="caf5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经对GLMs有了很好的理解，我们将继续学习其他类型。我们将只简要介绍这些内容，因为它们是建立在我们之前讨论的基础之上的。</p><h1 id="78b8" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">计数数据:泊松和负二项式回归</h1><p id="b6cf" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">泊松使用对数链接，因此回归方程看起来像</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/d4f18c86583ac0aa76f6f3dae5088b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-tpxrTTSFvnVZwxOs2PkHg.png"/></div></figure><p id="bf9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如前所述，log链接的魔力允许我们采用ln()，即使计数可以是0。</p><p id="8a4c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有时，由于曝光单位不同，我们对速率而不是计数感兴趣。如果我们想模拟每个城市有多少人生病，仅从人口规模来看，一个有10万居民的城市应该比一个有1000居民的小城市多。</p><p id="e804" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在逻辑回归中，我们使用权重参数，但是在泊松回归中，权重实际上不应该被触及。相反，我们使用偏移量。其中y是事件数，我们重新排列回归方程:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ph"><img src="../Images/b17954fb2f5fd4ce872edfe77aaa5ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iECSs33noldYOQneY2EVbg.png"/></div></div></figure><p id="5f4c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">偏移仅仅意味着我们不估计斜率，而是将其固定为1，例如RHS中的ln(n)。在R中:</p><pre class="lk ll lm ln gt mz na nb nc aw nd bi"><span id="9676" class="ne ls iq na b gy nf ng l nh ni">glm(y~x+offset(log(n)), family = 'poisson')</span></pre><p id="7274" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在实践中，泊松回归是不够的。许多数据表现出过度分散，即观察值的方差高于泊松分布所表明的值。这可以通过检查皮尔逊残差来发现；它们中的许多将位于(-2，2)之外。</p><p id="2ec4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我建议先试试泊松回归。如果有过度分散的证据，尝试负二项式回归，它也使用对数链接，但方差有一个过度分散参数。在R中，这是glm.nb()函数。</p><p id="571f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture" rel="noopener ugc nofollow" target="_blank">负二项式</a>真的很工整。如果对泊松分布求平均，其中每个泊松分布的λ根据伽马分布而变化，则得到负二项分布。换句话说，我们明确地建模，即使两个单元可能具有相同的X，它们的λ <strong class="kh ir"> </strong>可能不同。这是解释过度离差的好方法，因为你有泊松和伽马的方差。</p><p id="77b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有时，你有比泊松或负二项式建议更多的零。如果你对数据生成过程有所了解，你可以使用<a class="ae lb" href="https://en.wikipedia.org/wiki/Zero-inflated_model" rel="noopener ugc nofollow" target="_blank">零膨胀回归</a>或栅栏模型(尽管这些在计算上很昂贵)。直觉参见<a class="ae lb" href="https://stats.stackexchange.com/questions/81457/what-is-the-difference-between-zero-inflated-and-hurdle-models" rel="noopener ugc nofollow" target="_blank">此螺纹</a>。</p><h1 id="4aad" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">列联表:对数线性模型</h1><p id="af99" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">如果我们有一组泊松随机变量，并且约束它们的总数是固定的，那么我们<a class="ae lb" href="https://online.stat.psu.edu/stat504/node/48/" rel="noopener ugc nofollow" target="_blank">得到多项式分布</a>。因此，即使列联表感觉应该被建模为多项式，泊松回归也可以完成这项工作。你应该高兴，因为处理多项式是一件痛苦的事。</p><p id="ac4c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对数线性模型是泊松回归的一个奇特术语，其中所有预测值都是分类的-在这种情况下，无论列联表中的类别是什么。我以前写过这方面的文章，所以如果你对更多感兴趣，请参考关于<a class="ae lb" rel="noopener" target="_blank" href="/the-most-important-statistical-test-dee01f4d50cf"> G-test </a>的部分。</p><h1 id="3325" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">分类结果:多项式回归</h1><p id="2bce" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">多项式回归是一种逻辑回归，它被扩展到预测二进制以上的结果。其实就是几个逻辑回归包在一起。在逻辑回归中，我们预测对数优势，其中优势是p/(1-p)。在多项式回归中，我们做同样的事情，除了选择一个参考类别来计算概率。</p><p id="00f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们有三个类别A、B和c。如果我们使用A作为参考类别，那么我们拟合两个逻辑回归模型:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/c7c4ea94eb7caf8a70436a0606811511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*Locwk4ePPuXC4lBawjLniw.png"/></div></figure><p id="9bcf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个模型仅针对响应为A或B的数据进行训练，第二个模型仅针对响应为A或c的数据进行训练。我们将预测值传递给<a class="ae lb" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax函数</a>以获得</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/de2dea54db10c853de7171f43f0176b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*AuGl8J7eMET_R3u_acVlqg.png"/></div></figure><p id="983a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1看起来很不合适，但请记住log(pA/pA) = log(1) = 0。</p><p id="f7b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">softmax函数只是一个逻辑函数，它可以推广到两个以上的类别。它经常被用来方便地将预测转化为概率，但在这里它不仅仅是方便:它是有意义的。假设除截距外没有其他预测值，我们观察到70 A、20 B和10 C，则:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/99af9fc793ebcbdb16c49307a158f830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*gXMjHTWsm6zSZ6eqnu-jZQ.png"/></div></figure><p id="41c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用MLE，选择哪个类别作为参考类别并不重要。对于正则化，如果你这样做就很重要了，所以正则化多项式回归通常不是作为一组逻辑回归来实现的。</p><p id="ee3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有这些关于sigmoid和softmax函数的讨论听起来都像是神经网络。事实上，最原始的神经网络是一堆与softmax函数捆绑在一起的逻辑回归，尽管自那以后该领域已经取得了很大进展。</p><h1 id="2e95" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">有序结果:有序回归</h1><p id="ab09" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">我个人认为有序回归很不直观。您希望将y编码为1，2，3，…，k，并希望预测y = i的概率，其中I是1到k之间的整数。我们使用累积链接函数:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/1d36f437d1129837d723d10dde8b6b32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*Xv50m2O3XA6TRo40OS7_oQ.png"/></div></figure><p id="c731" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的回归方程看起来像</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/e030f7da7325bbd8d1c8dd5053e85092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*rh3jBbd1VsUnrI8H-OABqg.png"/></div></figure><p id="d12a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我只上到k-1。记住P(y &gt; k)是0。在回归方程中，β<strong class="kh ir">T5不包括截距。我们用一个基本事实来做预测</strong></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/efb96115cd6fa54fa620dcd406d995a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Zp5zCcZZzfcunMc85G-cCA.png"/></div></figure><p id="7691" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并且从</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi pn"><img src="../Images/be4a49f474255167c16ea37081033ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qPwyYS3ysuFpUg46U0kM3Q.png"/></div></div></figure><p id="1675" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好的，那么这些等式是什么意思呢？我们正在拟合一个sigmoid函数来预测以X为条件的<a class="ae lb" href="https://en.wikipedia.org/wiki/Cumulative_distribution_function" rel="noopener ugc nofollow" target="_blank"> CDF </a>。在回归方程中，β对于所有I都是固定的，但θ会随着I而变化。当X固定时，我们拟合一个单一函数，而θ决定分界点。</p><p id="dcc4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">问题和直觉就在这里。概率总和必须为1。想象一根长度为1的棍子，我们在{1，2，3，4，5}中有一个序数变量y。我们想把棍子分成5段，每段的长度对应于落入某一类别的概率。最简单的解决方案是选择4个点进行切割:k-1θ。</p><h1 id="df00" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">更多资源</h1><p id="cf61" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">我是这个<a class="ae lb" href="https://online.stat.psu.edu/stat504/node/49/" rel="noopener ugc nofollow" target="_blank">课程网站</a>的忠实粉丝，它是我自学旅程中的好伙伴。和往常一样，如果你有改进的建议，请告诉我。</p></div></div>    
</body>
</html>