# 论文剖析:完形填空驱动的自我注意网络预训练:艾

> 原文：<https://towardsdatascience.com/paper-dissection-cloze-driven-pretraining-of-self-attention-networks-a91adb2d9b18?source=collection_archive---------45----------------------->

## 一种用转换器预训练语言模型的新方法。

[完形填空驱动的自我注意网络预训练](https://arxiv.org/pdf/1903.07785.pdf)是脸书人工智能研究所发布的一种新型语言预训练，旨在使用 transformer 模块获得改进的单词嵌入进行微调。由于这是一次论文剖析会议，如果读者需要更好地理解，我将尝试从直觉和数学两方面解释论文的主要思想。如果读者能同时打开这篇文章和报纸，并开始一起阅读，那就太好了。我将跳过摘要、介绍和相关工作部分，从第 3 部分开始。

# 先决条件

关于[变形金刚](https://arxiv.org/pdf/1706.03762.pdf)的知识。杰伊·阿拉姆马的这篇[博客](http://jalammar.github.io/illustrated-transformer/)和原始论文一起，对理解《变形金刚》非常有帮助。

# **3。双塔型号:**

在这一部分，本文揭示了他们的新颖的结构，以预训练单词嵌入。在直接跳到架构之前，让我们先建立这个预训练的直觉。

现在，什么是完形填空阅读？**完形填空阅读**是一种教学策略，要求用户从单词库中用正确的单词填空。例如，给定一个句子，**这是我的第一篇中文章，**预训练的思想是预测**我的，**给定**这是**和**第一篇中文章。**有道理？如果没有，不要担心。一会儿我会展示一个示意图，让你超级清楚。

让我们来看一下双塔的类比。现在，假设这两座塔是两个黑匣子。由于单词/令牌 **my** 在短语**和**之间，这是**和**，**这是**将前往左塔，而**第一中间文章**将前往右塔作为输入，以最终预测 **my。**左塔或前塔从左到右工作，这意味着给定**这是，**它试图预测**我的，**右塔或后塔从右到左工作，这意味着给定**文章介质第一，**它试图预测**我的。**句子的开头和结尾都附有 **< s >** 标记。由于两座塔的输入句子长度不同，需要进行屏蔽。

## 3.1 块状结构

![](img/aad1fca85a6a53b9e307de39e95052c7.png)

图 1:完形填空预训练的模型结构。来源:原始[论文](https://arxiv.org/pdf/1903.07785.pdf)

在这一节中，我将详细介绍双塔。这些塔是堆叠在一起的变压器解码器模块，如图 1 所示。绿色方块是前塔的一部分，蓝色方块是后塔的一部分。从给定的图中可以看出，给定前塔的 **< s >、a** 和后塔的 **c、<s>****b**需要最终预测。

本文使用了一种不同的使用 CNN 编码的单词嵌入。编码的细节可以在这里找到[。](https://arxiv.org/pdf/1508.06615.pdf)简而言之，给定的单词输入被分解成字符并生成字符嵌入。除此之外，还使用不同滤波器尺寸的 Conv1D 层，从而获得不同尺寸的输出。在此之后，应用最大时间池操作来获得单词的固定维度表示，将其提供给高速公路网络以获得最终的单词嵌入。这个过程如图 2 所示。

![](img/f4a015cd676015692f1eebc6c81673b7.png)

图 CNN 编码的细节。从文字输入到高速公路网是本文的相关内容。来源原文[论文](https://arxiv.org/pdf/1508.06615.pdf)。

完形填空预训练使用这种 CNN 编码来生成单词嵌入。固定正弦位置编码(如 transformer 论文中所述)与 CNN 编码的单词嵌入相加，以提供单词在句子中的相对位置。为了让读者更好地理解，我准备了模型架构的扩展版本。这如图 3 所示。CNN 编码+位置编码生成单词/令牌嵌入，用作转换器模块的输入。输入嵌入在两个塔之间共享。每层前后塔的块数相同。明确一下，**块 11** 表示第一层的第一块，**块 N3** 表示第 N 层的第三块。在预训练时，如果我们想要预测第 *i* 个令牌/单词(例如 **my** ，在转发塔中，他们屏蔽了所有在 *i* 之后的令牌，包括第 *i* 个令牌。类似地，在向后的塔中，它们屏蔽了在 *i* 之前的所有记号，包括第 *i* 个记号，使得模型在预测第 *i* 个记号时不会得到该记号的信息。

![](img/372fc89219e395e636a19b00ebf43c15.png)

图 3:完形填空预训练的扩展模型结构。

这里需要注意的一点是，在左边(绿色)侧，**模块 11** 仅从**s>T21 获取输入，**模块 12** 同时从**s<s>t**和**获取输入，这个**和**模块 13** 从**s>获取输入，这个**和**就是****

## 3.2 表示的组合

在本节中，我将讨论图 3 所示的**联合收割机**块。一旦两座塔建成，最后一层的输出将与自我关注层结合。在此之后，使用一个前馈模块，通过大小为 **V** (其中 V 是词汇大小)的 softmax 激活来预测正确的单词/令牌。

他们没有建议简单地馈送前向和后向塔的最后层的所有输出，而是在这样做之前使用掩蔽。屏蔽几乎与前面类似，即对于前向塔，屏蔽第 *i* 块之后的所有输出，包括第 *i* 块；对于后向塔，在预测第 *i* 令牌时，屏蔽第 *i* 块之前的所有输出，包括第 *i* 块。如果， **FL1 (** 同图 3 **中的绿色**区块 N1**)**和 **BL1 (** 同图 3 **中的蓝色**区块 N1**)**是最后一层区块 1 的输出(符号中的 L 和图 3 中的 N)，则 **FL1、FL2、…、FL(I-1)【T24 最终层的剩余块被屏蔽以预测第 *i* 个令牌，如图 3 所示。在自我关注层中， **FL(i-1)** 和 **BL(i+1)** 被求和并串接，分别作为基模型和大模型的查询向量。基于前向和后向块的其他未屏蔽的最终层输出来创建关键字和值向量。**

第 *i* 个模块的输出屏蔽在预训练期间完成，但在微调时屏蔽被移除，这表明测试结果有所改善。

# 4 微调

不同的过程用于不同的微调任务。

## 分类和回归任务

图 4 显示了单句任务的微调过程。变化是:(a)输入句子的所有记号都到每个塔。(b)移除具有 softmax 激活的前馈层。在这种情况下，语言模型的输出将是自我关注层的输出，即:**【批量大小 X 时间步长 X 关注度】。**由于所有输入的句子在开头和结尾都附加了记号 **< s >** ，所以很容易得到这两个记号的自我注意输出向量，并且将它们连接起来产生一个向量**2 * attention-dim(attention-dim = 1024****)**size**。**基本上，这两个令牌用于计算分类和回归任务的最终输出。如果这是一个分类问题，在级联向量的顶部，一个前馈层与 softmax 激活一起使用，以输出期望数量的类。对于回归，输出维度是 1，激活是线性的。

![](img/be975767f88c82413ea4b2729aa9ae50.png)

图 4:对单句任务进行微调的图示，其中第一个和最后一个标记的输出被提供给特定于任务的分类器(W)。最终组合层(梳状)的掩膜被移除，这导致基于所有前向和后向状态的表示

当输入是两个句子或问答对时，则在两个输入句子之间使用一个分隔符( **< sep >** )。所以，在自我关注层的输出连同 **< s >** token 向量，我们也得到了 **< sep >** token 的向量输出。在句子对问题中，不是仅连接 **< s >** 记号向量，而是连同 **< s >** 记号向量 **< sep >** 记号向量也被连接以产生 **3*attention-dim** 向量。现在在回归或分类问题的基础上，前馈层被用于描述单句任务。

## 结构化预测任务

对于命名实体识别，输入将是预先训练的语言模型的输出，但是没有仅在自我注意层的输入处的掩蔽。

## 无屏蔽

可以看出，在自关注层的输入处禁用掩蔽改善了测试结果。但它不建议在塔的最后一层之前禁用掩蔽。

# 结论

我跳过了其余部分，因为它们以非常简单易懂的方式描述了模型参数、数据集和结果。请在评论中告诉我，我是否应该将这篇文章扩展到实验设置和结果部分。