<html>
<head>
<title>XLM: Cross-Lingual Language Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XLM:跨语言语言模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/xlm-cross-lingual-language-model-33c1fd1adf82?source=collection_archive---------28-----------------------#2020-09-13">https://towardsdatascience.com/xlm-cross-lingual-language-model-33c1fd1adf82?source=collection_archive---------28-----------------------#2020-09-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="433c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解基于变压器的自监督架构</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/db3b3704db9cac867b0bb097fe01706d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qKnoK405q_7aNRzK"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">莱昂纳多·大久保俊郎在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1577" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像伯特这样的模特(<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">德夫林等人。艾尔。</a>)或 GPT ( <a class="ae ky" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">拉德福德等人。艾尔。在语言理解方面达到了最先进的水平。然而，这些模型仅针对一种语言进行预训练。最近，已经做出努力来减轻单语表示并建立通用的跨语言模型，该模型能够将任何句子编码到共享的嵌入空间中。</a></p><p id="ece6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将讨论由艾提出的论文<a class="ae ky" href="https://arxiv.org/abs/1901.07291" rel="noopener ugc nofollow" target="_blank"/>。作者提出了两种跨语言语言建模的方法:</p><ol class=""><li id="181f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">无人监管，依赖单语数据</li><li id="1f17" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">受监督，依赖并行数据。</li></ol><h1 id="f1d2" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">跨语言语言模型(XLM)</h1><p id="492b" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在本节中，我们将讨论训练 XLM 的建议方法。</p><h2 id="1757" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">共享子词词汇</h2><p id="1abc" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">该模型对所有语言使用相同的共享词汇表。这有助于为所有语言的标记建立一个公共的嵌入空间。因此，很明显，具有相同文字(字母表)或相似单词的语言更好地映射到这个公共嵌入空间。</p><p id="3799" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了对语料库进行标记，使用了字节对编码(BPE)。</p><h2 id="d84e" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">因果语言建模(CLM)</h2><p id="ff72" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">这是常规的语言建模目标，其中我们最大化一个标记<strong class="lb iu"> <em class="ns"> x_t </em> </strong>出现在给定序列中第'<strong class="lb iu"> <em class="ns"> t </em> </strong>'个位置的概率，给定该序列中的所有标记<strong class="lb iu"> <em class="ns"> x_ &lt; t </em> </strong>(在第'<strong class="lb iu"> <em class="ns"> t </em> </strong>个标记之前的所有标记)。即</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/2a3f934044e713758ba8b2742a502a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/0*WJPu8-1GzXRQrSSm.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet 论文</a>进行因果语言建模</p></figure><p id="fabb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">OpenAI 的 GPT 和 GPT-2 就是为此目的而训练的。如果你对这个目标的细节感兴趣，可以参考我写的关于<a class="ae ky" href="https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4" rel="noopener"> GPT </a>和<a class="ae ky" href="https://medium.com/swlh/openai-gpt-2-language-models-are-multitask-learners-1c6d42d406ae" rel="noopener"> GPT-2 </a>的文章。</p><h2 id="3eb6" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">掩蔽语言建模(MLM)</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/85430a69b6a025f19dc6dd0acb27f50d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ekN7wGw3QjcF4R_VAjfybQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MLM 经<a class="ae ky" href="https://arxiv.org/abs/1901.07291" rel="noopener ugc nofollow" target="_blank"> XLM 纸</a></p></figure><p id="e292" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一种去噪自动编码目标，也称为完形填空任务。这里，我们最大化给定屏蔽记号<strong class="lb iu"> <em class="ns"> x_t </em> </strong>出现在给定序列中第‘<strong class="lb iu"><em class="ns">t</em></strong>位置的概率，给定该序列中的所有记号，<strong class="lb iu"> <em class="ns"> x_hat </em> </strong>。即</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/0a8c6c140fb3a4366a2c96c5ff525011.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/0*colEBede8-dNFu0w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet 论文</a>进行屏蔽语言建模</p></figure><p id="e70e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">伯特和罗伯塔接受过这方面的训练。如果你对这个目标的细节感兴趣，你可以参考我写的关于伯特和罗伯塔的文章。</p><p id="66fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，伯特和 XLM 的方法之间的唯一区别是，伯特使用成对的句子，而 XLM 使用任意数量的句子流，一旦长度为 256 就截断。</p><h2 id="e6fe" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">翻译语言建模(TLM)</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/5a0af98e6ee89b23d616cc62a3d4d8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EcaP5C5UhcCqkeNqvYscTA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TLM 经<a class="ae ky" href="https://arxiv.org/abs/1901.07291" rel="noopener ugc nofollow" target="_blank"> XLM 纸</a></p></figure><p id="9de4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CLM 和 MLM 的任务在单语语料库上工作良好，然而，它们没有利用可用的平行翻译数据。因此，作者提出了一个翻译语言建模目标，其中我们从翻译数据中提取一系列平行句子，并从源句子和目标句子中随机屏蔽标记<strong class="lb iu">。例如，在上图中，我们有来自英语和法语句子的屏蔽词。<strong class="lb iu">序列中的所有单词都有助于预测给定的屏蔽单词</strong>，从而在记号之间建立跨语言映射。</strong></p><h2 id="fe70" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">XLM</h2><blockquote class="nx ny nz"><p id="7c06" class="kz la ns lb b lc ld ju le lf lg jx lh oa lj lk ll ob ln lo lp oc lr ls lt lu im bi translated">在这项工作中，我们考虑了跨语言语言模型预处理与 CLM，MLM，或 MLM 结合 TLM 使用。</p><p id="6cff" class="kz la ns lb b lc ld ju le lf lg jx lh oa lj lk ll ob ln lo lp oc lr ls lt lu im bi translated">— <a class="ae ky" href="https://arxiv.org/abs/1901.07291" rel="noopener ugc nofollow" target="_blank"> XLM 纸业</a></p></blockquote><h1 id="645c" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">XLM 预培训</h1><p id="1f77" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在本节中，我们将讨论如何在下游任务中利用 XLM 预培训，例如:</p><ol class=""><li id="968a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">零镜头跨语言分类</li><li id="7bbb" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">监督和非监督神经机器翻译</li><li id="fd99" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">低资源语言的语言模型</li><li id="0fe9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">无监督的跨语言单词嵌入</li></ol><h2 id="94dd" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">零镜头跨语言分类</h2><p id="ce80" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">就像在任何其他基于 Transformer 的单语模型中一样，XLM 也在 XNLI 数据集上进行了微调，以获得跨语言分类。</p><p id="0290" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将分类图层添加到 XLM 的顶部，并在英语 NLI 训练数据集上对其进行训练。然后在 15 种 XNLI 语言上对该模型进行了测试。</p><p id="3d1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于<strong class="lb iu">模型还没有被调整来对来自这些语言</strong>的句子进行分类，所以它是一个零尝试的学习例子。</p><h2 id="d78f" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">无人监管的 NMT</h2><p id="80c6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">对于这项任务，作者建议<strong class="lb iu">以跨语言语言建模为目标，预训练一个完整的编码器-解码器</strong>架构。该模型在几个翻译基准上进行评估，包括 WMT 的 14 英-法，WMT 的 16 英-德和 WMT 的 16 英-罗。</p><h2 id="503c" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">监督 NMT</h2><p id="b33c" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">这里，<strong class="lb iu">编码器和解码器加载了来自 XLM </strong>的预训练权重，然后在监督翻译数据集上进行微调。这实质上实现了多语言的语言翻译。</p><p id="a578" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更多关于多语言的 NMT 的信息，请参考这篇博客。</p><h2 id="3a07" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">低资源语言建模</h2><p id="f3cc" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">这就是“具有相同脚本或相似单词的语言提供更好的映射”的原因。例如，维基百科上用尼泊尔语写的句子有 10 万个，用印地语写的句子大约多 6 倍。此外，这些语言有 80%的标记是相同的。</p><p id="fac2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，跨语言语言模型将明显有益于尼泊尔语的语言模型，因为它是在相对更多的相似对应数据上训练的。</p><h2 id="abcd" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">无监督的跨语言单词嵌入</h2><p id="6f70" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">最后，由于我们有一个共享的词汇表，XLM 模型的<strong class="lb iu">查找表</strong>(或嵌入矩阵)给了我们跨语言的单词嵌入。</p><h1 id="d981" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结论</h1><p id="4450" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在本文中，我们讨论了跨语言语言模型不仅有利于在一般的下游任务中获得更好的结果，而且有利于通过对类似的高资源语言进行训练来提高低资源语言的模型质量，从而获得更多相关数据。</p><p id="8add" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个链接指向最初的 XLM GitHub 库。</p><p id="c8ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://huggingface.co/transformers/model_doc/xlm.html" rel="noopener ugc nofollow" target="_blank">这里有一个链接</a>链接到 huggingface 的 XLM 架构实现和预训练权重。</p><h1 id="3852" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><div class="od oe gp gr of og"><a href="https://arxiv.org/abs/1901.07291" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">跨语言语言模型预训练</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">最近的研究证明了生成性预训练对英语自然语言理解的有效性</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">arxiv.org</p></div></div></div></a></div></div></div>    
</body>
</html>