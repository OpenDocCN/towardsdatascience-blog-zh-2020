<html>
<head>
<title>Machine Learning in Apache Spark for Beginners — Healthcare Data Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向初学者的 Apache Spark 中的机器学习—医疗保健数据分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-in-apache-spark-for-beginners-healthcare-data-analysis-diabetes-276156b97e92?source=collection_archive---------36-----------------------#2020-02-04">https://towardsdatascience.com/machine-learning-in-apache-spark-for-beginners-healthcare-data-analysis-diabetes-276156b97e92?source=collection_archive---------36-----------------------#2020-02-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/5edbb5a027e6f1d93bd7b6abc6b56394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*faNZ9T2l-Xp7t8Lh6uk9PA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片由<a class="ae jd" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=895567" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的 Gerd Altmann 提供</p></figure><h2 id="d795" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph">大数据/数据科学/教程/指南/ Apache Spark /机器学习</h2><div class=""/><blockquote class="km"><p id="819c" class="kn ko jg bd kp kq kr ks kt ku kv kw dk translated">使用 Databricks 在 Apache Spark 中构建第一个机器学习模型的分步指南</p></blockquote><h1 id="b671" class="kx ky jg bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">简介:</h1><p id="83d5" class="pw-post-body-paragraph lv lw jg lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr kw ij bi translated">Apache Spark 是一个集群计算框架，旨在实现快速高效的计算。它可以用相对较低的计算能力处理数百万个数据点。Apache Spark 构建在 Hadoop 的 Map-Reduce 之上，是其扩展，可以有效地使用不同的集群计算组合。Spark 的主要特性是内存集群计算，它提高了应用程序的速度，包括交互式查询和流处理。</p><p id="1e81" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">这篇文章是使用数据块在 Spark 中开发预测模型的快速入门指南。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/d29fdc454f5ddbdd5915541fcaa631fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*95EWGufoluQ1ymU_"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://databricks.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nc">图:数据块:统一数据分析</strong> </a></p></figure><p id="08f2" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">我将使用 Databricks 的免费社区版本，感谢他们！</p><h1 id="e79c" class="kx ky jg bd kz la lb lc ld le lf lg lh li nd lk ll lm ne lo lp lq nf ls lt lu bi translated">关于数据/背景信息:</h1><p id="70ba" class="pw-post-body-paragraph lv lw jg lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr kw ij bi translated">在这篇文章中，我将使用机器学习来帮助我们预测患者患糖尿病的概率。数据集是从<a class="ae jd" href="http://archive.ics.uci.edu/ml/index.php" rel="noopener ugc nofollow" target="_blank">UCI 机器学习库</a>下载的。</p><p id="d00b" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">在这里，我使用提供的关于患者的信息来预测糖尿病的概率。这是一个二元分类问题，我将尝试预测属于糖尿病类别的观察值的概率。</p><p id="8218" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">我将首先演示最少量的探索性分析，然后跳转到机器学习模型(即回归和基于树的模型)，并比较和总结结果。</p><h1 id="11d9" class="kx ky jg bd kz la lb lc ld le lf lg lh li nd lk ll lm ne lo lp lq nf ls lt lu bi translated">数据预处理和探索；</h1><p id="846b" class="pw-post-body-paragraph lv lw jg lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr kw ij bi translated">以下代码行加载数据并创建 dataframe 对象。将<strong class="lx jq"> Inferschema </strong>设置为<strong class="lx jq"> true </strong>可以很好地猜测每一列的数据类型。</p><pre class="my mz na nb gt ng nh ni nj aw nk bi"><span id="95c2" class="nl ky jg nh b gy nm nn l no np">#The Applied options are for CSV files<br/>df = spark.read.format("csv") \<br/>     .option("inferSchema","true") \<br/>     .option("header","true") \<br/>     .option("sep",",") \<br/>     .load(file_location)</span></pre><p id="6314" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">我还创建了一个字典来存储与数据类型相关的特性。在我们的例子中，我们有一个“整型”和“双精度型”。</p><pre class="my mz na nb gt ng nh ni nj aw nk bi"><span id="0870" class="nl ky jg nh b gy nm nn l no np">from collections import defaultdict</span><span id="e5b3" class="nl ky jg nh b gy nq nn l no np">data_types = defaultdict(list)<br/>for entry in df.schema.fields:<br/>  data_types[str(entry.dataType)].append(entry.name)</span></pre><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/3316711cb03dbdc6d57b68f62b079355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/0*7Jb6omnFe2n26PKo"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">查看列及其数据类型的代码输出</p></figure><p id="9c5c" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">让我们看看数据集的前 5 行。</p><pre class="my mz na nb gt ng nh ni nj aw nk bi"><span id="b9b6" class="nl ky jg nh b gy nm nn l no np">display(df.limit(5))</span></pre><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/fd6ad2c16d6d1f4f22591831e7d5b26d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kQljNqY7HdiyEv6HiI24Og.png"/></div></div></figure><p id="c914" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">糖尿病数据集由 768 个数据点组成，每个数据点有 9 个特征:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/1101ae3272d332220310d1657eb212b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wWfhi2JCqJqs3vlr"/></div></div></figure><p id="4e01" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">“结果”是我们要预测的特征，其中 0 表示患者没有糖尿病，1 表示患者确实有糖尿病。在这 768 个数据点中，500 个标记为 0，268 个标记为 1。</p><pre class="my mz na nb gt ng nh ni nj aw nk bi"><span id="dc76" class="nl ky jg nh b gy nm nn l no np">display(df.groupby('Outcome').count())</span></pre><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/d0164a0bed3a26c713663cca3e4990ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9XTztgvQ2u57QFy4u7cZg.png"/></div></div></figure><p id="651f" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">使用 Databricks 的一个优点是，它有助于将查询可视化为一些基本的绘图选项，以便更好地理解数据和代码。</p><p id="569d" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">我们有一个完整的数据集，没有任何缺失值，但是要找到更多关于处理缺失数据的信息，您可以参考这篇文章:</p><p id="35ed" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated"><a class="ae jd" href="https://www.analyticsvidhya.com/blog/tag/missing-values-treatment/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/tag/missing-values-treatment/</a></p><h1 id="943b" class="kx ky jg bd kz la lb lc ld le lf lg lh li nd lk ll lm ne lo lp lq nf ls lt lu bi translated">处理分类数据:</h1><p id="3ec7" class="pw-post-body-paragraph lv lw jg lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr kw ij bi translated">在我们的数据中，我们只有一个分类列，即超过 17 个类别的“怀孕”。下面的代码显示了如何将分类列/特征转换为一键编码。在 Spark 中，使用“字符串索引器”为每个类别分配一个唯一的整数值。0 被分配给最频繁的类别，1 被分配给下一个最频繁的类别，依此类推。</p><pre class="my mz na nb gt ng nh ni nj aw nk bi"><span id="6fa2" class="nl ky jg nh b gy nm nn l no np">from pyspark.ml import Pipeline<br/>from pyspark.ml.feature import OneHotEncoder, StringIndexer</span><span id="4813" class="nl ky jg nh b gy nq nn l no np">stage_string = [StringIndexer(inputCol= c, outputCol=<br/>        c+"_string_encoded") for c in strings_used]<br/>stage_one_hot = [OneHotEncoder(inputCol= c+"_string_encoded",<br/>        outputCol= c+ "_one_hot") for c in strings_used]</span><span id="bd0a" class="nl ky jg nh b gy nq nn l no np">ppl = Pipeline(stages= stage_string + stage_one_hot)<br/>df = ppl.fit(df).transform(df)</span></pre><p id="eff1" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">在上面的代码中，我使用了一个管道，它可以在一次迭代中有效地处理一系列任务。人们可以列出任务清单，管道会处理所有事情。</p><p id="bdeb" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">一般来说，机器学习管道描述了编写代码、将其发布到生产、进行数据提取、创建训练模型和调整算法的过程。在 ML 平台上工作是一个连续的过程。但是对于 Apache Spark 来说，管道是一个将步骤转换、评估和装配到一个对象中的对象。这些步骤被称为 ml 工作流。</p><h1 id="1ef4" class="kx ky jg bd kz la lb lc ld le lf lg lh li nd lk ll lm ne lo lp lq nf ls lt lu bi translated">向量汇编程序:</h1><p id="8a17" class="pw-post-body-paragraph lv lw jg lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr kw ij bi translated">这里的想法是将一个给定的列列表组装成一个向量列，并将它们捆绑在一起。这是 Spark 的机器学习模型所需的额外步骤。这一步骤通常在数据探索和预处理步骤结束时执行。在这个阶段，我正在使用一些原始的和一些转换的特征来训练一个模型。</p><pre class="my mz na nb gt ng nh ni nj aw nk bi"><span id="d642" class="nl ky jg nh b gy nm nn l no np">from pyspark.ml.feature import VectorAssembler</span><span id="4116" class="nl ky jg nh b gy nq nn l no np">features = ['Pregnancies_one_hot','Glucose','BloodPressure',<br/>'SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']</span><span id="ea4b" class="nl ky jg nh b gy nq nn l no np">vector_assembler = VectorAssembler(inputCols = features, <br/>                                   outputCol= "features")<br/>data_training_and_test = vector_assembler.transform(df)</span></pre><h1 id="d2cc" class="kx ky jg bd kz la lb lc ld le lf lg lh li nd lk ll lm ne lo lp lq nf ls lt lu bi translated">模型拟合:</h1><p id="e231" class="pw-post-body-paragraph lv lw jg lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr kw ij bi translated">我们有几个内置的分类器，包括随机森林、提升树、逻辑回归等。首先，作为一个例子，我实现了 Random Forest，指定了分类器中树的数量，并将其余参数保留为默认值。</p><p id="9e2b" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">为了评估我们模型的性能，我使用 ROC 曲线度量。您可以选择自己喜欢的“metricName”。</p><p id="c52f" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">该模型的准确率为 82.5%。这表明我们的模型在默认参数下运行良好。</p><pre class="my mz na nb gt ng nh ni nj aw nk bi"><span id="8a9a" class="nl ky jg nh b gy nm nn l no np">from pyspark.ml.classification import RandomForestClassifier<br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator</span><span id="94ca" class="nl ky jg nh b gy nq nn l no np">(training_data, test_data) = data_training_and_test.randomSplit([0.7, 0.3], 2017)<br/>rf = RandomForestClassifier(labelCol = "Outcome", <br/>                        featuresCol = "features", numTrees = 20)<br/>rf_model = rf.fit(training_data)</span><span id="fae3" class="nl ky jg nh b gy nq nn l no np">predictions = rf_model.transform(test_data)<br/>evaluator= BinaryClassificationEvaluator(labelCol = "Outcome", rawPredictionCol="probability", metricName= "areaUnderROC")<br/>accuracy = evaluator.evaluate(predictions)</span><span id="c9ff" class="nl ky jg nh b gy nq nn l no np">print("Accuracy:",accuracy*100)</span></pre><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/2a5a56529aa8fb2b2679509c886e7833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pGE5kkc8vj80MzppY53q4g.png"/></div></div></figure><h1 id="fbb9" class="kx ky jg bd kz la lb lc ld le lf lg lh li nd lk ll lm ne lo lp lq nf ls lt lu bi translated">功能选择:</h1><p id="2d9b" class="pw-post-body-paragraph lv lw jg lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr kw ij bi translated">特征选择过程有助于过滤掉不太重要的变量，从而得到更简单、更稳定的模型。在 Spark 中，实现特性选择不像在 Python 的 scikit-learn 中那样简单，但是可以通过将特性选择作为管道的一部分来管理。这个想法是:</p><ol class=""><li id="f10f" class="nw nx jg lx b ly ms mc mt mg ny mk nz mo oa kw ob oc od oe bi translated">首先安装分类器。例如，您可以选择回归模型或基于树的模型，任何您选择的模型。</li><li id="225c" class="nw nx jg lx b ly of mc og mg oh mk oi mo oj kw ob oc od oe bi translated">如果使用随机森林，则查找要素重要性；如果使用逻辑回归，则查找系数。</li><li id="c20a" class="nw nx jg lx b ly of mc og mg oh mk oi mo oj kw ob oc od oe bi translated">将最重要的功能集存储在列表中。</li><li id="78a7" class="nw nx jg lx b ly of mc og mg oh mk oi mo oj kw ob oc od oe bi translated">从<strong class="lx jq"> ml </strong>库中使用'<strong class="lx jq"> VectorSlicer </strong>'方法，并从刚才选择的列表中创建一个新的 vector。</li></ol><p id="3f5a" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">下面的代码显示了如何从我们之前拟合的模型中创建一个重要特性的列表。大于 0.03 的特征被保留，rf_model 是拟合的随机森林模型。</p><pre class="my mz na nb gt ng nh ni nj aw nk bi"><span id="b584" class="nl ky jg nh b gy nm nn l no np">importance_list = pd.Series(rf_model.featureImportances.values)<br/>sorted_imp = importance_list.sort_values(ascending= False)<br/>kept = list((sorted_imp[sorted_imp &gt; 0.03]).index)</span></pre><p id="538d" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">取 0.03 是随机的，可以基于 AUC 度量尝试不同的值。后来我用矢量切片器收集所有重要性大于 0.03 的特征。</p><pre class="my mz na nb gt ng nh ni nj aw nk bi"><span id="2c30" class="nl ky jg nh b gy nm nn l no np">from pyspark.ml.feature import VectorSlicer<br/>vector_slicer = VectorSlicer(inputCol= "features", <br/>                         indices= kept, outputCol= "feature_subset")<br/>with_selected_feature = vector_slicer.transform(training_data)</span><span id="b596" class="nl ky jg nh b gy nq nn l no np">rf_modified = RandomForestClassifier(numTrees=20,<br/>                labelCol = "Outcome", featuresCol="feature_subset")<br/>test_data = vector_slicer.transform(test_data)<br/>prediction_modified = rf_modified.fit(with_selected_feature)<br/>                                 .transform(test_data)</span><span id="4a8c" class="nl ky jg nh b gy nq nn l no np">evaluator_modified = BinaryClassificationEvaluator(labelCol = "Outcome",rawPredictionCol="probability", metricName= "areaUnderROC")<br/>accuracy = evaluator_modified.evaluate(prediction_modified)</span><span id="c2bf" class="nl ky jg nh b gy nq nn l no np">print("Accuracy: ",accuracy*100)</span></pre><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/23781fd1c446286efe3bedd46aa3e19d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZ9rH0AQ3MEeCwO0ToPOJA.png"/></div></div></figure><p id="c7dd" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">通过特征选择，我们看到准确率提高了 1%,总体准确率为 83%。从完整的特征集中，我们得到了 82%的准确率。在处理大数据时，即使 1%的改进也很重要。</p><p id="1838" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">最后，我只想说，Apache Spark 是一个简洁易用的开源框架。</p></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><p id="322f" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">感谢您抽出时间阅读！</p><p id="0bf6" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">我总是期待着学习和成长，如果您有任何问题或建议，请联系我！</p><p id="d635" class="pw-post-body-paragraph lv lw jg lx b ly ms ma mb mc mt me mf mg mu mi mj mk mv mm mn mo mw mq mr kw ij bi translated">领英|<em class="os">sagardaswani1703@gmail.com</em>|<a class="ae jd" href="https://github.com/Sagar401" rel="noopener ugc nofollow" target="_blank">Github</a></p></div></div>    
</body>
</html>