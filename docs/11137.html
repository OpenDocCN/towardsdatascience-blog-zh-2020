<html>
<head>
<title>Object Detection in 6 steps using Detectron2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用探测器 2 分 6 步进行物体探测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-detection-in-6-steps-using-detectron2-705b92575578?source=collection_archive---------2-----------------------#2020-08-03">https://towardsdatascience.com/object-detection-in-6-steps-using-detectron2-705b92575578?source=collection_archive---------2-----------------------#2020-08-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b6ba" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让我们看看如何使用 FAIR(脸书人工智能研究所)的 Detectron 2 对涉及文本识别的自定义数据集进行实例检测。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/844b231bd3b814beaa652080a7786216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2c5DytyDMLlYma3nqYCbLg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用 Canva 设计</p></figure><p id="f158" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你有没有尝试过使用自己选择的自定义数据集从头开始训练一个对象检测模型？</p><p id="14d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果是的话，你就知道这个过程有多乏味了。如果我们选择基于区域建议的方法，如更快的 R-CNN，或者我们也可以使用一次性检测器算法，如 SSD 和 YOLO，我们需要从使用特征金字塔网络结合区域建议网络构建模型开始。</p><p id="fbaf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们想从头开始实现它，这两种方法都有点复杂。我们需要一个框架，在这个框架中，我们可以轻松地使用最先进的模型，如 Fast、Faster 和 Mask R-CNN。然而，重要的是至少尝试一次从头构建模型，以理解其背后的数学原理。</p><p id="3808" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们想用自定义数据集快速训练一个对象检测模型，Detectron 2 就能帮上忙。Detectron 2 库的<a class="ae lu" href="https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md" rel="noopener ugc nofollow" target="_blank">模型动物园</a>中的所有模型都在 COCO 数据集上进行了预训练。我们只需要在预训练的模型上微调我们的自定义数据集。</p><p id="6fc2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Detectron 2 是对 2018 年发布的第一款 Detectron 的完全重写。前任是在 Caffe2 上写的，这是一个深度学习框架，也是由脸书支持的。Caffe2 和 Detectron 现在都被弃用了。Caffe2 现在是 PyTorch 的一部分，后继者 Detectron 2 完全写在 PyTorch 上。</p><blockquote class="lv lw lx"><p id="5928" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated">Detectron2 旨在通过提供快速培训和解决公司从研究走向生产时面临的问题来推进机器学习。</p></blockquote><p id="e2d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些是 Detectron 2 提供的各种类型的对象检测模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/eee10dd2c7055d0fda539ae657a0cfdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAsdGoLsEcZ_XUdv957rcQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">【https://research.fb.com/wp-content/uploads/2019/12/4. T4】-detectron2.pdf </p></figure></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="0ba2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们直接进入<strong class="la iu">实例检测</strong>。</p><p id="921a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实例检测是指对周围有边界框的对象进行分类和定位。在本文中，<strong class="la iu">我们将使用 Detectron 2 的模型动物园中更快的 RCNN 模型来处理从图像中识别文本语言的问题。</strong></p><p id="388c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，我们将把我们的语言限制为 2。</p><p id="b349" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们标识了印度语文本和英语文本，并为其他语言添加了一个标记为其他的类。</p><div class="kj kk kl km gt ab cb"><figure class="mk kn ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/10880480bfe29093a56e49f1f1c4223b.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*oBnLI6bZocr9qOkkoaYDtw.png"/></div></figure><figure class="mk kn mq mm mn mo mp paragraph-image"><img src="../Images/eda14480b28d6015bb48b2ca0cc08d63.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*xvmwAXjnkN7kY118HjNT7A.png"/><p class="ku kv gj gh gi kw kx bd b be z dk mr di ms mt translated">Colab 的最终结果</p></figure></div><p id="be9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将实现一个输出如下的模型。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="af29" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们开始吧！</p><p id="6ec8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用 Detectron 2，可以使用七个步骤对任何自定义数据集执行对象检测。所有的步骤都在这个<a class="ae lu" href="https://github.com/aakarsh7599/Text-Detection-using-Detectron2" rel="noopener ugc nofollow" target="_blank"> Google Colab 笔记本</a>中，你可以直接运行它！</p><p id="92e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用 Google Colab 将是一项简单的任务，因为我们可以使用 GPU 进行更快的训练。</p><h2 id="168b" class="mu mv it bd mw mx my dn mz na nb dp nc lh nd ne nf ll ng nh ni lp nj nk nl nm bi translated">步骤 1:安装 Detectron 2</h2><p id="0f90" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">从安装一些依赖项开始，比如 Torch Vision 和 COCO API，然后检查<strong class="la iu"> CUDA </strong>是否可用。CUDA 帮助跟踪当前选择的<strong class="la iu"> GPU </strong>。然后安装 Detectron2。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="8aa3" class="mu mv it nt b gy nx ny l nz oa"># install dependencies: <br/>!pip install -U torch==1.5 torchvision==0.6 -f <a class="ae lu" href="https://download.pytorch.org/whl/cu101/torch_stable.html" rel="noopener ugc nofollow" target="_blank">https://download.pytorch.org/whl/cu101/torch_stable.html</a><br/>!pip install cython pyyaml==5.1<br/>!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'</span><span id="baff" class="mu mv it nt b gy ob ny l nz oa">import torch, torchvision<br/>print(torch.__version__, torch.cuda.is_available())<br/>!gcc --version</span><span id="4cbe" class="mu mv it nt b gy ob ny l nz oa"># install detectron2:<br/>!pip install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html</span></pre><h2 id="f1fa" class="mu mv it bd mw mx my dn mz na nb dp nc lh nd ne nf ll ng nh ni lp nj nk nl nm bi translated">步骤 2:准备和注册数据集</h2><p id="d1d2" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">导入一些必要的包。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="823e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 detectron2 中有内置支持的数据集在<a class="ae lu" href="https://detectron2.readthedocs.io/tutorials/builtin_datasets.html" rel="noopener ugc nofollow" target="_blank">内置数据集</a>中列出。如果您想在重用 detectron2 的数据加载器的同时使用自定义数据集，您将需要<strong class="la iu">注册</strong>您的数据集(即，告诉 detectron2 如何获取您的数据集)。</p><p id="1b9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们使用我们的文本检测数据集，它有三个类:</p><ol class=""><li id="fc6f" class="oe of it la b lb lc le lf lh og ll oh lp oi lt oj ok ol om bi translated">英语</li><li id="dc42" class="oe of it la b lb on le oo lh op ll oq lp or lt oj ok ol om bi translated">印地语</li><li id="b1e4" class="oe of it la b lb on le oo lh op ll oq lp or lt oj ok ol om bi translated">其他人</li></ol><p id="1583" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将从现有的在 COCO 数据集上预训练的模型中训练一个文本检测模型，该数据集可在 detectron2 的模型动物园中获得。</p><p id="12cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您有兴趣了解从数据集的原始格式到 Detectron 2 接受的格式的转换，请查看这个<a class="ae lu" href="https://colab.research.google.com/drive/1q-gwQteO79r8sX59oYnHYCNtP9zXWFPN?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab 笔记本</a>。</p><p id="42ea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据输入模型的方式有一定的格式，如 YOLO 格式、PASCAL VOC 格式、COCO 格式等。Detectron2 接受数据集的 COCO 格式。数据集的 COCO 格式由 JSON 文件组成，该文件包括图像的所有细节，例如大小、注释(即，边界框坐标)、对应于其边界框的标签等。举个例子，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/ed7ab08cb4730e91df1d04687132b42b.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*wleaRUAKGGwxe3YMcKWRbA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Detectron 2 的数据集格式</p></figure><p id="0df2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一张图片的 JSON 外观。边界框表示有不同类型的格式。它必须是<a class="ae lu" href="https://detectron2.readthedocs.io/modules/structures.html#detectron2.structures.BoxMode" rel="noopener ugc nofollow" target="_blank">结构的成员。探测器 2 的 BoxMode </a>。有 5 种这样的格式。但目前支持<strong class="la iu"> BoxMode。XYXY_ABS，BoxMode。XYWH_ABS </strong>。我们使用第二种格式。(X，Y)表示边界框的一个坐标，W，H 表示该框的宽度和高度。category_id 是指盒子所属的类别。</p><p id="cf80" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们需要注册我们的数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="7d1d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了验证数据加载是否正确，让我们可视化训练集中随机选择的样本的注释。</p><h2 id="cee7" class="mu mv it bd mw mx my dn mz na nb dp nc lh nd ne nf ll ng nh ni lp nj nk nl nm bi translated">步骤 3:可视化训练集</h2><p id="56fa" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">我们将从数据集的 train 文件夹中随机选取 3 张图片，看看边界框是什么样子的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="2cc7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出是这样的，</p><div class="kj kk kl km gt ab cb"><figure class="mk kn ot mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/b1eedc1d9c084d87962229f01c0c9f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*NBJ0WWr2UIUGEtKyaLd_GA.png"/></div></figure><figure class="mk kn ou mm mn mo mp paragraph-image"><img src="../Images/3b98a3be49dc11b51d803c7cb3029bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*glLEkV3D8qJjRwfFz-JJWg.png"/></figure></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/3ea0a1bf8aa9c03d8327da8a7902ed24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ptB_bLDSfhQhMeBcrQCsaw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Colab 的结果</p></figure><h2 id="4717" class="mu mv it bd mw mx my dn mz na nb dp nc lh nd ne nf ll ng nh ni lp nj nk nl nm bi translated">步骤 4:训练模型</h2><p id="8082" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated"><strong class="la iu">大步</strong>。在这一步，我们给出配置，并设置模型以准备接受训练。从技术上讲，我们只是在数据集上微调我们的模型，因为该模型已经在 COCO 数据集上进行了预训练。</p><p id="6af9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 Detectron2 的模型动物园中有大量模型可用于对象检测。在这里，我们使用<strong class="la iu"> faster_rcnn_R_50_FPN_3x </strong>模型，从这个角度来看，它是一个高水平的模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/6dde06303224ac3d869ad88876a75c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WN_6BJsPnC5qWpU0TJ-vTQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae lu" href="https://medium.com/@hirotoschwert/digging-into-detectron-2-47b2e794fabd" rel="noopener">来源</a></p></figure><p id="732f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将有一个主干网络(在这种情况下为 Resnet ),用于从图像中提取特征，然后是一个区域建议网络，用于提出区域建议，还有一个框头，用于收紧边界框。</p><p id="cad6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在我的<a class="ae lu" rel="noopener" target="_blank" href="/understanding-fast-r-cnn-and-faster-r-cnn-for-object-detection-adbb55653d97">上一篇文章</a>中读到更多关于 R-CNN 如何更快工作的信息。</p><p id="d12c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们为培训设置配置。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="9e73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我不会说这是最好的配置。当然，其他配置的精度也可能会提高。毕竟，这取决于选择正确的超参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/956e1ada023c03945a7ab2079c75681e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1q1CWg9d4X2d3Kzi8fC4RQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">培训过程(来自 Colab 的结果)</p></figure><p id="06d9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，这里我们还计算了验证集上每 500 次迭代的精确度。</p><h2 id="0ad7" class="mu mv it bd mw mx my dn mz na nb dp nc lh nd ne nf ll ng nh ni lp nj nk nl nm bi translated">步骤 5:使用训练好的模型进行推理</h2><p id="0edf" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">是时候通过在验证集上测试模型来推断结果了。</p><p id="c51a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">成功完成存储最终权重的训练后，输出文件夹会保存在本地存储中。您可以保存此文件夹，以便将来根据此模型进行推理。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="fce4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">结果:</strong></p><div class="kj kk kl km gt ab cb"><figure class="mk kn oy mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/761b00da74e100e020ae10cbe2dc6f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*3X7nRb6Pc91pR5JESUa5aw.png"/></div></figure><figure class="mk kn oz mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/968916c50848fd2b1c2537a02213548e.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*oBnLI6bZocr9qOkkoaYDtw.png"/></div></figure></div><h2 id="10bd" class="mu mv it bd mw mx my dn mz na nb dp nc lh nd ne nf ll ng nh ni lp nj nk nl nm bi translated">步骤 6:评估训练好的模型</h2><p id="c612" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">通常，模型按照 COCO 评估标准进行评估。平均精度(mAP)用于评估模型的性能。这里有一篇<a class="ae lu" href="https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/" rel="noopener ugc nofollow" target="_blank">文章</a>精确地给出了地图上的一个概念。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/a627278bc26a24d2e7a8c9c8af31833f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tG6cctl_yn1AIySOxxv9lA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">评估指标(来自 Colab 的结果)</p></figure><p id="6ff5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于 0.5 的 IoU，我们可以获得大约 79.4%的准确率，这还不算太差。当然可以通过调整参数和增加迭代次数来增加。但是要密切关注广泛的训练，因为模型可能会超出训练集。</p><p id="c3a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您需要从保存的模型中进行推断，请浏览本<a class="ae lu" href="https://colab.research.google.com/drive/1d0kXs-TE7_3CXldJNs1WsEshXf8Gw_5n?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab 笔记本</a>。</p><h1 id="bfe4" class="pb mv it bd mw pc pd pe mz pf pg ph nc jz pi ka nf kc pj kd ni kf pk kg nl pl bi translated">结论</h1><p id="15cd" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">在本文中，我强调了使用 detectron 2 使用自定义数据集进行异议检测的过程，而不是关注获得更高的准确性。</p><p id="494b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然这似乎是一个非常简单的过程，但在 Detectron 2 的库中还有很多东西需要探索。我们有大量的优化参数，可以进一步调整以获得更高的精度，这完全取决于一个人的自定义数据集。</p><p id="c4f5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">希望你今天学到了新东西。</p><p id="b857" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以从我的<a class="ae lu" href="https://github.com/aakarsh7599/Text-Detection-using-Detectron2" rel="noopener ugc nofollow" target="_blank"> Github 库</a>下载笔记本，并尝试在 Google Colab 或 Jupyter 笔记本上运行。</p><p id="831b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在我的<a class="ae lu" rel="noopener" target="_blank" href="/understanding-object-detection-and-r-cnn-e39c16f37600">上一篇文章</a>中读到更多关于物体探测和传统 R-CNN 的内容。</p><p id="9d07" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想联系我，请在<a class="ae lu" href="https://www.linkedin.com/in/aakarsh-yelisetty-6b691b171/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p></div></div>    
</body>
</html>