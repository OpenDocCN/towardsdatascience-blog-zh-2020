<html>
<head>
<title>Fine-Tuning GPT2 on Colab GPU… For Free!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Colab GPU 上微调 GPT2 免费！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-gpt2-on-colab-gpu-for-free-340468c92ed?source=collection_archive---------24-----------------------#2020-06-12">https://towardsdatascience.com/fine-tuning-gpt2-on-colab-gpu-for-free-340468c92ed?source=collection_archive---------24-----------------------#2020-06-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="405f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用 Google Colab 的 GPU 来微调预训练的 GPT2</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/11187b0e7c9cf787e1f8a198c7ee773e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cKOrZC8aPblVwR-u"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@celinen?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">塞琳娜·纳多</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="e8d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在的模特都很大，我们大多数人都没有资源从头开始训练她们。幸运的是，<a class="ae ky" href="http://huggingface.co" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a>已经慷慨地在 PyTorch 中提供了<a class="ae ky" href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">预训练模型，并且</a><a class="ae ky" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>允许使用他们的 GPU(在固定时间内)。否则，即使在没有 NVIDIA GPU 的情况下，在我的本地计算机上微调数据集也会花费大量时间。虽然这里的教程是针对 GPT2 的，但这可以针对 HuggingFace 给出的任何预训练模型进行，也可以针对任何尺寸。</p><h1 id="1385" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">设置 Colab 使用 GPU…免费</h1><p id="c783" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">转到<a class="ae ky" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>并创建一个新笔记本。它应该看起来像这样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/42c01fae751e04353251f59059e829dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OZWa0Qo4NaE1dqSrIg-s4Q.png"/></div></div></figure><p id="529b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">点击<code class="fe mt mu mv mw b">Runtime</code> &gt; <code class="fe mt mu mv mw b">Change runtime type</code>设置使用 GPU</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/dc767a81f9fb42bc811fe19124ff2462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0DGCbSvkQv5QTRZJ_vsXhw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/991c97ea8bde52f7c5db02aa4540b48b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*At8t3yEmao71z1xUxIuDHA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/0dabbf7d7faf9aa91041e1222a92e2fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L-g2KEBUF1KCvjeCft4adA.png"/></div></div></figure><p id="2ca1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后点击<code class="fe mt mu mv mw b">Save</code>。</p><h1 id="0781" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">安装依赖项</h1><p id="923e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们通常会在 Bash 中运行<code class="fe mt mu mv mw b">pip3 install transformers</code>，但是因为这是在 Colab 中，所以我们必须用<code class="fe mt mu mv mw b">!</code>来运行它</p><pre class="kj kk kl km gt mz mw na nb aw nc bi"><span id="4da4" class="nd lw it mw b gy ne nf l ng nh">!pip3 install transformers</span></pre><h1 id="63d4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">获取 WikiText 数据</h1><p id="8f41" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">你可以在这里阅读更多关于 WikiText 数据的信息。总的来说，有 WikiText-2 和 WikiText-103。我们将使用 WikiText-2，因为它更小，而且我们在 GPU 上运行的时间以及在 Colab 中可以加载到内存中的数据量都有限制。要下载并运行，请在单元格中运行</p><pre class="kj kk kl km gt mz mw na nb aw nc bi"><span id="a8f1" class="nd lw it mw b gy ne nf l ng nh">%%bash<br/>wget <a class="ae ky" href="https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip" rel="noopener ugc nofollow" target="_blank">https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip</a><br/>unzip wikitext-2-raw-v1.zip</span></pre><h1 id="ec05" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">微调 GPT2</h1><p id="6e1e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">HuggingFace 实际上提供了一个脚本来帮助微调模型<a class="ae ky" href="https://github.com/huggingface/transformers/tree/master/examples/language-modeling" rel="noopener ugc nofollow" target="_blank">这里</a>。我们可以通过运行以下命令来下载脚本</p><pre class="kj kk kl km gt mz mw na nb aw nc bi"><span id="e9a9" class="nd lw it mw b gy ne nf l ng nh">!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py</span></pre><p id="b229" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们准备微调。</p><p id="76ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">脚本有很多参数，你可以通过<a class="ae ky" href="https://github.com/huggingface/transformers/tree/master/examples/language-modeling" rel="noopener ugc nofollow" target="_blank">阅读手册</a>来理解。我只是想复习一下基础训练中重要的内容。</p><ul class=""><li id="d080" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated"><code class="fe mt mu mv mw b">output_dir</code>是模型输出的位置</li><li id="3de1" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><code class="fe mt mu mv mw b">model_type</code>就是你要用什么型号。在我们的例子中，它是<code class="fe mt mu mv mw b">gpt2</code>。如果你有更多的内存和时间，你可以选择更大的<code class="fe mt mu mv mw b">gpt2</code>尺寸，这些尺寸列在<a class="ae ky" href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">抱脸预训练型号列表</a>中。</li><li id="3294" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><code class="fe mt mu mv mw b">model_name_or_path</code>是通往模型的道路。如果您想从头开始培训，可以留空。在我们的例子中，它也是<code class="fe mt mu mv mw b">gpt2</code></li><li id="8b75" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><code class="fe mt mu mv mw b">do_train</code>告诉它要训练</li><li id="6090" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><code class="fe mt mu mv mw b">train_data_file</code>指向培训文件</li><li id="c3ea" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><code class="fe mt mu mv mw b">do_eval</code>告诉它事后评估。不总是必需的，但最好有</li><li id="a55a" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><code class="fe mt mu mv mw b">eval_data_file</code>指向评估文件</li></ul><p id="f47e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些你可能关心的额外的，但是你也可以跳过这个。</p><ul class=""><li id="48b8" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated"><code class="fe mt mu mv mw b">save_steps</code>是何时保存检查点。如果您的内存有限，您可以将此设置为<code class="fe mt mu mv mw b">-1</code>,这样它会跳过保存直到最后</li><li id="e4b2" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><code class="fe mt mu mv mw b">per_gpu_train_batch_size</code>是 GPU 的批量大小。如果你的 GPU 有足够的内存，你可以增加这个。为了安全起见，你可以从 1 开始，如果你还有内存的话，就增加它</li><li id="7766" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><code class="fe mt mu mv mw b">num_train_epochs</code>是要训练的历元数。由于我们正在微调，我将把它设置为<code class="fe mt mu mv mw b">2</code></li><li id="d267" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">如果<code class="fe mt mu mv mw b">output_dir</code>中已经有内容，就使用<code class="fe mt mu mv mw b">overwrite_output_dir</code>,您可以覆盖现有的模型</li></ul><p id="bbc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总而言之，要训练，在牢房里跑这个</p><pre class="kj kk kl km gt mz mw na nb aw nc bi"><span id="6d9f" class="nd lw it mw b gy ne nf l ng nh">%%bash<br/>export TRAIN_FILE=wikitext-2-raw/wiki.train.raw<br/>export TEST_FILE=wikitext-2-raw/wiki.test.raw<br/>export MODEL_NAME=gpt2<br/>export OUTPUT_DIR=output</span><span id="c124" class="nd lw it mw b gy nw nf l ng nh">python run_language_modeling.py <br/>--output_dir=$OUTPUT_DIR \<br/>--model_type=$MODEL_NAME \<br/>--model_name_or_path=$MODEL_NAME \<br/>--do_train \<br/>--train_data_file=$TRAIN_FILE \<br/>--do_eval \<br/>--eval_data_file=$TEST_FILE \<br/>--per_gpu_train_batch_size=1 \<br/>--save_steps=-1 \<br/>--num_train_epochs=2</span></pre><p id="c9ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，如果您想要微调您刚刚训练的模型，您可以将<code class="fe mt mu mv mw b">MODEL_NAME=gpt2</code>更改为<code class="fe mt mu mv mw b">MODEL_NAME=output/</code>，这样它将加载我们刚刚训练的模型</p><h2 id="96b8" class="nd lw it bd lx nx ny dn mb nz oa dp mf li ob oc mh lm od oe mj lq of og ml oh bi translated">花很长时间跑</h2><p id="0fb3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">当您运行这个程序时，如果花了一些时间没有任何输出，您可以将鼠标悬停在右上角的 RAM/Disk 上，看看发生了什么。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/58940d4c88d248fa0f2ea067afd5da02.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*f8r0_1LUMqSFIbPkI-x1Sg.png"/></div></figure><p id="d39b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Colab GPU 的缺点是它是在 Colab 用户之间共享的。这意味着可能不会立即执行，因为另一个用户正在使用它。当这种情况发生时，它会说</p><pre class="kj kk kl km gt mz mw na nb aw nc bi"><span id="5ec3" class="nd lw it mw b gy ne nf l ng nh">Waiting for 'Python 3 Google Compute Engine backend (GPU(' to finish its current execution.</span></pre><p id="4473" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了静观其变，实在没什么可做的。</p><h1 id="706b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结果</h1><p id="f718" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">运行完模型后，您可以检查它是否存在于输出目录中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/51959ab9eb7cc1df52fb84593c7d84f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*14PAPGwBFd7Vmyu2ZSobVg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/642ac9c12ed51b50a257f4fbf7804e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*vEqvIBDZr4TokorbO8V_xA.png"/></div></figure><p id="01a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要使用它，您可以运行类似</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="8721" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<code class="fe mt mu mv mw b">= Toronto Raptors =</code>相当于把<code class="fe mt mu mv mw b">Toronto Raptors</code>描述为文章标题。</p><p id="5e60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我得到的结果(和你的会有所不同)是</p><pre class="kj kk kl km gt mz mw na nb aw nc bi"><span id="1578" class="nd lw it mw b gy ne nf l ng nh"> = Toronto Raptors = </span><span id="517f" class="nd lw it mw b gy nw nf l ng nh"> Toronto's first @-@ round draft pick in 2006 was selected by the Toronto Raptors with the seventh overall pick. He played in all 82 games, averaging 18 points per game and 6 assists. The Raptors won their third straight NBA championship in 2007, and won the 2009 NBA All @-@ Star Game. He played in a record 16 games for Toronto, averaging 19 points on 5 @.@ 6 rebounds and 6 assists in a season that saw Toronto win the Eastern Conference finals. He also played in the 2008 All @-@ Star Game and was named to the All @-@ Star Game MVP for the first time. He also was named to the All @-@ Star Game's all @-@ time career scoring list, and was the first player to accomplish the feat. He finished the season with an assist and an assist in eight games, and was the first player in NBA history to score in double figures. He was named to the All @-@ Star Game's All @-@ time scoring list in 2011, and was the first player to do this in consecutive seasons.</span><span id="1d0b" class="nd lw it mw b gy nw nf l ng nh"> = = Draft = =</span><span id="4457" class="nd lw it mw b gy nw nf l ng nh"> Toronto selected Jordan Matthews with the seventh overall pick in</span></pre><p id="ecd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的例子中，我只生成了前 250 个单词，这就是为什么它会被突然删除的原因。如果你想的话，你可以扩展它。请注意，对<code class="fe mt mu mv mw b">Toronto Raptors</code>的描述完全是假的，因为<a class="ae ky" href="https://en.wikipedia.org/wiki/Jordan_Mathews" rel="noopener ugc nofollow" target="_blank">乔丹·马修斯</a>从未为猛龙队效力。文本一致性也可以更好，这可以通过使用更多的 epochs 进行调整，或者简单地使用更大的模型。然而，这需要更多的内存，所以要小心。</p><h1 id="dea8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">压缩/压缩模型</h1><p id="6e88" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了让我们保存这个模型，我们应该压缩它并保存在某个地方。这可以很容易地完成</p><pre class="kj kk kl km gt mz mw na nb aw nc bi"><span id="2aa9" class="nd lw it mw b gy ne nf l ng nh">! tar -czf gpt2-tuned.tar.gz output/</span></pre><p id="d750" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这会创建一个名为<code class="fe mt mu mv mw b">gpt2-tuned.tar.gz</code>的文件</p><h1 id="a4d4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">保存到 Google Drive</h1><p id="c2eb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">要将它从 Colab 保存到您的 Google Drive，首先您必须有一个 Google 帐户/Gmail 帐户。在你的牢房里，你可以跑</p><pre class="kj kk kl km gt mz mw na nb aw nc bi"><span id="e8f7" class="nd lw it mw b gy ne nf l ng nh">from google.colab import drive<br/>drive.mount('/content/drive')</span></pre><p id="ffe3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不需要安装任何额外的东西，因为<code class="fe mt mu mv mw b">google.colab</code>图书馆附带使用谷歌 Colab。当您运行上述代码时，您应该会看到类似这样的内容</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/67cf84257bf74deba9ab3d4b4da9fd57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IEAGm5tToegIqx934MCWjQ.png"/></div></div></figure><p id="ed24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你必须点击链接，登录并允许你的 Google Drive 访问你的 Colab。最后，你会看到类似这样的东西</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/fb4ccbf7c3fdebe02e11fff60af723ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VJzDHY_fd92RbsFGIN6xXw.png"/></div></div></figure><p id="c3c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">复制并粘贴到你的笔记本上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/e7b05f98345a79c7cc7e605e24a5e8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*hDKtCJZSab91DydGgP4aSw.png"/></div></figure><p id="6592" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，您可以通过运行以下命令将输出模型复制到您的 Google Drive 中</p><pre class="kj kk kl km gt mz mw na nb aw nc bi"><span id="036e" class="nd lw it mw b gy ne nf l ng nh">!cp gpt2-tuned.tar.gz /content/drive/My\ Drive/</span></pre><h1 id="c14c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="ba42" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">瞧啊。您已经成功地在 GPU 上调优了一个预训练模型，并将其保存在您的 Google Drive 中。而且你完全免费。</p><p id="6779" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对我们的工作有任何疑问或改进，请在评论中告诉我。</p><h1 id="7e4f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">奖励:Colab 笔记本</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="1135" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以运行这个 Colab 笔记本来重现上面显示的所有内容</p></div></div>    
</body>
</html>