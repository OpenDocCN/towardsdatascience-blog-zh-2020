<html>
<head>
<title>Playing Treasure Drop with Deep Reinforcement Learning — Part 2/3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用深度强化学习玩宝藏游戏—第2/3部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/playing-treasure-drop-with-deep-reinforcement-learning-part-2-3-e7097478d402?source=collection_archive---------83-----------------------#2020-05-13">https://towardsdatascience.com/playing-treasure-drop-with-deep-reinforcement-learning-part-2-3-e7097478d402?source=collection_archive---------83-----------------------#2020-05-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2211" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">艾玩拼图</h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/0c79c6ac9f4a5c7761dea3c2db061859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CINe-_itkSwA7A3fA-xBYA.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">这是代理在与其他玩家在线对战时“看到”的内容。</p></figure><p id="db77" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在这个系列文章中，我解释了我从事的强化学习(RL)项目。最后，你会知道这是怎么回事:</p><p id="4955" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><a class="ae lj" href="https://youtu.be/FhhPJIZnJ1M" rel="noopener ugc nofollow" target="_blank">观看这个人工智能在实时益智游戏中击败人类玩家</a></p></div><div class="ab cl lk ll hu lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ij ik il im in"><p id="0287" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我讨论了问题的独特和值得注意的方面，我使用了什么技术以及为什么。希望对深度强化学习感兴趣的读者会发现所有这些都很有见地。</p><p id="9748" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在<a class="ae lj" href="https://medium.com/p/2eb789c2ff5e/edit" rel="noopener">第一部</a>中，对<em class="lr">宝藏掉落</em>的游戏进行了说明，并对游戏的缩小版使用了表格q-learning方法。在获得了表格方法所需的验证和早期成功之后，这里实现了Deep Q网络。</p><h1 id="a2c3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">摘要</h1><p id="cfc2" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">在获得了表格方法所需的验证和早期成功之后，使用Keras库实现了Deep Q网络来解决与第1部分相同的问题。然而，由于项目的性质，学习过程变得不稳定。实现了几种方法来缓解该问题。即，优先经验重放、目标Q网络、具有学习率查找器的循环学习率。</p><p id="7a9f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kn ja">第2/3部分的GitHub存储库:</strong><a class="ae lj" href="https://github.com/arapfaik/td-deepreinforcementlearning-part2" rel="noopener ugc nofollow" target="_blank">TD-deepreinforcementlearning-Part 2</a></p><h1 id="8c96" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">表格q-学习的局限性</h1><p id="4cf8" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">在<a class="ae lj" href="https://medium.com/p/2eb789c2ff5e/edit" rel="noopener">第1部分</a>中，q表适用于尺寸为3x2(最大可能为4x2)的电路板。在4x2上，q-table花了很长时间才有收敛值。这是因为在4x2中，状态空间具有4⁹ = 2个⁸状态。这相当于大约26万个州。这对于表格实现来说太多了。原始游戏桌是4x5，2⁶⁰有不同的可能状态。这使得表格q学习对我们的问题不可行。</p><p id="5972" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在许多有实际意义的情况下，包括这个例子，状态比一个表中的条目要多得多。在这些情况下，函数必须是近似的，使用某种更紧凑的参数化函数表示。</p><p id="60d9" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">对此的解决方案是近似q值函数。猜猜什么是好的函数逼近器？是的，你答对了！👏👏<strong class="kn ja">深度神经网络</strong>！本文是关于使用深度神经网络来逼近q值函数。它们通常出现在强化学习环境中，被称为深度Q学习网络(DQN)。</p><h1 id="7d16" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">从Q表到DQN的过渡</h1><p id="c908" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">这很简单。代码中唯一的不同是在我们更新q表的那一行:</p><figure class="mv mw mx my gt ka"><div class="bz fp l di"><div class="mz na l"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">agentQTable类中的q表更新，代理的表格实现。</p></figure><p id="d6b3" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">相反，我们将把td_target和状态一起馈送给神经网络。理论上讲，下一次网络看到状态时，它会输出正确的q值。这种情况适用于较小的棋盘尺寸，但对于较大的棋盘尺寸和游戏的原始尺寸来说就不那么容易了。以下更新将取代上面的表格q-learning更新:</p><figure class="mv mw mx my gt ka"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="1aa2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">神经网络</h1><h2 id="ec46" class="nb lt iq bd lu nc nd dn ly ne nf dp mc kw ng nh mg la ni nj mk le nk nl mo iw bi translated">体系结构</h2><p id="1676" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">决定采用60 x 300 x 300 x 300 x 300 x 300 x 300 x 8的形状，尽管输入层和输出层因电路板尺寸而异。例如，对于4x3游戏，它们分别是30和4。所有层都是完全连接的。</p><blockquote class="nm nn no"><p id="4778" class="kl km lr kn b ko kp kq kr ks kt ku kv np kx ky kz nq lb lc ld nr lf lg lh li ij bi translated">任务的理想网络架构必须通过验证集误差引导的实验来找到。</p><p id="5c00" class="kl km lr kn b ko kp kq kr ks kt ku kv np kx ky kz nq lb lc ld nr lf lg lh li ij bi translated">古德费勒，本吉奥&amp;库维尔律师事务所。<a class="ae lj" href="https://www.amazon.com/Deep-Learning-NONE-Ian-Goodfellow-ebook/dp/B01MRVFGX4/ref=sr_1_3?dchild=1&amp;keywords=deep+learning&amp;qid=1587862365&amp;sr=8-3" rel="noopener ugc nofollow" target="_blank"><em class="iq"/></a><em class="iq"/>(2016)</p></blockquote><p id="b9a2" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在线游戏期间，游戏控制器类从屏幕上收集状态。回合也是从屏幕上推断出来的。然后，将状态作为相应回合的输入馈入模型。输出是q值，每个对应于棋盘上的一个动作。具有最高q值的动作由代理挑选。</p><figure class="mv mw mx my gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ns"><img src="../Images/2c39059502c2a504893cef807e5ac66f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L8DU2EYbFYPJUfh0BkolMA.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">模型和游戏之间的信息流动图。</p></figure><h2 id="934e" class="nb lt iq bd lu nc nd dn ly ne nf dp mc kw ng nh mg la ni nj mk le nk nl mo iw bi translated">优化器和损耗</h2><p id="3e87" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">动量= 0.9的随机梯度下降(SGD)用于所有模型。使用系数0.5对渐变进行剪裁。不剪裁梯度有时会导致爆炸结果，神经网络的权重最终是NaN。</p><p id="dd65" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">损失函数是行动的预测q值和贝尔曼方程计算值之间的MSE。</p><h1 id="d0e6" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">学习过程的稳定性</h1><p id="c917" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">在这一点上，DQN可以工作，但是只适用于非常小的电路板。它有时会收敛，有时不会。深度强化学习以稳定著称，因为这是当前的热门研究领域。关于不稳定最流行的术语是“致命三重奏”。如果在一个学习算法中存在特定的三个品质，就说学习算法有很大的机会出现分歧。</p><p id="92a8" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">当使用具有强化学习的深度非线性函数空间时，关于发散是否普遍，以及致命的三元组是否是罪魁祸首，几乎没有指导。</p><h1 id="51c5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">致命三和弦</h1><p id="6f2c" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">在DQN方法中，我们做了三件值得注意的事情。这些是:</p><ol class=""><li id="6965" class="nt nu iq kn b ko kp ks kt kw nv la nw le nx li ny nz oa ob bi translated">函数逼近:我们通过深度神经网络逼近q值函数，而不是用数学方法制作实际函数，或者存储每个输入的输出值:</li><li id="a3ed" class="nt nu iq kn b ko oc ks od kw oe la of le og li ny nz oa ob bi translated">Bootstrapping:我们部分基于其他估计来更新q值估计。我们从另一个猜测中学习一个猜测——我们<em class="lr">引导。</em></li><li id="2d4c" class="nt nu iq kn b ko oc ks od kw oe la of le og li ny nz oa ob bi translated">政策外学习:当学习发生时，我们并不总是遵循政策。这是因为我们采取的是概率为ε(ε)的随机行动。</li></ol><p id="972c" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这三者的结合会产生有害的学习动力，导致函数参数的发散。由于这种分歧的可能性，这三者的结合被称为“致命三重奏”。</p><p id="ee51" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在为3x2游戏训练时，我注意到模型有时收敛，有时不收敛。由于某种原因，学习过程似乎不稳定。我归咎于致命三和弦，虽然还不清楚这是否是问题所在。尽管如此，学习过程具有所有这三个特点。本文的其余部分是关于为了使学习过程稳定而实现的内容。</p><h1 id="4c63" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">体验回放</h1><figure class="mv mw mx my gt ka"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="c954" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在数据科学中，数据的底层分布决定了模型学习的内容。经验回放是一个非常非常流行的技术，只要RL和DQN走到一起就可以实现。用当前事务更新模型很酷，但是浪费数据。模型只看到一个数据。</p><p id="97db" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">与此相反，经验回放使用随机的先前交易样本来更新Q值。通过保存以前发生的交易的记忆，在每一步中随机抽样。这引入了两个新的超参数:内存大小(max_memory)和每次抽取的样本数量(batch_size)。</p><p id="daa4" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">Mnih等人(2015)为各种Atari游戏上的DQN任务将他们的内存缓冲区大小设置为10⁶。⁴:以此作为起点是有意义的。在项目的后期，我意识到它会影响性能，所以将它调整到了一个较低的数值。至于批量大小，10似乎最适合这个游戏。</p><h1 id="2376" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">优先体验重放</h1><p id="388c" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">体验重放将在线学习代理从按照他们体验的确切顺序处理过渡中解放出来。优先重放进一步解放了代理，使其不用考虑与他们经历的频率相同的转换。⁵</p><p id="4321" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">直觉上，有些交易比其他交易更重要。有可能以更明智的方式对交易进行抽样。现在的问题是，哪种交易比其他交易更重要？</p><p id="2a7b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">代理商必须对那些大额交易更加敏感。这有双重优势。首先，代理可以感觉到高回报并采取适当的行动。第二，代理可以看到敌人是否可以在某个移动后获得高奖励，并可以通过限制自己的行动来防止让敌人获得奖励。</p><p id="0383" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">此时，我们只需要一个度量来相应地分配概率。</p><p id="77de" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">预测高标量值比预测低标量值有更多的MSE损失。这意味着，交易的回报越高，MSE的损失也就越大。</p><p id="c00b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">因此，我们可以放心地将损失视为优先级的衡量标准。损失越大，交易越重要。每次从经验缓冲区中提取一个事务时，都会在其上运行一个预测。由预测导致的丢失被更新，并且事务被放回具有新的优先级度量的缓冲器中。</p><p id="13ab" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在训练迭代次数相同的情况下，经过优先体验回放训练的模型似乎比没有经过优先体验回放训练的模型更胜一筹。</p><h1 id="36ee" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">目标Q网络</h1><p id="ae62" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">这解决了我们学习过程的自举质量。这不是一个万全之策，因为自举仍然存在。然而，它已被证明有助于各种各样的强化学习任务，包括DQN。</p><p id="ba6f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">该逻辑只是从与被更新的网络不同的网络中引导。换句话说，有两个网络，互相复制。估计值来自一个网络，值在另一个网络(称为目标网络)上更新。在每个tn_steps(目标网络步骤的简称)处，权重从目标网络复制到原始网络，网络估计从其导出。</p><p id="bece" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">目标Q网络在训练过程的早期阶段具有稳定作用。使用tn_steps = 1的超参数。意思是每次游戏后复制目标网络。每场比赛持续100步左右。所以tn_steps = 1实际上意味着每大约100次更新就复制一次目标网络。</p><h1 id="4484" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">学习率</h1><blockquote class="oh"><p id="ffe8" class="oi oj iq bd ok ol om on oo op oq li dk translated">学习率是要调整的最重要的超参数。—吴恩达</p></blockquote><p id="b5e2" class="pw-post-body-paragraph kl km iq kn b ko or kq kr ks os ku kv kw ot ky kz la ou lc ld le ov lg lh li ij bi translated">为了找到最佳的学习速率并系统地调整其他超参数，定义了超参数搜索类。在寻找一个恒定的速率后，<strong class="kn ja"> 0.009 </strong>被确定为目前最好的。</p><h2 id="a655" class="nb lt iq bd lu nc nd dn ly ne nf dp mc kw ng nh mg la ni nj mk le nk nl mo iw bi translated">学习率查找器和循环学习率</h2><p id="cc26" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">Leslie Smith等人描述了一种被称为“超级收敛”的现象，即神经网络的训练速度可能比标准方法快。⁶在这个项目中，在给定相同次数的情况下，以循环学习率训练的代理人似乎比其他人在这个问题上做得更好。</p><h2 id="b3f1" class="nb lt iq bd lu nc nd dn ly ne nf dp mc kw ng nh mg la ni nj mk le nk nl mo iw bi translated">LR取景器</h2><p id="f95d" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">首先，我们做一个叫做LR范围测试的东西。它决定了该方法是否适用于该问题。训练以非常小的学习率开始，并随着情节的增加而增加。理想情况下，在某一点上，模型停止改进，精确度达到稳定状态，停止增加。此时的学习率是循环学习率计划中使用的最大学习率。三分之一或四分之一是最低学习率。</p><p id="d059" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">但是，这个问题没有准确性。相反，我使用的是一集训练中发生的平均损失。SGD的批量大小设置为100–300左右。因此，每次损失都是在30，000次转换中产生的。这是需要的，使情节顺利如下。否则会太吵，无法解释。</p><figure class="mv mw mx my gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ow"><img src="../Images/163a33865aeb99ebe25b229a4f6d3404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iFCOYoOOu4tttRIbU93hTQ.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">根据LR查找器方法，最大学习率大约在10^-3.5到10^-3.之间</p></figure><p id="e519" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">从上面的图中，我们可以得出结论，最大学习率可以是10^-3和最小学习率。四分之一。</p><h2 id="75dd" class="nb lt iq bd lu nc nd dn ly ne nf dp mc kw ng nh mg la ni nj mk le nk nl mo iw bi translated">循环学习率计划</h2><p id="8ccf" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">澄清一下，“循环”这个词只是指学习率从min_lr到max_lr，然后下降，看起来就像下面的图。转折点不一定是一半，也可以早到四分之一。这个想法基于一个叫做<a class="ae lj" href="https://en.wikipedia.org/wiki/Simulated_annealing" rel="noopener ugc nofollow" target="_blank">模拟退火</a>的概念，在深度学习文献中很流行。</p><figure class="mv mw mx my gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ox"><img src="../Images/3eeb141b52ad07310f834f2a75942f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0TqYfIQwZ85jfnLW7A5Dg.png"/></div></div></figure><h1 id="dbf1" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">培养</h1><p id="1771" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">如果您希望自己训练模型，请准备好将东西转移到AWS。训练一个在4x5(原始)游戏上玩得好的代理需要几个小时。只是澄清一下，使用GPU不会加快速度，因为模型中没有卷积层。你也不需要AWS上的超级强大的实例，我在<strong class="kn ja"> c5.xlarge EC2 </strong>实例上做了所有的训练。</p><h1 id="c2d5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">模型验证和行动⚔️</h1><p id="c5f4" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated"><a class="ae lj" href="https://github.com/arapfaik/td-deepreinforcementlearning-part2" rel="noopener ugc nofollow" target="_blank">链接到GitHub知识库</a></p><p id="b354" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">虽然不是最自动化的方法，但验证模型的一种方法是简单地查看并比较模型预测的等效行动的q值。另一种方法是比较镜像态的预测q值。为什么这些会是好的启发法的解释在<a class="ae lj" href="https://medium.com/p/2eb789c2ff5e" rel="noopener">第1部分</a>中解释。将结果与第一部分中的表格模型的输出进行比较，我们看到DQN的输出不如表格q-learning的输出一致。这是因为我们在做近似计算，而且表的大小要大得多。</p><p id="a99a" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">然而，代理无论如何都会选择具有最高q值的动作。即使这些数字之间有很大的差异，在这两种情况下，代理将执行相同(等效)的动作。由于状态沿y轴镜像，动作1和6彼此对应。</p><figure class="mv mw mx my gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oy"><img src="../Images/1b31a1b78d715d458fa00670af99f673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACIf5Vq49ngXzFJH7JJv2g.png"/></div></div></figure><p id="35c5" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们的学习算法TD(0)被证明对于算法的表格实现确定性地收敛于单个答案。表格法往往能找到精确解，即往往能精确找到最优值函数和最优策略。</p><p id="4981" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这与这里描述的近似方法不同，近似方法只能找到近似解，但反过来可以有效地应用于更大的问题。如同所有的人工智能一样，适用性的广度和数学的易处理性之间存在矛盾。学习最优策略的代理人做得很好，但在实践中这很少发生。对于这项任务，只有在计算成本极高的情况下才能生成最优策略。我们只能近似到不同的程度。</p><p id="afb6" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">模型的手动验证显然很耗时，而且根本不实用。因此，下一部分将解释启发式指标以及损失和事件的绘制。</p><h1 id="555b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">下一步是什么？</h1><p id="2854" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">测量和绘图是深度学习研究的关键。第三部分介绍了绘图方法。此外，代理是最终确定的，并对玩家在线挑战。接受来自屏幕的输入，点击适当的点，代理能够击败大多数玩家。</p><p id="654b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><a class="ae lj" href="https://medium.com/p/e4a2992112a1" rel="noopener">链接到第3部分</a></p></div><div class="ab cl lk ll hu lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ij ik il im in"><h1 id="36d5" class="ls lt iq bd lu lv oz lx ly lz pa mb mc md pb mf mg mh pc mj mk ml pd mn mo mp bi translated">关于我</h1><p id="f26e" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">我是一名数据科学家，住在旧金山。热衷于从数据中寻找答案。在Linkedin上找到我:<a class="ae lj" href="http://linkedin.com/in/sakarya" rel="noopener ugc nofollow" target="_blank">梅尔·萨卡里亚</a></p><h1 id="efa8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">参考</h1><p id="9426" class="pw-post-body-paragraph kl km iq kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">[1]:萨顿&amp;巴尔托。<a class="ae lj" href="https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation-ebook/dp/B07JN1QFW5/ref=sr_1_2?crid=352FL757QDCAK&amp;dchild=1&amp;keywords=sutton+and+barto+reinforcement+learning&amp;qid=1587862288&amp;sprefix=sutton+barto+rein%2Caps%2C249&amp;sr=8-2" rel="noopener ugc nofollow" target="_blank"> <em class="lr">【强化学习】</em> </a> <em class="lr"> </em> (2018)</p><p id="2ed5" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">[2]:古德费勒，本吉奥&amp;库维尔。<a class="ae lj" href="https://www.amazon.com/Deep-Learning-NONE-Ian-Goodfellow-ebook/dp/B01MRVFGX4/ref=sr_1_3?dchild=1&amp;keywords=deep+learning&amp;qid=1587862365&amp;sr=8-3" rel="noopener ugc nofollow" target="_blank"><em class="lr"/></a><em class="lr"/>(2016)</p><p id="bb61" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">[3]:哈塞尔特等人<a class="ae lj" href="https://arxiv.org/abs/1812.02648" rel="noopener ugc nofollow" target="_blank"> <em class="lr">深度强化学习与致命三重奏</em> </a> <em class="lr">。arXiv:1812.02648v1 [cs。2018年12月6日</em></p><p id="f8c2" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">[4]: Hasselt等<a class="ae lj" href="https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning" rel="noopener ugc nofollow" target="_blank"> <em class="lr">通过深度强化学习的人级控制</em> </a>。《自然》518，第529-533页。2015</p><p id="0834" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">[5]: Schaul等人<a class="ae lj" href="https://arxiv.org/abs/1511.05952" rel="noopener ugc nofollow" target="_blank"> <em class="lr">优先化经验回放</em> </a>。arXiv:1511.05952v4 [cs。2016年2月25日</p><p id="b171" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">[6]: Smith et al. <a class="ae lj" href="https://arxiv.org/abs/1708.07120" rel="noopener ugc nofollow" target="_blank"> <em class="lr">超收敛:利用大学习率非常快速的训练神经网络</em> </a> <em class="lr">。</em>arXiv:1708.07120 v3【cs。2018年5月17日</p></div></div>    
</body>
</html>