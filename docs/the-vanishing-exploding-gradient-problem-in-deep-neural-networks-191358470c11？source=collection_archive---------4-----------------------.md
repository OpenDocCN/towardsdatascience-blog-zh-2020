# 深度神经网络中的消失/爆炸梯度问题

> 原文：<https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11?source=collection_archive---------4----------------------->

## 理解我们在构建深度神经网络时面临的障碍

我们在训练深度神经网络时面临的一个困难是梯度的消失或爆炸。在很长一段时间里，这个障碍是训练大型网络的主要障碍。然而，到本文结束时，你不仅会知道渐变消失和渐变爆炸的问题是什么，你还会知道当你面临这些问题时如何检测，以及一些可能的解决方案来调试你的模型。

![](img/b28743c464a2bb83f6abc8b1c73cdfb5.png)

[陈京达](https://unsplash.com/@jingdachen?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

**什么问题？**

当使用基于梯度的学习和反向传播来训练深度神经网络时，我们通过从最终层(y_hat)到初始层遍历网络来找到偏导数。使用链式法则，网络中更深处的层要经历连续的矩阵乘法，以计算它们的导数。

在一个有 *n* 个隐藏层的网络中， *n* 个导数会相乘在一起。如果导数很大，那么随着我们沿着模型向下传播，梯度将呈指数增加，直到它们最终爆炸，这就是我们所说的 ***爆炸梯度*** 的问题。或者，如果导数很小，那么当我们通过模型传播时，梯度将呈指数下降，直到它最终消失，这就是 ***消失梯度*** 问题。

在爆炸梯度的情况下，大导数的累积导致模型非常不稳定并且不能有效学习，模型权重的大变化产生非常不稳定的网络，在极端值时，权重变得如此之大以至于导致溢出，导致其权重值不能再被更新。另一方面，小梯度的累积导致模型不能学习有意义的见解，因为倾向于从输入数据(X)学习核心特征的初始层的权重和偏差将不会被有效地更新。在最坏的情况下，梯度将为 0，这又将停止网络，从而停止进一步的训练。

**怎么知道？**

*爆炸渐变*

有几个微妙的方法，你可以用来确定你的模型是否受到爆炸梯度的问题；

![](img/f22602e72176f50fba456062f2b36077.png)

Yosh Ginsu 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

*   该模型在训练数据上学习不多，因此导致了较差的损失。
*   由于模型的不稳定性，该模型在每次更新时损失会有大的变化。
*   在训练中，模特的损失将会很大。

当面对这些问题时，要确认问题是否是由于爆炸梯度引起的，有一些更明显的迹象，例如:

*   模型权重呈指数增长，并且在训练模型时变得非常大。
*   模型权重在训练阶段变为 NaN。
*   导数是不断的

*消失渐变*

还有一些方法可以检测出你的深层网络是否存在梯度消失的问题

![](img/a48be618c4c13c269f9ec4aa3060ce1b.png)

照片由[大卫·贝什](https://unsplash.com/@david_besh?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

*   在训练阶段，模型的改进非常缓慢，也有可能训练很早就停止了，这意味着任何进一步的训练都不会改进模型。
*   越靠近模型输出图层的权重变化越大，而越靠近输入图层的权重变化就越小(如果有变化的话)。
*   训练模型时，模型权重呈指数级收缩，变得非常小。
*   模型权重在训练阶段变为 0。

**解决方案**

有许多解决爆炸和消失梯度的方法；本节列出了您可以使用的三种方法。

1.  **减少层数**

这是可以在两种情况下使用的解决方案(爆炸和消失渐变)。然而，通过减少网络中的层数，我们放弃了一些模型的复杂性，因为更多的层数使网络更有能力表示复杂的映射。

**2。渐变剪辑(爆炸渐变)**

检查和限制梯度的大小，而我们的模型火车是另一个解决方案。深入研究这种技术的细节已经超出了本文的范围，但是你可以在一篇由 Wanshun Wong 撰写的标题为什么是[渐变裁剪](/what-is-gradient-clipping-b8e815cdfb48)的文章中阅读更多关于渐变裁剪的内容。

**3。重量初始化**

为您的网络选择更仔细的随机初始化往往是部分解决方案，因为它不能完全解决问题。查看詹姆斯·德林杰的这篇文章 - [神经网络中的权重初始化:从基础到明凯的旅程](/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)

**结论**

在这篇文章中，我们学习了什么是爆炸和消失渐变，如何检测它们和一些解决方案。我知道在这篇文章中，我没有详细介绍 RNN 结构，这种结构容易消失梯度，有用的资源，以了解更多将在下面链接。

**其他资源**

[齐-汪锋](https://medium.com/u/9ddaaec52a09?source=post_page-----191358470c11--------------------------------) - [消失渐变问题](/the-vanishing-gradient-problem-69bf08b15484)

[Eniola Alese](https://medium.com/u/34fc912a20c6?source=post_page-----191358470c11--------------------------------) - [消失的奇案&爆炸渐变](https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb)