<html>
<head>
<title>Multiclass Classification with Support Vector Machines (SVM), Dual Problem and Kernel Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用支持向量机(SVM)、对偶问题和核函数进行多类分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multiclass-classification-with-support-vector-machines-svm-kernel-trick-kernel-functions-f9d5377d6f02?source=collection_archive---------1-----------------------#2020-06-09">https://towardsdatascience.com/multiclass-classification-with-support-vector-machines-svm-kernel-trick-kernel-functions-f9d5377d6f02?source=collection_archive---------1-----------------------#2020-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6e9f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最后，通过 scikit-learn 了解 Python 中 SVM +实现背后的概念</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b513b512b0f6cd00affb316c88c9932d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FdOgc4ABLTWwF_n6_oHg1Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://unsplash.com/photos/VcQkZl4Wf1Y" rel="noopener ugc nofollow" target="_blank"> unsplash </a> (Bekky Bekks)</p></figure><p id="2b09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">支持向量机(SVM) </em>不是新的，但仍然是一个强大的分类工具，因为它们不会过度拟合，但在许多情况下表现良好。如果你只对某个话题感兴趣，就滚动话题。这些是按时间顺序排列的主题:</p><ul class=""><li id="973b" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><em class="lv">支持向量机背后的</em> <strong class="lb iu"> <em class="lv">数学概念</em> </strong> <em class="lv">是什么？</em></li><li id="040f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv">什么是内核，什么是</em> <strong class="lb iu"> <em class="lv">内核函数</em> </strong> <em class="lv">？</em></li><li id="ccd4" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv">什么是</em> <strong class="lb iu"> <em class="lv">内核绝招</em> </strong> <em class="lv">？</em></li><li id="df82" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv">什么是 SVM 的</em> <strong class="lb iu"> <em class="lv">对偶问题</em> </strong> <em class="lv">？</em></li><li id="bfac" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv"/><strong class="lb iu"><em class="lv">多类分类</em> </strong> <em class="lv">是如何发生的？</em></li><li id="0460" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">通过 Python 和 scikit-learn 实现</strong></li></ul><p id="6931" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">如果你只对如何使用 Python 和 scikit-learn 实现感兴趣，向下滚动到最后！</strong></p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="19a6" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">让我们开始吧。</h2><p id="9b8c" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">目标是在 n 维空间中找到一个超平面，该超平面将数据点分成它们的潜在类别。超平面应该定位在距离数据点最远的位置。到超平面距离最小的数据点称为<em class="lv">支持向量</em>。<em class="lv"> </em>由于它们的位置接近，它们对超平面的确切位置的影响比其他数据点大。在下图中，支持向量是位于直线上的 3 个点(2 个蓝色，1 个绿色)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/d8fdcb4a8a8f837ef24bed1d55ee02c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TSjvQlDde90FQtoD9-9iHw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://en.wikipedia.org/wiki/Support_vector_machine#/media/File:SVM_margin.png" rel="noopener ugc nofollow" target="_blank">维基百科</a> (Larhmam)</p></figure><p id="caa1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">支持向量机也被称为<em class="lv">内核化 SVM </em>，因为它们的内核将输入数据空间转换到一个更高维的空间。</p><p id="800a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入空间 X 由 X 和 X’组成。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/9752dd6cafe47aa01b35f00a000c68e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:92/format:webp/1*2RqLeuecs0rUHQ2OhrX5xg.png"/></div></figure><p id="163f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代表<em class="lv">内核函数</em>将输入空间变成一个更高维的空间，这样并不是每个数据点都被显式映射。</p><p id="3897" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">内核函数也可以写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/69762e1a89acc28da0a133f92fb9de28.png" data-original-src="https://miro.medium.com/v2/resize:fit:118/format:webp/1*KJgX1U0TrYQTvTeZzfGXWQ.png"/></div></figure><p id="d536" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如何定义该函数以及该函数如何用于铺设超平面取决于数据:</p><h2 id="007b" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated"><strong class="ak">内核函数</strong></h2><p id="337d" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">在<em class="lv"> scikit-learn </em>中也可用的最流行的内核函数是<em class="lv">线性、多项式、径向基函数</em>和<em class="lv"> sigmoid </em>。更多功能请访问<a class="ae ky" href="https://data-flair.training/blogs/svm-kernel-functions/" rel="noopener ugc nofollow" target="_blank">数据文件夹</a>。在下面你可以看到这四个内核函数的样子:</p><ol class=""><li id="df70" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu ns mc md me bi translated"><strong class="lb iu">线性函数</strong></li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d1437344e33e5ae8026d807624487c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*YFm1ncXJ-8OqSsgQWyE8Hw.png"/></div></figure><p id="a98f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2。多项式函数</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/60a324c00a2f20e26756b408992096b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*DFGdQETP11oUR9k99_UU3A.png"/></div></figure><p id="139c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3。径向基函数(RBF) </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/3683f653e4b725fa078ed6b79d3a51fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*lAcYtI4Yp8SfHIAxbImnkw.png"/></div></figure><p id="347b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 4。Sigmoid 函数</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/b05e7a72a70962c781d8c1506f290eb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*OqLGb9KhdJYRxufryJ0a2w.png"/></div></figure><h1 id="7dd6" class="nx ms it bd mt ny nz oa mw ob oc od mz jz oe ka nc kc of kd nf kf og kg ni oh bi translated">内核技巧</h1><p id="9372" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">内核函数是做什么的？</p><p id="2f73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它取两个数据点 x_n 和 x_m，并计算它们的距离分数。数据点越近，得分越高，反之亦然。使用该分数有助于将数据点转换为更高维的映射，这减少了计算工作量和时间，并且对于大量数据特别有用。它避免了更复杂转换的需要。<br/>这就是为什么这一步通常被称为<em class="lv">内核技巧</em>的原因。</p><p id="4d16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从下图可以看出，数据点的映射是使用核函数(φ(( <em class="lv"> a </em>，<em class="lv">b</em>)=(<em class="lv">a</em>，<em class="lv"> b </em>，<em class="lv"> a </em> 2 + <em class="lv"> b </em> 2))从 2D 转到三维空间的。当变成 3D 空间时，先前居中的红点现在也位于垂直下方。不明显可分的数据点现在可以通过使用核来更好地划分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/ae65418f7eef14477e7c66f8e368a524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HVA5GkCF0u7lPSzPEC1ayw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://en.wikipedia.org/wiki/Support_vector_machine#/media/File:Kernel_trick_idea.svg" rel="noopener ugc nofollow" target="_blank">维基百科</a>(、纪)</p></figure><p id="d9eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，不同的核可以帮助通过数据点的云铺设不同形状的超平面。显然，由于线性超平面对不同形状的适应性有限，它们的极限很快就被超过了。基于这一事实，不同的核函数被开发出来。</p><div class="oj ok gp gr ol om"><a href="https://medium.com/subscribe/@hucker.marius" rel="noopener follow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd iu gy z fp or fr fs os fu fw is bi translated">请继续关注马里乌斯·哈克的新文章</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">请继续关注 Marius Hucker 的新文章。如果您还没有注册，您将创建一个中型帐户…</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">medium.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa ks om"/></div></div></a></div></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="2a46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更好地理解不同超平面的分离，不同种类的核函数在下图中被可视化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/932952b098eed256fcc257343fa66f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qZFPN60NvwbFE_tRg2YPoQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同内核函数的可视化。</p></figure><h1 id="69f6" class="nx ms it bd mt ny nz oa mw ob oc od mz jz oe ka nc kc of kd nf kf og kg ni oh bi translated">SVM 背后的数学概念</h1><p id="8d80" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">下面的公式提出了由支持向量机解决的优化问题。下面将进一步解释(scikit-learn，未注明日期):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/6c2c91af39b7212a1371b203749008d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6fwgnBpoG33a7En943dIDA.png"/></div></div></figure><p id="2cc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目标是通过最大化从<em class="lv">支持向量</em>到超平面的裕度，同时最小化该项，来正确分类尽可能多的数据点</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/0ef471e53b4603bc21176c5dcb624caf.png" data-original-src="https://miro.medium.com/v2/resize:fit:128/format:webp/1*9t4RJmDESH4oiEiPrHfBXA.png"/></div></figure><p id="08e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，目标也可以解释为找到大多数样本被正确预测的最优<strong class="lb iu"> w </strong>和<strong class="lb iu"> b </strong>。<br/>大多数情况下，并不是所有的数据点都能完美分配，因此到正确边距的距离由下式表示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/44919141599ad265ad53513659301859.png" data-original-src="https://miro.medium.com/v2/resize:fit:48/format:webp/1*tD0EHgdJVAvvR4Pss5JiiA.png"/></div></figure><p id="fb8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">法向量</em>创建一条穿过坐标原点的线。超平面在一定距离处正交切割这条直线</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/6801f7fb00596a91ce9b7e77c157c463.png" data-original-src="https://miro.medium.com/v2/resize:fit:170/format:webp/1*MT1-LdGqgNA3YrZsuMUvgg.png"/></div></figure><p id="a83b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从原点出发。</p><p id="2390" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于理想情况(毕晓普，第 325 页起。, 2006)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/974af7cc97705eebf76ca3360d5cdda9.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*WLDKO5dZv3ehoLgS-4SH0Q.png"/></div></figure><p id="badc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">会≥ 1，然后被完美预测。现在有了到它们理想位置的距离的数据点，让我们修正≥ 1 到的理想情况</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/34596b6f098a9fc48ebbbd8b437b0b07.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*BnvT-oGs70T8-00IIMCE0w.png"/></div></figure><p id="1247" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同时在最小化公式中引入了惩罚项。c 充当<em class="lv">正则化参数</em>并控制关于有多少数据点被错误地分配了总距离</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/0d291861644d8b5c3a80a0bf174d71ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:162/format:webp/1*C1UicdzUA-lsQcj8WOCqsQ.png"/></div></figure></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="a865" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated"><strong class="ak">对偶问题</strong></h2><p id="b8e5" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">优化任务可以被称为<em class="lv">对偶问题</em>，试图最小化参数，同时最大化余量。为了解决对偶问题，利用拉格朗日乘子(α≥0)。</p><p id="aa35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这导致了一个拉格朗日函数(毕晓普，第 325 页 ff。, 2006):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/4db36ec48c066b002aa8d615a18866d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cLCL9DWYNXAhHt88kLQ9VA.png"/></div></div></figure><p id="3390" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用以下两个条件(毕晓普，第 325 页起。, 2006):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/84d125577e2c4c940cff4ef0641864f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*Jwnwoc8-UWfZPZ448Eruhg.png"/></div></figure><p id="9ba9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">w 和 b 可以从<em class="lv"> L(w，b，a) </em>中消去。这导致下面的拉格朗日函数最大化为(Bishop，p.325 ff。, 2006):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/4fcbfe04f01773d0e2b152f38ea89299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*hurjBqOMUBzgTSXrx1IsDQ.png"/></div></figure><p id="a341" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决优化问题时，可以使用(Bishop，第 325 页及以下)对新的数据点进行分类。, 2006):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/40bcc89055eb94452b9c005a10bbc7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*Ave3OAuzHa81cXlttp88Vg.png"/></div></figure><p id="dbfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于核函数<em class="lv"> k(x_n，x_m) </em>可以填入之前解释的核函数(sigmoid、线性、多项式、rbf)。</p><p id="6a19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">就这样！如果你能理解数学，你现在就能理解支持向量机背后的原理。很容易理解如何将一群数据点分成两类，但是对于多个类是如何做到的呢？让我们看看这是如何工作的。</strong></p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="4315" class="nx ms it bd mt ny pn oa mw ob po od mz jz pp ka nc kc pq kd nf kf pr kg ni oh bi translated">想看更多这样的故事？</h1><h2 id="68da" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated"><a class="ae ky" href="https://medium.com/@hucker.marius/membership" rel="noopener">开始</a></h2></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="d915" class="nx ms it bd mt ny pn oa mw ob po od mz jz pp ka nc kc pq kd nf kf pr kg ni oh bi translated">基于支持向量机的多类分类</h1><p id="363b" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">在其最简单的类型中，SVM 应用于二元分类，将数据点分为 1 或 0。对于多类分类，利用相同的原理。多类问题被分解为多个二元分类情况，也称为<em class="lv">一对一</em>。在 scikit 中，一对一学习不是默认的，需要显式选择(如下面的代码所示)。<em class="lv">一对多</em>设置为默认。它基本上把 x 类和 rest 中的数据点进行了划分。连续地，某一类与所有其他类相区别。</p><p id="ac77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">一对一多类分类</em>所需的分类器数量可通过以下公式得到(n 为类的数量):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/dbef8183e62b86474ed39541fc3c1efd.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*mINM1fqkFO46oX_MeULx6A.png"/></div></figure><p id="20db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一对一方法中，每个分类器分离两个不同类别的点，并且包括所有一对一分类器导致多类别分类器。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="e71c" class="nx ms it bd mt ny pn oa mw ob po od mz jz pp ka nc kc pq kd nf kf pr kg ni oh bi translated">把手举起来。如何使用 scikit-learn 在 Python 中应用它</h1><p id="2925" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">您可能已经对 iris 数据集感到厌烦了，但这是演示它的最简单的方式，所以让我们来看看一些代码。你也可以在<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>下找到部分代码。</p><pre class="kj kk kl km gt pt pu pv pw aw px bi"><span id="2c65" class="mr ms it pu b gy py pz l qa qb"><strong class="pu iu">#Importing the necessary packages and libaries</strong><em class="lv"><br/></em>from sklearn.metrics import confusion_matrix<br/>from sklearn.model_selection import train_test_split<br/>from sklearn import svm, datasets<br/>import matplotlib.pyplot as plt<br/>import numpy as np</span></pre><p id="d867" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们将 iris 数据集作为 iris 加载，并存储目标和特征变量:</p><pre class="kj kk kl km gt pt pu pv pw aw px bi"><span id="d61a" class="mr ms it pu b gy py pz l qa qb">iris = datasets.load_iris()</span><span id="9535" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu">#Store variables as target y and the first two features as X (sepal length and sepal width of the iris flowers)</strong><br/>X = iris.data[:, :2]<br/>y = iris.target</span></pre><p id="fb18" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们在训练和测试集中拆分数据集，以进行以下训练和预测:</p><pre class="kj kk kl km gt pt pu pv pw aw px bi"><span id="a939" class="mr ms it pu b gy py pz l qa qb">X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 0)</span></pre><p id="1846" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一步，我们来看看不同的内核函数。对于所有分类器，惩罚项 C 被设置为 1。对于多类分类，指定一对一类型，如在 decision_function_shape='ovo '中可以看到的。对于多项式函数，选择 3 次，这对于其他核函数是不必要的。<br/>所有其他参数都设置为默认值。在这里你可以阅读更多关于 scikit-learn 的<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank"> SVC 功能。</a></p><pre class="kj kk kl km gt pt pu pv pw aw px bi"><span id="1387" class="mr ms it pu b gy py pz l qa qb">linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)</span><span id="4e72" class="mr ms it pu b gy qc pz l qa qb">rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train, y_train)</span><span id="55b4" class="mr ms it pu b gy qc pz l qa qb">poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(X_train, y_train)</span><span id="170d" class="mr ms it pu b gy qc pz l qa qb">sig = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train, y_train)</span></pre><p id="fdca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们指定网格，我们将在其中绘制结果。</p><pre class="kj kk kl km gt pt pu pv pw aw px bi"><span id="f293" class="mr ms it pu b gy py pz l qa qb"><strong class="pu iu">#stepsize in the mesh, it alters the accuracy of the plotprint<br/>#to better understand it, just play with the value, change it and print it</strong><br/>h = .01</span><span id="bf9d" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu">#create the mesh</strong><br/>x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1<br/>y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1<br/>xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))</span><span id="fc36" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu"># create the title that will be shown on the plot<br/></strong>titles = ['Linear kernel','RBF kernel','Polynomial kernel','Sigmoid kernel']<br/></span></pre><p id="8f86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们将使用一个 for 循环来绘制所有 4 个内核函数:</p><pre class="kj kk kl km gt pt pu pv pw aw px bi"><span id="522c" class="mr ms it pu b gy py pz l qa qb">for i, clf in enumerate((linear, rbf, poly, sig)):<br/>    <strong class="pu iu">#defines how many plots: 2 rows, 2columns=&gt; leading to 4 plots</strong><br/>    plt.subplot(2, 2, i + 1) <strong class="pu iu">#i+1 is the index<br/>    #space between plots</strong><br/>    plt.subplots_adjust(wspace=0.4, hspace=0.4) </span><span id="9130" class="mr ms it pu b gy qc pz l qa qb">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><span id="bdb7" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu">    # Put the result into a color plot</strong><br/>    Z = Z.reshape(xx.shape)<br/>    plt.contourf(xx, yy, Z, cmap=plt.cm.PuBuGn, alpha=0.7)</span><span id="1e7f" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu">    # Plot also the training points</strong><br/>    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.PuBuGn,     edgecolors='grey')</span><span id="8960" class="mr ms it pu b gy qc pz l qa qb">    plt.xlabel('Sepal length')<br/>    plt.ylabel('Sepal width')<br/>    plt.xlim(xx.min(), xx.max())<br/>    plt.ylim(yy.min(), yy.max())<br/>    plt.xticks(())<br/>    plt.yticks(())<br/>    plt.title(titles[i])</span><span id="1985" class="mr ms it pu b gy qc pz l qa qb">    plt.show()</span></pre><p id="b70e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您可能已经认识到的，结果就是文章中上面的图片:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/932952b098eed256fcc257343fa66f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qZFPN60NvwbFE_tRg2YPoQ.png"/></div></div></figure><p id="bdfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步，我们使用 4 个不同的核函数对测试数据集进行预测:</p><pre class="kj kk kl km gt pt pu pv pw aw px bi"><span id="7aab" class="mr ms it pu b gy py pz l qa qb">linear_pred = linear.predict(X_test)<br/>poly_pred = poly.predict(X_test)<br/>rbf_pred = rbf.predict(X_test)<br/>sig_pred = sig.predict(X_test)</span></pre><p id="d739" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了了解它们的表现如何，我们使用了一个性能指标——准确性。</p><pre class="kj kk kl km gt pt pu pv pw aw px bi"><span id="a929" class="mr ms it pu b gy py pz l qa qb"><strong class="pu iu"># retrieve the accuracy and print it for all 4 kernel functions</strong><br/>accuracy_lin = linear.score(X_test, y_test)<br/>accuracy_poly = poly.score(X_test, y_test)<br/>accuracy_rbf = rbf.score(X_test, y_test)<br/>accuracy_sig = sig.score(X_test, y_test)</span><span id="e7f0" class="mr ms it pu b gy qc pz l qa qb">print(“Accuracy Linear Kernel:”, accuracy_lin)<br/>print(“Accuracy Polynomial Kernel:”, accuracy_poly)<br/>print(“Accuracy Radial Basis Kernel:”, accuracy_rbf)<br/>print(“Accuracy Sigmoid Kernel:”, accuracy_sig</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qd"><img src="../Images/0ad04e78ac52c49fad160981e78c49b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x_CYebxxX0naw3JF_6El_A.png"/></div></div></figure><p id="3624" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如精度所揭示的，根据数据的不同，一些核函数比其他的更有用。显然，更多的数据也有助于改善结果(虹膜数据的大小并不大，只有 50 个样本:-)。</p><p id="1ef7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们进入最后一步——打印 4 个内核函数的混淆矩阵，以了解预测的方式和内容:</p><pre class="kj kk kl km gt pt pu pv pw aw px bi"><span id="e744" class="mr ms it pu b gy py pz l qa qb"><strong class="pu iu"># creating a confusion matrix<br/></strong>cm_lin = confusion_matrix(y_test, linear_pred)<br/>cm_poly = confusion_matrix(y_test, poly_pred)<br/>cm_rbf = confusion_matrix(y_test, rbf_pred)<br/>cm_sig = confusion_matrix(y_test, sig_pred)</span><span id="d9fb" class="mr ms it pu b gy qc pz l qa qb">print(cm_lin)<br/>print(cm_poly)<br/>print(cm_rbf)<br/>print(cm_sig)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/d51b112c535662bdef5c65128536ade4.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*NTIkRUPo6ePkASniUX-Qtg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">4 个混淆矩阵</p></figure><p id="5937" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样。我希望它有助于更好地理解支持向量机，数学概念，核心技巧和主题多类分类通过 SVM。</p><p id="0fc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">感谢阅读，感谢反馈！</strong></p><div class="oj ok gp gr ol om"><a href="https://medium.com/subscribe/@hucker.marius" rel="noopener follow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd iu gy z fp or fr fs os fu fw is bi translated">请继续关注马里乌斯·哈克的新文章</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">请继续关注 Marius Hucker 的新文章。如果您还没有注册，您将创建一个中型帐户…</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">medium.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa ks om"/></div></div></a></div></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="cff1" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">来源</h2><ol class=""><li id="1db6" class="lw lx it lb b lc nk lf nl li qf lm qg lq qh lu ns mc md me bi translated">毕晓普，2006，<a class="ae ky" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" rel="noopener ugc nofollow" target="_blank">模式识别与机器学习</a>，第 325 页起。</li><li id="f3ee" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">Vapnik 和 Cortes，1995 年，<a class="ae ky" href="https://link.springer.com/content/pdf/10.1007/BF00994018.pdf" rel="noopener ugc nofollow" target="_blank">支持向量网络</a></li><li id="dc77" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">Scikit-learn，<a class="ae ky" href="https://scikit-learn.org/stable/modules/svm.html" rel="noopener ugc nofollow" target="_blank">支持向量机</a></li><li id="db44" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">Scikit-learn，<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html" rel="noopener ugc nofollow" target="_blank">在虹膜数据集中绘制不同的 SVM 分类器</a></li><li id="9711" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu ns mc md me bi translated">Dataflair，<a class="ae ky" href="https://data-flair.training/blogs/svm-kernel-functions/" rel="noopener ugc nofollow" target="_blank">内核函数—SVM 内核简介&amp;示例</a></li></ol></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="6fa3" class="mr ms it bd mt mu mv dn mw mx my dp mz li na nb nc lm nd ne nf lq ng nh ni nj bi translated">整个代码</h2><pre class="kj kk kl km gt pt pu pv pw aw px bi"><span id="5abc" class="mr ms it pu b gy py pz l qa qb"><strong class="pu iu">#Importing the necessary packages and libaries</strong><em class="lv"><br/></em>from sklearn.metrics import confusion_matrix<br/>from sklearn.model_selection import train_test_split<br/>from sklearn import svm, datasets<br/>import matplotlib.pyplot as plt<br/>import numpy as np</span><span id="8b62" class="mr ms it pu b gy qc pz l qa qb">iris = datasets.load_iris()</span><span id="62ec" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu">#Store variables as target y and the first two features as X (sepal length and sepal width of the iris flowers)</strong><br/>X = iris.data[:, :2]<br/>y = iris.target</span><span id="3192" class="mr ms it pu b gy qc pz l qa qb">linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)</span><span id="0022" class="mr ms it pu b gy qc pz l qa qb">rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train, y_train)</span><span id="df38" class="mr ms it pu b gy qc pz l qa qb">poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(X_train, y_train)</span><span id="9ad5" class="mr ms it pu b gy qc pz l qa qb">sig = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train, y_train)</span><span id="37b2" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu">#stepsize in the mesh, it alters the accuracy of the plotprint<br/>#to better understand it, just play with the value, change it and print it</strong><br/>h = .01</span><span id="209c" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu">#create the mesh</strong><br/>x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1<br/>y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1<br/>xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))</span><span id="cd4b" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu"># create the title that will be shown on the plot<br/></strong>titles = ['Linear kernel','RBF kernel','Polynomial kernel','Sigmoid kernel']</span><span id="03e8" class="mr ms it pu b gy qc pz l qa qb">for i, clf in enumerate((linear, rbf, poly, sig)):<br/>    <strong class="pu iu">#defines how many plots: 2 rows, 2columns=&gt; leading to 4 plots</strong><br/>    plt.subplot(2, 2, i + 1) <strong class="pu iu">#i+1 is the index<br/>    #space between plots</strong><br/>    plt.subplots_adjust(wspace=0.4, hspace=0.4)</span><span id="661b" class="mr ms it pu b gy qc pz l qa qb">Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><span id="e9e2" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu"># Put the result into a color plot</strong><br/>    Z = Z.reshape(xx.shape)<br/>    plt.contourf(xx, yy, Z, cmap=plt.cm.PuBuGn, alpha=0.7)</span><span id="995d" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu"># Plot also the training points</strong><br/>    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.PuBuGn,     edgecolors='grey')</span><span id="e8c0" class="mr ms it pu b gy qc pz l qa qb">plt.xlabel('Sepal length')<br/>    plt.ylabel('Sepal width')<br/>    plt.xlim(xx.min(), xx.max())<br/>    plt.ylim(yy.min(), yy.max())<br/>    plt.xticks(())<br/>    plt.yticks(())<br/>    plt.title(titles[i])</span><span id="8f49" class="mr ms it pu b gy qc pz l qa qb">plt.show()</span><span id="89ec" class="mr ms it pu b gy qc pz l qa qb">linear_pred = linear.predict(X_test)<br/>poly_pred = poly.predict(X_test)<br/>rbf_pred = rbf.predict(X_test)<br/>sig_pred = sig.predict(X_test)</span><span id="5d65" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu"># retrieve the accuracy and print it for all 4 kernel functions</strong><br/>accuracy_lin = linear.score(X_test, y_test)<br/>accuracy_poly = poly.score(X_test, y_test)<br/>accuracy_rbf = rbf.score(X_test, y_test)<br/>accuracy_sig = sig.score(X_test, y_test)</span><span id="19af" class="mr ms it pu b gy qc pz l qa qb">print(“Accuracy Linear Kernel:”, accuracy_lin)<br/>print(“Accuracy Polynomial Kernel:”, accuracy_poly)<br/>print(“Accuracy Radial Basis Kernel:”, accuracy_rbf)<br/>print(“Accuracy Sigmoid Kernel:”, accuracy_sig</span><span id="77a1" class="mr ms it pu b gy qc pz l qa qb"><strong class="pu iu"># creating a confusion matrix<br/></strong>cm_lin = confusion_matrix(y_test, linear_pred)<br/>cm_poly = confusion_matrix(y_test, poly_pred)<br/>cm_rbf = confusion_matrix(y_test, rbf_pred)<br/>cm_sig = confusion_matrix(y_test, sig_pred)</span><span id="6a4e" class="mr ms it pu b gy qc pz l qa qb">print(cm_lin)<br/>print(cm_poly)<br/>print(cm_rbf)<br/>print(cm_sig)</span></pre></div></div>    
</body>
</html>