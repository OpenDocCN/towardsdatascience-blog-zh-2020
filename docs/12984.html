<html>
<head>
<title>Style Transfer for Line Drawings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线条画的风格转换</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/style-transfer-for-line-drawings-3c994492b609?source=collection_archive---------50-----------------------#2020-09-06">https://towardsdatascience.com/style-transfer-for-line-drawings-3c994492b609?source=collection_archive---------50-----------------------#2020-09-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e7c6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用 ML 从线条画生成图像</h2></div><p id="5939" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我将以类似教程的方式介绍我最近做的一个机器学习项目。这是一种从线条画中以艺术风格生成完整图像的方法。</p><h1 id="f31d" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">资料组</h1><p id="8495" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">我在<a class="ae ly" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> Imagenet </a>数据集的 10%上进行训练。这是计算机视觉任务中常用的基准数据集。Imagenet 数据集不是公开可用的；它仅限于那些需要使用它来计算性能基准以便与其他方法进行比较的研究人员。因此，通常要求您提交一份申请表。但是如果你只是随便用的话，这里有<a class="ae ly" href="https://academictorrents.com/collection/imagenet-2012" rel="noopener ugc nofollow" target="_blank">。除了个人项目之外，我不会用这个做任何事情。请注意，数据集非常大，这就是为什么我只用了 1/10 来训练我的模型。它由 1000 个类组成，所以我用了其中的 100 个图像类进行训练。</a></p><p id="11aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">几周前，我在一个不同的个人项目中使用了 Imagenet，所以我已经在 Google Drive 中收集了大量文件。然而，不幸的是，将这 140，000 张图片上传到 Google Drive 需要大约 20 个小时。有必要在 Google Colab 的在线 GPU 上训练模型，但这需要你将图像上传到 Google Drive，因为你没有在本地托管你的编码环境。</p><h1 id="2f29" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">数据输入管道</h1><p id="26cc" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">我有一个 Colab Pro 帐户，但即使有额外的 RAM，我也肯定无法处理 140，000 个线条画，每个都是 256x256 像素大小，以及它们的 256x256 像素彩色对应物。因此，我必须使用 TensorFlow 数据输入管道随时加载数据。</p><p id="5286" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们开始设置管道之前，让我们导入所需的库(这些是我的代码中的所有导入语句):</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="d8ee" class="mi lc iq me b gy mj mk l ml mm">import matplotlib.pyplot as plt<br/>import numpy as np<br/>import cv2<br/>from tqdm.notebook import tqdm<br/>import glob<br/>import random<br/>import threading, queue</span><span id="1b10" class="mi lc iq me b gy mn mk l ml mm">from tensorflow.keras.models import *<br/>from tensorflow.keras.layers import *<br/>from tensorflow.keras.optimizers import *<br/>from tensorflow.keras.regularizers import *<br/>from tensorflow.keras.utils import to_categorical<br/>import tensorflow as tf</span></pre><p id="70bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们加载指向 Imagenet 子集中每个图像的文件路径，假设您已经将它们上传到适当目录结构下的 Drive，并将 Google Colab 实例连接到 Google Drive。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="72e8" class="mi lc iq me b gy mj mk l ml mm">filepaths = glob.glob("drive/My Drive/1-100/**/*.JPEG")</span><span id="b3a9" class="mi lc iq me b gy mn mk l ml mm"># Shuffle the filepaths<br/>random.shuffle(filepaths)</span></pre><p id="d631" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你不想使用<code class="fe mo mp mq me b">glob</code>模块，你可以使用<code class="fe mo mp mq me b">os</code>库中的函数，这样通常效率更高。</p><p id="86a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里有几个我需要的助手函数:</p><ul class=""><li id="3f58" class="mr ms iq kh b ki kj kl km ko mt ks mu kw mv la mw mx my mz bi translated">标准化数据</li><li id="e135" class="mr ms iq kh b ki na kl nb ko nc ks nd kw ne la mw mx my mz bi translated"><em class="nf">色调分离</em>图像数据</li></ul><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="260d" class="mi lc iq me b gy mj mk l ml mm">def normalize(x):<br/>  return (x - x.min()) / (x.max() - x.min())</span></pre><h2 id="b1bc" class="mi lc iq bd ld ng nh dn lh ni nj dp ll ko nk nl ln ks nm nn lp kw no np lr nq bi translated">色调分离</h2><p id="6742" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">上述色调分离过程将图像作为输入，并通过将颜色值舍入到某个最接近的值，将平滑的渐变转换为更清晰分离的颜色部分。这里有一个例子:</p><figure class="lz ma mb mc gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi nr"><img src="../Images/69a45b2fd8ef74add0d6602a71efec76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iE5DY2GY2iEp_CptkhMJ5Q.png"/></div></div><p class="nz oa gj gh gi ob oc bd b be z dk translated">色调分离</p></figure><p id="bc76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如您所看到的，生成的图像具有不太平滑的渐变，这些渐变被替换为单独的颜色部分。我实现这一点的原因是因为我可以将输出图像限制为一组颜色，允许我将学习问题格式化为图像中每个像素的分类问题。对于每一种可用的颜色，我分配一个标签。该模型通过最后一个通道<code class="fe mo mp mq me b">num_colors</code>输出由 softmax 函数激活的形状<code class="fe mo mp mq me b">(height, width, num_colors)</code>的图像。给定一个变量<code class="fe mo mp mq me b">num_values</code>，我允许颜色值限制在<code class="fe mo mp mq me b">np.arange(0, 255, 255 / num_values)</code>的所有 RGB 组合。这意味着<code class="fe mo mp mq me b">num_colors = num_values ** 3</code>。这里有一个例子:</p><figure class="lz ma mb mc gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi od"><img src="../Images/3324f3fcfe91209d7c501b57db204e3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6vGtr9cRNhykPm6q2Fsqvw.png"/></div></div><p class="nz oa gj gh gi ob oc bd b be z dk translated">色调分离</p></figure><p id="fd88" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为我如何实现这一点的例子，这里有一个演示:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="0737" class="mi lc iq me b gy mj mk l ml mm">def get_nearest_color(color, colors):<br/>  """<br/>  Args:<br/>   - color: A vector of size 3 representing an RGB color<br/>   - colors: NumPy array of shape (num_colors, 3)<br/>  Returns:<br/>   - The index of the color in the provided set of colors that is<br/>     closest in value to the provided color<br/>  """</span><span id="7034" class="mi lc iq me b gy mn mk l ml mm">  return np.argmin([np.linalg.norm(color - c) for c in colors])</span><span id="9e44" class="mi lc iq me b gy mn mk l ml mm">def posterize_with_limited_colors(image, colors):<br/>  """<br/>  Args:<br/>   - colors: NumPy array of shape (num_colors, 3)<br/>  Returns:<br/>   - Posterized image of shape (height, width, 1), where each value <br/>     is an integer label associated with a particular index of the<br/>     provided colors array<br/>  """</span><span id="cab9" class="mi lc iq me b gy mn mk l ml mm">  image = normalize(image)<br/>  posterized = np.array([[get_nearest_color(x, colors) for x in y] for y in image])<br/>  return posterized</span></pre><h2 id="918a" class="mi lc iq bd ld ng nh dn lh ni nj dp ll ko nk nl ln ks nm nn lp kw no np lr nq bi translated">边缘抽取</h2><p id="552f" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">为了从我们的彩色图像中创建输入数据，我们需要一种从图像中提取边缘的方法，这种方法类似于描画或画线。</p><p id="6bb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用<a class="ae ly" href="https://en.wikipedia.org/wiki/Canny_edge_detector" rel="noopener ugc nofollow" target="_blank"> Canny 边缘检测算法</a>。让我们编写助手函数，它输入图像的路径并输出相关的示例/(X，Y)训练对，包括彩色输入的色调分离以及黑白边缘提取:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="9451" class="mi lc iq me b gy mj mk l ml mm">def preprocess(path):<br/>  color = cv2.imread(path)<br/>  color = cv2.resize(color, input_image_size)</span><span id="bfe5" class="mi lc iq me b gy mn mk l ml mm">  # Assuming your pipelines generator function ignores None<br/>  if color.shape &lt; input_image_size:<br/>    return None, None</span><span id="4037" class="mi lc iq me b gy mn mk l ml mm">  color = (normalize(color) * 255).astype(np.uint8)</span><span id="2609" class="mi lc iq me b gy mn mk l ml mm">  gray = cv2.cvtColor(color, cv2.COLOR_RGB2GRAY<br/>  # Removes noise while preserving edges<br/>  filtered = cv2.bilateralFilter(gray, 3, 60, 120)</span><span id="0357" class="mi lc iq me b gy mn mk l ml mm">  # Automatically determine threshold for edge detection algorithm<br/>  # based upon the median color value of the image<br/>  m = np.median(filtered)<br/>  preservation_factor = 0.33<br/>  low = max(0, int(m - 255 * preservation_factor))<br/>  high = int(min(255, m + 255 * preservation_factor))<br/>  filtered_edges = cv2.Canny(filtered, low, high)<br/>  filtered_edges = normalize(filtered_edges)<br/>  filtered_edges = np.expand_dims(filtered_edges, axis = -1)</span><span id="5b7f" class="mi lc iq me b gy mn mk l ml mm">  color = cv2.resize(color, output_image_size)<br/>  color /= 255.<br/>  color = posterize_with_limited_colors(color, colors)</span><span id="4fa9" class="mi lc iq me b gy mn mk l ml mm">  return filtered_edges, color</span></pre><p id="6303" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自动 Canny 边缘检测只是我对这篇文章中使用的小函数的修改。</p><h2 id="b220" class="mi lc iq bd ld ng nh dn lh ni nj dp ll ko nk nl ln ks nm nn lp kw no np lr nq bi translated">管道</h2><p id="a460" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">正如我所说的，我正在使用输入管道现场加载数据。因此，我需要定义一个生成器对象，以便在需要时加载这些数据。我的生成器函数很简单，因为我们基本上刚刚定义了它。它所添加的只是过滤掉<code class="fe mo mp mq me b">preprocess</code>函数的<code class="fe mo mp mq me b">None</code>输出(分辨率低于<code class="fe mo mp mq me b">input_image_size</code>的图像),并过滤掉任何包含<code class="fe mo mp mq me b">nan</code>或<code class="fe mo mp mq me b">inf</code>值的结果。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="6ae1" class="mi lc iq me b gy mj mk l ml mm">def generate_data(paths):<br/>  for path in paths:<br/>    edges, color = preprocess(path.decode())<br/>    if not edges is None:<br/>      if not np.any(np.isnan(edges)) or np.any(np.isnan(color)):<br/>        if not np.any(np.isinf(edges)) or np.any(np.isinf(color))):<br/>          # Yield the clean data<br/>          yield edges, color</span></pre><p id="03db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我用<code class="fe mo mp mq me b">(128, 128)</code>来表示<code class="fe mo mp mq me b">input_image_size</code>和<code class="fe mo mp mq me b">output_image_size</code>。128x128 像素的图像并不是低分辨率的，所以对于我们的目的来说没有明显的缺点。此外，Imagenet 图像的分辨率通常要高得多，因此如果需要，我们可以使用更高的分辨率。</p><p id="caef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们建立管道。我使用多线程来提高速度。TensorFlow 的<code class="fe mo mp mq me b">.interleave()</code>允许我们这样做:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="0186" class="mi lc iq me b gy mj mk l ml mm">thread_names = np.arange(0, 8).astype(str)<br/>dataset = tf.data.Dataset.from_tensor_slices(thread_names)</span><span id="5aa8" class="mi lc iq me b gy mn mk l ml mm">dataset = dataset.interleave(lambda x:<br/>  tf.data.Dataset.from_generator(<br/>    generate_data,<br/>    output_types = (tf.float32, tf.float32),<br/>    output_shapes = ((*input_image_size, 1),<br/>                     (*output_image_size, 1)),<br/>    args = (train_paths,)),<br/>    cycle_length = 8,<br/>    block_length = 8,<br/>    num_parallel_calls = 8)</span><span id="c7f1" class="mi lc iq me b gy mn mk l ml mm">dataset = dataset.batch(batch_size).repeat()</span></pre><h2 id="ca0d" class="mi lc iq bd ld ng nh dn lh ni nj dp ll ko nk nl ln ks nm nn lp kw no np lr nq bi translated">测试管道</h2><p id="5612" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">让我们通过管道加载一个培训示例:</p><figure class="lz ma mb mc gt ns gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/04b6110ac7b0df1b703a3049d83c8106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*-M_BHRyGO6RLS_StW6nMdg.png"/></div><p class="nz oa gj gh gi ob oc bd b be z dk translated">一个带有输入线条画/边缘(右)和输出着色(左)的训练示例</p></figure><p id="f808" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完全如你所愿。请注意，左边描绘的图像并不完全是管道输出的图像。回想一下，管道返回的是引用每个像素颜色的索引。我只是引用了每种相关的颜色来创建可视化。这里有一个简单得多的例子。</p><figure class="lz ma mb mc gt ns gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f4bad428c8796535f55c8a254496f2fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*6bVWk6OcwKShF4nvqRZGig.png"/></div><p class="nz oa gj gh gi ob oc bd b be z dk translated">更简单的培训示例</p></figure><p id="3faf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你会看到在左边我们有输出，色调分离的彩色图像，这部分类似于一幅画。在右侧，您可以看到输入边提取，类似于草图。</p><p id="bbef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，并不是所有的训练样本都比其他样本具有更好的边缘提取。当颜色更难分离时，产生的轮廓可能会有点嘈杂和/或分散。然而，这是我能想到的提取边缘最准确的方法。</p><h1 id="a4dd" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">模型架构</h1><p id="b39b" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">让我们继续讨论模型架构。</p><p id="4a31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我从<code class="fe mo mp mq me b">input_image_size = (128, 128)</code>开始，从而在展开最后一个轴后输入形状<code class="fe mo mp mq me b">(128, 128, 1)</code>。我将图层输入形状缩小 2 倍，直到它等于 1。然后，我用<code class="fe mo mp mq me b">stride = 1</code>再应用两个卷积层，因为我们不能再减少前两个轴的形状了。然后，我对转置的层执行相反的操作。每个卷积层有<code class="fe mo mp mq me b">padding = 'valid'</code>，每个卷积层之间有一个批量归一化层。所有卷积层都有 ReLU 激活，除了最后一个，当然它在最后一个热编码颜色标签通道上有 softmax 激活。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="f535" class="mi lc iq me b gy mj mk l ml mm">_________________________________________________________________ Layer (type)                 Output Shape              Param #    ================================================================= input_35 (InputLayer)        [(None, 128, 128, 1)]     0          _________________________________________________________________ conv2d_464 (Conv2D)          (None, 64, 64, 3)         30         _________________________________________________________________ batch_normalization_388 (Bat (None, 64, 64, 3)         12         _________________________________________________________________ conv2d_465 (Conv2D)          (None, 32, 32, 9)         252        _________________________________________________________________ batch_normalization_389 (Bat (None, 32, 32, 9)         36         _________________________________________________________________ conv2d_466 (Conv2D)          (None, 16, 16, 27)        2214       _________________________________________________________________ batch_normalization_390 (Bat (None, 16, 16, 27)        108        _________________________________________________________________ conv2d_467 (Conv2D)          (None, 8, 8, 81)          19764      _________________________________________________________________ batch_normalization_391 (Bat (None, 8, 8, 81)          324        _________________________________________________________________ conv2d_468 (Conv2D)          (None, 4, 4, 243)         177390     _________________________________________________________________ batch_normalization_392 (Bat (None, 4, 4, 243)         972        _________________________________________________________________ conv2d_469 (Conv2D)          (None, 2, 2, 729)         1595052    _________________________________________________________________ batch_normalization_393 (Bat (None, 2, 2, 729)         2916       _________________________________________________________________ conv2d_470 (Conv2D)          (None, 1, 1, 2187)        14351094   _________________________________________________________________ batch_normalization_394 (Bat (None, 1, 1, 2187)        8748       _________________________________________________________________ conv2d_471 (Conv2D)          (None, 1, 1, 2187)        43048908   _________________________________________________________________ batch_normalization_395 (Bat (None, 1, 1, 2187)        8748       _________________________________________________________________ conv2d_472 (Conv2D)          (None, 1, 1, 2187)        43048908   _________________________________________________________________ batch_normalization_396 (Bat (None, 1, 1, 2187)        8748       _________________________________________________________________ conv2d_transpose_229 (Conv2D (None, 1, 1, 2187)        43048908   _________________________________________________________________ batch_normalization_397 (Bat (None, 1, 1, 2187)        8748       _________________________________________________________________ conv2d_transpose_230 (Conv2D (None, 1, 1, 2187)        43048908   _________________________________________________________________ batch_normalization_398 (Bat (None, 1, 1, 2187)        8748       _________________________________________________________________ conv2d_transpose_231 (Conv2D (None, 2, 2, 2187)        43048908   _________________________________________________________________ batch_normalization_399 (Bat (None, 2, 2, 2187)        8748       _________________________________________________________________ conv2d_transpose_232 (Conv2D (None, 4, 4, 2187)        43048908   _________________________________________________________________ batch_normalization_400 (Bat (None, 4, 4, 2187)        8748       _________________________________________________________________ conv2d_transpose_233 (Conv2D (None, 8, 8, 729)         14349636   _________________________________________________________________ batch_normalization_401 (Bat (None, 8, 8, 729)         2916       _________________________________________________________________ conv2d_transpose_234 (Conv2D (None, 16, 16, 243)       1594566    _________________________________________________________________ batch_normalization_402 (Bat (None, 16, 16, 243)       972        _________________________________________________________________ conv2d_transpose_235 (Conv2D (None, 32, 32, 81)        177228     _________________________________________________________________ batch_normalization_403 (Bat (None, 32, 32, 81)        324        _________________________________________________________________ conv2d_transpose_236 (Conv2D (None, 64, 64, 27)        19710      _________________________________________________________________ up_sampling2d_1 (UpSampling2 (None, 128, 128, 27)      0          _________________________________________________________________ batch_normalization_404 (Bat (None, 128, 128, 27)      108        ================================================================= Total params: 290,650,308 Trainable params: 290,615,346 Non-trainable params: 34,962 _________________________________________________________________</span></pre><h1 id="b8dd" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">培养</h1><p id="1528" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">让我们创建一些列表来存储整个培训过程中的指标。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="26af" class="mi lc iq me b gy mj mk l ml mm">train_losses, train_accs = [], []</span></pre><p id="2b7d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，训练时期数量的变量</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="202c" class="mi lc iq me b gy mj mk l ml mm">epochs = 100</span></pre><p id="3325" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是我们的训练脚本</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="c578" class="mi lc iq me b gy mj mk l ml mm">for epoch in range(epochs):<br/>  random.shuffle(filepaths)<br/>  history = model.fit(dataset,<br/>                      steps_per_epoch = steps_per_epoch,<br/>                      use_multiprocessing = True,<br/>                      workers = 8,<br/>                      max_queue_size = 10)</span><span id="1246" class="mi lc iq me b gy mn mk l ml mm">  train_loss = np.mean(history.history["loss"])<br/>  train_acc = np.mean(history.history["accuracy"])</span><span id="aa90" class="mi lc iq me b gy mn mk l ml mm">  train_losses = train_losses + history.history["loss"]<br/>  train_accs = train_accs + history.history["accuracy"]</span><span id="2a84" class="mi lc iq me b gy mn mk l ml mm">  print ("Epoch: {}/{}, Train Loss: {:.6f}, Train Accuracy: {:.6f}, Val Loss: {:.6f}, Val Accuracy: {:.6f}".format(epoch + 1, epochs, train_loss, train_acc, val_loss, val_acc))</span><span id="5e98" class="mi lc iq me b gy mn mk l ml mm">  if epoch &gt; 0:<br/>    fig = plt.figure(figsize = (10, 5))<br/>    plt.subplot(1, 2, 1)<br/>    plt.plot(train_losses)<br/>    plt.xlim(0, len(train_losses) - 1)<br/>    plt.xlabel("Epoch")<br/>    plt.ylabel("Loss")<br/>    plt.title("Loss")<br/>    plt.subplot(1, 2, 2)<br/>    plt.plot(train_accs)<br/>    plt.xlim(0, len(train_accs) - 1)<br/>    plt.ylim(0, 1)<br/>    plt.xlabel("Epoch")<br/>    plt.ylabel("Accuracy")<br/>    plt.title("Accuracy")<br/>    plt.show()</span><span id="6f44" class="mi lc iq me b gy mn mk l ml mm">  model.save("model_{}.h5".format(epoch))<br/>  np.save("train_losses.npy", train_losses)<br/>  np.save("train_accs.npy", train_accs)</span></pre></div></div>    
</body>
</html>