<html>
<head>
<title>Generative Classification Algorithms from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的生成分类算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generative-classification-algorithms-from-scratch-d6bf0a81dcf7?source=collection_archive---------18-----------------------#2020-09-05">https://towardsdatascience.com/generative-classification-algorithms-from-scratch-d6bf0a81dcf7?source=collection_archive---------18-----------------------#2020-09-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="52cc" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/ml-from-scratch" rel="noopener" target="_blank">机器从零开始学习</a></h2><div class=""/><div class=""><h2 id="2ac5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">艾达、QDA 和朴素贝叶斯</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/686b7b006da589b9ab951d4aaccb0712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cb9Q2VK9h3fNOg61Wzd0XQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">二次判别分析决策边界。(图片由作者提供。<a class="ae lh" href="https://dafriedman97.github.io/mlbook/content/c4/construction.html" rel="noopener ugc nofollow" target="_blank">来源</a>。)</p></figure><p id="1788" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">概率生成算法(如朴素贝叶斯、线性判别分析和二次判别分析)已经成为流行的分类工具。这些方法可以通过<a class="ae lh" href="https://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>在 Python 中轻松实现，或者通过<a class="ae lh" href="https://cran.r-project.org/web/packages/e1071/e1071.pdf" rel="noopener ugc nofollow" target="_blank"> e1071 </a>在 R 中轻松实现。但是这些方法实际上是如何工作的呢？本文从零开始推导它们。</p><p id="a5b0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">(注意本文改编自我的书<a class="ae lh" href="https://dafriedman97.github.io/mlbook/content/introduction.html" rel="noopener ugc nofollow" target="_blank">机器从零开始学习</a>中的一章，网上免费提供)。</p><h1 id="eda2" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">符号和词汇</h1><p id="4dd7" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在本文中，我们将使用以下约定。</p><ul class=""><li id="c6a8" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated">设<strong class="lk jd"> v </strong> [ <em class="nk"> i </em> ]为向量<strong class="lk jd"> v </strong>中的第<em class="nk"> i </em>个条目。</li><li id="7f9a" class="nb nc it lk b ll nl lo nm lr nn lv no lz np md ng nh ni nj bi translated"><em class="nk">目标</em>是我们试图建模的变量。预测值是我们用来模拟目标的变量。</li><li id="c54d" class="nb nc it lk b ll nl lo nm lr nn lv no lz np md ng nh ni nj bi translated">目标是一个标量，记为<em class="nk"> y. </em>预测值组合成一个矢量，记为<strong class="lk jd"> <em class="nk"> x </em> </strong>。我们还假设<strong class="lk jd"> <em class="nk"> x </em> </strong>中的第一个条目是 1，对应截距项。</li><li id="ba30" class="nb nc it lk b ll nl lo nm lr nn lv no lz np md ng nh ni nj bi translated"><em class="nk">P(</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">)</em>指的是<strong class="lk jd"><em class="nk"/></strong>x<em class="nk">P(y = k)</em>指的是<em class="nk"> y </em>等于<em class="nk"> k 的概率</em></li></ul><h1 id="0203" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">1.生成分类</h1><p id="5ec0" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">大多数分类算法分为两类:判别分类器和生成分类器。判别分类器将目标变量<em class="nk"> y </em>建模为预测变量<strong class="lk jd"> <em class="nk"> x </em> </strong>的直接函数。例如，逻辑回归使用以下模型，其中<strong class="lk jd"><em class="nk"/></strong>是系数的长度-D 向量，而<strong class="lk jd"> <em class="nk"> x </em> </strong>是预测值的长度-D 向量:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/cf5b2e1aa8d4c7d6667065abd4e9cdbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IBpC-43gPqF_Ritq.png"/></div></div></figure><p id="e3cb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">逻辑回归模型</p><p id="9de4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">相反，生成分类器将预测器视为根据其类别生成的——即，它们将<strong class="lk jd"> x </strong>视为<em class="nk"> y </em>的函数，而不是相反。然后他们使用贝叶斯法则从<em class="nk">P(</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">| y = k)</em>到<em class="nk">P(y = k |</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">)</em>，如下所述。</p><p id="4c07" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">创成式模型可以分为以下三个步骤。假设我们有一个分类任务，有<em class="nk"> K 个</em>无序类，用<em class="nk"> k = 1，2，…，K. </em>表示</p><ol class=""><li id="ffa8" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md nr nh ni nj bi translated">估计目标属于任何给定类别的<em class="nk">先验</em>概率。即对于<em class="nk"> k = 1，2，…，K. </em>估计<em class="nk"> P(y = k) </em></li><li id="519f" class="nb nc it lk b ll nl lo nm lr nn lv no lz np md nr nh ni nj bi translated">在属于每个类别的目标上估计预测值<em class="nk">条件</em>的密度。即估计<em class="nk">p(</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">| y = k)</em>为<em class="nk"> k = 1，2，…，K. </em></li><li id="ccab" class="nb nc it lk b ll nl lo nm lr nn lv no lz np md nr nh ni nj bi translated">计算目标属于任何给定类别的后验概率。即通过贝叶斯法则计算出<em class="nk">P(y = k |</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">)</em>与<em class="nk">P(</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">| y = k)P(y = k)</em>。</li></ol><p id="2078" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们将一个观察结果分类为属于类别<em class="nk"> k </em>，对于该类别，下面的表达式是最大的:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/7676ea8ea011c9c3547812f15af855c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rNQ2cq6nKyBpNMQF.png"/></div></div></figure><p id="279b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，我们不需要<em class="nk">p(</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">)</em>，它将是贝叶斯规则公式中的分母，因为它在所有类中都是相等的。</p><h1 id="9bd2" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">2.模型结构</h1><p id="c70c" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">生成分类器模拟两种来源的随机性。首先，我们假设在𝐾可能的类之外，每个观察独立地属于𝑘类，其概率由向量<strong class="lk jd"><em class="nk"/></strong><em class="nk">中的第<em class="nk"> k </em>个条目给出。</em>即<strong class="lk jd"><em class="nk">𝝅</em></strong><em class="nk">【k】</em>给出<em class="nk"> P(y = k)。</em></p><p id="c985" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第二，我们假设<strong class="lk jd"><em class="nk"/></strong>的一些分布以<em class="nk"> y. </em>为条件。我们通常假设<strong class="lk jd"><em class="nk"/></strong>x 来自于相同的<em class="nk">族</em>分布，而不管<em class="nk"> y，</em>如何，尽管它的参数取决于类。例如，我们可以假设</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c49673e9841e20025ae2deabf491d494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/0*YlPOA9rNJnwfhfv5.png"/></div></figure><p id="d5ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然我们不会假设<strong class="lk jd"> <em class="nk"> x </em> </strong>如果<em class="nk"> y = 1 </em>是分布 MVN 而是分布多元- <em class="nk"> t </em>否则<em class="nk">。</em>注意，然而，向量<strong class="lk jd"> <em class="nk"> x </em> </strong>中的各个变量可能遵循不同的分布。例如，我们可以假设<strong class="lk jd"><em class="nk"/></strong>中的<em class="nk"> i </em> th 和<em class="nk"> j </em> th 变量分布如下</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/dfe02fc253d43dc32b87332ed23a823e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/0*aMkvR0pBDqNnf_iv.png"/></div></figure><p id="e10b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，机器学习任务是估计这些分布的参数——目标变量<em class="nk"> y </em>的<strong class="lk jd"> <em class="nk"> 𝝅 </em> </strong>以及索引<strong class="lk jd"><em class="nk">x</em></strong><em class="nk">| y = k</em>(在上述第一种情况下，<strong class="lk jd"><em class="nk"/><em class="nk">_ k</em><strong class="lk jd"><em class="nk"/></strong>和<strong class="lk jd">𝚺<strong class="lk jd">) K. 一旦完成，我们就可以为每个类计算<em class="nk"> P(y = k) </em>和<em class="nk">P(</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">| y = k)</em>。 然后通过贝叶斯法则，选择使<em class="nk"> P 最大化的类<em class="nk">k</em>(y = k |</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">)。</em></strong></strong></strong></p><h1 id="1a36" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">3.参数估计</h1><p id="a7ce" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">现在让我们开始估计模型的参数。回想一下，我们计算<em class="nk">P(y = k |</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">)</em>用</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/2897ce8255a25fea0842238efad3f340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jIQus0Q_pJ-3YJMN.png"/></div></div></figure><p id="2497" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了计算这个概率，我们需要首先估计<strong class="lk jd"><em class="nk"/></strong>(这告诉我们<em class="nk"> P(y = k) </em>)然后再估计分布<em class="nk">p(</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">| y = k)中的参数。这些被称为类别先验和数据似然性。</em></p><p id="515a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意:由于我们将讨论跨观测值的数据，所以让<em class="nk"> y_n </em>和<strong class="lk jd"><em class="nk">x</em></strong><em class="nk">_ n</em>分别作为第<em class="nk"> n </em>次观测值的目标和预测值。(下面的数学在原<a class="ae lh" href="https://dafriedman97.github.io/mlbook/content/c4/concept.html" rel="noopener ugc nofollow" target="_blank">本</a>中稍微整齐一点。)</p><h1 id="7eae" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">3.1 类别优先级</h1><p id="9096" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">让我们从推导<strong class="lk jd"><em class="nk"/></strong>的估计开始，类别先验。设<em class="nk"> I_nk </em>为指示器，如果<em class="nk"> y_n = k </em>则等于 1，否则等于 0。我们希望在给定数据的情况下找到一个表达式来表示<strong class="lk jd"> <em class="nk"> 𝝅 </em> </strong>的可能性。我们可以将第一次观察具有目标值的概率写为<strong class="lk jd"> </strong>如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/5b0162f5743e0b7a2573102820ccf3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/0*B6KydX6j3qdozUu1.png"/></div></figure><p id="0fc8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这相当于<strong class="lk jd"> <em class="nk"> 𝝅 </em> </strong>给定单个目标变量的可能性。要找出所有变量的可能性，我们只需使用乘积:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/303a6663a32136caa8678d637db00d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/0*53uHP5Oenx11TsaD.png"/></div></figure><p id="7ac3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这给了我们类先验概率。为了估计<strong class="lk jd"> <em class="nk"> 𝝅 </em> </strong>通过的最大似然，我们先取对数。这给了</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/2f41d256f98aaae01f6b3e09dbfefe0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wfvK-QGkY-WzCsO8.png"/></div></div></figure><p id="8198" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中类别<em class="nk"> k </em>中的观测值数量由下式给出</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/46781dc192e685a550a716a9f9708469.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/0*g_BpLpbQBWqXdc5G.png"/></div></figure><p id="f8fc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们准备通过优化对数似然来寻找<strong class="lk jd"> <em class="nk"> 𝝅 </em> </strong>的 MLE。为此，我们需要使用拉格朗日函数，因为我们有一个约束条件，即<strong class="lk jd"><em class="nk"/></strong>中的条目之和必须等于 1。这个优化问题的拉格朗日函数如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/89e99f0081f62a6debd9f55ead5b1390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/0*XFGV_OpWN9W0qJcN.png"/></div></figure><p id="433b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">拉格朗日最优化。第一个表达式表示对数似然，第二个表示约束。</p><p id="43bb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">更多关于拉格朗日的内容可以在<a class="ae lh" href="https://dafriedman97.github.io/mlbook/content/c4/concept.html#class-priors" rel="noopener ugc nofollow" target="_blank">原著</a>中找到。接下来，我们对𝜆和<strong class="lk jd"><em class="nk"/></strong>中的每一项进行拉格朗日导数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/5e17ba8bb25aa3a3faa15094116a94a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*A0-j3uihqT2ItsE5.png"/></div></div></figure><p id="72f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个方程组给出了下面直观的解，即我们对<em class="nk"> P(y = k) </em>的估计只是来自<em class="nk"> k. </em>类的观察值的样本分数</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/a7b69fce591b8ec8edf000610045180c.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/0*PTluz9WqK0vtsGkK.png"/></div></figure><h1 id="8344" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">3.2 数据可能性</h1><p id="b3f6" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">下一步是对给定<em class="nk"> y </em>的<strong class="lk jd"> <em class="nk"> x </em> </strong>的条件分布进行建模，以便我们可以估计这个分布的参数。这当然取决于我们选择用来建模<strong class="lk jd"> <em class="nk"> x </em> </strong>的分布族。下面详细介绍三种常见的方法。</p><p id="8990" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 3.2.1 线性判别分析</strong></p><p id="36ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在 LDA 中，我们假设下面的分布为<strong class="lk jd"> <em class="nk"> x </em> </strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d135433554e45b86550c93f471b1f450.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/0*qAlu1lfVhcPdbS3D.png"/></div></figure><p id="f225" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于<em class="nk"> k = 1，2，…，k。</em>注意，每个类都有相同的协方差矩阵，但有唯一的均值向量。</p><p id="bb79" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们推导这种情况下的参数估计。首先，我们来求似然和对数似然。请注意，我们可以将所有观测值的联合似然性写成</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi od"><img src="../Images/dfcf78796c642f3870a15630ee328b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/0*F-2r541l6UESx59M.png"/></div></figure><p id="22a8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/03b91500d0f8adb3a047aa48e072bb7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PVV00R45ugnG4SQv.png"/></div></div></figure><p id="1187" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们插入多元正态 PDF(去掉乘法常数)并取对数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/2dc6d9275eb512c3ea276e9b58b371a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Q3RLX4eYOU9MgruG.png"/></div></div></figure><p id="a220" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们有我们的数据可能性。现在我们通过最大化这个表达式来估计参数。</p><p id="a0d6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们从𝚺.开始首先，简化对数似然，使相对于<strong class="lk jd"> 𝚺 </strong>的梯度更加明显。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/98adad81d46bbd160cd84d39963f140f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fWlUdtkhyQ5LtfGH.png"/></div></div></figure><p id="6214" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们求导。注意，这里使用了“数学笔记”<a class="ae lh" href="https://dafriedman97.github.io/mlbook/content/c4/concept.html#data-likelihood" rel="noopener ugc nofollow" target="_blank">中介绍的矩阵导数(2)和(3)</a>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/b772bd72b437c71fcdc82f5a38ddb013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lkgidxPYsgf0z2Y8.png"/></div></div></figure><p id="07db" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后我们设置这个梯度等于 0，并求解<strong class="lk jd"> 𝚺.</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/78d5f525697e8ef0eabedaa2fe0910bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pZboipT5Zbfmy-xv.png"/></div></div></figure><p id="ac88" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在哪里</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3c42f007c3ded92f56e76fc27b696164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/0*90Zryx3D_wl3Vp3_.png"/></div></figure><p id="c0f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">半路上！现在来估计<strong class="lk jd"><em class="nk"/></strong><em class="nk">_ k</em>(第<em class="nk"> k </em>类的均值向量)，让我们单独看一下每个类。设<em class="nk"> C_k </em>是类<em class="nk"> k. </em>中的一组观测值，只看涉及<strong class="lk jd"> <em class="nk"> 𝝁 </em> </strong> <em class="nk"> _k、</em>的项，我们得到</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/0aa6f97106e357b4b162f09cdef73015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nP4rpboN_R4vnbhb.png"/></div></div></figure><p id="38fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用“数学笔记”<a class="ae lh" href="https://dafriedman97.github.io/mlbook/content/c4/concept.html#data-likelihood" rel="noopener ugc nofollow" target="_blank">中的等式(4)在这里</a>，我们得到梯度为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/661fc4bf37b56e525420bb40fd9e5b84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/0*rEZTxKtswslMQVQb.png"/></div></figure><p id="ed4c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们将这个梯度设置为 0，并找到我们对平均向量的估计:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/61ca22dba92c0ca2d3071a9482ba0db6.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/0*xrhspGG59Wj59kvw.png"/></div></figure><p id="6222" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中最后一项给出了类别<em class="nk"> k. </em>中<strong class="lk jd"> <em class="nk"> x </em> </strong>的样本均值</p><p id="8faf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 3.2.2 二次判别分析(QDA) </strong></p><p id="efe0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">QDA 看起来非常类似于 LDA，但是假设每个类都有自己的协方差矩阵。即，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/60d7fd2420e042fff7180df1a94b0bc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/0*6BzUoFW2ox4lo7x5.png"/></div></figure><p id="47d4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对数似然在 LDA 中是相同的，除了我们把<strong class="lk jd"> 𝚺 </strong>与<strong class="lk jd"> 𝚺 </strong> <em class="nk"> _k: </em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/7dd1a2357b32a9609c7e6ad3227c2a5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2PI8pW6qDK70h3X_.png"/></div></div></figure><p id="35f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">同样，让我们单独看看第<em class="nk"> k </em>类的参数。类别<em class="nk"> k </em>的对数似然由下式给出</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/4967e44558c02745db3610332ee80361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dCUbeax7R6IvnPMg.png"/></div></div></figure><p id="f934" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以取这个对数似然相对于<strong class="lk jd"><em class="nk">【𝝁</em></strong><em class="nk">_ k</em>的梯度，并将其设为 0，以求解我们对<em class="nk"/><strong class="lk jd"><em class="nk">【𝝁</em></strong><em class="nk">_ k</em>的估计。然而，我们也可以注意到，LDA 方法的这个估计是成立的，因为这个表达式不依赖于协方差项(这是我们唯一改变的)。因此，我们再次得到</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/9071c6bc677b843698b4b80e7ba027d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/0*7PlDHoI6lA6wTUuO.png"/></div></figure><p id="4c90" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了估计<strong class="lk jd"> 𝚺 </strong> <em class="nk"> _k，</em>我们取类<em class="nk"> k. </em>的对数似然的梯度</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/fd744a027262531eff9131543ed8bd06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oxgWov90oqQ3BG8X.png"/></div></div></figure><p id="3d45" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后我们将它设为 0，得到我们的估计值:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/c6d8f28af4844dd9a78a9aaba018240e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F559gYI6v2rFL5YW.png"/></div></div></figure><p id="983c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在哪里</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi os"><img src="../Images/ff77991efe7b8db3c73518bf7808afb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/0*75whk6pyTL8yrV-s.png"/></div></figure><p id="4ab4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">3.2.3 朴素贝叶斯</p><p id="dff7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">朴素贝叶斯假设<strong class="lk jd"> <em class="nk"> x </em> </strong>内的随机变量是<em class="nk">独立的</em>取决于观察的类别。即如果<strong class="lk jd"> <em class="nk"> x </em> </strong>是 D 维的，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/95b2c25644409407d9773d2d6db45589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HDoip-ZEZD9ny24n.png"/></div></div></figure><p id="fc22" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这使得计算<em class="nk">p(</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">| y = k)</em>变得非常容易——为了估计<em class="nk">p(</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">【j】| y)</em>除了<em class="nk"> j </em> th 之外，我们可以忽略<strong class="lk jd"> <em class="nk"> x </em> </strong>中的所有变量。</p><p id="8252" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">作为一个例子，假设<strong class="lk jd"> <em class="nk"> x </em> </strong>是二维的，我们使用下面的模型，其中为了简单起见，σ是已知的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/97f753b56fb65b6ea6eabc8773975957.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*V9sB3Y1PiDcWygdS.png"/></div></figure><p id="ef04" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">和以前一样，我们通过只查看每一类中的项来估计每一类中的参数。设<strong class="lk jd"> θ </strong> <em class="nk"> _k = (μ_k，σ_k，p_k) </em>包含类别<em class="nk"> k 的相关参数</em>类别<em class="nk"> k </em>的可能性由下式给出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/7c9391d808b8a38fda89f9925111d0a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2ApRvfqooRqzjOyN.png"/></div></div></figure><p id="d646" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中，由于<strong class="lk jd"> <em class="nk"> x </em> </strong>中的条目之间的假设独立性，两者相等。代入法向密度和伯努利密度分别为<em class="nk"> x_n1 </em>和<em class="nk"> x_n2，</em><em class="nk">，</em>我们得到</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/c5f118369a17e5f8b5bd049136fe710a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JIQ8C_EFccOoT0WC.png"/></div></div></figure><p id="993c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么我们可以采用如下的对数似然</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/9261c12fbfafedd1861d2172442e0e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kDFVZYwPu4qAmDAF.png"/></div></div></figure><p id="8ac0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们准备寻找我们的估计。对<em class="nk"> p_k 求导，</em>我们得到</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a818da782fb38dc667ba3a07c5e2480d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/0*r9_G1MT_DTCquIjZ.png"/></div></figure><p id="f637" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这将给我们一个合理的结果</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/2fbd62377dcb5cbedd221b087ccef43b.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/0*RUl0Yqe0WpoS3mOP.png"/></div></figure><p id="deec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，这只是 x_2 的平均值。同样的过程将给出<em class="nk"> μ_k </em>和<em class="nk"> σ_k. </em>的典型结果</p><h1 id="11cc" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">4.进行分类</h1><p id="eebc" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">不管我们对<em class="nk">p(</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">| y = k)</em>的建模选择如何，对新的观察值进行分类都很容易。考虑一个测试观察值<strong class="lk jd"><em class="nk">×x</em></strong><em class="nk">_ 0</em>。对于<em class="nk"> k = 1，2，…，K </em>，我们用贝叶斯法则计算</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/c60ce24854a079124ca11479433e542d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/0*h--L_7Ta78f9i72V.png"/></div></figure><p id="8539" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中𝑝̂给出了以<em class="nk"> y_0 为条件的<strong class="lk jd"><em class="nk">x</em></strong><em class="nk">_ 0</em>的估计密度。然后，我们预测最大化上述表达式的任何值 k。</em></p><h1 id="9d64" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">结论</h1><p id="b02f" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">LDA、QDA 和朴素贝叶斯等生成模型是最常用的分类方法。然而，他们的试衣过程的细节(尽管是艰苦的)经常被掩盖。这篇文章的目的是让这些细节清晰。</p><p id="368f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然为生成模型估计参数的低级细节可能相当复杂，但高级直觉相当简单。让我们用几个简单的步骤来回顾一下这种直觉。</p><ol class=""><li id="e942" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md nr nh ni nj bi translated">估计观测值来自任何给定类别的先验概率<em class="nk"> k. </em>在数学中，估计<em class="nk"> p(y = k) </em>的每个值<em class="nk"> k. </em></li><li id="8a2b" class="nb nc it lk b ll nl lo nm lr nn lv no lz np md nr nh ni nj bi translated">估计预测值的密度<em class="nk">条件</em>对观测值的分类。即估计<em class="nk"> k. </em>的每个值的<em class="nk">p(</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">| y = k)</em></li><li id="545e" class="nb nc it lk b ll nl lo nm lr nn lv no lz np md nr nh ni nj bi translated">使用贝叶斯规则获得一个观察值来自任何给定其预测值的类<em class="nk">的概率(达到一个比例常数):<em class="nk">p(y = k |</em><strong class="lk jd"><em class="nk">x</em></strong><em class="nk">)。</em></em></li><li id="6538" class="nb nc it lk b ll nl lo nm lr nn lv no lz np md nr nh ni nj bi translated">选择哪个值<em class="nk"> k </em>最大化第三步中的概率(称之为<em class="nk"> k*) </em>估计<em class="nk"> y = k*。</em></li></ol><p id="0b5b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就是这样！要想从头看到更多像这样的衍生，请查看我的免费在线书籍<a class="ae lh" href="https://dafriedman97.github.io/mlbook" rel="noopener ugc nofollow" target="_blank">！我保证他们大部分数学都比较差。</a></p></div></div>    
</body>
</html>