# 充分利用有限的硬件进行计算机视觉竞赛

> 原文：<https://towardsdatascience.com/making-the-most-out-of-limited-hardware-for-computer-vision-kaggle-competitions-a8894511026e?source=collection_archive---------34----------------------->

## 关于在只有少量资源的情况下训练和改进计算机视觉模型的一些想法。

神经网络在过去几年中一直是计算机视觉任务的重要组成部分，自从现在臭名昭著的 2012 年 AlexNet 日以来，它们已经改变了不同图像识别任务的完成方式:对象检测，图像分类，语义分割，一直到创建新图像或用 GANs 修改现有图像。有无数的博客帖子、文章和关于他们历史的讨论，以及关于如何用几行代码构建一个简单模型的教程。

然而，这些工具需要越来越多的计算能力，因为学术界和研究机构似乎非常倾向于创建越来越深的模型，这些模型有大量的参数，除了最大的实验室和公司之外，它们需要不切实际的计算量。作为个人，甚至是一个小团体，这些都是完全够不着的；虽然有很多很棒的教程、介绍等等。我已经写了一些关于如何开始使用 X 或 Y framework 的文章，但感觉似乎缺少“实用”指南，如何帮助那些没有最新最好的计算机硬件的人。

希望这篇文章能为如何降低一些计算机视觉任务的计算预算提供一些思路。这篇文章主要围绕 Kaggle 竞赛，但也可以推广到许多不同的应用。Kaggle 是一个学习数据科学和计算机视觉的好地方，但可能会非常令人生畏，尤其是在刚开始的时候，而且自从 Kaggle 引入了每周 30 小时的 GPU 使用时间限制以来，情况就更是如此。起初，这听起来更像是一个挑战，与以前的比赛相比，资源有限，在以前的比赛中，一个人可以旋转 4 个 GPU 笔记本电脑实例来通宵运行(现在我想起来，这可能是限制的原因，但更实际的是阻止人们挖掘密码和其他非故意的用途)。

我必须先说一些背景情况:由于我的工程背景，我在数学和物理方面有相当扎实的背景，但在数据和计算机科学方面，我几乎是自学的。其中很大一部分要归功于 Kaggle，以及我们可以在几乎无限的时间内免费获得的计算资源(这是大约一年前写这篇文章的时候)。这允许许多错误和修补，而不必担心浪费或必须密切跟踪宝贵的 GPU 资源。希望这可以帮助那些在他们自己的旅途中不再奢侈但仍然想学习这个的人(过度？)在深度学习的时代，计算机视觉是一个时髦但非常有趣的领域。

这里有一个前提:对于几乎任何相对复杂的计算机视觉任务/比赛来说，深度学习都是当今的艺术状态。深度学习非常需要 GPU 来进行训练。这个想法是，我们如何分解一切，只在我们需要的时候使用 GPU，而不再使用？(在更广泛的范围内，这也适用于现实世界应用程序的深度学习模型的实际开发，在这些应用程序中，当用 GPU 打包时，旋转云实例的成本要高得多)。

数据科学，尤其是计算机视觉任务、竞赛、问题等。需要创建一个模型，该模型获取(大部分时间是带标签的)数据，并从中“学习”以识别标签中的任何模式。这意味着传递图像(或任何其他类型的数据)，并不断调整和改变模型的内部结构，以最好地减少损失函数。虽然我们不会在这里详细讨论为什么和如何做(同样，网上有足够多的书面主题)，但记住大图仍然很重要。传递数据和调整模型的这一步需要繁重的 GPU(或 TPU)处理。然而，这只是制作模型的步骤之一，有限的原始计算能力意味着我们必须尽可能多地从训练前后的所有步骤中挤出时间。预处理和错误分析是我们将在这里讨论的前后步骤的两个例子。

![](img/d680cc2d2e6457c2751672c5625466dc.png)

[清理数据代表了数据科学家的大部分工作](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#5edc1a416f63)。虽然这对竞争机器学习来说不一定是真的，但在一些竞争中仍然有一些空间

当进行 Kaggle 比赛时，这些 30 小时的 GPU 时间意味着当你将笔记本电脑切换到使用 GPU 时，时钟正在滴答作响，即使你所做的都是 CPU 绑定的。从那里，我试着只在 CPU 上做一些明显的任务，比如数据准备/探索。但我试图更进一步，尽可能减少所有可以删除的非 GPU 操作。这主要适用于可以在图像上进行的任何预处理。我说的不是数据扩充，而是调整图像大小、裁剪、居中、标准化等。我通常用这些预处理过的图像创建一个新的数据集，这样我只需要将它们加载到一个模型中，就可以开始了。

最初，在尝试微调模型之前，我想确保我的模型确实在工作。过早优化是浪费时间，也是浪费 GPU 时间。当我开始一个新项目时，我总是试图创建一个微小的分层数据集，代表我的更大的数据集，以使训练花费很短的时间(我在这里说的是几秒钟)。我真的很喜欢[这篇推文&关于“10 秒规则”](https://twitter.com/jeremyphoward/status/1224664283780284420)的文章，我会尽可能地应用它。我们的想法是尽可能快地让事情运转起来。此时模型的输出并不重要，它只需要输出一些东西。这意味着要注意校正形状、尺寸、张量输入/输出等。这通常需要一点时间，如果我也能在 CPU 上完成，那我就去做。我发现许多在线教程和讨论向您展示了如何制作模型，但从未展示如何从数据集开始创建模型(这是假设您首先有一个数据集，很多时候大量工作实际上是在创建它)。然而，似乎大部分时间都花在了调试上，试图找出模型不输出的原因，试图找出一个人的 GPU VRAM 可以容纳多大的批量，等等。所有这些步骤要么不需要 GPU，要么可以用原始数据集的很小一部分来完成。

另一个技巧是试图减少图像本身的大小，而不仅仅是用于测试的数量。我甚至可以更进一步，将我所有的图像转换成一个单一的通道(灰度)而不是 RGB，但这将扰乱模型的架构，所以我通常保留 RGB，但显著减小图像的大小。最棒的是，如果你把高度和宽度减少 2，你就把图像的尺寸减少了 4。这意味着(大约)4 倍的训练速度。显然，您会损失一些信息(从而损失性能)，但是以我个人的浅见，首先尝试和测试这些东西是值得的。我也开始研究一些降维方法，或者图像的压缩算法，试图在保持尽可能多的信息的同时减小图像的大小。主成分分析(PCA)常用于表格数据，可用于图像；我以前在处理卫星图像时使用它来减少总的数据集大小。简而言之，一些卫星图像有多达 13 个波段(与通常的 3 个波段，红、蓝和绿形成对比)，但使用 PCA 将这些波段减少到 3 个，可以在该任务中产生更好的结果和更快的训练。可能有更聪明的方法来做到这一点，这是一个例子，表明有许多方法来减少数据集的大小，同时试图保留尽可能多的有用信息。

当在训练一个更好的模型的过程中，我也通常不在 GPU 上做任何推断。一旦我有了一个训练好的模型，我就保存它，但之后我通常会移动到我的笔记本电脑上的一个本地实例(在 2Gb VRAM mx150 上这也很棘手，但它总是比 CPU 好)。对来自验证集的一些图像进行推断，以查看模型的结果实际上是什么样子，并了解模型学习得好还是不好。推断比训练快得多，因为在训练模型的过程中，通常也只有较少的图像可以推断。这是一个较长时间不成问题的步骤，因为它可以在模型在 GPU 上训练时完成。由于某种原因，错误分析似乎相当被忽视，至少从 Kaggle 比赛中可以找到的公众评论和笔记本来看是如此。然而，了解模型的正确之处和错误之处是非常有益的。我有物理学和机械工程的背景，几乎每一个现象都有物理定律，这让我深深地理解为什么事物会以这样的方式反应。然而，似乎我们仍然有一个非常试错的方法来进行深度学习微调。深入的错误分析，除了看假阳性和假阴性之外，似乎很容易被忽视，但我认为这对于竞赛和现实世界的应用来说是很有价值的。例如，[类激活图](http://cnnlocalization.csail.mit.edu/) (CAMs)提供了对图像的哪些部分用于进行预测的洞察。假设你有猫的室内图像和狗的室外图像，并且正在为猫和狗制作二元分类器。如果你所有的猫图像都在室内，所有的狗图像都在室外，我们怎么知道模型是在分类猫&狗还是室内&室外？一种方法是从户外的猫和室内的狗那里获得更多的数据，但是摄像头也可以提供一些答案。这些不需要太多的处理能力，也不需要额外的数据。

![](img/85dd712fb8e1ef9c8ef288d333ce4fa1.png)

孟加拉文字游戏的多输出分类器上的[类激活图示例。这些可以让我们看到模型在进行预测时使用最多的图像区域(红色)。](https://www.kaggle.com/maxlenormand/multi-class-activation-map-with-efficientnetb0)

还有一些事情，比如试图找到最有效的架构，但通常，这些想法会被广泛地探索。该领域正从 ResNets 转向高效的网络架构、更好的性能、更快的培训和双赢！如果没有像样的 GPU，这些很难评估，而且又一次不得不与所有能够访问它们的人竞争。无法访问高端计算意味着试图在模型架构微调之外的其他主题中改善结果。

直到最近，在 Kaggle 上，在笔记本上运行一个模型意味着必须重新运行它来提交它，或者在完成后下载它(但这意味着让浏览器标签一直打开，有崩溃的风险，并可能失去几个小时的宝贵培训时间。但是如果你犯了，在完成之前你根本不知道发生了什么。如果，在 3 个时代之后，你的模型在做垃圾但是仍然运行，你不一定知道。这在做 k 倍训练时更糟糕，从而不得不训练多个模型。因此，为了解决这个问题，我和一个朋友花了一些时间利用 [Slack Python API](/python-and-slack-a-natural-match-60b136883d4d?source=bookmarks---------6------------------&gi=c277b9a067fb) ，在每个模型训练结束时向我们发送一条消息(我计划在每个时期结束时实现它，但还没有这样做)。每次训练结束后，我都会给自己发一条信息，上面有损失+指标、训练时间等图表。这样，如果我有一个 5 折模型培训，第一折后不如我想象的好，我就可以手动停止提交，防止我的配额白白减少。

使用 Slack API 发送消息(一旦设置正确)非常简单:

```
def send_message_to_slack(channel,
                          message):

    slack.chat.post_message(channel=channel,
                           username='YourName',
                           text=message)message = 'Message to send'
send_message_to_slack(channel = 'kaggle_notification',           message=message)
```

在这个例子中，我在我用于个人项目的 Slack 上创建了一个`# kaggle_notification`通道，我在每个 epoch、fold 或任何其他地方的末尾调用`send_message_to_slack`。例如，通过一些修补，还可以发送日志或预测的图像。请注意，您当然需要您的脚本来访问互联网。

![](img/fa1b7b04f52d7b55bdf115c33ff89044.png)

在为[孟加拉文字游戏比赛](https://www.kaggle.com/c/bengaliai-cv19/overview)训练多输出 EfficientNet 分类器时，我会给自己发送一个 Slack 消息的例子。这里是第 4 个时期所有 3 个输出的结果。

没有最新最棒的计算并不意味着无法为一个试图获得最佳结果的团队做出贡献。在最近的一次计算机视觉中，[我加入了一个团队，该团队在没有提供太多模型微调支持的情况下，而是通过在预处理和错误分析上花费时间，取得了前 3%的成绩](https://www.kaggle.com/c/bengaliai-cv19/discussion/137024)。除了在 GPU 上运行更大模型的更暴力的方法之外，还有许多更有趣的主题可以挖掘。这意味着试图理解我们的模型是如何学习的，它在哪里出错了，多久出错一次，它在哪里做对了，等等。这使我们能够知道调整学习率优化或改变模型中的一个层首先是否有意义。

尝试不同的模型、架构并花些时间对它们进行微调仍然很重要。Google Colab 是一个很好的开始方式(他们的付费层对于一些很棒的资源来说是相当实惠的！)，并找到[一些其他的技巧来充分利用它](https://www.kaggle.com/c/bengaliai-cv19/discussion/133857#763596)。就像 Kaggle 笔记本一样，当试图在相对较大的数据集上训练大模型时，这些都有其局限性。但同样地，寻找技巧来最大限度地利用它们也是乐趣的一部分。

最后，有些事情我也想做得更好，但我认为对整个数据科学来说相当棘手:跟踪实验及其结果，模型超参数的每个变化如何影响结果。我多次重复做一个实验，因为我没有正确地记录以前测试过的所有东西。注释代码和使用 git 看起来在十几个实验中工作得很好，但是不能扩展到更多。作为一个团队在一个单一的模型上工作也是很棘手的，这是我想更好地在上面浪费更少的时间和资源。[几个月前(撰写本文时)，安德烈·卡帕西就这个主题做了一次精彩的演讲。](https://www.youtube.com/watch?v=IHH47nZ7FZU)

最后，我真的认为对硬件访问的严格限制会引发一些创造性思维，问问自己为什么要做你正在做的事情，如果没有 GPU 来扔模型，你怎么工作。然而，我也认为这种方法有一些限制。到目前为止，你只能使用 CPU 或低端 GPU，而且你仍然需要更强大的硬件来完成计算机视觉任务。我认为记住这一点很重要，尽管我很喜欢不怎么使用 GPU 的失败者的故事。最终，当我有钱时，我会给自己买一个 GPU，因为我确实认为它释放了很多潜力。但我也很高兴我在没有强大硬件的情况下开始了数据科学，尤其是计算机视觉！我认为这是一个很好的锻炼，我觉得这对我的专业工作有很大的帮助。了解如何找到瓶颈，如何用一小部分数据在几个小时内运行一个非常基本的模型；所有这些都是很好的学习练习。