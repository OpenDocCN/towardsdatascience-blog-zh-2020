<html>
<head>
<title>Opening the Black Box of Clustering — KMeans</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">打开集群的黑匣子— KMeans</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/opening-the-black-box-of-clustering-kmeans-e970062ff415?source=collection_archive---------29-----------------------#2020-02-23">https://towardsdatascience.com/opening-the-black-box-of-clustering-kmeans-e970062ff415?source=collection_archive---------29-----------------------#2020-02-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="34f0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">无监督学习系列的第一部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e76d00282359e435f476a1a457a4884c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QtBRTHHjgHGdi4h5"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Brian Kostiuk 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="778a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是聚类的三部分系列的第一部分，其中我将介绍一些最流行的聚类算法，包括<strong class="lb iu"> K-Means </strong>、<strong class="lb iu">凝聚聚类</strong>和<strong class="lb iu">高斯混合模型</strong>。这些是分别基于分区/距离、层次和密度的不同聚类方法。</p><p id="5723" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文专门介绍 K-Means 聚类。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><blockquote class="mc md me"><p id="f996" class="kz la mf lb b lc ld ju le lf lg jx lh mg lj lk ll mh ln lo lp mi lr ls lt lu im bi translated">“无监督学习是未来大多数人的学习方式。你脑子里有这个世界如何运转的模型，你正在对它进行提炼，以预测你认为未来会发生什么。”— <strong class="lb iu">马克·扎克伯格</strong></p></blockquote><p id="7474" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无监督学习形成了机器学习的一个非常小众的部分，只是因为大多数任务都有一个标签(<em class="mf">监督</em>)。然而，在我们缺少这些标记为<em class="mf">的</em>数据的情况下，聚类方法可以通过对数据集进行推断来帮助我们找到模式。应用聚类的常见领域包括客户细分(针对广告定位)、人口分析(了解人口统计数据)以及异常检测。</p><p id="c1d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些人认为无监督学习是机器学习中的一个灰色区域，因为有时很难解释算法输出的聚类类型，因为没有单一的“度量”可以告诉我们这些预测的聚类有多有意义。</p><h1 id="11b0" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">k 均值</h1><p id="2857" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">K-means 聚类是一种基于<em class="mf">距离的</em>聚类方法，用于在一组未标记的数据中寻找聚类和聚类中心。这是一个久经考验的方法，可以使用<em class="mf"> sci-kit learn </em>轻松实现。</p><p id="304b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-Means 的目标相当简单——将“相似”(基于距离)的点分组在一起。这是通过将数据点的中心视为相应聚类(质心)的中心来实现的。</p><p id="e19f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">核心思想是这样的:通过迭代计算更新聚类质心，迭代过程将继续，直到满足某些收敛标准。</p><h2 id="a1a4" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">工作原理:</h2><ol class=""><li id="527c" class="ns nt it lb b lc nb lf nc li nu lm nv lq nw lu nx ny nz oa bi translated">首先，选择期望的集群数量，比如说<em class="mf"> K </em>。</li><li id="50cc" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated">从数据集中选择 K 个随机点作为聚类质心。</li><li id="94b8" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated">在每一步，选择一个未分配的数据点，并检查哪个质心最接近它。亲密度的定义通常由距离度量来衡量，通常以欧几里德距离的形式。</li><li id="575c" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated">为每个点分配一个聚类后，使用同一聚类内数据点的<em class="mf">平均值</em>重新计算新的聚类质心。这用于优化群集质心的位置。</li><li id="e884" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated">重复步骤 3 和 4，直到<strong class="lb iu"> a) </strong>质心已经稳定(不超过阈值)或者<strong class="lb iu"> b) </strong>已经达到期望的迭代次数。</li></ol><p id="936d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，质心实际上不是数据集中的实际数据点。</p><h2 id="25ad" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">使用虹膜数据集的示例</h2><p id="4f00" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">对于那些不熟悉这个数据集的人来说，它包含了<em class="mf">鸢尾</em>花的变异数据，特征包括<code class="fe og oh oi oj b">sepal_length</code>、<code class="fe og oh oi oj b">sepal_width</code>、<code class="fe og oh oi oj b">petal_length</code>和<code class="fe og oh oi oj b">petal_width</code>。由于这是一个无人监督的问题，我不会透露有多少种不同类型的虹膜(<em class="mf">要了解更多关于数据集的信息，请访问此</em> <a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noopener ugc nofollow" target="_blank"> <em class="mf">链接</em> </a>)。</p><p id="aae6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用<code class="fe og oh oi oj b">iris</code>数据集来展示 K-Means 如何工作。让我们首先导入数据并将其可视化。</p><pre class="kj kk kl km gt ok oj ol om aw on bi"><span id="f5e9" class="ng mk it oj b gy oo op l oq or">import numpy as np<br/>from sklearn import datasets, cluster<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="78c5" class="ng mk it oj b gy os op l oq or">iris = datasets.load_iris()<br/>x = iris.data[:, [1,3]] # takes the 2nd and 4th column of the data<br/>plt.scatter(x[:,0], x[:,1])<br/>plt.xlabel('Sepal width')<br/>plt.ylabel('Petal width')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/8d03cc850077e57c2cca25ac67be315f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*4M5dSui0MXnPTmG0v_uIEA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">上述代码块的输出</p></figure><p id="1cbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的形象化来看，似乎有两个不同的星团。现在，让我们使用 sci-kit learn 的 K-Means 算法。</p><pre class="kj kk kl km gt ok oj ol om aw on bi"><span id="a8f5" class="ng mk it oj b gy oo op l oq or">from sklearn.cluster import KMeans</span><span id="5233" class="ng mk it oj b gy os op l oq or">x_ = iris.data # Note that in we use all 4 columns here<br/>random_state = 2020</span><span id="b7ae" class="ng mk it oj b gy os op l oq or">kmeans = KMeans(n_clusters=2, init='k-means++', random_state=random_state)<br/>kmeans.fit_transform(x_)<br/>label = kmeans.labels_<br/>color = ['red','green','blue']</span><span id="3e1d" class="ng mk it oj b gy os op l oq or">plt.figure(figsize=(8, 6))<br/>plt.scatter(x_fit[:,0], x_fit[:,1], c=[color[i] for i in label], marker='+')<br/>plt.title('Kmeans on Iris dataset with 2 clusters', fontweight='bold', fontsize=14)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/3b45bd45277342ff4ad6a5c3bcb776e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*3vdgXKBPihEg2Y8esEAW7w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">虹膜数据集上的 k-均值算法</p></figure><p id="d8a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来使用<code class="fe og oh oi oj b">n_clusters = 2</code>的分离产生了相当不错的结果，除了少量可能被错误聚类的数据点(中间的绿色数据点应该是红色的)。</p><p id="5eb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面，我们通过视觉检查数据(使用两个特征)来确定集群的数量<code class="fe og oh oi oj b">n_clusters</code>。很明显，确定最优的<code class="fe og oh oi oj b">n_clusters</code>是聚类中非常重要的一步，尤其是 K-Means，这个数字必须在算法中预先指定。通过使用两个特征来直观地检查数据显然是不够的，因为如果我们的数据是高维的，可能会有额外的聚类，并且我们的人类判断可能会失败(特别是对于任何更三维的数据)。</p><h2 id="070f" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">如何确定最佳聚类数</h2><p id="d32d" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated"><strong class="lb iu">方法 1: </strong>弯头绘图</p><p id="802d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该方法使用 sklearn 的<code class="fe og oh oi oj b">inertia_</code>方法计算总的<strong class="lb iu">类内误差平方和</strong> (SSE)。肘部扭结的点(梯度急剧变化)表明收益递减，我们通常希望在这种情况发生之前采取<code class="fe og oh oi oj b">n_clusters</code>。直觉上，我们想要一个最小化 SSE 的数字<code class="fe og oh oi oj b">k</code>，但同时增加的<code class="fe og oh oi oj b">k</code>会自然地将 SSE 减少到零(其中每个数据点都是它自己的聚类)。在下面的例子中，<code class="fe og oh oi oj b">n_clusters = 3</code>似乎是最理想的。</p><pre class="kj kk kl km gt ok oj ol om aw on bi"><span id="3d0e" class="ng mk it oj b gy oo op l oq or">elbow = []<br/>kmax = 10</span><span id="365d" class="ng mk it oj b gy os op l oq or">for k in range(2, kmax+1):<br/>    kmeans = KMeans(n_clusters = k).fit(x_)<br/>    elbow.append(kmeans.inertia_)<br/>    <br/>plt.figure(figsize=(8,6))<br/>plt.plot(np.arange(2,11), elbow)<br/>plt.xlabel('Number of Clusters')<br/>plt.ylabel('Inertia (Intra cluster sum of squares)')<br/>plt.title('Inertia vs n_clusters to determine optimal cluster size', fontweight='bold')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/e02d79619da8b69106d98b18c88b34d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*9m7VQlCZi11rsyHHOD-v-Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">利用惯性确定最佳 n _ 簇</p></figure><p id="27a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，当数据不是很集中时，这种方法可能不太适用。因此，用我们的下一个方法做交叉检查可能更好。</p><p id="b695" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">方法 2: </strong>剪影图</p><p id="2cbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">轮廓系数本质上就是我们所说的<em class="mf">簇间</em>距离。一般来说，聚类旨在最小化类内距离，同时最大化类间距离。使用这两种方法将确保<code class="fe og oh oi oj b">n_clusters</code>被更好地定义。</p><pre class="kj kk kl km gt ok oj ol om aw on bi"><span id="ce41" class="ng mk it oj b gy oo op l oq or">from sklearn.metrics import silhouette_score<br/>sil = []<br/>kmax = 10</span><span id="fe02" class="ng mk it oj b gy os op l oq or">for k in range(2, kmax+1):<br/>    kmeans = KMeans(n_clusters = k).fit(x_)<br/>    labels = kmeans.labels_<br/>    sil.append(silhouette_score(x_, labels, metric = 'euclidean'))<br/>    <br/>plt.figure(figsize=(8,6))<br/>plt.plot(np.arange(2,11), elbow)<br/>plt.xlabel('Number of Clusters')<br/>plt.ylabel('Silhouette Coefficient (Inter-cluster distance)')<br/>plt.title('Silhouette vs n_clusters to determine optimal cluster size', fontweight='bold')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/92cb001c60ef0edb91fede579d9e0694.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*Qe7_V2dqxdZtqcPFNcfaKw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">利用剪影系数确定最佳 n 簇</p></figure><p id="c4a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理想情况下，我们希望轮廓系数尽可能高(即最大化聚类间距离)。在这种情况下，<code class="fe og oh oi oj b">n_clusters = 2</code>似乎是最理想的。</p><p id="384e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">协调肘部和轮廓之间的差异</strong></p><p id="127d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然手肘图提示<code class="fe og oh oi oj b">n_clusters = 3</code>而侧影图提示<code class="fe og oh oi oj b">n_clusters = 2</code>我们该怎么办？</p><p id="2be9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">方法一:</strong>尽量把数据可视化！在这种方法中，我们使用主成分分析(PCA ),以便我们可以绘制数据的三维图。因为这个主题是关于聚类的，所以我不会深究 PCA 的细节(如果你们想了解它，请在评论中告诉我！).</p><pre class="kj kk kl km gt ok oj ol om aw on bi"><span id="61fa" class="ng mk it oj b gy oo op l oq or">from sklearn import decomposition<br/>from mpl_toolkits.mplot3d import Axes3D</span><span id="4413" class="ng mk it oj b gy os op l oq or">x_full = iris.data</span><span id="7a25" class="ng mk it oj b gy os op l oq or">pca = decomposition.PCA()<br/>xr = pca.fit_transform(x_full)</span><span id="ccf4" class="ng mk it oj b gy os op l oq or">kmeans = cluster.KMeans(n_clusters=3, init='k-means++', random_state=random_state)<br/>kmeans.fit(xr)<br/>label = kmeans.labels_<br/>color = ['red', 'green', 'blue']</span><span id="0a4f" class="ng mk it oj b gy os op l oq or">plt.figure(figsize=(12, 12))<br/>fig = plt.subplot(1, 1, 1, projection='3d')<br/>fig.scatter(xr[:,0], xr[:,1], xr[:,2], c=[color[i] for i in label])<br/>fig.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='yellow', s=100)<br/>fig.set_xlabel('Principal Component 1')<br/>fig.set_ylabel('Principal Component 2')<br/>fig.set_zlabel('Principal Component 3')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/c26d65cf14ead3a9de1c21abad2fb5b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*OTix7ilvoygs1gNl_bJw_g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">虹膜数据集的三维图(PCA 后)</p></figure><p id="bbc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的三维图来看，似乎确实有三个不同的集群——尽管不能完全确定，因为集群间的距离并不高。</p><p id="7712" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">方法 2: </strong>使用描述性统计来理解聚类</p><p id="cc9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">描述性统计涉及使用平均值、最大值、最小值和中值等度量来找出聚类之间的差异。在无监督学习中，手头的问题没有“正确”的答案，大多数时候，找到最佳数量的聚类需要查看这些聚类的结果(即，比较这些不同聚类的描述性统计数据)。</p><p id="97ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(<em class="mf">然而，在这种情况下，如果我们想要“作弊”，我们确实知道在数据集中实际上有 3 种不同类型的虹膜变体(因为它被标记了！)</em></p><h2 id="c204" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">K 均值评估</h2><p id="29fb" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated"><em class="mf">优点</em>:</p><ul class=""><li id="501c" class="ns nt it lb b lc ld lf lg li ow lm ox lq oy lu oz ny nz oa bi translated">易于使用和理解</li><li id="f4e1" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu oz ny nz oa bi translated">适用于聚类之间的层次关系易于检测的数据集</li><li id="41e8" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu oz ny nz oa bi translated">相对较低的时间复杂度和较高的计算效率，从而带来较高的可扩展性</li></ul><p id="a529" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mf">缺点</em>:</p><ul class=""><li id="5b5d" class="ns nt it lb b lc ld lf lg li ow lm ox lq oy lu oz ny nz oa bi translated">不适合非凸数据</li><li id="c315" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu oz ny nz oa bi translated">对聚类质心的初始化高度敏感——值得调整超参数，如<code class="fe og oh oi oj b">n_init</code>和<code class="fe og oh oi oj b">init='random'</code>,以确保获得一致的聚类结果</li><li id="4a3b" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu oz ny nz oa bi translated">对异常值高度敏感——K-Means 中的<em class="mf">表示</em>表示异常值会显著扭曲聚类质心——其他使用众数或中位数的算法不太容易出现异常值</li><li id="5756" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu oz ny nz oa bi translated">容易陷入局部最优</li><li id="5751" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu oz ny nz oa bi translated">聚类结果对聚类数很敏感</li></ul><h2 id="5eb1" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">最后的想法和结束语</h2><p id="80e4" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">对于任何聚类问题，K-Means 都是一个很好的入门模型，因为它能够适应高维数据。但是，重要的是要知道正在使用哪种距离度量、模型对异常值的敏感度，以及最重要的是，使用领域知识来相应地调整模型。</p><p id="f221" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个 3 部分系列的第二部分中，我将探索一个不同的数据集，并使用<strong class="lb iu">凝聚聚类</strong>(层次聚类的两种变体之一)进行分析。我还将通过一些方法来获得模型所产生的集群的描述性统计数据。</p><p id="9e7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="mf">专业提示</em> </strong> <em class="mf">:解释生成的聚类与获取聚类本身同等重要(如果不是更重要的话)！</em></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="f89b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请继续关注这个聚类系列剩下的两个部分——<strong class="lb iu">凝聚聚类</strong>和<strong class="lb iu">高斯混合模型</strong>！</p></div></div>    
</body>
</html>