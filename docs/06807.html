<html>
<head>
<title>Deep learning on a combination of time series and tabular data.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">结合时间序列和表格数据的深度学习。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-on-a-combination-of-time-series-and-tabular-data-b8c062ff1907?source=collection_archive---------37-----------------------#2020-05-27">https://towardsdatascience.com/deep-learning-on-a-combination-of-time-series-and-tabular-data-b8c062ff1907?source=collection_archive---------37-----------------------#2020-05-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/2692525293614ea9f606d687bb96abf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aOJa-pr4tyGnRflsOGM4QA.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片来源:<a class="ae kf" href="https://unsplash.com/photos/dr_9oxbVnuc" rel="noopener ugc nofollow" target="_blank">取消快照</a></p></figure><p id="02e4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们经常发现自己处于这样一种情况，即我们想要在模型中利用不同特性的组合。模型的输入数据是时间序列和表格数据的混合。在这种情况下，设计一个深度学习模型是一个有趣的问题。</p><p id="bc69" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个示例场景是:您有来自 fitbit 等设备的数据，并且您想要预测任何给定时刻的睡眠阶段:</p><p id="efd7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你混合了:</p><p id="55ee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">时间序列输入:</p><ul class=""><li id="1b00" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">心率序列</li><li id="d83c" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">呼吸频率序列</li></ul><p id="01b2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">表格特征:</p><ul class=""><li id="2b60" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">睡眠开始后的时间</li><li id="d2ed" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">代表这种用户睡眠模式的个性化嵌入</li></ul><p id="2740" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">和许多其他功能。</p><p id="a489" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解决这个问题的一种方法是将其视为多模态深度学习。</p><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ls"><img src="../Images/0a99d1987647683f2708a767899f0656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQuKitv5YYrPkflxbHKb3g.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">多模态学习——在<a class="ae kf" href="https://www.researchgate.net/figure/Baseline-of-multimodal-deep-learning-model-It-deals-with-multisource-data-directly-and_fig1_334532323" rel="noopener ugc nofollow" target="_blank">研究之门</a>上的照片</p></figure><p id="2fb5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在谷歌研究<a class="ae kf" href="https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html" rel="noopener ugc nofollow" target="_blank">这里</a>引入了“广泛和深度学习”</p><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lx"><img src="../Images/c58e6d3e1c4abda1de730c0a5d734999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FKR1nrt6RxlYUcJnr5RBZg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">广泛而深入的学习——谷歌<a class="ae kf" href="https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html" rel="noopener ugc nofollow" target="_blank">博客</a>上的照片</p></figure><h2 id="6043" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">那么我们该如何着手呢？</h2><ol class=""><li id="ab52" class="le lf it ki b kj mr kn ms kr mt kv mu kz mv ld mw lk ll lm bi translated">通过 RNN 或 LSTM 或 1D CNN 传递时间序列序列，并捕获隐藏状态或 CNN 嵌入作为序列的表示。</li><li id="0dd8" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld mw lk ll lm bi translated">将每个序列的嵌入与其他表格特征连接起来。</li></ol><h2 id="61da" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">一些有趣的决定需要考虑:</h2><p id="28ff" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">对于多个时间序列输入序列:</p><ul class=""><li id="fd13" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">你是否将序列视为独立的，并在后期融合/连接表示(后期融合)？</li><li id="cb92" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">还是把它们当做多通道输入，每个时间序列作为一个通道(早期融合)？</li></ul><p id="6802" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一般来说，晚期融合似乎比早期融合工作得更好，并且当不同的输入序列长度不同时，也不需要填充输入。然而，它确实依赖于问题空间和输入序列的相关性。</p><p id="2ba2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一般来说，RNN 似乎更适合较短的序列，双向 LSTM 更适合较长的序列。在后期融合中，你可以混合搭配 RNN/LSTM/1d 有线电视新闻网的不同序列。</p><h2 id="ed4c" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">下面是一个示例模型实现(用 pytorch 编写):</h2><p id="d136" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">该实施例使用具有后期融合的 2 层比迪 LSTM。</p><p id="0670" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于每个时间序列特征(特征 1 和特征 2 ),我们也有基线值，因此我们有融合层，其中我们融合了序列的表示(LSTM 的隐藏状态)和基线值。然而，该融合层不是必需的，并且在没有基线值的情况下不是必需的。</p><pre class="lt lu lv lw gt na nb nc nd aw ne bi"><span id="b6be" class="ly lz it nb b gy nf ng l nh ni">class TestModel(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.input_size = 70<br/>        self.hidden_size = 8<br/>        self.num_layers = 2<br/>        self.output_dim = 2<br/>        self.lstm_feature_1 = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True,<br/>                                      bidirectional=True)<br/>        self.lstm_feature_2 = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True,<br/>                                      bidirectional=True)<br/>        self.fc_feature_1 = nn.Linear((self.hidden_size * 2) + 1, 1)<br/>        self.fc_feature_2 = nn.Linear((self.hidden_size * 2) + 1, 1)<br/><br/>        self.fc = nn.Linear(4, self.output_dim)<br/><br/>    def forward(self, f, device=None):<br/>        if not device:<br/>            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")<br/><br/>        x_f1, f1, x_f2, f2, f3, f4 = f<br/><br/>        # x_f1 is feature_1_seq<br/>        # f1 is feature_1_baseline<br/>        # x_f2 is feature_2_seq<br/>        # f2 is feature_2_baseline<br/><br/>        # f3 and f4 are tabular features<br/><br/>        x_f1 = x_f1.view(x_f1.shape[0], 1, -1)<br/>        h0_f1, c0_f1 = self.init_hidden(x_f1, device)<br/>        h_t_f1, c_t_f1 = self.lstm_feature_1(x_f1, (h0_f1, c0_f1))<br/>        x_f1 = h_t_f1<br/>        x_f1 = x_f1.view(x_f1.shape[0], -1)<br/><br/>        x_f2 = x_f2.view(x_f2.shape[0], 1, -1)<br/>        h0_f2, c0_f2 = self.init_hidden(x_f2, device)<br/>        h_t_f2, c_t_f2 = self.lstm_feature_2(x_f2, (h0_f2, c0_f2))<br/>        x_f2 = h_t_f2<br/>        x_f2 = x_f2.view(x_f2.shape[0], -1)<br/><br/>        x_f1 = torch.cat((x_f1, f1), 1)<br/>        x_f1 = self.fc_feature_1(x_f1)<br/><br/>        x_f2 = torch.cat((x_f2, f2), 1)<br/>        x_f2 = self.fc_feature_2(x_f2)<br/><br/>        x = torch.cat((x_f1, x_f2, f3, f4), 1)<br/>        x = self.fc(x)<br/>        x = F.log_softmax(x, dim=1)<br/>        return x<br/><br/>    def init_hidden(self, x, device):<br/>        batch_size = x.size(0)<br/>        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)<br/>        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(device)<br/>        return h0, c0</span></pre><p id="5dfd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你曾处理过一个涉及类似输入方式的问题:我很想听听你对你有用的东西或者你尝试过的其他方法。</p></div></div>    
</body>
</html>