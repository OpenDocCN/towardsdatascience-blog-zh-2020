<html>
<head>
<title>Custom Object Detection Using Keras and OpenCV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Keras 和 OpenCV 的自定义对象检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-weapon-detection-system-using-keras-and-opencv-67b19234e3dd?source=collection_archive---------4-----------------------#2020-09-09">https://towardsdatascience.com/how-to-build-a-weapon-detection-system-using-keras-and-opencv-67b19234e3dd?source=collection_archive---------4-----------------------#2020-09-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0ce5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">建立一个可以在给定的图像或画面中识别武器的系统</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d6ed2371c633413380d2cb601ec82fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HH0ViYXe9t_Z_TrGCDZErA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">武器探测系统(<a class="ae kv" href="https://www.pxfuel.com/en/free-photo-ovfsz" rel="noopener ugc nofollow" target="_blank">原图</a>)</p></figure><p id="e7b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我最近完成了一个令我非常自豪的项目，我想我应该分享它，以防其他人有兴趣实现类似于他们特定需求的东西。在我开始本教程之前，我想对<a class="ae kv" href="https://www.pyimagesearch.com/" rel="noopener ugc nofollow" target="_blank"> PyImageSearch </a>的创建者 Adrian Rosebrock 博士表示深深的感谢。我是一个自学成才的程序员，所以没有他的资源，这个项目的大部分是不可能的。他是一个典型的<em class="ls">男人- </em>我非常感激他在自己的网站上提供的资源。如果你想学习先进的深度学习技术，但发现教科书和研究论文很枯燥，我强烈建议访问上面链接的他的网站。</p><p id="bb67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在大多数与武器分类相关的项目中，我只能找到最多 100-200 张图片的数据集。这提出了一个问题，因为根据我的经验，很难用这么少的图像得到一个工作模型。为了收集图像，我用我的树莓皮刮了一下<a class="ae kv" href="http://www.imfdb.org/wiki/Main_Page" rel="noopener ugc nofollow" target="_blank">IMFDB.com</a>——一个枪支爱好者张贴照片的网站，照片中的模型枪出现在一部电影的画面或剪辑中。如果你访问网站，这一点会更清楚。要访问我用过的图片，你可以访问我的<a class="ae kv" href="https://drive.google.com/file/d/1EZZKhCk0DK3S9zB53o3nWhKrZUbmN2Up/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Drive </a>。在这个 zip 文件中，您将找到所有在这个项目中使用的图像和相应的。边界框的 xml 文件。如果你需要一个大数据集的边界框，我强烈推荐<a class="ae kv" href="https://scaleops.ai/" rel="noopener ugc nofollow" target="_blank"> ScaleOps。AI </a>，一家专注于机器学习算法数据标注的公司。目前，我有来自 IMFDB 网站的 120，000 张图片，但是由于时间和资金的限制，我只使用了大约 5000 张。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="cbf9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们来看看逻辑。这个项目的架构遵循<a class="ae kv" href="https://www.pyimagesearch.com/2020/07/13/r-cnn-object-detection-with-keras-tensorflow-and-deep-learning/" rel="noopener ugc nofollow" target="_blank">这个</a>网站上显示的逻辑。虽然我们在这里实现了这个逻辑，但是在许多领域它是不同的，因此它可以用于我们的特定问题——检测武器。该项目使用 6 个基本步骤:</p><ol class=""><li id="f7dc" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">使用 OpenCV 选择性搜索分割构建数据集</li><li id="619f" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">建立一个 CNN 来检测你想要分类的物体(在我们的例子中，0 =没有武器，1 =手枪，2 =步枪)</li><li id="85fa" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">在根据选择性搜索分割构建的图像上训练模型</li><li id="69a4" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">为新图像创建边界框时，通过选择性搜索分割运行图像，然后抓取图片的每一部分。</li><li id="0591" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">通过算法运行图像的每一部分，每当算法预测到你要寻找的对象时，用边界框标记位置</li><li id="ca23" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">如果标记了多个边界框，应用非最大值抑制，只包括具有高置信度/感兴趣区域的框(这部分我还在弄清楚…你将在下面看到我的问题)</li></ol><p id="8620" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是展示算法如何工作的 gif。对于给定的图像，每个方块将被输入到神经网络中。如果一个正方形被预测为阳性(手枪或步枪)，我们将标记我们输入到原始图像的区域。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/daff86c61340da4ecff35a0c6971a067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*hBT9swO3CzxRezROo717eg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">滑动窗口方法:对象检测(作者图片)</p></figure><p id="ba30" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想看这个项目的完整代码，请访问我的 GitHub Repo，在那里我会更深入地解释这些步骤。</p><p id="9238" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我上面链接的数据包含了很多文件夹，我需要解释一下，以便理解正在发生的事情。解压文件夹后，这些文件和文件夹对项目很重要:AR、FinalImages、Labels、Pistol、Stock_AR、Stock_Pistol 和 PATHS.csv。所以对于 AR 文件夹，你会发现里面有突击步枪的图片。在标签文件夹中，您会看到。类文件夹中所有图像的 xml 标签。最后，PATHS.csv 将指向算法中使用的每一张图像。出于本教程的目的，这些是您唯一需要担心的文件夹/文件:</p><ol class=""><li id="c397" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">最终图像/NoWeapon</li><li id="4f34" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">最终图像/手枪</li><li id="3f1f" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">最终图像/步枪</li></ol><p id="08aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些文件夹中的图像制作方式如下。</p><ul class=""><li id="8455" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mp mg mh mi bi translated">对于每个有边界框的图像，提取边界框并将其放入相应的类文件夹中。因此，对于一个人拿着手枪的图像，手枪周围的边界框将变成正的，而边界框以外的每个部分都将变成负的(没有武器)</li><li id="67ee" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mp mg mh mi bi translated">在下图中，想象一个包围左侧图像的边界框。在提取边界框内的像素(右边的图像)后，我们将该图像放入另一个文件夹(FinalImages/Pistol)，同时将边界框周围的所有空白区域放入 NoWeapons 文件夹。</li><li id="3e0a" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mp mg mh mi bi translated">虽然右边的图像看起来像是左边图像的调整版本，但它实际上是一个分段图像。想象在左边的枪周围有一个边界框。右边的图像是<em class="ls">仅仅是</em>边界框，没有别的(移除框外的所有东西)。这种技术被称为感兴趣区域(ROI)。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/0627843f536a1e6d9eb16862f872287f.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*9PK49fAlIOhYNuioqNmgdA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ROI 提取(图片由作者提供)</p></figure><p id="67cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在收集数据集(可以在 Separated/FinalImages 中找到)后，我们需要为我们的算法使用这些文件，我们需要以这样的方式准备它，我们有一个 RGB 值列表和相应的标签(0=没有武器，1 =手枪，2 =步枪)</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="ad15" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您运行上面的代码，在当前目录的之外有一个单独的文件夹<em class="ls">，您会看到一个 tqdm 窗口，显示它正在加载图像。该过程完成后，您应该会看到以下内容:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/2fc90d073971beebe825061d53afacbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*loWCSzU7MA9mjX8CvZJXmA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="884f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在是神经网络的时候了。在下面的代码中，该函数将返回一个给定维度大小的模型。如果您在上面的代码中注意到，照片的尺寸被调整为(150，150，3)。如果您希望使用不同的尺寸，请确保您更改了上面的变量 dim，以及下面函数中的 DIM</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="53c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面返回的模型将具有如下所示的体系结构:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/b901a3b77b56612ca4c0416e4d1f3dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*uoVfR0lS3j5lCaIPwSCnzA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CNN 架构(图片由作者提供)</p></figure><p id="cf7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们有了训练集和测试集，我们需要做的就是把它放到我们的模型中。运行下面的代码将开始训练过程。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="2d6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果运行代码时没有任何错误，您应该会看到如下窗口:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/6cc8f837010c89616e2c684de6b479ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I61KTjqRcojL2XCWWX8dXg.png"/></div></div></figure><p id="19c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想指出的是，我将历元设置为 1000，但提前停止将防止算法过度拟合，因此它不应运行超过 30–50 个历元。模型完成后，您应该会在您的目录中看到一个名为 ModelWeights.h5 的. h5 文件。该文件是模型生成的权重，因此将它们加载到模型中会在模型开始溢出之前加载模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/e6c57cedbec073a6d9762fd7a0a79615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*pMlOayRDUdax2jxGRXrhyQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型的准确性(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/e14e470d67271529b56c9c1708557c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*p20L6OnePc2ipnqs_axc5g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每个班级的 ROC(图片由作者提供)</p></figure><p id="f813" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑到一个平衡的数据集，这个精确度是相当不错的。查看 ROC 曲线，我们还可以假设每个类别下的面积非常接近 1，这是一个非常好的分类。</p><p id="a34b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在是物体探测的时间了！以下逻辑用于创建边界框:</p><ol class=""><li id="4cfe" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">输入视频中的图像或帧并检索基本预测</li><li id="deee" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">应用选择性搜索分割来创建数百或数千个包围盒命题</li><li id="0055" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">通过训练的算法运行每个边界框，并检索预测与基本预测相同的位置(在步骤 1 中)</li><li id="73de" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">检索到算法预测与基本预测相同的位置后，在算法运行的位置上标记一个边界框</li><li id="ed75" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">如果选择了多个边界框，则应用非最大值抑制来抑制除一个框之外的所有框，留下具有最高概率和最佳感兴趣区域(ROI)的框</li></ol><ul class=""><li id="6305" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mp mg mh mi bi translated">注意:非最大值抑制仍在进行中。在某些情况下，它只能检测枪的特征，而不是整个枪本身(见下面的模型比较)。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="3945" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在运行上面的代码之前，创建一个文件夹 Tests，从互联网上下载任何图像，并将该图像命名为您想要预测的类。运行上面的代码将搜索 Tests 文件夹中的每张图片，并使用上面构建的 CNN 通过我们的对象检测算法运行该图片。</p><p id="0e70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我测试的图像如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/d850da695de8ec8028b5c2ce2a554c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aOhSoF1MobbEttshp2cewg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基础图像</p></figure><p id="4b5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">运行上面的代码后，这些是算法输出的预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/1009660bddd33d96feb677af24560c38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DTD3qEB010zXk8uaLFGF6w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型结果(图片由作者提供)[点击<a class="ae kv" href="https://github.com/HeeebsInc/NN_Weapon_Detection/blob/master/Figures/ModelComparisons/Normal/Normal.png" rel="noopener ugc nofollow" target="_blank">此处</a>查看大图]</p></figure><p id="4ceb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如您在上面看到的，非极大值抑制并不完美，但在某种意义上确实有效。我这里的问题是，有多个具有 100%置信度的边界框，所以很难选择哪一个是最好的<em class="ls">。</em>此外，当帧中没有武器时(绵羊图像)，该算法无法检测非武器。</p><p id="a80a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用上面实现的逻辑，<a class="ae kv" href="https://www.youtube.com/watch?v=e6tFxT-BpFg&amp;t=1s&amp;ab_channel=SamuelMohebban" rel="noopener ugc nofollow" target="_blank">这里的</a>是我将代码应用到视频的一个很酷的视觉效果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na ms l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">演示武器检测(视频由作者提供)</p></figure><p id="a044" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于上面的例子，我们看到这个算法离完美还有一段距离。这没什么，因为我们仍然创建了一个非常酷的模型，只用了 5000 张图片。就像我之前说的，我从 IMFDB.com 收集了总共 120，000 张图片，所以随着我们在培训期间传递更多的图片，这只会变得更好。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="734e" class="nb nc iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">石灰:特征提取</h2><p id="8883" class="pw-post-body-paragraph kw kx iq ky b kz nu jr lb lc nv ju le lf nw lh li lj nx ll lm ln ny lp lq lr ij bi translated">构建和测试神经网络的一个困难部分是，它的工作方式基本上是一个黑盒，这意味着你不明白为什么权重是它们的样子，或者算法在图像中使用什么来进行预测。使用<a class="ae kv" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank"> LIME </a>，我们可以更好地理解我们的算法是如何执行的，以及图片中的哪些内容对于预测是重要的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">石灰预测</p></figure><p id="2012" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">运行上面的代码将创建一个如下所示的图像:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/5b333506989220f1b119dadbb4dfb8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z8VNlmeQFRMhDB-AMSYqRw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">石灰(图片由作者提供)</p></figure><p id="82aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">绿色区域是算法认为“重要”的区域，而红色区域则相反。考虑到我们希望算法检测枪的特征，而不是手或图像的其他部分，我们在上面看到的是好的。</p><p id="764b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们可以说我们创造了我们自己的有意识的生物…是时候面对现实了。与现有的工具相比，我们制作的模型微不足道。这使我转而学习…我们看到了一些很酷的结果。为了这个教程，我不会把代码放在这里，但是你可以在我的<a class="ae kv" href="https://github.com/HeeebsInc/NN_Weapon_Detection" rel="noopener ugc nofollow" target="_blank"> GitHub Repo </a>上找到它</p><h2 id="6abc" class="nb nc iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">Mobilenet</h2><ul class=""><li id="3db8" class="ma mb iq ky b kz nu lc nv lf oa lj ob ln oc lr mp mg mh mi bi translated">在下面的例子中，mobilenet 更擅长预测不是武器的物体，并且在正确的区域周围有边界框。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/97224139d9c2d2816e0f3fc8b53f7d6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hNc1zbmj7U22NuTInCuCrg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Mobilenet(图片由作者提供)[点击<a class="ae kv" href="https://github.com/HeeebsInc/NN_Weapon_Detection/blob/master/Figures/ModelComparisons/Mobilenet/Mobilenet.png" rel="noopener ugc nofollow" target="_blank">此处</a>查看大图]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/0072bd665c380d9958e1e7123497e74a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-UoAFOsj28fAuzJJf44Zng.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Mobilenet LIME(图片由作者提供)</p></figure><h2 id="f7ca" class="nb nc iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">VGG16</h2><ul class=""><li id="2d84" class="ma mb iq ky b kz nu lc nv lf oa lj ob ln oc lr mp mg mh mi bi translated">在下面的例子中，VGG16 无法像我们自己构建的架构那样区分非武器。它错误地将 3 张手枪图像中的 1 张归类，而将其余的正确归类为手枪</li><li id="7074" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mp mg mh mi bi translated">尽管它错误地将手枪归类为无武器(右边第四个)，但边界框并不在枪上，因为它停留在握枪的手上。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/7f8102b3a27d36cc5b9d3ef07a8c9cfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aExnnNRQFDAYr47AGQpgyw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">VGG16(图片由作者提供)[点击<a class="ae kv" href="https://github.com/HeeebsInc/NN_Weapon_Detection/blob/master/Figures/ModelComparisons/VGG16/VGG16.png" rel="noopener ugc nofollow" target="_blank">此处</a>查看大图]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/616f86c657dd9a606c6f88d7512c1e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3tNKHkaZbVlPc59YKbjCPg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">VGG16:石灰(图片由作者提供)</p></figure><h2 id="1a12" class="nb nc iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">结论</h2><ul class=""><li id="0e58" class="ma mb iq ky b kz nu lc nv lf oa lj ob ln oc lr mp mg mh mi bi translated">这个项目的目标是创建一种算法，它可以将自己集成到传统的监控系统中，并比人更快地防止糟糕的情况(考虑到当今社会的不幸情况)。</li><li id="ce21" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mp mg mh mi bi translated">虽然这很酷，但我的电脑中的硬件还没有。分割图像并处理图像的每个部分大约需要 10-45 秒，这对于直播视频来说太慢了。</li><li id="984f" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mp mg mh mi bi translated">我上面展示的视频演示是一个 30 秒的剪辑，大约需要 20 分钟来处理。</li><li id="f229" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mp mg mh mi bi translated">然而，尽管使用 RX 580 进行视频直播是不可行的，但使用新的 Nvidia GPU (3000 系列)可能会有更好的效果。</li><li id="a828" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mp mg mh mi bi translated">此外，这种技术可用于追溯检查事件，如人体摄像机镜头或抗议。</li></ul><p id="37fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">**注意**如果您想了解整个项目，请访问我的<a class="ae kv" href="https://github.com/HeeebsInc/WeaponDetection" rel="noopener ugc nofollow" target="_blank"> GitHub </a> **</p></div></div>    
</body>
</html>