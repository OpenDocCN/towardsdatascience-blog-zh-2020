<html>
<head>
<title>How to handle Multiclass Imbalanced Data?- Say No To SMOTE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何处理多类不平衡数据？-对 SMOTE 说不</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-handle-multiclass-imbalanced-data-say-no-to-smote-e9a7f393c310?source=collection_archive---------8-----------------------#2020-08-31">https://towardsdatascience.com/how-to-handle-multiclass-imbalanced-data-say-no-to-smote-e9a7f393c310?source=collection_archive---------8-----------------------#2020-08-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dd4a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">不需要再打了。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/59f938f0417ae46b7aac07816a74c8f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*oDmeGslvHMUtxmVMvdOGCg.png"/></div></figure><p id="043a" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">机器学习中的一个常见问题是处理不平衡数据，其中目标类中存在高度不相称的数据。</p><p id="3bef" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">你好，世界，这是我为数据科学社区写的第二篇博客。在这篇博客中，我们将看到如何处理多类不平衡数据问题。</p><h1 id="5b97" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">什么是多类不平衡数据？</h1><p id="f417" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">当分类问题的目标类(两个或两个以上)不是均匀分布时，那么我们称之为不平衡数据。如果我们不能处理这个问题，那么这个模型将会变成一场灾难，因为使用阶级不平衡数据的模型偏向于大多数阶级。</p><p id="a4da" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">处理不平衡数据有不同的方法，最常用的方法是过采样和创建合成样本。</p><h1 id="64de" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">什么是 SMOTE？</h1><p id="db87" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">SMOTE 是一种过采样技术，它从数据集生成合成样本，从而提高少数类的预测能力。尽管没有信息丢失，但它有一些限制。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/51eeca46f25ea31f65f533130b4ee3b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1i7SEN4dg9dkjYoaci6QBg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">合成样品</p></figure><p id="9279" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> <em class="mq">局限性:</em> </strong></p><ol class=""><li id="87e3" class="mr ms iq kp b kq kr kt ku kw mt la mu le mv li mw mx my mz bi translated">SMOTE 对于高维数据不是很好</li><li id="3c63" class="mr ms iq kp b kq na kt nb kw nc la nd le ne li mw mx my mz bi translated">可能会发生类的重叠，这会给数据带来更多的噪声。</li></ol><p id="640b" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">因此，为了跳过这个问题，我们可以用'<strong class="kp ir"> class_weight </strong>'参数为类手动分配权重。</p><h1 id="883a" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">为什么使用类权重？</h1><p id="9c82" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">类权重通过给具有不同权重的类一个惩罚来直接修改损失函数。它意味着有目的地增加少数阶级的权力，减少多数阶级的权力。因此，它比 SMOTE 给出更好的结果。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="8942" class="lk ll iq bd lm ln nm lp lq lr nn lt lu jw no jx lw jz np ka ly kc nq kd ma mb bi translated">概述:</h1><p id="2da3" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">我的目标是让这个博客非常简单。我们有一些最常用的技术来获得数据的权重，这些技术对我不平衡的学习问题有效。</p><ol class=""><li id="99f6" class="mr ms iq kp b kq kr kt ku kw mt la mu le mv li mw mx my mz bi translated">Sklearn utils。</li><li id="61a4" class="mr ms iq kp b kq na kt nb kw nc la nd le ne li mw mx my mz bi translated">计数到长度。</li><li id="36b7" class="mr ms iq kp b kq na kt nb kw nc la nd le ne li mw mx my mz bi translated">平滑重量。</li><li id="11ef" class="mr ms iq kp b kq na kt nb kw nc la nd le ne li mw mx my mz bi translated">样本权重策略。</li></ol><h1 id="f9a2" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">1.Sklearn 实用程序:</h1><p id="1a9d" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">我们可以使用 sklearn 计算类权重来获得类权重。通过在训练模型时将那些权重添加到少数类，可以在分类类时帮助性能。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="d990" class="nw ll iq ns b gy nx ny l nz oa">from sklearn.utils import class_weight</span><span id="f91a" class="nw ll iq ns b gy ob ny l nz oa">class_weight = class_weight.compute_class_weight('balanced,<br/>                                                np.unique(target_Y),<br/>                                                target_Y)</span><span id="6e25" class="nw ll iq ns b gy ob ny l nz oa">model = LogisticRegression(class_weight = class_weight)<br/>model.fit(X,target_Y)</span><span id="cb79" class="nw ll iq ns b gy ob ny l nz oa"># ['balanced', 'calculated balanced', 'normalized'] are hyperpaameters whic we can play with.</span></pre><p id="abd1" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">从逻辑回归到 Catboost，几乎所有的分类算法都有一个 class_weight 参数。但是 XGboost 有针对二元分类的 scale_pos_weight 和针对二元和多类问题的 sample_weights(参考 4)。</p><h1 id="e676" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">2.计数与长度之比:</h1><p id="5503" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">非常简单直白！将每类的计数除以行数。然后</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="611e" class="nw ll iq ns b gy nx ny l nz oa">weights = df[target_Y].value_counts()/len(df)<br/>model = LGBMClassifier(class_weight = weights)<br/>model.fit(X,target_Y)</span></pre><h1 id="739b" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">3.平滑权重技术:</h1><p id="899f" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">这是选择权重的优选方法之一。</p><p id="aaac" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">labels_dict 是包含每个类的计数的字典对象。</p><p id="0d2b" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">log 函数平滑不平衡类的权重。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="943a" class="nw ll iq ns b gy nx ny l nz oa">def class_weight(labels_dict,mu=0.15):<br/>    total = np.sum(labels_dict.values())<br/>    keys = labels_dict.keys()<br/>    weight = dict()</span><span id="fbb2" class="nw ll iq ns b gy ob ny l nz oa">for i in keys:<br/>        score = np.log(mu*total/float(labels_dict[i]))<br/>        weight[i] = score if score &gt; 1 else 1</span><span id="3b3d" class="nw ll iq ns b gy ob ny l nz oa">return weight</span><span id="2712" class="nw ll iq ns b gy ob ny l nz oa"># random labels_dict<br/>labels_dict = <!-- -->df[target_Y].value_counts().to_dict()</span><span id="7473" class="nw ll iq ns b gy ob ny l nz oa">weights = class_weight(labels_dict)</span><span id="7b93" class="nw ll iq ns b gy ob ny l nz oa">model = RandomForestClassifier(class_weight = weights)<br/>model.fit(X,target_Y)</span></pre><h1 id="27fc" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">4.样品重量策略:</h1><p id="aedc" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">以下函数不同于 class_weight 参数，该参数用于获取 XGboost 算法的样本权重。它为每个训练样本返回不同的权重。</p><blockquote class="oc od oe"><p id="eadd" class="kn ko mq kp b kq kr jr ks kt ku ju kv of kx ky kz og lb lc ld oh lf lg lh li ij bi translated">Sample_weight 是一个与数据长度相同的数组，包含应用于每个样本的模型损失的权重。</p></blockquote><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="0ecb" class="nw ll iq ns b gy nx ny l nz oa">def BalancedSampleWeights(y_train,class_weight_coef):<br/>    classes = np.unique(y_train, axis = 0)<br/>    classes.sort()<br/>    class_samples = np.bincount(y_train)<br/>    total_samples = class_samples.sum()<br/>    n_classes = len(class_samples)<br/>    weights = total_samples / (n_classes * class_samples * 1.0)<br/>    class_weight_dict = {key : value for (key, value) in              zip(classes, weights)}<br/>    class_weight_dict[classes[1]] = class_weight_dict[classes[1]] * <br/>    class_weight_coef<br/>    sample_weights = [class_weight_dict[i] for i in y_train]<br/>    return sample_weights</span><span id="5f8f" class="nw ll iq ns b gy ob ny l nz oa">#Usage<br/>weight=BalancedSampleWeights(<!-- -->target_Y<!-- -->,class_weight_coef)<br/>model = XGBClassifier(sample_weight = weight)<br/>model.fit(X, <!-- -->target_Y<!-- -->)</span></pre><p id="487a" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> <em class="mq">类 _ 权重 vs 样本 _ 权重:</em> </strong></p><p id="4901" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe oi oj ok ns b">sample_weights</code>用于给出每个训练样本的权重。这意味着您应该传递一个一维数组，其中的元素数量与您的训练样本完全相同。</p><p id="4746" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe oi oj ok ns b">class_weights</code>用于为每个目标类赋予权重。这意味着你应该为你要分类的每个类传递一个权重。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="08a0" class="lk ll iq bd lm ln nm lp lq lr nn lt lu jw no jx lw jz np ka ly kc nq kd ma mb bi translated">结论:</h1><p id="2ac3" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">以上是为你的分类器寻找类权重和样本权重的几种方法。我提到了几乎所有对我的项目有效的技术。</p><p id="4ab0" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我请求读者尝试一下这些可以帮助你的技术，如果不是把它当作学习的话😄下次可能会对你有帮助😜</p><p id="4d7b" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">通过 LinkedIn 联系我😍</p></div></div>    
</body>
</html>