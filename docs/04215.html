<html>
<head>
<title>Using Deep Learning for End to End Multiclass Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习进行端到端多类文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-deep-learning-for-end-to-end-multiclass-text-classification-39b46aecac81?source=collection_archive---------4-----------------------#2020-04-17">https://towardsdatascience.com/using-deep-learning-for-end-to-end-multiclass-text-classification-39b46aecac81?source=collection_archive---------4-----------------------#2020-04-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d5237834105a1f389b35e827fdc31b1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4LRWXnee4Chnu__u59KWSQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由<a class="ae jg" href="https://pixabay.com/users/Anrita1705-11109462/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4980360" rel="noopener ugc nofollow" target="_blank"> Anrita1705 </a>来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4980360" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><div class=""/><p id="2b44" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你有没有想过有毒评论是如何在 Quora 或 Reddit 这样的平台上被自动标记的？或者邮件如何被标记为垃圾邮件？或者是什么决定了向你展示哪些在线广告？</p><p id="ca77" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以上都是文本分类在不同领域应用的例子。<a class="ae jg" href="https://lionbridge.ai/services/text-classification/" rel="noopener ugc nofollow" target="_blank">文本分类</a>是自然语言处理(NLP)中的一项常见任务，它将长度不定的文本序列转换成单一类别。</p><p id="b3a7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的例子中出现的一个主题是所有的都有一个二进制的目标类。比如要么评论有毒没毒，要么评论假没假。简而言之，只有两个目标类，因此称为二进制。</p><p id="a8e7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但情况并非总是如此，有些问题可能有两个以上的目标类。这些问题被方便地称为多类分类，这就是我们在这篇文章中要关注的问题。多类分类的一些例子包括:</p><ul class=""><li id="a943" class="le lf jj ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">评论的情绪:积极、消极或中立(三类)</li><li id="beb3" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">新闻按类型分类:娱乐、教育、政治等。</li></ul><p id="d3c3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另外，如果你对在工作中使用的定制深度学习工作站或服务器感兴趣，Exxact Corporation 有一系列基于人工智能的解决方案，起价为 3700 美元，带有几个英伟达 RTX 30 系列 GPU，3 年保修和深度学习软件堆栈。</p><p id="29b6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"> <em class="ls">在本帖中，我们将使用各种深度学习方法来解决一个多类文本分类问题。</em>T11】</strong></p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="7762" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">数据集/问题描述</h1><p id="5768" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">在这篇文章中，我使用了来自 Kaggle 的<a class="ae jg" href="https://www.kaggle.com/jessicali9530/kuc-hackathon-winter-2018" rel="noopener ugc nofollow" target="_blank"> UCI ML 药物评论数据集</a>。它包含超过 200，000 个患者药物评论，以及相关条件。数据集有许多列，但是我们将只使用其中的两列来完成 NLP 任务。</p><p id="7c76" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，我们的数据集看起来像这样:</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nd"><img src="../Images/0ea03516592518f9baf71af6bb2403a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GVtRnYftKqmkLYe1.png"/></div></div></figure><p id="0e32" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">任务:我们希望根据药物综述对主要疾病进行分类。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/a4cde7c446de65f50576c1a939fdc7ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wHgDSE-03tRkdoFh.png"/></div></div></figure></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="06a8" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">word2vec 嵌入入门:</h1><p id="e71f" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">在我们进一步研究文本分类之前，我们需要一种在词汇表中用数字表示单词的方法。为什么？因为我们大部分的 ML 模型都需要数字，而不是文本。</p><p id="ffaf" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实现这个目标的一种方法是使用单词向量的一次性编码，但这不是正确的选择。给定大量的词汇，这种表示将占用大量的空间，并且它不能准确地表达不同单词之间的相似性，例如如果我们想要找到数字单词 x 和 y 之间的余弦相似性:</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/ac776b68374f402f60be1b2e6062110c.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/0*0TEItxNIsb9Lqfe5.png"/></div></figure><p id="2a98" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定独热编码向量的结构，不同单词之间的相似度总是为 0。</p><p id="3d4e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Word2Vec 通过为我们提供一个固定长度(通常比词汇表大小小得多)的单词向量表示来克服上述困难。它还捕捉不同单词之间的相似性和类比关系。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/3eb8ad893dca9702659f1e11ae5de156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CoDZqHgJEaRPajGj.png"/></div></div></figure><p id="5c05" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词向量的学习方式允许我们学习不同的类比。这使我们能够对单词进行代数运算，这在以前是不可能的。</p><p id="c067" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">比如:什么是王者——男人+女人？结果是皇后。</p><p id="d889" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Word2Vec 向量也帮助我们找到单词之间的相似性。如果我们寻找与“好”相似的词，我们会发现很棒，很棒，等等。word2vec 的这一特性使得它在文本分类中具有不可估量的价值。有了这个，我们的深度学习网络就明白了“好”和“伟大”是意思相近的词。</p><p id="73da" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简单地说，word2vec 为单词创建了固定长度的向量，为字典中的每个单词(和常见的二元模型)提供了一个 d 维向量。</p><p id="9a46" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些词向量通常是预先训练好的，并由其他人在像维基百科、推特等大型文本语料库上训练后提供。最常用的预训练词向量有<a class="ae jg" href="https://www.kaggle.com/takuok/glove840b300dtxt" rel="noopener ugc nofollow" target="_blank">手套</a>和 300 维词向量的快速文本。在这篇文章中，我们将使用手套词向量。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="3cb4" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">数据预处理</h1><p id="38b3" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">在大多数情况下，文本数据并不完全干净。来自不同来源的数据具有不同的特征，这使得文本预处理成为分类管道中最关键的步骤之一。例如，来自 Twitter 的文本数据不同于在 Quora 或其他新闻/博客平台上找到的文本数据，每种数据都需要区别对待。然而，我们将在这篇文章中讨论的技术对于你在 NLP 的丛林中可能遇到的几乎任何类型的数据都足够通用。</p><h2 id="3efe" class="nl mb jj bd mc nm nn dn mg no np dp mk kr nq nr mo kv ns nt ms kz nu nv mw nw bi translated">a)清除特殊字符并删除标点符号</h2><p id="4b31" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">我们的预处理管道很大程度上依赖于我们将用于分类任务的 word2vec 嵌入。原则上，我们的预处理应该匹配在训练单词嵌入之前使用的预处理。由于大多数嵌入没有为标点符号和其他特殊字符提供向量值，所以我们要做的第一件事就是去掉文本数据中的特殊字符。</p><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="9dfb" class="nl mb jj ny b gy oc od l oe of"># Some preprocesssing that will be common to all the text classification methods you will see.</span><span id="b5cb" class="nl mb jj ny b gy og od l oe of">import re<br/><br/>def clean_text(x):<br/>    pattern = r'[^a-zA-z0-9\s]'<br/>    text = re.sub(pattern, '', x)<br/>    return x</span></pre><h2 id="735a" class="nl mb jj bd mc nm nn dn mg no np dp mk kr nq nr mo kv ns nt ms kz nu nv mw nw bi translated">b)清洁编号</h2><p id="d372" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">为什么我们要用#s 代替数字？因为包括 Glove 在内的大多数嵌入都以这种方式对其文本进行了预处理。</p><p id="64ed" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"> <em class="ls">小 Python 绝招:</em> </strong>我们在下面的代码中使用 if 语句来预先检查文本中是否存在数字，因为<code class="fe oh oi oj ny b">if</code>总是比<code class="fe oh oi oj ny b">re.sub</code>命令快，而且我们的大部分文本都不包含数字。</p><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="2ff7" class="nl mb jj ny b gy oc od l oe of">def clean_numbers(x):<br/>    if bool(re.search(r'\d', x)):<br/>        x = re.sub('[0-9]{5,}', '#####', x)<br/>        x = re.sub('[0-9]<strong class="ny jk">{4}</strong>', '####', x)<br/>        x = re.sub('[0-9]<strong class="ny jk">{3}</strong>', '###', x)<br/>        x = re.sub('[0-9]<strong class="ny jk">{2}</strong>', '##', x)<br/>    return x</span></pre><h2 id="8b7c" class="nl mb jj bd mc nm nn dn mg no np dp mk kr nq nr mo kv ns nt ms kz nu nv mw nw bi translated">c)消除收缩</h2><p id="e1ed" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">缩写是我们用撇号写的单词。缩写的例子是像“不是”或“不是”这样的词。因为我们想标准化我们的文本，所以扩展这些缩写是有意义的。下面我们使用收缩映射和正则表达式函数来完成。</p><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="3c8d" class="nl mb jj ny b gy oc od l oe of">contraction_dict = {"ain't": "is not", "aren't": "are not","can't": "cannot", "'cause": "because", "could've": "could have"}</span><span id="c8ee" class="nl mb jj ny b gy og od l oe of">def _get_contractions(contraction_dict):<br/>    contraction_re = re.compile('(<strong class="ny jk">%s</strong>)' % '|'.join(contraction_dict.keys()))<br/>    return contraction_dict, contraction_re<br/>contractions, contractions_re = _get_contractions(contraction_dict)</span><span id="bfb8" class="nl mb jj ny b gy og od l oe of">def replace_contractions(text):<br/>    def replace(match):<br/>        return contractions[match.group(0)]<br/>    return contractions_re.sub(replace, text)</span><span id="d7b4" class="nl mb jj ny b gy og od l oe of"><em class="ls"># Usage</em><br/>replace_contractions("this's a text with contraction")</span></pre><p id="c782" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除了以上技巧，你可能还想做拼写纠正。但是因为我们的帖子已经很长了，我们现在就离开它。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="3b6c" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">数据表示:序列创建</h1><p id="9cf4" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">使深度学习成为 NLP 的首选的一件事是，我们不必从我们的文本数据中手工设计特征；深度学习算法将一系列文本作为输入，像人类一样学习其结构。由于机器不能理解文字，它们希望数据是数字形式的。因此，我们需要将文本数据表示为一系列数字。</p><p id="167b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了理解这是如何做到的，我们需要了解一些关于 Keras Tokenizer 函数的知识。其他记号赋予器也是可行的，但是 Keras 记号赋予器对我来说是个不错的选择。</p><h2 id="964d" class="nl mb jj bd mc nm nn dn mg no np dp mk kr nq nr mo kv ns nt ms kz nu nv mw nw bi translated">a)记号赋予器</h2><p id="5b6d" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">简而言之，记号赋予器是一个将句子拆分成单词的实用函数。<code class="fe oh oi oj ny b">keras.preprocessing.text.Tokenizer</code>将文本标记(拆分)成标记(单词)，同时只保留文本语料库中出现次数最多的单词。</p><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="ecca" class="nl mb jj ny b gy oc od l oe of">#Signature:<br/>Tokenizer(num_words=None, filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n',<br/>lower=True, split=' ', char_level=False, oov_token=None, document_count=0, **kwargs)</span></pre><p id="0f3c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">num_words 参数只保留文本中预先指定数量的单词。这是有帮助的，因为我们不希望我们的模型因为考虑不经常出现的单词而受到很多干扰。在真实世界的数据中，我们使用 num_words 参数留下的大多数单词通常都是拼写错误的单词。缺省情况下，记号赋予器还会过滤掉一些不想要的记号，并将文本转换成小写。</p><p id="2e0d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦适合数据，tokenizer 还保留一个单词索引(一个我们可以用来为单词分配唯一编号的字典)，可以通过 tokenizer.word_index 访问。索引词典中的单词按出现频率排列。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ca"><img src="../Images/042c8ded9081e071a843413b4c1efa38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Goa-wyaLPtxoEiRg.png"/></div></div></figure><p id="89ad" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，使用记号赋予器的全部代码如下:</p><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="4d3f" class="nl mb jj ny b gy oc od l oe of">from keras.preprocessing.text import Tokenizer</span><span id="ff2c" class="nl mb jj ny b gy og od l oe of">## Tokenize the sentences<br/>tokenizer = Tokenizer(num_words=max_features)<br/>tokenizer.fit_on_texts(list(train_X)+list(test_X))<br/>train_X = tokenizer.texts_to_sequences(train_X)<br/>test_X = tokenizer.texts_to_sequences(test_X)</span></pre><p id="9278" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe oh oi oj ny b">train_X</code>和<code class="fe oh oi oj ny b">test_X</code>是语料库中的文档列表。</p><h2 id="d0fc" class="nl mb jj bd mc nm nn dn mg no np dp mk kr nq nr mo kv ns nt ms kz nu nv mw nw bi translated">b)填充序列</h2><p id="f0e6" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">通常，我们的模型期望每个文本序列(每个训练示例)具有相同的长度(相同数量的单词/标记)。我们可以使用<code class="fe oh oi oj ny b">maxlen</code>参数对此进行控制。</p><p id="4f2d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如:</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f2b880c447735bf9d215e4a48d9dabb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/0*_7YICpoXiNeVKMBW.png"/></div></figure><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="8b62" class="nl mb jj ny b gy oc od l oe of">train_X = pad_sequences(train_X, maxlen=maxlen)<br/>test_X = pad_sequences(test_X, maxlen=maxlen)</span></pre><p id="cc70" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们的训练数据包含了一个数字列表。每个列表都有相同的长度。我们还有<code class="fe oh oi oj ny b">word_index</code>，它是文本语料库中出现最多的单词的字典。</p><h2 id="a7da" class="nl mb jj bd mc nm nn dn mg no np dp mk kr nq nr mo kv ns nt ms kz nu nv mw nw bi translated">c)编码目标变量的标签</h2><p id="eabb" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">Pytorch 模型期望目标变量是一个数字，而不是一个字符串。我们可以使用来自<code class="fe oh oi oj ny b">sklearn</code>的标签编码器来转换我们的目标变量。</p><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="4d6f" class="nl mb jj ny b gy oc od l oe of">from sklearn.preprocessing import LabelEncoder<br/>le = LabelEncoder()<br/>train_y = le.fit_transform(train_y.values)<br/>test_y = le.transform(test_y.values)</span></pre></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="2253" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">负载嵌入</h1><p id="565b" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">首先，我们需要加载所需的手套嵌入。</p><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="967f" class="nl mb jj ny b gy oc od l oe of">def load_glove(word_index):<br/>    EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'<br/>    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]<br/>    embeddings_index = dict(get_coefs(*o.split(" ")) for o in open(EMBEDDING_FILE))<br/>    <br/>    all_embs = np.stack(embeddings_index.values())<br/>    emb_mean,emb_std = -0.005838499,0.48782197<br/>    embed_size = all_embs.shape[1]</span><span id="a35d" class="nl mb jj ny b gy og od l oe of">nb_words = min(max_features, len(word_index)+1)<br/>    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))<br/>    for word, i in word_index.items():<br/>        if i &gt;= max_features: continue<br/>        embedding_vector = embeddings_index.get(word)<br/>        if embedding_vector is not None: <br/>            embedding_matrix[i] = embedding_vector<br/>        else:<br/>            embedding_vector = embeddings_index.get(word.capitalize())<br/>            if embedding_vector is not None: <br/>                embedding_matrix[i] = embedding_vector<br/>    return embedding_matrix</span><span id="3b63" class="nl mb jj ny b gy og od l oe of">embedding_matrix = load_glove(tokenizer.word_index)</span></pre><p id="475e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一定要把下载这些手套向量的文件夹路径放进去。<code class="fe oh oi oj ny b">embeddings_index</code>包含什么？它是一个字典，其中的键是单词，值是单词向量，一个长度为 300 的<code class="fe oh oi oj ny b">np.array</code>。这部词典的长度大约是十亿。</p><p id="affe" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们只想要嵌入在我们的<code class="fe oh oi oj ny b">word_index</code>中的单词，我们将使用来自我们的标记器的单词索引创建一个只包含所需嵌入的矩阵。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/6553258bb97738520f0b9e586fecda8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dOT0TpHoU2MOy0pJ.png"/></div></div></figure></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="211a" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">深度学习模型</h1><h2 id="00a4" class="nl mb jj bd mc nm nn dn mg no np dp mk kr nq nr mo kv ns nt ms kz nu nv mw nw bi translated">1.TextCNN</h2><p id="5679" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">在 Yoon Kim 的论文<a class="ae jg" href="https://www.aclweb.org/anthology/D14-1181" rel="noopener ugc nofollow" target="_blank">用于句子分类的卷积神经网络</a>中首次提出了使用 CNN 对文本进行分类的想法。</p><p id="e27d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">表示:这个想法的中心概念是把我们的文档看作图像。但是怎么做呢？假设我们有一个句子，我们有<code class="fe oh oi oj ny b">maxlen</code> = 70，嵌入大小= 300。我们可以创建一个形状为 70×300 的数字矩阵来表示这个句子。图像也有一个矩阵，其中各个元素是像素值。但是任务的输入不是图像像素，而是用矩阵表示的句子或文档。矩阵的每一行对应一个单词向量。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/61976b8bdf2d215bd84fe138ee9c2868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IuiERsPVLeQhtoxK.png"/></div></div></figure><p id="4cc0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">卷积思想:对于图像，我们移动我们的 conv。水平和垂直过滤，但对于文本，我们将内核大小固定为 filter_size x embed_size，即(3，300)，我们将垂直向下移动卷积，同时查看三个单词，因为我们在这种情况下的过滤器大小为 3。这个想法似乎是正确的，因为我们的卷积滤波器不分裂字嵌入；它会查看每个单词的完整嵌入。此外，我们可以将过滤器的大小想象成一元、二元、三元等。因为我们正在分别查看 1、2、3 和 5 个单词的上下文窗口。</p><p id="81bf" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里是 CNN 网络编码在<a class="ae jg" rel="noopener" target="_blank" href="/moving-from-keras-to-pytorch-f0d4fff4ce79�"> Pytorch </a>的文本分类。</p><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="0966" class="nl mb jj ny b gy oc od l oe of">class CNN_Text(nn.Module):    <br/>    def __init__(self):<br/>        super(CNN_Text, self).__init__()<br/>        filter_sizes = [1,2,3,5]<br/>        num_filters = 36<br/>        n_classes = len(le.classes_)<br/>        self.embedding = nn.Embedding(max_features, embed_size)<br/>        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))<br/>        self.embedding.weight.requires_grad = False<br/>        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])<br/>        self.dropout = nn.Dropout(0.1)<br/>        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes)</span><span id="925c" class="nl mb jj ny b gy og od l oe of">def forward(self, x):<br/>        x = self.embedding(x)  <br/>        x = x.unsqueeze(1)  <br/>        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] <br/>        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  <br/>        x = torch.cat(x, 1)<br/>        x = self.dropout(x)  <br/>        logit = self.fc1(x) <br/>        return logit</span></pre><h2 id="0f03" class="nl mb jj bd mc nm nn dn mg no np dp mk kr nq nr mo kv ns nt ms kz nu nv mw nw bi translated">2.双向 RNN (LSTM/GRU)</h2><p id="f5ab" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">TextCNN 很适合文本分类，因为它处理近距离的单词。比如它可以一起看《纽约》。然而，它仍然不能处理特定文本序列中提供的所有上下文。它仍然没有学习数据的顺序结构，其中每个单词都依赖于前一个单词或前一个句子中的一个单词。</p><p id="3f73" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RNNs 可以帮助我们。他们可以使用隐藏状态记住以前的信息，并将其与当前任务联系起来。</p><p id="379f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">长短期记忆网络(LSTM)是 RNN 的一个子类，专门用于长时间记忆信息。此外，双向 LSTM 保持了两个方向的上下文信息，这在文本分类任务中非常有用(但是，它不适用于时间序列预测任务，因为在这种情况下我们无法看到未来)。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/df6fc0d07fef3184de5e434215c7ae57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TSgnfH6aeDviE8SB.png"/></div></div></figure><p id="b3a9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了简单解释双向 RNN，可以把 RNN 单元想象成一个黑盒，它接受一个隐藏状态(向量)和一个字向量作为输入，给出一个输出向量和下一个隐藏状态。这个盒子有一些权重需要使用损耗的反向传播来调整。此外，相同的单元格应用于所有单词，以便句子中的单词共享权重。这种现象被称为重量共享。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d00a5c507369e12ee7ae75e5e820061f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/0*uWuFDeFvBjyFjxqM.png"/></div></figure><p id="33c6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe oh oi oj ny b">Hidden state, Word vector -&gt;(RNN Cell) -&gt; Output Vector , Next Hidden state</code></p><p id="cf54" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于像“你永远不会相信”这样的长度为 4 的序列，RNN 单元给出 4 个输出向量，这些向量可以连接起来，然后用作密集前馈架构的一部分。</p><p id="4b02" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在双向 RNN 中，唯一的变化是我们以通常的方式以及相反的方式阅读文本。所以我们并行堆叠两个 rnn，我们得到 8 个输出向量来附加。</p><p id="228c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们得到了输出向量，我们就将它们发送到一系列密集层，最后是 softmax 层，以构建文本分类器。</p><p id="6a64" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在大多数情况下，您需要了解如何在神经网络中堆叠一些层以获得最佳结果。如果性能更好，我们可以在网络中尝试多个双向 GRU/LSTM 层。</p><p id="c2d3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于 RNNs 的局限性，例如不记得长期依赖关系，在实践中，我们几乎总是使用 LSTM/GRU 来建模长期依赖关系。在这种情况下，您可以将上图中的 RNN 单元格想象为 LSTM 单元格或 GRU 单元格。</p><p id="fa75" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是 Pytorch 中用于该网络的一些代码:</p><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="d9ef" class="nl mb jj ny b gy oc od l oe of">class <strong class="ny jk">BiLSTM</strong>(nn.Module):<br/>    def __init__(self):<br/>        super(BiLSTM, self).__init__()<br/>        self.hidden_size = 64<br/>        drp = 0.1<br/>        n_classes = len(le.classes_)<br/>        self.embedding = nn.Embedding(max_features, embed_size)<br/>        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))<br/>        self.embedding.weight.requires_grad = False<br/>        self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True)<br/>        self.linear = nn.Linear(self.hidden_size*4 , 64)<br/>        self.relu = nn.ReLU()<br/>        self.dropout = nn.Dropout(drp)<br/>        self.out = nn.Linear(64, n_classes)<br/><br/><br/>    def forward(self, x):<br/>        <em class="ls">#rint(x.size())</em><br/>        h_embedding = self.embedding(x)<br/>        <em class="ls">#_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))</em><br/>        h_lstm, _ = self.lstm(h_embedding)<br/>        avg_pool = torch.mean(h_lstm, 1)<br/>        max_pool, _ = torch.max(h_lstm, 1)<br/>        conc = torch.cat(( avg_pool, max_pool), 1)<br/>        conc = self.relu(self.linear(conc))<br/>        conc = self.dropout(conc)<br/>        out = self.out(conc)<br/>        return out</span></pre></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="b563" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">培养</h1><p id="3112" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">下面是我们用来训练 BiLSTM 模型的代码。代码注释的很好，请大家通读代码理解。你可能也想看看我在<a class="ae jg" rel="noopener" target="_blank" href="/moving-from-keras-to-pytorch-f0d4fff4ce79"> Pytorch </a>上的帖子。</p><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="a093" class="nl mb jj ny b gy oc od l oe of">n_epochs = 6<br/>model = BiLSTM()<br/>loss_fn = nn.CrossEntropyLoss(reduction='sum')<br/>optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)<br/>model.cuda()<br/><br/><em class="ls"># Load train and test in CUDA Memory</em><br/>x_train = torch.tensor(train_X, dtype=torch.long).cuda()<br/>y_train = torch.tensor(train_y, dtype=torch.long).cuda()<br/>x_cv = torch.tensor(test_X, dtype=torch.long).cuda()<br/>y_cv = torch.tensor(test_y, dtype=torch.long).cuda()<br/><br/><em class="ls"># Create Torch datasets</em><br/>train = torch.utils.data.TensorDataset(x_train, y_train)<br/>valid = torch.utils.data.TensorDataset(x_cv, y_cv)<br/><br/><em class="ls"># Create Data Loaders</em><br/>train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)<br/>valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)<br/><br/>train_loss = []<br/>valid_loss = []<br/><br/>for epoch <strong class="ny jk">in</strong> range(n_epochs):<br/>    start_time = time.time()<br/>    <em class="ls"># Set model to train configuration</em><br/>    model.train()<br/>    avg_loss = 0.  <br/>    for i, (x_batch, y_batch) <strong class="ny jk">in</strong> enumerate(train_loader):<br/>        <em class="ls"># Predict/Forward Pass</em><br/>        y_pred = model(x_batch)<br/>        <em class="ls"># Compute loss</em><br/>        loss = loss_fn(y_pred, y_batch)<br/>        optimizer.zero_grad()<br/>        loss.backward()<br/>        optimizer.step()<br/>        avg_loss += loss.item() / len(train_loader)<br/>    <br/>    <em class="ls"># Set model to validation configuration -Doesn't get trained here</em><br/>    model.eval()        <br/>    avg_val_loss = 0.<br/>    val_preds = np.zeros((len(x_cv),len(le.classes_)))<br/>    <br/>    for i, (x_batch, y_batch) <strong class="ny jk">in</strong> enumerate(valid_loader):<br/>        y_pred = model(x_batch).detach()<br/>        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)<br/>        <em class="ls"># keep/store predictions</em><br/>        val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy()<br/>    <br/>    <em class="ls"># Check Accuracy</em><br/>    val_accuracy = sum(val_preds.argmax(axis=1)==test_y)/len(test_y)<br/>    train_loss.append(avg_loss)<br/>    valid_loss.append(avg_val_loss)<br/>    elapsed_time = time.time() - start_time <br/>    print('Epoch <strong class="ny jk">{}</strong>/<strong class="ny jk">{}</strong> <strong class="ny jk">\t</strong> loss=<strong class="ny jk">{:.4f}</strong> <strong class="ny jk">\t</strong> val_loss=<strong class="ny jk">{:.4f}</strong>  <strong class="ny jk">\t</strong> val_acc=<strong class="ny jk">{:.4f}</strong>  <strong class="ny jk">\t</strong> time=<strong class="ny jk">{:.2f}</strong>s'.format(<br/>                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, elapsed_time))</span></pre><p id="85ef" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">培训输出如下所示:</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi op"><img src="../Images/3efcffe341e5096489b72e9a810e7cac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bBWkPuZALujJBSna.png"/></div></div></figure><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/66865d635d699610c671ecbcd9124992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MNaMuXROxT0C-EkP.png"/></div></div></figure></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="c34c" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">结果/预测</h1><pre class="ne nf ng nh gt nx ny nz oa aw ob bi"><span id="e5ff" class="nl mb jj ny b gy oc od l oe of">import scikitplot as skplt<br/>y_true = [le.classes_[x] for x <strong class="ny jk">in</strong> test_y]<br/>y_pred = [le.classes_[x] for x <strong class="ny jk">in</strong> val_preds.argmax(axis=1)]<br/>skplt.metrics.plot_confusion_matrix(<br/>    y_true, <br/>    y_pred,<br/>    figsize=(12,12),x_tick_rotation=90)</span></pre><p id="9ad0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是 BiLSTM 模型结果的混淆矩阵。我们可以看到，我们的模型做得相当好，在验证数据集上有 87%的准确率。</p><figure class="ne nf ng nh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/ac1f8fb33f7d1cd71caed2ea491eabca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xqxnRa0OaOlsqX7n.png"/></div></div></figure><p id="6867" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有趣的是，即使在模型表现不佳的地方，这也是可以理解的。例如，该模型混淆了减肥和肥胖、抑郁和焦虑、抑郁和双相情感障碍。我不是专家，但是这些病确实感觉挺像的。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="4ae0" class="ma mb jj bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">结论</h1><p id="cd36" class="pw-post-body-paragraph kg kh jj ki b kj my kl km kn mz kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">在这篇文章中，我们介绍了用于文本分类的深度学习架构，如 LSTM 和 CNN，并解释了 NLP 深度学习中使用的不同步骤。</p><p id="bc44" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">仍然有很多可以做的来改进这个模型的性能。改变学习速率、使用学习速率表、使用额外的特征、丰富嵌入、去除拼写错误等。我希望这段样板代码能为您可能面临的任何文本分类问题提供一个参考基准。</p><p id="62b3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在<a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/tree/master/multiclass" rel="noopener ugc nofollow" target="_blank"> Github </a>或者这个<a class="ae jg" href="https://www.kaggle.com/mlwhiz/multiclass-text-classification-pytorch?scriptVersionId=30273958" rel="noopener ugc nofollow" target="_blank"> Kaggle 内核</a>上找到完整的工作代码。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="e9ec" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想了解更多关于自然语言处理的知识，我想从 DeepLearning.ai 中调出一门关于<a class="ae jg" href="https://coursera.pxf.io/9WjZo0" rel="noopener ugc nofollow" target="_blank">自然语言处理</a>的优秀课程，一定要去看看。</p><p id="a524" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我以后也会写更多这样的帖子。让我知道你对这个系列的看法。在<a class="ae jg" href="https://medium.com/@rahul_agarwal" rel="noopener"> <strong class="ki jk">中</strong> </a>关注我或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX" rel="noopener ugc nofollow" target="_blank"> <strong class="ki jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter <a class="ae jg" href="https://twitter.com/MLWhiz" rel="noopener ugc nofollow" target="_blank"> @mlwhiz </a>联系。</p><p id="c95f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，一个小小的免责声明——这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p><p id="5b64" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个故事最初发表于<a class="ae jg" href="https://lionbridge.ai/articles/using-deep-learning-for-end-to-end-multiclass-text-classification/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div></div>    
</body>
</html>