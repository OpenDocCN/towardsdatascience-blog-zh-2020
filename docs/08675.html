<html>
<head>
<title>Higher accuracy and less process time in text classification with LDA and TF-IDF</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LDA和TF-IDF在文本分类中具有更高的准确率和更少的处理时间</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/higher-accuracy-and-less-process-time-in-text-classification-with-lda-and-tf-idf-d2d949e344c3?source=collection_archive---------14-----------------------#2020-06-23">https://towardsdatascience.com/higher-accuracy-and-less-process-time-in-text-classification-with-lda-and-tf-idf-d2d949e344c3?source=collection_archive---------14-----------------------#2020-06-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/7f93be275a5426ce4c012b8e9e190cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2Q7DkLcmwYUm5mfHDCyUw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">帕特里克·托马索在<a class="ae kf" href="/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="306a" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">文本分类中特征选择方法的实现</h2><p id="bced" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">变量或特征的大小被称为数据集的维度。在文本分类方法上，特征的大小可以列举很多。在这篇文章中，我们将使用线性判别分析-LDA实现tf-idf分解降维技术。</p><p id="e0de" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">我们在这项研究中的途径:</p><p id="03d8" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">1.正在准备数据集</p><p id="8599" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">2.将文本转换为特征向量</p><p id="4496" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">3.应用过滤方法</p><p id="2241" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">4.应用线性判别分析</p><p id="e142" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">5.构建随机森林分类器</p><p id="a181" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">6.结果比较</p><p id="1784" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">所有的源代码和笔记本都已经上传到这个<a class="ae kf" href="https://github.com/Qmoein/LDA-Text-Classification" rel="noopener ugc nofollow" target="_blank"> Github仓库</a>。</p><h1 id="31a8" class="mc kh it bd ki md me mf kl mg mh mi ko mj mk ml ks mm mn mo kw mp mq mr la ms bi translated">问题定式化</h1><p id="b608" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">提高文本分类的准确性和减少处理时间。</p><h1 id="8de9" class="mc kh it bd ki md me mf kl mg mh mi ko mj mk ml ks mm mn mo kw mp mq mr la ms bi translated">数据探索</h1><p id="2020" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">我们使用来自Kaggle数据集的“欧洲515，000酒店评论数据”。这些数据是从Booking.com搜集来的。文件中的所有数据对每个人都是公开的。数据最初归Booking.com所有，你可以通过Kaggle上的这个简介下载。我们需要的数据集包含515，000条正面和负面评论。</p><h1 id="1717" class="mc kh it bd ki md me mf kl mg mh mi ko mj mk ml ks mm mn mo kw mp mq mr la ms bi translated">导入库</h1><p id="9e67" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">我们用过的最重要的库是Scikit-Learn和pandas。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="8421" class="kg kh it my b gy nc nd l ne nf">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import requests<br/>import json</span><span id="4ce7" class="kg kh it my b gy ng nd l ne nf">from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.model_selection import train_test_split <br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.feature_selection import VarianceThreshold<br/>from sklearn.metrics import accuracy_score, roc_auc_score<br/>from sklearn.preprocessing import StandardScaler<br/>from nltk.stem.snowball import SnowballStemmer<br/>from string import punctuation<br/>from textblob import TextBlob<br/>import re</span></pre><h1 id="7cec" class="mc kh it bd ki md me mf kl mg mh mi ko mj mk ml ks mm mn mo kw mp mq mr la ms bi translated">正在准备数据集</h1><p id="9be5" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">我们将只研究两类，积极的和消极的。因此，我们为每个类别选择5000行，并将它们复制到Pandas数据帧中(每个部分5000行)。我们在这个项目中使用了Kaggle的笔记本，因此数据集被加载为本地文件。如果您正在使用其他工具或作为脚本运行，您可以下载它。让我们看一下数据集:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="eb7b" class="kg kh it my b gy nc nd l ne nf">fields = ['Positive_Review', 'Negative_Review']<br/>df = pd.read_csv(<br/>    '../input/515k-hotel-reviews-data-in-europe/Hotel_Reviews.csv',<br/>    usecols= fields, nrows=5000)</span><span id="340f" class="kg kh it my b gy ng nd l ne nf">df.head()</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/80d23a634f0833136d81785761cf7739.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XFmOM-V1vYDHKSNzjvu-Gg.png"/></div></div></figure><h2 id="a44b" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated"><strong class="ak">使用NLTK SnowballStemmer对单词进行词干分析:</strong></h2><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="e045" class="kg kh it my b gy nc nd l ne nf">stemmer = SnowballStemmer('english')<br/>df['Positive_Review'] = df['Positive_Review'].apply(<br/>    lambda x:' '.join([stemmer.stem(y) for y in x.split()]))</span><span id="5bca" class="kg kh it my b gy ng nd l ne nf">df['Negative_Review'] = df['Negative_Review'].apply(<br/>    lambda x: ' '.join([stemmer.stem(y) for y in x.split()]))</span></pre><h2 id="3c7f" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated"><strong class="ak">删除停用词:</strong></h2><p id="69bb" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">用<a class="ae kf" href="https://countwordsfree.com/stopwords/english/json" rel="noopener ugc nofollow" target="_blank">countwordsfree.com</a>列表理解和pandas.DataFrame.apply排除停用词</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="d6ac" class="kg kh it my b gy nc nd l ne nf">url = "<a class="ae kf" href="https://countwordsfree.com/stopwords/english/json" rel="noopener ugc nofollow" target="_blank">https://countwordsfree.com/stopwords/english/json</a>"<br/>response = pd.DataFrame(data = json.loads(requests.get(url).text))<br/>SW = list(response['words'])</span><span id="3b10" class="kg kh it my b gy ng nd l ne nf">df['Positive_Review'] = df['Positive_Review'].apply(<br/>    lambda x: ' '.join([word for word in x.split() if word not in (SW)]))</span><span id="0541" class="kg kh it my b gy ng nd l ne nf">df['Negative_Review'] = df['Negative_Review'].apply(<br/>    lambda x: ' '.join([word for word in x.split() if word not in (SW)]))</span></pre><h2 id="8b6b" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated"><strong class="ak">移除数字并初始化数据集:</strong></h2><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="c270" class="kg kh it my b gy nc nd l ne nf">df_Positive = df['Positive_Review'].copy()<br/>df_Positive = df_Positive.str.replace('\d+', '')</span><span id="e195" class="kg kh it my b gy ng nd l ne nf">df_Negative = df['Negative_Review'].copy()<br/>df_Negative = df_Negative.str.replace('\d+', '')</span></pre><h1 id="4f2f" class="mc kh it bd ki md me mf kl mg mh mi ko mj mk ml ks mm mn mo kw mp mq mr la ms bi translated">将文本转换为特征向量</h1><p id="8e82" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">单词包和TF-IDF是用于检测文档主题的两种方法。它们之间的区别是，BoW使用一个词在文档中出现的次数作为度量，而TF-IDF在检测主题时给每个词一个权重。换句话说，在TF-IDF中，使用单词分数而不是单词计数，因此我们可以说TF-IDF测量相关性，而不是频率。</p><p id="5441" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">在这一部分，我们为每一列使用了Scikit-Learn TfidfVectorizer，并为矢量化构建了一个N元模型，该模型可用作估计器的输入。然后，我们添加一个目标列，对于正面评价，给定值为“1”，对于负面评价，给定值为“0”。注意，我们定义了“min_df=2”。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="348f" class="kg kh it my b gy nc nd l ne nf">tfidf = TfidfVectorizer(min_df=2,max_df=0.5, ngram_range=(1,3))<br/>features = tfidf.fit_transform(df_Positive)</span><span id="eb7a" class="kg kh it my b gy ng nd l ne nf">df_Positive = pd.DataFrame(<br/>    features.todense(),<br/>    columns=tfidf.get_feature_names()<br/>)</span><span id="9dd2" class="kg kh it my b gy ng nd l ne nf">df_Positive['Target'] = '1'<br/>df_Positive.head()</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/8c23d5dbcc6d16a3cf23f1887933cf25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*164CixR772M2dFhlzO-dXw.png"/></div></div></figure><p id="a424" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">如您所见，结果显示了7，934个特征，这是一个很大的数字，在目标列中有值“1”。</p><p id="a4a2" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">对于负面评价，我们也会这样做:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="528d" class="kg kh it my b gy nc nd l ne nf">tfidf = TfidfVectorizer(min_df=2,max_df=.05, ngram_range=(1,3))<br/>features = tfidf.fit_transform(df_Negative)</span><span id="ccd1" class="kg kh it my b gy ng nd l ne nf">df_Negative = pd.DataFrame(<br/>    features.todense(),<br/>    columns=tfidf.get_feature_names()<br/>)</span><span id="a064" class="kg kh it my b gy ng nd l ne nf">df_Negative['Target'] = '0'<br/>df_Negative.head()</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/0b48a95ac017b9fa3f0423ec4e0c79df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JbkW55gf_LsG58btAkYZOw.png"/></div></div></figure><p id="b02c" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">有6，754个特征，负面评价的目标值为“0”。现在我们将通过追加一个到另一个来合并这两个数据集。共同特征将被认为是一个，但目标是不同的。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="ad4b" class="kg kh it my b gy nc nd l ne nf">df = df_Positive.append(df_Negative)<br/>df.shape<br/>#out[] (10000, 12676)</span></pre><p id="5c09" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">因此，最终数据集有10，000个实例(每个实例有5，000个实例)和12，676个要素，这意味着‭2,012要素是相互的((7934+6754)-12676)。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="1a62" class="kg kh it my b gy nc nd l ne nf">#change the value of NaN to Zero<br/>df = df.fillna(0)</span></pre><h2 id="8c14" class="kg kh it bd ki kj kk dn kl km kn dp ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">定义训练/测试分割:</h2><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="08bb" class="kg kh it my b gy nc nd l ne nf">#define x and y:<br/>x = df.drop('Target',axis=1)<br/>y = df['Target']</span><span id="0531" class="kg kh it my b gy ng nd l ne nf">x.shape, y.shape<br/>#out[] ((10000, 12675), (10000,))</span><span id="75fd" class="kg kh it my b gy ng nd l ne nf">#define train-test-slpit:<br/>x_train, x_test, y_train, y_test = train_test_split(<br/>    x, y, test_size = 0.2, random_state = 0, stratify = y)</span></pre><h1 id="4d5e" class="mc kh it bd ki md me mf kl mg mh mi ko mj mk ml ks mm mn mo kw mp mq mr la ms bi translated">应用过滤方法-移除常量要素</h1><p id="1850" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">常量要素是数据集中所有输出仅包含一个值的要素。所以他们不能给我们任何有价值的信息来帮助分类器。因此，最好将它们移除。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="a2c0" class="kg kh it my b gy nc nd l ne nf">constant_filter = VarianceThreshold(threshold = 0.0002)<br/>constant_filter.fit(x_train)<br/>feature_list = x_train[x_train.columns[<br/>    constant_filter.get_support(indices=True)]]</span><span id="b9d6" class="kg kh it my b gy ng nd l ne nf">print('Number of selected features: ' ,len(list(feature_list)),'\n')<br/>print('List of selected features: \n' ,list(feature_list))</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nk"><img src="../Images/de7b3396ca2e31f64e40ef45cf61abcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VV4wlH5gKadoeqi7EyJ0ag.png"/></div></div></figure><p id="4a0c" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">您可以看到所选功能的列表。它显示了之前被词干化的716个单词的数量。这个列表可以帮助我们通过分析列表及其项目与我们的主题(正面和负面评论)的联系来分析我们输出的质量。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="ef21" class="kg kh it my b gy nc nd l ne nf">x_train_filter = constant_filter.transform(x_train)<br/>x_test_filter = constant_filter.transform(x_test)</span><span id="e507" class="kg kh it my b gy ng nd l ne nf">x_train_filter.shape, x_test_filter.shape, x_train.shape<br/>#out[] ((8000, 716), (2000, 716), (8000, 12675))</span></pre><p id="c2a2" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">因此，结果显示，94.35%的特征是不变的，而模型丢弃了它们。因此，我们删除了11，959个特征，这是一个相当大的数字。所以我们必须定义一个新的训练，测试数据帧:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="18f0" class="kg kh it my b gy nc nd l ne nf">x_train_filter = pd.DataFrame(x_train_filter)<br/>x_test_filter = pd.DataFrame(x_test_filter)</span></pre><h1 id="1083" class="mc kh it bd ki md me mf kl mg mh mi ko mj mk ml ks mm mn mo kw mp mq mr la ms bi translated">应用过滤方法-移除相关要素</h1><p id="8f27" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">相关特征是在线性空间中彼此接近的特征。移除它们的函数写在下面。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="53cd" class="kg kh it my b gy nc nd l ne nf">def get_correlation(data, threshold):<br/>    corr_col = set()<br/>    cormat = data.corr()<br/>    for i in range(len(cormat.columns)):<br/>        for j in range(i):<br/>            if abs(cormat.iloc[i,j]) &gt; threshold:<br/>                colname = cormat.columns[i]<br/>                corr_col.add(colname)<br/>    return corr_col</span><span id="7328" class="kg kh it my b gy ng nd l ne nf">corr_features = get_correlation(x_train_filter, 0.70)</span></pre><p id="9e4c" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">然后，在移除相关特征之后，我们定义新的数据帧。结果显示我们去除了23个相关特征。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="33d8" class="kg kh it my b gy nc nd l ne nf">x_train_uncorr = x_train_filter.drop(labels= corr_features, axis = 1)<br/>x_test_uncorr = x_test_filter.drop(labels= corr_features, axis = 1)<br/>x_train_uncorr = pd.DataFrame(x_train_uncorr)<br/>x_test_uncorr = pd.DataFrame(x_test_uncorr)<br/>x_train_uncorr.shape, x_test_uncorr.shape</span><span id="1122" class="kg kh it my b gy ng nd l ne nf">out[] ((8000, 693), (2000, 693), (8000, 716))</span></pre><h1 id="f4af" class="mc kh it bd ki md me mf kl mg mh mi ko mj mk ml ks mm mn mo kw mp mq mr la ms bi translated">应用线性判别分析— LDA</h1><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nl"><img src="../Images/684e7b7bfeec47dfda1a59e7c42a628a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*giXTdpOyq8hcYQg61vFzMw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">伊万·弗拉尼奇在<a class="ae kf" href="/s/photos/offroad?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7210" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">现在，我们准备在数据集上建立LDA模型，进行特征选择，然后在其上运行随机森林分类器。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="1a6d" class="kg kh it my b gy nc nd l ne nf">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA</span><span id="15a9" class="kg kh it my b gy ng nd l ne nf">lda = LDA(n_components=1)<br/>x_train_lda = lda.fit_transform(x_train_uncorr, y_train)<br/>x_test_lda = lda.fit_transform(x_test_uncorr, y_test)</span></pre><h1 id="f860" class="mc kh it bd ki md me mf kl mg mh mi ko mj mk ml ks mm mn mo kw mp mq mr la ms bi translated">构建随机森林分类器(RFC)</h1><p id="217e" class="pw-post-body-paragraph lc ld it le b lf lg lh li lj lk ll lm kp ln lo lp kt lq lr ls kx lt lu lv lw im bi translated">它是一种元估计器，将多个决策树分类器拟合到数据集的各个子样本上，并使用平均来提高预测精度和控制过拟合。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="1a9a" class="kg kh it my b gy nc nd l ne nf">def runRandomForest(x_train, x_test, y_train, y_test):<br/>    clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)<br/>    clf.fit(x_train, y_train)<br/>    y_pred = clf.predict(x_test)<br/>    print ('accracy is: ', accuracy_score(y_test, y_pred))</span></pre><p id="8074" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">使用LDA模型运行随机森林分类器:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="0da7" class="kg kh it my b gy nc nd l ne nf">#Run calssifier <strong class="my iu">using </strong>LDA model<br/>%%time<br/>runRandomForest(x_train_lda, x_test_lda, y_train, y_test)</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/d7d177e2e5172fd4ccc513cd1a931569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*864njIF_wbVmKeEZrfpNrg.png"/></div></div></figure><p id="d821" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">LDA模型的准确度为97.95%，壁时间约为709毫秒。在非LDA模型中，结果仍可接受，这是因为tf-idf矢量化效果很好。但是正如你在墙下面看到的，时间是14秒，比LDA模型慢19.74倍。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="a206" class="kg kh it my b gy nc nd l ne nf">##Run calssifier <strong class="my iu">without </strong>using LDA model<br/>%%time<br/>runRandomForest(x_train, x_test, y_train, y_test)</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/8822d296e1cfa2c0ea5b1ce2c625a28a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c81WLDcsbkNH-SSmmepS5A.png"/></div></div></figure><p id="1973" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">所有的源代码和笔记本都已经上传到这个<a class="ae kf" href="https://github.com/Qmoein/LDA-Text-Classification" rel="noopener ugc nofollow" target="_blank"> Github库</a>中。我期待听到任何反馈。<a class="ae kf" href="mailto:moein.ghodsi@gmail.com" rel="noopener ugc nofollow" target="_blank">联系我。</a></p><p id="2422" class="pw-post-body-paragraph lc ld it le b lf lx lh li lj ly ll lm kp lz lo lp kt ma lr ls kx mb lu lv lw im bi translated">此外，你可以在这里阅读更多关于NLP“文本分类:来自5个Kaggle竞赛的所有提示和技巧”<a class="ae kf" href="https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>