<html>
<head>
<title>Machine Learning Basics: Naive Bayes Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习基础:朴素贝叶斯分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-basics-naive-bayes-classification-964af6f2a965?source=collection_archive---------8-----------------------#2020-09-15">https://towardsdatascience.com/machine-learning-basics-naive-bayes-classification-964af6f2a965?source=collection_archive---------8-----------------------#2020-09-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1e19" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解朴素贝叶斯算法，并通过实现朴素贝叶斯分类模型来解决一个著名的虹膜数据集问题</h2></div><p id="cb35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在之前的<a class="ae le" rel="noopener" target="_blank" href="/machine-learning-basics-simple-linear-regression-bc83c01baa07">故事</a>中，我已经解释了各种<strong class="kk iu"> <em class="lf">回归</em> </strong>模型的实现程序。此外，我还描述了逻辑回归，KNN 和 SVM 分类模型的实施。在本文中，我们将通过一个例子来介绍著名的朴素贝叶斯分类模型的算法。</p><h2 id="a136" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">朴素贝叶斯分类综述</h2><p id="85ac" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">朴素贝叶斯就是这样一种分类算法，由于其“<strong class="kk iu"><em class="lf"/></strong>”的特性，在分类中是一个不容忽视的算法。它假设测量的特征是相互独立的。</p><p id="e169" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，如果一种动物有猫眼、胡须和一条长尾巴，它就可以被认为是猫。即使这些特征相互依赖或依赖于其他特征的存在，所有这些特性独立地促成了这种动物是猫的可能性，这就是为什么它被称为“天真”。</p><p id="7b28" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据贝叶斯定理，各种特征是相互独立的。对于两个独立的事件，<code class="fe me mf mg mh b">P(A,B) = P(A)P(B)</code>。贝叶斯定理的这个假设可能在实践中从未遇到过，因此它解释了朴素贝叶斯中的“朴素”部分。贝叶斯定理表述为:<strong class="kk iu"><em class="lf">P(a | b)=(P(b | a)* P(a))/P(b)。</em> </strong>其中 P(a|b)是给定 b 的概率。</p><p id="aa8a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们用一个简单的例子来理解这个算法。如果这个学生在考试那天穿红色的衣服，他就会通过考试。我们可以使用上面讨论的后验概率方法来解决它。</p><p id="6cd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由贝叶斯定理，<em class="lf"> P(通|红)= P(红|通)* P(通)/ P(红)。</em></p><p id="6870" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从数值来看，我们假设 P(红|通)= 3/9 = 0.33，P(红)= 5/14 = 0.36，P(通)= 9/14 = 0.64。现在 P(通|红)= 0.33 * 0.64 / 0.36 = 0.60，概率较高。</p><p id="b187" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这样，朴素贝叶斯使用类似的方法来预测基于各种属性的不同类的概率。</p><h2 id="c875" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">问题分析</h2><p id="5c1b" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">为了实现朴素贝叶斯分类，我们将使用一个非常著名的鸢尾花数据集，它由 3 类花组成。其中有 4 个自变量，分别是:<strong class="kk iu"><em class="lf"/></strong><strong class="kk iu"><em class="lf">萼片宽度</em></strong><strong class="kk iu"><em class="lf">花瓣长度</em> </strong>和<strong class="kk iu"> <em class="lf">花瓣宽度</em> </strong>。因变量是<strong class="kk iu"> <em class="lf">物种</em> </strong>，我们将利用花的四个独立特征对其进行预测。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mi"><img src="../Images/3e71a747fdca3cbef9359df6a42a11e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nVRWD8La455Ld7f5"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">安妮·斯普拉特在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="8411" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有 3 类物种，即<em class="lf">刚毛藻、</em>云芝和<em class="lf">海滨锦葵</em>。这个数据集最初是由罗纳德·费雪在 1936 年提出的。使用花的各种特征(独立变量)，我们必须使用朴素贝叶斯分类模型对给定的花进行分类。</p><h2 id="c597" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">步骤 1:导入库</h2><p id="4177" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">和往常一样，第一步总是包括导入库，即 NumPy、Pandas 和 Matplotlib。</p><pre class="mj mk ml mm gt my mh mz na aw nb bi"><span id="7fcf" class="lg lh it mh b gy nc nd l ne nf">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span></pre><h2 id="a403" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">步骤 2:导入数据集</h2><p id="319f" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">在这一步中，我们将导入 Iris Flower 数据集，该数据集作为<code class="fe me mf mg mh b">IrisDataset.csv</code>存储在我的 github 存储库中，并将其保存到变量<code class="fe me mf mg mh b">dataset. </code>中。此后，我们将 4 个自变量分配给<strong class="kk iu"><em class="lf"/></strong>X，将因变量“物种”分配给<strong class="kk iu"> <em class="lf"> Y </em> </strong>。显示数据集的前 5 行。</p><pre class="mj mk ml mm gt my mh mz na aw nb bi"><span id="0524" class="lg lh it mh b gy nc nd l ne nf">dataset = pd.read_csv('<a class="ae le" href="https://raw.githubusercontent.com/mk-gurucharan/Classification/master/IrisDataset.csv'" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/mk-gurucharan/Classification/master/IrisDataset.csv'</a>)</span><span id="27cb" class="lg lh it mh b gy ng nd l ne nf">X = dataset.iloc[:,:4].values<br/>y = dataset['species'].values</span><span id="20ad" class="lg lh it mh b gy ng nd l ne nf">dataset.head(5)</span><span id="5fde" class="lg lh it mh b gy ng nd l ne nf">&gt;&gt;<br/>sepal_length  sepal_width  petal_length  petal_width   species<br/>5.1           3.5          1.4           0.2           setosa<br/>4.9           3.0          1.4           0.2           setosa<br/>4.7           3.2          1.3           0.2           setosa<br/>4.6           3.1          1.5           0.2           setosa<br/>5.0           3.6          1.4           0.2           setosa</span></pre><h2 id="2f4c" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">步骤 3:将数据集分为训练集和测试集</h2><p id="da31" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">一旦我们获得了数据集，我们必须将数据分成训练集和测试集。在这个数据集中，有 150 行，3 个类中的每一个都有 50 行。由于每个类都是以连续的顺序给出的，所以我们需要随机分割数据集。这里，我们有<code class="fe me mf mg mh b">test_size=0.2</code>，这意味着数据集的<strong class="kk iu"><em class="lf"/></strong>的 20%<strong class="kk iu"><em class="lf"/></strong>将用于测试目的，剩余的<strong class="kk iu"><em class="lf"/></strong>的 80%<strong class="kk iu"><em class="lf"/></strong>将用于训练朴素贝叶斯分类模型。</p><pre class="mj mk ml mm gt my mh mz na aw nb bi"><span id="e086" class="lg lh it mh b gy nc nd l ne nf">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)</span></pre><h2 id="97ff" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">步骤 4:特征缩放</h2><p id="5c56" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">使用要素缩放选项将数据集缩小到更小的范围。在这种情况下，<code class="fe me mf mg mh b">X_train</code>和<code class="fe me mf mg mh b">X_test </code>值都被缩小到更小的值，以提高程序的速度。</p><pre class="mj mk ml mm gt my mh mz na aw nb bi"><span id="1743" class="lg lh it mh b gy nc nd l ne nf">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><h2 id="42da" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">步骤 5:在训练集上训练朴素贝叶斯分类模型</h2><p id="f561" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">在这一步，我们引入从<code class="fe me mf mg mh b">sklearn.naive_bayes</code>库中使用的类<code class="fe me mf mg mh b">GaussianNB</code>。这里，我们使用了高斯模型，还有其他几种模型，如伯努利模型、分类模型和多项式模型。这里，我们将 GaussianNB 类分配给变量<code class="fe me mf mg mh b">classifier</code>，并为其拟合 X_train 和 y_train 值，用于训练目的。</p><pre class="mj mk ml mm gt my mh mz na aw nb bi"><span id="9a53" class="lg lh it mh b gy nc nd l ne nf">from sklearn.naive_bayes import GaussianNB<br/>classifier = GaussianNB()<br/>classifier.fit(X_train, y_train)</span></pre><h2 id="ea7c" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">步骤 6:预测测试集结果</h2><p id="f31b" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">一旦模型被训练，我们使用<code class="fe me mf mg mh b">classifier.predict()</code>来预测测试集的值，并且预测的值被存储到变量<code class="fe me mf mg mh b">y_pred.</code></p><pre class="mj mk ml mm gt my mh mz na aw nb bi"><span id="4fb2" class="lg lh it mh b gy nc nd l ne nf">y_pred = classifier.predict(X_test) <br/>y_pred</span></pre><h2 id="0d33" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">步骤 7:混淆矩阵和准确性</h2><p id="95f7" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">这是分类技术中最常用的一步。在这里，我们看到了训练模型的准确性，并绘制了混淆矩阵。</p><p id="37ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">混淆矩阵是一个表，用于显示当测试集的真实值已知时，对分类问题的正确和错误预测的数量。它的格式如下</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/74881daa5f3eb9f2b0c834afe2b00c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*aDcJceSYfH7GBxJJpzwvKA.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">作者图片</p></figure><p id="f506" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">真实值是正确预测的次数。</p><pre class="mj mk ml mm gt my mh mz na aw nb bi"><span id="2086" class="lg lh it mh b gy nc nd l ne nf">from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)</span><span id="91d7" class="lg lh it mh b gy ng nd l ne nf">from sklearn.metrics import accuracy_score <br/>print ("Accuracy : ", accuracy_score(y_test, y_pred))<br/>cm</span><span id="10bc" class="lg lh it mh b gy ng nd l ne nf">&gt;&gt;Accuracy :  0.9666666666666667</span><span id="d988" class="lg lh it mh b gy ng nd l ne nf">&gt;&gt;array([[14,  0,  0],<br/>       [ 0,  7,  0],<br/>       [ 0,  1,  8]])</span></pre><p id="c917" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的混淆矩阵，我们推断，在 30 个测试集数据中，29 个被正确分类，只有 1 个被错误分类。这给了我们 96.67%的高准确率。</p><h2 id="28d3" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">步骤 8:将实际值与预测值进行比较</h2><p id="9813" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">在这个步骤中，创建一个 Pandas DataFrame 来比较原始测试集(<strong class="kk iu"> <em class="lf"> y_test </em> </strong>)和预测结果(<strong class="kk iu"> <em class="lf"> y_pred </em> </strong>)的分类值。</p><pre class="mj mk ml mm gt my mh mz na aw nb bi"><span id="39c0" class="lg lh it mh b gy nc nd l ne nf">df = pd.DataFrame({'Real Values':y_test, 'Predicted Values':y_pred})<br/>df</span><span id="1e61" class="lg lh it mh b gy ng nd l ne nf">&gt;&gt; <br/>Real Values   Predicted Values<br/>setosa        setosa<br/>setosa        setosa<br/>virginica     virginica<br/>versicolor    versicolor<br/>setosa        setosa<br/>setosa        setosa<br/>...  ...   ...  ...  ...<br/>virginica     versicolor<br/>virginica     virginica<br/>setosa        setosa<br/>setosa        setosa<br/>versicolor    versicolor<br/>versicolor    versicolor</span></pre><p id="e10f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个步骤是一个额外的步骤，它不像混淆矩阵那样提供很多信息，并且主要用于回归以检查预测值的准确性。</p><p id="5e60" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如你所见，有一个错误的预测是预测了<em class="lf">云芝</em>而不是<em class="lf">弗吉尼亚</em>。</p><h2 id="6d11" class="lg lh it bd li lj lk dn ll lm ln dp lo kr lp lq lr kv ls lt lu kz lv lw lx ly bi translated">结论—</h2><p id="0caa" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr mb kt ku kv mc kx ky kz md lb lc ld im bi translated">因此，在这个故事中，我们已经成功地建立了一个<strong class="kk iu"> <em class="lf">朴素贝叶斯分类</em> </strong>模型，该模型能够根据 4 个特征对一朵花进行分类。这个模型可以用网上的其他几个分类数据集来实现和测试。</p><p id="e41a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我还附上了我的 GitHub 资源库的链接，你可以在那里下载这个 Google Colab 笔记本和数据文件供你参考。</p><div class="ni nj gp gr nk nl"><a href="https://github.com/mk-gurucharan/Classification" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">MK-guru charan/分类</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">这是一个由 Python 代码组成的知识库，用于构建不同类型的分类模型，以评估和…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">github.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz ms nl"/></div></div></a></div><p id="80c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您还可以在下面找到该程序对其他分类模型的解释:</p><ul class=""><li id="76f8" class="oa ob it kk b kl km ko kp kr oc kv od kz oe ld of og oh oi bi translated"><a class="ae le" rel="noopener" target="_blank" href="/machine-learning-basics-logistic-regression-890ef5e3a272">逻辑回归</a></li><li id="fe62" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated"><a class="ae le" rel="noopener" target="_blank" href="/machine-learning-basics-k-nearest-neighbors-classification-6c1e0b209542">K-最近邻(KNN)分类</a></li><li id="ee02" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated"><a class="ae le" rel="noopener" target="_blank" href="/machine-learning-basics-support-vector-machine-svm-classification-205ecd28a09d">支持向量机(SVM)分类</a></li><li id="ecf2" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated">朴素贝叶斯分类</li><li id="1a1d" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated">随机森林分类(即将推出)</li></ul><p id="6940" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在接下来的文章中，我们将会遇到更复杂的回归、分类和聚类模型。到那时，快乐的机器学习！</p></div></div>    
</body>
</html>