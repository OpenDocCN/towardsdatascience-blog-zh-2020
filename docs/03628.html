<html>
<head>
<title>An Intuitive Explanation of Kernels in Support Vector Machine (SVM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机(SVM)内核的直观解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-explanation-of-kernels-in-support-vector-machine-svm-9ef366e8d5fb?source=collection_archive---------45-----------------------#2020-04-05">https://towardsdatascience.com/an-intuitive-explanation-of-kernels-in-support-vector-machine-svm-9ef366e8d5fb?source=collection_archive---------45-----------------------#2020-04-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="9c8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，内核是帮助我们更快完成某些计算的捷径，否则这些计算将涉及到更高维度空间的计算。这听起来相当抽象。在这篇博文中，我将带你看一个只需要基本算法的简单例子。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="3898" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">简单的例子:</strong></p><p id="6915" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们有一个三维向量x = (x1，x2，x3)。我们这样定义这个运算f(x):f(x)=(x1x 1，x1x2，x1x3，x2x1，x2x2，x2x3，x3x1，x3x2，x3x3)。换句话说，它想把x中的每一对相乘，产生一个9维向量。</p><p id="6f1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们插上一些数字，更直观一点！假设x = (1，2，3)；y = (4，5，6)。然后:<br/> f(x) = (1，2，3，2，4，6，3，6，9) <br/> f(y) = (16，20，24，20，25，30，24，30，36)</p><p id="e50e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，由于某种原因，我们实际上并不关心f(x)和f(y)。我们只想知道点积，<f f="">。提醒一下，点积意味着f(x)的第一维乘以f(y)的第一维，f(x)的第二维乘以f(y)的第二维，……f(x)的第九维乘以f(y)的第九维，我们把它们都加起来。所以:</f></p><p id="b0c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><f f="">= 16+40+72+40+100+180+72+180+324 = 1024</f></p><p id="e1f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一大堆代数！主要是因为f是从3维到9维空间的映射。尽管最终的答案只有一个数字，但我们必须在中间“膨胀”，在9维空间中完成所有这些繁琐的计算，然后才能浓缩成一个数字。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ks"><img src="../Images/46698a9767701b6ef376d2e2b58158cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*5BL8T6ml6ylLOpkxR3YDWQ.png"/></div></figure><p id="de4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我告诉你有一条捷径呢？</p><p id="9290" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我声称“内核”，K(x，y ) = ( <x y="">)，达到了同样的效果。就是我们在x和y上做点积而不是f(x)和f(y)，然后平方。</x></p><p id="4f40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来测试一下:</p><p id="8947" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">提醒:x = (1，2，3)；y = (4，5，6)。</p><p id="278c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><x y="">= x1 y1+x2 y2+x3y 3 = 1×4+2×5+3×6 = 32<br/>K(x，y) = ( &lt; x，y &gt; ) = 32 = 1024</x></p><p id="26f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">瞧，同样的结果。但是这个计算要容易得多，因为我们只在三维空间中操作。我们从未涉足9维空间！</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="bebe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">数学定义</strong>:</p><p id="beed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们继续讨论一些数学形式。</p><p id="b22b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">K(x，y) = <f f=""/></p><p id="79ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">k表示核函数。这里x，y是n维输入。<em class="la"> f </em>是从<em class="la"> n </em>维度到<em class="la"> m </em>维度空间的映射。通常<em class="la"> m </em>比<em class="la"> n </em>大很多。<strong class="jp ir">内核是一个函数，取<em class="la"> x </em>和y<em class="la">T28】作为输入，不需要计算<em class="la"> f(x) </em>和<em class="la"> f(y) </em>就能得到与&lt;T29】f(x)</em>、<em class="la"> f(y) </em> &gt;相同的结果。</strong></p><p id="e32e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">内核还有一个好处:它们允许我们在无限的维度上做事情！<em class="la"> f(x) </em>可以是从n维到无限维的映射，因而不可能先写出<em class="la"> f(x) </em>和<em class="la"> f(y) </em>，再做点积。那么内核给了我们一个绝妙的捷径。一个这样的例子是径向基函数(RBF)内核。</p><p id="7842" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与SVM的关系:这与SVM有什么关系？SVM的思想是y = wφ(x)+b，其中w是权重，φ是特征向量，b是偏差。如果y &gt;为0，那么我们将数据分类为1类，否则分类为0类。我们希望找到一组权重和偏差，以使利润最大化。一些教材说，对于SVM来说，内核使数据线性可分。我认为更准确的说法是，<em class="la">内核不能让数据线性分离。特征向量phi(x)使数据线性可分</em>。核是为了让计算过程更快更容易，特别是当特征向量phi的维数非常高的时候。</p><p id="be09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">为什么也可以理解为相似度的度量:</strong> <br/>如果我们把核的定义放在上面，&lt; f(x)，f(y) &gt;，在SVM和特征向量的上下文中，就变成了&lt; phi(x)，phi(y) &gt;。点积意味着phi(x)到phi(y)的投影，或者通俗地说，x和y在它们的特征空间中有多少重叠。换句话说，他们是多么的相似。点积是衡量相似性的指标。内核只是一种实现点积效果的方法，而不需要在f(x)和f(y)之间做点积。</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="a2e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="la">原载于</em><a class="ae lb" href="https://www.quora.com/What-are-kernels-in-machine-learning-and-SVM-and-why-do-we-need-them/answer/Lili-Jiang" rel="noopener ugc nofollow" target="_blank"><em class="la">www.quora.com</em></a><em class="la">。</em></p></div></div>    
</body>
</html>