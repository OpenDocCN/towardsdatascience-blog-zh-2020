<html>
<head>
<title>Adam Optimization Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">亚当优化算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adam-optimization-algorithm-1cdc9b12724a?source=collection_archive---------11-----------------------#2020-05-31">https://towardsdatascience.com/adam-optimization-algorithm-1cdc9b12724a?source=collection_archive---------11-----------------------#2020-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c281" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种有效优化算法综述</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/982e7772fc0bc51c3162ecb322f0b0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HJNr01GKB67Orl85"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:<a class="ae ky" href="https://unsplash.com/@ratushny?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dmitry Ratushny</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="a3b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据牛津词典的定义，优化是对一种情况或资源进行最佳或最有效利用的行为，或者简单地说，就是尽最大努力。通常，如果某样东西可以用数学建模，那么它很可能可以被优化。这在深度学习领域(老实说，可能是整个人工智能)发挥着至关重要的作用，因为你选择的优化算法可能会在几分钟、几小时或几天内，在某些情况下，几周、几个月或一年内获得高质量的结果。</p><p id="4cc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，你将学到:</p><ul class=""><li id="cd8c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">什么是亚当优化？</li><li id="59ca" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在你的深度学习模型中使用Adam进行优化有什么好处？</li><li id="b371" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">亚当是如何工作的？</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="3594" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">亚当是什么？</h2><p id="0e6a" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">Adam优化是对随机梯度下降的扩展，可以用来代替经典的随机梯度下降来更有效地更新网络权重。</p><p id="06c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，Adam这个名字并不是一个缩写词，事实上，OpenAI的Diederik P. Kingma和多伦多大学的吉米·巴雷在论文中指出，这个名字来源于<em class="no">自适应矩估计，该论文最初是在2015年<a class="ae ky" href="https://iclr.cc/" rel="noopener ugc nofollow" target="_blank"> ICLR </a>的一篇会议论文中提出的，题为<a class="ae ky" href="https://arxiv.org/pdf/1412.6980.pdf" rel="noopener ugc nofollow" target="_blank"> Adam:一种随机优化的方法</a>。</em></p><p id="d88a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者们立即列举了将Adam应用于非凸优化问题的许多迷人的好处，我将继续分享如下:</p><ul class=""><li id="debc" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">实现简单(我们将在本文后面实现Adam，您将直接看到，利用强大的深度学习框架如何用更少的代码行使实现变得更加简单。)</li><li id="dba4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">计算效率高</li><li id="8889" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">几乎没有内存需求</li><li id="7d71" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">梯度的对角缩放不变(这意味着Adam对于梯度乘以仅具有正因子的对角矩阵是不变的——为了更好地理解这一点<a class="ae ky" href="https://stats.stackexchange.com/questions/360322/what-does-diagonal-rescaling-of-the-gradients-mean-in-adam-paper#:~:text=invariant%20to%20diagonal%20rescaling%20of%20the%20gradients%2C%20%5B...%5D&amp;text=Second%2C%20while%20the%20magnitudes%20of,with%20the%20magnitudes%20of%20parameters." rel="noopener ugc nofollow" target="_blank">阅读该堆栈交换</a></li><li id="3750" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">非常适合在数据和/或参数方面很大的问题</li><li id="a3f3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">适用于非静止目标</li><li id="e8d8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">适用于非常嘈杂和/或稀疏梯度的问题</li><li id="07cc" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">超参数有直观的解释，通常需要很少的调整(我们将在配置部分详细介绍)</li></ul><h2 id="9b63" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">嗯……它是如何工作的？</h2><p id="e923" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">简单来说，Adam利用动量和自适应学习率来更快地收敛。</p><p id="9624" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">动量</strong></p><p id="7a8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在解释动量时，研究人员和实践者都喜欢使用球滚下山坡的类比，球滚向局部最小值的速度更快，但本质上我们必须知道的是，动量算法加速了相关方向的随机梯度下降，并抑制了振荡。</p><p id="2343" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了将动量引入我们的神经网络，我们将时间元素添加到当前更新向量的过去时间步长的更新向量中。这使得球的动量增加了一些。这可以用数学方法表示，如图2所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5f971c99f16cbeb1b391698d2983157e.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*Rehsh930jHaWl-Qqri-HLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:动量更新方法，其中θ是网络的参数，即权重、偏差或激活度，η是学习率，J是我们试图优化的目标函数，γ是常数项，也称为动量。Vt-1(注意t-1是下标)是过去的时间步，Vt(注意t是下标)是当前的时间步。</p></figure><p id="df9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">动量项γ通常被初始化为0.9或Sebastian Ruder论文<a class="ae ky" href="https://arxiv.org/pdf/1609.04747.pdf" rel="noopener ugc nofollow" target="_blank">中提到的某个类似项【梯度下降优化算法概述</a>。</p><p id="df60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">自适应学习率</strong></p><p id="3464" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自适应学习率可以被认为是通过将学习率降低到预定义的时间表来调整训练阶段的学习率，我们在AdaGrad、RMSprop、Adam和AdaDelta中可以看到这种时间表——这也称为学习率时间表，关于这个主题的更多细节<a class="nq nr ep" href="https://medium.com/u/3e95520ee2b9?source=post_page-----1cdc9b12724a--------------------------------" rel="noopener" target="_blank">淑熙·刘</a>写了一篇关于这个主题的非常翔实的博客文章，名为<a class="ae ky" rel="noopener" target="_blank" href="/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1">深度学习的学习率时间表和自适应学习率方法</a>。</p><blockquote class="ns nt nu"><p id="c9c6" class="kz la no lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated">在不深入AdaGrad优化算法的情况下，我将解释RMSprop以及它如何改进AdaGrad，以及它如何随着时间的推移改变学习速率。</p></blockquote><p id="032a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">RMSprop，即均方根传播，由Geoff Hinton开发，如梯度下降优化算法概述中所述，其目的是解决AdaGrad的学习率急剧下降的问题。简而言之，RMSprop改变学习速率的速度比AdaGrad慢，但AdaGrad的好处(更快的收敛)仍然可以从RMSprop中获得，数学表达式见图3。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/3fa9c6392a758238a93c9048d50c1a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YiQFPfkWFi_9cAwXWE99Ng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:E[g]t的第一个方程是梯度平方的指数衰减平均值。Geoff Hinton建议将γ设置为0.9，而学习率η的默认值为0.001。</p></figure><p id="e960" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这使得学习率能够随着时间的推移而适应，理解这一点很重要，因为这种现象在Adam中也存在。当我们将两者(Momentum和RMSprop)放在一起时，我们得到Adam——图4显示了详细的算法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/b9c943f9caa9068463a3d61e18741d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KpvdPgxTIQ2Wy6rKoYrVpg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:随机优化的Adam算法；来源于Kingma，D和Ba，J . (2015) Adam:一种随机优化方法。可在:【https://arxiv.org/pdf/1412.6980.pdf T2】(访问时间:2020年5月31日)。</p></figure><p id="49fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您阅读到这里，下面将链接更多阅读材料，如果您想与我联系，您可以在LinkedIn上找到我，我的名字是<a class="ae ky" href="https://www.linkedin.com/in/kurtispykes/" rel="noopener ugc nofollow" target="_blank"> Kurtis Pykes </a>(点击我的名字可直接访问)。</p><p id="f286" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">进一步阅读:</strong></p><ul class=""><li id="2551" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">金玛，D和巴，J . (2015) <em class="no">亚当:一种随机优化的方法</em>。可在:<a class="ae ky" href="https://arxiv.org/pdf/1412.6980.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1412.6980.pdf</a></li><li id="708e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">Ruder，S. (2017) <em class="no">梯度下降优化算法概述</em>。可在:<a class="ae ky" href="https://arxiv.org/pdf/1609.04747.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1609.04747.pdf</a></li><li id="99c8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">用于在线学习和随机优化的自适应次梯度方法。可在:<a class="ae ky" href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" rel="noopener ugc nofollow" target="_blank">http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a></li></ul><p id="6021" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">有用视频:</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure></div></div>    
</body>
</html>