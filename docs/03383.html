<html>
<head>
<title>Decision Tree Fundamentals</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树基础</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-fundamentals-388f57a60d2a?source=collection_archive---------5-----------------------#2020-03-31">https://towardsdatascience.com/decision-tree-fundamentals-388f57a60d2a?source=collection_archive---------5-----------------------#2020-03-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="778d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习基尼系数、熵以及如何构建决策树</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ae45279cdc9e9de56d48b3dfdbcf71e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHFJl1o0uAWgaVhGoEwxNQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com/s/photos/tree?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae ky" href="https://unsplash.com/@veeterzy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> veeterzy </a>拍摄的照片</p></figure><p id="d731" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">当谈到决策树的时候，我总是想象当我的女朋友不知道她晚饭想吃什么的时候，我会问她一系列的问题:你想吃点面条吗？你想花多少？亚洲人还是西方人？健康还是垃圾食品？</p><p id="f879" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">制作一个问题列表来缩小选择范围，本质上是决策树背后的思想。更正式地说，决策树是一种算法，它根据观察值的特征将它们划分为相似的数据点。</p><p id="ffbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树是一种监督学习模型，具有树状结构，即包含<strong class="lb iu">根</strong>、<strong class="lb iu">父/子</strong>节点、<strong class="lb iu">叶</strong>。决策树可用于<strong class="lb iu">分类</strong>或<strong class="lb iu">回归</strong>问题。下图是我用来判断她要不要快餐的二叉分类树的简短版本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi me"><img src="../Images/66fdc38e7f1871703fb116d524f94f6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dc_342kIsHCzuko1TtyEGQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">快餐还是非快餐决策树</p></figure><h1 id="06e6" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">I)如何构造决策树？</h1><p id="ff49" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">有两种流行的树构建算法:<strong class="lb iu">分类和回归树(CART) </strong>和<strong class="lb iu"> ID3 </strong>。这两个模型之间的主要区别是它们使用的成本函数。<strong class="lb iu">成本函数</strong>决定问哪个问题以及如何分割每个节点。构建决策树的伪代码是:</p><ol class=""><li id="02d4" class="nc nd it lb b lc ld lf lg li ne lm nf lq ng lu nh ni nj nk bi translated">选择一个具有<strong class="lb iu">最佳索引的特征。使用我将在下一节介绍的成本函数来计算指数</strong></li><li id="ee64" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">基于所选要素分割数据集</li><li id="e28d" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">重复这个过程，直到它到达叶子(或满足停止标准)</li></ol><p id="6b17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将讨论两个成本函数:基尼系数<strong class="lb iu">和熵值</strong></p><h1 id="be5a" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">(二)基尼系数和基尼系数</h1><h2 id="4a02" class="nq mg it bd mh nr ns dn ml nt nu dp mp li nv nw mr lm nx ny mt lq nz oa mv ob bi translated">1)基尼不纯</h2><p id="f1e1" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated"><strong class="lb iu">基尼系数</strong>是在<strong class="lb iu"> CART </strong>方法中使用的损失函数。基尼系数衡量一个类别有多少噪音<strong class="lb iu">。</strong>首先，每个数据<strong class="lb iu">特征</strong>可能包含许多<strong class="lb iu">类别</strong>。例如，<em class="oc">天气</em>特征可以有类别:下雨、晴天或下雪；一个数字特征如<em class="oc">等级</em>可分为两个块:&lt; 70或≥70。基尼系数可以通过以下公式计算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/b016469b6a082a5148c685db52113a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gc1d1Sf8F7NoXAEnRlitbg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基尼杂质公式</p></figure><p id="2438" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意最大基尼系数是0.5。这可以用一些微积分知识来检验。我创建了一个玩具数据集，以更好地说明成本函数对决策树的影响。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="fd33" class="nq mg it of b gy oj ok l ol om">import pandas as pd<br/>classification=['Freshman','Freshman','Sophomore','Junior','Freshman','Sophomore']<br/>hour_of_practice=['&gt;2h','&gt;2h','&gt;2h','&lt;2h','&gt;2h','&lt;2h']<br/>pass_the_quiz=['Yes','Yes','Yes', 'Yes', 'No','No']<br/>df=pd.DataFrame({'Classification':classification, <br/>                'hour of practice':hour_of_practice, <br/>                "Pass the quiz":pass_the_quiz })<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/50fbe12dcea0f173b8cb88926a90df3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fn-1rkFphbU4nFHcpLV7fA.png"/></div></div></figure><p id="35e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">观察:</strong>大一(F)3人，大二(S)2人，大三(J)1人。从数据来看，大一大二学生学习2h以上通过测试，学习2h以下不及格。学弟骗系统，一直过关。</p><p id="1d1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们计算<em class="oc">分类</em>栏的基尼系数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/ba71a013d1fb038516f9f76e0aabd637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-hnfRtiUyNNGc1HMQQ7Dw.png"/></div></div></figure><p id="6aca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基尼值告诉我们数据集中存在的噪声值。在这种情况下，初级<em class="oc">没有噪声，因为我们知道所有初级都将通过测试。另一方面，大二学生</em>的噪音最大。</p><h2 id="3609" class="nq mg it bd mh nr ns dn ml nt nu dp mp li nv nw mr lm nx ny mt lq nz oa mv ob bi translated">2)基尼指数</h2><p id="a5e4" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated"><strong class="lb iu">基尼指数</strong>将类别噪声结合在一起得到<strong class="lb iu">特征噪声</strong>。基尼指数是基尼系数的加权总和，基于该类别在特征中的相应分数。公式是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/33e5904a335d76e7095633583b439f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iz7pbBjeKpNTgWfmOCTgtg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基尼指数公式</p></figure><p id="f0b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将基尼指数应用于<em class="oc">分类</em>我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/a8932ba81dac9f9611a6c745f012a1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i4VDhee-Pck9jpphTofN-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征“分类”的基尼指数</p></figure><p id="d084" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你应该能够手工计算出学习时间的基尼指数。为了多样化，我创建了下面的代码来计算基尼系数和基尼系数:</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="9417" class="nq mg it of b gy oj ok l ol om"># Input Format(df, feature name, category name ,target name, list of all classes)<br/># where df is the current node observation</span><span id="295a" class="nq mg it of b gy or ok l ol om">def gini(df,feature,category,target,classes_list):<br/>    df=df[df[feature]==category]<br/>    def P_i_K(i):<br/>        return len(df[df[target]==i])/len(df)<br/>        <br/>    result=1-sum([(P_i_K(i))**2 for i in classes_list])<br/>    return result</span><span id="5ec9" class="nq mg it of b gy or ok l ol om"># Input Format(df, feature name, target name, list of all classes)</span><span id="47f9" class="nq mg it of b gy or ok l ol om">def Gini_index(df,feature,target,classes_list):<br/>    def P_k_a(category):<br/>        return len(df[df[feature]==category])/len(df)<br/>    <br/>    result=0<br/>    for category in df[feature].unique():<br/>        gini_value=gini(df,feature,category,target,classes_list) <br/>        P_k_a_value=P_k_a(category)<br/>        result+=gini_value*P_k_a_value</span><span id="13c9" class="nq mg it of b gy or ok l ol om">return result</span><span id="10b3" class="nq mg it of b gy or ok l ol om">print("Gini Index of Classification",<br/>      Gini_index(df,"Classification","Pass the quiz",['Yes','No']))<br/>print("Gini Index of hour of practice", <br/>      Gini_index(df,"hour of practice","Pass the quiz",['Yes','No']))</span><span id="ec77" class="nq mg it of b gy or ok l ol om">&gt;&gt;&gt; Gini Index of Classification 0.38888888888888884<br/>&gt;&gt;&gt; Gini Index of hour of practice 0.41666666666666663</span></pre><p id="40cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于<em class="oc">分类</em>比<em class="oc">小时练习</em>噪音小，所以第一次分割针对<em class="oc">分类</em>特征<em class="oc">。在讨论完熵的概念后，我们将把树形象化。</em></p><h1 id="37b0" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">III)熵和信息增益</h1><h2 id="00fd" class="nq mg it bd mh nr ns dn ml nt nu dp mp li nv nw mr lm nx ny mt lq nz oa mv ob bi translated">1)熵</h2><p id="f501" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在物理学中，熵代表随机变量的不可预测性。公平硬币中出现头<em class="oc">或尾</em>或<em class="oc">尾</em>的几率为50/50，因此其熵值为1，这是随机性的最高值。另一方面，具有值0表示相应的事件是命中注定的。熵与基尼系数相似，都表明了类别的不可预测性。熵的公式是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/c3449452e75e848a885b00f6674a8f69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gY-jHFkNz6uOJ_EOTiBq4g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">熵公式</p></figure><p id="800d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将此公式应用于大一、大二和大三学生，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/4b9eb6a03b29aca3dc4fc3eef7031691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SSYNKDypa0PZgUZsEX9tSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征“分类”的熵</p></figure><p id="eabb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，二次熵的行为就像一枚公平的硬币，所以它有最高的价值。新生的值非常接近1，因为它的类是不平衡的。</p><h2 id="9e71" class="nq mg it bd mh nr ns dn ml nt nu dp mp li nv nw mr lm nx ny mt lq nz oa mv ob bi translated">信息增益</h2><p id="43ba" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在获得每个类别的熵之后，我们可以将它们组合起来，以获得特征的<strong class="lb iu">信息增益</strong>值。我们获得的信息越多越好。公式是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/86083f4933f533e5a2d361145c31a853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dRbCpuzFzXnMoMvFUeTWhA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">信息增益公式</p></figure><p id="184f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将此公式应用于我们的“分类”功能会产生:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/b5b306097091cbda7102d9765d4e1757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E5a4GMo9zzdmrpUpwZWJbw.png"/></div></div></figure><p id="d1cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我还提供了计算熵和信息增益的代码:</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="b932" class="nq mg it of b gy oj ok l ol om"># Input Format(df,feature name K,category name,target name, list of all classes)<br/># Pass feature, category None if want to find the entropy of the whole observation<br/>import math<br/>def entropy(df,feature,category,target,classes_list):<br/>    <br/>    if (feature!=None)|(category!=None):<br/>        df=df[df[feature]==category]<br/>        <br/>    def P_i_k(class_):<br/>        return len(df[df[target]==class_])/len(df)<br/>    <br/>    result=0<br/>    for class_ in classes_list:<br/>        P_i_k_value=P_i_k(class_)<br/>        if P_i_k_value!=0:<br/>            result+=P_i_k_value*math.log(P_i_k_value,2)<br/>    return -1*result</span><span id="91e3" class="nq mg it of b gy or ok l ol om"># Input Format(df,feature name K,category name,target name,list of all classes)<br/>def InfoGain(df,feature,target,classes_list):<br/>    H_T=entropy(df,None,None,target,classes_list)    <br/>    def P_i_a(category,feature):<br/>        return len(df[df[feature]==category])/len(df)<br/>    <br/>    result=0<br/>    <br/>    for category in df[feature].unique():<br/>        result+=P_i_a(category, feature)*entropy(df,feature,category,target,classes_list)<br/>    result=H_T-result<br/>    return result</span><span id="7adb" class="nq mg it of b gy or ok l ol om">print("Information Gain of Classification", <br/>      InfoGain(df,'Classification','Pass the quiz',['Yes','No']))<br/>print("Information Gain of hour of practice", <br/>      InfoGain(df,'hour of practice','Pass the quiz',['Yes','No']))</span><span id="194d" class="nq mg it of b gy or ok l ol om">&gt;&gt;&gt;Information Gain of Classification 0.12581458369391152<br/>&gt;&gt;&gt;Information Gain of hour of practice 0.044110417748401076</span></pre><p id="01dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，<em class="oc">分类</em>有更多的信息，所以这将是我们的第一次分裂。</p><h1 id="bc0f" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">IV)可视化树</h1><p id="ac30" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在这两种方法中，<em class="oc">分类</em>是首选。第二个分割是剩余的特性，<em class="oc"/><em class="oc">小时的学习</em>，我们得到下面的树:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/cfac285f61a3a897f2fae047f93aeb06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ss_MwsysjX8tQkigsXFLwg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们数据集的决策树</p></figure><p id="1205" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，这个树是极端有偏差的，因为数据集只有6个观察值。真实数据上的决策树要大得多，复杂得多。</p><h1 id="bc47" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">v)停止分裂树的标准</h1><p id="5127" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">随着数据变得越来越复杂，决策树也在扩展。尽管如此，如果我们保持树的增长，直到所有的训练数据被分类，我们的模型将会过度拟合。因此，学会何时停止是非常重要的。让我们来看看一些常用的标准:</p><p id="da67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">节点中的观测值数量:</strong>理想的上限是总训练数据集的5%。</p><p id="deac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">节点的纯度:</strong>基尼指数显示当前数据集的每个特征有多少噪声，然后选择噪声最小的特征来应用递归。我们可以在这里设置基尼系数的最大值作为停止标准，通知分行是时候做决定了。</p><p id="4b09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">树的深度:</strong>我们可以预先指定深度的限制，这样树在面对复杂数据集时不会过度膨胀。</p><p id="a563" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">最大特征</strong>:由于树是按特征分割的，减少特征的数量将导致树的大小减小。只有选择具有高信息增益或低基尼系数的特征才是好主意。</p><h1 id="d86b" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated"><strong class="ak">六)结论</strong></h1><p id="c643" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">当我们看到树是如何构造的以及它是如何调整的，我们可以得出一些关于决策树的结论:</p><ul class=""><li id="4217" class="nc nd it lb b lc ld lf lg li ne lm nf lq ng lu ox ni nj nk bi translated"><strong class="lb iu">很容易解释</strong>。决策树类似于人类做决策的方式。因此，决策树是一个简单的模型，可以为企业带来巨大的机器学习透明度。</li><li id="4b38" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu ox ni nj nk bi translated"><strong class="lb iu">它不需要缩放/标准化数据</strong>，因为没有涉及使用数据值的计算。</li><li id="75c0" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu ox ni nj nk bi translated"><strong class="lb iu">未必创造出最好的树</strong>。原因是决策树背后的构造只关注每个节点的最优特征。不能保证最终结果是最优解。</li><li id="498a" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu ox ni nj nk bi translated"><strong class="lb iu">也许过度拟合</strong>。很难阻止树的子样本停留在某些节点上。拥有一个从特定群体中学习的树会导致过度拟合。</li></ul><p id="1cbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以访问我的<a class="ae ky" href="https://github.com/williamhuybui/Blog-Decision-Tree-Fundamental" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>下载代码。感谢阅读！</p></div></div>    
</body>
</html>