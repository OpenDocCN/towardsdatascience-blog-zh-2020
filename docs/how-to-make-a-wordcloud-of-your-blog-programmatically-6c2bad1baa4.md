# å¦‚ä½•ä»¥ç¼–ç¨‹çš„æ–¹å¼ä¸ºä½ çš„åšå®¢åˆ¶ä½œæ–‡å­—äº‘ï¼Ÿ

> åŸæ–‡ï¼š<https://towardsdatascience.com/how-to-make-a-wordcloud-of-your-blog-programmatically-6c2bad1baa4?source=collection_archive---------39----------------------->

## Python ä¸­çš„æ–‡æœ¬è¯­æ–™åº“å¯è§†åŒ–å·¥å…·

æœ€è¿‘ï¼Œæˆ‘éœ€è¦ä¸€å¼ æˆ‘ä»¬åšå®¢çš„å›¾ç‰‡ï¼Œå¸Œæœ›å®ƒæœ‰ä¸€äº›*å“‡æ•ˆæœ*ï¼Œæˆ–è€…è‡³å°‘æœ‰ä¸€ä¸ª*æ¯”æˆ‘ä»¬ä¸€ç›´ä½¿ç”¨çš„ä»»ä½•å…¸å‹çš„*æ›´é€‚åˆ*ã€‚*æ€ç´¢äº†ä¸€ä¼šå„¿ï¼Œäº‘å­—åœ¨è„‘æµ·ä¸­é—ªè¿‡ã€‚ğŸ’¡é€šå¸¸ï¼Œä½ åªéœ€è¦ä¸€é•¿ä¸²æ–‡æœ¬å°±å¯ä»¥ç”Ÿæˆä¸€ä¸ªï¼Œä½†æˆ‘æƒ³åˆ°äº†è§£ææˆ‘ä»¬çš„æ•´ä¸ªåšå®¢æ•°æ®ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ä»€ä¹ˆæœ‰è¶£çš„ä¸œè¥¿å†’å‡ºæ¥ï¼Œå¹¶è·å¾—æˆ‘ä»¬åšå®¢ä½¿ç”¨çš„å…³é”®è¯çš„æ•´ä½“è§†å›¾ã€‚æ‰€ä»¥ï¼Œæˆ‘æŠŠè¿™ä¸ªä½œä¸ºè‡ªå·±çš„å‘¨æœ«è¶£å‘³é¡¹ç›®ã€‚

> PS:å½¢è±¡åœ¨è¥é”€ä¸­éå¸¸é‡è¦ã€‚ç»™å®ƒè´¨é‡ï¼ğŸ‘€

# å¼„è„ä½ çš„æ‰‹:

æˆ‘ä»¬çš„åšå®¢æ‰˜ç®¡åœ¨***Ghost****ä¸Šï¼Œå®ƒå…è®¸æˆ‘ä»¬å°†æ‰€æœ‰çš„å¸–å­å’Œè®¾ç½®å¯¼å‡ºåˆ°ä¸€ä¸ªå•ç‹¬çš„ã€ç²¾å½©çš„ JSON æ–‡ä»¶ä¸­ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬åœ¨ python ä¸­å†…ç½®äº† *json* åŒ…ï¼Œç”¨äºè§£æ json æ•°æ®ã€‚æˆ‘ä»¬çš„èˆå°å·²ç»å‡†å¤‡å¥½äº†ã€‚ğŸ¤*

*å¯¹äºå…¶ä»–æµè¡Œçš„å¹³å°ï¼Œå¦‚ WordPressï¼ŒBloggerï¼ŒSubstack ç­‰ã€‚ å®ƒå¯èƒ½æ˜¯ä¸€ä¸ªæˆ–å¤šä¸ª XML æ–‡ä»¶ï¼Œä½ å¯èƒ½éœ€è¦ç›¸åº”åœ°åˆ‡æ¢åŒ…å¹¶åœ¨ python ä¸­åšåŸºç¡€å·¥ä½œã€‚*

*åœ¨é˜…è¯» python ä¸­çš„ JSON ä¹‹å‰ï¼Œæ‚¨åº”è¯¥äº†è§£å®ƒçš„ç»“æ„ã€éœ€è¦é˜…è¯»çš„å†…å®¹ã€éœ€è¦è¿‡æ»¤çš„å†…å®¹ç­‰ç­‰ã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨ä¸€äº› json å¤„ç†å™¨æ¥æ¼‚äº®åœ°æ‰“å°ä½ çš„ JSON æ–‡ä»¶ï¼Œæˆ‘ä½¿ç”¨äº†[jqplay.org](https://www.jqplay.org)ï¼Œå®ƒå¸®åŠ©æˆ‘æ‰¾å‡ºæˆ‘çš„å¸–å­ä½äºâ¡
t0ã€‘*

*æ¥ä¸‹æ¥ï¼Œæ‚¨æƒ³è°ƒç”¨`pd.json.normalize()`å°†æ‚¨çš„æ•°æ®è½¬æ¢æˆä¸€ä¸ªå¹³é¢è¡¨ï¼Œå¹¶ä¿å­˜ä¸ºä¸€ä¸ªæ•°æ®æ¡†ã€‚*

> **ğŸ‘‰æ³¨æ„:ä½ åº”è¯¥å·²ç»ä¸º`*pd.json.normalize()*`å®‰è£…äº†ç†ŠçŒ«çš„æ›´æ–°ç‰ˆæœ¬ï¼Œå› ä¸ºå®ƒå·²ç»ä¿®æ”¹äº†æ—§ç‰ˆæœ¬ä¸­çš„åå­—ã€‚å¦å¤–ï¼Œä¿æŒç¼–ç ä¸º UTF-8ï¼Œå¦åˆ™ï¼Œä½ å¾ˆå¯èƒ½ä¼šé‡åˆ° UnicodeDecodeErrorsã€‚(æˆ‘ä»¬æœ‰è¿™äº›åäºº:' \xa0 'ã€' \n 'å’Œ' \t 'ç­‰ã€‚)**

```
*import pandas as pd
import jsonwith open('fleetx.ghost.2020-07-28-20-18-49.json', encoding='utf-8') as file: 
    data = json.load(file)  
posts_df = pd.json_normalize(data['db'][0]['data']['posts']) 
posts_df.head()*
```

*![](img/2ee82cbb87703df1260d3697b9a7108d.png)*

*å‘å¸ƒæ•°æ®æ¡†æ¶*

*æŸ¥çœ‹æ•°æ®å¸§ï¼Œæ‚¨å¯ä»¥çœ‹åˆ° ***ghost*** ä¿ç•™äº†æˆ‘ä»¬åˆ›å»ºçš„å¸–å­çš„ä¸‰ç§æ ¼å¼ï¼Œ [mobiledoc](https://github.com/bustle/mobiledoc-kit/blob/master/MOBILEDOC.md) (æ²¡æœ‰ HTML è§£æå™¨çš„ç®€å•å¿«é€Ÿå‘ˆç°å™¨)ã€HTML å’Œæ˜æ–‡ï¼Œä»¥åŠå¸–å­çš„å…¶ä»–å±æ€§èŒƒå›´ã€‚æˆ‘é€‰æ‹©ä½¿ç”¨æ˜æ–‡ç‰ˆæœ¬ï¼Œå› ä¸ºå®ƒéœ€è¦æœ€å°‘çš„æ¸…ç†ã€‚*

## *æ¸…æ´å·¥ä½œ:*

*   *åˆ é™¤ä¸¢å¤±çš„å€¼(ä½ å¯èƒ½æœ‰çš„ä»»ä½•ç©ºç™½å¸–å­),ä»¥å…åœ¨ä»¥åç»˜åˆ¶å›¾è¡¨æ—¶å¦¨ç¢ä½ çš„åˆ†æã€‚æˆ‘ä»¬æœ‰ä¸€ç¯‡è‰ç¨¿ä¸­çš„åšå®¢æ–‡ç« ï¼Œé‡Œé¢ä»€ä¹ˆä¹Ÿæ²¡æœ‰ã€‚ğŸ¤·â€â™‚ï¸*
*   *å¸–å­ä¸­çš„ ***æ˜æ–‡*** å‡ ä¹åŒ…å«äº†æ‰€æœ‰å¯èƒ½ä¸éœ€è¦çš„å­—ç¬¦ï¼Œä»ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦ *(\nï¼Œ\xaoï¼Œ\t)* ï¼Œåˆ°è¯­æ³•æ ‡ç‚¹ç¬¦å·(ç‚¹ã€é€—å·ã€åˆ†å·ã€å†’å·ã€ç ´æŠ˜å·ã€è¿å­—ç¬¦ã€s ç­‰)çš„ 14 ä¸ªæ ‡è®°ã€‚)ç”šè‡³æ˜¯è¦ç‚¹ã€‚ç”¨ç©ºæ ¼æ›¿æ¢å®ƒä»¬ã€‚*
*   *æ¥ä¸‹æ¥ï¼Œæˆ‘å°†æ¯ç¯‡åšæ–‡ä¸­çš„å•è¯æ‹†åˆ†åˆ° ***æ˜æ–‡*** åˆ—ä¸‹ï¼Œç„¶åå°†æ¯ä¸ªå•å…ƒæ ¼çš„ç»“æœåˆ—è¡¨è¿æ¥èµ·æ¥ï¼Œå¾—åˆ°ä¸€ä¸ªå¾ˆé•¿çš„å•è¯åˆ—è¡¨ã€‚è¿™å¯¼è‡´äº† 34000 å­—ï¼›æˆ‘ä»¬æœ‰å¤§çº¦ 45 ç¯‡å·²å‘è¡¨çš„åšå®¢ï¼Œæ¯ç¯‡å¹³å‡ 700 å­—ï¼Œè¿˜æœ‰ä¸€äº›è‰ç¨¿ï¼Œæ‰€ä»¥ç®—å‡ºæ¥æ˜¯ 45*700=31500 å­—ã€‚ä¸€è‡´ï¼ğŸ¤œ*

```
*posts_df.dropna(subset=['plaintext'], axis=0, inplace=True)posts_df.plaintext = posts_df.plaintext.str.replace('\n', ' ')
.str.replace('\xa0',' ').str.replace('.',' ').str.replace('Â·', ' ')
.str.replace('â€¢',' ').str.replace('\t', ' ').str.replace(',',' ')
.str.replace('-', ' ').str.replace(':', ' ').str.replace('/',' ')
.str.replace('*',' ')posts_df.plaintext = posts_df.plaintext.apply(lambda x: x.split())
words_list =[]
for i in range(0,posts_df.shape[0]):
    words_list.extend(posts_df.iloc[i].plaintext)*
```

*å¦‚æœä½ ç°åœ¨å°±æ¸´æœ›å¾—åˆ°ç»“æœï¼Œä½ å¯ä»¥åœ¨é‚£ä¸ª`words_list`ä¸Šè¿è¡Œ`collections.Counter`ï¼Œå¹¶è·å¾—æ¯ä¸ªè¯çš„é¢‘ç‡ï¼Œä»è€Œäº†è§£ä½ çš„è¯äº‘å¯èƒ½æ˜¯ä»€ä¹ˆæ ·å­ã€‚*

```
*import collectionsword_freq = collections.Counter(words_list)
word_freq.most_common(200)*
```

*çŒœçŒœåšå®¢ä¸­æœ€å¸¸ç”¨çš„è¯æ˜¯ä»€ä¹ˆï¼ŸğŸ¤
å¦‚æœä½ è¯´***ã€theã€‘***ä½ å°±å¯¹äº†ã€‚å¯¹äºéå¸¸é•¿çš„æ–‡æœ¬ï¼Œå† è¯***â€˜theâ€™***å°†ä¼˜å…ˆäºä»»ä½•å…¶ä»–å•è¯ã€‚è€Œä¸”ï¼Œä¸ä»…ä»…æ˜¯â€œthe â€,è¿˜æœ‰å…¶ä»–å‡ ä¸ªä»‹è¯ã€ä»£è¯ã€è¿è¯å’ŒåŠ¨ä½œåŠ¨è¯å‡ºç°åœ¨æœ€é«˜é¢‘ç‡åˆ—è¡¨ä¸­ã€‚æˆ‘ä»¬å½“ç„¶ä¸éœ€è¦å®ƒä»¬ï¼Œè¦ç§»é™¤å®ƒä»¬ï¼Œæˆ‘ä»¬å¿…é¡»é¦–å…ˆå®šä¹‰å®ƒä»¬ã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å°†ç”¨æ¥ç”Ÿæˆ wordcloud çš„åº“è‡ªå¸¦äº†é»˜è®¤çš„åœç”¨è¯ï¼Œä½†å®ƒç›¸å½“ä¿å®ˆï¼Œåªæœ‰ 192 ä¸ªè¯ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­çš„åº“ï¼Œå®ƒä»¬å¤„ç†å¤§é‡çš„æ–‡æœ¬å¹¶è‡´åŠ›äºæ­¤ç±»ä»»åŠ¡ã€‚ğŸ”*

*   *å›½å®¶è¯­è¨€å·¥å…·åŒ…åº“(NLTK):å®ƒæœ‰ 179 ä¸ªåœç”¨è¯ï¼Œç”šè‡³æ¯” wordcloud åœç”¨è¯åº“è¿˜å°‘ã€‚ä¸è¦ä»…ä»…å› ä¸ºè¿™ä¸ªåŸå› å°±ç»™å®ƒä¸€ä¸ªé‚ªæ¶çš„çœ¼ç¥ï¼Œè¿™æ˜¯ python ä¸­é¢†å…ˆçš„ NLP åº“ã€‚*
*   *Genism:å®ƒçš„é›†åˆä¸­æœ‰ 337 ä¸ªåœç”¨è¯ã€‚*
*   *Sci-kit learn:ä»–ä»¬è¿˜æœ‰ä¸€ä¸ª 318 ä¸ªå•è¯çš„åœç”¨è¯åº“ã€‚*
*   *æ­¤å¤–ï¼Œè¿˜æœ‰ Spacy:å®ƒæœ‰ 326 ä¸ªåœç”¨è¯ã€‚*

*æˆ‘é€‰æ‹©äº†ç©ºé—´ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½é€‰æ‹©ã€‚*

## *ä½†æ˜¯â€¦ğŸ˜“*

*è¿™è¿˜ä¸å¤Ÿï¼ä¸è¿‡ï¼Œä»è¥é”€çš„è§’åº¦æ¥çœ‹ï¼Œæœ‰äº›è¯çœ‹èµ·æ¥ä¸å¤ªå¥½ï¼Œè€Œä¸”æˆ‘ä»¬ä¹Ÿæ²¡æœ‰å°½å¯èƒ½åœ°åšå¥½æ¸…ç†å·¥ä½œã€‚å› æ­¤ï¼Œæˆ‘å°†å®ƒä»¬æ”¾åœ¨ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­(æ¯ä¸ªå•è¯å ä¸€è¡Œ)ï¼Œç„¶åè¯»å–å®ƒï¼Œå¹¶åŠ å…¥ spacy çš„åœç”¨è¯è¡¨ã€‚*

> *[è®¾ç½®ç©ºé—´çš„è¯´æ˜](https://spacy.io/usage/models#quickstart)ã€‚*

```
*import spacynlp=spacy.load('en_core_web_sm')
spacy_stopwords = nlp.Defaults.stop_wordswith open("more stopwords.txt") as file:
    more_stopwords = {line.rstrip() for line in file}final_stopwords = spacy_stopwords | more_stopwords*
```

## *è®¾ç«‹è®¾è®¡å®¤:*

*ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†é‡æ–°è®¾è®¡çš„åœç”¨è¯è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ç¥å¥‡çš„åˆ¶é€ è€…â¡çš„å•è¯äº‘åŠŸèƒ½äº†ã€‚é€šè¿‡ Jupyter/CLI/Conda ä½¿ç”¨ pip å‘½ä»¤å®‰è£… wordcloud åº“ã€‚*

```
*pip install wordcloudimport matplotlib.pyplot as plt
import wordcloud#Instantiate the wordcloud objectwc = wordcloud.WordCloud(background_color='white', max_words=300, stopwords=final_stopwords, collocations=False, max_font_size=40, random_state=42)# Generate word cloud
wc=wc.generate(" ".join(words_list).lower())# Show word cloud
plt.figure(figsize=(20,15))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()# save the wordcloud
wc.to_file('wordcloud.png');*
```

*å¯¹äº python ç”¨æˆ·æ¥è¯´ï¼Œä¸Šè¿°ä»£ç å—çš„å¤§éƒ¨åˆ†å†…å®¹éƒ½æ˜¯ä¸è¨€è‡ªæ˜çš„ï¼Œä¸è¿‡è®©æˆ‘ä»¬åšä¸€ä¸ªç®€çŸ­çš„ä»‹ç»:*

*   *`background_color`:ä½ çš„æ–‡å­—äº‘çš„èƒŒæ™¯ï¼Œ*é»‘*ç™½*æœ€å¸¸è§ã€‚**
*   *`max_words`:ä½ æƒ³åœ¨ wordcloud ä¸­æ˜¾ç¤ºçš„æœ€å¤§å­—æ•°ï¼Œé»˜è®¤ä¸º 200 ä¸ªã€‚*
*   *`stopwords`:è¦ä» wordcloud ä¸­åˆ é™¤çš„åœç”¨è¯é›†åˆã€‚*
*   *`collocations`:æ˜¯å¦åŒ…å«ä¸¤ä¸ªè¯çš„æ­é…(äºŒå…ƒç»„)ï¼Œé»˜è®¤ä¸ºçœŸã€‚*

## *ä»€ä¹ˆæ˜¯äºŒå…ƒæ¨¡å‹ï¼Ÿ*

*è¿™æ˜¯ä¸¤ä¸ªç›¸é‚»å•è¯çš„åºåˆ—ã€‚çœ‹çœ‹ä¸‹é¢çš„ä¾‹å­ã€‚*

*![](img/f70fb0b148f9b272cf9763659cfb2fe6.png)*

*å¥å­çš„äºŒå…ƒç»“æ„*

> *æ³¨æ„:å°†æ‰€æœ‰æ–‡æœ¬ä»¥å°å†™è§£æåˆ° wordcloud generatorï¼Œå› ä¸ºæ‰€æœ‰åœç”¨è¯éƒ½æ˜¯ä»¥å°å†™å®šä¹‰çš„ã€‚å®ƒä¸ä¼šæ¶ˆé™¤å¤§å†™åœç”¨è¯ã€‚*

*å¥½çš„ï¼Œè¾“å‡ºæ˜¯è¿™æ ·çš„:*

*![](img/d6a57c99c0165c5e40567c6d42c6b826.png)*

*åšå®¢æ•°æ®çš„æ–‡å­—äº‘*

*å¯¹äºä¸€ä¸ªåšè½¦é˜Ÿç®¡ç†çš„å…¬å¸æ¥è¯´ï¼Œè¿™æ˜¯æ­£ç¡®çš„ï¼è½¦é˜Ÿç®¡ç†è¿™ä¸ªå…³é”®è¯æ¯”ä»€ä¹ˆéƒ½æœ‰åˆ†é‡ã€‚*

*è™½ç„¶ï¼Œä¸Šé¢çš„å›¾ç‰‡é”™è¿‡äº†æ‰€æœ‰è¿™ä¸€åˆ‡çš„å…ƒç´ :è½¦è¾†ã€‚å¹¸è¿çš„æ˜¯ï¼Œä½ å¯ä»¥ç”¨ wordcloud åº“åœ¨ä½ é€‰æ‹©çš„å›¾ç‰‡ä¸Šå±è”½ä¸Šé¢çš„ wordcloudã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬å¼€å§‹å§ã€‚*

*   *é€‰æ‹©æ‚¨é€‰æ‹©çš„çŸ¢é‡å›¾åƒã€‚æˆ‘ä» [Vecteezy](https://www.vecteezy.com/vector-art/329166-truck-illustration-isolated-on-a-white-background) ä¸­æŒ‘é€‰äº†æˆ‘çš„å›¾ç‰‡ã€‚
    è¿™æ¬¡æ‚¨è¿˜éœ€è¦å¯¼å…¥ *Pillow* å’Œ *NumPy* åº“æ¥è¯»å–å›¾åƒå¹¶å°†å…¶è½¬æ¢ä¸º NumPy æ•°ç»„ã€‚*
*   *ä¸‹é¢æ˜¯ç”Ÿæˆå±è”½çš„ wordcloud çš„æ³¨é‡Šä»£ç å—ï¼Œå¤§éƒ¨åˆ†å†…å®¹å’Œä»¥å‰ä¸€æ ·ã€‚*

```
*import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import os# Read your image and convert it to an numpy array.
truck_mask=np.array(Image.open("Truck2.png"))# Instantiate the word cloud object.
wc = wordcloud.WordCloud(background_color='white', max_words=500, stopwords=final_stopwords, mask= truck_mask, scale=3, width=640, height=480, collocations=False, contour_width=5, contour_color='steelblue')# Generate word cloud
wc=wc.generate(" ".join(words_list).lower())# Show word cloud
plt.figure(figsize=(18,12))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()# save the masked wordcloud
wc.to_file('masked_wsordcloud.png');*
```

## *ä»¥ä¸‹æ˜¯è¾“å‡ºç»“æœ:*

*![](img/e2a6f2425106dcc9abf9f323cc99c1a0.png)*

*ç§å•Šã€‚æˆ‘ä»¬ä»¥ç¼–ç¨‹çš„æ–¹å¼åˆ¶ä½œäº†æˆ‘ä»¬çš„ wordcloudï¼ğŸššğŸ’¨*

*è°¢è°¢ä½ è¯»åˆ°è¿™é‡Œï¼ğŸ™Œ*

## ***å‚è€ƒ:***

*   *[https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)*
*   *ã€https://nlp.stanford.edu/fsnlp/promo/colloc.pdfã€‘*