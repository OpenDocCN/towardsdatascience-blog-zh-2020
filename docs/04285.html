<html>
<head>
<title>Detecting Credit Card Fraud with Autoencoders in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python中的自动编码器检测信用卡欺诈</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/detecting-credit-card-fraud-with-autoencoders-in-python-98391cace8a3?source=collection_archive---------25-----------------------#2020-04-18">https://towardsdatascience.com/detecting-credit-card-fraud-with-autoencoders-in-python-98391cace8a3?source=collection_archive---------25-----------------------#2020-04-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="73db" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Tensorflow和Keras的两种方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d27f5d0e362f44265c3946b4ddc4992d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qQVZF1iaWGJTi7Di"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@claybanks?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">粘土银行</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="0dbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将演示两种不同的使用自动编码器的方法。特别是，我们将尝试使用自动编码器将信用卡交易分为欺诈性和非欺诈性。我们将要使用的数据集是“信用卡欺诈检测”数据集，可以在<a class="ae ky" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>中找到。完整代码可在<a class="ae ky" href="https://github.com/dpanagop/ML_and_AI_examples/blob/master/Credit_Fraud_detection_with_autoencoders.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。其中有一个在Colab中打开和执行代码的链接，所以请随意尝试。代码是用Python写的，用了Tensorflow和Keras。</p><p id="2b78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该数据集包含欧洲持卡人的284，807笔信用卡交易。出于安全原因，数据集的原始特征不可用。可用的是28个特征，它们是原始特征的PCA的结果。还有，每笔交易的金额和“时间”一栏。最后一个函数计算每个事务和集合中第一个事务之间的秒数。最后，每个交易的类型在“类”列中。欺诈交易用1表示，非欺诈交易用0表示。该数据集高度不平衡，非欺诈交易占总数的99.8%。因此，我们的分类问题也可以被视为异常值检测问题，欺诈性交易被视为异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/c0c8f78bd9d497844728ebb5901ee9fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*yW7t1be7wK7KeJ0zAGwtUw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">284，807笔交易中只有492笔是欺诈</p></figure><p id="a752" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们的例子，我们将忽略“时间”列。训练和测试数据集中的标准分割方法将用于评估每种方法。因为一个类的情况太少，我们将把数据集分成两半，而不是通常的70%-30%。</p><p id="6b0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如开始所述，我们将使用自动编码器来完成分类任务。根据<a class="ae ky" href="https://en.wikipedia.org/wiki/Autoencoder" rel="noopener ugc nofollow" target="_blank">维基百科</a>:</p><blockquote class="lw lx ly"><p id="d085" class="kz la lz lb b lc ld ju le lf lg jx lh ma lj lk ll mb ln lo lp mc lr ls lt lu im bi translated">自动编码器是一种人工神经网络，用于以无监督的方式学习有效的数据编码。</p></blockquote><p id="18f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，自动编码器是一个神经网络，它被训练来重新创建作为输入的任何内容。在输入层和输出层之间有一堆隐藏层。在正中间有一层，包含的神经元比输入少。这一层的输出是自动编码器的所谓编码器部分的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi md"><img src="../Images/665ad2f814e509349cfa8851f0425d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*wKE69-fX180Q_gkzYzGbwg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个自动编码器的表示(作者:<a class="ae ky" href="https://commons.wikimedia.org/w/index.php?title=User:Chervinskii&amp;action=edit&amp;redlink=1" rel="noopener ugc nofollow" target="_blank">切尔文斯基</a>来源:<a class="ae ky" href="https://en.wikipedia.org/wiki/File:Autoencoder_structure.png" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="299a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用自动编码器的理由是，隐藏层将以一种良好的方式将输入映射到向量空间(无论“良好”可能意味着什么)。通过使用具有很少神经元的层的结果，我们将我们的输入从高维空间映射到更低维的空间。</p><h2 id="52d2" class="me mf it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">第一种方法:使用重建误差</h2><p id="f232" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">我们的第一个方法是创建一个自动编码器，只在非欺诈交易上训练它。合乎逻辑的是，预期欺诈情况下该自动编码器的重构误差将高于非欺诈情况下的重构误差。我在Venelin Valkok的优秀媒体文章中读到过这种技术。我建议你学习它，因为在它里面，这个方法以一种非常详细和容易理解的方式被解释。</p><div class="nc nd gp gr ne nf"><a href="https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd" rel="noopener follow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">使用Keras中的自动编码器检测信用卡欺诈——面向黑客的TensorFlow(第七部分)</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">信用卡交易中的异常检测是如何工作的？</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">medium.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt ks nf"/></div></div></a></div><p id="dc4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于演示的原因，我们将创建一个简单的由三层组成的自动编码器。输入层、一个隐藏层和输出层。隐藏层将有12个神经元。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/02b318d2c142a7711aeba155059ee60f.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*Q18-lDLv5mwnd15t5E65_w.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第一种方法中使用的简单自动编码器的表示</p></figure><p id="2ee7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，网络仅使用训练集的非欺诈案例进行<strong class="lb iu">训练。在100个时期之后，我们获得了一个网络，我们用所有的训练集案例来喂养这个网络。然后，我们可以计算输入和输出之间的误差(重建误差)。结果如下表所示。</strong></p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="d3ed" class="me mf it nw b gy oa ob l oc od">+----------------+--------------------+--------------------+<br/>|     Class      | Mean Squared Error | Standard Deviation |<br/>+----------------+--------------------+--------------------+<br/>| Non-Fraudulent |      0.767519      |       3.439808     |<br/>|    Fraudulent  |     29.855354      |      43.107802     |<br/>+----------------+--------------------+--------------------+</span></pre><p id="0900" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于这些结果，在测试数据集中，如果实例的重构误差大于平均值的标准偏差的三倍，即大于0.767519+3*3.439808=11.078922，则我们将该实例描述为欺诈性实例。当然，阈值的选择是我们的模型的超参数，在实际应用中，它应该被微调。</p><p id="9d05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，我们的模型在测试数据集的243个(46.5%)欺诈案例中检测到了113个。此外，142161个非欺诈案件中有771个(0.5%)被归类为欺诈案件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/838bd0693e31e54a97856e57ef45cecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*vLUPIK-7IwmC9OTGKuBPpQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第一种方法的混淆矩阵</p></figure><h2 id="e568" class="me mf it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">第二种方法:编码器和k-NN</h2><p id="a1ee" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在我们的第二种方法中，我们将使用自动编码器的编码器部分。编码器将把实例映射到低维空间，并且k-最近邻(k-NN)将用于分类。在这种方法<strong class="lb iu">中，欺诈性和非欺诈性交易都将用于训练编码器</strong>。可以说，编码器将用于降维，从而加速k-NN的执行。</p><p id="8f4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用与第一种方法相同的模型。输入层和具有12个神经元的内部隐藏层将是编码器部分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/6f2e43c95afcd148818d59cfe401a136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*L3-myJMLJsKWx_fO0qjlOA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">编码器由自动编码器的前两层组成</p></figure><p id="1fb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于分类部分，所有实例(来自训练集和测试集)将通过编码器映射到12维空间。对于测试集中的每个实例，训练集的三个最接近的相邻案例将决定它是否是欺诈性的。</p><p id="6fe5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二种方法从测试数据集的243个欺诈案例中检测出184个(75.7%)。此外，142161个非欺诈案件中有12个(0.008%)被归类为欺诈案件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/1a617d9f3d1fd7e78b3d60170e5ddbfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*NNQi9sJacrUxRZe59X4aPA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第二种方法的混淆矩阵</p></figure><h2 id="ace6" class="me mf it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">结束语</h2><p id="c6a3" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">我们很快看到了两种使用自动编码器进行分类的方法。欢迎您尝试使用Colab代码。可以尝试的事情有:</p><ul class=""><li id="0132" class="oh oi it lb b lc ld lf lg li oj lm ok lq ol lu om on oo op bi translated">在第一种方法中改变误差阈值</li><li id="b4af" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">向自动编码器添加更多层</li><li id="95e2" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">改变编码器最后一层的神经元数量</li></ul><h2 id="8395" class="me mf it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">进一步阅读</h2><ul class=""><li id="9813" class="oh oi it lb b lc mx lf my li ov lm ow lq ox lu om on oo op bi translated">Venelin Valkok的Medium文章更详细地解释了第一种方法<a class="ae ky" href="https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd" rel="noopener">https://medium . com/@ curious ly/信用卡欺诈检测使用auto encoders-in-keras-tensor flow-for-hacker-part-VII-20 e0c 85301 BD</a></li><li id="ed76" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">第二种方法<a class="ae ky" href="https://hub.packtpub.com/using-autoencoders-for-detecting-credit-card-fraud-tutorial/" rel="noopener ugc nofollow" target="_blank">https://hub . packtpub . com/using-auto encoders-for-detecting-credit-card-fraud-tutorial/</a></li></ul></div></div>    
</body>
</html>