<html>
<head>
<title>Machine Learning 101 — Artificial Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习101 —人工神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-101-artificial-neural-networks-3-46ccb04cba30?source=collection_archive---------20-----------------------#2020-02-20">https://towardsdatascience.com/machine-learning-101-artificial-neural-networks-3-46ccb04cba30?source=collection_archive---------20-----------------------#2020-02-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="faae" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这篇文章旨在解释一般的人工神经网络，尤其是计算。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4f3f802266a128edac946ebffc90dc62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMLoQANtpQgqwWSCgdeRfg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://pixabay.com/images/id-1343877/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/images/id-1343877/</a></p></figure><h1 id="cf97" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">人工神经网络</h1><p id="d6e3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">关于人工神经网络(ANN ),你首先要了解的是，它来自于对大脑建模的想法。因此，我们在人工神经网络中使用的术语与神经网络密切相关，只是略有变化。</p><ul class=""><li id="531c" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">神经元在人工智能中也被称为神经元，</li><li id="f92f" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">但是树突被称为输入，</li><li id="7c89" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">轴突被称为输出，</li><li id="5c51" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">最后，突触被称为重量</li></ul><p id="76b9" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">一开始，学习这些术语就足够了。</p><h1 id="bc4a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">单层感知器</h1><p id="a2cf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">单层感知器是人工神经网络的原始版本。人工神经网络的灵感来自生物神经回路的功能[ <a class="ae ky" href="https://www.tutorialspoint.com/tensorflow/tensorflow_single_layer_perceptron.htm" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]，所以单层感知器自然是神经回路的代表。这个神经回路是一个神经元，其输入具有相应的权重，并作为计算的输出输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/8f6adcc8a1f0f2e3e9cd530410a61027.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*4vhc21ANTtsAhhkKk8epLQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nh">图1 </strong>具有两个输入和一个输出的单层感知器的表示</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/a6d5d945572b1a19c2e960c7ef801169.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*__OJdSSBeIM6KhBKyDwP8Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2单层感知器的一般方程</p></figure><p id="9334" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">在单层感知器中，x代表输入，w代表权重，θ代表阈值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/52f454d88ba84a73e568f366b566e742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*jycl_91dMcrzuQtvWepS1Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nh">图3 </strong>根据图1表示的计算</p></figure><p id="955b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我试图尽可能基本地解释单层感知器的方程。过了这一点就更复杂了。y的输出值是根据函数类型构造的。就功能而言，有许多类型，但在开始时，3种就足够了。</p><h2 id="6f19" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">1.阶跃函数</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/73177bfac12ab55d7d8f235b43736e06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*8cT5g7hIymsuHaR1XiLbhw.png"/></div></figure><p id="6dad" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">根据输入的值，输出将是0或1。</p><h2 id="a6a0" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">2.符号函数</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d2f5cd75dd806a93bedb969ce08c27cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*VJSZqd0bbl6u-qWTsZyAUQ.png"/></div></figure><h2 id="8ae6" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">3.Sigmoid函数</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/55d0d9e636c55939959d2d7dd2c316f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*tVYUMed2p7Z-OYAnDMohGQ.png"/></div></figure><p id="1ddb" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">在这种方法中，感知器的作用就像一个线性回归模型。sigmoid函数将这些值归一化到[0，1]的范围内，以便负值在0附近缩放，正值在1附近缩放。因此，您不需要处理可能导致计算错误的负值或非常大的数字。</p><h2 id="e29f" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">单层感知器训练</h2><ol class=""><li id="e7a7" class="mn mo it lt b lu lv lx ly ma nx me ny mi nz mm oa mv mw mx bi translated">输入和相关的输出值被提供给模型。丁:<x/></li><li id="b3e1" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm oa mv mw mx bi translated">所有的权重值都是以一定的方式定义的，在这一点上我们说随机在[-0.5，0.5]的范围内。</li><li id="5760" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm oa mv mw mx bi translated">根据这些值计算输出，并计算误差以进行比较。学习率(0 </li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/a267232754cfdb687b8ebbfea84c1a1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*U7w7yIbHopcq6hreFj1ngg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nh">图4 </strong>训练单层感知器的方程式</p></figure><p id="98a3" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">4.通过改变权重值，根据阈值来校正误差。</p><p id="4596" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">5.从步骤3继续到下一个纪元。</p><p id="ba85" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">例1: </strong>我们用阶跃函数把OR运算映射到单层感知器。</p><p id="0072" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">下表列出了可能的输入和输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/b3ae701e0c737c0be8fbe47bfe01f0bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hynYMSNj9RQOvl6OpGdlzQ.png"/></div></div></figure><p id="ddf0" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">可接受的默认值:</strong>𝜃= 0.02；𝜇=0.1 ;w1 = 0.3w2=-0.1</p><p id="10af" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu"> <em class="od">第一纪元:</em> </strong></p><p id="e638" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y = x1*w1 + x2*w2-𝜃</p><p id="6538" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y1 = 0.3 * 0+(-0.1)* 0–0.02 =-0.02-0.02&lt;0 → y1 = 0</p><p id="8053" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y2 = 0.3*0 + (-0.1)*1–0.02 = -0.12 -0.12&lt;0 → y2 = 0</p><p id="1c22" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y3 = 0.3*1 + (-0.1)*0–0.02 = 0.28 0.28&gt;0→y3 = 1</p><p id="dcc4" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y4 = 0.3 * 1+(-0.1)* 1–0.02 = 0.18 0.18 &gt; 0→y4 = 1</p><p id="6a21" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y3和y4的计算输出和预期输出不同。所以我们需要相应地计算学习率和新的权重。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/84804f3739e1e2b2a904c3372700180e.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*FrcsyJZBB83Oy8xGOFodxg.png"/></div></figure><p id="ce5f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw11 = 0.1 *(0–0)* 0 = 0</p><p id="c573" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw21 = 0.1 *(0–0)* 0 = 0</p><p id="8e76" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw13 = 0；δw23 = 0；δw14 = 0；w24 = 0</p><p id="3ce6" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw13 = 0.1 *(1–0)* 0 = 0；δw23 = 0.1 *(1–0)* 1 = 0.1</p><p id="4f7a" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">最新的重量值计算如下:</p><p id="904d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w13 ' = w13+δw13</p><p id="ce07" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w13' = 0.3 + 0 = 0.3</p><p id="bb95" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w23 ' = w23+δw23</p><p id="3ff2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w23' =(-0.1) + 0.1 = 0</p><p id="7eb8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu"> <em class="od">第二纪元:</em> </strong></p><p id="aa51" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">再次计算输出；</p><p id="6a6f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y1 = 0.3 * 0+0 * 0-0.02 =-0.02&lt;0 → y1 = 0</p><p id="511b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y2 = 0.3*0 + 0*1 -0.02 = -0.02 &lt;0 → y2 = 0</p><p id="ae6c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y3 = 0.3*1 + 0*0 -0.02 = 0.28 &gt;0→y3 = 1</p><p id="59c8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y4 = 0.3*1 + 0*1 -0.02 = 0.28 &gt; 0 → y4 = 1</p><p id="33f3" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw11 = 0；δw21 = 0；δw13 = 0；δw23 = 0；δw14 = 0；w24 = 0</p><p id="7773" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw13 = 0.1 *(1–0)* 0 = 0→w13 ' = 0+0.3 = 0.3</p><p id="6c22" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw23 = 0.1 *(1–0)* 1 = 0.1→w24 ' = 0.1+0 = 0.1</p><p id="f18a" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">仍然无法达到正确的重量值。所以我们仍然需要调整。</p><p id="ad7d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu"> <em class="od">第三纪元:</em> </strong></p><p id="7d90" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y1 = 0.3 * 0+0.1 * 0-0.02 =-0.02&lt;0 → y1 = 0</p><p id="fb29" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y2 = 0.3*0 + 0.1*1 -0.02 = 0.08 &gt;0→y2 = 1</p><p id="c83e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y3 = 0.3 * 1+0.1 * 0-0.02 = 0.28 &gt; 0→y3 = 1</p><p id="b853" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y4 = 0.3 * 1+0.1 * 1-0.02 =-0.38 &gt; 0→y4 = 1</p><p id="cb3e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw11 = 0；δw21 = 0；δw12 = 0；δw22 = 0；</p><p id="b3a2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw13 = 0；δw23 = 0；δw14 = 0；w24 = 0</p><p id="7733" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">似乎我们找到了正确的重量值。这就是单层感知器的工作原理。理解模型是非常必要的，因为更复杂的模型是基于单层感知器的。</p><h1 id="b8a3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">多层感知器</h1><p id="8dff" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">用单层感知器解决OR运算就足够了，但是当涉及到XOR运算时，事情已经开始发生变化，因为单层感知器本身不足以解决问题。因此，多层感知器(MLP)的想法刚刚出现。如图5所示，多层感知器由三层组成:输入层、隐藏层和输出层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/e90d8430554d44d3792d88bec37ab403.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*efxKcE8BcmYqIX5BcB158A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nh">图5 </strong>多层感知器的示意图</p></figure><p id="77f5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">此外，MLP是深度学习的一个良好开端。它被称为深度人工神经网络，但当然没有卷积神经网络那么深。</p><h2 id="9cdf" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">反向传播</h2><p id="f944" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在MLP，属于一个层的每个计算都被接受为下一层的输出。反向传播看起来非常复杂，但其实不然。计算误差时，通过反向计算重新排列权重。</p><blockquote class="og"><p id="e943" class="oh oi it bd oj ok ol om on oo op mm dk translated">在反向传递中，使用反向传播和微积分的链式法则，误差函数的偏导数通过MLP反向传播各种权重和偏差。这种微分行为给了我们一个梯度，或者说一幅误差图，沿着这个梯度，当MLP向误差最小值靠近一步时，参数可以被调整。这可以通过任何基于梯度的优化算法来实现，例如随机梯度下降。网络继续打网球，直到错误不能再低。这种状态被称为收敛[ <a class="ae ky" href="https://pathmind.com/wiki/multilayer-perceptron" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]。</p></blockquote><figure class="or os ot ou ov kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/a4bcc98bd88bbfd2547124d1702aaaa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qye6x43Xy_eqlz0qRrju3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nh">图6 </strong> E表示y和d之间的欧氏距离，y_t表示样本t的误差</p></figure><p id="539e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">示例2: </strong>让我们通过使用阶跃函数来映射具有多层感知器的XOR运算。</p><p id="57fd" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">下表列出了可能的输入和输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/5b129b3f43a8579e3e7c1107c32763aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9mbPHmkg5GGkNx0xNKhO_A.png"/></div></div></figure><p id="4e3b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">当您检查<em class="od">结果列</em>时，您将意识到这两个等式都是y = x2*w2和y=x1*w1 <strong class="lt iu">大于0，但这是不可能的，因为x1和x2是输入，对于XOR运算，它们中的任何一个都应该等于<strong class="lt iu">0。此外，根据<em class="od">结果栏</em>中的等式，y=x1*w1 + x2*w2 <strong class="lt iu">小于0</strong>，但这也是不可能的。因此，在这种情况下，单层感知器不能作为方程的解，建议使用多层感知器来寻找解。</strong></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/dae123e2e6cc405b6a3eeaeac5f8a2f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*jw584T99qdQRpGRpUkelOA.png"/></div></figure><p id="e591" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w13 = 0.5</p><p id="bdfb" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w23 = 0.4</p><p id="7e8d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w35 = -1.2</p><p id="d384" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w14 = 0.9</p><p id="62a5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w24 = 1 w45 = 1.1</p><p id="816a" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi">𝜃3 = 0.8 𝜃4 = -0.1 𝜃5 = 0.3</p><p id="2cd4" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi">𝜇 = 0.1</p><p id="a12e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">让我们接受上述值作为开始。在XOR运算中，当x1=x2=0时，y将等于y=0。由于这些值是连续的，我们可以使用sigmoid函数。</p><p id="35f7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y3 = sigmoid(x1*w13+x2*w23-𝜃3)</p><p id="1bdc" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">= 1/(1+e-(0.5 * 1+1 * 0.4-1 * 0.8))=<strong class="lt iu">0.525</strong></p><p id="b6ff" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y4 =sigmoid(x1*w14+x2*w24-𝜃4)=<strong class="lt iu">0.881</strong></p><p id="5a68" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">y5 = sigmoid(y3*w35+y4*w45-𝜃3)</p><p id="ab65" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">= 1/(1+e-(0.525 *(1.2)+0.88 * 1.1–0.3))=<strong class="lt iu">0.51</strong>-&gt;<strong class="lt iu">实际输出</strong></p><p id="db81" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">误差=ɛ=(0–0.51)=-0.51</p><p id="23a8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">输出层:</strong></p><p id="b4aa" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">𝜍5 = y5 *(1-y5)*ɛ= 0.5097 *(1–0.5097)*(-0.5097)≠<strong class="lt iu">-0.13</strong></p><p id="3088" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw35 =𝜍5*y3*𝜇=-0.12 * 0.525 * 0.1 =<strong class="lt iu">-0.0068；</strong></p><p id="396e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw45 =𝜍5*y4*𝜇=-0.12 * 0.88 * 0.1 =<strong class="lt iu">-0.012</strong></p><p id="b071" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w35 ' =-0.0068+(-0.12)=<strong class="lt iu">-1，2068 </strong></p><p id="0aaa" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w45' = (-0，12)+1.1 = <strong class="lt iu"> 1.088 </strong></p><p id="d5f8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">隐藏层:</strong></p><p id="831b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">𝜍3 = y3 *(1-y3)* 𝜍5*w35 = &gt; δw23=𝜇*x2*𝜍3；δw13=𝜇*x1*𝜍3</p><p id="1319" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">𝜍3 = 0.53 *(1–0.53)*-0.13 *(1.2)=<strong class="lt iu">0.0389</strong></p><p id="b3c2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw23 = 0.039 * 1 * 0.1 =<strong class="lt iu">0.0039</strong>；δw13 = 0.039 * 1 * 0，1 = <strong class="lt iu"> 0.0039 </strong></p><p id="218c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w23 ' = 0.0039+0.4 =<strong class="lt iu">0.4039</strong>；w13'=0.0039+0.5= <strong class="lt iu"> 0.5039 </strong></p><p id="7ace" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">𝜍4 = y4 *(1-y4)* 𝜍5*w45 = &gt; δw24=𝜇*x2*𝜍4；δw14=𝜇*x1*𝜍4</p><p id="4988" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">𝜍4 = 0.0881 *(1–0.0881)*(-0.13)* 1.1 =<strong class="lt iu">-0.015</strong></p><p id="ff63" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">δw24 =-0.015 * 1 * 0.1 =<strong class="lt iu">-0.0015；</strong>δw14 =-0.015 * 1 * 0.1 =<strong class="lt iu">-0.0015</strong></p><p id="e92b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w24 ' =-0.0015+1 =<strong class="lt iu">0.985</strong>；w14 ' =-0.0015+0.9 =<strong class="lt iu">0.8985</strong></p><p id="404c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu"> <em class="od">新更新权重:</em> </strong></p><p id="e821" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">w14 =-0.0015；w24 = 0.8985w13 = 0.5039w23 = 0.4039w45 = 1.088w35=1.2068</p><p id="7140" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们正在计算每一个权重，直到找到正确的结果。隐层应该有多少个神经元，网络应该有多少个隐层，这是一个应该由这个网络的设计师来回答的问题。最大值经典人工神经网络的隐含层数为3。</p><h2 id="7b19" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">重量更新</h2><p id="29a0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu"> <em class="od"> 1。增量模式</em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/bc9b3578070b5ff5d0f59311ed27f312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3QDy-9N9jZQ9XHBHZAvMyw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每次训练后计算新的重量</p></figure><p id="e473" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu"> <em class="od"> 2。批处理模式</em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/148492c1bbf405f3f7dd6785ff5a5a8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WXBHlTgCCbzT2nmoqCqTTw.png"/></div></div></figure><p id="2836" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">动量:</strong>该项连接t和(t-1)时刻之间的变化。它主要用于敏感。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/bdbed8b30aa87e1f6f0d0cb5265673f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R3B9TzqgN7ix1tbzZlQQDQ.png"/></div></div></figure><h1 id="e85a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="eebc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">人工神经网络(ANN)是深度学习的良好开端。在这篇文章中，我打算解释人工神经网络的类型和基本计算，以调整它们的核心权重值。</p><h2 id="94e8" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">参考资料:</h2><ol class=""><li id="e67d" class="mn mo it lt b lu lv lx ly ma nx me ny mi nz mm oa mv mw mx bi translated">Tensorflow单层感知器，<a class="ae ky" href="https://www.tutorialspoint.com/tensorflow/tensorflow_single_layer_perceptron.htm" rel="noopener ugc nofollow" target="_blank">https://www . tutorialspoint . com/tensor flow/tensor flow _ Single _ Layer _ Perceptron . htm</a></li><li id="b03f" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm oa mv mw mx bi translated">多层感知器，<a class="ae ky" href="https://pathmind.com/wiki/multilayer-perceptron" rel="noopener ugc nofollow" target="_blank">https://pathmind.com/wiki/multilayer-perceptron</a></li></ol></div></div>    
</body>
</html>