<html>
<head>
<title>Credit Risk Management: Feature Scaling &amp; Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">信用风险管理:特征缩放和选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/credit-risk-management-feature-scaling-selection-b734049867ea?source=collection_archive---------26-----------------------#2020-08-05">https://towardsdatascience.com/credit-risk-management-feature-scaling-selection-b734049867ea?source=collection_archive---------26-----------------------#2020-08-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1973" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在特征工程之后，这一部分进入数据准备过程的下一步:特征缩放和选择，在建模之前将数据集转换为更易消化的数据集。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/05ec991f7083b76d3f38d89879566c54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wL7BP-A3YGzaEgGhk4hlWg.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图片来源:<a class="ae lb" href="https://unsplash.com/photos/JG35CpZLfVs" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/JG35CpZLfVs</a></p></figure><p id="3f5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">提醒一下，这个端到端项目旨在解决数据科学(尤其是金融行业)中的分类问题，分为 3 个部分:</p><ol class=""><li id="006b" class="lc ld iq jp b jq jr ju jv jy le kc lf kg lg kk lh li lj lk bi translated">解释性数据分析(EDA)和特征工程</li><li id="c44e" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated"><strong class="jp ir">特征缩放和选择(奖励:不平衡数据处理)</strong></li><li id="9131" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated">机器学习建模(分类)</li></ol><p id="bf49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你错过了第一部分，在阅读接下来的第二部分之前，请在这里查看<a class="ae lb" rel="noopener" target="_blank" href="/credit-risk-management-eda-feature-engineering-81cc34efc428"><strong class="jp ir"/></a>，以便更好地理解上下文。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="e118" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">A.特征缩放</h1><p id="65f6" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><em class="na">什么是特征缩放，为什么我们在建模之前需要它？</em></p><p id="d4dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据维基百科，</p><blockquote class="nb nc nd"><p id="6642" class="jn jo na jp b jq jr js jt ju jv jw jx ne jz ka kb nf kd ke kf ng kh ki kj kk ij bi translated">特征缩放是一种用于<strong class="jp ir">标准化</strong>独立变量或数据特征范围的方法。在数据处理中，它也被称为数据规范化，通常在数据预处理步骤中执行。</p></blockquote><p id="68cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您还记得第一部分，我们已经在两个数据集(A &amp; B)上完成了所有特征的工程设计，如下所示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nh"><img src="../Images/7712433b6acc4c0c775a53466e9fc44c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7T4dYhJh55OIAJatAiYVcQ.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">数据集 A(无目标编码)</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ni"><img src="../Images/8c457bb798c600a4e263072ce9d9e435.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E5MvNmHr6k3YAgM0.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">数据集 B(用目标编码)</p></figure><p id="d169" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所述，所有特征之间的数据范围和分布彼此相对不同，更不用说一些带有异常值的变量了。也就是说，强烈建议我们对整个数据集一致地应用特征缩放，以使其更容易被机器学习算法消化。</p><p id="a66f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事实上，市场上有许多不同的方法，但我将只关注我认为相对有特色的三种:<strong class="jp ir">标准缩放器</strong>、<strong class="jp ir">最小最大缩放器</strong>和<strong class="jp ir">鲁棒缩放器</strong>。简而言之，</p><ul class=""><li id="c52d" class="lc ld iq jp b jq jr ju jv jy le kc lf kg lg kk nj li lj lk bi translated"><strong class="jp ir"> StandardScaler </strong>:该方法移除平均值，并将数据缩放至单位方差(平均值= 0，标准差= 1)。然而，它受异常值的影响很大，特别是那些边缘极端的异常值，这些异常值会将缩放后的数据范围扩展到超过 1 个标准偏差。</li><li id="75a7" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk nj li lj lk bi translated"><strong class="jp ir"> MinMaxScaler </strong>:该方法减去特征中的最小值，再除以范围(即原始最大值和最小值之差)。本质上，它将数据集重新调整到 0 和 1 的范围内。然而，这种方法相对有限，因为它将所有数据点压缩到一个狭窄的范围内，并且在存在异常值的情况下帮助不大。</li><li id="0479" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk nj li lj lk bi translated"><strong class="jp ir"> RobustScaler </strong>:该方法基于百分位数，减去中位数，除以四分位数范围(75% — 25%)。它通常比其他两种缩放器更可取，因为它不会受到大的边缘异常值(如果有的话)的很大影响。</li></ul><p id="4eff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看三个定标器在我们的数据集中有何不同:</p><pre class="km kn ko kp gt nk nl nm nn aw no bi"><span id="f139" class="np ly iq nl b gy nq nr l ns nt">from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler</span><span id="5c4f" class="np ly iq nl b gy nu nr l ns nt">#StandardScaler<br/>x_a_train_ss = pd.DataFrame(StandardScaler().fit_transform(x_a_train), columns=x_a_train.columns)</span><span id="f90a" class="np ly iq nl b gy nu nr l ns nt">#MinMaxScaler<br/>x_a_train_mm = pd.DataFrame(MinMaxScaler().fit_transform(x_a_train), columns=x_a_train.columns)</span><span id="8837" class="np ly iq nl b gy nu nr l ns nt">#RobustScaler<br/>x_a_train_rs = pd.DataFrame(RobustScaler().fit_transform(x_a_train), columns=x_a_train.columns)</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nv"><img src="../Images/3dc8ad8b6270558586efade27646c54c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LFDAFlzoz7RKGiBIN-gZ8w.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">标准缩放器</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nw"><img src="../Images/2f453a85d0f4520e3a7d9009abec0e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FR0o5MpFZ3VvCcr6oq8HxQ.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">最小最大缩放器</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nx"><img src="../Images/61dd6596a4ce335a5e884efded0d4d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lP0FcW8yd4KjZnX1WzoOSA.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">鲁棒定标器</p></figure><p id="6c7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所述，RobustScaler 的缩放数据范围看起来比其他两个缩放器更容易理解，这可能有助于机器学习算法更快、更有效地推动处理运行时间。然而，这是我在建模之前的假设，但是我们可以在那个阶段进行试验。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="a7a8" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">B.不平衡数据处理</h1><p id="020f" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><em class="na">什么是不平衡数据，我们应该如何处理？</em></p><p id="8f26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，不平衡数据集是指目标分布存在严重偏差的数据集，这对于建模来说可能并不理想。作为一个更清楚的例子，让我们看看我们的数据集是否不平衡:</p><pre class="km kn ko kp gt nk nl nm nn aw no bi"><span id="4a32" class="np ly iq nl b gy nq nr l ns nt">a_target_0 = df_a[df_a.target == 0].target.count() / df_a.target.count()<br/>a_target_1 = df_a[df_a.target == 1].target.count() / df_a.target.count()</span></pre><p id="7bad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结果是 76%的数据被归类为目标 0，而剩余的 24%被归类为目标 1。</p><p id="5247" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事实上，没有一个清晰的基准，我们可以依靠它来准确地确定我们的数据集是否不平衡。有些人说是 9:1，而有些人说是 8:2，这实际上取决于数据集的性质以及您正在解决的环境/问题。在我的例子中，我认为上述结果是不平衡的，并将对数据集进行“重新采样”以使其相对平衡。</p><p id="5b92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="na">只是一个免责声明</em> </strong>，我在这里采取的所有预处理步骤并不意味着它们都是必须要做的，并且对我们以后的模型的准确性有积极的影响。这意味着我的目标是测试所有可能有助于改进我的模型的场景。</p><p id="c74b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回到重采样，我们可能听说过两种常见的方法:过采样和欠采样。简而言之，</p><ul class=""><li id="4309" class="lc ld iq jp b jq jr ju jv jy le kc lf kg lg kk nj li lj lk bi translated"><strong class="jp ir">过采样</strong>:该方法从少数类中复制样本，并将它们添加到数据集(训练集)。</li><li id="d7f2" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk nj li lj lk bi translated"><strong class="jp ir">欠采样</strong>:这与上面从多数类中删除一些样本相反。</li></ul><p id="51a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">视觉上，你可以想象成这样:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ni"><img src="../Images/a11136918c9cb37b9e10143884fcb866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UiNxMJ5Go69XDHrQ.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图片鸣谢:<a class="ae lb" rel="noopener" target="_blank" href="/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb">https://towards data science . com/having-an-unbalanced-dataset-here-is-how-you-can-solve-it-1640568947 EB</a></p></figure><p id="1db6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们测试一下这两者:</p><pre class="km kn ko kp gt nk nl nm nn aw no bi"><span id="8128" class="np ly iq nl b gy nq nr l ns nt"># Under-sampling<br/>from imblearn.under_sampling import RandomUnderSampler</span><span id="ceaf" class="np ly iq nl b gy nu nr l ns nt">undersample = RandomUnderSampler()</span><span id="9d64" class="np ly iq nl b gy nu nr l ns nt">x_a_train_rs_under, y_a_train_under = undersample.fit_resample(x_a_train_rs, y_a_train)<br/>print(Counter(y_a_train_under))</span><span id="b92e" class="np ly iq nl b gy nu nr l ns nt"># Over-sampling<br/>from imblearn.over_sampling import SMOTE<br/>from collections import Counter</span><span id="5fb4" class="np ly iq nl b gy nu nr l ns nt">oversample = SMOTE()</span><span id="39d3" class="np ly iq nl b gy nu nr l ns nt">x_a_train_rs_over, y_a_train_over = oversample.fit_resample(x_a_train_rs, y_a_train)<br/>print(Counter(y_a_train_over))</span></pre><p id="7e7a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每种方法，都有多种选择来对数据集进行重采样，但我选择了最常用的一种，即<strong class="jp ir"> RandomUnderSampler </strong>(欠采样)和<strong class="jp ir"> SMOTE </strong>(过采样)。重采样后的类分布为:</p><ul class=""><li id="23b2" class="lc ld iq jp b jq jr ju jv jy le kc lf kg lg kk nj li lj lk bi translated">随机欠采样:0: 5814，1: 5814</li><li id="ca4f" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk nj li lj lk bi translated">SMOTE: 1: 18324，0: 18324</li></ul><p id="537f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">两者各有利弊，但是顾名思义，RandomUnderSampler 从多数类中“随机”选择要移除的数据，这可能会导致信息丢失，因为不是整个数据集都被纳入建模。也就是说，我选择了 SMOTE。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="04ee" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">C.特征选择</h1><p id="d950" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">什么是功能选择，有哪些选项可供我们参考？</p><p id="ba57" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，特征选择是在数据集中选择与目标变量有很大关联/影响的变量的过程。特别是，当涉及到更大的数据集时，我们可能会面对数百个特征，其中一些可能不相关，甚至与输出无关。因此，我们需要在建模之前进行特征选择，以达到最高的精度。</p><p id="220f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事实上，有一些不同的技术可以归为两大类:(1) <strong class="jp ir">特征选择</strong>和(2) <strong class="jp ir">维度缩减</strong>。我相信这些名字对你来说听起来很熟悉，但本质上它们是相同的，只是为每一个做的技术相对不同。</p><h2 id="c3e7" class="np ly iq bd lz ny nz dn md oa ob dp mh jy oc od ml kc oe of mp kg og oh mt oi bi translated">1.特征选择</h2><p id="ccdb" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">如果你正在寻找一个完整的技巧列表，请随意参考<a class="ae lb" href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/" rel="noopener ugc nofollow" target="_blank">这篇博客文章</a>，它列出了所有可能的试验方法。在这个项目中，为了简单起见，我将只应用两个:(a) <strong class="jp ir">特征重要性</strong>和(b) <strong class="jp ir">相关矩阵</strong>。</p><p id="835a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于<strong class="jp ir">特征重要性</strong>，顾名思义，我们将选择与目标变量相关率最高的前几个特征(如前 10 或 15，取决于特征总数)。具体来说，该技术部署了 ExtraTreeClassifier 算法的属性:<strong class="jp ir"> feature_importances_ </strong></p><pre class="km kn ko kp gt nk nl nm nn aw no bi"><span id="7ce0" class="np ly iq nl b gy nq nr l ns nt">from sklearn.ensemble import ExtraTreesClassifier</span><span id="a01c" class="np ly iq nl b gy nu nr l ns nt">fi = ExtraTreesClassifier()<br/>fi_a = fi.fit(x_a_train_rs_over, y_a_train_over)</span><span id="df53" class="np ly iq nl b gy nu nr l ns nt">df_fi_a = pd.DataFrame(fi_a.feature_importances_,index=x_a_train_rs_over.columns)<br/>df_fi_a.nlargest(10,df_fi_a.columns).plot(kind='barh')<br/>plt.show()</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oj"><img src="../Images/9b1dd98bf967dc9467931912e6bc3072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nz_TGbT4cjooiVDRawhvww.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">。功能 _ 重要性 _</p></figure><p id="181e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如你所见，年收入是最重要的特征，其次是年龄和就业年份。事实上，这真的取决于你要选择的特性的数量。</p><p id="ccba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">转到特征选择的第二种方法，<strong class="jp ir">相关矩阵</strong>是显示数据集中变量之间相关系数的表格。本质上，数字越高，任何两个变量之间的相关性越强。</p><p id="eae3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了更好地展示，让我们更直观地看看:</p><pre class="km kn ko kp gt nk nl nm nn aw no bi"><span id="d809" class="np ly iq nl b gy nq nr l ns nt">df_b_train_processed = pd.concat([x_b_train_rs_over, y_b_train_over], axis=1) #combine processed features with their target</span><span id="9f25" class="np ly iq nl b gy nu nr l ns nt">cm_b = df_b_train_processed.corr()</span><span id="52b1" class="np ly iq nl b gy nu nr l ns nt">print(cm_b.target.sort_values().tail(10))</span><span id="e182" class="np ly iq nl b gy nu nr l ns nt">plt.figure(figsize=(20,20))<br/>sns.heatmap(cm_b, xticklabels=df_b_train_processed.columns, yticklabels=df_b_train_processed.columns,annot=True)</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/c626964e36ec4c4a2afa13905ca677b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*rPtKPg-AtPemB3itzkQvFA.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">df.corr()</p></figure><p id="206b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所述，我们只需要考虑表中的最后一列，即所有自变量与目标的相关性。看起来所有的特征与相同色调的目标共享相似的系数。这可能与我们刚刚提到的(1)特性重要性略有不同。然而，没有明确的对错答案，因为每种技术的设计和功能都不同。</p><h2 id="16af" class="np ly iq bd lz ny nz dn md oa ob dp mh jy oc od ml kc oe of mp kg og oh mt oi bi translated">2.降维</h2><p id="2cc5" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">降维基本类似于特征选择，但有自己的技术。我经常使用的常见选项可以分为基于组件的(PCA)和基于投影的(t-SNE 和 UMAP)。</p><ul class=""><li id="df56" class="lc ld iq jp b jq jr ju jv jy le kc lf kg lg kk nj li lj lk bi translated"><strong class="jp ir">基于组件(PCA) </strong>:顾名思义，它基于数据集中的原始特征，这些特征被转换成一组与目标具有更好相关性的新变量。</li><li id="0f71" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk nj li lj lk bi translated"><strong class="jp ir">基于投影(t-SNE 和 UMAP) </strong>:这种技术背后的数学概念很复杂，但简而言之，它指的是将多维数据投影到一个更低维的空间，这有助于减少数据集中的特征数量。</li></ul><p id="e777" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请记住，使用这些技术时需要进行特征缩放！</p><pre class="km kn ko kp gt nk nl nm nn aw no bi"><span id="c69c" class="np ly iq nl b gy nq nr l ns nt">from sklearn.decomposition import PCA</span><span id="b203" class="np ly iq nl b gy nu nr l ns nt">pca = PCA(.95)</span><span id="f416" class="np ly iq nl b gy nu nr l ns nt">pca_a_train = pca.fit(x_a_train_rs_over, y_a_train_over)<br/>print(pca_a_train.n_components_)</span><span id="689e" class="np ly iq nl b gy nu nr l ns nt">plt.plot(np.cumsum(pca_a_train.explained_variance_ratio_))<br/>plt.show()</span><span id="ceab" class="np ly iq nl b gy nu nr l ns nt">x_a_train_rs_over_pca = pd.DataFrame(pca_a_train.transform(x_a_train_rs_over))<br/>x_a_train_rs_over_pca.head()</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ol"><img src="../Images/60029a1b5269ad985c108d1bbfec670c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DKO_hRaYwgs_1Wib_sqGJA.png"/></div></div></figure><p id="7b88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">至于 PCA，当我调用语法时，我将 PCA 的解释方差设置为. 95，这意味着我希望得到一组新的变量，它与原始变量相比有 95%的方差。在这种情况下，在我们将训练数据拟合到 PCA 之后，计算出我们只需要 46 个特征中的 24 个。此外，查看解释方差比率图，在 24 个特征之后，线停止增加，这可能是应用 PCA 后的理想特征数。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi om"><img src="../Images/2aac59e740c044f5b64a9660cfb09cc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vSlIb1YZEem4aCawQzV3Rg.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">pca.fit_transform()</p></figure><p id="c573" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">至于基于投影的，t-SNE 对大数据集很好，但它被证明有局限性，特别是计算时间低和大规模信息丢失，而 UMAP 在运行时方面表现更好，同时保留了信息。</p><p id="ac42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，UMAP 的工作原理是，它先计算高维空间中的点与点之间的距离，投影到低维空间中，再计算这个低维空间中的点与点之间的距离。然后，它使用随机梯度下降来最小化这些距离之间的差异。</p><pre class="km kn ko kp gt nk nl nm nn aw no bi"><span id="8fd1" class="np ly iq nl b gy nq nr l ns nt">import umap</span><span id="9051" class="np ly iq nl b gy nu nr l ns nt">um = umap.UMAP(n_components=24)</span><span id="8e0a" class="np ly iq nl b gy nu nr l ns nt">umap_a_train = um.fit_transform(x_a_train_rs_over)<br/>x_a_train_rs_over_umap = pd.DataFrame(umap_a_train)<br/>x_a_train_rs_over_umap.head()</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi on"><img src="../Images/e026eaec769b6412d8364797b243dad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ms6Dni62nSnXFfRNmBs6-A.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">umap。UMAP.fit_transform()</p></figure><p id="bfd4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了在性能方面比较 PCA 和 UMAP，它依赖于我们的数据集的规模和复杂性，以便确定使用哪一个。然而，为了简单和更好的运行时间，我选择在数据集之间应用 PCA，并在建模阶段利用它。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><p id="09ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">瞧啊。我们已经完成了这个端到端项目的第二部分，重点是功能缩放和选择！</p><p id="c980" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我真的希望你能发现它内容丰富且易于理解，所以请在这里留下你的评论吧！请注意这个项目的第三个最后部分，它利用所有的数据准备步骤来应用<strong class="jp ir">机器学习算法</strong>。</p><p id="6f29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与此同时，我们来连线:</p><p id="5767" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">github:<a class="ae lb" href="https://github.com/andrewnguyen07" rel="noopener ugc nofollow" target="_blank">https://github.com/andrewnguyen07</a><br/>领英:<a class="ae lb" href="http://www.linkedin.com/in/andrewnguyen07" rel="noopener ugc nofollow" target="_blank">www.linkedin.com/in/andrewnguyen07</a></p><p id="416e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">谢谢！</p></div></div>    
</body>
</html>