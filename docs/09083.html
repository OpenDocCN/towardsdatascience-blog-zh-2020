<html>
<head>
<title>The fastML Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">fastML 指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-fastml-guide-9ada1bb761cf?source=collection_archive---------60-----------------------#2020-06-29">https://towardsdatascience.com/the-fastml-guide-9ada1bb761cf?source=collection_archive---------60-----------------------#2020-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4f4c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">作为一名数据科学家、人工智能或 ML 工程师，速度是从事一个项目的基本要素。fastML 为您提供了速度和灵活性，让您可以针对多种算法测试您的模型。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/176761d8a704d1e10b06d872ebcfb358.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3yqUqoFm2kx63-rCsjDdCA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">FastML 标志由<a class="ae ky" href="https://www.linkedin.com/in/divine-kofi-alorvor-86775117b" rel="noopener ugc nofollow" target="_blank"> Divine Kofi Alorvor </a></p></figure><p id="d613" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据科学家、人工智能和机器学习工程师的工作很大程度上围绕着许多不同算法的使用，这些算法使我们的工作变得更容易和相对更快。然而，我们经常发现，为特定的用例选择特定的算法是相当困难的，因为有许多算法可以同等地执行我们需要执行的任务。</p><p id="f331" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">fastML 是一个 python 包，它允许您使用自己喜欢的测试大小来测试和训练已处理的数据，并使用很少几行代码对准备好的数据运行多种分类算法。这使您可以观察您决定运行的所有算法的行为，以确定哪些算法最适合您的数据，以便使用您选择的算法进行进一步开发。这也为您节省了在不使用 fastML 的情况下手动编写近 300 行代码的压力。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="4c6d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">入门指南</h1><p id="477c" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">fastML 发布到<strong class="lb iu"/><a class="ae ky" href="https://pypi.org/project/fastML/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">pypi</strong></a><strong class="lb iu"/>，这使得使用 python 包安装程序 pip 在本地安装变得容易。要安装 FastML 进行本地开发，请确保您已经安装了 python 和 pip 并将其添加到 path 中。如果你需要帮助，你可以在这里查看 Python 文档。</p><p id="8bba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要安装 fastML，请打开您的终端(Linux/mac)或命令提示符(windows)并输入命令:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="6a8c" class="ne md it na b gy nf ng l nh ni">pip install fastML</span></pre><h1 id="e104" class="mc md it bd me mf nj mh mi mj nk ml mm jz nl ka mo kc nm kd mq kf nn kg ms mt bi translated">使用 fastML</h1><p id="88bf" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本指南中，我将通过一个流行的 Iris 数据集的例子来教你如何在你的项目中使用 fastML 包。在处理任何项目时，要做的第一件事就是导入项目所需的库和包。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="4a15" class="ne md it na b gy nf ng l nh ni">##importing needed libraries and packages including fastML</span><span id="0254" class="ne md it na b gy no ng l nh ni">from fastML import fastML<br/>from sklearn import datasets</span></pre><p id="afb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在要做的下一件事是将 Iris 数据集加载到我们的项目中以供使用。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="6f85" class="ne md it na b gy nf ng l nh ni">##loading the Iris dataset</span><span id="494c" class="ne md it na b gy no ng l nh ni">df = datasets.load_iris()</span></pre><p id="3f76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于虹膜数据集已经进行了预处理，因此没有必要再次处理我们的数据。但是，对于您将在自己的项目中使用的数据，您必须确保数据经过良好的处理，以避免在项目中遇到错误和不希望的输出。</p><p id="502f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在要做的下一件事是准备用于训练和测试的数据，并将所需的数据列指定为特性和目标列。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="e901" class="ne md it na b gy nf ng l nh ni">##assigning the desired columns to X and Y in preparation for running fastML</span><span id="a216" class="ne md it na b gy no ng l nh ni">X = df.data[:, :4]<br/>Y = df.target</span></pre><p id="d768" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据您拥有的数据类型，您的目标数据可能最需要编码。对你的目标数据进行编码是很好的，因为它选取了可以解释目标数据的值，并帮助机器学习算法理解目标数据到底是什么。使用 fastML python 包对目标数据进行编码也非常容易。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="37ba" class="ne md it na b gy nf ng l nh ni">##importing the encoding function from the fastML package and running the EncodeCategorical function from fastML to handle the process of categorial encoding</span><span id="7deb" class="ne md it na b gy no ng l nh ni">from fastML import EncodeCategorical<br/>Y = EncodeCategorical(Y)</span></pre><p id="7ca4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将所需的 test_size 值赋给变量“size”。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="cbaa" class="ne md it na b gy nf ng l nh ni">size = 0.3</span></pre><p id="0c71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后要做的事情是了解我们想要用来测试数据的所有算法，并将它们全部导入到我们的项目中。fastML 附带了一个用 keras 构建的准备好的神经网络分类器，用于深度学习分类。您可以将包括神经网络分类器在内的所有算法导入到您的项目中。例如:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="a7d6" class="ne md it na b gy nf ng l nh ni">##importing the desired algorithms into our project</span><span id="e0cc" class="ne md it na b gy no ng l nh ni">from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.svm import SVC</span><span id="02b2" class="ne md it na b gy no ng l nh ni">##importing the neural net classifier from fastML</span><span id="84ad" class="ne md it na b gy no ng l nh ni">from nnclassifier import neuralnet</span></pre><p id="7ce7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们运行主要的 fastML 函数。该功能具有灵活性，允许您自由调整任何单个算法的超参数。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="b6d2" class="ne md it na b gy nf ng l nh ni">## running the fastML function from fastML to run multiple classification algorithms on the given data</span><span id="032f" class="ne md it na b gy no ng l nh ni">fastML(X, Y, size, SVC(), RandomForestClassifier(), DecisionTreeClassifier(), KNeighborsClassifier(), LogisticRegression(max_iter = 7000), special_classifier_epochs=200,special_classifier_nature ='fixed',          include_special_classifier = True)</span></pre><p id="196c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是运行主 fastML 函数后的类似输出:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="b35a" class="ne md it na b gy nf ng l nh ni">Using TensorFlow backend.<br/><br/>    <br/>   __          _   __  __ _      <br/>  / _|        | | |  \/  | |     <br/> | |_ __ _ ___| |_| \  / | |        <br/> |  _/ _` / __| __| |\/| | |     <br/> | || (_| \__ \ |_| |  | | |____ <br/> |_| \__,_|___/\__|_|  |_|______|<br/>                                 <br/>                                 <br/><br/>____________________________________________________<br/>____________________________________________________<br/>Accuracy Score for SVC is <br/>0.9811320754716981<br/><br/><br/>Confusion Matrix for SVC is <br/>[[16  0  0]<br/> [ 0 20  1]<br/> [ 0  0 16]]<br/><br/><br/>Classification Report for SVC is <br/>              precision    recall  f1-score   support<br/><br/>           0       1.00      1.00      1.00        16<br/>           1       1.00      0.95      0.98        21<br/>           2       0.94      1.00      0.97        16<br/><br/>    accuracy                           0.98        53<br/>   macro avg       0.98      0.98      0.98        53<br/>weighted avg       0.98      0.98      0.98        53<br/><br/><br/><br/>____________________________________________________<br/>____________________________________________________<br/>____________________________________________________<br/>____________________________________________________<br/>Accuracy Score for RandomForestClassifier is <br/>0.9622641509433962<br/><br/><br/>Confusion Matrix for RandomForestClassifier is <br/>[[16  0  0]<br/> [ 0 20  1]<br/> [ 0  1 15]]<br/><br/><br/>Classification Report for RandomForestClassifier is <br/>              precision    recall  f1-score   support<br/><br/>           0       1.00      1.00      1.00        16<br/>           1       0.95      0.95      0.95        21<br/>           2       0.94      0.94      0.94        16<br/><br/>    accuracy                           0.96        53<br/>   macro avg       0.96      0.96      0.96        53<br/>weighted avg       0.96      0.96      0.96        53<br/><br/><br/><br/>____________________________________________________<br/>____________________________________________________<br/>____________________________________________________<br/>____________________________________________________<br/>Accuracy Score for DecisionTreeClassifier is <br/>0.9622641509433962<br/><br/><br/>Confusion Matrix for DecisionTreeClassifier is <br/>[[16  0  0]<br/> [ 0 20  1]<br/> [ 0  1 15]]<br/><br/><br/>Classification Report for DecisionTreeClassifier is <br/>              precision    recall  f1-score   support<br/><br/>           0       1.00      1.00      1.00        16<br/>           1       0.95      0.95      0.95        21<br/>           2       0.94      0.94      0.94        16<br/><br/>    accuracy                           0.96        53<br/>   macro avg       0.96      0.96      0.96        53<br/>weighted avg       0.96      0.96      0.96        53<br/><br/><br/><br/>____________________________________________________<br/>____________________________________________________<br/>____________________________________________________<br/>____________________________________________________<br/>Accuracy Score for KNeighborsClassifier is <br/>0.9811320754716981<br/><br/><br/>Confusion Matrix for KNeighborsClassifier is <br/>[[16  0  0]<br/> [ 0 20  1]<br/> [ 0  0 16]]<br/><br/><br/>Classification Report for KNeighborsClassifier is <br/>              precision    recall  f1-score   support<br/><br/>           0       1.00      1.00      1.00        16<br/>           1       1.00      0.95      0.98        21<br/>           2       0.94      1.00      0.97        16<br/><br/>    accuracy                           0.98        53<br/>   macro avg       0.98      0.98      0.98        53<br/>weighted avg       0.98      0.98      0.98        53<br/><br/><br/><br/>____________________________________________________<br/>____________________________________________________<br/>____________________________________________________<br/>____________________________________________________<br/>Accuracy Score for LogisticRegression is <br/>0.9811320754716981<br/><br/><br/>Confusion Matrix for LogisticRegression is <br/>[[16  0  0]<br/> [ 0 20  1]<br/> [ 0  0 16]]<br/><br/><br/>Classification Report for LogisticRegression is <br/>              precision    recall  f1-score   support<br/><br/>           0       1.00      1.00      1.00        16<br/>           1       1.00      0.95      0.98        21<br/>           2       0.94      1.00      0.97        16<br/><br/>    accuracy                           0.98        53<br/>   macro avg       0.98      0.98      0.98        53<br/>weighted avg       0.98      0.98      0.98        53<br/><br/><br/><br/>____________________________________________________<br/>____________________________________________________<br/>Included special classifier with fixed nature<br/>Model: "sequential_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense_1 (Dense)              (None, 4)                 20        <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 16)                80        <br/>_________________________________________________________________<br/>dense_3 (Dense)              (None, 3)                 51        <br/>=================================================================<br/>Total params: 151<br/>Trainable params: 151<br/>Non-trainable params: 0<br/>_________________________________________________________________<br/>Train on 97 samples, validate on 53 samples<br/>Epoch 1/200<br/>97/97 [==============================] - 0s 1ms/step - loss: 1.0995 - accuracy: 0.1443 - val_loss: 1.1011 - val_accuracy: 0.3019<br/>97/97 [==============================] - 0s 63us/step - loss: 0.5166 - accuracy: 0.7010 - val_loss: 0.5706 - val_accuracy: 0.6038<br/>Epoch 100/200<br/>97/97 [==============================] - 0s 88us/step - loss: 0.5128 - accuracy: 0.7010 - val_loss: 0.5675 - val_accuracy: 0.6038<br/>Epoch 200/200<br/>97/97 [==============================] - 0s 79us/step - loss: 0.3375 - accuracy: 0.8969 - val_loss: 0.3619 - val_accuracy: 0.9057<br/>97/97 [==============================] - 0s 36us/step<br/>____________________________________________________<br/>____________________________________________________<br/>Accuracy Score for neuralnet is <br/>0.8969072103500366<br/><br/><br/>Confusion Matrix for neuralnet is <br/>[[16  0  0]<br/> [ 0 16  5]<br/> [ 0  0 16]]<br/><br/><br/>Classification Report for neuralnet is <br/>              precision    recall  f1-score   support<br/><br/>           0       1.00      1.00      1.00        16<br/>           1       1.00      0.76      0.86        21<br/>           2       0.76      1.00      0.86        16<br/><br/>    accuracy                           0.91        53<br/>   macro avg       0.92      0.92      0.91        53<br/>weighted avg       0.93      0.91      0.91        53<br/><br/><br/><br/>____________________________________________________<br/>____________________________________________________<br/>                    Model            Accuracy<br/>0                     SVC  0.9811320754716981<br/>1  RandomForestClassifier  0.9622641509433962<br/>2  DecisionTreeClassifier  0.9622641509433962<br/>3    KNeighborsClassifier  0.9811320754716981<br/>4      LogisticRegression  0.9811320754716981<br/>5               neuralnet  0.8969072103500366</span></pre><p id="a933" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这个输出，我们可以确定什么算法最适合我们的用例，并选择该算法进行进一步的开发和部署。</p><p id="12b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">fastML 是免费开源的，你可以在<a class="ae ky" href="https://github.com/Team-fastML/fastML" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到源代码和测试文件。当你在使用 fastML 的过程中遇到问题或错误时，我们的贡献者团队可以随时提供帮助。此外，您可以提交您希望我们实施的 bug 或功能更新问题，我们会完成它。检查项目，如果你喜欢这个项目，别忘了留下一颗星。</p></div></div>    
</body>
</html>