<html>
<head>
<title>Your First Apache Spark ML Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你的第一个Apache Spark ML模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/your-first-apache-spark-ml-model-d2bb82b599dd?source=collection_archive---------0-----------------------#2020-06-17">https://towardsdatascience.com/your-first-apache-spark-ml-model-d2bb82b599dd?source=collection_archive---------0-----------------------#2020-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a00b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何用Apache Spark和Python构建一个基本的机器学习模型？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fc4ec15136a23c33b87b81ac590be2a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ukzT2NYA_t3d3_37GfVz0A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Héizel Vázquez插图</p></figure><p id="273d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我爱阿帕奇火花。这是我用于机器学习和数据科学的首批框架之一。在过去的几年里，它一直在稳步增长，我们已经接近它的第三个版本。我们预期的变化主要是在查询和流程的优化方面，所以API不会有太大的变化。这意味着你在这篇文章中学到的东西会在一段时间内起作用(我们不知道会持续多久，因为生活很微妙)。</p><p id="8ebe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章分为三个部分:</p><ul class=""><li id="b8e7" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">Apache Spark的基础知识</li><li id="47ac" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">安装和使用Apache Spark</li><li id="d565" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">创建您的第一个Apache Spark机器学习模型</li></ul><p id="688d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">前两节中您将看到的许多内容来自我以前写的两篇关于Apache Spark的文章:</p><div class="mg mh gp gr mi mj"><a rel="noopener follow" target="_blank" href="/deep-learning-with-apache-spark-part-1-6d397c16abd"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd ir gy z fp mo fr fs mp fu fw ip bi translated">使用Apache Spark进行深度学习—第1部分</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">第一部分全面讨论了如何使用Apache Spark进行分布式深度学习。这一部分:什么是火花…</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx kp mj"/></div></div></a></div><div class="mg mh gp gr mi mj"><a rel="noopener follow" target="_blank" href="/how-to-use-pyspark-on-your-computer-9c7180075617"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd ir gy z fp mo fr fs mp fu fw ip bi translated">如何在你的电脑上使用PySpark</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">我发现在您的本地上开始使用Apache Spark(这里将重点介绍PySpark)有点困难…</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="my l mu mv mw ms mx kp mj"/></div></div></a></div><p id="1e3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是是时候更新了:)</p><h1 id="ef1b" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">Apache Spark的基础知识</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d90828e7278e4724d87e25952237cc04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BLEa-ukD8olN0VfNET-zqg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://www.freepik.com/free-vector/background-several-colored-pieces_1174026.htm#page=1&amp;query=blocks&amp;position=10" rel="noopener ugc nofollow" target="_blank">弗里皮克</a>配合<a class="ae kv" href="https://instagram.com/heizelvazquez" rel="noopener ugc nofollow" target="_blank">伊泽尔·巴斯克斯</a></p></figure><p id="7d38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">几年前，Apache Spark被其创建者定义为:</p><blockquote class="nr"><p id="62fb" class="ns nt iq bd nu nv nw nx ny nz oa lr dk translated">大规模数据处理的快速通用引擎。</p></blockquote><p id="9289" class="pw-post-body-paragraph kw kx iq ky b kz ob jr lb lc oc ju le lf od lh li lj oe ll lm ln of lp lq lr ij bi translated">“<strong class="ky ir">快速”</strong>部分意味着它比以前的方法更快地处理大数据，如经典的MapReduce。更快的秘密是Spark在内存(RAM)上运行，这使得处理速度比在磁盘上快得多。</p><p id="8048" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“<strong class="ky ir">通用”</strong>部分意味着它可以用于多种用途，比如运行分布式SQL、创建数据管道、将数据摄取到数据库中、运行机器学习算法、处理图形、数据流等等。</p><p id="8ad2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在他们把定义改成了:</p><blockquote class="nr"><p id="e850" class="ns nt iq bd nu nv nw nx ny nz oa lr dk translated">Apache Spark是一个用于大规模数据处理的统一分析引擎。</p></blockquote><p id="57c7" class="pw-post-body-paragraph kw kx iq ky b kz ob jr lb lc oc ju le lf od lh li lj oe ll lm ln of lp lq lr ij bi translated">我们仍然有通用部分，但现在它的范围更广了，有了单词“<strong class="ky ir"> unified，”</strong>，这是为了解释它可以在数据科学或机器学习工作流中做几乎所有的事情。对于端到端的项目，可以单独使用Spark框架。</p><p id="f5ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">“大规模”</strong>部分意味着这是一个可以完美处理大量数据的框架，我们过去称之为“大数据”(有趣的是事物变化的速度)。</p><h1 id="1c43" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">RDD</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/e165a8f49dd414ad7c52264e917357f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/0*BNi7E0frdDd_k_Mh.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">杰弗里·汤普森的PySpark图片。</p></figure><p id="bcc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Apache Spark的核心抽象和开端是弹性分布式数据集(RDD)。</p><p id="ad9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">RDD是可以并行操作的容错元素集合。您可以在驱动程序中并行化现有集合，或者引用外部存储系统中的数据集来创建它们，例如共享文件系统、HDFS、HBase或任何提供Hadoop InputFormat的数据源。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/5320d8661afa5f73ac3da438887917de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7Nct0LsVLSneDlEI.png"/></div></div></figure><p id="be4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于Spark，有一点非常重要，那就是所有的<strong class="ky ir">转换</strong>都是惰性的，这意味着它们不会马上计算出结果。相反，Spark会记住应用于某个基本数据集(如文件)的转换。只有当一个<strong class="ky ir">动作</strong>需要将结果返回给驱动程序时，才会计算转换。</p><p id="68f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">默认情况下，每次对变换后的RDD执行操作时，都会对其进行重新计算。然而，您也可以使用持久化(或缓存)方法在内存中<em class="oi">持久化</em>一个RDD，在这种情况下，Spark会将元素保留在集群上，以便下次查询时可以更快地访问。还支持在磁盘上持久化rdd或跨多个节点复制rdd。</p><h1 id="7f32" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">数据框架</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b0ec2c4c771a5eb01443ae390f81b2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/0*NurazIrmziyOBS8a.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">杰弗里·汤普森的PySpark图片。</p></figure><p id="d020" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从Spark 2.0.0开始，数据帧就是一个被组织成命名列的<em class="oi">数据集</em>。它在概念上相当于关系数据库中的一个表或R/Python中的一个数据帧，但是在底层有更丰富的优化。</p><p id="f791" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据帧可以由各种各样的<a class="ae kv" href="https://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources" rel="noopener ugc nofollow" target="_blank">源</a>构建，例如:结构化数据文件、Hive中的表、外部数据库或现有的rdd。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/31c7a82355d585389de6ac9d027b85fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*flAF9ghyCVpURedV.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://aspgems.com/blog/big-data/migrando-de-pandas-spark-dataframes" rel="noopener ugc nofollow" target="_blank">https://aspgems . com/blog/big-data/migrando-de-pandas-spark-data frames</a></p></figure><p id="adb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简而言之，Dataframes API是Spark创造者简化框架中数据处理的方法。它们非常类似于Pandas数据帧或R数据帧，但有几个优点。首先，它们可以分布在一个集群中，因此可以处理大量数据；其次，它们经过了优化。</p><p id="0aed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是社区迈出的非常重要的一步。到了2014年，用Spark搭配Scala或者Java，速度快了很多，整个Spark世界都因为性能变成了Scala。但是有了DF API，这不再是一个问题，现在您可以在R、Python、Scala或Java中使用它获得相同的性能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/5bbd8b62b2990bc58fe46806fa91d84f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5jZRcRICD9PTFoTt.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" rel="noopener ugc nofollow" target="_blank">数据块</a></p></figure><p id="b6c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">负责这种优化的是催化剂。你可以把它想象成一个向导，它会接受你的查询(哦，是的！，您可以在Spark中运行类似SQL的查询，在DF上运行它们，它们也会被并行化)和您的操作，并创建一个优化的计划来分配计算。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/0d4bce7105d6a16fe20c67e632c517d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iXAJTUbzHC8o8Qof.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html" rel="noopener ugc nofollow" target="_blank">数据砖块</a></p></figure><p id="4912" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">过程没那么简单，但是作为程序员的你根本不会注意到。现在它一直在那里帮助你。</p><p id="0b98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Spark 3.0中，我们将获得一种称为“自适应查询执行”(AQE)的东西，它将根据查询执行过程中收集的运行时统计数据来重新优化和调整查询计划。这将对性能产生巨大影响，例如<a class="ae kv" href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html" rel="noopener ugc nofollow" target="_blank">的例子</a>假设我们正在运行查询</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="54ce" class="os na iq oo b gy ot ou l ov ow">SELECT max(i) FROM table GROUP BY column</span></pre><p id="aed4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">没有AQE，Spark将启动五个任务进行最终聚合:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/e1ab0b4958cdd60aa4c2b8eea14a5485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MHZhfGEGUolRZS5s.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html" rel="noopener ugc nofollow" target="_blank">https://databricks . com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-SQL-at-runtime . html</a></p></figure><p id="a97b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是有了AQE，Spark将把这三个小分区合并成一个，因此，最终的聚合现在只需要执行三个任务，而不是五个:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/3bb47213c04c0f4ae18aa385092a28e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*SjyMHZ5QhGmlOW5R.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html" rel="noopener ugc nofollow" target="_blank">https://databricks . com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-SQL-at-runtime . html</a></p></figure><p id="5c75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个新版本中，Spark将解决一个大问题:基于成本的优化。如果你想了解更多，请查看上面两张图片中的链接。</p><p id="5be3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在接下来的会议中，我们将看到更多关于Spark及其机器学习(ML)库的内容。</p><h1 id="0e54" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">安装和使用Apache Spark</h1><p id="213a" class="pw-post-body-paragraph kw kx iq ky b kz oz jr lb lc pa ju le lf pb lh li lj pc ll lm ln pd lp lq lr ij bi translated"><strong class="ky ir"> <em class="oi">备注:</em> </strong></p><ul class=""><li id="e2a7" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir"><em class="oi">“$”符号将表示在shell中运行(但不要复制该符号)。</em> </strong></li><li id="3807" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">&gt;&gt;&gt;符号表示Python外壳(不要复制符号)。</strong></li></ul><p id="d63e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在本地安装PySpark，然后使用Jupyter来处理它。使用Spark还有更多方法，如果你想了解更多，请查看这篇文章。</p><h2 id="0710" class="os na iq bd nb pe pf dn nf pg ph dp nj lf pi pj nl lj pk pl nn ln pm pn np po bi translated">安装PySpark</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pp"><img src="../Images/a80e10d89603fb4165a51589b1cc1fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DWeusB9iZH4FtzRqiSqBg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://www.freepik.com/free-vector/website-setup-illustration-concept_6193236.htm#page=1&amp;query=install&amp;position=12" rel="noopener ugc nofollow" target="_blank"> Freepik </a></p></figure><p id="9023" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PySpark，可以想象，就是Apache Spark的Python API。这是我们使用Python与框架交互的方式。安装非常简单。这些是步骤:</p><ul class=""><li id="55be" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">在计算机上安装Java 8或更高版本。</li><li id="faaa" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">安装Python(我推荐&gt;来自<a class="ae kv" href="https://www.continuum.io/downloads" rel="noopener ugc nofollow" target="_blank"> Anaconda </a>的Python 3.6)</li><li id="fc12" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">安装PySpark:</li></ul><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="86f5" class="os na iq oo b gy ot ou l ov ow">$ pip3 install pyspark</span></pre><p id="9d14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">目前的默认版本是3.0.0，这是实验性的，但是它应该适用于我们的实验。</p><p id="a354" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要测试您的安装，请转到您的终端，然后打开Python。然后写:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="8022" class="os na iq oo b gy ot ou l ov ow">&gt;&gt;&gt; import pyspark</span></pre><p id="dfa8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你没有得到一个错误，你就在正确的道路上。要检查Spark write的安装版本:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="267d" class="os na iq oo b gy ot ou l ov ow">pyspark.__version__</span></pre><p id="db2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你应该得到‘3 . 0 . 0’。</p><ul class=""><li id="1953" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">在Python笔记本中测试Spark:创建一个新的Jupyter笔记本，编写和以前一样的内容。<strong class="ky ir">如果出现错误，直接从笔记本上安装PySpark。下面是</strong>一款笔记本电脑实现这一功能的要点:</li></ul><h1 id="3bcc" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">创建您的第一个Apache Spark ML模型</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/398df7d15341e08f7261171a24727864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C-Wug1UhbXb3whDuXdyMnA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://www.freepik.com/free-vector/programmer-concept-illustration_8611162.htm#page=1&amp;query=programming&amp;position=6" rel="noopener ugc nofollow" target="_blank">免费版</a></p></figure><p id="cddb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Spark的机器学习库叫做MLlib(机器学习库)。它在很大程度上基于Scikit-learn关于管道的想法。在本库中，创建ML模型的基本概念是:</p><ul class=""><li id="1558" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">DataFrame:这个ML API使用Spark SQL中的DataFrame作为ML数据集，可以保存各种数据类型。例如，数据帧可以具有存储文本、特征向量、真实标签和预测的不同列。</li><li id="4f2c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">转换器:转换器是一种可以将一个数据帧转换成另一个数据帧的算法。例如，ML模型是将具有特征的数据帧转换成具有预测的数据帧的转换器。</li><li id="7697" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">估计器:估计器是一种算法，它可以适合一个数据帧来产生一个转换器。例如，学习算法是在数据帧上训练并产生模型的估计器</li><li id="dd55" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">管道:管道将多个转换器和评估器链接在一起，以指定ML工作流</li><li id="dd18" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">参数:所有的转换器和估算器现在共享一个公共的API来指定参数。</li></ul><p id="8b4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想知道更多关于API和它们如何工作的信息，请查看官方文档<a class="ae kv" href="https://spark.apache.org/docs/latest/ml-pipeline.html" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="d80a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个例子，我们将使用一个非常基本的数据集。泰坦尼克号数据集，希望你们都熟悉这个案例和数据。首先，我们必须下载数据，为此我们使用Kaggle:</p><div class="mg mh gp gr mi mj"><a href="https://www.kaggle.com/c/titanic/data" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd ir gy z fp mo fr fs mp fu fw ip bi translated">泰坦尼克号:机器从灾难中学习</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">从这里开始！预测泰坦尼克号上的生存并熟悉ML基础知识</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">www.kaggle.com</p></div></div><div class="ms l"><div class="pq l mu mv mw ms mx kp mj"/></div></div></a></div><p id="09d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">只需下载“train.csv”文件就可以了:)。</p><p id="dbc5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将根据乘客的特征来预测其是否幸存。</p></div><div class="ab cl pr ps hu pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="ij ik il im in"><p id="5790" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">启动前:确保关闭并停止所有其他Spark笔记本。当使用多个Spark实例时，Java有时会报错。</strong></p></div><div class="ab cl pr ps hu pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="ij ik il im in"><h2 id="674b" class="os na iq bd nb pe pf dn nf pg ph dp nj lf pi pj nl lj pk pl nn ln pm pn np po bi translated">将数据加载到Spark</h2><p id="777d" class="pw-post-body-paragraph kw kx iq ky b kz oz jr lb lc pa ju le lf pb lh li lj pc ll lm ln pd lp lq lr ij bi translated">为了加载数据，我们使用Spark DataFrames。它比熊猫稍微复杂一点。不能只做“导入-&gt; read_csv()”。您首先需要启动一个Spark会话，来完成该写操作:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="87a0" class="os na iq oo b gy ot ou l ov ow">from pyspark.sql import SparkSession</span><span id="832b" class="os na iq oo b gy py ou l ov ow">spark = SparkSession \<br/>    .builder \<br/>    .appName('Titanic Data') \<br/>    .getOrCreate()</span></pre><p id="242c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在如果你写:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="af9d" class="os na iq oo b gy ot ou l ov ow">spark</span></pre><p id="b373" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在您的笔记本上，您应该会看到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pz"><img src="../Images/77f1e0fa7dd56bbe8df59f7d229a1f8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HprjCLyDXItiH8qVXrpmgQ.png"/></div></div></figure><p id="0b77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着您在所有内核(即*)上本地使用Spark，版本为3.0.0，会话名称为“Titanic Data”。认为“Spark UI”在你使用Spark时会很有用，如果你点击它，你会看到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qa"><img src="../Images/c43c93b0fb894f22cc15bcd516172401.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1kWO_WHjsRJ-z_doLzIBQ.png"/></div></div></figure><p id="29d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我没有时间详细解释UI，但如果你想了解更多，请告诉我:)。</p><p id="fdf3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">酷！现在我们已经准备好读取数据了。为此，请编写:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="d681" class="os na iq oo b gy ot ou l ov ow">df = (spark.read<br/>          .format("csv")<br/>          .option('header', 'true')<br/>          .load("train.csv"))</span></pre><p id="d50c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！您已经创建了您的第一个Spark数据框架。要查看数据帧的内部，请编写:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="4ba2" class="os na iq oo b gy ot ou l ov ow">df.show(5)</span></pre><p id="213d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你会得到一些不漂亮但至少有用的东西:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="6732" class="os na iq oo b gy ot ou l ov ow">+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+<br/>|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|<br/>+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+<br/>|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| null|       S|<br/>|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|<br/>|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| null|       S|<br/>|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|<br/>|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05| null|       S|<br/>+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+<br/>only showing top 5 rows</span></pre><p id="d278" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是的，我知道，没那么漂亮。使用Python的一个好处是，你可以很容易地与熊猫互动。为了以更漂亮的格式显示我们的数据，您可以写:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="f0a1" class="os na iq oo b gy ot ou l ov ow">df.toPandas()</span></pre><p id="cd84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您将获得:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qb"><img src="../Images/78fba0b3afbdad67b92c9107f9325bc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JVFrH2H9qffhQSG78neZ0w.png"/></div></div></figure><p id="d833" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是小心我的朋友们！！！只有当您处理的数据足够小时，您才能这样做，因为当您执行“toPandas()”时，您将一次获得所有数据，并且数据不会分布在该单元格中。所以它必须适合内存。</p><h2 id="c066" class="os na iq bd nb pe pf dn nf pg ph dp nj lf pi pj nl lj pk pl nn ln pm pn np po bi translated">检查关于您的数据的信息</h2><p id="245b" class="pw-post-body-paragraph kw kx iq ky b kz oz jr lb lc pa ju le lf pb lh li lj pc ll lm ln pd lp lq lr ij bi translated">有一些基本函数可以从数据集中获取更多信息。在下一段代码中，我加入了基本的代码(<strong class="ky ir"> - &gt;表示计算的结果</strong>):</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="1954" class="os na iq oo b gy ot ou l ov ow"># How many rows we have<br/>df.count()<br/>-&gt; 891</span><span id="e32f" class="os na iq oo b gy py ou l ov ow"># The names of our columns<br/>df.columns<br/>-&gt; ['PassengerId',<br/> 'Survived',<br/> 'Pclass',<br/> 'Name',<br/> 'Sex',<br/> 'Age',<br/> 'SibSp',<br/> 'Parch',<br/> 'Ticket',<br/> 'Fare',<br/> 'Cabin',<br/> 'Embarked']</span><span id="e594" class="os na iq oo b gy py ou l ov ow"># Types of our columns<br/>df.dtypes<br/>-&gt; [('PassengerId', 'string'),<br/> ('Survived', 'string'),<br/> ('Pclass', 'string'),<br/> ('Name', 'string'),<br/> ('Sex', 'string'),<br/> ('Age', 'string'),<br/> ('SibSp', 'string'),<br/> ('Parch', 'string'),<br/> ('Ticket', 'string'),<br/> ('Fare', 'string'),<br/> ('Cabin', 'string'),<br/> ('Embarked', 'string')]</span></pre><p id="36ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后是一些关于我们数据的统计数据:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="c058" class="os na iq oo b gy ot ou l ov ow"># Basics stats from our columns<br/>df.describe().toPandas()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qb"><img src="../Images/d404f1f0b25bef6cb7d3035c44cdd237.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v5Rl6amtKUZnPS90P_bSGQ.png"/></div></div></figure><h2 id="1531" class="os na iq bd nb pe pf dn nf pg ph dp nj lf pi pj nl lj pk pl nn ln pm pn np po bi translated">数据准备和特征工程</h2><p id="7dd4" class="pw-post-body-paragraph kw kx iq ky b kz oz jr lb lc pa ju le lf pb lh li lj pc ll lm ln pd lp lq lr ij bi translated">我们从上面的数据探索中注意到的一件事是，所有的列都是字符串类型。但这似乎不对。其中一些应该是数字。所以我们打算让他们出演。此外，由于时间关系，我只选择了几个变量进行建模，因此我们不必处理整个数据集:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="f87c" class="os na iq oo b gy ot ou l ov ow">from pyspark.sql.functions import col</span><span id="b3fa" class="os na iq oo b gy py ou l ov ow">dataset = df.select(col('Survived').cast('float'),<br/>                         col('Pclass').cast('float'),<br/>                         col('Sex'),<br/>                         col('Age').cast('float'),<br/>                         col('Fare').cast('float'),<br/>                         col('Embarked')<br/>                        )</span><span id="7e60" class="os na iq oo b gy py ou l ov ow">dataset.show()</span></pre><p id="7eb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，你会得到这个数据集:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="e506" class="os na iq oo b gy ot ou l ov ow">+--------+------+------+----+-------+--------+<br/>|Survived|Pclass|   Sex| Age|   Fare|Embarked|<br/>+--------+------+------+----+-------+--------+<br/>|     0.0|   3.0|  male|22.0|   7.25|       S|<br/>|     1.0|   1.0|female|38.0|71.2833|       C|<br/>|     1.0|   3.0|female|26.0|  7.925|       S|<br/>|     1.0|   1.0|female|35.0|   53.1|       S|<br/>|     0.0|   3.0|  male|35.0|   8.05|       S|<br/>|     0.0|   3.0|  male|null| 8.4583|       Q|<br/>|     0.0|   1.0|  male|54.0|51.8625|       S|<br/>|     0.0|   3.0|  male| 2.0| 21.075|       S|<br/>|     1.0|   3.0|female|27.0|11.1333|       S|<br/>|     1.0|   2.0|female|14.0|30.0708|       C|<br/>|     1.0|   3.0|female| 4.0|   16.7|       S|<br/>|     1.0|   1.0|female|58.0|  26.55|       S|<br/>|     0.0|   3.0|  male|20.0|   8.05|       S|<br/>|     0.0|   3.0|  male|39.0| 31.275|       S|<br/>|     0.0|   3.0|female|14.0| 7.8542|       S|<br/>|     1.0|   2.0|female|55.0|   16.0|       S|<br/>|     0.0|   3.0|  male| 2.0| 29.125|       Q|<br/>|     1.0|   2.0|  male|null|   13.0|       S|<br/>|     0.0|   3.0|female|31.0|   18.0|       S|<br/>|     1.0|   3.0|female|null|  7.225|       C|<br/>+--------+------+------+----+-------+--------+</span></pre><p id="57a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们运行这个:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="c738" class="os na iq oo b gy ot ou l ov ow">from pyspark.sql.functions import isnull, when, count, col</span><span id="13a6" class="os na iq oo b gy py ou l ov ow">dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()</span></pre><p id="ae42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们得到:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="5538" class="os na iq oo b gy ot ou l ov ow">+--------+------+---+---+----+--------+<br/>|Survived|Pclass|Sex|Age|Fare|Embarked|<br/>+--------+------+---+---+----+--------+<br/>|       0|     0|  0|177|   0|       2|<br/>+--------+------+---+---+----+--------+</span></pre><p id="8f19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到在一些列中也有空值，所以我们只消除它们:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="da0d" class="os na iq oo b gy ot ou l ov ow">dataset = dataset.replace('?', None)\<br/>        .dropna(how='any')</span></pre><p id="4fcf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，Spark ML库只处理数字数据。但我们还是想用《性》和《登船》专栏。为此，我们需要对它们进行编码。为此，让我们使用一个叫做<a class="ae kv" href="https://spark.apache.org/docs/latest/ml-features#stringindexer" rel="noopener ugc nofollow" target="_blank"> StringIndexer </a>的东西:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="229d" class="os na iq oo b gy ot ou l ov ow">from pyspark.ml.feature import StringIndexer</span><span id="d99a" class="os na iq oo b gy py ou l ov ow">dataset = StringIndexer(<br/>    inputCol='Sex', <br/>    outputCol='Gender', <br/>    handleInvalid='keep').fit(dataset).transform(dataset)</span><span id="3dd8" class="os na iq oo b gy py ou l ov ow">dataset = StringIndexer(<br/>    inputCol='Embarked', <br/>    outputCol='Boarded', <br/>    handleInvalid='keep').fit(dataset).transform(dataset)</span><span id="4fad" class="os na iq oo b gy py ou l ov ow">dataset.show()</span></pre><p id="58d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你这样做，你会得到:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="7199" class="os na iq oo b gy ot ou l ov ow">+--------+------+------+----+-------+--------+------+-------+<br/>|Survived|Pclass|   Sex| Age|   Fare|Embarked|Gender|Boarded|<br/>+--------+------+------+----+-------+--------+------+-------+<br/>|     0.0|   3.0|  male|22.0|   7.25|       S|   0.0|    0.0|<br/>|     1.0|   1.0|female|38.0|71.2833|       C|   1.0|    1.0|<br/>|     1.0|   3.0|female|26.0|  7.925|       S|   1.0|    0.0|<br/>|     1.0|   1.0|female|35.0|   53.1|       S|   1.0|    0.0|<br/>|     0.0|   3.0|  male|35.0|   8.05|       S|   0.0|    0.0|<br/>|     0.0|   3.0|  male|null| 8.4583|       Q|   0.0|    2.0|<br/>|     0.0|   1.0|  male|54.0|51.8625|       S|   0.0|    0.0|<br/>|     0.0|   3.0|  male| 2.0| 21.075|       S|   0.0|    0.0|<br/>|     1.0|   3.0|female|27.0|11.1333|       S|   1.0|    0.0|<br/>|     1.0|   2.0|female|14.0|30.0708|       C|   1.0|    1.0|<br/>|     1.0|   3.0|female| 4.0|   16.7|       S|   1.0|    0.0|<br/>|     1.0|   1.0|female|58.0|  26.55|       S|   1.0|    0.0|<br/>|     0.0|   3.0|  male|20.0|   8.05|       S|   0.0|    0.0|<br/>|     0.0|   3.0|  male|39.0| 31.275|       S|   0.0|    0.0|<br/>|     0.0|   3.0|female|14.0| 7.8542|       S|   1.0|    0.0|<br/>|     1.0|   2.0|female|55.0|   16.0|       S|   1.0|    0.0|<br/>|     0.0|   3.0|  male| 2.0| 29.125|       Q|   0.0|    2.0|<br/>|     1.0|   2.0|  male|null|   13.0|       S|   0.0|    0.0|<br/>|     0.0|   3.0|female|31.0|   18.0|       S|   1.0|    0.0|<br/>|     1.0|   3.0|female|null|  7.225|       C|   1.0|    1.0|<br/>+--------+------+------+----+-------+--------+------+-------+<br/>only showing top 20 rows</span></pre><p id="70bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，我们创建了两个新列“性别”和“登船”，包含与“性别”和“登船”相同的信息，但现在它们是数字。让我们对数据类型做最后一次检查:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="f981" class="os na iq oo b gy ot ou l ov ow">dataset.dtypes</span><span id="2867" class="os na iq oo b gy py ou l ov ow">-&gt; [('Survived', 'float'),<br/> ('Pclass', 'float'),<br/> ('Sex', 'string'),<br/> ('Age', 'float'),<br/> ('Fare', 'float'),<br/> ('Embarked', 'string'),<br/> ('Gender', 'double'),<br/> ('Boarded', 'double')]</span></pre><p id="029d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以我们想要的所有列都是数字。我们现在必须删除旧列“Sex”和“apolloed ”,因为我们不会使用它们:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="4907" class="os na iq oo b gy ot ou l ov ow"># Drop unnecessary columns<br/>dataset = dataset.drop('Sex')<br/>dataset = dataset.drop('Embarked')</span><span id="ba1b" class="os na iq oo b gy py ou l ov ow">dataset.show()</span></pre><p id="faa0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您将获得:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="39b9" class="os na iq oo b gy ot ou l ov ow">+--------+------+----+-------+------+-------+<br/>|Survived|Pclass| Age|   Fare|Gender|Boarded|<br/>+--------+------+----+-------+------+-------+<br/>|     0.0|   3.0|22.0|   7.25|   0.0|    0.0|<br/>|     1.0|   1.0|38.0|71.2833|   1.0|    1.0|<br/>|     1.0|   3.0|26.0|  7.925|   1.0|    0.0|<br/>|     1.0|   1.0|35.0|   53.1|   1.0|    0.0|<br/>|     0.0|   3.0|35.0|   8.05|   0.0|    0.0|<br/>|     0.0|   1.0|54.0|51.8625|   0.0|    0.0|<br/>|     0.0|   3.0| 2.0| 21.075|   0.0|    0.0|<br/>|     1.0|   3.0|27.0|11.1333|   1.0|    0.0|<br/>|     1.0|   2.0|14.0|30.0708|   1.0|    1.0|<br/>|     1.0|   3.0| 4.0|   16.7|   1.0|    0.0|<br/>|     1.0|   1.0|58.0|  26.55|   1.0|    0.0|<br/>|     0.0|   3.0|20.0|   8.05|   0.0|    0.0|<br/>|     0.0|   3.0|39.0| 31.275|   0.0|    0.0|<br/>|     0.0|   3.0|14.0| 7.8542|   1.0|    0.0|<br/>|     1.0|   2.0|55.0|   16.0|   1.0|    0.0|<br/>|     0.0|   3.0| 2.0| 29.125|   0.0|    2.0|<br/>|     0.0|   3.0|31.0|   18.0|   1.0|    0.0|<br/>|     0.0|   2.0|35.0|   26.0|   0.0|    0.0|<br/>|     1.0|   2.0|34.0|   13.0|   0.0|    0.0|<br/>|     1.0|   3.0|15.0| 8.0292|   1.0|    2.0|<br/>+--------+------+----+-------+------+-------+</span></pre><p id="8911" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在进入机器学习部分之前，只剩下一步了。Spark实际上是用一个列来进行预测的，这个列将所有的特性组合成一个类似列表的结构。例如，如果您具有以下功能:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="a162" class="os na iq oo b gy ot ou l ov ow">+--------+------+----+-------+------+-------+<br/>|Survived|Pclass| Age|   Fare|Gender|Boarded|<br/>+--------+------+----+-------+------+-------+<br/>|     0.0|   3.0|22.0|   7.25|   0.0|    0.0|<br/>+--------+------+----+-------+------+-------+</span></pre><p id="11f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而你要预测“幸存”，就需要把“Pclass”、“年龄”、“票价”、“性别”、“登车”这几列信息组合成一列。我们通常称之为列特性，它应该是这样的:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="4894" class="os na iq oo b gy ot ou l ov ow">+--------+------+----+-------+------+-------+----------------------+<br/>|Survived|Pclass| Age|   Fare|Gender|Boarded|    features         |<br/>+--------+------+----+-------+------+-------+----------------------+<br/>|     0.0|   3.0|22.0|   7.25|   0.0|    0.0|[3.0, 22.0, 7.25, 0, 0] |<br/>+--------+------+----+-------+------+-------+----------------------+</span></pre><p id="321e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如您所看到的，新的列功能包含了我们所有功能的相同信息，但是是在一个类似列表的对象中。为了在Spark中做到这一点，我们使用了<a class="ae kv" href="https://spark.apache.org/docs/latest/ml-features#vectorassembler" rel="noopener ugc nofollow" target="_blank"> VectorAssembler </a>:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="edd1" class="os na iq oo b gy ot ou l ov ow"># Assemble all the features with VectorAssembler</span><span id="e3f2" class="os na iq oo b gy py ou l ov ow">required_features = ['Pclass',<br/>                    'Age',<br/>                    'Fare',<br/>                    'Gender',<br/>                    'Boarded'<br/>                   ]</span><span id="2bc9" class="os na iq oo b gy py ou l ov ow">from pyspark.ml.feature import VectorAssembler</span><span id="07ec" class="os na iq oo b gy py ou l ov ow">assembler = VectorAssembler(inputCols=required_features, outputCol='features')</span><span id="d987" class="os na iq oo b gy py ou l ov ow">transformed_data = assembler.transform(dataset)</span></pre><p id="73c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果我们检查我们的数据，我们有:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="2916" class="os na iq oo b gy ot ou l ov ow">+--------+------+----+-------+------+-------+--------------------+<br/>|Survived|Pclass| Age|   Fare|Gender|Boarded|            features|<br/>+--------+------+----+-------+------+-------+--------------------+<br/>|     0.0|   3.0|22.0|   7.25|   0.0|    0.0|[3.0,22.0,7.25,0....|<br/>|     1.0|   1.0|38.0|71.2833|   1.0|    1.0|[1.0,38.0,71.2833...|<br/>|     1.0|   3.0|26.0|  7.925|   1.0|    0.0|[3.0,26.0,7.92500...|<br/>|     1.0|   1.0|35.0|   53.1|   1.0|    0.0|[1.0,35.0,53.0999...|<br/>|     0.0|   3.0|35.0|   8.05|   0.0|    0.0|[3.0,35.0,8.05000...|<br/>|     0.0|   1.0|54.0|51.8625|   0.0|    0.0|[1.0,54.0,51.8624...|<br/>|     0.0|   3.0| 2.0| 21.075|   0.0|    0.0|[3.0,2.0,21.07500...|<br/>|     1.0|   3.0|27.0|11.1333|   1.0|    0.0|[3.0,27.0,11.1332...|<br/>|     1.0|   2.0|14.0|30.0708|   1.0|    1.0|[2.0,14.0,30.0708...|<br/>|     1.0|   3.0| 4.0|   16.7|   1.0|    0.0|[3.0,4.0,16.70000...|<br/>|     1.0|   1.0|58.0|  26.55|   1.0|    0.0|[1.0,58.0,26.5499...|<br/>|     0.0|   3.0|20.0|   8.05|   0.0|    0.0|[3.0,20.0,8.05000...|<br/>|     0.0|   3.0|39.0| 31.275|   0.0|    0.0|[3.0,39.0,31.2749...|<br/>|     0.0|   3.0|14.0| 7.8542|   1.0|    0.0|[3.0,14.0,7.85419...|<br/>|     1.0|   2.0|55.0|   16.0|   1.0|    0.0|[2.0,55.0,16.0,1....|<br/>|     0.0|   3.0| 2.0| 29.125|   0.0|    2.0|[3.0,2.0,29.125,0...|<br/>|     0.0|   3.0|31.0|   18.0|   1.0|    0.0|[3.0,31.0,18.0,1....|<br/>|     0.0|   2.0|35.0|   26.0|   0.0|    0.0|[2.0,35.0,26.0,0....|<br/>|     1.0|   2.0|34.0|   13.0|   0.0|    0.0|[2.0,34.0,13.0,0....|<br/>|     1.0|   3.0|15.0| 8.0292|   1.0|    2.0|[3.0,15.0,8.02919...|<br/>+--------+------+----+-------+------+-------+--------------------+<br/>only showing top 20 rows</span></pre><p id="84ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这正是我们想要的。</p><h2 id="6685" class="os na iq bd nb pe pf dn nf pg ph dp nj lf pi pj nl lj pk pl nn ln pm pn np po bi translated">建模</h2><p id="04c1" class="pw-post-body-paragraph kw kx iq ky b kz oz jr lb lc pa ju le lf pb lh li lj pc ll lm ln pd lp lq lr ij bi translated">现在是有趣的部分，对吗？不要！哈哈。建模很重要，但是如果没有前面的步骤，这是不可能的。所以在所有的步骤中享受乐趣:)</p><p id="320e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在建模之前，让我们按照惯例将训练和测试分开:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="997f" class="os na iq oo b gy ot ou l ov ow">(training_data, test_data) = transformed_data.randomSplit([0.8,0.2])</span></pre><p id="04d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好的。建模。这意味着，在这种情况下，为我们的数据集构建并拟合一个ML模型，以预测“幸存”的列和所有其他列。我们将使用一个<a class="ae kv" href="https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier" rel="noopener ugc nofollow" target="_blank">随机森林分类器</a>。这实际上是一个我们必须适应的<a class="ae kv" href="https://spark.apache.org/docs/latest/ml-pipeline.html#estimators" rel="noopener ugc nofollow" target="_blank">估计器</a>。</p><p id="c777" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这实际上是最简单的部分:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="0e56" class="os na iq oo b gy ot ou l ov ow">from pyspark.ml.classification import RandomForestClassifier</span><span id="41b9" class="os na iq oo b gy py ou l ov ow">rf = RandomForestClassifier(labelCol='Survived', <br/>                            featuresCol='features',<br/>                            maxDepth=5)</span></pre><p id="20f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们符合这个模型:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="c1bc" class="os na iq oo b gy ot ou l ov ow">model = rf.fit(training_data)</span></pre><p id="5572" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将给我们一个叫做<a class="ae kv" href="https://spark.apache.org/docs/latest/ml-pipeline.html#transformers" rel="noopener ugc nofollow" target="_blank">变压器</a>的东西。最后，我们使用测试数据集预测:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="174a" class="os na iq oo b gy ot ou l ov ow">predictions = model.transform(test_data)</span></pre><p id="69e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！你做到了。恭喜:)。你的第一个火花ML模型。现在让我们看看我们做得有多好。为此，我们将使用一个称为<a class="ae kv" href="https://en.wikipedia.org/wiki/Accuracy_and_precision" rel="noopener ugc nofollow" target="_blank">准确度</a>的基本指标:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="652e" class="os na iq oo b gy ot ou l ov ow"># Evaluate our model<br/>from pyspark.ml.evaluation import MulticlassClassificationEvaluator</span><span id="7558" class="os na iq oo b gy py ou l ov ow">evaluator = MulticlassClassificationEvaluator(<br/>    labelCol='Survived', <br/>    predictionCol='prediction', <br/>    metricName='accuracy')</span></pre><p id="b402" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们要获得我们所做的准确性:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="01fb" class="os na iq oo b gy ot ou l ov ow">accuracy = evaluator.evaluate(predictions)<br/>print('Test Accuracy = ', accuracy) -&gt; 0.843</span></pre><p id="f27d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的基本模型给出了0.843的精度。一点也不差:)。</p><p id="199f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">应该注意的是，我们必须做更多的事情来实际构建一个数据科学项目和一个好的ML模型，比如交叉验证、特征选择，我们还必须测试更多的模型，等等。</p></div><div class="ab cl pr ps hu pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="ij ik il im in"><p id="a544" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我在GitHub中创建了一个repo，包含了本文的所有代码。您可以在这里找到它:</p><div class="mg mh gp gr mi mj"><a href="https://github.com/FavioVazquez/first_spark_model" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd ir gy z fp mo fr fs mp fu fw ip bi translated">法维奥巴斯克斯/first_spark_model</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">你的第一个Apache Spark模型:)。通过在…上创建帐户，为FavioVazquez/first_spark_model开发做出贡献</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">github.com</p></div></div></div></a></div></div><div class="ab cl pr ps hu pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="ij ik il im in"><p id="6a7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望您能从这篇文章中学到一些新东西。我将在这个主题上创造更多，我也将很快与我的公司<a class="ae kv" href="https://www.linkedin.com/company/closter/" rel="noopener ugc nofollow" target="_blank"> Closter </a>一起推出关于Spark、Python和数据科学的课程。你可以通过以下方式联系我:favio [at] closer [dot] net</p><p id="4dda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下期文章再见:)</p></div></div>    
</body>
</html>