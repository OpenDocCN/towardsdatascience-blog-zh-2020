<html>
<head>
<title>BLiTZ — A Bayesian Neural Network library for PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BLiTZ——py torch的贝叶斯神经网络库</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/blitz-a-bayesian-neural-network-library-for-pytorch-82f9998916c7?source=collection_archive---------9-----------------------#2020-04-04">https://towardsdatascience.com/blitz-a-bayesian-neural-network-library-for-pytorch-82f9998916c7?source=collection_archive---------9-----------------------#2020-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2f68" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">blitz——Torch Zoo中的贝叶斯层是一个简单且可扩展的库，用于在PyTorch上创建贝叶斯神经网络层。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2ad9305f51c33b6b98ee1c6c224a5f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OChOOfEYuOQD073KS76cKg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝叶斯回归图解。来源:<a class="ae ky" href="https://ericmjl.github.io/bayesian-deep-learning-demystified/images/linreg-bayesian.png" rel="noopener ugc nofollow" target="_blank">https://ericmjl . github . io/Bayesian-deep-learning-demystified/images/linreg-Bayesian . png</a>(2020–03–30访问)</p></figure><p id="a114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一篇关于使用库进行深度贝叶斯学习的文章。如果你是这个主题的新手，你可能想在medium上寻找关于它的许多帖子中的一个，或者只是我们lib repo 的Bayesian DL的<a class="ae ky" href="https://github.com/piEsposito/blitz-bayesian-deep-learning#Bayesian-Deep-Learning-in-a-Nutshell" rel="noopener ugc nofollow" target="_blank">文档部分。</a></p><p id="9f2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于越来越需要收集神经网络预测的不确定性，使用贝叶斯神经网络层成为最直观的方法之一-这可以通过贝叶斯网络作为深度学习研究领域的趋势得到证实。</p><p id="a0a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实是，尽管PyTorch有成为主要深度学习框架的趋势(至少对于研究而言)，但没有一个库让用户像使用<code class="fe lv lw lx ly b">nn.Linear</code>和<code class="fe lv lw lx ly b">nn.Conv2d</code>那样轻松地引入贝叶斯神经网络层。</p><p id="d380" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从逻辑上来说，这对于任何想要灵活地使用贝叶斯方法进行数据建模的人来说都是一个瓶颈，因为用户必须开发贝叶斯层的整个部分以供使用，而不是专注于其模型的架构。</p><p id="390b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BLiTZ就是为了解决这一瓶颈而产生的。通过与PyTorch(包括与<code class="fe lv lw lx ly b">nn.Sequential</code>模块)完全集成，并易于扩展为贝叶斯深度学习库，BLiTZ允许用户在其神经网络上引入不确定性，只需调整其超参数。</p><p id="523d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本帖中，我们将讨论如何使用BLiTZ层和采样工具创建、训练和推断不确定性引入的神经网络。</p><h1 id="c81c" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">贝叶斯深度学习层</h1><p id="1f86" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">正如我们所知，贝叶斯深度学习的主要思想是，在每个前馈操作中，贝叶斯层从正态分布中采样其权重，而不是具有确定性的权重。</p><p id="4614" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，该层的可训练参数是确定该分布的平均值和方差的参数。</p><p id="3fd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从数学上讲，操作将从:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/ac963788863c450b7261876eae72c0cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/0*QAUBCeDiqs-oNtJs"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">确定性“香草”神经网络前馈操作。</p></figure><p id="68a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">收件人:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/551a06efc218b09dcebe583cef8f6af5.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/1*dvwwEkrYb0uHO6fJbg7KHQ.gif"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/9147df84a7ecd69f80244d1a58a3608c.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/1*zCjR-P5-PFioeemKu1xXjQ.gif"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/e8cd797ab5d48d7608dde9e72c7b228f.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/1*wRnowElpz_7VByLvaT_syA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝叶斯神经网络层的前馈操作。</p></figure><p id="e9e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Torch上实现ρ和μ为可训练参数的层可能很难，除此之外，创建超参数可调层可能更难。BLiTZ有一个内置的<code class="fe lv lw lx ly b">BayesianLinear</code>层，可以很容易地引入到模型中:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="5cce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它像一个普通的Torch <code class="fe lv lw lx ly b">nn.Module</code>网络一样工作，但是它的<code class="fe lv lw lx ly b">BayesianLinear</code>模块利用之前解释的其权重的不确定性来执行训练和推理。</p><h1 id="c875" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated"><strong class="ak">损失计算</strong></h1><p id="b769" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">正如在其原始论文中提出的，贝叶斯神经网络成本函数是“复杂性成本”与“数据拟合成本”的组合。在所有的代数争论之后，对于每个前馈操作，我们有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/5fffd5a8688364aa9e985c9509e2b336.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/1*BNUGrYYSAmTLLxTWUIltBg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝叶斯神经网络的代价函数。</p></figure><p id="5002" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相对于简单得多的预定义pdf函数，复杂性成本(P(W))由(网络上每个贝叶斯层的)采样权重的概率密度函数之和组成。通过这样做，我们确保了在优化的同时，我们的模型相对于预测的方差将会减少。</p><p id="6198" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，BLiTZ为我们带来了引入一些方法的<code class="fe lv lw lx ly b">variational_estimator</code>装饰器，比如将<code class="fe lv lw lx ly b">nn_kl_divergence</code>方法引入我们的<code class="fe lv lw lx ly b">nn.Module</code>。给定数据点、其标签和标准，我们可以通过以下方式获得预测的损失:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="3d4d" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">简单的模型优化</h1><p id="cb30" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">在优化和继续之前，贝叶斯神经网络通常通过对同一批次的损失进行多次采样来优化，这是为了补偿权重的随机性，并避免因受离群值影响的损失而优化它们。</p><p id="5bfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BLiTZ的<code class="fe lv lw lx ly b">variational_estimator</code>装饰器也用<code class="fe lv lw lx ly b">sample_elbo</code>方法为神经网络提供动力。给定<code class="fe lv lw lx ly b">inputs</code>、<code class="fe lv lw lx ly b">outputs</code>、<code class="fe lv lw lx ly b">criterion</code>和<code class="fe lv lw lx ly b">sample_nbr</code>，它估计在计算批次<code class="fe lv lw lx ly b">sample_nbr</code>次损失时进行迭代过程，并收集其平均值，返回复杂度损失与拟合值之和。</p><p id="fda2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优化贝叶斯神经网络模型非常容易:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="9624" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">看一个例子:</h1><p id="bc4d" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">我们现在通过<a class="ae ky" href="https://github.com/piEsposito/blitz-bayesian-deep-learning/blob/master/blitz/examples/bayesian_regression_boston.py" rel="noopener ugc nofollow" target="_blank">这个例子</a>，使用BLiTZ创建一个贝叶斯神经网络来估计波士顿房屋sklearn内置数据集的房价的置信区间。如果你想寻找其他的例子，库上有更多的<a class="ae ky" href="https://github.com/piEsposito/blitz-bayesian-deep-learning/tree/master/blitz/examples" rel="noopener ugc nofollow" target="_blank"/></p><h2 id="177a" class="nd ma it bd mb ne nf dn mf ng nh dp mj li ni nj ml lm nk nl mn lq nm nn mp no bi translated">必要的进口</h2><p id="2956" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">除了已知的模块，我们将从BLiTZ带来<code class="fe lv lw lx ly b">variational_estimator</code>装饰器，它帮助我们处理模块上的贝叶斯层，保持它与Torch的其余部分完全集成，当然，还有<code class="fe lv lw lx ly b">BayesianLinear</code>，它是我们的层，具有权重不确定性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><h2 id="78f1" class="nd ma it bd mb ne nf dn mf ng nh dp mj li ni nj ml lm nk nl mn lq nm nn mp no bi translated">加载和缩放数据</h2><p id="4172" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">这并不是什么新鲜事，我们正在导入和标准化数据来帮助训练。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><h2 id="f00a" class="nd ma it bd mb ne nf dn mf ng nh dp mj li ni nj ml lm nk nl mn lq nm nn mp no bi translated">创建我们的回归类</h2><p id="09c3" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">我们可以通过继承<code class="fe lv lw lx ly b">nn.Module</code>来创建我们的类，就像我们对任何火炬网络所做的那样。我们的装饰者介绍了处理贝叶斯特征的方法，计算贝叶斯层的复杂性成本，并进行许多前馈(对每一个采样不同的权重)来采样我们的损失。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><h2 id="94ad" class="nd ma it bd mb ne nf dn mf ng nh dp mj li ni nj ml lm nk nl mn lq nm nn mp no bi translated">定义置信区间评估函数</h2><p id="5d5a" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">该函数确实为我们试图对标签值进行采样的批次上的每个预测创建了一个置信区间。然后，我们可以通过寻找有多少预测分布包含数据点的正确标签来衡量我们预测的准确性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><h2 id="1b10" class="nd ma it bd mb ne nf dn mf ng nh dp mj li ni nj ml lm nk nl mn lq nm nn mp no bi translated">创建回归变量并加载数据</h2><p id="1fa2" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">注意这里我们创建了我们的<code class="fe lv lw lx ly b">BayesianRegressor</code>,就像我们创建其他神经网络一样。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><h2 id="24c6" class="nd ma it bd mb ne nf dn mf ng nh dp mj li ni nj ml lm nk nl mn lq nm nn mp no bi translated">我们的主要培训和评估循环</h2><p id="3732" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">我们做了一个训练循环，唯一不同于普通火炬训练的是通过<code class="fe lv lw lx ly b">sample_elbo</code>方法对其损耗进行采样。所有其他的事情都可以正常完成，因为我们使用BLiTZ的目的是让你轻松地用不同的贝叶斯神经网络迭代数据。</p><p id="79b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们非常简单的训练循环:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="297a" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">结论</h1><p id="52ba" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">BLiTZ是一个有用的lib，可以在深度学习实验中使用贝叶斯层进行迭代，并且对通常的代码进行很小的更改。它的层和装饰器与神经网络的Torch模块高度集成，使得创建定制网络和提取它们的复杂性成本没有困难。</p><p id="4aeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，这是我们回购的链接:<a class="ae ky" href="https://github.com/piEsposito/blitz-bayesian-deep-learning" rel="noopener ugc nofollow" target="_blank">https://github.com/piEsposito/blitz-bayesian-deep-learning</a></p><h1 id="3288" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">参考</h1><div class="np nq gp gr nr ns"><a href="https://github.com/piEsposito/blitz-bayesian-deep-learning" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">piEsposito/blitz-贝叶斯深度学习</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">BLiTZ是一个简单且可扩展的库，用于创建贝叶斯神经网络层(基于Weight…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">github.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og ks ns"/></div></div></a></div><div class="np nq gp gr nr ns"><a href="https://arxiv.org/abs/1505.05424" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">神经网络中的权重不确定性</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">我们介绍了一个新的，有效的，原则性和反向传播兼容的算法学习概率…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">arxiv.org</p></div></div></div></a></div></div></div>    
</body>
</html>