<html>
<head>
<title>Creating a Custom Environment for TensorFlow Agent — Tic-tac-toe Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为TensorFlow代理创建自定义环境—井字游戏示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-a-custom-environment-for-tensorflow-agent-tic-tac-toe-example-b66902f73059?source=collection_archive---------29-----------------------#2020-05-26">https://towardsdatascience.com/creating-a-custom-environment-for-tensorflow-agent-tic-tac-toe-example-b66902f73059?source=collection_archive---------29-----------------------#2020-05-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a9f7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于自定义TF-Agent环境的介绍性博客</h2></div><h1 id="f237" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">背景</h1><p id="7281" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">强化学习是人工智能的一个新兴领域，在游戏、机器人、制造和航空航天等领域显示了很大的前景。在21世纪10年代中期，强化学习在围棋和国际象棋等游戏中击败人类冠军后，获得了牵引力。谷歌收购了DeepMind[3]，这是一家备受尊敬的人工智能初创公司，为2010年的大多数强化学习突破做出了贡献。类似地，OpenAI在2015年末由埃隆·马斯克、萨姆·奥特曼和其他人[4]创立，他们承诺投入10亿美元进行人工智能领域的研究。OpenAI表示，他们的目标是以造福全人类的方式促进和发展友好的人工智能。<a class="ae lw" href="https://openai.com/projects/five/" rel="noopener ugc nofollow" target="_blank"> OpenAI的项目OpenAI Five </a>，在Dota 2游戏上展示了达到专家级性能、学习人机合作、互联网规模操作的能力[5]。最近，谷歌在芯片布局上使用了强化学习，这是芯片设计过程中最复杂和最耗时的阶段之一，目标是最小化PPA(功率、性能和面积)，并表明生成的布局是超人的[6]。</p><p id="aa7e" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">强化学习自20世纪50年代就已经出现，在游戏和机器控制中产生了许多有趣的应用。直到2013年，DeepMind的研究人员展示了它在Atari游戏中的使用，在大多数游戏中，它的表现都超过了人类，他们才获得了头条新闻[8]。决定性的改进是使用神经网络来学习Q值[9]。与人工智能的其他领域一样，神经网络随着深度强化学习的引入，彻底改变了强化学习领域[9]。从那以后，强化学习无处不在，并以前所未有的规模普及开来。在最近的ICLR会议(ICLR 2020)中，我们可以看到强化学习是最常见的标签[10]。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/e7b73a08e6ea7acf381916ad23b1f63b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WIu09-i7qQ5oOinwQZcsAw.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">ICLR强化学习集群</p></figure><h1 id="af44" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">那么，什么是强化学习呢？</h1><p id="fdfa" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">与标签数据可用的监督机器学习不同，强化学习不提供明确的标签数据。在强化学习中，一个主体在某个环境中执行一些动作，由于这些动作，环境的状态发生变化。基于环境对某些行为给出的反馈(奖励或惩罚)，算法学习最优策略。一个学习自己走路的孩子类似于强化学习范例。平衡自己的孩子是奖励阶段，而失去平衡的孩子是惩罚或失败阶段。更多的理论解释可以在强化学习入门博客上找到，如果读者对强化学习不熟悉，强烈建议他/她这样做。</p><h1 id="b860" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">TF-代理</h1><p id="ff98" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">TF-Agents是TensorFlow中用于强化学习的库，它通过提供各种经过良好测试、可修改和可扩展的模块化组件，使强化学习算法的设计和实现变得更加容易。这有助于研究人员和开发人员快速建立原型和基准。</p><p id="3e8d" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">可以使用以下代码安装TF-Agents稳定版:</p><pre class="md me mf mg gt ms mt mu mv aw mw bi"><span id="9479" class="mx kj it mt b gy my mz l na nb"><em class="nc">pip install --user tf-agents<br/>pip install --user tensorflow==2.1.0</em></span></pre><p id="0840" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">更多关于TF-Agents的细节可以在<a class="ae lw" href="https://www.tensorflow.org/agents" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="67af" class="mx kj it bd kk nd ne dn ko nf ng dp ks lj nh ni ku ln nj nk kw lr nl nm ky nn bi translated">环境</h2><p id="5d84" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">环境是代理执行动作的环境或设置。代理与环境交互，环境的状态发生变化。在某些应用程序上实现强化学习算法时，需要应用程序的环境。尽管TensorFlow为一些流行的问题(如CartPole)提供了环境，但我们还是遇到了需要构建自定义环境的情况。在这里，我将通过构建一个自定义环境来展示井字游戏的实现。</p><h2 id="ca7f" class="mx kj it bd kk nd ne dn ko nf ng dp ks lj nh ni ku ln nj nk kw lr nl nm ky nn bi translated">井字游戏的自定义环境</h2><p id="0476" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了更专注于构建定制环境，我们简化了井字游戏。简化的井字游戏只有一个玩家，而不是两个玩家。玩家随机选择位置，如果他/她选择的位置已经被选择，游戏结束。</p><p id="d38f" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">让我们首先从所需的导入开始。</p><pre class="md me mf mg gt ms mt mu mv aw mw bi"><span id="c97c" class="mx kj it mt b gy my mz l na nb">import tensorflow as tf<br/>import numpy as np</span><span id="e83a" class="mx kj it mt b gy no mz l na nb">from tf_agents.environments import py_environment<br/>from tf_agents.environments import tf_environment<br/>from tf_agents.environments import tf_py_environment<br/>from tf_agents.environments import utils<br/>from tf_agents.specs import array_spec<br/>from tf_agents.environments import wrappers<br/>from tf_agents.environments import suite_gym<br/>from tf_agents.trajectories import time_step as ts</span></pre><p id="1574" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">环境可以是Python环境或TensorFlow环境。Python环境实现起来很简单，但是TensorFlow环境更高效，并且允许自然并行化。我们在这里做的是创建Python环境，并使用我们的一个包装器将其自动转换为TensorFlow环境。</p><h2 id="4a06" class="mx kj it bd kk nd ne dn ko nf ng dp ks lj nh ni ku ln nj nk kw lr nl nm ky nn bi translated">成分</h2><p id="6742" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">创建自定义环境主要由四种方法组成:<em class="nc"> action_spec </em>、<em class="nc"> observation_spec </em>、<em class="nc"> _reset </em>和<em class="nc"> _step </em>。让我们看看它们各自的含义:</p><p id="f4a5" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><em class="nc"> action_spec </em>:描述步骤<br/> <em class="nc"> observation_spec </em>所期望的动作的规格(<em class="nc">tensor spec</em>);定义环境<br/> <em class="nc"> _reset </em>所提供的观测的规格(<em class="nc">tensor spec</em>)；重置环境<br/><em class="nc">_步骤</em>后返回当前情况(<em class="nc">时间步长</em>)；应用动作并返回新的</p><h2 id="9c27" class="mx kj it bd kk nd ne dn ko nf ng dp ks lj nh ni ku ln nj nk kw lr nl nm ky nn bi translated">SimplifiedTicTacToe类</h2><p id="5821" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，让我们开始创建<em class="nc"> SimplifiedTicTacToe </em>类。该类继承自<em class="nc"> py_environment。PyEnvironment </em>类，以便提取已经可用的方法和属性。</p><p id="6e6e" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">井字游戏棋盘有九个位置。让我们从0到8(包括0和8)对它们进行标记。玩家可以将标记放在其中一个位置。所以，一个动作是一个从0到8的值。</p><p id="7c0d" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">观察是环境的状态。观察规范具有由环境提供的观察规范。因为棋盘有9个位置，所以观察的形状是(1，9)。如果某个位置被占用，我们可以用1表示该位置的状态，否则用0表示。最初，棋盘是空的，所以我们用九个零来表示环境的状态。</p><pre class="md me mf mg gt ms mt mu mv aw mw bi"><span id="71ab" class="mx kj it mt b gy my mz l na nb"><strong class="mt iu">class SimplifiedTicTacToe</strong>(py_environment<strong class="mt iu">.</strong>PyEnvironment):</span><span id="3e4a" class="mx kj it mt b gy no mz l na nb">  <strong class="mt iu">def</strong> <strong class="mt iu">__init__</strong>(self):<br/>    self<strong class="mt iu">.</strong>_action_spec <strong class="mt iu">=</strong> array_spec<strong class="mt iu">.</strong>BoundedArraySpec(<br/>        shape<strong class="mt iu">=</strong>(), dtype<strong class="mt iu">=</strong>np<strong class="mt iu">.</strong>int32, minimum<strong class="mt iu">=</strong>0, maximum<strong class="mt iu">=</strong>8, name<strong class="mt iu">=</strong>'play')<br/>    self<strong class="mt iu">.</strong>_observation_spec <strong class="mt iu">=</strong> array_spec<strong class="mt iu">.</strong>BoundedArraySpec(<br/>        shape<strong class="mt iu">=</strong>(1,9), dtype<strong class="mt iu">=</strong>np<strong class="mt iu">.</strong>int32, minimum<strong class="mt iu">=</strong>0, maximum<strong class="mt iu">=</strong>1, name<strong class="mt iu">=</strong>'board')<br/>    self<strong class="mt iu">.</strong>_state <strong class="mt iu">=</strong> [0, 0, 0, 0, 0, 0, 0, 0, 0]<br/>    self<strong class="mt iu">.</strong>_episode_ended <strong class="mt iu">=</strong> False</span><span id="02dc" class="mx kj it mt b gy no mz l na nb">  <strong class="mt iu">def</strong> <strong class="mt iu">action_spec</strong>(self):<br/>    <strong class="mt iu">return</strong> self<strong class="mt iu">.</strong>_action_spec</span><span id="91f1" class="mx kj it mt b gy no mz l na nb">  <strong class="mt iu">def</strong> <strong class="mt iu">observation_spec</strong>(self):<br/>    <strong class="mt iu">return</strong> self<strong class="mt iu">.</strong>_observation_spec</span></pre><p id="ffc8" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">游戏结束后，我们应该重置环境(或状态)。为此，我们可以在我们创建的定制环境中编写一个名为<em class="nc"> _reset </em>的方法。该方法必须返回游戏开始时环境的默认状态。</p><pre class="md me mf mg gt ms mt mu mv aw mw bi"><span id="d508" class="mx kj it mt b gy my mz l na nb">def _reset(self):<br/>  # state at the start of the game<br/>  self._state = [0, 0, 0, 0, 0, 0, 0, 0, 0]<br/>  self._episode_ended = False<br/>  return ts.restart(np.array([self._state], dtype=np.int32))</span></pre><p id="5399" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">这里值得一提的是<em class="nc">第</em>集和<em class="nc">第</em>步。一集<em class="nc">是一个游戏的实例(或者游戏的生命)。如果游戏结束或生命减少，该集结束。<em class="nc">另一方面，步长</em>是在<em class="nc">情节</em>中单调增加的时间或某个离散值。随着游戏状态的每次改变，步骤<em class="nc">的值</em>增加，直到游戏结束。</em></p><p id="b0af" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">让我们定义两种方法来检查某个点是否是空的，以及是否所有的点都被占用了。</p><pre class="md me mf mg gt ms mt mu mv aw mw bi"><span id="a5f8" class="mx kj it mt b gy my mz l na nb"><strong class="mt iu">def</strong> <strong class="mt iu">__is_spot_empty</strong>(self, ind):<br/>    <strong class="mt iu">return</strong> self<strong class="mt iu">.</strong>_state[ind] <strong class="mt iu">==</strong> 0</span><span id="f37f" class="mx kj it mt b gy no mz l na nb"><strong class="mt iu">def</strong> <strong class="mt iu">__all_spots_occupied</strong>(self):<br/>    <strong class="mt iu">return</strong> all(i <strong class="mt iu">==</strong> 1 <strong class="mt iu">for</strong> i <strong class="mt iu">in</strong> self<strong class="mt iu">.</strong>_state)</span></pre><p id="e7c6" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">现在，我们需要编写最后一个方法:<em class="nc"> _step </em>。它应用动作并返回游戏中的新情况。这种情况属于TensorFlow中的类<em class="nc">时间步长</em>。<em class="nc"> TimeStep </em>有四个信息:<em class="nc">观察，奖励，step_type </em>和<em class="nc">折扣</em>。关于每个信息的细节可以在<a class="ae lw" href="https://www.tensorflow.org/agents/tutorials/2_environments_tutorial#python_environments" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="4cfb" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">在写<em class="nc"> _step </em>方法的同时，先看看插曲是否已经结束。如果已经结束，我们需要调用<em class="nc"> _reset </em>方法。否则，我们会查看要标记的位置是否为空。如果不为空，则该集结束。如果该位置为空，我们在该位置放置标记，并查看这是否是最后一步。根据是否是最后一步，我们分别返回终止或转换。</p><pre class="md me mf mg gt ms mt mu mv aw mw bi"><span id="816e" class="mx kj it mt b gy my mz l na nb"><strong class="mt iu">def</strong> <strong class="mt iu">_step</strong>(self, action):    <br/>    <strong class="mt iu">if</strong> self<strong class="mt iu">.</strong>_episode_ended:<br/>        <strong class="mt iu">return</strong> self<strong class="mt iu">.</strong>reset()</span><span id="b38a" class="mx kj it mt b gy no mz l na nb"><strong class="mt iu">    if</strong> self<strong class="mt iu">.</strong>__is_spot_empty(action):        <br/>        self<strong class="mt iu">.</strong>_state[action] <strong class="mt iu">=</strong> 1<br/>        <br/>        <strong class="mt iu">if</strong> self<strong class="mt iu">.</strong>__all_spots_occupied():<br/>            self<strong class="mt iu">.</strong>_episode_ended <strong class="mt iu">=</strong> True<br/>            <strong class="mt iu">return</strong> ts<strong class="mt iu">.</strong>termination(np<strong class="mt iu">.</strong>array([self<strong class="mt iu">.</strong>_state], dtype<strong class="mt iu">=</strong>np<strong class="mt iu">.</strong>int32), 1)<br/>        <strong class="mt iu">else</strong>:<br/>            <strong class="mt iu">return</strong> ts<strong class="mt iu">.</strong>transition(np<strong class="mt iu">.</strong>array([self<strong class="mt iu">.</strong>_state], dtype<strong class="mt iu">=</strong>np<strong class="mt iu">.</strong>int32), reward<strong class="mt iu">=</strong>0.05, discount<strong class="mt iu">=</strong>1.0)<br/>    <strong class="mt iu">else</strong>:<br/>        self<strong class="mt iu">.</strong>_episode_ended <strong class="mt iu">=</strong> True<br/>        <strong class="mt iu">return</strong> ts<strong class="mt iu">.</strong>termination(np<strong class="mt iu">.</strong>array([self<strong class="mt iu">.</strong>_state], dtype<strong class="mt iu">=</strong>np<strong class="mt iu">.</strong>int32), <strong class="mt iu">-</strong>1)</span></pre><p id="7ce0" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">每走一步给予0.05的奖励。当我们所有的9个位置都成交时，奖励值为1。如果游戏结束时少于9个位置记号，则获得负奖励-1。在这里，使用1.0的折扣，以便没有关于时间/步骤的报酬衰减。</p><p id="9de3" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">现在，让我们创建张量流环境。</p><pre class="md me mf mg gt ms mt mu mv aw mw bi"><span id="14cd" class="mx kj it mt b gy my mz l na nb">python_environment = SimplifiedTicTacToe()<br/>tf_env = tf_py_environment.TFPyEnvironment(python_environment)</span></pre><p id="fde9" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">万岁</strong>！已经创建了<em class="nc"> TensorFlow环境</em>！</p><h2 id="06c1" class="mx kj it bd kk nd ne dn ko nf ng dp ks lj nh ni ku ln nj nk kw lr nl nm ky nn bi translated">让我们来玩吧！</h2><p id="c1ce" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，我们来玩10000集的游戏。</p><pre class="md me mf mg gt ms mt mu mv aw mw bi"><span id="ef47" class="mx kj it mt b gy my mz l na nb">time_step = tf_env.reset()<br/>rewards = []<br/>steps = []<br/>number_of_episodes = 10000</span><span id="a622" class="mx kj it mt b gy no mz l na nb">for _ in range(number_of_episodes):<br/>  reward_t = 0<br/>  steps_t = 0<br/>  tf_env.reset()<br/>  while True:<br/>    action = tf.random.uniform([1], 0, 9, dtype=tf.int32)<br/>    next_time_step = tf_env.step(action)<br/>    if tf_env.current_time_step().is_last():<br/>      break<br/>    episode_steps += 1<br/>    episode_reward += next_time_step.reward.numpy()<br/>  rewards.append(episode_reward)<br/>  steps.append(episode_steps)</span></pre><p id="56ef" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我想知道平均步数。所以，我执行下面的代码。</p><pre class="md me mf mg gt ms mt mu mv aw mw bi"><span id="f79f" class="mx kj it mt b gy my mz l na nb">mean_no_of_steps = np.mean(steps)</span></pre><p id="f5a5" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我得到的平均步数为<strong class="lc iu"> <em class="nc"> 3.4452 </em> </strong>。这意味着人们可以期待游戏在第四步结束。我们播放了10000集。因此，我们相信均值能够很好地估计分布的期望。因此，让我们找出随机变量的理论期望，并看看它如何与我们通过实验估计的相吻合。</p><h2 id="a7a9" class="mx kj it bd kk nd ne dn ko nf ng dp ks lj nh ni ku ln nj nk kw lr nl nm ky nn bi translated">步骤数的期望值</h2><p id="2fd5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">设<strong class="lc iu"> <em class="nc"> X </em> </strong>为随机变量，表示重复发生后的步数。</p><p id="2aef" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu"> <em class="nc"> X </em> </strong>由九个随机变量组成，<em class="nc"> X_1，X_2，…，X_9 </em>。<em class="nc">如果直到第<em class="nc"> i </em>步没有重复，X_i </em>为1。我们需要找到<strong class="lc iu"> <em class="nc"> X </em> </strong>的期望。</p><p id="724f" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">对于<em class="nc"> i的某个值，X _ I = 0；对于<em class="nc"> j &gt; i </em>的所有值，X_j = 0 </em>。</p><p id="a225" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">所以，<em class="nc">E[X]= E[X _ 1]+E[X _ 2 | X _ 1]+…+E[X _ 9 | X _ 1，…，X_8] </em></p><p id="87ac" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">现在，我们来计算<em class="nc"> E[X_i | X_1，…，X_(i - 1)] </em>的值。</p><p id="d64e" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">如果直到第<em class="nc"> i </em>步没有重复，随机变量<em class="nc"> X_i </em>为<em class="nc"> 1 </em>。这种可能性是:</p><p id="6ac5" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><em class="nc"> P(第I步之前不重复)=第I步之前不重复排列的数目/第I步之前排列的总数<br/> = P(9，i) / (9 ^ i) </em></p><p id="6806" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">概率乘以<em class="nc"> 1 </em>给出概率本身。所以期望变成了这些概率的总和。</p><p id="4eca" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">因此，期望是:</p><p id="436e" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><em class="nc"> E[X] =从i=1到i=9的和(P(9，I)/(9 ^ I))</em>≈<strong class="lc iu"><em class="nc">3.46</em></strong></p><p id="7376" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">另一种找到期望值的优雅方法可以在这里找到<a class="ae lw" href="https://www.quora.com/Suppose-we-are-drawing-a-number-randomly-from-the-list-of-numbers-1-to-9-After-each-draw-the-number-is-replaced-How-can-we-find-the-expected-number-of-draws-after-which-we-encounter-the-first-repetition/answer/Henk-Brozius?__filter__=all&amp;__nsrc__=1&amp;__sncid__=5298994587&amp;__snid3__=8434034083" rel="noopener ugc nofollow" target="_blank"/>——感谢<a class="ae lw" href="https://www.quora.com/profile/Henk-Brozius" rel="noopener ugc nofollow" target="_blank"> Henk Brozius </a>。</p><p id="8525" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">理论预期非常非常接近实验预期。这给了我一些满足感。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="f622" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">这就是如何在TensorFlow中创建自定义环境。TF-Agents提供了模块化的组件，使得原型开发更加容易。你有了一个良好的开端。我们可以用TF-Agents做更多的事情。继续探索！天空是无限的！</p><p id="6746" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">强烈建议感兴趣的读者点击以下链接，了解有关创建自定义TensorFlow环境的更多信息。</p><ol class=""><li id="18c1" class="nw nx it lc b ld lx lg ly lj ny ln nz lr oa lv ob oc od oe bi translated">环境:<a class="ae lw" href="https://www.tensorflow.org/agents/tutorials/2_environments_tutorial" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/agents/tutorials/2 _ environments _ tutorial</a></li><li id="3573" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated">TF _ agents . specs . bounddarrayspec:<a class="ae lw" href="https://www.tensorflow.org/agents/api_docs/python/tf_agents/specs/BoundedArraySpec" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/agents/API _ docs/python/TF _ agents/specs/bounddarrayspec</a></li></ol><p id="d655" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">如果您有任何问题或意见或困惑，请在此随意评论。我会尽力回答他们。</p><h1 id="b1e9" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">参考</h1><ol class=""><li id="b237" class="nw nx it lc b ld le lg lh lj ok ln ol lr om lv ob oc od oe bi translated"><a class="ae lw" href="https://www.newscientist.com/article/2132086-deepminds-ai-beats-worlds-best-go-player-in-latest-face-off/" rel="noopener ugc nofollow" target="_blank">https://www . new scientist . com/article/2132086-deep minds-ai-beats-worlds-best-go-player-in-latest-face-off/</a></li><li id="895c" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated"><a class="ae lw" href="https://www.theguardian.com/technology/2017/dec/07/alphazero-google-deepmind-ai-beats-champion-program-teaching-itself-to-play-four-hours" rel="noopener ugc nofollow" target="_blank">https://www . the guardian . com/technology/2017/dec/07/alpha zero-Google-deep mind-ai-beats-champion-program-teaching-self-to-play-four-hours</a></li><li id="078e" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated"><a class="ae lw" href="https://techcrunch.com/2014/01/26/google-deepmind/" rel="noopener ugc nofollow" target="_blank">https://techcrunch.com/2014/01/26/google-deepmind/</a></li><li id="ab1b" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated"><a class="ae lw" href="https://openai.com/blog/introducing-openai/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/introducing-openai/</a></li><li id="b2d3" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated"><a class="ae lw" href="https://openai.com/projects/five/" rel="noopener ugc nofollow" target="_blank">https://openai.com/projects/five/</a></li><li id="e54d" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated"><a class="ae lw" href="https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2020/04/chip-design-with-deep-reinforcement . html</a></li><li id="f2f9" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated"><a class="ae lw" href="http://incompleteideas.net/book/first/ebook/node12.html" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/first/ebook/node12.html</a></li><li id="4564" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated"><a class="ae lw" href="https://arxiv.org/abs/1312.5602" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1312.5602</a></li><li id="c3e0" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated">aurélien géRon——使用Scikit-Learn、Keras和TensorFlow进行动手机器学习_构建智能系统的概念、工具和技术(2019年，O'Reilly Media)</li><li id="b34d" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated"><a class="ae lw" href="https://iclr.cc/virtual_2020/paper_vis.html" rel="noopener ugc nofollow" target="_blank">https://iclr.cc/virtual_2020/paper_vis.html</a></li><li id="01b0" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated"><a class="ae lw" href="https://www.tensorflow.org/agents" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/agents</a></li><li id="6325" class="nw nx it lc b ld of lg og lj oh ln oi lr oj lv ob oc od oe bi translated">https://www . quora . com/subscribe-we-are-drawing-a-number-random-from-the-list-of-numbers-1-9-After-drawing-the-numbers-is-replaced-How-we-can-find-the-expected-number-drawing-After-the-we-conference-the-first-repetition/answer/Henk-Brozius？_ _ filter _ _ = all&amp;_ _ nsrc _ _ = 1&amp;_ _ sncid _ _ = 5298994587&amp;_ _ snid 3 _ _ = 8434034083</li></ol></div></div>    
</body>
</html>