<html>
<head>
<title>Custom neural networks in Keras: a street fighter’s guide to build a graphCNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras中的自定义神经网络:街头战士构建图表指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/custom-neural-networks-in-keras-a-street-fighters-guide-to-build-a-graphcnn-e91f6b05f12e?source=collection_archive---------33-----------------------#2020-06-22">https://towardsdatascience.com/custom-neural-networks-in-keras-a-street-fighters-guide-to-build-a-graphcnn-e91f6b05f12e?source=collection_archive---------33-----------------------#2020-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2f6f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何建立具有自定义结构和层的神经网络:Keras中的图形卷积神经网络(GCNN)。</h2></div><p id="fdd9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们生活中的某个时刻，Tensorflow <strong class="kk iu"> Keras </strong>中预定义的图层已经不够用了！我们想要更多！我们希望建立具有创造性结构和奇异层的定制神经网络！幸运的是，我们可以通过定义自定义层和模型，在Keras中轻松执行这项任务。在这个循序渐进的教程中，我们将建立一个具有并行层的神经网络，包括图形卷积层。等一下！什么是图上的卷积？</p><h2 id="6ca8" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">图形卷积神经网络</h2><p id="3001" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在传统的神经网络层中，我们执行层输入矩阵<em class="mc"> X </em>和可训练权重矩阵<em class="mc"> W </em>之间的矩阵乘法。然后我们应用激活函数<em class="mc"> f. </em>因此，下一层的输入(当前层的输出)可以表示为<em class="mc"> f(XW)。</em>在图卷积神经网络中，我们假设相似的实例在图中连接(例如，引用网络、基于距离的网络等)。)并且来自邻域的特征在(无人监督的)任务中可能是有用的。设<em class="mc"> A </em>是图的邻接矩阵，那么我们要在一个卷积层中执行的操作就是<em class="mc"> f(AXW) </em>。对于图中的每个节点，我们将聚集来自其他连接节点的特征，然后将该聚集乘以权重矩阵，然后应用激活。这种图形卷积公式是最简单的。这对于我们的教程来说很好，但是graphCNN要复杂得多！</p><p id="9a7b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好吧！现在，我们准备好了！</p><h2 id="bbd0" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">第一步。准备</h2><p id="25ac" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">首先，我们需要导入一些包。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="3a22" class="le lf it mj b gy mn mo l mp mq"># Import packages<br/>from tensorflow import __version__ as tf_version, float32 as tf_float32, Variable<br/>from tensorflow.keras import Sequential, Model<br/>from tensorflow.keras.backend import variable, dot as k_dot, sigmoid, relu<br/>from tensorflow.keras.layers import Dense, Input, Concatenate, Layer<br/>from tensorflow.keras.losses import SparseCategoricalCrossentropy<br/>from tensorflow.keras.utils import plot_model<br/>from tensorflow.random import set_seed as tf_set_seed<br/>from numpy import __version__ as np_version, unique, array, mean, argmax<br/>from numpy.random import seed as np_seed, choice<br/>from pandas import __version__ as pd_version, read_csv, DataFrame, concat<br/>from sklearn import __version__ as sk_version<br/>from sklearn.preprocessing import normalize</span><span id="43da" class="le lf it mj b gy mr mo l mp mq">print("tensorflow version:", tf_version)<br/>print("numpy version:", np_version)<br/>print("pandas version:", pd_version)<br/>print("scikit-learn version:", sk_version)</span></pre><p id="0eec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您应该收到作为输出的导入包的版本。在我的例子中，输出是:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="3273" class="le lf it mj b gy mn mo l mp mq">tensorflow version: 2.2.0 <br/>numpy version: 1.18.5 <br/>pandas version: 1.0.4 <br/>scikit-learn version: 0.22.2.post1</span></pre><p id="58ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本教程中，我们将使用<a class="ae md" href="https://relational.fit.cvut.cz/dataset/CORA" rel="noopener ugc nofollow" target="_blank"> CORA </a>数据集:</p><blockquote class="ms mt mu"><p id="2519" class="ki kj mc kk b kl km ju kn ko kp jx kq mv ks kt ku mw kw kx ky mx la lb lc ld im bi translated">Cora数据集包括2708份科学出版物，分为七类。引文网络由5429个链接组成。数据集中的每个出版物由0/1值的词向量来描述，该词向量指示字典中相应词的存在与否。这部词典由1433个独特的单词组成。</p></blockquote><p id="036b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们加载数据，创建邻接矩阵并准备特征矩阵。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d2e1" class="le lf it mj b gy mn mo l mp mq"># Load cora data</span><span id="fd40" class="le lf it mj b gy mr mo l mp mq">dtf_data = read_csv("https://raw.githubusercontent.com/ngshya/datasets/master/cora/cora_content.csv").sort_values(["paper_id"], ascending=True)<br/>dtf_graph = read_csv("https://raw.githubusercontent.com/ngshya/datasets/master/cora/cora_cites.csv")</span><span id="a421" class="le lf it mj b gy mr mo l mp mq"># Adjacency matrix<br/>array_papers_id = unique(dtf_data["paper_id"])<br/>dtf_graph["connection"] = 1<br/>dtf_graph_tmp = DataFrame({"cited_paper_id": array_papers_id, "citing_paper_id": array_papers_id, "connection": 0})<br/>dtf_graph = concat((dtf_graph, dtf_graph_tmp)).sort_values(["cited_paper_id", "citing_paper_id"], ascending=True)<br/>dtf_graph = dtf_graph.pivot_table(index="cited_paper_id", columns="citing_paper_id", values="connection", fill_value=0).reset_index(drop=True)<br/>A = array(dtf_graph)<br/>A = normalize(A, norm='l1', axis=1)<br/>A = variable(A, dtype=tf_float32)</span><span id="9ca6" class="le lf it mj b gy mr mo l mp mq"># Feature matrix<br/>data = array(dtf_data.iloc[:, 1:1434])</span><span id="af6d" class="le lf it mj b gy mr mo l mp mq"># Labels<br/>labels = array(<br/>    dtf_data["label"].map({<br/>        'Case_Based': 0,<br/>        'Genetic_Algorithms': 1,<br/>        'Neural_Networks': 2,<br/>        'Probabilistic_Methods': 3,<br/>        'Reinforcement_Learning': 4,<br/>        'Rule_Learning': 5,<br/>        'Theory': 6<br/>    })<br/>)</span><span id="06eb" class="le lf it mj b gy mr mo l mp mq"># Check dimensions<br/>print("Features matrix dimension:", data.shape, "| Label array dimension:", labels.shape, "| Adjacency matrix dimension:", A.shape)</span></pre><p id="93d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，让我们定义一些对神经网络训练有用的参数。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="00b0" class="le lf it mj b gy mn mo l mp mq"># Training parameters<br/>input_shape = (data.shape[1], )<br/>output_classes = len(unique(labels))<br/>iterations = 50<br/>epochs = 100<br/>batch_size = data.shape[0]<br/>labeled_portion = 0.10</span></pre><p id="5f09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如您可以从上面的代码中推断出的，对于每个模型，我们将执行50次迭代，在每次迭代中，我们将随机选择10%的标记集(训练集)，并训练模型100个时期。</p><p id="62fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">需要指出的是，本教程的范围不是在CORA数据集上训练最准确的模型。相反，我们只想提供一个用keras定制层实现定制模型的例子！</p><h2 id="ec11" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">模型1:具有连续层的神经网络</h2><p id="55e1" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">作为基线，我们使用具有<strong class="kk iu">顺序层</strong>的标准神经网络(一个熟悉的<strong class="kk iu"> keras顺序模型</strong>)。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b806" class="le lf it mj b gy mn mo l mp mq"># Model 1: standard sequential neural network</span><span id="9364" class="le lf it mj b gy mr mo l mp mq">tf_set_seed(1102)<br/>np_seed(1102)</span><span id="3fa6" class="le lf it mj b gy mr mo l mp mq">model1 = Sequential([<br/>    Dense(32, input_shape=input_shape, activation='relu'),<br/>    Dense(16, activation='relu'),<br/>    Dense(output_classes, activation='softmax')<br/>], name="Model_1")<br/>model1.save_weights("model1_initial_weights.h5")</span><span id="7e19" class="le lf it mj b gy mr mo l mp mq">model1.summary()<br/>plot_model(model1, 'model1.png', show_shapes=True)</span></pre><p id="3630" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以绘制模型来查看顺序结构。</p><figure class="me mf mg mh gt mz gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c8f29b59e5a3a4f314296dfee49cc0bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*LOCxb4K4UXOBhN8KHr-fAA.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图片作者。模型1的结构:连续致密层。</p></figure><p id="733a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看这个模型表现如何。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b100" class="le lf it mj b gy mn mo l mp mq"># Testing model 1</span><span id="8750" class="le lf it mj b gy mr mo l mp mq">tf_set_seed(1102)<br/>np_seed(1102)</span><span id="0a7f" class="le lf it mj b gy mr mo l mp mq">acc_model1 = []</span><span id="b421" class="le lf it mj b gy mr mo l mp mq">for _ in range(iterations):</span><span id="b952" class="le lf it mj b gy mr mo l mp mq">    mask = choice([True, False], size=data.shape[0], replace=True, p=[labeled_portion, 1-labeled_portion])<br/>    labeled_data = data[mask, :]<br/>    unlabeled_data = data[~mask, :]<br/>    labeled_data_labels = labels[mask]<br/>    unlabeled_data_labels = labels[~mask]</span><span id="e535" class="le lf it mj b gy mr mo l mp mq">    model1.load_weights("model1_initial_weights.h5")</span><span id="5ff7" class="le lf it mj b gy mr mo l mp mq">    model1.compile(<br/>        optimizer='adam',<br/>        loss=SparseCategoricalCrossentropy(from_logits=False),<br/>        metrics=['accuracy']<br/>    )</span><span id="50a0" class="le lf it mj b gy mr mo l mp mq">    model1.fit(labeled_data, labeled_data_labels, epochs=epochs, batch_size=batch_size, verbose=0)</span><span id="7219" class="le lf it mj b gy mr mo l mp mq">    acc_model1.append(sum(argmax(model1.predict(unlabeled_data, batch_size=batch_size), axis=1) == unlabeled_data_labels) / len(unlabeled_data_labels) * 100)</span><span id="2887" class="le lf it mj b gy mr mo l mp mq">print("\nAverage accuracy on unlabeled set:", mean(acc_model1), "%")</span></pre><p id="9f54" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你应该获得55%的平均准确率。</p><h2 id="01d8" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">模型2:具有平行层的神经网络</h2><p id="bd47" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">下面介绍一个对之前模型的小修改。这一次我们希望有一个具有两个平行隐藏层的网络。我们使用<a class="ae md" href="https://keras.io/guides/functional_api/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> Keras功能API </strong> </a>。使用函数式API，我们可以构建具有非线性拓扑的模型、具有共享层的模型以及具有多个输入或输出的模型。基本上，我们需要将每一层分配给一个变量，然后引用该变量来连接不同的层，以便创建一个有向无环图(DAG)。然后，可以通过传递输入层和输出层来构建模型。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9073" class="le lf it mj b gy mn mo l mp mq"># Model 2: neural network with parallel layers</span><span id="c24e" class="le lf it mj b gy mr mo l mp mq">tf_set_seed(1102)<br/>np_seed(1102)</span><span id="106e" class="le lf it mj b gy mr mo l mp mq">m2_input_layer = Input(shape=input_shape)<br/>m2_dense_layer_1 = Dense(32, activation='relu')(m2_input_layer)<br/>m2_dense_layer_2 = Dense(16, activation='relu')(m2_input_layer)<br/>m2_merged_layer = Concatenate()([m2_dense_layer_1, m2_dense_layer_2])<br/>m2_final_layer = Dense(output_classes, activation='softmax')(m2_merged_layer)</span><span id="2ed3" class="le lf it mj b gy mr mo l mp mq">model2 = Model(inputs=m2_input_layer, outputs=m2_final_layer, name="Model_2")<br/>model2.save_weights("model2_initial_weights.h5")</span><span id="71bc" class="le lf it mj b gy mr mo l mp mq">model2.summary()<br/>plot_model(model2, 'model2.png', show_shapes=True)</span></pre><p id="fc28" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">平行层<em class="mc"> m2_dense_layer_1 </em>和<em class="mc"> m2_dense_layer_2 </em>依赖于同一个输入层<em class="mc"> m2_input_layer </em>，然后在<em class="mc"> m2_merged_layer </em>中串接形成一个唯一层。这个神经网络应该是这样的:</p><figure class="me mf mg mh gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/e59b21b3e86d244e6a0937468ee798f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mFsOnNeAn370ROQVXFwJew.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图片作者。模型2的结构:平行密集层。</p></figure><p id="9888" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们测试这个模型。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="2393" class="le lf it mj b gy mn mo l mp mq"># Testing model 2</span><span id="4173" class="le lf it mj b gy mr mo l mp mq">tf_set_seed(1102)<br/>np_seed(1102)</span><span id="e863" class="le lf it mj b gy mr mo l mp mq">acc_model2 = []</span><span id="a23f" class="le lf it mj b gy mr mo l mp mq">for _ in range(iterations):</span><span id="8574" class="le lf it mj b gy mr mo l mp mq">    mask = choice([True, False], size=data.shape[0], replace=True, p=[labeled_portion, 1-labeled_portion])<br/>    labeled_data = data[mask, :]<br/>    unlabeled_data = data[~mask, :]<br/>    labeled_data_labels = labels[mask]<br/>    unlabeled_data_labels = labels[~mask]</span><span id="b148" class="le lf it mj b gy mr mo l mp mq">    model2.load_weights("model2_initial_weights.h5")<br/>    model2.compile(<br/>        optimizer='adam',<br/>        loss=SparseCategoricalCrossentropy(from_logits=False),<br/>        metrics=['accuracy']<br/>    )</span><span id="c854" class="le lf it mj b gy mr mo l mp mq">    model2.fit(labeled_data, labeled_data_labels, epochs=epochs, batch_size=batch_size, shuffle=False, verbose=0)</span><span id="ca3d" class="le lf it mj b gy mr mo l mp mq">    acc_model2.append(sum(argmax(model2.predict(unlabeled_data, batch_size=batch_size), axis=1) == unlabeled_data_labels) / len(unlabeled_data_labels) * 100)</span><span id="7058" class="le lf it mj b gy mr mo l mp mq">print("\nAverage accuracy on unlabeled set:", mean(acc_model2), "%")</span></pre><p id="b92b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">平均准确率近60% (+5)！</p><h2 id="3a7e" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">模型2:具有图形conv层的神经网络</h2><p id="5bcb" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">到目前为止，我们已经看到了如何使用Keras Functional API创建自定义网络结构。如果我们需要用用户定义的操作定义<strong class="kk iu">自定义层</strong>怎么办？在我们的例子中，我们想定义一个简单的<strong class="kk iu">图卷积层</strong>，如本教程开始时所解释的。为此，我们需要从类<strong class="kk iu">层</strong>创建一个子类，并定义方法<strong class="kk iu"> __init__ </strong>、<strong class="kk iu"> build </strong>和<strong class="kk iu"> call </strong>。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="45af" class="le lf it mj b gy mn mo l mp mq"># Graph convolutional layer</span><span id="8eb5" class="le lf it mj b gy mr mo l mp mq">class GraphConv(Layer):</span><span id="8437" class="le lf it mj b gy mr mo l mp mq">    def __init__(self, num_outputs, A, activation="sigmoid", **kwargs):<br/>        super(GraphConv, self).__init__(**kwargs)<br/>        self.num_outputs = num_outputs<br/>        self.activation_function = activation<br/>        self.A = Variable(A, trainable=False)</span><span id="23d2" class="le lf it mj b gy mr mo l mp mq">    def build(self, input_shape):<br/>        # Weights<br/>        self.W = self.add_weight("W", shape=[int(input_shape[-1]), self.num_outputs])<br/>        # bias<br/>        self.bias = self.add_weight("bias", shape=[self.num_outputs])</span><span id="304a" class="le lf it mj b gy mr mo l mp mq">    def call(self, input):<br/>        if self.activation_function == 'relu':<br/>            return relu(k_dot(k_dot(self.A, input), self.W) + self.bias)<br/>        else:<br/>            return sigmoid(k_dot(k_dot(self.A, input), self.W) + self.bias)</span></pre><p id="5e0b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在初始化过程中，你可以要求和保存任何有用的参数(如激活函数，输出神经元的数量)。在我们的例子中，我们还需要邻接矩阵<em class="mc"> A </em>。在构建方法中，初始化层的可训练权重。在call方法中，声明了向前传递计算。</p><p id="692a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与前面的模型一样，我们定义了一个具有平行层的网络。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="77a8" class="le lf it mj b gy mn mo l mp mq"># Model 3: neural network with graph convolutional layer</span><span id="baa2" class="le lf it mj b gy mr mo l mp mq">tf_set_seed(1102)<br/>np_seed(1102)</span><span id="33d3" class="le lf it mj b gy mr mo l mp mq">m3_input_layer = Input(shape=input_shape)<br/>m3_dense_layer = Dense(32, activation='relu')(m3_input_layer)<br/>m3_gc_layer = GraphConv(16, A=A, activation='relu')(m3_input_layer)<br/>m3_merged_layer = Concatenate()([m3_dense_layer, m3_gc_layer])<br/>m3_final_layer = Dense(output_classes, activation='softmax')(m3_merged_layer)</span><span id="5f3d" class="le lf it mj b gy mr mo l mp mq">model3 = Model(inputs=m3_input_layer, outputs=m3_final_layer, name="Model_3")</span><span id="69fe" class="le lf it mj b gy mr mo l mp mq">model3.save_weights("model3_initial_weights.h5")</span><span id="e872" class="le lf it mj b gy mr mo l mp mq">model3.summary()<br/>plot_model(model3, 'model3.png', show_shapes=True)</span></pre><p id="f45f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它看起来像前面的模型，但有一层是卷积的:每个实例的固有特征与从邻域计算的聚合特征连接在一起。</p><figure class="me mf mg mh gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nl"><img src="../Images/f0cfc44c171fe600d55232a240517b5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iGlnAnnxDUhFKWBJH9o2Zw.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图片作者。模型3的结构:卷积层和自定义结构。</p></figure><p id="59c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在编制该模型时应进一步注意。由于卷积层需要整个邻接矩阵，我们需要传递整个特征矩阵(标记和未标记的实例)，但是模型应该只在标记的实例上训练。因此，我们定义了一个定制的损失函数，其中稀疏分类cossentropy仅在标记的实例上计算。此外，我们将未标记实例的标签随机化，以确保它们不会在训练中使用。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="66ec" class="le lf it mj b gy mn mo l mp mq"># Testing model 3</span><span id="0306" class="le lf it mj b gy mr mo l mp mq">tf_set_seed(1102)<br/>np_seed(1102)</span><span id="1d91" class="le lf it mj b gy mr mo l mp mq">acc_model3 = []</span><span id="920e" class="le lf it mj b gy mr mo l mp mq">for i in range(iterations):<br/>    mask = choice([True, False], size=data.shape[0], replace=True, p=[labeled_portion, 1-labeled_portion])<br/>    unlabeled_data_labels = labels[~mask]<br/>    # Randomize the labels of unlabeled instances<br/>    masked_labels = labels.copy()<br/>    masked_labels[~mask] = choice(range(7), size=sum(~mask), replace=True)</span><span id="76cb" class="le lf it mj b gy mr mo l mp mq">    model3.load_weights("model3_initial_weights.h5")<br/>    model3.compile(<br/>        optimizer='adam', <br/>        loss=lambda y_true, y_pred: SparseCategoricalCrossentropy(from_logits=False)(y_true[mask], y_pred[mask]),<br/>        metrics=['accuracy']<br/>    )</span><span id="bc70" class="le lf it mj b gy mr mo l mp mq">    model3.fit(data, masked_labels, epochs=epochs, batch_size=batch_size, shuffle=False, verbose=0)</span><span id="d6e0" class="le lf it mj b gy mr mo l mp mq">    predictions = argmax(model3.predict(data, batch_size=batch_size), axis=1)<br/>    acc_model3.append(sum(predictions[~mask] == unlabeled_data_labels) / len(unlabeled_data_labels) * 100)</span><span id="97e3" class="le lf it mj b gy mr mo l mp mq">print("\nAverage accuracy on unlabeled set:", mean(acc_model3), "%")</span></pre><p id="eeb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个实验的平均准确率为63% (+3)。</p><p id="8240" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有趣的是，在最后一个实验中，我们基本上是在用graphCNN 执行一个<a class="ae md" href="https://en.wikipedia.org/wiki/Semi-supervised_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">半监督学习</strong> </a> <strong class="kk iu">:来自未标记实例的信息与标记实例一起用于构建一个<strong class="kk iu">基于图的直推模型</strong>。</strong></p><p id="2ef6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">包含代码的完整Jupyter笔记本可以在<a class="ae md" href="https://github.com/ngshya/tf-notes/blob/master/graphcnn_keras_custom_layer.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="9023" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">参考</h2><ul class=""><li id="9b16" class="nm nn it kk b kl lx ko ly kr no kv np kz nq ld nr ns nt nu bi translated"><a class="ae md" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">https://tkipf.github.io/graph-convolutional-networks/</a></li><li id="f62d" class="nm nn it kk b kl nv ko nw kr nx kv ny kz nz ld nr ns nt nu bi translated"><a class="ae md" href="https://www.tensorflow.org/api_docs/python/tf/keras" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/keras</a></li></ul></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><p id="bd6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">联系人:<a class="ae md" href="https://www.linkedin.com/in/shuyiyang/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>|<a class="ae md" href="https://twitter.com/deltarule" rel="noopener ugc nofollow" target="_blank">Twitter</a></p></div></div>    
</body>
</html>