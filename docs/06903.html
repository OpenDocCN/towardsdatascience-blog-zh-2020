<html>
<head>
<title>Exploring few useful functions in the Pytorch Library on Tensors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索 Pytorch 库中几个有用的张量函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-few-useful-functions-in-the-pytorch-library-on-tensors-d23ce14d142?source=collection_archive---------54-----------------------#2020-05-28">https://towardsdatascience.com/exploring-few-useful-functions-in-the-pytorch-library-on-tensors-d23ce14d142?source=collection_archive---------54-----------------------#2020-05-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e211" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过探索 Pytorch 的库开始使用 py torch</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/279b8aacbc7b2aa99e5a6b3f77f892ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LsT-OHq4Bp21nOTB"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马库斯·斯皮斯克在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="8b65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> D </span>由脸书人工智能研究实验室开发的<strong class="ky ir"> <em class="mb"> PyTorch </em> </strong>如今被广泛用作深度学习框架，原因很多，从小规模的机器学习原型到生产层面的应用。</p><blockquote class="mc md me"><p id="7ece" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">“Pytorch 是一个开源的机器学习框架，它加速了从研究原型到生产部署的道路”，官方<a class="ae kv" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">网站</a>上的描述说。</p></blockquote><p id="b6e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇博客故事是由<a class="ae kv" href="https://jovian.ml/" rel="noopener ugc nofollow" target="_blank"> Jovian.ml </a>与<a class="ae kv" href="https://www.freecodecamp.org/" rel="noopener ugc nofollow" target="_blank"> freecodecamp </a>合作提供的关于 Pytorch 深度学习的 6 周课程的一部分。#零托甘人</p><p id="b658" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我们将深入探讨<strong class="ky ir"> <em class="mb"> Pytorch </em> </strong>库中关于张量的 5 个有用函数。让我们开始吧。</p><p id="666d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先:<strong class="ky ir">进口 Pytorch </strong></p><pre class="kg kh ki kj gt mi mj mk ml aw mm bi"><span id="c46c" class="mn mo iq mj b gy mp mq l mr ms">import torch</span></pre><h1 id="5738" class="mt mo iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">torch.rand():</h1><p id="21d5" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">该函数返回一个张量，其中填充了区间[0，1]上均匀分布的随机数。它的一些参数如下所示:</p><blockquote class="mc md me"><p id="42a1" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">size(int)-定义输出张量形状的整数序列。可以是可变数量的参数或集合，如列表或元组。</p><p id="faec" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">dtype (torch.dtype，可选)-返回张量的所需数据类型。默认值:如果没有，则使用全局默认值(请参见 torch.set_default_tensor_type())。</p><p id="ac31" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">requires_grad (bool，可选)-如果自动签名应该记录对返回张量的操作。默认值:False。</p></blockquote><p id="05fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看这个相当简单的函数的几个例子。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">工作示例-1</p></figure><p id="d956" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以观察到，我们得到了区间[0，1]中 9 个元素的张量向量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">工作示例-2</p></figure><p id="3ee5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，我们得到一个形状为 3x4 的张量，其元素在区间[0，1]中</p><blockquote class="mc md me"><p id="ef20" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">何时使用该功能？</p><p id="e94a" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">当我们希望以随机值的张量作为[0，1]范围内的元素时，可以使用这个函数。</p></blockquote></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><h1 id="5b11" class="mt mo iq bd mu mv ny mx my mz nz nb nc jw oa jx ne jz ob ka ng kc oc kd ni nj bi translated">torch.mean():</h1><p id="502c" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">torch.mean 函数返回张量的平均值。下面列出了它的一些参数。</p><blockquote class="mc md me"><p id="8b2e" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated"><strong class="ky ir">输入</strong>(张量)——输入张量。</p><p id="f4cc" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated"><strong class="ky ir"> dim </strong> (int 或 python 的 tuple:ints)-要减少的一个或多个维度。</p><p id="9db2" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated"><strong class="ky ir">keep dim</strong>(bool)——输出张量是否保留 dim。</p><p id="cdfe" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated"><strong class="ky ir"> out </strong>(张量，可选)—输出张量。</p></blockquote><p id="912e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们看几个使用这个非常方便的函数的例子。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="afb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在单元格中，通过对张量向量应用 torch.mean()，我们可以观察到张量向量中的所有数字相加并除以元素的个数。</p><p id="ae96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们用这个函数来解决一些复杂的问题。</p><p id="7471" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有时我们可能不想在整个张量上计算平均值，而是在行或列上计算。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用 torch.mean()的“dim”参数</p></figure><p id="1c78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上述单元格中，torch.mean()应用于(3，5)张量，并带有一个名为“dim”的附加参数。此参数指定应该对行(如果 dim=0，则为每一列)或列(如果 dim=1，则为每一行)取平均值</p><blockquote class="mc md me"><p id="6c47" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="iq">有人曾经说过，“当你知道如何打破一个东西，那么你就真正知道如何用好它。”现在，让我们试着打破这个函数，我的意思是让我们理解这个函数什么时候不能像预期的那样工作。</em> </strong></p></blockquote><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">打破火炬</p></figure><p id="79db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在单元格中，我们得到的运行时误差”只能计算浮点类型的平均值。却得到了布尔。”这意味着 torch.mean()只能应用于 floating 类型，其他所有情况下都会失败。</p><p id="a545" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不知道这是 bug 还是特性。我会把它留给你去想象。</p><blockquote class="mc md me"><p id="27c0" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">何时使用该功能？</p><p id="b8e3" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">当有人想要计算张量的平均值时，例如当我们想要使用均方根误差(RMSE)作为损失函数时，可以使用该函数。</p></blockquote></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><h1 id="a84f" class="mt mo iq bd mu mv ny mx my mz nz nb nc jw oa jx ne jz ob ka ng kc oc kd ni nj bi translated">torch.view(形状):</h1><p id="23a1" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">PyTorch 有趣的一点是，它允许一个张量成为一个现有张量的视图。视图张量与其基础张量共享相同的底层数据。视图避免了显式的数据复制，因此允许我们进行快速且节省内存的整形、切片和基于元素的操作。</p><p id="83ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mb">该函数返回一个新的张量，其数据与自张量相同，但形状不同。</em>T9】</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="270e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们可以观察到的，我们得到了给定张量的一个不同大小的副本，我们可以对它进行运算。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="dfcf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是使用 view 函数的另一个例子。这里的-1 是从其他维度推断出来的。</p><p id="64b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">看看这个功能什么时候不行。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="02f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，我们得到一个错误，因为形状应该匹配输入张量的大小。</p><blockquote class="mc md me"><p id="91a4" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">torch.view 的灵感来源于 numpy . ndarray . shape()或 numpy.reshape()。它创建了张量的新视图，只要新形状与原始张量的形状兼容。</p><p id="3241" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">torch.reshape()函数将返回一个视图，与使用 torch 完全相同。Tensor.view()只要新的形状与原始张量的形状兼容。否则，它将返回一个副本。</p><p id="f2d5" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">然而，torch.reshape()的注释警告说:</p><p id="14e1" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">连续输入和具有兼容步幅的输入可以在不复制的情况下被重新整形，但是不应该依赖于复制和查看行为。</p></blockquote></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><h1 id="938b" class="mt mo iq bd mu mv ny mx my mz nz nb nc jw oa jx ne jz ob ka ng kc oc kd ni nj bi translated">torch.linspace():</h1><p id="2995" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">个人感觉这个功能以后会很好用。</p><p id="b945" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该函数返回“开始”和“结束”参数之间等距点的一维张量向量。点数是“步数”参数。它的一些重要参数是:</p><blockquote class="mc md me"><p id="b11d" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">start(float)-点集的起始值</p><p id="4ec7" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">结束(浮点)-点集的结束值</p><p id="e91c" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">steps(int)-开始和结束之间的采样点数。默认值:100。</p><p id="7b74" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">out(张量，可选)-输出张量。</p><p id="d034" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">dtype (torch.dtype，可选)-返回张量的所需数据类型。默认值:如果没有，则使用全局默认值(请参见 torch.set_default_tensor_type())。</p><p id="48fb" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">requires_grad (bool，可选)—如果<strong class="ky ir">亲笔签名的</strong>应该记录对返回张量的操作。默认值:False。</p></blockquote><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="96d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在单元格中，开始=10，结束=4，步数=25。因此，我们得到 25 个等间距的点，在 10 和 4 之间(包括 10 和 4)。默认情况下，<strong class="ky ir"> PyTorch </strong>给我们的元素是浮点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="bf8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这类似于上面的例子，但是这里我们得到的是类型为<strong class="ky ir"> int32 </strong>的元素。</p><p id="119b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">现在我们来试着破函数。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="7e42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">torch.linspace()的“out”参数，它指定要由 torch.linspace 函数返回的值替换的所需张量。运行时错误“dtype Int 不匹配 dtype of out 参数(Float)”告诉我们，要使用“out”参数，两种类型的张量应该相同。</p><blockquote class="mc md me"><p id="36db" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">何时使用该功能？</p><p id="54cd" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">当我们知道真实数据位于特定区间时，可以使用 torch.linspace 创建张量形式的数据。</p></blockquote></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><h1 id="9ce6" class="mt mo iq bd mu mv ny mx my mz nz nb nc jw oa jx ne jz ob ka ng kc oc kd ni nj bi translated">torch.trace():</h1><p id="2caa" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">该函数返回输入二维矩阵的主对角线(从左上到右下)的元素之和。它唯一需要的参数是一个输入 2D 张量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="9190" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们所见，该函数接受输入的 2D 张量，并计算对角线上元素的总和。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="a6cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们所见，该函数接受输入的 2D 张量，并计算对角线上元素的总和。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="4103" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个函数只需要一个 2D 矩阵，给它任何东西都会破坏它。这里，我们给出了一个 1D 数组。</p><p id="43d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然现在还不可用，但是这个函数的一个简单的新特性是允许我们得到任意对角线元素的和。但那不会被称为矩阵的“迹”。</p><blockquote class="mc md me"><p id="1d17" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">何时使用该功能？</p><p id="a06a" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">单独来看，跟踪操作并不有趣，但是它提供了一种更简单的符号，并且它被用作其他关键矩阵操作中的一个元素。</p></blockquote></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><h1 id="2672" class="mt mo iq bd mu mv ny mx my mz nz nb nc jw oa jx ne jz ob ka ng kc oc kd ni nj bi translated">结论:</h1><p id="4bb3" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">这里，我们展示了一些与 PyTorch 张量的一些核心概念相关的基本函数，如:</p><ul class=""><li id="d8ce" class="od oe iq ky b kz la lc ld lf of lj og ln oh lr oi oj ok ol bi translated">如何生成元素在[0，1]范围内的随机张量？</li><li id="f5e5" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">如何计算张量及其不同变量的平均值？</li><li id="babf" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">如何为给定的张量创建不同的<strong class="ky ir">视图</strong>。</li><li id="c522" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">如何创建一个张量，其元素在点与点之间等距。</li><li id="77e7" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">如何计算 2D 张量的轨迹？</li></ul><p id="7d2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望你喜欢这个博客。感谢您的阅读，我希望这对您有所帮助。</p><p id="3baa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">参考资料:</p><ul class=""><li id="8d89" class="od oe iq ky b kz la lc ld lf of lj og ln oh lr oi oj ok ol bi translated">链接到我的笔记本:<a class="ae kv" href="https://jovian.ml/shivasankeerth/01-tensor-operations" rel="noopener ugc nofollow" target="_blank">https://jovian.ml/shivasankeerth/01-tensor-operations</a></li><li id="7c60" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">链接开始 py torch ZerotoGANs by freeCodeCamp:<a class="ae kv" href="https://www.freecodecamp.org/news/free-deep-learning-with-pytorch-live-course/" rel="noopener ugc nofollow" target="_blank">https://www . freeCodeCamp . org/news/free-deep-learning-with-py torch-live-course/</a></li><li id="ef4c" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated"><a class="ae kv" href="https://pytorch.org/docs/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Pytorch 文档</a></li></ul></div></div>    
</body>
</html>