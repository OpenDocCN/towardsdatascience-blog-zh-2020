<html>
<head>
<title>Walking around the globe with Action Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带着动作识别环游世界</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/walking-around-the-globe-with-action-recognition-64bceaa65435?source=collection_archive---------57-----------------------#2020-05-03">https://towardsdatascience.com/walking-around-the-globe-with-action-recognition-64bceaa65435?source=collection_archive---------57-----------------------#2020-05-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/fa137d5bd8fc942cdcdf708effb96e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xW29v6geiYHenSEZ"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">古伊列梅·斯特卡内拉在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="f2b0" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">如何在家使用 AI 技术环游地球？</h2></div><p id="7048" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我们将构建一个能够使用 Tensorflow 识别一些动作的应用程序，并将结果发送到 Google Street View，由它来执行这些动作。下面的 GIF 显示了应用程序的输出:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/13cc80e94a4c0c1726b6dc6451bde441.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*ulwmaScJ84IBLbpkD-N7WA.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">实时动作识别示例及其在谷歌街景中的应用</p></figure><p id="38f1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lw">在开始之前，需要注意的是有两个人参与了这个项目。因此，在本文中，我们将使用复数。</em></p><p id="d4ef" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有各种各样的方法来解决这个问题:在意识到没有多少现成的动作识别模型来重新训练所需的动作后，任何人脑海中出现的第一个想法可能是使用在具有不同动作的巨大数据集上训练的卷积神经网络，将每个图像映射到人正在执行的动作。该领域受过更多培训的人可能会考虑使用 Inception Network 或 ResNet，并在 ImageNet 或 COCO 数据集上转移学习。</p><p id="337c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，经过一些思考后，人们可以意识到，将图像输入神经网络并期望获得准确的分类是远远不现实的。模型将如何识别人在哪里？如果图像中不止一个人呢？假设地球上有和人一样多的体形，我们如何做出一个可接受的概括？</p><p id="8f5d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过前面的段落，我们想指出，在某些情况下，最初的想法并不是更好的想法，对问题的反思可能会导致更深入、更稳健的解决方案。<em class="lw">(看到我们在那里做了什么吗？)</em></p><h1 id="959d" class="lx ly jg bd lz ma mb mc md me mf mg mh km mi kn mj kp mk kq ml ks mm kt mn mo bi translated">“两阶段”方法</h1><p id="5d32" class="pw-post-body-paragraph kv kw jg kx b ky mp kh la lb mq kk ld le mr lg lh li ms lk ll lm mt lo lp lq ij bi translated">在我们看来，这不是一个单一的神经网络可以解决的问题，至少有两个阶段必须顺序工作:首先，应该有一种检测人身体关键点(或接合点)的方法，从而解决不同体型的问题。第二，分类系统应该在第一阶段的结果上工作，以关键点作为坐标，并且以这种方式使其不知道初始图像的具体属性。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mu"><img src="../Images/2a3209b1416e682471aaa5141759a12a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7CmxgwL5JCK4f2WEDp0nnw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">动作识别的基本模型</p></figure><ol class=""><li id="3e0a" class="mv mw jg kx b ky kz lb lc le mx li my lm mz lq na nb nc nd bi translated">姿态检测</li></ol><p id="a5d8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">姿势检测的问题非常广泛，对于我们大多数必须在 Google Colab 中训练他们的模型的人来说，这可能是不可估量的。幸运的是，有一些选项(训练过的模型)表现很好并且具有很高的准确性，比如<a class="ae jd" href="https://github.com/tensorflow/tfjs-models/tree/master/posenet" rel="noopener ugc nofollow" target="_blank"> PoseNet </a>或者<a class="ae jd" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> OpenPose </a>。这里我们只有一个小条件:选择的模型必须在推理模式下以合适的帧速率运行，因为我们希望实时识别动作。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/7fc3ebb80c2ad057871fe7f5947ca18a.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*zlLbpslnXIlgp_Xoc_rhUA.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">姿态估计。来源:<a class="ae jd" href="https://www.tensorflow.org/lite/models/pose_estimation/overview" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/lite/models/pose _ estimation/overview</a></p></figure><p id="9fae" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">受限于这个条件，我们准备选择 PoseNet，即使在手机中也是实时运行的。此外，PoseNet 更容易使用，因为它提供了 Tensorflow Lite 和 tfjs 文件。PoseNet 有两种不同的“风格”:MobileNet 和 ResNet，第一种更快，但是(经过一些试错过程)非常不准确。因此，我们将使用以 ResNet 为主干的 PoseNet。</p><p id="1112" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦我们得到了关键点，下一步就是用分类模型识别动作。</p><p id="329b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2.分类模型</p><p id="f464" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们停下来想一想，我们要对哪些动作进行分类:行走、向左或向右看、站立。从关键点的角度来看，第一个(行走)与最后一个(站立)的不同之处在于它们跨帧的移动:行走是不同站立帧的拼接，而站立是相等站立帧的拼接。因此，对分类模型的输入必须来自一系列帧，而不是一个单独的帧。以前的模式应该相应地改变:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/6d097b05d03d44e56434a622b2acb490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*flcFI8ILrjW2jllZ2qwN8Q.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">动作识别的选定模型</p></figure><p id="d4b5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因为我们想要处理时间演化，所以一个好主意是选择递归神经网络，其输入将具有维度(#帧，#关键点)，其中#帧表示预测动作所需的图像数量。此外，人们必须注意到关键点带有一定程度的信心，这意味着从一帧到下一帧可能会有“消失”的关键点。为了解决这个问题，我们施加了这样的条件，即一个系列的第一帧应该呈现眼睛和臀部，并且在同时出现的帧中消失的关键点将通过先前的高置信度关键点来推断。这一推理的动机来自于这样一个事实，即一幅图像在两帧之间不会有很大的不同，为了实现这一点，我们需要一个高的帧速率。</p><p id="2499" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将使用一个带有一层 LSTM 细胞的 RNN，后面跟着一个前馈神经网络。使用正常反向传播和 SGD 进行训练，下面的图片显示了训练过程的准确性(左)和损失(右):</p><div class="ls lt lu lv gt ab cb"><figure class="ng is nh ni nj nk nl paragraph-image"><img src="../Images/e96b4aa5272fc3b8c3d9c6275fbb87d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*ir-1y81hRsSFor35dwxhzg.png"/></figure><figure class="ng is nh ni nj nk nl paragraph-image"><img src="../Images/d679af70587406f6799308cfffac6efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*6aXGbKCwM26sjWfHAiF1Jw.png"/><p class="iz ja gj gh gi jb jc bd b be z dk nm di nn no translated">左:模型精度。右:损失。绿色表示培训结果，灰色表示验证结果</p></figure></div><p id="34b1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">识别出这个动作后，剩下的唯一工作就是将它传输到谷歌街景中。鉴于街景可以通过键盘控制，Selenium 是一个很好的选择，可以让我们与网页进行交互。建立连接后，向前执行一步，向驱动程序发送一个点击:</p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="np nq l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">使用 Selenium 在谷歌街景中前行</p></figure><p id="1fdf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">并且按键事件将向左或向右移动相机:</p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="np nq l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">使用 Selenium 旋转谷歌街景的摄像头</p></figure><p id="aabe" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这样，关于如何制作一个动作识别系统的基本概念将被涵盖。如果您想改进我们的模型或者只是玩玩它，请随意浏览 GitHub 中的资源库。</p><div class="ip iq gp gr ir nr"><a href="https://github.com/Moving-AI/virtual-walk" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jh gy z fp nw fr fs nx fu fw jf bi translated">移动人工智能/虚拟行走</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">在隔离期间，由于新冠肺炎疫情，我们现在有权利在街上自由行动…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">github.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of ix nr"/></div></div></a></div><p id="70ef" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此外，让我知道任何关于模型或文章本身的方法的建议或意见。</p></div></div>    
</body>
</html>