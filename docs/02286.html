<html>
<head>
<title>A Guide to Neural Network Loss Functions with Applications in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络损失函数指南及其在Keras中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-neural-network-loss-functions-with-applications-in-keras-3a3baa9f71c5?source=collection_archive---------24-----------------------#2020-03-04">https://towardsdatascience.com/a-guide-to-neural-network-loss-functions-with-applications-in-keras-3a3baa9f71c5?source=collection_archive---------24-----------------------#2020-03-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="41a6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">二元交叉熵、余弦邻近度、铰链损耗，还有6个更多</em></h2></div><p id="be69" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">损失函数是训练神经网络的重要部分，选择正确的损失函数有助于神经网络知道它有多远，因此它可以正确地利用其优化器。本文将讨论Keras支持的几个损失函数——它们是如何工作的，它们的应用，以及实现它们的代码。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="68ea" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">交叉熵</h1><p id="e062" class="pw-post-body-paragraph kj kk it kl b km me ju ko kp mf jx kr ks mg ku kv kw mh ky kz la mi lc ld le im bi translated">二进制交叉熵在数学上定义为—</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mj"><img src="../Images/59b867614aac23f9b0c17eecf683f0f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fhgwEPXuPtqRZIDN.png"/></div></div></figure><p id="de50" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">—给定正确的目标值<em class="mv"> t </em>和预测值<em class="mv"> p </em>。</p><p id="22ef" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">给定正确目标值0的<em class="mv"> p </em>值，二进制交叉熵值可以绘制为—</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mw"><img src="../Images/3941f6310aea3f536239fe9c2d036fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*A1JmEFw_oBzVpCmV.png"/></div></div></figure><p id="f5a4" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">给定正确目标1的<em class="mv"> p </em>值，二进制交叉熵值可以绘制为—</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mx"><img src="../Images/162e88ed51cd35982f948907bf3b201c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*f0oxIfNTFvYK6hZ_.png"/></div></div></figure><p id="8c67" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">熵是对某一分布不确定性的度量，交叉熵是代表目标分布和预测分布之间不确定性的值。</p><pre class="mk ml mm mn gt my mz na nb aw nc bi"><span id="0007" class="nd ln it mz b gy ne nf l ng nh">#FOR COMPILING<br/>model.compile(loss='binary_crossentropy', optimizer='sgd')<br/># optimizer can be substituted for another one</span><span id="db98" class="nd ln it mz b gy ni nf l ng nh">#FOR EVALUATING<br/>keras.losses.binary_crossentropy(y_true, y_pred, from_logits=<strong class="mz iu">False</strong>, label_smoothing=0)</span></pre><p id="81ed" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">分类交叉熵和稀疏分类交叉熵是二元交叉熵的版本，适用于几个类别。当一个样本有几个类别或标签是软概率时，应该使用分类交叉熵，当类别互斥时，应该使用稀疏分类交叉熵。</p><p id="f015" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">分类交叉熵:</p><pre class="mk ml mm mn gt my mz na nb aw nc bi"><span id="ff57" class="nd ln it mz b gy ne nf l ng nh">#FOR COMPILING<br/>model.compile(loss='categorical_crossentropy', optimizer='sgd')<br/># optimizer can be substituted for another one</span><span id="3942" class="nd ln it mz b gy ni nf l ng nh">#FOR EVALUATING<br/>keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=<strong class="mz iu">False</strong>, label_smoothing=0)</span></pre><p id="9bf7" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">稀疏分类交叉熵:</p><pre class="mk ml mm mn gt my mz na nb aw nc bi"><span id="ace6" class="nd ln it mz b gy ne nf l ng nh">#FOR COMPILING<br/>model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd')<br/># optimizer can be substituted for another one</span><span id="06c0" class="nd ln it mz b gy ni nf l ng nh">#FOR EVALUATING<br/>keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=<strong class="mz iu">False</strong>, axis=-1)</span></pre></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="178d" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">余弦近似/余弦相似</h1><p id="1a66" class="pw-post-body-paragraph kj kk it kl b km me ju ko kp mf jx kr ks mg ku kv kw mh ky kz la mi lc ld le im bi translated">余弦相似性是两个向量之间相似性的度量。数学表示是—</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nj"><img src="../Images/08ac6f9af178952fabe47eb385aa44ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JKz9cK3WqyAelW25MU3-fw.png"/></div></div></figure><p id="173b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">—给定两个向量<em class="mv"> A </em>和<em class="mv"> B </em>，其中<em class="mv"> A </em>表示预测向量，<em class="mv"> B </em>表示目标向量。</p><p id="03e4" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">较高的余弦接近度/相似度表示较高的准确度。完全相反的向量的余弦相似度为-1，完全正交的向量的余弦相似度为0，相同的向量的余弦相似度为1。</p><p id="7862" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">余弦近似可以在Keras中实现:</p><pre class="mk ml mm mn gt my mz na nb aw nc bi"><span id="ee9d" class="nd ln it mz b gy ne nf l ng nh">#FOR COMPILING<br/>model.compile(loss='cosine_proximity', optimizer='sgd')<br/># optimizer can be substituted for another one</span><span id="c062" class="nd ln it mz b gy ni nf l ng nh">#FOR EVALUATING<br/>keras.losses.cosine_proximity(y_true, y_pred, axis=-1)</span></pre></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="59c5" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">铰链损耗</h1><p id="c88c" class="pw-post-body-paragraph kj kk it kl b km me ju ko kp mf jx kr ks mg ku kv kw mh ky kz la mi lc ld le im bi translated">铰链损耗在数学上定义为—</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nk"><img src="../Images/d5cd5738f284e2d2fbfed6d4485e8105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aDVOc81XmuxzJyqMdaVojg.png"/></div></div></figure><p id="b501" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">—给定预测<em class="mv"> y </em>和目标值<em class="mv"> t </em>的<em class="mv">1<em class="mv">。</em>注意<em class="mv"> y </em>应该是一个概率而不是一个单一的类标签。</em></p><p id="8c40" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">下面是铰链损耗图，它是线性负值，直到达到1的<em class="mv"> x </em>。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/d94c16f33d372e55fdda89328b8dc489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/0*ciQOiS74m2vynWB_.png"/></div><p class="nm nn gj gh gi no np bd b be z dk translated"><a class="ae nq" href="https://i.stack.imgur.com/Ifeze.png" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><pre class="mk ml mm mn gt my mz na nb aw nc bi"><span id="e27c" class="nd ln it mz b gy ne nf l ng nh">#FOR COMPILING<br/>model.compile(loss='hinge', optimizer='sgd')<br/># optimizer can be substituted for another one</span><span id="889e" class="nd ln it mz b gy ni nf l ng nh">#FOR EVALUATING<br/>keras.losses.hinge(y_true, y_pred)</span></pre><p id="15e0" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">Hinge还有另外一个离经叛道的，<em class="mv">平方hinge </em>，这(正如你能猜到的)就是铰链函数，平方。</p><pre class="mk ml mm mn gt my mz na nb aw nc bi"><span id="7dd8" class="nd ln it mz b gy ne nf l ng nh">#FOR COMPILING<br/>model.compile(loss='squared_hinge', optimizer='sgd')<br/># optimizer can be substituted for another one</span><span id="52a7" class="nd ln it mz b gy ni nf l ng nh">#FOR EVALUATING<br/>keras.losses.squared_hinge(y_true, y_pred)</span></pre></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="6db4" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">胡伯损失</h1><p id="b3db" class="pw-post-body-paragraph kj kk it kl b km me ju ko kp mf jx kr ks mg ku kv kw mh ky kz la mi lc ld le im bi translated">Huber损耗在数学上定义为</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/6c8a256c0e0bdfe2c8b781dae1e2fe6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*B0HfkWkGayPQMDn5KCabig.png"/></div></figure><p id="8eed" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">…对于系数<em class="mv"> c </em>，其中<em class="mv"> t </em>表示目标值和预测值之间的差值，可绘制为</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0392b209dfdf9b00a496abc4c1dd7eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*DRfBlV5wTRHX5Lbr.png"/></div></figure><p id="bed7" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">…对于<em class="mv"> c. </em>的各种值</p><p id="2948" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这可以在Keras中实现为</p><pre class="mk ml mm mn gt my mz na nb aw nc bi"><span id="32e7" class="nd ln it mz b gy ne nf l ng nh">#FOR COMPILING<br/>model.compile(loss='huber_loss', optimizer='sgd')<br/># optimizer can be substituted for another one</span><span id="acf5" class="nd ln it mz b gy ni nf l ng nh">#FOR EVALUATING<br/>keras.losses.huber_loss(y_true, y_pred, delta=1.0)</span></pre></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="535a" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">双曲余弦的对数</h1><p id="cee7" class="pw-post-body-paragraph kj kk it kl b km me ju ko kp mf jx kr ks mg ku kv kw mh ky kz la mi lc ld le im bi translated">双曲余弦函数是log(cosh( <em class="mv"> x </em>))，图形如下</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nt"><img src="../Images/327016f30ebfffe05865ee9e32d04f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0NEQrBILVeAadn8u8ymymQ.png"/></div></div></figure><p id="70eb" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">…其中<em class="mv"> x </em>表示预测值和目标值之间的差值。</p><pre class="mk ml mm mn gt my mz na nb aw nc bi"><span id="1807" class="nd ln it mz b gy ne nf l ng nh">#FOR COMPILING<br/>model.compile(loss='logcosh', optimizer='sgd')<br/># optimizer can be substituted for another one</span><span id="5c36" class="nd ln it mz b gy ni nf l ng nh">#FOR EVALUATING<br/>keras.losses.logcosh(y_true, y_pred)</span></pre></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="2cae" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">对数误差平方</h1><p id="938f" class="pw-post-body-paragraph kj kk it kl b km me ju ko kp mf jx kr ks mg ku kv kw mh ky kz la mi lc ld le im bi translated">平均绝对误差的对数或log( <em class="mv"> x </em>)如下图所示。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nu"><img src="../Images/907b306e917f7259b2886a1dfc37ae01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6VctqsM5WfJOL8SoilNrag.png"/></div></div></figure><p id="3c80" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">误差平方的对数比较特殊，因为如果误差更接近0(比如0.2到0.1)，那么误差减少0.1，损失函数的下降幅度会比更大(比如1.2到1.1)。</p><p id="7954" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这可以在Keras中实现为:</p><pre class="mk ml mm mn gt my mz na nb aw nc bi"><span id="4277" class="nd ln it mz b gy ne nf l ng nh">#FOR COMPILING<br/>model.compile(loss='mean_squared_logarithmic_error', optimizer='sgd')<br/># optimizer can be substituted for another one</span><span id="2e0a" class="nd ln it mz b gy ni nf l ng nh">#FOR EVALUATING<br/>keras.losses.mean_squared_logarithmic_error(y_true, y_pred)</span></pre></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="62e3" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">如果你喜欢，看看其他帖子:</p><ul class=""><li id="71de" class="nv nw it kl b km kn kp kq ks nx kw ny la nz le oa ob oc od bi translated"><a class="ae nq" rel="noopener" target="_blank" href="/a-guide-to-neural-network-layers-with-applications-in-keras-40ccb7ebb57a?source=post_stats_page---------------------------">神经网络层指南及其在Keras中的应用</a></li><li id="a284" class="nv nw it kl b km oe kp of ks og kw oh la oi le oa ob oc od bi translated"><a class="ae nq" href="https://medium.com/@andre_ye/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4" rel="noopener">神经网络优化器指南及其在Keras中的应用</a></li></ul></div></div>    
</body>
</html>