<html>
<head>
<title>Model Pruning in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的模型剪枝</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scooping-into-model-pruning-in-deep-learning-da92217b84ac?source=collection_archive---------35-----------------------#2020-06-21">https://towardsdatascience.com/scooping-into-model-pruning-in-deep-learning-da92217b84ac?source=collection_archive---------35-----------------------#2020-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="606b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/production-ml" rel="noopener" target="_blank">生产中的机器学习</a></h2><div class=""/><div class=""><h2 id="9ab1" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">本文讨论深度学习背景下的剪枝技术。</h2></div><p id="5e19" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这篇文章是我上一篇关于<a class="ae ln" rel="noopener" target="_blank" href="/a-tale-of-model-quantization-in-tf-lite-aebe09f255ca"> <strong class="kt jd"> <em class="lo">量子化</em> </strong> </a>文章的后续。在本文中，我们将在深度学习的背景下讨论模型修剪的机制。模型修剪是一种丢弃那些不代表模型性能的权重的艺术。精心修剪的网络会产生更好的压缩版本，它们通常适用于设备上的部署场景。</p><p id="2240" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">文章的内容分为以下几个部分:</p><ul class=""><li id="2580" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">函数和神经网络中“无意义”的概念</li><li id="85e0" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">修剪训练好的神经网络</li><li id="fe95" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">不同模型之间的代码片段和性能比较</li><li id="1d50" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">现代修剪技术</li><li id="1cbc" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">最后的想法和结论</li></ul><p id="3c6b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">(我们将要讨论的代码片段将基于 TensorFlow (2)和<a class="ae ln" href="https://www.tensorflow.org/model_optimization" rel="noopener ugc nofollow" target="_blank"> TensorFlow 模型优化工具包</a>)</p><p id="7859" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="lo">注</em> </strong>:文章镜像<a class="ae ln" href="https://app.wandb.ai/authors/pruning/reports/Scooping-into-Model-Pruning-in-Deep-Learning--VmlldzoxMzcyMDg" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><h1 id="28b2" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">函数中“无意义”的概念</h1><p id="4293" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">神经网络是函数逼近器。我们训练他们学习函数，这些函数捕获了表达输入数据点的基本表示。神经网络的权重和偏差被称为其(可学习的)<strong class="kt jd"> <em class="lo">参数</em> </strong>。通常，权重被称为正在学习的函数的<strong class="kt jd"> <em class="lo">系数</em> </strong>。</p><p id="5c18" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">考虑下面的函数-</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/34c3b68d252775bdeb222b11cc458e60.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*_lNJSr9obG2SDa7E8iI21Q.png"/></div></figure><p id="fd8b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在上面的函数中，我们在 RHS 上有两项:<em class="lo"> x </em>和 x，系数分别是 1 和 5。在下图中，我们可以看到，当轻推第一个系数时，函数的行为没有太大变化。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e49cb383bd351145a79f52b8c579b49f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*s8yPAQqGCLJftxvK.png"/></div></figure><p id="d8e1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下面是原函数的不同变型中的系数，可称为<strong class="kt jd"><em class="lo"/></strong>。丢弃这些系数不会真正改变函数的行为。</p><h1 id="c01f" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">神经网络的扩展</h1><p id="e425" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">上述概念也可以应用于神经网络。这需要更多的细节来展开。考虑一个训练好的网络的权重。我们如何理解不重要的权重呢？这里的前提是什么？</p><p id="3d68" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">要回答这个问题，请考虑梯度下降的优化过程。不是所有的权重都使用相同的梯度幅度来更新。给定损失函数的梯度是相对于权重(和偏差)取的。在优化过程中，用比其他权重更大的梯度幅度(正的和负的)来更新一些权重。这些权重被优化器认为是<strong class="kt jd"> <em class="lo">显著的</em> </strong>，以最小化训练目标。接收相对较小梯度的权重可以被认为是<strong class="kt jd"> <em class="lo">不重要的</em> </strong>。</p><p id="aa43" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">训练完成后，我们可以逐层检查网络的权重大小，并找出重要的权重。这个决定是用几种试探法做出的-</p><ul class=""><li id="24fd" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">我们可以按降序对权重大小进行排序，并选取队列中出现较早的权重。这通常与我们想要达到的稀疏水平(要修剪的权重的百分比)相结合。</li><li id="631e" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">我们可以指定一个阈值，其幅度高于该阈值的所有权重将被认为是重要的。这个方案可以有几种风格:</li></ul><p id="db7a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> i. </strong>阈值可以是整个网络中最低的权重幅度。</p><p id="7441" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">二。</strong>阈值可以是网络内部各层的局部权重值。在这种情况下，重要的权重被逐层过滤掉。</p><p id="5d44" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果所有这些变得难以理解，不要担心。在下一节中，事情会变得更清楚。</p><h1 id="8146" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">修剪训练好的神经网络</h1><p id="bae3" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">既然我们已经对所谓的重要权重有了相当多的了解，我们可以讨论基于<strong class="kt jd"> <em class="lo">幅度的</em> </strong>修剪。在基于大小的剪枝中，我们认为权重大小是剪枝的标准。通过修剪，我们真正的意思是把不重要的权重归零。以下代码片段可能有助于理解这一点</p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="7e8f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">(这段代码片段来自<a class="ae ln" href="https://colab.research.google.com/github/matthew-mcateer/Keras_pruning/blob/master/Model_pruning_exploration.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a>)</p><p id="b0b8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里有一个图形表示，在 之后，权重<strong class="kt jd"> <em class="lo">将会发生变化</em></strong></p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi nl"><img src="../Images/a45f29d4d812b9906c83040cc306400f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oIlGQx3aqjDgpRvYXt0fOA.png"/></div></div></figure><p id="6abf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">它也适用于偏见。值得注意的是，这里我们考虑接收形状为<code class="fe nq nr ns nt b">(1,2)</code>的输入并包含 3 个神经元的整个层。在网络被修剪以补偿其性能的任何下降之后，重新训练网络通常是明智的。在进行这种再训练时，重要的是要注意，在再训练期间，被修剪的权重<strong class="kt jd"> <em class="lo">不会被更新</em> </strong>。</p><h1 id="81f1" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">观看活动中的事物</h1><p id="bf08" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">废话少说！让我们来看看这些东西的作用。为了简单起见，我们将在 MNIST 数据集上测试这些概念，但是您也应该能够将它们扩展到更复杂的数据集。我们将使用一个浅层全连接网络，其拓扑结构如下-</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/6c6749635079c17a20671fe8a1d8e30d.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/0*WNjZka0kShuqCE2W.png"/></div></figure><p id="205c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该网络总共有 20，410 个可训练参数。对这个网络进行 10 个纪元的训练可以让我们得到一个很好的基线-</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi nv"><img src="../Images/2d0eac9a2c9bd1836256bf1389b55bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QkH6kWGkfhLqy4DTfvTytA.png"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">在这里与剧情<a class="ae ln" href="https://app.wandb.ai/authors/pruning" rel="noopener ugc nofollow" target="_blank">互动</a></p></figure><p id="0be4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们现在修剪它！我们将使用<code class="fe nq nr ns nt b">tensorflow_model_optimization</code>(别名为<code class="fe nq nr ns nt b">tfmot</code>)。<code class="fe nq nr ns nt b">tfmot</code>为我们提供了两种修剪方法:</p><ul class=""><li id="1878" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">选择一个训练有素的网络，用更多的训练来修剪它。</li><li id="1033" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">随机初始化一个网络，从头开始修剪训练它。</li></ul><p id="c589" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将对他们两个进行实验。这两种方法都应该包括一个修剪时间表，我们稍后将讨论这个时间表。以训练的形式修剪网络的基本原理是更好地引导训练目标，使得梯度更新可以相应地发生，从而以有效的方式调整未修剪的权重。</p><p id="ba79" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">注意</strong>在你的模型中修剪特定的层也是可能的，并且<code class="fe nq nr ns nt b">tfmot</code>允许你这么做。查看本指南以了解更多信息。</p><h1 id="3a0f" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">食谱 1:选择一个训练有素的网络，用更多的训练来修剪它</h1><p id="37e5" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">从现在开始，我们鼓励您跟随顶部提到的这个 Colab 笔记本。</p><p id="e8cd" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将采用我们之前训练的网络，并从那里对其进行删减。我们将应用一个修剪计划，在整个训练过程中保持稀疏水平不变(由开发人员指定)。表达这一点的代码如下:</p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="9d00" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我们开始训练它之前，修剪过的模型需要重新编译。我们以同样的方式编辑它，并打印它的摘要-</p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="72c6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们看到现在参数的数量已经改变了。这是因为<code class="fe nq nr ns nt b">tfmot</code>为网络中的每个权重添加了不可训练的掩码，以指示给定的权重是否应该被删减。掩码为 0 或 1。</p><p id="6249" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们训练它。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oa"><img src="../Images/78d7e11eea7c2157f777bbe35b5c21ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyB3QhBVn3WWUtNLnklsBQ.png"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">与剧情互动<a class="ae ln" href="https://app.wandb.ai/authors/pruning" rel="noopener ugc nofollow" target="_blank">这里</a></p></figure><p id="7f71" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以看到，修剪模型并不影响性能。红线对应于修剪实验。</p><p id="09d9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">注意</strong>:</p><ul class=""><li id="aa49" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">修剪计划是必须指定的，以便在训练模型时实际修剪模型。我们还指定了<code class="fe nq nr ns nt b"><a class="ae ln" href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/UpdatePruningStep" rel="noopener ugc nofollow" target="_blank">UpdatePruningStep</a></code>回调，以便它在训练期间负责修剪更新。</li><li id="a70b" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated"><code class="fe nq nr ns nt b"><a class="ae ln" href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PruningSummaries" rel="noopener ugc nofollow" target="_blank">PruningSummaries</a></code>提供了关于在训练期间如何保持稀疏度和幅度阈值的总结。你可以在这里看到一个这样的例子<a class="ae ln" href="https://tensorboard.dev/experiment/sRQnrycaTMWQOaswXzClYA/#scalars&amp;_smoothingWeight=0" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="a8ca" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">将修剪计划视为一个超参数。<code class="fe nq nr ns nt b">tfmot</code>提供了另一个现成的修剪计划- <code class="fe nq nr ns nt b"><a class="ae ln" href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PolynomialDecay" rel="noopener ugc nofollow" target="_blank">PolynomialDecay</a></code>。</li><li id="fe08" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">您可能希望将修剪计划中的<code class="fe nq nr ns nt b">end_step</code>参数设置为小于或等于您将为其训练模型的时期数。此外，您可能需要试验一下<code class="fe nq nr ns nt b">frequency</code>参数(它表示应该应用修剪的频率),以便获得良好的性能和期望的稀疏性。</li></ul><p id="2166" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们还可以通过编写如下测试来验证<code class="fe nq nr ns nt b">tfmot</code>是否达到了目标稀疏度:</p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="7a4c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在修剪过的模型上运行它应该为所有被修剪的层产生<code class="fe nq nr ns nt b">True</code>。</p><h1 id="f273" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">诀窍 2:随机初始化一个网络，通过从头开始训练来修剪它</h1><p id="0618" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">在这种情况下，除了我们不是从一个已经训练好的网络开始，而是从一个随机初始化的网络开始之外，一切都保持不变。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ob"><img src="../Images/ac7b459e8e4c08099cb304227206806a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oo7M5C_AJ5YFzmZAs3Uiww.png"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">在这里与剧情<a class="ae ln" href="https://app.wandb.ai/authors/pruning" rel="noopener ugc nofollow" target="_blank">互动</a></p></figure><p id="8bdf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">绿线对应于从头修剪实验。我们可以观察到其他两个模型的性能有所下降，但这是意料之中的，因为我们不是从一个已经训练好的模型开始的。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/866ac672f2f42af2786611e1329d6ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*dKZwp42khAQ8hJ_4BqgFcA.png"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">在这里与剧情<a class="ae ln" href="https://app.wandb.ai/authors/pruning" rel="noopener ugc nofollow" target="_blank">互动</a></p></figure><p id="af2a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当我们通过从头开始训练来修剪网络时，通常会花费最多的时间。这也是预期的，因为网络正在计算如何最好地更新参数，以便达到目标稀疏度水平。</p><p id="fb91" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所有这些都很好，但是为了能够真正体会到修剪的力量，我们需要挖掘得更深一些:</p><ul class=""><li id="1017" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">导出同一网络的修剪和未修剪的变体，压缩它们，并记录它们的大小。</li><li id="b1f4" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">量化它们，压缩它们的量化版本，记录它们的大小，并评估它们的性能。</li></ul><p id="6c82" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们在下一节处理它。</p><h1 id="2a99" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">性能赋值</h1><p id="d8a6" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">我们将使用标准的<code class="fe nq nr ns nt b">zipfile</code>库将模型压缩成<code class="fe nq nr ns nt b">.zip</code>格式。当序列化修剪后的模型时，我们需要使用<code class="fe nq nr ns nt b"><a class="ae ln" href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/strip_pruning" rel="noopener ugc nofollow" target="_blank">tfmot.sparsity.keras.strip_pruning</a></code>，它将删除由<code class="fe nq nr ns nt b">tfmot</code>添加到模型中的修剪包装器。否则，我们将无法在修剪后的模型中看到任何压缩优势。</p><p id="297c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，压缩常规 Keras 模型保持不变。</p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="a224" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><code class="fe nq nr ns nt b">file</code>应该是一个已经序列化的 Keras 模型的路径(修剪的和常规的)。</p><p id="f87c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在下图中，我们可以看到压缩模型的重量比常规的 Keras 模型轻，但它们仍然有很好的性能。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi od"><img src="../Images/8a76387cdb3ebcf0d3d6e819cf1ce700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rpblyk2W-8eNcXubgZzf7A.png"/></div></div></figure><p id="447c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以使用 TensorFlow Lite 量化我们的模型，在不影响性能的情况下进一步减小它们的大小。<strong class="kt jd">注意</strong>在将<strong class="kt jd">修剪后的模型</strong>传递给 TensorFlow Lite 的转换器时，您应该在剥离修剪包装后传递它们。本报告更详细地讨论了量化。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oe"><img src="../Images/70d216fe8166392f751195488d5c6d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0WIoyTf_rtzLA6l6G456og.png"/></div></div></figure><p id="a84b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">除了精度测量之外，<strong class="kt jd">压缩比</strong>是另一种广泛使用的技术，用于测量特定修剪算法的功效。压缩比是剪枝网络中剩余参数分数的倒数。</p><p id="7010" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种量化也被称为<strong class="kt jd">训练后量化</strong>。因此，这里有一个简单的方法供您遵循，以优化您的部署模型:</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi of"><img src="../Images/d310537b4f165db2ad0b27b128945a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*J4XMr7VJPv5n3ufU.png"/></div></div></figure><p id="059e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在下一节中，我们将学习一些现代的修剪食谱。如果你想更多地追求模型优化领域，这些想法将值得进一步探索。</p><h1 id="f76c" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">一些现代修剪食谱</h1><p id="ac36" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">让我们从以下激励性问题开始这一部分:</p><ol class=""><li id="0662" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm og lv lw lx bi translated">当我们重新训练一个修剪过的网络时，如果将<strong class="kt jd"> <em class="lo">个未修剪的权重</em> </strong>初始化为它们原来的初始幅度会怎么样？如果你从一个已经训练好的网络(比如网络 A)中得到一个修剪过的网络，考虑网络 A 的这些初始大小。</li><li id="db1e" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm og lv lw lx bi translated">当在具有预训练网络的迁移学习机制中执行<strong class="kt jd"> <em class="lo">基于幅度的</em> </strong>修剪时，我们如何决定权重的重要性？</li></ol><h2 id="71f5" class="oh me it bd mf oi oj dn mj ok ol dp mn la om on mp le oo op mr li oq or mt iz bi translated">赢得门票的机会</h2><p id="0af4" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">弗兰克尔等人在他们关于彩票假说的开创性论文中对第一个问题进行了大量的探索。因此，在修剪了一个已经训练好的网络之后，具有上述初始化的子网被称为优胜标签。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi os"><img src="../Images/dedacb2f4d3d2d561ea5eb4a5b2b75c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*809Ul4bihgUmSRES.png"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">来源:原始文件</p></figure><p id="72f8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">作为这种方法背后的基本原理，您可以推断在网络的初始训练期间，参数的特定初始化引导了优化过程。现在，在优化领域反应良好的权重(意味着它们比其他权重走得更远)实际上最终赢得了彩票。因此，为了让它(重新)训练得更好，如果我们将权重初始化为它们最大的初始大小，优化过程就会很好地利用它们。感谢 Yannic Kilcher 的<a class="ae ln" href="https://www.youtube.com/watch?v=ZVVnvZdUMUk" rel="noopener ugc nofollow" target="_blank">这个漂亮的解释</a>。</p><p id="e036" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这篇论文提供了大量不同的实验来支持这个假设，绝对值得一读。</p><h2 id="8726" class="oh me it bd mf oi oj dn mj ok ol dp mn la om on mp le oo op mr li oq or mt iz bi translated">彩票假说的系统探索</h2><p id="d043" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">在最初的彩票假设论文中，Frankle 等人只研究了如果幸存的权重在重新训练之前被重新初始化到它们的最大初始幅度，修剪后的网络如何表现。就在彩票假说<a class="ae ln" href="https://www.youtube.com/watch?v=s7DqRZVvRiQ" rel="noopener ugc nofollow" target="_blank">在 2019 </a>上提出后，周等人发表了一篇关于<a class="ae ln" href="https://arxiv.org/abs/1905.01067" rel="noopener ugc nofollow" target="_blank">解构彩票</a>的论文，研究了在修剪过程中处理存活和未存活的权重的不同方法。还提出了<strong class="kt jd"> <em class="lo"> supermasks </em> </strong>基本都是可以学习的面具。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/d7ea9917e120607ae330e27956d498b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*ov-emJ0eWEIvxL2M.png"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">来源:原始文件</p></figure><h2 id="db28" class="oh me it bd mf oi oj dn mj ok ol dp mn la om on mp le oo op mr li oq or mt iz bi translated">彩票假说的推广</h2><p id="5005" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">为了能够将彩票假设扩展到像 ImageNet 这样的数据集，Frankle 等人发表了一篇关于<a class="ae ln" href="https://arxiv.org/abs/1912.05671" rel="noopener ugc nofollow" target="_blank">线性模式连通性</a>的论文，这是彩票假设的一种推广。它提出了权重回卷作为一种潜在的方式来初始化修剪网络的幸存权重。之前，我们用它们的最大初始星等来初始化它们。权重倒带所做的是，它将幸存的权重倒带到原始网络训练中的某个地方。换句话说，幸存的权重被初始化为来自原始网络训练的时段 5 的幅度。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ou"><img src="../Images/40d38d556a598dfb1060cbc2e25c8b4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LGLEGCYcFYcNzOrc.png"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">来源:原始文件</p></figure><p id="72c8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">扩展这个想法，Renda 等人发表了一篇关于<a class="ae ln" href="https://openreview.net/forum?id=S1gSj0NKvB" rel="noopener ugc nofollow" target="_blank">学习速率倒回</a>的论文，该论文适用于在重新训练修剪过的网络时倒回学习速率表。作者还提出这是微调的替代方案。</p><p id="3770" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以，这些令人兴奋的想法主要是围绕着<strong class="kt jd"> <em class="lo">基于幅度的</em> </strong>修剪发展而来的。在最后一节中，我们将看到一种比基于数量的剪枝更好的剪枝方法，特别是对于迁移学习机制。</p><h2 id="6288" class="oh me it bd mf oi oj dn mj ok ol dp mn la om on mp le oo op mr li oq or mt iz bi translated">基于权重移动的修剪</h2><p id="03a8" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">在他们关于<a class="ae ln" href="https://arxiv.org/abs/2005.07683" rel="noopener ugc nofollow" target="_blank">移动修剪</a>的论文中，Sanh 等人提出了一种基于幅度的修剪的替代方法，该方法专门用于处理迁移学习任务的预训练模型的修剪。</p><p id="9d93" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">基于幅度的剪枝与我们之前已经讨论过的重要性概念正相关。在这种情况下，这里的重要性简单地表示权重的绝对大小。这些数值越低，意义越小。现在，当我们尝试使用一个在不同数据集上预先训练的模型进行迁移学习时，这种重要性实际上可以改变。优化源数据集时重要的权重对于目标数据集可能不重要。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/6563825ef60b473ce6176972a77b397b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*FMO9mUQQm8zpX-H2.png"/></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">来源:原始文件</p></figure><p id="371d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，在迁移学习过程中，<strong class="kt jd"> <em class="lo">将</em> </strong>移向零的预训练权重实际上可以认为相对于目标任务是不显著的，而<strong class="kt jd"> <em class="lo">将</em> </strong>移得更远的权重可以认为是显著的。这就是这种方法的名字来源于——<strong class="kt jd"><em class="lo">移动</em> </strong>修剪。再次感谢 Yannic 的<a class="ae ln" href="https://www.youtube.com/watch?v=nxEr4VNgYOE" rel="noopener ugc nofollow" target="_blank">精彩解说</a>。</p><h1 id="34b8" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">结论、致谢和最后的想法</h1><p id="d803" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated">如果坚持到最后，太好了！我希望这份报告让你对深度学习中的修剪有一个公平的想法。我要感谢(谷歌的)拉结尔和云璐，他们为我提供了关于<code class="fe nq nr ns nt b">tfmot</code>的重要信息和一些关于修剪本身的额外想法。</p><p id="879b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我想在这方面进一步探讨的一些想法是:</p><ul class=""><li id="2667" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">如果我们可以在训练和再训练网络时使用有区别的修剪时间表呢？</li><li id="4408" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">在进行幅度修剪时，周等人向我们展示了在修剪后的网络中处理幸存权重的初始化的不同方式。我们可以学习一种有效的方法来系统地将它们结合起来吗？</li></ul><p id="2712" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在撰写本报告时(2020 年 6 月)，最近的修剪方法之一是<a class="ae ln" href="https://arxiv.org/abs/2006.05467" rel="noopener ugc nofollow" target="_blank"> SynFlow </a>。SynFlow <em class="lo">不需要任何数据</em>来修剪网络，它使用<em class="lo">突触显著性分数</em>来确定网络中参数的重要性。</p><p id="a11b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我愿意通过 Twitter ( <a class="ae ln" href="http://twitter.com/RisingSayak" rel="noopener ugc nofollow" target="_blank"> @RisingSayak </a>)听取你的反馈。</p><h1 id="7c1c" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">参考</h1><p id="503d" class="pw-post-body-paragraph kr ks it kt b ku mv kd kw kx mw kg kz la mx lc ld le my lg lh li mz lk ll lm im bi translated"><strong class="kt jd"> <em class="lo">(排名不分先后)</em> </strong></p><ul class=""><li id="394d" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">Matthew Mcateer 在 Keras 中的模型修剪探索</li><li id="ac95" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated"><a class="ae ln" href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras" rel="noopener ugc nofollow" target="_blank">官方</a><code class="fe nq nr ns nt b"><a class="ae ln" href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras" rel="noopener ugc nofollow" target="_blank">tfmot</a></code><a class="ae ln" href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras" rel="noopener ugc nofollow" target="_blank">Keras 修剪指南</a></li><li id="28be" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated"><a class="ae ln" href="http://www.jfrankle.com/dissecting-pruned-neural-nets.pdf" rel="noopener ugc nofollow" target="_blank">剖析修剪过的神经网络</a>作者 Frankle 等人。</li><li id="5334" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated"><a class="ae ln" href="https://www.youtube.com/watch?v=ZVVnvZdUMUk" rel="noopener ugc nofollow" target="_blank">彩票假说解释视频</a>作者 Yannic Kilcher</li><li id="6f6b" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated"><a class="ae ln" href="https://www.youtube.com/watch?v=nxEr4VNgYOE" rel="noopener ugc nofollow" target="_blank">运动修剪解说视频</a>作者 Yannic Kilcher</li></ul></div></div>    
</body>
</html>