<html>
<head>
<title>Text Analysis &amp; Feature Engineering with NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自然语言处理的文本分析和特征工程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d?source=collection_archive---------3-----------------------#2020-06-09">https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d?source=collection_archive---------3-----------------------#2020-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bf7a59d41b19f964ce71b34753c515bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgqxDMP5qD1HE_uM33zZrg.png"/></div></div></figure><div class=""/><div class=""><h2 id="4761" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">语言检测、文本清理、长度测量、情感分析、命名实体识别、N元语法频率、词向量、主题建模</h2></div><h2 id="d7de" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">摘要</h2><p id="dd40" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">在本文中，我将使用NLP和Python来解释如何为您的机器学习模型分析文本数据和提取特征。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mi"><img src="../Images/7c5fba1b8cdc020431fb02b50c98e346.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pZVocDqN8uRdnGyrrWbnlQ.png"/></div></div></figure><p id="a8ad" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated"><a class="ae ms" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"><strong class="lr jf">【NLP(自然语言处理)</strong> </a>是人工智能的一个领域，研究计算机和人类语言之间的交互，特别是如何给计算机编程，以处理和分析大量的自然语言数据。NLP通常用于文本数据的分类。<strong class="lr jf">文本分类</strong>就是根据文本数据的内容给文本数据分配类别的问题。文本分类最重要的部分是<strong class="lr jf">特征工程</strong>:从原始文本数据中为机器学习模型创建特征的过程。</p><p id="854e" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">在本文中，我将解释不同的方法来分析文本并提取可用于构建分类模型的特征。我将展示一些有用的Python代码，这些代码可以很容易地应用于其他类似的情况(只需复制、粘贴、运行)，并通过注释遍历每一行代码，以便您可以复制这个示例(下面是完整代码的链接)。</p><div class="is it gp gr iu mt"><a href="https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd jf gy z fp my fr fs mz fu fw jd bi translated">mdipietro 09/data science _人工智能_实用工具</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">github.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh ja mt"/></div></div></a></div><p id="27b4" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我将使用“<strong class="lr jf">新闻类别数据集</strong>”(下面的链接)，其中为您提供了从<em class="ni">赫芬顿邮报</em>获得的2012年至2018年的新闻标题，并要求您将它们归类到正确的类别中。</p><div class="is it gp gr iu mt"><a href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd jf gy z fp my fr fs mz fu fw jd bi translated">新闻类别数据集</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">根据标题和简短描述识别新闻的类型</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">www.kaggle.com</p></div></div><div class="nc l"><div class="nj l ne nf ng nc nh ja mt"/></div></div></a></div><p id="841e" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">特别是，我将经历:</p><ul class=""><li id="3a34" class="nk nl je lr b ls mn lv mo lc nm lg nn lk no mh np nq nr ns bi translated">环境设置:导入包并读取数据。</li><li id="e33e" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">语言检测:了解哪些自然语言数据在。</li><li id="b116" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">文本预处理:文本清洗和转换。</li><li id="c1d3" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">长度分析:用不同的度量标准测量。</li><li id="a72f" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">情感分析:确定文本是正面的还是负面的。</li><li id="cb65" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">命名实体识别:用预定义的类别(如人名、组织、位置)标记文本。</li><li id="a777" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">词频:找到最重要的<em class="ni">n</em>-克。</li><li id="784a" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">单词向量:将单词转换成数字。</li><li id="779a" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">主题建模:从语料库中提取主要主题。</li></ul></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h2 id="4dc8" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">设置</h2><p id="3390" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">首先，我需要导入以下库。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="9b9c" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## for data</strong><br/>import <strong class="og jf">pandas </strong>as pd<br/>import <strong class="og jf">collections<br/></strong>import <strong class="og jf">json</strong></span><span id="011d" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for plotting</strong><br/>import <strong class="og jf">matplotlib</strong>.pyplot as plt<br/>import <strong class="og jf">seaborn </strong>as sns<br/>import <strong class="og jf">wordcloud</strong></span><span id="fcef" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for text processing<br/></strong>import <strong class="og jf">re</strong><br/>import <strong class="og jf">nltk</strong></span><span id="7726" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for language detection</strong><br/>import <strong class="og jf">langdetect </strong></span><span id="9f92" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for sentiment</strong><br/>from <strong class="og jf">textblob </strong>import TextBlob</span><span id="2c2f" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for ner</strong><br/>import <strong class="og jf">spacy</strong></span><span id="ef53" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for vectorizer<br/></strong>from <strong class="og jf">sklearn </strong>import feature_extraction, manifold</span><span id="5b49" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for word embedding<br/></strong>import <strong class="og jf">gensim</strong>.downloader as gensim_api</span><span id="7b39" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## for topic modeling</strong><br/>import <strong class="og jf">gensim</strong></span></pre><p id="7db2" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">数据集包含在一个json文件中，所以我首先用<em class="ni"> json </em>包将它读入一个字典列表，然后将其转换成一个<em class="ni"> pandas </em> Dataframe。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="3400" class="kt ku je og b gy ok ol l om on">lst_dics = []<br/>with <strong class="og jf">open</strong>('data.json', mode='r', errors='ignore') as json_file:<br/>    for dic in json_file:<br/>        lst_dics.append( json.loads(dic) )</span><span id="19c5" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## print the first one</strong>      <br/>lst_dics[0]</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi op"><img src="../Images/00918c4395287e1c75fd5a05284d2172.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BkkQ0N0hSAp75PkmK29BpQ.png"/></div></div></figure><p id="741e" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">原始数据集包含超过30个类别，但是出于本教程的目的，我将使用3个类别的子集:娱乐、政治和技术。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="9140" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## create dtf</strong><br/>dtf = pd.DataFrame(lst_dics)</span><span id="c39b" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## filter categories</strong><br/>dtf = dtf[ dtf["category"].isin(['<strong class="og jf">ENTERTAINMENT</strong>','<strong class="og jf">POLITICS</strong>','<strong class="og jf">TECH</strong>']) ][["category","headline"]]</span><span id="40eb" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## rename columns</strong><br/>dtf = dtf.rename(columns={"category":"<strong class="og jf">y</strong>", "headline":"<strong class="og jf">text</strong>"})</span><span id="b317" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## print 5 random rows</strong><br/>dtf.sample(5)</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/c2f7e2aa08346334432fa144103f1498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hU3TMpthvym-IYU_VfnjuQ.png"/></div></div></figure><p id="3a77" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">为了理解数据集的组成，我将通过用条形图显示标签频率来研究单变量分布(只有一个变量的概率分布)。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="32d7" class="kt ku je og b gy ok ol l om on"><strong class="og jf">x = "y"</strong></span><span id="eeea" class="kt ku je og b gy oo ol l om on">fig, ax = plt.subplots()<br/>fig.suptitle(x, fontsize=12)<br/>dtf[x].reset_index().groupby(x).count().sort_values(by= <br/>       "index").plot(kind="barh", legend=False, <br/>        ax=ax).grid(axis='x')<br/>plt.show()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/7b355d77b762ada760bacb93f7775b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kmnby6Nv9O9zryhd60r2Bg.png"/></div></div></figure><p id="b673" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">数据集是不平衡的:与其他的相比，科技新闻的比例真的很小。这可能是建模过程中的一个问题，数据集的重新采样可能会有所帮助。</p><p id="34f6" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">现在一切都设置好了，我将从清理数据开始，然后我将从原始文本中提取不同的见解，并将它们作为dataframe的新列添加。这个新信息可以用作分类模型的潜在特征。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/787c3e5bf8b10dd63fabb9e0366f118b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XTAOAC5NwouKoHdMKVVbqQ.png"/></div></div></figure><p id="1a23" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我们开始吧，好吗？</p><h2 id="b5fc" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">语言检测</h2><p id="bdd8" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">首先，我想确保我处理的是同一种语言，并且使用了<em class="ni"> langdetect </em>包，这真的很简单。为了举例说明，我将在数据集的第一个新闻标题上使用它:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="4d1a" class="kt ku je og b gy ok ol l om on">txt = dtf["text"].iloc[0]</span><span id="9eb7" class="kt ku je og b gy oo ol l om on">print(txt, " --&gt; ", <strong class="og jf">langdetect</strong>.detect(txt))</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/77dcc8b3bc714ebc1c99e901105febec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2uOA7anybVqoa--nFZcs4g.png"/></div></div></figure><p id="5037" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">让我们为整个数据集添加一个包含语言信息的列:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="c76e" class="kt ku je og b gy ok ol l om on">dtf['<strong class="og jf">lang</strong>'] = dtf[<strong class="og jf">"text"</strong>].apply(lambda x: <strong class="og jf">langdetect</strong>.detect(x) if <br/>                                 x.strip() != "" else "")</span><span id="8722" class="kt ku je og b gy oo ol l om on">dtf.head()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/23a0eed7dc601d1526205fedd280a255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IA_4tmym6sx_OW0q-v9WVw.png"/></div></div></figure><p id="d193" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">dataframe现在有一个新列。使用之前的相同代码，我可以看到有多少种不同的语言:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/a57a3c861c68e63e94542cefa36f4f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EwPDIVgdXU6xKVGYttuDfw.png"/></div></div></figure><p id="2533" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">即使有不同的语言，也是以英语为主。因此，我将过滤英语新闻。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="de4c" class="kt ku je og b gy ok ol l om on">dtf = dtf[dtf["<strong class="og jf">lang</strong>"]=="<strong class="og jf">en</strong>"]</span></pre><h2 id="f83b" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">文本预处理</h2><p id="f061" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">数据预处理是准备原始数据以使其适合机器学习模型的阶段。对于自然语言处理，这包括文本清理，停用词删除，词干和词汇化。</p><p id="8add" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated"><strong class="lr jf">文本清理</strong>步骤根据数据类型和所需任务而有所不同。通常，在文本被标记化之前，字符串被转换成小写，标点符号被删除。<strong class="lr jf">记号化</strong>是将一个字符串拆分成一系列字符串(或“记号”)的过程。</p><p id="083c" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">让我们再次以第一个新闻标题为例:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="a26d" class="kt ku je og b gy ok ol l om on"><strong class="og jf">print("--- original ---")</strong><br/>print(txt)</span><span id="f3c8" class="kt ku je og b gy oo ol l om on"><strong class="og jf">print("--- cleaning ---")</strong><br/>txt = re.sub(r'[^\w\s]', '', str(txt).lower().strip())<br/>print(txt)</span><span id="6e37" class="kt ku je og b gy oo ol l om on"><strong class="og jf">print("--- tokenization ---")</strong><br/>txt = txt.split()<br/>print(txt)</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/ad049a7e8f072e93f757a4b42ceee4b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bB49dtGEqeIAXoabDm5rw.png"/></div></div></figure><p id="13fc" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我们要保留列表中的所有令牌吗？我们没有。事实上，我们希望删除所有不能提供额外信息的单词。在这个例子中，最重要的词是“<em class="ni"> song </em>”，因为它可以将任何分类模型指向正确的方向。相比之下，像“<em class="ni">和</em>”、“<em class="ni">代表</em>”、“<em class="ni">代表</em>”这样的词就没什么用了，因为它们可能出现在数据集中的几乎每个观察值中。这些是<strong class="lr jf">停用词</strong>的例子。这种表达通常指的是一种语言中最常见的单词，但没有一个通用的停用词列表。</p><p id="a63a" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我们可以用<em class="ni">NLTK(</em><a class="ae ms" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">自然语言工具包</a>)为英语词汇创建一个通用停用词列表，NLTK是一套用于符号和统计自然语言处理的库和程序。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="312e" class="kt ku je og b gy ok ol l om on">lst_stopwords = <strong class="og jf">nltk</strong>.corpus.stopwords.words("<strong class="og jf">english</strong>")<br/>lst_stopwords</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/51c3877c60751b45f23c8db89eaecb74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ewa4nh870iIUxDGpehVDIw.png"/></div></div></figure><p id="bf47" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">让我们从第一个新闻标题中去掉这些停用词:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="e615" class="kt ku je og b gy ok ol l om on"><strong class="og jf">print("--- remove stopwords ---")</strong><br/>txt = [word for word in txt if word not in lst_stopwords]<br/>print(txt)</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/d3e256d9d322271925132a3663ee5a0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAKprI1YjjjaFcUjPMwcFQ.png"/></div></div></figure><p id="c01b" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我们需要非常小心停用词，因为如果您删除了错误的令牌，您可能会丢失重要信息。例如，单词“<em class="ni"> will </em>”被删除，我们丢失了这个人是威尔·史密斯的信息。考虑到这一点，在删除停用词之前对原始文本进行一些手动修改会很有用(例如，用“<em class="ni">威尔·史密斯</em>”替换“<em class="ni">威尔·史密斯</em>”)。</p><p id="8e1e" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">现在我们有了所有有用的标记，我们可以应用单词转换了。<strong class="lr jf">词干</strong>和<strong class="lr jf">词条化</strong>都生成单词的词根形式。区别在于词干可能不是一个实际的单词，而lemma是一个实际的语言单词(而且词干通常更快)。那些算法都是由NLTK提供的。</p><p id="f70b" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">继续这个例子:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="986e" class="kt ku je og b gy ok ol l om on"><strong class="og jf">print("--- stemming ---")</strong><br/>ps = <strong class="og jf">nltk</strong>.stem.porter.<strong class="og jf">PorterStemmer</strong>()<br/>print([ps.stem(word) for word in txt])</span><span id="f9fd" class="kt ku je og b gy oo ol l om on"><strong class="og jf">print("--- lemmatisation ---")</strong><br/>lem = <strong class="og jf">nltk</strong>.stem.wordnet.<strong class="og jf">WordNetLemmatizer</strong>()<br/>print([lem.lemmatize(word) for word in txt])</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/e7199af187ae66128ba7061e2160ff52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HbpHWb8EB80dFUpPDv46QA.png"/></div></div></figure><p id="3437" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">正如你所看到的，一些单词发生了变化:“joins”变成了它的词根形式“join”，就像“cups”一样。另一方面，“官方”只是在词干上有所变化，变成了词干“offici”，它不是一个单词，是通过去掉后缀“-al”创建的。</p><p id="2de5" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我将把所有这些预处理步骤放在一个函数中，并将其应用于整个数据集。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="e3bc" class="kt ku je og b gy ok ol l om on"><strong class="og jf">'''<br/>Preprocess a string.<br/>:parameter<br/>    :param text: string - name of column containing text<br/>    :param lst_stopwords: list - list of stopwords to remove<br/>    :param flg_stemm: bool - whether stemming is to be applied<br/>    :param flg_lemm: bool - whether lemmitisation is to be applied<br/>:return<br/>    cleaned text<br/>'''</strong><br/>def <strong class="og jf">utils_preprocess_text</strong>(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):<br/>    <strong class="og jf">## clean (convert to lowercase and remove punctuations and characters and then strip)</strong><br/>    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())<br/>            <br/>    <strong class="og jf">## Tokenize (convert from string to list)</strong><br/>    lst_text = text.split()</span><span id="b4f0" class="kt ku je og b gy oo ol l om on"><strong class="og jf">    ## remove Stopwords</strong><br/>    if lst_stopwords is not None:<br/>        lst_text = [word for word in lst_text if word not in <br/>                    lst_stopwords]<br/>                <br/>    <strong class="og jf">## Stemming (remove -ing, -ly, ...)</strong><br/>    if flg_stemm == True:<br/>        ps = nltk.stem.porter.PorterStemmer()<br/>        lst_text = [ps.stem(word) for word in lst_text]<br/>                <br/>    <strong class="og jf">## Lemmatisation (convert the word into root word)</strong><br/>    if flg_lemm == True:<br/>        lem = nltk.stem.wordnet.WordNetLemmatizer()<br/>        lst_text = [lem.lemmatize(word) for word in lst_text]<br/>            <br/>    <strong class="og jf">## back to string from list</strong><br/>    text = " ".join(lst_text)<br/>    return text</span></pre><p id="726e" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">请注意，您不应该同时应用词干化和词汇化。这里我将使用后者。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="4239" class="kt ku je og b gy ok ol l om on">dtf["text_clean"] = dtf["text"].apply(lambda x: <strong class="og jf">utils_preprocess_text</strong>(x, flg_stemm=False, <strong class="og jf">flg_lemm=True</strong>, lst_stopwords))</span></pre><p id="e7a7" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">和以前一样，我创建了一个新的专栏:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="4274" class="kt ku je og b gy ok ol l om on">dtf.head()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/57514435f462e982c50d00640ba2dee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PyvmSLgprHGzSjQTY8nYQg.png"/></div></div></figure><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="b1e4" class="kt ku je og b gy ok ol l om on">print(dtf["<strong class="og jf">text</strong>"].iloc[0], " --&gt; ", dtf["<strong class="og jf">text_clean</strong>"].iloc[0])</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/f688c5b323c23c351c29a3d25775918b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Alel4dIt3YKM-Xm5MxsCRg.png"/></div></div></figure><h2 id="99a8" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">长度分析</h2><p id="8a1e" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">看一看文本的长度是很重要的，因为这是一个简单的计算，可以给出很多见解。例如，也许我们足够幸运地发现一个类别比另一个类别系统地更长，并且长度仅仅是构建模型所需的唯一特征。不幸的是，情况并非如此，因为新闻标题有相似的长度，但值得一试。</p><p id="bbe9" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">文本数据有几种长度度量。我将举一些例子:</p><ul class=""><li id="e992" class="nk nl je lr b ls mn lv mo lc nm lg nn lk no mh np nq nr ns bi translated"><strong class="lr jf">字数统计</strong>:统计文本中的记号数(用空格隔开)</li><li id="4986" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated"><strong class="lr jf">字符数</strong>:合计每个令牌的字符数</li><li id="ef1e" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated"><strong class="lr jf">句子计数</strong>:计算句子的数量(用句号分隔)</li><li id="0018" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated"><strong class="lr jf">平均字数</strong>:字数除以字数(字数/字数)</li><li id="fbd9" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated"><strong class="lr jf">平均句子长度</strong>:句子长度之和除以句子数量(字数/句子数)</li></ul><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="d321" class="kt ku je og b gy ok ol l om on">dtf['word_count'] = dtf["text"].apply(lambda x: len(str(x).split(" ")))</span><span id="7bdc" class="kt ku je og b gy oo ol l om on">dtf['char_count'] = dtf["text"].apply(lambda x: sum(len(word) for word in str(x).split(" ")))</span><span id="da6c" class="kt ku je og b gy oo ol l om on">dtf['sentence_count'] = dtf["text"].apply(lambda x: len(str(x).split(".")))</span><span id="86f9" class="kt ku je og b gy oo ol l om on">dtf['avg_word_length'] = dtf['char_count'] / dtf['word_count']</span><span id="8cdf" class="kt ku je og b gy oo ol l om on">dtf['avg_sentence_lenght'] = dtf['word_count'] / dtf['sentence_count']</span><span id="ff53" class="kt ku je og b gy oo ol l om on">dtf.head()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/ebad7776d5237ef228e749eda7c3d8ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EQkl6tyvJH9SfzfIvhTcNQ.png"/></div></div></figure><p id="e13e" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">让我们看看我们通常的例子:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/1d53aaa35a3a495fae24032c1796fdef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pCdrbvdTSuh1wJ5l8a1tnA.png"/></div></div></figure><p id="7c19" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">这些新变量相对于目标的分布是怎样的？为了回答这个问题，我将看看二元分布(两个变量如何一起移动)。首先，我将把整个观察集分成3个样本(政治、娱乐、科技)，然后比较样本的直方图和密度。如果分布不同，那么变量是可预测的，因为3组具有不同的模式。</p><p id="b4ff" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">例如，让我们看看字符数是否与目标变量相关:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="2baf" class="kt ku je og b gy ok ol l om on"><strong class="og jf">x, y = "char_count", "y"</strong></span><span id="6aa9" class="kt ku je og b gy oo ol l om on">fig, ax = plt.subplots(nrows=1, ncols=2)<br/>fig.suptitle(x, fontsize=12)<br/>for i in dtf[y].unique():<br/>    sns.distplot(dtf[dtf[y]==i][x], hist=True, kde=False, <br/>                 bins=10, hist_kws={"alpha":0.8}, <br/>                 axlabel="histogram", ax=ax[0])<br/>    sns.distplot(dtf[dtf[y]==i][x], hist=False, kde=True, <br/>                 kde_kws={"shade":True}, axlabel="density",   <br/>                 ax=ax[1])<br/>ax[0].grid(True)<br/>ax[0].legend(dtf[y].unique())<br/>ax[1].grid(True)<br/>plt.show()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/3de80e0d2aa558bf8be6cc7aeabdc00d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbgUz0GTt1rZxE9GlT6-bw.png"/></div></div></figure><p id="6aa2" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">这三个类别具有相似的长度分布。这里，密度图非常有用，因为样本大小不同。</p><h2 id="4ab6" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">情感分析</h2><p id="02cd" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">情感分析是将文本数据的主观情感通过数字或类别表示出来。计算情感是自然语言处理中最困难的任务之一，因为自然语言充满了歧义。例如，短语“<em class="ni">这是如此糟糕，以至于它是好的</em>”有不止一种解释。一个模型可以给单词“<em class="ni">好的</em>”分配一个积极的信号，给单词“<em class="ni">坏的</em>”分配一个消极的信号，产生一个中性的情绪。发生这种情况是因为背景未知。</p><p id="e208" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">最好的方法是训练你自己的情绪模型，使之与你的数据完全吻合。当没有足够的时间或数据时，可以使用预先训练的模型，如<em class="ni"> Textblob </em>和<em class="ni"> Vader </em>。<a class="ae ms" href="https://textblob.readthedocs.io/en/dev/index.html" rel="noopener ugc nofollow" target="_blank"><em class="ni">text blob</em></a><em class="ni">，</em>建立在<em class="ni"> NLTK之上，</em>是最受欢迎的一个，它可以给单词分配极性，并把整篇文本的情感估计为平均值。另一方面，<a class="ae ms" href="https://github.com/cjhutto/vaderSentiment" rel="noopener ugc nofollow" target="_blank"> <em class="ni">【维达】</em> </a> <em class="ni"> </em>(价觉词典和情感推理机)是一个基于规则的模型，在社交媒体数据上工作得特别好。</p><p id="a2f6" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我将使用<em class="ni"> Textblob </em>添加一个情感特征:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="33b4" class="kt ku je og b gy ok ol l om on">dtf["sentiment"] = dtf[column].apply(lambda x: <br/>                   <strong class="og jf">TextBlob</strong>(x).sentiment.polarity)<br/>dtf.head()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/af12f691fc26d2b848ab12e39e68fc3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UewQmushViEGsGJBz2ktSw.png"/></div></div></figure><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="82c9" class="kt ku je og b gy ok ol l om on">print(dtf["text"].iloc[0], " --&gt; ", dtf["sentiment"].iloc[0])</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ca"><img src="../Images/e65000efa0e133c118651b766d5abcbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R8xq45xo8BxP4hgE06TslA.png"/></div></div></figure><p id="e277" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">品类和情绪之间有模式吗？</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/8f1da290efebe0a632d01450386b8133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VGIyH8rKW08WS3mISJ6I5Q.png"/></div></div></figure><p id="f652" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">大多数标题都有一种中性的情绪，除了政治新闻倾向于负面，科技新闻倾向于正面。</p><h2 id="50a6" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">命名实体识别</h2><p id="5f52" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">NER ( <a class="ae ms" href="https://en.wikipedia.org/wiki/Named-entity_recognition" rel="noopener ugc nofollow" target="_blank">命名实体识别</a>)是用预定义的类别(如人名、组织、位置、时间表达式、数量等)标记非结构化文本中提到的命名实体的过程。</p><p id="c35a" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">训练NER模型非常耗时，因为它需要非常丰富的数据集。幸运的是，有人已经为我们做了这项工作。最好的开源NER工具之一是<a class="ae ms" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> <em class="ni"> SpaCy </em> </a>。它提供了不同的NLP模型，能够识别几类实体。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/732df2597768de8e1ec31cc5804f9b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*twFglsA8MgX3fwUx9BXCBQ.png"/></div></div><p class="pi pj gj gh gi pk pl bd b be z dk translated">来源:<a class="ae ms" href="https://spacy.io/api/annotation#section-named-entities" rel="noopener ugc nofollow" target="_blank">空间</a></p></figure><p id="f8b8" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我将在我们通常的标题(原始文本，未经预处理)上使用<em class="ni">SpaCy</em>model<em class="ni">en _ core _ web _ LG</em>(基于web数据训练的英语大模型)来举例说明:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="226b" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## call model</strong><br/>ner = <strong class="og jf">spacy</strong>.load("<strong class="og jf">en_core_web_lg</strong>")</span><span id="2881" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## tag text</strong><br/>txt = dtf["text"].iloc[0]<br/>doc = <strong class="og jf">ner</strong>(txt)</span><span id="ebf3" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## display result</strong><br/>spacy.<strong class="og jf">displacy</strong>.render(doc, style="ent")</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pm"><img src="../Images/0be64a72725c27810c0b64b0deff8708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*byEbQlMOTbeezYg_IY8Wew.png"/></div></div></figure><p id="a257" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">这很酷，但是我们如何把它变成一个有用的特性呢？这是我要做的:</p><ul class=""><li id="11bd" class="nk nl je lr b ls mn lv mo lc nm lg nn lk no mh np nq nr ns bi translated">对数据集中的每个文本观察值运行NER模型，就像我在前面的例子中所做的那样。</li><li id="74d7" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">对于每个新闻标题，我将把所有已识别的实体放入一个新的列(名为“tags”)中，同时列出该实体在文本中出现的次数。在本例中，应该是</li></ul><blockquote class="pn"><p id="e98e" class="po pp je bd pq pr ps pt pu pv pw mh dk translated">{('威尔·史密斯'，'人'):1，<br/>('迪普'，'人'):1，<br/>('尼基·贾姆'，'人'):1，<br/>(“2018世界杯的'，'事件'):1 }</p></blockquote><ul class=""><li id="c4e3" class="nk nl je lr b ls px lv py lc pz lg qa lk qb mh np nq nr ns bi translated">然后，我将为每个标签类别(Person、Org、Event等)创建一个新列，并计算每个类别中找到的实体的数量。在上面的例子中，这些特性是</li></ul><blockquote class="pn"><p id="0d63" class="po pp je bd pq pr ps pt pu pv pw mh dk translated">tags_PERSON = 3</p><p id="9504" class="po pp je bd pq pr qc qd qe qf qg mh dk translated">tags_EVENT = 1</p></blockquote><pre class="qh qi qj qk ql of og oh oi aw oj bi"><span id="8dc6" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## tag text and exctract tags into a list</strong><br/>dtf["tags"] = dtf["text"].apply(lambda x: [(tag.text, tag.label_) <br/>                                for tag in ner(x).ents] )</span><span id="20d8" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## utils function to count the element of a list<br/></strong>def <strong class="og jf">utils_lst_count</strong>(lst):<br/>    dic_counter = collections.Counter()<br/>    for x in lst:<br/>        dic_counter[x] += 1<br/>    dic_counter = collections.OrderedDict( <br/>                     sorted(dic_counter.items(), <br/>                     key=lambda x: x[1], reverse=True))<br/>    lst_count = [ {key:value} for key,value in dic_counter.items() ]<br/>    return lst_count</span><span id="5fae" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><br/>## count tags</strong><br/>dtf["tags"] = dtf["tags"].apply(lambda x: <strong class="og jf">utils_lst_count</strong>(x))</span><span id="2e4c" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><br/>## utils function create new column for each tag category<br/></strong>def <strong class="og jf">utils_ner_features</strong>(lst_dics_tuples, tag):<br/>    if len(lst_dics_tuples) &gt; 0:<br/>        tag_type = []<br/>        for dic_tuples in lst_dics_tuples:<br/>            for tuple in dic_tuples:<br/>                type, n = tuple[1], dic_tuples[tuple]<br/>                tag_type = tag_type + [type]*n<br/>                dic_counter = collections.Counter()<br/>                for x in tag_type:<br/>                    dic_counter[x] += 1<br/>        return dic_counter[tag]<br/>    else:<br/>        return 0</span><span id="af0d" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><br/>## extract features</strong><br/>tags_set = []<br/>for lst in dtf["tags"].tolist():<br/>     for dic in lst:<br/>          for k in dic.keys():<br/>              tags_set.append(k[1])<br/>tags_set = list(set(tags_set))<br/>for feature in tags_set:<br/>     dtf["tags_"+feature] = dtf["tags"].apply(lambda x: <br/>                             <strong class="og jf">utils_ner_features</strong>(x, feature))</span><span id="702c" class="kt ku je og b gy oo ol l om on"><br/><strong class="og jf">## print result</strong><br/>dtf.head()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qm"><img src="../Images/e8fc8e78d80454d89d2204391732e445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p-MJh95d0ysUCL45PmmC1A.png"/></div></div></figure><p id="e5dc" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">现在我们可以对标签类型分布有一个宏观的看法。让我们以ORG标签(公司和组织)为例:</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qn"><img src="../Images/57665e5abfa8b866616726f7f26e6209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SnGDJFoBukXl8L31wRke0Q.png"/></div></div></figure><p id="aeb6" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">为了更深入地分析，我们需要解包我们在前面的代码中创建的列“tags”。让我们为其中一个标题类别画出最常见的标签:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="f53a" class="kt ku je og b gy ok ol l om on">y = "<strong class="og jf">ENTERTAINMENT</strong>"<br/> <br/>tags_list = dtf[dtf["y"]==y]["tags"].sum()<br/>map_lst = list(map(lambda x: list(x.keys())[0], tags_list))<br/>dtf_tags = pd.DataFrame(map_lst, columns=['tag','type'])<br/>dtf_tags["count"] = 1<br/>dtf_tags = dtf_tags.groupby(['type',  <br/>                'tag']).count().reset_index().sort_values("count", <br/>                 ascending=False)<br/>fig, ax = plt.subplots()<br/>fig.suptitle("Top frequent tags", fontsize=12)<br/>sns.barplot(x="count", y="tag", hue="type", <br/>            data=dtf_tags.iloc[:top,:], dodge=False, ax=ax)<br/>ax.grid(axis="x")<br/>plt.show()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qo"><img src="../Images/49187001d6a18221c1d43a5a6cb4ed2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ONKEb0gFnCSfO1mvY88IyA.png"/></div></div></figure><p id="2b59" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">继续NER的另一个有用的应用:你还记得当我们从"<em class="ni"/>"的名字中去掉单词"<em class="ni"/>"的时候吗？这个问题的一个有趣的解决方案是用“<em class="ni"> Will_Smith </em>”替换“<em class="ni">威尔·史密斯</em>”，这样它就不会受到停用词删除的影响。因为遍历数据集中的所有文本来更改名称是不可能的，所以让我们使用<em class="ni"> SpaCy </em>来实现。我们知道，<em class="ni"> SpaCy </em>可以识别人名，因此我们可以用它来进行<strong class="lr jf">姓名检测</strong>然后修改字符串。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="c173" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## predict wit NER<br/></strong>txt = dtf["text"].iloc[0]<br/>entities = ner(txt).ents</span><span id="530c" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## tag text</strong><br/>tagged_txt = txt<br/>for tag in entities:<br/>    tagged_txt = re.sub(tag.text, "_".join(tag.text.split()), <br/>                        tagged_txt) </span><span id="8e0c" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## show result</strong><br/>print(tagged_txt)</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qp"><img src="../Images/510829d3d63ae4f5efb37f808cb14f01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ECe4eGbUE93Dqlvt1_0Jlw.png"/></div></div></figure><h2 id="0c27" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">字频率</h2><p id="3deb" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">到目前为止，我们已经看到了如何通过分析和处理整个文本来进行特征工程。现在我们将通过计算单词的出现频率来了解单词的重要性。一个<strong class="lr jf"> <em class="ni"> n </em> -gram </strong>是来自给定文本样本的<em class="ni"> n </em>项的连续序列。当<em class="ni"> n </em> -gram的大小为1时，称为一元gram(大小为2的是二元gram)。</p><p id="e511" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">例如，短语“<em class="ni">我喜欢这篇文章</em>”可以分解为:</p><ul class=""><li id="da28" class="nk nl je lr b ls mn lv mo lc nm lg nn lk no mh np nq nr ns bi translated">4 unigrams:“<em class="ni">I</em>”、<em class="ni"> like </em>、<em class="ni"> this </em>、<em class="ni"> article </em>”</li><li id="e44f" class="nk nl je lr b ls nt lv nu lc nv lg nw lk nx mh np nq nr ns bi translated">3个大人物:“<em class="ni">我喜欢</em>”、“<em class="ni">喜欢这个</em>”、“<em class="ni">这篇文章</em>”</li></ul><p id="abf1" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我将以政治新闻为例，展示如何计算单词和双词的频率。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="c70c" class="kt ku je og b gy ok ol l om on">y = "<strong class="og jf">POLITICS</strong>"<br/>corpus = dtf[dtf["y"]==y]["text_clean"]</span><span id="bba3" class="kt ku je og b gy oo ol l om on">lst_tokens = <strong class="og jf">nltk</strong>.tokenize.word_tokenize(corpus.str.cat(sep=" "))<br/>fig, ax = plt.subplots(nrows=1, ncols=2)<br/>fig.suptitle("Most frequent words", fontsize=15)<br/>    <br/><strong class="og jf">## unigrams</strong><br/>dic_words_freq = nltk.FreqDist(lst_tokens)<br/>dtf_uni = pd.DataFrame(dic_words_freq.most_common(), <br/>                       columns=["Word","Freq"])<br/>dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(<br/>                  kind="barh", title="Unigrams", ax=ax[0], <br/>                  legend=False).grid(axis='x')<br/>ax[0].set(ylabel=None)<br/>    <br/><strong class="og jf">## bigrams</strong><br/>dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))<br/>dtf_bi = pd.DataFrame(dic_words_freq.most_common(), <br/>                      columns=["Word","Freq"])<br/>dtf_bi["Word"] = dtf_bi["Word"].apply(lambda x: " ".join(<br/>                   string for string in x) )<br/>dtf_bi.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(<br/>                  kind="barh", title="Bigrams", ax=ax[1],<br/>                  legend=False).grid(axis='x')<br/>ax[1].set(ylabel=None)<br/>plt.show()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qq"><img src="../Images/45880135613f72997ee87a2010d83d42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o25L1NVxV7IRls9kQtEFZA.png"/></div></div></figure><p id="3d49" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">如果有只出现在一个类别中的<em class="ni"> n </em> -grams(例如政治新闻中的“共和党”)，这些可以成为新的特征。一种更费力的方法是将整个语料库矢量化，并将所有单词用作特征(单词袋方法)。</p><p id="fb66" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">现在，我将向您展示如何在您的数据框架中添加词频作为一项功能。我们只需要来自<em class="ni"> Scikit-learn的<em class="ni">计数矢量器</em>，</em>Python中最流行的机器学习库之一。矢量器将文本文档的集合转换成令牌计数的矩阵。我用3个n-grams举个例子:“<em class="ni">票房</em>(娱乐圈频繁)、<em class="ni">共和党</em>(政界频繁)、“苹果”(科技界频繁)。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="6fb8" class="kt ku je og b gy ok ol l om on">lst_words = ["<strong class="og jf">box office</strong>", "<strong class="og jf">republican</strong>", "<strong class="og jf">apple</strong>"]</span><span id="43c3" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><em class="ni">## count<br/></em></strong>lst_grams = [len(word.split(" ")) for word in lst_words]<br/>vectorizer = feature_extraction.text.<strong class="og jf">CountVectorizer</strong>(<br/>                 vocabulary=lst_words, <br/>                 ngram_range=(min(lst_grams),max(lst_grams)))</span><span id="e8be" class="kt ku je og b gy oo ol l om on">dtf_X = pd.DataFrame(vectorizer.fit_transform(dtf["text_clean"]).todense(), columns=lst_words)</span><span id="0aff" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><em class="ni">## add the new features as columns<br/></em></strong>dtf = pd.concat([dtf, dtf_X.set_index(dtf.index)], axis=1)<br/>dtf.head()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qr"><img src="../Images/6f448b6b16176a22337dce647a5755a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ylAKPMTyufePkgAOlRJOfg.png"/></div></div></figure><p id="88e7" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">可视化相同信息的一个好方法是使用<strong class="lr jf">单词云</strong>，其中每个标签的频率用字体大小和颜色显示。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="bea2" class="kt ku je og b gy ok ol l om on">wc = <strong class="og jf">wordcloud</strong>.WordCloud(background_color='black', max_words=100, <br/>                         max_font_size=35)<br/>wc = wc.generate(str(corpus))<br/>fig = plt.figure(num=1)<br/>plt.axis('off')<br/>plt.imshow(wc, cmap=None)<br/>plt.show()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mi"><img src="../Images/7c5fba1b8cdc020431fb02b50c98e346.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pZVocDqN8uRdnGyrrWbnlQ.png"/></div></div></figure><h2 id="ee5a" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">词向量</h2><p id="8bbf" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">最近，NLP领域开发了新的语言模型，该模型依赖于神经网络架构，而不是更传统的n-gram模型。这些新技术是一套语言建模和特征学习技术，其中单词被转换成实数向量，因此它们被称为<strong class="lr jf">单词嵌入</strong>。</p><p id="c8c6" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">单词嵌入模型通过构建在所选单词之前和之后会出现什么标记的概率分布，将特定单词映射到向量。这些模型很快变得流行起来，因为一旦有了实数而不是字符串，就可以进行计算了。例如，为了找到相同上下文的单词，可以简单地计算向量距离。</p><p id="449f" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">有几个Python库可以处理这种模型。<em class="ni"> SpaCy </em>是一个，不过既然我们已经用过了，我就说说另一个著名的包:<a class="ae ms" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank"> <em class="ni"> Gensim </em> </a> <em class="ni">。</em>一个使用现代统计机器学习的无监督主题建模和自然语言处理的开源库。使用<em class="ni"> Gensim </em>，我将加载一个预先训练好的<em class="ni">手套</em>模型。<a class="ae ms" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <em class="ni"> GloVe </em>(全局向量)</a>是一种无监督学习算法，用于获得大小为300的单词的向量表示。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="4681" class="kt ku je og b gy ok ol l om on">nlp = <strong class="og jf">gensim_api</strong>.load("<strong class="og jf">glove-wiki-gigaword-300</strong>")</span></pre><p id="f862" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我们可以使用这个对象将单词映射到向量:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="0a66" class="kt ku je og b gy ok ol l om on"><strong class="og jf">word = "love"</strong></span><span id="3978" class="kt ku je og b gy oo ol l om on">nlp[word]</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qs"><img src="../Images/bbb4e8ad98f75efdba65ab2087dc1ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jEvFIgRb2FxhBebg72Uf2A.png"/></div></div></figure><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="3269" class="kt ku je og b gy ok ol l om on">nlp[word].shape</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi qt"><img src="../Images/88a1e2b6b66cd3e705ebea81ab982ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*Ovq5KPzNrNY7CsVot3yAyQ.png"/></div></figure><p id="287b" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">现在让我们看看什么是最接近的词向量，或者换句话说，最常出现在相似上下文中的词。为了在二维空间中绘制向量，我需要将维度从300减少到2。我将用来自<em class="ni"> Scikit-learn的<em class="ni">t-分布式随机邻居嵌入</em>来做这件事。</em> t-SNE是一种可视化高维数据的工具，它将数据点之间的相似性转换为联合概率。</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="d31f" class="kt ku je og b gy ok ol l om on"><strong class="og jf">## find closest vectors</strong><br/>labels, X, x, y = [], [], [], []<br/>for t in nlp.<strong class="og jf">most_similar</strong>(word, topn=20):<br/>    X.append(nlp[t[0]])<br/>    labels.append(t[0])</span><span id="c47e" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## reduce dimensions</strong><br/>pca = manifold.<strong class="og jf">TSNE</strong>(perplexity=40, n_components=2, init='pca')<br/>new_values = pca.fit_transform(X)<br/>for value in new_values:<br/>    x.append(value[0])<br/>    y.append(value[1])</span><span id="5e3c" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## plot</strong><br/>fig = plt.figure()<br/>for i in range(len(x)):<br/>    plt.scatter(x[i], y[i], c="black")<br/>    plt.annotate(labels[i], xy=(x[i],y[i]), xytext=(5,2), <br/>               textcoords='offset points', ha='right', va='bottom')</span><span id="aca3" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## add center</strong><br/>plt.scatter(x=0, y=0, c="red")<br/>plt.annotate(word, xy=(0,0), xytext=(5,2), textcoords='offset <br/>             points', ha='right', va='bottom')</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qu"><img src="../Images/16c2a564ab78876e5f8515276f9f1d32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jfUqgQAwXE2VF6BhcYz7kQ.png"/></div></div></figure><h2 id="ce6a" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">主题建模</h2><p id="d298" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">Genism 包专门用于主题建模。主题模型是一种用于发现出现在文档集合中的抽象“主题”的统计模型。</p><p id="5657" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我将展示如何使用<em class="ni"> LDA </em>(潜在的狄利克雷分配)提取主题:一种生成统计模型，它允许通过未观察到的组来解释观察集，从而解释为什么数据的某些部分是相似的。基本上，文档被表示为潜在主题的随机混合，其中每个主题由单词的分布来表征。</p><p id="5f53" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">让我们看看我们能从科技新闻中提取出什么话题。我需要指定模型必须聚类的主题数量，我将尝试使用3:</p><pre class="mj mk ml mm gt of og oh oi aw oj bi"><span id="a671" class="kt ku je og b gy ok ol l om on">y = "<strong class="og jf">TECH</strong>"<br/>corpus = dtf[dtf["y"]==y]["text_clean"]</span><span id="311d" class="kt ku je og b gy oo ol l om on"><strong class="og jf"><br/>## pre-process corpus</strong><br/>lst_corpus = []<br/>for string in corpus:<br/>    lst_words = string.split()<br/>    lst_grams = [" ".join(lst_words[i:i + 2]) for i in range(0, <br/>                     len(lst_words), 2)]<br/>    lst_corpus.append(lst_grams)</span><span id="de0c" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## map words to an id</strong><br/>id2word = gensim.corpora.Dictionary(lst_corpus)</span><span id="74cb" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## create dictionary word:freq</strong><br/>dic_corpus = [id2word.doc2bow(word) for word in lst_corpus] </span><span id="891c" class="kt ku je og b gy oo ol l om on"><strong class="og jf">## train LDA</strong><br/>lda_model = gensim.models.ldamodel.<strong class="og jf">LdaModel</strong>(corpus=dic_corpus, id2word=id2word, <strong class="og jf">num_topics=3</strong>, random_state=123, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)<br/>   <br/><strong class="og jf">## output</strong><br/>lst_dics = []<br/>for i in range(0,<strong class="og jf">3</strong>):<br/>    lst_tuples = lda_model.get_topic_terms(i)<br/>    for tupla in lst_tuples:<br/>        lst_dics.append({"topic":i, "id":tupla[0], <br/>                         "word":id2word[tupla[0]], <br/>                         "weight":tupla[1]})<br/>dtf_topics = pd.DataFrame(lst_dics, <br/>                         columns=['topic','id','word','weight'])<br/>    <br/><strong class="og jf">## plot</strong><br/>fig, ax = plt.subplots()<br/>sns.barplot(y="word", x="weight", hue="topic", data=dtf_topics, dodge=False, ax=ax).set_title('Main Topics')<br/>ax.set(ylabel="", xlabel="Word Importance")<br/>plt.show()</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qv"><img src="../Images/5290f18efc0e40c3d0337e087c1d726f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a1VMbD9mKTsVJ34ByRIrJw.png"/></div></div></figure><p id="b76d" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">试图只在3个主题中捕捉6年的内容可能有点困难，但正如我们所见，关于苹果公司的一切都以同一个主题结束。</p><h2 id="f8ef" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">结论</h2><p id="700d" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">这篇文章是一个教程，演示了<strong class="lr jf">如何用NLP分析文本数据，并为机器学习模型</strong>提取特征。</p><p id="823f" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我展示了如何检测数据使用的语言，以及如何预处理和清理文本。然后我解释了不同的长度度量，用<em class="ni"> Textblob </em>做了情感分析，我们使用<em class="ni"> SpaCy </em>进行命名实体识别。最后，我解释了使用<em class="ni"> Scikit-learn </em>的传统词频方法和使用<em class="ni"> Gensim </em>的现代语言模型之间的区别。现在，您已经基本了解了NLP的所有基础知识，可以开始处理文本数据了。</p><p id="68f1" class="pw-post-body-paragraph lp lq je lr b ls mn kf lu lv mo ki lx lc mp lz ma lg mq mc md lk mr mf mg mh im bi translated">我希望你喜欢它！如有问题和反馈，或者只是分享您感兴趣的项目，请随时联系我。</p><blockquote class="pn"><p id="3199" class="po pp je bd pq pr qc qd qe qf qg mh dk translated">👉<a class="ae ms" href="https://linktr.ee/maurodp" rel="noopener ugc nofollow" target="_blank">我们来连线</a>👈</p></blockquote></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><blockquote class="qw qx qy"><p id="20ac" class="lp lq ni lr b ls mn kf lu lv mo ki lx qz mp lz ma ra mq mc md rb mr mf mg mh im bi translated">本文是系列文章<strong class="lr jf"> NLP与Python </strong>的一部分，参见:</p></blockquote><div class="is it gp gr iu mt"><a rel="noopener follow" target="_blank" href="/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd jf gy z fp my fr fs mz fu fw jd bi translated">使用NLP的文本摘要:TextRank vs Seq2Seq vs BART</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">使用Python、Gensim、Tensorflow、Transformers进行自然语言处理</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="rc l ne nf ng nc nh ja mt"/></div></div></a></div><div class="is it gp gr iu mt"><a rel="noopener follow" target="_blank" href="/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd jf gy z fp my fr fs mz fu fw jd bi translated">基于自然语言处理的文本分类:Tf-Idf vs Word2Vec vs BERT</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">预处理、模型设计、评估、词袋的可解释性、词嵌入、语言模型</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="rd l ne nf ng nc nh ja mt"/></div></div></a></div><div class="is it gp gr iu mt"><a rel="noopener follow" target="_blank" href="/text-classification-with-no-model-training-935fe0e42180"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd jf gy z fp my fr fs mz fu fw jd bi translated">用于无模型训练的文本分类的BERT</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">如果没有带标签的训练集，请使用BERT、单词嵌入和向量相似度</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="re l ne nf ng nc nh ja mt"/></div></div></a></div><div class="is it gp gr iu mt"><a rel="noopener follow" target="_blank" href="/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd jf gy z fp my fr fs mz fu fw jd bi translated">带NLP的AI聊天机器人:语音识别+变形金刚</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">用Python构建一个会说话的聊天机器人，与你的人工智能进行对话</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="rf l ne nf ng nc nh ja mt"/></div></div></a></div></div></div>    
</body>
</html>