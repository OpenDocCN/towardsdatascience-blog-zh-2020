# ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡æœ¬åˆ†æå’Œç‰¹å¾å·¥ç¨‹

> åŸæ–‡ï¼š<https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d?source=collection_archive---------3----------------------->

![](img/bf7a59d41b19f964ce71b34753c515bf.png)

## è¯­è¨€æ£€æµ‹ã€æ–‡æœ¬æ¸…ç†ã€é•¿åº¦æµ‹é‡ã€æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€N å…ƒè¯­æ³•é¢‘ç‡ã€è¯å‘é‡ã€ä¸»é¢˜å»ºæ¨¡

## æ‘˜è¦

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ NLP å’Œ Python æ¥è§£é‡Šå¦‚ä½•ä¸ºæ‚¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹åˆ†ææ–‡æœ¬æ•°æ®å’Œæå–ç‰¹å¾ã€‚

![](img/7c5fba1b8cdc020431fb02b50c98e346.png)

[**ã€NLP(è‡ªç„¶è¯­è¨€å¤„ç†)**](https://en.wikipedia.org/wiki/Natural_language_processing) æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œç ”ç©¶è®¡ç®—æœºå’Œäººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•ç»™è®¡ç®—æœºç¼–ç¨‹ï¼Œä»¥å¤„ç†å’Œåˆ†æå¤§é‡çš„è‡ªç„¶è¯­è¨€æ•°æ®ã€‚NLP é€šå¸¸ç”¨äºæ–‡æœ¬æ•°æ®çš„åˆ†ç±»ã€‚**æ–‡æœ¬åˆ†ç±»**å°±æ˜¯æ ¹æ®æ–‡æœ¬æ•°æ®çš„å†…å®¹ç»™æ–‡æœ¬æ•°æ®åˆ†é…ç±»åˆ«çš„é—®é¢˜ã€‚æ–‡æœ¬åˆ†ç±»æœ€é‡è¦çš„éƒ¨åˆ†æ˜¯**ç‰¹å¾å·¥ç¨‹**:ä»åŸå§‹æ–‡æœ¬æ•°æ®ä¸­ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹åˆ›å»ºç‰¹å¾çš„è¿‡ç¨‹ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†è§£é‡Šä¸åŒçš„æ–¹æ³•æ¥åˆ†ææ–‡æœ¬å¹¶æå–å¯ç”¨äºæ„å»ºåˆ†ç±»æ¨¡å‹çš„ç‰¹å¾ã€‚æˆ‘å°†å±•ç¤ºä¸€äº›æœ‰ç”¨çš„ Python ä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºå…¶ä»–ç±»ä¼¼çš„æƒ…å†µ(åªéœ€å¤åˆ¶ã€ç²˜è´´ã€è¿è¡Œ)ï¼Œå¹¶é€šè¿‡æ³¨é‡Šéå†æ¯ä¸€è¡Œä»£ç ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥å¤åˆ¶è¿™ä¸ªç¤ºä¾‹(ä¸‹é¢æ˜¯å®Œæ•´ä»£ç çš„é“¾æ¥)ã€‚

[](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb) [## mdipietro 09/data science _ äººå·¥æ™ºèƒ½ _ å®ç”¨å·¥å…·

### permalink dissolve GitHub æ˜¯è¶…è¿‡ 5000 ä¸‡å¼€å‘äººå‘˜çš„å®¶å›­ï¼Œä»–ä»¬ä¸€èµ·å·¥ä½œæ¥æ‰˜ç®¡å’Œå®¡æŸ¥ä»£ç ï¼Œç®¡ç†â€¦

github.com](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb) 

æˆ‘å°†ä½¿ç”¨â€œ**æ–°é—»ç±»åˆ«æ•°æ®é›†**â€(ä¸‹é¢çš„é“¾æ¥)ï¼Œå…¶ä¸­ä¸ºæ‚¨æä¾›äº†ä»*èµ«èŠ¬é¡¿é‚®æŠ¥*è·å¾—çš„ 2012 å¹´è‡³ 2018 å¹´çš„æ–°é—»æ ‡é¢˜ï¼Œå¹¶è¦æ±‚æ‚¨å°†å®ƒä»¬å½’ç±»åˆ°æ­£ç¡®çš„ç±»åˆ«ä¸­ã€‚

[](https://www.kaggle.com/rmisra/news-category-dataset) [## æ–°é—»ç±»åˆ«æ•°æ®é›†

### æ ¹æ®æ ‡é¢˜å’Œç®€çŸ­æè¿°è¯†åˆ«æ–°é—»çš„ç±»å‹

www.kaggle.com](https://www.kaggle.com/rmisra/news-category-dataset) 

ç‰¹åˆ«æ˜¯ï¼Œæˆ‘å°†ç»å†:

*   ç¯å¢ƒè®¾ç½®:å¯¼å…¥åŒ…å¹¶è¯»å–æ•°æ®ã€‚
*   è¯­è¨€æ£€æµ‹:äº†è§£å“ªäº›è‡ªç„¶è¯­è¨€æ•°æ®åœ¨ã€‚
*   æ–‡æœ¬é¢„å¤„ç†:æ–‡æœ¬æ¸…æ´—å’Œè½¬æ¢ã€‚
*   é•¿åº¦åˆ†æ:ç”¨ä¸åŒçš„åº¦é‡æ ‡å‡†æµ‹é‡ã€‚
*   æƒ…æ„Ÿåˆ†æ:ç¡®å®šæ–‡æœ¬æ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ã€‚
*   å‘½åå®ä½“è¯†åˆ«:ç”¨é¢„å®šä¹‰çš„ç±»åˆ«(å¦‚äººåã€ç»„ç»‡ã€ä½ç½®)æ ‡è®°æ–‡æœ¬ã€‚
*   è¯é¢‘:æ‰¾åˆ°æœ€é‡è¦çš„*n*-å…‹ã€‚
*   å•è¯å‘é‡:å°†å•è¯è½¬æ¢æˆæ•°å­—ã€‚
*   ä¸»é¢˜å»ºæ¨¡:ä»è¯­æ–™åº“ä¸­æå–ä¸»è¦ä¸»é¢˜ã€‚

## è®¾ç½®

é¦–å…ˆï¼Œæˆ‘éœ€è¦å¯¼å…¥ä»¥ä¸‹åº“ã€‚

```
**## for data**
import **pandas** as pd
import **collections** import **json****## for plotting**
import **matplotlib**.pyplot as plt
import **seaborn** as sns
import **wordcloud****## for text processing** import **re**
import **nltk****## for language detection**
import **langdetect** **## for sentiment**
from **textblob** import TextBlob**## for ner**
import **spacy****## for vectorizer** from **sklearn** import feature_extraction, manifold**## for word embedding** import **gensim**.downloader as gensim_api**## for topic modeling**
import **gensim**
```

æ•°æ®é›†åŒ…å«åœ¨ä¸€ä¸ª json æ–‡ä»¶ä¸­ï¼Œæ‰€ä»¥æˆ‘é¦–å…ˆç”¨ *json* åŒ…å°†å®ƒè¯»å…¥ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œç„¶åå°†å…¶è½¬æ¢æˆä¸€ä¸ª *pandas* Dataframeã€‚

```
lst_dics = []
with **open**('data.json', mode='r', errors='ignore') as json_file:
    for dic in json_file:
        lst_dics.append( json.loads(dic) )**## print the first one**      
lst_dics[0]
```

![](img/00918c4395287e1c75fd5a05284d2172.png)

åŸå§‹æ•°æ®é›†åŒ…å«è¶…è¿‡ 30 ä¸ªç±»åˆ«ï¼Œä½†æ˜¯å‡ºäºæœ¬æ•™ç¨‹çš„ç›®çš„ï¼Œæˆ‘å°†ä½¿ç”¨ 3 ä¸ªç±»åˆ«çš„å­é›†:å¨±ä¹ã€æ”¿æ²»å’ŒæŠ€æœ¯ã€‚

```
**## create dtf**
dtf = pd.DataFrame(lst_dics)**## filter categories**
dtf = dtf[ dtf["category"].isin(['**ENTERTAINMENT**','**POLITICS**','**TECH**']) ][["category","headline"]]**## rename columns**
dtf = dtf.rename(columns={"category":"**y**", "headline":"**text**"})**## print 5 random rows**
dtf.sample(5)
```

![](img/c2f7e2aa08346334432fa144103f1498.png)

ä¸ºäº†ç†è§£æ•°æ®é›†çš„ç»„æˆï¼Œæˆ‘å°†é€šè¿‡ç”¨æ¡å½¢å›¾æ˜¾ç¤ºæ ‡ç­¾é¢‘ç‡æ¥ç ”ç©¶å•å˜é‡åˆ†å¸ƒ(åªæœ‰ä¸€ä¸ªå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒ)ã€‚

```
**x = "y"**fig, ax = plt.subplots()
fig.suptitle(x, fontsize=12)
dtf[x].reset_index().groupby(x).count().sort_values(by= 
       "index").plot(kind="barh", legend=False, 
        ax=ax).grid(axis='x')
plt.show()
```

![](img/7b355d77b762ada760bacb93f7775b46.png)

æ•°æ®é›†æ˜¯ä¸å¹³è¡¡çš„:ä¸å…¶ä»–çš„ç›¸æ¯”ï¼Œç§‘æŠ€æ–°é—»çš„æ¯”ä¾‹çœŸçš„å¾ˆå°ã€‚è¿™å¯èƒ½æ˜¯å»ºæ¨¡è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªé—®é¢˜ï¼Œæ•°æ®é›†çš„é‡æ–°é‡‡æ ·å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚

ç°åœ¨ä¸€åˆ‡éƒ½è®¾ç½®å¥½äº†ï¼Œæˆ‘å°†ä»æ¸…ç†æ•°æ®å¼€å§‹ï¼Œç„¶åæˆ‘å°†ä»åŸå§‹æ–‡æœ¬ä¸­æå–ä¸åŒçš„è§è§£ï¼Œå¹¶å°†å®ƒä»¬ä½œä¸º dataframe çš„æ–°åˆ—æ·»åŠ ã€‚è¿™ä¸ªæ–°ä¿¡æ¯å¯ä»¥ç”¨ä½œåˆ†ç±»æ¨¡å‹çš„æ½œåœ¨ç‰¹å¾ã€‚

![](img/787c3e5bf8b10dd63fabb9e0366f118b.png)

æˆ‘ä»¬å¼€å§‹å§ï¼Œå¥½å—ï¼Ÿ

## è¯­è¨€æ£€æµ‹

é¦–å…ˆï¼Œæˆ‘æƒ³ç¡®ä¿æˆ‘å¤„ç†çš„æ˜¯åŒä¸€ç§è¯­è¨€ï¼Œå¹¶ä¸”ä½¿ç”¨äº† *langdetect* åŒ…ï¼Œè¿™çœŸçš„å¾ˆç®€å•ã€‚ä¸ºäº†ä¸¾ä¾‹è¯´æ˜ï¼Œæˆ‘å°†åœ¨æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªæ–°é—»æ ‡é¢˜ä¸Šä½¿ç”¨å®ƒ:

```
txt = dtf["text"].iloc[0]print(txt, " --> ", **langdetect**.detect(txt))
```

![](img/77dcc8b3bc714ebc1c99e901105febec.png)

è®©æˆ‘ä»¬ä¸ºæ•´ä¸ªæ•°æ®é›†æ·»åŠ ä¸€ä¸ªåŒ…å«è¯­è¨€ä¿¡æ¯çš„åˆ—:

```
dtf['**lang**'] = dtf[**"text"**].apply(lambda x: **langdetect**.detect(x) if 
                                 x.strip() != "" else "")dtf.head()
```

![](img/23a0eed7dc601d1526205fedd280a255.png)

dataframe ç°åœ¨æœ‰ä¸€ä¸ªæ–°åˆ—ã€‚ä½¿ç”¨ä¹‹å‰çš„ç›¸åŒä»£ç ï¼Œæˆ‘å¯ä»¥çœ‹åˆ°æœ‰å¤šå°‘ç§ä¸åŒçš„è¯­è¨€:

![](img/a57a3c861c68e63e94542cefa36f4f1a.png)

å³ä½¿æœ‰ä¸åŒçš„è¯­è¨€ï¼Œä¹Ÿæ˜¯ä»¥è‹±è¯­ä¸ºä¸»ã€‚å› æ­¤ï¼Œæˆ‘å°†è¿‡æ»¤è‹±è¯­æ–°é—»ã€‚

```
dtf = dtf[dtf["**lang**"]=="**en**"]
```

## æ–‡æœ¬é¢„å¤„ç†

æ•°æ®é¢„å¤„ç†æ˜¯å‡†å¤‡åŸå§‹æ•°æ®ä»¥ä½¿å…¶é€‚åˆæœºå™¨å­¦ä¹ æ¨¡å‹çš„é˜¶æ®µã€‚å¯¹äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œè¿™åŒ…æ‹¬æ–‡æœ¬æ¸…ç†ï¼Œåœç”¨è¯åˆ é™¤ï¼Œè¯å¹²å’Œè¯æ±‡åŒ–ã€‚

**æ–‡æœ¬æ¸…ç†**æ­¥éª¤æ ¹æ®æ•°æ®ç±»å‹å’Œæ‰€éœ€ä»»åŠ¡è€Œæœ‰æ‰€ä¸åŒã€‚é€šå¸¸ï¼Œåœ¨æ–‡æœ¬è¢«æ ‡è®°åŒ–ä¹‹å‰ï¼Œå­—ç¬¦ä¸²è¢«è½¬æ¢æˆå°å†™ï¼Œæ ‡ç‚¹ç¬¦å·è¢«åˆ é™¤ã€‚**è®°å·åŒ–**æ˜¯å°†ä¸€ä¸ªå­—ç¬¦ä¸²æ‹†åˆ†æˆä¸€ç³»åˆ—å­—ç¬¦ä¸²(æˆ–â€œè®°å·â€)çš„è¿‡ç¨‹ã€‚

è®©æˆ‘ä»¬å†æ¬¡ä»¥ç¬¬ä¸€ä¸ªæ–°é—»æ ‡é¢˜ä¸ºä¾‹:

```
**print("--- original ---")**
print(txt)**print("--- cleaning ---")**
txt = re.sub(r'[^\w\s]', '', str(txt).lower().strip())
print(txt)**print("--- tokenization ---")**
txt = txt.split()
print(txt)
```

![](img/ad049a7e8f072e93f757a4b42ceee4b4.png)

æˆ‘ä»¬è¦ä¿ç•™åˆ—è¡¨ä¸­çš„æ‰€æœ‰ä»¤ç‰Œå—ï¼Ÿæˆ‘ä»¬æ²¡æœ‰ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›åˆ é™¤æ‰€æœ‰ä¸èƒ½æä¾›é¢å¤–ä¿¡æ¯çš„å•è¯ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæœ€é‡è¦çš„è¯æ˜¯â€œ *song* â€ï¼Œå› ä¸ºå®ƒå¯ä»¥å°†ä»»ä½•åˆ†ç±»æ¨¡å‹æŒ‡å‘æ­£ç¡®çš„æ–¹å‘ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåƒâ€œ*å’Œ*â€ã€â€œ*ä»£è¡¨*â€ã€â€œ*ä»£è¡¨*â€è¿™æ ·çš„è¯å°±æ²¡ä»€ä¹ˆç”¨äº†ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½å‡ºç°åœ¨æ•°æ®é›†ä¸­çš„å‡ ä¹æ¯ä¸ªè§‚å¯Ÿå€¼ä¸­ã€‚è¿™äº›æ˜¯**åœç”¨è¯**çš„ä¾‹å­ã€‚è¿™ç§è¡¨è¾¾é€šå¸¸æŒ‡çš„æ˜¯ä¸€ç§è¯­è¨€ä¸­æœ€å¸¸è§çš„å•è¯ï¼Œä½†æ²¡æœ‰ä¸€ä¸ªé€šç”¨çš„åœç”¨è¯åˆ—è¡¨ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨*NLTK(*[è‡ªç„¶è¯­è¨€å·¥å…·åŒ…](https://www.nltk.org/))ä¸ºè‹±è¯­è¯æ±‡åˆ›å»ºä¸€ä¸ªé€šç”¨åœç”¨è¯åˆ—è¡¨ï¼ŒNLTK æ˜¯ä¸€å¥—ç”¨äºç¬¦å·å’Œç»Ÿè®¡è‡ªç„¶è¯­è¨€å¤„ç†çš„åº“å’Œç¨‹åºã€‚

```
lst_stopwords = **nltk**.corpus.stopwords.words("**english**")
lst_stopwords
```

![](img/51c3877c60751b45f23c8db89eaecb74.png)

è®©æˆ‘ä»¬ä»ç¬¬ä¸€ä¸ªæ–°é—»æ ‡é¢˜ä¸­å»æ‰è¿™äº›åœç”¨è¯:

```
**print("--- remove stopwords ---")**
txt = [word for word in txt if word not in lst_stopwords]
print(txt)
```

![](img/d3e256d9d322271925132a3663ee5a0f.png)

æˆ‘ä»¬éœ€è¦éå¸¸å°å¿ƒåœç”¨è¯ï¼Œå› ä¸ºå¦‚æœæ‚¨åˆ é™¤äº†é”™è¯¯çš„ä»¤ç‰Œï¼Œæ‚¨å¯èƒ½ä¼šä¸¢å¤±é‡è¦ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå•è¯â€œ *will* â€è¢«åˆ é™¤ï¼Œæˆ‘ä»¬ä¸¢å¤±äº†è¿™ä¸ªäººæ˜¯å¨å°”Â·å²å¯†æ–¯çš„ä¿¡æ¯ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œåœ¨åˆ é™¤åœç”¨è¯ä¹‹å‰å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œä¸€äº›æ‰‹åŠ¨ä¿®æ”¹ä¼šå¾ˆæœ‰ç”¨(ä¾‹å¦‚ï¼Œç”¨â€œ*å¨å°”Â·å²å¯†æ–¯*â€æ›¿æ¢â€œ*å¨å°”Â·å²å¯†æ–¯*â€)ã€‚

ç°åœ¨æˆ‘ä»¬æœ‰äº†æ‰€æœ‰æœ‰ç”¨çš„æ ‡è®°ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨å•è¯è½¬æ¢äº†ã€‚**è¯å¹²**å’Œ**è¯æ¡åŒ–**éƒ½ç”Ÿæˆå•è¯çš„è¯æ ¹å½¢å¼ã€‚åŒºåˆ«åœ¨äºè¯å¹²å¯èƒ½ä¸æ˜¯ä¸€ä¸ªå®é™…çš„å•è¯ï¼Œè€Œ lemma æ˜¯ä¸€ä¸ªå®é™…çš„è¯­è¨€å•è¯(è€Œä¸”è¯å¹²é€šå¸¸æ›´å¿«)ã€‚é‚£äº›ç®—æ³•éƒ½æ˜¯ç”± NLTK æä¾›çš„ã€‚

ç»§ç»­è¿™ä¸ªä¾‹å­:

```
**print("--- stemming ---")**
ps = **nltk**.stem.porter.**PorterStemmer**()
print([ps.stem(word) for word in txt])**print("--- lemmatisation ---")**
lem = **nltk**.stem.wordnet.**WordNetLemmatizer**()
print([lem.lemmatize(word) for word in txt])
```

![](img/e7199af187ae66128ba7061e2160ff52.png)

æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œä¸€äº›å•è¯å‘ç”Ÿäº†å˜åŒ–:â€œjoinsâ€å˜æˆäº†å®ƒçš„è¯æ ¹å½¢å¼â€œjoinâ€ï¼Œå°±åƒâ€œcupsâ€ä¸€æ ·ã€‚å¦ä¸€æ–¹é¢ï¼Œâ€œå®˜æ–¹â€åªæ˜¯åœ¨è¯å¹²ä¸Šæœ‰æ‰€å˜åŒ–ï¼Œå˜æˆäº†è¯å¹²â€œofficiâ€ï¼Œå®ƒä¸æ˜¯ä¸€ä¸ªå•è¯ï¼Œæ˜¯é€šè¿‡å»æ‰åç¼€â€œ-alâ€åˆ›å»ºçš„ã€‚

æˆ‘å°†æŠŠæ‰€æœ‰è¿™äº›é¢„å¤„ç†æ­¥éª¤æ”¾åœ¨ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ã€‚

```
**'''
Preprocess a string.
:parameter
    :param text: string - name of column containing text
    :param lst_stopwords: list - list of stopwords to remove
    :param flg_stemm: bool - whether stemming is to be applied
    :param flg_lemm: bool - whether lemmitisation is to be applied
:return
    cleaned text
'''**
def **utils_preprocess_text**(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):
    **## clean (convert to lowercase and remove punctuations and characters and then strip)**
    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())

    **## Tokenize (convert from string to list)**
    lst_text = text.split() **## remove Stopwords**
    if lst_stopwords is not None:
        lst_text = [word for word in lst_text if word not in 
                    lst_stopwords]

    **## Stemming (remove -ing, -ly, ...)**
    if flg_stemm == True:
        ps = nltk.stem.porter.PorterStemmer()
        lst_text = [ps.stem(word) for word in lst_text]

    **## Lemmatisation (convert the word into root word)**
    if flg_lemm == True:
        lem = nltk.stem.wordnet.WordNetLemmatizer()
        lst_text = [lem.lemmatize(word) for word in lst_text]

    **## back to string from list**
    text = " ".join(lst_text)
    return text
```

è¯·æ³¨æ„ï¼Œæ‚¨ä¸åº”è¯¥åŒæ—¶åº”ç”¨è¯å¹²åŒ–å’Œè¯æ±‡åŒ–ã€‚è¿™é‡Œæˆ‘å°†ä½¿ç”¨åè€…ã€‚

```
dtf["text_clean"] = dtf["text"].apply(lambda x: **utils_preprocess_text**(x, flg_stemm=False, **flg_lemm=True**, lst_stopwords))
```

å’Œä»¥å‰ä¸€æ ·ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ä¸“æ :

```
dtf.head()
```

![](img/57514435f462e982c50d00640ba2dee8.png)

```
print(dtf["**text**"].iloc[0], " --> ", dtf["**text_clean**"].iloc[0])
```

![](img/f688c5b323c23c351c29a3d25775918b.png)

## é•¿åº¦åˆ†æ

çœ‹ä¸€çœ‹æ–‡æœ¬çš„é•¿åº¦æ˜¯å¾ˆé‡è¦çš„ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªç®€å•çš„è®¡ç®—ï¼Œå¯ä»¥ç»™å‡ºå¾ˆå¤šè§è§£ã€‚ä¾‹å¦‚ï¼Œä¹Ÿè®¸æˆ‘ä»¬è¶³å¤Ÿå¹¸è¿åœ°å‘ç°ä¸€ä¸ªç±»åˆ«æ¯”å¦ä¸€ä¸ªç±»åˆ«ç³»ç»Ÿåœ°æ›´é•¿ï¼Œå¹¶ä¸”é•¿åº¦ä»…ä»…æ˜¯æ„å»ºæ¨¡å‹æ‰€éœ€çš„å”¯ä¸€ç‰¹å¾ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ï¼Œå› ä¸ºæ–°é—»æ ‡é¢˜æœ‰ç›¸ä¼¼çš„é•¿åº¦ï¼Œä½†å€¼å¾—ä¸€è¯•ã€‚

æ–‡æœ¬æ•°æ®æœ‰å‡ ç§é•¿åº¦åº¦é‡ã€‚æˆ‘å°†ä¸¾ä¸€äº›ä¾‹å­:

*   **å­—æ•°ç»Ÿè®¡**:ç»Ÿè®¡æ–‡æœ¬ä¸­çš„è®°å·æ•°(ç”¨ç©ºæ ¼éš”å¼€)
*   **å­—ç¬¦æ•°**:åˆè®¡æ¯ä¸ªä»¤ç‰Œçš„å­—ç¬¦æ•°
*   **å¥å­è®¡æ•°**:è®¡ç®—å¥å­çš„æ•°é‡(ç”¨å¥å·åˆ†éš”)
*   **å¹³å‡å­—æ•°**:å­—æ•°é™¤ä»¥å­—æ•°(å­—æ•°/å­—æ•°)
*   **å¹³å‡å¥å­é•¿åº¦**:å¥å­é•¿åº¦ä¹‹å’Œé™¤ä»¥å¥å­æ•°é‡(å­—æ•°/å¥å­æ•°)

```
dtf['word_count'] = dtf["text"].apply(lambda x: len(str(x).split(" ")))dtf['char_count'] = dtf["text"].apply(lambda x: sum(len(word) for word in str(x).split(" ")))dtf['sentence_count'] = dtf["text"].apply(lambda x: len(str(x).split(".")))dtf['avg_word_length'] = dtf['char_count'] / dtf['word_count']dtf['avg_sentence_lenght'] = dtf['word_count'] / dtf['sentence_count']dtf.head()
```

![](img/ebad7776d5237ef228e749eda7c3d8ee.png)

è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬é€šå¸¸çš„ä¾‹å­:

![](img/1d53aaa35a3a495fae24032c1796fdef.png)

è¿™äº›æ–°å˜é‡ç›¸å¯¹äºç›®æ ‡çš„åˆ†å¸ƒæ˜¯æ€æ ·çš„ï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘å°†çœ‹çœ‹äºŒå…ƒåˆ†å¸ƒ(ä¸¤ä¸ªå˜é‡å¦‚ä½•ä¸€èµ·ç§»åŠ¨)ã€‚é¦–å…ˆï¼Œæˆ‘å°†æŠŠæ•´ä¸ªè§‚å¯Ÿé›†åˆ†æˆ 3 ä¸ªæ ·æœ¬(æ”¿æ²»ã€å¨±ä¹ã€ç§‘æŠ€)ï¼Œç„¶åæ¯”è¾ƒæ ·æœ¬çš„ç›´æ–¹å›¾å’Œå¯†åº¦ã€‚å¦‚æœåˆ†å¸ƒä¸åŒï¼Œé‚£ä¹ˆå˜é‡æ˜¯å¯é¢„æµ‹çš„ï¼Œå› ä¸º 3 ç»„å…·æœ‰ä¸åŒçš„æ¨¡å¼ã€‚

ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å­—ç¬¦æ•°æ˜¯å¦ä¸ç›®æ ‡å˜é‡ç›¸å…³:

```
**x, y = "char_count", "y"**fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in dtf[y].unique():
    sns.distplot(dtf[dtf[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(dtf[dtf[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(dtf[y].unique())
ax[1].grid(True)
plt.show()
```

![](img/3de80e0d2aa558bf8be6cc7aeabdc00d.png)

è¿™ä¸‰ä¸ªç±»åˆ«å…·æœ‰ç›¸ä¼¼çš„é•¿åº¦åˆ†å¸ƒã€‚è¿™é‡Œï¼Œå¯†åº¦å›¾éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºæ ·æœ¬å¤§å°ä¸åŒã€‚

## æƒ…æ„Ÿåˆ†æ

æƒ…æ„Ÿåˆ†ææ˜¯å°†æ–‡æœ¬æ•°æ®çš„ä¸»è§‚æƒ…æ„Ÿé€šè¿‡æ•°å­—æˆ–ç±»åˆ«è¡¨ç¤ºå‡ºæ¥ã€‚è®¡ç®—æƒ…æ„Ÿæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æœ€å›°éš¾çš„ä»»åŠ¡ä¹‹ä¸€ï¼Œå› ä¸ºè‡ªç„¶è¯­è¨€å……æ»¡äº†æ­§ä¹‰ã€‚ä¾‹å¦‚ï¼ŒçŸ­è¯­â€œ*è¿™æ˜¯å¦‚æ­¤ç³Ÿç³•ï¼Œä»¥è‡³äºå®ƒæ˜¯å¥½çš„*â€æœ‰ä¸æ­¢ä¸€ç§è§£é‡Šã€‚ä¸€ä¸ªæ¨¡å‹å¯ä»¥ç»™å•è¯â€œ*å¥½çš„*â€åˆ†é…ä¸€ä¸ªç§¯æçš„ä¿¡å·ï¼Œç»™å•è¯â€œ*åçš„*â€åˆ†é…ä¸€ä¸ªæ¶ˆæçš„ä¿¡å·ï¼Œäº§ç”Ÿä¸€ä¸ªä¸­æ€§çš„æƒ…ç»ªã€‚å‘ç”Ÿè¿™ç§æƒ…å†µæ˜¯å› ä¸ºèƒŒæ™¯æœªçŸ¥ã€‚

æœ€å¥½çš„æ–¹æ³•æ˜¯è®­ç»ƒä½ è‡ªå·±çš„æƒ…ç»ªæ¨¡å‹ï¼Œä½¿ä¹‹ä¸ä½ çš„æ•°æ®å®Œå…¨å»åˆã€‚å½“æ²¡æœ‰è¶³å¤Ÿçš„æ—¶é—´æˆ–æ•°æ®æ—¶ï¼Œå¯ä»¥ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ï¼Œå¦‚ *Textblob* å’Œ *Vader* ã€‚[*text blob*](https://textblob.readthedocs.io/en/dev/index.html)*ï¼Œ*å»ºç«‹åœ¨ *NLTK ä¹‹ä¸Šï¼Œ*æ˜¯æœ€å—æ¬¢è¿çš„ä¸€ä¸ªï¼Œå®ƒå¯ä»¥ç»™å•è¯åˆ†é…ææ€§ï¼Œå¹¶æŠŠæ•´ç¯‡æ–‡æœ¬çš„æƒ…æ„Ÿä¼°è®¡ä¸ºå¹³å‡å€¼ã€‚å¦ä¸€æ–¹é¢ï¼Œ [*ã€ç»´è¾¾ã€‘*](https://github.com/cjhutto/vaderSentiment) (ä»·è§‰è¯å…¸å’Œæƒ…æ„Ÿæ¨ç†æœº)æ˜¯ä¸€ä¸ªåŸºäºè§„åˆ™çš„æ¨¡å‹ï¼Œåœ¨ç¤¾äº¤åª’ä½“æ•°æ®ä¸Šå·¥ä½œå¾—ç‰¹åˆ«å¥½ã€‚

æˆ‘å°†ä½¿ç”¨ *Textblob* æ·»åŠ ä¸€ä¸ªæƒ…æ„Ÿç‰¹å¾:

```
dtf["sentiment"] = dtf[column].apply(lambda x: 
                   **TextBlob**(x).sentiment.polarity)
dtf.head()
```

![](img/af12f691fc26d2b848ab12e39e68fc3b.png)

```
print(dtf["text"].iloc[0], " --> ", dtf["sentiment"].iloc[0])
```

![](img/e65000efa0e133c118651b766d5abcbe.png)

å“ç±»å’Œæƒ…ç»ªä¹‹é—´æœ‰æ¨¡å¼å—ï¼Ÿ

![](img/8f1da290efebe0a632d01450386b8133.png)

å¤§å¤šæ•°æ ‡é¢˜éƒ½æœ‰ä¸€ç§ä¸­æ€§çš„æƒ…ç»ªï¼Œé™¤äº†æ”¿æ²»æ–°é—»å€¾å‘äºè´Ÿé¢ï¼Œç§‘æŠ€æ–°é—»å€¾å‘äºæ­£é¢ã€‚

## å‘½åå®ä½“è¯†åˆ«

NER ( [å‘½åå®ä½“è¯†åˆ«](https://en.wikipedia.org/wiki/Named-entity_recognition))æ˜¯ç”¨é¢„å®šä¹‰çš„ç±»åˆ«(å¦‚äººåã€ç»„ç»‡ã€ä½ç½®ã€æ—¶é—´è¡¨è¾¾å¼ã€æ•°é‡ç­‰)æ ‡è®°éç»“æ„åŒ–æ–‡æœ¬ä¸­æåˆ°çš„å‘½åå®ä½“çš„è¿‡ç¨‹ã€‚

è®­ç»ƒ NER æ¨¡å‹éå¸¸è€—æ—¶ï¼Œå› ä¸ºå®ƒéœ€è¦éå¸¸ä¸°å¯Œçš„æ•°æ®é›†ã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰äººå·²ç»ä¸ºæˆ‘ä»¬åšäº†è¿™é¡¹å·¥ä½œã€‚æœ€å¥½çš„å¼€æº NER å·¥å…·ä¹‹ä¸€æ˜¯ [*SpaCy*](https://spacy.io/) ã€‚å®ƒæä¾›äº†ä¸åŒçš„ NLP æ¨¡å‹ï¼Œèƒ½å¤Ÿè¯†åˆ«å‡ ç±»å®ä½“ã€‚

![](img/732df2597768de8e1ec31cc5804f9b28.png)

æ¥æº:[ç©ºé—´](https://spacy.io/api/annotation#section-named-entities)

æˆ‘å°†åœ¨æˆ‘ä»¬é€šå¸¸çš„æ ‡é¢˜(åŸå§‹æ–‡æœ¬ï¼Œæœªç»é¢„å¤„ç†)ä¸Šä½¿ç”¨*SpaCy*model*en _ core _ web _ LG*(åŸºäº web æ•°æ®è®­ç»ƒçš„è‹±è¯­å¤§æ¨¡å‹)æ¥ä¸¾ä¾‹è¯´æ˜:

```
**## call model**
ner = **spacy**.load("**en_core_web_lg**")**## tag text**
txt = dtf["text"].iloc[0]
doc = **ner**(txt)**## display result**
spacy.**displacy**.render(doc, style="ent")
```

![](img/0be64a72725c27810c0b64b0deff8708.png)

è¿™å¾ˆé…·ï¼Œä½†æ˜¯æˆ‘ä»¬å¦‚ä½•æŠŠå®ƒå˜æˆä¸€ä¸ªæœ‰ç”¨çš„ç‰¹æ€§å‘¢ï¼Ÿè¿™æ˜¯æˆ‘è¦åšçš„:

*   å¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ–‡æœ¬è§‚å¯Ÿå€¼è¿è¡Œ NER æ¨¡å‹ï¼Œå°±åƒæˆ‘åœ¨å‰é¢çš„ä¾‹å­ä¸­æ‰€åšçš„é‚£æ ·ã€‚
*   å¯¹äºæ¯ä¸ªæ–°é—»æ ‡é¢˜ï¼Œæˆ‘å°†æŠŠæ‰€æœ‰å·²è¯†åˆ«çš„å®ä½“æ”¾å…¥ä¸€ä¸ªæ–°çš„åˆ—(åä¸ºâ€œtagsâ€)ä¸­ï¼ŒåŒæ—¶åˆ—å‡ºè¯¥å®ä½“åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œåº”è¯¥æ˜¯

> {('å¨å°”Â·å²å¯†æ–¯'ï¼Œ'äºº'):1ï¼Œ
> ('è¿ªæ™®'ï¼Œ'äºº'):1ï¼Œ
> ('å°¼åŸºÂ·è´¾å§†'ï¼Œ'äºº'):1ï¼Œ
> (â€œ2018 ä¸–ç•Œæ¯çš„'ï¼Œ'äº‹ä»¶'):1 }

*   ç„¶åï¼Œæˆ‘å°†ä¸ºæ¯ä¸ªæ ‡ç­¾ç±»åˆ«(Personã€Orgã€Event ç­‰)åˆ›å»ºä¸€ä¸ªæ–°åˆ—ï¼Œå¹¶è®¡ç®—æ¯ä¸ªç±»åˆ«ä¸­æ‰¾åˆ°çš„å®ä½“çš„æ•°é‡ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œè¿™äº›ç‰¹æ€§æ˜¯

> tags_PERSON = 3
> 
> tags_EVENT = 1

```
**## tag text and exctract tags into a list**
dtf["tags"] = dtf["text"].apply(lambda x: [(tag.text, tag.label_) 
                                for tag in ner(x).ents] )**## utils function to count the element of a list** def **utils_lst_count**(lst):
    dic_counter = collections.Counter()
    for x in lst:
        dic_counter[x] += 1
    dic_counter = collections.OrderedDict( 
                     sorted(dic_counter.items(), 
                     key=lambda x: x[1], reverse=True))
    lst_count = [ {key:value} for key,value in dic_counter.items() ]
    return lst_count **## count tags**
dtf["tags"] = dtf["tags"].apply(lambda x: **utils_lst_count**(x)) **## utils function create new column for each tag category** def **utils_ner_features**(lst_dics_tuples, tag):
    if len(lst_dics_tuples) > 0:
        tag_type = []
        for dic_tuples in lst_dics_tuples:
            for tuple in dic_tuples:
                type, n = tuple[1], dic_tuples[tuple]
                tag_type = tag_type + [type]*n
                dic_counter = collections.Counter()
                for x in tag_type:
                    dic_counter[x] += 1
        return dic_counter[tag]
    else:
        return 0 **## extract features**
tags_set = []
for lst in dtf["tags"].tolist():
     for dic in lst:
          for k in dic.keys():
              tags_set.append(k[1])
tags_set = list(set(tags_set))
for feature in tags_set:
     dtf["tags_"+feature] = dtf["tags"].apply(lambda x: 
                             **utils_ner_features**(x, feature)) **## print result**
dtf.head()
```

![](img/e8fc8e78d80454d89d2204391732e445.png)

ç°åœ¨æˆ‘ä»¬å¯ä»¥å¯¹æ ‡ç­¾ç±»å‹åˆ†å¸ƒæœ‰ä¸€ä¸ªå®è§‚çš„çœ‹æ³•ã€‚è®©æˆ‘ä»¬ä»¥ ORG æ ‡ç­¾(å…¬å¸å’Œç»„ç»‡)ä¸ºä¾‹:

![](img/57665e5abfa8b866616726f7f26e6209.png)

ä¸ºäº†æ›´æ·±å…¥åœ°åˆ†æï¼Œæˆ‘ä»¬éœ€è¦è§£åŒ…æˆ‘ä»¬åœ¨å‰é¢çš„ä»£ç ä¸­åˆ›å»ºçš„åˆ—â€œtagsâ€ã€‚è®©æˆ‘ä»¬ä¸ºå…¶ä¸­ä¸€ä¸ªæ ‡é¢˜ç±»åˆ«ç”»å‡ºæœ€å¸¸è§çš„æ ‡ç­¾:

```
y = "**ENTERTAINMENT**"

tags_list = dtf[dtf["y"]==y]["tags"].sum()
map_lst = list(map(lambda x: list(x.keys())[0], tags_list))
dtf_tags = pd.DataFrame(map_lst, columns=['tag','type'])
dtf_tags["count"] = 1
dtf_tags = dtf_tags.groupby(['type',  
                'tag']).count().reset_index().sort_values("count", 
                 ascending=False)
fig, ax = plt.subplots()
fig.suptitle("Top frequent tags", fontsize=12)
sns.barplot(x="count", y="tag", hue="type", 
            data=dtf_tags.iloc[:top,:], dodge=False, ax=ax)
ax.grid(axis="x")
plt.show()
```

![](img/49187001d6a18221c1d43a5a6cb4ed2e.png)

ç»§ç»­ NER çš„å¦ä¸€ä¸ªæœ‰ç”¨çš„åº”ç”¨:ä½ è¿˜è®°å¾—å½“æˆ‘ä»¬ä»"*"çš„åå­—ä¸­å»æ‰å•è¯"*"çš„æ—¶å€™å—ï¼Ÿè¿™ä¸ªé—®é¢˜çš„ä¸€ä¸ªæœ‰è¶£çš„è§£å†³æ–¹æ¡ˆæ˜¯ç”¨â€œ *Will_Smith* â€æ›¿æ¢â€œ*å¨å°”Â·å²å¯†æ–¯*â€ï¼Œè¿™æ ·å®ƒå°±ä¸ä¼šå—åˆ°åœç”¨è¯åˆ é™¤çš„å½±å“ã€‚å› ä¸ºéå†æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ–‡æœ¬æ¥æ›´æ”¹åç§°æ˜¯ä¸å¯èƒ½çš„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä½¿ç”¨ *SpaCy* æ¥å®ç°ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œ *SpaCy* å¯ä»¥è¯†åˆ«äººåï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥è¿›è¡Œ**å§“åæ£€æµ‹**ç„¶åä¿®æ”¹å­—ç¬¦ä¸²ã€‚**

```
****## predict wit NER** txt = dtf["text"].iloc[0]
entities = ner(txt).ents**## tag text**
tagged_txt = txt
for tag in entities:
    tagged_txt = re.sub(tag.text, "_".join(tag.text.split()), 
                        tagged_txt) **## show result**
print(tagged_txt)**
```

**![](img/510829d3d63ae4f5efb37f808cb14f01.png)**

## **å­—é¢‘ç‡**

**åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•é€šè¿‡åˆ†æå’Œå¤„ç†æ•´ä¸ªæ–‡æœ¬æ¥è¿›è¡Œç‰¹å¾å·¥ç¨‹ã€‚ç°åœ¨æˆ‘ä»¬å°†é€šè¿‡è®¡ç®—å•è¯çš„å‡ºç°é¢‘ç‡æ¥äº†è§£å•è¯çš„é‡è¦æ€§ã€‚ä¸€ä¸ª ***n* -gram** æ˜¯æ¥è‡ªç»™å®šæ–‡æœ¬æ ·æœ¬çš„ *n* é¡¹çš„è¿ç»­åºåˆ—ã€‚å½“ *n* -gram çš„å¤§å°ä¸º 1 æ—¶ï¼Œç§°ä¸ºä¸€å…ƒ gram(å¤§å°ä¸º 2 çš„æ˜¯äºŒå…ƒ gram)ã€‚**

**ä¾‹å¦‚ï¼ŒçŸ­è¯­â€œ*æˆ‘å–œæ¬¢è¿™ç¯‡æ–‡ç« *â€å¯ä»¥åˆ†è§£ä¸º:**

*   **4 unigrams:â€œ*I*â€ã€ *like* ã€ *this* ã€ *article* â€**
*   **3 ä¸ªå¤§äººç‰©:â€œ*æˆ‘å–œæ¬¢*â€ã€â€œ*å–œæ¬¢è¿™ä¸ª*â€ã€â€œ*è¿™ç¯‡æ–‡ç« *â€**

**æˆ‘å°†ä»¥æ”¿æ²»æ–°é—»ä¸ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•è®¡ç®—å•è¯å’ŒåŒè¯çš„é¢‘ç‡ã€‚**

```
**y = "**POLITICS**"
corpus = dtf[dtf["y"]==y]["text_clean"]lst_tokens = **nltk**.tokenize.word_tokenize(corpus.str.cat(sep=" "))
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle("Most frequent words", fontsize=15)

**## unigrams**
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[0], 
                  legend=False).grid(axis='x')
ax[0].set(ylabel=None)

**## bigrams**
dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))
dtf_bi = pd.DataFrame(dic_words_freq.most_common(), 
                      columns=["Word","Freq"])
dtf_bi["Word"] = dtf_bi["Word"].apply(lambda x: " ".join(
                   string for string in x) )
dtf_bi.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Bigrams", ax=ax[1],
                  legend=False).grid(axis='x')
ax[1].set(ylabel=None)
plt.show()**
```

**![](img/45880135613f72997ee87a2010d83d42.png)**

**å¦‚æœæœ‰åªå‡ºç°åœ¨ä¸€ä¸ªç±»åˆ«ä¸­çš„ *n* -grams(ä¾‹å¦‚æ”¿æ²»æ–°é—»ä¸­çš„â€œå…±å’Œå…šâ€)ï¼Œè¿™äº›å¯ä»¥æˆä¸ºæ–°çš„ç‰¹å¾ã€‚ä¸€ç§æ›´è´¹åŠ›çš„æ–¹æ³•æ˜¯å°†æ•´ä¸ªè¯­æ–™åº“çŸ¢é‡åŒ–ï¼Œå¹¶å°†æ‰€æœ‰å•è¯ç”¨ä½œç‰¹å¾(å•è¯è¢‹æ–¹æ³•)ã€‚**

**ç°åœ¨ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•åœ¨æ‚¨çš„æ•°æ®æ¡†æ¶ä¸­æ·»åŠ è¯é¢‘ä½œä¸ºä¸€é¡¹åŠŸèƒ½ã€‚æˆ‘ä»¬åªéœ€è¦æ¥è‡ª *Scikit-learn çš„*è®¡æ•°çŸ¢é‡å™¨*ï¼Œ*Python ä¸­æœ€æµè¡Œçš„æœºå™¨å­¦ä¹ åº“ä¹‹ä¸€ã€‚çŸ¢é‡å™¨å°†æ–‡æœ¬æ–‡æ¡£çš„é›†åˆè½¬æ¢æˆä»¤ç‰Œè®¡æ•°çš„çŸ©é˜µã€‚æˆ‘ç”¨ 3 ä¸ª n-grams ä¸¾ä¸ªä¾‹å­:â€œ*ç¥¨æˆ¿*(å¨±ä¹åœˆé¢‘ç¹)ã€*å…±å’Œå…š*(æ”¿ç•Œé¢‘ç¹)ã€â€œè‹¹æœâ€(ç§‘æŠ€ç•Œé¢‘ç¹)ã€‚**

```
*lst_words = ["**box office**", "**republican**", "**apple**"]***## count*** lst_grams = [len(word.split(" ")) for word in lst_words]
vectorizer = feature_extraction.text.**CountVectorizer**(
                 vocabulary=lst_words, 
                 ngram_range=(min(lst_grams),max(lst_grams)))dtf_X = pd.DataFrame(vectorizer.fit_transform(dtf["text_clean"]).todense(), columns=lst_words)***## add the new features as columns*** dtf = pd.concat([dtf, dtf_X.set_index(dtf.index)], axis=1)
dtf.head()*
```

*![](img/6f448b6b16176a22337dce647a5755a9.png)*

*å¯è§†åŒ–ç›¸åŒä¿¡æ¯çš„ä¸€ä¸ªå¥½æ–¹æ³•æ˜¯ä½¿ç”¨**å•è¯äº‘**ï¼Œå…¶ä¸­æ¯ä¸ªæ ‡ç­¾çš„é¢‘ç‡ç”¨å­—ä½“å¤§å°å’Œé¢œè‰²æ˜¾ç¤ºã€‚*

```
*wc = **wordcloud**.WordCloud(background_color='black', max_words=100, 
                         max_font_size=35)
wc = wc.generate(str(corpus))
fig = plt.figure(num=1)
plt.axis('off')
plt.imshow(wc, cmap=None)
plt.show()*
```

*![](img/7c5fba1b8cdc020431fb02b50c98e346.png)*

## *è¯å‘é‡*

*æœ€è¿‘ï¼ŒNLP é¢†åŸŸå¼€å‘äº†æ–°çš„è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¾èµ–äºç¥ç»ç½‘ç»œæ¶æ„ï¼Œè€Œä¸æ˜¯æ›´ä¼ ç»Ÿçš„ n-gram æ¨¡å‹ã€‚è¿™äº›æ–°æŠ€æœ¯æ˜¯ä¸€å¥—è¯­è¨€å»ºæ¨¡å’Œç‰¹å¾å­¦ä¹ æŠ€æœ¯ï¼Œå…¶ä¸­å•è¯è¢«è½¬æ¢æˆå®æ•°å‘é‡ï¼Œå› æ­¤å®ƒä»¬è¢«ç§°ä¸º**å•è¯åµŒå…¥**ã€‚*

*å•è¯åµŒå…¥æ¨¡å‹é€šè¿‡æ„å»ºåœ¨æ‰€é€‰å•è¯ä¹‹å‰å’Œä¹‹åä¼šå‡ºç°ä»€ä¹ˆæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå°†ç‰¹å®šå•è¯æ˜ å°„åˆ°å‘é‡ã€‚è¿™äº›æ¨¡å‹å¾ˆå¿«å˜å¾—æµè¡Œèµ·æ¥ï¼Œå› ä¸ºä¸€æ—¦æœ‰äº†å®æ•°è€Œä¸æ˜¯å­—ç¬¦ä¸²ï¼Œå°±å¯ä»¥è¿›è¡Œè®¡ç®—äº†ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†æ‰¾åˆ°ç›¸åŒä¸Šä¸‹æ–‡çš„å•è¯ï¼Œå¯ä»¥ç®€å•åœ°è®¡ç®—å‘é‡è·ç¦»ã€‚*

*æœ‰å‡ ä¸ª Python åº“å¯ä»¥å¤„ç†è¿™ç§æ¨¡å‹ã€‚ *SpaCy* æ˜¯ä¸€ä¸ªï¼Œä¸è¿‡æ—¢ç„¶æˆ‘ä»¬å·²ç»ç”¨è¿‡äº†ï¼Œæˆ‘å°±è¯´è¯´å¦ä¸€ä¸ªè‘—åçš„åŒ…: [*Gensim*](https://radimrehurek.com/gensim/) *ã€‚*ä¸€ä¸ªä½¿ç”¨ç°ä»£ç»Ÿè®¡æœºå™¨å­¦ä¹ çš„æ— ç›‘ç£ä¸»é¢˜å»ºæ¨¡å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„å¼€æºåº“ã€‚ä½¿ç”¨ *Gensim* ï¼Œæˆ‘å°†åŠ è½½ä¸€ä¸ªé¢„å…ˆè®­ç»ƒå¥½çš„*æ‰‹å¥—*æ¨¡å‹ã€‚ [*GloVe* (å…¨å±€å‘é‡)](https://nlp.stanford.edu/projects/glove/)æ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œç”¨äºè·å¾—å¤§å°ä¸º 300 çš„å•è¯çš„å‘é‡è¡¨ç¤ºã€‚*

```
*nlp = **gensim_api**.load("**glove-wiki-gigaword-300**")*
```

*æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªå¯¹è±¡å°†å•è¯æ˜ å°„åˆ°å‘é‡:*

```
***word = "love"**nlp[word]*
```

*![](img/bbb4e8ad98f75efdba65ab2087dc1ca6.png)*

```
*nlp[word].shape*
```

*![](img/88a1e2b6b66cd3e705ebea81ab982ff2.png)*

*ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ä»€ä¹ˆæ˜¯æœ€æ¥è¿‘çš„è¯å‘é‡ï¼Œæˆ–è€…æ¢å¥è¯è¯´ï¼Œæœ€å¸¸å‡ºç°åœ¨ç›¸ä¼¼ä¸Šä¸‹æ–‡ä¸­çš„è¯ã€‚ä¸ºäº†åœ¨äºŒç»´ç©ºé—´ä¸­ç»˜åˆ¶å‘é‡ï¼Œæˆ‘éœ€è¦å°†ç»´åº¦ä» 300 å‡å°‘åˆ° 2ã€‚æˆ‘å°†ç”¨æ¥è‡ª *Scikit-learn çš„*t-åˆ†å¸ƒå¼éšæœºé‚»å±…åµŒå…¥*æ¥åšè¿™ä»¶äº‹ã€‚* t-SNE æ˜¯ä¸€ç§å¯è§†åŒ–é«˜ç»´æ•°æ®çš„å·¥å…·ï¼Œå®ƒå°†æ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§è½¬æ¢ä¸ºè”åˆæ¦‚ç‡ã€‚*

```
***## find closest vectors**
labels, X, x, y = [], [], [], []
for t in nlp.**most_similar**(word, topn=20):
    X.append(nlp[t[0]])
    labels.append(t[0])**## reduce dimensions**
pca = manifold.**TSNE**(perplexity=40, n_components=2, init='pca')
new_values = pca.fit_transform(X)
for value in new_values:
    x.append(value[0])
    y.append(value[1])**## plot**
fig = plt.figure()
for i in range(len(x)):
    plt.scatter(x[i], y[i], c="black")
    plt.annotate(labels[i], xy=(x[i],y[i]), xytext=(5,2), 
               textcoords='offset points', ha='right', va='bottom')**## add center**
plt.scatter(x=0, y=0, c="red")
plt.annotate(word, xy=(0,0), xytext=(5,2), textcoords='offset 
             points', ha='right', va='bottom')*
```

*![](img/16c2a564ab78876e5f8515276f9f1d32.png)*

## *ä¸»é¢˜å»ºæ¨¡*

*Genism åŒ…ä¸“é—¨ç”¨äºä¸»é¢˜å»ºæ¨¡ã€‚ä¸»é¢˜æ¨¡å‹æ˜¯ä¸€ç§ç”¨äºå‘ç°å‡ºç°åœ¨æ–‡æ¡£é›†åˆä¸­çš„æŠ½è±¡â€œä¸»é¢˜â€çš„ç»Ÿè®¡æ¨¡å‹ã€‚*

*æˆ‘å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ *LDA* (æ½œåœ¨çš„ç‹„åˆ©å…‹é›·åˆ†é…)æå–ä¸»é¢˜:ä¸€ç§ç”Ÿæˆç»Ÿè®¡æ¨¡å‹ï¼Œå®ƒå…è®¸é€šè¿‡æœªè§‚å¯Ÿåˆ°çš„ç»„æ¥è§£é‡Šè§‚å¯Ÿé›†ï¼Œä»è€Œè§£é‡Šä¸ºä»€ä¹ˆæ•°æ®çš„æŸäº›éƒ¨åˆ†æ˜¯ç›¸ä¼¼çš„ã€‚åŸºæœ¬ä¸Šï¼Œæ–‡æ¡£è¢«è¡¨ç¤ºä¸ºæ½œåœ¨ä¸»é¢˜çš„éšæœºæ··åˆï¼Œå…¶ä¸­æ¯ä¸ªä¸»é¢˜ç”±å•è¯çš„åˆ†å¸ƒæ¥è¡¨å¾ã€‚*

*è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬èƒ½ä»ç§‘æŠ€æ–°é—»ä¸­æå–å‡ºä»€ä¹ˆè¯é¢˜ã€‚æˆ‘éœ€è¦æŒ‡å®šæ¨¡å‹å¿…é¡»èšç±»çš„ä¸»é¢˜æ•°é‡ï¼Œæˆ‘å°†å°è¯•ä½¿ç”¨ 3:*

```
*y = "**TECH**"
corpus = dtf[dtf["y"]==y]["text_clean"] **## pre-process corpus**
lst_corpus = []
for string in corpus:
    lst_words = string.split()
    lst_grams = [" ".join(lst_words[i:i + 2]) for i in range(0, 
                     len(lst_words), 2)]
    lst_corpus.append(lst_grams)**## map words to an id**
id2word = gensim.corpora.Dictionary(lst_corpus)**## create dictionary word:freq**
dic_corpus = [id2word.doc2bow(word) for word in lst_corpus] **## train LDA**
lda_model = gensim.models.ldamodel.**LdaModel**(corpus=dic_corpus, id2word=id2word, **num_topics=3**, random_state=123, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)

**## output**
lst_dics = []
for i in range(0,**3**):
    lst_tuples = lda_model.get_topic_terms(i)
    for tupla in lst_tuples:
        lst_dics.append({"topic":i, "id":tupla[0], 
                         "word":id2word[tupla[0]], 
                         "weight":tupla[1]})
dtf_topics = pd.DataFrame(lst_dics, 
                         columns=['topic','id','word','weight'])

**## plot**
fig, ax = plt.subplots()
sns.barplot(y="word", x="weight", hue="topic", data=dtf_topics, dodge=False, ax=ax).set_title('Main Topics')
ax.set(ylabel="", xlabel="Word Importance")
plt.show()*
```

*![](img/5290f18efc0e40c3d0337e087c1d726f.png)*

*è¯•å›¾åªåœ¨ 3 ä¸ªä¸»é¢˜ä¸­æ•æ‰ 6 å¹´çš„å†…å®¹å¯èƒ½æœ‰ç‚¹å›°éš¾ï¼Œä½†æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œå…³äºè‹¹æœå…¬å¸çš„ä¸€åˆ‡éƒ½ä»¥åŒä¸€ä¸ªä¸»é¢˜ç»“æŸã€‚*

## *ç»“è®º*

*è¿™ç¯‡æ–‡ç« æ˜¯ä¸€ä¸ªæ•™ç¨‹ï¼Œæ¼”ç¤ºäº†**å¦‚ä½•ç”¨ NLP åˆ†ææ–‡æœ¬æ•°æ®ï¼Œå¹¶ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹**æå–ç‰¹å¾ã€‚*

*æˆ‘å±•ç¤ºäº†å¦‚ä½•æ£€æµ‹æ•°æ®ä½¿ç”¨çš„è¯­è¨€ï¼Œä»¥åŠå¦‚ä½•é¢„å¤„ç†å’Œæ¸…ç†æ–‡æœ¬ã€‚ç„¶åæˆ‘è§£é‡Šäº†ä¸åŒçš„é•¿åº¦åº¦é‡ï¼Œç”¨ *Textblob* åšäº†æƒ…æ„Ÿåˆ†æï¼Œæˆ‘ä»¬ä½¿ç”¨ *SpaCy* è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ã€‚æœ€åï¼Œæˆ‘è§£é‡Šäº†ä½¿ç”¨ *Scikit-learn* çš„ä¼ ç»Ÿè¯é¢‘æ–¹æ³•å’Œä½¿ç”¨ *Gensim* çš„ç°ä»£è¯­è¨€æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«ã€‚ç°åœ¨ï¼Œæ‚¨å·²ç»åŸºæœ¬äº†è§£äº† NLP çš„æ‰€æœ‰åŸºç¡€çŸ¥è¯†ï¼Œå¯ä»¥å¼€å§‹å¤„ç†æ–‡æœ¬æ•°æ®äº†ã€‚*

*æˆ‘å¸Œæœ›ä½ å–œæ¬¢å®ƒï¼å¦‚æœ‰é—®é¢˜å’Œåé¦ˆï¼Œæˆ–è€…åªæ˜¯åˆ†äº«æ‚¨æ„Ÿå…´è¶£çš„é¡¹ç›®ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚*

> *ğŸ‘‰[æˆ‘ä»¬æ¥è¿çº¿](https://linktr.ee/maurodp)ğŸ‘ˆ*

> *æœ¬æ–‡æ˜¯ç³»åˆ—æ–‡ç«  **NLP ä¸ Python** çš„ä¸€éƒ¨åˆ†ï¼Œå‚è§:*

*[](/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09) [## ä½¿ç”¨ NLP çš„æ–‡æœ¬æ‘˜è¦:TextRank vs Seq2Seq vs BART

### ä½¿ç”¨ Pythonã€Gensimã€Tensorflowã€Transformers è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†

towardsdatascience.com](/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09) [](/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) [## åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡æœ¬åˆ†ç±»:Tf-Idf vs Word2Vec vs BERT

### é¢„å¤„ç†ã€æ¨¡å‹è®¾è®¡ã€è¯„ä¼°ã€è¯è¢‹çš„å¯è§£é‡Šæ€§ã€è¯åµŒå…¥ã€è¯­è¨€æ¨¡å‹

towardsdatascience.com](/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) [](/text-classification-with-no-model-training-935fe0e42180) [## ç”¨äºæ— æ¨¡å‹è®­ç»ƒçš„æ–‡æœ¬åˆ†ç±»çš„ BERT

### å¦‚æœæ²¡æœ‰å¸¦æ ‡ç­¾çš„è®­ç»ƒé›†ï¼Œè¯·ä½¿ç”¨ BERTã€å•è¯åµŒå…¥å’Œå‘é‡ç›¸ä¼¼åº¦

towardsdatascience.com](/text-classification-with-no-model-training-935fe0e42180) [](/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9) [## å¸¦ NLP çš„ AI èŠå¤©æœºå™¨äºº:è¯­éŸ³è¯†åˆ«+å˜å½¢é‡‘åˆš

### ç”¨ Python æ„å»ºä¸€ä¸ªä¼šè¯´è¯çš„èŠå¤©æœºå™¨äººï¼Œä¸ä½ çš„äººå·¥æ™ºèƒ½è¿›è¡Œå¯¹è¯

towardsdatascience.com](/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9)*