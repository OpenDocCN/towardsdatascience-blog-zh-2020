# #NLP365 的第 109 天:NLP 论文摘要—研究适当评分范围内的摘要评估指标

> 原文：<https://towardsdatascience.com/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27?source=collection_archive---------48----------------------->

![](img/61a393f717c1685f581a48076c22cd22.png)

阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在 [Unsplash](https://unsplash.com/s/photos/research-papers?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片。

## [内线艾](https://medium.com/towards-data-science/inside-ai/home) [NLP365](http://towardsdatascience.com/tagged/nlp365)

## NLP 论文摘要是我总结 NLP 研究论文要点的系列文章

项目#NLP365 (+1)是我在 2020 年每天记录我的 NLP 学习旅程的地方。在这里，你可以随意查看我在过去的 100 天里学到了什么。

今天的 NLP 论文是 ***研究适当评分范围*** 内的摘要评价度量。以下是研究论文的要点。

# 目标和贡献

评估指标的作用极其重要，因为它们在很大程度上指导着特定领域的研究进展。自动评估指标的目标是准确地评估生成的接近人类判断的摘要。该论文表明，在表现相似的评价指标和得分较高的评价指标之间存在强烈的分歧。这种分歧意味着我们不知道在评估我们生成的摘要时应该信任哪些指标。本文的贡献如下:

1.  引入一种方法学来研究高分范围内的评价指标，发现指标之间存在低/负相关性。这项工作希望鼓励研究人员在适当的评分范围内收集更多的人类注释

没有很多手动标注的数据集，现有的数据集是在 2008 年的共享任务中创建的，因此与当前水平相比，标注的摘要是平均水平。下图对此进行了说明。如你所见，基本事实摘要的分数分布(蓝色)不同于现代摘要系统生成的摘要的分数分布(红色)。不能保证评估指标在红色分布(高分范围)中的行为与人工评估相似，本文的目的是评估高分范围中的评估指标，以评估它们的评估能力是否与人工评估一致和相关。该文件计算不同评分范围中的度量对之间的相关性，而不需要人工评估。

![](img/a78c7b034f1c28ad3de13128ad5b67ab.png)

不同分数级别摘要之间的分布[1]

# 实验设置—数据生成

本文研究了以下指标:

1.  *胭脂-2 (R-2)* 。生成的摘要和基本事实之间的二元模型重叠
2.  *胭脂-L (R-L)* 。生成的摘要和地面实况之间的最长公共子序列
3.  *胭脂-WE (R-WE)* 。基于余弦相似度和单词嵌入的软匹配
4.  *JS 发散(JS-2)* 。使用 Jensen-Shannon 散度来衡量二元模型分布之间的差异
5.  *S3* 。最大化其与手动金字塔注释相关性的度量

作者使用遗传算法进行总结，以生成优化每个指标的总结。生成的数据集(表示为 W)由 160，523 个汇总组成，每个指标大约有 1763 个汇总。为了关注高分摘要，我们使用 LexRank 过滤掉表现不佳的摘要。在去除重复和过滤之后，这导致了每个主题大约 102 个摘要的最终数据集(T)。人类判断摘要被表示为。

# 相关分析

## 辛普森悖论

![](img/878cba6b2469ac4271e25efb3c8de558.png)

不同评分级别摘要上不同评估指标之间的成对相关性[1]

从上图中，我们可以看到，对于数据集 A 和 W，评估指标之间有很高的相关性，其中 R-2 和 JS-2 的相关性最强。这可以用他们都是基于二元模型的事实来解释。R-L 与其他指标的相关性最小。然而，在高分汇总(T)中，度量之间的相关性很低，有些甚至是负的。当我们检查比 LexRank 更好的摘要时，衡量改进的度量标准之间没有全球一致的意见。事实上，结果显示，这种分歧随着得分较高的总结而增加，如下图所示。

![](img/a4ce41a90c5f28bece9d3b34e43c0ed7.png)

汇总分数的度量标准之间的不一致程度[1]

这就是众所周知的辛普森悖论，根据你从哪个亚人群中得出不同的结论。结果告诉我们，我们当前的评估标准善于区分非常差的概要和非常好的概要，但是不能区分高分的概要。

## 跨指标衡量一致的改进

给定一组评估指标，为了跨指标测量一致的改进(指标彼此一致)，我们计算以下 F/N 比:

1.  选择一个摘要
2.  在这些摘要中，哪一个比 s 好一个度量(N)
3.  在所有指标(F)的总结中，哪些优于 s
4.  f 除以 N 得到比值

下图显示了对 5000 个随机样本汇总重复的这一过程。结果表明，随着总结平均分的增加，持续改进的比例(F/N 比)迅速下降。通过使用多个彼此不一致的评估度量，很难识别出具有高置信度的高分摘要。

![](img/fca835735213481cf4580e3ad318cb70.png)

随着总结得分的增加，对持续改进的衡量[1]

# 结论和未来工作

高分汇总中评估指标之间的不一致意味着很难评估高分汇总，研究人员面临辛普森悖论的风险很高。该分析是在 TAC-2008 和 TAC-2009 数据集上进行的，因为它们是评估评估指标的标准数据集。未来的工作可以将这种分析扩展到其他数据集和/或其他 NLP 任务，如机器翻译。

## 来源:

[1] Peyrard，m .，2019 年 7 月。在适当的评分范围内研究总结评估指标。《计算语言学协会第 57 届年会会议录》(第 5093–5100 页)。网址:【https://www.aclweb.org/anthology/P19-1502.pdf 

【https://ryanong.co.uk】原载于 2020 年 4 月 18 日[](https://ryanong.co.uk/2020/04/18/day-109-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-appropriate-scoring-range/)**。**