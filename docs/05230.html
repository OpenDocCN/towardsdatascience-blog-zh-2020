<html>
<head>
<title>Differential Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">差分机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/differential-machine-learning-f207c158064d?source=collection_archive---------26-----------------------#2020-05-04">https://towardsdatascience.com/differential-machine-learning-f207c158064d?source=collection_archive---------26-----------------------#2020-05-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="94c5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用导数的新用途进行不合理的有效函数逼近</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/0e4e1d15f40eb084ca65d59db1cbaa64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*aG57bjhRprxtW2mOUSH50w.png"/></div></figure><div class="kq kr gp gr ks kt"><a href="https://arxiv.org/abs/2005.02347" rel="noopener  ugc nofollow" target="_blank"><div class="ku ab fo"><div class="kv ab kw cl cj kx"><h2 class="bd iu gy z fp ky fr fs kz fu fw is bi translated">差分机器学习</h2><div class="la l"><h3 class="bd b gy z fp ky fr fs kz fu fw dk translated">工作底稿</h3></div><div class="lb l"><p class="bd b dl z fp ky fr fs kz fu fw dk translated">arxiv.org/abs/2005.02347</p></div></div></div></a></div><div class="kq kr gp gr ks kt"><a href="https://github.com/differential-machine-learning" rel="noopener  ugc nofollow" target="_blank"><div class="ku ab fo"><div class="kv ab kw cl cj kx"><h2 class="bd iu gy z fp ky fr fs kz fu fw is bi translated">差分机器学习概述</h2><div class="la l"><h3 class="bd b gy z fp ky fr fs kz fu fw dk translated">同伴回购</h3></div><div class="lb l"><p class="bd b dl z fp ky fr fs kz fu fw dk translated">github.com/differential-machine-leanring-</p></div></div><div class="lc l"><div class="ld l le lf lg lc lh ko kt"/></div></div></a></div><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="li lj l"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">为2020年5月28日彭博烧烤研讨会录制的5分钟视频概述</p></figure><p id="2cd7" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated"><a class="ae mk" href="https://www.risk.net/awards/2133160/quants-year-jesper-andreasen-and-brian-huge-danske-bank" rel="noopener ugc nofollow" target="_blank"> Brian Huge </a>和我刚刚发布了一篇<a class="ae mk" href="https://arxiv.org/abs/2005.02347" rel="noopener ugc nofollow" target="_blank">的工作论文</a>，这是在丹斯克银行进行了六个月的人工智能(AI)函数逼近的研究和开发之后。一个主要发现是，当训练标签相对于训练输入的梯度可用时，用于回归(即值的预测，而不是类的预测)的训练机器学习(ML)模型可以得到极大的改善。给定这些<em class="ml">差分标签</em>，我们可以编写简单但不合理有效的训练算法，能够以稳定的方式从小数据集以显著的速度和准确度学习精确的函数近似，而不需要额外的正则化或超参数优化，例如通过交叉验证。</p><blockquote class="mm mn mo"><p id="a5ae" class="lo lp ml lq b lr ls ju lt lu lv jx lw mp ly lz ma mq mc md me mr mg mh mi mj im bi translated">在这篇文章中，我们以<em class="it">差分机器学习</em>的名义简要总结了这些算法，强调了主要的直觉和好处，并评论了TensorFlow实现代码。所有细节都可以在工作文件、<a class="ae mk" href="https://github.com/differential-machine-learning/appendices" rel="noopener ugc nofollow" target="_blank">在线附录</a>和<a class="ae mk" href="https://github.com/differential-machine-learning/notebooks" rel="noopener ugc nofollow" target="_blank">Colab笔记本</a>中找到。</p></blockquote><p id="3b0d" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">在金融衍生品定价近似的背景下，训练集是用蒙特卡罗模型<em class="ml">模拟的</em>。每个训练示例都是在一个蒙特卡罗路径上模拟的，其中标签是交易的最终<em class="ml">收益</em>，输入是市场的初始状态向量。微分标签是对状态的收益wrt的<em class="ml">路径梯度</em>，并且用<a class="ae mk" rel="noopener" target="_blank" href="/automatic-differentiation-15min-video-tutorial-with-application-in-machine-learning-and-finance-333e18c0ecbb">自动伴随微分(AAD) </a>有效地计算。由于这个原因，差分机器学习<em class="ml"> </em>在金融中特别有效，尽管它也适用于所有其他可以获得高质量一阶导数wrt训练输入的情况。</p><p id="fbea" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">模型不仅在输入和标签的扩充数据集上训练，而且在差异数据集上训练:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/e22929b23c9738b9fb504d2cfe9f6651.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*Is1Y64J2RlRMd5j7S7Ga0w.png"/></div></figure><p id="b25c" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">通过最小化值<em class="ml">和导数</em>的预测误差的组合成本:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/bb211980fd32acb66451fc5a3cd3226f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*abqqZZ56c4ugwGejiSFxPA.png"/></div></figure><p id="344d" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">给出了值和导数<em class="ml">标签</em>。我们通常通过推理计算<em class="ml">预测的</em>值，通过反向传播计算<em class="ml">预测的</em>导数。虽然这种方法适用于任意复杂的架构，但为了简单起见，我们在此讨论普通前馈网络。</p><p id="246a" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">回想一下普通前馈方程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/5073307cea91bf408958b878056a6b07.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*USMMV2RjQE2XBinIm7Zy3Q.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">前馈方程</p></figure><p id="5f28" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">其中符号是标准的，并在论文中规定(索引3是为了与论文一致)。</p><p id="7f61" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">本文中的所有代码都摘自演示笔记本，其中还包括注释和实际的实现细节。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://colab.research.google.com/github/differential-machine-learning/notebooks/blob/master/DifferentialML.ipynb"><div class="gh gi mv"><img src="../Images/9234e7c798f5ae16784a3ba37993592b.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*IkTylN1V6Iv-nDDVIPdbdQ.png"/></div></a></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw lj l"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">标准进口</p></figure><p id="815a" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">下面是前馈方程的TensorFlow (1.x)实现。我们选择显式编写矩阵运算来代替高级Keras层，以突出代码中的等式。我们选择了softplus激活。ELU是另一个选择。出于论文中解释的原因，激活必须是连续可微的，排除了例如RELU和SELU。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw lj l"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">代码中的前馈方程</p></figure><p id="7a84" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">用反向传播预测输出wrt输入的导数。回想反向传播方程是作为前馈方程的<em class="ml">附件</em>推导出来的，或者参见<a class="ae mk" rel="noopener" target="_blank" href="/automatic-differentiation-15min-video-tutorial-with-application-in-machine-learning-and-finance-333e18c0ecbb">我们的教程</a>了解一下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/3389e772a518e8562ec882f96ef8d3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*bqwILu6ZVW9VPF50uYfcMQ.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">反向传播方程</p></figure><p id="f460" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">或者在代码中，回想一下softplus的导数是sigmoid:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw lj l"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">代码中的反向传播方程</p></figure><p id="8883" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我们再次明确地编写了反向传播方程来代替对<em class="ml"> tf.gradients() </em>的调用。我们选择这样做，首先，再次突出代码中的方程，同时避免在训练期间嵌套反向传播层，如下所示。为了避免疑问，通过调用一次<em class="ml"> tf.gradients() </em>来替换这段代码也是可行的。</p><p id="bf2e" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">接下来，我们在一个网络中结合了前馈和反向传播，我们称之为<em class="ml">双网络</em>，这是一个两倍深度的神经网络，能够以两倍的计算成本同时预测值<em class="ml">和导数</em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/1695a76ca5327454ab22521a6bbf4089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*Gf6vpkzxAUpalghZPrmTww.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">孪生网络</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw lj l"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">前馈和反向传播相结合的孪生网络</p></figure><p id="a6e9" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">孪生网络在两个方面是有益的。<em class="ml">训练</em>后，在需要导数预测的应用中，它可以有效地预测给定输入的值和导数。例如，在金融领域，它们是价格对市场状态变量的敏感性，也称为<em class="ml">希腊</em>(因为交易员给它们取希腊字母)，也对应于<em class="ml">对冲比率</em>。</p><p id="dadf" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">双生网络也是<em class="ml">差异训练</em>的基本构造。组合成本函数通过孪生网络的推理、预测值和导数来计算。成本函数的<em class="ml">梯度</em>通过twin网络反向传播来计算，包括由TensorFlow作为其优化循环的一部分静默进行的反向传播部分。回想一下神经网络的标准训练循环:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw lj l"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">普通训练循环</p></figure><p id="1c5c" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">差分训练循环实际上是相同的，对于成本函数的定义是安全的，现在结合了值和导数的均方误差:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw lj l"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">差分训练循环</p></figure><p id="57e3" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">TensorFlow在幕后无缝区分twin网络，以满足优化需求。网络的一部分本身就是反向传播，这无关紧要。这只是矩阵运算的另一个序列，TensorFlow没有困难地进行微分。</p><p id="90f8" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">笔记本的其余部分涉及标准数据准备、训练和测试，以及对金融教科书数据集的应用:Black &amp; Scholes中的欧式看涨期权和correlated Bachelier中的篮子期权。结果证明了差分深度学习的<em class="ml">不合理</em>有效性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ba705bd2ef50f1c4b4f7ec4c17cfaa10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*nATGFYFt0ONZhrbIfp8sDA.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">一些测试结果展示了差分深度学习的强大功能，可在笔记本电脑上重现</p></figure><p id="8e46" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated"><a class="ae mk" href="https://github.com/differential-machine-learning/appendices" rel="noopener ugc nofollow" target="_blank">在在线附录</a>中，我们探索了差分机器学习在其他类型的ML模型中的应用，如基函数回归和主成分分析(PCA)，结果同样显著。</p><p id="592b" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">差分训练对不正确的导数施加惩罚，与常规正则化(如ridge/Tikhonov)倾向于小权重的方式相同。与常规正则化相反，差分ML有效地减轻了过拟合<em class="ml">，而没有引入偏差</em>。因此，不存在偏倚-方差权衡或通过交叉验证调整超参数的必要性。它只是工作。</p><p id="e6f4" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">差分机器学习更类似于<em class="ml">数据扩充</em>，这反过来可以被视为一种更好的正则化形式。数据扩充一直被应用于例如计算机视觉中，并取得了成功。这个想法是从一个单独的图像产生多个标记的图像，例如通过裁剪、缩放、旋转或重新着色。除了以可忽略不计的成本扩展训练集之外，数据扩充教会了ML模型重要的不变性。同样，衍生品标签，不仅以非常小的成本增加了训练集中的信息量(只要它们是用AAD计算的)，而且教会了ML模型定价函数的<em class="ml">形状</em>。</p><p id="46b8" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated"><strong class="lq iu">工作论文</strong>:【https://arxiv.org/abs/2005.02347<br/>T5】Github repo:<a class="ae mk" href="https://github.com/differential-machine-learning" rel="noopener ugc nofollow" target="_blank">github.com/differential-machine-learning</a><br/><strong class="lq iu">Colab笔记本</strong>:<a class="ae mk" href="https://colab.research.google.com/github/differential-machine-learning/notebooks/blob/master/DifferentialML.ipynb" rel="noopener ugc nofollow" target="_blank">https://Colab . research . Google . com/Github/differential-machine-learning/notebooks/blob/master/differential ml . ipynb</a></p><p id="7147" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">安托万·萨维恩</p></div></div>    
</body>
</html>