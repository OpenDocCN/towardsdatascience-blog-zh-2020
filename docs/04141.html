<html>
<head>
<title>Myths and Reality around Correlation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">围绕相关性的神话与现实</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/myths-and-reality-around-correlation-9b359456d8e1?source=collection_archive---------45-----------------------#2020-04-15">https://towardsdatascience.com/myths-and-reality-around-correlation-9b359456d8e1?source=collection_archive---------45-----------------------#2020-04-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fb7c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">正确理解相关性概念的终极指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fe85c0db6916315e56840f14471d62c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jd2cgliwmVv3on82"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片<a class="ae ky" href="https://unsplash.com/@photoholgic" rel="noopener ugc nofollow" target="_blank">霍尔格链接</a>通过上<a class="ae ky" href="https://images.unsplash.com/photo-1531956656798-56686eeef3d4?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1378&amp;q=80" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="d2ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Correlation_and_dependence" rel="noopener ugc nofollow" target="_blank">相关性</a>是两个<a class="ae ky" href="https://en.wikipedia.org/wiki/Random_variable" rel="noopener ugc nofollow" target="_blank">随机变量</a>之间的统计关系。这是两个变量相关的程度。</p><p id="3761" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正相关意味着两个变量同向变动，一个变量上升也会导致另一个变量上升，同样，一个变量下降也会导致另一个变量下降。类似地，当负相关时，变量向相反方向移动。相关性的概念广泛应用于数据科学和其他领域。找到变量之间的相关性是任何探索性数据分析的最基本和最重要的部分。</p><p id="8fe5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" href="https://en.wikipedia.org/wiki/Finance" rel="noopener ugc nofollow" target="_blank">金融</a>部门，相关性用于检查股票相对于市场和其他股票的运动(上涨和下跌)。在<a class="ae ky" href="https://en.wikipedia.org/wiki/Retail" rel="noopener ugc nofollow" target="_blank">零售</a>部门，相关性可用于查看一件商品的销售如何与其他商品的销售相关联，等等。</p><p id="9a0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">检查变量之间的相关性并丢弃高度相关的特征使得模型简单、直观和透明。这有助于消除<a class="ae ky" href="https://en.wikipedia.org/wiki/Multicollinearity" rel="noopener ugc nofollow" target="_blank">多重共线性</a>。</p><p id="aa48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">说够了，言归正传，这个博客是关于什么的。这个博客是这个概念的实践指南。这篇博客也是关于相关性可以揭示什么，以及如何在不落入陷阱的情况下得到正确的结果。让我们从概念上开始深入探讨。</p><h1 id="0b93" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">计算相关性</h1><p id="5aeb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">相关性解释了两个变量之间的依赖程度，即这两个变量是如何相关的。可以计算各种相关系数。一些著名的相关系数是<a class="ae ky" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">皮尔逊相关系数</a>和<a class="ae ky" href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">斯皮尔曼等级相关系数</a>。</p><h2 id="7b86" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">1.皮尔逊相关系数</h2><p id="7914" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">皮尔逊相关系数计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/4ffe522912461589ac77a9497f67cb61.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*BKxjgx9o9hW5WrdL8OiuhA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源~ <a class="ae ky" href="https://wikimedia.org/api/rest_v1/media/math/render/svg/43219265dc2c827cb4f5b34f2e3fb797bed2e820" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="df29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中ρ是随机变量“X”和“Y”之间的相关系数。</p><p id="b694" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">μ是变量的平均值，σ是变量的标准差。</p><p id="0ea3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">E[]是期望值。</p><h2 id="882d" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">2.斯皮尔曼等级相关系数</h2><p id="ca39" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">斯皮尔曼相关系数计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8f6a65b462a7ffcbf22b47d41c748a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*lrxh2VhYi8iXfo0lfN63Gw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源<a class="ae ky" href="https://wikimedia.org/api/rest_v1/media/math/render/svg/c06f5a0bc5c4b924334eeac566fe1a59f9578ff8" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="8ca1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中 X 和 Y 被转换成等级(rg)。剩下的公式与皮尔逊相关系数相同。Spearman 系数情况下的等级排序重新映射了 X 和 Y 的值，使其不同于 Pearson 系数。</p><blockquote class="ng nh ni"><p id="0c8e" class="kz la nj lb b lc ld ju le lf lg jx lh nk lj lk ll nl ln lo lp nm lr ls lt lu im bi translated">关于 Spearman 等级相关系数最好的事情是，人们可以创建他们自己的等级排序函数，根据问题或用例来调整相关系数。</p></blockquote><h1 id="d77c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">围绕相关性的神话与现实</h1><h2 id="74e4" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">1.相关性解释了依赖的程度</h2><p id="3df3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu"> <em class="nj">神话是相关性只能捕捉变量之间的线性关系。事实上，皮尔逊系数捕捉到了线性关系，但其他系数，如斯皮尔逊等级系数，也适用于许多(如果不是全部)非线性关系。</em>T11】</strong></p><p id="7483" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一个例子</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="dafc" class="ms lw it no b gy ns nt l nu nv">y = (4x — 30)²¹</span></pre><p id="9125" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据生成，</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="9046" class="ms lw it no b gy ns nt l nu nv">y = []<br/>from random import randint<br/>x = [randint(1, 100) for i in range(1000)]<br/>for elem in x:<br/>    y.append(math.pow((elem * 4 -30),21))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/6b11b1eef2840b94905f6c68a841e385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dVq8qeT3PExS97Zu1JgODg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">x 和 y 之间的非线性关系</p></figure><p id="3552" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算相关系数，</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="c780" class="ms lw it no b gy ns nt l nu nv">Perason Coefficient Value 0.48<br/>Spearman's Coefficient value 1.0</span></pre><p id="69e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">皮尔逊系数不能捕捉非线性关系，但斯皮尔曼的等级系数可以。</p><h2 id="3954" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">2.一个接近+1 或-1 的相关性，我们能推断出更多什么？</h2><p id="6650" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">皮尔逊相关系数的值介于+1 和 1 之间，其中+1 表示完全正线性相关，0 表示没有线性相关，1 表示完全负线性相关。[[来源<a class="ae ky" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">维基百科</a> ]]</p><p id="da2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设您在数据集中观察到两个或多个变量的皮尔逊相关系数几乎为 1 或-1。这是什么意思？</p><blockquote class="nx"><p id="987f" class="ny nz it bd oa ob oc od oe of og lu dk translated">一个人需要非常清醒，不要落入“<em class="oh">”的陷阱，“相关性意味着因果关系</em>”。事实上，<a class="ae ky" href="https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation" rel="noopener ugc nofollow" target="_blank">关联并不意味着因果关系</a>。但是，在这种情况下，我说的是相关系数几乎为 1 或-1 的特殊情况。而且显然有原因/关系。</p></blockquote><p id="48c0" class="pw-post-body-paragraph kz la it lb b lc oi ju le lf oj jx lh li ok lk ll lm ol lo lp lq om ls lt lu im bi translated">有一篇关于<a class="ae ky" rel="noopener" target="_blank" href="/why-correlation-does-not-imply-causation-5b99790df07e">为什么相关性并不意味着因果关系</a>的文章。可以参考了解更多的话题。</p><div class="on oo gp gr op oq"><a rel="noopener follow" target="_blank" href="/why-correlation-does-not-imply-causation-5b99790df07e"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">为什么相关性并不意味着因果关系？</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">相关性和因果关系是最容易被误解的术语，并且经常互换使用。了解这两者…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe ks oq"/></div></div></a></div><p id="c1ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着两个变量很有可能是从另一个变量派生出来的。神话是，即使在+1/-1 皮尔逊相关系数的情况下，找到公式/关系可能是困难的，因为我们不知道它会有多复杂。相反，事实是，无论线性关系有多复杂，它总是可以简化为 y = Wx + b 的形式，并且我们可以使用线性回归或线性方程求解器以高度的正确性来近似它。T9】</p><p id="14af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一个例子，</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="f1e3" class="ms lw it no b gy ns nt l nu nv">y = (x * 7 - 30)/4 * 8 + 7</span></pre><p id="9743" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">y 是用一些复杂的公式从 x 推导出来的。但是，我们总是可以把它简化成 y = Wx + b 的形式</p><p id="eaf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="a1f4" class="ms lw it no b gy ns nt l nu nv">y = 14x - 53</span></pre><p id="c36f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们首先检查相关系数值的值</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="7c7d" class="ms lw it no b gy ns nt l nu nv">y = []<br/>from random import randint<br/>x = [randint(1, 100) for i in range(1000)]<br/>for elem in x:<br/>    y.append(((elem * 7 - 30)/4) * 8 + 7)</span></pre><p id="6a32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将数据可视化，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/2273fd943d73f7486a912cd452b6acbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2dPLDGYMtI5HhlPMMEjMZA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">x 和 y 之间的线性关系</p></figure><p id="d0ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算相关系数</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="716e" class="ms lw it no b gy ns nt l nu nv">import scipy.stats<br/>pearsonr(x,y),spearmanr(x,y)</span></pre><p id="2320" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果:</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="4e9e" class="ms lw it no b gy ns nt l nu nv">Perason Coefficient Value 1.0<br/>Spearman's Coefficient value 1.0</span></pre><p id="157b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两者，相关系数导致 1</p><p id="0496" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们运行一个回归，并验证假设，如果皮尔逊相关性接近+1 或-1，我们总是可以反向工程找到公式/关系。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="c399" class="ms lw it no b gy ns nt l nu nv">reg = LinearRegression().fit(x,y)<br/>print("Intercept: {} Coefficient: {}".format(reg.intercept_,reg.coef_))<br/>y_pred = reg.predict(x)</span></pre><p id="134a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">导致，</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="581c" class="ms lw it no b gy ns nt l nu nv">Intercept: -53.00 Coefficient: [14.]</span></pre><p id="9c64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这才是真正的关系。</p><h2 id="341b" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">3.如何得到正确的相关系数？</h2><p id="0c23" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu"> <em class="nj">神话是相关系数表示依赖程度。事实上，相关系数只有在计算方法正确的情况下才表示依赖程度。</em>T13】</strong></p><p id="945a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两只股票在一段时间内有很高的正相关性并不一定意味着它们以相似的模式向同一个方向移动。当分析这两只股票在多个时期的市场下跌和上涨，然后找到相关系数，将给出正确的图片。</p><p id="1686" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，两个显示相似或相反销售模式的项目可能只是由于促销或特定季节。在收集多个时间段的销售数据，然后计算相关系数之后，可以确定正确的关系。</p><h1 id="8166" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="fab3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">有很多关于相关性的文章。我试图提出一个不同的观点，揭示这个主题周围较少探索和隐藏的领域。这篇博客也是关于相关性可以揭示什么，以及如何在不落入陷阱的情况下得到正确的结果。希望你喜欢。</p><p id="de8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nj">我的 Youtube 频道获取更多内容:</em> </strong></p><div class="on oo gp gr op oq"><a href="https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">阿布舍克·蒙戈利</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">嗨，伙计们，欢迎来到频道。该频道旨在涵盖各种主题，从机器学习，数据科学…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">www.youtube.com</p></div></div><div class="oz l"><div class="pg l pb pc pd oz pe ks oq"/></div></div></a></div><blockquote class="ng nh ni"><p id="a0aa" class="kz la nj lb b lc ld ju le lf lg jx lh nk lj lk ll nl ln lo lp nm lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">关于作者-: </em> </strong></p><p id="31be" class="kz la nj lb b lc ld ju le lf lg jx lh nk lj lk ll nl ln lo lp nm lr ls lt lu im bi translated">Abhishek Mungoli 是一位经验丰富的数据科学家，拥有 ML 领域的经验和计算机科学背景，跨越多个领域并具有解决问题的思维方式。擅长各种机器学习和零售业特有的优化问题。热衷于大规模实现机器学习模型，并通过博客、讲座、聚会和论文等方式分享知识。</p><p id="712d" class="kz la nj lb b lc ld ju le lf lg jx lh nk lj lk ll nl ln lo lp nm lr ls lt lu im bi translated">我的动机总是把最困难的事情简化成最简单的版本。我喜欢解决问题、数据科学、产品开发和扩展解决方案。我喜欢在闲暇时间探索新的地方和健身。在<a class="ae ky" href="https://medium.com/@mungoliabhishek81" rel="noopener"> <strong class="lb iu">中</strong> </a>、<strong class="lb iu"/><a class="ae ky" href="https://www.linkedin.com/in/abhishek-mungoli-39048355/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">Linkedin</strong></a><strong class="lb iu"/>或<strong class="lb iu"/><a class="ae ky" href="https://www.instagram.com/simplyspartanx/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">insta gram</strong></a><strong class="lb iu"/>关注我，查看我<a class="ae ky" href="https://medium.com/@mungoliabhishek81" rel="noopener">以前的帖子</a>。我欢迎反馈和建设性的批评。我的一些博客-</p></blockquote><ul class=""><li id="e1fd" class="ph pi it lb b lc ld lf lg li pj lm pk lq pl lu pm pn po pp bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/5-mistakes-every-data-scientist-should-avoid-bcc8142d7693">每个数据科学家都应该避免的 5 个错误</a></li><li id="3502" class="ph pi it lb b lc pq lf pr li ps lm pt lq pu lu pm pn po pp bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/analyzing-fitbit-data-to-demystify-bodily-pattern-changes-amid-pandemic-lockdown-5b0188fec0f0">分析 Fitbit 数据，揭开疫情封锁期间身体模式变化的神秘面纱</a></li><li id="f1d5" class="ph pi it lb b lc pq lf pr li ps lm pt lq pu lu pm pn po pp bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/decomposing-a-time-series-in-a-simple-and-intuitive-way-19d3213c420b?source=---------7------------------">以简单&amp;直观的方式分解时间序列</a></li><li id="983d" class="ph pi it lb b lc pq lf pr li ps lm pt lq pu lu pm pn po pp bi translated"><a class="ae ky" href="https://medium.com/walmartlabs/how-gpu-computing-literally-saved-me-at-work-fc1dc70f48b6" rel="noopener">GPU 计算如何在工作中拯救了我？</a></li><li id="126d" class="ph pi it lb b lc pq lf pr li ps lm pt lq pu lu pm pn po pp bi translated">信息论&amp; KL 分歧<a class="ae ky" rel="noopener" target="_blank" href="/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e">第一部分</a>和<a class="ae ky" rel="noopener" target="_blank" href="/part-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d">第二部分</a></li><li id="3be0" class="ph pi it lb b lc pq lf pr li ps lm pt lq pu lu pm pn po pp bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/process-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25">使用 Apache Spark 处理维基百科，创建热点数据集</a></li><li id="1937" class="ph pi it lb b lc pq lf pr li ps lm pt lq pu lu pm pn po pp bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/a-semi-supervised-embedding-based-fuzzy-clustering-b2023c0fde7c">一种基于半监督嵌入的模糊聚类</a></li><li id="6a6b" class="ph pi it lb b lc pq lf pr li ps lm pt lq pu lu pm pn po pp bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/compare-which-machine-learning-model-performs-better-4912b2ed597d">比较哪种机器学习模型表现更好</a></li><li id="21e6" class="ph pi it lb b lc pq lf pr li ps lm pt lq pu lu pm pn po pp bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/identify-your-datas-distribution-d76062fc0802">确定您的数据分布</a></li></ul></div></div>    
</body>
</html>