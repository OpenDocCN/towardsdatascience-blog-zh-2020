<html>
<head>
<title>How Much Of These Machine Learning Terms Can You Recall? (Learning On Medium Gamified)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这些机器学习术语你能回忆起多少？(在媒体上学习游戏化)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-much-of-these-machine-learning-terms-can-you-recall-learning-on-medium-gamified-a280a53a0737?source=collection_archive---------52-----------------------#2020-06-19">https://towardsdatascience.com/how-much-of-these-machine-learning-terms-can-you-recall-learning-on-medium-gamified-a280a53a0737?source=collection_archive---------52-----------------------#2020-06-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ba8e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">挑战你自己，而我试着制造中等的乐趣</h2></div><h1 id="c559" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="523e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">机器学习是一个广泛的领域，包含各种其他子领域，如计算机视觉、自然语言处理、语音识别等等。</p><p id="a1b9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">尽管机器学习有许多分支子领域，但一些关键术语在所有子领域中都是通用的。</p><p id="750e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">本文介绍了一些常见的机器学习术语，并以游戏化的方式对这些术语进行了描述和解释。</p><h2 id="d5d7" class="mb kj it bd kk mc md dn ko me mf dp ks lj mg mh ku ln mi mj kw lr mk ml ky mm bi translated">通过阅读这篇文章，你可以获得以前看不到的机器学习词汇的知识，或者重温你在机器学习实践中可能遇到的术语</h2></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="6fe9" class="ki kj it bd kk kl mu kn ko kp mv kr ks jz mw ka ku kc mx kd kw kf my kg ky kz bi translated"><strong class="ak">游戏规则</strong></h1><ol class=""><li id="9c26" class="mz na it lc b ld le lg lh lj nb ln nc lr nd lv ne nf ng nh bi translated">每一节都以单词开头，供您定义</li><li id="c549" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated">花一两分钟时间，确保你能够回忆起作品的定义</li><li id="2dcc" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated">请随意写下您的描述，以便与我的进行比较</li><li id="5e59" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated">当你写下你对这些术语的定义时，向下滚动页面查看我对这些术语的定义和解释</li><li id="4009" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated">尽可能地学习，甚至在评论区随意添加一些你的定义。</li><li id="aee7" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated">玩得开心</li><li id="cf7b" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated">欣赏来自<a class="ae nn" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>的图片</li></ol></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="8fe2" class="ki kj it bd kk kl mu kn ko kp mv kr ks jz mw ka ku kc mx kd kw kf my kg ky kz bi translated">1.激活功能</h1><p id="d447" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="no">在向下滚动之前，思考或写出你对激活功能的描述。</em></p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi np"><img src="../Images/5ae1dcd92f58f42c9d0cf6aae63195be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oVRr4W9MG80fr044vKPGFw.jpeg"/></div></div><p class="ob oc gj gh gi od oe bd b be z dk translated">照片由<a class="ae nn" href="https://unsplash.com/@chne_?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Tachina Lee </a>在<a class="ae nn" href="/s/photos/thinking?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="b627" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们开始吧……</p><p id="d44f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 3 </em> </strong></p><p id="5d02" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 2 </em> </strong></p><p id="c9c2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 1 </em> </strong></p><p id="4c63" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> … </strong></p><p id="7e65" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">激活函数</strong>是将神经元的结果或信号转换成标准化输出的数学运算。</p><p id="9fc6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">作为神经网络组件的激活函数的目的是在网络中引入非线性。包含激活函数使神经网络具有更大的表示能力和解决复杂的函数。</p><p id="1ce3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">激活功能也被称为“<em class="no">挤压功能</em>”。</p><p id="6da0" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">常见激活功能的示例如下:</p><ul class=""><li id="d3bd" class="mz na it lc b ld lw lg lx lj of ln og lr oh lv oi nf ng nh bi translated">Sigmoid函数</li><li id="4683" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv oi nf ng nh bi translated">热卢</li><li id="dd36" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv oi nf ng nh bi translated">Softmax函数</li></ul><p id="6202" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">记住你的解释不一定要和我写的一样，只要你包括或记住了一些要点。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="1c58" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在，你已经知道如何理解这篇文章了……从某种意义上来说，它就像抽认卡。</p><p id="4102" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="no">下一个。</em></p><h1 id="97a9" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">2.整流线性单位</h1><p id="9a84" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="no">这是你思考或写作的时间，不要太长……</em></p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi oj"><img src="../Images/bd404efff74c61b0672d26f873cd0e13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xXkKZmN0gFN2JUxjcpeIjQ.jpeg"/></div></div><p class="ob oc gj gh gi od oe bd b be z dk translated">由<a class="ae nn" href="https://unsplash.com/@punttim?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">蒂姆·高</a>在<a class="ae nn" href="/s/photos/thinking?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5926" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 3 </em> </strong></p><p id="f52c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 2 </em> </strong></p><p id="2b04" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 1 </em> </strong></p><p id="353c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> … </strong></p><p id="2e06" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">一种转换神经元值结果的激活函数。</p><p id="655c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">ReLU对来自神经元的值施加的变换由公式<strong class="lc iu"> <em class="no"> y=max(0，x) </em> </strong>表示。ReLU激活函数将来自神经元的任何负值钳制为0，而正值保持不变。</p><p id="40f6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这种数学变换的结果被用作当前层的输出，并被用作神经网络内的连续层的输入。</p><p id="f10f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在<em class="no">消失梯度</em>的问题上，ReLU是限制或避免消失梯度对神经网络的影响的标准解决方案。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="20c3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果你得到了前一个任期，那么这应该是在公园散步。</p><h1 id="2351" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">3.Softmax</h1><p id="bda5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果不是，也没关系，花一两分钟考虑一下。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi ok"><img src="../Images/8e933ec624609b933617a3b338138423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dQWYfinE8FzLp0glHZb7hw.jpeg"/></div></div><p class="ob oc gj gh gi od oe bd b be z dk translated"><a class="ae nn" href="https://unsplash.com/@icons8?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Icons8团队</a>在<a class="ae nn" href="/s/photos/time?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="771d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">你现在知道该怎么做了。</p><p id="91ec" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">T37】3T39】</strong></p><p id="67c6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 2 </em> </strong></p><p id="f740" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 1 </em> </strong></p><p id="0ff4" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> … </strong></p><p id="fff6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">一种激活函数，用于导出输入向量中一组数的概率分布。</p><p id="b62e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">softmax激活函数的输出是一个向量，其中它的一组值表示一个类或事件发生的概率。向量中的值加起来都是1。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="7174" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果你喜欢这篇文章的格式，请留下你的评论。</p><h1 id="de51" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">4.Glorot统一初始化器</h1><p id="f304" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="no">这个有点难，放心踏出</em></p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi ol"><img src="../Images/c0d9759472497c6bb58c99ec775ff992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L_gJuNAiKE5rV3c7ZEx0FQ.jpeg"/></div></div><p class="ob oc gj gh gi od oe bd b be z dk translated">本杰明·戴维斯在<a class="ae nn" href="/s/photos/thinking?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="f348" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="no">这么快就回来了…好了，我们走吧</em></p><p id="3553" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 3 </em> </strong></p><p id="bf00" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 2 </em> </strong></p><p id="5683" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 1 </em> </strong></p><p id="d336" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi">…</p><p id="1dbf" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">Glorot统一初始化器是一种神经网络的权重初始化方法，用作解决神经网络内不稳定梯度的解决方案。</p><p id="5617" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">它根据某个<strong class="lc iu">范围</strong>内的值分布初始化网络的权重，平均值评估为零，方差恒定。分布的最大值是范围的正值，最小值是范围的负值。</p><p id="0631" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no">范围=【值，-值】</em> </strong></p><p id="fdf1" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">用于确定分布范围的值来自以下公式:</p><p id="0749" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no">值= sqrt(6/fan _ in+fan _ out)</em></strong></p><p id="498b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="no"> fan_in </em>是输入到层的数目，<em class="no"> fan_out </em>是层内神经元的数目。</p><p id="05b9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">更多关于纸张<a class="ae nn" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">这里</strong> </a> <strong class="lc iu">。</strong></p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="d669" class="ki kj it bd kk kl mu kn ko kp mv kr ks jz mw ka ku kc mx kd kw kf my kg ky kz bi translated">4.损失函数</h1><p id="51b1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">既然我们在中途，这里有一个简单的术语。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi om"><img src="../Images/3e1101dde1e5405096a9bf30c1bbdbea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ceHJjq8wNgI5zUk1u9nTw.jpeg"/></div></div><p class="ob oc gj gh gi od oe bd b be z dk translated">照片由<a class="ae nn" href="https://unsplash.com/@brucemars?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">布鲁斯·马尔斯</a>在<a class="ae nn" href="/s/photos/happy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="50dc" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="no">告诉你这很容易</em></p><p id="9ad2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 3 </em> </strong></p><p id="301c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 2 </em> </strong></p><p id="a304" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 1 </em> </strong></p><p id="1373" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi">…</p><p id="6c13" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">损失函数是一种量化机器学习模型表现得有多好的方法。量化是基于一组输入的输出(成本)，这些输入被称为参数值。参数值用于估计预测，而“损失”是预测值和实际值之间的差异。</p><h1 id="6d41" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">5.优化(神经网络)</h1><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi on"><img src="../Images/9104df8a26cbc138a65bff396afb7d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qfe_Q06GfJCT4msWEoIabA.jpeg"/></div></div><p class="ob oc gj gh gi od oe bd b be z dk translated">马修·施瓦茨在<a class="ae nn" href="/s/photos/thinking?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="c470" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 3 </em> </strong></p><p id="3f5b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 2 </em> </strong></p><p id="98ef" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 1 </em> </strong></p><p id="48b2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi">…</p><p id="6a49" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">神经网络中的<strong class="lc iu">优化器</strong>是一种算法实现，通过最小化经由损失函数提供的损失值来促进神经网络中的梯度下降过程。为了减少损失，适当地选择网络内的权重值是至关重要的。</p><p id="b64d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">优化算法的示例:</p><ul class=""><li id="6681" class="mz na it lc b ld lw lg lx lj of ln og lr oh lv oi nf ng nh bi translated">随机梯度下降</li><li id="2777" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv oi nf ng nh bi translated">小批量梯度下降</li><li id="654a" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv oi nf ng nh bi translated">内斯特罗夫加速梯度</li></ul></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="628c" class="ki kj it bd kk kl mu kn ko kp mv kr ks jz mw ka ku kc mx kd kw kf my kg ky kz bi translated">6.多层感知器(MLP)</h1><p id="345a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="no">这是一个老的学期，让我们看看你是怎么做的……</em></p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi oo"><img src="../Images/ddc21febb6a582188c0f1721145f9b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RoB1hOfs4dLmcyFg36AeUw.jpeg"/></div></div><p class="ob oc gj gh gi od oe bd b be z dk translated">马修·班尼特在<a class="ae nn" href="/s/photos/old?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="eed5" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">多层感知器(MLP)是几层感知器一个接一个地连续堆叠。MLP由一个输入层、一个或多个称为隐藏层的TLU层以及一个称为输出层的最终层组成。</p><p id="058b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 3 </em> </strong></p><p id="368b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 2 </em> </strong></p><p id="aaba" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 1 </em> </strong></p><p id="9e3e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi">…</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="df4d" class="ki kj it bd kk kl mu kn ko kp mv kr ks jz mw ka ku kc mx kd kw kf my kg ky kz bi translated">7.三重威胁</h1><ul class=""><li id="5bff" class="mz na it lc b ld le lg lh lj nb ln nc lr nd lv oi nf ng nh bi translated"><strong class="lc iu">学习率</strong></li><li id="76f6" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv oi nf ng nh bi translated"><strong class="lc iu">学习率计划表</strong></li><li id="c47f" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv oi nf ng nh bi translated"><strong class="lc iu">学习率衰减</strong></li></ul><p id="5a85" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="no">保持冷静，你能行的</em></p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi op"><img src="../Images/78f91fe1af7d8314c84e2e467f072765.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Da-lxDE0P7SW6pWBdy4Wg.jpeg"/></div></div><p class="ob oc gj gh gi od oe bd b be z dk translated">由<a class="ae nn" href="https://unsplash.com/@yogagenapp?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">麦迪逊·拉弗恩</a>在<a class="ae nn" href="/s/photos/thinking?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="6bc4" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="no">记住这都是关于学习的，所以如果你想不起来几个也没关系。</em></p><p id="c0bd" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 3 </em> </strong></p><p id="940a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 2 </em> </strong></p><p id="9556" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 1 </em> </strong></p><p id="6214" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi">…</p><p id="2de9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">学习率</strong>是神经网络不可或缺的组成部分，因为它是一个决定网络权值更新水平的因子值。</p><p id="bc69" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">学习率时间表</strong>:在神经网络的训练过程中可以使用恒定的学习率，但这会增加达到最佳神经网络性能所需的训练量。通过利用学习速率表，我们在训练期间引入学习速率的适时减少或增加，以达到神经网络的最佳训练结果。</p><p id="f9ce" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">学习率衰减:</strong>学习率衰减减少了梯度下降过程中向局部最小值移动的步长的振荡。通过将学习率降低到与训练开始时使用的学习率值相比更小的值，我们可以将网络导向在最小值附近的更小范围内振荡的解。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="a491" class="ki kj it bd kk kl mu kn ko kp mv kr ks jz mw ka ku kc mx kd kw kf my kg ky kz bi translated">8.神经类型转移</h1><p id="1683" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="no">这是最后一个</em></p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi oq"><img src="../Images/c6ea30db76df0f3ff131ae2f4e221674.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RW5oBfcSSKDOOGlBuafy6A.jpeg"/></div></div><p class="ob oc gj gh gi od oe bd b be z dk translated">弗雷德里克·图比尔蒙特在<a class="ae nn" href="/s/photos/end?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="d66b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 3 </em> </strong></p><p id="a90f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 2 </em> </strong></p><p id="ff6e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> <em class="no"> 1 </em> </strong></p><p id="40ab" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi">…</p><p id="c5c0" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">神经风格转移(NST) </strong>是一种涉及利用深度卷积神经网络和算法从一幅图像中提取内容信息并从另一幅参考图像中提取风格信息的技术。在提取样式和内容之后，生成组合图像，其中所得图像的内容和样式源自不同的图像。</p><p id="0d94" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">NST是一种图像风格化的方法，这是一种涉及使用输入参考图像来提供具有从输入图像导出的风格差异的输出图像的过程。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="a093" class="ki kj it bd kk kl mu kn ko kp mv kr ks jz mw ka ku kc mx kd kw kf my kg ky kz bi translated">我希望这篇文章的内容对你有用。</h1><p id="4987" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">要联系我或找到更多类似本文的内容，请执行以下操作:</p><ol class=""><li id="f43a" class="mz na it lc b ld lw lg lx lj of ln og lr oh lv ne nf ng nh bi translated">订阅我的<a class="ae nn" href="https://www.youtube.com/channel/UCNNYpuGCrihz_YsEpZjo8TA" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu"> YouTube频道</strong> </a>即将上线的视频内容<a class="ae nn" href="https://www.youtube.com/channel/UCNNYpuGCrihz_YsEpZjo8TA" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">这里</strong> </a></li><li id="d701" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated">跟我上<a class="ae nn" href="https://medium.com/@richmond.alake" rel="noopener"> <strong class="lc iu">中</strong> </a></li><li id="7e2f" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated">通过<a class="ae nn" href="https://www.linkedin.com/in/richmondalake/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu"> LinkedIn </strong> </a>联系我</li></ol></div></div>    
</body>
</html>