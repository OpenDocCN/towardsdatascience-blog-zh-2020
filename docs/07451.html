<html>
<head>
<title>A modern guide to Spark RDDs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark RDDs的现代指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-modern-guide-to-spark-rdds-725cd7c14059?source=collection_archive---------36-----------------------#2020-06-05">https://towardsdatascience.com/a-modern-guide-to-spark-rdds-725cd7c14059?source=collection_archive---------36-----------------------#2020-06-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/890c3af4830354acfe76a3f09161b4cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vBBDvMrwUuLBHBZGmZke5w.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者照片。2011年春天，西班牙阿尔特亚。</p></figure><div class=""/><div class=""><h2 id="08b6" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">每天都有机会发挥PySpark的全部潜力</h2></div><p id="0cbc" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">网络上充满了Apache Spark教程、备忘单、技巧和诀窍。最近，他们中的大多数人都在关注<a class="ae lt" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark SQL和Dataframes </a>，因为它们提供了一个温和的学习曲线，具有熟悉的SQL语法，而不是旧的<a class="ae lt" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> RDD API </a>所需的陡峭曲线。然而，正是rdd的多功能性和稳定性点燃了2015年Spark的采用，并使其成为分布式数据处理的主导框架。</p><p id="fe54" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">随着您成为Spark的正式用户，主要有三种情况会让您考虑RDD API:</p><ul class=""><li id="955e" class="lu lv ji kz b la lb ld le lg lw lk lx lo ly ls lz ma mb mc bi translated">访问原始非结构化数据源</li><li id="a060" class="lu lv ji kz b la md ld me lg mf lk mg lo mh ls lz ma mb mc bi translated">执行比SQL更适合通用编程语言的数据操作。避免UDF</li><li id="3be3" class="lu lv ji kz b la md ld me lg mf lk mg lo mh ls lz ma mb mc bi translated">并行执行第三方库</li></ul><p id="2848" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这份固执己见的材料涵盖了这种情况以及由此经常产生的问题，因此您可以充分享受PySpark的潜力。</p><h1 id="b95d" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">目录</h1><p id="bb41" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated"><a class="ae lt" href="#c5f3" rel="noopener ugc nofollow">关于样式</a> <br/> — <a class="ae lt" href="#78aa" rel="noopener ugc nofollow">垂直扩展您的函数式python</a><br/>—<a class="ae lt" href="#645c" rel="noopener ugc nofollow">将Spark类型附加到变量名</a></p><p id="a29c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><a class="ae lt" href="#c7c6" rel="noopener ugc nofollow">将数据帧转换成rdd&amp;反之亦然</a> <br/> — <a class="ae lt" href="#e69f" rel="noopener ugc nofollow">数据帧到RDD </a> <br/> — <a class="ae lt" href="#c178" rel="noopener ugc nofollow"> RDD到数据帧</a></p><p id="9243" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><a class="ae lt" href="#c047" rel="noopener ugc nofollow">富于表现力的Python字典</a> <br/> — <a class="ae lt" href="#25aa" rel="noopener ugc nofollow">单行字典转换</a> <br/> — <a class="ae lt" href="#c1df" rel="noopener ugc nofollow"> Python集合&amp;字典不可修改</a></p><p id="3c8e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><a class="ae lt" href="#9638" rel="noopener ugc nofollow">缓存&amp;广播</a> <br/> — <a class="ae lt" href="#2a90" rel="noopener ugc nofollow">缓存RDDs </a> <br/> — <a class="ae lt" href="#7b71" rel="noopener ugc nofollow">不持久化RDDs </a> <br/> — <a class="ae lt" href="#bf34" rel="noopener ugc nofollow">清理整个RDD缓存</a></p><p id="efb3" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><a class="ae lt" href="#4aec" rel="noopener ugc nofollow">Python库的分布式执行</a> <br/> — <a class="ae lt" href="#f7cd" rel="noopener ugc nofollow"> Numpy —一种通用方法</a> <br/> — <a class="ae lt" href="#b154" rel="noopener ugc nofollow"> NLTK —分区设置</a></p><h1 id="c5f3" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">关于风格的一点注记</h1><p id="4720" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">当函数式编程遇到大数据时，代码可读性尤其重要，一方面是因为传统的IDE调试工具是围绕命令式编程设计的，另一方面是因为为了调试目的而运行多个代码既昂贵又耗时。</p><h2 id="78aa" class="nf mj ji bd mk ng nh dn mo ni nj dp ms lg nk nl mu lk nm nn mw lo no np my nq bi translated">垂直发展您的功能Python</h2><p id="03fd" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">火花逻辑被定义为<a class="ae lt" href="https://data-flair.training/blogs/dag-in-apache-spark/" rel="noopener ugc nofollow" target="_blank">Dag</a>。通过在每个<a class="ae lt" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations" rel="noopener ugc nofollow" target="_blank">转换</a>或<a class="ae lt" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions" rel="noopener ugc nofollow" target="_blank">动作</a>之后用<strong class="kz jj">反斜杠“\”</strong>换行，保持代码的简洁以提高可读性。</p><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="645c" class="nf mj ji bd mk ng nh dn mo ni nj dp ms lg nk nl mu lk nm nn mw lo no np my nq bi translated">将火花类型附加到变量名</h2><p id="58f0" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">分布式数据主要属于以下三种类型之一:</p><ul class=""><li id="37db" class="lu lv ji kz b la lb ld le lg lw lk lx lo ly ls lz ma mb mc bi translated"><a class="ae lt" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> RDD </a></li><li id="425c" class="lu lv ji kz b la md ld me lg mf lk mg lo mh ls lz ma mb mc bi translated"><a class="ae lt" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank">数据帧</a></li><li id="c293" class="lu lv ji kz b la md ld me lg mf lk mg lo mh ls lz ma mb mc bi translated"><a class="ae lt" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables" rel="noopener ugc nofollow" target="_blank">广播变量</a></li></ul><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h1 id="c7c6" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">将数据帧转换成rdd，反之亦然</h1><p id="f047" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">数据帧的主干是一个RDD[ <a class="ae lt" href="https://spark.apache.org/docs/2.3.4/api/python/pyspark.sql.html?highlight=row#pyspark.sql.Row" rel="noopener ugc nofollow" target="_blank">行</a> ]，一个Spark类型，其行为非常类似于Python字典。正如您在下面看到的，这个<a class="ae lt" href="https://spark.apache.org/docs/2.3.4/api/python/pyspark.sql.html?highlight=row#pyspark.sql.Row" rel="noopener ugc nofollow" target="_blank">行</a>类型充当了两个API之间的桥梁。</p><h2 id="e69f" class="nf mj ji bd mk ng nh dn mo ni nj dp ms lg nk nl mu lk nm nn mw lo no np my nq bi translated">RDD的数据帧</h2><ul class=""><li id="bd49" class="lu lv ji kz b la na ld nb lg nx lk ny lo nz ls lz ma mb mc bi translated"><strong class="kz jj">方法:</strong> Dataframe - &gt; RDD【排】- &gt; RDD【排】</li></ul><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="c178" class="nf mj ji bd mk ng nh dn mo ni nj dp ms lg nk nl mu lk nm nn mw lo no np my nq bi translated">RDD到数据帧</h2><ul class=""><li id="90c5" class="lu lv ji kz b la na ld nb lg nx lk ny lo nz ls lz ma mb mc bi translated"><strong class="kz jj">方法一:</strong>【字典】- &gt; RDD【排】- &gt; Dataframe</li></ul><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><ul class=""><li id="27f1" class="lu lv ji kz b la lb ld le lg lw lk lx lo ly ls lz ma mb mc bi translated"><strong class="kz jj">方法B: </strong> RDD【元组】- &gt;数据框架</li></ul><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h1 id="c047" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">富有表现力的Python词典</h1><p id="e79f" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">随着RDD上的操作链越来越长，保持代码的可读性和逻辑的可理解性变得越来越困难。Python字典在这方面起着关键作用。与Scala元组相反，它们允许通过名称而不是位置来访问字段，与Scala Case类相反，它们可以在没有外部定义的情况下内联创建。</p><h2 id="25aa" class="nf mj ji bd mk ng nh dn mo ni nj dp ms lg nk nl mu lk nm nn mw lo no np my nq bi translated">单行字典转换</h2><p id="3451" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">Lambda函数在语法上被限制为单个表达式。在需要RDD[dict]转换的常见场景中，考虑这些单行lambdas。注意，<strong class="kz jj"> **old_dict </strong>导致了浅层复制，但是在RDD操作中不需要<a class="ae lt" href="https://docs.python.org/3/library/copy.html" rel="noopener ugc nofollow" target="_blank">深层复制操作</a>，因为PySpark保证了新字典是完全独立的，即<strong class="kz jj">rdd是不可变的</strong>。</p><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="c1df" class="nf mj ji bd mk ng nh dn mo ni nj dp ms lg nk nl mu lk nm nn mw lo no np my nq bi translated">Python集合和字典是不可混淆的类型</h2><p id="260b" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">诸如<a class="ae lt" href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey" rel="noopener ugc nofollow" target="_blank"> groupByKey </a>和<a class="ae lt" href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey" rel="noopener ugc nofollow" target="_blank"> reduceByKey </a>之类的RDD聚合要求密钥为可散列类型，以便进行混洗。因此，避免使用Python字典和集合作为混排键。如果必须的话，考虑使用<strong class="kz jj"> frozensets。</strong></p><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h1 id="9638" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">缓存和广播</h1><p id="f0ca" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">学习如何定制Spark的分布式内存处理，可以为ETL和ML培训作业带来最佳的资源使用。此外，对于<a class="ae lt" href="https://jupyter.org/" rel="noopener ugc nofollow" target="_blank"> Jupyter </a>和<a class="ae lt" href="https://zeppelin.apache.org/" rel="noopener ugc nofollow" target="_blank"> Zeppelin </a>笔记本来说，这也是快速流畅的<a class="ae lt" href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop" rel="noopener ugc nofollow" target="_blank"> REPL </a>体验的关键要素。</p><h2 id="2a90" class="nf mj ji bd mk ng nh dn mo ni nj dp ms lg nk nl mu lk nm nn mw lo no np my nq bi translated">缓存rdd</h2><p id="6635" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">持久化到内存的常见用例见<a class="ae lt" href="http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=cache#pyspark.RDD.cache" rel="noopener ugc nofollow" target="_blank"> RDD.cache </a>，其他存储级别见<a class="ae lt" href="http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=cache#pyspark.RDD.persist" rel="noopener ugc nofollow" target="_blank"> RDD.persist </a>。请注意，下面的<strong class="kz jj">行4 </strong>设置了缓存指令，但只有像<strong class="kz jj">行6 </strong>这样的动作才会触发DAG执行和随后的内存存储。</p><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="7b71" class="nf mj ji bd mk ng nh dn mo ni nj dp ms lg nk nl mu lk nm nn mw lo no np my nq bi translated">不持久rdd</h2><p id="fe59" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">调用<a class="ae lt" href="http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=cache#pyspark.RDD.unpersist" rel="noopener ugc nofollow" target="_blank"> RDD.unpersist </a>并从内存和磁盘中移除其所有块主要有两个原因:</p><ol class=""><li id="f06d" class="lu lv ji kz b la lb ld le lg lw lk lx lo ly ls oa ma mb mc bi translated">你已经用完了RDD。依赖于RDD的所有<a class="ae lt" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions" rel="noopener ugc nofollow" target="_blank">操作</a>都已经执行，您想要释放存储空间，以用于管道或ETL作业中的后续步骤。</li><li id="b5ec" class="lu lv ji kz b la md ld me lg mf lk mg lo mh ls oa ma mb mc bi translated">你想<em class="ob">修改</em>持久化的RDD，这是在Jupyter/Zeppelin笔记本上工作时的一个常见用例。鉴于RDD是不可变的，您可以做的是重用RDD名称来指向新的RDD。因此，如果上面的代码运行两次，您将在同一个<strong class="kz jj"> cached_rdd </strong>的内存中得到两次分配。第一次运行后，<strong class="kz jj"> cached_rdd </strong>将指向下面的第一个分配，然后指向第二个分配，使第一个分配成为孤儿。</li></ol><figure class="nr ns nt nu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/31677b70e8990153d8a0e2000c714653.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GsgdVkO5btLcEXSZQ4_JNQ.png"/></div></div></figure><p id="955b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了避免重复的内存分配，在初始化RDD的Jupyter段落前添加一个<strong class="kz jj"> try-unpersist </strong>(下面的第1–4行):</p><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="bf34" class="nf mj ji bd mk ng nh dn mo ni nj dp ms lg nk nl mu lk nm nn mw lo no np my nq bi translated">清理RDD缓存</h2><p id="a614" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">取消所有rdd的持久性可以通过以下方式实现:</p><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h1 id="4aec" class="mi mj ji bd mk ml mm mn mo mp mq mr ms ko mt kp mu kr mv ks mw ku mx kv my mz bi translated">Python库的分布式执行</h1><p id="685b" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">RDDs的灵活性允许在运行几乎任何Python代码时分发有效负载。对于计算成本较低的任务，如<strong class="kz jj"> O(n) </strong>及以下任务，需要真正的大数据才能让并行化的优势显而易见。然而，对于线性以上的复杂性，并行化可以轻松地将中型数据作业的几小时变成几分钟，或者将小型数据作业的几分钟变成几秒钟。</p><h2 id="f7cd" class="nf mj ji bd mk ng nh dn mo ni nj dp ms lg nk nl mu lk nm nn mw lo no np my nq bi translated">Numpy —一种通用方法</h2><p id="0a31" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">为了并行执行库，如<a class="ae lt" href="https://www.statsmodels.org/" rel="noopener ugc nofollow" target="_blank"> statsmodels </a>、<a class="ae lt" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>、<a class="ae lt" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> numpy </a>，只需从一个<strong class="kz jj">映射</strong>、<strong class="kz jj">平面映射</strong>、<strong class="kz jj">过滤器</strong>或任何其他<a class="ae lt" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations" rel="noopener ugc nofollow" target="_blank">转换</a>内部调用它们。在下面的例子中，<strong class="kz jj"> np.median </strong>调用在RDD映射中，因此，它将在每个Spark执行器中本地运行:</p><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="b154" class="nf mj ji bd mk ng nh dn mo ni nj dp ms lg nk nl mu lk nm nn mw lo no np my nq bi translated">NLTK —分区设置</h2><p id="2e71" class="pw-post-body-paragraph kx ky ji kz b la na kj lc ld nb km lf lg nc li lj lk nd lm ln lo ne lq lr ls im bi translated">除了导入之外，自然语言工具包(<a class="ae lt" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK </a>)库还需要进一步设置。下面例子中的<a class="ae lt" href="https://www.nltk.org/api/nltk.html#nltk.downloader.download" rel="noopener ugc nofollow" target="_blank"> nltk.download </a>调用必须在<strong class="kz jj">的每个执行器</strong>中运行，以保证本地nltk数据的可用性。在这种情况下，考虑使用<a class="ae lt" href="https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=mappartitions#pyspark.RDD.mapPartitions" rel="noopener ugc nofollow" target="_blank"> RDD.mapPartitions </a>来避免在同一个执行器中对nltk.download的多余调用。RDD映射分区调用允许对每个分区的RDD条目的整个列表进行操作，而RDD映射/平面映射/过滤器对每个RDD条目进行操作，并且不提供该条目属于哪个分区的可见性:</p><figure class="nr ns nt nu gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><p id="d099" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj"> …更多章节即将推出。</strong></p></div></div>    
</body>
</html>