<html>
<head>
<title>Will Deep Learning Hit the Wall?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习会碰壁吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/will-deep-learning-hit-the-wall-6017bed1f62b?source=collection_archive---------49-----------------------#2020-07-30">https://towardsdatascience.com/will-deep-learning-hit-the-wall-6017bed1f62b?source=collection_archive---------49-----------------------#2020-07-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4a1e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">更好的算法还是更强的计算能力？</h2></div><p id="27c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你对深度学习感兴趣，那么你可能已经听说过美国、韩国和巴西大学和实验室的研究人员最近发表的<a class="ae lb" href="https://arxiv.org/pdf/2007.05558.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><blockquote class="lc ld le"><p id="1633" class="kf kg lf kh b ki kj jr kk kl km ju kn lg kp kq kr lh kt ku kv li kx ky kz la ij bi translated"><strong class="kh ir"> Neil C. Thompson </strong>，麻省理工学院计算机科学和人工智能实验室，<strong class="kh ir"> Kristjan Greenewald </strong>，麻省理工学院数字经济倡议，<strong class="kh ir"> Keeheon Lee </strong>，首尔延世大学安德伍德国际学院，<strong class="kh ir"> Gabriel F. Manso，</strong>巴西利亚大学 FGA 分校。</p></blockquote><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/e42150e542a5df83411814fe2c1df80c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vIzAZ7xnAChLzWC1"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">安迪·凯利在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="6b36" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在他们的研究中，他们分析了图像分类、对象检测、问题回答、命名实体识别和机器翻译等领域的 1000 多篇研究论文，发现深度学习性能的进步在很大程度上是基于计算能力的提高。</p><p id="d2db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一般来说，每个计算机领域的进步都可以通过两种主要方式来实现:</p><ul class=""><li id="842a" class="lz ma iq kh b ki kj kl km ko mb ks mc kw md la me mf mg mh bi translated">要么提供更高的计算能力，这不仅意味着更快的 CPU 或更多的节点，还意味着更多的内存和存储</li><li id="40a4" class="lz ma iq kh b ki mi kl mj ko mk ks ml kw mm la me mf mg mh bi translated">或者通过研究新的算法和方法</li></ul><p id="3863" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，研究人员发现，在上述领域取得的重大进展是由于计算能力的提高，而不是新算法的创造和采用。简而言之——<strong class="kh ir">去年深度学习的许多成就都是因为计算机变得更快了</strong>，现在可以比以前更快地执行相同的旧算法。</p><p id="1476" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很糟糕吗？不一定。计算能力的增长本身是中性的，没有好坏之分，这只是世界不得不接受的事实——计算能力总是随着时间的推移而增长，如果你环顾四周，你会发现它在许多(如果不是所有)领域都有更好的性能。</p><p id="93b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是非常直观的——如果在计算机上执行某个任务，它可以用更快的 CPU 更快地执行<em class="lf"/>,或者通过允许任务用更快的 CPU 或更大的内存处理更多数据来产生更好的结果<em class="lf"/>。</p><blockquote class="lc ld le"><p id="dc07" class="kf kg lf kh b ki kj jr kk kl km ju kn lg kp kq kr lh kt ku kv li kx ky kz la ij bi translated">正如研究中提到的那样，机器学习的计算成本总是很高。</p></blockquote><p id="3645" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">毕竟，没有一项研究表明计算能力是推动机器学习的唯一因素。但是研究发现了两个有趣的点:</p><ol class=""><li id="513c" class="lz ma iq kh b ki kj kl km ko mb ks mc kw md la mn mf mg mh bi translated">深度学习模型的实际计算负担比理论上的(已知)下限增长得更快，这表明实质性的改进是可能的</li><li id="bd5a" class="lz ma iq kh b ki mi kl mj ko mk ks ml kw mm la mn mf mg mh bi translated"><em class="lf">“如果按照目前的路线发展下去，这些计算需求将很快在技术上和经济上变得令人望而却步。”</em></li></ol><p id="1a89" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，对于这两种观点，我们都可以找到相反的论据。也就是说，如果深度学习模型的扩展速度比理论下限更快，那么我们就可以假设理论还不够精确。</p><p id="ff67" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们对计算要求的技术和经济界限的估计是基于我们对目前用于生产计算资源的技术方法的当前知识、当前生产和拥有成本以及根据我们当前的理解和知识所做的预测。</p><p id="fdda" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，也许我们不应该争论研究的结论，我们应该考虑另一点:<strong class="kh ir">如果我们可以发现新的或改进现有的算法，那么我们就可以大大提高结果的质量</strong>，无论是什么——分类，物体检测，机器翻译，等等。</p><blockquote class="lc ld le"><p id="7d5e" class="kf kg lf kh b ki kj jr kk kl km ju kn lg kp kq kr lh kt ku kv li kx ky kz la ij bi translated">当我看到 GAN 的实验和过去几年我们看到的架构改进时，我可以将其视为该领域的一个很好的例子，新方法产生了辉煌的结果。</p></blockquote><p id="dcec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">据研究人员估计，<strong class="kh ir">三年的算法改进相当于计算能力提高 10 倍。</strong></p><p id="8360" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但正如研究中也提到的，有时新的改进算法本身需要更多的计算能力。你知道，有些算法比其他算法更需要资源。这可能是问题本身，尽管是暂时的——可能我们只是需要获得更多的计算能力来尝试新的训练方法，然后运行模型。</p><p id="3cb7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计算能力的增长是我们在过去几年中所拥有的，并且有望在未来实现，这意味着我们肯定会看到机器学习的改进。但是，这只是因为计算能力的增加而稳步提高，还是因为算法的改进而显著提高？</p><p id="8643" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我个人希望最后。</p><h1 id="b99d" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">资源</h1><p id="901b" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">Neil C. Thompson，Kristjan Greenewald，Keeheon Lee，Gabriel F. Manso</p></div></div>    
</body>
</html>