<html>
<head>
<title>K Means Clustering with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k 表示使用 Python 进行集群</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-with-python-66288925e5f6?source=collection_archive---------30-----------------------#2020-07-24">https://towardsdatascience.com/k-means-clustering-with-python-66288925e5f6?source=collection_archive---------30-----------------------#2020-07-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a3aa" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本文中，我们将了解 K 均值聚类的基础知识，并使用著名的机器学习库 Scikit-learn 在 Python 中实现它</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a980ba702b8abb0012f0685c5207f829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KZyTvZCFML4BJr3D"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">尼克尼斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="46ad" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是 K 均值聚类？</h1><p id="08d6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu"> K 均值聚类</strong>是一种<strong class="lt iu">无监督机器学习算法</strong>。它接收<strong class="lt iu">混合数据</strong>并且<strong class="lt iu">基于<strong class="lt iu">数据</strong>中的<strong class="lt iu">模式</strong>将</strong>数据分成小的<strong class="lt iu">组/簇</strong>。</p><h1 id="228a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">K 均值算法的目标</h1><p id="fef1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://www.education-ecosystem.com/andreybu/REaxr-machine-learning-model-python-sklearn-kera/oPGdP-machine-learning-model-python-sklearn-kera/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">奥德里布</strong> </a>曾经说过:</p><blockquote class="mn mo mp"><p id="3cd2" class="lr ls mq lt b lu mr ju lw lx ms jx lz mt mu mc md mv mw mg mh mx my mk ml mm im bi translated">K-means 的目标很简单:将相似的数据点组合在一起，发现潜在的模式。为了实现这个目标，K-means 在数据集中寻找固定数量的聚类(<em class="it"> k </em>)。</p></blockquote><h1 id="5764" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">K 均值聚类的工作原理</h1><p id="5d45" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了解释<strong class="lt iu"> K 均值算法</strong>的工作原理，让我们假设我们有一些使用<strong class="lt iu">散点图</strong>绘制出来的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/fef97787622f949c59f4724da94e2dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*9mE_PEug-s1jX5dDXshP7w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据绘制在散点图上，照片由作者提供</p></figure><p id="3cfd" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">现在在<strong class="lt iu">稍微分析</strong>数据之后，我们可以看到我们的数据可以分成两个独立的<strong class="lt iu">组/簇</strong>。现在，<strong class="lt iu"> K 表示</strong>将要做的是，它还会<strong class="lt iu">将<strong class="lt iu">数据</strong>划分为<strong class="lt iu">两个簇</strong>，并为每个<strong class="lt iu">簇标记一些<strong class="lt iu">边界</strong>。</strong>因此，每当一些新的<strong class="lt iu">数据</strong>被馈送到<strong class="lt iu">模型</strong>时，它将检查这个<strong class="lt iu">数据点</strong>落入了什么<strong class="lt iu">边界</strong>，并在最后告诉我们<strong class="lt iu">集群的名称</strong>或<strong class="lt iu">编号。</strong></strong></p><h1 id="985f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">k 均值聚类算法讲解</h1><p id="709f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">首先我们会选择<strong class="lt iu">簇数(k) </strong>(本例中<strong class="lt iu"> k=2 </strong>)。这意味着现在我们将随机假设<strong class="lt iu"> 2 个点</strong>，它们将作为我们的<strong class="lt iu">簇质心</strong>。(<strong class="lt iu">簇形心</strong>是<strong class="lt iu">簇</strong>的<strong class="lt iu">中心点</strong>)。</p><p id="d774" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated"><strong class="lt iu"> K 均值聚类</strong>有两个主要步骤:</p><ul class=""><li id="a91e" class="na nb it lt b lu mr lx ms ma nc me nd mi ne mm nf ng nh ni bi translated"><strong class="lt iu">聚类分配步骤</strong>:在该步骤中，靠近质心的数据点将分别落在那些质心聚类中。</li><li id="bb50" class="na nb it lt b lu nj lx nk ma nl me nm mi nn mm nf ng nh ni bi translated"><strong class="lt iu">移动质心步骤</strong>:在这一步中，我们将计算一个聚类中所有数据点的平均值，并将该聚类的质心移动到该平均值位置。</li></ul><p id="23ec" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">一旦后的<strong class="lt iu">条件之一<strong class="lt iu">为真:</strong>，我们将<strong class="lt iu">重复上述<strong class="lt iu">两步</strong></strong></strong></p><ol class=""><li id="359d" class="na nb it lt b lu mr lx ms ma nc me nd mi ne mm no ng nh ni bi translated">我们的质心停止改变位置。</li><li id="0c0f" class="na nb it lt b lu nj lx nk ma nl me nm mi nn mm no ng nh ni bi translated">达到最大迭代次数。</li></ol><p id="32ce" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">我们的<strong class="lt iu">数据</strong>现在被安排到<strong class="lt iu">簇中。</strong></p><h1 id="8a16" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">选择 K 的值</h1><p id="3f76" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了选择最适合我们数据的<strong class="lt iu">簇数</strong>，我们可以使用著名的<strong class="lt iu">肘法</strong>。这种<strong class="lt iu">肘方法</strong>背后的基本思想是，它用<strong class="lt iu">变化的集群数量(k) </strong>来绘制<strong class="lt iu">成本(误差)</strong>的各种值。随着<strong class="lt iu">组数(k) </strong> <strong class="lt iu">增加</strong>，每个组的<strong class="lt iu">数据点数</strong> <strong class="lt iu">减少</strong>。因此<strong class="lt iu">平均失真减少</strong>。<strong class="lt iu">小于</strong>一个<strong class="lt iu">簇</strong>中的<strong class="lt iu">个数的数据点</strong>，这些<strong class="lt iu">数据点</strong>离它们的<strong class="lt iu">质心</strong>越近<strong class="lt iu">。所以<strong class="lt iu"> k </strong>处<strong class="lt iu">畸变下降</strong>最大的值称为<strong class="lt iu">拐点。</strong></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/b55a918bc03dc7f8bcee30c863db540d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KQum0WbQLVynFhPFPfbRCw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">选择合适的<strong class="bd nq"> k </strong>值，作者照片</p></figure><p id="e75f" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">有时我们图的<strong class="lt iu">斜率</strong>是<strong class="lt iu">相当平滑</strong>所以<strong class="lt iu">很难</strong>选择<strong class="lt iu"> k </strong>的值，因为没有明确的<strong class="lt iu">拐点。</strong>在这种情况下，我们利用我们的<strong class="lt iu">行业经验</strong>和<strong class="lt iu">连续实验</strong>来确定 k 的<strong class="lt iu">值。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ba4d486b15b97d2ed87b9043392ced27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*4eTIrML8wKeusOpUXJ7aCQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">没有清晰的<strong class="bd nq">肘点</strong>，作者照片</p></figure></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h1 id="a88b" class="kz la it bd lb lc nz le lf lg oa li lj jz ob ka ll kc oc kd ln kf od kg lp lq bi translated">用 Python 实现</h1><p id="6524" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在我们已经很好地理解了<strong class="lt iu"> K 意味着聚类算法</strong>是如何工作的。所以现在我们将在一个数据集<strong class="lt iu">上实现<strong class="lt iu"> K Means </strong>以获得关于它的更清晰的<strong class="lt iu">直觉</strong>。为此，我们将使用<strong class="lt iu"> Python 的</strong>著名的<strong class="lt iu">机器学习库</strong>、<strong class="lt iu"> Scikit-learn。</strong></strong></p><h1 id="4ee8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是 Scikit-learn？</h1><p id="2e43" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu"> Scikit-learn </strong>(又名<strong class="lt iu"> sklearn </strong>)是一个针对 Python 的机器学习库。它包括各种<strong class="lt iu">分类</strong>、<strong class="lt iu">回归、</strong>和<strong class="lt iu">聚类算法</strong>以及<strong class="lt iu">支持向量机(SVM) </strong>、<strong class="lt iu">随机森林</strong>、<strong class="lt iu">梯度提升</strong>、<strong class="lt iu">、<em class="mq"> k </em> -means </strong>和<strong class="lt iu"> DBSCAN </strong>，并且被设计为与像<strong class="lt iu"> NumPy </strong>、<strong class="lt iu">这样的 Python 库一起工作</strong></p><h1 id="ee9b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">k 表示使用 Scikit-learn 进行聚类</h1><p id="3e61" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu"> K 表示聚类</strong>是一种非常直接且易于使用的<strong class="lt iu">算法</strong>。特别是在这个<strong class="lt iu"> Scikit learn </strong>库的帮助下，它的实现和使用变得相当容易。现在，我们开始使用<strong class="lt iu"> Sklearn </strong>。</p><p id="918d" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated"><strong class="lt iu"> <em class="mq">在 Python 中导入重要库</em> </strong></p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="1f29" class="oj la it of b gy ok ol l om on">import seaborn as sns<br/>import matplotlib.pyplot as plt</span></pre><p id="2595" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated"><strong class="lt iu"> <em class="mq">创建人工数据</em> </strong></p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="37a9" class="oj la it of b gy ok ol l om on">from sklearn.datasets import make_blobs<br/>data = make_blobs(n_samples=200, n_features=2,centers=4, cluster_std=1.8,random_state=101)</span></pre><p id="8015" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated"><strong class="lt iu"> <em class="mq">可视化我们的数据</em> </strong></p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="4b7d" class="oj la it of b gy ok ol l om on">plt.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='rainbow')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/fef97787622f949c59f4724da94e2dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*9mE_PEug-s1jX5dDXshP7w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">可视化我们的数据，作者照片</p></figure><p id="6662" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated"><strong class="lt iu"> <em class="mq">创建集群</em> </strong></p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="3eef" class="oj la it of b gy ok ol l om on">from sklearn.cluster import KMeans<br/>kmeans = KMeans(n_clusters=4)<br/>kmeans.fit(data[0])</span></pre><p id="717b" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">现在，在从<code class="fe oo op oq of b">sklearn.cluster</code>导入了<code class="fe oo op oq of b">KMeans </code>之后，我们创建了一个<code class="fe oo op oq of b">KMeans</code>类的对象<code class="fe oo op oq of b">kmeans</code>。在这里，你可能会发现<strong class="lt iu">奇怪</strong>的一点是我们指定了<code class="fe oo op oq of b">n_clusters=4</code>。这是因为在创建<strong class="lt iu">数据</strong>时，我们指定了<code class="fe oo op oq of b">centers=4</code>，所以我们知道该数据应该有<strong class="lt iu"> 4 个簇</strong>。所以我们手动指定了它。但是如果我们不知道这些中心，那么我们将不得不使用<strong class="lt iu">肘方法</strong>来确定正确的星团数量。</p><p id="ae4b" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">好了，现在继续向前，我们的代码<strong class="lt iu">的这<code class="fe oo op oq of b">kmeans.fit(data[0])</code>段分析数据</strong>，<strong class="lt iu">使集群、</strong>甚至<strong class="lt iu">将每个<strong class="lt iu">集群</strong>的质心</strong>匹配到它们的<strong class="lt iu">适当位置</strong>。</p><p id="be27" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">现在为了检查我们的<strong class="lt iu">质心</strong>的<strong class="lt iu">位置</strong>，我们可以使用下面的代码。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="cf87" class="oj la it of b gy ok ol l om on">print(kmeans.cluster_centers_)</span></pre><p id="fdf5" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated">它会打印出一个<strong class="lt iu"> (4，2) </strong> <strong class="lt iu">数组</strong>，分别显示每个<strong class="lt iu">簇</strong>的<strong class="lt iu">质心</strong>的位置。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/295dc2e8dc1ad2300c56a6dd8cb8979d.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*mqGOTt3MFD5yv3xX556b5A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个聚类的质心值，照片由作者提供</p></figure><p id="1439" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated"><strong class="lt iu"> <em class="mq">应用 K 后对比原始数据集 VS 表示</em> </strong></p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="dabd" class="oj la it of b gy ok ol l om on">f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,6))<br/>ax1.set_title('K Means')<br/>ax1.scatter(data[0][:,0],data[0][:,1],c=kmeans.labels_,cmap='rainbow')<br/>ax2.set_title("Original")<br/>ax2.scatter(data[0][:,0],data[0][:,1])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/c73cbfcf9b412681bca97d93ae201f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*oTNVDgp0QiDW1cmh1Hv9Lw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ot">对比</em> <strong class="bd nq"> <em class="ot">原始数据集</em> </strong> <em class="ot"> VS 应用</em> <strong class="bd nq"> <em class="ot"> K 表示</em> </strong> <em class="ot">，作者照片</em></p></figure><p id="e1d5" class="pw-post-body-paragraph lr ls it lt b lu mr ju lw lx ms jx lz ma mu mc md me mw mg mh mi my mk ml mm im bi translated"><strong class="lt iu">恭喜恭喜！</strong>我们已经在我们的<strong class="lt iu">数据集</strong>上成功实现了<strong class="lt iu"> K 均值聚类</strong>。</p><h1 id="e5aa" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">学习成果</strong></h1><p id="c76e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">到目前为止，我们已经了解了什么是<strong class="lt iu"> K 均值聚类算法</strong>，它的<strong class="lt iu">工作</strong>，以及<strong class="lt iu">如何选择 K 的值</strong>。另外，我们已经使用<strong class="lt iu"> Python 的</strong>著名的<strong class="lt iu">机器学习库</strong>，即<strong class="lt iu"> Scikit-learn，在<strong class="lt iu">数据集</strong>上实现了<strong class="lt iu"> K Means </strong>。</strong></p></div></div>    
</body>
</html>