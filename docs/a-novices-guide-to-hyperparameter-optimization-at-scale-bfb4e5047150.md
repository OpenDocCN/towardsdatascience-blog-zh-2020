# 大规模超参数优化新手指南

> 原文：<https://towardsdatascience.com/a-novices-guide-to-hyperparameter-optimization-at-scale-bfb4e5047150?source=collection_archive---------56----------------------->

## 可扩展 HPO 策略综述与研究

![](img/10bad63348d3359dea5b387a0cda3baf.png)

*作者图片*

尽管机器学习(ML)取得了巨大的成功，但现代算法仍然依赖于各种免费的不可训练的超参数。最终，我们选择质量超参数的能力决定了给定模型的性能。在过去，甚至现在，超参数都是通过反复试验手动选择的。整个领域都致力于改进这一选择过程；这被称为超参数优化(HPO)。本质上，HPO 需要测试许多不同的超参数配置，因此可以从大规模并行资源中受益匪浅，如我们正在国家能源研究科学计算中心( [NERSC](https://www.nersc.gov) )建造的[佩尔穆特系统](https://www.nersc.gov/systems/perlmutter/)。当我们为 Perlmutter 做准备时，我们希望探索存在于感兴趣的模型上的众多 HPO 框架和策略。本文就是这种探索的产物，旨在根据我最近的经验和结果，介绍 HPO 的方法，并提供大规模运行 HPO 的指导。

免责声明；这篇文章包含了大量关于 HPO 的一般非软件特定信息，但是对适用于我们在 NERSC 的系统的自由开源软件有偏见。

# 在本文中，我们将涵盖…

*   [带射线调谐的可伸缩 HPO](#1139)
*   [调度程序 vs 搜索算法](#f624)
*   [并非所有超参数都可以被同等对待](#0357)
*   [求解时间研究](#ae69)
*   [PBT 最优调度](#a8c6)
*   [选择 HPO 策略的备忘单](#e813)
*   [技术提示](#3de1) —雷调、蜻蜓、思乐姆、TB、W & B
*   [关键要点](#1522)

# 带光线调节的可伸缩 HPO

能够利用现代计算资源的能力来大规模运行 HPO 对于高效搜索超参数空间非常重要，尤其是在深度学习(DL)时代，神经网络的规模不断增加。对我们所有人来说幸运的是， [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) 的人让可扩展 HPO 变得简单了。下图是在 NERSC 上运行光线调节的一般程序。 [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) 是基于 Ray 构建的分布式 HPO 的开源 python 库。Ray Tune 的一些亮点:

*   支持任何 ML 框架
*   根据可用资源在内部处理作业调度
*   集成外部优化包(如 Ax、蜻蜓、HyperOpt、SigOpt)
*   实施最先进的调度程序(例如 ASHA、AHB、PBT)

我很喜欢使用 [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) ，但是如果你选择不同的 HPO 框架，不用担心，本文中仍然有大量的通用信息。

![](img/8d66f4051e7116675d49dc89a971ed04.png)

*作者图片*

# 调度程序与搜索算法

关于 HPO 策略，我想指出的第一个区别是调度程序和搜索算法之间的区别。搜索算法控制超参数空间的采样和优化方式(如随机搜索)。从实践的角度来看，搜索算法提供了一种选择超参数配置(即试验)进行测试的机制。HPO 总是需要一个搜索算法。或者，调度程序通过提前终止没有希望的尝试来提高 HPO 的整体效率。例如，如果我使用随机搜索，一些试验预计会表现不佳，因此能够提前终止这些试验就好了，这样可以节省宝贵的计算资源。这就是调度程序的工作。对于 HPO 来说，调度程序并不是必需的，但是它们可以极大地提高性能。

下面是我在本文中研究的调度程序和搜索算法的简要描述和参考。

## 异步连续减半算法(ASHA —调度程序)

首先，我想定义连续减半算法(SHA)，而不是我自己做，我真的很喜欢这篇[论文](https://arxiv.org/pdf/1810.05934.pdf)中给出的定义——如果你感兴趣，他们也有 SHA 和 ASHA 的伪代码。

> *SHA 背后的思想很简单:为每个配置分配一个小的预算，评估所有配置并保留前 1/η，将每个配置的预算增加η倍，并重复直到达到每个配置的最大预算 R。*

![](img/373c9f30fb83025478cef4cfc1b95274.png)

*作者图片—* 改编自汽车[帖子](https://www.automl.org/blog_bohb/)

SHA 不能很好地并行化，因为在选择前 1/η之前，需要对所有配置进行短时间评估。这在每个梯级处产生了瓶颈(每个连续的减半被称为一个梯级)。ASHA 将试验推进和梯级完成分离，这样试验可以在任何给定时间推进到下一个梯级。如果一个试用无法升级，可以将其他试用添加到基本等级，这样就可以进行更多的升级。

SHA 和 ASHA 的一个主要假设是，如果一项试验在最初的短时间间隔内表现良好，它将在更长的时间间隔内表现良好。这种假设可以打破的一个经典例子是调整学习率。较大的学习速率可能在短时间内胜过较小的学习速率，导致较小的学习速率试验被错误地终止。实际上，我真的不确定这有多重要。

## 异步超高频带(AHB —调度程序)

[Hyperband](https://arxiv.org/pdf/1603.06560.pdf) (HB)是一个调度器，旨在减轻 SHA 对初始性能的偏见。HB 本质上以各种减半率在 SHA 上循环，试图平衡提前终止与每次试验提供更多资源，而不管初始性能如何。SHA 的每个循环都被视为一个支架，它可以有多个梯级。见下图。除了在 ASHA 上空盘旋之外，AHB 与 HB 完全相同。射线调谐中使用的 AHB 和 ASHA 实现在本[论文](https://arxiv.org/pdf/1810.05934.pdf)中描述。

![](img/a9f2323a3b59259e488c92615bff9fb2.png)

*作者图片*

## 基于人群的培训(PBT —混合)

我称 PBT 为混合型，因为它同时具有调度器和搜索算法的特点。它也可以作为 HPO 战略和教练的功能。更多的是，不是所有的超参数都在同一个部分。在高层次上，PBT 类似于遗传算法。存在工人群体，其中每个工人被分配超参数的随机配置(试验)，并且在设定的间隔超参数配置被群体中表现更好的工人替换(开发)和随机扰动(探索)。用户可以设置开发和探索之间的平衡。这里有一些资源可以让你了解更多，[博客](https://deepmind.com/blog/article/population-based-training-neural-networks)和[论文](https://arxiv.org/pdf/1711.09846.pdf)。

![](img/4279256b9ea822d99b423011e27c1415.png)

*作者图片*

## 随机搜索(RS-搜索算法)

当感兴趣的超参数空间相当大，对于网格搜索来说太大时，默认算法是随机搜索。听起来确实如此，超参数配置或试验是从搜索空间中随机选择的。如果有足够计算时间，RS 工作得相当好。

## 贝叶斯优化(BO-搜索算法)

BO 提供了一种算法方法来确定最佳超参数，而不是随机搜索。因为目标函数在 HPO 是未知的，所以像 BO 这样的黑盒优化器是必要的。在阿伯替代模型中，在这种情况下，目标函数和采集函数用于采样新点或新的超参数配置。高斯过程通常用作 HPO 业务对象的替代模型。理想情况下，BO 可以比随机搜索更有效地收敛到最优超参数。

# 并非所有的超参数都可以被同等对待

在 ML 中有两种主要类型的超参数，它们决定了什么样的 HPO 策略是可能的。

**模型超参数:**建立模型架构

*   卷积层数
*   完全连接的层数
*   等等。

**算法超参数:**参与学习过程

*   学习率
*   批量
*   动力
*   等等。

重要的一点是，并非所有的 HPO 策略都能处理模型和算法超参数。PBT 就是一个很好的例子。PBT 被设计成从群体中的其他高绩效工人进化和继承超参数；然而，如果员工有不同的网络架构，我不清楚这到底是如何工作的。使用 PBT 可能有一种方法可以做到这一点，但它不是标准的，并且不能与 Ray Tune 一起开箱即用。

# 求解时间研究

为了比较不同的 HPO 策略，我决定保持简单，专注于平均解决时间，这是一个相对简单易懂的指标。对于我的结果，有几点需要注意的地方:

1.  我是带着一个特定的模型和问题做这项工作的(下面会有更多)，所以我不期望这些结果是完全通用的。
2.  在各种 HPO 策略中，有许多任意的选择可能会改变结果。

## 调查 HPO 战略

*   随机搜索(RS) —无调度程序的搜索算法
*   异步连续减半算法/随机搜索(ASHA/RS)—调度器和搜索算法
*   异步超波段/随机搜索(AHB/遥感)—调度程序和搜索算法
*   异步连续减半算法/贝叶斯优化(ASHA/BO)—调度器和搜索算法

## 模型细节

我感兴趣的优化超参数的模型是在催化领域中用于预测吸附能的图形神经网络。具体细节可以在这里找到[。](https://doi.org/10.1021/acs.jpclett.9b01428)

## 超参数优化

我检查了六个超参数，如下所示:

*   学习率
*   批量
*   原子嵌入尺寸
*   图形卷积层数
*   完全连接的特征尺寸
*   完全连接的层数

**专业提示:**在决定要搜索的超参数空间的大小时，要考虑内存使用情况。在调整网络架构和批量大小时，我在 NERSC 的 16GB GPUs 上遇到了内存问题。

## 探索的问题

1.  调度程序的影响是什么？
2.  一个复杂的搜索算法能在多大程度上改善 HPO？

我想研究的第一个问题是使用调度程序的影响。为了解决这个问题，我比较了 ASHA/遥感、AHB/遥感和遥感使用相同计算资源的求解时间(4 个柯里 GPU 节点 8 小时)。除了 ASHA 和 AHB 调度程序之外，所有三种策略都使用相同的搜索算法。我使用的符号是调度/搜索算法。

除了调度程序，我很好奇一个“更智能”的搜索算法，比如 BO，能在多大程度上提高 HPO 的性能。为了探索这个问题，我比较了 ASHA/遥感和 ASHA/BO 使用相同计算资源的求解时间(4 小时 4 个柯里 GPU 节点)。

## 结果和讨论

给定相同计算资源的情况下，比较 ASHA/遥感、AHB/遥感和遥感的平均求解时间图

![](img/19087538b4e7e37cbd52ddb8f4d8d7b5.png)

ASHA/遥感明显优于 AHB/遥感和遥感，在较短的时间内达到较低的平均测试 MAE。与 RS 相比，ASHA/RS 将解决问题的时间缩短了至少 5 倍。我说至少 5 倍，因为 RS 没有在 8 小时的限制内收敛到测试 MAE 的下限。此外，更多的 ASHA/遥感试验接近平均值，导致较小的标准偏差。在所有情况下，前 6 个试验是时间平均值。我怀疑 ASHA/斯普斯卡共和国的表现主要是因为完成的审判数量。ASHA RS 完成了 AHB RS 将近 2 倍的试验，超过 RS 的 8 倍。完成的试验数量可以在右上角看到。我还应该提到，由于我所做的检查点的数量，that 斯普斯卡共和国和 AHB/斯普斯卡共和国的审判数量没有达到上限。**最小检查点对于基于 SHA 的 HPO 策略的性能至关重要。**下面的 that 遥感实验中使用较少检查点完成的试验数量说明了这一点——在一半的时间内完成相同数量的试验。与 RS 相比，减少的检查点将 ASHA/RS 的求解时间提高了大约 10 倍！

给出相同计算资源的情况下，比较 ASHA/RS 和 ASHA/BO 的平均求解时间图

![](img/4378bc796f9fc4ceb57bc1b79105da50.png)

从上图可以看出，对于我的特定型号，添加业务对象平均没有任何好处。我的假设是，我试图优化的超参数曲面有一堆局部最小值(想想鸡蛋纸盒)，没有明显的全局最小值，这会降低 BO 的好处。我可以看到 BO 工作良好的情况是具有更好定义的全局最小值的大型超参数搜索空间——不是说你可以先验地知道这个*。总的来说，我认为 HPO 的一个好方法是按需构建复杂性。**关于 BO 的最后一个注意事项，虽然使用 BO 并没有得到平均的改善，但我发现的最佳试验是使用 ASHA/BO。**因此，如果我必须选择一种超参数配置，我会选择这种配置。*

*ASHA/RS 和 ASHA/BO 曲线之间的时间延迟是可能的，因为在对新的超参数配置进行采样之前，BO 中使用的采集函数需要以一定量的数据为条件。*

# *基于 PBT 的最优调度*

*PBT 的一个很好的特性是能够开发一个理想的调度程序。例如，我可以在整个训练过程中确定理想的学习速度，这通常非常重要。在我的例子中，我想要一个超参数配置和一个学习率调度器，我可以用它来反复训练我的模型。大多数 ML 框架包括学习速率调度器(例如，多步、平台上减少、指数衰减等。)随着训练的进行降低学习率。因此，可以使用 PBT 开发定制的学习速率调度器，并将其合并到给定的 ML 框架中用于后续训练。*

*或者，如果您的应用不需要重复训练，PBT 可以直接用作训练程序，并且可以同时为所有算法超参数开发理想的时间表。*

*就实际时间而言，使用 PBT 进行培训非常有效，事实上，它使用的时间与您的正常培训程序大致相同，但总计算时间增加了，因为需要多个工作人员，可能需要 16-32 个 GPU。在 Ray Tune 中，如果工作线程的数量超过资源大小，也可以对工作线程进行时间复用。*

## *最佳学习率——结果和讨论*

*我想用 PBT 进行实验，并为我的模型找到一个学习率时间表(如上所述)。这是结果。*

*![](img/f5212b4085458c32760b6395ffd03861.png)*

*上面的图显示了群体中最佳试验的试验 MAE。在试验 MAE 中有一些跳跃，可能是尝试了随机扰动，由于没有改进，扰动最终被逆转。下方的图显示了作为训练迭代函数的学习率。看起来我的理想学习速度可以合理地用多步计划程序来模拟。*

# *选择 HPO 策略的备忘单*

*选择 HPO 策略实际上取决于您的特定应用。对于我感兴趣的许多化学和材料科学应用，达到 85%的合理的超参数就足够了。或者，你们中的一些人可能对给定模型的最后一点性能感兴趣。没有一个放之四海而皆准的解决方案，但是我已经整理了一个小备忘单来帮助想法流动。*

*![](img/5f51c4c3b7a3b6905dad2fd842032c12.png)*

**作者图片**

# *技术提示*

## *射线调谐*

*Ray Tune 非常用户友好，当设置它来运行您的模型时，您只需要考虑一些事情(我在这里不打算深入讨论，因为 Ray Tune 的[文档](https://docs.ray.io/en/latest/tune/index.html)和[示例](https://github.com/ray-project/ray/tree/master/python/ray/tune/examples)非常棒):1 .定义一个可训练的 API，基于函数或类——我推荐类选项，因为它允许你做更多的事情。通过`tune.run()`编写运行 Tune 的脚本*

***一般提示***

*   *检查以确保你的模型被放在正确的设备上，这听起来很傻，但很值得。如果您使用的是类 API，那么在您的`_setup`函数中放一个 print 语句来进行双重检查*
*   *射线调有一堆方便的[函数](https://docs.ray.io/en/master/tune/api_docs/grid_random.html#custom-conditional-search-spaces)(例如`tune.uniform`)来生成随机分布*

*`**Tune.run()**` **性能标志***

*   *`checkpoint_at_end=False`默认值为 False，不管其他检查点设置如何，我都会让它保持原样。真，不应该与基于 SHA 的策略一起使用*
*   *这可以提高性能，但可能作用有限——这取决于检查点的频率*
*   *我喜欢这个标志，因为它会在试验失败后立即终止试验，否则试验会经历所有的训练迭代，每次迭代都会失败*
*   *`reuse_actors=True`这个标志可以提高 ASHA 和 PBT 的性能，但是它要求你在你的可训练类中增加一个`reset_config`功能。在某种程度上，此标志可以节省资源，因为它不会在每次旧的试验终止和新的试验开始时重新加载数据集。*

## *蜻蜓—博*

*我喜欢贝叶斯优化的[蜻蜓](https://github.com/dragonfly/dragonfly)，因为它能够处理离散和连续变量。许多业务对象包只能处理连续变量，你必须想办法解决这个问题。然而，我确实发现实际定义超参数空间有点棘手。下面是我用来设置蜻蜓波使用射线调的代码片段。*

## *Slurm —工作管理*

*对于那些使用 Slurm 的人来说，就像我们在 NERSC 所做的一样，[这里的](https://github.com/NERSC/slurm-ray-cluster)是支持使用 Ray Tune 的脚本。可以直接复制`start-head.sh`和`start-worker.sh`文件；只有提交脚本需要少量的修改，以便在资源和选择的环境中执行您的代码。如果你遇到工作节点不能启动的问题，并且你看到这样的错误`ValueError: The argument None must be a bytes object`在启动这个[线](https://github.com/NERSC/slurm-ray-cluster/blob/master/submit-ray-cluster.sbatch#L36)上的头节点后延长睡眠时间。这不是一个 bug——head 节点需要设置一个变量，有时需要一段时间。*

## *TensorBoard —测井/可视化*

*默认情况下，光线调整日志使用 TensorBoard (TB)。关于 HPO 肺结核和雷的一些想法:*

*   *TB 允许您轻松过滤您的结果，这在您使用 ASHA 运行 1000 次试验时非常重要*
*   *使用 [HParams 仪表盘](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams)实现良好的可视化*
*   *TB 在 Ray Tune 中与基于 SHA 的策略配合得很好，我唯一的抱怨是与 PBT 的集成不太好*

*对于 NERSC 用户来说，这里的[是我通常运行 TB 的方式。一个缺点是一次只能打开一个 TB 客户端。](https://docs.nersc.gov/machinelearning/tensorboard/)*

## *权重和偏差—记录/可视化*

*W&B 有一个与 Ray Tune 集成的[记录器](https://docs.wandb.com/library/integrations/ray-tune)，我在测试的模型中使用了它。显然存在很大的潜力，总的来说我喜欢 W & B 平台，但当时(2020 年 3 月/4 月)我很难用 W & B 记录大规模的 HPO 活动。我相信一些更新/升级正在进行中。*

# *关键要点*

## *调查的结果*

1.  *与单独的随机搜索相比，ASHA 调度程序将我的模型的求解时间提高了至少 10 倍*
2.  *BO 可能不总是能提高平均 HPO 性能，但是我能够用 ASHA/BO 找到我的最佳超参数配置*
3.  *使用 PBT，我找到了我的最佳学习速率，它可以用多步调度器合理地建模*

## *结论*

1.  *光线调节是一个简单和可伸缩的 HPO 框架*
2.  *使用调度程序来提高 HPO 效率至关重要*
3.  *BO 等更复杂的搜索算法可能会带来一些好处，但并不总是值得投资*
4.  *如果模型不需要经常保留，PBT 对于开发理想的调度器和训练非常有用*
5.  *对于 HPO，没有放之四海而皆准的解决办法。简单起步，按需构建复杂性—ASHA/遥感是一种合理的默认战略*

*感谢:我要感谢[扎卡里·乌里西](https://ulissigroup.cheme.cmu.edu)(CMU)[穆斯塔法·穆斯塔法](https://www.nersc.gov/about/nersc-staff/data-analytics-services/mustafa-mustafa/) (NERSC)和[理查德·廖](https://github.com/richardliaw)(雷·调)让这部作品成为可能。*

**原载于 2020 年 8 月 31 日*[*https://wood-b . github . io*](https://wood-b.github.io/post/a-novices-guide-to-hyperparameter-optimization-at-scale/)*。**