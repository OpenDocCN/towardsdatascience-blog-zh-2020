# 有效的利他主义、人工智能安全以及从世界状态中学习人类偏好

> 原文：<https://towardsdatascience.com/effective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3?source=collection_archive---------47----------------------->

## [苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

## Rohin Shah 在 [TDS 播客](https://towardsdatascience.com/tagged/tds-podcast)

要选择章节，请访问我们的 Youtube 视频[这里](https://www.youtube.com/watch?v=uHiL6GNXHvw&feature=emb_title)

*编者按:这一集是我们关于数据科学和机器学习新兴问题的播客系列的一部分*，*由 Jeremie Harris 主持。除了主持播客，Jeremie 还帮助运营一家名为*[*sharpes minds*](http://sharpestminds.com)*的数据科学导师初创公司。你可以听下面的播客:*

收听[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2)、[谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz)、 [Spotify](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU)

如果你走进一个房间，里面满是随机散落的物品，你会认为这些物品有多重要或多贵？

如果你走进同一个房间，却发现那些物品被精心地排列成一种非常特殊的配置，而这种配置不太可能是偶然发生的，那会怎么样？

这两个场景暗示了一些重要的东西:人类以反映我们价值观的方式塑造了我们的环境。在我的公寓里花 10 分钟散步，你可能会比花 30 分钟和我聊天更了解我重视什么，因为我试图用语言表达我的人生哲学。

这是一个非常重要的想法，因为事实证明，今天高级人工智能中最重要的挑战之一是找到向机器传达我们的价值观的方法。如果我们的环境隐含地编码了我们价值体系的一部分，那么我们也许能够教会机器去观察它，并且在我们不必明确表达它们的情况下学习我们的偏好。

利用从人类居住环境的状态中获取人类价值的想法最初是由伯克利博士和即将上任的 DeepMind 研究员 Rohin Shah 共同撰写的一篇论文提出的。Rohin 在过去的几年里一直致力于人工智能安全，并发表了广泛阅读的人工智能联盟时事通讯——他非常友好地加入了我们这一集的“走向数据科学”播客，在这里我们讨论了他的人工智能安全方法，以及他对高级人工智能系统风险缓解策略的想法。

以下是我们谈话中我最喜欢的一些带回家的东西:

*   像许多人一样，Rohin 在一定程度上是通过接触有效利他主义社区的成员来推动人工智能联盟和人工智能安全工作的。有效利他主义是一场哲学运动，其重点是确定人们如何通过慈善捐赠或职业生涯对世界产生最大的积极影响。它专注于问这样的问题:我能做什么样的举动，才能最大化我对世界贡献的预期价值？对于 Rohin 来说，强大的人工智能系统在未来可能造成的伤害——以及这些系统如果安全开发可能带来的好处——使人工智能对齐变得很有吸引力。
*   Rohin 讨论了高级 AI 系统的两种重要的潜在故障模式，它们已经以不同的形式出现在当前的系统中。
*   首先，他强调了不良归纳的风险:人工智能从它们的训练数据中学到了错误的教训，导致它们以人类可能没有预料到或不想要的方式进行归纳。作为一个例子，他引用了对 OpenAI 的 GPT-3 模型的最初关注，当一名开发人员用一个无意义的问题提示该模型时(比如，“一个 woolrop 中有多少 glubuxes？”).GPT-3 号没有“诚实地”回答这个问题，而是说了一些类似“我不知道——我不熟悉这些词”的话，试图用最佳猜测来回答，比如“一个 woolrop 中有 3 个 glubuxes。”你可能会说这实际上是一个不错的概括:GPT-3 的回答基本上就像一个参加考试的学生，他想通过纯粹基于问题的上下文进行猜测来隐藏他们不知道问题答案的事实。但是，如果我们希望建立一个诚实的语言模型——一个在适当的时候承认自己无知的模型——那么未经微调的 GPT-3 似乎无法通过测试。随着人工智能系统变得更加强大，这种行为可能会变得更加有害，因此 Rohin 认为这值得关注。
*   其次，Rohin 讨论了向人工智能传达人类偏好的挑战。这是一个困难的问题:大多数人实际上不知道他们想要从生活中得到什么，甚至更没有能力与其他人交流这些愿望和价值观——更不用说与机器交流了，因为机器目前在推理方面缺乏上下文意识和灵活性。这就是 Rohin 教授机器从环境中推断人类偏好的工作的用武之地:他认为这种策略显示了作为人类偏好数据的额外来源的前景，机器可以用它来解读人类的价值观，而不需要我们能够明确地表达它们。我们讨论了这种策略的许多有趣的优点和缺点。

你可以[在这里](https://twitter.com/rohinmshah)的 Twitter 上关注罗欣，在这里注册他的[人工智能校准简讯，或者](https://rohinshah.com/alignment-newsletter/)[在这里](https://twitter.com/jeremiecharris)的 Twitter 上关注我。

![](img/be6825fcf4036388a6034e0833384262.png)

## 章节:

*   0:00 介绍
*   有效的利他主义
*   6:50 Rohin 对 AI 安全工作的介绍
*   11:18 为什么 AI 风险如此严重
*   18:33 育儿类比
*   22:15 统计学习理论
*   25:09 什么是偏好学习？
*   32:23 应用到更高的抽象层次
*   34:45 揭示偏好和世界状态
*   36:26 打碎花瓶的比喻
*   时间范围规则
*   49:03 总结

## 下面是第二季第四集的脚本:

Jeremie (00:00):
嗨，大家好。欢迎来到“走向数据科学”播客的另一集。我叫 Jeremie，除了主持播客之外，我还是 SharpestMinds 数据科学导师项目的团队成员。我对今天的节目非常兴奋，因为我已经想了很久让今天的嘉宾上播客了。我很高兴我们终于实现了。

Jeremie (00:16):
他现在正处于从伯克利毕业的过渡期。他在人类兼容人工智能中心工作，他将过渡到 DeepMind，在那里他将做一些校准工作。他的名字是罗欣·沙阿。此外，除了是人工智能和人工智能比对方面非常多产的研究人员之外，他还是人工智能比对通讯的出版商，如果你想了解人工智能比对方面的一些公开问题和未决问题，这是一个非常非常好的资源。我真的建议去看看。

Jeremie (00:47):
我们将谈论一大堆不同的事情，包括人工智能的哲学、机器学习和人工智能对齐的哲学、实现它的方法、存在的一些挑战，我认为我们将探讨 Rohin 提出的最有趣的提议之一，这是一个关于从环境状态中提取人类偏好的想法。所以，基本上，这里的想法是，人类，通过他们的活动，已经将他们的偏好隐含地编码在他们的环境中，我们做一大堆不同的事情，不同的行动，揭示了我们的偏好。如果我们能让人工智能观察这个世界，并根据这个世界的状态来推断出我们的偏好，那就太好了。这可能是一个伟大的方式来引导人工智能对齐的努力。

Jeremie (01:30):
我们将深入讨论这个提议，以及一大堆其他事情。我真的很期待这一集，所以我希望你喜欢。事不宜迟，我们开始吧。

杰里米(01:39):
你好，非常感谢你参加我们的播客。

罗欣(01:41):
是的，谢谢你邀请我。我很兴奋。

耶雷米(01:44):
很高兴你能来。总的来说，您在校准领域做了很多有趣的事情。但是，在我们解决一些更技术性的问题之前，有一个观察，我认为任何花时间从事比对工作或与比对研究人员交谈的人最终都会在某个点上做出结论，即该领域的绝大多数人似乎来自有效利他主义社区。我很想听听你的看法，第一，什么是有效的利他主义社区，什么是有效的利他主义，第二，为什么你认为在 EA，有效的利他主义，人工智能联盟和人工智能安全研究之间有这种深刻的联系。

罗欣(02:20):
是的，当然。最重要的思想有效的利他主义，非常容易维护，不涉及细节，是无论金钱，时间，资源，无论你愿意无私地花费什么，你都应该尽力做好，而不是…你应该考虑一下。很难反驳这一点。我不认为我真的见过有人不同意这一点。

Rohin (02:56):
现在，在实践中，有效的利他主义运动有一大堆额外的前提，旨在支持这项技能，但更具争议性。我认为有效利他主义的最基本的理念是事业优先化。很多人会说，“好吧，我想要，比如说，非洲的干净水。我将为此而努力。”他们会考虑在非洲获得清洁水的不同方法，也许你可以尝试对人们已经获得的水进行消毒，或者你可以尝试建立一些新的处理厂，以便为每个人提供新鲜的，流动的饮用水。他们会考虑如何最好地实现他们提供干净水的目标。

Rohin (03:47):
人们很少会想，“好吧，我应该为非洲的人们提供干净的水，还是在美国反对种族主义？我应该努力做哪一件事？还是我的钱成？”有效利他主义的主要前提是，事实上，你可以做到这一点。事实上，原因之间有很大的不同，仔细想想，首先选择正确的原因会产生更大的影响。

罗欣(04:20):
它非常专注于这一点，认真对待想法，实际评估它们，判断它们是否真实，而不是它们听起来是否疯狂。它们听起来是否疯狂确实与它们是否真实有些关系，但它们不一定相同。我认为这也是为什么它是人工智能安全研究的温床。人工智能安全的 EA 案例，人工智能安全的工作，是人工智能有很好的机会在下个世纪极具影响力，比如说。

Rohin (05:00):
有一些论点值得商榷，但你似乎不能排除它。看起来至少有一定的可能性，如果我们不注意我们如何做到这一点，人工智能系统可能会“接管”,因为所有关于世界的重要决定都是由人工智能系统而不是人类做出的。一个可能的后果是人类灭绝。我稍后会讨论这个问题，我确定，但是-

耶雷米(05:36):
所以【相声 00:05:37】—

rohin(05:37):
【cross talk 00:05:37】相信这个论点，然后它就变得极其重要和有影响力。这听起来很疯狂，但对我来说，EA 的优势之一是它将听起来疯狂的东西与真实的东西分开。

Jeremie (05:52):
看起来，实际上，重点是有一个额外的缺失步骤，许多人在决定贡献什么事业、做什么工作、花什么时间时没有考虑到这个步骤，这就是“哪些领域会给我的时间带来巨大的回报？”

罗欣(06:09):
是的，没错。

Jeremie (06:12):
我真的能回想起我与人们关于慈善事业的大部分对话，这些对话通常都集中在诸如慈善机构的管理费是多少之类的问题上。哦，我想把我所有的钱都捐给这个事业，而不是问一个更基本的问题，从造福每个人或造福人类的角度来看，这个事业真的会带来最好的投资回报吗？有趣的是，这种思维，一种更加第一原则的方法，将许多人引向人工智能校准和人工智能安全领域。如你所说，这是有道理的，你得到了这种超高风险高回报的配置文件。

Jeremie (06:50):
是什么吸引了你，例如，人工智能联盟，特别是人工智能安全工作，而不是其他任何生物恐怖主义，我可以想象所有可能发生在我们身上的可怕事情，但为什么特别是人工智能联盟？

罗欣(07:05):
是的，所以我的故事有点奇怪。这可能是一个经典的人工智能故事，因为它被非常怪异的论点所说服。2014 年进入有效利他。我大概在一年内听到了关于人工智能风险的争论。我被他们深深地说服了。我就是没买。

Rohin (07:37):
所以，直到 2017 年，我基本上没有怎么接触 AI 安全。我也不相信，基本上，有一个伦理学领域叫做人口伦理学，它试图解决这样一个问题，当不同的世界有不同的人口时，你如何比较它们有多好？我们不需要深入细节，但这是一个非常令人困惑的领域。很多不可能的结果表明，你可能想要这六个非常直观的属性，但是，不，你不可能同时拥有它们，就像这样。所以你是[相声 00:08:21]-

耶雷米(08:20):
这里的想法是，一个有 100 个体面快乐的人的世界比一个有 1000 个体面快乐的人的世界更好吗？是那种计算吗？

罗欣(08:31):
是的。这是我要处理的问题的一个例子。

Rohin (08:35):
所以，不管怎么说，2017 年夏天我一直在思考这个问题。最终，我想，“好吧，我想我应该给它一个合理的权重，”当然不是确定性，而是合理的权重，认为更多快乐的人实际上意味着一个更好的世界，即使他们是在未来。一旦你有了一个相当大的可能性，确保未来继续存在并拥有幸福的人就变得极其重要，因为它与现在如此相关。

Rohin (09:21):
所以，我想做一些更面向未来的事情，我在计算机科学和数学方面有很多技能，基本上，你想在人工智能校准方面工作的一切。我仍然不太相信人工智能风险，但我想，“好吧，一群聪明人已经考虑过这个问题，也许我应该研究一段时间，看看它是否有意义。”这就是导致我实际上改变的原因，一年后，我实际上开始相信这些论点。

耶雷米(09:57):
太有意思了。

罗欣(09:58):
我也看到了不同的论点。

Jeremie (10:00):
你被……比最初的争论本身更容易被这个问题吸引的人的素质所引导吗？你还记得当你在做这个东西的时候，你会说，“嗯，等一下。这其实是真的。”我现在明白了为什么尼克·博斯特罗姆，也许还有埃利泽·尤德科夫斯基，以及当时谈论这个话题的其他人说得有道理了。

罗欣(10:21):
我从来没有真正的啊哈时刻。我记得，在某一点上，我就像，“我想我现在相信这些论点，”但它不像我…我想我现在相信人工智能风险是实质性的和真实的。我不能指出一个具体的时间点，是的，现在我相信了。有一天，我正在思考这个问题，注意到，“哦，是的。我以前不相信这个。现在我知道了。”

耶雷米(10:52):
太有意思了。看起来在阅读超智能的人和阅读较少错误的人之间存在分歧，他们对这个问题感到非常兴奋，并且立刻感到非常害怕，因为，不管出于什么原因，他们天生就是这样的。然后是像你一样的人。这就像一个缓慢的燃烧，你慢慢地进入其中。我想这是问题的一部分，如果需要很长时间才能让人们认为这是一件非常重要的事情，那几乎是阐明问题的一部分。

耶雷米(11:18):
当你试图向人们解释为什么人工智能风险如此严重时，你有什么策略吗？为什么你认为可能对你有作用的概率是不平凡的，以加速这个过程？

罗欣(11:32):
是的，我应该指出，我仍然……例如，我对《超级智能》中的论点不是很满意。我要说的是，对我来说，激励它的是略微不同的论点，仍然相当强调超智能的东西。

耶雷米(11:50):
顺便说一下，我想很多人都没有听说过超智能。

罗欣(11:54):
哦，是的。

耶雷米(11:55):
如果你想讨论你提出的任何论点，也请随意提供背景资料。

Rohin (12:00):
是的，也许我会谈谈我个人喜欢的论点，因为我可以更好地解释它们。但是，就上下文而言,《超级智能》是牛津大学教授尼克·博斯特罗姆写的一本书。它于 2014 年出版，是第一篇[听不清 00:12:19]论述为什么人工智能风险是可能发生的事情，为什么我们应该认为它可能是合理的，什么解决方案看起来像是应该调查的，等等。

Rohin (12:33):
然后，就我个人而言，我将给出的论点是…所以，首先，我们将[听不清 00:12:45]作为一个前提，我们建立智能的人工智能系统，比如说，像人类一样智能。我们可以以后再谈，但那完全是另外一个话题了。我只想说，我认为这不太可能……我认为[听不清 00:13:03]在下个世纪发生的可能性相当大。但是，现在，把它当作一个假设。

罗欣(13:10):
智慧意味着你可以适应新的情况，当你面对新的情况时，你会了解它，然后做一些事情，这些事情是连贯的。有道理。我举的一个例子是 GPT-3 的一个具体例子，在这个例子中，我们甚至可以看到当前的神经网络。我相信观众会熟悉 GPT-3…听众，而不是观众。但如果不是，GPT-3 是 OpenAI 最近开发并发布的[听不清 00:13:47]语言一代[听不清 00:13:48]。

罗欣(13:51):
我想我喜欢《邮报》上给 GPT-3 做图灵测试的一个例子。GPT-3 的背景是一堆问题和答案。GPT-3 会提出一个问题，一个硬币里有多少硬币。这些都是废话，你没有听错。GPT-3，从某种意义上说，这超出了它的训练分布。据推测，它从未在其训练语料库中见过这句话。它可能从未见过 bonk 和 quoit 这两个词。这是实际的分布转移，你依赖于某种分布的一般化。

罗欣(14:43):
尽管如此，我认为我们都可以预测 GPT-3 不会输出一些随机的字符串。它可能会说些明智的话。事实上，它说的是，“你知道，一个杯子里有三块糖。”为什么是三个？我不知道。但是，你知道，在某种意义上这是明智的。它产生了一个听起来像英语的答案。

耶雷米(15:10):
在某种程度上，我们都有过这样的经历，如果我们写考试或其他什么，我们会被问及一个硬币里有多少颗糖，我们没有学习，嘿，一个硬币里有三颗糖。我们走吧。

罗欣(15:18):
完全正确，对吗？在某种意义上，GPT-3 确实概括了，它概括了一个学生参加考试的方式。在最初的帖子中，这被视为 GPT-3 实际上不合理的证据，因为它不知道如何说，“这个问题是无意义的。”

罗欣(15:39):
但是接下来的一个帖子是这样的，“实际上，你完全可以让 GPT-3 这样做！”如果你告诉 GPT-3…如果在上下文中，你说每当它看到一个无意义的问题，人工智能回应，“哟，是真实的。”然后，当被问到一个硬币里有多少颗糖时？它说，“哟，真实一点。”所以你知道它有能力辨别这是废话，它只是以一种更像一个应试者而不像一个谈话中的人的方式进行概括。我们提前知道了吗？不，我们没有。为了弄清楚这一点，我们不得不实际运行 GPT 3 号。

罗欣(16:23):
我认为人工智能的风险基本上是这样的，但如果你的人工智能系统是人类水平的智能，它肯定会被部署在新的领域，在新的情况下，我们以前没有见过。我们真的没有令人信服的理由相信它会继续做我们训练它做的事情，而不是别的事情。在 GPT 3 号，我们训练它做什么？好吧，在训练数据集上，至少，我们训练它做人类在那种情况下会写的任何事情。

罗欣(17:05):
当你看到一个杯子里有三颗糖……对不起，一个杯子里有几颗糖？在那种情况下人类会怎么做？我不知道。这并没有很好的定义，GPT 3 号做了一些明智的事情。我不认为你可以合理地说它没有做我们训练它做的事情，它只是做了一些连贯的事情。同样，如果你有人类水平或更高智能的人工智能系统，对世界采取超级有影响力的行动，并且它们被置于这些新的情况下，在这些情况下，它们将如何概括并不是事实，那么它们可能会采取对世界有很大影响的行动，而这不是我们想要的。

罗欣(17:49):
然后，也许是直觉【听不清 00:17:52】为什么这可能非常非常糟糕，就像人类灭绝级别的糟糕。一个特别的分布变化是，你从人类比人工智能拥有更多权力并可以关闭人工智能的训练环境，到人工智能足够智能并被广泛部署，但没有人可以…或者说人类作为一个整体不能关闭它的环境。在那种情况下，那是一种新的情况。AI 以前从未有过这种能力。它会以某种方式使用它吗……它会以某种不同于我们在训练中预期的方式进行概括吗？我们真的没有理由说不，它不会那样做。

耶雷米(18:33):
你认为这和养育孩子有相似之处吗？我只是在想人类的代际繁衍，我们的祖先在 17 世纪，至少在西方，我肯定会对我们今天处理性的方式，我们与长辈沟通的方式，我们管理机构的方式等等，我们所有的等级制度都完全不同。而且，在许多方面，我们[听不清 00:19:00]是中世纪或文艺复兴早期的道德框架。

耶雷米(19:07):
我想这里有所不同，至少我们仍然运行在相同的基础硬件上，或者非常相似的东西。也许这确保了最低水平的一致性，但是这种类比在某种程度上是否会破裂呢？

罗欣(19:18):
我认为这是一个很好的直觉。这个类比在某些方面是不成立的，比如，嗯…这个类比不太成立，因为这些原因，我会说稍微减轻一点重量。一个是，在抚养孩子的过程中，你对孩子有一定的影响力，但是你不能做一个完整的训练过程，在这个过程中，你要给他们一个梯度，让他们做每一步动作。你可能希望，鉴于你可以在人工智能系统上有更多的选择压力，你将能够避免这个问题。

Rohin (20:00):
但是，是的，我认为这就是我所指的基本动力。你对这些代理有一定的影响力，但是那些代理遇到新的情况，他们在这些情况下做一些事情，你没有提前考虑这些情况，你没有训练他们做正确的事情。

耶雷米(20:23):
我完全同意人工智能风险的观点，这是非常重大的风险。赌注非常高。当谈到你认为最有希望的解决方案或策略时，你自己是专业的，显然，在一个类别中，每个人都必须是，在对齐问题域中的一个子空间中。你决定关注的领域是什么？为什么你认为在这一点上最值得关注？

罗欣(20:50):
到目前为止，我告诉你的故事是一个概括。主要的问题是我们不知道如何概括，而且，很可能，你会得到一心一意追求权力的人工智能系统，这与超级智能的故事相似，可能会导致人类灭绝。基本机制是糟糕的概括，或者说是像你的能力那样的概括。你做了一些连贯且影响深远的事情，但是相对于人类想要的，你试图做的事情并不一般化。

Rohin (21:31):
我最感兴趣的许多事情在某种程度上都与泛化有关。我感兴趣的一件事是，我们能从经验上更好地理解神经网络是如何进行归纳的吗？对此我们能说些什么吗？有很多理论试图解释为什么神经网络有如此好的泛化能力。这不能用统计学习理论来解释，因为神经网络可以记忆随机噪声，但尽管如此，当标签不是随机噪声时，它似乎可以很好地概括。

Jeremie (22:15):
你介意解释一下统计学习理论作为参考吗？实际上，我不确定我是否能理解这种联系。

罗欣(22:23):
统计学习理论就像机器学习理论的一个分支，试图做几件事。但是，除了其他事情之外，尝试证明如果我们用这样那样的训练属性在这样那样的训练数据上训练一个机器学习模型，那么我们知道它将以这样那样的方式进行推广，并且它证明了关于这一点的定理。

Rohin (22:50):
重要的是，目前大多数方法都专注于对您的模型、您的假设类别做出假设。这些假设通常排除了对任意大小的数据集进行过度拟合的能力，因为如果可以的话，那么你真的不能说任何关于一般化的东西。但事实是神经网络真的可以适应任何数据集。他们可以记住字面上随机噪音的标签。所以，这些假设不适用于神经网络。

罗欣(23:28):
我感到兴奋的事情是，我们可以谈论数据集的假设，而不仅仅是模型吗？如果我们考虑数据集上的假设和模型上的假设，那么我们能说说神经网络是如何进行归纳的吗？这就像是一个超级模糊不清的希望，我还没有真正开始努力，据我所知，其他人也没有。

Rohin (23:55):
关于神经网络有太多的经验性的东西让我非常困惑，比如深度双重下降。我不明白。这是一种经验现象。不知道的可以去查一下。它可能不值得我去探究，只是太令人困惑了。不知道为什么会这样。对我来说毫无意义。我想知道为什么，我认为如果我们理解了这样的事情，我们也许能够开始陈述神经网络如何倾向于概括，也许这可以转化为我们可以说的关于安全的事情。

耶雷米(24:27):
这很有趣，因为泛化的故事似乎是问题的一个组成部分，当然，然后是另一个组成部分，我的意思是，有一些重叠，但看起来它们确实有不同的组成部分。告诉机器人类的偏好是一个挑战，我们告诉彼此我们想要从生活中得到什么的能力已经非常有限了，我的意思是，至少我个人认为这是一个有点不和谐的前景，不仅要表达我们的偏好，还要量化它们，并将其转化为某种损失函数，然后输入到模型中。你在这方面做了很多有趣的工作。

耶雷米(25:09):
事实上，我想谈谈你的一篇论文。我们在开始录音前讨论过这个问题，我很高兴听到这也是你认为最有趣的一个。我们观点一致，至少在这一点上。这是一个想法……嗯，论文的标题是世界状态中隐含的偏好。我想，首先，我想问一个问题来做个铺垫。什么是偏好学习？那是什么概念？

罗欣(25:34):
这实际上是我接下来要说的让我感到兴奋的事情。

耶雷米(25:38):
哦，太好了。

罗欣(25:39):
我已经谈到了概括，但是在你概括之前，你首先要训练正确的事情。这似乎是人工智能系统的一个很好的起点。如果你没有，你可能就完了。关于如何通过编写一个程序或一个方程来指定你想要的东西，这实际上是非常典型的，已经洒了很多墨水，正如你所知，深度强化学习或任何深度学习系统是如何工作的。但它通常与深度强化学习联系在一起。

罗欣(26:21):
优先学习的理念是，你不用写下一个等式来说明你想要什么，而是用一些更简单的方法来说明。例如，你可以在一个强化环境中观察两个轨迹，你可以观察代理采取的两个行为，你可以说，“啊，是的，左边那个。那个更好。”这给了代理一些关于它应该做什么的反馈。它不是试图写下一个方程，在每一种可能的情况下捕捉理想的行为。只是说，这两个里面，哪个更好？你可能会认为这对人类来说更容易做到，也更有可能是正确的。

Rohin (27:08):
这是一个偏好学习的领域，我认为这是一个我们如何为人类设计向人工智能系统提供反馈的机制的领域，这样我们就可以提供反馈，激励我们真正想要的行为，并且我们不会像在奖励功能方面那样犯很多错误。

耶雷米(27:34):
所以，我发现在这方面真正令人兴奋的是，人类表达的欲望和透露的欲望，或者表达的意图和透露的意图之间有这种众所周知的差异。我会说今天要健身三个小时，要做一堆编码，下个月要吃一堆纯素餐。如果你下个月来看我，我将不会做所有这些事情，我几乎不会做所有这些事情。问题是，哪个我是我？我是那个说嘿，我想成为那个人的有抱负的自己吗？或者我是那个一直坐在沙发上看网飞的傻瓜？

耶雷米(28:16):
这似乎真的解决了问题，因为如果探针显示偏好，无论是好是坏，我想这也可能是一种失败模式。你认为这种方法有价值吗？

罗欣(28:29):
是的，我认为你想同时使用两种信息来源，而不是其中任何一种。实际上，让我后退一步，区分你可能尝试做的两件不同的事情。有一件事，你试图了解人类重视什么，这是你正在谈论的那种事情，还有另一种框架，你只是想，“我想让我的人工智能系统做这样那样的任务，我想训练它这样做，但我不能写下这项任务的奖励函数。”

Rohin (29:01):
老实说，我实际上对后者更感兴趣，但前者也是我花了很多时间研究的，我对此感到兴奋。现在，我们谈论的是前者。

耶雷米(29:13):
我可以问一个天真的问题吗？我想我知道区别是什么，但我只是想让你明确地解决它。那两件事有什么区别？

罗欣(29:24):
一件事是，我可能想让我的人工智能系统用吸尘器打扫我的地板，或者其他什么。用真空吸尘器清扫我的地板的任务并不是仅仅由那句话来明确说明的。任何有 Roomba 的人都会告诉你 Roomba 超级笨的故事。其中一些只是 Roomba 不够智能，但也有一些是任务不是非常明确。

Rohin (29:57):
代理人应该在圣诞树下用吸尘器清扫吗？那里有一堆针，可能会破坏他们的吸尘器。谁知道呢。如果地板上有一些随机闪亮的按钮，应该用真空吸尘器清理还是放在一边？因为也许那个按钮很重要。什么样的东西，猫应该被吸尘？这只猫的毛很多，到处都是。如果你给猫吸尘，看起来会让你的房子更干净。

罗欣(30:31):
这里有很多模糊之处。我不会真的说这些是人类价值观，就像教你的 Roomba 如何吸尘似乎和教 Roomba 人类价值观不是一回事。首先，你真的不能在这里谈论太多暴露的偏好，因为我不经常用吸尘器打扫我的房子。如果一个人工智能系统要排队清空，我可能会让它更频繁地清空。

耶雷米(31:06):
你会说这是人类偏好的狭隘应用吗？狭义人工智能和 AGI 之间的区别似乎映射到了这一点上。

罗欣(31:16):
是的，我想我同意这一点。我会说，但在这个意义上，一切都是狭隘的人工智能。你只能得到变得越来越普遍的狭义人工智能，在某种程度上，我们决定不再称它为狭义人工智能，而开始称它为 AGI，因为它已经变得如此广泛。

Rohin (31:34):
我喜欢你的想法，你可以从一些可以应用于当今系统的东西开始，然后对其进行扩展。它变得越来越有能力，越来越通用，但它总是相同的技术。最终，我们用它创造的系统，我们会给它们贴上 AGI 或人类智能或超级智能的标签。同样的技术，同样的原理。这就是为什么我对这个问题的框架更感兴趣，而不是人类价值垃圾邮件。

罗欣(32:07):
当你进入更一般的系统，它与人类的价值观融合在一起。一旦你有了设计政府政策或其他东西的人工智能系统，无论你给它们什么样的反馈，它都会更好地教会它们人类的价值观。

耶雷米(32:23):
是的，我想，希望我们在越来越高的抽象层次上开始这样做，就像你说的那样。从某种意义上来说，我们向上填充卷积滤波器。

罗欣(32:34):
是的，正是如此。你问了一个关于表露偏好与口头偏好或表达偏好的问题。我想，是的，这是一个重要的区别。我绝对希望我们提出的任何方法都不依赖于一个或另一个，而是同时使用两者，这将会有冲突，我最希望的是，我们可以让人工智能系统搁置冲突，并根据其中任何一个集来做非常好的事情。也许，你必须有一些冲突解决机制，但是在某种意义上，人类已经必须这么做了。我们似乎有可能做这件事。

罗欣(33:28):
我认为这是一个非常好的方面，你不必在每种可能的情况下都致力于寻找行为。我们只是不知道这些。老实说，我们的价值观还不够明确，这是不对的。当我们遇到新情况时，我们的价值观会不断更新。现在，我们谈论民主，一人一票。如果有一天，在超人类主义的未来，如果有一天复制人成为可能，我想我们很快就不再想要一人一票了。因为如果你足够富有，你可以花钱让任何人当选。

耶雷米(34:19):
是啊。或者，我想，仅仅是在有限的关于大脑状态的更好的信息中，我们可以说，当然，这个政策让大多数人更快乐，但是它让人们更不快乐，我的意思是，看看那个可怕的多巴胺循环。这些人真的受到了很大的打击，你唤醒了他们的反应。

罗欣(34:37):
对，对，你肯定可以为社会福利做更好的优化，也许你不希望只有一人一票。

耶雷米(34:45):
好的，现在，我想这让我们回到了世界状态中隐含的偏好，大概有一些关于世界结构的事情透露了我们的偏好，我想这主要是透露了偏好，对吗？

罗欣(34:57):
是的。

耶雷米(34:57):
我们实际上做了什么。

罗欣(34:58):
是的，这绝对是一个显示偏好的方法。我认为这一点的一个重要方面是人们将…我认为我对此特别兴奋的原因之一，我想说的是作为前奏，这不是试图做困难的事情。当人们想到价值学习时，他们会想如果无人驾驶汽车可以在撞到两名乘客或杀死司机之间做出选择，它应该怎么做？这些都是艰难的伦理问题。我对它们不太感兴趣。我想从我们能否得到一个可靠地知道它不应该杀死人类的人工智能系统开始。如果有两个选择，是的…

罗欣(35:50):
总之，我们都同意或几乎都同意的基本内容，所以我认为观察世界的状况是了解这一点的好方法，这里的基本直觉是，我们已经在这个世界上生活了很长时间。我们有偏好，我们一直在重新安排世界，以适应我们希望世界成为的样子。因此，你可以颠倒这个过程，找出我们可能想要的东西。

Rohin (36:26):
有一个很好的玩具例子可以说明这一点。假设有一个房间，在房间的中间有一个易碎的花瓶。花瓶一旦被打破，就再也无法修复了。我们假设人工智能知道这一点。我们将假设人工智能知道所有的经验事实。它知道世界是如何运作的，它知道人类可以采取什么行动，它知道人类可以采取什么行动，它知道它自己可以采取什么行动，它知道世界的可能状态，但它不知道关于[听不清 00:36:58]功能的任何事情，它相当于人类的价值观。

罗欣(37:02):
它知道经验事实。它知道，这个花瓶，一旦打破，就无法修复。我们将把胶水之类的东西放在一边。然后它看着它看到的事实，它被部署在这个房间里，它看到它的人类，我称之为爱丽丝，在房间里，花瓶没有被打碎。现在你可以提出假设性的问题，比如好吧，如果爱丽丝想打破花瓶，我会看到什么？嗯，我会看到一个破碎的花瓶。如果爱丽丝不在乎花瓶，我还能指望什么呢？嗯，可能，在某些时候，最有效的方法是在房间里走来走去，同时打翻花瓶。所以，很可能在那种情况下，我也会看到打碎的花瓶。

罗欣(37:58):
如果爱丽丝不希望花瓶破碎，或者积极地希望花瓶不被打碎，我会期望看到什么？在这种情况下，我实际上看到了一个完整的花瓶，可能。因为我实际上看到了一个未破碎的花瓶，这告诉我，在这三种情况中，只有最后一种似乎与我的观察一致。所以，很可能，爱丽丝不想打破花瓶。你可以通过观察世界的状态，看到花瓶没有碎，来推断爱丽丝不想打碎花瓶的事实。

耶雷米(38:33):
这似乎与热力学第二定律有很深的联系，宇宙有很多方法可以解决花瓶破碎的情况，但是没有花瓶破碎的事实是一个巨大的信息。

罗欣(38:52):
对，完全正确。

Jeremie (38:57):
基本上，到了[相声 00:38:59]-

罗欣(38:58):
我想我没有什么要补充的了。

耶雷米(38:59):
这让我感到震惊，这是我的物理学家本能，但就世界看起来与我们对纯热力学随机性的预期有所不同而言，这里的假设是这些差异来自人类的偏好。这是描述…的公平方式吗

罗欣(39:17):
是的，没错。

耶雷米(39:19):
那这是否意味着某种故障模式呢？因为我想我们在我们的环境中编码信息，我想这是[听不清 00:39:25]显示偏好的事情，但含蓄地说，我已经把我的大脑状态硬编码到我的公寓里，每件事情的安排，任何厌恶女人，任何种族主义，任何恋足癖，所有可能是或可能不是我个性的一部分的怪异怪癖都含蓄地编码在房间里。这是应用这种技术的部分风险吗？

罗欣(39:54):
是的，所以，从理论上来说，如果你[听不清 00:39:59]这种方法，它会…它会得到所有显示的偏好吗？还有，嗯，我不知道它得到了一切。但大致来说，它得到了你透露的偏好。我相信有些事情是它不明白的。有时候，你只是不喜欢你表露出来的偏好，你认为它们应该是不同的。

罗欣(40:27):
你有一个明显的偏好，许多人都有一个明显的偏好，就是拖延，他们可能实际上并不赞同，他们不希望他们的人工智能系统给他们越来越多令人上瘾的材料，以便他们可以更好地拖延，这似乎是可能发生的事情。我将不得不更加努力地思考这到底是如何发生的，但我可以相信这将是一种影响。

Rohin (41:03):
同样，我到目前为止解释的技术似乎是世界上只有一个人，如果有多个人类，事情会变得复杂得多，我到目前为止一直忽略这种情况。

耶雷米(41:20):
这是你离开地面所需要的，对吗？

罗欣(41:22):
是的。

耶雷米(41:26):
在这种情况下，我想至少还有另一种风险模式，就是说，在这个花瓶的例子中，我们假设这个人实际上并不在乎这个花瓶，只是碰巧，在她的演示中，他避开了这个花瓶。有没有这样的风险，我想这是机器学习中的一个风险，听起来就像一个额外的分布抽样的例子，就像你会学到的-

罗欣(41:57):
是的。

耶雷米(41:57):
好的。

罗欣(41:58):
对，没错。如果花瓶被放在一个不显眼的地方，实际上爱丽丝在房间里走动时不太可能打碎花瓶，我们实际上把它写在纸上，我们表明在那种环境下，你实际上不会学到任何关于花瓶的重要东西。你就像，“呃，她可能不想把它弄坏。”你推断出她并不十分渴望花瓶被打碎，但是你没有推断出比这更强烈的东西。你不确定打碎花瓶是件坏事还是件坏事，是的，这并不重要。

耶肋米亚(42:44):
有意思。

罗欣(42:45):
如果打碎花瓶可以提高效率，那么你推断并观察到花瓶没有被打碎，然后你推断打碎花瓶是不好的。仍然有可能的是，人类，我们不是完美的最佳人，我们可能不会提高效率，所以我们可能会绕过花瓶，即使去花瓶会更快，即使我们不在乎花瓶。是的，这个方法会做出错误的推断。总的来说，在偏好学习中，你假设人类做人类做的事情来反映他们想要的，这两者之间有很大的矛盾。并不总是如此。

耶雷米(43:37):
没错，我想有时候纯粹是因为愚蠢。我们可能想要一件东西，只是不知道如何让它发生。

罗欣(43:45):
是的，没错。这在偏好学习中是一个巨大的挑战，事实上，包括我在内的许多人都试图解决这个问题。但我不会说，在把愚蠢和你真正想要的东西分开方面，已经取得了巨大的进步。

Jeremie (44:10):
我认为如果我们这样做，我们最终会解决很多其他问题[相声 00:44:12]。实际上，关于论文，我还有一个问题想问你。我认为，时间范围的规则，或者说时间范围在论文中的规则，真的很有趣，因为机器人或人工智能对人类对这个动作的时间范围有一定的假设，如果对时间范围的改变有假设，你会开始看到不同的行为。我很想听你详细说明一下，并描述一下那个场景。

罗欣(44:43):
我认为我关于时间范围的主要观点是，如果你假设时间范围很短，那么状态没有完全优化的情况就更情有可原了，因为人类没有足够的时间将状态完全优化到对他们来说最佳的状态，所以你可以-

耶雷米(45:11):
也许我应该填补这个空白，我知道这有点模糊，但是从时间的角度来看，我想我们谈论的是人类从房间的一个地方到一个理想的终点需要的时间，对吗？[相声 00:45:24]

罗欣(45:23):
是的，这就像机器人出现之前，机器人假设人类已经在环境中活动的时间。在房间的情况下，它的机器人被部署，看到一个完整的花瓶，它就像，“啊，是的，人类已经在这个房间里走了一个小时，”或类似的事情。

耶雷米(45:40):
对，如果你在房间里走了整整一个小时，花瓶还在那里，你就可以认为这个花瓶可能非常重要。

罗欣(45:48):
对，正是这样。论文中的实际设置略有不同，但这是正确的直觉。是的，花瓶的例子不能很好地说明这个问题，但是想象一下，你正在用卡片建造一座房子。这是另一个例子，世界的状态真的很能说明你的偏好。纸牌屋是超级的，超级的不熵。可以推断出很多。

耶雷米(46:21):
是的，安排越具体，我想就越…这很有趣，因为这正是保护人类的挑战所在，这种方法几乎有一种哲学上的保守倾向，因为我们假设我们已经到了值得保护的地方，因为我们已经对自己进行了编码。 我们在环境中已经有了这么多好的东西，而且看起来我喜欢这个时间范围的东西是它背后的政治哲学，它几乎给了你一个刻度盘，你可以通过假设不同的时间范围从光谱的进步端调谐到保守端。 如果你认为我们刚刚来到这里，这是一张白纸，那么，嘿，我们可以尝试任何事情。相反，我们并不确定人类在这种环境中想要什么…

罗欣(47:12):
是的。

耶雷米(47:12):
对吗？是啊。

罗欣(47:13):
没错，我从来没有这样想过，但是你说得对。基本上就是这样。另一种思考方式是，我实际上到了这一步，到了写这篇论文的地步，问我自己为什么我们给无为行动以特权？我们会说安全的行动是什么都不做。为什么？只是一个动作。这是一个答案，我们已经优化了环境，随机行动是可能的，所以…当前状态在我们的偏好排名中很高。随机的行为把我们从那个状态带到另一个随机的状态，所以可能期望我们的排名变低，而什么都不做的行为保持了它，所以它是好的。时间跨度越长，你就越想默认什么都不做。

耶雷米(48:07):
是的，是的，我记得在报纸上看到过，实际上，这几乎是直觉的衍生，当你看到它像那样摆放时，它是如此美丽。

罗欣(48:15):
我知道，所以很好。

耶雷米(48:18):
是的，是的。在某种程度上，这让我想到不同政治派别的人之间的许多争论会变得容易得多，如果我们应用这样的玩具模型，你可以说，嗯，嘿，对保守派来说是有价值的。进步是有价值的。不管怎样，我们最终都会陷入反面乌托邦，这里有一个参数，我们可以调整它来看看反面乌托邦的事情是如何发展的，这取决于我们对事物的重视程度。

耶雷米(48:43):
是的，不管怎样，我喜欢这项工作，我认为这是……不管怎样，对于任何对哲学、道德哲学和人工智能的交叉感兴趣的人来说，这是一个非常酷的工作。

罗欣(48:58):
好的，谢谢。我喜欢它的原因基本相同。

耶雷米(49:03):
甜。嗯，我很高兴我们有兼容的[听不清 00:49:06]，然后。太棒了。嗯，我想我们已经谈了很多，但是你还有什么想谈的吗？我想确定的一件事是参考您发布的校准时事通讯。我认为每个人都应该检查一下，特别是如果你想进入这个领域。Rohin 发布了这个惊人的时事通讯，无论如何，我们会在播客附带的博客上链接到它。

杰雷米(49:30):
你有什么社交媒体链接或类似的东西想分享吗？

罗欣(49:35):
我认为联盟时事通讯是让我了解当前想法的最佳途径。如果你是这个领域的新手，我可能会推荐其他东西。我喜欢的更具介绍性的具体排名…它不完全是介绍性的，而是更永恒的材料，在对齐论坛上有一系列我写的博客帖子，称为价值学习序列。我喜欢这是一个很好的介绍，在那个论坛上还有另外两个推荐的序列，我也推荐，我也认为非常棒。

罗欣(50:21):
在社交媒体方面，我有一个 Twitter。它是@RohinMShah，但大多数情况下，它只是发送校准简讯链接。人们也可以随时给我发电子邮件。我的电子邮件在我的网站上，不能保证我会给你回复，因为我确实收到了很多电子邮件，但我认为我的回复率相当高。

耶雷米(50:50):
是的，好吧，我可以在我这边证明这一点。感谢你抽出时间，真的很感激，我真的很期待把这个放出来，也祝 DeepMind 好运，因为你几天后就要去那里了，真的，对吗？

罗欣(51:04):
是的，我周一会去那里。又是两个工作日。

耶雷米(51:09):
好吧，是的，享受这个漫长的周末吧。

罗欣(51:13):
凉。谢了。

耶雷米(51:13):
太棒了，非常感谢，罗欣。