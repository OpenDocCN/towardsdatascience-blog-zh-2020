<html>
<head>
<title>Unbalanced data loading for multi-task learning in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 中多任务学习的不平衡数据加载</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unbalanced-data-loading-for-multi-task-learning-in-pytorch-e030ad5033b?source=collection_archive---------11-----------------------#2020-01-07">https://towardsdatascience.com/unbalanced-data-loading-for-multi-task-learning-in-pytorch-e030ad5033b?source=collection_archive---------11-----------------------#2020-01-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a245" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在多个不平衡数据集上训练多任务模型的 PyTorch 实用指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cacf674a42ca16c5918d914c272a3e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XrINyU1KA2jnEvqWfKNlQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="http://www.freepik.com" rel="noopener ugc nofollow" target="_blank">由 Kjpargeter / Freepik </a>设计</p></figure><p id="005c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决<a class="ae ky" href="https://ruder.io/multi-task/" rel="noopener ugc nofollow" target="_blank">多任务学习</a> (MTL)问题需要独特的培训设置，主要是在数据处理、模型架构和绩效评估指标方面。</p><p id="dd08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将回顾数据处理部分。具体来说，如何在多个数据集上训练多任务学习模型，以及如何处理高度不平衡数据集的任务。</p><p id="43fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将分三步描述我的建议:</p><ol class=""><li id="5ef2" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">将两个(或更多)数据集合并成一个 PyTorch <em class="me">数据集。</em>该数据集将成为 PyTorch <em class="me">数据加载器的输入。</em></li><li id="64e4" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated">修改批制备过程，以在每批中产生一个任务，或者在每批中混合来自两个任务的样品。</li><li id="e869" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated">通过使用<em class="me">批量取样器</em>作为<em class="me">数据加载器的一部分来处理高度不平衡的数据集。</em></li></ol><p id="173d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我只审查了<em class="me">数据集</em>和<em class="me">数据加载器</em>的相关代码，忽略了其他重要模块，如模型、优化器和指标定义。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="82da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了简单起见，我使用一个通用的两个数据集的例子。然而，数据集的数量和数据的类型不应该影响主设置。我们甚至可以使用同一个数据集的几个实例，以防同一组样本有多组标签。例如，具有对象类别和空间位置的图像数据集，或者具有每个图像的面部情感和年龄标签的面部情感数据集。</p><p id="3073" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个<a class="ae ky" href="https://pytorch.org/docs/stable/data.html#dataset-types" rel="noopener ugc nofollow" target="_blank"> PyTorch <em class="me">数据集</em> </a>类需要实现<code class="fe mr ms mt mu b">__getitem__()</code>函数。该函数处理给定索引的样本获取和准备。当使用两个数据集时，就可能有两种不同的方法来创建样本。因此，我们甚至可以使用单个数据集，获得具有不同标签的样本，并改变样本处理方案(输出样本应该具有相同的形状，因为我们将它们作为批量张量进行堆叠)。</p><p id="62e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们定义两个数据集:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="9720" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们定义两个(二进制)数据集，一个有 10 个 1 的样本(平均分布)，第二个有 55 个样本，50 个数字 5 的样本和 5 个数字 5 的样本。这些数据集仅用于说明。在真实的数据集中，你应该既有样本又有标签，你可能会从数据库中读取数据或者从数据文件夹中解析数据，但是这些简单的数据集足以理解主要概念。</p><p id="26ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们需要定义一个<a class="ae ky" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank"> <em class="me">数据加载器</em> </a>。我们用我们的<em class="me"> concat_dataset </em>提供它，并设置加载器参数，比如批量大小，以及是否重排样本。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="cc3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一部分的输出如下所示:</p><pre class="kj kk kl km gt mx mu my mz aw na bi"><span id="ad7d" class="nb nc it mu b gy nd ne l nf ng">tensor([ 5.,  5.,  5.,  5., -5.,  5., -5.,  5.])<br/>tensor([5., 5., 5., 5., 5., 5., 5., 5.])<br/>tensor([-1., -5.,  5.,  1.,  5., -1.,  5., -1.])<br/>tensor([5., 5., 5., 5., 5., 5., 5., 5.])<br/>tensor([ 5.,  5.,  5.,  5., -5.,  1.,  5.,  5.])<br/>tensor([ 5.,  5.,  5.,  1.,  5.,  5.,  5., -1.])<br/>tensor([ 5.,  5.,  5.,  5., -1.,  5.,  1.,  5.])<br/>tensor([ 5., -5.,  1.,  5.,  5.,  5.,  5.,  5.])<br/>tensor([5.])</span></pre><p id="8b62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每一批都是来自我们的<em class="me"> concat_dataset </em>的 8 个样本的张量。顺序是随机设置的，样本是从样本池中选择的。</p><p id="faf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直到现在，一切都相对简单。这些数据集被组合成一个数据集，并从两个原始数据集中随机选取样本来构建小批量。现在让我们试着控制和操作每批样品。我们希望在每个小批量中只从一个数据集获取样本，每隔一批在它们之间切换。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="5806" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个<em class="me"> BatchSchedulerSampler </em>类的定义，它创建了一个新的样本迭代器。首先，通过为每个内部数据集创建一个<em class="me"> RandomSampler </em>。第二种方法是从每个内部数据集迭代器中抽取样本(实际上是样本索引)。因此，构建一个新的样本索引列表。使用 8 的批量意味着我们需要从每个数据集获取 8 个样本。</p><p id="3be4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们使用一个新的<em class="me">数据加载器</em>运行并打印样本，它将我们的<em class="me"> BatchSchedulerSampler </em>作为输入采样器(使用采样器时，shuffle 不能设置为<em class="me"> True </em>)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="1cad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出现在看起来像这样:</p><pre class="kj kk kl km gt mx mu my mz aw na bi"><span id="a1b2" class="nb nc it mu b gy nd ne l nf ng">tensor([-1., -1.,  1.,  1., -1.,  1.,  1., -1.])<br/>tensor([5., 5., 5., 5., 5., 5., 5., 5.])<br/>tensor([ 1., -1., -1., -1.,  1.,  1., -1.,  1.])<br/>tensor([5., 5., 5., 5., 5., 5., 5., 5.])<br/>tensor([-1., -1.,  1.,  1.,  1., -1.,  1., -1.])<br/>tensor([ 5.,  5., -5.,  5.,  5., -5.,  5.,  5.])<br/>tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  1.])<br/>tensor([5., 5., 5., 5., 5., 5., 5., 5.])<br/>tensor([-1., -1., -1., -1.,  1.,  1.,  1., -1.])<br/>tensor([ 5., -5.,  5.,  5.,  5.,  5., -5.,  5.])<br/>tensor([-1.,  1., -1.,  1., -1.,  1.,  1., -1.])<br/>tensor([ 5.,  5.,  5.,  5.,  5., -5.,  5.,  5.])<br/>tensor([ 1., -1., -1.,  1.,  1.,  1.,  1., -1.])<br/>tensor([5., 5., 5., 5., 5., 5., 5.])</span></pre><p id="2d69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">万岁！！！<br/>对于每个小批量，我们现在只获得一个数据集样本。为了对更重要的任务进行降采样或升采样，我们可以尝试这种调度方式。</p><p id="f59f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们批次中剩下的问题来自第二个高度不平衡的数据集。这是 MTL 经常出现的情况，有一个主任务和一些其他的卫星子任务。一起训练主任务和子任务可能会提高性能，并有助于整体模型的<a class="ae ky" href="https://ruder.io/multi-task/" rel="noopener ugc nofollow" target="_blank">一般化</a>。问题是子任务的样本通常非常稀疏，只有几个正(或负)样本。让我们使用我们以前的逻辑，但是也强制在每个任务中关于样本分布的平衡批处理。</p><p id="1982" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了处理不平衡的问题，我们需要用一个<em class="me">ImbalancedDatasetSampler</em>替换<em class="me"> BatchSchedulerSampler </em>类中的随机采样器(我使用了这个<a class="ae ky" href="https://github.com/ufoym/imbalanced-dataset-sampler" rel="noopener ugc nofollow" target="_blank">资源库</a>中的一个很好的实现)。这个类处理数据集的平衡。我们也可以混合使用<em class="me"> RandomSampler </em>用于一些任务，使用<em class="me">不平衡数据采样器</em>用于其他任务。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="4fbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先创建<em class="me">ExampleImbalancedDatasetSampler</em>，它继承了<em class="me"> ImbalancedDatasetSampler </em>，只修改了<em class="me"> _get_label </em>函数来适应我们的用例。</p><p id="8696" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们使用<em class="me">BalancedBatchSchedulerSampler</em>，它类似于前面的<em class="me"> BatchSchedulerSampler </em>类，只是将<em class="me"> RandomSampler </em>用于不平衡任务的用法替换为<em class="me">examplembanceddatasetsampler</em>。</p><p id="443f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们运行新的<em class="me">数据加载器</em>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="1522" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出如下所示:</p><pre class="kj kk kl km gt mx mu my mz aw na bi"><span id="99ff" class="nb nc it mu b gy nd ne l nf ng">tensor([-1.,  1.,  1., -1., -1., -1.,  1., -1.])<br/>tensor([ 5.,  5.,  5.,  5., -5., -5., -5., -5.])<br/>tensor([ 1.,  1.,  1., -1.,  1., -1.,  1.,  1.])<br/>tensor([ 5., -5.,  5., -5., -5., -5.,  5.,  5.])<br/>tensor([-1., -1.,  1., -1., -1., -1., -1.,  1.])<br/>tensor([-5.,  5.,  5.,  5.,  5., -5.,  5., -5.])<br/>tensor([-1., -1.,  1.,  1.,  1.,  1., -1., -1.])<br/>tensor([-5.,  5.,  5.,  5.,  5., -5.,  5.,  5.])<br/>tensor([ 1., -1.,  1.,  1.,  1., -1.,  1., -1.])<br/>tensor([ 5.,  5.,  5., -5.,  5., -5.,  5.,  5.])<br/>tensor([-1., -1., -1., -1.,  1.,  1.,  1.,  1.])<br/>tensor([-5.,  5.,  5.,  5.,  5.,  5., -5.,  5.])<br/>tensor([-1.,  1., -1.,  1.,  1.,  1.,  1.,  1.])<br/>tensor([-5., -5.,  5.,  5., -5., -5.,  5.])</span></pre><p id="9cb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不平衡任务的小批量现在更加平衡了。</p><p id="ea7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个设置还有很大的发挥空间。我们可以以平衡的方式组合任务，通过将<em class="me"> samples_to_grab </em>设置为 4，这是批量大小的一半，我们可以获得一个混合的小批量，每个任务中有 4 个样本。为了对更重要的任务产生 1:2 的比率，我们可以为第一个任务设置<em class="me"> samples_to_grab=2 </em>，为第二个任务设置<em class="me"> samples_to_grab=6 </em>。</p><p id="33e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样。完整的代码可以从我的<a class="ae ky" href="https://github.com/bomri/code-for-posts/tree/master/mtl-data-loading" rel="noopener ugc nofollow" target="_blank">库</a>下载。</p></div></div>    
</body>
</html>