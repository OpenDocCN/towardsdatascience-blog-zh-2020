<html>
<head>
<title>Playing Treasure Drop with Deep Reinforcement Learning — Part 1/3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用深度强化学习玩宝藏游戏—第1/3部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/playing-treasure-drop-with-reinforcement-learning-tabular-q-learning-part-1-x-2eb789c2ff5e?source=collection_archive---------73-----------------------#2020-05-13">https://towardsdatascience.com/playing-treasure-drop-with-reinforcement-learning-tabular-q-learning-part-1-x-2eb789c2ff5e?source=collection_archive---------73-----------------------#2020-05-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9c20" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">艾玩拼图</h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/0c79c6ac9f4a5c7761dea3c2db061859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CINe-_itkSwA7A3fA-xBYA.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">这是代理在与其他玩家在线对战时“看到”的内容。</p></figure><h2 id="da15" class="kl km iq bd kn ko kp dn kq kr ks dp kt ku kv kw kx ky kz la lb lc ld le lf iw bi translated">在这个系列文章中，我解释了我从事的强化学习(RL)项目。最后，你会知道这是怎么回事:</h2><p id="4955" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated"><a class="ae mb" href="https://youtu.be/FhhPJIZnJ1M" rel="noopener ugc nofollow" target="_blank">观看这个人工智能在实时益智游戏中击败人类玩家</a></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="0d64" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">将强化学习应用于一个新奇的、尚未探索的问题的想法一直吸引着我。我有很多空闲时间在疫情冠状病毒中忙碌。另外，我年轻的时候也玩过这个游戏。这个项目只是在这段时间里的一个完美的侧面。</p><p id="eeb7" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">我讨论了问题的独特和值得注意的方面，我使用了什么技术以及为什么。希望对深度强化学习感兴趣的读者会发现所有这些都很有见地。</p><h2 id="1d91" class="kl km iq bd kn ko kp dn kq kr ks dp kt ku kv kw kx ky kz la lb lc ld le lf iw bi translated">在第1部分(本部分)，</h2><p id="3a44" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">通过缩小游戏桌的尺寸，开始着手解决原始问题的一个更小更简单的版本。这大大减少了状态空间，使问题更容易解决。除了这个小问题之外，还有一个相对简单的技术，适用于更小的状态空间:表格Q-learning。这有助于建立对问题的直觉。这种直觉在后来用深度Q学习网络(DQN)解决问题时非常有用。</p><h2 id="c931" class="kl km iq bd kn ko kp dn kq kr ks dp kt ku kv kw kx ky kz la lb lc ld le lf iw bi translated">在第2部分中，</h2><p id="8c1f" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">在获得了表格方法所需的验证和早期成功之后，使用Keras库实现了DQN来解决与第1部分相同的问题。然而，由于项目的性质，学习过程变得不稳定。从研究论文中借用了几种方法，并实施这些方法来缓解这个问题。即，优先经验重放、目标Q网络、具有学习率查找器的循环学习率。(<a class="ae mb" href="https://medium.com/p/e7097478d402" rel="noopener">第二部</a>)</p><h2 id="1a0c" class="kl km iq bd kn ko kp dn kq kr ks dp kt ku kv kw kx ky kz la lb lc ld le lf iw bi translated">最后，在第3部分中，</h2><p id="b839" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">跟踪学习进度和绘图是深度学习研究的关键。为了跟踪学习进度，解释了启发式度量，并绘制了情节迭代的MSE损失。经过训练的模型相互挑战，以了解哪个学习过程产生更好的代理。代理接受不同回合游戏(1、2、3和4)的培训，并被组织起来进行在线表演。解释了从计算机屏幕获取输入和执行动作的框架。接受来自屏幕的输入，点击适当的点，代理能够击败大多数玩家。 ( <a class="ae mb" href="https://medium.com/p/e4a2992112a1" rel="noopener">第三部分</a>)</p><p id="8167" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated"><strong class="li ja">1/3部分的GitHub知识库:</strong><a class="ae mb" href="https://github.com/arapfaik/td-deepreinforcementlearning-part1" rel="noopener ugc nofollow" target="_blank">TD-deepreinforcementlearning-Part 1</a></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h2 id="541f" class="kl km iq bd kn ko kp dn kq kr ks dp kt ku kv kw kx ky kz la lb lc ld le lf iw bi translated">在这一部分，</h2><p id="34cd" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">我们的问题域，<em class="mo">宝藏掉落</em>的游戏，是按照任何强化学习应用的五大要素来解释和细分的:<strong class="li ja"> <em class="mo">环境</em>、<em class="mo">智能体</em>、<em class="mo">状态</em>、<em class="mo">动作</em>和<em class="mo">奖励</em> </strong>。接下来，学习算法被简单地提及并且给出了为什么它适合这个问题的原因。接下来，定义了收敛的启发式度量，我们的模型由这两个度量以及(有趣的部分)与之对抗来评估。</p><p id="ffd2" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">让我们从定义环境开始。</p><h1 id="6ea3" class="mp km iq bd kn mq mr ms kq mt mu mv kt mw mx my kx mz na nb lb nc nd ne lf nf bi translated">环境</h1><p id="e15c" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">每个玩家轮流把一枚硬币一次扔进棋盘的顶部。这个游戏的目标是使硬币从棋盘底部掉落到投币口。因此得名，<em class="mo">宝降！</em>积分值越大越好。关于游戏的更多描述:<a class="ae mb" href="https://yppedia.puzzlepirates.com/Official:Treasure_Drop" rel="noopener ugc nofollow" target="_blank">链接到官方网站</a></p><p id="2359" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">如果你对最终代理在网上与真实玩家的对战表现很好奇，可以随意看看下面的视频。为了更好地理解游戏，强烈建议观看，因为仅仅通过文字描述来解释游戏动态是很有挑战性的。</p><p id="c4a9" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated"><strong class="li ja">观察:代理人在“真实世界”中的表现</strong></p><p id="3144" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">正如你可能在视频中看到的，这个游戏分四轮进行。每一轮，棋盘底部的分数都不一样。为了简化模型训练，忽略了轮次，它们不是模型的输入。对于给定的模型，只考虑第一轮(第一轮)。</p><p id="3472" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">你可能想知道，视频中的代理人在4轮游戏中表现如何？答案是，实际上有4个模型，每一个都被单独训练了不同的回合，不同的分数，用一个包装类放在一起。根据游戏回合的不同，只有一个被激活。</p><p id="8369" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">游戏机制是用python模拟的，以创建一个训练环境。<code class="fe ng nh ni nj b">TreasureDrop</code>类是游戏的抽象。下面是<code class="fe ng nh ni nj b">printState()</code>方法的控制台输出。另外，代理类的<code class="fe ng nh ni nj b">demo()</code>方法让你通过从控制台接受输入来对抗它。在这个项目的过程中，看到经纪人如何与我对抗是非常有用的。</p><p id="556b" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">demo()方法还打印预测的q值。由于熟悉这个游戏，我能够很快判断q值是否有意义。在下图中，“O”表示槽中有硬币的杠杆，而“V”表示没有硬币的杠杆。现在不要担心回合，这部分只考虑第一回合。想象他们不存在。</p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi nk"><img src="../Images/9411e64a5d3b532689d33a294377a7ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y2CRQSGC_CN-fxMerTsRfA.png"/></div></div></figure><p id="80e1" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">上面的板子是4x5的。纸板尺寸是指{顶行宽度} x {纸板高度}。在任何项目中从小处着手都是好的。针对较小的董事会规模培训代理更容易。此外，它还验证了这个问题可以通过强化学习技术来学习的假设。对于这一部分，我们将解决较小的电路板尺寸，例如:</p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi nk"><img src="../Images/22ab10b2c3e3a5cf3672acf9086b8016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KYV7pQvR-HU1dQHhvLK4vA.png"/></div></div></figure><p id="0314" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">为了应用一种成功率更高的技术，并验证我的假设，即这个游戏是可以学习的，首先使用表格的方法来解决这个问题。使用较小的尺寸是为了使问题可行。在原始(4x5)游戏表中有2⁶⁰可能的状态。用2⁶⁰条目构建一个表在计算上是不可行的。但是上面的尺寸还可以，</p><h1 id="d4d4" class="mp km iq bd kn mq mr ms kq mt mu mv kt mw mx my kx mz na nb lb nc nd ne lf nf bi translated">状态</h1><p id="f676" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">这将我们的讨论带到了<em class="mo">状态</em>的定义。游戏有几个数据点可以包含在状态中，并且可以被模型感知。也就是说，回合、分数、轮到谁以及杠杆的位置。为了保持模型的简单性和训练过程的鲁棒性，只有集体<em class="mo">杆状态</em>被认为包括在状态中。<em class="mo">杆状态</em>是单个杆的状态，编码如下:</p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi np"><img src="../Images/a56d43f9fc5ddbac9c85c2ef591fed8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JzEPfixibHqN9dJanT0e8w.png"/></div></div></figure><p id="b647" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">正如你在视频中可能注意到的，这正是模型对杠杆的感知。游戏的<em class="mo">状态</em>只是单个<em class="mo">操纵杆状态</em>的集合。每个杠杆状态从左到右、从上到下相加，顺序与书面英语相同。给定游戏桌的相应状态如下所示:</p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi np"><img src="../Images/f3621e3567eb8b0ec09c9b14de376cfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcZfHYV_FTBarThh12SzbQ.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated"><strong class="bd nq">状态</strong>是模型的输入。它是通过连接杠杆状态组合在一起的。</p></figure><h1 id="77e0" class="mp km iq bd kn mq mr ms kq mt mu mv kt mw mx my kx mz na nb lb nc nd ne lf nf bi translated">行动、奖励和下一个状态</h1><p id="997e" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">一个<em class="mo">动作</em>是指从游戏桌面的任何一个投币口投出一枚硬币。插槽的索引从0开始，向右移动时递增1。根据所采取的<em class="mo">动作</em>，一个<em class="mo">奖励</em>和<em class="mo">下一个状态</em>将确定地跟随。奖励是采取行动后获得的总点数的总和。这是由硬币到达棋盘底部的分数决定的。</p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/e236c73a528f20aee0423ed8a7ec1901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rruZ6uICSCpqDIc9eTSLvw.png"/></div></div></figure><h1 id="bbef" class="mp km iq bd kn mq mr ms kq mt mu mv kt mw mx my kx mz na nb lb nc nd ne lf nf bi translated">代理人</h1><p id="5364" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">政策决定下一步采取什么行动。每个动作的q值将由基础模型决定。代理遵循<em class="mo">贪婪策略</em>，这意味着它将选择具有最高q值的动作。</p><p id="f6f9" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">强化学习是从互动中学习如何表现以达到目标。强化学习代理及其环境在整个训练过程中通过一系列离散的时间步骤进行交互。同样，我们的目标是训练一个能够做出好的决策序列的智能体。</p><p id="04cc" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">这个决策过程被建模为马尔可夫决策过程(MDP)。在MDPs中，我们估计每个州s中每个行动a的值Q(s，a)。q值，𝑄(𝑠，𝑎)是<em class="mo">预期贴现回报</em>如果我们在𝑠州执行行动𝑎，那么从那时起遵循最优策略。这些状态和动作相关的量对于准确地将长期后果的信用分配给单个动作选择是至关重要的。</p><p id="a4f3" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">考虑上图。给定状态s和动作0、1、2和3，我们的策略在Q(s，0)、Q(s，1)、Q(s，2)和Q(s，3)中挑选具有最高<strong class="li ja">Q值的动作。因此，强化学习的任务变成了寻找正确输出Q值的函数Q。</strong></p><p id="7646" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">为了实现这个功能，在这一部分中，我们将为每个(状态、动作)对建立一个包含条目(q值)的表。这就是所谓的Q表。在下面的部分中，这个Q值函数将被近似。然而，表格和DQN实现中的学习算法将保持不变。</p><h1 id="6950" class="mp km iq bd kn mq mr ms kq mt mu mv kt mw mx my kx mz na nb lb nc nd ne lf nf bi translated">学习算法</h1><blockquote class="nr"><p id="02e4" class="ns nt iq bd nu nv nw nx ny nz oa ma dk translated">如果一个人必须确定一个核心的和新颖的强化学习的想法，它无疑将是时间差TD学习。</p><p id="81a5" class="ns nt iq bd nu nv nw nx ny nz oa ma dk translated">—萨顿&amp;巴尔托公司</p></blockquote><p id="df01" class="pw-post-body-paragraph lg lh iq li b lj ob ll lm ln oc lp lq ku od ls lt ky oe lv lw lc of ly lz ma ij bi translated">选择称为TD学习的时间差学习作为学习算法。它的工作是在每个时间步更新Q值函数。这意味着每当代理采取行动时，它将更新Q表中的一些条目。</p><p id="14b7" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">TD方法优于其他强化学习方法的一个优点是，TD方法可以直接从原始经验中学习，而不需要环境的动态模型。此外，TD更新可以在每一步调用，这与蒙特卡罗(MC)方法形成对比，在蒙特卡罗方法中，算法要等到剧集结束时才进行更新。这使得TD以在线方式实现变得非常自然。这个项目中使用的方法称为一步TD，即TD(0)。一步式仅仅意味着更新方程只考虑一步奖励。</p><p id="3ebf" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">最初，所有Q值都被初始化为任意值。代理通过遵循ε-贪婪策略来玩游戏，在每次迭代中，代理丢硬币并获得奖励，在每次迭代中，Q值函数根据贝尔曼方程更新:</p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi og"><img src="../Images/0e5a97e85b252bb272f157381a750593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9CNWwfvW-KN-BiMiyO38IQ.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">贝尔曼方程是Q学习的核心。这也是在TD(0)算法的每一步中更新表格的方式。</p></figure><p id="0081" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">对于固定大小的策略，TD(0)被证明确定性地收敛到算法的表格实现的单个答案。</p><h1 id="4f7b" class="mp km iq bd kn mq mr ms kq mt mu mv kt mw mx my kx mz na nb lb nc nd ne lf nf bi translated">对贝尔曼方程的修正</h1><p id="30f6" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">由于问题的性质，需要对贝尔曼方程做一个小的改动。在一些流行和众所周知的强化学习问题中，如<a class="ae mb" href="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html" rel="noopener ugc nofollow" target="_blank"> gridworld </a>、<a class="ae mb" rel="noopener" target="_blank" href="/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055">月球着陆器</a>和<a class="ae mb" rel="noopener" target="_blank" href="/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288"> cartpole </a>，下一个状态的动作也是由agent决定的。换句话说，下一个状态属于同一个代理。我们的问题是一个回合制游戏，下一个状态不属于代理。事实上，<strong class="li ja">下一个状态其实是属于敌人的。</strong></p><p id="22d7" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">在登月过程中，最好去一个q值高的州。然而在我们的问题中，情况恰恰相反。代理人必须确保在收集最多奖励的同时，下一个状态必须总是尽可能低的值。重复一遍，(下一个状态，行动)的最大值对我们来说和奖励本身一样重要。这是一个零和游戏，获胜的策略是在获得奖励的同时限制敌人的奖励。</p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/9ca3c403674dfe66dd7e95abd2378dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*sTjVwXJKP1VqzTJNNu0L_w.gif"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">有经验的最终代理对在线对手。</p></figure><p id="e1ae" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">代理使用用户名“Playwme”进行游戏。虽然并不总是如此，但它通常会避免对手采取这样的行动，因为它知道下一个状态可能会给敌人带来比短期奖励更多的分数。由于这种现象，特工<strong class="li ja">必须</strong>确保敌人不会做出太有价值的行动，至少不会比我们得到的奖励更好。</p><p id="e64b" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">这个关于游戏动力学的知识需要对TD(0)算法使用的更新方程(贝尔曼方程)做一个小的改变，以适应这个问题。</p><p id="6830" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">由于这种特定于领域的现象，将属于<em class="mo">折扣因子</em>的加号以及最优未来值的<em class="mo">估计值切换为负数是有意义的。在下面的修正贝尔曼方程中，它用红色表示:</em></p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi og"><img src="../Images/79dd9f7155449ec11eb4e071bd1c5dda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tHEQW47YFxdyIzeY52x69w.png"/></div></div></figure><p id="6205" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">用书面英语翻译这个变化，意思是:{状态值，动作} = {我得到的奖励}–{下一个状态的最大值，属于敌人的动作}</p><h1 id="ba88" class="mp km iq bd kn mq mr ms kq mt mu mv kt mw mx my kx mz na nb lb nc nd ne lf nf bi translated">超参数</h1><p id="b958" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">这里没有太多的超参数调整，因为这只是一个玩具例子。但是，提到探索率的影响是值得的。</p><p id="eee5" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">探索率对主体在整个学习过程中遇到的状态分布有影响。<strong class="li ja">在数据科学中，数据的底层分布决定了模型学习的内容。</strong></p><p id="552d" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">如果探索率较低，(大约0.05–0.1)代理人会比探索率较高的情况更频繁地暴露于竞争游戏中可能遇到的状态。相反，如果探索率是1，代理随机地玩，并且当动作被随机地挑选时暴露于状态。就像人类一样，代理人将会更多地了解它所接触到的东西。</p><p id="1062" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">让我们假设有一个<em class="mo">边缘状态</em>，一个不经过深入探索就不可能达到的状态。如果勘探率较低，则不能很好地达到该状态，从而导致表中的q值不太准确。让我们假设有一个<em class="mo">核心状态</em>，这是一个在竞技游戏中经常遇到的重要状态，为了更多地暴露于该状态，探索率需要较低。代理越多地暴露于核心状态，它就越知道在竞争游戏中面对核心状态时如何表现。</p><p id="ee5f" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">无论探索率是0.05还是1，Q-table迟早会用正确的Q值填充，这取决于表的大小。边缘和核心状态被探索只是时间问题，取决于探索速度。TD(0)的表格实现保证收敛到正确的解。</p><h1 id="4809" class="mp km iq bd kn mq mr ms kq mt mu mv kt mw mx my kx mz na nb lb nc nd ne lf nf bi translated">模型验证</h1><p id="45ba" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">虽然不是最科学的，但验证模型最有趣的方式是和它玩。作为一种测试模型的自动化方法，没有像登月那样直接的方法，在登月中，累积奖励是有意义的。然而，我们有几种方法可以获得模型捕捉正确q值能力的“启发式”评分。其中之一是查看下面两个镜像电路板的q值。两个镜像板简单地意味着游戏桌被y轴颠倒。</p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oi"><img src="../Images/cf6df9e355032fd91decda43c4266105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Fc7NJwt6-9nxG1FyoeCKg.png"/></div></div></figure><p id="e4a1" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">注意到彼此对应的动作有非常接近的Q值吗？为什么这很重要？如果与另一张桌子上的等效动作相对应的动作的Q值非常接近，这意味着我们的模型可以看到这两个动作带来了相同的奖励并过渡到等效状态。值得注意的是，该模型能够捕捉到这一点，而无需我们提供游戏机制或让模型知道这两种状态是彼此沿y轴的镜像。</p><p id="f1f7" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">注意，分数可能不存在，更不用说接近了，如果状态是在竞争游戏期间不可能发生的。毕竟，我们在这里处理的是一个统计模型，没有魔法。如果代理在训练中没有遇到这种状态，它很可能不会对此有任何意见。</p><p id="c222" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">另一个启发是基于这样的事实，一些相邻的动作对转移到相同的下一个状态，并获得相同的回报。更具体地说，如果最上面一排的杠杆上有一枚硬币，那么向左和向右的动作对该杠杆产生相同的效果，因此对整个棋盘也产生相同的效果。</p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oi"><img src="../Images/9513737946481c5400fd183b61a43467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wY0H0PQw6GgndcrIDfuGrQ.png"/></div></div></figure><p id="a27d" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">如上图所示，0 &amp; 1和4 &amp; 5的Q值非常接近。这是因为简单地说，两个动作都会导致相同的回报和下一个状态，都是通过移动杠杆上的硬币。如果代理人发现了这一点，很可能就发现了整个游戏。</p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oj"><img src="../Images/9dbeaebdc1499a2b7d1eb2c3cf0eeadc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*csQnFeSS_H6zea2xEjqD_w.gif"/></div></div></figure><p id="1879" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">虽然动作2和动作3彼此相邻，但情况并非如此。因此，这两个动作共享一个非常接近的q学习是有意义的。同样，如果模型可以说这些动作的q值非常接近，这是一个强有力的指标，表明模型非常了解这个游戏。</p><h1 id="89e9" class="mp km iq bd kn mq mr ms kq mt mu mv kt mw mx my kx mz na nb lb nc nd ne lf nf bi translated">⚔️行动模型</h1><p id="38ec" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated"><a class="ae mb" href="https://github.com/arapfaik/td-deepreinforcementlearning-part1" rel="noopener ugc nofollow" target="_blank">链接到GitHub库</a></p><p id="ff8a" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">去查查笔记本吧。GitHub资源库中有两个笔记本。一个用于从已经计算的q表中运行代理。另一方面，你可以从头开始训练一个代理，只需要一路运行细胞。</p><p id="b979" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">看你能不能打败它！</p><figure class="nl nm nn no gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ok"><img src="../Images/8cd33f3ee7dd6f8865526b5288297437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HcGK3ogUX4bpz9x0JHZrUg.png"/></div></div></figure><h1 id="b7fc" class="mp km iq bd kn mq mr ms kq mt mu mv kt mw mx my kx mz na nb lb nc nd ne lf nf bi translated">下一步是什么？</h1><p id="ed78" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">现在我们知道这个问题可以通过强化学习方法来解决。在第2部分中，我们将DQN的表格实现转换为适用于更大的问题。</p><p id="6c95" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">表格法往往能找到精确解，即往往能精确找到最优值函数和最优策略。这与下一部分中描述的近似方法形成对比，后者只能找到近似解，但反过来可以有效地应用于更大的问题。</p><p id="3d95" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated"><a class="ae mb" href="https://medium.com/p/e7097478d402" rel="noopener">链接到第二部分</a></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="c593" class="mp km iq bd kn mq ol ms kq mt om mv kt mw on my kx mz oo nb lb nc op ne lf nf bi translated">关于我</h1><p id="f26e" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">我是一名数据科学家，住在旧金山。热衷于从数据中寻找答案。在Linkedin上找到我:<a class="ae mb" href="http://linkedin.com/in/sakarya" rel="noopener ugc nofollow" target="_blank">梅尔·萨卡里亚</a></p><h1 id="50ae" class="mp km iq bd kn mq mr ms kq mt mu mv kt mw mx my kx mz na nb lb nc nd ne lf nf bi translated">参考</h1><p id="2120" class="pw-post-body-paragraph lg lh iq li b lj lk ll lm ln lo lp lq ku lr ls lt ky lu lv lw lc lx ly lz ma ij bi translated">[1]: <a class="ae mb" href="https://yppedia.puzzlepirates.com/Official:Treasure_Drop" rel="noopener ugc nofollow" target="_blank">宝藏掉落:益智海贼官网</a></p><p id="f693" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">[2]:萨顿&amp;巴尔托。<a class="ae mb" href="https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation-ebook/dp/B07JN1QFW5/ref=sr_1_2?crid=352FL757QDCAK&amp;dchild=1&amp;keywords=sutton+and+barto+reinforcement+learning&amp;qid=1587862288&amp;sprefix=sutton+barto+rein%2Caps%2C249&amp;sr=8-2" rel="noopener ugc nofollow" target="_blank"> <em class="mo">【强化学习】</em> </a> <em class="mo"> </em> (2018)</p><p id="2ed5" class="pw-post-body-paragraph lg lh iq li b lj mj ll lm ln mk lp lq ku ml ls lt ky mm lv lw lc mn ly lz ma ij bi translated">[3]:古德费勒，本吉奥&amp;库维尔。<a class="ae mb" href="https://www.amazon.com/Deep-Learning-NONE-Ian-Goodfellow-ebook/dp/B01MRVFGX4/ref=sr_1_3?dchild=1&amp;keywords=deep+learning&amp;qid=1587862365&amp;sr=8-3" rel="noopener ugc nofollow" target="_blank"><em class="mo"/></a><em class="mo"/>(2016)</p></div></div>    
</body>
</html>