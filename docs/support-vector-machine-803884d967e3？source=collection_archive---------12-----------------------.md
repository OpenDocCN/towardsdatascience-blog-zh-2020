# 支持向量机

> 原文：<https://towardsdatascience.com/support-vector-machine-803884d967e3?source=collection_archive---------12----------------------->

## 对 SVM 模式的深入探究

在这篇文章中，我们将讨论使用支持向量机(SVM)作为分类模型。我们将从探索其背后的思想开始，将这个思想转化为一个数学问题，并使用二次规划(QP)来解决它。

先从分析模型背后的直觉开始。线性分类算法的目标是将输入空间划分为由不同类别标记的区域。这样，我们可以根据任何新数据点所属的区域来预测它的类别。不同的统计模型在如何找到这个决策边界上存在分歧，在本帖中，我们将讨论 SVM 使用的策略。

下面我们有一个包含两个类的数据集。这里我们展示了由线 1 和线 2 表示的两个决策边界。很容易看出，有许多其他线可以以所有观察值都被正确分类的方式划分输入空间——事实上，有无限多条线。

那么 SVM 是如何选择路线的呢？主要有两个思路。注意点 A 离线 1 有多远。根据线 1 定义的决策边界，我们似乎可以直观地得出这样的结论:预测点 A 属于圆类比预测点 b 属于圆类更有把握。SVM 旨在最大化其在训练集中的预测可信度。还要注意，有许多点非常靠近第 2 行:这意味着对决策边界参数的任何调整都可能导致不正确地预测其中一些点的类别。这两个想法结合起来形成了 SVM 背后的直觉。

注意:在这篇文章中，你会看到超平面这个词的使用。超平面只是一个比其周围空间少一个维度的子集的花哨名称，SVM 决策边界就是这种情况。这意味着，如果输入空间在ℝᵖ，则决策边界具有(p-1)维。例如，在下图中，输入空间位于ℝ，因此 SVM 决策边界是一维的:一条线。

![](img/fe8198d7cd442a2e9c3e98c79da404c5.png)

*作者图片*

# 利润

SVM 是一个线性分类模型。对于输出 y ∈ {-1，1}，我们可以将假设函数写成输入的线性组合:

![](img/f91ea102c21b8b4318d0b521541e4010.png)

我们预测:

![](img/2d79633c53bfa9dcf9973f40dd4895b9.png)

直觉上，假设值离零越远，我们对自己的预测就越有信心。如果 h(x) = 10，我们比 *h(x) = 0.1* 时更有信心预测 y=1。

这可能会导致这样的想法，即我们希望找到参数( **w** ，b)，当 y⁽ⁱ⁾ = 1 时，该参数将最大化 h 的值，而当 y⁽ⁱ⁾ = -1 时，该参数将最小化 h 的值。换句话说，我们可能倾向于最大化**功能边际** γ̂:

![](img/84ee957bd4190c4680fa16f2fef58b6b.png)

但这有一个问题:注意预测的类如何只依赖于 h 的符号。这意味着我们可以缩放参数，例如( **w** ，b) → (10 **w** ，10b)，而不改变预测的类。这会将 h 的值缩放 10 倍，并给出一个错误的想法，即我们的模型对其预测有 10 倍的信心。

这就是几何余量概念的由来。**几何余量** γ̂被定义为第 I 次观察到判定边界的距离。与功能裕度不同，该度量对于参数的缩放是不变的。毕竟 **w** ᵀ **x** + b = 0 定义的超平面和 10**w**ᵀ**x**+10b = 0 定义的超平面是一模一样的。

SVM 寻找离每个类最近的成员尽可能远的超平面。看下面的图，我们可以看到 P₁和 P₂是每个班级最接近的观测值。SVM 判决边界与 P₁和 P₂等距，即 d₁ = d₂.最后也是最重要的一点是，在对训练集中的每个观察值进行正确分类的所有决策边界中，支持向量机具有最大的最小距离 d₂).min(d₁

换句话说，如果我们定义γ =最小γ⁽ⁱ⁾，SVM 寻找具有最大γ值的超平面。

> SVM 旨在最大化最小几何余量。

![](img/729897db76cf2ddac8ff887edbe98503.png)

*作者图片*

在继续之前，我们需要谈论一件处理 SVM 时非常重要的事情:**特征缩放**。由于 SVM 优化的目标是基于几何距离，具有非常不同的缩放比例的轴使模型倾向于具有最大值的方向。

例如，在下图中，我们可以看到，在缩放要素之前，SVM 会寻找一个决策边界，以使距离矢量 d₁具有尽可能大的垂直分量。这就是为什么我们应该在拟合 SVM 之前应用要素缩放。

> 在拟合 SVM 之前，请始终缩放要素

![](img/76f2e8b815d5b1e03eea7a5ed987d081.png)

*作者图片*

# 非线性分类

正如我们之前所说，SVM 是一个线性模型，这意味着它擅长在数据中寻找线性关系。但是正如你所想象的，大多数真实世界的数据集不是线性的。虽然我们不能改变 SVM 本身的性质，但我们可以改变输入模型的内容。让我们看看为什么以及如何帮助。

下图很好地说明了更改输入如何帮助 SVM 模型建立非线性关系。在左边，数据显然不是线性可分的，也就是说，我们找不到一个点，它的右边是一个类，而左边是另一个类。

但是，让我们看看当我们向这个数据集添加一个额外的特征时会发生什么:即，每个观察到点(0)的距离。这意味着在原始输入空间上由(-3)定义的点现在由对(-3，3)表示；类似地，由(5)表示的点变成(5，5)。我们正在将输入空间从一维重新映射到二维。正如您在右图中看到的，这个强大而简单的技巧使数据在ℝ中可以线性分离，因此更适合 SVM。

> 我们可以通过对原始特征应用非线性变换来避开模型的线性限制。

![](img/6ec1aef2daa11ba12a428e3a5214e240.png)

*作者图片*

实现这一思想的一种常见方式是向训练集添加更高次的多项式。下图显示了一个具有两个要素(x₂x₁)的数据集，该数据集在ℝ显然不是线性可分的。

我们需要以一种数据变得线性可分的方式来转换输入(不要忘记在之后缩放它！).我们将添加三次多项式特征，重新映射从ℝ到ℝ ⁰的输入空间，使得(x₁，x₂)变成(1，x₁，x₂，x₁x₂，x₁，x，x。不幸的是，我们看不到很酷的 10 维图，所以我们满足于用原始特征来表示 2D。请注意，要对新数据点进行预测，我们需要在将其输入 SVM 之前应用相同的多项式变换。

![](img/c5547f0822696fcfc66b5c2171ae96fd.png)

*作者图片*

另一种解决 SVM 线性限制的方法是使用**相似性度量**，它是量化两个数据点之间相似性的简单函数。最常见的相似性函数之一是*高斯径向基函数* (RBF)，其定义为:

![](img/c306b10aae2bd76afd5b69fc808acf4b.png)

我们可以选择几个标志点，并将输入重新映射到包含到每个标志点的距离的向量。因此，如果我们有三个标志点( **l** ₁、 **l** ₂、 **l** ₃)，并且我们正在使用径向基函数相似度函数ϕ，那么原本由 **x** 表示的观察结果就变成了(ϕ( **x** 、 **l** ₁)、ϕ( **x** 、 **l** ₂)、ε(**x**、)

但是我们如何挑选数据点作为地标呢？一种策略是只选择训练集中的每一点。这显然不能很好地扩展，因为我们为每个新的观察值创建一个额外的特征，这样一个具有 500 个观察值和三个特征的训练集就变成了一个具有 500 个观察值和 500 个特征的训练集(假设我们丢弃了原始特征)。

在这篇文章的后面，我们将介绍一种以更加优雅和高效的方式实现这个想法的方法。现在，我们可以在下面看到这个概念的一个更简单的实现，使用训练集中的每个观察值作为一个标志，以及它如何影响 SVM。

![](img/28e1a2446a882f9b48b85587cecd4819.png)

*作者图片*

# **模型背后的数学原理**

如前所述，SVM 是一个线性分类模型，我们可以将其假设函数写成输入的线性组合:

![](img/4de375799bef47e946796aaf90f09f2b.png)

我们预测:

![](img/f3f51a6248fb463e7d1439a52b36e0d3.png)

现在是时候将寻找最大几何余量的 SVM 目标转化为一个数学问题了。给定一个点 **x** ⁽ⁱ⁾，它到由 **w** ᵀ **x** + b = 0 定义的决策边界的距离是多少？嗯，我们知道 **w** 是这个超平面的法向量，也就是说它垂直于平面。而如果 **w** 是法向量，那么 **w** /|| **w** ||就是垂直于平面的单位向量。

![](img/0e442b5c83e5e3da3ffa06e2023e71f0.png)

*作者图片*

现在让γ是从 **x** 到超平面的距离——我们可以说**x**-γ*(T24)w/| |**w**| |)落在超平面上(在上图中用 **p** 表示)。最后由于 **p** 是超平面上的向量，这意味着它必须满足超平面的方程 **w** ᵀ **p** + b = 0。这给了我们下面的方程组:

![](img/0ed4a2af1950ad9e0febaa15b8053a77.png)

而如果我们求解γ，我们发现一个点 **x** ⁽ⁱ⁾到 SVM 判决边界的距离由γ⁽ⁱ⁾=(**w**ᵀ**x**⁽ⁱ⁾+b)/| |**w**| |给出。注意这个等式只对与法向量 **w** 方向相同的点有效，因为我们从 **x** 中减去了它；否则我们将不得不添加γ **w /w** ||。稍微调整一下，我们可以将广义的**几何余量** γ写成:

![](img/bba7c9eaa99be502e71b4a06ab6d91f2.png)

我们可以再次验证，如前所述，几何余量对参数的缩放是不变的。还要注意，如果|| **w** ||= 1，那么几何余量和功能余量是相同的。

## 硬边际 SVM

硬边界 SVM 假设数据是线性可分的，它强制模型对训练集上的每个观察值进行正确分类。如果第 I 个观察值的功能裕度大于零，则该观察值被正确分类:

![](img/88f51ef77fc847c72fd1932839deb055.png)

还记得我们说过预测的类只依赖于 h 的符号，几何边界对于参数的缩放是不变的吗？这意味着我们可以在不影响模型的情况下向参数添加任意约束。对于任何拟合的 SVM，我们可以施加一个约束，如|| **w** || = 1，然后我们需要做的就是重新调整其参数，模型保持不变。

我们可以使用这个结果以及当|| **w** || = 1 时几何和功能裕度相等的事实来写出硬裕度 SVM 的优化目标:

![](img/daac27b01e31a446e105ea5293bc9771.png)

第一个约束迫使每个观察值被正确分类。第二个约束迫使γ不仅是功能裕度的下限，而且是几何裕度的下限。这正是**硬边际 SVM** 的优化目标:

> 为了在没有任何错误分类的情况下最大化最小几何余量

如果我们有工具来解决这个约束优化问题，我们就可以到此为止了。但是不幸的是，|| **w** || = 1 是一个非凸约束，所以我们需要做一些修改来使这个问题变得更加友好。

我们可以通过将目标除以范数来摆脱讨厌的约束——如果γ是功能裕度的下界，那么γ/|| **w** ||是几何裕度的下界。所以我们的新问题可以写成:

![](img/c07632f7e9a8ce43f58cb3840172f680.png)

现在是目标函数非凸，但是我们又近了一步。记住最后一次，我们可以给参数添加任意约束。因此，我们可以设置γ = 1，同样，这不会改变模型，只需重新调整参数即可满足要求。

我们新的优化目标是最大化 1/|| **w** ||，这相当于最小化|| **w** ||。最后，由于|| **w** ||在 0 处不可微，我们将最小化(1/2)*|| **w** ||取而代之，它的导数就是 **w** 。优化算法在可微分函数上工作得更好。

最后，我们将**硬边际 SVM** 优化目标定义为:

![](img/b27d40e3de4312b93f3fa819e7ff94fa.png)

既然我们已经以一种可以有效估计的方式定义了 SVM 优化目标(一个二次目标和线性约束)，我们就可以编写代码来解决它了。通读下面的代码(或者在这个 [repo](https://github.com/victorvrv/blog-resources/blob/master/2-SVM/models.ipynb) 中)或者尝试使用 [SciPy 的最小化](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)函数自己实现它。如果你阅读文档，你会看到要最小化的目标函数以一个列表作为参数，所以我们将把 **w** 和 b 放在同一个向量中。它还需要一个 *_constraints_* 参数，该参数可以表示一个等式(约束函数必须返回 0)或一个不等式(约束函数必须返回非负值)。我们将把广义约束重写为 y⁽ⁱ⁾(**w**ᵀ**x**⁽ⁱ⁾+b)——1≥0，以符合 *_SciPy_* 的规范。

现在我们可以看到我们模型与 *_SKLearn_* 的实现进行对比时的表现。不算太差！

![](img/87f8cd12c4f392d53ce48f15a9b91097.png)

*作者图片*

## 软利润 SVM

硬边际 SVM 有两个非常重要的局限性:

-它只对线性可分数据有效；

-对异常值非常敏感。

如果我们想要更多的灵活性，我们需要为模型引入一种允许错误分类的方法，我们使用松弛变量的概念来实现这一点。每个观察都有自己的松弛度量，它代表了特定数据点违反边界的程度。我们的新约束可以改写为:y⁽ⁱ⁾(**w**ᵀ**x**⁽ⁱ⁾+b)≥1-ϵ⁽ⁱ⁾.请注意，我们现在需要为每个观察值估计一个额外的变量。

现在只有一个问题。如果我们仍然试图最小化|| **w** ||，我们可以简单地让ϵ → ∞并且确保新的约束将被遵守。我们需要新的目标函数来表示软利润 SVM 冲突目标:我们仍然希望利润尽可能宽，我们希望松弛变量尽可能小，以防止利润违规。这为我们提供了**软边缘 SVM** 分类器目标:

![](img/7cd30ad08816b90b2d30b7ce51df01ad.png)

我们引入了一个新的超参数 C，让我们控制冲突目标之间的权衡。随着 c 的增长，它迫使优化算法为ϵ.寻找更小的值下面你可以看到不同的 C 值是如何影响 SVM 的。

![](img/84e16b4cf41c95543b090ce524a5e799.png)

*作者图片*

在我们对软利润 SVM 的实现进行编码之前，我们需要引入一个新概念。

# 二次规划

二次规划(QP)是一个解决特定类型的优化问题的系统，其中目标函数是二次的，约束是线性的-正是我们一直在处理的问题类型！

不同的算法如何解决 QP 优化超出了本文的范围。相反，我们将专注于推导 SVM 优化问题，该问题由像 [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) 这样的库来解决。

给定 QP 问题定义为:

![](img/95ccf91da0781da1bd57c6096131e67b.png)

我们需要做的就是找出代表 SVM 问题的 P，q，G，h 的值，然后我们可以使用一个求解器(稍后我们将使用 [cvxopt](https://cvxopt.org/userguide/coneprog.html#quadratic-programming) 来得到参数的估计值。

注意，我们需要重写 SVM 约束来适应 QP 约束的形状。它们将是:

![](img/2a93646f4c244ca9a19dbcc2f868b392.png)

因此，对于**软余量 SVM** ，让 X 是由 m×(n+1)矩阵(m 个观察值和 n 个特征)表示的训练集，用 1 列向量为截距进行左填充。然后 SVM·QP 问题被定义为:

![](img/f1723d0efac0b3a04b9eec68d0a6d08b.png)

为什么由于我们需要估计 b， **w** ，和 **ϵ** ，QP 问题的向量 **u** 就变成了【b，w₁，…，wₙ，ε₁，…，εₘ].然后:

-**p**矩阵用恒等式把 **w** 弄回来，用零抵消 b 和**ϵ**；

——**q**向量将 b 和 **w** 乘以 0，将 **ϵ** 乘以 c，这样我们就有了优化目标的第二部分——∑ᵢ₌₁ᵐε⁽ⁱ⁾；

——**k**矩阵给我们-y⁽ⁱ⁾(**w**ᵀ**x**⁽ⁱ⁾+b ),它右边的-Iₘ减去εᵢ. **G** 矩阵的底行代表-ϵ⁽ⁱ⁾≤0；

-最后 **h** 向量有-1 和 0，就像约束条件一样(≤ -1 和≤ 0)。

现在我们所要做的就是把这些值给 QP 软件，我们就有了**软边际 SVM** 的参数。你可以查看下面实现这个想法的代码(或者在这个 [repo](https://github.com/victorvrv/blog-resources/blob/master/2-SVM/models.ipynb) 里)。我们已经取得了很大进展，但我们还没有完成。

还记得我们在本文前面讨论过相似性度量作为解决 SVM 线性限制的一种方法吗？事实证明，对于这个问题，有一个比明确地将每个观察作为一个标志并转换输入更优雅和有效的解决方案。

我们接下来将讨论拉格朗日对偶。这将导致我们对软边际 SVM 优化问题的不同表示(称为其对偶形式)。我们将能够以更有效的方式在输入空间中应用非线性变换，使得 SVM 即使在非常高的维度中也能很好地工作。

# **拉格朗日对偶**

拉格朗日方法的总体思想是通过将约束移入目标函数，将约束优化问题(**原始形式**)转化为无约束优化问题(**对偶形式**)。将 SVM 最优化问题写成对偶形式有两个主要原因:

- **内核技巧**:SVM 的训练和预测将仅通过它们的内积依赖于数据点。这个强大的结果允许我们对训练集应用任何变换(甚至将其重新映射到无限维空间！)只要我们能计算出那个内积；

- **支持向量**:预测将仅依赖于训练集上称为支持向量的几个点。对偶形式给了我们一个简单的方法来识别这些点。

希望在这篇文章结束时，这些原因会变得清晰。现在，让我们暂时把 SVM 放在一边，考虑一下如何解决下面这样的约束优化问题。

![](img/4e82211add2e48f0317f2bca8a286a81.png)

为简单起见，我们将分析一个只有不等式约束的问题(如 SVM)，但结果可以很容易地扩展到等式。重写这个问题的一种方法是使用无限阶跃函数 g(u):

![](img/25022e260c53ad495890ba0aed92f4f7.png)

然后，我们可以将原问题改写为:

![](img/dfc9664a5731d636c3590e0001bfd9a3.png)

请注意，如果任何原始约束不满足，我们用正无穷项惩罚 J(x)。并且当所有的约束条件都满足时，那么 g(fᵢ(x)) = 0 ∀ i 和 J(x) = f₀(x).所以最小化 J(x)等价于原问题，但是我们还不能真正解决这个新的优化——阶跃函数既不连续也不可微。

如果我们用一个像 g(u) = α u 这样的线性函数代替无限阶跃函数会怎么样？如果我们强制α ≥ 0，那么当不满足约束时(α u > 0)，惩罚至少是在正确的方向上。此外，g 现在是连续可微的:更适合于最优化问题。

因此，如果我们在上面的 J(x)方程中代入新的线性惩罚，我们得到 x 和α的函数，称为**拉格朗日函数**:

![](img/7fbe9276f9bd996a122c701405f3aad9.png)

注意，如果我们最大化关于α的拉格朗日函数，我们会得到 j(x):αᵢ= 0 如果 fᵢ(x) < 0 (since αᵢ ≤ 0) and αᵢ → ∞ if fᵢ(x) > 0。所以原来的问题现在可以写成拉格朗日函数:

![](img/3920996e06d1910c2269943266e97317.png)

最后，由于对于任何函数 f，最小最大 f ≥最大最小 f，我们有:

![](img/c3472606304f5270125f5030d4a04595.png)

minₓ L(x， **α** )称为对偶函数，它的最大值是原(原始)优化问题的一个下界。此外，可以证明，在某些条件下:

![](img/4e0d97b44b3b3b1d768ed4d68f3d15a4.png)

这样我们就可以解决对偶问题，而不是原始问题。幸运的是，SVM 满足这些[条件](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions)(特别是 Slater 条件):目标函数和约束函数是凸的且连续可微的。

由于 SVM 满足正则性条件，如果原始问题有解，它必然在满足卡鲁什-库恩-塔克(KKT)条件的拉格朗日函数的驻点(x* *，α* *)之间。此外，在 SVM(凸可微)的情况下，KKT 条件不仅是必要的，而且是最优性的充分条件。

KKT 条件是什么？如果原问题有解 x* *，α* *是对偶问题的解，那么 L(x* *，α* *) = f₀(x*).那么必须满足 KKT 条件:

![](img/2b24b9b4a3bb6e90bcff74bed84f64c2.png)

# **SVM 双重形态**

下面我们有软边际 SVM 优化问题:

![](img/de02e91cd58f824cfcba5fc55100c6cb.png)

如果我们将这两个约束重写为:

![](img/38c363cca56b9d2727b61a1accbc7542.png)

拉格朗日量可以写成:

![](img/6a87673fbb4b1e363ab3d1d75c26c2ee.png)

记住，由于 SVM 最优化问题满足一些特殊的条件(目标和约束函数是凸的和连续可微的)，拉格朗日函数的任何解都是原始最优化的解，并且它必须满足 KKT 条件。从 KKT 我们有:

![](img/5e1b3e0f3ea4e76c879cbcb0cead0d77.png)

从双重可行性条件，我们有:

![](img/ca57d6f1905be4ae930791c512ab9257.png)

因此，基于 b 上的偏导数，基于 **w** 上的梯度，以及基于对偶可行性条件，我们导出对偶问题的约束:

![](img/f309f9b0b12cf1821250424237cc1f16.png)

我们快到了！回到拉格朗日函数，让我们用从 **w** 梯度得到的结果(**w**=∑ᵢ₌₁ᵐα⁽ⁱ⁾y⁽ⁱ⁾**x**⁽ⁱ⁾)代入拉格朗日和的最后一项:

![](img/64f956e8125c2638c268e3c02c2fb249.png)

让

![](img/480b4260defa0b6496252225fd5f1a11.png)

然后把所有东西都代入拉格朗日方程:

![](img/a25752465e9044367dd8991b8bae09e1.png)

最后，由于∑α⁽ⁱ⁾y⁽ⁱ⁾=λ⁽ⁱ⁾-α⁽ⁱ⁾= 0，我们终于有了软裕度 SVM 的**对偶形式(在乘以-1 并将最大化转化为最小化问题之后):**

![](img/88b4f1d10654b96101201c48a5221dfc.png)

现在我们只需要从拉格朗日乘数 **α** 中取回权重矢量 **w** 和偏差项 b。对于权重，我们可以简单地使用这样的事实:w =α⁽ⁱ⁾y⁽ⁱ⁾x ⁽ⁱ⁾.注意，我们只关心α⁽ⁱ⁾为 0 的指数。

为了让 b 回来，让我们最后看一下 KKT 的条件——从免费的宽松，我们有:

![](img/ece430b0f23912f9552b096af3419eb0.png)

从第一个等式中，我们看到，当αᵢ* > 0 时，第 I 个约束必须有效，也就是说，它必须满足等式:

![](img/8660db2b1d12007f4d69ab492b439cae.png)

基于λ⁽ⁱ⁾ ϵ⁽ⁱ⁾ = 0 和 C - α⁽ⁱ⁾ - λ⁽ⁱ⁾ = 0，我们得到 0 < α⁽ⁱ⁾ < C => λ⁽ⁱ⁾ > 0 => ϵ⁽ⁱ⁾ = 0。我们将对训练样本的这个子集进行平均，以计算 b。如果 0 < α⁽ⁱ⁾ < C, we have:

![](img/b0298246c94313daf4d02803040a8157.png)

where M is the subset of the training sample that satisfy 0 < α⁽ⁱ⁾ < C and nₘ is its size. Last, to make a prediction about a new observation **x** ，我们需要计算:

![](img/fab006e88b51cec50e7ff7ac5a87e3ca.png)

这是一个漫长的旅程，但我们刚刚导出了流行的库 [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) 解决的精确优化问题！稍后我们将使用一个标准的 QP 求解器，但是如果你对他们如何更有效地求解它感兴趣，你可以看看这篇[论文](https://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf)，其中他们展示了 *_libsvm_* 库所基于的序列最小优化(SMO)算法。

## **双重形式结论**

有两个非常重要的结果需要强调:

1.SVM 预测仅依赖于α⁽ⁱ⁾ > 0 的观测值。这些点代表训练样本的一个非常特殊的子集，称为**支持向量**。

> 一旦模型被训练，我们通常可以丢弃大部分训练集，只保留支持向量。

2.在训练和进行预测的过程中，我们只能通过内积来依赖数据点(x⁽ⁱ⁾)ᵀ x⁽ʲ⁾.这个美丽而重要的结果就是允许内核欺骗。

# **内核**

在非线性分类部分，我们讨论了在拟合 SVM 之前对原始要素应用非线性变换。这个简单的技巧允许线性 SVM 捕捉数据中的非线性关系。

例如，假设ϕ是一个特征映射函数，如下所示:

![](img/9ec805707661e8b44c14d6b1d7e45b8a.png)

我们希望使用ϕ( **x** ，而不是使用原始特征(x₁、x₂)来拟合 SVM。我们已经看到，当使用对偶形式时，SVM 仅通过它们的内积依赖于数据点。这意味着我们只需要计算:

![](img/91b6dfd086314d5f4f6413c1ff9341ca.png)

这就是内核的用武之地。在机器学习中，**内核**是仅基于原始向量( **x** ⁽ⁱ⁾、 **x** ⁽ʲ⁾).)计算内积ϕ(**x**⁽ⁱ⁾)ᵀϕ(**x**⁽ʲ⁾的函数不需要计算变换ϕ( **x** )甚至不需要知道它！

让我们看一个简单的例子。设ϕ( **x** 为如下定义的特征映射函数:

![](img/234c67a37d4e205030985edf8c487f45.png)

那么两个向量**a**=【a₁，a₂】和 **b** =[b₁，b₂】在应用变换ϕ后的内积为:

![](img/fcbc11f739a81dd720dd9bff4f62ac18.png)

这是一个非常有趣的结果。变换向量的内积等于原始向量的内积的平方。您根本不需要应用转换！

如果我们要对此数据进行 SVM 拟合，我们可以简单地取原始向量的内积的平方，事实上，我们将为转换后的向量拟合一个模型。

内核的另一个例子是我们之前提到的 RBF 函数。原来，嵌入在 RBF 核中的转换将每个训练实例映射到一个无限维空间。

> 这是内核如何有用的一个完美例子:它允许 SVM 在一个高(或无限！)维特征空间，而只需要计算原始向量的核函数。

这都回避了一个问题:每个函数都可以用作内核吗？可以证明，如果一个函数 K( **a** ， **b** )满足几个条件([默瑟条件](https://en.wikipedia.org/wiki/Mercer%27s_theorem#Mercer's_condition)，那么存在一个函数ϕ，它将 **a** 和 **b** 映射到另一个空间，这样:K( **a** ，**b**)= ϕ(**a**)ᵀϕ(**b**)。然后我们可以用 k 作为核，因为我们知道ϕ存在。所以只要一个函数满足 Mercer 的条件，就可以作为内核使用。

最后一点注意:如果我们使用核，我们通常不能从拉格朗日乘数得到权重向量。发生这种情况是因为，在转换输入后，**w**=∑ᵢα⁽ⁱ⁾y⁽ⁱ⁾ϕ(**x**⁽ⁱ⁾)和ϕ不一定是已知的。但是这没有问题，因为在其他地方我们只需要计算ϕ(**x**⁽ⁱ⁾)ϕ(**x**⁽ʲ⁾)= k(**x**⁽ⁱ⁾， **x** ⁽ʲ⁾).

# **最终代码**

既然我们知道了如何推导和解决 SVM 优化问题，我们就可以理解它背后的代码了。原始代码可在[本报告](https://gist.github.com/mblondel/586753)中获得——唯一的变化是通过对 0 < α⁽ⁱ⁾ < C(而不是 0 < α⁽ⁱ⁾).)的实例进行平均来估计 b

就是这样！我希望你和我一样通过写这篇文章学到了很多。你可以在这里找到一个笔记本来玩我们在这个帖子[中编码的三个模型。](https://github.com/victorvrv/blog-resources/blob/master/2-SVM/models.ipynb)

# **参考文献**

——【http://cs229.stanford.edu/notes/cs229-notes3.pdf 

-[https://www-cs . Stanford . edu/people/David Knowles/la grangian _ duality . pdf](https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf)

——[https://gist.github.com/mblondel/586753](https://gist.github.com/mblondel/586753)

——Christopher Bishop——模式识别和机器学习([链接](https://www.springer.com/gp/book/9780387310732))

- Géron，aurélien-使用 Scikit-Learn、Keras 和 TensorFlow 进行机器实践学习，第二版([链接](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/))