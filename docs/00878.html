<html>
<head>
<title>Deep Kernels and Gaussian Processes for Few-Shot Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于少镜头学习的深核和高斯过程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-kernels-and-gaussian-processes-for-few-shot-learning-38a4ac0b64db?source=collection_archive---------10-----------------------#2020-01-25">https://towardsdatascience.com/deep-kernels-and-gaussian-processes-for-few-shot-learning-38a4ac0b64db?source=collection_archive---------10-----------------------#2020-01-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/2377658ddcc29fe452e9c735d98d9ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7sDQefXxmUYJNwbq.jpg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">缺少一个可靠的预测器</p></figure><p id="0548" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi ld translated"><span class="l le lf lg bm lh li lj lk ll di">在</span>这篇文章中，我们将对Patacchiola等人的论文<a class="ae lm" href="https://arxiv.org/abs/1910.05199" rel="noopener ugc nofollow" target="_blank"> <em class="ln">中的关键概念进行分解，并展示一个简单的关于少量拍摄图像回归任务的例子。本文假设了一些关于<a class="ae lm" rel="noopener" target="_blank" href="/quick-start-to-gaussian-process-regression-36d838810319">高斯过程</a>的背景知识，以及它们是如何在监督学习中使用的(比如获得后验分布和选择核函数)。我们将讨论少投学习的高层次方面，并对该方法进行总结，但不涉及太多的数学细节，这些可以在上面链接的论文中读到。</em></a></p><p id="55f1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">链接到我的其他文章:</p><ol class=""><li id="0034" class="lo lp it kh b ki kj km kn kq lq ku lr ky ls lc lt lu lv lw bi translated"><a class="ae lm" rel="noopener" target="_blank" href="/custom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a">tensor flow中的自定义损失函数</a></li><li id="f075" class="lo lp it kh b ki lx km ly kq lz ku ma ky mb lc lt lu lv lw bi translated"><a class="ae lm" rel="noopener" target="_blank" href="/the-softmax-function-neural-net-outputs-as-probabilities-and-ensemble-classifiers-9bd94d75932"> Softmax分类</a></li><li id="5fb3" class="lo lp it kh b ki lx km ly kq lz ku ma ky mb lc lt lu lv lw bi translated"><a class="ae lm" rel="noopener" target="_blank" href="/analyzing-climate-patterns-with-self-organizing-maps-soms-8d4ef322705b">气候分析</a></li><li id="ca94" class="lo lp it kh b ki lx km ly kq lz ku ma ky mb lc lt lu lv lw bi translated"><a class="ae lm" href="https://medium.com/@hhl60492/black-swans-and-hockey-riots-extreme-value-analysis-and-generalized-extreme-value-distributions-d4b4b84cd374" rel="noopener">曲棍球骚乱和极端值</a></li></ol><p id="7dbd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">简介</strong></p><p id="779d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">少量学习(FSL)是学习理论和ML架构的一个领域，它专注于理解监督学习模型如何从少量训练样本中有效地学习和预测。FSL的相关方面是优雅地获得预测的不确定性，并增加模型的鲁棒性和泛化能力。FSL是一个重要的研究领域，因为很大一部分生产监督的机器学习系统，如特斯拉的自动驾驶仪，需要大量的数据和训练时间，以便至少在某种程度上是可靠的，即使这样也容易出现<a class="ae lm" href="https://www.forbes.com/sites/thomasbrewster/2019/04/01/hackers-use-little-stickers-to-trick-tesla-autopilot-into-the-wrong-lane/#64362c727c18" rel="noopener ugc nofollow" target="_blank">重大泛化问题</a>(因此我仍然喜欢驾驶自己的汽车)。与此相关的是，这些系统必须能够处理呈现给它们的信息的模糊性，并在它们的预测中给出不确定性的度量，以便在现实生活情况中是稳健的。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mc"><img src="../Images/fb7cebf8c45f2560c4aab5e441b2ae62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lRsikAvVP1x2qVLN.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">特斯拉新推出的HW3自动驾驶硬件系统。每个特斯拉“芯片”都集成了一个GPU和两个用于神经网络预测的特殊硬件加速器。此外，这次有更多的传感器，而不仅仅是摄像机，但同样的原理也适用于处理雷达和超声波回波的摄像机数据。</p></figure><p id="4f70" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">许多最受欢迎的监督学习模型，如深度神经网络(DNNs)和提升树，在训练样本数量少于数千个时，无法给出足够可靠的预测，更不用说数十个甚至一个样本了。从动物和人类学习的角度来看，少射学习来得相当自然。一个天真的动物(通常)不需要不止一次地吃一种有毒的水果来避免它和后来其他模糊相似的水果。然而，由于<a class="ae lm" href="https://en.wikipedia.org/wiki/No_free_lunch_theorem" rel="noopener ugc nofollow" target="_blank"> <em class="ln">没有免费的午餐</em> </a>，某些FSL方法存在局限性，比如讽刺的是，当我们有大量样本时，正如我们将看到的。</p><p id="65cf" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><a class="ae lm" rel="noopener" target="_blank" href="/custom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a">我的一篇早期文章</a>介绍了在oldschool Tensoflow (TF 1.x)中实现的一个有趣的FSL架构，该架构利用了从高斯过程(GP)分类器模型到深度神经网络的<em class="ln">知识提炼</em>，其中来自GP和DNN的联合损失在DNN进行了优化，以便训练DNN从极少的训练样本中给出更现实的预测，这些预测反映了数据中的不确定性。2019年底，Patacchiola等人推出了一种新方法，该方法将深度神经网络和GPs的结合进行了进一步，也许更优雅。为简便起见，我们将他们的方法称为深度内核传输或DKT。</p><p id="48ae" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">DKT方法的主要贡献是在少量回归和跨域分类(即，在不同数据集上训练分类器)方面优于FSL专业DNNs的性能。</p><p id="f7a4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">基本概念</strong></p><p id="1232" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">简而言之，在预测阶段，DNN获得我们希望GP拟合的(高维)输入，并对其应用非线性维度缩减，将其转换为低得多的维度表示。然后，将变换后的输出馈入GP核函数，并通过常规GP后验计算来计算后验。</strong></p><p id="78b1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">在训练过程中，使用一个<em class="ln">证据</em> <em class="ln">损失</em>对DNN和GP核的参数进行联合优化，该损失是GP的联合对数边际概率或可能性<em class="ln">。</em>DNN和GP各自通过采用(部分)梯度w.r.t .获得其各自的相对于联合证据损失的损失，并通过合适的优化器进行更新(Adam是作者的选择)。</strong></p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/3ca70c4528e4c975bd64ffe691e62b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*E1J0Sz3vCZDs4M5YmENrxQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">DKT培训流程图</p></figure><p id="dee6" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">值得注意的是，DNN和核仍然是分开的，即DNN不是整个核，也不能取代GP核的<em class="ln">形式</em>的选择(事实上，作者声明径向基核和谱核在他们的实验中表现良好)。DNN的参数实际上是神经网络的权重，GP核的参数是核函数中的参数(如<a class="ae lm" href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel" rel="noopener ugc nofollow" target="_blank"> RBF核</a>中的参数<em class="ln"> σ </em>)。</p><p id="ac84" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">原则上，当处理2D图像或张量并计算GP后验时，这解决了高维计算难题。通过多类分类中不同类的几个示例或回归任务中的几个示例训练DNN，允许DKT将GP的预测属性与DNNs的信息表示能力相结合。</p><p id="df51" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">神经网络反馈协方差核</strong></p><p id="8135" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">核函数k( <strong class="kh iu"> x，x’</strong>)计算两个“点”之间的协方差，或一般的向量、矩阵或张量。这个核函数用于填充协方差矩阵<strong class="kh iu"> K </strong>的元素，我们用它来计算GP中的后验分布。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/1a823a06c7842f1f64ef9c5f4a35fd0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/0*YuCe3CAUCnTPwuJQ"/></div></figure><p id="7c28" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然后，我们使用由其权重<strong class="kh iu"><em class="ln"/></strong>参数化的DNN<strong class="kh iu">φ</strong>将我们的训练数据前馈到由<strong class="kh iu"> <em class="ln"> θ </em> </strong> <em class="ln">参数化的合适的内核<strong class="kh iu">θ</strong>。</em></p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/9a74a43a652086e837cc7273311d3e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*prIvLDHLaliTKj1vzaicEQ.png"/></div></figure><p id="eff1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">经验贝叶斯近似</strong></p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mk"><img src="../Images/00356128a2f20f4f1737510b5d550036.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*hROYdVptQyoBXrS_x7Wetg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">用参数的点估计逼近边际似然后的联合损失</p></figure><p id="91cc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">拥有贝叶斯GP框架的好处在于，我们可以假设所有要拟合的参数和输入(<strong class="kh iu"> <em class="ln"> φ，θ，</em> x </strong> ) <strong class="kh iu"> <em class="ln"> </em> </strong>在以目标输出为条件的一些高维空间中形成联合高斯分布。事实上，一般GP概念是贝叶斯推理的一个极好的例子。我们拥有作为输入和模型参数的先验数据，以及作为目标的证据数据，并且我们计算后验分布，该后验分布填充我们假设对数据进行建模的核函数的无限维空间希尔伯特空间。</p><p id="b84b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然而，当试图计算预测中的全部后验概率时，出现了一些计算问题，因为需要变分推断方法来计算全部后验概率，并且会使计算变得难以进行。作者通过将参数集(由hats表示)固定为点估计值(在联合高斯公式中，参数集被假设为分布，与<a class="ae lm" rel="noopener" target="_blank" href="/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd">贝叶斯神经网络</a>中的权重分布假设完全相同)来近似后验概率，并对对数联合边际似然性应用直接的最大似然估计，这被称为<em class="ln">经验贝叶斯</em>或MLE II型近似，因此边际似然性由上面的正则似然函数<em class="ln"> L </em>近似。损失的<a class="ae lm" href="https://stats.stackexchange.com/questions/280105/log-marginal-likelihood-for-gaussian-process" rel="noopener ugc nofollow" target="_blank">解析项由作者给出:</a></p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/96178128bd04835e8c1a36dad6909661.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*vQZgkRW30GpCbzXhir1O2Q.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">近似后损失的解析项</p></figure><p id="262a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">其中协方差矩阵<em class="ln"> K </em>由核θ填充(吸收了高斯噪声参数)，<em class="ln"> y </em>是目标集合，<em class="ln"> N </em>是训练示例或跨域任务(例如，不同数据集)的数量。每组参数<em class="ln"/><strong class="kh iu"><em class="ln"/></strong>和<em class="ln"> θ </em>然后在它们各自对<em class="ln"> L </em>的偏导数上得到更新。这种损失形式的好处在于，它自然地用-log|K|项进行了正则化，这不利于模型的复杂性。</p><p id="bac2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">所有这些要花多少钱？</p><p id="a9d3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">可能和最高期权特斯拉Cybertruck一样多。从我收集的信息来看，训练一个Convnet的时间复杂度大约为O(n⁵，计算损耗的矩阵求逆运算至少为O(n)，因此有理由假设我们在训练DKT时至少有巨大的O(n⁵) + O(n ) = O(n⁸的时间复杂度。当然，权衡的结果是在少量的学习中，你只需要很少的训练样本。然而，与此同时，全球定位系统的独特优势是预测本身的预测不确定性的可用性。人们将不得不训练大量的dnn或增强树，以获得类似的稳健的不确定性度量。</p><p id="efd4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">几个镜头影像回归</strong></p><p id="07fc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">好吧，那么这个模型实际上做得有多好？让我们看看试图从一幅图像中的人脸预测人类年龄的问题…只有98幅图像可以用来训练。这些图片来自https://github.com/JingchunCheng/All-Age-Faces-Dataset的<a class="ae lm" href="https://github.com/JingchunCheng/All-Age-Faces-Dataset" rel="noopener ugc nofollow" target="_blank"/></p><p id="739a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了简单起见，我们将调整图像的大小为80x80像素的固定大小，并将其下采样为只有一个颜色通道的灰度图像。我们还将使用一个简单的RBF内核，因为这对作者来说似乎工作得很好。回购情况如下:</p><p id="3b6b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><a class="ae lm" href="https://github.com/hhl60492/deep-kernel-transfer" rel="noopener ugc nofollow" target="_blank"><strong class="kh iu">https://github.com/hhl60492/deep-kernel-transfer</strong></a></p><p id="2a27" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">一些实施注意事项:</p><ol class=""><li id="679c" class="lo lp it kh b ki kj km kn kq lq ku lr ky ls lc lt lu lv lw bi translated">最初的repo使用PyTorch并启用了CUDA，但是在我的fork中，CUDA依赖性被移除了，因此没有CUDA/GPU支持的读者可以运行该模型</li><li id="485f" class="lo lp it kh b ki lx km ly kq lz ku ma ky mb lc lt lu lv lw bi translated">修改了一些数据加载器脚本和张量的内部形状，以允许对AAF图像进行训练-在复杂的ML模型/管道中，将数据争论和处理抽象为单独的脚本，并尽可能模块化数据加载和训练/预测组件是一个好主意！</li></ol><p id="932f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">结果</strong></p><p id="cd72" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">GPNet回归模型在98个随机选择的男性和女性80×80 JPG灰度图像上训练，以它们的年龄标签(来自图像文件名)作为目标，持续100个时期。发现将年龄目标标准化为[-1，1]对于给出有用的结果是必要的。一般来说，这是深度学习系统的一个大问题，因为必须仔细选择与网络中的激活函数(尤其是回归的输出节点)和损耗(参见<a class="ae lm" href="https://stats.stackexchange.com/questions/56658/how-do-you-interpret-rmsle-root-mean-squared-logarithmic-error" rel="noopener ugc nofollow" target="_blank">对数RMSE损耗</a>)相对应的输入和目标归一化(或缺乏归一化)。</p><p id="ea04" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">此外，在训练之前，没有对图像应用数据增强、对比度调整、去噪或其他处理方法，以证明该方法的稳健性。</p><p id="08ce" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在没有GPU的慢速Core i5笔记本电脑上，此培训只需不到几分钟的时间。一组5幅图像的选定测试结果如下:</p><pre class="md me mf mg gt mm mn mo mp aw mq bi"><span id="28f0" class="mr ms it mn b gy mt mu l mv mw">Test image filelists/AAF/test/07896A10_gs.jpg <strong class="mn iu">pred 13.78601193428039 95% CI[ -18.663501739501953 , 46.23552858829498 ] target: 9.999999403953552</strong></span><span id="2578" class="mr ms it mn b gy mx mu l mv mw">Test image filelists/AAF/test/07275A76_gs.jpg <strong class="mn iu">pred 68.74524354934692 95% CI[ 36.295726895332336 , 101.19476318359375 ] target: 75.99999904632568</strong></span><span id="922c" class="mr ms it mn b gy mx mu l mv mw">Test image filelists/AAF/test/testA27_gs.jpg <strong class="mn iu">pred 27.942177653312683 95% CI[ -4.507339000701904 , 60.39169430732727 ] target: 26.999999582767487</strong></span><span id="fc7d" class="mr ms it mn b gy mx mu l mv mw">Test image filelists/AAF/test/07588A05_gs.jpg <strong class="mn iu">pred 9.622436761856079 95% CI[ -22.82707691192627 , 42.07195341587067 ] target: 5.0000011920928955</strong></span><span id="1d5a" class="mr ms it mn b gy mx mu l mv mw">Test image filelists/AAF/test/07455A02_gs.jpg <strong class="mn iu">pred 7.124289870262146 95% CI[ -25.32522678375244 , 39.57380652427673 ] target: 2.000001072883606</strong></span></pre><p id="2b4b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">结果非常好，绝对平均年龄误差只有4.7岁左右。此外，给出了置信区间(负区间值可以指示较年轻年龄的不确定性偏差)，因此确定性较低的预测具有较宽的区间，反之亦然。一些有趣的事情可以尝试，将旋转/扭曲的图像以及非人类的物体/脸给模型，看看测试结果是什么。同样，随着更多的训练时期，可以预期更好的结果和更小的置信区间。</p><p id="765f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">总的来说，结果是在或超过人类水平的表现…毕竟，你能猜出在下面的测试集中的人的年龄吗？同样，你能估计出这个猜测95%的置信区间吗？</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi my"><img src="../Images/ab25722fc77638578980fbaa6726e823.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*VCFDXY4uwqsgKg9_RAX-Ig.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">一些测试图像。我的个人资料图片(testA27_gs.jpg)上的预测相当吓人。</p></figure><p id="6a66" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">最后注意:由于将GP映射到交叉熵损失之后，使用这种方法的少数镜头分类任务有点复杂，但公式在论文中，代码也包含在原作者的报告中。</p><p id="db3e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">结论</strong></p><p id="a0ec" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们看到了DKT是如何组合在一起的，GPs在预测方面的有趣之处，以及它们如何应用于少量学习中的问题。如果未来的安全关键型实时机器学习系统，如特斯拉的autopilot或竞争对手，结合了GPs的一些元素，以提高它们在各种现实世界环境中的鲁棒性，而不仅仅是白天沙漠中的一些空高速公路，我不会感到惊讶。一如既往，请随意回复这篇文章，祝数据科学快乐。</p></div></div>    
</body>
</html>