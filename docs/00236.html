<html>
<head>
<title>Conjugate Prior Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">共轭先验解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/conjugate-prior-explained-75957dc80bfb?source=collection_archive---------2-----------------------#2020-01-08">https://towardsdatascience.com/conjugate-prior-explained-75957dc80bfb?source=collection_archive---------2-----------------------#2020-01-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b2d7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">有例子和证据</h2></div><h1 id="139e" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">1.什么是先验？</h1><p id="010a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">先验概率是在我们看到数据之前某个事件的概率<strong class="lc iu"/>。<br/> <a class="ae lw" href="https://medium.com/@aerinykim/bayesian-inference-intuition-and-example-148fd8fb95d6" rel="noopener">在贝叶斯推断</a>中，先验是我们在新数据可用之前，基于我们现在所知的对概率的猜测。</p><h1 id="d57e" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">2.什么是共轭先验？</h1><p id="4c8f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">不知道贝叶斯推理就无法理解共轭先验。</p><div class="lx ly gp gr lz ma"><a href="https://medium.com/@aerinykim/bayesian-inference-intuition-and-example-148fd8fb95d6" rel="noopener follow" target="_blank"><div class="mb ab fo"><div class="mc ab md cl cj me"><h2 class="bd iu gy z fp mf fr fs mg fu fw is bi translated">贝叶斯推理——直觉和实现</h2><div class="mh l"><h3 class="bd b gy z fp mf fr fs mg fu fw dk translated">贝叶斯推理的艺术在于你如何实现它...</h3></div></div><div class="mi l"><div class="mj l mk ml mm mi mn mo ma"/></div></div></a></div><p id="f223" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">对于博客的其余部分，我假设你知道先验、抽样和后验的概念。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="4e0a" class="nb kj it bd kk nc nd dn ko ne nf dp ks lj ng nh ku ln ni nj kw lr nk nl ky nm bi translated"><strong class="ak">本质上共轭先验</strong></h2><p id="6268" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">对于一些似然函数，如果选择某个先验，</strong>后验最终与先验处于相同的分布。这样的先验称为共轭先验。</p><p id="1dd4" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">通过例子来理解总是最好的。下面是<strong class="lc iu">计算二项式概率的后验概率的代码。θ </strong>是成功的概率，我们的目标是选择最大化后验概率的<strong class="lc iu"> θ。</strong></p><figure class="nn no np nq gt nr"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="2ca5" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">问你一个问题:在上面的代码块中有什么让你担心的吗？</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="2b08" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">有两件事使得后验计算很昂贵。</p><p id="1f85" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated"><strong class="lc iu">首先，我们计算每个θ </strong>的后验概率。</p><p id="2a75" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">为什么我们要计算成千上万个 thetas 的后验概率？因为你正在使后部正常化(第 21 行)。即使选择不归一化后验概率，最终目标也是找到后验概率的<strong class="lc iu">最大值</strong>(<a class="ae lw" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">最大后验概率</a>)。为了以简单的方式找到最大值，我们需要考虑每个候选者——对于每个<strong class="lc iu"> θ的可能性<strong class="lc iu"> P(X|θ) </strong>。</strong></p><p id="b1d3" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated"><strong class="lc iu">第二，如果没有后验分布的闭合形式公式，我们就得通过数值优化来求最大值，比如</strong> <a class="ae lw" rel="noopener" target="_blank" href="/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1"> <strong class="lc iu">梯度下降</strong> </a> <strong class="lc iu">或者牛顿法。</strong></p><h1 id="ab51" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">3.共轭先验有什么帮助？</h1><p id="ed7a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当你知道你的先验是共轭先验时，你可以跳过<code class="fe nu nv nw nx b">posterior = likelihood * prior</code>的计算。此外，如果你的先验分布有一个封闭形式的表达式，你已经知道最大后验概率是多少。</p><p id="b0ef" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">在上面的例子中，贝塔分布在二项式可能性之前是共轭的。这是什么意思？这意味着<strong class="lc iu">在建模阶段，我们已经知道后验也将是贝塔分布。</strong>因此，在进行更多的实验后，<strong class="lc iu">你可以简单地将接受和拒绝的次数分别加到现有的参数α，β</strong>上来计算后验概率，而不是将似然性乘以先验分布<strong class="lc iu">。</strong>这个很方便！(下一节证明。)</p><p id="0b94" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">作为一名数据/ML 科学家，你的模型永远不会完整。随着更多数据的到来，你必须更新你的模型(这就是为什么我们使用贝叶斯推理)。如你所见，贝叶斯推理中的计算可能很繁重，有时甚至难以处理。然而，如果我们可以使用共轭先验的封闭形式公式，计算变得非常轻。</p><h1 id="b079" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">4.证明——为什么贝塔分布是二项式可能性之前的共轭分布？</h1><p id="7b38" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当我们使用贝塔分布作为先验时，二项似然的后验也将遵循贝塔分布。</p><h2 id="d53d" class="nb kj it bd kk nc nd dn ko ne nf dp ks lj ng nh ku ln ni nj kw lr nk nl ky nm bi translated">显示贝塔产生贝塔。</h2><p id="f860" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">二项式和 Beta 的 pdf 是什么样子的？</p><figure class="nn no np nq gt nr gh gi paragraph-image"><div role="button" tabindex="0" class="nz oa di ob bf oc"><div class="gh gi ny"><img src="../Images/13d31238addd293dee1b2cecee41a5de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FNjdWCUroWLhKn-bs_X2Cw.png"/></div></div></figure><p id="ef34" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">让我们把它们代入著名的贝叶斯公式。</p><p id="8d2c" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated"><strong class="lc iu"> θ </strong>是成功的概率。</p><p id="8bf8" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated"><strong class="lc iu"> x </strong>是成功次数。</p><p id="e1be" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated"><strong class="lc iu"> n </strong>是试验的总次数，因此<strong class="lc iu"> n-x </strong>是失败的次数。</p><figure class="nn no np nq gt nr gh gi paragraph-image"><div role="button" tabindex="0" class="nz oa di ob bf oc"><div class="gh gi oe"><img src="../Images/55fb5c9bb95c81c409bafb5e50ba6fe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xjRaB2R2A3aDS8RstiErMQ.png"/></div></div><p class="of og gj gh gi oh oi bd b be z dk translated">为什么最后一个积分变成<strong class="bd oj"> B(x+α，n-x+β) </strong>？→<a class="ae lw" href="https://bit.ly/2t1i2KT" rel="noopener ugc nofollow" target="_blank">https://bit.ly/2t1i2KT</a></p></figure><p id="f752" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">先验分布<strong class="lc iu"> P(θ) </strong>为<strong class="lc iu">β(α，β) </strong>，从实验中得到<strong class="lc iu"> x </strong>个成功和<strong class="lc iu"> n-x </strong>个失败后，后验也变成参数为<strong class="lc iu"> (x+α，n-x+β)的β分布。</strong></p><p id="14b1" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">这里的好处是，你不用做计算，就可以通过分析知道这个。</p><h1 id="1bdf" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">5.共轭先验分布</h1><p id="00f7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">贝塔分布是<strong class="lc iu">伯努利分布、二项式分布、负二项式分布</strong>和<strong class="lc iu">几何分布</strong>的共轭先验分布(看起来这些分布涉及成功&amp;失败)。</p><pre class="nn no np nq gt ok nx ol om aw on bi"><span id="8c3d" class="nb kj it nx b gy oo op l oq or"><strong class="nx iu">&lt;Beta posterior&gt;</strong><br/>Beta prior * <strong class="nx iu">Bernoulli</strong> likelihood → Beta posterior<br/>Beta prior * <strong class="nx iu">Binomial</strong> likelihood → Beta posterior<br/>Beta prior * <strong class="nx iu">Negative Binomial</strong> likelihood → Beta posterior<br/>Beta prior * <strong class="nx iu">Geometric</strong> likelihood → Beta posterior<br/></span><span id="b41c" class="nb kj it nx b gy os op l oq or"><strong class="nx iu">&lt;Gamma posterior&gt;</strong><br/>Gamma prior * <strong class="nx iu">Poisson</strong> likelihood → Gamma posterior<br/>Gamma prior * <strong class="nx iu">Exponential</strong> likelihood → Gamma posterior</span><span id="136d" class="nb kj it nx b gy os op l oq or"><strong class="nx iu">&lt;Normal posterior&gt; </strong><br/>Normal prior * Normal likelihood (mean) → Normal posterior</span></pre><p id="5db4" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">这就是为什么这三种分布(<strong class="lc iu">β</strong>、<strong class="lc iu">γ</strong>和<strong class="lc iu">正态</strong>)被大量用作先验分布的原因。</p><p id="f4ab" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">一种有趣的说法是，即使你做了所有这些实验，并将你的可能性乘以先验，你最初选择的先验分布<strong class="lc iu">如此之好，以至于最终分布与先验分布</strong>相同。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="623d" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">等式中的共轭先验<strong class="lc iu"> P(θ) </strong>:</p><pre class="nn no np nq gt ok nx ol om aw on bi"><span id="c41d" class="nb kj it nx b gy oo op l oq or"><strong class="nx iu">P(θ) such that P(θ|D) = P(θ)</strong></span></pre><blockquote class="ot"><p id="6e3a" class="ou ov it bd ow ox oy oz pa pb pc lv dk translated">共轭先验=方便先验</p></blockquote></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="5186" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">需要注意一些事情:</p><ol class=""><li id="236f" class="pd pe it lc b ld mp lg mq lj pf ln pg lr ph lv pi pj pk pl bi translated">当我们使用共轭先验时，顺序估计(每次观察后更新计数)给出的结果与批量估计相同。</li><li id="bc1c" class="pd pe it lc b ld pm lg pn lj po ln pp lr pq lv pi pj pk pl bi translated">为了<strong class="lc iu">找到最大后验</strong>、<strong class="lc iu">，你不必归一化</strong>似然(抽样)和先验(分母中每个可能的<strong class="lc iu"> θ </strong>的积分)的乘积。</li></ol><figure class="nn no np nq gt nr gh gi paragraph-image"><div role="button" tabindex="0" class="nz oa di ob bf oc"><div class="gh gi pr"><img src="../Images/33e0825d3ef59d167cd722fb57e9342e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B56cwtHsFisJCAiRPZELKw.png"/></div></div></figure><p id="2bff" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">不用归一化还是能找到最大值。然而，如果你想比较不同模型的后验概率，或者计算点估计，你需要归一化。</p></div></div>    
</body>
</html>