<html>
<head>
<title>Understanding Maximum Likelihood Estimation (MLE)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解最大似然估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-maximum-likelihood-estimation-mle-7e184d3444bd?source=collection_archive---------4-----------------------#2020-03-07">https://towardsdatascience.com/understanding-maximum-likelihood-estimation-mle-7e184d3444bd?source=collection_archive---------4-----------------------#2020-03-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/8ee4d677c396c417e38315c5992cd654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hijnAEgiCc6BPQdr"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">马库斯·斯皮斯克在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="0262" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">这是什么？它是用来做什么的？</h2></div><p id="2745" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated">我第一次学习MLE的时候，我记得我只是在想，“嗯？”这听起来更哲学和理想主义，而不是实际的。但事实证明，MLE实际上非常实用，是一些广泛使用的数据科学工具(如逻辑回归)的关键组成部分。</p><p id="3d52" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来看看MLE是如何工作的，以及如何用它来估计逻辑回归模型的贝塔系数。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="778a" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">MLE是什么？</h1><p id="c474" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">最简单地说，MLE是一种估计参数的方法。每次我们拟合统计或机器学习模型时，我们都在估计参数。单变量线性回归有以下等式:</p><blockquote class="nh"><p id="9aab" class="ni nj jj bd nk nl nm nn no np nq lt dk translated">Y = B0 + B1*X</p></blockquote><p id="6d44" class="pw-post-body-paragraph ky kz jj la b lb nr kk ld le ns kn lg lh nt lj lk ll nu ln lo lp nv lr ls lt im bi translated">我们拟合该模型的目的是在给定Y和x的观测值的情况下估计参数B0和B1。我们使用普通最小二乘法(OLS)而不是最大似然法来拟合线性回归模型并估计B0和B1。但是类似于OLS，MLE是一种在给定我们观察到的情况下估计模型参数的方法。</p><p id="83c2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">MLE提出问题，<strong class="la jk">“给定我们观察到的数据(我们的样本)，最大化观察到的数据发生的可能性的模型参数是什么？”</strong></p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="b775" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">简单的例子</h1><p id="e1bc" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">那是相当多的。让我们用一个简单的例子来说明我们的意思。假设我们有一个有盖的盒子，里面装着未知数量的红球和黑球。如果我们从有替换的盒子里随机选择10个球，最后我们得到了9个黑色的球和1个红色的球，这告诉我们盒子里的球是什么？</p><p id="545f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我们开始相信盒子里有相同数量的红色和黑色的球，观察到我们观察到的现象的概率是多少？</p><pre class="nw nx ny nz gt oa ob oc od aw oe bi"><span id="5cb4" class="of ml jj ob b gy og oh l oi oj"><strong class="ob jk">Probability of drawing 9 black and 1 red (assuming 50% are black):</strong></span><span id="230a" class="of ml jj ob b gy ok oh l oi oj">We can do this 10 possible ways (see picture below).</span><span id="f07b" class="of ml jj ob b gy ok oh l oi oj">Each of the 10 has probability = 0.5^10 = 0.097%</span><span id="b3bd" class="of ml jj ob b gy ok oh l oi oj">Since there are 10 possible ways, we multiply by 10:</span><span id="bae2" class="of ml jj ob b gy ok oh l oi oj">Probability of 9 black and 1 red = 10 * 0.097% = <strong class="ob jk">0.977%</strong></span></pre><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/c78db46d75294ba98f0b31ef98917fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MmUI_syD6F0rfLU7zuxfSA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">抽取1个红球和9个黑球的10种可能方法</p></figure><p id="f473" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们也可以用一些代码来证实这一点(比起计算概率，我总是更喜欢模拟):</p><pre class="nw nx ny nz gt oa ob oc od aw oe bi"><span id="aa0e" class="of ml jj ob b gy og oh l oi oj"><strong class="ob jk"><em class="om">In:</em></strong></span><span id="4ef1" class="of ml jj ob b gy ok oh l oi oj">import numpy as np</span><span id="a5d2" class="of ml jj ob b gy ok oh l oi oj"># Simulate drawing 10 balls 100000 times to see how frequently<br/># we get 9<br/>trials = [np.random.binomial(10, 0.5) for i in range(1000000)]<br/>print('Probability = ' + str(round(float(sum([1 for i\<br/>                                              in trials if i==9]))\<br/>                                   /len(trials),5)*100) + '%')</span><span id="1a32" class="of ml jj ob b gy ok oh l oi oj"><strong class="ob jk"><em class="om">Out:</em></strong></span><span id="e066" class="of ml jj ob b gy ok oh l oi oj">Probability = <strong class="ob jk">0.972%</strong></span></pre><p id="a780" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模拟概率与我们计算的概率非常接近(它们并不完全匹配，因为模拟概率有方差)。</p><p id="6e60" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以我们的结论是，假设盒子里有50%的球是黑色的，那么像我们这样挑出尽可能多的黑球的可能性是非常低的。作为通情达理的人，我们会假设黑色球的百分比一定不是50%，而是更高。那百分比是多少？</p><p id="9085" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是MLE的用武之地。<strong class="la jk">回想一下，MLE是我们估计参数的一种方法。问题中的参数是盒子中黑色球的百分比。</strong></p><blockquote class="nh"><p id="b5e0" class="ni nj jj bd nk nl nm nn no np nq lt dk translated">MLE问这个百分比应该是多少，才能最大化观察到我们观察到的现象的可能性(从盒子里抽出9个黑球和1个红球)。</p></blockquote><p id="b298" class="pw-post-body-paragraph ky kz jj la b lb nr kk ld le ns kn lg lh nt lj lk ll nu ln lo lp nv lr ls lt im bi translated">我们可以使用蒙特卡罗模拟来探索这一点。下面的代码块遍历一个概率范围(盒子中黑色球的百分比)。对于每种概率，我们模拟抽取10个球100，000次，以查看我们最终得到9个黑色球和1个红色球的频率。</p><pre class="nw nx ny nz gt oa ob oc od aw oe bi"><span id="bce1" class="of ml jj ob b gy og oh l oi oj"># For loop to simulate drawing 10 balls from box 100000 times where<br/># each loop we try a different value for the percentage of balls <br/># that are black</span><span id="3b75" class="of ml jj ob b gy ok oh l oi oj">sims = 100000</span><span id="32cb" class="of ml jj ob b gy ok oh l oi oj">black_percent_list = [i/100 for i in range(100)]<br/>prob_of_9 = []</span><span id="9e5d" class="of ml jj ob b gy ok oh l oi oj"># For loop that cycles through different probabilities<br/>for p in black_percent_list:<br/>    # Simulate drawing 10 balls 100000 times to see how frequently<br/>    # we get 9<br/>    trials = [np.random.binomial(10, p) for i in range(sims)]<br/>    prob_of_9.append(float(sum([1 for i in trials if i==9]))/len(trials))</span><span id="3db8" class="of ml jj ob b gy ok oh l oi oj">plt.subplots(figsize=(7,5))<br/>plt.plot(prob_of_9)<br/>plt.xlabel('Percentage Black')<br/>plt.ylabel('Probability of Drawing 9 Black, 1 Red')<br/>plt.tight_layout()<br/>plt.show()<br/>plt.savefig('prob_of_9', dpi=150)</span></pre><p id="0846" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们以下面的情节结束:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/2ffca37596f4d342363736aa1264544d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*avn0mJq74Aa3iFmCoWcA5A.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">抽取9个黑球和1个红球的概率</p></figure><p id="d4d1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看到那座山峰了吗？这就是我们要找的。抽中9个黑球和1个红球的概率最大的黑球百分比值是其最大似然估计值— <strong class="la jk">我们的参数(黑球百分比)的估计值，最符合我们观察到的情况</strong>。</p><p id="0cc1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，MLE有效地执行了以下操作:</p><ul class=""><li id="ae45" class="oo op jj la b lb lc le lf lh oq ll or lp os lt ot ou ov ow bi translated">写一个概率函数，将我们观察到的概率与我们试图估计的参数联系起来:我们可以将我们的概率写成<strong class="la jk"> <em class="om"> P(9个黑色，1个红色|黑色百分比= b)</em></strong>——假设盒子中黑色球的百分比等于b，抽取9个黑色球和1个红色球的概率</li><li id="ad1d" class="oo op jj la b lb ox le oy lh oz ll pa lp pb lt ot ou ov ow bi translated">然后我们找到使<strong class="la jk"> <em class="om"> P最大化的b的值(9黑1红|百分比黑=b) </em> </strong>。</li></ul><p id="21aa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从图片中很难看出，但是最大化观察我们所做的概率的黑色百分比值是90%。似乎显而易见，对吗？虽然这个结果似乎明显是个错误，但支持MLE的潜在拟合方法实际上是非常强大和通用的。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="4f16" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">最大似然估计和逻辑回归</h1><p id="0caa" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">现在我们知道了它是什么，让我们看看如何使用MLE来拟合一个<strong class="la jk">逻辑回归(</strong> <a class="ae jg" rel="noopener" target="_blank" href="/understanding-logistic-regression-using-a-simple-example-163de52ea900"> <strong class="la jk">)如果你需要一个关于逻辑回归的复习，在这里查看我以前的帖子</strong> </a> <strong class="la jk"> ) </strong>。</p><p id="0a08" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑回归的输出是类别概率。在我之前关于它的博客中，输出是投篮的概率。但是我们的数据是以1和0的形式出现的，而不是概率。例如，如果我从不同的距离投篮10次，我的Y变量，即每次投篮的结果，看起来会像这样(1代表投篮成功):</p><pre class="nw nx ny nz gt oa ob oc od aw oe bi"><span id="3e67" class="of ml jj ob b gy og oh l oi oj">y = [0, 1, 0, 1, 1, 1, 0, 1, 1, 0]</span></pre><p id="5c52" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">而我的X变量，每次投篮离篮筐的距离(以英尺为单位)，看起来就像:</p><pre class="nw nx ny nz gt oa ob oc od aw oe bi"><span id="78d3" class="of ml jj ob b gy og oh l oi oj">X = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</span></pre><p id="b926" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们如何从1和0到概率？<strong class="la jk">我们可以把每一次击球看作是一个二项分布的随机变量</strong>的结果。<a class="ae jg" rel="noopener" target="_blank" href="/fun-with-the-binomial-distribution-96a5ecabf65b">关于二项分布的更多内容，请阅读我之前的文章</a>。简单地说，这意味着每一次尝试都是它自己的尝试(就像扔硬币一样),有一些潜在的成功概率。<strong class="la jk">只不过我们不仅仅是在估算一个单一的静态成功概率；相反，我们是根据我们投篮时离篮筐有多远来估计成功的概率。</strong></p><p id="1616" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以我们可以把我们的问题重新架构成一个条件概率(y =投篮的结果):</p><blockquote class="nh"><p id="2034" class="ni nj jj bd nk nl nm nn no np nq lt dk translated">P(y |离篮筐的距离)</p></blockquote><p id="88aa" class="pw-post-body-paragraph ky kz jj la b lb nr kk ld le ns kn lg lh nt lj lk ll nu ln lo lp nv lr ls lt im bi translated">为了使用MLE，我们需要一些参数来拟合。在单变量逻辑回归中，这些参数是回归beta:B0和B1。在下面的等式中，Z是投篮命中率的对数(<a class="ae jg" rel="noopener" target="_blank" href="/understanding-logistic-regression-using-a-simple-example-163de52ea900">如果你不知道这是什么意思，这里解释一下</a>)。</p><blockquote class="nh"><p id="9b1e" class="ni nj jj bd nk nl nm nn no np nq lt dk translated">Z = B0 + B1*X</p></blockquote><p id="d335" class="pw-post-body-paragraph ky kz jj la b lb nr kk ld le ns kn lg lh nt lj lk ll nu ln lo lp nv lr ls lt im bi translated"><strong class="la jk">你可以把B0和B1想象成描述距离和投篮概率之间关系的隐藏参数。</strong>对于B0和B1的某些值，射击精度和距离之间可能存在强正相关关系。对其他人来说，它可能是微弱的积极的，甚至是消极的(斯蒂芬库里)。如果B1设置为等于0，则根本没有关系:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/0fe010c94faab85011e1c814461ff9c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NBj77Y0IuyETyaqceHsLpQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">不同B0和B1参数对概率的影响</p></figure><p id="a8f9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于每组B0和B1，我们可以使用蒙特卡罗模拟来计算出观察到数据的概率。<strong class="la jk">我们模拟的概率是对于一组猜测的B0，B1值，观察到我们的精确投篮序列(y=[0，1，0，1，1，1，0，1，1，0]，给定离篮筐的距离=[1，2，3，4，5，6，7，8，9，10])的概率。</strong></p><blockquote class="nh"><p id="c4cf" class="ni nj jj bd nk nl nm nn no np nq lt dk translated">对于给定的B0和B1，P(y=[0，1，0，1，1，1，0，1，0] | Dist=[1，2，3，4，5，6，7，8，9，10])</p></blockquote><p id="8f36" class="pw-post-body-paragraph ky kz jj la b lb nr kk ld le ns kn lg lh nt lj lk ll nu ln lo lp nv lr ls lt im bi translated">通过尝试一组不同的值，我们可以找到使<strong class="la jk"> P(y=[0，1，0，1，1，1，0，1，1，0] | Dist=[1，2，3，4，5，6，7，8，9，10]) </strong>最大化的B0和B1的值。这些将是B0和B1的最大似然估计。</p><p id="3a61" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">显然，在逻辑回归和一般的MLE中，我们不会进行强力猜测。相反，我们创建了一个成本函数，它基本上是我们试图最大化的概率的倒置形式。这个代价函数与<strong class="la jk"> P(y=[0，1，0，1，1，1，0，1，1，0] | Dist=[1，2，3，4，5，6，7，8，9，10]) </strong>成反比，和它一样，代价函数的值随着我们的参数B0和B1而变化。<strong class="la jk">我们可以通过使用梯度下降来最小化该成本函数，从而找到B0和B1的最佳值。</strong></p><p id="2e86" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但在精神上，我们对MLE一如既往地做的是询问和回答以下问题:</p><blockquote class="nh"><p id="b7ae" class="ni nj jj bd nk nl nm nn no np nq lt dk translated">给定我们观察到的数据，使观察到的数据发生的可能性最大化的模型参数是什么？</p></blockquote></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="2811" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">我在这篇文章中提到了以下文章:</strong></p><p id="f0ce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/understanding-logistic-regression-using-a-simple-example-163de52ea900"> <em class="om">了解逻辑回归</em> </a></p><p id="95b5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/fun-with-the-binomial-distribution-96a5ecabf65b"> <em class="om">二项分布</em> </a></p></div></div>    
</body>
</html>