# 使用 VC 维计算样本量；一种机器学习的方式。

> 原文：<https://towardsdatascience.com/calculating-sample-size-using-vc-dimensions-a-machine-learning-way-748abbe1b1e4?source=collection_archive---------55----------------------->

## 我们如何以及为什么需要知道最佳样本量？？

数据收集过程本身就是一项冗长乏味的任务，需要花费数十年的时间。在医疗保健等领域，这一过程涉及大量成本，通常需要数年时间。例如，要获得某一特定疾病的信息，比如说 100 个数据点，就需要筛选两倍或三倍于所需样本量的患者。获取患者数据不仅是一项耗时的工作，而且代价高昂。虽然在无所事事的情况下收集大量数据是一项非常徒劳的任务，但类似地，当机器学习算法应用于数据时，无法满足数据要求也不会产生好的结果。最终，估算最佳样本量是一项决定性的任务，也是在对数据进行任何分析之前需要考虑的事情。

在这里，我将讨论一个有趣的技术，使用 VC 维度的概念来近似样本大小，通过调整它来使我们更容易和舒适。在继续之前，我想先介绍一种统计方法或近似样本大小的方法，然后进一步说明其局限性，并向您展示 VC 维方法的必要性。

**统计样本量估计**

为了解释估计样本量的统计方法，我引用了波士顿大学的 Lisa Sullivan 教授在[这里](http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html)所做的工作。*我建议所有读者在继续阅读之前先浏览给定的链接，因为我会准确地解释其中提出的方法*。所提出的方法提出了一种估计最佳样本大小的方法，该方法可以减少由于数据中存在的方差而由输入特征引起的*误差容限 E* 。为了确定最佳的样本大小，必须首先选择期望的误差范围，这通常是领域专家的工作，并且根据问题而变化。以上述方式计算样本量的公式如下

![](img/ba0078d54f1e00f8099a184a4d66b027.png)

该公式生成样本大小' **n'** ，确保误差范围 **E** 不超过规定值。为了求解 **n** ，我们必须输入“ **Z** 、“ **σ”、“T19”和“ **E** ”**

**‘σ’是样本方差**

**‘E’是期望的误差率**

**‘Z’是给定置信水平的 Z 分布值**

从上面的公式中可以看出，样本大小“n”取决于所选的误差容限“ *E”，这一点很重要。*

确定样本量的统计方法虽然在逻辑上听起来很棒，但有一个非常明显的先天限制。

> 统计样本大小估计没有给出选择样本大小的方法，该方法可以减少误差或增加应用于数据的预测机器学习模型的准确性。

事实上，以这种方式估计的样本大小没有以任何方式考虑应用于它的任何机器学习模型或算法。因此，这使得统计方法非常幼稚，帮助不大。

有没有一种选择样本大小的方法可以减少应用于数据的机器学习模型的误差？？或者有没有办法确定一个机器学习模型对于特定样本量的预期误差是多少？这些问题的答案与风险资本维度的概念紧密相连。

> VC 维方法给出了一种确定样本大小的方法，这种方法可以减少应用于其上的任何机器学习模型的误差。

**VC 尺寸**

VC 维的概念为基于机器学习算法确定测试误差铺平了道路，该算法被选择应用于数据和与其相关联的训练误差。

让我先简单介绍一下**VAP Nik–Chervonenkis(VC)维度**的概念及其工作原理。

*注意:我对 VC 维概念的解释非常简略，因为本文的主要焦点是使用它计算样本大小，而不是概念本身*。

Vc 维测量统计分类算法学习一组函数的能力。简单地说，它衡量分类模型的能力。

> *统计分类算法的 VC 维的典型定义是:算法可以**粉碎**的最大点集的*基数。

**粉碎**

这里我直接从[维基百科](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension#:~:text=In%20Vapnik%E2%80%93Chervonenkis%20theory%2C%20the,by%20a%20statistical%20classification%20algorithm.)借用*粉碎*的清晰定义。

“具有某个参数向量‘θ’的分类模型' *f '* 被认为粉碎了一组数据点{X1，X2…，Xn}，如果对于这些点的所有标签分配，存在一个‘θ’,使得模型' *f '* 在评估该组数据点时不会出错。”

![](img/11ba39a28afb04220df8e3591ddd304f.png)

打碎

因此，一个算法可以粉碎的点数定义了它的 VC 维数。

现在转到本文的中心主题，即 VC- dimensions 如何方便地识别正确的样本大小，让我介绍一个公式，它给出了算法产生的测试误差量的概率上限。

![](img/9d65699697d685fdb581172e65bc74f2.png)

虽然乍一看可能很吓人，但让我一部分一部分地揭示它。此处的公式考虑了模型的训练误差、应用算法的 VC 维“D”、应用模型的样本大小“N”以及 0≤η≤1 的“η”。

因此，简而言之，上述公式考虑了模型的 Vc 维、样本的大小以及由此获得的训练误差，然后生成在给定所有上述因素的情况下可以获得的测试误差的概率上界。到目前为止，除了解释 Vc-dimension 的一般概念和工作方式之外，我还没有提出任何新的东西。好吧，实际上我们已经走到了尽头，却并不知道。稍微调整一下上面的公式，我们就可以得到预期的样本大小的近似值。

我们用这样一种方法破解上面的公式，得到样本大小。因为我们现在的目标不是得到测试误差的上界，而是可以减少测试误差的样本大小，所以我们现在选择一个测试误差，我们可以根据我们正在处理的问题来考虑这个测试误差，并且扫过‘N’的值的范围，直到我们达到期望的测试误差。因此，我们不再像之前那样求解测试误差，而是求解样本大小。

一般来说，当我们选择一个更严格的测试误差时,“N”的值会增加。因此，我们想要的测试误差越大，我们的样本量就应该越大。因此，我们得到的样本大小近似值可以减少测试误差。

这种近似的方式非常简单，并且对于我们应用于数据的机器学习算法也非常灵活。近似样本大小因型号而异。

希望这是有益的！！

*一些定义和数字来自维基百科和互联网。