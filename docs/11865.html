<html>
<head>
<title>XGBoost: theory and practice</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost:理论与实践</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/xgboost-theory-and-practice-fb8912930ad6?source=collection_archive---------9-----------------------#2020-08-17">https://towardsdatascience.com/xgboost-theory-and-practice-fb8912930ad6?source=collection_archive---------9-----------------------#2020-08-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/28e86720f5eaa35faf3c5562888fc677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*I7r0g2KGJDC2nKtx"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Marc-Olivier Jodoin 在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="3cee" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">了解最流行的算法之一是如何工作的，以及如何使用它</h2></div><h1 id="9f6a" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">介绍</h1><p id="e6a9" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">XGBoost 代表 e<strong class="ls jk">X</strong>treme<strong class="ls jk">G</strong>radient<strong class="ls jk">Boost</strong>ing，它是<strong class="ls jk">梯度提升树</strong>算法的开源实现。由于其预测能力和易用性，它已经成为 Kaggle 竞赛中最受欢迎的机器学习技术之一。它是一种<strong class="ls jk">监督学习</strong>算法，可用于<strong class="ls jk">回归</strong>或<strong class="ls jk">分类</strong>任务。</p><p id="f498" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">不管它的未来主义名称如何，只要我们先过几个概念，它其实并没有那么难理解:<strong class="ls jk">决策树</strong>和<strong class="ls jk">梯度提升</strong>。如果你已经熟悉了这些，可以直接跳到“<strong class="ls jk">XGBoost 如何工作</strong>”。</p><h1 id="5e2c" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">决策树</h1><p id="6976" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">决策树可以说是你能找到的最容易解释的 ML 算法，如果与正确的技术结合使用，可能会非常强大。</p><p id="7bbe" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">决策树有这个名字是因为它的视觉形状，看起来像一棵树，有一个根和许多节点和叶子。想象一下，你有一份泰坦尼克号幸存者的名单，上面有一些信息，比如他们的年龄和性别，还有一个二元变量告诉你谁在灾难中幸存，谁没有。您现在想要创建一个分类模型，根据这些数据来预测谁将幸存下来。一个非常简单的例子是这样的:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mr"><img src="../Images/fad8ec5ffd2ae48cac7916ac60c7d7be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aoi6xlRYvjDRN9oghseMaA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="fae0" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">正如您所看到的，决策树只是一系列简单的决策规则，这些规则组合在一起，产生了对所需变量的预测。</p><h1 id="bf3e" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">梯度推进</h1><p id="e66f" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls jk"> Boosting </strong>是一种<strong class="ls jk">集成方法</strong>，这意味着它是一种将几个模型的预测组合成一个模型的方法。它是通过依次获取每个预测值并基于其前任的误差对其建模(对表现更好的预测值给予更大的权重)来实现的:</p><ol class=""><li id="9288" class="mw mx jj ls b lt mm lw mn lz my md mz mh na ml nb nc nd ne bi translated">使用原始数据拟合第一模型</li><li id="fb50" class="mw mx jj ls b lt nf lw ng lz nh md ni mh nj ml nb nc nd ne bi translated">使用第一模型的残差拟合第二模型</li><li id="1373" class="mw mx jj ls b lt nf lw ng lz nh md ni mh nj ml nb nc nd ne bi translated">使用模型 1 和模型 2 的总和创建第三个模型</li></ol><p id="c210" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls jk">梯度增强</strong>是一种特定类型的增强，之所以这样称呼是因为它使用<strong class="ls jk">梯度下降算法</strong>来最小化损失函数。</p><h1 id="5081" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">XGBoost 如何工作</h1><p id="7359" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">既然您已经理解了<strong class="ls jk">决策树</strong>和<strong class="ls jk">梯度提升</strong>，那么理解<strong class="ls jk"> XGBoost </strong>就变得容易了:它是一种梯度提升算法，使用决策树作为其“弱”预测器。除此之外，它的实现是专门为优化<strong class="ls jk">性能</strong>和<strong class="ls jk">速度</strong>而设计的。</p><p id="c149" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">从历史上看，XGBoost 对于<strong class="ls jk">结构化</strong>表格数据表现得相当好。如果您正在处理<strong class="ls jk">非结构化</strong>数据，如图像，<strong class="ls jk">神经网络</strong>通常是更好的选择。</p><h1 id="9680" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">超参数</h1><p id="ed5e" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">实现 XGBoost 时要选择哪些最重要的超参数，如何调优？</p><h2 id="1666" class="nk kz jj bd la nl nm dn le nn no dp li lz np nq lk md nr ns lm mh nt nu lo nv bi translated">助推器</h2><p id="bb4c" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe nw nx ny nz b">booster</code>是 boosting 算法，有三种选择:<code class="fe nw nx ny nz b">gbtree</code>、<code class="fe nw nx ny nz b">gblinear</code>或<code class="fe nw nx ny nz b">dart</code>。默认选项是<code class="fe nw nx ny nz b">gbtree</code>，这是我在本文中解释的版本。<code class="fe nw nx ny nz b">dart</code>是一个类似的版本，它使用 dropout 技术来避免过度拟合，而<code class="fe nw nx ny nz b">gblinear</code>使用广义线性回归来代替决策树。</p><h2 id="7e11" class="nk kz jj bd la nl nm dn le nn no dp li lz np nq lk md nr ns lm mh nt nu lo nv bi translated">寄存器 _alpha 和寄存器 _lambda</h2><p id="d4f2" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe nw nx ny nz b">reg_alpha</code>和<code class="fe nw nx ny nz b">reg_lambda</code>分别是 L1 和 L2 的正规化术语。这些数字越大，模型就越保守(不容易过度拟合，但可能会遗漏相关信息)。两者的推荐值都在 0-1000 之间。</p><h2 id="5161" class="nk kz jj bd la nl nm dn le nn no dp li lz np nq lk md nr ns lm mh nt nu lo nv bi translated">最大深度</h2><p id="7f0c" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe nw nx ny nz b">max_depth</code>设置决策树的最大深度。这个数字越大，模型就越不保守。如果设置为 0，那么树的深度没有限制。</p><h2 id="e772" class="nk kz jj bd la nl nm dn le nn no dp li lz np nq lk md nr ns lm mh nt nu lo nv bi translated">子样品</h2><p id="da91" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe nw nx ny nz b">subsample</code>是训练预测器时使用的样本比率的大小。默认值为 1，表示没有采样，我们使用全部数据。例如，如果设置为 0.7，则 70%的观测值将被随机采样以用于每次提升迭代(每次迭代取一个新样本)。这有助于防止过度拟合。</p><h2 id="18af" class="nk kz jj bd la nl nm dn le nn no dp li lz np nq lk md nr ns lm mh nt nu lo nv bi translated">数量估计者</h2><p id="eaa2" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe nw nx ny nz b">num_estimators</code>设置助推轮数，等于设置要使用的助推树数。这个数字越大，过度拟合的风险就越大(但是低数字也会导致低性能)。</p><h1 id="78d4" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">如何使用 XGBoost</h1><p id="2190" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">为了展示 XGBoost 在实践中是如何工作的，让我们做一个简单的练习，使用 Python 实际预测 Kaggle 比赛中的泰坦尼克号幸存者。</p><p id="b909" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">从 Kaggle 下载我们的数据后，我们将导入所有必要的库和我们的训练数据:</p><pre class="ms mt mu mv gt oa nz ob oc aw od bi"><span id="4e79" class="nk kz jj nz b gy oe of l og oh"><strong class="nz jk">IN:<br/></strong>import pandas as pd<br/>from xgboost import XGBClassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score</span><span id="c60e" class="nk kz jj nz b gy oi of l og oh">df = pd.read_csv("C:/Users/p005520/Downloads/titanic/train.csv")</span></pre><p id="75a4" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">请注意<code class="fe nw nx ny nz b">from xgboost import XGBClassifier</code>。这仅仅是因为我们已经通过从终端运行<code class="fe nw nx ny nz b">pip install xgboost</code>在我们的计算机上安装了 xgboost。<code class="fe nw nx ny nz b">XGBClassifier</code>用在这里是因为这是一个分类问题。对于回归问题，用<code class="fe nw nx ny nz b">XGBRegressor</code>代替。用于处理数据和计算性能指标的其他库。</p><pre class="ms mt mu mv gt oa nz ob oc aw od bi"><span id="bc3d" class="nk kz jj nz b gy oe of l og oh"><strong class="nz jk">IN:</strong><br/>dummies = pd.get_dummies(df['Sex'])<br/>df = pd.concat([df, dummies], axis=1)</span><span id="4311" class="nk kz jj nz b gy oi of l og oh">X = df[[‘Age’,’female’,’male’]]<br/>y = df[‘Survived’]</span><span id="6a80" class="nk kz jj nz b gy oi of l og oh">seed = 42<br/>test_size = 0.3<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)</span></pre><p id="2f5b" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们的数据中有许多变量，但这一次，我们将坚持上一个示例中的两个变量:“性别”和“年龄”。</p><p id="c378" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这个程序块的前两行从“Sex”中创建虚拟变量。这需要将“性别”从<em class="oj">字符串</em>转换为<em class="oj">整数</em>，成为两个不同的变量:“男性”和“女性”，根据乘客的性别，这两个变量等于 1 或 0。</p><p id="592e" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">接下来的两行定义了我们的目标变量(“存活”)和我们将用于预测它的变量。最后 4 行用于分割我们的训练集和测试集:我们的训练集将用于创建我们的 XGBoost 模型，而测试集将用于测量它的性能。</p><pre class="ms mt mu mv gt oa nz ob oc aw od bi"><span id="de61" class="nk kz jj nz b gy oe of l og oh"><strong class="nz jk">IN:<br/></strong>model = XGBClassifier(subsample = 0.7, max_depth = 4)<br/>model.fit(X_train, y_train)<br/>print(model)</span><span id="bd52" class="nk kz jj nz b gy oi of l og oh">y_pred = model.predict(X_test)</span><span id="0ac0" class="nk kz jj nz b gy oi of l og oh">accuracy = accuracy_score(y_test, y_pred)<br/>print(“Accuracy: %.2f%%” % (accuracy * 100.0))</span><span id="993e" class="nk kz jj nz b gy oi of l og oh"><strong class="nz jk">OUT:<br/></strong>XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,<br/>              colsample_bynode=1, colsample_bytree=1, gamma=0,<br/>              learning_rate=0.1, max_delta_step=0, max_depth=4,<br/>              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,<br/>              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,silent=None, subsample=1, verbosity=1)</span><span id="781d" class="nk kz jj nz b gy oi of l og oh">Accuracy: 80.60%</span></pre><p id="6903" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这里是我们实际训练 XGBoost 的地方，在前两行。注意<code class="fe nw nx ny nz b">subsample = 0.7</code>和<code class="fe nw nx ny nz b">max_depth = 4</code>，这里我手动定义了一些超参数。如果您想设置不同于默认选项的新超参数，只需将它们添加到列表中。我们的模型在测试集中产生了 80.6%的准确率，现在让我们看看如何将它应用于新数据:</p><pre class="ms mt mu mv gt oa nz ob oc aw od bi"><span id="cd70" class="nk kz jj nz b gy oe of l og oh">submission_input = pd.read_csv("C:/Users/p005520/Downloads/titanic/test.csv")</span><span id="59a6" class="nk kz jj nz b gy oi of l og oh">dummies = pd.get_dummies(submission_input[‘Sex’])<br/>submission_input = pd.concat([submission_input, dummies], axis=1)</span><span id="0b0f" class="nk kz jj nz b gy oi of l og oh">submission_X = submission_input[[‘Age’,’female’,’male’]]</span><span id="5213" class="nk kz jj nz b gy oi of l og oh">submission = model.predict(submission_X)<br/>submission_df = pd.DataFrame({‘PassengerId’:submission_input[‘PassengerId’],<br/> ‘Survived’:submission})<br/>submission_df.to_csv(‘submission.csv’, index = False)</span></pre><p id="5117" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在这个模块中，我们导入了 Kaggle 的提交文件，进行了与我们的训练集相同的虚拟处理，用<code class="fe nw nx ny nz b">model.predict(submission_X)</code>应用了我们训练过的预测器，然后将其保存到 csv。它在 Kaggle 的排行榜上得分如何？</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/7d34970b0dcbde9985d769c4a7843ce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2wt84d79tUjNGgNZTRrE1Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="4537" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">正如你所看到的，我们离排行榜的顶端还很远，但我们仍然成功地预测了 76%的人的存活率，只使用了两个特征，没有特征工程和没有超参数调整，这将是我们工作的逻辑下一步。</p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="7d0f" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我希望您现在理解了 XGBoost 是如何工作的，以及如何将它应用到真实数据中。尽管 XGBoost 具有固有的性能，但是<strong class="ls jk">超参数调整</strong>和<strong class="ls jk">特征工程</strong>可以使您的结果产生巨大的差异。</p><p id="804c" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果你想了解更多关于<strong class="ls jk">特征工程</strong>的知识来改进你的预测，你应该阅读这篇文章，它概述了你可以用来转换你的变量和创建新变量的主要技术:</p><div class="is it gp gr iu os"><a rel="noopener follow" target="_blank" href="/feature-engineering-3a380ad1aa36"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jk gy z fp ox fr fs oy fu fw ji bi translated">特征工程</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">了解数据科学工作流程中最重要的步骤</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg ja os"/></div></div></a></div><blockquote class="ph pi pj"><p id="3720" class="lq lr oj ls b lt mm kk lv lw mn kn ly pk mo mb mc pl mp mf mg pm mq mj mk ml im bi translated"><em class="jj">随时联系我</em> <a class="ae jg" href="https://www.linkedin.com/in/melloarthur/" rel="noopener ugc nofollow" target="_blank"> <em class="jj"> LinkedIn </em> </a> <em class="jj">如果你想进一步讨论，这将是一种荣幸(老实说)。</em></p></blockquote></div></div>    
</body>
</html>