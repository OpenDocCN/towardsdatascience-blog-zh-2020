<html>
<head>
<title>BART for Paraphrasing with Simple Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BART 用于解释简单的变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c?source=collection_archive---------5-----------------------#2020-08-05">https://towardsdatascience.com/bart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c?source=collection_archive---------5-----------------------#2020-08-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0d7b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">释义是用不同的词表达某事，同时保留原来的意思的行为。让我们看看如何利用 BART(一种序列间变压器模型)实现这一点。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/810e091d5255bc51449bd5672cdfc750.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U62pm0IqCa2XH1MY"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@alex_tsl?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚历山德拉</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="18de" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><blockquote class="lr ls lt"><p id="2ebc" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">BART 是一个去噪自动编码器，用于预处理序列到序列模型。通过(1)用任意噪声函数破坏文本，以及(2)学习模型以重建原始文本来训练 BART。</p><p id="f2ae" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">- <a class="ae ky" href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener ugc nofollow" target="_blank"> BART:用于自然语言生成、翻译和理解的去噪序列间预训练</a></p></blockquote><p id="831f" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">如果这听起来有点复杂，不要担心；我们将对其进行分解，看看这一切意味着什么。在我们深入 BART 之前，添加一点背景知识，现在是时候用自我监督的模型来进行迁移学习了。在过去的几年里，人们已经说过很多次了，但是变形金刚确实在各种各样的自然语言处理(NLP)任务中取得了令人难以置信的成功。</p><p id="0b54" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">BART 使用标准的转换器架构(编码器-解码器),就像最初用于神经机器翻译的<a class="ae ky" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">转换器模型</a>,但也结合了来自 BERT(仅使用编码器)和 GPT(仅使用解码器)的一些变化。更多细节可以参考<a class="ae ky" href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener ugc nofollow" target="_blank"> BART 论文</a>的<em class="lw"> 2.1 架构</em>部分。</p><h1 id="1b20" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">培训前 BART</h1><p id="9257" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">通过最小化解码器输出和原始序列之间的交叉熵损失来预训练 BART。</p><h2 id="2b3e" class="mz la it bd lb na nb dn lf nc nd dp lj mr ne nf ll ms ng nh ln mt ni nj lp nk bi translated">掩蔽语言建模(MLM)</h2><p id="7133" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">诸如 BERT 之类的 MLM 模型被预先训练来预测屏蔽令牌。这个过程可以分解如下:</p><ol class=""><li id="997c" class="nl nm it lx b ly lz mb mc mr nn ms no mt np mq nq nr ns nt bi translated">用一个<em class="lw">掩码标记[MASK]替换输入的随机子集。</em>(添加噪音/损坏)</li><li id="1a25" class="nl nm it lx b ly nu mb nv mr nw ms nx mt ny mq nq nr ns nt bi translated">该模型预测每个[掩码] <em class="lw"> </em>记号的原始记号。(去噪)</li></ol><p id="a146" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">重要的是，当试图预测原始标记时，BERT 模型可以“看到”完整的输入序列(一些标记被替换为[MASK])。这使得 BERT 成为双向模型，即它可以“看到”屏蔽令牌之前和之后的令牌。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/255c3cdca6ad84741edcf84e39816b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*4yZZwHGGAZXK_T2ftVw74g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BART <a class="ae ky" href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中的图 1 ( a)</p></figure><p id="0fc2" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">这适用于分类等任务，在这些任务中，您可以使用全序列中的信息来执行预测。但是，它不太适合于文本生成任务，在这种任务中，预测只依赖于前面的单词。</p><h2 id="7194" class="mz la it bd lb na nb dn lf nc nd dp lj mr ne nf ll ms ng nh ln mt ni nj lp nk bi translated">自回归模型</h2><p id="7190" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">用于文本生成的模型(如 GPT2)被预先训练，以在给定前一个记号序列的情况下预测下一个记号。这种预先训练的目标导致模型非常适合文本生成，但不适合分类之类的任务。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a1ae2e548891bda0723757bb7015103f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*yw0pmRwx6Tfmkldffvp3zA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自 BART <a class="ae ky" href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的图 1 ( b)</p></figure><h2 id="78d3" class="mz la it bd lb na nb dn lf nc nd dp lj mr ne nf ll ms ng nh ln mt ni nj lp nk bi translated">BART 序列到序列</h2><p id="dd5a" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">巴特既有编码器(像伯特)又有解码器(像 GPT)，基本上是两全其美。</p><p id="91b0" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">编码器使用类似于 BERT 的<em class="lw">去噪</em>目标，而解码器试图使用先前的(未损坏的)令牌和编码器的输出，逐个令牌地再现原始序列<em class="lw">(自动编码器)</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/87ae1e4f4d866c45762bdaa7686264c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jWecxbzBsJEbNgiy_LOIvw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BART <a class="ae ky" href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中的图 1 ( c)</p></figure><p id="b87d" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">这种设置的一个显著优点是选择讹误方案的无限灵活性；包括改变原始输入的长度。或者，用更花哨的术语来说，文本可能被任意噪声函数<em class="lw">破坏。</em></p><p id="fc3e" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">本文中使用的腐败方案总结如下。</p><ol class=""><li id="c34b" class="nl nm it lx b ly lz mb mc mr nn ms no mt np mq nq nr ns nt bi translated">令牌屏蔽—输入的随机子集被替换为[屏蔽]令牌，就像在 BERT 中一样。</li><li id="131c" class="nl nm it lx b ly nu mb nv mr nw ms nx mt ny mq nq nr ns nt bi translated">令牌删除-从输入中删除随机令牌。模型必须决定丢失了哪些位置(因为标记只是被删除了，而没有被其他任何东西替换)。</li><li id="adaa" class="nl nm it lx b ly nu mb nv mr nw ms nx mt ny mq nq nr ns nt bi translated">文本填充—许多文本跨度(长度可变)都被替换为单个[MASK]标记。</li><li id="c9db" class="nl nm it lx b ly nu mb nv mr nw ms nx mt ny mq nq nr ns nt bi translated">句子置换—输入基于句点(.)，而且句子都是混排的。</li><li id="5a63" class="nl nm it lx b ly nu mb nv mr nw ms nx mt ny mq nq nr ns nt bi translated">文档旋转—随机选择一个令牌，并旋转序列，使其从所选令牌开始。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="b96b" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">作者指出，用<em class="lw">文本填充</em>来训练 BART，可以在许多任务中产生最稳定的表现。</p><p id="26f3" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">对于我们感兴趣的任务，即<strong class="lx iu">转述</strong>，预训练的 BART 模型可以直接使用输入序列(原始短语)和目标序列(转述句子)作为序列到序列模型进行微调。</p><p id="b008" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">这也适用于总结和抽象问题回答。</p><h1 id="d6ad" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">设置</h1><p id="11ae" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">我们将使用<a class="ae ky" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank">简单变形金刚</a>库，基于抱脸<a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>库来训练模型。</p><p id="609e" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">1.从<a class="ae ky" href="https://www.anaconda.com/distribution" rel="noopener ugc nofollow" target="_blank">这里</a>安装 Anaconda 或者 Miniconda 包管理器。</p><p id="f3f2" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">2.创建新的虚拟环境并安装软件包。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="f87e" class="mz la it oe b gy oi oj l ok ol">conda create -n st python pandas tqdm</span><span id="bf28" class="mz la it oe b gy om oj l ok ol">conda activate st</span></pre><p id="a00d" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">3.如果使用 CUDA:</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="1709" class="mz la it oe b gy oi oj l ok ol">conda install pytorch&gt;=1.6 cudatoolkit=10.2 -c pytorch</span></pre><p id="0b19" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">否则:</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="a039" class="mz la it oe b gy oi oj l ok ol">conda install pytorch cpuonly -c pytorch</span></pre><p id="5df2" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">4.安装简单变压器。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="b840" class="mz la it oe b gy oi oj l ok ol">pip install simpletransformers</span></pre><h1 id="fa40" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">数据准备</h1><p id="61f9" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">我们将组合三个数据集，作为 BART 释义模型的训练数据。</p><ol class=""><li id="819c" class="nl nm it lx b ly lz mb mc mr nn ms no mt np mq nq nr ns nt bi translated"><a class="ae ky" href="https://github.com/google-research-datasets/paws#paws-wiki" rel="noopener ugc nofollow" target="_blank"> Google PAWS-Wiki 标注(最终版)</a></li><li id="3483" class="nl nm it lx b ly nu mb nv mr nw ms nx mt ny mq nq nr ns nt bi translated"><a class="ae ky" href="https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs" rel="noopener ugc nofollow" target="_blank"> Quora 问题对数据集</a></li><li id="4ae4" class="nl nm it lx b ly nu mb nv mr nw ms nx mt ny mq nq nr ns nt bi translated"><a class="ae ky" href="https://msropendata.com/datasets/e235323f-f23c-4246-b2e6-27d7a654d6cc" rel="noopener ugc nofollow" target="_blank">微软研究院释义语料库</a> (MSRP)</li></ol><p id="b3f1" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">下面的 bash 脚本可以用来轻松下载和准备前两个数据集，但是 MSRP 数据集必须从链接中手动下载。(微软没有提供直接链接😞)</p><p id="97f1" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated"><em class="lw">确保将文件放在同一个目录(</em> <code class="fe on oo op oe b"><em class="lw">data</em></code> <em class="lw">)中，以避免示例代码中文件路径的麻烦。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="a940" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">我们还有几个助手函数，一个用于加载数据，一个用于清理训练数据中不必要的空间。这两个功能都在<code class="fe on oo op oe b">utils.py</code>中定义。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="e802" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">有些数据的标点符号前有空格，我们需要删除。 <code class="fe on oo op oe b"><em class="lw">clean_unnecessary_spaces()</em></code> <em class="lw">功能就是用于这个目的。</em></p><h1 id="468e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">向 BART 解释</h1><p id="ef4c" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">一旦准备好数据，训练模型就相当简单了。</p><p id="465e" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated"><em class="lw">注意，你可以在</em> <a class="ae ky" href="https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/seq2seq/paraphrasing" rel="noopener ugc nofollow" target="_blank"> <em class="lw">这里</em> </a> <em class="lw">找到简单变形金刚例子中的所有代码。</em></p><p id="15c3" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">首先，我们导入所有必要的东西并设置日志记录。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="1412" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">接下来，我们加载数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="af9c" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">然后，我们设置模型和超参数值。请注意，我们正在使用预训练的<code class="fe on oo op oe b">facebook/bart-large</code>模型，并在我们自己的数据集上对其进行微调。</p><p id="055a" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">最后，我们将为测试数据中的每个句子生成释义。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="f08e" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated"><em class="lw">这将把预测写到</em> <code class="fe on oo op oe b"><em class="lw">predictions</em></code> <em class="lw">目录中。</em></p><h2 id="619e" class="mz la it bd lb na nb dn lf nc nd dp lj mr ne nf ll ms ng nh ln mt ni nj lp nk bi translated">超参数</h2><p id="3925" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">超参数值被设置为常规、合理的值，而不进行超参数优化。对于这个任务，<em class="lw">基本事实</em>并不代表唯一可能的正确答案(也不一定是<em class="lw">最佳</em>答案)。因此，调整超参数以使生成的文本尽可能接近<em class="lw">基本事实</em>没有多大意义。</p><p id="fa5a" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">我们的目标是生成好的释义序列，而不是从数据集中产生精确的释义序列。</p><p id="68b7" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated"><em class="lw">如果你对简单变压器的超参数优化感兴趣(对分类等其他模型/任务特别有用)，请点击这里查看我的指南。</em></p><div class="oq or gp gr os ot"><a rel="noopener follow" target="_blank" href="/hyperparameter-optimization-for-optimum-transformer-models-b95a32b70949"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd iu gy z fp oy fr fs oz fu fw is bi translated">最佳变压器模型的超参数优化</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">如何使用简单的转换器调整超参数，以实现更好的自然语言处理。</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">towardsdatascience.com</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph ks ot"/></div></div></a></div><p id="aaa1" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">所使用的解码算法(和相关的超参数)对生成的文本的质量和性质有相当大的影响。我选择的值(如下所示)通常适合生成“自然”文本。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="2b0a" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated"><em class="lw">更多信息请参考优秀抱脸指南</em> <a class="ae ky" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank"> <em class="lw">这里</em> </a> <em class="lw">。</em></p><h2 id="5a76" class="mz la it bd lb na nb dn lf nc nd dp lj mr ne nf ll ms ng nh ln mt ni nj lp nk bi translated">用你自己的句子试一下这个模式</h2><p id="944e" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">你可以使用下面的脚本在任何句子上测试这个模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h1 id="dbd8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结果</h1><p id="84dc" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">让我们来看看模型为测试数据生成的一些转述序列。对于每个输入序列，该模型将生成三个(<code class="fe on oo op oe b">num_return_sequences</code>)释义序列。</p><p id="b08a" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi">1.</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="42db" class="mz la it oe b gy oi oj l ok ol">Orignal:</span><span id="8a50" class="mz la it oe b gy om oj l ok ol">A recording of folk songs done for the Columbia society in 1942 was largely arranged by Pjetër Dungu.</span><span id="a4b8" class="mz la it oe b gy om oj l ok ol">Truth:</span><span id="3f63" class="mz la it oe b gy om oj l ok ol">A recording of folk songs made for the Columbia society in 1942 was largely arranged by Pjetër Dungu.</span><span id="020c" class="mz la it oe b gy om oj l ok ol">Prediction:</span><span id="da87" class="mz la it oe b gy om oj l ok ol">A recording of folk songs made in 1942 for the Columbia Society was largely arranged by Pjetr Dungu.</span><span id="9b2d" class="mz la it oe b gy om oj l ok ol">A recording of folk songs for the Columbia society in 1942 was largely arranged by Pjetr Dungu.</span><span id="8b2f" class="mz la it oe b gy om oj l ok ol">A recording of folk songs done for the Columbia Society in 1942 was largely arranged by Pjetr Dungu.</span></pre><p id="745a" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi">2.</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="fff7" class="mz la it oe b gy oi oj l ok ol">Original:</span><span id="1c1e" class="mz la it oe b gy om oj l ok ol">In mathematical astronomy, his fame is due to the introduction of the astronomical globe, and his early contributions to understanding the movement of the planets.</span><span id="5c54" class="mz la it oe b gy om oj l ok ol">Truth:</span><span id="0eeb" class="mz la it oe b gy om oj l ok ol">His fame is due in mathematical astronomy to the introduction of the astronomical globe and to his early contributions to the understanding of the movement of the planets.</span><span id="6675" class="mz la it oe b gy om oj l ok ol">Prediction:</span><span id="c09c" class="mz la it oe b gy om oj l ok ol">His fame in mathematical astronomy is due to the introduction of the astronomical globe and his early contributions to understanding the movement of the planets.</span><span id="57c1" class="mz la it oe b gy om oj l ok ol">In mathematical astronomy, his fame is due to the introduction of the astronomical globe and his early contributions to understanding the motion of the planets.</span><span id="0f3f" class="mz la it oe b gy om oj l ok ol">In mathematical astronomy his fame is due to the introduction of the astronomical globe and his early contributions to understanding the movement of the planets.</span></pre><p id="d26f" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi">3.</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="226d" class="mz la it oe b gy oi oj l ok ol">Original:</span><span id="7953" class="mz la it oe b gy om oj l ok ol">Why are people obsessed with Cara Delevingne?</span><span id="aaa7" class="mz la it oe b gy om oj l ok ol">Truth:</span><span id="1cf3" class="mz la it oe b gy om oj l ok ol">Why are people so obsessed with Cara Delevingne?</span><span id="ac36" class="mz la it oe b gy om oj l ok ol">Prediction:</span><span id="a538" class="mz la it oe b gy om oj l ok ol">Why do people fall in love with Cara Delevingne?</span><span id="ce1f" class="mz la it oe b gy om oj l ok ol">Why is everyone obsessed with Cara Delevingne?</span><span id="9a62" class="mz la it oe b gy om oj l ok ol">Why do people like Cara Delevingne?</span></pre><p id="57c9" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi">4.</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="7c5f" class="mz la it oe b gy oi oj l ok ol">Original:</span><span id="1bd3" class="mz la it oe b gy om oj l ok ol">Earl St Vincent was a British ship that was captured in 1803 and became a French trade man.</span><span id="9548" class="mz la it oe b gy om oj l ok ol">Truth:</span><span id="fc5f" class="mz la it oe b gy om oj l ok ol">Earl St Vincent was a British ship that was captured and became a French merchantman in 1803.</span><span id="a1cf" class="mz la it oe b gy om oj l ok ol">Prediction:</span><span id="7cf5" class="mz la it oe b gy om oj l ok ol">Earl St Vincent was a British ship captured in 1803 and became a French trader.</span><span id="d9e1" class="mz la it oe b gy om oj l ok ol">Earl St Vincent was a British ship captured in 1803 and became a French trade man.</span><span id="21cb" class="mz la it oe b gy om oj l ok ol">Earl St Vincent was a British ship that was captured in 1803 and became a French trade man.</span></pre><p id="2d66" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi">5.</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="0fad" class="mz la it oe b gy oi oj l ok ol">Original:</span><span id="a829" class="mz la it oe b gy om oj l ok ol">Worcester is a town and county city of Worcestershire in England.</span><span id="a3b9" class="mz la it oe b gy om oj l ok ol">Truth:</span><span id="b064" class="mz la it oe b gy om oj l ok ol">Worcester is a city and county town of Worcestershire in England.</span><span id="d1a3" class="mz la it oe b gy om oj l ok ol">Prediction:</span><span id="df8f" class="mz la it oe b gy om oj l ok ol">Worcester is a town and county of Worcestershire in England.</span><span id="f972" class="mz la it oe b gy om oj l ok ol">Worcester is a town and county town in Worcestershire in England.</span><span id="ab73" class="mz la it oe b gy om oj l ok ol">Worcester is a town and county town of Worcestershire in England.</span></pre><p id="11d8" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">6.域外句子</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="894c" class="mz la it oe b gy oi oj l ok ol">Original:</span><span id="784f" class="mz la it oe b gy om oj l ok ol">The goal of any Deep Learning model is to take in an input and generate the correct output.</span><span id="429a" class="mz la it oe b gy om oj l ok ol">Predictions &gt;&gt;&gt;<br/>The goal of any deep learning model is to take an input and generate the correct output.</span><span id="0521" class="mz la it oe b gy om oj l ok ol">The goal of a deep learning model is to take an input and generate the correct output.</span><span id="e151" class="mz la it oe b gy om oj l ok ol">Any Deep Learning model the goal of which is to take in an input and generate the correct output.</span></pre><p id="d7ec" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">从这些例子中可以看出，我们的 BART 模型已经很好地学会了生成释义！</p><h1 id="0eb5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">讨论</h1><h2 id="dfe6" class="mz la it bd lb na nb dn lf nc nd dp lj mr ne nf ll ms ng nh ln mt ni nj lp nk bi translated">潜在的问题</h2><p id="4bcd" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">生成的释义有时会有一些小问题，下面列出了其中一些。</p><ol class=""><li id="8ca8" class="nl nm it lx b ly lz mb mc mr nn ms no mt np mq nq nr ns nt bi translated">生成的序列几乎与原始序列完全相同，只有一两个词的细微差别。</li><li id="0c7d" class="nl nm it lx b ly nu mb nv mr nw ms nx mt ny mq nq nr ns nt bi translated">不正确或笨拙的语法。</li><li id="dc9c" class="nl nm it lx b ly nu mb nv mr nw ms nx mt ny mq nq nr ns nt bi translated">在域外(来自训练数据)输入上可能没有那么好。</li></ol><p id="386e" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">令人鼓舞的是，这些问题似乎非常罕见，并且最有可能通过使用更好的训练数据来避免(同样的问题有时也可以在训练数据中看到)。</p><h2 id="3bc2" class="mz la it bd lb na nb dn lf nc nd dp lj mr ne nf ll ms ng nh ln mt ni nj lp nk bi translated">包裹</h2><p id="882b" class="pw-post-body-paragraph lu lv it lx b ly mu ju ma mb mv jx md mr mw mg mh ms mx mk ml mt my mo mp mq im bi translated">像 BART 这样的序列到序列模型是 NLP 实践者箭筒中的另一支箭。它们对于涉及文本生成的任务特别有用，例如释义、摘要和抽象问题回答。</p><p id="9fad" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">解释可用于数据扩充，您可以通过解释可用数据来创建更大的数据集。</p></div></div>    
</body>
</html>