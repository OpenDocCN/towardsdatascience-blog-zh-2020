<html>
<head>
<title>Distributed Processing with PyArrow-Powered New Pandas UDFs in PySpark 3.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark 3.0 中基于 PyArrow 的新熊猫 UDF 的分布式处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distributed-processing-with-pyarrow-powered-new-pandas-udfs-in-pyspark-3-0-8f1fe4c15208?source=collection_archive---------13-----------------------#2020-08-19">https://towardsdatascience.com/distributed-processing-with-pyarrow-powered-new-pandas-udfs-in-pyspark-3-0-8f1fe4c15208?source=collection_archive---------13-----------------------#2020-08-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3796" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何使用 Spark 3.0.0 支持的 PySpark 实现高性能的类似熊猫的用户定义函数(UDF)</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/87595c4e6ed047e6c19a55cc9c0bbf83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xq56Kyb_iwKq7Fllv0Krpg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://pixabay.com/illustrations/block-chain-data-records-concept-4115197/" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kw">来源</strong> </a></p></figure><p id="fa32" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">数据处理时间非常宝贵，因为用户花费的每一分钟都要付出经济代价。本文主要面向希望使用 Apache Spark 最新增强功能的数据科学家和数据工程师，因为在很短的时间内，Apache Spark 已经成为下一代大数据处理引擎，并以前所未有的速度在整个行业得到广泛应用。</p><p id="9865" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Spark 的整合结构支持兼容和可构建的<a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/index.html" rel="noopener ugc nofollow" target="_blank"> API </a> s，这些 API 通过优化程序中构建的各种库和函数来实现高性能，使用户能够构建超越现有库的应用程序。它也为用户提供了在上面编写自己的分析库的机会。</p><p id="f96d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">数据迁移成本很高，因此 Spark 专注于对数据进行计算，而不管数据位于何处。在用户交互 API 中，Spark 努力管理这些看似广泛相关的存储系统，以防应用程序不需要关心它们的数据在哪里。</p><p id="18f0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">当数据太大而无法在一台机器上运行时，在一台机器上执行计算需要很长时间，这就促使 it 将数据放在多台服务器或计算机上。这种逻辑要求以分布式方式处理数据。<a class="ae kv" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark DataFrame </a>是一个终极的结构化 API，提供一个包含行和列的数据表。通过其列和列类型的模式，它可以跨越大量的数据源。</p><p id="cf45" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">本文的目的是介绍 Spark 3.0 当前发布的一个特性的好处，该特性与 pandas 和 Apache Arrow 使用 PySpark 相关，以便能够以并行方式执行类似 Pandas 的 UDF。在下面的标题中，PyArrow 对 PySpark 会话配置的重要使用，PySpark 启用的 Pandas UDFs 将通过提供相应主题的代码片段来详细解释。在文章的结尾，为进一步的研究添加了参考文献和附加资源。</p><h1 id="fce6" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">1.PyArrow 和 PySpark</h1><p id="1d23" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在以前版本的 Spark 中，在 PySpark 中将 DataFrame 转换为 Pandas 的步骤效率很低，需要将所有行收集到 Spark 驱动程序，将每一行序列化为 Python 的 pickle 格式<em class="mq">(逐行)、</em>，然后将它们发送到 Python 工作进程。在这个转换过程的最后，它将每一行拆成一个庞大的元组列表。为了能够克服这些无效操作，可以使用与<a class="ae kv" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>集成的<a class="ae kv" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Arrow </a>来实现更快的列数据传输和转换。</p><h2 id="d7bc" class="mr lu iq bd lv ms mt dn lz mu mv dp md lg mw mx mf lk my mz mh lo na nb mj nc bi translated">1.1.为什么使用 PyArrow 和 PySpark</h2><p id="ffcd" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><a class="ae kv" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Arrow </a>有助于加速从传统列内存到 pandas 对象的转换，提供高性能的内存列数据结构。</p><p id="6a16" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">以前，Spark 揭示了一个基于行的接口，用于解释和运行用户定义函数(UDF)。这在序列化和反序列化中引入了很高的开销，并且使得很难使用 Python 库，例如<a class="ae kv" href="http://www.numpy.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mq"> NumPy </em> </a>、<a class="ae kv" href="https://pandas.pydata.org/docs/" rel="noopener ugc nofollow" target="_blank"> <em class="mq"> Pandas </em> </a>，这些库是用本地 Python 编码的，这使得它们能够更快地编译成机器代码。</p><p id="dab3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在新提出的 UDF 中，它主张引入新的 API 来支持 Python 中的矢量化 UDF，其中通过逐块序列化<a class="ae kv" href="http://arrow.apache.org/overview/" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir"/></a><strong class="kz ir"/>而不是逐行序列化<strong class="kz ir"/>，将数据块以某种列格式转移到 Python 中执行。</p><p id="38be" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae kv" href="https://pandas.pydata.org/docs/" rel="noopener ugc nofollow" target="_blank"> Pandas </a>包得到了机器学习和数据科学专家的认可，因为它与大量 Python 库和包进行了一致的集成，包括<a class="ae kv" href="http://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"><em class="mq">scikit-learn</em></a><em class="mq">、</em><a class="ae kv" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"><em class="mq">matplotlib</em></a>和<a class="ae kv" href="http://www.numpy.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mq"> NumPy </em> </a>。</p><p id="3a63" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">此外，Pandas UDFs 支持用户在 Apache Spark 中分发数据负载和使用 Pandas APIs。</p><p id="5d7e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">用户自定义功能可参考<a class="ae kv" href="https://spark.apache.org/docs/3.0.2/sql-pyspark-pandas-with-arrow.html" rel="noopener ugc nofollow" target="_blank">官方网站</a>执行:</p><ul class=""><li id="220b" class="nd ne iq kz b la lb ld le lg nf lk ng lo nh ls ni nj nk nl bi translated"><a class="ae kv" href="http://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Arrow </a>通过利用 Arrow 列式内存布局加快字符串数据的处理，实现了数据在<a class="ae kv" href="https://en.wikipedia.org/wiki/Java_virtual_machine" rel="noopener ugc nofollow" target="_blank"> Java 虚拟机</a>和 Python 执行器之间的精确传输，且序列化成本为零。</li><li id="65ec" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls ni nj nk nl bi translated"><a class="ae kv" href="https://pandas.pydata.org/docs/" rel="noopener ugc nofollow" target="_blank"> Pandas </a>库使用它的实例和 API。</li></ul><h2 id="847b" class="mr lu iq bd lv ms mt dn lz mu mv dp md lg mw mx mf lk my mz mh lo na nb mj nc bi translated">1.2.火花会话配置</h2><p id="c443" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">为了获得更好的性能，在执行作业时，应设置如下配置。</p><p id="9e26" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了能够从 PyArrow 优化中获益，可以通过将此配置设置为默认禁用的<code class="fe nr ns nt nu b"><strong class="kz ir">true</strong></code> <strong class="kz ir"> </strong>来启用以下配置:<code class="fe nr ns nt nu b">spark.sql.execution.arrow.pyspark.enabled</code></p><p id="c9aa" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在出现错误的情况下，上面启用的优化可以退回到非箭头优化实现情况。为了应对 Spark 中实际计算出现的这个问题，<code class="fe nr ns nt nu b">fallback.enabled</code>将被设置为<code class="fe nr ns nt nu b"><strong class="kz ir">true</strong></code> <strong class="kz ir"> : </strong> <code class="fe nr ns nt nu b">spark.sql.execution.arrow.pyspark.fallback.enabled</code></p><p id="9758" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">由于以下原因，parquet-summary-metadata<strong class="kz ir">无法</strong>有效地启用以下配置:</p><ol class=""><li id="9884" class="nd ne iq kz b la lb ld le lg nf lk ng lo nh ls nv nj nk nl bi translated"><strong class="kz ir"><em class="mq">merge schema</em>=<em class="mq">false:</em></strong>假设所有拼花局部文件的模式都是相同的，因此，可以从任何局部文件中读取页脚。</li><li id="b6ec" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls nv nj nk nl bi translated"><strong class="kz ir"><em class="mq">merge schema</em>=<em class="mq">true</em></strong><em class="mq">:</em>所有文件都需要读取页脚来实现合并过程。</li></ol><pre class="kg kh ki kj gt nw nu nx ny aw nz bi"><span id="ae69" class="mr lu iq nu b gy oa ob l oc od">spark.sql.parquet.mergeSchema <strong class="nu ir">false</strong><br/>spark.hadoop.parquet.enable.summary-metadata <strong class="nu ir">false</strong></span></pre><p id="d7fb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">综上所述，Arrow 优化配置的最终推荐列表如下:</p><pre class="kg kh ki kj gt nw nu nx ny aw nz bi"><span id="31f5" class="mr lu iq nu b gy oa ob l oc od">"spark.sql.execution.arrow.pyspark.enabled", "<strong class="nu ir">true</strong>"<strong class="nu ir"><br/></strong>"spark.sql.execution.arrow.pyspark.fallback.enabled", "<strong class="nu ir">true</strong>"<br/>"spark.sql.parquet.mergeSchema", "<strong class="nu ir">false</strong>"<br/>"spark.hadoop.parquet.enable.summary-metadata", "<strong class="nu ir">false</strong>"</span></pre><h2 id="ebd7" class="mr lu iq bd lv ms mt dn lz mu mv dp md lg mw mx mf lk my mz mh lo na nb mj nc bi translated">1.3.要升级的包</h2><p id="5746" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">正确使用<strong class="kz ir"> PyArrow </strong>和<strong class="kz ir"> PandasUDF </strong>需要在 PySpark 开发平台中升级一些包。</p><p id="fa9c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">需要更新以下软件包列表，以便能够正确使用 Spark 3.0 的 PandasUDF 最新版本。</p><pre class="kg kh ki kj gt nw nu nx ny aw nz bi"><span id="ee72" class="mr lu iq nu b gy oa ob l oc od"><strong class="nu ir"># Install with Conda</strong><br/>conda install -c conda-forge pyarrow</span><span id="d394" class="mr lu iq nu b gy oe ob l oc od"><strong class="nu ir"># Install PyArrow with Python</strong><br/>pip install pyarrow==0.15.0</span><span id="13f4" class="mr lu iq nu b gy oe ob l oc od"><strong class="nu ir"># Install Py4j with Python</strong><br/>pip install py4j==0.10.9</span><span id="340d" class="mr lu iq nu b gy oe ob l oc od"><strong class="nu ir"># Install pyspark with Python</strong><br/>pip install pyspark==3.0.0</span></pre><p id="3757" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">此外，您可能需要分配一个新的环境变量，以便在运行 Pandas UDFs 时不会面临 py arrow 0 . 15 . 1 升级的任何问题。</p><pre class="kg kh ki kj gt nw nu nx ny aw nz bi"><span id="5fbc" class="mr lu iq nu b gy oa ob l oc od"><strong class="nu ir"># Environment Variable Setting for PyArrow Version Upgrade</strong><br/>import os<br/>os.environ["ARROW_PRE_0_15_IPC_FORMAT"] = "1"</span></pre><h1 id="69de" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.Python 的 PyArrow</h1><h2 id="6b22" class="mr lu iq bd lv ms mt dn lz mu mv dp md lg mw mx mf lk my mz mh lo na nb mj nc bi translated">2.1.更快地处理拼花格式的文件</h2><p id="1a2b" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><a class="ae kv" href="https://arrow.apache.org/docs/python/install.html" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir">py arrow</strong></a><strong class="kz ir"/>在读取拼花文件而非其他文件格式时，性能差距较大。在这个博客中，你可以找到关于不同文件格式读取的基准研究。</p><p id="5f2b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">它可用于 Python 中处理时间不同的不同种类的包:</p><ul class=""><li id="88a8" class="nd ne iq kz b la lb ld le lg nf lk ng lo nh ls ni nj nk nl bi translated"><strong class="kz ir">拼花箭头:</strong> <code class="fe nr ns nt nu b">pyarrow.parquet</code></li></ul><pre class="kg kh ki kj gt nw nu nx ny aw nz bi"><span id="fc9b" class="mr lu iq nu b gy oa ob l oc od"><strong class="nu ir"># Importing PyArrow </strong><br/>import pyarrow.parquet as pq</span><span id="ae16" class="mr lu iq nu b gy oe ob l oc od">path = "dataset/<a class="ae kv" href="https://www.kaggle.com/yassinealouini/m5-sales-hierarchy-dataset" rel="noopener ugc nofollow" target="_blank"><strong class="nu ir">dimension</strong></a>"</span><span id="b8b7" class="mr lu iq nu b gy oe ob l oc od">data_frame <strong class="nu ir">=</strong> pq<strong class="nu ir">.</strong>read_table(path)<strong class="nu ir">.</strong>to_pandas()</span></pre><ul class=""><li id="559d" class="nd ne iq kz b la lb ld le lg nf lk ng lo nh ls ni nj nk nl bi translated"><strong class="kz ir">拼花到带熊猫的箭头数据框:</strong> <code class="fe nr ns nt nu b">pyarrow.parquet</code>然后转换成<code class="fe nr ns nt nu b">pandas.DataFrame</code></li></ul><pre class="kg kh ki kj gt nw nu nx ny aw nz bi"><span id="9731" class="mr lu iq nu b gy oa ob l oc od">import pandas as pd<br/>import pyarrow as pa<br/>import pyarrow.parquet as pq</span><span id="9452" class="mr lu iq nu b gy oe ob l oc od">pandas_df = pd.DataFrame(data={'column_1': [1, 2], 'column_2': [3, 4], 'column_3': [5, 6]})</span><span id="1069" class="mr lu iq nu b gy oe ob l oc od">table = pa.Table.from_pandas(pandas_df, preserve_index=True)</span><span id="96d0" class="mr lu iq nu b gy oe ob l oc od">pq.write_table(table, 'pandas_dataframe.parquet')</span></pre><h2 id="b1a4" class="mr lu iq bd lv ms mt dn lz mu mv dp md lg mw mx mf lk my mz mh lo na nb mj nc bi translated">2.2.计算脚本处理时间</h2><p id="b900" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">只要我们关心编写的脚本的性能和处理速度，了解如何度量它们的处理时间是有益的。</p><p id="09ac" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">执行 Python 脚本时，存在两种类型的时间流逝处理计算。</p><blockquote class="of og oh"><p id="7b98" class="kx ky mq kz b la lb jr lc ld le ju lf oi lh li lj oj ll lm ln ok lp lq lr ls ij bi translated"><strong class="kz ir">处理器时间</strong>:测量特定进程在 CPU 上活动执行的时间。睡眠、等待 web 请求或时间不包括在内。<code class="fe nr ns nt nu b"><em class="iq">time.process_time()</em></code></p><p id="0f27" class="kx ky mq kz b la lb jr lc ld le ju lf oi lh li lj oj ll lm ln ok lp lq lr ls ij bi translated"><strong class="kz ir">挂钟时间</strong>:它计算“挂在墙上的钟”已经过了多少时间，即外面的实时时间。<code class="fe nr ns nt nu b"><em class="iq">time.perf_counter()</em></code></p></blockquote><p id="cba6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">还有其他方法可以计算运行脚本所花费的时间。</p><blockquote class="of og oh"><p id="04b4" class="kx ky mq kz b la lb jr lc ld le ju lf oi lh li lj oj ll lm ln ok lp lq lr ls ij bi translated"><code class="fe nr ns nt nu b"><em class="iq">time.time()</em></code>功能也是 quantifes time-passed 作为挂钟时间；然而，它可以被校准。为此，需要回到过去重置它。</p><p id="20c7" class="kx ky mq kz b la lb jr lc ld le ju lf oi lh li lj oj ll lm ln ok lp lq lr ls ij bi translated"><code class="fe nr ns nt nu b"><em class="iq">time.monotonic()</em></code>函数单调，简单向前；然而，与<code class="fe nr ns nt nu b"><em class="iq">time.perf_counter()</em></code>相比，其精度性能有所下降</p></blockquote><h1 id="9086" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">3.PySpark 与熊猫 UDF</h1><p id="4d32" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><a class="ae kv" href="https://docs.databricks.com/spark/latest/spark-sql/udf-python-pandas.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kz ir"> Pandas 用户定义的函数</strong> </a> <strong class="kz ir"> </strong>可以被识别为由 Apache Arrow 提供支持的矢量化 UDF，与一次一行<a class="ae kv" href="https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/udf-python" rel="noopener ugc nofollow" target="_blank">Python UDF</a>相比，它允许提供更高性能的矢量化操作。通过对定制功能的分布式处理，它们可以被视为<a class="ae kv" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>中最具影响力的改进。它们带来了无数的好处，包括让用户能够使用熊猫 API 和提高性能。</p><p id="5a75" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在 Python 中摄取<a class="ae kv" href="https://www.slideshare.net/ueshin/apache-arrow-and-pandas-udf-on-apache-spark" rel="noopener ugc nofollow" target="_blank"> Spark 定制函数结构</a>向 SQL 用户展示了它的高级功能，允许他们调用函数，而无需生成额外的脚本来连接他们的功能。</p><p id="8c29" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">功能可以通过<a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row" rel="noopener ugc nofollow" target="_blank"> <em class="mq">行</em></a><em class="mq"/><a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData" rel="noopener ugc nofollow" target="_blank"><em class="mq">组</em></a><em class="mq"/><em class="mq">窗口</em> 来执行，数据格式对于列可以使用<strong class="kz ir"><em class="mq"/></strong>，对于表结构可以使用<strong class="kz ir"><em class="mq">data frame</em></strong><em class="mq"/>。</p><h2 id="d266" class="mr lu iq bd lv ms mt dn lz mu mv dp md lg mw mx mf lk my mz mh lo na nb mj nc bi translated">3.1.标量熊猫 UDF</h2><p id="e348" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><em class="mq">标量型熊猫 UDF </em>可以描述为将一个或多个<strong class="kz ir">熊猫系列</strong>转化为一个<strong class="kz ir">熊猫系列</strong>。最终返回的数据序列大小应该与输入数据序列的大小相同。</p><pre class="kg kh ki kj gt nw nu nx ny aw nz bi"><span id="fb0b" class="mr lu iq nu b gy oa ob l oc od">import pandas as pd</span><span id="29d5" class="mr lu iq nu b gy oe ob l oc od">from pyspark.sql.functions import pandas_udf<br/>from pyspark.sql import Window</span><span id="e062" class="mr lu iq nu b gy oe ob l oc od">dataframe = spark.createDataFrame(<br/> [(1, 5), (2, 7), (2, 8), (2, 10), (3, 18), (3, 22), (4, 36)],<br/> (“index”, “weight”))</span><span id="b202" class="mr lu iq nu b gy oe ob l oc od"><strong class="nu ir"># The function definition and the UDF creation</strong><br/>@pandas_udf(“int”)<br/>def weight_avg_udf(weight: pd.Series) -&gt; float:<br/> return weight.mean()</span><span id="3a0f" class="mr lu iq nu b gy oe ob l oc od">dataframe.select(weight_avg_udf(dataframe[‘weight’])).show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/c6e25212dbd9514668ca10931e961b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*M8iDfjXtl67Vdzx6oTXZ-g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><h2 id="f59d" class="mr lu iq bd lv ms mt dn lz mu mv dp md lg mw mx mf lk my mz mh lo na nb mj nc bi translated">3.2.熊猫 UDF</h2><p id="3044" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><em class="mq">熊猫 UDF </em>的分组 Agg 可以定义为一个或多个<strong class="kz ir">熊猫系列</strong>转化为一个<strong class="kz ir">标量</strong>。最终返回的数据值类型要求是原语<em class="mq"> (boolean、byte、char、short、int、long、float、double) </em>数据类型。</p><pre class="kg kh ki kj gt nw nu nx ny aw nz bi"><span id="cd5c" class="mr lu iq nu b gy oa ob l oc od"><strong class="nu ir"># Aggregation Process on Pandas UDF</strong></span><span id="6764" class="mr lu iq nu b gy oe ob l oc od">dataframe.groupby("index").agg(weight_avg_udf(dataframe['weight'])).show()</span><span id="5e58" class="mr lu iq nu b gy oe ob l oc od">w = Window \<br/>    .partitionBy('index') \<br/>    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/9f1ad86cd88bcdf9a82d6cdbd9d13187.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*2BdXQtSL8EzcFnW_Er6frQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><pre class="kg kh ki kj gt nw nu nx ny aw nz bi"><span id="df15" class="mr lu iq nu b gy oa ob l oc od"><a class="ae kv" href="https://github.com/pinarersoy/PyArrow-Powered-New-Pandas-UDFs-in-Spark-3/blob/master/PySpark_PandasUDFs_in_one_file.ipynb?short_path=808df3f" rel="noopener ugc nofollow" target="_blank"><strong class="nu ir"># Print the windowed results</strong></a></span><span id="969e" class="mr lu iq nu b gy oe ob l oc od">dataframe.withColumn('avg_weight', weight_avg_udf(dataframe['weight']).over(w)).show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/c7727bb119626c8516e15536bf10d82e.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*Bm0rEQXngrgpAxsjgNldlQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><h2 id="4520" class="mr lu iq bd lv ms mt dn lz mu mv dp md lg mw mx mf lk my mz mh lo na nb mj nc bi translated">3.2.熊猫 UDF 组图</h2><p id="bb7b" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><em class="mq">熊猫 UDF 分组图</em>可以被识别为一个或多个<strong class="kz ir">熊猫数据帧</strong>到一个<strong class="kz ir">熊猫数据帧</strong>的转换。最终返回的数据大小可以是任意的。</p><pre class="kg kh ki kj gt nw nu nx ny aw nz bi"><span id="bd3b" class="mr lu iq nu b gy oa ob l oc od">import numpy as np</span><span id="1a22" class="mr lu iq nu b gy oe ob l oc od"><strong class="nu ir"># Pandas DataFrame generation</strong><br/>pandas_dataframe = pd.DataFrame(np.random.rand(200, 4))</span><span id="d1e5" class="mr lu iq nu b gy oe ob l oc od">def weight_map_udf(pandas_dataframe):<br/>    weight = pandas_dataframe.weight<br/>    return pandas_dataframe.assign(weight=weight - weight.mean())</span><span id="dcb6" class="mr lu iq nu b gy oe ob l oc od">dataframe.groupby("index").applyInPandas(weight_map_udf, schema="index int, weight int").show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/971dcc6c2cb25f0f3a6132a8fa634a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*-UI7SX_735J8QX1e7WVARQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="2675" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">根据输入和输出数据的规格，您可以通过向这些矢量化 UDF 添加更复杂的函数来在它们之间进行切换。</p><p id="47a1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">完整的实现代码和 Jupyter 笔记本都在我的<a class="ae kv" href="https://github.com/pinarersoy/PyArrow-Powered-New-Pandas-UDFs-in-Spark-3/blob/master/PySpark_PandasUDFs_in_one_file.ipynb?short_path=cba9063" rel="noopener ugc nofollow" target="_blank"> <strong class="kz ir"> GitHub </strong> </a> <strong class="kz ir">上。</strong></p><p id="53e7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">非常感谢您的提问和评论！</p></div><div class="ab cl op oq hu or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="ij ik il im in"><h1 id="69e0" class="lt lu iq bd lv lw ow ly lz ma ox mc md jw oy jx mf jz oz ka mh kc pa kd mj mk bi translated">4.参考</h1><ol class=""><li id="c7a3" class="nd ne iq kz b la ml ld mm lg pb lk pc lo pd ls nv nj nk nl bi translated"><a class="ae kv" href="https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/udf-python" rel="noopener ugc nofollow" target="_blank"> Python 用户定义函数</a></li><li id="ba02" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls nv nj nk nl bi translated"><a class="ae kv" href="https://pandas.pydata.org/pandas-docs/stable/reference/index.html" rel="noopener ugc nofollow" target="_blank">熊猫 API </a></li><li id="8bf5" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls nv nj nk nl bi translated"><a class="ae kv" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇火花</a></li><li id="870e" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls nv nj nk nl bi translated"><a class="ae kv" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇箭头</a></li></ol><h1 id="790a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">5.额外资源</h1><ol class=""><li id="1e3b" class="nd ne iq kz b la ml ld mm lg pb lk pc lo pd ls nv nj nk nl bi translated"><a class="ae kv" href="https://databricks.com/session/vectorized-udf-scalable-analysis-with-python-and-pyspark" rel="noopener ugc nofollow" target="_blank">矢量化 UDF:使用 Python 和 PySpark 进行可扩展分析</a></li><li id="16d9" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls nv nj nk nl bi translated"><a class="ae kv" href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/142158605138935/3546232059139201/7497868276316206/latest.html" rel="noopener ugc nofollow" target="_blank">Apache Arrow Tokyo Meetup 2018 演示</a></li><li id="571a" class="nd ne iq kz b la nm ld nn lg no lk np lo nq ls nv nj nk nl bi translated"><a class="ae kv" href="https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/" rel="noopener ugc nofollow" target="_blank">星火:权威指南</a></li></ol></div></div>    
</body>
</html>