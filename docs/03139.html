<html>
<head>
<title>COVID-19 Bert Literature Search Engine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">新冠肺炎伯特文学搜索引擎</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/covid-19-bert-literature-search-engine-4d06cdac08bd?source=collection_archive---------28-----------------------#2020-03-25">https://towardsdatascience.com/covid-19-bert-literature-search-engine-4d06cdac08bd?source=collection_archive---------28-----------------------#2020-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="8c69" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在过去的几周内，研究工作和研究论文的数量在持续增加，以对抗这种冠状新冠肺炎邪恶病毒。组织这些海量的数据现在是最重要的，这是这项工作(<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search" rel="noopener ugc nofollow" target="_blank">代码</a>，<a class="ae ko" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search#data-links" rel="noopener ugc nofollow" target="_blank">数据</a>)试图实现的，作为<a class="ae ko" href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=568" rel="noopener ugc nofollow" target="_blank">一个Kaggle竞赛</a>的一部分，该竞赛试图找到智能的解决方案来组织大量不断增加的研究知识。</p><p id="a6b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的方法是嵌入研究论文的段落，嵌入查询，然后运行余弦相似性来获得最相似的段落</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="ab gu cl ku"><img src="../Images/ce6eefc15fdfb1fb7a81a19378e22a3b.png" data-original-src="https://miro.medium.com/v2/1*UP-QyAhWIN3Y1Ky8o2M6Qg.gif"/></div></figure><p id="72b0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们构建了这个<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search" rel="noopener ugc nofollow" target="_blank">代码</a>以在google colab上无缝运行，与google drive连接，并使用<a class="ae ko" href="https://github.com/Kaggle/kaggle-api" rel="noopener ugc nofollow" target="_blank"> kaggle api </a>将数据直接下载到google colab，因此没有数据被下载到您的设备，也不需要强大的GPU，因为所有这些都是通过google colab免费完成的，您可以在这里找到处理后的数据和结果嵌入<a class="ae ko" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search#data-links" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="c4d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本教程中，我们的目标是通过一种使用BERT构建搜索引擎的方法，您可以向它提供一个问题(<strong class="js iu"> query </strong>)，比如</p><ul class=""><li id="a852" class="kx ky it js b jt ju jx jy kb kz kf la kj lb kn lc ld le lf bi translated">关于医疗保健已经发表了什么？</li><li id="a891" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">在实时医疗保健服务中使用人工智能，以人工无法完成的方式评估干预措施、风险因素和结果</li><li id="09bd" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">动员增援医务人员解决不堪重负的社区的短缺问题。</li><li id="b5d8" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">等问题。</li></ul></div><div class="ab cl ll lm hx ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="im in io ip iq"><p id="9497" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">我们的方法是</strong></p><ol class=""><li id="4045" class="kx ky it js b jt ju jx jy kb kz kf la kj lb kn ls ld le lf bi translated">提取每篇研究论文的段落(<a class="ae ko" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search#data-links" rel="noopener ugc nofollow" target="_blank">处理过的数据</a> ) ( <a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search#Data-Processing" rel="noopener ugc nofollow" target="_blank">代码段</a>)</li><li id="389d" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn ls ld le lf bi translated">从预训练的BERT中获得上下文化嵌入，该BERT在自然语言推理(NLI)数据(<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search#BERT" rel="noopener ugc nofollow" target="_blank">代码段</a>)上进行了微调</li><li id="8425" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn ls ld le lf bi translated">对查询应用上下文嵌入(<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search#BERT" rel="noopener ugc nofollow" target="_blank">代码段</a></li><li id="cd89" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn ls ld le lf bi translated">对段落和查询应用余弦相似度，得到最相似的段落，然后返回这些段落的论文(<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search#BERT" rel="noopener ugc nofollow" target="_blank">代码段</a>)</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="ab gu cl ku"><img src="../Images/ce6eefc15fdfb1fb7a81a19378e22a3b.png" data-original-src="https://miro.medium.com/v2/1*UP-QyAhWIN3Y1Ky8o2M6Qg.gif"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">BERT用于嵌入，然后余弦相似度得到相似段落</p></figure><h1 id="5c60" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">伯特是什么？</h1><p id="5049" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">已经提出了多种语言建模方法，它们可以分为两大类</p><ul class=""><li id="181c" class="kx ky it js b jt ju jx jy kb kz kf la kj lb kn lc ld le lf bi translated">基于递归的seq2seq模型</li><li id="ec50" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">基于变压器的模型(BERT)</li></ul><h2 id="0898" class="na ly it bd lz nb nc dn md nd ne dp mh kb nf ng ml kf nh ni mp kj nj nk mt nl bi translated">基于递归的seq2seq模型</h2><p id="4db3" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">使用<a class="ae ko" href="http://bit.ly/eazysum_tu4" rel="noopener ugc nofollow" target="_blank">LSTM(RNN的改进)</a>，用于<a class="ae ko" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">编码器解码器</a>架构</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nm"><img src="../Images/d6aa52cfd6ca565e6284df9ebe07a384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BVcONihxUPkEWSm1VQfMDQ.jpeg"/></div></div></figure><p id="86de" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">编码器</strong>使用<a class="ae ko" href="https://medium.com/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f" rel="noopener">双向LSTM </a>构建，对输入文本进行编码，构建内部编码，</p><p id="dbdb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">解码器</strong>接收生成的内部编码和参考字，解码器还包含<a class="ae ko" href="https://medium.com/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f" rel="noopener"> LSTM </a>，能够一次生成一个字的输出。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nr"><img src="../Images/d2b8ce4560850dc0ffa3f0733d4a2f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1P7RKicaFf7FfJaai70hvA.gif"/></div></div></figure><p id="b610" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以在我们的<a class="ae ko" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">系列</a>中了解更多关于使用基于seq2seq LSTM的模型进行<a class="ae ko" href="https://hackernoon.com/text-summarizer-using-deep-learning-made-easy-490880df6cd?source=post_stats_page---------------------------" rel="noopener ugc nofollow" target="_blank">文本摘要</a>的方法，在那里我们将详细介绍这些模型是如何构建的。</p><h2 id="3c0f" class="na ly it bd lz nb nc dn md nd ne dp mh kb nf ng ml kf nh ni mp kj nj nk mt nl bi translated">基于变压器的模型</h2><p id="0b7e" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">另一项研究工作试图在不使用递归模型的情况下建立语言模型，以使系统在处理长句时更加强大，因为LSTM发现很难表示长序列的数据，因此很难表示长句。</p><p id="7f61" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">变压器依赖于注意力模型，特别是<strong class="js iu">自我注意力</strong>，这是一种神经网络，用于理解如何关注输入句子中的特定单词，变压器也内置于编码器/解码器结构中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi ns"><img src="../Images/f1b4051051c970379df9441b858e7932.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KnpDCL3f9rT4ezum579TlQ.jpeg"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">来自http://jalammar.github.io/illustrated-transformer/<a class="ae ko" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">的</a></p></figure><p id="073c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">编码器和解码器各自包含一组块，</p><p id="2253" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">编码器:</strong>包含一个块堆栈，每个块包含(自关注，前馈网络)，在这里它接收输入，并以双向方式关注来自输入的所有文本，前一个和下一个单词，然后将其传递给前馈网络，这个结构(块)根据编码器中的块数重复多次</p><p id="ab7c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">解码器:</strong>然后在编码完成后，编码器将这个内部编码传递到解码器步骤，解码器步骤也包含多个块，其中每个块都包含相同的<em class="nt">自关注*(带有catch) </em>和一个编码器解码器关注，然后是一个前馈网络。自我关注的不同之处在于，它只关注前面的单词，而不是整个句子。因此解码器接收编码器的参考和内部编码(在概念上与seq2seq编码器-解码器循环模型的<a class="ae ko" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0?source=post_stats_page---------------------------" rel="noopener ugc nofollow" target="_blank">编码器相同)</a></p><p id="971c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以在jalammar的博客中了解更多关于Transformer架构的信息</p><h2 id="8835" class="na ly it bd lz nb nc dn md nd ne dp mh kb nf ng ml kf nh ni mp kj nj nk mt nl bi translated">现在伯特来了</h2><p id="f74b" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">结果是，<a class="ae ko" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">我们不需要整个Transformer采用一个可微调的语言模型来完成NLP任务</a>，我们可以只使用解码器，就像<a class="ae ko" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank"> OpenAI提出的</a>一样，然而，由于它使用解码器，模型只训练一个正向模型，而不考虑前面和后面(因此是双向的)，这就是为什么引入了BERT，我们只使用Transformer编码器。</p><p id="1df9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">BERT是对原始转换器的修改，它只依赖于编码器结构，我们只使用编码器块来应用双向方式，这看起来很不直观，事实就是如此！！，由于双向条件反射将允许每个单词在多层上下文中间接看到自己(这里有更多关于它的信息<a class="ae ko" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank"/>)，所以BERT在训练中使用了使用面具的巧妙方法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nu"><img src="../Images/e02d8a400878bf8461a8dd8ca5346d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ScoRNq4Gil8fwp-1.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">来自http://jalammar.github.io/illustrated-bert/<a class="ae ko" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">的</a></p></figure><p id="180f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在给定大量文本的情况下训练BERT，对15%的单词应用掩码，然后训练它预测被掩码的单词。</p><p id="76bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们主要使用一个预训练的BERT模型，然后使用它作为我们任务的基石，这些任务主要分为两种类型</p><ol class=""><li id="7a2a" class="kx ky it js b jt ju jx jy kb kz kf la kj lb kn ls ld le lf bi translated">任务具体任务(<strong class="js iu">问答，文本摘要，分类，单句标注</strong>，……。)</li><li id="b409" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn ls ld le lf bi translated">构建一个<strong class="js iu">语境化的单词嵌入</strong>，<em class="nt">这就是我们今天的目标</em>。</li></ol><p id="3c87" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以让我们建立一个<strong class="js iu">语境化的单词嵌入</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nv"><img src="../Images/a68f44b5241054087de395f262843801.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*U6SDizeTWD4a7JBs.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">来自<a class="ae ko" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-bert/</a></p></figure><p id="d694" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">实际上有多种方式从BERT编码器块(在这个例子中是12个块)生成嵌入</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nw"><img src="../Images/5211fac8ee0f74a5921f17805443911a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vbutJtm-V-je_x8F.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">来自<a class="ae ko" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-bert/</a></p></figure><p id="58c8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本教程中，我们将专注于使用预训练的BERT构建句子嵌入的任务，我们将简单地将我们的句子传递给预训练的BERT，以生成我们自己的上下文嵌入。</p><h1 id="b9cd" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">B-我们的方法:</h1><h2 id="a868" class="na ly it bd lz nb nc dn md nd ne dp mh kb nf ng ml kf nh ni mp kj nj nk mt nl bi translated">1.将冠状病毒新冠肺炎的文献数据集分成段落，数据集<a class="ae ko" href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge" rel="noopener ugc nofollow" target="_blank">可以在这里找到</a>在kaggle竞赛中，(<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search#Data-Processing" rel="noopener ugc nofollow" target="_blank">代码段</a></h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6282d50cc0ab35435e94aab1440b797f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*BavWxGLsgs9wJlLiUpEV2w.jpeg"/></div></figure><p id="771d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">处理过的数据集可以在<a class="ae ko" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search#data-links" rel="noopener ugc nofollow" target="_blank">这里</a>找到，读取和处理json文件的步骤可以在<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search#Data-Processing" rel="noopener ugc nofollow" target="_blank">这里</a>找到，在这里我们将json文件转换成csv，我们使用与<a class="ae ko" href="https://www.kaggle.com/maksimeren/covid-19-literature-clustering" rel="noopener ugc nofollow" target="_blank"> maksimeren </a>相同的过程</p><h2 id="bb17" class="na ly it bd lz nb nc dn md nd ne dp mh kb nf ng ml kf nh ni mp kj nj nk mt nl bi translated">2.对句子进行编码(<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search#Data-Processing" rel="noopener ugc nofollow" target="_blank">代码段</a>)</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/af403d66dc871f82a70dd583cb71c21a.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/1*OyQEFeuK8eOsdX86eONBiA.gif"/></div></figure><p id="8e61" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们使用由<a class="ae ko" href="https://github.com/UKPLab" rel="noopener ugc nofollow" target="_blank"> UKPLab </a>提供的名为<a class="ae ko" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">句子转换器</a>的库，这个库使得使用BERT和其他架构如ALBERT和XLNet进行句子嵌入变得非常容易，它们还提供了简单的接口来查询和聚集数据。</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="a48e" class="na ly it oa b gy oe of l og oh">!pip install -U sentence-transformers</span></pre><p id="d534" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们将下载预训练的BERT模型，该模型根据自然语言推理(NLI)数据进行了微调(<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search#BERT" rel="noopener ugc nofollow" target="_blank">代码部分</a>)</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="51be" class="na ly it oa b gy oe of l og oh"><strong class="oa iu">from</strong> <strong class="oa iu">sentence_transformers</strong> <strong class="oa iu">import</strong> SentenceTransformer<br/><strong class="oa iu">import</strong> <strong class="oa iu">scipy.spatial</strong><br/><strong class="oa iu">import</strong> <strong class="oa iu">pickle</strong> <strong class="oa iu">as</strong> <strong class="oa iu">pkl</strong><br/>embedder = SentenceTransformer('bert-base-nli-mean-tokens')</span></pre><p id="ea9c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后我们将对段落列表进行编码(<a class="ae ko" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search" rel="noopener ugc nofollow" target="_blank">处理后的数据可以在这里找到</a>)</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="5013" class="na ly it oa b gy oe of l og oh">corpus = df_sentences_list<br/><em class="nt">corpus_embeddings = embedder.encode(corpus,show_progress_bar=True)</em></span></pre><h2 id="e7aa" class="na ly it bd lz nb nc dn md nd ne dp mh kb nf ng ml kf nh ni mp kj nj nk mt nl bi translated">3.编码查询并运行相似性(<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search#BERT" rel="noopener ugc nofollow" target="_blank">代码段</a>)</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/1b0d782a3d6f5603c053479629d15a22.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/1*Dp-5xG9EE4bcX8-KJqDpuA.gif"/></div></figure><p id="cafc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">查询是我们需要找到答案的句子，或者换句话说，在段落数据集中搜索相似的段落，从而找到相似的文献论文</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="a605" class="na ly it oa b gy oe of l og oh"><em class="nt"># Query sentences:</em><br/>queries = ['What has been published about medical care?',</span><span id="1500" class="na ly it oa b gy oi of l og oh">           'Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest',</span><span id="05f5" class="na ly it oa b gy oi of l og oh">           'Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually',</span><span id="4aa0" class="na ly it oa b gy oi of l og oh">           'Resources to support skilled nursing facilities and long term care facilities.',</span><span id="b7f1" class="na ly it oa b gy oi of l og oh">           'Mobilization of surge medical staff to address shortages in overwhelmed communities .',</span><span id="1dcf" class="na ly it oa b gy oi of l og oh">           'Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure – particularly for viral etiologies .']</span><span id="2377" class="na ly it oa b gy oi of l og oh">query_embeddings = embedder.encode(queries,show_progress_bar=<strong class="oa iu">True</strong>)</span></pre><p id="4abd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们将运行嵌入的查询和先前嵌入的段落之间的余弦相似性，并返回5个最相似的段落，以及它们的论文的细节</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="eaf0" class="na ly it oa b gy oe of l og oh"><em class="nt"># Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity</em><br/>closest_n = 5<br/>print("<strong class="oa iu">\n</strong>Top 5 most similar sentences in corpus:")<br/><strong class="oa iu">for</strong> query, query_embedding <strong class="oa iu">in</strong> zip(queries, query_embeddings):<br/>    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, "cosine")[0]<br/><br/>    results = zip(range(len(distances)), distances)<br/>    results = sorted(results, key=<strong class="oa iu">lambda</strong> x: x[1])<br/><br/>    <strong class="oa iu">for</strong> idx, distance <strong class="oa iu">in</strong> results[0:closest_n]:<br/>        print("Score:   ", "(Score: <strong class="oa iu">%.4f</strong>)" % (1-distance) , "<strong class="oa iu">\n</strong>" )<br/>        print("Paragraph:   ", corpus[idx].strip(), "<strong class="oa iu">\n</strong>" )<br/>        row_dict = df.loc[df.index== corpus[idx]].to_dict()<br/>        print("paper_id:  " , row_dict["paper_id"][corpus[idx]] , "<strong class="oa iu">\n</strong>")<br/>        print("Title:  " , row_dict["title"][corpus[idx]] , "<strong class="oa iu">\n</strong>")<br/>        print("Abstract:  " , row_dict["abstract"][corpus[idx]] , "<strong class="oa iu">\n</strong>")<br/>        print("Abstract_Summary:  " , row_dict["abstract_summary"][corpus[idx]] , "<strong class="oa iu">\n</strong>")</span></pre><h1 id="e0ce" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">C-结果</h1><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="a4ed" class="na ly it oa b gy oe of l og oh">=========================================================<br/>==========================Query==========================<br/>=== What has been published about medical care? =========<br/>=========================================================<br/>Score:    (Score: 0.8296)<br/>Paragraph:    how may state authorities require persons to undergo medical treatment<br/>Title:    Chapter 10 Legal Aspects of Biosecurity</span><span id="832c" class="na ly it oa b gy oi of l og oh">----------------------------------</span><span id="89cc" class="na ly it oa b gy oi of l og oh">Score:    (Score: 0.8220) <br/>Paragraph:    to identify how one health has been used recently in the medical literature<br/>Title:    One Health and Zoonoses: The Evolution of One&lt;br&gt;Health and Incorporation of Zoonoses<br/></span><span id="2924" class="na ly it oa b gy oi of l og oh">=========================================================<br/>==========================Query==============================<br/>=== Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest =====<br/>=========================================================</span><span id="ad02" class="na ly it oa b gy oi of l og oh">Score:    (Score: 0.8139) <br/>Paragraph:    clinical signs in hcm are explained by leftsided chf complications of arterial thromboembolism ate lv outflow tract obstruction or arrhythmias capable of<br/>Title:    Chapter 150 Cardiomyopathy</span><span id="4b70" class="na ly it oa b gy oi of l og oh">------------------------------------<br/>Score:    (Score: 0.7966) <br/>Paragraph:    the term arrhythmogenic cardiomyopathy is a useful expression that refers to recurrent or persistent ventricular or atrial arrhythmias in the setting of a normal echocardiogram the most commonly observed rhythm disturbances are pvcs and ventricular tachycardia vt however atrial rhythm disturbances may be recognized including atrial fibrillation paroxysmal or sustained atrial tachycardia and atrial flutter<br/>Title:    Chapter 150 Cardiomyopathy</span><span id="4e98" class="na ly it oa b gy oi of l og oh">=========================================================<br/>==========================Query==========================<br/>=== Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually <br/>=========================================================<br/>Score:    (Score: 0.8002) <br/><br/>Paragraph:    conclusion several methods and approaches could be used in the healthcare arena time series is an analytical tool to study diseases and resources management at healthcare institutions the flexibility to follow up and recognize data patterns and provide explanations must not be neglected in studies of healthcare interventions in this study the arima model was introduced without the use of mathematical details or other extensions to the model the investigator or the healthcare organization involved in disease management programs could have great advantages when using analytical methodology in several areas with the ability to perform provisions in many cases despite the analytical possibility by statistical means this approach does not replace investigators common sense and experience in disease interventions</span><span id="637a" class="na ly it oa b gy oi of l og oh">Title:    Disease management with ARIMA model in time&lt;br&gt;series</span><span id="34bf" class="na ly it oa b gy oi of l og oh"><br/>-------------------------------------------<br/>Score:    (Score: 0.7745) <br/>Paragraph:    whether the health sector is in fact more skillintensive than all other sectors is an empirical question as is that of whether the incidence of illness and the provision and effectiveness of health care are independent of labour type in a multisectoral model with more than two factors possibly health carespecific and other reallife complexities the foregoing predictions are unlikely to be wholly true nevertheless these effects will still operate in the background and thus give a useful guide to the interpretation of the outcomes of such a model</span><span id="8a8b" class="na ly it oa b gy oi of l og oh">Title:    A comparative analysis of some policy options&lt;br&gt;to reduce rationing in the UK's NHS: Lessons from a&lt;br&gt;general equilibrium model incorporating positive&lt;br&gt;health effects</span></pre><p id="d36e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有关完整结果，请参考<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search" rel="noopener ugc nofollow" target="_blank">我们的准则笔记本</a></p><h1 id="1108" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">D-评论</h1><p id="2e29" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">我们对两者都印象深刻，</p><ul class=""><li id="8edc" class="kx ky it js b jt ju jx jy kb kz kf la kj lb kn lc ld le lf bi translated">简单易用的<a class="ae ko" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">句子转换器</a>库，这使得应用BERT嵌入和提取相似性变得非常容易。</li><li id="fa8a" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">结果的质量给我们留下了深刻的印象，因为BERT是建立在表示文本上下文的概念上的，使用它可以得到真正相关的答案</li><li id="79fa" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">我们相信，通过使用段落本身，而不仅仅是论文的摘要，我们不仅能够返回最相似的论文，而且能够返回论文中最相似的部分。</li><li id="20fa" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">我们希望通过这一点，我们正在帮助构建一个不断增加的文献研究工作的世界，以对抗这种冠状新冠肺炎病毒。</li></ul><h1 id="553e" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">电子参考文献</h1><ul class=""><li id="008d" class="kx ky it js b jt mv jx mw kb oj kf ok kj ol kn lc ld le lf bi translated">我们使用由<a class="ae ko" href="https://github.com/UKPLab" rel="noopener ugc nofollow" target="_blank"> UKPLab </a>提供的名为<a class="ae ko" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">句子转换器</a>的库，这个库使得使用BERT和其他架构如ALBERT、XLNet进行句子嵌入变得非常容易，它们还提供了一个简单的接口来查询和聚集数据。</li><li id="1d83" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">我们使用了来自<a class="ae ko" href="https://www.kaggle.com/maksimeren/covid-19-literature-clustering" rel="noopener ugc nofollow" target="_blank"> maksimeren </a>的代码进行数据处理，我们真心感谢他。</li><li id="22d4" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">我们使用了画伯特的概念，在这里讨论<a class="ae ko" href="http://jalammar.github.io/" rel="noopener ugc nofollow" target="_blank"> Jay Alammar </a>在说明我们的建筑如何工作时，我们也参考了他所做的多个插图和解释，他的博客信息量非常大，很容易理解。</li><li id="65c5" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">我们使用Conneau等人在2017年讨论的预训练模型，在InferSent-Paper(从自然语言推理数据中监督学习通用句子表示)中显示，对自然语言推理(NLI)数据的训练可以产生通用句子嵌入。</li><li id="dc68" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated"><a class="ae ko" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">注意就是你所需要的</strong></a><strong class="js iu"/>变压器纸</li><li id="be0d" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated"><a class="ae ko" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">伯特</a>，<a class="ae ko" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">伯特代码</a></li></ul></div><div class="ab cl ll lm hx ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="im in io ip iq"><p id="40c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本教程的代码可以在这里找到<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search" rel="noopener ugc nofollow" target="_blank">这里</a>，代码被构建为在google colab上无缝运行，使用其免费使用的GPU，我们还使用kaggle API将数据直接下载到google colab，因此不需要既在本地运行代码也不需要在本地下载数据集。</p><p id="9d23" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们还提供了所有经过处理的数据集和嵌入的段落(1GB)，这些都需要一些时间来嵌入(所以你可以简单地使用它而无需再次运行嵌入)，链接<a class="ae ko" href="https://github.com/theamrzaki/COVID-19-BERT-ResearchPapers-Semantic-Search" rel="noopener ugc nofollow" target="_blank">这里</a>，这些都托管在google drive上，我们已经构建了<a class="ae ko" href="https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search" rel="noopener ugc nofollow" target="_blank">代码</a>，以无缝连接到google drive，(了解更多有关连接google drive <a class="ae ko" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">这里</a>)</p></div><div class="ab cl ll lm hx ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="im in io ip iq"><p id="32f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们真的相信，通过本教程，您已经对BERT有了更多的了解，以及如何方便地使用它，我们也希望通过本教程，我们能够帮助研究界对抗冠状病毒(新冠肺炎)。</p><p id="7f66" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">希望你们都平安无事。</p></div></div>    
</body>
</html>