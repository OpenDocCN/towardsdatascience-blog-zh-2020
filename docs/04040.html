<html>
<head>
<title>Statistical Learning (III) Tree-Based Method</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">统计学习(三)基于树的方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/statistical-learning-iii-tree-based-method-fdade017bee7?source=collection_archive---------58-----------------------#2020-04-13">https://towardsdatascience.com/statistical-learning-iii-tree-based-method-fdade017bee7?source=collection_archive---------58-----------------------#2020-04-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9d6e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">浏览装袋、随机森林和助推的概念。使用python进行随机森林和梯度增强的实际体验。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ef468572cb7b3bf311228ed19b1e08e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCpreZia1SaivPsndZ6FHw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">尼基塔·万托林在<a class="ae ky" href="https://unsplash.com/collections/1680217/workspace?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="3667" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如今，越来越多的人喜欢应用基于树的模型来处理回归和分类方面的机器学习问题。使用基于树的模型有几个好处，比如输出更好的结果、易于使用以及知道特征的重要性。另一方面，它需要更多的计算能力，并经常导致过拟合问题。基于树的方法可以关注线性回归和分类问题。这篇博客文章将进一步阐明分类问题。</p><p id="cd36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，您将了解到:</p><p id="8ded" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(1)基于树的模型的测量值</p><p id="c95d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(2)介绍装袋、出袋误差、随机森林、助推。</p><p id="77af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(3)随机森林和梯度增强在Python中的应用</p><p id="7eba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">基尼指数:</strong>I级总方差的量度，p(i)代表分类到第I级的概率。值的范围从0到1，其中0表示大多数元素属于某个类，1表示所有元素随机分布在各个类中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/7844864dafbd5bd00ab9e920dafe396f.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*zzJ1fXnBRvIGn6gmZKf8Yw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基尼指数</p></figure><p id="24c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">交叉熵:</strong>交叉熵对概率值取对数，优点是照顾低概率值。这种函数将防止概率值减小，并用于分类模型的损失函数。在其他讲座中，熵被称为树分裂指数。熵和交叉熵都可以用于基于树的模型中的分割方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/9665b55b1af8d93a16d2892472a14704.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*IlwEf1vhN4J2LveN1b1eSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">交叉熵</p></figure><p id="e0ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基尼指数和交叉熵都被用来确定分类树上分支的分裂。当父节点的基尼指数或交叉熵高于左右子节点的平均值时，就会发生分裂。你可以链接到“<a class="ae ky" rel="noopener" target="_blank" href="/gini-index-vs-information-entropy-7a7e4fed3fcb">关于基尼指数和交叉熵</a>的更多细节”的文章。</p><h1 id="968f" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">制袋材料</h1><p id="a2a3" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">装袋的目的是克服决策树的高方差问题。Bagging是决策树的常用方法，它依赖bootstrap来减少统计学习方法的方差，而bootstrap是一种常用的统计重采样技术来近似采样分布。假设有A1，A2，…..一个n独立的例子，每个例子都有方差<strong class="lb iu"> σ，</strong>例子的平均方差给出为<strong class="lb iu"> σ /n </strong>。因此<strong class="lb iu">平均</strong>法有助于<strong class="lb iu">减少方差</strong>。换句话说，bagging从训练数据集中获取几个<strong class="lb iu">自举样本</strong>，并计算所有预测的平均值。的确，装袋可以改善方差的减少，尤其是对于决策树环境下的回归问题。就分类而言，bagging将对测试集中的预测类进行多数表决。</p><h1 id="7118" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">袋外误差估计</h1><p id="9578" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">打包总是分别采用增强的示例子集来拟合模型。平均而言，每棵袋装树占总自举观测值的三分之二，剩余的三分之一称为袋外(OOB)观测值。每次模型在第1次OOB观测时进行预测，根据分类误差对预测类取<strong class="lb iu"> RMSE </strong> ( <strong class="lb iu">回归</strong>)或<strong class="lb iu">多数票</strong>(<strong class="lb iu">分类</strong>)。对于bagging来说，从大数据集估计OOB测试误差相对方便，而对于大的计算资源，则需要交叉验证方法。</p><h1 id="1a07" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">随机森林</h1><p id="3e11" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">随机森林是一种在自举训练示例上结合几个决策树的方法。查看树上的每个分割，从所有p个预测因子(m∞√p)中以1/m的随机样本进行分割，而每个决策树中总共有m个预测因子。</p><h2 id="1401" class="mu ly it bd lz mv mw dn md mx my dp mh li mz na mj lm nb nc ml lq nd ne mn nf bi translated">变量(特征)重要性</h2><p id="c2bb" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">为了解释基于树的方法的结果，树结构图通常是一种很好的方法，可以看出考虑到分裂方法的每个方差中的基尼指数有多高。总体来看，<strong class="lb iu">套袋分类树</strong>考虑了<strong class="lb iu">基尼指数</strong>，而<strong class="lb iu"> RSS(残差平方和)</strong>则是<strong class="lb iu">套袋回归树</strong>。只要模型变得更复杂，就很难分析袋装树增加时每个方差的重要性。对于回归和分类，当每棵树形成时，RSS和Gini指数的值将在每次分裂中降低。然后，在每棵树中累加减少的值。然后，随机森林模型取所有树中值的平均值，而值越大，预测值对模型性能的影响就越重要。</p><h2 id="fd48" class="mu ly it bd lz mv mw dn md mx my dp mh li mz na mj lm nb nc ml lq nd ne mn nf bi translated">随机森林装袋法</h2><p id="750a" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">一般来说，随机森林方法优于bagging方法，因为随机森林模型更有效地<strong class="lb iu">减少方差</strong>，并使其在模型性能上更可靠。在袋装树的集合中，大多数树被顶部的强预测器分割，因此每棵树产生相似的性能并具有高度相关的结果。因此，取高度相关树的平均值比取不相关树的平均值要差得多。另一方面，随机森林能够通过为每次分裂考虑预测因子的子集来避免该问题。由于p是每棵树中的预测值，m是预测值的总数，所以<strong class="lb iu"> (p-m)/p </strong>分裂的平均值不仅会选择强预测值，还会选择其他预测值。因此，每棵装袋的树最终都被<strong class="lb iu">去相关</strong>并更好地减少差异。</p><p id="713d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从下图可以看出，函数m ≈ √p优于其余两个函数，因为<strong class="lb iu"> m ≈ √p </strong>用于<strong class="lb iu">随机森林</strong>分类器中的预测函数，而<strong class="lb iu"> m=p </strong>代表<strong class="lb iu">打包</strong>函数。当参数p等于500时，我们可以看到，当树在450左右生长时，测试误差线变得稳定。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/d86e6fc1fbe25c5f401ac68c04d42a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QADL8z8os9rfmSGCXSbJyg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">摘自《统计学习导论》第322页</p></figure><h1 id="188c" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">助推</h1><p id="6471" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">与bagging不同，boosting不为每个决策树获取引导样本，而是使用来自前一个树的信息按顺序生长每个树。每一次，树都基于残差来拟合模型。当随后的树被输入到拟合函数中时，残差保持更新。因此，改进的拟合函数允许树以不同的形状生长，这有助于减少残差。同时，这种统计方法学习缓慢，因为收缩参数λ减慢了该过程。</p><h2 id="d253" class="mu ly it bd lz mv mw dn md mx my dp mh li mz na mj lm nb nc ml lq nd ne mn nf bi translated">关于三个主要参数的更多细节描述如下:</h2><ul class=""><li id="4f5e" class="nh ni it lb b lc mp lf mq li nj lm nk lq nl lu nm nn no np bi translated"><strong class="lb iu">树的数量(B) </strong>:交叉验证用于选择B。与套袋和随机森林相比，当B较大时，套袋会导致过度拟合。</li><li id="c714" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><strong class="lb iu">收缩参数(λ) </strong>:已知boosting的学习率，在0.01-0.001之间。为了获得良好的性能，<strong class="lb iu">小λ </strong>伴随着B 的<strong class="lb iu">大值。</strong></li><li id="ae7d" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><strong class="lb iu">每棵树中d分裂的数量</strong>:它指的是增强系综的复杂度。d变量决定了模型的<strong class="lb iu">交互深度</strong>，d输出树中的d+1个节点。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/3020241b7f787bd200922e34806913c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9fHlxKUzrRgatqqqAyftkQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">摘自《统计学习导论》一书，第323页</p></figure><h1 id="8ab6" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">实践经验:</h1><p id="e6f3" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我们将使用Python包遍历应用于示例数据集的基于树的模型。该数据集是《ka ggle:Titanic——灾难中的机器学习》中最受欢迎的用例。这个用例是用机器学习算法预测泰坦尼克号上的生存。kaggle网站链接:<a class="ae ky" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank">泰坦尼克号数据集</a>。在通过基于树的模型深入研究预测方法之前，让我们先了解一些基本方法，以便为预测准备好训练和测试数据。数据集已经过预处理，以<strong class="lb iu">排除空值</strong>、<strong class="lb iu">低相关列</strong>，并为文本特征创建<strong class="lb iu">虚拟变量</strong>。</p><h2 id="339b" class="mu ly it bd lz mv mw dn md mx my dp mh li mz na mj lm nb nc ml lq nd ne mn nf bi translated">随机森林:</h2><p id="54cb" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">为了生成下面的简单树形图，模型设置有参数max_depth=5，max_leaf_nodes=15，其中通过相对减少杂质来选择<strong class="lb iu"> max_leaf_nodes </strong>。叶节点是下面没有任何子节点的末端节点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/22ef02c4f14530dd897c4456307d735c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZ72lNZe2AOghkIiLNG8iA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成树形图的代码片段</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/0ca17c07df5e7dde8619b44c47a368dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GfraUd6oKS8A8FveW_K8WQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机森林模型中的树图</p></figure><p id="9361" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong></p><ul class=""><li id="8818" class="nh ni it lb b lc ld lf lg li ny lm nz lq oa lu nm nn no np bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c"> Link1 </a>，<a class="ae ky" rel="noopener" target="_blank" href="/random-forest-in-python-24d0893d51c0"> Link2 </a>:关于如何从随机森林模型生成树图的更多细节</li><li id="206f" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">需要GraphViz开源工具。建议通过brew命令行安装，而不是通过pip</li></ul><p id="1308" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据模型结果，利用不同的特征对随机森林的分类问题进行预测。使用基尼系数的平均下降值<strong class="lb iu">计算特征重要性值</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/1e6c85080ba61d49b47c8f7fea021597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5S5mklSbMkfRZE3z0-IGyA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">功能重要性代码片段</p></figure><h1 id="26a6" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">在Python中提升模型</h1><h2 id="101b" class="mu ly it bd lz mv mw dn md mx my dp mh li mz na mj lm nb nc ml lq nd ne mn nf bi translated">XGBoost V.S. AdaBoost V.S .梯度升压</h2><p id="3e2a" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated"><strong class="lb iu"> XGBoost: </strong>应用牛顿推进，提供了到达局部最小值的路径，但不是通过梯度下降。</p><p id="eff0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> AdaBoost: </strong>更新误分类数据点的权重</p><p id="3ef7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">梯度推进:</strong>通过残差计算的损失函数更新预测器</p><p id="dc38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几篇很棒的文章给出了每种提升方法的详细信息。<br/> <a class="ae ky" rel="noopener" target="_blank" href="/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725">链接1 </a>、<a class="ae ky" href="https://hackernoon.com/boosting-algorithms-adaboost-gradient-boosting-and-xgboost-f74991cad38c" rel="noopener ugc nofollow" target="_blank">链接2 </a>、<a class="ae ky" href="https://medium.com/hackernoon/gradient-boosting-and-xgboost-90862daa6c77" rel="noopener">链接3 </a></p><h2 id="83d8" class="mu ly it bd lz mv mw dn md mx my dp mh li mz na mj lm nb nc ml lq nd ne mn nf bi translated">梯度推进分类器:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/510bdff221a8bd4fbe050bbcb698e103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3InDZDYBANV6plPaWF1M7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度增强代码段</p></figure><p id="7fb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着估计数的增加，模型过度拟合的可能性越大。建议进行参数调整或指定其他参数来克服过拟合问题，而不是最大化决策树的总数。</p><p id="9c78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从下面的图中，我们可以看到整体提升模型的性能比随机森林模型好得多。很明显，深度1模型比深度2模型显示出更低的测试误差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/cbd26b0394f84930644031f71fd5fcab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N0kTxnL8OqyzTMuwYAbzbA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">摘自《统计学习导论》一书，第324页</p></figure><h1 id="c2e1" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">总之:</h1><ul class=""><li id="4bd8" class="nh ni it lb b lc mp lf mq li nj lm nk lq nl lu nm nn no np bi translated">Bagging使用bootstrapped方法，该方法获取训练数据的多个副本，拟合每个决策树中的每个重复样本，并组合所有树以获得预测模型。</li><li id="b880" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">随机森林模型为每个决策树使用boostrapped示例。特征重要性值由基尼指数(分类问题)和残差平方和(回归问题)决定。</li><li id="407f" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">当新树基于先前的树生长时，提升优于随机森林模型。此外，每棵树的深度越小，模型的性能就越好。在Boosting中，每棵树都从以前生长的树中学习，因此需要更多的时间进行训练。</li><li id="5304" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">基于树的模型需要超参数调整，因为这通常会导致过度拟合。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/b07e6cc5a2103c9109bc0c99bd18ab61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u10EOgtnRawYG8U7Pq63Rg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://www.memesmonkey.com/topic/easter#&amp;gid=1&amp;pid=1" rel="noopener ugc nofollow" target="_blank">HAPPY-WISHES.NET</a></p></figure><p id="5261" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">享受漫长的周末！！</strong></p><h2 id="6cb9" class="mu ly it bd lz mv mw dn md mx my dp mh li mz na mj lm nb nc ml lq nd ne mn nf bi translated">感谢令人敬畏的教科书:统计学习导论。我从中学习概念，并推荐读者阅读以了解更多细节。</h2></div></div>    
</body>
</html>