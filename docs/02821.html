<html>
<head>
<title>Word embeddings with code2vec, GloVe, and spaCy.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">code2vec、GloVe 和 spaCy 的单词嵌入。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-embeddings-with-code2vec-glove-and-spacy-5b26420bf632?source=collection_archive---------21-----------------------#2020-03-18">https://towardsdatascience.com/word-embeddings-with-code2vec-glove-and-spacy-5b26420bf632?source=collection_archive---------21-----------------------#2020-03-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7371" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何根据您的用例选择单词嵌入算法？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e72028ca5a3ed5748e5db407463d1575.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eKjt3uCh5k8PNqa5B00azQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://pixabay.com/users/lukasbieri-4664461/" rel="noopener ugc nofollow" target="_blank">卢卡斯比耶里</a>经由<a class="ae ky" href="https://pixabay.com/photos/laptop-macbook-home-office-switched-2838917/" rel="noopener ugc nofollow" target="_blank">皮克斯巴伊</a> (CC0)</p></figure><p id="d036" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">改善你的机器学习模型的一个有效方法是使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank">单词嵌入</a>。使用单词嵌入，您可以捕获文档中单词的上下文，然后找到语义和句法的相似性。</p><p id="203d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本帖中，我们将讨论单词嵌入技术的一个不寻常的应用。我们将努力为 OpenAPI 规范找到最好的单词嵌入技术。作为 OpenAPI 规范的一个例子，我们将使用来自<a class="ae ky" href="https://apis.guru/" rel="noopener ugc nofollow" target="_blank"> apis-guru </a>的 OpenAPI 规范的免费资源😎。</p><p id="dbd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最大的挑战是 OpenAPI 规范既不是自然语言，也不是代码。但这也意味着我们可以自由使用任何可用的嵌入模型。在这个实验中，我们将研究三个可能的候选者:code2vec、GloVe 和 spaCy。</p><p id="14a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://urialon.cswp.cs.technion.ac.il/wp-content/uploads/sites/83/2018/12/code2vec-popl19.pdf" rel="noopener ugc nofollow" target="_blank"> code2vec </a>是一个学习与源代码相关的类比的神经模型。该模型是在 Java 代码数据库上训练的，但是您可以将其应用于任何代码库。</p><p id="8b6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后是<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>。GloVe 是自然语言处理(NLP)的一种常用算法。它在维基百科和 Gigawords 上接受了训练。</p><p id="1e88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们有<a class="ae ky" href="https://spacy.io/usage/vectors-similarity" rel="noopener ugc nofollow" target="_blank">空间</a>。虽然 spaCy 是最近才开发的，但该算法已经以世界上最快的单词嵌入而闻名。</p><p id="c0c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看这些算法中哪一个更适合 OpenAPI 数据集，哪一个更适合 OpenAPI 规范👀。我把这篇文章分成六个部分，每一部分都包含代码示例和一些将来使用的技巧，还有一个结论。</p><ol class=""><li id="f040" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">下载数据集</li><li id="54a9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">下载词汇</li><li id="15ab" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">提取字段名称</li><li id="b7db" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">令牌化密钥</li><li id="8d57" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">创建字段名称的数据集</li><li id="6fc1" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">测试嵌入</li><li id="c8b4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">结论</li></ol><p id="75d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以开始了。</p><h1 id="ddf7" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak"> 1。下载数据集✅ </strong></h1><p id="7db1" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">首先，我们需要下载整个<a class="ae ky" href="https://apis.guru/" rel="noopener ugc nofollow" target="_blank">API-guru</a>数据库。</p><p id="79ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您会注意到大多数 API-guru 规范都是 Swagger 2.0 格式的。但是……open API 规范的最新版本是<a class="ae ky" href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.3.md" rel="noopener ugc nofollow" target="_blank"> OpenAPI 3.0 </a>。所以让我们通过使用 Unmock 脚本将整个数据集转换成这种格式！您可以按照<a class="ae ky" href="https://github.com/meeshkan/unmock-openapi-scripts/blob/master/README.md" rel="noopener ugc nofollow" target="_blank"> unmock-openapi-scripts 自述文件</a>中的说明完成此操作。</p><p id="c3ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可能需要一段时间(你不会变成🧓，但我们说的是几个小时⏰)最终，您将获得一个具有各种规格的大型数据集🎓。</p><h1 id="e66e" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak"> 2。下载词汇✅ </strong></h1><p id="9284" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated"><strong class="lb iu">代码 2vec </strong></p><ol class=""><li id="51ed" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">从<a class="ae ky" href="https://github.com/tech-srl/code2vec" rel="noopener ugc nofollow" target="_blank"> code2vec GitHub 页面</a>下载模型。按照 README.md 的快速入门部分中的说明进行操作，然后导出训练好的令牌。</li><li id="c8db" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">使用<a class="ae ky" href="https://pypi.org/project/gensim/" rel="noopener ugc nofollow" target="_blank"> gensim </a>库加载。</li></ol><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="c081" class="nl mk it nh b gy nm nn l no np">model = word2vec.load_word2vec_format(vectors_text_path, binary=False)</span><span id="aef9" class="nl mk it nh b gy nq nn l no np">model = word2vec.load_word2vec_format(vectors_text_path, binary=False)</span></pre><p id="817b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">手套</strong></p><ol class=""><li id="d321" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">从网站上下载一个<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>词汇。我们选择了最大的一个，因为它找到我们所有单词的几率更高。您可以选择想要下载它的位置，但是为了方便起见，最好将它存储在工作目录中。</li><li id="6a1c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">手动加载手套词汇。</li></ol><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="2ec7" class="nl mk it nh b gy nm nn l no np">embeddings_dict = {}<br/>with open("../glove/glove.6B.300d.txt", 'r', encoding="utf-8") as f:<br/>    for line in f:<br/>        values = line.split()<br/>        word = values[0]<br/>        vector = np.asarray(values[1:], "float32")<br/>        embeddings_dict[word] = vector</span></pre><p id="54bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">空间</strong></p><p id="744b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">加载大空间词汇:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="ed01" class="nl mk it nh b gy nm nn l no np">nlp = spacy.load(‘en_core_web_lg’).</span></pre><h1 id="5efa" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak"> 3。提取字段名称✅ </strong></h1><p id="a010" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">OpenAPI 规范名称的完整列表可以从<code class="fe nr ns nt nh b">scripts/fetch-list.sh</code>文件中获得，或者通过使用以下函数(对于 Windows)获得:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="eff1" class="nl mk it nh b gy nm nn l no np">def getListOfFiles(dirName):<br/>    listOfFile = os.listdir(dirName)<br/>    allFiles = list()<br/>    for entry in listOfFile:<br/>        fullPath = posixpath.join(dirName, entry)<br/>        if posixpath.isdir(fullPath):<br/>            allFiles = allFiles + getListOfFiles(fullPath)<br/>        else:<br/>            allFiles.append(fullPath)<br/>                <br/>    return allFiles</span></pre><p id="6f6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一件大事是从我们的 OpenAPI 规范中获取字段名。为此，我们将使用<a class="ae ky" href="https://pypi.org/project/openapi-typed-2/" rel="noopener ugc nofollow" target="_blank"> openapi 类型库</a>。让我们定义一个<code class="fe nr ns nt nh b">get_fields</code>函数，它采用 OpenAPI 规范并返回一个字段名列表:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="94d2" class="nl mk it nh b gy nm nn l no np">def get_fields_from_schema(o: Schema) -&gt; Sequence[str]:<br/>    return [<br/>        *(o['properties'].keys() if ('properties' in o) and (type(o['properties']) == type({})) else []),<br/>        *(sum([<br/>            get_fields_from_schema(schema) for schema in o['properties'].values() if not ('$ref' in schema) and type(schema) == type({})], []) if ('properties' in o) and ($        *(get_fields_from_schema(o['additionalProperties']) if ('additionalProperties' in o) and (type(o['additionalProperties']) == type({})) else []),<br/>        *(get_fields_from_schema(o['items']) if ('items' in o) and  (type(o['items'] == type({}))) else []),<br/>    ]<br/><br/>def get_fields_from_schemas(o: Mapping[str, Union[Schema, Reference]]) -&gt; Sequence[str]:<br/>    return sum([get_fields_from_schema(cast(Schema, maybe_schema)) for maybe_schema in o.values() if not ('$ref' in maybe_schema) and (type(maybe_schema) == type({}))], [])<br/><br/><br/>def get_fields_from_components(o: Components) -&gt; Sequence[str]:<br/>    return [<br/>        *(get_fields_from_schemas(o['schemas']) if 'schemas' in o else []),<br/>            ]                                                                                                                                                                       <br/><br/>def get_fields(o: OpenAPIObject) -&gt; Sequence[str]:<br/>    return [<br/>        *(get_fields_from_components(o['components']) if 'components' in o else []),<br/>    ]</span></pre><p id="ca93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">恭喜你。现在我们的数据集准备好了。</p><h1 id="cc33" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak"> 4。令牌化密钥✅ </strong></h1><p id="c895" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">字段名可能包含标点符号，如<code class="fe nr ns nt nh b">_</code>和<code class="fe nr ns nt nh b">-</code>符号，或骆驼大小写单词。我们可以把这些单词分割成称为记号的小块。</p><p id="7aeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的<code class="fe nr ns nt nh b">camel-case</code>函数识别这些 camel case 单词。首先，它检查是否有任何标点符号。如果是，那就不是骆驼案。然后，它检查单词中是否有大写字母(不包括第一个和最后一个字符)。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="4fe8" class="nl mk it nh b gy nm nn l no np">def camel_case(example):      <br/>    if  any(x in example for x  in string.punctuation)==True:<br/>        return False<br/>    else:<br/>        if any(list(map(str.isupper, example[1:-1])))==True:<br/>            return True<br/>        else:<br/>            return False</span></pre><p id="1ce5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一个函数<code class="fe nr ns nt nh b">camel_case_split</code>将 camel case 单词拆分成多个片段。为此，我们应该识别大写字母，并标记出大小写变化的地方。该函数返回拆分后的单词列表。例如，字段名<code class="fe nr ns nt nh b">BodyAsJson</code>转换成列表<code class="fe nr ns nt nh b">[‘Body’, ‘As’,'Json']</code>。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="a513" class="nl mk it nh b gy nm nn l no np">def camel_case_split(word):<br/>    idx = list(map(str.isupper, word))<br/>    case_change = [0]<br/>    for (i, (x, y)) in enumerate(zip(idx, idx[1:])):<br/>        if x and not y:  <br/>            case_change.append(i)<br/>        elif not x and y:  <br/>            case_change.append(i+1)<br/>    case_change.append(len(word))<br/>    return [word[x:y] for x, y in zip(case_change, case_change[1:]) if x &lt; y]</span></pre><p id="67ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个<code class="fe nr ns nt nh b">camel_case_split</code>函数然后被用在下面的记号化算法中。这里，我们首先检查单词中是否有标点符号。然后，我们把这个词分成几部分。有可能这些片段是骆驼大小写单词。如果是这种情况，我们可以把它分成更小的块。最后，在拆分每个元素后，整个列表被转换为小写。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="8fbb" class="nl mk it nh b gy nm nn l no np">def tokenizer(mylist):<br/>    tokenized_list=[]<br/>    for word in mylist:<br/><br/>        if '_'  in word:<br/>            splitted_word=word.split('_')<br/>            for elem in splitted_word:<br/>                if camel_case(elem):<br/>                    elem=camel_case_split(elem)<br/>                    for el1 in elem:<br/>                        tokenized_list.append(el1.lower())<br/>                else:    <br/>                    tokenized_list.append(elem.lower())<br/>        elif '-' in word:<br/>            hyp_word=word.split('-')<br/>            for i in hyp_word:<br/>                if camel_case(i):<br/>                    i=camel_case_split(i)<br/>                    for el2 in i:<br/>                        tokenized_list.append(el2.lower())<br/>                else: <br/>                    tokenized_list.append(i.lower())<br/>        elif camel_case(word):<br/>            word=camel_case_split(word)<br/>            for el in word:<br/>                tokenized_list.append(el.lower())<br/>        else:<br/>            tokenized_list.append(word.lower())<br/>    return(tokenized_list)<br/>tokenizer(my_word)</span></pre><h1 id="3fc3" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak"> 5。创建字段名为✅的数据集</strong></h1><p id="29a6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">现在，让我们用所有规范中的字段名创建一个大数据集。</p><p id="c8a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的<code class="fe nr ns nt nh b">dict_dataset</code>函数获取文件名和路径列表，并打开每个规范文件。对于每个文件，<code class="fe nr ns nt nh b">get_field</code>函数返回一个字段名列表。一些字段名称可能在一个规范中重复。为了消除这种重复，让我们使用<code class="fe nr ns nt nh b">list(dict.fromkeys(col))</code>将字段名列表从列表转换到字典，然后再转换回来。然后我们可以标记列表。最后，我们创建一个字典，以文件名作为键，以字段名列表作为值。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="3331" class="nl mk it nh b gy nm nn l no np">def dict_dataset(datasets):<br/>    dataset_dict={}<br/>    for i in datasets:<br/>        with open(i, 'r') as foo:<br/>            col=algo.get_fields(yaml.safe_load(foo.read()))<br/>            if col:<br/>                mylist = list(dict.fromkeys(col))<br/>                tokenized_list=tokenizer(mylist)<br/>                dataset_dict.update({i: tokenized_list})<br/>            else:<br/>                continue<br/>    return (dataset_dict)</span></pre><h1 id="4d61" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak"> 6。测试嵌入✅ </strong></h1><p id="4626" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated"><strong class="lb iu">代码 2vec 和手套</strong></p><p id="59f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以找出词汇表外的单词(not_identified_c2v)并统计这些单词占 code2vec 词汇表的百分比。以下代码也适用于 GloVe。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="1c5d" class="nl mk it nh b gy nm nn l no np">not_identified_c2v=[]<br/>count_not_indent=[]<br/>total_number=[]<br/><br/>for ds in test1:<br/>    count=0<br/>    for i in data[ds]:<br/>        if not i in model:<br/>            not_identified_c2v.append(i)<br/>            count+=1<br/>    count_not_indent.append(count)<br/>    total_number.append(len(data[ds]))<br/><br/>total_code2vec=sum(count_not_indent)/sum(total_number)*100</span></pre><p id="36fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">空间</strong></p><p id="e5f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">空间词汇是不同的，所以我们需要相应地修改我们的代码:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="01a9" class="nl mk it nh b gy nm nn l no np">not_identified_sp=[]<br/>count_not_indent=[]<br/>total_number=[]<br/><br/>for ds in test1:<br/>    count=0<br/>    for i in data[ds]:<br/>        f not i in nlp.vocab:<br/>                count+=1<br/>                not_identified_sp.append(i)<br/>    count_not_indent.append(count)<br/>    total_number.append(len(data[ds]))<br/><br/>        <br/>total_spacy=sum(count_not_indent)/sum(total_number)*100</span></pre><p id="6647" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于 code2vec、GloVe 和 spaCy，未识别单词的结果百分比分别为<code class="fe nr ns nt nh b">3.39, 2.33, 2.09</code>。由于每个算法的百分比相对较小且相似，我们可以做另一个测试。</p><p id="1297" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们用所有 API 规范中应该相似的单词创建一个测试字典:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="7aa6" class="nl mk it nh b gy nm nn l no np">test_dictionary={'host': 'server',<br/>'pragma': 'cache',<br/>'id': 'uuid',<br/>'user': 'client',<br/>'limit': 'control',<br/>'balance': 'amount',<br/>'published': 'date',<br/>'limit': 'dailylimit',<br/>'ratelimit': 'rate',<br/>'start': 'display',<br/>'data': 'categories'}</span></pre><p id="d037" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于 GloVe 和 code2vec，我们可以使用 gensim 库提供的<code class="fe nr ns nt nh b">similar_by_vector</code>方法。spaCy 还没有实现这个方法——但是我们可以自己找到最相似的单词。</p><p id="7f62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们需要格式化输入向量，以便在距离函数中使用。我们将在字典中创建每个键，并检查相应的值是否在 100 个最相似的单词中。首先，我们将格式化词汇表，以便在<code class="fe nr ns nt nh b">distance.cdist</code>函数中使用。这个函数计算词汇表中每对向量之间的距离。然后，我们将从最小距离到最大距离对列表进行排序，取前 100 个单词。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="21d3" class="nl mk it nh b gy nm nn l no np">from scipy.spatial import distance<br/><br/>for k, v in test_dictionary.items():<br/>    input_word = k<br/>    p = np.array([nlp.vocab[input_word].vector])</span><span id="901b" class="nl mk it nh b gy nq nn l no np">    closest_index = distance.cdist(p, vectors)[0].argsort()[::-1][-100:]<br/>    word_id = [ids[closest_ind] for closest_ind in closest_index]<br/>    output_word = [nlp.vocab[i].text for i in word_id]<br/>    #output_word<br/>    list1=[j.lower() for j in output_word]<br/>    mylist = list(dict.fromkeys(list1))[:50]<br/>    count=0<br/>    if test_dictionary[k] in mylist:<br/>        count+=1<br/>        print(k,count, 'yes')<br/>    else:<br/>        print(k, 'no')</span></pre><p id="d24f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果总结在下表中。spaCy 显示单词“client”在前 100 个与单词“user”最相似的单词中。它对于几乎所有的 OpenAPI 规范都是有用的，并且可以用于 OpenAPI 规范相似性的未来分析。单词“balance”的向量接近单词“amount”的向量。我们发现它对支付 API 特别有用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/0e0914c0a3c78cc433e66d05b991a3f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*FIQW-Tn0AUc8KzfeMOiTXA.png"/></div></figure><h1 id="9e98" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">结论</strong></h1><p id="d9c8" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们已经为 OpenAPI 规范尝试了三种不同的单词嵌入算法。尽管这三个词在这个数据集上都表现得很好，但是对最相似的词的额外比较表明 spaCy 更适合我们的情况。</p><p id="c771" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">spaCy 比其他算法更快。与 GloVe 或 code2vec 词汇表相比，spaCy 词汇表上传速度快五倍。然而，缺乏内置函数——比如<code class="fe nr ns nt nh b">similar_by_vector</code>和<code class="fe nr ns nt nh b">similar_word</code>——是使用这种算法的一个障碍。</p><p id="aa9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，spaCy 适用于我们的数据集这一事实并不意味着 spaCy 对世界上的每个数据集都更好。因此，请随意为您自己的数据集尝试不同的单词嵌入，并在评论中让我们知道哪一种更适合您！</p><p id="eceb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div></div>    
</body>
</html>