<html>
<head>
<title>Real time object detection [PyTorch]||[YOLO]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实时目标检测[PyTorch]||[YOLO]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/real-time-object-detection-pytorch-yolo-f7fec35afb64?source=collection_archive---------34-----------------------#2020-04-24">https://towardsdatascience.com/real-time-object-detection-pytorch-yolo-f7fec35afb64?source=collection_archive---------34-----------------------#2020-04-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7480" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">Python-PyTorch</h2><div class=""/><div class=""><h2 id="76c1" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用PyTorch在Python上实现的YOLO统一实时目标检测</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/bce708e530a2e4dbbf8a243a06d86ec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XZ002_X3ZQyJeUvo"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@agkdesign?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚历山大·奈特</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="9b3b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">计算机能理解它们看到的东西吗？他们能区分狗和猫、男人和女人或者汽车和自行车吗？让我们在这里找到它！</p><p id="a8e4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">目标检测和识别是当今计算机视觉研究的前沿领域之一。研究人员正在寻找新的方法让计算机理解他们看到的东西。新的最先进的模式正在制定，以大幅度击败他们以前的模式。但是，计算机实际上还远远没有“看到”它们所看到的东西。</p><p id="acb5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章中，我将详细介绍YOLO[你只看一次]——2016年的一项研究工作，创下了实时对象检测的新高。这篇文章是对YOLO模型的简要描述和实现，让你开始进入计算机视觉和物体检测领域。</p><h2 id="334e" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">YOLO:简介</h2><p id="bfb0" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">YOLO将对象检测重新构建为一个单一的回归问题，直接从图像像素到边界框坐标和类别概率。YOLO使用单个卷积层来同时预测多个边界框和这些框的类别概率。因此，YOLO的一个主要优点是我们可以用它来检测帧的速度。即使在这样的速度下，YOLO也设法达到了其他实时系统平均精度(mAP)的两倍多<em class="nb">！</em></p><blockquote class="nc nd ne"><p id="e487" class="li lj nb lk b ll lm kd ln lo lp kg lq nf ls lt lu ng lw lx ly nh ma mb mc md im bi translated">Mean Average precision是每个类的平均精度的平均值。换句话说，平均精度是所有类的平均精度。</p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/7b055669093e88d65365e2b246636537.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*08xjsV5-cABR95vtYKuF_A.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">平均精度方程</p></figure><h2 id="580c" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">侦查</h2><p id="fa3c" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">YOLO是一个统一的物体检测过程— <em class="nb">统一？</em> YOLO将涉及物体检测的几项任务统一到一个单一的神经网络中。网络在进行边界框预测时会考虑整个图像。因此，它能够在同一时间<em class="nb">预测所有类的所有边界框。</em></p><p id="594d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，图像被分成一个<em class="nb"> S </em> x <em class="nb"> S </em>网格。如果网格的中心落在网格单元中，则网格单元应该能够检测到对象。那么，我们所说的检测是什么意思呢？准确地说，网格单元预测<em class="nb"> B </em>边界框，每个框的置信度得分告诉我们模型对该框包含一个对象有多少置信度，以及该框在覆盖该对象方面有多准确。因此，这个框给了我们5个预测— <em class="nb"> x，y，w，h </em>和<em class="nb">置信度得分</em>。网格单元因此给了我们<em class="nb"> B </em>这样的预测。</p><p id="cf5f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了边界框上的预测，网格单元还给出了我们<em class="nb"> C个条件类概率</em>。假定网格单元包含一个对象，这些基本上给出了类的概率。换句话说，假设一个对象存在，条件类概率给我们这个对象可能属于哪个类的概率。每个格网像元仅预测一组类别概率-无论存在多少个边界框</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nj"><img src="../Images/772fff3c2879d1cc74b4f5e5fea05948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jew7ma91GVlw2ympf5AGQw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">模型功能概述—【https://arxiv.org/abs/1506.02640 T2】</p></figure><h2 id="d8f8" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">网络</h2><p id="5430" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">该模型受GoogleNet图像分类模型的启发。它有24个卷积层，后面是2个全连接层。与GoogLeNet使用的inception模块不同，它使用1 × 1缩减层，然后是3 × 3卷积层。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nk"><img src="../Images/d29a35e34fbb6a62d11d545161188cd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iG0-MRwbe7dg1sBbhhkt8A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">模型架构概述—【https://arxiv.org/abs/1506.02640 T4】</p></figure><p id="3ba7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">交替的1 × 1卷积层减少了来自前面层的特征空间。整个网络以一半的分辨率针对ImageNet分类问题进行预训练，然后分辨率加倍用于检测。</p><h2 id="0ddb" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">丢失和激活</h2><p id="ad1f" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">最后一层使用线性激活函数，而所有其他层使用泄漏-ReLU激活函数。Leaky-ReLU激活可以表示为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/d01b5b1b9024ef7155e0ff42eb506627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*-3mPPTCPzyBmEUmLfan95A.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">漏流激活</p></figure><p id="0432" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">损失函数是简单的误差平方和，写为—</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/31d201238a06f8a04a372cd2bc11652a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*QG4cHseKDSjM2VlOY5e_JA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">平方和误差</p></figure><p id="8400" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，如果我们像这样定义损失函数，它将把分类和定位误差结合在一起，并且从模型的观点来看，将同等重视这两者。</p><p id="9e4e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了防止这种情况发生，增加边界框坐标预测的损失，并减少不包含对象的框的置信度预测的损失。两个参数λcoord和λnoobj用于实现这一点——λcoord设置为5，λnoobj设置为0.5(参见损失函数方程中关于这些超参数的更多信息)</p><p id="bc6a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">SSE还带来了大小包围盒权重误差相等的问题。为了形象化这个问题，请看下面的图片—</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/1c428206675bb7013a562aa0c4753a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*-Y97Ug1erjBzLuddgsmnKg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">狗和碗的图片(仅供参考)——谷歌图片</p></figure><p id="90b7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里我们可以看到，狗和碗已经被注释为图像中的对象。狗的边框尺寸较大，碗的尺寸较小。现在，如果狗和碗的边界框减少相同的正方形像素，它会觉得碗与框相比具有更差的注释。这是为什么呢？因为在位移和尺寸方面的边界框精度应该与边界框尺寸而不是图像尺寸相比较。</p><p id="4ac1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，来看我们的损失函数，我们可以看到，损失函数并没有特别注意边界框的大小。它没有考虑到这一点。为了克服这个障碍，网络不预测边界框的高度和宽度，而是预测其平方根。这有助于将差异保持在最小。</p><p id="5a6e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">考虑到所有这些因素，多部分损失函数可以写成—</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/b29fbe6380a52aeda61515e22ebeacef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*juSHUliinEXmcBGJX_jhig.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">网络的总损失函数—<a class="ae lh" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1506.02640</a></p></figure><p id="b89f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">既然我们已经完成了YOLO物体检测模型的基础，让我们开始研究代码吧！</p><h2 id="bad0" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">建造它！</h2><p id="27cf" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">YOLO网络简单易建。问题在于边界框。绘制边界框和保存图像、编写置信度得分和标签以及配置整个训练代码会使本文变得不必要的长。因此，我将直接实现这个模型。自己承担责任，在这个周末作为一个项目完成代码，并在评论区告诉我结果！<em class="nb">我将上传这篇文章的续集，以你的代码和你的结果为特色，并给予满分；)</em></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="np nq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">YOLO网络模块</p></figure><p id="02e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是一个关于yolov 3(YOLO的最新即兴版本)结果的短片—</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nr nq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">YOLOv3</p></figure><p id="d96d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">很兴奋吧。嗯，你应该是因为计算机视觉突飞猛进！现在是开始的时候了。如果你被YOLO的实现卡住了，请在评论区告诉我。来帮忙了:)</p><p id="de12" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">查看我的博客以获得更快的更新，并订阅优质内容:D</p><div class="ns nt gp gr nu nv"><a href="https://www.theconvolvedblog.vision" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd jd gy z fp oa fr fs ob fu fw jc bi translated">卷积博客</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">克罗伊斯，吕底亚(小亚细亚)的国王，曾经问特尔斐的神谕，他是否应该对波斯开战…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">www.theconvolvedblog.vision</p></div></div><div class="oe l"><div class="of l og oh oi oe oj lb nv"/></div></div></a></div><p id="576d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nb"> Hmrishav Bandyopadhyay是印度Jadavpur大学电子与电信系的二年级学生。他的兴趣在于深度学习、计算机视觉和图像处理。可以在——hmrishavbandyopadhyay@gmail.com找到他。</em></p></div></div>    
</body>
</html>