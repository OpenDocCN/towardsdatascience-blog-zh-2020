<html>
<head>
<title>First Order Motion Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一阶运动模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/first-order-motion-model-ab3908407208?source=collection_archive---------10-----------------------#2020-02-18">https://towardsdatascience.com/first-order-motion-model-ab3908407208?source=collection_archive---------10-----------------------#2020-02-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b964" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个有效的框架。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/998ff198322629cd89123868f0cd7dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*2VlAXsgGP9P7eoDHTV3bxg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一阶运动模型的作用</p></figure><p id="fac1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> S </span> <strong class="la iu"> eeing </strong>曾为<strong class="la iu">T5】belieng</strong>。感谢艾，我们终于要告别这种可爱幼稚，却又危险的信仰了。因为事实上从来都不是。在20世纪，照片被专制政权修饰过。通过深度学习，我们体验到了重新阐释现实的新方法。这不是一种危险；这是一个机会。</p><p id="9995" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在各种方法中，由<em class="md"> Aliaksandr Siarohin等人</em>提出的<strong class="la iu"/><a class="ae me" href="https://aliaksandrsiarohin.github.io/first-order-model-website/" rel="noopener ugc nofollow" target="_blank"><strong class="la iu">用于图像动画的一阶运动模型</strong></a><strong class="la iu">】</strong>的框架和论文以其绝妙的构思吸引了人们的注意:</p><blockquote class="mf mg mh"><p id="603c" class="ky kz md la b lb lc ju ld le lf jx lg mi li lj lk mj lm ln lo mk lq lr ls lt im bi translated">图像动画包括生成视频序列，以便源图像中的对象根据驱动视频的运动进行动画制作。我们的框架解决了这个问题<strong class="la iu">，而没有使用任何注释或关于特定对象的先验信息来制作动画</strong>。一旦<strong class="la iu">在一组描述同一类别</strong>(例如人脸、人体等<em class="it">)的对象的视频上进行训练，我们的方法就可以<strong class="la iu">应用于该类别的任何对象</strong>。(<a class="ae me" href="https://aliaksandrsiarohin.github.io/first-order-model-website/" rel="noopener ugc nofollow" target="_blank">来源</a>，我强调)</em></p></blockquote><p id="72e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关键点与变换一起设置(类似于Photoshop中的木偶工具，或运动捕捉套装上的传感器)，因此训练的运动可以转移到目标图像。</p><p id="56cb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要求是相同的对象类别。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ml"><img src="../Images/fb5dc98a5b42e0c52bda8ee309040860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VhySxPqjToB71AonkPqWkg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae me" href="https://aliaksandrsiarohin.github.io/first-order-model-website/" rel="noopener ugc nofollow" target="_blank">https://aliaksandrsiarohin . github . io/first-order-model-website/</a>1)</p></figure><p id="593c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">简而言之</strong>，无监督学习方法分析源素材中的运动数据，将其普遍化，并应用于目标素材。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mm"><img src="../Images/b0833f7a4b94f679e6b55c22d15169b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*uSQjcKtkMp5YcarjB8d25A.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自纸张的运动样本(<a class="ae me" href="https://aliaksandrsiarohin.github.io/first-order-model-website/" rel="noopener ugc nofollow" target="_blank">来源</a> ) <strong class="bd mn"> 1) </strong></p></figure><h2 id="dce4" class="mo mp it bd mq mr ms dn mt mu mv dp mw lh mx my mz ll na nb nc lp nd ne nf ng bi translated">换脸:Deepfakes？</h2><p id="7b7e" class="pw-post-body-paragraph ky kz it la b lb nh ju ld le ni jx lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">它还允许以与<a class="ae me" rel="noopener" target="_blank" href="/14-deep-learning-uses-that-blasted-me-away-2019-206a5271d98?source=friends_link&amp;sk=4ac313764b2ca90e765566714dd2c88e"> face2face-approach </a>完全不同的方式交换脸。当<a class="ae me" rel="noopener" target="_blank" href="/face2face-a-pix2pix-demo-that-mimics-the-facial-expression-of-the-german-chancellor-b6771d65bf66"> face2face </a>接合<a class="ae me" href="https://github.com/davisking/dlib/blob/master/examples/face_detection_ex.cpp" rel="noopener ugc nofollow" target="_blank">人脸检测器</a>并将面部特征应用于目标图像时，框架“<strong class="la iu">一阶运动模型</strong>进行另一种方式:</p><blockquote class="mf mg mh"><p id="1099" class="ky kz md la b lb lc ju ld le lf jx lg mi li lj lk mj lm ln lo mk lq lr ls lt im bi translated">运动被描述为一组关键点位移和局部仿射变换。生成器网络<strong class="la iu">将源图像的外观和驾驶视频的运动表示</strong>相结合。此外，我们提出明确地<strong class="la iu">模型遮挡</strong>，以便向生成器网络指示哪些图像部分应该被修补(<a class="ae me" href="http://papers.nips.cc/paper/8935-first-order-motion-model-for-image-animation.pdf" rel="noopener ugc nofollow" target="_blank">源</a>)。</p></blockquote><p id="406e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">而且效果惊人的好。</p><p id="d85a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以使用<a class="ae me" href="https://github.com/AliaksandrSiarohin/first-order-model" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> GitHub库</strong> </a>或者<a class="ae me" href="https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb#scrollTo=UCMFMJV7K-ag" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> Colab笔记本</strong> </a>来尝试一下。</p><p id="2f4c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我用AI先锋<strong class="la iu"> Geoffrey Hinton </strong> 的<a class="ae me" href="https://www.youtube.com/watch?v=XG-dwZMc7Ng" rel="noopener ugc nofollow" target="_blank">镜头在<strong class="la iu">娜芙蒂蒂</strong>上试了试运气。这段录像是用笔记本送来的。可以用另一个视频素材。它必须符合特定的要求和尺寸。</a></p><p id="b4fe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结果非常令人信服:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/e658ae3bd5e24cc214c20159f1d6eeaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*0LgOnnv9ySzYuCdM2lluMw.gif"/></div></figure><p id="b90d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相反，应用我自己的照片会带来一些小问题，尤其是关于眼镜。我想，这些模式在镜头中是缺失的，所以关键点的分配有时会失败:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0df0d30bfcfc017f2df35160f0d78702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*voggtpwDmJah6luwbIYdmA.gif"/></div></figure><p id="2379" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用<strong class="la iu"> Arcimboldo </strong>的一幅画，我们可以看到运动分配是如何工作的——一些图形特征仍然被检测为背景，并且没有动画。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/d00c6a37f50f509b83cd1fc2a9bcc879.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*IFJfxerxQHD8pD-5e2JBEQ.gif"/></div></figure><p id="1599" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最有趣的效果发生在我们使用没有外貌特征的图像时——甚至是抽象艺术。我用的是波洛克的作品:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0c9c82939ede8bee00993e1f583fda49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*2qrSVg6ubjflZnlLTMXZhQ.gif"/></div></figure><p id="53ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我们可以观察到模型是如何试图在混乱的结构中确定自己的方向——并让它到达某一点。</p><p id="b0fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">论文展示了其他面部交换实验:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/776d618fb1fb86544a2fcb542936943e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*fPUNCO2ETgDdg4WFnINhnQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae me" href="https://aliaksandrsiarohin.github.io/first-order-model-website/" rel="noopener ugc nofollow" target="_blank">https://aliaksandrsiarohin . github . io/first-order-model-website/</a>、<strong class="bd mn"> 1) </strong></p></figure><p id="e6ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">或许如果我们将这种方法与<strong class="la iu">风格转移</strong>——甚至与<a class="ae me" href="https://medium.com/merzazine/ai-creativity-alien-elements-with-style-27ee7e45df92?source=friends_link&amp;sk=4f8bf8dd08b1e76b3fcaa88676add697" rel="noopener"> <strong class="la iu">深度画风调和</strong> </a>相结合，我们会获得更加连贯的效果。</p><h2 id="a689" class="mo mp it bd mq mr ms dn mt mu mv dp mw lh mx my mz ll na nb nc lp nd ne nf ng bi translated">DeepFakes时代:一个机会？</h2><p id="038d" class="pw-post-body-paragraph ky kz it la b lb nh ju ld le ni jx lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">如果你看到这样的模型，你首先想到的肯定是:<strong class="la iu"> <em class="md"> DeepFake </em> </strong>。我们已经太偏向了(并且一直偏向我们的神经网络)。</p><p id="0652" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还能相信视觉吗？数字时代我们能区分<em class="md">真</em>和<em class="md">假</em>吗？<strong class="la iu">答案是:“<em class="md">我们再也不能了”</em>。</strong>但是:答案仍然没有在那一点上结束。</p><p id="3f67" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种天真地相信图像背后的真相——如此诱人，因为如此简单——是一种危险的欺骗。一幅图像背后有无数的真相。即使是没有任何后期制作痕迹的raw图像也不是真相，而是特别选择的视角。未知本体一瞥。Pars pro toto我们应该注意它在语义上的脆弱性。</p><p id="c00b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们必须重新思考我们与真理概念的关系。因为像人这样的道理太多了。当然，欺骗某人很容易，伪造一个图像或视频片段，并将其作为真实的东西呈现出来。人们已经这样做了——从友好的恶作剧到总统使用。</p><p id="70d9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是信不信由我们自己。我们需要比以前更多地研究和分析。当然，这对我们的大脑和感知来说是更多的工作，但这就是现实。理解世界并不像我们过去几个世纪所认为的那样容易。</p><p id="3ddf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">相信影像是一种舒服的自欺。我感谢所有人工智能打破这一信念的努力。</strong></p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="98bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">来源:</strong></p><ol class=""><li id="c729" class="nv nw it la b lb lc le lf lh nx ll ny lp nz lt oa ob oc od bi translated">框架方案和GIF演示，标有<strong class="la iu"><em class="md">1)</em></strong>:<br/><a class="ae me" href="https://aliaksandrsiarohin.github.io/first-order-model-website/" rel="noopener ugc nofollow" target="_blank">网站</a> // <a class="ae me" href="http://papers.nips.cc/paper/8935-first-order-motion-model-for-image-animation" rel="noopener ugc nofollow" target="_blank">论文</a></li></ol><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="fb5b" class="mo mp it of b gy oj ok l ol om">@InProceedings{Siarohin_2019_NeurIPS,<br/>  author={Siarohin, Aliaksandr and Lathuilière, Stéphane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},<br/>  title={First Order Motion Model for Image Animation},<br/>  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},<br/>  month = {December},<br/>  year = {2019}<br/>}</span></pre><p id="e6c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 2。</strong>视频片段。采访杰弗里·辛顿:<a class="ae me" href="https://www.youtube.com/watch?v=XG-dwZMc7Ng" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=XG-dwZMc7Ng</a></p></div></div>    
</body>
</html>