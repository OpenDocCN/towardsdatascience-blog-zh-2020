<html>
<head>
<title>Examining the Performance of RCNs on Popular Datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在流行数据集上检验RCNs的性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/examining-the-performance-of-rcns-on-popular-datasets-1d6a2a8852c1?source=collection_archive---------36-----------------------#2020-02-02">https://towardsdatascience.com/examining-the-performance-of-rcns-on-popular-datasets-1d6a2a8852c1?source=collection_archive---------36-----------------------#2020-02-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="e8ff" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">皇家护理学院</h2><div class=""/><div class=""><h2 id="06ea" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">大脑启发的计算机视觉模型有多好？</h2></div><p id="b8c8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">递归皮层网络基于与我们在深度学习中习惯的想法有很大不同的想法。那么，那些受神经科学启发的想法有多好呢？在这篇文章中，我们检查了rcn在不同任务上的表现，并对作者引入的想法进行了消融研究。除非另有说明，本文全部基于RCNs的论文[1]及其补充文件。</p><p id="4413" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="lk">注:本文所有图表均取自【1】。</em>T3】</strong></p><p id="c38d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">RCNs的第一个测试是基于文本的验证码，它可以灵活地处理。500个reCAPTCHA图像用于设置一些超参数的值，包括RCN将要被训练的字体。是的，RCN实际上不是直接在reCAPTCHA图像上训练的，而是在对其进行了10次变换的单个字母上训练的，如图1所示，产生了总共260个图像的训练数据集。请记住，中间层仍在SHREC 3D数据集上进行训练，不包括在这些数字中。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/41efcfefc1134aec9f320503386438ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmVwZsNdmjWsckkMYQBICg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">图1:为验证码图片构建的RCN的训练数据集。</p></figure><p id="007d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这产生了一个模型，该模型以84.2%的成功率为reCAPTCHA提供答案，而人类工人达到了87.4%的准确率。然而，需要注意的是，如果答案产生的字符串长度与实际长度相似，并且有0或1个错误字符，则接受答案。这个成功率在逻辑上打破了基于文本的验证码，例如，你不希望一个机器人以这个成功率在你的网站上创建假账户。破解基于文本的验证码是否是一个壮举是另一个讨论，但这里明确的壮举是数据效率和推广到看不见的条件的能力，这将在剩余的实验中变得更加清楚。</p><p id="a487" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了了解它与ConvNet的对比，作者建立了一个reCAPTCHA控制集，并基于谷歌街景和reCAPTCHA团队2013年的一篇论文建立了一个conv net模型[2]。该论文声称，在困难的例子上达到99.8%的准确率，但训练集“在数百万的数量级”。RCN的作者在79，000张图像上训练了ConvNet，并添加了残余连接，因为论文是旧的。前一段的ConvNet和RCN得分相当。然而，在增加测试集中字符之间的水平间距时(该间距不在训练集中，但应该使字母更清晰、更容易识别)，ConvNet的性能急剧下降，而RCN的性能则有所增加，如表1所示。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mb"><img src="../Images/3fd555c8c521918708f820147b728329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5FwKCExJYRf3XuRvFkUa3Q.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">表1:reCAPTCHA控制集上RCN和ConvNet的性能比较。“间距”是水平间距。</p></figure><p id="37f4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了展示rcn的高数据效率，他们在一次性学习Omniglot挑战赛上进行了测试。数据集由20个符号组成，每个符号有一个训练图像和一个测试图像。从Omniglot挑战的3年进度报告中，您可以看到他们的成果仍然令人印象深刻，如摘自该报告的表2所示[3]。在您的评估中，您应该考虑到rcn与ConvNets不同，几十年来没有经过不同研究人员的改进。需要注意的是，BPL并不是像RCNs那样的通用架构，这也是它表现如此出色的原因。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/0361a83f7abccd468a9f841bfca17395.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*xN7U8kRh7MQ29CU352slLA.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">表2:不同型号在单镜头Omniglot挑战赛上的表现。</p></figure><p id="585f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">RCN论文中的另一个重要实验是对MNIST噪声变体的测试。实验如下进行。他们在MNIST数据集的干净图像上训练了3个模型:1000幅图像上的RCN，1000幅图像上的CNN，以及60000幅图像上的CNN。然后，他们在MNIST测试图像上测试了每个模型的性能，但添加了6种不同强度的不同类型的噪声，如表3所示。RCN在这项测试中的优势从数字中显而易见，因为它始终胜过ConvNets。但是，我仍然希望您记住，rcn是在SHREC 3D数据集的10K图像上预先训练的。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi md"><img src="../Images/bd2ecf7396b95a202c170b62e3eb8a31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uiQvtdRZoRPORNXlP0ZPxg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">表3:一个RCN和两个ConvNets在噪声MNIST上的性能。报告的数字是测试精度，括号中的数字是标准偏差。</p></figure><p id="d486" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，作者进行了一项消融研究，以观察横向连接和反向传递是否有用。在这项研究中，他们测试了具有不同配置的RCN在噪声<em class="lk">小于</em> MNIST方面的性能，如表4所示。这个测试表明，对于高数据效率，RCNs中的所有思想都非常有用。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi me"><img src="../Images/6b2366955bd37680cfc40c496e076aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*km14iVKNCJCw-WjG9ZWBLg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">表4:消融研究结果。报告的数字是分类精度。FP指的是向前传递，BP指的是向后传递，Lats指的是横向连接。扰动因子是横向连接的超参数。</p></figure><p id="9f2a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">作者做了更多的实验，包括在ICDAR挑战上测试rcn，在不同的CAPTCHA数据集上测试，在从有噪声的MNIST图像重建上测试，以及在3D对象的分类上测试。然而，它们不会在本文中讨论，但是我可能会将它们包含在后续文章中，该后续文章将对rcn进行更具批判性的分析，以及我们可以从中学到什么。</p><p id="e593" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">您可能会有一个迫切的问题，如果rcn是数据有效的，为什么不在现实生活中应用。这有多种原因，但即使是作者也承认的两个主要原因是，它不适用于大规模数据集，并且它假设背景是单一的东西，不包含信息。用作者自己的话说，这是两个原因:</p><blockquote class="mf mg mh"><p id="f3db" class="ko kp lk kq b kr ks ka kt ku kv kd kw mi ky kz la mj lc ld le mk lg lh li lj ij bi translated">“…因为RCN将每个训练样本的原型存储在其倒数第二层(用于分类)，所以扩展到非常大的数据集，如ImageNet和其他对象识别基准，其中CNN(如AlexNet)是最先进的，可能会很困难…”</p><p id="129d" class="ko kp lk kq b kr ks ka kt ku kv kd kw mi ky kz la mj lc ld le mk lg lh li lj ij bi translated">在真实图像中，背景上下文提供了关于场景中可能存在的对象的无价信息。这个信息目前被RCN忽略，RCN只对前景对象建模，忽略上下文。另一个极端是，CNN不加区分地汇总前景和背景信息(这样就有可能通过显示脱离上下文的对象来欺骗CNN)”</p></blockquote><p id="0c68" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">那为什么要写各种各样的文章呢？因为这两个缺点并不存在于所有的数据集中(想想资源不足的语言的OCR ),因为想法在开始时有很多限制是正常的，就像胶囊网络一样。</p><p id="2385" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">本文到此为止。如果你想了解更多关于RCNs的知识，你可以查看它的论文[1]和补充材料文档，或者你可以阅读我的关于RCNs结构的文章。</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h1 id="5709" class="mt mu iq bd mv mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">参考资料:</h1><p id="5bde" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi translated">[1] <a class="ae ml" href="https://science.sciencemag.org/content/358/6368/eaag2612.full?ijkey=DmvGldXIEXVoQ&amp;keytype=ref&amp;siteid=sci" rel="noopener ugc nofollow" target="_blank"> D. George，W. Lehrach，K. Kansky等，一种以高数据效率进行训练并打破基于文本的验证码的生成视觉模型(2017)，科学杂志(第358卷—第6368期)。</a></p><p id="4570" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[2] <a class="ae ml" href="https://arxiv.org/abs/1312.6082" rel="noopener ugc nofollow" target="_blank"> I. Goodfellow，Y. Bulatov，J. Ibraz等，利用深度卷积神经网络从街景图像中识别多位数号码(2013)，Arxiv。</a></p><p id="df0d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[3] <a class="ae ml" href="https://arxiv.org/pdf/1902.03477.pdf" rel="noopener ugc nofollow" target="_blank"> B. Lake，R. Salakhutdinov，&amp; J. Tenenbaum，《Omniglot挑战:三年进度报告》(2019年)，Arxiv。</a></p></div></div>    
</body>
</html>