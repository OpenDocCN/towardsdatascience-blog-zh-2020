<html>
<head>
<title>How are decision trees built?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树是如何构建的？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-are-decision-trees-built-a8e5af57ce8?source=collection_archive---------34-----------------------#2020-07-10">https://towardsdatascience.com/how-are-decision-trees-built-a8e5af57ce8?source=collection_archive---------34-----------------------#2020-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9a20" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从头开始构建决策树的入门指南</h2></div><p id="edf8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">理解用于构建决策树的基本原则不是很棒吗？在这篇文章中，我将演示如何构建一个决策树，特别是一个分类树，使用两个不同的标准:<em class="le">基尼系数</em>和<em class="le">熵</em>，并辅以一步一步的解释。我希望在这篇文章结束时，你能更好地理解决策树是如何构建的！🎓</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/a0c5841a6402fd4cadfac520a352241a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5UGnZAGpI8Gd3wjk68cbQg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">决策树建立在过去的经验上，用来评估是否要看某部电影</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="e028" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">1.数据📦</h1><p id="1d80" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">为了让事情易于管理，并希望有点乐趣，我们将创建一个微小的虚拟数据，灵感来自情景喜剧<a class="ae mz" href="https://www.imdb.com/title/tt0108778/" rel="noopener ugc nofollow" target="_blank">的 6 个主要角色:</a></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi na"><img src="../Images/8308fe5858879acc45cc4ff3296c0b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ED6HAl-7TAGJoYFcFc5SfQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">注意:数据中的值已经过调整，以适合示例</p></figure><p id="cc34" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们假设这个数据对本文的目的来说是正确的。我们将构建一个决策树，使用其余的列来分类一个字符是否是父字符。换句话说，我们将构建一个具有以下输入和输出的分类树:<br/> ◼ ️ <strong class="kk iu">输入|特征:</strong> <em class="le"> was_on_a_break，is_married，has _ pet</em>t11】◼️<strong class="kk iu">输出|目标:</strong> <em class="le"> is_parent </em></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="fa8d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">2.构建分类树🔨</h1><p id="7a45" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">如果你喜欢数学，我鼓励你和这个指南一起手动计算，以充分利用这个博客。在本节中，为了简洁起见，作为父母的角色被缩写为<strong class="kk iu"> pa </strong>，而非父母的角色被缩写为<strong class="kk iu"> np </strong>。</p><h2 id="cf2c" class="nb md it bd me nc nd dn mi ne nf dp mm kr ng nh mo kv ni nj mq kz nk nl ms nm bi translated">2.1.使用基尼系数<strong class="ak">(又名基尼指数或基尼系数)</strong> ☝️</h2><p id="cc05" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">决策树是通过自上而下递归分割成二进制节点来构建的。我们可以通过以下步骤找到节点的最佳分裂:<br/> <strong class="kk iu">步骤 1: </strong>计算要从<br/> <strong class="kk iu">分裂的节点的 gini 杂质(此处为向上的 gini)步骤 2: </strong>找到所有可能的分裂<br/> <strong class="kk iu">步骤 3: </strong>计算每个分裂的两个节点的 Gini<br/><strong class="kk iu">步骤 4: </strong>计算每个分裂的加权平均 Gini<br/><strong class="kk iu">步骤 5: </strong>确定最佳分裂</p><p id="3eb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">包括来自训练数据的每个人的最顶层节点被称为<em class="le">根节点</em>。让我们用这些步骤来确定根节点的最佳分割。</p><p id="12e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🚪<strong class="kk iu">第一步:计算根节点的基尼系数</strong> <br/> <strong class="kk iu"> ➗公式:</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/ef3795a7885e8aea7add95521a97fe71.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*FCfa7EL4nCaIs3psOM7SlQ.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">这个公式的两个变体将给出完全相同的结果</p></figure><p id="23fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以将这个通用公式简化为以下公式，并计算基尼系数:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ad410e39bbbb150377bec48deb9b6670.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*I73zmXJvfYgHeun6TnV3Eg.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">这个简化的公式将从这里开始使用</p></figure><p id="bea9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们知道在 6 个角色中有 2 个非父母和 4 个父母。使用该信息，我们发现根节点处的基尼系数为 0.444。因此，根节点可以总结如下:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/53c4686e0a26446842e992274051b468.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*h3Ti0tgGNnMAQN8WbRToww.png"/></div></figure><p id="5899" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🚪<strong class="kk iu">步骤 2: </strong>从根节点找到所有可能的分裂<br/>使用三个特征中的任何一个，有三种方法来分裂。例如，我们可以把 6 个角色分成 2 组:一组给有宠物的人，另一组给没有宠物的人。其他两个特性也是如此。</p><p id="2da0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🚪<strong class="kk iu">步骤 3: </strong>计算每次拆分的两个节点的基尼系数<br/>让我们计算三次拆分的每个节点的基尼系数。</p><p id="8d8b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">📌练习:</strong>在继续之前，看看您能否计算出所有 6 个节点的基尼系数。</p><p id="aaf7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">🔑答案:</strong>使用与步骤 1 相同的逻辑，我们发现每个节点的基尼系数如下:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nq"><img src="../Images/3de9f9dced01e4702d2550883ac66a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jw7MQC1oFy1mvfR8cXawqw.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">希望这些微小的数字清晰可辨👀(如果太小，尝试放大)</p></figure><p id="c80f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🚪<strong class="kk iu">第四步:</strong>计算每次拆分的加权平均基尼系数<br/>现在，我们需要找到每次拆分的加权平均基尼系数，记为<em class="le"> w_gini，</em>。以<em class="le"> was_on_a_break </em>为例，我们计算 w_gini 如下<em class="le"> : </em></p><p id="a8ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> ➗公式:</strong>左:左边的节点，右:右边的节点</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/b6023c6dc5a49313eaa55915fed778ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*0F3WR_VUbj_7k25c6nCzwA.png"/></div></figure><p id="919f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">📌练习:</strong>看看能不能算出另外两个的加权基尼。</p><p id="6811" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">🔑答案:</strong>你的答案符合这些吗？</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/66cd40cfda177d915d6b1c3cf72eceed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*T-7AQ78igQkLqHGLhQlu7A.png"/></div></figure><p id="6f1a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🚪<strong class="kk iu">第 5 步:</strong>通过检查上一步的结果，确定最佳分割(最低加权平均基尼系数)<br/>，使用<em class="le">is _ marted</em>进行分割得到最低加权平均基尼系数。如果我们也看看步骤 3 中的表格，我们可以直观地理解这个决定。当<em class="le">上的分裂与</em>结合时，树能够将一半的数据分裂成左边的纯节点。这个节点是纯的，因为它只包含父节点。对于最纯的节点，Gini 是 0，对于最不纯的节点，Gini 是 0.5(例如，对于其他分裂的右节点)。</p><p id="0030" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">🚪<strong class="kk iu">第六步:</strong>计算信息增益:如果为正，👉我们了解到，如果我们要分手，最好用结婚。现在让我们看看我们是否能从分裂中获得任何信息。信息增益被定义为顶部节点的基尼系数与底部节点的加权平均基尼系数之差。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/df147a5fc319b071e49b5eb98dfde533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*3OO2zEPQVgciJGoXXj-dMg.png"/></div></figure><p id="4b20" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">信息增益为正。换句话说，我们通过分裂获得信息。因此，正确的决定是从根节点使用<em class="le">is _ marted</em>进行拆分。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nu"><img src="../Images/4f987aa53a58853c7e6ff60ead5133a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WVc2DMrlQifL_EK8e_8FiQ.png"/></div></div></figure><p id="f3d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们刚刚学习了如何确定分裂。⭐️:让我们重复同样的步骤，建立树的其余部分。是时候评估芥末节点的分配了！</p><p id="d659" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">也许我们可以从更简单的开始:左边的节点包含已婚节点。我们看到所有已婚角色都是父母，因此基尼=0。如果我们从这个节点分裂，即使从分裂中加权基尼为 0，我们也不会获得任何信息。所以，正确的决定是不拆分。在这种情况下，这个节点被认为是一个<em class="le">终端节点，</em>不再进一步分裂。另一方面，对于右边另一个节点中的非婚角色，我们有一些工作要做。记住这些知识的最好方法是自己练习，为什么不试试通过下面的练习来应用我们刚刚学到的知识呢:</p><p id="e953" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">📌练习:</strong>完成所有步骤，找到正确节点的正确分割</p><p id="750c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">🔑答案:<br/>步骤 1: </strong>我们已经知道了之前拆分的答案:0.444 <br/> <strong class="kk iu">步骤 2: </strong>我们可以使用<em class="le"> was_on_a_break </em>或<em class="le"> has_pet </em> <br/> <strong class="kk iu">步骤 3 &amp;步骤 4: </strong>见下图<br/> <strong class="kk iu">步骤 5: </strong>最好的拆分是使用<em class="le"> was_on_a_break </em></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/16189268a43f01e9c22f8a5266a40402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*4HmtEbTVIzm4_mzQ3dF6Mw.png"/></div></figure><p id="9b24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结合输出，最终的决策树如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nw"><img src="../Images/192759823ed8aa3ff0cb4920ce44ff67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v6UVJfp1vv0X0eqBbo-LNQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated"><strong class="bd nx">根节点:</strong>一个<strong class="bd nx"> </strong>起始节点，包括所有人<br/> <strong class="bd nx">内部节点:</strong>根节点和终端节点之间的所有其他节点。<br/> <strong class="bd nx">终端节点:</strong>到达决策的节点</p></figure><p id="a968" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Yay❕:我们已经建立了一个简单的决策树。</p><h2 id="e1e2" class="nb md it bd me nc nd dn mi ne nf dp mm kr ng nh mo kv ni nj mq kz nk nl ms nm bi translated">2.2.使用熵✌️</h2><p id="cfba" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">让我们了解一下，如果用熵代替基尼，会有什么变化。步骤保持不变，除了我们每次都计算熵。<br/> <strong class="kk iu"> ➗公式:</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/34b35bd0ab082ad46d18ec8cef07d958.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*3Q-WUioGHoOmOVs8_A536w.png"/></div></figure><p id="d36d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以将这个通用公式简化为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/d84dd0744dc9137fdfc4d95e6e04cd9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*QcBiZVDf706nZbSj-APPoA.png"/></div></figure><p id="ee26" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们再次使用熵从根节点开始完成所有步骤，步骤 1、3 和 4 的输出将变为如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oa"><img src="../Images/cdf493ac72a314b6433547d9891e6d61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*InxsNWjdbYXpRIXvhA5Rtg.png"/></div></div></figure><p id="85ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了这个输出，看起来<em class="le">是已经结合的</em>再次是从根节点的最佳分割。信息增益以类似的方式评估:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/23b17c99193aeb995bcac58080b5377e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ufDaq2SEZHgI1_2iq-ZWdQ.png"/></div></figure><p id="b645" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">鉴于我们获得了信息，分开是有道理的。让我们像以前一样对底部节点继续相同的步骤。</p><p id="ac08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为已婚角色的左节点是纯的，所以我们不再需要从中分离。但是我们将按照步骤尝试改进正确的节点。步骤 1、3 和 4 的输出如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d8e414eb6be9b16bfa7f6440fd7838f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*soivC7EJtilsX842IW5c6A.png"/></div></figure><p id="3067" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是最后一棵树🌴：</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi od"><img src="../Images/ca1fbe9d36dc652be0d142a6916ad96f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sh-UgMDKEKNJpz6NMoHhvw.png"/></div></div></figure><p id="2c40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你注意到了吗，在最不纯的情况下，gini 是 0.5，熵是 1，在这种情况下，节点在两个类之间平均分配，而对于只包含一个类的最纯的节点，这两个值都是 0。</p><p id="fcf2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您热衷于通过更多的实践来巩固您的学习，请随意使用您自己的小数据集来构建一个简单的决策树。您可以使用下面的示例脚本根据 sklearn 输出检查您的树:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="oe of l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">不要忘记用您要使用的数据替换这些数据</p></figure><h1 id="bc12" class="mc md it bd me mf og mh mi mj oh ml mm jz oi ka mo kc oj kd mq kf ok kg ms mt bi translated">3.结束语💭</h1><p id="1121" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">在实践中，使用决策树作为构建块的更健壮的算法可能比决策树本身更常用作预测模型。</p><p id="2e91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你不小心，决策树很容易过度拟合。我们可以说<em class="le"> was_on_a_break </em>不是一个很好的特性，因为它恰好是一个非常具体的特性，只适用于训练数据中的记录。因此，使用该特征来建立模型会导致模型过度适应训练数据中的噪声。</p><p id="4df4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不管怎样，我认为理解构建决策树的基本原则仍然是有价值的。✨</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ol"><img src="../Images/0087b5aed260bbc5c403c3aa4354aae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*L31yleBub6XsRrRM"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">照片由<a class="ae mz" href="https://unsplash.com/@aaronburden?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚伦·伯顿</a>在<a class="ae mz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="5399" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果你使用</em> <a class="ae mz" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="le">我的推荐链接</em> </a>，<em class="le">成为会员，你的一部分会费会直接去支持我。</em></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="5253" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le">好玩的事实:</em> </strong> <em class="le">你听说过谷歌给朋友的复活节彩蛋吗？在谷歌上搜索</em> <a class="ae mz" href="https://www.google.com/search?q=ross+geller&amp;rlz=1C1CHBF_enAU847AU848&amp;oq=ross+geller&amp;aqs=chrome..69i57j0l7.4197j0j7&amp;sourceid=chrome&amp;ie=UTF-8" rel="noopener ugc nofollow" target="_blank"> <em class="le">罗斯·盖勒，点击他照片正下方右侧的小沙发。</em>🙊另外 5 个角色也有这个功能！</a></p><p id="e768" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谢谢你看我的帖子。我希望你已经学到了一些东西，✂️.如果你感兴趣的话， 以下是我的其他帖子的链接:<br/> ◼️ <a class="ae mz" rel="noopener" target="_blank" href="/transforming-variables-in-a-pandas-dataframe-bce2c6ef91a1">如何在熊猫数据框架中转换变量</a> <br/> ◼️ <a class="ae mz" href="https://medium.com/@zluvsand/two-simple-ways-to-scrape-text-from-wikipedia-in-python-9ce07426579b" rel="noopener">用 Python 从维基百科中抓取文本的两种简单方法</a> <br/> ◼️ <a class="ae mz" rel="noopener" target="_blank" href="/simple-wordcloud-in-python-2ae54a9f58e5">用 Python 编写简单的 word cloud</a><br/>◼️️<a class="ae mz" rel="noopener" target="_blank" href="/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96">自然语言处理简介—第 1 部分:用 Python 预处理文本</a> <br/> ◼️ <a class="ae mz" href="https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc" rel="noopener">自然语言处理简介—第 2 部分:词汇化和词干化的区别</a> <br/> ◼️ <a class="ae mz" href="https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc" rel="noopener">自然语言处理简介</a></p><p id="756b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再见🏃💨</p></div></div>    
</body>
</html>