<html>
<head>
<title>12 Main Dropout Methods: Mathematical and Visual Explanation for DNNs, CNNs, and RNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">12种主要的退出方法:DNNs、CNN和RNNs的数学和视觉解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/12-main-dropout-methods-mathematical-and-visual-explanation-58cdc2112293?source=collection_archive---------12-----------------------#2020-06-04">https://towardsdatascience.com/12-main-dropout-methods-mathematical-and-visual-explanation-58cdc2112293?source=collection_archive---------12-----------------------#2020-06-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="82bb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入研究用于正则化、蒙特卡罗不确定性和模型压缩的DNNs、CNN和RNNs丢弃方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/af64989da4525b5c8c9b908e7d795811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EqBoJoNt6VhWF_AXVYrjCw.png"/></div></div></figure><h1 id="eb4f" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">动机</h1><p id="7a0f" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在<strong class="lo iu">(深度)机器学习</strong>中训练模型的<strong class="lo iu">重大挑战</strong>之一就是<strong class="lo iu">协同改编</strong>。这意味着神经元非常依赖彼此。它们在很大程度上相互影响，并且在输入方面不够独立。同样常见的情况是，一些神经元具有比其他神经元更显著的预测能力。换句话说，我们的输出过度依赖于一个神经元。</p><p id="0b36" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">这些影响<strong class="lo iu">必须避免</strong>并且重量必须分配给<strong class="lo iu">以防止过度配合</strong>。一些神经元的共同适应性和高预测能力可以用不同的<strong class="lo iu">正则化</strong>方法来调节。最常用的一种是<strong class="lo iu">辍学</strong>。然而，辍学方法的全部能力很少被使用。</p><p id="22b1" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">根据是<strong class="lo iu"> DNN </strong>，还是<strong class="lo iu"> CNN </strong>还是<strong class="lo iu"> RNN </strong>，可以采用不同的<strong class="lo iu">退出方式</strong>。实际操作中，<strong class="lo iu">我们只用一个</strong>(或者差不多)。我认为这是一个可怕的陷阱。所以在这篇文章中，我们将从数学上<strong class="lo iu">和视觉上</strong>深入到辍学者的世界中去理解:</p><ul class=""><li id="46d4" class="mn mo it lo b lp mi ls mj lv mp lz mq md mr mh ms mt mu mv bi translated">标准辍学方法</li><li id="1c58" class="mn mo it lo b lp mw ls mx lv my lz mz md na mh ms mt mu mv bi translated">标准辍学的变体</li><li id="ffdd" class="mn mo it lo b lp mw ls mx lv my lz mz md na mh ms mt mu mv bi translated">应用于CNN的退出方法</li><li id="3122" class="mn mo it lo b lp mw ls mx lv my lz mz md na mh ms mt mu mv bi translated">应用于RNNs的退出方法</li><li id="81e7" class="mn mo it lo b lp mw ls mx lv my lz mz md na mh ms mt mu mv bi translated">其他压差应用(蒙特卡罗和压缩)</li></ul><p id="50b0" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">(不好意思停不下来，所以是12法多一点…😄)</p><h1 id="a338" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">记号</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/0c4cae7f7b8c48e98e6852c670b0a52a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uNI2MEBhWUqTcZy1ZreRtw.png"/></div></div></figure><h1 id="e121" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">标准辍学</h1><p id="d77c" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">最广为人知和使用的<strong class="lo iu">退出方法是Hinton等人在2012年推出的<strong class="lo iu">标准退出</strong>【1】..通常简称为“<strong class="lo iu">辍学”</strong>，出于显而易见的原因，在本文中我们将称之为标准辍学。</strong></p><div class="kj kk kl km gt ab cb"><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/36ccfb99d7eaa5ae4dad193a8d400bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*TCxHXQixO2IHUxg0MbqzDQ.png"/></div></figure><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/8df86b96e99b95611a37b72b4edb37af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*n-7aBFrGvc3P_HjfybTPKw.png"/></div><p class="ni nj gj gh gi nk nl bd b be z dk nm di nn no translated">p=0.5时的辍学</p></figure></div><p id="bd67" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">为了<strong class="lo iu">防止过拟合</strong>在<strong class="lo iu">训练</strong>阶段，神经元被<strong class="lo iu">随机省略</strong>。在密集(或全连接)网络中引入，<strong class="lo iu">对于每一层</strong>我们给出了<strong class="lo iu">丢失</strong>的概率<code class="fe np nq nr ns b">p</code>。在每次迭代中，每个神经元都有被忽略的概率<code class="fe np nq nr ns b">p</code>。Hinton等人的论文推荐了输入层上的丢失概率<code class="fe np nq nr ns b">p=0.2</code>和隐藏层上的概率<code class="fe np nq nr ns b">p=0.5</code>。显然，我们对输出层感兴趣，这是我们的预测。因此，我们不在输出层应用辍学。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/aa8feaf5927f5cfeb3460f45273b81cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QXGCIgSDrhEiqTJi2kQxtg.png"/></div></div></figure><p id="b4e0" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">在数学上，我们说每个神经元的遗漏概率遵循一个<strong class="lo iu">伯努利</strong>概率分布<code class="fe np nq nr ns b">p</code>。因此，我们在神经元(层)的向量与<strong class="lo iu">掩码之间进行<strong class="lo iu">元素方式的</strong>乘法，其中每个元素都是遵循伯努利分布</strong>的随机变量。</p><p id="2be8" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">在<strong class="lo iu">测试</strong>(或推断)阶段，没有<strong class="lo iu">没有脱落</strong>。所有的神经元都是活跃的。为了补偿与训练阶段相比的额外信息，我们<strong class="lo iu">根据出现的概率</strong>进行加权。所以一个神经元不被忽略的概率。是<code class="fe np nq nr ns b">1-p</code>。</p><h1 id="52b4" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">下拉连接</h1><p id="3cc2" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">也许你已经熟悉了标准的辍学方法。但是有很多变化。要调整密集网络的前向传递，您可以对神经元应用下降。L. Wan等人介绍的<strong class="lo iu">drop connect</strong>【2】不是<strong class="lo iu">而是<strong class="lo iu">直接在神经元上施加压降，而是在连接这些神经元的权重和偏置</strong>上施加压降。</strong></p><div class="kj kk kl km gt ab cb"><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/36ccfb99d7eaa5ae4dad193a8d400bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*TCxHXQixO2IHUxg0MbqzDQ.png"/></div></figure><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/4a0acddf7ee57b8b4fa1aa885cf3e01c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bW1D2Cf8bygBkRNg-hm_Rg.png"/></div><p class="ni nj gj gh gi nk nl bd b be z dk nm di nn no translated">p=0.5时的辍学</p></figure></div><p id="dcfa" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">因此，我们发现了与标准辍学方法相同的机制。除了掩模(其元素是遵循分布的随机变量)不是应用于<strong class="lo iu">神经元</strong> <strong class="lo iu">向量</strong> <strong class="lo iu">而是应用于将该层连接到前一层的权重矩阵</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/4731db0ed906020e299907987362f20d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l5FTocBWKdVvp4OGqPULRg.png"/></div></div></figure><p id="df13" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">对于测试阶段，可以使用与标准退出方法相同的逻辑。我们可以乘以<strong class="lo iu">出现的概率</strong>。但这不是L. Wan等人提出的方法。这很有趣，因为他们提出了<strong class="lo iu">一种即使在测试阶段</strong>也能通过应用DropConnect的<strong class="lo iu">高斯近似</strong>来处理dropout】的随机方法。然后通过<strong class="lo iu">从该高斯表示中随机抽取样本</strong>。我们将在突出部分之后回到高斯近似。</p><h1 id="8154" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">杰出的人</h1><p id="eec3" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">L. J. Ba和B. Frey介绍的<strong class="lo iu">脱颖而出</strong>【3】作为一种标准的脱颖而出方法，是基于一个<strong class="lo iu">伯努利</strong>面具(我会根据这些面具所遵循的分布来称呼它们，这样会更简单)。不同之处在于，在层上，省略神经元的概率<code class="fe np nq nr ns b">p</code>是<strong class="lo iu">而不是常数。根据重量</strong>的<strong class="lo iu">值<strong class="lo iu">自适应</strong>。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/ef8fda8899f74e35bc2e77ad06f526c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdAxjZlABzYVNhS1LQbPrw.png"/></div></div></figure><p id="2680" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">这可以用于任何<code class="fe np nq nr ns b">g</code>激活函数，甚至可以是一个单独的神经网络。同样，对于<code class="fe np nq nr ns b">Ws</code>，它可以是<code class="fe np nq nr ns b">W</code>的函数。然后对于测试阶段，我们通过出现的概率来平衡。</p><h2 id="d464" class="nw kv it bd kw nx ny dn la nz oa dp le lv ob oc lg lz od oe li md of og lk oh bi translated">例子</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/3299299151568ea768e3fcbfaa95e717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VdXDL8muSS3IPjMpjj3hiA.png"/></div></div></figure><p id="72ca" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">有点晦涩，举个例子吧。在他们的论文中，他们表明在实践中<strong class="lo iu">信任网络权重</strong>可以近似为权重的<strong class="lo iu">仿射函数</strong>。又比如我会把乙状结肠的<strong class="lo iu">绝对值作为<strong class="lo iu">激活函数</strong>。</strong></p><div class="kj kk kl km gt ab cb"><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/3499d793cd361ab39f04014135124c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Fgy_ZOKucO-Ma6aKix16nA.png"/></div></figure><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/98272530768203e737dd7e76a1ab9dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*P0ZgcBHjeePpSCb_WAXmMg.png"/></div><p class="ni nj gj gh gi nk nl bd b be z dk nm di nn no translated">与突出示例相比，p=0.5时的标准压差</p></figure></div><p id="f1c6" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">因此，我们可以看到<strong class="lo iu">权重</strong>越大，神经元被忽略的<strong class="lo iu">概率</strong>就越大。这有力地限制了某些神经元可能具有的高预测能力。</p><h1 id="2ed1" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">高斯漏失</h1><p id="9171" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">应用于神经网络的退出方法的列表继续增长。因此，在继续讨论DNNs之外的其他内容之前，我想谈谈一类辍学方法，这无疑是最令人着迷的。</p><p id="e8e4" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">仅举几个例子，<strong class="lo iu">快速辍学</strong>【4】，<strong class="lo iu">变化辍学</strong>【5】或<strong class="lo iu">具体辍学</strong>【6】是从<strong class="lo iu">贝叶斯角度</strong>解释辍学的方法。具体地说，我们有一个掩码，其元素是遵循高斯分布 ( <strong class="lo iu">正态</strong>分布)的随机<strong class="lo iu">变量，而不是伯努利掩码。这里不赘述<strong class="lo iu">大数定律</strong>的论证，那不是重点。所以我们试着直观的理解一下这个<strong class="lo iu"/>。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/76034811b27bfea73daa396e4cd0656f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sO6RgR__S5sWGj95AtVZpQ.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">p=0.5时的辍学</p></figure><p id="c314" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">论文[4]、[5]和[6]表明我们可以用正态定律T20为我们的辍学者模拟T17伯努利面具T18。但这有什么区别。<strong class="lo iu">万物与虚无同时存在。由于<strong class="lo iu">共同适应</strong>和/或我们神经元的<strong class="lo iu">预测能力</strong>，它不会改变任何关于这些方法与<strong class="lo iu">过度拟合</strong>的相关性的事情。但是与之前介绍的方法相比，它改变了训练阶段所需的执行时间。</strong></p><p id="d288" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">从逻辑上讲，通过在每次迭代中省略掉丢失的神经元，<strong class="lo iu">那些在迭代中被省略的神经元在反向传播期间不被更新</strong>。他们不存在。所以训练阶段是<strong class="lo iu">放慢了</strong>。另一方面，通过使用高斯丢弃方法，<strong class="lo iu">所有神经元在每次迭代</strong>和每个训练样本中暴露。这样就避免了减速。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/6680346f9f7f2e9c446e5878565c7865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pgvyS519-G_ebE6Ixg-BoQ.png"/></div></div></figure><p id="e57b" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">在数学上，存在与高斯掩码的乘法(例如以1为中心，具有伯努利定律标准偏差<code class="fe np nq nr ns b">p(1-p)</code>)。这个<strong class="lo iu">通过在每次迭代中保持所有神经元<strong class="lo iu">活动</strong>来随机<strong class="lo iu">加权它们的预测能力</strong>来模拟</strong>退出。这种方法的另一个实际优势集中在1:在测试阶段，与没有脱落的模型相比，不需要进行修改。</p><h1 id="fe3a" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">集体辍学</h1><p id="619c" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">本文的“难”理解部分结束了。保留更多的<strong class="lo iu">直观的</strong>部分给我们<strong class="lo iu">更好的性能</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/05f61bbaa46ed38f50221622b3a19447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jyww8BNA-6xRom6Z1Q-hig.png"/></div></div></figure><p id="f1da" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated"><strong class="lo iu">图像</strong>或<strong class="lo iu">特征图</strong>的问题是像素非常<strong class="lo iu">依赖于它们的邻居</strong>。简而言之，在一张猫的图片上，如果你取一个对应于它皮毛的像素，那么所有相邻的像素都将对应于同一皮毛。几乎没有或者<strong class="lo iu">没有区别</strong>。</p><p id="deb7" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">所以我们理解标准辍学方法的<strong class="lo iu">极限</strong>。我们甚至可以说它是<strong class="lo iu">低效的</strong>，它带来的唯一改变是<strong class="lo iu">额外的计算时间</strong>。如果我们随机忽略图像上的像素，那么几乎没有信息被删除。<strong class="lo iu">省略的像素与其周围的像素</strong>几乎相同。表示<strong class="lo iu">性能差</strong>防止过拟合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/42c0eeb9729afd554a0f801d8431ba48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3VnFDZNKypFsV8oVAaGOww.png"/></div></div></figure><p id="2ac0" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">为什么不利用<strong class="lo iu">适当的</strong>并且经常在<strong class="lo iu">CNN</strong>中使用的层。例如<strong class="lo iu">最大池层</strong>。对于那些不知道的人:最大池层是一张图片上传递的过滤器或(特征图)选择重叠区域的最大激活。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/9a31ec9c52b837934e54ffa7f0b67460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rMnN_OuCRXmqJSYs3xkkCA.png"/></div></div></figure><p id="cdce" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated"><strong class="lo iu">Max-Pooling Dropout</strong>【7】是H. Wu和X. Gu提出的一种应用于细胞神经网络的退出方法。在执行池化操作之前，它将伯努利遮罩直接应用于<strong class="lo iu">最大池化层</strong>内核<strong class="lo iu">。直觉上，这使得高活化剂的聚集最小化。限制某些神经元沉重的预测能力是一个非常好的观点。在测试阶段，您可以像前面的方法一样，根据出现的概率进行加权。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/26c26e0b16cc8140cdf93673642b01ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S1zdA9EqnciiwoPeXBU4mw.png"/></div></div></figure><p id="f926" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">最大池层已被作为一个例子，但同样可以用<strong class="lo iu">其他池层</strong>来完成。例如，对于<strong class="lo iu">平均池层</strong>，我们可以在培训阶段以同样的方式应用退出。然后在测试阶段，将不会有变化，因为它已经是一个加权平均。</p><h1 id="d821" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">空间缺失</strong></h1><p id="da3a" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">对于CNN，我们可以利用池层。但是，我们也可以通过遵循J. Tompson等人提出的<strong class="lo iu">空间丢失</strong>【8】方法来变得更聪明。他们建议<strong class="lo iu">克服经典丢失方法的问题</strong>，因为<strong class="lo iu">相邻像素</strong>与<strong class="lo iu">高度相关</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/371e35b55580faf5bc5d146e2799fee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KkqxjvXTIV_b365B41ltfg.png"/></div></div></figure><p id="e046" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们可以考虑<strong class="lo iu">对每个特征映射</strong>应用一个丢弃，而不是随机地对像素应用一个丢弃。如果我们以我们的猫为例，那么这就像从图像中移除红色，并迫使它对图像的蓝色和绿色进行概括。然后，在下一次迭代中随机丢弃其他特征图。</p><p id="317a" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我不知道如何恰当地用数学来表达，使它变得易懂。但是如果你理解了前面的方法，你就不会有任何问题了。在<strong class="lo iu">训练</strong>阶段，按照具有遗漏概率<code class="fe np nq nr ns b">p</code>的特征图应用<strong class="lo iu">伯努利掩码</strong><strong class="lo iu">。然后，在<strong class="lo iu">测试</strong>阶段，没有丢失，而是通过出现概率<code class="fe np nq nr ns b">1-p</code>对</strong>进行<strong class="lo iu">加权。</strong></p><h1 id="d982" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">断流器</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/2f06f55823d994efd1ddd5a11ce53b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bDAv5Rh-90cSszPnsddzBA.png"/></div></div></figure><p id="506b" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">让我们<strong class="lo iu">深入</strong>我们的方法，以克服相邻像素高度相关的事实。代替对每个特征图应用伯努利掩模，它们可以在区域中<strong class="lo iu">应用。这就是T. DeVries和G. W. Taylor提出的<strong class="lo iu">剪切</strong>方法【9】。</strong></p><p id="78c3" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">通过最后一次以我们的猫图像为例:这种方法可以通过<strong class="lo iu">隐藏图像的区域</strong>来进行归纳，从而限制过度拟合。我们最终得到猫的头垂下的图像。这迫使CNN去识别描述一只猫的不太明显的属性。</p><p id="f080" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">这一节也没有数学。这种方法在很大程度上取决于我们的想象力:正方形区域、长方形、圆形、所有的特征地图、一次一个或者可能几个……这由你<strong class="lo iu">决定</strong>。😃</p><h1 id="5cc8" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">最大落差</h1><p id="0c2d" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">最后，在结束关于CNN的这一节之前，我必须指出，显然可以将几种<strong class="lo iu">方法结合起来</strong>。当我们知道不同的方法时，这就是让我们强大的原因:我们可以同时利用它们的好处。这就是S. Park和N. Kwak提出的他们的<strong class="lo iu">最大落差</strong>方法【10】。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/76034811b27bfea73daa396e4cd0656f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sO6RgR__S5sWGj95AtVZpQ.png"/></div></div></figure><p id="201e" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">这种方法在某种程度上是池丢失和高斯丢失的混合。在<strong class="lo iu">最大池层</strong>上执行退出，但是使用<strong class="lo iu">贝叶斯方法</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/57d5eacd001d0cbff05fa4f591ef963e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PpsBp9a3NmXXH-5YbsANAg.png"/></div></div></figure><p id="a472" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">在他们的论文中，他们展示了这种方法给出的结果<strong class="lo iu"> </strong>与<strong class="lo iu">一样有效</strong>与空间丢失一样。此外，在每次迭代中，所有神经元都保持激活，这<strong class="lo iu">限制了训练阶段的减速</strong>。<br/>这些结果是在= 0.02和σ = 0.05时获得的。</p><h1 id="fd97" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">RNNDrop</h1><div class="kj kk kl km gt ab cb"><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/d3fcc384bcb8fe2a3395de73da50d813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*VeUzm8zAWYJIOu89on_irQ.png"/></div></figure><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/926f51fcba11903c1084a100e1f45866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*5vzCb_vDRRjLnyiQ5DgFoQ.png"/></div></figure></div><p id="19d7" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">嗯，我们已经看到了DNNs和CNN的一些退出方法。该研究还试图找出哪些方法对循环神经网络 (RNNs)有效。他们通常依赖于LSTMs，所以我将采用RNNs的这种特殊情况。它将<strong class="lo iu">推广</strong>到其他rnn。</p><p id="0c7f" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">问题很简单:对RNN申请退学是危险的。从这个意义上来说，RNN的目的是<strong class="lo iu">保持对事件的长期记忆</strong>。但是传统的辍学方法效率不高，因为它们<strong class="lo iu">会产生噪音</strong>，阻止这些模型保持长期记忆。将要介绍的方法允许<strong class="lo iu">长期保存这种记忆</strong>。</p><div class="kj kk kl km gt ab cb"><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/2868e874a0853c3f3a2ac61a346e3c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7dOgTorubL2oih51Qdlp9Q.png"/></div></figure><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/82e13a46264b96e686e49fd9549d3dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7iWaDTmGFgdFpYHQWMNaIg.png"/></div></figure></div><p id="fd14" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">T. Moon等人提出的<strong class="lo iu">RNNDrop</strong>【11】是最简单的方法。一个<strong class="lo iu">伯努利</strong>掩码仅应用于隐藏的<strong class="lo iu">单元状态</strong>。但是这个屏蔽从序列到另一个保持<strong class="lo iu">相同。这被称为<strong class="lo iu">丢失</strong>的每序列采样。这仅仅意味着在每次迭代中，我们创建一个随机掩码。然后从一个序列到另一个序列，该<strong class="lo iu">屏蔽保持不变</strong>。因此被丢弃的元素保持被丢弃，而当前的元素保持存在。所有的序列都是这样。</strong></p><h1 id="f9a3" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">经常辍学</h1><div class="kj kk kl km gt ab cb"><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/bce158c17080740fd29210b4939f63a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*l7xVOOzKgp9d0mRRtStd0g.png"/></div></figure><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/5f511ea0314f85d878191c7e9e2dd0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*V5omZ493bo_6o5SxlNX-_g.png"/></div></figure></div><p id="10d5" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">S. Semeniuta等人提出的<strong class="lo iu">复发性辍学</strong>【12】是一个有趣的变体。单元状态保持不变。下降仅适用于更新单元状态的<strong class="lo iu">部分。所以在每次迭代中，伯努利掩模使得一些元素不再对长期记忆有贡献。但是<strong class="lo iu">内存没有改变</strong>。</strong></p><h1 id="d870" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">变化的RNN辍学</h1><div class="kj kk kl km gt ab cb"><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/22e692975a2081a5e0d8e5006efdd337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*N_NxTuAkqG03ehi2JVJiaA.png"/></div></figure><figure class="nc kn nd ne nf ng nh paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/743e635de289ad7fc6286675eb8bc3cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*JeTJJCCY4R6S790s1g-Zdg.png"/></div></figure></div><p id="1acd" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">最后，Y. Gal和Z. Ghahramani介绍的<strong class="lo iu"> RNN丢弃</strong>【13】是在<strong class="lo iu">内部门</strong>之前应用基于序列的丢弃。这导致LSTM的<strong class="lo iu">不同点</strong>上的信号丢失。</p><h1 id="418a" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">打开我们的思维</strong></h1><p id="9130" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">仍然有许多不同的辍学方法，但是我们将在这里停止这篇文章。完成它，我发现知道<strong class="lo iu">辍学方法不仅仅是正规化方法</strong>非常有趣。</p><h2 id="78f1" class="nw kv it bd kw nx ny dn la nz oa dp le lv ob oc lg lz od oe li md of og lk oh bi translated">蒙特卡洛辍学</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/d44210eb4238ace9c4e994c8ee452c24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*KpaDxwalfKUvKnLw_6yFMw.gif"/></div></div></figure><p id="fcbb" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">退出方法还可以提供模型<strong class="lo iu">不确定性</strong>的<strong class="lo iu">指标</strong>。让我解释一下。对于相同的输入，经历退出的模型将在每次迭代中拥有<strong class="lo iu">不同的架构</strong>。这导致<strong class="lo iu">输出</strong>中的<strong class="lo iu">差异</strong>。如果网络<strong class="lo iu">相当一般化</strong>并且如果<strong class="lo iu">共同适应</strong>受限，则预测<strong class="lo iu">分布</strong>在整个模型中。这导致在具有相同输入的每次迭代中输出的<strong class="lo iu">较低的方差</strong>。研究这个方差可以给出分配给模型的<strong class="lo iu">置信度</strong>的概念。这可以从Y. Gal和Z. Ghahramani的方法中看出[14]。</p><h2 id="c422" class="nw kv it bd kw nx ny dn la nz oa dp le lv ob oc lg lz od oe li md of og lk oh bi translated">模型压缩</h2><p id="d65a" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">最后<strong class="lo iu">直观的</strong>，通过随机应用漏失，我们可以看到给定神经元甚至层的<strong class="lo iu">效率或无效率</strong>进行预测。根据这一观察，我们可以通过减少参数数量来压缩模型，同时将性能下降最小化。K. Neklyudov等人[15]已经提出了这样一种方法，该方法使用变分丢失来修剪DNNs和CNN。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/a41213d2ce618721d92c099fba464fc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eGtKKRvZfNm6mIdQlVXisA.png"/></div></div></figure><blockquote class="oo op oq"><p id="cd08" class="lm ln or lo b lp mi ju lr ls mj jx lu os mk lx ly ot ml mb mc ou mm mf mg mh im bi translated">知识就是分享。 <br/> <strong class="lo iu">支持</strong>我，一键获得<strong class="lo iu">访问<a class="ae ov" href="https://axel-thevenot.medium.com/membership" rel="noopener"> <strong class="lo iu">中我所有文章的</strong>。</a></strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/94e903d7d66ff043ca9645dbc42c33bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y7Q5U2MjIji7pvt6DRzq3A.png"/></div></div></figure><h1 id="a420" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">来源和参考</h1><p id="c01e" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">[1] G. E. Hinton，N. Srivastava，A. Krizhevsky，I. Sutskever和R. R. Salakhutdinov，<strong class="lo iu">通过防止特征检测器的共同适应来改进神经网络</strong></p><p id="e36c" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[2] L. Wan，m .，S. Zhang，Y. LeCun和R. Fergus，<strong class="lo iu">使用dropconnect调整神经网络</strong></p><p id="b659" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[3] L. J. Ba和B. Frey，<strong class="lo iu">用于训练深度神经网络的自适应退出</strong></p><p id="2ca0" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[4] S .王和c .曼宁，<strong class="lo iu">快速辍学训练</strong></p><p id="6582" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[5] D. P .金玛、t .萨利曼斯和m .韦林，<strong class="lo iu">变分退出和局部重新参数化技巧</strong></p><p id="88c0" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[6] Y. Gal，J. Hron，A. Kendall，<strong class="lo iu">混凝土脱落</strong></p><p id="3b08" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[7] H. Wu和X. Gu，<strong class="lo iu">卷积神经网络的丢失训练</strong></p><p id="288e" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[8] J. Tompson、R. Goroshin、A. Jain、Y. LeCun和C. Bregler，<strong class="lo iu">使用卷积网络的有效目标定位</strong></p><p id="2f90" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[9] T. DeVries和G. W. Taylor，<strong class="lo iu">改进了截断卷积神经网络的正则化</strong></p><p id="f7df" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[10] S. Park和N. Kwak，<strong class="lo iu">卷积神经网络中的丢包效应分析</strong></p><p id="f99e" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[11] T. Moon，H. Choi，H. Lee和I. Song，<strong class="lo iu"> Rnndrop </strong></p><p id="6561" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[12]s . semen uita，A. Severyn和E. Barth，<strong class="lo iu">无记忆丧失的经常性辍学</strong></p><p id="2778" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[13] Y. Gal和Z. Ghahramani，<strong class="lo iu">递归神经网络中辍学的理论基础应用</strong></p><p id="d3dd" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[14] Y. Gal和Z. Ghahramani，<strong class="lo iu">辍学作为贝叶斯近似:表示深度学习中的模型不确定性</strong></p><p id="cf71" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[15] K. Neklyudov、D. Molchanov、A. Ashukha和D. P. Vetrov，<strong class="lo iu">通过对数正态乘法噪声进行结构化贝叶斯修剪</strong></p><p id="37d6" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">[16] A. Labach，H. Salehinejad，<strong class="lo iu">深度神经网络退出方法调查</strong></p><p id="2306" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated"><strong class="lo iu">所有图片和gif都是自制的，可以免费使用</strong></p></div></div>    
</body>
</html>