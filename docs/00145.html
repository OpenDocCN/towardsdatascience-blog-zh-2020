<html>
<head>
<title>2019 — Year of BERT and Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2019 —伯特和变压器年</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/2019-year-of-bert-and-transformer-f200b53d05b9?source=collection_archive---------8-----------------------#2020-01-05">https://towardsdatascience.com/2019-year-of-bert-and-transformer-f200b53d05b9?source=collection_archive---------8-----------------------#2020-01-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e98a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一篇关于2019年BERT和NLP项目的短文</h2></div><p id="b483" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">BERT论文于2018年底发布，之后在2019年，我们看到了自然语言处理(NLP)和自然语言理解(NLU)领域的大量活动，从BERT在谷歌的搜索生产中使用到其他几个架构，这些架构显示出了比BERT更好的结果。BERT在情感分析、问题回答、句子相似度等多项自然语言处理任务中表现出色。我在2019年1月使用BERT预训练的权重进行了一个<a class="ae le" href="https://blog.insightdatascience.com/using-transfer-learning-for-nlp-with-small-data-71e10baf99a6" rel="noopener ugc nofollow" target="_blank">项目</a>，所以这可能是因为我在这一领域工作，但我在任何地方都会看到一些关于BERT的参考资料——比如中型文章、iOS上的BERT、BERT的硬件加速、Kaggle比赛、受BERT启发的更大模型等。我也认为<a class="ae le" href="https://en.wikipedia.org/wiki/Sesame_Street" rel="noopener ugc nofollow" target="_blank">芝麻街</a>的名字与伯特的流行有一定关系，尽管开创名字潮流的功劳归于来自人工智能<a class="ae le" href="https://allenai.org/" rel="noopener ugc nofollow" target="_blank">艾伦研究所</a>的<a class="ae le" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank"> ELMO论文</a>(深度语境化的单词表达)。BERT使用双向变压器和许多其他架构，这是在BERT也使用了某种形式的变压器之后出现的，基于论文“<a class="ae le" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>”。</p><h2 id="a309" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">BERT之前的一些主要NLP项目的时间表:</h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/e65c7880b5f9036ad0372d2d09b3dc42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cozibGuv9jX8bheqyirEvA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">一些主要NLP项目的时间表</p></figure><p id="a87c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">2013年发布的Word2Vec </a>模型仍然非常受欢迎，它通常是任何NLP问题中的第一个尝试。<a class="ae le" href="https://fasttext.cc/" rel="noopener ugc nofollow" target="_blank">快速文本</a>和<a class="ae le" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>嵌入也非常有用。</p><p id="0513" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">语言模型</strong></p><p id="4ab8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ELMO— Elmo使用双向LSTM。预训练的Elmo重量可以从<a class="ae le" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank">这里</a>下载。要使用Elmo嵌入，可以使用<a class="ae le" href="https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md" rel="noopener ugc nofollow" target="_blank"> AllenNLP </a>库、<a class="ae le" href="https://tfhub.dev/google/elmo/3" rel="noopener ugc nofollow" target="_blank"> Tensorflow hub </a>或<a class="ae le" href="https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md" rel="noopener ugc nofollow" target="_blank"> Flair </a>库。</p><p id="e5ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank"> ULMFiT </a> —杰瑞米·霍华德和塞巴斯蒂安·鲁德的ULMFiT论文描述了为特定任务微调语言模型的技术，它使用了LSTM的。你可以在这里找到如何使用ULMFiT使用fastai <a class="ae le" href="https://docs.fast.ai/text.html" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="856d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述两篇论文出现在BERT之前，没有使用基于变压器的架构。</p><p id="72be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> BERT </strong> —原文是<a class="ae le" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">这里</a>，还有一个很好的教程，里面有Jay Alammar的插图<a class="ae le" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">这里</a>。预训练的重量可以从官方Github repo <a class="ae le" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">这里</a>下载。BERT也可以作为Tensorflow hub <a class="ae le" href="https://tfhub.dev/google/collections/bert/1" rel="noopener ugc nofollow" target="_blank">模块</a>使用。有各种各样的其他库，也可以很容易地使用预训练的嵌入来微调它们，稍后会在本帖中提到。下面的时间线是2019年之后出现的一些主要论文。谷歌甚至开始在产品中使用BERT来改善搜索结果。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mv"><img src="../Images/23cb46811aece523a568c31c9da79d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZUHC1z5V6AsR_MOltzW-NQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">BERT之后的项目时间表</p></figure><p id="ef63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1901.02860" rel="noopener ugc nofollow" target="_blank">Transformer-XL</a>—<a class="ae le" href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html" rel="noopener ugc nofollow" target="_blank">2019年1月发布的Transformer-XL </a>通过使用一种允许学习超出固定长度上下文的架构，对Transformer进行了改进。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/575c57c16bc27801a85b9aae2edbe12f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/0*cC5U4z8Q9gnKAG6k.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">《变形金刚》与《进化金刚》的对比— <a class="ae le" href="https://ai.googleblog.com/2019/06/applying-automl-to-transformer.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="0429" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1901.11117" rel="noopener ugc nofollow" target="_blank">进化变压器</a>——大约与Transformer-XL同时，<a class="ae le" href="https://ai.googleblog.com/2019/06/applying-automl-to-transformer.html" rel="noopener ugc nofollow" target="_blank">进化变压器</a>发布，这是一种通过进行基于<a class="ae le" href="https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html" rel="noopener ugc nofollow" target="_blank">进化的</a>神经架构搜索(NAS)开发的变压器架构。</p><p id="1d6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a> —继伯特之后，我认为另一个获得最多新闻报道的项目是OpenAI的<a class="ae le" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>，因为它能够生成几乎类似人类的句子，而且OpenAI最初决定不发布最大的模型，因为担心该模型被用于制造假新闻等。近10个月后，他们发布了最大的型号。你可以通过<a class="ae le" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>在<a class="ae le" href="https://talktotransformer.com/" rel="noopener ugc nofollow" target="_blank">https://talktotransformer.com/</a>和<a class="ae le" href="https://transformer.huggingface.co/" rel="noopener ugc nofollow" target="_blank">https://transformer.huggingface.co/</a>玩模型。我想如果它换个名字，甚至是一个芝麻街角色，它会更受欢迎:)</p><p id="563f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1904.09223" rel="noopener ugc nofollow" target="_blank">厄尼</a>和<a class="ae le" href="https://arxiv.org/abs/1907.12412" rel="noopener ugc nofollow" target="_blank">厄尼2 </a> —目前厄尼2.0在GLUE排行榜上排名第一。<a class="ae le" href="https://github.com/PaddlePaddle/ERNIE" rel="noopener ugc nofollow" target="_blank"> Github回购。</a></p><p id="9ff8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNET </a></p><p id="f9d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"> RoBERTa </a> —本文公正地衡量了BERT的各种超参数的影响，并表明原始BERT模型训练不足，通过更多的训练/调整，它可以超越最初的结果。目前，RoBERTa的结果在GLUE排行榜上排名第8！</p><p id="165f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/" rel="noopener ugc nofollow" target="_blank"> Salesforce CTRL </a> — CTRL模型有16亿个参数，并提供控制人工文本生成的方法。</p><p id="167e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank"> ALBERT </a> —本文描述了参数缩减技术，以降低内存缩减并提高BERT模型的训练速度。<a class="ae le" href="https://github.com/google-research/ALBERT" rel="noopener ugc nofollow" target="_blank">阿尔伯特·雷波</a>拥有预先训练好的砝码。艾伯特基本模型有12M个参数，而伯特基本模型有110M个参数！</p><p id="5160" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1910.13034" rel="noopener ugc nofollow" target="_blank">大鸟</a></p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="b147" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">基准测试</strong></p><p id="18d0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">评估这些语言模型的方法之一是Glue Benchmark，它包括评估模型的各种NLP任务，如分类、QA等。在其发布时，BERT位于该表的顶端，但在短短一年内，它已移动到第19位(截至2020年1月2日)。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mx"><img src="../Images/35262c5028958841fda13ca14acf0bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8WjKoA1jPzLNQGisEo0toQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">截至2020年1月2日的Glue基准排行榜— <a class="ae le" href="https://gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="acf1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在有一个<a class="ae le" href="https://super.gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank">强力胶</a>基准测试，由更难的语言理解任务组成。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi my"><img src="../Images/bc229ac313335fc60eaec83b95257419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hSkFJLEOD2mMzKskARvu_g.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">截至2020年1月2日的强力胶基准排行榜— <a class="ae le" href="https://super.gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="cb78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于评估问答系统<a class="ae le" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">班</a> ( <strong class="kk iu"> S </strong>坦福德<strong class="kk iu">曲<strong class="kk iu">估计</strong>A</strong>nswering<strong class="kk iu">D</strong>ataset)是常用的，BERT和后来的基于transformer的模型也在这里名列前茅:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mz"><img src="../Images/29eb136604478a5bb6cb1f70d2caea11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tbie_0WG7jmNB0X8SVV8aA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">截至2020年1月2日的SQuAD 2.0排行榜— <a class="ae le" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="1764" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还有各种其他排行榜，如<a class="ae le" href="https://leaderboard.allenai.org/orb/submissions/public" rel="noopener ugc nofollow" target="_blank"> ORB </a>、<a class="ae le" href="https://leaderboard.allenai.org/open_book_qa/submissions/public" rel="noopener ugc nofollow" target="_blank"> OpenBookQA </a>等关于<a class="ae le" href="https://leaderboard.allenai.org/" rel="noopener ugc nofollow" target="_blank">https://leaderboard.allenai.org/</a>的排行榜，并且在大多数排行榜中都提到了伯特！</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="5d66" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">其他BERT相关项目</strong></p><p id="9401" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank">蒸馏伯特</a>——更小的伯特使用来自Huggingface的模型蒸馏，博文<a class="ae le" href="https://medium.com/huggingface/distilbert-8cf3380435b5" rel="noopener">此处</a>。</p><p id="464c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">威震天-LM——Nvidia的高效训练超大型语言模型的项目。</p><p id="d4e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://github.com/dmis-lab/biobert" rel="noopener ugc nofollow" target="_blank">BioBERT</a>——由<a class="ae le" href="https://dmis.korea.ac.kr/" rel="noopener ugc nofollow" target="_blank"> DMIS实验室</a>在生物医学领域训练的基于BERT的模型。</p><p id="1cb2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://deepset.ai/german-bert" rel="noopener ugc nofollow" target="_blank">德语伯特</a></p><p id="c9e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://camembert-model.fr/" rel="noopener ugc nofollow" target="_blank"> CamemBERT </a> —基于脸书罗伯塔建筑的法语语言模型。</p><p id="8d15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">BERT硬件加速——谷歌使用TPU更快地训练大型BERT模型，其他公司如<a class="ae le" href="https://www.graphcore.ai/posts/new-graphcore-ipu-benchmarks" rel="noopener ugc nofollow" target="_blank">graph core</a>(<a class="ae le" href="https://github.com/graphcore/examples/tree/master/applications/popart/bert" rel="noopener ugc nofollow" target="_blank">code</a>)和<a class="ae le" href="https://habana.ai/habana-labs-goya-delivers-inferencing-on-bert/" rel="noopener ugc nofollow" target="_blank"> Habana </a>也展示了他们的定制SoC如何加速BERT推理和训练。</p><p id="6309" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1904.01766" rel="noopener ugc nofollow" target="_blank">video Bert</a>—<a class="ae le" href="https://ai.googleblog.com/2019/09/learning-cross-modal-temporal.html" rel="noopener ugc nofollow" target="_blank">video Bert</a>是一个跨模态(或多模态)深度学习的例子，其中模型以自我监督的方式从视频中学习。</p><p id="207e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/abs/1908.02265" rel="noopener ugc nofollow" target="_blank"> ViLBERT </a> —学习联合视觉和语言表征的视觉和语言BERT模型。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="7ab5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> NLP库</strong></p><p id="d713" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是一些我认为在NLP领域工作的人必须知道的库</p><p id="f123" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Spacy  — Spacy是一个流行且快速的库，用于各种NLP任务，如标记化、词性等。它还带有预先训练好的模型，用于命名实体识别(NER)等。最近它通过<a class="ae le" href="https://github.com/explosion/spacy-transformers" rel="noopener ugc nofollow" target="_blank"> spacy-transformers </a>增加了对基于transformer的语言模型的支持，这个库使用了Huggingface transformer库。</p><p id="66db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是第一个提供BERT py torch实现的库，最初它被称为“PyTorch-pretrained-bert”。后来，他们增加了更多的模型，如GPT-2，XLNET等，现在这个库就叫做“变形金刚”。在不到一年的时间里，它已经成为最受欢迎的NLP库之一，并使得使用BERT和其他模型变得非常容易。使用他们的另一个库(【https://github.com/huggingface/swift-coreml-transformers】)你可以在iOS上使用BERT，GPT-2等！</p><p id="0537" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://allennlp.org/" rel="noopener ugc nofollow" target="_blank"> AllenNLP </a> —这是艾伦人工智能研究所的NLP图书馆，建在PyTorch的顶部。</p><p id="a5d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://github.com/flairNLP/flair" rel="noopener ugc nofollow" target="_blank"> Flair </a> —也是一个NLP库，带有NER、POS等模型，也支持<a class="ae le" href="https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md" rel="noopener ugc nofollow" target="_blank"> BERT、ELMO、XLNET </a>等嵌入。</p><p id="6682" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://gluon-nlp.mxnet.io/" rel="noopener ugc nofollow" target="_blank">gluonlp</a>—它是基于<a class="ae le" href="https://mxnet.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache MXNet </a>之上的NLP工具包，是首批包含预训练BERT嵌入的库之一。它有很多有用的<a class="ae le" href="https://gluon-nlp.mxnet.io/examples/index.html" rel="noopener ugc nofollow" target="_blank">例子/教程</a>。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="ce73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望在2020年，自然语言处理领域会有更多的创新和突破。要了解更多关于自然语言处理和人工智能的知识，你可以在twitter上关注我——<a class="ae le" href="https://twitter.com/MSuryavansh" rel="noopener ugc nofollow" target="_blank">https://twitter.com/MSuryavansh</a></p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><div class="lz ma mb mc gt na"><a href="https://blog.insightdatascience.com/using-transfer-learning-for-nlp-with-small-data-71e10baf99a6" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd iu gy z fp nf fr fs ng fu fw is bi translated">使用迁移学习进行小数据的自然语言处理</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">使用较小的数据集实现高文本分类准确率。</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">blog.insightdatascience.com</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no mi na"/></div></div></a></div><div class="np nq gp gr nr na"><a href="https://www.blog.google/products/search/search-language-understanding-bert/" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd iu gy z fp nf fr fs ng fu fw is bi translated">比以往更好地理解搜索</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">如果说我在谷歌搜索工作的15年中学到了什么，那就是人们的好奇心是无止境的…</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">www.blog.google</p></div></div><div class="nj l"><div class="ns l nl nm nn nj no mi na"/></div></div></a></div><div class="np nq gp gr nr na"><a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd iu gy z fp nf fr fs ng fu fw is bi translated">开源BERT:自然语言处理的前沿培训</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">自然语言处理面临的最大挑战之一是训练数据的短缺。因为NLP是一个…</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">ai.googleblog.com</p></div></div><div class="nj l"><div class="nt l nl nm nn nj no mi na"/></div></div></a></div><div class="np nq gp gr nr na"><a href="https://blog.rasa.com/compressing-bert-for-faster-prediction-2/" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd iu gy z fp nf fr fs ng fu fw is bi translated">了解如何让BERT变得更小更快</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">在这篇博文中，我们将讨论如何让像BERT这样的大型模型变得更小更快。</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">blog.rasa.com</p></div></div><div class="nj l"><div class="nu l nl nm nn nj no mi na"/></div></div></a></div><div class="np nq gp gr nr na"><a href="https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd iu gy z fp nf fr fs ng fu fw is bi translated">ALBERT:一个用于语言表达自我监督学习的Lite BERT</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">自从一年前BERT问世以来，自然语言研究已经采用了一种新的范式，利用了大量的…</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">ai.googleblog.com</p></div></div><div class="nj l"><div class="nv l nl nm nn nj no mi na"/></div></div></a></div></div></div>    
</body>
</html>