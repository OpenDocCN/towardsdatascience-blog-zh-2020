<html>
<head>
<title>The path from Maximum Likelihood Estimation to Hidden Markov Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从最大似然估计到隐马尔可夫模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-path-from-maximum-likelihood-estimation-to-hidden-markov-models-61aba5ba901c?source=collection_archive---------15-----------------------#2020-01-29">https://towardsdatascience.com/the-path-from-maximum-likelihood-estimation-to-hidden-markov-models-61aba5ba901c?source=collection_archive---------15-----------------------#2020-01-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="193e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你认为你很了解一个话题，试着解释给别人听。如果你能解释清楚，那么你就真正理解了这个主题。</p><p id="a7bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，在这篇文章中，我将尽可能全面地解释什么是隐马尔可夫模型。为此，我还需要描述一些主题，如最大似然估计(MLE)、马尔可夫链、向前向后算法和Baum-Welch算法。</p><p id="4cf7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在深入研究之前，我们需要问自己的第一个问题是:<strong class="js iu">学习分布有帮助吗？</strong>当然！以朴素贝叶斯为例。</p><p id="601f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，朴素贝叶斯处于监督学习的保护伞之下。<strong class="js iu">我们能在无监督学习上做些什么？</strong></p><h1 id="d9cd" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated"><strong class="ak">生成模型:</strong></h1><p id="b778" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">假设分布<em class="lr"> D </em>具有某种已知的形式。</p><p id="0158" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">分布的一些参数是未知的，我们的<strong class="js iu">目标</strong>是<strong class="js iu">根据一些样本<em class="lr"> S~D </em>找到这些参数</strong></p><p id="6260" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">应用生成模型可以更好地分类、推断(参数可以告诉我们关于数据的故事)、隐马尔可夫模型等。</p><p id="5a93" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lr">例:<br/> </em>我们来考虑一下伯努利分布:<em class="lr"> θ ∈ [0，1]，D _θ=伯努利(θ)。<br/> </em>现在我们来考虑一个样本<em class="lr"> S=(x_1，…，x_n) <br/> </em> <strong class="js iu">如何从S估计θ？我们稍后会回答这个问题。</strong></p><h1 id="2ab7" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated"><strong class="ak">最大似然估计:</strong></h1><p id="6164" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">最大可能性已经在过去的许多帖子中讨论过了。即便如此，为了这篇文章的完整性，我将提供(我认为是)一个相对简单的解释。</p><p id="6949" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">最大似然估计</strong> (MLE)是一种基于观测数据集估计分布参数的方法。</p><p id="437b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，我们需要定义这个问题:</p><ul class=""><li id="d458" class="ls lt it js b jt ju jx jy kb lu kf lv kj lw kn lx ly lz ma bi translated">分布<em class="lr"> D_θ </em></li><li id="690a" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn lx ly lz ma bi translated">样本<em class="lr"> S=(x_1，…，x_n) </em></li><li id="8bd6" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn lx ly lz ma bi translated">参数空间——我们分布的参数可能值的集合<br/>例1:伯努利分布<em class="lr">θ=【0，1】</em><br/>例2:高斯分布<em class="lr">θ=</em><strong class="js iu"><em class="lr">ℝ⋅ℝ+</em></strong></li></ul><p id="d2ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于实际的θ未知，我们希望使用<em class="lr"> S </em>来估算θ。换句话说，我们想要检查对于每个<em class="lr">θ’∈θ，从<em class="lr"> D_θ </em>生成<em class="lr"> S </em>的概率。</em></p><p id="09e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">MLE背后的直觉如下:</strong> <br/>选择<em class="lr">θ”</em>，这将使数据集<em class="lr"> S </em>相对于分布<em class="lr"> D_θ </em>最不令人惊讶。换句话说，目标是找到尽可能接近真实分布参数<em class="lr"> θ </em>的<em class="lr">θ】</em>。</p><p id="71a3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们用公式表示离散密度的定义:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/517d1d6be5980dfca19ef46791a0d080.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*meFZtc8a3HJ65rqhgwOITw.png"/></div></figure><p id="a107" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为使用对数总和要容易得多，所以让我们对等式应用log:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/8c39917075298d04ef742d0f8b39c0b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*YZjrpbgp6V2-BkRwWLMcEA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated"><strong class="bd mt">θ'相对于S的对数似然</strong></p></figure><p id="1b28" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用该表达式，我们可以将<strong class="js iu">最大似然估计值</strong>定义为使对数似然最大化的θ*:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/eee390ca29c8da52f7e0d682b40ed6b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*AOabuj4dUUVD9dxk8AOk4g.png"/></div></figure><p id="f4fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="lr">回到我们的伯努利例子:</em> </strong></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/7a67728786e973abf0fa4872c93f26ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*32LcYs7acqki_Jzo0CpYqQ.png"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/df4be09c6661f68df11da3ffb4e430c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*8krUjOOnwV-RLTHR1Ee_mw.png"/></div></figure><p id="1c40" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">计算可能性:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/899452fd32a42479606eb66b058fea6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*Ah9xxHYTeyJ8K15RQYRxxg.png"/></div></figure><p id="8752" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">计算对数似然性:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/c00b5e410ac6e5f9791745c6b5d6b155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*-VscfDd1WTrKQc3z7OfQIw.png"/></div></figure><p id="ec4d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，为了找到MLE，我们将对表达式求微分，并与零进行比较:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi my"><img src="../Images/b177278d2f1ba75d86650d500a74899f.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*nwiM4i0NMt7HFCQ4caQDcg.png"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a4351c66172263990ee0e89e6348d61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*1x4F6IPhd5arOjAUyb130g.png"/></div></figure><p id="143f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">意思是——我们的MLE是S中所有点的平均值，如果你仔细想想，这是有意义的。</p><p id="752a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们知道什么是最大似然，我们甚至看到了如何计算它的例子。</p><p id="2925" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，由于不是所有的分布都有封闭形式的解，我们需要一种方法来有效地估计最大似然估计。</p><p id="3481" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这可以通过<strong class="js iu">期望最大化(EM) </strong>算法有效地完成。该算法使用迭代过程在具有潜在变量的统计模型中执行最大似然估计。</p><h1 id="9633" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">马尔可夫链:</h1><p id="5c64" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">让我们定义一个有限马尔可夫链:</p><p id="b0b8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">给定n个状态:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi na"><img src="../Images/bb6eaca72401a1b80f4df34968b821fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*NLNNX3AT3vJoU1XU-ntK8g.png"/></div></figure><p id="1c4b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中n是有限的，并且初始起始状态<em class="lr"> S_0 </em>(固定的或从初始分布中提取的)，我们可以将转换定义为从当前状态<em class="lr"> S_t </em>移动到下一个状态<em class="lr"> S_(t+1) </em>。</p><p id="199b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">马尔可夫链中的转移必须满足马尔可夫性质:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/150fe6cd0bf2e8d32bae80f5b9fee7b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*3wMMbF0AGTvKiEXNtCwiwg.png"/></div></figure><p id="8476" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">换句话说，马尔可夫链是描述序列的随机模型，其中对于1≤t≤n，<em class="lr"> S_t </em>的概率仅取决于<em class="lr"> S_(t-1)的概率</em></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/7982c1c01fe4d1a81a29438a9a33789d.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*Opl6nkO1Rv7Ga7Es9KILHg.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated"><strong class="bd mt">来源:</strong><a class="ae nd" href="https://en.wikipedia.org/wiki/Markov_chain#/media/File:Markovkate_01.svg" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Markov _ chain #/media/File:Markov Kate _ 01 . SVG</a></p></figure><h1 id="4486" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated"><strong class="ak">隐马尔可夫模型:</strong></h1><p id="9b66" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">现在我们知道什么是马尔可夫链，我们可以定义隐马尔可夫模型。</p><p id="d655" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">隐马尔可夫模型(HMM)是一种模型，其中除了马尔可夫状态序列之外，我们还有一个输出序列。</p><p id="4d5c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">HMM可以用以下公式描述:</p><ul class=""><li id="9aa3" class="ls lt it js b jt ju jx jy kb lu kf lv kj lw kn lx ly lz ma bi translated"><strong class="js iu">状态数<em class="lr"> m </em>状态数</strong></li><li id="6184" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn lx ly lz ma bi translated">初始状态分布:</li></ul><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3ab3aebbe1e0ca92b6e910d44f4c62e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*-uk8qdSKfAaZUGGWHKVjoA.png"/></div></figure><ul class=""><li id="a3a1" class="ls lt it js b jt ju jx jy kb lu kf lv kj lw kn lx ly lz ma bi translated">转换模型(记住马尔可夫属性):</li></ul><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/83462803af2f26435c9902619c30cdc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*HGYe-KRlB4k4tsVn3xyYVQ.png"/></div></figure><ul class=""><li id="ed43" class="ls lt it js b jt ju jx jy kb lu kf lv kj lw kn lx ly lz ma bi translated">输出(排放)模型:</li></ul><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/aa250bda610c60c0ae81fccdc5a42080.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*WxyK8Uz04KcICMBpYYqX8g.png"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/dc8dc4aff29f48f1caaa025a4478cb33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*fcysYbwiRohYB2XJTps0PQ.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated"><strong class="bd mt">来源:</strong><a class="ae nd" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#/media/File:HiddenMarkovModel.svg" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Hidden _ Markov _ model #/media/File:hiddenmarkovmodel . SVG</a></p></figure><p id="f7d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">HMM是一种潜在变量模型，我们只能观察模型的输出:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/96ac49bf1be641701f86ba65682bb953.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*HCMytH1NKM-XH4YPwzV7LA.png"/></div></figure><p id="af8e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">而状态序列对我们是“隐藏”的。</p><p id="4cdb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将解决该领域的两个基本问题:</p><ol class=""><li id="d75d" class="ls lt it js b jt ju jx jy kb lu kf lv kj lw kn nj ly lz ma bi translated">学习—如何学习HMM模型</li><li id="d400" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn nj ly lz ma bi translated">评估-如何评估输出序列相对于给定模型的概率</li></ol><h1 id="486c" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated"><strong class="ak">评价:</strong></h1><p id="05a8" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">我们的目标是评估一个已知序列的概率:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/0f9ab3e01a10b095fdd842161b96db8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*ytCHgqlK6N4MAFqo0crQxA.png"/></div></figure><p id="d5ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">关于经过训练的HMM模型。</p><p id="b877" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用初始状态分布、转移模型和输出模型来描述HMM模型。</p><p id="b14e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lr">先试试</em> —我们可以用一种天真的方式，通过把所有隐藏状态序列的概率相加来评估概率。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/407a422e972f0bb6fa1a1d1f73dd8a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*4veWfsMpOBxWs2iX8PR10w.png"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/2a46e1d7bb992c95786f47dc99d17883.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*7bp017T5RzdIDTbbPE7VCg.png"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0ce38901829074ee908148258e6bede8.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*ZpacudjwJL-pyQZcBQF9jA.png"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/598945ae36fc6473a8d74fdfcbc767f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*EL0cPEzgXSryoKySWf50hw.png"/></div></figure><p id="9ba1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这将花费我们成倍的时间。不太好。但是你不要担心，我们可以利用模型的结构在多项式时间内计算出这个概率！</p><p id="06e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lr">第二次尝试</em> —</p><h2 id="1659" class="no kp it bd kq np nq dn ku nr ns dp ky kb nt nu lc kf nv nw lg kj nx ny lk nz bi translated">向前向后算法</h2><p id="f2df" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">该算法利用动态规划的思想在多项式时间内解决评价问题。从它的名字我们可以假设它以一种向前向后的方式(分而治之)在序列上工作。</p><p id="c41e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">远期概率:</strong></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/28949c5a074e6085eb81a2b9c76e9083.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*CNfoBUmXVd6u08qdpBOJkA.png"/></div></figure><p id="0570" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这意味着<em class="lr"> α_t(i) </em>表示我们观察到序列<em class="lr"> O_0，…，O_t </em>的概率，并且在<em class="lr"> t </em>位置，隐藏状态是<em class="lr"> s_t=i. </em></p><p id="3478" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，通过将<em class="lr"> α_t(i)除以所有α_t(j ),我们得到:</em></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/9020d252f4958c672a705a7b45c52bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*Lzs5AKMeeX5T8LgZ4mmVXQ.png"/></div></figure><p id="03ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以使用HMM概率以如下方式定义<em class="lr"> α_t(i) </em>:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/f296cb480dc8a016ae722ba5dbb889a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*PdCmBIwl4KUK_EdXtVZurg.png"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/c383cd69fbfb8799c647056be854b84f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*M6Xz4B972lNyyGPRTNoIgQ.png"/></div></figure><p id="c47c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">后向概率:</strong></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/b008c1fc8041a8e01cd4ab9beb0a7b01.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*VqDQxzK-u4RDUSAeuKx4Uw.png"/></div></figure><p id="c038" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">也就是说给定当前状态，从状态I得到未来事件O_(t+1)，…，O_n的概率是多少。</p><p id="59b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以使用HMM概率以如下方式定义β_t(i ):</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi od"><img src="../Images/0df245a7219267dbaa7796115c51e396.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*nOrjV1g24ubB3XjBxxSZnw.png"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/3ce33ef29643c8210358d1caadb3f2fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*X66_fIx7A6a1bOC2hh15yg.png"/></div></figure><p id="1636" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用αt，βt，我们现在可以评估后验概率:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/693295c6663ed1a5c562edbe70758e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*jpK6TVcuH6s6tckihCBfVw.png"/></div></figure><p id="823b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们需要计算的是</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi of"><img src="../Images/084a6bf2f7fb1e6cdb37b3eb92b19298.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*6qT9qttFOgCxhhMt77xVRg.png"/></div></figure><p id="be7b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对任意的<em class="lr"> t=0，…，n </em>在时间<em class="lr"> t </em>对所有可能的状态求和</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/294792f28cac1c0c3c5f928a6e54cb39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*PDMEX1Jfa3szsvKZXHM0TQ.png"/></div></figure><p id="ffaf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">前向-后向算法求解O(n⋅ m中的评估，其中<em class="lr"> m </em>是隐藏状态的数量。</p><h1 id="8b2e" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated"><strong class="ak">学习:</strong></h1><p id="c5af" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">现在我们知道了如何基于给定的模型来评估序列的概率，接下来我们将学习如何使用输出序列来估计HMM概率。</p><p id="31d4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">换句话说，我们希望估计:初始概率，转移概率和输出概率。</p><p id="58c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可能会问自己，为什么我们不能用最大似然估计？这个问题问得好，但是只有一个问题HMM的找MLE是NP难的！</p><p id="c347" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是不用担心，我们会使用Baum-Welch算法，这是我们之前提到的期望最大化(EM)算法的一个特例。</p><p id="5560" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">鲍姆-韦尔奇算法的一般思想如下:</p><p id="dc38" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当观察一个输出序列<strong class="js iu"> <em class="lr"> S </em> </strong>时，假设我们有HMM概率，我们可以估计隐藏状态。另一方面，给定隐藏状态，我们可以估计HMM概率。我们将在两者之间交替进行。</p><p id="d31a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在深入研究该算法之前，我们先定义以下内容:</p><p id="f256" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从前向后算法中回忆αt，βt。</p><p id="534e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们定义两个新的概率γ_t(i)，ξ_t(i):</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi og"><img src="../Images/5a05120c5f6249df9f9957054481a7fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JD3dvLcG7tcr2MIp_F5Nmg.png"/></div></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi ol"><img src="../Images/e1eb3bedb6946251fe47e29120108d5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HyW7eRXOQT48chigsXaz1Q.png"/></div></div></figure><p id="f6d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在可以描述Baum-Welch算法的伪代码(对于离散情况):<br/>给定L个序列的形式:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi om"><img src="../Images/c423a4e9ca49042274926f321712d76e.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*gkIkLJDMnuKnC-gDV-WRAg.png"/></div></figure><p id="b63c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">重复直到收敛:</strong></p><ol class=""><li id="0870" class="ls lt it js b jt ju jx jy kb lu kf lv kj lw kn nj ly lz ma bi translated"><strong class="js iu"> E步骤:</strong>计算隐藏状态的后验概率。<br/>计算:γ_t(i)，ξ_t(i)对于所有序列，I，j，t=0，…，n_𝔩</li><li id="68bf" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn nj ly lz ma bi translated"><strong class="js iu"> M步:</strong>根据E步的后验概率计算HMM模型概率。</li></ol><p id="f947" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">初始概率:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi on"><img src="../Images/f2b9e78eadd1a663df83cf3bdf268d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*473uvB_2j4zLse977ODqig.png"/></div></figure><p id="d979" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">转移概率:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/878aae7b4772fb6b5f2bb2cf33746011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*mnM20JPmJOzP-kgwTL_Ppw.png"/></div></figure><p id="73a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出概率:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/900b3534246a7a3a9522dcb14495e4c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*kr9q_RoQYTCDRp5zVHqCIg.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">算法描述基于https://www . cs . bgu . AC . il/~ inabd 191/wiki . files/lecture 20 _讲义. pdf</p></figure><p id="c841" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一直持续到收敛。</p><p id="c517" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们在这里没有讨论的一个话题是推理问题，我们如何根据观察序列找到最可能的隐藏状态序列。</p><h1 id="1c56" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">总结:</h1><ul class=""><li id="5cd3" class="ls lt it js b jt lm jx ln kb op kf oq kj or kn lx ly lz ma bi translated"><strong class="js iu"> <em class="lr">我们学习了什么是最大似然估计，以及如何在特定情况下计算它</em> </strong></li><li id="fa34" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn lx ly lz ma bi translated"><strong class="js iu"/></li><li id="2815" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn lx ly lz ma bi translated"><strong class="js iu"> <em class="lr">我们简单介绍了一下马尔可夫链</em> </strong></li><li id="50af" class="ls lt it js b jt mb jx mc kb md kf me kj mf kn lx ly lz ma bi translated"><strong class="js iu"> <em class="lr">我们学习了如何定义HMM模型，如何使用Baum-Welch算法估计模型参数，以及如何评估给定模型下输出序列的概率</em> </strong></li></ul><p id="e26e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在你(希望)知道什么是最大似然估计，如何计算，当计算最大似然估计是NP-hard时你能做什么，如何训练HMM模型，以及如何基于给定的模型评估输出序列。</p></div></div>    
</body>
</html>