<html>
<head>
<title>Apache/Airflow and PostgreSQL with Docker and Docker Compose</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache/Airflow 和 PostgreSQL 与 Docker 和 Docker Compose</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/apache-airflow-and-postgresql-with-docker-and-docker-compose-5651766dfa96?source=collection_archive---------4-----------------------#2020-09-07">https://towardsdatascience.com/apache-airflow-and-postgresql-with-docker-and-docker-compose-5651766dfa96?source=collection_archive---------4-----------------------#2020-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9bdb" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">带有码头工人的 ETL</h2><div class=""/><div class=""><h2 id="0170" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何使用 Docker 和 Docker Compose 设置带有 PostgreSQL 和 LocalExecutor 的官方 Apache/Airflow 容器</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e5ffcad7c2b07e53225c7c8d4a4792d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WZfAx83Z9WOEuioh"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@r3dmax?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔纳森派</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="e1a9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你好，在这篇文章中，我将向你展示如何使用 docker 和 docker-compose 用 PostgreSQL 和 LocalExecutor 设置官方的 Apache/Airflow。在这篇文章中，我不会讨论气流，它是什么，以及它是如何使用的。请查看<strong class="lk jd"/><a class="ae lh" href="https://airflow.apache.org/docs/stable/" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd"/>官方文档</a>了解更多相关信息。</p><p id="10e9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在设置和运行 Apache Airflow 之前，请安装<a class="ae lh" href="https://docs.docker.com/get-docker/" rel="noopener ugc nofollow" target="_blank">对接器</a>和<a class="ae lh" href="https://docs.docker.com/compose/install/" rel="noopener ugc nofollow" target="_blank">对接器组合</a>。</p><h1 id="63d0" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">对于那些赶时间的人...</h1><p id="34cd" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在这一章中，我将向您展示运行 airflow 所需的文件和目录，在下一章中，我将逐文件、逐行解释正在发生的事情。</p><p id="5fa8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，在根目录下再创建三个目录:<strong class="lk jd"> dags </strong>、<strong class="lk jd"> logs、</strong>和<strong class="lk jd"> scripts </strong>。此外，创建以下文件:<strong class="lk jd">。env、docker-compose.yml、entrypoint.sh </strong>和<strong class="lk jd"> dummy_dag.py. </strong>请确保这些文件和目录遵循以下结构。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="2676" class="ng mf it nc b gy nh ni l nj nk"><strong class="nc jd">#project structure</strong></span><span id="090e" class="ng mf it nc b gy nl ni l nj nk">root/<br/>├── dags/<br/>│   └── dummy_dag.py<br/>├── scripts/<br/>│   └── entrypoint.sh<br/>├── logs/<br/>├── .env<br/>└── docker-compose.yml</span></pre><p id="4ce2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">创建的文件应包含以下内容:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="8659" class="ng mf it nc b gy nh ni l nj nk"><strong class="nc jd">#docker-compose.yml</strong></span><span id="96d9" class="ng mf it nc b gy nl ni l nj nk">version: '3.8'<br/>services:<br/>    postgres:<br/>        image: postgres<br/>        environment:<br/>            - POSTGRES_USER=airflow<br/>            - POSTGRES_PASSWORD=airflow<br/>            - POSTGRES_DB=airflow<br/>    scheduler:<br/>        image: apache/airflow<br/>        command: scheduler<br/>        restart_policy:<br/>            condition: on-failure<br/>        depends_on:<br/>            - postgres<br/>        env_file:<br/>            - .env<br/>        volumes:<br/>            - ./dags:/opt/airflow/dags<br/>            - ./logs:/opt/airflow/logs<br/>    webserver:<br/>        image: apache/airflow<br/>        entrypoint: ./scripts/entrypoint.sh<br/>        restart_policy:<br/>            condition: on-failure<br/>        depends_on:<br/>            - postgres<br/>            - scheduler<br/>        env_file:<br/>            - .env<br/>        volumes:<br/>            - ./dags:/opt/airflow/dags<br/>            - ./logs:/opt/airflow/logs<br/>            - ./scripts:/opt/airflow/scripts<br/>        ports:<br/>            - "8080:8080"</span></pre></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><pre class="nb nc nd ne aw nf bi"><span id="dd13" class="ng mf it nc b gy nt nu nv nw nx ni l nj nk"><strong class="nc jd">#entrypoint.sh</strong></span><span id="d4ed" class="ng mf it nc b gy nl ni l nj nk">#!/usr/bin/env bash<br/>airflow initdb<br/>airflow webserver</span></pre></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><pre class="nb nc nd ne aw nf bi"><span id="3ec3" class="ng mf it nc b gy nt nu nv nw nx ni l nj nk"><strong class="nc jd">#.env</strong></span><span id="83b2" class="ng mf it nc b gy nl ni l nj nk">AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow<br/>AIRFLOW__CORE__EXECUTOR=LocalExecutor</span></pre></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><pre class="nb nc nd ne aw nf bi"><span id="2055" class="ng mf it nc b gy nt nu nv nw nx ni l nj nk"><strong class="nc jd">#dummy_dag.py</strong></span><span id="6f78" class="ng mf it nc b gy nl ni l nj nk">from airflow import DAG<br/>from airflow.operators.dummy_operator import DummyOperator<br/>from datetime import datetime</span><span id="33dd" class="ng mf it nc b gy nl ni l nj nk">with DAG('example_dag', start_date=datetime(2016, 1, 1)) as dag:<br/>    op = DummyOperator(task_id='op')</span></pre><p id="4286" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在根目录中定位并在终端中执行<strong class="lk jd">“docker-compose up”</strong>应该可以在<a class="ae lh" href="http://localhost:8080/" rel="noopener ugc nofollow" target="_blank"> localhost:8080 </a>上访问气流。下图显示了最终结果。</p><blockquote class="ny nz oa"><p id="b0ce" class="li lj ob lk b ll lm kd ln lo lp kg lq oc ls lt lu od lw lx ly oe ma mb mc md im bi translated">如果您遇到权限错误，请在所有子目录上运行“chmod -R 777”，例如“chmod -R 777 logs/”</p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/6e6c91ae28176f39a45d5f701e0a5006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iiImXcSmkKWeX9dFKL3kvg.jpeg"/></div></div></figure><h1 id="1e02" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">为了好奇的人...</h1><p id="4a91" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">按照 Leyman 的说法，<strong class="lk jd"> docker </strong>用于管理单个容器，而<strong class="lk jd"> docker-compose </strong>可用于管理多容器应用程序。它还将您在<strong class="lk jd"> docker run </strong>中输入的许多选项移动到<strong class="lk jd"> docker-compose.yml </strong>文件中，以便于重用。它在 docker 使用的相同 docker API 之上作为前端“脚本”工作。您可以使用 docker 命令和大量 shell 脚本完成 docker-compose 所做的一切。</p><p id="0bf8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在运行我们的多容器 docker 应用程序之前，必须配置<strong class="lk jd"> docker-compose.yml </strong>。有了这个文件，我们定义了将在<strong class="lk jd"> docker-compose up </strong>上运行的服务。</p><p id="09f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">docker-compose.yml 的第一个属性是<strong class="lk jd">版本，</strong>是合成文件格式版本。有关文件格式和所有配置选项的最新版本，请单击此处的<a class="ae lh" href="https://docs.docker.com/compose/compose-file/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="48fb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第二个属性是<strong class="lk jd">服务</strong>，服务下一层的所有属性表示我们的多容器应用程序中使用的容器。这些是<strong class="lk jd"> postgres、调度器</strong>和<strong class="lk jd">web 服务器。</strong>每个容器都有<strong class="lk jd">图像</strong>属性，该属性指向用于该服务的基本图像。</p><p id="3f38" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于每个服务，我们定义服务容器内部使用的环境变量。对于 postgres，它是由<strong class="lk jd">环境</strong>属性定义的，但是对于 scheduler 和 webserver，它是由<strong class="lk jd">定义的。env </strong>文件。因为<strong class="lk jd">。env </strong>是一个外部文件，我们必须用<strong class="lk jd"> env_file </strong>属性指向它。</p><p id="bdcc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过打开<strong class="lk jd">。env </strong>文件我们可以看到定义了两个变量。一个定义将要使用的执行器，另一个表示连接字符串。每个连接字符串必须以下列方式定义:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="46b4" class="ng mf it nc b gy nh ni l nj nk">dialect+driver://username:password@host:port/database</span></pre><p id="36a3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">方言名称包括 SQLAlchemy 方言的识别名称，如<code class="fe og oh oi nc b">sqlite</code>、<code class="fe og oh oi nc b">mysql</code>、<code class="fe og oh oi nc b">postgresql</code>、<code class="fe og oh oi nc b">oracle</code>或<code class="fe og oh oi nc b">mssql</code>。Driver 是用于连接数据库的 DBAPI 的名称，全部使用小写字母。在我们的例子中，连接字符串由以下内容定义:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="aa06" class="ng mf it nc b gy nh ni l nj nk">postgresql+psycopg2://airflow:airflow@postgres/airflow</span></pre><p id="eeab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在主机部分后省略 port 表示我们将使用在自己的 docker 文件中定义的默认 postgres 端口。</p><p id="2e62" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">每个服务都可以定义<strong class="lk jd">命令</strong>，该命令将在 Docker 容器中运行。如果一个服务需要执行多个命令，可以通过定义一个可选的<strong class="lk jd">来完成。sh </strong>文件并用<strong class="lk jd"> entrypoint </strong>属性指向它。在我们的例子中，在<strong class="lk jd">脚本</strong>文件夹中有<strong class="lk jd"> entrypoint.sh </strong>，一旦执行，就会运行<strong class="lk jd"> airflow initdb </strong>和<strong class="lk jd"> airflow webserver </strong>。两者都是气流正常运行的必要条件。</p><p id="d34e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">定义<strong class="lk jd">依赖于</strong>属性，我们可以表达服务之间的依赖关系。在我们的示例中，webserver 仅在 scheduler 和 postgres 都已启动时启动，而且 scheduler 仅在 postgres 启动后启动。</p><p id="3077" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们的容器崩溃，我们可以通过<strong class="lk jd"> restart_policy </strong>重启它。restart_policy 配置容器退出时是否以及如何重新启动容器。其他选项包括条件、延迟、最大尝试次数和窗口。</p><p id="1bef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦服务运行，它就在容器定义的端口上被服务。要访问该服务，我们需要将容器端口暴露给主机端口。这是由<strong class="lk jd">端口</strong>属性完成的。在我们的例子中，我们将容器的端口<strong class="lk jd"> 8080 </strong>暴露给主机的<strong class="lk jd"> 127.0.0.1 (localhost) </strong>上的 TCP 端口<strong class="lk jd"> 8080 </strong>。<code class="fe og oh oi nc b">:</code>左侧定义主机端口，右侧定义容器端口。</p><p id="2472" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，<strong class="lk jd">卷</strong>属性定义了主机文件系统和 docker 容器之间的共享卷(目录)。因为 airflows 的默认工作目录是<em class="ob"> /opt/airflow/ </em>我们需要将我们指定的卷从根文件夹指向 airflow containers 工作目录。这是通过以下命令完成的:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="3e45" class="ng mf it nc b gy nh ni l nj nk"><strong class="nc jd">#general case for airflow</strong></span><span id="92bc" class="ng mf it nc b gy nl ni l nj nk">- ./&lt;our-root-subdir&gt;:/opt/airflow/&lt;our-root-subdir&gt;</span><span id="5f89" class="ng mf it nc b gy nl ni l nj nk"><strong class="nc jd">#our case</strong></span><span id="731d" class="ng mf it nc b gy nl ni l nj nk">- ./dags:/opt/airflow/dags<br/>- ./logs:/opt/airflow/logs<br/>- ./scripts:/opt/airflow/scripts<br/>           ...</span></pre><p id="cb80" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这样，当调度程序或 web 服务器将日志写入其<strong class="lk jd"> logs </strong>目录时，我们可以从<strong class="lk jd"> logs </strong>目录中的文件系统访问它。当我们向 dags 文件夹添加新的 dag 时，它将自动添加到容器 dag 包中，依此类推。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="c6ac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">今天到此为止，谢谢你阅读这个故事，我会很快发布更多。如果你注意到任何错误，请让我知道。</p><p id="39de" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">编辑</strong>:“重新启动 _ 策略:”前需要“部署:”根据意见</p></div></div>    
</body>
</html>