<html>
<head>
<title>ML for Anomaly Detection and Importance of Precision-Recall Curve</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">异常检测的 ML 和精确召回曲线的重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/forensic-analytics-application-of-machine-learning-to-anomaly-detection-ccd7bef58097?source=collection_archive---------39-----------------------#2020-06-14">https://towardsdatascience.com/forensic-analytics-application-of-machine-learning-to-anomaly-detection-ccd7bef58097?source=collection_archive---------39-----------------------#2020-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2bf3" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">使用机器学习的欺诈检测</h2><div class=""/><div class=""><h2 id="9301" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">处理不平衡数据集以检测欺诈概率</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3d61f867fd04e99ab256cb9cfc8b791c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E1mIZ8HnllzEeJhKwE-XXQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="9017" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae md" href="https://sarit-maitra.medium.com/membership" rel="noopener">https://sarit-maitra.medium.com/membership</a></p><p id="41be" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> F </span>在电子商务、医疗保健、支付和银行系统等许多不同领域都可以看到欺诈行为。财务欺诈对投资者、监管者、审计师和公众都有重要影响。数据挖掘在检测在线交易中的金融欺诈中起着重要的作用。然而，数据挖掘变得具有挑战性，因为正常和欺诈行为的特征不断变化，并且欺诈数据集高度扭曲。</p><p id="7864" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这里，我们有银行支付的数据集如下。显示交易欺诈性(1)还是有效(0)的目标变量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mn"><img src="../Images/067775899e829c91e29196e6ed4c35a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ckVg9V9b0ftLrfURDLtmew.png"/></div></div></figure><p id="8cf6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该数据集包含了有关数字金融欺诈风险上升的信息，强调了获取此类数据的难度。它对预测欺诈构成的技术挑战是 600 万行数据中正负类之间的高度不平衡分布。为了更好地理解金融交易数据集，让我们按特性统计不同值的数量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mo"><img src="../Images/72e739531adf5736eebddff25d91911f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QmInbDV4wldHaNrarbfBSw.png"/></div></div></figure><p id="f5c6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正如我们在下图和 outlier_fraction 中看到的，数据是不平衡的。</p><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="04c6" class="mu mv it mq b gy mw mx l my mz">ax = (df['fraud'].value_counts()*100.0 /len(df)).plot(kind='bar', stacked = True, rot = 0)<br/>ax.yaxis.set_major_formatter(mtick.PercentFormatter())<br/>ax.set_ylabel('Frequency Percentage')<br/>ax.set_xlabel('Class')<br/>ax.set_title('Frequency Percentage by Class')<br/>totals = []  </span><span id="2849" class="mu mv it mq b gy na mx l my mz">for i in ax.patches:<br/> totals.append(i.get_width())</span><span id="9132" class="mu mv it mq b gy na mx l my mz">total = sum(totals)  </span><span id="a3d1" class="mu mv it mq b gy na mx l my mz">for i in ax.patches:<br/> ax.text(i.get_x()+.15, i.get_height()-3.5, \<br/> str(round((i.get_height()/total), 1))+'%', color='black', weight = 'bold')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/928580afd0daf2c1f864ba43eaf1226f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*9QlYl7PfoSdvk3GTFU1HZQ.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/0f012982035bcd9800c177a5b366683c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*QvcV1sdO9k7m9botRqZxjg.png"/></div></figure><h2 id="26c0" class="mu mv it bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns iz bi translated">数据预处理:</h2><p id="d58f" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">只有一个唯一的邮政编码值，因此我们将删除它们。</p><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="5736" class="mu mv it mq b gy mw mx l my mz">print("Unique zipCodeOri values: ",df.zipcodeOri.nunique())<br/>print("Unique zipMerchant values: ",df.zipMerchant.nunique())</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/a1eeee77c48440208d05009db361d7ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*XKitNue1CtSz5sX7zRdZbw.png"/></div></figure><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="73d2" class="mu mv it mq b gy mw mx l my mz"># dropping zipcodeori and zipMerchant <br/>reducedDF = df.drop(['zipcodeOri','zipMerchant'],axis=1)</span><span id="a4cc" class="mu mv it mq b gy na mx l my mz"># changing object dtype to categorical for easing the transformation process<br/>categorical = reducedDF.select_dtypes(include= ['object']).columns<br/>for col in categorical:<br/>reducedDF[col] = reducedDF[col].astype('category')</span><span id="ca96" class="mu mv it mq b gy na mx l my mz"># categorical values to numeric values<br/>reducedDF[categorical] = reducedDF[categorical].apply(lambda x: x.cat.codes)<br/>reducedDF.head()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/95f1b968e913cd0994ee5a1b640c6f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*qsvr5WE0yUZ6oiErtk4P7g.png"/></div></figure><p id="d88c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">可以注意到，为了计算容易，这里避免了一位热编码；但是，通常最好将这些类别值转换为虚拟值，因为它们在大小上没有关系(即客户 1 不大于客户 2)。</p><p id="1402" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">创建特征矩阵 X 和标签数组 y</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/bbd4bc202ccb0b5ac09535605d3074f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*ZOXBZoU9q7JJkazAmCKSZQ.png"/></div></figure><p id="81fe" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们重新调整特征矩阵，使均值为零，标准差为一。</p><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="266f" class="mu mv it mq b gy mw mx l my mz">featuresToScale = X.columns<br/>sX = pp.StandardScaler(copy=True)<br/>X.loc[:,featuresToScale] = sX.fit_transform(X[featuresToScale])</span></pre><h2 id="46d0" class="mu mv it bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns iz bi translated">特征的相关性:</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/1e627592d098902df77a601e14c8db21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9f--YqD9me0dPk7U6dq4g.png"/></div></div></figure><h1 id="4cbe" class="oc mv it bd nd od oe of ng og oh oi nj ki oj kj nm kl ok km np ko ol kp ns om bi translated">模型准备:</h1><p id="62c5" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">现在数据准备好了，就来准备模型吧。我们需要将数据分为训练集和测试集，选择一个成本函数。</p><h2 id="e29d" class="mu mv it bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns iz bi translated">选择成本函数:</h2><p id="d3c0" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">在训练之前，我们需要将误差率的成本函数应用于算法。该算法将通过从训练示例中学习来尝试最小化该成本函数。我们将使用二元分类对数损失，这将计算真实标签和基于模型的预测之间的交叉熵。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/16ea21d67c8f4f2f72508ecb74192b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*J21aw5ztCldJ14CxA-P4ow.png"/></div></figure><p id="dbbb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里，<em class="oo"> n </em>是观察次数；<em class="oo"> m </em>是类标签的个数；<em class="oo"> log </em>是自然对数；如果观察值<em class="oo"> i </em>在类<em class="oo"> j </em>中则<em class="oo"> 1 </em>否则<em class="oo">0</em>1；并且是观测值<em class="oo"> i </em>在类<em class="oo"> j </em>中的预测概率。</p><p id="4c30" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该模型将为每笔交易生成欺诈概率。欺诈概率越接近真实标签，对数损失函数的值越低。ML 的目标是尽量减少测井损失。</p><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="bf98" class="mu mv it mq b gy mw mx l my mz">featuresToScale = X.columns<br/>sX = pp.StandardScaler(copy=True)<br/>X.loc[:,featuresToScale] = sX.fit_transform(X[featuresToScale])</span><span id="0a5d" class="mu mv it mq b gy na mx l my mz"># Define resampling method and split into train and test<br/>method = SMOTE(kind='borderline1')<br/>trainX, testX, trainY, testY = train_test_split(X,y, test_size = 0.2, random_state = 42, stratify=y)</span><span id="9037" class="mu mv it mq b gy na mx l my mz"># Apply resampling to the training data only<br/>X_resampled, y_resampled = method.fit_sample(trainX, np.ravel(trainY))</span></pre><h2 id="4d27" class="mu mv it bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns iz bi translated">安装灯 GBM:</h2><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="fd4a" class="mu mv it mq b gy mw mx l my mz">model = LGBMClassifier().fit(X_resampled, y_resampled)</span><span id="203c" class="mu mv it mq b gy na mx l my mz"># Get model performance metrics<br/>predicted = model.predict(testX)<br/>print(classification_report(np.ravel(testY), predicted))</span><span id="5f83" class="mu mv it mq b gy na mx l my mz">probabilities = model.fit(trainX,<br/>                          np.ravel(trainY)).predict_proba(testX)<br/>print('AUPRC = {}'.format(average_precision_score(testY, \<br/>probabilities[:, 1])))</span><span id="836d" class="mu mv it mq b gy na mx l my mz"># Probabilities for the positive outcome only<br/>lgb_probs = probabilities[:, 1]</span><span id="7ba1" class="mu mv it mq b gy na mx l my mz"># Precision-recall AUC<br/>precision, recall, _ = precision_recall_curve(np.ravel(testY), lgb_probs)<br/>auc_score = auc(recall, precision)<br/>print('LightGBM PR AUC: %.3f' % auc_score)</span><span id="1d5f" class="mu mv it mq b gy na mx l my mz">average_precision = average_precision_score(testY, lgb_probs)</span><span id="8b84" class="mu mv it mq b gy na mx l my mz">plt.figure(figsize=(10,6))<br/># calculate the no skill line as the proportion of the positive class<br/>no_skill = len(y[y==1]) / len(y)</span><span id="4746" class="mu mv it mq b gy na mx l my mz"># plot the no skill precision-recall curve<br/>plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')</span><span id="ebb1" class="mu mv it mq b gy na mx l my mz"># plot the model precision-recall curve<br/>plt.plot(recall, precision, marker='.', label='LightGBM')<br/>plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')</span><span id="c7f2" class="mu mv it mq b gy na mx l my mz"># axis labels<br/>plt.xlabel('Recall')<br/>plt.ylabel('Precision')</span><span id="51aa" class="mu mv it mq b gy na mx l my mz">plt.legend()<br/>plt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(average_precision))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/91d3de8538a1cab54fffffc5ce412938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RYMBKEFPVT3OwIXknXK7iQ.png"/></div></div></figure><h2 id="0b58" class="mu mv it bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns iz bi translated">灵敏度、特异性、精确度、F1 和 MCC:</h2><p id="3595" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">敏感性和特异性通常被认为是混淆矩阵的特征。</p><ul class=""><li id="8a20" class="oq or it lj b lk ll ln lo lq os lu ot ly ou mc ov ow ox oy bi translated">敏感度或召回率是指检测到的欺诈行为将产生阳性测试结果的可能性(真阳性率)。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/6b70cb0a0b351af3f1c7e4054a3a693b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8f6ucmYNIfx1WHh-uXySHg.png"/></div></div></figure><p id="e9dc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们在这里看到，公式不包含 FP 和 TN；因此，敏感性可能导致有偏差的结果，特别是对于我们的不平衡类用例。</p><p id="bfda" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，当分类器在 10 个交易中有 8 个报告肯定有可能欺诈时，灵敏度为 0.8 (80%)。它代表了我们的分类器检测欺诈的能力；低敏感度分类器不能识别许多欺诈交易，而高敏感度分类器在结果为否定时有助于排除交易。因此，我们看到，灵敏度是假阴性率的补充(即假阴性率加上灵敏度= 100%)。</p><ul class=""><li id="6657" class="oq or it lj b lk ll ln lo lq os lu ot ly ou mc ov ow ox oy bi translated">特异性是指欺诈性交易在非欺诈性交易中测试结果为阴性的可能性(真-负比率)。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/97fe659adc1929b4132021967436ab4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bcFt6s8fHyKPepPpCU480g.png"/></div></div></figure><p id="eb8f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，当分类器在 10 个无欺诈交易中的 8 个中报告否定时，特异性为 0.8 (80%)。特异性代表我们的分类器正确识别欺诈交易的程度，因为特异性高的分类器的假阳性率低。具有低特异性的分类器将许多真实的交易信号化为欺诈性的。它是假阳性率的补充。然而，这里我们也看到，公式不包含 FN 和 TP；对于不平衡的类，特异性也可能给我们有偏见的结果。</p><ul class=""><li id="9366" class="oq or it lj b lk ll ln lo lq os lu ot ly ou mc ov ow ox oy bi translated">精确度也称为阳性预测值。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/80c04bb4332a3e486e47147fa119d274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L5xtT9XMrKKTO1PEgNcbgw.png"/></div></div></figure><p id="7646" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里，公式也不包含 FN 和 TN，同样，精度可能会对我们的不平衡类给出有偏差的结果。它给出了所有预测欺诈中正确预测欺诈的百分比。</p><ul class=""><li id="b350" class="oq or it lj b lk ll ln lo lq os lu ot ly ou mc ov ow ox oy bi translated">F1 分数结合了召回率和精确度，与上述 3 个指标相比，呈现了一个更平衡的视图，但在该场景中可能会给出有偏差的结果，因为它不包括 TN。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/8fdabff464a549b9d7e4242353065ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*CoYpVbAcib7Ib5AfhwDEdA.png"/></div></figure><ul class=""><li id="63b0" class="oq or it lj b lk ll ln lo lq os lu ot ly ou mc ov ow ox oy bi translated">马修斯相关系数(MCC)在其公式中考虑了混淆矩阵的所有单元。然而，我们没有在这项工作中试验 MCC，尽管一些报告表明 MCC 易于解释，并且对预测目标的变化也是稳健的。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/fd94c7039e092b2bdef881abc481a885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k-wGJ_B9GZqGqm9_xlxVAQ.png"/></div></div></figure><p id="503c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们的案例研究中，欺诈占总案例的 1.2%。欺诈检测可能无法通过获得高准确率来实现。因此，我们将考虑其他性能指标，特别是灵敏度、精确召回曲线下面积(AURPC)和 F1 分数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/beb22c76f8d190425c03de6628cdab70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*3e3xU8p3Vv4b0cg-bbT5VQ.png"/></div></figure><p id="7fef" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">准确率代表两个类别中正确分类的观察值的百分比:</p><p id="b7da" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">百分比精度= (T P +T N / T N +F N +T P +F P) * 100</p><p id="0f95" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">灵敏度，即真阳性率(TPR)和回忆，代表了被正确分类为阳性的阳性的比例。这些参数至关重要，我们认为这些参数与精确度一起是一种性能指标。</p><p id="2210" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">灵敏度= T P / T P +F N</p><p id="1858" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，单独的灵敏度也有误导性，因为它允许忽略大量的假阳性。我们的目标是在这两个参数之间找到平衡。我们需要获得高欺诈检测率(灵敏度)，以及尽可能高的准确性。为了解决这个问题，我们考虑了像 AUPRC 和 F1 分数这样的权衡措施。</p><h2 id="6d98" class="mu mv it bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns iz bi translated">精确度-召回曲线:</h2><p id="4e27" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">对于我们不平衡的数据集，更好的评估结果的方法是使用精度和召回率。精度-召回曲线显示了精度(结果相关性的度量)和召回(返回多少相关结果的度量)之间的权衡。我们已经讨论了精确度、召回率和 F1 分数。</p><ul class=""><li id="124e" class="oq or it lj b lk ll ln lo lq os lu ot ly ou mc ov ow ox oy bi translated">高精度意味着，在我们所有的积极预测中，许多是真正的积极预测。</li><li id="e12c" class="oq or it lj b lk pf ln pg lq ph lu pi ly pj mc ov ow ox oy bi translated">高召回率意味着模型已经捕获了大部分的实际阳性(换句话说，它具有低的假阴性率)</li></ul><p id="a2f3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最优解需要高精度和高召回率。因此，我们在这里看到的是精度和召回之间的权衡。这通常由算法设置的阈值来确定，以将阳性病例与阴性病例分开。为了评估精度-召回曲线，我们需要计算平均精度，即在每个阈值达到的精度的加权平均值。平均精度越高，解决方案越好。</p><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="27c8" class="mu mv it mq b gy mw mx l my mz">print("Classification Report for LightGBM: \n", classification_report(testY, predicted))</span><span id="d97c" class="mu mv it mq b gy na mx l my mz">print("Confusion Matrix of LightGBM: \n", confusion_matrix(testY, predicted))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/18e478dbedc778d3c675b84332168812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*3j60c-22AWlYbeOkKzyXaA.png"/></div></figure><p id="52f7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">鉴于我们的金融交易数据集高度不平衡，使用混淆矩阵将没有意义。考虑到这种不平衡的阶级问题，混淆矩阵在捕捉这种次优结果方面表现不佳。</p><h2 id="b65a" class="mu mv it bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns iz bi translated">拟合逻辑回归模型:</h2><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="1579" class="mu mv it mq b gy mw mx l my mz"># fit Logistic Regression model<br/>logreg.fit(trainX, trainY)<br/>log_pred = logreg.predict_proba(testX)<br/>log_probs = y_pred[:, 1]</span><span id="b72d" class="mu mv it mq b gy na mx l my mz"># calculate the precision-recall auc<br/>precision, recall, _ = precision_recall_curve(testY, log_probs)<br/>auc_score = auc(recall, precision)<br/>print('LogReg PRAUC: %.3f' % auc_score)</span><span id="33cc" class="mu mv it mq b gy na mx l my mz">log_probabilities = logreg.fit(trainX, trainY).predict_proba(testX)<br/>avg_prec = average_precision_score(testY, log_probabilities[:, 1])</span><span id="1c2c" class="mu mv it mq b gy na mx l my mz">plt.figure(figsize=(10,6))<br/># calculate the no skill line as the proportion of the positive class<br/>no_skill = len(y[y==1]) / len(y)<br/># plot the no skill precision-recall curve<br/>plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')<br/># plot the model precision-recall curve<br/>plt.plot(recall, precision, marker='.', label='LogisticRegression')<br/>plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')</span><span id="55cf" class="mu mv it mq b gy na mx l my mz">plt.xlabel('Recall')<br/>plt.ylabel('Precision')<br/>plt.legend()<br/>plt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(avg_prec))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/987aab7fefb013cd74c046d636830872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EwnzuGszdPdZgCOihP9nyg.png"/></div></div></figure><h2 id="d035" class="mu mv it bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns iz bi translated">LightGBM 和 Logistic 回归的集成</h2><p id="170f" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">堆叠概括提供了一种利用异质分类方法集合的集体辨别能力的机制。这包括使用顶级分类模型，该模型能够从基础级模型的预测(和分类偏差)中学习，以实现更大的分类能力。考虑到 XGBoost 和 LightGBM 来自同一个家族，我们将选择 LightGBM 和 Logistic 回归模型用于集成方法。</p><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="cfae" class="mu mv it mq b gy mw mx l my mz">kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=42)</span><span id="120d" class="mu mv it mq b gy na mx l my mz"># create the sub models<br/>estimators = []<br/>model1 = LogisticRegression()<br/>estimators.append(('logistic', model1))</span><span id="1ca5" class="mu mv it mq b gy na mx l my mz">model2 = LGBMClassifier()<br/>estimators.append(('lgb', model2))</span><span id="3f56" class="mu mv it mq b gy na mx l my mz"># create the ensemble model<br/>ensemble = VotingClassifier(estimators)<br/>= model_selection.cross_val_score(ensemble, trainX, trainY, cv=kfold)</span><span id="f56e" class="mu mv it mq b gy na mx l my mz">print(results.mean())</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/537cbeed127ae48771bbba0faa4aa048.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5LvRQ0bDoRtdMowmGYCSzA.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pn"><img src="../Images/2f8c91a18b0507eec2836bfb727de822.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mP---zq2hGdVniawODajdg.png"/></div></div></figure><p id="4dae" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这种性能比逻辑回归好不了多少，但远不如前两种模型。因此，我们将拒绝这一点。</p><p id="0bc6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用无监督的 LightGBM 模型，我们实现了 0.90 的平均精度。我们将尝试安装自动编码器，以检查无监督模型是否带来更好的准确性。</p><h2 id="c49a" class="mu mv it bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns iz bi translated">自动编码器:</h2><p id="6ce9" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">这里，我们将通过对欺诈案例的数量进行过采样来调整我们的训练集。我们希望将更多欺诈案例添加到我们的数据集中，以便我们训练的自动编码器能够更容易地将有效交易与欺诈交易分开。</p><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="d272" class="mu mv it mq b gy mw mx l my mz">oversample_multiplier = 100<br/>trainX_original = trainX.copy()<br/>trainY_original = trainY.copy()<br/>testX_original = testX.copy()<br/>testY_original = testY.copy()</span><span id="7426" class="mu mv it mq b gy na mx l my mz">trainX_oversampled = trainX.copy()<br/>trainY_oversampled = trainY.copy()</span><span id="c393" class="mu mv it mq b gy na mx l my mz">trainX_oversampled = trainX_oversampled.append( \[trainX_oversampled[trainY==1]]*oversample_multiplier, ignore_index=False)<br/>trainY_oversampled = trainY_oversampled.append( \[trainY_oversampled[trainY==1]]*oversample_multiplier, ignore_index=False)</span><span id="827c" class="mu mv it mq b gy na mx l my mz">trainX = trainX_oversampled.copy()<br/>trainY = trainY_oversampled.copy()</span><span id="f324" class="mu mv it mq b gy na mx l my mz">model = Sequential()<br/>model.add(Dense(units=20, activation='linear', <br/>activity_regularizer=regularizers.l1(10e-5), input_dim=7,name='hidden_layer'))</span><span id="3717" class="mu mv it mq b gy na mx l my mz">model.add(Dropout(0.02))<br/>model.add(Dense(units=1, activation='linear'))<br/>model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])</span><span id="08f4" class="mu mv it mq b gy na mx l my mz">num_epochs = 5<br/>batch_size = 32</span><span id="1d87" class="mu mv it mq b gy na mx l my mz">history = model.fit(x=trainX, y=trainY,epochs=num_epochs,<br/>batch_size=batch_size,shuffle=True,validation_split=0.20,verbose=1)<br/>predictions = model.predict(testX, verbose=1)<br/>anomalyScoresAE = anomalyScores(testX, predictions)<br/>preds, average_precision = plotResults(testY, anomalyScoresAE, True)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi po"><img src="../Images/a9247f786a2f9e27c8707d0b12fd022a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EsgBQwA3P43q_Qw83kzZXw.png"/></div></div></figure><p id="56a5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基于精度-召回曲线的测试的平均精度是 0.40，这是数据的最差表示。我们可以尝试将监督和非监督结合起来，创建一个半监督模型来检查性能。然而，由于我们现有的数据是有标记的，无监督算法可能会带来更好的结果。</p><h2 id="813b" class="mu mv it bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns iz bi translated">精确召回曲线的阈值:</h2><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="aa89" class="mu mv it mq b gy mw mx l my mz">fscore = (2 * precision * recall) / (precision + recall)<br/>ix = argmax(fscore)<br/>print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))</span><span id="50e5" class="mu mv it mq b gy na mx l my mz">no_skill = len(testY[testY==1]) / len(testY)<br/>plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')<br/>plt.plot(recall, precision, marker='.', label='LightGBM')<br/>plt.scatter(recall[ix], precision[ix], s=(10*2)**2, marker='s', color='black', label='Best')</span><span id="7949" class="mu mv it mq b gy na mx l my mz">plt.xlabel('Recall')<br/>plt.ylabel('Precision')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/bde28d37c084e63d1bff4502ce2812f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*Om94-mL_u0bf8PzLIVeFaw.png"/></div></figure><p id="d51c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">可以进一步调整该图以获得最佳性能；然而，在这里，最佳 F1 分数被标注为阈值 0.74。</p><h1 id="c768" class="oc mv it bd nd od oe of ng og oh oi nj ki oj kj nm kl ok km np ko ol kp ns om bi translated">生产管道:</h1><p id="bcc4" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">一旦我们确定了用于生产的模型，让我们设计一个简单的管道，对新的传入数据执行三个简单的步骤:加载数据，缩放特征，并使用我们已经训练并选择用于生产的 LightGBM 模型生成预测:</p><pre class="ks kt ku kv gt mp mq mr ms aw mt bi"><span id="b405" class="mu mv it mq b gy mw mx l my mz"><em class="oo"># Pipeline for New Data</em></span><span id="5aa6" class="mu mv it mq b gy na mx l my mz"># first, import new data into a data frame called ‘newData’<br/># Scale all the variables to a range of 0 to 1<br/>newData.loc[:, featuresToScale] = sX.transform(newData[featuresToScale])</span><span id="8cc6" class="mu mv it mq b gy na mx l my mz"># third, predict using LightGBM<br/>lgb.predict(newData, num_iteration=lgb.best_iteration)</span></pre><h1 id="f601" class="oc mv it bd nd od oe of ng og oh oi nj ki oj kj nm kl ok km np ko ol kp ns om bi translated">关键要点</h1><p id="1325" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">一个欺诈检测系统应该具有下面提到的一些属性，以使现实世界的商业意义。</p><ol class=""><li id="00b6" class="oq or it lj b lk ll ln lo lq os lu ot ly ou mc pq ow ox oy bi translated">该系统应该能够处理偏斜分布，因为所有交易中只有一小部分是欺诈性的。为了克服这一点，需要将训练集分成分布不那么偏斜的部分。</li><li id="0f3a" class="oq or it lj b lk pf ln pg lq ph lu pi ly pj mc pq ow ox oy bi translated">系统应该能够处理噪声，即数据中的错误。无论训练有多广泛，数据中的噪声都会限制归纳的准确性。</li><li id="f363" class="oq or it lj b lk pf ln pg lq ph lu pi ly pj mc pq ow ox oy bi translated">如果使用重采样技术，应该小心使用；它们不应该作为独立的解决方案使用，而必须与问题的返工相结合，以达到特定的目标</li><li id="da2f" class="oq or it lj b lk pf ln pg lq ph lu pi ly pj mc pq ow ox oy bi translated">系统应该能够适应新类型数据。欺诈者总是不断改变他们的行为，因此，过一段时间后，成功的欺诈技术会降低效率。</li><li id="d16c" class="oq or it lj b lk pf ln pg lq ph lu pi ly pj mc pq ow ox oy bi translated">良好的度量对于检查分类器性能至关重要。偏斜分布的总体高分并不意味着系统能够捕捉所有欺诈交易。</li><li id="03e2" class="oq or it lj b lk pf ln pg lq ph lu pi ly pj mc pq ow ox oy bi translated">该系统还应考虑到检测欺诈交易的成本和阻止欺诈交易的相关成本。欺诈检测系统之上的决策层可能有助于降低成本。分类器和决策规则必须相对于最小化成本来设置。</li></ol><h1 id="ed5b" class="oc mv it bd nd od oe of ng og oh oi nj ki oj kj nm kl ok km np ko ol kp ns om bi translated">结论:</h1><p id="d60d" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">随着时间的推移，解决方案将需要重新培训，因为欺诈的模式总是在变化。此外，我们可以试验其他 ML 算法，它们的性能与梯度提升一样好，并且可以将它们包括在集成中，以提高整体欺诈检测性能。通过足够的迭代，通常有可能找到一个适当的机器学习模型，在偏差与方差以及精度与召回之间取得正确的平衡。此外，可解释性对于机器学习的现实应用非常重要。我们将为下一次讨论保留可解释性。</p><p id="d29d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果不进行全面调查，永远无法确认财务欺诈，欺诈检测方法的进步可能会发出危险信号，警告利益相关方欺诈的可能性。</p><p id="7fdc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">我可以到达</strong> <a class="ae md" href="https://www.linkedin.com/in/saritmaitra/" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd"> <em class="oo">这里</em> </strong> </a> <strong class="lj jd">。</strong></p></div></div>    
</body>
</html>