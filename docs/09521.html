<html>
<head>
<title>Case Study: Breast Cancer Classification Using a Support Vector Machine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">案例研究:使用支持向量机的乳腺癌分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/case-study-breast-cancer-classification-svm-2b67d668bbb7?source=collection_archive---------18-----------------------#2020-07-07">https://towardsdatascience.com/case-study-breast-cancer-classification-svm-2b67d668bbb7?source=collection_archive---------18-----------------------#2020-07-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="38c8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">创建一个模型，根据肿瘤特征预测患者是否患有乳腺癌</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4f4db42c5202b6797322e65c3eeada04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ak-v8ir2wOd0Z33V"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@peterboccia?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">彼得·波恰</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="dfa0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我们将创建一个模型，根据几个肿瘤特征来预测患者是否有阳性乳腺癌诊断。</p><h1 id="6a95" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">问题陈述</strong></h1><p id="9ab3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">乳腺癌数据库是来自 UCI 机器学习知识库的公开可用的数据集。它提供了肿瘤特征的信息，如肿瘤的大小、密度和质地。</p><p id="3fe4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">目标:</strong>创建一个分类模型，根据几个特征预测癌症诊断是良性还是恶性。</p><p id="1d5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">使用的数据:</strong><a class="ae kv" href="https://www.kaggle.com/merishnasuwal/breast-cancer-prediction-dataset?select=Breast_cancer_data.csv" rel="noopener ugc nofollow" target="_blank">Kaggle-乳腺癌预测数据集</a></p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="3762" class="ls lt iq bd lu lv mw lx ly lz mx mb mc jw my jx me jz mz ka mg kc na kd mi mj bi translated"><strong class="ak">第一步:探索数据集</strong></h1><p id="280f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">首先，让我们了解我们的数据集:</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="2de1" class="ng lt iq nc b gy nh ni l nj nk"><strong class="nc ir">#import required libraries<br/></strong>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import seaborn as sns</span><span id="75fd" class="ng lt iq nc b gy nl ni l nj nk"><strong class="nc ir">#import models from scikit learn module:<br/></strong>from sklearn.model_selection import train_test_split<br/>from sklearn import metrics<br/>from sklearn.svm import SVC</span><span id="3eb0" class="ng lt iq nc b gy nl ni l nj nk"><strong class="nc ir">#import Data</strong><br/>df_cancer = pd.read_csv('Breast_cancer_data.csv')<br/>df_cancer.head()</span><span id="44c8" class="ng lt iq nc b gy nl ni l nj nk"><strong class="nc ir">#get some information about our Data-Set<br/></strong>df_cancer.info()<br/>df_cancer.describe()</span><span id="1a99" class="ng lt iq nc b gy nl ni l nj nk"><strong class="nc ir">#visualizing data<br/></strong>sns.pairplot(df_cancer, hue = 'diagnosis')</span><span id="9779" class="ng lt iq nc b gy nl ni l nj nk">plt.figure(figsize=(7,7))<br/>sns.heatmap(df_cancer['mean_radius mean_texture mean_perimeter mean_area mean_smoothness diagnosis'.split()].corr(), annot=True)</span><span id="cce3" class="ng lt iq nc b gy nl ni l nj nk">sns.scatterplot(x = 'mean_texture', y = 'mean_perimeter', hue = 'diagnosis', data = df_cancer)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/ccb076d4c86387741f1e821e8b8c7b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*RXPjxj7rXRjIU3EM5Q5TWg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乳腺癌数据</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/d81640a1ed5910adeaec3b34ff0eac92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J9UzHF9r9YVWl2939ONbAg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">关于数据集的一些信息</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/76ad1b87d441bdfa3104fa94cf43b151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*3aOVvRQEK_1WJyep4SfnPQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Data.describe()</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/f5dc45e43ac1e7656a846c0526517698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C_SQQKYHUzXYmVv-ZSXWYw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乳腺癌数据的特征对图</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/674ea7e2b19b0760dfd5d1f2519b2003.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*GF0yyABUdprBKaazezTLag.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">特征之间的相关性</p></figure><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="6085" class="ng lt iq nc b gy nh ni l nj nk"><strong class="nc ir">#visualizing features correlation<br/></strong>palette ={0 : 'orange', 1 : 'blue'}<br/>edgecolor = 'grey'</span><span id="e4b7" class="ng lt iq nc b gy nl ni l nj nk">fig = plt.figure(figsize=(12,12))<br/>plt.subplot(221)</span><span id="8e68" class="ng lt iq nc b gy nl ni l nj nk">ax1 = sns.scatterplot(x = df_cancer['mean_radius'], y = df_cancer['mean_texture'], hue = "diagnosis",<br/>data = df_cancer, palette =palette, edgecolor=edgecolor)<br/>plt.title('mean_radius vs mean_texture')</span><span id="c595" class="ng lt iq nc b gy nl ni l nj nk">plt.subplot(222)<br/>ax2 = sns.scatterplot(x = df_cancer['mean_radius'], y = df_cancer['mean_perimeter'], hue = "diagnosis",<br/>data = df_cancer, palette =palette, edgecolor=edgecolor)<br/>plt.title('mean_radius vs mean_perimeter')</span><span id="613e" class="ng lt iq nc b gy nl ni l nj nk">plt.subplot(223)<br/>ax3 = sns.scatterplot(x = df_cancer['mean_radius'], y = df_cancer['mean_area'], hue = "diagnosis",<br/>data = df_cancer, palette =palette, edgecolor=edgecolor)<br/>plt.title('mean_radius vs mean_area')</span><span id="704f" class="ng lt iq nc b gy nl ni l nj nk">plt.subplot(224)<br/>ax4 = sns.scatterplot(x = df_cancer['mean_radius'], y = df_cancer['mean_smoothness'], hue = "diagnosis",<br/>data = df_cancer, palette =palette, edgecolor=edgecolor)<br/>plt.title('mean_radius vs mean_smoothness')</span><span id="95f3" class="ng lt iq nc b gy nl ni l nj nk">fig.suptitle('Features Correlation', fontsize = 20)<br/>plt.savefig('2')<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/d7eda3ac430dac6cb3a2818d43149431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zr_3kWacbZI7FEEozMwA_Q.png"/></div></div></figure></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="8dbd" class="ls lt iq bd lu lv mw lx ly lz mx mb mc jw my jx me jz mz ka mg kc na kd mi mj bi translated"><strong class="ak">第二步:缺失/分类数据的处理</strong></h1><ul class=""><li id="aa3d" class="ns nt iq ky b kz mk lc ml lf nu lj nv ln nw lr nx ny nz oa bi translated">在应用任何方法之前，我们需要检查是否有任何值丢失，如果有，就处理它们。在这个数据集中，没有丢失的值，但是始终保持检查数据集中的空值的习惯！</li><li id="f519" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated">由于机器学习模型基于数学方程，我们需要对分类变量进行编码。这里我使用了<strong class="ky ir">标签编码</strong>，因为我们在“诊断”列中有两个不同的值:</li></ul><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="35e1" class="ng lt iq nc b gy nh ni l nj nk"><strong class="nc ir">#check how many values are missing (NaN)<br/></strong>here we do not have any missing values<br/>df_cancer.isnull().sum()</span><span id="77b7" class="ng lt iq nc b gy nl ni l nj nk"><strong class="nc ir">#handling categorical data<br/></strong>df_cancer['diagnosis'].unique()</span><span id="6590" class="ng lt iq nc b gy nl ni l nj nk">df_cancer['diagnosis'] = df_cancer['diagnosis'].map({'benign':0,'malignant':1})</span><span id="f07e" class="ng lt iq nc b gy nl ni l nj nk">df_cancer.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/875ba2455de783cbe73f4b2bb52cd9b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*d3u8L_4FQlzdjBLL8HOrqg.png"/></div></figure><p id="ddb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们继续攀升我们的数据集:</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="2a2c" class="ng lt iq nc b gy nh ni l nj nk"><strong class="nc ir">#visualizing diagnosis column &gt;&gt;&gt; 'benign':0,'malignant':1<br/></strong>sns.countplot(x='diagnosis',data = df_cancer)</span><span id="19ff" class="ng lt iq nc b gy nl ni l nj nk">plt.title('number of Benign_0 vs Malignan_1')</span><span id="261c" class="ng lt iq nc b gy nl ni l nj nk"><strong class="nc ir"># correlation between features<br/></strong>df_cancer.corr()['diagnosis'][:-1].sort_values().plot(kind ='bar')<br/>plt.title('Corr. between features and target')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/e2776f6ff3fa796eb2352fba98aea8e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fPjQEeJwbaXD3Zeej3jaKg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">计数图—相关性</p></figure></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="6164" class="ls lt iq bd lu lv mw lx ly lz mx mb mc jw my jx me jz mz ka mg kc na kd mi mj bi translated"><strong class="ak"> <em class="oi">第三步:将数据集分割成训练集和测试集</em> </strong></h1><p id="5a41" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">数据分为<code class="fe oj ok ol nc b">Train</code>组和<code class="fe oj ok ol nc b">Test</code>组。我们使用<code class="fe oj ok ol nc b">Train</code>集合让算法学习数据的行为，然后在<code class="fe oj ok ol nc b">Test</code>集合上检查我们的模型的准确性。</p><ul class=""><li id="d4e1" class="ns nt iq ky b kz la lc ld lf om lj on ln oo lr nx ny nz oa bi translated">特性(<code class="fe oj ok ol nc b">X</code>):插入到我们的模型中的列将用于进行预测。</li><li id="b423" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated">预测(<code class="fe oj ok ol nc b">y</code>):特征预测的目标变量。</li></ul><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="3519" class="ng lt iq nc b gy nh ni l nj nk"><strong class="nc ir">#define X variables and our target(y)<br/></strong>X = df_cancer.drop(['diagnosis'],axis=1).values<br/>y = df_cancer['diagnosis'].values</span><span id="3f38" class="ng lt iq nc b gy nl ni l nj nk"><strong class="nc ir">#split Train and Test<br/></strong>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)</span></pre><h1 id="8baf" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak"> <em class="oi">第四步:数据建模——支持向量机</em> </strong></h1><p id="ceab" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><strong class="ky ir">支持向量机(SVM) </strong>是最有用的监督 ML 算法之一。它可用于分类和回归任务。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/b5d64724ab1679446eebe9a32ec8ac5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*7YhsGAxX3KPdgDSgAW8R7Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://www.researchgate.net/figure/Linear-support-vector-machine-SVM-in-a-two-dimensional-example_fig2_328739480" rel="noopener ugc nofollow" target="_blank">研究之门/图</a></p></figure><p id="2f3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们首先需要理解几个概念:</p><ul class=""><li id="6c1e" class="ns nt iq ky b kz la lc ld lf om lj on ln oo lr nx ny nz oa bi translated"><strong class="ky ir">SVM 的工作是什么？SVM 选择类之间分离最大的超平面。</strong></li><li id="f6ab" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated"><strong class="ky ir">什么是硬边距和软边距？</strong>如果数据可以线性分离，SVM 可能会返回最大的准确性(硬利润)。当数据不是线性可分的时候，我们需要做的就是放宽界限，允许错误分类<strong class="ky ir"> ( </strong>软界限)。</li><li id="7548" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated"><strong class="ky ir">什么是超参数 C？</strong>可以使用 C 参数控制误分类错误的数量，C 参数对超平面有直接影响。</li><li id="66cd" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated"><strong class="ky ir">什么是超参数伽玛？</strong> Gamma 用于对接近支持向量的点进行加权。换句话说，改变 gamma 值会改变超平面的形状。</li><li id="07af" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated"><strong class="ky ir">什么是内核绝招？</strong>如果我们的数据不是线性可分的，我们可以应用“核技巧”方法，将非线性数据映射到更高维空间。</li></ul><p id="3893" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们回到我们的代码！</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="947c" class="ng lt iq nc b gy nh ni l nj nk"><strong class="nc ir">#Support Vector Classification model<br/></strong>from sklearn.svm import SVC<br/>svc_model = SVC()<br/>svc_model.fit(X_train, y_train)</span></pre></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="0ee5" class="ls lt iq bd lu lv mw lx ly lz mx mb mc jw my jx me jz mz ka mg kc na kd mi mj bi translated"><strong class="ak">第五步:模型评估</strong></h1><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="7222" class="ng lt iq nc b gy nh ni l nj nk">from sklearn.metrics import classification_report, confusion_matrix</span><span id="f498" class="ng lt iq nc b gy nl ni l nj nk">y_predict = svc_model.predict(X_test)<br/>cm = confusion_matrix(y_test, y_predict)<br/>sns.heatmap(cm, annot=True)</span></pre><p id="2968" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oj ok ol nc b">confusion_matrix</code>信息结果是什么意思？：</p><ul class=""><li id="950a" class="ns nt iq ky b kz la lc ld lf om lj on ln oo lr nx ny nz oa bi translated">在我们的测试集中有 143 名女性。</li><li id="1acb" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated">在 55 名预测不会患乳腺癌的女性中，有两名被归类为没有患，而实际上她们已经患了(第一类错误)。</li><li id="81ed" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated">在 88 名预测患有乳腺癌的女性中，14 名被归类为患有乳腺癌，而她们并没有(第二类错误)。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/0446d2b06bae4acbbfb3d3d2b0e319c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cu9ywK-npP_JEc7VRgy_Ag.png"/></div></div></figure><p id="af84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个分类报告结果意味着什么？基本上，这意味着 SVM 模型能够以 89%的准确率将肿瘤分为恶性和良性。</p><p id="ba19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意:</p><ul class=""><li id="f4c7" class="ns nt iq ky b kz la lc ld lf om lj on ln oo lr nx ny nz oa bi translated"><strong class="ky ir">精度</strong>是相关结果的分数。</li><li id="b528" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated"><strong class="ky ir"> Recall </strong>是所有相关结果中被正确分类的部分。</li><li id="6890" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated"><strong class="ky ir">F1-得分</strong>是精确度和召回率之间的调和平均值，范围在 0(糟糕)到 1(完美)之间。</li></ul></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="ead0" class="ls lt iq bd lu lv mw lx ly lz mx mb mc jw my jx me jz mz ka mg kc na kd mi mj bi translated">第六步:我们能做些什么来改进我们的模型？</h1><h2 id="a5bb" class="ng lt iq bd lu or os dn ly ot ou dp mc lf ov ow me lj ox oy mg ln oz pa mi pb bi translated"><strong class="ak"> 1。数据标准化</strong></h2><p id="f983" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">特征缩放将帮助我们从相同的镜头(相同的比例)看到所有的变量，这样我们将把所有的值带入范围[0，1]:</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="213f" class="ng lt iq nc b gy nh ni l nj nk"><strong class="nc ir">#normalized scaler - fit&amp;transform on train, fit only on test<br/></strong>from sklearn.preprocessing import MinMaxScaler<br/>n_scaler = MinMaxScaler()</span><span id="4bf4" class="ng lt iq nc b gy nl ni l nj nk">X_train_scaled = n_scaler.fit_transform(X_train.astype(np.float))<br/>X_test_scaled = n_scaler.transform(X_test.astype(np.float))</span><span id="d4f9" class="ng lt iq nc b gy nl ni l nj nk"><strong class="nc ir">#Support Vector Classification model -  apply on scaled data<br/></strong>from sklearn.svm import SVC<br/>svc_model = SVC()<br/>svc_model.fit(X_train_scaled, y_train)</span><span id="aea3" class="ng lt iq nc b gy nl ni l nj nk">from sklearn.metrics import classification_report, confusion_matrix<br/>y_predict_scaled = svc_model.predict(X_test_scaled)<br/>cm = confusion_matrix(y_test, y_predict_scaled)<br/>sns.heatmap(cm, annot=True)</span><span id="daa3" class="ng lt iq nc b gy nl ni l nj nk">print(classification_report(y_test, y_predict_scaled))</span></pre><p id="fa37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">混淆矩阵信息结果是什么意思？:</strong></p><ul class=""><li id="5d61" class="ns nt iq ky b kz la lc ld lf om lj on ln oo lr nx ny nz oa bi translated">我们的测试中有 143 名女性</li><li id="bcb0" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated">在 55 名预测未患乳腺癌的妇女中，4 名妇女被归类为未患，而实际上她们患了(1 型错误)</li><li id="a584" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated">在 88 名预测患有乳腺癌的妇女中，7 名被归类为患有乳腺癌，而她们并没有(2 型错误)</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/721bb8bad9a1193da931e67cfe3eb300.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hTAv7tP4EhZ50fNNXPO4TA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SVC 模型的结果-缩放数据集</p></figure><p id="f446" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">W <strong class="ky ir">这个分类报告结果是什么意思？</strong>基本上，这意味着 SVM 模型能够以 92%的准确率将肿瘤分为恶性/良性。</p><h2 id="bf38" class="ng lt iq bd lu or os dn ly ot ou dp mc lf ov ow me lj ox oy mg ln oz pa mi pb bi translated"><strong class="ak"> 2。SVM 参数优化</strong></h2><p id="2f05" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><strong class="ky ir"> C 参数</strong> —如我们所说，它控制着<code class="fe oj ok ol nc b">Train</code>数据上错误分类的成本。</p><ul class=""><li id="3c90" class="ns nt iq ky b kz la lc ld lf om lj on ln oo lr nx ny nz oa bi translated"><strong class="ky ir">较小的 C: </strong>较低的方差但较高的偏差(软裕度)并降低误分类的成本(较少的惩罚)。</li><li id="9877" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated"><strong class="ky ir">更大的 C: </strong>更低的偏倚和更高的方差(硬边际)以及增加误分类的成本(更严格)。</li></ul><p id="00b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">Gamma:<br/>Gamma 越小:</strong>方差越大，到达距离越远，解越一般化。<br/> <strong class="ky ir">更大的 Gamma: </strong>高方差低偏倚，接近到达，也更接近的数据点具有更高的权重。</p><p id="9a65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，让我们使用网格搜索找到模型的最佳参数:</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="27b4" class="ng lt iq nc b gy nh ni l nj nk"><strong class="nc ir">#find best hyper parameters<br/></strong>from sklearn.model_selection import GridSearchCV</span><span id="bb1e" class="ng lt iq nc b gy nl ni l nj nk">param_grid = {'C':[0.1,1,10,100,1000],'gamma':[1,0.1,0.01,0.001,0.001], 'kernel':['rbf']}</span><span id="368a" class="ng lt iq nc b gy nl ni l nj nk">grid = GridSearchCV(SVC(),param_grid,verbose = 4)<br/>grid.fit(X_train_scaled,y_train)</span><span id="15ad" class="ng lt iq nc b gy nl ni l nj nk">grid.best_params_<br/>grid.best_estimator_</span><span id="7099" class="ng lt iq nc b gy nl ni l nj nk">grid_predictions = grid.predict(X_test_scaled)<br/>cmG = confusion_matrix(y_test,grid_predictions)<br/>sns.heatmap(cmG, annot=True)</span><span id="0796" class="ng lt iq nc b gy nl ni l nj nk">print(classification_report(y_test,grid_predictions))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/ad7fe2c34bd84f1d3444a3439cd00b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*vgFlS9Qjrg-817a82FMK5A.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/5a6b2ece69e40eac13e6422f82d7f308.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*kBieDS_-HjUXNoptL5uS8A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用 SVC 模型的结果(缩放数据+最佳参数)</p></figure><p id="69a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，在这种情况下，最后一次模型改进并没有产生正确的百分比。然而，我们成功地减少了第二类错误。</p><p id="eb55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这有助于你更好地理解这个话题。欢迎任何反馈，因为它让我获得新的见解，并纠正任何错误！</p></div></div>    
</body>
</html>