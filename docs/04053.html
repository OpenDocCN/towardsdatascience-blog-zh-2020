<html>
<head>
<title>A Step-By-Step Introduction to PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对 PCA 的逐步介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-step-by-step-introduction-to-pca-c0d78e26a0dd?source=collection_archive---------4-----------------------#2020-04-14">https://towardsdatascience.com/a-step-by-step-introduction-to-pca-c0d78e26a0dd?source=collection_archive---------4-----------------------#2020-04-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="40d0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于如何使用 python 对数据集应用主成分分析的指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c61785dac750b8a99884f5ff68743a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J6OpQTzH0Rqu7a6kJcqalQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">鸢尾花。图片由 S. Hermann &amp; F. Richter 从<a class="ae kv" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2502898" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>拍摄。</p></figure><p id="5ae3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我将讲述如何解决对高维数据集(即每个样本都有大量测量值的数据集)进行可视化、分析和建模的问题。对于这种类型的数据集，很难确定要素之间的关系，也很难可视化它们之间的关系。将模型应用于高维数据集时，通常会导致过拟合，即不在训练集中的样本性能较差。</p><p id="3f95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我今天要讨论的方法是一种无监督的降维技术，称为主成分分析，简称 PCA。在这篇文章中，我将讨论执行 PCA 的步骤。我还将使用 python 演示数据集上的 PCA。你可以在这里找到完整的代码脚本<a class="ae kv" href="https://gist.github.com/conorposullivan/7b40c9a84605ed9447ba520a5695a4bd" rel="noopener ugc nofollow" target="_blank"/>。执行 PCA 的步骤如下:</p><ol class=""><li id="0542" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">将数据标准化。</li><li id="26e9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">从数据集中计算要素的协方差矩阵。</li><li id="ad38" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">对协方差矩阵执行特征分解。</li><li id="d71c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">根据相应特征值的大小，按降序对特征向量进行排序。</li><li id="1920" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">确定 k，即要选择的顶部主成分的数量。</li><li id="b001" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">从所选数量的顶部主成分构建投影矩阵。</li><li id="0802" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">计算新的 k 维特征空间。</li></ol><h1 id="3763" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">选择数据集</h1><p id="de8f" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">为了使用示例演示 PCA，我们必须首先选择一个数据集。我选择的数据集是 Fisher 收集的虹膜数据集。</p><p id="5f6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该数据集由来自三种不同类型鸢尾的 150 个样本组成:刚毛鸢尾、杂色鸢尾和海滨鸢尾。对于每个样本，数据集有四个测量值。这些测量值是萼片长度、萼片宽度、花瓣长度和花瓣宽度。为了访问这个数据集，我们将从 sklearn 库导入它:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="1f48" class="ni mh iq ne b gy nj nk l nl nm">from sklearn.datasets import load_iris</span></pre><p id="040c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在数据集已经导入，可以通过执行以下操作将其加载到数据框中:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="9dbb" class="ni mh iq ne b gy nj nk l nl nm">iris = load_iris()<br/>colors = ["blue", "red", "green"]<br/>df = DataFrame(<br/>    data=np.c_[iris["data"], iris["target"]], columns=iris["feature_names"] + ["target"]<br/>)</span></pre><p id="adb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然数据集已经加载，我们可以像这样显示一些样本:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/1de4abcac398fdc3eb0da0054ea51629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/0*Q3Dxkb02wR19mAMh"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="no">使用 df.sample 从数据集中选择一些样本(n=5)。</em></p></figure><p id="7f91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">箱线图是可视化数据分布的好方法。可以使用以下方法创建一组箱线图:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="bf23" class="ni mh iq ne b gy nj nk l nl nm">df.boxplot(by="target", layout=(2, 2), figsize=(10, 10))</span></pre><p id="f8f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这给出了:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/83fe24cf6364343276ec3d0311280222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RQPYegORErLiQaAj"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="no">箱线图显示每种类型虹膜的每次测量值的分布。</em></p></figure><p id="ce61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">箱线图向我们展示了许多细节，比如弗吉尼亚的花瓣长度中值最大。我们将在本文的后面回到这些箱线图。</p><h1 id="43b4" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">使数据标准化</h1><p id="2aa0" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">既然数据集已经加载，就必须为降维做准备。当所有特征都在同一尺度上时，大多数机器学习和优化算法的性能会更好。为了做到这一点，可以实施标准化方法。通过使用以下计算，特征值 xⁱ可以变成标准化的特征值 xⁱₛ:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/335170c2a17d92d4d3c4142928d2ec76.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*KZkZfPzDB5rr8yxijbTWaw.png"/></div></figure><p id="c401" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中，μₓ是特征列的平均值，σₓ是相应的样本方差。这导致特征值具有平均值 0 和标准偏差 1，因此具有与正态分布相同的参数。例如，对于具有从 0 到 5 的值的特征列，应用标准化将产生以下新值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/f1dda4efb1a9a556c72f200723c7a736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g8-DfStF0ju07B44pqW1kA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">值从 0 到 5 被标准化的特征列的示例。</p></figure><p id="5a4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就我们的数据集而言，虹膜特征的标准化可以使用 sklearn 实现，如下所示:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="dd75" class="ni mh iq ne b gy nj nk l nl nm">X = StandardScaler().fit_transform(X)</span></pre><h1 id="872f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">计算协方差矩阵</h1><p id="185c" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">协方差衡量两个要素之间的差异。正协方差表示要素同时增加和减少。反之，负协方差表示两个特征的变化方向相反。对于两个特征向量 xⱼ和 xₖ，它们之间的协方差σⱼₖ可以使用下面的等式来计算:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c24022f15f4fc7d60b35c57afe009e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*Bz3uj1iQjbDibXq1Vh2bmQ.png"/></div></figure><p id="00f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">协方差矩阵包含要素之间的协方差值，形状为 d × d。因此，对于我们的数据集，协方差矩阵应如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/0c6d2471e66c4f9078655f16d5f2f5b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*Mf4ir_RpeOBwZuF4iQKA_w.png"/></div></figure><p id="b1d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于特征列已被标准化，因此它们各自的均值为零，协方差矩阵σ可通过以下公式计算:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/7544d9916b27395fd055d7eec44c2fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*fvpr-iuLWLm7dkO0hkTOPg.png"/></div></figure><p id="6e5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中 Xᵗ是 x 的转置。如果你需要矩阵乘法如何工作的提示，<a class="ae kv" href="http://matrixmultiplication.xyz/" rel="noopener ugc nofollow" target="_blank">这里的</a>是一个很好的链接。</p><p id="544d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这可以用 python 实现，如下所示:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="1324" class="ni mh iq ne b gy nj nk l nl nm">cov = (X.T @ X) / (X.shape[0] - 1)</span></pre><h1 id="c01d" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">执行特征分解</h1><p id="970f" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">特征向量代表协方差矩阵的主要分量(最大方差的方向)。特征值是它们相应的大小。具有最大相应特征值的特征向量代表最大方差的方向。特征向量 v 满足以下条件:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/da81378d7827c2dcd87f90594efaddf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:130/format:webp/1*z-t17U2CDwjco1aGeiwyvw.png"/></div></figure><p id="6efb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中λ是一个标量，称为特征值。手动计算是相当复杂的，可能是一个帖子本身。然而，如果你想知道更多，我推荐你看看这个<a class="ae kv" href="https://www.youtube.com/watch?v=3-xfmbdzkqc" rel="noopener ugc nofollow" target="_blank">视频</a>。相反，我将使用 python 中的特征分解函数:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="16b4" class="ni mh iq ne b gy nj nk l nl nm">eig_values, eig_vectors = np.linalg.eig(cov)</span></pre><p id="00c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这给出了协方差矩阵的特征向量(主分量)和特征值。</p><h1 id="e750" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">确定选择哪些主成分</h1><p id="9e4e" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">既然已经计算了特征对，现在需要根据它们的特征值的大小对它们进行排序。这可以在 python 中通过执行以下操作来完成:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="4c34" class="ni mh iq ne b gy nj nk l nl nm">idx = np.argsort(eig_values, axis=0)[::-1]<br/>sorted_eig_vectors = eig_vectors[:, idx]</span></pre><p id="f9ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然已经根据主成分对应特征值的大小对主成分进行了排序，那么是时候决定选择多少主成分进行降维了。这可以通过绘制特征值的累积和来实现。累积和的计算方法如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/679c36383db34c87e85c37d0b731a0ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:126/format:webp/1*e3Ud73T1QZaock4MBb-lMw.png"/></div></figure><p id="8608" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述公式可以计算并绘制如下:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="6b3a" class="ni mh iq ne b gy nj nk l nl nm">cumsum = np.cumsum(eig_values[idx]) / np.sum(eig_values[idx])<br/>xint = range(1, len(cumsum) + 1)<br/>plt.plot(xint, cumsum)<br/><br/>plt.xlabel("Number of components")<br/>plt.ylabel("Cumulative explained variance")<br/>plt.xticks(xint)<br/>plt.xlim(1, 4, 1)</span></pre><p id="52ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该图显示了以下内容:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/11e443b8043cdcd32efdd4332f9da9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*uCZB-4MQfq_yCWTl"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示特征值累积和的图。</p></figure><p id="fcfc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从图中，我们可以看到，超过 95%的方差包含在两个最大的主成分中。因此，选择前两个最大的主成分构成投影矩阵 w 是可以接受的。</p><h1 id="dc19" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">计算转换</h1><p id="dd72" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">既然已经决定了有多少个主分量构成投影矩阵 W，则得分 Z 可以计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/4d4db8e011595653e26d441684171d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/1*5jIShpDaLqOfdbDOqpG03A.png"/></div></figure><p id="bf57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这可以在 python 中通过执行以下操作来计算:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="c5de" class="ni mh iq ne b gy nj nk l nl nm">eig_scores = np.dot(X, sorted_eig_vectors[:, :2])</span></pre><h1 id="da7b" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">绘制结果</h1><p id="6de6" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">现在数据集已经被投影到一个新的更低维度的子空间上，结果可以如下绘制:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="46c3" class="ni mh iq ne b gy nj nk l nl nm">def biplot(score, coeff, labels):<br/>    xs = score[:, 0]<br/>    ys = score[:, 1]<br/>    n = coeff.shape[0]<br/><br/>    for i, u in enumerate(iris["target_names"]):<br/>        xi = [<br/>            score[j, 0] for j in range(score.shape[0]) if df["target"].tolist()[j] == u<br/>        ]<br/>        yi = [<br/>            score[j, 1] for j in range(score.shape[0]) if df["target"].tolist()[j] == u<br/>        ]<br/>        plt.scatter(xi, yi, c=colors[i], label=u)<br/>    for i in range(n):<br/>        plt.arrow(<br/>            0, 0, coeff[i, 0], coeff[i, 1], color="r", head_width=0.05, head_length=0.1<br/>        )<br/>        plt.text(<br/>            coeff[i, 0] * 1.35,<br/>            coeff[i, 1] * 1.35,<br/>            labels[i],<br/>            color="g",<br/>            ha="center",<br/>            va="center",<br/>        )<br/><br/><br/>plt.xlabel("PC{}".format(1))<br/>plt.ylabel("PC{}".format(2))<br/>plt.grid()<br/><br/>biplot(scores, sorted_eig_vectors, iris["feature_names"])<br/>plt.legend()</span></pre><p id="9b2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这给出了:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/6184e6219ebb88209cbe7b950eec8094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rRu2dJpJReI3cILK"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">绘制在二维特征空间上的虹膜样本。</p></figure><p id="9cbc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从图中可以看出，versicolor 和 virignica 样品之间的距离较近，而 setosa 与它们之间的距离较远。如果你还记得上面的双标图，海滨锦鸡儿有最大的平均萼片长度、花瓣长度和花瓣宽度。然而，刚毛藻具有最高的平均萼片宽度。通过跟踪原始特征的轴可以看出这是正确的。</p><h1 id="6e63" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">使用奇异值分解(SVD)计算 X 的替代方法</h1><p id="b5d6" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">特征分解的一些缺点是计算量大，并且需要一个方阵作为输入。部分由于这些原因，寻找 PCA 的主成分的更流行的方法是使用奇异值分解(SVD)。SVD 将矩阵分解为满足以下条件的三个独立矩阵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/e695663c801089c4eb4b928f4e74e913.png" data-original-src="https://miro.medium.com/v2/resize:fit:174/format:webp/1*2h6144-oc97QJKvmI-0tBg.png"/></div></div></figure><p id="118c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中 U 是左奇异向量，V*是右奇异向量的复共轭，S 是奇异值。奇异值与从特征分解计算的特征值相关。SVD 的另一个有用特性是奇异值是数量级的，因此不需要重新排序。右奇异向量与通过特征分解得到的特征向量相同，因此 W=V。使用 python，矩阵的 SVD 可以如下计算:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="3959" class="ni mh iq ne b gy nj nk l nl nm">u, s, vh = np.linalg.svd(X)</span></pre><p id="1748" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由此，现在可以计算分数:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="720b" class="ni mh iq ne b gy nj nk l nl nm">svd_scores = np.dot(X, vh.T[:, :2])</span></pre><p id="f01d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从这些分数可以画出双标图，当使用特征分解时，将返回与上面相同的结果。查看<a class="ae kv" href="https://gist.github.com/conorposullivan/7b40c9a84605ed9447ba520a5695a4bd" rel="noopener ugc nofollow" target="_blank">代码</a>了解全部细节。</p><h1 id="1338" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">摘要</h1><p id="ea06" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在这篇文章中，我们讨论了 PCA，以及如何使用它来更清楚地了解数据集特征之间的关系，同时消除不必要的噪声。我们经历了每一步，也讨论了不同的计算方法。我希望这篇文章能对您未来的数据科学工作有所帮助。</p></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><p id="6751" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="oh">原载于 2020 年 4 月 25 日 datasciencesamurai.com</em><a class="ae kv" href="https://datasciencesamurai.com/step-by-step-principal-component-analysis-pca-python" rel="noopener ugc nofollow" target="_blank"><em class="oh"/></a><em class="oh">。</em></p><h2 id="c078" class="ni mh iq bd mi oi oj dn mm ok ol dp mq lf om on ms lj oo op mu ln oq or mw os bi translated"><a class="ae kv" href="https://datasciencesamurai.ck.page/77020f333b" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">加入我的邮件列表，了解更多关于数据科学的知识。</strong>T11】</a></h2></div></div>    
</body>
</html>