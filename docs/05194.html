<html>
<head>
<title>Getting Started with Feature Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-feature-selection-3ecfb4957fd4?source=collection_archive---------51-----------------------#2020-05-03">https://towardsdatascience.com/getting-started-with-feature-selection-3ecfb4957fd4?source=collection_archive---------51-----------------------#2020-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="55cd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">选择有用功能的初学者指南</h2></div><p id="f3c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你不会用你能做的俯卧撑的数量来决定巴士到达的时间吧？同样，在预测建模中，我们删除无用的特征，以降低最终模型的复杂性。简而言之，在开发预测模型时，特征选择减少了输入特征的数量。</p><p id="8f98" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我讨论了特性选择的三个主要类别；过滤方法、包装方法和嵌入方法。此外，我使用 Python 示例并利用一些框架，如用于机器学习的 scikit-learn(参见<a class="ae le" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank">文档</a>)、用于数据操作的 Pandas ( <a class="ae le" href="https://pandas.pydata.org/docs/" rel="noopener ugc nofollow" target="_blank">文档</a>)和用于交互式数据可视化的 Plotly ( <a class="ae le" href="https://plotly.com/python/" rel="noopener ugc nofollow" target="_blank">文档</a>)。要访问本文中使用的代码，请访问下面共享的链接中的 my Github。</p><div class="lf lg gp gr lh li"><a href="https://github.com/kurtispykes/demo" rel="noopener  ugc nofollow" target="_blank"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">kurtispykes/演示</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">与中等博客文章相关的演示代码。-路径/到/文件；链接到文章有效的数据可视化…</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">github.com</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw lx li"/></div></div></a></div><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/f465ca533f9f740a4557f9a5a281a774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FSs3m_uwBdJCn8Vx"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">图 1:<a class="ae le" href="https://unsplash.com/@zuizuii?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Zui Hoang</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的服装架照片</p></figure><p id="dcc6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">为什么要做特征选择？</strong></p><p id="bf3d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一段旨在传达的信息是，有时有些特征不能为预测最终结果提供足够有用的信息，因此通过将它包括在我们的模型中，我们正在使我们的模型变得不必要的复杂。丢弃无用的特征导致了简约的模型，这又导致了减少的评分时间。此外，特征选择也使得解释模型变得更加容易，这在所有的商业案例中都是非常普遍的。</p><blockquote class="mn"><p id="c248" class="mo mp it bd mq mr ms mt mu mv mw ld dk translated">“在大多数实际情况下，应用特征选择不太可能提供大的性能增益。但是，它仍然是功能工程师工具箱中的一个有价值的工具。”—(穆勒(Müller，(2016)，用 Python 介绍机器学习，奥瑞利传媒)</p></blockquote><p id="e7b6" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated"><strong class="kk iu">方法</strong></p><p id="724e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有各种方法可用于执行特征选择，它们属于 3 个类别之一。每种方法都有自己的优缺点。Guyon 和 Elisseeff (2003 年)对这些类别描述如下:</p><ul class=""><li id="6e1c" class="nc nd it kk b kl km ko kp kr ne kv nf kz ng ld nh ni nj nk bi translated"><strong class="kk iu">过滤方法</strong> -选择变量子集作为预处理步骤，独立于所选预测值。</li><li id="131d" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated"><strong class="kk iu">包装器方法</strong> -利用感兴趣的学习机作为黑盒，根据变量的预测能力对变量子集进行评分。</li><li id="e0cc" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated"><strong class="kk iu">嵌入式方法</strong>——在训练过程中进行变量选择，通常针对给定的学习机。</li></ul></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="c183" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">过滤方法</strong></p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nx"><img src="../Images/58ed420912be51d790a154d47b60f370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fjJfgmQHgcT3NOc2"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">图 2:由<a class="ae le" href="https://unsplash.com/@jtylernix?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">泰勒·尼克斯</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上过滤一张热饮照片</p></figure><p id="78f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">过滤方法使用单变量统计来评估每个输入要素与目标要素(目标变量/因变量)之间是否存在统计意义上的显著关系，这正是我们试图预测的。提供最高置信度的特征是我们为最终模型保留的特征，因此该方法独立于我们决定用于建模的选择模型。</p><blockquote class="mn"><p id="3e8b" class="mo mp it bd mq mr ms mt mu mv mw ld dk translated">“即使变量排序不是最佳的，它也可能比其他变量子集选择方法更可取，因为它的计算和统计可伸缩性。”——Guyon 和 Elisseeff (2003 年)</p></blockquote><p id="1558" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">过滤方法的一个例子是皮尔逊相关系数——你可能在高中的统计课上遇到过。这是一种统计数据，用于测量输入 X 要素和输出 Y 要素之间的线性相关量。范围从+1 到-1，其中 1 表示完全正相关，而-1 表示完全负相关，因此 0 表示没有线性相关。</p><p id="574b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要计算皮尔逊相关系数，请将输入要素 X 和输出要素 y 的协方差除以两个要素标准差的乘积，公式如图 3 所示。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ny"><img src="../Images/cd076b446a9cd7fcccd6255776bab546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YURqs7JPHRZwMr1ISuzt1Q.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">图 3:皮尔逊相关系数公式，其中 Cov 是协方差，σX 是 X 的标准差，σy 是 y 的标准差。</p></figure><blockquote class="nz oa ob"><p id="5d83" class="ki kj oc kk b kl km ju kn ko kp jx kq od ks kt ku oe kw kx ky of la lb lc ld im bi translated">对于下面的代码示例，如前所述，我使用 Scikit-Learn 框架中可用的波士顿房价—参见<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html" rel="noopener ugc nofollow" target="_blank">文档</a> —以及用于数据操作的 Pandas 参见<a class="ae le" href="https://pandas.pydata.org/docs/" rel="noopener ugc nofollow" target="_blank">文档</a>。</p></blockquote><pre class="lz ma mb mc gt og oh oi oj aw ok bi"><span id="92b4" class="ol om it oh b gy on oo l op oq">import pandas as pd<br/>from sklearn.datasets import load_boston</span><span id="7959" class="ol om it oh b gy or oo l op oq"># load data<br/>boston_bunch = load_boston()<br/>df = pd.DataFrame(data= boston_bunch.data,<br/>                  columns= boston_bunch.feature_names)</span><span id="db77" class="ol om it oh b gy or oo l op oq"># adding the target variable<br/>df["target"] = boston_bunch.target<br/>df.head() </span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi os"><img src="../Images/35c62288325f2c442148e09c94cef601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j6V3TFZQuoMMtNWK7WF-vg.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">图 4:上述代码单元的输出；显示波士顿房价数据集的预览。</p></figure><p id="22a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">python 中实现的要素选择的 pearson 相关系数示例。</p><pre class="lz ma mb mc gt og oh oi oj aw ok bi"><span id="53fb" class="ol om it oh b gy on oo l op oq"># Pearson correlation coefficient<br/>corr = df.corr()["target"].sort_values(ascending=False)[1:]</span><span id="ec2d" class="ol om it oh b gy or oo l op oq"># absolute for positive values<br/>abs_corr = abs(corr)</span><span id="707b" class="ol om it oh b gy or oo l op oq"># random threshold for features to keep<br/>relevant_features = abs_corr[abs_corr&gt;0.4]<br/>relevant_features</span><span id="1518" class="ol om it oh b gy or oo l op oq">&gt;&gt;&gt; RM         0.695360<br/>NOX        0.427321<br/>TAX        0.468536<br/>INDUS      0.483725<br/>PTRATIO    0.507787<br/>LSTAT      0.737663<br/>Name: target, dtype: float64</span></pre><p id="b8cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后简单地选择如下输入特征…</p><pre class="lz ma mb mc gt og oh oi oj aw ok bi"><span id="5b2f" class="ol om it oh b gy on oo l op oq">new_df = df[relevant_features.index]</span></pre><p id="8831" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="oc">优点</em> </strong></p><ul class=""><li id="4333" class="nc nd it kk b kl km ko kp kr ne kv nf kz ng ld nh ni nj nk bi translated">抗过拟合能力强(引入偏差)</li><li id="0e87" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated">比包装方法快得多</li></ul><p id="d8a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="oc">缺点</em> </strong></p><ul class=""><li id="4068" class="nc nd it kk b kl km ko kp kr ne kv nf kz ng ld nh ni nj nk bi translated">不考虑其他功能之间的相互作用</li><li id="beef" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated">不考虑正在使用的模型</li></ul></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="3447" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">包装方法</strong></p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ot"><img src="../Images/fd0a02e9f1753daa39492b13a6b0f309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bbF5HeycNCV0qNmg"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">图 4:包装一个盒子；照片由<a class="ae le" href="https://unsplash.com/@kadh?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Kira auf der Heide </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4c1e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">维基百科将包装器方法描述为使用“预测模型对特征子集进行评分”。每一个新的子集都用来训练一个模型，该模型在一个保留集上进行测试。计算在坚持的集合上犯的错误的数量(模型的错误率)给出了该子集的分数。— <a class="ae le" href="https://en.wikipedia.org/wiki/Feature_selection" rel="noopener ugc nofollow" target="_blank">包装器方法维基百科</a>。包装器方法所采用的算法被称为贪婪算法，因为它试图找到产生最佳性能模型的最佳特征组合。</p><blockquote class="mn"><p id="c4e2" class="mo mp it bd mq mr ms mt mu mv mw ld dk translated">“包装器特征选择方法创建具有输入特征的各种不同子集的许多模型，并根据一些性能度量来选择导致最佳性能模型的那些特征。”—杰森·布朗利</p></blockquote><p id="99a4" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">一种包装方法是递归特征消除(RFE ),正如该算法的名称所示，它的工作方式是递归地移除特征，然后使用剩余的特征建立一个模型，然后计算模型的精度。</p><p id="4c6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank">sci kit-learn 中 RFE 实施的文档</a>。</p><pre class="lz ma mb mc gt og oh oi oj aw ok bi"><span id="7638" class="ol om it oh b gy on oo l op oq">from sklearn.feature_selection import RFE<br/>from sklearn.linear_model import LinearRegression</span><span id="946f" class="ol om it oh b gy or oo l op oq"># input and output features<br/>X = df.drop("target", axis= 1)<br/>y = df["target"]</span><span id="0178" class="ol om it oh b gy or oo l op oq"># defining model to build<br/>lin_reg = LinearRegression()</span><span id="9d1a" class="ol om it oh b gy or oo l op oq"># create the RFE model and select 6 attributes<br/>rfe = RFE(lin_reg, 6)<br/>rfe.fit(X, y)</span><span id="8200" class="ol om it oh b gy or oo l op oq"># summarize the selection of the attributes<br/>print(f"Number of selected features: {rfe.n_features_}\n\<br/>Mask: {rfe.support_}\n\<br/>Selected Features:", [feature for feature, rank in zip(X.columns.values, rfe.ranking_) if rank==1],"\n\<br/>Estimator : {rfe.estimator_}")</span></pre><p id="40e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的打印语句返回…</p><pre class="lz ma mb mc gt og oh oi oj aw ok bi"><span id="a297" class="ol om it oh b gy on oo l op oq">Number of selected features: 6</span><span id="f129" class="ol om it oh b gy or oo l op oq">Mask: [False False False  True  True  True False  True False False  True False   True]</span><span id="b66f" class="ol om it oh b gy or oo l op oq">Selected Features: ['CHAS', 'NOX', 'RM', 'DIS', 'PTRATIO', 'LSTAT']  </span><span id="e0c3" class="ol om it oh b gy or oo l op oq">Estimator : {rfe.estimator_}</span></pre><p id="59eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="oc">优点</em> </strong></p><ul class=""><li id="0557" class="nc nd it kk b kl km ko kp kr ne kv nf kz ng ld nh ni nj nk bi translated">能够检测特征之间发生的相互作用</li><li id="a4ef" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated">通常比过滤方法产生更好的预测准确性</li><li id="04f2" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated">查找最佳特征子集</li></ul><p id="089f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="oc">缺点</em> </strong></p><ul class=""><li id="074a" class="nc nd it kk b kl km ko kp kr ne kv nf kz ng ld nh ni nj nk bi translated">计算成本高</li><li id="5653" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated">倾向于过度拟合</li></ul></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="a669" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">嵌入方法</strong></p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ou"><img src="../Images/010204f9eabbbf6a92dd2d026f08d0aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*n71bwEfKVUDc3suv"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">图 5:嵌入式组件；克里斯·里德在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="3199" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嵌入式方法类似于包装器方法，因为这种方法也优化预测模型的目标函数，但是将这两种方法分开的是，在嵌入式方法中，在学习建立模型期间使用了内在的度量。因此，嵌入式方法需要有监督的学习模型，该模型反过来将内在地确定每个特征对于预测目标特征的重要性。</p><blockquote class="nz oa ob"><p id="9e3a" class="ki kj oc kk b kl km ju kn ko kp jx kq od ks kt ku oe kw kx ky of la lb lc ld im bi translated">注意:用于特征选择的模型不一定是用作最终模型的模型。</p></blockquote><p id="7791" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LASSO ( <em class="oc">最小绝对收缩和选择操作符</em>)是嵌入方法的一个很好的例子。维基百科将 LASSO 描述为“一种回归分析方法，它执行变量选择和正则化，以提高其产生的统计模型的预测准确性和可解释性。”深入了解 Lasso 如何工作超出了本文的范围，但是可以在 Analytics Vidhya 博客上找到一篇关于该算法的好文章，作者是<a class="ov ow ep" href="https://medium.com/u/daf479ff8e76?source=post_page-----3ecfb4957fd4--------------------------------" rel="noopener" target="_blank"> Aarshay Jain </a>，标题是<a class="ae le" href="https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/" rel="noopener ugc nofollow" target="_blank">关于 Python 中脊和 LASSO 回归的完整教程</a>。</p><pre class="lz ma mb mc gt og oh oi oj aw ok bi"><span id="0abb" class="ol om it oh b gy on oo l op oq"># train model<br/>lasso = Lasso()<br/>lasso.fit(X, y)</span><span id="ca9b" class="ol om it oh b gy or oo l op oq"># perform feature selection<br/>kept_cols = [feature for feature, weight in zip(X.columns.values, lasso.coef_) if weight != 0]</span><span id="d0a3" class="ol om it oh b gy or oo l op oq">kept_cols</span></pre><p id="eb9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这将返回 Lasso 回归模型认为相关的列…</p><pre class="lz ma mb mc gt og oh oi oj aw ok bi"><span id="51ce" class="ol om it oh b gy on oo l op oq">['CRIM', 'ZN', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']</span></pre><p id="e58b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们也可以使用瀑布图来显示系数…</p><pre class="lz ma mb mc gt og oh oi oj aw ok bi"><span id="c4b1" class="ol om it oh b gy on oo l op oq">figt = go.Figure(<br/>         go.Waterfall(name= "Lasso Coefficients",<br/>                      orientation= "h",<br/>                      y = X.columns.values,<br/>                      x = lasso.coef_))</span><span id="dea2" class="ol om it oh b gy or oo l op oq">fig.update_layout(title = "Coefficients of Lasso Regression Model")</span><span id="b7b7" class="ol om it oh b gy or oo l op oq">fig.show()</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ox"><img src="../Images/2eeb6603d09e06c6b254080c51183397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t1yWfVVaC7oW78Xh7dvWVA.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">图 6:先前代码的输出；显示各功能系数的瀑布图；请注意，3 个特征被设置为 0，这意味着它们被模型忽略。</p></figure><p id="f927" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="oc">优点</em> </strong></p><ul class=""><li id="a4bd" class="nc nd it kk b kl km ko kp kr ne kv nf kz ng ld nh ni nj nk bi translated">计算速度比包装方法快得多</li><li id="55b8" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated">比过滤方法更准确</li><li id="ff1f" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated">一次考虑所有功能</li><li id="6772" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated">不容易过度合身</li></ul><p id="3c97" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点<em class="oc">缺点</em>缺点</strong></p><ul class=""><li id="a155" class="nc nd it kk b kl km ko kp kr ne kv nf kz ng ld nh ni nj nk bi translated">选择特定于模型的特征</li><li id="8f66" class="nc nd it kk b kl nl ko nm kr nn kv no kz np ld nh ni nj nk bi translated">不如包装方法强大</li></ul></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><blockquote class="nz oa ob"><p id="8dd5" class="ki kj oc kk b kl km ju kn ko kp jx kq od ks kt ku oe kw kx ky of la lb lc ld im bi translated"><strong class="kk iu">提示</strong>:没有最佳特征选择方法。适用于一个业务用例的方法可能不适用于另一个业务用例，因此您需要进行实验，看看什么方法最有效。</p></blockquote><p id="b59e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结论</strong></p><p id="e1eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我介绍了执行特性选择的不同方法。当然，还有其他方法可以进行特征选择，如方差分析、反向特征消除和使用决策树。要获得一篇了解这些方法的好文章，我建议阅读<a class="ov ow ep" href="https://medium.com/u/a4a48b9cfaca?source=post_page-----3ecfb4957fd4--------------------------------" rel="noopener" target="_blank"> Madeline McCombe </a>的文章，标题为<a class="ae le" rel="noopener" target="_blank" href="/intro-to-feature-selection-methods-for-data-science-4cae2178a00a"> <em class="oc">介绍数据科学的特征选择方法</em> </a>。</p><p id="0ef9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">又及:非常感谢您抽出时间。对于那些想深入了解特性选择的人，下面有一些有用的资源！</p><div class="lf lg gp gr lh li"><a href="https://github.com/kurtispykes/demo" rel="noopener  ugc nofollow" target="_blank"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">kurtispykes/演示</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">与中等博客文章相关的演示代码。-路径/到/文件；链接到文章有效的数据可视化…</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">github.com</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw lx li"/></div></div></a></div></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="f8ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">其他资源</strong></p><p id="4999" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ov ow ep" href="https://medium.com/u/f374d0159316?source=post_page-----3ecfb4957fd4--------------------------------" rel="noopener" target="_blank"> Jason Brownlee </a> - <a class="ae le" href="https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/" rel="noopener ugc nofollow" target="_blank">如何选择机器学习的特征选择方法</a></p><p id="5ef5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ov ow ep" href="https://medium.com/u/3c5638fcb6c5?source=post_page-----3ecfb4957fd4--------------------------------" rel="noopener" target="_blank">vikashraj luhaniwal</a>-<a class="ae le" rel="noopener" target="_blank" href="/feature-selection-using-wrapper-methods-in-python-f0d352b346f">使用 Python 中的包装方法进行特征选择</a></p><p id="c9cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Sebastian Raschka - <a class="ae le" href="https://sebastianraschka.com/faq/docs/feature_sele_categories.html" rel="noopener ugc nofollow" target="_blank">用于特征选择的过滤器、包装器和嵌入式方法之间有什么区别</a></p></div></div>    
</body>
</html>