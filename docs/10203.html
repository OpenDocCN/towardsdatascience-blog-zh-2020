<html>
<head>
<title>Teaching Computers to Recognize Human Actions in Videos</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">教计算机识别视频中的人类动作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/teaching-computers-to-recognize-human-actions-in-videos-81b2e2d62768?source=collection_archive---------31-----------------------#2020-07-18">https://towardsdatascience.com/teaching-computers-to-recognize-human-actions-in-videos-81b2e2d62768?source=collection_archive---------31-----------------------#2020-07-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ee72" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">预测和聚类:基于无监督骨架的动作识别</h2></div><p id="37f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者:伊莱·什利泽曼</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/cfd6dfd5bc8629c6bfc2a98abd728e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aKFSu7VNG82VSswb"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@brucemars?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布鲁斯·马尔斯</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="3de0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于人类来说，仅仅通过观看视频来识别人们用身体做出的各种动作是一项自然而简单的任务。例如，大多数人可以很容易地识别出一个主题，比如，“<em class="lv">来回跳跃</em>，或者“<em class="lv">用脚击球</em>”。即使视频素材中显示的对象发生变化或从不同的视角录制，这也很容易识别。<strong class="kk iu">如果我们希望计算机系统或游戏机(如 Xbox、PlayStation 或类似设备)也能做到这一点，会怎么样？这可能吗？</strong></p><p id="449c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于一个人工系统来说，这个看似基本的任务并不像人类那样自然，它需要几层人工智能能力，例如(I)知道哪个<strong class="kk iu"/><strong class="kk iu"><em class="lv">特征</em></strong>在做决定时跟踪，以及(ii)命名或标记特定动作的<strong class="kk iu"> <em class="lv">能力。</em></strong></p><p id="858c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于(I)，视觉感知和计算机视觉的研究表明，至少对于人体来说，关节的 3D 坐标，即<strong class="kk iu"> <em class="lv">骨架特征</em> </strong>足以识别不同的动作。此外，当前的鲁棒算法能够使用几乎任何视频源镜头实时跟踪这些特征，例如 OpenPose [1]。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lw"><img src="../Images/f553e6bee2922eeb869f2dfc426bf719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJXiZ6BIXttil4kFjF8Jfw.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">Sam Sabourin 在标有骨骼特征的 Unsplash 上的照片</p></figure><p id="7fd9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于(ii)，教导计算机系统使用这些特征在点的集合和动作之间进行<strong class="kk iu"> <em class="lv">预测关联</em> </strong>证明是比仅仅单独选择所述特征更具挑战性的任务。这是因为系统被期望将特征序列分组到“类”中，并且随后将这些与相应动作的名称相关联。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lx"><img src="../Images/daaae97594ae29c77d7199e57edc1046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQuZxkgpAxvZWYZc996G-A.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">基于骨架的动作识别:点(时间序列)集合和动作之间的预测关联</p></figure><p id="b275" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现有的深度学习系统试图通过一个称为“<strong class="kk iu">监督学习</strong>”的过程来学习这种类型的关联，其中系统从几个给定的例子中学习，每个例子都有对它所代表的动作的解释。这种技术在每一步都需要相机和深度输入(RGB+D)。虽然监督动作识别已经显示出有希望的进步，但是它依赖于大量序列的注释，并且每次考虑另一个主题、观点或新动作时都需要重新进行。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ly"><img src="../Images/298cde51dd1cd4932b1937299c5b4acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sFFrecRl4YT8Uym0"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@raymondrasmusson?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">雷蒙德·拉斯姆森</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="4da2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，特别有趣的是，相反，创造试图模仿人类感知能力的系统，这些系统学会以一种不受监督的方式进行这些关联。</p><p id="9b40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们最近名为“<a class="ae le" href="http://openaccess.thecvf.com/content_CVPR_2020/html/Su_PREDICT__CLUSTER_Unsupervised_Skeleton_Based_Action_Recognition_CVPR_2020_paper.html" rel="noopener ugc nofollow" target="_blank">预测&amp;聚类:无监督的基于骨架的动作识别</a>”[2]的研究中，我们开发了这样一个无监督系统。我们已经提出，系统将学习如何通过<strong class="kk iu">‘编码器-解码器’</strong>学习来<strong class="kk iu">预测</strong>序列，而不是教计算机将序列与它们的动作进行分类。这种系统是完全无人监督的，并且仅通过输入来操作，而不需要在任何阶段对动作进行标记。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lz"><img src="../Images/518f1882e7a31c51abba4777eb87fcad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PFHHeU9lEggX7LOUiyPnyQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">预测和聚类:基于无监督骨架的动作识别</p></figure><p id="0393" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">特别地，编码器-解码器神经网络系统学习将每个序列编码成代码，解码器将使用该代码来生成完全相同的序列。原来，在<strong class="kk iu">学习编码然后解码</strong>的过程中，Seq2Seq 深度神经网络<strong class="kk iu">将序列</strong>自组织成截然不同的<strong class="kk iu">簇</strong>。我们开发了一种方法来确保学习是最优的(通过固定解码器的权重或状态)，以便创建这样的组织，并开发了工具来读取该组织，以将每个集群与一个动作相关联。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ma"><img src="../Images/cc2e481315facb4f09998168c0c950ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R18vlnlkgWYoJ41yMrOiYA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">预测和聚类示意图</p></figure><p id="c1ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们能够获得动作识别结果，<strong class="kk iu">优于</strong>之前的无监督和有监督方法。我们的发现为使用任何特征输入的任何类型动作的新型学习铺平了道路。这可能包括从识别飞虫飞行模式的行为到识别互联网活动中的恶意行为。</p><p id="bf1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有关更多信息，请参见下面的概述视频和[2]中的论文。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="97a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">带着<a class="ae le" href="https://medium.com/@sukun1045" rel="noopener"/>和<a class="ae le" href="https://medium.com/@liuxiulong1995" rel="noopener">刘秀龙</a>。</p><h1 id="0000" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">参考</h1><p id="ed1f" class="pw-post-body-paragraph ki kj it kk b kl mv ju kn ko mw jx kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">[1]打开姿势:<a class="ae le" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank">https://github.com/CMU-Perceptual-Computing-Lab/openpose</a></p><p id="deeb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]苏、昆、刘秀龙和伊莱·什利泽曼。"预测和聚类:基于无监督骨架的动作识别."<em class="lv">IEEE/CVF 计算机视觉和模式识别会议论文集</em>。2020.</p></div></div>    
</body>
</html>