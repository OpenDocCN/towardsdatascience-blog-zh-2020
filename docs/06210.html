<html>
<head>
<title>Introduction to Regret in Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习中的后悔介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-regret-in-reinforcement-learning-f5b4a28953cd?source=collection_archive---------17-----------------------#2020-05-19">https://towardsdatascience.com/introduction-to-regret-in-reinforcement-learning-f5b4a28953cd?source=collection_archive---------17-----------------------#2020-05-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1b92" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">有时候后悔是改善的好方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f783e073741fbefae02bd51863cd0794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VPJZGIzuZEqGbBV8"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@freetousesoundscom?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上免费使用声音</a></p></figure><p id="b361" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新</strong>:学习和练习强化学习的最好方式是去<a class="ae ky" href="http://rl-lab.com/" rel="noopener ugc nofollow" target="_blank">http://rl-lab.com</a></p><blockquote class="lv lw lx"><p id="9dd4" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">“最终，我们只后悔没有抓住机会”<br/> <strong class="lb iu">刘易斯·卡罗尔</strong></p></blockquote><h1 id="c7c2" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">介绍</h1><p id="e603" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">几乎可以肯定的是，每个人一生中都有后悔的事情(实际上是很多事情)。后悔没有在价格还能承受的时候买票，后悔没有做出职业决定，后悔个人或社会的举动，等等……当然，后悔是有苦味的，尽管它可能具有教育意义，但现实是机会往往会失去，并且没有回头路。</p><p id="ce4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但在训练机器或算法时，情况可能就不一样了。</p><h1 id="57ba" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">遗憾</h1><p id="cb5d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">你最后悔的行动是本应该(更有可能)使用或采取的行动。所以采取这个行动的概率和你后悔没采取行动的程度成正比。</p><p id="e28c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从数学上来说，后悔被表达为一个可能的行动的回报(奖励或回报)和已经实际采取的行动的回报之间的差异。如果我们将收益函数表示为<strong class="lb iu"> <em class="ly"> u </em> </strong>，则公式变为:</p><p id="9814" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ly">后悔= u(可能的行动)- u(采取的行动)</em> </strong></p><p id="8e42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，我们感兴趣的是'<strong class="lb iu"><em class="ly"/></strong>'可能行动的收益胜过'<strong class="lb iu"> <em class="ly">采取行动</em> </strong>'的收益的情况，所以我们考虑积极的遗憾，忽略零和消极的遗憾。</p><p id="cc77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，使用一个行为而不是实际使用的行为的概率与它产生的遗憾成正比。</p><p id="39a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，如果我们采取行动a1，得到u(a1) = 1，那么我们计算出u(a2)= 2，u(a3) = 4，u(a4) = 7。相应的后悔将是后悔(a2) = u(a2) - u(a1) = 1，后悔(a3) = 3，后悔(a4) = 6。<br/>总遗憾是遗憾(a1) +遗憾(a2) +遗憾(a3) +遗憾(a4) = 0 + 1 + 3 + 6 = 10。</p><p id="b6c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很容易看出最后悔的动作是a4。为了在数字上反映这一点，我们更新了我们的策略，表示为σ，例如σ(a2) = 1/10 = .1，σ(a3) = 3/10 = .3，σ(a4) = 6/10 = .6。</p><p id="93e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，你可能会问，为什么不显式地给动作a4一个概率1 (σ(a4) = 1)？仅仅是因为当面对另一个演员时，比如在游戏中，后悔的概念被使用。在游戏中以确定性的方式进行游戏会给你的对手一个反击你的策略并赢得胜利的机会。</p><h2 id="f9ec" class="mz md it bd me na nb dn mi nc nd dp mm li ne nf mo lm ng nh mq lq ni nj ms nk bi translated">石头剪刀布示例</h2><p id="e1b5" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">考虑一个<a class="ae ky" href="https://en.wikipedia.org/wiki/Rock_paper_scissors" rel="noopener ugc nofollow" target="_blank">石头剪刀布</a> (RPS)的游戏，其点数系统如下:</p><ul class=""><li id="8cd2" class="nl nm it lb b lc ld lf lg li nn lm no lq np lu nq nr ns nt bi translated">损失-1分</li><li id="07fd" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">抽签(两个相同类型的项目)的结果是0分</li><li id="737b" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">赢给获胜者1分</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/1c5617662a729e844336e5516e9d23f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*KWl1rmEcoQ6dQxeyul6pbw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Rock_paper_scissors" rel="noopener ugc nofollow" target="_blank">石头剪刀布</a>游戏的支付格</p></figure><p id="85b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下表给出了玩这个游戏的不同组合，以及结果和如何改进策略。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/b26cb1bbfa4144ff9ec7c1a6009f5e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqnSwSbPxkjUvOtYVuNTlQ.png"/></div></div></figure><p id="61d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">表格的第一部分(实际游戏)显示了你与对手的比赛，以及每集的“Y <strong class="lb iu">我们的结果</strong>”。“<strong class="lb iu">迭代</strong>”列是针对ex: R对R，或S对P等的相同组合发生的发作次数</p><p id="972c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二部分(你的其他游戏场景)包含假设对手以同样的方式玩游戏，你为了提高(或不提高)你的结果可以玩的场景。它也显示了没有玩一个给定动作的遗憾。<br/>积云遗憾列包含累计遗憾，累计遗憾是遗憾的总和。</p><p id="8b0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对累积遗憾的需求源于这样一个事实，即独立计算遗憾，并不能捕捉到在其他游戏或剧集中发生的事情。这意味着算法没有从它的经验中学习。</p><p id="6f87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个人，你在记忆中保留你以前玩过的游戏，以及如何利用这些过去的经验。但是为了让一个算法做同样的事情，应该有一个考虑到以前发生过的事情的计算。</p><p id="e0b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第三部分(策略调整)，计算每个动作(石头、布、剪刀)的概率，以最大化你的结果，总是假设对手以同样的方式玩。</p><p id="fe00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些概率被计算为<strong class="lb iu"> <em class="ly">(对行动的累积后悔)/总后悔</em> </strong>。其中总遗憾是同一行的正累积遗憾的总和。如果总遗憾为零，我们为每个行动分配相等的概率(检查第二行)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/b26cb1bbfa4144ff9ec7c1a6009f5e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqnSwSbPxkjUvOtYVuNTlQ.png"/></div></div></figure><p id="9ab6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上表第一行，你打了R，对手打了R，所以结果是平局(0)。如果你出了P而对手出了R那就更好了，所以你的后悔是1没出P<br/>策略调整说明你不后悔石头，也不后悔剪刀但是你后悔没用纸。<br/>第二排，你用R对S获胜，策略调整部分显示2局后，你没有遗憾。</p><p id="c13b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着情节的继续，我们看到策略的变化达到了平衡，每个动作应该用1/3。<br/> PS。这是RPS中的最佳策略，因为它使所有3个动作的概率相等，因此移动是对手不可预测的。</p><h2 id="fb3d" class="mz md it bd me na nb dn mi nc nd dp mm li ne nf mo lm ng nh mq lq ni nj ms nk bi translated">极端情况</h2><p id="b24a" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">现在，如果一个场景比其他场景播放得多会发生什么？<br/>例如，在下表中，S对S这一集发生了1000次，结果是1000次平局。这导致在这些情况下后悔1000，并且策略转向100%的时间使用石头。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/2eb42174ba2dbbe924266f7d6a1ea203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQ9TuT-94l_JSiCPlqQVCg.png"/></div></div></figure><p id="d825" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下面的例子中，P对S发生了1000次，导致1000次损失，2000次后悔没有用石头，1000次后悔没有用剪刀。<br/>因此，该策略被调整为67%的时间使用石头，33%的时间使用剪刀。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/5afbc218e412a3ca5f5d624ea8987c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mXyI7XXWR7AG3tR5uaklog.png"/></div></div></figure><p id="56af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，下面的例子中有一个陷阱，R vs S出现1000次，结果是千胜万败，无怨无悔。因为没有遗憾，所以算法不更新策略。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/e0d9456eb9083678b2dec2ad3a11cea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nfplaC-wxLUzK25-iK71pg.png"/></div></div></figure><h2 id="170a" class="mz md it bd me na nb dn mi nc nd dp mm li ne nf mo lm ng nh mq lq ni nj ms nk bi translated">密码</h2><p id="20c0" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">下面是Google Colab书籍的链接，其中包含一个简单的后悔算法的代码。<br/> <strong class="lb iu">重要提示:为了运行或编辑代码，你需要复制一本书。</strong></p><div class="oe of gp gr og oh"><a href="https://colab.research.google.com/drive/1FB57Jfi1llITSL6DUyUr_9P7lsXmOP-8#scrollTo=_9p-AnRTy8rI" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">简单RPS示例</h2><div class="oo l"><p class="bd b dl z fp om fr fs on fu fw dk translated">colab.research.google.com</p></div></div><div class="op l"><div class="oq l or os ot op ou ks oh"/></div></div></a></div><h1 id="24cd" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">自娱自乐后悔</h1><p id="c2f6" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">到目前为止，我们假设对手一直以同样的方式玩，使用同样的策略。然而，这不可能是真的！任何对手最终都会发现你策略中的任何偏差，并加以利用。</p><p id="cb01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以训练不能针对固定的策略。为了补救这种情况，我们使用自我游戏。Self Play不是训练一个演员对抗一个固定的策略，而是训练所有演员互相对抗。这是通过“复制”第一个演员所做的序列，并将其应用于其他演员来完成的。因此，每个演员现在都维护自己的数据结构，其中包含自己的策略、遗憾等。在每一集之后，每个演员都从自己的角度计算结果，以及可以(可能)做些什么来改善结果。</p><h2 id="09cd" class="mz md it bd me na nb dn mi nc nd dp mm li ne nf mo lm ng nh mq lq ni nj ms nk bi translated">密码</h2><p id="60f2" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">下面的链接指向一个Google Colab书籍，它实现了一个简单的后悔自我游戏。</p><p id="7483" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">重要提示:为了运行或编辑代码，你需要复制一本书。</strong></p><div class="oe of gp gr og oh"><a href="https://colab.research.google.com/drive/1FB57Jfi1llITSL6DUyUr_9P7lsXmOP-8#scrollTo=-ej-kJVb8j0y" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">RPS自我游戏示例</h2><div class="oo l"><p class="bd b dl z fp om fr fs on fu fw dk translated">colab.research.google.com</p></div></div><div class="op l"><div class="ov l or os ot op ou ks oh"/></div></div></a></div><p id="4397" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有趣的是，我们注意到，无论两个参与者从什么策略开始，他们都收敛到RPS游戏的最佳策略，即每个动作的概率都是1/3。这确保了所有行动的可能性相等，并防止任何可能被对手利用的偏差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/ce067bbb3bdae61f843a54f1da90c3c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BsNHwfTONmG44I5UfAjbEQ.png"/></div></div></figure><h1 id="49c8" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="93cc" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">本文介绍了简单直观的后悔技巧。它允许玩家通过跟踪过去游戏的遗憾来达到平衡游戏，使未来游戏与积极的遗憾成比例。这项技术是更精细的技术的基础，比如反事实后悔最小化(CFR)和深度CFR。</p></div></div>    
</body>
</html>