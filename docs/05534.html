<html>
<head>
<title>Converging Support Vector Classifiers and Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">收敛支持向量分类器和逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-classifiers-and-logistic-regression-similarity-97ff06aa6ec3?source=collection_archive---------44-----------------------#2020-05-09">https://towardsdatascience.com/support-vector-classifiers-and-logistic-regression-similarity-97ff06aa6ec3?source=collection_archive---------44-----------------------#2020-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jq jr js jt"><div class="bz fp l di"><div class="ju jv l"/></div><p class="jw jx gj gh gi jy jz bd b be z dk translated">我将展示一个逻辑回归的决策边界何时收敛到SVC的决策边界的例子(从左上到右下的图解)，如这里在逻辑成本函数中减少正则化参数的步骤所示(C是正则化的逆)。</p></figure></div><div class="ab cl ka kb hx kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="im in io ip iq"><h1 id="ea50" class="kh ki it bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">介绍</h1><p id="6a06" class="pw-post-body-paragraph lf lg it lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">支持向量分类器(SVC)和逻辑回归(LR)可以对齐到它们可以是完全相同的东西的程度。理解SVC和LR何时完全相同将有助于直观地了解它们到底有什么不同。</p><p id="2676" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">逻辑回归(LR)是一种使用sigmoid函数的概率分类模型，而支持向量分类器(SVC)是一种更具几何性的方法，可以最大限度地提高每类的利润率。它们的相似之处在于，它们都可以用决策边界来划分特征空间。我经常听到关于这两种机器学习算法之间差异的问题，对此的答案总是集中在LR中log-odds广义线性模型的精确细节以及SVC的最大边际超平面、软边际和硬边际上。</p><p id="5c17" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated"><strong class="lh iu">我觉得看相反的情况很有用:</strong>LR和SVC什么时候完全一样？！</p><p id="4201" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated"><strong class="lh iu">答案:</strong>当专门使用具有二元结果预测器的二元特征、没有正则化的LR模型和具有线性核的SVC时。</p><p id="b1b2" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">让我们简单回顾一下LR和SVC，然后举例说明这些决策界限可以完全相同。</p></div><div class="ab cl ka kb hx kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="im in io ip iq"><h1 id="812f" class="kh ki it bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">逻辑回归</h1><p id="2d8a" class="pw-post-body-paragraph lf lg it lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们举一个例子，使用癫痫发作的样子(符号学)和我们在大脑扫描中发现的东西，尝试确定癫痫发作是来自大脑中的颞叶还是其他地方(颞外)。</p><p id="848f" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated"><strong class="lh iu">特征:</strong>癫痫发作时会发生什么，例如手臂抖动或拨弄手指(符号学)以及脑部扫描异常，称为海马硬化(HS)(所有特征都是二元的)</p><p id="5cdd" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">目标预测因子:癫痫发作来自颞叶吗？(二进制)</p><figure class="mj mk ml mm gt jt gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mi"><img src="../Images/bc1f23238ab3db81f29cc02955a130fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11za238_p9a-bEnWdmbduA.png"/></div></div><p class="jw jx gj gh gi jy jz bd b be z dk translated">图1:逻辑回归总结</p></figure><p id="2192" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">逻辑回归是一个分类模型，尽管它的名字。基本思想是给模型一组输入x，它可以是多维的，并得到一个概率，如图1的右图所示。当我们希望二进制目标的概率在0和1之间时，这可能是有用的，而不是线性回归(图1的左图)。如果我们重新排列上面的sigmoid函数并求解x，我们会得到logit函数(也称为log-odds ),这是一个清晰的广义线性模型(图1中的底部方程)。</p><p id="ba2a" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">LR是在该模型中寻找最佳(θ)系数，从而通过梯度下降将总误差(成本函数)降至最低(图2):</p><figure class="mj mk ml mm gt jt gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mt"><img src="../Images/a50d64861bdaa7a47c8c331c0f571743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*etIYunJSTVjGXsQXee23DQ.png"/></div></div><p class="jw jx gj gh gi jy jz bd b be z dk translated">图2: LR成本函数使用颞侧与颞外癫痫病灶作为目标预测因子。左边的图像用cost = -y.log(p(x)) — (1-y)表示。log(1-p(x))。其中y是真正的二进制标签。右图显示了使用系数偏导数的梯度下降。</p></figure><p id="4371" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">向成本函数添加正则化参数(1/C)(乘以系数的平方和):</p><figure class="mj mk ml mm gt jt gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/75d88d0b2d88728739f9879b2bb3adea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*E5GfIYFx1amEGG_fXSYRqw.png"/></div><p class="jw jx gj gh gi jy jz bd b be z dk translated">等式1:LR的L2正则化的成本函数。L2意味着θ系数是平方的。y是真正的二进制标签。y’是模型预测的标签概率:p(x)。</p></figure><p id="aefa" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">Sklearn有一个<a class="ae mv" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank"> LogisticRegression() </a>分类器，默认的正则化参数是penalty=L2和C=1。这个术语惩罚过度拟合。不幸的是，在下面的特殊情况下，这也可能导致拟合不足，从而导致交叉验证性能变差。但是当这种规范化被移除时，对于下面的特殊情况，我们将看到它合并成与SVC相同的模型。</p><h1 id="444f" class="kh ki it bd kj kk mw km kn ko mx kq kr ks my ku kv kw mz ky kz la na lc ld le bi translated">线性支持向量分类器</h1><figure class="mj mk ml mm gt jt gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nb"><img src="../Images/f0fa8059d8094e2668fdd80dca45a6b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ul1cIoc9-VlIFn4sDl4cqA.png"/></div></div><p class="jw jx gj gh gi jy jz bd b be z dk translated">图3:左边的线性决策边界。右图显示了从SVC优化得到的最大硬边界超平面。图片来自<a class="ae mv" rel="noopener" target="_blank" href="/svm-feature-selection-and-kernels-840781cc1a6c">https://towards data science . com/SVM-feature-selection-and-kernels-840781 C1 a6 c</a></p></figure><p id="852a" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">LR将绘制一个单一的边界来分隔时间和时间外癫痫发作，而线性SVC首先找到两个支持向量，然后找到最佳超平面(图3的右图)。为了优化这一点，SVC使用了一种带有这些约束的<a class="ae mv" href="https://en.wikipedia.org/wiki/Lagrange_multiplier" rel="noopener ugc nofollow" target="_blank">拉格朗日乘数</a>方法:边缘超平面必须尽可能多地分离数据，并且两者之间的距离必须最大化。通过<a class="ae mv" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank"> svm中确定核的方法。SVC(kernel='linear') </a>，我们有一个线性SVM。</p></div><div class="ab cl ka kb hx kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="im in io ip iq"><h1 id="e6a3" class="kh ki it bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">当SVC和LR收敛时</h1><p id="d3fd" class="pw-post-body-paragraph lf lg it lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在让我们以2D二元特征空间分类问题为例:</p><figure class="mj mk ml mm gt jt gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nc"><img src="../Images/862a7dde79f7e0606719141295e12c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fL0gSgEhtQRk1hOKL0f91Q.png"/></div></div><p class="jw jx gj gh gi jy jz bd b be z dk translated">图4:当线性SVC和LR收敛时</p></figure><p id="7584" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">SVC模型的边缘线重合，因为右下角和左上角的所有数据都重合。因此，SVC的最大裕量为零，其决策边界为黄色(图4)。</p><p id="b6d4" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">LR有一个正则化参数1/C，它与对数损失成本函数(图2，等式1)相结合，可以降低系数的幅度(图1 ),导致蓝色决策边界。通过将C设置为一个大的数字，从而将正则化减少到零，两者的决策边界(蓝色和黄色)开始收敛。</p><p id="4aa1" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">这将适用于大多数二进制特征数据。接下来是Python代码，因此您可以复制图4中的可视化的更简单版本，其中决策边界完全收敛。</p></div><div class="ab cl ka kb hx kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="im in io ip iq"><h1 id="b4b7" class="kh ki it bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">代码和结论:</h1><p id="1d69" class="pw-post-body-paragraph lf lg it lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">可以在GitHub上找到一些简单的代码，用于合成基本数据，并拟合SVC和LR(有和没有正则化),以便我们可以看到它们的决策边界收敛和重合:</p><p id="a09c" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated"><a class="ae mv" href="https://github.com/thenineteen/ML_intuition" rel="noopener ugc nofollow" target="_blank"><strong class="lh iu"/>https://github.com/thenineteen/ML_intuition</a></p><p id="26b5" class="pw-post-body-paragraph lf lg it lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">最终结果:</p><figure class="mj mk ml mm gt jt gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/ed676e9d4a8d494f24f2a29866f928ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*xXoS5FxrA9YahWuL83y4Bw.jpeg"/></div></figure><figure class="mj mk ml mm gt jt"><div class="bz fp l di"><div class="ju jv l"/></div><p class="jw jx gj gh gi jy jz bd b be z dk translated">图5:(上图)SVC决策边界。(下图)LR决策边界作为其成本函数中的正则化参数被减少。</p></figure><h1 id="098e" class="kh ki it bd kj kk mw km kn ko mx kq kr ks my ku kv kw mz ky kz la na lc ld le bi translated">摘要</h1><p id="1263" class="pw-post-body-paragraph lf lg it lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">线性核SVC和逻辑回归可以产生相同的决策边界，即完全相同的模型，因此可以产生完全相同的性能指标，尽管使用完全不同的方法。当所有特征都是二进制的，目标变量是二进制的，并且LR的正则化参数设置为零时，会发生这种情况。</p></div></div>    
</body>
</html>