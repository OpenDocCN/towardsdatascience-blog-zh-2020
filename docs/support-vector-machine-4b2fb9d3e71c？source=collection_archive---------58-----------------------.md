# 支持向量机

> 原文：<https://towardsdatascience.com/support-vector-machine-4b2fb9d3e71c?source=collection_archive---------58----------------------->

## 更简单直观的解释

支持向量机学习算法简称 SVM，常用于分类问题。这给了我们一个更好的方法来理解一个非线性的决策边界。

让我们首先考虑一个场景，在这个场景中，我们可能想要对以下数据进行分类—

![](img/d8d47b049857248c2f921b7eff7917f6.png)

用于说明简单 SVM 模型的数据集。

很明显，可以使用简单的线性判定边界将数据分成各自的类别。然而，有多个这样的决策界限可以用来区分这两个类别。事实上，有无限多的线性决策边界可以用来区分红色和蓝色类别。考虑以下决策界限—

![](img/60756e5888702faf6b816309fb655ac9.png)

可用于数据分类的可能决策界限。

*   左边的一个太接近蓝色类，暗示“每当有疑问时，将该点分类为红色”。
*   中间图形中的决策边界更接近红色类。接近红色类别的判定边界意味着，如果模型将一个点分类为红色，那么如果它将同一个点分类为蓝色，那么它对其判定更有把握，从而遵循“每当有疑问时将该点分类为蓝色”。此外，直觉告诉我们可以做得比完全直线的决策边界更好。
*   在最后一张图中，决策边界靠近红色和蓝色类的实例。

从上面的讨论中，我们可以得出结论，我们想要一个决策边界，它不要太靠近这两个类中的任何一个。为了帮助我们找到这样一条线，我们使用了 SVM。

支持向量机学习算法集中在两类中彼此接近的那些点上。这个想法是，如果一个分类器在分类具有挑战性的点方面做得很好，它很可能在数据上表现得更好。SVM 通过从这两个被称为**支持向量的类中查找最近的点来做到这一点。一旦 SVM 找到了它的支持向量，它将试图找到一条距离这些点最远的线。在两点的情况下，垂直平分线是等距的线，因此离两点的距离最大。任何其他线都会比另一条线更靠近一个点。**

我们还可以说，给定支持向量和多条线，我们将选择距离所有支持向量尽可能大的那条线作为我们的决策边界。

![](img/ae161e590dc0325a50f3daee382cd80a.png)

支持向量和超平面说明了 SVM 模型。

因此，SVM 试图找到一个超平面，将数据分为两类，同时试图最大化超平面和类的支持向量之间的距离。

n 维空间中的**超平面**是该空间的 n-1 维子集。它把空间分成两个独立的部分。在 1 维空间的情况下，*点*是超平面，在 2 维空间的情况下，*线*是超平面，在 3 维空间的情况下，*平面*是超平面，依此类推。

在实际场景中，数据并不像拟合线性决策边界那样简单。看一下下面的数据集——

用于 SVM 模型的数据集。

![](img/29116cb24c4c2bf2061cba9ddca14ae8.png)

具有非线性决策边界的数据集。

对于数据集来说，使用直线作为决策边界绝对不是一个好的选择。

这里的解决方案是将我们的二维数据转换成一个更高的三维数据，从而使其可分离。让我们添加第三维度 *z* 作为 *z=x*y.*

这里应用的变换将一个*二维数据集*映射到一个更高的*三维数据集*以找到一个最佳超平面。这些变换被称为 SVM 的**内核技巧。**在内核技巧中，使用一个函数将数据映射到更高维度，以便数据变得可分离，并找到决策边界。

![](img/859a90e6c45a504a153a35c704c0491c.png)

将数据转换到更高的平面。

观察到，通过应用核，蓝色类的所有点都被限制在 x-y 平面上，绿色类的所有点都被提升到 x-y-z 空间，从而给我们一个平面作为我们的最优超平面。

为了简单起见，让我们假设 *z=1* 是最佳决策边界。然而，SVM 将输出一个超平面，使得它与任一类的支持向量的距离最大。

![](img/84c67294fe030b9e25f66d4e5b63e3c7.png)

变换数据上的超平面。

我们通过使用转换 *z=x*y* 将我们的 2D 数据映射到 3D 数据。现在我们将把决策边界 *z=1* 从 3D 空间映射到 2D 空间。这给了我们 *x*y=1。这就是 SVM 变换数据并输出决策边界的方式。*

![](img/f63a0d80488317fbdbba9270bf2ef41e.png)

映射的决策边界。

线性、径向基滤波器、多项式、Sigmoid 和拉普拉斯是几种常用的 SVM 核。

内核的详细讨论超出了本文的范围。然而，让我们简要地讨论一下 SVM 模型的两个基本参数

## 规范化

正则化参数解决了这样一个问题，您希望在多大程度上避免对训练数据进行错误分类。值越高，训练数据的错误分类就越少。然而，模型可能会过度拟合。在 sklearn 中用 *C* 表示。

![](img/24be4a5d975d1903047076c29317144b.png)

左图:较小的 C 值给出了简单的广义边界。右图:C 值越大，边界越倾向于过度拟合。

## 微克

Gamma 决定将多少个样本作为支持向量。较小的值意味着只有更接近可能的超平面的实例将用于进一步拟合它。然而，大的值意味着在拟合超平面时也使用远离超平面的实例。

![](img/95c2879931d2f44424045fbd90490bd6.png)

左图:Gamma 值越小，决策边界周围的实例数量越少。右图:gamma 值越大，意味着在拟合决策边界时包含的实例数量越多。

乍一看，你可能会认为 SVM 类似于逻辑回归，但是最优超平面由 SVM 中的支持向量控制，而不是逻辑回归中考虑的所有数据点。此外，逻辑回归遵循统计方法并输出概率，然而 SVM 是由几何方法支持的。SVM 旨在通过最大化支持向量之间的距离来输出最优超平面，从而降低错误率。

建议从逻辑回归开始，然后用线性核进行 SVM。在跟踪所选性能指标的同时，可以进一步尝试其他内核，如 RBF。

这是一个将癌症分为恶性或良性的例子。该示例使用具有线性核的 SVM，并使用准确度作为性能度量。请随意尝试不同的内核，并试验模型的超级参数。

[](https://colab.research.google.com/drive/1WfnJQ4z3tXVst7ZJxVjGBCtDHNeRdwPE?usp=sharing) [## 谷歌合作实验室| Abhishek Kumar

### 基于支持向量机的癌症分类

colab.research.google.com](https://colab.research.google.com/drive/1WfnJQ4z3tXVst7ZJxVjGBCtDHNeRdwPE?usp=sharing) 

SVM 已在以下文件中详细讨论过—

[](https://scikit-learn.org/stable/modules/svm.html) [## 1.4.支持向量机-sci kit-学习 0.23.2 文档

### 在高维空间有效。在维数大于…的情况下仍然有效

scikit-learn.org](https://scikit-learn.org/stable/modules/svm.html) 

我希望这篇文章能帮助你对 SVM 背后的直觉和相关参数如核、正则化和伽玛有一个公平的理解。