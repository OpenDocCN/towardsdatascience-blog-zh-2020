<html>
<head>
<title>A Simple Gradient Boosting Trees Explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个简单的梯度增强树解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-simple-gradient-boosting-trees-explanation-a39013470685?source=collection_archive---------50-----------------------#2020-07-19">https://towardsdatascience.com/a-simple-gradient-boosting-trees-explanation-a39013470685?source=collection_archive---------50-----------------------#2020-07-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="02f6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">梯度推进树的简单介绍。</h2></div><h1 id="fbb3" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="afb2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">几年前，Kaggle 的官方博客“没有自由的预感”发布了一篇 Kaggle 大师本·戈尔曼(Ben Gorman)解释渐变增强的文章。文章发表后不久，我在博客上评论了这篇文章。快进到大约一年后，我正在为我的博客查看我的谷歌分析账户，我注意到我的评论得到了很多点击。在进一步的检查中，我发现原来的“没有自由的预感”文章被删除了，因此解释了为什么我的文章得到了这么多的点击(即人们在寻找原来的文章)。鉴于这些新信息，我更新了我的文章，并向人们介绍了我在 GitHub 上的一些工作，这些工作可能会有所帮助。有趣的是，我从来没有喜欢过卡格尔的原创文章。我有一些想法来真正削减脂肪和解释核心概念，这正是这篇文章！我希望这篇文章对那些刚刚开始理解梯度推进树的人有用。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/b7bed3b2a4c01cfce183e6f65df6383a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFvJEnjQ0knmvKdK0z3FVQ.jpeg"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">在<a class="ae mj" href="https://unsplash.com/s/photos/teach?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae mj" href="https://unsplash.com/@bel2000a?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Belinda Fewings </a>拍摄的照片</p></figure><h1 id="998a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">最简短的解释</h1><p id="bc62" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在梯度推进树中，我们不断地在我们的模型误差上构建决策树，并且我们使用这些误差预测来修正\更新我们的原始模型预测。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mk"><img src="../Images/19e714fe185ce8c3ac0b84f7d87f9d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QnslEiE74gaD6RUUPCET4g.jpeg"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">照片由<a class="ae mj" href="https://unsplash.com/@themephotos?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">主题照片</a>在<a class="ae mj" href="https://unsplash.com/s/photos/repair?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="abdf" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">更详细的简短解释</h1><p id="ac0d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们建立一个决策树。这个模型对于每个数据点都有一个误差。如果我们可以在这些错误的基础上建立另一个决策树，并使用我们预测的错误来修改我们原来的预测，会怎么样？我们可以！我们可以根据错误建立另一个决策树，并使用这些预测来修改我们最初的预测。我们可以继续这个过程，直到我们决定停止！</p><h1 id="4718" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">更详细的简短解释+可视化</h1><p id="6528" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="ml">注意:以下图片并非来自 Jupyter 或 Python 或任何实数，我们的可视化只是在 Microsoft Paint 中创建的草图。</em></p><p id="3e1c" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">首先，我们有一些数据。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mr"><img src="../Images/5a05c38499107ad2fdce8f2b2057d440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JL-mfgGFPrjz3YB9zznnUA.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">由作者在 Microsoft Paint 中创建</p></figure><p id="e5dd" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">因此，我们想用 X 来预测 Y。在此之前，可以通过取 Y 的平均值来推导出一个简单的 Y 模型(所以先忽略 X)，我们可以在未来的所有情况下用它来预测 Y。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mr"><img src="../Images/dbdf9bb76a2dc0f03087d4ebbe9f4d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EIRuTy8L-ITPGWphaLBeFw.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">由作者在 Microsoft Paint 中创建</p></figure><p id="3f0d" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">这种模式似乎在某些方面做得不错，而在其他方面做得很差。因此，每个数据点都有一个错误，下面用红色表示。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mr"><img src="../Images/f9e2b4eeab1c375407cbc089e2ad0c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uY85vCojw8RNFx789z2tiQ.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">由作者在 Microsoft Paint 中创建</p></figure><p id="5650" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">如果我们可以建立一个误差模型，这样我们就可以在未来修正我们的误差(因为我们手头没有因变量)。嗯，我们可以！</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mr"><img src="../Images/15a1971621cc6d23dbc187adaa78f391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tja1wCsbyov7W6xxAgH6MQ.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">由作者在 Microsoft Paint 中创建</p></figure><p id="b9d2" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">上面我们看到了 X 的残差图，我们可以使用 X 来帮助预测第一个模型的残差。这个模型说我们想要增加 X 低端的值，减少所有其他的值。我们可以用这些预测来更新我们原来的预测！</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mr"><img src="../Images/e2c9427847332b92bf03daaa738eeb38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EhwwhIQQuuC9aYQNwvDsdQ.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">由作者在 Microsoft Paint 中创建</p></figure><p id="e025" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">所以我们采用了最初的预测，并用第二个模型的预测误差对它们进行了修正。</p><p id="b1a6" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">我们可以一遍又一遍地继续这个过程。实际上，我们通常使用“学习率”来只取误差\误差更新的一小部分。当我们多次运行(包括学习率)时，我们可能会得到如下结果。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mr"><img src="../Images/096c5de77bde7251f8f478f5b068cd5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*59ejmfxp1gdbkuc4yoFXpg.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">由作者在 Microsoft Paint 中创建</p></figure><p id="7ce6" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">就是这样！</p><p id="840b" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">你可以看到这个算法是如何得到它的名字的，它的灵感来源于梯度下降。</p><h1 id="443e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">技术细节和演示</h1><p id="6b0d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">要了解更多技术细节，以及使用 Python 的演示，您可以点击这里查看我在 GitHub <a class="ae mj" href="https://github.com/yeamusic21/Coursera-Machine-Learning-Specialization-Extra-Assignments/blob/master/Classification/Gradient%20Boosting.ipynb" rel="noopener ugc nofollow" target="_blank">上的分类示例。</a></p><h1 id="fc9e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">概括起来</h1><ul class=""><li id="7dce" class="ms mt iq kz b la lb ld le lg mu lk mv lo mw ls mx my mz na bi translated">梯度推进树只是根据我们的模型误差不断构建决策树的过程，我们使用这些预测来修改\更新我们的原始模型预测。</li><li id="b396" class="ms mt iq kz b la nb ld nc lg nd lk ne lo nf ls mx my mz na bi translated">关于使用数据和代码的演示，你可以点击这里查看我在 GitHub <a class="ae mj" href="https://github.com/yeamusic21/Coursera-Machine-Learning-Specialization-Extra-Assignments/blob/master/Classification/Gradient%20Boosting.ipynb" rel="noopener ugc nofollow" target="_blank">上的分类示例。</a></li></ul><p id="f03a" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">感谢阅读！！:-D</p></div></div>    
</body>
</html>