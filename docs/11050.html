<html>
<head>
<title>T5: Text-To-Text Transfer Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">T5:文本到文本转换转换器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/t5-text-to-text-transfer-transformer-643f89e8905e?source=collection_archive---------4-----------------------#2020-08-01">https://towardsdatascience.com/t5-text-to-text-transfer-transformer-643f89e8905e?source=collection_archive---------4-----------------------#2020-08-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c337" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解基于变压器的自监督架构</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/619623d1eba95e03a79eb0c4621bae51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*gzDajSOL6UfOVFpW.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">通过<a class="ae ku" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a>为 QnA 提供 T5</p></figure><p id="eefd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">随着<a class="ae ku" href="https://en.wikipedia.org/wiki/Transfer_learning" rel="noopener ugc nofollow" target="_blank">迁移学习</a>的蓬勃发展，深度学习已经取得了许多奇迹。更确切地说，在 NLP 中，随着 Transformer 的兴起(<a class="ae ku" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Vaswani 等人。艾尔。</a>)，出现了各种“语言建模”的方法，其中我们通过为一个非常通用的任务预先训练模型，然后针对特定的下游问题对其进行微调，来利用迁移学习。</p><p id="25f7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在这篇文章中，我们将讨论 Google 的最新技术，<strong class="kx iu">T5</strong>—<strong class="kx iu">T</strong>ext-<strong class="kx iu">T</strong>o-<strong class="kx iu">T</strong>ext<strong class="kx iu">T</strong>Transfer<strong class="kx iu">T</strong>Transformer 模型，该模型是今年早些时候在论文中提出的，“用统一的文本到文本转换器探索迁移学习的极限”。本文本质上是对语言理解中使用的现代迁移学习技术的综述，因此提出了一个统一的框架，试图将所有的语言问题结合到一个文本到文本的格式中。我们将在接下来的章节中更详细地讨论这种方法。此外，作者还开源了一个新的数据集(为了方便他们的工作)，名为<strong class="kx iu">C4</strong>—<strong class="kx iu">C</strong>olossal<strong class="kx iu">C</strong>lean<strong class="kx iu">C</strong>rawled<strong class="kx iu">C</strong>or pus。</p><h1 id="f836" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">T5—文本到文本转换转换器</h1><p id="6a9b" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">如前所述，T5 试图将所有下游任务组合成文本到文本的格式。</p><h2 id="1605" class="mo ls it bd lt mp mq dn lx mr ms dp mb le mt mu md li mv mw mf lm mx my mh mz bi translated">文本到文本的框架</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8b039ca9d460a5ce00be791576a2a746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*xfXDPjASztwmJlOa.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">通过<a class="ae ku" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a>为所有下游任务提供统一框架</p></figure><p id="2aba" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">考虑 BERT 风格架构的示例，该架构针对屏蔽 LM 和下一句预测目标进行预训练，然后针对下游任务进行微调(例如预测分类中的类别标签或 QnA 中的输入跨度)。这里，我们在不同的下游任务上分别微调预训练模型的不同实例。</p><p id="e95f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">相反，文本到文本框架建议在所有 NLP 任务上使用相同的模型、相同的损失函数和相同的超参数。在这种方法中，输入以这样一种方式建模，即模型应该识别任务，而输出只是预期结果的“文本”版本。参考上面的动画可以更清楚地看到这一点。</p><blockquote class="nb nc nd"><p id="6572" class="kv kw ne kx b ky kz ju la lb lc jx ld nf lf lg lh ng lj lk ll nh ln lo lp lq im bi translated">有趣的事实:我们甚至可以通过训练 T5 来输出预期输出的字符串表示，从而将 T5 应用于回归任务。</p></blockquote><h2 id="5994" class="mo ls it bd lt mp mq dn lx mr ms dp mb le mt mu md li mv mw mf lm mx my mh mz bi translated">C4——庞大干净的爬行语料库</h2><p id="838c" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">在巨大的未标记数据集上预先训练语言模型是一种刻板印象。通用爬虫就是这样的数据集之一。它是通过抓取网页并忽略 HTML 中的标记而获得的。它每月产生大约 20TB 的废弃数据。然而，普通爬行包含大量的像菜单或错误信息，或重复文本的乱码文本。此外，对于我们的任务来说，还有相当数量的无用文本，比如攻击性的词语、占位符文本或源代码。</p><p id="2a55" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">对于 C4，作者从 2019 年 4 月开始使用普通抓取，并对其应用了一些清理过滤器:</p><ol class=""><li id="b18e" class="ni nj it kx b ky kz lb lc le nk li nl lm nm lq nn no np nq bi translated">保留仅以有效的结尾标点符号(句号、感叹号、问号或结束引号)结尾的句子。</li><li id="a59f" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">移除任何包含出现在“<a class="ae ku" href="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words" rel="noopener ugc nofollow" target="_blank">肮脏、淘气、淫秽或其他不良词语列表</a>中的攻击性词语的页面。</li><li id="46d5" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">“必须启用 JavaScript”类型警告通过过滤掉任何包含单词 JavaScript 的行来移除。</li><li id="66a5" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">带有占位符文本(如“lorem ipsum ”)的页面将被移除。</li><li id="6adc" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">通过移除任何包含花括号“{”的页面来移除源代码(因为花括号出现在许多众所周知的编程语言中)。</li><li id="7308" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">为了消除重复，考虑三句话的跨度。相同的 3 个句子的任何重复出现被过滤掉。</li><li id="c48e" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">最后，由于下游任务多为英语语言，<a class="ae ku" href="https://pypi.org/project/langdetect/" rel="noopener ugc nofollow" target="_blank"> langdetect </a>用于过滤掉任何概率至少为 0.99 的未归类为英语的页面。</li></ol><p id="2a81" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这产生了 750GB 的数据集，不仅比大多数预训练数据集合理地大，而且包含相对非常干净的文本。</p><h2 id="5b01" class="mo ls it bd lt mp mq dn lx mr ms dp mb le mt mu md li mv mw mf lm mx my mh mz bi translated">输入和输出表示</h2><p id="042a" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">这是 T5 的主要关注点之一，因为这使得统一的文本到文本的方法成为可能。为了使所有下游任务使用相同的模型，将特定于任务的文本前缀添加到提供给模型的原始输入中。这个文本前缀也被认为是超参数。</p><blockquote class="nb nc nd"><p id="2731" class="kv kw ne kx b ky kz ju la lb lc jx ld nf lf lg lh ng lj lk ll nh ln lo lp lq im bi translated">例如，要求模型翻译句子“那很好”从英语到德语，模型将被输入序列“<strong class="kx iu">将英语翻译成德语:很好。</strong>“并将被训练输出”<strong class="kx iu"> Das ist gut。</strong></p><p id="bd59" class="kv kw ne kx b ky kz ju la lb lc jx ld nf lf lg lh ng lj lk ll nh ln lo lp lq im bi translated">— <a class="ae ku" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank"> T5 纸</a></p></blockquote><p id="8c74" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">类似地，对于分类任务，模型预测对应于目标标签的单个单词。</p><blockquote class="nb nc nd"><p id="7491" class="kv kw ne kx b ky kz ju la lb lc jx ld nf lf lg lh ng lj lk ll nh ln lo lp lq im bi translated">例如，在 MNLI 基准上，目标是预测一个前提是否暗示("<strong class="kx iu">蕴涵</strong>")、矛盾("<strong class="kx iu">矛盾</strong>")，或者都不是("<strong class="kx iu">中性</strong>")一个假设。经过我们的预处理，输入序列变成了"<strong class="kx iu"> mnli 前提:我讨厌鸽子。假设:我对鸽子充满了敌意。</strong>"与对应的目标词<strong class="kx iu"/>"蕴涵。</p><p id="fc9c" class="kv kw ne kx b ky kz ju la lb lc jx ld nf lf lg lh ng lj lk ll nh ln lo lp lq im bi translated">— <a class="ae ku" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank"> T5 纸</a></p></blockquote><p id="e642" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里有一个问题。如果预测的单词是其他的东西，即不是“蕴涵”、“矛盾”或“中性”的，该怎么办？嗯，在这种情况下，模型被训练成认为所有其他单词都是错误的。</p><h2 id="21e4" class="mo ls it bd lt mp mq dn lx mr ms dp mb le mt mu md li mv mw mf lm mx my mh mz bi translated">模型</h2><p id="a454" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">所提出的模型本质上是一个编码器-解码器变换器。艾尔。)进行了一些架构上的改变(如在子块之前应用层归一化，然后将初始输入添加到子块输出；也称为预规范)。而且，模型配置类似于 BERT base ( <a class="ae ku" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> Devlin et。艾尔。</a>)。</p><p id="42d9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们将跳过这些架构，因为它们超出了本文的范围。如果您特别想了解这些型号的规格，我已经在下面的文章中介绍了它们:</p><ol class=""><li id="52f9" class="ni nj it kx b ky kz lb lc le nk li nl lm nm lq nn no np nq bi translated">变形金刚:<a class="ae ku" rel="noopener" target="_blank" href="/transformers-explained-65454c0f3fa7">https://towards data science . com/transformers-explained-65454 c0 F3 fa 7</a></li><li id="b2a1" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">变形金刚实现:<a class="ae ku" href="https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453" rel="noopener">https://medium . com/swlh/abstract ive-text-summary-using-transformers-3e 774 cc 42453</a></li><li id="12fa" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">BERT:<a class="ae ku" href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener">https://medium . com/swlh/BERT-pre-training-of-transformers-for-language-understanding-5214 FBA 4a 9 af</a></li></ol><h2 id="b1b5" class="mo ls it bd lt mp mq dn lx mr ms dp mb le mt mu md li mv mw mf lm mx my mh mz bi translated">培训方法</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi nw"><img src="../Images/e66ffe4239d02337a69614eb25d0f0ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OUVz8RfmUOB3cE1Ut3uMhQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">模型结构来自<a class="ae ku" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="2dde" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在架构层面上，选择训练方法有几种选择:本文是对许多现代语言理解方法的详尽调查。因此，已经研究和比较了许多架构规范。</p><ol class=""><li id="53f2" class="ni nj it kx b ky kz lb lc le nk li nl lm nm lq nn no np nq bi translated"><strong class="kx iu">编码器-解码器(左):</strong>这是标准的编码器-解码器，seq2seq 架构，其中编码器以伯特风格、<strong class="kx iu">完全可见</strong>的方式训练(即，每个标记都有助于序列中每个其他标记的注意力计算)，解码器以 GPT 风格<strong class="kx iu">因果</strong>的方式训练(即，序列中出现在它之前的所有标记都参与每个标记)。</li><li id="11b1" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated"><strong class="kx iu">语言模型(中):</strong>这本质上就是前面讨论过的因果注意机制。这是一种自回归建模方法。</li><li id="691f" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated"><strong class="kx iu">前缀 LM(右):</strong>这是 BERT 风格和语言模型方法的组合。例如，将英语翻译成德语的任务可以具有伯特式的注意力:“将英语翻译成德语:很好。目标:“。然后翻译成“这是直觉”将自回归出席。</li></ol><p id="6b6a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">通过实验，编码器-解码器方法获得了最佳结果。</p><h2 id="3834" class="mo ls it bd lt mp mq dn lx mr ms dp mb le mt mu md li mv mw mf lm mx my mh mz bi translated">无监督目标</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi ob"><img src="../Images/59c632175cdf8f7b64c09360d306f562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xYqyt4Gzl1SAioip9NdaXQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">从<a class="ae ku" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">到</a>纸张的损坏跨度</p></figure><p id="0d26" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">关于培训前的目标，作者也在实践中探索了一些方法:</p><ol class=""><li id="dbe4" class="ni nj it kx b ky kz lb lc le nk li nl lm nm lq nn no np nq bi translated"><strong class="kx iu">语言建模:</strong>这种方法主要包括因果预测任务，即考虑该单词之前的所有单词来预测句子中的下一个单词。</li><li id="da54" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated"><strong class="kx iu">去混洗:</strong>将一个句子中的所有单词进行混洗，并训练模型来预测原文。</li><li id="3ae1" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated"><strong class="kx iu"> Corrupting Spans: </strong>屏蔽句子中的单词序列，并训练模型预测这些屏蔽的单词，如上图所示。它也被称为去噪目标。</li></ol><p id="fb23" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">经过探索，去噪目标具有最有希望的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nx ny di nz bf oa"><div class="gh gi oc"><img src="../Images/d879bb27f06a8eca4d79d559c74f31c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YHQBMsSowhn4Swl-ctvhDw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">从<a class="ae ku" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">论文中探索无监督目标</a></p></figure><h1 id="af42" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">结果</h1><p id="8df2" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">首先，T5 已经在许多粘合、强力胶任务以及翻译和摘要基准测试中达到了最先进的水平。</p><blockquote class="nb nc nd"><p id="fa9b" class="kv kw ne kx b ky kz ju la lb lc jx ld nf lf lg lh ng lj lk ll nh ln lo lp lq im bi translated">T5 惊人地擅长这项任务。完整的 110 亿参数模型在<a class="ae ku" href="https://nlp.cs.washington.edu/triviaqa/" rel="noopener ugc nofollow" target="_blank"> TriviaQA </a>、<a class="ae ku" href="https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a" rel="noopener ugc nofollow" target="_blank">网络提问</a>和<a class="ae ku" href="https://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html" rel="noopener ugc nofollow" target="_blank">自然问题</a>上分别有 50.1%、37.4%和 34.5%的时间产生答案的精确文本。</p><p id="f0e6" class="kv kw ne kx b ky kz ju la lb lc jx ld nf lf lg lh ng lj lk ll nh ln lo lp lq im bi translated">— <a class="ae ku" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a></p></blockquote><p id="f881" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了生成真实的文本，T5 依赖于填空类型的任务，由于预先训练，T5 对填空类型的任务很熟悉。因此，作者创建了一个新的下游任务，称为<strong class="kx iu">大小的填空</strong>。例如，给定句子“<em class="ne">我喜欢吃花生酱和 _4_ 三明治，【T11”)，模型将被训练来预测空白的大约 4 个单词。</em></p><blockquote class="nb nc nd"><p id="89c1" class="kv kw ne kx b ky kz ju la lb lc jx ld nf lf lg lh ng lj lk ll nh ln lo lp lq im bi translated"><strong class="kx iu">有趣的事实:该模型还根据所需的缺失文本大小来调整其预测。</strong></p></blockquote><p id="3d19" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">以上的演示，参考<a class="ae ku" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">官博</a>。</p><h1 id="a01d" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">把所有的放在一起</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/ce7a8b6828cd529f31c818c04dc50c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*8WIzJDlbitZVDGeA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">通过<a class="ae ku" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a>对 T5 进行预训练和微调</p></figure><ul class=""><li id="b04e" class="ni nj it kx b ky kz lb lc le nk li nl lm nm lq od no np nq bi translated">T5 首先在 C4 数据集上被预训练，用于利用编码器-解码器架构的去噪、破坏跨度目标。</li><li id="6e23" class="ni nj it kx b ky nr lb ns le nt li nu lm nv lq od no np nq bi translated">然后在下游任务上使用监督目标进行微调，并为文本到文本设置建立适当的输入模型。</li></ul><h1 id="6e75" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">结论</h1><p id="378d" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">在这篇文章中，我们深入研究了 Google 的 T5 模型，它是语言理解领域最先进的模型之一。我们看到了新的数据集:C4。这篇文章的主要收获是 T5 作者获得的关于训练方法、模型架构和数据集的实证结果。此外，还可以观察到，DL 越来越接近实现人类质量理解——在这种情况下，概括为许多 NLP 任务的一个模型。</p><p id="f25a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">github repo:<a class="ae ku" href="https://github.com/google-research/text-to-text-transfer-transformer" rel="noopener ugc nofollow" target="_blank">https://github . com/Google-research/text-to-text-transfer-transformer</a></p><p id="fada" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">huggingface 用于模型架构和预训练权重的 API:<a class="ae ku" href="https://huggingface.co/transformers/model_doc/t5.html" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/transformers/model_doc/t5.html</a></p><p id="f19e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">https://www.tensorflow.org/datasets/catalog/c4 C4 张量流数据集:<a class="ae ku" href="https://www.tensorflow.org/datasets/catalog/c4" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="178f" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">参考</h1><p id="c110" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">T5 论文:【https://arxiv.org/abs/1910.10683 T2】</p><div class="oe of gp gr og oh"><a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">用 T5 探索迁移学习:文本到文本的迁移转换器</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">在过去的几年里，迁移学习在自然语言领域引发了一波新的前沿成果…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">ai.googleblog.com</p></div></div><div class="oq l"><div class="or l os ot ou oq ov ko oh"/></div></div></a></div><div class="oe of gp gr og oh"><a rel="noopener follow" target="_blank" href="/transformers-explained-65454c0f3fa7"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">变形金刚解释</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">对谷歌 Transformer 模型的详尽解释；从理论到实施</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">towardsdatascience.com</p></div></div><div class="oq l"><div class="ow l os ot ou oq ov ko oh"/></div></div></a></div><div class="oe of gp gr og oh"><a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener follow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">伯特:语言理解变形金刚的前期训练</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">了解基于变压器的自监督架构</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">medium.com</p></div></div><div class="oq l"><div class="ox l os ot ou oq ov ko oh"/></div></div></a></div></div></div>    
</body>
</html>