<html>
<head>
<title>Overview of Clustering Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聚类算法概述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overview-of-clustering-algorithms-27e979e3724d?source=collection_archive---------40-----------------------#2020-07-20">https://towardsdatascience.com/overview-of-clustering-algorithms-27e979e3724d?source=collection_archive---------40-----------------------#2020-07-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ff15" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">实践聚类算法:Python 中的演练！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d234406ff6e7511a761eced6c4414c63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oXAR7zbmngd-bieH"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">凯利·西克玛在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="60f9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">使聚集</h1><p id="0e71" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">聚类是一种无监督的技术，其中相似数据点的集合被分组在一起以形成聚类。如果簇内(同一簇内的数据点)相似性高，而簇间(簇外的数据点)相似性低，则称该簇是好的。聚类也可以被视为一种<strong class="lt iu">数据压缩</strong>技术，其中一个聚类的数据点可以被视为一个组。聚类也称为<strong class="lt iu">数据分割</strong>，因为它对数据进行分区，使得一组相似的数据点形成一个聚类。</p><h2 id="bf20" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">聚类与分类有何不同？</h2><p id="a9ff" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">分类算法是区分组和分类的好技术。分类需要手动标注数据，当数据集很大时，这是一个很累的过程。反过来的过程怎么样？即将相似的数据点分割在一起，并给它们分配聚类标签。聚类算法不需要标签来进一步处理，因为它是一种无监督的技术。</p><h1 id="7aab" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">集群的要求</h1><ul class=""><li id="31cf" class="mz na it lt b lu lv lx ly ma nb me nc mi nd mm ne nf ng nh bi translated">可量测性</li><li id="f971" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">应对不同属性的潜力</li><li id="c710" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">对噪声和异常值的鲁棒性</li><li id="1aff" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">面对高维数据的能力</li></ul><h1 id="6188" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">聚类方法</h1><p id="25c5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最常见的聚类方法是，</p><ul class=""><li id="3892" class="mz na it lt b lu nn lx no ma np me nq mi nr mm ne nf ng nh bi translated">分割方法</li><li id="0cb8" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">分层方法</li><li id="e6c1" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">基于密度的方法</li><li id="5578" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">基于模型的方法</li></ul><blockquote class="ns nt nu"><p id="c7e9" class="lr ls nv lt b lu nn ju lw lx no jx lz nw nx mc md ny nz mg mh oa ob mk ml mm im bi translated"><strong class="lt iu"> <em class="it">分区方法:</em> </strong> <em class="it">分区方法包括对数据进行分区，对相似项的组进行聚类。该方法中常用的算法有，</em></p></blockquote><ul class=""><li id="89d7" class="mz na it lt b lu nn lx no ma np me nq mi nr mm ne nf ng nh bi translated">k 均值</li><li id="3797" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">k-水母类</li><li id="da48" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">k 模式</li></ul><blockquote class="ns nt nu"><p id="e9c7" class="lr ls nv lt b lu nn ju lw lx no jx lz nw nx mc md ny nz mg mh oa ob mk ml mm im bi translated"><strong class="lt iu"> <em class="it">分层方法:</em> </strong> <em class="it">分层方法是将数据进行分层分解。存在两种类型的分层方法，</em></p></blockquote><ul class=""><li id="efd5" class="mz na it lt b lu nn lx no ma np me nq mi nr mm ne nf ng nh bi translated">凝聚(自下而上的方法)</li><li id="87fa" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">分裂(自上而下的方法)</li></ul><blockquote class="ns nt nu"><p id="44c7" class="lr ls nv lt b lu nn ju lw lx no jx lz nw nx mc md ny nz mg mh oa ob mk ml mm im bi translated"><strong class="lt iu"> <em class="it">基于密度的方法:</em> </strong> <em class="it">基于密度的方法用于异常检测。高密度的数据点被分组在一起，留下低密度的数据点。</em></p></blockquote><ul class=""><li id="d283" class="mz na it lt b lu nn lx no ma np me nq mi nr mm ne nf ng nh bi translated">带噪声应用的基于密度的空间聚类</li><li id="588a" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">光学(排序点以识别簇结构)</li></ul><blockquote class="ns nt nu"><p id="618d" class="lr ls nv lt b lu nn ju lw lx no jx lz nw nx mc md ny nz mg mh oa ob mk ml mm im bi translated"><strong class="lt iu"> <em class="it">基于模型的方法:</em> </strong> <em class="it">基于模型的方法涉及应用模型来寻找最佳的聚类结构。</em></p></blockquote><ul class=""><li id="604d" class="mz na it lt b lu nn lx no ma np me nq mi nr mm ne nf ng nh bi translated">EM(期望最大化)算法</li></ul><p id="0946" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma nx mc md me nz mg mh mi ob mk ml mm im bi translated">在这篇文章中，我们集中讨论一些<strong class="lt iu">分割方法和基于密度的方法的算法。</strong></p><h1 id="a5e1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">用 Python 加载所需的库</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><h1 id="548a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">分割方法</h1><h2 id="6395" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">1.k 均值聚类</h2><p id="7add" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">k 均值聚类是一种经典的聚类方法。K-Means 通过计算一个聚类的均值来迭代地重新定位聚类中心。</p><ul class=""><li id="d7a6" class="mz na it lt b lu nn lx no ma np me nq mi nr mm ne nf ng nh bi translated">最初，K-Means 随机选择<em class="nv"> k </em>个聚类中心。</li><li id="4cfd" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">计算每个数据点和聚类中心之间的距离(通常使用欧几里德距离)。数据点被分配给与其非常接近的聚类。</li><li id="bca9" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">在所有数据点被分配到一个聚类之后，该算法计算聚类数据点的平均值，并将聚类中心重新定位到其对应的聚类平均值。</li><li id="90e4" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">这个过程一直持续到聚类中心不变。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/3c754080f3bc97dfbc47e04e6b715a1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fCorh8GSH3OrpJqAwIs9Fw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PC:作者</p></figure><p id="4cb3" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma nx mc md me nz mg mh mi ob mk ml mm im bi translated">使用 K-Means 的优点是可伸缩性。K-Means 在大数据上表现很好。使用 K-Means 的主要缺点是对异常值敏感。在计算聚类的均值时，离群点会产生严重的影响。聚类结果根据<em class="nv"> k </em>值和聚类中心的初始选择而不同。K-Means 算法仅适用于球形数据，而不适用于任意形状的数据。</p><h2 id="efb3" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">2.k-模式聚类</h2><p id="9f5b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">K-Means 适用于连续数据。分类数据呢？K-Modes 聚类可以解决这个问题。该算法非常类似于 K-Means，但是 K-Modes 不是计算聚类的平均值，而是计算聚类的模式(一个经常出现的值)。</p><ul class=""><li id="3d7c" class="mz na it lt b lu nn lx no ma np me nq mi nr mm ne nf ng nh bi translated">最初，K-Mode 随机选择<em class="nv"> k </em>个聚类中心。</li><li id="28a4" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">计算每个数据点和聚类中心之间的相似性。数据点被分配给与其具有高相似性的聚类。</li><li id="ecef" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">在所有数据点被分配到一个聚类之后，该算法计算聚类数据点的模式，并将聚类中心重新定位到其对应的聚类模式。</li><li id="790e" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">这个过程一直持续到聚类中心不变。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><h2 id="d235" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">如何为<em class="of"> k </em>找到一个最优值？</h2><p id="fc03" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">肘方法和轮廓指数是最常用的方法来寻找 k 的最佳值。</p><h1 id="c349" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">基于密度的方法</h1><h2 id="d563" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">1.基于密度的噪声应用空间聚类</h2><p id="d7ad" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">DBSCAN(带噪声的应用程序的基于密度的空间聚类)是一种基于密度的聚类算法，能够对任意形状的数据执行良好的操作。DBSCAN 查找密集的数据点，并将其分配给一个聚类，将不太密集的数据点从该聚类中分离出来</p><p id="0c3e" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma nx mc md me nz mg mh mi ob mk ml mm im bi translated"><strong class="lt iu">数据库扫描术语</strong></p><ul class=""><li id="13a2" class="mz na it lt b lu nn lx no ma np me nq mi nr mm ne nf ng nh bi translated">如果一个数据点<em class="nv"> q </em>在另一个数据点<em class="nv"> p </em>的<strong class="lt iu">半径ϵ </strong>内，那么该数据点<em class="nv"> q </em>被认为是该数据点<em class="nv"> p </em>的<strong class="lt iu">ϵ-neighborhood(ε邻域)</strong>。</li><li id="3e9f" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">如果点<em class="nv"> p </em>的ϵ-neighborhood 由<strong class="lt iu"> <em class="nv"> MinPts(最小点)中提到的值组成，则称数据点<em class="nv"> p </em>为<strong class="lt iu">核心对象</strong>。</em> </strong>例如，考虑<strong class="lt iu"> MinPts = 5，如果<em class="nv"> p </em>的ϵ-neighborhood 由 5 个数据点组成，则</strong> <em class="nv"> p </em>被称为核心对象。</li><li id="4ab5" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">如果点<em class="nv"> p </em>在<em class="nv"> q </em>的ϵ-neighborhood 内，则称一个数据点<em class="nv"> p </em>是从点<em class="nv"> q </em>直接密度可达的。</li></ul><p id="dd0b" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma nx mc md me nz mg mh mi ob mk ml mm im bi translated">它是如何工作的？DBSCAN 检查每个数据点的ϵ-neighborhood。如果<em class="nv"> p </em>是核心对象，并且其在ϵ-neighborhood 的数据点高于 MinPts 的值，则它在核心对象<em class="nv"> p </em>周围形成新的聚类。DBSCAN 迭代地直接寻找密度可达的数据点，并且可以合并其他少数聚类。该过程继续进行，直到不再有要分析的点。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/cbd17d7dc10119300d3f620fa1616fb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B79_1IVWCFSAlatOw3zQ8g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PC:作者</p></figure><p id="d507" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma nx mc md me nz mg mh mi ob mk ml mm im bi translated">使用 DBSCAN 的缺点是存在超参数ϵ和<em class="nv"> MinPts。</em>产生的结果因超参数的选择值而异。</p><h2 id="2ce2" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated">2.光学</h2><p id="0269" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu"> OPTICS(排序点以识别聚类结构)</strong>是一种聚类算法，克服了在 DBSCAN 中使用超参数所面临的缺点。光学与 DBSCAN 非常相似，但它使用灵活的半径ϵ.值</p><p id="ff22" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma nx mc md me nz mg mh mi ob mk ml mm im bi translated"><strong class="lt iu">光学术语</strong></p><ul class=""><li id="040c" class="mz na it lt b lu nn lx no ma np me nq mi nr mm ne nf ng nh bi translated">使点<em class="nv"> p </em>成为核心对象的𝜖′的最小值(小于𝜖′的值)称为<em class="nv"> p </em>的<strong class="lt iu">核心距离</strong>。</li><li id="57e4" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">p 的核距离和 p 与 q 之间的欧氏距离的最大值称为点 q 相对于另一个对象 p 的<strong class="lt iu">可达距离</strong></li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/50b6359ba483484137d793566b05fc5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6GlagN7uCUrVEM8v2IDcGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PC:作者</p></figure><h1 id="493c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><p id="97f6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">聚类在销售和营销行业中用于识别正确的客户群。网飞使用聚类算法将兴趣相似的观众分组。聚类是研究工作的一个领域，可用算法的许多变体都处于开发阶段。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="4892" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma nx mc md me nz mg mh mi ob mk ml mm im bi translated"><strong class="lt iu">在我的 Kaggle 笔记本中找到这篇文章:</strong><a class="ae ky" href="https://www.kaggle.com/srivignesh/overview-of-clustering-algorithms" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/srivignesh/overview-of-clustering-algorithms</a></p><p id="ed6d" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma nx mc md me nz mg mh mi ob mk ml mm im bi translated"><strong class="lt iu">参考文献:</strong></p><p id="ba9b" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma nx mc md me nz mg mh mi ob mk ml mm im bi translated">[1]韩家玮和 Micheline Kamber，数据挖掘:概念和技术第二版(2006)</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="2b36" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma nx mc md me nz mg mh mi ob mk ml mm im bi translated"><em class="nv">在</em><a class="ae ky" href="https://www.linkedin.com/in/srivignesh-rajan-123569151/" rel="noopener ugc nofollow" target="_blank"><em class="nv">LinkedIn</em></a><em class="nv"/><a class="ae ky" href="https://twitter.com/RajanSrivignesh" rel="noopener ugc nofollow" target="_blank"><em class="nv">Twitter</em></a><em class="nv">上与我联系！</em></p><p id="ad8c" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma nx mc md me nz mg mh mi ob mk ml mm im bi translated"><strong class="lt iu">快乐的机器学习！</strong></p><h2 id="79d3" class="mn la it bd lb mo mp dn lf mq mr dp lj ma ms mt ll me mu mv ln mi mw mx lp my bi translated"><strong class="ak">谢谢！</strong></h2></div></div>    
</body>
</html>