<html>
<head>
<title>Medical Report Generation Using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习的医疗报告生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-captioning-using-deep-learning-fe0d929cf337?source=collection_archive---------16-----------------------#2020-07-29">https://towardsdatascience.com/image-captioning-using-deep-learning-fe0d929cf337?source=collection_archive---------16-----------------------#2020-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f1df" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用编解码模型和注意机制生成胸部 x 光医学报告。</h2></div><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/94fe9c5bb6320b36de5c23b3fae6fc13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OOJL1gKkKr5y6Kv3LkMFkg.jpeg"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">由<a class="ae lg" href="https://unsplash.com/@donramxn?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">拉蒙·萨利内罗</a>在<a class="ae lg" href="https://unsplash.com/s/photos/technology?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="33a7" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">内容:</h1><ol class=""><li id="4c78" class="lz ma it mb b mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated"><strong class="mb iu">介绍</strong>——<em class="mr">理解问题</em></li><li id="f0a0" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><strong class="mb iu">先决条件</strong></li><li id="740b" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><strong class="mb iu">数据</strong></li><li id="bf59" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><strong class="mb iu">获取结构化数据</strong></li><li id="1e07" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><strong class="mb iu">准备文本数据— </strong> <em class="mr">自然语言处理</em></li><li id="e5f9" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><strong class="mb iu">获取图像特征</strong> — <em class="mr">迁移学习</em></li><li id="52e9" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><strong class="mb iu">输入管道</strong> — <em class="mr">数据生成器</em></li><li id="96db" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><strong class="mb iu">编解码模型</strong> — <em class="mr">训练、贪婪搜索、波束搜索、BLEU </em></li><li id="3f4d" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><strong class="mb iu">注意机制</strong>——<em class="mr">训练、贪婪搜索、光束搜索、BLEU </em></li><li id="a1ff" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><strong class="mb iu">总结</strong></li><li id="c261" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><strong class="mb iu">未来工作</strong></li><li id="9a19" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><strong class="mb iu">参考文献</strong></li></ol><h1 id="3021" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">1.介绍</h1><p id="93f2" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">图像字幕是一个具有挑战性的人工智能问题，它是指根据图像内容从图像中生成文本描述的过程。例如，请看下图:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi nk"><img src="../Images/b632bcf08d0bc62ecdf6494e9c6c8a44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbV6XW0sDABHT3vMePJqAA.jpeg"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">由<a class="ae lg" href="https://unsplash.com/@anthonytran?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Anthony Tran </a>在<a class="ae lg" href="https://unsplash.com/s/photos/woman-with-guitar?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片|你会如何描述这张照片？</p></figure><p id="f278" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">一个常见的答案是“<em class="mr">一个弹吉他的女人</em>”。我们人类可以看着一幅画，用适当的语言描述里面的任何东西。这很简单。让我给你看另一个:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/f593fc3bb594fc04c0503a11c77f6a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*sRnzSvO4J0dx5qerwFqsrA.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">嗯，你怎么描述这个？</p></figure><p id="d3de" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">对于我们这些“非放射科医生”来说，一个常见的答案是“胸透”。嗯，我们没有错，但是放射学家可能会有一些不同的解释。他们撰写文字报告，叙述成像研究中检查的身体各部位的发现，特别是各部位是否发现正常、异常或潜在异常。他们可以从一幅这样的图像中获得如此有价值的信息并做出医疗报告。</p><p id="ed4e" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">对于缺乏经验的放射科医生和病理学家，尤其是那些在医疗质量相对较低的农村地区工作的放射科医生和病理学家来说，撰写医学成像报告是一项艰巨的任务，或者另一方面，对于有经验的放射科医生和病理学家来说，撰写成像报告可能是乏味且耗时的。</p><p id="82b2" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">因此，为了解决所有这些问题，如果一台计算机能够像放射科医生那样将如上图所示的胸部 x 光片作为输入，并将结果以文本形式输出，岂不是很棒？</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi nr"><img src="../Images/1c26d5e80619e212552a6421ea7c7130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-G85uEegwKKSTP-Wpu6pfw.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">图片作者|我们现在是在要求机器做医生吗？！！</p></figure><p id="eb01" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">但是，你真的能写出这样的计算机程序吗？如果答案是否定的，你就不会读到这个故事。</p><h1 id="1e82" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">2.先决条件</h1><p id="9757" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">这项工作假设一些深度学习熟悉的主题，如神经网络，CNN，RNNs，迁移学习，Python 编程和 Keras 库。下面提到的两个模型将用于解决我们的问题，稍后将在本博客中简要说明:</p><ol class=""><li id="dc03" class="lz ma it mb b mc nl me nm mg ns mi nt mk nu mm mn mo mp mq bi translated">编码器-解码器模型</li><li id="4be2" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">注意机制</li></ol><p id="0601" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">对它们有足够的了解将有助于你更好地理解模型。</p><h1 id="95b4" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">3.数据</h1><p id="3c10" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">您可以从以下链接获得解决此问题所需的数据:</p><ul class=""><li id="7e0b" class="lz ma it mb b mc nl me nm mg ns mi nt mk nu mm nv mo mp mq bi translated"><a class="ae lg" href="http://academictorrents.com/details/5a3a439df24931f410fac269b87b050203d9467d" rel="noopener ugc nofollow" target="_blank"> <strong class="mb iu"> <em class="mr">图像</em> </strong> </a> <strong class="mb iu"> <em class="mr"> - </em> </strong>包含所有胸部 x 光片。</li><li id="2ab4" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm nv mo mp mq bi translated"><a class="ae lg" href="http://academictorrents.com/details/66450ba52ba3f83fbf82ef9c91f2bde0e845aba9" rel="noopener ugc nofollow" target="_blank"> <strong class="mb iu"> <em class="mr">报表</em></strong></a><strong class="mb iu"><em class="mr">-</em>-</strong>包含了上图对应的报表。</li></ul><p id="ac3d" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">图像数据集包含一个人的多张胸部 x 光照片。例如:x 射线的侧视图、多个正视图等。正如放射科医师使用所有这些图像来编写发现一样，模型也将一起使用所有这些图像来生成相应的发现。数据集中有 3955 个报告，每个报告都有一个或多个相关联的图像。</p><h2 id="e325" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">3.1.从 XML 文件中提取所需的数据</h2><p id="f89a" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">数据集中的报告是 XML 文件，每个文件对应一个人。与该人相关联的图像 id 和相应的发现包含在这些文件中。下面显示了一个示例:</p><div class="kr ks kt ku gt ab cb"><figure class="oi kv oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/537587d053ddb5e1ec968c0645373c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*FELqIdwnMaWM_BA359zC5A.png"/></div></figure><figure class="oi kv oo ok ol om on paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/e43cfbaa66302aefbd7b1b006ba87a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*FR7XJ0DV9g-TJvGesEUs3g.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk op di oq or translated">作者图片|左图显示文件中的“发现”,而右图显示同一文件的给定图片 id</p></figure></div><p id="dd1d" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">突出显示的信息是您需要从这些文件中提取的内容。这可以在 python 的 XML 库的帮助下完成。</p><p id="c7f8" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">注:调查结果也将被称为报告。它们将在博客的其余部分互换使用。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">从 XML 文件中提取结果和报告</p></figure><h1 id="2303" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">4.获取结构化数据</h1><p id="6488" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">从 XML 文件中提取所需数据后，数据被转换成结构化格式，以便于理解和访问。如前所述，有多个图像与单个报告相关联。因此，我们的模型在生成报告时也需要看到这些图像。但是有些报告只有 1 个相关联的图像，而有些报告有 2 个，最多 4 个。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ou"><img src="../Images/fcd6052cb1a9b0f1fadac2f3d1bab913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gqAU3meXr30vuUdmTdDnPA.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">按作者分类的图片|显示每份报告相关图片数量的图表</p></figure><p id="4873" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">所以问题来了，我们应该一次向模型中输入多少图像来生成一个报告？为了使模型输入一致，一次选择成对的图像，即两幅图像作为输入。如果一个报表只有一个图像，那么同一个图像将作为第二个输入被复制。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ov"><img src="../Images/ef306eabb0ab7de7be7044e4760e8cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IKEsz-LxHjnHSTzlJCDK4Q.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">作者图片|结构化数据外观一览</p></figure><p id="05fe" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">现在我们有了一个合适的、可理解的结构化数据来处理。图像以其绝对地址的名称保存。这将有助于加载数据。</p><h2 id="7359" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">训练-测试 _ 拆分</h2><p id="4eb4" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">在最后一步之后，我们将多次获得一个人的数据点。例如，如果一个人有四个与报告相关联的图像，并且由于我们正在拍摄成对的图像，将会生成该人的多个数据点。<strong class="mb iu">因此，有必要将该数据集分割成个体，而不是生成的数据点，以避免数据泄露问题</strong>。</p><p id="d595" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">在制作图像对之前，确保使用独特的“person_id”功能将数据分为 train、cv 和 test，然后训练模型。</p><h1 id="05b2" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">5.准备文本数据</h1><p id="065d" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">从 XML 文件中获得结果后，在将结果输入模型之前，应该对它们进行适当的清理和准备。下图显示了清理前的一些结果示例。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ow"><img src="../Images/0197f47960fddfaefb95876068ede65a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VNtDVl9cAvG333hkdJcrTw.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">作者图片|清理前的示例调查结果</p></figure><p id="f738" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们将通过以下方式清理文本:</p><ol class=""><li id="a4b7" class="lz ma it mb b mc nl me nm mg ns mi nt mk nu mm mn mo mp mq bi translated">将所有字符转换成小写。</li><li id="cbcb" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">执行基本的解收缩，即像不会、不能等单词将分别转换为不会、不能等。</li><li id="9b55" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">删除文本中的标点符号。请注意，句号不会被删除，因为调查结果包含多个句子，所以我们需要模型通过识别句子以类似的方式生成报告。</li><li id="5f05" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">删除文本中的所有数字。</li><li id="70e1" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">删除所有长度小于或等于 2 的单词。例如，删除了“是”、“到”等。这些话没有提供太多信息。但是“不”这个词不会被删除，因为它增加了价值。在句子中加上“不”会完全改变它的意思。因此，在执行这些清洁步骤时，我们必须小心。你需要确定哪些词要保留，哪些词要避免。</li><li id="0b6f" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">还发现一些文本包含多个句号、空格或重复多次的“X”。这样的字符也被删除。</li></ol><p id="ca0a" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们将开发的模型将生成一个报告，给出两个图像的组合，该报告将一次生成一个单词。将提供先前生成的单词序列作为输入。因此，我们将需要一个'<em class="mr">首字</em>来开始生成过程，并需要一个'<em class="mr">尾字'</em>来表示报告的结束。为此，我们将使用字符串'<em class="mr"> startseq' </em>和'<em class="mr"> endseq' </em>。这些字符串被添加到我们的发现。现在这样做很重要，因为当我们对文本进行编码时，我们需要这些字符串被正确编码。</p><p id="1844" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">编码文本的主要步骤是创建从单词到唯一整数值的一致映射，称为<strong class="mb iu">标记化</strong>。为了让我们的计算机理解任何文本，我们需要以我们的机器能够理解的方式来分解那个单词或句子。如果不执行标记化，我们就无法处理文本数据。标记化是一种将一段文本分成称为标记的更小单元的方法。记号可以是单词或字符，但在我们的例子中是单词。Keras 为此提供了一个内置的库。</p><pre class="kr ks kt ku gt ox oy oz pa aw pb bi"><span id="cfb0" class="nw li it oy b gy pc pd l pe pf">from tensorflow.keras.preprocessing.text import Tokenizer<br/>tokenizer = Tokenizer(filters='!"#$%&amp;()*+,-/:;&lt;=&gt;?@[\\]^_`{|}~\t\n')<br/>tokenizer.fit_on_texts(reports)</span></pre><p id="86b3" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">现在，我们已经对文本进行了适当的清理和标记，以备将来使用。在我的 GitHub 账户中可以找到所有这些的完整代码，在这个故事的结尾提供了它的链接。</p><h1 id="0a56" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">6.获取图像特征</h1><p id="1f00" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">图像和部分报告是我们模型的输入。我们需要将每张图像转换成固定大小的向量，然后作为模型的输入。为此，我们将使用<strong class="mb iu">迁移学习</strong>。</p><blockquote class="pg"><p id="e135" class="ph pi it bd pj pk pl pm pn po pp mm dk translated"><em class="pq">“在迁移学习中，我们首先在一个基本数据集和任务上训练一个基本网络，然后我们重新调整已学习的特征，或者将它们转移到第二个目标网络，以便在目标数据集和任务上进行训练。如果特性是通用的，即既适合基本任务又适合目标任务，而不是特定于基本任务，那么这个过程将会起作用。”</em></p></blockquote><p id="b682" class="pw-post-body-paragraph mx my it mb b mc pr ju mz me ps jx na mg pt nc nd mi pu nf ng mk pv ni nj mm im bi translated">VGG16、VGG19 或 InceptionV3 是迁移学习常用的 CNN。这些是在像 Imagenets 这样的数据集上训练的，这些数据集的图像与胸部 x 光照片完全不同。所以从逻辑上来说，他们似乎不是我们任务的好选择。那么，我们应该使用哪个网络来解决我们的问题呢？</p><p id="327d" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">如果你不熟悉，我给你介绍一下<strong class="mb iu"> CheXNet </strong>。CheXNet 是一个在 ChestX-ray14 上训练的 121 层卷积神经网络，ChestX-ray 14 是目前最大的公开可用胸部 X 射线数据集，包含 14 种疾病的 100，000 多幅正面 X 射线图像。然而，我们在这里的目的不是对图像进行分类，而仅仅是获得每个图像的瓶颈特征。因此，不需要该网络的最后一个分类层。</p><p id="f737" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">你可以从<a class="ae lg" href="https://drive.google.com/file/d/19BllaOvs2x5PLV_vlWMy4i8LapLb2j6b/view" rel="noopener ugc nofollow" target="_blank"> <strong class="mb iu"> <em class="mr">这里</em> </strong> </a>下载 CheXNet 训练好的权重。</p><pre class="kr ks kt ku gt ox oy oz pa aw pb bi"><span id="d237" class="nw li it oy b gy pc pd l pe pf">from tensorflow.keras.applications import densenet</span><span id="0d99" class="nw li it oy b gy pw pd l pe pf">chex = densenet.DenseNet121(include_top=False, weights = None,   input_shape=(224,224,3), pooling="avg")</span><span id="f160" class="nw li it oy b gy pw pd l pe pf">X = chex.output<br/>X = Dense(14, activation="sigmoid", name="predictions")(X)</span><span id="c2d2" class="nw li it oy b gy pw pd l pe pf">model = Model(inputs=chex.input, outputs=X)</span><span id="3c79" class="nw li it oy b gy pw pd l pe pf">model.load_weights('load_the_downloaded_weights.h5')</span><span id="101c" class="nw li it oy b gy pw pd l pe pf">chexnet = Model(inputs = model.input, outputs = model.layers[-2].output)</span></pre><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi px"><img src="../Images/941be9433c7776557f71fa05bde28269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_7J4z5s5o1qm4tx2nz1nSQ.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">作者图片 CheXNet 的最后几层</p></figure><p id="8d91" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">如果你忘了，我们有 2 张图片作为模型的输入。下面是瓶颈特征是如何获得的:</p><p id="4e0f" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">每个图像的大小被调整为(224，224，3)并通过 CheXNet，获得 1024 长度的特征向量。稍后，这两个特征向量被连接以获得 2048 个特征向量。如果你注意到，我们已经添加了一个平均池层作为最后一层。这有一个<strong class="mb iu">具体原因</strong>。因为我们要连接两个图像，所以模型可能会知道一些连接的顺序。例如，image1 总是在 image2 之后，反之亦然，但这里不是这样。在连接它们时，我们不保持任何顺序。这个问题是通过创建位置差异的池来解决的。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi py"><img src="../Images/f89732ef4a658339f4ac9872fdbab9be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uxjvxR7I1b2bq2nvuVbJww.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">作者图片|获取图片特征</p></figure><p id="a510" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">这方面的代码如下:</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="da0c" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">这些特性以 pickle 格式存储在一个字典中，可以供将来使用。</p><h1 id="6961" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">7.输入管道</h1><p id="6f54" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">考虑这样一种情况，您有大量的数据，多到无法一次将所有数据都存储在 RAM 中。购买更多 RAM 显然不是每个人的选择。</p><p id="76df" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">解决方案可以是将我们的小批量数据动态地输入到模型中。这正是<strong class="mb iu">数据生成器</strong>要做的事情。它们可以动态生成模型输入，从而形成从存储器到 RAM 的管道，以便在需要时加载数据。这种管道的另一个优点是，在准备将数据输入模型时，可以很容易地对这些小批量数据应用预处理例程。</p><p id="55e1" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们将使用<a class="ae lg" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank"><strong class="mb iu"><em class="mr">TF . data</em></strong></a>来解决我们的问题。</p><p id="1811" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们首先将数据集分为两部分，一个训练数据集和一个验证数据集。在划分的时候，要确保你有足够的数据点用于训练，也有足够的数据点用于验证。我选择的比例允许我在训练集中有 2560 个数据点，在验证集中有 1147 个数据点。</p><p id="4318" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">现在是时候为数据集创建生成器了。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">数据生成程序</p></figure><p id="c1c7" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">这里我们创建了两个数据生成器，用于训练的 train_dataset 和用于验证的 cv_dataset。create_dataset 函数获取 id(这是字典的键，用于前面创建的瓶颈特性)和预处理报告，并创建生成器。生成器一次生成 BATCH_SIZE 数量的数据点。</p><p id="5d7a" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">如前所述，我们将要创建的模型将是一个逐字逐句的模型。该模型将图像特征和部分序列作为输入，以生成序列中的下一个单词。</p><p id="12fc" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">例如:假设对应于“图像 _ 特征 _1”的报告是“开始序列心脏轮廓和纵隔尺寸在正常范围内结束序列”。</p><p id="038e" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">那么输入序列将被分成 11 个输入-输出对来训练模型:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pz"><img src="../Images/673aede81ac0a01190350d694d51d31d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*55P4cuMudCKl-DvWhQ2_tQ.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">作者图片|模型将图片和部分报告作为输入，并输出输出单词</p></figure><p id="1673" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">注意</strong>我们是<strong class="mb iu">而不是</strong>通过生成器创建这些输入输出对。生成器一次只为我们提供 BATCH_SIZE 数量的图像特征及其相应的完整报告。输入-输出对是在训练过程中稍后生成的，稍后将对此进行解释。</p><h1 id="1114" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">8.编码器-解码器模型</h1><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qa"><img src="../Images/8aed6a2f9b9032e9f171f65be8d82659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o5rtb9TUrbwX7MNVTQQqCQ.jpeg"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">照片由<a class="ae lg" href="https://unsplash.com/@skabrera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Sergi Kabrera </a>在<a class="ae lg" href="https://unsplash.com/s/photos/sequence?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a> |让我们解码一些序列！！</p></figure><p id="06a3" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">序列到序列模型是一种深度学习模型，它采用一个项目序列(在我们的情况下，是图像的特征)并输出另一个项目序列(报告)。</p><p id="92d2" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">编码器处理输入序列中的每一项，它将捕获的信息编译成一个称为上下文的向量。处理完整个输入序列后，编码器将上下文发送给解码器，解码器开始逐项产生输出序列。</p><p id="1c26" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">在我们的例子中，编码器是一个 CNN，它通过获取我们的图像特征来产生一个上下文向量。解码器是一个递归神经网络。</p><p id="fee0" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">Marc Tanti 在他的论文<a class="ae lg" href="https://arxiv.org/abs/1703.09137" rel="noopener ugc nofollow" target="_blank"> <strong class="mb iu"> <em class="mr">中介绍了许多架构，如 init-inject、par-inject、pre-inject 和 merge，指定了在创建图像标题生成器时应该在哪里注入图像。对于我们的问题，我们将使用他的论文中指定的合并架构。</em></strong></a></p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qb"><img src="../Images/6256db4a403bf0b66732f2c7d8818590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xf1Jjo36bQSev7OsegFRFw.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">作者图片|建筑</p></figure><p id="3d8d" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">在“合并”架构中，RNN 在任何点都不暴露于图像向量(或从图像向量导出的向量)。相反，在前缀被 RNN 完整编码后，图像被引入语言模型。这是一个后期绑定架构，它不会在每个时间步修改图像表示。</p><p id="cb43" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">他的论文中的一些重要结论被用于我们实现的架构中。它们是:</p><ul class=""><li id="abca" class="lz ma it mb b mc nl me nm mg ns mi nt mk nu mm nv mo mp mq bi translated">RNN 输出需要与辍学正规化。</li><li id="4d96" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm nv mo mp mq bi translated">图像向量不应该具有非线性激活函数，也不应该用丢失来正则化。</li><li id="fa91" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm nv mo mp mq bi translated">图像输入向量在被馈送到神经网络之前必须被归一化，这是在从 CheXNet 获得特征时完成的。</li></ul><p id="e257" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">嵌入层:</strong></p><p id="376c" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">单词嵌入是一类使用密集向量表示来表示单词和文档的方法。Keras 提供了一个<a class="ae lg" href="https://keras.io/api/layers/core_layers/embedding/#embedding" rel="noopener ugc nofollow" target="_blank"> <strong class="mb iu"> <em class="mr">嵌入</em> </strong> </a>层，可用于文本数据上的神经网络。它也可以用一个从别处学来的单词嵌入。在自然语言处理领域中，学习、保存和自由使用单词嵌入是很常见的。</p><p id="b95b" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">在我们的模型中，利用嵌入层，每个单词已经使用预先训练的<a class="ae lg" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <strong class="mb iu"> <em class="mr">手套</em> </strong> </a>模型被映射成 300 维表示。使用预训练嵌入时，请记住，应通过设置参数“trainable = False”来冻结层的权重，以便权重在训练时不会更新。</p><p id="f363" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">型号代码:</strong></p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="27b9" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">型号汇总:</strong></p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qc"><img src="../Images/e7683c98740fc8a9b6332ecff0d35c3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tT4Yyt18B8nVOGvTJE6y_w.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">模型参数汇总</p></figure><h2 id="ddf6" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">8.1 培训</h2><p id="31d1" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated"><strong class="mb iu">损失函数:</strong></p><p id="2415" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">为此问题创建了掩蔽损失函数。例如:</p><p id="b1db" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">如果我们有一个令牌序列- [3]、[10]、[7]、[0]、[0]、[0]、[0]、[0]</p><p id="5cb2" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们在这个序列中只有 3 个单词，零对应于实际上不是报告一部分的填充。但是模型会认为零也是序列的一部分，并开始学习它们。当模型开始正确预测零点时，损失将会减少，因为对于模型来说，它正在正确地学习。但对我们来说，只有当模型正确预测实际单词(非零)时，损失才会减少。</p><p id="6c8e" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">因此，我们应该屏蔽序列中的零，这样模型就不会关注它们，只学习报告中需要的单词。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">隐蔽损失</p></figure><p id="080a" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">输出字是一个热编码，因此分类交叉熵将是我们的损失函数。</p><pre class="kr ks kt ku gt ox oy oz pa aw pb bi"><span id="42f3" class="nw li it oy b gy pc pd l pe pf">optimizer = tf.keras.optimizers.Adam(0.001)<br/>encoder_decoder.compile(optimizer, loss = maskedLoss)</span></pre><p id="4a77" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">还记得我们的数据生成器吗？现在是时候使用它们了。</p><p id="f7e2" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">这里，生成器提供的批次不是我们用于训练的实际数据批次。请记住，它们不是逐字的输入输出对。他们只是返回图像及其相应的整个报告。</p><p id="d739" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们将从生成器中检索每一批数据，并从该批数据中手动创建输入-输出序列，也就是说，我们将为训练创建我们自己的定制数据批。所以在这里，BATCH_SIZE 逻辑上是模型在单个批次中看到的图像对的数量。我们可以根据我们的系统能力改变它。我发现这种方法比其他博客中提到的传统定制生成器要快得多。</p><p id="d14b" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">由于我们正在创建自己的训练数据批次，因此我们将使用“train_on_batch”来训练我们的模型。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">培训步骤</p></figure><p id="4191" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">代码中提到的 convert 函数将来自生成器的数据转换为一个字一个字的输入输出对表示。然后，部分报告被填充到报告的最大长度。</p><p id="f9f5" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">转换功能:</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="8126" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">Adam optimizer 的学习率为 0.001。该模型被训练了 40 个时期，但是在第 35 个时期获得了最好的结果。由于随机性，您得到的结果可能会有所不同。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qd"><img src="../Images/50e5236946c12b5f1245e20ac0a47f2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8wU_ofFIrk70VTtqd5lquQ.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">图片由作者| Tensorboard 提供，显示了模型的损耗图</p></figure><p id="ec7c" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">注:</strong>以上培训已在 Tensorflow 2.1 中实现。</p><h2 id="a160" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">8.2 推理</h2><p id="15d7" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">既然我们已经训练了我们的模型，是时候准备我们的模型来预测报告了。</p><p id="c653" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">为此，我们必须对我们的模型进行一些调整。这将为我们节省一些测试时间。</p><p id="45b0" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">首先，我们将编码器和解码器部分从模型中分离出来。编码器预测的特征将与部分报告一起用作解码器的输入。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">推理设置</p></figure><p id="44de" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">通过这样做，我们将只需要预测编码器特征一次，同时我们将它用于我们的<strong class="mb iu">贪婪搜索</strong>和<strong class="mb iu">波束搜索</strong>算法。</p><p id="2d12" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们将实现这两种生成文本的算法，并看看哪一种效果最好。</p><h2 id="b154" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">8.3 贪婪搜索算法</h2><p id="24ef" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">贪婪搜索是一种算法范式，它一点一点地构建解决方案，总是选择下一个提供最明显好处的方案。</p><p id="2cd5" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu"> <em class="mr">贪婪搜索步骤</em> : </strong></p><ol class=""><li id="14e0" class="lz ma it mb b mc nl me nm mg ns mi nt mk nu mm mn mo mp mq bi translated">编码器输出我们图像的特征。编码器的工作到此结束。一旦我们有了我们需要的特性，我们就不需要关心编码器了。</li><li id="c2e3" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">这个特征向量连同开始标记-‘start seq’(我们的初始输入序列)作为第一个输入提供给解码器。</li><li id="d2fe" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">解码器预测整个词汇的概率分布，具有最大概率的单词将被选为下一个单词。</li><li id="d654" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">这个预测的字连同先前的输入序列将是我们对解码器的下一个输入序列。</li><li id="fa4d" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">继续步骤 3-4，直到我们遇到结束标记，即‘end seq’。</li></ol><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="2295" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">让我们检查一下在使用 greedysearch 生成报告之后，我们的模型表现如何。</p><blockquote class="qe qf qg"><p id="9760" class="mx my mr mb b mc nl ju mz me nm jx na qh nn nc nd qi no nf ng qj np ni nj mm im bi translated"><strong class="mb iu"> BLEU 评分—贪婪搜索:</strong></p></blockquote><p id="6ebc" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">双语评估替角评分</strong>，简称 BLEU，是一个将生成的句子评估为参考句子的度量。</p><p id="9f73" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">完全匹配的得分为 1.0，而完全不匹配的得分为 0.0。该方法通过计算候选文本中的 n 元语法与参考文本中的 n 元语法的匹配来工作，其中 1 元语法或单元语法将是每个单词，双元语法比较将是每个单词对。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/def861445e566e08e435fd4e112996bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*HaO1-uJNcXLdrl8aDcTH6A.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">通过贪婪搜索生成报告后的 BLEU 分数</p></figure><p id="ac4c" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">满分在实践中是不可能的，因为译文必须与参考文献完全匹配。这甚至是人类翻译所做不到的。用于计算 BLEU 分数的参考文献的数量和质量意味着跨数据集比较分数可能会很麻烦。</p><p id="cfcf" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">想了解更多关于 BLEU 的信息，点击<a class="ae lg" href="https://machinelearningmastery.com/calculate-bleu-score-for-text-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="mb iu"> <em class="mr">这里</em> </strong> </a>。</p><h2 id="0f27" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">8.4 波束搜索</h2><p id="f272" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">波束搜索是一种算法，它扩展了贪婪搜索并返回最可能的输出序列列表。每个序列都有一个与之相关的分数。得分最高的序列作为最终结果。</p><p id="9eee" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">在构建序列时，波束搜索不是贪婪地选择最有可能的下一步，而是扩展所有可能的下一步，并保持最有可能的<em class="mr"> k </em>，其中 k(称为波束宽度)是用户指定的参数，控制波束的数量或概率序列中的并行搜索。</p><p id="f9cb" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">波束宽度为 1 的波束搜索只不过是你的贪婪搜索。常见的波束宽度值为 5-10，但甚至高达 1000 或 2000 以上的值也用于研究，以从模型中挤出最佳性能。点击<a class="ae lg" href="https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/" rel="noopener ugc nofollow" target="_blank"> <strong class="mb iu"> <em class="mr">此处</em> </strong> </a>阅读更多关于光束搜索的内容。</p><p id="eaf6" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">但是请记住，随着波束宽度的增加，时间复杂度也会增加。因此，这些比贪婪搜索慢得多。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">波束搜索</p></figure><p id="1946" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">波束搜索并不总是保证更好的结果，但在大多数情况下，它给你一个。</p><p id="2211" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">您可以使用上面给出的函数检查光束搜索的 BLEU 分数。但是请记住，评估它们需要一段时间(几个小时)。</p><h2 id="b0d6" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">8.5 示例</h2><p id="372d" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">现在让我们来看一些胸部 x 光的预测报告:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/9bb3c7eb63fdad8a71222cf2fe26580e.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*mcPRvgkya2FofUZMrER_vg.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">训练图像对 1</p></figure><p id="13f3" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">图像对 1 的原始</strong>报告:<em class="mr">心脏正常大小。纵隔不明显。肺部没问题。</em></p><p id="6d78" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">预测的</strong>报告为图像对 1:<em class="mr">心脏正常大小。纵隔不明显。肺清</em>。”</p><p id="cfe2" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">对于这个示例，模型预测的是完全相同的报告。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/6a42c540811b88a79aa012445003ee8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*uWBG5f5NusBZH7qXOiLNsA.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">训练图像对 2</p></figure><p id="1f01" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">图像对 2 的原始</strong>报告:“<em class="mr">心脏大小和肺血管分布在正常范围内。未发现局部浸润性胸腔积液。</em></p><p id="bd9d" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">图像对 2 的预测</strong>报告:“<em class="mr">心脏大小和肺部血管分布在正常范围内。肺部是自由的局灶性空域疾病。没有胸腔积液。</em>”</p><p id="7b87" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">虽然不完全相同，但预测结果与原始报告几乎相似。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/20c82b7f7c3b62c285cc3da151455585.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*0hunViD18wIKvZFeJWcLyQ.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">训练图像对 3</p></figure><p id="c096" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">图像对 3 的原始</strong>报告:“<em class="mr">肺部过度膨胀但清晰。没有局灶性浸润渗出。心脏和纵隔轮廓在正常范围内。发现钙化的纵隔。</em></p><p id="5946" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">图像对 3 的预测</strong>报告:“<em class="mr">心脏大小正常。纵隔轮廓在正常范围内。肺部没有任何病灶浸润。没有结节肿块。没有可见的气胸。没有明显的胸腔积液。这是非常正常的。横膈膜下无可见的腹腔内自由空气</em></p><p id="0342" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">你并不期望这个模型完美无缺，对吧？没有一个模型是完美的，这个也不是。尽管从图像对 3 中可以正确识别出一些细节，但是产生了许多额外的细节，这些细节可能是正确的，也可能是不正确的。</p><p id="20f9" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们创建的模型绝不是完美的，但它确实为我们的图像生成了不错的报告。</p><p id="5883" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">现在让我们来看一个高级模型，看看它是否提高了当前的性能！！</p><h1 id="d572" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated"><strong class="ak"> 9。注意机制</strong></h1><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qm"><img src="../Images/094a2150d555e67f540b5a3eeb738d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xEOLl2-qU5fQCzkzEybNag.jpeg"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">保罗·斯科鲁普斯卡斯在<a class="ae lg" href="https://unsplash.com/s/photos/focus?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片|现在让我们来关注一下吧！！</p></figure><p id="1952" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">注意机制是对编解码模型的一种改进。上下文向量被证明是这类模型的瓶颈。这对他们处理长句子来说是一个挑战。在<a class="ae lg" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank"> <em class="mr"> Bahdanau 等人，2014 </em> </a>和<a class="ae lg" href="https://arxiv.org/abs/1508.04025" rel="noopener ugc nofollow" target="_blank"> <em class="mr"> Luong 等人，</em> 2015 </a>中提出了解决方案。这些论文介绍并完善了一种叫做“注意力”的技术，这种技术极大地提高了机器翻译系统的质量。注意力允许模型根据需要关注输入序列的相关部分。后来这个想法被实现为图像字幕在论文中，<a class="ae lg" href="https://arxiv.org/pdf/1502.03044.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mr">展示，出席，讲述:视觉</em>注意</a>的神经图像字幕生成。</p><p id="fa67" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">那么，我们如何为图像的注意力机制建模呢？</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qn"><img src="../Images/d824f3339cae0a3e7ae7702f39fad2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xE0FboJMCtsLE3d1eM_P3g.jpeg"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">马库斯·温克勒在<a class="ae lg" href="https://unsplash.com/s/photos/smileys?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="61f3" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">在文本的情况下，我们对输入序列的每个位置都有一个表示。但是对于图像，我们通常使用来自网络的完全连接层之一的表示，但是该表示不包含任何位置信息(想想看，它们是完全连接的)。我们需要查看图像的特定部分(位置)来描述那里有什么。例如，要从 x 光片上描述一个人心脏的大小，我们只需要看他的心脏区域，而不是他的手臂或任何其他部位。那么注意力机制的输入应该是什么呢？</p><p id="36e0" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">好吧，我们使用具有空间信息的卷积层之一(迁移学习)的输出，而不是完全连接的表示。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qo"><img src="../Images/9405443f91e82979320ee117e4705d80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tfRgmvO_uf_K1zJ-Cz8R5A.jpeg"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">图片作者| Conv 图层包含空间信息</p></figure><p id="fb80" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">例如，假设最后一个卷积层的输出是一个(7*14*1024)大小的特征图。这里，“7*14”是对应于图像中某些部分的实际位置，1024 是通道。我们不是关注通道，而是关注图像的位置。因此，这里我们有 7*14 = 98 个这样的位置。我们可以把它想象成 98 个位置，每个位置都有一个 1024 维的表示。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qp"><img src="../Images/8d620f8f23af1c57369432376e594581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2dPIbdRGtTMMyscojeNyQ.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">作者图片|关注组件</p></figure><p id="7cd4" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">现在我们有 98 个时间步，每个时间步有 1024 个维度表示。我们现在需要决定模型应该如何关注这 98 个时间步长或位置。一个简单的方法是给每个位置分配一些权重，得到所有这 98 个位置的加权和。如果特定的时间步长在预测输出时非常重要，则该时间步长将具有较高的权重。让这些重量被表示为阿尔法。</p><p id="e5d0" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">现在我们知道，阿尔法决定了一个特定位置的重要性。阿尔法值越高，重要性越高。但是我们如何找到α的值呢？没有人会给我们这些值，模型本身应该从数据中学习这些值。为此，我们定义了一个函数:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/e4121925940439a2dbaafd2d6b272dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*nnAnoA-2HFBCuPCrmd80uQ.jpeg"/></div></figure><p id="696a" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">这个量捕获了第 j 个输入对于解码第 t 个输出的重要性。h_j 是第 j 个位置表示，s_t-1 是解码器直到该点的状态。我们需要这两个提到的量来确定 e_jt。ATT 只是一个函数，我们将在后面定义。</p><p id="7741" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">在所有输入中，现在我们希望这个量(e_jt)的总和为 1。这就像一个概率分布，其中输入的重要性是多少。通过取 softmax 将 e_jt 转换成概率分布。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/d2f8dc5b07dacf683aa9ef5be08f45bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*STZNoSw0SfLkKtdct2L0fw.jpeg"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">使用 softmax 将 e_jt 转换为概率分布</p></figure><p id="b602" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">现在我们有阿尔法战士了。！阿尔法是我们 e _ jts 的软 max。α_ JT 表示聚焦于第 j 个输入以产生第 t 个输出的概率。</p><p id="fb92" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">是时候定义函数 f_ATT 了。许多其他可能的选择之一如下:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qs"><img src="../Images/8bc91c3fcb9f9626df1eb1afe0aff104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OKQqiP9kA-gNSIora_6kiA.jpeg"/></div></div></figure><p id="a7f4" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">v、U 和 W 是将在训练期间学习的参数，以确定 e_jt 的值。</p><p id="e501" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们有阿尔法，我们有输入，现在我们只需要得到加权和，以产生新的上下文向量，它将被馈送到解码器。实际上，这些模型比编码器解码器模型工作得更好。</p><h2 id="7784" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">模型实现:</h2><p id="e09c" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">像上面提到的编码器-解码器模型一样，这个模型也将由两部分组成，一个编码器和一个解码器，但这次解码器将有一个额外的注意力组件，即一步注意力解码器。为了更好地理解，现在让我们用代码编写上面解释的注意步骤:</p><pre class="kr ks kt ku gt ox oy oz pa aw pb bi"><span id="acf3" class="nw li it oy b gy pc pd l pe pf"><strong class="oy iu"><em class="mr">The onestep attention layer will consist of the attention part which<br/>will be calculated for each time step of the decoder.</em></strong></span><span id="7b42" class="nw li it oy b gy pw pd l pe pf"><strong class="oy iu"><em class="mr"># Calculating e_jts</em></strong><br/>score = self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))</span><span id="461c" class="nw li it oy b gy pw pd l pe pf"><strong class="oy iu"><em class="mr"># Converting our scores to probability distributions using softmax</em></strong><br/>attention_weights = tf.nn.softmax(score, axis=1)</span><span id="afa0" class="nw li it oy b gy pw pd l pe pf"><strong class="oy iu"><em class="mr"># Calculating the context vector(weighted sum)</em></strong><br/>context_vector = attention_weights * features</span></pre><p id="3032" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们将使用 keras 的子类 API，这给了我们更多的可定制性和对我们架构的控制。你可以从文档本身<strong class="mb iu"> <em class="mr">中阅读更多关于子类 API <a class="ae lg" href="https://www.tensorflow.org/guide/keras/custom_layers_and_models" rel="noopener ugc nofollow" target="_blank"> <strong class="mb iu"> <em class="mr">这里</em> </strong> </a> <strong class="mb iu"> <em class="mr"> </em> </strong>。</em>T13】</strong></p><p id="3772" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们将实现教师强制训练我们的模型，这一次我们不必将我们的文本转换成逐字逐句的模型。但是对于图像特征，我们将从 CheXNet 网络的最后一个 conv 层获取特征。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi px"><img src="../Images/3d7d4fa15e1f74dc91ecd56d9a8d2ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NzfIOwfDPOJ6z8AC-iVquw.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">作者图片|提取图像特征以引起注意</p></figure><p id="9c75" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">组合我们的两个图像后的最终输出形状将是(无，7，14，1024)。因此，整形后编码器的输入将是(None，98，1024)。为什么要整形？嗯，这已经在注意介绍中解释过了，如果你有任何疑问，请确保你再次阅读解释。</p><p id="238a" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">型号:</strong></p><ol class=""><li id="f326" class="lz ma it mb b mc nl me nm mg ns mi nt mk nu mm mn mo mp mq bi translated">编码器</li></ol><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="9f4c" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">编码器层只是对我们的图像执行一些操作，并输出将作为输入提供给解码器的特征。</p><p id="c57f" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">2.解码器</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="27e4" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">解码器为每个解码器时间步长调用单步关注层，并计算分数和关注权重。每个时间步的所有输出都存储在“所有输出”变量中。每个解码器步骤的输出是序列中的下一个字。“全力输出”将是我们的最终输出。</p><p id="d3e7" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">3.单步解码器</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="534e" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">4.创建模型</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div></figure><pre class="kr ks kt ku gt ox oy oz pa aw pb bi"><span id="628f" class="nw li it oy b gy pc pd l pe pf"><strong class="oy iu">model1 = Attention_Model(vocab_size, units, max_capt_len, att_units, BATCH_SIZE)</strong></span></pre><h2 id="99ab" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">9.1 培训</h2><p id="7537" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">如前所述，我们将使用教师强制来训练我们的模型。因此，我们将不需要我们用于编码器解码器模型的额外功能。我们可以直接使用来自生成器的自定义报告。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="0b10" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">“train_dataset”是我们的生成器，它为我们提供图像特征和相应的 155 维填充报告。</p><pre class="kr ks kt ku gt ox oy oz pa aw pb bi"><span id="aaba" class="nw li it oy b gy pc pd l pe pf"><strong class="oy iu"># teacher forcing<br/>res = model1.train_on_batch([img, rep[:,:-1]], rep[:,1:])</strong></span></pre><p id="7b43" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">可以看到，解码器的输入比输出落后一个时间步长。我们不希望模型预测的输入与输出相同。我们希望它能预测序列中的下一个单词。</p><p id="a13b" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">为了引起注意，Adam optimizer 以 0.001 的学习率使用。该模型仅用 10 个时期的训练就给出了不错的结果。由于随机性，您得到的结果可能会有所不同。稀疏分类交叉熵是在这种情况下使用的损失函数，因为我们没有将输出转换为 OHE 向量。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qt"><img src="../Images/b6a2cb4e74523431d6b3e76a46afa84c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Thk4nygAXyWyRRiwviJyHw.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">图片由作者| Tensorboard 提供，显示了损失图</p></figure><p id="ee18" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">所有东西的代码都可以从我的 GitHub 上获取。它的链接已经在这个博客的末尾提供了。</p><h2 id="33d1" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">9.2 推理</h2><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="os ot l"/></div></figure><h2 id="1adb" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">9.3 贪婪搜索</h2><p id="b0eb" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">现在我们已经建立了模型，让我们检查一下获得的 BLEU 分数是否确实比以前的模型有所改进:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi qu"><img src="../Images/cbc0568e6e2214920f3a9f7632de0a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*t464ymnn4Sfytj8YBCa4FQ.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">贪婪搜索后注意模型的 BLEU 分数</p></figure><p id="80c4" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">我们可以看到，它比带有贪婪搜索的编码器-解码器模型具有更好的性能。因此，这肯定是对前一个的改进。</p><h2 id="21cb" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">9.4 波束搜索</h2><p id="755c" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">现在让我们看看光束搜索的一些分数:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi qv"><img src="../Images/8e01979da49f56a06f002a91d34189db.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*v6Jb9oW0LpMxNp3a0nJ0yQ.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">波束搜索后的 BLEU 分数</p></figure><p id="da1f" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">BLEU 分数低于 greedy 分数，但相差不远。但是值得注意的是，随着波束宽度的增加，分数实际上也在增加。因此，可能存在一些 beam_width 值，分数实际上与贪婪值交叉。</p><h2 id="ee44" class="nw li it bd lj nx ny dn ln nz oa dp lr mg ob oc lt mi od oe lv mk of og lx oh bi translated">9.5 示例</h2><p id="eeba" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">以下是模型使用贪婪搜索生成的一些报告:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/146c43e1a90be44b5505df0ad3b49934.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*go7P_3v_400BOOXz2-EknA.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">训练图像对 1</p></figure><p id="06c7" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">图像对 1 的原始</strong>报告:“<em class="mr">心脏大小和肺血管分布在正常范围内。未发现局部浸润性胸腔积液。</em></p><p id="9c18" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">图像对 1 的预测</strong>报告:“<em class="mr">心脏大小和纵隔轮廓在正常范围内。肺部没问题。没有胸腔积液。没有急性骨质发现。</em></p><p id="d1a2" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">这些预测几乎与原始报告相似。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/b7c46059c2ed9c447b666d46b5650532.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*OD7_B73fUL3oOl2fPQ3cpw.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">训练图像对 2</p></figure><p id="03f1" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">图像对 2 的原始</strong>报告:“<em class="mr">心脏大小和肺部血管分布在正常范围内。肺部是自由的局灶性空域疾病。没有胸腔积液。</em>”</p><p id="a2d2" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">图像对 2 的预测</strong>报告:“<em class="mr">心脏大小和肺部血管分布在正常范围内。肺部是自由的局灶性空域疾病。没有胸腔积液。</em></p><p id="6210" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">预测的报告一模一样！！</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/8b7faf9527b705335fc0111ef6274241.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*kx7eiSI9GsjT71epUIXhMg.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">训练图像对 3</p></figure><p id="4b57" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">图像对 3 的原始</strong>报告:<em class="mr">心脏正常大小。纵隔不明显。肺部没问题。</em></p><p id="e881" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">预测的</strong>图像对 3 的报告:<em class="mr">心脏正常大小。纵隔不明显。肺部没问题。</em></p><p id="0402" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">在这个例子中，模型也做得非常好。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/93a4aa2d980463bd974be1fe0be6407c.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*KSA7JiByXieZL63m0vm-dQ.png"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">训练图像对 4</p></figure><p id="4b95" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">图像对 4 的原始报告:“<em class="mr">双肺清晰。具体来说，没有证据表明局灶性实变气胸胸腔积液。心脏纵隔轮廓不起眼。可视化骨骼结构胸部无急性异常。</em>”</p><p id="498d" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated"><strong class="mb iu">图像对 4 的预测</strong>报告:“<em class="mr">心脏大小和纵隔轮廓在正常范围内。肺部没问题。没有胸腔积液。</em>”</p><p id="ddce" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">你可以看到这个预测并没有真正的说服力。</p><p id="f440" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi qw translated"><span class="l ki kj kk bm kl km kn ko kp di">" B</span><strong class="mb iu"><em class="mr">ut</em></strong><em class="mr">本例中的波束搜索预测了完全相同的报告，尽管它对整个测试数据组合产生了较低的 BLEU 分数！！!"</em></p><p id="8504" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">那么，选哪个呢？好吧，这取决于我们。只需选择一个通用的方法。</p><p id="da1e" class="pw-post-body-paragraph mx my it mb b mc nl ju mz me nm jx na mg nn nc nd mi no nf ng mk np ni nj mm im bi translated">在这里，即使我们的注意力模型也不能准确预测每一幅图像。正如我们从示例中看到的，这一对没有侧视图像，或者如果我们查看原始报告中的单词，会发现有些复杂的单词通过一些 EDA 并不经常出现。这些可能是我们在某些情况下没有一个好的预测的一些原因。请记住，我们只是在 2560 个数据点上训练这个模型。为了了解更复杂的特征，模型将需要更多的数据。</p><h1 id="e9a5" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">10.摘要</h1><p id="afcb" class="pw-post-body-paragraph mx my it mb b mc md ju mz me mf jx na mg nb nc nd mi ne nf ng mk nh ni nj mm im bi translated">既然我们已经结束了这个项目，让我们总结一下我们所做的一切:</p><ul class=""><li id="782a" class="lz ma it mb b mc nl me nm mg ns mi nt mk nu mm nv mo mp mq bi translated">我们刚刚看到了图像字幕在医学领域的应用。我们理解这个问题和对这种应用的需要。</li><li id="eb6e" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm nv mo mp mq bi translated">我们看到了如何为输入管道使用数据生成器。</li><li id="bc8f" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm nv mo mp mq bi translated">创建了一个编码器-解码器模型，给了我们不错的结果。</li><li id="6678" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm nv mo mp mq bi translated">通过建立注意力模型改进了基本结果。</li></ul><h1 id="a9e7" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">11.未来的工作</h1><ul class=""><li id="f6e5" class="lz ma it mb b mc md me mf mg mh mi mj mk ml mm nv mo mp mq bi translated">正如我们提到的，我们没有一个大的数据集来完成这项任务。更大的数据集将产生更好的结果。</li><li id="e8d6" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm nv mo mp mq bi translated">没有对任何模型进行主要的超参数调整。因此，更好的超参数调整可能会产生更好的结果。</li><li id="7271" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm nv mo mp mq bi translated">利用更先进的技术，如变压器或 BERT，可能会产生更好的结果。</li></ul><h1 id="f4f8" class="lh li it bd lj lk ll lm ln lo lp lq lr jz ls ka lt kc lu kd lv kf lw kg lx ly bi translated">12.参考</h1><ol class=""><li id="6021" class="lz ma it mb b mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated"><a class="ae lg" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li><li id="55b5" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">https://arxiv.org/abs/1502.03044<a class="ae lg" href="https://arxiv.org/abs/1502.03044" rel="noopener ugc nofollow" target="_blank"/></li><li id="0abc" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated">【https://www.aclweb.org/anthology/P18-1240/ T4】</li><li id="98d1" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><a class="ae lg" href="https://arxiv.org/abs/1703.09137" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1703.09137</a></li><li id="f477" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><a class="ae lg" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1409.0473</a></li><li id="5533" class="lz ma it mb b mc ms me mt mg mu mi mv mk mw mm mn mo mp mq bi translated"><a class="ae lg" href="https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/develop-a-deep-learning-caption-generation-model-in-python/</a></li></ol></div><div class="ab cl qx qy hx qz" role="separator"><span class="ra bw bk rb rc rd"/><span class="ra bw bk rb rc rd"/><span class="ra bw bk rb rc"/></div><div class="im in io ip iq"><blockquote class="qe qf qg"><p id="ad14" class="mx my mr mb b mc nl ju mz me nm jx na qh nn nc nd qi no nf ng qj np ni nj mm im bi translated">这个项目的完整代码可以从我的<a class="ae lg" href="https://github.com/vysakh10/Image-Captioning" rel="noopener ugc nofollow" target="_blank"><strong class="mb iu"><em class="it">GitHub</em></strong></a>中访问。</p><p id="b23f" class="mx my mr mb b mc nl ju mz me nm jx na qh nn nc nd qi no nf ng qj np ni nj mm im bi translated">也可以在我的<a class="ae lg" href="https://www.linkedin.com/in/i-am-vysakh/" rel="noopener ugc nofollow" target="_blank"><strong class="mb iu"><em class="it">LinkedIn</em></strong></a>上和我联系。</p></blockquote><blockquote class="pg"><p id="631a" class="ph pi it bd pj pk re rf rg rh ri mm dk translated">希望你喜欢这个项目。感谢阅读:)</p></blockquote></div></div>    
</body>
</html>