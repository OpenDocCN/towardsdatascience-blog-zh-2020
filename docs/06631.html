<html>
<head>
<title>The 4 types of additive Feature Importances</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">4种类型的附加特征重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-4-types-of-additive-feature-importances-5a89f8111996?source=collection_archive---------30-----------------------#2020-05-25">https://towardsdatascience.com/the-4-types-of-additive-feature-importances-5a89f8111996?source=collection_archive---------30-----------------------#2020-05-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5170" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">内容:</strong></p><ol class=""><li id="c221" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">哪个目的对于可变的重要性？<br/> </strong> 1.1 <strong class="jp ir"> </strong>设定你的目标…<br/>1.2…通过选择象限</li><li id="a625" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">4个面向目标的象限<br/> </strong> 2.1沙普利效应区<br/>2.2 SHAP区<br/> 2.3沙普洛斯区<br/> 2.4塞奇区</li><li id="bf4c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">每个象限的Shapley解<br/> </strong> 3.1 Shapley值<br/> 3.2应用于特征属性</li><li id="f720" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">未来展望和要点信息</strong></li><li id="a0fa" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">参考文献</strong></li></ol><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi kz"><img src="../Images/0a9e91b2585bde9428936235d52b9d85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TTf-Av-hxsIViZ0Mxil7ug.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">由作者提供的来自<a class="ae lp" href="https://datajms.com/post/variable_importance_feature_attribution/" rel="noopener ugc nofollow" target="_blank">datajms.com</a>的附加特性重要性的4个目标导向象限的概述。</p></figure><p id="c913" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可能听说过特征重要性方法:有很多这样的方法，它们对于变量选择和模型解释非常有用。但是还有更多:增加可变重要性的景观最近已经变得结构化并系统化。</p><p id="40c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章不仅仅是SHAP的另一篇文章，它提出了相似的观点，分享了一个重要的组成部分:<strong class="jp ir"> Shapley值</strong>。提出了一个结构化的2乘2矩阵，以便根据目标和范围更好地考虑可变的重要性。侧重于附加特征归因方法，4个确定的象限与他们的“最佳”方法一起提出:SHAP，夏普利效应，夏普勒斯和最近的SAGE。然后，我们将研究Shapley值及其性质，这使得4种方法在理论上是最优的。最后，我将分享我对可变重要性方法的看法。</p><p id="d291" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 1。哪个目的的重要性可变？</strong></p><p id="36a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么，什么是可变重要性，它们应该具有哪些性质？我们将关注具有以下两个要求的可变重要性:</p><ul class=""><li id="446c" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk lq kr ks kt bi translated"><strong class="jp ir">特征属性</strong>:表示我们的模型<em class="lr"> f </em>的<strong class="jp ir">兴趣数量</strong>对每个特征的依赖程度。</li><li id="641d" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk lq kr ks kt bi translated"><strong class="jp ir">相加重要性</strong>:将重要性相加应该产生一个有意义的<em class="lr">量</em>(通常是模型<em class="lr"> f </em>的感兴趣的量)。</li></ul><p id="0eb0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然<em class="lr">特征归属</em>属性是可变重要性的本质，但是<em class="lr">附加重要性</em>要求更具挑战性。更著名的可变重要性方法打破了它:Breiman随机森林变量重要性、特征切除、排列重要性等。让我们关注这两个属性的可变重要性。</p><h2 id="499b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">1.1.设定你的目标…</h2><p id="34fa" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">让我们来关注一个重要的概念:<strong class="jp ir">兴趣量</strong>。感兴趣的<strong class="jp ir">量</strong>是您想要“分割”为变量总和的度量。如果你觉得这个定义太模糊，你会喜欢下面的Shapley值部分。</p><p id="d843" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">选择感兴趣的<strong class="jp ir">数量</strong>是下一步，应该符合你的目标。不同视角对应多种选择:</p><ul class=""><li id="8bfd" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk lq kr ks kt bi translated">局部与全局范围:变量重要性应该对数据集的每一行求和，还是在总体范围内求和？<strong class="jp ir">局部范围</strong>适用于关注一个数据点的相关情况，或应沿着一维分析重要性的情况。而<strong class="jp ir">全局范围</strong>与用于高层决策的汇总指标相关:变量选择、因素优先级等等。</li><li id="0c7d" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk lq kr ks kt bi translated"><strong class="jp ir">灵敏度与预测能力</strong>度量:变量重要性应该是模型<em class="lr"> f </em>如何变化的度量，还是预测性能如何随之提高？从<strong class="jp ir">灵敏度</strong>的角度来看，重要性应该集中在用<em class="lr"> f </em>计算如何依赖一个变量。而<strong class="jp ir">预测能力</strong>方法设置重要性，以说明一个变量对提高预测性能(减少损失函数)有多大贡献。</li></ul><h2 id="f5a8" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">1.2.…通过选择象限</h2><p id="371d" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">这些<strong class="jp ir">局部与全局范围</strong>和<strong class="jp ir">灵敏度与预测能力指标</strong>定义了一个2×2目标导向矩阵。每个象限都以重要性度量命名，从理论上讲，重要性度量对于其感兴趣的数量是“最优的”。这些方程是每个感兴趣的量的加法分解的简化版本。</p><p id="26eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更具体地说，让我们介绍一些符号。假设从你的变量<em class="lr"> X=(X₁，X₂，…，Xᵣ) </em>，你尝试用你的模型<em class="lr"> f(X)∈ </em> ℝ最小化损失函数<em class="lr"> l(y，f(x)) </em>预测<em class="lr"> Y </em>。<em class="lr"> x </em>和<em class="lr"> y </em>是指一个数据点，而<em class="lr"> X </em>和<em class="lr"> Y </em>是总体水平(随机变量)。𝔼和𝕍分别表示一个变量的期望值(“平均值”)和方差。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi mq"><img src="../Images/7b310900e669ba47ade4c6ef416ed2e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dyWLTCti7GDAgO4K0tAb9w.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">附加变量重要性的4个面向目标的象限，带有感兴趣的数量和附加细分。每个象限都用象限的最优方法命名。图来自<a class="ae lp" href="https://datajms.com/post/variable_importance_feature_attribution/" rel="noopener ugc nofollow" target="_blank">datajms.com</a>，由作者提供。</p></figure><h1 id="e74b" class="mr lt iq bd lu ms mt mu lx mv mw mx ma my mz na md nb nc nd mg ne nf ng mj nh bi translated">2.4个以目标为导向的象限</h1><p id="30e4" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">让我们来看看4个象限和它们解决的不同问题。我们将按时间顺序进行这次旅行，因为它讲述了两个不同的研究团体如何最终相遇的好故事！</p><h2 id="ad4d" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">2.1.沙普利效应区</h2><p id="6aa8" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">改进Sobol指数(1993) [1]，Owen在2014年引入了一个重要性度量[2]，该度量已由Song等人在2016年开发并命名为“Shapley效应[3]”(另见Iooss等人在2017年的进一步工作和数值实验[4])。它来自于<em class="lr">敏感性分析</em>和<em class="lr">不确定性量化</em>领域，旨在量化模型<em class="lr"> f </em>(例如一组复杂方程的计算机模拟)的输出在多大程度上取决于<em class="lr"> X </em>输入参数。Shapley效应也完全与机器学习相关，它关注于学习模型<em class="lr"> f </em>的变化在多大程度上依赖于变量<em class="lr"> X </em>。</p><p id="c7c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感兴趣的量是𝕍( <em class="lr"> f(X) </em>。方差是量化变异的自然选择。请注意，在敏感性分析社区中，指数通常由总方差归一化，因此所有变量的重要性总和为1(或Sobol指数“接近”1)。</p><p id="a332" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过查看4个象限，一个问题出现了:为什么不选择𝔼( <em class="lr"> f(X) </em>作为感兴趣的量？这绝对是全球性的。然而，它与解释变化无关:正的和负的变化将消失在0全局贡献中。</p><p id="f7d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们继续到2017年，开始机器学习社区的Lundberg传奇。</p><h2 id="8c51" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">2.2.SHAP区</h2><p id="6646" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">由Lundberg于2017年设计并实现[5]，shap a具有局部敏感性焦点。请注意，尽管shap是在机器学习会议上发表的，但它并不涉及<em class="lr"> Y </em>目标或模型<em class="lr"> f </em>的任何学习。这就是为什么我能够把它应用到一个非学习的、基于专家的<a class="ae lp" href="https://datajms.com/post/covid_variable_importances_shapley/" rel="noopener ugc nofollow" target="_blank">算法中，用于新冠肺炎患者定位</a>。然而，它非常适合机器学习社区，因为它的快速模型特定的实现。</p><p id="cc56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">兴趣量坚持最自然的选择:<em class="lr"> f(x) </em> for <em class="lr"> x∈ X </em>。与全局范围不同，在这里同时有正面和负面的贡献是有意义的。了解变化的方向是完全相关的，并允许对shap值进行良好的可视化探索(在<a class="ae lp" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> shap包</a>中实现)。</p><h2 id="fa16" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">2.3.沙普罗斯区</h2><p id="fd77" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">2020年发表在Nature[6](但<a class="ae lp" href="https://arxiv.org/pdf/1905.04610.pdf" rel="noopener ugc nofollow" target="_blank">预印</a>在2019年)，Lundberg等人提出了一项创新！虽然本文主要关注基于树的模型，但提出了一个新的想法:使用shap将模型误差分解为特征贡献(参见本文的2.7.4和图5)，这对于生产中模型的监督性能监控非常有用。我虚构了SHAPloss这个名字，以强调所实现的不同目标，尽管实现是在<a class="ae lp" href="https://github.com/slundberg/shap/blob/master/notebooks/tree_explainer/Explaining%20the%20Loss%20of%20a%20Model.ipynb" rel="noopener ugc nofollow" target="_blank"> shap </a>包内完成的，只需改变<em class="lr"> TreeExplainer </em>中的<em class="lr"> model_output </em>参数。</p><p id="9b66" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感兴趣的量是对于<em class="lr"> (x，y)∈ (X，Y)的局部损失<em class="lr">-l(Y，f(x)) </em>。</em>注意<em class="lr"> l </em>自然可以是分类问题的对数损失，同时是回归的MSE。添加了负号，因此较大的正贡献ϕᵢ意味着可以大大提高性能的特性。</p><h2 id="1dae" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">2.4.圣贤区</h2><p id="8de7" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">在2020年4月提交的预印本[7]中，Covert和Lundberg等人介绍了SAGE(Shapley Additive Global importancE)，一种SHAPloss全局公式的解决方案以及计算它的有效方法。请注意，本文远远超出了SHAPloss简单的局部到全局的概括，但它也包括对现有重要性方法的回顾，并介绍了理论上的通用预测能力。此外，SAGE论文明确提到了我们所谓的Shapley效应区，解释了SAGE在目标上的不同之处。在某些方面，它关闭了我们已经探索过的四象限回路。</p><p id="6637" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感兴趣的量是𝔼[<em class="lr">-l(y，f(X)) </em> ]，局部SHAPloss公式的自然集合。与SHAP到沙普利效应的转换不同，这里采用原始预期。这是因为几乎没有正负湮灭，因为增加一个变量通常不会增加损失。</p><h1 id="61bf" class="mr lt iq bd lu ms mt mu lx mv mw mx ma my mz na md nb nc nd mg ne nf ng mj nh bi translated">3.每个象限的Shapley解</h1><p id="6a6d" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">既然已经设定了目的及其感兴趣的数量，Shapley值[8]理论为每个象限提供了给定期望属性的最佳解决方案。让我们先介绍一下<strong class="jp ir"> Shapley值</strong>，看看它如何应用于各种<strong class="jp ir">感兴趣的量</strong>。</p><h2 id="f565" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">3.1.沙普利值</h2><p id="e221" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">沙普利值<em class="lr"> ϕᵢᵐ </em>是一种“公平”分享联盟<em class="lr">pᵣ={1<em class="lr">m(pᵣ</em>获得的利益数量的归属方法，2，..，r} </em>每个实体之间<em class="lr"> i∈ Pᵣ </em>。<em class="lr"> m(u) </em>是返回联盟的利息数量<em class="lr"> u </em>的函数。联盟是一组实体:有2ʳ可能的联盟，包括∅和Pᵣ。最后，让我们用<em class="lr"> Sᵢʳ </em>来表示不包含实体 <em class="lr"> i </em>的所有可能联盟<strong class="jp ir">的集合。</strong></p><p id="e708" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">沙普利值<em class="lr"> ϕᵢᵐ </em>是唯一满足5个期望属性(查看SAGE论文[7]的3.1了解其含义)的数量权重，这5个属性分别是对称性、线性度、单调性、虚拟性以及效率，我们在这里写道:<em class="lr"> m(Pᵣ)=m(∅)+ ∑ ϕᵢᵐ </em></p><p id="4227" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于这个Shapley值的公式，有很多要讲的。但这有点跑题了，我更愿意把重点放在如何将Shapley的想法应用到4个象限上。</p><h2 id="df34" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">3.2.应用于特征属性</h2><p id="b3f4" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">在我们的上下文中使用Shapley值意味着<em class="lr"> i </em>实体就是<em class="lr"> Xᵢ </em>变量。剩下的两个任务是选择感兴趣的量，并为变量 <em class="lr"> u </em>的每个联合定义<em class="lr"> f </em> <strong class="jp ir">。为我们的4象限选择的解决方案是沿着缺失变量取期望值:<em class="lr">f</em>ᵤ(<em class="lr">x</em>)=𝔼(<em class="lr">f</em>(<em class="lr">x|xᵤ=xᵤ</em>)。有关更多信息，请参见SAGE文件中的详细信息。</strong></p><p id="abd7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4个感兴趣的量转化为4个<em class="lr"> m(u) </em>函数，这导致了象限的4个名称:具有期望属性的可变重要性方法！</p><p id="1709" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们用更精确的感兴趣的量<em class="lr"> m(u) </em>重写2乘2矩阵，这些量是<em class="lr"> f </em>和所有特征联盟<em class="lr"> u </em> ( <em class="lr"> u∈ </em> { <em class="lr"> ∅、</em> { <em class="lr"> X </em> ₁}、{ <em class="lr"> X </em> ₂}、..、{ <em class="lr"> X </em> ₁ <em class="lr">，X </em> ₂}、..}).</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi ni"><img src="../Images/cf3a3d1ccd2943701487aab524df24f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgM-3pWWo-Y0swmGdNLz7Q.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">可加变量重要性的4个面向目标的象限，详细的公式。图来自<a class="ae lp" href="https://datajms.com/post/variable_importance_feature_attribution/" rel="noopener ugc nofollow" target="_blank">datajms.com</a>，作者提供。</p></figure><p id="d866" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">四个沙普利值<a class="ae lp" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">ϕ<em class="lr">ᵢ</em>shap(<em class="lr">x</em>)</a>、<a class="ae lp" href="https://github.com/slundberg/shap/blob/master/notebooks/tree_explainer/Explaining%20the%20Loss%20of%20a%20Model.ipynb" rel="noopener ugc nofollow" target="_blank"> ϕ <em class="lr"> ᵢ </em>损失(<em class="lr"> x </em> ) </a>、<a class="ae lp" href="https://gitlab.com/CEMRACS17/shapley-effects" rel="noopener ugc nofollow" target="_blank"> ϕ <em class="lr"> ᵢ </em> EFF </a>和<a class="ae lp" href="https://github.com/icc2115/sage" rel="noopener ugc nofollow" target="_blank"> ϕ <em class="lr"> ᵢ </em>圣人</a>是每个象限的“最优”解。请注意，这些值之间有两个链接:</p><ul class=""><li id="b285" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk lq kr ks kt bi translated">ϕ <em class="lr"> ᵢ </em>圣人= 𝔼[ϕ <em class="lr"> ᵢ </em>损失(<em class="lr"> x </em> )]</li><li id="2bb8" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk lq kr ks kt bi translated">潜在地，如果损失函数<em class="lr"> l </em>是MSE，我们有ϕ<em class="lr">ᵢ</em>eff=ϕ<em class="lr">ᵢ</em>sage与<em class="lr">y</em>=<em class="lr">f</em>(<em class="lr">x</em>)。</li></ul><h1 id="032f" class="mr lt iq bd lu ms mt mu lx mv mw mx ma my mz na md nb nc nd mg ne nf ng mj nh bi translated">4.未来展望</h1><p id="333b" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">我们刚刚看到，对于每个象限,“最佳”解决方案已经被定义，并且实现是可用的。那么，完整的故事被讲述了吗？</p><p id="647e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一方面，我认为附加重要性测量领域已经达到了一个成熟的里程碑，最佳地填充了4个象限，从而结束了循环。在SAGE的文章之前，我不知道敏感性分析和预测能力重要性之间有任何明确的联系。</p><p id="5a4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，在可变重要性和特征属性方面仍有改进的空间，包括更好地利用这些技术和探索此范围之外的价值:</p><p id="f118" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更好地使用象限中的方法:</p><ul class=""><li id="7ac3" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk lq kr ks kt bi translated"><strong class="jp ir">SHAPloss在数据科学社区中的传播</strong>:尽管SHAP在数据科学社区中的采用率非常快，但shap loss目前仍不为人所知(除了Hall的一本<a class="ae lp" href="https://github.com/jphall663/interpretable_machine_learning_with_python/blob/master/debugging_resid_analysis_redux.ipynb" rel="noopener ugc nofollow" target="_blank">励志笔记本</a>)。我看到了对生产中的模型进行受监督的(当基本事实标签已知时)性能监控的价值。</li><li id="77b3" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk lq kr ks kt bi translated"><strong class="jp ir">计算效率的提高</strong>:除了基于树的模型之外，这些方法都是计算密集型的，并且会很快变得难以处理。实现和统计估计的改进可以提高可用性(参见最近的工作[9])。</li></ul><p id="d25b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了2乘2矩阵:</p><ul class=""><li id="db50" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk lq kr ks kt bi translated"><strong class="jp ir">基于公平的利益数量</strong>:为什么不设想其他列？理解模型行为和模型性能是首要的重要步骤。但是，负责任的数据科学也包括相关的偏见和公平性监测。理论上，似乎可以选择一个基于公平的相关兴趣量<em class="lr"> m(u) </em>并构建其Shapley值，以查看“不公平”将如何在特征之间划分。伦德伯格<a class="ae lp" href="https://github.com/slundberg/shap/blob/master/notebooks/general/Explaining%20Quantitative%20Measures%20of%20Fairness.ipynb" rel="noopener ugc nofollow" target="_blank">用人口均等指标开辟了道路</a>，巧妙地保持在SHAP区域内。</li><li id="562a" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk lq kr ks kt bi translated"><strong class="jp ir">探索非附加的</strong>特征归属方法。如果使用乘法分解或完全不同的重新加权方法[10]来完成，量化感兴趣的量在多大程度上依赖于输入特征仍然是广泛的研究和实践领域。</li></ul><p id="c17a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一些外卖信息。我希望这篇文章:</p><ul class=""><li id="ca4d" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk lq kr ks kt bi translated">启发你<strong class="jp ir">理解<strong class="jp ir">变量重要性</strong>的不同目标和范围</strong>。</li><li id="92b3" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk lq kr ks kt bi translated">让你相信，自从Sobol在20世纪90年代开创了这个领域以来，<strong class="jp ir">相加重要性度量</strong>领域无疑比以往任何时候都更加成熟。</li><li id="6e96" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk lq kr ks kt bi translated">根据你的目的，让你思考选择最有效的象限。</li></ul><p id="9096" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对SHAP和沙普利效应区的结果和代码感兴趣吗？你可以查看我的文章<a class="ae lp" href="https://datajms.com/post/covid_variable_importances_shapley/" rel="noopener ugc nofollow" target="_blank">新冠肺炎患者定位算法</a>的可变重要性。此外，您可以查看SAGE论文[7]以获得更多非最优但计算量更小的方法的示例，以及它们如何适合2×2矩阵。</p><h1 id="9188" class="mr lt iq bd lu ms mt mu lx mv mw mx ma my mz na md nb nc nd mg ne nf ng mj nh bi translated">5.参考</h1><p id="d94e" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">[1]: <em class="lr">索博尔，I. M. (1993)。非线性数学模型的灵敏度估计。数学建模和计算实验，1(4)，407–414</em>。顺便说一句，我找到的唯一在线版本是一份由I. M. Sobol本人手写注释的复印件，寄给了著名的敏感性分析研究员Andrea Saltelli。</p><p id="67d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]: <a class="ae lp" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.377.6450&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">欧文，A. B. (2014)。索博指数和沙普利值。SIAM/ASA不确定性量化期刊，2(1)，245–251。</a></p><p id="f817" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3]: <a class="ae lp" href="https://pdfs.semanticscholar.org/6a25/48b159bc3bf6c74e13b74a037917951d75ca.pdf" rel="noopener ugc nofollow" target="_blank"> Song，e .，Nelson，B. L .，&amp; Staum，J. (2016)。全局灵敏度分析的Shapley效应:理论和计算。SIAM/ASA不确定性量化期刊，4(1)，1060–1083。</a></p><p id="430c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[4]:<a class="ae lp" href="https://hal.inria.fr/hal-01556303v3/document" rel="noopener ugc nofollow" target="_blank">伊奥斯，b .，T15普里尔，C. (2019)。相关输入敏感性分析的Shapley效应:与Sobol指数的比较，数值估计和应用。国际不确定性量化杂志，9(5)。</a></p><p id="6770" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[5]:<a class="ae lp" href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf" rel="noopener ugc nofollow" target="_blank">伦德伯格，S. M .，T16李，S. I. (2017)。解释模型预测的统一方法。神经信息处理系统进展(第4765-4774页)。</a></p><p id="aca0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[6]: Lundberg，S. M .，Erion，g .，Chen，h .，DeGrave，a .，Prutkin，J. M .，Nair，b .，Katz，r .，Himmelfarb，j .，Bansal，n .，&amp; Lee，s . I .(2020)。用可解释的人工智能对树木从局部解释到全局理解。自然机器智能，2(1)，2522–5839。</p><p id="c540" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[7]:<a class="ae lp" href="https://arxiv.org/pdf/2004.00668.pdf" rel="noopener ugc nofollow" target="_blank">隐蔽，我，伦德伯格，s .，T17李，S. I. (2020)。通过附加重要性测量了解全局特征贡献。arXiv预印本:2004.00668。</a></p><p id="9c0b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">沙普利，L. S. (1953)。n人游戏的一个值。对博弈论的贡献，2(28)，307–317。</p><p id="649b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[9]:普利斯克等人最近的预印本将沙普利效应的计算提高了几个数量级:<a class="ae lp" href="https://arxiv.org/pdf/2002.12024.pdf" rel="noopener ugc nofollow" target="_blank">普利斯克，e .拉比蒂，g .&amp;博格诺沃，E. (2020)。灵敏度分析中沙普利效应的计算。arXiv预印本:2002.12024。</a></p><p id="81bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[10]: <a class="ae lp" href="https://hal.archives-ouvertes.fr/hal-01897642/document" rel="noopener ugc nofollow" target="_blank">巴乔克，f .，甘博亚，f .，卢贝斯，J. M .，&amp;里塞尔，L. (2018)。熵变量促进机器学习中的可解释性&amp;。</a></p></div></div>    
</body>
</html>