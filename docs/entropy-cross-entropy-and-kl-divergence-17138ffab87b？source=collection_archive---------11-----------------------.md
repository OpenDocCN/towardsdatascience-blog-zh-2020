# 熵、交叉熵和 KL 散度

> 原文：<https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-17138ffab87b?source=collection_archive---------11----------------------->

## 让我们通俗地理解熵

首先，感谢您对本文的兴趣。有很多媒介可能会把你重定向到这里——社交媒体网站、媒介内部推荐、直接链接、电子邮件、谷歌搜索结果等。不过，既然你被重定向了，我今天有个有趣的话题——熵。

![](img/ecf0d06b9337735658db44b077c1a23b.png)

[黄贯中](https://unsplash.com/@paul_wong?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照

# 熵

## 如果你是一个随机读者…

如果文章推荐没有个性化，你对我来说就是一个随机读者。让我们假设有 *50%* 的几率一个随机读者会阅读整篇文章。所以，读完整篇文章的概率和不读整篇文章的概率是一样的。你可能会读，也可能不会。假设有一个完美的预测引擎，如果某个用户愿意读这篇文章，它会给我一些信息。所以，引擎说你是否会读这篇文章。它为我提供任何格式的信息——文本、音频、图像等。然而，无论是哪种格式，引擎，我都会得到一位信息— *读/不读*。

每当一个新的随机用户访问我的文章时，我都会得到一点信息。那么对于一个新的随机用户，我平均能得到多少位信息呢？这些事件发生的可能性是相等的，所以一个新随机用户得到的平均信息量是 *1* 。这就是 ***熵*** 。这个随机事件的熵是 *1* 。

所以，事件的熵是事件的随机性或者事件结果的不确定性。如果我们知道事件的结果，它等于我们得到的平均信息量。

## 那么，如何计算熵呢？

熵是我们需要获得的平均信息量，以了解事件。为了知道事件的结果，我们需要将不确定性减少到 0(即确定性减少到 1)。如果某个事件 *A* 的概率是 *p* ，知道它的结果意味着用 *1/p* 的因子来减少不确定性。所以，我们需要 *lg(1/p)* 位数来了解事件，等于 *-lg(p)* 。这是事件 *A* 发生时的熵值。同样，事件 *A* 没有发生时的熵值为 *-lg(1-p)* 。如果概率分布为*伯努利*带 *p* ，则事件的平均熵为 *-p * lg(p) -(1-p) * lg(1-p)。*

## 如果你总是看我的整篇文章呢？

我想在这里记住我的女朋友。不管我写什么话题，她都喜欢看我的文章(咳咳……)。所以，这个事件在她的情况下是确定的。在这种情况下，结果没有不确定性，没有随机性，对吗？所以，猜测它的熵为 0 是有道理的，不是吗？让我们看看在这种情况下是否得到零熵。如果事件是确定的，概率是 1。这意味着我们不需要任何位数来了解事件。所以，我们需要得到的位数是 0。因此，熵为零。

## 现在，我的追随者怎么办？

我的关注者关注了我，因为他们对我通常写的主题感兴趣。所以，对于他们来说，阅读整篇文章的概率大于随机用户。我们假设概率是 *0.75* 。当我们思考这种情况下的事件时，不确定性减少了，对吗？让我们找出答案。

当关注者实际阅读整篇文章时，概率增加一个因子 *4/3* [≈ *lg(1.33)* 位数】*。*但当关注者没有阅读整篇文章时，概率增加一个更大的因子，即 *4* ( *2* 位数)。我的关注者阅读文章的次数占 75%，所以这个概率分布的平均位数是:
≈*0.75 * LG(1.33)+0.25 * LG(4)*≈*0.81*

*0.81* 当然小于 *1* 。所以，我的追随者的熵更低。

## 如果有两个以上的可能事件呢？

熵公式可以推广到 *n* 个可能事件。公式是:
*熵=-*[*∑*](https://graphemica.com/%E2%88%91)*(I = 1…n)p _ I * LG(p _ I)*

## 熵与事件数量

让我们来比较熵和这里的一些事件。直觉上，事件数量的增加会反过来增加熵，因为这增加了不确定性。让我们取 *n* 个等概率事件，计算概率分布的熵。

概率分布的熵变成:
*-n *(1/n)* LG(1/n)= LG(n)*

对数函数(任意底)是严格单调递增的，因此随着事件数量的增加，熵也增加。是的，我们的直觉是正确的。:)

# 交叉熵

了解事件所需的平均位数不同于用于传输信息的平均位数。交叉熵是用于传输信息的平均位数。交叉熵总是大于或等于熵。对于随机用户预测机，用于传送信息的比特数是 *1* ，所以交叉熵是 1。

让我们来看一个有四种可能结果的概率分布——概率分别为 *0.5，0.25，0.125，*和 *0.125* 。如果我们用两个比特来传递这个信息，交叉熵就变成了 *2* 。等等，这种情况下的熵是多少？
*熵= 0.5 * LG(2)+0.25 * LG(4)+0.125 * LG(8)+0.125 * LG(8)
= 0.5+0.5+0.375+0.375 = 1.75*

在这种情况下，熵是 1.75，但我们使用 2 位来传输该信息，因此交叉熵= 2。这种交叉熵与熵之差叫做 ***KL 散度*** 。

当使用两位来传输所有情况的信息时，我们假设所有事件的概率为 *1/(2 )* 。所以，实际 vs 预测(或假设)的概率分布是:
*0.5 vs 0.25，0.25 vs 0.25，0.125 vs 0.25，0.125 vs 0.25。*

如果我们使用 1 位来传输第一事件信息(1)，2 位来传输第二事件信息(10)，3 位来传输第三事件信息(101)，还有 3 位来传输第四事件信息(100)，我们将使用最佳消息长度。因此，当我们对不同的事件采用不同的消息长度时，我们隐含地预测了概率分布。我们得到的交叉熵值越大，预测的概率分布就越偏离实际的概率分布。因此，交叉熵也可以用于计算实际概率分布和预测概率分布之间的距离。这就是交叉熵损失在机器学习中处理分类时的使用方式。

## 猫和狗的分类

最流行的分类问题之一，猫狗分类，要求我们预测概率分布，给定一个图像。实际的概率分布是标签，逻辑函数给我们预测的概率分布。交叉熵以如下方式用作该任务的损失函数:

假设我们有一幅狗的图像，概率分布中的第一个元素是狗的。

实际概率分布为:*【1，0】*

现在，假设逻辑函数输出概率分布:*【0.7，0.3】*

*0.7* 概率这里的意思是——*LG(0.7)*当图像是狗的时候用来传递信息的比特数。

因此，交叉熵将是:*-1 * LG(0.7)-0 * LG(0.3)= 0.51*

由于最小化某个函数与最小化它的正标量倍数是相同的，所以我们可以使用自然对数(或任何底对数)来代替 2-底对数来定义机器学习中的损失。

## KL 散度何时为 0？

KL 散度是相对熵或交叉熵和熵之间的差或实际概率分布和预测概率分布之间的某个距离。当预测概率分布与实际概率分布相同时，等于 0。

希望你现在对熵，交叉熵和 KL 散度有一个好的概念。请在评论区告诉我你的想法。

此外，我们鼓励读者浏览 YouTube 上奥雷里奥·杰龙的精彩视频。