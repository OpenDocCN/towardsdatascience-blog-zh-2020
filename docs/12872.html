<html>
<head>
<title>Wav2Lip: A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Wav2Lip:一个唇同步专家是你在野外进行语音到唇生成所需要的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/wav2lip-a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild-b1cb48787190?source=collection_archive---------17-----------------------#2020-09-04">https://towardsdatascience.com/wav2lip-a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild-b1cb48787190?source=collection_archive---------17-----------------------#2020-09-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6b2535ca22252a715940dd364e594e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bWkXdSzk-wLOAlI8ouH6FQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">Wav2Lip 模型架构(<a class="ae kc" href="https://arxiv.org/pdf/2008.10010v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2008.10010v1.pdf</a>)</p></figure><p id="6874" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文提出了 Wav2Lip，它是 SyncNet 模型的一种改进，在视频-音频唇形同步任务方面优于所有先前的与说话人无关的方法。</p><p id="db52" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者指出，虽然现有方法在呈现训练集中不存在的说话者的视频时通常不能通用化，但 Wav2Lip 能够对各种说话者产生准确的嘴唇运动。</p><p id="d01b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他们继续总结了本文的主要意图:</p><ol class=""><li id="e77d" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">识别先前方法不能推广到各种说话者的原因。</li><li id="2968" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">通过结合强大的唇同步鉴别器来解决所述问题。</li><li id="bf44" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">提出新的基准来评估对口型任务的方法的性能。</li></ol><h1 id="0ad9" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">介绍</h1><p id="7a27" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">作者的第一点是关于最近视频和音频内容消费的繁荣。除此之外，越来越需要将音像翻译成多种语言，以促进更多公众的参与。因此，将机器学习应用于诸如不受约束的视频-音频内容的自动口型同步这样的任务有很大的动机。</p><p id="317d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，不幸的是，早期的方法通常不能推广到多种说话人身份，只有在对组成其训练集的潜在说话人的小子集进行评估时才表现良好。</p><p id="3977" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法将不能满足上述实际应用的严格要求，其中合适的模式必须能够精确地同步各种扬声器。</p><p id="3d84" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于在实践中使用这种方法的苛刻要求，人们需要一种可以推广到各种说话者身份的模型。结果，出现了与说话者无关的方法。这些模型在数千个说话者身份上被训练。然而，即使是在以前的出版物中采用的这些方法也不能满足本工作作者的期望。他们承认，先前的与说话者无关的模型虽然能够在单独的静态图像上生成准确的唇同步，但不适用于动态内容。对于诸如电视连续剧和电影的翻译的应用，要求一种方法对于不同的无约束视频中说话者的变化的唇形更具普遍性。</p><p id="c07b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者指出，人类可以检测到大约 0.05-0.1 秒不同步的视频片段，因此这意味着一个具有细微误差的广泛挑战。</p><p id="ef11" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本节最后简要总结了作者的贡献:</p><ol class=""><li id="6221" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">他们提出了<em class="ms"> Wav2Lip </em>，这明显优于以前的方法。</li><li id="33d4" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">他们引入了一套新的基准/指标来评估模型在这项任务中的表现。</li><li id="b62f" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">他们发布自己的数据集，以评估他们的方法在呈现从野外采样的看不见的视频音频内容时的性能。</li><li id="a8c0" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">Wav2Lip 是第一个与说话者无关的方法，它经常匹配真实同步视频的准确性；根据人类评估，他们的方法在大约 90%的情况下优于现有的方法。</li><li id="05f2" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">他们将为配音视频生成同步视频帧的 FID 分数从 12.87 (LipGAN)提高到 11.84 (Wav2Lip + GAN)，将平均用户偏好从 2.35% (LipGAN)提高到 60.2% (Wav2Lip + GAN)。</li></ol><h1 id="189e" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">现有文献综述</h1><p id="fa4c" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">(我将把这一部分限制在作者提及这些论文的原因上，而省略掉具体关于这些作品的方法的深入信息)</p><p id="9d97" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者承认以前的方法和在现实世界中完全工作的方法的要求之间存在一些差异:</p><ol class=""><li id="8fba" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">一些方法需要大量的训练数据。</li><li id="ec91" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">该模型在词汇学习范围方面的局限性。</li><li id="2942" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">在具有有限词汇集的数据集上进行训练妨碍了现有方法学习各种音位-视位映射的能力。</li></ol><p id="b1ca" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他们继续争论为什么当呈现来自野外的看不见的视频内容时，先前的方法通常不能产生准确的唇同步:</p><ol class=""><li id="a83d" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated"><em class="ms">像素级重建损失是对口型同步的弱判断</em>:先前工作中包含的损失函数不足以惩罚不准确的口型同步生成。</li><li id="03ac" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><em class="ms">弱唇同步鉴别器</em>:Lip gan 模型架构中的鉴别器在检测不同步视频音频内容时只有 56%的准确率，而 Wav2Lip 的鉴别器在同一测试集上区分同步内容和不同步内容的准确率为 91%。</li></ol><h2 id="87a3" class="mt lq iq bd lr mu mv dn lv mw mx dp lz ko my mz md ks na nb mh kw nc nd ml ne bi translated">你只需要一个对口型专家</h2><p id="0502" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">最后，作者提出了他们的方法，考虑到上述两个问题在以前的工作。</p><ul class=""><li id="9eba" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la nf lh li lj bi translated">使用预先训练的唇形同步鉴别器，该鉴别器在检测原始的、不受约束的样本中的不同步视频-音频内容方面已经是准确的。</li><li id="b8ac" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la nf lh li lj bi translated">为此任务调整先前存在的 SyncNet 模型。(这个我就不深究了，我只强调一下 Wav2Lip 架构)。</li></ul><h1 id="e3d5" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">Wav2Lip 模型架构概述</h1><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6b2535ca22252a715940dd364e594e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bWkXdSzk-wLOAlI8ouH6FQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">Wav2Lip 模型架构(【https://arxiv.org/pdf/2008.10010v1.pdf】T4)</p></figure><h2 id="4fc5" class="mt lq iq bd lr mu mv dn lv mw mx dp lz ko my mz md ks na nb mh kw nc nd ml ne bi translated">术语</h2><p id="089c" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">作者使用以下术语来指代他们网络的各个部分，我将在本节后面继续使用这些术语:</p><ul class=""><li id="bffc" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la nf lh li lj bi translated"><em class="ms">随机参考片段</em>:用于识别特定说话人的一段连续帧的随机样本，提供特定于前述说话人的身份的网络上下文。</li><li id="a304" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la nf lh li lj bi translated"><em class="ms">身份编码器</em>:对基础真相帧和随机参考片段的连接进行编码，为网络提供可视上下文以适当适应任何特定的说话者。</li><li id="c038" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la nf lh li lj bi translated"><em class="ms">语音编码器</em>:编码音频数据(不言自明)。</li><li id="c218" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la nf lh li lj bi translated"><em class="ms">人脸解码器</em>:将串接的特征向量解码成一系列重构帧。</li></ul><h2 id="c6af" class="mt lq iq bd lr mu mv dn lv mw mx dp lz ko my mz md ks na nb mh kw nc nd ml ne bi translated">方法学</h2><p id="6aa3" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">在高级别上，Wav2Lip 输入特定音频片段的 Mel-频谱图表示，以及对应的基本事实帧(下半部分被屏蔽)和随机参考片段的拼接，该随机参考片段的扬声器确认基本事实片段的扬声器。它通过卷积层减少这种输入，以形成音频和帧输入的特征向量。然后，它连接这些特征表示，通过一系列转置卷积层将结果矩阵投影到一段重建帧上。在身份编码器和面部解码器的层之间存在剩余的跳过连接。</p><p id="0b3e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Wav2Lip 试图从它们的屏蔽副本中完全重建地面真实帧。我们计算重建帧和地面真实帧之间的 L1 重建损失。然后，重构的帧通过预训练的“专家”唇形同步检测器馈送，而重构的帧和地面真实帧都通过视觉质量鉴别器馈送。视觉质量鉴别器试图区分重构帧和真实帧，以提高帧生成器的视觉生成质量。</p><h1 id="8bc5" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">损失函数</h1><h2 id="7ae5" class="mt lq iq bd lr mu mv dn lv mw mx dp lz ko my mz md ks na nb mh kw nc nd ml ne bi translated">发电机</h2><p id="8a5b" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">生成器旨在最小化重建帧$L_g$和地面真实帧$L_G$之间的 L1 损耗:</p><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/ba531fef2b5dbb99c7239e61a69ca180.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*nhEQc-HmicENfE-cdCDoXQ.png"/></div></figure><p id="5dc1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中，𝑵是普遍接受的表示批量的符号。</p><h2 id="cf60" class="mt lq iq bd lr mu mv dn lv mw mx dp lz ko my mz md ks na nb mh kw nc nd ml ne bi translated">唇形同步鉴别器</h2><p id="c75d" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">对于唇同步，他们使用二进制交叉熵损失实现余弦相似性，从而计算给定两帧的概率。更具体地，在重新激活的视频和语音嵌入𝑣和𝑠.之间计算损失这会产生一个概率列表，每个样本一个，表示相应样本同步的概率。</p><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/250bb0487f8673cea1831b5b059a4333.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*1jjqMdOLyovT4-_EC1l6tw.png"/></div></figure><p id="8198" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中应用的 ReLU 激活可以描述为:</p><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/693f028b0e52b8009ed869cf0617d35d.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*rH5TA8GrdwbATvS2DuPYXw.png"/></div></figure><p id="673c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的专家鉴别器损耗通过取分布$P_{sync}$的交叉熵来计算，如下所示:</p><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e8a53ca22762503ca189db98c1435f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*sn7nCeegMjt58x6TTzB7Yg.png"/></div></figure><h2 id="b4bb" class="mt lq iq bd lr mu mv dn lv mw mx dp lz ko my mz md ks na nb mh kw nc nd ml ne bi translated">视觉质量鉴别器</h2><p id="9b41" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">视觉质量鉴别器被训练成最大化以下损失:</p><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/e51bb191785958a949cb34b7ea91319d.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*ndDxFZOw2lCHV0c1sX0_Kg.png"/></div></figure><p id="159f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中，发电机损耗$L_{gen}$的计算公式如下:</p><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5e739332fbc080211155f95372eac9a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*S59sjbEh_4vxIe4zXgkKGg.png"/></div></figure><p id="561f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，生成器试图最小化重建损失、同步损失和对抗损失的加权和(回想一下，我们正在处理两个鉴别器):</p><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/e63a6b182ed9ee0daed5afb75a28cf3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*tC1AYqZNp6E7qWFqzDk3wg.png"/></div></figure><p id="089a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中$s_w$是表示归因于同步的惩罚的加权值，而$s_g$是对抗性损失。</p><p id="be22" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这两个不相交的鉴别器允许网络实现优越的同步精度和视觉生成质量。</p><h1 id="882c" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">结论和进一步阅读</h1><p id="4e49" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">这就是<em class="ms">“一个唇同步专家是你在野外进行语音到唇生成所需要的一切”</em>。如果您想阅读更深入的内容，或者关于我在这里没有谈到的一些事情:</p><ul class=""><li id="c9d7" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la nf lh li lj bi translated">提议的度量/评估系统</li><li id="93b0" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la nf lh li lj bi translated">Wav2Lip 和先前模型之间的基准比较</li><li id="ce9d" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la nf lh li lj bi translated">作者使用的详细训练程序和超参数</li><li id="5c9b" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la nf lh li lj bi translated">Wav2Lip 的真实世界评估</li></ul><p id="d3a6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以通过阅读报纸进一步调查:【https://arxiv.org/pdf/2008.10010v1.pdf<a class="ae kc" href="https://arxiv.org/pdf/2008.10010v1.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="e5a8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">模型架构图片归功于<em class="ms">的作者“一个唇同步专家是你在野外进行语音到唇生成所需要的一切”。</em></p></div></div>    
</body>
</html>