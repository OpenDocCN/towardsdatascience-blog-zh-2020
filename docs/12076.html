<html>
<head>
<title>Hands-on Guide to Plotting a Decision Surface for ML in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 绘制 ML 决策面的实践指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hands-on-guide-to-plotting-a-decision-surface-for-ml-in-python-149710ee2a0e?source=collection_archive---------15-----------------------#2020-08-20">https://towardsdatascience.com/hands-on-guide-to-plotting-a-decision-surface-for-ml-in-python-149710ee2a0e?source=collection_archive---------15-----------------------#2020-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c64e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用 matplotlib 可视化 Python 中分类算法的决策边界</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b44fdf3694584130d8d3f5e7f6082713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zCGFy-b3c2L0hvvkuE_adA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@jancanty?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">扬·坎迪</a>在<a class="ae ky" href="https://unsplash.com/s/photos/boundaries?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h1 id="0909" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="1837" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最近，我一直在努力将一个分类模型的生成模型可视化。我只依靠分类报告和混淆矩阵来衡量模型性能。</p><p id="1c96" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，将分类结果可视化有其魅力，也更有意义。所以，我建立了一个决策面，当我成功的时候，我决定把它作为一个学习的过程写下来，并且写给任何可能陷入同样问题的人。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="b120" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">教程内容</h1><p id="b990" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本教程中，我将从 Sklearn 库中内置的数据集包开始，重点介绍实现步骤。之后，我将使用一个预处理数据(没有缺失数据或异常值)在应用标准缩放器后绘制决策面。</p><ul class=""><li id="b873" class="ne nf it lt b lu mn lx mo ma ng me nh mi ni mm nj nk nl nm bi translated">决策面</li><li id="5f54" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">导入重要的库</li><li id="d1dc" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">数据集生成</li><li id="dc8e" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">生成决策面</li><li id="bcbc" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm nj nk nl nm bi translated">申请真实数据</li></ul></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="68a3" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">决策面</h1><p id="6aee" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">机器学习中的分类是指训练你的数据给输入的例子分配标签。</p><p id="e083" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">每个输入特征在特征空间上定义一个轴。平面的特征在于最少两个输入要素，点表示输入空间中的输入坐标。如果有三个输入变量，特征空间将是三维体积。</p><p id="6637" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">分类的最终目标是分离特征空间，以便尽可能正确地将标签分配给特征空间中的点。</p><p id="c9f1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这种方法称为决策面或决策边界，它是一种演示工具，用于解释分类预测建模任务中的模型。如果您有两个以上的输入要素，我们可以为每对输入要素创建一个决策表面。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="574f" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">导入重要的库</h1><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="d373" class="nx la it nt b gy ny nz l oa ob">import numpy as np<br/>import pandas as pd</span><span id="d146" class="nx la it nt b gy oc nz l oa ob">import matplotlib.pyplot as plt<br/>from matplotlib.colors import ListedColormap</span><span id="9d7b" class="nx la it nt b gy oc nz l oa ob">from sklearn import datasets<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.metrics import accuracy_score, confusion_matrix<br/>from sklearn.model_selection import train_test_split</span></pre></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="9e12" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">生成数据集</h1><p id="fb31" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我将使用 Sklearn 库中 datasets 类中的<code class="fe od oe of nt b">make_blobs()</code>函数来生成一个定制数据集。这样做可以将重点放在实现上，而不是清理数据。然而，步骤是相同的，并且是典型的模式。<br/>为了简单起见，让我们从定义具有 1000 个样本、只有两个特征和标准偏差为 3 的数据集变量开始。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="69c9" class="nx la it nt b gy ny nz l oa ob">X, y = datasets.make_blobs(n_samples = 1000, <br/>                           centers = 2, <br/>                           n_features = 2, <br/>                           random_state = 1, <br/>                           cluster_std = 3)</span></pre><p id="bb9d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一旦数据集生成，我们就可以绘制一个散点图来查看变量之间的可变性。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="dc9d" class="nx la it nt b gy ny nz l oa ob"># create scatter plot for samples from each class<br/>for class_value in range(2):</span><span id="4824" class="nx la it nt b gy oc nz l oa ob">    # get row indexes for samples with this class<br/>    row_ix = np.where(y == class_value)</span><span id="5b17" class="nx la it nt b gy oc nz l oa ob">    # create scatter of these samples<br/>    plt.scatter(X[row_ix, 0], X[row_ix, 1])</span><span id="f408" class="nx la it nt b gy oc nz l oa ob"># show the plot<br/>plt.show()</span></pre><p id="2289" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这里，我们循环遍历数据集，并在每个由类标签着色的<code class="fe od oe of nt b">X</code>和<code class="fe od oe of nt b">y </code>之间绘制点。在下一步中，我们需要建立一个预测分类模型来预测看不见的点的类别。在这种情况下可以使用逻辑回归，因为我们只有两个类别。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/8aa811259f95b68441af6f236dd43531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-MoZpXOKDYjeh1zC5sV-g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">散点图 1</p></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="8750" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">开发逻辑回归模型</h1><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="9011" class="nx la it nt b gy ny nz l oa ob">regressor = LogisticRegression()</span><span id="49ce" class="nx la it nt b gy oc nz l oa ob"># fit the regressor into X and y<br/>regressor.fit(X, y)</span><span id="a279" class="nx la it nt b gy oc nz l oa ob"># apply the predict method <br/>y_pred = regressor.predict(X)</span></pre><p id="81ed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所有的<code class="fe od oe of nt b">y_pred</code>都可以使用<code class="fe od oe of nt b">sklearn</code>库中的<code class="fe od oe of nt b">accuracy_score</code>类进行评估。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="5806" class="nx la it nt b gy ny nz l oa ob">accuracy = accuracy_score(y, y_pred)<br/>print('Accuracy: %.3f' % accuracy)</span><span id="0095" class="nx la it nt b gy oc nz l oa ob">## Accuracy: 0.972</span></pre></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="acd4" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">生成决策面</h1><p id="3e66" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><code class="fe od oe of nt b">matplotlib</code>提供了一个叫做<code class="fe od oe of nt b">contour()</code>的便捷函数，可以在点与点之间插入颜色。然而，正如文档所建议的，我们需要在特征空间中定义<code class="fe od oe of nt b">y</code>的点<code class="fe od oe of nt b">X</code>的网格。起点是找到每个特征的最大值和最小值，然后加 1，以确保覆盖整个空间。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="c5ab" class="nx la it nt b gy ny nz l oa ob">min1, max1 = X[:, 0].min() - 1, X[:, 0].max() + 1 #1st feature<br/>min2, max2 = X[:, 1].min() - 1, X[:, 1].max() + 1 #2nd feature</span></pre><p id="af37" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后我们可以使用<code class="fe od oe of nt b">numpy</code>库中的<code class="fe od oe of nt b">arange()</code>函数定义坐标的比例，分辨率为<code class="fe od oe of nt b">0.01</code>以获得比例范围。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="d906" class="nx la it nt b gy ny nz l oa ob">x1_scale = np.arange(min1, max1, 0.1)<br/>x2_scale = np.arange(min2, max2, 0.1)</span></pre><p id="e3ce" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下一步是将<code class="fe od oe of nt b">x1_scale</code>和<code class="fe od oe of nt b">x2_scale</code>转换成网格。<code class="fe od oe of nt b">numpy</code>库中的函数<code class="fe od oe of nt b">meshgrid()</code>正是我们所需要的。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="85f3" class="nx la it nt b gy ny nz l oa ob">x_grid, y_grid = np.meshgrid(x1_scale, x2_scale)</span></pre><p id="faea" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">生成的<code class="fe od oe of nt b">x_grid</code>是一个二维数组。为了能够使用它，我们需要使用来自<code class="fe od oe of nt b">numpy</code>库的<code class="fe od oe of nt b">flatten()</code>方法将大小减少到一维数组。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="6201" class="nx la it nt b gy ny nz l oa ob"># flatten each grid to a vector<br/>x_g, y_g = x_grid.flatten(), y_grid.flatten()<br/>x_g, y_g = x_g.reshape((len(x_g), 1)), y_g.reshape((len(y_g), 1))</span></pre><p id="3d00" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，像原始数据集一样，以更高的分辨率将向量并排堆叠为输入数据集中的列。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="171a" class="nx la it nt b gy ny nz l oa ob">grid = np.hstack((x_g, y_g))</span></pre><p id="ece8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们可以拟合模型来预测值。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="9f2c" class="nx la it nt b gy ny nz l oa ob"># make predictions for the grid<br/>y_pred_2 = model.predict(grid)</span><span id="90cb" class="nx la it nt b gy oc nz l oa ob">#predict the probability<br/>p_pred = model.predict_proba(grid)</span><span id="27e8" class="nx la it nt b gy oc nz l oa ob"># keep just the probabilities for class 0<br/>p_pred = p_pred[:, 0]</span><span id="5b83" class="nx la it nt b gy oc nz l oa ob"># reshaping the results<br/>p_pred.shape<br/>pp_grid = p_pred.reshape(x_grid.shape)</span></pre><p id="a2c6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，已经生成了跨特征空间的值和预测类标签的网格。</p><p id="b382" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">随后，我们将使用<code class="fe od oe of nt b">contourf()</code>将这些网格绘制成等高线图。<br/><code class="fe od oe of nt b">contourf()</code>功能需要每个轴独立的网格。为了实现这一点，我们可以利用<code class="fe od oe of nt b">x_grid</code>和<code class="fe od oe of nt b">y_grid</code>并重塑预测<code class="fe od oe of nt b">(y_pred)</code>使其具有相同的形状。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="26b2" class="nx la it nt b gy ny nz l oa ob"># plot the grid of x, y and z values as a surface<br/>surface = plt.contourf(x_grid, y_grid, pp_grid, cmap='Pastel1')<br/>plt.colorbar(surface)</span><span id="bf5e" class="nx la it nt b gy oc nz l oa ob"># create scatter plot for samples from each class<br/>for class_value in range(2):<br/># get row indexes for samples with this class<br/>    row_ix = np.where(y == class_value)</span><span id="fc17" class="nx la it nt b gy oc nz l oa ob">    # create scatter of these samples<br/>    plt.scatter(X[row_ix, 0], X[row_ix, 1], cmap='Pastel1')</span><span id="7885" class="nx la it nt b gy oc nz l oa ob"># show the plot<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/72b213a8ee9a500b9136428e09c9c0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cgexp8r2e2RqfYqAh-oNfw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两个特征的决策面</p></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="9bb6" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">应用于真实数据</h1><p id="33de" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在是时候将前面的步骤应用于真实数据以连接一切。正如我前面提到的，这个数据集已经被清理过了，没有遗漏点。该数据集根据年龄和年薪代表了样本人群的购车历史。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="23d7" class="nx la it nt b gy ny nz l oa ob">dataset = pd.read_csv('../input/logistic-reg-visual/Social_Network_Ads.csv')</span><span id="4fae" class="nx la it nt b gy oc nz l oa ob">dataset.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a0a8aa2741b523078e014e4aabb2e466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*sGIsqYqNKvDq37y6LTIBrw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">社交网络广告数据集</p></figure><p id="030b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">数据集有两个特性<code class="fe od oe of nt b">Age</code>和<code class="fe od oe of nt b">EstimatedSalary</code>以及一个作为二进制列购买的因变量。值 0 表示年龄和薪水相似的人没有买车。然而，一个意味着这个人确实买了这辆车。下一步是将因变量从 X 和 y 特征中分离出来</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="a667" class="nx la it nt b gy ny nz l oa ob">X = dataset.iloc[:, :-1].values<br/>y = dataset.iloc[:, -1].values</span><span id="f2b3" class="nx la it nt b gy oc nz l oa ob"># splitting the dataset<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>                                               X, y, <br/>                                               test_size = 0.25,<br/>                                               random_state = 0)</span></pre></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="f918" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">特征缩放</h1><p id="7c2b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们需要这一步，因为<code class="fe od oe of nt b">Age</code>和<code class="fe od oe of nt b">salary</code>不在同一尺度上</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="dbe8" class="nx la it nt b gy ny nz l oa ob">sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="382b" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">建立逻辑模型并拟合训练数据</h1><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="2ac3" class="nx la it nt b gy ny nz l oa ob">classifier = LogisticRegression(random_state = 0)</span><span id="a8a3" class="nx la it nt b gy oc nz l oa ob"># fit the classifier into train data<br/>classifier.fit(X_train, y_train)</span><span id="db8a" class="nx la it nt b gy oc nz l oa ob"># predicting the value of y <br/>y_pred = classifier.predict(X_test)</span></pre></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="9cbe" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">绘制决策面—训练结果</h1><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="5ad1" class="nx la it nt b gy ny nz l oa ob">#1. reverse the standard scaler on the X_train<br/>X_set, y_set = sc.inverse_transform(X_train), y_train</span><span id="e638" class="nx la it nt b gy oc nz l oa ob">#2. Generate decision surface boundaries<br/>min1, max1 = X_set[:, 0].min() - 10, X_set[:, 0].max() + 10 # for Age<br/>min2, max2 = X_set[:, 1].min() - 1000, X_set[:, 1].max() + 1000 # for salary</span><span id="9d0f" class="nx la it nt b gy oc nz l oa ob">#3. Set coordinates scale accuracy<br/>x_scale ,y_scale = np.arange(min1, max1, 0.25), np.arange(min2, max2, 0.25)</span><span id="7045" class="nx la it nt b gy oc nz l oa ob">#4. Convert into vector <br/>X1, X2 = np.meshgrid(x_scale, y_scale)</span><span id="3a99" class="nx la it nt b gy oc nz l oa ob">#5. Flatten X1 and X2 and return the output as a numpy array<br/>X_flatten = np.array([X1.ravel(), X2.ravel()])</span><span id="84e7" class="nx la it nt b gy oc nz l oa ob">#6. Transfor the results into it's original form before scaling<br/>X_transformed = sc.transform(X_flatten.T)</span><span id="5f5b" class="nx la it nt b gy oc nz l oa ob">#7. Generate the prediction and reshape it to the X to have the same shape<br/>Z_pred = classifier.predict(X_transformed).reshape(X1.shape)</span><span id="cdae" class="nx la it nt b gy oc nz l oa ob">#8. set the plot size<br/>plt.figure(figsize=(20,10))</span><span id="39a8" class="nx la it nt b gy oc nz l oa ob">#9. plot the contour function<br/>plt.contourf(X1, X2, Z_pred,<br/>                     alpha = 0.75, <br/>                     cmap = ListedColormap((<!-- -->'#386cb0', '#f0027f'<!-- -->)))</span><span id="02e8" class="nx la it nt b gy oc nz l oa ob">#10. setting the axes limit<br/>plt.xlim(X1.min(), X1.max())<br/>plt.ylim(X2.min(), X2.max())</span><span id="fa3d" class="nx la it nt b gy oc nz l oa ob">#11. plot the points scatter plot ( [salary, age] vs. predicted classification based on training set)</span><span id="9bfc" class="nx la it nt b gy oc nz l oa ob">for i, j in enumerate(np.unique(y_set)):<br/>    plt.scatter(X_set[y_set == j, 0], <br/>                X_set[y_set == j, 1], <br/>                c = ListedColormap(('red', 'green'))(i), <br/>                label = j)<br/>    <br/>#12. plot labels and adjustments<br/>plt.title('Logistic Regression (Training set)')<br/>plt.xlabel('Age')<br/>plt.ylabel('Estimated Salary')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/d72dc53001e374d05335ebef3c52f88d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AwHG4QR3_D7SMM8ey-r-YA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策面—训练集</p></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="372a" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">测试集的决策图</h1><p id="06c4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">它与前面的代码完全相同，但是不使用 train，而是使用 test set。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/d42a77cad0e2bcb729816e58fb8394e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qeTPndkT9wErdW9t4pjkEQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策图—测试集</p></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="ec1a" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">结论</h1><p id="8be2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最后，我希望这个样板文件可以帮助可视化分类模型的结果。我建议使用另一个分类模型应用相同的步骤，例如，具有两个以上特征的 SVM。<br/>感谢阅读，我期待着任何建设性的意见。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="6a4a" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">参考</h1><ol class=""><li id="4dbb" class="ne nf it lt b lu lv lx ly ma ol me om mi on mm oo nk nl nm bi translated">Sklearn.datasets A  PI</li><li id="0952" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm oo nk nl nm bi translated"><a class="ae ky" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html" rel="noopener ugc nofollow" target="_blank">利用熊猫转换数据</a></li><li id="cc7b" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm oo nk nl nm bi translated">matplotlib . c<a class="ae ky" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contour.html" rel="noopener ugc nofollow" target="_blank">ontour()A</a>PI</li><li id="1f8b" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm oo nk nl nm bi translated">numpy.meshgrid() A  PI</li><li id="fbfa" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm oo nk nl nm bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html" rel="noopener ugc nofollow" target="_blank">在 iris 数据集上绘制决策树的决策面</a> — sklearn 示例</li><li id="9d4c" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm oo nk nl nm bi translated"><a class="ae ky" href="https://www.kaggle.com/salmaeng/plot-decision-boundry-classification/edit" rel="noopener ugc nofollow" target="_blank">全工作卡格尔笔记本</a></li><li id="064a" class="ne nf it lt b lu nn lx no ma np me nq mi nr mm oo nk nl nm bi translated"><a class="ae ky" href="https://github.com/salma71/blog_post" rel="noopener ugc nofollow" target="_blank"> GitHub 回购</a></li></ol></div></div>    
</body>
</html>