<html>
<head>
<title>Deep Learning For NLP with PyTorch and Torchtext</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 PyTorch 和 Torchtext 实现自然语言处理的深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-for-nlp-with-pytorch-and-torchtext-4f92d69052f?source=collection_archive---------4-----------------------#2020-05-24">https://towardsdatascience.com/deep-learning-for-nlp-with-pytorch-and-torchtext-4f92d69052f?source=collection_archive---------4-----------------------#2020-05-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="cbb6" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">PyTorch 和 Torchtext 教程</h2><div class=""/><div class=""><h2 id="39ba" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">Torchtext 的预训练单词嵌入，数据集 API，迭代器 API，以及带有 Torchtext 和 PyTorch 的训练模型</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/0fc0fc494ecdc6dcfc7e860d0fb77dbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KDLm8JIgbJmZkIXnEQmXBw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片由<a class="ae le" href="https://unsplash.com/@clarephotolover" rel="noopener ugc nofollow" target="_blank">克拉丽莎·沃森</a>在<a class="ae le" href="https://unsplash.com/photos/jAebodq7oxk" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="acc8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">PyTorch 是我一直在使用的一个非常棒的深度学习框架。然而，说到 NLP，不知何故我找不到像<em class="mb"> torchvision </em>那样好的实用程序库。原来 PyTorch 有这个<em class="mb"> torchtext </em>，在我看来，缺少如何使用它的例子，文档[6]可以改进。此外，有一些很好的教程，如[1]和[2]，但是，我们仍然需要更多的例子。</p><p id="ceea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本文的目的是向读者提供关于如何使用<em class="mb"> torchtext </em>的示例代码，特别是使用预先训练的单词嵌入、使用数据集 API、使用迭代器 API 进行小批量，以及最后如何结合使用这些来训练模型。</p><h1 id="db89" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">使用 Torchtext 预训练单词嵌入</h1><p id="3fde" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">在预训练的单词嵌入中有一些替代方法，如 Spacy [3]，Stanza (Stanford NLP)[4]，Gensim [5]，但在本文中，我想重点介绍使用<em class="mb"> torchtext </em>进行单词嵌入。</p><h2 id="d0ad" class="mz md iq bd me na nb dn mi nc nd dp mm lo ne nf mo ls ng nh mq lw ni nj ms iw bi translated">可用单词嵌入</h2><p id="8997" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">你可以在<em class="mb"> torchtext </em> 看到<a class="ae le" href="https://pytorch.org/text/vocab.html#pretrained-word-embeddings" rel="noopener ugc nofollow" target="_blank">预训练单词嵌入列表。在撰写本文时，支持 3 个预先训练好的单词嵌入类:GloVe、FastText 和 CharNGram，没有关于如何加载的更多细节。这里的</a><a class="ae le" href="https://pytorch.org/text/vocab.html#torchtext.vocab.Vocab.load_vectors" rel="noopener ugc nofollow" target="_blank">列出了详尽的列表</a>，但我有时会花时间去阅读，所以我会在这里列出列表。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="4430" class="mz md iq nl b gy np nq l nr ns"><strong class="nl ja">charngram.100d<br/>fasttext.en.300d<br/>fasttext.simple.300d<br/>glove.42B.300d<br/>glove.840B.300d<br/>glove.twitter.27B.25d<br/>glove.twitter.27B.50d<br/>glove.twitter.27B.100d<br/>glove.twitter.27B.200d<br/>glove.6B.50d<br/>glove.6B.100d<br/>glove.6B.200d<br/>glove.6B.300d</strong></span></pre><p id="c831" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有两种方法可以加载预训练的单词嵌入:启动单词嵌入对象或使用<code class="fe nt nu nv nl b">Field</code>实例。</p><p id="176d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">使用现场实例</strong></p><p id="5442" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你需要一些玩具数据集来使用它，所以让我们设置一个。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="dde6" class="mz md iq nl b gy np nq l nr ns">df = pd.DataFrame([<br/>    ['my name is Jack', 'Y'],<br/>    ['Hi I am Jack', 'Y'],<br/>    ['Hello There!', 'Y'],<br/>    ['Hi I am cooking', 'N'],<br/>    ['Hello are you there?', 'N'],<br/>    ['There is a bird there', 'N'],<br/>], columns=['text', 'label'])</span></pre><p id="b399" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后我们可以构造<code class="fe nt nu nv nl b">Field </code>对象来保存特性列和标签列的元数据。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="3034" class="mz md iq nl b gy np nq l nr ns">from torchtext.data import Field</span><span id="0dea" class="mz md iq nl b gy nw nq l nr ns">text_field = Field(<br/>    tokenize='basic_english', <br/>    lower=True<br/>)</span><span id="7948" class="mz md iq nl b gy nw nq l nr ns">label_field = Field(sequential=False, use_vocab=False)</span><span id="b7ba" class="mz md iq nl b gy nw nq l nr ns"># sadly have to apply preprocess manually<br/>preprocessed_text = df['text'].apply(lambda x: text_field.preprocess(x))</span><span id="5d88" class="mz md iq nl b gy nw nq l nr ns"># load fastext simple embedding with 300d<br/>text_field.build_vocab(<br/>    preprocessed_text, <br/>    vectors='fasttext.simple.300d'<br/>)</span><span id="8dcb" class="mz md iq nl b gy nw nq l nr ns"># get the vocab instance<br/>vocab = text_field.vocab</span></pre><p id="b475" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">要获得预训练单词嵌入的真实实例，可以使用</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="a5ae" class="mz md iq nl b gy np nq l nr ns">vocab.vectors</span></pre><p id="56d9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">启动 Word 嵌入对象</strong></p><blockquote class="nx ny nz"><p id="a54e" class="lf lg mb lh b li lj ka lk ll lm kd ln oa lp lq lr ob lt lu lv oc lx ly lz ma ij bi translated">对于这些代码中的每一个，它将下载大量的单词嵌入，所以你必须有耐心，不要一次执行下面所有的代码。</p></blockquote><p id="80f9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">快速文本</em></p><p id="737a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">FastText 对象有一个参数:language，它可以是“simple”或“en”。目前他们只支持 300 个嵌入维度，如上面的嵌入列表中所提到的。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="435c" class="mz md iq nl b gy np nq l nr ns">from torchtext.vocab import FastText<br/>embedding = FastText('simple')</span></pre><p id="2d4f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">查恩格拉姆</em></p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="d90a" class="mz md iq nl b gy np nq l nr ns">from torchtext.vocab import CharNGram<br/>embedding_charngram = CharNGram()</span></pre><p id="cf75" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">手套</em></p><p id="ee60" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">手套对象有两个参数:名称和尺寸。您可以查看每个参数支持的可用嵌入列表。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="9d01" class="mz md iq nl b gy np nq l nr ns">from torchtext.vocab import GloVe<br/>embedding_glove = GloVe(name='6B', dim=100)</span></pre><h2 id="4ce8" class="mz md iq bd me na nb dn mi nc nd dp mm lo ne nf mo ls ng nh mq lw ni nj ms iw bi translated">使用单词嵌入</h2><p id="1ccb" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">使用<em class="mb"> torchtext </em> API 使用单词嵌入超级简单！假设您已经将嵌入存储在变量<code class="fe nt nu nv nl b">embedding</code>中，那么您可以像使用 python 的<code class="fe nt nu nv nl b">dict</code>一样使用它。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="f42c" class="mz md iq nl b gy np nq l nr ns"># known token, in my case print 12<br/>print(vocab['are'])<br/># unknown token, will print 0<br/>print(vocab['crazy'])</span></pre><p id="972b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如你所看到的，它已经处理了未知的令牌，没有抛出错误！如果您尝试将单词编码成整数，您会注意到，默认情况下，unknown token 将被编码为<code class="fe nt nu nv nl b">0</code>，而 pad token 将被编码为<code class="fe nt nu nv nl b">1</code>。</p><h1 id="5ade" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">使用数据集 API</h1><p id="03f2" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">假设变量<code class="fe nt nu nv nl b">df</code>已经如上定义，我们现在通过为特征和标签构建<code class="fe nt nu nv nl b">Field</code>来准备数据。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="5367" class="mz md iq nl b gy np nq l nr ns">from torchtext.data import Field</span><span id="d7c1" class="mz md iq nl b gy nw nq l nr ns">text_field = Field(<br/>    sequential=True,<br/>    tokenize='basic_english', <br/>    fix_length=5,<br/>    lower=True<br/>)</span><span id="0640" class="mz md iq nl b gy nw nq l nr ns">label_field = Field(sequential=False, use_vocab=False)</span><span id="3ac3" class="mz md iq nl b gy nw nq l nr ns"># sadly have to apply preprocess manually<br/>preprocessed_text = df['text'].apply(<br/>    lambda x: text_field.preprocess(x)<br/>)</span><span id="7df5" class="mz md iq nl b gy nw nq l nr ns"># load fastext simple embedding with 300d<br/>text_field.build_vocab(<br/>    preprocessed_text, <br/>    vectors='fasttext.simple.300d'<br/>)</span><span id="7bcb" class="mz md iq nl b gy nw nq l nr ns"># get the vocab instance<br/>vocab = text_field.vocab</span></pre><blockquote class="nx ny nz"><p id="ecef" class="lf lg mb lh b li lj ka lk ll lm kd ln oa lp lq lr ob lt lu lv oc lx ly lz ma ij bi translated">这里有一点警告，<code class="fe nt nu nv nl b">Dataset.split</code>可能返回 3 个数据集(<strong class="lh ja"> train，val，test </strong>)，而不是定义的 2 个值</p></blockquote><h1 id="9fb4" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">使用迭代器类进行小型批处理</h1><p id="406f" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">我没有找到任何现成的<code class="fe nt nu nv nl b">Dataset</code> API 来加载 pandas <code class="fe nt nu nv nl b">DataFrame</code>到<em class="mb"> torchtext </em>数据集，但是创建一个还是很容易的。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="b918" class="mz md iq nl b gy np nq l nr ns">from torchtext.data import Dataset, Example<br/></span><span id="f239" class="mz md iq nl b gy nw nq l nr ns">ltoi = {l: i for i, l in enumerate(df['label'].unique())}<br/>df['label'] = df['label'].apply(lambda y: ltoi[y])</span><span id="55c0" class="mz md iq nl b gy nw nq l nr ns">class DataFrameDataset(Dataset):<br/>    def __init__(self, df: pd.DataFrame, fields: list):<br/>        super(DataFrameDataset, self).__init__(<br/>            [<br/>                Example.fromlist(list(r), fields) <br/>                for i, r in df.iterrows()<br/>            ], <br/>            fields<br/>        )</span></pre><p id="bef3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们现在可以构建<code class="fe nt nu nv nl b">DataFrameDataset</code>并用熊猫数据帧初始化它。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="b677" class="mz md iq nl b gy np nq l nr ns">train_dataset, test_dataset = DataFrameDataset(<br/>    df=df, <br/>    fields=(<br/>        ('text', text_field),<br/>        ('label', label_field)<br/>    )<br/>).split()</span></pre><p id="809d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后我们使用<code class="fe nt nu nv nl b">BucketIterator</code>类来轻松地<strong class="lh ja">构造迷你批处理迭代器</strong>。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="e655" class="mz md iq nl b gy np nq l nr ns">from torchtext.data import BucketIterator</span><span id="28a3" class="mz md iq nl b gy nw nq l nr ns">train_iter, test_iter = BucketIterator.splits(<br/>    datasets=(train_dataset, test_dataset), <br/>    batch_sizes=(2, 2),<br/>    sort=False<br/>)</span></pre><p id="b231" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">记住使用 sort=False </strong>否则当你试图迭代<code class="fe nt nu nv nl b">test_iter</code>时会导致错误，因为我们还没有定义 sort 函数，然而不知何故，默认情况下<code class="fe nt nu nv nl b">test_iter</code>被定义为排序。</p><blockquote class="nx ny nz"><p id="374d" class="lf lg mb lh b li lj ka lk ll lm kd ln oa lp lq lr ob lt lu lv oc lx ly lz ma ij bi translated">小提示:虽然我同意我们应该使用<code class="fe nt nu nv nl b">DataLoader</code> API 来处理迷你批处理，但是目前<strong class="lh ja">我还没有探索过</strong>如何使用<code class="fe nt nu nv nl b">DataLoader</code>和 torchtext。</p></blockquote><h1 id="848f" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">培训 PyTorch 模型的示例</h1><p id="e3eb" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">让我们使用 1 个嵌入层和 1 个线性层定义一个任意 PyTorch 模型。在当前的例子中，我没有使用预训练的单词嵌入，而是使用新的未训练的单词嵌入。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="f56d" class="mz md iq nl b gy np nq l nr ns">import torch.nn as nn<br/>import torch.nn.functional as F<br/>from torch.optim import Adam</span><span id="e3a4" class="mz md iq nl b gy nw nq l nr ns">class ModelParam(object):<br/>    def __init__(self, param_dict: dict = dict()):<br/>        self.input_size = param_dict.get('input_size', 0)<br/>        self.vocab_size = param_dict.get('vocab_size')<br/>        self.embedding_dim = param_dict.get('embedding_dim', 300)<br/>        self.target_dim = param_dict.get('target_dim', 2)<br/>        <br/>class MyModel(nn.Module):<br/>    def __init__(self, model_param: ModelParam):<br/>        super().__init__()<br/>        self.embedding = nn.Embedding(<br/>            model_param.vocab_size, <br/>            model_param.embedding_dim<br/>        )<br/>        self.lin = nn.Linear(<br/>            model_param.input_size * model_param.embedding_dim, <br/>            model_param.target_dim<br/>        )<br/>        <br/>    def forward(self, x):<br/>        features = self.embedding(x).view(x.size()[0], -1)<br/>        features = F.relu(features)<br/>        features = self.lin(features)<br/>        return features</span></pre><p id="6728" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后，我可以很容易地重复如下的训练(和测试)例程。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div></figure><h2 id="931d" class="mz md iq bd me na nb dn mi nc nd dp mm lo ne nf mo ls ng nh mq lw ni nj ms iw bi translated">重用预先训练的单词嵌入</h2><p id="dcac" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">很容易将当前定义的模型修改为使用预训练嵌入的模型。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="8bdc" class="mz md iq nl b gy np nq l nr ns">class MyModelWithPretrainedEmbedding(nn.Module):<br/>    def __init__(self, model_param: ModelParam, embedding):<br/>        super().__init__()<br/>        self.embedding = embedding<br/>        self.lin = nn.Linear(<br/>            model_param.input_size * model_param.embedding_dim, <br/>            model_param.target_dim<br/>        )<br/>        <br/>    def forward(self, x):<br/>        features = self.embedding[x].reshape(x.size()[0], -1)<br/>        features = F.relu(features)<br/>        features = self.lin(features)<br/>        return features</span></pre><p id="a3ef" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我做了 3 行修改。您应该注意到，我已经更改了构造函数输入以接受嵌入。此外，我还将<code class="fe nt nu nv nl b">view</code>方法改为<code class="fe nt nu nv nl b">reshape </code>，并使用 get 操作符<code class="fe nt nu nv nl b">[] </code>而不是 call 操作符<code class="fe nt nu nv nl b">() </code>来访问嵌入。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="7b55" class="mz md iq nl b gy np nq l nr ns">model = MyModelWithPretrainedEmbedding(model_param, vocab.vectors)</span></pre><h1 id="0fd4" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">结论</h1><p id="40c9" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">我已经完成了使用<em class="mb"> torchtext </em>在 PyTorch 中处理文本数据的探索。我开始写这篇文章是因为我在使用互联网上现有的教程时遇到了困难。我希望这篇文章也能减少其他人的开销。</p><p id="dfa1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">写这段代码需要帮助吗？这里有一个谷歌 Colab 的链接。</p><blockquote class="of"><p id="f301" class="og oh iq bd oi oj ok ol om on oo ma dk translated"><a class="ae le" href="https://colab.research.google.com/github/ariepratama/python-playground/blob/master/dl-for-nlp-pytorch-torchtext.ipynb" rel="noopener ugc nofollow" target="_blank">链接到 Google Colab </a></p></blockquote><h1 id="841e" class="mc md iq bd me mf mg mh mi mj mk ml mm kf op kg mo ki oq kj mq kl or km ms mt bi translated">参考</h1><p id="124e" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">[1]聂，A . torch text 教程。2017.<a class="ae le" href="http://anie.me/On-Torchtext/" rel="noopener ugc nofollow" target="_blank">http://anie.me/On-Torchtext/</a></p><p id="b9bf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2]用 TorchText 教程进行文本分类。<a class="ae le" href="https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/初学者/text _ 情操 _ngrams_tutorial.html </a></p><p id="a9a2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3]节文档。<a class="ae le" href="https://stanfordnlp.github.io/stanza/" rel="noopener ugc nofollow" target="_blank">https://stanfordnlp.github.io/stanza/</a></p><p id="3f1e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[4] Gensim <a class="ae le" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank">文档。https://radimrehurek.com/gensim/</a></p><p id="794b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[5]空间文件。<a class="ae le" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">https://spacy.io/</a></p><p id="7f47" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[6] Tor <a class="ae le" href="https://pytorch.org/text/" rel="noopener ugc nofollow" target="_blank"> chtext 文档。https://pytorch.org/text/</a></p></div></div>    
</body>
</html>