<html>
<head>
<title>Nvidia gave me a $15K Data Science Workstation — here’s what I did with it</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Nvidia给了我一个价值15000美元的数据科学工作站——以下是我用它做的事情</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35?source=collection_archive---------2-----------------------#2020-02-25">https://towardsdatascience.com/nvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35?source=collection_archive---------2-----------------------#2020-02-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="befc" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/video-tutorial" rel="noopener" target="_blank">视频教程</a></h2><div class=""/><div class=""><h2 id="be79" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">在几个小时而不是几个星期内，在数据科学WhisperStation上重建一个大规模的Pubmed文献搜索项目</h2></div><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="kt ku l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><a class="ae kz" href="https://www.youtube.com/watch?v=WCz8AF-wT1I" rel="noopener ugc nofollow" target="_blank">在YouTube上观看</a>并查看<a class="ae kz" href="https://github.com/kylegallatin/gpu-accerelated-search-notebook" rel="noopener ugc nofollow" target="_blank">相关的git回购</a></p></figure><p id="c384" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">当NVIDIA问我是否想试试最新的数据科学工作站时，我很兴奋。然而，一个发人深省的想法伴随着兴奋而来:<em class="lw">我到底应该用它来做什么？</em></p><p id="02c6" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">作为一名机器学习工程师，我做了很多深度学习，但我绝对不是谷歌大脑研究员。我可以运行基准测试，时间工作等…但我不在英伟达工作，老实说，这听起来不太有趣。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi lx"><img src="../Images/5f905f5383888eda939ce482bbbed46a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1nrITWY9QGOGMUUb.jpg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><a class="ae kz" href="https://www.nvidia.com/content/dam/en-zz/Solutions/deep-learning/deep-learning-solutions/data-science/data-science-laptop-workstation-4c25-p@2x.jpg" rel="noopener ugc nofollow" target="_blank">该死的</a> —基于NVIDIA的数据科学工作站</p></figure><p id="ff3f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我不断推销自己的想法，并开始思考强大计算对数据科学家的真正价值。精心设计的GPU计算可以节省成本，降低服务延迟，简化大型模型的训练，但我最感兴趣的是快速迭代。</p><p id="fa32" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">数据科学是一个以实验为基础的领域。对于大数据或大模型，如果没有大量资源，科学家尝试新配置或参数的次数是有限的。每个人都知道启动一个计算密集型进程的痛苦，只是在运行它的几个小时内被一个不可预见的错误弄得措手不及。然后你要改正，重新开始。</p><p id="e688" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我回想起我的第一个数据科学项目:一个大规模、多语言的医学文献搜索引擎。如果我在2020年回到2017年，能够访问我现在拥有的计算和GPU库，我可能已经能够完成什么？我能以多快的速度完成它？</p><p id="5ebb" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，我试图通过构建一个仅使用GPU资源的Pubmed搜索引擎来回答这个问题。</p><h1 id="9769" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">该项目</h1><p id="c8c8" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">最初的项目来自中国的一个团队，那里的制药公司有责任向医疗保健从业者(hcp)提供非品牌医疗信息。该团队希望构建一个新颖的搜索工具，它需要:</p><ul class=""><li id="8b70" class="nb nc iq lc b ld le lg lh lj nd ln ne lr nf lv ng nh ni nj bi translated">多语言(<em class="lw">中文/中文</em>)</li><li id="b464" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv ng nh ni nj bi translated">可通过多种渠道(网络/微信)获得</li><li id="1dd1" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv ng nh ni nj bi translated">可定制(能够调整算法以获得最佳结果)</li><li id="0b93" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv ng nh ni nj bi translated">低延迟(快速搜索结果)</li><li id="8287" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv ng nh ni nj bi translated">大数据(所有的Pubmed摘要—这里我以过去10年为试点)</li><li id="6684" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv ng nh ni nj bi translated">准确(比Pubmed搜索结果更好)</li></ul><p id="e4b7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我的工作是设计一个能够满足并支持所有这些需求的解决方案。此外，这个解决方案需要通过迭代相对快速地周转，以便我们能够基于来自主题专家(SME)的输入快速地测试、评估和更新系统。</p><p id="eaea" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">然而，作为一名数据科学家，我有一个大问题…</p><h1 id="cbd1" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">数据</h1><p id="6325" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">对于那些不知道的人来说，Pubmed是一个包含超过3000万次引用的生物医学文献数据库。虽然不是所有的全文文章都是开源的，但是每个引用都有摘要。所有这些信息都可以通过API获得，或者以XML文件的形式从<a class="ae kz" href="https://mbr.nlm.nih.gov/Download/Baselines/2019/" rel="noopener ugc nofollow" target="_blank">批量下载，不幸的是，对我来说，大约300GB分布在一千个文件中。</a></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi np"><img src="../Images/e0654c692a03f7c9f7e5b97e62a01ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ObaDO2bu-RqLr1q--oSa0Q.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">这张照片体现了“搜索”吗？</p></figure><p id="1280" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在解析每个XML以提取有用的字段(标题、摘要、出版年份等)之后，数据大小减少到接近25GB。然而，这仍然不是超级本地可管理的。就像我之前说过的，数据科学是关于实验的——我需要做很多实验。作为一个有点新的数据科学家(和糟糕的工程师)，我基本上只有一个资源:Python。没有20个节点的Spark或Elasticsearch集群来救我。</p><p id="2d26" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是，如果我有一两个GPU呢？</p><h1 id="bef3" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">工作站</h1><p id="9e40" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">我通过Microway &amp;获得访问权限的<a class="ae kz" href="https://www.microway.com/preconfiguredsystems/data-science-whisperstation-nvidia-data-science-workstation/" rel="noopener ugc nofollow" target="_blank">数据科学耳语站</a>Nvidia具有以下特性:</p><ul class=""><li id="1094" class="nb nc iq lc b ld le lg lh lj nd ln ne lr nf lv ng nh ni nj bi translated">双英特尔至强10核CPU</li><li id="f572" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv ng nh ni nj bi translated">192GB内存</li><li id="1bf7" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv ng nh ni nj bi translated">高速1TB暂存驱动器</li><li id="4343" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv ng nh ni nj bi translated">带NVLink的双NVIDIA Quadro RTX 6000 GPU</li><li id="e01d" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv ng nh ni nj bi translated">预装和配置Python，Docker，RAPIDs和我这个项目需要的所有机器学习库</li></ul><p id="7b59" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们真正关心的是最后两颗子弹。你可能知道，GPU计算在数据科学中非常流行。使用GPU库运行工作流可以将代码速度提高几个数量级，这意味着每次实验运行只需几个小时，而不是几周。</p><p id="78eb" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">此外，如果您曾经从零开始建立过数据科学环境，您会知道这真的很糟糕。安装和配置Docker、RAPIDs、tensorflow、pytorch和其他所有现成的软件节省了安装时间。</p><p id="f334" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">直到最近，GPU主要用于数据科学中的深度学习。然而，不久前Nvidia发布了<a class="ae kz" href="https://rapids.ai/" rel="noopener ugc nofollow" target="_blank">RAPIDS</a>——一个针对GPU的通用数据科学库。急流由以下部分组成:</p><ul class=""><li id="626a" class="nb nc iq lc b ld le lg lh lj nd ln ne lr nf lv ng nh ni nj bi translated">cudf (基本是熊猫)</li><li id="3e86" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv ng nh ni nj bi translated"><em class="lw"> cuml </em>(基本上是scikit-learn)</li><li id="7640" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv ng nh ni nj bi translated"><em class="lw"> cugraph </em>(基本是网络-X)</li></ul><p id="9487" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这些通用数据科学库为传统的CPU处理(数据加载、清理、特征工程、线性模型等)提供了大量的计算增强，为数据科学的全新前沿铺平了道路。</p><h1 id="34fe" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">熊猫到急流城</h1><p id="3b91" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">在现代，简单的搜索是一个相当简单的过程。单词可以用数字向量来表示，然后计算这些向量之间的距离，就可以看出段落有多“相似”。最简单的方法是使用TF-IDF字向量的余弦相似度。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nq"><img src="../Images/afbb16c5c4f5c98b128fc026fbdc33c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CmYmTmrFAFf-HEfS.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><a class="ae kz" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fdevblogs.nvidia.com%2Fgpu-accelerated-analytics-rapids%2F&amp;psig=AOvVaw3tl2r6V5aeZNUhBe_5KKnC&amp;ust=1582231062758000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCLiuvZa83ucCFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">急流</a></p></figure><p id="4e3c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">然而，为了“矢量化”我们的文本，我们首先需要实际读入并预处理它。假设XML → csv预处理已经完成，现在我们只需要将它们作为数据帧读入并执行相关的预处理。</p><p id="4e84" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在，我们有大约23GB的数据，只有两个GPU。我并不幻想能够用Python将所有这些都放入GPU内存中。幸运的是，就像科学文献一样，只有最近的文章才是相关的。为了评估专家搜索的准确性，我真的只需要Pubmed最近10年的数据——根据我的数据集，大约有800万篇文章。</p><p id="943b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">使用受CPU限制的进程会很费力，但是用cuda加速会有很大的不同。我想:</p><ol class=""><li id="459f" class="nb nc iq lc b ld le lg lh lj nd ln ne lr nf lv nr nh ni nj bi translated">读入数据帧</li><li id="9f33" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv nr nh ni nj bi translated">清除“摘要”列中的字符串</li><li id="3a0f" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv nr nh ni nj bi translated">仅保留≥ 2009年的年份</li><li id="1020" class="nb nc iq lc b ld nk lg nl lj nm ln nn lr no lv nr nh ni nj bi translated">重写为csv</li></ol><p id="0a29" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这就是cudf的用武之地。我真的写了熊猫代码，做了查找/替换，我还有GPU加速代码！</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ns ku l"/></div></figure><p id="b970" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这比在本地处理数据帧要快得多。以下是使用pandas的本地输出示例:</p><pre class="ko kp kq kr gt nt nu nv nw aw nx bi"><span id="0548" class="ny mf iq nu b gy nz oa l ob oc">Processed 13783 abstracts in 0.84604811668396 seconds<br/>Processed 21714 abstracts in 1.2190630435943604 seconds<br/>Processed 20259 abstracts in 1.1971170902252197 seconds</span></pre><p id="20b9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">下面是使用cudf的工作站上的流程输出:</p><pre class="ko kp kq kr gt nt nu nv nw aw nx bi"><span id="8560" class="ny mf iq nu b gy nz oa l ob oc">Processed 23818 abstracts in 0.3909769058227539 seconds<br/>Processed 23609 abstracts in 0.5951714515686035 seconds<br/>Processed 23929 abstracts in 0.3672349452972412 seconds</span></pre><p id="6b9e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">每个文件的处理速度提高了一倍以上，而代码只需要一个GPU！更好的是，我们可以通过创建cuda集群和使用dask来使用两个GPU中的所有内存。</p><p id="66ad" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">如果我只是想阅读所有的摘要，然后用它们做些别的事情，dask用最少的代码行就能做到非常高效。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ns ku l"/></div></figure><p id="bc9f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">上面的代码在我的Pubmed数据子集上产生了以下输出(所有的Pubmed都会抛出一个内存错误——这并不奇怪)。</p><pre class="ko kp kq kr gt nt nu nv nw aw nx bi"><span id="7a22" class="ny mf iq nu b gy nz oa l ob oc">Read 7141779 abstract in 64.332682 seconds</span></pre><p id="a740" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">使用<code class="fe od oe of nu b">watch -n 0.5 nvidia-smi</code>在单独的窗口中检查GPU使用的输出，您可以观察您的进程运行并监控内存使用。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi og"><img src="../Images/6af5b6b9b5748b8cc354dcf392af4286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LFkS2O0mX52FrHVX5_RLMA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">监控工作站上的GPU使用情况</p></figure><h1 id="9d5d" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">GPU加速余弦相似度</h1><p id="d564" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">既然我现在知道我可以将过去十年的Pubmed数据加载到GPU内存中，我就可以进入有趣的部分了:实际的TF-IDF矢量化。在scikit中——了解这一点相当容易，请在这里查看我的<a class="ae kz" href="https://gist.github.com/kylegallatin/0860a1b51101c7bd9c2fcc0d7b6f0906" rel="noopener ugc nofollow" target="_blank">完整CPU实现</a>。使用cuml，我们应该能够像对熊猫那样找到并替换，但不幸的是…</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/b678c655b6638fc8631831899884195d.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*tToSnZ07s-Xc9P_zwKYIdA.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">失败:(</p></figure><p id="a254" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">根据<a class="ae kz" href="https://github.com/rapidsai/cuml/issues/1266" rel="noopener ugc nofollow" target="_blank">这期Github</a>的消息，在撰写本文时，cuml的文本特征提取库仍在工作中(但一旦完成，我会用代码更新！).这意味着我们的矢量器仍然需要用scikit-learn来实现，我们还不能在这个TF-IDF任务上获得GPU加速。这只是针对训练步骤，但这意味着我们的TF-IDF矢量器将仍然受到CPU的限制，因此效率低下。</p><p id="0a5c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">然而，这还没有结束。即使训练本身是低效的，那一步真的只需要发生一次。幸运的是，sklearn的TF-IDF矢量器的输出只是一个稀疏矩阵——当我们回到处理矩阵时，我们可以从一些经典的张量库中获得帮助。我决定用tensorflow。</p><p id="c668" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">正如所料，矩阵乘法是任何张量库的隐含部分。在sklearn中训练了我的矢量器之后，我可以用tensorflow将实际的矢量移植回GPU来执行矩阵乘法。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ns ku l"/></div></figure><p id="c903" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在，从理论上讲，这对于Pubmed的一小部分非常有效——但是它没有伸缩性。在所有的Pubmed(甚至我们的子集)中，有相当多的独特的单词。由于2009年以后Pubmed中的每一个引用都有一个向量，我们的稀疏矩阵变得庞大。我想大概是800万T4乘以100万T5。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi oi"><img src="../Images/64051ff7968d6e7b818ee7c6ad13c6d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tPPiDSrz5HVE7ofxmkL9PA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">是的，大惊喜，凯尔，干得好，伙计</p></figure><p id="8296" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">受到阻碍的不是硬件，而是软件。试图在sklearn和tensorflow之间来回切换导致了一系列问题。意识到这种方法需要更多的时间和技能，这是我继续前进或者成为一名更好的工程师的时候了。是时候转向深度学习表示了。</p><h1 id="2e3b" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">创建GPU加速的BERT索引</h1><h2 id="d650" class="ny mf iq bd mg oj ok dn mk ol om dp mo lj on oo mq ln op oq ms lr or os mu iw bi translated">使用BERT向量化Pubmed</h2><p id="fb7a" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">NLP中的变压器的最新进展已经在各种任务中显示出巨大的改进。虽然此后出现了许多模型，但这场革命的起源是谷歌的BERT。像其他一些基于DL的模型一样，BERT为句子生成一个<em class="lw">上下文</em>向量。维数(向量的长度)等于隐藏层的大小，在最新推荐的BERT-large模型中是1024。</p><p id="ff4f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这是巨大的。即使我们不能再使用稀疏矩阵，我们向量的大小也从几百万x几百万→几百万x几千。在空间有限的GPU上，这一点非常重要。</p><p id="3426" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">通常BERT用于分类任务，但是在我们的例子中，我们只是想用它来提取Pubmed摘要的矢量化表示，这样它们就可以被索引和搜索。感谢腾讯研究院，我们已经有了一个设计良好且支持GPU的库:<a class="ae kz" href="https://github.com/hanxiao/bert-as-service" rel="noopener ugc nofollow" target="_blank"> BERT as a service </a>。</p><p id="2a1c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">您可以按照repo中的说明来实际安装该服务。一旦您的环境中有了它，您所要做的就是下载您喜欢的BERT模型并启动它。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ns ku l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">下载模型并启动服务</p></figure><p id="17f6" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在您已经运行了服务，可以调用简单的Python来获取您想要BERT表示的任何文本的向量。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ns ku l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">使用bert-as-service对文本进行矢量化</p></figure><p id="4690" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">很简单。通过使用工作站上两个GPU的BERT服务，大量摘要以惊人的速度通过模型。下面是我为每个csv计时时的输出:</p><pre class="ko kp kq kr gt nt nu nv nw aw nx bi"><span id="2c40" class="ny mf iq nu b gy nz oa l ob oc">Vectorized 23727 abstracts in 53.800883 seconds<br/>Vectorized 25402 abstracts in 56.999314 seconds<br/>Vectorized 25402 abstracts in 57.235494 seconds<br/>Vectorized 23575 abstracts in 50.786675 seconds<br/>Vectorized 17773 abstracts in 33.936309 seconds<br/>Vectorized 24190 abstracts in 53.914434 seconds</span></pre><p id="fb58" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">即使有了工作站，这个过程也需要一段时间——这让你知道没有它需要多长时间。同样值得注意的是，这次包括用cudf读入数据。为了说明GPU加速和本地计算之间的差距有多大，下面是使用我的个人笔记本电脑的相同过程:</p><pre class="ko kp kq kr gt nt nu nv nw aw nx bi"><span id="4f90" class="ny mf iq nu b gy nz oa l ob oc">Vectorized 13172 abstracts in 2048.069033 seconds</span></pre><p id="7f65" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="lw"> 30分钟</em>。我花了将近30分钟在工作站上对&lt; 60秒处理的<em class="lw">一半</em>的摘要进行矢量化处理。即使只是从如此大的模型中获取向量，GPU计算也让我在代码运行时不用整天摆弄手指。</p><h2 id="f109" class="ny mf iq bd mg oj ok dn mk ol om dp mo lj on oo mq ln op oq ms lr or os mu iw bi translated">使用Faiss索引</h2><p id="7dc2" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">这一次，我将把它交给一个设计良好的快速索引库，而不是自己做矩阵乘法。脸书的faiss易于使用，GPU能力使其成为索引伯特矢量的完美工具。要在faiss中创建一个基于GPU的平面，我们只需要大约10行代码。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ns ku l"/></div></figure><p id="af1d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一旦你有了索引，你要做的就是把向量放进去。为了节省GPU内存，我建议先单独使用BERT服务对文本进行矢量化，然后保存到磁盘。然后，您可以加载和索引向量，而不需要服务在后台运行。但是，如果您愿意，也可以一次性完成。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ns ku l"/></div></figure><p id="eb28" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">创建索引本身后，可以在一行中完成搜索。但是这个规模会大吗？如果我想使用这段代码来检索结果，或者甚至将一个模型投入生产，我想确保搜索尽可能快地运行。我对大约300万份摘要进行了基准搜索，搜索结果仍然花费了不到0.1秒。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/edaaabe7f38d50a088b88fe2339b6f7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*BPP20FdMqNKTtTSxd92FLg.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">即使是250万份摘要，在工作站上使用faiss的搜索查询时间仍然不到10毫秒</p></figure><p id="be74" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">终于？健全检查。我一直在假设SME能够评估搜索的准确性的情况下进行搜索。然而，如果搜索结果如此糟糕，那就毫无意义了，我将不得不从头开始，或者彻底重构我的方法。幸运的是，情况并非如此。</p><pre class="ko kp kq kr gt nt nu nv nw aw nx bi"><span id="86ab" class="ny mf iq nu b gy nz oa l ob oc">&gt;&gt;&gt; search_term = "parkinsons disease" # search parkinsons<br/>&gt;&gt;&gt; search_vector = bc.encode([search_term]) # encode term<br/>&gt;&gt;&gt; distances,indicies = index.search( # get top 3 results<br/>    search_vector.astype('float32'), <br/>    k=3)<br/>&gt;&gt;&gt;for i in indicies[0]:<br/>...    print(text[i][0:500], sep = "\n") # print first 500 char</span><span id="87ea" class="ny mf iq nu b gy ou oa l ob oc">Deep brain stimulation (DBS) improves motor symptoms in Parkinson's disease (PD), but questions remain regarding neuropsychological decrements sometimes associated with this treatment, including rates of statistically and clinically meaningful change, and whether there are differences in outcome related to surgical target.</span><span id="f4e5" class="ny mf iq nu b gy ou oa l ob oc">Neuropsychological functioning was assessed in patients with Parkinson's disease (PD) at baseline and after 6 months in a prospective, randomised, controlled study comparing</span><span id="fd6d" class="ny mf iq nu b gy ou oa l ob oc">Kennedy's disease (KD) is a progressive degenerative disorder affecting lower motor neurons. We investigated the correlation between disease severity and whole brain white matter microstructure, including upper motor neuron tracts, by using diffusion-tensor imaging (DTI) in eight patients with KD in whom disease severity was evaluated using the Amyotrophic Lateral Sclerosis Functional Rating Scale (ALSFRS).</span><span id="0e07" class="ny mf iq nu b gy ou oa l ob oc">From DTI acquisitions we obtained maps of fractional anisotropy (FA), mean diffusivity (</span><span id="bc03" class="ny mf iq nu b gy ou oa l ob oc">Autophagy is associated with the pathogenesis of Lewy body disease, including Parkinson's disease (PD) and dementia with Lewy bodies (DLB). It is known that several downstream autophagosomal proteins are incorporated into Lewy bodies (LBs). We performed immunostaining and Western blot analysis using a cellular model of PD and human brain samples to investigate the involvement of upstream autophagosomal proteins (ULK1, ULK2, Beclin1, VPS34 and AMBRA1), which initiate autophagy and form autophago</span></pre><p id="2bf1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">快速浏览显示，aa上下文搜索“帕金森病”会返回该领域的相关摘要(以我外行的评价)。</p><p id="5e2a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，让我们回顾一下需求，看看这种方法是否解决了该项目的所有需求:</p><p id="bc9b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">多语言(en/zh) </em> ✅: </strong> BERT支持104种语言！</p><p id="3b24" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">多渠道(网络/微信)</em></strong>✅:用API包好，端上桌。</p><p id="3415" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">可定制(能够调整算法以获得最佳结果)</em> ✅: </strong>我使用的是BERT base，但这里也可以使用Bio-BERT或任何其他微调过的BERT。此外，我们可以在这些结果上叠加轻量级分类算法或启发式算法，以进一步提高准确性。</p><p id="4541" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">低延迟(快速搜索结果)</em> ✅: </strong>使用Pubmed abstracts的近1/3，延迟仍为&lt; 0.1秒，看起来比例合理。</p><p id="3e71" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">支持大数据(所有Pubmed摘要等)</em> ✅: </strong>我们只使用2009年的引文进行验证，但是有了更多的GPU和更好的工程师，你可以很容易地将其扩展到整个Pubmed。</p><p id="a592" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">准确(比Pubmed更好的搜索结果)</em>🤔:</strong>有待观察。中小企业必须用Pubmed search对搜索结果进行评级和比较，并随着时间的推移调整算法。然而，由于大约700万份摘要的短期周转，工作站使得这种周转相对较快的方法非常可行。此外，虽然健全性检查缺乏规模，但它至少表明这种方法可能值得探索。</p><h1 id="8300" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">结论</h1><p id="e678" class="pw-post-body-paragraph la lb iq lc b ld mw ka lf lg mx kd li lj my ll lm ln mz lp lq lr na lt lu lv ij bi translated">在充斥着杂乱无章的文档的大公司中，信息检索非常重要。检索这些文档的智能解决方案非常受欢迎。虽然许多供应商都提供了强大的企业级解决方案，但要在如此短的时间内组织如此大规模的信息，现在只有通过2010年代后期的硬件和软件进步才有可能。</p><p id="b2cd" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在几周的空闲时间里，我创造、重复和修改了我解决这个问题的方法。多亏了工作站和开源的力量，我在那段时间里成功地完成了我的目标。我没有等待几个星期来运行代码，而是不断收到反馈，并尽早解决错误。结果，我的代码和这个个人项目以指数级的速度进步。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/c6e2b05f458b26fd8a957e9beac6e222.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*glD7bNJG3SlO0_xNmSGPcQ.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><a class="ae kz" href="https://www.docker.com/sites/default/files/social/docker_facebook_share.png'" rel="noopener ugc nofollow" target="_blank">我太喜欢这个小家伙了，他们帮我解决了几个小时的头痛问题</a></p></figure><p id="cee9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因为我基本上已经在一个生产环境中工作，所以也很容易过渡到更多托管的云主机进行部署。虽然我的代码与生产代码相去甚远，但使用Docker使我能够确保我构建的所有东西都可以被预打包，并被发送到我喜欢的任何映像注册中心和部署方案。</p><p id="5660" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">显然，15000美元对于某些硬件来说是一笔不小的数目。但是，如果你是一个寻求快速试验和周转的企业组织，这是有意义的。作为对比，这里有一个专用AWS p3.8x large (4特斯拉V100s)的报价。一年75，000美元，以及自己安装所有库和工具的麻烦。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ow"><img src="../Images/52306cddd52279dd010275f9116663eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x1Z2nuMBNzcCJEtZtNKhLA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">专用GPU资源的AWS定价</p></figure><p id="bcd5" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个问题有不涉及GPU的解决方案。由于elasticsearch现在有了对向量评分的<a class="ae kz" href="https://www.elastic.co/jp/blog/text-similarity-search-with-vectors-in-elasticsearch" rel="noopener ugc nofollow" target="_blank">支持</a>，您可以非常容易地在一个20节点的集群上部署相同的解决方案，比我在这里使用的大约30行代码多得多。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ox"><img src="../Images/a12d8d64680fb93a1e6ca7e55851cc80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6WcFEzEcghiNg-1I.jpg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><a class="ae kz" href="https://www.nvidia.com/content/dam/en-zz/Solutions/deep-learning/dgx-saturnv/nvidia-dgx-saturnv-real-world-quality-assurance-4c25-m@2x.jpg" rel="noopener ugc nofollow" target="_blank">英伟达DGX </a></p></figure><p id="9266" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">然而，这里仅在两个dgx上实现的效率和规模应该显示了使用GPU的工作情况。通过高级Python APIs的可访问性现在使普通数据科学家能够以最小的努力执行高度优化的任务。感谢您的阅读，请务必改进这些解决方案！</p><p id="dcd4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="lw">不要脸塞:正在制作</em> <a class="ae kz" href="https://twitter.com/kylegallatin" rel="noopener ugc nofollow" target="_blank"> <em class="lw">我的Twitter游戏</em> </a> <em class="lw">并随时连接上</em><a class="ae kz" href="https://www.linkedin.com/in/kylegallatin/" rel="noopener ugc nofollow" target="_blank"><em class="lw">LinkedIn</em></a><em class="lw">！</em></p></div></div>    
</body>
</html>