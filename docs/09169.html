<html>
<head>
<title>Reinforcement Learning with TensorFlow Agents â€” Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä½¿ç”¨å¼ é‡æµä»£ç†çš„å¼ºåŒ–å­¦ä¹ â€”æ•™ç¨‹</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/reinforcement-learning-with-tensorflow-agents-tutorial-4ac7fa858728?source=collection_archive---------13-----------------------#2020-07-01">https://towardsdatascience.com/reinforcement-learning-with-tensorflow-agents-tutorial-4ac7fa858728?source=collection_archive---------13-----------------------#2020-07-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9fb5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">ç”¨è¿™ä¸ªç®€å•çš„æ•™ç¨‹è¯•è¯• TF-Agents for RLï¼Œå®ƒä»¥ Google colab ç¬”è®°æœ¬çš„å½¢å¼å‘å¸ƒï¼Œæ‰€ä»¥ä½ å¯ä»¥ç›´æ¥ä»ä½ çš„æµè§ˆå™¨ä¸Šè¿è¡Œå®ƒã€‚</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c67ccb32d8794b70100da02248bb18df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aH7Mg1IUna1SJcgmZDNW3A.jpeg"/></div></div></figure><p id="cfc9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">å‡ å‘¨å‰ï¼Œæˆ‘å†™äº†ä¸€ç¯‡æ–‡ç« ï¼Œåˆ—ä¸¾äº†å¯ä»¥ç”¨æ¥åœ¨é¡¹ç›®ä¸­å®ç°å¼ºåŒ–å­¦ä¹ (RL)çš„ä¸åŒæ¡†æ¶ï¼Œå±•ç¤ºäº†æ¯ä¸ªæ¡†æ¶çš„ç››è¡°ï¼Œå¹¶æƒ³çŸ¥é“æ˜¯å¦æœ‰ä»»ä½•ä¸€ä¸ªæ¡†æ¶ä¼šåœ¨æŸä¸ªæ—¶å€™ç»Ÿæ²»æ‰€æœ‰æ¡†æ¶ã€‚ä»é‚£ä»¥åï¼Œæˆ‘å¼€å§‹çŸ¥é“äº†<a class="ae lq" href="https://github.com/tensorflow/agents" rel="noopener ugc nofollow" target="_blank"> TF Agents </a>ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº TensorFlow çš„ RL åº“ï¼Œå¹¶å¾—åˆ°äº†å…¶ç¤¾åŒºçš„å…¨åŠ›æ”¯æŒ(æ³¨æ„ï¼ŒTF Agents ä¸æ˜¯ Google çš„å®˜æ–¹äº§å“ï¼Œä½†å®ƒæ˜¯ä½œä¸º Github ä¸Šçš„å®˜æ–¹ TensorFlow å¸æˆ·çš„å­˜å‚¨åº“å‘å¸ƒçš„)ã€‚</p><p id="ea1c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æˆ‘ç›®å‰æ­£åœ¨ä¸€ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨ TF ä»£ç†ï¼Œç”±äºå®ƒçš„è‰¯å¥½æ–‡æ¡£ï¼ŒåŒ…æ‹¬<a class="ae lq" href="https://github.com/tensorflow/agents/tree/master/docs/tutorials" rel="noopener ugc nofollow" target="_blank">æ•™ç¨‹</a>ï¼Œå¼€å§‹ä½¿ç”¨å®ƒå¾ˆå®¹æ˜“ã€‚å®ƒå®šæœŸæ›´æ–°ï¼Œæœ‰è®¸å¤šè´¡çŒ®è€…ï¼Œè¿™ä½¿æˆ‘è®¤ä¸ºæˆ‘ä»¬æœ‰å¯èƒ½åœ¨ä¸ä¹…çš„å°†æ¥çœ‹åˆ° TF ä»£ç†ä½œä¸ºå®ç° RL çš„æ ‡å‡†æ¡†æ¶ã€‚æ­£å› ä¸ºå¦‚æ­¤ï¼Œæˆ‘å†³å®šå†™è¿™ç¯‡æ–‡ç« ç»™ä½ ä¸€ä¸ªå¿«é€Ÿçš„ä»‹ç»ï¼Œè¿™æ ·ä½ ä¹Ÿå¯ä»¥ä»è¿™ä¸ªåº“å—ç›Šã€‚æˆ‘å·²ç»<a class="ae lq" href="https://colab.research.google.com/drive/1Pd63OyiOnw4j401f3FN2tja4mH6EFMCc?usp=sharing" rel="noopener ugc nofollow" target="_blank">å‘å¸ƒäº†è¿™é‡Œä½¿ç”¨çš„æ‰€æœ‰ä»£ç ï¼Œä½œä¸ºä¸€ä¸ª Google colab ç¬”è®°æœ¬</a>ï¼Œæ‰€ä»¥ä½ å¯ä»¥å¾ˆå®¹æ˜“åœ°åœ¨çº¿è¿è¡Œå®ƒã€‚</p><p id="5150" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ° Github çš„æ‰€æœ‰ä»£ç å’Œæ–‡æ¡£ã€‚ä½ ä¸éœ€è¦å…‹éš†ä»–ä»¬çš„åº“ï¼Œä½†æ˜¯æœ‰å®˜æ–¹çš„ Github ä½œä¸ºå‚è€ƒæ€»æ˜¯æœ‰ç”¨çš„ã€‚æˆ‘å®ç°äº†ä¸‹é¢çš„ä¾‹å­ï¼Œéƒ¨åˆ†éµå¾ªä»–ä»¬çš„æ•™ç¨‹(1_dqn_tutorial ),ä½†æˆ‘è¿›ä¸€æ­¥ç®€åŒ–äº†å®ƒï¼Œå¹¶åœ¨æœ¬æ–‡ä¸­ç”¨å®ƒæ¥ç© Atari æ¸¸æˆã€‚è®©æˆ‘ä»¬åŠ¨æ‰‹å§ã€‚</p><h1 id="5e49" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">å®‰è£… TF ä»£ç†å’Œä¾èµ–é¡¹</h1><p id="6976" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">å¦‚å‰æ‰€è¿°ï¼ŒTF-Agents è¿è¡Œåœ¨ TensorFlow ä¸Šï¼Œæ›´å…·ä½“åœ°è¯´æ˜¯ TensorFlow 2.2.0ã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨è¿˜æ²¡æœ‰ä»¥ä¸‹è½¯ä»¶åŒ…ï¼Œæ‚¨éœ€è¦å®‰è£…å®ƒä»¬:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="0bd8" class="mt ls it mp b gy mu mv l mw mx">pip install tensorflow==2.2.0<br/>pip install tf-agents</span></pre><h1 id="9e0d" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">ä¸º CartPole å®ç° DQN ä»£ç†</h1><p id="c833" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">æˆ‘ä»¬å°†å®ç°ä¸€ä¸ª DQN ä»£ç†(<a class="ae lq" href="https://daiwk.github.io/assets/dqn.pdf" rel="noopener ugc nofollow" target="_blank"> Mnih et al. 2015 </a>)ï¼Œå¹¶å°†å…¶ç”¨äºç»å…¸æ§åˆ¶é—®é¢˜ CartPoleã€‚å¦‚æœä½ æƒ³è§£å†³ä¸€äº›æ›´ä»¤äººå…´å¥‹çš„äº‹æƒ…ï¼Œæ¯”å¦‚è¯´ï¼Œä¸€ä¸ª Atari æ¸¸æˆï¼Œä½ åªéœ€è¦ä»æ‰€æœ‰å¯ç”¨çš„ OpenAI ç¯å¢ƒä¸­é€‰æ‹©ä¸€ä¸ªä½ æƒ³è¦çš„ç¯å¢ƒåã€‚</p><p id="aa68" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æˆ‘ä»¬ä»æ‰€æœ‰å¿…è¦çš„è¿›å£å¼€å§‹ã€‚æ­£å¦‚ä½ åœ¨ä¸‹é¢çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬ä» TF-Agents å®ç°äº†ç›¸å½“å¤šçš„å¯¹è±¡ã€‚è¿™äº›éƒ½æ˜¯æˆ‘ä»¬å¯ä»¥ä¸ºæˆ‘ä»¬çš„å®ç°å®šåˆ¶å’Œåˆ‡æ¢çš„ä¸œè¥¿ã€‚</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="ea9c" class="mt ls it mp b gy mu mv l mw mx">from __future__ import absolute_import, division, print_function</span><span id="81fd" class="mt ls it mp b gy my mv l mw mx">import base64<br/>import IPython<br/>import matplotlib<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import tensorflow as tf</span><span id="91a7" class="mt ls it mp b gy my mv l mw mx">from tf_agents.agents.dqn import dqn_agent<br/>from tf_agents.drivers import dynamic_step_driver<br/>from tf_agents.environments import suite_gym<br/>from tf_agents.environments import tf_py_environment<br/>from tf_agents.eval import metric_utils<br/>from tf_agents.metrics import tf_metrics<br/>from tf_agents.networks import q_network<br/>from tf_agents.replay_buffers import tf_uniform_replay_buffer<br/>from tf_agents.trajectories import trajectory<br/>from tf_agents.utils import common</span></pre><h2 id="579e" class="mt ls it bd lt mz na dn lx nb nc dp mb ld nd ne md lh nf ng mf ll nh ni mh nj bi translated">ç¯å¢ƒ</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5d6e42c1f7a68272a4bbd026306c707b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*pdbYBlfTAQxucgZ5nwqkow.gif"/></div><p class="nl nm gj gh gi nn no bd b be z dk translated">OpenAI å¥èº«æˆ¿çš„ç¿»ç­‹æ–—ç¯å¢ƒ[GIF from <a class="ae lq" href="https://github.com/jaekookang" rel="noopener ugc nofollow" target="_blank">ç§¯è°·åº·</a>/<a class="ae lq" href="https://github.com/jaekookang/RL-cartpole" rel="noopener ugc nofollow" target="_blank">RL-ç¿»ç­‹æ–—</a>ã€‚]</p></figure><p id="3af8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬å¼€å§‹åˆ›é€ æˆ‘ä»¬çš„ç¯å¢ƒã€‚åœ¨ CartPole ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªé¡¶éƒ¨æœ‰ä¸€æ ¹æ†å­çš„æ¨è½¦ï¼Œä»£ç†çš„ä»»åŠ¡æ˜¯å­¦ä¹ ä¿æŒæ†å­ï¼Œå·¦å³ç§»åŠ¨æ¨è½¦ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å·²ç»åŒ…å«åœ¨ TF-Agents ä¸­çš„ suite_gym ä¸­çš„ e ç¯å¢ƒï¼Œè¿™æ˜¯ OpenAI Gym ç¯å¢ƒçš„ä¸€ä¸ªç¨å¾®å®šåˆ¶(å¹¶é’ˆå¯¹å…¶ä¸ TF-Agents çš„ä½¿ç”¨è¿›è¡Œäº†æ”¹è¿›)çš„ç‰ˆæœ¬(å¦‚æœæ‚¨æ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹ä¸ OpenAI çš„å®ç°<a class="ae lq" href="https://github.com/tensorflow/agents/blob/master/tf_agents/environments/suite_gym.py" rel="noopener ugc nofollow" target="_blank">è¿™é‡Œ</a>çš„å·®å¼‚)ã€‚æˆ‘ä»¬è¿˜å°†ä¸ºæˆ‘ä»¬çš„ç¯å¢ƒä½¿ç”¨ä¸€ä¸ªåä¸º TFPyEnvironment çš„åŒ…è£…å™¨ï¼Œå®ƒå°†ç”¨äºçŠ¶æ€è§‚å¯Ÿã€æ“ä½œå’Œå¥–åŠ±çš„ numpy æ•°ç»„è½¬æ¢ä¸º TensorFlow å¼ é‡ã€‚åœ¨å¤„ç†å¼ é‡æµæ¨¡å‹(å³ç¥ç»ç½‘ç»œ)æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ é‡ï¼Œå› æ­¤é€šè¿‡ä½¿ç”¨è¿™ä¸ªåŒ…è£…å™¨ï¼Œæˆ‘ä»¬å¯ä»¥èŠ‚çœä¸€äº›è½¬æ¢è¿™äº›æ•°æ®æ‰€éœ€çš„å·¥ä½œã€‚</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="527b" class="mt ls it mp b gy mu mv l mw mx">env = suite_gym.load('CartPole-v1')<br/>env = tf_py_environment.TFPyEnvironment(env)</span></pre><h2 id="facf" class="mt ls it bd lt mz na dn lx nb nc dp mb ld nd ne md lh nf ng mf ll nh ni mh nj bi translated">ä»£ç†äºº</h2><p id="17ce" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">TF ä¸­æœ‰ä¸åŒçš„è¯å‰‚â€”â€”æˆ‘ä»¬å¯ä»¥ä½¿ç”¨çš„è¯å‰‚:<a class="ae lq" href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank"> DQN </a>ã€<a class="ae lq" href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" rel="noopener ugc nofollow" target="_blank">å¢æ´</a>ã€<a class="ae lq" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank"> DDPG </a>ã€<a class="ae lq" href="https://arxiv.org/pdf/1802.09477.pdf" rel="noopener ugc nofollow" target="_blank"> TD3 </a>ã€<a class="ae lq" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank"> PPO </a>å’Œ<a class="ae lq" href="https://arxiv.org/abs/1801.01290" rel="noopener ugc nofollow" target="_blank"> SAC </a>ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ DQNã€‚ä»£ç†çš„ä¸€ä¸ªä¸»è¦å‚æ•°æ˜¯å®ƒçš„ Q(ç¥ç»)ç½‘ç»œï¼Œå®ƒå°†ç”¨äºè®¡ç®—æ¯ä¸€æ­¥ä¸­åŠ¨ä½œçš„ Q å€¼ã€‚q_network æœ‰ä¸¤ä¸ªå¼ºåˆ¶å‚æ•°:å®šä¹‰è§‚å¯Ÿå½¢çŠ¶å’ŒåŠ¨ä½œå½¢çŠ¶çš„ input_tensor_spec å’Œ action_specã€‚æˆ‘ä»¬å¯ä»¥ä»æˆ‘ä»¬çš„ç¯å¢ƒä¸­å¾—åˆ°è¿™ä¸€ç‚¹ï¼Œå› æ­¤æˆ‘ä»¬å°†æˆ‘ä»¬çš„ q_network å®šä¹‰å¦‚ä¸‹:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="23dd" class="mt ls it mp b gy mu mv l mw mx">q_net = q_network.QNetwork(env.observation_spec(), <br/>                           env.action_spec())</span></pre><p id="a890" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æ­£å¦‚ä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„<a class="ae lq" href="https://github.com/tensorflow/agents/blob/master/tf_agents/networks/q_network.py" rel="noopener ugc nofollow" target="_blank"/>ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæˆ‘ä»¬çš„ q_network å®šåˆ¶æ›´å¤šçš„å‚æ•°ï¼Œä½†æ˜¯ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é»˜è®¤çš„å‚æ•°ã€‚ä»£ç†è¿˜éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨æ¥æŸ¥æ‰¾ q_network å‚æ•°çš„å€¼ã€‚è®©æˆ‘ä»¬ä¿æŒç»å…¸ï¼Œç”¨äºšå½“ã€‚</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="e9aa" class="mt ls it mp b gy mu mv l mw mx">optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)</span></pre><p id="54ec" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æœ€åï¼Œæˆ‘ä»¬ç”¨ä»¥ä¸‹å‚æ•°å®šä¹‰å¹¶åˆå§‹åŒ–æˆ‘ä»¬çš„ä»£ç†:</p><ul class=""><li id="847c" class="np nq it kw b kx ky la lb ld nr lh ns ll nt lp nu nv nw nx bi translated">time_step_specï¼Œå®ƒæ˜¯æˆ‘ä»¬ä»ç¯å¢ƒä¸­è·å¾—çš„ï¼Œå®šä¹‰äº†æˆ‘ä»¬çš„æ—¶é—´æ­¥é•¿æ˜¯å¦‚ä½•å®šä¹‰çš„ã€‚</li><li id="c59f" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">action_specï¼Œä¸ q_network ç›¸åŒã€‚</li><li id="50cb" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ Q ç½‘ã€‚</li><li id="6360" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ä¼˜åŒ–å™¨ã€‚</li><li id="0565" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">TD è¯¯å·®æŸå¤±å‡½æ•°ï¼Œç±»ä¼¼äºç¥ç»ç½‘ç»œä¸­çš„æŸå¤±å‡½æ•°ã€‚</li><li id="b52f" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">åˆ—è½¦æ­¥æ•°è®¡æ•°å™¨ï¼Œè¿™åªæ˜¯ä¸€ä¸ª 0 é˜¶å¼ é‡(ä¹Ÿç§°ä¸ºæ ‡é‡)ï¼Œå®ƒå°†è®¡ç®—æˆ‘ä»¬åœ¨ç¯å¢ƒä¸­çš„æ­¥æ•°ã€‚</li></ul><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="94f2" class="mt ls it mp b gy mu mv l mw mx">train_step_counter = tf.Variable(0)</span><span id="3a54" class="mt ls it mp b gy my mv l mw mx">agent = dqn_agent.DqnAgent(env.time_step_spec(),<br/>                           env.action_spec(),<br/>                           q_network=q_net,<br/>                           optimizer=optimizer,<br/>                           td_errors_loss_fn= <br/>                                  common.element_wise_squared_loss,<br/>                           train_step_counter=train_step_counter)</span><span id="e9cc" class="mt ls it mp b gy my mv l mw mx">agent.initialize()</span></pre><h2 id="d239" class="mt ls it bd lt mz na dn lx nb nc dp mb ld nd ne md lh nf ng mf ll nh ni mh nj bi translated">è¾…åŠ©æ–¹æ³•:å¹³å‡ç´¯ç§¯å›æŠ¥å’Œæ”¶é›†æ•°æ®</h2><p id="e1e9" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›è¾…åŠ©æ–¹æ³•ã€‚ç¬¬ä¸€ä¸ªå°†åœ¨ç¯å¢ƒä¸­è¿­ä»£è‹¥å¹²é›†ï¼Œåº”ç”¨ç­–ç•¥æ¥é€‰æ‹©è¦éµå¾ªçš„æ“ä½œï¼Œå¹¶è¿”å›è¿™äº›é›†ä¸­çš„å¹³å‡ç´¯ç§¯å¥–åŠ±ã€‚è¿™å°†æœ‰åŠ©äºè¯„ä¼°æˆ‘ä»¬çš„ä»£ç†äº†è§£åˆ°çš„ç­–ç•¥ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬ä¹Ÿåœ¨æˆ‘ä»¬çš„ç¯å¢ƒä¸­å°è¯•è¯¥æ–¹æ³• 10 é›†ã€‚</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="4383" class="mt ls it mp b gy mu mv l mw mx">def compute_avg_return(environment, policy, num_episodes=10):<br/>    total_return = 0.0<br/>    for _ in range(num_episodes):<br/>        time_step = environment.reset()<br/>        episode_return = 0.0</span><span id="4e1f" class="mt ls it mp b gy my mv l mw mx">        while not time_step.is_last():<br/>            action_step = policy.action(time_step)<br/>            time_step = environment.step(action_step.action)<br/>            episode_return += time_step.reward<br/>        total_return += episode_return</span><span id="848c" class="mt ls it mp b gy my mv l mw mx">    avg_return = total_return / num_episodes<br/>    return avg_return.numpy()[0]</span><span id="5fe4" class="mt ls it mp b gy my mv l mw mx"># Evaluate the agent's policy once before training.<br/>avg_return = compute_avg_return(env, agent.policy, 5)<br/>returns = [avg_return]</span></pre><p id="437f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æˆ‘ä»¬è¿˜å°†åœ¨åŸ¹è®­æˆ‘ä»¬çš„ä»£ç†æ—¶å®ç°ä¸€ç§æ”¶é›†æ•°æ®çš„æ–¹æ³•ã€‚DQN çš„çªç ´ä¹‹ä¸€æ˜¯ç»éªŒå›æ”¾ï¼Œæˆ‘ä»¬å°†ä»£ç†äººçš„ç»éªŒ(çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±)å­˜å‚¨èµ·æ¥ï¼Œç”¨å®ƒåœ¨æ¯ä¸€æ­¥æ‰¹é‡è®­ç»ƒ Q ç½‘ç»œã€‚è¿™é€šè¿‡ä½¿å­¦ä¹ æ›´å¿«å’Œæ›´ç¨³å®šæ¥æ”¹è¿›å­¦ä¹ ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼ŒTF-Agents åŒ…å«äº† TFUniformReplayBuffer å¯¹è±¡ï¼Œå®ƒå­˜å‚¨äº†è¿™äº›ç»éªŒä»¥ä¾¿ä»¥åé‡ç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆåˆ›å»ºè¿™ä¸ªæˆ‘ä»¬ä»¥åä¼šç”¨åˆ°çš„å¯¹è±¡ã€‚</p><p id="8960" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€ä¸ªç¯å¢ƒã€ä¸€ä¸ªç­–ç•¥å’Œä¸€ä¸ªç¼“å†²åŒºï¼Œé‡‡ç”¨ç”±å®ƒçš„çŠ¶æ€è§‚å¯Ÿå’Œå¥–åŠ±å½¢æˆçš„å½“å‰æ—¶é—´æ­¥é•¿ã€ç­–ç•¥é€‰æ‹©çš„åŠ¨ä½œä»¥åŠä¸‹ä¸€ä¸ªæ—¶é—´æ­¥é•¿ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å®ƒå­˜å‚¨åœ¨é‡æ”¾ç¼“å†²åŒºä¸­ã€‚æ³¨æ„ï¼Œé‡æ”¾ç¼“å†²åŒºå­˜å‚¨äº†ä¸€ä¸ªåä¸º Trajectory çš„å¯¹è±¡ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨å‰é¢å‘½åçš„å…ƒç´ åˆ›å»ºäº†è¿™ä¸ªå¯¹è±¡ï¼Œç„¶åä½¿ç”¨ add_batch æ–¹æ³•å°†å®ƒä¿å­˜åˆ°ç¼“å†²åŒºã€‚</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="9b76" class="mt ls it mp b gy mu mv l mw mx">replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(<br/>                                data_spec=agent.collect_data_spec,                                                                <br/>                                batch_size=env.batch_size,                                                              <br/>                                max_length=100000)</span><span id="e032" class="mt ls it mp b gy my mv l mw mx">def collect_step(environment, policy, buffer):<br/>    time_step = environment.current_time_step()<br/>    action_step = policy.action(time_step)<br/>    next_time_step = environment.step(action_step.action)<br/>    traj = trajectory.from_transition(time_step, <br/>                                      action_step, <br/>                                      next_time_step)</span><span id="e772" class="mt ls it mp b gy my mv l mw mx"># Add trajectory to the replay buffer<br/>    buffer.add_batch(traj)</span></pre><h2 id="4877" class="mt ls it bd lt mz na dn lx nb nc dp mb ld nd ne md lh nf ng mf ll nh ni mh nj bi translated">åˆ—è½¦ä»£ç†</h2><p id="f1b6" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">æˆ‘ä»¬ç»ˆäºå¯ä»¥è®­ç»ƒæˆ‘ä»¬çš„ç‰¹å·¥äº†ã€‚æˆ‘ä»¬å®šä¹‰äº†æˆ‘ä»¬åœ¨æ¯ä¸€æ¬¡è¿­ä»£ä¸­çš„æ­¥æ•°ï¼Œåœ¨è¿™ä¸ªæ­¥æ•°ä¹‹åï¼Œæˆ‘ä»¬å°†åœ¨æ¯ä¸€æ¬¡è¿­ä»£ä¸­è®­ç»ƒæˆ‘ä»¬çš„ä»£ç†ï¼Œä¿®æ”¹å®ƒçš„ç­–ç•¥ã€‚ç°åœ¨è®©æˆ‘ä»¬æ¯æ¬¡è¿­ä»£åªä½¿ç”¨ä¸€æ­¥ã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†æˆ‘ä»¬çš„ Q ç½‘ç»œå°†è¢«è®­ç»ƒçš„æ‰¹é‡å¤§å°å’Œä¸€ä¸ªè¿­ä»£å™¨ï¼Œä»¥ä¾¿æˆ‘ä»¬è¿­ä»£ä»£ç†çš„ç»éªŒã€‚</p><p id="488d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ç„¶åï¼Œæˆ‘ä»¬å°†ä¸ºç¼“å†²å™¨æ”¶é›†ä¸€äº›ç»éªŒï¼Œå¹¶ä»å¸¸è§çš„ RL ç¯è·¯å¼€å§‹ã€‚é€šè¿‡å¯¹ç¯å¢ƒé‡‡å–è¡ŒåŠ¨ã€åŸ¹è®­æ”¿ç­–å’Œé‡å¤æ¥è·å¾—ç»éªŒã€‚æˆ‘ä»¬å¦å¤–æ‰“å°æŸå¤±ï¼Œå¹¶åˆ†åˆ«æ¯ 200 å’Œ 1000 æ­¥è¯„ä¼°ä»£ç†çš„æ€§èƒ½ã€‚</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="1bb2" class="mt ls it mp b gy mu mv l mw mx">collect_steps_per_iteration = 1<br/>batch_size = 64<br/>dataset = replay_buffer.as_dataset(num_parallel_calls=3, <br/>                                   sample_batch_size=batch_size, <br/>                                   num_steps=2).prefetch(3)<br/>iterator = iter(dataset)<br/>num_iterations = 20000<br/>env.reset()</span><span id="a175" class="mt ls it mp b gy my mv l mw mx">for _ in range(batch_size):<br/>    collect_step(env, agent.policy, replay_buffer)</span><span id="6005" class="mt ls it mp b gy my mv l mw mx">for _ in range(num_iterations):<br/>    # Collect a few steps using collect_policy and save to the replay buffer.<br/>    for _ in range(collect_steps_per_iteration):<br/>        collect_step(env, agent.collect_policy, replay_buffer)</span><span id="d06e" class="mt ls it mp b gy my mv l mw mx">    # Sample a batch of data from the buffer and update the agent's network.<br/>    experience, unused_info = next(iterator)<br/>    train_loss = agent.train(experience).loss</span><span id="2c62" class="mt ls it mp b gy my mv l mw mx">    step = agent.train_step_counter.numpy()</span><span id="4b1b" class="mt ls it mp b gy my mv l mw mx">    # Print loss every 200 steps.<br/>    if step % 200 == 0:<br/>        print('step = {0}: loss = {1}'.format(step, train_loss))</span><span id="1335" class="mt ls it mp b gy my mv l mw mx">    # Evaluate agent's performance every 1000 steps.<br/>    if step % 1000 == 0:<br/>        avg_return = compute_avg_return(env, agent.policy, 5)<br/>        print('step = {0}: Average Return = {1}'.format(step, avg_return))<br/>        returns.append(avg_return)</span></pre><h2 id="3a6f" class="mt ls it bd lt mz na dn lx nb nc dp mb ld nd ne md lh nf ng mf ll nh ni mh nj bi translated">æƒ…èŠ‚</h2><p id="c307" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">æˆ‘ä»¬ç°åœ¨å¯ä»¥ç”»å‡ºå½“æˆ‘ä»¬è®­ç»ƒä»£ç†äººæ—¶ï¼Œç´¯ç§¯çš„å¹³å‡å›æŠ¥æ˜¯å¦‚ä½•å˜åŒ–çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ matplotlib åˆ¶ä½œä¸€ä¸ªéå¸¸ç®€å•çš„ç»˜å›¾ã€‚</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="41b0" class="mt ls it mp b gy mu mv l mw mx">iterations = range(0, num_iterations + 1, 1000)<br/>plt.plot(iterations, returns)<br/>plt.ylabel('Average Return')<br/>plt.xlabel('Iterations')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/4fd44b8335a11188f95cc6508376ba62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7fMiahhZlGIhBbNU0gmTHA.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">ã€ŠDQN ç‰¹å·¥ã€‹5 é›†çš„å¹³å‡å›æŠ¥ç‡ã€‚éšç€ä»£ç†å˜å¾—æ›´æœ‰ç»éªŒï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ€§èƒ½æ˜¯å¦‚ä½•éšç€æ—¶é—´çš„æ¨ç§»è€Œæé«˜çš„ã€‚</p></figure><h1 id="50d5" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">å®Œå…¨ç </h1><p id="2b6b" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">æˆ‘å·²ç»æŠŠæœ¬æ–‡ä¸­çš„<a class="ae lq" href="https://colab.research.google.com/drive/1Pd63OyiOnw4j401f3FN2tja4mH6EFMCc?usp=sharing" rel="noopener ugc nofollow" target="_blank">æ‰€æœ‰ä»£ç ä½œä¸º Google Colab ç¬”è®°æœ¬</a>è¿›è¡Œäº†åˆ†äº«ã€‚ä½ å¯ä»¥ç›´æ¥è¿è¡Œæ‰€æœ‰ä»£ç ï¼Œå¦‚æœä½ æƒ³æ”¹å˜å®ƒï¼Œä½ å¿…é¡»æŠŠå®ƒä¿å­˜åœ¨ä½ è‡ªå·±çš„ Google drive è´¦æˆ·ä¸Šï¼Œç„¶åä½ å¯ä»¥åšä»»ä½•ä½ æƒ³åšçš„äº‹æƒ…ã€‚å¦‚æœä½ æ„¿æ„ï¼Œä½ ä¹Ÿå¯ä»¥ä¸‹è½½å®ƒåœ¨ä½ çš„æœ¬åœ°è®¡ç®—æœºä¸Šè¿è¡Œã€‚</p><h1 id="0407" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">ä»è¿™é‡Œå»å“ªé‡Œ</h1><ul class=""><li id="f08c" class="np nq it kw b kx mj la mk ld oe lh of ll og lp nu nv nw nx bi translated">ä½ å¯ä»¥è·Ÿéš Github ä¸Š TF-Agents åº“ä¸­çš„<a class="ae lq" href="https://github.com/tensorflow/agents/tree/master/docs/tutorials" rel="noopener ugc nofollow" target="_blank">æ•™ç¨‹</a></li><li id="aa1b" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">å¦‚æœä½ æƒ³æŸ¥çœ‹ RL çš„å…¶ä»–å¥½æ¡†æ¶ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æˆ‘ä»¥å‰çš„å¸–å­:</li></ul><div class="oh oi gp gr oj ok"><a rel="noopener follow" target="_blank" href="/5-frameworks-for-reinforcement-learning-on-python-1447fede2f18"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd iu gy z fp op fr fs oq fu fw is bi translated">Python å¼ºåŒ–å­¦ä¹ çš„ 5 ä¸ªæ¡†æ¶</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">ä»å¤´å¼€å§‹ç¼–ç¨‹ä½ è‡ªå·±çš„å¼ºåŒ–å­¦ä¹ å®ç°å¯èƒ½ä¼šæœ‰å¾ˆå¤šå·¥ä½œï¼Œä½†æ˜¯ä½ ä¸éœ€è¦åšâ€¦</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">towardsdatascience.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy ks ok"/></div></div></a></div><ul class=""><li id="0116" class="np nq it kw b kx ky la lb ld nr lh ns ll nt lp nu nv nw nx bi translated">ä½ ä¹Ÿå¯ä»¥åœ¨æˆ‘å‰æ®µæ—¶é—´å†™çš„å¦ä¸€ç¯‡æ–‡ç« çš„<a class="ae lq" href="https://medium.com/@mauriciofadelargerich/reinforcement-learning-environments-cff767bc241f" rel="noopener">ä¸­æŸ¥çœ‹å…¶ä»–å¯ä»¥å°è¯• TF-Agents(æˆ–è€…ä»»ä½•ä½ é€‰æ‹©çš„ RL ç®—æ³•)çš„ç¯å¢ƒã€‚</a></li></ul><p id="c242" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">è€è§„çŸ©ï¼Œæ„Ÿè°¢é˜…è¯»ï¼è¯·åœ¨å›å¤ä¸­å‘Šè¯‰æˆ‘ä½ å¯¹ TF-Agents çš„çœ‹æ³•ï¼Œå¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–å‘ç°ä»»ä½•é—®é¢˜ï¼Œä¹Ÿè¯·å‘Šè¯‰æˆ‘ğŸ›åœ¨ä»£ç ä¸­ã€‚</p></div></div>    
</body>
</html>