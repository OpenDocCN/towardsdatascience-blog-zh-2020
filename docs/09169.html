<html>
<head>
<title>Reinforcement Learning with TensorFlow Agents — Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用张量流代理的强化学习—教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-with-tensorflow-agents-tutorial-4ac7fa858728?source=collection_archive---------13-----------------------#2020-07-01">https://towardsdatascience.com/reinforcement-learning-with-tensorflow-agents-tutorial-4ac7fa858728?source=collection_archive---------13-----------------------#2020-07-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9fb5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用这个简单的教程试试 TF-Agents for RL，它以 Google colab 笔记本的形式发布，所以你可以直接从你的浏览器上运行它。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c67ccb32d8794b70100da02248bb18df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aH7Mg1IUna1SJcgmZDNW3A.jpeg"/></div></div></figure><p id="cfc9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">几周前，我写了一篇文章，列举了可以用来在项目中实现强化学习(RL)的不同框架，展示了每个框架的盛衰，并想知道是否有任何一个框架会在某个时候统治所有框架。从那以后，我开始知道了<a class="ae lq" href="https://github.com/tensorflow/agents" rel="noopener ugc nofollow" target="_blank"> TF Agents </a>，这是一个基于 TensorFlow 的 RL 库，并得到了其社区的全力支持(注意，TF Agents 不是 Google 的官方产品，但它是作为 Github 上的官方 TensorFlow 帐户的存储库发布的)。</p><p id="ea1c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我目前正在一个项目中使用 TF 代理，由于它的良好文档，包括<a class="ae lq" href="https://github.com/tensorflow/agents/tree/master/docs/tutorials" rel="noopener ugc nofollow" target="_blank">教程</a>，开始使用它很容易。它定期更新，有许多贡献者，这使我认为我们有可能在不久的将来看到 TF 代理作为实现 RL 的标准框架。正因为如此，我决定写这篇文章给你一个快速的介绍，这样你也可以从这个库受益。我已经<a class="ae lq" href="https://colab.research.google.com/drive/1Pd63OyiOnw4j401f3FN2tja4mH6EFMCc?usp=sharing" rel="noopener ugc nofollow" target="_blank">发布了这里使用的所有代码，作为一个 Google colab 笔记本</a>，所以你可以很容易地在线运行它。</p><p id="5150" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可以在这里找到 Github 的所有代码和文档。你不需要克隆他们的库，但是有官方的 Github 作为参考总是有用的。我实现了下面的例子，部分遵循他们的教程(1_dqn_tutorial ),但我进一步简化了它，并在本文中用它来玩 Atari 游戏。让我们动手吧。</p><h1 id="5e49" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">安装 TF 代理和依赖项</h1><p id="6976" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">如前所述，TF-Agents 运行在 TensorFlow 上，更具体地说是 TensorFlow 2.2.0。此外，如果您还没有以下软件包，您需要安装它们:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="0bd8" class="mt ls it mp b gy mu mv l mw mx">pip install tensorflow==2.2.0<br/>pip install tf-agents</span></pre><h1 id="9e0d" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">为 CartPole 实现 DQN 代理</h1><p id="c833" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">我们将实现一个 DQN 代理(<a class="ae lq" href="https://daiwk.github.io/assets/dqn.pdf" rel="noopener ugc nofollow" target="_blank"> Mnih et al. 2015 </a>)，并将其用于经典控制问题 CartPole。如果你想解决一些更令人兴奋的事情，比如说，一个 Atari 游戏，你只需要从所有可用的 OpenAI 环境中选择一个你想要的环境名。</p><p id="aa68" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们从所有必要的进口开始。正如你在下面看到的，我们从 TF-Agents 实现了相当多的对象。这些都是我们可以为我们的实现定制和切换的东西。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="ea9c" class="mt ls it mp b gy mu mv l mw mx">from __future__ import absolute_import, division, print_function</span><span id="81fd" class="mt ls it mp b gy my mv l mw mx">import base64<br/>import IPython<br/>import matplotlib<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import tensorflow as tf</span><span id="91a7" class="mt ls it mp b gy my mv l mw mx">from tf_agents.agents.dqn import dqn_agent<br/>from tf_agents.drivers import dynamic_step_driver<br/>from tf_agents.environments import suite_gym<br/>from tf_agents.environments import tf_py_environment<br/>from tf_agents.eval import metric_utils<br/>from tf_agents.metrics import tf_metrics<br/>from tf_agents.networks import q_network<br/>from tf_agents.replay_buffers import tf_uniform_replay_buffer<br/>from tf_agents.trajectories import trajectory<br/>from tf_agents.utils import common</span></pre><h2 id="579e" class="mt ls it bd lt mz na dn lx nb nc dp mb ld nd ne md lh nf ng mf ll nh ni mh nj bi translated">环境</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5d6e42c1f7a68272a4bbd026306c707b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*pdbYBlfTAQxucgZ5nwqkow.gif"/></div><p class="nl nm gj gh gi nn no bd b be z dk translated">OpenAI 健身房的翻筋斗环境[GIF from <a class="ae lq" href="https://github.com/jaekookang" rel="noopener ugc nofollow" target="_blank">积谷康</a>/<a class="ae lq" href="https://github.com/jaekookang/RL-cartpole" rel="noopener ugc nofollow" target="_blank">RL-翻筋斗</a>。]</p></figure><p id="3af8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们开始创造我们的环境。在 CartPole 中，我们有一个顶部有一根杆子的推车，代理的任务是学习保持杆子，左右移动推车。请注意，我们将使用已经包含在 TF-Agents 中的 suite_gym 中的 e 环境，这是 OpenAI Gym 环境的一个稍微定制(并针对其与 TF-Agents 的使用进行了改进)的版本(如果您感兴趣，可以查看与 OpenAI 的实现<a class="ae lq" href="https://github.com/tensorflow/agents/blob/master/tf_agents/environments/suite_gym.py" rel="noopener ugc nofollow" target="_blank">这里</a>的差异)。我们还将为我们的环境使用一个名为 TFPyEnvironment 的包装器，它将用于状态观察、操作和奖励的 numpy 数组转换为 TensorFlow 张量。在处理张量流模型(即神经网络)时，我们使用张量，因此通过使用这个包装器，我们可以节省一些转换这些数据所需的工作。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="527b" class="mt ls it mp b gy mu mv l mw mx">env = suite_gym.load('CartPole-v1')<br/>env = tf_py_environment.TFPyEnvironment(env)</span></pre><h2 id="facf" class="mt ls it bd lt mz na dn lx nb nc dp mb ld nd ne md lh nf ng mf ll nh ni mh nj bi translated">代理人</h2><p id="17ce" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">TF 中有不同的药剂——我们可以使用的药剂:<a class="ae lq" href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank"> DQN </a>、<a class="ae lq" href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" rel="noopener ugc nofollow" target="_blank">增援</a>、<a class="ae lq" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank"> DDPG </a>、<a class="ae lq" href="https://arxiv.org/pdf/1802.09477.pdf" rel="noopener ugc nofollow" target="_blank"> TD3 </a>、<a class="ae lq" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank"> PPO </a>和<a class="ae lq" href="https://arxiv.org/abs/1801.01290" rel="noopener ugc nofollow" target="_blank"> SAC </a>。如上所述，我们将使用 DQN。代理的一个主要参数是它的 Q(神经)网络，它将用于计算每一步中动作的 Q 值。q_network 有两个强制参数:定义观察形状和动作形状的 input_tensor_spec 和 action_spec。我们可以从我们的环境中得到这一点，因此我们将我们的 q_network 定义如下:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="23dd" class="mt ls it mp b gy mu mv l mw mx">q_net = q_network.QNetwork(env.observation_spec(), <br/>                           env.action_spec())</span></pre><p id="a890" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如你在这里看到的<a class="ae lq" href="https://github.com/tensorflow/agents/blob/master/tf_agents/networks/q_network.py" rel="noopener ugc nofollow" target="_blank"/>，我们可以为我们的 q_network 定制更多的参数，但是现在，我们将使用默认的参数。代理还需要一个优化器来查找 q_network 参数的值。让我们保持经典，用亚当。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="e9aa" class="mt ls it mp b gy mu mv l mw mx">optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)</span></pre><p id="54ec" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们用以下参数定义并初始化我们的代理:</p><ul class=""><li id="847c" class="np nq it kw b kx ky la lb ld nr lh ns ll nt lp nu nv nw nx bi translated">time_step_spec，它是我们从环境中获得的，定义了我们的时间步长是如何定义的。</li><li id="c59f" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">action_spec，与 q_network 相同。</li><li id="50cb" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">我们之前创建的 Q 网。</li><li id="6360" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">我们之前创建的优化器。</li><li id="0565" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">TD 误差损失函数，类似于神经网络中的损失函数。</li><li id="b52f" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">列车步数计数器，这只是一个 0 阶张量(也称为标量)，它将计算我们在环境中的步数。</li></ul><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="94f2" class="mt ls it mp b gy mu mv l mw mx">train_step_counter = tf.Variable(0)</span><span id="3a54" class="mt ls it mp b gy my mv l mw mx">agent = dqn_agent.DqnAgent(env.time_step_spec(),<br/>                           env.action_spec(),<br/>                           q_network=q_net,<br/>                           optimizer=optimizer,<br/>                           td_errors_loss_fn= <br/>                                  common.element_wise_squared_loss,<br/>                           train_step_counter=train_step_counter)</span><span id="e9cc" class="mt ls it mp b gy my mv l mw mx">agent.initialize()</span></pre><h2 id="d239" class="mt ls it bd lt mz na dn lx nb nc dp mb ld nd ne md lh nf ng mf ll nh ni mh nj bi translated">辅助方法:平均累积回报和收集数据</h2><p id="e1e9" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">我们还需要一些辅助方法。第一个将在环境中迭代若干集，应用策略来选择要遵循的操作，并返回这些集中的平均累积奖励。这将有助于评估我们的代理了解到的策略。下面，我们也在我们的环境中尝试该方法 10 集。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="4383" class="mt ls it mp b gy mu mv l mw mx">def compute_avg_return(environment, policy, num_episodes=10):<br/>    total_return = 0.0<br/>    for _ in range(num_episodes):<br/>        time_step = environment.reset()<br/>        episode_return = 0.0</span><span id="4e1f" class="mt ls it mp b gy my mv l mw mx">        while not time_step.is_last():<br/>            action_step = policy.action(time_step)<br/>            time_step = environment.step(action_step.action)<br/>            episode_return += time_step.reward<br/>        total_return += episode_return</span><span id="848c" class="mt ls it mp b gy my mv l mw mx">    avg_return = total_return / num_episodes<br/>    return avg_return.numpy()[0]</span><span id="5fe4" class="mt ls it mp b gy my mv l mw mx"># Evaluate the agent's policy once before training.<br/>avg_return = compute_avg_return(env, agent.policy, 5)<br/>returns = [avg_return]</span></pre><p id="437f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们还将在培训我们的代理时实现一种收集数据的方法。DQN 的突破之一是经验回放，我们将代理人的经验(状态、动作、奖励)存储起来，用它在每一步批量训练 Q 网络。这通过使学习更快和更稳定来改进学习。为了做到这一点，TF-Agents 包含了 TFUniformReplayBuffer 对象，它存储了这些经验以便以后重用，所以我们首先创建这个我们以后会用到的对象。</p><p id="8960" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这种方法中，我们采用一个环境、一个策略和一个缓冲区，采用由它的状态观察和奖励形成的当前时间步长、策略选择的动作以及下一个时间步长。然后，我们将它存储在重放缓冲区中。注意，重放缓冲区存储了一个名为 Trajectory 的对象，所以我们用前面命名的元素创建了这个对象，然后使用 add_batch 方法将它保存到缓冲区。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="9b76" class="mt ls it mp b gy mu mv l mw mx">replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(<br/>                                data_spec=agent.collect_data_spec,                                                                <br/>                                batch_size=env.batch_size,                                                              <br/>                                max_length=100000)</span><span id="e032" class="mt ls it mp b gy my mv l mw mx">def collect_step(environment, policy, buffer):<br/>    time_step = environment.current_time_step()<br/>    action_step = policy.action(time_step)<br/>    next_time_step = environment.step(action_step.action)<br/>    traj = trajectory.from_transition(time_step, <br/>                                      action_step, <br/>                                      next_time_step)</span><span id="e772" class="mt ls it mp b gy my mv l mw mx"># Add trajectory to the replay buffer<br/>    buffer.add_batch(traj)</span></pre><h2 id="4877" class="mt ls it bd lt mz na dn lx nb nc dp mb ld nd ne md lh nf ng mf ll nh ni mh nj bi translated">列车代理</h2><p id="f1b6" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">我们终于可以训练我们的特工了。我们定义了我们在每一次迭代中的步数，在这个步数之后，我们将在每一次迭代中训练我们的代理，修改它的策略。现在让我们每次迭代只使用一步。我们还定义了我们的 Q 网络将被训练的批量大小和一个迭代器，以便我们迭代代理的经验。</p><p id="488d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，我们将为缓冲器收集一些经验，并从常见的 RL 环路开始。通过对环境采取行动、培训政策和重复来获得经验。我们另外打印损失，并分别每 200 和 1000 步评估代理的性能。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="1bb2" class="mt ls it mp b gy mu mv l mw mx">collect_steps_per_iteration = 1<br/>batch_size = 64<br/>dataset = replay_buffer.as_dataset(num_parallel_calls=3, <br/>                                   sample_batch_size=batch_size, <br/>                                   num_steps=2).prefetch(3)<br/>iterator = iter(dataset)<br/>num_iterations = 20000<br/>env.reset()</span><span id="a175" class="mt ls it mp b gy my mv l mw mx">for _ in range(batch_size):<br/>    collect_step(env, agent.policy, replay_buffer)</span><span id="6005" class="mt ls it mp b gy my mv l mw mx">for _ in range(num_iterations):<br/>    # Collect a few steps using collect_policy and save to the replay buffer.<br/>    for _ in range(collect_steps_per_iteration):<br/>        collect_step(env, agent.collect_policy, replay_buffer)</span><span id="d06e" class="mt ls it mp b gy my mv l mw mx">    # Sample a batch of data from the buffer and update the agent's network.<br/>    experience, unused_info = next(iterator)<br/>    train_loss = agent.train(experience).loss</span><span id="2c62" class="mt ls it mp b gy my mv l mw mx">    step = agent.train_step_counter.numpy()</span><span id="4b1b" class="mt ls it mp b gy my mv l mw mx">    # Print loss every 200 steps.<br/>    if step % 200 == 0:<br/>        print('step = {0}: loss = {1}'.format(step, train_loss))</span><span id="1335" class="mt ls it mp b gy my mv l mw mx">    # Evaluate agent's performance every 1000 steps.<br/>    if step % 1000 == 0:<br/>        avg_return = compute_avg_return(env, agent.policy, 5)<br/>        print('step = {0}: Average Return = {1}'.format(step, avg_return))<br/>        returns.append(avg_return)</span></pre><h2 id="3a6f" class="mt ls it bd lt mz na dn lx nb nc dp mb ld nd ne md lh nf ng mf ll nh ni mh nj bi translated">情节</h2><p id="c307" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">我们现在可以画出当我们训练代理人时，累积的平均回报是如何变化的。为此，我们将使用 matplotlib 制作一个非常简单的绘图。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="41b0" class="mt ls it mp b gy mu mv l mw mx">iterations = range(0, num_iterations + 1, 1000)<br/>plt.plot(iterations, returns)<br/>plt.ylabel('Average Return')<br/>plt.xlabel('Iterations')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/4fd44b8335a11188f95cc6508376ba62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7fMiahhZlGIhBbNU0gmTHA.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">《DQN 特工》5 集的平均回报率。随着代理变得更有经验，您可以看到性能是如何随着时间的推移而提高的。</p></figure><h1 id="50d5" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">完全码</h1><p id="2b6b" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">我已经把本文中的<a class="ae lq" href="https://colab.research.google.com/drive/1Pd63OyiOnw4j401f3FN2tja4mH6EFMCc?usp=sharing" rel="noopener ugc nofollow" target="_blank">所有代码作为 Google Colab 笔记本</a>进行了分享。你可以直接运行所有代码，如果你想改变它，你必须把它保存在你自己的 Google drive 账户上，然后你可以做任何你想做的事情。如果你愿意，你也可以下载它在你的本地计算机上运行。</p><h1 id="0407" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">从这里去哪里</h1><ul class=""><li id="f08c" class="np nq it kw b kx mj la mk ld oe lh of ll og lp nu nv nw nx bi translated">你可以跟随 Github 上 TF-Agents 库中的<a class="ae lq" href="https://github.com/tensorflow/agents/tree/master/docs/tutorials" rel="noopener ugc nofollow" target="_blank">教程</a></li><li id="aa1b" class="np nq it kw b kx ny la nz ld oa lh ob ll oc lp nu nv nw nx bi translated">如果你想查看 RL 的其他好框架，你可以在这里看到我以前的帖子:</li></ul><div class="oh oi gp gr oj ok"><a rel="noopener follow" target="_blank" href="/5-frameworks-for-reinforcement-learning-on-python-1447fede2f18"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd iu gy z fp op fr fs oq fu fw is bi translated">Python 强化学习的 5 个框架</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">从头开始编程你自己的强化学习实现可能会有很多工作，但是你不需要做…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">towardsdatascience.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy ks ok"/></div></div></a></div><ul class=""><li id="0116" class="np nq it kw b kx ky la lb ld nr lh ns ll nt lp nu nv nw nx bi translated">你也可以在我前段时间写的另一篇文章的<a class="ae lq" href="https://medium.com/@mauriciofadelargerich/reinforcement-learning-environments-cff767bc241f" rel="noopener">中查看其他可以尝试 TF-Agents(或者任何你选择的 RL 算法)的环境。</a></li></ul><p id="c242" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">老规矩，感谢阅读！请在回复中告诉我你对 TF-Agents 的看法，如果你有任何问题或发现任何问题，也请告诉我🐛在代码中。</p></div></div>    
</body>
</html>