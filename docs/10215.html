<html>
<head>
<title>Maximum Likelihood Estimation in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">R 中的最大似然估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/maximum-likelihood-estimation-in-r-b21f68f1eba4?source=collection_archive---------2-----------------------#2020-07-19">https://towardsdatascience.com/maximum-likelihood-estimation-in-r-b21f68f1eba4?source=collection_archive---------2-----------------------#2020-07-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b74f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过这份快速简单的指南，最大限度地提高您在统计方面取得成功的可能性</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/165d58004bd0fb68cefb9f4406d89ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kU6n0w12LzyaHXnLqZCZjg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">🇨🇭·克劳迪奥·施瓦茨| @purzlbaum 在<a class="ae kv" href="https://unsplash.com/s/photos/coin?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="94da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常，你会有某种程度的直觉——或者具体的证据——来表明一组观察结果是由特定的统计分布产生的。与你正在模拟的现象相似的现象可能已经被证明可以用某种分布很好地解释。您正在研究的情况或问题的设置可能会自然地建议您尝试一系列的发行版。或者，你可能只是想通过将你的数据拟合到某个模糊的模型来获得一点乐趣，看看会发生什么(如果你在这方面受到挑战，告诉人们你正在做探索性的数据分析，并且当你在你的区域时，你不喜欢被打扰<em class="ls">。</em></p><p id="39be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，有许多方法可以根据你所拥有的数据来估计你所选择的模型的参数。其中最简单的是<em class="ls">矩量法</em>——一个有效的工具，但是<a class="ae kv" href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)" rel="noopener ugc nofollow" target="_blank">也有它的缺点</a>(值得注意的是，这些估计值经常<em class="ls">有偏差</em>)。</p><p id="1fe7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可能想考虑的另一种方法是<em class="ls">最大似然估计(MLE)</em>，它倾向于对模型参数产生更好的(即更无偏的)估计。技术含量高一点，但没什么我们不能处理的。让我们看看它是如何工作的。</p><h2 id="acfb" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">什么是可能性？</h2><p id="d8a2" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><em class="ls">似然</em>——更准确地说是<em class="ls">似然函数</em>——是一个表示从给定模型中获得某一组观察值的可能性有多大的函数。我们认为这组观察值是固定的——它们已经发生了，它们发生在过去——现在我们考虑在哪组模型参数下我们最有可能观察到它们。</p><h2 id="e0f8" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">一个简单的抛硬币的例子</h2><p id="02bb" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">考虑一个例子。假设我们将一枚硬币抛 100 次，观察到 52 个正面和 48 个反面。我们想出一个模型来预测如果我们继续翻转另外 100 次，我们会得到多少个头。</p><p id="6428" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">把这个问题正式化一点，让我们想想 100 次抛硬币得到的人头数。鉴于:</p><ul class=""><li id="6f59" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">只有两种可能的结果(正面和反面)，</li><li id="d9b1" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">有固定数量的“试验”(100 次抛硬币)，而且</li><li id="531d" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">“成功”的可能性是固定的。</li></ul><p id="88f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以合理地建议使用<em class="ls">二项式分布来模拟这种情况。</em></p><p id="9a5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用 R 来如下设置问题(查看用于本文的<a class="ae kv" href="https://github.com/andrewhetherington/python-projects/blob/master/Blog%E2%80%94Maximum%20Likelihood%20Estimation%20in%20R/Maximum%20Likelihood%20Estimation%20in%20R.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>以了解更多细节):</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="e806" class="lt lu iq ng b gy nk nl l nm nn"># I don’t know about you but I’m feeling<br/>set.seed(22)</span><span id="1f08" class="lt lu iq ng b gy no nl l nm nn"># Generate an outcome, ie number of heads obtained, assuming a fair coin was used for the 100 flips<br/>heads &lt;- rbinom(1,100,0.5)</span><span id="aa36" class="lt lu iq ng b gy no nl l nm nn">heads</span><span id="d98c" class="lt lu iq ng b gy no nl l nm nn"># 52</span></pre><p id="82be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">(为了生成数据，我们使用了 50%的机会得到正面/反面，尽管我们打算暂时假装不知道。对于几乎所有现实世界中的问题，我们都无法获得生成我们正在查看的数据的过程的这种信息——这就是为什么我们有动力去估计这些参数！)</em></p><p id="4d5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们将正面/反面过程公式化为二项式过程的情况下，我们假设每次掷硬币都有获得正面的概率<em class="ls"> p </em>。由此延伸，在 100 次翻转后获得 52 个头的概率由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/69d1243729d9bee82497aa69b8bdaccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/1*zoSSQJadKUKHR3Mv1yFUqA.gif"/></div></figure><p id="6d82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个概率就是我们的似然函数——它允许我们计算概率，即在给定正面概率的情况下，我们的数据集被观察到的可能性有多大。给定这种技术的名称，你也许能猜到下一步——我们必须找到使这个似然函数最大化的<em class="ls"> p </em>的值。</p><p id="0f01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以很容易地在 R 中用两种不同的方法计算这个概率:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="214d" class="lt lu iq ng b gy nk nl l nm nn"># To illustrate, let's find the likelihood of obtaining these results if p was 0.6—that is, if our coin was biased in such a way to show heads 60% of the time.<br/>biased_prob &lt;- 0.6</span><span id="0dfd" class="lt lu iq ng b gy no nl l nm nn"># Explicit calculation<br/>choose(100,52)*(biased_prob**52)*(1-biased_prob)**48</span><span id="55d4" class="lt lu iq ng b gy no nl l nm nn"># 0.0214877567069514</span><span id="3356" class="lt lu iq ng b gy no nl l nm nn"># Using R's dbinom function (density function for a given binomial distribution)<br/>dbinom(heads,100,biased_prob)</span><span id="734e" class="lt lu iq ng b gy no nl l nm nn"># 0.0214877567069514</span></pre><p id="21da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回到我们的问题——我们想知道我们的数据暗示的 p 的值。对于像正在考虑的这种简单情况，可以对关于被估计参数的似然函数进行微分，并使结果表达式等于零，以求解<em class="ls"> p </em>的最大似然估计。然而，对于更复杂(和现实)的过程，您可能不得不求助于数值方法。</p><p id="70bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">幸运是，这也是一件轻松的事情！我们的方法如下:</p><ol class=""><li id="9610" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr nq mx my mz bi translated">定义一个函数，该函数将计算给定值<em class="ls"> p </em>的似然函数；然后</li><li id="df96" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr nq mx my mz bi translated">搜索产生最高可能性的<em class="ls"> p </em>的值。</li></ol><p id="50c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从第一步开始:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="50eb" class="lt lu iq ng b gy nk nl l nm nn">likelihood &lt;- function(p){<br/>  dbinom(heads, 100, p)<br/>}</span><span id="88fa" class="lt lu iq ng b gy no nl l nm nn"># Test that our function gives the same result as in our earlier example<br/>likelihood(biased_prob)</span><span id="ca35" class="lt lu iq ng b gy no nl l nm nn"># 0.0214877567069513</span></pre><p id="3aa7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在考虑第二步。在 R 中有许多不同的优化(即最大化或最小化)函数的方法——我们在这里考虑的一种方法利用了代表非线性最小化的<em class="ls"> nlm </em>函数。如果你给 nlm 一个函数，并指出你希望它改变哪个参数，它将遵循一个算法并迭代工作，直到它找到使函数值最小的那个参数值。</p><p id="9fb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可能会担心我引入了一个工具来<em class="ls">最小化</em>一个函数值，而我们实际上是在寻找<em class="ls">最大化——</em>这毕竟是最大似然估计！幸运的是，<em class="ls">最大化一个函数相当于最小化这个函数乘以-1。</em>如果我们创建一个新函数，简单地将可能性乘以-1，那么使这个新函数的值最小化的参数将与使我们的原始可能性最大化的参数完全相同。</p><p id="ab2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，对我们的职能进行一点小小的调整是合适的:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="de1b" class="lt lu iq ng b gy nk nl l nm nn">negative_likelihood &lt;- function(p){<br/>  dbinom(heads, 100, p)*-1<br/>}</span><span id="60ea" class="lt lu iq ng b gy no nl l nm nn"># Test that our function is behaving as expected<br/>negative_likelihood(biased_prob)</span><span id="3589" class="lt lu iq ng b gy no nl l nm nn"># -0.0214877567069513</span></pre><p id="6b18" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很好——我们现在准备好找到我们的<em class="ls"> p </em>的 MLE 值。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="f1c2" class="lt lu iq ng b gy nk nl l nm nn">nlm(negative_likelihood,0.5,stepmax=0.5)</span><span id="33e5" class="lt lu iq ng b gy no nl l nm nn"># $minimum<br/># -0.07965256</span><span id="2039" class="lt lu iq ng b gy no nl l nm nn"># $estimate<br/># 0.5199995</span><span id="d2ff" class="lt lu iq ng b gy no nl l nm nn"># $gradient<br/># -2.775558e-11</span><span id="d37d" class="lt lu iq ng b gy no nl l nm nn"># $code<br/># 1</span><span id="0a75" class="lt lu iq ng b gy no nl l nm nn"># $iterations<br/># 4</span></pre><p id="159a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">nlm 函数返回了一些关于寻找 p 的最大似然估计的信息。</p><ul class=""><li id="7650" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">$minimum 表示找到的负可能性的最小值，因此最大可能性就是这个值乘以负一，即 0.07965…；</li><li id="9864" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">$estimate 是我们对 p 的极大似然估计；</li><li id="c31e" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">$gradient 是我们对 p 的估计值附近的似然函数的梯度——对于成功的估计，我们希望它非常接近于零；</li><li id="7f32" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">$code 解释使用最小化算法终止的原因——值 1 表示最小化可能已经成功；和</li><li id="7a7c" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">$iterations 告诉我们 nlm 为获得参数的最佳值而必须经历的迭代次数。</li></ul><p id="bc8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">知道这些信息很好——但我们真正关心的是，它告诉我们，我们对<em class="ls"> p </em>的极大似然估计是 0.52。我们可以凭直觉判断这是正确的——在 100 次投掷中，哪个硬币更有可能让我们 52 次正面朝上，而不是 52%的时候正面朝上？</p><h2 id="a20d" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">结论</h2><p id="117c" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">在我们今天看到的这个相当琐碎的例子中，我们似乎已经经历了很多争论才得出一个相当明显的结论。但是，请考虑一个问题，其中您有一个更复杂的分布和多个参数要优化，最大似然估计的问题变得更加困难，幸运的是，我们今天探索的过程可以很好地扩展到这些更复杂的问题。</p><p id="9334" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最终，如果你想建立稳健的模型，你最好很好地掌握最大似然估计——在我看来，你只是朝着最大化你的成功机会又迈进了一步——或者你更愿意认为这是最小化你的失败概率？</p></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><h1 id="5f0a" class="ny lu iq bd lv nz oa ob ly oc od oe mb jw of jx me jz og ka mh kc oh kd mk oi bi translated">学分和更多信息</h1><p id="f789" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><strong class="ky ir"> Andrew Hetherington </strong>是英国伦敦的一名见习精算师和数据爱好者。</p><ul class=""><li id="545d" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">查看我的<a class="ae kv" href="https://www.andrewhetherington.com/" rel="noopener ugc nofollow" target="_blank">网站</a>。</li><li id="2c42" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">在<a class="ae kv" href="https://www.linkedin.com/in/andrewmhetherington/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系。</li><li id="109e" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">看看我在<a class="ae kv" href="https://github.com/andrewhetherington/python-projects" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上摆弄什么。</li><li id="baa8" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">用于制作本文作品的笔记本可以在<a class="ae kv" href="https://github.com/andrewhetherington/python-projects/blob/master/Blog%E2%80%94Maximum%20Likelihood%20Estimation%20in%20R/Maximum%20Likelihood%20Estimation%20in%20R.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</li></ul><p id="0389" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">硬币照片由<a class="ae kv" href="https://unsplash.com/@purzlbaum?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">🇨🇭·克劳迪奥·施瓦茨| @purzlbaum </a>在<a class="ae kv" href="https://unsplash.com/s/photos/coin?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄。</p></div></div>    
</body>
</html>