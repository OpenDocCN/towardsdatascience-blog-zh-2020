<html>
<head>
<title>Semantic Image Segmentation using Fully Convolutional Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于全卷积网络的语义图像分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semantic-image-segmentation-using-fully-convolutional-networks-bf0189fa3eb8?source=collection_archive---------10-----------------------#2020-05-10">https://towardsdatascience.com/semantic-image-segmentation-using-fully-convolutional-networks-bf0189fa3eb8?source=collection_archive---------10-----------------------#2020-05-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d790" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">语义分割/扩展卷积</h2><div class=""/><div class=""><h2 id="2b0b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Severstal钢缺陷检测——案例研究</h2></div><p id="2b7f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi ln translated">人类有一种与生俱来的能力，能够识别他们在周围世界看到的物体。我们大脑中的视觉皮层几乎可以在短时间内毫不费力地区分猫和狗。这不仅适用于猫和狗，也适用于我们看到的几乎所有物体。但是计算机没有人脑聪明，无法自己做到这一点。在过去的几十年里，深度学习研究人员试图通过一种特殊类型的人工神经网络(称为卷积神经网络(CNN))来弥合人脑和计算机之间的这种差距。</p><h1 id="f43c" class="lw lx it bd ly lz ma mb mc md me mf mg ki mh kj mi kl mj km mk ko ml kp mm mn bi translated">什么是卷积神经网络？</h1><p id="1e6a" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">在对哺乳动物大脑进行了大量研究后，研究人员发现，大脑的特定部分在特定类型的刺激下会被激活。例如，当我们看到垂直边缘时，视觉皮层的一些部分被激活，当我们看到水平边缘时，一些部分被激活，当我们看到特定的形状、颜色、脸等时，一些部分被激活。ML的研究人员将这些部分想象成神经网络的一层，并考虑了这种层的大型网络可以模仿人脑的想法。<br/>这种直觉导致了CNN的出现，这是一种神经网络，其构建块是卷积层。卷积层只不过是一组称为核或滤波器的权重矩阵，用于对诸如图像的特征矩阵进行卷积运算。</p><p id="8c9c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">卷积:<br/> </strong> 2D卷积是一个相当简单的操作，你从一个内核开始，然后“跨越”(滑动)2D输入数据，与它当前所在的输入部分执行逐元素乘法，然后将结果相加到单个输出单元中。内核对它滑过的每个位置重复这个过程，将2D特征矩阵转换成另一个2D特征矩阵。<br/>内核在输入特征矩阵上滑动的步长称为<strong class="kt jd"> <em class="mt">步距</em> </strong>。在下面的动画中，输入矩阵的四边都添加了额外的零条纹，以确保输出矩阵的大小与输入矩阵的大小相同。这称为(零)填充。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mu"><img src="../Images/150a600ffbbb7b6f3f76d0b8b7755966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*O06nY1U7zoP4vE5AZEnxKA.gif"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated"><strong class="bd nk"> 2D卷积</strong> : <strong class="bd nk"> </strong>内核大小=3x3，填充=1或‘相同’，步幅= 1<strong class="bd nk"/>【GIF来源:<a class="ae nl" href="https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/" rel="noopener ugc nofollow" target="_blank"> pyimagesearch </a></p></figure><h1 id="8280" class="lw lx it bd ly lz ma mb mc md me mf mg ki mh kj mi kl mj km mk ko ml kp mm mn bi translated">语义图像分割</h1><p id="1900" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">图像分割是基于一些特征将数字图像分割成多个片段(像素组)的任务。目标是将图像简化或改变成更有意义和更容易分析的表示。<br/>语义分割是指给定图像中的每个像素分配一个类别标签。参见下面的例子。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/221e79a0349d80f6e4ea74f8101976d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*rHRTfsEGRD0_gSyWB-e3wQ.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">图片来源:<a class="ae nl" href="https://commons.wikimedia.org/wiki/File:Image-segmentation-example-segmented.png" rel="noopener ugc nofollow" target="_blank">马丁托马/ CC0 </a></p></figure><blockquote class="nn no np"><p id="bcb8" class="kr ks mt kt b ku kv kd kw kx ky kg kz nq lb lc ld nr lf lg lh ns lj lk ll lm im bi translated">请注意，分段不同于分类。在分类中，完整的图像被分配一个类别标签，而在分割中，图像中的每个像素被分类到一个类别中。</p></blockquote><h1 id="107b" class="lw lx it bd ly lz ma mb mc md me mf mg ki mh kj mi kl mj km mk ko ml kp mm mn bi translated">1.商业问题</h1><p id="9181" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">对卷积网络和语义图像分割有了一个公平的想法，让我们进入我们需要解决的问题。</p><div class="mv mw mx my gt ab cb"><figure class="nt mz nu nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/a8755b4e184a6416af9a853ede2d1e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*GKv2VDQ-cntclZ_nUyuknA.png"/></div></figure><figure class="nt mz nz nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/b45a30514a9adee8e483fe593ad60bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1654/format:webp/1*eXpil3a8QZsgn1Zms_Du5w.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk oa di ob oc translated">图片来源:<a class="ae nl" href="https://www.kaggle.com/c/severstal-steel-defect-detection" rel="noopener ugc nofollow" target="_blank"> Kaggle </a></p></figure></div><p id="ac5c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">俄罗斯北方钢铁公司是世界50大钢铁生产商之一，也是俄罗斯高效钢铁开采和生产的最大参与者。谢韦尔钢铁公司的主要产品之一是钢板。平板钢板的生产工艺精细。从加热和轧制，到干燥和切割，当扁钢准备装运时，几台机器会接触到它。为了确保钢板生产的质量，今天，谢韦尔钢铁公司使用高频摄像机的图像来驱动缺陷检测算法。<br/>通过这次比赛，Severstal希望人工智能社区能够通过<strong class="kt jd">定位和分类钢板上的表面缺陷</strong>来改进算法。</p><h2 id="ea96" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">业务目标和约束</h2><ol class=""><li id="21d8" class="oo op it kt b ku mo kx mp la oq le or li os lm ot ou ov ow bi translated">有缺陷的纸张必须被预测为有缺陷的，因为如果我们将有缺陷的纸张错误地分类为无缺陷的，将会有严重的质量问题。即每个类别都需要高召回值。</li><li id="10e2" class="oo op it kt b ku ox kx oy la oz le pa li pb lm ot ou ov ow bi translated">我们不需要一眨眼就给出给定图像的结果。(没有严格的延迟问题)</li></ol><h1 id="718c" class="lw lx it bd ly lz ma mb mc md me mf mg ki mh kj mi kl mj km mk ko ml kp mm mn bi translated">2.机器学习问题</h1><h2 id="2cca" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">2.1.将业务问题映射到ML问题</h2><p id="0201" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">我们的任务是</p><ol class=""><li id="8ba6" class="oo op it kt b ku kv kx ky la pc le pd li pe lm ot ou ov ow bi translated">使用图像分割检测/定位钢板中的缺陷</li><li id="36c0" class="oo op it kt b ku ox kx oy la oz le pa li pb lm ot ou ov ow bi translated">将检测到的缺陷分成[1，2，3，4]中的一个或多个类别</li></ol><p id="5f06" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">放在一起就是一个语义图像分割问题。</p><h2 id="5048" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">2.2.绩效指标</h2><p id="f483" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">使用的评估指标是平均Dice系数。Dice系数可用于比较预测的分割与其对应的基本事实之间的逐像素一致性。该公式由下式给出:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pf"><img src="../Images/6ef3075d7c058cb87f9818bb564fa04c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n5pgnKIcchRFWlw0YeoElA.png"/></div></div></figure><p id="1fe5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">其中X是预测的像素组，Y是地面实况。<br/> <a class="ae nl" href="https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient" rel="noopener ugc nofollow" target="_blank">在这里阅读更多关于骰子系数的内容</a>。</p><h2 id="c06b" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">2.3.数据概述</h2><p id="c178" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">我们收到了一个2GB大小的zip文件夹，其中包含以下内容:</p><ul class=""><li id="64ed" class="oo op it kt b ku kv kx ky la pc le pd li pe lm pg ou ov ow bi translated"><code class="fe ph pi pj pk b"><strong class="kt jd">train_images</strong></code>—包含12，568张训练图像的文件夹(。jpg文件)</li><li id="aef2" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated"><code class="fe ph pi pj pk b"><strong class="kt jd">test_images</strong></code> —包含5506个测试图像的文件夹(。jpg文件)。我们需要检测和定位这些图像中的缺陷</li><li id="f149" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated"><code class="fe ph pi pj pk b"><strong class="kt jd">train.csv</strong></code> —为属于ClassId = [1，2，3，4]的缺陷提供分段的训练注释</li><li id="dd36" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated"><code class="fe ph pi pj pk b"><strong class="kt jd">sample_submission.csv</strong></code> —正确格式的样本提交文件，每个<em class="mt"> ImageId </em>重复4次，4个缺陷类别各一次。</li></ul><p id="4fb6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">关于数据的更多细节将在下一节讨论。</p><h1 id="2f49" class="lw lx it bd ly lz ma mb mc md me mf mg ki mh kj mi kl mj km mk ko ml kp mm mn bi translated">3.探索性数据分析</h1><p id="6a62" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">解决任何机器学习问题的第一步应该是彻底研究原始数据。这为我们解决问题的方法提供了一个合理的思路。通常，它还能帮助我们发现数据的一些潜在方面，这些方面可能对我们的模型有用。让我们分析这些数据，试着得出一些有意义的结论。</p><h2 id="f833" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">3.1.正在加载train.csv文件</h2><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/1992e496a4769c6c06664ad5771b96b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*EROWw6t9LdFaAVFfGn-CzQ.png"/></div></figure><p id="c06a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="mt"> train.csv </em> </strong>告知图像中哪个像素位置存在哪种类型的缺陷。它包含以下列:</p><ul class=""><li id="275d" class="oo op it kt b ku kv kx ky la pc le pd li pe lm pg ou ov ow bi translated"><code class="fe ph pi pj pk b"><strong class="kt jd">ImageId</strong></code>:图像文件名。jpg扩展</li><li id="ab3b" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated"><code class="fe ph pi pj pk b"><strong class="kt jd">ClassId</strong></code>:缺陷的类型/等级，为【1、2、3、4】中的一种</li><li id="a9e5" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated"><code class="fe ph pi pj pk b"><strong class="kt jd">EncodedPixels</strong></code>:以游程编码像素的形式表示图像中缺陷像素的范围(缺陷开始的像素数&lt;空格&gt;缺陷的像素长度)。<br/> <em class="mt">例如“29102 12”表示缺陷从像素29102开始，总共有12个像素，即像素29102、29103、………、29113有缺陷。像素从上到下，然后从左到右进行编号:1对应于像素(1，1)，2对应于(2，1)，依此类推。</em></li></ul><pre class="mv mw mx my gt pm pk pn po aw pp bi"><span id="0b91" class="od lx it pk b gy pq pr l ps pt"><strong class="pk jd">train_df.ImageId.describe()</strong></span><span id="0fe2" class="od lx it pk b gy pu pr l ps pt">count              7095<br/>unique             6666<br/>top       ef24da2ba.jpg<br/>freq                  3<br/>Name: ImageId, dtype: object</span></pre><ul class=""><li id="7c1f" class="oo op it kt b ku kv kx ky la pc le pd li pe lm pg ou ov ow bi translated">有7095个数据点对应于包含缺陷的6666张钢板图像。</li></ul><h2 id="d458" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">3.2.分析train_images和test_images文件夹</h2><p id="91ed" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated"><strong class="kt jd">训练和测试图像的数量<br/> </strong>让我们了解一下训练和测试图像的比例，并检查有多少训练图像包含缺陷。</p><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="pv pw l"/></div></figure><pre class="mv mw mx my gt pm pk pn po aw pp bi"><span id="7842" class="od lx it pk b gy pq pr l ps pt">Number of train images :  12568<br/>Number of test images :  5506<br/>Number of non-defective images in the train_images folder: 5902</span></pre><ul class=""><li id="98b9" class="oo op it kt b ku kv kx ky la pc le pd li pe lm pg ou ov ow bi translated">train_images文件夹中的图像比<em class="mt"> train.csv </em>中的唯一图像id还多。这意味着，并非train_images文件夹中的所有图像都具有缺陷1、2、3、4中的至少一个。</li></ul><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi px"><img src="../Images/652478bd0c981e383d4ceb1407a6ca6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*6zvz5gEgkVnVkJNEz1LmQQ.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated"><strong class="bd nk">列车数据中有缺陷和无缺陷图像的百分比</strong></p></figure><p id="20f3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">训练图像和测试图像的尺寸<br/> </strong>让我们检查一下训练图像和测试图像的尺寸是否相同。如果没有，我们必须使它们大小相同。</p><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="pv pw l"/></div></figure><pre class="mv mw mx my gt pm pk pn po aw pp bi"><span id="a30b" class="od lx it pk b gy pq pr l ps pt">{(256, 1600, 3)}<br/>{(256, 1600, 3)}</span></pre><ul class=""><li id="d092" class="oo op it kt b ku kv kx ky la pc le pd li pe lm pg ou ov ow bi translated">训练和测试文件夹中的所有图像大小相同(256×1600×3)</li></ul><h2 id="7174" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">3.3.标签分析:ClassId</h2><p id="026d" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">让我们看看训练数据是如何分布在各个类中的。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi py"><img src="../Images/561354fd89813d75058f9a3acfdbe043.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*KWd6TAvDWGzPbLYz9PGcaA.png"/></div></figure><pre class="mv mw mx my gt pm pk pn po aw pp bi"><span id="3409" class="od lx it pk b gy pq pr l ps pt">Number of images in class 1 : <strong class="pk jd">5150 (77.258 %)</strong><br/>Number of images in class 2 : <strong class="pk jd">897 (13.456 %)</strong><br/>Number of images in class 3 : <strong class="pk jd">801 (12.016 %)</strong><br/>Number of images in class 4 : <strong class="pk jd">247 (3.705 %)</strong></span></pre><ul class=""><li id="ce22" class="oo op it kt b ku kv kx ky la pc le pd li pe lm pg ou ov ow bi translated">数据集看起来不平衡。</li><li id="0233" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated">与其他类别的图像相比，具有类别3缺陷的图像的数量非常高。77%的缺陷图像具有3级缺陷。</li><li id="9e2e" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated">类别2是最少出现的类别，train.csv中只有3.7 %的图像属于类别2。</li></ul><blockquote class="nn no np"><p id="1d94" class="kr ks mt kt b ku kv kd kw kx ky kg kz nq lb lc ld nr lf lg lh ns lj lk ll lm im bi translated">注意，上述分析中的百分比值之和大于100，这意味着一些图像具有属于一个以上类别的缺陷。</p></blockquote><p id="4bcd" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">每幅图像标记的标签数量</strong></p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi py"><img src="../Images/0437792428e4ba7664119e958f685f08.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*pqdBH4ns6LQRpreKQKXg8A.png"/></div></figure><pre class="mv mw mx my gt pm pk pn po aw pp bi"><span id="e20d" class="od lx it pk b gy pq pr l ps pt">Number of images having 1 class label(s): <strong class="pk jd">6239 (93.594%)</strong><br/>Number of images having 2 class label(s): <strong class="pk jd">425 (6.376%)</strong><br/>Number of images having 3 class label(s): <strong class="pk jd">2 (0.03%)</strong></span></pre><ul class=""><li id="1ffb" class="oo op it kt b ku kv kx ky la pc le pd li pe lm pg ou ov ow bi translated">大多数图像(93.6%)只有一类缺陷。</li><li id="62a0" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated">只有2个图像(0.03%)具有3类缺陷的组合。</li><li id="a0f0" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated">其余的图像(6.37%)具有两类缺陷的组合。</li><li id="4e72" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated">没有图像具有所有4类缺陷。</li></ul><h1 id="83cf" class="lw lx it bd ly lz ma mb mc md me mf mg ki mh kj mi kl mj km mk ko ml kp mm mn bi translated">4.数据准备</h1><p id="7548" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">在我们继续训练深度学习模型之前，我们需要将原始数据转换为可以输入模型的形式。此外，我们需要构建一个数据管道，它将执行所需的预处理，并为训练生成成批的输入和输出图像。</p><p id="8355" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">作为第一步，我们创建一个pandas dataframe，包含列<code class="fe ph pi pj pk b"><strong class="kt jd">ImageId</strong></code>下的列车图像的文件名，以及一个或多个列<code class="fe ph pi pj pk b"><strong class="kt jd">Defect_1</strong></code>、<code class="fe ph pi pj pk b"><strong class="kt jd">Defect_2, Defect_3, Defect_4 </strong></code>下的<strong class="kt jd"> <em class="mt">编码像素</em>，这取决于<em class="mt"> train.csv中图像的<strong class="kt jd"><em class="mt">ClassId</em></strong>。</em>没有任何缺陷的图像将这4列全部留空。以下是数据帧的示例:</strong></p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pz"><img src="../Images/69610736dc9ad6579bf8f7919e618cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PvVGX6omgoswPo1_LfyAlA.png"/></div></div></figure><h2 id="c04e" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">4.1.列车，CV分流85:15</h2><p id="17e7" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">我会在85%的训练图像上训练我的模型，并在15%上验证。</p><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="pv pw l"/></div></figure><pre class="mv mw mx my gt pm pk pn po aw pp bi"><span id="8458" class="od lx it pk b gy pq pr l ps pt">(10682, 5)<br/>(1886, 5)</span></pre><h2 id="977b" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">4.2.用于将RLE编码像素转换为遮罩的实用函数，反之亦然</h2><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="pv pw l"/></div></figure><p id="5026" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们想象一下每个班级的一些图片和他们的面具。钢板图像中属于缺陷区域的像素在掩模图像中用黄色表示。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/8a688ba3fa299ec03ef4208ae1542245.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*RZCovjZWPuUGJjdpIP_UnQ.png"/></div></figure><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/b91553733ca8cc708d5ab5c2666fcc00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*AAOWxfJflBhRVrw3fmUPng.png"/></div></figure><p id="4176" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们的深度学习模型将钢板图像作为输入(X)并返回四个掩码(Y)(对应于4个类别)作为输出。这意味着，为了训练我们的模型，我们需要将成批的训练图像和它们相应的掩码输入到模型中。<br/>我已经为train_images文件夹中的所有图像生成了遮罩，并将它们存储在名为train_masks的文件夹中。</p><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="pv pw l"/></div></figure><h2 id="6655" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">4.3.使用tensorflow.data的数据生成器</h2><p id="164c" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">以下代码是数据管道，用于对输入图像应用预处理、增强，并生成用于训练的批次。</p><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="pv pw l"/></div></figure><h2 id="fc4d" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">4.4.定义度量和损失函数</h2><p id="e869" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">我使用了一个混合损失函数，它是由<strong class="kt jd"><em class="mt">【BCE】</em></strong>和<strong class="kt jd"> <em class="mt">骰子损失</em> </strong>组合而成的。BCE对应于每个像素的二进制分类(当与地面真实掩模比较时，0表示该像素处缺陷的错误预测，1表示正确预测)。骰子损失由(1骰子系数)给出。<br/> <strong class="kt jd"> <em class="mt"> BCE骰子损失= BCE +骰子损失</em> </strong></p><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="pv pw l"/></div></figure><h1 id="48fa" class="lw lx it bd ly lz ma mb mc md me mf mg ki mh kj mi kl mj km mk ko ml kp mm mn bi translated">5.模型</h1><p id="3dcd" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">有几种用于语义图像分割的模型/架构。在这个案例研究中，我尝试了其中的两个:i)U-Net和ii) Google的DeepLabV3+。</p><h2 id="ba41" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated"><strong class="ak"> 5.1。第一个切割解决方案:用于语义图像分割的U-Net</strong></h2><p id="6f64" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">这个模型是基于德国弗赖堡大学<em class="mt">的<em class="mt"> Olaf Ronneberger </em>、<em class="mt"> Philipp Fischer </em>和<em class="mt"> Thomas Brox </em>于2015年发表的研究论文<a class="ae nl" href="https://arxiv.org/abs/1505.04597v1" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> U-Net:卷积网络用于生物医学图像分割</strong> </a>。在本文中，作者构建了一个优雅的架构，称为“<strong class="kt jd">全卷积网络</strong>”。他们已经将此用于电子显微镜堆栈和少数其他生物医学图像分割数据集中的神经元结构的分割。</em></p><p id="eee2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 5.1.1。架构</strong> <br/>网络的架构如下图所示。它由收缩路径(左侧)和扩张路径(右侧)组成。扩展路径与收缩路径对称，使得网络的形状类似于英文字母“U”。由于这个原因，这个网络被称为U-Net。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qc"><img src="../Images/342f2c88bc2f14bea027a44026df8a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvXoKMHoPJMKpKK7keZMEA.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">U-net架构(以最低分辨率的32x32像素为例)。每个蓝框对应一个多通道特征图。盒子的顶部标明了频道的数量。x-y尺寸位于框的左下边缘。白色方框表示复制的要素地图。箭头表示不同的操作。[图片来源:<a class="ae nl" href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" rel="noopener ugc nofollow" target="_blank">弗赖堡大学</a> ]</p></figure><p id="5521" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">收缩路径遵循卷积网络的典型架构。它由两个3×3卷积(无填充卷积)的重复应用组成，每个卷积后跟一个整流线性单元(ReLU)和一个跨距为2的2×2最大池操作，用于下采样。在每个下采样步骤中，特征通道的数量会翻倍。在此路径中，模型从图像中捕获重要的特征(类似于钢板中的缺陷)并丢弃不重要的特征，从而降低图像在每个卷积+最大池层的分辨率。<br/>在扩展路径中，每一步都包括特征图的上采样，随后是将特征通道的数量减半的2x2卷积(<strong class="kt jd">上卷积</strong>)、与来自收缩路径的相应裁剪的特征图的连接，以及两个3x3卷积，每个卷积之后是ReLU。由于每次卷积都会丢失边界像素，因此裁剪是必要的。在最后一层，使用1x1卷积将每个64分量的特征向量映射到所需的类别数(在我们的例子中为4)。<br/>为了精确定位，来自收缩路径的高分辨率特征被裁剪并与上采样输出相结合，并被馈送到后续卷积层，该卷积层将学习组装更精确的输出。</p><ul class=""><li id="76c5" class="oo op it kt b ku kv kx ky la pc le pd li pe lm pg ou ov ow bi translated">不是在第一层中使用64个滤波器，而是仅使用8个滤波器(后续层中的滤波器数量也相应改变)。这导致较不复杂的模型，与具有64个过滤器的模型相比，训练更快。</li><li id="d2ce" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated">钢板图像的原始尺寸为256x1600。大尺寸图像包含更多像素，因此需要更多卷积、池化等计算。由于计算资源的限制，我已经将图像的大小调整到一半(128x800)。</li><li id="c9ff" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated">我在每个卷积块后添加了一个小的dropout = 0.2，以避免模型过度拟合。</li></ul><blockquote class="nn no np"><p id="202b" class="kr ks mt kt b ku kv kd kw kx ky kg kz nq lb lc ld nr lf lg lh ns lj lk ll lm im bi translated">U-Net模型的代码可以在我的<a class="ae nl" href="https://github.com/aruns2120/Semantic-Segmentation-Severstal/tree/master/U-Net" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中找到。</p></blockquote><p id="33f6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">5.1.2。训练<br/> 我已经使用<a class="ae nl" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam" rel="noopener ugc nofollow" target="_blank"> Keras Adam optimizer </a>以50个时期的默认学习速率训练了模型。优化器试图最小化的损失函数是<strong class="kt jd"> bce_dice_loss </strong>，在前面的<a class="ae nl" href="#fc4d" rel="noopener ugc nofollow">第4.4节</a>中定义。</p><p id="54cc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随着培训的进行，我已经使用<a class="ae nl" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint" rel="noopener ugc nofollow" target="_blank"> Keras模型检查点</a>来监控<strong class="kt jd">验证骰子系数</strong>，并保存具有最佳验证骰子系数分数的模型。TensorBoard已被用于在训练时动态绘制损失和得分。</p><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="pv pw l"/></div></figure><p id="6205" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 5.1.3。训练图</strong></p><div class="mv mw mx my gt ab cb"><figure class="nt mz qd nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/acb8b13f6b0e4efba30e9687b725de01.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*VPwFUVbTAJoszfk1iUvRiQ.jpeg"/></div></figure><figure class="nt mz qe nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/4257dbc67e105673661f150160563ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*OlsHzXY9MwSRCg22gACOIQ.jpeg"/></div></figure></div><p id="5d20" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">5.1.4。测试<br/></p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qf"><img src="../Images/cec23b9934b7c714d896b9d811febb2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qWGAT9YIGiA5od0NvOGK6A.jpeg"/></div></div></figure><blockquote class="nn no np"><p id="0e98" class="kr ks mt kt b ku kv kd kw kx ky kg kz nq lb lc ld nr lf lg lh ns lj lk ll lm im bi translated">由于Kaggle要求我们提交原始大小的预测，而不是一半大小的图像，我用输入大小= (256，1600，3)重建了模型，并加载了在128×800图像上训练的模型的权重。我这么做是因为CNN对于不同的输入大小是相当稳定的。</p></blockquote><figure class="mv mw mx my gt mz"><div class="bz fp l di"><div class="pv pw l"/></div></figure><ul class=""><li id="af9f" class="oo op it kt b ku kv kx ky la pc le pd li pe lm pg ou ov ow bi translated">当我在Kaggle上上传这个模型的预测时，Dice系数得分相当不错。我在私人排行榜得到了<strong class="kt jd"> 0.80943 </strong>的分数，在公共排行榜得到了<strong class="kt jd"> 0.81369 </strong>的分数。</li></ul><h2 id="9839" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">5.2.最终解决方案:DeepLab V3+</h2><p id="0efc" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">DeepLab是谷歌在2016年设计并开源的最先进的语义分割模型。从那以后，对该模型进行了多次改进，包括DeepLab V2、DeepLab V3和最新的DeepLab V3+。<br/> DeepLab V3+基于Google于2018年发表的论文<a class="ae nl" href="https://arxiv.org/abs/1802.02611" rel="noopener ugc nofollow" target="_blank">采用阿特鲁可分离卷积进行语义图像分割的编解码器</a>。</p><p id="bf35" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 5.2.1。架构<br/> </strong>与前面讨论的U-Net类似，DeepLab V3+也是一种编解码架构。主要区别在于它使用阿特鲁卷积而不是简单卷积。在这一节的后面，我们将了解更多关于阿特鲁卷积的知识。</p><div class="mv mw mx my gt ab cb"><figure class="nt mz qg nv nw nx ny paragraph-image"><img src="../Images/157d5c3211f35119ef8eac7bf56031e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*lIQDSI_o1ck8qCf_0sTW5w.png"/></figure><figure class="nt mz qh nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/d55c28fe5e623e50b2a9f9b997dc2649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*ALCCidPL6aWAfChXh8Wqig.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk qi di qj oc translated"><strong class="bd nk"> DeepLab V3+架构</strong>【图片来源:<a class="ae nl" href="https://arxiv.org/abs/1802.02611v3" rel="noopener ugc nofollow" target="_blank">原创研究论文</a></p></figure></div><p id="9811" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">编码器模块通过在多个尺度上应用复杂的卷积来编码多尺度上下文信息，而简单的eﬀective解码器模块沿着对象边界重新定义分割结果。</p><p id="ef1c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">阿特鲁卷积<br/>在二维信号的情况下，对于输出特征图<strong class="kt jd"> <em class="mt"> y </em> </strong>上的每个位置<strong class="kt jd"> <em class="mt"> i </em> </strong>和卷积滤波器<strong class="kt jd"> <em class="mt"> w </em> </strong>，如下所示对输入特征图<strong class="kt jd"><em class="mt">×T17】</em></strong>应用atrous卷积:</strong></p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/210889a29396735f4086f964e823d7eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*qfvi8mJq1e-8Q3EPBhLdmA.png"/></div></figure><p id="f414" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">其中，<strong class="kt jd"> <em class="mt"> r </em> </strong>决定了我们对输入信号进行采样的步距。注意，标准卷积是速率r = 1的特例。通过改变膨胀/收缩率值，滤波器的视野得到自适应修改。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ql"><img src="../Images/c4aeff0cd7566a42422c45c1567f0199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*LEd-BuGZBKYL9iDnV3_YFQ.gif"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated"><strong class="bd nk">标准卷积</strong>(左)vs <strong class="bd nk">扩张或阿特鲁卷积</strong>(右)【GIF来源:<a class="ae nl" href="https://towardsdatascience.com/@sh.tsang?source=post_page-----d527e1a8fb5----------------------" rel="noopener" target="_blank"> <br/> Sik-Ho Tsang </a>经由<a class="ae nl" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">走向数据科学</a></p></figure><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qm"><img src="../Images/1d2417b14886b100086a8f8fb33a83a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CqUsDX1SD-hNufde9AyX_g.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">图片来源:<a class="ae nl" href="https://www.researchgate.net/figure/Atrous-convolution-kernel-green-dilated-with-different-rates_fig6_329213875" rel="noopener ugc nofollow" target="_blank"> ResearchGate/CC </a></p></figure><p id="3fb6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">深度方向可分离卷积<br/> </strong>深度方向可分离卷积通过将标准卷积分成两个子部分<br/> i .深度方向卷积<br/> ii，大大降低了计算复杂度。逐点卷积。</p><p id="ea94" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第一部分是深度方向卷积，为每个输入通道独立执行空间卷积。其后是逐点卷积(即1×1卷积)，用于合并深度卷积的输出。</p><p id="93cf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们借助一个例子来理解这一点。假设我们有一个由3个通道组成的12×12的图像。我们希望对该输入应用5×5的卷积，得到8×8×256的输出。</p><p id="b84e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在第一部分，深度方向卷积，我们给输入图像一个不改变深度的卷积。我们通过使用3个形状为5×5×1的核来实现。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qn"><img src="../Images/4caaa4073b5ea62509a2c2d0bbbbbc5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZw5wCT30Blg0fiHpz9BHg.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated"><strong class="bd nk">第一部分:深度卷积</strong>【图片来源:<a class="ae nl" href="https://towardsdatascience.com/@reina.wang?source=post_page-----b99ec3102728----------------------" rel="noopener" target="_blank">齐-汪锋</a>经由<a class="ae nl" rel="noopener" target="_blank" href="/a-basic-introduction-to-separable-convolutions-b99ec3102728">走向数据科学</a></p></figure><p id="57f3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">逐点卷积之所以如此命名，是因为它使用1×1核或遍历每个单点的核。无论输入图像有多少通道，该内核都具有深度；在我们这里是3个。因此，我们通过我们的8×8×3图像迭代1×1×3核，得到8×8×1图像。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qo"><img src="../Images/02ee8a0fda7c088171f4422f812a42ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kEnW3ocIcj7Wgt7WwmkIJA.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated"><strong class="bd nk">第二部分:逐点卷积</strong>【图片来源:<a class="ae nl" href="https://towardsdatascience.com/@reina.wang?source=post_page-----b99ec3102728----------------------" rel="noopener" target="_blank">齐-汪锋</a>经由<a class="ae nl" rel="noopener" target="_blank" href="/a-basic-introduction-to-separable-convolutions-b99ec3102728">走向数据科学</a></p></figure><p id="7a84" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了获得8×8×256的输出图像，我们需要简单地将1×1×3核的数量增加到256。</p><p id="99d2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">编码器架构<br/> </strong> DeepLab V3+编码器使用<a class="ae nl" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank"> <em class="mt">例外架构</em> </a>进行以下修改—</p><ul class=""><li id="d18c" class="oo op it kt b ku kv kx ky la pc le pd li pe lm pg ou ov ow bi translated">我们在中间流程中添加更多的层</li><li id="64d0" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated">所有的最大池操作都被具有步长的深度可分卷积所取代</li><li id="b665" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated">在每个3×3深度方向卷积之后，添加额外的批标准化和ReLU。</li></ul><p id="86aa" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">编码器的输出是比输入特征图小16倍的特征图。这由解码器进行补偿，该解码器将编码器特征图上采样4倍两次(<a class="ae nl" href="#bf35" rel="noopener ugc nofollow">参考模型架构图</a>)。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qp"><img src="../Images/3fd7a92ecbeed38089748647d893aaa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kTipoUeual6G2GpR98Gfmg.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated"><strong class="bd nk"> DeepLab V3+编码器架构</strong>【图片来源:<a class="ae nl" href="https://arxiv.org/abs/1802.02611v3" rel="noopener ugc nofollow" target="_blank">原创研究论文</a></p></figure><p id="5433" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 5.2.2。训练<br/> </strong>我已经使用<a class="ae nl" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam" rel="noopener ugc nofollow" target="_blank"> Keras Adam optimizer </a>以47个时期的默认学习率训练了模型。优化器试图最小化的损失函数是<strong class="kt jd"> bce_dice_loss </strong>，在前面的<a class="ae nl" href="#fc4d" rel="noopener ugc nofollow">第4.4节</a>中定义</p><p id="6a3b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">与U-Net的情况一样，我保存了具有最佳验证dice_coefficient的模型的权重。</p><p id="d721" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">5.2.3。训练图</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qq"><img src="../Images/ad6104e9acb1976bad199474bf1f9876.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iJIcuEufWdJtPg0zWeimog.jpeg"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated"><strong class="bd nk">请注意，模型在第37个时期后开始过度拟合，验证分数没有进一步提高。因此，我使用了保存在第37纪元的模型权重。</strong></p></figure><p id="bfed" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 5.2.4。测试<br/> </strong>下图显示了来自验证数据的一些样本图像，以及它们的基本事实遮罩和预测遮罩。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qr"><img src="../Images/dcab725de05b3f76e68ac7e6e0aa3c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9iLVfOkhSfVciKRRPOGtUQ.png"/></div></div></figure><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qs"><img src="../Images/8b463b2e6f679499f6f33f595af2e7fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BfQJfgY_v9EY7uNbDyxxeA.png"/></div></div></figure><blockquote class="nn no np"><p id="fcc7" class="kr ks mt kt b ku kv kd kw kx ky kg kz nq lb lc ld nr lf lg lh ns lj lk ll lm im bi translated">在这种情况下，使用原始输入大小(256，1600，3)重建模型并加载按一半大小训练的模型的权重效果不佳。我不得不使用不同的策略——我使用训练好的模型在128×800的图像上进行预测，并将预测的遮罩大小调整为256×1600。这种方法在DeepLab V3+上运行得非常好。</p></blockquote><h2 id="46ce" class="od lx it bd ly oe of dn mc og oh dp mg la oi oj mi le ok ol mk li om on mm iz bi translated">结果比较和最终提交</h2><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi qt"><img src="../Images/da1f7e2db1cabb340f829f2b849843f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*QuKaHI7TIbW-A4YEJjg2aA.png"/></div></figure><p id="712a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我最后提交的是DeepLab V3+，无论是私信还是公信都给了一个像样的分数。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi qu"><img src="../Images/d2578ab89a1bc2132c569ad5494e2ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K70GlHnT4uEfNiYfy9rdIw.png"/></div></div></figure><h1 id="b75b" class="lw lx it bd ly lz ma mb mc md me mf mg ki mh kj mi kl mj km mk ko ml kp mm mn bi translated">现有方法和改进</h1><p id="61b5" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">这个Kaggle比赛很受欢迎，许多人用不同的方法解决了这个问题。然而，他们中的大多数都使用了U-Net的某种变体或类似的编码器-解码器架构。</p><p id="1a3e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我使用simple U-Net作为我的第一个cut解决方案，它在测试数据上给出了不错的性能，这要归功于半尺寸上的训练和全尺寸策略上的预测。</p><p id="d3aa" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我已经实现了DeepLab V3+，这是一种从零开始的语义图像分割技术。帮我把分数从0.809提高到0.838。</p><h1 id="d3c9" class="lw lx it bd ly lz ma mb mc md me mf mg ki mh kj mi kl mj km mk ko ml kp mm mn bi translated">未来的工作</h1><ul class=""><li id="9cd4" class="oo op it kt b ku mo kx mp la oq le or li os lm pg ou ov ow bi translated">一些超参数调整可以用U-Net来完成。</li><li id="e36f" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated">可以尝试其他图像分割架构，如U-Net++、SegNet和Mask R-CNN。</li><li id="4085" class="oo op it kt b ku ox kx oy la oz le pa li pb lm pg ou ov ow bi translated">可以利用在大数据集上训练的各种骨干的迁移学习的思想。</li></ul><h1 id="88dd" class="lw lx it bd ly lz ma mb mc md me mf mg ki mh kj mi kl mj km mk ko ml kp mm mn bi translated"><strong class="ak">结论</strong></h1><p id="3d7b" class="pw-post-body-paragraph kr ks it kt b ku mo kd kw kx mp kg kz la mq lc ld le mr lg lh li ms lk ll lm im bi translated">谢谢你看了这么长的博客，感谢你的耐心。我非常喜欢写它，希望你也喜欢读它。</p><p id="b83a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我跳过了大部分代码，因为我不想用代码淹没读者。请参考我的<a class="ae nl" href="https://github.com/aruns2120/Semantic-Segmentation-Severstal" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>获取完整的Keras代码。</p><p id="6659" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果您有任何疑问、建议或讨论，请随时在下面的评论区提出。我将尽我所知努力解决这些问题。<br/>你可以在LinkedIn上联系我，<a class="ae nl" href="https://www.linkedin.com/in/arunsingh314" rel="noopener ugc nofollow" target="_blank">这是我的简介</a>。</p><h1 id="8567" class="lw lx it bd ly lz ma mb mc md me mf mg ki mh kj mi kl mj km mk ko ml kp mm mn bi translated">参考</h1><ol class=""><li id="aa17" class="oo op it kt b ku mo kx mp la oq le or li os lm ot ou ov ow bi translated"><a class="ae nl" href="https://www.kaggle.com/c/severstal-steel-defect-detection/" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/severstal-steel-defect-detection/</a></li><li id="d52f" class="oo op it kt b ku ox kx oy la oz le pa li pb lm ot ou ov ow bi translated"><a class="ae nl" href="https://arxiv.org/abs/1505.04597v1" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1505.04597v1</a></li><li id="c9ba" class="oo op it kt b ku ox kx oy la oz le pa li pb lm ot ou ov ow bi translated"><a class="ae nl" href="https://arxiv.org/abs/1802.02611v3" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1802.02611v3</a></li><li id="d8cd" class="oo op it kt b ku ox kx oy la oz le pa li pb lm ot ou ov ow bi translated"><a class="ae nl" href="https://www.analyticsvidhya.com/blog/2019/02/tutorial-semantic-segmentation-google-deeplab/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2019/02/tutorial-semantic-segmentation-Google-deep lab/</a></li><li id="bc32" class="oo op it kt b ku ox kx oy la oz le pa li pb lm ot ou ov ow bi translated"><a class="ae nl" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1610.02357</a></li><li id="555b" class="oo op it kt b ku ox kx oy la oz le pa li pb lm ot ou ov ow bi translated"><a class="ae nl" href="https://github.com/MLearing/Keras-Deeplab-v3-plus/blob/master/model.py" rel="noopener ugc nofollow" target="_blank">https://github . com/MLearing/Keras-deep lab-v3-plus/blob/master/model . py</a></li><li id="707d" class="oo op it kt b ku ox kx oy la oz le pa li pb lm ot ou ov ow bi translated">https://www.appliedaicourse.com/<a class="ae nl" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank"/></li></ol></div></div>    
</body>
</html>