<html>
<head>
<title>A Brief Introduction to XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost 简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-brief-introduction-to-xgboost-3eaee2e3e5d6?source=collection_archive---------12-----------------------#2020-07-07">https://towardsdatascience.com/a-brief-introduction-to-xgboost-3eaee2e3e5d6?source=collection_archive---------12-----------------------#2020-07-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f789" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用 XGBoost 实现极限梯度提升！</h2></div><h2 id="625c" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">XGBoost 简介</h2><p id="e8b3" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">XGBoost 是一个优化的梯度增强机器学习库。它最初是用 C++编写的，但是有其他几种语言的 API。核心 XGBoost 算法是可并行化的，即它在单个树中进行并行化。使用 XGBoost 有一些缺点:</p><ol class=""><li id="ca2f" class="lu lv iq ld b le lw lh lx ko ly ks lz kw ma lt mb mc md me bi translated">它是最强大的算法之一，具有高速度和高性能。</li><li id="419f" class="lu lv iq ld b le mf lh mg ko mh ks mi kw mj lt mb mc md me bi translated">它可以利用现代多核计算机的所有处理能力。</li><li id="ccc5" class="lu lv iq ld b le mf lh mg ko mh ks mi kw mj lt mb mc md me bi translated">在大数据集上训练是可行的。</li><li id="b6a0" class="lu lv iq ld b le mf lh mg ko mh ks mi kw mj lt mb mc md me bi translated">始终优于所有单一算法方法。</li></ol><p id="09b9" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated">这里有一个简单的源代码来理解 XGBoost 的基础知识。</p><h2 id="e9d0" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">代码:</h2><p id="606d" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">步骤 1:导入所有必要的库，对于 xgboost 我们需要导入“XGBoost”库，然后使用 pandas 读取文件。</p><figure class="mn mo mp mq gt mr"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="4b17" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated">步骤 2:将整个数据集按照称为 X 的特征和称为 y 的目标向量分割成样本矩阵，如下所示:</p><figure class="mn mo mp mq gt mr"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="5df4" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated">第三步:分割数据集进行训练和测试，这里我把它分割成 80%训练和 20%测试，然后实例化 XGBClassifier(因为输出需要分类的形式，要么是 1，要么是 0)。它的一些超参数是“客观的”，它指定了所用算法的类型，这里我使用了“二元:逻辑”,这意味着二元分类的逻辑回归，输出概率和“n_estimators”，它调整了决策树的数量。</p><figure class="mn mo mp mq gt mr"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="60db" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated">第四步:拟合和预测模型，然后计算精度。</p><figure class="mn mo mp mq gt mr"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="89a5" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated">精确度:0.78333</p><h2 id="87ff" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">让我们更深入地了解这个超级强大的算法！</h2><p id="f6ee" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">XGBoost 通常使用树作为基本学习器，决策树由一系列二元问题组成，最终预测发生在叶子上。XGBoost 本身就是一个系综方法。迭代地构造树，直到满足停止标准。</p><p id="d6ac" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated">XGBoost 使用 CART(分类和回归树)决策树。CART 是在每个叶子中包含实值分数的树，不管它们是用于分类还是回归。如果有必要，实值分数可以被转换成用于分类的类别。</p><h1 id="1627" class="mu kg iq bd kh mv mw mx kk my mz na kn jw nb jx kr jz nc ka kv kc nd kd kz ne bi translated">XGBoost 中的模型评估</h1><p id="62a8" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">在这里，我们将看到模型评估过程与<em class="nf">交叉验证的过程。</em>那么，什么是交叉验证呢？</p><h2 id="9d78" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">交叉验证</h2><p id="4f11" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">交叉验证是一种稳健的方法，通过将许多非重叠的训练/测试拆分为训练数据并报告所有数据拆分的平均测试集性能，来估计模型在未知数据上的性能。</p><p id="8492" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated">下面是一个例子，</p><figure class="mn mo mp mq gt mr"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="0efa" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated"><em class="nf">下面是上面代码涉及的步骤:</em></p><p id="cb82" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated">第 2 行和第 3 行包括必要的导入。第 6 行包括加载数据集。第 9 行包括将数据集转换成 XGBoost 的创建者创建的优化的数据结构，该数据结构赋予包性能和效率增益，称为<strong class="ld ir"> <em class="nf"> DMatrix。</em> </strong>为了使用<em class="nf"> XGBoost cv </em>对象，这是 XGBoost 的学习 API 的一部分，我们必须首先明确地将我们的数据转换成一个<em class="nf"> DMatrix。</em></p><p id="7c41" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated">第 12 行包括创建一个参数字典来传递给交叉验证，这是必要的，因为 cv 方法不知道使用了哪种 XGBoost 模型。第 15 行包括调用 cv 方法，并传入存储所有数据的 DMatrix 对象(参数字典、交叉验证折叠数和需要构建的树数、要计算的度量、是否将输出存储为 pandas 数据帧)。第 18 行包括将公制转换为精确度，结果是<strong class="ld ir"> 0.88315。</strong></p><h1 id="c548" class="mu kg iq bd kh mv mw mx kk my mz na kn jw nb jx kr jz nc ka kv kc nd kd kz ne bi translated">XGBoost 与梯度升压</h1><p id="e4c5" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">XGBoost 是一种更加<strong class="ld ir">规则化的渐变增强形式</strong>。XGBoost 使用高级正则化(L1 &amp; L2)，这提高了模型泛化能力。</p><p id="36a9" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated">与梯度增强相比，XGBoost 提供了高性能。它的训练速度非常快，可以跨集群<strong class="ld ir">并行化</strong>。</p><h1 id="a330" class="mu kg iq bd kh mv mw mx kk my mz na kn jw nb jx kr jz nc ka kv kc nd kd kz ne bi translated">什么时候使用 XGBoost？</h1><ol class=""><li id="e706" class="lu lv iq ld b le lf lh li ko ng ks nh kw ni lt mb mc md me bi translated">当有大量训练样本时。理想情况下，多于 1000 个训练样本，少于 100 个特征，或者我们可以说特征的数量&lt; number of training samples.</li><li id="20dc" class="lu lv iq ld b le mf lh mg ko mh ks mi kw mj lt mb mc md me bi translated">When there is a mixture of categorical and numeric features or just numeric features.</li></ol><h1 id="cee4" class="mu kg iq bd kh mv mw mx kk my mz na kn jw nb jx kr jz nc ka kv kc nd kd kz ne bi translated">When not to use XGBoost?</h1><ol class=""><li id="9970" class="lu lv iq ld b le lf lh li ko ng ks nh kw ni lt mb mc md me bi translated">Image Recognition</li><li id="6885" class="lu lv iq ld b le mf lh mg ko mh ks mi kw mj lt mb mc md me bi translated">Computer Vision</li><li id="9ee7" class="lu lv iq ld b le mf lh mg ko mh ks mi kw mj lt mb mc md me bi translated">When the number of training samples is significantly smaller than the number of features.</li></ol><p id="6227" class="pw-post-body-paragraph lb lc iq ld b le lw jr lg lh lx ju lj ko mk ll lm ks ml lo lp kw mm lr ls lt ij bi translated"><em class="nf">这是关于 XGBoost 算法的简要信息。</em></p></div></div>    
</body>
</html>