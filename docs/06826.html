<html>
<head>
<title>Decision Trees: 6 key things to always remember</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树:永远记住的6件关键事情</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-6-important-things-to-always-remember-85636858da51?source=collection_archive---------56-----------------------#2020-05-27">https://towardsdatascience.com/decision-trees-6-important-things-to-always-remember-85636858da51?source=collection_archive---------56-----------------------#2020-05-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7b6d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们不需要深入算法背后的数学知识就能了解决策树。</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/0be57151986110398adad1821501878f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*20rDXEPBLWNlBGKBJpvYrg.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://unsplash.com/@leliejens?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">延斯·勒列</a>在<a class="ae kz" href="https://unsplash.com/s/photos/decision-trees?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="b902" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在本帖中，我们将介绍决策树背后的基本原理以及一个实际的实现。</p><p id="9a5d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">决策树是通用且强大的机器学习算法，可以执行分类和回归任务，甚至多输出任务。决策树也是随机森林的基本组件，是当今最强大的机器学习算法之一。</p><p id="8fe0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下面你会发现一个决策树的基本实现(如何训练模型并可视化其分裂),这是使用著名的IRIS数据集完成的。</p><pre class="kk kl km kn gt lw lx ly lz aw ma bi"><span id="8a2b" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu"># Importing the necessary libraries</strong><br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>from sklearn.datasets import load_iris<br/>from sklearn import tree<br/>from sklearn.tree import DecisionTreeClassifier</span><span id="e8d8" class="mb mc it lx b gy mh me l mf mg"><strong class="lx iu"># Load iris</strong><br/>iris=load_iris()<br/>X= iris.data[:,:2]<br/>y= iris.target</span><span id="f64d" class="mb mc it lx b gy mh me l mf mg"><strong class="lx iu"># Creating the decision tree object</strong><br/>decision_tree= DecisionTreeClassifier(max_depth=2)</span><span id="28b8" class="mb mc it lx b gy mh me l mf mg"><strong class="lx iu"># Training the Model</strong><br/>classifier=decision_tree.fit(X,y)</span></pre><h1 id="329f" class="mi mc it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">可视化决策树和拆分</h1><pre class="kk kl km kn gt lw lx ly lz aw ma bi"><span id="e4f1" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu"># Visualizing the decision tree</strong><br/>plt.figure(figsize=(15,10))<br/>tree.plot_tree(classifier, filled=True)</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/4717fd0a5804ec8bbbfaa8f867e20c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*MhlGKBNf7ms-YohZEVrITA.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">决策树及其分裂的可视化</p></figure><pre class="kk kl km kn gt lw lx ly lz aw ma bi"><span id="1a3e" class="mb mc it lx b gy md me l mf mg">print(tree.export_text(classifier))</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi na"><img src="../Images/96950b0d3cd4cbe1475665acf52dc2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*HPVkwZvli9iOcuOMhSmmwg.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">打印决策树拆分</p></figure><p id="37fc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Scikit-Learn使用CART(分类和回归树)来训练决策树，后者只产生二叉树:非叶节点总是有两个子节点。</p><p id="6156" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">其思想是决策树使用一个<em class="nb">单一特征</em> <strong class="lc iu"> <em class="nb"> k </em> </strong>和一个<em class="nb">阈值</em> <strong class="lc iu"> <em class="nb"> tk </em> </strong>(例如X[0] &lt; 5.45)将训练集分成两个子集。</p><p id="44c1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">如何选择<em class="nb"> k </em>和<em class="nb"> tk </em>？</strong></p><p id="b604" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">它通过寻找(k，t <em class="nb"> k </em>)的最佳对来产生最纯粹的子集(根据它们的大小加权)。</p><p id="63e9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">CART算法试图最小化的成本函数是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/9c33ad4cdfae2ad3be6edf828a37d647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*hl8ZeQfoUwH5YKnfWfVzqg.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">Cart训练算法(成本函数)“按作者分类的图像”</p></figure><p id="f530" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然而，其他算法，如ID3，可以生成具有两个以上子节点的决策树(Quinlan 1986)</p><blockquote class="nd ne nf"><p id="26d7" class="la lb nb lc b ld le ju lf lg lh jx li ng lk ll lm nh lo lp lq ni ls lt lu lv im bi translated">”还没等龚转发一个小故事来消消气，就已经是相当的焦头烂额了。我从一位教授那里学到了这个著名的算法(ID3 ),这让我着迷，而这位教授正是这个算法的发现者😊"</p></blockquote><h1 id="fdfa" class="mi mc it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated"><strong class="ak">计算复杂度</strong></h1><p id="e38d" class="pw-post-body-paragraph la lb it lc b ld nj ju lf lg nk jx li lj nl ll lm ln nm lp lq lr nn lt lu lv im bi translated">即使在处理较大的训练集时，预测也非常快，这是因为决策树需要从根到叶遍历整个树，因为它们通常是平衡的，遍历它们大约需要遍历:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi no"><img src="../Images/16c5a7781144f71b6036ab4a761d9cde.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*cOWY2zQj2jQu0MW5IbLwyw.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">(决策树的计算复杂性)“作者图片”</p></figure><h1 id="d854" class="mi mc it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated"><strong class="ak">熵还是基尼杂质？</strong></h1><p id="3fd3" class="pw-post-body-paragraph la lb it lc b ld nj ju lf lg nk jx li lj nl ll lm ln nm lp lq lr nn lt lu lv im bi translated">考虑到决策树及其家族的众多特性之一，它们需要很少的数据准备，它们根本不需要特征缩放。</p><p id="9d56" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然而，非常重要的是要注意这种差异。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3a3629bbb90757dc3eb46bf00e8dab1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*-A9N3HNePom4JW7stOD6aA.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">(基尼系数杂质和熵方程)“作者图片”</p></figure><p id="5074" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">大多数情况下，这并没有很大的区别，它们往往导致类似的树。但是，主要区别是:</p><ul class=""><li id="2388" class="nq nr it lc b ld le lg lh lj ns ln nt lr nu lv nv nw nx ny bi translated"><strong class="lc iu">基尼杂质</strong>计算起来略快(等式1)。然而，当它们不同时，基尼系数分离出它自己分支中最常见的类别</li><li id="ba33" class="nq nr it lc b ld nz lg oa lj ob ln oc lr od lv nv nw nx ny bi translated"><strong class="lc iu">熵</strong>(等式2)产生稍微更平衡的树。</li></ul><p id="f543" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在Scikit-learn中，默认情况下，模型是用基尼系数杂质设置的。</p><blockquote class="nd ne nf"><p id="eb37" class="la lb nb lc b ld le ju lf lg lh jx li ng lk ll lm nh lo lp lq ni ls lt lu lv im bi translated"><strong class="lc iu">决策树参数</strong></p><p id="318c" class="la lb nb lc b ld le ju lf lg lh jx li ng lk ll lm nh lo lp lq ni ls lt lu lv im bi translated">(CCP _阿尔法=0.0，类_重量=无，<strong class="lc iu">标准= '基尼'</strong>，<br/>最大深度=2，最大特征=无，最大叶节点=无，<br/>最小杂质_减少=0.0，最小杂质_分离=无，<br/>最小样本_叶=1，最小样本_分离=2，<br/>最小重量_分数_叶=0.0，预排序= '已弃用'，<br/>随机状态=无</p></blockquote><h1 id="e28e" class="mi mc it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated"><strong class="ak">正则化超参数</strong></h1><p id="a485" class="pw-post-body-paragraph la lb it lc b ld nj ju lf lg nk jx li lj nl ll lm ln nm lp lq lr nn lt lu lv im bi translated">对训练数据做了一些假设:假设数据是非线性的。在大多数情况下，如果决策树不受约束，树结构将非常适合数据集，过度适合数据，不能很好地推广到测试数据。</p><p id="198e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">作为一个非参数模型，它有很多参数。但是因为参数的数目在训练之前没有确定，所以模型可以自由地坚持训练数据。为了避免过度拟合训练数据，我们需要在训练过程中限制决策树的自由度(正则化)。</p><p id="399b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这可以通过限制决策树的最大深度数来实现，特别是在Scikit中，通过设置<strong class="lc iu"> <em class="nb"> max_depth超参数</em> </strong>(默认设置为none: Unlimited)来实现学习。减少后者将使模型规范化，并降低过度拟合的风险</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/5b8a8a0539bfef66dcb38dee581b88d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*ipEvPScwzdUY2XN1cyIS7w.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">来自:数据科学堆栈交换</p></figure><p id="39cb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">从上图中我们可以看到，拥有一个<strong class="lc iu"> <em class="nb"> max_depth为5 </em> </strong>，会产生一个方差很大的模型，而限制决策树的最大深度对减少模型的过拟合起着关键作用。当然，将它减少到两个对更好地规范模型有巨大的影响。</p><p id="c874" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">此外，决策树喜欢制作正交边界(所有分割都垂直于轴),如上图所示，这使它们对训练数据中的小变化非常敏感，并使模型非常不稳定。防止这种情况的一种方法是采用<strong class="lc iu">随机森林，</strong>后者可以通过对许多树进行平均来限制这种不稳定性，这个模型将在未来的帖子中详细分析。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi of"><img src="../Images/e9dc4472cf1217e003fb915b637915ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*5qrr2MKuqW61bCEP4cfSew.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">来源:Takashi J. OZAKI via KDnuggets</p></figure><h1 id="caf7" class="mi mc it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">该算法的6个关键要点:</h1><ul class=""><li id="9b23" class="nq nr it lc b ld nj lg nk lj og ln oh lr oi lv nv nw nx ny bi translated">易于解释(非参数模型)，功能强大，用途广泛。</li><li id="b289" class="nq nr it lc b ld nz lg oa lj ob ln oc lr od lv nv nw nx ny bi translated">使用少量训练数据进行有效学习，但是对训练数据的微小变化非常敏感。</li><li id="3b63" class="nq nr it lc b ld nz lg oa lj ob ln oc lr od lv nv nw nx ny bi translated">不需要功能缩放。</li><li id="2b3a" class="nq nr it lc b ld nz lg oa lj ob ln oc lr od lv nv nw nx ny bi translated">关于训练数据的假设很少(数据是非线性的)。</li><li id="ae50" class="nq nr it lc b ld nz lg oa lj ob ln oc lr od lv nv nw nx ny bi translated">很可能会对训练数据进行过度拟合(没有很好地推广)。</li><li id="26bc" class="nq nr it lc b ld nz lg oa lj ob ln oc lr od lv nv nw nx ny bi translated">喜欢正交决策边界。</li></ul><p id="f0f4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">希望这个简短的ML故事能提供信息！谢谢你</p></div></div>    
</body>
</html>