# 分类 ML 模型的 10 大模型性能指标

> 原文：<https://towardsdatascience.com/top-10-model-evaluation-metrics-for-classification-ml-models-a0a0f1d51b9?source=collection_archive---------5----------------------->

## 机器学习基础

## 解释非常规，这将作为评估分类机器学习模型的详尽列表。

![](img/d10350f643f39b4eb869daec78fa2d56.png)

# 介绍

在本帖中，我们将了解 10 个最重要的模型性能指标，这些指标可用于评估分类模型的模型性能。

以下是 10 个指标的列表，我们将通过示例以相互关联的方式进行研究:

1.  **混淆矩阵**
2.  **第一类错误**
3.  **第二类错误**
4.  **精度**
5.  **回忆或真阳性率或灵敏度**
6.  **精度**
7.  **特异性**
8.  **F1 得分**
9.  **ROC 曲线- AUC 评分**
10.  **PR 曲线**

一旦我们学会了正确的用法以及如何根据您的问题陈述来解释这些指标，那么评估分类模型的强度就变得轻而易举了。

让我们开始吧！

我们将使用一个数据集示例，该数据集具有用于训练逻辑回归模型的“是”和“否”标签。此用例可以是任何分类问题—垃圾邮件检测、癌症预测、流失率预测、活动目标预测等。在本帖中，我们将根据需要引用特殊用例。现在，我们将考虑一个简单的逻辑模型，它必须预测是或否

> 首先，逻辑模型可以给出两种输出:
> 
> 1.它给出类别标签作为输出值(是/否，1/0，恶性/良性，流失/保留，垃圾邮件/非垃圾邮件等)。)
> 
> 2.它给出 0 到 1 之间的概率值作为输出值，以表示某一事件对于特定观察的可能性。

类别标签场景可以进一步细分为平衡或不平衡数据集的情况，这两种情况都不能/不应该基于相似的度量来判断。一些指标更适合另一个，反之亦然。类似地，概率场景与类标签场景具有不同的模型性能度量。

下面是流程图，它是这篇文章的完美总结和完美序言，我们将在最后再次回顾这个流程图，以确保我们理解了所有的指标。

![](img/0d84f48842a12e46fb2b8c655a375290.png)

# 1.混淆矩阵

![](img/251b5831c7eee3d457fc2530cba60e38.png)

在构建任何统计或 ML 模型时，我们从开发数据集开始。将数据集分为两部分:训练和测试。将测试数据集放在一边，并使用训练数据集训练模型。一旦模型准备好进行预测，我们就尝试对测试数据集进行预测。一旦我们将结果分割成类似于上图所示的矩阵，我们就可以看到我们的模型能够正确预测多少，以及它的预测有多少是错误的。

我们用测试数据集中的数字填充下面的 4 个单元格(例如，有 1000 个观察值)。

![](img/205b71d7fe89e79ad6ad2d639797064e.png)

1.  **TP(真阳性):**在测试数据集中，该列的实际标签为“是”，而我们的逻辑回归模型也预测为“是”。(500 次观察)
2.  **TN(真阴性):**在测试数据集中，该列的实际标签为“否”，而我们的逻辑回归模型也预测为“否”。(200 项观察)
3.  **FP(假阳性):**在测试数据集中，该列的实际标签为“否”，但我们的逻辑回归模型预测为“是”。(100 项观察)
4.  **FN(假阴性):**在测试数据集中，该列的实际标签为“是”，但我们的逻辑回归模型预测为“否”。(200 项观察)

> 这 4 个单元格构成了“混淆矩阵”,因为在该矩阵中，通过清楚地描绘出我们模型的预测能力，可以减轻对我们模型的良好性的所有混淆。
> 
> 混淆矩阵是一个表格，通常用于**描述分类模型**(或“分类器”)对一组真实值已知的测试数据的性能。

# 2.第一类错误

![](img/04d2bf3326d4e3aa2c95d91e50a499c8.png)

> 第 1 类错误也称为假阳性，当分类模型错误地预测了最初错误观察的真实结果时就会发生。

例如:假设我们的逻辑模型正在处理垃圾邮件，而不是垃圾邮件。如果我们的模型将一封重要的电子邮件标记为垃圾邮件，那么这就是我们的模型的第一类错误的例子。在这个特定的问题陈述中，我们对尽可能减少 I 类错误非常敏感，因为重要的电子邮件进入垃圾邮件会产生严重的后果。

# 3.第二类错误

![](img/3d9f2005e79ace3552e58889a458f840.png)

> 第二类错误也称为假阴性，发生在分类模型错误地预测了原本真实的观察结果的错误结果时。

例如:假设我们的逻辑模型正在处理一个用例，它必须预测一个人是否患有癌症。如果我们的模型将一个患有癌症的人标记为健康人，并将其错误分类，那么这是我们模型的第二类错误的一个例子。在这个特定的问题陈述中，我们对尽可能减少 II 型错误非常敏感，因为在这种情况下，如果疾病在受影响的人中继续未被诊断，假阴性可能导致死亡。

# 4.准确(性)

现在，上面讨论的三个指标是通用指标，与您拥有的训练和测试数据的类型以及您为您的问题陈述部署的分类算法的类型无关。

我们现在将讨论非常适合特定类型数据的指标。

让我们从这里开始讨论准确性，这是一个最适合用于平衡数据集的指标。参考下图，该图来源于本媒体[文章](https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5)。

![](img/5aa5076c7086cbb27277f41c8a49ed0a.png)

来源:[链接](https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5)

如您所见，一个平衡的数据集是这样的:1 和 0、是和否、正和负由训练数据均等地表示。另一方面，如果两个类别标签的比率是倾斜的，那么我们的模型将偏向一个类别。

假设我们有一个平衡的数据集，让我们学习什么是准确性。

![](img/a0bf842d8d67d4b6d1aebdf82f493f2e.png)

> *精度*是测量结果与真实值的接近程度。它告诉我们，我们的分类模型能够多准确地预测问题陈述中给出的类别标签。

例如:假设我们的分类模型试图预测客户流失情况。在上图中，在总共 700 个实际流失客户(TP+FN)中，模型能够正确地对 500 个流失客户进行分类(TP)。同样，在总共 300 个保留客户(FP+TN)中，该模型能够正确地对 200 个保留客户(TN)进行分类。

> 准确率=(TP+TN)/客户总数

在上面的场景中，我们看到模型在 1000 个客户的测试数据集上的准确率是 70%。

现在，我们知道了准确性是一个应该只用于平衡数据集的指标。为什么会这样呢？让我们看一个例子来理解这一点。

![](img/7fb5fd49320c419dfffe811b8ee1e234.png)

在这个例子中，这个模型是在不平衡的数据集上训练的，甚至测试数据集也是不平衡的。准确性指标的得分为 72%，这可能给我们的印象是我们的模型在分类方面做得很好。但是，仔细看，这个模型在预测负面的阶级标签方面做得很糟糕。它只预测了 100 个总阴性标签观察中的 20 个正确结果。这就是为什么如果数据集不平衡，就不应该使用精度指标。

下一个问题是，如果你有一个不平衡的数据集，该怎么做呢？答案是召回率和精确度。下面我们来详细了解一下这些。

# 5.召回/灵敏度/ TPR

![](img/a42caedec272577c289989a553fda853.png)

> **回忆/敏感度/ TPR(真阳性率)**尝试回答以下问题:
> 
> ***正确识别实际阳性的比例是多少？***

![](img/91483c65444312fa9037347b030f5ae1.png)

来源:[维基百科](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)

这个指标给出了 78%的召回分数，如上图所示。召回通常用在真理检测至关重要的用例中。例如:癌症预测、股市分类等。在这里，问题陈述要求假阴性最小化，这意味着召回/灵敏度最大化。

# 6.精确

![](img/f366dfa0287f5ee900558f5dd0432a5b.png)

> **精密**试图回答以下问题:
> 
> ***实际上有多少比例的正面认同是正确的？***

![](img/32ef34ce2fe6a588961709f998d744de.png)

来源:[维基百科](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)

上图中显示的示例向我们展示了精度分数为 75%。精度通常用于最重要的是不要有大量误报的情况。例如:在垃圾邮件检测案例中，正如我们上面所讨论的，误报是指不是垃圾邮件但被我们的分类模型分类为垃圾邮件的观察结果。太多的误报会破坏垃圾邮件分类器模型的目的。因此，在这种情况下，Precision 在判断模型性能时非常方便。

# 7.特征

![](img/ab0147125c21c9bde9f25790db617e7a.png)

> **特异性**(也称为**真阴性率**)衡量被正确识别的实际阴性的比例。

![](img/76cc2d713088aca3e19d506d9e8a95b7.png)

来源:[维基百科](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)

基于我们用来理解精确度的同一个垃圾邮件检测分类器示例。特异性告诉我们，我们的模型能够准确地分类多少否定。在本例中，我们看到特异性=33%，这对于垃圾邮件检测模型来说不是一个好的分数，因为这意味着大多数非垃圾邮件被错误地分类为垃圾邮件。通过查看特异性度量，我们可以得出这个模型需要改进的结论。

# 8.F1 分数

我们分别在第 6 点和第 7 点讨论了召回率和精确度。我们知道，在一些问题陈述中，较高的召回率优先于较高的准确率，反之亦然。

但是在一些用例中，区别不是很明显，作为开发人员，我们希望同时重视召回率和精确度。在这种情况下，可以使用另一个指标——F1 分数。它依赖于精确度和召回率。

> 在二进制分类的统计分析中， **F1 得分**(也称为 **F 得分**或 **F 度量**)是测试准确度的一种度量。它同时考虑了测试的精度 *p* 和召回率 *r* 来计算分数

![](img/61e9cb0f25b878e07f57b7c23e9e2aee.png)

来源:[维基百科](https://en.wikipedia.org/wiki/F1_score)

在讨论最后两个指标之前，下面是维基百科上提供的一个很好的汇总表，涵盖了我们到目前为止在本文中讨论的所有指标。放大看看图像是否看起来不清楚。

![](img/b429cb62db48a415da0d2f4cb2363827.png)

来源:[维基百科](https://en.wikipedia.org/wiki/F1_score)

现在，我们在这篇文章的最后一站。到目前为止，我们已经讨论了预测类别标签的分类模型的模型性能度量。现在，让我们研究基于概率运行的模型的度量。

# 9.ROC 曲线- AUC 评分

曲线下面积(AUC)，受试者工作特征曲线(ROC)

这是用于衡量模型性能的最重要的指标之一，在数据科学家中广受欢迎。

让我们从一个例子开始理解这一点。我们有一个分类模型，给出 0-1 范围内的概率值来预测一个人肥胖与否的概率。接近 0 的概率值表示所考虑的人肥胖的概率非常低，而接近 1 的概率值表示人肥胖的概率非常高。现在，默认情况下，如果我们考虑 0.5 的阈值，那么所有概率≤0.5 的人将被归类为“不肥胖”，而概率> 0.5 的人将被归类为“肥胖”。但是，我们可以改变这个阈值。如果我把它变成 0.3 或者 0.9 呢。让我们看看会发生什么。

为了便于理解，我们在样本中选取了 10 个人。

> 为了绘制 ROC 曲线，我们必须在 x 轴上绘制(1-特异性)即假阳性率，在 y 轴上绘制灵敏度即真阳性率。

ROC ( *接收器操作特性*)曲线告诉我们该模型能够多好地区分两种情况(*例如* *患者是否肥胖*)。更好的模型可以准确区分两者。然而，一个差的模型将很难区分这两者。

我们将看到 4 种不同的场景，其中我们将选择不同的阈值，并将计算 ROC 曲线的相应 x 轴和 y 轴值。

![](img/ffdf12a7211852535cd7182689794bb8.png)

场景 1:阈值=0.9

![](img/de8408ebea9f339c634b77998c688b4c.png)

场景 2:阈值=0.6

![](img/7bc7ee2ee0218062e81068d8b4ffd1c2.png)

场景 3:阈值=0.3

![](img/facba54fe81f2df950277e0baa9eafde.png)

场景 4:阈值=0

现在，我们有 4 个数据点，在它们的帮助下，我们将绘制我们的 ROC 曲线，如下所示。

![](img/0dc98017953af86e1545edb23b337bc5.png)![](img/f41f24c8bc83331975975ab91eb70256.png)

因此，这就是如何为分类模型绘制 ROC 曲线，方法是分配不同的阈值以创建不同的数据点来生成 ROC 曲线。ROC 曲线下的面积称为 AUC。AUC 越多，你的模型就越好。你的 ROC 曲线离中间线性线越远，你的模型越好。这就是 ROC-AUC 如何帮助我们判断分类模型的性能，并为我们提供从许多分类模型中选择一个模型的方法。

# 10.PR 曲线

在数据主要位于负标签的情况下，ROC-AUC 将给出一个不能很好代表现实的结果，因为我们主要关注正比率方法，TPR 在 y 轴上，FPR 在 x 轴上。

例如，看看下面的例子:

![](img/36f92cbb86881de3e81f3120aac80626.png)

在这里，您可以看到大部分数据位于负标签下，ROC-AUC 不会捕获这些信息。在这种情况下，我们求助于 PR 曲线，它只不过是精确召回曲线。

在 PR 曲线中，我们将在 Y 轴上计算并绘制精度，在 X 轴上绘制召回，以查看我们的模型表现如何。

就是这样！我们已经到达这篇文章的结尾。我希望它有帮助。

你可以从我的个人资料中查看 ML 上的其他帖子。我发表的文章是关于特性缩放的，如果你喜欢的话，可以读一读。

[](/clearly-explained-what-why-and-how-of-feature-scaling-normalization-standardization-e9207042d971) [## 清楚地解释:什么，为什么和如何特征缩放-规范化和标准化

### 特征缩放的重要性以及如何应用它。我的机器学习模型会从规范化中受益吗？

towardsdatascience.com](/clearly-explained-what-why-and-how-of-feature-scaling-normalization-standardization-e9207042d971) 

请关注这个空间，了解更多关于机器学习、数据科学和统计学的内容！

快乐学习:)