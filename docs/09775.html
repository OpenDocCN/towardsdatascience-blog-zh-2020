<html>
<head>
<title>What is EDA? Yes, another post on EDA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EDA 是什么？是的，关于 EDA 的另一个帖子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-eda-yes-another-post-on-eda-d8b5c06269a9?source=collection_archive---------26-----------------------#2020-07-11">https://towardsdatascience.com/what-is-eda-yes-another-post-on-eda-d8b5c06269a9?source=collection_archive---------26-----------------------#2020-07-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bd7dad50afa1d0c27b9d04b045f47995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p-Rsob3HmkLmRs0g5fV-FQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">马库斯·斯皮斯克在<a class="ae jg" href="https://unsplash.com/s/photos/math?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="156c" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">通过 EDA 库、特征重要性、特征选择和特征提取，使您的数据美观易懂</h2></div><blockquote class="ky kz la"><p id="e016" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">包含所有相关代码的笔记本可以在<a class="ae jg" href="https://github.com/Christophe-pere/EDA" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p></blockquote><h1 id="56e8" class="ly lz jj bd ma mb mc md me mf mg mh mi kp mj kq mk ks ml kt mm kv mn kw mo mp bi translated">I —探索性数据分析或通常的 EDA</h1><p id="d9d6" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">是的，这是许多关于 EDA 主题的文章中的一篇新文章。这一步是数据科学项目中最重要的一步。为什么？因为它允许你获得关于你的数据、想法和直觉的知识，以便能够在以后对数据建模。</p><p id="053e" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">EDA 是让数据说话的艺术。能够控制它们的质量(丢失的数据、错误的类型、错误的内容……)。能够确定数据之间的相关性。能够知道基数。</p><p id="0004" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">EDA 不仅仅是探索数据。当你有一个目标，一列包含标签(监督学习)你也有特征选择和特征重要性。没有标签，你就有了特征提取(无监督学习)。</p><p id="b997" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">多年来，最好的方法是不厌其烦地编写相同的函数来计算相关性，绘制变量，手动浏览列来计算感兴趣的变量，等等…</p><p id="5acf" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">但是现在有更简单、更快速、更有效的方法来完成所有这些:</p><h2 id="dd3d" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">内务部。熊猫-侧写</h2><p id="5fae" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">首先，<a class="ae jg" href="https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/pages/introduction.html" rel="noopener ugc nofollow" target="_blank"> pandas-profiling </a>创建关于<em class="ld">数据帧</em>内容的报告，采用 HTML 格式，界面非常友好。基于 pandas，它允许以优异的性能(高达一百万行，这是作者推荐的最大值)对数据进行完整的探索。该报告可以通过<em class="ld"> jupyter lab </em>或<em class="ld">笔记本</em>中的小工具进行整合。或者，它也可以呈现为一个框架。</p><p id="2c88" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">正如作者指出的，您将获得以下信息:</p><blockquote class="ky kz la"><p id="5f86" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jk">类型推断</strong>:检测数据帧中列的类型。</p><p id="87c7" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jk">要素</strong>:类型、唯一值、缺失值</p><p id="038e" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jk">分位数统计</strong>如最小值、Q1、中值、Q3、最大值、范围、四分位间距</p><p id="1051" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jk">描述性统计</strong>如均值、众数、标准差、总和、中位数绝对偏差、变异系数、峰度、偏度</p><p id="b5d4" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jk">最频繁值</strong></p><p id="090e" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jk">直方图</strong></p><p id="0e60" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jk">相关性</strong>突出显示高度相关的变量、Spearman、Pearson 和 Kendall 矩阵</p><p id="e482" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jk">缺失值</strong>缺失值的矩阵、计数、热图和树状图</p><p id="fe93" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jk">重复行</strong>列出最常出现的重复行</p><p id="486b" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jk">文本分析</strong>了解文本数据的类别(大写、空格)、脚本(拉丁文、西里尔文)和块(ASCII)</p><p id="87e2" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">资料来源:<a class="ae jg" href="https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/pages/introduction.html" rel="noopener ugc nofollow" target="_blank">熊猫简介</a></p></blockquote><p id="6789" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">您可以在库的<strong class="le jk"> <em class="ld"> GitHub </em> </strong>页面上找到示例，例如:</p><ul class=""><li id="49b3" class="nk nl jj le b lf lg li lj ms nm mu nn mw no lx np nq nr ns bi translated"><a class="ae jg" href="https://pandas-profiling.github.io/pandas-profiling/examples/master/meteorites/meteorites_report.html" rel="noopener ugc nofollow" target="_blank"> NASA 陨石降落</a>这个报告是<em class="ld"> profil_report() </em>函数的输出，它显示了这个库是多么强大。</li></ul><p id="6b28" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">怎么用？让我用几行代码向您展示:</p><p id="8772" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">与硬编码的东西相比，它需要几秒钟来计算，并得到令人印象深刻的结果。</p><p id="8e29" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">在小部件中显示报告时的结果:</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/fa48597b72145e4fd4d132551be8826e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BdW-PIF8PlLUB0njavhjxA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">pandas-widget 中的配置文件报告(渲染)</p></figure><p id="ec78" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">当您在笔记本内部的框架中显示报告时，结果是:</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/c0655f6fcdf8ce6bd41a14e124481acc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QgXO0ueoUd-fCfVT5bo51w.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">熊猫-在框架中分析 HTML(渲染)</p></figure><h2 id="514d" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">Ib。Dataprep.eda</h2><p id="5214" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">另一个很棒的库是<strong class="le jk"> <em class="ld"> dataprep </em> </strong>带模块<strong class="le jk"> <em class="ld"> eda </em> </strong>。它在做什么？</p><p id="3f76" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">您有三个主要功能:</p><ul class=""><li id="9e2f" class="nk nl jj le b lf lg li lj ms nm mu nn mw no lx np nq nr ns bi translated"><strong class="le jk">剧情</strong></li></ul><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="dcb2" class="my lz jj oa b gy oe of l og oh">plot(df)</span></pre><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/100ef24385eff3fc24f28f86ee84106d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1DI3azqH2ENkiircwvWmAQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">波士顿房价数据集上 dataprep.eda 包的绘图函数</p></figure><p id="bd33" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">该函数将显示每个特征的直方图。每个情节都基于<strong class="le jk">散景</strong>库进行交互。您可以使用不同的参数来显示所需数据的信息。</p><ul class=""><li id="a055" class="nk nl jj le b lf lg li lj ms nm mu nn mw no lx np nq nr ns bi translated"><strong class="le jk"> plot_correlation </strong></li></ul><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="a796" class="my lz jj oa b gy oe of l og oh">plot_correlation(df)</span></pre><p id="acae" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">该函数允许您计算三种相关矩阵(Pearson、Spearman 和 KendallTau)。其优点是该图也是交互式的，只需将光标放在上面就可以看到数值。</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/ec3a3eaa72e6b174201c42da4611ef9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*ZqLi8cHnCYnPf0XtgDsyHg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">波士顿房价数据的 plot_correlation</p></figure><ul class=""><li id="a365" class="nk nl jj le b lf lg li lj ms nm mu nn mw no lx np nq nr ns bi translated"><strong class="le jk">剧情 _ 失踪</strong></li></ul><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="7c89" class="my lz jj oa b gy oe of l og oh">plot_missing(df)</span></pre><p id="7add" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">最后一个函数非常有趣，如下图所示。它允许您可视化列中缺失数据的位置及其百分比。</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/ccd92a4c60f4ada163b9156c4b323e29.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*6qEJdtUj4v_ITqSEHlW05Q.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">plot_missing 函数</p></figure><h2 id="9769" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">Ic。Sweetviz</h2><p id="9cfb" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">最后一个有趣的库是<a class="ae jg" href="https://github.com/fbdesignpro/sweetviz" rel="noopener ugc nofollow" target="_blank"> sweetviz </a>。基于 pandas-profiling，该库允许比较不同列或训练和测试数据部分，以确定测试集是否代表训练。就像熊猫简介一样，每一栏都有大量的信息。下图是库生成的 HTML 报表的仪表盘。</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/cb63a2483235ed3f5f254203b4e59133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0JzgLNzOUvAYTIAbsulRPA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">使用 Sweetviz 进行训练和测试的比较</p></figure><h1 id="7def" class="ly lz jj bd ma mb mc md me mf mg mh mi kp mj kq mk ks ml kt mm kv mn kw mo mp bi translated">II —特征选择</h1><p id="907c" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">EDA 不仅仅关注数据内部的内容。还可以通过以下几个部分进行更深入的分析。要素选择是减少数据集中要素数量的一种方式。</p><p id="d2c5" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">在这里，我只提出三种方法。<a class="ae jg" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> sklearn </a>库拥有强大的模块来做你想要的选择或提取数据的工作。</p><h2 id="fdef" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">IIa。移除低方差特征</h2><p id="abc3" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">这种技术只允许我们选择方差低于阈值的特征。默认情况下，它会移除所有零方差要素，即在所有样本中具有相同值的要素。在下面的代码中，丢失 80%以上数据的列将被自动删除。</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="5039" class="my lz jj oa b gy oe of l og oh">from sklearn.feature_selection import VarianceThreshold<br/>threshold = 0.8 <em class="ld"># 80% of low variance</em></span><span id="5bbe" class="my lz jj oa b gy om of l og oh">fe_low_variance = VarianceThreshold(threshold=(threshold * (1 - threshold)))<br/>X_variance = fe_low_variance.fit_transform(X)</span></pre><h2 id="1420" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">联合调查局。单变量选择</h2><p id="c460" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">在监督学习中，你有一个目标特征(俗称<strong class="le jk"> <em class="ld"> y </em> </strong>)。单变量选择的目标很简单:选取一个特征，对其进行变异，并观察这如何影响目标。最后，单变量选择将基于单变量统计测试选择最佳特征。</p><p id="e3f1" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">有了 sklearn，你有 4 种方法可以做到。</p><ul class=""><li id="d41e" class="nk nl jj le b lf lg li lj ms nm mu nn mw no lx np nq nr ns bi translated"><a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" rel="noopener ugc nofollow" target="_blank"><strong class="le jk">【select kbest</strong></a>:这将选择数据集的最佳 k(由用户手动选择)特征，并移除其他特征。这个函数需要一个计分器，一个度量函数来应用选择。常用的计分器功能有<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2" rel="noopener ugc nofollow" target="_blank"><strong class="le jk"><em class="ld">chi2</em></strong></a>。</li><li id="7449" class="nk nl jj le b lf on li oo ms op mu oq mw or lx np nq nr ns bi translated"><a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile" rel="noopener ugc nofollow" target="_blank"><strong class="le jk">select percentile</strong></a>:与<em class="ld"> SelectKBest </em>相同你需要传递一个计分器，但是你传递的不是一个<strong class="le jk"> <em class="ld"> k </em> </strong>数量的特征，而是一个百分位值。</li><li id="07e1" class="nk nl jj le b lf on li oo ms op mu oq mw or lx np nq nr ns bi translated"><a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html#sklearn.feature_selection.SelectFpr" rel="noopener ugc nofollow" target="_blank"><strong class="le jk">SelectFpr</strong></a><strong class="le jk">/</strong><a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr.html#sklearn.feature_selection.SelectFdr" rel="noopener ugc nofollow" target="_blank"><strong class="le jk">SelectFdr</strong></a><strong class="le jk">/</strong><a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html#sklearn.feature_selection.SelectFwe" rel="noopener ugc nofollow" target="_blank"><strong class="le jk">SelectFwe</strong></a>:由<strong class="le jk"> <em class="ld"> pvalues </em> </strong>根据误报率、误发现率和族错误进行选择。</li><li id="0d36" class="nk nl jj le b lf on li oo ms op mu oq mw or lx np nq nr ns bi translated"><a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html#sklearn.feature_selection.GenericUnivariateSelect" rel="noopener ugc nofollow" target="_blank"><strong class="le jk">GenericUnivariateSelect</strong></a>:在这里您可以用可配置的策略定制您的估计器。</li></ul><p id="f2ff" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">在下面的代码中我使用了<strong class="le jk"><em class="ld">SelecKBest</em></strong>with<strong class="le jk"><em class="ld">chi2</em></strong>scorer:</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="4681" class="my lz jj oa b gy oe of l og oh">from sklearn.feature_selection import SelectKBest<br/>from sklearn.feature_selection import chi2</span><span id="8e77" class="my lz jj oa b gy om of l og oh">#apply SelectKBest class to extract top 10 best features<br/>select_best_features = SelectKBest(score_func=chi2, k=10) <em class="ld"># where k is the number of features you want</em><br/>fit = select_best_features.fit(X,y)<br/>df_scores = pd.DataFrame(fit.scores_)<br/>df_columns = pd.DataFrame(X.columns) <em class="ld"># where X is your data</em><br/>#concat two dataframes for better visualization <br/>feature_scores = pd.concat([df_columns,df_scores],axis=1)<br/>feature_scores.columns = ['Specs','Score']  #naming the dataframe columns<br/>print(feature_scores.nlargest(10,'Score'))  #print 10 best features</span></pre><h2 id="05d3" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">IIc。递归特征消除</h2><p id="b6b9" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">如下图所示，RFE 的原理很简单。估计器拟合数据并计算特征重要性(目标上数据的权重)。在每次迭代中，模型将删除重要性较低的特征，直到达到所需的<strong class="le jk"> <em class="ld"> k </em> </strong>个特征的数量。</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/f163e96b2fab0e9dec63ea87f48ca04d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AWnrEWagQiol5hUeqe52kw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">RFE 的图式</p></figure><p id="c27a" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">我们该如何编码呢？我在这里展示了<strong class="le jk"> <em class="ld">、SVM </em> </strong>和<strong class="le jk"> <em class="ld">逻辑回归</em> </strong>的一个实现。</p><p id="6f4c" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">SVM: </p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="eb42" class="my lz jj oa b gy oe of l og oh">from sklearn.svm import SVC<br/>from sklearn.model_selection import StratifiedKFold<br/>from sklearn.feature_selection import RFECV</span><span id="eff9" class="my lz jj oa b gy om of l og oh"><em class="ld"># SVM implementation</em><br/>svc = SVC(kernel="linear")<br/><em class="ld"># The "accuracy" scoring is proportional to the number of correct</em><br/>    <br/>rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(5),<br/>                  scoring='accuracy')<br/>rfecv.fit(X, y)</span><span id="a8ae" class="my lz jj oa b gy om of l og oh">print("Optimal number of features : %d" % rfecv.n_features_)</span></pre><p id="17f7" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated"><strong class="le jk">逻辑回归:</strong></p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="bd78" class="my lz jj oa b gy oe of l og oh"><em class="ld"># Feature Extraction with RFE</em><br/>from sklearn.feature_selection import RFE<br/>from sklearn.linear_model import LogisticRegression</span><span id="0a67" class="my lz jj oa b gy om of l og oh">model = LogisticRegression(solver='lbfgs', max_iter=5000)<br/>rfe = RFE(model, 3)<br/>fit = rfe.fit(X, Y)<br/>print("Num Features: %d" % fit.n_features_)<br/>print("Selected Features: %s" % fit.support_)<br/>print("Feature Ranking: %s" % fit.ranking_)</span></pre><h2 id="2b54" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">IId。SelectFromModel</h2><p id="0142" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">最后一种方法是前一种方法的推广，因为<strong class="le jk"><em class="ld">SelectFromModel</em></strong>采用<strong class="le jk"> <em class="ld">估计器</em> </strong>并返回一个包含缩减维度的新矩阵。</p><p id="2bf9" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">下面的代码显示了如何实现它:</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="00e8" class="my lz jj oa b gy oe of l og oh">from sklearn.svm import LinearSVC<br/>from sklearn.feature_selection import SelectFromModel</span><span id="de43" class="my lz jj oa b gy om of l og oh">lsvc = LinearSVC(C=0.01, penalty="l1", dual=False) <em class="ld"># estimator</em><br/>lsvc.fit(X, y)<br/>model = SelectFromModel(lsvc, prefit=True)<br/>X_new = model.transform(X)<br/>print(f"The new number of feature is {X_new.shape[1]}")</span></pre><h1 id="006d" class="ly lz jj bd ma mb mc md me mf mg mh mi kp mj kq mk ks ml kt mm kv mn kw mo mp bi translated">III —特征提取</h1><h2 id="dbf1" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">IIIa。主成分分析</h2><p id="8f4d" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">主成分分析是一种用于降低数据集维度的方法。原理很简单，PCA 会用一条线或一个平面来拟合这些点，以创建数据的另一种表示。</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/0770d49056bb024fa15070c3559591b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lh5TKadyKILDVlaazuE1UA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">主成分分析投影</p></figure><p id="51f4" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">代码使用起来很简单。你只需要指定 N_var 参数，它代表你想要的维数。</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="5730" class="my lz jj oa b gy oe of l og oh">from sklearn.decomposition import PCA<br/>N_var = 2<br/>pca = PCA(n_components=N_var)<br/>X_pca = pca.fit_transform(X)<br/>df_pca = pd.DataFrame(data = X_pca, columns = ['PC1', 'PC2'])</span></pre><h2 id="26f2" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">IIIb。独立成分分析</h2><p id="012f" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">ICA 是分离线性混合的多变量独立信号的一种强有力的技术。这项技术允许我们在信号处理中分离不同的信号。</p><p id="9162" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">下面的代码显示了一个<strong class="le jk"> <em class="ld"> FastICA </em> </strong>的实现:</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="6f8d" class="my lz jj oa b gy oe of l og oh">from sklearn.decomposition import FastICA<br/>N_var = 2<br/>ica = FastICA(n_components=N_var)<br/>X_ica = ica.fit_transform(X)</span></pre><h2 id="8189" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">IIIc。线性判别分析(LDA)</h2><blockquote class="ky kz la"><p id="e2d0" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">LDA […]是对<strong class="le jk"> Fisher 线性判别式</strong>的推广，这是一种在<a class="ae jg" href="https://en.wikipedia.org/wiki/Statistics" rel="noopener ugc nofollow" target="_blank">统计</a>、<a class="ae jg" href="https://en.wikipedia.org/wiki/Pattern_recognition" rel="noopener ugc nofollow" target="_blank">模式识别</a>和<a class="ae jg" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>中使用的方法，用于寻找<a class="ae jg" href="https://en.wikipedia.org/wiki/Features_(pattern_recognition)" rel="noopener ugc nofollow" target="_blank">特征</a>的<a class="ae jg" href="https://en.wikipedia.org/wiki/Linear_combination" rel="noopener ugc nofollow" target="_blank">线性组合</a>，以表征或分离两类或更多类的对象或事件。得到的组合可以用作<a class="ae jg" href="https://en.wikipedia.org/wiki/Linear_classifier" rel="noopener ugc nofollow" target="_blank">线性分类器</a>，或者更常见的是，在稍后的<a class="ae jg" href="https://en.wikipedia.org/wiki/Statistical_classification" rel="noopener ugc nofollow" target="_blank">分类</a>之前用于<a class="ae jg" href="https://en.wikipedia.org/wiki/Dimensionality_reduction" rel="noopener ugc nofollow" target="_blank">维度缩减</a>。<a class="ae jg" href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" rel="noopener ugc nofollow" target="_blank">(来源:维基百科)</a></p></blockquote><p id="f049" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">与 sklearn 一起使用的简单方法:</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="0d76" class="my lz jj oa b gy oe of l og oh">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br/>N_var = 2<br/>lda = LinearDiscriminantAnalysis(n_components=N_var)</span><span id="a62c" class="my lz jj oa b gy om of l og oh"><em class="ld"># run an LDA and use it to transform the features</em><br/>X_lda = lda.fit(X, y).transform(X)</span></pre><h2 id="bb8e" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">IIId。局部线性嵌入(LLE)</h2><p id="bb53" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">像所有以前的特征提取方法一样，LLE 是一种无监督的降维技术(n 维到 k 维，其中 k 是确定的)。目标是保留原始非线性特征结构的几何特征。LLE 是基于<strong class="le jk"> <em class="ld"> k 近邻</em> </strong> (k-NN)技术。该算法将计算聚类，估计每个聚类的中心，并将其映射为线性加权表示。因此，聚类中包含的点在这个向量中是近似的，它最好地再现了聚类。</p><p id="be87" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">实施:</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="bf76" class="my lz jj oa b gy oe of l og oh">from sklearn.manifold import locally_linear_embedding<br/>N_var = 2<br/>lle, error = locally_linear_embedding(X, n_neighbors=5, n_components=N_var, random_state=42, n_jobs=-1)</span></pre><h2 id="f33a" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">IIIe。t 分布随机邻居嵌入(t-SNE)</h2><blockquote class="ky kz la"><p id="baa1" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jk">T-分布式随机邻居嵌入(t-SNE) </strong>是由<a class="ae jg" href="https://en.wikipedia.org/w/index.php?title=Laurens_van_der_Maaten&amp;action=edit&amp;redlink=1" rel="noopener ugc nofollow" target="_blank"> Laurens van der Maaten </a>和<a class="ae jg" href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" rel="noopener ugc nofollow" target="_blank"> Geoffrey Hinton </a>开发的用于<a class="ae jg" href="https://en.wikipedia.org/wiki/Data_visualization" rel="noopener ugc nofollow" target="_blank">可视化</a>的<a class="ae jg" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>算法。<a class="ae jg" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding#cite_note-MaatenHinton-1" rel="noopener ugc nofollow" target="_blank">【2】</a>这是一种<a class="ae jg" href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction" rel="noopener ugc nofollow" target="_blank">非线性降维</a>技术，非常适合在二维或三维的低维空间中嵌入用于可视化的高维数据。具体而言，它通过二维或三维点对每个高维对象进行建模，以这种方式，相似的对象通过附近的点进行建模，而不相似的对象通过远处的点以高概率进行建模。(<a class="ae jg" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding#cite_note-MaatenHinton-1" rel="noopener ugc nofollow" target="_blank">来源:维基百科</a>)</p></blockquote><p id="b653" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated"><strong class="le jk"> <em class="ld"> t-SNE </em> </strong>通常被比作<strong class="le jk"> <em class="ld"> PCA </em> </strong>因为用<strong class="le jk"> <em class="ld"> t-SNE </em> </strong>比<strong class="le jk"> <em class="ld"> PCA </em> </strong>对数据的可视化表示更好。<strong class="le jk"> <em class="ld"> t-SNE </em> </strong>比<strong class="le jk"> <em class="ld"> PCA </em> </strong>更精确地分离尺寸。</p><p id="4812" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">在<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank"> sklearn </a>中提供了一个简单的实现:</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="c6c2" class="my lz jj oa b gy oe of l og oh">from sklearn.manifold import TSNE<br/>N_var = 2<br/>X_embedded = TSNE(n_components=N_var).fit_transform(X)</span></pre><h1 id="a888" class="ly lz jj bd ma mb mc md me mf mg mh mi kp mj kq mk ks ml kt mm kv mn kw mo mp bi translated">VI —特征重要性</h1><h2 id="5b7f" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">VIa。树形方法</h2><p id="4c0a" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">树算法中的每个节点都是表示数据的一种方式。节点允许将要素的值(概率)分割成单个值。这种表示导致要素权重的确定(您想要预测的要素)。因此该算法可以计算目标上每个特征的重要性。您可以使用树模型的<strong class="le jk"><em class="ld">feature _ importance _</em></strong>参数来显示每个特征的值和标准差。</p><p id="d310" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">这里我提供了两种使用基于树的方法来计算这个特性重要性的方法。第一个使用<strong class="le jk"><em class="ld"/></strong>(我过去经常使用它来确定我的特征的重要性)和<strong class="le jk"> <em class="ld"> RandomForest </em> </strong>。</p><p id="9a19" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated"><strong class="le jk"><em class="ld"/></strong></p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="7625" class="my lz jj oa b gy oe of l og oh">from sklearn.ensemble import ExtraTreesClassifier<br/>forest = ExtraTreesClassifier(n_estimators=250,<br/>                                  random_state=0)</span><span id="f62e" class="my lz jj oa b gy om of l og oh">forest.fit(X, y)<br/>importances = forest.feature_importances_<br/>std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)<br/>indices = np.argsort(importances)[::-1]</span><span id="d686" class="my lz jj oa b gy om of l og oh"><em class="ld"># Print the feature ranking</em><br/>print("Feature ranking:")</span><span id="176c" class="my lz jj oa b gy om of l og oh">for f in range(X.shape[1]):<br/>    print("%d. feature %d (%f)" % (f + 1, indices[f],   importances[indices[f]]))</span></pre><p id="0aff" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated"><strong class="le jk"><em class="ld">RandomForest:</em></strong></p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="a2a3" class="my lz jj oa b gy oe of l og oh">from sklearn.ensemble import RandomForestRegressor</span><span id="b743" class="my lz jj oa b gy om of l og oh">rf = RandomForestRegressor(n_estimators = 100,<br/>                               n_jobs = -1,<br/>                               oob_score = True,<br/>                               bootstrap = True,<br/>                               random_state = 42)<br/>rf.fit(X, y)</span><span id="b954" class="my lz jj oa b gy om of l og oh">print('R^2 Training Score: {:.2f} \nOOB Score: {:.2f} '.format(rf.score(X, y), rf.oob_score_,))</span><span id="1a3d" class="my lz jj oa b gy om of l og oh">results = pd.DataFrame(data=rf.feature_importances_, index=X.columns)<br/>results.columns = ["Importance"]<br/>results.sort_values(by=["Importance"], ascending=False)<br/>importances = rf.feature_importances_</span></pre><h2 id="5829" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">VIb。排列方法</h2><p id="e005" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">在这篇文章中，你将看到的最后一件事是排列方法。目标很简单。估计器将采用与目标相关的特征，并将其混洗以测量其与目标的关联程度。它决定了估计器如何依赖于该特征。</p><blockquote class="ky kz la"><p id="a054" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这种技术受益于模型不可知，并且可以用特征的不同排列进行多次计算。(<a class="ae jg" href="https://scikit-learn.org/stable/modules/permutation_importance.html" rel="noopener ugc nofollow" target="_blank">来源:sklearn </a>)</p></blockquote><p id="1a08" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">这里，我提供了使用置换方法的三个实现。第一个包由<a class="ae jg" href="https://github.com/parrt/random-forest-importances" rel="noopener ugc nofollow" target="_blank">RF pipe</a>提供:</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="febf" class="my lz jj oa b gy oe of l og oh">from sklearn.metrics import r2_score<br/>from rfpimp import permutation_importances</span><span id="c568" class="my lz jj oa b gy om of l og oh">def r2(rf, X_train, y_train):<br/>    return r2_score(y_train, rf.predict(X_train))</span><span id="362d" class="my lz jj oa b gy om of l og oh">perm_imp_rfpimp = permutation_importances(rf, X, y, r2)<br/>importances = perm_imp_rfpimp.Importance</span></pre><p id="dcd9" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated">eli5 库提供了一个版本的<a class="ae jg" href="https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html" rel="noopener ugc nofollow" target="_blank">置换重要性</a>:</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="5c72" class="my lz jj oa b gy oe of l og oh">import eli5<br/>from eli5.sklearn import PermutationImportance</span><span id="fa67" class="my lz jj oa b gy om of l og oh">perm = PermutationImportance(rf, cv = None, refit = False, n_iter = 50).fit(X, y)<br/>results = pd.DataFrame(data= perm.feature_importances_, index=X.columns)<br/>results.columns = ["Importance"]<br/>results.sort_values(by=["Importance"], ascending=False)<br/>importances = perm.feature_importances_</span></pre><p id="cac3" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ms lm ln lo mu lq lr ls mw lu lv lw lx im bi translated"><a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance" rel="noopener ugc nofollow" target="_blank">置换 _ 重要性</a>通过基于 R2 估计的 sklearn:</p><pre class="nu nv nw nx gt nz oa ob oc aw od bi"><span id="0ca3" class="my lz jj oa b gy oe of l og oh">from sklearn.linear_model import Ridge<br/>from sklearn.inspection import permutation_importance<br/>    <br/>X_train, X_val, y_train, y_val = train_test_split(<br/>         X, y, random_state=42)<br/>    <br/>model = Ridge(alpha=1e-2).fit(X_train, y_train)<br/>    <br/>r = permutation_importance(model, X_val, y_val,<br/>                               n_repeats=30, random_state=42)<br/>    <br/>for i in r.importances_mean.argsort()[::-1]:<br/>    if r.importances_mean[i] - 2 * r.importances_std[i] &gt; 0:<br/>        print(f"{r.importances_mean[i]:.3f}"<br/>                   f" +/- {r.importances_std[i]:.3f}")</span></pre><h1 id="688f" class="ly lz jj bd ma mb mc md me mf mg mh mi kp mj kq mk ks ml kt mm kv mn kw mo mp bi translated">结论</h1><p id="e454" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">嗨，你终于做到了。你已经到达这篇关于 EDA 和其他技术的文章的末尾。你知道如何用不同的库做 EDA。您知道选择数据集最佳要素的不同方法、降维以及要素重要性。现在，您已经准备好深入探索您的数据，并以可视化的方式表示它。我希望这篇文章和相关的笔记本能帮助你。</p><h1 id="b5f3" class="ly lz jj bd ma mb mc md me mf mg mh mi kp mj kq mk ks ml kt mm kv mn kw mo mp bi translated">参考</h1><p id="47bc" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">[2]范德马腾，法学家委员会；Hinton，G.E. <a class="ae jg" href="http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" rel="noopener ugc nofollow" target="_blank">“使用 t-SNE 可视化数据”</a>(2008 年 11 月)。<em class="ld">机器学习研究杂志</em>。9:2579–2605。</p><h2 id="76a3" class="my lz jj bd ma mz na dn me nb nc dp mi ms nd ne mk mu nf ng mm mw nh ni mo nj bi translated">注意</h2><p id="5c9c" class="pw-post-body-paragraph lb lc jj le b lf mq kk lh li mr kn lk ms mt ln lo mu mv lr ls mw mx lv lw lx im bi translated">本文中提供的图片由作者生成或由他绘制。</p></div></div>    
</body>
</html>