<html>
<head>
<title>Explainable AI Needs More Humans</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的人工智能需要更多的人</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-ai-needs-more-humans-64e4c239e412?source=collection_archive---------34-----------------------#2020-02-24">https://towardsdatascience.com/explainable-ai-needs-more-humans-64e4c239e412?source=collection_archive---------34-----------------------#2020-02-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f20ba52f13389936510b99b7423c2387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5vupqNXbO1rWXUPmLki2kA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">资料来源:Petr Kratochvil</p></figure><div class=""/><div class=""><h2 id="3fe3" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">在 LIME、Shap 等技术术语中，你可能会忘记目标是向一个人解释一些事情。</h2></div><p id="cde9" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">对于许多人来说，可解释或可解释的 AI 或 ML 意味着在一套旧算法的基础上叠加一套新算法，以更好地理解旧算法的输出。因此，围绕可解释 ML 的讨论有时会围绕着 LIME 或 Shapley 值，而不会涉及到任何可能会使用模型输出的人。</p><p id="6f15" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在创建新算法的热潮中，模型用户想知道算法如何做出决策的问题被遗忘了。我们甚至没有开始讨论最终用户是否是一种人，或者实际上是一群不同类型的人，他们可能有不同的需求。也就是说，对理解一组新算法的关注分散了对模型用户需求的理解。</p><p id="66f6" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这可能是有问题的，因为如果用户不理解问题域内潜在变量的意义或重要性，简单地知道哪些变量在特定方向上导致 x 单位效应直观上不是有用的解释。</p><p id="c156" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">回到不同类型用户的问题上，这就引出了一个问题:我们是否应该讨论用一种解释来制作一个模型，或者，我们是否真的需要多个模型来处理同一件事情，因为不同的用户群需要不同类型的解释？例如，当思考医疗 AI 时，对医疗专业人员的解释有可能被患者理解吗？或者，一个被病人理解的解释可能对医学专业人员有用吗？例如，它会有足够的细节吗？</p><p id="c79e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">Leo Breiman 的罗生门集(一组不同的模型，有不同的解释)表明，在某些时候，制作这种情景模型至少是可行的。用于患者解释和医生解释的模型可能不会给出完全相同的结果，但是如果它们给出大致相同的结果，则可以提供所需的两个层次的解释。</p><p id="886e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">当然，并不是简单的教育水平决定了什么样的模型解释是合适的。冒着过度延伸医学类比的风险，与试图优化慢性健康疾病治疗的医生相比，急诊室医生可能需要更快理解的解释。</p><p id="4f50" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">一个数据科学家不可能知道并应用所有关于 UI/ UX 设计的知识，但是从这个领域考虑一个有用的概念是“用户角色”。这基本上是对将要使用你的产品的人的一个虚构的描述。以这种方式理解产品的用户应该是构建可解释模型的首要任务之一，因为要解释你的模型的人是一个特定的人，如果他们要理解你的模型及其输出，就需要理解他们的能力、约束和愿望。</p><p id="06e7" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">罗伯特·德格拉夫的书《管理你的数据科学项目》<a class="ae lu" href="https://www.amazon.com/Managing-Your-Data-Science-Projects/dp/1484249062/ref=pd_rhf_ee_p_img_1?_encoding=UTF8&amp;psc=1&amp;refRID=4X4S14FQEBKHZSDYYMZY" rel="noopener ugc nofollow" target="_blank"><em class="lt"/></a><em class="lt">》已经通过出版社出版。</em></p><p id="86a9" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><a class="ae lu" href="https://twitter.com/RobertdeGraaf2" rel="noopener ugc nofollow" target="_blank"> <em class="lt">在 Twitter 上关注罗伯特</em> </a></p></div></div>    
</body>
</html>