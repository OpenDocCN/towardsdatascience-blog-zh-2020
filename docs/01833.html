<html>
<head>
<title>How I used machine learning to strategize my GRE preparation.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我如何使用机器学习来制定GRE备考策略。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8?source=collection_archive---------10-----------------------#2020-02-20">https://towardsdatascience.com/how-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8?source=collection_archive---------10-----------------------#2020-02-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6d00" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">GRE备考最具挑战性的部分是词汇部分。至少对我来说是这样，直到我的机器学习模型帮我解决了这个问题。</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/67d8d95c5af6b13b67c32f7ef7ca407e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OStckVJzGb02CZt1RY8csw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">巴伦的333个单词(红色)及其同义词的图表。</p></figure><p id="dca1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我开始准备GRE时，在浏览了许多资源(词汇部分)后，我发现有些单词在考试中经常出现，Barron的高频单词表是解决这个问题的著名资源之一。首先，我选择了Barron的333，这是一个包含333个在GRE中出现频率最高的单词的单词列表。下一个挑战是学习这些单词，所以我想出了一个计划。如果我能以某种方式把相似的单词组合在一起，学习过程就会容易得多。但是怎么做呢？手动将这些单词分组比简单地按原样学习单词更具挑战性。思考了一段时间后，我想到为什么不让机器做所有的艰苦工作！我认为<em class="lu">每秒超过一百万次浮点运算的能力</em>比我更适合这些类型的任务，所以让我们开始，看看如何从头开始构建一个模型，将相似的单词聚集在一起。</p><p id="11af" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将涵盖几个机器学习概念，如自然语言处理(<strong class="la iu"> NLP </strong>)、词频-逆文档频率(<strong class="la iu"> TF-IDF </strong>)、奇异值分解(<strong class="la iu"> SVD </strong>)、<strong class="la iu"> K-Means </strong>、t-分布式随机邻居嵌入(<strong class="la iu"> t </strong> - <strong class="la iu"> SNE </strong>)以及许多其他数据搜集、特征工程和数据可视化技术，以展示我们如何从零开始对数据进行聚类。</p><blockquote class="lv lw lx"><p id="2921" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">注意:我将在这个项目中使用python 3.7。</strong></p></blockquote><p id="329b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">博客将分为以下几个部分-</p><ul class=""><li id="dd15" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">数据收集:抓取网站来收集数据。</li><li id="574d" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">数据清理</li><li id="e15f" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">特征工程</li><li id="5dfe" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">建模</li><li id="2df7" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">可视化结果</li></ul><p id="6f64" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们知道了问题陈述和数据流，让我们开始吧。</p><blockquote class="lv lw lx"><p id="b38f" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">抓取数据</strong></p></blockquote><p id="0106" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一项任务是收集数据，即巴伦的333个高频词。这可以通过手动键入单词并创建列表来完成，也可以通过自动化过程来完成。我使用BeaulifulSoup和request创建了一个从不同网站自动抓取数据的功能，让我们简单了解一下这些库以及如何使用它们。</p><ul class=""><li id="162b" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated"><strong class="la iu"> <em class="lu"> Numpy: </em> </strong>一个库，增加了对大型多维数组和矩阵的支持，以及对这些数组进行操作的大量高级数学函数。</li><li id="e3d8" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><strong class="la iu"> <em class="lu"> Pandas: </em> </strong>一个为数据操作和分析而编写的库。特别是，它提供了操纵数值表的数据结构和操作。</li><li id="e415" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><strong class="la iu"> <em class="lu"> BeautifulSoup: </em> </strong>一个解析HTML和XML文档的库。它为解析过的页面创建了一个解析树，可以用来从HTML中提取数据，这对web抓取很有用。</li><li id="24be" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><strong class="la iu"> <em class="lu">请求:</em> </strong>请求模块允许您使用Python发送HTTP请求。HTTP请求返回一个包含所有响应数据(内容、编码、状态等)的响应对象。</li></ul><p id="88d7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码将使用<strong class="la iu">请求</strong>从目标网站获得响应，然后使用<strong class="la iu"> BeautifulSoup </strong>解析html响应，从页面中提取所需信息，并使用<strong class="la iu"> pandas以表格格式存储信息。<br/> </strong>要了解一个html页面的格式，可以查看<a class="ae mp" href="https://www.w3schools.com/html/" rel="noopener ugc nofollow" target="_blank">本教程</a>。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="7e5b" class="mv mw it mr b gy mx my l mz na"># importing necessary libraries</span><span id="992e" class="mv mw it mr b gy nb my l mz na">import requests<br/>from bs4 import BeautifulSoup<br/>import re<br/>from functools import reduce<br/>import numpy as np<br/>import pandas as pd</span></pre><p id="efde" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们从<a class="ae mp" href="https://quizlet.com/2832581/barrons-333-high-frequency-words-flash-cards/" rel="noopener ugc nofollow" target="_blank">这个网站</a>上刮下巴伦的333个单词及其含义</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="7788" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"># chrome中网页的HTML代码可以在mac上使用<strong class="la iu"> ⌘+shift+c </strong>或者在windows/Linux上使用c <strong class="la iu"> trl+shift+c </strong>来访问。</p><p id="12ae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里我使用“span”作为标记，class _ = ' term text not translate lang-en ',因为包含单词和含义的元素具有相同的标记和类，并且在每个行元素中只有两个这样的元素，第一个对应于单词，第二个对应于含义。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/5a049045f8fb33866872b57139b862c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TlHPuTuo-vnLXl1Y__H-KQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">需要废弃的元素的HTML结构的屏幕截图。</p></figure><p id="d3c5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是以表格形式收集的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/82f9ca6436d9c1c574ac15c6d2c6f260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B1evIg_8BvaDpMiHYwqLPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">废弃单词和意义的数据框架。</p></figure><p id="dfe8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些数据是不够的，所以让我们通过从这个网站收集每个单词的同义词来增加更多的数据</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/a7a62957853c3b038fdf1a65f152bec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3jfzEZeasCMd6uKm8GDe6g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这里有一个同义词的元素。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/19a4a334297356315ec088efb8b09ef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PLr0hPzITQY5pY5JH_XO2g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">包含单词和同义词的数据帧</p></figure><p id="9f84" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们连接两个数据框(含义和同义词):</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="deee" class="mv mw it mr b gy mx my l mz na">result = pd.merge(df.word, data, on='word')<br/>result.fillna('', inplace=True)<br/>print(result)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/63d6069329df11a0d79a24af4498de95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iPHWY7srA9FlwLVrLIxUWg.png"/></div></div></figure><p id="0872" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到，数据需要一些清理，因为它包含停用词，如和，或，和其他元素，如标点符号。此外，我们必须注意不能、不会、不这些必须分别转换为不能、不愿意、不做的缩写。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="2afa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，既然数据已经被清理，让我们做一些特征工程。</p><blockquote class="lv lw lx"><p id="7b79" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">特色工程</strong></p></blockquote><p id="49de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我注意到的一件事是，其中一些单词的同义词也属于Barron的333列表。如果我能以某种方式连接这些单词的同义词，它可以提高我们的模型的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/7bd52d1f39571b116d873fc067e517f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m7RGuamAlyh_gNf_ACvY-A.png"/></div></div></figure><ul class=""><li id="64a5" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">例如对于<strong class="la iu"><em class="lu"/></strong>，同义词有<em class="lu">迂回、迂回、迂回、迂回。</em></li><li id="0ec8" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">在这3个同义词中，<strong class="la iu">错综复杂</strong>出现在巴伦的333个单词列表中，它的同义词<em class="lu">错综复杂，错综复杂。</em></li><li id="73d7" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">所以，<strong class="la iu">曲折的</strong>最后的同义词应该是<em class="lu">迂回的、迂回的、间接的、错综的、迷宫般的、令人费解的</em>。</li></ul><p id="7f64" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这一步之后，对于Barron的333个单词列表中的每个单词，我们都有直接同义词、间接同义词(在上面的例子中，<strong class="la iu">错综复杂</strong>的同义词是<strong class="la iu">曲折复杂</strong>的间接同义词)和含义。在这篇博客中，我将对这些数据使用符号<strong class="la iu"> set </strong>。</p><blockquote class="lv lw lx"><p id="684a" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">集合:</strong>关于一个词的数据(这里这个词来自barron的333)比如它的直接同义词、间接同义词和意义。词的集合包括词本身。</p></blockquote><p id="f6de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们已经获得了需要聚类的干净的单词集，但是记住我们首先需要将这些集合转换成某种数字数据，因为我们的模型需要数字来处理。</p><p id="6021" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将使用<strong class="la iu"> TF-IDF </strong>对数据进行矢量化。在开始之前，让我们先了解一下什么是tf-idf</p><p id="add4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> TF-IDF </strong>术语频率的缩写-逆文档频率是一种数字统计，旨在反映一个词对语料库中的文档有多重要。让我们用<strong class="la iu">单词袋</strong>来理解这一点。</p><p id="f1b9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">单词袋模型是在自然语言处理和信息检索中使用的简化表示。在这个模型中，文本被表示为它的单词包。<br/>简单来说，单词包只不过是文档的基本数字表示，它是通过首先创建包含所有文档中所有不同单词的单词的<strong class="la iu">词汇表</strong>来完成的。现在，每个文档都用一个有“n”个元素的向量来表示(这里，n是词汇表中的单词数，所以每个元素对应于词汇表中的一个单词)，每个元素都有一个数值，告诉我们这个单词在文档中出现了多少次。让我们考虑一个例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b77dd30233bb38afaef51e0a208eaa91.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*EVu8iK6PijpfCxqjhFRpaQ.jpeg"/></div></figure><ul class=""><li id="7f66" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">列<strong class="la iu">字</strong>代表词汇。</li><li id="c24f" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">在该表中，列文档1和文档2分别表示文档1和文档2的弓。</li><li id="a526" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">数字表示相应的单词在文档中出现的次数。</li><li id="6b3e" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">现在来了TF-IDF，简直就是词频和逆文档频的乘积。</li></ul><p id="7749" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">词频</em> </strong> <em class="lu"> : </em>表示一个词在文档中出现的频率的数字。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/c6205ec1f8e079e40824c51a2b115ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*rQlC66HYBiVPYOXbUl1tvQ.png"/></div></figure><p id="7b27" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">逆文档频率</em> </strong> <em class="lu"> : </em>它是找到包含该单词的文档的概率的倒数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/cd6b0eaaa02d1e3ec73a965d0e832651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*xtgXRlC-gSmoFOIG2vrSTQ.png"/></div></figure><p id="36ba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请这样想:如果这个词在所有文档中广泛使用，那么它在特定文档中的存在将无法提供关于该文档本身的同样多的特定信息。因此，第二个词可以被看作是一个惩罚词，用来惩罚常见的词，如“a”、“the”、“and”等。因此，tf-idf可以被视为特定文档中单词相关性的加权方案。</p><p id="877d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们来看看两个文档的TF-IDF:<br/>文档1:“TF代表词频”，terms: 5 <br/>文档2:“and IDF代表逆文档频”，terms: 7</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/1df3899183a0073a38aef4450d26c232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CzVrAaxzaLfs0bF8ZbkLSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一袋单词、TF和IDF值。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/a759a86f305bfd0aa22d3a573bb8c040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bueWekM8K0tTHXRMI_0xSg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF-IDF值。</p></figure><p id="bdf4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">二元模型:</em> </strong>二元模型是一串记号中两个相邻元素的序列，这些记号是典型的字母、音节或单词。下面是一个从文档中生成的单词和双词的例子。<br/><strong class="la iu"><em class="lu">doc:</em></strong>【TF代表词频】<br/> <em class="lu">单字:</em> ['tf '，'代表'，'代表'，'词频']<br/><em class="lu"/>[' TF代表'，'代表'，'代表词频']</p><p id="d5d6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">n元语法的优点是它们增加了关于文档中单词序列的信息。</p><p id="834b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我写了一个计算TF-IDF的函数，这个函数同时使用了一元和二元。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="17a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们已经有了每个集合的TF-IDF嵌入，我们可以继续建模了。</p><p id="c36f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意:我们目前拥有的数据是一个表格，包含m行和n列，其中‘<strong class="la iu">m’是Barron的333个单词列表中的单词数，而‘n’是单词词汇表的大小。</strong>这个表格数据也可以表示为一个维度数组(<strong class="la iu">m×n)。在博客的后面，我将使用数组来表示数据。</strong></p><p id="ab43" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在建模之前，我想分享一些我觉得非常有用的东西。与其直接使用TF-IDF值进行建模，不如降低数据的维度？这意味着，我们有对应于每个集合的TF-IDF值，并且因为这些TF-IDF值被表示为n个元素的向量，其中‘n’也对应于我们所有集合中不同单词的数量。如果两个集合具有几乎相同的单词，则n维超平面中对应点之间的距离将会非常小，反之亦然。类似地，如果2个集合有非常少的共同单词，则n-超平面中对应点之间的距离将会大得多，反之亦然。现在，我不再使用n-超平面来表示这些点，而是将这些点的维度降低到32维(为什么是32维？稍后将在博客中讨论)。可以通过挑选32个随机维度并忽略其他维度来降低维度，但这太愚蠢了，所以我尝试使用不同的维度降低技术，并发现截断的SVD可以为给定的数据创造奇迹。</p><blockquote class="lv lw lx"><p id="0df3" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">我使用降维的方法给数据添加某种形式的<a class="ae mp" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">正则化</a>。</p></blockquote><p id="bbc0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">让我们理解截断奇异值分解- </strong></p><p id="fd90" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">SVD是奇异值分解的缩写，它是一种矩阵分解技术，将任何给定的矩阵分解为三个矩阵U、S和v。<br/>它遵循以下等式-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/7773091777d40d94b5ced6335438651b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ieoRDguEa0BVrLyO.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这里，A是包含m行和n列的输入矩阵。</p></figure><p id="789a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我解释一下什么是U，S，V。<br/> <strong class="la iu"> <em class="lu"> U(又名左奇异):</em> </strong>是一个正交矩阵，其列是<strong class="la iu"> AᵀA </strong> <em class="lu">的特征向量。<br/> </em> <strong class="la iu"> <em class="lu"> S(单数):</em> </strong>为对角矩阵，其对角元素为按降序排列的<strong class="la iu"> AᵀA </strong>或<strong class="la iu"> AAᵀ </strong>(两者具有相同的特征值)的特征值的平方根即</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/e4154493cc89f4e01b923b4c4c2f9345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVknwpIrbPkFOIyemab5HA.png"/></div></div></figure><p id="c6da" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu"> V(又名右奇异):</em> </strong>是一个正交矩阵，其列是<strong class="la iu"> AAᵀ </strong> <em class="lu">的特征向量。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/6b4fa7df11560494622f4cd70f130d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mJ12be_KbhuS8Ta0.jpeg"/></div></div></figure><p id="20b9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">特征向量:</strong>特征向量是当对其进行线性变换时，其方向保持不变的向量。考虑下图，其中显示了三个向量。绘制绿色方块只是为了说明应用于这三个向量的线性变换。<br/>注意，这些特征向量的方向不变，但它们的长度变了，而<strong class="la iu">特征值</strong>是它们长度变化的因子。<br/>来源:<a class="ae mp" href="https://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/" rel="noopener ugc nofollow" target="_blank">https://www . vision dummy . com/2014/03/特征值-特征向量/ </a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b982d2c43f9a1293b2b476968419a559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/0*7SkivPxZSwPO1NsQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">对特征向量(红色)应用线性变换(例如缩放)时，它们不会改变方向。其他载体(黄色)有。</p></figure><p id="4852" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们知道了如何分解矩阵，我们可以使用截断奇异值分解来降低矩阵的维数，截断奇异值分解是奇异值分解的简单扩展。</p><p id="94b1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">截断奇异值分解:</strong>假设我们有一个维数为(m x n)的输入矩阵，我们希望将它简化为(m x r)其中r &lt; n. <br/>我们简单地计算<strong class="la iu"> AᵀA </strong>的第一个<strong class="la iu">‘r’</strong>特征向量并将其存储为u的列，然后我们计算<strong class="la iu"> AAᵀ </strong>的第一个<strong class="la iu">‘r’</strong>特征向量并将其存储为v的列，最后存储为的根</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/a831c2fbb5b025e9e842edbaa5f58b92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ajW4rgk6fhrYmNG_.gif"/></div></div></figure><p id="3a78" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简而言之，截断SVD是一种聪明的技术，它通过保留尽可能多的信息(方差)以一种聪明的方式降低了给定数据的维数。</p><p id="3018" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想更深入地研究奇异值分解，可以看看W. Gilbert Strang教授的讲座和关于奇异值分解的博客。</p><p id="4e74" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Scikit-learn带有内置的<a class="ae mp" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html" rel="noopener ugc nofollow" target="_blank">截断SVD </a>，可以直接导入和使用。</p><blockquote class="lv lw lx"><p id="4013" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">建模</strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/cc94dce170bbd5837a7776343711ea05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4gydrw9A3FqJfb7m.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae mp" href="http://graphalchemist.github.io/Alchemy/images/features/cluster_team.png" rel="noopener ugc nofollow" target="_blank">http://graph alchem . github . io/Alchemy/images/features/cluster _ team . png</a></p></figure><p id="7398" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们已经完成了数据预处理和特征工程部分，让我们看看如何使用聚类算法将相似的词分组在一起。</p><h1 id="c95c" class="nu mw it bd nv nw nx ny nz oa ob oc od jz oe ka of kc og kd oh kf oi kg oj ok bi translated">k均值</h1><p id="76d6" class="pw-post-body-paragraph ky kz it la b lb ol ju ld le om jx lg lh on lj lk ll oo ln lo lp op lr ls lt im bi translated"><strong class="la iu"> K </strong> - <strong class="la iu">的意思是</strong>聚类是一种无监督学习，当你有未标记的数据(即没有定义类别或组的数据)时使用。该算法的目标是在数据中寻找组，组的数量由变量<strong class="la iu"> K </strong>表示。</p><p id="6b46" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看K-means是如何工作的。<br/>假设在一个二维平面上有一些点，我们想把这些点聚集成<strong class="la iu"> K </strong>簇。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq nd l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">内容来源:<a class="ae mp" href="https://www.youtube.com/watch?v=5I3Ei69I40s&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=5I3Ei69I40s&amp;feature = youtu . be</a></p></figure><p id="9a96" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">步骤很简单-</p><ul class=""><li id="80d8" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">在平面上随机定义<strong class="la iu"> K个</strong>点。让我们称这些点为群集<strong class="la iu">质心</strong>。</li><li id="5396" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">迭代数据中的每个点，检查最近的质心，并将该点分配给其最近的质心。</li><li id="f83c" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">经过上述步骤，每个质心必须有一些最接近它的点，让我们称这些点集为<strong class="la iu">簇</strong>。</li><li id="8097" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">通过计算每个聚类中所有点的x和y坐标的平均值来更新该聚类的质心。计算的平均值(x，y)是该聚类的更新质心的坐标。</li><li id="4e54" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">重复最后3个步骤，直到质心的坐标没有更新太多。</li></ul><p id="3dbc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">但是如何决定K(聚类数)的正确值呢？</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/9c713fbc70ff860c11d36cc21bc18c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aSeXkf9At7WSbGy5s_d4vw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae mp" href="http://www.semspirit.com/artificial-intelligence/machine-learning/clustering/k-means-clustering/k-means-clustering-in-python/" rel="noopener ugc nofollow" target="_blank">http://www . SEM spirit . com/artificial-intelligence/machine-learning/clustering/k-means-clustering/k-means-clustering-in-python/</a></p></figure><p id="6cd6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">失真可以用来检查对于给定的<strong class="la iu"> K </strong>值，聚类算法的效率如何。<br/>K的最佳值可以通过计算不同K值的失真值，然后绘制它们来确定。<br/>该图被称为<strong class="la iu">弯头图</strong>。只需查看肘形图，我们就可以确定最佳“K ”,即失真停止快速下降的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/df73b7d8cc01acd0cc06fcfe9350aef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/0*OIKw1kVoK0GVfDiB.png"/></div></figure><p id="2db3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个肘形图，通过观察，我们可以说最佳超参数是k=3。</p><p id="e784" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它被称为肘图，因为它看起来像一只手臂(也许是棍子人的手臂)，这只手臂的肘部代表最佳k。</p><p id="4dbc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有一件事，我将使用余弦距离作为度量来计算点之间的距离(包括质心)。让我们快速了解什么是<strong class="la iu">余弦距离</strong>利用<em class="lu">余弦相似度。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/7ce9e28f3a2741e0413c58c5877166f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/0*1Z6SiZT4cIUj05Hk"/></div></figure><p id="0b0e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">余弦相似度</strong>是计算2个向量平行程度的度量。它是使用两个向量之间的角度的余弦来计算的。很容易计算出来，用-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/a5f4b0cdad9d2694f753af2b7e3de25b.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/0*NJpAgmY9Xml5RPeY"/></div></figure><p id="a7b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以如果2个向量a和b平行，那么它们之间的角度将为0，余弦相似度将为cos(0) = 1。类似地，如果两个向量a和b指向相反的方向，它们之间的角度将是𝛑，余弦相似度将是cos(𝛑) = -1。<br/>简而言之，余弦相似性告诉我们两个向量在多大程度上指向相似的方向。接近1的值告诉我们，向量指向非常相似的方向，而接近-1的值对应于指向相反的方向。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/b5eda7d91b149a683d4fb79f84ae99a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GVHoeYeb-P8nySnEQo8JkQ.png"/></div></div></figure><p id="7af3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在2点之间的<strong class="la iu">余弦距离</strong>无非是<br/> <strong class="la iu"> <em class="lu"> 1 -(多维空间中代表它们的向量的余弦相似度)</em> </strong>。<br/>所以范围从(0到2)，其中0对应非常相似的点，2对应非常不相似的点。<br/>你猜两点间的余弦距离什么时候会是1？</p><p id="d82e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要了解更多信息，请查看这个博客。</p><p id="e731" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将从头开始实现K-means，因为Scikit知道K-Means不支持余弦距离。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="8041" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">是时候对预处理数据运行算法并检查正确的超参数了。<br/>超参数调整是一个确定正确的超参数的过程，使模型对给定数据非常有效。在这种情况下，有两个超参数-</p><ul class=""><li id="fe3a" class="mb mc it la b lb lc le lf lh md ll me lp mf lt mg mh mi mj bi translated">1来自截断SVD: n_components(降维)</li><li id="8d00" class="mb mc it la b lb mk le ml lh mm ll mn lp mo lt mg mh mi mj bi translated">1 from K-表示:‘K’(聚类数)。</li></ul><p id="1a30" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我已经画出了不同n_component值的肘图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/ca7d6120015947bf57d99b7201ca8bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pMWankiqTj-wyeWiCftndQ.png"/></div></div></figure><p id="6989" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过观察这些图，我们可以说最佳的超参数是- <br/> - n_components: 32 <br/> - K(聚类数):50</p><p id="a99b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，使用最佳超参数初始化截断的SVD和K-Means，并对数据进行聚类。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="82f5" class="mv mw it mr b gy mx my l mz na">from sklearn.decomposition import TruncatedSVD</span><span id="fef5" class="mv mw it mr b gy nb my l mz na">&gt; trans = TruncatedSVD(n_components=32)<br/>&gt; data_updated = trans.fit_transform(words_tfidf.toarray())<br/>&gt; model = custom_KMeans(n_clusters=50)<br/>&gt; model.train(data_updated)</span></pre><blockquote class="lv lw lx"><p id="5c6f" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">可视化结果</strong></p></blockquote><p id="b2c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些是对数据进行聚类后得到的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/4d825c421f236bfce07ed87db6403c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UockM02CJAXyktGngP2EyA.png"/></div></div></figure><p id="5673" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来看看一些集群:<br/>我将使用<a class="ae mp" href="https://networkx.github.io/documentation/stable/" rel="noopener ugc nofollow" target="_blank"> networkx库</a>来创建集群。<br/>在每个聚类中，红色节点对应于Barron的333个单词列表中的单词，以及它们之间的联系和它们的同义词。<br/>你可以在这里查看networkx <a class="ae mp" href="https://networkx.github.io/documentation/stable/" rel="noopener ugc nofollow" target="_blank">的文档。另外，我将在最后用一个例子来演示它有多棒。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/1afb146fe814f64cecb8e88e03407264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qzIVJO_m0UUOemKXinmGgQ.png"/></div></div></figure><p id="61d2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，让我们用t-SNE三维可视化数据，但首先，</p><p id="a01f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">我们来说说t-SNE: </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/b3efde44e603142d1ceb231a867ce785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*erGNe7ChcAcpJzdX.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae mp" href="https://www.youtube.com/watch?v=wvsE8jm1GzE" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=wvsE8jm1GzE</a></p></figure><p id="4999" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">t-SNE</strong>(t-分布式随机邻居嵌入)是一种用于探索高维数据的非线性降维算法。它将多维数据映射到两个或更多维，使得在较低维中的每个嵌入表示较高维中的值。此外，这些嵌入以保持邻域点之间的距离的方式放置在较低的维度中。所以，t-SNE也保留了数据的本地结构。<br/>我会试着解释它是如何工作的。<br/>对于n维超空间中的给定点，它计算该点与所有其他点的距离，并将这些距离分布转换为<a class="ae mp" href="https://en.wikipedia.org/wiki/Student%27s_t-distribution" rel="noopener ugc nofollow" target="_blank">学生的t分布</a>。对所有点都这样做，最终，每个点到所有其他点的距离都有自己的t分布。<br/>现在，这些点被随机分散在低维空间中，每个点被移位一定距离，使得在完成所有点的移位之后，如果我们重新计算每个点到剩余点的距离的t分布(这次是在低维空间中完成的)，该分布将与我们在n维超空间中获得的分布相同。<br/>在t-SNE中有2个主要的超参数- <br/> <strong class="la iu"> <em class="lu">困惑:</em> </strong>不用计算到所有其他点的距离，我们可以只使用‘k’个最近的点。这个“k”值被称为困惑值。<br/> <strong class="la iu"> <em class="lu">迭代:</em> </strong>我们希望t-SNE更新低维空间中的点的迭代次数。<br/>由于随机性，算法可能会因不同的困惑值而表现不同，因此作为一种良好的实践，最好针对不同的困惑值和不同的迭代次数运行t-SNE。<br/>想了解更多关于霸王龙SNE的信息，请查看<a class="ae mp" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank">这个很棒的博客</a>，它用互动可视化很好地解释了霸王龙SNE。</p><p id="fd39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是使用t-SNE表示3d空间中的点的代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pa nd l"/></div></figure><p id="2283" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上图中，每个气泡代表3d空间中的一个单词。我们可以看到一些紫色和橙色的点很好地聚集在一起。我们还可以在橙色中看到一个太远的点，这意味着它在语义上不同于大多数单词。这种表示可能不是很好，因为聚类是使用余弦距离计算的，而t-SNE使用一些不同的距离度量，并且因为t-SNE是随机的，所以它需要一些超参数调整，所以如果您想使用另一个数据集或使用欧几里德距离的较少数量的聚类进行实验，希望它会工作。</p><p id="c1de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是另一个使用t-SNE的图，只使用不同困惑和迭代值的二维。我们可以看到t-SNE与困惑20和2000迭代工作得很好。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/ab085e565e84596adca57eb04b68b43d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RQ1ZtC9P8ZhcQPbHsUhsEg.png"/></div></div></figure><p id="4f4d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，这里是使用networkx的所有单词及其同义词的完整图表(我为每个单词使用了4个同义词)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/a3459e65aca9d7cbee995d8bd1c6b9b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5MjLkhDoPNcU-eVi2T-J3A.png"/></div></div></figure><blockquote class="lv lw lx"><p id="624b" class="ky kz lu la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated"><strong class="la iu">最终注释</strong></p></blockquote><p id="e46e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您阅读博客。我希望它对那些有志于从事NLP、无监督机器学习、数据处理、数据可视化项目的人有用。</p><p id="fca6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你对这个项目有任何疑问，请在这个项目的回复部分或GitHub repo中留下评论。</p><p id="6ec0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我的Github上有完整的项目:<br/><a class="ae mp" href="https://github.com/SarthakV7/Clustering-Barron-s-333-word-list-using-unsupervised-machine-learning" rel="noopener ugc nofollow" target="_blank">https://Github . com/SarthakV7/Clustering-Barron-s-333-word-list-using-unsupervised-machine-learning</a></p><p id="6ea5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在LinkedIn上找到我:<a class="ae mp" href="http://www.linkedin.com/in/sarthak-vajpayee" rel="noopener ugc nofollow" target="_blank">www.linkedin.com/in/sarthak-vajpayee</a></p><p id="cf0f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">和平！☮</p></div></div>    
</body>
</html>