<html>
<head>
<title>Recommendation System Series Part 4: The 7 Variants of Matrix Factorization For Collaborative Filtering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推荐系统系列第4部分:用于协同过滤的矩阵分解的7种变体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5?source=collection_archive---------2-----------------------#2020-02-24">https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5?source=collection_archive---------2-----------------------#2020-02-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1f05" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">RecSys系列</h2><div class=""/><div class=""><h2 id="bd04" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">矩阵分解的数学深度探究</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/3ea3d78b743dd61e460a17e6e48201c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b4M7o7W8bfRRxdMxtFoVBQ.png"/></div></div></figure><p id="f248" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">更新:</em> </strong> <em class="lw">本文是我探索学术界和工业界推荐系统系列文章的一部分。查看完整系列:</em> <a class="ae lx" rel="noopener" target="_blank" href="/recommendation-system-series-part-1-an-executive-guide-to-building-recommendation-system-608f83e2630a"> <em class="lw">第一部分</em> </a> <em class="lw">，</em> <a class="ae lx" rel="noopener" target="_blank" href="/recommendation-system-series-part-2-the-10-categories-of-deep-recommendation-systems-that-189d60287b58"> <em class="lw">第二部分</em> </a> <em class="lw">，</em> <a class="ae lx" rel="noopener" target="_blank" href="/recommendation-system-series-part-3-the-6-research-directions-of-deep-recommendation-systems-that-3a328d264fb7"> <em class="lw">第三部分</em> </a> <em class="lw">，</em> <a class="ae lx" rel="noopener" target="_blank" href="/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5"> <em class="lw">第四部分</em> </a> <em class="lw">，</em> <a class="ae lx" rel="noopener" target="_blank" href="/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883"> <em class="lw">第五部分</em> </a> <em class="lw">和</em> <a class="ae lx" rel="noopener" target="_blank" href="/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7"/></p><p id="9789" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">协同过滤是任何现代推荐系统的核心，它已经在亚马逊、网飞和Spotify等公司取得了相当大的成功。它的工作原理是收集人们对某个特定领域的项目的判断(称为评级),并将具有相同信息需求或相同品味的人匹配在一起。协同过滤系统的用户共享他们对他们消费的每个项目的分析判断和意见，以便系统的其他用户可以更好地决定消费哪些项目。反过来，协同过滤系统为新项目提供有用的个性化推荐。</p><p id="11a9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">协同过滤的两个主要领域是(1)邻域方法和(2)潜在因素模型。</p><ul class=""><li id="4a31" class="ly lz iq lc b ld le lg lh lj ma ln mb lr mc lv md me mf mg bi translated"><strong class="lc ja">邻域法</strong>专注于计算项目之间或用户之间的关系。这种方法基于同一用户对相邻项目的评级来评估用户对项目的偏好。一个项目的邻居是由同一用户评价时倾向于获得相似评价的其他产品。</li><li id="ce75" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated"><strong class="lc ja">潜在因素法</strong>通过从评级模式中推断出的许多因素来描述项目和用户的特征，从而解释评级。例如，在音乐推荐中，发现的因素可能测量精确的维度，如嘻哈与爵士、高音量或歌曲长度，以及不太明确的维度，如歌词背后的含义，或完全无法解释的维度。对于用户来说，每个因素衡量用户有多喜欢在相应歌曲因素上得分高的歌曲。</li></ul><p id="9580" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一些最成功的潜在因素模型是基于<strong class="lc ja">矩阵分解。</strong>在其自然形式中，矩阵因子分解使用从项目评级模式推断的因子向量来表征项目和用户。项目和用户因素之间的高度对应导致推荐。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mm"><img src="../Images/63b77596b298513008883fa6d6a811b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5js-e2mRMKYim8UuAhIL2Q.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><em class="mr"> Sharmistha Chatterjee —使用Python的矩阵分解技术概述(</em><a class="ae lx" rel="noopener" target="_blank" href="/overview-of-matrix-factorisation-techniques-using-python-8e3d118a9b39"><em class="mr">https://towardsdatascience . com/Overview-of-Matrix-Factorization-Techniques-using-Python-8e 3d 118 a9 b 39</em></a><em class="mr">)</em></p></figure><p id="d9a9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在这篇文章和接下来的文章中，我将介绍推荐系统的创建和训练，因为我目前正在做这个主题的硕士论文。</p><ul class=""><li id="7eaa" class="ly lz iq lc b ld le lg lh lj ma ln mb lr mc lv md me mf mg bi translated"><a class="ae lx" rel="noopener" target="_blank" href="/recommendation-system-series-part-1-an-executive-guide-to-building-recommendation-system-608f83e2630a">第1部分</a>提供了推荐系统的高级概述，它们是如何构建的，以及它们如何用于改善各行业的业务。</li><li id="a362" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated"><a class="ae lx" rel="noopener" target="_blank" href="/recommendation-system-series-part-2-the-10-categories-of-deep-recommendation-systems-that-189d60287b58">第2部分</a>对正在进行的关于这些模型的优势和应用场景的研究计划进行了仔细的回顾。</li><li id="ff9a" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated"><a class="ae lx" rel="noopener" target="_blank" href="/recommendation-system-series-part-3-the-6-research-directions-of-deep-recommendation-systems-that-3a328d264fb7">第3部分</a>提供了几个可能与推荐系统学者社区相关的研究方向。</li></ul><p id="01a6" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在第4部分中，我深入探讨了矩阵分解的数学细节，这可以说是目前推荐系统研究中最常见的基线模型。更具体地说，我将向您展示可以构建的矩阵分解的七种变体——从边特征的使用到贝叶斯方法的应用。</p><h2 id="412c" class="ms mt iq bd mu mv mw dn mx my mz dp na lj nb nc nd ln ne nf ng lr nh ni nj iw bi translated">1 —标准矩阵分解</h2><p id="418c" class="pw-post-body-paragraph la lb iq lc b ld nk ka lf lg nl kd li lj nm ll lm ln nn lp lq lr no lt lu lv ij bi translated">一个简单的矩阵分解模型将用户和项目都映射到一个维度为D的联合潜在因素空间，这样用户-项目交互就被建模为该空间中的内积。</p><ul class=""><li id="94d2" class="ly lz iq lc b ld le lg lh lj ma ln mb lr mc lv md me mf mg bi translated">因此，每个项目I与向量q_i相关联，并且每个用户u与向量p_u相关联</li><li id="8283" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">对于一个给定的项目I，q_i的元素测量该项目拥有这些因素的程度，积极的或消极的。</li><li id="083e" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">对于给定的用户u，p_u的元素测量用户对项目的兴趣程度，该项目在相应的正面或负面因素上是高的。</li><li id="5fb7" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">得到的点积(q_i * p_u)抓住了用户u和物品I之间的交互，是用户对物品特征的整体兴趣。</li></ul><p id="e066" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，我们有如下等式1:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c3c5843278eae4f672974a41069a86c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*Sd5jAoQpCYHDXNvA93ZgQA.png"/></div></figure><p id="784f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">最大的挑战是计算每个项目和用户到因子向量q_i和p_u的映射。矩阵分解通过最小化已知评级集的正则化平方误差来实现这一点，如下面的等式2所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/2305bd4504704668a362cbe5bb0ee27f.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*EPdhUbPCJtDZcOKkVPUSgQ.png"/></div></figure><p id="6e16" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">通过拟合先前观察到的评级来学习该模型。然而，目标是以预测未来/未知评级的方式来概括那些先前的评级。因此，我们希望通过向每个元素添加L2正则化惩罚来避免过度拟合观察到的数据，并同时利用随机梯度下降来优化学习到的参数。</p><p id="afc1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">Shay Palachy的这篇文章很好地解释了直觉，以下是快速注释:</p><ul class=""><li id="f61b" class="ly lz iq lc b ld le lg lh lj ma ln mb lr mc lv md me mf mg bi translated">当我们使用SGD来将模型的参数拟合到手头的学习问题时，我们在算法的每次迭代中在解空间中朝着损失函数相对于网络参数的梯度前进一步。由于我们推荐的用户-项目交互矩阵非常稀疏，这种学习方法可能会过度适应训练数据。</li><li id="2b80" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">L2(也称为<a class="ae lx" href="https://en.wikipedia.org/wiki/Tikhonov_regularization" rel="noopener ugc nofollow" target="_blank">吉洪诺夫正则化</a>或岭回归)是一种正则化成本函数的特定方法，增加了一个表示复杂性的项。该项是用户和项目潜在因素的平方欧几里得范数。添加一个附加参数λ，以允许控制正则化的强度。</li><li id="07f6" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">加入L2项通常会使整个模型的参数变得更小。</li></ul><p id="9859" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们看看这在代码中是什么样子的:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="af56" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个模型的完整实验可以在这里访问:<a class="ae lx" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/Vanilla-MF" rel="noopener ugc nofollow" target="_blank">https://github . com/khanhnamle 1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/Vanilla-MF</a></p><h2 id="6209" class="ms mt iq bd mu mv mw dn mx my mz dp na lj nb nc nd ln ne nf ng lr nh ni nj iw bi translated">2 —有偏差的矩阵分解</h2><p id="86a3" class="pw-post-body-paragraph la lb iq lc b ld nk ka lf lg nl kd li lj nm ll lm ln nn lp lq lr no lt lu lv ij bi translated">协作过滤的矩阵分解方法的一个好处是它在处理各种数据方面和其他应用特定的需求方面的灵活性。回想一下，等式1试图捕捉产生不同评级值的用户和项目之间的交互。然而，许多观察到的评分值的变化是由于与用户或项目相关的影响，称为<strong class="lc ja">偏差</strong>，与任何交互无关。这背后的直觉是，一些用户给出比其他人高的评级，一些项目系统地获得比其他人高的评级。</p><p id="5fcb" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，我们可以将等式1扩展到等式3，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/4547c652b29052da23026ad243025dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*NDzahxyPrLmFjNOvngyviA.png"/></div></div></figure><ul class=""><li id="bf54" class="ly lz iq lc b ld le lg lh lj ma ln mb lr mc lv md me mf mg bi translated">总体平均评级中包含的偏差用b表示。</li><li id="4f48" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">参数w_i和w_u分别表示项目I和用户u与平均值的观测偏差。</li><li id="6c8d" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">请注意，观察到的评级分为4个部分:(1)用户-项目互动，(2)全球平均，(3)项目偏见，和(4)用户偏见。</li></ul><p id="6e00" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">通过最小化新的平方误差函数来学习该模型，如下面的等式4所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/f360b90d59faf4a0149ddb0f71326b3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8o-cQ_9-8HpUBpB47lIu9A.png"/></div></div></figure><p id="473e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们看看这在代码中是什么样子的:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="b4f9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个模型的完整实验可以在这里访问:<a class="ae lx" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/MF-Biases" rel="noopener ugc nofollow" target="_blank">https://github . com/khanhnamle 1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/MF-bias</a></p><h2 id="235a" class="ms mt iq bd mu mv mw dn mx my mz dp na lj nb nc nd ln ne nf ng lr nh ni nj iw bi translated">3 —带辅助特征的矩阵分解</h2><p id="1383" class="pw-post-body-paragraph la lb iq lc b ld nk ka lf lg nl kd li lj nm ll lm ln nn lp lq lr no lt lu lv ij bi translated">协同过滤中的一个常见挑战是冷启动问题，因为它无法处理新项目和新用户。或者许多用户提供非常少的评级，使得用户-项目交互矩阵非常稀疏。缓解这个问题的一个方法是加入关于用户的额外信息来源，也就是<strong class="lc ja">的侧面特征</strong>。这些可以是用户属性(人口统计)和隐式反馈。</p><p id="9e66" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">回到我的例子，假设我知道用户的职业。对于这个侧面特征，我有两种选择:将其作为一种偏见添加(艺术家比其他职业更喜欢电影)和作为一种向量添加(房地产经纪人喜欢房地产节目)。矩阵分解模型应该将所有信号源与增强的用户表示相结合，如等式5所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/6d4b2be4f370d04560063f5d658df521.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*YRCh51ugL2y_CTHZnvPpfA.png"/></div></figure><ul class=""><li id="2127" class="ly lz iq lc b ld le lg lh lj ma ln mb lr mc lv md me mf mg bi translated">对职业的偏好用d_o表示，这意味着职业像速率一样变化。</li><li id="1053" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">职业的向量由t_o表示，意味着职业根据项目(q_i * t_o)而变化。</li><li id="a307" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">请注意，必要时项目可以得到类似的处理。</li></ul><p id="b2d1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">损失函数现在是什么样子的？下面的等式6表明:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/2b63e79d3668ebd9be6cb8a84a20b2c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WznSzoEaptgYnc-1r213vA.png"/></div></div></figure><p id="f84b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们看看这在代码中是什么样子的:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="6703" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个模型的完整实验可以在这里访问:<a class="ae lx" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/MF-Side-Features" rel="noopener ugc nofollow" target="_blank">https://github . com/khanhnamle 1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/MF-Side-Features</a></p><h2 id="c3b1" class="ms mt iq bd mu mv mw dn mx my mz dp na lj nb nc nd ln ne nf ng lr nh ni nj iw bi translated">4-具有时间特征的矩阵分解</h2><p id="65fb" class="pw-post-body-paragraph la lb iq lc b ld nk ka lf lg nl kd li lj nm ll lm ln nn lp lq lr no lt lu lv ij bi translated">到目前为止，我们的矩阵分解模型是静态的。在现实中，项目受欢迎程度和用户偏好是不断变化的。因此，我们应该考虑反映用户-项目交互的动态性质的时间效应。为了实现这一点，我们可以添加一个影响用户偏好的时间项，从而影响用户和项目之间的交互。</p><p id="bfdb" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">为了更复杂一点，让我们用时间t的评级的动态预测规则来尝试下面的新等式7:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f2e2c36d7a54fd6128a89e3ab91a926e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*PG9MXtCC9c1RpRHjeZnw9A.png"/></div></figure><ul class=""><li id="96e4" class="ly lz iq lc b ld le lg lh lj ma ln mb lr mc lv md me mf mg bi translated">p_u (t)将用户因素作为时间的函数。另一方面，q_i保持不变，因为项目是静态的。</li><li id="24a4" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">我们根据用户的不同有职业的变化(p_u * t_o)。</li></ul><p id="4872" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">等式8显示了包含时间特征的新损失函数:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/1236f6b05562ac3c71616f1a91db6efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFs4hUuNTP_w1EKZBbWUTg.png"/></div></div></figure><p id="26f3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们看看这在代码中是什么样子的:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="f745" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个模型的完整实验可以在这里访问:<a class="ae lx" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/MF-Temporal-Features" rel="noopener ugc nofollow" target="_blank">https://github . com/khanhnamle 1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/MF-Temporal-Features</a></p><h2 id="7ded" class="ms mt iq bd mu mv mw dn mx my mz dp na lj nb nc nd ln ne nf ng lr nh ni nj iw bi translated">5 —因式分解机器</h2><p id="fc8f" class="pw-post-body-paragraph la lb iq lc b ld nk ka lf lg nl kd li lj nm ll lm ln nn lp lq lr no lt lu lv ij bi translated">推荐系统的一个更强大的技术叫做<strong class="lc ja">因式分解机器</strong>，它具有强大的表达能力来概括矩阵因式分解方法。在许多应用程序中，我们有大量的项目元数据可以用来进行更好的预测。这是对特征丰富的数据集使用因式分解机的好处之一，对于这种情况，有一种自然的方式可以将额外的特征包括在模型中，并且可以使用维度参数d对高阶交互进行建模。对于稀疏数据集，二阶因式分解机模型就足够了，因为没有足够的信息来估计更复杂的交互。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/99d75156d885dd7758cc25314da23f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*C1x_WWZOWjMkZqtaTM3J8Q.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><em class="mr"> Berwyn Zhang —因式分解机(</em><a class="ae lx" href="http://berwynzhang.com/2017/01/22/machine_learning/Factorization_Machines/" rel="noopener ugc nofollow" target="_blank"><em class="mr">)http://Berwyn Zhang . com/2017/01/22/machine _ learning/Factorization _ Machines/</em></a><em class="mr">)</em></p></figure><p id="5afd" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">等式9显示了二阶FM模型的情况:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/83afa315f42a917341e90ad0349931b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*0OKsaenRz8-0Yd4wgPnWig.png"/></div></figure><p id="9512" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">其中v代表与每个变量(用户和项目)相关的k维潜在向量，括号运算符代表内积。根据Steffen Rendle关于因式分解机的<a class="ae lx" href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>，如果我们假设每个x(j)向量仅在位置u和I处非零，我们得到带有偏差的经典矩阵因式分解模型(等式3):</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/4547c652b29052da23026ad243025dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*NDzahxyPrLmFjNOvngyviA.png"/></div></div></figure><p id="1594" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这两个方程之间的主要区别是因式分解机器在潜在向量方面引入了更高阶的相互作用，潜在向量也受到分类或标签数据的影响。这意味着模型超越了共现，以发现每个特征的潜在表示之间更强的关系。</p><p id="d752" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因式分解机器模型的损失函数就是均方误差和特征集的和，如公式10所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ob"><img src="../Images/bfe60a412fe96098e6f5fc691b739735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7uYJNZiwHJy1AI7S4s634A.png"/></div></div></figure><p id="42c9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们看看这在代码中是什么样子的:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="eb9e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个模型的完整实验可以在这里访问:<a class="ae lx" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/Factorization-Machines" rel="noopener ugc nofollow" target="_blank">https://github . com/khanhnamle 1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/Factorization-Machines</a></p><h2 id="8c40" class="ms mt iq bd mu mv mw dn mx my mz dp na lj nb nc nd ln ne nf ng lr nh ni nj iw bi translated">6-混合口味的矩阵分解</h2><p id="cba5" class="pw-post-body-paragraph la lb iq lc b ld nk ka lf lg nl kd li lj nm ll lm ln nn lp lq lr no lt lu lv ij bi translated">到目前为止，提出的技术隐含地将用户的口味视为单峰的——也就是在单个潜在向量中。这可能会导致在代表用户时缺乏细微差别，在这种情况下，主流口味可能会压倒更多的小众口味。此外，这可能降低项目表示的质量，减少属于多个品味/风格的项目组之间的嵌入空间的分离。</p><p id="daa1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae lx" href="https://arxiv.org/pdf/1711.08379.pdf" rel="noopener ugc nofollow" target="_blank">马切伊·库拉</a>提出并评估了将用户表示为几种不同口味的混合物，由不同的口味向量来表示。每个味道向量都与一个注意力向量相关联，描述了它在评估任何给定项目时的能力。然后，用户的偏好被建模为所有用户口味的加权平均值，权重由每个口味与评估给定项目的相关程度提供。</p><p id="4ced" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">等式11给出了这种混合味道模型的数学公式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/53a2bd9895f10457493d0f3204e4f499.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*iwTeaqFoX_TAwoHIOL2IJg.png"/></div></figure><ul class=""><li id="912c" class="ly lz iq lc b ld le lg lh lj ma ln mb lr mc lv md me mf mg bi translated">U_u是代表用户u的m个口味的m×k矩阵。</li><li id="1247" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">A_u是一个m×k矩阵，表示来自U_u的每一种口味的相似性，用于表示特定的项目。</li><li id="56a6" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">\sigma是软最大激活函数。</li><li id="1b7e" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">\sigma(A_u * q_i)给出混合概率。</li><li id="c60c" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">U_u * q_i给出了每种混合物成分的推荐分数。</li><li id="c055" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">请注意，我们假设所有混合成分的单位方差矩阵。</li></ul><p id="09b1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，下面的等式12表示损失函数:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/8f6da5723f666b2b99de0a16ee4ac58f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_4b2SyQUkZeRSxLNeYcoA.png"/></div></div></figure><p id="4c78" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们看看这在代码中是什么样子的:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="26b9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个模型的完整实验可以在这里访问:<a class="ae lx" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/MF-Mixture-Tastes" rel="noopener ugc nofollow" target="_blank">https://github . com/khanhnamle 1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/MF-Mixture-Tastes</a></p><h2 id="edfe" class="ms mt iq bd mu mv mw dn mx my mz dp na lj nb nc nd ln ne nf ng lr nh ni nj iw bi translated">7 —变分矩阵分解</h2><p id="d756" class="pw-post-body-paragraph la lb iq lc b ld nk ka lf lg nl kd li lj nm ll lm ln nn lp lq lr no lt lu lv ij bi translated">我想介绍的矩阵分解的最后一个变体叫做变分矩阵分解。到目前为止，这篇博文讨论的大部分内容是关于优化模型参数的点估计，而variable是关于优化后验估计，粗略地说，它表达了一系列与数据一致的模型配置。</p><p id="4e4f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">以下是改变的实际原因:</p><ul class=""><li id="a646" class="ly lz iq lc b ld le lg lh lj ma ln mb lr mc lv md me mf mg bi translated">变分法可以提供替代的正则化。</li><li id="9924" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">变分法可以度量你的模型不知道的东西。</li><li id="ca64" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">变分法可以揭示蕴涵以及分组数据的新方法。</li></ul><p id="68b4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们可以通过以下方式改变等式3中的矩阵分解:(1)用来自分布的样本替换点估计，以及(2)用正则化新的分布替换正则化该点。数学是相当复杂的，所以我不会试图在这篇博文中解释它。<a class="ae lx" href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" rel="noopener ugc nofollow" target="_blank">关于变分贝叶斯方法的维基百科页面</a>是一个有用的入门指南。最常见的变分贝叶斯使用<a class="ae lx" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> Kullback-Leibler散度</a>作为去相似性函数的选择，这使得损失最小化变得容易处理。</p><p id="f880" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们看看这在代码中是怎样的:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="fb32" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个模型的完整实验可以在这里访问:<a class="ae lx" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/Variational-MF" rel="noopener ugc nofollow" target="_blank">https://github . com/khanhnamle 1994/transfer-rec/tree/master/Matrix-Factorization-Experiments/variation-MF</a></p><h2 id="5cf5" class="ms mt iq bd mu mv mw dn mx my mz dp na lj nb nc nd ln ne nf ng lr nh ni nj iw bi translated">模型评估</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/7d22dd83295764512b290e1adfb42723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFjj9K_b8ktDUvli3z4HKg.png"/></div></div></figure><p id="9b2c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">你可以在<a class="ae lx" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Matrix-Factorization-Experiments" rel="noopener ugc nofollow" target="_blank"> this repository </a>查看我为MovieLens1M数据集做的所有7个矩阵分解实验。所有模型都经过50个时期的训练，结果在TensorBoard中捕获。评估指标是均方误差，即预测评分和实际评分之间所有平方差的总和。</p><p id="5de9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">结果表位于自述文件的底部，如您所见:</p><ul class=""><li id="eae1" class="ly lz iq lc b ld le lg lh lj ma ln mb lr mc lv md me mf mg bi translated">变分矩阵分解的训练损失最小。</li><li id="1bb4" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">具有边特征的矩阵分解具有最低的测试损失。</li><li id="2918" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">因式分解机器的训练时间最快。</li></ul><h2 id="803e" class="ms mt iq bd mu mv mw dn mx my mz dp na lj nb nc nd ln ne nf ng lr nh ni nj iw bi translated">结论</h2><p id="732b" class="pw-post-body-paragraph la lb iq lc b ld nk ka lf lg nl kd li lj nm ll lm ln nn lp lq lr no lt lu lv ij bi translated">在这篇文章中，我讨论了矩阵分解的直观意义及其在协同过滤中的应用。我还谈到了它的许多不同的扩展:(1)添加偏见，(2)添加侧面特征，(3)添加时间特征，(4)升级到因子分解机器以利用高阶交互，(5)使用带有“注意”机制的混合口味，(6)使模型变得不同。我希望您已经发现这种深入矩阵分解世界的数学方法是有帮助的。请继续关注本系列未来的博客文章，这些文章将超越矩阵分解的领域，深入探讨协作过滤的深度学习方法。</p><h2 id="abe7" class="ms mt iq bd mu mv mw dn mx my mz dp na lj nb nc nd ln ne nf ng lr nh ni nj iw bi translated">参考</h2><ul class=""><li id="cc4b" class="ly lz iq lc b ld nk lg nl lj of ln og lr oh lv md me mf mg bi translated"><a class="ae lx" href="https://dl.acm.org/doi/10.1109/MC.2009.263" rel="noopener ugc nofollow" target="_blank"> <em class="lw">推荐系统的矩阵分解技术</em> </a> <em class="lw">。</em>耶胡达·科伦，罗伯特·贝尔，克里斯·沃林斯基。2009年8月</li><li id="4938" class="ly lz iq lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated"><a class="ae lx" href="https://cemoody.github.io/simple_mf/" rel="noopener ugc nofollow" target="_blank"><em class="lw">py torch</em></a><em class="lw">中简单灵活的深度推荐器。克里斯·穆迪。2018</em></li></ul><p id="5ecc" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="lw">现在继续上</em> <a class="ae lx" rel="noopener" target="_blank" href="/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883"> <em class="lw">推荐系统第五部分</em> </a> <em class="lw">！</em></p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><p id="c2b7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="lw">如果你想关注我在推荐系统、深度学习和数据科学新闻方面的工作，你可以查看我的</em> <a class="ae lx" href="https://medium.com/@james_aka_yale" rel="noopener"> <em class="lw">中的</em> </a> <em class="lw">和</em><a class="ae lx" href="https://github.com/khanhnamle1994" rel="noopener ugc nofollow" target="_blank"><em class="lw">GitHub</em></a><em class="lw">，以及在</em><a class="ae lx" href="https://jameskle.com/" rel="noopener ugc nofollow" target="_blank"><em class="lw">【https://jameskle.com/】</em></a><em class="lw">的其他项目。你也可以在</em> <a class="ae lx" href="https://twitter.com/le_james94" rel="noopener ugc nofollow" target="_blank"> <em class="lw">推特</em> </a> <em class="lw">，</em> <a class="ae lx" href="mailto:khanhle.1013@gmail.com" rel="noopener ugc nofollow" target="_blank"> <em class="lw">直接发邮件给我</em> </a> <em class="lw">，或者</em> <a class="ae lx" href="http://www.linkedin.com/in/khanhnamle94" rel="noopener ugc nofollow" target="_blank"> <em class="lw">在LinkedIn上找我</em> </a> <em class="lw">。</em> <a class="ae lx" href="http://eepurl.com/deWjzb" rel="noopener ugc nofollow" target="_blank"> <em class="lw">注册我的简讯</em> </a> <em class="lw">就在你的收件箱里接收我关于数据科学、机器学习和人工智能的最新想法吧！</em></p></div></div>    
</body>
</html>