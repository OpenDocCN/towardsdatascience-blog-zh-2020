<html>
<head>
<title>7 tips to choose the best optimizer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">选择最佳优化器的 7 个技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e?source=collection_archive---------7-----------------------#2020-07-25">https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e?source=collection_archive---------7-----------------------#2020-07-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3bb3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">根据我的经验。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7249fe72b178cba1f94710f84e810a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A_j6HyHskjnrSJgr"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马库斯·温克勒在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="e6c7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="723a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在机器学习中，当我们需要计算预测值和实际值之间的距离时，我们使用所谓的损失函数。</p><p id="8ca6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">与许多人认为的相反，<strong class="lq ir">损失函数和成本函数</strong>不是一回事。虽然损失函数计算单个预测与其实际值的距离，但成本函数通常更通用。事实上，成本函数可以是，例如，训练集上的损失函数之和加上一些正则化。</p><p id="d971" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">另一个经常被错误地用作前两个的同义词的术语由目标函数表示，它是在训练期间被优化的任何函数的最一般的术语。</p><p id="5a2e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一旦澄清了正确的术语，我们就可以给出优化器的定义。</p><p id="f269" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">机器学习中的优化器用于调整神经网络的参数，以便<strong class="lq ir">最小化成本函数。</strong></p><p id="96e6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，优化器的选择是一个重要的方面，可以区分好的训练和坏的训练。</p><p id="51c5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">实际上，有许多优化器，所以选择并不简单。在本文中，我将简要描述最常用的优化器，然后给你一些在不同项目中帮助过我的指导原则，我希望在为你的任务选择最合适的优化器时，这些指导原则也能帮助你。</p><p id="d64a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">关于具体优化器如何工作的进一步信息，你可以看看这个<a class="ae kv" href="https://ruder.io/optimizing-gradient-descent/index.html" rel="noopener ugc nofollow" target="_blank">网站</a>。</p><h1 id="517e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">有多少优化者？</h1><p id="b1e3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">就像刚才说的，有很多优化器。它们中的每一种都有与特定任务相关的优点和缺点。</p><p id="6006" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我喜欢把优化器分为两个家族:梯度下降优化器和<em class="mp">自适应</em>优化器。这种划分完全基于一个操作方面，在梯度下降算法的情况下，强制您手动调整<strong class="lq ir">学习率</strong>，而在自适应算法中自动调整<em class="mp"/>——这就是我们使用这个名称的原因。</p><p id="2e87" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">梯度下降:</strong></p><ul class=""><li id="5679" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">批量梯度下降</li><li id="63b0" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">随机梯度下降</li><li id="b329" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">小批量梯度下降</li></ul><p id="f0c2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">自适应:</strong></p><ul class=""><li id="d344" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">阿达格拉德</li><li id="0acd" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">阿达德尔塔</li><li id="8991" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">RMSprop</li><li id="7a46" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</li></ul><h1 id="8b26" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">梯度下降优化器</h1><p id="9c81" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">有三种类型的梯度下降优化器，它们在我们使用多少数据来计算目标函数的梯度方面有所不同。</p><h2 id="dddc" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">批量梯度下降</h2><p id="ef58" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">也被称为香草梯度下降，这是三种算法中最基本的算法。它为整个训练集计算目标函数<em class="mp"> J </em>相对于参数<em class="mp"> θ </em>的梯度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c88e3f91ce0a1163e3cdeaab6a97943d.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/0*vlKJxSNXnpfWPw0v"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">批量梯度下降中更新重量的公式。梯度乘以学习速率η，然后用于更新网络的参数。</p></figure><p id="9035" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">由于我们使用整个数据集仅执行<strong class="lq ir">一步，</strong>批量梯度下降可能会非常慢。此外，它不适合不适合内存的数据集。</p><h2 id="20c6" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">随机梯度下降</h2><p id="6bd3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">它是批量梯度下降的改进版本。它不是计算整个数据集的梯度，而是为数据集中的每个示例执行参数更新<strong class="lq ir">。</strong></p><p id="8bc0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，该公式现在还取决于输入<em class="mp"> x </em>和输出<em class="mp"> y </em>的值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/d4570ee9e2cad94be0aaacf3fea6d1d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/0*4v19i2QuYBna62Q8"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用于更新 SGD 中权重的公式</p></figure><p id="70d4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">SGD 的问题是更新频繁且具有高方差，因此目标函数在训练期间波动很大。</p><p id="f26e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">相对于批量梯度下降，这种波动可能是有利的，因为它允许函数跳到更好的局部最小值，但同时它可能代表相对于在特定局部最小值中的收敛的不利之处。</p><p id="aabe" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个问题的解决方案是缓慢降低学习率值，以便使更新越来越小，从而避免高振荡。</p><h2 id="b6f8" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">小批量梯度下降</h2><p id="6857" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这种算法背后的直觉是利用两种梯度下降方法的优势，我们已经看到了。</p><p id="5320" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">它主要计算小批量数据的梯度，以减少更新的方差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/533192dd3f2323bb2a82b246474769a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/0*dEorRmc_Btu6n4O1"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在小批量梯度下降中更新重量的公式</p></figure><h1 id="91fe" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">外卖#1 </strong></h1><ul class=""><li id="f7b3" class="mq mr iq lq b lr ls lu lv lx nt mb nu mf nv mj mv mw mx my bi translated"><strong class="lq ir">小批量梯度下降是大多数情况下三者中的最佳选择</strong>。</li><li id="9bcb" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated"><strong class="lq ir">学习率调优问题:</strong>都受制于一个好的学习率的选择。不幸的是，这个选择并不简单。</li><li id="0401" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated"><strong class="lq ir">不适合稀疏数据:</strong>没有机制来证明很少出现的特征。所有参数同等更新。</li><li id="8cbf" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">陷入<strong class="lq ir">次优局部最小值</strong>的可能性很高。</li></ul><h1 id="f3a3" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">自适应优化器</h1><p id="4258" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这个优化器家族已经被引入来解决梯度下降算法的问题。它们最重要的特点是不需要调整学习率值。实际上，一些库——比如 Keras——仍然允许你手动调整它来进行更高级的测试。</p><h2 id="7c1f" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">阿达格拉德</h2><p id="12ab" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">它使学习速率适应参数，对频繁出现的特征进行小更新，对最罕见的特征进行大更新。</p><p id="f11d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">通过这种方式，网络能够捕获属于不经常出现的特征的信息，将它们放在证据中，并给予它们适当的权重。</p><p id="7ad1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Adagrad 的问题是，它根据所有过去的梯度来调整每个参数的学习速率。因此，在高数量的步骤之后具有非常小的学习率的可能性——由所有过去梯度的累积产生——是相关的。</p><p id="e4cb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果学习率太小，我们就不能更新权重，结果是网络不再学习。</p><h2 id="dec1" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">阿达德尔塔</h2><p id="afd2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">它通过引入历史窗口来改进先前的算法，该历史窗口设置了在训练期间要考虑的固定数量的过去梯度。</p><p id="dae7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这样，我们就不存在学习率消失的问题。</p><h2 id="69f8" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">RMSprop</h2><p id="8d99" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">它与 Adadelta 非常相似。唯一的区别是他们管理过去梯度的方式。</p><h2 id="47b0" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h2><p id="0e66" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">它增加了 Adadelta 和 RMSprop 的优点，即存储类似于动量的过去梯度的指数衰减平均值。</p><h1 id="04f7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">外卖#2</h1><ul class=""><li id="66d2" class="mq mr iq lq b lr ls lu lv lx nt mb nu mf nv mj mv mw mx my bi translated"><strong class="lq ir">亚当在大多数情况下是自适应优化者</strong>中最好的 <strong class="lq ir">。</strong></li><li id="ee96" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated"><strong class="lq ir">擅长稀疏数据</strong>:自适应学习速率非常适合这种类型的数据集。</li><li id="416c" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">没有必要关注学习率值</li></ul><h1 id="94c5" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">梯度下降与自适应</h1><p id="e883" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">总的来说亚当是最好的选择。无论如何，许多最近的论文指出，如果结合一个良好的学习率退火计划，SGD 可以带来更好的结果，该计划旨在管理其在培训期间的价值。</p><p id="bd2a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我的建议是<strong class="lq ir">在任何情况下都首先尝试 Adam</strong>，因为它更有可能在没有高级微调的情况下返回好的结果。</p><p id="e355" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，如果 Adam 取得了好的结果，那么打开 SGD 看看会发生什么是个好主意。</p></div></div>    
</body>
</html>