<html>
<head>
<title>Keep Calm and Stack Up— Implement Stacking Regression in Python using mlxtend</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">保持冷静和堆叠—使用 mlxtend 在 Python 中实现堆叠回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/just-keep-stacking-implement-stacking-regression-in-python-using-mlxtend-3250ff327ee5?source=collection_archive---------28-----------------------#2020-09-06">https://towardsdatascience.com/just-keep-stacking-implement-stacking-regression-in-python-using-mlxtend-3250ff327ee5?source=collection_archive---------28-----------------------#2020-09-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/5c63a64a009d5676e1bc5e08e35155be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tMR9qc350bXHcE-H"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://unsplash.com/@jdubs?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">王占山</a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="9359" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你曾经在一些 Kaggle 比赛中结合多个 ML 模型来提高你在排行榜上的分数，你知道这是怎么回事。事实上，这些 Kaggle 竞赛的许多获胜解决方案使用集合模型，而不仅仅是单一的微调模型。</p><p id="3095" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">集成模型背后的直觉非常简单:有效地组合不同的 ML 算法可以降低不幸选择一个差模型的风险。</p><p id="762e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我将讨论<strong class="ki iu">堆叠</strong>，一种流行的集成方法，以及如何使用<em class="le"> mlxtend </em>库在 Python 中实现一个简单的 2 层堆叠回归模型。我选择的示例任务是 Airbnb 价格预测。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="54c2" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">古玩目录</h1><ol class=""><li id="987a" class="mk ml it ki b kj mm kn mn kr mo kv mp kz mq ld mr ms mt mu bi translated"><a class="ae kf" href="#ce45" rel="noopener ugc nofollow">什么是堆叠，它是如何工作的？</a></li><li id="0dd6" class="mk ml it ki b kj mv kn mw kr mx kv my kz mz ld mr ms mt mu bi translated"><a class="ae kf" href="#7224" rel="noopener ugc nofollow">与基本型号相比，堆叠型号的性能如何？</a></li><li id="6d1c" class="mk ml it ki b kj mv kn mw kr mx kv my kz mz ld mr ms mt mu bi translated"><a class="ae kf" href="#16b4" rel="noopener ugc nofollow">堆叠有哪些注意事项？</a></li><li id="fc60" class="mk ml it ki b kj mv kn mw kr mx kv my kz mz ld mr ms mt mu bi translated"><a class="ae kf" href="#8ac8" rel="noopener ugc nofollow">接下来我们能做什么？</a></li></ol></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="ce45" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">概观</h1><p id="d8a3" class="pw-post-body-paragraph kg kh it ki b kj mm kl km kn mn kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">堆叠，或堆叠泛化，是一种元学习算法，它学习如何以最佳方式组合每个基础算法的预测[1]。</p><p id="bb96" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简单来说，这就是如何建立一个两层堆叠模型。第一层，有些人称之为基础层，包括基础模型。在分类的上下文中，您可以将基本模型视为任何可用于进行预测的分类器，如神经网络、SVM、决策树等。类似的逻辑也适用于回归。在我们使用训练集训练这些模型之后，第二层通过从这些模型中获取预测以及对<strong class="ki iu">样本外</strong>数据的预期输出来构建元模型。你可以想到这种元模型的最简单的情况是平均出基础模型的预测。</p><p id="56c0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">避免过度拟合的常见方法是在这些基础模型上执行<a class="ae kf" href="https://machinelearningmastery.com/k-fold-cross-validation/" rel="noopener ugc nofollow" target="_blank">交叉验证</a>，然后使用折叠外预测和输出来构建元模型。</p><p id="38a1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看大卫·沃尔菲特的论文，了解更多关于堆叠工作原理的技术细节。</p><p id="3c18" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我选择了 Kaggle ( <a class="ae kf" href="https://www.kaggle.com/c/digit-recognizer" rel="noopener ugc nofollow" target="_blank"> link </a>)的 Airbnb 价格预测任务作为这里的示例，因为它是一个相对简单的数据集，具有较小的样本大小和一组功能。任务是根据卧室数量、租赁类型(整个公寓或私人房间或合租房间)等信息预测 Airbnb 上的公寓租赁挂牌价格。</p><p id="3f0b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要查看完整的 Python 代码，请查看我的<a class="ae kf" href="https://www.kaggle.com/dehaozhang/stacking-ensemble" rel="noopener ugc nofollow" target="_blank"> Kaggle 内核</a>。</p><p id="3c3a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事不宜迟，让我们进入细节！</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="5d9b" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">探测</h1><p id="40df" class="pw-post-body-paragraph kg kh it ki b kj mm kl km kn mn kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">出于本文的目的，我不会讨论预处理步骤，但请参考 Kaggle 内核了解全部细节。</p><p id="3479" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在高层次上，我检查了每个特性的分布，删除了异常值，为分类“room_type”创建了虚拟变量(因为只有三个类别)，并对特性进行了标准化。</p><p id="a5dd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">训练/测试分割</strong></p><p id="db68" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将数据集的 70%设置为训练集，剩下的 30%将用作测试集。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="54ca" class="nm ln it ni b gy nn no l np nq">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)</span></pre><p id="bf00" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">基本型号</strong></p><p id="7717" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先建立一些基础模型。这里我选择了<strong class="ki iu"> Xgboost </strong>、<strong class="ki iu">套索回归</strong>和<strong class="ki iu"> KNN </strong>作为三个基础模型。原则上，当模型在不同的范围内，并且它们的预测误差尽可能不相关时，叠加效果最好，这样单个模型的弱点和偏差可以被其他模型的优点抵消[2]。</p><p id="fab7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">绩效指标</strong></p><p id="4144" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在评估模型的性能度量方面，我选择使用<strong class="ki iu">平均绝对误差(MAE) </strong>，它衡量预测值与测量值的差距。</p><p id="9bd6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">基本型号的性能</strong></p><p id="cb81" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们快速检查三个基本模型的性能(所有默认超参数):</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="caa4" class="nm ln it ni b gy nn no l np nq">Model: XGBRegressor, MAE: 35.80629559761084<br/>Model: Lasso, MAE: 35.0691542225316<br/>Model: KNeighborsRegressor, MAE: 38.35917874396135</span></pre><p id="0876" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">MAEs 在 35-38 之间，这意味着预测价格平均与真实价格相差 35-38 美元。</p><p id="7224" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">使用 mlxtend </strong>构建堆叠模型</p><p id="a8f7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们可以为元学习者建立基础模型。或者，我们可以使用<em class="le"> mlxtend </em>库(<a class="ae kf" href="http://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/#api" rel="noopener ugc nofollow" target="_blank">文档</a>)中的‘StackingCVRegressor’来创建快捷方式:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="921b" class="nm ln it ni b gy nn no l np nq">from mlxtend.regressor import StackingCVRegressor</span><span id="29e0" class="nm ln it ni b gy nr no l np nq">stack = StackingCVRegressor(regressors=(XGBRegressor(), <br/>                            Lasso(), KNeighborsRegressor()),<br/>                            meta_regressor=Lasso(), cv=10,<br/>                            use_features_in_secondary=True,<br/>                            store_train_meta_features=True,<br/>                            shuffle=False,<br/>                            random_state=1)</span></pre><p id="99a5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用相同的三个基础模型，对于元学习者，我选择使用 Lasso。我们也可以尝试其他回归模型作为元学习者，然后在上面构建另一个元学习者(三层堆叠！)注意，在这种配置中，我还选择使用原始训练数据作为元模型的输入，因为它可以为元模型提供更多的上下文。</p><p id="8842" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们将训练数据放入模型中，并在测试集上检查性能:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="0211" class="nm ln it ni b gy nn no l np nq">stack.fit(X_train, y_train)<br/>X_test.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11'] # Xgboost internally renames the features<br/>pred = stack.predict(X_test)<br/>score = mean_absolute_error(y_test, pred)<br/>print('Model: {0}, MAE: {1}'.format(type(stack).__name__, score))<br/>--------------------------------------------------------------------<br/>Model: StackingCVRegressor, MAE: 34.01755981865483</span></pre><p id="b7e5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们看到新的 MAE 大约是 34，与最佳基础模型 Lasso 的结果相比减少了 1。</p><p id="0ad9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然这种差异看起来微不足道，但像这样的小收获实际上可以在排行榜上产生巨大的差异。</p><p id="10cd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想研究集成神经网络，看看这篇论文。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="16b4" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">警告</h1><p id="f98d" class="pw-post-body-paragraph kg kh it ki b kj mm kl km kn mn kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">尽管有堆叠功能，但还是要记住以下几点:</p><ol class=""><li id="64bc" class="mk ml it ki b kj kk kn ko kr ns kv nt kz nu ld mr ms mt mu bi translated">有效地使用堆栈有时需要一些尝试和错误，并且不能保证堆栈在所有情况下都会提高性能。</li><li id="9ddc" class="mk ml it ki b kj mv kn mw kr mx kv my kz mz ld mr ms mt mu bi translated">堆叠在计算上可能是昂贵的，尤其是当堆叠层数很高时。</li><li id="79d5" class="mk ml it ki b kj mv kn mw kr mx kv my kz mz ld mr ms mt mu bi translated">随着叠加层数的增加，模型的可解释性降低。对于 Kaggle 竞争来说，这不是一个大问题，但是对于商业案例来说，理解特性的重要性及其对结果变量的增量影响可能更重要。</li></ol></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="8ac8" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">后续步骤</h1><p id="d09d" class="pw-post-body-paragraph kg kh it ki b kj mm kl km kn mn kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">以下是我们接下来可以尝试的几件事:</p><ol class=""><li id="a585" class="mk ml it ki b kj kk kn ko kr ns kv nt kz nu ld mr ms mt mu bi translated">超参数调整—我们可以在选定的模型中调整一些超参数，以进一步优化性能。在这个过程中,“GridSearchCV”可能是一个有用的工具。</li><li id="7822" class="mk ml it ki b kj mv kn mw kr mx kv my kz mz ld mr ms mt mu bi translated">继续堆叠—尝试构建额外的堆叠层，并绘制性能指标与层数的关系图，以检查边际回报递减是否成立(应该成立！).</li></ol></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="1cbe" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">摘要</h1><p id="1c22" class="pw-post-body-paragraph kg kh it ki b kj mm kl km kn mn kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">让我们快速回顾一下。</p><p id="6501" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用<em class="le"> mlxtend </em>库在 Python 中实现了一个简单的两层堆叠回归模型，将其测试 MAE 与三个基本模型的测试 MAE 进行了比较，并观察到了改进。</p><p id="e412" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这篇博文，并请分享你的想法:)</p><p id="0559" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看我的另一篇关于使用 t-SNE 降维的文章:</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/dimensionality-reduction-using-t-distributed-stochastic-neighbor-embedding-t-sne-on-the-mnist-9d36a3dd4521"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">在 MNIST 上使用 t-分布随机邻居嵌入(t-SNE)进行降维…</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">t-SNE vs PCA vs PCA &amp; t-SNE</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om jz ny"/></div></div></a></div></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="fd8b" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">参考</h1><p id="533d" class="pw-post-body-paragraph kg kh it ki b kj mm kl km kn mn kp kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">[1]<a class="ae kf" href="https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/" rel="noopener ugc nofollow" target="_blank">https://machinelementmastery . com/stacking-ensemble-machine-learning-with-python/</a><br/><a class="ae kf" href="http://support.sas.com/resources/papers/proceedings17/SAS0437-2017.pdf" rel="noopener ugc nofollow" target="_blank">http://support . SAS . com/resources/papers/proceedings 17/SAS 0437-2017 . pdf</a></p></div></div>    
</body>
</html>