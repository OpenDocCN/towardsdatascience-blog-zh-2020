<html>
<head>
<title>Building a LSTM by hand on PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在PyTorch上手工制作LSTM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091?source=collection_archive---------3-----------------------#2020-05-25">https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091?source=collection_archive---------3-----------------------#2020-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4146" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">能够从零开始建立一个LSTM细胞使你能够在架构上做出自己的改变，并把你的研究带到一个新的水平。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/13f0afea79a21829e37f80551cd539e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*s7_EO0rjXAw99RnH1x4s_g.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">LSTM细胞图解—来源:<a class="ae ku" href="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/300px-The_LSTM_cell.png" rel="noopener ugc nofollow" target="_blank">https://upload . wikimedia . org/Wikipedia/commons/thumb/3/3b/The _ LSTM _ cell . png/300 px-The _ LSTM _ cell . png</a>—2020年5月24日获取</p></figure><p id="00d6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">所有提到的代码都在下面的列表中或者在<a class="ae ku" href="https://github.com/piEsposito/pytorch-lstm-by-hand" rel="noopener ugc nofollow" target="_blank">我们的回购</a>中。</p><p id="bce2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">LSTM细胞是深度学习的递归神经网络研究领域中最有趣的架构之一:它不仅使模型能够从长序列中学习，而且还为长期和短期记忆创建了一个数字抽象，能够在需要时用一个代替另一个。</p><p id="239a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在这篇文章中，我们不仅要浏览LSTM单元的架构，还要在PyTorch上手工实现它。</p><p id="7ab4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后但同样重要的是，我们将展示如何对我们的实现做一些小的调整，以实现一些确实出现在LSTM研究领域的新想法，如窥视孔连接。</p><h1 id="4ec8" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">LSTM建筑</h1><p id="0e96" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">LSTM有翼被称为门控结构:一些数学运算的组合，使信息从计算图上的那个点流动或被保留。正因为如此，它能够在长期记忆和短期记忆之间“做出决定”,并对序列数据输出可靠的预测:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/efcf7cba2782c6cfb2233519a1958a18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VXNy36Ay_Rq3m1LE.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">LSTM单元格中的预测序列。注意，它不仅流过预测h_t，还流过c_t，它是长期记忆的代表。来源:<a class="ae ku" href="https://medium.com/turing-talks/turing-talks-27-modelos-de-predi%C3%A7%C3%A3o-lstm-df85d87ad210" rel="noopener">https://medium . com/turing-talks/turing-talks-27-modelos-de-predi % C3 % A7 % C3 % A3o-lstm-df 85d 87 ad 210</a>。访问时间:2020年5月24日</p></figure><p id="7cef" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们将一部分一部分地讲述:</p><h2 id="aa40" class="mt ls it bd lt mu mv dn lx mw mx dp mb le my mz md li na nb mf lm nc nd mh ne bi translated">遗忘之门</h2><p id="bcb3" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">遗忘门是输入信息和候选信息一起操作的门，作为长期记忆。请注意，在输入、隐藏状态和偏置的第一个线性组合上，应用了一个sigmoid函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/972543ce9a5869506a1f26c999cda3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*huVOAJFuhX3bSWH5.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">忘记LSTM牢房的门。来源:<a class="ae ku" href="https://medium.com/turing-talks/turing-talks-27-modelos-de-predi%C3%A7%C3%A3o-lstm-df85d87ad210" rel="noopener">https://medium . com/turing-talks/turing-talks-27-modelos-de-predi % C3 % A7 % C3 % A3o-lstm-df 85d 87 ad 210</a>。访问时间:2020年5月24日</p></figure><p id="5c45" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这个sigmoid将遗忘门的输出从0“缩放”到1——通过将其乘以候选值，我们可以将其设置为零，这表示长时间记忆中的“遗忘”,或者设置为一个更大的数字，这表示我们从长时间记忆中记住了“多少”。</p><h2 id="0ad2" class="mt ls it bd lt mu mv dn lx mw mx dp mb le my mz md li na nb mf lm nc nd mh ne bi translated">新长时记忆的输入门及其解决方案</h2><p id="4ec5" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">输入门是包含在输入和隐藏状态上的信息被组合，然后与候选和部分候选c'_t一起操作的地方:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/860aa931ec5a9967620218bed1a98d97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lipRMhGsnzGrEhaS.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">LSTM池的输入门。来源:<a class="ae ku" href="https://medium.com/turing-talks/turing-talks-27-modelos-de-predi%C3%A7%C3%A3o-lstm-df85d87ad210" rel="noopener">https://medium . com/turing-talks/turing-talks-27-modelos-de-predi % C3 % A7 % C3 % A3o-lstm-df 85d 87 ad 210</a>。访问时间:2020年5月24日</p></figure><p id="fc19" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在这些操作中，决定了有多少新信息将被引入内存以及它将如何改变——这就是为什么我们使用一个双曲正切函数(“从-1到1的标度”)。我们结合来自短时和长时记忆的部分候选项，并将其设置为候选项。</p><p id="f61e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在我们可以进入输出门了。</p><h1 id="bb02" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">单元的输出门和隐藏状态(输出)</h1><p id="8504" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">之后，我们可以收集o_t作为LSTM细胞的输出门，然后将其乘以已经通过适当操作更新的候选(长期记忆)的tanh。网络的输出将是h_t。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/ebdeeafc374cbf47bcf28af2f9fc6a09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lirs3HLo70pNlj0O.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">LSTM电池的输出门。来源:<a class="ae ku" href="https://medium.com/turing-talks/turing-talks-27-modelos-de-predi%C3%A7%C3%A3o-lstm-df85d87ad210" rel="noopener">https://medium . com/turing-talks/turing-talks-27-modelos-de-predi % C3 % A7 % C3 % A3o-lstm-df 85d 87 ad 210</a>。访问时间:2020年5月24日</p></figure><p id="f0c9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后，我们有:</p><h2 id="8544" class="mt ls it bd lt mu mv dn lx mw mx dp mb le my mz md li na nb mf lm nc nd mh ne bi translated">LSTM电池的方程式:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/b4982f8c4268286450f43f0769dd798c.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*cmpPLozIVuuZQ2EauZIZFg.png"/></div></figure><h1 id="a932" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">在PyTorch上实现它</h1><p id="f9d0" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">为了在PyTorch上实现它，我们将首先进行适当的导入。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="619b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们现在将通过从<code class="fe ni nj nk nl b">nn.Module</code>继承来创建它的类，然后实例化它的参数和权重初始化，您将在下面看到(注意它的形状由网络的输入大小和输出大小决定):</p><h2 id="ba04" class="mt ls it bd lt mu mv dn lx mw mx dp mb le my mz md li na nb mf lm nc nd mh ne bi translated">设置参数</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="d661" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了了解每个砝码的形状，让我们看看:</p><p id="142b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">矩阵的输入形状为(batch_size，sequence_length，feature_length)-因此将乘以序列中每个元素的权重矩阵必须具有形状(feature _ length，output_length)。</p><p id="c713" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">序列中每个元素的隐藏状态(也称为输出)的形状为(batch_size，output_size)，这在序列处理结束时会产生(batch_size，sequence_length，output_size)的输出形状。—因此，与其相乘的weight_matrix必须具有与单元格的参数hidden_sz相对应的形状(output_size，output_size)。</p><p id="6ad4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里是权重初始化，我们使用的和PyTorch默认的一样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h2 id="f6ea" class="mt ls it bd lt mu mv dn lx mw mx dp mb le my mz md li na nb mf lm nc nd mh ne bi translated">前馈操作</h2><p id="cb51" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">前馈操作接收<code class="fe ni nj nk nl b">init_states</code>参数，该参数是具有上述等式的(h_t，c_t)参数的元组，如果不引入，该参数被设置为零。然后，我们对保持(h_t，c_t)的每个序列元素执行LSTM方程的前馈，并引入它作为序列的下一个元素的状态。</p><p id="e1ae" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后，我们返回预测和最后的状态元组。让我们看看它是如何发生的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h2 id="e42d" class="mt ls it bd lt mu mv dn lx mw mx dp mb le my mz md li na nb mf lm nc nd mh ne bi translated">现在和优化版本</h2><p id="069f" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">这种LSTM在运算方面是正确的，但在计算时间方面不是很优化:我们分别执行8次矩阵乘法，这比以向量化的方式执行要慢得多。我们现在将展示如何通过将它减少到2个矩阵乘法来实现，这将使它更快。</p><p id="06a1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了进行这种操作，我们设置两个矩阵U和V，它们的权重包含在4次矩阵乘法中。然后，我们对已经通过线性组合+偏置运算的矩阵执行门控运算。</p><p id="343d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">通过矢量化运算，LSTM单元的方程将为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/96bdff49f93df46ee6ba0efb93027e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*glHcEaVYSqhSgonWQ4zFcQ.png"/></div></figure><p id="8901" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">所以它的<code class="fe ni nj nk nl b">nn.Module</code>类应该是:</p><h2 id="9699" class="mt ls it bd lt mu mv dn lx mw mx dp mb le my mz md li na nb mf lm nc nd mh ne bi translated">优化的LSTM细胞等级</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="2f8c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后但同样重要的是，我们可以展示使用LSTM窥视孔连接来调整您的实现有多容易。</p><h2 id="b453" class="mt ls it bd lt mu mv dn lx mw mx dp mb le my mz md li na nb mf lm nc nd mh ne bi translated">LSTM窥视孔</h2><p id="a5c2" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">LSTM窥视孔对其前馈操作进行了细微调整，优化后的情况是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e54449cc21524539f32ab62af6e40de6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*83ZFW20BItVrG4-Ghhiceg.png"/></div></figure><p id="e212" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">有了LSTM的良好实现和优化实现，我们就可以添加窥视孔连接的选项，并做一些小的调整:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="616f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这样我们的LSTM就完成了。你可能想在<a class="ae ku" href="https://github.com/piEsposito/pytorch-lstm-by-hand" rel="noopener ugc nofollow" target="_blank">我们的回购</a>上看到它，并用我们的LSTM文本情感分析笔记本进行测试，我们准备用火炬LSTM内置层进行测试和比较。</p><h1 id="f311" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">结论</h1><p id="e1a2" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">我们可以得出一个结论，尽管这是一种深度学习的禁忌，但是如果一步一步地完成，并且有干净和良好的编码，实际上很容易将它的操作执行成一个干净、易于使用的<code class="fe ni nj nk nl b">nn.Module</code>。我们也看到了改变和调整它的连接来完成像窥视孔连接这样的操作是多么容易。</p><h1 id="f322" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">参考</h1><div class="no np gp gr nq nr"><a href="https://github.com/piEsposito/pytorch-lstm-by-hand" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">piEsposito/py torch-lstm-手工</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">一个关于如何制作LSTM神经网络的小而简单的教程。PyTorch上的手工模块。记得执行bash…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">github.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of ko nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">了解LSTM网络</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">2015年8月27日发布人类不是每秒钟都从零开始思考。当你读这篇文章时，你…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">colah.github.io</p></div></div></div></a></div><div class="no np gp gr nq nr"><a href="https://medium.com/turing-talks/turing-talks-27-modelos-de-predi%C3%A7%C3%A3o-lstm-df85d87ad210" rel="noopener follow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">LSTM神经网</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">机器学习</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">medium.com</p></div></div><div class="oa l"><div class="og l oc od oe oa of ko nr"/></div></div></a></div></div></div>    
</body>
</html>