<html>
<head>
<title>GPT-3: The New Mighty Language Model from OpenAI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPT-3:来自OpenAI的新的强大语言模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gpt-3-the-new-mighty-language-model-from-openai-a74ff35346fc?source=collection_archive---------7-----------------------#2020-05-31">https://towardsdatascience.com/gpt-3-the-new-mighty-language-model-from-openai-a74ff35346fc?source=collection_archive---------7-----------------------#2020-05-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="200f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用175B参数将深度学习推向极限</h2></div><h1 id="aef5" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="c998" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae lt" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>最近<a class="ae lt" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank">发布了其新的强大<em class="lu">语言模型</em> GPT-3的</a>预印本。它是其前身GPT-2的更大更好的版本。事实上，由于有接近175B的可训练参数，GPT-3在尺寸上比其他任何东西都要大得多。这里是最近流行的预训练NLP模型的一些参数的比较，GPT-3明显突出。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/c980ca9fa09dda1004e5e811f6ef5c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*C-KNWQC_wXh-Q2wc6VPK1g.png"/></div></figure><h1 id="7189" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">有什么新鲜事？</h1><p id="2197" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在Bert成功之后，NLP领域越来越多地朝着创建<em class="lu">预训练</em> <em class="lu">语言模型</em>的方向发展，这些语言模型在巨大的文本语料库上进行训练(以无监督的方式)，随后使用更小的特定任务数据集在特定任务上进行微调，例如翻译、问题回答等。</p><p id="8f0c" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">虽然这种类型的<em class="lu">迁移学习</em>消除了使用特定于任务的模型架构的需要，但您仍然需要特定于任务的数据集，这很难收集，以实现良好的性能。</p><p id="416a" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">相比之下，人类以一种非常不同的方式学习，并且有能力根据很少的例子来学习一项新的任务。GPT-3旨在解决这一特定的痛点，即它是一个任务不可知的模型，它需要零到非常有限的例子来做得很好，并在许多NLP任务上实现接近艺术状态的性能</p><h1 id="e208" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">术语</h1><p id="74f6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在我们深入探讨之前，定义一些常用术语可能是有用的:</p><ul class=""><li id="6d1e" class="mi mj iq kz b la md ld me lg mk lk ml lo mm ls mn mo mp mq bi translated"><strong class="kz ir"> NPL任务:</strong>这些是与人类语言有关的任务，例如语言翻译、文本分类(例如情感提取)、阅读理解、命名实体识别(例如识别文本中的人、位置、公司名称)</li><li id="fb69" class="mi mj iq kz b la mr ld ms lg mt lk mu lo mv ls mn mo mp mq bi translated">语言模型:这些模型可以预测给定的一组单词中最有可能出现的下一个单词(以及它们的概率)。事实证明，这种类型的模型对许多其他任务也是有用的，尽管它们可能是在普通的下一个单词预测上训练的</li><li id="d8f4" class="mi mj iq kz b la mr ld ms lg mt lk mu lo mv ls mn mo mp mq bi translated"><strong class="kz ir">零个/一个/几个镜头学习:</strong>指的是模特通过观看一项新任务的零个/一个/几个例子来学习该任务的能力</li><li id="a173" class="mi mj iq kz b la mr ld ms lg mt lk mu lo mv ls mn mo mp mq bi translated"><strong class="kz ir">迁移学习:</strong>指的是深度学习中的概念，其中你为一项任务(例如图像中的对象检测)训练一个模型，但为其他一些不同的任务(例如评估MRI扫描)利用和建立该模型的能力。在计算机视觉取得巨大成功后，它现在在NLP中很流行。</li><li id="7820" class="mi mj iq kz b la mr ld ms lg mt lk mu lo mv ls mn mo mp mq bi translated"><strong class="kz ir"> Transformer Models </strong>:深度学习模型系列，主要用于NLP，它构成了当今大多数最先进的NLP架构的基本构建模块。你可以在我之前的<a class="ae lt" rel="noopener" target="_blank" href="/recent-advancements-in-nlp-2-2-df2ee75e189">博客</a>中读到更多关于<em class="lu">变形金刚</em>的内容</li></ul><h1 id="97b3" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">方法</h1><p id="63c8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">该模型是使用<em class="lu"> Transformer </em>、<em class="lu"> Attention </em>等标准概念，并使用典型的<em class="lu">普通爬虫、维基百科、书籍</em>和一些额外的数据源构建的。很多东西——预训练、模型、数据都类似于GPT新协议，但所有东西(模型大小、数据大小、训练时间)都要大得多。事实上，它巨大的体积是这款车型的最大优势。</p><p id="d2a9" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">下图显示了作为模型参数数量函数的各种零/一/少量发射任务的精度优势，很明显，由于尺寸按比例放大，获得了较大的收益。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/b5a602e5af35448851cf5d1f8575d4ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-iY_0W7uX-D30qjCv-OBmg.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">资料来源:报纸</p></figure><p id="db63" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">模型中使用的大部分东西都非常巨大——例如96 <em class="lu">注意</em>层，3.2M的<em class="lu">批量</em>，175B <em class="lu">参数</em>——它们与过去的任何东西都不一样。就参数数量而言，该模型比下一个最接近的模型(微软图灵NLG，17B参数)大10倍</p><p id="80b5" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">使用GPT-3模型执行各种任务时，无需进行梯度/参数更新(微调)。人们可以使用自然语言与模型进行交互，并且/或者提供一些您正在尝试执行的任务的示例，模型就会执行这些任务！</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/4b53f5d09c838dbf9848e154173dd8c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*gtoP7YMNNKr0bvN6JxWp5w.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">资料来源:报纸</p></figure><h1 id="9233" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">这一切意味着什么？</h1><p id="729b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">除了不需要特定于任务的模型架构之外，不需要大型定制的特定于任务的数据集的概念是朝着使前沿NLP更易访问的方向迈出的一大步。</p><p id="0235" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">虽然GPT-3在许多NLP任务上表现出色，例如单词预测、常识推理，但它并不是在所有事情上都做得一样好。例如，它在文本合成、一些阅读理解任务等方面表现不佳。除此之外，它还受到数据中的<a class="ae lt" rel="noopener" target="_blank" href="/can-ai-algorithms-be-biased-6ab05f499ed6">偏差</a>的影响，这可能导致模型生成刻板的或有偏见的内容。因此，这里还有更多工作要做。</p><p id="bd78" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">除此之外，GPT-3的巨大体积使它几乎不为任何人所知，除了世界上少数几家公司和研究实验室。根据作者的说法，该模型非常通用，包含特定任务不需要的非常广泛的技能，并且可能有使用<a class="ae lt" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank"><em class="lu"/></a>概念创建更小、更易管理的特定任务模型的范围。</p><p id="0637" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">看到这个东西在未来如何发展会很令人兴奋。</p></div></div>    
</body>
</html>