<html>
<head>
<title>Include these Spark Window Functions in your Data Science Workflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在您的数据科学工作流中包含这些 Spark 窗口函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/include-these-spark-window-functions-in-your-data-science-workflow-c6bac5824475?source=collection_archive---------20-----------------------#2020-05-19">https://towardsdatascience.com/include-these-spark-window-functions-in-your-data-science-workflow-c6bac5824475?source=collection_archive---------20-----------------------#2020-05-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/cd07fe873bfe017dfab1829e97db8454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nNr1yloBCw4RoYnXpB741A.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://pixabay.com/photos/window-woman-morning-girl-1148929/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><div class=""/><div class=""><h2 id="9c86" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">使用 pySpark 创建等级、滞后和滚动特征</h2></div><p id="3cce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我关于 Spark 的最后几篇文章中，我解释了如何使用 PySpark RDDs 和 Dataframes。</p><p id="e426" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然这些帖子解释了很多关于如何使用 rdd 和数据帧操作的内容，如果读者想学习 Spark 基础知识，我会要求他们通读这些帖子，但我仍然没有提到很多关于使用 PySpark 数据帧的内容。</p><p id="d1ae" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中一个就是火花窗函数。</p><p id="c467" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最近，当我在做我的一个项目时，我意识到了它们的力量，所以这篇<strong class="la jk">文章将是关于 Spark 中一些最重要的窗口功能。</strong></p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="9012" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">数据</h1><p id="ab37" class="pw-post-body-paragraph ky kz jj la b lb mt kk ld le mu kn lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">我将与南韩的<a class="ae jg" href="https://www.kaggle.com/kimjihoo/coronavirusdataset" rel="noopener ugc nofollow" target="_blank">新冠肺炎数据科学</a>合作，这是 COVID 在互联网上最详细的数据集之一。</p><p id="d1c9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，我将使用这个数据集来展示窗口函数，但这不应该以任何方式被视为对这个奇妙数据集的数据探索练习。</p><p id="9775" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于本文，我将使用 TimeProvince 数据框架，它包含每个省的每日病例信息。</p><figure class="mz na nb nc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/1d90e326e96367d64f28886c40f190c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAennXpCnDel5e8a2aU16A.png"/></div></div></figure></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="5421" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">等级</h1><p id="dc4a" class="pw-post-body-paragraph ky kz jj la b lb mt kk ld le mu kn lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">使用此功能，您可以在一个组上获得<code class="fe nd ne nf ng b">rank</code>和<code class="fe nd ne nf ng b">dense_rank</code>。例如，您可能希望 timeprovince 表中有一列提供某个省中每天的排名。我们可以通过以下方式做到这一点:</p><pre class="mz na nb nc gt nh ng ni nj aw nk bi"><span id="e528" class="nl mc jj ng b gy nm nn l no np">from pyspark.sql.window import Window<br/>from pyspark.sql import functions as F</span><span id="059f" class="nl mc jj ng b gy nq nn l no np">windowSpec = Window().partitionBy(['province']).orderBy(F.desc('confirmed'))<br/>timeprovince.withColumn("rank",F.rank().over(windowSpec)).show()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/af48dc4a303486e47910a723b61029b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ydvDXahlkNeqOFWoVL6DIQ.png"/></div></div></figure><p id="8d9d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，我们首先创建一个由<code class="fe nd ne nf ng b">province</code>划分的<code class="fe nd ne nf ng b">window</code>，并按照<code class="fe nd ne nf ng b">confirmed</code>案例的降序排列。人们可以把<code class="fe nd ne nf ng b">window</code>看作是按照用户提供的顺序排列的一组特定省份的行。然后，我们可以通过在这个窗口上使用 rank 函数轻松地添加等级，如上所示。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="0a19" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">滞后变量</h1><p id="ed8e" class="pw-post-body-paragraph ky kz jj la b lb mt kk ld le mu kn lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">有时，我们的数据科学模型可能需要基于滞后的功能。例如，一个模型可能有像<code class="fe nd ne nf ng b">price_last_week</code>或<code class="fe nd ne nf ng b">sales_ quantity_previous_day</code>这样的变量。我们可以使用窗口函数的滞后函数来创建这样的特征。在这里，我试图得到 7 天前确诊的病例。我正在过滤以显示结果，因为头几天的电晕案例为零。您可以在这里看到 lag_7 day 特性移动了 7 天。</p><pre class="mz na nb nc gt nh ng ni nj aw nk bi"><span id="b200" class="nl mc jj ng b gy nm nn l no np">from pyspark.sql.window import Window<br/>windowSpec = Window().partitionBy(['province']).orderBy('date')<br/>timeprovinceWithLag = timeprovince.withColumn("lag_7",F.lag("confirmed", 7).over(windowSpec))</span><span id="2275" class="nl mc jj ng b gy nq nn l no np">timeprovinceWithLag.filter(timeprovinceWithLag.date&gt;'2020-03-10').show()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/f4a3320808b51b4910b811c0714a0adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*deE34rlGaCf1lQH1iLhCGA.png"/></div></div></figure></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="70d8" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">滚动聚合</h1><p id="6198" class="pw-post-body-paragraph ky kz jj la b lb mt kk ld le mu kn lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">有时，为我们的模型提供滚动平均值会有所帮助。例如，我们可能希望将连续 7 天的销售额总和/平均值作为销售回归模型的一个特性。让我们计算一下过去 7 天确诊病例的移动平均数。这是很多人已经在用这个数据集做的事情，来看真实的趋势。</p><pre class="mz na nb nc gt nh ng ni nj aw nk bi"><span id="87d1" class="nl mc jj ng b gy nm nn l no np">from pyspark.sql.window import Window</span><span id="922e" class="nl mc jj ng b gy nq nn l no np">windowSpec = Window().partitionBy(['province']).orderBy('date')<strong class="ng jk">.rowsBetween(-6,0)</strong></span><span id="979e" class="nl mc jj ng b gy nq nn l no np">timeprovinceWithRoll = timeprovince.withColumn("roll_7_confirmed",F.mean("confirmed").over(windowSpec))</span><span id="5c2f" class="nl mc jj ng b gy nq nn l no np">timeprovinceWithRoll.filter(timeprovinceWithLag.date&gt;'2020-03-10').show()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/b2c9dcd796af69404bab23cc041ad894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MBW-cxmevqPkA1vd5fbo2Q.png"/></div></div></figure><p id="0117" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里有几件事需要了解。首先是我们在这里使用的<code class="fe nd ne nf ng b"> <strong class="la jk">rowsBetween(-6,0)</strong></code> <strong class="la jk"> </strong>函数。该函数有一个包含开始和结束的<code class="fe nd ne nf ng b">rowsBetween(start,end)</code>形式。使用此功能，我们只查看特定窗口中过去七天的情况，包括当前日期。这里 0 指定当前行，而-6 指定当前行之前的第七行。记住，我们从 0 开始计数。</p><p id="15f2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，为了得到日期为<code class="fe nd ne nf ng b">2020–03–22</code>的<code class="fe nd ne nf ng b">roll_7_confirmed</code>，我们查看日期为<code class="fe nd ne nf ng b">2020–03–22 to 2020–03–16</code>的确诊病例，并取其平均值。</p><p id="c68c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们使用了<code class="fe nd ne nf ng b"><strong class="la jk">rowsBetween(-7,-1)</strong></code> <strong class="la jk"> </strong>，我们将只查看过去七天的数据，而不是当前日期。</p><p id="5de4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还可以找到<code class="fe nd ne nf ng b">rowsBetween(Window.unboundedPreceding, Window.currentRow)</code>的一种用法，我们从窗口的第一行和 current_row 之间的行中获取运行总数。我在这里计算累计 _ 确认。</p><pre class="mz na nb nc gt nh ng ni nj aw nk bi"><span id="2a20" class="nl mc jj ng b gy nm nn l no np">from pyspark.sql.window import Window</span><span id="7d7a" class="nl mc jj ng b gy nq nn l no np">windowSpec = Window().partitionBy(['province']).orderBy('date').rowsBetween(Window.unboundedPreceding,Window.currentRow)<br/>timeprovinceWithRoll = timeprovince.withColumn("cumulative_confirmed",F.sum("confirmed").over(windowSpec))<br/>timeprovinceWithRoll.filter(timeprovinceWithLag.date&gt;'2020-03-10').show()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/8f86eeeea2d731dd5a218ed7c7fc99e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-jfjnbRiCpQxOAviCBnlGw.png"/></div></div></figure></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="8442" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">结论</h1><p id="1317" class="pw-post-body-paragraph ky kz jj la b lb mt kk ld le mu kn lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">在这里，我尝试总结了一些我在使用 Spark 时使用的窗口函数。你可能会想到使用这些窗口函数的许多其他方式，但是我希望这篇文章能让你清楚地知道如何使用它们而不会太混乱。</p><p id="1f78" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="nv">你可以在</em> </strong> <a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/tree/master/sparkwindowfunc" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk"> <em class="nv"> GitHub 库找到这篇文章的所有代码。</em> </strong> </a></p><p id="405a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，如果您需要学习 Spark 基础知识，可以看看我之前的帖子:</p><div class="is it gp gr iu nw"><a rel="noopener follow" target="_blank" href="/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jk gy z fp ob fr fs oc fu fw ji bi translated">使用 Spark 处理大数据的指南</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">不仅仅是介绍</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok ja nw"/></div></div></a></div><p id="8fb3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有，如果你想了解更多关于 Spark 和 Spark DataFrames 的知识，我想调出这些关于<a class="ae jg" href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=467035.11468293556&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fbig-data-essentials" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">大数据基础知识的优秀课程:HDFS、MapReduce 和 Spark RDD </strong> </a>和<a class="ae jg" href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=467035.11468293488&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fbig-data-analysis" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">大数据分析:Hive、Spark SQL、DataFrames 和 GraphFrames </strong> </a>由 Yandex 在 Coursera 上提供。</p><p id="9c6e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" rel="noopener"> <strong class="la jk">媒体</strong> </a>关注我，或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter<a class="ae jg" href="https://twitter.com/MLWhiz?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"><strong class="la jk">@ mlwhiz</strong></a>联系</p><p id="d23b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，一个小小的免责声明——这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p></div></div>    
</body>
</html>