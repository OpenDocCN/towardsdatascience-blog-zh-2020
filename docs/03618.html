<html>
<head>
<title>Playing a Game Using Q-Learning in Numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Numpy 中使用 Q-Learning 玩游戏</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-q-learning-in-numpy-to-teach-an-agent-to-play-a-game-4fee32eb922a?source=collection_archive---------35-----------------------#2020-04-05">https://towardsdatascience.com/using-q-learning-in-numpy-to-teach-an-agent-to-play-a-game-4fee32eb922a?source=collection_archive---------35-----------------------#2020-04-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/41aaeff7a086eae54c6c6dfb4d46af27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pvADcGrbHeiFJIHeTBDlUg.png"/></div></div></figure><p id="a8e9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有一些机器学习模型可以被训练以基于训练期间使用的输入-输出对将任何给定的输入映射到期望的输出。通过输入输出对，我显然是指输入和它各自的基础事实或标签。这种算法被称为监督学习算法。分类和回归就是其中的一些例子。</p><p id="9489" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">此外，存在一类机器学习模型，它们在数据中寻找潜在的模式，而不需要明确地知道标签。这些算法被称为无监督学习算法。聚类和密度估计就是其中的一些例子。</p><p id="8a5f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">除了监督学习和非监督学习，还有第三种机器学习范式，称为强化学习，它与前两种类型有着根本的不同。</p><p id="e45d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这基本上包括在一个环境中做出连续的决策，以最大化累积的未来回报。稍后我会更详细地解释这一点。</p><p id="a827" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> Q </strong> - <strong class="kd iu">学习</strong>就是这样一种无模型强化学习算法，它学习在各种情况下做出正确的动作。</p><p id="c65d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们首先熟悉强化学习设置的基本术语。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="b0c5" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">让我们熟悉一些基本的 RL 术语</h1><p id="889d" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">有一种叫做<strong class="kd iu">环境</strong>的东西，它实际上包含了构成系统的所有东西。它有一个<strong class="kd iu">代理</strong>，它观察环境(从环境中获取输入)，然后根据输入采取<strong class="kd iu">动作</strong>，改变环境的<strong class="kd iu">状态</strong>，并在此过程中收集<strong class="kd iu">奖励</strong>。</p><p id="c977" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果这对你来说还不够完美，请不要担心！</p><p id="1f2e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们借助一个例子来理解这一点。</p><p id="cf6e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">于是就有了 6x 6 的网格。我们让代理在网格中 6x6=36 个可能的单元中随机产生。同样，在我们网格世界的另外两个细胞中，我们有一瓶啤酒和一种致命的病毒。药剂、一瓶啤酒和致命病毒每次都会在独特的细胞位置上繁殖。代理人想要够到啤酒瓶子，以避免可能感染它的病毒，并因此结束游戏。对代理移动的限制是它只能在任何对角线方向上移动一个单元格。</p><p id="79ec" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这个设置中，我们的 6x6 网格世界就是<strong class="kd iu">环境</strong>。代理可以访问它相对于一瓶啤酒(奖励)和致命病毒(敌人)的位置，我们称这个代理可以访问的环境信息为<strong class="kd iu">状态</strong>。根据环境的当前状态，代理可以采取某些预定义的<strong class="kd iu">动作</strong>，比如在四个对角线方向的任何一个方向上移动一个单元。代理的行为可能会给它一瓶啤酒(一个<strong class="kd iu">正奖励</strong>)或者它可能会让代理被致命病毒感染(一个<strong class="kd iu">负奖励</strong>)。因此，代理人的目标是通过与环境互动来了解它的环境，并最终学会采取行动，以使未来的累计回报最大化。</p><p id="dff1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">状态、动作和奖励的顺序，直到它全部结束(要么伸手拿啤酒瓶，要么被病毒感染)，被称为<strong class="kd iu">集</strong>。</p><p id="09c5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">导致情节终止的状态称为<strong class="kd iu">终端状态</strong>。在我们的网格世界的例子中，代理人相对于啤酒瓶或病毒的相对位置变为零的状态是终端状态。</p><p id="b910" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">回报</strong>是未来奖励的累计总和。在没有终端状态的情况下，这可能达到无穷大，也称为<strong class="kd iu">非情节</strong>任务。为了使它成为一个有限的总和，我们使用一个叫做<strong class="kd iu"> gamma </strong>的贴现因子来贴现未来的奖励。</p><p id="ae16" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面是 t 时刻的回报公式，用不同时间点的回报来表示。这里 T 是最终的时间步长，<strong class="kd iu">γ</strong>是贴现率，使得 0≤<strong class="kd iu">γ</strong>≤1</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mj"><img src="../Images/ddb6b5c8a1f447fb2cfcc53afbf5c95e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D_d-3CmNEnKne9q-J3fQYw.png"/></div></div></figure><p id="adb6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">价值函数</strong>是状态(或状态-动作对)的函数，估计代理处于给定状态有多好(或在给定状态执行给定动作有多好)</p><p id="e630" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">价值函数总是相对于称为策略的特定行为方式来定义的。</p><p id="03a8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一个<strong class="kd iu">政策</strong>只不过是从状态到选择每个可能行动的概率的映射。</p><p id="8e9b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我想多谈谈<strong class="kd iu">状态-行动价值函数</strong>，因为我们将在博客的其余部分使用它。</p><p id="40c8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如前所述，状态-动作函数是一个返回预期收益的函数，从状态“s”开始，采取动作“a”，然后遵循策略<strong class="kd iu"> π </strong>。</p><p id="45b0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，比较给定状态的状态-动作函数值和动作空间中的所有动作，可以引导我们找到在该状态下可以采取的最佳可能动作。这实际上是我们的代理如何选择在给定状态下可以采取的最佳行动。</p><p id="d675" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> Q 表</strong>是由所有状态-动作对组成的表。这就像一个查找表，如果给出一个当前的状态和动作，我们可以得到状态-动作或 q 值。</p><p id="363a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们最初用随机数初始化 q 表，然后随着代理在各种状态下继续采取各种行动，继续更新每个状态-行动对的值。在代理对环境进行了一定程度的探索之后，我们可以通过使用 q 表来判断对于任何给定的状态应该采取哪种最佳行动！</p><p id="9788" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">是不是很疯狂！:D</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="4ad4" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">让我们把手弄脏吧！</h1><p id="35c7" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">我们现在要学习如何实现 q-learning。这将是一次有趣的旅程。:D，系好你的安全带</p><p id="daf5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们从导入所有必需的库开始。</p><figure class="mk ml mm mn gt ju"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="253b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我们定义了 q 学习算法和显示窗口的所有超参数。</p><figure class="mk ml mm mn gt ju"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="9381" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">接下来，我们为我们的代理、啤酒瓶(正奖励)和病毒(负奖励)定义图像的路径，并读取它们以显示在屏幕上。这只是为了让显示窗口看起来有趣。</p><figure class="mk ml mm mn gt ju"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="bfe9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们需要创建一个名为<strong class="kd iu">【Blob】</strong>的类，我们的代理、啤酒瓶和病毒将继承这个类。一个 Blob 类对象将有一个相关联的生成位置(x 和 y 坐标),它将能够根据其<strong class="kd iu">‘移动’</strong>方法中传递的输入进行对角移动。我们也能够加减两个斑点对象。它将简单地增加或减少两个斑点对象的 x 和 y 坐标。</p><figure class="mk ml mm mn gt ju"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="4af0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在让我们来定义 q 表，它将被一次又一次地用来选择给定状态下的最佳行动。我们将不断更新此表，以提高我们代理商的决策能力。</p><p id="6a08" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请注意，这里我们对状态空间的取值范围是<strong class="kd iu"> -SIZE+1 到 SIZE </strong>。这是因为我们将状态空间定义为啤酒瓶和病毒之间的相对位置。所以这可以是正面的，也可以是负面的。例如，如果网格世界的大小是 6，那么代理相对于啤酒瓶的相对位置将从-5 变化到 5。</p><p id="5551" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在不要担心更新 q 表中 q 值的更新规则。我们稍后会谈到这一点。</p><figure class="mk ml mm mn gt ju"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="d927" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在每一集里，我们的特工、啤酒瓶和病毒都需要在特定网格世界的特定细胞位置繁殖。但是我们的 Blob 类只是在 grid-world 中的一个随机单元位置上产生了一些人。这可能会导致角色在同一位置产生，这是我们不希望的。为此，我们编写了一个函数，它将一个元组列表作为输入，该列表包含已经被繁殖的个体的坐标。这将确保任何两个家伙，代理人，啤酒瓶和病毒不共享他们的细胞位置。</p><figure class="mk ml mm mn gt ju"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="1efb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">设置和定义好一切后，我们开始训练代理(更新 q 表)。</p><p id="9bd9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这就是奇迹发生的地方。</p><p id="390f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们一行一行来理解。所以我们从一个循环开始，这个循环将运行规定的集数。</p><p id="3778" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们使用前面已经定义的<strong class="kd iu">get _ unique _ spacing _ location</strong>函数和<strong class="kd iu"> Blob </strong>类来初始化播放器、啤酒和病毒对象。</p><p id="a26f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我们使用<strong class="kd iu"> SHOW_EVERY </strong>参数来打印 epsilon 的当前值、到目前为止的平均奖励(这应该随着训练而增加)以及用于显示实际网格世界的显示参数<strong class="kd iu">‘SHOW’</strong>。</p><p id="5005" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">接下来，我们将<strong class="kd iu">‘插曲 _ 奖励’</strong>初始化为 0，代理开始采取 200 个时间步长的动作。</p><p id="7b7f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于每个时间步长，我们需要环境的状态，我们将它定义为代理相对于啤酒瓶和病毒的相对位置。</p><p id="8a36" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">那么代理将需要采取行动，这也将取决于ε的当前值。我们从均匀分布中抽取一个随机数，并将其与 epsilion 的当前值进行比较。如果随机值大于ε的当前值，则代理使用 q 表并选择 q 表中具有最大 q 值的动作。否则代理采取随机行动。</p><p id="2151" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">代理利用 q 表挑选具有最高 q 值的动作的第一种情况被称为<strong class="kd iu">利用</strong>，而代理采取随机动作的第二种情况被称为<strong class="kd iu">探索</strong>。参数ε负责<strong class="kd iu">开发-勘探权衡</strong>。</p><p id="5ffa" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我们检查代理是否接触到啤酒瓶，或者它是否被病毒感染。我们相应地定义当前时间步长的奖励。</p><p id="4f13" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后代理采取行动，导致它在网格世界中的状态发生变化。</p><p id="ebdd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我们计算新的 q 值，并使用下面提到的公式更新 q 表。</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/9fc7a115a6216b8a69a57d39f67aadae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_M6HvopqYUBxjAEEE7ZK3Q.png"/></div></div></figure><p id="56be" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了显示网格世界和这些家伙，我们制作了一个空画布，根据<strong class="kd iu"> DISPLAY_SIZE </strong>参数调整大小，并将代理、啤酒瓶和病毒的图像粘贴到它们各自的当前位置。</p><p id="a4e3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后，如果代理人得到了他的啤酒或者他被病毒感染了，我们就打破了这个循环。</p><figure class="mk ml mm mn gt ju"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="8ab2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们计算移动平均线，并绘制回报图。</p><figure class="mk ml mm mn gt ju"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="967b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后，我们保存更新的 q 表。</p><figure class="mk ml mm mn gt ju"><div class="bz fp l di"><div class="mo mp l"/></div></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="dd4c" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">结果</h1><p id="26c6" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">下面是报酬移动平均值的曲线。它的上升趋势表明，随着越来越多的训练，代理变得越来越聪明。</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/8289dfd89060d39d01cac6323945f9f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*M9Tdkis9-VoRNY_th4lucA.jpeg"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">移动平均数</p></figure><p id="5109" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里有一些 gif 展示了代理如何在每一次训练中变得更聪明。</p><p id="f89c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是一个口渴的代理用随机初始化的 q 表寻找一瓶啤酒。这意味着代理还没有关于环境的线索。</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/317fb2ac1be5e92637336c407cdb0502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*V95r-iywnhAYgZ0Q7sn8PA.gif"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">愚蠢的代理人想要啤酒，但没有得到它</p></figure><p id="4996" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">经过一些训练后，代理人在做出连续决策方面做得相对更好。他还不是很快，但他最终找到了啤酒。</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/6f36d5f414a3fe4d743690b729df5f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*55txIxQuv8VTs-95JNnBhA.gif"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">嗯，这样更好</p></figure><p id="6d62" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后，经过数千次的训练，代理人真的很擅长做连续的决定，并且很快就找到了啤酒！:D</p><figure class="mk ml mm mn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/fcf4078c888d16233967604cab9cb773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*xooExiFLqb-gig95j62IwA.gif"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">这才是个聪明的特工！b)</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="7631" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">Q 学习的优势</h1><p id="2b1c" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">这是一个易于理解和实施的简单概念。它非常适合没有巨大国家行为空间的环境。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="dd30" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">Q 学习的局限性</h1><p id="30ed" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">q 学习包括从 q 表中查找。并且 q 表由每个可能的单个状态-动作对的值组成。因此，随着状态空间的增加，q 表的大小呈指数增长。因此，Q-Learning 有利于在状态-动作空间较小的环境中训练智能体。对于更复杂的设置，使用深度 Q 学习。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="f7ed" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">密码</h1><p id="8f6e" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">你可以在我的 GitHub 账号上找到这个 q-learning 项目的代码，点击这个链接<a class="ae mx" href="https://github.com/n0obcoder/Q-Learning" rel="noopener ugc nofollow" target="_blank"><strong class="kd iu">【https://github.com/n0obcoder/Q-Learning】</strong></a></p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="00b5" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">参考</h1><ol class=""><li id="9157" class="my mz it kd b ke me ki mf km na kq nb ku nc ky nd ne nf ng bi translated">【https://www.youtube.com/playlist? T4】list = plqvvvaa 0 qudezjfiou 5 wddfy 4 e 9 vdnx-7</li><li id="89de" class="my mz it kd b ke nh ki ni km nj kq nk ku nl ky nd ne nf ng bi translated"><a class="ae mx" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/fundamentals-of-reinforcement-learning</a></li><li id="54f7" class="my mz it kd b ke nh ki ni km nj kq nk ku nl ky nd ne nf ng bi translated"><a class="ae mx" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">https://web . Stanford . edu/class/psych 209/Readings/suttonbartoiprlbook 2 nded . pdf</a></li></ol></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="fd94" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我写这篇博客是因为我通过阅读别人的博客学到了很多东西，我觉得我也应该尽可能多地写下并分享我的学习和知识。所以，请在下面的评论区留下你的反馈，让我知道如何改进我未来的博客！:D</p><p id="1ace" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我也是一个独立的音乐艺术家，喜欢在空闲时间演奏和录制音乐。也许你可以看看我的艺人页面，表示支持:)<br/><a class="ae mx" href="https://open.spotify.com/artist/7G2BgSnludIYl1gFyJKG6X?si=Bv5L4ZAVQrmIsl5SgGRAUw" rel="noopener ugc nofollow" target="_blank">Spotify 上的 8 楼和声！</a></p></div></div>    
</body>
</html>