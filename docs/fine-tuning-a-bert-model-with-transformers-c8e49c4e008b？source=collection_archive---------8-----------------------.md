# ç”¨å˜å‹å™¨å¾®è°ƒ BERT æ¨¡å‹

> åŸæ–‡ï¼š<https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b?source=collection_archive---------8----------------------->

## è®¾ç½®è‡ªå®šä¹‰æ•°æ®é›†ï¼Œç”¨ Transformers Trainer å¾®è°ƒ BERTï¼Œå¹¶é€šè¿‡ ONNX å¯¼å‡ºæ¨¡å‹

è¿™ç¯‡æ–‡ç« æè¿°äº†ä¸€ä¸ªå¼€å§‹å¾®è°ƒå˜å‹å™¨æ¨¡å‹çš„ç®€å•æ–¹æ³•ã€‚å®ƒå°†æ¶µç›–åŸºç¡€çŸ¥è¯†ï¼Œå¹¶å‘æ‚¨ä»‹ç»æ¥è‡ª`transformers`åº“çš„æƒŠäººçš„`Trainer`ç±»ã€‚ä½ å¯ä»¥ä» [Google Colab](https://colab.research.google.com/github/thigm85/blog/blob/master/_notebooks/2020-11-12-fine-tune-bert-basic-transformers-trainer.ipynb) è¿è¡Œä»£ç ï¼Œä½†æ˜¯ä¸è¦å¿˜è®°å¯ç”¨ GPU æ”¯æŒã€‚

![](img/095c0c0558640c517da5175e001a37b1.png)

[Samule å­™](https://unsplash.com/@samule?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)åœ¨ [Unsplash](https://unsplash.com/s/photos/transformers?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) ä¸Šçš„ç…§ç‰‡

æˆ‘ä»¬ä½¿ç”¨ä»[æ–°å† è‚ºç‚å…¬å¼€ç ”ç©¶æ•°æ®é›†æŒ‘æˆ˜èµ›](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)æ„å»ºçš„æ•°æ®é›†ã€‚è¿™é¡¹å·¥ä½œæ˜¯ä¸€ä¸ªæ›´å¤§çš„é¡¹ç›®çš„ä¸€å°éƒ¨åˆ†ï¼Œè¯¥é¡¹ç›®æ˜¯å»ºç«‹ [cord19 æœç´¢åº”ç”¨](https://cord19.vespa.ai/)ã€‚

## å®‰è£…æ‰€éœ€çš„åº“

```
!pip install pandas transformers
```

## åŠ è½½æ•°æ®é›†

ä¸ºäº†å¾®è°ƒ cord19 åº”ç”¨ç¨‹åºçš„ BERT æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ç»„æŸ¥è¯¢æ–‡æ¡£ç‰¹å¾å’Œæ ‡ç­¾ï¼Œä»¥æŒ‡ç¤ºå“ªäº›æ–‡æ¡£ä¸ç‰¹å®šæŸ¥è¯¢ç›¸å…³ã€‚åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`query`å­—ç¬¦ä¸²è¡¨ç¤ºæŸ¥è¯¢ï¼Œä½¿ç”¨`title`å­—ç¬¦ä¸²è¡¨ç¤ºæ–‡æ¡£ã€‚

```
training_data = read_csv("https://thigm85.github.io/data/cord19/cord19-query-title-label.csv")
training_data.head()
```

![](img/f60fc903e5f16d549486026ba1663c22.png)

æœ‰ 50 ä¸ªå”¯ä¸€çš„æŸ¥è¯¢ã€‚

```
len(training_data["query"].unique())50
```

å¯¹äºæ¯ä¸ªæŸ¥è¯¢ï¼Œæˆ‘ä»¬éƒ½æœ‰ä¸€ä¸ªæ–‡æ¡£åˆ—è¡¨ï¼Œåˆ†ä¸ºç›¸å…³(`label=1`)å’Œä¸ç›¸å…³(`label=0`)ã€‚

```
training_data[["title", "label"]].groupby("label").count()
```

![](img/0cd780c37ee91c06f727e17443d4e10c.png)

## æ•°æ®åˆ†å‰²

ä¸ºäº†ä¾¿äºè¯´æ˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚å³ä½¿æˆ‘ä»¬åœ¨è€ƒè™‘ç‹¬ç‰¹çš„æŸ¥è¯¢å’Œæ–‡æ¡£å¯¹æ—¶æœ‰è¶…è¿‡ 5 ä¸‡ä¸ªæ•°æ®ç‚¹ï¼Œæˆ‘ç›¸ä¿¡è¿™ä¸ªç‰¹å®šçš„æ¡ˆä¾‹å°†å—ç›Šäºäº¤å‰éªŒè¯ï¼Œå› ä¸ºå®ƒåªæœ‰ 50 ä¸ªåŒ…å«ç›¸å…³æ€§åˆ¤æ–­çš„æŸ¥è¯¢ã€‚

```
from sklearn.model_selection import train_test_split
train_queries, val_queries, train_docs, val_docs, train_labels, val_labels = train_test_split(
    training_data["query"].tolist(), 
    training_data["title"].tolist(), 
    training_data["label"].tolist(), 
    test_size=.2
)
```

## åˆ›å»º BERT ç¼–ç 

åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯ç¼–ç ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©[ä½¿ç”¨å“ªä¸ª BERT æ¨¡å‹](https://huggingface.co/transformers/pretrained_models.html)ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[å¡«å……å’Œæˆªæ–­](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)ï¼Œå› ä¸ºè®­ç»ƒä¾‹ç¨‹æœŸæœ›ä¸€æ‰¹ä¸­çš„æ‰€æœ‰å¼ é‡å…·æœ‰ç›¸åŒçš„ç»´æ•°ã€‚

```
from transformers import BertTokenizerFast

model_name = "google/bert_uncased_L-4_H-512_A-8"
tokenizer = BertTokenizerFast.from_pretrained(model_name)

train_encodings = tokenizer(train_queries, train_docs, truncation=True, padding='max_length', max_length=128)
val_encodings = tokenizer(val_queries, val_docs, truncation=True, padding='max_length', max_length=128)
```

## åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†

ç°åœ¨æˆ‘ä»¬æœ‰äº†ç¼–ç å’Œæ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ª`Dataset`å¯¹è±¡ï¼Œå¦‚å˜å½¢é‡‘åˆšç½‘é¡µä¸­å…³äº[è‡ªå®šä¹‰æ•°æ®é›†](https://huggingface.co/transformers/custom_datasets.html)çš„æè¿°ã€‚

```
import torch

class Cord19Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = Cord19Dataset(train_encodings, train_labels)
val_dataset = Cord19Dataset(val_encodings, val_labels)
```

## å¾®è°ƒ BERT æ¨¡å‹

æˆ‘ä»¬å°†ä½¿ç”¨`BertForSequenceClassification`ï¼Œå› ä¸ºæˆ‘ä»¬è¯•å›¾å°†æŸ¥è¯¢å’Œæ–‡æ¡£å¯¹åˆ†ä¸ºä¸¤ä¸ªä¸åŒçš„ç±»åˆ«(ä¸ç›¸å…³ã€ç›¸å…³)ã€‚

```
from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained(model_name)
```

æˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰åŸºæœ¬æ¨¡å‹å‚æ•°çš„`requires_grad`è®¾ç½®ä¸º`False`ï¼Œä»¥ä¾¿ä»…å¾®è°ƒç‰¹å®šäºä»»åŠ¡çš„å‚æ•°ã€‚

```
for param in model.base_model.parameters():
    param.requires_grad = False
```

ç„¶åæˆ‘ä»¬å¯ä»¥ç”¨`Trainer`å¾®è°ƒæ¨¡å‹ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå¸¦æœ‰ä¸€ç»„ç°æˆå‚æ•°çš„åŸºæœ¬ä¾‹ç¨‹ã€‚é€‰æ‹©ä¸‹é¢çš„å‚æ•°æ—¶åº”è¯¥å°å¿ƒï¼Œä½†è¿™è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ã€‚

```
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    evaluation_strategy="epoch",     # Evaluation is done at the end of each epoch.
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints.    
)

trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()
```

## å°†æ¨¡å‹å¯¼å‡ºåˆ° ONNX

ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [ONNX](https://onnx.ai/) æ ¼å¼å¯¼å‡ºæ¨¡å‹ï¼Œä»¥éƒ¨ç½²åˆ°å…¶ä»–åœ°æ–¹ã€‚ä¸‹é¢æˆ‘å‡è®¾ä½ æœ‰ä¸€ä¸ª GPUï¼Œæ¯”å¦‚ä½ å¯ä»¥ä» Google Colab è·å¾—ã€‚

```
from torch.onnx import export

device = torch.device("cuda") 

model_onnx_path = "model.onnx"
dummy_input = (
    train_dataset[0]["input_ids"].unsqueeze(0).to(device), 
    train_dataset[0]["token_type_ids"].unsqueeze(0).to(device), 
    train_dataset[0]["attention_mask"].unsqueeze(0).to(device)
)
input_names = ["input_ids", "token_type_ids", "attention_mask"]
output_names = ["logits"]
export(
    model, dummy_input, model_onnx_path, input_names = input_names, 
    output_names = output_names, verbose=False, opset_version=11
)
```

## ç»“æŸè¯­

å¦‚å‰æ‰€è¿°ï¼Œè¿™ç¯‡æ–‡ç« æ¶µç›–äº†åŸºæœ¬çš„åŸ¹è®­è®¾ç½®ã€‚è¿™æ˜¯ä¸€ä¸ªéœ€è¦æ”¹è¿›çš„è‰¯å¥½èµ·ç‚¹ã€‚æœ€å¥½ä»ç®€å•çš„å¼€å§‹ï¼Œç„¶åè¡¥å……ï¼Œè€Œä¸æ˜¯ç›¸åï¼Œå°¤å…¶æ˜¯åœ¨å­¦ä¹ æ–°ä¸œè¥¿çš„æ—¶å€™ã€‚æˆ‘å°†è¶…å‚æ•°è°ƒä¼˜ã€äº¤å‰éªŒè¯å’Œæ›´è¯¦ç»†çš„æ¨¡å‹éªŒè¯ç­‰é‡è¦ä¸»é¢˜ç•™åˆ°åç»­æ–‡ç« ä¸­ã€‚ä½†æ˜¯æœ‰ä¸€ä¸ªåŸºæœ¬çš„è®­ç»ƒè®¾ç½®æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç¬¬ä¸€æ­¥ã€‚