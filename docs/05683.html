<html>
<head>
<title>7 Tips To Maximize PyTorch Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最大化PyTorch性能的7个技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-tips-for-squeezing-maximum-performance-from-pytorch-ca4a40951259?source=collection_archive---------1-----------------------#2020-05-12">https://towardsdatascience.com/7-tips-for-squeezing-maximum-performance-from-pytorch-ca4a40951259?source=collection_archive---------1-----------------------#2020-05-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/89f8e646766bc10d44f0b141ddbde16e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tBvWaBAK0A5VCDoa8xWswQ.jpeg"/></div></div></figure><p id="88cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在过去的10个月里，在开发<a class="ae kw" href="https://github.com/PyTorchLightning/pytorch-lightning" rel="noopener ugc nofollow" target="_blank"> PyTorch Lightning </a>、<a class="ae kw" href="https://pytorch-lightning.readthedocs.io/en/stable/governance.html" rel="noopener ugc nofollow" target="_blank">时，团队和我</a>已经接触到了许多构建PyTorch代码的风格，我们已经确定了一些我们看到人们无意中引入瓶颈的关键地方。</p><p id="66ec" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们非常小心地确保PyTorch Lightning不会在我们为您自动化的代码中犯任何这些错误，当我们检测到这些错误时，我们甚至会尝试为用户纠正它们。然而，因为Lightning只是结构化的PyTorch，而你仍然控制着所有的科学PyTorch，所以在很多情况下我们不能为用户做什么。</p><p id="50f8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，如果您不使用Lightning，您可能会无意中在代码中引入这些问题。</p><p id="6257" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了帮助你更快地训练，这里有8个你应该知道的技巧，它们可能会降低你的代码速度。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="f120" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">在数据加载器中使用工作线程</h1><figure class="md me mf mg gt jr gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/04f049bb5e44a477c0ec8f3b749a1582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*pzR30Q6gEZ-UkKUDJ70paA.png"/></div></figure><p id="c2c3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这第一个错误很容易纠正。PyTorch允许在多个进程上同时加载数据(<a class="ae kw" href="https://pytorch.org/docs/stable/data.html#multi-process-data-loading" rel="noopener ugc nofollow" target="_blank">文档</a>)。</p><p id="b5bd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这种情况下，PyTorch可以通过处理8个批处理来绕过GIL锁，每个批处理在一个单独的进程中进行。你应该用多少工人？一个好的经验法则是:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="4400" class="mm lf iq mi b gy mn mo l mp mq">num_worker = 4 * num_GPU</span></pre><p id="da32" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/7" rel="noopener ugc nofollow" target="_blank">这个answe </a> r对此有很好的论述。</p><p id="0083" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">警告:坏处是你的内存使用量也会增加(<a class="ae kw" href="https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/2" rel="noopener ugc nofollow" target="_blank">来源</a>)。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="ba28" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">引脚存储器</h1><figure class="md me mf mg gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/aee13f61e0e175cbd385d54b142a5344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*qFiV_2eW9QB1bVhIYNSfuw.png"/></div></figure><p id="f452" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你知道有时你的GPU内存显示已满，但你很确定你的模型没有使用那么多吗？这种开销被称为<em class="ms">固定内存。这个内存已经作为一种“工作分配”被保留</em></p><p id="b634" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当您在数据加载器中启用pinned_memory时，它会“自动将获取的数据张量放入pinned memory，并使数据更快地传输到支持CUDA的GPU”(<a class="ae kw" href="https://pytorch.org/docs/stable/data.html#memory-pinning" rel="noopener ugc nofollow" target="_blank">source</a>)。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/4edaa942608f4cf949fff2f496c897d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xw4-jfQXEXpLNZFmgMxOQg.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">在<a class="ae kw" href="https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/" rel="noopener ugc nofollow" target="_blank">这篇NVIDIA博客文章</a>中描述的固定内存。</p></figure><p id="9459" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这也意味着您不应该不必要地呼叫:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="8670" class="mm lf iq mi b gy mn mo l mp mq">torch.cuda.empty_cache()</span></pre></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="3da6" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">避免CPU到GPU的传输，反之亦然</h1><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="f4c5" class="mm lf iq mi b gy mn mo l mp mq"># bad</span><span id="97ad" class="mm lf iq mi b gy my mo l mp mq">.cpu()<br/>.item()<br/>.numpy()</span></pre><p id="14ac" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我看到大量使用。项目()或。cpu()或。numpy()调用。这对性能非常不利，因为这些调用中的每一个都将数据从GPU传输到CPU，并且<strong class="ka ir">显著地</strong>降低了性能。</p><p id="f655" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您试图弄清楚附加的计算图，请使用。请改为分离()。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="73e8" class="mm lf iq mi b gy mn mo l mp mq"># good</span><span id="7251" class="mm lf iq mi b gy my mo l mp mq">.detach()</span></pre><p id="1261" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这不会将内存转移到GPU，它会删除任何附加到该变量的计算图形。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="02bd" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">直接在GPU上构造张量</h1><p id="3db1" class="pw-post-body-paragraph jy jz iq ka b kb mz kd ke kf na kh ki kj nb kl km kn nc kp kq kr nd kt ku kv ij bi translated">大多数人都是这样在GPU上创建张量的</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="b7ab" class="mm lf iq mi b gy mn mo l mp mq">t = tensor.rand(2,2).cuda()</span></pre><p id="50eb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是，这个先创建CPU张量，然后再传递给GPU……这个真的很慢。相反，直接在你想要的设备上创建张量。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="2cb5" class="mm lf iq mi b gy mn mo l mp mq">t = tensor.rand(2,2, device=torch<strong class="mi ir">.</strong>device('cuda:0'))</span></pre><p id="022a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您使用的是Lightning，我们会自动将您的模型和批次放在正确的GPU上。但是，如果你在代码中的某个地方创建了一个新的张量(例如:VAE的样本随机噪声，或者类似的东西)，那么你必须自己放置这个张量。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="8680" class="mm lf iq mi b gy mn mo l mp mq">t = tensor.rand(2,2, device=self.device)</span></pre><p id="db7a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">每个LightningModule都有一个方便的self.device调用，无论你是在CPU、多GPU还是TPU上，它都可以工作(即:lightning将为该张量选择正确的设备。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="6133" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">使用分布式数据并行而不是数据并行</h1><p id="f33b" class="pw-post-body-paragraph jy jz iq ka b kb mz kd ke kf na kh ki kj nb kl km kn nc kp kq kr nd kt ku kv ij bi translated">PyTorch有两个主要模型用于在多个GPU上进行训练。第一个，<em class="ms">DataParallel(</em><strong class="ka ir"><em class="ms">DP</em></strong><em class="ms">)</em>，跨多个GPU拆分一个批处理。但这也意味着模型必须复制到每个GPU，一旦在GPU 0上计算了梯度，它们必须同步到其他GPU。</p><p id="5026" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这需要大量昂贵的GPU传输！相反，<em class="ms">DistributedDataParallel(</em><strong class="ka ir"><em class="ms">DDP</em></strong><em class="ms">)</em>在每个GPU上创建模型的孤岛副本(在它自己的进程中)，并且只使一部分数据对那个GPU可用。这就像有N个独立的模型训练，除了一旦每个模型计算了梯度，它们都跨模型同步梯度…这意味着我们在每个批处理期间只跨GPU<strong class="ka ir">传输数据一次</strong>。</p><p id="3ae3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在<a class="ae kw" href="https://github.com/PyTorchLightning/pytorch-lightning" rel="noopener ugc nofollow" target="_blank">闪电</a>中，你可以在两者之间轻松切换</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="eb65" class="mm lf iq mi b gy mn mo l mp mq">Trainer(distributed_backend='ddp', gpus=8)<br/>Trainer(distributed_backend='dp', gpus=8)</span></pre><p id="adc3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注意<a class="ae kw" href="https://pytorch.org/docs/stable/notes/cuda.html#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>和<a class="ae kw" href="https://pytorch-lightning.readthedocs.io/en/stable/multi_gpu.html#data-parallel-dp" rel="noopener ugc nofollow" target="_blank"> Lightning </a>都不鼓励DP使用。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="9655" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">使用16位精度</h1><p id="9e32" class="pw-post-body-paragraph jy jz iq ka b kb mz kd ke kf na kh ki kj nb kl km kn nc kp kq kr nd kt ku kv ij bi translated">这是另一种加快训练速度的方法，我们没有看到很多人使用。在16位训练中，模型和数据的一部分从32位数字变为16位数字。这有几个优点:</p><ol class=""><li id="6964" class="ne nf iq ka b kb kc kf kg kj ng kn nh kr ni kv nj nk nl nm bi translated">您使用了一半的内存(这意味着您可以将批量加倍，并将训练时间减半)。</li><li id="9c8a" class="ne nf iq ka b kb nn kf no kj np kn nq kr nr kv nj nk nl nm bi translated">某些GPU(V100，2080Ti)可以自动加速(快3-8倍)，因为它们针对16位计算进行了优化。</li></ol><p id="2668" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在Lightning中，这很容易实现:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="81c1" class="mm lf iq mi b gy mn mo l mp mq">Trainer(precision=16)</span></pre><p id="5486" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注意:在PyTorch 1.6之前，你还必须安装Nvidia Apex…现在16位是PyTorch的原生版本。但是如果你使用的是Lightning，它支持这两种方式，并根据检测到的PyTorch版本自动切换。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="fcf8" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">剖析您的代码</h1><p id="fce7" class="pw-post-body-paragraph jy jz iq ka b kb mz kd ke kf na kh ki kj nb kl km kn nc kp kq kr nd kt ku kv ij bi translated">如果没有Lightning，最后一个技巧可能很难做到，但是您可以使用类似于<a class="ae kw" href="https://docs.python.org/3/library/profile.html" rel="noopener ugc nofollow" target="_blank"> cprofiler </a>这样的工具来做到这一点。但是，在Lightning中，您可以通过两种方式获得培训期间所有通话的摘要:</p><p id="97d4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一、内置的<a class="ae kw" href="https://pytorch-lightning.readthedocs.io/en/stable/profiler.html" rel="noopener ugc nofollow" target="_blank">基本剖析器</a></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="8e21" class="mm lf iq mi b gy mn mo l mp mq">Trainer(profile=True)</span></pre><p id="4696" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它给出了这样的输出:</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/36ccf4bfdaf94920f02daa0bd202a514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vurXh5DD2-J9WG8A-Cg7cA.png"/></div></div></figure><p id="409a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">或者高级分析器:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="522f" class="mm lf iq mi b gy mn mo l mp mq">profiler <strong class="mi ir">=</strong> AdvancedProfiler()<br/>trainer <strong class="mi ir">=</strong> Trainer(profiler<strong class="mi ir">=</strong>profiler)</span></pre><p id="1537" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">变得非常精细</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/b3e9c6d983537bd13e7014bff5f9e8b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OmaLSwe9YevGfYvhmHrM4Q.png"/></div></div></figure><p id="e32c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">闪电剖面仪的完整文档可以在<a class="ae kw" href="https://pytorch-lightning.readthedocs.io/en/stable/profiler.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="a066" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">在你的代码中采用闪电</h1><p id="6c66" class="pw-post-body-paragraph jy jz iq ka b kb mz kd ke kf na kh ki kj nb kl km kn nc kp kq kr nd kt ku kv ij bi translated">PyTorch闪电只不过是结构化的PyTorch。</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/80b578d59a56cc03c470d017c4653851.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WxlxhXgdlUQFqCGCs7pXLQ.png"/></div></div></figure><p id="412c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你已经准备好让这些技巧中的大部分为你自动完成(并经过良好的测试)，那么看看这个关于将PyTorch代码重构为Lightning格式的视频<a class="ae kw" href="https://www.youtube.com/watch?v=QHww1JH7IDU" rel="noopener ugc nofollow" target="_blank">！</a></p></div></div>    
</body>
</html>