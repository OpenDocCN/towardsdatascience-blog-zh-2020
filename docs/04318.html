<html>
<head>
<title>Backpropagation from scratch on Mini-Batches</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在小批量上从头开始反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/backpropagation-from-scratch-on-mini-batches-e6efdaa281a2?source=collection_archive---------9-----------------------#2020-04-19">https://towardsdatascience.com/backpropagation-from-scratch-on-mini-batches-e6efdaa281a2?source=collection_archive---------9-----------------------#2020-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b698" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">逐步执行方程的小批量反向传播算法的实现。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c2a58d15e0b4e4e66403d6aa506b77b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nR9pQfAnKF4rJAaRYRwqHA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约翰·福斯特在<a class="ae ky" href="https://unsplash.com/s/photos/conveyerbelt?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1a1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你一定在想，又一个从头开始的博客？是的，但我想通了，想出了一些东西，你可以用它来修改，还有一些容易理解的方程，你通常会写下来理解算法。</p><p id="ff39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本博客将重点关注在小批量数据集上逐步实现反向传播算法。有大量的教程和博客来详细演示反向传播算法以及微积分和代数背后的所有逻辑。所以我将跳过这一部分，直接讨论数学中的方程和使用 Python 的实现(为什么不呢)。</p><h1 id="e135" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">为什么从零开始？</h1><p id="c985" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这是一个很长时间的社区问题，为什么我们要从头实现一个算法，即使它已经可以被几乎所有的框架使用。显然，在使用某些高级框架时，你甚至不会注意到反向传播的神奇之处。为了彻底理解它，你应该试着用这个东西弄脏你的手。反向传播是可以边玩边做实验的东西。</p><h1 id="b266" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">为什么是小批量？</h1><p id="2725" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">小批量背后的原因很简单。它通过将数据分成小批来节省内存和处理时间，并在训练循环的每次迭代中向算法提供数据集的一部分。一次进给一个 10000x10000 的矩阵不仅会耗尽内存，而且需要很长时间来运行。相反，将它降低到每次迭代 50 次不仅会减少内存使用，而且您可以跟踪进度。</p><p id="348e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注- </strong>这与随机方法不同，在随机方法中，我们从每个类别的数据中抽取分层样本，并在此基础上假设模型可以推广。</p><h1 id="4e62" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">实施时间！</h1><p id="9d3a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这是我将在这个实现中使用的数据的头部。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/c9e3e686344c2b98fff579b7b10b7b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kf9tHT1oIcaYy9kARkHqVQ.png"/></div></div></figure><p id="e82c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的目标变量是占有率，它是一个分类变量(0/1)。这将是我们将要编码的架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/7d6c3d30351cde33a56cec0f17ff77d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*7bzPGB8aCIyVeSHOC85Ofg.png"/></div></figure><h2 id="8f92" class="mu lw it bd lx mv mw dn mb mx my dp mf li mz na mh lm nb nc mj lq nd ne ml nf bi translated">算法:</h2><p id="7587" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">对于 i:=1 至 i:=m:</p><ol class=""><li id="5e69" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated">执行前向传播或前向传递，以计算每层神经元的激活值。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/272d0fc1be9245946e1dea936e952b02.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*y-YmqaI-WzmtfKQLKZYhbQ.png"/></div></figure><p id="dea1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.反向传播步骤:</p><ul class=""><li id="f674" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nq nm nn no bi translated">使用数据中的标签计算误差项(MSE 或 LogLoss 或您的愿望):</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c593e6ee4cad60ef9cf1b7d012150426.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*WEfuJJxbvFZ64gvo0PHQbg.png"/></div></figure><ul class=""><li id="dbc8" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nq nm nn no bi translated">隐藏层中的误差项计算如下:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/df517249901ad849f474c94d672c8cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*A6gqnDGM8ZVi31zLlSOoow.png"/></div></figure><ul class=""><li id="fcb7" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nq nm nn no bi translated">设置渐变:<br/>初始化δ= 0</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/b8cd2ca921906b49169364e0cdbc2a22.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*iBqAeR6dKODq2yeXqL60Gg.png"/></div></figure><p id="6249" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.梯度下降和重量更新步骤:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/6b800932bfb7d33828fda751032015be.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*V_WYefQDfuPCEH43IgLvKg.png"/></div></figure></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="2fc5" class="mu lw it bd lx mv mw dn mb mx my dp mf li mz na mh lm nb nc mj lq nd ne ml nf bi translated">现在实现:</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="199a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，这将是一个三层网络。我们保持层数简洁，以便以更好和更容易的方式识别梯度和误差方程。之后，我们将定义一个函数，它将被用作网络中的转发传播器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="a9cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="oe">这里需要注意的一点是，我认为输入层是我的第 0 层。可能有其他博客/教程认为它是第一。所以怎么索引完全是你自己的选择。</em></p><p id="5330" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，在初始化权重和偏差并定义前向传播函数后，我们将定义小批量的后向传播函数，大小=数据集大小/N。您可以调整 N 来调整所需的批量大小。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="4f7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们一环一环地分解它</p><ul class=""><li id="7abc" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nq nm nn no bi translated">如上所述，Ist 循环迭代你想要让模型遍历你的数据的次数，简单地把它放在神经网络术语“时期”中。</li><li id="b8e7" class="ng nh it lb b lc of lf og li oh lm oi lq oj lu nq nm nn no bi translated">第二次循环:在指定批次数量后，该循环对每个时期“I”的每个小批次进行迭代</li><li id="241b" class="ng nh it lb b lc of lf og li oh lm oi lq oj lu nq nm nn no bi translated">第三个循环遍历该小批量中的每个训练示例，并计算梯度和误差值</li><li id="82f5" class="ng nh it lb b lc of lf og li oh lm oi lq oj lu nq nm nn no bi translated">最后，对于每个批次，执行梯度下降步骤，并对重量矩阵进行更改。</li></ul><p id="69eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这个！！你终于自己完成了小批量反向传播的实现。这里需要注意的一点是，我对网络中的每一层都使用了一个矩阵变量，当你的网络规模增长时，这是一个愚蠢的举动，但同样，这样做只是为了了解事情实际上是如何工作的。<br/>如果您想增加隐藏层的数量，您可以简单地使用 3d 矩阵进行误差和梯度计算，其中第三维度将保存层值。例如，dim(δ)=(2，3，2)，这表示您正在计算第二个隐藏层中 W(2，3)的梯度。<br/>另外，我正在开发一个带有贝叶斯初始化的博客，希望很快会有这样的博客。</p><p id="571f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！继续实验！</p><p id="0489" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">编辑</strong>:在这里  <em class="oe">可以在 Github repo <a class="ae ky" href="https://github.com/theAayushbajaj/Backpropagation_MiniBatches" rel="noopener ugc nofollow" target="_blank"> <em class="oe">中找到使用过的数据集以及 ipython 笔记本。</em></a></em></p></div></div>    
</body>
</html>