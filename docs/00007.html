<html>
<head>
<title>GAN Pix2Pix Generative Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GAN Pix2Pix生成模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gan-pix2pix-generative-model-c9bf5d691bac?source=collection_archive---------6-----------------------#2020-01-01">https://towardsdatascience.com/gan-pix2pix-generative-model-c9bf5d691bac?source=collection_archive---------6-----------------------#2020-01-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="af43" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Pix2Pix模型进行图像到图像的翻译</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4abd6ddd476250294cf6e266e41082fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CWwcld7Md7zyFsdQT6g0ig.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Pix2Pix</p></figure><h1 id="5190" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">Pix2Pix GAN:简介</h1><p id="976c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们听到很多关于深度学习的语言翻译，其中神经网络学习从一种语言到另一种语言的映射。事实上，谷歌翻译用它来翻译100多种语言。但是，我们能对图像做类似的工作吗？当然，是的！如果有可能捕捉错综复杂的语言，那就一定有可能将一幅图像翻译成另一幅图像。的确，这显示了深度学习的力量。</p><p id="64be" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">Pix2Pix GAN论文早在2016年就由<a class="ae mr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Isola%2C+P" rel="noopener ugc nofollow" target="_blank">菲利普·伊索拉</a>、<a class="ae mr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu%2C+J" rel="noopener ugc nofollow" target="_blank">、</a>、<a class="ae mr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou%2C+T" rel="noopener ugc nofollow" target="_blank">周廷辉</a>、<a class="ae mr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Efros%2C+A+A" rel="noopener ugc nofollow" target="_blank">阿列克谢·阿夫罗斯</a>发表。在这里找到论文<a class="ae mr" href="https://arxiv.org/abs/1611.07004" rel="noopener ugc nofollow" target="_blank"/>。后来在2018年进行了修订。当它发表后，互联网用户尝试了一些创造性的东西。他们将pix2pix GAN系统用于各种不同的场景，如模仿一个人的动作，将一个人的视频逐帧翻译给另一个人。很酷，不是吗？使用pix2pix，我们可以将任何图像映射到任何其他图像，就像对象的边缘映射到对象的图像一样。此外，我们将详细探讨它的架构和工作原理。现在，让我们开始吧！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/8084e46a04eedb0e7486b5aa9a03667f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ga-0-EktAHDXDv_zs59VxA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt">来源:GitHub </strong></p></figure><h1 id="58cd" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">Pix2Pix GAN如何工作？</h1><h1 id="189d" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">cGAN:概述</h1><p id="a11c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">听说过生成逼真合成图像的GANs(生成对抗网络)吗？类似地，Pix2pix属于一种称为条件GAN或cGAN的类型。他们有一些条件设置，并在这种条件下学习图像到图像的映射。而基本GAN从随机分布向量生成图像，不应用任何条件。迷茫？试着得到这个。</p><p id="79a3" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">比方说，我们有一个用MS-COCO数据集的图像训练的GAN。在GANs中，用发生器网络产生的输出图像是随机的。也就是说，它可以生成数据集中任何对象的图像。但是，有了cGAN，我们可以生成我们想要的图像。如果我们想让它生成一个人，它会生成一个人的图像。这是通过调节GAN实现的。</p><h1 id="99fc" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">Pix2Pix GAN:概述</h1><p id="c5ad" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">让我们再举一个图像到图像转换任务的例子，即“黑白到彩色图像”的转换。在pix2pix cGAN中，B&amp;W图像作为生成器模型的输入。并且，所生成的模型的输出和给定的输入(B&amp;W图像)图像对是所生成的对(伪对)。B&amp;W输入图像和目标输出(即输入B&amp;W图像的真实颜色版本)形成真实对。</p><p id="51e9" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">鉴别器将给定的图像对分类为真实图像对或生成图像对。Pix2Pix中使用的那个与我们通常期望的分类器输出不同。它生成一个输出分类，对输入图像对中的多个面片进行分类(patchGAN)。我将详细解释它。在下面的描述中，连接被表示为⊕.</p><h1 id="e3d5" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">Pix2Pix GAN架构</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/4c7ad1d75e8db02242bfd9326944d145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60EbkAOFZDMEPYX6FnmrkQ.jpeg"/></div></div></figure><p id="271a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">Pix2Pix GAN有一个生成器和一个鉴别器，就像普通GAN一样。对于我们的黑白图像彩色化任务，输入B&amp;W由生成器模型处理，它生成输入的彩色版本作为输出。在Pix2Pix中，生成器是一个U-net架构的卷积网络。</p><p id="6651" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">它接收输入图像(B&amp;W，单通道)，将其通过一系列卷积和上采样层。最后，它产生一个输出图像，其大小与输入相同，但有三个通道(彩色)。但是在训练之前，生成器只产生随机输出。</p><p id="c449" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在生成器之后，合成图像与输入B&amp;W图像连接。因此，颜色通道的数量将是四个(高x宽x 4)。该级联张量作为输入馈入鉴别器网络。在Pix2Pix中，作者采用了一种不同类型的鉴别器网络(patchGAN类型)。patchGAN网络采用连接的输入图像，并产生大小为NxN的输出。</p><h1 id="4de4" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">损失函数</h1><h2 id="368c" class="mv kz it bd la mw mx dn le my mz dp li lz na nb lk md nc nd lm mh ne nf lo ng bi translated">鉴频器损耗</h2><p id="11a1" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">鉴别器损失函数衡量鉴别器预测的好坏。鉴别器损失越小，识别合成图像对就越准确。</p><p id="14a2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">GANs中使用的普通二进制分类器只产生一个输出神经元来预测真假。但是，patchGAN的NxN输出预测输入图像中的许多重叠面片。例如，在Pix2Pix中，输出大小为30x30x1，可预测输入的每个70×70面片。我们将在另一篇文章中看到更多关于patchGANs的内容。30×30输出馈入对数损耗函数，该函数将其与30×30零矩阵进行比较(因为它是生成的而不是真实的)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/f6543ba8f80d3ac6e7adce0d3a7d0357.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*4ispSlAOULKxDkdY5J2yqA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt">合成图像对丢失</strong></p></figure><p id="9caf" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这就是所谓的发电损失。从数据集中计算B&amp;W对及其相应的彩色图像的真实损失。这是真的一对。因此，“实际损失”是NxN输出的sigmoid交叉熵和一个NxN大小的矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ebea64ecbb73923081e796e529ef7468.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*K0Xpha8R84QVTrU7t9PShw.jpeg"/></div></figure><p id="6738" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">鉴频器总损耗是上述两种损耗的总和。损失函数的梯度是相对于鉴别器网络计算的，并且被反向传播以最小化损失。当鉴别器损耗反向传播时，发电机网络的权重被冻结。唷！现在我们差不多完成了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/13f0c61bfb1a9d7363bd6b651642c198.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*BaXc1zu6okoMIeKbZJ9rIw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt">鉴别器的损失函数</strong></p></figure><h2 id="8303" class="mv kz it bd la mw mx dn le my mz dp li lz na nb lk md nc nd lm mh ne nf lo ng bi translated">发电机损耗</h2><p id="0e03" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">发生器损耗衡量合成图像的真实程度。通过最小化这个，生成器可以产生更真实的图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/702409fb8108d95c15296757f48a2cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*elkUCeankeGTf2NQ0B18yw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt">发电机损耗功能</strong></p></figure><p id="e5fd" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这种损耗几乎与生成损耗相同，只是它是NxN鉴频器输出的sigmoid交叉熵和一个矩阵。当这种损耗反向传播时，鉴频器网络的参数被冻结。并且只调整生成器的权重。</p><p id="beac" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">为了提高生成图像的美观性，pix2pix论文的作者添加了一个L1损失项。它计算目标图像和生成图像之间的L1距离。然后乘以参数“λ”,并添加到发电机损耗中。</p><h1 id="e36b" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">训练Pix2Pix模型</h1><p id="336f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">为了训练模型将B&amp;W图像转换为彩色图像，我们必须向网络提供输入和目标图像。因此，可以使用任何带有彩色图像的数据集，如ImageNet。数据集可以通过将彩色图像转换成B&amp;W来构成输入。并且彩色图像本身形成目标。因此，可以通过迭代数据集、将图像一个接一个地或成批地馈送到pix2pix模型来训练网络。</p><h1 id="4062" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">就这样…完成了！</h1><p id="b439" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">那是给你的pix2pix！希望你对pix2pix GAN是什么以及它是如何工作的有一个清晰的认识。</p><p id="3a93" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">了解如何在自定义数据集上训练cGAN模型。</p><p id="3be0" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><a class="ae mr" href="https://hackerstreak.com/colorizing-pokemon-with-deep-learning/" rel="noopener ugc nofollow" target="_blank"> 1。如何用5个简单的步骤训练一个人工智能cGAN模型</a></p></div></div>    
</body>
</html>