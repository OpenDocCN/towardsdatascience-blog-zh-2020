<html>
<head>
<title>How to implement Linear Regression with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 PyTorch 实现线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-implement-linear-regression-with-pytorch-5737339296a6?source=collection_archive---------22-----------------------#2020-09-12">https://towardsdatascience.com/how-to-implement-linear-regression-with-pytorch-5737339296a6?source=collection_archive---------22-----------------------#2020-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8f7f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过实现线性回归学习 PyTorch 基础知识</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/86d1a6b456663f72bdd67a4812bcbf1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HpOx-JwXFRYciyBPul4ncA.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1b91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">很可能，用 PyTorch 实现线性回归是大材小用。这个库是为神经网络、复杂的深度学习架构等更复杂的东西而创建的。尽管如此，我认为使用它来实现一个更简单的机器学习方法，比如线性回归，对于那些想开始学习 PyTorch 的人来说是一个很好的练习。</p><p id="6cbb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在其核心，PyTorch 只是一个类似于 NumPy 的数学库，但是有两个重要的改进:</p><ul class=""><li id="0fb6" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">它可以使用 GPU 来加快运算速度。如果您正确配置了兼容的 GPU，只需做一些更改，就可以让代码在 GPU 上运行。</li><li id="10f7" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">它能够自动区分；这意味着对于基于梯度的方法，您不需要手动计算梯度，PyTorch 会为您完成。</li></ul><p id="f588" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以把 PyTorch 想象成服用类固醇的 NumPy。</p><p id="3520" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然这两个功能对于我们在这里想要做的事情(线性回归)来说似乎不是很大的改进，因为这不是非常昂贵的计算，并且手动计算梯度非常简单，但它们在深度学习中有很大的不同，在深度学习中，我们需要大量的计算能力，并且手动计算梯度非常困难。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><p id="2753" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在着手实现之前，让我们先简单回忆一下什么是线性回归:</p><p id="0bc8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mp">线性回归是通过一些其他已知变量以线性方式估计未知变量。视觉上，我们通过我们的数据点拟合一条线(或更高维的超平面)。</em></p><p id="1f68" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你对这个概念不太适应，或者想更好地理解它背后的数学原理，你可以阅读我以前写的关于线性回归的文章:</p><div class="mq mr gp gr ms mt"><a rel="noopener follow" target="_blank" href="/understanding-linear-regression-eaaaed2d983e"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd iu gy z fp my fr fs mz fu fw is bi translated">了解线性回归</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">线性回归背后的数学详细解释</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh ks mt"/></div></div></a></div><p id="1d5a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们跳到编码部分。</p><p id="e7fd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，很明显，我们需要导入一些库。我们导入<code class="fe ni nj nk nl b">torch</code>，因为它是我们用于实现的主要内容，<code class="fe ni nj nk nl b">matplotlib</code>用于可视化我们的结果，<code class="fe ni nj nk nl b">sklearn</code>中的<code class="fe ni nj nk nl b">make_regression</code>函数，我们将使用它来生成一个回归数据集作为示例，以及 python 的内置<code class="fe ni nj nk nl b">math</code>模块。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="2146" class="nq nr it nl b gy ns nt l nu nv"><strong class="nl iu">import</strong> <strong class="nl iu">torch</strong></span><span id="0694" class="nq nr it nl b gy nw nt l nu nv"><strong class="nl iu">import</strong> <strong class="nl iu">matplotlib.pyplot</strong> <strong class="nl iu">as</strong> <strong class="nl iu">plt</strong></span><span id="4079" class="nq nr it nl b gy nw nt l nu nv"><strong class="nl iu">from</strong> <strong class="nl iu">sklearn.datasets</strong> <strong class="nl iu">import</strong> make_regression</span><span id="4edb" class="nq nr it nl b gy nw nt l nu nv"><strong class="nl iu">import</strong> <strong class="nl iu">math</strong></span></pre><p id="fd2f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后我们将使用以下方法创建一个<code class="fe ni nj nk nl b">LinearRegression</code>类:</p><ul class=""><li id="7000" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><code class="fe ni nj nk nl b">.fit()</code> —该方法将实际学习我们的线性回归模型；在这里，我们将找到最佳权重</li><li id="b07a" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe ni nj nk nl b">.predict()</code> —这个将用于预测；它将返回我们的线性模型的输出</li><li id="424d" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe ni nj nk nl b">.rmse()</code> —用给定的数据计算我们的模型的均方根误差；这个指标有点像“从我们的模型估计值到真实 y 值的平均距离”</li></ul><p id="0a70" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在<code class="fe ni nj nk nl b">.fit()</code>中做的第一件事是将一个额外的 1 列连接到我们的输入矩阵 x。这是为了简化我们的数学，并将偏差视为一个始终为 1 的额外变量的权重。</p><p id="0d59" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe ni nj nk nl b">.fit()</code>方法将能够通过使用封闭形式的公式或随机梯度下降来学习参数。为了选择使用哪个，我们将有一个名为 method 的参数，该参数需要一个字符串“solve”或“sgd”。</p><p id="94e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当<code class="fe ni nj nk nl b">method</code>设置为“求解”时，我们将通过以下公式获得模型的权重:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/81cf1575b2c991ff9ababd19c7e41831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*zIbw0VcfIiZVjuKliUNgRQ.png"/></div></figure><p id="3895" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这要求矩阵 X 具有满列秩；因此，我们将检查这一点，否则我们会显示一条错误消息。</p><p id="e41c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的第一部分<code class="fe ni nj nk nl b">.fit()</code>方法是:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="8859" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意<code class="fe ni nj nk nl b">method</code>之后的其他参数是可选的，仅在我们使用 SGD 的情况下使用。</p><p id="27e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该方法的第二部分处理<code class="fe ni nj nk nl b">method = ‘sgd’</code>的情况，它不要求 X 具有完整的列秩。</p><p id="57a7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的最小二乘线性回归的 SGD 算法概述如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/10ce771fffe67668a5488a8a04325a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MdsoOAIddbg2TEUdPNKuhw.png"/></div></div></figure><p id="2df2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将通过将 weights 类属性初始化为一个张量来开始此算法，该张量是一个列向量，其值取自均值为 0、标准差为 1/(列数)的正态分布。我们将标准偏差除以列数，以确保在算法的初始阶段不会得到太大的输出值。这是为了帮助我们更快地收敛。</p><p id="92b1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在每次迭代的开始，我们随机地打乱我们的数据行。然后，对于每一批，我们计算梯度并将其从当前权重向量中减去(乘以学习率),以获得新的权重。</p><p id="254e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上面描述的 SGD 算法中，我们已经展示了手动计算的梯度；就是那个表达式乘以 alpha(学习率)。但是在下面的代码中，我们不会显式地计算这个表达式；相反，我们计算损失值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/8db44ab9d7f8802d462587b9257bc9e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*-7KNBCYVFYqvNywH5HrmRQ.png"/></div></figure><p id="1611" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后我们让 PyTorch 为我们计算梯度。</p><p id="f1e2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是我们<code class="fe ni nj nk nl b">.fit()</code>方法的后半部分:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="fc6b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了计算损失相对于权重的梯度，我们需要在<code class="fe ni nj nk nl b">self.weights</code>张量上调用<code class="fe ni nj nk nl b">.requires_grad_(True)</code>方法，然后我们根据上面给出的公式计算损失。计算损失后，我们在损失张量上调用<code class="fe ni nj nk nl b">.backward()</code>方法，该方法将计算梯度并将其存储在<code class="fe ni nj nk nl b">self.weights</code>的<code class="fe ni nj nk nl b">.grad</code>属性中。在我们完成更新后，我们调用<code class="fe ni nj nk nl b">.detach()</code>来获得一个新的张量，上面没有记录任何运算，这样下次我们计算梯度时，我们将只基于单次迭代中的运算。</p><p id="aee6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们从这个方法返回<code class="fe ni nj nk nl b">self</code>，以便能够像这样连接构造函数和<code class="fe ni nj nk nl b">.fit()</code>的调用:<code class="fe ni nj nk nl b">lr = LinearRegression().fit(X, y, ‘solve’)</code>。</p><p id="06ea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe ni nj nk nl b">.predict()</code>方法非常简单。我们首先检查之前是否调用了<code class="fe ni nj nk nl b">.fit()</code>，然后将一列 1 连接到 X，并验证 X 的形状允许与权重向量相乘。如果一切正常，我们只需返回 X 和权重向量相乘的结果作为预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="e5aa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<code class="fe ni nj nk nl b">.rmse()</code>中，我们首先使用<code class="fe ni nj nk nl b">.predict()</code>获得模型的输出，然后如果预测期间没有错误，我们计算并返回均方根误差，该误差可以被认为是“从我们的模型估计值到真实 y 值的平均距离”。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="70af" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是<code class="fe ni nj nk nl b">LinearRegression</code>类的完整代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h2 id="d4a8" class="nq nr it bd ob oc od dn oe of og dp oh lh oi oj ok ll ol om on lp oo op oq or bi translated">在一个例子中使用我们的<code class="fe ni nj nk nl b">LinearRegression</code>类</h2><p id="9e0a" class="pw-post-body-paragraph ky kz it la b lb os ju ld le ot jx lg lh ou lj lk ll ov ln lo lp ow lr ls lt im bi translated">为了展示我们的线性回归实现，我们将使用来自<code class="fe ni nj nk nl b">sklearn</code>的<code class="fe ni nj nk nl b">make_regression()</code>函数生成一个回归数据集。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="dea5" class="nq nr it nl b gy ns nt l nu nv">X, y = make_regression(n_features=1, n_informative=1,<br/>                       bias=1, noise=35)</span></pre><p id="5c23" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们绘制这个数据集，看看它是什么样子的:</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="9500" class="nq nr it nl b gy ns nt l nu nv">plt.scatter(X, y)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c4c426dbb406d396e9a87a9e0177f1ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r16GAeat5ldXPUXdESxrgA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="a661" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe ni nj nk nl b">make_regression()</code>返回的 y 是平面向量。我们将把它重新整形为一个列向量，用于我们的<code class="fe ni nj nk nl b">LinearRegression</code>类。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="8308" class="nq nr it nl b gy ns nt l nu nv">y = y.reshape((-1, 1))</span></pre><p id="5cb6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们将使用<code class="fe ni nj nk nl b">method = ‘solve’</code>来拟合回归线:</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="b677" class="nq nr it nl b gy ns nt l nu nv">lr_solve = LinearRegression().fit(X, y, method='solve')</span><span id="8c8b" class="nq nr it nl b gy nw nt l nu nv">plt.scatter(X, y)</span><span id="33f8" class="nq nr it nl b gy nw nt l nu nv">plt.plot(X, lr_solve.predict(X), color='orange')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6f895ffa1783815e57aece8bc8d7b309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*duxW8aMgmESopdlh-YnpqA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="0624" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上述回归模型的均方根误差为:</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="4212" class="nq nr it nl b gy ns nt l nu nv">lr_solve.rmse(X, y)</span><span id="828a" class="nq nr it nl b gy nw nt l nu nv"># tensor(31.8709, dtype=torch.float64)</span></pre><p id="98d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们也使用<code class="fe ni nj nk nl b">method = ‘sgd’</code>，我们将让其他参数有它们的默认值:</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="e4c6" class="nq nr it nl b gy ns nt l nu nv">lr_sgd = LinearRegression().fit(X, y, method='sgd')</span><span id="2c44" class="nq nr it nl b gy nw nt l nu nv">plt.scatter(X, y)</span><span id="efe6" class="nq nr it nl b gy nw nt l nu nv">plt.plot(X, lr_sgd.predict(X), color='orange')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/651170c45d16a9e972e443f805b28b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZuwUdenLKvla98zcr_Vcvg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b00c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如你所见，上面两幅图中方法“solve”和“sgd”的回归线几乎相同。</p><p id="6ae9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用“sgd”时，我们得到的均方根误差为:</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="c02f" class="nq nr it nl b gy ns nt l nu nv">lr_sgd.rmse(X, y)</span><span id="cf70" class="nq nr it nl b gy nw nt l nu nv"># tensor(31.9000, dtype=torch.float64)</span></pre><p id="01f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是 Jupyter 笔记本，包含所有代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><p id="f836" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望这些信息对你有用，感谢你的阅读！</p><p id="9074" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章也贴在我自己的网站<a class="ae ox" href="https://www.nablasquared.com/how-to-implement-linear-regression-with-pytorch/" rel="noopener ugc nofollow" target="_blank">这里</a>。随便看看吧！</p></div></div>    
</body>
</html>