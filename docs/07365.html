<html>
<head>
<title>5 Frameworks for Reinforcement Learning on Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 强化学习的 5 个框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-frameworks-for-reinforcement-learning-on-python-1447fede2f18?source=collection_archive---------3-----------------------#2020-06-04">https://towardsdatascience.com/5-frameworks-for-reinforcement-learning-on-python-1447fede2f18?source=collection_archive---------3-----------------------#2020-06-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9387" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从头开始编写自己的强化学习实现可能需要大量的工作，但是您不需要这样做。有很多很棒的、简单的和免费的框架可以让你在几分钟内开始。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7568592cf6ff471fd02c207ed1304e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7P7OZm9pn1G0QbZL"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过重用现有的 RL 库，你可以节省很多精力。]</p></figure><p id="f4cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有很多用于监督和非监督机器学习的标准库，如 Scikit-learn、XGBoost 甚至 Tensorflow，它们可以让你立即开始，并且你可以在网上找到支持的日志。遗憾的是，对于强化学习(RL)来说，情况并非如此。</p><p id="e0fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这并不是说没有框架，事实上，有很多关于 RL 的框架。问题是现在还没有标准，所以在网上寻找支持来开始、解决问题或定制解决方案并不容易。这可能是因为虽然 RL 是一个非常受欢迎的研究主题，但它仍处于行业实施和使用的早期阶段。</p><p id="888f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是这并不意味着没有伟大的框架可以帮助你开始使用 RL 来解决你喜欢的任何问题。我在这里列出了一些我逐渐了解和使用的框架，以及它们的优点和缺点。我希望这能让你对目前可用的 RL 框架有一个快速的了解，这样你就可以选择一个更适合你需求的框架。</p><h1 id="b619" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Keras-RL</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/ab1a6d3ad8bb85a8addc126b8855faba.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*2RRziVQPa9jjGiULr06qHQ.gif"/></div></figure><p id="1ec0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不得不承认从整个榜单来看，这是我的最爱。我相信是目前为止最容易理解的几种 RL 算法的代码实现包括深度 Q 学习(DQN)、双 DQN、深度确定性策略梯度(DDPG)、连续 DQN (CDQN 或 NAF)、交叉熵方法(CEM)、决斗 DQN)和 SARSA。当我说<em class="mo">“最简单易懂的代码”</em>时，我指的不是使用，而是定制它，并将其作为您项目的构建模块*。Keras-RL github 也包含了一些例子，你可以使用它们立刻开始。当然，它使用 Keras，您可以将它与 Tensorflow 或 PyTorch 一起使用。</p><p id="c149" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，Keras-RL 已经有一段时间没有得到很好的维护，它的官方文档也不是最好的。这给了这个项目的一个分支<a class="ae ky" href="https://github.com/wau/keras-rl2" rel="noopener ugc nofollow" target="_blank"> Keras-RL2 </a>以启示。</p><blockquote class="mp mq mr"><p id="53a2" class="kz la mo lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated">(*)我用这个框架做什么？嗯，我很高兴你问了——或者是我？我用这个框架创建了一个定制的辅导 DQN 代理，你可以在这里了解更多。</p></blockquote><h1 id="5096" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Keras-RL2</h1><p id="3a04" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">Keras-RL2 是 Keras-RL 的派生产品，因此它支持与 Keras-RL2 相同的代理，并且易于定制。这里大的变化是 Keras-RL2 维护更好，使用 Tensorflow 2.1.0。不幸的是，没有这个库的文档，尽管 Keras-RL 的文档也可以很容易地用于这个分支。</p><h1 id="a872" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">OpenAI 基线</h1><p id="2666" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated"><a class="ae ky" href="https://github.com/openai/baselines" rel="noopener ugc nofollow" target="_blank"> OpenAI Baselines </a>是 OpenAI 的一套高质量 RL 算法实现，OpenAI 是人工智能特别是 RL 研发领域的领先公司之一。它的构想是让研究人员可以容易地比较他们的 RL 算法，使用 OpenAI 的最先进的实现作为基线——因此得名。该框架包含许多流行代理的实现，如<a class="ae ky" href="https://github.com/openai/baselines/blob/master/baselines/a2c" rel="noopener ugc nofollow" target="_blank"> A2C </a>、<a class="ae ky" href="https://github.com/openai/baselines/blob/master/baselines/ddpg" rel="noopener ugc nofollow" target="_blank"> DDPG </a>、<a class="ae ky" href="https://github.com/openai/baselines/blob/master/baselines/deepq" rel="noopener ugc nofollow" target="_blank"> DQN </a>、<a class="ae ky" href="https://github.com/openai/baselines/blob/master/baselines/ppo2" rel="noopener ugc nofollow" target="_blank"> PPO2 </a>和<a class="ae ky" href="https://github.com/openai/baselines/blob/master/baselines/trpo_mpi" rel="noopener ugc nofollow" target="_blank"> TRPO </a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/b3fc0634a082d2f808285fa1f40e3d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ktF5FHvsvT3my4S_n9FEgw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">【图来自<a class="ae ky" href="http://htmlpreview.github.io/?https://github.com/openai/baselines/blob/master/benchmarks_atari10M.htm" rel="noopener ugc nofollow" target="_blank">稳定基线基准</a>。]</p></figure><p id="7087" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不利的一面是，OpenAI 基线没有得到很好的记录，尽管代码中有很多有用的注释。此外，因为它是作为基线而不是构建块开发的，所以如果您想为您的项目定制或修改一些代理，代码就不那么友好了。事实上，下一个框架就是从这里派生出来的，解决了大部分问题。</p><h1 id="5923" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">稳定基线</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/c73c9281f30e2aeefc8da04bb6e29fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*vhcFaoSKKU4hLsnCVqY-8g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[图片来自<a class="ae ky" href="https://stable-baselines.readthedocs.io/en/master/" rel="noopener ugc nofollow" target="_blank">稳定基线文档</a>。]</p></figure><p id="b3d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/hill-a/stable-baselines" rel="noopener ugc nofollow" target="_blank">稳定基线</a>是<a class="ae ky" href="https://github.com/openai/baselines" rel="noopener ugc nofollow" target="_blank">开放 AI 基线</a>的分支，有重大的结构重构和代码清理。在其官方文档网站中列出的<a class="ae ky" href="https://stable-baselines.readthedocs.io/en/master/" rel="noopener ugc nofollow" target="_blank">变更如下:</a></p><ul class=""><li id="7150" class="nc nd it lb b lc ld lf lg li ne lm nf lq ng lu nh ni nj nk bi translated">所有算法的统一结构</li><li id="d4be" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">PEP8 兼容(统一代码风格)</li><li id="9b17" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">记录的函数和类</li><li id="60d3" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">更多测试和更多代码覆盖</li><li id="d32d" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">附加算法:SAC 和 TD3(她支持 DQN、DDPG、SAC 和 TD3)</li></ul><p id="8c57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我个人在过去使用过稳定的基线，我可以确认它确实有很好的文档记录并且易于使用。甚至有可能用一句台词来训练一个用于<a class="ae ky" href="https://github.com/openai/gym" rel="noopener ugc nofollow" target="_blank">开放式健身房环境</a>的代理:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a537" class="nv lw it nr b gy nw nx l ny nz"><strong class="nr iu">from</strong> <strong class="nr iu">stable_baselines</strong> <strong class="nr iu">import</strong> PPO2</span><span id="62bf" class="nv lw it nr b gy oa nx l ny nz">model = PPO2('MlpPolicy', 'CartPole-v1').learn(10000)</span></pre><h1 id="ae7a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">顶点</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/7a30fabf6863d997b13aca327163c7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RUT-8gDLsy_-TJfb4NKuCg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">郊狼已经使用 ACME 几十年了，远远领先于他的时代！【图片来自<a class="ae ky" href="https://comicbookandbeyond.com/wile-e-coyote-live-action-finally-gets-a-director/" rel="noopener ugc nofollow" target="_blank">漫画及超越</a>。]</p></figure><p id="05f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Acme 来自 DeepMind，可能是研究中最知名的致力于 RL 的公司。因此，它是为构建可读、高效、面向研究的 RL 算法而开发的，并包含多种先进代理的实现，如 D4PG、DQN、R2D2、R2D3 等。Acme 使用 Tensorflow 作为后端，并且一些代理实现使用了<a class="ae ky" href="https://github.com/google/jax/blob/master/README.md" rel="noopener ugc nofollow" target="_blank"> JAX </a>和 Tensorflow 的组合。</p><p id="64c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Acme 的开发牢记使其代码尽可能可重用，因此其设计是模块化的，易于定制。它的<a class="ae ky" href="https://github.com/deepmind/acme/blob/master/docs/index.md" rel="noopener ugc nofollow" target="_blank">文档</a>并不丰富，但足以给你一个很好的库介绍，也有一些例子让你开始使用 Jupyter 笔记本。</p><h1 id="dfa4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">外卖食品</h1><p id="59df" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">这里列出的所有框架都是任何 RL 项目的可靠选择；决定使用哪一个取决于你的偏好和你到底想用它做什么。为了更好地形象化每个框架及其优缺点，我做了以下直观总结:</p><h2 id="d4af" class="nv lw it bd lx oc od dn mb oe of dp mf li og oh mh lm oi oj mj lq ok ol ml om bi translated"><strong class="ak">Keras-RL—</strong><a class="ae ky" href="https://github.com/keras-rl/keras-rl" rel="noopener ugc nofollow" target="_blank"><strong class="ak">Github</strong></a></h2><p id="b71d" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">RL 算法的选择:☆☆☆ <br/>文档:☆☆☆ <br/>定制:☆☆☆☆ <br/>维护:☆ <br/>后端:Keras 和 Tensorflow 1.14。</p><h2 id="0294" class="nv lw it bd lx oc od dn mb oe of dp mf li og oh mh lm oi oj mj lq ok ol ml om bi translated"><strong class="ak">Keras-RL2—</strong><a class="ae ky" href="https://github.com/wau/keras-rl2" rel="noopener ugc nofollow" target="_blank">T3】GithubT5】</a></h2><p id="8e99" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">RL 算法的选择:☆☆ <br/>文档:不可用<br/>定制:☆☆☆☆ <br/>维护:☆☆ <br/>后端:Keras 和 Tensorflow 2.1.0。</p><h2 id="0ffd" class="nv lw it bd lx oc od dn mb oe of dp mf li og oh mh lm oi oj mj lq ok ol ml om bi translated"><strong class="ak"> OpenAI 基线—</strong><a class="ae ky" href="https://github.com/openai/baselines" rel="noopener ugc nofollow" target="_blank"><strong class="ak">Github</strong></a></h2><p id="8073" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">RL 算法的选择:☆☆☆ <br/>文档:☆☆ <br/>定制:☆☆ <br/>维护:☆☆☆ <br/>后端:Tensorflow 1.14。</p><h2 id="4d51" class="nv lw it bd lx oc od dn mb oe of dp mf li og oh mh lm oi oj mj lq ok ol ml om bi translated"><strong class="ak">稳定基线—</strong><a class="ae ky" href="https://github.com/hill-a/stable-baselines" rel="noopener ugc nofollow" target="_blank"><strong class="ak">Github</strong></a></h2><p id="27f5" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">RL 算法的选择:☆☆☆ <br/>文档:☆☆☆☆ <br/>定制:☆☆☆ <br/>维护:☆☆☆☆ <br/>后端:Tensorflow 1.14。</p><h2 id="d54e" class="nv lw it bd lx oc od dn mb oe of dp mf li og oh mh lm oi oj mj lq ok ol ml om bi translated"><strong class="ak">极致—</strong><a class="ae ky" href="https://github.com/deepmind/acme/tree/master/acme" rel="noopener ugc nofollow" target="_blank"><strong class="ak">Github</strong></a></h2><p id="4efe" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">RL 算法的选择:☆☆☆ <br/>文档:☆☆ <br/>定制:☆☆☆ <br/>维护:☆☆☆☆ <br/>后端:Tensorflow v2+和 JAX</p><p id="3930" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您已经决定了使用什么框架，那么现在您所需要的就是一个环境。您可以开始使用 OpenAI Gym，它已经在这些框架的大多数示例中使用，但是如果您想在其他任务上尝试 RL，如交易股票、联网或产生推荐，您可以在这里找到一个易于理解的现成可用环境列表:</p><div class="on oo gp gr op oq"><a href="https://medium.com/@mauriciofadelargerich/reinforcement-learning-environments-cff767bc241f" rel="noopener follow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">强化学习环境</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">我最近一直在研究强化学习(RL ),我发现有很多很棒的文章、教程…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">medium.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe ks oq"/></div></div></a></div><p id="9235" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你知道任何其他好的 RL 框架，请在下面的回复中告诉我！感谢阅读！:)</p></div></div>    
</body>
</html>