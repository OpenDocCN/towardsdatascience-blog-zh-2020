<html>
<head>
<title>Multi-Label, Multi-Class Text Classification with BERT, Transformers and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 BERT、Transformers 和 Keras 的多标签、多类别文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-label-multi-class-text-classification-with-bert-transformer-and-keras-c6355eccb63a?source=collection_archive---------4-----------------------#2020-08-25">https://towardsdatascience.com/multi-label-multi-class-text-classification-with-bert-transformer-and-keras-c6355eccb63a?source=collection_archive---------4-----------------------#2020-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bfde" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本文中，我将展示如何使用<a class="ae ki" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> Huggingface Transformers </a>库和<a class="ae ki" href="https://www.tensorflow.org/guide/keras/" rel="noopener ugc nofollow" target="_blank"> Tensorflow Keras API </a>完成多标签、多类别的文本分类任务。这样，您将了解如何使用 Transformer 中的 BERT 模型作为使用 Keras API 构建的 Tensorflow 模型中的一个层。</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/df2caa2f13f2c812108779a97b2aa317.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3RUx0Xi2-5Kepj4C58MwsQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">使用 BERT、Transformers 和 Keras 的多标签、多类别文本分类</p></figure><p id="55bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">互联网上充满了文本分类的文章，其中大多数是 BoW 模型与某种 ML 模型的结合，通常解决二进制文本分类问题。随着 NLP 的兴起，特别是 BERT(如果你不熟悉 BERT，请看这里的<a class="ae ki" rel="noopener" target="_blank" href="/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">)和其他基于多语言转换器的模型，越来越多的文本分类问题现在可以得到解决。</a></p><p id="0947" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，当涉及到使用<a class="ae ki" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> Huggingface Transformers </a>、<a class="ae ki" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank"> BERT </a>和<a class="ae ki" href="https://www.tensorflow.org/guide/keras/" rel="noopener ugc nofollow" target="_blank"> Tensorflow Keras </a>解决多标签、多类别文本分类问题时，文章的数量确实非常有限，就我个人而言，我还没有找到任何…还没有！</p><p id="fb01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在大量与 BERT、Keras 中的多标签分类或其他有用信息相关的博客帖子、教程和 GitHub 代码片段的帮助和启发下，我将向您展示如何构建一个工作模型，解决这个问题。</p><p id="108c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么使用拥抱脸变形金刚而不是谷歌自己的 BERT 解决方案？因为使用变压器很容易在不同型号之间切换，如伯特、阿尔伯特、XLnet、GPT-2 等。这意味着，您或多或少地“只是”在代码中用一个模型替换另一个模型。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="5369" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">从哪里开始</h2><p id="d354" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">用数据。在寻找可以用于多标签多类别文本分类任务的文本数据时，我偶然发现了来自 data.gov 的<a class="ae ki" href="https://www.consumerfinance.gov/data-research/consumer-complaints/" rel="noopener ugc nofollow" target="_blank">“消费者投诉数据库】</a>。看起来很管用，所以我们就用这个。</p><p id="8986" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来是探索性数据分析。显然，这对于正确理解数据的样子、可能存在的缺陷、数据的质量等等至关重要。但是我现在跳过这一步，只是因为这篇文章的目的纯粹是如何建立一个模型。</p><p id="5b99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你不喜欢四处搜索，可以看看这两篇关于这个主题的文章:<a class="ae ki" rel="noopener" target="_blank" href="/nlp-part-3-exploratory-data-analysis-of-text-data-1caa8ab3f79d"> NLP 第 3 部分|文本数据的探索性数据分析</a>和<a class="ae ki" rel="noopener" target="_blank" href="/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a">一个完整的文本数据的探索性数据分析和可视化</a>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="48e2" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">继续吧</h2><p id="82f4" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们有了数据，现在是编码部分。</p><p id="2b56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将加载所需的库。</p><pre class="kk kl km kn gt na nb nc nd aw ne bi"><span id="b492" class="mc md it nb b gy nf ng l nh ni">#######################################<br/>### -------- Load libraries ------- ###</span><span id="594a" class="mc md it nb b gy nj ng l nh ni"># Load Huggingface transformers<br/>from transformers import TFBertModel,  BertConfig, BertTokenizerFast</span><span id="f30e" class="mc md it nb b gy nj ng l nh ni"># Then what you need from tensorflow.keras<br/>from tensorflow.keras.layers import Input, Dropout, Dense<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras.optimizers import Adam<br/>from tensorflow.keras.callbacks import EarlyStopping<br/>from tensorflow.keras.initializers import TruncatedNormal<br/>from tensorflow.keras.losses import CategoricalCrossentropy<br/>from tensorflow.keras.metrics import CategoricalAccuracy<br/>from tensorflow.keras.utils import to_categorical</span><span id="f946" class="mc md it nb b gy nj ng l nh ni"># And pandas for data import + sklearn because you allways need sklearn<br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split</span></pre><p id="d1d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将导入我们的数据，并围绕它展开讨论，使其符合我们的需求。没什么特别的。请注意，我们将仅使用数据集中的“消费者投诉叙述”、“产品”和“问题”列。“消费者投诉叙述”将作为我们模型的输入，“产品”和“问题”将作为我们的两个输出。</p><pre class="kk kl km kn gt na nb nc nd aw ne bi"><span id="86be" class="mc md it nb b gy nf ng l nh ni">#######################################<br/>### --------- Import data --------- ###</span><span id="9412" class="mc md it nb b gy nj ng l nh ni"># Import data from csv<br/>data = pd.read_csv('dev/Fun with BERT/complaints.csv')</span><span id="cdf3" class="mc md it nb b gy nj ng l nh ni"># Select required columns<br/>data = data[['Consumer complaint narrative', 'Product', 'Issue']]</span><span id="c0e4" class="mc md it nb b gy nj ng l nh ni"># Remove a row if any of the three remaining columns are missing<br/>data = data.dropna()</span><span id="2299" class="mc md it nb b gy nj ng l nh ni"># Remove rows, where the label is present only ones (can't be split)<br/>data = data.groupby('Issue').filter(lambda x : len(x) &gt; 1)<br/>data = data.groupby('Product').filter(lambda x : len(x) &gt; 1)</span><span id="4632" class="mc md it nb b gy nj ng l nh ni"># Set your model output as categorical and save in new label col<br/>data['Issue_label'] = pd.Categorical(data['Issue'])<br/>data['Product_label'] = pd.Categorical(data['Product'])</span><span id="8400" class="mc md it nb b gy nj ng l nh ni"># Transform your output to numeric<br/>data['Issue'] = data['Issue_label'].cat.codes<br/>data['Product'] = data['Product_label'].cat.codes</span><span id="c653" class="mc md it nb b gy nj ng l nh ni"># Split into train and test - stratify over Issue<br/>data, data_test = train_test_split(data, test_size = 0.2, stratify = data[['Issue']])</span></pre><p id="f989" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将加载一些不同的变压器类。</p><pre class="kk kl km kn gt na nb nc nd aw ne bi"><span id="a7f6" class="mc md it nb b gy nf ng l nh ni">#######################################<br/>### --------- Setup BERT ---------- ###</span><span id="e36e" class="mc md it nb b gy nj ng l nh ni"># Name of the BERT model to use<br/>model_name = 'bert-base-uncased'</span><span id="b008" class="mc md it nb b gy nj ng l nh ni"># Max length of tokens<br/>max_length = 100</span><span id="cc12" class="mc md it nb b gy nj ng l nh ni"># Load transformers config and set output_hidden_states to False<br/>config = BertConfig.from_pretrained(model_name)<br/>config.output_hidden_states = False</span><span id="02ad" class="mc md it nb b gy nj ng l nh ni"># Load BERT tokenizer<br/>tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)</span><span id="6b2a" class="mc md it nb b gy nj ng l nh ni"># Load the Transformers BERT model<br/>transformer_model = TFBertModel.from_pretrained(model_name, config = config)</span></pre><p id="9629" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们首先加载一个控制模型、记号赋予器等的 BERT 配置对象。</p><p id="f63b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将在稍后的脚本中使用一个记号赋予器，将我们的文本输入转换成 BERT 记号，然后填充并截断到我们的最大长度。记号赋予器<a class="ae ki" href="https://huggingface.co/transformers/model_doc/bert.html#berttokenizer" rel="noopener ugc nofollow" target="_blank">已经被很好地记录了</a>，所以我不会在这里深入讨论。</p><p id="ffb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将加载 BERT 模型本身作为 BERT Transformers TF 2.0 Keras 模型(这里我们使用<a class="ae ki" href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">12 层 bert-base-uncased </a>)。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="c1ae" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">现在是有趣的部分</h2><p id="e37a" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们准备好建立我们的模型。在变形金刚库中，有许多<a class="ae ki" href="https://huggingface.co/transformers/model_doc/bert.html" rel="noopener ugc nofollow" target="_blank">不同的 BERT 分类模型可以使用</a>。所有模型的母亲是简单地称为“BertModel”(py torch)或“TFBertModel”(tensor flow)的模型，因此也是我们想要的模型。</p><p id="5dc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Transformers 库还附带了一个预构建的用于序列分类的 BERT 模型，称为“TFBertForSequenceClassification”。如果你看一下这里的代码<a class="ae ki" href="https://huggingface.co/transformers/_modules/transformers/modeling_tf_bert.html#TFBertForSequenceClassification" rel="noopener ugc nofollow" target="_blank">你会发现，他们从加载一个干净的 BERT 模型开始，然后简单地添加一个漏失层和一个密集层。因此，我们要做的只是简单地添加两个密集层，而不是只有一个。</a></p><p id="e11e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们模型的样子:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/78597a7b32595e6f10b4792a0860143e.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*0OTdwlfFjjX3XC4wYaUsSA.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">基于 BERT、Transformer 和 Keras 模型的多标签多类别文本分类</p></figure><p id="b3fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更详细的模型视图:</p><pre class="kk kl km kn gt na nb nc nd aw ne bi"><span id="1a14" class="mc md it nb b gy nf ng l nh ni">Model: "BERT_MultiLabel_MultiClass"<br/>___________________________________________________________________<br/>Layer (type)      Output Shape       Param #    Connected to       <br/>===================================================================<br/>input_ids         [(None, 100)]      0                             <br/>(InputLayer)<br/>___________________________________________________________________<br/>bert              (                  109482240  input_ids[0][0]    <br/>(TFBertMainLayer)  (None, 100, 768),<br/>                   (None, 768)<br/>                  )<br/>___________________________________________________________________<br/>pooled_output     (None, 768)        0          bert[1][1]         <br/>(Dropout)<br/>___________________________________________________________________<br/>issue             (None, 159)        122271     pooled_output[0][0]<br/>(Dense)<br/>___________________________________________________________________<br/>product           (None, 18)         13842      pooled_output[0][0]<br/>(Dense)<br/>===================================================================<br/>Total params: 109,618,353<br/>Trainable params: 109,618,353<br/>Non-trainable params: 0<br/>___________________________________________________________________</span></pre><p id="7eef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想更多地了解 BERTs 建筑本身，<a class="ae ki" href="https://medium.com/analytics-vidhya/understanding-bert-architecture-3f35a264b187" rel="noopener">看看这里</a>。</p><p id="0e3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了模型架构，我们需要做的就是用代码编写它。</p><pre class="kk kl km kn gt na nb nc nd aw ne bi"><span id="8c6a" class="mc md it nb b gy nf ng l nh ni">#######################################<br/>### ------- Build the model ------- ###</span><span id="d381" class="mc md it nb b gy nj ng l nh ni"># TF Keras documentation: <a class="ae ki" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/keras/Model</a></span><span id="52a9" class="mc md it nb b gy nj ng l nh ni"># Load the MainLayer<br/>bert = transformer_model.layers[0]</span><span id="38a8" class="mc md it nb b gy nj ng l nh ni"># Build your model input<br/>input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')<br/>inputs = {'input_ids': input_ids}</span><span id="9ad5" class="mc md it nb b gy nj ng l nh ni"># Load the Transformers BERT model as a layer in a Keras model<br/>bert_model = bert(inputs)[1]<br/>dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')<br/>pooled_output = dropout(bert_model, training=False)</span><span id="29fe" class="mc md it nb b gy nj ng l nh ni"># Then build your model output<br/>issue = Dense(units=len(data.Issue_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='issue')(pooled_output)<br/>product = Dense(units=len(data.Product_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='product')(pooled_output)<br/>outputs = {'issue': issue, 'product': product}</span><span id="8d38" class="mc md it nb b gy nj ng l nh ni"># And combine it all in a model object<br/>model = Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')</span><span id="8944" class="mc md it nb b gy nj ng l nh ni"># Take a look at the model<br/>model.summary()</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="ae9c" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">让魔法开始吧</h2><p id="f406" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">然后剩下要做的就是编译我们的新模型，并根据我们的数据进行拟合。</p><pre class="kk kl km kn gt na nb nc nd aw ne bi"><span id="bbdd" class="mc md it nb b gy nf ng l nh ni">#######################################<br/>### ------- Train the model ------- ###</span><span id="fd76" class="mc md it nb b gy nj ng l nh ni"># Set an optimizer<br/>optimizer = Adam(<br/>    learning_rate=5e-05,<br/>    epsilon=1e-08,<br/>    decay=0.01,<br/>    clipnorm=1.0)</span><span id="c629" class="mc md it nb b gy nj ng l nh ni"># Set loss and metrics<br/>loss = {'issue': CategoricalCrossentropy(from_logits = True), 'product': CategoricalCrossentropy(from_logits = True)}<br/>metric = {'issue': CategoricalAccuracy('accuracy'), 'product': CategoricalAccuracy('accuracy')}</span><span id="feab" class="mc md it nb b gy nj ng l nh ni"># Compile the model<br/>model.compile(<br/>    optimizer = optimizer,<br/>    loss = loss, <br/>    metrics = metric)</span><span id="e12d" class="mc md it nb b gy nj ng l nh ni"># Ready output data for the model<br/>y_issue = to_categorical(data['Issue'])<br/>y_product = to_categorical(data['Product'])</span><span id="f124" class="mc md it nb b gy nj ng l nh ni"># Tokenize the input (takes some time)<br/>x = tokenizer(<br/>    text=data['Consumer complaint narrative'].to_list(),<br/>    add_special_tokens=True,<br/>    max_length=max_length,<br/>    truncation=True,<br/>    padding=True, <br/>    return_tensors='tf',<br/>    return_token_type_ids = False,<br/>    return_attention_mask = False,<br/>    verbose = True)</span><span id="516c" class="mc md it nb b gy nj ng l nh ni"># Fit the model<br/>history = model.fit(<br/>    x={'input_ids': x['input_ids']},<br/>    y={'issue': y_issue, 'product': y_product},<br/>    validation_split=0.2,<br/>    batch_size=64,<br/>    epochs=10)</span></pre><p id="748c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦模型被拟合，我们就可以根据我们的测试数据对它进行评估，看看它的表现如何。</p><pre class="kk kl km kn gt na nb nc nd aw ne bi"><span id="1776" class="mc md it nb b gy nf ng l nh ni">#######################################<br/>### ----- Evaluate the model ------ ###</span><span id="beba" class="mc md it nb b gy nj ng l nh ni"># Ready test data<br/>test_y_issue = to_categorical(data_test['Issue'])<br/>test_y_product = to_categorical(data_test['Product'])<br/>test_x = tokenizer(<br/>    text=data_test['Consumer complaint narrative'].to_list(),<br/>    add_special_tokens=True,<br/>    max_length=max_length,<br/>    truncation=True,<br/>    padding=True, <br/>    return_tensors='tf',<br/>    return_token_type_ids = False,<br/>    return_attention_mask = False,<br/>    verbose = True)</span><span id="7105" class="mc md it nb b gy nj ng l nh ni"># Run evaluation<br/>model_eval = model.evaluate(<br/>    x={'input_ids': test_x['input_ids']},<br/>    y={'issue': test_y_issue, 'product': test_y_product}<br/>)</span></pre><p id="0336" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实证明，我们的模型表现得相当好，具有相对较好的准确性。尤其是考虑到我们的输出“产品”由 18 个标签组成，而“问题”由 159 个不同的标签组成。</p><pre class="kk kl km kn gt na nb nc nd aw ne bi"><span id="1365" class="mc md it nb b gy nf ng l nh ni">####################################################################<br/>Classification metrics for Product</span><span id="608d" class="mc md it nb b gy nj ng l nh ni">                             precision    recall  f1-score   support<br/>    Bank account or service       0.63      0.36      0.46      2977<br/>Checking or savings account       0.60      0.75      0.67      4685<br/>              Consumer Loan       0.48      0.29      0.36      1876<br/>                Credit card       0.56      0.42      0.48      3765<br/>Credit card or prepaid card       0.63      0.71      0.67      8123<br/>           Credit reporting       0.64      0.37      0.47      6318<br/>   Credit reporting, credit <br/>  repair services, or other<br/>  personal consumer reports       0.81      0.85      0.83     38529<br/>            Debt collection       0.80      0.85      0.82     23848<br/>    Money transfer, virtual <br/> currency, or money service       0.59      0.65      0.62      1966<br/>            Money transfers       0.50      0.01      0.01       305<br/>                   Mortgage       0.89      0.93      0.91     13502<br/>    Other financial service       0.00      0.00      0.00        60<br/>                Payday loan       0.57      0.01      0.02       355<br/>Payday loan, title loan, or<br/>              personal loan       0.46      0.40      0.43      1523<br/>               Prepaid card       0.82      0.14      0.24       294<br/>               Student loan       0.83      0.87      0.85      5332<br/>      Vehicle loan or lease       0.49      0.51      0.50      1963<br/>           Virtual currency       0.00      0.00      0.00         3</span><span id="8fb9" class="mc md it nb b gy nj ng l nh ni">                   accuracy                           0.76    115424<br/>                  macro avg       0.57      0.45      0.46    115424<br/>               weighted avg       0.75      0.76      0.75    115424</span><span id="2436" class="mc md it nb b gy nj ng l nh ni">####################################################################<br/>Classification metrics for Issue (only showing summarized metrics)</span><span id="5182" class="mc md it nb b gy nj ng l nh ni">precision    recall  f1-score   support<br/>                   accuracy                           0.41    115424<br/>                  macro avg       0.09      0.08      0.06    115424</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="8be4" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">下一步做什么？</h2><p id="fe62" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">但是，您可以做很多事情来提高这个模型的性能。在这里，我尽量做到简单，但如果您希望获得更好的性能，请考虑以下几点:</p><ul class=""><li id="299f" class="nl nm it lb b lc ld lf lg li nn lm no lq np lu nq nr ns nt bi translated">摆弄优化器中设置的超参数，或者改变优化器本身</li><li id="f56c" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">使用消费者投诉数据库数据训练一个语言模型——要么从头开始，要么通过微调现有的 BERT 模型(<a class="ae ki" href="https://github.com/huggingface/transformers/tree/master/examples/language-modeling" rel="noopener ugc nofollow" target="_blank">在这里看看</a>)。然后加载该模型，而不是这里使用的“bert-base-uncased”。</li><li id="1032" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">使用多个输入。在我们当前的设置中，我们只使用令牌 id 作为输入。然而，如果我们在输入中加入注意力屏蔽，我们可能会获得一些性能提升。它非常简单，看起来像这样:</li></ul><pre class="kk kl km kn gt na nb nc nd aw ne bi"><span id="ae0b" class="mc md it nb b gy nf ng l nh ni"># Build your model input</span><span id="d605" class="mc md it nb b gy nj ng l nh ni">input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')</span><span id="ed20" class="mc md it nb b gy nj ng l nh ni">attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32')</span><span id="c778" class="mc md it nb b gy nj ng l nh ni">inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}</span></pre><p id="2794" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nz">(记得在拟合你的模型时添加 attention_mask，并在你的 tokenizer 中设置 return_attention_mask 为 True。更多关于注意力面具的信息，</em> <a class="ae ki" href="https://huggingface.co/transformers/glossary.html#attention-mask" rel="noopener ugc nofollow" target="_blank"> <em class="nz">看这里</em> </a> <em class="nz">。此外，我在下面的要点中添加了注意力面具，并将其注释掉，以供你的</em>灵感<em class="nz">。)</em></p><ul class=""><li id="96a1" class="nl nm it lb b lc ld lf lg li nn lm no lq np lu nq nr ns nt bi translated">尝试另一个模型，如艾伯特、罗伯塔、XLM，甚至是自回归模型，如 GPT-2 或 XLNet——所有这些都可以通过 Transformers 库轻松导入到您的框架中。您可以在此处找到所有直接可用型号的概述<a class="ae ki" href="https://huggingface.co/transformers/model_summary.html" rel="noopener ugc nofollow" target="_blank">。</a></li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="ce11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样——希望你喜欢这个如何用 BERT、Transformer 和 Keras 进行“多标签、多类文本分类”的小演示。如果你有任何反馈或问题，请在下面的评论中提出。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="oa ob l"/></div></figure></div></div>    
</body>
</html>