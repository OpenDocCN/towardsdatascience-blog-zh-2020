<html>
<head>
<title>Sentiment analysis in less than 50 lines of Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不到 50 行 Python 代码中的情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-analysis-in-less-than-50-lines-of-python-fc6451114c6?source=collection_archive---------45-----------------------#2020-05-02">https://towardsdatascience.com/sentiment-analysis-in-less-than-50-lines-of-python-fc6451114c6?source=collection_archive---------45-----------------------#2020-05-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="77da" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">神经网络并不总是答案……</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/2f251567ba1b6220a9294a8f6a6877f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SnEwD6vd2JZWUjPivhSXTQ.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">照片由 pixabay 上的<a class="ae kz" href="https://pixabay.com/users/Free-Photos-242387/" rel="noopener ugc nofollow" target="_blank"> Free-Photos-242387 </a>拍摄</p></figure><p id="1b77" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">自然语言处理(NLP)可能是一个难以理解的话题。NLP 涵盖了大量不同的学科和建模技术，从主题分析到机器翻译和语音识别到语音合成。情感分析包含在 NLP 系列中，是对一段文本中表达的情感进行量化的过程。通常，情感是根据文本的积极或消极程度来评估的。</p><p id="f028" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最先进的情感分析技术通常涉及循环神经网络，如长期短期记忆模型(LSTMs)或计算成本高昂的 BERT 嵌入，大量的<a class="ae kz" rel="noopener" target="_blank" href="/sentiment-analysis-using-lstm-step-by-step-50d074f09948">优秀材料</a>已被写入如何创建和训练这些。然而，也许你不需要复杂的建模或大量的代码来实现有竞争力的性能。</p><p id="012e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">那么，我们如何着手创建自己的情感分析模型呢？</p><h2 id="44e5" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">数据</h2><p id="4dc9" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">这里使用的数据可以在<a class="ae kz" href="https://www.kaggle.com/c/tweet-sentiment-extraction/overview" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上找到，包括训练集中的 27，481 条带标签的推文和测试集中的 3，534 条推文。因此，让我们继续使用每个数据科学家最好的朋友熊猫来加载数据:</p><pre class="kk kl km kn gt mu mv mw mx aw my bi"><span id="2b80" class="lw lx it mv b gy mz na l nb nc">import pandas as pd</span><span id="b29a" class="lw lx it mv b gy nd na l nb nc">training_df = pd.read_csv("train.csv")<br/>training_df.head()</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ne"><img src="../Images/253ee2e007edf9c90a78e2ef88da766b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YKv6SITTFuRWCc-ydda0nw.png"/></div></div></figure><p id="b106" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们对测试集做同样的事情:</p><pre class="kk kl km kn gt mu mv mw mx aw my bi"><span id="cb75" class="lw lx it mv b gy mz na l nb nc">test_df = pd.read_csv("train.csv")<br/>test_df.head()</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nf"><img src="../Images/09e6e0f8ee30ad61a0a974816f9ed854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qGQ6hVH8MuijiJMVSgu17w.png"/></div></div></figure><p id="ddcd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">注意，测试集没有<code class="fe ng nh ni mv b">selected_text</code>列。因此，让我们确保我们所做的任何数据清理都适用于训练集和测试集的<code class="fe ng nh ni mv b">text</code>列。</p><h2 id="f405" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">数据处理</h2><p id="1b99" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">为了简单起见，我们不想在清理方面做得太多，但我们可以做一些简单的事情来帮助任何模型识别情绪。数据清理过程如下:</p><ol class=""><li id="3fcd" class="nj nk it lc b ld le lg lh lj nl ln nm lr nn lv no np nq nr bi translated">删除推文中的所有超链接</li><li id="b35f" class="nj nk it lc b ld ns lg nt lj nu ln nv lr nw lv no np nq nr bi translated">替换常见的缩写</li><li id="d5fd" class="nj nk it lc b ld ns lg nt lj nu ln nv lr nw lv no np nq nr bi translated">删除标点符号(分词器为我们做了这件事)</li></ol><p id="f8fc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在我们清理的同时，我们还可以将情感标签映射到整数，并从同一个函数返回它们。</p><pre class="kk kl km kn gt mu mv mw mx aw my bi"><span id="1571" class="lw lx it mv b gy mz na l nb nc">import re</span><span id="1514" class="lw lx it mv b gy nd na l nb nc">CONTRACTIONS_DICT = {"can`t":"can not",<br/>                     "won`t":"will not",<br/>                     "don`t":"do not",<br/>                     "aren`t":"are not",<br/>                     "i`d":"i would",<br/>                     "couldn`t": "could not",<br/>                     "shouldn`t": "should not",<br/>                     "wouldn`t": "would not",<br/>                     "isn`t": "is not",<br/>                     "it`s": "it is",<br/>                     "didn`t": "did not",<br/>                     "weren`t": "were not",<br/>                     "mustn`t": "must not",<br/>                    }</span><span id="8c1a" class="lw lx it mv b gy nd na l nb nc">def prepare_data(df:pd.DataFrame) -&gt; pd.DataFrame:<br/>    <br/>    df["text"] = df["text"] \<br/>              .apply(lambda x: re.split('http:\/\/.*', str(x))[0]) \<br/>              .str.lower() \<br/>              .apply(lambda x: replace_words(x,contractions_dict))<br/>        <br/>    df["label"] = df["sentiment"].map(<br/>                        {"neutral": 1, "negative":0, "positive":2 }<br/>                        )<br/>    return df.text.values, df.label.values</span><span id="0427" class="lw lx it mv b gy nd na l nb nc">def replace_words(string:str, dictionary:dict):<br/>    for k, v in dictionary.items():<br/>        string = string.replace(k, v)<br/>    return string</span><span id="be0c" class="lw lx it mv b gy nd na l nb nc">train_tweets, train_labels = prepare_data(train_df)<br/>test_tweets, test_labels = prepare_data(test_df)</span></pre><h2 id="d529" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">句子分词器:</h2><p id="2844" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">现在我们需要将每条 tweet 标记成一个固定长度的向量——特别是一个嵌入了的<a class="ae kz" rel="noopener" target="_blank" href="/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089"> TFIFD。为了实现这一点，我们可以使用 Keras 的内置<code class="fe ng nh ni mv b">Tokenizer()</code>，适合训练数据。</a></p><pre class="kk kl km kn gt mu mv mw mx aw my bi"><span id="d5a8" class="lw lx it mv b gy mz na l nb nc">from keras.preprocessing.text import Tokenizer</span><span id="dcb1" class="lw lx it mv b gy nd na l nb nc">tokenizer = Tokenizer()<br/>tokenizer.fit(train_tweets)</span><span id="57a5" class="lw lx it mv b gy nd na l nb nc">train_tokenized = tokenizer.texts_to_matrix(<br/>                             train_tweets,<br/>                             mode='tfidf'<br/>                             )</span><span id="d3b2" class="lw lx it mv b gy nd na l nb nc">test_tokenized = tokenizer.texts_to_matrix(<br/>                             test_tweets,<br/>                             mode='tfidf'<br/>                             )</span></pre><h2 id="6f65" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">系统模型化</h2><p id="c0ab" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">接下来，我们需要选择我们的模型。当谈到情感分析时，世界是你的牡蛎，然而为了长话短说，我们求助于每个数据科学家的第二好朋友，<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit" rel="noopener ugc nofollow" target="_blank"> sk-learn 的随机森林分类器</a>:</p><pre class="kk kl km kn gt mu mv mw mx aw my bi"><span id="fde7" class="lw lx it mv b gy mz na l nb nc">from  sklearn.ensemble import RandomForestClassifier</span><span id="7422" class="lw lx it mv b gy nd na l nb nc">forest = RandomForestClassifier(<br/>                                n_estimators=500, <br/>                                min_samples_leaf=2,<br/>                                oob_score=True,<br/>                                n_jobs=-1,<br/>                                )<br/>forest.fit(train_tokenized,train_labels)</span><span id="9ce0" class="lw lx it mv b gy nd na l nb nc">print(f"Train score: {forest.score(train_tokenized,train_labels)}")<br/>print(f"OOB score: {forest.oob_score_}")</span></pre><p id="5c62" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">选择<code class="fe ng nh ni mv b">n_estimators</code>和<code class="fe ng nh ni mv b">num_samples_leaf</code>时进行了少量的超参数调整，以尝试最大化分数，同时避免过度拟合。这给了我们 0.77 的训练得分和 0.69 的出袋得分(关于出袋得分的更多信息，<a class="ae kz" rel="noopener" target="_blank" href="/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710">这篇</a>文章很好地解释了这一点，但是 TL；DR 是随机森林的验证分数的代理)。</p><p id="8bcd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，让我们来看看测试集上的性能:</p><pre class="kk kl km kn gt mu mv mw mx aw my bi"><span id="13e6" class="lw lx it mv b gy mz na l nb nc">print("Test score: {forest.score(test_tokenized,test_labels)"}</span></pre><p id="da00" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这个值为 0.69，所以到目前为止，我们进展顺利！</p><h2 id="67ab" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">混淆矩阵</h2><p id="b52c" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">sk-learn 可以轻松获取分类器和测试数据来生成混淆矩阵，显示测试集的性能，如下所示:</p><pre class="kk kl km kn gt mu mv mw mx aw my bi"><span id="73c7" class="lw lx it mv b gy mz na l nb nc">from sklearn.metrics import plot_confusion_matrix</span><span id="ae53" class="lw lx it mv b gy nd na l nb nc">plot_confusion_matrix(<br/>    forest,<br/>    test_encoded_docs,<br/>    test_labels,<br/>    display_labels=["Negative","Neutral","Positive"],<br/>    normalize='true'<br/>)</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/057289b1a1101dd465414fd41c41781c.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*8ez87K3qzuzmgn9-WnbVLA.png"/></div></figure><p id="0f46" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">正如混淆矩阵所显示的，到目前为止，当谈到我们卑微的随机森林时，一切都很好。</p><p id="b5d6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这告诉我们很多关于我们所犯错误的类型。大多数错误发生在预测积极和中性情绪以及消极和中性情绪之间的差异时，在错误的大格局中，这并不是最糟糕的。然而值得庆幸的是，我们很少混淆积极情绪和消极情绪，反之亦然。</p><h2 id="1ffc" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">我们与 VADER 相比如何？</h2><p id="212b" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">VADER 是一个“基于规则的情绪分析工具，专门针对社交媒体表达的情绪”。但是，对我们来说重要的是，这是一个现成的模型，可以用作基准。</p><pre class="kk kl km kn gt mu mv mw mx aw my bi"><span id="7040" class="lw lx it mv b gy mz na l nb nc">from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer</span><span id="61f1" class="lw lx it mv b gy nd na l nb nc">analyzer = SentimentIntensityAnalyzer()<br/>vs=[]<br/>for sentence in test_tweets:<br/>    vs.append(analyzer.polarity_scores(sentence)["compound"])</span><span id="ddc8" class="lw lx it mv b gy nd na l nb nc">vader_scores = []<br/>for v in vs:<br/>    if v &lt; -0.5:<br/>        score = 0<br/>    elif v &lt; 0.5:<br/>        score = 1<br/>    else:<br/>        score = 2<br/>    vader_scores.append(score)</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/46255ef84c49f84bf5204eadd288c3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*scdsjsw6nn_OZ6dfT2uibg.png"/></div></figure><p id="8367" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">给定阈值 0.5，VADER 在中性情绪类上实现了大致相同的真阳性率，但在积极和消极情绪类上的表现都比我们的模型差——很高兴知道训练我们自己的模型是值得的！</p><h2 id="32aa" class="lw lx it bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn mo bi translated">那么我们在这里学到了什么？</h2><p id="e675" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">情绪分析并不像看起来那么神秘。当然，如果我们增加模型的复杂性，改进数据清理和使用更复杂的嵌入，我们可以做得更好，我们仍然能够实现合理的性能，而不需要任何东西。当你考虑到当前 Kaggle 竞赛的领先者得了 0.73 分，而我们用不到 50 行代码就达到了 0.69 分，我很高兴把它留在那里…</p></div></div>    
</body>
</html>