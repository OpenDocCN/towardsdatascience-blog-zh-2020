<html>
<head>
<title>Value Iteration for Q-function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Q-函数的值迭代</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/value-iteration-for-q-function-ac9e508d85bd?source=collection_archive---------47-----------------------#2020-06-15">https://towardsdatascience.com/value-iteration-for-q-function-ac9e508d85bd?source=collection_archive---------47-----------------------#2020-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f172" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/deep-r-l-explained" rel="noopener" target="_blank">深度强化学习讲解— 11 </a></h2><div class=""/><div class=""><h2 id="1f8e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Q函数的冻结湖码</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fa0f6596bcb8f7d913d0517d5a751d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDhcpkgdzkisARJU93i5xw.png"/></div></div></figure><p id="d3cb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在<a class="ae lz" rel="noopener" target="_blank" href="/value-iteration-for-v-function-d7bcccc1ec24">的上一篇文章</a>中，我们介绍了如何通过求解冰湖环境来实现计算状态值V-function的值迭代法。在本帖中，我们将回顾Q函数，并展示学习动作值以创建策略的值迭代方法。</p><p id="3f1d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">正如我们将在本文后面看到的，Q值在实践中更方便，对于代理来说，基于Q值做出行动决策比基于V值更简单。</p><blockquote class="ma mb mc"><p id="f93a" class="ld le md lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><a class="ae lz" href="https://medium.com/aprendizaje-por-refuerzo/4-programaci%C3%B3n-din%C3%A1mica-924c5abf3bfc" rel="noopener">本出版物西班牙文版</a></p></blockquote><div class="mh mi gp gr mj mk"><a href="https://medium.com/aprendizaje-por-refuerzo/4-programaci%C3%B3n-din%C3%A1mica-924c5abf3bfc" rel="noopener follow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">4.数字电视节目</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">请访问第4页的自由介绍</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">medium.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="ae5c" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">Q函数的值迭代</h1><p id="1410" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">值迭代法可用于学习V值或Q函数。也就是说，将状态值或动作值存储在一个表中。在这里，我们将介绍如何使用值迭代方法来计算Q值，而不是上一篇文章中介绍的V值。</p><p id="9d52" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">基于V-function的先前值迭代方法实现，在动作值的情况下，只需要对先前的代码进行微小的修改。最明显的变化是我们的价值表。在前一个例子中，我们保存了状态的值，所以字典中的键只是一个状态。现在我们需要存储Q函数的值，它有两个参数:state和action，所以值表中的键现在是一个复合键。</p><p id="c918" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">关于更新，在<a class="ae lz" rel="noopener" target="_blank" href="/the-bellman-equation-59258a0d3fa7">关于贝尔曼方程的帖子</a>中，我们表明我们行动状态的最优值可以定义为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/6233cacf1a53924c7a312e4edcdb48df.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*n1lYko22IRGQSHIn0gABEA.png"/></div></figure><p id="3054" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">接下来，我们将使用Q函数来求解冰湖环境。</p><h1 id="e67c" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">Q函数的值迭代在实践中的应用</h1><blockquote class="ma mb mc"><p id="c1db" class="ld le md lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated">这篇文章的全部代码可以在GitHub 上找到，而<a class="ae lz" href="https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_11_VI_Algorithm_for_Q.ipynb" rel="noopener ugc nofollow" target="_blank">可以通过这个链接</a>作为一个谷歌笔记本来运行。</p></blockquote><p id="d25e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后，我们将在用Q-函数构成值迭代方法的代码中展示与V-函数版本的主要区别。</p><p id="917c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">保存我们的表和函数的中心数据，我们将在训练循环中使用，与前面的V-function示例相同。主要的变化是对<code class="fe nx ny nz oa b">values</code>表的修改，这个字典现在将一个状态-动作对映射到这个动作的计算值。</p><p id="1834" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在前一个例子中，我们保存了状态的值，所以字典中的键只是一个状态。现在我们需要存储Q函数的值，它有两个参数:state和action，所以值表中的键现在是一个复合键。这意味着另一个区别在于<code class="fe nx ny nz oa b">calc_action_value</code>函数。我们只是不再需要它，因为我们的动作值存储在值表中。</p><p id="8f09" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后，代码中最重要的变化是代理的<code class="fe nx ny nz oa b">value_iteration()</code>方法。以前，它只是一个围绕<code class="fe nx ny nz oa b">calc_action_value()</code>调用的包装器，完成贝尔曼近似的工作。现在，由于这个函数已经消失并被一个值表所取代，我们需要在<code class="fe nx ny nz oa b">value_iteration()</code>方法中做这个近似。</p><h1 id="d316" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">Q函数的冻结湖码</h1><p id="165a" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">让我们看看代码。由于与之前的实现几乎相同，我将直接跳到主要区别，读者可以在<a class="ae lz" href="https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_11_VI_Algorithm_for_Q.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub代码</a>中了解细节。先说主要功能<code class="fe nx ny nz oa b">value_iteration()</code>:</p><pre class="ks kt ku kv gt ob oa oc od aw oe bi"><span id="3599" class="of na it oa b gy og oh l oi oj">def value_iteration_for_Q(self):<br/>    for state in range(self.env.observation_space.n):<br/>        for action in range(self.env.action_space.n):<br/>            action_value = 0.0<br/>            target_counts = self.transits[(state, action)]<br/>            total = sum(target_counts.values())<br/>            for tgt_state, count in target_counts.items():<br/>                key = (state, action, tgt_state)<br/>                reward = self.rewards[key]<br/>                best_action = self.select_action(tgt_state)<br/>                val = reward + GAMMA * \<br/>                      self.values[(tgt_state, best_action)]<br/>                action_value += (count / total) * val<br/>           self.values[(state, action)] = action_value</span></pre><p id="4081" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">读者会注意到，该代码相当于前面实现中的<code class="fe nx ny nz oa b">calc_action_value</code>代码。其思想是，对于给定的状态和动作，它需要使用我们通过函数<code class="fe nx ny nz oa b">play_n_random_steps</code>收集的信息来计算动作值，该函数从环境中随机播放<code class="fe nx ny nz oa b">N</code>步骤，用随机经验填充<code class="fe nx ny nz oa b">reward</code>和<code class="fe nx ny nz oa b">transits</code>表。</p><p id="fad6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然而，在前面的实现中，我们将V函数存储在值表中，所以我们只是从该表中取出它。我们不能再这样做了，所以我们必须调用<code class="fe nx ny nz oa b">select_action</code>方法，它将为我们选择Q值最大的最佳动作，然后我们将这个Q值作为目标状态的值。</p><p id="fb22" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">事实上，这个方法的实现是不同的，因为它不再调用<code class="fe nx ny nz oa b">calc_action_valu</code>方法，而是我们只是迭代动作，并在我们的值表中查找它们的值。</p><pre class="ks kt ku kv gt ob oa oc od aw oe bi"><span id="e94d" class="of na it oa b gy og oh l oi oj">def select_action(self, state):<br/>    best_action, best_value = None, None<br/>    for action in range(self.env.action_space.n):<br/>        action_value = self.values[(state, action)]<br/>        if best_value is None or best_value &lt; action_value:<br/>           best_value = action_value<br/>           best_action = action<br/>    return best_action</span></pre><p id="1c21" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">正如你所注意到的，学习循环的代码和前一篇文章中的一样。以及测试客户端并在TensorBoard中绘制结果的代码:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/191fb15aae849168922cc818f757a426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*YXjRV_GhS9KNvrzhBIfLlQ.png"/></div></figure><p id="b601" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后，读者可以像以前一样，用FrozenLake8x8环境进行测试，或者测试其他超参数。</p><blockquote class="ma mb mc"><p id="7e0f" class="ld le md lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><strong class="lf jd">鸣谢:这篇文章中的代码灵感来自于马克西姆·拉潘的代码</strong> <a class="ae lz" href="https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/tree/master/Chapter04" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">，他写了一本关于</strong> </a> <strong class="lf jd">主题的优秀实用书籍。</strong></p></blockquote><h1 id="c473" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">结论</h1><p id="d66c" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">在实践中，Q值要方便得多，对于代理来说，基于Q值做出关于动作的决策要比基于V值简单得多。在Q值的情况下，为了基于状态选择动作，代理只需要使用当前状态计算所有可用动作的Q值，并选择具有最大Q值的动作。</p><p id="2074" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了使用状态的值来做同样的事情，<strong class="lf jd"> V值，</strong>代理<strong class="lf jd">不仅需要知道值，还需要知道转移的概率。</strong>在实践中，我们很少预先知道它们，所以代理需要估计每个动作和状态对的转移概率。</p><p id="9d60" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在V-函数的值迭代方法中，这种对概率的依赖给代理增加了额外的负担。也就是说，了解这种方法很重要，因为它们是高级方法的重要组成部分。</p><p id="6932" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/reviewing-essential-concepts-from-part-1-e28234ee7f4f">下期见！</a></p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><h1 id="8d67" class="mz na it bd nb nc os ne nf ng ot ni nj ki ou kj nl kl ov km nn ko ow kp np nq bi translated">深度强化学习讲解系列</h1><p id="c09a" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated"><strong class="lf jd">由</strong> <a class="ae lz" href="https://www.upc.edu/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> UPC巴塞罗那理工</strong> </a> <strong class="lf jd">和</strong> <a class="ae lz" href="https://www.bsc.es/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">巴塞罗那超级计算中心</strong> </a></p><p id="4cf4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一个轻松的介绍性<a class="ae lz" href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener ugc nofollow" target="_blank">系列</a>以一种实用的方式逐渐向读者介绍这项令人兴奋的技术，它是人工智能领域最新突破性进展的真正推动者。</p><div class="mh mi gp gr mj mk"><a href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">深度强化学习解释-乔迪托雷斯。人工智能</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">本系列的内容</h3></div></div><div class="mt l"><div class="ox l mv mw mx mt my lb mk"/></div></div></a></div><h1 id="89c0" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">关于这个系列</h1><p id="87a4" class="pw-post-body-paragraph ld le it lf b lg nr kd li lj ns kg ll lm nt lo lp lq nu ls lt lu nv lw lx ly im bi translated">我在五月份开始写这个系列，那是在巴塞罗那的禁闭期。老实说，由于封锁，在业余时间写这些帖子帮助我到了<a class="ae lz" href="https://twitter.com/hashtag/StayAtHome?src=hashtag_click" rel="noopener ugc nofollow" target="_blank"><strong class="lf jd">#寄宿家庭</strong> </a>。感谢您当年阅读这份刊物；这证明了我所做的努力。</p><p id="9082" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">免责声明</strong> —这些帖子是在巴塞罗纳封锁期间写的，目的是分散个人注意力和传播科学知识，以防对某人有所帮助，但不是为了成为DRL地区的学术参考文献。如果读者需要更严谨的文档，本系列的最后一篇文章提供了大量的学术资源和书籍供读者参考。作者意识到这一系列的帖子可能包含一些错误，如果目的是一个学术文件，则需要对英文文本进行修订以改进它。但是，尽管作者想提高内容的数量和质量，他的职业承诺并没有留给他这样做的自由时间。然而，作者同意提炼所有那些读者可以尽快报告的错误。</p></div></div>    
</body>
</html>