<html>
<head>
<title>NVIDIA NeMo — Building Custom Speech Recognition Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NVIDIA NeMo —构建自定义语音识别模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nvidia-nemo-building-custom-speech-recognition-model-d614289f0277?source=collection_archive---------42-----------------------#2020-09-07">https://towardsdatascience.com/nvidia-nemo-building-custom-speech-recognition-model-d614289f0277?source=collection_archive---------42-----------------------#2020-09-07</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><figure class="iq ir gq gs is it gi gj paragraph-image"><div role="button" tabindex="0" class="iu iv di iw bf ix"><div class="gi gj ip"><img src="../Images/503bbfe0985936e8e5652c18628a6530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zi_M2YZmOAjZlYGVr1y0NA.jpeg"/></div></div><p class="ja jb gk gi gj jc jd bd b be z dk translated"><a class="ae je" href="https://www.pexels.com/photo/aluminum-audio-battery-broadcast-270288/" rel="noopener ugc nofollow" target="_blank">https://www . pexels . com/photo/aluminum-audio-battery-broadcast-270288/</a></p></figure><div class=""/><div class=""><h2 id="b9a1" class="pw-subtitle-paragraph ke jg jh bd b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv dk translated">介绍</h2></div><p id="693d" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">NVIDIA NeMo 是一个对话式人工智能工具包。该工具包是一个加速器，它帮助研究人员和从业人员对复杂的神经网络架构进行实验。语音处理(识别和合成)和自然语言处理是该平台的重要功能。因为它来自 NVIDIA，所以完全支持 GPU。该框架依赖 PyTorch 作为深度学习框架。</p><p id="561a" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">在本笔记本中，我们将尝试如何创建自动语音识别(ASR)。在本教程中，我们将使用 LibriSpeech 数据集。</p><h2 id="89eb" class="ls lt jh bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">设置</h2><p id="a251" class="pw-post-body-paragraph kw kx jh ky b kz ml ki lb lc mm kl le lf mn lh li lj mo ll lm ln mp lp lq lr ik bi translated">对于这个实验，以下软件:Ubuntu 16.04 Anaconda 4 . 7 . 11 NeMo—【https://github.com/NVIDIA/NeMo】T2 卡拉迪—<a class="ae je" href="https://github.com/kaldi-asr/kaldi" rel="noopener ugc nofollow" target="_blank">https://github.com/kaldi-asr/kaldi</a>按照软件自述文件中的说明运行代码。确保您安装的 PyTorch 支持 GPU。硬件规格至少需要 6g 的 GPU RAM。</p><h2 id="f277" class="ls lt jh bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">数据</h2><p id="ecb1" class="pw-post-body-paragraph kw kx jh ky b kz ml ki lb lc mm kl le lf mn lh li lj mo ll lm ln mp lp lq lr ik bi translated">LibriSpeech 是一个开放域语音识别数据集。我们可以从这里下载数据<a class="ae je" href="http://www.openslr.org/12." rel="noopener ugc nofollow" target="_blank">http://www.openslr.org/12.</a>对于本教程，我们使用的是 dev-clean 数据集—<a class="ae je" href="http://www.openslr.org/resources/12/dev-clean.tar.gz" rel="noopener ugc nofollow" target="_blank">http://www.openslr.org/resources/12/dev-clean.tar.gz</a>。为了在一个非常小的 GPU 占用空间中轻松进行培训，我们从文件夹“dev-clean/84/121123/84”和“dev-clean/84/121550/”中选择数据。</p><p id="8cf0" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">语音文件存储在。flac 格式，应该转换成。' wav '格式的 NeMo 工作。NeMo 培训需要一个“清单”文件。“清单”文件包含“”的路径。wav '(演讲录音)，演讲持续时间，以及每个录音的抄本。</p><p id="b197" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">为了让生活变得简单，我们创建了一个实用程序来转换。flac' to '。“wav”和元数据文件。</p><pre class="mq mr ms mt gu mu mv mw mx aw my bi"><span id="0d25" class="ls lt jh mv b gz mz na l nb nc">from wavconvert import create_nemo_manifest</span></pre><h2 id="331a" class="ls lt jh bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">创建培训清单文件</h2><pre class="mq mr ms mt gu mu mv mw mx aw my bi"><span id="3118" class="ls lt jh mv b gz mz na l nb nc">flac_path = "/home/jaganadhg/AI_RND/nvidianemo/LibriSpeech/dev-clean/84/121550/"<br/>meta_apth = "meta_train.json"<br/><br/>create_nemo_manifest(flac_path,<br/>    meta_apth)</span><span id="2d2d" class="ls lt jh mv b gz nd na l nb nc">flac_path = "/home/jaganadhg/AI_RND/nvidianemo/LibriSpeech/dev-clean/84/121123/"<br/>meta_apth = "meta_val.json"<br/><br/>create_nemo_manifest(flac_path,<br/>    meta_apth)</span></pre><h2 id="071e" class="ls lt jh bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">模型训练</h2><p id="6118" class="pw-post-body-paragraph kw kx jh ky b kz ml ki lb lc mm kl le lf mn lh li lj mo ll lm ln mp lp lq lr ik bi translated">让我们跳到建立一个模型。我们稍后将讨论 FFT、频谱和语言模型。创建一个实用程序脚本来抽象该过程。QuartzNet15x5 型号用作基础型号。语音识别结果用单词错误率(WER)来评估。实用程序脚本实现了一个 WER 计算器。</p><h2 id="363c" class="ls lt jh bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">注意-相应地调整历元值以得到一个合适的模型。</h2><pre class="mq mr ms mt gu mu mv mw mx aw my bi"><span id="9246" class="ls lt jh mv b gz mz na l nb nc">from asrtrainer import (train_model,<br/>        computer_wer)<br/>from ruamel.yaml import YAML</span><span id="d63a" class="ls lt jh mv b gz nd na l nb nc">config_path = 'quartznet_15x5.yaml'<br/>train_manfest = "metadata.json"<br/>val_manifest = "metadata_validation.json"<br/><br/>yaml = YAML(typ='safe')<br/>with open(config_path) as f:<br/>    model_params = yaml.load(f)<br/>        <br/>my_asr_model = train_model(model_params,<br/>                            train_manfest,<br/>                            val_manifest,<br/>                            5,<br/>                            False)<br/>    <br/>wer = computer_wer(model_params,<br/>                    my_asr_model)</span></pre><h2 id="061b" class="ls lt jh bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">保存的模型可以保存到。nemo 的格式。</h2><pre class="mq mr ms mt gu mu mv mw mx aw my bi"><span id="5a3f" class="ls lt jh mv b gz mz na l nb nc">my_asr_model.save_to("tutorial.nemo")</span></pre><h2 id="aa4d" class="ls lt jh bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">后续步骤</h2><p id="f1b4" class="pw-post-body-paragraph kw kx jh ky b kz ml ki lb lc mm kl le lf mn lh li lj mo ll lm ln mp lp lq lr ik bi translated">在本教程中，我们创建了一个非常简单的模型，它的性能可能并不好。我们可以尝试构建一个更大的数据集，也许是整个 LibriSpeech dev-clean。时代的增加(我尝试了 1000 个时代，转录看起来不错！).</p><p id="0cf8" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">如果您有兴趣进一步了解，可以在“quartznet_13x5.yaml”文件中找到模型配置。</p><p id="00ac" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ik bi translated">该代码可在 https://github.com/jaganadhg/nemoexamples 获得。</p></div></div>    
</body>
</html>