<html>
<head>
<title>K Means Clustering Without Libraries</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k 表示没有库的集群</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-without-libraries-python-feb3572e2eef?source=collection_archive---------4-----------------------#2020-08-28">https://towardsdatascience.com/k-means-without-libraries-python-feb3572e2eef?source=collection_archive---------4-----------------------#2020-08-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6d5247ed27e7a46127e599d5d0bfcd72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mmHsZlwXnaWJRpW50gHBHg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Billy Huynh 在<a class="ae jg" href="https://unsplash.com/s/photos/cluster?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="d97e" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated"><strong class="ak"> <em class="ky"> —使用 Python </em> </strong></h2></div><p id="8646" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> Kmeans 是一种广泛使用的聚类工具，用于分析和分类数据。然而，我怀疑，人们往往并不完全了解幕后发生了什么。如果你理解最终产品传达了什么，这不一定是一件坏事，但是通过从头开始构建算法来了解发生了什么肯定会导致对其背后的推理的更深入的理解。</em></p><p id="e558" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lw translated"><span class="l lx ly lz bm ma mb mc md me di">我</span> <strong class="lb jk">想先强调一下</strong>互联网是程序员和工程师的绝佳去处。答案和资源随处可得，只需谷歌搜索即可。假装这一切都是我自己想出来的是愚蠢的。我很乐意承认，有时需要通读其他人在算法方面的工作，才能理解如何更好地接近它。代码的美妙之处在于它可以用许多不同的方式编写，每种方式强调的质量都略有不同。在你的学习中利用这一点。</p><p id="395b" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我已经谈到了那一点，让我们开始吧！</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="8f93" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb jk"> K 表示聚类</strong>最简单的形式是一种算法，它在数据簇中找到密切的关系，并将它们分组以便于分类。</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mm"><img src="../Images/4dec8573fee40abc8b00e7b5375dd295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJDFY2nkjy1WEiKtxtEQSQ.png"/></div></div></figure><p id="6674" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你在这里看到的是一种算法，根据特定的质量将不同的数据点分类成组或段…接近(或接近)一个中心点。</p><p id="295f" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最常见的是 Scikit-Learn 的 KMeans 算法，看起来像这样:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="d47c" class="mw mx jj ms b gy my mz l na nb">from sklearn.cluster import KMeans</span><span id="f2eb" class="mw mx jj ms b gy nc mz l na nb">km = KMeans(<br/>       <em class="lv">n_clusters</em>=3, <em class="lv">init</em>='random',<br/>       <em class="lv">n_init</em>=10, <em class="lv">max_iter</em>=300,<br/>       <em class="lv">random_state</em>=42<br/>       )</span><span id="a3c1" class="mw mx jj ms b gy nc mz l na nb">y_km = km.fit_predict(X)</span></pre><p id="4d3c" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能不太理解这些部分，但它的方法相当简单。它主要做的是，它说我们需要 3 个集群，从 10 次迭代开始(或运行，每次细化集群和位置)，3 个中心点的初始化是随机的，最大迭代次数是 300，随机状态只是指每次运行它，它都是相同的。然后我们运行预测。更多信息可在此处<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank">阅读</a>关于可使用的不同参数。</p><p id="f5f5" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，我们如何从头开始创建这些代码呢……尤其是在我们不确定发生了什么的情况下？我们来想办法吧！</p><p id="8850" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一步是思考并描述正在发生的事情。首先，<a class="ae jg" rel="noopener" target="_blank" href="/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0">这篇文章</a>很好地描述了每一步。总之，我们在散点图上绘制出<em class="lv"> k </em>数量的点(也称为<em class="lv">质心)</em>，通常是随机的，并找到最接近这些点的数据。然后，我们不断地重新计算从数据到质心的平均距离和质心位置，直到每个<em class="lv"> k </em>质心周围都有清晰的数据组。</p><p id="738a" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我失去你了吗？希望不会。让我们浏览一下每个流程，看看发生了什么:</p><ul class=""><li id="abdf" class="nd ne jj lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">第一步是我们需要决定要将数据分成多少个簇。这有一个方法，但是为了简单起见，我们说我们将使用 3 个集群，或者，<em class="lv"> k = </em> 3。代码看起来像这样:</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="7a24" class="mw mx jj ms b gy my mz l na nb">k = 3<br/>clusters = {}</span><span id="301a" class="mw mx jj ms b gy nc mz l na nb">for i in range(k):<br/>clusters[i] = []</span></pre><p id="ff87" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你在上面看到的只是创建了 3 个空的集群。看起来是这样的…</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="7a92" class="mw mx jj ms b gy my mz l na nb">{0: [], 1: [], 2: []}</span></pre><p id="8b1b" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很简单，对吧？</p><p id="37a4" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们以类似的方式设置质心，但这次我们使用我们正在使用的数据。在我的例子中，我使用的是<a class="ae jg" href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.data" rel="noopener ugc nofollow" target="_blank">波士顿住房数据集</a>。<em class="lv"> X，在本例中，</em>是我从数据集中选择的两个数据点的数组。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="a3d4" class="mw mx jj ms b gy my mz l na nb">for i in range(k):<br/>    centroids[i] = X[i]</span></pre><p id="0766" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们需要找到每个数据点到质心的距离。这个概念很简单，但是下一个块一开始看起来有点混乱。我建议搜索和阅读这篇文章的不同部分，以便更好地了解正在发生的事情。<em class="lv">例如，如果你在谷歌上搜索“np.linalg.norm ”,你会发现</em> <a class="ae jg" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html" rel="noopener ugc nofollow" target="_blank"> <em class="lv">这个页面</em> </a> <em class="lv">描述了它是什么以及它的作用。</em></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="b243" class="mw mx jj ms b gy my mz l na nb">for data in X:<br/>    euc_dist = []<br/>    for j in range(k):<br/>        euc_dist.append(np.linalg.norm(data - centroids[j]))<br/>    clusters[euc_dist.index(min(euc_dist))].append(data)</span></pre><p id="3c1e" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">初始化质心和聚类后，我们要重新计算这两个值！为什么？因为它们在某种程度上是随机初始化的，所以我们需要慢慢地将它们移向数据自然分段的最理想方式(如果有的话，<em class="lv">，但那是另一个讨论</em>)。</p><p id="b0d2" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我写了两个函数来实现这个功能。让我们来看看:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="eb0b" class="mw mx jj ms b gy my mz l na nb">def recalculate_clusters(<em class="lv">X</em>, <em class="lv">centroids</em>, <em class="lv">k</em>):<br/>    """ Recalculates the clusters """<br/>    # Initiate empty clusters<br/>    clusters = {}<br/>    # Set the range for value of k (number of centroids)<br/>    for i in range(k):<br/>        clusters[i] = []<br/>    for data in X:<br/>        euc_dist = []<br/>        for j in range(k):<br/>            euc_dist.append(np.linalg.norm(data - centroids[j]))<br/>        # Append the cluster of data to the dictionary<br/>        clusters[euc_dist.index(min(euc_dist))].append(data)<br/>    return clusters</span><span id="bcca" class="mw mx jj ms b gy nc mz l na nb">def recalculate_centroids(<em class="lv">centroids</em>, <em class="lv">clusters</em>, <em class="lv">k</em>):<br/>    """ Recalculates the centroid position based on the plot """<br/>    for i in range(k):<br/>        centroids[i] = np.average(clusters[i], <em class="lv">axis</em>=0)<br/>    return centroids</span></pre><p id="a485" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望您能在这两个函数中识别出一些相同的代码。请密切注意不同的零件和部件。学习的最好方法之一是剖析内在发生的事情。再一次，我要求你用谷歌搜索这段代码的个别部分。这就像把收音机拆开，然后再组装起来。把那个内在的工程师拿出来！</p><p id="b380" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从那里，我们将把一个绘图函数放在一起，绘制每个集群，并为其分配不同的颜色。这就像将数据输入分拣机，分拣机根据数据的去向对不同的数据进行颜色编码。出来的东西看起来像这样:</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/90239a08f681ad583097de4193b2f8e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*rHLgpA7I40jE6KjD4zNQ8w.png"/></div></figure><p id="5059" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到数据被清晰地分割成不同的部分，尽管它们分布得不是很好。这是因为这仅仅是数据的第一次迭代！我还应该提到，这种形状并不完全适合于聚类，这本身就是一个关于算法的优点和缺点的教训。</p><p id="f618" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，当我们希望在集群之间有更好的分布时，我们该怎么做呢？…重新计算，重新计算，重新计算！在这种情况下，如果我们运行它，比如说，10 次，它将如下所示:</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/09e5423f3dc506ff0af5f1f1d1a39839.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*YlrWLlAGzln5BOldjSM5EQ.png"/></div></figure><p id="ee8a" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在有一个稍微好一点的分配，不是吗？我们看到的是数据中 3 种不同的非监督分类。算法告诉我们，这三种颜色可能意味着数据中至少值得研究的东西。请注意，这并不意味着它实际上很重要。这就是数据的有趣之处。计算机努力增强我们在数据中寻找关系的能力，但最终还是要由我们来决定这些关系(如果有的话)意味着什么。你的数据科学工作可能会持续一段时间。唷！</p><p id="aaa6" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更有趣的旁注之一…当决定运行多少次迭代时(或者换句话说，你想要重新计算多少次)，你可以把通常所说的“肘图”放在一起，看看迭代真正开始失去区分能力的地方。我的看起来像这样:</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/8e8046130439e7a84824a5a3ed8f7c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*qq_4LI5uh0LqFPn-gh9wAw.png"/></div></figure><p id="daa9" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以看到，大约 3 到 4 次重复后，它开始失去每次迭代在调整这些簇和质心时产生的动量。这是一个很好的检查有多少计算你真的想运行。毕竟这对于一个公司来说，时间就是金钱，资源一般都是有限的！当然，这是非常低级的东西，所以没什么大不了的，但它总是值得你和/或团队就项目和分析进行一次对话。</p><p id="df2d" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要查看整个(<em class="lv">进行中</em>)笔记本，请导航到我的 GitHub 库，这里是<a class="ae jg" href="https://github.com/lechemrc/CS-Build-Week" rel="noopener ugc nofollow" target="_blank">T5！你将会看到我如何选择数据集以及最初探索数据集的一些细节。</a></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="3631" class="pw-post-body-paragraph kz la jj lb b lc ld kk le lf lg kn lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢 K 均值聚类的概述！请注意，我对数据科学的世界还相当陌生，所以我绝对愿意接受本文中详述的观点和方法的修正。毕竟，我觉得学习是持续的，我一点也不介意提高我解释和利用这些方法的能力。我们都从错误中学习！如果您觉得有任何错误或需要澄清的地方，请联系我们。谢谢大家！</p></div></div>    
</body>
</html>