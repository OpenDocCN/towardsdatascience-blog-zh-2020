# 我的 2019 年十大深度 RL 论文

> 原文：<https://towardsdatascience.com/my-top-10-deep-rl-papers-of-2019-68d0000704e2?source=collection_archive---------17----------------------->

## 引人入胜的一年的主要趋势和亮点

2019 年是深度强化学习(DRL)研究的一年，也是我作为该领域博士生的第一年。像每个博士新手一样，我不得不花大量时间阅读论文，实现可爱的想法&对大问题有所感悟。在这篇博文中，我想分享我在 2019 年文献中的一些亮点。

![](img/dd8a7a5e921864ef8c695a97fa50c0a9.png)

为了让这篇文章更有条理，我决定将论文分成 5 个主要类别，并选出一个冠军和一个亚军。事不宜迟，以下是我 2019 年的十大 DRL 论文。

**免责声明**:我没有阅读 2019 年以来的每一篇 DRL 论文(这将是相当大的挑战)。相反，我试图提取一些关键的叙事以及让我兴奋的故事。这就是我的*个人十大*——如果我错过了你最喜欢的论文，请告诉我！🤖🌏🧠

# 第一类:大型项目

Deep RL(例如 ATARI DQNs、AlphaGo/Zero)在 2019 年之前取得的大多数突破性成就都是在具有有限行动空间、完全可观察状态空间以及适度信用分配时间尺度的领域中取得的。部分可观测性、长时间尺度以及广阔的行动空间仍然是虚幻的。另一方面，2019 年证明，我们还远远没有达到将函数近似与基于奖励的目标优化相结合的极限。诸如[《雷神之锤 3》/【夺旗】](https://deepmind.com/blog/article/capture-the-flag-science)、[《星际争霸 2》](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning)、[《Dota 2》](https://openai.com/projects/five/)以及[《机器人手操纵》](https://openai.com/blog/solving-rubiks-cube/)这样的挑战仅仅凸显了现代 DRL 有能力应对的令人兴奋的新领域的一个子集。我试图根据科学贡献来选择第一类的获奖者，而不仅仅是现有算法的大规模扩展。**每个人——只要有足够的计算能力——都可以用疯狂的批量进行 PPO**。

![](img/de04dc3201746a9c1d9bad80b262f4e0.png)

**🏆-** [**DeepMind 的 AlphaStar (Vinyals 等人，2019)**](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning)

“大规模 DRL 项目”类别的第一名是……”(插入一只手拿着麦克风笨拙地打开信封)+🥁:·deep mind 的 AlphaStar 项目，由 Oriol Vinyals 领导。在阅读《自然》杂志的论文时，我意识到这个项目在很大程度上是基于用于处理《雷神之锤 3》的 FTW 设置:将**分布式 IMPALA 演员-学习者设置与诱导结构化探索的强大先验结合起来**。

![](img/03d6f04159d6b2274a02df5bd54b873c.png)

FTW 使用基于两个 LSTMs 的时间尺度层次的先验，而 AlphaStar 使用人类演示。*专家演示*用于通过监督最小化 KL 目标来预先训练代理的策略&提供有效的调整，以确保代理的探索行为不会被星际争霸的维数灾难淹没。但这绝对不是全部。科学贡献包括一个独特版本的**优先虚构自我游戏**(又名*联盟*)、一个带有指针网络的自回归分解策略、**上行策略更新**(UPGO——结构化动作空间的 V-trace 偏离策略重要性采样校正的演变)以及**分散连接**(一种特殊的嵌入形式，可保持地图层中实体的空间一致性)。就个人而言，我真的很欣赏 DeepMind，尤其是 Oriol Vinyals 对星际社区的关注。科幻小说常常使我们的认知偏向于认为 ML 是一场军备竞赛。但是它是人类为了提高我们的生活质量而创造的。

**2️⃣ -** [**OpenAI 的‘魔方求解’(open ai，2019)**](https://arxiv.org/abs/1910.07113)

众所周知，深度学习能够解决需要提取和处理高级特征的任务。另一方面，低水平的灵巧性，一种对我们来说如此自然的能力，为当前的系统提供了一个主要挑战。至少我们是这么认为的。OpenAI 的灵巧性努力中我最喜欢的贡献是**自动域随机化(ADR** ):在机器人任务上训练深度 RL 代理的一个关键挑战是将在模拟中学到的知识转移到物理机器人上。模拟器仅捕获现实世界中有限的一组机制&精确模拟摩擦需要计算时间。昂贵的时间&可以用于在环境中产生更多(但有噪声的)转换。已经提出了域随机化以获得稳健的策略。不是用一组生成环境的超参数在单个环境中训练代理，而是在大量不同的配置中训练代理。 **ADR 旨在设计一个环境复杂的课程，以最大化学习进度**。通过*根据代理的学习进度自动*增加/减少可能的环境配置的范围，ADR 为代理提供了一个伪自然的课程。令人惊讶的是，这(以及基于 PPO-LSTM-GAE 的政策)引发了一种元学习形式，而这种形式似乎还没有达到其全部能力(截至本文发表之时)。Twitter 上有很多关于“解决”这个词的讨论。该算法没有端到端地“完全”了解解决立方体&的正确移动顺序，然后进行所需的灵巧操作。但老实说，哪一个更令人印象深刻:在疯狂的回报稀疏的情况下进行手动操作，还是学习一个相当短的符号变换序列？Woj Zaremba 在 NeurIPS 2019 的“学习可转移技能”研讨会上提到，他们花了一天时间与 DRL &一起“解决立方体”,这表明完全端到端地完成整个哑谜是可能的。令人印象深刻。

# 第二类:基于模型的 RL

虽然前两个项目令人兴奋地展示了 DRL 的潜力，但它们的样本效率低得离谱。我不想知道电费，OpenAI & DeepMind 要交。好消息是，有人通过在潜在空间产生幻觉来提高样本(但不一定是计算)的效率。传统上，**基于模型的 RL 一直在努力学习高维状态空间的动力学**。通常，许多模型容量不得不“浪费”在状态空间的不相关部分(例如，ATARI 帧的最外部像素)，这很少与成功相关。最近，有多种在抽象空间进行规划/想象的提议(例如，一个**抽象 MDP** )。这是我最喜欢的两种方法:

![](img/a1ef765c17f991bad9914a6d807e9d6a.png)

**🏆-** [**穆泽罗(Schrittwieser 等人，2019)**](https://arxiv.org/pdf/1911.08265.pdf)

MuZero 提供了从 AlphaGo/AlphaZero 项目中移除约束的下一次迭代。具体来说，它克服了过渡动态的认可。因此，通用 MCTS +函数逼近工具箱对更通用的问题设置开放，例如基于视觉的问题(例如 ATARI)。

![](img/ff08f74383d5a7688f72ccbf2d663a02.png)

该问题被简化为预测奖励、价值和政策的回归，以及将观察映射到抽象空间的表示函数 *h* 的学习，动态函数 *g* 以及政策和价值预测器 *f* 。然后可以通过在给定嵌入观察的潜在空间中展开确定性动力学模型来进行规划。与之前一样，下一个动作是根据 MCTS 展示&与访问计数成比例的采样来选择的。使用 BPTT &对整个架构进行端到端训练，在低样本状态下，其性能优于 AlphaGo 和 ATARI 基线。有趣的是，能够模拟奖励、价值观政策似乎是有效计划所需要的。作者表示，潜在空间的规划也开启了 MCTS 在随机转变环境中的应用——如果你问我，我会觉得相当令人兴奋。

**2️⃣ -** [**梦想家(又名。星球 2.0；哈夫纳等人，2019)**](https://arxiv.org/pdf/1912.01603.pdf)

另一方面，梦想家提供了对连续行动空间的原则性扩展，能够基于高维视觉输入驯服长期任务。表征学习问题被分解为迭代学习表征、转换和奖励模型。通过使用想象的轨迹训练基于行动者-批评者的策略，整个优化过程是交错的。**梦想家通过世界模型的想象轨迹传播学习状态值的“分析”梯度来学习**。更具体地说，多步回报的随机梯度通过使用重新参数化技巧的神经网络预测被有效地传播。该方法在 DeepMind 控制套件中进行了评估，能够基于 64 x 64 x 3 维视觉输入控制行为。最后，作者还比较了不同的表征学习方法(奖励预测、像素重构&对比估计/观察重构),并表明像素重构通常优于对比估计。

# 第三类:多代理 RL

代理超越了简单的中央控制模式。我们的日常生活充满了需要预测和心理理论的情况。我们不断假设其他人的反应，并根据最近的证据重新调整我们的信念。通过梯度下降的简单独立优化容易陷入局部最优。这在一个由两个特工甘训练的简单社会中已经变得很明显了。联合学习导致了环境中一种形式的非平稳性，这是多智能体 RL (MARL)的核心挑战。两篇精选的 MARL 论文强调了两个中心点:从经典的集中训练+分散控制范式走向社会奖励塑造&自我游戏的规模化使用和意想不到的结果:

![](img/6064905e62f16ec64f2f484216bbf36b.png)

**🏆-** [**【社会影响】作为内在动机(Jaques et al .，2019)**](https://arxiv.org/abs/1810.08647)

虽然传统的内在动机方法通常是临时的和人工定义的，但本文引入了一个因果概念，即通过由有影响力的行为产生的伪奖励来实现社会授权。关键的想法是奖励导致其他代理行为相对较大变化的行为。

![](img/f586223a0f836209a01cbf5597370b06.png)

因此，影响的概念是基于一种反事实的评估:如果我在这种情况下采取不同的行动，另一个代理人的行动会如何改变。边际和其他代理人的行动条件政策之间的 KL 差异可以被视为社会影响的一个衡量标准。作者在一组连续的社会困境中测试了提出的内在动机公式，并为增强的紧急协调提供了证据。此外，当允许向量值通信时，社会影响奖励成形导致信息&稀疏通信协议。最后，他们通过让代理学习预测彼此的行为，一种心理理论的软版本，来摆脱对其他代理策略的集中访问。

**2️⃣ -** [**自动课程&紧急工具使用(OpenAI，2019)**](https://arxiv.org/abs/1909.07528)

严格来说，OpenAI 的这项工作可能不会被视为一个纯粹的泥灰文件。有一个基于 A3C-PPO-LSTM-GAE 的中央控制器，而不是学习一组分散的控制器。尽管如此，训练是通过多智能体自我游戏和你能想象到的最简单的奖励来进行的:在多智能体捉迷藏游戏中生存。作者展示了这样一个简单的奖励结构如何与自我游戏相结合，可以导致比内在动机更有效的自我监督技能习得。用作者的话说:

> *“当一个新的成功策略或突变出现时，它会改变邻近代理需要解决的隐含任务分配，并产生新的适应压力。”*

这种自主课程的出现和显性策略的区分最终导致了意想不到的解决方案(比如在物体上冲浪)。代理人经历了 6 个不同阶段的主导策略，其中的转变是基于与环境中的工具的相互作用。由于基于团队的奖励，隐藏者学会了分工。最后，关于大规模实施的一些有趣的观察:

1.  在 MARL 中训练中央控制器时，大批量是非常重要的。它们不仅显著地稳定了学习，而且允许更大的学习速率和更大的时期。
2.  根据对所有代理的状态观察来调节评论家，能够向行动者提供更强有力的反馈信号。这是洛等人(2017) 在马-DDPG 的论文中已经做出的观察。

# 第四类:学习动力

深度学习中的学习动力仍远未被理解。与监督学习不同，在监督学习中，训练数据在某种程度上是给定的，并被视为 IID(独立且同分布)，RL 需要一个代理来生成他们自己的训练数据。这可能导致显著的不稳定性(例如致命三和弦)，这是任何玩弄 dqn 的人都会经历的。仍然有一些围绕新发现的重大理论突破(如神经正切核)。动态类别的两个获奖者强调了基于记忆的元学习(比 RL 更普遍)以及基于策略的 RL 的基本特征:

![](img/a10a4691bc59cabca7afe3106d28a935.png)

**🏆——**[**非交错元学习者动力学(Rabinowitz，2019)**](https://arxiv.org/abs/1905.01320)

最近，在理解深度学习的学习动力学方面有了一些进展&随机梯度下降。其中包括关于交错任务发现的发现(如 [Saxe et al .，2013](https://arxiv.org/pdf/1312.6120)；[拉哈曼等人，2018](https://arxiv.org/pdf/1806.08734) 。理解元学习的动力学(例如，[王等，2016](https://arxiv.org/abs/1611.05763) ) &另一方面，外环和内环学习之间的关系仍然是虚幻的。本文试图解决这一问题。

![](img/ae56e98a8108979ab3993952a1cd7ad4.png)

作者从经验上证明了元学习内环经历了非常不同的动态。**元学习者不是顺序地发现任务结构，而是同时学习整个任务。**这让人想起贝叶斯最优推理&为元学习&经验贝叶斯之间的联系提供了证据。因此，外部学习循环对应于在内部循环期间学习用于快速适应的最佳先验。每当系统的实际学习行为很重要时(例如，课程学习、安全探索以及人在回路中的应用)，这些发现都很重要。最后，它可能有助于我们设计出允许快速适应的学习信号。

**2️⃣ -** [**射线干涉(Schaul et al .，2019)**](https://arxiv.org/abs/1904.11455)

光线干扰是在(多目标)深度 RL 中观察到的一种现象，当学习动力学通过一系列平台时。**这些学习曲线步骤的转变与交错的发现相关联(&忘却！)和路径是由学习和数据生成的耦合引起的，这种耦合是由于政策上的展开而产生的，因此是一种干扰。这限制了代理一次学习一件事，而对单个上下文的并行学习将是有益的。作者推导出一个分析关系的动力系统，并显示了鞍点过渡的联系。实证验证是在情境强盗上进行的。我很想知道在经典的基于策略的连续控制任务中，干扰问题有多严重。此外，我个人对这可能与基于群体的训练(PBT)等进化方法的关系感到特别兴奋。作者指出，PBT 可以抵御这种有害的政策影响。PBT 不是训练单个代理，而是并行训练具有不同超参数的群体。因此，合奏可以产生多种多样的体验，这些体验可以通过群体成员的多样性来克服高原。**

# 第五类:合成和先验

获得有效且快速适应的代理的一种方法是知情的先验。代理可以依赖先前以先验分布形式提取的知识，而不是基于非信息知识库进行学习，但是如何获得这样的知识呢？下面两篇论文提出了两种不同的方法:同时学习目标不可知的默认策略&学习能够表示大量专家行为的密集嵌入空间。

![](img/7df7dffedeb82ee27f67d3bd5772b23e.png)

**🏆-**[**KL-正则化 RL 中的信息不对称(Galashov et al .，2019)**](https://arxiv.org/abs/1905.01240)

作者提供了一种在学习问题中利用重复结构的方法。更具体地，默认策略的学习是通过限制默认策略接收的状态信息(例如，外部对本体感受)来实施的。KL 正则化的预期回报目标然后可以被重写，使得计算代理的策略和接收部分输入的默认策略之间的差异。然后，通过在代理策略的梯度下降更新(标准 KL 目标-正则化)和默认策略(给定代理策略轨迹的监督学习-提取)之间交替进行优化。

![](img/8a9328acf8d437331fc24d7894574a90.png)

在几个实验中表明，这可能会导致在稀疏奖励环境中的可重用行为。通常 DeepMindLab 的大动作空间是被一个人类的先验(或者偏见)缩小了。作者表明，这可以通过学习一个约束行动空间的默认策略来规避&从而降低探索问题的复杂性。可以表明，存在各种连接到信息瓶颈的想法，以及学习生成模型使用变分 EM 算法。

**2️⃣ -** [**NPMP:神经概率运动原语(Merel et al .，2019)**](https://arxiv.org/abs/1811.11711)

少击学习一直被认为是智力的关键。这需要大量的归纳&我们人类一直在这样做。实现这种灵活性的一种机制是子程序的模块化重用。因此，在电机控制文献中，已经讨论了一组可以有效重组和整形的电机原语/默认值。在今天帖子的最后一篇论文中，Merel 等人(2019)将这种直觉投射到深度概率模型领域。作者介绍了一种具有潜在变化瓶颈的自动编码器架构，以在潜在嵌入空间中提取大量专家策略。重要的是，专家策略不是任意的预先训练的 RL 代理，而是 2 秒钟的动作捕捉数据片段**。他们的主要目标是提取不仅能够对行为的关键维度进行编码，而且在执行过程中也容易回忆起来的表征。该模型归结为一个状态条件行动序列的自回归潜变量模型。给定当前历史和一个小的前瞻片段，模型必须预测实现这种转换的动作(也称为逆模型)。因此，该动作可以被认为是未来轨迹和过去潜在状态之间的瓶颈。考虑到这样一个强大的“运动原语”嵌入，我们仍然需要获得学生策略。Merel 等人(2019 年)反对行为克隆的观点，因为这往往导致样本效率低下或不稳健。相反，他们将专家概念化为围绕单一*名义轨迹*的非线性反馈控制器。然后，他们记录每个动作-状态对的雅可比矩阵，并优化扰动目标，这类似于一种去噪自动编码器。他们的实验表明，这能够提取 2707 个专家&执行有效的一次性转移，从而产生平滑的行为。**

# **结论**

**总而言之，2019 年凸显了深度 RL 在以前无法想象的维度中的巨大潜力。突出显示的大型项目仍然远非样本高效。但是这些问题正在被当前寻找有效的归纳偏差、先验和基于模型的方法所解决。**

**![](img/9a98027098bfe727dbf01818ad9f6a55.png)**

**我对 2020 年即将到来的事情感到兴奋&我相信这是一个在这个领域工作的绝佳时机。存在重大问题，但一个人所能产生的影响也是相当大的。没有比现在更好的生活时机了。**

***原载于 2020 年 1 月 14 日*[*https://roberttlange . github . io*](https://roberttlange.github.io/posts/2019/12/blog-post-9/)*。***