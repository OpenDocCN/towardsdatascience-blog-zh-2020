<html>
<head>
<title>Discover the Sentiment of Reddit Subgroup using RoBERTa Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用 RoBERTa 模型发现 Reddit 子群的情感</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/discover-the-sentiment-of-reddit-subgroup-using-roberta-model-10ab9a8271b8?source=collection_archive---------27-----------------------#2020-07-14">https://towardsdatascience.com/discover-the-sentiment-of-reddit-subgroup-using-roberta-model-10ab9a8271b8?source=collection_archive---------27-----------------------#2020-07-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1d1d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们将使用预训练的 RoBERTa 模型来建立情感分析模型，以发现 Reddit 子群的情感。</h2></div><p id="fd3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">被困在付费墙后面？点击这里阅读这篇文章和我的朋友链接。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/f252509a98eb7b059f03afb3631ad0b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yyCFsRdWPImHJrwcnTDNPQ.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">资料来源:jeffrey grospe(联合国统计司)</p></figure><p id="920b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当你登录社交媒体账号，阅读开篇帖子时，你有什么感受？它是让你微笑还是让你悲伤或生气？我有一个复杂的经历。大多数时候，社交媒体上的帖子让我很开心。怎么会？好吧，我们不能控制其他人发布什么，但我们可以控制我们想在自己的社交媒体账户上看到什么。</p><p id="26e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你加入了一个负面评价很高的群体，那么你会更频繁地阅读那些评论。那会让你生气和难过。在它对你精神健康造成损害之前，离开那些有毒的群体。</p><p id="dc92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以如果我让你找出你社交媒体账户的有毒群体，你能做到吗？</p><p id="1076" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章将帮助你建立一个模型，帮助你总结所有帖子或评论的观点。所以你可以在他们让你觉得想退出社交媒体之前离开那些群。</p><p id="f958" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在本文中使用 Reddit 社交媒体参考。我将分析我的 Reddit 子群。并且检查这些子群是否具有高数量的负面评论。</p><h2 id="2f77" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">为什么是 Reddit？</h2><p id="2bd0" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">像 Reddit 和 twitter 这样的社交媒体将允许你通过 API 访问用户的帖子和评论。您可以在 Reddit 数据上测试和实现情感分析模型。</p><p id="6635" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">本文分为两部分。在第一部分中，我将建立一个 RoBERTa 模型。在第二部分，我们分析了 Reddit 子群的情感。</strong></p><h1 id="4611" class="mt lw it bd lx mu mv mw ma mx my mz md jz na ka mg kc nb kd mj kf nc kg mm nd bi translated">罗伯塔模型的建立</h1><p id="219b" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">我们将使用 twitter 数据集对预训练模型 RoBERTa 进行训练和微调。你可以在这里找到数据<a class="ae le" href="https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/" rel="noopener ugc nofollow" target="_blank"/>。该数据集包含积极和消极情绪的推文。我选择了二元情感数据来提高准确性。二元预测很容易解读。此外，它使决策过程变得容易。</p><p id="e288" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Huggingface 团队变形金刚库将帮助我们访问预训练的罗伯塔模型。RoBERTa 模型在 NLP 基准，通用语言理解评估(GLUE)上表现非常好。RoBERTa 模型的性能与人类水平的性能相匹配。点击了解更多关于罗伯塔<a class="ae le" href="https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/" rel="noopener ugc nofollow" target="_blank">的信息。点击</a>了解更多关于变形金刚库<a class="ae le" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">的信息。</a></p><p id="6f3d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们依次检查代码的不同部分。</p><h2 id="b9f5" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">第一部分。配置和令牌化</h2><p id="ced8" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">预训练模型具有包含信息片段的配置文件，例如层数和注意力头的数量。RoBERTa 模型配置文件的细节如下所述。</p><pre class="lg lh li lj gt ne nf ng nh aw ni bi"><span id="4a91" class="lv lw it nf b gy nj nk l nl nm">{<br/>    “architectures”: [<br/>    “RobertaForMaskedLM”<br/>    ],<br/>    “attention_probs_dropout_prob”: 0.1,<br/>    “bos_token_id”: 0,<br/>    “eos_token_id”: 2,<br/>    “hidden_act”: “gelu”,<br/>    “hidden_dropout_prob”: 0.1,<br/>    “hidden_size”: 768,<br/>    “initializer_range”: 0.02,<br/>    “intermediate_size”: 3072,<br/>    “layer_norm_eps”: 1e-05,<br/>    “max_position_embeddings”: 514,<br/>    “model_type”: “roberta”,<br/>    “num_attention_heads”: 12,<br/>    “num_hidden_layers”: 12,<br/>    “pad_token_id”: 1,<br/>    “type_vocab_size”: 1,<br/>    “vocab_size”: 50265<br/>}</span></pre><p id="1dbe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">标记化意味着将 python 字符串或句子转换成整数的数组或张量，这是模型词汇表中的索引。每个模型都有自己的标记器。此外，它还有助于为模型准备数据。</p><pre class="lg lh li lj gt ne nf ng nh aw ni bi"><span id="fc77" class="lv lw it nf b gy nj nk l nl nm">from transformers import RobertaTokenizer<br/>roberta_tokenizer = RobertaTokenizer.from_pretrained(“roberta-base”)</span></pre><p id="d22f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">注意:代码的最终版本可以在本文末尾找到。</strong></p><h2 id="9abc" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">第二部分。数据预处理</h2><p id="3380" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在本节中，我们使用分词器对句子或输入数据进行分词。这种模式需要在序列的开头和结尾添加标记，如[SEP]、[CLS]或或<s>。</s></p><pre class="lg lh li lj gt ne nf ng nh aw ni bi"><span id="023f" class="lv lw it nf b gy nj nk l nl nm">def convert_example_to_feature(review):<br/>  return roberta_tokenizer.encode_plus(review,<br/>                                       add_special_tokens=True,<br/>                                       max_length=max_length,<br/>                                       pad_to_max_length=True,<br/>                                       return_attention_mask=True,<br/>  )</span><span id="9b23" class="lv lw it nf b gy nn nk l nl nm">def encode_examples(ds, limit=-1):<br/>     # prepare list, so that we can build up final TensorFlow dataset from slices.<br/>  input_ids_list = []<br/>  attention_mask_list = []<br/>  label_list = []<br/>  if (limit &gt; 0):<br/>    ds = ds.take(limit)<br/>  for review, label in tfds.as_numpy(ds):<br/>    bert_input = convert_example_to_feature(review.decode())<br/>    input_ids_list.append(bert_input[‘input_ids’])<br/>    attention_mask_list.append(bert_input[‘attention_mask’])<br/>    label_list.append([label])<br/>  return tf.data.Dataset.from_tensor_slices((input_ids_list,<br/>                                             attention_mask_list,<br/>                              label_list)).map(map_example_to_dict)</span></pre><p id="3698" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">max_length:这个变量表示句子允许的最大长度。此变量的最大值不应超过 512。</p><p id="4183" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">pad_to_max_length:如果为真，标记器在句尾添加[PAD]。</p><p id="7ce2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">罗伯塔模型需要 3 个输入。</p><p id="3470" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1.input_ids:数据点的序列或索引。</p><p id="541d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.attention_mask:它将原词与特殊记号或填充词区分开来。</p><p id="dc4c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.标签:带标签的数据</p><h2 id="a69d" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">第三部分。模型训练和微调</h2><p id="29f0" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">Transformers 库在一行代码中加载预训练的 RoBERTa 模型。权重被下载并缓存在本地机器上。我们根据 NLP 任务微调这些模型。</p><pre class="lg lh li lj gt ne nf ng nh aw ni bi"><span id="c351" class="lv lw it nf b gy nj nk l nl nm">from transformers import TFRobertaForSequenceClassification</span><span id="afe1" class="lv lw it nf b gy nn nk l nl nm">model = TFRobertaForSequenceClassification.from_pretrained(“roberta-base”)<br/>optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)<br/>loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)<br/>metric = tf.keras.metrics.SparseCategoricalAccuracy(‘accuracy’)<br/>model.compile(optimizer=optimizer, loss=loss, metrics=[metric])<br/>model.fit(ds_train_encoded, <br/>          epochs=number_of_epochs, <br/>          validation_data=ds_test_encoded, <br/>          callbacks=[metrics])</span></pre><p id="fcb0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用以下指针微调模型。</p><p id="95be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1.1e-05 到 1e-06 之间的 learning_rate 变量的值给出了良好的准确度分数。</p><p id="b2bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.批量的增加提高了准确性，也增加了训练时间。</p><p id="ab83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.预训练模型不需要在更多的时期上训练。3 到 10 之间的纪元就可以了。</p><h2 id="72d3" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">第四部分。准确性、F1 得分并保存模型</h2><p id="05b9" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">准确性分数有助于您检测模型中的偏差和差异。模型的改进主要取决于准确性得分。在平衡数据中使用准确度分数，在不平衡数据中使用 F1 分数。F1 分数告诉我们模型是否平等地学习了所有数据。我们将使用 Keras 回调函数来计算模型的 F1 分数。</p><pre class="lg lh li lj gt ne nf ng nh aw ni bi"><span id="8d4a" class="lv lw it nf b gy nj nk l nl nm">class ModelMetrics(tf.keras.callbacks.Callback):</span><span id="33cc" class="lv lw it nf b gy nn nk l nl nm">  def on_train_begin(self, logs={}):<br/>    self.count_n = 1</span><span id="4ffc" class="lv lw it nf b gy nn nk l nl nm">  def on_epoch_end(self, batch, logs={}):<br/>    os.mkdir(‘/create/folder/’ + str(self.count_n))<br/>    self.model.save_pretrained(‘/folder/to/save/model/’ + str(self.count_n))<br/>    y_val_pred = tf.nn.softmax(self.model.predict(ds_test_encoded))<br/>    y_pred_argmax = tf.math.argmax(y_val_pred, axis=1)<br/>    testing_copy = testing_sentences.copy()<br/>    testing_copy[‘predicted’] = y_pred_argmax<br/>    f1_s = f1_score(testing_sentences[‘label’],<br/>                    testing_copy[‘predicted’])<br/>    print(‘\n f1 score is :’, f1_s)<br/>    self.count_n += 1</span><span id="2d28" class="lv lw it nf b gy nn nk l nl nm">metrics = ModelMetrics()</span></pre><p id="894d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用 save_pretrained 方法来保存模型。您可以保存每个历元的模型。我们将保留具有高准确度的模型，并删除其余的。</p><h1 id="497c" class="mt lw it bd lx mu mv mw ma mx my mz md jz na ka mg kc nb kd mj kf nc kg mm nd bi translated">分析 Reddit 子群的情绪</h1><p id="261a" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">一旦你完成了 RoBERTa 模型的构建，我们将检测 Reddit 子群的情绪。这些是你完成任务的步骤。</p><pre class="lg lh li lj gt ne nf ng nh aw ni bi"><span id="e742" class="lv lw it nf b gy nj nk l nl nm">1. Fetch the comment of the Reddit subgroup. Learn more about how to fetch comments from Reddit here.</span><span id="1daf" class="lv lw it nf b gy nn nk l nl nm">2. Check the sentiment of each comment using your RoBERTa model.</span><span id="6061" class="lv lw it nf b gy nn nk l nl nm">3. Count the positive and negative comments of the Reddit subgroup.</span><span id="43f1" class="lv lw it nf b gy nn nk l nl nm">4. Repeat the process for different Reddit subgroup.</span></pre><p id="6ab8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在这里找到第 1 步和第 2 步<a class="ae le" rel="noopener" target="_blank" href="/automate-sentiment-analysis-process-for-reddit-post-textblob-and-vader-8a79c269522f">的详细解释。我选择了我最喜欢的五个子网格进行分析。我们分析排名前 10 的每周帖子的评论。由于 Reddit API 请求的限制，我限制了评论。</a></p><p id="a0bd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正面和负面评论的数量会给你 Reddit 子群的整体情绪。我已经在代码中实现了这些步骤。你可以在本文末尾找到这段代码。</p><p id="fa41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我最喜欢的 5 个 Reddit 子群的情感分析图。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi no"><img src="../Images/5f8a78adf65b78cce40424aa52845ad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lGVXZdnNnGDRMXj-kf2RqA.jpeg"/></div></div></figure><p id="26af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Reddit 子群被他们的版主高度监管。如果你的评论违反了任何子编辑规则，那么 reddit 机器人将删除你的评论。Reddit 机器人不会根据他们的情绪删除评论。但你可以说，大多数负面评论都打破了 subreddit 规则。</p><h1 id="c79a" class="mt lw it bd lx mu mv mw ma mx my mz md jz na ka mg kc nb kd mj kf nc kg mm nd bi translated">结论</h1><p id="e7f3" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在这篇文章中，你可以学习如何发现社交媒体平台 Reddit 的情绪。本文还介绍了情感分析任务的 RoBERTa 模型的构建。在预先训练好的模型的帮助下，我们可以解决很多 NLP 问题。</p><p id="c568" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">NLP 领域中的模型正在变得成熟和强大。Huggingface 变形金刚库使得访问这些模型变得非常容易。用不同的配置和任务尝试这些模型。</p><p id="0767" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">作者其他文章</strong></p><ol class=""><li id="c425" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated"><a class="ae le" href="https://medium.com/analytics-vidhya/first-step-in-eda-descriptive-statistics-analysis-f49ca309da15" rel="noopener">EDA 的第一步:描述性统计分析</a></li><li id="7d1b" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated"><a class="ae le" rel="noopener" target="_blank" href="/analysis-and-visualization-of-unstructured-text-data-2de07d9adc84">路透社文章的文本数据分析和可视化</a></li><li id="a30b" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated"><a class="ae le" rel="noopener" target="_blank" href="/automate-sentiment-analysis-process-for-reddit-post-textblob-and-vader-8a79c269522f">为 Reddit Post: TextBlob 和 VADER 自动化情感分析流程</a></li></ol><p id="5a83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">构建 RoBERTa 分类模型的代码</strong></p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="e292" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">分析 Reddit 子组的代码</strong></p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="od oe l"/></div></figure></div></div>    
</body>
</html>