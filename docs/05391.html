<html>
<head>
<title>Explainable AI and AI interpretability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的人工智能和人工智能的可解释性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-ai-and-ai-interpretability-54a3d2b22052?source=collection_archive---------46-----------------------#2020-05-06">https://towardsdatascience.com/explainable-ai-and-ai-interpretability-54a3d2b22052?source=collection_archive---------46-----------------------#2020-05-06</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><h2 id="c436" class="it iu iv bd b dl iw ix iy iz ja jb dk jc translated" aria-label="kicker paragraph"><a class="ae ep" href="https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2" rel="noopener ugc nofollow" target="_blank">苹果</a> | <a class="ae ep" href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz" rel="noopener ugc nofollow" target="_blank">谷歌</a> | <a class="ae ep" href="https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU" rel="noopener ugc nofollow" target="_blank"> SPOTIFY </a> | <a class="ae ep" href="https://anchor.fm/towardsdatascience" rel="noopener ugc nofollow" target="_blank">其他</a></h2><div class=""/><div class=""><h2 id="0740" class="pw-subtitle-paragraph kb je iv bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">Bahador Khaleghi 在<a class="ae kt" rel="noopener" target="_blank" href="https://towardsdatascience.com/podcast/home"> TDS 播客</a></h2></div><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ku"><img src="../Images/594c8f30aa482a71d8dd3cc01c1eacc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nhoplo_vtLneeJU2zQ7gBQ.png"/></div></div></figure><p id="2223" class="pw-post-body-paragraph lg lh iv li b lj lk kf ll lm ln ki lo lp lq lr ls lt lu lv lw lx ly lz ma mb io bi translated"><em class="mc">编者按:迈向数据科学播客的“攀登数据科学阶梯”系列由 Jeremie Harris 主持。Jeremie 帮助运营一家名为</em><a class="ae kt" href="http://sharpestminds.com" rel="noopener ugc nofollow" target="_blank"><em class="mc">sharpes minds</em></a><em class="mc">的数据科学导师初创公司。可以听下面的播客:</em></p><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="99c4" class="pw-post-body-paragraph lg lh iv li b lj lk kf ll lm ln ki lo lp lq lr ls lt lu lv lw lx ly lz ma mb io bi translated">如果我让你解释为什么你会读这篇博客，你可以用很多不同的方式来回答。</p><p id="d707" class="pw-post-body-paragraph lg lh iv li b lj lk kf ll lm ln ki lo lp lq lr ls lt lu lv lw lx ly lz ma mb io bi translated">例如，你可以告诉我“这是因为我喜欢它”，或者“因为我的神经元以特定的方式激活，导致我点击了广告给我的链接”。或者你可以更进一步，把你的答案和量子物理的基本定律联系起来。</p><p id="a1d4" class="pw-post-body-paragraph lg lh iv li b lj lk kf ll lm ln ki lo lp lq lr ls lt lu lv lw lx ly lz ma mb io bi translated">关键是，为了有效，解释需要针对一定的抽象层次。</p><p id="0055" class="pw-post-body-paragraph lg lh iv li b lj lk kf ll lm ln ki lo lp lq lr ls lt lu lv lw lx ly lz ma mb io bi translated">在生活中确实如此，但在机器学习中也是如此，可解释的人工智能作为一种确保模型正常工作的方式，以对我们有意义的方式，受到了越来越多的关注。理解可解释性以及如何利用它变得越来越重要，这就是为什么我想与 H20.ai 的数据科学家 Bahador Khaleghi 交谈，他的技术重点是机器学习中的可解释性和可解释性。</p><p id="e9d8" class="pw-post-body-paragraph lg lh iv li b lj lk kf ll lm ln ki lo lp lq lr ls lt lu lv lw lx ly lz ma mb io bi translated">我们的谈话涵盖了很多领域，但这里有一些我最喜欢的要点:</p><ul class=""><li id="9711" class="mf mg iv li b lj lk lm ln lp mh lt mi lx mj mb mk ml mm mn bi translated">明确如何分解数据科学生命周期非常重要。尽管在数据探索和预处理之间，或者在特征工程和建模之间并不总是有明确的界限，但是将分析数据的过程分成定义明确的块会使您的工作更加模块化，并使协作更加容易。</li><li id="0727" class="mf mg iv li b lj mo lm mp lp mq lt mr lx ms mb mk ml mm mn bi translated">最近，无论是行业还是监管机构，对可解释人工智能的兴趣都有了巨大的提升。这主要是因为我们使用的模型越来越复杂，因此其行为越来越难以解释。鉴于我们现在依赖机器学习来实现许多任务关键型应用，如无人驾驶汽车、贷款审批甚至医疗程序，可解释性将变得更加重要，作为确保模型不会表现出危险或意外行为的一种方式。</li><li id="2b84" class="mf mg iv li b lj mo lm mp lp mq lt mr lx ms mb mk ml mm mn bi translated">不太可能被输入中的微小变化所迷惑的模型被称为“稳健的”，事实证明，可解释的模型往往也更稳健。在某种程度上，这有点直观:对你的预测有一个连贯的解释意味着你的推理是合理的，而合理的推理不太可能被一点点噪音打乱。</li><li id="3c7e" class="mf mg iv li b lj mo lm mp lp mq lt mr lx ms mb mk ml mm mn bi translated">autoML 的出现使得可解释性变得更加重要，因为 autoML 本质上把整个数据管道变成了一个黑盒(而不仅仅是机器学习模型)。当您使用 autoML 时，您甚至没有明确地决定如何设计或选择您的功能，您将不会对您的模型的决策过程有太多的可见性，这就为问题和病态敞开了大门。</li></ul><p id="9e7e" class="pw-post-body-paragraph lg lh iv li b lj lk kf ll lm ln ki lo lp lq lr ls lt lu lv lw lx ly lz ma mb io bi translated">你可以<a class="ae kt" href="https://medium.com/@bahador.khaleghi" rel="noopener">在 Twitter 上关注巴哈多</a>，在 Medium 上阅读他的一些帖子<a class="ae kt" href="https://medium.com/@bahador.khaleghi/why-enterprise-machine-learning-is-struggling-and-how-automl-can-help-8ac03023bf01" rel="noopener">这里</a>和<a class="ae kt" href="https://medium.com/@bahador.khaleghi/a-critical-overview-of-automl-solutions-cb37ab0eb59e" rel="noopener">这里</a>。</p><p id="6984" class="pw-post-body-paragraph lg lh iv li b lj lk kf ll lm ln ki lo lp lq lr ls lt lu lv lw lx ly lz ma mb io bi translated">你可以在推特上关注我。</p></div><div class="ab cl mt mu hz mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="io ip iq ir is"><h2 id="897f" class="na nb iv bd nc nd ne dn nf ng nh dp ni lp nj nk nl lt nm nn no lx np nq nr jb bi translated">订阅《走向数据科学》的<a class="ae kt" href="https://medium.com/towards-data-science/newsletters/monthly-edition" rel="noopener">月刊</a>，直接在你的邮箱✨中接收我们最好的文章、视频和播客</h2></div></div>    
</body>
</html>