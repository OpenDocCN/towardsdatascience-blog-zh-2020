<html>
<head>
<title>Day 108 of #NLP365: NLP Papers Summary — Simple BERT Models for Relation Extraction and Semantic Role Labelling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365 的第 108 天:NLP 论文摘要——用于关系提取和语义角色标注的简单 BERT 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7?source=collection_archive---------31-----------------------#2020-04-17">https://towardsdatascience.com/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7?source=collection_archive---------31-----------------------#2020-04-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/61a393f717c1685f581a48076c22cd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDhaS8ed285Bb9L80C5U0g.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jg" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="8b40" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="9ba9" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">NLP 论文摘要是我总结 NLP 研究论文要点的系列文章</h2></div><p id="daf1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">项目#NLP365 (+1)是我在 2020 年每天记录我的 NLP 学习旅程的地方。在这里，你可以随意查看我在过去的 100 天里学到了什么。</p><p id="0f81" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">今天的 NLP 论文是<strong class="lj jt"> <em class="md">用于关系抽取和语义角色标注的简单 BERT 模型</em> </strong>。以下是研究论文的要点。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="8d7a" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">目标和贡献</h1><p id="5ff6" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">使用基于 BERT 的模型来实现关系提取和语义角色标记的 SOTA 性能。该论文声称是第一个成功地将 BERT 应用于关系抽取和语义角色标注的论文。</p><h2 id="d080" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">什么是语义角色标注？</h2><p id="ce98" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">对于语义角色标注，目标是提取句子的述元结构。这意味着我们要找出是什么事件，什么时候发生的，谁参与了，在哪里发生的等等。这对于逻辑推理很重要的下游 NLP 任务很有用。</p><h1 id="a628" class="ml mm jj bd mn mo nt mq mr ms nu mu mv ky nv kz mx lb nw lc mz le nx lf nb nc bi translated">用于关系抽取的 BERT</h1><p id="eb79" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">给定一个句子和两个实体跨度(非重叠)，我们的目标是预测两个实体之间的关系。下图是 BERT 架构的示意图。</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/374b1ade060796e81fae321dd2f17ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/0*nGyfnKDEP2OQbr_t.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">用于关系抽取的 BERT 体系结构[1]</p></figure><p id="3795" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">流程如下:</p><ol class=""><li id="cde0" class="od oe jj lj b lk ll ln lo lq of lu og ly oh mc oi oj ok ol bi translated">向输入句子添加特殊标记([CLS]和[SEP])，并用屏蔽标记屏蔽实体提及，以防止过度拟合。有两种论元类型(主语和宾语)和两种实体类型(位置和人)。例如，[S-PER]表示主体实体是人。请参见上图中带有特殊标记的输入句子示例。</li><li id="09bb" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">使用单词块标记器进行标记化，并输入 BERT 编码器以获得上下文化的表示</li><li id="1b60" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">删除第一个[SEP]标记后的任何序列</li><li id="d798" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">计算相对于主体实体和客体实体的位置序列</li><li id="6e93" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">将两个位置序列转换为位置嵌入，并将其连接到上下文化的表示中</li><li id="7f24" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">把它放进一个单层的 BiLSTM</li><li id="f21f" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">两个方向上的最终隐藏状态被送入一个多层(一个隐藏层)感知器</li></ol><h2 id="c5f4" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">结果</h2><p id="caf4" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">用于关系提取的评估数据集是 TAC 关系提取数据集(TACRED)。结果如下所示。BERT-base 模型能够胜过所有其他现有的独立模型，然而，当与集合模型比较时，它仍然下降。</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/97a519e87a104fe9df8306c1a3898e7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/0*fAYb3bZjEMo4q2Rg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">关于关系抽取任务的 BERT 结果[1]</p></figure><h1 id="b8a6" class="ml mm jj bd mn mo nt mq mr ms nu mu mv ky nv kz mx lb nw lc mz le nx lf nb nc bi translated">语义角色标注的 BERT</h1><p id="1b0b" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">语义角色标记由 4 个子任务组成:</p><ol class=""><li id="3648" class="od oe jj lj b lk ll ln lo lq of lu og ly oh mc oi oj ok ol bi translated">谓词检测</li><li id="79cd" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">谓词意义歧义消除</li><li id="b948" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">变元识别</li><li id="5af2" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">论点分类</li></ol><p id="b3d5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">可以使用基于跨度和/或基于依赖来完成参数注释。本文将这两种标注方法统一起来。</p><h2 id="546a" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">谓词意义歧义消除</h2><p id="53a9" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">这里的目标是正确标记上下文中谓词的含义。步骤如下:</p><ol class=""><li id="31c7" class="od oe jj lj b lk ll ln lo lq of lu og ly oh mc oi oj ok ol bi translated">使用单词块标记器的标记化</li><li id="446c" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">谓词(或标记)用适当的标签进行标记</li><li id="ed6f" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">其余的单词用“O”或“X”标记。“o”代表任何单词的第一个标记，而“X”代表单词的剩余部分</li><li id="aaa8" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">将序列馈入 BERT 编码器以获得上下文化的表示</li><li id="ba07" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">将谓词指示符嵌入连接到上下文化嵌入，以让模型知道哪些标记是谓词标记</li><li id="c103" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">将级联的嵌入和标签集馈送到 MLP 中用于最终预测</li></ol><h2 id="862a" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">论点识别和分类</h2><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/bf82d55a8367db3d9082e9ce2300d403.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/0*n_FcqBqkco3_yeBe.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">语义角色标注的 BERT 架构[1]</p></figure><p id="bdc4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里的目标是识别论元跨度或句法中心，并将它们映射到正确的语义角色标签。我们的输入是一个句子-谓词对，我们需要预测一个序列，其中标签集在生物标签方案和谓词参数之间重叠。模型如上图所示。</p><ol class=""><li id="ddd1" class="od oe jj lj b lk ll ln lo lq of lu og ly oh mc oi oj ok ol bi translated">将句子格式输入到[CLS]句子[SEP]谓词[SEP]中，以便谓词可以通过注意机制与整个句子进行交互</li><li id="013d" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">将输入馈入 BERT 编码器</li><li id="9ac7" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">将输出与谓词指示器嵌入连接起来</li><li id="f7a9" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">将连接的输出送入一层 BiLSTM</li><li id="7d2b" class="od oe jj lj b lk om ln on lq oo lu op ly oq mc oi oj ok ol bi translated">每个标记隐藏状态与谓词隐藏状态连接在一起，并被送入 MLP 进行最终预测</li></ol><h2 id="4409" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">结果</h2><p id="63b4" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">对于基于跨度的语义角色标注，评估数据集是 CoNLL 2005 和 2012。对于基于依赖的语义角色标记，评估数据集是 CoNLL 2009。</p><p id="dcd8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">结果如下所示。在表 2 中，我们有谓词意义消歧的准确性结果。以前的 SOTA 是一个具有语言学特征的 BiLSTM，基于 BERT 的模型能够胜过使用 MLP 模型的模型。表 3 展示了排除谓词意义歧义的论证识别和分类的结果。在比较中，基于 BERT 的模型再次胜过所有模型。</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/a47a1aa008384362130e03f33546e78a.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/0*98vYqrG9Q1cbW0qK.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">语义角色标注的 BERT 结果[1]</p></figure><p id="98e0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">表 4 和表 5 显示了基于依赖性和基于跨度的语义角色标记的端到端结果。我们可以在表 4 中看到，伯特-LSTM-大型模型能够在 F1 分数方面胜过之前的 SOTA，并且击败没有任何语言特征的不同集成模型。对于基于跨度的语义角色标注，伯特-LSTM-大型模型优于包括集成模型在内的所有模型，但在 CoNLL 2012 数据集上，其 F1 分数略低于集成模型。</p><figure class="nz oa ob oc gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/510d064db6c1f3159c56f69ffdf47bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/0*FBGyqwUoJXCW6-LJ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">BERT 与基于依赖和基于跨度的 SRL 的结果比较[1]</p></figure><h1 id="c317" class="ml mm jj bd mn mo nt mq mr ms nu mu mv ky nv kz mx lb nw lc mz le nx lf nb nc bi translated">结论和未来工作</h1><p id="0bbb" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">基于 BERT 的简单模型产生的结果为未来的研究提供了强有力的基线。潜在的未来工作可能涉及重新引入句法特征，以评估它是否进一步改善目前的结果。额外的工作可以是多任务学习，看看我们是否能从同时学习关系提取和语义角色标记中受益。</p><h2 id="2ebe" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">来源:</h2><p id="80eb" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">[1]石，p .和林，j . 2019 .用于关系抽取和语义角色标注的简单 BERT 模型。<em class="md"> arXiv 预印本 arXiv:1904.05255 </em>。网址:【https://arxiv.org/pdf/1904.05255.pdf T2】</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="68f1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【https://ryanong.co.uk】原载于 2020 年 4 月 17 日<a class="ae jg" href="https://ryanong.co.uk/2020/04/17/day-108-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-role-labelling/" rel="noopener ugc nofollow" target="_blank"><em class="md"/></a><em class="md">。</em></p></div></div>    
</body>
</html>