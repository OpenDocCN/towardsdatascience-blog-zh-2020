<html>
<head>
<title>Quick Start to Multi-GPU Deep Learning on AWS Sagemaker using TF.Distribute</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TF在AWS Sagemaker上快速入门多GPU深度学习。分配</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quick-start-to-multi-gpu-deep-learning-on-aws-sagemaker-using-tf-distribute-9ee08bc9612b?source=collection_archive---------33-----------------------#2020-05-30">https://towardsdatascience.com/quick-start-to-multi-gpu-deep-learning-on-aws-sagemaker-using-tf-distribute-9ee08bc9612b?source=collection_archive---------33-----------------------#2020-05-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b81b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">向霍洛沃德说再见，向TF问好。分配</h2></div><h1 id="1889" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="db32" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">本文是使用AWS Sagemaker和TensorFlow 2.2.0 tf.distribute运行分布式多GPU深度学习的快速入门指南。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/eba57569ffb904ce7f91b1af2c59ab3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x1bTh0u8_db7BJuaZJTazA.jpeg"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">马库斯·斯皮斯克在<a class="ae mj" href="https://unsplash.com/s/photos/distribute?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="28f0" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">密码</h1><p id="e302" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我所有与本文相关的代码都可以在我的GitHub资源库中找到，这里是<a class="ae mj" href="https://github.com/yeamusic21/DistilBert-TF2-Keras-Multi-GPU-Sagemaker-Training" rel="noopener ugc nofollow" target="_blank"/>。我的存储库中的代码是在Kaggle的数据上运行BERT版本的一个例子，具体来说就是<a class="ae mj" href="https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification" rel="noopener ugc nofollow" target="_blank"> Jigsaw多语言有毒评论分类</a>竞赛。<em class="mk">(我的很多代码都是采用了很棒的</em> <a class="ae mj" href="https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta" rel="noopener ugc nofollow" target="_blank"> <em class="mk">顶级公共内核</em> </a> <em class="mk">。)</em></p><h1 id="9e19" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">了解信息的需要</h1><h2 id="0f82" class="ml kg iq bd kh mm mn dn kl mo mp dp kp lg mq mr kr lk ms mt kt lo mu mv kv mw bi translated">入门指南</h2><p id="ceda" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">首先，我们需要了解我们在AWS Sagemaker上运行深度学习的选项。</p><ol class=""><li id="0539" class="mx my iq kz b la mz ld na lg nb lk nc lo nd ls ne nf ng nh bi translated">在笔记本实例中运行您的代码</li><li id="83c1" class="mx my iq kz b la ni ld nj lg nk lk nl lo nm ls ne nf ng nh bi translated">在定制的Sagemaker TensorFlow容器中运行您的代码</li></ol><p id="4368" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">在本文中，我们将重点放在选项#2上，因为它更便宜，并且是Sagemaker的预期设计。</p><p id="f0b3" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated"><em class="mk">(选项#1是一个很好的开始方式，但是它更贵，因为你要为笔记本实例运行的每一秒付费)。</em></p><h2 id="9616" class="ml kg iq bd kh mm mn dn kl mo mp dp kp lg mq mr kr lk ms mt kt lo mu mv kv mw bi translated">运行Sagemaker TensorFlow容器</h2><p id="a723" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Sagemaker TensorFlow容器有很大的灵活性，但我们将重点放在最基本的东西上。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nq"><img src="../Images/70cfb8b9aa1826ee98b0a30ab673da1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KQ6laoRaKI_rhGswxWD3Yw.jpeg"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">照片由<a class="ae mj" href="https://unsplash.com/@thefallofmath?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">乌帕德克·马特米</a>在<a class="ae mj" href="https://unsplash.com/s/photos/container?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="8089" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">首先，我们需要启动一个Sagemaker笔记本实例，并将我们的数据存储在S3上。如果你不知道如何做到这一点，我在我的博客上回顾了一些简单的选择。一旦我们在S3有了数据，我们就可以启动一个Jupyter笔记本<em class="mk">(从我们的笔记本实例)</em>并开始编码。该笔记本将负责启动您的培训工作，即您的Sagemaker TensorFlow容器。</p><p id="a50f" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">同样，我们将把重点放在最基本的东西上。我们需要一个变量来指示数据的位置，然后我们需要将该位置添加到字典中。</p><pre class="lu lv lw lx gt nr ns nt nu aw nv bi"><span id="3e01" class="ml kg iq ns b gy nw nx l ny nz">data_s3 = 's3://&lt;your-bucket&gt;/'<br/>inputs = {'data':data_s3}</span></pre><p id="f3a7" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">很简单。现在我们需要创建一个Sagemaker TensorFlow容器对象。</p><p id="3db3" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">我们的<strong class="kz ir">入口点</strong>是一个Python脚本<em class="mk">(我们稍后会制作)</em>，它包含了我们所有的建模代码。我们还希望<strong class="kz ir"> script_mode </strong> =True，因为我们正在运行自己的训练脚本。</p><p id="f0ab" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">我们的<strong class="kz ir"> train_instance_type </strong>是一个多GPU Sagemaker实例类型。您可以在这里找到Sagemaker实例类型的完整列表<a class="ae mj" href="https://aws.amazon.com/sagemaker/pricing/instance-types/" rel="noopener ugc nofollow" target="_blank">。注意一个ml.p3.8xlarge运行4个</a><a class="ae mj" href="https://www.nvidia.com/en-us/data-center/v100/" rel="noopener ugc nofollow" target="_blank">V100 NVIDIA GPU</a>。由于我们将使用MirroredStrategy <em class="mk">(稍后将详细介绍)</em>我们需要<em class="mk"/>train _ instance _ count = 1。也就是说，1台机器配有4台V100s。</p><p id="a5a2" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">我们还需要将<strong class="kz ir">输出路径</strong>设置到S3的一个位置。这是Sagemaker将我们保存到路径“/opt/ml/model”中的所有内容自动存储到的地方。例如，如果我们将最终的模型保存到训练脚本中的容器路径“/opt/ml/model”中，那么当训练工作完成时，Sagemaker会将模型加载到我们的S3位置。</p><p id="3d81" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">其他的设置你可以暂时不管，或者根据需要进一步研究。综上所述，我们需要弄对的主要设置有<strong class="kz ir">入口_点、脚本_模式、训练_实例_类型、</strong>和<strong class="kz ir">输出_路径</strong>。<em class="mk">(然后对于MirroredStrategy我们需要train_instance_count=1)。</em></p><pre class="lu lv lw lx gt nr ns nt nu aw nv bi"><span id="69e0" class="ml kg iq ns b gy nw nx l ny nz"># create estimator<br/>estimator = TensorFlow(entry_point='jigsaw_DistilBert_SingleRun_v1_sm_tfdist0.py',<br/>                       train_instance_type='ml.p3.8xlarge',<br/>                       output_path="s3://&lt;your-bucket&gt;",<br/>                       train_instance_count=1,<br/>                       role=sagemaker.get_execution_role(),<br/>                       framework_version='2.1.0',<br/>                       py_version='py3',<br/>                       script_mode=True)</span></pre><p id="a11a" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">我们可以通过运行下面的代码来开始我们的培训工作。</p><pre class="lu lv lw lx gt nr ns nt nu aw nv bi"><span id="e55c" class="ml kg iq ns b gy nw nx l ny nz">estimator.fit(inputs)</span></pre><p id="9e4f" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">注意，我们包含了我们的字典<em class="mk">(包含我们在S3的位置)</em>作为‘fit()’的输入。在运行这段代码之前，我们需要创建Python脚本，并将其分配给<strong class="kz ir"> entry_point </strong> <em class="mk">(否则我们的容器将没有任何代码可以运行:-P) </em> <strong class="kz ir">。</strong></p><p id="e543" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated"><strong class="kz ir">创建培训脚本</strong></p><p id="1141" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">我在GitHub上的训练脚本非常繁忙，因为我正在对Kaggle的一些数据运行BERT版本。我们唯一需要的是访问我们的数据，然后我们可以将任何结果保存到容器路径“/opt/ml/model”。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oa"><img src="../Images/dae7c855a8ee0f1b7f05a87cd14eb6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RsGiSRY_tvczq2URzcRi6w.jpeg"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">在<a class="ae mj" href="https://unsplash.com/s/photos/script?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae mj" href="https://unsplash.com/@baleibee?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Brooks Leibee </a>拍摄的照片</p></figure><p id="40f6" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">获取数据的最简单方法是在培训脚本中硬编码您在S3的位置。</p><p id="29cb" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">我们还可以从“estimator.fit(inputs)”传递的值中获取S3位置。我们可以使用<a class="ae mj" href="https://pypi.org/project/argparse/" rel="noopener ugc nofollow" target="_blank"> argparse </a>来做到这一点。</p><pre class="lu lv lw lx gt nr ns nt nu aw nv bi"><span id="943c" class="ml kg iq ns b gy nw nx l ny nz">def parse_args(): <br/>    parser = argparse.ArgumentParser()<br/>    parser.add_argument(‘ — data’, <br/>                        type=str,   <br/>                        default=os.environ.get(‘SM_CHANNEL_DATA’)) <br/>    return parser.parse_known_args()</span><span id="7b95" class="ml kg iq ns b gy ob nx l ny nz">args, _ = parse_args()</span></pre><p id="ac63" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">如果我们想做的只是在Sagemaker容器中运行我们的培训工作，那基本上就是我们所需要的！现在，如果我们想使用tf.distribute运行多GPU训练，我们还需要一些东西。</p><h2 id="f1b4" class="ml kg iq bd kh mm mn dn kl mo mp dp kp lg mq mr kr lk ms mt kt lo mu mv kv mw bi translated">和霍洛佛德说再见，和TF打招呼。分配</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oc"><img src="../Images/81d754f8558135d11b98564c4745513f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t1oTTHszusQnBj42Zdk63Q.jpeg"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">照片由<a class="ae mj" href="https://unsplash.com/@tvick?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">泰勒维克</a>在<a class="ae mj" href="https://unsplash.com/s/photos/servers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="fb47" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">首先，我们需要表明我们想要运行多GPU训练。我们可以用下面的代码很容易地做到这一点。</p><pre class="lu lv lw lx gt nr ns nt nu aw nv bi"><span id="697f" class="ml kg iq ns b gy nw nx l ny nz">strategy = tf.distribute.MirroredStrategy()</span></pre><p id="dc4f" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">有关MirroredStrategy的更多信息，我会查看<a class="ae mj" href="https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy" rel="noopener ugc nofollow" target="_blank"> TensorFlow文档</a>。现在我们将在整个训练代码中使用我们的<strong class="kz ir">策略</strong>对象。接下来，我们需要通过包含以下行来调整多GPU训练的批量大小。</p><pre class="lu lv lw lx gt nr ns nt nu aw nv bi"><span id="027a" class="ml kg iq ns b gy nw nx l ny nz">BATCH_SIZE = 16 * strategy.num_replicas_in_sync</span></pre><p id="a27b" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">我们将在后面的“model.fit()”调用中使用变量BATCH_SIZE。</p><p id="ab51" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">最后，在定义模型时，我们需要再次使用<strong class="kz ir">策略</strong>。</p><pre class="lu lv lw lx gt nr ns nt nu aw nv bi"><span id="a43b" class="ml kg iq ns b gy nw nx l ny nz">with strategy.scope():<br/>    # define model here</span></pre><p id="51ca" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">就是这样！然后，我们可以继续运行我们通常使用的“model.fit()”。</p><p id="5787" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">同样，与本文相关的完整代码可以在我的GitHub存储库中找到，这里是<a class="ae mj" href="https://github.com/yeamusic21/DistilBert-TF2-Keras-Multi-GPU-Sagemaker-Training" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="610b" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nn li lj lk no lm ln lo np lq lr ls ij bi translated">感谢您的阅读，并希望您发现这是有帮助的！</p></div></div>    
</body>
</html>