<html>
<head>
<title>PMF for Recommender Systems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推荐系统的PMF</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pmf-for-recommender-systems-cbaf20f102f0?source=collection_archive---------15-----------------------#2020-05-07">https://towardsdatascience.com/pmf-for-recommender-systems-cbaf20f102f0?source=collection_archive---------15-----------------------#2020-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7778" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">概率矩阵分解和协同过滤</h2></div><p id="0c71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自动推荐系统通常用于基于关于偏好的现有数据向用户提供他们感兴趣的产品建议。文献描述了不同类型的推荐系统。但是，我们将强调两个主要类别，然后进一步扩展第二个类别:</p><p id="c6c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">基于内容的过滤:</strong>这些利用用户偏好来进行新的预测。当用户提供关于他/她的偏好的明确信息时，这被系统记录并用于自动提出建议。我们日常常用的许多网站和社交媒体都属于这一类。</p><p id="52ae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">协同过滤:</strong>当用户提供的信息不足以进行商品推荐时会发生什么？在这种情况下，我们可以使用其他有类似偏好的用户提供的数据。这类方法利用一组用户过去选择的历史来引出推荐。</p><p id="b322" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在第一种情况下，期望给定的用户建立一个明确陈述偏好的简档，而在第二种情况下，该信息可能不完全可用，但是我们期望我们的系统仍然能够基于相似用户提供的证据来做出推荐。一种被称为<em class="le">概率矩阵分解</em>，或者简称为<em class="le"> PMF </em>的方法，通常用于协同过滤，并且将是我们在本文剩余部分讨论的主题。现在让我们深入研究这个算法的细节以及它的直觉。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h2 id="8f62" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">概率矩阵分解解释</h2><p id="0521" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">假设我们有一组用户<em class="le"> u </em> 1，<em class="le"> u </em> 2，<em class="le"> u </em> 3 … <em class="le"> u </em> N，他们对一组项目<em class="le"> v </em> 1，<em class="le"> v </em> 2，<em class="le"> v </em> 3 … <em class="le"> v </em> M进行评级。然后我们可以将评级构造为矩阵<strong class="kk iu"> <em class="le"> R </em> </strong></p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mk"><img src="../Images/1d84ca0c60b0d47eb8ac6655e20fa01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qNO-Su8dXe1RGTW82Z5M1w.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">评级映射。它可以被认为是一个矩阵，其中每个用户(行)对许多项目(列)进行评级</p></figure><p id="dda5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="le"> R </em> </strong>矩阵的一个重要特点是<em class="le">稀疏</em>。也就是说，只有一些单元格具有非空的等级值，而其他单元格则没有。对于给定的用户<em class="le"> A </em>，系统应该能够根据他/她的偏好以及类似用户的选择提供商品推荐。然而，对于用户<em class="le"> A </em>来说，没有必要明确地对要推荐的特定项目进行评级。具有相似偏好的其他用户将弥补关于用户<em class="le"> A </em>的缺失数据。这就是为什么<em class="le">概率矩阵分解</em>属于<em class="le">协同过滤</em>推荐系统的范畴。</p><p id="64a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们考虑一下电影推荐系统。想象一下，如果我们被要求观看并评价在特定季节放映的每一部电影，情况会是怎样。那将是非常不切实际的，不是吗？我们只是缺少这样做的时间。</p><p id="b4a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">鉴于并非所有用户都能够对所有可用商品进行评级，我们必须找到一种方法来填补<strong class="kk iu"> <em class="le"> R </em> </strong>矩阵中的信息空白，并且仍然能够提供相关的建议。PMF通过利用类似用户提供的评级来解决这个问题。从技术上来说，它利用了贝叶斯学习的一些原则，这些原则也适用于我们拥有稀缺或不完整数据的其他场景。</p><p id="2502" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以使用两个低秩矩阵<strong class="kk iu"><em class="le"/></strong>和<strong class="kk iu"> <em class="le"> V </em> </strong>来估计<strong class="kk iu"> <em class="le"> R </em> </strong>矩阵，如下所示:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi na"><img src="../Images/ee8bb2ee85d18c7d9bf88fcece0137b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*_sxo66jA-6g5ZEuVxMprCQ.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">R矩阵的分量</p></figure><p id="9a0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<strong class="kk iu">T33U</strong>T是一个<em class="le"> N </em> x <em class="le"> D </em>矩阵，其中<em class="le"> N </em>是注册用户数，<em class="le"> D </em>是排名。<strong class="kk iu"> <em class="le"> V </em> </strong>是一个<em class="le"> D </em> x <em class="le"> M </em>矩阵，其中<em class="le"> M </em>是要评定的项目数。因此，<em class="le"> N </em> x <em class="le"> M </em>额定值矩阵<strong class="kk iu"> <em class="le"> R </em> </strong>可通过以下方式近似计算:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/58e5fb1405b0ad517bff9a1a9648374a.png" data-original-src="https://miro.medium.com/v2/resize:fit:228/format:webp/1*74xwu_tfOEffakAZc5PmYw.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">等式1: R表达式</p></figure><p id="a4e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们从现在开始的工作是找到<strong class="kk iu"><em class="le"/></strong>T和<strong class="kk iu"> <em class="le"> V </em> </strong>，它们将依次成为我们模型的参数。因为<strong class="kk iu"> <em class="le"> U </em> </strong>和<strong class="kk iu"> <em class="le"> V </em> </strong>是低秩矩阵，所以PMF又称为<em class="le">低秩矩阵因式分解问题</em>。此外，<strong class="kk iu"><em class="le"/></strong>和<strong class="kk iu"> <em class="le"> V </em> </strong>矩阵的这一特殊特性使得PMF即使对于包含数百万条记录的数据集也是可伸缩的。</p><p id="8a88" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PMF从贝叶斯学习中获得参数估计的直觉。一般来说，我们可以说，在贝叶斯推断中，我们的目标是通过诉诸贝叶斯规则来找到模型参数的后验分布:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/9eea34483b4ac5708abab97b03ba8957.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*Xdr80jh82N_Evz-it_Z2iw.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">等式2:参数推断的贝叶斯规则</p></figure><p id="450f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<strong class="kk iu"> X </strong>是我们的数据集，θ是分布的参数或参数集。α是分布的超参数。p(θ| <strong class="kk iu"> X </strong>，α)是<em class="le">后验分布</em>，也称为<em class="le">后验</em>。p( <strong class="kk iu"> X </strong> |θ，α)为<em class="le">似然</em>，p(θ|α)为<em class="le">在先</em>。训练过程的整体思想是，随着我们获得更多关于数据分布的信息，我们将调整模型参数θ以适应数据。从技术上讲，后验分布的参数将被插入到先验分布中，用于训练过程的下一次迭代。也就是说，给定训练步骤的后验分布将最终成为下一步骤的先验。将重复该过程，直到各步之间的后验分布p(θ| <strong class="kk iu"> X </strong>，α)变化很小。</p><p id="16a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们回到我们对PMF的直觉。如前所述，我们的模型参数将是<strong class="kk iu"><em class="le"/></strong>和<strong class="kk iu"> <em class="le"> V </em> </strong> <em class="le">，</em>而<strong class="kk iu"> <em class="le"> R </em> </strong>将是我们的数据集。一旦经过训练，我们将最终得到一个经过修改的<strong class="kk iu"> <em class="le"> R* </em> </strong>矩阵，该矩阵还将包含对最初在<strong class="kk iu"> <em class="le"> R </em> </strong>中为空的用户项目单元格的评级。我们将使用修改后的评分矩阵进行预测。考虑到这些因素，我们将:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/2713706922228d37ea333bc6376a3438.png" data-original-src="https://miro.medium.com/v2/resize:fit:226/format:webp/1*hsMiF1QHIjuuYd7wi-_W0A.png"/></div></figure><p id="c3b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中，σ是零均值球形高斯分布的标准偏差。然后，通过替换等式2中的这些表达式，我们将得到:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/9510a5b5ef1d684cd81121639a59ef73.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*UQyAqD_6gqXSWghqLYTaqA.png"/></div></figure><p id="6829" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然<strong class="kk iu"> <em class="le"> U </em> </strong>和<strong class="kk iu"> <em class="le"> V </em> </strong>矩阵是相互独立的(用户和项目独立发生)，那么这个表达式也可以这样写:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/e786f37e51bb718782b2d482eb64b7aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*JiqSH1p0LJ8cGYOmjUHvtA.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">等式3:PMF的后验分布</p></figure><p id="2617" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在是时候找出这个等式的每一个组成部分等同于什么了。首先，似然函数由下式给出:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/01a6199c5bee53574ea64e8da02232a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*Nbtq8N76OX0mJ58QFGeEnw.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">等式4:观察评级的分布[1]</p></figure><p id="ad44" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<em class="le"> I </em> { <em class="le"> ij </em> }是一个指示器，当第<em class="le"> i </em>行和第<em class="le"> j </em>列的等级存在时，其值为1，否则为零。正如我们所看到的，该分布是具有以下参数的球形高斯分布:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/e2779432adc8d1ec6acb3e85edf0061c.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*B-UenwGHrQtjCU30OtD_5A.png"/></div></figure><p id="c714" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">反过来，<strong class="kk iu"><em class="le"/></strong>和<strong class="kk iu"> <em class="le"> V </em> </strong>的先验分布由下式给出:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/bbc7cef7e3e126f946d512047ea81d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*Imv-Hf4IwrrZTFMXagZDaA.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">等式5和6:U和v的先验分布。</p></figure><p id="4aae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是两个零均值球面高斯函数。然后，将4、5和6替换为3，我们将得到:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nj"><img src="../Images/ad91270b8b7909d99a40ee16f5b74ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRm1bK6dukOQoF2dUMbzww.png"/></div></div></figure><p id="be5d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了训练我们的模型，我们将通过使参数<strong class="kk iu"><em class="le"/></strong>和<strong class="kk iu"> <em class="le"> V </em> </strong> <em class="le"> </em>的导数等于零来寻求最大化该函数。然而，这样做将非常困难，因为高斯函数中有<em class="le"> exp </em>函数。为了解决这个问题，我们应该将对数应用到前面等式的两边，然后应用所需的导数。因此，通过将对数应用于前面等式的两边，我们将得到:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nk"><img src="../Images/bfba949b0e23ed2301f98283be505f6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5de7_laPTZ-jW236tu9F-Q.png"/></div></div></figure><p id="6b97" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个更容易区分的表达式。我们还知道，根据定义，高斯PDF由下式给出:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/7106a5fa33a5a9c93a012ad69ebea61e.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*wJLVwNKlI8j4ARm80Fteig.png"/></div></figure><p id="54f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，对数后验概率的表达式如下所示(注意:为了简单起见，我们去掉了常数):</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nm"><img src="../Images/f28b1cb342c9cb4f52d8a074102d2926.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lVNL3hIBwuaUevBeMDZFHg.png"/></div></div></figure><p id="d9b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，后缀的<em class="le">表示Frobenius范数，由下式给出:</em></p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/03c73b6a2277f2fe2ddec6a42f1bbe11.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*sq41UbWH08HuirbMZ_opgw.png"/></div></figure><p id="61f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，通过引入一些附加符号来识别模型的超参数，我们将得到:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi no"><img src="../Images/7b81e5fad5c27dbec91b084fa060b5dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4nfInVOr2RihWEzKGL0mSg.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">等式7:PMF的对数后验概率</p></figure><p id="7650" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi np"><img src="../Images/462e7a467c0c00d79ca2b6078eeda0a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*mXkWnpiQOIo_CEqLyf714g.png"/></div></figure><p id="eb22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，通过对方程7的参数求导，并使导数等于零，我们将得到:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/482ef3bd423871b7bf7d65dcdb74c3e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*VyGy6DMWd-Q4tSAbLmkjCg.png"/></div></figure><p id="b5d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这里我们可以推导出更新<strong class="kk iu"> <em class="le"> U </em> </strong> <em class="le"> i </em>和<strong class="kk iu"> <em class="le"> V </em> </strong> <em class="le"> j </em>的表达式:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/aad73c75d385b288eebb71fc5f760b96.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*LRjmDiRhRl76Mx1zPyaGjw.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">等式8和9:更新U和V的表达式</p></figure><p id="4353" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设<em class="le"> λU </em>和<em class="le"> λV </em>都不为零，则所涉及的逆矩阵保证存在。作为训练过程的一部分，我们将迭代更新<strong class="kk iu"><em class="le"/></strong><em class="le">I</em>和<strong class="kk iu"> <em class="le"> V </em> </strong> <em class="le"> j </em>。一旦我们找到这些的最佳值，我们将能够通过使用等式7获得log-MAP(最大后验概率)的值。正如我们将在Python实现中看到的，我们可以使用这个值来监控训练收敛。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h2 id="f01c" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">用Python实现</h2><p id="0466" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated"><em class="le">注意:实现的完整源代码可在https://bit.ly/35Cr5kl</em><a class="ae ns" href="https://bit.ly/35Cr5kl" rel="noopener ugc nofollow" target="_blank"/>获得</p><p id="44e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">出于训练的目的，我们利用了IMDB电影数据库的子集，然后将它分成两部分，分别用于训练和验证。</p><p id="a956" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">初始化:</strong>为了初始化<strong class="kk iu"> <em class="le"> V </em> </strong>，我们从一个标准差为1/ <em class="le"> λV的零均值高斯中抽取随机数</em>此外，秩值<em class="le"> D </em>被设置为相对较小的值10。</p><figure class="ml mm mn mo gt mp"><div class="bz fp l di"><div class="nt nu l"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">初始化代码</p></figure><p id="4d92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">更新参数:</strong>为了更新<strong class="kk iu"><em class="le"/></strong>和<strong class="kk iu"> <em class="le"> V </em> </strong>，我们使用等式8和9:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/aad73c75d385b288eebb71fc5f760b96.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*LRjmDiRhRl76Mx1zPyaGjw.png"/></div></figure><p id="cc2d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对应的Python代码如下:</p><figure class="ml mm mn mo gt mp"><div class="bz fp l di"><div class="nt nu l"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">更新参数</p></figure><p id="d10e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">计算对数后验概率:</strong>对数后验概率由等式7给出:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi no"><img src="../Images/7b81e5fad5c27dbec91b084fa060b5dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4nfInVOr2RihWEzKGL0mSg.png"/></div></div></figure><p id="bc68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用以下Python代码:</p><figure class="ml mm mn mo gt mp"><div class="bz fp l di"><div class="nt nu l"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">对数后验计算</p></figure><p id="4e40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">训练循环:</strong>为了训练模型，我们调用先前的函数，并监控在训练和测试集上评估的对数后验以及RMSE(均方根误差):</p><figure class="ml mm mn mo gt mp"><div class="bz fp l di"><div class="nt nu l"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">训练循环</p></figure><p id="72aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们运行训练循环150次迭代，结果如下:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nv"><img src="../Images/a9485242c7ac36f8ead0410cf1fb91ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZMxg1WOkfdt6vhIFKrcADg.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">训练图</p></figure><p id="267c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在左边，我们可以看到当我们训练模型时对数后验概率是如何演变的。在右侧，我们可以看到在训练集和测试集上评估的RMSE值。考虑到<strong class="kk iu"> <em class="le"> R </em> </strong>预测值可能超出0–5的评级范围，我们使用线性插值来确保<strong class="kk iu"> <em class="le"> R </em> </strong>值在此区间内。原始论文[1]提出了其他方法，如使用逻辑函数和线性插值。对于训练，还建议使用带动量的梯度下降来处理较大的数据集。</p><p id="7fc8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，以下是数据库中用户id为45的一些电影推荐:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nw"><img src="../Images/d113b83959eb0e24c0be6658a7bb3b8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkoynWtV49LT6z0vvox50A.png"/></div></div></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h2 id="79ec" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">结论</h2><p id="84ef" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">PMF是一个强大的协同过滤算法。它利用具有相似偏好的用户提供的数据向特定用户提供推荐。它也称为低秩矩阵分解方法，因为它使用低秩矩阵来估计评级<strong class="kk iu"> <em class="le"> R </em> </strong>矩阵，然后进行有用的预测。</p><p id="5b0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这篇文章！如果你有任何问题，请随时来找我。我会继续写更多这样的材料。敬请关注。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h2 id="c16e" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">参考</h2><p id="c11f" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">[1]萨拉赫胡季诺夫、鲁斯兰&amp;姆尼赫、安德烈。<em class="le">概率矩阵分解</em>。NIPS’07:第20届神经信息处理系统国际会议论文集，第1257-1264页，2007。</p><p id="1915" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]布鲁克斯-巴特利特·乔尼。<em class="le">概率概念讲解:参数估计的贝叶斯推断</em>。地点:【https://bit.ly/3bajPNC T2】</p></div></div>    
</body>
</html>