<html>
<head>
<title>Build an Extreme Learning Machine in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 构建一个极限学习机</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-an-extreme-learning-machine-in-python-91d1e8958599?source=collection_archive---------11-----------------------#2020-05-29">https://towardsdatascience.com/build-an-extreme-learning-machine-in-python-91d1e8958599?source=collection_archive---------11-----------------------#2020-05-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9d01" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">无参数调整的神经网络构建指南。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/38e99f471a512e3e4c0f1e4e672830a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VC2EDGCxMqwK63ha"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔希·里默尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="48ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">极端学习机(ELMs)是单隐层前馈神经网络(SLFNs ),与基于梯度的学习技术相比，能够更快地学习。这就像一个没有学习过程的经典的单隐层神经网络。这种神经网络不执行迭代调整，使得它比使用反向传播方法训练的网络更快，具有更好的泛化性能。</p><p id="6b7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">elm 基于通用近似定理，该定理指出:</p><blockquote class="lv lw lx"><p id="3010" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">在对激活函数的温和假设下，具有包含有限数量神经元的单个隐藏层的前馈网络可以逼近 R^n 的紧凑子集上的连续函数</p></blockquote><p id="c4d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这仅仅意味着，如果 ELMs 具有足够的隐藏神经元和训练数据来学习所有隐藏神经元，则 ELMs 可以以显著的准确度解决分类和回归任务。</p><p id="4a74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解 ELM 是如何工作的，让我向您展示一个插图和构建模型的步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/25db54425530f17cc614ad0946276175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fddSNzjVfXa_Hh5FML9frg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者的极限学习机插图</p></figure><p id="7cc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，鉴于以下情况:</p><ul class=""><li id="ecec" class="md me it lb b lc ld lf lg li mf lm mg lq mh lu mi mj mk ml bi translated">训练集</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/f220ea886cd84e12926dd2005168c155.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*fUidfLnsAJd3H_YPJzHXTA.png"/></div></figure><ul class=""><li id="4d55" class="md me it lb b lc ld lf lg li mf lm mg lq mh lu mi mj mk ml bi translated">隐藏节点输出函数<em class="ly"> H </em> ( <strong class="lb iu"> w </strong>，<em class="ly"> b </em>，<strong class="lb iu"> x </strong>)</li><li id="432f" class="md me it lb b lc mn lf mo li mp lm mq lq mr lu mi mj mk ml bi translated">隐藏节点数<em class="ly"> L </em></li></ul><p id="bab0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过三个简单的步骤来实现 ELM:</p><ol class=""><li id="23f3" class="md me it lb b lc ld lf lg li mf lm mg lq mh lu ms mj mk ml bi translated">随机分配隐藏节点<strong class="lb iu"> ( <em class="ly"> w </em>，<em class="ly"> b </em> ) </strong>的参数</li><li id="6499" class="md me it lb b lc mn lf mo li mp lm mq lq mr lu ms mj mk ml bi translated">计算隐藏层输出矩阵<strong class="lb iu"> H </strong></li><li id="02d6" class="md me it lb b lc mn lf mo li mp lm mq lq mr lu ms mj mk ml bi translated">计算输出权重<strong class="lb iu"> β </strong></li></ol><p id="fbe7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们进入编程部分。我希望你知道如何用 python 编程，并且熟悉已经在机器学习中使用的包，例如 scikit-learn、numpy 和 pandas。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/9d165597571503dbc4a517b5f91b8915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*R6HOYxrURvb5N2Yc.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MNIST 手写数字数据集由<a class="ae ky" href="https://stathwang.github.io/" rel="noopener ugc nofollow" target="_blank"> stathwang </a>在<a class="ae ky" href="https://stathwang.github.io/category/machine-learning.html" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上</p></figure><p id="3eac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用 MNIST 数据集训练网络来分类手写数字。</p><p id="0382" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要导入必要的包来构建模型。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="6dc3" class="mz na it mv b gy nb nc l nd ne">import numpy as np<br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import OneHotEncoder<br/>from sklearn.preprocessing import MinMaxScaler</span></pre><p id="d3ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们需要加载数据集来训练网络和测试模型。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="1a55" class="mz na it mv b gy nb nc l nd ne">train = pd.read_csv('mnist_train.csv')<br/>test = pd.read_csv('mnist_test.csv')</span></pre><p id="b473" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一种预处理技术，我们将使用 scikit-learn 包中的 MinMaxScaler 和 OneHotEncoder 在(0，1)的范围内归一化我们的特征，并将我们的目标转换为 one-hot 编码格式。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="0597" class="mz na it mv b gy nb nc l nd ne">onehotencoder = OneHotEncoder(categories='auto')<br/>scaler = MinMaxScaler()</span><span id="18c7" class="mz na it mv b gy nf nc l nd ne">X_train = scaler.fit_transform(train.values[:,1:])<br/>y_train = onehotencoder.fit_transform(train.values[:,:1]).toarray()</span><span id="5d70" class="mz na it mv b gy nf nc l nd ne">X_test = scaler.fit_transform(test.values[:,1:])<br/>y_test = onehotencoder.fit_transform(test.values[:,:1]).toarray()</span></pre><p id="81e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了初始化我们的网络，我们需要识别以下内容:<br/> 1 .输入层的大小，即输入特征的数量<br/> 2。隐藏神经元的数量<br/> 3。隐藏权重<br/> 4 的输入。隐藏层激活功能</p><p id="69cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入图层的大小是指数据集输入要素的数量。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="59cd" class="mz na it mv b gy nb nc l nd ne">input_size = X_train.shape[1]</span></pre><p id="4cb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们将隐藏神经元的数量初始化为 1000。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="84a5" class="mz na it mv b gy nb nc l nd ne">hidden_size = 1000</span></pre><p id="f69e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们需要从高斯分布中随机初始化我们的输入权重和偏差。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="c859" class="mz na it mv b gy nb nc l nd ne">input_weights = np.random.normal(size=[input_size,hidden_size])<br/>biases = np.random.normal(size=[hidden_size])</span></pre><p id="debd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用一个<strong class="lb iu">整流线性单元(ReLU) </strong>作为我们的隐藏层激活函数。</p><p id="889a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:您可以使用不同的激活功能。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="79c6" class="mz na it mv b gy nb nc l nd ne">def relu(x):<br/>   return np.maximum(x, 0, x)</span></pre><p id="243c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们完成了网络初始化！</p><p id="d6f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来要做的是创建一个函数来计算输出权重，这就是我们的<strong class="lb iu"> <em class="ly"> β </em> </strong>。目标是使用范数最小二乘解最小化目标(训练标签)和输出(预测标签)之间的最小二乘误差:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/b365f31d9df55bcbffb46e2af4faedcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/format:webp/1*0t6skmjOIi7jfIiBApiXSA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/1c7204dda8bc2a3c4d44e57b8a68bf76.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*rsTvOrWMhu-j3ZWQPFcQ4Q.png"/></div></figure><p id="801d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> <em class="ly"> H </em> </strong>(匕首)是矩阵<strong class="lb iu"> <em class="ly"> H </em> </strong>，<strong class="lb iu"> <em class="ly"> T </em> </strong>是我们的目标。</p><p id="73f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的<strong class="lb iu"> <em class="ly"> H </em> </strong>这里是我们网络的隐藏层。让我们创建一个函数来计算我们的<strong class="lb iu"> <em class="ly"> H </em> </strong>向量。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="85bf" class="mz na it mv b gy nb nc l nd ne">def hidden_nodes(X):<br/>    G = np.dot(X, input_weights)<br/>    G = G + biases<br/>    H = relu(G)<br/>    return H</span></pre><p id="a6e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们只用一行代码就可以用 python 计算出我们的<strong class="lb iu"> <em class="ly"> β </em> </strong>。让我们将输出矩阵分配给一个变量名<strong class="lb iu"> output_weights </strong>。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="cb4a" class="mz na it mv b gy nb nc l nd ne">output_weights = np.dot(pinv2(hidden_nodes(X_train)), y_train)</span></pre><p id="8e09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代码使用函数<strong class="lb iu"> pinv </strong>计算<strong class="lb iu"><em class="ly"/></strong>的 Moore-Penrose 伪逆，我们得到<strong class="lb iu"><em class="ly"/></strong>【dagger】和<strong class="lb iu"> <em class="ly"> T </em> </strong>的点积。结果是计算出的<strong class="lb iu"> <em class="ly"> β </em> </strong>(隐藏为输出权重)。</p><p id="65b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们有了一个模型。我们没有应用任何技术来调整我们的权重，我们只是简单地计算它们。</p><p id="31a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了确保我们的模型产生一个好的结果，我们必须首先测试它。让我们创建一个函数来处理测试。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="dfe1" class="mz na it mv b gy nb nc l nd ne">def predict(X):<br/>    out = hidden_nodes(X)<br/>    out = np.dot(out, output_weights)<br/>    return out</span></pre><p id="0a40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经可以开始预测了。为此，让我们编写以下代码:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="96ac" class="mz na it mv b gy nb nc l nd ne">prediction = predict(X_test)<br/>correct = 0<br/>total = X_test.shape[0</span><span id="aef0" class="mz na it mv b gy nf nc l nd ne">for i in range(total):<br/>    predicted = np.argmax(prediction[i])<br/>    actual = np.argmax(y_test[i])<br/>    correct += 1 if predicted == actual else 0<br/>accuracy = correct/total<br/>print('Accuracy for ', hidden_size, ' hidden nodes: ', accuracy)</span></pre><p id="3b1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型的精度是<strong class="lb iu"> 0.9439，</strong>考虑到我们只有一个具有 1000 个隐藏节点的隐藏层，以及用于学习的非迭代调整，这是一个很好的结果，使得它比任何基于梯度的技术都快。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c978dd909cc22820862322f05653dec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*rrixYuEDR9W4Ju21-gpyWw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每增加一个隐藏节点的精度</p></figure><p id="16e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参考上图，当将隐藏节点的数量扩展到 1000 ( <strong class="lb iu"> 0.9439 </strong>到<strong class="lb iu"> 0.977) </strong>)时，ELM 的精度显著提高。这意味着，只要我们设置了正确的网络隐藏节点数，ELM 就能更好地进行归纳。</p><p id="66e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个实现的 jupyter 笔记本可以在下面我的 GitHub <strong class="lb iu"> </strong>教程库中找到。</p><div class="nj nk gp gr nl nm"><a href="https://github.com/glenngara/tutorials" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">Glenn gara/教程</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">在 GitHub 上创建一个帐户，为 glenngara/tutorials 开发做出贡献。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">github.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa ks nm"/></div></div></a></div><h2 id="9387" class="mz na it bd ob oc od dn oe of og dp oh li oi oj ok lm ol om on lq oo op oq or bi translated"><span class="l os ot ou bm ov ow ox oy oz di"> R </span>引用</h2><p id="71ab" class="pw-post-body-paragraph kz la it lb b lc pa ju le lf pb jx lh li pc lk ll lm pd lo lp lq pe ls lt lu im bi translated">[1]光，，秦，，萧启庆，<a class="ae ky" href="https://www.sciencedirect.com/science/article/abs/pii/S0925231206000385" rel="noopener ugc nofollow" target="_blank">极限学习机:理论与应用</a> (2006)，神经计算</p><p id="e1db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]光，<a class="ae ky" href="https://www.ntu.edu.sg/home/egbhuang/" rel="noopener ugc nofollow" target="_blank">极限学习机</a>，南洋理工大学</p><p id="4e09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]光，，秦，，萧启庆，<a class="ae ky" href="https://ieeexplore.ieee.org/document/1380068" rel="noopener ugc nofollow" target="_blank">极限学习机:前馈神经网络的一种新的学习方案</a> (2004)，IEEE 国际神经网络联合会议</p></div></div>    
</body>
</html>