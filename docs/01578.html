<html>
<head>
<title>Who’s a Big Ol’ Fibber?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谁是大骗子？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/whos-a-big-ol-fibber-b6b1259304db?source=collection_archive---------26-----------------------#2020-02-12">https://towardsdatascience.com/whos-a-big-ol-fibber-b6b1259304db?source=collection_archive---------26-----------------------#2020-02-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="290c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一次从多个Politifact配置文件中找出意义，这是实际聚类分析中的一个练习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d2a0ee5f053b9ec45118c764cf8c23c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4p9QDjgs0uE3cp5NAkrktw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">paweczerwi ski在<a class="ae kv" href="https://unsplash.com/s/photos/election?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2c08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated">乖乖女的圣诞节！没错，我们四年一次的总统选举周期开始了。每四年，里面的政党举行一次形式上的加冕仪式，而外面的政党装满一辆小丑车进行全国巡演。除非第22修正案生效，否则这就是一场小丑赛车。</p><p id="8185" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">今年只有一辆小丑车，但马戏团已经拿出一大包来装满它。我们如何区分它们？有很多标准，你需要优先考虑你自己的。这通常包括研究，观看辩论，或者只是等待别人为你做决定。我将只处理“研究”角度的一个方面。与其用更多的废话来填充这篇文章，我将切入正题。我在比较<a class="ae kv" href="http://www.politifact.com" rel="noopener ugc nofollow" target="_blank"> Politifact </a>记分卡。你可能已经知道什么是Politifact记分卡。</p><p id="8dbc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你不知道，Politifact是一家报纸运营的在线“事实核查”实体。它对各种公共实体的声明进行六分制评级。因此，它有以下限制:它是“值得注意”的偏见。声明必须由政治上认为值得注意的人做出，而且声明本身必须有值得注意的来源和值得注意的重要性。由于Politifact没有无限的员工和预算，这意味着声明和故事会被忽略。因为决策是由人做出的，所以存在偏见。这背后没有恶意或议程。其次，它的评级包含一些偏见因素，尤其是当评级处于区间的中间时。这是不可避免的。有时需要做出判断。“裤子着火了”和“真实”很容易判断，但是当你处于中间时，可能会有些模糊。</p><p id="c1ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，Politifact对某个人的评价越多，我们对这个人的整体形象就越有信心。这是由于“中心极限定理”，该定理指出，从总体(陈述)中获得的样本(评级)越多，“模糊”元素相互抵消得越多。</p><p id="b3e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，偏见问题对于利用Politifact档案来说并不是什么大不了的事情。是<em class="mb">简介</em>问题。我什么意思？大多数人似乎以一种精心挑选的方式使用Politifact。他们会放大他们认为合适的特定规则。如果你的目标是研究具体问题，那<em class="mb">没什么不对。<em class="mb">如果你的目标是对政治人物进行广泛的比较，那就大错特错了。当我们关注单个数据点时，我们让自己的情绪来驱动结论。如果政治家A对我们的一个个人热点问题说了一些令人讨厌的话，而那句令人讨厌的话恰好被裁定为“错误”，那么它会影响我们对政治家A的整体看法，而不管该政治家的其他历史。</em></em></p><h1 id="41e5" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">问题是</h1><p id="1326" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">不幸的是，我们大多数人都不会硬连线比较个人资料，尤其是在选举前的一年，我们可能需要比较<em class="mb">多个</em>个人资料。看看这个:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/02ec8bab6ff9160b5992d68eb97eeec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/1*4GI3zfBuWakFkv3eWtKEtA.gif"/></div></div></figure><p id="23c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是特朗普先生、准民主党候选人(截至2010年2月10日)、彭斯先生和国会四名政党领导人的聚合politifact简介。误差棒是使用每个人的总裁定数作为“n”的比例的标准误差。条目按照从最多到最少的规则排序。举例来说，很容易一眼就看出特朗普先生与桑德斯先生不相似，但他们两人如何同时与沃伦女士相比较，以及如何同时与其他所有人相比较？</p><h1 id="b6e1" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">一个解决方案？</h1><p id="a166" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">我们并不是天生就能通过查看多个个人资料来回答这些问题。幸运的是，有一种方法可以回答这些问题:集群。聚类有很多种类型。我不打算详细讨论所有这些问题。如果不熟悉，可以从这篇文章开始<a class="ae kv" href="https://en.wikipedia.org/wiki/Cluster_analysis" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="fb45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我在本文中使用的类型称为层次聚类。我碰巧喜欢它，因为它链接了整个数据集，这使得很容易看到一个集群如何与另一个集群相关联。在我开始具体分析之前，我将切入正题，向您展示我的聚类分析的基本结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/1b7a5b5d274f199ccb162eb4f3fbef78.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/1*HQ7tsdoycH-pVl-RQKp9rQ.gif"/></div></figure><p id="18bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">颜色对应于集群。有三个基本的集群，加上特朗普先生在他自己的现实世界里。共和党人的名字用下划线标出。我们得到了一些有趣的结果。南希·佩洛西和Mssrs是一类人。彭斯和麦卡锡。米奇·麦康奈尔和Mssrs是一类人。桑德斯、布蒂吉格、拜登和舒默，还有加巴德。你们中的一些人可能看着柱形图并不同意。<strong class="ky ir">好</strong>，因为如果你把上面的聚类当成<em class="mb">那个</em>聚类，你就没有理解聚类的所有问题。</p><h1 id="94e9" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">比宣传的还要乱！</h1><p id="85ac" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">这棵树是几种选择的结果。没有一种聚类形式是客观的。所有的聚类都要求原始信息的某些方面比其他方面更重要。结果<em class="mb">将</em>受到这些选择的影响。我现在要脱光了。我将公开我所做的所有选择，以及它们可能如何改变输出。首先，为什么是四个集群？有各种各样令人困惑的方法来为层次聚类选择“最佳”数量的聚类。R包"<a class="ae kv" href="https://cran.r-project.org/web/packages/NbClust/" rel="noopener ugc nofollow" target="_blank"> NbClust </a>"有超过30个索引，它可以通过计算找到层次聚类中的“最佳”聚类数。对于给定的聚类，它们通常彼此不一致。我用了“<a class="ae kv" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank">肘法</a>，基于“T6”聚类平方和。还有一些我没有提到的方法。</p><p id="166d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">聚类通常从创建“距离矩阵”开始。这是所有数据点之间所有“距离”的集合。但什么是“距离”呢？这并不简单。通常我们说的“距离”是指“欧氏距离”。这是两点之间直线的长度。但是还有其他类型的“距离”。“城市街区”或“曼哈顿”的距离就是它听起来的样子。你也可以使用所谓的“相关距离”。我为上面的树选择的是“分数距离”。有很好的理由对超过三维的数据集使用分数距离。其他人对它们的解释比我好得多。我也做了<em class="mb">选择</em>来解释不确定性。档案中的规则越少，档案相对于实际人物与事实的关系就越不确定。为此，我简单地将原始距离矩阵除以标准误差。这并不复杂，但它确实意味着一个人的不确定性越大，他和其他人就越“亲近”。</p><h1 id="25e9" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">如此遥远…</h1><p id="8542" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">但是如果我们不使用分数距离呢？如果我们使用欧几里得距离、曼哈顿距离或相关距离会怎么样？这是什么:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/703b172cb100059db665915ecb958dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*nnq2u9AW9CgKIHNBlV-rjg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同距离方法的结果</p></figure><p id="09d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">他们彼此相似，但你可以看到不同之处。每种距离方法强调数据的不同方面。哪个是“真”的？这可能取决于数据。这也可能取决于您想要探索数据的哪些方面。例如，相关距离(1-相关系数)强调两个数据趋势之间的关系如何变化。</p><h1 id="1fc2" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">“错误”并不总是坏事。</h1><p id="d68e" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">你可能还记得，我说过我把不确定性粗略地融入了分析中。如果我没有呢？如果我简单地说“比例就是比例”，并(愚蠢地)假定Politifact数据的快照总是对人物的准确描述，会怎么样呢？这是如果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/3579f42955e3b881c893976e625db07c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*fLgqBokri0cXnbBp_Iyqpw.gif"/></div></div></figure><p id="80ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不一样，不是吗？我们在这里看到了什么？我们看到了小样本对Patrick，Steyer和Bennet的影响。小样本意味着他们的原始数据是极端的。误差函数部分地照顾到了这一点，并使它们有更高的机会出现在先前树的其他聚类中。我们还看到了“垃圾聚类”效应，其中聚类算法无法清晰地分离样本，大多数样本最终聚集在一个大的聚类中。为什么我把这些树分成四簇？主要是为了与第一组树进行比较。</p><h1 id="9950" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">这些树不只是生长。</h1><p id="e0f5" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">您会注意到，我谈到了距离度量，但没有提到我如何使用它们来构建树。这是因为我没有对任何一棵树使用单一的度量标准。相反，我用五种不同的方法计算了一棵共识树。为什么？在构建层次聚类树的许多不同方法中，哪种方法比其他方法更好还没有定论。所以我结合了几个互不违背对方假设的。</p><p id="2080" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">具体来说，我使用了“<a class="ae kv" href="https://en.wikipedia.org/wiki/Single-linkage_clustering" rel="noopener ugc nofollow" target="_blank">单个</a>”、“<a class="ae kv" href="https://en.wikipedia.org/wiki/UPGMA" rel="noopener ugc nofollow" target="_blank"> UPGMA </a>”、“<a class="ae kv" href="https://en.wikipedia.org/wiki/Complete-linkage_clustering" rel="noopener ugc nofollow" target="_blank">完全</a>”、“<a class="ae kv" href="https://www.sciencedirect.com/science/article/pii/S1110016815001933" rel="noopener ugc nofollow" target="_blank">整除</a>”、“<a class="ae kv" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4527350/" rel="noopener ugc nofollow" target="_blank"> MiniMax </a>”的方法将数据链接起来(又名“链接方法”)。还有其他人，但他们有我不想处理的假设或缺点。例如，沃德的方法只有在使用平方欧几里得距离时才能正常工作。中值/中值/质心方法有可能产生负的<em class="mb">分支长度。甚至我的思想也不去那里。</em></p><p id="a1c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些链接方法中的每一种都从相同的距离矩阵产生不同的树。如果我们输入用于创建第一棵树的分数距离度量，这是每个链接方法的输出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/6a728d88386af2312a56384bd09a04c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*dfEze5IExBdxOg-2gGHZ9w.gif"/></div></div></figure><p id="74dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有明显的不同，但也有相似之处。你会注意到，在大多数树中，特朗普先生是一个异数。佩洛西和彭斯在五棵树中的三棵树上成对出现。如果你看的话，你可以找到其他的相似之处。当达成共识时会发生什么？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/1b7a5b5d274f199ccb162eb4f3fbef78.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/1*HQ7tsdoycH-pVl-RQKp9rQ.gif"/></div></figure><p id="7105" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回家了，回家了，吉吉吉吉！</p><h1 id="f057" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">打开引擎盖</h1><p id="7157" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">虽然以上这些可能很可爱，但如果我不具体解释我是如何做到的，那就既没用也不道德。我的矩阵和树是我和T2一起做的。另外，我使用了<a class="ae kv" href="https://cran.r-project.org/web/packages/ape/index.html" rel="noopener ugc nofollow" target="_blank">猿</a>、<a class="ae kv" href="https://cran.r-project.org/web/packages/protoclust/" rel="noopener ugc nofollow" target="_blank"> protoclust </a>、<a class="ae kv" href="https://cran.r-project.org/web/packages/cluster/index.html" rel="noopener ugc nofollow" target="_blank">簇</a>、<a class="ae kv" href="https://cran.r-project.org/web/packages/proxy/" rel="noopener ugc nofollow" target="_blank">代理</a>和<a class="ae kv" href="https://cran.r-project.org/web/packages/clue/" rel="noopener ugc nofollow" target="_blank">线索</a>包。我通过写一个脚本部分自动化了这个过程(如下)。平方和的聚类是使用旧GMD包中的脚本完成的。Cran知识库不再支持GMD。相反，你必须从<a class="ae kv" href="https://github.com/cran/GMD" rel="noopener ugc nofollow" target="_blank"> GitHub </a>手动添加它。</p><p id="fa2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">脚本的命令是<code class="fe ne nf ng nh b">Pol.Trees(ipnut, meth = "frac.dist", ErrAdj = TRUE, consensus = TRUE, frac.f = 2/3)</code></p><p id="bcd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">参数:</p><p id="f4cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">IP nut:<em class="mb">count</em>数据的数据框架或矩阵，类别为列名，主题为行名。</p><p id="eb2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">方法:距离矩阵生成法。可以尝试通过base R或proxy与dist函数一起使用的任何方法。此外，我在脚本中编写了一个名为“chi.dist”的校正“chi distance”。<strong class="ky ir"> <em class="mb">警告:</em> </strong>慎用方法，有些零数据点的获取方式不靠谱。</p><p id="0358" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ErrAdj:是否要包括误差调整。默认值为TRUE。</p><p id="0d14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">共识:是否希望生成共识树。默认值为TRUE。该树采用“phylo”格式。一个错误反而会生成树的线索集合。</p><p id="7f4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">frac.f:使用分数距离(frac.dist)时使用的默认分数。</p><p id="7cde" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我不得不承认一些事情:这个脚本不会安排和给树上色。他们会很光秃秃的。ape中有一些命令可以处理这个问题。</p><h1 id="d615" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">这一切的意义是什么？</h1><p id="71c3" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">我为什么要写这个呢？你为什么要费心去读它？让我们面对现实吧，有很多情况下我们需要一次比较两个以上的东西，聚类是一个很好的方法。它以我们直观理解的方式直观地展示事物。也可以很好玩。当政治家的行为不符合政党路线时(如佩洛西-彭斯案)，这就是一个发现。</p><p id="9569" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这不会拯救世界，但它可能会帮助人们从太多的数据中找到意义。像Politifact这样的网站是有好处的，但是它们会把我们埋葬在数据中。聚类可以将数据转化为理解。</p><p id="7268" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，去收集你自己的计数数据，和树一起玩吧。</p><pre class="kg kh ki kj gt ni nh nj nk aw nl bi"><span id="b4ab" class="nm md iq nh b gy nn no l np nq">Pol.Trees &lt;- function(ipnut, meth = "frac.dist", ErrAdj = TRUE, consensus = TRUE, frac.f = 2/3){<br/>#This computes and returns a consensus tree based on count data profiles (such as from Politifact). It presumes you are interested<br/>#in comparing proportions, not raw counts. In addition, it automatically adjusts for uncertainty ("error") due to small counts. Even <br/>#then, I don't recommend having 4 or less total counts per subject.</span><span id="3b01" class="nm md iq nh b gy nr no l np nq">#The parameters are as follows:<br/>    #ipnut: The count data set, wich subject names as the rownames<br/>    #meth: Method to determine distance matrix, default is "frac.dist". WARNING: Not all methods will work properly. Recommended you<br/>    #stick to manhattan, euclidean, frac.dist, chi.dist (both included here), or correlation. The proxy package is a little wonky with some other metrics.<br/>        #Why manhattan distance and not euclidean? <a class="ae kv" href="https://bib.dbvis.de/uploadedFiles/155.pdf" rel="noopener ugc nofollow" target="_blank">https://bib.dbvis.de/uploadedFiles/155.pdf</a><br/>    #ErrAdj is whether to apply the error adjustment. Default is TRUE</span><span id="1aed" class="nm md iq nh b gy nr no l np nq">#Several libraries are needed for this script to function.<br/><br/>    library(ape)<br/>    library(protoclust)<br/>    library(cluster)<br/>    library(proxy)<br/>    library(clue)</span><span id="7ecb" class="nm md iq nh b gy nr no l np nq">#This section creates then activates the "error adjustment" distance metric.    <br/>    DistErrAdj &lt;- function(x,y)<br/>    {<br/>        sing.err &lt;- sqrt((x^2) + (y^2))<br/>        sum(sing.err)<br/>    }<br/>    <br/>    if (pr_DB$entry_exists("DistErrAdj") == TRUE)<br/>    {<br/>        pr_DB$delete_entry("DistErrAdj")<br/>        pr_DB$set_entry(FUN = DistErrAdj, names = c("DistErrAdj"))<br/>    }<br/>    else<br/>    {<br/>        pr_DB$set_entry(FUN = DistErrAdj, names = c("DistErrAdj"))<br/>    }<br/>    <br/>    #This section creates then activates the "fractional" distance metric. It uses a default f of 2/3, which can be altered with the frac.f input.<br/>    frac.dist &lt;- function(x, y, frac)<br/>    {<br/>        frac = frac.f<br/>        sum(singfrac &lt;- (abs(x - y) ^ frac) ^ 1/frac)<br/>    }<br/>    <br/>    if (pr_DB$entry_exists("frac.dist") == TRUE)<br/>    {<br/>        pr_DB$delete_entry("frac.dist")<br/>        pr_DB$set_entry(FUN = frac.dist, names = c("frac.dist"))<br/>    }<br/>    else<br/>    {<br/>        pr_DB$set_entry(FUN = frac.dist, names = c("frac.dist"))<br/>    }<br/>    <br/>    #This section creates then activates a correction for the chi distance. The version in proxy generates negative distances.<br/>    chi.dist &lt;- function(x, y)<br/>    {<br/>        sqrt(sum(((x-y)^2)/(colsum/sum(colsum))))    <br/>    }<br/>    <br/>    <br/>    if (pr_DB$entry_exists("chi.dist") == TRUE)<br/>    {<br/>        pr_DB$delete_entry("chi.dist")<br/>        pr_DB$set_entry(FUN = chi.dist, names = c("chi.dist"))<br/>    }<br/>    else<br/>    {<br/>        pr_DB$set_entry(FUN = chi.dist, names = c("chi.dist"))<br/>    }<br/>    <br/>#Creating the proportions and errors for each data point.</span><span id="91b1" class="nm md iq nh b gy nr no l np nq">#This creates a consensus tree without bootstraps and does not automatically plot. It is the non-default. Use it if you want to have a tree<br/>#to manipulate or annotate manually.<br/>        x &lt;- ipnut / rowSums(ipnut)<br/>        q &lt;- sqrt((x * (1 - x)) / rowSums(ipnut))<br/>        colsum &lt;- colSums(x)<br/>        rawdist &lt;- dist(x, method = meth)<br/>        errs &lt;- dist(q, method = "DistErrAdj") / if(ErrAdj == TRUE) {1} else {dist(q, method = "DistErrAdj")}<br/>        findist &lt;- rawdist / errs<br/>        tr.ave = as.hclust(agnes(findist, diss = TRUE, method = "average"))<br/>        tr.prot = protoclust(findist)<br/>        tr.div = as.hclust(diana(findist, diss=TRUE))<br/>        tr.single = as.hclust(agnes(findist, diss = TRUE, method = "single"))<br/>        tr.comp = as.hclust(agnes(findist, diss = TRUE, method = "complete"))<br/>        tr.ensemble &lt;- cl_ensemble(tr.ave, tr.prot, tr.div, tr.single, tr.comp)<br/>        tr.cons &lt;- ladderize(as.phylo(as.hclust(cl_consensus(tr.ensemble))))</span><span id="b0d4" class="nm md iq nh b gy nr no l np nq">if(consensus == TRUE)<br/>        {<br/>            tr.cons<br/>        }<br/>    <br/>        else<br/>        {<br/>            tr.ensemble<br/>        }</span><span id="7935" class="nm md iq nh b gy nr no l np nq">}</span></pre></div></div>    
</body>
</html>