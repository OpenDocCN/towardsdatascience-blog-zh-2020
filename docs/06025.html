<html>
<head>
<title>Policy gradient in multi-task/meta-learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多任务/元学习中的策略梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/policy-gradient-in-multi-task-meta-learning-2aeeaf23817d?source=collection_archive---------47-----------------------#2020-05-16">https://towardsdatascience.com/policy-gradient-in-multi-task-meta-learning-2aeeaf23817d?source=collection_archive---------47-----------------------#2020-05-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a08c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这个故事介绍了多任务强化学习问题和策略梯度&amp;它们的多任务/元对应物。第二部分介绍Q-学习和多任务Q-学习。这个故事是对课程'<a class="ae ki" href="https://www.youtube.com/watch?v=UPT4Rndftc8&amp;list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&amp;index=6" rel="noopener ugc nofollow" target="_blank">斯坦福CS330:多任务和元学习，2019 |第六讲—强化学习入门'</a>的简短总结。</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/6f0612da46eba245d2778c96c1fccf75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1aU70oZX_OgoVPtQe4DCnw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">via <a class="ae ki" href="https://wordart.com/" rel="noopener ugc nofollow" target="_blank">艺术字</a></p></figure><p id="7bfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">什么时候不需要顺序决策？当您的系统正在做出一个单独的决策(例如分类、回归)，并且该决策不影响未来的输入或决策时，您不需要进行顺序决策。常见的应用领域包括机器人、语言和对话、自动驾驶、商业运营和金融。</p><p id="debe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个故事将涵盖</p><ul class=""><li id="03f8" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">多任务强化学习问题</li><li id="866e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">策略梯度及其多任务/元对应物</li></ul><p id="2fc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个故事的第二部分将涵盖Q-学习和多任务Q-学习。这个故事是课程'<a class="ae ki" href="https://www.youtube.com/watch?v=UPT4Rndftc8&amp;list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&amp;index=6" rel="noopener ugc nofollow" target="_blank"> Stanford CS330:多任务和元学习，2019 |第六讲—强化学习入门'</a>的简短总结。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="651c" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">多任务强化学习问题</h1><p id="eb75" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在监督学习中，数据是iid，我们需要一个大的带标签的、精选的数据集，而在顺序决策中，操作影响下一个状态而不是iid，对于数据集，如何收集数据？有哪些标签？</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nn"><img src="../Images/4660e1672524ff88a0558c3fda2ebc3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5f8YO298jCGrVsbrwjFxqw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">监督学习</p></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi no"><img src="../Images/28274607b0fd889696c85a6939ec3d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/1*Byi2CO1ZN30c0fnhOIcZ6g.gif"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">RL设置。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="ee46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于监督学习，我们有一个由θ参数化的函数π，它接受o并生成输出a。例如，输入可以是图像，而输出可以是图像类(s.t. tiger)。在RL中，我们使用将采取行动的策略，这些行动将影响下一个状态。这将是一个从行动到观察的反馈循环，我们的类(a)将是行动，比如“逃跑”、“忽视”或“抚摸老虎”。o表示代理(系统)作为输入接收的观察值，a表示动作，π表示由θ参数化的策略。<strong class="lb iu"> <em class="np">通常我们假设世界s存在某种潜在状态，在完全观测的情况下，s被观测到。在部分观察设置中，观察到o。</em>T11】</strong></p><p id="eda3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">o和s的具体区别是什么？例如，你试图追逐一只鬣狗，如果给你一张图像，那么这些图像将是一个观察o，而相反，如果给你相应动物的姿势，那么这些姿势将是s。你基本上能够完全观察到系统的潜在状态以及世界上对决策有影响的事情。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nq"><img src="../Images/0cacb665ea31321abbba5941cd0bf30e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2onRDD24Ljk68nUvq7lq7g.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">模仿学习。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="d484" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决顺序决策问题的一个基本方法是将其视为监督学习(SL)问题。 比如<strong class="lb iu">你想模仿某些专家</strong>。<a class="ae ki" href="https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf" rel="noopener ugc nofollow" target="_blank">也许您可以收集一组驾驶数据</a>，收集人们看到的观察结果，并收集他们在这些状态下采取的行动，将其放入一些大的训练数据集中，然后在监督学习期间从这些训练数据集中采样iid，以训练您的策略根据观察结果预测行动。如果您有大量关于执行正确操作的专家数据，这种方法在某些情况下很有效。但是这些系统不以任何方式推理结果，而只是模仿数据在做什么。</p><p id="4d07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在RL中，需要<strong class="lb iu">奖励函数</strong>。这些奖励函数应该捕捉什么样的状态和行为对系统更好或更坏。它通常接受状态和动作(<code class="fe nr ns nt nu b">r(s,a)</code>)，并告诉我们哪些状态和动作更好。例如，如果我们在开车，如果我们开得平稳，我们可能会得到很高的奖励，如果我们发生车祸，我们会得到很低的奖励。</p><p id="b182" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总的来说，状态<code class="fe nr ns nt nu b">s</code>、行动<code class="fe nr ns nt nu b">a</code>和回报<code class="fe nr ns nt nu b">r(s,a)</code>，以及系统的动态<code class="fe nr ns nt nu b">p(s'|s,a)</code>定义了一个<strong class="lb iu">马尔可夫决策过程，</strong>因为这包含了顺序决策问题的概念。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/50f8b4e17efb4f705a42382dd9e95357.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/1*nzUjAVp3icmn72i3_P1SIA.gif"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">RL示例。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="b9ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，强化学习的目标通常是学习作为输入的策略参数。 在这种情况下，我们将把完全观察到的设定值作为输入状态，并对动作做出预测。目标是了解政策的参数。在深度RL设置中，您的策略可能会被参数化为神经网络，其中状态作为输入被处理，动作作为输出。行动反馈给世界，然后世界给你下一个状态，反馈给你的政策。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nw"><img src="../Images/097eb929e571018958bca3566938c7d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AtDGAsxeyc2Fq4jU2A2Ymw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">RL图形模型。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="093d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，我们可以将系统描述为图形模型，其中我们有一个策略，它接受观察并产生一个动作(部分观察的设置)，动态接受当前状态，当前动作产生下一个状态的分布。<strong class="lb iu"> <em class="np">动态函数独立于前一状态，这就是所谓的马尔可夫性质。</em> </strong>基本上一个<strong class="lb iu"> <em class="np">马尔可夫决策过程</em> </strong>中的一个状态的定义就是<strong class="lb iu"> <em class="np">你可以从那个状态变量中的信息独立于之前的状态来完全定义奖励函数和动力学。</em> </strong>你看这里的图，只取决于St和at，不取决于st-1。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nx"><img src="../Images/ab72e114b615565716ac0fe4eb185416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XdI9VDj4FxJW6IjU1xuRIA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">无限/有限视野情况下的RL目标。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="daa6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">RL的具体目标是最大化政策下的期望报酬。在无限期的情况下，我们希望在平稳分布下最大化回报函数，平稳分布在政策产生的状态和行动上。在有限范围的情况下，我们可能有一些范围资本T，当推出我们的政策时，您希望最大化州和行动的回报。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ny"><img src="../Images/150bdf9277956ab16c7373ee0100b56f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*98XsZdoDJvfjRuwQ24cA4Q.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">SL任务和RL任务</p></figure><p id="7213" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经讨论了RL问题，那么RL的任务是什么呢？<em class="np">如果将RL与监督学习设置进行比较，初始状态分布和动态基本上与数据生成分布相同。回报函数对应于损失函数，状态和行为空间只是告诉你，你的状态和行为所处的一般集合。</em>所以这只是作为一个马尔可夫决策过程。但是如果不同的MDP是不同的任务，那么这就不仅仅是任务的语义意义了。因为不同的任务可能有相同的确切奖励函数，但有不同的行动空间或不同的动态。因此，我们不严格地使用术语“任务”来描述这些不同的马尔可夫决策过程。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nz"><img src="../Images/a222e54dbd20e077201751d10bc8491b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tEmz6e5noIgPzfrFQjSklQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">个性化推荐和跨策略。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="31e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如何在元学习中应用RL？<strong class="lb iu">的一个应用是个性化推荐，</strong>不同的人有不同的任务。动力对应于那个人将如何对你采取的特定行动作出反应，奖励函数对应于你是否在一个好的状态下向结果推荐一些东西。在某些情况下，初始状态分布也可能因人而异，这取决于你如何表述你的问题。另一个应用是跨机动，例如，跨不同机动在计算机图形中制作不同角色的动画。如果你把这当成一个多任务学习问题，不同的任务在环境中会有不同的奖励函数，但是动力是一样的，状态和动作空间也是一样的。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oa"><img src="../Images/0304e4d0138f3e5191d299aaeef26fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZzBjvStjEq9Jp5R5Hmgh3g.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">RL的另类观点。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="1e2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看多任务RL的另一种方法如下。<strong class="lb iu"> <em class="np">我们通常有某种任务标识符，它是状态的一部分，这是使它成为完全可观察的设置或完全可观察的MDP </em> </strong>所必需的。s条表示原始状态，zi表示任务标识符。基本上，任务被视为标准的马尔可夫决策过程，其中状态空间和动作空间是原始任务中的状态空间和动作空间的并集。初始状态分布恰好对应于初始状态分布上的混合分布。对于这些任务中的每一个，动态和奖励函数是单个动态和单个奖励函数，其将任务标识符作为输入，并产生下一个状态或奖励。所以你基本上可以应用标准的单任务RL算法来解决多任务问题。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b0ad35bc849f82fd41b4e787992cd088.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*BbeM0i-AHafIa_GutMJNvQ.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">以目标为条件的强化学习的奖励函数</p></figure><p id="01ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">多任务RL </strong>与单任务RL问题相同，<strong class="lb iu">除了任务标识符是状态的一部分(s = (s bar，zi))。</strong>例如，任务标识符可以是一次性任务ID、任务的语言描述或您想要达到的目标状态。<em class="np">这个</em> <strong class="lb iu"> <em class="np">目标状态</em> </strong> <em class="np">会被称为</em> <strong class="lb iu"> <em class="np">目标制约RL </em> </strong> <em class="np">其中你把它限制在你希望将来能够达到的特定状态</em>。<strong class="lb iu">奖励函数可以与之前的相同</strong> w <em class="np">这里，它将任务id作为输入，并输出对应于该状态的任务的奖励函数，或者对于像目标条件RL这样的事情，它可以简单地对应于您的(原始)当前状态和目标状态之间的负距离。</em>距离函数的一些例子可以是某个潜在空间中的欧几里德距离或稀疏0/1，其中1表示s条等于sg，0表示不相等。如果这仍然是标准的马尔可夫决策过程，那么为什么不应用标准的RL算法呢？你可以，但这将比单独的任务更具挑战性，因为你将有更广泛的事情要做。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="98b0" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">策略梯度及其多任务/元对应项</h1><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oc"><img src="../Images/c81ccd085627b64eb1ad7f4f528f3ef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dfgij3OSWSnaIjYLyDipKw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">强化学习算法的剖析。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="ca2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="np">您通常可以在下面的流程图中查看RL算法，我们首先在环境中生成样本，这通常只是向前运行策略，然后我们拟合一些模型来估计回报，然后我们使用该模型来改进策略。</em> <em class="np">不同的算法通常只对应这个绿框和这个蓝框的差异</em>。例如，拟合模型的一个例子可能是估计经验回报，如使用蒙特卡罗政策梯度。估计回报的另一个例子可能是尝试拟合Q函数，例如使用动态编程算法。拟合模型的另一个例子是估计模拟动态的函数。例如，一旦我们有了这些模型，我们就可以将策略梯度应用于我们的策略参数，我们可以通过取当前Q函数的最大Q值来改进策略，或者在基于模型的算法的情况下，我们可以通过将模型反向传播到策略中来优化策略。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi od"><img src="../Images/4ef7a37c5ae1f74d542b58d8f8246930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ccghglnv2a3ryZ80K3ao5Q.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">评估目标。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="48d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以先说政策梯度。以上是我们在RL中的目标，所以我们希望能够从我们的策略中采样轨迹并估计回报。我们将这个目标称为θ的J。我们可以重写θ的J。例如，您可以将此视为或估计为滚动和轨迹，如右图所示，这是计算每个轨迹的奖励，可能第一个轨迹的奖励高，中间轨迹的奖励中等，最后一个轨迹的奖励差。<strong class="lb iu"> <em class="np">第一个总和是我们策略中样本的总和，第二个总和是一段时间的总和。</em>T3】</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oe"><img src="../Images/2086e03c52670a78ec20f9f8b2e6823b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aFakXg0UrkluNgx0uboCkw.png"/></div></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi of"><img src="../Images/5ab6e0a71dc37e3a523423576dc83440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9WhY3_hJEECf9u4YRjzNQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">直接政策分化。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="0ff9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="np">我们能否将这一目标直接转化为我们的政策？</em> </strong>所以<em class="np">如果我们的目标是预期收益，我们可以用轨迹</em>的收益来估计。你可以把这个期望看作π θ上的积分，因为这个期望是关于τ的π θ的。如果您想要计算该目标相对于我们的政策参数的梯度，您可以<em class="np">将梯度移入积分中，因为这是一个线性操作，然后您基本上就有了政策乘以回报函数的梯度在轨迹上的积分</em>。我们如何评估这个梯度呢？我们不需要对所有可能的轨迹进行积分。所以我们打算<strong class="lb iu"> <em class="np">使用似然比技巧。</em>两个π被抵消，它等于π的梯度或策略的梯度与策略参数的比值。在应用方便的恒等式之后，<strong class="lb iu"> <em class="np">我们可以简单地评估梯度，或者通过对从我们的策略中采样的轨迹进行期望来估计梯度，并使用这些样本来评估我们的策略的对数概率的梯度，该对数概率由该轨迹的回报进行加权。</em></strong>π的完整轨迹可以分解为初始态密度乘以一个随时间乘积的政策概率和动力学概率。经过计算，最终的梯度是我们可以评估的。</strong></p><div class="kk kl km kn gt ab cb"><figure class="og ko oh oi oj ok ol paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><img src="../Images/12bd64375e40515a95479a4fef5c1f00.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*P02PmxPmI1Bfj6BqvCd8TA.png"/></div></figure><figure class="og ko om oi oj ok ol paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><img src="../Images/d8c9049d04b85c3953a40e81c5d488bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*Dfgij3OSWSnaIjYLyDipKw.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk on di oo op translated">评估政策梯度。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure></div><p id="f489" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们基本上可以排除我们的策略来获得轨迹，然后通过在log π乘以奖励函数的时间内对这些轨迹进行平均来估计策略梯度，然后将梯度应用于我们的策略参数。如果我们回到图中，收集数据对应于橙色框(对应于右侧公式中的橙色下划线部分)，评估回报对应于绿色框(对应于绿色下划线部分)，在最后一步中实际使用它来改进策略对应于蓝色框(对应于蓝色下划线部分)。<strong class="lb iu"> <em class="np">增强算法是从您的策略中显式采样轨迹，然后使用这些轨迹计算梯度，然后使用估计的梯度更新您的策略参数。然后你可以重复这一步来迭代改进你的策略</em> </strong>。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oq"><img src="../Images/353b5ccf404082b6d020814b462387c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0EIlpORfA7KSlDuOxFgWfQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">与最大似然比较。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="8dc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这与模仿学习，如专家行动的最大可能性相比如何？<strong class="lb iu"> <em class="np">最大似然目标看起来非常类似于</em> </strong>的策略形式。特别是，区别在于右边的奖励项。所以基本上政策梯度将对应于最大化有高回报的行动的概率。如果他们有一个低回报，那么你将试图最大化它本质上更少。因为它基本上是一种梯度下降算法，所以很容易将多任务学习算法应用于它。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi or"><img src="../Images/8e49b84e445a74d65a7190f70e037a44.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/1*-Zdy8swDRKMlqUV-k61eig.gif"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="2832" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们进行最大似然模仿学习，我们只是试图模仿最佳轨迹，而在政策梯度中，我们在这些轨迹上有一些分布，然后我们将试图增加具有高回报的行动的概率，并在具有低回报的行动上放置较少的概率质量。因此，我们基本上只是让好的东西更有可能，让坏的东西得到坏的回报的可能性更小，并使“试错”的概念正式化。</p><p id="3943" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是政策梯度。这很容易与多任务学习结合起来。它也很容易与元学习结合起来。元学习算法(如MAML和黑盒方法)假设你可以得到你的目标的一些梯度。因此，我们可以很容易地将这些算法与策略梯度算法结合使用。</p><p id="0ace" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，让我们把MAML和政策梯度结合起来。有两个任务:向前跑和向后跑。我们不以任何方式评价泛化。我们只是要看看它是否能学会用一个单一的梯度步长来适应这两个任务中的一个。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi os"><img src="../Images/9911d3b1c7f4525a6cd5c1edb76088b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YXtSEoVaROGQ17f8-BuirQ.gif"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">在元学习结束时，但在对其中一个任务采取梯度步骤之前，我们得到了一个看起来正确的策略:原地运行，准备向两个方向中的任何一个方向运行。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ot"><img src="../Images/65b38d796d3f2912e9b889ef0dbf4e21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*uy0F06z0irXHm4Fb_khhJA.gif"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">如果我们对具有向后/向前运行的奖励函数的任务取一个梯度，我们得到在相应方向上运行的策略。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="b601" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的例子表明，存在一种表示，在这种表示下，RL是快速和有效的。</p><p id="3028" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以结合黑盒元学习和政策梯度。这是迷宫的例子。这个实验是学习走迷宫，它在1000个小迷宫上训练，在伸出的小迷宫和大迷宫上测试。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/e60b40c47998aef36db33b133390a6b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/1*ru367kfcGQX35LZpYKVk7Q.gif"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">在这种情况下，它不知道任务，它需要在迷宫中导航。左边是代理人的观点，右边是迷宫。在它获得经验之后，它就能够学习如何用基本上只有一条轨迹的方法来解决迷宫。所以首先在迷宫中导航探索，然后在这一集结束时，建筑的记忆不会被重置。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="08c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MAML在监督学习环境中很有表现力，但由于策略梯度，它在RL中实际上不是很有表现力。基本上，如果你所有轨迹的回报函数都是0，那么你的梯度将永远是0。因此，即使它在零回报的情况下获得了大量丰富的环境经验，它实际上也无法将这些经验融入到政策的更新中。因此，<strong class="lb iu"> <em class="np">带有政策梯度的MAML实际上并不是很有表现力。</em> </strong> <em class="np">一般来说，这些针对RL设置的元学习算法很容易与策略梯度相结合，将它们与Q-learning和actor-critic算法等方法相结合则更具挑战性</em>。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="d38c" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">汇总策略梯度</h1><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ov"><img src="../Images/ccd93955646b748a36aba9b160191345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wVRggW8w4bitxEMuyYluCg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">政策梯度。来源:<a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></p></figure><p id="5be9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">策略梯度方法简单</strong>且<strong class="lb iu">易于与现有的多任务和元学习算法</strong>相结合。但是<strong class="lb iu">它产生了一个高方差梯度，</strong>它可以用基线或信赖域来减轻。政策梯度的另一个缺点是<strong class="lb iu">它需要政策数据</strong>。你可以看到对ωθ的期望，ωθ是你当前的政策。因此，为了改进您的策略，您需要来自您当前策略的数据。这真的很重要，因为这意味着你不能重复使用以前政策中的任何数据来试图改进你的政策。结果，这些算法往往比能够重用来自先前策略和其他经验等的数据的其他算法的样本效率低。像重要性权重这样的东西可以对此有所帮助。因此，您基本上可以添加一个权重，该权重对应于您当前的策略与您用来收集数据的策略之间的比率。但是这些重要性权重也给了你很高的方差，特别是当这两个策略非常不同时。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="c9b6" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">参考</h1><ol class=""><li id="12b0" class="lv lw it lb b lc ni lf nj li ow lm ox lq oy lu oz mb mc md bi translated"><a class="ae ki" href="https://www.youtube.com/watch?v=UPT4Rndftc8&amp;list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&amp;index=6" rel="noopener ugc nofollow" target="_blank">斯坦福CS330:多任务和元学习，2019 |讲座6 —强化学习入门’</a>。</li><li id="6364" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oz mb mc md bi translated">T <a class="ae ki" href="http://cs330.stanford.edu/slides/cs330_mtrl.pdf" rel="noopener ugc nofollow" target="_blank">他航向滑梯</a></li><li id="04fe" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oz mb mc md bi translated"><a class="ae ki" href="https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf" rel="noopener ugc nofollow" target="_blank">自动驾驶汽车的端到端学习</a></li><li id="7425" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oz mb mc md bi translated"><a class="ae ki" href="https://arxiv.org/abs/1703.03400" rel="noopener ugc nofollow" target="_blank">用于深度网络快速适应的模型不可知元学习</a></li><li id="3cff" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oz mb mc md bi translated"><a class="ae ki" href="https://arxiv.org/abs/1707.03141" rel="noopener ugc nofollow" target="_blank">一个简单的神经注意力元学习者</a></li></ol></div></div>    
</body>
</html>