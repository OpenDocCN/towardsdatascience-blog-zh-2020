# NLP ä»‹ç»â€”â€”ç¬¬ 1 éƒ¨åˆ†:ç”¨ Python é¢„å¤„ç†æ–‡æœ¬

> åŸæ–‡ï¼š<https://towardsdatascience.com/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96?source=collection_archive---------19----------------------->

æ¬¢è¿æ¥åˆ° NLP å…¥é—¨ï¼è¿™æ˜¯ 5 ç¯‡ç³»åˆ—æ–‡ç« çš„ç¬¬ä¸€éƒ¨åˆ†ã€‚è¿™ç¯‡æ–‡ç« å°†å±•ç¤ºä¸€ç§é¢„å¤„ç†æ–‡æœ¬çš„æ–¹æ³•ï¼Œä½¿ç”¨ä¸€ç§å«åšå•è¯åŒ…çš„æ–¹æ³•ï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬éƒ½ç”¨å®ƒçš„å•è¯æ¥è¡¨ç¤ºï¼Œè€Œä¸ç®¡å®ƒä»¬å‡ºç°çš„é¡ºåºæˆ–åµŒå…¥çš„è¯­æ³•ã€‚é¢„å¤„ç†æ—¶ï¼Œæˆ‘ä»¬å°†å®Œæˆä»¥ä¸‹æ­¥éª¤:

1.  è±¡å¾åŒ–
2.  æ­£å¸¸åŒ–
3.  åˆ é™¤åœç”¨è¯
4.  è®¡æ•°çŸ¢é‡
5.  è½¬æ¢åˆ° tf-idf è¡¨ç¤º

ğŸ’¤è¿™äº›æœ¯è¯­å¯¹ä½ æ¥è¯´åƒæ˜¯èƒ¡è¨€ä¹±è¯­å—ï¼Ÿä¸è¦æ‹…å¿ƒï¼Œå½“ä½ è¯»å®Œè¿™ç¯‡æ–‡ç« çš„æ—¶å€™ï¼Œä»–ä»¬å·²ç»ä¸åœ¨äº†ï¼ğŸ“

![](img/7450b6d78637a26f5fe5f0d32854ecc8.png)

è¨æ³•å°”Â·è¨æ³•ç½—å¤«åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„çš„ç…§ç‰‡

# 0.Python è®¾ç½®ğŸ”§

æˆ‘å‡è®¾è¯»è€…(ğŸ‘€æ˜¯çš„ï¼Œä½ ï¼)å¯ä»¥è®¿é—®å¹¶ç†Ÿæ‚‰ Pythonï¼ŒåŒ…æ‹¬å®‰è£…åŒ…ã€å®šä¹‰å‡½æ•°å’Œå…¶ä»–åŸºæœ¬ä»»åŠ¡ã€‚å¦‚æœä½ æ˜¯ Python çš„æ–°æ‰‹ï¼Œ[è¿™ä¸ª](https://www.python.org/about/gettingstarted/)æ˜¯ä¸€ä¸ªå…¥é—¨çš„å¥½åœ°æ–¹ã€‚

æˆ‘å·²ç»ä½¿ç”¨å¹¶æµ‹è¯•äº† Python 3.7.1 ä¸­çš„è„šæœ¬ã€‚åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ç¡®ä¿ä½ æœ‰åˆé€‚çš„å·¥å…·ã€‚

## â¬œï¸ç¡®ä¿å®‰è£…äº†æ‰€éœ€çš„è½¯ä»¶åŒ…:pandasï¼Œ *nltk* & sklearn

æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å¼ºå¤§çš„ç¬¬ä¸‰æ–¹è½¯ä»¶åŒ…:

*   *ç†ŠçŒ«*:æ•°æ®åˆ†æåº“ï¼Œ
*   *nltk:* è‡ªç„¶è¯­è¨€å·¥å…·åŒ…åº“å’Œ
*   *sklearn:* æœºå™¨å­¦ä¹ åº“ã€‚

## â¬œï¸ **ä» nltk ä¸‹è½½â€œåœç”¨è¯â€å’Œâ€œwordnetâ€è¯­æ–™åº“**

ä¸‹é¢çš„è„šæœ¬å¯ä»¥å¸®åŠ©ä½ ä¸‹è½½è¿™äº›è¯­æ–™åº“ã€‚å¦‚æœæ‚¨å·²ç»ä¸‹è½½äº†ï¼Œè¿è¡Œæ­¤ç¨‹åºå°†é€šçŸ¥æ‚¨å®ƒä»¬æ˜¯æœ€æ–°çš„:

```
import nltk
nltk.download('stopwords') 
nltk.download('wordnet')
```

# 1.æ•°æ®ğŸ“¦

ä¸ºäº†ä½¿äº‹æƒ…æ˜“äºç®¡ç†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¾®å°çš„æ–‡æœ¬æ•°æ®ï¼Œè¿™å°†å…è®¸æˆ‘ä»¬ç›‘è§†æ¯ä¸€æ­¥çš„è¾“å…¥å’Œè¾“å‡ºã€‚å¯¹äºè¿™ä¸ªæ•°æ®ï¼Œæˆ‘é€‰æ‹©äº†æƒ…æ™¯å–œå‰§[è€å‹è®°](https://www.imdb.com/title/tt0108778/)ä¸­ä¹”ä¼Šä¸ºé’±å¾·å‹’å’Œè«å¦®å¡çš„å©šç¤¼å‡†å¤‡çš„æ¼”è®²ç¨¿ã€‚

ä»–çš„æ¼”è®²æ˜¯è¿™æ ·çš„:

```
part1 = """We are gathered here today on this joyous occasion to celebrate the special love that Monica and Chandler share. It is a love based on giving and receiving as well as having and sharing. And the love that they give and have is shared and received. And
through this having and giving and sharing and receiving, we too can share and love and have... and receive."""part2 = """When I think of the love these two givers and receivers share I cannot help but envy the lifetime ahead of having and loving and giving and receiving."""
```

å¦‚æœä½ æ²¡æœ‰çœ‹åˆ°æˆ‘æåˆ°çš„éƒ¨åˆ†ï¼ŒYouTube ä¸Šæœ‰ä¸€äº›çŸ­è§†é¢‘(å…³é”®è¯:ä¹”ä¼Šçš„å©šç¤¼è‡´è¾)ã€‚æˆ‘è®¤ä¸ºä¹”ä¼Šçš„è¡¨æ¼”å’Œè«å¦®å¡å’Œé’±å¾·å‹’çš„ååº”ç»å¯¹è®©è¿™ä¸ªæ¼”è®²æ¯”å•çº¯çš„æ–‡å­—æœ‰è¶£å¤šäº†ã€‚å†™è¿™ä¸ªå¸–å­ç»™äº†æˆ‘ä¸€ä¸ªå¾ˆå¥½çš„å€Ÿå£æ¥åå¤è§‚çœ‹è¿™ä¸ªåœºæ™¯ï¼Œæˆ‘æ— æ³•æ»¡è¶³å®ƒã€‚ğŸ™ˆ

# 2.æœ€ç»ˆä»£ç ğŸ“ƒ

é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç”¨åŒ…å’Œæ•°æ®å‡†å¤‡ç¯å¢ƒ:

```
# Import packages and modules
import pandas as pd
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer# Create a dataframe
X_train = pd.DataFrame([part1, part2], columns=['speech'])
```

å…¶æ¬¡ï¼Œè®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæ–‡æœ¬é¢„å¤„ç†å‡½æ•°ï¼Œå°†å…¶ä¼ é€’ç»™ *TfidfVectorizer* :

```
def preprocess_text(text):
    # Tokenise words while ignoring punctuation
    tokeniser = RegexpTokenizer(r'\w+')
    tokens = tokeniser.tokenize(text)

    # Lowercase and lemmatise 
    lemmatiser = WordNetLemmatizer()
    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]

    # Remove stopwords
    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]
    return keywords
```

æœ€åï¼Œè®©æˆ‘ä»¬åˆ©ç”¨å‰é¢å®šä¹‰çš„å‡½æ•°å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œé¢„å¤„ç†:

```
# Create an instance of TfidfVectorizer
vectoriser = TfidfVectorizer(analyzer=preprocess_text)# Fit to the data and transform to feature matrix
X_train = vectoriser.fit_transform(X_train['speech'])# Convert sparse matrix to dataframe
X_train = pd.DataFrame.sparse.from_spmatrix(X_train)# Save mapping on which index refers to which words
col_map = {v:k for k, v in vectoriser.vocabulary_.items()}# Rename each column using the mapping
for col in X_train.columns:
    X_train.rename(columns={col: col_map[col]}, inplace=True)
X_train
```

![](img/b0feb48c4d3af68149eb1173cca8d28b.png)

tf-idf çŸ©é˜µ(æœªæ˜¾ç¤ºæ‰€æœ‰åˆ—)

Ta-daâ•æˆ‘ä»¬å°†æ–‡æœ¬é¢„å¤„ç†æˆç‰¹å¾çŸ©é˜µã€‚è¿™äº›è„šæœ¬åœ¨æ²¡æœ‰ä»»ä½•è§£é‡Šçš„æƒ…å†µä¸‹æœ‰æ„ä¹‰å—ï¼Ÿè®©æˆ‘ä»¬åœ¨ä¸‹ä¸€èŠ‚é€šè¿‡ä¾‹å­æ¥åˆ†è§£å’Œç†è§£å¼€å¤´æåˆ°çš„ 5 ä¸ªæ­¥éª¤ã€‚

# 3.æœ€ç»ˆä»£ç åˆ†è§£å’Œè§£é‡ŠğŸ”

## **ç¬¬ä¸€æ­¥:æ ‡è®°åŒ–**

> *ğŸ’¡* " [ç»™å®šä¸€ä¸ªå­—ç¬¦åºåˆ—å’Œä¸€ä¸ªå®šä¹‰å¥½çš„æ–‡æ¡£å•å…ƒï¼Œæ ‡è®°åŒ–å°±æ˜¯æŠŠå®ƒåˆ†å‰²æˆå°å—çš„ä»»åŠ¡ï¼Œå«åšæ ‡è®°ï¼Œä¹Ÿè®¸åŒæ—¶æ‰”æ‰æŸäº›å­—ç¬¦ï¼Œæ¯”å¦‚æ ‡ç‚¹ç¬¦å·ã€‚](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)

åœ¨è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†æŠŠä¸€ä¸ªå­—ç¬¦ä¸² *part1* è½¬æ¢æˆä¸€ä¸ªè®°å·åˆ—è¡¨ï¼ŒåŒæ—¶å»æ‰æ ‡ç‚¹ç¬¦å·ã€‚æˆ‘ä»¬æœ‰è®¸å¤šæ–¹æ³•å¯ä»¥å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚æˆ‘å°†é€šè¿‡ä½¿ç”¨ *nltk:* ä¸­çš„ *RegexpTokenizer* å‘æ‚¨å±•ç¤ºä¸€ç§æ–¹æ³•

```
# Import module
from nltk.tokenize import RegexpTokenizer# Create an instance of RegexpTokenizer for alphanumeric tokens
tokeniser = RegexpTokenizer(r'\w+')# Tokenise 'part1' string
tokens = tokeniser.tokenize(part1)
print(tokens)
```

è®©æˆ‘ä»¬æ¥çœ‹çœ‹*ä»¤ç‰Œ*æ˜¯ä»€ä¹ˆæ ·å­çš„:

![](img/709c1842fa95995bc604b42def5775cc.png)

ä»£å¸

æˆ‘ä»¬çœ‹åˆ°æ¯ä¸ªå•è¯ç°åœ¨æ˜¯ä¸€ä¸ªå•ç‹¬çš„å­—ç¬¦ä¸²ã€‚ä½ æ³¨æ„åˆ°åŒä¸€ä¸ªè¯æœ‰ä¸åŒçš„ç”¨æ³•äº†å—ï¼Ÿä¾‹å¦‚:å•è¯çš„å¤§å°å†™å¯ä»¥ä¸åŒ:â€œandâ€å’Œâ€œandâ€æˆ–å®ƒä»¬çš„åç¼€:â€œshareâ€ã€â€œsharedâ€å’Œâ€œsharingâ€ã€‚è¿™å°±æ˜¯æ ‡å‡†åŒ–çš„ç”±æ¥ã€‚

## ç¬¬äºŒæ­¥ã€‚æ­£å¸¸åŒ–

> *ğŸ’¡* *æŠŠä¸€ä¸ªè¯è§„æ ¼åŒ–å°±æ˜¯æŠŠå®ƒè½¬æ¢æˆå®ƒçš„è¯æ ¹å½¢å¼ã€‚*

è¯å¹²åˆ†æå’Œè¯æ¡è§£é‡Šæ˜¯è§„èŒƒåŒ–æ–‡æœ¬çš„å¸¸ç”¨æ–¹æ³•ã€‚åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ lemmatisation å°†å•è¯è½¬æ¢ä¸ºå®ƒä»¬çš„å­—å…¸å½¢å¼ï¼Œå¹¶é€šè¿‡å°†æ‰€æœ‰å•è¯è½¬æ¢ä¸ºå°å†™æ¥æ¶ˆé™¤å¤§å°å†™å·®å¼‚ã€‚

ğŸ”—å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºè¯å¹²å’Œè¯å°¾çš„çŸ¥è¯†ï¼Œä½ å¯èƒ½æƒ³çœ‹çœ‹è¿™ä¸ªç³»åˆ—çš„ç¬¬äºŒéƒ¨åˆ†ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ *nltk* ä¸­çš„ *WordNetLemmatizer* æ¥å¯¹æˆ‘ä»¬çš„*ä»¤ç‰Œ*è¿›è¡Œç¬¦å·åŒ–:

```
# Import module
from nltk.stem import WordNetLemmatizer# Create an instance of WordNetLemmatizer
lemmatiser = WordNetLemmatizer()# Lowercase and lemmatise tokens
lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]
print(lemmas)
```

![](img/9d6003152a3521db85f603df0d2ac265.png)

å‰é¢˜

è¿™äº›å•è¯ç°åœ¨è¢«è½¬æ¢æˆå®ƒçš„å­—å…¸å½¢å¼ã€‚ä¾‹å¦‚ï¼Œâ€œåˆ†äº«â€ã€â€œåˆ†äº«â€å’Œâ€œå…±äº«â€ç°åœ¨éƒ½åªæ˜¯â€œåˆ†äº«â€ã€‚

```
# Check how many words we have
len(lemmas)
```

![](img/bef057f2f174be4902e69f9d90a280e0.png)

æˆ‘ä»¬æœ‰ 66 ä¸ªå•è¯ï¼Œä½†ä¸æ˜¯æ‰€æœ‰çš„å•è¯å¯¹æ–‡æœ¬æ„ä¹‰çš„è´¡çŒ®éƒ½æ˜¯ç›¸åŒçš„ã€‚æ¢å¥è¯è¯´ï¼Œæœ‰äº›è¯å¯¹å…³é”®ä¿¡æ¯ä¸æ˜¯ç‰¹åˆ«æœ‰ç”¨ã€‚è¿™å°±æ˜¯åœç”¨è¯å‡ºç°çš„åœ°æ–¹ã€‚

## ç¬¬ä¸‰æ­¥ã€‚åˆ é™¤åœç”¨è¯

> *ğŸ’¡åœç”¨è¯æ˜¯å¸¸è§çš„è¯ï¼Œå¯¹æ–‡æœ¬çš„æ„ä¹‰æ²¡æœ‰ä»€ä¹ˆä»·å€¼ã€‚*

æƒ³ä¸€æƒ³:å¦‚æœä½ å¿…é¡»ç”¨ä¸‰ä¸ªè¯å°½å¯èƒ½è¯¦ç»†åœ°æè¿°ä½ è‡ªå·±ï¼Œä½ ä¼šåŒ…æ‹¬â€œæˆ‘â€è¿˜æ˜¯â€œæˆ‘â€ï¼Ÿå¦‚æœæˆ‘è®©ä½ åœ¨ä¹”ä¼Šæ¼”è®²ä¸­çš„å…³é”®è¯ä¸‹é¢åˆ’çº¿ï¼Œä½ ä¼šåˆ’â€˜aâ€™è¿˜æ˜¯â€˜theâ€™ï¼Ÿå¤§æ¦‚ä¸ä¼šã€‚Iã€amã€a å’Œ The éƒ½æ˜¯åœç”¨è¯çš„ä¾‹å­ã€‚æˆ‘æƒ³ä½ æ˜ç™½äº†ã€‚

æ ¹æ®æ–‡æœ¬æ¶‰åŠçš„é¢†åŸŸï¼Œå¯èƒ½éœ€è¦ä¸åŒçš„åœç”¨è¯é›†ã€‚åœ¨è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ *nltk çš„åœç”¨è¯*è¯­æ–™åº“ã€‚æ‚¨å¯ä»¥å®šä¹‰è‡ªå·±çš„åœç”¨å­—è¯é›†ï¼Œæˆ–è€…é€šè¿‡æ·»åŠ é€‚åˆæ–‡æœ¬é¢†åŸŸçš„å¸¸ç”¨æœ¯è¯­æ¥ä¸°å¯Œæ ‡å‡†åœç”¨å­—è¯ã€‚

è®©æˆ‘ä»¬å…ˆç¨å¾®ç†Ÿæ‚‰ä¸€ä¸‹*çš„å¸¸ç”¨è¯*:

```
# Import module
from nltk.corpus import stopwords# Check out how many stop words there are 
print(len(stopwords.words('english')))# See first 5 stop words
stopwords.words('english')[:5]
```

![](img/b13599db8e0e889f4fbd95551b841f4b.png)

åœ¨å†™è¿™ç¯‡æ–‡ç« çš„æ—¶å€™ï¼Œnltk çš„åœç”¨è¯è¯­æ–™åº“ä¸­æœ‰ 179 ä¸ªè‹±è¯­åœç”¨è¯ã€‚ä¸€äº›ä¾‹å­åŒ…æ‹¬:â€œæˆ‘â€ã€â€œæˆ‘â€ã€â€œæˆ‘çš„â€ã€â€œæˆ‘è‡ªå·±â€ã€â€œæˆ‘ä»¬â€ã€‚å¦‚æœæ‚¨å¾ˆæƒ³çœ‹åˆ°å®Œæ•´çš„åˆ—è¡¨ï¼Œåªéœ€ä»æœ€åä¸€è¡Œä»£ç ä¸­åˆ é™¤`[:5]`ã€‚

æ³¨æ„è¿™äº›åœç”¨è¯æ˜¯å¦‚ä½•å°å†™çš„ï¼Ÿä¸ºäº†æœ‰æ•ˆåœ°åˆ é™¤åœç”¨è¯ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿æ‰€æœ‰å•è¯éƒ½æ˜¯å°å†™çš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»åœ¨ç¬¬äºŒæ­¥ä¸­è¿™æ ·åšäº†ã€‚

ä½¿ç”¨åˆ—è¡¨ç†è§£ï¼Œè®©æˆ‘ä»¬ä»åˆ—è¡¨ä¸­åˆ é™¤æ‰€æœ‰åœç”¨è¯:

```
keywords = [lemma for lemma in lemmas if lemma not in stopwords.words('english')]
print(keywords)
```

![](img/d2ed7094735272a5a290b61d0fdb318b.png)

å…³é”®è¯

```
# Check how many words we have
len(keywords)
```

å»æ‰åœç”¨è¯åï¼Œæˆ‘ä»¬åªæœ‰ 26 ä¸ªè¯ï¼Œè€Œä¸æ˜¯ 66 ä¸ªï¼Œä½†è¦ç‚¹ä»ç„¶ä¿ç•™ã€‚

ç°åœ¨ï¼Œå¦‚æœæ‚¨å‘ä¸Šæ»šåŠ¨åˆ°ç¬¬ 2 éƒ¨åˆ†(æœ€ç»ˆä»£ç )å¹¶å¿«é€ŸæŸ¥çœ‹ä¸€ä¸‹`preprocess_text`å‡½æ•°ï¼Œæ‚¨å°†ä¼šçœ‹åˆ°è¿™ä¸ªå‡½æ•°æ•è·äº†æ­¥éª¤ 1 åˆ° 3 ä¸­æ‰€ç¤ºçš„è½¬æ¢è¿‡ç¨‹ã€‚

## ç¬¬å››æ­¥ã€‚è®¡æ•°çŸ¢é‡

> *ğŸ’¡* [*Count vectorise æ˜¯å°†ä¸€ç»„æ–‡æœ¬æ–‡æ¡£è½¬æ¢æˆä¸€ä¸ª token counts çš„çŸ©é˜µ*](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) *ã€‚*

ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹æ­¥éª¤ 3 ä¸­*å…³é”®è¯*ä¸­æ¯ä¸ªå•è¯çš„è®¡æ•°:

```
{word: keywords.count(word) for word in set(keywords)}
```

![](img/c5d232d86c24e97a1619dad6daf4937c.png)

å…³é”®è¯è®¡æ•°

â€œç»™äºˆâ€è¿™ä¸ªè¯å‡ºç°äº†ä¸‰æ¬¡ï¼Œè€Œâ€œå¿«ä¹â€åªå‡ºç°äº†ä¸€æ¬¡ã€‚

è¿™å°±æ˜¯è®¡æ•°çŸ¢é‡å™¨å¯¹æ‰€æœ‰è®°å½•æ‰€åšçš„äº‹æƒ…ã€‚*è®¡æ•°çŸ¢é‡å™¨*é€šè¿‡ *n* å°†æ–‡æœ¬è½¬æ¢æˆä¸€ä¸ª *m* çš„çŸ©é˜µï¼Œå…¶ä¸­ m æ˜¯æ–‡æœ¬è®°å½•çš„æ•°é‡ï¼Œn æ˜¯æ‰€æœ‰è®°å½•ä¸­å”¯ä¸€çš„*æ ‡è®°*çš„æ•°é‡ï¼ŒçŸ©é˜µçš„å…ƒç´ æŒ‡çš„æ˜¯ç»™å®šè®°å½•çš„*æ ‡è®°*çš„è®¡æ•°ã€‚

åœ¨è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†æŠŠæ–‡æœ¬æ•°æ®å¸§è½¬æ¢æˆè®¡æ•°çŸ©é˜µã€‚æˆ‘ä»¬å°†æŠŠæˆ‘ä»¬çš„è‡ªå®šä¹‰é¢„å¤„ç†å™¨å‡½æ•°ä¼ é€’ç»™*è®¡æ•°çŸ¢é‡å™¨:*

```
# Import module
from sklearn.feature_extraction.text import CountVectorizer# Create an instance of CountfVectorizer
vectoriser = CountVectorizer(analyzer=preprocess_text)# Fit to the data and transform to feature matrix
X_train = vectoriser.fit_transform(X_train['speech'])
```

è¾“å‡ºç‰¹å¾çŸ©é˜µå°†æ˜¯ç¨€ç–çŸ©é˜µå½¢å¼ã€‚è®©æˆ‘ä»¬å°†å®ƒè½¬æ¢æˆå…·æœ‰é€‚å½“åˆ—åçš„ dataframeï¼Œä½¿å®ƒæ›´æ˜“äºé˜…è¯»:

```
# Convert sparse matrix to dataframe
X_train = pd.DataFrame.sparse.from_spmatrix(X_train)# Save mapping on which index refers to which terms
col_map = {v:k for k, v in vectoriser.vocabulary_.items()}# Rename each column using the mapping
for col in X_train.columns:
    X_train.rename(columns={col: col_map[col]}, inplace=True)
X_train
```

![](img/0106c90d53621016a0c243bf3b4715aa.png)

è®¡æ•°çŸ©é˜µ(æœªæ˜¾ç¤ºæ‰€æœ‰åˆ—)

ä¸€æ—¦æˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºæ•°æ®å¸§ï¼Œåˆ—å°†åªæ˜¯ç´¢å¼•(å³ä» 0 åˆ° n-1 çš„æ•°å­—)ï¼Œè€Œä¸æ˜¯å®é™…çš„å•è¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦é‡å‘½åè¿™äº›åˆ—ï¼Œä»¥ä¾¿äºè§£é‡Šã€‚

å½“å‘é‡å™¨é€‚åˆæ•°æ®æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä»`vectoriser.vocabulary_`ä¸­æ‰¾åˆ°å•è¯çš„ç´¢å¼•æ˜ å°„ã€‚æ­¤ç´¢å¼•æ˜ å°„çš„æ ¼å¼ä¸º{word:index}ã€‚è¦é‡å‘½ååˆ—ï¼Œæˆ‘ä»¬å¿…é¡»å°†é”®å€¼å¯¹åˆ‡æ¢åˆ°{index:word}ã€‚è¿™åœ¨ç¬¬äºŒè¡Œä»£ç ä¸­å®Œæˆï¼Œå¹¶ä¿å­˜åœ¨`col_map`ä¸­ã€‚

åœ¨ä»£ç æœ«å°¾ä½¿ç”¨ for å¾ªç¯ï¼Œæˆ‘ä»¬ä½¿ç”¨æ˜ å°„é‡å‘½åæ¯ä¸€åˆ—ï¼Œè¾“å‡ºåº”è¯¥ç±»ä¼¼äºä¸Šè¡¨ä¸­çš„å†…å®¹(ç”±äºç©ºé—´é™åˆ¶ï¼Œåªæ˜¾ç¤ºäº†éƒ¨åˆ†è¾“å‡º)ã€‚

ä»è¿™ä¸ªçŸ©é˜µä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°â€œgiveâ€åœ¨ *part1(è¡Œç´¢å¼•=0)* ä¸­è¢«æåŠ 3 æ¬¡ï¼Œåœ¨ *part2(è¡Œç´¢å¼•=1)* ä¸­è¢«æåŠ 1 æ¬¡ã€‚

åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åªæœ‰ 2 æ¡è®°å½•ï¼Œæ¯æ¡è®°å½•åªåŒ…å«å°‘é‡çš„å¥å­ï¼Œæ‰€ä»¥è®¡æ•°çŸ©é˜µéå¸¸å°ï¼Œç¨€ç–æ€§ä¹Ÿä¸é«˜ã€‚ç¨€ç–æ€§æ˜¯æŒ‡çŸ©é˜µä¸­æ‰€æœ‰å…ƒç´ ä¸­é›¶å…ƒç´ çš„æ¯”ä¾‹ã€‚å½“æ‚¨å¤„ç†åŒ…å«æ•°ç™¾ã€æ•°åƒç”šè‡³æ•°ç™¾ä¸‡æ¡è®°å½•(æ¯æ¡è®°å½•éƒ½ç”±å¯Œæ–‡æœ¬è¡¨ç¤º)çš„çœŸå®æ•°æ®æ—¶ï¼Œè®¡æ•°çŸ©é˜µå¯èƒ½ä¼šéå¸¸å¤§ï¼Œå¹¶ä¸”å¤§éƒ¨åˆ†åŒ…å« 0ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œä½¿ç”¨ç¨€ç–æ ¼å¼å¯ä»¥èŠ‚çœå­˜å‚¨å†…å­˜ï¼Œå¹¶åŠ å¿«è¿›ä¸€æ­¥çš„å¤„ç†ã€‚å› æ­¤ï¼Œåœ¨ç°å®ç”Ÿæ´»ä¸­é¢„å¤„ç†æ–‡æœ¬æ—¶ï¼Œæ‚¨å¯èƒ½å¹¶ä¸æ€»æ˜¯åƒæˆ‘ä»¬è¿™é‡Œä¸¾ä¾‹è¯´æ˜çš„é‚£æ ·å°†ç¨€ç–çŸ©é˜µè½¬æ¢æˆæ•°æ®å¸§ã€‚

## ç¬¬äº”æ­¥ã€‚è½¬æ¢åˆ° TF-IDF è¡¨ç¤º

> *ğŸ’¡* tf-idf *ä»£è¡¨è¯é¢‘é€†æ–‡æ¡£é¢‘ç‡ã€‚*

å½“è½¬æ¢ä¸º *tf-idf* è¡¨ç¤ºæ—¶ï¼Œæˆ‘ä»¬å°†è®¡æ•°è½¬æ¢ä¸ºåŠ æƒé¢‘ç‡ï¼Œå…¶ä¸­æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¸€ä¸ªåä¸º*é€†æ–‡æ¡£é¢‘ç‡*çš„æƒé‡ï¼Œå¯¹ä¸å¤ªé¢‘ç¹çš„è¯èµ‹äºˆæ›´é«˜çš„é‡è¦æ€§ï¼Œå¯¹è¾ƒé¢‘ç¹çš„è¯èµ‹äºˆè¾ƒä½çš„é‡è¦æ€§ã€‚

ğŸ”—æˆ‘å·²ç»åœ¨[ç³»åˆ—çš„ç¬¬ä¸‰éƒ¨åˆ†](https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc)ä¸­ä¸“é—¨å†™äº†ä¸€ä¸ªå•ç‹¬çš„å¸–å­æ¥è¯¦ç»†è§£é‡Šè¿™ä¸€ç‚¹ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºå®ƒåº”è¯¥æœ‰è‡ªå·±çš„ä¸€èŠ‚ã€‚

```
# Import module
from sklearn.feature_extraction.text import TfidfTransformer# Create an instance of TfidfTransformer
transformer = TfidfTransformer()# Fit to the data and transform to tf-idf
X_train = pd.DataFrame(transformer.fit_transform(X_train).toarray(), columns=X_train.columns)
X_train
```

åœ¨æœ€åä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬ç¡®ä¿è¾“å‡ºä»ç„¶æ˜¯æ­£ç¡®å‘½åçš„æ•°æ®å¸§:

![](img/b0feb48c4d3af68149eb1173cca8d28b.png)

tf-idf çŸ©é˜µ(æœªæ˜¾ç¤ºæ‰€æœ‰åˆ—)

æ—¢ç„¶æˆ‘ä»¬å·²ç»åˆ†åˆ«ç†è§£äº†ç¬¬ 4 æ­¥å’Œç¬¬ 5 æ­¥ï¼Œæˆ‘æƒ³æŒ‡å‡ºï¼Œä½¿ç”¨ *TfidfVectorizer æœ‰ä¸€ç§æ›´æœ‰æ•ˆçš„æ–¹æ³•æ¥å®Œæˆç¬¬ 4 æ­¥å’Œç¬¬ 5 æ­¥ã€‚*è¿™æ˜¯ä½¿ç”¨ä»¥ä¸‹ä»£ç å®Œæˆçš„:

```
# Import module
from sklearn.feature_extraction.text import TfidfVectorizer# Create an instance of TfidfVectorizer
vectoriser = TfidfVectorizer(analyzer=preprocess_text)# Fit to the data and transform to tf-idf
X_train = vectoriser.fit_transform(X_train['speech'])
```

è¦å°†è¿™ä¸ªç¨€ç–çŸ©é˜µè¾“å‡ºåˆ°å…·æœ‰ç›¸å…³åˆ—åçš„ dataframe ä¸­ï¼Œæ‚¨çŸ¥é“è¯¥æ€ä¹ˆåš(æç¤º:å‚è§æˆ‘ä»¬åœ¨æ­¥éª¤ 4 ä¸­æ‰€åšçš„)ã€‚

ä»‹ç»å®Œæ‰€æœ‰æ­¥éª¤åï¼Œå¦‚æœæ‚¨å†æ¬¡å›åˆ°ç¬¬ 2 éƒ¨åˆ†(æœ€ç»ˆä»£ç )ä¸­çš„è„šæœ¬ï¼Œæ˜¯å¦ä¼šæ¯”æ‚¨ç¬¬ä¸€æ¬¡çœ‹åˆ°æ—¶æ›´ç†Ÿæ‚‰ï¼ŸğŸ‘€

![](img/4e5926e65fad0fcc2c2fcda060cf19da.png)

Gabriel Beaudry åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„çš„ç…§ç‰‡

*æ‚¨æƒ³è®¿é—®æ›´å¤šè¿™æ ·çš„å†…å®¹å—ï¼Ÿåª’ä½“ä¼šå‘˜å¯ä»¥æ— é™åˆ¶åœ°è®¿é—®åª’ä½“ä¸Šçš„ä»»ä½•æ–‡ç« ã€‚å¦‚æœä½ ä½¿ç”¨* [*æˆ‘çš„æ¨èé“¾æ¥*](https://zluvsand.medium.com/membership)*æˆä¸ºä¼šå‘˜ï¼Œä½ çš„ä¸€éƒ¨åˆ†ä¼šè´¹ä¼šç›´æ¥å»æ”¯æŒæˆ‘ã€‚*

æ„Ÿè°¢æ‚¨èŠ±æ—¶é—´é˜…è¯»è¿™ç¯‡æ–‡ç« ã€‚æˆ‘å¸Œæœ›ä½ ä»é˜…è¯»å®ƒä¸­å­¦åˆ°ä¸€äº›ä¸œè¥¿ã€‚å…¶ä½™å¸–å­çš„é“¾æ¥æ•´ç†å¦‚ä¸‹:
â—¼ï¸ **ç¬¬ä¸€éƒ¨åˆ†:Python ä¸­çš„æ–‡æœ¬é¢„å¤„ç†**
â—¼ï¸ [ç¬¬äºŒéƒ¨åˆ†:å¼•ç†å’Œè¯å¹²åŒ–çš„åŒºåˆ«](https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc)
â—¼ï¸ [ç¬¬ä¸‰éƒ¨åˆ†:TF-IDF è§£é‡Š](https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc)
â—¼ï¸ [ç¬¬å››éƒ¨åˆ†:Python ä¸­çš„ç›‘ç£æ–‡æœ¬åˆ†ç±»æ¨¡å‹](https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267)
â—¼ï¸ [ç¬¬äº”éƒ¨åˆ†:Python ä¸­çš„æ— ç›‘ç£ä¸»é¢˜æ¨¡å‹(sklearn)](/introduction-to-nlp-part-5a-unsupervised-topic-model-in-python-733f76b3dc2d) ã€T29

å¿«ä¹é¢„å¤„ç†ï¼å†è§ğŸƒğŸ’¨

# 4.å‚è€ƒğŸ“

*   [Christopher D. Manningï¼ŒPrabhakar Raghavan å’Œ Hinrich SchÃ¼tzeï¼Œ*ä¿¡æ¯æ£€ç´¢å¯¼è®º*ï¼Œå‰‘æ¡¥å¤§å­¦å‡ºç‰ˆç¤¾ï¼Œ2008 å¹´](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)
*   [ä¼¯å¾·ã€å²è’‚æ–‡ã€çˆ±å¾·åÂ·æ´›ç€å’Œä¼Šä¸‡Â·å…‹è±æ©ï¼Œ*ç”¨ Python è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†*ã€‚å¥¥è±åˆ©åª’ä½“å…¬å¸ï¼Œ2009 å¹´](http://www.nltk.org/book/)
*   [*ç‰¹å¾æå–*ï¼Œsklearn æ–‡æ¡£](https://scikit-learn.org/stable/modules/feature_extraction.html)