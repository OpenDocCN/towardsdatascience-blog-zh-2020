<html>
<head>
<title>Churn Prediction using Neural Networks and ML models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用神经网络和 ML 模型的流失预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/churn-prediction-using-neural-networks-and-ml-models-c817aadb7057?source=collection_archive---------19-----------------------#2020-09-11">https://towardsdatascience.com/churn-prediction-using-neural-networks-and-ml-models-c817aadb7057?source=collection_archive---------19-----------------------#2020-09-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/f8c46a8002ae60c9992760e82c24e370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d9dRbNikQWsetmZ3"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae jd" href="https://unsplash.com/@abstraction_by_alexa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Alexa </a>拍摄的照片</p></figure><div class=""/><p id="4440" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个故事是我上传到 Kaggle 上的一个笔记本的演练。最初，它只使用机器学习模型，从那以后，我添加了几个基本的神经网络模型。许多媒体博客和 Kaggle 上的笔记本广泛报道了客户流失预测主题，然而，很少有使用神经网络的。将神经网络应用于结构化数据本身在文献中很少涉及。我通过 Coursera 上的 deeplearning.ai 专门化和 Keras 的 Tensorflow 文档学习了神经网络。</p><h1 id="0609" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">简介</strong></h1><p id="301e" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">当客户或订户停止与公司或服务做生意时，就会发生客户流失或客户流失。客户流失是一个关键指标，因为保留现有客户比获得新客户更具成本效益，因为它节省了销售和营销成本。留住客户更具成本效益，因为你已经赢得了现有客户的信任和忠诚。</p><p id="1aaa" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有多种方法可以计算这一指标，因为流失率可能代表客户流失总数、客户流失占公司客户总数的百分比、经常性业务流失的价值或经常性价值流失的百分比。但是，在这个数据集中，它被定义为每个客户的二进制变量，计算费率不是目的。因此，这里的目标是确定和量化影响流失率的因素。</p><p id="96e6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个相当简单的初级项目，变数较少。对于神经网络来说，这不是一个有用的应用，因为训练样本的数量相对较少，但是使用它很容易理解神经网络。</p><h1 id="e035" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">探索性数据分析</h1><p id="7451" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">这里跳过了数据清理步骤。丢失的值只是微小的，在总费用栏中找到，因此被删除。由于多重共线性，没有要素被删除，因为只存在少数几个要素。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi me"><img src="../Images/bd6cf4132c9f9abf973467c259387069.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*q3NJUTuoQBqJO1ktPCWiZw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">熟悉自己的功能。</p></figure><p id="aafc" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据分析的第一步是熟悉数据变量、特征和目标。该数据集包含 20 个特征和一个目标变量。客户 ID 特性是一个字符串标识，因此对预测没有用处。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/64bf95ab44f2f015958146288310f3c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*QGIMMrYULWoAxl7ITxWriA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">恰好有 3 个类的分类特征的唯一值。</p></figure><p id="cda8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在分类特征中，有些特征是二进制的，有些正好有 3 个唯一值。在检查时，注意到只有合同和互联网服务在分类特征中具有不同数量的唯一值。在 Kaggle 的一些笔记本中,“无互联网服务”类可以被指定为“否”。然而，虚拟变量似乎是更好的编码选择，因为在前一种情况下，尽管有互联网服务，但客户选择不选择服务，将会丢失数据。在特征的数量较大的情况下，将考虑标签编码或映射，因为然后一个热编码将变成大的稀疏矩阵。</p><p id="bb5a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">检查数据中提供的这些特征的分布是很重要的，以检查特征值是否公平分布。下面的函数用于绘制分布图。</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="c4ec" class="mp lc jg ml b gy mq mr l ms mt">def srt_dist(df=df,cols=cat_feats):<br/>    fig, axes = plt.subplots(8, 2,squeeze=True)<br/>    axes = axes.flatten()<br/><br/>    for i, j <strong class="ml jh">in</strong> zip(cols, axes):<br/><br/>        (df[i].value_counts()*100.0 /len(df)).plot.pie(autopct='<strong class="ml jh">%.1f%%</strong>',figsize =(10,37), fontsize =15,ax=j )                                                                      <br/>        j.yaxis.label.set_size(15)<br/>srt_dist()</span></pre><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/06143245651b748fd9887e1797e8f892.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*eiBFCK2xMH6uH8lgODRsFw.png"/></div></figure><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/bbe061bbf958c87b659c641d065994f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*W0hOWcDUEhaSaJB-Cnz1Ig.png"/></div></figure><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mw"><img src="../Images/e3fee05fffd6a0424f24cf1a3c15ca01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eqJTSLSXpaQTlKEvJ67v5w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">一些分类特征的分布</p></figure><p id="510f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">据观察，很少是老年人，只有 30%有家属，只有 10%没有电话服务。因此，从这些变量中得出的相关性是值得怀疑的。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/2e251b095b566adbd66527b5690c618b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*ffDwVe0mHgomnVkR0nsg-A.png"/></div></figure><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi my"><img src="../Images/30e7b8fa2db3aa299c391a8b77da04b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*73-gzTT_4r8993TWQcxKWQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">合同任期。</p></figure><p id="4391" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自然，月合同客户的任期低于两年合同客户。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/5a67ed06909049dde5599ce718a4c07e.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*H10LHHFu5du1QCMkpmCU0g.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">目标变量的分布。</p></figure><p id="c59e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，目标变量有 73 %的“无流失”实例。机器学习模型将因此而被扭曲，并且在更多“流失”的情况下，在看不见的数据上不会表现得那么好。</p><p id="c86a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">抵消这种类不平衡的一种方法是使用分层交叉验证，这种方法使实例的折叠具有统一的比例。</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="95ba" class="mp lc jg ml b gy mq mr l ms mt">sns.pairplot(df,vars = ['tenure','MonthlyCharges','TotalCharges'], hue="Churn")</span></pre><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi na"><img src="../Images/44cbe9974c82f882c21b620ef3689457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*G6-ChS3G1cunWpRnZURZgQ.png"/></div></figure><p id="0682" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从顶行中间图可以观察到，租期较短和月费较高的客户倾向于流失，几乎呈线性关系。可以注意到，如总费用分布所示，神经网络模型也表明总费用较低的客户会流失。但是，这可能是因为随着时间的推移，任期越长的客户总费用越多。</p><p id="a5ae" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也正如你在下面看到的；逐月合同和光纤 obtic 互联网对客户流失概率有着巨大的影响。</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="93f9" class="mp lc jg ml b gy mq mr l ms mt">cat_feats=['gender', 'SeniorCitizen', 'Partner', 'Dependents','PhoneService','MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup','DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies','Contract', 'PaperlessBilling', 'PaymentMethod'] # As formed in notebook in upper blocks<br/>fig,axes = plt.subplots(16)<br/>axes = axes.flatten()<br/>for i, j in zip(cat_feats, axes):<br/>sortd = df.groupby([i])['Churn'].median().sort_values(ascending=False)<br/>    j=sns.catplot(x=i,<br/>                y='Churn',<br/>                data=df,<br/>                kind='bar')<br/>    j.set_ylabels("Churn Probability")</span></pre><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/f940010097b0ae227ac8d68f2b036a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*fYtCxA0kFJB8zdeB_Bg0kg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">根据不同的分类特征，此处显示了合同的流失概率。</p></figure><p id="7057" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">据观察，对于高度相关的功能，如无合作伙伴或依赖关系、无技术支持、按月合同等，客户流失的可能性很高。</p><h1 id="d91a" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">系统模型化</h1><p id="66bf" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">数据集根据范围为 0 到 1 的最小最大缩放器进行缩放，训练集是根据分配的前 3993 个观察值。</p><p id="de1f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下函数用于分层交叉验证。</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="5748" class="mp lc jg ml b gy mq mr l ms mt">def stratified_cv(X, y, clf_class, shuffle=True,  **kwargs):<br/>    stratified_k_fold = StratifiedKFold().split(X,y)<br/>    y_pred = y.copy()<br/>    for ii, jj in stratified_k_fold: <br/>        Xtrain, Xtest = X.iloc[ii], X.iloc[jj]<br/>        ytrain = y.iloc[ii]<br/>        clf = clf_class(**kwargs)<br/>        clf.fit(X_train,y_train)<br/>        y_pred.iloc[jj] = clf.predict(Xtest)<br/>    return y_pred</span><span id="a569" class="mp lc jg ml b gy nb mr l ms mt">print('Gradient Boosting Classifier:\n {}\n'.format(<br/>    metrics.classification_report(y, stratified_cv(X, y,                                                  ensemble.GradientBoostingClassifier))))                                            <br/>print('Support vector machine(SVM):\n {}\n'.format(<br/>    metrics.classification_report(y, stratified_cv(X, y, svm.SVC))))<br/>print('Random Forest Classifier:\n {}\n'.format(<br/>       metrics.classification_report(y, stratified_cv(X, y,                                                  ensemble.RandomForestClassifier))))<br/>print('K Nearest Neighbor Classifier:\n {}\n'.format(<br/>       metrics.classification_report(y, stratified_cv(X, y,                                                  neighbors.KNeighborsClassifier,n_neighbors=11))))<br/>print('Logistic Regression:\n {}\n'.format(<br/>       metrics.classification_report(y, stratified_cv(X, y,                                            linear_model.LogisticRegression))))<br/>print('XGBoost Classifier:\n {}\n'.format(<br/>     metrics.classification_report(y, stratified_cv(X, y,        XGBClassifier))))</span><span id="0633" class="mp lc jg ml b gy nb mr l ms mt">Code used for classification reports of ML model</span></pre><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c976041df8bbe90b8c495acf0d89b4a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*kTve5ohQaBdDiTgjBjRGZw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">模型的加权平均 F1 分数。</p></figure><blockquote class="nd ne nf"><p id="5f6f" class="kd ke ng kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">最大似然模型的超参数调整</p></blockquote><p id="64e3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于随机森林，调整每棵树的估计器数量和最大特征数:</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="cf83" class="mp lc jg ml b gy mq mr l ms mt"># Tuning Random Forest<br/>from sklearn.ensemble import RandomForestClassifier<br/># Create param grid.   <br/>param_rf=[{'n_estimators' : list(range(10,150,15)),<br/>            'max_features' : list(range(6,32,5))}]<br/># Create grid search object<br/>clf = RandomizedSearchCV(RandomForestClassifier(), param_distributions = param_rf, n_iter=50, cv = 5, refit=True,verbose=1, n_jobs=-1,)<br/># Fit on data<br/>best_clf = clf.fit(X, y)<br/>print(best_clf.best_params_)<br/>best_clf.best_score_</span></pre><p id="0199" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Out[]:</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="f77a" class="mp lc jg ml b gy mq mr l ms mt">{'n_estimators': 130, 'max_features': 6}<br/>0.78967497404260</span></pre><p id="4b2e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于逻辑回归，调整逆正则化参数 C:</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="5a43" class="mp lc jg ml b gy mq mr l ms mt"># Tuning Logistic Regression<br/>from sklearn.linear_model import LogisticRegression<br/>param_grid = [<br/>    {'penalty' : ['l1', 'l2'],<br/>    'C' : np.logspace(-5, 5, 20),<br/>    'solver' : ['liblinear'] }]<br/>clf = RandomizedSearchCV(LogisticRegression(), param_distributions = param_grid, n_iter=20, cv = 5, refit=True,verbose=1, n_jobs=-1,)</span><span id="9c08" class="mp lc jg ml b gy nb mr l ms mt"># Fit on data</span><span id="097d" class="mp lc jg ml b gy nb mr l ms mt">best_clf = clf.fit(X, y)<br/>print(best_clf.best_params_)<br/>best_clf.best_score_</span></pre><p id="161b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Out[]:</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="242c" class="mp lc jg ml b gy mq mr l ms mt">{'solver': 'liblinear', 'penalty': 'l2', 'C': 8858.667904100832}<br/>0.8043221203472578</span></pre><h1 id="c4d8" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">神经网络</h1><p id="b980" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">对于神经网络，两种类型的建模，预制估计和 Keras 序列模型都被使用。此外，我遇到的大多数参考资料是关于卷积神经网络和图像分类的超调预制估计器。Keras 模型对学习速率和层数进行了超调。超参数调整模型显示了类似的性能，因为数据集比通常的神经网络应用程序小。</p><p id="ec06" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了使博客简洁，这里只展示了 Keras 模型。</p><p id="9668" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用一个 64–8–1 密集分层模型，学习率衰减，批量大小为 32。还使用了每一层的 L2 正则化和去除。</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="74d4" class="mp lc jg ml b gy mq mr l ms mt"><em class="ng"># Model 1</em><br/>nn_model = Sequential()<br/>nn_model.add(Dense(64,kernel_regularizer=tf.keras.regularizers.l2(0.001), input_dim=46, activation='relu' ))<br/>nn_model.add(Dropout(rate=0.2))<br/>nn_model.add(Dense(8,kernel_regularizer=tf.keras.regularizers.l2(0.001),activation='relu'))<br/>nn_model.add(Dropout(rate=0.1))<br/>nn_model.add(Dense(1, activation='sigmoid'))<br/>lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay( 0.001,<br/>      decay_steps=(X_train.shape[0]/32)*50,<br/>      decay_rate=1,<br/>      staircase=False)<br/>#This time decay means for every 50 epochs the learning rate will be half of 0.001 value<br/>def get_optimizer():<br/>    return tf.keras.optimizers.Adam(lr_schedule)<br/>def get_callbacks():<br/>    return [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=70,restore_best_weights=True)]</span><span id="6ee8" class="mp lc jg ml b gy nb mr l ms mt">nn_model.compile(loss = "binary_crossentropy", <br/>                  optimizer = get_optimizer(), <br/>                  metrics=['accuracy'])<br/>    <br/><br/>history = nn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=32,   callbacks=get_callbacks(),verbose=0)<br/><br/>plt.plot(history.history['accuracy']) <br/>plt.plot(history.history['val_accuracy']) <br/>plt.title('model accuracy') <br/>plt.ylabel('accuracy')<br/>plt.xlabel('epoch') <br/>plt.legend(['train', 'test'], loc='upper left') <br/>plt.show()</span></pre><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/a83ae1c0149adb02ce1c640f43f5bffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*ke2R3BVHwA7iCk5oX3NL0g.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">根据被训练的时期的训练和准确性。</p></figure><p id="1b6a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，由于批量较小，该模型在 20 个时期左右收敛得更快。测试集上的模型准确率为 80.72%。</p><blockquote class="nd ne nf"><p id="57df" class="kd ke ng kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">估价</p></blockquote><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="a8d4" class="mp lc jg ml b gy mq mr l ms mt">yprednn=nn_model.predict(X_test)<br/>yprednn=yprednn.round()<br/>print('Neural Network:<strong class="ml jh">\n</strong> <strong class="ml jh">{}\n</strong>'.format(<br/>    metrics.classification_report(yprednn, y_test)))<br/>nn_conf_matrix=metrics.confusion_matrix(yprednn,y_test)<br/>conf_mat_nn = pd.DataFrame(nn_conf_matrix, <br/>    columns=["Predicted NO", "Predicted YES"], <br/>    index=["Actual NO", "Actual YES"])<br/>print(conf_mat_nn)</span></pre><p id="4c5a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Out[]:</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="5b46" class="mp lc jg ml b gy mq mr l ms mt">Neural Network:<br/>               precision    recall  f1-score   support<br/>         0.0       0.92      0.84      0.87      2443<br/>         1.0       0.51      0.69      0.58       596</span><span id="19f3" class="mp lc jg ml b gy nb mr l ms mt">    accuracy                           0.81      3039<br/>   macro avg       0.71      0.76      0.73      3039<br/>weighted avg       0.84      0.81      0.82      3039<br/>Confusion Matrix :<br/>            Predicted NO  Predicted YES<br/>Actual NO           2042            401<br/>Actual YES           185            411</span></pre><blockquote class="nd ne nf"><p id="40a1" class="kd ke ng kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">使用 Keras 进行超参数调谐</p></blockquote><p id="982d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Keras tuner 的文档对此做了很好的解释。这里隐藏单元的数量、隐藏层中神经元的数量、学习率和辍学率都是超调的。</p><p id="41bd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据吴恩达的课程，学习率是最重要的，其次是动量贝塔、小批量和隐藏单元的数量。</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="eef0" class="mp lc jg ml b gy mq mr l ms mt">from tensorflow import keras<br/>from tensorflow.keras import layers<br/>from kerastuner.tuners import RandomSearch<br/>import IPython<br/>import kerastuner as kt<br/>def build_model(hp):<br/>    inputs = tf.keras.Input(46,)<br/>    x = inputs<br/>    for i <strong class="ml jh">in</strong> range(hp.Int('num_layers', 1,3)):<br/>        x =  tf.keras.layers.Dense(units=hp.Int('units_' + str(i),32,256, step=32, default=64),<br/>             kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)<br/>        x = tf.keras.layers.BatchNormalization()(x)<br/>        x = tf.keras.layers.ReLU()(x)<br/>    x = tf.keras.layers.Dense(<br/>      hp.Int('hidden_size', 4,64, step=4, default=8),<br/>             kernel_regularizer=tf.keras.regularizers.l2(0.001),<br/>             activation='relu')(x)<br/>    x = tf.keras.layers.Dropout(<br/>      hp.Float('dropout', 0, 0.5, step=0.1, default=0.5))(x)<br/>    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)</span><span id="8907" class="mp lc jg ml b gy nb mr l ms mt">model = tf.keras.Model(inputs, outputs)<br/>    model.compile(<br/>    optimizer=tf.keras.optimizers.Adam(<br/>      hp.Float('learning_rate', 1e-3,1e-1, sampling='log')),<br/>    loss="binary_crossentropy", <br/>    metrics=['accuracy'])<br/>    return model<br/><br/>tuner = RandomSearch(<br/>    build_model,<br/>    objective='val_accuracy',<br/>    max_trials=10,<br/>    executions_per_trial=1)<br/>batch_size=32<br/>tuner.search(X_train, y_train,<br/>                     epochs=100,batch_size=batch_size,<br/>                     validation_data=(X_test,y_test),<br/>                     callbacks= [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',                                                               patience=40,restore_best_weights=True)],verbose=False)<br/>best_hp = tuner.get_best_hyperparameters()[0] <br/>best_hp.values</span></pre><p id="9112" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Out[]:</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="d53f" class="mp lc jg ml b gy mq mr l ms mt">{'num_layers': 3,<br/> 'units_0': 96,<br/> 'hidden_size': 52,<br/> 'dropout': 0.5,<br/> 'learning_rate': 0.0075386035028952945,<br/> 'units_1': 64,<br/> 'units_2': 96}</span></pre><p id="992d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">超调建议采用 5 层模型，前三个单元如输出所示。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/8e7f8560f820ec3d15eacd12d27d5157.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*yJiZUPughE-6_FBBJL-Z2g.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">超调模型的历史图输出。</p></figure><p id="8b35" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该图显示了一个复杂的模型可以有很大的差异。该模型的性能类似，但由于复杂性而略低。</p><p id="bae7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">超调对于小数据集可能没有大数据集有用。然而，在 Keras 文档的过拟合和欠拟合部分中，得出的结论是，随着神经网络的容量“变小”,精度会提高。因此，由于这是一个类似大小的数据集，我们也可以使用最多具有 64 到 128 个隐藏单元的 3 个隐藏层的网络。</p><p id="8835" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">测试不同的批量大小:小于 32 和大于 32 都会导致性能稍低。因此，此代码块可以跳过，但可以在笔记本中找到。具有 10e3 观测值的中小型数据集通常使用的批量大小为 8、32、64、128。8 的倍数使得批处理大小适合内存，并且运行更快。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/c39ad712952bb954551b1e78ecdec666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*-QxtERj8KRuD7myYGl_D2A.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">批量大小为 1024 需要更长的时间来收敛，超过 100 个历元。</p></figure><h1 id="5b5a" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">模型性能</h1><p id="a189" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">混淆矩阵和 ROC 曲线给出了真正的正负准确性的意义，然而在不平衡的数据集中，正是精确-召回曲线给出了准确性的意义。在这个数据集中，负实例比正实例多，因此精确召回曲线显示了真实的性能。</p><p id="4e64" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ROC 可能过于乐观，因为如果模型正确地预测了负面实例，但在正面实例上失败，而精确召回曲线是基于正面实例的，则 ROC 会更乐观。</p><p id="48e3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">混淆矩阵值显示为百分比，因为神经网络模型使用一个集合验证，而不是 CV。XGBoost 性能与随机森林非常相似，因此这里没有显示。</p><blockquote class="nd ne nf"><p id="538e" class="kd ke ng kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">1.随机森林性能</p></blockquote><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="e564" class="mp lc jg ml b gy mq mr l ms mt">rf_conf_matrix  = metrics.confusion_matrix(y, stratified_cv(X, y, ensemble.RandomForestClassifier,n_estimators=113))<br/>conf_mat_rf = pd.DataFrame(rf_conf_matrix, <br/>    columns=["Predicted NO", "Predicted YES"], <br/>    index=["Actual NO", "Actual YES"])<br/>print((conf_mat_rf/7032)*100)<br/>cv=StratifiedKFold(n_splits=6)<br/>classifier=RandomForestClassifier(n_estimators=113)<br/>from sklearn.metrics import auc<br/>from sklearn.metrics import plot_roc_curve<br/>from sklearn.model_selection import StratifiedKFold<br/>tprs=[]<br/>aucs=[]<br/>mean_fpr=np.linspace(0,1,100)<br/>fig,ax=plt.subplots()<br/>for i,(train,test) <strong class="ml jh">in</strong> enumerate(cv.split(X,y)):<br/>    classifier.fit(X.iloc[train],y.iloc[train])<br/>viz=plot_roc_curve(classifier,X.iloc[test],y.iloc[test],name='ROC fold <strong class="ml jh">{}</strong>'.format(i),alpha=0.3,lw=1,ax=ax)<br/>    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)<br/>    interp_tpr[0] = 0.0<br/>    tprs.append(interp_tpr)<br/>    aucs.append(viz.roc_auc)<br/><br/>ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',<br/>        label='Chance', alpha=.8)<br/>mean_tpr = np.mean(tprs, axis=0)<br/>mean_tpr[-1] = 1.0<br/>mean_auc = auc(mean_fpr, mean_tpr)<br/>std_auc = np.std(aucs)<br/>ax.plot(mean_fpr, mean_tpr, color='b',label=r'Mean ROC (AUC = <strong class="ml jh">%0.2f</strong> $\pm$ <strong class="ml jh">%0.2f</strong>)' % (mean_auc, std_auc),lw=2, alpha=.8)<br/>std_tpr = np.std(tprs, axis=0)<br/>tprs_upper = np.minimum(mean_tpr + std_tpr, 1)<br/>tprs_lower = np.maximum(mean_tpr - std_tpr, 0)<br/>ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,label=r'$\pm$ 1 std. dev.')<br/>ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],<br/>       title="Receiver operating characteristic example")<br/>ax.legend(loc="lower right")<br/>plt.show()<br/># Precision Recall # break code block</span><span id="ddf9" class="mp lc jg ml b gy nb mr l ms mt">rfmodel=RandomForestClassifier(n_estimators= 130, max_features= 6,n_jobs=-1)<br/>rfmodel.fit(X_train,y_train)<br/>lg_probs = rfmodel.predict_proba(X_test)<br/>lg_probs=lg_probs[:,1]<br/>yhat = rfmodel.predict(X_test)<br/>lr_precision, lr_recall, _ = precision_recall_curve(y_test,lg_probs)<br/>lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)<br/><em class="ng"># summarize scores</em><br/>print('RF: f1=<strong class="ml jh">%.3f</strong> auc=<strong class="ml jh">%.3f</strong>' % (lr_f1, lr_auc))<br/><em class="ng"># plot the precision-recall curves</em><br/>no_skill = len(y_test[y_test==1]) / len(y_test)<br/>pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')<br/>pyplot.plot(lr_recall, lr_precision, marker='.', label='RF')<br/><em class="ng"># axis labels</em><br/>pyplot.xlabel('Recall')<br/>pyplot.ylabel('Precision')<br/><em class="ng"># show the legend</em><br/>pyplot.legend()<br/><em class="ng"># show the plot</em><br/>pyplot.show()</span><span id="c8d1" class="mp lc jg ml b gy nb mr l ms mt">Out[]:<br/>Confusion Matrix:<br/> Predicted NO  Predicted YES<br/>Actual NO      70.036974       3.384528<br/>Actual YES      6.143345      20.435154<br/>Precision- Recall:<br/>RF: f1=0.543 auc=0.603</span></pre><div class="mf mg mh mi gt ab cb"><figure class="nn is no np nq nr ns paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/95f871b0329f78e245c85f55782593f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*0QNS2l78LR4bDWjM9NNDuQ.jpeg"/></div></figure><figure class="nn is nt np nq nr ns paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/4f0029ca8db9a96ce80012eca8a1a561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*mCbDHol43X_BWHTbeU7DJw.jpeg"/></div></figure></div><blockquote class="nd ne nf"><p id="8c40" class="kd ke ng kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">2.逻辑回归</p></blockquote><p id="fc40" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代码与上面类似。</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="a519" class="mp lc jg ml b gy mq mr l ms mt">Confusion matrix:<br/>Predicted NO  Predicted YES<br/>Actual NO      65.799204       7.622298<br/>Actual YES     12.044937      14.533561<br/>Precision-Recall AUC:<br/>Logistic: f1=0.543 auc=0.640</span></pre><div class="mf mg mh mi gt ab cb"><figure class="nn is nu np nq nr ns paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/6cfdddc570d8f5425b13c49d0ef692e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*8yUvOj1TrKljUucRZ62vqQ.jpeg"/></div></figure><figure class="nn is nv np nq nr ns paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/f533bc6535c5e8cbb6162cc56bfd7297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*vXUSSNVJuT5AR0Riojr4tA.jpeg"/></div></figure></div><blockquote class="nd ne nf"><p id="7aa2" class="kd ke ng kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">3.神经网络模型性能</p></blockquote><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="4082" class="mp lc jg ml b gy mq mr l ms mt">Confusion Matrix:<br/>Predicted NO  Predicted YES<br/>Actual NO      67.193156      13.195130<br/>Actual YES      6.087529      13.524186<br/>ROC:<br/>No Skill: ROC AUC=0.500<br/>Neural Network: ROC AUC=0.832<br/>Precision-Recall:<br/>Neural Network: f1=0.584 auc=0.628</span></pre><div class="mf mg mh mi gt ab cb"><figure class="nn is nw np nq nr ns paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/2d2113a139ce1ed6b658b148a60fa865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*-8F27LeggN4C_u6IPBg5xA.jpeg"/></div></figure><figure class="nn is nx np nq nr ns paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/9714e838e79188d0e8f0a2d9bf95252d.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*-BkDnQ4e75ndbYxBN7jy-Q.jpeg"/></div></figure></div><p id="55dd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到，Random Forest 和 XGBoost 是最准确的模型，逻辑回归概括得最好，并且同样准确地预测了流失和不流失这两个类别。因此，根据精确召回曲线，逻辑回归具有最佳性能。神经网络在精确召回率上也比 RF 和 XGBoost 表现得更好。</p><p id="cf3c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，如果在看不见的数据中存在更多积极的实例，即流失标签，则逻辑回归将预测得更好。</p><h1 id="350d" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">4.特征重要性</h1><blockquote class="nd ne nf"><p id="be66" class="kd ke ng kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">1)根据逻辑回归的特征重要性。</p></blockquote><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="2125" class="mp lc jg ml b gy mq mr l ms mt">weights = pd.Series(lgmodel.coef_[0],index=X.columns.values)<br/>print (weights.sort_values(ascending = False)[:20].plot(kind='bar'))</span></pre><div class="mf mg mh mi gt ab cb"><figure class="nn is ny np nq nr ns paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/2fbf7f4f621821ef0fe7b308f3a5e1e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*cxS-VKgv51HuUSkpDaTmlg.jpeg"/></div></figure><figure class="nn is nz np nq nr ns paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/4f5881453b19d689d71a46fae7ee7b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*_Jq-LB96fWh-ozlv_aRTNw.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk oa di ob oc translated">逻辑回归模型使用的正负权重</p></figure></div><blockquote class="nd ne nf"><p id="54d4" class="kd ke ng kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">2)根据随机森林的特征重要性</p></blockquote><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="95f4" class="mp lc jg ml b gy mq mr l ms mt">rf =  ensemble.RandomForestClassifier(n_estimators=130,max_features=6, n_jobs=-1)<br/>rf.fit(X, y)<br/>feature_importance = rf.feature_importances_<br/>feat_importances = pd.Series(rf.feature_importances_, index=X.columns)<br/>feat_importances = feat_importances.nlargest(19)<br/>feat_importances.plot(kind='barh' , figsize=(10,10))</span></pre><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/602b4509c1739647ef0a248724eb5db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4GFWWJjB_Lum-lpJl1CZIw.png"/></div></div></figure><blockquote class="nd ne nf"><p id="6618" class="kd ke ng kf b kg kh ki kj kk kl km kn nh kp kq kr ni kt ku kv nj kx ky kz la ij bi translated">3)神经网络特征重要性</p></blockquote><p id="14c9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为 Keras 在文档中没有提供特性重要性特性，所以我演示了两种方法。参考是一个堆栈流答案。</p><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="32f4" class="mp lc jg ml b gy mq mr l ms mt">from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor<br/>import eli5<br/>from eli5.sklearn import PermutationImportance<br/><br/>def base_model():<br/>    nn_model = Sequential()    nn_model.add(Dense(64,kernel_regularizer=tf.keras.regularizers.l2(0.001),<br/>                input_dim=46, activation='relu' ))<br/>    nn_model.add(Dropout(rate=0.2))<br/>nn_model.add(Dense(8,kernel_regularizer=tf.keras.regularizers.l2(0.001),<br/>                    activation='relu'))<br/>    nn_model.add(Dropout(rate=0.1))<br/>    nn_model.add(Dense(1, activation='sigmoid'))<br/>    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(<br/>                  0.001,<br/>                  decay_steps=(X_train.shape[0]/32)*50,<br/>                  decay_rate=1,<br/>                  staircase=False)<br/><br/>    def get_optimizer():<br/>        return tf.keras.optimizers.Adam(lr_schedule)<br/>    def get_callbacks():<br/>        return [<br/>            tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=70,restore_best_weights=True)]<br/>    nn_model.compile(loss = "binary_crossentropy", <br/>                  optimizer = get_optimizer(), <br/>                  metrics=['accuracy'])<br/>    return nn_model<br/><br/><br/>my_model = KerasRegressor(build_fn=base_model)    <br/>my_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=32,<br/>                    callbacks= get_callbacks(),verbose=0)<br/><br/>perm = PermutationImportance(my_model, random_state=1).fit(X[:500].values,y[:500].values,verbose=False)<br/>eli5.show_weights(perm, feature_names = X.columns.tolist())</span></pre><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/13c460aea2cf96f953782624da2d9535.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*EUAsOkz-dONrUCRQtcb1lw.png"/></div></figure><pre class="mf mg mh mi gt mk ml mm mn aw mo bi"><span id="7db3" class="mp lc jg ml b gy mq mr l ms mt">import shap<br/>from tensorflow.keras import Sequential<br/><em class="ng"># load JS visualization code to notebook</em><br/>shap.initjs()<br/><br/><em class="ng"># explain the model's predictions using SHAP</em><br/><em class="ng"># (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)</em><br/>explainer = shap.DeepExplainer(nn_model,data=X[:500].values)<br/>shap_values = explainer.shap_values(X.values)<br/><br/><em class="ng"># visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)</em><br/><em class="ng">#shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])</em><br/><br/>shap.summary_plot(shap_values, X, plot_type="bar")</span></pre><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi of"><img src="../Images/7518ed675b79515f429a7dbff62fe742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oV-ZVLrm8qPRN7HzoY7O1A.png"/></div></div></figure><h1 id="e1d1" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><ol class=""><li id="a572" class="og oh jg kf b kg lz kk ma ko oi ks oj kw ok la ol om on oo bi translated">可以看出，总费用是最重要的功能，理所当然。如果客户发现服务昂贵或负担不起，他们会“流失”的首要原因。</li><li id="8bf2" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">任期也很重要，那些长期使用这项服务或签有长期合同的客户一般来说更便宜，不太可能流失。有趣的是观察到任期被列为更重要的神经网络模型。</li><li id="22be" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">正如在 EDA 中观察到的，大多数月合同的客户更有可能流失。可以假设，原因是由于客户的个人原因，对长期合同有所保留，或每月合同导致单位时间成本较高。</li><li id="a74c" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">正如 EDA 中所看到的，其他重要的功能是在线安全、电子支付方式、光纤互联网服务、技术支持。</li><li id="0ece" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">不重要的功能是性别、家属、伴侣、流媒体电视、备份和设备保护。</li></ol><p id="67bd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">优惠和提高流失率:</p><ol class=""><li id="be55" class="og oh jg kf b kg kh kk kl ko ou ks ov kw ow la ol om on oo bi translated">折扣:由于最重要的特征是总费用，其次是月费用，通过建模确定的潜在客户应在下个月或几个月的合同中获得巨大折扣。这涵盖了 80 %的搅动原因。对于这个模型，应该最小化假阴性率或者最大化召回率，以便将折扣发送给最大的潜在顾客。</li><li id="c06e" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">新合同:应执行六个月或四个月的合同。这将鼓励想要短期合同的保留客户，并增加他们在服务中的任期，从而使他们不太可能流失。</li><li id="2abe" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">在线安全:这项服务应该得到更多的推广，并根据公司的成本在试用期内免费提供。没有在线安全的客户更有可能流失，因此此优惠可以与第一个提到的优惠相结合，折扣只能在此基础上提供。</li><li id="f9fa" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">光纤:光纤互联网的成本很高，因此要么向适当的目标受众推广，要么采用更好的技术来降低这项服务的成本。最终，市场研究团队必须决定这项服务的盈亏平衡点，它的利润是否与它造成的客户流失一样多。</li></ol><p id="aa22" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一种量化报价的方法是使用手动生成的特征及其对模型的影响。</p><h1 id="c1d2" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">参考</h1><ol class=""><li id="0502" class="og oh jg kf b kg lz kk ma ko oi ks oj kw ok la ol om on oo bi translated">预测客户流失的随机森林与神经网络，Abhinav Sagar，Medium，<a class="ae jd" rel="noopener" target="_blank" href="/random-forest-vs-neural-networks-for-predicting-customer-churn-691666c7431e">https://towards data science . com/random-Forest-vs-Neural-Networks-for-Predicting-Customer-Churn-691666 c 7431 e</a></li><li id="8c80" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">电信客户流失预测，<a class="ae jd" href="https://www.kaggle.com/pavanraj159" rel="noopener ugc nofollow" target="_blank"> Pavan Raj </a>，<a class="ae jd" href="https://www.kaggle.com/pavanraj159/telecom-customer-churn-prediction" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/pavanraj 159/telecom-Customer-Churn-Prediction</a></li><li id="698f" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">电信客户流失预测，Melda Dede，<a class="ae jd" href="https://www.kaggle.com/meldadede/churn-prediction-of-telco-customers" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/meldadede/turn-Prediction-of-Telco-Customers</a></li><li id="1a5d" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">使用基本神经网络预测流失，Laurier Mantel，<a class="ae jd" href="https://www.kaggle.com/lauriermantel/using-basic-neural-networks-to-predict-churn" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/Laurier Mantel/using-basic-neural-networks-to-predict-churn</a></li><li id="66f2" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">如何在 Python 中使用 ROC 曲线和精度召回曲线进行分类，Jason Brownlee，<a class="ae jd" href="https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/ROC-Curves-and-Precision-Recall-Curves-for-class ification-in-Python/</a></li><li id="c0ae" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">Tensorflow、Keras、sklearn 文档</li><li id="9f95" class="og oh jg kf b kg op kk oq ko or ks os kw ot la ol om on oo bi translated">神经网络的特征重要性，StackOverFlow，<a class="ae jd" href="https://stackoverflow.com/questions/45361559/feature-importance-chart-in-neural-network-using-keras-in-python#:~:text=It%20most%20easily%20works%20with,using%20it%20is%20very%20straightforward.&amp;text=At%20the%20moment%20Keras%20doesn,to%20extract%20the%20feature%20importance" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/45361559/feature-importance-chart-in-Neural-Network-using-keras-in-python #:~:text = It % 20 most % 20 easily % 20 works % 20 with，using % 20it % 20is % 20very %直截了当。&amp; text=At%20the%2 </a></li></ol></div></div>    
</body>
</html>