<html>
<head>
<title>Evolution of Extreme Learning Machines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">极限学习机的进化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evolution-of-extreme-learning-machines-2c7caf08e76b?source=collection_archive---------46-----------------------#2020-07-23">https://towardsdatascience.com/evolution-of-extreme-learning-machines-2c7caf08e76b?source=collection_archive---------46-----------------------#2020-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0455" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">理解 ML</h2><div class=""/><div class=""><h2 id="1900" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">这些年来榆树是如何进化的，现在它们的地位如何？</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/4602b76879f18fca4e7cbf78e3ae8910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*M7KOSXXstESciFhm.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="le">多层结构。来源:</em> <a class="ae lf" href="https://pdfs.semanticscholar.org/8df9/c71f09eb0dabf5adf17bee0f6b36190b52b2.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="le">用榆树进行大数据的具象学习</em> </a></p></figure><blockquote class="lg lh li"><p id="3a98" class="lj lk ll lm b ln lo ka lp lq lr kd ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><em class="iq">注意！这只是榆树进化的一个主要概述。它并没有包括所有可能的版本和这些年来对 ELMs 所做的调整。</em></p></blockquote><h1 id="a263" class="mg mh iq bd mi mj mk ml mm mn mo mp mq kf mr kg ms ki mt kj mu kl mv km mw mx bi translated">榆树是什么？</h1><p id="f070" class="pw-post-body-paragraph lj lk iq lm b ln my ka lp lq mz kd ls na nb lv lw nc nd lz ma ne nf md me mf ij bi translated">ELM(极限学习机)是前馈神经网络。于 2006 年由<em class="ll"> G. Huang </em>发明，它基于逆矩阵近似的思想。</p><p id="7727" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated">如果你不熟悉 ELMs，请先看看我的文章“<a class="ae lf" rel="noopener" target="_blank" href="/introduction-to-extreme-learning-machines-c020020ff82b">极限学习机简介</a>”。</p><h1 id="8c6a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq kf mr kg ms ki mt kj mu kl mv km mw mx bi translated">进化是什么时候开始的？</h1><h2 id="956d" class="ng mh iq bd mi nh ni dn mm nj nk dp mq na nl nm ms nc nn no mu ne np nq mw iw bi translated"><a class="ae lf" href="https://www.researchgate.net/profile/Chee_Siew/publication/6928613_Universal_Approximation_Using_Incremental_Constructive_Feedforward_Networks_With_Random_Hidden_Nodes/links/00b4952f8672bc0621000000.pdf" rel="noopener ugc nofollow" target="_blank">我-ELM (2006 年)</a></h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/2f9117886026a3e2b0abfac93c40a7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/0*CwTJj_LZQe2qtsle.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="le">工字形榆树结构。来源:</em> <a class="ae lf" href="https://www.researchgate.net/publication/341365884_An_improved_algorithm_for_incremental_extreme_learning_machine" rel="noopener ugc nofollow" target="_blank"> <em class="le">增量式极限学习机的一种改进算法</em> </a></p></figure><p id="ae28" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated">在 2006 年的原始论文发表后，黄和他的同事们发表了另一篇关于不同类型的 ELM 的论文，称为<a class="ae lf" href="https://www.researchgate.net/profile/Chee_Siew/publication/6928613_Universal_Approximation_Using_Incremental_Constructive_Feedforward_Networks_With_Random_Hidden_Nodes/links/00b4952f8672bc0621000000.pdf" rel="noopener ugc nofollow" target="_blank"> I-ELM </a>(增量 ELM)。顾名思义，I-ELM 是标准 ELM 网络的增量版本。I-ELM 的想法很简单:</p><p id="55c9" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated">从 l=0 开始定义最大隐节点数 l 和期望训练精度<em class="ll">ϵ</em>(l 为当前隐节点数):</p><ul class=""><li id="0587" class="ns nt iq lm b ln lo lq lr na nu nc nv ne nw mf nx ny nz oa bi translated">增量 l_t = l_{t-1} + 1</li><li id="fad8" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">随机初始化新添加的隐藏神经元的权重 w_l 和偏置 b_l(不要重新初始化已经存在的神经元)</li><li id="4f3e" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">计算输出向量 H</li><li id="c5ce" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">计算权重向量<em class="ll"> β </em> ^</li><li id="de4c" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">添加节点后计算错误</li><li id="77f3" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">检查 E &lt;<em class="ll"> ϵ </em></li><li id="c79f" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">如果没有，则增加隐藏节点的数量，并重复该过程。</li></ul><p id="ed1d" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated">有一种可能是，l &gt; L 在过程中的某一点和<em class="ll"> E </em> &gt; <em class="ll"> ϵ </em>。此时，我们应该重复训练和初始化的整个过程。</p><p id="0c9f" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated">增加网络大小的想法并不新鲜，通常比“手动”设置网络大小产生更好的结果。就 ELMs 而言，有一个缺点特别重要，那就是计算时间。如果你的网络碰巧很大(假设有 1000 个隐藏节点)，在更糟糕的情况下，我们必须进行 1000 次矩阵求逆。</p><p id="1a42" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated">如果您对 I-ELM 感兴趣，您应该知道它有许多变体:</p><ul class=""><li id="ed0b" class="ns nt iq lm b ln lo lq lr na nu nc nv ne nw mf nx ny nz oa bi translated">II-ELM(改进的 I-ELM)</li><li id="4cdb" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">CI-ELM(凸 I-ELM)</li><li id="81fa" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">EI-ELM(增强型 I-ELM)</li></ul><p id="c517" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated">我不打算一一解释，因为这篇文章应该只是一个快速总结和一个起点，而不是关于榆树所有变种的整本书。除此之外，可能每个阅读这篇文章的人都在这里，如果他/她知道要找什么，他/她知道如何找到关于一个有趣主题的更多信息:P</p><h2 id="78eb" class="ng mh iq bd mi nh ni dn mm nj nk dp mq na nl nm ms nc nn no mu ne np nq mw iw bi translated"><a class="ae lf" href="https://www.researchgate.net/publication/222429523_A_fast_pruned-extreme_learning_machine_for_classification_problem" rel="noopener ugc nofollow" target="_blank"> P-ELM (2008 年)</a></h2><p id="6386" class="pw-post-body-paragraph lj lk iq lm b ln my ka lp lq mz kd ls na nb lv lw nc nd lz ma ne nf md me mf ij bi translated">在引入 ELM 的增量版本之后，另一个改进是使用修剪来实现网络的最佳结构。P-ELM(修剪榆树)于 2008 年由荣海军引进。该算法从一个非常大的网络开始，并删除与预测无关的节点。“不相关”是指节点不参与预测输出值(即输出值接近 0)。这个想法能够产生更小的分类器，并且最适合模式分类。</p><h2 id="4943" class="ng mh iq bd mi nh ni dn mm nj nk dp mq na nl nm ms nc nn no mu ne np nq mw iw bi translated"><a class="ae lf" href="https://www.researchgate.net/publication/26665344_Error_Minimized_Extreme_Learning_Machine_With_Growth_of_Hidden_Nodes_and_Incremental_Learning" rel="noopener ugc nofollow" target="_blank">埃姆-埃尔姆(2009 年)</a></h2><p id="7046" class="pw-post-body-paragraph lj lk iq lm b ln my ka lp lq mz kd ls na nb lv lw nc nd lz ma ne nf md me mf ij bi translated">这个版本的 ELM 不是一个独立的版本，而是对 I-ELM 的改进。EM 代表误差最小化，允许添加一组节点而不是一个节点。这些节点被随机插入网络，直到误差不低于<em class="ll"> ϵ </em>。</p><h2 id="4180" class="ng mh iq bd mi nh ni dn mm nj nk dp mq na nl nm ms nc nn no mu ne np nq mw iw bi translated"><a class="ae lf" href="https://www.researchgate.net/publication/224453283_Regularized_Extreme_Learning_Machine" rel="noopener ugc nofollow" target="_blank">规则化榆树(2009) </a></h2><p id="dc12" class="pw-post-body-paragraph lj lk iq lm b ln my ka lp lq mz kd ls na nb lv lw nc nd lz ma ne nf md me mf ij bi translated">从 2009 年开始，郑研究了 ELM 的稳定性和推广性能。他和他的团队想出了在计算<em class="ll"> β </em> ^的原始公式中加入正则化的想法。</p><p id="f635" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated">现在看起来像是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/d163df64a718ffcfd0fae29a677141fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*4KX6LhvSwTmVFjV8pdqEKg.png"/></div></figure><h2 id="c4e1" class="ng mh iq bd mi nh ni dn mm nj nk dp mq na nl nm ms nc nn no mu ne np nq mw iw bi translated"><a class="ae lf" href="https://dl.acm.org/doi/10.1016/j.neucom.2010.07.012" rel="noopener ugc nofollow" target="_blank"> TS-ELM (2010 年)</a></h2><p id="a5f6" class="pw-post-body-paragraph lj lk iq lm b ln my ka lp lq mz kd ls na nb lv lw nc nd lz ma ne nf md me mf ij bi translated">两级 ELM (TS-ELM)是一个再次最小化网络结构的建议。顾名思义，它包括两个阶段:</p><ol class=""><li id="456f" class="ns nt iq lm b ln lo lq lr na nu nc nv ne nw mf oh ny nz oa bi translated">应用前向递归算法从每一步随机产生的候选节点中选择隐节点。添加隐藏节点，直到满足停止标准。</li><li id="5c2e" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf oh ny nz oa bi translated">对现有结构的审查。即使我们用最少数量的节点创建了一个网络来满足我们的标准，其中一些节点可能不再有用。在这一阶段，我们将删除不重要的节点。</li></ol><h2 id="9541" class="ng mh iq bd mi nh ni dn mm nj nk dp mq na nl nm ms nc nn no mu ne np nq mw iw bi translated"><a class="ae lf" href="https://dl.acm.org/doi/10.1145/2851613.2851882" rel="noopener ugc nofollow" target="_blank">凯尔姆(2010) </a></h2><p id="ee22" class="pw-post-body-paragraph lj lk iq lm b ln my ka lp lq mz kd ls na nb lv lw nc nd lz ma ne nf md me mf ij bi translated">引入了基于内核的 ELM (KELM ),它使用内核函数来代替<em class="ll"> H^T H </em>。这个想法是受 SVM 的启发，ELMs 使用的主要核函数是 RBF ( <a class="ae lf" href="https://en.wikipedia.org/wiki/Radial_basis_function" rel="noopener ugc nofollow" target="_blank">径向基函数</a>)。凯尔姆被用来设计深榆树。</p><h2 id="4aae" class="ng mh iq bd mi nh ni dn mm nj nk dp mq na nl nm ms nc nn no mu ne np nq mw iw bi translated"><a class="ae lf" href="https://www.researchgate.net/publication/220313291_Voting_based_extreme_learning_machine" rel="noopener ugc nofollow" target="_blank"> V-ELM (2012) </a></h2><p id="de50" class="pw-post-body-paragraph lj lk iq lm b ln my ka lp lq mz kd ls na nb lv lw nc nd lz ma ne nf md me mf ij bi translated">基于投票的 ELM (V-ELM)是在 2012 年提出的，旨在提高分类任务的性能。问题是 ELM 的标准训练过程可能无法达到分类的最佳边界，然后随机添加节点。因此，靠近该边界的一些样本可能会被错误分类。在 V-ELM 中，我们不是只训练一个网络，而是训练许多网络，然后基于多数投票法，选择最佳网络。</p><h2 id="8b0e" class="ng mh iq bd mi nh ni dn mm nj nk dp mq na nl nm ms nc nn no mu ne np nq mw iw bi translated"><a class="ae lf" href="https://pdfs.semanticscholar.org/8df9/c71f09eb0dabf5adf17bee0f6b36190b52b2.pdf" rel="noopener ugc nofollow" target="_blank"> ELM-AE (2013) </a></h2><p id="88cb" class="pw-post-body-paragraph lj lk iq lm b ln my ka lp lq mz kd ls na nb lv lw nc nd lz ma ne nf md me mf ij bi translated">当 2013 年像<a class="ae lf" href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" rel="noopener ugc nofollow" target="_blank"> RBM </a>和<a class="ae lf" href="https://en.wikipedia.org/wiki/Autoencoder" rel="noopener ugc nofollow" target="_blank">自动编码器</a>这样的想法开始流行时，卡斯努发表了一篇关于 ELM-AE (ELM 自动编码器)的论文。主要目标是能够再现输入向量，就像标准的自动编码器一样。ELM-AE 的结构看起来与标准 ELM 相同</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/3257793e43f0f7ef9b1ac84cf29c91f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*57VD1A4QITRsRlU3.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="le">榆树-AE 结构。来源:</em> <a class="ae lf" href="https://pdfs.semanticscholar.org/8df9/c71f09eb0dabf5adf17bee0f6b36190b52b2.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="le">大数据用 ELMs 的具象学习</em> </a></p></figure><p id="146f" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated">有三种类型的 ELM-AE:</p><ul class=""><li id="6bc1" class="ns nt iq lm b ln lo lq lr na nu nc nv ne nw mf nx ny nz oa bi translated">压缩。高维输入空间到低维隐藏层(比输入少的隐藏节点)。</li><li id="fead" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">平等代表权。数据维度保持不变(隐藏和输入中的节点数量相同)</li><li id="8e09" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">稀疏化。高维隐藏层的低维输入空间(比输入更多的隐藏节点)</li></ul><p id="e184" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated">标准 ELMs 和 ELM-AE 有两个主要区别。第一个是 ELM-AE 是无人监管的。作为输出，我们使用与输入相同的向量。第二件事是 ELM-AE 中的权重是正交的，隐藏层中的偏差也是如此。这很重要，因为 ELM-AE 用于创建 ELMs 的深度版本。</p><h2 id="7430" class="ng mh iq bd mi nh ni dn mm nj nk dp mq na nl nm ms nc nn no mu ne np nq mw iw bi translated"><a class="ae lf" href="https://pdfs.semanticscholar.org/8df9/c71f09eb0dabf5adf17bee0f6b36190b52b2.pdf" rel="noopener ugc nofollow" target="_blank"> MLELM (2013) </a></h2><p id="a144" class="pw-post-body-paragraph lj lk iq lm b ln my ka lp lq mz kd ls na nb lv lw nc nd lz ma ne nf md me mf ij bi translated">在同一篇论文(使用 ELMs 进行大数据的表征学习)中，Kasnu 提出了一种称为多层 ELM 的 ELM 版本。这个想法基于堆叠式自动编码器，由多个 ELM-AE 组成。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/5ee8603edc8ca5a6eba96fbba0b66c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dxFIYU7Re5lGqhDh.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="le">多层结构。来源:</em> <a class="ae lf" href="https://pdfs.semanticscholar.org/8df9/c71f09eb0dabf5adf17bee0f6b36190b52b2.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="le">用榆树进行大数据的具象学习</em> </a></p></figure><p id="6401" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated">你可能会问“为什么还要用 ELMs 创建类似于堆栈式自动编码器的东西呢？”。如果我们看看 MLELM 是如何工作的，我们会发现它不需要微调。这使得它比标准的自动编码器网络构建起来要快得多。就像我说过的，MLELM 使用 ELM-AE 来训练每个层中的参数，并删除输出层，因此我们只剩下 ELM-AEs 的输入层和隐藏层。</p><h2 id="936f" class="ng mh iq bd mi nh ni dn mm nj nk dp mq na nl nm ms nc nn no mu ne np nq mw iw bi translated"><a class="ae lf" href="https://www.researchgate.net/publication/277881335_Deep_Extreme_Learning_Machine_and_Its_Application_in_EEG_Classification" rel="noopener ugc nofollow" target="_blank"> DELM (2015) </a></h2><p id="e368" class="pw-post-body-paragraph lj lk iq lm b ln my ka lp lq mz kd ls na nb lv lw nc nd lz ma ne nf md me mf ij bi translated">Deep ELM 是最新的(也是在撰写本文时 ELM 发展的最后一次主要迭代)之一。delm 基于 MLELMs 的思想，使用 KELM 作为输出层。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/0f00c12e4c36c0a61aa9d245ede5d816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S3KP42Tr-xu0kuGU.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="le"> DELM 结构。来源:</em> <a class="ae lf" href="https://www.researchgate.net/publication/277881335_Deep_Extreme_Learning_Machine_and_Its_Application_in_EEG_Classification" rel="noopener ugc nofollow" target="_blank"> <em class="le">深度极限学习机及其在脑电分类中的应用。</em>T15】</a></p></figure><h1 id="9d46" class="mg mh iq bd mi mj mk ml mm mn mo mp mq kf mr kg ms ki mt kj mu kl mv km mw mx bi translated">结论</h1><p id="08ca" class="pw-post-body-paragraph lj lk iq lm b ln my ka lp lq mz kd ls na nb lv lw nc nd lz ma ne nf md me mf ij bi translated">这些年来，ELMs 一直在进化，并且肯定抄袭了机器学习领域的一些主要思想。其中一些想法真的很棒，在设计现实生活中的模型时会很有用。你应该记得那只是对榆树领域所发生的事情的一个简短的总结，而不是一个完整的回顾(甚至不接近)。如果您在 ELM 之前键入某个前缀，很可能已经有一个版本的 ELM 带有该前缀:)</p><h1 id="d81b" class="mg mh iq bd mi mj mk ml mm mn mo mp mq kf mr kg ms ki mt kj mu kl mv km mw mx bi translated">参考资料:</h1><ul class=""><li id="c6d0" class="ns nt iq lm b ln my lq mz na ok nc ol ne om mf nx ny nz oa bi translated">光-黄斌，秦-朱钰，徐志敬。《极限学习机:理论与应用》，2006 年<a class="ae lf" href="https://www.ntu.edu.sg/home/egbhuang/pdf/ELM-NC-2006.pdf" rel="noopener ugc nofollow" target="_blank">出版</a></li><li id="d8dc" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">光、雷震、萧启庆。“使用带有随机隐藏节点的增量构造前馈网络的通用逼近”，2006 年<a class="ae lf" href="https://www.researchgate.net/profile/Chee_Siew/publication/6928613_Universal_Approximation_Using_Incremental_Constructive_Feedforward_Networks_With_Random_Hidden_Nodes/links/00b4952f8672bc0621000000.pdf" rel="noopener ugc nofollow" target="_blank">出版物</a></li><li id="b34c" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">荣，王海俊，姚谭，阿惠，朱，泽轩。(2008).一种用于分类问题的快速剪枝极端学习机。神经计算。<a class="ae lf" href="https://www.researchgate.net/publication/222429523_A_fast_pruned-extreme_learning_machine_for_classification_problem" rel="noopener ugc nofollow" target="_blank">出版</a></li><li id="0daf" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">冯、、黄、林广斌、庆平、盖伊、罗伯特。(2009).隐节点增长和增量学习的误差最小化极限学习机。<a class="ae lf" href="https://www.researchgate.net/publication/26665344_Error_Minimized_Extreme_Learning_Machine_With_Growth_of_Hidden_Nodes_and_Incremental_Learning" rel="noopener ugc nofollow" target="_blank">出版</a></li><li id="6f6c" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">、邓、郑、清华、陈、林。(2009).正则化极限学习机。<a class="ae lf" href="https://www.researchgate.net/publication/224453283_Regularized_Extreme_Learning_Machine" rel="noopener ugc nofollow" target="_blank">出版</a></li><li id="328f" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">蓝，苏，黄广彬(2010)。用于回归的两阶段极限学习机。<a class="ae lf" href="https://dl.acm.org/doi/10.1016/j.neucom.2010.07.012" rel="noopener ugc nofollow" target="_blank">出版</a></li><li id="cf5a" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">萧、和。2016.一种用于回归的极限学习机优化方法。<a class="ae lf" href="https://dl.acm.org/doi/10.1145/2851613.2851882" rel="noopener ugc nofollow" target="_blank">出版</a></li><li id="d72d" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">曹，九文，林，志平，黄，刘广斌，南。(2012).基于投票的极限学习机。<a class="ae lf" href="https://www.researchgate.net/publication/220313291_Voting_based_extreme_learning_machine" rel="noopener ugc nofollow" target="_blank">出版</a></li><li id="2dd9" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">卡森，李亚娜阿拉奇&amp;周，洪明&amp;黄，光斌&amp;冯，智民。(2013).面向大数据的 ELMs 表示学习。<a class="ae lf" href="https://pdfs.semanticscholar.org/8df9/c71f09eb0dabf5adf17bee0f6b36190b52b2.pdf" rel="noopener ugc nofollow" target="_blank">出版</a></li><li id="c781" class="ns nt iq lm b ln ob lq oc na od nc oe ne of mf nx ny nz oa bi translated">丁、、张、南、徐、新郑、郭、丽丽、张、简。(2015).深度极限学习机及其在脑电分类中的应用。<a class="ae lf" href="https://www.researchgate.net/publication/277881335_Deep_Extreme_Learning_Machine_and_Its_Application_in_EEG_Classification" rel="noopener ugc nofollow" target="_blank">出版</a></li></ul></div><div class="ab cl on oo hu op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="ij ik il im in"><p id="7231" class="pw-post-body-paragraph lj lk iq lm b ln lo ka lp lq lr kd ls na lu lv lw nc ly lz ma ne mc md me mf ij bi translated"><em class="ll">最初发布于</em><a class="ae lf" href="https://erdem.pl/2020/07/evolution-of-extreme-learning-machines" rel="noopener ugc nofollow" target="_blank"><em class="ll">https://erdem . pl</em></a><em class="ll">。</em></p></div></div>    
</body>
</html>