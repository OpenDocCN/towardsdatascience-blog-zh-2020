<html>
<head>
<title>Neural Style Transfer using VGG model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于VGG模型的神经风格迁移</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-style-transfer-using-vgg-model-ff0f9757aafc?source=collection_archive---------18-----------------------#2020-01-16">https://towardsdatascience.com/neural-style-transfer-using-vgg-model-ff0f9757aafc?source=collection_archive---------18-----------------------#2020-01-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="20fd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种采用不同图像风格的数字图像变换技术</h2></div><p id="f36e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">简介</strong>:</p><p id="19c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们开始之前，让我们去<a class="ae le" href="https://deepart.io/" rel="noopener ugc nofollow" target="_blank">这个</a>网站获取一些灵感。在网站上，我们从本地计算机中选择一张照片(假设图片名为Joey.jpg)。我们姑且称之为内容图像。然后，我们选择另一个图像，说风格图像命名为style1.jpg从本地计算机。该网站所做的是生成一个混合图像，它保留了内容图像的轮廓，并将样式图像的纹理和颜色模式添加到内容图像中。以下是结果。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/43df01ff7ef4a1ea58821296d6ec1e92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yNbhRIReKMFZ7Nf2xfLyQA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">左:原始图像，右:风格图像，中间:混合图像</p></figure><p id="df84" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">描述</strong>:</p><p id="b7c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这被称为神经风格转移(NST)，通过使用深度学习，具体来说是卷积神经网络(CNN)来完成。我想你对CNN很熟悉。如果没有，我会强烈推荐CNN<a class="ae le" href="https://www.coursera.org/learn/convolutional-neural-networks?" rel="noopener ugc nofollow" target="_blank">的吴恩达课程。</a></p><p id="c75e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们借助下面的流程图来理解NST的基础知识。它显示了具有13个卷积层的样式转移算法(为简单起见，仅显示了几个)。两个图像被输入到神经网络，即内容图像和风格图像。我们的目的是生成一个混合图像，它包含内容图像的轮廓和样式图像的纹理、颜色模式。我们通过优化几个损失函数来做到这一点。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lv"><img src="../Images/6fd1729845799195c26508e216207d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VhvzxTyAcK8f7pEs5TsHJA.png"/></div></div></figure><p id="f2d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">内容图像的损失函数最小化在一层或多层上为对应于混合图像(最初只是逐渐改善的噪声图像)的内容图像激活的特征的差异。这将内容图像的轮廓保留到合成的混合图像中。</p><p id="4367" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">而风格图像的损失函数最小化了风格图像和混合图像之间的所谓Gram矩阵之间的差异。这是在一层或多层完成的。Gram矩阵的用途是识别在给定的层上哪些特征被同时激活。然后，我们模仿同样的行为，将其应用于混合图像。</p><p id="8afc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用TensorFlow，我们将内容和风格图像的这些组合损失函数的梯度更新到令人满意的水平。Gram矩阵的某些计算，存储效率的中间值，图像去噪的损失函数，归一化组合损失函数，使得两个图像相对于彼此缩放。</p><p id="fda4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">编码</strong>:</p><p id="e313" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经理解了算法，让我们开始编码。<a class="ae le" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">原纸</a>用的是VGG-19型号。但是这里我们将使用公开的VGG-16模型。从<a class="ae le" href="https://s3.amazonaws.com/cadl/models/vgg16.tfmodel" rel="noopener ugc nofollow" target="_blank">这里</a>下载VGG-16模型(请记住是~550MB文件)。</p><p id="8fc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在根目录下，新建一个文件夹，命名为<strong class="kk iu"> vgg16 </strong>，将上面的文件和Github链接中的vgg.py粘贴过来。此外，我们通过注释掉<strong class="kk iu"> maybe_download </strong>函数修改了vgg16.py文件(因为您已经下载了vgg16.tfmodel文件)</p><p id="c7cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们先导入库。然后导入vgg16型号。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lw"><img src="../Images/2527ad9c05d9517abe0c8b06934ad051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Kq53wt7yi_8FNCXUVqCaw.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lx"><img src="../Images/7ac1910fdbffe1e21fc682af74d2cd0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eGrsvv6YOZ6UXVbvm1g-g.png"/></div></div></figure><p id="0e98" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们为图像操作定义几个辅助函数。</p><p id="718f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> load_image </strong>加载一个图像并返回一个numpy浮点数组。图像被调整到最大高度或宽度。</p><p id="4f2f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> save_image </strong>将图像保存为像素值在0 ad 255之间的jpeg文件</p><p id="322a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> plot_image_big </strong>绘制更大的图像。</p><p id="c6a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> plot_images </strong>绘制内容、样式和混合图像。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ly"><img src="../Images/3a818e49e0e655153a28c689ed85b78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2DI8XANfx_j3hqosmGajQ.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lz"><img src="../Images/7cf6860b21b15936d3593a99de87ace4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f96Y_5FD0kaaVNnhiPSSGw.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ma"><img src="../Images/2988346b00c8388ae6d1e00e04e701bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oAvfbAy5umroY5zQEfQpjg.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mb"><img src="../Images/de1a313bfba6fb07582145a110dc3fe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l1dgVLG1lkrLczF5xlBCrQ.png"/></div></div></figure><p id="2a75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们将定义在Tensorflow中用于优化的损失函数。</p><p id="b70c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> mean_squared_error </strong>运算将返回一个张量，即两个输入张量之间的均方误差(MSE)差。</p><p id="b2f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> create_content_loss </strong>将计算内容和混合图像之间的MSE。损失被最小化，使得内容的激活特征与混合图像相似，从而将轮廓从内容图像转移到混合图像。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mc"><img src="../Images/6760f4174ba53536cfad008b5a9b7109.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w16jSrT9q6wXa16Bxc4Oig.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi md"><img src="../Images/268f83ef82f93dd5e068b75bb1785d26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8yMnzyxZQ1g57ka-SuC8sQ.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi me"><img src="../Images/7d171b4acbfe2f808d02557350dee7f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MwIwDUXd4KOvkbJmWeTybA.png"/></div></div></figure><p id="0888" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们的动机是捕捉混合图像的风格特征。为了做到这一点，我们将做一些类似的事情，即测量哪些功能同时激活的风格层，并复制这个模式到混合图像。一种有效的方法是计算Gram矩阵。Gram矩阵本质上是特征激活层向量的点积。如果矩阵中的条目具有较小的值，则意味着给定层中的两个特征不会同时激活，反之亦然。</p><p id="9040" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先将计算文法矩阵定义为<strong class="kk iu"> gram_matrix </strong>，然后是<strong class="kk iu"> create_style_loss </strong>，它计算gram矩阵的MSE，而不是两个原始张量(正如我们在<strong class="kk iu"> create_content_loss </strong>中所做的)。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mf"><img src="../Images/8038555428c8cb07aece59aa72085707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9wSjW_NlI64-SSYRBzbQ2A.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi me"><img src="../Images/a68fe550dce47979ba1811fb038b7b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fZHNpWX0G5DG6jwU6M20og.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mg"><img src="../Images/6b6ddf03a93e4da35557fe930de2d482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldPUz0RvBBY3NiM4HKJ--A.png"/></div></div></figure><p id="2ec9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了减少合成混合图像中的噪声，我们使用一种称为“全变差去噪”的去噪滤波算法，使用以下代码。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi md"><img src="../Images/dcf1fac760405987245eaab60bb6fcdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lxPZOK7fbEa0x5HadIWIcA.png"/></div></div></figure><p id="8787" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下一段代码是核心。我们将定义计算损失函数梯度下降的风格转移算法。该算法使用归一化，使得损失值等于一，这有助于选择独立于内容和样式层的损失权重。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mh"><img src="../Images/3a560c8c7e5a9fc84ea80574840f4335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VAlIQ1M9OcFVFqs0HE93NQ.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi md"><img src="../Images/a33565e428c4ea4dc914cec39eb58ee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o4YXCOvoWMR-XjqiUQagVg.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mi"><img src="../Images/6055efa017cdfe40e84a1d3c5be0b5de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CL8xk8VMw7VhH9KHN22hYg.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mi"><img src="../Images/2b10f0e62cc45a6dee4b650125c2481b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E0vgFGiJSAHjXESWRxpHGA.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi md"><img src="../Images/3c6b1819562a0d945a3feeb6cf684cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mo-DzubAddH72B629Y1LTA.png"/></div></div></figure><p id="59e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们将载入我们希望包含在混合图像中的具有轮廓特征的内容图像。在这里，我把它命名为1.jpg</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi md"><img src="../Images/2bffe58a4c0a6826cc19e4d0d80bd62e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ahqnzet4H4rxp__ByltYBw.png"/></div></div></figure><p id="2877" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将加载我们希望其颜色和纹理出现在混合图像中的样式图像。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi md"><img src="../Images/9102b7ef4b7234ce76391cd3893b0290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dC5LHBTcajSwe9mAiLyM3g.png"/></div></div></figure><p id="6355" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，我们将定义我们想要匹配内容图像的层索引。通常是刚开始几层。VGG-16中的第5层(指数为4)似乎工作良好。类似地，样式图像的层索引。通常，它位于总图层的末尾。这里我们将定义第13层。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mg"><img src="../Images/8245befce4421b699527a941015d0583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mAb8ChlgEt_fLa5kyPucqQ.png"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi md"><img src="../Images/cf8a8fd1fe32790583d694b001c403a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRnwcMx7MyFWUvkeo4FbIQ.png"/></div></div></figure><p id="1b95" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">执行的最后一部分是将样式转换应用于我们定义的内容和样式图像。根据定义，它自动为内容和样式层创建损失函数，并根据迭代次数执行优化。最后，显示混合图像。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mj"><img src="../Images/c3c627e46f41f87df38fb2ff3855f41f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LZzgI8nxYj-lmOQOBYCdwA.png"/></div></div></figure><p id="0767" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结果</strong>:</p><p id="23c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个过程可能在CPU上运行缓慢(我用的是CPU)。看看给出什么结果。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/871aae360c74b7c29a82ce786c193c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*VXACDfBm5d1W8gqNbPo5Vg.png"/></div></figure><p id="d9ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结论</strong>:</p><p id="ef24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还不错！结果证明了组合两个图像的基本思想。他们并不像这些技术的先驱<a class="ae le" href="https://deepart.io/" rel="noopener ugc nofollow" target="_blank">迪帕克</a>那样不相上下。也许更多的迭代、更小的步长、更高分辨率的图像、变化的样式和内容层索引或者更高的计算能力会提高混合图像的质量。</p><p id="3340" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您朗读这篇文章。希望有帮助。</p><p id="1fce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在<a class="ae le" href="https://github.com/darshanadakane/neuralStyleTransfer_usingVGG" rel="noopener ugc nofollow" target="_blank">这个</a>链接找到github repo。</p><p id="1d4d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">参考</strong>:</p><p id="1bfe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1] Hvass-Labs，TensorFlow教程#15 Style Transfer (2018)，<a class="ae le" href="https://www.youtube.com/watch?v=LoePx3QC5Js&amp;t=268s" rel="noopener ugc nofollow" target="_blank">来源</a></p></div></div>    
</body>
</html>