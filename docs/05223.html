<html>
<head>
<title>Build A Voice-Controlled Mouse In 5 minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在5分钟内制作一个声控鼠标</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-a-voice-controlled-mouse-keyboard-in-5-minutes-952bc8f101fc?source=collection_archive---------19-----------------------#2020-05-04">https://towardsdatascience.com/build-a-voice-controlled-mouse-keyboard-in-5-minutes-952bc8f101fc?source=collection_archive---------19-----------------------#2020-05-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0850" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python中语音识别和GUI自动化的初学者指南</h2></div><p id="c479" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个故事中，我们将使用Python构建一个应用程序，它将接受来自用户的语音命令，并使用鼠标和键盘执行某些基于GUI的操作。你可以把它想象成你自己的语音数字助理。它可以播放媒体，打开应用程序，发送电子邮件，移动鼠标指针等等，所有这些都由你的语音命令触发。我们将通过使用以下两个python库来完成—<a class="ae le" href="https://pypi.org/project/PyAutoGUI/" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"><em class="lf"/></strong></a>&amp;<a class="ae le" href="https://pypi.org/project/SpeechRecognition/" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"><em class="lf">Speech _ Recognition</em></strong></a>。你只需要一台安装了Python的笔记本电脑和一个麦克风。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/6e40cd8bb2894bfeeb20a8ffd015117b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bY6Hld2GYeqoXibt"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@designmesk?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Marek Levák </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="87f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在详细介绍如何在本演示中使用上述工具之前，让我们看看它们是如何适应我们的应用程序设计的。</p><p id="0947" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图非常简单明了。用户将语音命令输入麦克风(内置或外置)，然后使用<em class="lf">语音识别</em>模块将其转换为相应的文本。然后，转换后的文本被映射到使用<em class="lf"> PyAutoGUI </em>模块执行的某些GUI动作(鼠标/键盘事件)。为了方便起见，我画了下图。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi md"><img src="../Images/bd5b22f19a921df9d1144ee562ab4629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*Bhysh5rjUZtF-qhK86Rtwg.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">端到端流程的流程图</p></figure><p id="7d8c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们更详细地看看这两个库！</p><h1 id="a3f3" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated"><strong class="ak"> 1。演讲人认可:</strong></h1><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mw"><img src="../Images/919da86203ad485b47d1676b4c76ff66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_qOcSImemE1-exnOHyQwXg.jpeg"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">照片由<a class="ae le" href="https://www.pexels.com/@freestocks?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">freestocks.org</a>从<a class="ae le" href="https://www.pexels.com/photo/black-microphone-64057/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a>拍摄</p></figure><p id="ecd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">语音识别特征在家庭自动化系统、语言学习、数字助理等领域具有各种应用。使用这个库，我们可以把我们的讲话转换成文本。它支持几种API(在线和离线)。在我们的例子中，我们将使用在线google API。您可能会注意到在语音到文本的转换过程中有时会有一些延迟，但是，在这个模块支持的其他API中，我发现Google API在我的情况下是最准确的。可以使用以下pip命令下载它。使用麦克风需要pyaudio。</p><pre class="lh li lj lk gt mx my mz na aw nb bi"><span id="c3b5" class="nc mf it my b gy nd ne l nf ng">pip install SpeechRecognition<br/>pip install pyaudio</span></pre><p id="c0e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的代码片段中，我们正在初始化recognizer对象，监听来自麦克风的输入语音，并使用“recognize_google”函数将其转换为文本。我们只在用<strong class="kk iu"><em class="lf"/></strong>粗体字母标记的行中做了所有这些。对recognizer . adjust _ for _ ambient _ noise()函数的调用是可选的。该功能监听输入声音，并根据房间内的噪音量改变<strong class="kk iu"> <em class="lf"> energy_threshold </em> </strong>值。它可以被认为是识别器灵敏度的度量。该值取决于环境噪声，较高的值通常意味着不太敏感。该参数的范围可以是50–4000之间的任何值。</p><pre class="lh li lj lk gt mx my mz na aw nb bi"><span id="c3f9" class="nc mf it my b gy nd ne l nf ng"><strong class="my iu">import speech_recognition</strong></span><span id="163e" class="nc mf it my b gy nh ne l nf ng"><strong class="my iu">recognizer = speech_recognition.Recognizer()</strong></span><span id="6de0" class="nc mf it my b gy nh ne l nf ng"><strong class="my iu">with speech_recognition.Microphone() as src:</strong><br/>    try:<br/>        <strong class="my iu">audio = recognizer.adjust_for_ambient_noise(src)</strong><br/>        print("Threshold Value After calibration:" + str(recognizer.energy_threshold))<br/>        print("Please speak:")<br/>        <strong class="my iu">audio = recognizer.listen(src)</strong><br/>        <strong class="my iu">speech_to_txt = recognizer.recognize_google(audio)</strong>.lower()<br/>        print(speech_to_txt)<br/>    except Exception as ex:<br/>        print("Sorry. Could not understand.")</span></pre><p id="ae5a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">比方说，代替麦克风，您的输入是一个音频文件(file.wav)，行号3可以替换如下:</p><pre class="lh li lj lk gt mx my mz na aw nb bi"><span id="47c6" class="nc mf it my b gy nd ne l nf ng">with speech_recognition.WavFile("file.wav") as src:</span></pre></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="b89e" class="me mf it bd mg mh ni mj mk ml nj mn mo jz nk ka mq kc nl kd ms kf nm kg mu mv bi translated">2.PyAutoGUI:</h1><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nn"><img src="../Images/a745b54dbf1d64acab706eed9cc13495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WxDjsGTmf4m9ks6B"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@kaitlynbaker?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯特琳·贝克</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="24be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Python为<strong class="kk iu"> GUI自动化</strong>提供了一个名为<a class="ae le" href="https://pyautogui.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"><em class="lf">PyAutoGUI</em></strong></a><strong class="kk iu"><em class="lf"/></strong>的库，可以模拟鼠标点击和按键，就像人类用户正在执行它们一样。例如，模拟鼠标移动，击键，截屏，最大化窗口，有很多事情可以做。完整列表可以参考<a class="ae le" href="https://pyautogui.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="lf">官方文档链接</em> </a>。为了这个演示的目的，我使用了像鼠标移动、鼠标点击、键盘按压&amp;在屏幕上查找图像这样的功能。</p><pre class="lh li lj lk gt mx my mz na aw nb bi"><span id="486a" class="nc mf it my b gy nd ne l nf ng">pip install PyAutoGUI</span></pre><p id="935a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">鼠标控制:</strong>要在屏幕上移动鼠标光标，我们需要(x，y)坐标。将显示器想象成一个具有x，y坐标的<a class="ae le" href="https://pyautogui.readthedocs.io/en/latest/mouse.html" rel="noopener ugc nofollow" target="_blank">二维平面</a>(如此处所示)，其中左上角是(0，0)。向右移动，x的值增加，而向底部移动，y的值增加。函数的作用是:返回屏幕的尺寸。</p><p id="aa43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以检查您的屏幕尺寸，如下所示:</p><pre class="lh li lj lk gt mx my mz na aw nb bi"><span id="cefd" class="nc mf it my b gy nd ne l nf ng">&gt;&gt;&gt; import pyautogui<br/>&gt;&gt;&gt; pyautogui.size()<br/>Size(width=1920, height=1080)</span></pre><p id="86b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的命令将鼠标指针从当前位置移动到(100，100)位置，需要0.25秒。下一个命令模拟双击，两次单击之间的间隔为0.25秒。</p><pre class="lh li lj lk gt mx my mz na aw nb bi"><span id="d435" class="nc mf it my b gy nd ne l nf ng">pyautogui.<strong class="my iu">moveRel</strong>(100, 100, duration=0.25)<br/>pyautogui.<strong class="my iu">click</strong>(button='left', clicks=2, interval=0.25)</span></pre><p id="157f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">键盘控制:</strong>为了模拟按键，使用了以下函数。它按下多媒体“静音”按钮。所有支持的键的完整列表可以在<a class="ae le" href="https://pyautogui.readthedocs.io/en/latest/keyboard.html#the-hotkey-function" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="lh li lj lk gt mx my mz na aw nb bi"><span id="cdc7" class="nc mf it my b gy nd ne l nf ng">pyautogui.<strong class="my iu">typewrite</strong>(['volumemute'])</span></pre><p id="9489" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">定位功能:</strong>有时我们并不知道想要点击的点(比如一个按钮)的(x，y)坐标。但是如果我们将按钮的图片存储为图像文件，pyautogui.locateOnScreen()函数可以逐个像素地查找该图像并返回坐标。关于locateOnScreen()特性需要注意的一点是，即使单个像素不匹配，它也无法检测图像并返回一个<em class="lf"> None </em>对象。然后，我们可以简单地将鼠标移动到给定的位置并执行单击。例如:-我把Chrome应用程序固定在我的windows电脑的任务栏上。我已经采取了图标截图(Chrome.PNG)如下:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi no"><img src="../Images/4f8bcceea557b0acd896507031fcb028.png" data-original-src="https://miro.medium.com/v2/resize:fit:148/format:webp/1*GEfHIMMYOYJFeGsGp35XFw.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">我的Windows任务栏中Chrome图标的屏幕截图</p></figure><p id="2d3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，locateOnScreen()函数的用法如下</p><pre class="lh li lj lk gt mx my mz na aw nb bi"><span id="b3f4" class="nc mf it my b gy nd ne l nf ng">icon_location = <strong class="my iu">pyautogui.locateOnScreen</strong>(r'screenshots\Chrome.PNG')<br/>print(icon_location)<br/>&gt;&gt;&gt; Box(left=446, top=1023, width=74, height=52)</span><span id="598c" class="nc mf it my b gy nh ne l nf ng">#Using the left &amp; top values as x &amp; y to click at that location<br/><strong class="my iu">pyautogui.click(x=446, y=1023, duration=0.25)</strong></span></pre><p id="dd5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们只需要知道这些。现在，剩下的唯一事情就是将文本映射到GUI动作。这是我选择的贴图。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi np"><img src="../Images/3c58cec50bd1395f27c2f762b1057e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zqNA28Fz6xPvxujwWhLphA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">我的命令的屏幕截图</p></figure><p id="04f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我附上了最终应用的演示视频和代码上传<a class="ae le" href="https://github.com/arindomjit/Voice_Controlled_Mouse" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">这里</strong> </a> <strong class="kk iu"> </strong>供参考。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="nq nr l"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">功能演示</p></figure><p id="ee45" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您碰巧阅读了我以前的一篇关于使用神经网络  进行手势识别的文章，我打算添加PyAutoGUI功能作为补充部分，以便根据预测的手势采取行动，这些动作基于这里PyAutoGUI部分讨论的相同原则。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="c179" class="me mf it bd mg mh ni mj mk ml nj mn mo jz nk ka mq kc nl kd ms kf nm kg mu mv bi translated">结论</h1><p id="d0e3" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr nu kt ku kv nv kx ky kz nw lb lc ld im bi translated">在这个演示中，我们看到了如何使用语音识别库将语音转换为文本，以及如何使用PyAutoGUI实现GUI自动化。通过结合这两种功能，我们建立了一个语音控制鼠标和键盘的基本版本。我希望您发现这些信息很有用，并以自己的方式使用它们。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><div class="lh li lj lk gt nx"><a rel="noopener follow" target="_blank" href="/face-detection-in-10-lines-for-beginners-1787aa1d9127"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">面向初学者的10行人脸检测</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">使用Python OpenCV在图像和视频中检测人脸的介绍。</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol lq nx"/></div></div></a></div><div class="om on gp gr oo nx"><a href="https://medium.com/swlh/artificial-neural-networks-for-absolute-beginners-a75bc1522e1d" rel="noopener follow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">绝对初学者的神经网络</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">用简单的英语介绍感知器</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">medium.com</p></div></div><div class="og l"><div class="op l oi oj ok og ol lq nx"/></div></div></a></div></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="486c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">参考文献:</strong></p><p id="71f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1] PyAutoGUI CheatSheet，【https://pyautogui.readthedocs.io/en/latest/quickstart.html T2】</p><p id="32a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]语音识别Python，【https://pypi.org/project/SpeechRecognition/ T4】</p></div></div>    
</body>
</html>