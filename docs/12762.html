<html>
<head>
<title>Text Classification on Disaster Tweets with LSTM and Word Embedding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 LSTM 和单词嵌入的灾难微博文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db?source=collection_archive---------15-----------------------#2020-09-02">https://towardsdatascience.com/text-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db?source=collection_archive---------15-----------------------#2020-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bdcd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">单词嵌入对文本分类准确率的影响</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2d9278613a0584d200a66e5e32fc2fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1Rd11f-V0IRhHbYo.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.istockphoto.com/photo/disaster-area-with-tornado-gm172448852-23567577" rel="noopener ugc nofollow" target="_blank">https://www . istock photo . com/photo/disaster-area-with tornado-GM 172448852-23567577</a></p></figure><p id="ef1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">这是我的第一个 Kaggle 笔记本，我想为什么不把它也写在介质上呢？</p><p id="ed9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的<a class="ae ky" href="https://github.com/emmanuellaanggi/disaster_tweet_sentiment" rel="noopener ugc nofollow" target="_blank"> Github </a>上的全部代码。</p><p id="9b5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将详细说明如何使用 fastText 和 GloVe 作为文本分类的 LSTM 模型上的单词嵌入。我在做关于自然语言生成的论文时对单词嵌入产生了兴趣。结果表明，嵌入层权重的嵌入矩阵提高了模型的性能。但由于是在 NLG，这种衡量是主观的。我也只用了快速文本。所以在这篇文章中，我想看看每种方法(有快速文本和手套以及没有)对预测的影响。在我的 Github 代码上，我也将结果与 CNN 进行了比较。我在这里使用的数据集来自 Kaggle 上的一个比赛，由推文组成，并标有推文是否使用灾难性的词语来通知真实的灾难，或者只是隐喻性地使用它。老实说，在第一次看到这个数据集时，我立即想到了 BERT 及其理解能力，这比我在本文中提出的要好得多。</p><p id="df67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是无论如何，在这篇文章中，我将集中讨论 fastText 和 GloVe。</p><p id="2fd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">走吧。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="146d" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated"><strong class="ak">数据+预处理</strong></h1><p id="bcf7" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">这些数据由 7613 条带有标签(专栏目标)的推文(专栏文本)组成，无论它们是否在谈论一场真正的灾难。其中 3271 行通知真实灾难，4342 行通知非真实灾难。kaggle 竞赛上分享的数据，如果你想了解更多的数据，你可以在这里阅读<a class="ae ky" href="https://www.kaggle.com/c/nlp-getting-started" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/d7d3c7009b8b234150380780bec8f8e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6GHVI0YT6W97_kRULUWOFA.png"/></div></div></figure><p id="d058" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">课文中真实灾难词的例子:</p><blockquote class="nj nk nl"><p id="a131" class="kz la nm lb b lc ld ju le lf lg jx lh nn lj lk ll no ln lo lp np lr ls lt lu im bi translated">“拉朗格萨斯克附近的森林<strong class="lb iu">失火</strong>。加拿大"</p></blockquote><p id="cc73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用灾难词但不谈论灾难的例子:</p><blockquote class="nj nk nl"><p id="5ca0" class="kz la nm lb b lc ld ju le lf lg jx lh nn lj lk ll no ln lo lp np lr ls lt lu im bi translated">“这些箱子准备<strong class="lb iu">爆炸</strong>！<strong class="lb iu">爆炸</strong>小猫终于来了！小猫游戏#探索小猫"</p></blockquote><p id="27db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据将被分为训练(6090 行)和测试(1523 行)，然后进行预处理。我们将只使用文本和目标列。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="0a18" class="nv mm it nr b gy nw nx l ny nz">from sklearn.model_selection import train_test_split</span><span id="2a46" class="nv mm it nr b gy oa nx l ny nz">data = pd.read_csv('train.csv', sep=',', header=0)</span><span id="86d2" class="nv mm it nr b gy oa nx l ny nz">train_df, test_df = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)</span></pre><p id="e531" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此处使用的预处理步骤:</p><ol class=""><li id="adbe" class="ob oc it lb b lc ld lf lg li od lm oe lq of lu og oh oi oj bi translated">案例折叠</li><li id="eb38" class="ob oc it lb b lc ok lf ol li om lm on lq oo lu og oh oi oj bi translated">清洗停止字</li><li id="923e" class="ob oc it lb b lc ok lf ol li om lm on lq oo lu og oh oi oj bi translated">符号化</li></ol><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="965c" class="nv mm it nr b gy nw nx l ny nz">from sklearn.utils import shuffle</span><span id="a01b" class="nv mm it nr b gy oa nx l ny nz">raw_docs_train = train_df['text'].tolist()<br/>raw_docs_test = test_df['text'].tolist()<br/>num_classes = len(label_names)</span><span id="93a9" class="nv mm it nr b gy oa nx l ny nz">processed_docs_train = []</span><span id="496e" class="nv mm it nr b gy oa nx l ny nz">for doc in tqdm(raw_docs_train):<br/>  tokens = word_tokenize(doc)<br/>  filtered = [word for word in tokens if word not in stop_words]<br/>  processed_docs_train.append(" ".join(filtered))</span><span id="15ab" class="nv mm it nr b gy oa nx l ny nz">processed_docs_test = []</span><span id="dc4b" class="nv mm it nr b gy oa nx l ny nz">for doc in tqdm(raw_docs_test):<br/>  tokens = word_tokenize(doc)<br/>  filtered = [word for word in tokens if word not in stop_words]<br/>  processed_docs_test.append(" ".join(filtered))</span><span id="f910" class="nv mm it nr b gy oa nx l ny nz">tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)<br/>tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  </span><span id="23c8" class="nv mm it nr b gy oa nx l ny nz">word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)<br/>word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)<br/>word_index = tokenizer.word_index</span><span id="59c9" class="nv mm it nr b gy oa nx l ny nz">word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)</span><span id="5a23" class="nv mm it nr b gy oa nx l ny nz">word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)</span></pre><h1 id="e46f" class="ml mm it bd mn mo op mq mr ms oq mu mv jz or ka mx kc os kd mz kf ot kg nb nc bi translated"><strong class="ak">文字嵌入</strong></h1><p id="09ec" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated"><strong class="lb iu">步骤一。下载预先训练好的模型</strong></p><p id="02bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<a class="ae ky" href="https://fasttext.cc/docs/en/english-vectors.html" rel="noopener ugc nofollow" target="_blank">快速文本</a>和<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>的第一步是下载每个预先训练好的模型。我用 Google Colab 防止笔记本电脑使用大内存，所以用<em class="nm">请求</em>库下载，直接在笔记本上解压。</p><p id="571c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我使用了两个单词嵌入中最大的预训练模型。fastText 模型给出了 200 万个单词向量(600B 个标记)，GloVe 给出了 220 万个单词向量(840B 个标记)，两者都是在普通爬行上训练的。</p><p id="f8a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> fastText 预培训下载</strong></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="774a" class="nv mm it nr b gy nw nx l ny nz">import requests, zipfile, io</span><span id="d7d1" class="nv mm it nr b gy oa nx l ny nz">zip_file_url = “https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"</span><span id="3f4a" class="nv mm it nr b gy oa nx l ny nz">r = requests.get(zip_file_url)</span><span id="75c6" class="nv mm it nr b gy oa nx l ny nz">z = zipfile.ZipFile(io.BytesIO(r.content))</span><span id="8633" class="nv mm it nr b gy oa nx l ny nz">z.extractall()</span></pre><p id="d866" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">手套预培训下载</strong></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="cdad" class="nv mm it nr b gy nw nx l ny nz">import requests, zipfile, io</span><span id="2648" class="nv mm it nr b gy oa nx l ny nz">zip_file_url = “http://nlp.stanford.edu/data/glove.840B.300d.zip"</span><span id="67d2" class="nv mm it nr b gy oa nx l ny nz">r = requests.get(zip_file_url)</span><span id="8ceb" class="nv mm it nr b gy oa nx l ny nz">z = zipfile.ZipFile(io.BytesIO(r.content))</span><span id="7b71" class="nv mm it nr b gy oa nx l ny nz">z.extractall()</span></pre><p id="ba46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第二步。将预训练模型加载到单词向量中</strong></p><p id="1b82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">FastText 给出了加载单词向量的格式，所以我用它来加载两个模型。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="871a" class="nv mm it nr b gy nw nx l ny nz">embeddings_index = {}</span><span id="229d" class="nv mm it nr b gy oa nx l ny nz">f = codecs.open(‘crawl-300d-2M.vec’, encoding=’utf-8')<br/># for Glove<br/># f = codecs.open(‘glove.840B.300d.txt’, encoding=’utf-8')</span><span id="7334" class="nv mm it nr b gy oa nx l ny nz">for line in tqdm(f):</span><span id="d82d" class="nv mm it nr b gy oa nx l ny nz">values = line.rstrip().rsplit(‘ ‘)</span><span id="4678" class="nv mm it nr b gy oa nx l ny nz">word = values[0]</span><span id="a00b" class="nv mm it nr b gy oa nx l ny nz">coefs = np.asarray(values[1:], dtype=’float32')</span><span id="d757" class="nv mm it nr b gy oa nx l ny nz">embeddings_index[word] = coefs</span><span id="4b72" class="nv mm it nr b gy oa nx l ny nz">f.close()</span></pre><p id="1f02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第三步。嵌入矩阵</strong></p><p id="b5ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于训练数据中每个单词的权重，将在嵌入层使用嵌入矩阵。它是通过枚举训练数据集中存在于标记化单词索引中的每个唯一单词，并使用来自 fastText orGloVe 的权重来定位嵌入权重(<a class="ae ky" href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)" rel="noopener ugc nofollow" target="_blank">更多关于嵌入矩阵</a>)。</p><p id="5e53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是有一种可能性是，有些词不在向量中，比如错别字、缩写或用户名。这些单词将被存储在一个列表中，我们可以比较 fastText 和 GloVe 处理单词的性能</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="870d" class="nv mm it nr b gy nw nx l ny nz">words_not_found = []</span><span id="7200" class="nv mm it nr b gy oa nx l ny nz">nb_words = min(MAX_NB_WORDS, len(word_index)+1)<br/>embedding_matrix = np.zeros((nb_words, embed_dim))</span><span id="2dc1" class="nv mm it nr b gy oa nx l ny nz">for word, i in word_index.items():<br/>  if i &gt;= nb_words:<br/>     continue<br/>  embedding_vector = embeddings_index.get(word)<br/>  <br/>  if (embedding_vector is not None) and len(embedding_vector) &gt; 0:<br/>     embedding_matrix[i] = embedding_vector<br/>  else:<br/>     words_not_found.append(word)</span><span id="8767" class="nv mm it nr b gy oa nx l ny nz">print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))</span></pre><p id="b02f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">fastText 上的空词嵌入数是 9175，GloVe 上是 9186。可以假设 fastText 处理更多的单词，即使预训练是在较少的单词上训练的。</p><h1 id="8ba5" class="ml mm it bd mn mo op mq mr ms oq mu mv jz or ka mx kc os kd mz kf ot kg nb nc bi translated"><strong class="ak">长短期记忆(LSTM) </strong></h1><p id="b93b" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">你可以对超参数或架构进行微调，但我将使用非常简单的一个嵌入层，LSTM 层，密集层和辍学层。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="bf9b" class="nv mm it nr b gy nw nx l ny nz">from keras.layers import BatchNormalization<br/>import tensorflow as tf</span><span id="400e" class="nv mm it nr b gy oa nx l ny nz">model = tf.keras.Sequential()</span><span id="cb5f" class="nv mm it nr b gy oa nx l ny nz">model.add(Embedding(nb_words, embed_dim, input_length=max_seq_len, weights=[embedding_matrix],trainable=False))</span><span id="ba8a" class="nv mm it nr b gy oa nx l ny nz">model.add(Bidirectional(LSTM(32, return_sequences= True)))<br/>model.add(Dense(32,activation=’relu’))</span><span id="a57f" class="nv mm it nr b gy oa nx l ny nz">model.add(Dropout(0.3))<br/>model.add(Dense(1,activation=’sigmoid’))</span><span id="11e4" class="nv mm it nr b gy oa nx l ny nz">model.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/a6132f5130f1a2e35a3b56dfc13c6c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Su4SgxSGwjMLMbo18kv_9Q.png"/></div></div></figure><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9da8" class="nv mm it nr b gy nw nx l ny nz">from keras.optimizers import RMSprop<br/>from keras.callbacks import ModelCheckpoint<br/>from tensorflow.keras.callbacks import EarlyStopping</span><span id="9b8a" class="nv mm it nr b gy oa nx l ny nz">model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="5a50" class="nv mm it nr b gy oa nx l ny nz">es_callback = EarlyStopping(monitor='val_loss', patience=3)</span><span id="eaeb" class="nv mm it nr b gy oa nx l ny nz">history = model.fit(word_seq_train, y_train, batch_size=256, epochs=30, validation_split=0.3, callbacks=[es_callback], shuffle=False)</span></pre><h1 id="55c9" class="ml mm it bd mn mo op mq mr ms oq mu mv jz or ka mx kc os kd mz kf ot kg nb nc bi translated">结果</h1><p id="ce40" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">fastText 以大约 83%的准确度给出了最好的性能，而 GloVe 给出了 81%的准确度。性能上的差异并不显著，但是与没有单词嵌入的模型的性能(68%)相比，我们可以看到单词嵌入在嵌入层权重上的显著使用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/6179a3d7f96b912eb1184d5197bdc73d.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*3-aFRQ2TKgcRqg0pNscH4w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">fastText 单词嵌入的准确性</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/24003e1621ae1a872f75e83504e246aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*JS1rkGZcBdT68L_bv9F6yA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">手套单词嵌入的准确性</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/966ee9ad7005fb41a90a700ffff84808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*b65NAuBhcFQ2LfKgT6nP_Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">无单词嵌入的准确性</p></figure><p id="b357" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于训练性能的更多信息，详细代码，如果你想在不同的数据集上应用它，你可以在我的<a class="ae ky" href="https://github.com/emmanuellaanggi" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上看到完整代码。</p><p id="ef27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读！</p></div></div>    
</body>
</html>