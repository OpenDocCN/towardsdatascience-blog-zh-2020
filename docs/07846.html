<html>
<head>
<title>Transformers Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-explained-65454c0f3fa7?source=collection_archive---------7-----------------------#2020-06-11">https://towardsdatascience.com/transformers-explained-65454c0f3fa7?source=collection_archive---------7-----------------------#2020-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c3c3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对谷歌Transformer模型的详尽解释；从理论到实施</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/39b5d659f2f00da61880f63ba93b943c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zrrUdDqkhP6VSlNq"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@wagner2074?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克里斯汀·瓦格纳</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="bcc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章是对谷歌研究的著名论文“<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>”中的变压器模型的深入阐释。该模型是序列转导任务(任何涉及将一个序列转换成另一个序列的任务)中许多SOTA(现有技术)方法的先驱。以下是这篇帖子的内容:</p><h1 id="e122" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">内容</h1><ol class=""><li id="5c3f" class="mn mo it lb b lc mp lf mq li mr lm ms lq mt lu mu mv mw mx bi translated"><strong class="lb iu">序列对序列模型的概述</strong></li><li id="186c" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">为什么是变压器？</strong></li><li id="61ee" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">注意？自我关注！</strong></li><li id="75b8" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">什么是变压器？</strong></li><li id="8d9d" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu mu mv mw mx bi translated"><strong class="lb iu">结论</strong></li></ol><p id="24f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将是一个漫长的，所以坐稳了！</p><h1 id="f6f9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">序列到序列模型概述</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/0aba265873ffe568f7ba7b6fe62ca067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8u-ZFSuH7mazT0wl.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GNMT中的seq2seq，由<a class="ae ky" href="https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a>可视化</p></figure><p id="b2a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">序列到序列编码器-解码器结构是序列转换任务的基础。它实质上建议立即编码完整的序列，然后使用该编码作为产生解码序列或目标序列的上下文。</p><blockquote class="ne nf ng"><p id="466e" class="kz la nh lb b lc ld ju le lf lg jx lh ni lj lk ll nj ln lo lp nk lr ls lt lu im bi translated">人们可能会把这与人类倾向于首先完整地“听”一个句子(序列)，然后相应地作出反应，无论是对话、翻译还是任何类似的任务。</p></blockquote><p id="4994" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">seq2seq模型由分别位于编码器和解码器的独立rnn组成。编码序列是编码器网络中RNN的<em class="nh">隐藏状态</em>。使用这个编码序列和(通常)单词级生成模型，seq2seq生成目标序列。由于编码是在单词级进行的，因此对于较长的序列，很难在编码器中保持上下文，因此将众所周知的注意机制与seq2seq结合，以<em class="nh">“注意”</em>序列中对目标序列的生成有显著贡献的特定单词。注意力是根据输入序列中的单个单词对目标序列生成的影响来衡量它们。</p><h1 id="590d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">为什么是变形金刚？</h1><p id="2d3e" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">使用RNNs的序列对序列是很棒的，注意它甚至更好。那变形金刚有什么了不起的？</p><p id="d3aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">rnn的主要问题在于它们不能在处理时提供并行化。RNN的处理是连续的，也就是说，我们不能计算下一个时间步长的值，除非我们有当前的输出。这使得基于RNN的方法进展缓慢。</p><p id="a64e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，脸书研究中心解决了这个问题，他们建议使用基于<strong class="lb iu">卷积的</strong>方法，该方法允许将并行化与GPU相结合。这些模型建立了单词之间的层次表示，其中在序列中出现得较近的单词在较低的层次上相互作用，而出现得较远的单词在层次中的较高层次上起作用。ConvS2S和ByteNet就是两个这样的型号。引入层次结构是为了解决长期的依赖性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/a69b49dbb5d65749bbd38259a9a63284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*p6frtDPFEfw3z4Uy.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XreFtRbhU5l" rel="noopener ugc nofollow" target="_blank"> Michal Chromiak的博客</a>形成ConvS2S的多步关注</p></figure><p id="4eab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然这实现了并行化，但它的计算量仍然很大。与RNNs和CNN提供的结果质量相比，它们每层的操作数量更不合理。最初的变压器论文已经提出了这些参数对于合格模型的比较:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3a6e2a161bdfe622ef92057f25b6aaa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/0*IOh4mUYh8AtH3bjp.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于计算效率度量的RNN、CNN和自我注意模型的比较</p></figure><p id="8eb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，d(或<em class="nh"> d_model </em>)是单词的表示维度或嵌入维度(通常在128-512的范围内)，n是序列长度(通常在40-70的范围内)，k是卷积的核大小，r是受限自我注意的注意窗口大小。从表中，我们可以推断出以下几点:</p><ul class=""><li id="2377" class="mn mo it lb b lc ld lf lg li nq lm nr lq ns lu nt mv mw mx bi translated">显然，自我关注的每层计算复杂度远低于其他人。</li><li id="2aba" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu nt mv mw mx bi translated">关于顺序操作，除了RNNs，所有其他方法都提供并行化，因此它们的复杂度是O(1)。</li><li id="8aa3" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu nt mv mw mx bi translated">最后一个度量是最大路径长度，它表面上意味着<em class="nh">参与</em>长期依赖或远距离单词的复杂性。因为卷积模型使用分层表示，所以它们的复杂度是nlog(n ),而自我注意模型在同一个步骤中关注所有单词，因此它们的复杂度是O(1)。</li></ul><p id="e7e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">转换器使用自关注机制，其中<em class="nh">关注权重</em>是使用输入序列中的所有单词一次性计算的，因此它有利于并行化。除此之外，由于转换器中的每层操作是在相同序列的字之间进行的，所以复杂度不超过O(n d)。因此，transformer被证明是有效的(因为它使用了注意力),同时也是一个计算高效的模型。</p><h1 id="981e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">注意？自我关注！</h1><p id="b38e" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在上一节中，我们讨论了转换器使用自我关注作为有效计算的手段。在这一节，我们将破译，到底什么是自我关注，它是如何在变压器使用。</p><h2 id="92d8" class="nu lw it bd lx nv nw dn mb nx ny dp mf li nz oa mh lm ob oc mj lq od oe ml of bi translated">查询、键和值</h2><p id="39ef" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">作为一般惯例，注意机制遵循查询、键、值模式。所有这三个都是来自输入序列的单词，它们以某种模式相互操作。查询和键最初经历某些操作，然后输出(通常)与值相乘。这将在下一小节中变得更清楚，我们将看到自我关注如何工作的图示描述。</p><h2 id="1ebf" class="nu lw it bd lx nv nw dn mb nx ny dp mf li nz oa mh lm ob oc mj lq od oe ml of bi translated">自我关注</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/7418aaee07beb6418532311388f0fbe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Mo4vYSC6nBGAuUZ_.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自我关注</p></figure><p id="8b53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，自我注意是“注意”来自同一序列的单词。</p><blockquote class="ne nf ng"><p id="a5bd" class="kz la nh lb b lc ld ju le lf lg jx lh ni lj lk ll nj ln lo lp nk lr ls lt lu im bi translated">从表面上看，自我关注决定了一个单词对句子的影响</p></blockquote><p id="933b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上图中，自我注意的运作是用一个句子的例子来解释的，“这就是注意”。单词“This”与句子中的其他单词连用。类似地，计算所有单词的注意力权重(这里是“是”和“注意力”)。这里没有<em class="nh">‘隐藏状态’</em>的概念。输入被直接使用。这从架构中移除了<em class="nh">的顺序性</em>，从而允许并行化。</p><p id="41e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在变形金刚的情况下，<strong class="lb iu"> <em class="nh">多头注意</em> </strong>被使用，这将在后面的帖子中介绍。</p><h1 id="2025" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是变压器？</h1><p id="be56" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">到目前为止，我们已经看到了在Transformer中实现的机制。此后，我们将实际看到这些毗邻的机制和几个具体到模型的组成部分是如何被纳入。</p><blockquote class="oh"><p id="3361" class="oi oj it bd ok ol om on oo op oq lu dk translated">我们将尝试自下而上地构建一个变压器</p></blockquote><h2 id="eb28" class="nu lw it bd lx nv or dn mb nx os dp mf li ot oa mh lm ou oc mj lq ov oe ml of bi translated">位置编码</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/f5dd0a43749b2ed7d7060b6e5d23105e.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/0*H145mFJk24W4uXKg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">“注意力是你所需要的一切”</a>在Transformer中进行位置编码</p></figure><p id="6757" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你观察，自我注意的计算没有单词在序列中排序的概念。尽管rnn很慢，但它们的顺序性确保了单词的顺序得以保留。因此，为了引出单词在序列中定位的概念，将<em class="nh">位置编码</em>添加到常规输入嵌入中。位置编码的维度与嵌入(<em class="nh"> d_model </em>)相同，便于两者的求和。在该论文中，位置编码是使用以下方法获得的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/9eb63348fb2c57ca5c0acfed1356a438.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/0*Zytxigz9R9Y9OtbA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">“注意力是你所需要的一切”</a>使用正弦波进行位置编码</p></figure><p id="398d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，<em class="nh"> i </em>是维度，<em class="nh"> pos </em>是字的位置。我们用正弦表示维度的偶数(2i)，余弦表示奇数(2i + 1)。位置编码有几种选择—学习的或固定的。这是固定的方法，因为论文陈述的学习和固定的方法获得了相同的结果。</p><blockquote class="ne nf ng"><p id="45ac" class="kz la nh lb b lc ld ju le lf lg jx lh ni lj lk ll nj ln lo lp nk lr ls lt lu im bi translated">背后的一般思想是，对于固定的偏移k，PEₚₒₛ₊ₖ可以表示为PEₚₒₛ.的线性函数</p></blockquote><h2 id="f421" class="nu lw it bd lx nv nw dn mb nx ny dp mf li nz oa mh lm ob oc mj lq od oe ml of bi translated">掩饰</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/43314b2feae56ad88394d24406003444.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/0*lSBS9s2RIlGPhJAn.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">“注意力就是你需要的全部”</a>在多头注意力中使用掩蔽</p></figure><p id="fd06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在变形金刚的多头注意机制中使用了两种遮罩。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/619da362d6ed52a097cf8337e9fb48a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*29IZNKCQonMVzqzg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">衬垫掩模的加工</p></figure><ul class=""><li id="38d7" class="mn mo it lb b lc ld lf lg li nq lm nr lq ns lu nt mv mw mx bi translated"><strong class="lb iu">填充掩码:</strong>序列的输入向量应该长度固定。因此，<em class="nh"> max_length </em>参数定义了转换器可以接受的序列的最大长度。长度大于<em class="nh"> max_length </em>的所有序列都被截断，而较短的序列用零<strong class="lb iu"><em class="nh"/></strong><em class="nh">填充。</em>然而，零填充既不应该有助于注意力计算，也不应该有助于目标序列生成。下图解释了填充遮罩的工作原理。这是变压器中的可选操作。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/01a373e9b6d36bbff5d35505f37752fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g-amoUtb3-ECjUjE.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">前瞻掩模的工作</p></figure><ul class=""><li id="7077" class="mn mo it lb b lc ld lf lg li nq lm nr lq ns lu nt mv mw mx bi translated"><strong class="lb iu">前瞻掩码:</strong>在解码器处生成目标序列时，由于转换器使用自关注，它倾向于包括来自解码器输入的所有单词。但是，实际上这是不正确的。只有当前单词之前的单词可以有助于下一个单词的生成。掩蔽的多头注意力确保了这一点。下图解释了前瞻掩码的工作原理。</li></ul><p id="1c91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码片段解释了填充是如何起作用的:</p><pre class="kj kk kl km gt pb pc pd pe aw pf bi"><span id="385d" class="nu lw it pc b gy pg ph l pi pj">&gt;&gt;&gt; a = tf.constant([0.6, 0.2, 0.3, 0.4, 0, 0, 0, 0, 0, 0])<br/>&gt;&gt;&gt; tf.nn.softmax(a)<br/>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=<br/>array([0.15330984, 0.10276665, 0.11357471, 0.12551947, 0.08413821,<br/>       0.08413821, 0.08413821, 0.08413821, 0.08413821, 0.08413821],<br/>      dtype=float32)&gt;<br/>&gt;&gt;&gt; b = tf.constant([0.6, 0.2, 0.3, 0.4, -1e9, -1e9, -1e9, -1e9, -1e9, -1e9])<br/>&gt;&gt;&gt; tf.nn.softmax(b)<br/>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=<br/>array([0.3096101 , 0.20753784, 0.22936477, 0.25348732, 0.        ,<br/>       0.        , 0.        , 0.        , 0.        , 0.        ],<br/>      dtype=float32)&gt;</span></pre><h2 id="ddf2" class="nu lw it bd lx nv nw dn mb nx ny dp mf li nz oa mh lm ob oc mj lq od oe ml of bi translated">标度点积注意力</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/fc415aaa3742c5b7c019d98befbb8af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/0*pTM4ig2wjYq7nXjK.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">“关注是你所需要的一切”</a>的成比例的点产品关注</p></figure><p id="73fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是主要的“注意力计算”步骤，我们之前在自我注意部分已经讨论过了。这包括几个步骤:</p><ul class=""><li id="58f2" class="mn mo it lb b lc ld lf lg li nq lm nr lq ns lu nt mv mw mx bi translated">这是一个矩阵点积运算。首先，查询和键经历这个操作。该操作可以用数学方法表示如下:</li></ul><blockquote class="oh"><p id="ab98" class="oi oj it bd ok ol pl pm pn po pp lu dk translated">马特穆尔(q，K) = Q.Kᵀ</p></blockquote><ul class=""><li id="255d" class="mn mo it lb b lc pr lf ps li pt lm pu lq pv lu nt mv mw mx bi translated"><strong class="lb iu"> Scale: </strong>点积运算的输出可能会导致较大的值，这可能会扰乱后面部分的softmax。因此，我们通过将它们除以缩放因子<strong class="lb iu"> <em class="nh"> √dₖ.来缩放它们</em> </strong></li><li id="396f" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu nt mv mw mx bi translated"><strong class="lb iu">遮罩:</strong>可选的填充遮罩已经在上面的遮罩部分讨论过了。</li><li id="b237" class="mn mo it lb b lc my lf mz li na lm nb lq nc lu nt mv mw mx bi translated"><strong class="lb iu">soft max:</strong>soft max函数将数值降低到一个概率分布，即[0，1]。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/a864433e33fb6ea2e3989890fe35bb06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*WbmLoUnqt4lESy-X.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">“注意力是你所需要的全部”</a>得出的比例点积注意力的最终等式</p></figure><p id="2974" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比例点积注意力是多头注意力的主要组成部分，我们将在下一小节中看到。</p><h2 id="c81a" class="nu lw it bd lx nv nw dn mb nx ny dp mf li nz oa mh lm ob oc mj lq od oe ml of bi translated">多头注意力</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi px"><img src="../Images/ac10ea1100ad750f170f493668f8b335.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/0*_KfXXgV1AEncXGl1.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">的多头关注</a></p></figure><p id="9594" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多头注意力本质上是前面讨论的所有微观概念的整合。</p><p id="8be6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在相邻的图中，<em class="nh"> h </em>是头数。就数学而言，多头注意力的初始输入被分成<em class="nh"> h </em>部分，每个部分都有查询、键和值，用于序列中的<em class="nh"> max_length </em>字，用于<em class="nh"> batch_size序列。</em>Q、K、V的尺寸称为<em class="nh">深度</em>，计算如下:</p><blockquote class="oh"><p id="9fab" class="oi oj it bd ok ol om on oo op oq lu dk translated">深度= d _模型// h</p></blockquote><p id="b189" class="pw-post-body-paragraph kz la it lb b lc pr ju le lf ps jx lh li py lk ll lm pz lo lp lq qa ls lt lu im bi translated">这就是<strong class="lb iu"> <em class="nh"> d_model </em>需要被<em class="nh">h</em></strong>T22】整除的原因。因此，在分割时，<em class="nh"> d_model </em>形状矢量被分割成形状<em class="nh">深度的<em class="nh"> h </em>矢量。</em>这些向量作为Q、K、V被传递给缩放的点积，并且通过再次将<em class="nh"> h </em>向量整形为形状<em class="nh"> d_model </em>的1个向量，输出为“<strong class="lb iu"> <em class="nh"> Concat </em> </strong>”。这个重组的向量然后通过前馈神经网络层。</p><h2 id="2de9" class="nu lw it bd lx nv nw dn mb nx ny dp mf li nz oa mh lm ob oc mj lq od oe ml of bi translated">点式前馈网络和残余漏失</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/af79b2889372ecfe0e91832542586ba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/0*LGbcFjsZrJ0deRnq.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">“关注是你所需要的一切”</a>的逐点FFN和剩余辍学</p></figure><p id="cbca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逐点前馈网络块本质上是两层线性变换，在整个模型架构中使用相同，通常在关注块之后。</p><p id="c4c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于正则化，在将每个子层的输出添加到子层的输入并进行归一化之前，对每个子层的输出应用缺失。</p><h2 id="ae7b" class="nu lw it bd lx nv nw dn mb nx ny dp mf li nz oa mh lm ob oc mj lq od oe ml of bi translated">最后，我们有完整的变压器！</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/053837e84a87f5da2f1baa01d2a85dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/0*HyKsci1062HqCmDa.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">变压器via <a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">“注意力是你所需要的一切”</a></p></figure><p id="03b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有的组件被组装在一起，以建立变压器。编码器模块在左边，解码器模块在右边。</p><blockquote class="ne nf ng"><p id="dc07" class="kz la nh lb b lc ld ju le lf lg jx lh ni lj lk ll nj ln lo lp nk lr ls lt lu im bi translated">编码器和解码器块可以可选地微调到Nₓ单位以调整模型。</p></blockquote><h1 id="1fad" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="0e66" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在这篇文章中，我们看到了序列转导任务中基于RNN的方法的问题，以及革命性的Transformer模型如何解决这些问题。后来，我们研究了变压器的基本机制及其工作原理。最后，我们看到了各种组件和底层机制如何与转换器一起工作。</p><p id="9604" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经在这里介绍了使用TensorFlow进行抽象文本摘要的Transformer的逐步实现<a class="ae ky" href="https://medium.com/@rojagtap/abstractive-text-summarization-using-transformers-3e774cc42453" rel="noopener">。</a></p><h1 id="f7ca" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><p id="e73a" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">《注意力是你所需要的全部》变形金刚原纸:【https://arxiv.org/abs/1706.03762<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"/></p><p id="8d7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Seq2Seq论文:【https://arxiv.org/abs/1409.3215 T4】</p><p id="a76c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">巴林岛注意事项:<a class="ae ky" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1409.0473</a></p><p id="6fba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ConvS2S和ByteNet论文:<a class="ae ky" href="https://arxiv.org/abs/1705.03122" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1705.03122</a>，<a class="ae ky" href="https://arxiv.org/abs/1610.10099" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1610.10099</a></p><div class="qd qe gp gr qf qg"><a rel="noopener follow" target="_blank" href="/self-attention-and-transformers-882e9de5edda"><div class="qh ab fo"><div class="qi ab qj cl cj qk"><h2 class="bd iu gy z fp ql fr fs qm fu fw is bi translated">自我关注和变形金刚</h2><div class="qn l"><h3 class="bd b gy z fp ql fr fs qm fu fw dk translated">从关注到自我关注到变形金刚</h3></div><div class="qo l"><p class="bd b dl z fp ql fr fs qm fu fw dk translated">towardsdatascience.com</p></div></div><div class="qp l"><div class="qq l qr qs qt qp qu ks qg"/></div></div></a></div><div class="qd qe gp gr qf qg"><a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XreFtRbhU5l" rel="noopener  ugc nofollow" target="_blank"><div class="qh ab fo"><div class="qi ab qj cl cj qk"><h2 class="bd iu gy z fp ql fr fs qm fu fw is bi translated">变形金刚-你需要的只是注意力。</h2><div class="qn l"><h3 class="bd b gy z fp ql fr fs qm fu fw dk translated">变形金刚——不仅仅是看上去那么简单！我们到了吗？良好的...不完全是，但是...消除复发怎么样…</h3></div><div class="qo l"><p class="bd b dl z fp ql fr fs qm fu fw dk translated">mchromiak.github.io</p></div></div><div class="qp l"><div class="qv l qr qs qt qp qu ks qg"/></div></div></a></div><p id="4677" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些很棒的变形金刚形象:</p><div class="qd qe gp gr qf qg"><a href="http://jalammar.github.io/illustrated-transformer/" rel="noopener  ugc nofollow" target="_blank"><div class="qh ab fo"><div class="qi ab qj cl cj qk"><h2 class="bd iu gy z fp ql fr fs qm fu fw is bi translated">图示的变压器</h2><div class="qn l"><h3 class="bd b gy z fp ql fr fs qm fu fw dk translated">讨论:黑客新闻(65分，4条评论)，Reddit r/MachineLearning (29分，3条评论)翻译…</h3></div><div class="qo l"><p class="bd b dl z fp ql fr fs qm fu fw dk translated">jalammar.github.io</p></div></div><div class="qp l"><div class="qw l qr qs qt qp qu ks qg"/></div></div></a></div></div></div>    
</body>
</html>