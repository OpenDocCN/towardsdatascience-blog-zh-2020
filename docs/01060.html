<html>
<head>
<title>Classifying homogeneous data, what you need to know!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类同质数据，你需要知道的！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classifying-homogeneous-data-what-you-need-to-know-7b3a86e5f855?source=collection_archive---------16-----------------------#2020-01-30">https://towardsdatascience.com/classifying-homogeneous-data-what-you-need-to-know-7b3a86e5f855?source=collection_archive---------16-----------------------#2020-01-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f973" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">过去一些研究人员做的很粗略，你应该做得更好！</h2></div><p id="8313" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在北岭加州州立大学和刘莉教授一起做人机交互(HCI)的硕士论文。在此期间，我发现了一些我们作为数据科学家可以改进的地方。我没有统计学学位，也没有在网上找到以前的相关工作，但我在大量相关论文中看到了这个问题，并决定与大家分享我的发现。我认为我们需要重新考虑处理同质数据的方式。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/0ec47b3663d80ba2f3e7f140fd5e0310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H_yu_VhFm96qB_YR.jpg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图片来自<a class="ae lu" href="https://pixabay.com/illustrations/question-mark-important-sign-1872665/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><h1 id="d7e1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是同质数据？</h1><p id="87fe" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">如果一个数据集是由彼此相似的事物组成的，那么它就是同构的。在本文的范围内，它意味着来自完全相同来源的数据。在监督学习的典型场景中，这将导致数据集在整个集合中具有完全相同的标签。</p><h1 id="c50a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">人们是如何处理同质数据的？</h1><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/2a33fd9185cab689c101a59f9a65c77e.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Ht-D-AyyJBbsbQ0HTVYm9g.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><a class="ae lu" href="http://www.cs.columbia.edu/4180/hw/keystroke.pdf" rel="noopener ugc nofollow" target="_blank">费边论文</a>中的非加权概率和</p></figure><p id="98c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在大多数过去的研究中，比如<a class="ae lu" href="http://www.cs.columbia.edu/4180/hw/keystroke.pdf" rel="noopener ugc nofollow" target="_blank">法比安关于击键动力学的论文</a>，研究人员会根据所有计算出的概率的总和来评分。</p><p id="4509" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">人工神经网络的 soft-max 层本质上做同样的事情。它向网络的下一层中的节点输出加权和。</p><p id="0bd1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这背后的直觉很简单:</p><blockquote class="mt"><p id="7530" class="mu mv it bd mw mx my mz na nb nc ld dk translated">直觉:如果更多的推理倾向于一个结果，那么这个结果最有可能是正确的。</p></blockquote><p id="e491" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">但远非完美…</p><h1 id="bdda" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">我们在处理同质数据时遇到了什么问题？</h1><p id="0ca7" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">概率总和背后的直觉是强大的，但远非完美。考虑这样一种情况，我们有两类数据，分别标记为 A 和 B。数据的分布如下图所示，其中 B 的分布区域由 A 包围。现在，如果我们发现样本来自区域 B，则样本可能具有标签 A 或 B。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/be934380ed9f2c7a6cef265b75960e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*Yi8Gte81y8CTCR1y5w3dPQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">典型数据分布图</p></figure><p id="5b88" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当两个标签的样本数量相似时，我们可以获取多个样本，然后判断它们属于 B 类。这是因为 B 区域较小，因此 B 类的密度较高。因此，在 B 区域中，B 类比 a 类更有可能被找到。概率总和仍然有效。然而，如果标签为 B 的样本少得多，或者如果区域大小相似且重叠而不是封闭，我们可能会从 A 类获得比 B 类更多的样本，并且无法正确地对其来源进行分类。</p><p id="e72c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">调整与偏差成比例的概率和的权重肯定会有助于这些情况，但这种近似是粗略的…</p><h1 id="309d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">我们应该如何处理同质数据？</h1><p id="387a" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在我们开始之前，让我们做 3 个假设:</p><ol class=""><li id="ced7" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld no np nq nr bi translated">多个获得的样本来自相同的来源。</li><li id="6004" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">抽样过程在概率上是独立的。</li><li id="2232" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">每个等级在他们的区域内都有一个概率分布。</li></ol><p id="177b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们可以考虑下面展示的两种直觉。为简单起见，让我们假设在每个区域内均匀分布。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/70116a398b5ab6f90277325506e5c4c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*arr3QzfHMTUow0AWTMv2eQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">典型数据分布图</p></figure><blockquote class="mt"><p id="6fb0" class="mu mv it bd mw mx ny nz oa ob oc ld dk translated">直觉 1:如果样本来自 A 区但在 B 区之外，那么来源就是 A 类。</p><p id="58b2" class="mu mv it bd mw mx my mz na nb nc ld dk translated">直觉 2:如果样本总是在区域 B 内，那么源极有可能是 B 类。</p></blockquote><p id="e3a4" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">据此，我们可以推导出以下等式:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi od"><img src="../Images/13492abe1e4db0bf326601e9149d9a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*edZZEX7IpzpppX5mxKUnqA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">具有特定标注的采样要素的概率</p></figure><p id="c6f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们假设两个圆内均匀分布的点。假设红色圆圈 A 覆盖的面积是蓝色圆圈 B 的两倍，并且每个标签的样本数量相等。然后，如果我们随机选择一个位于蓝色圆圈内的点，因为红色点的密度是一半，所以选择的点实际上有 2 / 3 的机会属于蓝色区域。</p><p id="d00c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们从蓝色圆圈内进行 10 次观察，总几率不会改变。但是如果我们应用我们的新技术，我们会得到:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oe"><img src="../Images/e1c8103602c0252181a5bb9cb66ce97b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PABcERxzzu-mnuFbe-_Big.png"/></div></div></figure><p id="7a59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们已经看到 10 个样本都来自蓝色圆圈内时，这是一个更有可能的估计。</p><h1 id="21fd" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">变异 k-NN 的性能</h1><p id="4dcc" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">为了测试这一点，我创建了自己版本的 k-NN。它是原始 k-NN 的变异版本。<a class="ae lu" rel="noopener" target="_blank" href="/how-to-build-a-k-nn-in-node-js-without-tensorflow-cac5753daa87">这里</a>是如何在 Node.js 中基于 k-d 树构建自己的 k-NN 的教程。输入数据来自我最近的一个实验，正如你所看到的，这些点彼此重叠。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi of"><img src="../Images/614fb5e0a2e1b153dbd18a03beb97842.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*XGWi56aLnxJHV-1U08z0IQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">重叠数据</p></figure><p id="7503" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">性能如下表所示，其中<em class="og"> n </em>是选择<em class="og"> u </em>样本的次数。由于变异的 k-NN 的性能由于数据量小而不稳定，所以用随机混洗的数据进行了多次实验。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oh"><img src="../Images/935998626fdf46afdfae02dafa924a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VtyjkuFrNUty3cRKuvVKcg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">变异 k-NN 的性能</p></figure><p id="7151" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，变异后的 k-NN 总体上确实有超越性的表现。只有当数据分布在如上所述的封闭或严重重叠的情况下，性能改进才会更好。这将为我们处理这些类型的数据提供一些见解。</p><h1 id="d674" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">注释在最后…</h1><p id="a6f6" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">我希望这篇文章对你有所帮助。我是一名计算机专业的学生，对很多事情都感兴趣。写作也是我的爱好，因为我喜欢把事情弄清楚并与他人分享，所以如果你在 Medium 上跟随我，你会看到更多这样的事情。</p><p id="59a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你感兴趣的话，我还有一篇关于我在数据科学领域的发现的非常老的文章:</p><p id="63b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lu" href="https://medium.com/@shenhuang_21425/how-apples-failure-teaches-data-scientist-a-lesson-b654489096b9" rel="noopener">苹果的失败如何给我们数据科学家上了一课</a></p></div></div>    
</body>
</html>