# ç”¨ Python è¿›è¡Œæœºå™¨å­¦ä¹ :åˆ†ç±»(å®Œæ•´æ•™ç¨‹)

> åŸæ–‡ï¼š<https://towardsdatascience.com/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec?source=collection_archive---------0----------------------->

![](img/bf7a59d41b19f964ce71b34753c515bf.png)

## æ•°æ®åˆ†æå’Œå¯è§†åŒ–ã€ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©ã€æ¨¡å‹è®¾è®¡å’Œæµ‹è¯•ã€è¯„ä¼°å’Œè§£é‡Š

## æ‘˜è¦

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨æ•°æ®ç§‘å­¦å’Œ Python è§£é‡Šåˆ†ç±»ç”¨ä¾‹çš„ä¸»è¦æ­¥éª¤ï¼Œä»æ•°æ®åˆ†æåˆ°ç†è§£æ¨¡å‹è¾“å‡ºã€‚

![](img/778b5ac9c09c7591b4d2a2cc3fd1efd5.png)

ç”±äºæœ¬æ•™ç¨‹å¯¹äºåˆå­¦è€…æ¥è¯´æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œæˆ‘å°†ä½¿ç”¨è‘—åçš„ Kaggle ç«èµ›ä¸­çš„â€œ **Titanic dataset** â€ï¼Œå…¶ä¸­ä¸ºæ‚¨æä¾›äº†ä¹˜å®¢æ•°æ®ï¼Œä»»åŠ¡æ˜¯å»ºç«‹ä¸€ä¸ªé¢„æµ‹æ¨¡å‹æ¥å›ç­”è¿™ä¸ªé—®é¢˜:â€œä»€ä¹ˆæ ·çš„äººæ›´æœ‰å¯èƒ½å¹¸å­˜ï¼Ÿâ€(ä¸‹é¢é“¾æ¥)ã€‚

[](https://www.kaggle.com/c/titanic/overview) [## æ³°å¦å°¼å…‹å·:æœºå™¨ä»ç¾éš¾ä¸­å­¦ä¹ 

### ä»è¿™é‡Œå¼€å§‹ï¼é¢„æµ‹æ³°å¦å°¼å…‹å·ä¸Šçš„ç”Ÿå­˜å¹¶ç†Ÿæ‚‰ ML åŸºç¡€çŸ¥è¯†

www.kaggle.com](https://www.kaggle.com/c/titanic/overview) 

æˆ‘å°†å±•ç¤ºä¸€äº›æœ‰ç”¨çš„ Python ä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥å¾ˆå®¹æ˜“åœ°ç”¨äºå…¶ä»–ç±»ä¼¼çš„æƒ…å†µ(åªéœ€å¤åˆ¶ã€ç²˜è´´ã€è¿è¡Œ)ï¼Œå¹¶é€šè¿‡æ³¨é‡Šéå†æ¯ä¸€è¡Œä»£ç ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥å¾ˆå®¹æ˜“åœ°å¤åˆ¶è¿™ä¸ªç¤ºä¾‹(ä¸‹é¢æ˜¯å®Œæ•´ä»£ç çš„é“¾æ¥)ã€‚

[](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/machine_learning/example_classification.ipynb) [## mdipietro 09/data science _ äººå·¥æ™ºèƒ½ _ å®ç”¨å·¥å…·

### permalink dissolve GitHub æ˜¯ 4000 å¤šä¸‡å¼€å‘äººå‘˜çš„å®¶å›­ï¼Œä»–ä»¬ä¸€èµ·å·¥ä½œæ¥æ‰˜ç®¡å’Œå®¡æŸ¥ä»£ç ï¼Œç®¡ç†â€¦

github.com](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/machine_learning/example_classification.ipynb) 

ç‰¹åˆ«æ˜¯ï¼Œæˆ‘å°†ç»å†:

*   ç¯å¢ƒè®¾ç½®:å¯¼å…¥åº“å¹¶è¯»å–æ•°æ®
*   æ•°æ®åˆ†æ:ç†è§£å˜é‡çš„æ„ä¹‰å’Œé¢„æµ‹èƒ½åŠ›
*   ç‰¹å¾å·¥ç¨‹:ä»åŸå§‹æ•°æ®ä¸­æå–ç‰¹å¾
*   é¢„å¤„ç†:æ•°æ®åˆ’åˆ†ã€å¤„ç†ç¼ºå¤±å€¼ã€ç¼–ç åˆ†ç±»å˜é‡ã€ç¼©æ”¾
*   ç‰¹å¾é€‰æ‹©:åªä¿ç•™æœ€ç›¸å…³çš„å˜é‡
*   æ¨¡å‹è®¾è®¡:è®­ç»ƒã€è°ƒæ•´è¶…å‚æ•°ã€éªŒè¯ã€æµ‹è¯•
*   ç»©æ•ˆè¯„ä¼°:é˜…è¯»æŒ‡æ ‡
*   å¯è§£é‡Šæ€§:ç†è§£æ¨¡å‹å¦‚ä½•äº§ç”Ÿç»“æœ

## è®¾ç½®

é¦–å…ˆï¼Œæˆ‘éœ€è¦å¯¼å…¥ä»¥ä¸‹åº“ã€‚

```
**## for data**
import **pandas** as pd
import **numpy** as np**## for plotting**
import **matplotlib**.pyplot as plt
import **seaborn** as sns**## for statistical tests**
import **scipy**
import **statsmodels**.formula.api as smf
import statsmodels.api as sm**## for machine learning**
from **sklearn** import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition**## for explainer**
from **lime** import lime_tabular
```

ç„¶åæˆ‘ä¼šæŠŠæ•°æ®è¯»å…¥ç†ŠçŒ«æ•°æ®æ¡†ã€‚

```
dtf = pd.read_csv('data_titanic.csv')
dtf.head()
```

![](img/2246b29c51699cd2d93e6b8094363655.png)

æœ‰å…³åˆ—çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨æ‰€æä¾›çš„æ•°æ®é›†é“¾æ¥ä¸­æ‰¾åˆ°ã€‚

è¯·æ³¨æ„ï¼Œè¡¨æ ¼çš„æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„ä¹˜å®¢(æˆ–è§‚å¯Ÿ)ã€‚å¦‚æœæ‚¨æ­£åœ¨å¤„ç†ä¸€ä¸ªä¸åŒçš„æ•°æ®é›†ï¼Œå®ƒæ²¡æœ‰è¿™æ ·çš„ç»“æ„ï¼Œå…¶ä¸­æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè§‚å¯Ÿï¼Œé‚£ä¹ˆæ‚¨éœ€è¦æ±‡æ€»æ•°æ®å¹¶è½¬æ¢å®ƒã€‚

ç°åœ¨ä¸€åˆ‡éƒ½è®¾ç½®å¥½äº†ï¼Œæˆ‘å°†ä»åˆ†ææ•°æ®å¼€å§‹ï¼Œç„¶åé€‰æ‹©ç‰¹å¾ï¼Œå»ºç«‹æœºå™¨å­¦ä¹ æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ã€‚

æˆ‘ä»¬å¼€å§‹å§ï¼Œå¥½å—ï¼Ÿ

## æ•°æ®åˆ†æ

åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œ[æ¢ç´¢æ€§æ•°æ®åˆ†æ](https://en.wikipedia.org/wiki/Exploratory_data_analysis)æ˜¯å¯¹æ•°æ®é›†çš„ä¸»è¦ç‰¹å¾è¿›è¡Œæ€»ç»“çš„è¿‡ç¨‹ï¼Œä»¥äº†è§£æ•°æ®åœ¨æ­£å¼å»ºæ¨¡æˆ–å‡è®¾æ£€éªŒä»»åŠ¡ä¹‹å¤–è¿˜èƒ½å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆã€‚

æˆ‘æ€»æ˜¯ä»è·å¾—æ•´ä¸ªæ•°æ®é›†çš„æ¦‚è¿°å¼€å§‹ï¼Œç‰¹åˆ«æ˜¯æˆ‘æƒ³çŸ¥é“æœ‰å¤šå°‘**åˆ†ç±»**å’Œ**æ•°å€¼**å˜é‡ï¼Œä»¥åŠ**ç¼ºå¤±æ•°æ®**çš„æ¯”ä¾‹ã€‚è¯†åˆ«å˜é‡çš„ç±»å‹æœ‰æ—¶ä¼šå¾ˆæ£˜æ‰‹ï¼Œå› ä¸ºç±»åˆ«å¯ä»¥ç”¨æ•°å­—è¡¨ç¤º(Su *rvived c* åˆ—ç”± 1 å’Œ 0 ç»„æˆ)ã€‚ä¸ºæ­¤ï¼Œæˆ‘å°†ç¼–å†™ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥å®Œæˆè¿™é¡¹å·¥ä½œ:

```
**'''
Recognize whether a column is numerical or categorical.
:parameter
    :param dtf: dataframe - input data
    :param col: str - name of the column to analyze
    :param max_cat: num - max number of unique values to recognize a column as categorical
:return
    "cat" if the column is categorical or "num" otherwise
'''**
def **utils_recognize_type**(dtf, col, max_cat=20):
    if (dtf[col].dtype == "O") | (dtf[col].nunique() < max_cat):
        return **"cat"**
    else:
        return **"num"**
```

è¿™ä¸ªåŠŸèƒ½éå¸¸æœ‰ç”¨ï¼Œå¯ä»¥ç”¨åœ¨å¾ˆå¤šåœºåˆã€‚ä¸ºäº†ä¸¾ä¾‹è¯´æ˜ï¼Œæˆ‘å°†ç»˜åˆ¶ dataframe çš„ [**çƒ­å›¾**](http://Heat map) ï¼Œä»¥å¯è§†åŒ–åˆ—ç±»å‹å’Œç¼ºå¤±çš„æ•°æ®ã€‚

```
dic_cols = {col:**utils_recognize_type**(dtf, col, max_cat=20) for col in dtf.columns}heatmap = dtf.isnull()
for k,v in dic_cols.items():
 if v == "num":
   heatmap[k] = heatmap[k].apply(lambda x: 0.5 if x is False else 1)
 else:
   heatmap[k] = heatmap[k].apply(lambda x: 0 if x is False else 1)sns.**heatmap**(heatmap, cbar=False).set_title('Dataset Overview')
plt.show()print("\033[1;37;40m Categerocial ", "\033[1;30;41m Numeric ", "\033[1;30;47m NaN ")
```

![](img/67b365ca697f7a83f958d1d74e0e5ee4.png)

æœ‰ 885 è¡Œå’Œ 12 åˆ—:

*   è¡¨ä¸­çš„æ¯ä¸€è¡Œéƒ½ä»£è¡¨ä¸€ä¸ªç”± *PassengerId* æ ‡è¯†çš„ç‰¹å®šä¹˜å®¢(æˆ–è§‚å¯Ÿ)ï¼Œæ‰€ä»¥æˆ‘å°†å®ƒè®¾ç½®ä¸º index(æˆ– SQL çˆ±å¥½è€…çš„è¡¨çš„[ä¸»é”®](https://en.wikipedia.org/wiki/Primary_key))ã€‚
*   *å¹¸å­˜çš„*æ˜¯æˆ‘ä»¬æƒ³è¦ç†è§£å’Œé¢„æµ‹çš„ç°è±¡(æˆ–ç›®æ ‡å˜é‡)ï¼Œæ‰€ä»¥æˆ‘å°†è¯¥åˆ—é‡å‘½åä¸ºâ€œ*Yâ€*ã€‚å®ƒåŒ…å«ä¸¤ä¸ªç±»åˆ«:å¦‚æœä¹˜å®¢å¹¸å­˜ï¼Œåˆ™ä¸º 1ï¼Œå¦åˆ™ä¸º 0ï¼Œå› æ­¤è¿™ä¸ªç”¨ä¾‹æ˜¯ä¸€ä¸ªäºŒå…ƒåˆ†ç±»é—®é¢˜ã€‚
*   *å¹´é¾„*å’Œ*è´¹ç”¨*æ˜¯æ•°å­—å˜é‡ï¼Œè€Œå…¶ä»–æ˜¯åˆ†ç±»å˜é‡ã€‚
*   åªæœ‰*å¹´é¾„*å’Œ*èˆ±å®¤*åŒ…å«ç¼ºå¤±æ•°æ®ã€‚

```
dtf = dtf.set_index("**PassengerId**")dtf = dtf.rename(columns={"**Survived**":"**Y**"})
```

æˆ‘ç›¸ä¿¡å¯è§†åŒ–æ˜¯æ•°æ®åˆ†æçš„æœ€å¥½å·¥å…·ï¼Œä½†æ˜¯ä½ éœ€è¦çŸ¥é“ä»€ä¹ˆæ ·çš„å›¾æ›´é€‚åˆä¸åŒç±»å‹çš„å˜é‡ã€‚å› æ­¤ï¼Œæˆ‘å°†æä¾›ä»£ç æ¥ä¸ºä¸åŒçš„ç¤ºä¾‹ç»˜åˆ¶é€‚å½“çš„å¯è§†åŒ–ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å•å˜é‡åˆ†å¸ƒ(åªæœ‰ä¸€ä¸ªå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒ)ã€‚ä¸€ä¸ª [**æ¡å½¢å›¾**](https://en.wikipedia.org/wiki/Bar_chart) é€‚ç”¨äºç†è§£å•ä¸ª**åˆ†ç±»**å˜é‡çš„æ ‡ç­¾é¢‘ç‡ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ç»˜åˆ¶ç›®æ ‡å˜é‡:

```
**y = "Y"**ax = dtf[y].value_counts().sort_values().plot(kind="barh")
totals= []
for i in ax.patches:
    totals.append(i.get_width())
total = sum(totals)
for i in ax.patches:
     ax.text(i.get_width()+.3, i.get_y()+.20, 
     str(round((i.get_width()/total)*100, 2))+'%', 
     fontsize=10, color='black')
ax.grid(axis="x")
plt.suptitle(y, fontsize=20)
plt.show()
```

![](img/e4f55046be10adab58009cf7154e8e77.png)

å¤šè¾¾ 300 åä¹˜å®¢å¹¸å­˜ï¼Œå¤§çº¦ 550 äººæ²¡æœ‰ï¼Œæ¢å¥è¯è¯´ï¼Œå­˜æ´»ç‡(æˆ–äººå£å¹³å‡å€¼)æ˜¯ 38%ã€‚

æ­¤å¤–ï¼Œä¸€ä¸ª [**ç›´æ–¹å›¾**](https://en.wikipedia.org/wiki/Histogram) å®Œç¾åœ°ç»™å‡ºäº†å•ä¸ª**æ•°å€¼**æ•°æ®çš„åŸºæœ¬åˆ†å¸ƒå¯†åº¦çš„ç²—ç•¥æ„Ÿè§‰ã€‚æˆ‘æ¨èä½¿ç”¨ä¸€ä¸ª [**ç®±çº¿å›¾**](https://en.wikipedia.org/wiki/Box_plot) æ¥å›¾å½¢åŒ–åœ°æç»˜æ•°æ®ç»„çš„å››åˆ†ä½æ•°ã€‚è®©æˆ‘ä»¬ä»¥*å¹´é¾„*å˜é‡ä¸ºä¾‹:

```
**x = "Age"**fig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)
fig.suptitle(x, fontsize=20)**### distribution**
ax[0].title.set_text('distribution')
variable = dtf[x].fillna(dtf[x].mean())
breaks = np.quantile(variable, q=np.linspace(0, 1, 11))
variable = variable[ (variable > breaks[0]) & (variable < 
                    breaks[10]) ]
sns.distplot(variable, hist=True, kde=True, kde_kws={"shade": True}, ax=ax[0])
des = dtf[x].describe()
ax[0].axvline(des["25%"], ls='--')
ax[0].axvline(des["mean"], ls='--')
ax[0].axvline(des["75%"], ls='--')
ax[0].grid(True)
des = round(des, 2).apply(lambda x: str(x))
box = '\n'.join(("min: "+des["min"], "25%: "+des["25%"], "mean: "+des["mean"], "75%: "+des["75%"], "max: "+des["max"]))
ax[0].text(0.95, 0.95, box, transform=ax[0].transAxes, fontsize=10, va='top', ha="right", bbox=dict(boxstyle='round', facecolor='white', alpha=1))**### boxplot** 
ax[1].title.set_text('outliers (log scale)')
tmp_dtf = pd.DataFrame(dtf[x])
tmp_dtf[x] = np.log(tmp_dtf[x])
tmp_dtf.boxplot(column=x, ax=ax[1])
plt.show()
```

![](img/d83c414c165b359c36453732132d62f9.png)

å¹³å‡è€Œè¨€ï¼Œä¹˜å®¢ç›¸å½“å¹´è½»:åˆ†å¸ƒåå‘å·¦ä¾§(å¹³å‡å€¼ä¸º 30 å²ï¼Œç¬¬ 75 ç™¾åˆ†ä½ä¸º 38 å²)ã€‚åŠ ä¸Šç®±å½¢å›¾ä¸­çš„å¼‚å¸¸å€¼ï¼Œå·¦å°¾éƒ¨çš„ç¬¬ä¸€ä¸ªå°–å³°è¡¨ç¤ºæœ‰å¤§é‡çš„å„¿ç«¥ã€‚

æˆ‘å°†æŠŠåˆ†æè¿›è¡Œåˆ°ä¸‹ä¸€ä¸ªå±‚æ¬¡ï¼Œå¹¶ç ”ç©¶åŒå˜é‡åˆ†å¸ƒï¼Œä»¥äº†è§£*å¹´é¾„*æ˜¯å¦å…·æœ‰é¢„æµ‹ *Y* çš„é¢„æµ‹èƒ½åŠ›ã€‚è¿™å°±æ˜¯**ç»å¯¹( *Y* )å¯¹æ•°å€¼(*å¹´é¾„* )** çš„æƒ…å†µï¼Œå› æ­¤æˆ‘å°†è¿™æ ·è¿›è¡Œ:

*   å°†æ€»ä½“(æ•´ä¸ªè§‚å¯Ÿé›†)åˆ†æˆ 2 ä¸ªæ ·æœ¬:Y = 1 (å­˜æ´»)å’Œ Y = 0 (æœªå­˜æ´»)çš„ä¹˜å®¢éƒ¨åˆ†ã€‚
*   ç»˜åˆ¶å¹¶æ¯”è¾ƒä¸¤ä¸ªæ ·æœ¬çš„å¯†åº¦ï¼Œå¦‚æœåˆ†å¸ƒä¸åŒï¼Œåˆ™å˜é‡æ˜¯å¯é¢„æµ‹çš„ï¼Œå› ä¸ºä¸¤ç»„å…·æœ‰ä¸åŒçš„æ¨¡å¼ã€‚
*   å°†æ•°å€¼å˜é‡(*å¹´é¾„*)åˆ†ç»„åˆ°ç®±(å­æ ·æœ¬)ä¸­ï¼Œå¹¶ç»˜åˆ¶æ¯ä¸ªç®±çš„ç»„æˆï¼Œå¦‚æœæ‰€æœ‰ç®±ä¸­ 1 çš„æ¯”ä¾‹ç›¸ä¼¼ï¼Œåˆ™è¯¥å˜é‡ä¸å…·æœ‰é¢„æµ‹æ€§ã€‚
*   ç»˜åˆ¶å¹¶æ¯”è¾ƒä¸¤ä¸ªæ ·æœ¬çš„ç®±çº¿å›¾ï¼Œæ‰¾å‡ºå¼‚å¸¸å€¼çš„ä¸åŒè¡Œä¸ºã€‚

```
**cat, num = "Y", "Age"**fig, ax = plt.subplots(nrows=1, ncols=3,  sharex=False, sharey=False)
fig.suptitle(x+"   vs   "+y, fontsize=20)

**### distribution**
ax[0].title.set_text('density')
for i in dtf[cat].unique():
    sns.distplot(dtf[dtf[cat]==i][num], hist=False, label=i, ax=ax[0])
ax[0].grid(True)**### stacked**
ax[1].title.set_text('bins')
breaks = np.quantile(dtf[num], q=np.linspace(0,1,11))
tmp = dtf.groupby([cat, pd.cut(dtf[num], breaks, duplicates='drop')]).size().unstack().T
tmp = tmp[dtf[cat].unique()]
tmp["tot"] = tmp.sum(axis=1)
for col in tmp.drop("tot", axis=1).columns:
     tmp[col] = tmp[col] / tmp["tot"]
tmp.drop("tot", axis=1).plot(kind='bar', stacked=True, ax=ax[1], legend=False, grid=True)**### boxplot **  
ax[2].title.set_text('outliers')
sns.catplot(x=cat, y=num, data=dtf, kind="box", ax=ax[2])
ax[2].grid(True)
plt.show()
```

![](img/7303e1d387717df21ee93576a67e28f1.png)

è¿™ 3 ä¸ªå›¾åªæ˜¯ç»“è®º*å¹´é¾„*å…·æœ‰é¢„æµ‹æ€§çš„ä¸åŒè§’åº¦ã€‚å¹´è½»ä¹˜å®¢çš„å­˜æ´»ç‡æ›´é«˜:1s åˆ†å¸ƒçš„å·¦å°¾æœ‰ä¸€ä¸ªå°–å³°ï¼Œç¬¬ä¸€ä¸ªç®±(0-16 å²)åŒ…å«æœ€é«˜ç™¾åˆ†æ¯”çš„å¹¸å­˜ä¹˜å®¢ã€‚

å½“â€œè§†è§‰ç›´è§‰â€æ— æ³•è¯´æœä½ æ—¶ï¼Œä½ å¯ä»¥æ±‚åŠ©äºå¤è€çš„ç»Ÿè®¡æ•°æ®æ¥è¿›è¡Œæµ‹è¯•ã€‚åœ¨è¿™ç§åˆ†ç±»(*Y*vs æ•°å€¼(*å¹´é¾„*)çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä¼šä½¿ç”¨ä¸€ä¸ª**o**[**n-way ANOVA æ£€éªŒ**](http://en.wikipedia.org/wiki/F_test#One-way_ANOVA_example) ã€‚åŸºæœ¬ä¸Šæ˜¯æ£€éªŒä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸Šç‹¬ç«‹æ ·æœ¬çš„å‡å€¼æ˜¯å¦æ˜¾è‘—ä¸åŒï¼Œæ‰€ä»¥å¦‚æœ p å€¼è¶³å¤Ÿå°(< 0.05)æ ·æœ¬çš„é›¶å‡è®¾æ„å‘³ç€ç›¸ç­‰å¯ä»¥è¢«æ‹’ç»ã€‚

```
**cat, num = "Y", "Age"**model = smf.**ols**(num+' ~ '+cat, data=dtf).fit()
table = sm.stats.**anova_lm**(model)
p = table["PR(>F)"][0]
coeff, p = None, round(p, 3)
conclusion = "Correlated" if p < 0.05 else "Non-Correlated"
print("Anova F: the variables are", conclusion, "(p-value: "+str(p)+")")
```

![](img/9ee7c4a5fa8c3efe835692542c2a90ff.png)

æ˜¾ç„¶ï¼Œä¹˜å®¢çš„å¹´é¾„å†³å®šäº†ä»–ä»¬çš„ç”Ÿå­˜ã€‚è¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºåœ¨ç”Ÿå‘½å—åˆ°å¨èƒçš„æƒ…å†µä¸‹ï¼Œå½“æ•‘ç”Ÿè‰‡ç­‰ç”Ÿå­˜èµ„æºæœ‰é™æ—¶ï¼Œå¦‡å¥³å’Œå„¿ç«¥çš„ç”Ÿå‘½å°†è¢«é¦–å…ˆæ‹¯æ•‘ï¼Œé€šå¸¸æ˜¯å¼ƒèˆ¹(ä»£ç "[å¦‡å¥³å’Œå„¿ç«¥ä¼˜å…ˆ](https://en.wikipedia.org/wiki/Women_and_children_first))ã€‚

ä¸ºäº†æ£€æŸ¥ç¬¬ä¸€ä¸ªç»“è®ºçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘å¿…é¡»åˆ†æ*æ€§åˆ«*å˜é‡ç›¸å¯¹äºç›®æ ‡å˜é‡çš„è¡Œä¸ºã€‚è¿™æ˜¯ä¸€ä¸ª**åˆ†ç±»( *Y* ) vs åˆ†ç±»(*æ€§åˆ«* )** çš„ä¾‹å­ï¼Œæ‰€ä»¥æˆ‘å°†ç»˜åˆ¶ä¸¤ä¸ªæ¡å½¢å›¾ï¼Œä¸€ä¸ªæ˜¯ä¸¤ä¸ªç±»åˆ«*æ€§åˆ«*(ç”·æ€§å’Œå¥³æ€§)ä¸­ 1 å’Œ 0 çš„æ•°é‡ï¼Œå¦ä¸€ä¸ªæ˜¯ç™¾åˆ†æ¯”ã€‚

```
**x, y = "Sex", "Y"**fig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)
fig.suptitle(x+"   vs   "+y, fontsize=20)**### count**
ax[0].title.set_text('count')
order = dtf.groupby(x)[y].count().index.tolist()
sns.catplot(x=x, hue=y, data=dtf, kind='count', order=order, ax=ax[0])
ax[0].grid(True)**### percentage**
ax[1].title.set_text('percentage')
a = dtf.groupby(x)[y].count().reset_index()
a = a.rename(columns={y:"tot"})
b = dtf.groupby([x,y])[y].count()
b = b.rename(columns={y:0}).reset_index()
b = b.merge(a, how="left")
b["%"] = b[0] / b["tot"] *100
sns.barplot(x=x, y="%", hue=y, data=b,
            ax=ax[1]).get_legend().remove()
ax[1].grid(True)
plt.show()
```

![](img/7bc8762ccb0d2d99a26cbd9e7076ba74.png)

200 å¤šåå¥³æ€§ä¹˜å®¢(å æœºä¸Šå¥³æ€§æ€»æ•°çš„ 75%)å’Œçº¦ 100 åç”·æ€§ä¹˜å®¢(ä¸åˆ° 20%)å¹¸å­˜ã€‚æ¢å¥è¯è¯´ï¼Œå¥³æ€§çš„å­˜æ´»ç‡æ˜¯ 75%,ç”·æ€§æ˜¯ 20%,å› æ­¤æ€§åˆ«æ˜¯å¯ä»¥é¢„æµ‹çš„ã€‚æ­¤å¤–ï¼Œè¿™è¯å®äº†ä»–ä»¬ä¼˜å…ˆè€ƒè™‘å¦‡å¥³å’Œå„¿ç«¥ã€‚

å°±åƒä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹è¯•è¿™ä¸¤ä¸ªå˜é‡çš„ç›¸å…³æ€§ã€‚ç”±äºå®ƒä»¬éƒ½æ˜¯åˆ†ç±»çš„ï¼Œæˆ‘å°†ä½¿ç”¨ C [**å¡æ–¹æ£€éªŒ:**](https://en.wikipedia.org/wiki/Chi-square_test) å‡è®¾ä¸¤ä¸ªå˜é‡æ˜¯ç‹¬ç«‹çš„(é›¶å‡è®¾)ï¼Œå®ƒå°†æ£€éªŒè¿™äº›å˜é‡çš„åˆ—è”è¡¨çš„å€¼æ˜¯å¦æ˜¯å‡åŒ€åˆ†å¸ƒçš„ã€‚å¦‚æœ p å€¼è¶³å¤Ÿå°(< 0.05)ï¼Œå¯ä»¥æ‹’ç»é›¶å‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥è¯´è¿™ä¸¤ä¸ªå˜é‡å¯èƒ½æ˜¯ç›¸å…³çš„ã€‚å¯ä»¥è®¡ç®— C [**ramer çš„ V**](https://en.wikipedia.org/wiki/Cram%C3%A9r's_V) **t** è¿™æ˜¯ä»è¯¥æµ‹è¯•å¾—å‡ºçš„ç›¸å…³æ€§åº¦é‡ï¼Œå®ƒæ˜¯å¯¹ç§°çš„(å°±åƒä¼ ç»Ÿçš„çš®å°”é€Šç›¸å…³æ€§)ï¼ŒèŒƒå›´åœ¨ 0 å’Œ 1 ä¹‹é—´(ä¸åƒä¼ ç»Ÿçš„çš®å°”é€Šç›¸å…³æ€§ï¼Œæ²¡æœ‰è´Ÿå€¼)ã€‚

```
**x, y = "Sex", "Y"**cont_table = pd.crosstab(index=dtf[x], columns=dtf[y])
chi2_test = scipy.stats.**chi2_contingency**(cont_table)
chi2, p = chi2_test[0], chi2_test[1]
n = cont_table.sum().sum()
phi2 = chi2/n
r,k = cont_table.shape
phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))
rcorr = r-((r-1)**2)/(n-1)
kcorr = k-((k-1)**2)/(n-1)
coeff = np.sqrt(phi2corr/min((kcorr-1), (rcorr-1)))
coeff, p = round(coeff, 3), round(p, 3)
conclusion = "Significant" if p < 0.05 else "Non-Significant"
print("Cramer Correlation:", coeff, conclusion, "(p-value:"+str(p)+")")
```

![](img/3b5a78769585f056ed75897828a1058e.png)

*å¹´é¾„*å’Œ*æ€§åˆ«*éƒ½æ˜¯é¢„æµ‹ç‰¹å¾çš„ä¾‹å­ï¼Œä½†å¹¶ä¸æ˜¯æ•°æ®é›†ä¸­çš„æ‰€æœ‰åˆ—éƒ½æ˜¯è¿™æ ·ã€‚ä¾‹å¦‚ï¼Œ*èˆ±*ä¼¼ä¹æ˜¯ä¸€ä¸ª**æ— ç”¨å˜é‡**ï¼Œå› ä¸ºå®ƒä¸æä¾›ä»»ä½•æœ‰ç”¨çš„ä¿¡æ¯ï¼Œæœ‰å¤ªå¤šçš„ç¼ºå¤±å€¼å’Œç±»åˆ«ã€‚

åº”è¯¥å¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªå˜é‡è¿›è¡Œè¿™ç§åˆ†æï¼Œä»¥å†³å®šå“ªäº›åº”è¯¥ä½œä¸ºæ½œåœ¨ç‰¹å¾ä¿ç•™ï¼Œå“ªäº›å› ä¸ºä¸å…·æœ‰é¢„æµ‹æ€§è€Œå¯ä»¥æ”¾å¼ƒ(æŸ¥çœ‹å®Œæ•´ä»£ç çš„é“¾æ¥)ã€‚

## ç‰¹å¾å·¥ç¨‹

æ˜¯æ—¶å€™ä½¿ç”¨é¢†åŸŸçŸ¥è¯†ä»åŸå§‹æ•°æ®åˆ›å»ºæ–°è¦ç´ äº†ã€‚æˆ‘å°†æä¾›ä¸€ä¸ªä¾‹å­:æˆ‘å°†å°è¯•é€šè¿‡ä» *Cabin* åˆ—ä¸­æå–ä¿¡æ¯æ¥åˆ›å»ºä¸€ä¸ªæœ‰ç”¨çš„ç‰¹å¾ã€‚æˆ‘å‡è®¾æ¯ä¸ªèˆ±å·å¼€å¤´çš„å­—æ¯(å³â€œ ***B*** *96* â€)è¡¨ç¤ºæŸç§åŒºåŸŸï¼Œä¹Ÿè®¸æœ‰ä¸€äº›å¹¸è¿åŒºåŸŸé è¿‘æ•‘ç”Ÿè‰‡ã€‚æˆ‘å°†é€šè¿‡æå–æ¯ä¸ªèˆ±å®¤çš„æˆªé¢æ¥æŒ‰ç»„æ€»ç»“è§‚å¯Ÿç»“æœ:

```
**## Create new column**
dtf["**Cabin_section**"] = dtf["**Cabin**"].apply(lambda x: str(x)[0])**## Plot contingency table** cont_table = pd.crosstab(index=dtf["**Cabin_section"**], 
             columns=dtf["**Pclass**"], values=dtf["**Y**"], aggfunc="sum")sns.**heatmap**(cont_table, annot=True, cmap="YlGnBu", fmt='.0f',
            linewidths=.5).set_title( 
            'Cabin_section vs Pclass (filter: Y)' )
```

![](img/6c216a48823f09b8364514e4c51dd629.png)

è¯¥å›¾æ˜¾ç¤ºäº†å¹¸å­˜è€…åœ¨å®¢èˆ±å„éƒ¨åˆ†å’Œå„ç­‰çº§ä¸­çš„åˆ†å¸ƒæƒ…å†µ(7 åå¹¸å­˜è€…åœ¨ A åŒºï¼Œ35 ååœ¨ B åŒºâ€¦â€¦)ã€‚å¤§å¤šæ•°åŒºæ®µè¢«åˆ†é…ç»™ç¬¬ä¸€ç±»å’Œç¬¬äºŒç±»ï¼Œè€Œå¤§å¤šæ•°ç¼ºå¤±åŒºæ®µ(" *n"* )å±äºç¬¬ä¸‰ç±»ã€‚æˆ‘å°†ä¿ç•™è¿™ä¸ªæ–°ç‰¹æ€§ï¼Œè€Œä¸æ˜¯åˆ— *Cabin:*

![](img/4ec62398ac8f632b3681b34b2c40a8a9.png)

## é¢„å¤„ç†

æ•°æ®é¢„å¤„ç†æ˜¯å‡†å¤‡åŸå§‹æ•°æ®ä»¥ä½¿å…¶é€‚åˆæœºå™¨å­¦ä¹ æ¨¡å‹çš„é˜¶æ®µã€‚ç‰¹åˆ«æ˜¯:

1.  æ¯ä¸ªè§‚å¯Ÿå¿…é¡»ç”¨ä¸€è¡Œæ¥è¡¨ç¤ºï¼Œæ¢å¥è¯è¯´ï¼Œä¸èƒ½ç”¨ä¸¤è¡Œæ¥æè¿°åŒä¸€ä¸ªä¹˜å®¢ï¼Œå› ä¸ºå®ƒä»¬å°†è¢«æ¨¡å‹åˆ†åˆ«å¤„ç†(æ•°æ®é›†å·²ç»æ˜¯è¿™æ ·çš„å½¢å¼ï¼Œæ‰€ä»¥âœ…).è€Œä¸”æ¯ä¸€åˆ—éƒ½åº”è¯¥æ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œæ‰€ä»¥ä½ ä¸åº”è¯¥ç”¨ *PassengerId* ä½œä¸ºé¢„æµ‹å™¨ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿™ç§è¡¨å«åšâ€œ**ç‰¹å¾çŸ©é˜µ**â€ã€‚
2.  æ•°æ®é›†å¿…é¡»**åˆ’åˆ†ä¸º**è‡³å°‘ä¸¤ç»„:æ¨¡å‹åº”åœ¨æ•°æ®é›†çš„é‡è¦éƒ¨åˆ†(æ‰€è°“çš„â€œè®­ç»ƒé›†â€)ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨è¾ƒå°çš„æ•°æ®é›†(â€œæµ‹è¯•é›†â€)ä¸Šè¿›è¡Œæµ‹è¯•ã€‚
3.  **ç¼ºå¤±å€¼**åº”è¯¥ç”¨ä¸œè¥¿æ›¿æ¢ï¼Œå¦åˆ™ä½ çš„æ¨¡å‹å¯èƒ½ä¼šå‡ºé—®é¢˜ã€‚
4.  **åˆ†ç±»æ•°æ®**å¿…é¡»ç¼–ç ï¼Œè¿™æ„å‘³ç€å°†æ ‡ç­¾è½¬æ¢æˆæ•´æ•°ï¼Œå› ä¸ºæœºå™¨å­¦ä¹ æœŸæœ›çš„æ˜¯æ•°å­—è€Œä¸æ˜¯å­—ç¬¦ä¸²ã€‚
5.  å¯¹æ•°æ®è¿›è¡Œ**ç¼©æ”¾**æ˜¯ä¸€ç§å¾ˆå¥½çš„åšæ³•ï¼Œè¿™æœ‰åŠ©äºåœ¨ç‰¹å®šèŒƒå›´å†…å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¹¶åŠ å¿«ç®—æ³•çš„è®¡ç®—é€Ÿåº¦ã€‚

å¥½çš„ï¼Œè®©æˆ‘ä»¬ä»**åˆ’åˆ†æ•°æ®é›†**å¼€å§‹ã€‚å½“æŠŠæ•°æ®åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶ï¼Œä½ å¿…é¡»éµå¾ªä¸€ä¸ªåŸºæœ¬è§„åˆ™:è®­ç»ƒé›†ä¸­çš„è¡Œä¸åº”è¯¥å‡ºç°åœ¨æµ‹è¯•é›†ä¸­ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šçœ‹åˆ°ç›®æ ‡å€¼ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥ç†è§£ç°è±¡ã€‚æ¢å¥è¯è¯´ï¼Œæ¨¡å‹å·²ç»çŸ¥é“è®­ç»ƒè§‚å¯Ÿçš„æ­£ç¡®ç­”æ¡ˆï¼Œåœ¨è¿™äº›åŸºç¡€ä¸Šæµ‹è¯•å°±åƒä½œå¼Šã€‚æˆ‘è§è¿‡å¾ˆå¤šäººæ¨é”€ä»–ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå£°ç§°æœ‰ 99.99%çš„å‡†ç¡®ç‡ï¼Œä½†å®é™…ä¸Šå´å¿½ç•¥äº†è¿™æ¡è§„åˆ™ã€‚å¹¸è¿çš„æ˜¯ï¼ŒS *cikit-learn* åŒ…çŸ¥é“:

```
**## split data**
dtf_train, dtf_test = **model_selection**.**train_test_split**(dtf, 
                      test_size=0.3)**## print info**
print("X_train shape:", dtf_train.drop("Y",axis=1).shape, "| X_test shape:", dtf_test.drop("Y",axis=1).shape)
print("y_train mean:", round(np.mean(dtf_train["Y"]),2), "| y_test mean:", round(np.mean(dtf_test["Y"]),2))
print(dtf_train.shape[1], "features:", dtf_train.drop("Y",axis=1).columns.to_list())
```

![](img/9bae2b2b50595c91a99d03c0dcde28d0.png)

ä¸‹ä¸€æ­¥:*å¹´é¾„*åˆ—åŒ…å«ä¸€äº›éœ€è¦å¤„ç†çš„**ç¼ºå¤±æ•°æ®** (19%)ã€‚åœ¨å®è·µä¸­ï¼Œæ‚¨å¯ä»¥ç”¨ä¸€ä¸ªç‰¹å®šçš„å€¼æ¥æ›¿æ¢ä¸¢å¤±çš„æ•°æ®ï¼Œæ¯”å¦‚ 9999ï¼Œå®ƒå¯ä»¥è·Ÿè¸ªä¸¢å¤±çš„ä¿¡æ¯ï¼Œä½†ä¼šæ”¹å˜å˜é‡çš„åˆ†å¸ƒã€‚æˆ–è€…ï¼Œä½ å¯ä»¥ä½¿ç”¨åˆ—çš„å¹³å‡å€¼ï¼Œå°±åƒæˆ‘è¦åšçš„ã€‚æˆ‘æƒ³å¼ºè°ƒçš„æ˜¯ï¼Œä»æœºå™¨å­¦ä¹ çš„è§’åº¦æ¥çœ‹ï¼Œé¦–å…ˆåˆ†æˆè®­ç»ƒå’Œæµ‹è¯•ï¼Œç„¶åä»…ç”¨è®­ç»ƒé›†çš„å¹³å‡å€¼æ›¿æ¢ *NAs* æ˜¯æ­£ç¡®çš„ã€‚

```
dtf_train["Age"] = dtf_train["Age"].**fillna**(dtf_train["Age"].**mean**())
```

ä»ç„¶æœ‰ä¸€äº›**åˆ†ç±»æ•°æ®**åº”è¯¥è¢«ç¼–ç ã€‚ä¸¤ç§æœ€å¸¸è§çš„ç¼–ç å™¨æ˜¯æ ‡ç­¾ç¼–ç å™¨(æ¯ä¸ªå”¯ä¸€çš„æ ‡ç­¾æ˜ å°„åˆ°ä¸€ä¸ªæ•´æ•°)å’Œä¸€ä½çƒ­ç¼–ç å™¨(æ¯ä¸ªæ ‡ç­¾æ˜ å°„åˆ°ä¸€ä¸ªäºŒè¿›åˆ¶å‘é‡)ã€‚ç¬¬ä¸€ç§æ–¹æ³•åªé€‚ç”¨äºæ™®é€šæ•°æ®ã€‚å¦‚æœåº”ç”¨äºæ²¡æœ‰æ™®é€šæ€§çš„åˆ—ï¼Œå¦‚ *Sex* ï¼Œå®ƒä¼šå°†å‘é‡*ã€m*aleï¼Œfemaleï¼Œfemaleï¼Œmaleï¼Œmaleï¼Œâ€¦ã€‘è½¬æ¢ä¸ºã€1ï¼Œ2ï¼Œ2ï¼Œ1ï¼Œâ€¦ã€‘,æˆ‘ä»¬ä¼šå¾—åˆ°é‚£ä¸ª female > maleï¼Œå¹³å‡å€¼ä¸º 1.5ï¼Œè¿™æ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚å¦ä¸€æ–¹é¢ï¼ŒOne-Hot-Encoder ä¼šå°†ä¹‹å‰çš„ç¤ºä¾‹è½¬æ¢ä¸ºä¸¤ä¸ª[è™šæ‹Ÿå˜é‡](https://en.wikipedia.org/wiki/Dummy_variable_(statistics))(äºŒåˆ†é‡åŒ–å˜é‡):Mal *e [1* ï¼Œ0ï¼Œ0ï¼Œ1ï¼Œâ€¦]å’Œ Fem *ale [0* ï¼Œ1ï¼Œ1ï¼Œ0ï¼Œâ€¦]ã€‚å®ƒçš„ä¼˜ç‚¹æ˜¯ç»“æœæ˜¯äºŒè¿›åˆ¶çš„è€Œä¸æ˜¯æœ‰åºçš„ï¼Œå¹¶ä¸”ä¸€åˆ‡éƒ½ä½äºæ­£äº¤å‘é‡ç©ºé—´ä¸­ï¼Œä½†æ˜¯å…·æœ‰é«˜åŸºæ•°çš„ç‰¹æ€§å¯èƒ½ä¼šå¯¼è‡´ç»´æ•°é—®é¢˜ã€‚æˆ‘å°†ä½¿ç”¨ One-Hot-Encoding æ–¹æ³•ï¼Œå°† 1 ä¸ªå…·æœ‰ n ä¸ªå”¯ä¸€å€¼çš„åˆ†ç±»åˆ—è½¬æ¢ä¸º n-1 ä¸ªè™šæ‹Ÿåˆ—ã€‚è®©æˆ‘ä»¬ä»¥ç¼–ç *æ€§åˆ«*ä¸º*T21 ä¸ºä¾‹:*

```
**## create dummy**
dummy = pd.get_dummies(dtf_train["**Sex**"], 
                       prefix="Sex",drop_first=True)
dtf_train= pd.concat([dtf_train, dummy], axis=1)
print( dtf_train.filter(like="Sex", axis=1).head() )**## drop the original categorical column**
dtf = dtf_train.drop("**Sex**", axis=1)
```

![](img/95da715165894e174ece18bcabaeffd2.png)

æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ï¼Œæˆ‘å°†**ç¼©æ”¾ç‰¹å¾**ã€‚æœ‰å‡ ç§ä¸åŒçš„æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘å°†åªä»‹ç»æœ€å¸¸ç”¨çš„æ–¹æ³•:æ ‡å‡†ç¼©æ”¾å™¨å’Œæœ€å°æœ€å¤§ç¼©æ”¾å™¨ã€‚ç¬¬ä¸€ç§å‡è®¾æ•°æ®å‘ˆæ­£æ€åˆ†å¸ƒï¼Œå¹¶å¯¹å…¶è¿›è¡Œé‡æ–°è°ƒæ•´ï¼Œä½¿åˆ†å¸ƒä»¥ 0 ä¸ºä¸­å¿ƒï¼Œæ ‡å‡†å·®ä¸º 1ã€‚ç„¶è€Œï¼Œå½“è®¡ç®—ç¼©å°ç‰¹å¾å€¼èŒƒå›´çš„ç»éªŒå¹³å‡å€¼å’Œæ ‡å‡†åå·®æ—¶ï¼Œç¦»ç¾¤å€¼å…·æœ‰å½±å“ï¼Œå› æ­¤è¯¥ç¼©æ”¾å™¨ä¸èƒ½åœ¨ç¦»ç¾¤å€¼å­˜åœ¨æ—¶ä¿è¯å¹³è¡¡çš„ç‰¹å¾å°ºåº¦ã€‚å¦ä¸€æ–¹é¢ï¼Œæœ€å°æœ€å¤§ç¼©æ”¾å™¨é‡æ–°ç¼©æ”¾æ•°æ®é›†ï¼Œä½¿æ‰€æœ‰ç‰¹å¾å€¼éƒ½åœ¨åŒä¸€èŒƒå›´å†…(0â€“1)ã€‚å®ƒå—ç¦»ç¾¤å€¼çš„å½±å“è¾ƒå°ï¼Œä½†å‹ç¼©äº†ä¸€ä¸ªç‹­çª„èŒƒå›´å†…çš„æ‰€æœ‰å†…è”å€¼ã€‚å› ä¸ºæˆ‘çš„æ•°æ®ä¸æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼Œæ‰€ä»¥æˆ‘å°†ä½¿ç”¨æœ€å°æœ€å¤§ç¼©æ”¾å™¨:

```
scaler = **preprocessing**.**MinMaxScaler**(feature_range=(0,1))
X = scaler.fit_transform(dtf_train.drop("Y", axis=1))dtf_scaled= pd.DataFrame(X, columns=dtf_train.drop("Y", axis=1).columns, index=dtf_train.index)
dtf_scaled["Y"] = dtf_train["Y"]
dtf_scaled.head()
```

![](img/eab93edce1b9eda3527143c28aa18746.png)

## ç‰¹å¾é€‰æ‹©

ç‰¹å¾é€‰æ‹©æ˜¯é€‰æ‹©ç›¸å…³å˜é‡çš„å­é›†æ¥æ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¿‡ç¨‹ã€‚å®ƒä½¿æ¨¡å‹æ›´å®¹æ˜“è§£é‡Šï¼Œå¹¶å‡å°‘è¿‡åº¦æ‹Ÿåˆ(å½“æ¨¡å‹é€‚åº”è®­ç»ƒæ•°æ®è¿‡å¤šï¼Œå¹¶ä¸”åœ¨è®­ç»ƒé›†ä¹‹å¤–è¡¨ç°ä¸ä½³æ—¶)ã€‚

åœ¨æ•°æ®åˆ†ææœŸé—´ï¼Œæˆ‘å·²ç»é€šè¿‡æ’é™¤ä¸ç›¸å…³çš„åˆ—è¿›è¡Œäº†ç¬¬ä¸€æ¬¡â€œæ‰‹åŠ¨â€ç‰¹å¾é€‰æ‹©ã€‚ç°åœ¨ä¼šæœ‰ä¸€ç‚¹ä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬å‡è®¾çŸ©é˜µä¸­çš„æ‰€æœ‰ç‰¹æ€§éƒ½æ˜¯ç›¸å…³çš„ï¼Œæˆ‘ä»¬æƒ³å»æ‰ä¸å¿…è¦çš„ç‰¹æ€§ã€‚å½“ä¸€ä¸ªç‰¹æ€§ä¸æ˜¯å¿…éœ€çš„æ—¶å€™ã€‚ç­”æ¡ˆå¾ˆç®€å•:å½“æœ‰æ›´å¥½çš„å¯¹ç­‰ç‰©ï¼Œæˆ–è€…åšåŒæ ·å·¥ä½œä½†æ›´å¥½çš„å¯¹ç­‰ç‰©æ—¶ã€‚

æˆ‘ç”¨ä¸€ä¸ªä¾‹å­æ¥è§£é‡Š: *Pclass* ä¸ *Cabin_section* é«˜åº¦ç›¸å…³ï¼Œå› ä¸ºï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼ŒæŸäº›éƒ¨åˆ†ä½äºä¸€ç­‰èˆ±ï¼Œè€Œå…¶ä»–éƒ¨åˆ†ä½äºäºŒç­‰èˆ±ã€‚è®©æˆ‘ä»¬è®¡ç®—ç›¸å…³çŸ©é˜µæ¥çœ‹çœ‹:

```
corr_matrix = dtf.copy()
for col in corr_matrix.columns:
    if corr_matrix[col].dtype == "O":
         corr_matrix[col] = corr_matrix[col].factorize(sort=True)[0]corr_matrix = corr_matrix.**corr**(method="pearson")
sns.heatmap(corr_matrix, vmin=-1., vmax=1., annot=True, fmt='.2f', cmap="YlGnBu", cbar=True, linewidths=0.5)
plt.title("pearson correlation")
```

![](img/588fd55ca48e3b40322600cdeea81d06.png)

*Pclass* å’Œ *Cabin_section* ä¸­çš„ä¸€ä¸ªå¯èƒ½æ˜¯ä¸å¿…è¦çš„ï¼Œæˆ‘ä»¬å¯ä»¥å†³å®šä¸¢å¼ƒå®ƒå¹¶ä¿ç•™æœ€æœ‰ç”¨çš„ä¸€ä¸ª(å³å…·æœ‰æœ€ä½ p å€¼çš„ä¸€ä¸ªæˆ–æœ€èƒ½é™ä½ç†µçš„ä¸€ä¸ª)ã€‚

æˆ‘å°†å±•ç¤ºä¸¤ç§ä¸åŒçš„æ–¹æ³•æ¥æ‰§è¡Œè‡ªåŠ¨ç‰¹å¾é€‰æ‹©:é¦–å…ˆï¼Œæˆ‘å°†ä½¿ç”¨æ­£åˆ™åŒ–æ–¹æ³•å¹¶å°†å…¶ä¸ä¹‹å‰å·²ç»æåˆ°çš„ ANOVA æµ‹è¯•è¿›è¡Œæ¯”è¾ƒï¼Œç„¶åæˆ‘å°†å±•ç¤ºå¦‚ä½•ä»é›†æˆæ–¹æ³•ä¸­è·å¾—ç‰¹å¾é‡è¦æ€§ã€‚

[**å¥—ç´¢æ­£åˆ™åŒ–**](https://en.wikipedia.org/wiki/Lasso_(statistics)) æ˜¯ä¸€ç§å›å½’åˆ†ææ–¹æ³•ï¼Œæ‰§è¡Œå˜é‡é€‰æ‹©å’Œæ­£åˆ™åŒ–ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

```
X = dtf_train.drop("Y", axis=1).values
y = dtf_train["Y"].values
feature_names = dtf_train.drop("Y", axis=1).columns**## Anova**
selector = **feature_selection.SelectKBest**(score_func=  
               feature_selection.f_classif, k=10).fit(X,y)
anova_selected_features = feature_names[selector.get_support()]

**## Lasso regularization**
selector = **feature_selection.SelectFromModel**(estimator= 
              linear_model.LogisticRegression(C=1, penalty="l1", 
              solver='liblinear'), max_features=10).fit(X,y)
lasso_selected_features = feature_names[selector.get_support()]

**## Plot** dtf_features = pd.DataFrame({"features":feature_names})
dtf_features["anova"] = dtf_features["features"].apply(lambda x: "anova" if x in anova_selected_features else "")
dtf_features["num1"] = dtf_features["features"].apply(lambda x: 1 if x in anova_selected_features else 0)
dtf_features["lasso"] = dtf_features["features"].apply(lambda x: "lasso" if x in lasso_selected_features else "")
dtf_features["num2"] = dtf_features["features"].apply(lambda x: 1 if x in lasso_selected_features else 0)
dtf_features["method"] = dtf_features[["anova","lasso"]].apply(lambda x: (x[0]+" "+x[1]).strip(), axis=1)
dtf_features["selection"] = dtf_features["num1"] + dtf_features["num2"]
sns.**barplot**(y="features", x="selection", hue="method", data=dtf_features.sort_values("selection", ascending=False), dodge=False)
```

![](img/e124811fdc3ce486a618fef7107fff7a.png)

è“è‰²è¦ç´ æ˜¯é€šè¿‡æ–¹å·®åˆ†æå’Œ LASSO é€‰æ‹©çš„è¦ç´ ï¼Œå…¶ä»–è¦ç´ æ˜¯é€šè¿‡ä¸¤ç§æ–¹æ³•ä¸­çš„ä¸€ç§é€‰æ‹©çš„ã€‚

[**éšæœºæ£®æ—**](https://en.wikipedia.org/wiki/Random_forest) æ˜¯ä¸€ç§é›†æˆæ–¹æ³•ï¼Œç”±è®¸å¤šå†³ç­–æ ‘ç»„æˆï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹éƒ½æ˜¯å•ä¸ªè¦ç´ ä¸Šçš„ä¸€ä¸ªæ¡ä»¶ï¼Œæ—¨åœ¨å°†æ•°æ®é›†ä¸€åˆ†ä¸ºäºŒï¼Œä»¥ä¾¿ç›¸ä¼¼çš„å“åº”å€¼æœ€ç»ˆå‡ºç°åœ¨åŒä¸€ç»„ä¸­ã€‚ç‰¹å¾é‡è¦æ€§æ˜¯æ ¹æ®æ¯ä¸ªç‰¹å¾å‡å°‘æ ‘ä¸­ç†µçš„å¤šå°‘æ¥è®¡ç®—çš„ã€‚

```
X = dtf_train.drop("Y", axis=1).values
y = dtf_train["Y"].values
feature_names = dtf_train.drop("Y", axis=1).columns.tolist()**## Importance**
model = ensemble.**RandomForestClassifier**(n_estimators=100,
                      criterion="entropy", random_state=0)
model.fit(X,y)
importances = model.**feature_importances_****## Put in a pandas dtf**
dtf_importances = pd.DataFrame({"IMPORTANCE":importances, 
            "VARIABLE":feature_names}).sort_values("IMPORTANCE", 
            ascending=False)
dtf_importances['cumsum'] =  
            dtf_importances['IMPORTANCE'].cumsum(axis=0)
dtf_importances = dtf_importances.set_index("VARIABLE")

**##** **Plot**
fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False)
fig.suptitle("Features Importance", fontsize=20)
ax[0].title.set_text('variables')
    dtf_importances[["IMPORTANCE"]].sort_values(by="IMPORTANCE").plot(
                kind="barh", legend=False, ax=ax[0]).grid(axis="x")
ax[0].set(ylabel="")
ax[1].title.set_text('cumulative')
dtf_importances[["cumsum"]].plot(kind="line", linewidth=4, 
                                 legend=False, ax=ax[1])
ax[1].set(xlabel="", xticks=np.arange(len(dtf_importances)), 
          xticklabels=dtf_importances.index)
plt.xticks(rotation=70)
plt.grid(axis='both')
plt.show()
```

![](img/3f5da971c94b0dfa7ed76b91094dc097.png)

å¾ˆæœ‰æ„æ€çš„æ˜¯ï¼Œ*å¹´é¾„*å’Œ*ç¥¨ä»·ã€*è¿™ä¸¤ä¸ªè¿™æ¬¡æœ€é‡è¦çš„ç‰¹å¾ï¼Œä»¥å‰å¹¶ä¸æ˜¯æœ€é‡è¦çš„ç‰¹å¾ï¼Œç›¸å*å®¢èˆ± _ E æ®µ*ã€ *F* å’Œ *D* åœ¨è¿™é‡Œä¼¼ä¹å¹¶ä¸å¤ªæœ‰ç”¨ã€‚

å°±æˆ‘ä¸ªäººè€Œè¨€ï¼Œæˆ‘æ€»æ˜¯å°½é‡å°‘ç”¨ä¸€äº›åŠŸèƒ½ï¼Œæ‰€ä»¥åœ¨è¿™é‡Œæˆ‘é€‰æ‹©äº†ä»¥ä¸‹å‡ ä¸ªï¼Œå¹¶ç»§ç»­è¿›è¡Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„è®¾è®¡ã€è®­ç»ƒã€æµ‹è¯•å’Œè¯„ä¼°:

```
X_names = ["Age", "Fare", "Sex_male", "SibSp", "Pclass_3", "Parch",
"Cabin_section_n", "Embarked_S", "Pclass_2", "Cabin_section_F", "Cabin_section_E", "Cabin_section_D"]X_train = dtf_train[X_names].values
y_train = dtf_train["Y"].valuesX_test = dtf_test[X_names].values
y_test = dtf_test["Y"].values
```

è¯·æ³¨æ„ï¼Œåœ¨ä½¿ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹ä¹‹å‰ï¼Œæ‚¨å¿…é¡»å¯¹å…¶è¿›è¡Œé¢„å¤„ç†ï¼Œå°±åƒæˆ‘ä»¬å¯¹è®­ç»ƒæ•°æ®æ‰€åšçš„é‚£æ ·ã€‚

## æ¨¡å‹è®¾è®¡

æœ€åï¼Œæ˜¯æ—¶å€™å»ºç«‹æœºå™¨å­¦ä¹ æ¨¡å‹äº†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©ä¸€ç§ç®—æ³•ï¼Œå®ƒèƒ½å¤Ÿä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å¦‚ä½•é€šè¿‡æœ€å°åŒ–ä¸€äº›è¯¯å·®å‡½æ•°æ¥è¯†åˆ«ç›®æ ‡å˜é‡çš„ä¸¤ä¸ªç±»åˆ«ã€‚

![](img/284d3d42aeeaa4f313e2b25343334cc7.png)

æ¥æº: [scikit-learn](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)

æˆ‘å»ºè®®æ€»æ˜¯å°è¯•ä¸€ä¸ª [**æ¸å˜æå‡**](https://en.wikipedia.org/wiki/Gradient_boosting) ç®—æ³•(åƒ XGBoost)ã€‚è¿™æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå®ƒä»¥å¼±é¢„æµ‹æ¨¡å‹çš„é›†åˆçš„å½¢å¼äº§ç”Ÿé¢„æµ‹æ¨¡å‹ï¼Œé€šå¸¸æ˜¯å†³ç­–æ ‘ã€‚åŸºæœ¬ä¸Šï¼Œå®ƒç±»ä¼¼äºä¸€ä¸ªéšæœºçš„æ£®æ—ï¼Œä¸åŒä¹‹å¤„åœ¨äºæ¯æ£µæ ‘éƒ½ç¬¦åˆå‰ä¸€æ£µæ ‘çš„è¯¯å·®ã€‚

![](img/02deb3ce2928935cb5df21ff45afac87.png)

æ¥æº: [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)

æœ‰å¾ˆå¤šè¶…å‚æ•°ï¼Œæ²¡æœ‰ä»€ä¹ˆæ˜¯æœ€å¥½çš„é€šç”¨è§„åˆ™ï¼Œæ‰€ä»¥ä½ åªéœ€è¦æ‰¾åˆ°æ›´é€‚åˆä½ çš„æ•°æ®çš„æ­£ç¡®ç»„åˆã€‚æ‚¨å¯ä»¥æ‰‹åŠ¨è¿›è¡Œä¸åŒçš„å°è¯•ï¼Œæˆ–è€…è®©è®¡ç®—æœºé€šè¿‡ GridSearch(å°è¯•æ¯ç§å¯èƒ½çš„ç»„åˆï¼Œä½†éœ€è¦æ—¶é—´)æˆ– RandomSearch(éšæœºå°è¯•å›ºå®šæ¬¡æ•°çš„è¿­ä»£)æ¥å®Œæˆè¿™é¡¹ç¹ççš„å·¥ä½œã€‚æˆ‘å°†å°è¯•å¯¹æˆ‘çš„**è¶…å‚æ•°è°ƒæ•´**è¿›è¡Œéšæœºæœç´¢:æœºå™¨å°†é€šè¿‡è®­ç»ƒæ•°æ®è¿­ä»£ n æ¬¡(1000 æ¬¡)ä»¥æ‰¾åˆ°å‚æ•°ç»„åˆ(åœ¨ä¸‹é¢çš„ä»£ç ä¸­æŒ‡å®š),æœ€å¤§åŒ–ç”¨ä½œ KPI(å‡†ç¡®æ€§ï¼Œæ­£ç¡®é¢„æµ‹æ•°ä¸è¾“å…¥æ ·æœ¬æ€»æ•°çš„æ¯”ç‡)çš„è¯„åˆ†å‡½æ•°:

```
**## call model**
model = ensemble.**GradientBoostingClassifier**()**## define hyperparameters combinations to try** param_dic = {'**learning_rate**':[0.15,0.1,0.05,0.01,0.005,0.001],      *#weighting factor for the corrections by new trees when added to the model* '**n_estimators**':[100,250,500,750,1000,1250,1500,1750],  *#number of trees added to the model*
'**max_depth**':[2,3,4,5,6,7],    *#maximum depth of the tree*
'**min_samples_split**':[2,4,6,8,10,20,40,60,100],    *#sets the minimum number of samples to split*
'**min_samples_leaf**':[1,3,5,7,9],     *#the minimum number of samples to form a leaf* '**max_features**':[2,3,4,5,6,7],     *#square root of features is usually a good starting point*
'**subsample**':[0.7,0.75,0.8,0.85,0.9,0.95,1]}       *#the fraction of samples to be used for fitting the individual base learners. Values lower than 1 generally lead to a reduction of variance and an increase in bias.***## random search**
random_search = model_selection.**RandomizedSearchCV**(model, 
       param_distributions=param_dic, n_iter=1000, 
       scoring="accuracy").fit(X_train, y_train)print("Best Model parameters:", random_search.best_params_)
print("Best Model mean accuracy:", random_search.best_score_)model = random_search.best_estimator_
```

![](img/e4840d9d2003b13faec0dbbbe578d4c0.png)

é…·ï¼Œè¿™æ˜¯æœ€å¥½çš„æ¨¡å‹ï¼Œå¹³å‡ç²¾åº¦ä¸º 0.85ï¼Œå› æ­¤åœ¨æµ‹è¯•é›†ä¸Šå¯èƒ½æœ‰ 85%çš„é¢„æµ‹æ˜¯æ­£ç¡®çš„ã€‚

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ **k å€äº¤å‰éªŒè¯**æ¥éªŒè¯è¯¥æ¨¡å‹ï¼Œè¯¥è¿‡ç¨‹åŒ…æ‹¬å°†æ•°æ®åˆ†æˆ k æ¬¡è®­ç»ƒå’ŒéªŒè¯é›†ï¼Œå¹¶ä¸”é’ˆå¯¹æ¯æ¬¡åˆ†å‰²å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚å®ƒç”¨äºæ£€æŸ¥æ¨¡å‹é€šè¿‡ä¸€äº›æ•°æ®è¿›è¡Œè®­ç»ƒçš„èƒ½åŠ›ï¼Œä»¥åŠé¢„æµ‹æœªçŸ¥æ•°æ®çš„èƒ½åŠ›ã€‚

æˆ‘æƒ³æ¾„æ¸…ä¸€ä¸‹ï¼Œæˆ‘ç§°**éªŒè¯é›†**ä¸ºä¸€ç»„ç”¨äºè°ƒæ•´åˆ†ç±»å™¨è¶…å‚æ•°çš„ä¾‹å­ï¼Œä»åˆ†å‰²è®­ç»ƒæ•°æ®ä¸­æå–ã€‚å¦ä¸€æ–¹é¢ï¼Œ**æµ‹è¯•é›†**æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿæ¨¡å‹åœ¨ç”Ÿäº§ä¸­çš„è¡¨ç°ï¼Œå½“å®ƒè¢«è¦æ±‚é¢„æµ‹ä»¥å‰ä»æœªè§è¿‡çš„è§‚å¯Ÿæ—¶ã€‚

é€šå¸¸ä¸ºæ¯ä¸ªæŠ˜å ç»˜åˆ¶ä¸€ä¸ª **ROC æ›²çº¿**ï¼Œè¯¥å›¾è¯´æ˜äº†äºŒå…ƒåˆ†ç±»å™¨çš„èƒ½åŠ›å¦‚ä½•éšç€å…¶åŒºåˆ†é˜ˆå€¼çš„å˜åŒ–è€Œå˜åŒ–ã€‚å®ƒæ˜¯é€šè¿‡åœ¨å„ç§é˜ˆå€¼è®¾ç½®ä¸‹ç»˜åˆ¶çœŸé˜³æ€§ç‡(æ­£ç¡®é¢„æµ‹çš„ 1)ä¸å‡é˜³æ€§ç‡(é¢„æµ‹çš„ 1 å®é™…ä¸Šæ˜¯ 0)æ¥åˆ›å»ºçš„ã€‚[**AUC**](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)**(ROC æ›²çº¿ä¸‹çš„é¢ç§¯)è¡¨ç¤ºåˆ†ç±»å™¨å°†éšæœºé€‰æ‹©çš„é˜³æ€§è§‚å¯Ÿå€¼( *Y=1* )æ’åºé«˜äºéšæœºé€‰æ‹©çš„é˜´æ€§è§‚å¯Ÿå€¼( *Y=0* )çš„æ¦‚ç‡ã€‚**

**ç°åœ¨ï¼Œæˆ‘å°†å±•ç¤ºä¸€ä¸ª 10 æ¬¡æŠ˜å çš„ç¤ºä¾‹(k=10):**

```
cv = model_selection.StratifiedKFold(n_splits=10, shuffle=True)
tprs, aucs = [], []
mean_fpr = np.linspace(0,1,100)
fig = plt.figure()i = 1
for train, test in cv.split(X_train, y_train):
   prediction = model.fit(X_train[train],
                y_train[train]).predict_proba(X_train[test])
   fpr, tpr, t = metrics.roc_curve(y_train[test], prediction[:, 1])
   tprs.append(scipy.interp(mean_fpr, fpr, tpr))
   roc_auc = metrics.auc(fpr, tpr)
   aucs.append(roc_auc)
   plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = 
            %0.2f)' % (i, roc_auc))
   i = i+1

plt.plot([0,1], [0,1], linestyle='--', lw=2, color='black')
mean_tpr = np.mean(tprs, axis=0)
mean_auc = metrics.auc(mean_fpr, mean_tpr)
plt.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = 
         %0.2f )' % (mean_auc), lw=2, alpha=1)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('K-Fold Validation')
plt.legend(loc="lower right")
plt.show()
```

**![](img/94d47619c9ea2a1e3991f5f24bb82559.png)**

**æ ¹æ®è¿™ä¸€éªŒè¯ï¼Œåœ¨å¯¹æµ‹è¯•è¿›è¡Œé¢„æµ‹æ—¶ï¼Œæˆ‘ä»¬åº”è¯¥é¢„æœŸ AUC åˆ†æ•°åœ¨ 0.84 å·¦å³ã€‚**

**å‡ºäºæœ¬æ•™ç¨‹çš„ç›®çš„ï¼Œæˆ‘è®¤ä¸ºæ€§èƒ½å¾ˆå¥½ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­ä½¿ç”¨éšæœºæœç´¢é€‰æ‹©çš„æ¨¡å‹ã€‚ä¸€æ—¦é€‰æ‹©äº†æ­£ç¡®çš„æ¨¡å‹ï¼Œå°±å¯ä»¥åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šå¯¹å…¶è¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚**

```
**## train**
model.**fit**(X_train, y_train)**## test**
predicted_prob = model.**predict_proba**(X_test)[:,1]
predicted = model.**predict**(X_test)
```

**åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘åšäº†ä¸¤ç§é¢„æµ‹:ç¬¬ä¸€ç§æ˜¯è§‚å¯Ÿå€¼ä¸º 1 çš„æ¦‚ç‡ï¼Œç¬¬äºŒç§æ˜¯æ ‡ç­¾(1 æˆ– 0)çš„é¢„æµ‹ã€‚ä¸ºäº†å¾—åˆ°åè€…ï¼Œä½ å¿…é¡»å†³å®šä¸€ä¸ªæ¦‚ç‡é˜ˆå€¼ï¼Œå¯¹äºè¿™ä¸ªé˜ˆå€¼ï¼Œä¸€ä¸ªè§‚å¯Ÿå¯ä»¥è¢«è®¤ä¸ºæ˜¯ 1ï¼Œæˆ‘ä½¿ç”¨é»˜è®¤çš„é˜ˆå€¼ 0.5ã€‚**

## **ä¼°ä»·**

**å…³é”®æ—¶åˆ»åˆ°äº†ï¼Œæˆ‘ä»¬è¦çœ‹çœ‹æ‰€æœ‰è¿™äº›åŠªåŠ›æ˜¯å¦å€¼å¾—ã€‚é‡ç‚¹æ˜¯ç ”ç©¶æ¨¡å‹åšå‡ºäº†å¤šå°‘æ­£ç¡®çš„é¢„æµ‹å’Œé”™è¯¯ç±»å‹ã€‚**

**æˆ‘å°†ä½¿ç”¨ä»¥ä¸‹å¸¸ç”¨æŒ‡æ ‡æ¥è¯„ä¼°è¯¥æ¨¡å‹:å‡†ç¡®æ€§ã€AUCã€ [**ç²¾åº¦å’Œå¬å›**](https://en.wikipedia.org/wiki/Precision_and_recall) ã€‚æˆ‘å·²ç»æåˆ°äº†å‰ä¸¤ä¸ªï¼Œä½†æˆ‘è®¤ä¸ºå…¶ä»–çš„æ›´é‡è¦ã€‚ç²¾åº¦æ˜¯æ¨¡å‹åœ¨æ‰€æœ‰é¢„æµ‹çš„ 1(æˆ– 0)ä¸­æ­£ç¡®é¢„æµ‹çš„ 1(æˆ– 0)çš„åˆ†æ•°ï¼Œå› æ­¤å®ƒå¯ä»¥è¢«è§†ä¸ºé¢„æµ‹ 1(æˆ– 0)æ—¶çš„ä¸€ç§ç½®ä¿¡åº¦ã€‚å¬å›æ˜¯æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸­æ‰€æœ‰ 1(æˆ– 0)ä¸­æ­£ç¡®é¢„æµ‹çš„ 1(æˆ– 0)çš„éƒ¨åˆ†ï¼ŒåŸºæœ¬ä¸Šå®ƒæ˜¯çœŸæ­£çš„ 1 ç‡ã€‚å°†ç²¾ç¡®åº¦å’Œå¬å›ç‡ä¸è°ƒå’Œå¹³å‡å€¼ç›¸ç»“åˆï¼Œå°±å¾—åˆ° F1 åˆ†æ•°ã€‚**

**è®©æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°:**

```
**## Accuray e AUC**
accuracy = metrics.**accuracy_score**(y_test, predicted)
auc = metrics.**roc_auc_score**(y_test, predicted_prob)
print("Accuracy (overall correct predictions):",  round(accuracy,2))
print("Auc:", round(auc,2))

**## Precision e Recall**
recall = metrics.**recall_score**(y_test, predicted)
precision = metrics.**precision_score**(y_test, predicted)
print("Recall (all 1s predicted right):", round(recall,2))
print("Precision (confidence when predicting a 1):", round(precision,2))
print("Detail:")
print(metrics.**classification_report**(y_test, predicted, target_names=[str(i) for i in np.unique(y_test)]))
```

**![](img/88dd8657970406033727964ea5544dbd.png)**

**æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæ¨¡å‹çš„æ€»ä½“å‡†ç¡®ç‡åœ¨ 85%å·¦å³ã€‚å®ƒä»¥ 84%çš„ç²¾åº¦æ­£ç¡®é¢„æµ‹äº† 71%çš„ 1ï¼Œä»¥ 85%çš„ç²¾åº¦æ­£ç¡®é¢„æµ‹äº† 92%çš„ 0ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™äº›æŒ‡æ ‡ï¼Œæˆ‘å°†æŠŠç»“æœåˆ†è§£æˆä¸€ä¸ª[æ··æ·†çŸ©é˜µ](https://en.wikipedia.org/wiki/Confusion_matrix):**

```
classes = np.unique(y_test)
fig, ax = plt.subplots()
cm = metrics.**confusion_matrix**(y_test, predicted, labels=classes)
sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)
ax.set(xlabel="Pred", ylabel="True", title="Confusion matrix")
ax.set_yticklabels(labels=classes, rotation=0)
plt.show()
```

**![](img/b21b64d0aebb464d415a51d692b565ca.png)**

**æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¯¥æ¨¡å‹é¢„æµ‹äº† 85 (70+15)ä¸ª 1ï¼Œå…¶ä¸­ 70 ä¸ªæ˜¯çœŸé˜³æ€§ï¼Œ15 ä¸ªæ˜¯å‡é˜³æ€§ï¼Œå› æ­¤åœ¨é¢„æµ‹ 1 æ—¶ï¼Œå®ƒçš„ç²¾åº¦ä¸º 70/85 = 0.82ã€‚å¦ä¸€æ–¹é¢ï¼Œè¯¥æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸­çš„æ‰€æœ‰ 96 ä¸ª(70+26)1 ä¸­å¾—åˆ°äº† 70 ä¸ª 1ï¼Œå› æ­¤å…¶å¬å›ç‡ä¸º 70/96 = 0.73ã€‚**

**é€‰æ‹©é˜ˆå€¼ 0.5 æ¥å†³å®šé¢„æµ‹æ˜¯ 1 è¿˜æ˜¯ 0 å¯¼è‡´äº†è¿™ç§ç»“æœã€‚æ¢ä¸€ä¸ªä¼šä¸ä¸€æ ·å—ï¼Ÿå½“ç„¶å¯ä»¥ï¼Œä½†æ˜¯æ²¡æœ‰ä¸€ä¸ªé˜ˆå€¼å¯ä»¥è®©å‡†ç¡®ç‡å’Œå¬å›ç‡éƒ½è¾¾åˆ°æœ€é«˜åˆ†ï¼Œé€‰æ‹©ä¸€ä¸ªé˜ˆå€¼æ„å‘³ç€åœ¨è¿™ä¸¤ä¸ªæŒ‡æ ‡ä¹‹é—´è¿›è¡ŒæŠ˜è¡·ã€‚æˆ‘å°†é€šè¿‡ç»˜åˆ¶æµ‹è¯•ç»“æœçš„ ROC æ›²çº¿å’Œç²¾ç¡®å›å¿†æ›²çº¿æ¥è¯´æ˜æˆ‘çš„æ„æ€:**

```
classes = np.unique(y_test)
fig, ax = plt.subplots(nrows=1, ncols=2)**## plot ROC curve**
fpr, tpr, thresholds = metrics.**roc_curve**(y_test, predicted_prob)
roc_auc = metrics.auc(fpr, tpr)     
ax[0].plot(fpr, tpr, color='darkorange', lw=3, label='area = %0.2f' % roc_auc)
ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')
ax[0].hlines(y=recall, xmin=0, xmax=1-cm[0,0]/(cm[0,0]+cm[0,1]), color='red', linestyle='--', alpha=0.7, label="chosen threshold")
ax[0].vlines(x=1-cm[0,0]/(cm[0,0]+cm[0,1]), ymin=0, ymax=recall, color='red', linestyle='--', alpha=0.7)
ax[0].set(xlabel='False Positive Rate', ylabel="True Positive Rate (Recall)", title="Receiver operating characteristic")     
ax.legend(loc="lower right")
ax.grid(True)**## annotate ROC thresholds**
thres_in_plot = []
for i,t in enumerate(thresholds):
     t = np.round(t,1)
     if t not in thres_in_plot:
         ax.annotate(t, xy=(fpr[i],tpr[i]), xytext=(fpr[i],tpr[i]), 
              textcoords='offset points', ha='left', va='bottom')
         thres_in_plot.append(t)
     else:
         next**## plot P-R curve**
precisions, recalls, thresholds = metrics.**precision_recall_curve**(y_test, predicted_prob)
roc_auc = metrics.auc(recalls, precisions)
ax[1].plot(recalls, precisions, color='darkorange', lw=3, label='area = %0.2f' % roc_auc)
ax[1].plot([0,1], [(cm[1,0]+cm[1,0])/len(y_test), (cm[1,0]+cm[1,0])/len(y_test)], linestyle='--', color='navy', lw=3)
ax[1].hlines(y=precision, xmin=0, xmax=recall, color='red', linestyle='--', alpha=0.7, label="chosen threshold")
ax[1].vlines(x=recall, ymin=0, ymax=precision, color='red', linestyle='--', alpha=0.7)
ax[1].set(xlabel='Recall', ylabel="Precision", title="Precision-Recall curve")
ax[1].legend(loc="lower left")
ax[1].grid(True)**## annotate P-R thresholds** thres_in_plot = []
for i,t in enumerate(thresholds):
    t = np.round(t,1)
    if t not in thres_in_plot:
         ax.annotate(np.round(t,1), xy=(recalls[i],precisions[i]), 
               xytext=(recalls[i],precisions[i]), 
               textcoords='offset points', ha='left', va='bottom')
         thres_in_plot.append(t)
    else:
         next
plt.show()
```

**![](img/f29c4970fdd980811435030b41b51f70.png)**

**è¿™äº›æ›²çº¿çš„æ¯ä¸€ç‚¹éƒ½ä»£è¡¨ç”¨ä¸åŒé˜ˆå€¼(æ›²çº¿ä¸Šå°çš„æ•°å­—)è·å¾—çš„æ··æ·†çŸ©é˜µã€‚æˆ‘å¯ä»¥ä½¿ç”¨ 0.1 çš„é˜ˆå€¼ï¼Œè·å¾— 0.9 çš„å¬å›ç‡ï¼Œè¿™æ„å‘³ç€è¯¥æ¨¡å‹å°†æ­£ç¡®é¢„æµ‹ 90%çš„ 1ï¼Œä½†ç²¾åº¦å°†ä¸‹é™åˆ° 0.4ï¼Œè¿™æ„å‘³ç€è¯¥æ¨¡å‹å°†é¢„æµ‹å¤§é‡çš„å‡é˜³æ€§ã€‚å› æ­¤ï¼Œè¿™å®é™…ä¸Šå–å†³äºç”¨ä¾‹çš„ç±»å‹ï¼Œå°¤å…¶æ˜¯å‡é˜³æ€§æ˜¯å¦æ¯”å‡é˜´æ€§çš„æˆæœ¬æ›´é«˜ã€‚**

**å½“æ•°æ®é›†æ˜¯å¹³è¡¡çš„ï¼Œå¹¶ä¸”é¡¹ç›®æ¶‰ä¼—æ²¡æœ‰æŒ‡å®šåº¦é‡æ ‡å‡†æ—¶ï¼Œæˆ‘é€šå¸¸é€‰æ‹©æœ€å¤§åŒ– F1 åˆ†æ•°çš„é˜ˆå€¼ã€‚æ–¹æ³•å¦‚ä¸‹:**

```
**## calculate scores for different thresholds**
dic_scores = {'accuracy':[], 'precision':[], 'recall':[], 'f1':[]}
XX_train, XX_test, yy_train, yy_test = model_selection.train_test_split(X_train, y_train, test_size=0.2)
predicted_prob = model.fit(XX_train, yy_train).predict_proba(XX_test)[:,1]thresholds = []
for threshold in np.arange(0.1, 1, step=0.1):
    predicted = (predicted_prob > threshold)
    thresholds.append(threshold)
        dic_scores["accuracy"].append(metrics.accuracy_score(yy_test, predicted))
dic_scores["precision"].append(metrics.precision_score(yy_test, predicted))
dic_scores["recall"].append(metrics.recall_score(yy_test, predicted))
dic_scores["f1"].append(metrics.f1_score(yy_test, predicted))

**## plot** dtf_scores = pd.DataFrame(dic_scores).set_index(pd.Index(thresholds))    
dtf_scores.plot(ax=ax, title="Threshold Selection")
plt.show()
```

**![](img/a8e861e49ebf16839dd50cf7313965cb.png)**

**åœ¨ç»§ç»­è¿™ç¯‡é•¿æ•™ç¨‹çš„æœ€åä¸€èŠ‚ä¹‹å‰ï¼Œæˆ‘æƒ³è¯´æˆ‘ä»¬è¿˜ä¸èƒ½è¯´è¿™ä¸ªæ¨¡å‹æ˜¯å¥½æ˜¯åã€‚ç²¾åº¦ 0.85ï¼Œé«˜å—ï¼Ÿç›¸æ¯”ä»€ä¹ˆï¼Ÿä½ éœ€è¦ä¸€ä¸ª**åŸºçº¿**æ¥æ¯”è¾ƒä½ çš„æ¨¡å‹ã€‚ä¹Ÿè®¸ä½ æ­£åœ¨åšçš„é¡¹ç›®æ˜¯å…³äºå»ºç«‹ä¸€ä¸ªæ–°çš„æ¨¡å‹æ¥å–ä»£ä¸€ä¸ªå¯ä»¥ç”¨ä½œåŸºçº¿çš„æ—§æ¨¡å‹ï¼Œæˆ–è€…ä½ å¯ä»¥åœ¨åŒä¸€è®­ç»ƒé›†ä¸Šè®­ç»ƒä¸åŒçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šæ¯”è¾ƒæ€§èƒ½ã€‚**

## **å¯è§£é‡Šæ€§**

**ä½ åˆ†æå¹¶ç†è§£äº†æ•°æ®ï¼Œä½ è®­ç»ƒäº†ä¸€ä¸ªæ¨¡å‹å¹¶æµ‹è¯•äº†å®ƒï¼Œä½ ç”šè‡³å¯¹æ€§èƒ½æ„Ÿåˆ°æ»¡æ„ã€‚ä½ ä»¥ä¸ºä½ å®Œäº†å—ï¼Ÿä¸å¯¹ã€‚é¡¹ç›®åˆ©ç›Šç›¸å…³è€…å¾ˆæœ‰å¯èƒ½ä¸å…³å¿ƒä½ çš„æŒ‡æ ‡ï¼Œä¸ç†è§£ä½ çš„ç®—æ³•ï¼Œæ‰€ä»¥ä½ å¿…é¡»è¯æ˜ä½ çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¸æ˜¯é»‘ç›’ã€‚**

***çŸ³ç°*åŒ…å¯ä»¥å¸®åŠ©æˆ‘ä»¬å»ºé€ ä¸€ä¸ª**è®²è§£å™¨**ã€‚ä¸ºäº†ä¸¾ä¾‹è¯´æ˜ï¼Œæˆ‘å°†ä»æµ‹è¯•é›†ä¸­éšæœºè§‚å¯Ÿï¼Œçœ‹çœ‹æ¨¡å‹é¢„æµ‹äº†ä»€ä¹ˆ:**

```
print("True:", y_test[4], "--> Pred:", predicted[4], "| Prob:", np.max(predicted_prob[4]))
```

**![](img/4a8fe8dad03fd0bb4181f667c1bb65ce.png)**

**æ¨¡å‹è®¤ä¸ºè¿™ä¸ªè§‚å¯Ÿå€¼æ˜¯ 1ï¼Œæ¦‚ç‡ä¸º 0.93ï¼Œäº‹å®ä¸Šè¿™ä¸ªä¹˜å®¢ç¡®å®æ´»äº†ä¸‹æ¥ã€‚ä¸ºä»€ä¹ˆï¼Ÿè®©æˆ‘ä»¬ä½¿ç”¨è§£é‡Šå™¨:**

```
explainer = lime_tabular.LimeTabularExplainer(training_data=X_train, feature_names=X_names, class_names=np.unique(y_train), mode="classification")
explained = explainer.explain_instance(X_test[4], model.predict_proba, num_features=10)
explained.as_pyplot_figure()
```

**![](img/9e64e733badbcf23ba07eab9bfa1ee40.png)**

**è¯¥ç‰¹å®šé¢„æµ‹çš„ä¸»è¦å› ç´ æ˜¯ä¹˜å®¢æ˜¯å¥³æ€§(æ€§åˆ« _ ç”·æ€§= 0)ã€å¹´è½»(å¹´é¾„â‰¤ 22)å¹¶ä¸”ä¹˜åå¤´ç­‰èˆ±æ—…è¡Œ(Pclass_3 = 0 å’Œ Pclass_2 = 0)ã€‚**

**æ··æ·†çŸ©é˜µæ˜¯æ˜¾ç¤ºæµ‹è¯•è¿›è¡Œæƒ…å†µçš„ä¸€ä¸ªå¾ˆå¥½çš„å·¥å…·ï¼Œä½†æ˜¯æˆ‘ä¹Ÿç»˜åˆ¶äº†**åˆ†ç±»åŒºåŸŸ**æ¥ç›´è§‚åœ°å¸®åŠ©æˆ‘ä»¬äº†è§£æ¨¡å‹æ­£ç¡®é¢„æµ‹äº†å“ªäº›è§‚å¯Ÿç»“æœï¼Œä»¥åŠå®ƒé”™è¿‡äº†å“ªäº›è§‚å¯Ÿç»“æœã€‚ä¸ºäº†ç»˜åˆ¶äºŒç»´æ•°æ®ï¼Œéœ€è¦è¿›è¡Œä¸€å®šç¨‹åº¦çš„é™ç»´(é€šè¿‡è·å¾—ä¸€ç»„ä¸»è¦å˜é‡æ¥å‡å°‘ç‰¹å¾æ•°é‡çš„è¿‡ç¨‹)ã€‚æˆ‘å°†ç»™å‡ºä¸€ä¸ªä¾‹å­ï¼Œä½¿ç”¨ [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) ç®—æ³•å°†æ•°æ®æ€»ç»“ä¸ºä¸¤ä¸ªå˜é‡ï¼Œè¿™äº›å˜é‡æ˜¯é€šè¿‡ç‰¹å¾çš„çº¿æ€§ç»„åˆè·å¾—çš„ã€‚**

```
**## PCA**
pca = decomposition.PCA(n_components=2)
X_train_2d = pca.fit_transform(X_train)
X_test_2d = pca.transform(X_test)**## train 2d model**
model_2d = ensemble.GradientBoostingClassifier()
model_2d.fit(X_train, y_train)

**## plot classification regions**
from matplotlib.colors import ListedColormap
colors = {np.unique(y_test)[0]:"black", np.unique(y_test)[1]:"green"}
X1, X2 = np.meshgrid(np.arange(start=X_test[:,0].min()-1, stop=X_test[:,0].max()+1, step=0.01),
np.arange(start=X_test[:,1].min()-1, stop=X_test[:,1].max()+1, step=0.01))
fig, ax = plt.subplots()
Y = model_2d.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)
ax.contourf(X1, X2, Y, alpha=0.5, cmap=ListedColormap(list(colors.values())))
ax.set(xlim=[X1.min(),X1.max()], ylim=[X2.min(),X2.max()], title="Classification regions")
for i in np.unique(y_test):
    ax.scatter(X_test[y_test==i, 0], X_test[y_test==i, 1], 
               c=colors[i], label="true "+str(i))  
plt.legend()
plt.show()
```

**![](img/778b5ac9c09c7591b4d2a2cc3fd1efd5.png)**

## **ç»“è®º**

**è¿™ç¯‡æ–‡ç« æ˜¯æ¼”ç¤º**å¦‚ä½•ç”¨æ•°æ®ç§‘å­¦å¤„ç†åˆ†ç±»ç”¨ä¾‹**çš„æ•™ç¨‹ã€‚æˆ‘ä»¥æ³°å¦å°¼å…‹å·æ•°æ®é›†ä¸ºä¾‹ï¼Œç»å†äº†ä»æ•°æ®åˆ†æåˆ°æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¯ä¸€æ­¥ã€‚**

**åœ¨æ¢ç´¢éƒ¨åˆ†ï¼Œæˆ‘åˆ†æäº†å•ä¸ªåˆ†ç±»å˜é‡ã€å•ä¸ªæ•°å€¼å˜é‡ä»¥åŠå®ƒä»¬å¦‚ä½•ç›¸äº’ä½œç”¨çš„æƒ…å†µã€‚æˆ‘ä¸¾äº†ä¸€ä¸ªä»åŸå§‹æ•°æ®ä¸­æå–ç‰¹å¾çš„ç‰¹å¾å·¥ç¨‹çš„ä¾‹å­ã€‚å…³äºé¢„å¤„ç†ï¼Œæˆ‘è§£é‡Šäº†å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼å’Œåˆ†ç±»æ•°æ®ã€‚æˆ‘å±•ç¤ºäº†é€‰æ‹©æ­£ç¡®ç‰¹å¾çš„ä¸åŒæ–¹æ³•ï¼Œå¦‚ä½•ä½¿ç”¨å®ƒä»¬æ¥æ„å»ºæœºå™¨å­¦ä¹ åˆ†ç±»å™¨ï¼Œä»¥åŠå¦‚ä½•è¯„ä¼°æ€§èƒ½ã€‚åœ¨æœ€åä¸€èŠ‚ï¼Œæˆ‘å°±å¦‚ä½•æé«˜ä½ çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯è§£é‡Šæ€§ç»™å‡ºäº†ä¸€äº›å»ºè®®ã€‚**

**ä¸€ä¸ªé‡è¦çš„æ³¨æ„äº‹é¡¹æ˜¯ï¼Œæˆ‘è¿˜æ²¡æœ‰ä»‹ç»åœ¨æ‚¨çš„æ¨¡å‹è¢«æ‰¹å‡†éƒ¨ç½²ä¹‹åä¼šå‘ç”Ÿä»€ä¹ˆã€‚è¯·è®°ä½ï¼Œæ‚¨éœ€è¦æ„å»ºä¸€ä¸ªç®¡é“æ¥è‡ªåŠ¨å¤„ç†æ‚¨å°†å®šæœŸè·å¾—çš„æ–°æ•°æ®ã€‚**

**ç°åœ¨ï¼Œæ‚¨å·²ç»çŸ¥é“å¦‚ä½•å¤„ç†æ•°æ®ç§‘å­¦ç”¨ä¾‹ï¼Œæ‚¨å¯ä»¥å°†è¿™äº›ä»£ç å’Œæ–¹æ³•åº”ç”¨äºä»»ä½•ç±»å‹çš„äºŒè¿›åˆ¶åˆ†ç±»é—®é¢˜ï¼Œæ‰§è¡Œæ‚¨è‡ªå·±çš„åˆ†æï¼Œæ„å»ºæ‚¨è‡ªå·±çš„æ¨¡å‹ï¼Œç”šè‡³è§£é‡Šå®ƒã€‚**

**æˆ‘å¸Œæœ›ä½ å–œæ¬¢å®ƒï¼å¦‚æœ‰é—®é¢˜å’Œåé¦ˆï¼Œæˆ–è€…åªæ˜¯åˆ†äº«æ‚¨æ„Ÿå…´è¶£çš„é¡¹ç›®ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚**

> **ğŸ‘‰[æˆ‘ä»¬æ¥è¿çº¿](https://linktr.ee/maurodp)ğŸ‘ˆ**

> **æœ¬æ–‡æ˜¯ç³»åˆ—**ç”¨ Python è¿›è¡Œæœºå™¨å­¦ä¹ **çš„ä¸€éƒ¨åˆ†ï¼Œå‚è§:**

**[](/machine-learning-with-python-regression-complete-tutorial-47268e546cea) [## Python æœºå™¨å­¦ä¹ :å›å½’(å®Œæ•´æ•™ç¨‹)

### æ•°æ®åˆ†æå’Œå¯è§†åŒ–ã€ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©ã€æ¨¡å‹è®¾è®¡å’Œæµ‹è¯•ã€è¯„ä¼°å’Œè§£é‡Š

towardsdatascience.com](/machine-learning-with-python-regression-complete-tutorial-47268e546cea) [](/clustering-geospatial-data-f0584f0b04ec) [## èšç±»åœ°ç†ç©ºé—´æ•°æ®

### ä½¿ç”¨äº¤äº’å¼åœ°å›¾ç»˜åˆ¶æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ èšç±»

towardsdatascience.com](/clustering-geospatial-data-f0584f0b04ec) [](/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0) [## Python æ·±åº¦å­¦ä¹ :ç¥ç»ç½‘ç»œ(å®Œæ•´æ•™ç¨‹)

### ç”¨ TensorFlow å»ºç«‹ã€ç»˜åˆ¶å’Œè§£é‡Šäººå·¥ç¥ç»ç½‘ç»œ

towardsdatascience.com](/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0) [](/modern-recommendation-systems-with-neural-networks-3cc06a6ded2c) [## åŸºäºç¥ç»ç½‘ç»œçš„ç°ä»£æ¨èç³»ç»Ÿ

### ä½¿ç”¨ Python å’Œ TensorFlow æ„å»ºæ··åˆæ¨¡å‹

towardsdatascience.com](/modern-recommendation-systems-with-neural-networks-3cc06a6ded2c)**