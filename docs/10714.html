<html>
<head>
<title>Temporal Graph Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时态图网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/temporal-graph-networks-ab8f327f2efe?source=collection_archive---------4-----------------------#2020-07-27">https://towardsdatascience.com/temporal-graph-networks-ab8f327f2efe?source=collection_archive---------4-----------------------#2020-07-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5d66" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">动态图上的深度学习</h2><div class=""/><div class=""><h2 id="43b7" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">许多现实世界的问题涉及各种性质的交易网络以及社会互动和参与，这些问题是动态的，并且可以被建模为图，其中节点和边随着时间的推移而出现。在这篇文章中，我们描述了时态图网络，这是 Twitter 开发的一个通用框架，用于在动态图上进行深度学习。</h2></div><p id="f03b" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><em class="ll">本帖与</em> <a class="ae lm" href="https://www.emanuelerossi.co.uk/" rel="noopener ugc nofollow" target="_blank"> <em class="ll">伊曼纽罗西</em> </a> <em class="ll">合著。</em></p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/168cf5ced1eb8fff6ca4abf2712ec793.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kWGR5wsMTeL_PYZV"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="ko">一个动态的 Twitter 用户网络，用户可以与 tweets 互动并互相关注。所有的边都有时间戳。给定这样一个动态图表，我们希望预测未来的互动，例如，用户会喜欢哪条推文，或者他们会关注谁。</em></p></figure></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><p id="f90d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi mk translated">今年，图形神经网络(GNNs)的研究已经成为机器学习领域最热门的话题之一。GNNs 已经在生物学、化学、社会科学、物理学和许多其他领域的问题上取得了一系列的成功。到目前为止，GNN 模型主要是为不随时间变化的静态图开发的。然而，许多有趣的现实世界的图表是动态的，随着时间的推移而演变，突出的例子包括社交网络、金融交易和推荐系统。在许多情况下，正是这种系统的动态行为传达了重要的见解，否则如果只考虑静态图，就会失去这些见解。</p><p id="0600" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">一个动态图可以被表示为一个有序的列表或者一个异步的定时事件“流”，比如节点和边的增加或删除[1]。像 Twitter 这样的社交网络是一个很好的例子:当一个人加入这个平台时，一个新的节点就产生了。当他们跟随另一个用户时，会创建一个跟随边。当他们改变他们的配置文件时，节点被更新。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mt"><img src="../Images/e972f81c80a0b2dc8bc7fc87fa8c02a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RCKsJWbDLibekTDc"/></div></div></figure><p id="75c8" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">该事件流被编码器神经网络接收，该编码器神经网络为图的每个节点产生依赖于时间的嵌入。然后，该嵌入可以被馈送到为特定任务设计的解码器中。一个示例任务是通过尝试回答以下问题来预测未来的交互:在时间<em class="ll"> t </em>节点<em class="ll"> i </em>和<em class="ll"> j </em>之间有边的概率是多少？回答这个问题的能力对于推荐系统至关重要，例如，推荐系统建议社交网络用户关注谁或决定显示哪些内容。下图说明了这种情况:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mu"><img src="../Images/bc60ff3b436b162f628fd8d743b60b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*V_gp4Mq1eJNDnUJq"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="ko">tgn 编码器摄取具有七条可见边(时间戳为 t₁到 t₇)的动态图的示例，目标是预测节点 2 和 4 在 t₈时间(灰色边)的未来交互。为此，TGN 在 t₈.时间计算节点 2 和 4 的嵌入然后，这些嵌入被连接并被馈送到解码器(例如，MLP)，该解码器输出交互发生的概率。</em></p></figure><p id="2962" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">上述设置中的关键部分是可以用任何解码器训练的编码器。在前面提到的未来交互预测的任务中，训练可以以自我监督的方式进行:在每个时期，编码器按照时间顺序处理事件，并根据先前的事件预测下一个交互[2]。</p><p id="4966" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi mk translated"><span class="l ml mm mn bm mo mp mq mr ms di">T</span>emporal Graph Network(TGN)是一个通用编码器架构，我们在 Twitter 上与同事法布里齐奥·弗拉斯卡、大卫·艾纳德、本·张伯伦和费德里科·蒙蒂共同开发。该模型可以应用于在表示为事件流的动态图上学习的各种问题。简而言之，TGN 编码器通过基于节点的交互来创建节点的压缩表示，并在每个事件发生时更新它们。为了实现这一点，TGN 具有以下主要组件:</p><p id="719d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">记忆。</strong>存储器存储所有节点的状态，充当节点过去交互的压缩表示。它类似于 RNN 的隐藏状态；但是，这里我们为每个节点<em class="ll"> i </em>都有一个单独的状态向量<strong class="kr ja"><em class="ll">s</em></strong><em class="ll">ᵢ</em>(<em class="ll">t</em>)。当一个新的节点出现时，我们添加一个相应的初始化为零向量的状态。此外，由于每个节点的内存只是一个状态向量(而不是一个参数)，当模型接受新的交互时，它也可以在测试时更新。</p><p id="bc02" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">消息功能</strong>是更新存储器的主要机制。给定在时间<em class="ll"> t </em>节点<em class="ll"> i </em>和<em class="ll"> j </em>之间的交互，消息函数计算两个消息(一个给<em class="ll"> i </em>一个给<em class="ll"> j </em>)，用于更新存储器。这类似于在消息传递图神经网络中计算的消息[4]。该消息是在交互之前的时间<em class="ll"> t⁻ </em>的情况下节点<em class="ll"> i </em>和<em class="ll"> j </em>的存储器、交互时间<em class="ll"> t </em>和边缘特征[5]的函数:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mv"><img src="../Images/5c011a8024fca4972165c51cb8df0c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7GDtYEv0FD0Yk9SyrFTnsA.png"/></div></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mu"><img src="../Images/0149d4ac4526a4008fb027827cf1d3a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8805s5Nc7AMq2E9d"/></div></div></figure><p id="863c" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">存储器更新器</strong>用于用新消息更新存储器。这个模块通常被实现为一个 RNN。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mw"><img src="../Images/c73c82269633f901063e6096f26a9a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kuyeVC6PLG0C9FAjgMFMbQ.png"/></div></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mu"><img src="../Images/5febde8926ab5df6f5dafea5eb0cba23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mpkrKms6NMu1psok"/></div></div></figure><p id="8eff" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">假设一个节点的记忆是一个随时间更新的向量，最直接的方法是直接用它作为节点嵌入。然而，实际上，由于<em class="ll">陈旧性</em>问题，这是一个坏主意:假设只有当节点参与交互时，内存才会更新，节点长时间的不活动会导致其内存过期。举个例子，想象一个用户离开 Twitter 几个月。当用户回来的时候，他们可能已经发展了新的兴趣，所以他们过去活动的记忆不再相关。因此，我们需要一种更好的方法来计算嵌入。</p><p id="57e2" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">嵌入。</strong>一种解决方案是查看节点邻居。为了解决陈旧问题，嵌入模块通过在节点的时空邻居上执行图形聚合来计算节点的时间嵌入。即使一个节点已经不活动了一段时间，它的一些邻居也可能是活动的，并且通过聚集它们的记忆，TGN 可以为该节点计算最新的嵌入。在我们的例子中，即使当用户离开 Twitter 时，他们的朋友仍然是活跃的，所以当他们回来时，朋友最近的活动很可能比用户自己的历史更相关。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mu"><img src="../Images/3ee7f24044478336853c04f5f7581c7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_vccf5DvvDcpCylp"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="ko">图嵌入模块通过对目标节点的时间邻域执行聚合来计算目标节点的嵌入。在上图中，当在大于 t </em> ₂ <em class="ko">、t </em> ₃ <em class="ko">和 t </em> ₄ <em class="ko">但小于 t </em> ₅ <em class="ko">的某个时间 t 计算节点 1 的嵌入时，时间邻域将仅包括在时间 t 之前出现的边。因此，具有节点 5 的边不包括在计算中，因为它在未来发生。相反，嵌入模块聚集来自邻居 2、3 和 4 的特征(v)和存储器(s)以及边上的特征，以计算节点 1 的表示。在我们的实验中，表现最好的图形嵌入模块是图形注意力，它能够根据邻居的记忆、特征和交互时间来学习哪些邻居是最重要的。</em></p></figure><p id="5f2a" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">下图总结了 TGN 对一批训练数据进行的整体计算:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mu"><img src="../Images/a858743393a186a3a9b8e9d41afc0a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bS68H4ztcv6LfU9X"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="ko">TGN 对一批训练数据进行的计算。一方面，嵌入模块使用时态图和节点的存储器产生嵌入(1)。然后，嵌入用于预测批次相互作用并计算损失(2，3)。另一方面，这些相同的交互用于更新存储器(4，5)。</em></p></figure><p id="ebe2" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">通过查看上图，您可能会想知道与内存相关的模块(<em class="ll">消息功能</em>、<em class="ll">消息聚合器、</em>和<em class="ll">内存更新器</em>)是如何训练的，因为它们似乎不会直接影响损耗，因此不会接收到梯度。为了让这些模块影响损失，我们需要在预测批次相互作用之前更新内存。然而，这将导致泄漏，因为内存中已经包含了我们试图预测的信息。我们提出的解决这个问题的策略是用来自<em class="ll">之前</em>批次的消息更新内存，然后预测交互。下图显示了训练存储器相关模块所需的 TGN 的操作流程:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mu"><img src="../Images/5be7e9044692e199b494dff31f440986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VJJMrYj-X-Grj-DJ"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="ko">训练记忆相关模块所需的 TGN 的操作流程。引入了一个新的组件，原始消息存储，它存储了计算消息所必需的信息，我们称之为原始消息，用于模型过去已经处理过的交互。这使得模型可以将交互带来的内存更新延迟到后面的批次。首先，使用从先前批次(1 和 2)中存储的原始消息计算的消息来更新存储器。然后可以使用刚刚更新的存储器(灰色链接)(3)来计算嵌入。通过这样做，存储器相关模块的计算直接影响损耗(4，5)，并且它们接收梯度。最后，这个批处理交互的原始消息被存储在原始消息存储库(6)中，以便在将来的批处理中使用。</em></p></figure><p id="7e7b" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi mk translated"><span class="l ml mm mn bm mo mp mq mr ms di">在</span>对各种动态图进行的大量实验验证中，TGN 在未来边缘预测和动态节点分类任务中，无论是在准确性还是速度方面，都明显优于竞争方法[6]。一个这样的动态图是 Wikipedia，其中用户和页面是节点，交互表示用户编辑页面。编辑文本的编码被用作交互特征。这种情况下的任务是预测用户在给定时间将编辑哪个页面。我们将 TGN 的不同变体与基线方法进行了比较:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mu"><img src="../Images/b9eac9decb69387dd13602154a913a94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KEM3MeRMDGDi3HB8"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="ko">根据预测准确性和时间，比较 TGN 和旧方法(戴雷普、TGAT 和乔迪)在维基百科数据集上的未来链接预测的各种配置。</em></p></figure><p id="f1ca" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">这项消融研究阐明了不同 TGN 模块的重要性，并让我们得出一些一般性结论。首先，记忆很重要:它的缺失会导致性能的大幅下降[7]。其次，嵌入模块的使用(与直接输出内存状态相反)很重要。基于图注意力的嵌入表现得最好。第三，具有存储器使得仅使用一个图关注层就足够了(这极大地减少了计算时间)，因为 1 跳邻居的存储器给予模型对 2 跳邻居信息的间接访问。</p><p id="ffb8" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi mk translated">作为结束语，我们认为对动态图的学习几乎是一个未开发的研究领域，有许多重要和令人兴奋的应用以及重大的潜在影响。我们相信，我们的 TGN 模型是向提高在动态图上学习的能力迈出的重要一步，巩固并扩展了以前的结果。随着这一研究领域的发展，更好、更大的基准将变得至关重要。我们现在正致力于创建新的动态图形数据集和任务，作为<a class="ae lm" href="https://ogb.stanford.edu/docs/team/" rel="noopener ugc nofollow" target="_blank">开放图形基准</a>的一部分。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><p id="bdac" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[1]这种场景通常被称为“连续时间动态图”。为简单起见，这里我们只考虑节点对之间的交互事件，用图中的边来表示。当一个端点不在图中时，节点插入被认为是新边的特殊情况。我们不考虑节点或边的删除，这意味着该图只能随着时间的推移而增长。由于一对节点之间可能有多条边，从技术上讲，我们拥有的对象是一个<a class="ae lm" href="https://en.wikipedia.org/wiki/Multigraph#:~:text=In%20mathematics%2C%20and%20more%20specifically,by%20more%20than%20one%20edge." rel="noopener ugc nofollow" target="_blank">多图</a>。</p><p id="6749" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[2]多个交互可以具有相同的时间戳，并且模型独立地预测它们中的每一个。此外，通过将<em class="ll">有序的</em>事件列表分割成固定大小的连续块来创建小批量。</p><p id="7344" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[3] E. Rossi 等<a class="ae lm" href="https://arxiv.org/abs/2006.10637" rel="noopener ugc nofollow" target="_blank">动态图上深度学习的时态图网络</a> (2020)。arXiv:2006.10637。</p><p id="6481" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[4]为了简单起见，我们假设该图是无向的。在有向图的情况下，将需要两个不同的消息函数，一个用于源，一个用于目的地。</p><p id="9cbc" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[5] J. Gilmer 等人<a class="ae lm" href="https://arxiv.org/abs/1704.01212" rel="noopener ugc nofollow" target="_blank">量子化学的神经信息传递</a> (2017)。arXiv:1704.01212。</p><p id="7ee2" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[6]我们并不是第一个研究动态图的人。然而，大多数先前的方法集中于离散时间动态图的更有限的情况，例如 A. Sankar 等人<a class="ae lm" href="https://arxiv.org/abs/1812.09430" rel="noopener ugc nofollow" target="_blank">通过自我注意网络的动态图表示学习</a>，Proc .WSDM 2020，或者时态知识图的具体场景，比如 A. García-Durán et al. <a class="ae lm" href="https://www.aclweb.org/anthology/D18-1516.pdf" rel="noopener ugc nofollow" target="_blank">学习序列编码器用于时态知识图完成</a> (2018)。继续。EMNLP 或 Z. Han 等.<a class="ae lm" href="https://arxiv.org/abs/2003.13432" rel="noopener ugc nofollow" target="_blank"> Graph Hawkes 神经网络用于时态知识图的预测</a>2020。参见 s .卡泽米等人的<a class="ae lm" href="https://arxiv.org/abs/1905.11485" rel="noopener ugc nofollow" target="_blank">动态图的表示学习:一个调查</a>以获得这种方法的更广泛的概述。另一方面，在动态图上进行深度学习的方法屈指可数，如 R. Trivedi 等人的 dy rep<a class="ae lm" href="https://arxiv.org/pdf/1803.04051.pdf" rel="noopener ugc nofollow" target="_blank">在动态图上进行表示学习</a> (2018)，arXiv:1803.04051，D. Xu 等人的<a class="ae lm" href="https://arxiv.org/abs/2002.07962" rel="noopener ugc nofollow" target="_blank">在时态图上进行归纳表示学习</a> (2020)，arXiv:2002.07962 和 S. Kumar 等人的 Jodie<a class="ae lm" href="https://arxiv.org/abs/1908.01207" rel="noopener ugc nofollow" target="_blank">在时态上预测动态嵌入轨迹我们证明了这些方法可以作为 TGN 的特殊构型得到。出于这个原因，TGN 似乎是迄今为止在动态图上学习的最通用的模型。</a></p><p id="63f4" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[7]虽然存储器包含关于节点过去所有交互的信息，但是图嵌入模块只能访问时间邻域的样本(出于计算原因),因此可能无法访问手头任务的关键信息。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><p id="77f4" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">作者感谢 Giorgos Bouritsas、Ben Chamberlain 和 Federico Monti 对本文的校对，并感谢 Stephan Günnemann 参考了动态知识图的早期作品。本帖的 <a class="ae lm" href="https://www.infoq.cn/article/D2cFv5V8IF8gvZDovq1p" rel="noopener ugc nofollow" target="_blank"> <em class="ll">中文翻译</em> </a> <em class="ll">由</em> <a class="ae lm" href="https://medium.com/@zhiyongliu" rel="noopener"> <em class="ll">刘止庸</em> </a> <em class="ll">提供。参见 Twitter Research GitHub 资源库中的</em> <a class="ae lm" href="https://github.com/twitter-research/tgn" rel="noopener ugc nofollow" target="_blank"> <em class="ll"> TGN 代码</em> </a> <em class="ll">。对图形 ML 和几何深度学习感兴趣？参见我的</em> <a class="ae lm" rel="noopener" target="_blank" href="https://towardsdatascience.com/graph-deep-learning/home"> <em class="ll">其他文章</em> </a> <em class="ll">在走向数据科学，</em> <a class="ae lm" href="https://michael-bronstein.medium.com/subscribe" rel="noopener"> <em class="ll">订阅</em> </a> <em class="ll">到我的帖子，获取</em> <a class="ae lm" href="https://michael-bronstein.medium.com/membership" rel="noopener"> <em class="ll">中等会员</em> </a> <em class="ll">，或者关注我的</em> <a class="ae lm" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"> <em class="ll">推特</em> </a> <em class="ll">。</em></p></div></div>    
</body>
</html>