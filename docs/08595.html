<html>
<head>
<title>End to End Pipeline for setting up Multiclass Image Classification for Data Scientists</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为数据科学家设置多类影像分类的端到端管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c?source=collection_archive---------15-----------------------#2020-06-22">https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c?source=collection_archive---------15-----------------------#2020-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/a307211d1b0435fd298acc703e51c399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zn9TmLugqsvWpgGFjmm-yA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=987783" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae jg" href="https://pixabay.com/users/louda2455-152087/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=987783" rel="noopener ugc nofollow" target="_blank"> Louise Dav </a>拍摄</p></figure><div class=""/><div class=""><h2 id="f5f3" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">使用PyTorch和迁移学习</h2></div><p id="404b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">你有没有想过脸书是如何处理一些用户分享的侮辱性和不恰当的图片的？或者脸书的标签功能是如何工作的？或者说Google Lens是如何通过图像识别产品的？</em></p><p id="b645" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以上都是<a class="ae jg" href="https://lionbridge.ai/services/image-annotation/" rel="noopener ugc nofollow" target="_blank">图像分类</a>在不同设置下的例子。多类图像分类是计算机视觉中的一项常见任务，我们将图像分为三类或更多类。</p><p id="47d7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以前我都是用Keras做计算机视觉项目。然而，最近当从事多类图像分类的机会出现时，我决定使用PyTorch。我已经把所有的NLP任务从Keras转移到PyTorch，那么为什么不把vision也转移呢？</p><blockquote class="lv"><p id="a448" class="lw lx jj bd ly lz ma mb mc md me lt dk translated"><a class="ae jg" href="https://redirect.viglink.com/?format=go&amp;jsonp=vglnk_159220215223010&amp;key=0d3176c012db018d69225ad1c36210fa&amp;libId=kbg41t7r0102t244000DAap2a6q0u&amp;subId=fd1ffa7fd7152e4e20568fbe49a489d0&amp;cuid=fd1ffa7fd7152e4e20568fbe49a489d0&amp;loc=https%3A%2F%2Fmlwhiz.com%2Fblog%2F2020%2F02%2F21%2Fds2020%2F&amp;v=1&amp;out=https%3A%2F%2Fclick.linksynergy.com%2Flink%3Fid%3DlVarvwc5BD0%26offerid%3D467035.14805039480%26type%3D2%26murl%3Dhttps%253A%252F%252Fwww.coursera.org%252Flearn%252Fdeep-neural-networks-with-pytorch&amp;ref=https%3A%2F%2Fwww.google.com%2F&amp;title=Become%20a%20Data%20Scientist%20in%202020%20with%20these%2010%20resources&amp;txt=Deep%20Neural%20Networks%20with%20Pytorch" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>功能强大，我也喜欢它更pythonic化的结构。</p></blockquote><p id="a557" class="pw-post-body-paragraph ky kz jj la b lb mf kk ld le mg kn lg lh mh lj lk ll mi ln lo lp mj lr ls lt im bi translated">在这篇文章中，我们将使用Pytorch为图像多类分类创建一个端到端的管道。 这将包括训练模型，将模型的结果以一种可以显示给业务伙伴的形式，以及帮助轻松部署模型的功能。作为一个附加的特性，我们也将使用Pytorch来增加测试时间。</p><p id="f236" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是在我们学习如何进行图像分类之前，我们先来看看迁移学习，这是处理这类问题最常见的方法。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="25d2" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">什么是迁移学习？</h1><p id="c6d8" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">迁移学习是将知识从一项任务转移到另一项任务的过程。从建模的角度来看，这意味着使用在一个数据集上训练的模型，并对其进行微调以用于另一个数据集。但是为什么会起作用呢？</p><p id="449a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">先说一些背景。每年视觉识别社区都会为一个非常特殊的挑战而聚集在一起:<a class="ae jg" href="http://image-net.org/explore" rel="noopener ugc nofollow" target="_blank">Imagenet挑战</a>。这项挑战的任务是将1，000，000幅图像分为1，000个类别。</p><p id="67a9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一挑战已经导致研究人员训练大型卷积深度学习模型。结果包括像Resnet50和Inception这样的优秀模型。</p><p id="7d0e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，训练一个神经模型意味着什么呢？本质上，这意味着研究人员在一百万张图像上训练模型后，已经学习了神经网络的权重。</p><p id="23de" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么，如果我们能得到这些重量呢？然后，我们可以使用它们，并将它们加载到我们自己的神经网络模型中，在测试数据集上进行预测，对吗？实际上，我们可以走得更远。我们可以在这些研究人员已经准备好的神经网络之上添加一个额外的层来对我们自己的数据集进行分类。</p><blockquote class="lv"><p id="11f5" class="lw lx jj bd ly lz ma mb mc md me lt dk translated">虽然这些复杂模型的确切工作方式仍然是一个谜，但我们知道较低的卷积层可以捕捉边缘和梯度等低级图像特征。相比之下，更高的卷积层捕捉越来越复杂的细节，如身体部位、面部和其他组成特征。</p></blockquote><figure class="np nq nr ns nt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/da6ce37148f080fc7424850b4b0cd651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*blBhEapJ2iFEEiK4dwBETQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://arxiv.org/pdf/1311.2901.pdf" rel="noopener ugc nofollow" target="_blank">可视化和理解卷积网络</a>。你可以看到前几层是如何捕捉基本形状的，在后面的层中形状变得越来越复杂。</p></figure><p id="01a8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上面来自ZFNet(Alex net的变体)的示例中，ZFNet是在Imagenet任务上获得成功的第一批卷积神经网络之一，您可以看到较低层是如何捕捉线条和边缘的，而后面的层是如何捕捉更复杂的特征的。通常假设最终的全连接层捕获与解决相应任务相关的信息，例如，ZFNet的全连接层指示哪些特征与将图像分类到1，000个对象类别之一相关。</p><p id="3c0c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于新的视觉任务，我们可以简单地使用预先在ImageNet上训练的最新CNN的现成特征，并在这些提取的特征上训练新的模型。</p><p id="3184" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个想法背后的直觉是，一个经过训练可以识别动物的模型也可以用来识别猫和狗。在我们的例子中，</p><blockquote class="lv"><p id="101c" class="lw lx jj bd ly lz ma mb mc md me lt dk translated">一个在1000个不同类别上训练过的模型已经看到了很多真实世界的信息，我们可以使用这些信息来创建我们自己的定制分类器。</p></blockquote><p id="144a" class="pw-post-body-paragraph ky kz jj la b lb mf kk ld le mg kn lg lh mh lj lk ll mi ln lo lp mj lr ls lt im bi translated"><strong class="la jk"> <em class="lu">理论和直觉就是这样。我们如何让它真正发挥作用？让我们看一些代码。你可以在</em></strong><a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/tree/master/compvisblog" rel="noopener ugc nofollow" target="_blank"><strong class="la jk"><em class="lu">Github</em></strong></a><strong class="la jk"><em class="lu">上找到这篇文章的完整代码。</em> </strong></p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="a5c1" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">数据探索</h1><p id="81d8" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">我们将从Kaggle的<a class="ae jg" href="https://www.kaggle.com/clorichel/boat-types-recognition/version/1" rel="noopener ugc nofollow" target="_blank">船数据集</a>开始，以理解多类图像分类问题。该数据集包含大约1，500张不同类型的船只照片:浮标、游轮、渡船、货船、平底船、充气船、皮艇、纸船和帆船。我们的目标是创建一个模型，该模型查看船只图像并将其分类到正确的类别中。</p><p id="d2f7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是数据集中的图像示例:</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/961d00dcc2b10f31ce9f4b0c7182873f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3iyRJRzKZGctnm3H8HVJQw.png"/></div></div></figure><p id="d702" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是类别计数:</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/366ea62a82634e35fe8453275b32d05d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TwvxjfbxIeJQn7SIjizmhg.png"/></div></div></figure><p id="73ca" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于类别<em class="lu">“货运船”<br/>“充气船”</em>“船”没有太多的图像；我们将在训练模型时删除这些类别。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="e057" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">创建所需的目录结构</h1><p id="8f35" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">在我们可以训练我们的深度学习模型之前，我们需要为我们的图像创建所需的目录结构。现在，我们的数据目录结构看起来像这样:</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="3206" class="of ms jj ob b gy og oh l oi oj">images<br/>    sailboat<br/>    kayak<br/>    .<br/>    .</span></pre><p id="7319" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要我们的图像被包含在3个文件夹<code class="fe ok ol om ob b">train</code>、<code class="fe ok ol om ob b">val</code>和<code class="fe ok ol om ob b">test</code>。然后，我们将对<code class="fe ok ol om ob b">train</code>数据集中的图像进行训练，对<code class="fe ok ol om ob b">val</code>数据集中的图像进行验证，最后对<code class="fe ok ol om ob b">test</code>数据集中的图像进行测试。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="5d89" class="of ms jj ob b gy og oh l oi oj">data<br/>    train<br/>        sailboat<br/>        kayak<br/>        .<br/>        .<br/>    val<br/>        sailboat<br/>        kayak<br/>        .<br/>        .<br/>    test<br/>        sailboat<br/>        kayak<br/>        .<br/>        .</span></pre><p id="5fe2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可能有不同格式的数据，但是我发现除了通常的库之外，<code class="fe ok ol om ob b">glob.glob</code>和<code class="fe ok ol om ob b">os.system</code>函数非常有用。在这里你可以找到完整的<a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/blob/master/compvisblog/Boats_DataExploration.ipynb" rel="noopener ugc nofollow" target="_blank">数据准备代码</a>。现在让我们快速浏览一下我在准备数据时发现有用的一些不常用的库。</p><h2 id="0b57" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">什么是glob.glob？</h2><p id="3652" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">简单地说，glob允许您使用正则表达式获取目录中文件或文件夹的名称。例如，您可以这样做:</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="d5ad" class="of ms jj ob b gy og oh l oi oj">from glob import glob<br/>categories = glob(“images/*”)<br/>print(categories)<br/>------------------------------------------------------------------<br/>['images/kayak', 'images/boats', 'images/gondola', 'images/sailboat', 'images/inflatable boat', 'images/paper boat', 'images/buoy', 'images/cruise ship', 'images/freight boat', 'images/ferry boat']</span></pre><h2 id="33dc" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">什么是os.system？</h2><p id="c298" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated"><code class="fe ok ol om ob b">os.system</code>是<code class="fe ok ol om ob b">os</code>库中的一个函数，它允许你在python中运行任何命令行函数。我通常用它来运行Linux函数，但它也可以用来运行python中的R脚本，如这里的<a class="ae jg" rel="noopener" target="_blank" href="/python-pro-tip-want-to-use-r-java-c-or-any-language-in-python-d304be3a0559">所示</a>。例如，在从pandas数据框中获取信息后，我在准备数据时使用它将文件从一个目录复制到另一个目录。我也使用<a class="ae jg" rel="noopener" target="_blank" href="/how-and-why-to-use-f-strings-in-python3-adbba724b251"> f字符串格式</a>。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="1ec6" class="of ms jj ob b gy og oh l oi oj">import os</span><span id="2c9f" class="of ms jj ob b gy oy oh l oi oj">for i,row in fulldf.iterrows():<br/>    # Boat category<br/>    cat = row['category']<br/>    # section is train,val or test<br/>    section = row['type']<br/>    # input filepath to copy<br/>    ipath = row['filepath']<br/>    # output filepath to paste<br/>    opath = ipath.replace(f"images/",f"data/{section}/")<br/>    # running the cp command<br/>    os.system(f"cp '{ipath}' '{opath}'")</span></pre><p id="a513" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，既然我们已经在所需的文件夹结构中有了数据，我们可以继续进行更令人兴奋的部分。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="757c" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">数据预处理</h1><h2 id="0466" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">转换:</h2><p id="958a" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated"><strong class="la jk"> 1。Imagenet预处理</strong></p><p id="1b44" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了将我们的图像用于在Imagenet数据集上训练的网络，我们需要以与Imagenet网络相同的方式预处理我们的图像。为此，我们需要将图像缩放到224×224，并按照Imagenet标准对其进行标准化。我们可以利用火炬视觉<code class="fe ok ol om ob b">transforms</code>图书馆做到这一点。这里我们取224×224的<code class="fe ok ol om ob b">CenterCrop</code>，按照Imagenet标准进行归一化。下面定义的操作按顺序发生。您可以在这里找到由PyTorch提供的所有转换的列表。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="2652" class="of ms jj ob b gy og oh l oi oj">transforms.Compose([<br/>        transforms.CenterCrop(size=224),  <br/>        transforms.ToTensor(),<br/>        transforms.Normalize([0.485, 0.456, 0.406],<br/>                             [0.229, 0.224, 0.225])  <br/>    ])</span></pre><p id="8c68" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> 2。数据扩充</strong></p><p id="2c5b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以为数据扩充做更多的预处理。神经网络在处理大量数据时效果更好。<a class="ae jg" href="https://lionbridge.ai/articles/data-augmentation-with-machine-learning-an-overview/" rel="noopener ugc nofollow" target="_blank">数据扩充</a>是一种策略，我们在训练时使用它来增加我们拥有的数据量。</p><p id="d8e6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，我们可以水平翻转一艘船的图像，它仍然是一艘船。或者我们可以随机裁剪图像或添加颜色抖动。这是我用过的图像转换字典，它适用于Imagenet预处理和增强。这本字典包含了我们在这篇<a class="ae jg" rel="noopener" target="_blank" href="/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce">伟大的文章</a>中使用的训练、测试和验证数据的各种转换。正如您所料，我们没有对测试数据和验证数据应用水平翻转或其他数据增强转换，因为我们不想在增强的图像上获得预测。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="20d0" class="of ms jj ob b gy og oh l oi oj"># Image transformations<br/>image_transforms = {<br/>    # Train uses data augmentation<br/>    'train':<br/>    transforms.Compose([<br/>        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),<br/>        transforms.RandomRotation(degrees=15),<br/>        transforms.ColorJitter(),<br/>        transforms.RandomHorizontalFlip(),<br/>        transforms.CenterCrop(size=224),  # Image net standards<br/>        transforms.ToTensor(),<br/>        transforms.Normalize([0.485, 0.456, 0.406],<br/>                             [0.229, 0.224, 0.225])  # Imagenet standards<br/>    ]),<br/>    # Validation does not use augmentation<br/>    'valid':<br/>    transforms.Compose([<br/>        transforms.Resize(size=256),<br/>        transforms.CenterCrop(size=224),<br/>        transforms.ToTensor(),<br/>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<br/>    ]),<br/>    <br/>        # Test does not use augmentation<br/>    'test':<br/>    transforms.Compose([<br/>        transforms.Resize(size=256),<br/>        transforms.CenterCrop(size=224),<br/>        transforms.ToTensor(),<br/>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<br/>    ]),<br/>}</span></pre><p id="e164" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是应用于训练数据集中图像的训练变换示例。我们不仅从一幅图像中得到许多不同的图像，而且它还帮助我们的网络变得对物体方向不变。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="7d9a" class="of ms jj ob b gy og oh l oi oj">ex_img = Image.open('/home/rahul/projects/compvisblog/data/train/cruise ship/cruise-ship-oasis-of-the-seas-boat-water-482183.jpg')</span><span id="9527" class="of ms jj ob b gy oy oh l oi oj">t = image_transforms['train']<br/>plt.figure(figsize=(24, 24))</span><span id="33a5" class="of ms jj ob b gy oy oh l oi oj">for i in range(16):<br/>    ax = plt.subplot(4, 4, i + 1)<br/>    _ = imshow_tensor(t(ex_img), ax=ax)</span><span id="2261" class="of ms jj ob b gy oy oh l oi oj">plt.tight_layout()</span></pre><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/65b7ab287d392a7e1997e81ba2e1b0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MXqNsUDICTyJ_NBOZi1THg.png"/></div></div></figure><h2 id="2976" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">数据加载器</h2><p id="a745" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">下一步是向PyTorch提供培训、验证和测试数据集的位置。我们可以通过使用PyTorch <code class="fe ok ol om ob b">datasets</code>和<code class="fe ok ol om ob b">DataLoader</code>类来做到这一点。如果我们将数据放在所需的目录结构中，这部分代码将基本保持不变。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="83f2" class="of ms jj ob b gy og oh l oi oj"># Datasets from folders</span><span id="e872" class="of ms jj ob b gy oy oh l oi oj">traindir = "data/train"<br/>validdir = "data/val"<br/>testdir = "data/test"</span><span id="b11b" class="of ms jj ob b gy oy oh l oi oj">data = {<br/>    'train':<br/>    datasets.ImageFolder(root=traindir, transform=image_transforms['train']),<br/>    'valid':<br/>    datasets.ImageFolder(root=validdir, transform=image_transforms['valid']),<br/>    'test':<br/>    datasets.ImageFolder(root=testdir, transform=image_transforms['test'])<br/>}</span><span id="1c8c" class="of ms jj ob b gy oy oh l oi oj"># Dataloader iterators, make sure to shuffle<br/>dataloaders = {<br/>    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True,num_workers=10),<br/>    'val': DataLoader(data['valid'], batch_size=batch_size, shuffle=True,num_workers=10),<br/>    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True,num_workers=10)<br/>}</span></pre><p id="0825" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些数据加载器帮助我们遍历数据集。例如，我们将在模型培训中使用下面的数据加载器。数据变量将包含形式为<code class="fe ok ol om ob b">(batch_size, color_channels, height, width)</code>的数据，而目标的形状为<code class="fe ok ol om ob b">(batch_size)</code>，并保存标签信息。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="d41e" class="of ms jj ob b gy og oh l oi oj">train_loader = dataloaders['train']<br/>for ii, (data, target) in enumerate(train_loader):</span></pre></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="29a4" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">建模</h1><h2 id="882f" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">1.使用预先训练的模型创建模型</h2><p id="c28c" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">现在，这些预先训练好的模型可以在<code class="fe ok ol om ob b">torchvision</code>图书馆使用:</p><ul class=""><li id="665b" class="pa pb jj la b lb lc le lf lh pc ll pd lp pe lt pf pg ph pi bi translated"><a class="ae jg" href="https://arxiv.org/abs/1404.5997" rel="noopener ugc nofollow" target="_blank"> AlexNet </a></li><li id="c236" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated"><a class="ae jg" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"> VGG </a></li><li id="7c52" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated"><a class="ae jg" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet </a></li><li id="d7ff" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated"><a class="ae jg" href="https://arxiv.org/abs/1602.07360" rel="noopener ugc nofollow" target="_blank">挤压网</a></li><li id="0169" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated"><a class="ae jg" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNet </a></li><li id="a051" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated"><a class="ae jg" href="https://arxiv.org/abs/1512.00567" rel="noopener ugc nofollow" target="_blank">开始</a> v3</li><li id="622c" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated"><a class="ae jg" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank">谷歌网</a></li><li id="5745" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">沙狐球网 v2</li><li id="4b66" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated"><a class="ae jg" href="https://arxiv.org/abs/1801.04381" rel="noopener ugc nofollow" target="_blank">移动互联网</a> v2</li><li id="34f6" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated"><a class="ae jg" href="https://arxiv.org/abs/1611.05431" rel="noopener ugc nofollow" target="_blank"> ResNeXt </a></li><li id="bbde" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated"><a class="ae jg" href="https://pytorch.org/docs/stable/torchvision/models.html#wide-resnet" rel="noopener ugc nofollow" target="_blank">广ResNet </a></li><li id="1067" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated"><a class="ae jg" href="https://arxiv.org/abs/1807.11626" rel="noopener ugc nofollow" target="_blank">多边核安全网</a></li></ul><p id="a662" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我将在我们的数据集上使用resnet50，但是您也可以根据自己的选择有效地使用任何其他模型。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="44f1" class="of ms jj ob b gy og oh l oi oj">from torchvision import models<br/>model = models.resnet50(pretrained=True)</span></pre><p id="4b77" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们从冻结模型重量开始，因为我们不想改变renet50模型的重量。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="ef9b" class="of ms jj ob b gy og oh l oi oj"># Freeze model weights<br/>for param in model.parameters():<br/>    param.requires_grad = False</span></pre><p id="62ab" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要做的下一件事是用我们的自定义分类器替换模型中的线性分类层。我发现，要做到这一点，最好先看看模型结构，以确定最终的线性层是什么。我们可以简单地通过打印模型对象来做到这一点:</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="4215" class="of ms jj ob b gy og oh l oi oj">print(model)<br/>------------------------------------------------------------------<br/>ResNet(<br/>  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<br/>  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>  (relu): ReLU(inplace=True)<br/>   .<br/>   .<br/>   .<br/>   .</span><span id="02e0" class="of ms jj ob b gy oy oh l oi oj">(conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      (relu): ReLU(inplace=True)<br/>    )<br/>  )  <br/>(avgpool): AdaptiveAvgPool2d(output_size=(1, 1))<br/>  <strong class="ob jk">(fc): Linear(in_features=2048, out_features=1000, bias=True)</strong><br/>)</span></pre><p id="f5cb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我们发现从卷积层获取输入的最后一个线性层被命名为<code class="fe ok ol om ob b">fc</code></p><p id="aa34" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以使用我们定制的神经网络简单地替换<code class="fe ok ol om ob b">fc</code>层。该神经网络将来自前一层的输入输入到<code class="fe ok ol om ob b">fc</code>，并给出形状<code class="fe ok ol om ob b">(batch_size x n_classes)</code>的log softmax输出。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="3b20" class="of ms jj ob b gy og oh l oi oj">n_inputs = model.fc.in_features<br/>model.fc = nn.Sequential(<br/>                      nn.Linear(n_inputs, 256), <br/>                      nn.ReLU(), <br/>                      nn.Dropout(0.4),<br/>                      nn.Linear(256, n_classes),                   <br/>                      nn.LogSoftmax(dim=1))</span></pre><p id="cae3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，现在添加的新层在默认情况下是完全可训练的。</p><h2 id="75fc" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">2.在GPU上加载模型</h2><p id="793a" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">我们可以使用PyTorch的DataParallel来使用单个或多个GPU(如果我们有)。这里是我们可以用来检测GPU以及在GPU上加载模型的GPU数量。现在，我正在双英伟达泰坦RTX图形处理器上训练我的模型。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="39de" class="of ms jj ob b gy og oh l oi oj"># Whether to train on a gpu<br/>train_on_gpu = cuda.is_available()<br/>print(f'Train on gpu: {train_on_gpu}')</span><span id="b835" class="of ms jj ob b gy oy oh l oi oj"># Number of gpus<br/>if train_on_gpu:<br/>    gpu_count = cuda.device_count()<br/>    print(f'{gpu_count} gpus detected.')<br/>    if gpu_count &gt; 1:<br/>        multi_gpu = True<br/>    else:<br/>        multi_gpu = False</span><span id="d66e" class="of ms jj ob b gy oy oh l oi oj">if train_on_gpu:<br/>    model = model.to('cuda')</span><span id="8b95" class="of ms jj ob b gy oy oh l oi oj">if multi_gpu:<br/>    model = nn.DataParallel(model)</span></pre><h2 id="10f7" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">3.定义标准和优化器</h2><p id="18dc" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">当你训练任何模型时，最重要的事情之一是损失函数和优化器的选择。这里我们想使用分类交叉熵，因为我们有一个多类分类问题和<a class="ae jg" href="https://cs231n.github.io/neural-networks-3/#ada" rel="noopener ugc nofollow" target="_blank"> Adam </a>优化器，这是最常用的优化器。但是，由于我们对模型的输出应用LogSoftmax运算，因此我们将使用NLL损耗。</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="dd82" class="of ms jj ob b gy og oh l oi oj">from torch import optim</span><span id="d05f" class="of ms jj ob b gy oy oh l oi oj">criteration = nn.NLLLoss()<br/>optimizer = optim.Adam(model.parameters())</span></pre><h2 id="ccf5" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">4.训练模型</h2><p id="d442" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">下面给出了用于训练模型的完整代码。它本身看起来可能很大，但本质上我们所做的如下:</p><ul class=""><li id="51f9" class="pa pb jj la b lb lc le lf lh pc ll pd lp pe lt pf pg ph pi bi translated">开始运行纪元。在每个时代-</li><li id="7a26" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">使用<code class="fe ok ol om ob b">model.train()</code>将模型模式设置为训练。</li><li id="ff58" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">使用列车数据加载器循环数据。</li><li id="5eb4" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">使用<code class="fe ok ol om ob b">data, target = data.cuda(), target.cuda()</code>命令将你的数据加载到GPU</li><li id="a93c" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">使用<code class="fe ok ol om ob b">optimizer.zero_grad()</code>将优化器中的现有梯度设置为零</li><li id="10db" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">使用<code class="fe ok ol om ob b">output = model(data)</code>向前运行批次</li><li id="4b7d" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">使用<code class="fe ok ol om ob b">loss = criterion(output, target)</code>计算损失</li><li id="2951" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">使用<code class="fe ok ol om ob b">loss.backward()</code>通过网络反向传播损耗</li><li id="2cce" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">使用<code class="fe ok ol om ob b">optimizer.step()</code>采取优化步骤改变整个网络的权重</li><li id="bf76" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">训练循环中的所有其他步骤只是为了维护历史和计算准确性。</li><li id="dfee" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">使用<code class="fe ok ol om ob b">model.eval()</code>将模型模式设置为评估。</li><li id="1d9d" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">使用<code class="fe ok ol om ob b">valid_loader</code>获得验证数据的预测，并计算<code class="fe ok ol om ob b">valid_loss</code>和<code class="fe ok ol om ob b">valid_acc</code></li><li id="08f4" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">打印每个<code class="fe ok ol om ob b">print_every</code>时期的验证损失和验证准确度结果。</li><li id="792d" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated">保存基于验证损失的最佳模型。</li><li id="dd9d" class="pa pb jj la b lb pj le pk lh pl ll pm lp pn lt pf pg ph pi bi translated"><strong class="la jk">提前停止:</strong>如果交叉验证损失在<code class="fe ok ol om ob b">max_epochs_stop</code>内没有改善，则停止训练并加载验证损失最小的最佳可用模型。</li></ul><figure class="nv nw nx ny gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><p id="3c69" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是运行上述代码的输出。只显示最近几个时代。在第一个纪元中，验证精度从大约55%开始，我们以大约90%的验证精度结束。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/2ceb3545c8fc125e29f9f1a85bd8c04a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cq7Nnw6-fBY1is8JrdcQuQ.png"/></div></div></figure><p id="02e2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是显示损失和准确性指标的训练曲线:</p><div class="nv nw nx ny gt ab cb"><figure class="pr iv ps pt pu pv pw paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/9634488751177881422fd059cfc7258e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*QpfccxZpUBjT4p_5-TJNTg.png"/></div></figure><figure class="pr iv px pt pu pv pw paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><img src="../Images/bb7c6ad5c00b9f664546d2ed648c4e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*1T-lWdwgyVkQhJnQAPMFuw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk py di pz qa translated">训练曲线</p></figure></div></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="d398" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">推理和模型结果</h1><p id="c200" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">我们希望我们的结果以不同的方式使用我们的模型。首先，我们需要测试精度和混淆矩阵。创建这些结果的所有代码都在代码笔记本中。</p><h2 id="a717" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">1.试验结果</h2><p id="e906" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">测试模型的总体精度为:</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="98e6" class="of ms jj ob b gy og oh l oi oj">Overall Accuracy: 88.65 %</span></pre><p id="12b2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是测试数据集结果的混淆矩阵。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qb"><img src="../Images/817bbc37df39ca83890fbc056aa13fff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LBQ1udxKdbJ-IQWuiN_uiQ.png"/></div></div></figure><p id="d564" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们也可以看看类别的准确性。我还添加了列车计数，以便从新的角度查看结果。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qc"><img src="../Images/452a22f295a70e0e9c56ccb0ed56aa98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eZH5K6mL9M8anG3tpcWgew.png"/></div></div></figure><h2 id="990b" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">2.单个图像的可视化预测</h2><p id="b20b" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">出于部署目的，能够获得对单个图像的预测是有帮助的。你可以从笔记本上找到密码。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qd"><img src="../Images/d632b2bfe722ee029534443f0e142089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SEa9bH0AA5olwDAp1i1yFw.png"/></div></div></figure><h2 id="883d" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">3.可视化类别的预测</h2><p id="0b96" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">我们还可以看到用于调试和演示的分类结果。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qe"><img src="../Images/7448f44fd0ae0d19f84e43952807011d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mc4d5qBcBHMHgJZWB22weQ.png"/></div></div></figure><h2 id="ebf9" class="of ms jj bd mt on oo dn mx op oq dp nb lh or os nd ll ot ou nf lp ov ow nh ox bi translated">4.测试时间增加后的测试结果</h2><p id="d69c" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">我们还可以增加测试时间来提高我们的测试精度。这里我使用了一个新的测试数据加载器和转换:</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="5717" class="of ms jj ob b gy og oh l oi oj"># Image transformations<br/>tta_random_image_transforms = transforms.Compose([<br/>        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),<br/>        transforms.RandomRotation(degrees=15),<br/>        transforms.ColorJitter(),<br/>        transforms.RandomHorizontalFlip(),<br/>        transforms.CenterCrop(size=224),  # Image net standards<br/>        transforms.ToTensor(),<br/>        transforms.Normalize([0.485, 0.456, 0.406],<br/>                             [0.229, 0.224, 0.225])  # Imagenet standards<br/>    ])</span><span id="c89c" class="of ms jj ob b gy oy oh l oi oj"># Datasets from folders<br/>ttadata = {<br/>    'test':<br/>    datasets.ImageFolder(root=testdir, transform=tta_random_image_transforms)<br/>}</span><span id="1343" class="of ms jj ob b gy oy oh l oi oj"># Dataloader iterators<br/>ttadataloader = {<br/>    'test': DataLoader(ttadata['test'], batch_size=512, shuffle=False,num_workers=10)<br/>}</span></pre><p id="6d9f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们可以使用以下函数获得测试集上的预测:</p><figure class="nv nw nx ny gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><p id="f9ac" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上面的函数中，在得到预测之前，我对每张图片应用了5次<code class="fe ok ol om ob b">tta_random_image_transforms</code>。最终预测是所有五次预测的平均值。当我们在整个测试数据集上使用TTA时，我们注意到准确率提高了大约1%</p><pre class="nv nw nx ny gt oa ob oc od aw oe bi"><span id="7602" class="of ms jj ob b gy og oh l oi oj">TTA Accuracy: 89.71%</span></pre><p id="07bf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，以下是TTA的结果与正常结果的分类比较:</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/11f38d56b4963885191c554c255847f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*E5IHYiI-yHvkaNg_suXo5A.png"/></div></figure><p id="77a7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个小数据集中，TTA似乎没有增加多少价值，但我注意到它增加了大数据集的价值。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="2f12" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">结论</h1><p id="6d2c" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">在这篇文章中，我谈到了使用PyTorch进行多类图像分类项目的端到端管道。我们致力于创建一些现成的代码来使用迁移学习训练模型，可视化结果，使用测试时间增加，并获得对单个图像的预测，以便我们可以在需要时使用任何工具部署我们的模型，如<a class="ae jg" rel="noopener" target="_blank" href="/how-to-write-web-apps-using-simple-python-for-data-scientists-a227a1a01582"> Streamlit </a>。</p><p id="dd6e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在<a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/tree/master/compvisblog" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk"> Github </strong> </a>上找到这篇文章的完整代码。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="0460" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想了解更多关于图像分类和卷积神经网络的知识，看看来自吴恩达的深度学习专业。此外，要了解PyTorch的更多信息并从基础开始，您可以看看IBM提供的<a class="ae jg" href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch?ranMID=40328&amp;ranEAID=lVarvwc5BD0&amp;ranSiteID=lVarvwc5BD0-qRrseBA2NWUg_WxbnxmDwQ&amp;siteID=lVarvwc5BD0-qRrseBA2NWUg_WxbnxmDwQ&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="noopener ugc nofollow" target="_blank">深度神经网络与PyTorch </a>课程。</p><p id="e16c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal" rel="noopener"> <strong class="la jk">媒体</strong> </a>关注我，或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过Twitter <a class="ae jg" href="https://twitter.com/MLWhiz" rel="noopener ugc nofollow" target="_blank"> @mlwhiz </a>联系到我。</p><p id="67a7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，一个小小的免责声明——这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p><p id="d445" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">本帖首发</em> <a class="ae jg" href="https://lionbridge.ai/articles/end-to-end-multiclass-image-classification-using-pytorch-and-transfer-learning/" rel="noopener ugc nofollow" target="_blank"> <em class="lu">此处</em> </a> <em class="lu">。</em></p></div></div>    
</body>
</html>