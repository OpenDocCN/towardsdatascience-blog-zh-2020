<html>
<head>
<title>Cluster-then-predict for classification tasks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类任务的先聚类后预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cluster-then-predict-for-classification-tasks-142fdfdc87d6?source=collection_archive---------0-----------------------#2020-02-10">https://towardsdatascience.com/cluster-then-predict-for-classification-tasks-142fdfdc87d6?source=collection_archive---------0-----------------------#2020-02-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c282" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">如何在监督学习问题中利用无监督学习</em></h2></div><h1 id="363c" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">介绍</h1><p id="1d86" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">监督分类问题需要一个数据集，该数据集具有(a)一个分类因变量(“目标变量”)和(b)一组独立变量(“特征”)，这些变量可能(也可能不！)对预测班级有用。建模任务是学习将特征及其值映射到目标类的函数。这方面的一个例子是<a class="ae lx" rel="noopener" target="_blank" href="/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8">逻辑回归</a>。</p><p id="d5a3" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">无监督学习采用没有标签的数据集，并试图在数据中找到一些潜在的结构。K-means 就是这样一种算法。在本文中，我将向您展示如何通过使用 k-means 发现数据集中的潜在“聚类”来提高分类器的性能，并使用这些聚类作为数据集中的新特征，或者按聚类对数据集进行分区，并在每个聚类上训练单独的分类器。</p><h1 id="0e04" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">资料组</h1><p id="5b8e" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们首先使用 sklearn 的 make_classification 实用程序生成一个 nonce 数据集。我们将模拟一个多类分类问题，并生成 15 个用于预测的特征。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="1a95" class="mm kk it mi b gy mn mo l mp mq">from sklearn.datasets import make_classification</span><span id="fb91" class="mm kk it mi b gy mr mo l mp mq">X, y = make_classification(n_samples=1000, n_features=8, n_informative=5, n_classes=4)</span></pre><p id="7abf" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">我们现在有一个 1000 行的数据集，有 4 个类和 8 个特征，其中 5 个是信息性的(另外 3 个是随机噪声)。为了便于操作，我们将这些转换成熊猫数据帧。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="984e" class="mm kk it mi b gy mn mo l mp mq">import pandas as pd</span><span id="2e49" class="mm kk it mi b gy mr mo l mp mq">df = pd.DataFrame(X, columns=['f{}'.format(i) for i in range(8)])</span></pre><h1 id="e4d2" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">分为训练/测试</h1><p id="b8af" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">现在，我们可以将数据分为训练集和测试集(75/25)两部分。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="02b8" class="mm kk it mi b gy mn mo l mp mq">from sklearn.model_selection import train_test_split</span><span id="90fc" class="mm kk it mi b gy mr mo l mp mq">X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.25, random_state=90210)</span></pre><h1 id="ab0a" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">应用 K-均值</h1><p id="4407" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">首先，您将想要确定给定数据集的最佳<em class="ms"> k </em>。</p><p id="fb0e" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">为了简洁起见，也为了不偏离本文的目的，我向读者推荐这篇优秀的教程:<a class="ae lx" href="https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb" rel="noopener">如何确定 K-Means 的最优 K？如果你想进一步了解这件事。</a></p><p id="9b64" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">在我们的例子中，因为我们使用了 make_classification 实用程序，所以参数</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="a229" class="mm kk it mi b gy mn mo l mp mq">n_clusters_per_class</span></pre><p id="90cf" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">已经设置，默认为 2。因此，我们不需要确定最优的<em class="ms">k；然而，我们确实需要识别集群！我们将使用以下函数来查找训练集中的 2 个分类，然后为我们的测试集预测它们。</em></p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="14a6" class="mm kk it mi b gy mn mo l mp mq">import numpy as np<br/>from sklearn.cluster import KMeans<br/>from typing import Tuple</span><span id="c79e" class="mm kk it mi b gy mr mo l mp mq">def get_clusters(X_train: pd.DataFrame, X_test: pd.DataFrame, n_clusters: int) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:<br/>    """<br/>    applies k-means clustering to training data to find clusters and predicts them for the test set<br/>    """<br/>    clustering = KMeans(n_clusters=n_clusters, random_state=8675309,n_jobs=-1)<br/>    clustering.fit(X_train)<br/>    # apply the labels<br/>    train_labels = clustering.labels_<br/>    X_train_clstrs = X_train.copy()<br/>    X_train_clstrs['clusters'] = train_labels<br/>    <br/>    # predict labels on the test set<br/>    test_labels = clustering.predict(X_test)<br/>    X_test_clstrs = X_test.copy()<br/>    X_test_clstrs['clusters'] = test_labels<br/>    return X_train_clstrs, X_test_clstrs</span><span id="7ab2" class="mm kk it mi b gy mr mo l mp mq">X_train_clstrs, X_test_clstrs = get_clusters(X_train, X_test, 2)</span></pre><p id="e185" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">我们现在有了一个新的特性，叫做“集群”，值为 0 或 1。</p><figure class="md me mf mg gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mt"><img src="../Images/ce48e3e6b8af8e69bcba3b52ab77c713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GrnYb_CJMBJ9Wnz_xeaInQ.png"/></div></div></figure><h1 id="3681" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">缩放比例</h1><p id="0283" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在我们拟合任何模型之前，我们需要缩放我们的特征:这确保所有的特征都在相同的数字尺度上。对于像逻辑回归这样的线性模型，在训练期间学习的系数的大小将取决于特征的尺度。如果您有 0-1 范围内的要素，而其他要素的范围为 0-100，则无法可靠地比较这些系数。</p><p id="95bd" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">为了扩展这些特征，我们使用下面的函数来计算每个特征的 z 分数，并将训练集的学习映射到测试集。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="59de" class="mm kk it mi b gy mn mo l mp mq">from sklearn.preprocessing import StandardScaler</span><span id="07d0" class="mm kk it mi b gy mr mo l mp mq">def scale_features(X_train: pd.DataFrame, X_test: pd.DataFrame) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:<br/>    """<br/>    applies standard scaler (z-scores) to training data and predicts z-scores for the test set<br/>    """<br/>    scaler = StandardScaler()<br/>    to_scale = [col for col in X_train.columns.values]<br/>    scaler.fit(X_train[to_scale])<br/>    X_train[to_scale] = scaler.transform(X_train[to_scale])<br/>    <br/>    # predict z-scores on the test set<br/>    X_test[to_scale] = scaler.transform(X_test[to_scale])<br/>    <br/>    return X_train, X_test</span><span id="2493" class="mm kk it mi b gy mr mo l mp mq">X_train_scaled, X_test_scaled = scale_features(X_train_clstrs, X_test_clstrs)</span></pre><p id="82ae" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">我们现在准备运行一些实验！</p><h1 id="7f0d" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">实验</h1><p id="e77d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我选择使用逻辑回归来解决这个问题，因为它非常快，并且通过检查系数可以快速评估特征的重要性。</p><p id="04f7" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">为了运行我们的实验，我们将在 4 个数据集上构建逻辑回归模型:</p><ol class=""><li id="83ad" class="nb nc it ld b le ly lh lz lk nd lo ne ls nf lw ng nh ni nj bi translated">没有聚类信息的数据集(基本)</li><li id="988d" class="nb nc it ld b le nk lh nl lk nm lo nn ls no lw ng nh ni nj bi translated">以“聚类”为特征的数据集(聚类-特征)</li><li id="643e" class="nb nc it ld b le nk lh nl lk nm lo nn ls no lw ng nh ni nj bi translated">df 的数据集["群集"] == 0(群集-0)</li><li id="2027" class="nb nc it ld b le nk lh nl lk nm lo nn ls no lw ng nh ni nj bi translated">df 的数据集[“群集”] == 1(群集-1)</li></ol><p id="bd15" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">我们的研究是 1x4 组间设计，以数据集[基础、聚类特征、聚类-0、聚类-1]为唯一因素。下面创建我们的数据集。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="9caf" class="mm kk it mi b gy mn mo l mp mq"># to divide the df by cluster, we need to ensure we use the correct class labels, we'll use pandas to do that<br/>train_clusters = X_train_scaled.copy()<br/>test_clusters = X_test_scaled.copy()<br/>train_clusters['y'] = y_train<br/>test_clusters['y'] = y_test</span><span id="9ec3" class="mm kk it mi b gy mr mo l mp mq"># locate the "0" cluster<br/>train_0 = train_clusters.loc[train_clusters.clusters &lt; 0] # after scaling, 0 went negtive<br/>test_0 = test_clusters.loc[test_clusters.clusters &lt; 0]<br/>y_train_0 = train_0.y.values<br/>y_test_0 = test_0.y.values</span><span id="0e78" class="mm kk it mi b gy mr mo l mp mq"># locate the "1" cluster<br/>train_1 = train_clusters.loc[train_clusters.clusters &gt; 0] # after scaling, 1 dropped slightly<br/>test_1 = test_clusters.loc[test_clusters.clusters &gt; 0]<br/>y_train_1 = train_1.y.values<br/>y_test_1 = test_1.y.values</span><span id="3ed4" class="mm kk it mi b gy mr mo l mp mq"># the base dataset has no "clusters" feature<br/>X_train_base = X_train_scaled.drop(columns=['clusters'])<br/>X_test_base = X_test_scaled.drop(columns=['clusters'])</span><span id="cc5d" class="mm kk it mi b gy mr mo l mp mq"># drop the targets from the training set<br/>X_train_0 = train_0.drop(columns=['y'])<br/>X_test_0 = test_0.drop(columns=['y'])<br/>X_train_1 = train_1.drop(columns=['y'])<br/>X_test_1 = test_1.drop(columns=['y'])</span><span id="cebd" class="mm kk it mi b gy mr mo l mp mq">datasets = {<br/>    'base': (X_train_base, y_train, X_test_base, y_test),<br/>    'cluster-feature': (X_train_scaled, y_train, X_test_scaled, y_test),<br/>    'cluster-0': (X_train_0, y_train_0, X_test_0, y_test_0),<br/>    'cluster-1': (X_train_1, y_train_1, X_test_1, y_test_1),<br/>}</span></pre><figure class="md me mf mg gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi np"><img src="../Images/3a4d71ceddbb39ffaceb94c0ce6ec86f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9O-9xAajrnSQbCCoGKnWCQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">“基础”数据集</p></figure><figure class="md me mf mg gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nu"><img src="../Images/1a9665c84775a03280923d4903983500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UkUpPq1nGTKLRDvor0rmqw.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">“聚类-特征”数据集</p></figure><figure class="md me mf mg gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nv"><img src="../Images/182cd699068785c9300fc73b65f72dba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fvWXq4_0dkwefqdoQb9Mtg.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">“0 类”数据集</p></figure><figure class="md me mf mg gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nw"><img src="../Images/dc4fe5bb328111be481a851ddf0865e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yvF_Z9342Nfuk7mItaH7VQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">“聚类-1”数据集</p></figure><p id="804c" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">为了有效地运行我们的实验，我们将使用下面的函数，该函数遍历 4 个数据集，并在每个数据集上运行 5 重交叉遍历。对于每个数据集，我们获得每个分类器的 5 个估计值:准确度、加权精度、加权召回率和加权 f1。我们将绘制这些图来观察总体性能。然后，我们从每个模型各自的测试集上获得分类报告，以评估细粒度的性能。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="9bac" class="mm kk it mi b gy mn mo l mp mq">from sklearn.linear_model import LogisticRegression<br/>from sklearn import model_selection<br/>from sklearn.metrics import classification_report</span><span id="71b1" class="mm kk it mi b gy mr mo l mp mq">def run_exps(datasets: dict) -&gt; pd.DataFrame:<br/>    '''<br/>    runs experiments on a dict of datasets<br/>    '''<br/>    # initialize a logistic regression classifier<br/>    model = LogisticRegression(class_weight='balanced', solver='lbfgs', random_state=999, max_iter=250)<br/>    <br/>    dfs = []<br/>    results = []<br/>    conditions = []<br/>    scoring = ['accuracy','precision_weighted','recall_weighted','f1_weighted']</span><span id="0dd3" class="mm kk it mi b gy mr mo l mp mq">for condition, splits in datasets.items():<br/>        X_train = splits[0]<br/>        y_train = splits[1]<br/>        X_test = splits[2]<br/>        y_test = splits[3]<br/>        <br/>        kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)<br/>        cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)<br/>        clf = model.fit(X_train, y_train)<br/>        y_pred = clf.predict(X_test)<br/>        print(condition)<br/>        print(classification_report(y_test, y_pred))</span><span id="8cf6" class="mm kk it mi b gy mr mo l mp mq">results.append(cv_results)<br/>        conditions.append(condition)</span><span id="25ec" class="mm kk it mi b gy mr mo l mp mq">this_df = pd.DataFrame(cv_results)<br/>        this_df['condition'] = condition<br/>        dfs.append(this_df)</span><span id="b44f" class="mm kk it mi b gy mr mo l mp mq">final = pd.concat(dfs, ignore_index=True)<br/>    <br/>    # We have wide format data, lets use pd.melt to fix this<br/>    results_long = pd.melt(final,id_vars=['condition'],var_name='metrics', value_name='values')<br/>    <br/>    # fit time metrics, we don't need these<br/>    time_metrics = ['fit_time','score_time'] <br/>    results = results_long[~results_long['metrics'].isin(time_metrics)] # get df without fit data<br/>    results = results.sort_values(by='values')<br/>    <br/>    return results</span><span id="4582" class="mm kk it mi b gy mr mo l mp mq">df = run_exps(datasets)</span></pre><figure class="md me mf mg gt mu gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ffaaf18050434de21ee74275a5d6fab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*2xfWs9m4MNZB-qjcxDqeOw.png"/></div></figure><h1 id="19e1" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">结果</h1><p id="9311" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">让我们画出我们的结果，看看每个数据集如何影响分类器的性能。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="cab0" class="mm kk it mi b gy mn mo l mp mq">import matplotlib<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="d859" class="mm kk it mi b gy mr mo l mp mq">plt.figure(figsize=(20, 12))<br/>sns.set(font_scale=2.5)<br/>g = sns.boxplot(x="condition", y="values", hue="metrics", data=df, palette="Set3")<br/>plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)<br/>plt.title('Comparison of Dataset by Classification Metric')</span></pre><figure class="md me mf mg gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ny"><img src="../Images/225ac71acc792311193d3d813c0e118f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i8cBrNcY704x_jDyXwUatw.png"/></div></div></figure><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="874a" class="mm kk it mi b gy mn mo l mp mq">pd.pivot_table(df, index='condition',columns=['metrics'],values=['values'], aggfunc='mean')</span></pre><figure class="md me mf mg gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi np"><img src="../Images/ac93c3681b5c04dd0b4e37315ce02b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-R_k0yW733VZGGoqx1muw.png"/></div></div></figure><p id="462d" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">一般来说，我们的“基本”数据集，没有聚类信息，创建了性能最差的分类器。通过添加我们的二进制“集群”作为特性，我们看到了性能的适度提升；然而，当我们在每个集群上安装一个模型时，我们会看到最大的性能提升。</p><p id="bbc6" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">当我们查看用于细粒度性能评估的分类报告时，画面变得非常清晰:当数据集按集群分段时，我们看到了性能的大幅提升。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="31c9" class="mm kk it mi b gy mn mo l mp mq">base<br/>              precision    recall  f1-score   support<br/><br/>           0       0.48      0.31      0.38        64<br/>           1       0.59      0.59      0.59        71<br/>           2       0.42      0.66      0.51        50<br/>           3       0.59      0.52      0.55        65<br/><br/>    accuracy                           0.52       250<br/>   macro avg       0.52      0.52      0.51       250<br/>weighted avg       0.53      0.52      0.51       250<br/><br/>cluster-feature<br/>              precision    recall  f1-score   support<br/><br/>           0       0.43      0.36      0.39        64<br/>           1       0.59      0.62      0.60        71<br/>           2       0.40      0.56      0.47        50<br/>           3       0.57      0.45      0.50        65<br/><br/>    accuracy                           0.50       250<br/>   macro avg       0.50      0.50      0.49       250<br/>weighted avg       0.50      0.50      0.49       250<br/><br/>cluster-0<br/>              precision    recall  f1-score   support<br/><br/>           0       0.57      0.41      0.48        29<br/>           1       0.68      0.87      0.76        30<br/>           2       0.39      0.45      0.42        20<br/>           3       0.73      0.66      0.69        29<br/><br/>    accuracy                           0.61       108<br/>   macro avg       0.59      0.60      0.59       108<br/>weighted avg       0.61      0.61      0.60       108<br/><br/>cluster-1<br/>              precision    recall  f1-score   support<br/><br/>           0       0.41      0.34      0.38        35<br/>           1       0.54      0.46      0.50        41<br/>           2       0.49      0.70      0.58        30<br/>           3       0.60      0.58      0.59        36<br/><br/>    accuracy                           0.51       142<br/>   macro avg       0.51      0.52      0.51       142<br/>weighted avg       0.51      0.51      0.51       142</span></pre><p id="01d9" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">考虑类别“0”，跨四个数据集的 f1 分数为</p><ul class=""><li id="d057" class="nb nc it ld b le ly lh lz lk nd lo ne ls nf lw nz nh ni nj bi translated">基数—“0”F1:0.38</li><li id="4d7c" class="nb nc it ld b le nk lh nl lk nm lo nn ls no lw nz nh ni nj bi translated">聚类特征—“0”F1:0.39</li><li id="524f" class="nb nc it ld b le nk lh nl lk nm lo nn ls no lw nz nh ni nj bi translated">群集-0—“0”F1:0.48</li><li id="619d" class="nb nc it ld b le nk lh nl lk nm lo nn ls no lw nz nh ni nj bi translated">群集 1—“0”F1:0.38</li></ul><p id="30ed" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">对于“0”类，在 cluster-0 数据集上训练的模型显示 f1 分数比其他模型和数据集相对提高了约 23%。</p><h1 id="a88e" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">结论和下一步措施</h1><p id="701b" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在本文中，我展示了如何利用“先聚类后预测”来解决分类问题，并梳理了一些表明这种技术可以提高性能的结果。在集群创建和结果评估方面，还有很多工作要做。</p><p id="c8ec" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">在我们的例子中，我们有一个包含两个集群的数据集；然而，在您的问题中，您可能会发现更多的集群。(一旦您在数据集上使用肘方法确定了最佳<em class="ms"> k </em>！)</p><p id="f3ca" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">在<em class="ms"> k &gt; 2 </em>的情况下，您可以将“clusters”特性视为分类变量，并应用一键编码在您的模型中使用它们。随着<em class="ms"> k </em>的增加，如果您决定为每个集群拟合一个模型，您可能会遇到<a class="ae lx" href="https://medium.com/predict/what-overfitting-is-and-how-to-fix-it-887da4bf2cba" rel="noopener">过度拟合</a>的问题。</p><p id="9698" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">如果您发现 K-Means 没有提高您的分类器的性能，也许您的数据更适合另一种聚类算法——参见<a class="ae lx" rel="noopener" target="_blank" href="/clustering-analyses-with-highly-imbalanced-datasets-27e486cd82a4">本文</a>介绍不平衡数据集上的层次聚类。</p><p id="f487" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">和所有数据科学问题一样，实验，实验，实验！对不同的技术进行测试，让数据指导您的建模决策。</p><h1 id="dcb2" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">参考</h1><p id="3f57" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><a class="ae lx" href="https://conference.scipy.org/proceedings/scipy2010/pdfs/mckinney.pdf" rel="noopener ugc nofollow" target="_blank">python 中统计计算的数据结构</a>，McKinney，第 9 届科学中的 Python 会议录，第 445 卷，2010 年。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="3347" class="mm kk it mi b gy mn mo l mp mq">@software{reback2020pandas,<br/>    author       = {The pandas development team},<br/>    title        = {pandas-dev/pandas: Pandas},<br/>    month        = feb,<br/>    year         = 2020,<br/>    publisher    = {Zenodo},<br/>    version      = {latest},<br/>    doi          = {10.5281/zenodo.3509134},<br/>    url          = {https://doi.org/10.5281/zenodo.3509134}<br/>}</span></pre><p id="1ecf" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">Harris，C.R .，Millman，K.J .，van der Walt，S.J .等人<em class="ms">用 NumPy 进行数组编程</em>。自然 585，357–362(2020)。DOI:<a class="ae lx" href="https://doi.org/10.1038/s41586-020-2649-2" rel="noopener ugc nofollow" target="_blank">10.1038/s 41586–020–2649–2</a>。</p><p id="6a1e" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated"><a class="ae lx" href="http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html" rel="noopener ugc nofollow" target="_blank">sci kit-learn:Python 中的机器学习</a>，Pedregosa <em class="ms">等人</em>，JMLR 12，第 2825–2830 页，2011 年。</p><p id="5b2b" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated"><a class="ae lx" href="https://doi.org/10.1109/MCSE.2007.55" rel="noopener ugc nofollow" target="_blank"> J. D. Hunter，“Matplotlib:2D 图形环境”，科学计算&amp;工程，第 9 卷，第 3 期，第 90–95 页，2007 年</a>。</p><p id="363e" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">瓦斯科姆，法学硕士，(2021 年)。seaborn:统计数据可视化。《开源软件杂志》，6 卷(60 期)，3021 页，<a class="ae lx" href="https://doi.org/10.21105/joss.03021" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.21105/joss.03021</a></p></div></div>    
</body>
</html>