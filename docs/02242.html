<html>
<head>
<title>Bayesian Hierarchical Modeling (or “more reasons why autoML cannot replace Data Scientists yet”)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯分层建模(或者“autoML 还不能取代数据科学家的更多原因”)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-hierarchical-modeling-or-more-reasons-why-automl-cannot-replace-data-scientists-yet-d01e7d571d3d?source=collection_archive---------8-----------------------#2020-03-03">https://towardsdatascience.com/bayesian-hierarchical-modeling-or-more-reasons-why-automl-cannot-replace-data-scientists-yet-d01e7d571d3d?source=collection_archive---------8-----------------------#2020-03-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ca03" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/frequentist-to-bayesian" rel="noopener" target="_blank">从频率主义到贝叶斯主义</a></h2><div class=""/><div class=""><h2 id="6901" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">贝叶斯网络允许对变量之间的因果关系进行建模，弥补数据提供的信息的不足。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/bf53a2877d59c5d6b8450de655f09fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oB4timpxL-ed6yuc"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@alinnnaaaa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿丽娜·格鲁布尼亚</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7fde" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本文中，我们将使用一个为 python 开发的<strong class="lh ja">概率编程</strong> <strong class="lh ja">库</strong>，<strong class="lh ja"> pymc3 </strong>。对贝叶斯方法在基本统计学中的效率和 pymc3 的初步介绍可以在这里的<a class="ae le" rel="noopener" target="_blank" href="/from-frequentism-to-bayesianism-hypothesis-testing-a-simple-illustration-11213232e551"><strong class="lh ja"/></a>和那里的<a class="ae le" rel="noopener" target="_blank" href="/from-frequentism-to-bayesianism-going-deeper-part-2-offline-a-b-test-d3324f7a39bb"><strong class="lh ja"/></a>中找到。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="ae3b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mi translated"><span class="l mj mk ml bm mm mn mo mp mq di"> S </span>自 2018 年初以来，<strong class="lh ja">自动化机器学习</strong>已经成为数据科学中最时尚的话题之一。仅举几个例子，亚马逊的 Sagemaker 或谷歌 AutoML 现在对大多数数据科学家来说都是可访问的，以至于一些人倾向于认为探索和理解数据不再是建立机器学习模型的必要条件。</p><p id="e370" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">AutoML 的承诺可以这样总结:</p><blockquote class="mr"><p id="2380" class="ms mt iq bd mu mv mw mx my mz na ma dk translated">高度自动化允许非专家利用机器学习模型和技术，而不需要首先成为特定领域的专家。</p></blockquote><p id="4afe" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">推理很简单；不再需要专业知识，只需将数据交给 AutoML 算法，在测试固定数量的预定义模型后，它会返回<em class="ng">最好的</em>一个。</p><p id="9631" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是这里有一个问题… <strong class="lh ja"> AutoML 算法忽略了<em class="ng">最佳</em>对我们来说意味着什么，而仅仅是试图最小化经验误差</strong>。仍然需要知识和专业技能来理解这个误差真正意味着什么，以及它在多大程度上不同于我们实际上希望最小化的误差。</p><h1 id="fbf7" class="nh ni iq bd nj nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">我们应该考虑什么样的误差度量？</h1><h2 id="fc0c" class="nz ni iq bd nj oa ob dn nn oc od dp nr lo oe of nt ls og oh nv lw oi oj nx iw bi translated">1.经验误差</h2><blockquote class="ok ol om"><p id="2a43" class="lf lg ng lh b li lj ka lk ll lm kd ln on lp lq lr oo lt lu lv op lx ly lz ma ij bi translated">给定 n 个数据点，经验误差由下式给出</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/084ccdf0f2985d721105146a1841f77a.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*LA-LhF1OPzNYPBACl-XVIQ.png"/></div></div></figure><blockquote class="ok ol om"><p id="eaa3" class="lf lg ng lh b li lj ka lk ll lm kd ln on lp lq lr oo lt lu lv op lx ly lz ma ij bi translated">对于特征 xi 和目标 yi 的所有可观测值上的特定函数 fn，其中 V 表示损失函数。</p></blockquote><p id="4048" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它通常通过交叉验证或训练/测试过程来计算。缺少协变量和目标之间的一些联系将产生偏差，常常导致欠拟合/过拟合，并最终导致更高的测试/交叉验证误差。</p><p id="b1ca" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">数据科学家(以及 autoML 引擎)常常错误地只考虑这一点。</p><h2 id="a5a8" class="nz ni iq bd nj oa ob dn nn oc od dp nr lo oe of nt ls og oh nv lw oi oj nx iw bi translated">2.泛化误差</h2><blockquote class="ok ol om"><p id="8041" class="lf lg ng lh b li lj ka lk ll lm kd ln on lp lq lr oo lt lu lv op lx ly lz ma ij bi translated"><strong class="lh ja">泛化误差</strong>(也称为<strong class="lh ja">样本外误差</strong> r)是一种衡量算法能够多准确地预测先前未见过的数据的结果值的方法。其定义如下:</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/3a464867e508a1b02cc3c90ce3cca934.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*_KffukZbRknXgMIsKDUJ5g.png"/></div></figure><blockquote class="ok ol om"><p id="fbab" class="lf lg ng lh b li lj ka lk ll lm kd ln on lp lq lr oo lt lu lv op lx ly lz ma ij bi translated">对于特征 x 和目标 y 的所有可能值上的特定函数 fn，其中 V 表示损失函数，ρ(x，y)是 x 和 y 的未知联合概率分布</p></blockquote><p id="287e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">显然，在训练我们的模型时，它通常是我们希望最小化的<strong class="lh ja">。不幸的是，它不能直接从数据中计算出来，不像经验数据那样仅仅是对前者的估计。</strong></p><p id="9ae5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有了足够多的高质量数据，我们就可以交换这些数据。现在想象一下，在训练时只有部分信息可用，或者根本没有足够的数据，就像机器学习中经常出现的情况一样。然后，我们的模型将在不同的分布<em class="ng"> ρ(x，y) </em>上进行训练，该分布不同于稍后运行的分布，因此，经验误差将偏离广义误差。</p><p id="a4a0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="ng">有偏差的数据导致有偏差的经验误差。</em>T13】</strong></p><p id="7b35" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们已经可以看出这是一个多么大的问题:经验误差不再估计期望的数量，而是我们唯一可以从数据中计算出来的。我们需要确保它在某种程度上仍然是可靠的。这是人类专业知识发挥作用并展示其全部潜力的地方。</p><p id="bedd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">同样，当变量之间的关系明确时，参数方法比非参数方法更受青睐，来自研究领域的知识通过显式建模一些相关性来弥补数据信息的不足。</p><p id="cc28" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">我们的模型从部分或截断分布中推断得越好，经验和泛化误差就越接近。</strong></p><h2 id="6bf1" class="nz ni iq bd nj oa ob dn nn oc od dp nr lo oe of nt ls og oh nv lw oi oj nx iw bi translated">3.混杂误差</h2><blockquote class="mr"><p id="83a5" class="ms mt iq bd mu mv mw mx my mz na ma dk translated">"相关性并不意味着因果关系."</p></blockquote><p id="d712" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">所有数据科学家都至少听过这句话一次，但事实证明，在实际建模时，只有少数人真正意识到其中的含义。因果关系的话题实际上大多数时候都被忽略了<strong class="lh ja">，有时是错误的，有时是合理的，但很少是故意的。</strong></p><ul class=""><li id="ce93" class="os ot iq lh b li lj ll lm lo ou ls ov lw ow ma ox oy oz pa bi translated"><em class="ng"> A </em> <strong class="lh ja"> <em class="ng">混杂因素</em> </strong> <em class="ng">是与协变量和利益结果都有因果关系的变量。作为一个因果概念，它不能用相关性来描述。</em></li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/fef6063888a07fc2c1c275f31e22d9f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*Ej8Xko-pJoHI5YmpvunWZw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">因果图:混杂因素 W 影响因变量 Y 和自变量 X</p></figure><p id="fb3d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">由混杂因素产生的误差不能用传统的统计方法完全测量，因为它本身不是统计误差。</p><p id="3bfc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有人可能会争辩说，当经验误差很低，甚至是泛化误差时；那么我们不应该关心我们的模型是利用了真正的因果关系还是虚假的相关性。在某些情况下这可能是真的，但是除非我们明确知道为什么，否则这种情况不应该被忽视。</p><h2 id="a331" class="nz ni iq bd nj oa ob dn nn oc od dp nr lo oe of nt ls og oh nv lw oi oj nx iw bi translated">一个令人困惑的例证:辛普森悖论</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/1133738347246616b8c207428cc46bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1r_s5QKOk2LRFjazaXghgA.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">By Pace~svwiki —自己的作品，CC BY-SA 4.0，<a class="ae le" href="https://commons.wikimedia.org/w/index.php?curid=62007681" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/w/index.php?curid=62007681</a></p></figure><blockquote class="ok ol om"><p id="750e" class="lf lg ng lh b li lj ka lk ll lm kd ln on lp lq lr oo lt lu lv op lx ly lz ma ij bi translated"><strong class="lh ja">辛普森悖论</strong>由 Edward Simpson 在 1951 年和 George Udny Yule 在 1903 年描述，是一个悖论，当数据被分割成单独的组时，统计趋势出现，但当组被组合时，趋势发生逆转。</p></blockquote><p id="eff1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当因果关系被忽略时，通常在存在混杂变量(这里用不同的颜色组表示)的情况下观察到。</p><p id="1b3b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">辛普森悖论对机器学习的影响是，当涉及到<strong class="lh ja">决策时，</strong>我们可以选择应该考虑哪些数据来采取行动，是聚合还是分割？</p><p id="0cca" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不幸的是，答案通常不能从数据本身推断出来。<strong class="lh ja">事实上，根据变量</strong>之间的因果关系，对于完全相似的值，我们可以有不同的答案 <strong class="lh ja">。<strong class="lh ja"/><strong class="lh ja">真正的误差来源于数据本身之外</strong>，最终测试或交叉验证误差有多低并不重要。除非我们能够首先正确地对环境建模，否则我们无法避免做出完全错误的决策。</strong></p><h1 id="d13c" class="nh ni iq bd nj nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">空间混杂和贝叶斯分层建模</h1><p id="8b24" class="pw-post-body-paragraph lf lg iq lh b li pd ka lk ll pe kd ln lo pf lq lr ls pg lu lv lw ph ly lz ma ij bi translated"><em class="ng">我们现在将通过一个具体的例子来说明在分层依赖的具体情况下如何处理这种情况。</em></p><p id="542b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通常，数据科学家必须处理地理数据，这些数据的缺点是不容易被经典的机器学习模型利用。位置要素通常具有很高的基数，并且经常是不平衡的。然而，很明显，模型的参数会因地区而异，取决于当地的变量，但仍然密切相关，因为它们对不同地方的相同现象进行建模。</p><p id="b414" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过下面的例子，我们将看到<strong class="lh ja">处理这种空间相关性</strong>的方法，从而<strong class="lh ja">最小化上面</strong>列出的所有类型的误差。</p><h2 id="ccc5" class="nz ni iq bd nj oa ob dn nn oc od dp nr lo oe of nt ls og oh nv lw oi oj nx iw bi translated">问题陈述</h2><p id="9a78" class="pw-post-body-paragraph lf lg iq lh b li pd ka lk ll pe kd ln lo pf lq lr ls pg lu lv lw ph ly lz ma ij bi translated">我们考虑估计孟加拉国避孕药具使用情况的问题，为此，我们使用了来自 1988 年孟加拉国生育率调查的数据。它包括分组在 60 个区的 1934 名妇女的子样本，变量定义如下:</p><ul class=""><li id="4bed" class="os ot iq lh b li lj ll lm lo ou ls ov lw ow ma ox oy oz pa bi translated">地区:每个地区的识别码。</li><li id="693d" class="os ot iq lh b li pi ll pj lo pk ls pl lw pm ma ox oy oz pa bi translated">LC:调查时存活儿童的数量。</li><li id="99e4" class="os ot iq lh b li pi ll pj lo pk ls pl lw pm ma ox oy oz pa bi translated">年龄:调查时妇女的年龄。</li><li id="e61a" class="os ot iq lh b li pi ll pj lo pk ls pl lw pm ma ox oy oz pa bi translated">城市:居住区域的类型。</li></ul><p id="477b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将考虑 3 个具有不同特征的逻辑贝叶斯回归来模拟不同的可能方法。</p><h2 id="1108" class="nz ni iq bd nj oa ob dn nn oc od dp nr lo oe of nt ls og oh nv lw oi oj nx iw bi translated">1:合并测量</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/c037a910ac9507fc9e3b11497f105ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*1gl2RVGbdz7i56BxzCs4HA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">汇集模型</p></figure><p id="8509" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这里，我们简单地通过不使用区域变量来忽略它的作用。结果是只有 4 个参数的简单逻辑回归，包括截距。位置不被看作是一个混杂因素，每个区域被假定为行为相似。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="po pp l"/></div></figure><h2 id="8b80" class="nz ni iq bd nj oa ob dn nn oc od dp nr lo oe of nt ls og oh nv lw oi oj nx iw bi translated">2:非池化回归</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/d56c596fd6c62b92c6dbce5489b0e233.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*F5rr0DGQu9UH6qVaSry0og.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">无池模型</p></figure><p id="3213" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于每个地区，我们拟合了不同的逻辑回归，导致总共 60 个模型，每个模型有 4 个参数。即使我们假设不同地区的行为有所不同，我们也没有利用它们之间的相似性。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="po pp l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">无泡 pymc 模型</p></figure><h2 id="fab7" class="nz ni iq bd nj oa ob dn nn oc od dp nr lo oe of nt ls og oh nv lw oi oj nx iw bi translated">3:贝叶斯分层逻辑回归</h2><blockquote class="ok ol om"><p id="0e8c" class="lf lg ng lh b li lj ka lk ll lm kd ln on lp lq lr oo lt lu lv op lx ly lz ma ij bi translated"><strong class="lh ja">贝叶斯分层建模</strong>是一种以多层编写的统计模型，使用贝叶斯方法估计后验分布的参数。子模型组合起来形成分层模型，贝叶斯定理用于将它们与观察到的数据相结合，并考虑所有存在的不确定性。⁴</p></blockquote><p id="ffc5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们假设当<em class="ng"> 𝛽 </em> s 对于每个区域是不同的，如在未轮询的情况下，现在系数都共享相似性。我们可以通过假设每个单独的系数来自共同的组分布来对此建模:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/28b2b45e465340020a5176b2d163a397.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/1*OOc4iDSu3S1mYqHF-vxJqQ.png"/></div></figure><p id="18c0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">随着</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/90e20a5ece3bcefc558afa1708acc032.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*YrCc0E4SjvV-LMtxfwtdrQ.png"/></div></figure><p id="13fd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然在分析上很难处理，但概率规划允许我们使用马尔可夫链蒙特卡罗(MCMC)从后验分布中采样来计算所有参数的后验。【pymc3 再次提供了一个非常直观的方法来建模我们的网络和计算后验概率！</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="po pp l"/></div></figure><p id="df02" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们还可以轻松计算贝叶斯网络的图形表示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pt"><img src="../Images/811eba0a58cc64a902951bc91dbd11f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UyrfBUFZx4GVOMa7dV2xkw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">逻辑回归的分层贝叶斯网络</p></figure><h1 id="5d43" class="nh ni iq bd nj nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">参数估计</h1><p id="cd63" class="pw-post-body-paragraph lf lg iq lh b li pd ka lk ll pe kd ln lo pf lq lr ls pg lu lv lw ph ly lz ma ij bi translated">首先，让我们试着了解一下我们的模型之间的差异。</p><p id="364d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">受<a class="ae le" href="https://docs.pymc.io/notebooks/GLM-hierarchical.html" rel="noopener ugc nofollow" target="_blank">https://docs.pymc.io/notebooks/GLM-hierarchical.html,</a>的启发，我们可以可视化不同回归中回归参数的演变。</p><div class="kp kq kr ks gt ab cb"><figure class="pu kt pv pw px py pz paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/da04c10c129612dfa0203b71b9f365b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1PyOHzCZuVoTjGLaX6-DvQ.png"/></div></figure><figure class="pu kt pv pw px py pz paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/e93522f5b4d2af61c146d5e68eae99a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*pwxEFRdvyMaY1QFL-OHcZQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk qa di qb qc translated">后验平均收缩率</p></figure></div><p id="160e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们显示了每个地区的非等级后验均值、等级后验均值和混合后验均值的系数。区一级可用的少量数据导致非 pool 后验分布很广，因此收缩效应非常重要，<strong class="lh ja">然而，分级模型中平均值之间的差异仍然很大，因为区与区之间的贝塔值在数量级上有所不同。</strong></p><h2 id="9457" class="nz ni iq bd nj oa ob dn nn oc od dp nr lo oe of nt ls og oh nv lw oi oj nx iw bi translated">模型误差</h2><p id="73a3" class="pw-post-body-paragraph lf lg iq lh b li pd ka lk ll pe kd ln lo pf lq lr ls pg lu lv lw ph ly lz ma ij bi translated">我们使用 ROC 曲线  <strong class="lh ja"> (AUC) </strong>下的<a class="ae le" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">面积作为我们比较模型的误差度量。它可以被看作是我们的模型对随机选择的正类的评分高于随机选择的负类的概率。我们对此特别感兴趣，因为它的优点是<strong class="lh ja">不需要设置阈值来分配标签。</strong></strong></a></p><p id="067d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了测量性能，我们考虑了两个测试集，一个是按地区分层的测试集，一个是非分层的测试集。非分层测试集的使用更能代表泛化误差与经验模型误差显著不同的情况，因为训练集和测试集之间的区域特征分布会显著不同！</p><p id="74cd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="ng">为了更好地代表真实性能，针对测试集采样的多个种子对测量值进行了平均。</em></p><h1 id="1ab4" class="nh ni iq bd nj nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">结果</h1><pre class="kp kq kr ks gt qd qe qf qg aw qh bi"><span id="5a94" class="nz ni iq qe b gy qi qj l qk ql">┌───────────────────┬────────┬──────────┬──────────────┐<br/>│                   │ POOLED │ UNPOOLED │ HIERARCHICAL │<br/>├───────────────────┼────────┼──────────┼──────────────┤<br/>│ Train             │  0.632 │    0.818 │        0.726 │<br/>│ Stratified Test   │  0.623 │    0.618 │        0.668 │<br/>│ Unstratified Test │  0.634 │    0.603 │        0.663 │<br/>└───────────────────┴────────┴──────────┴──────────────┘</span></pre><h2 id="891e" class="nz ni iq bd nj oa ob dn nn oc od dp nr lo oe of nt ls og oh nv lw oi oj nx iw bi translated">分层集合</h2><ul class=""><li id="d8c0" class="os ot iq lh b li pd ll pe lo qm ls qn lw qo ma ox oy oz pa bi translated">正如训练和测试 AUC 之间的巨大差距所表明的，未规划的模型严重过度拟合。</li><li id="adeb" class="os ot iq lh b li pi ll pj lo pk ls pl lw pm ma ox oy oz pa bi translated">另一方面，集合模型有很强的偏向性，并且明显对数据进行了欠拟合。</li><li id="0cc8" class="os ot iq lh b li pi ll pj lo pk ls pl lw pm ma ox oy oz pa bi translated">最后，通过利用参数地理相似性，我们的层次模型比其他模型表现得更好。</li></ul><p id="5439" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">收缩效应为我们提供了一个改进的统计能力，也可以被看作是一个聪明的调整方法。</p><h2 id="0f87" class="nz ni iq bd nj oa ob dn nn oc od dp nr lo oe of nt ls og oh nv lw oi oj nx iw bi translated">非分层的</h2><p id="6069" class="pw-post-body-paragraph lf lg iq lh b li pd ka lk ll pe kd ln lo pf lq lr ls pg lu lv lw ph ly lz ma ij bi translated">一些地区只有极少数的个体可以训练，因此，对于未规划的模型，这些地区的非分层测试误差变得更大。<strong class="lh ja">部分汇集考虑了参数</strong>之间的相似性，为低密度区提供来自其他区的信息，同时保持其特殊性。</p><p id="cf86" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，对于非分层测试集，模型之间的差异甚至更加显著，向我们展示了<strong class="lh ja">其泛化能力更大</strong>，因为性能几乎不受分层策略的影响。</p><p id="5552" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> = &gt; </strong>当我们有多组我们期望具有相似性的测量时，多级分层贝叶斯模型优于基本方法。</p><p id="8ebb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">将这种贝叶斯方法与其他经典的数据预处理方法(区域变量的不同编码)或算法(梯度推进、随机森林等)进行比较会很有趣。).</p><h1 id="9388" class="nh ni iq bd nj nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">结论</h1><ul class=""><li id="4a40" class="os ot iq lh b li pd ll pe lo qm ls qn lw qo ma ox oy oz pa bi translated">AutoML 还不能取代数据科学家，因为它不能从实际业务目标中区分经验误差度量，也不能正确建模协变量和目标之间的依赖关系。</li><li id="ec4a" class="os ot iq lh b li pi ll pj lo pk ls pl lw pm ma ox oy oz pa bi translated">专家们仍然需要理解数据并正确地模拟问题。为此，他们可以使用一系列数学和信息工具，包括<strong class="lh ja"> pymc3 库和贝叶斯分层模型</strong>，从而可以在分层结构数据的常见情况下轻松建模和计算分布</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="6b8a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[1]:维基百科贡献者。“自动化机器学习。”<em class="ng">维基百科，免费百科</em>。维基百科，免费的百科全书，2020 年 2 月 18 日。网络。2020 年 3 月 1 日。</p><p id="5fbf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2，3]:维基百科贡献者。(2020 年 2 月 22 日)。泛化错误。在<em class="ng">维基百科，免费百科</em>。2020 年 3 月 1 日 16:40 从<a class="ae le" href="https://en.wikipedia.org/w/index.php?title=Generalization_error&amp;oldid=942140633" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/w/index.php?检索 title = Generalization _ error&amp;oldid = 942140633</a></p><p id="1c39" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[4]:维基百科贡献者。“贝叶斯分层建模。”<em class="ng">维基百科，免费百科</em>。维基百科，免费百科，2019 年 12 月 12 日。网络。2020 年 3 月 1 日。</p><h1 id="6df8" class="nh ni iq bd nj nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">参考</h1><ul class=""><li id="f818" class="os ot iq lh b li pd ll pe lo qm ls qn lw qo ma ox oy oz pa bi translated"><em class="ng">因果关系:模型、推理和推论</em>，剑桥大学出版社(2000 年，2009 年第二版)。<a class="ae le" href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" rel="noopener ugc nofollow" target="_blank">国际标准书号</a>0–521–77362–8。</li><li id="80ad" class="os ot iq lh b li pi ll pj lo pk ls pl lw pm ma ox oy oz pa bi translated"><a class="ae le" href="https://docs.pymc.io/notebooks/GLM-hierarchical.html" rel="noopener ugc nofollow" target="_blank">https://docs.pymc.io/notebooks/GLM-hierarchical.html</a></li><li id="3321" class="os ot iq lh b li pi ll pj lo pk ls pl lw pm ma ox oy oz pa bi translated">n . m . huq 和 j . Cleland，1990 年。1989 年孟加拉国生育率调查(主要报告)。达卡:<em class="ng">国家人口研究和培训研究所</em></li><li id="f5ee" class="os ot iq lh b li pi ll pj lo pk ls pl lw pm ma ox oy oz pa bi translated">空间自相关:麻烦还是新范例？生态学。1993;74:1659–1673.</li></ul></div></div>    
</body>
</html>