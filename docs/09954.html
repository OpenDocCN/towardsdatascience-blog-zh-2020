<html>
<head>
<title>On-Policy v/s Off-Policy Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">政策上的 v/s 政策外的学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/on-policy-v-s-off-policy-learning-75089916bc2f?source=collection_archive---------6-----------------------#2020-07-14">https://towardsdatascience.com/on-policy-v-s-off-policy-learning-75089916bc2f?source=collection_archive---------6-----------------------#2020-07-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9ea7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解政策外学习的重要抽样</h2></div><p id="d7c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将试图理解策略上学习和策略外学习的区别，这对于刚接触强化学习的人来说可能会有点困惑。并将深入研究非政策学习的重要抽样概念。在进一步讨论之前，让我们先来看看两个术语。</p><ol class=""><li id="b27f" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">目标策略 pi(a|s):它是代理试图学习的策略，即代理正在学习该策略的值函数。</li><li id="4434" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">行为策略 b(a|s):它是代理用于动作选择的策略，即代理遵循该策略与环境交互。</li></ol><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/0bf842029241ccc706fa6b546216fafc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gRkH9tWDa4IQHg1VQXnECw.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">行为和目标政策示例，通过<a class="ae mf" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank">https://app.diagrams.net/</a>制作的图像</p></figure><h2 id="6384" class="mg mh iq bd mi mj mk dn ml mm mn dp mo ko mp mq mr ks ms mt mu kw mv mw mx my bi translated">政策学习:</h2><p id="4639" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">基于策略的学习算法是评估和改进用于选择动作的相同策略的算法。这意味着我们将尝试评估和改进代理已经用于操作选择的相同策略。简而言之，[目标策略==行为策略]。策略上算法的一些例子是策略迭代、值迭代、用于策略上的蒙特卡罗、Sarsa 等。</p><h2 id="a1a4" class="mg mh iq bd mi mj mk dn ml mm mn dp mo ko mp mq mr ks ms mt mu kw mv mw mx my bi translated">政策外学习:</h2><p id="3644" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">偏离策略学习算法评估和改进不同于用于动作选择的策略的策略。总之，【目标政策！=行为政策】。非策略学习算法的一些例子是 Q 学习、预期 sarsa(可以以两种方式起作用)等。</p><blockquote class="ne nf ng"><p id="5cd2" class="kf kg nh kh b ki kj jr kk kl km ju kn ni kp kq kr nj kt ku kv nk kx ky kz la ij bi translated">注意:行为策略必须覆盖目标策略，即 pi(a|s) &gt; 0，其中 b(a|s) &gt; 0。</p></blockquote><h2 id="19cc" class="mg mh iq bd mi mj mk dn ml mm mn dp mo ko mp mq mr ks ms mt mu kw mv mw mx my bi translated">为什么使用非策略？</h2><p id="ef68" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">非策略方法的一些好处如下:</p><ol class=""><li id="7941" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">持续探索:当代理正在学习其他策略时，它可以用于在学习最优策略的同时继续探索。而按策略学习次优策略。</li><li id="34cd" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">从演示中学习:代理可以从演示中学习。</li><li id="8cdf" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">并行学习:这加速了收敛，即学习可以很快。</li></ol></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="4da4" class="ns mh iq bd mi nt nu nv ml nw nx ny mo jw nz jx mr jz oa ka mu kc ob kd mx oc bi translated">对不符合策略的情况使用重要抽样</h1><p id="1446" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">到目前为止，我们知道了保单外和保单内的区别。所以出现的问题是，我们如何在遵循另一个政策的同时，在一个政策下得到国家价值的期望值。这就是重要采样派上用场的地方。让我们用蒙特卡洛更新规则来理解。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi od"><img src="../Images/fe90d01dc88ef706e92582f60e991c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*_5HQtVXqh-mAvGTAe4c1Yw.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图像通过强化学习:介绍<br/>理查德 s 萨顿和安德鲁 g 巴尔托</p></figure><p id="6f18" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如您所见，更新规则由来自一个州的所有抽样奖励的平均值组成。这些奖励是按照行为策略 b(a|s)抽样的，但是我们想要估计目标策略 pi(a|s)的值，并且需要从目标策略 pi(a|s)抽样的奖励。我们可以通过简单地将“ρ”乘以从行为政策中抽取的每个奖励来实现。<strong class="kh ir">‘ρ’的值等于目标策略 pi(a|s)下的轨迹概率除以行为策略 b(a|s)下的轨迹概率</strong>。轨迹的这些概率被定义为代理在状态 St '采取行动' At '并进入状态' St+1 '然后在时间 t 之前采取行动' At+1 '的概率。这个概率可以分成两部分，即在某个状态' St '采取行动' At '的概率和通过在状态' S '采取行动' At '而在某个状态' St+1 '结束的概率。简而言之，随机政策和随机环境。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/5ee0f7721712f63831af49ca223ae832.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*ZrjYf7POhCTFKgSKWMxOzA.png"/></div></figure><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi of"><img src="../Images/851dbe9811c5e0cd53454f2538c1b14c.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*AhJey8iW97Vn55XTC24Hjg.png"/></div></figure><h2 id="5fd1" class="mg mh iq bd mi mj mk dn ml mm mn dp mo ko mp mq mr ks ms mt mu kw mv mw mx my bi translated">重要抽样的推导:</h2><p id="31f7" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">假设从概率分布‘b’中抽取一个随机变量‘x ’,我们想估计‘x’相对于目标分布‘pi’的期望值。期望可以写成</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi og"><img src="../Images/32afdfd5def2d5cbe79c901b8e6f4ceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*N_1WNtJjzl654fI8uyU70Q.png"/></div></figure><p id="d152" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们可以通过‘b’乘以采样 x 的概率。通过将 b(x)移至 pi(x)以下，我们得到了重要的采样比‘ρ’。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/44f69dfee9756e86c29c1482b56fba6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*53-C8aDRZ9DEgUlKp3PXcw.png"/></div></figure><p id="7f50" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，如果我们把 xρ(x)看作一个新变量，那么我们可以把它写成 b 下的期望。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7b4b403ba4854441b91b2b018b5d0207.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/1*MDvicn2OHNA5Cn-DmirgKg.png"/></div></figure><p id="1618" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们可以将数据的期望值写成加权样本平均值，其中“ρ”用作权重。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/dc2d482fd76c88a42ae66909032ef6ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/format:webp/1*8tG0zrTYvzuh-KZgNJFV-w.png"/></div></figure><p id="dc42" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们可以从“b”中抽取“x ”,并使用上面的公式估计它在“pi”下的期望值。</p><p id="6368" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，本文到此结束。谢谢你的阅读，希望你喜欢并且能够理解我想要解释的东西。希望你阅读我即将发表的文章。哈里奥姆…🙏</p><h1 id="c924" class="ns mh iq bd mi nt ok nv ml nw ol ny mo jw om jx mr jz on ka mu kc oo kd mx oc bi translated">参考资料:</h1><div class="op oq gp gr or os"><a href="https://www.coursera.org/specializations/reinforcement-learning" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd ir gy z fp ox fr fs oy fu fw ip bi translated">强化学习</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">由阿尔伯塔大学提供。强化学习专业化包括 4 门课程探索权力…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">www.coursera.org</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg lz os"/></div></div></a></div><div class="op oq gp gr or os"><a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd ir gy z fp ox fr fs oy fu fw ip bi translated">强化学习，第二版</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">显着扩大和更新的广泛使用的文本强化学习的新版本，最…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">mitpress.mit.edu</p></div></div><div class="pb l"><div class="ph l pd pe pf pb pg lz os"/></div></div></a></div></div></div>    
</body>
</html>