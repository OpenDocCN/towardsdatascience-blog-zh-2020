<html>
<head>
<title>LASSO Regression Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">套索回归教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lasso-regression-tutorial-fd68de0aa2a2?source=collection_archive---------16-----------------------#2020-01-08">https://towardsdatascience.com/lasso-regression-tutorial-fd68de0aa2a2?source=collection_archive---------16-----------------------#2020-01-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7410" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 LASSO 回归的偏差-方差评估-游轮数据集</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6a8b3c3ca5a1bd17b2efe7af5e276132.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1X_5VpDYfMx_-K9_0h_QhQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Benjamin O. Tayo 拍摄的照片</p></figure><p id="0c9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> L </span> ASSO 回归是正则化回归的一个例子。正则化是一种通过添加额外信息来解决过度拟合问题的方法，从而缩小模型的参数值以导致对复杂性的惩罚。正则化线性回归的 3 种最流行的方法是所谓的岭回归、最小绝对收缩和选择算子(LASSO)和弹性网格法。</p><p id="b85c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我将侧重于套索，但脊和弹性网的扩展是直截了当的。</p><p id="f1c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们想要在具有 n 个观测值和 m 个特征的数据集上构建一个正则化回归模型。</p><p id="6582" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LASSO 回归是一种 L1 惩罚模型，我们只需将权重的<a class="ae me" href="http://mathworld.wolfram.com/L1-Norm.html" rel="noopener ugc nofollow" target="_blank"> L1 范数<strong class="lb iu"> </strong> </a>添加到最小二乘成本函数中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/1a4af39a62f066136cc869a105fcd392.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*SoeElvBiiddQlk8kDCyc9A.png"/></div></figure><p id="50cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/097a72c63ce7d3b4eee3c8c62c7e8ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*8vO5jrsyip3p_o8mabYg5A.png"/></div></figure><p id="8800" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过增加超参数α的值，我们增加了正则化强度并缩小了模型的权重。请注意，我们没有正则化截距项 w0。还要注意，alpha = 0 对应于标准回归分析。</p><p id="0981" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据正则化强度，某些权重可以变为零，这使得套索方法成为一种非常强大的降维技术。</p><h1 id="fcf8" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">LASSO 伪代码</h1><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="bdbc" class="ne mi it na b gy nf ng l nh ni">1) For given alpha, simply minimize the cost function to find the  weights or model parameters w.</span><span id="dfd9" class="ne mi it na b gy nj ng l nh ni">2) Then compute the norm of w (excluding w0) using the equation below:</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/0bd8447517d29eeb5ff1ef3957d27784.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*aZJKX8LL1V0qH3iEPzJMJw.png"/></div></figure><h1 id="e948" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">案例研究:使用游轮数据集预测船员人数</h1><p id="6273" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">我们将使用游轮数据集<a class="ae me" href="https://github.com/bot13956/ML_Model_for_Predicting_Ships_Crew_Size" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">cruise _ ship _ info . CSV</strong></a><strong class="lb iu">来说明套索技术。</strong></p><h2 id="947c" class="ne mi it bd mj nq nr dn mn ns nt dp mr li nu nv mt lm nw nx mv lq ny nz mx oa bi translated">1.导入必要的库</h2><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="3671" class="ne mi it na b gy nf ng l nh ni">import numpy as np</span><span id="0105" class="ne mi it na b gy nj ng l nh ni">import pandas as pd</span><span id="d5ae" class="ne mi it na b gy nj ng l nh ni">import matplotlib.pyplot as plt</span></pre><h2 id="8258" class="ne mi it bd mj nq nr dn mn ns nt dp mr li nu nv mt lm nw nx mv lq ny nz mx oa bi translated">2.读取数据集并显示列</h2><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="cb5b" class="ne mi it na b gy nf ng l nh ni">df = pd.read_csv("cruise_ship_info.csv")</span><span id="19e8" class="ne mi it na b gy nj ng l nh ni">df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/80caa4dfc70023bcf79d5ce8b2c4fc07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WPQvVTcypPiZKW3BJpjJmw.png"/></div></div></figure><h2 id="3049" class="ne mi it bd mj nq nr dn mn ns nt dp mr li nu nv mt lm nw nx mv lq ny nz mx oa bi translated">3.选择重要变量</h2><p id="4037" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">在另一篇文章(<a class="ae me" href="https://medium.com/towards-artificial-intelligence/feature-selection-and-dimensionality-reduction-using-covariance-matrix-plot-b4c7498abd07" rel="noopener"> <strong class="lb iu">利用协方差矩阵图进行特征选择和降维</strong> </a>)中，我们看到了协方差矩阵图可以用于特征选择和降维。使用游轮数据集<a class="ae me" href="https://github.com/bot13956/ML_Model_for_Predicting_Ships_Crew_Size" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">cruise _ ship _ info . CSV</strong></a><strong class="lb iu">，</strong>我们发现，在 6 个预测特征[' <strong class="lb iu">年龄</strong>'、<strong class="lb iu">吨位</strong>'、<strong class="lb iu">乘客</strong>'、<strong class="lb iu">长度</strong>'、<strong class="lb iu">舱室</strong>、<strong class="lb iu">乘客密度</strong> ]中，如果我们假设重要特征的相关系数为 0 那么目标变量“<strong class="lb iu">乘员</strong>”与 4 个预测变量:“<strong class="lb iu">吨位</strong>”、“<strong class="lb iu">乘客</strong>”、“<strong class="lb iu">长度</strong>、“<strong class="lb iu">车厢</strong>”强相关。 因此，我们能够将特征空间的维数从 6 降低到 4。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="b3bb" class="ne mi it na b gy nf ng l nh ni">cols_selected = ['Tonnage', 'passengers', 'length', 'cabins','crew']</span><span id="ae86" class="ne mi it na b gy nj ng l nh ni">df[cols_selected].head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/9b87f6df0f8ea8c8cc2024a8ca48256f.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*vkKMFeoaWmvJygc6qLYKzg.png"/></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="8970" class="ne mi it na b gy nf ng l nh ni">X = df[cols_selected].iloc[:,0:4].values    # features matrix </span><span id="7ff8" class="ne mi it na b gy nj ng l nh ni">y = df[cols_selected]['crew'].values        # target variable</span></pre><h2 id="f52c" class="ne mi it bd mj nq nr dn mn ns nt dp mr li nu nv mt lm nw nx mv lq ny nz mx oa bi translated">4.LASSO 回归实现</h2><p id="0bd0" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated"><strong class="lb iu"> a .将数据集分成训练集和测试集</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="9e98" class="ne mi it na b gy nf ng l nh ni">from sklearn.model_selection import train_test_split</span><span id="4e23" class="ne mi it na b gy nj ng l nh ni">X_train, X_test, y_train, y_test = train_test_split( X, y, <br/>                                      test_size=0.4, random_state=0)</span></pre><p id="db49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> b .标准化特征</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="f6cc" class="ne mi it na b gy nf ng l nh ni">from sklearn.preprocessing import StandardScaler</span><span id="dd38" class="ne mi it na b gy nj ng l nh ni">sc_y = StandardScaler()</span><span id="27b0" class="ne mi it na b gy nj ng l nh ni">sc_x = StandardScaler()</span><span id="9878" class="ne mi it na b gy nj ng l nh ni">y_std = sc_y.fit_transform(y_train[:, np.newaxis]).flatten()</span><span id="2cca" class="ne mi it na b gy nj ng l nh ni">X_train_std = sc_x.fit_transform(X_train)</span><span id="313d" class="ne mi it na b gy nj ng l nh ni">X_test_std = sc_x.transform(X_test)</span><span id="8d9d" class="ne mi it na b gy nj ng l nh ni">y_train_std = sc_y.fit_transform(y_train[:, np.newaxis]).flatten()</span></pre><p id="cc1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> c .实施套索回归</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="25c9" class="ne mi it na b gy nf ng l nh ni">from sklearn.linear_model import Lasso</span><span id="05e4" class="ne mi it na b gy nj ng l nh ni">from sklearn.metrics import r2_score</span><span id="6591" class="ne mi it na b gy nj ng l nh ni">alpha = np.linspace(0.01,0.4,10)</span><span id="ab05" class="ne mi it na b gy nj ng l nh ni">r2_train =[]</span><span id="0c75" class="ne mi it na b gy nj ng l nh ni">r2_test =[]</span><span id="4d1b" class="ne mi it na b gy nj ng l nh ni">norm = []</span><span id="03cc" class="ne mi it na b gy nj ng l nh ni">alpha = np.linspace(0.01,0.4,10)</span><span id="4fc5" class="ne mi it na b gy nj ng l nh ni">for i in range(10):</span><span id="9944" class="ne mi it na b gy nj ng l nh ni">    lasso = Lasso(alpha = alpha[i])</span><span id="066f" class="ne mi it na b gy nj ng l nh ni">    lasso.fit(X_train_std,y_train_std)</span><span id="3db4" class="ne mi it na b gy nj ng l nh ni">    y_train_std = lasso.predict(X_train_std)</span><span id="21e9" class="ne mi it na b gy nj ng l nh ni">    y_test_std = lasso.predict(X_test_std)</span><span id="34b2" class="ne mi it na b gy nj ng l nh ni">    r2_train = np.append(r2_train,<br/>              r2_score(y_train,sc_y.inverse_transform(y_train_std)))</span><span id="db1a" class="ne mi it na b gy nj ng l nh ni">    r2_test = np.append(r2_test,<br/>              r2_score(y_test,sc_y.inverse_transform(y_test_std)))</span><span id="03cd" class="ne mi it na b gy nj ng l nh ni">    norm = np.append(norm,np.linalg.norm(lasso.coef_))</span></pre><p id="e156" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> d .输出的可视化</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="20d0" class="ne mi it na b gy nf ng l nh ni">plt.figure(figsize=(8,6))<br/>plt.scatter(alpha,r2_train,label='r2_train')<br/>plt.plot(alpha,r2_train)<br/>plt.scatter(alpha,r2_test,label='r2_test')<br/>plt.plot(alpha,r2_test)<br/>plt.scatter(alpha,norm,label = 'norm')<br/>plt.plot(alpha,norm)<br/>plt.ylim(-0.1,1)<br/>plt.xlim(0,.43)<br/>plt.xlabel('alpha', size = 14)<br/>plt.ylabel('R2_score',size = 14)<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c413fc9efccee590b847f771d459406b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*cUcpf_KZJdOd1x47sDrIaA.png"/></div></figure><p id="0c2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们观察到，随着正则化参数α的增加，回归系数的范数变得越来越小。这意味着更多的回归系数被迫为零，这往往会增加偏差误差(过度简化)。平衡偏差-方差权衡的最佳值是当 alpha 保持较低时，比如 alpha = 0.1 或更低。在决定使用哪种方法进行降维之前，应该将这种方法与主成分分析(PCA)进行比较。</p><p id="c46c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">岭回归和弹性网回归可以用同样的方式实现。更多信息，请参见以下书籍:拉什卡、塞巴斯蒂安和瓦希德·米尔贾利利<strong class="lb iu">。</strong> <em class="oe"> Python 机器学习，第二版</em>。Packt 出版公司，2017 年。</p><h1 id="34d0" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">参考</h1><ol class=""><li id="3d9d" class="of og it lb b lc nl lf nm li oh lm oi lq oj lu ok ol om on bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/training-a-machine-learning-model-on-a-dataset-with-highly-correlated-features-debddf5b2e34" rel="noopener">在具有高度相关特征的数据集上训练机器学习模型</a>。</li><li id="fb9c" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/feature-selection-and-dimensionality-reduction-using-covariance-matrix-plot-b4c7498abd07" rel="noopener">使用协方差矩阵图进行特征选择和降维</a>。</li><li id="9791" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">拉什卡、塞巴斯蒂安和瓦希德·米尔贾利利<strong class="lb iu">。</strong> <em class="oe"> Python 机器学习，第二版</em>。Packt 出版公司，2017 年。</li><li id="4d9d" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">Benjamin O. Tayo，<em class="oe">预测船只船员规模的机器学习模型</em>，<a class="ae me" href="https://github.com/bot13956/ML_Model_for_Predicting_Ships_Crew_Size" rel="noopener ugc nofollow" target="_blank">https://github . com/bot 13956/ML _ Model _ for _ Predicting _ Ships _ Crew _ Size</a>。</li></ol></div></div>    
</body>
</html>