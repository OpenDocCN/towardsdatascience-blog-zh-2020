<html>
<head>
<title>Neural Machine Translation (NMT) with Attention Mechanism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有注意机制的神经机器翻译(NMT)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-machine-translation-nmt-with-attention-mechanism-5e59b57bd2ac?source=collection_archive---------14-----------------------#2020-06-05">https://towardsdatascience.com/neural-machine-translation-nmt-with-attention-mechanism-5e59b57bd2ac?source=collection_archive---------14-----------------------#2020-06-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5bb8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深度学习语言翻译指南！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ce34d7af99c9db446b8ff3d82be5c2b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lBY0wuKO6FHnejWCkFCCpg.jpeg"/></div></div></figure><h1 id="a7c5" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">概观</h1><p id="d31f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">不可否认的事实是，在这个全球化的时代，语言翻译在不同国家的人们之间的交流中起着至关重要的作用。此外，在一个像印度这样的国家——一个使用多种语言的国家，语言差异在它自己的各邦都可以观察到！因此，考虑到语言翻译的重要性，有必要开发一个能够将未知语言翻译成已知语言的系统。</p><p id="859b" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">据此，在这个故事中，我们将制作一个深度学习模型，将英语句子翻译成马拉地语句子。我选择了马拉地语，因为它容易辨认。您可以使用任何其他您觉得舒服的语言，因为模型几乎保持不变。而且，我在这里会试着简单解释一下语言处理中的一个主要概念叫做注意机制！</p><h1 id="25c5" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">先决条件</h1><ol class=""><li id="d584" class="mk ml iq ll b lm ln lp lq ls mm lw mn ma mo me mp mq mr ms bi translated"><a class="ae mt" rel="noopener" target="_blank" href="/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">长短时记忆(LSTM) </a>细胞的工作</li><li id="728a" class="mk ml iq ll b lm mu lp mv ls mw lw mx ma my me mp mq mr ms bi translated">TensorFlow、Keras 和其他一些强制性 python 库的工作。</li></ol><h1 id="9609" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">什么是注意力机制？</h1><p id="c025" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">序列递归神经网络的主要缺点是只能处理短序列。<strong class="ll ir">编码器模型很难记忆长序列并将其转换成固定长度的向量。</strong>此外，解码器仅接收一个信息，即最后一个编码器隐藏状态。因此，解码器很难一次汇总大量的输入序列。那么，我们如何克服这个问题呢？</p><blockquote class="mz na nb"><p id="9edf" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">如果我们给解码器模型一个从每一个编码器步骤的向量表示怎么样！</p></blockquote><p id="ab2b" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">现在，这就是<strong class="ll ir">‘注意力机制</strong>’概念的来源。关于这一点的主要直觉是，它通过关注序列的几个相关部分而不是查看整个序列来预测下一个单词。</p><blockquote class="mz na nb"><p id="36a8" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">通俗地说，它可以被描述为编码器和解码器之间的干扰，它从编码器中提取有用的信息，并将其传输回解码器。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/fd9989b7d31a1b00d3f388bf58ca83ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EJ406jTayuLZKlLa.gif"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">注意力层动画</p></figure><p id="d110" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><em class="nc">此处</em>  <em class="nc">参考</em> <a class="ae mt" rel="noopener" target="_blank" href="/attn-illustrated-attention-5ec4ad276ee3"> <em class="nc">详细了解注意机制。</em></a></p><p id="0e9a" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">主要有两种类型的注意机制:</p><ul class=""><li id="5782" class="mk ml iq ll b lm mf lp mg ls nl lw nm ma nn me no mq mr ms bi translated">全球关注</li><li id="f477" class="mk ml iq ll b lm mu lp mv ls mw lw mx ma my me no mq mr ms bi translated">当地的关注</li></ul><h2 id="c603" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">全球关注</h2><p id="510d" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">全局注意力是那些通过编码器的所有隐藏状态向量来获得上下文向量的注意力。</p><h2 id="5ea3" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">当地的关注</h2><p id="896c" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">局部注意力是那些仅考虑编码器的少数隐藏状态向量来生成上下文向量的注意力。</p><p id="2c3d" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">在这个故事中，我们将利用全球的注意力。现在让我们利用注意机制，开发一个语言翻译器，将英语句子转换成马拉地语句子。</p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h1 id="67fc" class="kr ks iq bd kt ku oi kw kx ky oj la lb jw ok jx ld jz ol ka lf kc om kd lh li bi translated">履行</h1><h2 id="afa8" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">库导入</h2><p id="03ea" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">打开 Jupyter 笔记本，导入一些需要的库:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="d371" class="np ks iq oo b gy os ot l ou ov">import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>import string<br/>from string import digits<br/>import re<br/>from sklearn.utils import shuffle<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences<br/>from tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed<br/>from tensorflow.keras.models import Model,load_model, model_from_json<br/>from tensorflow.keras.utils import plot_model<br/>from tensorflow.keras.preprocessing.text import one_hot, Tokenizer<br/>from tensorflow.keras.callbacks import EarlyStopping<br/>import pickle as pkl<br/>import numpy as np</span></pre></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="df94" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">下载数据集</h2><blockquote class="mz na nb"><p id="2904" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">我们将在这里  <em class="iq">使用一个可用的语言数据集<a class="ae mt" href="http://www.manythings.org/anki/" rel="noopener ugc nofollow" target="_blank"> <em class="iq">。</em></a></em></p></blockquote><p id="b069" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">该网站包含许多语言的数据集及其英语翻译。你可以根据自己的喜好和舒适度下载任何语言数据集。但是，记住要选择一个相当庞大的数据集，这样我们在训练模型后才能得到更好的结果。在这里，我将下载由 38696 个句子组成的马拉地语-英语数据集。</p><p id="0e84" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">下载加载数据集后，按如下所述导入数据:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="5911" class="np ks iq oo b gy os ot l ou ov">with open('mar.txt','r') as f:<br/>  data = f.read()</span></pre></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="032c" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">预处理数据集</h2><p id="5c0f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated"><strong class="ll ir">数据转换</strong></p><p id="cbf6" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">正如你所看到的，这是一个原始的文本文件，因此有必要根据我们的喜好对其进行清理和转换。我们将分离马拉地语和英语句子，并形成一个列表，通过将它存储到一个数据帧中来延续它，这样我们就可以很容易地再次使用它。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="9c69" class="np ks iq oo b gy os ot l ou ov">uncleaned_data_list = data.split('\n')<br/>len(uncleaned_data_list)<br/>uncleaned_data_list = uncleaned_data_list[:38695]<br/>len(uncleaned_data_list)</span><span id="6a32" class="np ks iq oo b gy ow ot l ou ov">english_word = []<br/>marathi_word = []<br/>cleaned_data_list = []<br/>for word in uncleaned_data_list:<br/>  english_word.append(word.split('\t')[:-1][0])<br/>  marathi_word.append(word.split('\t')[:-1][1])</span><span id="e7bc" class="np ks iq oo b gy ow ot l ou ov">language_data = pd.DataFrame(columns=['English','Marathi'])<br/>language_data['English'] = english_word<br/>language_data['Marathi'] = marathi_word</span><span id="c381" class="np ks iq oo b gy ow ot l ou ov">language_data.to_csv('language_data.csv', index=False)</span></pre><blockquote class="mz na nb"><p id="345d" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">language_data.head()</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/4e2b0859e28b3c9ccf0efa3256ff1425.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*Nl0lTfBoXvutufRXPce3-g.jpeg"/></div></div></figure><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="86a5" class="np ks iq oo b gy os ot l ou ov">english_text = language_data['English'].values<br/>marathi_text = language_data['Marathi'].values<br/>len(english_text), len(marathi_text)</span></pre><p id="28e1" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir">数据清理</strong></p><p id="d25b" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">现在让我们清理数据，使其适合我们的模型。在清理过程中，我们将转换成小写，删除所有标点符号和其他不必要的字母和数字。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="9981" class="np ks iq oo b gy os ot l ou ov"><strong class="oo ir">#to lower case</strong><br/>english_text_ = [x.lower() for x in english_text]<br/>marathi_text_ = [x.lower() for x in marathi_text]</span><span id="9ad7" class="np ks iq oo b gy ow ot l ou ov"><strong class="oo ir">#removing inverted commas</strong><br/>english_text_ = [re.sub("'",'',x) for x in english_text_]<br/>marathi_text_ = [re.sub("'",'',x) for x in marathi_text_]</span><span id="84c0" class="np ks iq oo b gy ow ot l ou ov">def remove_punc(text_list):<br/>  table = str.maketrans('', '', string.punctuation)<br/>  removed_punc_text = []<br/>  for sent in text_list:<br/>    sentance = [w.translate(table) for w in sent.split(' ')]<br/>    removed_punc_text.append(' '.join(sentance))<br/>  return removed_punc_text<br/>english_text_ = remove_punc(english_text_)<br/>marathi_text_ = remove_punc(marathi_text_)</span><span id="f09c" class="np ks iq oo b gy ow ot l ou ov">remove_digits = str.maketrans('', '', digits)<br/>removed_digits_text = []<br/>for sent in english_text_:<br/>  sentance = [w.translate(remove_digits) for w in sent.split(' ')]<br/>  removed_digits_text.append(' '.join(sentance))<br/>english_text_ = removed_digits_text</span><span id="d9e8" class="np ks iq oo b gy ow ot l ou ov"><strong class="oo ir"># removing the digits from the marathi sentances</strong><br/>marathi_text_ = [re.sub("[२३०८१५७९४६]","",x) for x in marathi_text_]<br/>marathi_text_ = [re.sub("[\u200d]","",x) for x in marathi_text_]</span><span id="fc31" class="np ks iq oo b gy ow ot l ou ov"><strong class="oo ir"># removing the stating and ending whitespaces</strong><br/>english_text_ = [x.strip() for x in english_text_]<br/>marathi_text_ = [x.strip() for x in marathi_text_]</span></pre><p id="0481" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">在马拉地语句子中添加“开始”和“结束”标签。这将帮助解码器知道从哪里开始解码以及何时结束解码。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="d025" class="np ks iq oo b gy os ot l ou ov"># Putting the start and end words in the marathi sentances<br/>marathi_text_ = ["start " + x + " end" for x in marathi_text_]</span><span id="93fd" class="np ks iq oo b gy ow ot l ou ov"># manipulated_marathi_text_<br/>marathi_text_[0], english_text_[0]</span></pre><blockquote class="mz na nb"><p id="03f6" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">(“开始जा结束”，“开始”)</p></blockquote></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="4cd3" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">建模的数据准备</h2><p id="ce98" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我们将以 0.1 的比率分割我们的数据集，以便我们训练的模型可以给出精确的结果。X_train 和 y_train 将是我们的训练集，而 X_test 和 y_test 将是我们的测试/验证集。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="f34f" class="np ks iq oo b gy os ot l ou ov">X = english_text_<br/>Y = marathi_text_</span><span id="08eb" class="np ks iq oo b gy ow ot l ou ov">X_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.1)</span></pre><p id="d569" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">让我们确定英语和马拉地语句子的最大长度:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="ffee" class="np ks iq oo b gy os ot l ou ov">def Max_length(data):<br/>  max_length_ = max([len(x.split(' ')) for x in data])<br/>  return max_length_</span><span id="064e" class="np ks iq oo b gy ow ot l ou ov">#Training data<br/>max_length_english = Max_length(X_train)<br/>max_length_marathi = Max_length(y_train)</span><span id="d33e" class="np ks iq oo b gy ow ot l ou ov">#Test data<br/>max_length_english_test = Max_length(X_test)<br/>max_length_marathi_test = Max_length(y_test)</span><span id="91c0" class="np ks iq oo b gy ow ot l ou ov">max_length_marathi, max_length_english</span></pre><blockquote class="mz na nb"><p id="242f" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi">(26, 32)</p></blockquote><p id="1359" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir">标记化:</strong></p><p id="c5ed" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">由于神经网络需要处理数字数据，因此有必要将我们的字符串输入转换为数字列表。一种方法是使用 keras 预处理库提供的<strong class="ll ir">标记器</strong>。</p><p id="2e00" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">此外，请记住，在序列到序列模型中，所有输入序列的长度必须相等。因此，我们将填充额外的“0”以使序列具有相同的长度。这将由<strong class="ll ir"> pad_sequence 完成。</strong></p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="68a8" class="np ks iq oo b gy os ot l ou ov">englishTokenizer = Tokenizer()<br/>englishTokenizer.fit_on_texts(X_train)<br/>Eword2index = englishTokenizer.word_index<br/>vocab_size_source = len(Eword2index) + 1</span><span id="59ac" class="np ks iq oo b gy ow ot l ou ov">X_train = englishTokenizer.texts_to_sequences(X_train)<br/>X_train = pad_sequences(X_train, maxlen=max_length_english, padding='post')</span><span id="f6c5" class="np ks iq oo b gy ow ot l ou ov">X_test = englishTokenizer.texts_to_sequences(X_test)<br/>X_test = pad_sequences(X_test, maxlen = max_length_english, padding='post')</span><span id="3fe0" class="np ks iq oo b gy ow ot l ou ov">marathiTokenizer = Tokenizer()<br/>marathiTokenizer.fit_on_texts(y_train)<br/>Mword2index = marathiTokenizer.word_index<br/>vocab_size_target = len(Mword2index) + 1</span><span id="9573" class="np ks iq oo b gy ow ot l ou ov">y_train = marathiTokenizer.texts_to_sequences(y_train)<br/>y_train = pad_sequences(y_train, maxlen=max_length_marathi, padding='post')</span><span id="3240" class="np ks iq oo b gy ow ot l ou ov">y_test = marathiTokenizer.texts_to_sequences(y_test)<br/>y_test = pad_sequences(y_test, maxlen = max_length_marathi, padding='post')</span><span id="f92d" class="np ks iq oo b gy ow ot l ou ov">vocab_size_source, vocab_size_target</span></pre><blockquote class="mz na nb"><p id="1ba7" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi">(5413, 12789)</p></blockquote><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="bc37" class="np ks iq oo b gy os ot l ou ov">X_train[0], y_train[0]</span></pre><blockquote class="mz na nb"><p id="3490" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">(array([ 1，157，5，134，4，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，<br/> 0，0，0，0，0，0，0，0，0，dtype=int32)，<br/> array([ 1，6，22，61，253，29，2，0，0，0，0，0，0，0，0，0</p></blockquote><p id="eac1" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">为了节省我们的预处理时间，我们将保存我们的重要属性。所以，我们先借助泡菜库来做。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="ea48" class="np ks iq oo b gy os ot l ou ov">with open('NMT_data.pkl','wb') as f:<br/>  pkl.dump([X_train, y_train, X_test, y_test],f)</span><span id="fcd0" class="np ks iq oo b gy ow ot l ou ov">with open('NMT_Etokenizer.pkl','wb') as f:<br/>  pkl.dump([vocab_size_source, Eword2index, englishTokenizer], f)</span><span id="062c" class="np ks iq oo b gy ow ot l ou ov">with open('NMT_Mtokenizer.pkl', 'wb') as f:<br/>  pkl.dump([vocab_size_target, Mword2index, marathiTokenizer], f)</span><span id="9937" class="np ks iq oo b gy ow ot l ou ov">X_train = np.array(X_train)<br/>y_train = np.array(y_train)<br/>X_test = np.array(X_test)<br/>y_test = np.array(y_test)</span></pre></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="ff57" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">模型结构</h2><p id="3ed7" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">取代简单的编码器-解码器架构，我们将使用本博客前面讨论过的注意机制。</p><p id="66cf" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">Keras 不正式支持关注层。因此，我们可以实现自己的关注层，也可以使用第三方实现。目前，我们将使用第三方关注机制。你可以从<a class="ae mt" href="https://github.com/thushv89/attention_keras/blob/master/layers/attention.py" rel="noopener ugc nofollow" target="_blank"> <strong class="ll ir">这里</strong> </a> <strong class="ll ir"> </strong>下载关注层，复制到另一个名为<strong class="ll ir"> attention.py </strong>的文件中。这个注意力是<a class="ae mt" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nc">【巴赫达瑙注意力】</em> </a> <em class="nc">的一个实现。</em></p><p id="5a6a" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">让我们定义模型的结构:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="b60f" class="np ks iq oo b gy os ot l ou ov">from attention import AttentionLayer<br/>from keras import backend as K <br/>K.clear_session() <br/>latent_dim = 500</span><span id="0caa" class="np ks iq oo b gy ow ot l ou ov"># Encoder <br/>encoder_inputs = Input(shape=(max_length_english,)) <br/>enc_emb = Embedding(vocab_size_source, latent_dim,trainable=True)(encoder_inputs)</span><span id="fc54" class="np ks iq oo b gy ow ot l ou ov">#LSTM 1 <br/>encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) <br/>encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)</span><span id="da16" class="np ks iq oo b gy ow ot l ou ov">#LSTM 2 <br/>encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) <br/>encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)</span><span id="6ea4" class="np ks iq oo b gy ow ot l ou ov">#LSTM 3 <br/>encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) <br/>encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)</span><span id="e6cb" class="np ks iq oo b gy ow ot l ou ov"># Set up the decoder. <br/>decoder_inputs = Input(shape=(None,)) <br/>dec_emb_layer = Embedding(vocab_size_target, latent_dim,trainable=True) <br/>dec_emb = dec_emb_layer(decoder_inputs)</span><span id="5f97" class="np ks iq oo b gy ow ot l ou ov">#LSTM using encoder_states as initial state<br/>decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) <br/>decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])</span><span id="4a39" class="np ks iq oo b gy ow ot l ou ov">#Attention Layer<br/>attn_layer = AttentionLayer(name='attention_layer') <br/>attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])</span><span id="7251" class="np ks iq oo b gy ow ot l ou ov"># Concat attention output and decoder LSTM output <br/>decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])</span><span id="e10a" class="np ks iq oo b gy ow ot l ou ov">#Dense layer<br/>decoder_dense = TimeDistributed(Dense(vocab_size_target, activation='softmax')) <br/>decoder_outputs = decoder_dense(decoder_concat_input)</span><span id="7413" class="np ks iq oo b gy ow ot l ou ov"># Define the model<br/>model = Model([encoder_inputs, decoder_inputs], decoder_outputs) </span><span id="287c" class="np ks iq oo b gy ow ot l ou ov">plot_model(model, to_file='train_model.png', show_shapes=True)</span></pre><p id="860e" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">您可以根据自己的选择和要求修改该模型，以获得更好的结果。你也可以改变层数、单位数或一些正则化技术。暂时先往前走，看看我们的模型是什么样子的！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/cae5cefe929b80477931e81aa91c4370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M6UKNwEUmdLdWnNHnva4BA.png"/></div></div></figure><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="06b5" class="np ks iq oo b gy os ot l ou ov">model.compile(optimizer='rmsprop',<br/>              loss='sparse_categorical_crossentropy', <br/>              metrics=['accuracy'])</span></pre></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="f05b" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">模特培训</h2><p id="e41f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我们将首先定义一些回调，以便将来模型可视化和评估变得容易。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="cf83" class="np ks iq oo b gy os ot l ou ov">es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)</span></pre><p id="9cc7" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">我们正在使用“<strong class="ll ir">教师强制</strong>”技术来更快地训练我们的模型。在教师强制方法中，我们还将目标数据作为输入传递给解码器。例如，如果我们要预测“hello ”,那么我们将把“hello”本身作为输入传递给解码器。因此，这使得学习过程更快。</p><p id="ff55" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">让我们训练我们的模型:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="c8d2" class="np ks iq oo b gy os ot l ou ov">history = model.fit([X_train, y_train[:,:-1]], y_train.reshape(y_train.shape[0], y_train.shape[1],1)[:,1:], <br/>                    epochs=50, <br/>                    callbacks=[es],<br/>                    batch_size=512,<br/>                    validation_data = ([X_test, y_test[:,:-1]],           y_test.reshape(y_test.shape[0], y_test.shape[1], 1)[:,1:]))</span></pre><blockquote class="mz na nb"><p id="22d6" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">在 12GB<strong class="ll ir">NVIDIA</strong>Tesla K80<strong class="ll ir">GPU</strong>上，执行时间约为每个纪元 39 秒。18 世纪实现了早期停止。</p></blockquote><p id="4ae3" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">我们可以将训练和验证阶段的损失差异形象化为:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="cefb" class="np ks iq oo b gy os ot l ou ov">from matplotlib import pyplot <br/>pyplot.plot(history.history['loss'], label='train') <br/>pyplot.plot(history.history['val_loss'], label='test') <br/>pyplot.legend() <br/>pyplot.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/3ed4def253906cf7d986ae266e86bce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*auQMVqklYia6LxLxcdJAog.png"/></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">损失比较</p></figure><blockquote class="mz na nb"><p id="ea7f" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">我们从我们的模型中得到了一些非常好的结果，验证准确率约为 90%，验证损失为 0.5303。</p></blockquote></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="a95d" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">模型保存和加载</h2><p id="ec4f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">让我们用适当的权重来保存我们训练好的模型。请记住像我一样保存模型，因为我们还必须为推理模型加载权重。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="cb02" class="np ks iq oo b gy os ot l ou ov">model_json = model.to_json()<br/>with open("NMT_model.json", "w") as json_file:<br/>    json_file.write(model_json)<br/># serialize weights to HDF5<br/>model.save_weights("NMT_model_weight.h5")<br/>print("Saved model to disk")</span></pre><p id="3154" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">负载模型:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="9a86" class="np ks iq oo b gy os ot l ou ov"># loading the model architecture and asigning the weights<br/>json_file = open('NMT_model.json', 'r')<br/>loaded_model_json = json_file.read()<br/>json_file.close()<br/>model_loaded = model_from_json(loaded_model_json, custom_objects={'AttentionLayer': AttentionLayer})<br/># load weights into new model<br/>model_loaded.load_weights("NMT_model_weight.h5")</span></pre></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="8bbd" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">推理模型</h2><blockquote class="mz na nb"><p id="f06a" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">在机器学习中，我们通过考虑来自预训练模型的权重，使用推理模型来预测我们的输出序列。换句话说，可以说它是一个模型，该模型推导出在训练阶段学习到的并且现在用于预测新序列的属性。</p></blockquote><p id="3467" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">让我们编写推理模型的代码:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="5e62" class="np ks iq oo b gy os ot l ou ov">latent_dim=500<br/># encoder inference<br/>encoder_inputs = model_loaded.input[0]  #loading encoder_inputs<br/>encoder_outputs, state_h, state_c = model_loaded.layers[6].output #loading encoder_outputs</span><span id="cc54" class="np ks iq oo b gy ow ot l ou ov">#print(encoder_outputs.shape)</span><span id="075f" class="np ks iq oo b gy ow ot l ou ov">encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])</span><span id="e335" class="np ks iq oo b gy ow ot l ou ov"># decoder inference<br/># Below tensors will hold the states of the previous time step<br/>decoder_state_input_h = Input(shape=(latent_dim,))<br/>decoder_state_input_c = Input(shape=(latent_dim,))<br/>decoder_hidden_state_input = Input(shape=(32,latent_dim))</span><span id="79e8" class="np ks iq oo b gy ow ot l ou ov"># Get the embeddings of the decoder sequence<br/>decoder_inputs = model_loaded.layers[3].output</span><span id="dca8" class="np ks iq oo b gy ow ot l ou ov">#print(decoder_inputs.shape)<br/>dec_emb_layer = model_loaded.layers[5]</span><span id="fd8b" class="np ks iq oo b gy ow ot l ou ov">dec_emb2= dec_emb_layer(decoder_inputs)</span><span id="59d3" class="np ks iq oo b gy ow ot l ou ov"># To predict the next word in the sequence, set the initial states to the states from the previous time step<br/>decoder_lstm = model_loaded.layers[7]<br/>decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])</span><span id="07f9" class="np ks iq oo b gy ow ot l ou ov">#attention inference<br/>attn_layer = model_loaded.layers[8]<br/>attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])</span><span id="bdb7" class="np ks iq oo b gy ow ot l ou ov">concate = model_loaded.layers[9]<br/>decoder_inf_concat = concate([decoder_outputs2, attn_out_inf])</span><span id="ad12" class="np ks iq oo b gy ow ot l ou ov"># A dense softmax layer to generate prob dist. over the target vocabulary<br/>decoder_dense = model_loaded.layers[10]<br/>decoder_outputs2 = decoder_dense(decoder_inf_concat)</span><span id="42e7" class="np ks iq oo b gy ow ot l ou ov"># Final decoder model<br/>decoder_model = Model(<br/>[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],<br/>[decoder_outputs2] + [state_h2, state_c2])</span></pre></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="ee08" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">预言</h2><p id="c8b8" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">现在，我们已经训练了序列对序列模型，并使用训练的模型创建了用于进行预测的推理模型。让我们从英语句子中预测一些马拉地语句子。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="2e46" class="np ks iq oo b gy os ot l ou ov">def decode_sequence(input_seq):<br/>    # Encode the input as state vectors.<br/>    e_out, e_h, e_c = encoder_model.predict(input_seq)</span><span id="b7be" class="np ks iq oo b gy ow ot l ou ov"># Generate empty target sequence of length 1.<br/>    target_seq = np.zeros((1,1))</span><span id="1ef1" class="np ks iq oo b gy ow ot l ou ov"># Chose the 'start' word as the first word of the target sequence<br/>    target_seq[0, 0] = Mword2index['start']</span><span id="4578" class="np ks iq oo b gy ow ot l ou ov">stop_condition = False<br/>    decoded_sentence = ''<br/>    while not stop_condition:<br/>        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])</span><span id="2b75" class="np ks iq oo b gy ow ot l ou ov"># Sample a token<br/>        sampled_token_index = np.argmax(output_tokens[0, -1, :])<br/>        if sampled_token_index == 0:<br/>          break<br/>        else:<br/>          sampled_token = Mindex2word[sampled_token_index]</span><span id="f041" class="np ks iq oo b gy ow ot l ou ov">if(sampled_token!='end'):<br/>              decoded_sentence += ' '+sampled_token</span><span id="0c3f" class="np ks iq oo b gy ow ot l ou ov"># Exit condition: either hit max length or find stop word.<br/>              if (sampled_token == 'end' or len(decoded_sentence.split()) &gt;= (26-1)):<br/>                  stop_condition = True</span><span id="868a" class="np ks iq oo b gy ow ot l ou ov"># Update the target sequence (of length 1).<br/>          target_seq = np.zeros((1,1))<br/>          target_seq[0, 0] = sampled_token_index</span><span id="6ae0" class="np ks iq oo b gy ow ot l ou ov"># Update internal states<br/>          e_h, e_c = h, c</span><span id="f19a" class="np ks iq oo b gy ow ot l ou ov">return decoded_sentence</span></pre><p id="892c" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">形成反向词汇:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="f70b" class="np ks iq oo b gy os ot l ou ov">Eindex2word = englishTokenizer.index_word<br/>Mindex2word = marathiTokenizer.index_word</span></pre><p id="e13f" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">在给函数一个字符串之前进行一些转换:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="eb14" class="np ks iq oo b gy os ot l ou ov">def seq2summary(input_seq):<br/>    newString=''<br/>    for i in input_seq:<br/>      if((i!=0 and i!=Mword2index['start']) and i!=Mword2index['end']):<br/>        newString=newString+Mindex2word[i]+' '<br/>    return newString</span><span id="c161" class="np ks iq oo b gy ow ot l ou ov">def seq2text(input_seq):<br/>    newString=''<br/>    for i in input_seq:<br/>      if(i!=0):<br/>        newString=newString+Eindex2word[i]+' '<br/>    return newString</span></pre><p id="6339" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">调用必要的函数，让我们测试我们的翻译模型:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="d5b5" class="np ks iq oo b gy os ot l ou ov">for i in range(10):  <br/>  print("Review:",seq2text(X_test[i]))<br/>  print("Original summary:",seq2summary(y_test[i]))<br/>  print("Predicted summary:",decode_sequence(X_test[i].reshape(1,32)))<br/>  print("\n")</span></pre><blockquote class="mz na nb"><p id="19d4" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">回顾:没人会告诉你<br/>原概要:तुला कोणीही सांगणार नाही <br/>预测概要:कोणीही तुला सांगणार नाही</p><p id="c87c" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">回顾:前瞻<br/>原创总结:समोर बघा <br/>预测总结:तिथे बघ</p><p id="0baf" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">回顾:我要把这个还给汤姆<br/>原摘要:मी हे टॉमला परत करायला जातोय <br/>预测摘要:मी ते स्वतःहून करणार आहे</p><p id="1ff8" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">回顾:一只雄鹰正在天空飞翔<br/>原概要:आकाशात एक गरुड आहे <br/>预测概要:न्यूयॉर्क अतिरेकी तो दुसर्याचा</p><p id="1700" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">回顾:他会说阿拉伯语<br/>原文摘要:तो अरबी बोलतो <br/>预测摘要:तो अरबी बोलतो</p><p id="5b37" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">回顾:收拾这个烂摊子<br/>原概要:हा पसारा साफ करून टाका <br/>预测概要:हा पसारा साफ कर</p><p id="f403" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">回顾:不要在课堂上说法语<br/>原概要:वर्गात फ्रेंचमध्ये बोलू नका <br/>预测概要:वर्गात जास्त कठीण राहू नकोस</p><p id="e422" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">回顾:我关灯了<br/>原概要:मी दिवे <br/>预测概要:मी दोन हात वर केला</p><p id="e287" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">回顾:你有多少球拍<br/>原概要:तुझ्याकडे किती रॅकेट आहेत <br/>预测概要:तुमच्याकडे किती बहिणी आहेत</p><p id="08b9" class="lj lk nc ll b lm mf jr lo lp mg ju lr nd mh lu lv ne mi ly lz nf mj mc md me ij bi translated">回顾:我给了汤姆·玛丽的电话号码<br/>原概要:मी टॉमला मेरीचा फोन नंबर दिला <br/>预测概要:मी टॉमला मेरीचा फोन क्रमांक दिला</p></blockquote><p id="368d" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">万岁！！</p><p id="e165" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">我们的模型可以很好地将英语句子翻译成马拉地语句子。</p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="5127" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">项目演示</h2><p id="0ef6" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我已经通过 Django 部署了我的模型，并在 heroku 上托管了它。你可以在这里看看。</p><div class="pa pb gp gr pc pd"><a href="https://hurdlenet.herokuapp.com/" rel="noopener  ugc nofollow" target="_blank"><div class="pe ab fo"><div class="pf ab pg cl cj ph"><h2 class="bd ir gy z fp pi fr fs pj fu fw ip bi translated">哈希尔·帕特尔</h2><div class="pk l"><p class="bd b dl z fp pi fr fs pj fu fw dk translated">hurdlenet.herokuapp.com</p></div></div></div></a></div></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="42cb" class="np ks iq bd kt nq nr dn kx ns nt dp lb ls nu nv ld lw nw nx lf ma ny nz lh oa bi translated">结尾注释</h2><p id="ac0b" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">在这个故事中，我们学习了注意机制的功能，并实现了一个语言翻译任务。这项任务在日常生活中可能有多个用例。例如，我们可以使用这种技术来构建一个多语言翻译器，它可以从一种语言翻译多种语言。此外，如果我们能把它与光学字符识别系统结合起来，我们就能直接从图像中翻译出文本。</p><p id="c5cd" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">如果您有任何其他使用案例或技术来处理翻译数据，并且如果您为 NMT 找到了一个更好的模型，请在下面的回复框中分享！</p><p id="6364" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">这篇文章的全部代码可以在<a class="ae mt" href="https://github.com/harshilpatel99/NMT_english2marthi" rel="noopener ugc nofollow" target="_blank"> <em class="nc">这里</em> </a>获得。如有任何反馈，欢迎随时联系我<a class="ae mt" href="https://www.linkedin.com/in/harshil-patel-708680148/" rel="noopener ugc nofollow" target="_blank"> <em class="nc"> LinkedIn </em> </a>。</p></div></div>    
</body>
</html>