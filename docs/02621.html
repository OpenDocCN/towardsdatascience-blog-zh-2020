<html>
<head>
<title>Yes, you should listen to Andrej Karpathy, and understand Backpropagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">是的，你应该听安德烈·卡帕西的，并且理解反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/back-propagation-721bfcc94e34?source=collection_archive---------17-----------------------#2020-03-13">https://towardsdatascience.com/back-propagation-721bfcc94e34?source=collection_archive---------17-----------------------#2020-03-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f33d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">不是穿着闪亮盔甲的骑士，但绝对是我们人工智能战斗卡中的无名英雄</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/49d95ef6d671a136e0591b21121b73c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*js_PT2bwbVKmhLVO"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">罗马法师在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="1901" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你问周围的任何人，他们会告诉你人工智能是最热门和发展最快的领域。我们在监督学习方面取得了巨大的进步——自然语言处理和计算机视觉。这两个领域解决非常不同的问题，并且没有很多共同的文献。在一个方面表现出色的模型在另一个方面表现不佳。但在内心深处，它们确实有一些共同点，那就是反向传播——这种机制有助于我们从输出到输入反向传播损失——在模型的训练阶段发挥着非常关键的作用。</p><p id="f199" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是首先让我们看看每一个监督学习模型将会有什么:</p><ul class=""><li id="4873" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu"> <em class="me">一个架构</em> </strong>:可以是CNN/RNN/NN。此外，层数、每层的激活函数和维数、学习速率等。</li><li id="8827" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated"><strong class="lb iu"> <em class="me">一个损失/成本函数</em> </strong>:当一个ML模型做一个预测的时候，我们需要一个方法来计算我们离地面真相有多远。</li><li id="2720" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated"><strong class="lb iu"> <em class="me">一个优化器</em> </strong>:一旦我们有了损失，我们需要更新我们的模型的权重，以便最小化我们的损失。</li></ul><p id="3277" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么反向传播在这幅图中处于什么位置呢？它连接了损失函数和优化器。损耗在输出层计算，然后我们使用反向传播将损耗以其梯度的形式传播回去，一直传播回输入层的输入(因此称为反向传播)。一旦back prop完成，我们知道每个节点的损失梯度，然后我们使用优化器来更新权重，试图在下一次迭代中最小化损失。Back prop是本地的</p><h1 id="ec32" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">如何计算渐变？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/403739e5e767c707e4b8467b9721953a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jEl1FPKYSNkQuBymmLLe-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算F=X*Y*Z的梯度</p></figure><p id="93b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">设<em class="me"> F= X*Y*Z </em>并要求你计算<strong class="lb iu"> <em class="me"> ∂F/ ∂X，</em></strong><strong class="lb iu"><em class="me">∂f/ ∂z</em></strong>。<br/>我们很容易弄清楚，<br/><em class="me">∂f/ ∂x</em>=<em class="me">y * z</em>=-2 *-3 = 6，<br/><em class="me">∂f/ ∂y</em>=<em class="me">x * z</em>= 6 *-3 =-18，<br/>和<em class="me"> ∂F/ ∂Z = X*Y </em> = 6*-2。</p><p id="129c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们如何使用上面的<a class="ae ky" href="https://www.youtube.com/watch?v=d14TUNcbn1k&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=4" rel="noopener ugc nofollow" target="_blank">计算图</a>来计算呢？</p><ol class=""><li id="55d4" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nd mb mc md bi translated">从最终输出开始，<em class="me"> F </em>。∂F/ ∂F是1。所以回流到乘法节点(<em class="me"> A*Z </em>)的上游梯度为1。</li><li id="50e0" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu nd mb mc md bi translated">计算节点输入的局部梯度<em class="me"> F=A*Z，∂F/∂A </em> = <em class="me"> Z </em>和<em class="me"> ∂F/∂Z = A. </em>将为输入回流的上游梯度将是来自特定节点输出 的<strong class="lb iu"> <em class="me">局部梯度*上游梯度。因此，a的上游梯度将为<em class="me"> ∂F/∂A * ∂F/∂F = Z*1 = -3 </em>，而z的上游梯度将为<em class="me"> ∂F/∂Z * ∂F/∂F = A*1 = -12。</em></em></strong></li><li id="e178" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu nd mb mc md bi translated">对节点<em class="me"> A = X*Z. <br/> </em>上游坡度= -3重复步骤2。<br/>x的局部梯度:<em class="me"> ∂A/∂X = Y = -2。<br/></em>y的局部梯度:<em class="me"> ∂A/∂Y = X= 6。<br/> </em>上游梯度回流x:<em class="me">localgradientx * upstreamgradientfromputput</em>=<em class="me">∂a/∂x * ∂f/∂a = ∂f/∂x =-2 *-3 = 6。<br/> </em>上游梯度回流y:<em class="me">localgradienty * upstreamgradientfromputput</em>=<em class="me">∂a/∂y * ∂f/∂a = ∂f/∂y = 6 *-3 =-18。</em></li></ol><h1 id="f006" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">神经网络中的反向传播</h1><p id="bde0" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">这里唯一改变的是每个节点上发生的计算。不是简单的乘法运算，每个节点都有一个<em class="me">激活</em> <em class="me">函数，Y =H(X1，X2，…，Xi)。<br/> </em>在正向传递本身中——当我们计算<em class="me"> Y </em>时——我们还会计算并存储当地的梯度<em class="me"> ∂Y/∂X1、∂Y/∂X2、∂Y/∂X？</em>和<em class="me"> ∂Y/∂Xi.</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/17978b8c5b7aa7a4a81b642f499f9d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2BUhTthxHqpQUB6sy-2gSg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">神经网络中梯度流和计算的示例。红色箭头显示渐变的流向。绿色箭头显示了向前传递的值的流向。</p></figure><p id="b996" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们得到反向传播中的上游梯度时，我们可以简单地将其与对应于每个输入的局部梯度相乘，并将其传回。在上面的例子中，我们从2个节点获得上游梯度，因此绿色节点接收的总梯度只是所有上游梯度的相加——在这种情况下是两个上游梯度。然后，我们将每个局部梯度乘以总的上游梯度，并将其向后传递给相应的输入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/ae989e7ed8323b70fd6ae92ad3ce984f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m9K5FXKErqp4EezgXMWo1Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">雅可比矩阵。每一列都是某个输入向量的局部梯度。<a class="ae ky" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="6cc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在神经网络中，节点的输入<em class="me"> X </em>和输出是向量。函数<em class="me"> H </em>是一个矩阵乘法运算。<em class="me"> Y =H(X) = W*X，其中W </em>是我们的权重矩阵<em class="me">。</em>局部梯度为<a class="ae ky" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant" rel="noopener ugc nofollow" target="_blank">雅可比矩阵</a>—<em class="me">Y</em>wrt<em class="me">X</em>各元素的微分。</p><p id="d548" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，输入向量<em class="me"> Xi </em>只影响输出向量中的<em class="me"> Fi </em>。</p><h1 id="ee28" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">RNN的反向支持——递归神经网络</h1><p id="9c1d" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">在RNNs中事情变得有点棘手，因为与NNs不同，在NNs中，节点的输出和输入彼此独立，当前步骤的输出作为输入被馈送到下一步骤中的相同节点。理论上，它对我们的反向传播流没有任何影响，但在实践中，它导致了臭名昭著的消失梯度和爆炸梯度问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/be1ef7fb40f65117184dd1b6537897f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i_pFOJvRuHAGq3WPAT-vfQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">未卷起的RNN中的梯度流。注意，每个时间步长的Fw保持不变。由于相互依赖，每个时间步的输出和输入都是变化的。</p></figure><p id="1a8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑上面的普通RNN，在每个时间步的向前传递中，<em class="me"> F(xi) = W*xi，其中xi </em>是第<em class="me"> i </em>时间步的输入，<em class="me"> W </em>是权重矩阵<em class="me">，W </em>对于每个步骤保持不变。在反向传播期间，每个节点的局部梯度被称为<em class="me"> h(xi)。</em>到达输入节点的梯度将是L* <em class="me"> h(xt)*h(xt-1)*..*h(x1)。</em>如果所有的<em class="me"> h(xi) </em>大于<em class="me"> k </em>，那么输入处的梯度大于<em class="me"> L*k^t. </em>对于深度RNNs <em class="me"> t </em>可能很大，在这种情况下，如果k &gt; 1，梯度继续呈指数增加，导致梯度变得太大的爆炸性梯度问题。<br/>类似地，在输入节点处的所有<em class="me"> h(xi) &lt; k，</em>梯度将小于<em class="me"> L*K^t </em>的情况下，如果k &lt; 1，梯度在到达输入层时将几乎变为0——消失梯度问题，其中梯度变得太小而无法引起任何有意义的更新。</p><p id="4cde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于<em class="me"> F(xi) </em>本质上是一个矩阵乘法，<em class="me">h(Xi)= F’(Xi)也就是~ W </em>。对于具有<em class="me"> t </em>时间步长的RNN，在反向传播期间，局部梯度将是<em class="me"> h(xi)，即</em>权重矩阵——或其一部分——并且在每一步，当我们计算<em class="me">local gradient * upstream gradient</em>时，它将一次又一次地与自身相乘，因为上游梯度是某个局部梯度(本质上是<em class="me"> W </em>或其一部分)及其在每个先前时间步长的相应上游梯度的递归乘积。根据<em class="me"> W </em>中元素的值，我们将面临两种梯度问题中的任何一种，大于1的元素会爆炸，更小的元素会消失。</p><h1 id="1009" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">CNN —卷积神经网络中的反向传播</h1><p id="5e1a" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">CNN的情况与其他案例略有不同，但基本概念保持不变。我们仍将通过乘以上游和局部梯度来计算梯度，但在这种情况下事情有点复杂。在前向传递中，这里的输入不像在NN中那样贡献给所有的输出，同时，它们也不像在RNN中那样只贡献给单个输出。在正向传递中，如下图所示，每个输出总是接受四个不同输入的贡献，而一个输入可以贡献的输出数量取决于其位置。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/b22fd65c161686df395c1f3e9c8d0579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DewvAgSZi6s3JUbnyPymbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CNN中的向前传递:一个2x2内核(显示为黄色)以步长1覆盖输入层(3x3 ),以给出一个2x2输出——具有四种不同的颜色，每种颜色突出显示在其计算中起作用的输入。</p></figure><p id="003c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在回柱过程中，我们将知道上游的坡度:<em class="me"> ∂L/ ∂y1、∂L/ ∂y2、∂L/ ∂y3 </em>和<em class="me"> ∂L/ ∂y4 </em>。当地梯度:<em class="me"> ∂y1/ ∂x1 </em>、∂y1/ ∂x2、∂y1/ ∂x3、∂y1/ ∂x4、∂y1/ ∂x5、∂y1/ ∂x6、∂y1/ ∂x7、∂y1/ ∂x8和∂y1/ ∂x9。</p><p id="5e70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们知道<em class="me"> y1 = w1*x1 +w2*x2 +w3*x4 +w4*x5。</em>因此，<br/> <em class="me"> ∂y1/ ∂x1 = w1。</em>从上图可以看出<em class="me"> x1 </em>只对<em class="me"> y1 </em>有贡献，所以<em class="me">∂l/ ∂x1 = ∂l/ ∂y1*∂y1/ ∂x1 = ∂l/ ∂y1 * w1。</em></p><p id="ed9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<strong class="lb iu"> ∂L/ ∂x5 </strong>，由于<em class="me"> x5 </em>对四路输出都有贡献，<br/><em class="me">∂l/ ∂x5 = ∂l/ ∂y1*∂l/ ∂y1*+∂l/ ∂y2*∂y2/ ∂x5+∂l/ ∂y3*∂y3/ ∂x5+∂l/ ∂y4*∂y4/ ∂x5，<br/> </em> ∂y1/ ∂x5 = <em class="me"> w4，<br/> ∂y2/ ∂x5 = w3，<br/>≈y1/≈X5 = w1</em><br/><em class="me">≈y1/≈X5 = w2。</em></p><p id="0bd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，我们可以通过将核旋转180度并使用旋转后的核对输出矩阵执行完全卷积来计算梯度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/9b75105797269d7a331b669d41177c83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XbRTJLom9i5s0lKsbfjJIQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">执行全卷积以计算梯度。<a class="ae ky" href="https://medium.com/@2017csm1006/forward-and-backpropagation-in-convolutional-neural-network-4dfa96d7b37e" rel="noopener">参考</a>。</p></figure></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="bf47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在他关于反向传播的文章中，Andrej Karpathy描述如下:</p><blockquote class="nv nw nx"><p id="4297" class="kz la me lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">反向传播是一种有漏洞的抽象；这是一个具有重要后果的信用分配方案。如果你因为“TensorFlow自动让我的网络学习”而试图忽略它在幕后是如何工作的，你将没有准备好应对它所带来的危险，你在构建和调试神经网络方面的效率将会低得多。</p></blockquote><p id="bc58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然您可能在不了解反向传播的情况下训练一个ML模型并获得良好的结果，但是理解它将使您在训练神经网络时占上风，这将使您能够更好地理解工作，从而提高其性能。</p><h2 id="ae78" class="ob ml it bd mm oc od dn mq oe of dp mu li og oh mw lm oi oj my lq ok ol na om bi translated">参考资料:</h2><ul class=""><li id="9f7c" class="lv lw it lb b lc ne lf nf li on lm oo lq op lu ma mb mc md bi translated"><a class="ae ky" href="https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">deep grid</a>——阅读更多关于CNN的信息。</li><li id="ead1" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated">了解全卷积:<a class="ae ky" href="https://medium.com/@2017csm1006/forward-and-backpropagation-in-convolutional-neural-network-4dfa96d7b37e" rel="noopener">https://medium . com/@ 2017 CSM 1006/卷积神经网络中的前向和后向传播-4dfa96d7b37e </a></li><li id="9fd6" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated">安德鲁·卡帕西的斯坦福笔记</li><li id="881a" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated">在斯坦福大学的讲座中，点击查看更多关于计算图的信息。</li></ul></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><blockquote class="nv nw nx"><p id="c3b2" class="kz la me lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">我很高兴你坚持到了这篇文章的结尾。<em class="it">👏我希望你的阅读体验和我写这篇文章时一样丰富。<em class="it">💖</em></em></p><p id="5b5b" class="kz la me lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">请点击这里查看我的其他文章。</p><p id="df1a" class="kz la me lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">如果你想联系我，我会选择推特。</p></blockquote></div></div>    
</body>
</html>