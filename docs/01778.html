<html>
<head>
<title>Cracking Cause and Effect with Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用强化学习破解因果</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525?source=collection_archive---------24-----------------------#2020-02-18">https://towardsdatascience.com/cracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525?source=collection_archive---------24-----------------------#2020-02-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eadd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用强化学习来推断因果关系可能会打开机器学习的新领域，并可能避免另一个人工智能冬天</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8a8b33c8524214f182bdabdbaf2efd12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F8ZAvew6t3ZXrsTh2GJR3g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@umby?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">翁贝托</a>在<a class="ae ky" href="https://unsplash.com/s/photos/ice-cracking?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="015e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">推断因果的困难无处不在。人类每天都面临这一挑战，并且在克服这一问题上做得非常出色，至少与我们的动物近亲相比是如此。Yoshua Bengio等机器学习先驱甚至建议，创建可以推断因果的算法是避免另一个人工智能冬天和解锁机器智能新前沿的关键。在这篇文章中，我解释了强化学习是如何被重新构建来推断因果关系的，类似于我们人类这样做的能力，也许有一天会远远超过它。</p><p id="bc03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于那些不熟悉强化学习的人来说，它指的是用于确定顺序决策任务中的最优性的算法子集。它通常表现为一个代理人学习在一个环境中采取一系列行动来获得奖励。这些算法很容易应用于可以“游戏化”的问题，从而有明确定义的行动和奖励。最近，强化学习算法因在围棋、星际争霸和各种视频游戏中击败人类而广受好评。</p><p id="f56e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种强化算法取得成功的基本手段是通过连续的反复试验训练使预测误差最小化的概念。该方法相当准确地对应于人类多巴胺学习系统，该系统也利用预测误差。</p><p id="459f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两个主要障碍使得推断因果关系成为一个难题，一个是复杂性，另一个是相关性。接下来我将解释强化学习如何被重新构建来解决这些问题。</p><p id="d3a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，强化学习通常表现为一个主体在一个环境中为追求某种回报(比如赢得一场游戏)而做出一系列决策。代理可以采取行动，根据环境的状态，这些行动要么得到奖励，要么没有。当他们得到奖励时，代理将这种奖励传播回所有导致它最终获得奖励的行为和环境状态。通过多次试验，它可以确定哪些行为和环境状态的组合有助于获得奖励，哪些是多余的。从另一个角度来看，代理确定哪些行为和环境状态的组合与它获得奖励有因果关系，哪些没有。实际上，代理正在推断<em class="lv">因果</em>关于它自己的行为和环境的特定回报状态。</p><p id="19bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重要的一点是，从根本上说，RL算法对于<em class="lv">来说是不可知的，环境中的哪个</em>组件是代理，哪个仅仅是<em class="lv">对象</em>。我们可以把环境中的所有对象看作可能的代理，把一个对象可能存在的所有状态看作它的动作集，而不是从代理、它的动作和环境的角度来考虑。这样，环境中给定的一组组件和特定的最终状态之间的任何因果关系都可以通过简单地改变哪个对象是代理以及被奖励的最终状态是什么来探索。</p><p id="aa3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用一个例子来解释这个问题。RL的一个相当常见的演示是匹配样本任务，其中代理人(通常是一种小型毛茸茸的哺乳动物)必须学会采取一系列行动来获得食物奖励，例如，当灯亮时按下控制杆。经过训练，许多动物和人工智能机器人都可以解决这类任务。(参见强化学习代理解决样本匹配任务的附带视频)</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="163e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在想象我们是一个天真的社会科学家，不知道这个实验是如何设计的，并且希望知道是什么导致食物奖励被解锁。换句话说，我们想知道匹配样本任务背后的因果故事。回答这个问题的一个潜在方法是采用RL算法，该算法顺序地通过环境中的对象，将每个对象视为“代理”，并尝试查看该代理/对象是否可以采取一系列动作来最小化关于解锁食物奖励的预测误差。与打开食物碗的奖励状态有因果关系的对象将具有一些动作，这些动作允许它最小化关于该奖励状态的预测误差。一个没有因果关系的将没有这样的状态/动作组合，不管它的动作集合<em class="lv">与奖励状态</em>有多好的关联。</p><p id="9090" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">记住，在我们的重构中，动作只是指一个对象可以存在的不同状态。例如，我们可以以这样的方式提出这个问题，打开和关闭的灯是代理，它的动作也是打开和关闭的。如果在我们选择作为代理的任何对象的行为和我们想要理解的回报状态之间存在因果关系，那么也存在关于其行为空间的预测误差，该误差可以通过连续的训练集被最小化。换句话说，有一种方式，这个对象/代理可以因果地影响奖励状态。</p><p id="7d08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解开环境中因果关系故事的一种潜在方法是迭代地移动环境中的对象，将它们视为采取行动的代理，并检查哪一个导致了关于奖励状态的可以最小化的预测误差。同样，与奖励状态没有因果关系或者仅仅与奖励状态相关的对象在其动作空间上将具有预测误差，该预测误差基本上是随机的，并且不会随着更多的训练集而减少。以这种方式，RL提供了一种在算法上探测给定环境中的元素的因果关系的方式，依次将每个元素视为试图通过一些动作来改变其行为以便影响奖励状态的代理。</p><p id="81cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，预测误差减小的速率，通常被称为代理学习曲线，将给出一些指示，表明对象/代理与奖励状态的因果距离。虽然蝴蝶扇动翅膀可能会在地球的另一边引起飓风，但是与最小化关于更接近的原因的预测误差所需的训练集的数量相比，预测这种情况所需的训练集的数量将是天文数字。因此，在所有其他条件都相同的情况下，学习曲线在衡量相对因果接近度方面非常有用。</p><p id="338d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多假设和限制适用于这种方法，即所有那些适用于强化学习本身。它还假设我们有一种方法来评估环境中每个代理/对象的动作空间，并且这满足马尔可夫特性。在匹配样本任务的例子中，环境中的每个对象都有一个小的明确定义的动作空间。对于具有大的或连续的动作空间的对象，探索因果关系可能变得难以计算。同样值得注意的是，这个用于推断因果关系的系统只适用于可以产生大量样本试验的情况，正如RL本身只适用于“自我播放”允许大量训练集的情况。虽然这可能是一种探索因果关系的计算密集型方法，但随着计算成本的降低，越来越多的环境可以通过这种方法接近。</p><p id="ca1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">科学的持久挑战之一仍然是确定复杂系统中的因果关系。例子包括人类基因组，其中无数的基因和环境因素促成了特定的表型。在这种情况下，因果关系可能很难推断，这种方法可能有助于揭示哪些"行为者"或国家有助于实现特定成果。它还向能够“解释”用于确定特定行动过程的因果推理的代理人开放了人工智能领域。另一个潜在的应用是生成先验知识，通过这些知识，人工智能可以存储以前推断的因果关系列表，并在遇到挑战性问题时将它们应用到新的行动空间。这种先验知识可以极大地减少成功完成一项任务所必需的训练，并可能导致人类所展示的一次性学习。</p></div></div>    
</body>
</html>