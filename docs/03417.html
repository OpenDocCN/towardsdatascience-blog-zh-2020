<html>
<head>
<title>Why use Mean Squared Error (MSE)?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么要用均方差(MSE)？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/where-does-mean-squared-error-mse-come-from-2002bbbd7806?source=collection_archive---------10-----------------------#2020-04-01">https://towardsdatascience.com/where-does-mean-squared-error-mse-come-from-2002bbbd7806?source=collection_archive---------10-----------------------#2020-04-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b050" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从概率角度看均方误差</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/5664c348afba3f0c483c4a5c8c6106f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CpPcWcHmy0uGYIy4"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">Genessa panainite 在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="006e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">大家好。</p><p id="0c12" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果你对机器学习感兴趣，但没有深入研究它背后的概率论，你可能会想知道损失函数从何而来。他们仅仅是因为在实践中效果更好而使用<em class="mc">吗？</em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi md"><img src="../Images/4ffcbbc30ed29656284c805edc49bd88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AUTX9YGD5VM7A9ZLjw3crQ.jpeg"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">一个来自<a class="ae lf" href="http://knowyourmeme.com" rel="noopener ugc nofollow" target="_blank">knowyourmeme.com</a>(<a class="ae lf" href="https://i.kym-cdn.com/photos/images/newsfeed/001/461/040/fcd.jpg" rel="noopener ugc nofollow" target="_blank">原始网址</a>)的常见模因模板</p></figure><p id="f9ad" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我想我会分享从<a class="ae lf" href="http://cs109.stanford.edu" rel="noopener ugc nofollow" target="_blank"> CS109(斯坦福大学计算机科学家的概率论)</a>中获得的一个见解，这是我最近参加的一门课程。我们将回答以下问题:</p><ul class=""><li id="62c2" class="me mf it li b lj lk lm ln lp mg lt mh lx mi mb mj mk ml mm bi translated">MSE 到底是从哪里来的？只是有人猜测一个好的损失函数吗？</li><li id="9d96" class="me mf it li b lj mn lm mo lp mp lt mq lx mr mb mj mk ml mm bi translated">为什么你想要预测和地面真相之间的差异的平方，而不是只取其绝对值，或将其提高到四次方，或六次方？？换句话说，<em class="mc">为什么 MSE 效果这么好？</em></li></ul><p id="df00" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">快速品尝一下答案:<strong class="li iu">这取决于我们对数据如何建模的合理假设的本质。</strong></p><p id="aed7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">三项免责声明:</p><ul class=""><li id="480d" class="me mf it li b lj lk lm ln lp mg lt mh lx mi mb mj mk ml mm bi translated">这不是激励 MSE 的唯一方法，但我确实没有见过，我认为这比“因为在实践中你可以更容易地得到它的导数”更有见地</li><li id="8d71" class="me mf it li b lj mn lm mo lp mp lt mq lx mr mb mj mk ml mm bi translated">我自己意识到了这一点，但我确信在我之前还有很多人意识到了这一点</li><li id="c7a9" class="me mf it li b lj mn lm mo lp mp lt mq lx mr mb mj mk ml mm bi translated">我假设你熟悉概率(什么是正态分布，不太清楚)，但不一定是上过概率论课的水平。如果有，太好了！你可以跳过一些背景。</li></ul><p id="bdf2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">但首先，了解一些背景知识。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="08ba" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">背景</h1><p id="9471" class="pw-post-body-paragraph lg lh it li b lj nk ju ll lm nl jx lo lp nm lr ls lt nn lv lw lx no lz ma mb im bi translated">你可能知道，机器学习与概率交织在一起，所以我们将在这里介绍一些相关的概念。</p><h2 id="8567" class="np mt it bd mu nq nr dn my ns nt dp nc lp nu nv ne lt nw nx ng lx ny nz ni oa bi translated">最大似然估计</h2><p id="f723" class="pw-post-body-paragraph lg lh it li b lj nk ju ll lm nl jx lo lp nm lr ls lt nn lv lw lx no lz ma mb im bi translated">在很多机器学习中，你的目标是为你的数据找到最佳模型(无论是找到将图像分类为包含猫或狗或其他模型的最佳 convnet)。这背后的一个核心概念是<em class="mc">最大似然估计。</em></p><p id="2c9e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">什么是最大似然估计？我们将用下面的例子来演示(改编自 Youtube 频道 StatQuest 的<a class="ae lf" href="https://www.youtube.com/watch?v=XepXtl9YKwc" rel="noopener ugc nofollow" target="_blank">这个很棒的视频)。</a></p><h2 id="cc26" class="np mt it bd mu nq nr dn my ns nt dp nc lp nu nv ne lt nw nx ng lx ny nz ni oa bi translated">例子</h2><p id="d7a6" class="pw-post-body-paragraph lg lh it li b lj nk ju ll lm nl jx lo lp nm lr ls lt nn lv lw lx no lz ma mb im bi translated">让我们假设你有一个神秘的正态分布，点被抽样，并说它代表老鼠的体重。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ob"><img src="../Images/1de2340c0df72f3a4bcde8147cb217b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTjT7suVHy7R-wVglhxuKg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">来源:<a class="ae lf" href="https://www.youtube.com/watch?v=XepXtl9YKwc" rel="noopener ugc nofollow" target="_blank"> Youtube/StatQuest </a></p></figure><p id="0401" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">统计学家做的一件有点奇怪的事情是<strong class="li iu">假设数据是从某个模型</strong>中抽取的。在这种情况下，我们可以假设这个小鼠体重数据是从正态分布中取样的。现在让我们找到它！(这可能看起来与说一些关于 MSE 的事情非常无关，但请原谅我。)</p><p id="e84e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">直觉上，您可能希望正态分布在样本中心附近达到峰值。你也希望正态分布根据样本分布的距离而展开。</p><p id="d56d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">所以，你会<em class="mc">而不是</em>想要这样的东西</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oc"><img src="../Images/5011dda54226eb323d54a098daae82d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gG0NHbjzs5dbWuuNM8YWOw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">“坏”模型；来源:<a class="ae lf" href="https://www.youtube.com/watch?v=XepXtl9YKwc" rel="noopener ugc nofollow" target="_blank"> Youtube/StatQuest </a></p></figure><p id="03af" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">但是你会想要这样的东西吗</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi od"><img src="../Images/542294f69dc88014bfd93a0250d959cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R5NwmTF3nKikhMIAXcJYfg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">一个“好”的模型；来源:<a class="ae lf" href="https://www.youtube.com/watch?v=XepXtl9YKwc" rel="noopener ugc nofollow" target="_blank"> Youtube/StatQuest </a></p></figure><p id="d791" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">原来有一种数学方法可以表达一个模型有多“好”(这一节很重要)。我们称之为给定模型的数据的<em class="mc">可能性</em>。它可以用函数 L 表示如下，其中θ是模型，I 是样本的指数，f 是概率密度函数(PDF)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/42b8045acd624762f72f40d6a9582451.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/1*02CRXhvrhkKFpzNw5Coaag.gif"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">可能性的定义</p></figure><p id="271a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">请注意，在函数 f 中，参数包含一个竖线|。这仅仅意味着我们正在评估函数<em class="mc">，因为</em>我们使用的是模型θ。</p><p id="409f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">似然性本质上包含所有样本的概率密度的乘积。换句话说，这是对数据集被赋予选择特定模型的可能性的综合衡量。</p><p id="e61e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然后，要找到最佳模型，只需取其“argmax”即可。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi of"><img src="../Images/fb922596ffc3742cd889e2a5bea68613.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/1*7c86ovSAlEodoxqTN9HACQ.gif"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">最佳模特</p></figure><p id="7687" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">实际上，如果足够简单，这将涉及<a class="ae lf" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-diff-analytical-applications-new/ab-5-4/a/applying-the-first-derivative-test-to-find-extrema" rel="noopener ugc nofollow" target="_blank">求导</a>，或者如果更复杂，将涉及其他一些优化方法，例如<a class="ae lf" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度上升</a>。</p><h2 id="b75d" class="np mt it bd mu nq nr dn my ns nt dp nc lp nu nv ne lt nw nx ng lx ny nz ni oa bi translated">关于可能性的直觉</h2><p id="1a9a" class="pw-post-body-paragraph lg lh it li b lj nk ju ll lm nl jx lo lp nm lr ls lt nn lv lw lx no lz ma mb im bi translated">为了巩固关于可能性如何工作的一点直觉，考虑所有样本都是 x_i=5.0 的数据集。然后，假设你的模型是一个正态分布，紧紧围绕 x=15。然后，PDF f(x_i|theta)会极小，乘以许多小值会产生一个极小的似然值。</p><p id="535c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">相比之下，考虑正态分布紧紧集中在 x=5.0 附近，正是样本出现的地方。直观来看，这款应该比上一款好很多。我们用似然来验证一下:PDF f(x_i|theta)会非常大，很多大值相乘会产生一个非常大的似然值。更好的模型可能性更大，这和我们的直觉不谋而合。</p><h2 id="8a5d" class="np mt it bd mu nq nr dn my ns nt dp nc lp nu nv ne lt nw nx ng lx ny nz ni oa bi translated">负对数似然</h2><p id="1b07" class="pw-post-body-paragraph lg lh it li b lj nk ju ll lm nl jx lo lp nm lr ls lt nn lv lw lx no lz ma mb im bi translated">在实践中，我们实际上稍微修改了一下上面的可能性表达式。首先，我们取可能性的对数。为什么？两个主要原因:</p><ol class=""><li id="c803" class="me mf it li b lj lk lm ln lp mg lt mh lx mi mb og mk ml mm bi translated">这对于浮点运算来说要好得多(你不想因为把一堆小数字相乘而损失精度)。</li><li id="e854" class="me mf it li b lj mn lm mo lp mp lt mq lx mr mb og mk ml mm bi translated">求和的导数比求积更容易。</li></ol><p id="1b76" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">由于似然性通常位于 0 和 1 之间，因此对数似然性(表示为 L1(θ))将位于负无穷大和 0 之间。</p><p id="f715" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在考虑最大化负值是很尴尬的，所以我们取其负值，称之为(恰当地命名为)“负对数似然”，而不是<em class="mc">最小化</em>这个。这通常成为我们熟悉的常规损失函数(交叉熵损失只是负对数可能性的伪装，但这是另一个故事)。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="e705" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">MSE？？</h1><p id="c1b1" class="pw-post-body-paragraph lg lh it li b lj nk ju ll lm nl jx lo lp nm lr ls lt nn lv lw lx no lz ma mb im bi translated">现在，我们将通过一个演示示例来展示 MSE 的来源。</p><p id="af03" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了简单起见，让我们执行线性回归；即找到最佳拟合线。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oh"><img src="../Images/6061121f99036a718b7fbafe9733bfc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*87aMm1RRoaxS4Sy8Q-XMDg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">线性回归；来源:<a class="ae lf" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">维基百科/线性回归</a></p></figure><p id="3305" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在让我们写下如何对数据建模(在鼠标重量的例子中，我们假设它是通过正态分布生成的)。这正是导致 MSE 的原因。</p><h2 id="1e4c" class="np mt it bd mu nq nr dn my ns nt dp nc lp nu nv ne lt nw nx ng lx ny nz ni oa bi translated">我们的假设</h2><p id="7d35" class="pw-post-body-paragraph lg lh it li b lj nk ju ll lm nl jx lo lp nm lr ls lt nn lv lw lx no lz ma mb im bi translated">让我们假设我们的数据是按照如下方式采样的:<strong class="li iu">给定 X 的某个值，Y 完全由 mx+b 确定，该值受到以 0 为中心的正态分布的随机样本的扰动</strong>(其中“m”是斜率，“b”是 Y 截距)。这是有意义的，因为直觉上给定一个 X，Y 并不总是精确的 mx+b，而是在 mx+b 附近有一些噪声(现实世界会妨碍)。</p><p id="45cb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">从视觉上，你可以想象我们的模型真的只是一堆正态分布沿对角线方向堆叠，形成山脉般的外观。对于那些数学爱好者来说，我们的模型可以表示为</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/621612abb4dbfd657c1e78f1975ab577.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/1*VdN95yH6NsyDCtCDHRjDbw.gif"/></div></figure><p id="1cd1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">其中“X”是某个变量，参数是“m”和“b”，而“Z”是某个预先确定的具有零均值和某个方差σ的高斯函数。</p><h2 id="a184" class="np mt it bd mu nq nr dn my ns nt dp nc lp nu nv ne lt nw nx ng lx ny nz ni oa bi translated">推导 MSE</h2><p id="4017" class="pw-post-body-paragraph lg lh it li b lj nk ju ll lm nl jx lo lp nm lr ls lt nn lv lw lx no lz ma mb im bi translated">现在我们想计算这样一个模型的可能性(这里唯一的参数是θ= m)。首先，回想一下高斯分布的 PDF 的定义:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/0c26b3382d5c038f394094c41a746e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/1*y5RsgPw9f_LJzZzdLqmKeA.gif"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">高斯分布的 PDF</p></figure><p id="811b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在，这个模型的可能性(我们可以忽略外部的正常数因子，因为它不影响我们的优化)是</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/3431065f69fa4c18485b74e3f394bd31.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/1*VAK7lQoCVT9mXuMsalCPBw.gif"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">此模型的可能性</p></figure><p id="3e3f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们简单地通过用高斯函数代替我们的 PDF 来获得之前的似然性定义。</p><p id="336f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在实践中，我们希望获得负对数似然。所以，经过一些代数运算后，对数似然是</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/dfb4bf1eb9c78d386d003081d6f55272.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/1*lIIrZLnh6FkCFTJc-HtzpQ.gif"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">此模型的对数似然</p></figure><p id="0c7a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然后，负对数似然</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi om"><img src="../Images/7c862d5ebc5d3c5ed9103022f0fb48cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/1*O07nzLvtFrn2eARXXI2BoA.gif"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">负对数似然</p></figure><p id="2e17" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">瞧啊。负对数似然，我们从假设真实数据对于给定的 x 呈正态分布中获得的损失函数是<strong class="li iu">非常精确的均方误差</strong>，即</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi on"><img src="../Images/327ffef7d894845eee0d16eb5a4dd183.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/1*bSbQgLZs3fMbDT5N71s7ig.gif"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">一般来说，均方误差的定义</p></figure><p id="2bbb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">只差一个常数！还不错。</p><h2 id="ab3e" class="np mt it bd mu nq nr dn my ns nt dp nc lp nu nv ne lt nw nx ng lx ny nz ni oa bi translated">一些反思</h2><p id="916c" class="pw-post-body-paragraph lg lh it li b lj nk ju ll lm nl jx lo lp nm lr ls lt nn lv lw lx no lz ma mb im bi translated">所以这就是均方差是如此普遍和巨大的损失函数的原因；<strong class="li iu">它基于极其合理的概率假设</strong>:给定一些自变量的值(也称为世界的某个状态)，因变量按照高斯分布分布。现在已经非常清楚(y_i - y_pred)的来源:<strong class="li iu">高斯分布 PDF 中的指数项</strong>。我认为这是一种更好的思考 MSE 的方式，而不仅仅是简单地“我们使用它是因为它很容易区分”(尽管这在某种程度上是正确的)。</p><p id="8c86" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果你开始将此推广到更复杂的推理任务，你就可以开始对 MSE 如何运作有真正的直觉了。例如，考虑用多项式拟合一组(x，y)点。你可以想象在曲线周围的 y 方向上上下下，我们有突出页面的高斯 pdf。现在，您可以将使用 MSE 的梯度下降视为在 y 方向上上下调整一组高斯函数，以最大化每个单独 x 值的可能性，并且整个问题已经简化为一个迭代鼠标重量的示例，其中我们正在计算如何放置 x 的每个值的正态分布<em class="mc">的平均值。这种直觉无缝地转化为许多 x 值(即一个 x 向量)的情况。</em></p><p id="b67b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在想象有多个 y 输出。然后，你试图为每个可能的 x 找到一个多元高斯分布的最佳平均向量。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="353b" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">总结想法</h1><p id="b418" class="pw-post-body-paragraph lg lh it li b lj nk ju ll lm nl jx lo lp nm lr ls lt nn lv lw lx no lz ma mb im bi translated">当我第一次在最大似然估计的背景下通读涵盖“线性回归精简版”(没有 y 截距的线性回归)的 CS109 课堂笔记时，我没有意识到它可以带来一些关于我已经使用和熟悉了相当长时间(MSE)的事物的直觉和见解。重温它们，再读一遍，认真思考，结果是一次颇有收获的经历。关于它的写作也填补了我理解上的一些漏洞。我建议至少为你感兴趣的课程做一些这样的事情；这会让你更加欣赏这个主题。</p><p id="69b9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">此外，这个小小的探索让我对人工智能和深度学习的技术有了更好的理解。许多现有的技术起初看起来像是任意构造的，但如果你深入研究概率论，就会发现它们都来自相同的核心思想。因此，我认为学习这些基本概念是必要的:它帮助你以一种更加基础的方式思考 ML，它甚至可能帮助你想出你自己的新的有趣的技术/算法！</p></div></div>    
</body>
</html>