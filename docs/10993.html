<html>
<head>
<title>Web scraping with Scrapy: Theoretical Understanding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Scrapy 网络抓取:理论理解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/web-scraping-with-scrapy-theoretical-understanding-f8639a25d9cd?source=collection_archive---------4-----------------------#2020-07-31">https://towardsdatascience.com/web-scraping-with-scrapy-theoretical-understanding-f8639a25d9cd?source=collection_archive---------4-----------------------#2020-07-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="63f8" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">网刮系列</h2><div class=""/><div class=""><h2 id="b62c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Scrapy 入门</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fc3a26f59c6c37e1f6fb4753308d81fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nn_1TII7Fl8fbVo3G6gm-w.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@the_roaming_platypus?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> timJ </a>在<a class="ae lh" href="https://unsplash.com/s/photos/theory?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="45d8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这个知识时代，数据就是一切。它或隐或显地驱动着我们的日常活动。在一个典型的数据科学项目中，数据收集和数据清理约占总工作的 80%。本教程和后续教程将集中在使用 Scrapy 通过网络搜集数据。Scrapy 是一个用于抓取网站和提取结构化数据的应用程序框架，可用于广泛的有用应用程序，如数据挖掘、信息处理或历史档案。</p><p id="f26b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Scrapy 有许多优点，其中一些是:</p><ul class=""><li id="1184" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">比其他网页抓取工具快 20 倍</li><li id="ac8c" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">最适合开发复杂的网络爬虫和抓取工具</li><li id="32bb" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">消耗更少的 RAM 并使用最少的 CPU 资源</li></ul><p id="ab73" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管有其优势，Scrapy 有一个陡峭的学习曲线和不适合初学者的名声。但是，一旦掌握了，它将成为网络抓取的首选工具。这篇教程是我的一点小小的尝试，让它对初学者更友好。我的目标是让你理解 Scrapy 的工作方式，并让你有信心使用 Python 作为编程语言来使用 Scrapy。要自信地使用 Scrapy，首先必须了解它是如何工作的。</p><p id="cb2d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是关于使用 Scrapy 和 Selenium 进行网络抓取的 4 部分教程系列的第一部分。其他部分可在以下网址找到</p><p id="82d8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/web-scraping-with-scrapy-practical-understanding-2fbdae337a3b">第二部分:用 Scrapy 进行网页抓取:实用理解</a></p><p id="5c62" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/web-scraping-with-selenium-d7b6d8d3265a">第 3 部分:用硒刮网</a></p><p id="d33c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://medium.com/swlh/web-scraping-with-selenium-scrapy-9d9c2e9d83b1" rel="noopener">第 4 部分:硒刮网&amp;刮网</a></p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="b05d" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">数据流概述</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/09760d7c516f9b75125c124b27942460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4pVKOGNE8MGgZbTgvDhKIQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">杂乱的数据流(来源:<a class="ae lh" href="https://docs.scrapy.org/en/latest/topics/architecture.html" rel="noopener ugc nofollow" target="_blank">https://docs.scrapy.org/en/latest/topics/architecture.html</a></p></figure><p id="32d9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上图清晰简明地概述了 Scrapy 的工作。让我尽可能清晰简单地解释这些步骤。</p><ol class=""><li id="e3d9" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md ns mk ml mm bi translated">Scrapy 的主要<em class="nt">工具</em>蜘蛛向 Scrapy 引擎发送请求。该引擎负责控制框架所有组件之间的数据流，并在特定操作发生时触发事件。这些初始请求启动了抓取过程。</li><li id="e31d" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ns mk ml mm bi translated">引擎将请求发送给<strong class="lk jd">调度器</strong>，调度器负责收集和调度蜘蛛发出的请求。您可能会问，“为什么需要调度程序？刮难道不是一个直截了当的过程吗？”。这些问题将在下一节中回答。让我们继续工作流程。</li><li id="1fb8" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ns mk ml mm bi translated">调度器将请求分派给引擎进行进一步处理。</li><li id="51f4" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ns mk ml mm bi translated">这些请求通过下载器中间件发送到<strong class="lk jd">下载器</strong>(在图中由引擎和下载器之间的深蓝色条表示)。关于<a class="ae lh" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-downloader-middleware" rel="noopener ugc nofollow" target="_blank">下载器中间件</a>的更多细节，请参考 Scrapy 文档。</li><li id="6de6" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ns mk ml mm bi translated">Downloader 然后下载请求的网页，生成响应，并将其发送回引擎。</li><li id="5f4d" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ns mk ml mm bi translated">引擎通过蜘蛛中间件将响应从下载器发送到生成请求的相应蜘蛛(在图中由引擎和蜘蛛之间的深蓝色条表示)。你可以在 Scrapy 文档中了解更多关于<a class="ae lh" href="https://docs.scrapy.org/en/latest/topics/spider-middleware.html#topics-spider-middleware" rel="noopener ugc nofollow" target="_blank">蜘蛛中间件</a>的信息。</li><li id="dd6f" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ns mk ml mm bi translated">Spider 通过提取所需的项来处理收到的响应，如果需要，从该响应生成进一步的请求，并将请求发送到引擎。</li><li id="4098" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ns mk ml mm bi translated">引擎将提取的项目发送到项目管道，以供进一步处理或存储。请点击关于<a class="ae lh" href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline" rel="noopener ugc nofollow" target="_blank">项目管道</a>的链接了解更多信息。引擎还将生成的请求发送给调度程序，并要求将下一个请求发送给下载器。</li><li id="32e5" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md ns mk ml mm bi translated">重复上述步骤，直到调度器不再有请求可用。</li></ol><h2 id="6541" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">为什么需要调度程序？</h2><p id="98ee" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">Scrapy 遵循异步处理，即请求进程不等待响应，而是继续执行进一步的任务。一旦响应到达，请求进程就开始操作响应。Scrapy 中的蜘蛛也以同样的方式工作。它们向引擎发出请求，这些请求又被发送到调度程序。可以有任意数量的蜘蛛，每个蜘蛛发送<em class="nt"> n </em>个请求(当然有一个条件。您的硬件的处理能力是极限)。为了处理这些请求，调度程序使用了队列。它不断向队列中添加新请求，并在引擎请求时从队列中调度请求。既然您已经知道需要一个调度器，那么让我来详细说明 Scrapy 是如何处理请求和响应的。</p><h2 id="d0f8" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">处理单个请求和响应</h2><p id="a879" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">在 Scrapy 中，提出请求是一个简单的过程。要生成请求，您需要想要从中提取有用数据的网页的 URL。你还需要一个<em class="nt">回调函数</em>。当有对请求的响应时，回调函数被调用。这些回调函数使 Scrapy 异步工作。因此，要发出请求，您需要:网页的 URL 和处理响应的回调函数。为了让您更好地理解，我将使用几行代码。典型的 Scrapy 请求如下所示。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="51c1" class="nu na it ol b gy op oq l or os">scrapy.Request(url="abc.com/page/1", callback=self.parse_page)</span></pre><p id="552c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，<code class="fe ot ou ov ol b">url</code>是要抓取的网页的地址，下载网页后的响应会发送给<code class="fe ot ou ov ol b">parse_page()</code>回调函数，参数是传递的<code class="fe ot ou ov ol b">response</code>，如下图所示。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="6793" class="nu na it ol b gy op oq l or os">def parse_page(self, response):<br/>    # Do your data extraction processes with the response</span></pre><p id="35cd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以使用 XPath 或 CSS 选择器从响应中提取所需的数据。这个提取过程将在后面的部分解释。蜘蛛可以发出任意数量的请求。最需要记住的是<strong class="lk jd"> <em class="nt">每个请求必须有一个对应的回调函数。</em>T12】</strong></p><h2 id="66b6" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">多请求和响应处理</h2><p id="67ac" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">从上一节中，您可能已经理解了如何处理一个请求及其响应。但是，在典型的 web 抓取场景中，会有多个请求。我会尽可能简单地让你理解它的工作原理。让我继续上一节的请求部分。即使这是一个请求，它仍然会被发送到调度程序。根据文档，使用 python 的<code class="fe ot ou ov ol b">yield</code>关键字将请求创建为<em class="nt"> iterables </em>。因此，用 python 的术语来说，请求是使用 python 生成器创建的。产生一个请求会触发引擎将它发送给调度程序，其工作原理前面已经解释过了。类似地，蜘蛛可以使用<code class="fe ot ou ov ol b">yield</code>发出多个请求。下面是一个例子。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="178b" class="nu na it ol b gy op oq l or os">def make_requests(self, urls):</span><span id="53f4" class="nu na it ol b gy ow oq l or os">    for url in urls:<br/>        yield scrapy.Request(url=url, callback=self.parse_url)</span></pre><p id="885e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上面的代码片段中，让我们假设<code class="fe ot ou ov ol b">urls</code>中有 10 个 URL 需要废弃。我们的<code class="fe ot ou ov ol b">make_requests()</code>将向调度程序产生 10 个<code class="fe ot ou ov ol b">scrapy.Request()</code>对象。一旦对其中一个请求有了响应，<code class="fe ot ou ov ol b">parse_url()</code>回调函数就会被调用。现在让我们深入了解响应处理的工作原理。像请求一样，回调函数也必须<code class="fe ot ou ov ol b">yield</code>它从响应中提取的项目。让我用上面代码片段中的回调函数<code class="fe ot ou ov ol b">parse_url()</code>来解释一下。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="6d30" class="nu na it ol b gy op oq l or os">def parse_url(self, response):<br/>    <br/>    item_name = # extract item name from response using XPath or CSS selector<br/>    item_price = # extract item price from response using XPath or CSS selector</span><span id="4895" class="nu na it ol b gy ow oq l or os"># yields a dictionary containing item's name and price<br/>    yield {<br/>        'name': name,<br/>        'price': price,<br/>    }</span></pre><p id="ea7d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">提取的项目可以在需要时使用，也可以存储以供持久使用。回调函数可以返回可迭代的请求、字典、项目对象、数据类对象、属性对象或什么都不返回。欲知详情，请点击<a class="ae lh" href="https://docs.scrapy.org/en/latest/topics/items.html#topics-items" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="212f" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">Scrapy 的安装和基本操作</h1><p id="9355" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">我希望你对 Scrapy 的工作原理有一个基本的了解，现在是你开始使用它的时候了。但首先，你需要安装 Scrapy。</p><h2 id="4f4f" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">安装刮刀</h2><p id="85d0" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">Scrapy 可以通过 anaconda 或 pip 安装。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="8750" class="nu na it ol b gy op oq l or os">conda install -c conda-forge scrapy</span></pre><p id="f698" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">或者</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="50cf" class="nu na it ol b gy op oq l or os">pip install Scrapy</span></pre><p id="4cf8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于在其他操作系统上安装或任何其他安装查询，请点击<a class="ae lh" href="https://docs.scrapy.org/en/latest/intro/install.html" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><h2 id="4eee" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">创建新项目</h2><p id="5094" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">现在你已经安装了 Scrapy，让我们创建一个新项目。Scrapy 提供了一个简单的方法来创建新的项目。导航到您想要创建新的 Scrapy 项目的目录，并键入以下命令。我已经将这个项目命名为<code class="fe ot ou ov ol b">tutorial</code>。您可以自由命名您的项目。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="3e5e" class="nu na it ol b gy op oq l or os">scrapy startproject tutorial</span></pre><p id="1ea9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这将创建一个名为<code class="fe ot ou ov ol b">tutorial</code>的文件夹，其中包含以下文件和文件夹。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="59af" class="nu na it ol b gy op oq l or os">tutorial/<br/>├── scrapy.cfg<br/>└── tutorial<br/>    ├── __init__.py<br/>    ├── items.py<br/>    ├── middlewares.py<br/>    ├── pipelines.py<br/>    ├── __pycache__<br/>    ├── settings.py<br/>    └── spiders<br/>        ├── __init__.py<br/>        └── __pycache__</span></pre><p id="f333" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我把你的注意力引向<code class="fe ot ou ov ol b">spiders</code>文件夹。这是你创造蜘蛛的地方。要了解每个生成文件的用途，请参考这个<a class="ae lh" href="https://docs.scrapy.org/en/latest/topics/commands.html#default-structure-of-scrapy-projects" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><h2 id="1770" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">创造蜘蛛</h2><p id="f11c" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">Scrapy 再次提供了一条简单的线来创建蜘蛛。下面显示的语法使用您提供的参数为新的蜘蛛创建了一个模板。</p><p id="5d48" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe ot ou ov ol b">scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</code></p><p id="c2b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有 4 种模板可用，即 4 种蜘蛛类型:<code class="fe ot ou ov ol b">basic</code>、<code class="fe ot ou ov ol b">crawl</code>、<code class="fe ot ou ov ol b">csvfeed</code>、<code class="fe ot ou ov ol b">xmlfeed</code>。在本教程中，我们将重点关注<code class="fe ot ou ov ol b">basic</code>和<code class="fe ot ou ov ol b">crawl</code>蜘蛛。<code class="fe ot ou ov ol b">&lt;name&gt;</code>参数被设置为蜘蛛的名称，而<code class="fe ot ou ov ol b">&lt;domain&gt;</code>参数用于生成<code class="fe ot ou ov ol b">allowed_domains</code>和<code class="fe ot ou ov ol b">start_urls</code>蜘蛛属性。这些<code class="fe ot ou ov ol b">&lt;name&gt;</code>和<code class="fe ot ou ov ol b">&lt;domain&gt;</code>参数都是强制性的。</p><h2 id="f734" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">创建一个基本的蜘蛛</h2><p id="f334" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">要为域<code class="fe ot ou ov ol b">example.com</code>创建一个基本的 spider，在 spider 项目的根目录下键入以下内容。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="cab3" class="nu na it ol b gy op oq l or os">scrapy genspider -t basic example_basic_spider example.com</span></pre><p id="b4dc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这将在<code class="fe ot ou ov ol b">spiders</code>文件夹中创建一个文件名为<code class="fe ot ou ov ol b">example_basic_spider.py</code>的基本蜘蛛。这个文件的内容应该是这样的。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="5ecf" class="nu na it ol b gy op oq l or os"># -*- coding: utf-8 -*-<br/>import scrapy</span><span id="030b" class="nu na it ol b gy ow oq l or os">class ExampleBasicSpiderSpider(scrapy.Spider):<br/>    name = 'example_basic_spider'<br/>    allowed_domains = ['example.com']<br/>    start_urls = ['http://example.com/']</span><span id="8cef" class="nu na it ol b gy ow oq l or os">def parse(self, response):<br/>        pass</span></pre><p id="8788" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我解释一下生成的模板的组件。</p><ul class=""><li id="8a8f" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><code class="fe ot ou ov ol b">name</code>:必须填写。Scrapy 通过蜘蛛的名字来识别它们。</li><li id="e0a5" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><code class="fe ot ou ov ol b">allowed_domains</code>:在抓取网页的过程中，你可能会碰到一个完全指向其他某个网站的 URL。这有助于限制蜘蛛爬行到不必要的域。您可以添加任意数量的想要爬网的域。</li><li id="749f" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><code class="fe ot ou ov ol b">start_urls</code>:指定蜘蛛抓取的起始 URL。您可以从多个 URL 开始。</li><li id="c14e" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><code class="fe ot ou ov ol b">parse(self, response)</code>:这是默认的回调函数。您可以编写代码来操作和提取响应中的数据。</li></ul><p id="4b52" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您现在可以自由地用您希望抓取的网页地址修改<code class="fe ot ou ov ol b">start_urls</code>。</p><p id="2724" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="nt">等一下！！您可以看到响应，但是请求是如何生成的呢？(你可能还记得上一节的<code class="fe ot ou ov ol b">scrapy.Request()</code>)。当 Scrapy 看到<code class="fe ot ou ov ol b">start_urls</code>时，它会使用<code class="fe ot ou ov ol b">start_urls</code>中的 URL 自动生成<code class="fe ot ou ov ol b">scrapy.Request()</code>，并将<code class="fe ot ou ov ol b">parse()</code>作为回调函数。如果您不希望 Scrapy 自动生成请求，您必须使用<code class="fe ot ou ov ol b">start_requests()</code>功能来生成请求。我将修改为基本蜘蛛生成的相同代码来说明<code class="fe ot ou ov ol b">start_requests()</code>。</em></strong></p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="1788" class="nu na it ol b gy op oq l or os"># -*- coding: utf-8 -*-<br/>import scrapy</span><span id="f746" class="nu na it ol b gy ow oq l or os">class ExampleBasicSpiderSpider(scrapy.Spider):<br/>    """<br/>    Modified Basic Spider to make use of start_requests()<br/>    """<br/>    name = 'example_basic_spider'<br/>    allowed_domains = ['example.com']</span><span id="c0ca" class="nu na it ol b gy ow oq l or os">def start_requests(self):<br/>        urls = ['http://example.com/']<br/>        for url in urls:<br/>            yield scrapy.Request(url=url, callback=self.parse)</span><span id="397f" class="nu na it ol b gy ow oq l or os">def parse(self, response):<br/>        pass</span></pre><p id="bb6f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，Scrapy 生成的代码只是一个模板。它并不要求你遵守它。你可以随意改变它。你可以定义你的回调函数，但是记住在发出请求的时候使用它。</p><h2 id="655c" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">创建爬行蜘蛛</h2><p id="4d1a" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">既然我们已经完成了基本的蜘蛛，让我们继续学习爬行蜘蛛。要创建爬行蜘蛛，请在蜘蛛项目的根目录下键入以下命令。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="0b2d" class="nu na it ol b gy op oq l or os">scrapy genspider -t crawl example_crawl_spider example.com</span></pre><p id="3769" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这将在<code class="fe ot ou ov ol b">spiders</code>文件夹中创建一个文件名为<code class="fe ot ou ov ol b">example_crawl_spider.py</code>的爬行蜘蛛。内容应该是这样的。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="9219" class="nu na it ol b gy op oq l or os"># -*- coding: utf-8 -*-<br/>import scrapy<br/>from scrapy.linkextractors import LinkExtractor<br/>from scrapy.spiders import CrawlSpider, Rule</span><span id="3e77" class="nu na it ol b gy ow oq l or os">class ExampleCrawlSpiderSpider(CrawlSpider):<br/>    name = 'example_crawl_spider'<br/>    allowed_domains = ['example.com']<br/>    start_urls = ['http://example.com/']</span><span id="24d4" class="nu na it ol b gy ow oq l or os">rules = (<br/>        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),<br/>    )</span><span id="13b2" class="nu na it ol b gy ow oq l or os">def parse_item(self, response):<br/>        item = {}<br/>        #item['domain_id'] = response.xpath('//input[@id="sid"]/@value').get()<br/>        #item['name'] = response.xpath('//div[@id="name"]').get()<br/>        #item['description'] = response.xpath('//div[@id="description"]').get()<br/>        return item</span></pre><p id="0db8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可能会奇怪看到这么多不同于基本的蜘蛛。爬行蜘蛛是一种特殊的基本蜘蛛，它提供内置的方式从<code class="fe ot ou ov ol b">start_urls</code>中抓取网页，而基本蜘蛛没有这个功能。</p><p id="7ed2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可能已经注意到了主要的区别:<code class="fe ot ou ov ol b">rules</code>。规则定义了抓取网站的特定行为。上述代码中的规则由 3 个参数组成:</p><ul class=""><li id="5b8e" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><code class="fe ot ou ov ol b">LinkExtractor(allow=r'Items/')</code>:这是爬行蜘蛛最重要的方面。LinkExtractor 提取正在抓取的网页上的所有链接，并且只允许那些遵循由<code class="fe ot ou ov ol b">allow</code>参数给出的模式的链接。在这种情况下，它提取以“Items/”开头的链接(<code class="fe ot ou ov ol b">start_urls</code> + <code class="fe ot ou ov ol b">allow</code>，即“http://example.com/Items/”)，并使用回调函数<code class="fe ot ou ov ol b">parse_item</code>为这些提取的链接生成一个请求</li><li id="d7f3" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><code class="fe ot ou ov ol b">callback='parse_item'</code>:为 LinkExtractor 提取的链接生成的请求的回调函数。</li><li id="1d31" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><code class="fe ot ou ov ol b">follow=True</code>:指定蜘蛛是否应该跟随提取的链接。如果设置为<code class="fe ot ou ov ol b">False</code>，蜘蛛将不会爬行，仅通过<code class="fe ot ou ov ol b">start_urls</code>停止。</li></ul><p id="8e05" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你想了解更多的规则，你可以点击<a class="ae lh" href="https://docs.scrapy.org/en/latest/topics/spiders.html#crawling-rules" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="bc7c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"><em class="nt">side note:</em></strong><em class="nt">可以修改基本蜘蛛跨 URL 爬行。这将在本教程的</em> <a class="ae lh" rel="noopener" target="_blank" href="/web-scraping-with-scrapy-practical-understanding-2fbdae337a3b"> <strong class="lk jd"> <em class="nt">第二部分</em> </strong> </a> <em class="nt">中举例说明。</em></p><h2 id="326e" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">从响应中提取感兴趣的项目</h2><p id="9fab" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">有两种方法可以从响应(即下载的网页)中提取感兴趣的项目/数据。</p><ul class=""><li id="f7b8" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">使用 CSS</li><li id="90dd" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">使用 XPath</li></ul><p id="009a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">响应将保持不变，区别只是从中提取数据的方式不同。在这里的例子中，我将使用 XPath 提取感兴趣的项目。选择 XPath 而不是 CSS 的原因是它们提供了更多的能力和功能。我强烈建议您使用 XPath 浏览一下关于<a class="ae lh" href="https://docs.scrapy.org/en/latest/intro/tutorial.html#xpath-a-brief-intro" rel="noopener ugc nofollow" target="_blank">的零碎文档。这里还有一个关于 XPath 的优秀教程</a><a class="ae lh" href="http://zvon.org/comp/r/tut-XPath_1.html" rel="noopener ugc nofollow" target="_blank"/>。<br/>我将解释示例中使用的几个 XPath 表达式。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/cdaf1acddfa54c935fcedd882b842410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Guaafpl4VAskgYFhEQklw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">示例 XPath 表达式(图片由作者提供)</p></figure><h2 id="370c" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">使用浏览器识别感兴趣项目的标签</h2><p id="0d66" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">打开包含您想要提取的数据的网页。右键点击网页上的任意位置，选择“检查”(Chrome)或“检查元素”(Firefox)。它将打开一个新的面板，显示网页的原始 HTML 版本。当您在 raw 版本上滚动时，网页上相应的元素会高亮显示。向上/向下滚动到要提取的项目，选择该项目，右键单击它，选择“复制”，然后从“新建”菜单中选择“XPath”。如果将复制的 XPath 粘贴到记事本或任何文本编辑器中，您将看到从根到当前项的完整 XPath。现在您已经有了感兴趣的项目的 XPath。您可以使用这个路径作为<code class="fe ot ou ov ol b">response.xpath()</code>的参数并提取值。</p><h2 id="cf75" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">使用 XPath 从感兴趣的项目中获取值</h2><p id="da27" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">使用 XPath 从感兴趣的项目中获取值有两种方法。</p><p id="18ae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe ot ou ov ol b">get()</code></p><ul class=""><li id="e01b" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">以字符串<em class="nt">的形式返回当前或第一个匹配标签/项目的值</em></li><li id="f80c" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">示例:<code class="fe ot ou ov ol b">response.xpath('//div[@id="name"]').get()</code>使用属性“id="name "”返回“div”标记内的值</li></ul><p id="aa1f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe ot ou ov ol b">getall()</code></p><ul class=""><li id="549b" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">以列表<em class="nt">的形式返回匹配标签/项目的所有值</em></li><li id="a2b7" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">示例:<code class="fe ot ou ov ol b">response.xpath('//div[@id="name"]').getall()</code>返回一个列表，该列表包含属性为“id="name "”的所有“div”标记的值</li></ul><h2 id="8c82" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">追踪蜘蛛</h2><p id="3cc7" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">如果你不能运行蜘蛛，所有这些工作都将是浪费，不是吗？不要担心。运行/执行蜘蛛只需要一行命令。你需要做的就是遵循这个语法:<code class="fe ot ou ov ol b">scrapy crawl &lt;spider_name&gt;</code>。</p><p id="fd26" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我试着运行我们刚刚创建的两个示例蜘蛛。<br/> <code class="fe ot ou ov ol b">scrapy crawl example_basic_spider</code> <br/> <code class="fe ot ou ov ol b">scrapy crawl example_crawl_spider</code></p><p id="513c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当您运行蜘蛛时，如果一切正常，没有错误或异常，所有提取的数据将被转储到终端或控制台。要存储这些提取的数据，你需要做的就是给<code class="fe ot ou ov ol b">scrapy crawl</code>命令添加一个选项。</p><p id="20c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">语法:<code class="fe ot ou ov ol b">scrapy crawl &lt;spider_name&gt; -o &lt;output_file&gt;</code></p><p id="4b7d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Scrapy 可以以 JSON、CSV、XML 和 Pickle 格式存储输出。Scrapy 还支持更多存储输出的方式。你可以跟随这个<a class="ae lh" href="https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-exports" rel="noopener ugc nofollow" target="_blank">链接</a>了解更多。</p><p id="7bf4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我用输出文件重新运行示例蜘蛛。<br/> <code class="fe ot ou ov ol b">scrapy crawl example_basic_spider -o output.json</code> <br/> <code class="fe ot ou ov ol b">scrapy crawl example_crawl_spider -o output.csv</code></p><p id="7880" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在典型的真实场景中，您可能需要使用许多蜘蛛来实现特定的目的。当你有很多蜘蛛时，你可能很难记住所有蜘蛛的名字。Scrapy 来救你了。它有一个命令来列出一个项目中所有可用的蜘蛛。</p><p id="79d5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">语法:<code class="fe ot ou ov ol b">scrapy list</code></p><p id="f612" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="nt">旁注</em> </strong> <em class="nt"> : Scrapy 有全局命令和项目专用命令。您可以参考这个</em> <a class="ae lh" href="https://docs.scrapy.org/en/latest/topics/commands.html#available-tool-commands" rel="noopener ugc nofollow" target="_blank"> <em class="nt">链接</em> </a> <em class="nt">来了解这些命令及其功能。</em></p><h2 id="9fec" class="nu na it bd nb nv nw dn nf nx ny dp nj lr nz oa nl lv ob oc nn lz od oe np iz bi translated">粗糙的外壳</h2><p id="7744" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">你已经学会了如何创建、提取数据和运行蜘蛛。但是如何得到满足您需要的正确的 XPath 表达式呢？您不能对 XPath 表达式的每一次试验和调整都运行整个项目。为此，Scrapy 为我们提供了一个交互式 shell，您可以在其中摆弄 XPath 表达式，直到对提取的数据感到满意为止。下面是调用交互式 shell 的语法。</p><p id="550c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">刺儿壳:<code class="fe ot ou ov ol b">scrapy shell &lt;url to scrape&gt;</code></p><p id="a57f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦 Scrapy 下载了与提供的 URL 相关的网页，您将看到一个带有<code class="fe ot ou ov ol b">In [1]:</code>的新终端提示。您可以开始测试您的 XPath 表达式或 CSS 表达式，无论您喜欢哪个，通过键入您的表达式与<code class="fe ot ou ov ol b">response</code>如下所示。</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="4520" class="nu na it ol b gy op oq l or os">scrapy shell <a class="ae lh" href="https://docs.scrapy.org/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">https://docs.scrapy.org/en/latest/index.html</a></span><span id="d116" class="nu na it ol b gy ow oq l or os">...</span><span id="33aa" class="nu na it ol b gy ow oq l or os">In [1]: response.xpath('//div[@id="scrapy-version-documentation"]/h1/text()').get()<br/>Out[1]: 'Scrapy 2.2 documentation'</span></pre><p id="6d49" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以在这个交互式 shell 中试验 XPath 或 CSS 表达式。要脱离这个 shell，只需像在 python 交互式 shell 中一样键入<code class="fe ot ou ov ol b">exit()</code>。我建议你首先利用这个 shell 来设计你的表达式，然后开始你的项目。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="bf3d" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">结束语</h1><p id="6027" class="pw-post-body-paragraph li lj it lk b ll of kd ln lo og kg lq lr oh lt lu lv oi lx ly lz oj mb mc md im bi translated">至此，本教程的理论部分已经结束。我们先从<a class="ae lh" rel="noopener" target="_blank" href="/web-scraping-with-scrapy-practical-understanding-2fbdae337a3b"> <strong class="lk jd"> <em class="nt">中的实际例子开始下一部分</em> </strong> </a>。</p><p id="8b7e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在那之前，祝你好运。保持安全和快乐的学习。！</p></div></div>    
</body>
</html>