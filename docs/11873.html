<html>
<head>
<title>Quora Question Pairs: Detecting Text Similarity using Siamese networks.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Quora 问题对:使用暹罗网络检测文本相似性。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quora-question-pairs-detecting-text-similarity-using-siamese-networks-a370f039731b?source=collection_archive---------17-----------------------#2020-08-17">https://towardsdatascience.com/quora-question-pairs-detecting-text-similarity-using-siamese-networks-a370f039731b?source=collection_archive---------17-----------------------#2020-08-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2b56" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Quora 相似问题:使用暹罗网络检测文本相似性。</h2></div><p id="e1a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> E </span> ver 想知道如何使用深度学习计算文本相似度？我们的目标是开发一个模型来检测文本之间的文本相似性。我们将使用<a class="ae lk" href="https://www.kaggle.com/c/quora-question-pairs/data" rel="noopener ugc nofollow" target="_blank"> Quora 问题对数据集。</a></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/248a6890806dacafa7ac39b0d02c4a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*KV6ZENiokZMi02r6B5bZ6Q.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">【https://unsplash.com/photos/askpr0s66Rg T4】</p></figure><h1 id="0539" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">要求</h1><ul class=""><li id="fbb1" class="mt mu iq kh b ki mv kl mw ko mx ks my kw mz la na nb nc nd bi translated">Python 3.8</li><li id="c064" class="mt mu iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">sci kit-学习</li><li id="6ef1" class="mt mu iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">张量流</li><li id="cd94" class="mt mu iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">天才论</li><li id="0d54" class="mt mu iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">NLTK</li></ul><h1 id="9993" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">资料组</h1><p id="4673" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi lb translated">让我们首先从探索数据集开始。我们的数据集包括:</p><ul class=""><li id="2250" class="mt mu iq kh b ki kj kl km ko nm ks nn kw no la na nb nc nd bi translated"><strong class="kh ir"> id: </strong>一对训练集的 id</li><li id="2287" class="mt mu iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated"><strong class="kh ir"> qid1，qid2 </strong>:问题的唯一 id</li><li id="db6e" class="mt mu iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated"><strong class="kh ir">问题 1 </strong>:问题 1 的文本</li><li id="4d98" class="mt mu iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated"><strong class="kh ir">问题 2 </strong>:问题 2 的文本</li><li id="3fb5" class="mt mu iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated"><strong class="kh ir">is _ duplicate</strong>:<strong class="kh ir">1</strong>如果问题 1 和问题 2 含义相同，否则<strong class="kh ir"> 0 </strong></li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi np"><img src="../Images/dec861214332d80d7bcd7bd42e2279d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WHG9ar1tOUBLtG8ghI6NKw.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">我们数据集的预览</p></figure><h1 id="8c1f" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">数据预处理</h1><p id="6927" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi lb translated">像任何机器学习项目一样，我们将从预处理数据开始。让我们首先加载数据，并将问题 1 和问题 2 组合起来形成词汇表。</p><pre class="lm ln lo lp gt nq nr ns nt aw nu bi"><span id="7f88" class="nv mc iq nr b gy nw nx l ny nz">def load_data(df):<br/>    question1 = df['"question1"'].astype(str).values<br/>    question2 = df['"question2"'].astype(str).values<br/>    # combined: to get the tokens<br/>    df['combined'] = df['"question1"'] + df['"question2"']<br/>    labels = df['"is_duplicate"'].values<br/>    return question1, question2, labels</span><span id="a512" class="nv mc iq nr b gy oa nx l ny nz">question1, question2, labels = load_data(df)<br/>question1 = list(question1)<br/>question2 = list(question2)<br/>combined = question1 + question2df.head()</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ob"><img src="../Images/8537a5ea46d6a8ccbb2c188e3c05b77e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q7MiwMwiYJehtOxOZfjmnw.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">我们修改过的数据框</p></figure><p id="b52a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们也将清理文本一点。</p><pre class="lm ln lo lp gt nq nr ns nt aw nu bi"><span id="0bbf" class="nv mc iq nr b gy nw nx l ny nz"><strong class="nr ir"># Remove Non ASCII characters from the dataset.</strong></span><span id="0924" class="nv mc iq nr b gy oa nx l ny nz">def cleanAscii(text):</span><span id="f1a8" class="nv mc iq nr b gy oa nx l ny nz">       return ''.join(i for i in text if ord(i) &lt; 128)</span></pre><h1 id="b08c" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">单词嵌入</h1><blockquote class="oc"><p id="05e3" class="od oe iq bd of og oh oi oj ok ol la dk translated">每个前馈神经网络都将词汇作为输入，并将它们作为向量嵌入到低维空间，然后通过反向传播进行微调，必然会产生作为第一层权重的单词嵌入，这通常被称为<em class="om">嵌入层(Ruder，2016) </em></p></blockquote><figure class="oo op oq or os lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi on"><img src="../Images/d7ed7a12c22bc5c080fccb2bd4366443.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VLLeJud7aavS3nRG.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">图片来自[3]</p></figure><p id="ad28" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> W </span> ord embedding 学习文本的句法和语义方面(Almeida 等人，2019)。由于我们的问题与文本的语义相关，我们将使用单词嵌入作为我们的暹罗网络的第一层。</p><p id="b178" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我们将使用<strong class="kh ir">流行的 GloVe(单词表示的全局向量)嵌入模型</strong>。我们将获得预训练的模型(<a class="ae lk" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a>)并将其作为我们的第一层作为嵌入层加载。</p><p id="db48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于手套的最近邻方法(或余弦相似度),它能够捕获语义相似的单词。在我们的模型中，我们将使用使用手套权重开发的嵌入矩阵，并为我们的每个句子获取单词向量。</p><p id="066d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们用所有的词汇构建一个标记器。</p><pre class="lm ln lo lp gt nq nr ns nt aw nu bi"><span id="784c" class="nv mc iq nr b gy nw nx l ny nz">max_words = 10000<br/>tok = Tokenizer(num_words=max_words, oov_token="&lt;OOV&gt;")<br/>tok.fit_on_texts(combined)</span><span id="3c47" class="nv mc iq nr b gy oa nx l ny nz"># Padding sequences to a max embedding length of 100 dim and max len of the sequence to 300</span><span id="1d6d" class="nv mc iq nr b gy oa nx l ny nz">sequences = tok.texts_to_sequences(combined)sequences = pad_sequences(sequences, maxlen=300, padding='post')</span></pre><p id="9cc6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在假设，我们已经从<a class="ae lk" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">这里</a>下载了手套预训练向量，我们用嵌入矩阵初始化我们的嵌入层。</p><pre class="lm ln lo lp gt nq nr ns nt aw nu bi"><span id="8da7" class="nv mc iq nr b gy nw nx l ny nz">max_words = 10000<br/>word_index = len(tok.word_index) + 1<br/>glove_dir = ''<br/>embeddings_index = {}<br/>f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))</span><span id="c389" class="nv mc iq nr b gy oa nx l ny nz">for line in f:</span><span id="5902" class="nv mc iq nr b gy oa nx l ny nz">values = line.split()</span><span id="43e5" class="nv mc iq nr b gy oa nx l ny nz">word = values[0]</span><span id="a3d1" class="nv mc iq nr b gy oa nx l ny nz">coefs = np.asarray(values[1:], dtype='float32')</span><span id="2e70" class="nv mc iq nr b gy oa nx l ny nz">embeddings_index[word] = coefs</span><span id="7009" class="nv mc iq nr b gy oa nx l ny nz">f.close()</span><span id="24e0" class="nv mc iq nr b gy oa nx l ny nz">print('Found %s word vectors.' % len(embeddings_index))</span><span id="9b03" class="nv mc iq nr b gy oa nx l ny nz">print(word_index)</span><span id="712b" class="nv mc iq nr b gy oa nx l ny nz"># matrix</span><span id="9588" class="nv mc iq nr b gy oa nx l ny nz">embedding_dim = 100</span><span id="ee26" class="nv mc iq nr b gy oa nx l ny nz">embedding_matrix = np.zeros((max_words, embedding_dim))</span><span id="440d" class="nv mc iq nr b gy oa nx l ny nz">for word, i in tok.word_index.items():</span><span id="cb85" class="nv mc iq nr b gy oa nx l ny nz">if i &lt; max_words:</span><span id="c74b" class="nv mc iq nr b gy oa nx l ny nz">embedding_vector = embeddings_index.get(word)</span><span id="75c3" class="nv mc iq nr b gy oa nx l ny nz">if embedding_vector is not None:</span><span id="d78f" class="nv mc iq nr b gy oa nx l ny nz">embedding_matrix[i] = embedding_vector</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/8c0640e89ed2f2d221004870c72451ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*hQrAlsta3VszQXfaDw9zmg.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">输出的屏幕截图</p></figure><h1 id="131c" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">模型</h1><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/9456aa73df425fae8b17b6d4f58617df.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*bqynOJ5Phbn8KO8MJUgn7A.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">图片来自[4]</p></figure><p id="dd76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">现在我们已经创建了我们的嵌入矩阵，我们也不会开始建立我们的模型。</span></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ov"><img src="../Images/6b7d929d2f4c315ae867b1477de2729d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cWszhqRl7MFbYZ-1GdsyJw.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">模型摘要</p></figure><pre class="lm ln lo lp gt nq nr ns nt aw nu bi"><span id="e257" class="nv mc iq nr b gy nw nx l ny nz">lstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2))</span><span id="c20f" class="nv mc iq nr b gy oa nx l ny nz"># loading our matrix<br/>emb = tf.keras.layers.Embedding(max_words, embedding_dim, input_length=300, weights=[embedding_matrix],trainable=False)</span><span id="7f33" class="nv mc iq nr b gy oa nx l ny nz">input1 = tf.keras.Input(shape=(300,))<br/>e1 = emb(input1)<br/>x1 = lstm_layer(e1)</span><span id="2e69" class="nv mc iq nr b gy oa nx l ny nz">input2 = tf.keras.Input(shape=(300,))<br/>e2 = emb(input2)<br/>x2 = lstm_layer(e2)</span><span id="dc22" class="nv mc iq nr b gy oa nx l ny nz">mhd = lambda x: tf.keras.backend.abs(x[0] - x[1])<br/>merged = tf.keras.layers.Lambda(function=mhd, output_shape=lambda x: x[0],<br/>name='L1_distance')([x1, x2])<br/>preds = tf.keras.layers.Dense(1, activation='sigmoid')(merged)<br/>model = tf.keras.Model(inputs=[input1, input2], outputs=preds)<br/>model.compile(loss='mse', optimizer='adam')<br/></span></pre><p id="35a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">我们</span>使用一个 LSTM 层来编码我们的 100 暗字嵌入。然后，我们计算<strong class="kh ir">曼哈顿距离</strong>(也称为 L1 距离)，然后激活 sigmoid，将输出压缩在 0 和 1 之间。(1 表示最大相似度，0 表示最小相似度)。我们使用 MSE 作为损失函数和 Adam 优化器。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/1c59b0366241408fd7e1523ca5699777.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*LILA1R8YJwDE1OjuGJ0--w.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">我们的模型结构</p></figure><h1 id="6ffe" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">培养</h1><p id="284e" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">我们</span>将我们的 train.csv 分割为训练、测试和验证集，以测试我们的模型。</p><pre class="lm ln lo lp gt nq nr ns nt aw nu bi"><span id="5259" class="nv mc iq nr b gy nw nx l ny nz">def create_data():<br/>    features, labels = df_train.drop(columns=['id', 'qid1', 'qid2', 'is_duplicate']).values, df_train['is_duplicate'].values<br/>    x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)<br/>    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)<br/>    <br/>    return x_train, x_test, y_train, y_test, x_val, y_val</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ox"><img src="../Images/c1171736f7477b694e3f208d6487540c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A6eGWjiLfYE9RLDeGquMKg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">问题 1 和问题 2</p></figure><p id="cc6a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了训练我们的模型，我们简单地调用 fit 函数，后跟输入。</p><pre class="lm ln lo lp gt nq nr ns nt aw nu bi"><span id="8bfa" class="nv mc iq nr b gy nw nx l ny nz">history = model.fit([x_train[:,0], x_train[:,1]], y_train, epochs=100, validation_data=([x_val[:,0], x_val[:,1]], y_val))</span></pre><h1 id="6ffb" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">参考</h1><ul class=""><li id="0bbd" class="mt mu iq kh b ki mv kl mw ko mx ks my kw mz la na nb nc nd bi translated">[1]<a class="ae lk" href="https://arxiv.org/abs/1901.09069" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1901.09069</a></li><li id="9b93" class="mt mu iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">[2]<a class="ae lk" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/pubs/glove.pdf</a></li><li id="d29b" class="mt mu iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated"><a class="ae lk" href="https://ruder.io/word-embeddings-1/" rel="noopener ugc nofollow" target="_blank">https://ruder.io/word-embeddings-1/</a></li><li id="f9df" class="mt mu iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">[4]<a class="ae lk" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12195/12023" rel="noopener ugc nofollow" target="_blank">https://www . aaai . org/OCS/index . PHP/AAAI/aaai 16/paper/download/12195/12023</a></li></ul></div></div>    
</body>
</html>