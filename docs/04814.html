<html>
<head>
<title>Good Grams: How to Find Predictive N-Grams for your Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Good Grams:如何为您的问题找到预测性的N-Grams</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/good-grams-how-to-find-predictive-n-grams-for-your-problem-c04a5f320b39?source=collection_archive---------31-----------------------#2020-04-27">https://towardsdatascience.com/good-grams-how-to-find-predictive-n-grams-for-your-problem-c04a5f320b39?source=collection_archive---------31-----------------------#2020-04-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fb97" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">找出哪些单词对你的问题有预测性是很容易的！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dd6589009725a0b3895ec64c28673882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aVCNljbK3V8Fp8j4DW-dGA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://unsplash.com/photos/CgPPfCOPIWM" rel="noopener ugc nofollow" target="_blank">肯德里克·米尔斯</a></p></figure><h1 id="983e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="d4a5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如今，NLP感觉就像是应用BERT并获得关于你的问题的最先进的结果。很多时候，我发现抓住一些有用的信息词汇也会有所帮助。通常，我会让一个专家来找我，说这五个词对这门课很有预测性。然后我会用这些词作为特征，瞧！你得到了一些性能上的改进或者更多的可解释性。但是如果你没有领域专家，你会怎么做呢？我喜欢尝试的一件简单的事情是在TF-IDF特性上训练一个简单的线性模型，并取前n个单词或n-grams:)。</p><p id="ac0e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇博文中，我们将:</p><ol class=""><li id="e9ab" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">使用SciKit-Learn训练一个简单的模型，获得最丰富的n-gram特征</li><li id="101c" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">然后在具有不同数量特性的模型上运行一些性能比较。</li></ol><p id="e5e0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">希望在本教程结束时，您将拥有一个有趣的新工具来揭示文本分类的良好特性。让我们开始吧。</p><h1 id="2d27" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak"> TLDR </strong></h1><p id="a9d0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">使用SciKit-Learn的TfidfVectorizer上的线性分类器，然后按权重对特征进行排序，取最上面的<code class="fe ng nh ni nj b">n</code>。您还可以使用TfidfVectorizer，通过使用词汇参数来为您的模型提取n元语法的子集。</p><h1 id="424d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">动机</h1><p id="ff5f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对文本进行分类的一个非常成功的方法是寻找与问题相关的预测词或短语。在说电影评论情绪的上下文中，我们可以查找单词“好”、“优秀”、“伟大”或“完美”来找到好的评论，而“坏”、“无聊”或“糟糕”来找到坏的评论。作为好的和坏的电影评论的主题专家，我们很容易想出这些特征。</p><p id="3063" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通常情况下，我不是一个主题专家，对我来说很难确定什么是好的预测词或短语。当这种情况发生时，我已经标记了数据，有一个快速的方法来找到描述性的单词和短语。只需训练一个线性模型，进行权重排序！</p><h1 id="79d0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">训练一个简单模型</h1><p id="4c8d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">SciKit-Learn使得训练线性模型和提取相关权重变得非常容易。让我们来看看在<a class="ae ky" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/version/1" rel="noopener ugc nofollow" target="_blank"> IMDB情感</a>数据集上训练一个模型。</p><pre class="kj kk kl km gt nk nj nl nm aw nn bi"><span id="cad0" class="no la it nj b gy np nq l nr ns">df = pd.read_csv("IMDB_Dataset.csv")<br/>df["split"] = np.random.choice(["train", "val", "test"], df.shape[0], [.7, .15, .15])<br/>x_train = df[df["split"] == "train"]<br/>y_train = x_train["sentiment"]<br/>x_val = df[df["split"] == "val"]<br/>y_val = x_val["sentiment"]</span><span id="c1fc" class="no la it nj b gy nt nq l nr ns">classifier = svm.LinearSVC(C=1.0, class_weight="balanced")<br/>tf_idf = Pipeline([<br/>     ('tfidf', TfidfVectorizer()),<br/>     ("classifier", classifier)<br/> ])</span><span id="f004" class="no la it nj b gy nt nq l nr ns">tf_idf.fit(x_train["review"], y_train)</span></pre><p id="a01e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个模型只需要几秒钟的训练时间，但是只用单字就可以得到相当不错的F值0.88。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1d49a831830795d266e1a91c91442555.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*xBpB_9NIfHyXPN4Gk2umCw.png"/></div></figure><p id="407d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有了这个新模型，我们只需从TF-IDF转换器中获取系数名称，从SVM中获取系数值，就可以找到最具预测性的特征。</p><pre class="kj kk kl km gt nk nj nl nm aw nn bi"><span id="16e4" class="no la it nj b gy np nq l nr ns">coefs = tf_idf.named_steps["classifier"].coef_<br/>if type(coefs) == csr_matrix:<br/>    coefs.toarray().tolist()[0]<br/>else:<br/>    coefs.tolist()</span><span id="05ee" class="no la it nj b gy nt nq l nr ns">feature_names = tf_idf.named_steps["tfidf"].get_feature_names()<br/>coefs_and_features = list(zip(coefs[0], feature_names))</span><span id="4fc5" class="no la it nj b gy nt nq l nr ns"># Most positive features<br/>sorted(coefs_and_features, key=lambda x: x[0], reverse=True)</span><span id="dbe1" class="no la it nj b gy nt nq l nr ns"># Most negative features<br/>sorted(coefs_and_features, key=lambda x: x[0])</span><span id="e506" class="no la it nj b gy nt nq l nr ns"># Most predictive overall<br/>sorted(coefs_and_features, key=lambda x: abs(x[0]), reverse=True)</span></pre><p id="4df5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过访问管道中名为step的“分类器”,我们可以获得模型赋予每个特征的权重。创建管道时，我们命名流程中的每一步，这样我们就可以用这个<code class="fe ng nh ni nj b">named_steps</code>函数来访问它们。大多数SciKit-Learn模型都有一个<code class="fe ng nh ni nj b">.coef_</code>参数，该参数将返回模型的系数，我们可以用它来找到最具预测性的模型。为了方便起见，我对稀疏矩阵做了一些类型检查，因为这些类型的词法特征可能非常非常稀疏。特征名存储在我们管道的<code class="fe ng nh ni nj b">tfidf</code>步骤中，我们以与分类器相同的方式访问它，但是调用<code class="fe ng nh ni nj b">get_feature_names</code>函数。</p><p id="b29d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们的十大积极词汇是:</p><pre class="kj kk kl km gt nk nj nl nm aw nn bi"><span id="13a8" class="no la it nj b gy np nq l nr ns">[(3.482397353551051, 'excellent'),<br/> (3.069350528649819, 'great'),<br/> (2.515865496104781, 'loved'),<br/> (2.470404287610431, 'best'),<br/> (2.4634974085860115, 'amazing'),<br/> (2.421134741115058, 'enjoyable'),<br/> (2.2237089115789166, 'perfect'),<br/> (2.196802503474607, 'fun'),<br/> (2.1811330282241426, 'today'),<br/> (2.1407707555282363, 'highly')]</span></pre><p id="a240" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们的十大负面词汇是:</p><pre class="kj kk kl km gt nk nj nl nm aw nn bi"><span id="c71b" class="no la it nj b gy np nq l nr ns">[(-5.115103657971178, 'worst'),<br/> (-4.486712890495122, 'awful'),<br/> (-3.676776745907702, 'terrible'),<br/> (-3.5051277582046536, 'bad'),<br/> (-3.4949920792779157, 'waste'),<br/> (-3.309000819824398, 'boring'),<br/> (-3.2772982524056973, 'poor'),<br/> (-2.9054813685114307, 'dull'),<br/> (-2.7129398526527253, 'nothing'),<br/> (-2.710497534821449, 'fails')]</span></pre><h1 id="b5af" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">使用我们的“好”功能</h1><p id="09f5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">既然我们已经发现了一些“好”的特征，我们可以构建更简单的模型，或者在类似领域的其他问题中使用这些特征。让我们建立一些简单的规则，如果这些预测词中的任何一个出现在评论中，将返回1，否则返回0。然后仅用这20个特征重新训练模型，看看我们做得如何。</p><p id="7005" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了做到这一点，我创建了一个简单的SciKit-Learn转换器，它将n-grams列表转换为regex规则，NLTK的 tokenizer可以搜索这些规则。不是超级快(那是轻描淡写，真的很慢。您应该使用TfidfVectorizer中的词汇参数，后面会有更多介绍。)但是它很容易阅读并完成工作。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="0848" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这段代码有三个主要部分。</p><p id="8923" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">第11行</strong>将一个表示n-gram的元组(比如(" good "、" movie ")转换成一个regex r "&lt;good&gt;&lt;movie&gt;"，NLTK可以用它在文本中搜索特定的n-gram。它基本上只是一个列表理解，用一个<a class="ae ky" href="https://www.burgaud.com/foldl-foldr-python" rel="noopener ugc nofollow" target="_blank"> foldl </a>将单词连接成一个正则表达式，遍历所有的n元语法。</p><p id="aa7d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">第13–26行</strong>通过遍历输入中的每一个句子，或者在本例中进行回顾，并将每个正则表达式应用于该句子，来执行转换。如果正则表达式找到了什么，它会在列表中与触发的n-gram对应的位置放置一个1。这将产生一个带有1和0的向量，表示哪个n元语法出现在哪个句子中。</p><p id="c88a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">第28–29行</strong>允许我们像以前一样获取相关的特性名称。只是方便而已。</p><p id="c1aa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有了这个新的便捷的转换器，我们可以使用前十个最好的和后十个最差的单词来重新训练我们的模型。</p><pre class="kj kk kl km gt nk nj nl nm aw nn bi"><span id="5402" class="no la it nj b gy np nq l nr ns">n_grams = [('excellent',), ('great',), ('perfect',),<br/>           ('best',), ('brilliant',), ('surprised',),<br/>           ('hilarious',), ('loved',), ('today',),<br/>           ('superb',), ('worst',), ('awful',),<br/>           ('waste',), ('poor',), ('boring',),<br/>           ('bad',), ('disappointment',), ('poorly',),<br/>           ('horrible',), ('bored',)]<br/>classifier = svm.LinearSVC(C=1.0, class_weight="balanced")<br/>rules = Pipeline([<br/>     ('rules', RuleTransformer(n_grams)),<br/>     ("classifier", classifier)<br/> ])</span><span id="f4d8" class="no la it nj b gy nt nq l nr ns">rules.fit(x_train["review"], y_train)</span></pre><p id="6150" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这20个特征使我们的F1减少了大约0.13，这看起来很多，但是我们只使用了最初的65，247个单词的0.03%。这真是太棒了！这20个特征编码了我们数据中的大部分信息，我们可以将它们用作其他管道中的特征！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6a63cc5ef350a73639dba1c60360da16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*N_wkHTrw8Ck8-GsNDYZrzA.png"/></div></figure><h2 id="5b08" class="no la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">用于规则提取的TfidfVectorizer</h2><p id="cbc0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我在上面构建了规则矢量器，但是我们可以通过使用tfidf矢量器并传入一个词汇表参数来获得相同的结果。最初的SciKit-Learn矢量器接受一个名为“词汇”的参数，该参数接受一个将单个单词或由空格分隔的n元语法映射为整数的字典。为了获得相同的效果，我们可以运行:</p><pre class="kj kk kl km gt nk nj nl nm aw nn bi"><span id="05e5" class="no la it nj b gy np nq l nr ns">top_feats = sorted(coefs_and_features,<br/>                   key=lambda x: abs(x[0]),<br/>                   reverse=True)[:20]<br/>vocab = {x[1]: i for i, x in enumerate(top_feats)}<br/>TfidfVectorizer(vocabulary=vocab)</span></pre><p id="45fd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里我们得到了特性的排序列表，然后我们创建了一个从特性名称到整数索引的映射，并将其传递给矢量器。如果你对地图的样子很好奇，大概是这样的:</p><pre class="kj kk kl km gt nk nj nl nm aw nn bi"><span id="1bac" class="no la it nj b gy np nq l nr ns">{"great": 0,<br/> "poor": 1,<br/> "very poor": 2,<br/> "very poor performance": 3}</span></pre><p id="b97f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">n-gram是通过在单词之间添加一个空格来表示的。如果我们使用上面的代码代替我们的RuleTransformer，我们将在很短的时间内得到相同的结果。</p><h2 id="f64a" class="no la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">要取多少特征？</h2><p id="8b88" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这20个字似乎很有力量。他们可以从一开始就为我们提供79 F1，但也许20不是正确的功能数量。我们可以通过对越来越多的顶级功能运行我们的分类器并绘制F1来找出答案。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/50ad577f91f5bbc378431c02ee19e992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t0YDmd-_2FyQytK6czPcug.png"/></div></div></figure><p id="2814" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这向我们表明，在大约13k的最具预测性的词之后，该模型开始收敛到最佳TF-IDF单字性能。因此，我们只需20%的原始功能集就能获得相同的性能！继续使用这些13k功能是一个更有原则的数字，我们仍然可以大幅减少原始功能的数量。</p><h1 id="16c0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="27da" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果我们纯粹着眼于词汇特征、特定单词及其数量，那么这可能是发现有用单词和短语的好方法。语言远比你使用的词语要复杂得多。在设计实际系统时，查看各种信息很重要。使用BERT，使用句法特征，比如句子是如何分析的。语言不仅仅是生字，还有很多东西，但是希望这个小技巧可以帮助你在遇到困难的时候找到一些好的单词。</p></div></div>    
</body>
</html>