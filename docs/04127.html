<html>
<head>
<title>Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recurrent-neural-networks-b7719b362c65?source=collection_archive---------31-----------------------#2020-04-15">https://towardsdatascience.com/recurrent-neural-networks-b7719b362c65?source=collection_archive---------31-----------------------#2020-04-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/4d479fb614cc94ad16cf7f0877600087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MhKCd-eDxltItE4KbId0BA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由<a class="ae jg" href="https://unsplash.com/@sonjalangford?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Sonja Langford </a>在<a class="ae jg" href="https://unsplash.com/s/photos/time-series?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="5a88" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">了解是什么使RNNs在处理序列数据时高效</h2></div><p id="c6b3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">递归神经网络是非常著名的深度学习网络，应用于序列数据:时间序列预测、语音识别、情感分类、机器翻译、命名实体识别等..<br/>对序列数据使用前馈神经网络提出了两个主要问题:</p><ul class=""><li id="5962" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">在不同的例子中，输入和输出可以有不同的长度</li><li id="eb9d" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">MLPs不共享跨数据样本的不同位置学习的特征</li></ul><p id="5c42" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将发现RNNs成功背后的数学原理，以及一些特殊类型的细胞，如LSTMs和GRUs。最后，我们将深入研究结合注意力机制的编码器-解码器架构。</p><p id="0a58" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意:因为Medium不支持LaTeX，所以数学表达式是作为图像插入的。因此，为了更好的阅读体验，我建议你关闭黑暗模式。</p><h1 id="d51d" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">目录</h1><ol class=""><li id="c048" class="lu lv jj la b lb na le nb lh nc ll nd lp ne lt nf ma mb mc bi translated"><strong class="la jk">符号</strong></li><li id="b2dc" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt nf ma mb mc bi translated"><strong class="la jk"> RNN车型</strong></li><li id="09f8" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt nf ma mb mc bi translated"><strong class="la jk">不同类型的无线网络</strong></li><li id="cfdf" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt nf ma mb mc bi translated"><strong class="la jk">高级细胞类型</strong></li><li id="6f9e" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt nf ma mb mc bi translated"><strong class="la jk">编码器&amp;解码器架构</strong></li><li id="61d9" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt nf ma mb mc bi translated"><strong class="la jk">注意机制</strong></li></ol></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="7ca0" class="mi mj jj bd mk ml nn mn mo mp no mr ms kp np kq mu ks nq kt mw kv nr kw my mz bi translated">1.注释</h1><p id="529d" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">作为示例，我们将考虑命名实体识别的任务，该任务包括定位和识别命名实体，例如专有名称:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/b29f72af4d68a7fcfb0b2c6e430f0e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z5uY8wrQmXqq7ZT7y_sWdQ.png"/></div></div></figure><p id="6777" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们表示:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/940bc958c0251c061080f8263ebbee3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4oehrNqujRMKmYFcX67PUQ.png"/></div></div></figure><p id="740f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当处理非数字数据时，例如文本，将它编码成数字向量是非常重要的:这个操作称为<code class="fe ob oc od oe b">embedding</code>。最著名的文本编码方式之一是由谷歌开发的Bert。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="3180" class="mi mj jj bd mk ml nn mn mo mp no mr ms kp np kq mu ks nq kt mw kv nr kw my mz bi translated">2.RNN模型</h1><p id="5184" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">RNNs代表神经网络的一种特殊情况，其中模型的参数以及所执行的操作在整个体系结构中是相同的。网络为序列中的每个元素执行相同的任务，该序列的<code class="fe ob oc od oe b">output depends on the input and the previous state of the memory</code>。<br/>下图显示了具有单层隐藏记忆的神经元的神经网络:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/7103727a22a1ac57c1f621ba8393dac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hmjuv3IrkI7AmyxfJVlI3A.png"/></div></div></figure><h2 id="953d" class="og mj jj bd mk oh oi dn mo oj ok dp ms lh ol om mu ll on oo mw lp op oq my or bi translated">方程式</h2><p id="8ba3" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">架构中的变量包括:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/96010cd583ac120fa92247b3e69ef6b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TiNtkV-bltGvoOsYMIe6kg.png"/></div></div></figure><p id="ebac" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/1cd6be2a5aa0174718909e6609a1d5a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4G38kqoSZEGIqUBsxmAfTg.png"/></div></div></figure><p id="de34" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ot">h(</em>-1)被随机初始化，<em class="ot"> ϕ </em>和<em class="ot"> ψ </em>是非线性函数，<em class="ot"> U </em>、<em class="ot"> V </em>和<em class="ot"> W </em>是各种线性回归的<code class="fe ob oc od oe b">parameters</code>，在非线性激活之前。<br/>需要注意的是，它们在整个架构中<strong class="la jk">是相同的</strong>。</p><h2 id="fe5d" class="og mj jj bd mk oh oi dn mo oj ok dp ms lh ol om mu ll on oo mw lp op oq my or bi translated">应用程序</h2><p id="edf3" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">递归神经网络显著改进了序列模型，特别是:</p><ul class=""><li id="4634" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">NLP任务、建模和文本生成</li><li id="a186" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">翻译机</li><li id="d8f5" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">声音识别</li></ul><p id="23a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在下表中总结了上述应用:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/51dcf20a3c2ddcb108b4d1ba5b765f18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hEa9ciQBUQcN2rtIDoK3CA.png"/></div></div></figure><h2 id="cad8" class="og mj jj bd mk oh oi dn mo oj ok dp ms lh ol om mu ll on oo mw lp op oq my or bi translated">学习算法</h2><p id="93ee" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">如同在经典神经网络中一样，在递归网络的情况下，通过优化关于<em class="ot"> U </em>、<em class="ot"> V </em>和<em class="ot"> W </em>的成本函数来进行学习。换句话说，我们的目标是从真实值<em class="ot"> yi </em>的输入<em class="ot"> xi </em>开始，找到给出最佳预测y^ <em class="ot"> i </em>的最佳参数。</p><p id="7f78" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为此，我们定义了一个名为<code class="fe ob oc od oe b">loss function</code>的目标函数，记为<code class="fe ob oc od oe b">J</code>，它量化了整个训练集的真实值和预测值之间的距离。</p><p id="05e9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们通过以下两个主要步骤来最小化J:</p><ul class=""><li id="91d5" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><code class="fe ob oc od oe b"><strong class="la jk">Forward Propagation</strong></code>:我们通过网络整体或分批传播数据，并计算这批数据的损失函数，该损失函数只不过是不同行的预测输出中的误差之和。</li><li id="37ad" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe ob oc od oe b"><strong class="la jk">Backward Propagation Through Time</strong></code>:包括计算成本函数相对于不同参数的梯度，然后应用下降算法对其进行更新。它被称为BPTT，因为每个输出端的梯度既取决于同一时刻的元素，也取决于前一时刻的记忆状态。</li></ul><p id="94e1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们多次重复相同的过程，称为<code class="fe ob oc od oe b">epoch number</code>。定义架构后，学习算法编写如下:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/832035e1a205cafc4b2d7f5542634fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6cXc95RLcIwlRTt16nEaQ.png"/></div></div></figure><p id="abcd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(∫)成本函数<em class="ot"> L </em>计算单个点上实际值和预测值之间的距离。</p><h2 id="6a72" class="og mj jj bd mk oh oi dn mo oj ok dp ms lh ol om mu ll on oo mw lp op oq my or bi translated">正向传播</h2><p id="aae9" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">让我们考虑通过神经网络预测单个序列的输出。<br/>在每个时刻<em class="ot"> t </em>，我们计算:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/12f2cf92a6876c47330c2144fd86f60b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PfrNuw39s8JXuV6MLmLdHA.png"/></div></div></figure><p id="a025" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">直到到达序列的末尾。<br/>同样，参数<em class="ot"> U </em>、<em class="ot"> W </em>和<em class="ot"> V </em>在整个神经网络中保持<strong class="la jk">相同</strong>。<br/>当处理一个<em class="ot"> m </em>行的数据集时，对每一行分别重复这些操作是非常昂贵的。因此，我们截断数据集，以便在相同的时间线中描述序列，即:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/8861faaf93c9be30c524eedc7351de56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0fPEAcxZ4xpTCm6JHZX44A.png"/></div></div></figure><p id="033b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以使用线性代数将其并行化，如下所示:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/384ae9a38d688d2d27e7057de1c3d814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qe5Ik2xzTUip8ar_fuxFZw.png"/></div></div></figure><h2 id="9fa0" class="og mj jj bd mk oh oi dn mo oj ok dp ms lh ol om mu ll on oo mw lp op oq my or bi translated">穿越时间的反向传播</h2><p id="1383" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">反向传播是学习的第二步，包括在预测(正向)阶段将<code class="fe ob oc od oe b">injecting the error</code>提交到网络中，并将其参数更新为<code class="fe ob oc od oe b">perform better on the next iteration</code>。因此，函数<em class="ot"> J </em>的优化通常通过下降法进行。</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/2d16f0c295bc143f5233627ef352716f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*33Mm9aJ_wQClM14x6nvrSw.png"/></div></div></figure><p id="2d27" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以应用一个下降方法，在我之前的<a class="ae jg" rel="noopener" target="_blank" href="/deep-learnings-mathematics-f52b3c4d2576">文章</a>中有详细描述。</p><h2 id="dfbb" class="og mj jj bd mk oh oi dn mo oj ok dp ms lh ol om mu ll on oo mw lp op oq my or bi translated">记忆问题</h2><p id="0475" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">我们对基于历史预测时间序列的演变感兴趣的领域有几个:音乐、金融、情感…等等。上面描述的内在循环网络被称为“香草”,具有记忆力差的缺点，在预测未来时不能考虑过去的几个因素。</p><p id="178b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">考虑到这一点，RNNs的各种扩展被设计来修整内部记忆:双向神经网络、LSTM细胞、注意力机制等等。记忆放大在某些领域是至关重要的，比如在金融领域，人们试图记住尽可能多的历史，以便预测一个金融序列。</p><p id="6bf3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">RNN的学习阶段也可能遭受<code class="fe ob oc od oe b">gradient vanishing</code>或<code class="fe ob oc od oe b">gradient exploding</code>问题，因为成本函数的梯度包括影响其记忆能力的<em class="ot"> W </em>的幂。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="e004" class="mi mj jj bd mk ml nn mn mo mp no mr ms kp np kq mu ks nq kt mw kv nr kw my mz bi translated">3.不同类型的rnn</h1><p id="57be" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">经典或“普通”递归神经网络有多种扩展，这些扩展旨在增加网络的存储容量以及特征提取能力。<br/>下图总结了不同的扩展:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/9f66503e959eb0455b73aa0e9bfd9454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5oHkoIp1CET7Z1YKVc46qw.png"/></div></div></figure><p id="a0ec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有其他类型的rnn有一个专门设计的隐藏层，我们将在下一章讨论。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="cedc" class="mi mj jj bd mk ml nn mn mo mp no mr ms kp np kq mu ks nq kt mw kv nr kw my mz bi translated">4.高级类型的细胞</h1><h2 id="c71b" class="og mj jj bd mk oh oi dn mo oj ok dp ms lh ol om mu ll on oo mw lp op oq my or bi translated">门控循环单元</h2><p id="b11e" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">GRU(门控循环单元)单元允许循环网络保存更多的历史信息，以便进行更好的预测。它引入了一个<code class="fe ob oc od oe b">update gate</code>，用于确定要从过去保留的信息量，以及一个<code class="fe ob oc od oe b">reset gate</code>，用于设置要忘记的信息量。下图示意了GRU细胞:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/4d2f8d353b520a216a346e976dd28d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*USdpSgHYYKSGqjLCLUCOgA.png"/></div></div></figure><p id="1254" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">方程<br/> </strong>我们将GRU单元中的方程定义如下:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/98e9ecfab4f255881ce2af3474a5eae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2wTdhf06Ae6W66MyKo1hpw.png"/></div></div></figure><p id="789c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ot"> ϕ </em>是非线性整数函数，参数<em class="ot"> W </em>由模型学习。</p><h2 id="75bc" class="og mj jj bd mk oh oi dn mo oj ok dp ms lh ol om mu ll on oo mw lp op oq my or bi translated">长短期记忆</h2><p id="b7bb" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">LSTMs(长短期记忆)也被引入来克服短记忆的问题，它们比普通的RNNs多4倍的记忆。这个模型使用了门的概念，有三个:</p><ul class=""><li id="1285" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">输入门<em class="ot"> i </em>:控制输入信息的流动。</li><li id="6bf8" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">遗忘门<em class="ot"> f </em>:控制前一存储状态的信息量。</li><li id="2db1" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">输出门<em class="ot"> o:控制输出信息流</em></li></ul><p id="99bb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图显示了LSTM电池的操作:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/3f709119a752124282d0395c4463aa71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfKdyeZA9Nk7YG7HFtf2GQ.png"/></div></div></figure><p id="ea34" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当输入和输出门关闭时，存储单元中的激活被阻止。</p><p id="fd73" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">方程式<br/> </strong>我们将LSTM单元中的方程式定义如下:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/f56ef2cf09ea6885a3b00d5be78a8197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ljUk9QYfnWh1Rqxgn0-Iwg.png"/></div></div></figure><p id="d4e5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">优点&amp;缺点<br/> </strong>我们可以把LSTM细胞的优点和缺点总结为四个要点:</p><ul class=""><li id="2024" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la jk">优点</strong></li></ul><p id="8484" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">+他们能够对长期序列依赖性进行建模。<br/>+与“普通”rnn相比，它们对短记忆问题更具鲁棒性，因为内部存储器的定义从:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/1146d4ed30b65eebf70f93077ec5598a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r36yJQBmDcdRbrTizuPWCQ.png"/></div></div></figure><ul class=""><li id="7d61" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la jk">缺点</strong></li></ul><p id="ae4b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">+与RNN相比，它们增加了计算复杂性，引入了更多要学习的参数。<br/>+由于存在多个内存单元，所需内存高于“普通”rnn。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="edec" class="mi mj jj bd mk ml nn mn mo mp no mr ms kp np kq mu ks nq kt mw kv nr kw my mz bi translated">5.编码器和解码器架构</h1><p id="acfd" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">它是由两个主要部分组成的顺序模型:</p><ul class=""><li id="1c3d" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><code class="fe ob oc od oe b">Encoder</code>:模型的第一部分处理序列，然后在最后返回整个序列的编码向量，称为<code class="fe ob oc od oe b">context vector</code>，其概括了不同输入的信息。</li><li id="65c4" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe ob oc od oe b">Decoder</code>:上下文向量然后被作为解码器的输入，以便进行预测。</li></ul><p id="e01e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图说明了该模型的架构:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/bc861c92842d1d7f82cb93721cca4cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WKOkgkYJ53wWB2Xx8u9KDg.png"/></div></div></figure><p id="6178" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">编码器可以认为是一个降维工具，事实上，上下文向量<em class="ot"> en </em>无非是对输入向量的编码( 0中的<em class="ot">，</em> 1中的<em class="ot">，<em class="ot"> inn </em>)，这些向量的大小之和远大于en的大小，因此有降维的概念。</em></p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="fbe5" class="mi mj jj bd mk ml nn mn mo mp no mr ms kp np kq mu ks nq kt mw kv nr kw my mz bi translated">6.注意机制</h1><p id="0534" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">引入注意机制是为了解决记忆限制的问题，主要回答以下两个问题:</p><ul class=""><li id="968c" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">编码器的每个输出<em class="ot"> ej </em>赋予什么权重(重要性)<em class="ot"> αj </em>？</li><li id="f2cb" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">我们如何克服编码器有限的内存，以便能够“记住”更多的编码过程？</li></ul><p id="d707" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该机制将其自身插入编码器和解码器之间，并帮助解码器显著选择对解码过程的每个步骤都很重要的编码输入<em class="ot"> outi </em>如下:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/8dcdafc03b094fa49126d95fd7430fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7drupBQ_5yAm5ktqZpYI7w.png"/></div></div></figure><h2 id="8cfa" class="og mj jj bd mk oh oi dn mo oj ok dp ms lh ol om mu ll on oo mw lp op oq my or bi translated">数学形式主义</h2><p id="6a1c" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">保持与之前相同的符号，我们将<em class="ot"> αi </em>，<em class="ot"> j </em>设置为由输出<em class="ot"> i </em>给予向量<em class="ot"> ej </em>的注意力，表示为<em class="ot"> outi </em>。<br/>注意力通过神经网络来计算，该神经网络将向量(<em class="ot"> e </em> 0、<em class="ot"> e </em> 1、…、<em class="ot"> en </em>)和先前的记忆状态<em class="ot"> h(i- </em> 1)作为输入，它由下式给出:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/4c84360fdfedcb6bf423360aed090fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vAiArIIr2-XuEvbNDZllFg.png"/></div></div></figure><h2 id="e427" class="og mj jj bd mk oh oi dn mo oj ok dp ms lh ol om mu ll on oo mw lp op oq my or bi translated">应用:翻译机</h2><p id="0385" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">注意机制的使用使得<code class="fe ob oc od oe b">visualize and interpret</code>模型内部正在做什么成为可能，特别是在预测的时候。<br/>例如，通过绘制翻译系统注意力矩阵的“热图”,我们可以看到第一种语言中的单词，该模型将每个单词翻译成第二种语言:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/62eb3042cedd74d7d8900fa42bdaaafa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BVSf25EZfCYpOBdH0UZnlQ.png"/></div></div></figure><p id="43fd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如上所述，当将单词翻译成英语时，系统特别关注相应的法语单词。</p><h2 id="95c6" class="og mj jj bd mk oh oi dn mo oj ok dp ms lh ol om mu ll on oo mw lp op oq my or bi translated">LSTM叠加与注意机制</h2><p id="c185" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">结合这两种方法来改善内部记忆是相关的，因为第一种方法允许考虑更多的过去的元素，而第二种方法选择在预测时仔细关注它们。<br/>注意机制的输出<em class="ot"> ct </em>是LSTM细胞的新输入，因此方程组变成如下:</p><figure class="nw nx ny nz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/402079456269a0072635de67cab4703d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vFhp8BdDFs4CNTNM7NtbxQ.png"/></div></div></figure><p id="d98c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ot"> ϕ </em>是非线性整数函数，参数w由模型学习。</p><h1 id="68d7" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">结论</h1><p id="d49e" class="pw-post-body-paragraph ky kz jj la b lb na kk ld le nb kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">RNNs是处理序列数据的一个非常强大的工具，它们提供了令人难以置信的记忆能力，并广泛应用于日常生活中。它们也有许多扩展，能够解决各种类型的数据驱动问题，尤其是讨论时间序列的问题。</p><p id="c7d3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不要犹豫，检查我以前的文章处理:</p><ul class=""><li id="f40e" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><a class="ae jg" href="https://medium.com/p/deep-learnings-mathematics-f52b3c4d2576" rel="noopener">深度学习的数学</a></li><li id="6da3" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae jg" href="https://medium.com/p/convolutional-neural-networks-mathematics-1beb3e6447c0" rel="noopener">卷积神经网络的数学</a></li><li id="e2a5" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae jg" href="https://medium.com/p/object-detection-face-recognition-algorithms-146fec385205" rel="noopener">物体检测&amp;人脸识别算法</a></li></ul></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="9d74" class="mi mj jj bd mk ml nn mn mo mp no mr ms kp np kq mu ks nq kt mw kv nr kw my mz bi translated">参考</h1><ul class=""><li id="fcc3" class="lu lv jj la b lb na le nb lh nc ll nd lp ne lt lz ma mb mc bi translated">Z.Lipton，J.Berkowitz，C.Elkan，<strong class="la jk">对用于序列学习的递归神经网络的评论</strong>，arXiv:156.00019 v4，2015。</li><li id="6772" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">H.Salehinejad，S.Sankar，J.Barfett，E.Colak，S.Valaee，<strong class="la jk">递归神经网络的最新进展</strong>，arXiv: 1801.01078v3，2018。</li><li id="9f69" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">Y.Baveye，C.Chamaret，E . del andréA，L.Chen，<strong class="la jk">情感视频内容分析:多学科洞察</strong>，HAL Id: hal-01489729，2017。</li><li id="9001" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">A.Azzouni，G.Pujolle，<strong class="la jk">一种用于网络流量矩阵预测的长短期记忆递归神经网络框架</strong>，arXiv: 1705.05690v3，2017。</li><li id="8acb" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">Y.g .希纳尔，H .米里萨伊，P .戈斯瓦米，E .高斯希尔，A .艾特-巴奇尔，V .斯特里约夫，<strong class="la jk">利用RNNs进行时间序列预测:一种扩展的注意机制对周期进行建模并处理缺失值</strong>，arXiv: 1703.10089v1，2017。</li><li id="e02b" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">K.徐，吴，王，冯，维特布洛克，谢宁，Graph2Seq: <strong class="la jk">基于注意的神经网络的图到序列学习</strong>，arXiv: 1804.00823v3，2018 .</li><li id="03ea" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">Rose Yu，，Cyrus Shahabi，Ugur Demiryurek，<strong class="la jk">深度学习:极端条件下交通预测的通用方法</strong>，南加州大学，2017年。</li></ul></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="fb78" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ot">原载于2020年4月15日https://www.ismailmebsout.com</em><a class="ae jg" href="https://www.ismailmebsout.com/recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank"><em class="ot"/></a><em class="ot">。</em></p></div></div>    
</body>
</html>