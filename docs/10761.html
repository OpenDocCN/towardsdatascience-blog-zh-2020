<html>
<head>
<title>Playing Doom with AI: Multi-objective optimization with Deep Q-learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用人工智能玩毁灭:深度 Q 学习的多目标优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2?source=collection_archive---------51-----------------------#2020-07-27">https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2?source=collection_archive---------51-----------------------#2020-07-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7719" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Pytorch 中强化学习的实现。</h2></div><h1 id="9796" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">简介</strong></h1><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi la"><img src="../Images/2c00005f424d709a7736fc6c58aa52e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/0*uzPLFK7TLjXMbSGK"/></div></figure><p id="dcab" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> O </span>在线学习方法是一个动态的算法家族，推动了过去十年强化学习的许多最新成就。在线学习方法属于<a class="ae mn" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff" rel="noopener">基于样本的学习</a>类强化学习方法，允许简单地通过重复观察来确定状态值，消除了对显式转换动态的需要。与它们的<a class="ae mn" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed" rel="noopener">离线对应物</a>、<strong class="lk iu">不同，诸如时间差异学习(TD)之类的在线学习方法，允许在代理-环境交互期间状态和动作值的增量更新，允许观察到持续的、增量的性能改进。</strong></p><p id="ba6b" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了 TD，我们还讨论了 Q-learning 的<a class="ae mn" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830" rel="noopener">理论</a>和<a class="ae mn" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">实际实现</a>，Q-learning 是 TD 的一种发展，旨在允许对环境中的状态-动作值进行越来越精确的估计。Q-learning 因成为模拟游戏环境的强化学习方法的支柱而闻名，如在 OpenAI 的健身房中观察到的那些。因为我们已经在<a class="ae mn" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">过去的文章</a>中涉及了 Q-learning 的理论方面，所以这里不再重复。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="mo mp l"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">一个代理扮演我们之前 Tensorflow 实现中的基本场景</p></figure><p id="e5d0" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们的<a class="ae mn" rel="noopener" target="_blank" href="/automating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c">上一篇文章</a>中，我们探索了如何通过使用开源的 OpenAI gym 包装库<a class="ae mn" href="https://github.com/shakenes/vizdoomgym" rel="noopener ugc nofollow" target="_blank"> Vizdoomgym </a>，将 Q-learning 应用于训练代理人在经典 FPS 游戏<strong class="lk iu"> Doom </strong>中扮演一个基本场景。我们将在那篇文章的基础上引入一个更复杂的 Vizdoomgym 场景，并用 Pytorch 构建我们的解决方案。这是研究各种末日 RL 算法系列文章的第一篇，作为我们的基线。</p><h1 id="e9b4" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">实现</strong></h1><p id="edfb" class="pw-post-body-paragraph li lj it lk b ll mu ju ln lo mv jx lq lr mw lt lu lv mx lx ly lz my mb mc md im bi translated">我们将要探索的环境是 Vizdoomgym 的防线场景。环境中，代理在走廊的一端，恶魔在另一端繁殖。环境的一些特征包括:</p><ul class=""><li id="0b6f" class="mz na it lk b ll lm lo lp lr nb lv nc lz nd md ne nf ng nh bi translated">一个 3 的动作空间:开火，左转，右转。不允许扫射。</li><li id="4a31" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">向玩家发射火球的棕色怪物，命中率为 100%。</li><li id="1f20" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">粉红色的怪物试图以曲折的方式靠近来咬玩家。</li><li id="e671" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">重生怪物的生命值明显更高。</li><li id="b87f" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">杀死一个怪物+1 点</li><li id="32b9" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">- 1 代表死亡。</li></ul><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a1e51d8eb69b12063d313ce475f8e7b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*Fg4Wob_g8UXKOM0oJCHLpg.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">防线场景的初始状态。</p></figure><p id="b0fd" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk iu">毫无疑问，在这个环境中的成功需要平衡多个目标</strong>:理想的玩家必须学会优先考虑褐色怪物，它们能够在产卵时伤害玩家，而粉红色怪物由于它们的旅行时间可以安全地忽略一段时间。这个设置与我们之前的末日文章形成了对比，在那篇文章中我们展示了单个目标。</p><p id="2430" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的 Google 协作实现是使用 Pytorch 用 Python 编写的，可以在<a class="ae mn" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> GradientCrescent Github 上找到。</a>我们的方法基于 Tabor 的优秀强化学习<a class="ae mn" href="https://www.manning.com/livevideo/reinforcement-learning-in-motion" rel="noopener ugc nofollow" target="_blank">课程</a>中详述的方法。由于这种方法的实现非常复杂，让我们<strong class="lk iu">总结一下所需动作的顺序</strong>:</p><ol class=""><li id="00f0" class="mz na it lk b ll lm lo lp lr nb lv nc lz nd md no nf ng nh bi translated">我们定义了最大化性能所需的预处理函数，并将它们作为自动化健身房环境的包装器引入。这些主要是通过使用元素最大值和帧堆叠来捕捉环境的运动。</li><li id="9355" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md no nf ng nh bi translated"><strong class="lk iu">我们定义我们的深度 Q 学习神经网络</strong>。这是一个 CNN，它拍摄游戏中的屏幕图像，并输出 Ms-Pacman gamespace 中每个动作的概率，或 Q 值。为了获得概率张量，我们在最后一层不包括任何激活函数。</li><li id="cad6" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md no nf ng nh bi translated">由于 Q-learning 要求我们了解当前和下一个状态，我们需要<strong class="lk iu">从数据生成</strong>开始。我们将表示初始状态<em class="np"> s </em>的游戏空间的预处理输入图像输入到网络中，并获得动作的初始概率分布，或 Q 值。在训练之前，这些值将是随机的和次优的。</li><li id="4cd4" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md no nf ng nh bi translated">利用我们的概率张量，我们然后<strong class="lk iu">使用 argmax()函数选择具有当前最高概率</strong>的动作，并使用它来构建 epsilon 贪婪策略。</li><li id="6c13" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md no nf ng nh bi translated">使用我们的策略，我们将选择动作<em class="np"> a </em>，并评估我们在健身房环境中的决策<strong class="lk iu">接收关于新状态<em class="np">s’</em>、奖励<em class="np"> r </em> </strong>以及该集是否已结束的信息。</li><li id="54f3" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md no nf ng nh bi translated">我们以列表形式<s>将该信息组合存储在一个缓冲区中，并重复步骤 2-4 预设次数，以建立一个足够大的缓冲区数据集。</s></li><li id="8e75" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md no nf ng nh bi translated">G <strong class="lk iu">生成我们的目标<em class="np"> y </em>值，<em class="np"> R' </em>和<em class="np"> A' </em> </strong>，这是损失计算所需要的。虽然前者只是从<em class="np"> R </em>中减去，但是我们通过将<em class="np">S’</em>输入到我们的网络中来获得 A’。</li><li id="d15a" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md no nf ng nh bi translated">有了所有的组件，我们就可以<strong class="lk iu">计算训练网络的损耗</strong>。</li><li id="cb3d" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md no nf ng nh bi translated">一旦训练结束，我们将评估我们的代理在新一集游戏中的表现，并记录他们的表现</li></ol><p id="27a5" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们从导入所有必需的包开始，包括 OpenAI 和 Vizdoomgym 环境。我们还将安装火炬视觉所需的 AV 包，我们将使用它进行可视化。请注意，安装完成后必须重新启动运行时。</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="9a10" class="nv kj it nr b gy nw nx l ny nz">!sudo apt-get update</span><span id="97e3" class="nv kj it nr b gy oa nx l ny nz">!sudo apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev libopenal-dev timidity libwildmidi-dev unzip</span><span id="3ab9" class="nv kj it nr b gy oa nx l ny nz"># Boost libraries</span><span id="d18a" class="nv kj it nr b gy oa nx l ny nz">!sudo apt-get install libboost-all-dev</span><span id="731c" class="nv kj it nr b gy oa nx l ny nz"># Lua binding dependencies</span><span id="f2b5" class="nv kj it nr b gy oa nx l ny nz">!apt-get install liblua5.1-dev</span><span id="5ab1" class="nv kj it nr b gy oa nx l ny nz">!sudo apt-get install cmake libboost-all-dev libgtk2.0-dev libsdl2-dev python-numpy git</span><span id="2370" class="nv kj it nr b gy oa nx l ny nz">!git clone <a class="ae mn" href="https://github.com/shakenes/vizdoomgym.git" rel="noopener ugc nofollow" target="_blank">https://github.com/shakenes/vizdoomgym.git</a></span><span id="6b46" class="nv kj it nr b gy oa nx l ny nz">!python3 -m pip install -e vizdoomgym/</span><span id="fc3b" class="nv kj it nr b gy oa nx l ny nz">!pip install av</span></pre><p id="4cd7" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们初始化我们的环境场景，检查观察空间和动作空间，并可视化我们的环境..</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="181b" class="nv kj it nr b gy nw nx l ny nz">import gym</span><span id="480b" class="nv kj it nr b gy oa nx l ny nz">import vizdoomgym</span><span id="40f8" class="nv kj it nr b gy oa nx l ny nz">env = gym.make(‘VizdoomDefendLine-v0’)</span><span id="c60d" class="nv kj it nr b gy oa nx l ny nz">n_outputs = env.action_space.n</span><span id="01e3" class="nv kj it nr b gy oa nx l ny nz">print(n_outputs)</span><span id="be88" class="nv kj it nr b gy oa nx l ny nz">observation = env.reset()</span><span id="de36" class="nv kj it nr b gy oa nx l ny nz">import matplotlib.pyplot as plt</span><span id="c48e" class="nv kj it nr b gy oa nx l ny nz">for i in range(22):</span><span id="1592" class="nv kj it nr b gy oa nx l ny nz">  if i &gt; 20:</span><span id="dd58" class="nv kj it nr b gy oa nx l ny nz">    print(observation.shape)</span><span id="1171" class="nv kj it nr b gy oa nx l ny nz">    plt.imshow(observation)</span><span id="c339" class="nv kj it nr b gy oa nx l ny nz">    plt.show()</span><span id="e7ab" class="nv kj it nr b gy oa nx l ny nz">    observation, _, _, _ = env.step(1)</span></pre><p id="d137" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们将定义预处理包装器。这些类继承自 OpenAI gym 基类，覆盖了它们的方法和变量，以便隐式地提供所有必要的预处理。我们将开始定义一个包装器来重复许多帧的每个动作，并执行元素方式的最大值以增加任何动作的强度。您会注意到一些三级参数，如<em class="np"> fire_first </em>和<em class="np">no _ ops</em>——这些是特定于环境的，在 Vizdoomgym 中对我们没有影响。</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="cccd" class="nv kj it nr b gy nw nx l ny nz">class RepeatActionAndMaxFrame(gym.Wrapper):<br/> #input: environment, repeat<br/> #init frame buffer as an array of zeros in shape 2 x the obs space<br/> def __init__(self, env=None, repeat=4, clip_reward=False, no_ops=0,<br/> fire_first=False):<br/> super(RepeatActionAndMaxFrame, self).__init__(env)<br/> self.repeat = repeat<br/> self.shape = env.observation_space.low.shape<br/> self.frame_buffer = np.zeros_like((2, self.shape))<br/> self.clip_reward = clip_reward<br/> self.no_ops = no_ops<br/> self.fire_first = fire_first</span><span id="178f" class="nv kj it nr b gy oa nx l ny nz">def step(self, action):<br/> t_reward = 0.0<br/> done = False<br/> for i in range(self.repeat):<br/> obs, reward, done, info = self.env.step(action)<br/> if self.clip_reward:<br/> reward = np.clip(np.array([reward]), -1, 1)[0]<br/> t_reward += reward<br/> idx = i % 2<br/> self.frame_buffer[idx] = obs<br/> if done:<br/> break</span><span id="7615" class="nv kj it nr b gy oa nx l ny nz"> max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])<br/> return max_frame, t_reward, done, info</span><span id="bfe8" class="nv kj it nr b gy oa nx l ny nz">def reset(self):<br/> obs = self.env.reset()<br/> no_ops = np.random.randint(self.no_ops)+1 if self.no_ops &gt; 0 else 0<br/> for _ in range(no_ops):<br/> _, _, done, _ = self.env.step(0)<br/> if done:<br/> self.env.reset()<br/> #Fire first seems quite useless, probably meant for something like space invader<br/> if self.fire_first:<br/> assert self.env.unwrapped.get_action_meanings()[1] == ‘FIRE’<br/> obs, _, _, _ = self.env.step(1)</span><span id="1acc" class="nv kj it nr b gy oa nx l ny nz"> self.frame_buffer = np.zeros_like((2,self.shape))<br/> self.frame_buffer[0] = obs</span><span id="f687" class="nv kj it nr b gy oa nx l ny nz"> return obs</span></pre><p id="737c" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们为我们的观察定义预处理函数。我们将使我们的环境对称，将它转换到盒子空间，将通道整数交换到张量的前面，并将其从原始(320，480)分辨率调整到(84，84)区域。我们也将我们的环境灰度化，并通过除以一个常数来归一化整个图像。</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="5cc4" class="nv kj it nr b gy nw nx l ny nz">class PreprocessFrame(gym.ObservationWrapper):<br/> #set shape by swapping channels axis<br/> #set observation space to new shape using gym.spaces.Box (0 to 1.0)<br/> def __init__(self, shape, env=None):<br/> super(PreprocessFrame, self).__init__(env)<br/> self.shape = (shape[2], shape[0], shape[1])<br/> self.observation_space = gym.spaces.Box(low=0.0, high=1.0,<br/> shape=self.shape, dtype=np.float32)</span><span id="7671" class="nv kj it nr b gy oa nx l ny nz">def observation(self, obs):<br/> new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)<br/> resized_screen = cv2.resize(new_frame, self.shape[1:],<br/> interpolation=cv2.INTER_AREA)<br/> new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)<br/> new_obs = new_obs / 255.0</span><span id="cadc" class="nv kj it nr b gy oa nx l ny nz"> return new_obs</span></pre><p id="4329" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们创建一个包装器来处理帧堆叠。这里的目标是通过将几个帧堆叠在一起作为单个批次，帮助从堆叠帧中捕捉运动和方向。这样，我们可以捕捉环境中元素的位置、平移、速度和加速度。通过堆叠，我们的输入采用(4，84，84，1)的形状。</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="50aa" class="nv kj it nr b gy nw nx l ny nz">class StackFrames(gym.ObservationWrapper):<br/> #init the new obs space (gym.spaces.Box) low &amp; high bounds as repeat of n_steps. These should have been defined for vizdooom<br/> #Create a return a stack of observations<br/> def __init__(self, env, repeat):<br/> super(StackFrames, self).__init__(env)<br/> self.observation_space = gym.spaces.Box(<br/> env.observation_space.low.repeat(repeat, axis=0),<br/> env.observation_space.high.repeat(repeat, axis=0),<br/> dtype=np.float32)<br/> self.stack = collections.deque(maxlen=repeat)</span><span id="d33c" class="nv kj it nr b gy oa nx l ny nz">def reset(self):<br/> self.stack.clear()<br/> observation = self.env.reset()<br/> for _ in range(self.stack.maxlen):<br/> self.stack.append(observation)</span><span id="bf5e" class="nv kj it nr b gy oa nx l ny nz"> return  np.array(self.stack).reshape(self.observation_space.low.shape)</span><span id="8098" class="nv kj it nr b gy oa nx l ny nz">def observation(self, observation):<br/> self.stack.append(observation)</span><span id="9ba9" class="nv kj it nr b gy oa nx l ny nz"> return np.array(self.stack).reshape(self.observation_space.low.shape)</span></pre><p id="1dd9" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，在返回最终环境供使用之前，我们将所有的包装器绑定到一个单独的<em class="np"> make_env() </em>方法中。</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="518e" class="nv kj it nr b gy nw nx l ny nz">def make_env(env_name, shape=(84,84,1), repeat=4, clip_rewards=False,<br/> no_ops=0, fire_first=False):<br/> env = gym.make(env_name)<br/> env = PreprocessFrame(shape, env)<br/> env = RepeatActionAndMaxFrame(env, repeat, clip_rewards, no_ops, fire_first)<br/> <br/> env = StackFrames(env, repeat)</span><span id="b85e" class="nv kj it nr b gy oa nx l ny nz"> return env</span></pre><p id="d142" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，让我们定义我们的模型，一个深度 Q 网络。这本质上是一个三层卷积网络，它采用预处理的输入观察值，将生成的展平输出馈送到一个全连接层，生成游戏空间中的状态-动作值作为输出。请注意，这里没有激活层，因为激活层的存在会导致二进制输出分布。我们的损失是我们计算的状态-动作值与我们预测的状态-动作值的平方差。我们将使用 RMSProp 优化器来最小化我们在训练期间的损失。</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="7414" class="nv kj it nr b gy nw nx l ny nz">import os<br/>import torch as T<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>import numpy as np</span><span id="6a2b" class="nv kj it nr b gy oa nx l ny nz">class DeepQNetwork(nn.Module):<br/> def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):<br/> super(DeepQNetwork, self).__init__()<br/> self.checkpoint_dir = chkpt_dir<br/> self.checkpoint_file = os.path.join(self.checkpoint_dir, name)</span><span id="ad1e" class="nv kj it nr b gy oa nx l ny nz"> self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)<br/> self.conv2 = nn.Conv2d(32, 64, 4, stride=2)<br/> self.conv3 = nn.Conv2d(64, 64, 3, stride=1)</span><span id="78c1" class="nv kj it nr b gy oa nx l ny nz"> fc_input_dims = self.calculate_conv_output_dims(input_dims)</span><span id="c0b5" class="nv kj it nr b gy oa nx l ny nz"> self.fc1 = nn.Linear(fc_input_dims, 512)<br/> self.fc2 = nn.Linear(512, n_actions)</span><span id="fd11" class="nv kj it nr b gy oa nx l ny nz"> self.optimizer = optim.RMSprop(self.parameters(), lr=lr)</span><span id="2bd1" class="nv kj it nr b gy oa nx l ny nz"> self.loss = nn.MSELoss()<br/> self.device = T.device(‘cuda:0’ if T.cuda.is_available() else ‘cpu’)<br/> self.to(self.device)</span><span id="11fb" class="nv kj it nr b gy oa nx l ny nz">def calculate_conv_output_dims(self, input_dims):<br/> state = T.zeros(1, *input_dims)<br/> dims = self.conv1(state)<br/> dims = self.conv2(dims)<br/> dims = self.conv3(dims)<br/> return int(np.prod(dims.size()))</span><span id="3c61" class="nv kj it nr b gy oa nx l ny nz">def forward(self, state):<br/> conv1 = F.relu(self.conv1(state))<br/> conv2 = F.relu(self.conv2(conv1))<br/> conv3 = F.relu(self.conv3(conv2))<br/> # conv3 shape is BS x n_filters x H x W<br/> conv_state = conv3.view(conv3.size()[0], -1)<br/> # conv_state shape is BS x (n_filters * H * W)<br/> flat1 = F.relu(self.fc1(conv_state))<br/> actions = self.fc2(flat1)</span><span id="93f2" class="nv kj it nr b gy oa nx l ny nz"> return actions</span><span id="6c43" class="nv kj it nr b gy oa nx l ny nz">def save_checkpoint(self):<br/> print(‘… saving checkpoint …’)<br/> T.save(self.state_dict(), self.checkpoint_file)</span><span id="dd52" class="nv kj it nr b gy oa nx l ny nz">def load_checkpoint(self):<br/> print(‘… loading checkpoint …’)<br/> self.load_state_dict(T.load(self.checkpoint_file))</span></pre><p id="d862" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">回想一下，Q-learning 的更新功能需要:</p><ul class=""><li id="4bb6" class="mz na it lk b ll lm lo lp lr nb lv nc lz nd md ne nf ng nh bi translated">当前状态<em class="np"> s </em></li><li id="c842" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">当前动作<em class="np">一</em></li><li id="a264" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">当前动作后的奖励<em class="np"> r </em></li><li id="1af3" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">下一个状态<em class="np">s’</em></li><li id="a43f" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">下一个动作<em class="np">a’</em></li></ul><p id="0ad7" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了以有意义的数量提供这些参数，我们需要按照一组参数评估我们当前的策略，并将所有变量存储在一个缓冲区中，我们将在训练期间从该缓冲区中提取迷你批次中的数据。因此，我们需要一个重放内存缓冲区来存储和提取观察值。</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="37c1" class="nv kj it nr b gy nw nx l ny nz">import numpy as np</span><span id="d1d2" class="nv kj it nr b gy oa nx l ny nz">class ReplayBuffer(object):<br/> def __init__(self, max_size, input_shape, n_actions):<br/> self.mem_size = max_size<br/> self.mem_cntr = 0<br/> self.state_memory = np.zeros((self.mem_size, *input_shape),<br/> dtype=np.float32)<br/> self.new_state_memory = np.zeros((self.mem_size, *input_shape),<br/> dtype=np.float32)</span><span id="516a" class="nv kj it nr b gy oa nx l ny nz"> self.action_memory = np.zeros(self.mem_size, dtype=np.int64)<br/> self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)<br/> self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)</span><span id="0cce" class="nv kj it nr b gy oa nx l ny nz">#Identify index and store the the current SARSA into batch memory<br/>def store_transition(self, state, action, reward, state_, done):<br/> index = self.mem_cntr % self.mem_size<br/> self.state_memory[index] = state<br/> self.new_state_memory[index] = state_<br/> self.action_memory[index] = action<br/> self.reward_memory[index] = reward<br/> self.terminal_memory[index] = done<br/> self.mem_cntr += 1</span><span id="41cf" class="nv kj it nr b gy oa nx l ny nz">def sample_buffer(self, batch_size):<br/> max_mem = min(self.mem_cntr, self.mem_size)<br/> batch = np.random.choice(max_mem, batch_size, replace=False)<br/><br/> states = self.state_memory[batch]<br/> actions = self.action_memory[batch]<br/> rewards = self.reward_memory[batch]<br/> states_ = self.new_state_memory[batch]<br/> terminal = self.terminal_memory[batch]</span><span id="fb75" class="nv kj it nr b gy oa nx l ny nz"> return states, actions, rewards, states_, terminal</span></pre><p id="a2d3" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们将定义我们的代理。我们的代理正在使用一个勘探率递减的ε贪婪策略，以便随着时间的推移最大化开发。为了学会预测使我们的累积奖励最大化的状态-行动-值，我们的代理人将使用通过记忆抽样获得的贴现的未来奖励。</p><p id="533a" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您会注意到，作为代理的一部分，我们初始化了 DQN 的两个副本，并使用方法将原始网络的权重参数复制到目标网络中。这种双网络方法允许我们在使用现有策略的训练过程中生成数据，同时仍然为下一个策略迭代优化我们的参数，减少损失振荡。</p><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="eeb0" class="nv kj it nr b gy nw nx l ny nz">import numpy as np<br/>import torch as T<br/>#from deep_q_network import DeepQNetwork<br/>#from replay_memory import ReplayBuffer</span><span id="e83d" class="nv kj it nr b gy oa nx l ny nz">class DQNAgent(object):<br/> def __init__(self, gamma, epsilon, lr, n_actions, input_dims,<br/> mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,<br/> replace=1000, algo=None, env_name=None, chkpt_dir=’tmp/dqn’):<br/> self.gamma = gamma<br/> self.epsilon = epsilon<br/> self.lr = lr<br/> self.n_actions = n_actions<br/> self.input_dims = input_dims<br/> self.batch_size = batch_size<br/> self.eps_min = eps_min<br/> self.eps_dec = eps_dec<br/> self.replace_target_cnt = replace<br/> self.algo = algo<br/> self.env_name = env_name<br/> self.chkpt_dir = chkpt_dir<br/> self.action_space = [i for i in range(n_actions)]<br/> self.learn_step_counter = 0</span><span id="b774" class="nv kj it nr b gy oa nx l ny nz">self.memory = ReplayBuffer(mem_size, input_dims, n_actions)</span><span id="8b54" class="nv kj it nr b gy oa nx l ny nz">self.q_eval = DeepQNetwork(self.lr, self.n_actions,<br/> input_dims=self.input_dims,<br/> name=self.env_name+’_’+self.algo+’_q_eval’,<br/> chkpt_dir=self.chkpt_dir)</span><span id="c7c1" class="nv kj it nr b gy oa nx l ny nz">self.q_next = DeepQNetwork(self.lr, self.n_actions,<br/> input_dims=self.input_dims,<br/> name=self.env_name+’_’+self.algo+’_q_next’,<br/> chkpt_dir=self.chkpt_dir)</span><span id="0a68" class="nv kj it nr b gy oa nx l ny nz">#Epsilon greedy action selection<br/> def choose_action(self, observation):<br/> if np.random.random() &gt; self.epsilon:<br/> state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)<br/> actions = self.q_eval.forward(state)<br/> action = T.argmax(actions).item()<br/> else:<br/> action = np.random.choice(self.action_space)</span><span id="b402" class="nv kj it nr b gy oa nx l ny nz">return action</span><span id="c79d" class="nv kj it nr b gy oa nx l ny nz">def store_transition(self, state, action, reward, state_, done):<br/> self.memory.store_transition(state, action, reward, state_, done)</span><span id="1d6c" class="nv kj it nr b gy oa nx l ny nz">def sample_memory(self):<br/> state, action, reward, new_state, done = \<br/> self.memory.sample_buffer(self.batch_size)</span><span id="4a70" class="nv kj it nr b gy oa nx l ny nz">states = T.tensor(state).to(self.q_eval.device)<br/> rewards = T.tensor(reward).to(self.q_eval.device)<br/> dones = T.tensor(done).to(self.q_eval.device)<br/> actions = T.tensor(action).to(self.q_eval.device)<br/> states_ = T.tensor(new_state).to(self.q_eval.device)</span><span id="12ec" class="nv kj it nr b gy oa nx l ny nz">return states, actions, rewards, states_, dones</span><span id="0741" class="nv kj it nr b gy oa nx l ny nz">def replace_target_network(self):<br/> if self.learn_step_counter % self.replace_target_cnt == 0:<br/> self.q_next.load_state_dict(self.q_eval.state_dict())</span><span id="4b94" class="nv kj it nr b gy oa nx l ny nz">def decrement_epsilon(self):<br/> self.epsilon = self.epsilon — self.eps_dec \<br/> if self.epsilon &gt; self.eps_min else self.eps_min</span><span id="e7c3" class="nv kj it nr b gy oa nx l ny nz">def save_models(self):<br/> self.q_eval.save_checkpoint()<br/> self.q_next.save_checkpoint()</span><span id="9df0" class="nv kj it nr b gy oa nx l ny nz">def load_models(self):<br/> self.q_eval.load_checkpoint()<br/> self.q_next.load_checkpoint()<br/> #Make sure you understand this line by line<br/> def learn(self):<br/> if self.memory.mem_cntr &lt; self.batch_size:<br/> return</span><span id="0ae8" class="nv kj it nr b gy oa nx l ny nz">self.q_eval.optimizer.zero_grad()</span><span id="8e32" class="nv kj it nr b gy oa nx l ny nz">self.replace_target_network()</span><span id="5f8b" class="nv kj it nr b gy oa nx l ny nz">states, actions, rewards, states_, dones = self.sample_memory()<br/> indices = np.arange(self.batch_size)</span><span id="14c1" class="nv kj it nr b gy oa nx l ny nz">q_pred = self.q_eval.forward(states)[indices, actions]<br/> q_next = self.q_next.forward(states_).max(dim=1)[0]</span><span id="bc14" class="nv kj it nr b gy oa nx l ny nz">q_next[dones] = 0.0<br/> q_target = rewards + self.gamma*q_next</span><span id="1e70" class="nv kj it nr b gy oa nx l ny nz">loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)<br/> loss.backward()<br/> self.q_eval.optimizer.step()<br/> self.learn_step_counter += 1</span><span id="f7c6" class="nv kj it nr b gy oa nx l ny nz"> self.decrement_epsilon()</span></pre><p id="7ff2" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">定义了所有支持代码后，让我们运行主训练循环。我们已经在最初的总结中定义了大部分，但是让我们为后代回忆一下。</p><ul class=""><li id="e673" class="mz na it lk b ll lm lo lp lr nb lv nc lz nd md ne nf ng nh bi translated">对于训练集的每一步，在使用ε-贪婪策略选择下一个动作之前，我们将输入图像堆栈输入到我们的网络中，以生成可用动作的概率分布</li><li id="36cf" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">然后，我们将它输入到网络中，获取下一个状态和相应奖励的信息，并将其存储到我们的缓冲区中。我们更新我们的堆栈，并通过一些预定义的步骤重复这一过程。</li><li id="ce41" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">在一集的结尾，我们将下一个状态输入到我们的网络中，以便获得下一个动作。我们还通过对当前奖励进行贴现来计算下一个奖励。</li><li id="b1a0" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">我们通过 Q 学习更新函数生成我们的目标 y 值，并训练我们的网络。</li><li id="8116" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">通过最小化训练损失，我们更新网络权重参数，以便为下一个策略输出改进的状态-动作值。</li><li id="d44a" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">我们通过跟踪模型的平均得分(在 100 个训练步骤中测量)来评估模型。</li></ul><pre class="lb lc ld le gt nq nr ns nt aw nu bi"><span id="2b52" class="nv kj it nr b gy nw nx l ny nz">env = make_env(‘VizdoomDefendLine-v0’)<br/>best_score = -np.inf<br/>load_checkpoint = False<br/>n_games = 500<br/>agent = DQNAgent(gamma=0.99, epsilon=1.0, lr=0.0001,input_dims=(env.observation_space.shape),n_actions=env.action_space.n, mem_size=5000, eps_min=0.1,batch_size=32, replace=1000, eps_dec=1e-5,chkpt_dir=’/content/’, algo=’DQNAgent’,env_name=’vizdoogym’)</span><span id="e39b" class="nv kj it nr b gy oa nx l ny nz">if load_checkpoint:<br/> agent.load_models()</span><span id="e612" class="nv kj it nr b gy oa nx l ny nz">fname = agent.algo + ‘_’ + agent.env_name + ‘_lr’ + str(agent.lr) +’_’+ str(n_games) + ‘games’<br/>figure_file = ‘plots/’ + fname + ‘.png’</span><span id="8e4f" class="nv kj it nr b gy oa nx l ny nz">n_steps = 0<br/>scores, eps_history, steps_array = [], [], []</span><span id="3470" class="nv kj it nr b gy oa nx l ny nz">for i in range(n_games):<br/> done = False<br/> observation = env.reset()</span><span id="bdc7" class="nv kj it nr b gy oa nx l ny nz">score = 0<br/> while not done:<br/> action = agent.choose_action(observation)<br/> observation_, reward, done, info = env.step(action)<br/> score += reward</span><span id="f290" class="nv kj it nr b gy oa nx l ny nz">if not load_checkpoint:<br/> agent.store_transition(observation, action,reward, observation_, int(done))<br/> agent.learn()<br/> observation = observation_<br/> n_steps += 1</span><span id="0a14" class="nv kj it nr b gy oa nx l ny nz">scores.append(score)<br/> steps_array.append(n_steps)</span><span id="e4f1" class="nv kj it nr b gy oa nx l ny nz">avg_score = np.mean(scores[-100:])</span><span id="5010" class="nv kj it nr b gy oa nx l ny nz">if avg_score &gt; best_score:<br/> best_score = avg_score<br/> <br/> <br/> print(‘Checkpoint saved at episode ‘, i)<br/> agent.save_models()</span><span id="ca95" class="nv kj it nr b gy oa nx l ny nz">print(‘Episode: ‘, i,’Score: ‘, score,’ Average score: %.2f’ % avg_score, ‘Best average: %.2f’ % best_score,’Epsilon: %.2f’ % agent.epsilon, ‘Steps:’, n_steps)</span><span id="e145" class="nv kj it nr b gy oa nx l ny nz">eps_history.append(agent.epsilon)<br/> if load_checkpoint and n_steps &gt;= 18000:<br/> break</span></pre><p id="82fc" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们绘制了 500、1000 和 2000 集的代理商平均得分和 epsilon 比率。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/4ea42d5325fd263ae5da8ac8a5ad9e31.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*orjsY5WBjAT6CC5CvWiWjw.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">500 集后我们经纪人的奖励分配。</p></figure><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e9917effb1ce1f9205135ee1d3afb728.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*RGz8g-JmiTFcf5eZ6PgPcg.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">1000 集后我们经纪人的奖励分配。</p></figure><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/dc753fded2745bc081f149cdbd04fbfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*NU3JsT8Jz45L35_9lPjifg.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">2000 集后我们经纪人的报酬分配。</p></figure><p id="a4c1" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">查看结果，您会注意到一些模式。</p><ul class=""><li id="99da" class="mz na it lk b ll lm lo lp lr nb lv nc lz nd md ne nf ng nh bi translated">在最初的 400 集里，我们观察到了平均分数为 12 的表演的最初增长。这段时间，代理人在重重地探索。</li><li id="c74a" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">在 400-750 个训练集之间，我们观察到 epsilon 衰减到 20%以下，表明探索率显著降低。因此，当代理试图最大化利用时，它的性能可能会显著提高或下降。</li><li id="234e" class="mz na it lk b ll ni lo nj lr nk lv nl lz nm md ne nf ng nh bi translated">然而，在过去的 750 集里，代理已经进行了足够的探索来找到改进的策略，导致模型性能的增长和稳定。</li></ul><p id="319f" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有趣的是，我们可以在游戏中观察到这些点。下面是我们的特工分别在 500 集、1000 集和 2000 集时的游戏片段。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="mo mp l"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">500 集特工训练的游戏性。</p></figure><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="mo mp l"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">特工的游戏性训练了 1000 集。</p></figure><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="mo mp l"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">2000 集特工训练的游戏性。</p></figure><p id="4c3e" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所有的特工都表现出持续射击——考虑到弹药消耗没有惩罚，这是可以理解的。请注意，在 500 集时训练过的特工如何表现出更大的转弯弧线，而训练更好的特工似乎坚持在地图的特定区域。这种行为可能是对棕色怪物产卵的预期，这是一种依靠粉色怪物走近穿越火线的策略。请注意，这个环境仍然相对简单，以便于相对容易的训练——引入弹药使用惩罚，或者增加行动空间包括扫射，将导致明显不同的行为。</p><p id="614e" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就结束了 Q-learning 的实现。在我们的下一篇文章中，我们将继续用更高级的 Q-learning 方法来检查我们的代理在这些环境中的性能。</p><p id="ace4" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们希望你喜欢这篇文章，并希望你查看 GradientCrescent 上的许多其他文章，涵盖人工智能的应用和理论方面。为了保持对<a class="ae mn" href="https://medium.com/@adrianitsaxu" rel="noopener"> GradientCrescent </a>的最新更新，请考虑关注该出版物并关注我们的<a class="ae mn" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> Github </a>资源库。</p><h1 id="14ee" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">来源</strong></h1><p id="d932" class="pw-post-body-paragraph li lj it lk b ll mu ju ln lo mv jx lq lr mw lt lu lv mx lx ly lz my mb mc md im bi translated">萨顿等人。al，“强化学习”</p><p id="49e0" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">塔博尔，“运动中的强化学习”</p></div></div>    
</body>
</html>