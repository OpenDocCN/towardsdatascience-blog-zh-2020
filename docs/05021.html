<html>
<head>
<title>Day 121 of #NLP365: NLP Papers Summary — Concept Pointer Network for Abstractive Summarization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">#NLP365的第121天:NLP论文摘要——抽象摘要的概念指针网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/day-121-of-nlp365-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization-cd55e577f6de?source=collection_archive---------53-----------------------#2020-04-30">https://towardsdatascience.com/day-121-of-nlp365-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization-cd55e577f6de?source=collection_archive---------53-----------------------#2020-04-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/fbe3831891625ccfa7a5401ede20b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmWzzuXHoD6w2K9Yp9p9Q.jpeg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">阅读和理解研究论文就像拼凑一个未解之谜。汉斯-彼得·高斯特在<a class="ae jc" href="https://unsplash.com/s/photos/research-papers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h2 id="fab1" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内线艾</a> <a class="ae ep" href="http://towardsdatascience.com/tagged/nlp365" rel="noopener" target="_blank"> NLP365 </a></h2><div class=""/><div class=""><h2 id="d8ef" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">NLP论文摘要是我总结NLP研究论文要点的系列文章</h2></div><p id="4d85" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">项目#NLP365 (+1)是我在2020年每天记录我的NLP学习旅程的地方。在这里，你可以随意查看我在过去的262天里学到了什么。在本文的最后，你可以找到以前的论文摘要，按自然语言处理领域分类:)</p><p id="6ff8" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天的NLP论文是<strong class="lf jp"> <em class="lz">概念指针网络，用于抽象概括</em> </strong> n .以下是研究论文的要点。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="6aaa" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">目标和贡献</h1><p id="d1af" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">提出了用于抽象概括的概念指针网络，它使用基于知识和上下文感知的概念化来导出一组候选概念。然后，当生成抽象摘要时，模型将在概念集和原始源文本之间进行选择。对生成的摘要进行自动和人工评估。</p><p id="ac94" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">提出的概念指针网络不只是简单地从源文档中复制文本，它还会从人类知识中生成新的抽象概念，如下所示:</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/d9f06dc8205415b8d61f02aaba87dd82.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*4DVPFGhTEJfJodJ_.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">指针网络框架概念[1]</p></figure><p id="31db" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在我们新颖的模型架构之上，我们还提出了一种远程监督学习技术，以允许我们的模型适应不同的数据集。自动和人工评估都显示出相对于SOTA基线的显著改进。</p><h1 id="40dd" class="mh mi jf bd mj mk nj mm mn mo nk mq mr ku nl kv mt kx nm ky mv la nn lb mx my bi translated">提议的模式</h1><p id="155e" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们的模型架构由两个模块组成:</p><ol class=""><li id="4215" class="no np jf lf b lg lh lj lk lm nq lq nr lu ns ly nt nu nv nw bi translated">编码器-解码器</li><li id="2a32" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">概念指针生成器</li></ol><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi oc"><img src="../Images/145904bd3a9871f578d9df400675335d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MbwcHB7AirSaX-sA.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">概念指针生成器的体系结构[1]</p></figure><h2 id="336f" class="oh mi jf bd mj oi oj dn mn ok ol dp mr lm om on mt lq oo op mv lu oq or mx jl bi translated">编码器-解码器</h2><p id="da49" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">编解码框架由两层双向LSTM-RNN编码器和一层带注意机制的LSTM-RNN解码器组成。输入序列中的每个单词都由向前和向后隐藏状态的串联来表示。通过对隐藏状态表示应用注意机制来计算上下文向量。这个上下文向量被馈送到我们的解码器，在那里它将使用上下文向量来确定从我们的词汇分布生成新单词(p_gen)的概率。</p><h2 id="434d" class="oh mi jf bd mj oi oj dn mn ok ol dp mr lm om on mt lq oo op mv lu oq or mx jl bi translated">概念指针生成器</h2><p id="0118" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">首先，我们使用微软概念图将一个单词映射到它的相关概念。这个知识库覆盖了一个巨大的概念空间，概念和实体之间的关系是概率性的，这取决于它们的相关程度。本质上，概念图将接受这个单词，并估计这个单词属于一个特定概念p(c|x)的概率。对于概率，这意味着给定每个单词，概念图将有一组它认为该单词所属的候选概念(具有不同的置信度)。为了让我们的模型选择正确的候选概念，例如，区分单词“apple”的水果和公司概念，我们将使用编码器-解码器框架中的上下文向量。</p><p id="cbc4" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将使用上下文向量来更新概念分布。我们通过将当前隐藏状态、上下文向量和当前候选概念馈送到softmax分类器中来计算更新的权重。然后，这个更新的权重被添加到现有的概念概率中，以考虑输入序列的上下文，从而允许我们导出上下文感知的概念概率。</p><p id="0a74" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们的概念指针网络由指向源文档的普通指针和指向给定源文档的相关概念的概念指针组成。概念指针通过注意力分布按元素进行缩放，并被添加到普通指针(注意力分布)。这将是模型复制的复制分布，它包括在原始源文档上的普通文本分布之上的概念分布。</p><h2 id="17af" class="oh mi jf bd mj oi oj dn mn ok ol dp mr lm om on mt lq oo op mv lu oq or mx jl bi translated">模型适应的远程监控</h2><p id="526b" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">如果我们的训练集的摘要-文档对不同于测试集，我们的模型将表现不佳。为了应对这种情况，我们需要重新训练我们的模型，以降低最终损失中的这种差异。为此，我们需要标签来表明我们的训练集与测试集有多接近。为了创建这些标签，我们使用每个训练参考概要和来自测试集的一组文档之间的KL散度。换句话说，训练对是远距离标记的。参考文献摘要和文档的表示都是通过对组成单词嵌入求和来计算的。这个KL散度损失函数包含在训练过程中，并且它测量测试集和我们的每个参考摘要-文档对之间的总距离。这允许我们确定我们的训练集对于模型适应是相关还是不相关。</p><h1 id="0b4e" class="mh mi jf bd mj mk nj mm mn mo nk mq mr ku nl kv mt kx nm ky mv la nn lb mx my bi translated">实验设置和结果</h1><p id="c91d" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">有两个评估数据集:千兆字和DUC-2004。评估指标是ROUGE分数。</p><h2 id="3b04" class="oh mi jf bd mj oi oj dn mn ok ol dp mr lm om on mt lq oo op mv lu oq or mx jl bi translated">模型比较</h2><p id="5fb0" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">有8种基线模型:</p><ol class=""><li id="8074" class="no np jf lf b lg lh lj lk lm nq lq nr lu ns ly nt nu nv nw bi translated"><em class="lz"> ABS+ </em>。抽象概括模型</li><li id="51c1" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><em class="lz">卢昂-NMT </em>。LSTM编码器-解码器</li><li id="8c6b" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><em class="lz">拉斯-埃尔曼</em>。CNN关注编码器，RNN关注解码器</li><li id="6601" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><em class="lz"> Seq2seq+att </em>。BiLSTM编码器和带注意力解码器的LSTM</li><li id="9b9d" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><em class="lz"> Lvt5k-lsent </em>。利用对解码器的时间关注来减少摘要中的重复</li><li id="eb78" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><em class="lz">季节</em>。使用选择门来控制从编码器到解码器的信息流</li><li id="1588" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><em class="lz">指针生成器</em>。正常PG</li><li id="611d" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">CGU 。使用卷积门控单元和自我关注进行编码</li></ol><h2 id="05f2" class="oh mi jf bd mj oi oj dn mn ok ol dp mr lm om on mt lq oo op mv lu oq or mx jl bi translated">结果</h2><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi os"><img src="../Images/c459c928f6c52d276ded1aff3a46cf64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RTEoNPTVX0uyWLaJ.png"/></div></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">表Concept Pointer和其他基准模型之间的ROUGE结果和比较。表2——词汇外问题分析。表3 —抽象性的度量[1]</p></figure><p id="ba26" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在表1中，我们的概念指针在所有指标上都优于所有基线模型，除了在Gigaword上的RG-2(CGU得分最高)。在表2中，我们显示由概念指针生成的摘要具有最低的UNK词百分比，缓解了OOV问题。在表3中，我们展示了我们生成的摘要的抽象性。我们证明了由我们的概念指针生成的摘要具有相对较高的抽象级别，并且接近于引用摘要级别。</p><p id="0d6e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们试验了两种不同的培训策略:强化学习(RL)和远程监督(DS)。应用于概念指针的两种训练策略都优于普通概念指针。此外，在DUC-2004数据集，概念指针+ DS持续优于概念指针+ RL，展示了远程监督对更好的模型适应的影响。</p><h2 id="5dca" class="oh mi jf bd mj oi oj dn mn ok ol dp mr lm om on mt lq oo op mv lu oq or mx jl bi translated">上下文感知概念化</h2><p id="0522" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们想衡量概念更新策略的影响，所以我们对不同数量的候选概念进行了实验。结果如下所示。在不同数量概念候选者之间，ROUGE分数只有很小的变化。</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/06aebd7d91f6079642ea58207a5a0f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/0*zgq308WB41ENM8Hc.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">ROUGE在Gigaword和DUC 2004数据集上的结果[1]</p></figure><h2 id="0c53" class="oh mi jf bd mj oi oj dn mn ok ol dp mr lm om on mt lq oo op mv lu oq or mx jl bi translated">人工评估</h2><p id="a9b5" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">我们进行了人体评估，每位志愿者都必须回答以下问题:</p><ol class=""><li id="56db" class="no np jf lf b lg lh lj lk lm nq lq nr lu ns ly nt nu nv nw bi translated"><em class="lz">抽象</em> —摘要中的抽象概念有多贴切？</li><li id="4cd6" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><em class="lz">总体质量</em> —摘要的可读性、相关性和信息量如何？</li></ol><p id="8354" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们随机选择了20个例子，每个例子都有三个不同的摘要(来自三个模型),并对每种类型的摘要被选中的频率进行评分。结果如下所示，显示了指针网络的概念优于seq2seq模型和指针生成器。生成的摘要看起来流畅且信息丰富，然而，它仍然不像人类参考摘要那样抽象。</p><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/fc96d7a8c682faa9c5c3c76847202a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/0*T_Xx-Ufd7ZtN3w7N.png"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated">人对抽象和整体质量的评价[1]</p></figure><h1 id="a878" class="mh mi jf bd mj mk nj mm mn mo nk mq mr ku nl kv mt kx nm ky mv la nn lb mx my bi translated">结论和未来工作</h1><p id="971c" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">在我们新颖的模型架构之上，我们还提出了一种远程监督学习技术，以允许我们的模型适应不同的数据集。自动和人工评估都显示出相对于SOTA基线的显著改进。</p><h2 id="1da2" class="oh mi jf bd mj oi oj dn mn ok ol dp mr lm om on mt lq oo op mv lu oq or mx jl bi translated">来源:</h2><p id="e1ad" class="pw-post-body-paragraph ld le jf lf b lg mz kp li lj na ks ll lm nb lo lp lq nc ls lt lu nd lw lx ly im bi translated">[1]王，王伟，高，黄海燕，周，2019，11月.用于抽象摘要的概念指针网络。在<em class="lz">2019自然语言处理经验方法会议暨第九届国际自然语言处理联合会议(EMNLP-IJCNLP) </em>论文集(第3067–3076页)。</p><p id="7fae" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">原载于2020年4月30日https://ryanong.co.uk</em><em class="lz"/><a class="ae jc" href="https://ryanong.co.uk/2020/04/30/day-121-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization/" rel="noopener ugc nofollow" target="_blank"><em class="lz">。</em></a></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="a30d" class="mh mi jf bd mj mk ml mm mn mo mp mq mr ku ms kv mt kx mu ky mv la mw lb mx my bi translated">特征提取/基于特征的情感分析</h1><ul class=""><li id="1f1e" class="no np jf lf b lg mz lj na lm ov lq ow lu ox ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41">https://towards data science . com/day-102-of-NLP 365-NLP-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-BDF 00 a 66 db 41</a></li><li id="dfdb" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3">https://towards data science . com/day-103-NLP-research-papers-utilizing-Bert-for-aspect-based-sense-analysis-via-construction-38ab 3e 1630 a3</a></li><li id="92b4" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32">https://towards data science . com/day-104-of-NLP 365-NLP-papers-summary-senthious-targeted-aspect-based-sensitive-analysis-f 24 a2 EC 1 ca 32</a></li><li id="1a02" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-105-of-nlp365-nlp-papers-summary-aspect-level-sentiment-classification-with-3a3539be6ae8">https://towards data science . com/day-105-of-NLP 365-NLP-papers-summary-aspect-level-sensation-class ification-with-3a 3539 be 6 AE 8</a></li><li id="2752" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-106-of-nlp365-nlp-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b874d007b6d0">https://towards data science . com/day-106-of-NLP 365-NLP-papers-summary-an-unsupervised-neural-attention-model-for-aspect-b 874d 007 b 6d 0</a></li><li id="f707" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-110-of-nlp365-nlp-papers-summary-double-embeddings-and-cnn-based-sequence-labelling-for-b8a958f3bddd">https://towardsdatascience . com/day-110-of-NLP 365-NLP-papers-summary-double-embedding-and-CNN-based-sequence-labeling-for-b8a 958 F3 bddd</a></li><li id="6c9c" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-112-of-nlp365-nlp-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35b7a5e245b5">https://towards data science . com/day-112-of-NLP 365-NLP-papers-summary-a-challenge-dataset-and-effective-models-for-aspect-based-35 B7 a5 e 245 b5</a></li></ul><h1 id="f87a" class="mh mi jf bd mj mk nj mm mn mo nk mq mr ku nl kv mt kx nm ky mv la nn lb mx my bi translated">总结</h1><ul class=""><li id="d39f" class="no np jf lf b lg mz lj na lm ov lq ow lu ox ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8">https://towards data science . com/day-107-of-NLP 365-NLP-papers-summary-make-lead-bias-in-your-favor-a-simple-effective-4c 52 B1 a 569 b 8</a></li><li id="aa95" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-109-of-nlp365-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-619f5acb1b27">https://towards data science . com/day-109-of-NLP 365-NLP-papers-summary-studing-summary-evaluation-metrics-in-the-619 F5 acb1 b 27</a></li><li id="8df8" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-113-of-nlp365-nlp-papers-summary-on-extractive-and-abstractive-neural-document-87168b7e90bc">https://towards data science . com/day-113-of-NLP 365-NLP-papers-summary-on-extractive-and-abstract-neural-document-87168 b 7 e 90 BC</a></li><li id="8601" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-116-of-nlp365-nlp-papers-summary-data-driven-summarization-of-scientific-articles-3fba016c733b">https://towards data science . com/day-116-of-NLP 365-NLP-papers-summary-data-driven-summary-of-scientific-articles-3 FBA 016 c 733 b</a></li><li id="9e6d" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-117-of-nlp365-nlp-papers-summary-abstract-text-summarization-a-low-resource-challenge-61ae6cdf32f">https://towards data science . com/day-117-of-NLP 365-NLP-papers-summary-abstract-text-summary-a-low-resource-challenge-61a E6 CDF 32 f</a></li><li id="0858" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-118-of-nlp365-nlp-papers-summary-extractive-summarization-of-long-documents-by-combining-aea118a5eb3f">https://towards data science . com/day-118-of-NLP 365-NLP-papers-summary-extractive-summary-of-long-documents-by-combining-AEA 118 a5 eb3f</a></li><li id="5af6" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-120-of-nlp365-nlp-papers-summary-a-simple-theoretical-model-of-importance-for-summarization-843ddbbcb9b">https://towards data science . com/day-120-of-NLP 365-NLP-papers-summary-a-simple-theory-model-of-importance-for-summary-843 ddbcb 9b</a></li></ul><h1 id="c803" class="mh mi jf bd mj mk nj mm mn mo nk mq mr ku nl kv mt kx nm ky mv la nn lb mx my bi translated">其他人</h1><ul class=""><li id="2e8c" class="no np jf lf b lg mz lj na lm ov lq ow lu ox ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-108-of-nlp365-nlp-papers-summary-simple-bert-models-for-relation-extraction-and-semantic-98f7698184d7">https://towards data science . com/day-108-of-NLP 365-NLP-papers-summary-simple-Bert-models-for-relation-extraction-and-semantic-98f 7698184 D7</a></li><li id="4232" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-111-of-nlp365-nlp-papers-summary-the-risk-of-racial-bias-in-hate-speech-detection-bff7f5f20ce5">https://towards data science . com/day-111-of-NLP 365-NLP-papers-summary-the-risk-of-race-of-bias-in-hate-speech-detection-BFF 7 F5 f 20 ce 5</a></li><li id="7870" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-115-of-nlp365-nlp-papers-summary-scibert-a-pretrained-language-model-for-scientific-text-185785598e33">https://towards data science . com/day-115-of-NLP 365-NLP-papers-summary-scibert-a-pre trained-language-model-for-scientific-text-185785598 e33</a></li><li id="54ee" class="no np jf lf b lg nx lj ny lm nz lq oa lu ob ly oy nu nv nw bi translated"><a class="ae jc" rel="noopener" target="_blank" href="/day-119-nlp-papers-summary-an-argument-annotated-corpus-of-scientific-publications-d7b9e2ea1097">https://towards data science . com/day-119-NLP-papers-summary-an-argument-annoted-corpus-of-scientific-publications-d 7 b 9 e 2e ea 1097</a></li></ul></div></div>    
</body>
</html>