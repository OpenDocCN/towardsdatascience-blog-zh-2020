<html>
<head>
<title>Limited-Memory Broyden-Fletcher-Goldfarb-Shanno Algorithm in ML.NET</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML 中的有限内存 Broyden-Fletcher-goldf ARB-Shanno 算法。网</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/limited-memory-broyden-fletcher-goldfarb-shanno-algorithm-in-ml-net-118dec066ba?source=collection_archive---------24-----------------------#2020-03-08">https://towardsdatascience.com/limited-memory-broyden-fletcher-goldfarb-shanno-algorithm-in-ml-net-118dec066ba?source=collection_archive---------24-----------------------#2020-03-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e33a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对 L-BFGS 算法及其使用 ML.NET 实现的简短理论介绍。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/38b8a026207f17120e56a9678b4e2c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ijy8WKjlTAk5UwY8KdX7Kg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://www.pexels.com/@500photos-com-15338" rel="noopener ugc nofollow" target="_blank">500photos.com</a>从<a class="ae ky" href="https://www.pexels.com/photo/93405" rel="noopener ugc nofollow" target="_blank">派克斯</a></p></figure><p id="12da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">前一段时间，我发表了一篇关于使用 ML.NET 实现朴素贝叶斯的文章。今天继续这个系列，我将向你介绍有限记忆的 Broyden-Fletcher-goldf ARB-Shanno 方法。我将从一个理论开始，并解释这个方法是关于什么的，它是用来做什么的。</p><p id="8c12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">一些事实</strong></p><p id="5e8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该算法的创造者是豪尔赫·诺塞达尔。它是由阿贡国家实验室和西北大学的合资企业优化中心创建的。最初的<a class="ae ky" href="http://users.iems.northwestern.edu/~nocedal/lbfgs.html" rel="noopener ugc nofollow" target="_blank">源代码</a>是用 FORTRAN 写的。它被称为大规模无约束优化软件。正如您从名字中所料，这种方法类似于 BFGS，但是它使用的内存更少。因此，它非常适合大型数据集。</p><p id="a76e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">左旋 BFGS </strong></p><p id="0a7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个来自拟牛顿法族的算法。这些是求函数局部极值的算法，是基于牛顿求函数驻点的方法。</p><p id="7986" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">该做些数学了</strong></p><p id="ec43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这些方法中，使用二次近似来寻找最小值函数<em class="lv"> f(x)。</em>函数<em class="lv"> f(x) </em>的泰勒级数如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/998534c1ad67efe4315107f83bb9715a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*rAHw-fEbLLPwWdF8AEdjkA.png"/></div></figure><p id="cb73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中δ<strong class="lb iu"><em class="lv">f</em></strong>是函数的一个梯度，<strong class="lb iu"> <em class="lv"> H </em> </strong>是其黑森。</p><p id="ce2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">泰勒级数的梯度看起来是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/29a33d5bd3867c2ae09f4a59eab485cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*kS1q_Y3D_CpR5OEUSMPCPg.png"/></div></figure><p id="6e39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们想找到最小值，也就是解方程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/b54a18fa2f26b2775988d21ebcd8b790.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*iDmZ-nFhBi7qMAltuXGaGw.png"/></div></figure><p id="9434" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这里开始:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/e2efb9400619816d1e1e1efeb03cbafc.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*xZx4jAs6PkKB1QqR2aEERQ.png"/></div></figure><p id="5d12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，应该任命海森。属于这个家族的每个方法都有不同的指定方式。我想我们可以结束这部分了。我不想用这些公式来烦你，但是我想让你对这是怎么回事有一个简单的概念。</p><p id="90bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">BFGS 和 L-BFGS 的区别</strong></p><p id="8624" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我前面提到的，L-BFGS 算法适用于大型数据集，因为它比标准 BFGS 需要更少的内存。两种算法都使用 Hessian 逆矩阵估计来控制变量空间搜索。虽然 BFGS 存储了对逆黑森的密集<strong class="lb iu"><em class="lv">【n】</em></strong><em class="lv"/>x<strong class="lb iu"><em class="lv">n</em></strong>逼近，但 L-BFGS 只存储了几个隐式表示逼近的向量。这就是节省内存的区别。</p><p id="caa2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">数据集</strong></p><p id="ea17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在实验中使用了 UCI 机器学习库中的<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation" rel="noopener ugc nofollow" target="_blank">皮肤分割</a>数据集。所分析的数据集具有 3 个特征和 2 个类别。这些类确定样本是否被认为是皮肤。该数据集有 245057 个实例，因此 L-BFGS 算法在这里将完美地工作。</p><p id="21e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">实施</strong></p><p id="4b3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在创建了一个控制台应用程序项目并从 NuGet 包中下载了 ML.NET 之后，您可以继续进行实现和模型创建。开始时，您应该创建对应于数据集属性的类。清单中显示了创建的类:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="b47a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，您可以继续加载数据集，并将其分为训练集和测试集。我建议采用流行的划分，即 70%是训练集，30%是测试集。</p><pre class="kj kk kl km gt mc md me mf aw mg bi"><span id="c2ff" class="mh mi it md b gy mj mk l ml mm">var dataPath = "../../skin-segmentation.csv";</span><span id="12d5" class="mh mi it md b gy mn mk l ml mm">var ml = new MLContext();</span><span id="3514" class="mh mi it md b gy mn mk l ml mm">var DataView = ml.Data.LoadFromTextFile&lt;Features&gt;(dataPath, hasHeader: true, separatorChar: ',');</span><span id="42c8" class="mh mi it md b gy mn mk l ml mm">var partitions = ml.Data.TrainTestSplit(DataView, testFraction: 0.3);</span></pre><p id="c249" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在您需要使模型结构适应 model 图书馆提出的标准。这意味着指定类的属性必须称为 Label。其余的属性必须压缩在名称 Features 下。</p><pre class="kj kk kl km gt mc md me mf aw mg bi"><span id="a2e0" class="mh mi it md b gy mj mk l ml mm">var pipeline = ml.Transforms.Conversion.MapValueToKey(inputColumnName: "Class", outputColumnName:"Label")<br/>.Append(ml.Transforms.Concatenate("Features", "V1","V2","V3")).AppendCacheCheckpoint(ml);</span></pre><p id="8fc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在是建立培训渠道的时候了。在这里，选择 L-BFGS 形式的分类器，在参数中指定标签和要素的列名。您还指示了表示预测标签的属性。</p><pre class="kj kk kl km gt mc md me mf aw mg bi"><span id="f174" class="mh mi it md b gy mj mk l ml mm">var trainingPipeline = pipeline.Append(ml.MulticlassClassification.Trainers.LbfgsMaximumEntropy("Label","Features")).Append(ml.Transforms.Conversion.<br/>MapKeyToValue("PredictedLabel"));</span></pre><p id="615c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完成前面的步骤后，您现在可以开始训练和测试模型了:</p><pre class="kj kk kl km gt mc md me mf aw mg bi"><span id="d4b4" class="mh mi it md b gy mj mk l ml mm">var trainedModel = trainingPipeline.Fit(partitions.TrainSet);</span><span id="75c6" class="mh mi it md b gy mn mk l ml mm">var testMetrics = ml.MulticlassClassification.<br/>Evaluate(trainedModel.Transform(partitions.TestSet));</span></pre><p id="0520" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结果和总结</strong></p><p id="ccba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">L-BFGS 算法的准确率为 91.8%。结果似乎很好，但是当然，需要更深入的分析和使用其他度量来确认其价值。在本文中，我向您介绍了 L-BFGS 算法，并展示了如何通过 ML.NET 来使用它。它在 lines 的使用仅限于几行代码，但是我认为了解一下这个算法是关于什么的是值得的。</p></div></div>    
</body>
</html>