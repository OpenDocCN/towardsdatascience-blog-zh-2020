<html>
<head>
<title>LiDAR point cloud based 3D object detection implementation with colab{Part 2 of 2}</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 colab 实现基于激光雷达点云的三维目标检测(第 2 部分，共 2 部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lidar-point-cloud-based-3d-object-detection-implementation-with-colab-part-2-of-2-f3ad55c3f38c?source=collection_archive---------16-----------------------#2020-09-13">https://towardsdatascience.com/lidar-point-cloud-based-3d-object-detection-implementation-with-colab-part-2-of-2-f3ad55c3f38c?source=collection_archive---------16-----------------------#2020-09-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="a49b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用 google colab 实现了基于点云的三维物体检测算法。</p><p id="a1eb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我以前的文章中，我解释了实现体素网所需的关键概念，体素网是一种端到端的 3d 对象检测学习模型，您可以在这里找到</p><p id="5392" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">延续<a class="ae ko" href="https://medium.com/@gkadusumilli/lidar-point-cloud-based-3d-object-detection-implementation-with-colab-part-1-of-2-e3999ea8fdd4" rel="noopener">上一篇文章</a>，我们将使用 KITTI 点云数据实现三维物体检测的体素网算法</p><p id="b397" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第一步:处理 KITTI 数据集训练模型【详细步骤】</strong></p><p id="2622" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下步骤对于准备数据来训练模型至关重要。KITTI 数据集需要下载、裁剪、处理并保存在驱动器中。</p><p id="6b2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">登录<a class="ae ko" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> google colab </a>，打开笔记本，暂时不需要 GPU。</p><blockquote class="kp kq kr"><p id="74de" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated">注意:如果选择了 GPU，请注意，每个用户都有大约 30GB 的虚拟空间，整个数据集的大小为大约 40+ GB，然后解压缩数据集的内存将耗尽。在接下来的步骤中，我们将下载 KITTI 数据集、处理、裁剪和创建。压缩并移动到驱动器，以便将来使用这些数据</p></blockquote><ul class=""><li id="c047" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">必需的数据集</li></ul><ol class=""><li id="61a7" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lf lc ld le bi translated">威力登点云(29 GB):将数据输入到体素网</li></ol><p id="eb0f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.对象数据集的训练标签(5 MB):体素网的输入标签</p><p id="89da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.目标数据集的摄像机校准矩阵(16 MB):用于预测的可视化</p><p id="03d3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.对象数据集的左侧彩色图像(12 GB):用于预测的可视化</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="a235" class="lp lq it ll b gy lr ls l lt lu">#clone the voxelnet git repo<br/>!git clone <a class="ae ko" href="https://github.com/gkadusumilli/Voxelnet.git" rel="noopener ugc nofollow" target="_blank">https://github.com/gkadusumilli/Voxelnet.git</a><br/>#change the cwd<br/>%cd /content/drive/My Drive/Voxelnet/crop_data</span></pre><ul class=""><li id="0672" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">下载数据集(。zip)直接加载到 colab 虚拟机[~ 15–20 分钟]</li></ul><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="d38e" class="lp lq it ll b gy lr ls l lt lu">#Data label file<br/>!wget (enter the link)</span><span id="b71c" class="lp lq it ll b gy lv ls l lt lu">#Calib file<br/>!wget (enter the link )</span><span id="112a" class="lp lq it ll b gy lv ls l lt lu">#Velodyne file<br/>!wget (enter the link)</span><span id="736f" class="lp lq it ll b gy lv ls l lt lu"># Image file<br/>!wget (enter the link)</span></pre><ul class=""><li id="eeb2" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">解压缩数据集(。zip)到文件夹中(大约 20- 30 分钟)</li></ul><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="98be" class="lp lq it ll b gy lr ls l lt lu">#Unzip the velodyne training folder<br/>!unzip /content/Voxelnet/crop_data/data_object_velodyne.zip 'training/*' -d /content/Voxelnet/crop_data</span><span id="8448" class="lp lq it ll b gy lv ls l lt lu">#Unzip the image training folder<br/>!unzip /content/Voxelnet/crop_data/data_object_image_2.zip 'training/*' -d /content/Voxelnet/crop_data</span><span id="d557" class="lp lq it ll b gy lv ls l lt lu">#unzip the object label<br/>!unzip /content/Voxelnet/crop_data/data_object_label_2.zip</span><span id="fcb1" class="lp lq it ll b gy lv ls l lt lu">#unzip the data object calib<br/>!unzip /content/Voxelnet/crop_data/data_object_calib.zip 'training/*' -d /content/Voxelnet/crop_data</span></pre><ul class=""><li id="8e8e" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">用于训练和验证的裁剪点云数据。图像坐标外的点云被移除。裁剪的点云将覆盖现有的原始点云[40–45 分钟]</li></ul><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="7004" class="lp lq it ll b gy lr ls l lt lu">#to run the 'crop.py' lower version of scipy is needed<br/>!pip install scipy==1.1.0</span><span id="6b06" class="lp lq it ll b gy lv ls l lt lu">#run crop.py<br/>!python crop.py</span></pre><ul class=""><li id="8a97" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">创建验证数据以评估模型[~ 10–15 分钟]</li></ul><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="0da5" class="lp lq it ll b gy lr ls l lt lu">#create a folder 'validation' and copy the content in training #folder</span><span id="c970" class="lp lq it ll b gy lv ls l lt lu">!mkdir /content/Voxelnet/crop_data/validation</span><span id="2bce" class="lp lq it ll b gy lv ls l lt lu">%cp -av /content/Voxelnet/crop_data/training /content/Voxelnet/crop_data/validation/</span></pre><ul class=""><li id="b507" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">我们将根据协议<a class="ae ko" href="https://xiaozhichen.github.io/files/mv3d/imagesets.tar.gz" rel="noopener ugc nofollow" target="_blank">在这里</a>【2-3 分钟】分开训练</li></ul><figure class="lg lh li lj gt lw"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="7237" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后一步…</p><ul class=""><li id="779d" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">创造。压缩已处理、裁剪数据的文件夹[30–40 分钟]</li></ul><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="6828" class="lp lq it ll b gy lr ls l lt lu">!zip -r /content/VoxelNet/data/data_lidar.zip /content/VoxelNet/crop_data</span></pre><ul class=""><li id="eb29" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">移动。压缩文件夹到驱动器[~5 分钟]</li></ul><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="f776" class="lp lq it ll b gy lr ls l lt lu">#rename the folder as you need in the drive, I've stored in dtive with the folder named 'AI'</span><span id="e72e" class="lp lq it ll b gy lv ls l lt lu">!mv "/content/VoxelNet/data/data_lidar.zip" "/content/gdrive/My Drive/AI"</span></pre><blockquote class="kp kq kr"><p id="bd7d" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated">下面是<a class="ae ko" href="https://github.com/gkadusumilli/Voxelnet/blob/master/VoxelNet_data_creation.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>到 colab jupyter 笔记本的所有上述步骤。</p></blockquote></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><h2 id="38d9" class="lp lq it bd mg mh mi dn mj mk ml dp mm kb mn mo mp kf mq mr ms kj mt mu mv mw bi translated">步骤 2:训练模型</h2><ul class=""><li id="3858" class="kw kx it js b jt mx jx my kb mz kf na kj nb kn lb lc ld le bi translated">登录<a class="ae ko" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> google colab </a>，创建一个新的笔记本</li><li id="3053" class="kw kx it js b jt nc jx nd kb ne kf nf kj ng kn lb lc ld le bi translated">要访问 GPU:点击<strong class="js iu">运行时&gt;更改运行时类型&gt; GPU </strong></li></ul><blockquote class="kp kq kr"><p id="9e58" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated">注意:colab GPU 运行时间大约在 12 小时后，运行时间将被断开，存储的数据将会丢失。因为在我们的情况下，每个历元将花费大约 2 小时，并且需要训练超过 20 个历元来观察初步结果。所以将使用 google drive 作为存储代码、检查点、预测结果等的路径</p></blockquote><ul class=""><li id="a64b" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">将当前工作目录(CWD)更改为 drive</li></ul><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="a443" class="lp lq it ll b gy lr ls l lt lu">#voxelnet is the folder name, you can rename as you need<br/>%cd /content/drive/My Drive/Voxelnet</span></pre><ul class=""><li id="1b47" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">体素网的实现需要几个依赖项。因此，我们将克隆整个存储库。</li></ul><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="63e3" class="lp lq it ll b gy lr ls l lt lu">!git clone <a class="ae ko" href="https://github.com/gkadusumilli/Voxelnet.git" rel="noopener ugc nofollow" target="_blank">https://github.com/gkadusumilli/Voxelnet.git</a></span></pre><ul class=""><li id="a704" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">建立档案</li></ul><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="2479" class="lp lq it ll b gy lr ls l lt lu">%cd /content/drive/My Drive/Voxelnet</span><span id="6cc4" class="lp lq it ll b gy lv ls l lt lu">!python setup.py build_ext --inplace</span></pre><ul class=""><li id="a58e" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">将处理后的数据集解压缩到 crop_data 文件夹中[35–40 分钟]</li></ul><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="b3a0" class="lp lq it ll b gy lr ls l lt lu">%cd /content/drive/My Drive/Voxelnet/crop_data</span><span id="a966" class="lp lq it ll b gy lv ls l lt lu">#Locate the zip folder in the drive and unzip using !unzip command<br/>!unzip "/content/drive/My Drive/AI/data_lidar.zip"</span></pre><figure class="lg lh li lj gt lw gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/97d2eb3a3332c3893749a39eeb5f7f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*XhgowBuyJ9H3SXXa2OLxwQ.png"/></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">数据文件夹结构</p></figure><ul class=""><li id="308b" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">训练模型</li></ul><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="f554" class="lp lq it ll b gy lr ls l lt lu">%cd /content/drive/My Drive/Voxelnet/</span></pre><p id="854b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">重要参数解析</p><figure class="lg lh li lj gt lw"><div class="bz fp l di"><div class="lx ly l"/></div></figure><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="9eb0" class="lp lq it ll b gy lr ls l lt lu">!python train.py \</span><span id="6ac0" class="lp lq it ll b gy lv ls l lt lu">--strategy="all" \</span><span id="5692" class="lp lq it ll b gy lv ls l lt lu">--n_epochs=16 \</span><span id="a90f" class="lp lq it ll b gy lv ls l lt lu">--batch_size=2 \</span><span id="bb25" class="lp lq it ll b gy lv ls l lt lu">--learning_rate=0.001 \</span><span id="8716" class="lp lq it ll b gy lv ls l lt lu">--small_addon_for_BCE=1e-6 \</span><span id="ea1d" class="lp lq it ll b gy lv ls l lt lu">--max_gradient_norm=5 \</span><span id="c7eb" class="lp lq it ll b gy lv ls l lt lu">--alpha_bce=1.5 \</span><span id="ca3c" class="lp lq it ll b gy lv ls l lt lu">--beta_bce=1 \</span><span id="7ac8" class="lp lq it ll b gy lv ls l lt lu">--huber_delta=3 \</span><span id="9e32" class="lp lq it ll b gy lv ls l lt lu">#if dump_vis == yes, boolean to save visualization results<br/>--dump_vis="no" \</span><span id="c07f" class="lp lq it ll b gy lv ls l lt lu">--data_root_dir="/content/drive/My Drive/Voxelnet/crop_data" \</span><span id="9fc2" class="lp lq it ll b gy lv ls l lt lu">--model_dir="model" \</span><span id="d4f4" class="lp lq it ll b gy lv ls l lt lu">--model_name="model6" \</span><span id="b299" class="lp lq it ll b gy lv ls l lt lu">--dump_test_interval=3 \</span><span id="567e" class="lp lq it ll b gy lv ls l lt lu">--summary_interval=2 \</span><span id="9a55" class="lp lq it ll b gy lv ls l lt lu">--summary_val_interval=40 \</span><span id="db40" class="lp lq it ll b gy lv ls l lt lu">--summary_flush_interval=20 \</span><span id="b04a" class="lp lq it ll b gy lv ls l lt lu">--ckpt_max_keep=10 \</span></pre><h2 id="fd54" class="lp lq it bd mg mh mi dn mj mk ml dp mm kb mn mo mp kf mq mr ms kj mt mu mv mw bi translated">用张量板可视化测井方向</h2><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="2d52" class="lp lq it ll b gy lr ls l lt lu">%load_ext tensorboard</span><span id="1a98" class="lp lq it ll b gy lv ls l lt lu">#summary_logdir is the logdir name<br/>%tensorboard --logdir summary_logdir</span></pre><p id="f6e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面的快照是日志目录结果@ epoch6</p><figure class="lg lh li lj gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi no"><img src="../Images/e13bdac71f6fa6d796448a878df7acbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2RvxEuVkXIUFxokcIhQXPg.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">张量板-logdir</p></figure><h2 id="5f6c" class="lp lq it bd mg mh mi dn mj mk ml dp mm kb mn mo mp kf mq mr ms kj mt mu mv mw bi translated">评估模型</h2><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="aee0" class="lp lq it ll b gy lr ls l lt lu">!python predict.py \<br/>--strategy="all" \<br/>--batch_size=2 \<br/>--dump_vis="yes" \<br/>--data_root_dir="../DATA_DIR/T_DATA/" \<br/>--dataset_to_test="validation" \<br/>--model_dir="model" \<br/>--model_name="model6" \<br/>--ckpt_name="" \</span></pre><ul class=""><li id="aad9" class="kw kx it js b jt ju jx jy kb ky kf kz kj la kn lb lc ld le bi translated">下载、裁剪和处理 KITTI 数据集的代码可以在这里找到<a class="ae ko" href="https://github.com/gkadusumilli/Voxelnet/blob/master/VoxelNet_data_creation.ipynb" rel="noopener ugc nofollow" target="_blank"/></li><li id="1edc" class="kw kx it js b jt nc jx nd kb ne kf nf kj ng kn lb lc ld le bi translated">从第 2 步开始执行的代码可以在<a class="ae ko" href="https://github.com/gkadusumilli/Voxelnet/blob/master/VoxelNet_implementation.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到</li></ul><p id="bd7c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是对模型进行 30 个时期的训练后获得的一些结果。</p><figure class="lg lh li lj gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nt"><img src="../Images/6ef01919727747b9f466ba1ab6f6135a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dLOztRDbQ57BS5EUNdBdVQ.jpeg"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">所有车辆都被检测到</p></figure><figure class="lg lh li lj gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nt"><img src="../Images/2b0566adb2c008a81c82a18224db380f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*05-AUPeCNUuE0ptPQ9PfAQ.jpeg"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">检测到两辆车</p></figure><figure class="lg lh li lj gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nt"><img src="../Images/adbbb88c64cb27271e8cb435702ce554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y9wu8PgdKD6yAZP8AyGB2A.jpeg"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">一辆车不见了</p></figure><p id="912b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">特别感谢<a class="ae ko" href="https://github.com/tsinghua-rll" rel="noopener ugc nofollow" target="_blank">清华机器人学习实验室</a>、<a class="ae ko" href="https://github.com/qianguih" rel="noopener ugc nofollow" target="_blank">黄千贵</a>、<a class="ae ko" href="https://github.com/steph1793" rel="noopener ugc nofollow" target="_blank">大卫·夏羽</a>为实现体素网做出的宝贵贡献</p><p id="1e39" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">特别感谢:</strong></p><p id="a008" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">Uma K Mudenagudi 博士，KLE 理工大学，项目导师。</strong></p><p id="2438" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参考资料:</p><p id="55ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">KITTI 原始数据集:@ ARTICLE {<a class="ae ko" href="http://www.cvlibs.net/publications/Geiger2013IJRR.pdf" rel="noopener ugc nofollow" target="_blank">Geiger 2013 jrr</a>，作者= { <a class="ae ko" href="http://www.cvlibs.net/" rel="noopener ugc nofollow" target="_blank"> Andreas Geiger </a>和<a class="ae ko" href="http://www.mrt.kit.edu/mitarbeiter_lenz.php" rel="noopener ugc nofollow" target="_blank"> Philip Lenz </a>和<a class="ae ko" href="http://www.mrt.kit.edu/mitarbeiter_stiller.php" rel="noopener ugc nofollow" target="_blank"> Christoph Stiller </a>和<a class="ae ko" href="http://ttic.uchicago.edu/~rurtasun" rel="noopener ugc nofollow" target="_blank"> Raquel Urtasun </a> }，标题= {视觉与机器人:KITTI 数据集}，期刊= {国际机器人研究期刊(IJRR)}，年份= {2013}</p></div></div>    
</body>
</html>