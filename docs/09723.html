<html>
<head>
<title>Logistic Regression-Theory and Practice</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归-理论与实践</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-theory-and-practice-6442ed0692ff?source=collection_archive---------25-----------------------#2020-07-10">https://towardsdatascience.com/logistic-regression-theory-and-practice-6442ed0692ff?source=collection_archive---------25-----------------------#2020-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/e50d2a34c3ca65fe4bc17f9b6ffc649a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IbzEZ7kZQcAet_b0"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://unsplash.com/@ja_ma?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> ja ma </a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h1 id="4bf6" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">简介:</h1><p id="f4f8" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这篇文章中，我将解释如何使用回归的概念，在具体的逻辑回归涉及分类的问题。分类问题在我们身边无处不在，经典的问题包括邮件分类、天气分类等。如果需要，所有这些数据可以用于训练逻辑回归模型，以预测任何未来实例的类别。</p><h1 id="139b" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">背景:</h1><p id="3e34" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">本文将涵盖以下子主题:</p><ol class=""><li id="171d" class="mc md it lg b lh me ll mf lp mg lt mh lx mi mb mj mk ml mm bi translated">分类问题介绍。</li><li id="ccaf" class="mc md it lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">逻辑回归及其所有性质，如假设、决策边界、成本、成本函数、梯度下降及其必要分析。</li><li id="1a8a" class="mc md it lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">使用 python、pandas、matplotlib 和 seaborn 从头开始开发逻辑回归模型，并在乳腺癌数据集上对其进行训练。</li><li id="7ce1" class="mc md it lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">使用乳腺癌数据集训练来自 sklearn 的内置逻辑回归模型，以验证之前的模型。</li></ol><h1 id="b0b9" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">分类问题介绍:</strong></h1><p id="c3ff" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">分类问题可以基于乳腺癌数据集来解释，其中有两种类型的肿瘤(良性和恶性)。它可以表示为:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/5151c7b4eda3b90b49ee749b93751206.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*PnYbzHBTyq_I7tSDMBC3eg.png"/></div></figure><p id="a38b" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">在哪里</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/01fae3c3a33ffc6dfcc288ce2ae35b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*i6zrprfzcLSIPRNYFKA9jA.png"/></div></figure><p id="4f8d" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">这是一个有两类的分类问题，0 和 1。通常，分类问题有多个类别，比如 0、1、2 和 3。</p><h1 id="1f00" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">数据集:</h1><p id="34cf" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">本文中使用的乳腺癌数据集的链接如下:</p><div class="nb nc gp gr nd ne"><a href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">乳腺癌威斯康星州(诊断)数据集</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">预测癌症是良性还是恶性</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.kaggle.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns jz ne"/></div></div></a></div><ol class=""><li id="7f4d" class="mc md it lg b lh me ll mf lp mg lt mh lx mi mb mj mk ml mm bi translated">让我们将数据集导入熊猫数据框架:</li></ol><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="2c07" class="ny kh it nu b gy nz oa l ob oc">import pandas as pd<br/>read_df = pd.read_csv('breast_cancer.csv')<br/>df = read_df.copy()</span></pre><p id="e3ea" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">2.获得以下数据帧:</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="290b" class="ny kh it nu b gy nz oa l ob oc">df.head()</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/658bed8335e83ce1bbe6af386f5b2462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PPyiGgocvjHbgIcs9yTWTA.png"/></div></div></figure><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="13b3" class="ny kh it nu b gy nz oa l ob oc">df.info()</span><span id="7749" class="ny kh it nu b gy oe oa l ob oc">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 569 entries, 0 to 568<br/>Data columns (total 33 columns):<br/> #   Column                   Non-Null Count  Dtype  <br/>---  ------                   --------------  -----  <br/> 0   id                       569 non-null    int64  <br/> 1   diagnosis                569 non-null    object <br/> 2   radius_mean              569 non-null    float64<br/> 3   texture_mean             569 non-null    float64<br/> 4   perimeter_mean           569 non-null    float64<br/> 5   area_mean                569 non-null    float64<br/> 6   smoothness_mean          569 non-null    float64<br/> 7   compactness_mean         569 non-null    float64<br/> 8   concavity_mean           569 non-null    float64<br/> 9   concave points_mean      569 non-null    float64<br/> 10  symmetry_mean            569 non-null    float64<br/> 11  fractal_dimension_mean   569 non-null    float64<br/> 12  radius_se                569 non-null    float64<br/> 13  texture_se               569 non-null    float64<br/> 14  perimeter_se             569 non-null    float64<br/> 15  area_se                  569 non-null    float64<br/> 16  smoothness_se            569 non-null    float64<br/> 17  compactness_se           569 non-null    float64<br/> 18  concavity_se             569 non-null    float64<br/> 19  concave points_se        569 non-null    float64<br/> 20  symmetry_se              569 non-null    float64<br/> 21  fractal_dimension_se     569 non-null    float64<br/> 22  radius_worst             569 non-null    float64<br/> 23  texture_worst            569 non-null    float64<br/> 24  perimeter_worst          569 non-null    float64<br/> 25  area_worst               569 non-null    float64<br/> 26  smoothness_worst         569 non-null    float64<br/> 27  compactness_worst        569 non-null    float64<br/> 28  concavity_worst          569 non-null    float64<br/> 29  concave points_worst     569 non-null    float64<br/> 30  symmetry_worst           569 non-null    float64<br/> 31  fractal_dimension_worst  569 non-null    float64<br/> 32  Unnamed: 32              0 non-null      float64<br/>dtypes: float64(31), int64(1), object(1)<br/>memory usage: 146.8+ KB</span></pre><h2 id="96e2" class="ny kh it bd ki of og dn km oh oi dp kq lp oj ok ku lt ol om ky lx on oo lc op bi translated">数据分析:</h2><p id="d253" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们画出丛的平均面积和它的分类，看看能否找到它们之间的关系。</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="1c59" class="ny kh it nu b gy nz oa l ob oc">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn import preprocessing<br/>label_encoder = preprocessing.LabelEncoder()<br/>df.diagnosis = label_encoder.fit_transform(df.diagnosis)<br/>sns.set(style = 'whitegrid')<br/>sns.lmplot(x = 'area_mean', y = 'diagnosis', data = df, height = 10, aspect = 1.5, y_jitter = 0.1)</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oq"><img src="../Images/fd8b5fd13ca1f053bbf5440b5bcf50bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4GwcY9g8eF9o5DT_IwQCdw.png"/></div></div></figure><p id="754b" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们可以从图中推断出，大多数面积小于 500 的肿瘤是良性的(用 0 表示)，而面积大于 1000 的肿瘤是恶性的(用 1 表示)。平均面积在 500 到 1000 之间的肿瘤既有良性的也有恶性的，因此表明分类取决于除平均面积之外的更多因素。还绘制了线性回归线用于进一步分析。</p><h1 id="f0e9" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">为什么线性回归不是完美的拟合？</h1><p id="bc3f" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果我们要基于绘制的线性回归线进行预测，我们可以将阈值分类器输出值设置为 0.5。</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi or"><img src="../Images/2b415bab9ef03a8c53cc181be4b5b4e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*w3yX9lOTUogvXB8_uJ306g.png"/></div></figure><p id="d4a0" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">这意味着平均面积小于阈值(y 轴上对应于 0.5 的面积)的肿瘤将被分类为良性，而平均面积大于阈值的肿瘤将被分类为恶性。</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi os"><img src="../Images/60a5eb31c78fa934e06873eae855d82d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ivW9WGyzjQFGsS3yt9mAhQ.png"/></div></div></figure><p id="4022" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">但是从图中，我们可以陈述线性回归不是完美模型的以下原因。</p><ol class=""><li id="be66" class="mc md it lg b lh me ll mf lp mg lt mh lx mi mb mj mk ml mm bi translated">对于具有小于 0.5 的相应假设值的 x 值，将其预测为 0(或恶性)以及反之亦然是不合适的。</li><li id="742e" class="mc md it lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">我们还可以看到，假设值大于 1，在某些情况下小于 0，这不可能是真的(因为只有两个类 0 和 1)。</li></ol><p id="759b" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">这就是逻辑回归发挥作用的地方。逻辑回归是一种专门用于分类问题的回归模型，即输出值是离散的。</p><h1 id="95fd" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">逻辑回归简介:</h1><p id="a387" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们从上面的部分观察到，当使用线性回归时，假设值不在[0，1]的范围内。这是逻辑回归的基本要求。</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/9daf2c2c8c1290655ba3e1ecc913badf.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*jmiDtuatuYY7I5gw1HmBrw.png"/></div></figure><p id="4340" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">这意味着所有的预测都应该在 0 和 1 之间。</p><h1 id="20f5" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">假设:</h1><p id="10a4" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">线性回归的假设由下式给出:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/d8d2e58d1a3ccb5e7b0348a8f7fe34f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*GGCPcus6ebJOwHOxqSLD5w.png"/></div></figure><p id="fefa" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">对于逻辑回归，上述假设稍加修改:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/eecad0ffd75c2bd6924ef01f52f1ee58.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*kKqBMI_dme5ztJzI-iDq_w.png"/></div></figure><p id="6f3e" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">其中:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/851b722ba3a740a7efb7bba5abef44fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*K0Dqux27c6LaWCcECaLHPQ.png"/></div></figure><p id="b0a6" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">z 是一个实数。g(z)被称为<strong class="lg iu"> Sigmoid 函数或逻辑函数</strong>(这给了我们逻辑回归的名字)。</p><p id="bc9b" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">将上面给出的两个等式放在一起:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/0fe303a6f7c218a4d47b1877e659b35f.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*6exGLGd41oD5urqXdVtdqg.png"/></div></figure><h1 id="100c" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">标绘物流功能:</strong></h1><p id="39eb" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们看看逻辑函数的形状:</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="b02d" class="ny kh it nu b gy nz oa l ob oc">def sigmoid(x):<br/> return 1/(1+ np.exp(-x))<br/>x = np.linspace(-10,10,num = 1000)<br/>fig = plt.figure(figsize = (10,10))<br/>sns.set(style = 'whitegrid')<br/>sns.lineplot(x = x, y = sigmoid(x))</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/4dce7bf65e47e192fed5b0ce3c751ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*FFh3OWDPGfK7ctvQGAKS6g.png"/></div></figure><p id="cbfa" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们可以从图表中推断出以下几点:</p><ol class=""><li id="c733" class="mc md it lg b lh me ll mf lp mg lt mh lx mi mb mj mk ml mm bi translated">它在 0.5 处穿过 y 轴。</li><li id="77aa" class="mc md it lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">当 z 趋于无穷大时，逻辑函数在 1 处渐近，当 z 趋于负无穷大时，逻辑函数在 0 处渐近。</li><li id="4ab7" class="mc md it lg b lh mn ll mo lp mp lt mq lx mr mb mj mk ml mm bi translated">当 g(z)取(0，1)范围内的值时，h(x)的值也位于(0，1)之间。</li></ol><p id="9c18" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">假设是这样的:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/0fe303a6f7c218a4d47b1877e659b35f.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*6exGLGd41oD5urqXdVtdqg.png"/></div></figure><p id="c2c4" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们需要找到适合训练示例的参数，以便可以使用假设进行预测。</p><h1 id="efb6" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">假设的解释:</h1><p id="8ada" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">假设 h(x)给出的输出可以解释为对于给定的输入 x，y = 1 的概率。</p><p id="3e4b" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">以乳腺癌数据集为例。特征平均面积的特征向量可以形成为:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/6a06cfe7ae839c5960969b835e64da56.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*yQ--XhciovBEFLmBmDk2LA.png"/></div></figure><p id="488f" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">假设假设 h(x)给我们的上述特征向量的值是 0.7。因为我们用 1 表示恶性肿瘤，用 0 表示良性肿瘤，所以我们可以说肿瘤是恶性的概率是 70%。</p><p id="b15e" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">这可以用数学方法表示如下:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/b8d395d62206b80f3a713866f49ab94f.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*nOXq_xT70kOlr0gOEavQPQ.png"/></div></figure><p id="361a" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">RHS 可以读作</p><blockquote class="pb"><p id="78eb" class="pc pd it bd pe pf pg ph pi pj pk mb dk translated">给定 x，用θ参数化，y=1 的概率</p></blockquote><p id="b009" class="pw-post-body-paragraph le lf it lg b lh pl lj lk ll pm ln lo lp pn lr ls lt po lv lw lx pp lz ma mb im bi translated">由于这是一个分类问题，我们知道 y 只能等于两个值 0 或 1(在这种情况下)。这既适用于训练示例，也适用于我们做出的任何未来预测。因此，给定假设 h(x ),我们也可以如下计算 y=0 的概率:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/1fdeec36462235079f312ad1459d0be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*oJgv4EVf4DOAMNTuSo4UWQ.png"/></div></figure><h1 id="2169" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">进一步分析假设:</h1><p id="c6ef" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">考虑我们上面绘制的逻辑函数:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/29b67f8737e565c2bb1a37b15d1d826b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*oFYjxw6UyiNwIflLz7-DYQ.png"/></div></figure><p id="6e96" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">假设做出 y=1 和 y=0 的预测时，我们更好理解。</p><p id="f879" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">假设预测 y=1 发生在</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pr"><img src="../Images/97de3f77fb6269458cbfd5edb8a2abe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*aDNOwqtzPYpvfg0-Gbahcg.png"/></div></div></figure><p id="e3f8" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">从该图中，我们注意到上述情况发生为:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/f9d330863df6d50c1cb724c5cf28be0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*_a2tXlPUOCmxHdv9Th60Ow.png"/></div></figure><p id="9b18" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">即，当 z 为正时，g(z)取大于 0.5 的值。</p><p id="b1f7" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">因此考虑到我们的假设 h(x ),我们可以说:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/e34af51ac20416e058d0e43e69d8e7bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*bDgph8HjbgwIsDYUVKGp4w.png"/></div></figure><p id="88e5" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">类似地，假设预测 y=0 发生在</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/e1571f0398ef914bfa7446280e116b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*Oa7GhFBAaE3xHRP6KVNPOA.png"/></div></figure><p id="97d5" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">通过与上面类似的论证:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/b8c37ac06303c255929a71b248ad85b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*k__S--Vlmq0svTRivKhXDA.png"/></div></figure><p id="a7a3" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们可以利用上述结论更好地理解逻辑回归的假设是如何做出预测的。</p><h1 id="88a6" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">决策界限:</h1><p id="950e" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">考虑下面显示的简单训练集:</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="5276" class="ny kh it nu b gy nz oa l ob oc">os = np.array([[0, 0.5], [0.5, 0.5], [1.5, 0.5], [1, 0.5], [0.5, 0.5], [0.5, 1.5], [0, 1.5], [0,2.5]])<br/>xs = np.array([[1.5, 3], [2, 3.5], [2, 3], [2, 2.5], [2.5, 3.5], [3, 3], [3, 1.5], [3, 2], [3, 1], [3.5, 1.5]])<br/>fig = plt.figure(figsize = (10,10))<br/>sns.set(style = 'whitegrid')<br/>ax = sns.scatterplot(x = os[:,0], y = os[:,1], marker = 's', s = 100, color = 'r')<br/>ax = sns.scatterplot(x = xs[:,0], y = xs[:,1], marker = 'x', s = 100, color = 'k')<br/>ax.set(xlabel = 'x1', ylabel = 'x2')</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/6f112747fb53af7983117510305c823e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*mTaw0yk2yn6VTSv9cIiZjA.png"/></div></figure><p id="a222" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们假设 X 对应于 y = 1，正方形对应于 y = 0。认为它的假设是:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/40cc6a4daadb4ac8ad5ef2e7cc65963a.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*r_uNNIw4tBPzrAYAAfMh8Q.png"/></div></figure><p id="ffd6" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">其中 x1 和 x2 是两个特征。</p><p id="9541" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">假设我们最终选择了符合方程的参数(选择参数的过程将在后面讨论):</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi px"><img src="../Images/e958bdc130ceb829b4f700f5736d7ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*xq4_slOb0vDFhGsxOExs-g.png"/></div></figure><p id="76bf" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">参数向量会是:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi py"><img src="../Images/700078db88e95182f1c79deb2091e4c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*b0uQ0csAfFgha9rs0KaAZw.png"/></div></figure><p id="bba2" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">参考上一节中的论点，预测 y=1 发生在以下情况:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/e34af51ac20416e058d0e43e69d8e7bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*bDgph8HjbgwIsDYUVKGp4w.png"/></div></figure><p id="b938" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">根据我们最后得到的参数，我们得到</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi px"><img src="../Images/73e516546e98d5746ab283481bc36eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*HLLYxofDCX1vzG3g4PZMaA.png"/></div></figure><p id="d1e9" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">绘制直线 x1+x2 = 3:</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="c01a" class="ny kh it nu b gy nz oa l ob oc">x = np.linspace(0, 3, num = 10)<br/>ax = sns.lineplot(x = x, y = 3-x)</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/7527da9bb717b3015668bc3ceb305c8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*Zwl9MwUD5mJtcS_GSLAmCA.png"/></div></figure><p id="43c6" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们可以看到，线右侧的区域对应于条件 x1+x2≥3，假设预测 y=1，如果在该区域，反之亦然。划分两个区域的这条线被称为该训练数据集的判定边界，并且它精确地对应于 h(x)=0.5。</p><p id="2f42" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">非常清楚的是，决策边界和区域是假设及其参数的属性，而不是训练数据集的属性。</p><h1 id="b9f3" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">非线性决策界限:</h1><p id="f81b" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">考虑下面给出的有点复杂的数据集:</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="04f7" class="ny kh it nu b gy nz oa l ob oc">os = np.array([[0,0], [0,0.5], [0.5,0.5], [0.5,0], [-0.5,0.5], [-0.5,0], [-0.5,-0.5], [0,-0.5], [0.5, -0.5],])<br/>xs = np.array([[1,1], [-1,1], [1,-1], [-1,-1], [0,1.5], [-1.5,0], [0,-1.5], [1.5,0]])<br/>fig = plt.figure(figsize = (10,10))<br/>sns.set(style = ‘whitegrid’)<br/>ax = sns.scatterplot(os[:,0], os[:,1], marker = ‘s’, s = 100, color = ‘r’)<br/>ax = sns.scatterplot(xs[:,0], xs[:,1], marker = ‘x’, s = 100, color = ‘k’)</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/54dde583940f0f83c0b2ed9415b06858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*muFqjdVDjkus8DIizUBWHw.png"/></div></figure><p id="f4a3" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">如上例所示，X 属于区域 y=1，正方形属于区域 y=0。假设我们的假设是这样的:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/445099d9451b110047954d12208ffd85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*2YmpVEdYY1NaB7uoyXjvZw.png"/></div></figure><p id="d1da" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">假设我们最终选择参数值为:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/0b17804d1288eca70b27b563c0a061c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*5H1xSQ8IX9r8AYLkAvHTCg.png"/></div></figure><p id="ba02" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">那么我们的参数向量看起来像:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/e6ddd18450ecd6ddbdcce98217ef745b.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*Fi9_bdTmNqto_da4ilWSrA.png"/></div></figure><p id="5779" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">根据我们之前的讨论，假设将预测 y=1，当:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/685e47d8b610d083d24ec96a649b49e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*zmtSTIvtTuZjQdHYB_1uzw.png"/></div></figure><p id="d8be" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们可以看到，上面的等式对应的是一个以原点为圆心，半径为 1 的圆。</p><p id="8c58" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">绘制方程式:</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="48ac" class="ny kh it nu b gy nz oa l ob oc">theta = np.linspace(0, 2*np.pi , 1000)<br/>r = np.sqrt(1)<br/>x = r*np.cos(theta)<br/>y = r*np.sin(theta)<br/>ax.plot(x,y, color = 'b')</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/6088f213db829691a7b24d84555ebe54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*uu8JbD0Dou7N9aaqX0fK4w.png"/></div></figure><p id="7f5f" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">圆圈是我们的决策边界，圆圈外的区域对应于 y=1，圆圈内的区域对应于 y=0。我们可以看到，决策边界不一定是直线，也可以是更复杂的形状，如圆形、椭圆形和任何其他不规则形状。同样，决策边界是假设及其参数的属性，而不是训练数据集的属性。</p><h1 id="f4d0" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">成本:</h1><p id="51c9" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">逻辑回归问题的成本由下式给出:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/e62394f533f80b84043be784fab85f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*IilwMmg2ttxEXNPlHYsL0g.png"/></div></figure><p id="aa7d" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">其中，如前所述，h(x)是假设的预测，y 是实际的类别标签。</p><h1 id="16b4" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">绘制成本图:</h1><p id="49c7" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们画出这个函数，看看它是如何对应每种情况的。我们需要将 h(x)的极限设为[0，1]，因为它位于逻辑回归的范围内。</p><ol class=""><li id="7acb" class="mc md it lg b lh me ll mf lp mg lt mh lx mi mb mj mk ml mm bi translated">如果 y=1</li></ol><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="09ad" class="ny kh it nu b gy nz oa l ob oc">x = np.linspace(0,1, num = 100)<br/>fig = plt.figure(figsize = (10,10))<br/>sns.set(style = 'whitegrid')<br/>ax = sns.lineplot(x = x, y = -np.log(x))<br/>ax.set(xlabel = 'h(x)', ylabel = 'cost')</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/4f7428b5a90a28fe3f1d182c92ca67bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*O1Xhm5MV_TxiQD2RYQKHFQ.png"/></div></figure><p id="b401" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">从这个情节可以得出以下推论:</p><p id="07b1" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">a.当 h(x)=1 且 y=1 时，成本变为零。这是显而易见的，因为假设预测 y 为 1，这是真的，那么成本将为零。</p><p id="a6b3" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">b.当 h(x)接近零时，成本趋于无穷大。这种情况的发生是因为该图特定于 y=1，但是当 h(x)预测其为 0 时，成本趋于无穷大。</p><p id="1081" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">2.如果 y=0</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="b81f" class="ny kh it nu b gy nz oa l ob oc">ax = sns.lineplot(x = x, y = -np.log(1-x))</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/9529f3ad704472f63ec428c15a863540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*7Mruatqf2650SMWIKiRrHg.png"/></div></figure><p id="48d1" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">如上所述，可以推断出以下情况:</p><p id="352f" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">a.当 h(x)=0 且 y=0 时，成本变为零。</p><p id="ac9c" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">b.由于 y=0，当 h(x)接近 1 时，成本趋于无穷大。</p><h1 id="668e" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">成本函数:</h1><p id="b024" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">逻辑回归的成本函数由下式给出:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/daa4bbba01a753b656809e35356a126d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*KX0DtSG3lmKsIvQCTs9DVQ.png"/></div></figure><p id="f13c" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">需要注意的是，在有两个类别的分类问题中，y=0 或 1 总是存在的。</p><p id="8817" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">成本可以用一行表示，如下所示:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi qk"><img src="../Images/24e8172c3c612076867a31543e45cbd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UAdxzF_h0Z5PyTVAeB8M4g.png"/></div></div></figure><p id="90d1" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">这是成本的更简洁的表示。因此，成本函数可以写成如下:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ql"><img src="../Images/e7433d15758807d24cde5808e06363d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPt_3Iabm2ZFhBvfNTxFXA.png"/></div></div></figure><h1 id="1931" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">梯度下降:</h1><p id="18de" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们需要找到参数，使成本函数值最小。这在数学上可以表示为:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi qm"><img src="../Images/33e30e9df0e1df6a2e2d5bbb7be5c8ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oJ877ofj2L3k2MbUl0Ab4g.png"/></div></div></figure><p id="f8c3" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">微分项由下式给出:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/c43415ef8cfe6bf48c339867f0b85b52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*9vL6gj9mOanS5UqDbe_UQw.png"/></div></figure><p id="cbf9" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">将其代入梯度下降方程:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi qo"><img src="../Images/789d7b10f16a5a77a580103e9bb84f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EhUW-yJXpViUEjuXf6Q16w.png"/></div></div></figure><h1 id="a79b" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">从头开始使用 python 进行逻辑回归:</h1><p id="7d19" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们使用上述等式对乳腺癌数据集执行逻辑回归。</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="41b9" class="ny kh it nu b gy nz oa l ob oc">x = df.area_mean<br/>y = df.diagnosis<br/>x = preprocessing.scale(x)<br/>theta_0_gd = 0<br/>theta_1_gd = 0<br/>alpha = 0.01<br/>h_theta_0_gd = 1<br/>h_theta_1_gd = 1<br/>epoch = 0<br/>m = len(x)<br/>fig = plt.figure(figsize = (10,10))<br/>sns.set(style = 'whitegrid')<br/>ax = sns.scatterplot(x,y)<br/>while h_theta_0_gd != 0 or h_theta_0_gd != 0:<br/>    if epoch &gt; 5000:<br/>        break<br/>    h_theta_0_gd = 0<br/>    h_theta_1_gd = 0<br/>    for i in range(len(x)):<br/>        h_theta_0_gd += ((1/(1+np.exp(-(theta_0_gd + (theta_1_gd * x[i]))))) - y[i])<br/>        h_theta_1_gd += (((1/(1+np.exp(-(theta_0_gd + (theta_1_gd * x[i]))))) - y[i]) * x[i])<br/>    h_theta_0_gd = (1/m) * h_theta_0_gd<br/>    h_theta_1_gd = (1/m) * h_theta_1_gd<br/>    theta_0_gd -= (alpha * h_theta_0_gd)<br/>    theta_1_gd -= (alpha * h_theta_1_gd)<br/>    epoch += 1<br/>ax = sns.lineplot(x,(1/(1+np.exp(-(theta_0_gd + (theta_1_gd * x))))) , color = 'r', linewidth = 3)</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/5a88da306d74687a540969dd200dfcb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*UgbObRoRdbi1bEuzzFF6gw.png"/></div></figure><p id="e4b4" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">θ_ 0 和θ_ 1 的值为:</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="1e43" class="ny kh it nu b gy nz oa l ob oc">print(theta_0_gd, theta_1_gd)</span><span id="f287" class="ny kh it nu b gy oe oa l ob oc">output&gt;&gt;-0.4173702618170074 3.0623106036104937</span></pre><p id="1ce5" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们还可以在每个时期之后绘制成本函数值，以检查算法的收敛性:</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="008f" class="ny kh it nu b gy nz oa l ob oc">x = np.array(df.area_mean)<br/>y = np.array(df.diagnosis)<br/>x = preprocessing.scale(x)<br/>theta_0_gd = 0<br/>theta_1_gd = 0<br/>alpha = 0.01<br/>h_theta_0_gd = 1<br/>h_theta_1_gd = 1<br/>epoch = 0<br/>Epoch = []<br/>m = len(x)<br/>j = 0<br/>J = []<br/>fig = plt.figure(figsize = (20,10))<br/>ax1 = fig.add_subplot(1,2,1)<br/>ax2 = fig.add_subplot(1,2,2)<br/>style.use('ggplot')<br/>ax1.scatter(x,y)<br/>while h_theta_0_gd != 0 or h_theta_0_gd != 0:<br/>    if epoch &gt; 5000:<br/>        break<br/>    h_theta_0_gd = 0<br/>    h_theta_1_gd = 0<br/>    for i in range(len(x)):<br/>        h_theta_0_gd += ((1/(1+np.exp(-(theta_0_gd + (theta_1_gd * x[i]))))) - y[i])<br/>        h_theta_1_gd += (((1/(1+np.exp(-(theta_0_gd + (theta_1_gd * x[i]))))) - y[i]) * x[i])<br/>    h_theta_0_gd = (1/m) * h_theta_0_gd<br/>    h_theta_1_gd = (1/m) * h_theta_1_gd<br/>    theta_0_gd -= (alpha * h_theta_0_gd)<br/>    theta_1_gd -= (alpha * h_theta_1_gd)<br/>    for i in range(m):<br/>        j += ((y[i] * (np.log(1/(1+np.exp(-(theta_0_gd + (theta_1_gd * x[i]))))))) - ((1-y[i]) * np.log(1-(1/(1+np.exp(-(theta_0_gd + (theta_1_gd * x[i]))))))))<br/>    J.append((-1/m) * j)<br/>    epoch += 1<br/>    Epoch.append(epoch)<br/>ax1.scatter(x,(1/(1+np.exp(-(theta_0_gd + (theta_1_gd * x))))) , color = 'k')<br/>ax2.plot(Epoch,J)<br/>plt.show()</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi qq"><img src="../Images/297702bac786201b57a04ed9f2577dec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghcfkZnK4csDk3zUGmLeZA.png"/></div></div></figure><p id="b1f2" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">我们可以看到，该算法在大约 500 个时期收敛，此时成本函数达到最小值-30，之后开始增加，直到 5000 个时期。这是因为学习率(0.01)非常大，所以算法在某一点后开始发散。考虑进一步降低学习率。</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi qq"><img src="../Images/565d52d5bf51353ed1869f0ca071f4b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1BfDGNEx2g1ixMd0oDcbuA.png"/></div></div></figure><p id="ecd0" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">上述曲线对应于 0.001 的学习率。我们可以看到，在成本函数值为-375 时，该算法在大约 6000 个时期收敛，之后它再次开始增加。我们可以由此得出结论，降低学习率有助于微调参数以获得成本函数的更小值，但是该算法需要更多的历元，即需要更多的时间来收敛。</p><h1 id="143c" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">使用 sklearn 逻辑回归模型:</h1><p id="ca9e" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们检查在我们的模型中获得的参数值是否与逻辑回归的 sklearn 模型相当</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="0cc9" class="ny kh it nu b gy nz oa l ob oc">X <strong class="nu iu">=</strong> x.reshape(<strong class="nu iu">-</strong>1,1)<br/>x_train, x_test, y_train, y_test <strong class="nu iu">=</strong> model_selection.train_test_split(X, y, test_size <strong class="nu iu">=</strong> 0.33)<br/>clf <strong class="nu iu">=</strong> LogisticRegression(max_iter <strong class="nu iu">=</strong> 5000)<br/>clf.fit(x_train, y_train)<br/>clf.coef_<br/>output&gt;&gt; array([[3.5028]])<br/>clf.intercept_<br/>output&gt;&gt; array([-0.31180412])</span></pre><p id="e7af" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">正如我们所看到的，当比较时，参数具有足够接近的值。我们甚至可以使用获得的参数绘制回归线，以检查我们是否得到了类似的曲线。</p><pre class="mt mu mv mw gt nt nu nv nw aw nx bi"><span id="1477" class="ny kh it nu b gy nz oa l ob oc">Theta_1 = clf.coef_<br/>Theta_0 = clf.intercept_<br/>fig = plt.figure(figsize = (10,10))<br/>ax = sns.scatterplot(X,y)<br/>ax = sns.lineplot(X, (1/(1+np.exp(-(Theta_0 + (Theta_1[0] * X))))), color = 'r')</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/420f0b0a70eae11802c6a1b54d8caec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*FL669srb57e_U4UHRQVWuQ.png"/></div></figure><h1 id="174c" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论:</h1><p id="bc98" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在本文中，我们已经看到了分类问题在数学上的含义，线性回归在分类问题中的作用，逻辑回归及其假设，成本，成本函数，决策边界和梯度下降。我们还从 scratch 建立了一个逻辑回归模型，并使用乳腺癌数据集对其进行了训练。我们还使用了 sklearn 的内置模型进行验证。</p><h1 id="6354" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">推论:</h1><figure class="mt mu mv mw gt ju"><div class="bz fp l di"><div class="qr qs l"/></div></figure><ol class=""><li id="1d01" class="mc md it lg b lh me ll mf lp mg lt mh lx mi mb mj mk ml mm bi translated">我正在从这个 Youtube 播放列表中学习大多数关于机器学习的概念。这很有帮助，也很容易理解。</li></ol><div class="nb nc gp gr nd ne"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">sklearn.linear_model。逻辑回归-sci kit-学习 0.23.1 文档</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">逻辑回归(又名 logit，MaxEnt)分类器。在多类的情况下，训练算法使用一对其余…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">scikit-learn.org</p></div></div><div class="nn l"><div class="qt l np nq nr nn ns jz ne"/></div></div></a></div><p id="07b7" class="pw-post-body-paragraph le lf it lg b lh me lj lk ll mf ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">2.考虑检查 sklearn 的逻辑回归分类的官方文档，以便进一步使用。</p></div></div>    
</body>
</html>