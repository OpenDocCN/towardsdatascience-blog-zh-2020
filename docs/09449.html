<html>
<head>
<title>Feature Selection With BorutaPy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 BorutaPy 进行特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-with-borutapy-f0ea84c9366?source=collection_archive---------24-----------------------#2020-07-06">https://towardsdatascience.com/feature-selection-with-borutapy-f0ea84c9366?source=collection_archive---------24-----------------------#2020-07-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a042" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">查找携带预测信息的所有要素</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bb13e5e0b826feb41d4d76ccac9a2f66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qtincxCLQrTNTwOjPCq6Ig.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/photos/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=945396" rel="noopener ugc nofollow" target="_blank"> Free-Photos </a>来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=945396" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="9895" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章将作为一个教程，介绍在为预测分类模型执行特征选择时如何实现 BorutaPy。当选择使用 BorutaPy 时，我会经历一些优点和一些缺点。我将使用坦桑尼亚油井分类数据集，尝试构建一个分类模型来预测一口井是正常工作、正常工作但需要维修还是不正常工作。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="df7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征选择是机器学习中最重要的步骤之一。将特征输入模型时，目标是为模型提供与预测类相关的特征。包括不相关的特征带来了数据中不必要的噪声的问题，导致较低的模型精度。通常，我们使用基于统计的特征选择方法，如 ANOVA 或卡方检验，评估每个预测变量与目标变量之间的关系。</p><blockquote class="mc md me"><p id="952d" class="kz la mf lb b lc ld ju le lf lg jx lh mg lj lk ll mh ln lo lp mi lr ls lt lu im bi translated">“Boruta 是一种所有相关特征选择方法，而大多数其它方法是最小最优的；这意味着它试图找到携带可用于预测的信息的所有特征，而不是找到某个分类器误差最小的可能紧凑的特征子集。”<em class="it"> - </em> <strong class="lb iu"> <em class="it">米隆·库尔萨</em>-</strong></p></blockquote><p id="23b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了 boruta，这些特性就可以被管理到“所有相关”的停止点，而不是“最小最优”的停止点。当用于预测时，所有相关变量都不是多余的。Boruta 基于两个思想，阴影特征和二项分布。当使用 boruta 时，这些特性不是用它们自己来评估，而是用它们的随机版本来评估。对于二项式分布的想法，boruta 采用了一个我们不知道是否有用的特征，并根据选择分布尾部定义的三个区域拒绝或接受该特征，例如 0.5%。</p><ul class=""><li id="e9a5" class="mj mk it lb b lc ld lf lg li ml lm mm lq mn lu mo mp mq mr bi translated"><strong class="lb iu">拒绝区域:</strong>特征被视为噪声而被丢弃的区域。</li><li id="66da" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated"><strong class="lb iu">犹豫不决的区域:</strong>博鲁塔对特性犹豫不决的区域</li><li id="0c74" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated"><strong class="lb iu">接受区域:</strong>特征被认为具有预测性的区域</li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="9077" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，我将构建一个三元分类模型，用于坦桑尼亚水井数据。这个分类器将预测水井的状况，数据集可以在这里检索到<a class="ae ky" href="https://www.drivendata.org/competitions/7/data/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"/></a>。我将从我如何处理模型数据开始。</p><p id="9391" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要安装 BorutaPy，请在您的笔记本或终端上运行以下代码。Boruta 的依赖项是 numpy、scipy 和 scikit-learn。</p><h2 id="e62d" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated"><strong class="ak">博鲁派文档</strong><em class="nq"/><a class="ae ky" href="https://github.com/scikit-learn-contrib/boruta_py" rel="noopener ugc nofollow" target="_blank"><em class="nq">h</em><strong class="ak"><em class="nq"/></strong></a>:</h2><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="dc2f" class="mx my it ns b gy nw nx l ny nz"><strong class="ns iu">###Installing BorutaPy</strong></span><span id="94cc" class="mx my it ns b gy oa nx l ny nz">pip install BorutaPy</span><span id="4a89" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Importing the libraries for data processing</strong></span><span id="f7a2" class="mx my it ns b gy oa nx l ny nz">import pandas as pd<br/>import numpy as np</span><span id="de9d" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Reading in the data</strong></span><span id="869c" class="mx my it ns b gy oa nx l ny nz">train_data = pd.read_csv(‘../data/train_data.csv’)</span><span id="350e" class="mx my it ns b gy oa nx l ny nz">train_target = pd.read_csv(‘../data/train_targets.csv’)</span><span id="877e" class="mx my it ns b gy oa nx l ny nz">test = pd.read_csv(‘../data/test_set_values.csv’)</span><span id="b51f" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Adding training label to training dataframe</strong></span><span id="576a" class="mx my it ns b gy oa nx l ny nz">train = train_data.merge(train_target, on='id', how='inner')</span><span id="d2e5" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###‘status group' needs to be converted to numerical values for the ###classification model and not ‘functional’, ‘non-functional’, and ###‘functional in need of repair’.</strong></span><span id="a3ed" class="mx my it ns b gy oa nx l ny nz">status_group_numeric = {'functional': 2,<br/>                        'functional needs repair': 1,<br/>                        'non functional': 0}<br/>train['status'] = train.status_group.replace(status_group_numeric)</span></pre><p id="ff4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">检查数据集中是否有缺失值</strong></p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="4189" class="mx my it ns b gy nw nx l ny nz">train.isna().sum()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/bf99572ec2e4def2a171e32297280338.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*P6O0KQ_9WdliiDrA4-V5sw.png"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="0fb3" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">填充缺失值</h2><p id="0d15" class="pw-post-body-paragraph kz la it lb b lc oc ju le lf od jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">许可字段中大约 69%的值为真，31%为假。我也选择用同样的比例来填充缺失的值。我还需要将这些值转换成数字。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="6991" class="mx my it ns b gy nw nx l ny nz">train['permit'].fillna(False, limit=947, inplace=True)<br/>train['permit'].fillna(True, inplace=True)</span><span id="adf8" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Function to take values in permit field and change them to ###numerical values</strong><br/>def permit(row):<br/>    if row['permit'] == True:<br/>        return 1<br/>    else:<br/>        return 0<br/>train['permit'] = train.apply(lambda x: permit(x), axis=1)</span></pre><p id="d2a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">公共会议字段有 91%的正确率和 9%的错误率。我选择用相同的比率来填充缺失的值。我还需要将值改为数字。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="2f77" class="mx my it ns b gy nw nx l ny nz">train['public_meeting'].fillna(False, limit=300, inplace=True)<br/>train['public_meeting'].fillna(True, inplace=True)</span><span id="554b" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Function to take values in public field and convert them to ###numerical values</strong><br/>def public(row):<br/>    if row['public_meeting'] == True:<br/>        return 1<br/>    else:<br/>        return 0<br/>train['public_meeting'] = train.apply(lambda x: public(x), axis=1)</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="aabc" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">宁滨特色</h2><p id="0e07" class="pw-post-body-paragraph kz la it lb b lc oc ju le lf od jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">出资者一栏显示了谁资助了油井。有 1，897 个不同的结果，但看起来只有前 7 个字段的值出现超过一千次。我将把这一栏分为 7 个不同的类别，其余的字段将归类为“其他”。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="6ae6" class="mx my it ns b gy nw nx l ny nz">def categorize_funder(train):<br/>    '''This function will go through every row in<br/>    the dataframe column funder and if the value<br/>    is any of the top 7 fields, will return those<br/>    values. If the value does not equal any of these<br/>    top 7 fields, it will return other.'''<br/>    if train['funder'] == 'Government Of Tanzania':<br/>        return 'govt'<br/>    elif train['funder'] == 'Danida':<br/>        return 'danida'<br/>    elif train['funder'] == 'Hesawa':<br/>        return 'hesawa'<br/>    elif train['funder'] == 'Rwssp':<br/>        return 'rwssp'<br/>    elif train['funder'] == 'World Bank':<br/>        return 'world_bank'<br/>    elif train['funder'] == 'Kkkt':<br/>        return 'kkkt'<br/>    elif train['funder'] == 'World Vision':<br/>        return 'world_vision'<br/>    else:<br/>        return 'other'</span><span id="379e" class="mx my it ns b gy oa nx l ny nz">train['funder'] = train.apply(lambda x: categorize_funder(x), axis=1)</span></pre><p id="782c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">宁滨将安装程序字段分为 7 类</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="1864" class="mx my it ns b gy nw nx l ny nz">def categorize_installer(train):<br/>    '''This function will go through<br/>    every row in the installer column<br/>    and if the value is equal to any of<br/>    the top 7, will return those values.<br/>    If not, will return other.'''<br/>    if train['installer'] == 'DWE':<br/>        return 'dwe'<br/>    elif train['installer'] == 'Government':<br/>        return 'govt'<br/>    elif train['installer'] == 'RWE':<br/>        return 'rwe'<br/>    elif train['installer'] == 'Commu':<br/>        return 'commu'<br/>    elif train['installer'] == 'DANIDA':<br/>        return 'danida'<br/>    elif train['installer'] == 'KKKT':<br/>        return 'kkkt'<br/>    elif train['installer'] == 'Hesawa':<br/>        return 'hesawa'<br/>    else:<br/>        return 'other'</span><span id="5312" class="mx my it ns b gy oa nx l ny nz">train['installer'] = train.apply(lambda x: categorize_installer(x), axis=1)</span></pre><p id="a39d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">宁滨把方案管理领域分为 7 类</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="03cd" class="mx my it ns b gy nw nx l ny nz">def categorize_scheme(row):<br/>    '''This function will go through each<br/>    row in the scheme management column<br/>    and if the value is equal to any of the <br/>    top 7, will return those values. If not,<br/>    will categorize the value as other.'''<br/>    if row['scheme_management'] == 'VWC':<br/>        return 'vwc'<br/>    elif row['scheme_management'] == 'WUG':<br/>        return 'wug'<br/>    elif row['scheme_management'] == 'Water authority':<br/>        return 'water_authority'<br/>    elif row['scheme_management'] == 'WUA':<br/>        return 'wua'<br/>    elif row['scheme_management'] == 'Water Board':<br/>        return 'water_board'<br/>    elif row['scheme_management'] == 'Parastatal':<br/>        return 'parastatal'<br/>    elif row['scheme_management'] == 'Private operator':<br/>        return 'private_operator'<br/>    else:<br/>        return 'other'</span><span id="f94a" class="mx my it ns b gy oa nx l ny nz">train['scheme_management'] = train.apply(lambda x: categorize_scheme(x), axis=1)</span></pre><p id="a781" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于数据集中有 55 个不同的建造年份，看起来建造年份字段对于我们的模型来说很难分类。我打算把宁滨的这些年变成几十年。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="9adc" class="mx my it ns b gy nw nx l ny nz">def categorize_contruction(row):<br/>    if row['construction_year'] &lt; 1970:<br/>        return '1960s'<br/>    elif row['construction_year'] &lt; 1980:<br/>        return '1970s'<br/>    elif row['construction_year'] &lt; 1990:<br/>        return '1980s'<br/>    elif row['construction_year'] &lt; 2000:<br/>        return '1990s'<br/>    elif row['construction_year'] &lt; 2010:<br/>        return '2000s'<br/>    elif row['construction_year'] &lt; 2020:<br/>        return '2010s'</span><span id="f0dc" class="mx my it ns b gy oa nx l ny nz">train['construction_year'] = train.apply(lambda x: categorize_contruction(x), axis=1)</span></pre><h2 id="9734" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">移除的功能</h2><p id="f59f" class="pw-post-body-paragraph kz la it lb b lc oc ju le lf od jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">我选择删除值与另一个值相似的字段，以及包含我找不到与预测模型相关的足够信息的值的字段。被删除的功能可能包含重要信息，我建议仔细查看这些数据。出于本示例的目的，我将在没有它们的情况下继续。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="919b" class="mx my it ns b gy nw nx l ny nz">- train = train.drop(columns=['subvillage'], axis=1)<br/>- train = train.drop(columns=['scheme_name'], axis=1)<br/>- train = train.drop(columns=['wpt_name'], axis=1)<br/>- train = train.drop(columns=['region'], axis=1)<br/>- train = train.drop(columns=['extraction_type', 'extraction_type_group'],<br/>                  axis=1)<br/>- train = train.drop(columns=['management_group'], axis=1)<br/>- train = train.drop(columns=['payment_type'], axis=1)<br/>- train = train.drop(columns=['water_quality'], axis=1)<br/>- train = train.drop(columns=['quantity_group'], axis=1)<br/>- train = train.drop(columns=['source_type'], axis=1)<br/>- train = train.drop(columns=['waterpoint_type_group'], axis=1)</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="211f" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">来自数据的状态</h2><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="9e15" class="mx my it ns b gy nw nx l ny nz">pivot_train = pd.pivot_table(train, index=['status_group'], <br/>                            values='status',<br/>                            aggfunc='count')<br/>pivot_train</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/af6b870d0043b0173b47bffc72932c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*4zTPhaXv7ciuIZJKrkXyjQ.png"/></div></figure><h2 id="478f" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">对训练数据集进行训练测试拆分</h2><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="7816" class="mx my it ns b gy nw nx l ny nz">X = train.drop(columns=['id', 'status_group', 'status', 'date_recorded'], axis=1)<br/>y = train.status</span><span id="b1b9" class="mx my it ns b gy oa nx l ny nz">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)</span></pre><h2 id="fdc9" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">编码和缩放</h2><p id="1ae0" class="pw-post-body-paragraph kz la it lb b lc oc ju le lf od jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">我用一个 Sklearn 的热编码器对分类特征进行编码，并用 Sklearn 的标准定标器对连续特征进行定标。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="e720" class="mx my it ns b gy nw nx l ny nz"><strong class="ns iu">###Instantiating One Hot Encoder<br/></strong>ohe = OneHotEncoder()</span><span id="0976" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Defining features to encode<br/></strong>ohe_features = ['funder', 'installer', 'basin', <br/>               'region_code', 'district_code', 'lga', 'public_meeting',<br/>               'scheme_management', 'permit', 'construction_year', <br/>               'extraction_type_class', 'management',<br/>               'payment', 'quality_group',<br/>               'quantity', 'source', 'waterpoint_type']</span><span id="df21" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Defining continuous numerical features<br/></strong>cont_features = ['amount_tsh', 'gps_height', 'longitude',<br/>                        'latitude', 'population'</span><span id="8824" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Creating series for categorical test and train<br/></strong>X_train_cat = X_train[ohe_features]<br/>X_test_cat = X_test[ohe_features]</span><span id="2352" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Fitting encoder to training categorical features and transforming ###test and train</strong><br/>X_train_ohe = ohe.fit_transform(X_train_cat)<br/>X_test_ohe = ohe.transform(X_test_cat)</span><span id="6900" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Converting series to dataframes<br/></strong>columns = ohe.get_feature_names(input_features=X_train_cat.columns)<br/>X_train_processed = pd.DataFrame(X_train_ohe.todense(), </span><span id="75c2" class="mx my it ns b gy oa nx l ny nz">columns=columns)<br/>X_test_processed = pd.DataFrame(X_test_ohe.todense(), columns=columns)</span><span id="6b49" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Instantiating Standard Scaler</strong><br/>ss = StandardScaler()</span><span id="ff88" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Converting continuous feature values to floats</strong><br/>X_train_cont = X_train[cont_features].astype(float)<br/>X_test_cont = X_test[cont_features].astype(float)</span><span id="5475" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Fitting scaler to training continuous features and transforming ###train and test</strong><br/>X_train_scaled = ss.fit_transform(X_train_cont)<br/>X_test_scaled = ss.transform(X_test_cont)</span><span id="9e1e" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Concatenating scaled and encoded dataframes<br/></strong>X_train_a2 = pd.concat([pd.DataFrame(X_train_scaled), X_train_processed], axis=1)<br/>X_test_a2 = pd.concat([pd.DataFrame(X_test_scaled), X_test_processed], axis=1)</span></pre><h2 id="1ecd" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">导入用于预测分类的库</h2><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="9182" class="mx my it ns b gy nw nx l ny nz"><strong class="ns iu">###Classification Algorithm<br/></strong>from sklearn.ensemble import RandomForestClassifier</span><span id="8023" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Model evaluation<br/></strong>from sklearn.model_selection import cross_val_score<br/>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score</span><span id="2cbd" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Feature Selection<br/></strong>from boruta import BorutaPy</span></pre><p id="001d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个例子，我选择使用随机森林分类器，这是一种基于系综树的学习算法。它是用随机选择的训练数据子集创建的一组决策树。随机森林分类器然后聚集来自不同决策树的投票来选择测试对象的最终类。随机森林分类器的第一次运行将包括所有的特性，然后我将运行 BorutaPy。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="cf2b" class="mx my it ns b gy nw nx l ny nz"><strong class="ns iu">###Instantiating Random Forest Classifier<br/></strong>rf = RandomForestClassifier(n_estimators=500, random_state=42)</span><span id="1272" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Fitting Random Forest Classifier to training data<br/></strong>rf.fit(X_train_a2, y_train)</span><span id="618e" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Print accuracy and recall scores for both test and train</strong></span><span id="9a1f" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Average is set to micro for recall score since this is a ###multi-class classification model. Micro-average aggregates the ###contributions of all classes to compute the average metric. Micro ###is preferred for data with class imbalance.<br/></strong>print('Test Accuracy:')<br/>print(accuracy_score(y_test, rf.predict(X_test_a2)))<br/>print('Test Recall:')<br/>print(recall_score(y_test, rf.predict(X_test_a2), average='micro'))<br/>print('Train Accuracy:')<br/>print(accuracy_score(y_train, rf.predict(X_train_a2)))<br/>print('Train Recall:')<br/>print(recall_score(y_train, rf.predict(X_train_a2),  average='micro'))</span><span id="bad4" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">####Output:</strong></span><span id="46b5" class="mx my it ns b gy oa nx l ny nz">Test Accuracy:<br/>0.80006734006734<br/>Test Recall:<br/>0.6729820394303477<br/>Train Accuracy:<br/>0.9955555555555555<br/>Train Recall:<br/>0.9902567898799216</span></pre><p id="7edc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这导致了 80%的测试分数(平均准确度)和 96%的训练分数，现在我将通过 BorutaPy 运行这些特性，看看哪些被选为相关的。</p><h2 id="1a34" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">博鲁塔方法论</h2><ul class=""><li id="85c7" class="mj mk it lb b lc oc lf od li oi lm oj lq ok lu mo mp mq mr bi translated">首先，通过创建重复要素并混洗每列中的值来移除它们与响应的相关性，从而为要素提供随机性。(阴影特征)</li><li id="8a43" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">在数据集上训练随机森林分类器，并通过收集 Z 分数来计算相关性/重要性。</li><li id="25f7" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">在阴影属性中查找最大 Z 得分，并为 Z 得分高于阴影要素最大 Z 得分的每个属性指定一个命中。(准确度损失除以准确度损失的标准偏差)</li><li id="1519" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">获取尚未确定为重要的每个属性，并使用阴影属性中的最大 Z 得分执行双边质量测试。</li><li id="731d" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">将重要性级别低于 MZSA 的属性标记为“不重要”。</li><li id="5316" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">将重要性级别高于 MZSA 的属性标记为“重要”。</li><li id="0764" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">移除所有阴影属性</li><li id="2713" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">重复此过程，直到计算出所有属性的重要性，或者算法达到设定的迭代次数</li></ul><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="cd92" class="mx my it ns b gy nw nx l ny nz"><strong class="ns iu">###define X and y for boruta, algorithm takes numpy arrays as inputs ###and not dataframe (why you see .values)</strong><br/>X_boruta = train_all.drop(columns=['status'], axis=1).values<br/>y_boruta = train_all.status.values</span><span id="7f1d" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###define random forest classifier, set n_jobs parameter to -1 to ###utilize all processors and set n_estimators parameter to 500, ###number of trees in the forest.</strong><br/>rf = RandomForestClassifier(n_jobs=-1, n_estimators=500, oob_score=True, max_depth=6)</span><span id="3d0e" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Define borutapy with rf as estimator and verbose parameter set to ###2 to output which features have been selected already</strong><br/>feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=42)</span><span id="f4d6" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###fit boruta selector to X and y boruta</strong><br/>feat_selector.fit(X_boruta, y_boruta)</span><span id="0d93" class="mx my it ns b gy oa nx l ny nz">Output:</span><span id="9828" class="mx my it ns b gy oa nx l ny nz"><em class="mf">Iteration: 	1 / 100<br/>Confirmed: 	0<br/>Tentative: 	277<br/>Rejected: 	0<br/>Iteration: 	2 / 100<br/>Confirmed: 	0<br/>Tentative: 	277<br/>Rejected: 	0<br/>Iteration: 	3 / 100<br/>Confirmed: 	0<br/>Tentative: 	277<br/>Rejected: 	0<br/>Iteration: 	4 / 100<br/>Confirmed: 	0<br/>Tentative: 	277<br/>Rejected: 	0<br/>Iteration: 	5 / 100<br/>Confirmed: 	0<br/>Tentative: 	277<br/>Rejected: 	0<br/>Iteration: 	6 / 100<br/>Confirmed: 	0<br/>Tentative: 	277<br/>Rejected: 	0<br/>Iteration: 	7 / 100<br/>Confirmed: 	0<br/>Tentative: 	277<br/>Rejected: 	0<br/>Iteration: 	8 / 100<br/>Confirmed: 	110<br/>Tentative: 	49<br/>Rejected: 	118<br/>Iteration: 	9 / 100<br/>Confirmed: 	110<br/>Tentative: 	49<br/>Rejected: 	118<br/>Iteration: 	10 / 100<br/>Confirmed: 	110<br/>Tentative: 	49<br/>Rejected: 	118</em></span><span id="d572" class="mx my it ns b gy oa nx l ny nz"><em class="mf"><br/>...</em></span><span id="5fe7" class="mx my it ns b gy oa nx l ny nz"><em class="mf"><br/>Iteration: 	90 / 100<br/>Confirmed: 	122<br/>Tentative: 	11<br/>Rejected: 	144<br/>Iteration: 	91 / 100<br/>Confirmed: 	122<br/>Tentative: 	11<br/>Rejected: 	144<br/>Iteration: 	92 / 100<br/>Confirmed: 	122<br/>Tentative: 	11<br/>Rejected: 	144<br/>Iteration: 	93 / 100<br/>Confirmed: 	122<br/>Tentative: 	11<br/>Rejected: 	144<br/>Iteration: 	94 / 100<br/>Confirmed: 	122<br/>Tentative: 	11<br/>Rejected: 	144<br/>Iteration: 	95 / 100<br/>Confirmed: 	122<br/>Tentative: 	11<br/>Rejected: 	144<br/>Iteration: 	96 / 100<br/>Confirmed: 	122<br/>Tentative: 	11<br/>Rejected: 	144<br/>Iteration: 	97 / 100<br/>Confirmed: 	122<br/>Tentative: 	11<br/>Rejected: 	144<br/>Iteration: 	98 / 100<br/>Confirmed: 	122<br/>Tentative: 	11<br/>Rejected: 	144<br/>Iteration: 	99 / 100<br/>Confirmed: 	122<br/>Tentative: 	11<br/>Rejected: 	144<br/><br/><br/>BorutaPy finished running.<br/><br/>Iteration: 	100 / 100<br/>Confirmed: 	122<br/>Tentative: 	2<br/>Rejected: 	144</em></span></pre><p id="431c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">检查接受的特征</strong></p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="e1dc" class="mx my it ns b gy nw nx l ny nz"><strong class="ns iu">###Print accepted features as well as features that boruta did not ###deem unimportant or important (area of irresolution)</strong><br/>accept = X.columns[feat_selector.support_].to_list()<br/>irresolution = X.columns[feat_selector.support_weak_].to_list()</span><span id="41c5" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Call transform on boruta to update X_boruta to selected features<br/></strong>X_filtered = feat_selector.transform(X_boruta)</span><span id="9361" class="mx my it ns b gy oa nx l ny nz">print('Accepted features:')<br/>print('----------------------------')<br/>print(list(accept))<br/>print('----------------------------')<br/>print(list(irresolution))</span></pre><h2 id="5ad6" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">为所选要素创建新的数据框</h2><p id="155d" class="pw-post-body-paragraph kz la it lb b lc oc ju le lf od jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">通过使用这些选择的特征并将超参数 max_depth 调整为 6，我们获得了改进的分数。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="352d" class="mx my it ns b gy nw nx l ny nz"><strong class="ns iu">###Creating new dataframe from original X dataframe with only the ###selected features from BorutaPy</strong><br/>new_x = train_all[accept]<br/>X2_boruta = new_x.drop(['status'], axis=1)<br/>y2_boruta = new_x.status</span><span id="2e0a" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Train test split on updated X</strong><br/>X_t, X_val, y_t, y_val = train_test_split(new_x, y, random_state=42)</span><span id="b9a3" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Instantiating Random Forest Classifier<br/></strong>rf2 = RandomForestClassifier(n_jobs=-1, n_estimators=500, oob_score=True, max_depth=6, random_state=42)</span><span id="c1bd" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Fitting Random Forest Classifier to train and test<br/></strong>rf2.fit(X_t, y_t)</span><span id="e73f" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Predicting on test data<br/></strong>y_pred = rf2.predict(X_val)</span><span id="586f" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Test Score<br/></strong>rf2.score(X_val, y_val)</span><span id="fede" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Training Score<br/></strong>rf2.score(X_t, y_t)</span><span id="99b5" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Print accuracy and recall scores for both test and train ###Average is set to micro for recall score since this is a ###multi-class classification model. Micro-average aggregates the ###contributions of all classes to compute the average metric. Micro ###is preferred for data with class imbalance.</strong></span><span id="c4b7" class="mx my it ns b gy oa nx l ny nz">print('Test Accuracy:')<br/>print(accuracy_score(y_test, rf.predict(X_test_a2)))<br/>print('Test Recall:')<br/>print(recall_score(y_test, rf.predict(X_test_a2), average='micro'))<br/>print('Train Accuracy:')<br/>print(accuracy_score(y_train, rf.predict(X_train_a2)))<br/>print('Train Recall:')<br/>print(recall_score(y_train, rf.predict(X_train_a2),  average='micro'))</span><span id="945b" class="mx my it ns b gy oa nx l ny nz"><strong class="ns iu">###Output:<br/></strong>Test Accuracy:<br/>0.797068340067343<br/>Test Recall:<br/>0.769820394303477<br/>Train Accuracy:<br/>0.818649338720538<br/>Train Recall:<br/>0.810256789879921</span></pre><p id="47ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅使用所选功能的第二次运行导致 80%的测试分数和 81%的训练分数。从我们的第一次迭代到 80%的测试分数和 96%的训练分数，这绝对是一个进步。我们正在最小化测试和训练分数之间的差距(模型越来越不适合)。</p><h2 id="cd26" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">结论</h2><p id="981d" class="pw-post-body-paragraph kz la it lb b lc oc ju le lf od jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">Boruta 是一个强大的特征选择算法，你可以在大多数数据集上实现。在时间紧迫的情况下，以及在包含大量弱相关预测变量的数据集的情况下，它会非常有用。主要缺点是计算时间，虽然很多算法可以在几秒或几毫秒内执行，但 boruta 的执行时间是以小时计算的。这使得调整参数极其困难，因为每次调整都需要大量的额外时间。对于您正在处理的数据集，Boruta 可能不是最佳选择，我建议您也测试其他算法并比较结果。</p><p id="8d30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong>:</p><ul class=""><li id="f67f" class="mj mk it lb b lc ld lf lg li ml lm mm lq mn lu mo mp mq mr bi translated"><em class="mf"> Bhattacharyya，I. (2018 年 9 月 18 日)。特征选择(Boruta/Light GBM/Chi Square)-分类特征选择。检索自</em><a class="ae ky" href="https://medium.com/@indreshbhattacharyya/feature-selection-categorical-feature-selection-boruta-light-gbm-chi-square-bf47e94e2558" rel="noopener"><em class="mf">https://medium . com/@ indreshbhattacharyya/feature-selection-category-feature-selection-boruta-light-GBM-chi-square-BF 47 e 94 e 2558</em></a></li><li id="179e" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated">丹尼尔·霍莫拉。(2016 年 02 月 08 日)。一种全相关特征选择方法。检索自<a class="ae ky" href="http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/" rel="noopener ugc nofollow" target="_blank"><em class="mf">http://danielhomola . com/2015/05/08/boru tapy-an-all-relevant-feature-selection-method/</em></a></li><li id="e33d" class="mj mk it lb b lc ms lf mt li mu lm mv lq mw lu mo mp mq mr bi translated"><em class="mf">博鲁塔。(未注明)。检索自</em><a class="ae ky" href="https://pypi.org/project/Boruta/" rel="noopener ugc nofollow" target="_blank"><em class="mf">https://pypi.org/project/Boruta/</em></a></li></ul></div></div>    
</body>
</html>