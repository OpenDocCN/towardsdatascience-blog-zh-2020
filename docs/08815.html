<html>
<head>
<title>Polynomial Regression with Scikit learn: What You Should Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Scikit学习多项式回归:你应该知道什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/polynomial-regression-with-scikit-learn-what-you-should-know-bed9d3296f2?source=collection_archive---------2-----------------------#2020-06-25">https://towardsdatascience.com/polynomial-regression-with-scikit-learn-what-you-should-know-bed9d3296f2?source=collection_archive---------2-----------------------#2020-06-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/0c325c73c691e99d816c2c450ea5b1c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GMSdmgn0IIW6Zb-vMKlexQ.png"/></div></div></figure><div class=""/><div class=""><h2 id="ab75" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">一个奇怪的结果让我更好的理解了多项式回归…</h2></div><h1 id="0405" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">多项式回归的一个简单例子</h1><p id="7fa8" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">多项式回归是一种众所周知的算法。这是线性回归的一个特例，因为我们在创建线性回归之前创建了一些多项式特征。或者可以认为是具有<a class="ae mh" rel="noopener" target="_blank" href="/overview-of-supervised-machine-learning-algorithms-a5107d036296"> <em class="mi">特征空间映射</em> </a>(又名<strong class="ln jf">多项式核</strong>)的线性回归。使用这个内核技巧，在某种程度上，可以创建一个次数无限的多项式回归！</p><p id="0171" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">在本文中，我们将处理经典的多项式回归。通过scikit learn，可以结合这两个步骤在流水线中创建一个(<strong class="ln jf">多项式特性</strong>和<strong class="ln jf">线性回归</strong>)。我将展示下面的代码。让我们看一个例子，一些简单的玩具数据，只有10分。我们也考虑一下度数是9。你可以在下面看到最终的结果。</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/6111059d92f010b20ea27f5fffa3f132.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*a0Jg6HCB4BItQNA769luDw.png"/></div></figure><p id="c143" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">你看出什么不对了吗？</p><p id="280e" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">嗯，理论上，这是错误的！<strong class="ln jf">对于10个点，一个9次多项式应该可以完美拟合！</strong></p><p id="b5e0" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">或者，我确信你们中的一些人在想:为什么你说这是错的？这可能是正确的模式。你认为模型应该是完美拟合的，但不是，你被<strong class="ln jf">多项式插值</strong>搞糊涂了！</p><p id="2a71" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">首先，您可以自己尝试使用下面的代码来创建模型。</p><h2 id="3afe" class="mt ku je bd kv mu mv dn kz mw mx dp ld lu my mz lf ly na nb lh mc nc nd lj ne bi translated">创建一些玩具数据</h2><pre class="mp mq mr ms gt nf ng nh ni aw nj bi"><span id="2aab" class="mt ku je ng b gy nk nl l nm nn">import pandas as pd</span><span id="8395" class="mt ku je ng b gy no nl l nm nn">xdic={'X': {11: 300, 12: 170, 13: 288, 14: 360, 15: 319, 16: 330, 17: 520, 18: 345, 19: 399, 20: 479}}</span><span id="01b3" class="mt ku je ng b gy no nl l nm nn">ydic={'y': {11: 305000, 12: 270000, 13: 360000, 14: 370000, 15: 379000, 16: 405000, 17: 407500, 18: 450000, 19: 450000, 20: 485000}}</span><span id="7229" class="mt ku je ng b gy no nl l nm nn">X=pd.DataFrame.from_dict(xdic)</span><span id="6c82" class="mt ku je ng b gy no nl l nm nn">y=pd.DataFrame.from_dict(ydic)</span><span id="27b4" class="mt ku je ng b gy no nl l nm nn">import numpy as np</span><span id="c1e7" class="mt ku je ng b gy no nl l nm nn">X_seq = np.linspace(X.min(),X.max(),300).reshape(-1,1)</span></pre><h2 id="c74e" class="mt ku je bd kv mu mv dn kz mw mx dp ld lu my mz lf ly na nb lh mc nc nd lj ne bi translated">创建模型</h2><pre class="mp mq mr ms gt nf ng nh ni aw nj bi"><span id="2c50" class="mt ku je ng b gy nk nl l nm nn">from sklearn.preprocessing import PolynomialFeatures</span><span id="7085" class="mt ku je ng b gy no nl l nm nn">from sklearn.pipeline import make_pipeline</span><span id="057c" class="mt ku je ng b gy no nl l nm nn">from sklearn.linear_model import LinearRegression</span><span id="cdbd" class="mt ku je ng b gy no nl l nm nn">degree=9</span><span id="f808" class="mt ku je ng b gy no nl l nm nn">polyreg=make_pipeline(PolynomialFeatures(degree),LinearRegression())</span><span id="0266" class="mt ku je ng b gy no nl l nm nn">polyreg.fit(X,y)</span></pre><h2 id="147f" class="mt ku je bd kv mu mv dn kz mw mx dp ld lu my mz lf ly na nb lh mc nc nd lj ne bi translated">创造情节</h2><pre class="mp mq mr ms gt nf ng nh ni aw nj bi"><span id="5d48" class="mt ku je ng b gy nk nl l nm nn">import matplotlib.pyplot as plt</span><span id="ccf1" class="mt ku je ng b gy no nl l nm nn">plt.figure()</span><span id="696b" class="mt ku je ng b gy no nl l nm nn">plt.scatter(X,y)</span><span id="9a1a" class="mt ku je ng b gy no nl l nm nn">plt.plot(X_seq,polyreg.predict(X_seq),color="black")</span><span id="3624" class="mt ku je ng b gy no nl l nm nn">plt.title("Polynomial regression with degree "+str(degree))</span><span id="12d3" class="mt ku je ng b gy no nl l nm nn">plt.show()</span></pre><h1 id="6a32" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">你不应该这样做！</h1><p id="8a4c" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">先说<strong class="ln jf">多项式回归</strong>和<strong class="ln jf">多项式插值</strong>的区别。先说一个我从scikit learn团队得到的答案:<em class="mi">你不应该这样做，展开到9次多项式是扯淡。scikit learn是为实际用例构建的，它使用有限精度的表示，而不是理论表示。</em></p><p id="4e48" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">是的，他们完全正确！看看这些数字，它们有多大:1e24！</p><p id="30aa" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">但是如果他们不能处理大数字，他们不应该抛出一个错误或警告吗？如果没有任何信息，人们会认为这个模型是正确的，然而，它实际上是错误的。</p><p id="e296" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">好吧好吧，我知道，你们中的一些人不相信结果是错误的，或者也许是不可能处理大的数字，让我们看看另一个包，numpy！</p><h1 id="a1ff" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">但是polyfit做得很好</h1><p id="022b" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">对于同一个示例，numpy的polyfit找到模型没有问题。你可以看到下面的情节和代码。</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/dd18f5a698a529fca3c0aee9dff7d011.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*Tfcah3OG9pssFfKgDStA5A.png"/></div></figure><pre class="mp mq mr ms gt nf ng nh ni aw nj bi"><span id="54d4" class="mt ku je ng b gy nk nl l nm nn">coefs = np.polyfit(X.values.flatten(), y.values.flatten(), 9)</span><span id="aeb3" class="mt ku je ng b gy no nl l nm nn">plt.figure()</span><span id="34aa" class="mt ku je ng b gy no nl l nm nn">plt.plot(X_seq, np.polyval(coefs, X_seq), color="black")</span><span id="0392" class="mt ku je ng b gy no nl l nm nn">plt.title("Polyfit degree "+str(degree))</span><span id="7bff" class="mt ku je ng b gy no nl l nm nn">plt.scatter(X,y)</span><span id="e1e0" class="mt ku je ng b gy no nl l nm nn">plt.show()</span></pre><p id="51da" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">现在我知道你们中的一些人在想:<em class="mi"> polyfit是一个非常不同的东西，它是一个插值而不是回归。</em></p><p id="3ea6" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">因为在四处打听的时候，我得到了一些这样的答案(但并不准确，或者说是错误的):</p><blockquote class="nq nr ns"><p id="cd15" class="ll lm mi ln b lo mj kf lq lr mk ki lt nt ml lw lx nu mm ma mb nv mn me mf mg im bi translated">polyfit正在做一件完全不同的事情。它对某个向量X到向量y执行单变量多项式拟合。这里，我们对某个特征空间X执行多项式展开，以便为多变量拟合表示高阶交互项(相当于用多项式核学习)。</p></blockquote><p id="1bce" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">好了，什么是<strong class="ln jf">多项式插值</strong>？</p><h1 id="19ce" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">什么是多项式插值？</h1><p id="472a" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">嗯，对于这种问题，维基百科是一个很好的来源。</p><blockquote class="nw"><p id="0f9c" class="nx ny je bd nz oa ob oc od oe of mg dk translated">在数值分析中，<strong class="ak">多项式插值</strong>是通过穿过数据集点的<strong class="ak">最低</strong>可能次数的多项式对给定数据集的插值。</p></blockquote><p id="5f85" class="pw-post-body-paragraph ll lm je ln b lo og kf lq lr oh ki lt lu oi lw lx ly oj ma mb mc ok me mf mg im bi translated">并且我们有这样的结果被证明:给定n+1个不同的点x_0，x_0，…，x_n和相应的值y_0，y_1，…，y_n，存在一个至多n次的唯一多项式来插值数据(x_0，y_0)，…，(x_n，y_n)。</p><p id="f6f4" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">回到我们的例子:有10个点，我们试图找到一个9次多项式。所以从技术上讲，我们是在做多项式插值。而<strong class="ln jf"> polyfit </strong>找到了这个唯一的多项式！对于<strong class="ln jf"> scikit learn的多项式回归管道</strong>，情况并非如此！</p><p id="5c8e" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">这正是为什么你们中的一些人会想:<strong class="ln jf"> polyfit </strong>不同于<strong class="ln jf"> scikit learn的多项式回归管道！</strong></p><p id="cb58" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">现在，等等！</p><p id="8aeb" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">在<strong class="ln jf"> polyfit </strong>中，有一个自变量，叫做<em class="mi">度</em>。所以可以修改度数，我们用5试试。</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/4eb8710dc27028bf26d86c2601613fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*UHlZk1jGcXwnYR5xcgAbtQ.png"/></div></figure><p id="8b59" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">是的，用<code class="fe ol om on ng b">polyfit</code>，可以选择多项式的次数，我们正在用它做<strong class="ln jf">多项式回归</strong>。而用户选择的9次是多项式插值的<strong class="ln jf">特例。</strong></p><p id="60bd" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">这是令人放心的，因为线性回归试图最小化平方误差。而且我们知道，如果有10个点，我们试着求一个9次多项式，那么误差可以是0(不能再低了！)因为多项式插值的定理。</p><p id="9f01" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">对于那些还在怀疑的人，polyfit有一个官方文档:最小二乘多项式拟合。将一个<em class="mi">度</em>的多项式<code class="fe ol om on ng b">p(x) = p[0] * x**deg + ... + p[deg]</code>拟合到点<em class="mi"> (x，y) </em>。按照<em class="mi">度</em>、<em class="mi">度-1 </em>、… <em class="mi"> 0 </em>的顺序返回最小化平方误差的系数矢量<em class="mi"> p </em>。</p><p id="37fa" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">好了，是时候回到我们的<strong class="ln jf"> scikit learn的多项式回归管道了。</strong>那么现在，为什么会有区别呢？真的有两种不同的多项式回归(或拟合)，都使用最小二乘法，但使用方式不同吗？</p><p id="4b69" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">我找到了这个答案，但我还没有得到它。</p><blockquote class="nq nr ns"><p id="fa33" class="ll lm mi ln b lo mj kf lq lr mk ki lt nt ml lw lx nu mm ma mb nv mn me mf mg im bi translated">两个模型都使用最小二乘法，但是使用这些最小二乘法的方程是完全不同的。polyfit将其应用于vandemonde矩阵，而线性回归则不能。</p></blockquote><h1 id="389e" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">特征比例效应</h1><p id="e222" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">在深入研究的同时，应该提到另一个重要的特性转换:特性缩放。</p><p id="c8df" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">在几本关于机器学习的书中，当执行多项式回归时，特征被缩放。也许从一开始，你们中的一些人就在说应该这样做。</p><p id="0569" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">而且没错，<strong class="ln jf"> scikit learn的多项式回归管道带特征缩放</strong>，好像相当于polyfit！根据剧情(我没有真的去查，但是目测他们是一样的)。</p><p id="ff24" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">您可以使用下面的代码:</p><pre class="mp mq mr ms gt nf ng nh ni aw nj bi"><span id="64ed" class="mt ku je ng b gy nk nl l nm nn">from sklearn.preprocessing import PolynomialFeatures</span><span id="b574" class="mt ku je ng b gy no nl l nm nn">from sklearn.pipeline import make_pipeline</span><span id="b322" class="mt ku je ng b gy no nl l nm nn">from sklearn.linear_model import LinearRegression</span><span id="8469" class="mt ku je ng b gy no nl l nm nn">from sklearn import preprocessing</span><span id="3b85" class="mt ku je ng b gy no nl l nm nn">scaler = preprocessing.StandardScaler()</span><span id="b21f" class="mt ku je ng b gy no nl l nm nn">degree=9</span><span id="42b2" class="mt ku je ng b gy no nl l nm nn">polyreg_scaled=make_pipeline(PolynomialFeatures(degree),scaler,LinearRegression())</span><span id="0657" class="mt ku je ng b gy no nl l nm nn">polyreg_scaled.fit(X,y)</span></pre><p id="ad7b" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">现在，我们没有回答之前的问题，我们有更多的问题:<strong class="ln jf">特征缩放对线性回归有影响吗</strong>？</p><p id="111a" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">答案是否定的。</p><p id="9304" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">为了讨论这一点，可以写另一篇文章，对于我们关于多项式回归效果的讨论，我们可以只做另一个变换。</p><pre class="mp mq mr ms gt nf ng nh ni aw nj bi"><span id="8202" class="mt ku je ng b gy nk nl l nm nn">X=pd.DataFrame.from_dict(xdic)/1000</span></pre><p id="d73b" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">没错，你只是把预测值除以1000。现在，你知道对线性回归模型的影响只是成比例的，但实际上，差别是巨大的。</p><div class="mp mq mr ms gt ab cb"><figure class="oo iv op oq or os ot paragraph-image"><img src="../Images/6111059d92f010b20ea27f5fffa3f132.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*a0Jg6HCB4BItQNA769luDw.png"/></figure><figure class="oo iv ou oq or os ot paragraph-image"><img src="../Images/fcb5638c6668f257d3f425c61b1eb46d.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*F9hcAbWnTPboltf4V542iA.png"/></figure></div><p id="db6e" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated"><strong class="ln jf">这就是为什么我们可以得出结论，对于scikit learn来说，初始数字太大了。</strong></p><h1 id="e773" class="kt ku je bd kv kw kx ky kz la lb lc ld kk le kl lf kn lg ko lh kq li kr lj lk bi translated">结论</h1><p id="b35c" class="pw-post-body-paragraph ll lm je ln b lo lp kf lq lr ls ki lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">最后我们可以说<strong class="ln jf"> scikit learn的多项式回归管道(有无缩放)</strong>，应该和numpy的polyfit是等价的，只是在大数处理方面的区别可以产生不同的结果。</p><p id="5b2f" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">我个人认为，在这种情况下，scikit learn应该抛出一个错误或至少一个警告。</p><p id="c02b" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">我真的很想知道你的意见！</p><p id="1df3" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">如果你想了解更多关于多项式回归与其他监督学习算法的关系，你可以阅读这篇文章:</p><div class="is it gp gr iu ov"><a rel="noopener follow" target="_blank" href="/overview-of-supervised-machine-learning-algorithms-a5107d036296"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd jf gy z fp pa fr fs pb fu fw jd bi translated">监督机器学习算法综述</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">大图如何通过连接点给我们洞察力和对ML的更好理解</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj ja ov"/></div></div></a></div><p id="66ac" class="pw-post-body-paragraph ll lm je ln b lo mj kf lq lr mk ki lt lu ml lw lx ly mm ma mb mc mn me mf mg im bi translated">你会看到多项式回归是一种特殊的<strong class="ln jf">特征空间映射</strong>。</p></div></div>    
</body>
</html>