<html>
<head>
<title>Principal Component Analysis (PCA) from scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中从头开始的主成分分析(PCA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-pca-from-scratch-in-python-7f3e2a540c51?source=collection_archive---------1-----------------------#2020-06-20">https://towardsdatascience.com/principal-component-analysis-pca-from-scratch-in-python-7f3e2a540c51?source=collection_archive---------1-----------------------#2020-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2147" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">以及一些低维空间的可视化。</h2></div><p id="d034" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主成分分析是一种用于降维的数学技术。它的目标是在保留大部分原始信息的同时减少特征的数量。今天我们将使用pure Numpy从头开始实现它。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/6027a541c5a4b5a1fb7797513e43123e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O0PZB5V1ZVrzjYXu"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">卢卡斯·本杰明在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e416" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想知道为什么PCA对你的普通机器学习任务有用，这里列出了3大好处:</p><ul class=""><li id="003a" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated"><strong class="kk iu">减少训练时间</strong>——由于数据集更小</li><li id="2dbd" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated"><strong class="kk iu">去除噪音</strong>——只保留相关的内容</li><li id="3990" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated"><strong class="kk iu">使可视化成为可能</strong>——在最多有3个主要成分的情况下</li></ul><p id="ed23" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后一个是个大问题——我们今天将看到它的实际应用。</p><p id="6d24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是这有什么大不了的呢？好问题。假设您有一个包含10个要素的数据集，并希望对其进行可视化。但是<strong class="kk iu">如何？</strong> 10个特征= 10个物理尺寸。当涉及到可视化3维以上的任何东西时，我们人类有点糟糕——因此需要降维技术。</p><p id="ffa4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我想在这里做一个重要的说明——<strong class="kk iu">主成分分析不是一个特征选择算法</strong>。我的意思是，主成分分析不会像正向选择那样给出前N个特征。相反，它会给出N个主成分，其中N等于原始特征的数量。</p><p id="6aca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果这听起来令人困惑，我强烈建议你观看这个视频:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="c9d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该视频深入理论推理，并解释了一切比我更好的能力。</p><p id="ec02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们今天的议程如下:</p><ul class=""><li id="f8de" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">加载数据集</li><li id="0d95" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">执行PCA</li><li id="69a9" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">制作令人惊叹的可视化效果</li></ul><p id="7b93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，事不宜迟，让我们开始吧。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="d568" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">数据集和导入</h1><p id="73da" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">我希望这里的一切都超级简单，所以我决定使用众所周知的<a class="ae lu" href="https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv" rel="noopener ugc nofollow" target="_blank">虹膜数据集。</a>它最初只有4个特征——仍然无法可视化。我们将在应用PCA后解决这个可视化问题。</p><p id="dca7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是导入和数据集加载:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="09ef" class="nv mu it nr b gy nw nx l ny nz">import numpy as np <br/>import pandas as pd</span><span id="c6d7" class="nv mu it nr b gy oa nx l ny nz">df = pd.read_csv(‘<a class="ae lu" href="https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv'" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv'</a>)<br/>df.head()</span></pre><p id="315d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">执行上面的代码应该会产生以下数据框:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ob"><img src="../Images/c2e912601f6552c80302cdbad011044a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_y7hwzoUhVlK__c-jjH1Vg.png"/></div></div></figure><p id="9397" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们继续PCA本身。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="eb3c" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">逐步PCA</h1><p id="5cfa" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">以下是所需步骤的简短总结:</p><ol class=""><li id="28a9" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld oc mb mc md bi translated"><strong class="kk iu">缩放数据</strong> —我们不希望由于缩放差异，某些功能被投票认为“更重要”。10m = 10000mm，但是这个算法不知道米和毫米(对不起我们读者)</li><li id="b6e1" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld oc mb mc md bi translated"><strong class="kk iu">计算协方差矩阵</strong> —给出随机向量的每对元素之间的协方差的方阵</li><li id="e23e" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld oc mb mc md bi translated"><strong class="kk iu">特征分解</strong>——我们会讲到的</li></ol><p id="c113" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以让我们从第一个(也是最简单的)开始。</p><h2 id="a1ef" class="nv mu it bd mv od oe dn mz of og dp nd kr oh oi nf kv oj ok nh kz ol om nj on bi translated">数据缩放</h2><p id="a091" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">我已经简要地谈到了我们为什么需要扩展数据的想法，所以我不会在这里重复我自己。把它当作一个必要的先决条件——不仅在这里，对任何机器学习任务都是如此。</p><p id="7e1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了执行缩放，我们将使用来自<em class="mj"> Scikit-Learn </em>的<em class="mj">标准缩放器</em>:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="7957" class="nv mu it nr b gy nw nx l ny nz">from sklearn.preprocessing import StandardScaler</span><span id="71a4" class="nv mu it nr b gy oa nx l ny nz">X_scaled = StandardScaler().fit_transform(X)<br/>X_scaled[:5]</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oo"><img src="../Images/db25b8d7d2d59fbb6490e732e93b3130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n--22iKVTzuANdMfCBVX9w.png"/></div></div></figure><p id="4532" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这部分就这样了。我们继续吧。</p><h2 id="7714" class="nv mu it bd mv od oe dn mz of og dp nd kr oh oi nf kv oj ok nh kz ol om nj on bi translated">协方差矩阵</h2><p id="9215" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">让我们回到这里，理解方差和协方差的区别。方差报告单个随机变量的变化，比如说一个人的体重，协方差报告两个随机变量的变化，比如一个人的体重和身高。</p><p id="d0cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">协方差矩阵的对角线上有方差，其他元素是协方差。</p><p id="5d9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们不要在这里深究数学，因为你有那部分的视频。下面是如何获得<em class="mj"> Numpy </em>中的协方差矩阵:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="d5c5" class="nv mu it nr b gy nw nx l ny nz">features = X_scaled.T<br/>cov_matrix = np.cov(features)<br/>cov_matrix[:5]</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi op"><img src="../Images/97ad3aa650ca4a7403825b25e2c102fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OEa-xBZAEyDeMessP_ZFsg.png"/></div></div></figure><p id="c3ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">酷毙了。如你所见，对角线元素是相同的，矩阵是对称的。接下来，特征分解。</p><h2 id="9470" class="nv mu it bd mv od oe dn mz of og dp nd kr oh oi nf kv oj ok nh kz ol om nj on bi translated">特征分解</h2><p id="7bbf" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">特征分解是将方阵分解为特征向量和特征值的过程。特征向量是简单的单位向量，特征值是给特征向量大小的系数。</p><p id="6cd5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，我们知道我们的协方差矩阵是对称的。原来，对称矩阵的<a class="ae lu" href="https://math.stackexchange.com/questions/82467/eigenvectors-of-real-symmetric-matrices-are-orthogonal" rel="noopener ugc nofollow" target="_blank">特征向量是正交的</a>。对于主成分分析，这意味着我们有第一个主成分，它解释了大部分的差异。与之正交的是第二个主成分，它解释了大部分剩余的方差。对N个主分量重复这一过程，其中N等于原始特征的数量。</p><p id="2ba1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这对我们来说很简单——主成分按照解释的方差百分比排序，因为我们可以决定保留多少。例如，如果我们最初有100个特征，但是前3个主成分解释了95%的方差，那么只保留这3个用于可视化和模型训练是有意义的。</p><p id="4f22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于这不是关于特征分解的数学讲座，我想接下来是时候做一些实际工作了。请自行探索理论部分。</p><p id="6aac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过<em class="mj"> Numpy </em>进行特征分解，它返回一个元组，其中第一个元素代表特征值，第二个元素代表特征向量:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="04f4" class="nv mu it nr b gy nw nx l ny nz">values, vectors = np.linalg.eig(cov_matrix)<br/>values[:5]</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oq"><img src="../Images/f9aff2d9097d5ac989ba222b44cca026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0QUEgQ51bpWXX4ssj1CItQ.png"/></div></div></figure><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="ae56" class="nv mu it nr b gy nw nx l ny nz">vectors[:5]</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi or"><img src="../Images/4755eb8a1e05f28689160370382579bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2M_mC_MgMGXzNsgHQYTnxw.png"/></div></div></figure><p id="dc39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由此，我们可以计算每个主成分的<strong class="kk iu">解释方差</strong>的百分比:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="3f47" class="nv mu it nr b gy nw nx l ny nz">explained_variances = []<br/>for i in range(len(values)):<br/>    explained_variances.append(values[i] / np.sum(values))<br/> <br/>print(np.sum(explained_variances), ‘\n’, explained_variances)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi os"><img src="../Images/ba468d07f4ec496cb9016d5d519f3da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieLEvZhkWwac2mkBi3KI2w.png"/></div></div></figure><p id="08c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一个值只是解释的方差之和，并且必须等于1。第二个值是一个数组，表示每个主成分的解释方差百分比。</p><p id="097b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">前两个主成分解释了数据中大约96%的方差。酷毙了。</p><p id="87ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们深入到一些可视化中，从中我们可以清楚地看到应用PCA的目的。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="13f7" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">形象化</h1><p id="7434" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">以前我们已经得出结论，我们人类看不到任何高于3维的东西。Iris数据集最初有4个维度(4个特征)，但在应用PCA后，我们成功地只用2个主成分解释了大部分差异。</p><p id="dbfa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们将创建一个由这两个组件组成的<em class="mj">熊猫</em> <em class="mj"> DataFrame </em>对象，以及目标类。代码如下:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="42e9" class="nv mu it nr b gy nw nx l ny nz">projected_1 = X_scaled.dot(vectors.T[0])<br/>projected_2 = X_scaled.dot(vectors.T[1])</span><span id="a225" class="nv mu it nr b gy oa nx l ny nz">res = pd.DataFrame(projected_1, columns=[‘PC1’])<br/>res[‘PC2’] = projected_2<br/>res[‘Y’] = y<br/>res.head()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/de86671d47901603c557be19d41634eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*mfdAyb2T3P0TyAJYeLN7hA.png"/></div></figure><p id="4b6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，现在借助Python可视化库的强大功能，让我们首先以一维方式可视化这个数据集——作为一条线。为此，我们需要舍弃第二个主成分。最简单的方法是将Y值硬编码为零，因为散点图需要X轴和Y轴的值:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="64be" class="nv mu it nr b gy nw nx l ny nz">import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="d9ec" class="nv mu it nr b gy oa nx l ny nz">plt.figure(figsize=(20, 10))<br/>sns.scatterplot(res[‘PC1’], [0] * len(res), hue=res[‘Y’], s=200)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ou"><img src="../Images/c780e27505ca3d895aa3d5dc97ceb54e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OcovneYbi-QnVLlkZbjCNw.png"/></div></div></figure><p id="8b79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">只要看看<em class="mj"> Setosa </em>类的可分性就知道了。Virginica 和<em class="mj"> Versicolor </em>更难分类，但我们仍然应该只用一个主成分就能得到大多数正确的分类。</p><p id="18f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们看看这在2D空间中是什么样子:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="19e5" class="nv mu it nr b gy nw nx l ny nz">plt.figure(figsize=(20, 10))<br/>sns.scatterplot(res[‘PC1’], [0] * len(res), hue=res[‘Y’], s=100)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ou"><img src="../Images/a8bb0b50b414a0ce83413881950f5453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M7XoDOcrNwVeniyqwGSgtw.png"/></div></div></figure><p id="d9af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太棒了。为了好玩，试着包含第三个主成分并绘制一个3D散点图。</p><p id="1a13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于本文来说就是这样。让我们在下一部分总结一下。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="e4c2" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">在你离开之前</h1><p id="01ad" class="pw-post-body-paragraph ki kj it kk b kl nl ju kn ko nm jx kq kr nn kt ku kv no kx ky kz np lb lc ld im bi translated">到目前为止，我已经看到了关于PCA的纯数学或纯基于库的文章。使用<em class="mj"> Scikit-Learn </em>很容易做到这一点，但是我想在这里采用一种更加手动的方法，因为缺少这样做的在线文章。</p><p id="ed8b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你已经理解了，并且降维的“抽象概念”不再那么抽象了。</p><p id="8603" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><p id="9bc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mj">喜欢这篇文章吗？成为</em> <a class="ae lu" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="mj">中等会员</em> </a> <em class="mj">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="ov ow gp gr ox oy"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd iu gy z fp pd fr fs pe fu fw is bi translated">通过我的推荐链接加入Medium-Dario rade ci</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">medium.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm lo oy"/></div></div></a></div></div></div>    
</body>
</html>