<html>
<head>
<title>Understanding K-Means, K-Means++ and, K-Medoids Clustering Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解 K-Means、K-Means++和 K-Medoids 聚类算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-k-means-k-means-and-k-medoids-clustering-algorithms-ad9c9fbf47ca?source=collection_archive---------1-----------------------#2020-06-11">https://towardsdatascience.com/understanding-k-means-k-means-and-k-medoids-clustering-algorithms-ad9c9fbf47ca?source=collection_archive---------1-----------------------#2020-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4769" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">K-means、K-means++和 K-Medoids 聚类算法及其关系概述。本文还包括它的从头开始的实现和使用 sklearn 库。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/741590104270e87c1a9250a8b7b2eccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pRXMkCPrkqCFCXnn"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卢卡斯·布拉塞克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="26ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">聚类是一种无监督的机器学习技术，它将群体或数据点分成几个组或聚类，使得相同组中的数据点与相同组中的其他数据点更相似，而与其他组中的数据点不相似。</p><ul class=""><li id="61d9" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">同一簇中的点彼此更接近。</li><li id="1ab2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">不同簇中的点相距很远。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/08f53ae17f44c24732a775213dec0ccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*ofQAxEbhten0c1b1VrvTEQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://medium.com/@Experfy/k-means-clustering-in-text-data-669358b54081" rel="noopener">来源</a>，样本数据点，(图 1)</p></figure><p id="163b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的二维数据集样本中，可以看到数据集形成了 3 个相距很远的聚类，并且同一聚类中的点彼此靠近。</p><h2 id="64ac" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">聚类的衡量标准是什么？</h2><p id="7eba" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">如果聚类具有最大的簇间距离和最小的簇内距离，则被认为是最佳的。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="f144" class="mk ml it nj b gy nn no l np nq"><strong class="nj iu"><em class="nr">Notes to avoid any confusion:</em></strong></span><span id="50bd" class="mk ml it nj b gy ns no l np nq"><strong class="nj iu">Intracluster distance:</strong> Distance between two point in the same cluster.<br/><strong class="nj iu">Intercluster distance:</strong> Distance between two points in the different clusters.</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/0895a9f847408bae63c157307b1a56ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*ev2R760JdqJwtQDGCGJ4_w.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)、簇内和簇间距离(图片 2)</p></figure><p id="3ad5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图(图 2)描述了什么是簇间和簇内距离。</p><p id="9a9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以使用各种评估度量来测量形成的集群的有效性。</p><blockquote class="nt nu nv"><p id="603f" class="kz la nr lb b lc ld ju le lf lg jx lh nw lj lk ll nx ln lo lp ny lr ls lt lu im bi translated"><strong class="lb iu">邓恩指数:</strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/7a029aaf30a0c7951693f8fb6e9bd66c.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*Ml1cUinYf_H2jBqJq77hFg.png"/></div></figure><p id="2ebe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述函数的<em class="nr">分子</em>测量属于两个不同聚类的每两个点(x_i，x_j)之间的最大距离。这代表<em class="nr">星团内距离</em>。</p><p id="8e8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述函数的<em class="nr">分母</em>测量属于同一聚类的每两个点(y_i，y_j)之间的最大距离。这代表<em class="nr">集群间距离</em>。</p><p id="b6ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有邓恩指数最大值的聚类被认为是最好的。</p><blockquote class="nt nu nv"><p id="67f7" class="kz la nr lb b lc ld ju le lf lg jx lh nw lj lk ll nx ln lo lp ny lr ls lt lu im bi translated"><strong class="lb iu">剪影分析:</strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/55333d255f1c9c222c5517d43bd2f6c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*jsW4zF_RT11hBZe0axReMw.png"/></div></figure><p id="3c8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这用于确定聚类之间的分离程度。对于每个样本。<strong class="lb iu"> a_i </strong>表示同一个聚类中所有数据点的平均距离。<strong class="lb iu"> b_i </strong>代表离最近聚类中所有数据点的平均距离。</p><p id="b0bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SA 的系数可以取区间[-1，1]内的值。</p><ul class=""><li id="8eee" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">SA = 0:样本非常接近相邻的聚类。</li><li id="2480" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">SA = 1:样本远离相邻聚类。</li><li id="7808" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">SA = -1:样本被分配到错误的聚类。</li></ul><p id="655d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们希望系数尽可能大，并且接近 1。</p><p id="2ff8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有不同类型的聚类技术，我们将讨论其中的一种。</p></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="596a" class="oi ml it bd mm oj ok ol mp om on oo ms jz op ka mv kc oq kd my kf or kg nb os bi translated">k 均值聚类:</h1><p id="b150" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">K-Means 算法是一种基于质心的聚类技术。该技术将数据集聚类成 k 个具有几乎相等数量的点的不同聚类。每个聚类是用一个质心点来表示的 k-means 聚类算法。</p><blockquote class="nt nu nv"><p id="fb51" class="kz la nr lb b lc ld ju le lf lg jx lh nw lj lk ll nx ln lo lp ny lr ls lt lu im bi translated"><strong class="lb iu">什么是质心点？</strong></p></blockquote><p id="f10c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">形心点是代表其群集的点。形心点是集合中所有点的平均值，将在每一步中发生变化，计算方法如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/7de6d448f7e19d6d63f1f088103f0285.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*0YpQaKAer4D-dWnzeJKHXA.png"/></div></figure><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="745f" class="mk ml it nj b gy nn no l np nq">For the above equation,<br/>C_i: i'th Centroid<br/>S_i: All points belonging to set_i with centroid as C_i<br/>x_j: j'th point from the set<br/>||S_i||: number of points in set_i</span></pre><p id="e9a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-Means 算法的思想是找到 K 个质心点，并且数据集中的每个点将属于具有最小欧几里德距离的 K 个集合中的任一个。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/e71e025684de931570d42e3dbf2dbf4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*pEUW6L-Xx6imQvB7-i8Y6A.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3</p></figure><p id="dec9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据上图(图 3)，点 x_i 到所有三个质心的距离为 d1、d2、d3，点 x_i 距离质心 _3 最近，距离为 d3，因此点 x_i 将属于质心 _3 的聚类，并且该过程将对数据集中的所有点继续进行。</p><h2 id="29d7" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">K 均值的成本函数:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/cb25ee4dd0fca28d24e8a512582dbcfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*efmspaoE4wD4T9euwgqePA.png"/></div></figure><p id="71e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-Means 算法的思想是找到 K 个质心点(C1，C1，.。。C_k)通过最小化每个聚类上该点与其质心之间距离的平方和。</p><p id="fb44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个代价是 NP 难的，并且具有指数级的时间复杂度。所以我们使用劳埃德算法的近似思想。</p><h2 id="4a90" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">劳埃德算法:</h2><p id="b57a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">劳埃德算法是一种用于聚类点的近似迭代算法。该算法的步骤如下:</p><ol class=""><li id="3b8a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ow mb mc md bi translated">初始化</li><li id="49c7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ow mb mc md bi translated">分配</li><li id="108c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ow mb mc md bi translated">更新质心</li><li id="ee20" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ow mb mc md bi translated">重复步骤 2 和 3，直到收敛。</li></ol><blockquote class="nt nu nv"><p id="beb2" class="kz la nr lb b lc ld ju le lf lg jx lh nw lj lk ll nx ln lo lp ny lr ls lt lu im bi translated"><strong class="lb iu">K 均值算法的迭代实现:</strong></p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="8f08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤#1:初始化:</strong></p><p id="683d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">初始 k 形心是从数据集中随机选取的(第 27-28 行)。</p><p id="3494" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤#2:分配:</strong></p><p id="6fc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于数据集中的每个点，找出该点和所有质心之间的欧几里德距离(第 33 行)。该点将被分配给质心最近的聚类。</p><p id="76a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤#3:质心的更新:</strong></p><p id="9e42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用新的平均值更新质心的值(第 39–40 行)。</p><p id="007a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤#4:重复:</strong></p><p id="5cca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除非达到收敛，否则重复步骤 2 和 3。如果实现了收敛，则中断循环(第 43 行)。收敛是指质心的先前值等于更新值的情况。</p><h2 id="dddf" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">结果:</h2><p id="7622" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">初始数据集的绘图(图 4)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/35786db2ff86e0415eae78864242d3c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*vWwg6mTug5lhLMpIaXaQAQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集的图(图 4)</p></figure><p id="510a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">k=2 的聚类结果图(图 5)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e14bb7a4f39b4b5ee7e4331f7fef9413.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*8fji_m4ZHOvnOplNq9eWyg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k=2 时的聚类图(图 5)</p></figure><p id="3e1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">k=3 的聚类结果图(图 6)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/ac23dbf25ed71553fa36f35a85dd504d.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*VpZieb8JA1tqXVWNDNPMKw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k=3 时的聚类图(图 6)</p></figure></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="9435" class="oi ml it bd mm oj ok ol mp om on oo ms jz op ka mv kc oq kd my kf or kg nb os bi translated">K-Means++聚类:</h1><p id="f545" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在使用劳埃德 K 均值聚类算法寻找初始质心的情况下，我们使用了随机化。初始的 k 形心是从数据点中随机选取的。</p><p id="b66a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种选取 k-质心点的随机化导致了初始化敏感性的问题。这个问题倾向于影响最终形成的簇。最终形成的聚类取决于初始质心是如何选取的。</p><p id="87d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是一些聚类的结果，其中质心的初始化是不同的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/1a18b78db2ad7fb63b06b74ec1efeffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*qTH-gjXCJuWTFmYBKm7Srg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过不同的初始化形成不同的最终聚类(图 7)</p></figure><p id="0b39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上图(图 7)中，最终形成的簇是不同的，因为最终形成的簇依赖于质心的初始化。在上面图像的第一部分，可以观察到质心(黑色*)和星团没有正确形成。在上面图像的第二部分，可以观察到质心(黑色*)和星团的形成。</p><p id="be89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两种方法可以避免初始化敏感性的问题:</p><ol class=""><li id="3427" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ow mb mc md bi translated"><strong class="lb iu">重复 K-means: </strong>多次重复算法和质心初始化，选择簇内距离小、簇间距离大的聚类方法。</li><li id="e301" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ow mb mc md bi translated"><strong class="lb iu"> K-Means++: </strong> K-Means++是一种智能质心初始化技术。</li></ol><p id="a682" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上两种方法可以用来避免初始化敏感性的问题，但在这两种方法中，K-Means++是最好的方法。</p><blockquote class="nt nu nv"><p id="cb6b" class="kz la nr lb b lc ld ju le lf lg jx lh nw lj lk ll nx ln lo lp ny lr ls lt lu im bi translated"><strong class="lb iu">K-Means ++是如何工作的？</strong></p></blockquote><p id="fe30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-Means++是一种智能质心初始化技术，算法的其余部分与 K-Means 相同。质心初始化的步骤如下:</p><ul class=""><li id="5eb1" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">随机选取第一个质心点(C1)。</li><li id="00d3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">计算数据集中所有点与所选质心的距离。x_i 点到最远质心的距离可以通过下式计算</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/a29eea31f67f6c934ecf8be6591cbabd.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*6UOjBbiiM7t-Ciz4x5la5Q.png"/></div></figure><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="d980" class="mk ml it nj b gy nn no l np nq">d_i: Distance of x_i point from the farthest centroid<br/>m: number of centroids already picked</span></pre><ul class=""><li id="8fa1" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">将点 x_i 作为新的质心，它具有与 d_i 成比例的最大概率。</li><li id="3203" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">重复上述两个步骤，直到你找到 k 形心。</li></ul><blockquote class="nt nu nv"><p id="2714" class="kz la nr lb b lc ld ju le lf lg jx lh nw lj lk ll nx ln lo lp ny lr ls lt lu im bi translated"><strong class="lb iu">使用 sklearn 实现 K-means++:</strong></p></blockquote><p id="2124" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面我们已经从头开始讨论了 K-Means 的迭代方法，为了实现 K-Means++算法，我们将使用 sklearn 库。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="06f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">实施和结果演练:</strong></p><ul class=""><li id="f1e3" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">加载或创建数据集(第 8 行)</li><li id="3f67" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">为数据集实现 k-means++算法(第 9 行)</li><li id="4a4b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">绘制原始数据集以观察数据集(第 25–26 行)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/d1079fcd305d2a90c93a6babf673c7d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*tuMmRG78qbz78hOKco4KCQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据点图(图 8)</p></figure><ul class=""><li id="f7a2" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">找到 k 形心点</li><li id="0a37" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">绘制聚类结果(圆形)和 k 形心(红色*)(第 29-32 行)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/deb4ad95e96ed511977b1e6b020f5132.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*GxBGEUL5qoCQZJqVPcFiKQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">聚类结果图(图 9)</p></figure></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><h1 id="fba2" class="oi ml it bd mm oj ok ol mp om on oo ms jz op ka mv kc oq kd my kf or kg nb os bi translated">K-Medoids 聚类:</h1><p id="2ab9" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">K-Means 和 K-Means++聚类的一个问题是最终的质心是不可解释的，或者换句话说，质心不是实际的点，而是该聚类中存在的点的平均值。这是不像数据集中真实点的 3-质心坐标。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/14bb09f379bb66a428fee3a5e64d84c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*Ol9ftZsK90ezeJe2RuO-LA.png"/></div></figure><p id="6bb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-Medoids 聚类的思想是将最终的质心作为实际的数据点。这个结果使形心可以解释。</p><p id="7e52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-Medoids 聚类的算法被称为 Medoids 周围划分(PAM ),它与 Lloyd's 算法几乎相同，只是在更新步骤上略有变化。</p><p id="fffd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PAM 算法要遵循的步骤:</p><ul class=""><li id="c07c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">初始化:</strong>与 K-Means++相同</li><li id="22cd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">赋值:</strong>同 K 均值赋值</li><li id="6bf3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">更新质心:</strong>在 K 均值的情况下，我们正在计算聚类中所有点的均值。但是对于 PAM 算法，质心的更新是不同的。如果在一个群集中有 m 个点，则用该群集中的所有其他(m-1)个点交换先前的质心，并将该点最终确定为具有最小损失的新质心。最小损失通过以下成本函数计算:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/4392089dbed304851163ba5faf97909a.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*K4pfLEYVcOtg5_tbea591Q.png"/></div></figure><ul class=""><li id="f290" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">重复:</strong>与 K 均值相同</li></ul><h1 id="297d" class="oi ml it bd mm oj pi ol mp om pj oo ms jz pk ka mv kc pl kd my kf pm kg nb os bi translated">如何挑选 K 的最佳值？</h1><p id="fd75" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">K 的最佳值可以使用<em class="nr">弯头法计算。K-Means、K-Means 和 K-Medoids 技术的成本函数是最小化簇间距离和最大化簇内距离。这可以通过最小化上面文章中讨论的损失函数来实现:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/2dcb046ce682a86b25079b042f29d6fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*wJlGAXk0X75cuJmAE6b3xg.png"/></div></figure><p id="8873" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了确定正确的“K ”,在损失和 K 之间画一个图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/3c2ae08a7b69d80cb422776b9fd2dcc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*JgaYTEyVfYV7H915rwJ3Yw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">损耗与 K 的关系图(图 10)</p></figure><p id="ca31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于上述曲线，可以观察到随着“k”值的增加，损耗减少。为了找到绘制 k-聚类的最佳 k 值，我们可以选择 k=3。</p><blockquote class="pp"><p id="139b" class="pq pr it bd ps pt pu pv pw px py lu dk translated">感谢您的阅读！</p></blockquote></div></div>    
</body>
</html>