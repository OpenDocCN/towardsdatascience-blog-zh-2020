<html>
<head>
<title>DBSCAN Clustering — Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DBSCAN 集群—解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dbscan-clustering-explained-97556a2ad556?source=collection_archive---------1-----------------------#2020-04-22">https://towardsdatascience.com/dbscan-clustering-explained-97556a2ad556?source=collection_archive---------1-----------------------#2020-04-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f535" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">详细的理论解释和 scikit-learn 实现</h2></div><p id="22f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">聚类是一种对一组数据点进行分组的方法，将相似的数据点分组在一起。因此，聚类算法寻找数据点之间的相似或相异之处。聚类是一种无监督的学习方法，因此没有与数据点相关联的标签。该算法试图找到数据的底层结构。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/780ed19044275029e9348d3032281fb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y2k0GWgiN3ysY-ERGBidyw.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">简·梅乌斯在<a class="ae lu" href="https://unsplash.com/s/photos/cluster?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="8e79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有不同的方法和算法来执行聚类任务，这些任务可以分为三个子类别:</p><ul class=""><li id="f5fe" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">基于分区的聚类:例如 k 均值、k 中值</li><li id="6e22" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">等级聚类:例如聚集、分裂</li><li id="5e95" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">基于密度的聚类:例如 DBSCAN</li></ul><p id="7489" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我将尝试详细解释 DBSCAN 算法。如果你想了解其他类型的聚类算法，你也可以访问下面的帖子:</p><div class="mj mk gp gr ml mm"><a rel="noopener follow" target="_blank" href="/k-means-clustering-explained-4528df86a120"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">k-均值聚类—已解释</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">详细的理论解释和 scikit-learn 实现</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na lo mm"/></div></div></a></div><div class="mj mk gp gr ml mm"><a rel="noopener follow" target="_blank" href="/hierarchical-clustering-explained-e58d2f936323"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">分层聚类—已解释</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">理论解释和科学学习范例</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="nb l mx my mz mv na lo mm"/></div></div></a></div></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h1 id="3492" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated"><strong class="ak">基于密度的聚类</strong></h1><p id="6eb7" class="pw-post-body-paragraph ki kj it kk b kl ob ju kn ko oc jx kq kr od kt ku kv oe kx ky kz of lb lc ld im bi translated">基于分区和层次聚类技术对于正常形状的聚类非常有效。然而，当涉及到任意形状的聚类或检测异常值时，基于密度的技术更有效。</p><p id="3325" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，使用 k-means 算法可以很容易地将下图中的数据集分成三个聚类。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi og"><img src="../Images/521f282d0720b9c8216b450b352f925e.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*IPxS4b84CEs1XPiEIHop2A.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">k 均值聚类</p></figure><p id="6df7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请考虑以下数字:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/3c22edd074b34d37807247bc1e88a4d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*nCt89v-DPIN05mrWOpEiDw.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d3f3160c96a10ffcd97d738b2cb72639.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*Rb3xvN8wUcYV3cZFmKhzlw.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/c19378a473e01d5ba2947d1fb9440712.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*vYK_uMNTUAtYNo-RG7XeHA.png"/></div></figure><p id="9836" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些图中的数据点以任意形状分组或者包含异常值。基于密度的聚类算法在发现高密度区域和离群点方面非常有效。对于某些任务来说，检测异常值是非常重要的，例如异常检测。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h1 id="94c1" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated"><strong class="ak"> DBSCAN 算法</strong></h1><p id="913d" class="pw-post-body-paragraph ki kj it kk b kl ob ju kn ko oc jx kq kr od kt ku kv oe kx ky kz of lb lc ld im bi translated">DBSCAN 代表<strong class="kk iu">d</strong>en sity-<strong class="kk iu">b</strong>ass<strong class="kk iu">c</strong>lustering of<strong class="kk iu">a</strong>应用与<strong class="kk iu"> n </strong> oise。它能够找到任意形状的聚类和带有噪声的聚类(即异常值)。</p><p id="7036" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DBSCAN 背后的主要思想是，如果一个点靠近来自该簇的许多点，则该点属于该簇。</p><p id="0e04" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DBSCAN 有两个关键参数:</p><ul class=""><li id="77f0" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated"><strong class="kk iu"> eps </strong>:指定邻居的距离。如果两点之间的距离小于或等于 eps，则认为这两点是相邻的。</li><li id="39b5" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated"><strong class="kk iu"> minPts: </strong>定义一个聚类的最小个数据点。</li></ul><p id="4d6c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基于这两个参数，点被分类为核心点、边界点或异常点:</p><ul class=""><li id="a7db" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated"><strong class="kk iu">核心点:</strong>如果一个点在其半径为 eps 的周围区域中至少有 minPts 个数的点(包括该点本身)，则该点是核心点。</li><li id="753f" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated"><strong class="kk iu">边界点:</strong>如果一个点可以从一个核心点到达，并且其周围区域内的点数小于 minPts，那么这个点就是边界点。</li><li id="32ce" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated"><strong class="kk iu">离群点:</strong>如果一个点不是核心点，并且从任何核心点都不可达，那么这个点就是离群点。</li></ul><p id="6dae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些观点可以用形象化来更好地解释。下图摘自维基百科:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/700a65cc6adaa27b46b2210d8b7e4a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*nZDyFlpq7X_k0zEYYAKfHg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><a class="ae lu" href="https://en.wikipedia.org/wiki/DBSCAN" rel="noopener ugc nofollow" target="_blank">图源</a></p></figure><p id="ea8a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，minPts 是 4。红色点是核心点，因为在其半径为 eps 的周围区域内至少有<strong class="kk iu">4 个点。该区域在图中用圆圈表示。黄色点是边界点，因为它们可以从核心点到达，并且其邻域内的点少于 4 个。可到达意味着在核心点的周围区域。点 B 和 C 在其邻域内(即以 eps 为半径的周围区域)有两个点(包括点本身)。最后，N 是一个异常值，因为它不是一个核心点，不能从核心点到达。</strong></p><p id="bd34" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经学习了参数和不同类型点的定义。现在我们可以谈谈算法是如何工作的。这其实很简单:</p><ul class=""><li id="ba24" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">确定 minPts 和 eps。</li><li id="2b94" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">随机选择一个起始点，使用半径 eps 确定其邻域。如果邻域中至少有 minPts 个数的点，则将该点标记为核心点，并开始形成聚类。如果不是，则该点被标记为噪声。一旦聚类形成开始(假设聚类 A)，初始点邻域内的所有点都成为聚类 A 的一部分。如果这些新点也是核心点，则它们邻域内的点也被添加到聚类 A 中。</li></ul><blockquote class="ol om on"><p id="2504" class="ki kj oo kk b kl km ju kn ko kp jx kq op ks kt ku oq kw kx ky or la lb lc ld im bi translated">注意:被标记为噪声的点可以被重新访问，并且是聚类的一部分。</p></blockquote><ul class=""><li id="04b1" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">下一步是在前面步骤中没有访问过的点中随机选择另一个点。然后同样的程序适用。</li><li id="31e7" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">当所有点都被访问时，该过程结束。</li></ul><blockquote class="ol om on"><p id="4145" class="ki kj oo kk b kl km ju kn ko kp jx kq op ks kt ku oq kw kx ky or la lb lc ld im bi translated">使用 k-means 算法中的距离测量方法来确定点之间的距离。最常用的方法是欧氏距离。</p></blockquote><p id="9372" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过应用这些步骤，DBSCAN 算法能够找到高密度区域并将它们从低密度区域中分离出来。</p><p id="8a9d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个集群包括相邻的核心点(即彼此可到达的)以及这些核心点的所有边界点。形成集群的必要条件是至少有一个核心点。尽管可能性很小，但我们可能有一个只有一个核心点及其边界点的集群。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h1 id="765f" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated"><strong class="ak"> Scikit-learn 实现</strong></h1><p id="f690" class="pw-post-body-paragraph ki kj it kk b kl ob ju kn ko oc jx kq kr od kt ku kv oe kx ky kz of lb lc ld im bi translated">我们首先使用 scikit-learn 的 datasets 模块创建一个样本数据集。创建样本数据点后，我们将使用 scikit-learn 的预处理模块中的 StandardScaler 类对值进行归一化。</p><blockquote class="ol om on"><p id="664f" class="ki kj oo kk b kl km ju kn ko kp jx kq op ks kt ku oq kw kx ky or la lb lc ld im bi translated"><strong class="kk iu">注意</strong>:归一化数值很重要，因为这样更容易找到邻域半径(eps)的合适距离。</p></blockquote><p id="37b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先导入库:</p><pre class="lf lg lh li gt os ot ou ov aw ow bi"><span id="f9dd" class="ox nk it ot b gy oy oz l pa pb">import numpy as np</span><span id="bdca" class="ox nk it ot b gy pc oz l pa pb">from sklearn.datasets import make_blobs<br/>from sklearn.preprocessing import StandardScaler</span><span id="abfc" class="ox nk it ot b gy pc oz l pa pb">import matplotlib.pyplot as plt<br/>%matplotlib inline</span></pre><p id="d3bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将创建一个包含 3 个聚类的数据集，每个聚类的标准偏差为 0.5。样本数量是 400，我们也选择三个点作为质心(聚类的中心)。您可以使用 make_blobs 函数随意调整这些参数。</p><pre class="lf lg lh li gt os ot ou ov aw ow bi"><span id="8558" class="ox nk it ot b gy oy oz l pa pb">#Determine centroids<br/>centers = [[0.5, 2], [-1, -1], [1.5, -1]]</span><span id="1ab5" class="ox nk it ot b gy pc oz l pa pb">#Create dataset<br/>X, y = make_blobs(n_samples=400, centers=centers, <br/>                  cluster_std=0.5, random_state=0)</span><span id="aebc" class="ox nk it ot b gy pc oz l pa pb">#Normalize the values<br/>X = StandardScaler().fit_transform(X)</span></pre><p id="9d62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还可以绘制数据集，以查看每个聚类的外观:</p><pre class="lf lg lh li gt os ot ou ov aw ow bi"><span id="eec6" class="ox nk it ot b gy oy oz l pa pb">plt.figure(figsize=(10,6))<br/>plt.scatter(X[:,0], X[:,1], c=y, cmap='Paired')</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/23981af13253a8843161489643742370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*Bv_yG9DugP-knE4JoNL04g.png"/></div></figure><p id="42da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在可以创建一个 DBSCAN 对象并拟合数据:</p><pre class="lf lg lh li gt os ot ou ov aw ow bi"><span id="f0e0" class="ox nk it ot b gy oy oz l pa pb">from sklearn.cluster import DBSCAN</span><span id="e6dd" class="ox nk it ot b gy pc oz l pa pb">db = DBSCAN(eps=0.4, min_samples=20)</span><span id="ade9" class="ox nk it ot b gy pc oz l pa pb">db.fit(X)</span></pre><p id="0e0a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们只需要使用<strong class="kk iu"> eps </strong>和<strong class="kk iu"> min_samples </strong>参数定义 eps 和 minPts 值。</p><blockquote class="ol om on"><p id="62ea" class="ki kj oo kk b kl km ju kn ko kp jx kq op ks kt ku oq kw kx ky or la lb lc ld im bi translated"><strong class="kk iu">注意</strong>:我们不必指定 DBSCAN 的聚类数，这是 DBSCAN 优于 k-means 聚类的一大优势。</p></blockquote><p id="f529" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看一下 DBSCAN 确定的集群:</p><pre class="lf lg lh li gt os ot ou ov aw ow bi"><span id="3b67" class="ox nk it ot b gy oy oz l pa pb">y_pred = db.fit_predict(X)</span><span id="0665" class="ox nk it ot b gy pc oz l pa pb">plt.figure(figsize=(10,6))<br/>plt.scatter(X[:,0], X[:,1],c=y_pred, cmap='Paired')<br/>plt.title("Clusters determined by DBSCAN")</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/9a36405746b5f634ced8fd682d97531b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*h8oHHdd_8elx5708A6-Kqg.png"/></div></figure><p id="3c54" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它能够检测异常值(用蓝色标记)。我们可以使用 labels_ attribute 来访问数据点的标签。噪声(或离群值)被赋予-1 标签。让我们检查异常值的数量:</p><pre class="lf lg lh li gt os ot ou ov aw ow bi"><span id="f906" class="ox nk it ot b gy oy oz l pa pb">db.labels_[db.labels_ == -1].size<br/>18</span></pre><p id="fc94" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该样本数据集中的分类实际上并不具有任意形状。但是 DBSCAN 在检测异常值方面表现得非常好，这对于基于分区(例如 k-means)或分层(例如凝聚)的聚类技术来说是不容易的。如果您还将 DBSCAN 应用于具有任意形状的集群的数据集，您也会看到 DBSCAN 的成功。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h1 id="751d" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated"><strong class="ak">DBS can 的利弊</strong></h1><p id="356c" class="pw-post-body-paragraph ki kj it kk b kl ob ju kn ko oc jx kq kr od kt ku kv oe kx ky kz of lb lc ld im bi translated"><strong class="kk iu">优点:</strong></p><ul class=""><li id="ec3d" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">不需要预先指定簇的数量。</li><li id="8618" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">适用于任意形状的集群。</li><li id="cb0f" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">DBSCAN 对异常值是鲁棒的，并且能够检测异常值。</li></ul><p id="1fa8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:</strong></p><ul class=""><li id="23d0" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">在某些情况下，确定适当的邻域距离(eps)并不容易，这需要领域知识。</li><li id="45d8" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">如果集群在集群内密度方面差异很大，那么 DBSCAN 就不太适合定义集群。聚类的特征由 eps-minPts 参数的组合来定义。因为我们将一个 eps-minPts 组合传递给该算法，所以它不能很好地推广到具有很大不同密度的聚类。</li></ul></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><p id="20dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读。如果您有任何反馈，请告诉我。</p><p id="57c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">参考文献</strong></p><ul class=""><li id="e1ae" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated"><a class="ae lu" href="https://en.wikipedia.org/wiki/DBSCAN" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/DBSCAN</a></li></ul></div></div>    
</body>
</html>