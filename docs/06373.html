<html>
<head>
<title>Quickstart: Apache Spark on Kubernetes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">快速入门:Kubernetes上的Apache Spark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quickstart-apache-spark-on-kubernetes-18d01ea5df7d?source=collection_archive---------48-----------------------#2020-05-21">https://towardsdatascience.com/quickstart-apache-spark-on-kubernetes-18d01ea5df7d?source=collection_archive---------48-----------------------#2020-05-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0b45" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Kubernetes的本地Apache Spark操作器，让您的大负载平稳运行</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/b08ef9ae33760d99a5af47cc33adc8f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XGFbv6WHxs2YuZl8"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><a class="ae kw" href="https://unsplash.com/@finalhugh?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">金赛</a>在<a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="8b1a" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">介绍</h1><h2 id="060f" class="lp ky iq bd kz lq lr dn ld ls lt dp lh lu lv lw lj lx ly lz ll ma mb mc ln md bi translated">Kubernetes的Apache Spark操作员</h2><p id="a7e6" class="pw-post-body-paragraph me mf iq mg b mh mi jr mj mk ml ju mm lu mn mo mp lx mq mr ms ma mt mu mv mw ij bi translated">自2014年由谷歌推出以来，Kubernetes与Docker本身一起获得了很大的人气，自2016年以来，它已成为事实上的容器编排者，并成为市场标准。在<strong class="mg ir">所有</strong><em class="mx">主要云</em>中提供云管理版本。<a class="ae kw" href="https://cloud.google.com/kubernetes-engine/" rel="noopener ugc nofollow" target="_blank">【1】</a><a class="ae kw" href="https://aws.amazon.com/eks/" rel="noopener ugc nofollow" target="_blank">【2】</a><a class="ae kw" href="https://docs.microsoft.com/en-us/azure/aks/" rel="noopener ugc nofollow" target="_blank">【3】</a>(包括<a class="ae kw" href="https://www.digitalocean.com/products/kubernetes/" rel="noopener ugc nofollow" target="_blank">数字海洋</a><a class="ae kw" href="https://www.alibabacloud.com/product/kubernetes" rel="noopener ugc nofollow" target="_blank">阿里巴巴</a>)。</p><p id="27a1" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">随着这种流行，出现了orchestrator的各种实现和<em class="mx">用例</em>，其中包括使用容器的<a class="ae kw" href="https://kubernetes.io/docs/tutorials/stateful-application/" rel="noopener ugc nofollow" target="_blank">有状态应用</a>和<a class="ae kw" href="https://vitess.io/zh/docs/get-started/kubernetes/" rel="noopener ugc nofollow" target="_blank">数据库的执行。</a></p><p id="7a55" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">托管编排数据库的动机是什么？这是一个很好的问题。但是让我们关注一下在Kubernetes上运行工作负载的<a class="ae kw" href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md" rel="noopener ugc nofollow" target="_blank"> Spark操作符</a>。</p><p id="e9a9" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">一个本地Spark运营商<a class="ae kw" href="https://github.com/kubernetes/kubernetes/issues/34377" rel="noopener ugc nofollow" target="_blank">的想法在2016年</a>出现，在此之前你不能本地运行Spark作业，除非是一些<em class="mx">的hacky替代品</em>，比如<a class="ae kw" href="https://kubernetes.io/blog/2016/03/using-spark-and-zeppelin-to-process-big-data-on-kubernetes/" rel="noopener ugc nofollow" target="_blank">在Kubernetes内部运行Apache Zeppelin </a>或者在Kubernetes内部创建你的<a class="ae kw" href="https://github.com/kubernetes/examples/tree/master/staging/spark" rel="noopener ugc nofollow" target="_blank"> Apache Spark集群(来自GitHub上的官方Kubernetes组织)</a>引用独立模式下的<a class="ae kw" href="http://spark.apache.org/docs/latest/spark-standalone.html" rel="noopener ugc nofollow" target="_blank">Spark workers</a>。</p><p id="e2b8" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">然而，本地执行更有意思的是利用负责分配资源的Kubernetes调度器，提供弹性和更简单的接口来管理Apache Spark工作负载。</p><p id="7a7b" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">考虑到这一点，<a class="ae kw" href="https://issues.apache.org/jira/browse/SPARK-18278" rel="noopener ugc nofollow" target="_blank"> Apache Spark Operator开发得到关注</a>，合并发布为<a class="ae kw" href="https://spark.apache.org/releases/spark-release-2-3-0.html" rel="noopener ugc nofollow" target="_blank"> Spark版本2.3.0 </a>于2018年<a class="ae kw" href="https://spark.apache.org/news/index.html" rel="noopener ugc nofollow" target="_blank">2月</a>推出。</p><p id="375e" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">如果你渴望阅读更多关于Apache Spark提案的内容，你可以前往Google Docs上发布的<a class="ae kw" href="https://docs.google.com/document/d/1_bBzOZ8rKiOSjQg78DXOA3ZBIo_KkDJjqxVuq0yXdew/edit#heading=h.9bhogel14x0y" rel="noopener ugc nofollow" target="_blank">设计文档。</a></p><h2 id="cff8" class="lp ky iq bd kz lq lr dn ld ls lt dp lh lu lv lw lj lx ly lz ll ma mb mc ln md bi translated">为什么是Kubernetes？</h2><p id="5dbb" class="pw-post-body-paragraph me mf iq mg b mh mi jr mj mk ml ju mm lu mn mo mp lx mq mr ms ma mt mu mv mw ij bi translated">由于公司目前正在寻求通过广为流传的数字化转型来重塑自己，以提高竞争力，最重要的是，在日益活跃的市场中生存下来，因此常见的方法包括大数据、人工智能和云计算<a class="ae kw" href="https://www.zdnet.com/article/how-to-use-cloud-computing-and-big-data-to-support-digital-transformation/" rel="noopener ugc nofollow" target="_blank">【1】</a><a class="ae kw" href="https://digitalhealth.london/cloud-big-data-ai-lead-nhs-digital-transformation/" rel="noopener ugc nofollow" target="_blank">【2】</a><a class="ae kw" href="https://www.ibm.com/blogs/cloud-computing/2018/11/05/guiding-framework-digital-transformation-garage/" rel="noopener ugc nofollow" target="_blank">【3】</a>。</p><p id="5e19" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">关于在大数据环境中使用云计算而不是本地服务器的优势的有趣对比，可以在<a class="ae kw" href="https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html" rel="noopener ugc nofollow" target="_blank"> Databricks博客</a>上阅读，该博客是由Apache Spark 的创建者创建的<a class="ae kw" href="https://www.washingtonpost.com/news/the-switch/wp/2016/06/09/this-is-where-the-real-action-in-artificial-intelligence-takes-place/" rel="noopener ugc nofollow" target="_blank">公司。</a></p><p id="f38f" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">随着我们看到云计算的广泛采用(即使是那些能够负担得起硬件并在本地运行的公司)，我们注意到大多数云实施都没有<a class="ae kw" href="https://hadoop.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Hadoop </a>，因为数据团队(BI/数据科学/分析)越来越多地选择使用像<a class="ae kw" href="https://cloud.google.com/bigquery/" rel="noopener ugc nofollow" target="_blank"> Google BigQuery </a>或<a class="ae kw" href="https://aws.amazon.com/redshift/" rel="noopener ugc nofollow" target="_blank"> AWS Redshift </a>这样的工具。因此，仅仅为了使用<a class="ae kw" href="https://hortonworks.com/apache/yarn/" rel="noopener ugc nofollow" target="_blank"> YARN </a>作为资源管理器而加速运行Hadoop是没有意义的。</p><p id="f9a2" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">另一种方法是使用Hadoop集群提供者，如<a class="ae kw" href="https://cloud.google.com/dataproc" rel="noopener ugc nofollow" target="_blank"> Google DataProc </a>或<a class="ae kw" href="https://aws.amazon.com/emr/" rel="noopener ugc nofollow" target="_blank"> AWS EMR </a>来创建临时集群。仅举几个例子。</p><p id="865b" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">为了更好地理解Spark Operator的设计，GitHub 上来自<a class="ae kw" href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operatoR/blob/master/docs/design.md#the-crd-controller" rel="noopener ugc nofollow" target="_blank"> GCP的doc是显而易见的。</a></p><h1 id="9688" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">让我们动手吧！</h1><h2 id="254e" class="lp ky iq bd kz lq lr dn ld ls lt dp lh lu lv lw lj lx ly lz ll ma mb mc ln md bi translated">预热发动机</h2><p id="6ec3" class="pw-post-body-paragraph me mf iq mg b mh mi jr mj mk ml ju mm lu mn mo mp lx mq mr ms ma mt mu mv mw ij bi translated">既然话已经传开了，那就让我们把它弄到手，展示一下发动机的运转吧。为此，让我们使用:</p><p id="e32c" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">一旦安装了必要的工具，就有必要在<code class="fe nd ne nf ng b">PATH</code>环境变量中包含Apache Spark path，以简化Apache Spark可执行文件的调用。只需运行:</p><pre class="kh ki kj kk gt nh ng ni nj aw nk bi"><span id="72f3" class="lp ky iq ng b gy nl nm l nn no">export PATH=${PATH}:/path/to/apache-spark-X.Y.Z/bin</span></pre><h2 id="ebfc" class="lp ky iq bd kz lq lr dn ld ls lt dp lh lu lv lw lj lx ly lz ll ma mb mc ln md bi translated">创建Minikube“集群”</h2><p id="8d43" class="pw-post-body-paragraph me mf iq mg b mh mi jr mj mk ml ju mm lu mn mo mp lx mq mr ms ma mt mu mv mw ij bi translated">最后，为了拥有一个Kubernetes“集群”,我们将启动一个<code class="fe nd ne nf ng b">minikube</code>,目的是运行一个来自<a class="ae kw" href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala" rel="noopener ugc nofollow" target="_blank"> Spark库</a>的示例，名为<code class="fe nd ne nf ng b">SparkPi</code>,作为演示。</p><pre class="kh ki kj kk gt nh ng ni nj aw nk bi"><span id="f07f" class="lp ky iq ng b gy nl nm l nn no">minikube start --cpus=2 \<br/>  --memory=4g</span></pre><h2 id="dc7d" class="lp ky iq bd kz lq lr dn ld ls lt dp lh lu lv lw lj lx ly lz ll ma mb mc ln md bi translated">建立码头工人形象</h2><p id="540a" class="pw-post-body-paragraph me mf iq mg b mh mi jr mj mk ml ju mm lu mn mo mp lx mq mr ms ma mt mu mv mw ij bi translated">让我们使用Minikube Docker守护进程来不依赖于外部注册表(并且只在VM上生成Docker映像层，便于以后的垃圾处理)。Minikube有一个让我们生活更轻松的包装:</p><pre class="kh ki kj kk gt nh ng ni nj aw nk bi"><span id="59d5" class="lp ky iq ng b gy nl nm l nn no">eval $(minikube docker-env)</span></pre><p id="e0d8" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">配置完守护进程环境变量后，我们需要一个Docker映像来运行作业。Spark仓库中有一个<a class="ae kw" href="https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh" rel="noopener ugc nofollow" target="_blank"> shell脚本来帮助解决这个问题。考虑到我们的<code class="fe nd ne nf ng b">PATH</code>已经正确配置，只需运行:</a></p><pre class="kh ki kj kk gt nh ng ni nj aw nk bi"><span id="4f99" class="lp ky iq ng b gy nl nm l nn no">docker-image-tool.sh -m -t latest build</span></pre><p id="bc49" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated"><em class="mx">参考消息:</em>这里的<code class="fe nd ne nf ng b">-m</code>参数表示一个迷你库的构建。</p><p id="df5b" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">让我们快速执行SparkPi，使用与Hadoop Spark集群<a class="ae kw" href="https://spark.apache.org/docs/latest/submitting-applications.html" rel="noopener ugc nofollow" target="_blank"> spark-submit </a>相同的命令。</p><p id="81e3" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">然而，Spark Operator支持使用<a class="ae kw" href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" rel="noopener ugc nofollow" target="_blank"> CRD </a>、<a class="ae kw" href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/tree/master/examples" rel="noopener ugc nofollow" target="_blank">用“Kubernetes方言”定义作业，下面是一些例子</a>——稍后介绍。</p><h1 id="d5e9" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">向洞里开火！</h1><p id="5f12" class="pw-post-body-paragraph me mf iq mg b mh mi jr mj mk ml ju mm lu mn mo mp lx mq mr ms ma mt mu mv mw ij bi translated">中间是Scala版本和<em class="mx">的差距。jar </em>当您使用Apache Spark版本进行参数化时:</p><pre class="kh ki kj kk gt nh ng ni nj aw nk bi"><span id="f56c" class="lp ky iq ng b gy nl nm l nn no">spark-submit --master k8s://https://$(minikube ip):8443 \<br/>    --deploy-mode cluster \<br/>    --name spark-pi \<br/>    --class org.apache.spark.examples.SparkPi \<br/>    --conf spark.executor.instances=2 \<br/>    --executor-memory 1024m \<br/>    --conf spark.kubernetes.container.image=spark:latest \<br/>    local:///opt/spark/examples/jars/spark-examples_2.11-X.Y.Z.jar # here</span></pre><p id="3742" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">新的是:</p><ul class=""><li id="9540" class="np nq iq mg b mh my mk mz lu nr lx ns ma nt mw nu nv nw nx bi translated"><code class="fe nd ne nf ng b">--master</code>:在URL中接受前缀<code class="fe nd ne nf ng b">k8s://</code>，用于Kubernetes主API端点，由命令<code class="fe nd ne nf ng b">https://$(minikube ip):8443</code>公开。BTW，万一你想知道，这是一个shell命令替换；</li><li id="5736" class="np nq iq mg b mh ny mk nz lu oa lx ob ma oc mw nu nv nw nx bi translated"><code class="fe nd ne nf ng b">--conf spark.kubernetes.container.image=</code>:配置Docker镜像在Kubernetes中运行。</li></ul><p id="8151" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">样本输出:</p><pre class="kh ki kj kk gt nh ng ni nj aw nk bi"><span id="9c5c" class="lp ky iq ng b gy nl nm l nn no">...</span><span id="d137" class="lp ky iq ng b gy od nm l nn no">19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: State changed,<br/>new state: pod name: spark-pi-1566485909677-driver namespace: default<br/>labels: spark-app-selector -&gt; spark-20477e803e7648a59e9bcd37394f7f60,<br/>spark-role -&gt; driver pod uid: c789c4d2-27c4-45ce-ba10-539940cccb8d<br/>creation time: 2019-08-22T14:58:30Z service account name: default<br/>volumes: spark-local-dir-1, spark-conf-volume, default-token-tj7jn<br/>node name: minikube start time: 2019-08-22T14:58:30Z container<br/>images: spark:docker phase: Succeeded status:<br/>[ContainerStatus(containerID=docker://e044d944d2ebee2855cd2b993c62025d<br/>6406258ef247648a5902bf6ac09801cc, image=spark:docker,<br/>imageID=docker://sha256:86649110778a10aa5d6997d1e3d556b35454e9657978f3<br/>a87de32c21787ff82f, lastState=ContainerState(running=null,<br/>terminated=null, waiting=null, additionalProperties={}),<br/>name=spark-kubernetes-driver, ready=false, restartCount=0,<br/>state=ContainerState(running=null,<br/>terminated=ContainerStateTerminated(containerID=docker://e044d944d2ebe<br/>e2855cd2b993c62025d6406258ef247648a5902bf6ac09801cc, exitCode=0,<br/>finishedAt=2019-08-22T14:59:08Z, message=null, reason=Completed,<br/>signal=null, startedAt=2019-08-22T14:58:32Z,<br/>additionalProperties={}), waiting=null, additionalProperties={}),<br/>additionalProperties={})]</span><span id="835c" class="lp ky iq ng b gy od nm l nn no">19/08/22 11:59:09 INFO LoggingPodStatusWatcherImpl: Container final<br/>statuses: Container name: spark-kubernetes-driver Container image:<br/>spark:docker Container state: Terminated Exit code: 0</span></pre><p id="5b2d" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">为了查看作业结果(以及整个执行过程),我们可以运行一个<code class="fe nd ne nf ng b">kubectl logs</code>,将驱动程序pod的名称作为参数传递:</p><pre class="kh ki kj kk gt nh ng ni nj aw nk bi"><span id="ea6e" class="lp ky iq ng b gy nl nm l nn no">kubectl logs $(kubectl get pods | grep 'spark-pi.*-driver')</span></pre><p id="e355" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">这将产生输出(省略了一些条目)，类似于:</p><pre class="kh ki kj kk gt nh ng ni nj aw nk bi"><span id="84f6" class="lp ky iq ng b gy nl nm l nn no">...<br/>19/08/22 14:59:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0<br/>(TID 1) in 52 ms on 172.17.0.7 (executor 1) (2/2)<br/>19/08/22 14:59:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose<br/>tasks have all completed, from pool19/08/22 14:59:08 INFO<br/>DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in<br/>0.957 s<br/>19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at<br/>SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473<br/>19/08/22 14:59:08 INFO SparkUI: Stopped Spark web UI at<br/><a class="ae kw" href="http://spark-pi-1566485909677-driver-svc.default.svc:4040" rel="noopener ugc nofollow" target="_blank">http://spark-pi-1566485909677-driver-svc.default.svc:4040</a><br/>19/08/22 14:59:08 INFO KubernetesClusterSchedulerBackend: Shutting<br/>down all executors<br/>19/08/22 14:59:08 INFO<br/>KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking<br/>each executor to shut down<br/>19/08/22 14:59:08 WARN ExecutorPodsWatchSnapshotSource: Kubernetes<br/>client has been closed (this is expected if the application is<br/>shutting down.)<br/>19/08/22 14:59:08 INFO MapOutputTrackerMasterEndpoint:<br/>MapOutputTrackerMasterEndpoint stopped!<br/>19/08/22 14:59:08 INFO MemoryStore: MemoryStore cleared<br/>19/08/22 14:59:08 INFO BlockManager: BlockManager stopped<br/>19/08/22 14:59:08 INFO BlockManagerMaster: BlockManagerMaster stopped<br/>19/08/22 14:59:08 INFO<br/>OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:<br/>OutputCommitCoordinator stopped!<br/>19/08/22 14:59:08 INFO SparkContext: Successfully stopped SparkContext<br/>19/08/22 14:59:08 INFO ShutdownHookManager: Shutdown hook called<br/>19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory<br/>/tmp/spark-aeadc6ba-36aa-4b7e-8c74-53aa48c3c9b2<br/>19/08/22 14:59:08 INFO ShutdownHookManager: Deleting directory<br/>/var/data/spark-084e8326-c8ce-4042-a2ed-75c1eb80414a/spark-ef8117bf-90<br/>d0-4a0d-9cab-f36a7bb18910<br/>...</span></pre><p id="101d" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">结果出现在:</p><pre class="kh ki kj kk gt nh ng ni nj aw nk bi"><span id="8b38" class="lp ky iq ng b gy nl nm l nn no">19/08/22 14:59:08 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 1.040608 s Pi is roughly 3.138915694578473</span></pre><p id="ede1" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated">最后，让我们删除Minikube生成的VM，以清理环境(除非您想继续玩它):</p><pre class="kh ki kj kk gt nh ng ni nj aw nk bi"><span id="6d8d" class="lp ky iq ng b gy nl nm l nn no">minikube delete</span></pre><h1 id="3007" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">临终遗言</h1><p id="545e" class="pw-post-body-paragraph me mf iq mg b mh mi jr mj mk ml ju mm lu mn mo mp lx mq mr ms ma mt mu mv mw ij bi translated">我希望您的好奇心得到了<em class="mx">的激发</em>，并且为您的大数据工作负载提出了一些进一步开发的想法。如果你有任何疑问或建议，请在评论区分享。</p></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><p id="1915" class="pw-post-body-paragraph me mf iq mg b mh my jr mj mk mz ju mm lu na mo mp lx nb mr ms ma nc mu mv mw ij bi translated"><em class="mx">原载于2020年5月21日</em><a class="ae kw" href="https://macunha.me/en/post/2020/05/quickstart-apache-spark-on-kubernetes/" rel="noopener ugc nofollow" target="_blank"><em class="mx">https://ma Cunha . me</em></a><em class="mx">。</em></p></div></div>    
</body>
</html>