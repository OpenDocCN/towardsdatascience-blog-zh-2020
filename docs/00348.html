<html>
<head>
<title>PyTorch Layer Dimensions: Get your layers to work every time (the complete guide)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch图层尺寸:让您的图层每次都能正常工作(完整指南)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd?source=collection_archive---------1-----------------------#2020-01-11">https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd?source=collection_archive---------1-----------------------#2020-01-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2f05" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第一次，每一次，让你的层平滑地适合。PyTorch中的张量和层维度入门指南。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f15ead30a9820d47330619dcbf17774e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8qfACpvBOW99U-zD"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用这些宝贵的知识，第一次就能让你的衣服层很好的贴合。—<a class="ae ky" href="https://unsplash.com/@snaps_by_clark?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Clark Van Der Beken</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="da69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">前言</strong></p><p id="aecb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">本文涵盖了定义张量、在PyTorch中正确初始化神经网络层等等！(原标题为</em> <a class="ae ky" rel="noopener" target="_blank" href="/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd?source=your_stories_page-------------------------------------"> PyTorch图层尺寸:什么尺寸，为什么？</a>)</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="c431" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">简介</strong></h1><p id="cab2" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">你可能会问:“我如何在PyTorch中初始化我的层尺寸而不被吼？”这一切都只是试错吗？不，真的…它们应该是什么？首先，您是否知道第一个<em class="lv"/><em class="lv">和第二个</em>需要一个<code class="fe na nb nc nd b">torch.nn.Conv2d</code>层的参数，而一个<code class="fe na nb nc nd b">torch.nn.Linear</code>层需要完全不同方面的完全相同的张量数据？如果你不知道这些，请继续阅读。</p><p id="b21d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">例1:相同，相同，但不同。</strong></p><blockquote class="ne nf ng"><p id="9be2" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated"><em class="it">构造卷积图层和线性图层在语法上是相似的，但是尽管能够对完全相同的输入数据进行操作(尽管数据的大小应该不同)，但参数并不期望得到相似的东西。</em></p></blockquote><pre class="kj kk kl km gt nk nd nl nm aw nn bi"><span id="9c5b" class="no me it nd b gy np nq l nr ns"><strong class="nd iu"># The __init__ method of a nn.Module class:</strong></span><span id="eedf" class="no me it nd b gy nt nq l nr ns">...</span><span id="9b86" class="no me it nd b gy nt nq l nr ns">def __init__(self):<br/>"""Initialize neural net layers."""</span><span id="55c5" class="no me it nd b gy nt nq l nr ns">    super(Net, self).__init__()<br/>    <br/>    # Intialize my 2 layers here:</span><span id="5837" class="no me it nd b gy nt nq l nr ns">    self.conv =<strong class="nd iu"> nn.Conv2d(1, 20, 3) </strong><em class="lv"># Give me </em><strong class="nd iu"><em class="lv">depth </em></strong><em class="lv">of input.</em></span><span id="a41f" class="no me it nd b gy nt nq l nr ns"><strong class="nd iu">    </strong>self.dense =<strong class="nd iu"> nn.Linear(2048, 10) </strong><em class="lv"># Give me </em><strong class="nd iu"><em class="lv">features </em></strong><em class="lv">of input.</em></span><span id="9ba8" class="no me it nd b gy nt nq l nr ns">...</span></pre><p id="f6d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在将数据集放入某些网络层之前，您需要了解PyTorch模型如何使用数据。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="4108" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">第一课:如何在PyTorch中读取张量大小</strong></h1><p id="d3ac" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">下面是PyTorch中遇到的一些常见张量大小以及何时使用它们的典型示例。知道你在看什么很重要，因为它们的结构并不像你希望的那样可预测(这种有些不直观的设计选择主要是为了性能利益而实现的，<em class="lv">这是</em>好的… <em class="lv">我猜是</em>)。</p><p id="8da0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当把张量输入卷积或线性层<em class="lv">(虽然</em> <em class="lv">不是RNN的)</em>时，我们有一个心理锚，那就是第一维总是<strong class="lb iu">批量</strong> <strong class="lb iu">大小(N) </strong>。<strong class="lb iu"> </strong>然而，剩下的维度取决于月亮的相位和你所在地区月亮升起时蟋蟀的间歇频率。开玩笑，没那么简单。你必须死记硬背。所以开始<em class="lv">旋转(下面的例子2)。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/b3d9557d5b2c973f8fc95c8a86f95b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AqRqfx5GweuLcbQD"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">吃他们说的红色药丸。他们说再深入一点。这个张量的第三维度应该是什么？！？—照片由<a class="ae ky" href="https://unsplash.com/@punttim?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Tim Gouw </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="86d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">了解PyTorch期望它的张量如何成形是很重要的——因为你可能非常满意你的28 x 28像素图像显示为Torch的张量。大小([28，28])。而PyTorch则认为你希望它查看你的28批28个特征向量。我只想说，在你学会如何用她的方式看待事情之前，你们不会成为朋友——所以，不要成为那样的人。研究你的张量维度！</p><p id="5d74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">例PyTorch喜欢的张量维数。</strong></p><pre class="kj kk kl km gt nk nd nl nm aw nn bi"><span id="0772" class="no me it nd b gy np nq l nr ns"><strong class="nd iu">"""Example tensor size outputs, how PyTorch reads them, and where you encounter them in the wild. </strong></span><span id="a030" class="no me it nd b gy nt nq l nr ns"><em class="lv">Note: the values below are only examples. Focus on the rank of the tensor (how many dimensions it has).</em><strong class="nd iu"><em class="lv">"""</em></strong></span><span id="6400" class="no me it nd b gy nt nq l nr ns"><strong class="nd iu">&gt;&gt;&gt; torch.Size([32])</strong><br/> <em class="lv">   # </em><strong class="nd iu"><em class="lv">1d:</em></strong><em class="lv"> [batch_size] <br/>    # use for target labels or predictions.</em></span><span id="5764" class="no me it nd b gy nt nq l nr ns"><strong class="nd iu">&gt;&gt;&gt; torch.Size([12, 256])</strong><br/>    <em class="lv"># </em><strong class="nd iu"><em class="lv">2d:</em></strong><em class="lv"> [batch_size, num_features (aka: C * H * W)]<br/>    # use for </em><strong class="nd iu"><em class="lv">nn.Linear</em></strong><em class="lv">() input.</em></span><span id="335c" class="no me it nd b gy nt nq l nr ns"><strong class="nd iu">&gt;&gt;&gt; torch.Size([10, 1, 2048])</strong><br/>   <em class="lv"> # </em><strong class="nd iu"><em class="lv">3d:</em></strong><em class="lv"> [batch_size, channels, num_features (aka: H * W)]<br/>    # when used as </em><strong class="nd iu"><em class="lv">nn.Conv1d</em></strong><em class="lv">() input.<br/>    # (but [seq_len, batch_size, num_features]<br/>    # if feeding an </em><strong class="nd iu"><em class="lv">RNN</em></strong><em class="lv">).</em></span><span id="92d6" class="no me it nd b gy nt nq l nr ns"><strong class="nd iu">&gt;&gt;&gt; torch.Size([16, 3, 28, 28])</strong><br/>    <em class="lv"># </em><strong class="nd iu"><em class="lv">4d:</em></strong><em class="lv"> [batch_size, channels, height, width]<br/>    # use for </em><strong class="nd iu"><em class="lv">nn.Conv2d()</em></strong><em class="lv"> input.</em></span><span id="fb26" class="no me it nd b gy nt nq l nr ns"><strong class="nd iu">&gt;&gt;&gt;  torch.Size([32, 1, 5, 15, 15])</strong><br/>    <em class="lv"># </em><strong class="nd iu"><em class="lv">5D:</em></strong><em class="lv"> [batch_size, channels, depth, height, width]<br/>    # use for </em><strong class="nd iu"><em class="lv">nn.Conv3d</em></strong><em class="lv">() input.</em></span></pre><blockquote class="ne nf ng"><p id="f66f" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">注意到<strong class="lb iu"> Conv2d层想要一个4d张量吗？1d或3d图层怎么样？</strong></p></blockquote><p id="9963" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，如果您想将一幅灰度为28 x 28像素的图像加载到Conv2d网络图层中，请在上面的示例中找到图层类型。因为它需要一个4d张量，而你已经有一个具有高度和宽度的2d张量，只需添加batch_size和channels <em class="lv">(参见下面channels的经验法则)</em>来填充额外的维度，就像这样:[1，1，28，28]。那样的话，你和PyTorch可以和好如初。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="c305" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">第二课:初始化torch.nn.Conv2d层</strong></h1><p id="ad11" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">文档描述了这样一个Conv2d层:</p><pre class="kj kk kl km gt nk nd nl nm aw nn bi"><span id="6afd" class="no me it nd b gy np nq l nr ns">"""<br/><em class="lv">Class</em><em class="lv"> </em></span><span id="6c71" class="no me it nd b gy nt nq l nr ns">torch.nn.Conv2d<!-- -->(<strong class="nd iu"><em class="lv">in_channels</em></strong>, <strong class="nd iu"><em class="lv">out_channels</em></strong>, <em class="lv">kernel_size</em>, <em class="lv">stride=1</em>, <em class="lv">padding=0</em>, <em class="lv">dilation=1</em>, <em class="lv">groups=1</em>, <em class="lv">bias=True</em>, <em class="lv">padding_mode='zeros'</em>)</span><span id="60b2" class="no me it nd b gy nt nq l nr ns"><em class="lv">Parameters</em></span><span id="1e5b" class="no me it nd b gy nt nq l nr ns"><strong class="nd iu"><em class="lv">in_channels</em></strong><em class="lv"> </em>(int) - Number of channels in the input image<br/><strong class="nd iu"><em class="lv">out_channels </em></strong>(int) - Number of channels produced by the convolution<br/>"""</span></pre><p id="7878" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还记得你的输入通道是哪个维度吗？如果你忘了，请看第一课。只需在第一个卷积层中使用该数字作为您的<code class="fe na nb nc nd b">in_channels</code>参数。<em class="lv">搞定。</em></p><blockquote class="ne nf ng"><p id="05f2" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated"><strong class="lb iu">第一个Conv2d层上“in_channels”的经验法则:</strong></p><p id="4f8d" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">—如果您的图像是黑白的，则为1通道。(您可以通过在dataloader的transforms参数中运行<code class="fe na nb nc nd b">transforms.Grayscale(1)</code>来确保这一点。)</p><p id="2c68" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">—如果您的图像是彩色的，则它是3个通道(RGB)。</p><p id="83c9" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">—如果有alpha(透明)通道，则它有4个通道。</p></blockquote><p id="8100" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着对于您的第一个Conv2d层，即使您的图像大小很大，如1080像素乘1080像素，您的<code class="fe na nb nc nd b">in_channels</code>通常会是1或3。</p><p id="10aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">注意:</em> </strong> <em class="lv">如果你用一些随机产生的张量测试这个，它仍然对你呕吐，你现在对着你的电脑大喊，深呼吸。没关系。确保它有正确的尺寸。你有没有</em> <code class="fe na nb nc nd b"><em class="lv">unsqueeze()</em></code> <em class="lv">这个张量？Pytorch想要批量。</em><a class="ae ky" href="https://pytorch.org/docs/stable/torch.html#torch.unsqueeze" rel="noopener ugc nofollow" target="_blank"><em class="lv">unsqueeze()</em></a><em class="lv">函数将添加一个表示批量为1的维度1。</em></p><h2 id="bbde" class="no me it bd mf nv nw dn mj nx ny dp mn li nz oa mp lm ob oc mr lq od oe mt of bi translated">但是，out_channels呢？</h2><p id="4a9a" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">你说的<code class="fe na nb nc nd b">out_channels</code>怎么样？你希望你的人际网络有多深，这是你的选择。基本上，Pytorch定义的您的<code class="fe na nb nc nd b">out_channels</code>维度是:</p><blockquote class="ne nf ng"><p id="5ef9" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated"><strong class="lb iu">out _ channels</strong>(<a class="ae ky" href="https://docs.python.org/3/library/functions.html#int" rel="noopener ugc nofollow" target="_blank"><em class="it">int</em></a>)—卷积产生的通道数</p></blockquote><p id="e20b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于您使用的每个卷积核，当通过该层时，您的输出张量变得更深一个通道。如果你想要大量的内核，把这个数字设得高一些，比如121，如果你只想要一些，把这个数字设得低一些，比如8或者12。您在这里选择的任何数字都将是下一个卷积层<strong class="lb iu"><em class="lv"/></strong>的<code class="fe na nb nc nd b">channels_in</code>值，以此类推。</p><blockquote class="ne nf ng"><p id="6c83" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">注意:<code class="fe na nb nc nd b">kernel_size</code>的值是自定义的，虽然很重要，但不会导致令人头疼的错误，因此在本教程中省略。取一个奇数即可，通常在3-11之间，但大小可能因应用程序而异。</p></blockquote><p id="061c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，网络前半部分的卷积层越来越深，而网络末端的全连接(又名:线性或密集)层越来越小。这里有一个来自<a class="ae ky" href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html" rel="noopener ugc nofollow" target="_blank"> 60分钟初学者闪电战</a>的有效例子(注意<code class="fe na nb nc nd b">self.conv1</code>的<em class="lv">出频道</em>变成了<code class="fe na nb nc nd b">self.conv2</code>的<em class="lv">入频道</em>):</p><pre class="kj kk kl km gt nk nd nl nm aw nn bi"><span id="c081" class="no me it nd b gy np nq l nr ns"><strong class="nd iu">class</strong> <strong class="nd iu">Net</strong>(nn<strong class="nd iu">.</strong>Module):<br/><br/>    <strong class="nd iu">def</strong> __init__(self):<br/>        super(Net, self)<strong class="nd iu">.</strong>__init__()<br/>        <em class="lv"># 1 input image channel, 6 output channels, 3x3 square convolution</em><br/>        <em class="lv"># kernel</em><br/>        self<strong class="nd iu">.</strong>conv1 <strong class="nd iu">=</strong> nn<strong class="nd iu">.</strong>Conv2d(1, <strong class="nd iu">6</strong>, 3)<br/>        self<strong class="nd iu">.</strong>conv2 <strong class="nd iu">=</strong> nn<strong class="nd iu">.</strong>Conv2d(<strong class="nd iu">6</strong>, 16, 3)</span><span id="be81" class="no me it nd b gy nt nq l nr ns">        <em class="lv"># an affine operation: y = Wx + b</em><br/>        self<strong class="nd iu">.</strong>fc1 <strong class="nd iu">=</strong> nn<strong class="nd iu">.</strong>Linear(16 <strong class="nd iu">*</strong> 6 <strong class="nd iu">*</strong> 6, 120)  <em class="lv"># 6*6 from image dimension</em><br/>        self<strong class="nd iu">.</strong>fc2 <strong class="nd iu">=</strong> nn<strong class="nd iu">.</strong>Linear(120, 84)<br/>        self<strong class="nd iu">.</strong>fc3 <strong class="nd iu">=</strong> nn<strong class="nd iu">.</strong>Linear(84, 10)</span></pre><p id="a727" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在来说说全连接层。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="207b" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">第三课:完全连接(torch.nn.Linear)图层</strong></h1><p id="66d1" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated"><a class="ae ky" href="https://pytorch.org/docs/stable/nn.html#linear" rel="noopener ugc nofollow" target="_blank">线性层的文档</a>告诉我们以下内容:</p><pre class="kj kk kl km gt nk nd nl nm aw nn bi"><span id="74c7" class="no me it nd b gy np nq l nr ns"><strong class="nd iu">"""<br/></strong><em class="lv">Class</em><strong class="nd iu"><br/></strong>  <br/>torch.nn.Linear<!-- -->(<strong class="nd iu"><em class="lv">in_features</em></strong>, <strong class="nd iu"><em class="lv">out_features</em></strong>, <em class="lv">bias=True</em>)</span><span id="85b8" class="no me it nd b gy nt nq l nr ns"><em class="lv">Parameters</em></span><span id="84ac" class="no me it nd b gy nt nq l nr ns"><strong class="nd iu">in_features </strong>– size of each input sample<br/><strong class="nd iu">out_features </strong>– size of each output sample</span><span id="0ef6" class="no me it nd b gy nt nq l nr ns">"""</span></pre><p id="8336" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我知道这些看起来很相似，但是不要混淆:“<code class="fe na nb nc nd b">in_features</code>”和“<code class="fe na nb nc nd b">in_channels</code>”是完全不同的，初学者经常混淆它们，认为它们是相同的属性。</p><pre class="kj kk kl km gt nk nd nl nm aw nn bi"><span id="7dd3" class="no me it nd b gy np nq l nr ns"><strong class="nd iu"># Asks for in_channels, out_channels,</strong> kernel_size, etc<br/>self.conv1 = nn.Conv2d(1, 20, 3)</span><span id="a368" class="no me it nd b gy nt nq l nr ns"><strong class="nd iu"># Asks for in_features, out_features</strong><br/>self.fc1 = nn.Linear(2048, 10)</span></pre><h2 id="a2e9" class="no me it bd mf nv nw dn mj nx ny dp mn li nz oa mp lm ob oc mr lq od oe mt of bi translated"><strong class="ak">计算尺寸。</strong></h2><p id="9085" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">对于所有的<code class="fe na nb nc nd b">nn.Linear</code>层网络，有两个特别重要的论点，无论你的网络有多深，你都应该知道。第<strong class="lb iu">个参数</strong>和第<strong class="lb iu">个参数</strong>。你有多少个完全连接的层在中间并不重要，这些维度很容易，你很快就会看到。</p><p id="babb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想把你的28 x 28的图像传入一个线性层，你必须知道两件事:</p><ol class=""><li id="c93d" class="og oh it lb b lc ld lf lg li oi lm oj lq ok lu ol om on oo bi translated"><strong class="lb iu">您的28 x 28像素图像不能作为[28，28]张量输入。</strong>这是因为<code class="fe na nb nc nd b">nn.Linear </code>会将其读取为28批28个特征长度的向量。既然它期望一个<code class="fe na nb nc nd b">[batch_size, num_features]</code>的输入，你必须以某种方式转置它<em class="lv"> </em>(见下面的<em class="lv">视图()</em><em class="lv">)。</em></li><li id="eea5" class="og oh it lb b lc op lf oq li or lm os lq ot lu ol om on oo bi translated"><strong class="lb iu">您的批次大小不变地通过您的所有层。</strong>不管你的数据在网络中如何变化，你的第一维最终将是你的<code class="fe na nb nc nd b">batch_size</code>，即使你从未在网络模块的定义中看到这个数字。</li></ol><blockquote class="ne nf ng"><p id="a4de" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated"><strong class="lb iu">使用</strong> <a class="ae ky" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">视图</strong> </a> <strong class="lb iu">()来改变你的张量的维度。</strong></p><p id="13cc" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated"><code class="fe na nb nc nd b">image = image.view(<strong class="lb iu">batch_size</strong>, -1)</code></p><p id="1090" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">您提供您的batch_size作为第一个数字，然后“-1”基本上告诉Pytorch，“您为我算出另一个数字…请。”你的张量现在将正确地馈入任何线性层。现在我们正在谈话！</p></blockquote><p id="1a80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，要初始化线性图层的第一个参数<strong class="lb iu">,请向其传递输入数据的要素数量。对于28×28，我们的新视图张量大小为[1，784] (1 * 28 * 28):</strong></p><p id="f2d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">示例3:使用视图调整大小()以适合线性图层</strong></p><pre class="kj kk kl km gt nk nd nl nm aw nn bi"><span id="5909" class="no me it nd b gy np nq l nr ns">batch_size = 1</span><span id="a832" class="no me it nd b gy nt nq l nr ns"># Simulate a 28 x 28 pixel, grayscale "image"<br/>input = torch.randn(1, 28, 28)</span><span id="093a" class="no me it nd b gy nt nq l nr ns"># Use view() to get [batch_size, num_features].<br/># -1 calculates the missing value given the other dim.<br/>input = input.view(batch_size, -1) # torch.Size([1, <strong class="nd iu">784</strong>])</span><span id="537c" class="no me it nd b gy nt nq l nr ns"># Intialize the linear layer.<br/>fc = torch.nn.Linear(<strong class="nd iu">784</strong>, 10)</span><span id="5ab0" class="no me it nd b gy nt nq l nr ns"># Pass in the simulated image to the layer.<br/>output = fc(input)</span><span id="9981" class="no me it nd b gy nt nq l nr ns">print(output.shape)<br/>&gt;&gt;&gt; torch.Size([1, 10])</span></pre><blockquote class="ne nf ng"><p id="3720" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">R <strong class="lb iu">记住这个</strong>——如果你曾经从卷积层输出转换到<em class="it"> </em>线性层输入，你必须使用view将其从4d调整到2d，如上面的图像示例所述。</p><p id="7932" class="kz la lv lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">所以，[32，21，50，50]的conv输出应该被“展平”成为[32，21 <strong class="lb iu"> * </strong> 50 <strong class="lb iu"> * </strong> 50]张量。并且线性图层的in_features也要设置为[21 <strong class="lb iu"> * </strong> 50 <strong class="lb iu"> * </strong> 50]。</p></blockquote><p id="f015" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性层的第二个参数<strong class="lb iu">如果你把它传递给更多的层，被称为隐藏层的<strong class="lb iu"> H </strong>。你只需要和H打定位乒乓球，让它成为上一个的最后一个和下一个的第一个，就像这样:</strong></p><pre class="kj kk kl km gt nk nd nl nm aw nn bi"><span id="42e7" class="no me it nd b gy np nq l nr ns"><strong class="nd iu">"""The in-between dimensions are the hidden layer dimensions, you just pass in the last of the previous as the first of the next."""</strong></span><span id="9f44" class="no me it nd b gy nt nq l nr ns">fc1 = torch.nn.Linear(784, 100) <em class="lv"># 100 is last.</em><br/>fc2 = torch.nn.Linear(100, 50) <em class="lv"># 100 is first, 50 is last.</em><br/>fc3 = torch.nn.Linear(50, 20) <em class="lv"># 50 is first, 20 is last.</em><br/>fc4 = torch.nn.Linear(20, 10) <em class="lv"># 20 is first. </em></span><span id="0bce" class="no me it nd b gy nt nq l nr ns"><em class="lv">"""This is the same pattern for convolutional layers as well, only it's channels, and not features that get passed along."""</em></span></pre><p id="0150" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">最后的输出</strong>，也就是你的<strong class="lb iu">输出层</strong>取决于你的模型和你的损失函数。如果你有10个类，像在MNIST，你正在做一个分类问题，你希望你的所有网络架构最终合并成最后的10个单元，这样你就可以确定你的输入预测的是这10个类中的哪一个。</p><p id="ed9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一层取决于您想从数据中推断出什么。为了得到您需要的答案，您可以做的操作是另一篇文章的主题，因为有很多内容需要讨论。但是现在你应该已经掌握了所有的基础知识。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="cd71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">就是这样！</strong></p><p id="7c68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你现在应该能够建立一个网络，而不用挠头或者被翻译吼了。请记住，您的batch_size或dim 0始终是相同的。卷积图层关注深度(通道)，线性图层关注要素数量。并学习如何阅读那些张量！</p><p id="3bf8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请留下评论，或者分享这篇文章，如果你喜欢它，并发现它很有帮助！</p></div></div>    
</body>
</html>