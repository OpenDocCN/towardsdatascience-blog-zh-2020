<html>
<head>
<title>What Is Reinforcement Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是强化学习？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-reinforcement-learning-99f9615918e3?source=collection_archive---------29-----------------------#2020-09-17">https://towardsdatascience.com/what-is-reinforcement-learning-99f9615918e3?source=collection_archive---------29-----------------------#2020-09-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b952" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">奖励和惩罚</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ad2c2d6ce6e8ae0f3b3f2232e3f556ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iizY8bZ-2kPoBn_z"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae ky" href="https://unsplash.com/@neonbrand?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> NeONBRAND </a>拍摄的照片</p></figure><p id="4c84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">机器学习是 IT 界讨论最多的领域之一。随着图像和语音识别、自动驾驶汽车、产品推荐和欺诈检测的大范围普及，机器学习无处不在。</span></p><p id="55ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ML 的一个子领域专注于通过自学发现问题的解决方案。这里，我们可以以电子游戏为例。当来自一家名为 Deepmind 的公司的一小组研究人员发表了一篇关于用强化学习玩雅达利的论文时，这个领域受到了很多关注。以至于谷歌后来花了一大笔钱收购了该公司。</p><p id="dbfd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当 AlphaGo 在围棋比赛中击败世界冠军时，强化学习的力量不再被忽视。读完这篇文章后，你将对强化学习及其应用有一个基本的了解。</p><h1 id="3954" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">强化学习</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/6e1919262ea5e26bf4bcf60f0adc751a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RG6XnTXhrcFyIRlI"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@cdr6934?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克里斯里德</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><blockquote class="mx my mz"><p id="f5d7" class="kz la na lb b lc ld ju le lf lg jx lh nb lj lk ll nc ln lo lp nd lr ls lt lu im bi translated">强化学习是对机器学习模型的训练，以针对给定场景做出一系列决策。</p></blockquote><p id="9593" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在其核心，我们有一个自主代理，如人、机器人或深度网络学习来导航不确定的环境。这个代理的目标是最大化数字奖励。</p><p id="51f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">体育就是一个很好的例子。让我们考虑一下我们的经纪人在网球比赛中需要处理的事情。</p><p id="3a22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代理人将不得不考虑其行动，如发球和截击。这些行为改变了游戏的状态。换句话说，当前设定的分数和领先的玩家。</p><p id="ab82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每一个行动都是有回报的。代理人必须赢得一分才能赢得一局、一盘和一场比赛。</p><p id="41a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的球员需要遵循一定的规则和策略，以最大限度地提高最终得分。</p><p id="105f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了在此基础上建立一个模型，它必须将一个状态和一个动作作为输入，然后将其转换为最大可能的回报。该模型还必须提前考虑，并考虑此类行动的长期结果。</p><p id="9688" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个过程因任务而异，这并不奇怪。建立一个可以打网球的模型和 Atari 是完全不同的。</p><h1 id="e954" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">监督学习与强化学习</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/30f1cfce95a6112d497f12bc606b2024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mTdy-i4tLxJsDWTI"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@leliejens?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">延斯·勒列</a>拍摄</p></figure><p id="ce7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强化学习不仅仅是监督学习的一种聪明说法。监督学习侧重于根据历史实例来理解环境。</p><p id="76b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这并不总是合适的。一个常见的例子是在交通中驾驶。</p><p id="4d8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想象一下，根据前一天的观察来做这件事，当时几乎没有任何汽车。这和开车时只看后视镜一样有效。</p><p id="54e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强化学习就是收集奖励。代理专注于正确转弯，必要时发出信号，以及不违反速度限制。此外，机器人可能会因危险行为而被扣分，例如超速行驶。</p><p id="fe8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目标是在给定当前交通状态的情况下最大化点数。这里强调的是，一个动作导致状态的改变，而监督学习并不关注这一点。</p><h1 id="e666" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">探索与开发</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/571462950455c9971ad7b7c06f19c3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pTg6cvp4CIzPd4MH"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@photoshobby?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">摄影爱好</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="7131" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们把一个全新的机器人放在一个房间里。他的目标是找到螺栓和螺丝钉。这里的机器人是智能体，房间是环境。他有四种可能的动作:向左、向右、向前或向后移动。</p><p id="5b9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器人的状态由几个东西组成，是他当前的位置和以前的位置。如果他移动，状态会改变，但是我们还不知道这个移动是对还是错。</p><p id="80e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我们让机器人探索房间。走了一会儿后，我们的机器人终于找到了一些螺丝钉。机器人因为做了正确的事情而获得奖励。</p><p id="3e44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们的机器人必须遵循的政策是积分最大化。他知道，通过再次采取同样的路线，他一定会达到目标，并得到一些积极的反馈。</p><p id="265a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这条路径远非最佳，任何类型的机器人都只能一次又一次地沿着同一条路径前进。探索和探索之间的权衡来了。</p><p id="1206" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们的机器人遵循同样的道路，他将利用他所学到的知识，并最终达到目标。然而，如果我们让机器人四处游荡寻找更好的路径，他将利用探索。</p><p id="51e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们给我们的机器人 10 个回合来了解环境并找到更好的路线。过了一段时间，他找到了一条路径，只需要以前一半的步骤。然而，在这个过程中，他也走了一些不好的路。</p><p id="b360" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，如果我们让另一个机器人跑 10 圈，并且只使用他找到的第一条路径，这个机器人可能会收集更多的螺栓和螺钉。</p><p id="fd5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是研究人员在开发强化学习模型时必须处理的权衡问题。</p><h1 id="67c4" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">现实生活中的例子</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/93d05db18195a675c1def02c7344facd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ne5PoXh5hC1_PbX2"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Bram Van Oost 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="361b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强化学习模型必须经过良好的训练和优化，以适应现实生活中的情况。代理周围的场景和环境每次都会发生变化。</p><p id="0538" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，我们在一辆自动驾驶汽车内，我们希望汽车在安全性方面得到优化。然后，如果我们看到前面汽车的刹车灯，可能是时候减速了。然而，如果我们在路上看到一块巨大的岩石，我们预计汽车会停下来。</p><p id="4ce2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个令人印象深刻的项目旨在制造假腿，它将能够识别行走模式并做出相应的调整。作为原型，研究人员已经开发了一种可以自主学习的虚拟跑步者。</p><p id="389c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经知道，强化学习是一个具有很大潜力的强大工具。唯一的事情是，我们需要大量的数据和训练，才能驾驭现实世界的情况。</p><p id="389e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在大规模计算领域已经有了一些令人乐观和印象深刻的成果。这些系统可以探索具有大量状态的大规模环境，例如大型视频游戏中的环境。</p><h1 id="bafa" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">最后的想法</h1><p id="551a" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">强化学习简单而强大。鉴于最近的发展和快速进步，它有可能成为深度学习领域的一支重要力量。然而，重要的是要记住，强化学习只是解决现实世界问题的许多现有方法之一，它有其优点和缺点。</p><p id="e6e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在 AlphaGo 的例子中看到的那样，强化学习有可能让机器变得更有创造力。这个领域的入门很简单，只需参加一个短期课程或开发一个小程序，例如机器人寻找螺栓。</p><h1 id="32e1" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">参考</h1><p id="e192" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">[1] Bajaj，P. (2020 年)。强化学习。2020 年 9 月 16 日检索，来自<a class="ae ky" href="https://www.geeksforgeeks.org/what-is-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/what-is-reinforcement-learning/</a></p><p id="3c96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]奥辛斯基，b .，&amp;布德克，K. (2018)。什么是强化学习？完全指南。检索于 2020 年 9 月 16 日，来自<a class="ae ky" href="https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/" rel="noopener ugc nofollow" target="_blank">https://deep sense . ai/what-is-reinforcement-learning-the-complete-guide/</a></p></div></div>    
</body>
</html>