<html>
<head>
<title>Using Tensorflow Lite for Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Tensorflow Lite 进行对象检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-tensorflow-lite-for-object-detection-2a0283f94aed?source=collection_archive---------5-----------------------#2020-08-22">https://towardsdatascience.com/using-tensorflow-lite-for-object-detection-2a0283f94aed?source=collection_archive---------5-----------------------#2020-08-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f671" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用在 Python 中的 COCO 数据集上预训练的 MobileNet SSD 模型，在具有完整代码和非最大抑制的边缘设备上运行。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3bec7b9395d5be72c2d7315b3bea3314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*N29PFrlEp10KBUFS"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">伊利亚·赫特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="d041" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Tensorflow 最近发布了用于 Tensorflow 2 的对象检测 API，tensor flow 2 有一个非常大的模型动物园。然而，他们只提供了一款采用 Tensorflow lite 的 MobileNet v1 SSD 型号，此处描述的是<a class="ae ky" href="https://www.tensorflow.org/lite/models/object_detection/overview" rel="noopener ugc nofollow" target="_blank"/>。在那篇博文中，他们提供了在 Android 和 IOS 设备上运行的代码，但没有为 edge 设备提供代码。随着 edge 设备的普及和 OpenCV 发布空间 AI 套件，我想填补这一空白。这正是我们将在这里看到的。</p><h1 id="3b08" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">要求</h1><p id="74c2" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这不需要安装 Tensorflow 对象检测 API 即可运行。一个简单的 Tensorflow 安装，以及用于图像处理的 OpenCV 足以运行它。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="7f74" class="mx lw it mt b gy my mz l na nb">pip install tensorflow<br/>pip install opencv</span></pre><p id="6850" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我使用了对象检测 API 中提供的一些代码来使工作变得更容易，但没有必要担心它，因为不需要显式安装它，你可以在这里找到完整的代码<a class="ae ky" href="https://github.com/vardanagarwal/Proctoring-AI/tree/master/coco%20models/tflite%20mobnetv1%20ssd" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="deaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预训练模型可以从 Tensorflow 的博客<a class="ae ky" href="https://www.tensorflow.org/lite/models/object_detection/overview" rel="noopener ugc nofollow" target="_blank">这里</a>下载，或者也提供代码。</p><h1 id="24b4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">标签</h1><p id="cb74" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们的第一步是将标签制作成随后需要的格式，这是一个嵌套的字典，里面有 id 和标签。我们将使用随模型提供的标签地图文本文件，并将其转换为所需的格式。</p><p id="f474" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nc">注</em> </strong> :-标签用“？？?"需要被忽略，并且除了第一个索引之外，跳过那些索引。所以每个标签都向后移动了一个位置。例如，标签“person”位于第一行，但它将被分配一个标签 0，依此类推。</p><p id="120a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可以在它的 Java 实现中找到注释。</p><blockquote class="nd ne nf"><p id="ca16" class="kz la nc lb b lc ld ju le lf lg jx lh ng lj lk ll nh ln lo lp ni lr ls lt lu im bi translated">// SSD Mobilenet V1 模型假设类 0 是后台类</p><p id="f3ae" class="kz la nc lb b lc ld ju le lf lg jx lh ng lj lk ll nh ln lo lp ni lr ls lt lu im bi translated">//在标签文件和类标签中，从 1 开始到 number_of_classes+1，</p><p id="982e" class="kz la nc lb b lc ld ju le lf lg jx lh ng lj lk ll nh ln lo lp ni lr ls lt lu im bi translated">//而 outputClasses 对应于从 0 到 number_of_classes 的类索引</p></blockquote><p id="f81e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可以通过下面的代码来完成:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="991c" class="mx lw it mt b gy my mz l na nb">def create_category_index(label_path='path_to/labelmap.txt'):<br/>    f = open(label_path)<br/>    category_index = {}<br/>    for i, val in enumerate(f):<br/>        if i != 0:<br/>            val = val[:-1]<br/>            if val != '???':<br/>                category_index.update({(i-1): {'id': (i-1), 'name': val}})<br/>            <br/>    f.close()<br/>    return category_index</span></pre><p id="d349" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，忽略第一行，我使用 if 语句并存储<code class="fe nj nk nl mt b">i-1</code>。这将创建一个字典，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/c7b6da9704a0588cd47af6f46f0564c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*QsbOomsxevKvd9-Nw5bAig.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">字典已创建</p></figure><p id="5f5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它将有 80 行，89 个键。</p><h1 id="b840" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">TfLite 解释程序</h1><p id="12bf" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">标签做好了，让我们来了解一下 TfLite 的解释器，以及如何得到结果。</p><h2 id="52cc" class="mx lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">初始化解释程序</h2><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9396" class="mx lw it mt b gy my mz l na nb">import tensorflow as tf</span><span id="5b63" class="mx lw it mt b gy ny mz l na nb">interpreter = tf.lite.Interpreter(model_path="path/detect.tflite")<br/>interpreter.allocate_tensors()</span></pre><p id="5a56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只需加载你的 tflite 模型的正确模型路径，分配张量即可。</p><h2 id="13f6" class="mx lw it bd lx nn no dn mb np nq dp mf li nr ns mh lm nt nu mj lq nv nw ml nx bi translated">输入和输出详细信息</h2><p id="0454" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">要获得输入和输出的详细信息，请编写:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="27b6" class="mx lw it mt b gy my mz l na nb">input_details = interpreter.get_input_details()<br/>output_details = interpreter.get_output_details()</span></pre><p id="7615" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们研究它们，看看给出什么类型的输入和我们将得到的输出。</p><p id="a274" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入细节是一个只有 1 个元素的列表，它是一个字典，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/0a15fb40ff36cd4466a6e5bd6c925789.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*ra22RSN8GnBFlBiXnD37PQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输入详细信息</p></figure><p id="d61c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们可以看到输入形状是<code class="fe nj nk nl mt b">[1, 300, 300, 3]</code>。除此之外，它要求输入图像的数据类型为<code class="fe nj nk nl mt b">np.uint8</code>。</p><p id="1814" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出细节是一个包含 4 个元素的列表，每个元素包含一个类似输入细节的字典。每个调用返回 10 个结果，第一项存储矩形边界框，第二项存储检测类，第三项存储检测分数，最后一项存储返回的检测数。返回的边界框是标准化的，因此它们可以缩放到任何输入尺寸。</p><p id="9cbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了获得输出，我们需要读取图像，如果使用 OpenCV，将其转换为 RGB，适当地调整其大小，并在用输入帧设置张量后调用解释器。然后可以使用解释器的<code class="fe nj nk nl mt b">get_tensor</code>功能获得所需的值。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="59dd" class="mx lw it mt b gy my mz l na nb">import cv2</span><span id="6c73" class="mx lw it mt b gy ny mz l na nb">img = cv2.imread('image.jpg')<br/>img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>img_rgb = cv2.resize(img_rgb, (300, 300), cv2.INTER_AREA)<br/>img_rgb = img_rgb.reshape([1, 300, 300, 3])</span><span id="08bb" class="mx lw it mt b gy ny mz l na nb">interpreter.set_tensor(input_details[0]['index'], img_rgb)<br/>interpreter.invoke()</span><span id="5d55" class="mx lw it mt b gy ny mz l na nb">de_boxes = interpreter.get_tensor(output_details[0]['index'])[0]<br/>det_classes = interpreter.get_tensor(output_details[1]['index'])[0]<br/>det_scores = interpreter.get_tensor(output_details[2]['index'])[0]<br/>num_det = interpreter.get_tensor(output_details[3]['index'])[0]</span></pre><h1 id="aab4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">使用对象检测代码进行绘制</h1><p id="6fa6" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了可视化，我使用了可用的 python 代码<a class="ae ky" href="https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py" rel="noopener ugc nofollow" target="_blank">这里是</a>，它不仅可以用来绘制边界框，如果需要的话还可以用来绘制关键点和实例遮罩。我们需要传递要绘制的图像、边界框、检测到的类、检测分数和标签字典。除此之外，当我们从解释器接收到归一化的边界框坐标时，我们还将归一化的坐标设置为 true。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9150" class="mx lw it mt b gy my mz l na nb">from object_detection.utils import visualization_utils as vis_util</span><span id="35fd" class="mx lw it mt b gy ny mz l na nb">vis_util.visualize_boxes_and_labels_on_image_array(<br/>    img,<br/>    output_dict['detection_boxes'],<br/>    output_dict['detection_classes'],<br/>    output_dict['detection_scores'],<br/>    category_index,<br/>    use_normalized_coordinates=True,<br/>    min_score_thresh=0.6,<br/>    line_thickness=3)</span></pre><h1 id="bd19" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结果</h1><p id="4eda" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">它将给出如下所示的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/b35a2769b57c2f925057e8fa6235ae1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*EEiuRfjh9ZbajB16o2xvUg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">结果</p></figure><p id="3039" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，仍然有一个问题需要解决，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/00feeebc3243ce246f12bb916f5ded84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*CGytZlfYHifS0YB1zcRIsw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">问题结果</p></figure><p id="2241" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可以通过非最大抑制来实现。</p><h1 id="3c0b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">非最大抑制</h1><p id="8ac9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我不打算解释它，因为它已经在互联网上的各种文章中深入讨论过了。一个这样的例子是这篇文章。为了实现它，我将使用<code class="fe nj nk nl mt b">combined_non_max_suppression</code> Tensorflow Image 来执行这个任务，因为它允许我们同时处理多个类。它获取输出并返回阈值后剩余的预测。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="e34b" class="mx lw it mt b gy my mz l na nb">def apply_nms(output_dict, iou_thresh=0.5, score_thresh=0.6):</span><span id="74a3" class="mx lw it mt b gy ny mz l na nb">q = 90 # no of classes<br/>    num = int(output_dict['num_detections'])<br/>    boxes = np.zeros([1, num, q, 4])<br/>    scores = np.zeros([1, num, q])<br/>    # val = [0]*q<br/>    for i in range(num):<br/>        # indices = np.where(classes == output_dict['detection_classes'][i])[0][0]<br/>        boxes[0, i, output_dict['detection_classes'][i], :] = output_dict['detection_boxes'][i]<br/>        scores[0, i, output_dict['detection_classes'][i]] = output_dict['detection_scores'][i]<br/>    nmsd = tf.image.combined_non_max_suppression(<br/>    boxes=boxes,<br/>    scores=scores,    <br/>    max_output_size_per_class=num,                                            <br/>    max_total_size=num,<br/>    iou_threshold=iou_thresh,<br/>    score_threshold=score_thresh,<br/>    pad_per_class=False,<br/>    clip_boxes=False)<br/>    <br/>    valid = nmsd.valid_detections[0].numpy()<br/>    output_dict = {<br/>                   'detection_boxes' : nmsd.nmsed_boxes[0].numpy()[:valid],<br/>                   'detection_classes' : nmsd.nmsed_classes[0].numpy().astype(np.int64)[:valid],<br/>                   'detection_scores' : nmsd.nmsed_scores[0].numpy()[:valid],<br/>                   }<br/>    return output_dict</span></pre><p id="708b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面给出了完整的代码，或者你可以访问我的 Github <a class="ae ky" href="https://github.com/vardanagarwal/Proctoring-AI/tree/master/coco%20models/tflite%20mobnetv1%20ssd" rel="noopener ugc nofollow" target="_blank"> repo </a>，它也包含了<code class="fe nj nk nl mt b">visualization_utils.py</code>和模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="4945" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在结束之前，我想澄清一件事，如果你试图在采用英特尔处理器的 Windows 上运行它，你会得到一个可怕的 fps。我在 i5 上得到了大约 2，作为比较，没有 tflite 的相同 Tensorflow 模型给了我大约 8 fps。这里的<a class="ae ky" href="https://stackoverflow.com/questions/54093424/why-is-tensorflow-lite-slower-than-tensorflow-on-desktop" rel="noopener ugc nofollow" target="_blank">就是解释这个</a>。然而，在 edge 设备上，这不是问题，而且内存占用相当少，这将有利于他们的内存限制。</p><p id="ab33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然这个模型不是很准确，但我希望我会提供一个样板文件，使您在使用 Tflite 的对象检测器时更容易完成任务。</p></div></div>    
</body>
</html>