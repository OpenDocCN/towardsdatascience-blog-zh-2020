<html>
<head>
<title>Finding a way to an igloo on a foggy lake with reinforcement learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用强化学习找到一条通往雾湖上冰屋的路</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/finding-a-way-to-an-igloo-on-a-foggy-lake-with-reinforcement-learning-abac293d798d?source=collection_archive---------71-----------------------#2020-04-26">https://towardsdatascience.com/finding-a-way-to-an-igloo-on-a-foggy-lake-with-reinforcement-learning-abac293d798d?source=collection_archive---------71-----------------------#2020-04-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9c26" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在“frozenlake-nonslippery”环境下使用交叉熵方法开始强化学习。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c24f8c44b4d53e1ea7b24a58ad1d359d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L0aIVB-AYN6bcyY0nak-Uw.jpeg"/></div></div></figure><p id="7915" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">环境中的智能体随着时间的推移对最优决策的学习通常是如何定义强化学习的。</p><p id="a022" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在高层次上，有几种强化学习的方法，以一种过于简化的方式分类和解释如下:</p><p id="90de" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 1。</strong> <strong class="kt ir">无模型还是有模型:<br/> </strong> a. <strong class="kt ir">无模型:</strong>代理先行动后思考的蛮力法。<br/> b. <strong class="kt ir">基于模型:</strong>代理根据历史数据做出预测，根据预测采取行动。</p><p id="ab2e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 2。</strong> <strong class="kt ir">基于价值还是基于政策:<br/> </strong> a. <strong class="kt ir">基于价值:</strong>从一个环境的给定状态中，代理人可以获得的总报酬折现。<br/> b. <strong class="kt ir">基于策略:</strong>当一个agent有很多动作空间，有了策略，解释器(we)就会控制agent的状态和动作。</p><p id="2b8a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 3。符合政策还是不符合政策:<br/> </strong> a. <strong class="kt ir">符合政策</strong>:学习是所采取的行动及其对国家的影响的结果。<br/> b. <strong class="kt ir">非政策</strong>:学习是几个行动的结果，然后累积奖励。</p><p id="7777" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">交叉熵方法</strong>是一种无模型、基于策略、基于策略的方法。</p><h1 id="e130" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">在结冰的湖面上:</strong></h1><p id="45ae" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">环境是一个冰冻的湖，一个代理试图到达一个目的地。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/dc4dcbb453ee8109e68d6d034abde025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQixKLkWAhCXPoak_6eFuw.png"/></div></div></figure><h1 id="16c5" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">挑战</h1><p id="810b" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">湖面上覆盖着冰和浓雾。对我们来说，冰原上有已知的开口，但特工并不知道。</p><p id="7b68" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">代理人穿着钉鞋，以防止他转弯时在冰上滑倒。</p><p id="f8b8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">演员的能见度也很差，导航也是一个挑战。对这个演员来说，每一步都是信念的飞跃。</p><p id="67a2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是一个生存游戏，目标是回家。因此，只有当我们到家时，才会收到奖励“1”。在余下的旅程中，我们的假设是，我们活着是为了迈出新的一步。</p><p id="dc13" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们没有从生存的环境中得到任何回报。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/979f11b6cf059d8ffa939a160cb73288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ag-RT5GZf7KmxlSdqO5DCQ.png"/></div></div></figure><p id="5c16" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">将案例转换为确定的非连续场景:</p><p id="8343" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Lake是一个4x4的网格，代理可以上下左右移动。因此，观察空间从0到15是离散的，动作空间从0到3也是如此。</p><p id="7711" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们继续创建一个类，通过使用onehot编码将观察空间和动作空间转换为二进制。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="c8b9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在代码中，我们的环境看起来像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/e6bad65bc26b8e624f8cd2a34cdf86e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*mTsGHWjt6YoTM6WGE5MxPQ.png"/></div></figure><h1 id="9f57" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">交叉熵方法:</strong></h1><p id="46c2" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">由于这是一种基于策略的方法，神经网络(非线性函数)确定代理人应该采取的行动，以使回报最大化。</p><p id="6537" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该方法的步骤:</p><ol class=""><li id="51c6" class="mo mp iq kt b ku kv kx ky la mq le mr li ms lm mt mu mv mw bi translated">使用当前模式和环境播放“n”集。</li><li id="978c" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">计算每集的总奖励，并确定一个奖励范围。通常，我们使用所有奖励的某个百分点，比如第50到第70。</li><li id="3d2e" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">扔掉所有低于界限的有奖励的剧集。</li><li id="9f1b" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">将观察结果作为输入，将发布的行动作为期望输出，对剩余的“精华”片段进行培训。</li><li id="a08d" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">从第一步开始重复，直到我们对结果满意或达到目标。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/9fb8fe17f615d34fd7f8f52bd1b87b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*0PwuHuSmgpAYdoKU08Yh2w.png"/></div></figure><p id="d0b0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我们冰冻的湖泊环境背景下，让我们看看“第二集”:</p><p id="6aab" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">观察1: <strong class="kt ir"> o1 </strong> =像素1，1，<strong class="kt ir"> a1 </strong> = 0，<strong class="kt ir"> r1 </strong> = 0 <br/>观察2: <strong class="kt ir"> o2 </strong> =像素1，2，<strong class="kt ir"> a1 </strong> =右，<strong class="kt ir"> r1 </strong> = 0 <br/>观察3: <strong class="kt ir"> o2 </strong> =像素2，2，<strong class="kt ir"> a1 </strong> =下，<strong class="kt ir"> r1 </strong> = 0(结束</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/3a1265495a2bc59c2bec46543451dbca.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*8yTsIogJUb1O_nIX02B3yA.png"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><h1 id="36f1" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">解决奖励问题:</strong></h1><p id="0ae9" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">我们将通过引入折扣系数为0.9到0.95的总奖励来解决奖励检测进度的问题。</p><p id="4551" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这都是在filter_batch函数中引入的:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="9449" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们会将成功的剧集或播放时间较长的剧集保留更长时间，以便模特可以从中学习，并通过降低学习速度来花更多时间学习。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><h1 id="4f36" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">结论和实用性:</h1><p id="b1fb" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">尽管在我们的日常生活中，我们并不挣扎着寻找回家的路，我们也知道目的地，但与深度学习领域相比，冰湖或强化学习领域中的任何其他案例都被批评为与企业世界无关。</p><p id="d50b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">强化学习本身依赖于深度学习世界中现有的方法。在强化学习中，我们看待问题的方式是不同的。</p><p id="78c5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">作为欺诈检测的一个示例，我们将不得不创建一个环境，其中包含欺诈者和客户的丰富信息。然后，我们将需要一个代理人谁将试图检测和解决欺诈的个人资料。</p><p id="b4bb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这将使我们能够根据欺诈可能存在且不在我们覆盖范围内的可能性来训练模型。</p><p id="2256" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你已经到了这一步，感谢你阅读这个故事，你可以在这里获得完整代码<a class="ae ne" href="https://gist.github.com/alaizaz/5ae4ad843ba7b68fb7be38797450816a.js" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="dedc" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">来源:</h1><p id="ffaa" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">非常感谢马克西姆·拉潘，他的书<a class="ae ne" href="https://www.amazon.co.uk/Deep-Reinforcement-Learning-Hands-optimization-ebook/dp/B07ZKDLZCR/ref=sr_1_1?crid=2KCF37GX8TAHM&amp;dchild=1&amp;keywords=deep+reinforcement+learning+hands-on&amp;qid=1587922972&amp;sprefix=deep+reinfocr%2Caps%2C151&amp;sr=8-1" rel="noopener ugc nofollow" target="_blank">“深度强化学习实践”</a>帮助我开始了强化学习的世界。</p></div></div>    
</body>
</html>