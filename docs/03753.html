<html>
<head>
<title>Creating Deep Neural Networks from Scratch, an Introduction to Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始创建深度神经网络，强化学习导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2?source=collection_archive---------12-----------------------#2020-04-08">https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2?source=collection_archive---------12-----------------------#2020-04-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="37bd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第一部分:体育馆环境与DNN建筑</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fc6b038db1fe8831644783cbe07508a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DSwT066NQhV-Nzly.jpg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">宠物强化学习！[图片鸣谢:<a class="ae kv" href="https://www.akc.org/expert-advice/training/lure-reward-training-dogs/" rel="noopener ugc nofollow" target="_blank"> <em class="kw">斯蒂芬妮·吉布奥特</em> </a></p></figure><blockquote class="kx ky kz"><p id="1d9a" class="la lb lc ld b le lf jr lg lh li ju lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">本文是三部分系列文章的第一部分，将详细介绍OpenAI gym上Cartpole-v1问题的解决方案——仅使用python库中的numpy。这个解决方案远非最佳解决方案(你可以在健身房网站上找到)，而是专注于从基本原则出发。<br/>运行本文代码的先决条件是python (3.x ),并安装了gym和numpy模块。</p></blockquote><p id="6ce6" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">当我第一次开始在OpenAI健身房寻找强化学习时，我无法找到任何关于如何开始自己构建解决方案的好资源。有非常强大的库(如Tensorflow和Pytorch ),允许您构建极其复杂的神经网络，并轻松解决cartpole问题，但我想从头开始创建神经网络，因为我相信理解现代机器学习技术的核心构建块是有价值的。我写的是我希望在我努力工作的时候能够找到的东西。所以让我们开始吧。</p><p id="1af3" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">一、什么是OpenAI健身房？在他们的<a class="ae kv" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank">网站</a>上有一个很好的简短介绍，“Gym是一个开发和比较强化学习算法的工具包。”“<a class="ae kv" href="https://github.com/openai/gym" rel="noopener ugc nofollow" target="_blank"> gym </a>库是一个测试问题——环境——的集合，你可以用它来制定你的强化学习算法。这些环境有一个共享的接口，允许你编写通用算法。”这意味着，围绕构建和渲染模拟真实世界场景的模型的工程已经为我们完成，所以我们可以专注于教代理玩好游戏。</p><p id="bb34" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">上面的描述也提到了强化学习。那是什么？这里有一个维基百科的总结——“<strong class="ld ir">强化学习</strong>是<a class="ae kv" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>的一个领域，涉及软件代理应该如何在一个环境中采取<a class="ae kv" href="https://en.wikipedia.org/wiki/Action_selection" rel="noopener ugc nofollow" target="_blank">行动</a>以最大化一些累积回报的概念。”</p><p id="0440" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">给你一个类比，想想一只狗是如何被训练的——有利的行为被积极地强化(以款待的形式)，而负面的行为被消极地强化。在某种程度上，即使是我们人类，也是复杂的强化学习代理，试图通过选择我们认为在未来会“有益于”我们(以更大回报的形式)的行动来最大化实现我们目标的机会。这张图展示了强化学习的循环，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/a49f94d8cc0fe87ea9c41d9c313fd293.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*vL8ffMdIDFc8wp4CEDt_tg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">强化学习周期[Image credit:<a class="ae kv" href="https://itnext.io/reinforcement-learning-with-q-tables-5f11168862c8" rel="noopener ugc nofollow" target="_blank">Mohit Mayank</a></p></figure><p id="a906" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">上图显示了一个<em class="lc">代理</em>(我们将构建的程序)将<em class="lc">环境</em>的状态和<em class="lc">奖励</em>作为输入，从之前的<em class="lc">动作</em>中选择一个后续动作并将其反馈给环境，然后再次观察环境。</p><p id="8500" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">很好，现在我们已经了解了强化学习中的基本概念，让我们回到我们试图解决的问题——cart pole。首先，具体看一下关于<a class="ae kv" href="https://gym.openai.com/envs/CartPole-v0/" rel="noopener ugc nofollow" target="_blank">侧翻问题</a>的文档。文档很好地概述了我们正在努力实现的目标。简而言之，我们控制着滑块的底部，顶部有一根垂直平衡的杆子。我们的目标是尽可能长时间地防止杆子脱落。如果极点(就其角度而言)下降到某个点以下，环境将被重置。下面是一个随机代理人在处理横竿问题。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mb mc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">扁担上的随机代理</p></figure><p id="9536" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">如你所见，不是很好！但这是意料之中的，因为这个代理会忽略环境的当前状态，并在每个时间步选择一个随机动作。让我们看看我们是否能做得更好。</p><h1 id="b90f" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">履行</h1><p id="8f8d" class="pw-post-body-paragraph la lb iq ld b le mv jr lg lh mw ju lj lx mx lm ln ly my lq lr lz mz lu lv lw ij bi translated">是写代码的时候了！我们将从导入将要使用的库开始。我们将需要gym用于上面讨论的开放人工智能环境，以及numpy用于一些数学和矩阵操作。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="4a14" class="nf me iq nb b gy ng nh l ni nj">import gym<br/>import numpy as np</span></pre><p id="3b35" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">接下来，我们需要导入gym为侧翻问题提供的环境。这是如何做到的:</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="850b" class="nf me iq nb b gy ng nh l ni nj">env=gym.make('CartPole-v1')</span></pre><p id="c1d2" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">我们还可以通过打印来观察这个特殊环境空间的一些特征:</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="3188" class="nf me iq nb b gy ng nh l ni nj">print(env.action_space) # Discrete(2)<br/>print(env.observation_space) # Box(4,)</span></pre><p id="4e8e" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">在<a class="ae kv" href="https://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank">文档</a>中有更多关于环境及其工作方式的信息，但是上面的值捕捉了定义这个环境的基本元素——可以执行的动作，以及在每个时间步的观察。</p><p id="0a98" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">动作空间是离散的，包含2个值:0和1。这些对应于代理能够执行的两个动作，即向左或向右推动滑块。</p><p id="798e" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">另一方面，观察空间是连续的，并且有四个组成部分(不要与数据结构框(4)相混淆，对于我们的目的，它仅仅意味着包含四个值的数组)。四个值是什么意思？它们是代表当时环境状态的数字——即大车的位置、大车的速度、杆子的角度、杆子的转速。[<a class="ae kv" href="https://github.com/openai/gym/issues/238#issuecomment-231129955" rel="noopener ugc nofollow" target="_blank">https://github . com/open ai/gym/issues/238 # issue comment-231129955</a>]</p><p id="aa92" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">这里要理解的一个基本事情是，观察和动作空间中的数字的含义只是为了完整性而解释的，我们的目标不是解释(动作空间或观察空间的)值，而是让代理学习这些值在上下文中的含义。让我们回到我们的程序，添加代码来运行一个基本的循环。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="c93e" class="nf me iq nb b gy ng nh l ni nj"># Global variables<strong class="nb ir"><br/></strong>NUM_EPISODES = 10<br/>MAX_TIMESTEPS = 1000</span><span id="1a9c" class="nf me iq nb b gy nk nh l ni nj"># The main program loop<strong class="nb ir"><br/></strong>for i_episode in range(NUM_EPISODES):<br/>    observation = env.reset()<br/>    # Iterating through time steps within an episode<strong class="nb ir"><br/></strong>    for t in range(MAX_TIMESTEPS):<br/>        env.render()<br/>        action = env.action_space.sample()<br/>        observation, reward, done, info = env.step(action)<br/>        if done:<br/>            # If the pole has tipped over, end this episode<strong class="nb ir"><br/></strong>            break</span></pre><p id="e773" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">上面的代码为剧集声明了一个主程序循环，并遍历了一集内的时间步长。在内部循环中，程序采取行动，观察结果，然后检查情节是否已经结束(要么是杆子倒下了，要么是滑块脱离了边缘)。如果有，环境被重置，内部循环重新开始。</p><p id="eb62" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">选择动作的行是从可用的动作中随机抽样的；事实上，它的行为与前面显示的随机代理完全一样。让我们修改一下，定义一个自定义方法来选择给定观察的动作。</p><p id="051d" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">现在，我们将定义一个代理(希望如此！)将学会在给定的状态下更聪明地选择行动。我们将在一个类中模拟这个代理:</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="720b" class="nf me iq nb b gy ng nh l ni nj"><strong class="nb ir">class RLAgent:<br/></strong>    env = None</span><span id="2eda" class="nf me iq nb b gy nk nh l ni nj">    def <strong class="nb ir">__init__</strong>(self, env):<br/>        self.<strong class="nb ir"><em class="lc">env </em></strong>= env<br/>        <br/>    def <strong class="nb ir">select_action</strong>(self, observation):<br/>        return env.action_space.sample()</span></pre><p id="733c" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">我们还需要向全局变量添加一个RLAgent实例，并更改操作选择，以便从实例化的类中调用该函数。注意，目前select_action函数做的事情和以前一样，但是我们以后会改变这一点。</p><h1 id="fc17" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">神经网络</h1><p id="2169" class="pw-post-body-paragraph la lb iq ld b le mv jr lg lh mw ju lj lx mx lm ln ly my lq lr lz mz lu lv lw ij bi translated">我们现在将创建我们的神经网络的元素。关于<a class="ae kv" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a>的快速入门:“人工神经网络(ANN)是一个被称为<a class="ae kv" href="https://en.wikipedia.org/wiki/Artificial_neuron" rel="noopener ugc nofollow" target="_blank">人工神经元</a>的连接单元或节点的集合，它松散地模拟了生物大脑中的<a class="ae kv" href="https://en.wikipedia.org/wiki/Neuron" rel="noopener ugc nofollow" target="_blank">神经元</a>。每个连接，就像生物大脑中的<a class="ae kv" href="https://en.wikipedia.org/wiki/Synapse" rel="noopener ugc nofollow" target="_blank">突触</a>，可以向其他神经元传递信号。人工神经元接收信号，然后进行处理，并向与之相连的神经元发出信号。”</p><p id="1e39" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">这就是我们的未来，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/fd78a24e31658151041e85b9cb5acf0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UhQDSRnrFiQX3n_xmLfn_Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">神经网络</p></figure><p id="75da" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">上图显示了一个神经网络，它有一个输入层、两个“隐藏”层(第2层和第3层)和一个输出层。该模型提供从观察空间到输入层的输入，这些输入被“前馈”到后续层，直到输出层，其中输出层中的值被用于选择动作。</p><p id="384a" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">例如，层3中的每个节点都是通过激活函数传递的层2的线性组合(加权和)。用于计算第3层的权重在矩阵中随机初始化，并通过称为<a class="ae kv" rel="noopener" target="_blank" href="/stochastic-gradient-descent-clearly-explained-53d239905d31"> <em class="lc">【随机梯度下降】</em> </a>的过程逐渐调整，以更好地预测输出。激活函数是一个简单的非线性函数，它允许分类器学习基础观察空间中的非线性规则。</p><p id="5e97" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">在我们的CartPole问题中，有5个输入(观察空间的元素+一个偏差项)和2个输出(我们可以推车的两个方向)。</p><p id="6205" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">神经网络层将被封装在一个NNLayer类中，</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="9748" class="nf me iq nb b gy ng nh l ni nj"><strong class="nb ir">class NNLayer:</strong><br/>    # class representing a neural net layer<br/>    def <strong class="nb ir">__init__</strong>(self, input_size, output_size, activation=None, lr       = 0.001):<br/>        self.<strong class="nb ir"><em class="lc">input_size </em></strong>= input_size<br/>        self.<strong class="nb ir"><em class="lc">output_size </em></strong>= output_size<br/>        self.<strong class="nb ir">weights </strong>= np.random.uniform(low=-0.5, high=0.5, size=(input_size, output_size))<br/>        self.<strong class="nb ir"><em class="lc">activation_function</em></strong> = activation<br/>        self.<strong class="nb ir"><em class="lc">lr</em></strong> = lr</span></pre><p id="cc91" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">这个类包含三个主要内容:</p><ol class=""><li id="00cc" class="nm nn iq ld b le lf lh li lx no ly np lz nq lw nr ns nt nu bi translated">图层的尺寸(输入和输出尺寸)。</li><li id="3c4a" class="nm nn iq ld b le nv lh nw lx nx ly ny lz nz lw nr ns nt nu bi translated">连接输入图层和输出图层的权重。</li><li id="fb24" class="nm nn iq ld b le nv lh nw lx nx ly ny lz nz lw nr ns nt nu bi translated">输出的激活函数(默认激活为无，也称为线性)。</li></ol><p id="825f" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">我们现在将这个类的用法添加到我们的RLAgent中。首先，我们将编辑选择动作函数，</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="9c86" class="nf me iq nb b gy ng nh l ni nj">def <strong class="nb ir">select_action</strong>(self, observation):<br/>        values = self.forward(observation)<br/>        if (np.random.random() &gt; self.epsilon):<br/>            return np.argmax(values)<br/>        else:<br/>            return np.random.randint(self.env.action_space.n)</span></pre><p id="6417" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">这个函数不是每次都随机选择一个值，而是将关于环境状态的信息传递给我们的神经网络，并计算每个行为的“预期回报”(values是一个接受(1,nᵢₙ)数组并返回(1,nₒᵤₜ)数组的函数)。然后，它会选择能带来最大预期回报的行动。注意，该函数仍然选择概率为ε的随机值。ε，也称为“探索率”，是强化学习中一个重要概念的实现:探索和利用之间的权衡。探索有助于模型不陷入局部最小值，通过不时探索明显次优的行动，可能揭示更大的回报。另一方面，利用允许代理使用其对当前状态的了解来选择最有利可图的动作。在大多数RL代理中，epsilon在开始时很高(接近1.0)，并且随着时间的推移逐渐降低到0，因为代理在给定状态下对动作的学习值变得更有信心。</p><p id="f086" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">select_action函数还调用self.forward (RLAgent。Forward)，下面是这个函数的代码，</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="52b1" class="nf me iq nb b gy ng nh l ni nj">def <strong class="nb ir">forward</strong>(self, observation, remember_for_backprop=True):<br/>        vals = np.copy(observation)<br/>        index = 0<br/>        for layer in self.layers:<br/>            vals = layer.forward(vals, remember_for_backprop)<br/>            index = index + 1<br/>        return vals</span></pre><p id="ed2e" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">上面的RLAgent.forward函数有一个简单的循环。它将输入(我们试图决定行动过程的观察)传递给网络，并获得每个行动的一组值。它在内部调用NNLayer.forward函数，收集每一层的输出并将其传递给下一层。为了完成select action函数的实现，下面是最后一部分NNLayer.forward函数。remember_for_backprop参数是一个布尔值，它指定是否需要存储某些计算值，以防止权重更新期间的重复计算(这将在<em class="lc">反向传播</em>一节中详细解释)。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="ca4a" class="nf me iq nb b gy ng nh l ni nj"># Compute the forward pass for this layer<br/>def <strong class="nb ir">forward</strong>(self, inputs, remember_for_backprop=True):<br/>        input_with_bias = np.append(np.ones((len(inputs),1)),inputs, axis=1)<br/>        unactivated = np.dot(input_with_bias, self.weights)<br/>        output = unactivated<br/>        if self.<strong class="nb ir"><em class="lc">activation_function </em></strong>!= None:<br/>            output = self.<strong class="nb ir"><em class="lc">activation_function</em></strong>(output)<br/>        if remember_for_backprop:<br/>            # store variables for backward pass     <br/>            self.<strong class="nb ir"><em class="lc">backward_store_in</em></strong> = input_with_bias<br/>            self.<strong class="nb ir"><em class="lc">backward_store_out</em></strong> = np.copy(unactivated)<br/>            <br/>        return output</span></pre><p id="cf52" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">这个功能—</p><ol class=""><li id="9525" class="nm nn iq ld b le lf lh li lx no ly np lz nq lw nr ns nt nu bi translated">将偏差项追加到输入中。</li><li id="89e1" class="nm nn iq ld b le nv lh nw lx nx ly ny lz nz lw nr ns nt nu bi translated">计算该层的输入和权重矩阵的乘积。</li><li id="50c5" class="nm nn iq ld b le nv lh nw lx nx ly ny lz nz lw nr ns nt nu bi translated">获取步骤2的输出，如果已经为该层定义了一个激活函数(在我们的例子中是ReLU)，则通过该函数发送这个输出。</li></ol><p id="a2ba" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">让我们在RLAgent的init函数中添加这些层的实例化，</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="1971" class="nf me iq nb b gy ng nh l ni nj">def <strong class="nb ir">__init__</strong>(self, env):<br/>   self.<strong class="nb ir"><em class="lc">env </em></strong>= env<br/>   self.<strong class="nb ir"><em class="lc">hidden_size </em></strong>= 24<br/>   self.<strong class="nb ir"><em class="lc">input_size </em></strong>= env.observation_space.shape[0]<br/>   self.<strong class="nb ir"><em class="lc">output_size </em></strong>= env.action_space.n<br/>   self.<strong class="nb ir"><em class="lc">num_hidden_layers </em></strong>= 2<br/>   self.<strong class="nb ir"><em class="lc">epsilon </em></strong>= 1.0</span><span id="a773" class="nf me iq nb b gy nk nh l ni nj">   self.<strong class="nb ir"><em class="lc">layers </em></strong>= [NNLayer(self.input_size + 1, self.hidden_size,       activation=relu)]<br/>   for i in range(self.num_hidden_layers-1):<br/>       self.<strong class="nb ir"><em class="lc">layers</em></strong>.append(NNLayer(self.hidden_size+1, self.hidden_size, activation=relu))<br/>   self.<strong class="nb ir"><em class="lc">layers</em></strong>.append(NNLayer(self.hidden_size+1, self.output_size))</span></pre><p id="3b9e" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">你可以看到上面我们总共有2个隐藏层和1个输出层。此外，在除了输出层之外的所有层中，我们都使用了一个激活函数，叫做<strong class="ld ir">Re</strong>ctived<strong class="ld ir">L</strong>linear<strong class="ld ir">U</strong>nit(ReLU)。这个函数是一个非常简单的函数，它给我们的神经网络引入了足够的非线性。这是它的实现，</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="d8f3" class="nf me iq nb b gy ng nh l ni nj">def <strong class="nb ir">relu</strong>(mat):<br/>    return np.multiply(mat,(mat&gt;0))</span></pre><p id="66de" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">此函数接收一个矩阵，并返回另一个具有相同值的矩阵，其中原始矩阵大于0，所有其他值都为0。最后，让我们将代理的初始化和epsilon decay添加到主程序循环中。这是新的主程序的样子:</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="3280" class="nf me iq nb b gy ng nh l ni nj"># Global variables<br/>NUM_EPISODES = 10<br/>MAX_TIMESTEPS = 1000<br/>model = RLAgent(env)</span><span id="0d58" class="nf me iq nb b gy nk nh l ni nj"># The main program loop<br/>for i_episode in range(NUM_EPISODES):<br/>    observation = env.reset()<br/>    # Iterating through time steps within an episode<br/>    for t in range(MAX_TIMESTEPS):<br/>        env.render()<br/>        action = model.select_action(observation)<br/>        observation, reward, done, info = env.step(action)<br/>        # epsilon decay<br/>        model.epsilon = model.epsilon if model.epsilon &lt; 0.01 else model.epsilon*0.995<br/>        if done:<br/>            # If the pole has tipped over, end this episode<br/>            print('Episode {} ended after {} timesteps, current exploration is {}'.format(i_episode, t+1,model.epsilon))<br/>            break</span></pre><p id="5ec0" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">这个模型目前做什么？它随机初始化神经网络的权重，并基于这些权重计算任何给定状态下的动作值。然而，我们需要一种方法来改进网络中的权重值，以便代理能够在任何给定的状态下采取最佳行动。如前所述，这是通过随机梯度下降实现的，并通过称为<strong class="ld ir">反向传播</strong>的技术实施。我将在下一篇文章中详细介绍反向传播以及强化学习的相关理论。</p><p id="45b4" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">总而言之，这就是我们迄今为止所取得的成就:</p><ol class=""><li id="f1f4" class="nm nn iq ld b le lf lh li lx no ly np lz nq lw nr ns nt nu bi translated">编写了与OpenAI gym中的CartPole的环境空间进行交互的主要程序组件。</li><li id="0d49" class="nm nn iq ld b le nv lh nw lx nx ly ny lz nz lw nr ns nt nu bi translated">将强化学习代理及其组件神经网络层封装在各自的类中。</li><li id="5058" class="nm nn iq ld b le nv lh nw lx nx ly ny lz nz lw nr ns nt nu bi translated">为我们的深度学习强化学习代理编码和初始化神经网络架构。</li><li id="9aa1" class="nm nn iq ld b le nv lh nw lx nx ly ny lz nz lw nr ns nt nu bi translated">实施“前馈”计算，通过神经网络传播对环境的观察，以计算行动值。</li></ol><p id="e6aa" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">在下一篇文章中，我们将致力于实现以下目标:</p><ol class=""><li id="7ecb" class="nm nn iq ld b le lf lh li lx no ly np lz nq lw nr ns nt nu bi translated">检查并形式化一个“累积奖励”的概念，即一个代理人在特定的状态下对掷球问题的期望。</li><li id="cf28" class="nm nn iq ld b le nv lh nw lx nx ly ny lz nz lw nr ns nt nu bi translated">理解代理应如何更新神经网络中的权重，以更接近其想法，即在特定状态下采取特定行动预期的正确累积回报。</li><li id="2145" class="nm nn iq ld b le nv lh nw lx nx ly ny lz nz lw nr ns nt nu bi translated">实现反向传播——实现上面2中提到的目标的算法。</li></ol><p id="757b" class="pw-post-body-paragraph la lb iq ld b le lf jr lg lh li ju lj lx ll lm ln ly lp lq lr lz lt lu lv lw ij bi translated">下次见！</p></div></div>    
</body>
</html>