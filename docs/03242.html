<html>
<head>
<title>Pytorch [Tabular] — Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">py torch[表格]-回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-tabular-regression-428e9c9ac93?source=collection_archive---------6-----------------------#2020-03-28">https://towardsdatascience.com/pytorch-tabular-regression-428e9c9ac93?source=collection_archive---------6-----------------------#2020-03-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/840b4e62b667c78ce2f768c8b11e4488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Dsdw-L4qVhT1WkyLvtsPg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">如何训练你的神经网络[图片[0]]</p></figure><h2 id="2bd9" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/akshaj-wields-pytorch" rel="noopener">如何训练你的神经网络</a></h2><div class=""/><div class=""><h2 id="170f" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">这篇博文将带您了解如何使用PyTorch实现对表格数据的回归。</h2></div><p id="c4ed" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们将使用Kaggle上的<a class="ae lz" href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009" rel="noopener ugc nofollow" target="_blank">红酒质量数据集</a>。该数据集有12列，其中前11列是要素，最后一列是目标列。数据集有1599行。</p><h1 id="738f" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">导入库</h1><p id="b724" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">我们使用<code class="fe mx my mz na b">tqdm</code>来启用训练和测试循环的进度条。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="153b" class="nj mb jf na b gy nk nl l nm nn">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from tqdm.notebook import tqdm<br/>import matplotlib.pyplot as plt</span><span id="59c2" class="nj mb jf na b gy no nl l nm nn">import torch<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>from torch.utils.data import Dataset, DataLoader</span><span id="6ee8" class="nj mb jf na b gy no nl l nm nn">from sklearn.preprocessing import MinMaxScaler    <br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import mean_squared_error, r2_score</span></pre><h1 id="414b" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">读出数据</h1><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="9294" class="nj mb jf na b gy nk nl l nm nn">df = pd.read_csv("data/tabular/classification/winequality-red.csv")</span><span id="a137" class="nj mb jf na b gy no nl l nm nn">df.head()</span></pre><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/d6076b9ef757739b113525f718219e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jM0yesrdrjHwEuuIKx0rFA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">输入数据帧[图像[2]]</p></figure><h1 id="26d1" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">EDA和预处理</h1><p id="ceed" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">首先，我们绘制输出行来观察类分布。这里有很多不平衡。类别3、4和8的样本数量很少。</p><p id="af98" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这里我们不会将输出变量视为类，因为我们正在执行回归。我们将把输出列，也就是所有的<code class="fe mx my mz na b">integers</code>，转换成<code class="fe mx my mz na b">float</code>值。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="6c15" class="nj mb jf na b gy nk nl l nm nn">sns.countplot(x = 'quality', data=df)</span></pre><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/519a35a132a4d93139865ba7322018ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*4KuhNm6wXTEDWSHW0t5wUQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">输出分布[图像[3]]</p></figure><h2 id="7166" class="nj mb jf bd mc nr ns dn mg nt nu dp mk lm nv nw mm lq nx ny mo lu nz oa mq jl bi translated">创建输入和输出数据</h2><p id="350e" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">为了将数据分成训练集、验证集和测试集，我们需要将输入和输出分开。</p><p id="c262" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">输入<code class="fe mx my mz na b">X</code>是除最后一列之外的所有列。输出<code class="fe mx my mz na b">y</code>是最后一列。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="1637" class="nj mb jf na b gy nk nl l nm nn">X = df.iloc[:, 0:-1]<br/>y = df.iloc[:, -1]</span></pre><h1 id="9f2a" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">培训-验证-测试</h1><p id="22bb" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">为了创建train-val-test分割，我们将使用Sklearn的<code class="fe mx my mz na b">train_test_split()</code>。</p><p id="56cf" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">首先，我们将数据分成<code class="fe mx my mz na b">train+val</code>和<code class="fe mx my mz na b">test</code>组。然后，我们将进一步分割我们的<code class="fe mx my mz na b">train+val</code>集合，以创建我们的<code class="fe mx my mz na b">train</code>和<code class="fe mx my mz na b">val</code>集合。</p><p id="1974" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">因为存在“类”的不平衡，所以我们希望在我们的训练、验证和测试集中，所有输出类的分布是均等的。</p><p id="dfe9" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为此，我们使用函数<code class="fe mx my mz na b">train_test_split()</code>中的<code class="fe mx my mz na b">stratify</code>选项。</p><p id="16a2" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">记住分层只对类有效，对数字无效。因此，一般来说，我们可以使用四分位数、十分位数、直方图(<code class="fe mx my mz na b">np.histogram()</code>)等将我们的数字分类。因此，您必须创建一个包含输出及其“类”的新数据帧。这个“类”是使用上述方法获得的。</p><p id="734e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在我们的例子中，让我们按原样使用这些数字，因为它们已经和类<em class="ob">一样了。拆分数据后，我们可以将输出转换为float(因为回归)。</em></p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="35d5" class="nj mb jf na b gy nk nl l nm nn"># Train - Test<br/>X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=69)</span><span id="40c6" class="nj mb jf na b gy no nl l nm nn"># Split train into train-val<br/>X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=21)</span></pre><h2 id="58b2" class="nj mb jf bd mc nr ns dn mg nt nu dp mk lm nv nw mm lq nx ny mo lu nz oa mq jl bi translated">标准化输入</h2><p id="c9be" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">神经网络需要介于(0，1)范围内的数据。网上有很多关于我们为什么需要这么做的资料。</p><p id="d09c" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了衡量我们的价值，我们将使用Sklearn中的<code class="fe mx my mz na b">MinMaxScaler()</code>。<code class="fe mx my mz na b">MinMaxScaler</code>通过将每个特征缩放到给定的范围(在我们的例子中是(0，1 ))来转换特征。</p><blockquote class="oc"><p id="9d8a" class="od oe jf bd of og oh oi oj ok ol ly dk translated">x _ scaled =(x-min(x))/(max(x)-min(x))</p></blockquote><p id="cb17" class="pw-post-body-paragraph ld le jf lf b lg om kp li lj on ks ll lm oo lo lp lq op ls lt lu oq lw lx ly ij bi translated">注意，我们在<code class="fe mx my mz na b">X_train</code>上使用<code class="fe mx my mz na b">.fit_transform()</code>，而在<code class="fe mx my mz na b">X_val</code>和<code class="fe mx my mz na b">X_test</code>上使用<code class="fe mx my mz na b">.transform()</code>。</p><p id="0561" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们这样做是因为我们希望使用与训练集相同的参数来扩展验证和测试集，以避免数据泄漏。<code class="fe mx my mz na b">fit_transform()</code>计算并应用缩放值，而<code class="fe mx my mz na b">.transform()</code>仅应用计算值。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="ebf0" class="nj mb jf na b gy nk nl l nm nn">scaler = MinMaxScaler()</span><span id="d8c4" class="nj mb jf na b gy no nl l nm nn">X_train = scaler.fit_transform(X_train)<br/>X_val = scaler.transform(X_val)<br/>X_test = scaler.transform(X_test)</span><span id="21fc" class="nj mb jf na b gy no nl l nm nn">X_train, y_train = np.array(X_train), np.array(y_train)<br/>X_val, y_val = np.array(X_val), np.array(y_val)<br/>X_test, y_test = np.array(X_test), np.array(y_test)</span></pre><h2 id="145c" class="nj mb jf bd mc nr ns dn mg nt nu dp mk lm nv nw mm lq nx ny mo lu nz oa mq jl bi translated">可视化培训、评估和测试中的类别分布</h2><p id="7de3" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">一旦我们将数据分成训练集、验证集和测试集，让我们确保类在所有三个集中的分布是相等的。</p><p id="cb97" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为此，让我们创建一个名为<code class="fe mx my mz na b">get_class_distribution()</code>的函数。该函数将obj <code class="fe mx my mz na b">y</code>即。<code class="fe mx my mz na b">y_train</code>、<code class="fe mx my mz na b">y_val</code>或<code class="fe mx my mz na b">y_test</code>。在函数内部，我们初始化一个字典，其中包含作为键的输出类和作为值的输出类计数。计数都被初始化为0。</p><p id="c429" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然后我们循环遍历我们的<code class="fe mx my mz na b">y</code>对象并更新我们的字典。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="1ee3" class="nj mb jf na b gy nk nl l nm nn">def get_class_distribution(obj):<br/>    count_dict = {<br/>        "rating_3": 0,<br/>        "rating_4": 0,<br/>        "rating_5": 0,<br/>        "rating_6": 0,<br/>        "rating_7": 0,<br/>        "rating_8": 0,<br/>    }<br/>    <br/>    for i in obj:<br/>        if i == 3: <br/>            count_dict['rating_3'] += 1<br/>        elif i == 4: <br/>            count_dict['rating_4'] += 1<br/>        elif i == 5: <br/>            count_dict['rating_5'] += 1<br/>        elif i == 6: <br/>            count_dict['rating_6'] += 1<br/>        elif i == 7: <br/>            count_dict['rating_7'] += 1  <br/>        elif i == 8: <br/>            count_dict['rating_8'] += 1              <br/>        else:<br/>            print("Check classes.")<br/>            <br/>    return count_dict</span></pre><p id="038a" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">一旦我们有了字典计数，我们就使用Seaborn库来绘制条形图。</p><p id="79b3" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了进行绘图，我们首先使用<code class="fe mx my mz na b">pd.DataFrame.from_dict([get_class_distribution(y_train)])</code>将字典转换成数据帧。</p><p id="6548" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">随后，我们将数据帧转换成格式，最后使用格式来构建图表。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="0b4a" class="nj mb jf na b gy nk nl l nm nn">fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(25,7))</span><span id="86f2" class="nj mb jf na b gy no nl l nm nn"># Train<br/>sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(y_train)]).melt(), x = "variable", y="value", hue="variable",  ax=axes[0]).set_title('Class Distribution in Train Set')</span><span id="bbbf" class="nj mb jf na b gy no nl l nm nn"># Val<br/>sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(y_val)]).melt(), x = "variable", y="value", hue="variable",  ax=axes[1]).set_title('Class Distribution in Val Set')</span><span id="b217" class="nj mb jf na b gy no nl l nm nn"># Test<br/>sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(y_test)]).melt(), x = "variable", y="value", hue="variable",  ax=axes[2]).set_title('Class Distribution in Test Set')</span></pre><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/4c95ab9ce97045dde6e48589c596a878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7X7LSXGR36_mhmn48hig3A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">训练值测试分割后的输出分布[图片[4]]</p></figure><h2 id="be56" class="nj mb jf bd mc nr ns dn mg nt nu dp mk lm nv nw mm lq nx ny mo lu nz oa mq jl bi translated">将输出变量转换为<code class="fe mx my mz na b">Float</code></h2><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="5fe0" class="nj mb jf na b gy nk nl l nm nn">y_train, y_test, y_val = y_train.astype(float), y_test.astype(float), y_val.astype(float)</span></pre><h1 id="b3a9" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">神经网络</h1><h2 id="486c" class="nj mb jf bd mc nr ns dn mg nt nu dp mk lm nv nw mm lq nx ny mo lu nz oa mq jl bi translated">初始化数据集</h2><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="5097" class="nj mb jf na b gy nk nl l nm nn">class RegressionDataset(Dataset):<br/>    <br/>    def __init__(self, X_data, y_data):<br/>        self.X_data = X_data<br/>        self.y_data = y_data<br/>        <br/>    def __getitem__(self, index):<br/>        return self.X_data[index], self.y_data[index]<br/>        <br/>    def __len__ (self):<br/>        return len(self.X_data)</span><span id="0873" class="nj mb jf na b gy no nl l nm nn">train_dataset = RegressionDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())</span><span id="1375" class="nj mb jf na b gy no nl l nm nn">val_dataset = RegressionDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())</span><span id="7b53" class="nj mb jf na b gy no nl l nm nn">test_dataset = RegressionDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())</span></pre><h2 id="21b6" class="nj mb jf bd mc nr ns dn mg nt nu dp mk lm nv nw mm lq nx ny mo lu nz oa mq jl bi translated">模型参数</h2><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="b588" class="nj mb jf na b gy nk nl l nm nn">EPOCHS = 150<br/>BATCH_SIZE = 64<br/>LEARNING_RATE = 0.001</span><span id="fe30" class="nj mb jf na b gy no nl l nm nn">NUM_FEATURES = len(X.columns)</span></pre><h2 id="bc6a" class="nj mb jf bd mc nr ns dn mg nt nu dp mk lm nv nw mm lq nx ny mo lu nz oa mq jl bi translated">初始化数据加载器</h2><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="fcd5" class="nj mb jf na b gy nk nl l nm nn">train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)</span><span id="5a8a" class="nj mb jf na b gy no nl l nm nn">val_loader = DataLoader(dataset=val_dataset, batch_size=1)</span><span id="e02f" class="nj mb jf na b gy no nl l nm nn">test_loader = DataLoader(dataset=test_dataset, batch_size=1)</span></pre><h2 id="3cf2" class="nj mb jf bd mc nr ns dn mg nt nu dp mk lm nv nw mm lq nx ny mo lu nz oa mq jl bi translated">定义神经网络架构</h2><p id="2083" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">我们这里有一个简单的3层前馈神经网络。我们使用<code class="fe mx my mz na b">ReLU</code>作为所有层的激活。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="3cce" class="nj mb jf na b gy nk nl l nm nn">class MultipleRegression(nn.Module):<br/>    def __init__(self, num_features):<br/>        super(MultipleRegression, self).__init__()<br/>        <br/>        self.layer_1 = nn.Linear(num_features, 16)<br/>        self.layer_2 = nn.Linear(16, 32)<br/>        self.layer_3 = nn.Linear(32, 16)<br/>        self.layer_out = nn.Linear(16, 1)<br/>        <br/>        self.relu = nn.ReLU()</span><span id="e4ee" class="nj mb jf na b gy no nl l nm nn">def forward(self, inputs):<br/>        x = self.relu(self.layer_1(inputs))<br/>        x = self.relu(self.layer_2(x))<br/>        x = self.relu(self.layer_3(x))<br/>        x = self.layer_out(x)</span><span id="0b21" class="nj mb jf na b gy no nl l nm nn">return (x)</span><span id="7b3a" class="nj mb jf na b gy no nl l nm nn">def predict(self, test_inputs):<br/>        x = self.relu(self.layer_1(test_inputs))<br/>        x = self.relu(self.layer_2(x))<br/>        x = self.relu(self.layer_3(x))<br/>        x = self.layer_out(x)</span><span id="7542" class="nj mb jf na b gy no nl l nm nn">return (x)</span></pre><h2 id="e574" class="nj mb jf bd mc nr ns dn mg nt nu dp mk lm nv nw mm lq nx ny mo lu nz oa mq jl bi translated">检查GPU</h2><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="9e0f" class="nj mb jf na b gy nk nl l nm nn">device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")</span><span id="0b3b" class="nj mb jf na b gy no nl l nm nn">print(device)<br/></span><span id="3208" class="nj mb jf na b gy no nl l nm nn">###################### OUTPUT ######################</span><span id="855d" class="nj mb jf na b gy no nl l nm nn">cuda:0</span></pre><p id="05c2" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">初始化模型、优化器和损失函数。将模型传输到GPU。</p><p id="3b79" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们使用均方误差损失。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="9b5f" class="nj mb jf na b gy nk nl l nm nn">model = MultipleRegression(NUM_FEATURES)<br/>model.to(device)</span><span id="efe5" class="nj mb jf na b gy no nl l nm nn">print(model)</span><span id="3cf6" class="nj mb jf na b gy no nl l nm nn">criterion = nn.MSELoss()<br/>optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)</span><span id="fe25" class="nj mb jf na b gy no nl l nm nn"><br/>###################### OUTPUT ######################</span><span id="eeb8" class="nj mb jf na b gy no nl l nm nn">MultipleRegression(<br/>  (layer_1): Linear(in_features=11, out_features=16, bias=True)<br/>  (layer_2): Linear(in_features=16, out_features=32, bias=True)<br/>  (layer_3): Linear(in_features=32, out_features=16, bias=True)<br/>  (layer_out): Linear(in_features=16, out_features=1, bias=True)<br/>  (relu): ReLU()<br/>)</span></pre><h1 id="715e" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">火车模型</h1><p id="08ed" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">在我们开始训练之前，让我们定义一个字典，它将存储训练集和验证集的丢失/时期。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="6fbe" class="nj mb jf na b gy nk nl l nm nn">loss_stats = {<br/>    'train': [],<br/>    "val": []<br/>}</span></pre><p id="019a" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">开始训练吧。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="1980" class="nj mb jf na b gy nk nl l nm nn">print("Begin training.")</span><span id="1956" class="nj mb jf na b gy no nl l nm nn">for e in tqdm(range(1, EPOCHS+1)):<br/>    <br/>    # TRAINING<br/>    train_epoch_loss = 0</span><span id="4356" class="nj mb jf na b gy no nl l nm nn">model.train()<br/>    for X_train_batch, y_train_batch in train_loader:<br/>        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)<br/>        optimizer.zero_grad()<br/>        <br/>        y_train_pred = model(X_train_batch)<br/>        <br/>        train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))<br/>        <br/>        train_loss.backward()<br/>        optimizer.step()<br/>        <br/>        train_epoch_loss += train_loss.item()<br/>        <br/>        <br/>    # VALIDATION    <br/>    with torch.no_grad():<br/>        <br/>        val_epoch_loss = 0<br/>        <br/>        model.eval()<br/>        for X_val_batch, y_val_batch in val_loader:<br/>            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)<br/>            <br/>            y_val_pred = model(X_val_batch)<br/>                        <br/>            val_loss = criterion(y_val_pred, y_val_batch.unsqueeze(1))<br/>            <br/>            val_epoch_loss += val_loss.item()</span><span id="6362" class="nj mb jf na b gy no nl l nm nn">loss_stats['train'].append(train_epoch_loss/len(train_loader))<br/>    loss_stats['val'].append(val_epoch_loss/len(val_loader))                              <br/>    <br/>    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f}')</span><span id="f274" class="nj mb jf na b gy no nl l nm nn">###################### OUTPUT ######################</span><span id="c3b7" class="nj mb jf na b gy no nl l nm nn">Epoch 001: | Train Loss: 31.22514 | Val Loss: 30.50931</span><span id="40ed" class="nj mb jf na b gy no nl l nm nn">Epoch 002: | Train Loss: 30.02529 | Val Loss: 28.97327</span><span id="261f" class="nj mb jf na b gy no nl l nm nn">.<br/>.<br/>.</span><span id="f0d5" class="nj mb jf na b gy no nl l nm nn">Epoch 149: | Train Loss: 0.42277 | Val Loss: 0.37748<br/>Epoch 150: | Train Loss: 0.42012 | Val Loss: 0.37028</span></pre><p id="4086" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">你可以看到我们在循环之前放了一个<code class="fe mx my mz na b">model.train()</code>。告诉PyTorch你正处于训练模式。</p><p id="492b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为什么我们需要这么做？如果使用在训练和评估期间表现不同的层，如<code class="fe mx my mz na b">Dropout</code>或<code class="fe mx my mz na b">BatchNorm</code>(例如<em class="ob">；评估期间不使用dropout</em>)，您需要告诉PyTorch采取相应的行动。</p><p id="2470" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">同样，当我们测试我们的模型时，我们将调用<code class="fe mx my mz na b">model.eval()</code>。我们将在下面看到。</p><p id="133f" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">回到训练；我们开始一个<em class="ob">循环</em>。在这个<em class="ob"> for循环</em>的顶部，我们将每个时期的损失初始化为0。在每个时期之后，我们将打印出损失并将其重置回0。</p><p id="a51b" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然后我们有另一个<em class="ob"> for循环</em>。这个<em class="ob"> for-loop </em>用于从<code class="fe mx my mz na b">train_loader</code>批量获取我们的数据。</p><p id="6901" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在我们做任何预测之前，我们做<code class="fe mx my mz na b">optimizer.zero_grad()</code>。由于<code class="fe mx my mz na b">backward()</code>函数累加梯度，我们需要为每个小批量手动将其设置为0。</p><p id="fd40" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">从我们定义的模型中，我们获得一个预测，得到小批量的损失(和精度)，使用<code class="fe mx my mz na b">loss.backward()</code>和<code class="fe mx my mz na b">optimizer.step()</code>执行反向传播。</p><p id="62fd" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最后，我们将所有小批量损失相加，以获得该时期的平均损失。我们将每个小批量的所有损失相加，最后除以小批量的数量，即。<code class="fe mx my mz na b">train_loader</code>的长度，以获得每个历元的平均损失。</p><p id="fc38" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们遵循的训练程序与验证程序完全相同，除了我们用<code class="fe mx my mz na b">torch.no_grad</code>将它包装起来，并且不执行任何反向传播。<code class="fe mx my mz na b">torch.no_grad()</code>告诉PyTorch我们不想执行反向传播，这样可以减少内存使用并加快计算速度。</p><h1 id="56f8" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">可视化损失和准确性</h1><p id="58ed" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">为了绘制损失线图，我们再次从“loss_stats”字典中创建一个数据帧。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="7fe4" class="nj mb jf na b gy nk nl l nm nn">train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})</span><span id="1599" class="nj mb jf na b gy no nl l nm nn">plt.figure(figsize=(15,8))</span><span id="9c0a" class="nj mb jf na b gy no nl l nm nn">sns.lineplot(data=train_val_loss_df, x = "epochs", y="value", hue="variable").set_title('Train-Val Loss/Epoch')</span></pre><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi os"><img src="../Images/586ae6922e725b1862af771a3aa3a913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7jkehh0mlmkUXSIw26Jn1g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">列车价值损失曲线[图片[6]]</p></figure><h1 id="c4eb" class="ma mb jf bd mc md me mf mg mh mi mj mk ku ml kv mm kx mn ky mo la mp lb mq mr bi translated">试验模型</h1><p id="bfea" class="pw-post-body-paragraph ld le jf lf b lg ms kp li lj mt ks ll lm mu lo lp lq mv ls lt lu mw lw lx ly ij bi translated">训练完成后，我们需要测试我们的模型进展如何。注意，在运行测试代码之前，我们已经使用了<code class="fe mx my mz na b">model.eval()</code>。为了告诉PyTorch我们不希望在推断过程中执行反向传播，我们使用了<code class="fe mx my mz na b">torch.no_grad()</code>，就像我们对上面的验证循环所做的那样。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="3e1c" class="nj mb jf na b gy nk nl l nm nn">y_pred_list = []</span><span id="9abc" class="nj mb jf na b gy no nl l nm nn">with torch.no_grad():<br/>    model.eval()<br/>    for X_batch, _ in test_loader:<br/>        X_batch = X_batch.to(device)<br/>        y_test_pred = model(X_batch)<br/>        y_pred_list.append(y_test_pred.cpu().numpy())</span><span id="07f4" class="nj mb jf na b gy no nl l nm nn">y_pred_list = [a.squeeze().tolist() for a in y_pred_list]</span></pre><p id="346e" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们检查MSE和R平方指标。</p><pre class="nb nc nd ne gt nf na ng nh aw ni bi"><span id="501f" class="nj mb jf na b gy nk nl l nm nn">mse = mean_squared_error(y_test, y_pred_list)<br/>r_square = r2_score(y_test, y_pred_list)</span><span id="28b4" class="nj mb jf na b gy no nl l nm nn">print("Mean Squared Error :",mse)<br/>print("R^2 :",r_square)</span><span id="7884" class="nj mb jf na b gy no nl l nm nn"><br/>###################### OUTPUT ######################</span><span id="673d" class="nj mb jf na b gy no nl l nm nn">Mean Squared Error : 0.40861496703609534<br/>R^2 : 0.36675687655886924</span></pre></div><div class="ab cl ot ou hu ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="ij ik il im in"><p id="0af5" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">感谢您的阅读。欢迎提出建议和建设性的批评。:)</p><p id="33db" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这篇博客是专栏“如何训练你的神经网络”的一部分。你可以在这里找到专栏<a class="ae lz" href="https://towardsdatascience.com/tagged/akshaj-wields-pytorch" rel="noopener" target="_blank">。</a></p><p id="3292" class="pw-post-body-paragraph ld le jf lf b lg lh kp li lj lk ks ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">你可以在<a class="ae lz" href="https://www.linkedin.com/in/akshajverma7/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lz" href="https://twitter.com/theairbend3r" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上找到我。如果你喜欢这个，看看我的其他<a class="ae lz" href="https://medium.com/@theairbend3r" rel="noopener">博客</a>。</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><a href="https://www.buymeacoffee.com/theairbend3r"><div class="gh gi pa"><img src="../Images/041a0c7464198414e6ce355f9235099e.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*SGCT6C60o4t58wRqeU2viQ.png"/></div></a></figure></div></div>    
</body>
</html>