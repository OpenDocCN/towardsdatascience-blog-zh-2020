<html>
<head>
<title>EM Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EM 算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/em-algorithm-aaaa181af127?source=collection_archive---------17-----------------------#2020-07-19">https://towardsdatascience.com/em-algorithm-aaaa181af127?source=collection_archive---------17-----------------------#2020-07-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d8ce" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">数学背景和例子</h2></div><h1 id="4dc7" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">具有潜在变量的模型</h1><p id="98aa" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">利用基于机器学习的随机方法，我们将信息来源视为一种概率分布。一旦我们估计了分布，对未知数据进行分类以及预测未来生成的数据就变得简单了。估计概率分布有两个阶段。<br/> 1)决定定义分布的模型，例如，概率密度函数的形式(高斯分布、多项式分布……)。这里，考虑高斯混合模型(GMM)作为例子。概率密度函数的形式可以定义为</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/1095a5001c4d791353754428a3ea1aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*4i5CGHIOYV-pJJl5OayBqw.png"/></div></figure><p id="9ac0" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">其中 w_k 是从第 k 个高斯分布产生的比率数据。</p><p id="e62f" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">2)在确定概率密度函数的形式后，我们从观测数据中估计其参数。例如，在高斯分布的情况下，均值和方差是要估计的参数。</p><p id="a063" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">我们目前已知的知识是观测数据集 D 和生成分布的形式(未知参数高斯分布)。然而，为了求解 2 ),我们需要关于每个观测数据是从哪个高斯分布生成的信息，并且该信息不直接显示在观测数据中。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/89491e8e587804d086f9fb1307f9cd49.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*DOoSnZustSkF0RuvCy-ieA.png"/></div></figure><p id="b5e6" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">在这种情况下，代表不能从观测数据集中直接获得的信息的变量被称为<strong class="lc iu">潜变量</strong>。在上面的例子中，w_k 是一个潜变量。</p><p id="605f" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">因此，在 GMM，有必要先估计潜在变量。接下来，我们使用估计的潜在变量来估计每个高斯分布的参数。</p><p id="e79a" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">在具有潜在变量的模型中估计参数的一种有效方法是<strong class="lc iu">期望</strong> <strong class="lc iu">和最大化算法</strong> ( <strong class="lc iu"> EM 算法</strong>)。</p><h1 id="7856" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">算法的推导</h1><p id="bbc4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们准备一下这部分用到的符号。</p><ul class=""><li id="37d9" class="mk ml it lc b ld me lg mf lj mm ln mn lr mo lv mp mq mr ms bi translated"><strong class="lc iu"> D </strong> ={ <strong class="lc iu"> x </strong> _i | i=1，2，3，…，N}:随机变量的观测数据集<strong class="lc iu"> x : </strong>其中<strong class="lc iu"> x </strong> _i 为 D 维向量。</li><li id="5016" class="mk ml it lc b ld mt lg mu lj mv ln mw lr mx lv mp mq mr ms bi translated"><strong class="lc iu"> z </strong>:潜在变量。<strong class="lc iu"> z </strong> _i 对应<strong class="lc iu"> x </strong> _i</li><li id="86f7" class="mk ml it lc b ld mt lg mu lj mv ln mw lr mx lv mp mq mr ms bi translated"><strong class="lc iu">θ</strong>:待估计的一组参数</li></ul><p id="c503" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">我们的目的是用 EM 算法从观测数据集<strong class="lc iu"> D </strong>中估计<strong class="lc iu">θ</strong>。</p><p id="232f" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">在观测数据为独立同分布的情况下，对数似然函数为</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi my"><img src="../Images/9f4422af3c7c4b86fc278615bf44219e.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*Eobh1miy1VV7a3BlxC_EnQ.png"/></div></figure><p id="455a" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">我们可以用下面的形式重写我们的目的。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ba4051e96832cf4ef20fbc5382af618b.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*7GpQ0WPCPoieB6TH7yl2Qg.png"/></div></figure><p id="d01e" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">对数似然函数 p(x，z |θ)中所示的概率可以用潜在变量 z 的概率表示为以下形式。第三个关系是潜在变量<strong class="lc iu"> z </strong>边际分布的结果</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi na"><img src="../Images/f00c53f6f41cdbff84a1c03dd675e6c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*Cpul6JYps1P4UFS9hQeT4g.png"/></div></figure><p id="c799" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">现在，我们的目标是确定使对数似然函数 log p(x|theta)最大化的参数 theta。然而，不可能从上述关系中直接最大化该值。因此，我们决定在最大化 log p(x|theta)的同时更新参数 theta 的过程。我们认为θ是要定义的最佳参数，θ(t)是参数θ的第 t 步值。在下面的过程中，我们倾向于定义一个更新规则来增加 log p(x|theta(t))与 log p(x|theta)的比较。</p><p id="daf6" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">这里，给定最近的参数θ和观测数据，我们用条件概率表示 q(z)。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/a2bf376a6bbe4de0e9c2aaec86735604.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*gXN5PG68eyn0CIvLL2yZvg.png"/></div></figure><p id="64a9" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">因此，log p(x |θ)可以写成</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/c34360f5c54fd6d58f066f7cf12b9e09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ciURRrjyj7eIFvT9YN9OQQ.png"/></div></div></figure><p id="44e4" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">我们首先关注更新 theta(t)时 log p(x|theta)-log p(x|theta(t))的变化。我们试图定义导致 log p(x|theta)-log p(x|theta(t))数量减少的规则。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nh"><img src="../Images/837b21de3ba5b809efc479da481caae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZP5w0mKqXuw8HN60jCafyQ.png"/></div></div></figure><p id="d406" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">重写这个关系，我们得到下面的形式。等式(1):</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ni"><img src="../Images/a1a221e0feaec1356028f67d5892722c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SXaUEp97joSOdm39iNXJYg.png"/></div></div></figure><p id="f3b1" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">现在，我们需要评估右侧，以找到更新参数θ的规则。为此，考虑一个众所周知的数学关系<br/> log x ≤ x-1。利用这个关系，我们可以得到下面的不等式。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/7e103588c5409026820f1e26f13d8bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*IsbRQtI1z2FpkkegFECvCw.png"/></div></figure><p id="3886" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">因此，等式(1)的第三项是</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/ae6c703988b18e92869b53822c081f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*P2-wiraNZbHfF6YuQMryNg.png"/></div></figure><p id="2d7a" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">等式(1)的第一项和第二项是非负的。之所以成立是因为，<br/>当我们用 theta(t)代替 theta，term1-term2=0 那么通过最大化第一项，term1-term2 变得大于或等于 0。</p><p id="99f3" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">考虑这个关系，log p(x | theta)-log p(x | theta(t))≥0。因此，为了最大化等式(1)的左侧，我们只需用最大化等式(1)右侧第一项的θ(t)值来更新θ(t)。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/31d8284411b68acf46104007e62ad89b.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*2GkGc7tut_yDkOgzIsP1lQ.png"/></div></figure><p id="b23a" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">设上面更新规则的 argmax 的主语是函数 Q(theta)。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c9de2f93ecaf182b03ffc4eed0a92125.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*Fn-tgvrwfmWR5VvLeOTHnQ.png"/></div></figure><p id="8f33" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">我们可以把这个关系翻译成当θ=θ(t)时 log p(x，z |θ)的一个期望值。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/39ecb595886b7b77e21e54d0513bb5d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*o6FmfNfPnLAyJrjgL94TOA.png"/></div></figure><p id="f109" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">从这次更新中，我们可以将 EM 算法的过程总结为以下 E 步骤和 M 步骤。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e90cc79f3f8024c2e8565e8eb5506e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*7U-EtJgMsG5JF3gSwudCyw.png"/></div></figure><h1 id="45a7" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">高斯混合模型的 EM 算法</h1><p id="41d5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们以二维高斯混合模型为例。</p><p id="2dd0" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">随机变量:<strong class="lc iu"> x </strong> _n (d 维向量)<br/>潜变量:z_m <br/>混合比:w_k <br/>均值:<strong class="lc iu"> mu </strong> _k (d 维向量)<br/>方差-协方差矩阵:<strong class="lc iu">适马</strong> _k (dxd 矩阵)</p><p id="0a9a" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">这里，如果观测数据<strong class="lc iu"> x </strong>由第 m 个高斯分布生成，则 z_m = 1，否则 z_m = 0。因此，潜在变量 z 的分布可以写成</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi no"><img src="../Images/d8430ce6b1d8e5f07456303cb2da68b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*27YMgRqZUXpJ9-NvF8HpqQ.png"/></div></figure><p id="0675" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">第 m 个高斯分布的概率密度函数由下式给出</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3e1d60b4e2dbab859077a88686ab83b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*68tF83PTU3Ddy-lQXLrU_g.png"/></div></figure><p id="2cc6" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">因此，数据<strong class="lc iu"> x </strong>属于第 m 个分布的概率为 p(z_m=1| <strong class="lc iu"> x </strong>，其计算公式为</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/32e452dd514f7cb857abe300aa59f6f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*VrTGi7MS01o23KgcFyec0A.png"/></div></figure><h2 id="5973" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">初态</h2><p id="b7d4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">随机初始化<strong class="lc iu">穆</strong>、<strong class="lc iu">适马</strong>和<strong class="lc iu"> w </strong>。<br/> t = 1。</p><h2 id="419e" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">e 步骤</h2><p id="4b97" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">计算 p(z|x，theta(t)) </strong></p><p id="3ca2" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">用θ(t)计算潜变量 z 的期望值。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi od"><img src="../Images/cec6930abe539c5127ee6e382702c13c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*qA8e4QngiHb8-AOlpse5qg.png"/></div></figure><p id="244e" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">因此，若 z_nm 为 x_n 的潜变量，N_m 为第 m 个分布的观测数据个数，则以下关系成立。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/82abf0f6cc31436b2f7dadc03bf7632d.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*9Y87iqZ92C1yiVoJPj34EA.png"/></div></figure><h2 id="870d" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">m 步</h2><p id="210e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">更新参数</strong></p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi of"><img src="../Images/5f3a1497ac1de90f531b08157fb23626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*qj8och75AEeFSiObm2LS6g.png"/></div></figure><p id="0251" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">我们在这一步的目标是定义 w_m，μ_m，适马 _ m，使 Q(θ|θ(t))最大化。</p><h2 id="85e5" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">更新<strong class="ak">穆(意思是)</strong></h2><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi og"><img src="../Images/acf785520b1ce2902a2e6e3fc94f3f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*7zNzHbDJ-5LmHE8HCzqspw.png"/></div></figure><p id="215c" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">解此方程，μ的更新为</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/97a475607d19f071f5183d7385cbb925.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*g4Len-3gHxjrRnSLurhYAg.png"/></div></figure><h2 id="516f" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">更新适马(方差-协方差矩阵)</h2><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d94f19ddd4f219b717e8cd7e0183cfe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*FZWEkv4SHHXyYstcvgYJww.png"/></div></figure><p id="3b1c" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">解完这个方程，适马的更新就是</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b0229c24a522efe4c60cf59a1da91fc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*DImVk13H1sggT2WqE8MVgw.png"/></div></figure><h2 id="de08" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">更新 w(混合比)</h2><p id="e09e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了推导 w 的更新关系，我们使用拉格朗日方法使 Q(theta|theta(t))最大化，使 w_1+w_2+…+w_M=1。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ok"><img src="../Images/91c8d87e9adb2d541de88393aec7adaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*OAWnrnlJlYaNakm3B2Cw2Q.png"/></div></div></figure><p id="1603" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">求解λ的这个方程并使用约束关系，w_m 的更新规则是</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/355873be216df09c98a979ec4ea2e844.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*zpJBM42V4q-3oveS3G1Hng.png"/></div></figure><p id="81aa" class="pw-post-body-paragraph la lb it lc b ld me ju lf lg mf jx li lj mg ll lm ln mh lp lq lr mi lt lu lv im bi translated">所有参数更新:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b7abb10abdbfbf7cf6a473ef73e35dbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*Z5SS-1IFD6nGYAqYfRmcrA.png"/></div></figure><h1 id="d848" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">用 Python 实现</h1><h2 id="28a6" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">人工数据生成</h2><pre class="lx ly lz ma gt on oo op oq aw or bi"><span id="6805" class="nr kj it oo b gy os ot l ou ov">n1=150; n2=250; n3=300<br/>n = n1+n2+n3<br/>x = []<br/>x.append(np.dot(np.random.randn(n1,2),np.sqrt(np.diag([1,4]))))<br/>x.append(np.random.randn(n2,2)+np.dot(np.asarray([[1.]*n2]).T,np.asarray([[8.0,4.0]])))<br/>x.append(np.dot(np.random.randn(n3,2),np.sqrt(np.asarray([[2.45,1],[1,4]])))+16.0)<br/>X = np.concatenate((x[0],x[1],x[2]),axis=0)</span></pre><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/89491e8e587804d086f9fb1307f9cd49.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*DOoSnZustSkF0RuvCy-ieA.png"/></div></figure><h2 id="ac04" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">初始化步骤</h2><pre class="lx ly lz ma gt on oo op oq aw or bi"><span id="c82e" class="nr kj it oo b gy os ot l ou ov">def genSigma():<br/>    A = np.random.rand(2,2)+np.eye(2,2)<br/>    #A = np.eye(2,2)*np.random.rand(1)<br/>    S = np.dot(A.T,A)<br/>    #S = np.diag(np.diag(S))<br/>    return S</span><span id="e839" class="nr kj it oo b gy ow ot l ou ov">def genMu(m):<br/>    mu = np.zeros((2,m))<br/>    mu[0] = np.linspace(np.min(X,axis=0)[0],np.max(X,axis=0)[0],m)<br/>    mu[1] = np.linspace(np.min(X,axis=0)[1],np.max(X,axis=0)[1],m)<br/>    return mu.T</span><span id="2c1c" class="nr kj it oo b gy ow ot l ou ov">Sigma = np.zeros((m,2,2))<br/>    for i in range(m):<br/>        Sigma[i] = (genSigma())</span><span id="1ded" class="nr kj it oo b gy ow ot l ou ov">Mu = genMu(m)    <br/>    W = np.asarray([1.]*m)<br/>    W = W/sum(W)<br/>    L = -float('inf')</span><span id="44a7" class="nr kj it oo b gy ow ot l ou ov">iteration = 0</span></pre><h2 id="4e44" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">e 步和 M 步</h2><pre class="lx ly lz ma gt on oo op oq aw or bi"><span id="f2ff" class="nr kj it oo b gy os ot l ou ov">while True:<br/>        if iteration &gt;=100:<br/>            break<br/>        iteration+=1</span><span id="fe66" class="nr kj it oo b gy ow ot l ou ov"># <strong class="oo iu">E step</strong></span><span id="a2d9" class="nr kj it oo b gy ow ot l ou ov">        wN = np.zeros((n,m))<br/>        for j in range(m):<br/>            for i in range(n):<br/>                wN[i,j]=W[j]*gauss(X[i],Sigma[j],Mu[j])</span><span id="5d6c" class="nr kj it oo b gy ow ot l ou ov">sum_wN = np.sum(wN,axis=1)<br/>        Lnew = np.sum(np.log(sum_wN))<br/>        if (Lnew-L) &lt; 0.00001:<br/>            print("Model Converged")<br/>            break<br/>        L = Lnew</span><span id="087f" class="nr kj it oo b gy ow ot l ou ov">print("iter#",iteration," Lnew:",Lnew)<br/>        iters.append(iteration)<br/>        Log.append(Lnew)</span><span id="8ffb" class="nr kj it oo b gy ow ot l ou ov"># <strong class="oo iu">M step</strong></span><span id="64f7" class="nr kj it oo b gy ow ot l ou ov">eta = np.zeros((n,m))<br/>        for i in range(n):<br/>            eta[i,:]=wN[i]/sum_wN[i]</span><span id="1a47" class="nr kj it oo b gy ow ot l ou ov">sum_eta = np.sum(eta,axis=0)<br/>        W = sum_eta/n</span><span id="b03a" class="nr kj it oo b gy ow ot l ou ov">for j in range(m):<br/>            tmp = np.zeros((2,))<br/>            for i in range(n):<br/>                tmp += eta[i,j]*X[i]<br/>            Mu[j]=tmp/sum_eta[j]</span><span id="001e" class="nr kj it oo b gy ow ot l ou ov">for j in range(m):<br/>            tmp = np.zeros((2,2))<br/>            for i in range(n):<br/>                xmu = (X[i]-Mu[j]).reshape(2,1)<br/>                tmp1 = np.dot(xmu,xmu.T)*eta[i][j]<br/>                tmp += tmp1<br/>            Sigma[j]=tmp/sum_eta[j]</span></pre><h2 id="e51b" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">其他功能</h2><pre class="lx ly lz ma gt on oo op oq aw or bi"><span id="6a7e" class="nr kj it oo b gy os ot l ou ov">import numpy as np<br/>import random<br/>from scipy.interpolate import griddata<br/>from scipy.stats import multivariate_normal<br/>import numpy.ma as ma<br/>from numpy.random import uniform, seed<br/>from matplotlib import cm<br/>from scipy.special import softmax<br/>import matplotlib.pyplot as plt</span><span id="daaa" class="nr kj it oo b gy ow ot l ou ov"># Gaussian pdf <br/>def gauss(x,sigma,mu):<br/>    n = multivariate_normal(mu,sigma)<br/>    return n.pdf(x)</span><span id="5a33" class="nr kj it oo b gy ow ot l ou ov"># Visualize<br/>def plot_countour(sigma,mu):<br/>    xy_range = np.zeros((2,2))<br/>    xy_range[0] = np.linspace(mu[0]-2*sigma[0,0],mu[0]+2*sigma[0,0],2)<br/>    xy_range[1] = np.linspace(mu[1]-2*sigma[1,1],mu[1]+2*sigma[1,1],2)<br/>    x, y = np.mgrid[xy_range[0][0]:xy_range[0][1]:.01, xy_range[1][0]:xy_range[1][1]:.01]<br/>    pos = np.dstack((x, y))<br/>    rv = multivariate_normal(mu, sigma)<br/>    z = rv.pdf(pos)<br/>    levels = np.linspace(np.min(z),np.max(z),5)<br/>    plt.contour(x, y, z,linewidths=1.0,colors='black',levels=levels)<br/>    #plt.contourf(x,y,z,15,cmap=plt.cm.jet)<br/>    plt.xlim([-5,25])</span></pre><h2 id="d2fc" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">结果(1) M=2</h2><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/3c63cb4a20205699aa1236f57250eacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*V6L3imVU5EOOmYC0FhVXQQ.png"/></div></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/8ae5fc57c73767e6eb2686e26c207d09.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*h_5Dvbi6wzneQ3Tz1ATQMA.png"/></div></figure><h2 id="081f" class="nr kj it bd kk ns nt dn ko nu nv dp ks lj nw nx ku ln ny nz kw lr oa ob ky oc bi translated">结果(2) M=3</h2><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/c328b873e17f92a1b9c9be408dae8beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*-4RGlLItPIy61-tI9XT-5A.png"/></div></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/43c2e93e934c5ee122f06b239e8f7ca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*OqDmQD0l5s-HqnFPIdMwBg.png"/></div></figure><h1 id="4a4a" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">讨论</h1><p id="dd70" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">从结果来看，使用 EM 算法，对数似然函数在重复更新参数规则后总是收敛的。然而，由于 EM 算法是一种迭代计算，它容易陷入局部最优状态。正如在结果(1)、(2)中看到的，M 值(混合模型的数量)和初始化的差异提供了对数似然收敛和估计分布的不同变化。为了解决这个问题，一个简单的方法是用几个初始化状态重复该算法，并从那些作品中选择最佳状态。</p></div></div>    
</body>
</html>