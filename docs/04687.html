<html>
<head>
<title>Intuitively, How Can We (Better) Understand Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">凭直觉，我们如何(更好地)理解逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuitively-how-can-we-better-understand-logistic-regression-97af9e77e136?source=collection_archive---------38-----------------------#2020-04-25">https://towardsdatascience.com/intuitively-how-can-we-better-understand-logistic-regression-97af9e77e136?source=collection_archive---------38-----------------------#2020-04-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8fde" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">逻辑回归和线性判别分析密切相关。这里有一个直观的方法来理解它们，并帮助我们定义Softmax回归</h2></div><p id="ebd3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我之前的文章中，我介绍了5个分类原则，它们帮助我们定义了5种以上的算法。</p><p id="2af8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们用于逻辑回归的直觉是“平滑直线”。平滑函数是一个<strong class="kk iu">逻辑函数</strong>。现在，我们如何更好地理解我们是如何想出这个逻辑函数的呢？</p><p id="31fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">和上一篇文章一样，我们将用蓝点和红点来解释1D情况的原理。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/0d7f6b9290ab6d11e1593b090b6181ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x8Ug-SvYJT_hbQcCdXFsHg.png"/></div></div></figure><h1 id="dff8" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">LDA和逻辑回归是如何关联的</h1><p id="a6db" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">为了解释LDA(线性判别分析)，思路是先建立两个<strong class="kk iu">正态分布</strong>。对于新的点x，我们可以考虑:</p><ul class=""><li id="3ef9" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated"><strong class="kk iu"> PDF_b(x) </strong>其中PDF_b:为<strong class="kk iu">蓝点</strong>的概率密度函数</li><li id="7a9b" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu"> PDF_r(x) </strong>带PDF _ r:<strong class="kk iu">红点</strong>的概率密度函数</li><li id="c86c" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu"> p(B) </strong>:蓝点比例</li><li id="e9d4" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><strong class="kk iu"> p(R) </strong>:红点比例</li></ul><p id="54f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">新点为蓝色的最终概率为:</p><p id="bad1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">p(B)×PDF _ B(x)/(p(B)×PDF _ B(x)+p(R)×PDF _ R(x))</p><p id="d7ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们看看普通的PDF:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/ee174345d74bd34138f92c3d638ed32e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*mJXTCYg0aJp7jH_uzT8L0Q@2x.png"/></div></figure><p id="03be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于我们考虑到在LDA的情况下，两个类别的标准偏差是相同的，因此我们可以简化，x项将消失，这就是为什么我们称之为<strong class="kk iu">线性</strong>判别分析。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nd"><img src="../Images/b1159ccc2580d066edc59e3391e2d382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fXeDbNjBFI1Ms1gYJl4jsg@2x.png"/></div></div></figure><p id="8f69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们不采用同标准差的假设(这意味着两个类别具有相同的标准差)，x项将保持不变，该算法称为<strong class="kk iu">二次</strong>判别分析。</p><p id="2f6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以对LDA来说，我们最终会得到这样的结果:</p><p id="2504" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1/(1+exp(ax+b))</p><p id="9d2f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是的，一个逻辑函数！</p><p id="4459" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当然，参数a和b与实际的logistic回归是不同的。</p><p id="36c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以比较结果，在这种情况下，我们可以看到结果实际上非常接近(绿色曲线是logistic回归，黑色曲线是LDA)。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/8705dfedd53f4154dcbb945fb8c61790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCcVXZ7pWv3tj-Gm6NZhOA.png"/></div></div></figure><p id="bdf9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结论:LDA和逻辑回归产生的最终概率是逻辑函数。这两种方法之间的唯一区别在于</p><ul class=""><li id="da25" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated">逻辑回归使用<em class="ne">最大似然</em>来估计参数</li><li id="aadf" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">LDA，参数来自一个<em class="ne">正态分布</em>的估计均值和方差以及每类的比例(先验概率)</li></ul><h1 id="55a3" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">简化普通PDF</h1><p id="b227" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">既然知道正态PDF中的x会在同方差的假设下不了了之，也许一开始就可以直接去掉。</p><p id="c372" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以我们可以直接考虑:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/6f02d92d70d1afb7e009adaf01fd08bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*mP3OzYFhD7cHitBaHyyzxg@2x.png"/></div></figure><p id="127a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以测试一些参数，以便绘制曲线。让我们从蓝色曲线fb(x)开始:</p><ul class=""><li id="d991" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated">首先，我们可以让a_b=1(蓝点的参数a)</li><li id="d9d2" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">对于b_b，我们可以说曲线应该通过点(x =蓝点的平均值，y=1)</li></ul><p id="6dcd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们认为红色曲线的情况是对称的:</p><ul class=""><li id="9893" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated">a_r =-1</li><li id="f35a" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">红色曲线应该通过该点(x =红点的平均值，y=1)</li></ul><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/42b0af7ef8ef919f64a99d2ee3eca251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_0idcIDFJo1WwKaNXympWg.png"/></div></div></figure><h1 id="1130" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">比率的计算</h1><p id="4bf3" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">当我们计算比率得到最终概率时</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/30d4a5309f2dbb738054134ce0a7eaa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*jDilljHdRz3f2BZvhZAJbg@2x.png"/></div></figure><p id="6125" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们最后还会有一个逻辑功能。</p><p id="b421" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面我们可以看到比例(黑线)。记住这里的参数a和b是手动选择的。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/2f23d73fb329b7752b33f645d16a8bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JfxTVSWMhgsSegLvTZJOyA.png"/></div></div></figure><p id="8faa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但即使如此，我们可以看到，在给定的情况下，它实际上并没有那么糟糕。下图中的绿线是逻辑回归模型，而黑线是用我们手动选择的参数计算的比率。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/a4cae54bac393420b5aba04376221db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oD1K9-Y7ZOPYIgfbGv-Fpg.png"/></div></div></figure><p id="36a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结论:logistic回归是一个归一化的指数函数(由两类定义)。</p><h1 id="bf61" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">Softmax回归</h1><p id="8991" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">凭着“平滑直线”的直觉，对于多个预测类的情况，不容易一概而论。但是有了归一化指数函数的思想，我们可以增加更多的类。</p><p id="11c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<strong class="kk iu"> K类</strong>，我们可以考虑这个归一化的指数函数来估计x属于<strong class="kk iu">类j </strong>的概率</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/8157aff959f2077020d794d35e8b96cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*yVenELoWGc7xOJs-3axcdA@2x.png"/></div></figure><p id="c845" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个包含3个类别的图表</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/a74493352b9662c79fa77314cba61e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gEwuoJ1MBo8pR8Wq3iDTKA.png"/></div></div></figure><p id="99c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这被称为<strong class="kk iu"> softmax回归</strong>，现在你知道在这个花哨的名字背后，它只是逻辑回归的一个非常简单的概括。</p><p id="079c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为您知道逻辑回归非常接近LDA，所以softmax回归的结果应该接近<strong class="kk iu">多类LDA </strong>，如下图所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/ec943fbff0cf86565a6fd9c2159669ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QXEyaz3CcTXKQOaXyksTtw.png"/></div></div></figure></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="5ce4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你发现一些不够直观的东西，或者如果你有任何问题，请评论，这将有助于我提高我的写作。</p></div></div>    
</body>
</html>