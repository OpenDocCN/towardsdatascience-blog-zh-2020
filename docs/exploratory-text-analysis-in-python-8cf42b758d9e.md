# Python ä¸­çš„æ¢ç´¢æ€§æ–‡æœ¬åˆ†æ

> åŸæ–‡ï¼š<https://towardsdatascience.com/exploratory-text-analysis-in-python-8cf42b758d9e?source=collection_archive---------22----------------------->

## *å»ºç«‹æƒ…æ„Ÿåˆ†ç±»å™¨çš„ä¸€æ­¥*

ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨å»ºç«‹æ¨¡å‹ä¹‹å‰è¦åšæ¢ç´¢æ€§çš„æ•°æ®åˆ†æï¼Ÿæˆ‘ä¼šè¯´â€œ*ä¸ºäº†æ›´å¥½åœ°ç†è§£æ•°æ®ï¼Œä»¥ä¾¿æˆ‘ä»¬ä»¥åˆé€‚çš„æ–¹å¼é¢„å¤„ç†æ•°æ®ï¼Œå¹¶é€‰æ‹©åˆé€‚çš„å»ºæ¨¡æŠ€æœ¯â€*ã€‚è¿™ç§ç†è§£æ•°æ®çš„å¿…è¦æ€§åœ¨å¤„ç†æ–‡æœ¬æ•°æ®æ—¶ä»ç„¶é€‚ç”¨ã€‚è¿™ç¯‡æ–‡ç« æ˜¯æ„å»ºæƒ…æ„Ÿåˆ†ç±»å™¨çš„ä¸‰ç¯‡ç³»åˆ—æ–‡ç« ä¸­çš„ç¬¬ä¸€ç¯‡ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹ä¸€ç§å¯¹æ–‡æœ¬è¿›è¡Œ*æ¢ç´¢æ€§æ•°æ®åˆ†æçš„æ–¹æ³•ï¼Œæˆ–è€…ä¸ºäº†ç®€æ´èµ·è§ï¼Œå°†*æ¢ç´¢æ€§æ–‡æœ¬åˆ†æ*ã€‚*

![](img/d64f98b063aac1ca2a61c3f7752d9bb4.png)

ç…§ç‰‡ç”±[å®‰å¾·é²Â·å°¼å°”](https://unsplash.com/@andrewtneel?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„

åœ¨æˆ‘ä»¬æ·±å…¥ç ”ç©¶ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆé€€ä¸€æ­¥ï¼Œçœ‹çœ‹æ›´å¤§çš„å›¾æ™¯ã€‚CRISP-DM æ–¹æ³•æ¦‚è¿°äº†æˆåŠŸçš„æ•°æ®ç§‘å­¦é¡¹ç›®çš„æµç¨‹ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†æ•°æ®ç§‘å­¦é¡¹ç›®çš„ç¬¬ 2-4 é˜¶æ®µã€‚åœ¨**æ•°æ®ç†è§£**é˜¶æ®µï¼Œæ¢ç´¢æ€§æ•°æ®åˆ†ææ˜¯å…³é”®ä»»åŠ¡ä¹‹ä¸€ã€‚

![](img/eb0c82b29b5f0dfa8ec8c5f3758ff76d.png)

CRISP-DM å·¥è‰ºæµç¨‹æ‘˜å½•

åœ¨ä»äº‹æ•°æ®ç§‘å­¦é¡¹ç›®æ—¶ï¼Œåœ¨ä¸åŒé˜¶æ®µä¹‹é—´æ¥å›åˆ‡æ¢è€Œä¸æ˜¯çº¿æ€§å‰è¿›å¹¶ä¸ç½•è§ã€‚è¿™æ˜¯å› ä¸ºæƒ³æ³•å’Œé—®é¢˜ä¼šåœ¨éšåçš„é˜¶æ®µå‡ºç°ï¼Œä½ æƒ³å›åˆ°ä¸€ä¸¤ä¸ªé˜¶æ®µå»å°è¯•è¿™ä¸ªæƒ³æ³•æˆ–æ‰¾åˆ°é—®é¢˜çš„ç­”æ¡ˆã€‚å®˜æ–¹çš„ CRISP-DM ä¸­æ²¡æœ‰ç²‰è‰²ç®­å¤´ï¼Œä½†æ˜¯ï¼Œæˆ‘è®¤ä¸ºè¿™äº›ç»å¸¸æ˜¯å¿…è¦çš„ã€‚äº‹å®ä¸Šï¼Œä¸ºäº†æ¢ç´¢æ€§çš„æ–‡æœ¬åˆ†æï¼Œæˆ‘ä»¬å°†åœ¨è¿™ç¯‡æ–‡ç« ä¸­åšä¸€äº›æ•°æ®å‡†å¤‡ã€‚å¯¹äºé‚£äº›æœ‰å…´è¶£äº†è§£ CRISP-DM æ›´å¤šä¿¡æ¯çš„äººæ¥è¯´ï¼Œ[è¿™ä¸ª](https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome)æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç®€çŸ­ä»‹ç»ï¼Œ[è¿™ä¸ªèµ„æº](https://www.sv-europe.com/crisp-dm-methodology/)æä¾›äº†æ›´è¯¦ç»†çš„è§£é‡Šã€‚

# 0.Python è®¾ç½®ğŸ”§

è¿™ç¯‡æ–‡ç« å‡è®¾è¯»è€…(ğŸ‘€æ˜¯çš„ï¼Œä½ ï¼)å¯ä»¥è®¿é—®å¹¶ç†Ÿæ‚‰ Pythonï¼ŒåŒ…æ‹¬å®‰è£…åŒ…ã€å®šä¹‰å‡½æ•°å’Œå…¶ä»–åŸºæœ¬ä»»åŠ¡ã€‚å¦‚æœä½ æ˜¯ Python çš„æ–°æ‰‹ï¼Œ[è¿™ä¸ª](https://www.python.org/about/gettingstarted/)æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚

æˆ‘åœ¨ Jupyter ç¬”è®°æœ¬é‡Œæµ‹è¯•è¿‡ Python 3.7.1 çš„è„šæœ¬ã€‚

è®©æˆ‘ä»¬åœ¨å¼€å§‹ä¹‹å‰ç¡®ä¿æ‚¨å·²ç»å®‰è£…äº†ä»¥ä¸‹åº“:
â—¼ï¸ **æ•°æ®æ“ä½œ/åˆ†æ:** *numpyï¼Œpandas* â—¼ï¸ **æ•°æ®åˆ†åŒº:** *sklearn* â—¼ï¸ **æ–‡æœ¬é¢„å¤„ç†/åˆ†æ:** nltkâ—¼ï¸ **å¯è§†åŒ–:** *matplotlibï¼Œseaborn*

ä¸€æ—¦ä½ å®‰è£…äº† *nltk* ï¼Œè¯·ç¡®ä¿ä½ å·²ç»ä» *nltk* ä¸‹è½½äº†*ã€punktã€‘**ã€åœç”¨è¯ã€‘*å’Œ*ã€wordnetã€‘*ï¼Œè„šæœ¬å¦‚ä¸‹:

```
import nltk
nltk.download('punkt') # for sent_tokenize
nltk.download('stopwords') 
nltk.download('wordnet') # for WordNetLemmatizer
```

å¦‚æœä½ å·²ç»ä¸‹è½½äº†ï¼Œè¿è¡Œè¿™ä¸ªä¼šé€šçŸ¥ä½ ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å‡†å¤‡å¯¼å…¥æ‰€æœ‰çš„åŒ…:

```
# Setting random seed
seed = 123# Data manipulation/analysis
import numpy as np
import pandas as pd# Data partitioning
from sklearn.model_selection import train_test_split# Text preprocessing/analysis
import re
from nltk import word_tokenize, sent_tokenize, FreqDist
from nltk.util import ngrams
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer# Visualisation
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid", context='talk', 
        palette=['#D44D5C', '#43AA8B'])
```

*å¤–å–æˆ–ç¬”è®°å°†é™„å¸¦*ğŸ€*æ¢ç´¢æ—¶*ã€‚

# 1.æ•°æ®ğŸ“¦

æˆ‘ä»¬å°†ä½¿ç”¨ IMDB ç”µå½±è¯„è®ºæ•°æ®é›†ã€‚æ‚¨å¯ä»¥åœ¨è¿™é‡Œä¸‹è½½æ•°æ®é›†[ï¼Œå¹¶å°†å…¶ä¿å­˜åœ¨æ‚¨çš„å·¥ä½œç›®å½•ä¸­ã€‚ä¿å­˜åï¼Œè®©æˆ‘ä»¬å°†å…¶å¯¼å…¥ Python:](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)

```
sample = pd.read_csv('IMDB Dataset.csv')
print(f"{sample.shape[0]} rows and {sample.shape[1]} columns")
sample.head()
```

![](img/4cd7b0ca77e89ffb684e7d846279c031.png)

ä»æŸ¥çœ‹æ•°æ®çš„å¤´éƒ¨ï¼Œæˆ‘ä»¬å¯ä»¥ç«‹å³çœ‹åˆ°ç¬¬äºŒæ¡è®°å½•ä¸­æœ‰ html æ ‡è®°ã€‚

ğŸ€è¿›ä¸€æ­¥æ£€æŸ¥ html æ ‡ç­¾ï¼Œçœ‹çœ‹å®ƒä»¬æœ‰å¤šæ™®éã€‚

è®©æˆ‘ä»¬æ¥çœ‹çœ‹æƒ…ç»ªä¹‹é—´çš„åˆ†æ­§:

```
sample['sentiment'].value_counts()
```

![](img/5e5c06fc380f7ac8acfe9701d65dedac.png)

åœ¨æ ·æœ¬æ•°æ®ä¸­ï¼Œæƒ…æ„Ÿæ˜¯å¹³å‡åˆ†é…çš„ã€‚åœ¨å¼€å§‹æ¢ç´¢æ€§æ–‡æœ¬åˆ†æä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæŠŠæ•°æ®åˆ†æˆä¸¤ç»„:*è®­ç»ƒ*å’Œ*æµ‹è¯•*ã€‚æˆ‘ä»¬å°†ç•™å‡º 5000 ç®±è¿›è¡Œæµ‹è¯•:

```
# Split data into train & test
X_train, X_test, y_train, y_test = train_test_split(sample['review'], sample['sentiment'], test_size=5000, random_state=seed, 
                                                    stratify=sample['sentiment'])# Append sentiment back using indices
train = pd.concat([X_train, y_train], axis=1)
test = pd.concat([X_test, y_test], axis=1)# Check dimensions
print(f"Train: {train.shape[0]} rows and {train.shape[1]} columns")
print(f"{train['sentiment'].value_counts()}\n")print(f"Test: {test.shape[0]} rows and {test.shape[1]} columns")
print(test['sentiment'].value_counts())
```

![](img/dd7a92b9923d4142d655cce22d3d6f01.png)

é€šè¿‡æŒ‡å®š`stratify`å‚æ•°ï¼Œæˆ‘ä»¬ç¡®ä¿äº†æƒ…æ„Ÿåœ¨ä¸¤ç»„ä¸­å¹³å‡åˆ†é…ã€‚

åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨*è®­ç»ƒ*è¿›è¡Œæ¢ç´¢æ€§æ–‡æœ¬åˆ†æã€‚ä¸€æ—¦æˆ‘ä»¬æ¢ç´¢äº†è®­ç»ƒæ•°æ®é›†ï¼Œå•ç‹¬æ£€æŸ¥*æµ‹è¯•*é›†çš„å…³é”®ç‰¹å¾å¯èƒ½æ˜¯æœ‰ç”¨çš„ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œè¿™ä¸¤ç»„éƒ½åº”è¯¥ä»£è¡¨æ½œåœ¨äººç¾¤ã€‚è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹è®­ç»ƒæ•°æ®é›†çš„å¤´éƒ¨:

```
train.head()
```

![](img/ff942e16a2a52ba3489fa71cb2246a08.png)

å¥½äº†ï¼Œæˆ‘ä»¬éƒ½å‡†å¤‡å¥½å»æ¢ç´¢äº†ï¼âœ¨

# **2ã€‚æ¢ç´¢æ€§æ–‡æœ¬åˆ†æğŸ”**

å¼•å¯¼æ¢ç´¢æ€§æ•°æ®åˆ†æçš„ä¸€ä¸ªæ–¹æ³•å°±æ˜¯å†™ä¸‹ä½ æ„Ÿå…´è¶£çš„é—®é¢˜ï¼Œç”¨æ•°æ®æ¥å›ç­”ã€‚æ‰¾åˆ°é—®é¢˜çš„ç­”æ¡ˆé€šå¸¸ä¼šå¼•å‡ºä½ å¯èƒ½æƒ³æ¢ç©¶çš„å…¶ä»–é—®é¢˜ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¯ä»¥å›ç­”çš„ä¸€äº›é—®é¢˜ç¤ºä¾‹:

ğŸ“‹ **2.1ã€‚çƒ­èº«:** *ä¸€å…±æœ‰å‡ æ ¹å¼¦ï¼Ÿ
*æœ€å¸¸è§çš„å­—ç¬¦ä¸²æœ‰å“ªäº›ï¼Ÿ
*æœ€çŸ­çš„å­—ç¬¦ä¸²æ˜¯ä»€ä¹ˆæ ·å­çš„ï¼Ÿ
*æœ€é•¿çš„å­—ç¬¦ä¸²æ˜¯ä»€ä¹ˆæ ·å­çš„ï¼Ÿ

ğŸ“‹ **2.2ã€‚ä»£å¸:**

> *ğŸ’¡* [ä»¤ç‰Œæ˜¯ä¸€ä¸ªå­—ç¬¦åºåˆ—ã€‚](https://www.nltk.org/book/ch01.html) [ä»¤ç‰Œé€šå¸¸è¢«ç¬¼ç»Ÿåœ°ç§°ä¸ºæ–‡å­—ã€‚](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)
> *ğŸ’¡* [è®°å·åŒ–æ˜¯å°†æ–‡æ¡£æ‹†åˆ†æˆè®°å·çš„è¿‡ç¨‹ï¼Œæœ‰æ—¶è¿˜ä¼šä¸¢å¼ƒæŸäº›å­—ç¬¦ï¼Œå¦‚æ ‡ç‚¹ç¬¦å·ã€‚](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)ç¤ºä¾‹:æ ‡è®°åŒ–å°†â€œè¿™éƒ¨ç”µå½±æ£’æäº†â€å˜æˆ 4 ä¸ªæ ‡è®°:[â€œè¿™éƒ¨ç”µå½±â€ã€â€œç”µå½±â€ã€â€œæ›¾ç»æ˜¯â€ã€â€œæ£’æäº†â€]

*æœ‰å¤šå°‘ä»£å¸ï¼Ÿ
*æœ‰å¤šå°‘ä¸ªå”¯ä¸€ä»¤ç‰Œï¼Ÿ
*æ¯ä¸ªä»¤ç‰Œçš„å¹³å‡å­—ç¬¦æ•°æ˜¯å¤šå°‘ï¼Ÿ

ğŸ“‹ **2.3ã€‚åœå­—:**

> ***ğŸ’¡*** åœç”¨è¯æ˜¯å¯¹æ–‡æœ¬æ„ä¹‰å‡ ä¹æ²¡æœ‰ä»·å€¼çš„å¸¸ç”¨è¯ã€‚ç¤ºä¾‹:å’Œã€‚

*æœ€å¸¸è§çš„åœç”¨è¯æ˜¯ä»€ä¹ˆï¼Ÿ
*è¿˜æœ‰å“ªäº›è¯ç»å¸¸å‡ºç°ï¼Œå¯ä»¥æ·»åŠ åˆ°åœç”¨è¯ä¸­ï¼Ÿ

ğŸ“‹ **2.4ã€‚å¸¸è§å•è¯ n-grams:**

> ***ğŸ’¡*** [å•è¯ n-grams æ˜¯æ–‡æœ¬æ•°æ®ä¸­ç›¸é‚» n ä¸ªå•è¯çš„æ‰€æœ‰ç»„åˆ](https://stackoverflow.com/questions/18193253/what-exactly-is-an-n-gram)ã€‚ä¾‹å¦‚:â€œè¿™éƒ¨ç”µå½±æ£’æäº†â€ä¸­çš„äºŒå…ƒç»“æ„æ˜¯[â€œè¿™éƒ¨ç”µå½±â€ã€â€œç”µå½±æ›¾ç»æ˜¯â€ã€â€œæ›¾ç»æ£’æäº†â€]

*æœ€å¸¸è§çš„ä»¤ç‰Œæ˜¯ä»€ä¹ˆï¼Ÿæœ€å¸¸è§çš„äºŒå…ƒæ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿæœ€å¸¸è§çš„ä¸‰å…ƒæ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿæœ€å¸¸è§çš„å››å­—æ ¼æ˜¯ä»€ä¹ˆï¼Ÿ

ğŸ“‹ **2.5ã€‚æ–‡ä»¶:**

> ***ğŸ’¡*** æ–‡æ¡£æ˜¯æ–‡æœ¬è®°å½•ã€‚ä¾‹å¦‚:æ¯ä¸ªç”µå½±è¯„è®ºéƒ½æ˜¯ä¸€ä¸ªæ–‡æ¡£ã€‚
> *ğŸ’¡*è¯­æ–™åº“æ˜¯æ–‡æ¡£çš„é›†åˆã€‚ç®€å•æ¥è¯´ï¼Œæ–‡æœ¬æ•°æ®å°±æ˜¯ä¸€ä¸ªè¯­æ–™åº“ã€‚ç¤ºä¾‹:æˆ‘ä»¬å¯ä»¥å°†è®­ç»ƒæ•°æ®ç§°ä¸ºè®­ç»ƒè¯­æ–™åº“ã€‚

*æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡å¥å­æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ
*æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡ä»¤ç‰Œæ•°æ˜¯å¤šå°‘ï¼Ÿ
*æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡å­—ç¬¦æ•°æ˜¯å¤šå°‘ï¼Ÿ
*æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡åœç”¨è¯æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ
*ç­”æ¡ˆå¦‚ä½•å› æƒ…ç»ªè€Œå¼‚ï¼Ÿ

ç°åœ¨ï¼Œæ˜¯æ—¶å€™æ‰¾åˆ°è¿™äº›é—®é¢˜çš„ç­”æ¡ˆäº†ï¼ğŸ˜ƒ

## ğŸ“‹ 2.1.W **æ‰‹è‡‚æŠ¬èµ·**

è®©æˆ‘ä»¬å°†æ‰€æœ‰çš„è¯„è®ºåˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç„¶ååœ¨ç©ºæ ¼å¤„å°†å…¶æ‹†åˆ†æˆå­å­—ç¬¦ä¸²(ä»¥ä¸‹ç®€ç§°ä¸ºå­—ç¬¦ä¸²)ã€‚è¿™ç¡®ä¿äº†å¯¹äºè¿™ç§é¢„çƒ­æ¢ç´¢ï¼Œè¯­æ–™åº“è¢«æœ€å°ç¨‹åº¦åœ°æ”¹å˜(ä¾‹å¦‚ï¼Œä¿æŒæ ‡ç‚¹ä¸å˜):

```
# Prepare training corpus into one giant string
train_string = " ".join(X_train.values)
print(f"***** Extract of train_string ***** \n{train_string[:101]}", "\n")# Split train_corpus by white space
splits = train_string.split()  
print(f"***** Extract of splits ***** \n{splits[:18]}\n")
```

![](img/7bf90154b7d470700fb80d5b071d42bf.png)

âœï¸ **2.1.1ã€‚æœ‰å¤šå°‘æ ¹å¼¦ï¼Ÿ**

```
print(f"Number of strings: {len(splits)}")
print(f"Number of unique strings: {len(set(splits))}")
```

![](img/87f19d349019ffc33e3929485cd5874e.png)

åœ¨è®­ç»ƒè¯­æ–™åº“ä¸­æœ‰è¶…è¿‡ 1000 ä¸‡ä¸ªå­—ç¬¦ä¸²ï¼Œå…¶ä¸­æœ‰å¤§çº¦ 41 ä¸‡ä¸ªç‹¬ç‰¹çš„å­—ç¬¦ä¸²ã€‚è¿™ç»™äº†æˆ‘ä»¬åˆæ­¥çš„å¤§æ¦‚æ•°å­—ã€‚åœ¨æˆ‘ä»¬æ­£ç¡®æ ‡è®°åï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è¿™äº›æ•°å­—æ˜¯å¦‚ä½•å¯»æ‰¾æ ‡è®°çš„ã€‚

âœï¸ã€‚æœ€å¸¸è§çš„å­—ç¬¦ä¸²æœ‰å“ªäº›ï¼Ÿ

è®©æˆ‘ä»¬ä¸ºæ¯ä¸ªå­—ç¬¦ä¸²å‡†å¤‡é¢‘ç‡åˆ†å¸ƒæ¥å›ç­”è¿™ä¸ªé—®é¢˜:

```
freq_splits = FreqDist(splits)
print(f"***** 10 most common strings ***** \n{freq_splits.most_common(10)}", "\n")
```

![](img/54b595632e63aff2b6734398713b3e3d.png)

çœ‹åˆ°æœ€å¸¸è§çš„å­—ç¬¦ä¸²æ˜¯åœç”¨è¯å¹¶ä¸å¥‡æ€ªã€‚æˆ‘ä»¬å°†åœ¨ç¬¬ 2.3 èŠ‚*ä¸­è¿›ä¸€æ­¥æ¢è®¨åœç”¨è¯ã€‚åœæ­¢è¨€è¯­*ã€‚

ğŸ€*åœ¨æŸ¥çœ‹æ™®é€šä»¤ç‰Œå’Œ n å…ƒè¯­æ³•ä¹‹å‰ï¼Œåˆ é™¤åœç”¨è¯ã€‚*

**âœï¸ã€‚æœ€çŸ­çš„å­—ç¬¦ä¸²æ˜¯ä»€ä¹ˆæ ·å­çš„ï¼Ÿ**

è®©æˆ‘ä»¬å°†çŸ­å­—ç¬¦ä¸²å®šä¹‰ä¸ºé•¿åº¦å°äº 4 ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸²ï¼Œå¹¶æ£€æŸ¥å®ƒä»¬çš„é¢‘ç‡:

```
short = set(s for s in splits if len(s)<4)
short = [(s, freq_splits[s]) for s in short]
short.sort(key=lambda x:x[1], reverse=True)
short
```

![](img/0849eea47688e8e2df335bcb1fbd5041.png)

*æ‘˜å½•è‡ª* ***çŸ­*** *ï¼Œæœªæ˜¾ç¤ºæ‰€æœ‰è¾“å‡º*

è®¸å¤šçŸ­å­—ç¬¦ä¸²ä¼¼ä¹æ˜¯åœç”¨è¯ï¼Œä½†ä¹Ÿæœ‰æ•°å­—å’Œå…¶ä»–çŸ­è¯ã€‚

ğŸ€*æœ‰ä¸åŒå½¢å¼çš„æ•°å­—:3ï¼Œ2ï¼Œ70ï¼Œ90% â€”æˆ‘ä»¬éœ€è¦å†³å®šæ˜¯æ”¾å¼ƒè¿˜æ˜¯ä¿ç•™å®ƒä»¬ã€‚*

ğŸ€*æœ‰å¤§å°å†™å˜åŒ–:â€œtheâ€ã€â€œTheâ€ã€â€œTheâ€-è¿™äº›éœ€è¦è§„èŒƒåŒ–ã€‚*

å› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰æ ‡è®°åŒ–ï¼Œä¸€äº›å­—ç¬¦ä¸²ç›®å‰åŒ…å«é™„åŠ åœ¨å•è¯ä¸Šçš„æ ‡ç‚¹ç¬¦å·ã€‚å› æ­¤ï¼Œåœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œç›¸åŒçš„å•è¯è¢«è®¤ä¸ºæ˜¯ä¸åŒçš„ï¼Œå¦‚ä¸‹ä¾‹æ‰€ç¤º:

![](img/75039ed8f4f322bea65fbda5514c8ae2.png)

*æ‘˜è‡ª* ***çŸ­*** *ï¼Œå¹¶éæ‰€æœ‰è¾“å‡ºéƒ½æ˜¾ç¤º*

ğŸ€*èˆå¼ƒæ ‡ç‚¹ç¬¦å·å°†æœ‰åŠ©äºè¿›ä¸€æ­¥è§„èŒƒæ–‡å­—ã€‚*

âœï¸ 2.1.4ã€‚æœ€é•¿çš„å­—ç¬¦ä¸²æ˜¯ä»€ä¹ˆæ ·å­çš„ï¼Ÿ

è®©æˆ‘ä»¬å°†é•¿å­—ç¬¦ä¸²å®šä¹‰ä¸º 16+å­—ç¬¦é•¿ï¼Œå¹¶é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚

```
long = set(s for s in splits if len(s)>15)
long = [(s, freq_splits[s]) for s in long]
long.sort(key=lambda x:x[1], reverse=True)
long
```

![](img/9a749293465333e00373bd4fe44b0552.png)

*æ‘˜å½•è‡ª* ***é•¿*** *ï¼Œå¹¶éæ‰€æœ‰è¾“å‡ºéƒ½æ˜¾ç¤º*

é•¿å¼¦çš„é¢‘ç‡çœ‹èµ·æ¥æ¯”çŸ­å¼¦ä½å¾—å¤šï¼Œè¿™å¹¶ä¸å¥‡æ€ªã€‚é•¿å­—ç¬¦ä¸²çœ‹èµ·æ¥å¾ˆæœ‰è¶£ï¼Œæœ‰å‡ ä¸ªè¦ç‚¹:

ğŸ€åŒä¸€ä¸ªè¯æœ‰ç¾å›½å’Œè‹±å›½çš„æ‹¼æ³•:â€œcharacteringâ€å’Œâ€œcharacteringâ€ã€‚ä½¿ç”¨ä¸‹é¢çš„è„šæœ¬å¿«é€Ÿæ£€æŸ¥åï¼Œè¿™ä¸ªå•è¯çš„ç¾å¼æ‹¼å†™çœ‹èµ·æ¥æ›´å ä¼˜åŠ¿ã€‚é‡åŒ–è¿™ä¸¤ç§æ‹¼å†™åœ¨æ•´ä¸ªè®­ç»ƒè¯­æ–™åº“ä¸­çš„æµè¡Œç¨‹åº¦æœ‰ç‚¹æ£˜æ‰‹ã€‚

```
print(f"characterisation: {sum([c for s, c in long if re.match(r'characterisation*', s.lower())])} strings")
print(f"characterization: {sum([c for s, c in long if re.match(r'characterization*', s.lower())])} strings")
```

![](img/3649c07d59bb4a3bd983b940ada03f8b.png)

ğŸ€*æœ‰å¸¦è¿å­—ç¬¦çš„è¯:â€œå‘äººæ·±çœâ€ã€â€œåå¯ç¤ºå½•â€ã€â€œä¸è¢«æ¬£èµâ€å’Œâ€œç”µå½±ç‰¹åˆ«â€(è¿™ä¸ªæœ‰åŒè¿å­—ç¬¦)ã€‚å¦‚æœæˆ‘ä»¬åœ¨ç©ºæ ¼æˆ–æ ‡ç‚¹ç¬¦å·ä¸Šåšè®°å·ï¼Œè¿™äº›å­—ç¬¦ä¸²å°†è¢«åˆ†å‰²æˆå•ç‹¬çš„å•è¯ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™å°†ä¿ç•™å¥å­çš„è¦ç‚¹ã€‚å¦‚æœæˆ‘ä»¬ä¿ç•™ç”¨è¿å­—ç¬¦è¿æ¥çš„å•è¯ï¼Œå®ƒä»¬å°±ä¸ä¼šåƒç”Ÿåƒ»å­—ä¸€æ ·å¸¸è§ï¼Œå› æ­¤ä¼šè¢«åˆ é™¤ã€‚*

ğŸ€*æœ‰æ–‡å­—ç»“åˆå…¶ä»–æ ‡ç‚¹ç¬¦å·(æœ‰äº›ç”±äºç¯‡å¹…ä¸å¤Ÿ)* ***: '*** *æ¼”å‘˜/å¥³æ¼”å‘˜'ï¼Œã€Šç¢Ÿä¸­è°:ä¸å¯èƒ½'ï¼Œã€Šæ¼”å‘˜(å“ˆï¼å“ˆï¼å“ˆï¼)â€¦ä»–ä»¬æ˜¯â€œï¼Œâ€˜ä¸åŒ:å…¶å®ï¼Œå¸ƒæ´›å…‹â€™ã€‚åœ¨æ ‡è®°æ—¶ï¼Œæœ€å¥½å°†è¿™äº›æƒ…å†µåˆ†æˆå•ç‹¬çš„å•è¯ã€‚å› æ­¤ï¼ŒåŸºäºç©ºæ ¼æˆ–æ ‡ç‚¹ç¬¦å·çš„æ ‡è®°å¯èƒ½æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚*

ğŸ€*æœ‰ç½‘å€å’Œé‚®ç®±:***'/>wwwã€‚ResidentHazard.com 'ï¼Œ****'****http://www.PetitionOnline.com/gh1215/petition.html', ' iamaseal 2 @ YAHOOã€‚COMâ€œâ€ã€‚ä½†æ˜¯ï¼Œå¥½åƒå¹¶ä¸å¤šã€‚**

*ğŸ€*æœ‰æŠŠåŒä¸€ä¸ªå­—é‡å¤ä¸¤æ¬¡ä»¥ä¸Šçš„ä¸æ³•è¯è¯­****:****â€œBooooringâ€¦â€¦ä¸è¦ï¼Œä¸è¦ï¼),'.å¦‚æœä½ çŸ¥é“è¿™äº›æ‹‰é•¿çš„å•è¯çš„æ­£ç¡®æœ¯è¯­ï¼Œæˆ‘å¾ˆæƒ³çŸ¥é“ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬å°†æŠŠå®ƒä»¬ç§°ä¸ºâ€œéæ³•è¯æ±‡â€ã€‚è¿™äº›è¿æ³•æ¡ˆä»¶ä¼¼ä¹å¾ˆå°‘å‡ºç°ã€‚**

*è¿˜æœ‰å…¶ä»–æœ‰è¶£çš„å‘ç°ï¼Œæˆ‘ä»¬å¯ä»¥æ·»åŠ åˆ°è¿™äº›ï¼Œä½†è¿™äº›æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¼€ç«¯ã€‚*

*ç†è§£æˆ‘ä»¬åˆšåˆšæ¢ç´¢çš„è¿™äº›æƒ…å†µæ˜¯å¦æ™®éåˆ°è¶³ä»¥è¯æ˜é¢å¤–çš„é¢„å¤„ç†æ­¥éª¤(å³æ›´é•¿çš„è¿è¡Œæ—¶é—´)æ˜¯æ­£ç¡®çš„ï¼Œè¿™ä¸€ç‚¹å¾ˆé‡è¦ã€‚è¿›è¡Œå®éªŒä»¥äº†è§£æ·»åŠ é¢å¤–çš„é¢„å¤„ç†æ­¥éª¤æ˜¯å¦ä¼šæé«˜æ¨¡å‹æ€§èƒ½æ˜¯å¾ˆæœ‰ç”¨çš„ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬ä¸‰ç¯‡æ–‡ç« ä¸­åšä¸€äº›ä»‹ç»ã€‚*

*âœï¸ 2.1.5ã€‚åˆ°ç›®å‰ä¸ºæ­¢å‡ºç°çš„åç»­é—®é¢˜*

*æˆ‘ä»¬å·²ç»å›ç­”äº†æ‰€æœ‰ 4 ä¸ªçƒ­èº«é—®é¢˜ï¼åœ¨å¯»æ‰¾ç­”æ¡ˆçš„åŒæ—¶ï¼Œæˆ‘ä»¬æ”¶é›†äº†æ›´å¤šçš„é—®é¢˜ã€‚åœ¨æˆ‘ä»¬è·³åˆ°ä¸‹ä¸€ç»„é¢„å®šä¹‰çš„é—®é¢˜ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿè·Ÿè¿›ä¸€äº›ç§»åŠ¨ä¸­å‡ºç°çš„é—®é¢˜ã€‚*

*â—¼ï¸**html æ ‡ç­¾å‡ºç°çš„é¢‘ç‡å¦‚ä½•ï¼Ÿ**
è¿™ä¸ªé—®é¢˜æ˜¯æˆ‘ä»¬åœ¨æŸ¥çœ‹*ç¬¬ 1 èŠ‚çš„æ ·æœ¬æ•°æ®å¤´æ—¶å‡ºç°çš„ã€‚æ•°æ®*ã€‚å½“æˆ‘ä»¬æ ¹æ®ç©ºç™½åˆ†å‰²æ•°æ®æ—¶ï¼Œç¤ºä¾‹ html æ ‡è®°:'< br / > < br / >'å°†è¢«åˆ†å‰²æˆä¸‰ä¸ªå­—ç¬¦ä¸²:'< br 'ï¼Œ'/ > < br 'å’Œ'/ >'ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œ[*<br>*æ ‡ç­¾](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/br#:~:text=The%20HTML%20element%20produces,division%20of%20lines%20is%20significant.)ä¼¼ä¹æ˜¯ç”¨æ¥æ¢è¡Œçš„ã€‚è®©æˆ‘ä»¬ç²—ç•¥ä¼°è®¡ä¸€ä¸‹ html æ ‡ç­¾æœ‰å¤šæ™®éã€‚*

**æ‰€æœ‰è·Ÿè¿›é—®é¢˜éƒ½æ˜¯ç›¸ä¼¼çš„ï¼Œå› ä¸ºæˆ‘ä»¬è¦è¯„ä¼°ç‰¹å®šç±»å‹å­—ç¬¦ä¸²çš„å‡ºç°é¢‘ç‡ã€‚ä¸ºäº†é¿å…é‡å¤æˆ‘ä»¬è‡ªå·±ï¼Œæˆ‘ä»¬æ¥åšä¸€ä¸ªå‡½æ•°ã€‚**

```
*def summarise(pattern, strings, freq):
    """Summarise strings matching a pattern."""
    # Find matches
    compiled_pattern = re.compile(pattern)
    matches = [s for s in strings if compiled_pattern.search(s)]

    # Print volume and proportion of matches
    print("{} strings, that is {:.2%} of total".format(len(matches), len(matches)/ len(strings)))

    # Create list of tuples containing matches and their frequency
    output = [(s, freq[s]) for s in set(matches)]
    output.sort(key=lambda x:x[1], reverse=True)

    return output# Find strings possibly containing html tag
summarise(r"/?>?w*<|/>", splits, freq_splits)*
```

*![](img/a84b446bac24d7c4818cbb92e290cb07.png)*

*æ²¡æœ‰æ˜¾ç¤ºæ‰€æœ‰è¾“å‡º*

*å¦‚æœæˆ‘ä»¬æ»šåŠ¨è¾“å‡ºï¼Œé™¤äº† *< br >* æ ‡ç­¾å’Œå°‘æ•° *< i >* æ ‡ç­¾ä¹‹å¤–ï¼Œæ²¡æœ‰å¤šå°‘ html æ ‡ç­¾ã€‚*

*ğŸ€*å¦‚æœæˆ‘ä»¬åœ¨åˆ†è¯æ—¶å»æ‰æ ‡ç‚¹ç¬¦å·ï¼Œé‚£ä¹ˆâ€œ/ > < brâ€å’Œâ€œ< brâ€å°†å˜æˆâ€œbr â€,æˆ‘ä»¬å¯ä»¥æ·»åŠ â€œbrâ€æ¥åœæ­¢å•è¯ã€‚**

***â—¼ ï¸How ç»å¸¸å‡ºç°çš„æ•°å­—æ˜¯å¤šå°‘ï¼Ÿ** æˆ‘ä»¬ä» 2.1.3 èŠ‚çš„*ä¸­æ‰¾åˆ°äº†ä¸€äº›æ•°å­—çš„å®ä¾‹ã€‚çŸ­å¼¦*ã€‚è®©æˆ‘ä»¬é€šè¿‡ä¸‹é¢çš„è„šæœ¬æ¥çœ‹çœ‹å®ƒä»¬å‡ºç°çš„é¢‘ç‡:*

```
*summarise(r"\d", splits, freq_splits)*
```

*![](img/843f4e6b4b2b7fbc32a866e3b0f3b8dd.png)*

*æ²¡æœ‰æ˜¾ç¤ºæ‰€æœ‰è¾“å‡º*

*åŒ…å«æ•°å­—çš„å­—ç¬¦ä¸²å¾ˆå°‘å‡ºç°ã€‚åœ¨ç”µå½±è¯„è®ºçš„èƒŒæ™¯ä¸‹ï¼Œå¾ˆéš¾ç›´è§‚åœ°ç†è§£æ•°å­—æ˜¯å¦‚ä½•æœ‰ç”¨çš„ã€‚10/10 å¯èƒ½æ˜¯ç§¯ææƒ…ç»ªçš„æ ‡å¿—ï¼Œä½†æˆ‘ä»¬èƒ½ä» 4ã€80 å’Œ 20 è¿™æ ·çš„æ•°å­—ä¸­æ¨æ–­å‡ºä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬å°†åœ¨æ ‡è®°æ—¶ä¸¢å¼ƒæ•°å­—ã€‚*

*æ ¹æ®é¡¹ç›®çš„æ—¶é—´è¡¨ï¼Œä½ å¯èƒ½æ²¡æœ‰è¶³å¤Ÿçš„æ—¶é—´å»å°è¯•æ‰€æœ‰æœ‰è¶£çš„æƒ³æ³•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¿ç•™ä¸€ä»½å¯ä»¥å°è¯•çš„é¡¹ç›®æ¸…å• ***æ˜¯å¾ˆæ–¹ä¾¿çš„ï¼Œä½ å¯ä»¥åœ¨æœ‰æ—¶é—´çš„æ—¶å€™å°è¯•ä¸€ä¸‹ã€‚æˆ‘ä»¬å°†åœ¨è¯¥åˆ—è¡¨ä¸­æ·»åŠ ä»¥ä¸‹ä»»åŠ¡:
1)ä¿å­˜æ•°å­—å¹¶å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬
2)åˆ›å»ºä¸€ä¸ªç‰¹å¾æ¥æŒ‡ç¤ºè¯„è®ºæ˜¯å¦åŒ…å«æ•°å­—****

***â—¼ ï¸How å¸¸ç”¨çš„æ˜¯è¿å­—ç¬¦çš„å•è¯å—ï¼Ÿ**
æˆ‘ä»¬åœ¨*ç¬¬ 2.1.4 èŠ‚*ä¸­æ£€æŸ¥é•¿å­—ç¬¦ä¸²æ—¶çœ‹åˆ°äº†è¿å­—ç¬¦ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒä»¬å‡ºç°çš„é¢‘ç‡:*

```
*summarise(r"\w+-+\w+", splits, freq_splits)*
```

*![](img/b99187d09445178fa729b6ffc867b15a.png)*

*æ²¡æœ‰æ˜¾ç¤ºæ‰€æœ‰è¾“å‡º*

*å¤§çº¦ä¸åˆ° 1%çš„å­—ç¬¦ä¸²åŒ…å«å¸¦è¿å­—ç¬¦çš„å•è¯ã€‚æµè§ˆç”¨è¿å­—ç¬¦è¿æ¥çš„å•è¯ï¼Œå°†å®ƒä»¬åˆ†å¼€ä»¥ä¿æŒæ•°æ®ç®€å•æ›´æœ‰æ„ä¹‰ã€‚ä¾‹å¦‚:æˆ‘ä»¬åº”è¯¥å°†â€œcamera-workâ€æ ‡è®°ä¸º 2 ä¸ªæ ‡è®°:['camera 'ï¼Œ' work']è€Œä¸æ˜¯ 1 ä¸ªæ ‡è®°:['camera-work']ã€‚æˆ‘ä»¬å¯ä»¥åœ¨*åˆ—è¡¨ä¸­æ·»åŠ â€œä¿æŒè¿å­—ç¬¦å•è¯çš„åŸæ ·â€ã€‚**

***â—¼ ï¸How å¸¸ç”¨çš„è¯æ˜¯ç”±å…¶ä»–æ ‡ç‚¹ç¬¦å·ç»„åˆè€Œæˆçš„å—ï¼Ÿ** å¾ˆåƒä¸Šä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨é•¿ä¸²æ¢ç´¢ä¸­çœ‹åˆ°äº†è¿™äº›æ¡ˆä¾‹ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒä»¬å‡ºç°çš„é¢‘ç‡:*

```
*summarise(r"\w+[_!&/)(<\|}{\[\]]\w+", splits, freq_splits)*
```

*![](img/941b0b224b350da458a58f772cb7d636.png)*

*æ²¡æœ‰æ˜¾ç¤ºæ‰€æœ‰è¾“å‡º*

*ä¸è¦å¤ªé¢‘ç¹ï¼Œè¿™äº›è‚¯å®šæ˜¯è¦åˆ†å¼€çš„ã€‚*

***â—¼ ï¸How é¢‘ç¹å‡ºç°çš„éƒ½æ˜¯ç»¿æ—å¥½æ±‰çš„è¯å—ï¼Ÿæˆ‘ä»¬çœ‹åˆ°äº†åƒ NOOOOOOIIIISEï¼)ï¼Œâ€˜æ—©å…ˆã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒä»¬æœ‰å¤šæ™®é:***

```
*def find_outlaw(word):
    """Find words that contain a same character 3+ times in a row."""
    is_outlaw = False
    for i, letter in enumerate(word):
        if i > 1:
            if word[i] == word[i-1] == word[i-2] and word[i].isalpha():
                is_outlaw = True
                break
    return is_outlawoutlaws = [s for s in splits if find_outlaw(s)]
print("{} strings, that is {:.2%} of total".format(len(outlaws), len(outlaws)/ len(splits)))
outlaw_freq = [(s, freq_splits[s]) for s in set(outlaws)]
outlaw_freq.sort(key=lambda x:x[1], reverse=True)
outlaw_freq*
```

*![](img/c656669fb94d93837832f50cef23783a.png)*

*æ²¡æœ‰æ˜¾ç¤ºæ‰€æœ‰è¾“å‡º*

*è¿™äº›ä¸å€¼å¾—çº æ­£ï¼Œå› ä¸ºæ¡ˆä¾‹å¤ªå°‘ã€‚*

*è¿™æ˜¯æœ€åä¸€ä¸ªè·Ÿè¿›é—®é¢˜ï¼æˆ‘ä»¬å·²ç»äº†è§£äº†ä¸€äº›æ•°æ®ã€‚å¸Œæœ›ä½ æ„Ÿè§‰æš–å’Œäº†ã€‚ğŸ’¦ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°ä¸æ–­æ‰©å±•æˆ‘ä»¬çš„é—®é¢˜ï¼Œä¸æ–­æ¢ç´¢ï¼Ÿå‡ºäºæ—¶é—´çš„è€ƒè™‘ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡Œåœæ­¢è¿™ä¸€éƒ¨åˆ†ï¼Œå¹¶å°½é‡ä½¿æ¥ä¸‹æ¥çš„éƒ¨åˆ†å°½å¯èƒ½ç®€æ´ã€‚å¦åˆ™ï¼Œè¿™ç¯‡æ–‡ç« ä¼šè¶…è¿‡å‡ ä¸ªå°æ—¶ã€‚ğŸ’¤*

## *ğŸ“‹ 2.2.ä»£å¸*

*è®©æˆ‘ä»¬ä¸€å£æ°”å›ç­”è¿™ä¸¤ä¸ªé—®é¢˜:*

*âœï¸2 . 2 . 1ã€‚æœ‰å¤šå°‘ä»£å¸ï¼Ÿ
âœï¸ **2.2.2ã€‚æœ‰å¤šå°‘ç‹¬ç‰¹çš„ä»£å¸ï¼Ÿ***

*æˆ‘ä»¬å¿…é¡»å…ˆå°†æ•°æ®ç¬¦å·åŒ–ã€‚å›æƒ³ä¸€ä¸‹ä¹‹å‰çš„æ¢ç´¢ï¼Œä¼¼ä¹æœ€å¥½åœ¨æ ‡è®°æ—¶å»æ‰æ ‡ç‚¹å’Œæ•°å­—ã€‚è®°ä½è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬å°†æ–‡æœ¬æ ‡è®°ä¸ºå­—æ¯æ ‡è®°:*

```
*tokeniser = RegexpTokenizer("[A-Za-z]+")
tokens = tokeniser.tokenize(train_string)
print(tokens[:20], "\n")*
```

*![](img/0775c5021c29976a5fd66cccefdb773e.png)*

*ç°åœ¨æˆ‘ä»¬å·²ç»æ ‡è®°åŒ–äº†ï¼Œæˆ‘ä»¬å¯ä»¥å›ç­”å‰ä¸¤ä¸ªé—®é¢˜:*

```
*print(f"Number of tokens: {len(tokens)}")
print(f"Number of unique tokens: {len(set(tokens))}")*
```

*![](img/64fcb6dbfcd3a29c1f0063f121eb7675.png)*

*è®­ç»ƒæ•°æ®ä¸­æœ‰è¶…è¿‡ 1000 ä¸‡ä¸ªä»¤ç‰Œï¼Œå…¶ä¸­å¤§çº¦æœ‰ 12.2 ä¸‡ä¸ªå”¯ä¸€ä»¤ç‰Œã€‚ç›®å‰ï¼Œâ€œæ‰‹è¡¨â€ã€â€œæ‰‹è¡¨â€å’Œâ€œè§‚çœ‹â€è¢«è§†ä¸ºä¸åŒçš„ä»¤ç‰Œã€‚å—¯ï¼Œå¦‚æœæˆ‘ä»¬èƒ½æŠŠå®ƒä»¬æ­£å¸¸åŒ–ä¸ºâ€œæ‰‹è¡¨â€,å¹¶æŠŠå®ƒä»¬ç®—ä½œä¸€ä¸ªå”¯ä¸€çš„ä»¤ç‰Œï¼Œé‚£ä¸æ˜¯å¾ˆå¥½å—ï¼Ÿå¦‚æœæˆ‘ä»¬è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå”¯ä¸€ä»¤ç‰Œçš„æ•°é‡å°†ä¼šå‡å°‘ã€‚è®©æˆ‘ä»¬å¿«é€Ÿåœ°åšä¸¤ä»¶äº‹:å°†æ‰€æœ‰çš„è®°å·è½¬æ¢æˆå°å†™ï¼Œå¹¶å¯¹å®ƒä»¬è¿›è¡Œ lemmatise:*

```
*lemmatiser = WordNetLemmatizer()
tokens_norm = [lemmatiser.lemmatize(t.lower(), "v") for t in tokens]
print(f"Number of unique tokens: {len(set(tokens_norm))}")*
```

*![](img/66c3430c031ad15a391ab4aaa7c21d01.png)*

*å¤ªå¥½äº†ï¼Œç‹¬ç‰¹ä»¤ç‰Œçš„æ•°é‡ä¸‹é™äº†çº¦ 30%ã€‚*

***ğŸ“Œç»ƒä¹ :**å¦‚æœä½ æ„Ÿå…´è¶£å¹¶ä¸”æœ‰æ—¶é—´ï¼Œä¸è¦åƒä¸Šé¢é‚£æ ·æŠŠä¸¤ä¸ªæ­¥éª¤ç»“åˆèµ·æ¥ï¼Œè¯•ç€åˆ†ç¦»å‡ºæ¥ï¼Œçœ‹çœ‹æ¯ä¸ªæ­¥éª¤ä¸­ç‹¬ç‰¹ä»£å¸çš„æ•°é‡æ˜¯å¦‚ä½•å˜åŒ–çš„ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥é¦–å…ˆå°†è®°å·è½¬æ¢æˆå°å†™ï¼Œå¹¶æ£€æŸ¥æ•°å­—ï¼Œç„¶åä½¿ç”¨ lemmatise å¹¶å†æ¬¡æ£€æŸ¥æ•°å­—ã€‚å¦‚æœæ”¹å˜è¿™ä¸¤ä¸ªæ“ä½œçš„é¡ºåºï¼Œå”¯ä¸€ä»¤ç‰Œçš„æœ€ç»ˆæ•°é‡æ˜¯å¦ä¸åŒäº 82562ï¼Ÿä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿ*

*ğŸ‘‚*å˜¶ï¼Œæˆ‘ä¼šåœ¨* [*ä¸‹ä¸€ç¯‡*](/preprocessing-text-in-python-923828c4114f) *ä¸ºæ¨¡å‹é¢„å¤„ç†æ–‡æœ¬æ—¶ï¼Œå±•ç¤º lemmatise çš„å¦ä¸€ç§æ–¹å¼ã€‚**

*âœï¸ **2.2.3ã€‚æ¯ä¸ªä»¤ç‰Œçš„å¹³å‡å­—ç¬¦æ•°æ˜¯å¤šå°‘ï¼Ÿ***

*è®©æˆ‘ä»¬æ‰¾å‡ºå¹³å‡ä»¤ç‰Œé•¿åº¦å¹¶æ£€æŸ¥å…¶åˆ†å¸ƒæƒ…å†µ:*

```
*# Create list of token lengths for each token
token_length = [len(t) for t in tokens]# Average number of characters per token
print(f"Average number of characters per token: {round(np.mean(token_length),4)}")# Plot distribution
plt.figure(figsize=(12, 12))
sns.countplot(y=token_length)
plt.title("Counts of token length", size=20);*
```

*![](img/c364a9172bb6f42badcfde0542a0a5b1.png)*

*æœ‰å‡ ä¸ªä»¤ç‰Œå¾ˆé•¿ï¼Œä½†ä¹Ÿå¾ˆç½•è§ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¶…è¿‡ 10 ä¸ªå­—ç¬¦çš„ç¡®åˆ‡æ•°é‡:*

```
*pd.DataFrame(data=token_length, columns=['length']).query("length>10").value_counts()*
```

*![](img/e8df79cb7715c1dcc93f9d4ec4ed52a8.png)*

*è¶…è¿‡ 17 ä¸ªå­—ç¬¦çš„é•¿å•è¯å¹¶ä¸å¸¸è§ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹å…¶ä¸­çš„ä¸€äº›:*

```
*[t for t in tokens if len(t)>=20]*
```

*![](img/ec7d6102ad0a7839772fd623b4ea4fde.png)*

*æ²¡æœ‰æ˜¾ç¤ºæ‰€æœ‰è¾“å‡º*

*æœ‰è¶£çš„æ˜¯ï¼Œæœ‰äº›æ˜¯æœ‰æ•ˆçš„é•¿å•è¯ï¼Œè€Œæœ‰äº›é•¿æ˜¯å› ä¸ºå®ƒä»¬ç¼ºå°‘ç©ºæ ¼æˆ–éæ³•å•è¯(å³æ‹‰é•¿)ã€‚*

*ğŸ€*åœ¨é¢„å¤„ç†æ—¶ï¼Œæˆ‘ä»¬åº”è¯¥ç¡®ä¿åƒè¿™æ ·éå¸¸ç½•è§çš„è®°å·è¢«ä¸¢å¼ƒï¼Œè¿™æ ·å®ƒä»¬å°±ä¸ä¼šåœ¨å°†è®°å·çŸ¢é‡åŒ–æˆçŸ©é˜µæ—¶åˆ›å»ºå•ç‹¬çš„åˆ—ã€‚**

## *ğŸ“‹ 2.3.åœæ­¢è¨€è¯­*

*âœï¸2 . 3 . 1ã€‚æœ€å¸¸ç”¨çš„åœç”¨è¯æ˜¯ä»€ä¹ˆï¼Ÿ*

*è®©æˆ‘ä»¬é¦–å…ˆæ£€æŸ¥æ‰€æœ‰åœç”¨è¯:*

```
*stop_words = stopwords.words("english")
print(f"There are {len(stop_words)} stopwords.\n")
print(stop_words)*
```

*![](img/734593cb3ce4b2d7ef59b933da21750d.png)*

*åœ¨å†™è¿™ç¯‡æ–‡ç« çš„æ—¶å€™ï¼Œæœ‰ 179 ä¸ªåœç”¨è¯ã€‚åœç”¨è¯çš„æ¸…å•å°†æ¥è¿˜ä¼šå¢åŠ ã€‚çœ‹èµ·æ¥æˆ‘ä»¬å¯ä»¥æ‰©å±•åœç”¨è¯æ¥åŒ…å«æ›´å¤šçš„åœç”¨è¯ã€‚äº‹å®ä¸Šï¼Œæˆ‘å·²ç»åœ¨ Github ä¸Š[æè®®](https://github.com/nltk/nltk/issues/2588)å°†ä¸‹é¢çš„é€šç”¨åœç”¨è¯æ·»åŠ åˆ° *nltk* çš„è‹±æ–‡åœç”¨è¯åˆ—è¡¨ä¸­ã€‚æˆ‘ä»¬è¿˜è¦ç¡®ä¿åœ¨åˆ—è¡¨ä¸­æ·»åŠ ä¸€ä¸ªè‡ªå®šä¹‰åœç”¨è¯â€œbrâ€*:**

```
**stop_words.extend(["cannot", "could", "done", "let", "may" "mayn",  "might", "must", "need", "ought", "oughtn", "shall", "would", "br"])
print(f"There are {len(stop_words)} stopwords.\n")**
```

**![](img/fd0538eb1cf136ef0a65c324ca93475c.png)**

**ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹æœ€å¸¸è§çš„åœç”¨è¯æ˜¯ä»€ä¹ˆ:**

```
**freq_stopwords = [(sw, tokens_norm.count(sw)) for sw in stop_words]
freq_stopwords.sort(key=lambda x: x[1], reverse=True)
freq_stopwords[:10]**
```

**![](img/cc60b4aa9d15d626582ca785312970c2.png)**

**é¢‘ç‡çœŸçš„å¾ˆé«˜(å’„ï¼Œæˆ‘çš„æ„æ€æ˜¯å®ƒä»¬æ˜¯åœç”¨è¯ï¼Œå½“ç„¶ä¼šé¢‘ç¹ğŸ˜ˆ)ï¼Œç‰¹åˆ«æ˜¯å¯¹äºâ€˜beâ€™å’Œâ€˜theâ€™ã€‚æ‰¾å‡ºåœç”¨è¯åœ¨æ ‡è®°ä¸­æ‰€å çš„æ¯”ä¾‹ä¸æ˜¯å¾ˆæœ‰è¶£å—ï¼Ÿè®©æˆ‘ä»¬å¿«é€Ÿæ£€æŸ¥ä¸€ä¸‹:**

```
**n_stopwords = len([t for t in tokens_norm if t in stop_words])
print(f"{n_stopwords} tokens are stop words.")
print(f"That is {round(100*n_stopwords/len(tokens_norm),2)}%.")**
```

**![](img/122515318663adb4725c1cfe2576fa7e.png)**

**å¤§çº¦ä¸€åŠçš„æ ‡è®°æ˜¯åœç”¨è¯ã€‚ğŸ’­**

**âœï¸ã€‚è¿˜æœ‰å“ªäº›è¯ç»å¸¸å‡ºç°ï¼Œå¯ä»¥æ·»åŠ åˆ°åœç”¨è¯ä¸­ï¼Ÿ**

**æˆ‘ä»¬å¾ˆå¿«ä¼šåœ¨æŸ¥çœ‹å¸¸è§ä»¤ç‰Œæ—¶å›ç­”è¿™ä¸ªé—®é¢˜ã€‚**

## **ğŸ“‹ 2.4.å¸¸è§ n å…ƒè¯­æ³•**

**æ˜¯æ—¶å€™æ‰¾å‡ºå¸¸è§çš„ n-gram äº†ã€‚è®©æˆ‘ä»¬ä¸€èµ·å›ç­”è¿™å››ä¸ªé—®é¢˜:**

**âœï¸**2 . 4 . 1â€“4ã€‚ä»€ä¹ˆæ˜¯æœ€å¸¸è§çš„ä»¤ç‰Œï¼ŒäºŒå…ƒï¼Œä¸‰å…ƒå’Œå››å…ƒï¼Ÿ****

**é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ é™¤åœç”¨è¯:**

```
**tokens_clean = [t for t in tokens_norm if t not in stop_words]
print(f"Number of tokens: {len(tokens_clean)}")**
```

**![](img/01c43b1f786e216d90403222e5174704.png)**

**è¿™æ˜¯å‰©ä¸‹çš„ 49%çš„ä»£å¸ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥å¸¸è§çš„è®°å·(å³ï¼Œä¸€å…ƒè¯)ã€äºŒå…ƒè¯ã€ä¸‰å…ƒè¯å’Œå››å…ƒè¯:**

```
**def preprocess_text(text):
    """Preprocess text into normalised tokens."""
    # Tokenise words into alphabetic tokens
    tokeniser = RegexpTokenizer(r'[A-Za-z]{2,}')
    tokens = tokeniser.tokenize(text)

    # Lowercase and lemmatise 
    lemmatiser = WordNetLemmatizer()
    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]

    # Remove stopwords
    keywords= [lemma for lemma in lemmas if lemma not in stop_words]
    return keywordsdef get_frequent_ngram(corpus, ngram, n=20):
    """Find most common n n-grams tokens."""
    # Preprocess each document
    documents = [preprocess_text(document) for document in corpus]

    # Find ngrams per document
    n_grams = [list(ngrams(document, ngram)) for document in documents]

    # Find frequency of ngrams
    n_grams_flattened = [item for sublist in n_grams for item in sublist]
    freq_dist = FreqDist(n_grams_flattened)
    top_freq = freq_dist.most_common(n)
    return pd.DataFrame(top_freq, columns=["ngram", "count"])# Get frequent ngrams for all 4
for i in range(1,5):
    mapping = {1:"uni", 2:"bi", 3:"tri", 4:"four"}
    plt.figure(figsize=(12,10))
    sns.barplot(x="count", y="ngram", data=get_frequent_ngram(train['review'], i))
    plt.title(f"Most common {mapping[i]}grams");**
```

**![](img/e7b2a7ba1b28621b027a16e726e58bfd.png)****![](img/3a3680e88b7ce3869280419c6f8ea4c2.png)****![](img/98163ffe99f931e4ed5e887d814b7d81.png)****![](img/354f553665945339ea693e4c724e67a6.png)**

**ä¸å…¶ä»–å¸¸ç”¨è¯ç›¸æ¯”ï¼Œå•è¯â€œfilmâ€å’Œâ€œmovieâ€çœ‹èµ·æ¥ç›¸å½“é¢‘ç¹ã€‚é—®é¢˜ 2.3.2 çš„ç­”æ¡ˆã€‚å°±æ˜¯æ½œåœ¨çš„åŠ ä¸Šâ€˜ç”µå½±â€™ï¼Œå’Œâ€˜ç”µå½±â€™æ¥åœæ­¢ç”¨è¯ã€‚æœ‰è¶£çš„æ˜¯ç»å¸¸çœ‹åˆ°äºŒå…ƒã€ä¸‰å…ƒå’Œå››å…ƒã€‚éšç€ n çš„å¢åŠ ï¼Œé¢‘ç‡å¦‚é¢„æœŸçš„é‚£æ ·ä¸‹é™ã€‚äºŒå…ƒæ¨¡å‹å¯èƒ½æ˜¯æ½œåœ¨æœ‰ç”¨çš„ï¼Œä½†æ˜¯ä¸‰å…ƒæ¨¡å‹å’Œå››å…ƒæ¨¡å‹ç›¸å¯¹äºæ ‡è®°é¢‘ç‡æ¥è¯´ä¸å¤Ÿé¢‘ç¹ã€‚**

## **ğŸ“‹ 2.5.æ–‡æ¡£**

**è®©æˆ‘ä»¬ä¸€èµ·æ¥å›ç­”è¿™äº›é—®é¢˜:**

**âœï¸2 . 5 . 1ã€‚æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡å¥å­æ•°æ˜¯å¤šå°‘ï¼Ÿ
**âœï¸ 2.5.2ã€‚æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡ä»¤ç‰Œæ•°æ˜¯å¤šå°‘ï¼Ÿ
âœï¸ã€‚æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡å­—ç¬¦æ•°æ˜¯å¤šå°‘ï¼Ÿ
âœï¸ã€‚æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡åœç”¨è¯æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ
âœï¸ã€‚è¿™äº›é—®é¢˜çš„ç­”æ¡ˆå¦‚ä½•å› æƒ…ç»ªè€Œå¼‚ï¼Ÿ****

**é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»å‡†å¤‡æ•°æ®:**

```
**# tokeniser = RegexpTokenizer("[A-Za-z]+")
train["n_sentences"] = train["review"].apply(sent_tokenize).apply(len)
train["tokens"] = train["review"].apply(tokeniser.tokenize)
train["n_tokens"] = train["tokens"].apply(len)
train["n_characters"] = train["review"].apply(len)
train["n_stopwords"] = train["tokens"].apply(lambda tokens: len([t for t in tokens if t in stop_words]))
train["p_stopwords"] = train["n_stopwords"]/train["n_tokens"]# Inspect head
columns = ['sentiment', 'n_sentences', 'n_tokens', 'n_characters', 'n_stopwords', 'p_stopwords']
train[columns].head()**
```

**![](img/9bfe626210a7e19b4735ac653a159c95.png)**

**è®©æˆ‘ä»¬æ£€æŸ¥æ„Ÿå…´è¶£çš„å˜é‡çš„æè¿°æ€§ç»Ÿè®¡æ•°æ®:**

```
**train.describe()**
```

**![](img/ff1265bc98cee8b42295f573b773f986.png)**

**åœ¨è¿™ä¸ªè¡¨æ ¼ä¸­ï¼Œæˆ‘ä»¬æœ‰å‰å››ä¸ªé—®é¢˜çš„ç­”æ¡ˆã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦å› æƒ…ç»ªè€Œä¸åŒã€‚å¦‚æœå®ƒä»¬æ˜¾è‘—ä¸åŒï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å˜é‡ä½œä¸ºæ¨¡å‹çš„ç‰¹å¾:**

```
**num_vars = train.select_dtypes(np.number).columns
train.groupby("sentiment")[num_vars].agg(["mean", "median"])**
```

**![](img/62679be7466e9371602e38825eed395c.png)**

**ä»é›†ä¸­è¶‹åŠ¿æ¥çœ‹ï¼Œæƒ…ç»ªä¼¼ä¹æ²¡æœ‰å®è´¨æ€§çš„ä¸åŒã€‚ä¸ºäº†ç¡®ä¿ä¸‡æ— ä¸€å¤±ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹åˆ†å¸ƒæƒ…å†µ:**

```
**def plot_distribution(df, var, hue):
    """Plot overlayed histogram and density plot per sentiment."""
    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=[16,4])

    # Histogram
    sns.histplot(data=df, x=var, hue=hue, bins=30, kde=False, ax=ax[0])
    ax[0].set_title(f"Histogram for {var}")

    # Density plot
    sns.kdeplot(data=df, x=var, hue=hue, shade=True, ax=ax[1])
    ax[1].set_title(f"Density plot for {var}");

# Plot for all numerical variables
for var in num_vars:
    plot_distribution(train, var, 'sentiment')**
```

**![](img/ef4787d0050a08099484c00beee3e686.png)****![](img/bcab9836b444a8e3862d5726c0ecc302.png)****![](img/6a924a05811f753eb19748c2d082f29f.png)**

**æƒ…æ„Ÿä¹‹é—´å˜é‡çš„åˆ†å¸ƒä¼¼ä¹éå¸¸ç›¸ä¼¼ã€‚å®ƒä»¬ä¸å¤ªå¯èƒ½ä½œä¸ºæœ‰ç”¨çš„ç‰¹æ€§ï¼Œä½†æ˜¯æˆ‘ä»¬æ€»æ˜¯å¯ä»¥å°è¯•ã€‚ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥æŠŠå®ƒæ·»åŠ åˆ°*ä¸€ä¸ªå€¼å¾—å°è¯•çš„é¡¹ç›®åˆ—è¡¨ä¸­*ï¼Ÿ**

**åœ¨æˆ‘ä»¬ç»“æŸä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æœ€åä¸€ä»¶äº‹â€”â€”å¸¸ç”¨è¯æ˜¯å¦å› æƒ…æ„Ÿä¸åŒè€Œä¸åŒã€‚è®©æˆ‘ä»¬ä¸ºæ¯ç§æƒ…ç»ªå‡†å¤‡æ•°æ®:**

```
**pos_documents = [preprocess_text(document) for document in train.loc[train['sentiment']=='positive', 'review']]
pos_tokens = [item for sublist in pos_documents for item in sublist]
pos_freq = FreqDist(pos_tokens)
pos_common = [word for word, frequency in pos_freq.most_common(20)]
print(f"***** 20 frequent tokens in positive reviews: *****\n{pos_common}\n")neg_documents = [preprocess_text(document) for document in train.loc[train['sentiment']=='negative', 'review']]
neg_tokens = [item for sublist in neg_documents for item in sublist]
neg_freq = FreqDist(neg_tokens)
neg_common = [word for word, frequency in neg_freq.most_common(20)]
print(f"***** 20 frequent tokens in negative reviews: *****\n{neg_common}\n")common = set(neg_common).union(pos_common)
print(f"***** Their union: *****\n{common}\n")**
```

**![](img/f051bab6efa6584ce96707310d28efd2.png)**

**è¿™ä¸¤ç§æƒ…ç»ªä¸­æœ€å¸¸è§çš„ä¸‰ä¸ªç¬¦å·æ˜¯â€œç”µå½±â€ã€â€œç”µå½±â€å’Œâ€œä¸€â€ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒä»¬çš„é¢‘ç‡:**

```
**# Create a dataframe containing the common tokens and their frequency
common_freq = pd.DataFrame(index=common, columns=["neg", "pos"])
for token in common:
    common_freq.loc[token, "pos"] = pos_freq[token]
    common_freq.loc[token, "neg"] = neg_freq[token]
common_freq.sort_values(by="pos", inplace=True)# Add ranks and rank difference
common_freq['pos_rank'] = common_freq['pos'].rank()
common_freq['neg_rank'] = common_freq['neg'].rank()
common_freq['rank_diff'] = common_freq['neg_rank'] - common_freq['pos_rank']
common_freq.sort_values(by='rank_diff', inplace=True)
common_freq.head()**
```

**![](img/f41bf816e25a161cba563674036e8a29.png)**

**ç°åœ¨ï¼Œæ˜¯æ—¶å€™æƒ³è±¡äº†:**

```
**fig, ax =plt.subplots(1, 2, figsize=(16, 10))
sns.barplot(x="pos", y=common_freq.index, data = common_freq, ax=ax[0])
sns.barplot(x="neg", y=common_freq.index, data = common_freq, ax=ax[1])
fig.suptitle('Top tokens and their frequency by sentiment');**
```

**![](img/b50f6c2ff03c7e203896b539809a041b.png)**

**å—¯ï¼Œæœ‰è¶£çš„æ˜¯ï¼Œåœ¨æ­£é¢è¯„è®ºä¸­,â€œfilmâ€æ¯”â€œmovieâ€å‡ºç°å¾—æ›´é¢‘ç¹ã€‚åœ¨è´Ÿé¢è¯„è®ºä¸­ï¼Œå®ƒè¢«ç¿»è½¬äº†ã€‚ä¹Ÿè®¸å®ƒä»¬ä¸åº”è¯¥è¢«æ·»åŠ åˆ°åœç”¨è¯ä¸­ï¼Œå°½ç®¡å®ƒä»¬å‡ºç°çš„é¢‘ç‡å¾ˆé«˜ã€‚æˆ‘ä»¬å†æ¥çœ‹ä¸€ä¸‹å›¾è¡¨ï¼Œä½†æ˜¯æ’é™¤è¿™ä¸¤ä¸ªå¸¸ç”¨è¯:**

```
**rest = common_freq.index.drop(['film', 'movie'])
fig, ax =plt.subplots(1, 2, figsize=(16, 10))
sns.barplot(x="pos", y=rest, data = common_freq.loc[rest], ax=ax[0])
sns.barplot(x="neg", y=rest, data = common_freq.loc[rest], ax=ax[1])
fig.suptitle('Top tokens and their frequency by sentiment');**
```

**![](img/a1d187aed98c4729a4efb4164297ce43.png)**

**å¾ˆç›´è§‚åœ°çœ‹åˆ°ï¼Œå•è¯â€œæ£’â€ã€â€œå¥½â€å’Œâ€œçˆ±â€åœ¨æ­£é¢è¯„ä»·ä¸­æ›´é¢‘ç¹å‡ºç°ï¼Œè€Œâ€œç”šè‡³â€å’Œâ€œä¸å¥½â€åœ¨è´Ÿé¢è¯„ä»·ä¸­æ›´é¢‘ç¹å‡ºç°ã€‚**

**è¿˜æœ‰å¾ˆå¤šä¸œè¥¿è¦æ¢ç´¢ï¼Œä½†æ˜¯æ˜¯æ—¶å€™æ€»ç»“äº†ï¼ğŸ•›**

# **3.ç»“æŸè¯­ğŸ’­**

**å¹²å¾—å¥½ï¼Œä½ èµ°äº†è¿™ä¹ˆè¿œï¼ğŸ˜è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹è¦ç‚¹:
â—¼ï¸åœ¨æ ‡è®°æ—¶åˆ é™¤æ ‡ç‚¹å’Œæ•°å­—
â—¼ï¸è§„èŒƒåŒ–æ–‡æœ¬(å°å†™ã€å­—æ¯ç­‰)
â—¼ï¸ç”¨â€œbrâ€å’Œå…¶ä»–ç¼ºå¤±çš„è¾…åŠ©åŠ¨è¯ä¸°å¯Œåœç”¨è¯
â—¼ï¸åˆ é™¤ç½•è§è¯**

**ä¸€ä¸ªä¸é”™çš„å°è¯•åˆ—è¡¨:
â—¼ï¸å°†è‹±å¼æ‹¼å†™è½¬æ¢ä¸ºç¾å¼æ‹¼å†™(åä¹‹äº¦ç„¶)
â—¼ï¸ä¿ç•™æ•°å­—å¹¶å°†å…¶è½¬æ¢ä¸ºå•è¯
â—¼ï¸åœ¨æ ‡è®°æ—¶ä¿ç•™è¿å­—ç¬¦
â—¼ï¸åŒ…å«äºŒå…ƒæ¨¡å‹
â—¼ï¸æ·»åŠ æ•°å­—ç‰¹å¾ï¼Œå¦‚å¥å­ã€æ ‡è®°ã€å­—ç¬¦å’Œåœç”¨è¯çš„æ•°é‡**

**![](img/be71aa5d3b12a6dde646e3e1a18c3812.png)**

**å›¾ç‰‡ç”± [Andreas Chu](https://unsplash.com/@andreaschu?utm_source=medium&utm_medium=referral) åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„**

***æ‚¨æƒ³è¦è®¿é—®æ›´å¤šè¿™æ ·çš„å†…å®¹å—ï¼Ÿåª’ä½“ä¼šå‘˜å¯ä»¥æ— é™åˆ¶åœ°è®¿é—®åª’ä½“ä¸Šçš„ä»»ä½•æ–‡ç« ã€‚å¦‚æœæ‚¨ä½¿ç”¨* [*æˆ‘çš„æ¨èé“¾æ¥*](https://zluvsand.medium.com/membership)*æˆä¸ºä¼šå‘˜ï¼Œæ‚¨çš„ä¸€éƒ¨åˆ†ä¼šè´¹å°†ç›´æ¥ç”¨äºæ”¯æŒæˆ‘ã€‚***

**è°¢è°¢ä½ çœ‹æˆ‘çš„å¸–å­ã€‚æ¢ç´¢æ€§æ•°æ®åˆ†ææ˜¯ä¸€é¡¹å¼€æ”¾å¼çš„ä¸»è§‚ä»»åŠ¡ã€‚æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œåœ¨æ¢ç´¢å’Œé¢„å¤„ç†æ—¶ï¼Œæˆ‘ä»¬ä¸å¾—ä¸åšå‡ºè®¸å¤šå°çš„é€‰æ‹©ã€‚æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½è®©ä½ ä½“ä¼šåˆ°å¦‚ä½•æ„å»ºåˆ†æï¼Œå¹¶å±•ç¤ºä¸€äº›ä½ å¯ä»¥åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­æ€è€ƒçš„é—®é¢˜ã€‚åšäº†ä¸€äº›æ¢ç´¢æ€§åˆ†æåï¼Œæˆ‘ä»¬ç¦»æ„å»ºæ¨¡å‹æ›´è¿‘äº†ä¸€æ­¥ã€‚åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®ã€‚ä»¥ä¸‹æ˜¯è¯¥ç³»åˆ—å¦å¤–ä¸¤ç¯‡å¸–å­çš„é“¾æ¥:â—¼ï¸ [ç”¨ Python é¢„å¤„ç†æ–‡æœ¬](/preprocessing-text-in-python-923828c4114f)
â—¼ï¸ [ç”¨ Python è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»](/sentiment-classification-in-python-da31833da01b)**

**ä»¥ä¸‹æ˜¯æˆ‘çš„å…¶ä»– NLP ç›¸å…³å¸–å­çš„é“¾æ¥:
â—¼ï¸[Python ä¸­çš„ç®€å• word cloud](/simple-wordcloud-in-python-2ae54a9f58e5)
*(ä¸‹é¢åˆ—å‡ºäº†ä¸€ç³»åˆ—å…³äº NLP ä»‹ç»çš„å¸–å­)*
â—¼ï¸ [ç¬¬ä¸€éƒ¨åˆ†:Python ä¸­çš„é¢„å¤„ç†æ–‡æœ¬](/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96)
â—¼ï¸ [ç¬¬äºŒéƒ¨åˆ†:è¯æ¡æ»¡è¶³å’Œè¯å¹²çš„åŒºåˆ«](https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc)
â—¼ï¸ [ç¬¬ä¸‰éƒ¨åˆ†:TF-IDF è§£é‡Š](https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc)
â—¼ï¸ [ç¬¬å››éƒ¨åˆ†:python ä¸­çš„ç›‘ç£æ–‡æœ¬åˆ†ç±»æ¨¡å‹](https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267)**

**å†è§ğŸƒğŸ’¨**