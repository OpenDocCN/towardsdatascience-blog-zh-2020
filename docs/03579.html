<html>
<head>
<title>Squeeze-and-Excitation Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">压缩和激励网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/squeeze-and-excitation-networks-fb91e7f64096?source=collection_archive---------36-----------------------#2020-04-04">https://towardsdatascience.com/squeeze-and-excitation-networks-fb91e7f64096?source=collection_archive---------36-----------------------#2020-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8a25" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">提高 CNN 性能的频道自我关注</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1c8b9fc06e6448a3edfa6182c9e03ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wK6wJ5-FvYLXmd_rvIMOtw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">悉尼港上空的烟火<a class="ae ky" href="https://en.wikipedia.org/wiki/Fireworks#/media/File:OperaSydney-Fuegos2006-342289398.jpg" rel="noopener ugc nofollow" target="_blank">(维基百科，CC by 2.0) </a></p></figure><p id="e860" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章描述了挤压和激励模块，这是一种可以插入卷积神经网络以提高性能的架构单元，只需增加少量的参数。挤压和激励模块明确地对渠道关系和渠道相互依赖性进行建模，并包括一种对渠道的自我关注形式。</p><p id="b7ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章的主要参考是原始论文，它被引用了 2500 多次:</p><p id="cc36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank">、、萨缪尔·阿尔巴尼、和吴。"挤压和激励网络."CVPR 2018。</a></p><h1 id="ba73" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">概述</strong></h1><p id="f42d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">挤压和激励(SE)模块旨在提高卷积神经网络表示的质量。卷积神经网络(CNN)的回顾可在<a class="ae ky" href="https://glassboxmedicine.com/2019/05/05/how-computers-see-intro-to-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="39e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于卷积神经网络的任何层，我们可以构建相应的 SE 块来重新校准特征映射:</p><ul class=""><li id="abc8" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">在“挤压”步骤中，我们使用全局平均池来聚合跨越其空间维度 H x W 的特征地图，以产生通道描述符。</li><li id="43ea" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">在“激励”步骤中，我们将完全连接的层应用于“挤压”步骤的输出，以产生每通道权重(“激活”)的集合，该集合被应用于特征映射，以生成 SE 块的最终输出。</li></ul><p id="4c27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">论文中的图 1 描述了一个 SE 模块:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/8fb79bfd054e21f7617ea868097b7ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*oisdgf4Jr_rzOppg"/></div></figure><h1 id="b5d2" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">论文批注&amp;多通道 2D 卷积复习</strong></h1><p id="0576" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们的输入是一个图像<strong class="lb iu"> X </strong>，它有<em class="nh"> s </em>个通道:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/f78469dfd686fc93b23e6c55d1ac40dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*T1R7y9XhTCS3fE37"/></div></div></figure><p id="117c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们定义一个卷积层<strong class="lb iu"> F </strong> _ <em class="nh"> tr </em>，它由滤波器<strong class="lb iu"> V </strong>组成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/5f4548b0411689b429de171060f34994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*L2MEi0Qhf_1-4Jz2"/></div></figure><p id="8763" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将由滤波器<strong class="lb iu"> V </strong>组成的卷积层<strong class="lb iu"> F </strong> _ <em class="nh"> tr </em>应用于我们的输入图像<strong class="lb iu"> X </strong>。</p><p id="9954" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了阐明符号以及输入图像<strong class="lb iu"> X </strong>和学习的卷积滤波器组<strong class="lb iu"> V </strong>之间的关系，这里快速回顾一下多通道输入图像的 2D 卷积，在这种情况下是 3 通道 RGB 输入图像。</p><p id="b740" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">变换<strong class="lb iu"> F </strong> _ <em class="nh"> tr </em>包括多个卷积滤波器<strong class="lb iu">V</strong>=【<strong class="lb iu">V _</strong>1、<strong class="lb iu"> v_ </strong> 2、…、<strong class="lb iu"> v </strong> _C】。3 通道图像的“2D 卷积”的一个过滤器实际上是三维的，正如我们从这个动画中可以看到的，其中滑动的白色轮廓表示单个过滤器，<em class="nh">例如</em> <strong class="lb iu"> v </strong> _1:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/0d1383962d2833162f0b1047861cee68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*5QIY4d94rq2xNTlu"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马丁·戈尔纳创作的动画。<a class="ae ky" href="https://sites.google.com/site/nttrungmtwiki/home/it/data-science---python/tensorflow/tensorflow-and-deep-learning-part-3?tmpl=%2Fsystem%2Fapp%2Ftemplates%2Fprint%2F&amp;showPrintDialog=1" rel="noopener ugc nofollow" target="_blank">原文链接</a>。此处也可用<a class="ae ky" href="https://stackoverflow.com/questions/42883547/intuitive-understanding-of-1d-2d-and-3d-convolutions-in-convolutional-neural-n" rel="noopener ugc nofollow" target="_blank">。</a></p></figure><p id="940b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑 RGB 图像<strong class="lb iu"> X </strong>，在下图中显示了两次——一次在顶部覆盖了淡橙色滤镜<strong class="lb iu"> v </strong> _1，一次在底部覆盖了紫色滤镜<strong class="lb iu"> v </strong> _2:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/793105f1b497fb6fdfaecb181273fb71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*mvkM48Ma2ARZfIwI"/></div></figure><p id="be50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图中我们可以看到，单个滤镜是三维的，为 x 的 3 个通道测量 k x k x 3，在这个例子中，<strong class="lb iu"> V </strong>包括两个滤镜，淡橙色滤镜<strong class="lb iu"> v </strong> _1 和紫色滤镜<strong class="lb iu"> v </strong> _2。</p><p id="5270" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文的符号中，上标 s 用于跟踪输入 X 的通道(本例中 s=1，2，3)，而下标 c 用于跟踪<strong class="lb iu"> V </strong>中的滤波器(本例中 c=1，2):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/424a833b1f833489f21c59db0539a067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*O-zpOP6d69BUKbmW"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/3991f4e159800ee55831f6afedf83d69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*L8RqlDArkd8y1G0W"/></div></figure><p id="9a7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是详细标注的过滤器:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/681aa65591dbce2dd091c10687d10f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*fTXgAi0Tkn0F6Wsc"/></div></figure><p id="cd1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过将滤波器<strong class="lb iu"> V </strong>应用于输入图像<strong class="lb iu"> X </strong>，我们获得输出滤波器图<strong class="lb iu"> U </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/24149f87815ddfb9d95b05735a870d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*kRHZs4E5zexOYR-l"/></div></figure><p id="6e95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单一输出滤波器映射，例如<strong class="lb iu"> u </strong> _1(下标 c=1)，通过所有通道(即通过上标 s=1、s=2 和 s=3)求和产生。以这种方式只能捕获隐含的渠道关系。引用作者的话，</p><blockquote class="nj nk nl"><p id="9696" class="kz la nh lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated"><em class="it">由于输出是通过所有通道求和产生的，通道相关性隐含在</em><strong class="lb iu"><em class="it">v</em></strong><em class="it">_ c 中，但是与滤波器捕获的局部空间相关性纠缠在一起。由卷积建模的信道关系固有地是隐含的和局部的(除了在最顶层的那些)。我们期望卷积特征的学习通过显式建模信道相互依赖性[即，使用 SE 块]来增强。</em></p></blockquote><p id="3af5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(注:在本文之后，为了简化符号，省略了偏差术语。)</p><p id="7f65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经回顾了论文的符号和 2D 多通道卷积，这里有一个挤压和激发块如何工作的解释。它们出奇的简单，只需要全局平均池和完全连接的层。</p><h1 id="1e32" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">挤压:全局信息嵌入</strong></h1><p id="a925" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">SE 块中的“挤压”步骤将全局空间信息挤压到通道描述符中。挤压步骤包括跨空间维度 H x W 的全局平均汇集，以产生逐通道统计。以下是这篇论文的摘录，其中包含挤压步骤的描述和等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/f4d4af7370036e67fae770db9e46b7b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*aQE2L2Wa_01_jGyg"/></div></div></figure><p id="95c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图说明了压缩操作，该操作将每个不同的 H x W 特征图<strong class="lb iu"> u </strong> _c 缩减为标量通道描述符<em class="nh"> z </em> _c:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/173a2e9fee20ba6f8b2b78cd6be31d8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*nw_o0CGS4fxKpuuL"/></div></figure><p id="93e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标量[ <em class="nh"> z </em> _1、<em class="nh"> z </em> _2、…、<em class="nh"> z </em> _C]一起形成一个长度为<em class="nh"> C </em>的向量<strong class="lb iu"> z </strong>，该向量将在激励步骤中使用。请注意，<strong class="lb iu"> z </strong>捕获全局信息，因为<strong class="lb iu"> z </strong>的每个元素是由全特征图高度<em class="nh"> H </em>和全特征图宽度<em class="nh"> W </em>的集合产生的。</p><h1 id="2076" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">激励:自适应重新校准</strong></h1><p id="7f00" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">激励操作旨在完全捕捉通道间的相关性。激励操作处理挤压步骤的输出(矢量<strong class="lb iu"> z </strong>)以产生激活矢量<strong class="lb iu"> s，</strong>，然后用于重新缩放特征图。(此激活向量<strong class="lb iu"> s </strong>不要与之前用于跟踪输入<strong class="lb iu"> X </strong>通道的<em class="nh"> s </em>相混淆)。</p><p id="ae26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">向量<strong class="lb iu"> s </strong>是从挤压输出<strong class="lb iu"> z </strong>中计算出来的，使用两个完全连接的层，瓶颈将表示缩小到大小<em class="nh"> C/r: </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/337e3e1d3c3459107494ea626a7c1ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/0*Sh_rErChStU13UNk"/></div></div></figure><p id="2431" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数<em class="nh"> r </em>被称为“减速比”如果<em class="nh"> r </em>越大，则中间表示越小。将表示的大小减小到<em class="nh"> C/r </em>然后将其扩展回<em class="nh"> C </em>的目标是(a)限制模型的复杂性和(b)帮助一般化。正如作者所说，缩减率<em class="nh"> r </em>“允许我们改变网络中 SE 块的容量和计算成本。”</p><p id="f160" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是激励步骤第一部分的草图，其中从挤压输出<strong class="lb iu"> z </strong>计算出<strong class="lb iu"> s </strong>中的激活:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/f42a9be70725651197b9cf1d357abbf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*vHh14D2BrjiTouEq"/></div></figure><p id="5d10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦<strong class="lb iu"> s </strong>被计算，则<strong class="lb iu"> s </strong>的元素被用于重新缩放<strong class="lb iu"> U </strong>的特征图，以获得 SE 块的最终输出，称为<strong class="lb iu">X</strong>-波浪号:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/6f3bd5a07cf735b0ff2358566d54b4ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*2m_8gGQ3qSptujQs"/></div></figure><p id="9028" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是最后一步的草图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/a36ca77efc3e6d902fdc4c8315bb0fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*hp9XznWN6OxGpCXU"/></div></figure><p id="1036" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从草图中可以看出，激活<em class="nh"> s </em> _1 必须平铺在<em class="nh">H</em>x<em class="nh">W</em>map<strong class="lb iu">u</strong>_ 1 上，以便<em class="nh"> s </em> _1 元素乘上<strong class="lb iu"> u </strong> _1 中的所有值，生成<strong class="lb iu"> x </strong> -tilde_1。</p><p id="b491" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重新校准的特征地图[<strong class="lb iu">x</strong>-波浪号 _1、<strong class="lb iu">x</strong>-波浪号 _2、…、<strong class="lb iu">x</strong>-波浪号 _C]的堆栈然后继续通过 CNN 的其余部分，因为它们与原始特征地图[ <strong class="lb iu"> u </strong> _1、<strong class="lb iu"> u </strong> _2、…、<strong class="lb iu"> u </strong> _C]的维度完全相同。</p><p id="8301" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，SE 块给 CNN 增加了一种自我关注的形式:</p><blockquote class="nj nk nl"><p id="0af4" class="kz la nh lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated"><em class="it"> SE 模块本质上引入了以输入为条件的动态，这可被视为通道上的自关注功能，其关系不限于卷积滤波器所响应的局部感受野。</em></p></blockquote><h1 id="3310" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">实施</strong></h1><p id="f494" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">挤压和激励模块可以插入任何 CNN 架构。文章中的图 3 说明了如何在 ResNet 中使用 se 块:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/f8b5caaf7d0c88b3d381d09ee0081b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*49sPwGJ-_asithzP"/></div></figure><p id="f421" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SE 块不会增加模型的计算复杂度太多。在以下比较中，“SE-ResNet-50”是指添加了 SE 模块的 ResNet-50 型号，而“vanilla ResNet-50”是指没有任何 SE 模块的基准 ResNet-50:</p><ul class=""><li id="aaf8" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">SE-ResNet-50 需要约 3.87 GFLOPs，而 vanilla ResNet-50 需要约 3.86 GFLOPs，相对增加了 0.26%(对于 224 x 224 像素输入图像的单次向前传递)；</li><li id="1866" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">SE-ResNet-50 来回一次需要 209 毫秒，而 vanilla ResNet-50 需要 190 毫秒，相对增加了 10%(在具有 8 个 NVIDIA Titan X GPUs 的服务器上，256 个图像的训练小批量的时间)；</li><li id="3c43" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">SE-ResNet-50 需要大约 2750 万个参数，而 vanilla ResNet-50 需要大约 2500 万个参数，相对增加了 10%。实际上，这些参数中的大部分来自网络的最后一级，在那里激励操作在最大数量的通道上执行。[……]这种相对昂贵的 SE 模块的最后一级可以以很小的性能成本去除(&lt;0.1% top-5 error on ImageNet) reducing the relative parameter increase to ~4%”</li></ul><p id="13d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Here’s an example implementation of an SE block (<a class="ae ky" rel="noopener" target="_blank" href="/squeeze-and-excitation-networks-9ef5e71eacd7">来源</a>):</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="603b" class="nv lw it nr b gy nw nx l ny nz"><em class="nh">def se_block(in_block, ch, ratio=16):<br/>     x = GlobalAveragePooling2D()(in_block)<br/>     x = Dense(ch//ratio, activation=’relu’)(x)<br/>     x = Dense(ch, activation=’sigmoid’)(x)<br/>     return multiply()([in_block, x])</em></span></pre><h1 id="6e40" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">结果</strong></h1><p id="4b93" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">作者将 SE 块添加到 ResNet 架构、ResNeXt 架构、VGG-16 和 Inception 架构，并表明 SE 块的添加提高了 ImageNet 分类的性能。性能改进显示在表 2 中 SENet 列中每个条目旁边的括号中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/4df920e8ea1b7295ce86105abac8c8c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*DNgNg5TtxVEahqFw"/></div></figure><p id="4d44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果部分的其他要点:</p><ul class=""><li id="4431" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">SE 块还提高了 CIFAR-10 和 CIFAR-100 数据集、用于场景分类的 Places365-Challenge 数据集以及用于对象检测的 COCO 数据集的模型性能。</li><li id="849c" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">对于减速比<em class="nh"> r </em>的不同值的范围，性能是稳定的。“增加的复杂性不会单调地提高性能，而较小的比率会显著增加模型的参数大小。设置<em class="nh"> r= </em> 16 在准确性和复杂性之间取得了很好的平衡。”</li><li id="a2c0" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">对于 squeeze 操作符，全局平均池的性能略好于全局最大池。</li><li id="864b" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">激励算子内部有一个 ReLU，最终的非线性是一个 s 形。用 ReLU 或 Tanh(而不是 sigmoid)代替激励算子的最终非线性会降低性能。</li></ul><h1 id="b4f0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">总结</strong></h1><ul class=""><li id="fe74" class="ms mt it lb b lc mn lf mo li oa lm ob lq oc lu mx my mz na bi translated">挤压和激励模块明确模拟渠道关系和渠道相互依赖性，并包括一种渠道自我关注的形式；</li><li id="6662" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">挤压和激励块使用全局平均池的“挤压”操作，继之以使用两个完全连接的层的“激励”操作，来重新校准特征图；</li><li id="6bb1" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">挤压和激励模块可以插入到任何 CNN 架构中，并且需要最小的计算开销；</li><li id="16b7" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">挤压和激励模块可以提高分类和目标检测任务的性能。</li></ul></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><p id="e97e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nh">原载于 2020 年 4 月 4 日 http://glassboxmedicine.com</em><em class="nh"/><a class="ae ky" href="https://glassboxmedicine.com/2020/04/04/squeeze-and-excitation-networks/" rel="noopener ugc nofollow" target="_blank"><em class="nh">。</em></a></p></div></div>    
</body>
</html>