<html>
<head>
<title>Bayesian Inference — Intuition and Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯推理——直觉和例子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-inference-intuition-and-example-148fd8fb95d6?source=collection_archive---------0-----------------------#2020-01-02">https://towardsdatascience.com/bayesian-inference-intuition-and-example-148fd8fb95d6?source=collection_archive---------0-----------------------#2020-01-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3d5b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Python 代码</h2></div><h1 id="5474" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">为什么有人要发明贝叶斯推理？</h1><p id="c098" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一句话:随着我们收集更多的数据，更新概率 <strong class="lc iu">。</strong></p><p id="f63c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">贝叶斯推理的核心是将两个不同的分布(似然和先验)组合成一个“更聪明”的分布(后验)。在经典的最大似然估计(MLE)没有考虑先验的意义上，后验是<strong class="lc iu">“更聪明”。</strong>一旦我们计算出后验概率，我们就用它来寻找“最佳”参数，而<strong class="lc iu">“最佳”就是在给定数据的情况下，最大化后验概率</strong><strong class="lc iu"/>。这个过程叫做<a class="ae mb" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">最大后验概率(MAP) </a>。MAP 中使用的优化和典型机器学习中使用的优化是一样的，比如梯度下降或者牛顿法等。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="61d1" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">当我研究著名的贝叶斯法则时，通过分析理解这个等式是相当容易的。但是你如何用数据实现它呢？</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mj"><img src="../Images/1e0aabd3624549634debdf7a3b61dfe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y5NJF_SyPD3ogYfWDSz1GQ.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">统计世界中最著名的方程式之一</p></figure><p id="1cdc" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">具体来说，<strong class="lc iu">我们会有大量的数据点 X，</strong>我们如何将<strong class="lc iu">概率 wrt X </strong>乘以<strong class="lc iu">概率 wrt θ？</strong></p><p id="6b60" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">显然，贝叶斯推理的艺术在于你如何实现它。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="1a9e" class="ki kj it bd kk kl mz kn ko kp na kr ks jz nb ka ku kc nc kd kw kf nd kg ky kz bi translated">示例:</h1><p id="4fa3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">每天大约有 2000 名读者访问我的中型博客。有人看完文章鼓掌有人不鼓掌。我想<strong class="lc iu">预测一下，当我将来写一篇新的博文时，会有百分之多少的人参与进来并鼓掌</strong>。</p><p id="e102" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这类问题适用面广。尝试将这一点应用到你自己的建模工作中——广告的点击率、在你的网站上实际购买的客户的转化率、同意与你约会的人的百分比、患乳腺癌的女性的生存机会等。</p><h1 id="824c" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">生成数据:</h1><p id="c768" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们生成数据<strong class="lc iu"> X </strong>。现实生活中，你对<strong class="lc iu"> X 没有任何控制</strong>这是你要观察的。</p><pre class="mk ml mm mn gt ne nf ng nh aw ni bi"><span id="33f0" class="nj kj it nf b gy nk nl l nm nn">import numpy as np<br/>np.set_printoptions(threshold=100)</span><span id="f657" class="nj kj it nf b gy no nl l nm nn"># Generating 2,000 readers' reponse. <br/># Assuming the claps follow a Bernoulli process - a sequence of binary (success/failure) random variables.<br/># 1 means clap. 0 means no clap.</span><span id="7409" class="nj kj it nf b gy no nl l nm nn"><strong class="nf iu"># We pick the success rate of 30%.</strong><br/><strong class="nf iu">clap_prob = 0.3</strong></span><span id="7a4a" class="nj kj it nf b gy no nl l nm nn"># IID (independent and identically distributed) assumption<br/><strong class="nf iu">clap_data = np.random.binomial(n=1, p=clap_prob, size=2000)</strong></span></pre><p id="dd0f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">拍手数据是二进制的。1 表示拍手，0 表示没有拍手。</p><p id="4a69" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">数据<strong class="lc iu"> X </strong>看起来像:</p><pre class="mk ml mm mn gt ne nf ng nh aw ni bi"><span id="ddcd" class="nj kj it nf b gy nk nl l nm nn">In [1]: clap_data<br/>Out[1]: array([0, 0, 0, ..., 0, 1, 0])</span><span id="acc5" class="nj kj it nf b gy no nl l nm nn">In [2]: len(clap_data)<br/>Out[2]: 2000</span></pre></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="aa81" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">贝叶斯推理有三个步骤。</p><p id="993b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">第一步。<strong class="lc iu">【先验】选择一个 PDF 来建模</strong>你的参数<strong class="lc iu"> θ </strong>，又名<strong class="lc iu"> </strong>先验分布<strong class="lc iu"> P(θ) </strong>。这是<strong class="lc iu">你在 </strong>看到数据<strong class="lc iu"> X </strong>之前关于参数<strong class="lc iu"> <em class="np">的最佳猜测。</em></strong></p><p id="589b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">第二步。<strong class="lc iu">【可能性】为<strong class="lc iu"> P(X|θ) </strong>选择一个 PDF </strong>。基本上你是在给定参数<strong class="lc iu"> θ </strong>的情况下模拟数据<strong class="lc iu"> X </strong>的样子。</p><p id="4478" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">第三步。<strong class="lc iu">【后验】计算后验</strong>分布<strong class="lc iu"> P(θ|X) </strong>取 P(θ|X)最高的<strong class="lc iu"> θ。</strong></p><p id="3139" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">后验成为新的先验。获得更多数据后，重复第 3 步。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="5de6" class="ki kj it bd kk kl mz kn ko kp na kr ks jz nb ka ku kc nc kd kw kf nd kg ky kz bi translated">第一步。先验 P(θ)</h1><p id="b529" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">第一步是<strong class="lc iu">选择 PDF </strong>对参数<strong class="lc iu"> θ </strong>建模。</p><blockquote class="nq nr ns"><p id="1d5a" class="la lb np lc b ld lw ju lf lg lx jx li nt ly ll lm nu lz lp lq nv ma lt lu lv im bi translated"><strong class="lc iu">参数θ代表什么？</strong></p></blockquote><p id="43ba" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">鼓掌声👏概率。</p><blockquote class="nq nr ns"><p id="b744" class="la lb np lc b ld lw ju lf lg lx jx li nt ly ll lm nu lz lp lq nv ma lt lu lv im bi translated">那么，<strong class="lc iu">我们应该用什么样的概率分布来建模一个概率呢？</strong></p></blockquote><p id="d8a8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">要表示一个概率，需要满足几个条件。首先，域的范围应该是从 0 到 1。第二，应该是连续分布。</p><p id="515a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">那么我能想到的有两个众所周知的概率分布:</p><p id="5c89" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">贝塔和狄利克雷。</p><p id="304f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">狄利克雷用于多变量，贝塔用于单变量。我们只有一件事可以预测，那就是一个概率，所以我们用<a class="ae mb" rel="noopener" target="_blank" href="/beta-distribution-intuition-examples-and-derivation-cf00f4db57af">贝塔分布</a>。</p><p id="840c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">(一个有趣的旁注:很容易在(0，1)上创建任意分布。取任何一个在 0 到 1 之间不爆炸的函数，保持正的。然后，简单的从 0 到 1 积分，用那个结果除函数。)</p><p id="4d34" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">要使用 Beta 分布，我们需要确定两个参数<strong class="lc iu"> α &amp; β </strong>。你可以把<strong class="lc iu"> α </strong>想成有多少人鼓掌(<strong class="lc iu">成功数</strong>)<strong class="lc iu">β</strong>想成有多少人不鼓掌(<strong class="lc iu">失败数</strong>)。这些参数——α<strong class="lc iu">α&amp;β</strong>的大小——将决定分布的形状。</p><p id="d311" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">假设您有昨天的数据，并观察到 2000 名访客中有 400 人鼓掌。</p><p id="d576" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">根据 beta 分布你会怎么写呢？</p><pre class="mk ml mm mn gt ne nf ng nh aw ni bi"><span id="8a16" class="nj kj it nf b gy nk nl l nm nn">import scipy.stats as stats<br/>import matplotlib.pyplot as plt</span><span id="596c" class="nj kj it nf b gy no nl l nm nn"><strong class="nf iu">a = 400<br/>b = 2000 - a</strong></span><span id="14ec" class="nj kj it nf b gy no nl l nm nn"># domain θ<br/>theta_range = np.linspace(0, 1, 1000)</span><span id="fb94" class="nj kj it nf b gy no nl l nm nn"># prior distribution P(θ)<br/>prior = <strong class="nf iu">stats.beta.pdf(x = theta_range, a=a, b=b)</strong></span></pre><p id="27d3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">让我们画出所有<strong class="lc iu"> θ </strong>值的先验分布。</p><pre class="mk ml mm mn gt ne nf ng nh aw ni bi"><span id="03bf" class="nj kj it nf b gy nk nl l nm nn"># Plotting the prior distribution<br/>plt.rcParams['figure.figsize'] = [20, 7]<br/>fig, ax = plt.subplots()<br/><strong class="nf iu">plt.plot(theta_range, prior,</strong> linewidth=3, color='palegreen')</span><span id="28f9" class="nj kj it nf b gy no nl l nm nn"># Add a title<br/>plt.title('[Prior] PDF of "Probability of Claps"', fontsize=20)</span><span id="28b7" class="nj kj it nf b gy no nl l nm nn"># Add X and y Label<br/>plt.xlabel('θ', fontsize=16)<br/>plt.ylabel('Density', fontsize=16)</span><span id="8ff1" class="nj kj it nf b gy no nl l nm nn"># Add a grid<br/>plt.grid(alpha=.4, linestyle='--')</span><span id="7688" class="nj kj it nf b gy no nl l nm nn"># Show the plot<br/>plt.show()</span></pre><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nw"><img src="../Images/6b4496b495233de75b272275f22437be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hSVCIUFwPIrddg9L8gPu0w.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">不熟悉 Y 轴上的术语“密度”？→ <a class="ae mb" rel="noopener" target="_blank" href="/pdf-is-not-a-probability-5a4b8a5d9531">阅读“PDF 不是概率”</a></p></figure><p id="b56b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">正如预期的那样，它的峰值达到了 20% (400 次鼓掌/ 2000 个读者)。两千个数据点似乎产生了一个强有力的先验。如果我们使用<strong class="lc iu">更少的数据点</strong>，比如说 100 个阅读器，曲线将会不那么尖锐。用<strong class="lc iu">α</strong>= 20&amp;<strong class="lc iu">β</strong>= 80 试试。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="2e8e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">对于那些想知道概率密度怎么会大于 1 的人。👉<a class="ae mb" rel="noopener" target="_blank" href="/pdf-is-not-a-probability-5a4b8a5d9531">概率密度不是概率。</a></p><h1 id="b241" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">第二步。可能性 P(X|θ)</h1><p id="0fe3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">选择</strong>一个概率模型<strong class="lc iu"> </strong>为<strong class="lc iu"> P(X|θ) </strong>，<strong class="lc iu"> </strong>给定特定参数<strong class="lc iu"> θ </strong>看到数据的概率<strong class="lc iu"> X </strong>。可能性也称为<strong class="lc iu">抽样分布</strong>。对我来说，“抽样分布”这个术语比“可能性”更直观。</p><p id="d37e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">要选择使用哪种概率分布来模拟抽样分布，我们首先需要问:</p><blockquote class="nq nr ns"><p id="9dcd" class="la lb np lc b ld lw ju lf lg lx jx li nt ly ll lm nu lz lp lq nv ma lt lu lv im bi translated">我们的数据<strong class="lc iu"> X </strong>是什么样子的？</p></blockquote><p id="dca1" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu"> X </strong>是二进制数组<code class="fe nx ny nz nf b">[0,1,0,1,...,0,0,0,1]</code>。</p><p id="0810" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们还有<strong class="lc iu">访客总数(n) </strong>和我们想要<strong class="lc iu">鼓掌的概率(p)。</strong></p><blockquote class="nq nr ns"><p id="7b2f" class="la lb np lc b ld lw ju lf lg lx jx li nt ly ll lm nu lz lp lq nv ma lt lu lv im bi translated">好的，<strong class="lc iu">n&amp;p</strong>……<strong class="lc iu"/>他们对你尖叫什么？</p></blockquote><p id="45fc" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">n&amp;p</strong>的二项分布。</p><pre class="mk ml mm mn gt ne nf ng nh aw ni bi"><span id="134b" class="nj kj it nf b gy nk nl l nm nn"># The sampling dist P(X|θ) with a prior θ</span><span id="961e" class="nj kj it nf b gy no nl l nm nn">likelihood = <strong class="nf iu">stats.binom.pmf(k = np.sum(clap_data), n = len(clap_data), p = a/(a+b))</strong></span></pre><p id="69c3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们之前的假设<strong class="lc iu"> θ </strong>可能性大吗？</p><pre class="mk ml mm mn gt ne nf ng nh aw ni bi"><span id="98d7" class="nj kj it nf b gy nk nl l nm nn">In [63]: likelihood</span><span id="11f6" class="nj kj it nf b gy no nl l nm nn">Out[63]: 4.902953768848812e-30</span></pre><p id="d9df" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">没有。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="9cba" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们来看一下<strong class="lc iu"> P(X|θ) </strong>对于所有可能的<strong class="lc iu"> θ </strong>的曲线图。</p><pre class="mk ml mm mn gt ne nf ng nh aw ni bi"><span id="761c" class="nj kj it nf b gy nk nl l nm nn"># Likelihood P(X|θ) <strong class="nf iu">for all θ's</strong><br/>likelihood = stats.binom.pmf(k = np.sum(clap_data), n = len(clap_data), <strong class="nf iu">p = theta_range</strong>)</span><span id="bad6" class="nj kj it nf b gy no nl l nm nn"># Create the plot<br/>fig, ax = plt.subplots()<br/><strong class="nf iu">plt.plot(theta_range, likelihood,</strong> linewidth=3, color='yellowgreen')</span><span id="ca9f" class="nj kj it nf b gy no nl l nm nn"># Add a title<br/>plt.title('[Likelihood] Probability of Claps' , fontsize=20)</span><span id="11f0" class="nj kj it nf b gy no nl l nm nn"># Add X and y Label<br/>plt.xlabel(’θ’, fontsize=16)<br/>plt.ylabel(’Probability’, fontsize=16)</span><span id="7728" class="nj kj it nf b gy no nl l nm nn"># Add a grid<br/>plt.grid(alpha=.4, linestyle='--')</span><span id="20b7" class="nj kj it nf b gy no nl l nm nn"># Show the plot<br/>plt.show()</span></pre><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/94b40c79e208bc020331e0ab7f301c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nR4MrowsRcP_RlljLMs2fw.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated"><strong class="bd ob">给定数据的可能性 P(X |θ)</strong></p></figure><h1 id="eb55" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">第三步。后。P(θ|X)</h1><p id="a16d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最后，让我们来回答我们在开始时提出的问题:</p><blockquote class="nq nr ns"><p id="3ce3" class="la lb np lc b ld lw ju lf lg lx jx li nt ly ll lm nu lz lp lq nv ma lt lu lv im bi translated">具体来说，<strong class="lc iu">我们会有大量的数据点 X，</strong>我们如何将<strong class="lc iu">概率 wrt X </strong>乘以<strong class="lc iu">概率 wrt θ？</strong></p></blockquote><p id="8e79" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">即使有数千个数据点，我们也可以通过将数据插入您选择的模型(在本例中，二项式分布)中，将它们转换为单个标量——可能性<strong class="lc iu"> P(X|θ) </strong> — <strong class="lc iu">。)</strong></p><p id="94eb" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">然后，我们对一个具体的<strong class="lc iu"> θ </strong>计算<strong class="lc iu">P(θ)</strong>&amp;<strong class="lc iu">P(X |θ)</strong>，并将它们相乘。如果你对每一个可能的<strong class="lc iu"> θ </strong>都这样做，你可以在不同的<strong class="lc iu"> θ </strong>的<strong class="lc iu">中挑选最高的<strong class="lc iu"> P(θ) * P(X|θ) </strong>。</strong></p><p id="ed1c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">你最初对参数的猜测是<strong class="lc iu"> P(θ)。</strong>现在你正在<strong class="lc iu">将简单的 P(θ)升级为更具信息性的东西——P(θ| X)——随着更多数据的可用。</strong> <br/> <strong class="lc iu"> P(θ|X) </strong>仍然是<strong class="lc iu"> θ </strong>的概率，就像<strong class="lc iu"> P(θ) </strong>是一样。<strong class="lc iu"> </strong>不过，<strong class="lc iu"> P(θ|X) </strong>是<strong class="lc iu"> P(θ) </strong>更聪明的版本。</p><p id="13c9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">代码胜过千言万语:</strong></p><pre class="mk ml mm mn gt ne nf ng nh aw ni bi"><span id="f2fb" class="nj kj it nf b gy nk nl l nm nn"># (cont.)</span><span id="85cb" class="nj kj it nf b gy no nl l nm nn">theta_range_e = theta_range + 0.001 </span><span id="911e" class="nj kj it nf b gy no nl l nm nn">prior = <strong class="nf iu">stats.beta.cdf(x = theta_range_e, a=a, b=b) - stats.beta.cdf(x = theta_range, a=a, b=b) <br/></strong># prior = stats.beta.pdf(x = theta_range, a=a, b=b)</span><span id="09e8" class="nj kj it nf b gy no nl l nm nn">likelihood = <strong class="nf iu">stats.binom.pmf(k = np.sum(clap_data), n = len(clap_data), p = theta_range) </strong></span><span id="d141" class="nj kj it nf b gy no nl l nm nn">posterior = <strong class="nf iu">likelihood * prior </strong># element-wise multiplication<br/>normalized_posterior = posterior / np.sum(posterior)</span></pre></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><pre class="ne nf ng nh aw ni bi"><span id="9ee3" class="nj kj it nf b gy oc od oe of og nl l nm nn">In [74]: np.argmax(prior)<br/>Out[74]: 199</span><span id="9794" class="nj kj it nf b gy no nl l nm nn">In [75]: np.argmax(likelihood)<br/>Out[75]: 306</span><span id="3f19" class="nj kj it nf b gy no nl l nm nn">In [76]: np.argmax(posterior)<br/>Out[76]: 253</span></pre></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><pre class="ne nf ng nh aw ni bi"><span id="ca04" class="nj kj it nf b gy oc od oe of og nl l nm nn"># Plotting all three together<br/>fig, axes = plt.subplots(3, 1, sharex=True, figsize=(20,7))<br/>plt.xlabel('θ', fontsize=24)</span><span id="e743" class="nj kj it nf b gy no nl l nm nn"><strong class="nf iu">axes[0].plot(theta_range, prior,</strong> label="<strong class="nf iu">Prior</strong>", linewidth=3, color='palegreen')<br/>axes[0].set_title("Prior", fontsize=16)</span><span id="7f0c" class="nj kj it nf b gy no nl l nm nn"><strong class="nf iu">axes[1].plot(theta_range, likelihood</strong>, label="<strong class="nf iu">Likelihood</strong>", linewidth=3, color='yellowgreen')<br/>axes[1].set_title("Sampling (Likelihood)", fontsize=16)</span><span id="fa5a" class="nj kj it nf b gy no nl l nm nn"><strong class="nf iu">axes[2].plot(theta_range, posterior,</strong> label='<strong class="nf iu">Posterior</strong>', linewidth=3, color='olivedrab')<br/>axes[2].set_title("Posterior", fontsize=16)<br/>plt.show()</span></pre><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oh"><img src="../Images/81efa5f90fcc9002e263715bb2d32801.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZX5rV10YByYZe8x9YHkidw.png"/></div></div></figure><p id="c4f3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">当你看后验图(第三张)时，<strong class="lc iu">注意这是可能性向先验转移的地方。</strong> <em class="np">先前</em>的鼓掌概率为 20%。数据的鼓掌概率被给定为 30%。现在，后验概率的峰值在 0.25%左右。</p><p id="4fc4" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">此外，请注意，在先验/可能性中，钟形曲线的宽度在后验中缩小了。因为我们通过采样整合了更多的信息，所以可能的参数范围现在变窄了。</p><p id="8b68" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">你收集的数据越多，后验概率图看起来就越像概率图，越不像先验概率图。换句话说，当你得到更多的数据时，原来的先验分布就不那么重要了。</p><p id="f213" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">最后，我们<strong class="lc iu">选择θ，其给出通过数值优化计算的最高后验</strong>，例如<a class="ae mb" rel="noopener" target="_blank" href="/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1">梯度下降</a>或牛顿法。这整个迭代过程被称为<strong class="lc iu">最大后验估计(MAP) </strong>。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="3731" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">脚注:我们通过减去两个<code class="fe nx ny nz nf b">stats.beta.<strong class="lc iu">cdf</strong></code>而不是使用<code class="fe nx ny nz nf b">stats.beta.<strong class="lc iu">pdf</strong></code>来计算先验，因为可能性<code class="fe nx ny nz nf b">stats.binom.<strong class="lc iu">pmf</strong></code> <strong class="lc iu"> </strong>是一个概率，而<code class="fe nx ny nz nf b">stats.beta.<strong class="lc iu">pdf</strong></code>返回一个<strong class="lc iu">密度</strong>。即使我们用密度来计算后验概率，也不会改变优化结果。然而，如果你想要单位匹配，<strong class="lc iu">将密度转换成概率</strong>是必要的。(还不清楚？<a class="ae mb" rel="noopener" target="_blank" href="/pdf-is-not-a-probability-5a4b8a5d9531">阅读“PDF 不是概率，而 PMF 是概率”。</a>)</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="5a07" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">需要注意一些事情:</p><ol class=""><li id="7b12" class="oi oj it lc b ld lw lg lx lj ok ln ol lr om lv on oo op oq bi translated">MAP 不仅可以通过数值优化来计算，还可以通过<strong class="lc iu"/>(仅适用于某些分布)、修改的期望最大化算法或蒙特卡罗方法来计算。</li><li id="3825" class="oi oj it lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">使用封闭形式计算后验概率非常方便，因为你不必进行昂贵的计算。(看贝叶斯公式分母中每一个可能的<strong class="lc iu"> θ </strong>的积分。非常贵。)<br/>在我们上面的例子中，贝塔分布是二项似然的共轭先验。这意味着，在建模阶段，我们已经知道后验也将是贝塔分布。那么在进行更多的实验后，我们只需将成功和失败的次数分别加到现有的参数<strong class="lc iu"> α </strong>和<strong class="lc iu"> β </strong>上，就可以计算出后验。<br/>我们什么时候可以将这种轻松的封闭形式用于后部？<br/>当你的先验有一个封闭形式(当然)并且是一个<strong class="lc iu">共轭先验</strong>——当先验乘以似然性再除以归一化常数，仍然是同一个分布。</li><li id="4ad4" class="oi oj it lc b ld or lg os lj ot ln ou lr ov lv on oo op oq bi translated">归一化(对分母中每个可能的<strong class="lc iu"> θ </strong>的积分)对于寻找最大后验概率是否必要<strong class="lc iu">？</strong></li></ol><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mj"><img src="../Images/1e0aabd3624549634debdf7a3b61dfe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y5NJF_SyPD3ogYfWDSz1GQ.png"/></div></div></figure><p id="2bff" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">没有。<strong class="lc iu">不用归一化</strong>还是能找到最大值。但是，如果您想比较不同模型的后验概率，或者计算点估计，您需要将其归一化。</p><p id="17a9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">4.MAP 估计可以被看作是通过先验引入更多信息的正则化 ML。</p><p id="1d23" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">5.你熟悉最大似然估计(MLE)，但不太熟悉最大后验概率(MAP)吗？<br/> <strong class="lc iu"> MLE 只是具有统一先验的映射的一个特例</strong>。</p><p id="75c8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">6.我写了<em class="np"/>先验是你最好的猜测<strong class="lc iu"> </strong>关于参数* <strong class="lc iu"> <em class="np">在* </em> </strong>看到数据之前<em class="np">，</em>然而在实际操作中，一旦我们计算了后验，后验就变成了新的先验，直到新的一批数据进来。这样，我们可以迭代地更新我们的先验和后验。马尔可夫链蒙特卡罗抽样方法就是基于这种思想。</p></div></div>    
</body>
</html>