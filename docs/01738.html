<html>
<head>
<title>Building a Reverb Detection System using Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习构建混响检测系统</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-reverb-detection-system-using-machine-learning-cba02a1710bf?source=collection_archive---------25-----------------------#2020-02-17">https://towardsdatascience.com/building-a-reverb-detection-system-using-machine-learning-cba02a1710bf?source=collection_archive---------25-----------------------#2020-02-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/e5f3d64cfb220070f672aa98cbeb9247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*P4AQM-CyIb_uhNzvRAVMrw.jpeg"/></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">德尔塔-梅尔光谱图</p></figure><h1 id="06c3" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">介绍</h1><p id="9deb" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">混响或简称混响是声学和心理声学中的一个术语，用来解释声音一旦产生后的持久性。一个例子是在一个大教堂里拍手(很有毅力)，而不是在一个铺着地毯的小房间里拍手(没有那么多毅力)。</p><p id="b4af" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">这是由构成声音产生环境的材料的反射和吸收造成的。与石头或砖造的大教堂相比，铺着地毯、有窗帘和家具的房间会吸收更多的声音。</p><p id="0af7" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">我们想要检测这一点的原因是，如果有人在过于混响的环境中对着手机说话或唱歌。我们可能希望能够警告进行录音的人，因为这可能会降低录制的声音质量。</p><p id="63be" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">这里有两个声音示例，分别演示了混响很小的录音和混响很大的录音。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">混响很小。</p></figure><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">很多混响。</p></figure><p id="93f7" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">我们实际上试图用我们的机器学习(ML)模型来测量的是混响衰减时间。这可以定义为声音逐渐消失所需要的时间，以秒为单位。这种情况的技术术语是RT-60时间，即声音的压力水平下降60db所需的时间。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mi"><img src="../Images/d0d9f3acd37dcf20a5c0e335e989a61d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aDll_JF8JQ5xalFID8Rqpw.png"/></div></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">图一。这说明了RT-60的衰变时间(来源:Wkipedia)。</p></figure><p id="a3f4" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">参考上面的两段录音，第二段录音听起来像是有人在小浴室里唱歌。在浴室唱歌的效果是歌手被所有回荡的声音淹没。这是不可取的。</p><p id="aeef" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">了解RT-60时间的另一个用例是，它可以作为另一个ML系统的输入，用于去混响信号。这种系统的一个例子可以在[1]中找到。</p><h1 id="e470" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">我们为什么要做这个项目？</h1><p id="25ba" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在<a class="ae mn" href="http://www.aimusic.co.uk" rel="noopener ugc nofollow" target="_blank">艾。我们正在开发一个允许用户对着手机唱歌的系统。我们将随后使用ML应用音频清理过程来处理歌手的声音。清理过程的目的是使歌手的声音听起来尽可能接近专业录音室录制的声音。然而，在用真实世界的数据进行测试时，我们发现用户在有太多混响的地方录音，比如浴室。这导致音频清理过程出现不良结果，我们需要降低该过程的风险。</a></p><h1 id="2780" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">我们是如何解决这个问题的？</h1><p id="152e" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们提出了一个使用卷积神经网络(CNN)来解决问题的ML模型，其中我们设计了自己的卷积滤波器来解决问题。我们实际上采用了[2]中的模型来解决我们的问题。我们还通过脉冲响应卷积生成了自己的数据集。</p><h1 id="b708" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">我们用了什么数据？</h1><p id="1411" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们收集了大约4000份人们唱歌和谈话的“干”录音。我们所说的“干燥”是指它们是在几乎没有声音反射(即混响)的环境中录制的。</p><p id="bf63" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">我们使用“干”录音的原因是，我们可以使用不同的RT-60时间为每个录音人工添加混响，即我们可以给1号录音1秒的RT-60时间，给2号录音1.1秒的RT-60时间。</p><p id="4470" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">我们通过称为卷积的过程将人工混响添加到每个信号中。我们取一个RT-60时间为1秒的脉冲响应，并将其与“干”信号卷积，得到“湿”信号。</p><p id="66d6" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">脉冲响应是指我们获取一个短暂的“脉冲”信号，并使其通过一个动态系统，以测量其输出，即其响应。为了捕捉音乐厅或大教堂的声学特征，我们会在空间中产生一个脉冲(我们刺破一个气球)并记录下来。一旦我们有了一个位置的脉冲响应，我们就可以用另一个录音(有人唱歌)进行卷积。这让我们能够听到他们在特定空间的声音。很酷吧？</p><p id="a90e" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">从OpenAIR数据集[3]中收集了70个脉冲响应，我们的模型的RT60s从0.24秒到12.3秒不等。OpenAIR是一个允许用户共享脉冲响应和相关声学信息的在线资源。</p><h1 id="0318" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">我们是如何创建数据集的？</h1><p id="7833" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们将4000个声音记录中的每一个与所有70个脉冲响应进行卷积，并根据其RT-60时间标记每个数据点。这给了我们总共280，000个不同RT-60时间的记录。</p><p id="b0b6" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">为了模拟真实的背景噪声，我们还在每个记录中随机添加了信噪比为-55db和-45db的粉红噪声。</p><p id="dc64" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">我们为所有RT-60时间创建了分类箱，这允许我们将它们分成20个不同的类。图2显示了RT-60时间的分布，表1描述了如何将它们放置到时间仓中。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/0f95ebaa48a8a5d67f00ca8a25e42a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*mJHQ-epIfWUIdhqA90oqdA.png"/></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">图2:所用数据集中的RT-60时间分布。</p></figure><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mp"><img src="../Images/d4a6ef3584846770f8129ac6aad96d55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Azs29_KFl9W9em8xMydGg.png"/></div></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">表1:用于RT-60分类器的时间仓。</p></figure><h1 id="1f3c" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">特征抽出</h1><p id="a9ab" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">接下来，我们必须提取相关特征，以获得音频的良好表示。我们最初选择使用Mel声谱图，因为这些声谱图很好地代表了声音的音色。</p><p id="b4a1" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">音色可以描述为声音的颜色。如果我在钢琴上弹奏一个中C音，然后在吉他上弹奏相同的音，它们的音高相同，但听起来不同。这种声音上的差异就是音色。</p><p id="7a02" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">Mel光谱图看起来像图3的左侧。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mq"><img src="../Images/5fee478ce45e1ac9c0ff747738394590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gbj0Wzvroiej0wfJ7j5YjA.png"/></div></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">图3:同一声音的Mel声谱图和delta-Mel声谱图。</p></figure><p id="665b" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">我们可以清楚地看到声音随着时间的推移而衰减。尾巴最长的部分在3-6秒。</p><p id="0ec6" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">在我们的特殊情况下，我们没有使用标准的Mel光谱图进行训练，而是使用delta-Mel光谱图。Delta-Mel频谱图是音色随时间变化的微分或轨迹。我们认为这将更好地呈现应用于每个人声的不同RT-60时间。</p><p id="359c" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">图3右手侧示出了δ-Mel谱图。你可以看到delta-Mel谱图中出现的视觉信息比Mel谱图中多得多。</p><p id="8eb0" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">一旦所有这些特征都被提取出来，我们需要把它们提供给CNN进行训练。我们用于特征提取的参数如下。参数:M = 40个梅尔频带，N = 2381帧，𝑓s = 44.1，1024样本/帧，布莱克曼-哈里斯窗口，2048样本FFT大小，1024样本跳跃大小。</p><h1 id="7f28" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">ML模型</h1><p id="bc8e" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">如前所述，我们使用CNN作为我们的ML模型，因为这种类型的ML模型已经被证明在音频和图像分类上都是有效的。我们使用了[2]中提出的滤波器模型，但是我们根据我们的问题调整了尺寸。</p><p id="154a" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">E-net滤波器模拟对应于早期反射的时间特征:</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mr"><img src="../Images/ea52df4d8a6d6bf25a79536396cee453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IL2F6yfImESaI5bhgE5EYg.jpeg"/></div></div></figure><p id="b074" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">R-net滤波器对与可接受的记录反射相对应的时间特征进行建模:</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ms"><img src="../Images/39e327aa700de5362fc96220946e8197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VU01BVyqpMytiFiQRiz0og.png"/></div></div></figure><p id="6554" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">T-net滤波器模拟对应于混响尾音中较长反射的时间特征:</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mt"><img src="../Images/d2b958c9f4254d91c2e03f6bc15e3ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b1BA3oRp63jPJuGULymvwA.png"/></div></div></figure><p id="d709" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">3个卷积层与ReLU激活功能并行放置(E-Net、R-Net、T-Net)。e网、r网、t网由1 × 𝑛滤波核组成，其中𝑛从短到长变化。</p><p id="fc4a" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">接下来是内核大小为4 x N的3个最大池层、1个平坦层和1个丢弃层，丢弃概率为50%，以避免过度拟合。</p><p id="b748" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">最后，有一个具有20路软最大激活函数的密集层对应于我们的类。</p><p id="6503" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">分类交叉熵被选为损失函数。Adam是最乐观的，初始学习率为0.01，在每个时期递减0.1。</p><p id="cdf1" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">选择80%训练/ 10%验证/ 10%测试分割，通过10重交叉验证来评估模型。该模型如图4所示。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mu"><img src="../Images/819361e502478cc87d6aefcb2135c1ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AAUDdi8z0-ugLGUh5t0AJA.png"/></div></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">图Keras中RT-60估算模型的图示。</p></figure><h1 id="1222" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">结果</h1><p id="f49a" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">结果显示该模型对于更长的RT-60时间是最有效的，而当我们具有更短的时间时会有些混乱。结果可以在图5中看到</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mv"><img src="../Images/e91d940f0af92ec7dcafa1e9646c4a8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kDNRIT8Zh9Ll1hPZ"/></div></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">图5:用于测试数据的RT-60估计模型的标准化混淆矩阵。</p></figure><h1 id="c30f" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">结论</h1><p id="dae0" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">结果是令人鼓舞的，我们设计的系统的精确度相差不远。然而，我们肯定可以做更多的事情来改进模型，例如数据集分布。</p><p id="e145" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">我们发现我们使用的过滤器类型非常有效，并且通过使用delta-Mel光谱图，我们得到了略微更好的结果。我们计划在一个更复杂的去混响模型中使用模型来通知所需的RT-60时间估计，该模型用于我们在<a class="ae mn" href="http://www.aimusic.co.uk" rel="noopener ugc nofollow" target="_blank"> AI的另一个正在进行的项目。音乐</a>。如果你想了解更多我们的工作，请查看我们的时事通讯(【https://www.aimusic.co.uk/newsletter-sign-up】T2)。</p><h1 id="ee6d" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">参考</h1><p id="4546" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">[1] —吴，博等，“一种基于深度神经网络的混响时间感知语音去混响方法”(2016)，IEEE/ACM音频、语音和语言处理汇刊。</p><p id="339d" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">[2] — Pons，j .和Serra，x .，“利用卷积神经网络设计用于建模时间特征的高效架构”(2017)，声学、语音和信号处理国际会议(ICASSP)。</p><p id="b66c" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">[3] — Murphy，Damian T .和Shelley，Simon，“OpenAIR:交互式听觉化网络资源和数据库”(2010)，音频工程学会第129次会议(AES)。</p><h1 id="392a" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">感谢</h1><p id="e0f1" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">非常感谢方大卫(【https://www.linkedin.com/in/davidwengweifong/】)为这个项目付出的努力。</p></div></div>    
</body>
</html>