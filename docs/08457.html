<html>
<head>
<title>Systematic Bias in Artificial Intelligence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能中的系统偏差</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/systematic-bias-in-artificial-intelligence-8698ffa20d57?source=collection_archive---------42-----------------------#2020-06-19">https://towardsdatascience.com/systematic-bias-in-artificial-intelligence-8698ffa20d57?source=collection_archive---------42-----------------------#2020-06-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="355d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">照亮了明天最大的问题之一</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0caa3141cefc25befc38b8991ddb998c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPZap7X4nhqiLrkQZe-6wQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据真的有更好的想法吗？鸣谢:弗兰基·查马基</p></figure><p id="3b73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最近，詹妮弗·林肯医生做了一个<a class="ae lu" href="https://www.tiktok.com/@drjenniferlincoln/video/6835696093255322886?lang=en" rel="noopener ugc nofollow" target="_blank">抖音</a>强调了非裔美国人在医疗保健中面临歧视的多种方式，例如接受较少的止痛药和在急诊室等待更长时间。该视频基于发表在《国家科学院院刊》( PNAS)上的<a class="ae lu" href="https://www.pnas.org/content/early/2016/03/30/1516047113.abstract" rel="noopener ugc nofollow" target="_blank">研究</a>,已在抖音获得40万次点击量，在推特上获得近800万次点击量。如果人工智能模型根据医疗记录进行训练，以预测患者的止痛药剂量，它可能会建议非裔美国人患者使用更低的剂量，因为它是在非裔美国人患者接受更低剂量的数据集上训练的。显然，这将变得非常成问题，因为这种人工智能的假设用例将进一步使种族主义制度化。</p><h1 id="2cb7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">偏见的后果</h1><p id="1372" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">随着人工智能日益融入当前系统，系统偏差是一个不容忽视的重要风险。当模型输入的数据存在对某个种族或性别的偏见时，它们就不能有效地达到预期目的。根据准确性、利润等指标评估的模型将试图最大化所述指标，而不考虑其形成的偏差。如果不采取措施解决这个问题，更重要的是，人类或监管机构可能会对人工智能失去信心，阻止我们释放这项技术的潜力。为了理解这个问题的严重性，这里有两个更可怕的人工智能偏见的例子。</p><ul class=""><li id="705b" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">正如论文“<a class="ae lu" href="https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf" rel="noopener ugc nofollow" target="_blank">仇恨言论检测中种族偏见的风险</a>”中所概述的那样，华盛顿大学的研究人员在超过500万条推文中测试了谷歌的人工智能仇恨言论检测器，发现来自非洲裔美国人的推文比其他种族的人更容易被归类为有毒言论。</li><li id="593a" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">COMPAS(替代制裁的矫正罪犯管理概况)是一种算法，由纽约、加利福尼亚和其他州用于预测获释囚犯再次犯罪的风险。在研究文章“<a class="ae lu" href="https://advances.sciencemag.org/content/4/1/eaao5580" rel="noopener ugc nofollow" target="_blank">预测累犯的准确性、公平性和局限性</a>”中，达特茅斯大学的研究人员得出结论，“没有累犯的黑人被告被错误地预测为重新犯罪的比例为44.9%，是白人被告23.5%的近两倍；而曾经再犯的白人被告被错误地预测为不会再犯的比例为47.7%，几乎是黑人被告(28.0%)的两倍。考虑到COMPAS分数会影响被告的刑期，这是非常麻烦的。</li></ul><h1 id="bc60" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">战斗偏见</h1><p id="0ae6" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">许多人工智能批评者表达的一个担忧是人工神经网络的“黑盒”性质:机器学习模型可以为我们提出的问题提供答案，但由于涉及的计算非常复杂，我们无法理解该模型如何得出答案。这种不透明性让偏见悄悄潜入。即使抛开偏见，消费者/企业也有兴趣了解人工智能是如何得出结论的。</p><p id="0d71" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">解释人工智能如何做出高风险决策的一个潜在解决方案是可解释的机器学习。顾名思义，可解释机器学习涉及创建模型，其决策过程比黑盒模型更容易理解。为了变得可解释，这些模型被设计成具有诸如附加约束和领域专家的输入等因素。例如，防止贷款申请中的偏见的另一个约束是合规性:该模型必须遵守公平贷款法，不得歧视某一种族的消费者。</p><p id="ffea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然可解释的机器学习模型由于其复杂性的增加而更加耗时和昂贵，但对于自动驾驶汽车、医疗保健或刑事司法等错误会产生严重影响的应用来说，可解释性层绝对是值得的。人性使社会抵制变化，但更透明的模型可以开始使公众/政府更容易接受人工智能的广泛采用。</p><p id="5bee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其他潜在的解决方案关注模型使用的数据，而不是模型如何使用数据。一种提议的方法涉及获取通常保持机密的大型数据集(例如医疗数据)并在删除个人身份信息后向公众发布。这个想法是偏见将在这些匿名数据集中被过滤掉。然而，这种策略有其自身的风险，因为黑客可以交叉引用数据，使其通过匿名层。与此同时，有意识地纳入代表性不足的人群将支持缺乏多样性的数据集。最后，在设计这些算法的工程师中培养多样性应该有助于对抗偏见。</p><p id="f0e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">随着人工智能的快速发展，主动打击偏见仍然是当务之急。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="0b2f" class="lv lw it bd lx ly nn ma mb mc no me mf jz np ka mh kc nq kd mj kf nr kg ml mm bi translated">参考</h1><p id="fb1e" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">[1] Cynthia Rudin，<a class="ae lu" href="https://www.nature.com/articles/s42256-019-0048-x#Sec8" rel="noopener ugc nofollow" target="_blank">停止解释高风险决策的黑盒机器学习模型，转而使用可解释的模型</a>，《自然》</p><p id="1e31" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]克里斯托夫·莫尔纳尔，<a class="ae lu" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">可解释机器学习</a>，Github</p><p id="1041" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3]朱丽亚·德雷塞尔，<a class="ae lu" href="https://advances.sciencemag.org/content/4/1/eaao5580" rel="noopener ugc nofollow" target="_blank">《预测累犯的准确性、公平性和限度》</a>，科学进展</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="974b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p><p id="44f5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我是Roshan，16岁，对人工智能的应用充满热情。如果你对人工智能更感兴趣，可以看看我关于人工智能在医疗保健中的应用的文章。</p><p id="544b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在Linkedin上联系我:<a class="ae lu" href="https://www.linkedin.com/in/roshan-adusumilli/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/roshan-adusumilli/</a></p></div></div>    
</body>
</html>