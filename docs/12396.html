<html>
<head>
<title>LaBSE: Language-Agnostic BERT Sentence Embedding by Google AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LaBSE:由 Google AI 嵌入语言不可知的 BERT 句子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/labse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f?source=collection_archive---------16-----------------------#2020-08-26">https://towardsdatascience.com/labse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f?source=collection_archive---------16-----------------------#2020-08-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0528" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">谷歌人工智能如何将多语言句子嵌入的限制推向 109 种语言</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/18247bf89157b6515fb5ec70d8858628.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*yt42XNbroxLZacn465C8Qw.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">通过<a class="ae ku" href="https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a>的多语言嵌入空间</p></figure><p id="260e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">多语言嵌入模型是将多种语言的文本映射到共享向量空间(或嵌入空间)的模型。这意味着，在这个嵌入空间中，<strong class="kx iu">相关或相似的词将位于彼此更近的</strong>，而<strong class="kx iu">不相关的词将位于彼此更远的</strong>(参见上图)。</p><p id="4c8f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在这篇文章中，我们将讨论冯等人最近在 T18 提出的<strong class="kx iu">LaBSE</strong>:<strong class="kx iu">L</strong>anguage-<strong class="kx iu">A</strong>gnostic<strong class="kx iu">B</strong>ERT<strong class="kx iu">S</strong>entence<strong class="kx iu">E</strong>mbed ing。艾尔。这是句子嵌入的最新技术。</p><h1 id="9a71" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">现有方法</h1><p id="53d9" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">现有的方法大多涉及在大量并行数据上训练模型。型号如<a class="ae ku" href="https://research.fb.com/downloads/laser-language-agnostic-sentence-representations/" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu">激光:</strong> <strong class="kx iu"> L </strong>语言-<strong class="kx iu">A</strong>gnostic<strong class="kx iu">SE</strong>tence<strong class="kx iu">R</strong>presentations</a>和<a class="ae ku" href="https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu"> m-USE: </strong> <strong class="kx iu"> M </strong>多语言<strong class="kx iu"> U </strong>通用<strong class="kx iu"> S </strong> entence <strong class="kx iu"> E </strong>编码器</a>它们在多种语言中表现得相当好。然而，它们的表现不如专用的双语建模方法，如<a class="ae ku" href="https://www.aclweb.org/anthology/W18-6317/" rel="noopener ugc nofollow" target="_blank">翻译排名</a>(我们即将讨论)。此外，由于有限的训练数据(尤其是低资源语言)和有限的模型容量，这些模型<strong class="kx iu">不再支持更多的语言</strong>。</p><p id="a40b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">NLP 的最新进展表明，在掩蔽语言建模(MLM)或类似的预训练目标上训练语言模型，然后在下游任务上对其进行微调。像<a class="ae ku" href="https://arxiv.org/abs/1901.07291" rel="noopener ugc nofollow" target="_blank"> XLM </a>这样的模型是对 MLM 目标的扩展，但是是在跨语言环境下。这些对下游任务很有效，但是<strong class="kx iu">由于缺少句子级目标而产生了糟糕的句子级嵌入</strong>。</p><blockquote class="mo mp mq"><p id="4a24" class="kv kw mr kx b ky kz ju la lb lc jx ld ms lf lg lh mt lj lk ll mu ln lo lp lq im bi translated">相反，与其他下游任务类似，从 MLMs 产生的句子嵌入必须通过微调来学习。</p><p id="5c31" class="kv kw mr kx b ky kz ju la lb lc jx ld ms lf lg lh mt lj lk ll mu ln lo lp lq im bi translated">— <a class="ae ku" href="https://arxiv.org/abs/2007.01852" rel="noopener ugc nofollow" target="_blank"> LaBSE 论文</a></p></blockquote><h1 id="317b" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">语言无关的 BERT 句子嵌入</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mv"><img src="../Images/6ae24ae3513873effab9077fe6430e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e3Mv90n4tJUm_wzjt6rpyg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">双向双编码器，带附加余量 Softmax 和通过<a class="ae ku" href="https://arxiv.org/abs/2007.01852" rel="noopener ugc nofollow" target="_blank"> LaBSE 纸</a>共享的参数</p></figure><p id="001a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">所提出的架构是基于一个<strong class="kx iu">双向双编码器</strong> <a class="ae ku" href="https://www.aclweb.org/anthology/W18-6317/" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu">(郭等。艾尔。</strong> </a> <strong class="kx iu">)与加性边距 Softmax ( </strong> <a class="ae ku" href="https://www.ijcai.org/Proceedings/2019/746" rel="noopener ugc nofollow" target="_blank"> <strong class="kx iu">)杨等人</strong> </a> <strong class="kx iu"> ) </strong>与改进。在接下来的几个小节中，我们将深入解读该模型:</p><h2 id="366f" class="na ls it bd lt nb nc dn lx nd ne dp mb le nf ng md li nh ni mf lm nj nk mh nl bi translated">翻译排名任务</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/eff233ca68a140a73ba41f049631a1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/0*FMRBhCtd5X06Dm3R.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">通过<a class="ae ku" href="https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a>的翻译排名任务</p></figure><p id="c503" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><a class="ae ku" href="https://www.aclweb.org/anthology/W18-6317/" rel="noopener ugc nofollow" target="_blank">先事后人，郭等人。艾尔。</a>使用翻译排序任务，本质上<strong class="kx iu">按照与源</strong>的兼容性对所有目标句子进行排序。这通常不是“所有”的句子，而是一些“<strong class="kx iu"> <em class="mr"> K - 1 </em> </strong>”的句子。目标是<strong class="kx iu">最大化源句子与其真实翻译</strong>之间的兼容性<strong class="kx iu">最小化与其他</strong> ( <strong class="kx iu">负采样</strong>)。</p><h2 id="7a5d" class="na ls it bd lt nb nc dn lx nd ne dp mb le nf ng md li nh ni mf lm nj nk mh nl bi translated">双向双编码器</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e5a6790347a3469f9d2e4b5c7ae4dda2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*Ptx5rgldNR4BgXfgDwfmPg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">双编码器架构经<a class="ae ku" href="https://www.aclweb.org/anthology/W18-6317/" rel="noopener ugc nofollow" target="_blank">郭等人。艾尔。</a></p></figure><p id="08a1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">双编码器架构本质上使用<strong class="kx iu">并行编码器</strong>对两个序列进行编码，然后使用<strong class="kx iu">点积</strong>获得两种编码之间的<strong class="kx iu">兼容性得分</strong>。<a class="ae ku" href="https://www.aclweb.org/anthology/W18-6317/" rel="noopener ugc nofollow" target="_blank">郭等人的模特。艾尔。</a>基本上是在一个平行语料库上训练的，用于上一节讨论的翻译分级任务。</p><p id="48e9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">就“双向”而言；它基本上在两个“方向”上获取兼容性得分，即从<strong class="kx iu">源到目标以及从目标到源</strong>。例如，如果从源<strong class="kx iu"> <em class="mr"> x </em> </strong>到目标<strong class="kx iu"> <em class="mr"> y </em> </strong> <em class="mr"> </em>的兼容性由<strong class="kx iu"><em class="mr">【y _ I)</em></strong><em class="mr"/>表示，则分数<strong class="kx iu"><em class="mr">【ɸ(y_i，x_i) </em> </strong> <em class="mr"> </em>也被考虑在内，并且各个损失被求和。</p><blockquote class="no"><p id="0326" class="np nq it bd nr ns nt nu nv nw nx lq dk translated">损耗= L+L′</p></blockquote><h2 id="c664" class="na ls it bd lt nb ny dn lx nd nz dp mb le oa ng md li ob ni mf lm oc nk mh nl bi translated">最大附加利润(AMS)</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi od"><img src="../Images/df804fe121bda80caa792d6797688f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kITjGdxEJQd6ISYvyASB7w.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">嵌入空间有无加性余量 Softmax 经<a class="ae ku" href="https://www.ijcai.org/Proceedings/2019/746" rel="noopener ugc nofollow" target="_blank">杨等</a></p></figure><p id="7745" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在向量空间中，分类边界可能非常窄，因此很难<strong class="kx iu">分离向量</strong>。AMS 建议在原 softmax loss 中引入一个参数<strong class="kx iu"> <em class="mr"> m </em> </strong> <em class="mr"> </em>来增加向量之间的可分性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oe"><img src="../Images/b98bf2efe359b5f979583617a55afd91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rP3WeFKJnE6SRdhdqrkElA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">AMS via <a class="ae ku" href="https://arxiv.org/abs/2007.01852" rel="noopener ugc nofollow" target="_blank"> LaBSE 论文</a></p></figure><p id="c029" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">请注意参数<strong class="kx iu"> <em class="mr"> m </em>是如何从正样本</strong>中减去的，而不是负责分类边界的负样本。</p><p id="57a8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果你有兴趣更好地了解 AMS，你可以参考这个博客。</p><h2 id="2204" class="na ls it bd lt nb nc dn lx nd ne dp mb le nf ng md li nh ni mf lm nj nk mh nl bi translated">交叉加速器负采样</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi of"><img src="../Images/2414f53fbb8184a0724e2736220e2d42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C3jeFWFLNxikKzxOUMi0LQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">通过<a class="ae ku" href="https://arxiv.org/abs/2007.01852" rel="noopener ugc nofollow" target="_blank"> LaBSE 纸</a>进行交叉加速器负采样</p></figure><p id="8ee2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">翻译分级任务建议对“<strong class="kx iu"><em class="mr">”K-1</em></strong>”和源句子的潜在不兼容翻译的其他句子使用负采样。这通常是通过<strong class="kx iu">从该批</strong>的其他人那里获取句子来完成的。这种批内负采样如上图所示(<strong class="kx iu">左</strong>)。然而，LaBSE 利用 BERT 作为其编码器网络。对于像这样的大型网络，批量大到足以为训练提供足够的负样本是不可行的。因此，提出的方法<strong class="kx iu">利用分布式训练方法</strong>在不同的加速器(GPU)之间共享批处理，并在最后广播它们。这里，所有的共享批次都被认为是负样本，只有本地批次中的句子被认为是正样本。如上图所示(<strong class="kx iu">右</strong>)。</p><h2 id="73fb" class="na ls it bd lt nb nc dn lx nd ne dp mb le nf ng md li nh ni mf lm nj nk mh nl bi translated">预训练和参数共享</h2><p id="9135" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">最后，如前所述，建议的架构使用 BERT 编码器，并在掩蔽语言模型(MLM)上进行预训练，如<a class="ae ku" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> Devlin 等人所述。艾尔</a>。和翻译语言模型(TLM)目标如在<a class="ae ku" href="https://arxiv.org/abs/1901.07291" rel="noopener ugc nofollow" target="_blank"> XLM(康诺和兰普尔)</a>。此外，这些是使用<strong class="kx iu"> 3 阶段渐进堆叠算法</strong>训练的，即首先针对<strong class="kx iu"> <em class="mr"> L / 4 </em> </strong>层，然后针对<strong class="kx iu"> <em class="mr"> L / 2 </em> </strong>层，最后针对<strong class="kx iu"> <em class="mr"> L </em> </strong>层，训练<strong class="kx iu">L/2</strong>层。</p><p id="b359" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">更多关于 BERT 前期训练的内容，你可以<a class="ae ku" href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener">参考我的博客</a>。</p><h1 id="11a5" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">把所有的放在一起</h1><p id="951c" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">LaBSE，</p><ol class=""><li id="029b" class="og oh it kx b ky kz lb lc le oi li oj lm ok lq ol om on oo bi translated">将所有现有方法(即预训练和微调策略)与双向双编码器翻译排序模型相结合。</li><li id="72a4" class="og oh it kx b ky op lb oq le or li os lm ot lq ol om on oo bi translated">是一个庞大的模型，支持 109 种语言。</li></ol><h1 id="3a72" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">结果</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ou"><img src="../Images/1d5cf6b93a6958356cef8b399eed3c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTMk3Q_WbZ608jFU4erf9Q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">通过<a class="ae ku" href="https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a>在<a class="ae ku" href="https://github.com/facebookresearch/LASER/tree/master/data/tatoeba/v1" rel="noopener ugc nofollow" target="_blank"> Tatoeba 数据集</a>上的平均准确率(%)</p></figure><p id="f6ea" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">LaBSE 在所有语言上的平均准确率达到 83.7%，明显优于其竞争对手。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/8bc054cce9c62dce370bccb0916472d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*NT9hPpg7--sI5ST3.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">通过<a class="ae ku" href="https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a>进行零拍设置</p></figure><p id="0942" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">LaBSE 还能够在没有训练数据的语言上产生不错的结果(零命中率)。</p><blockquote class="mo mp mq"><p id="08cf" class="kv kw mr kx b ky kz ju la lb lc jx ld ms lf lg lh mt lj lk ll mu ln lo lp lq im bi translated">有趣的事实:该模型使用 500k 的词汇量来支持 109 种语言，并为甚至零命中率的情况提供跨语言支持。</p></blockquote><h1 id="3a5e" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">结论</h1><p id="2430" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">我们讨论了语言不可知的 BERT 句子嵌入模型，以及如何结合预训练方法来获得最先进的句子嵌入。</p><p id="8ea1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">该模型在<a class="ae ku" href="https://tfhub.dev/google/LaBSE/1" rel="noopener ugc nofollow" target="_blank"> TFHub 这里</a>开源。</p><h1 id="d82c" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">参考</h1><p id="1b6f" class="pw-post-body-paragraph kv kw it kx b ky mj ju la lb mk jx ld le ml lg lh li mm lk ll lm mn lo lp lq im bi translated">LaBSE 论文:【https://arxiv.org/abs/2007.01852 T4】</p><p id="2aa5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">带 AMS 的双编码器:<a class="ae ku" href="https://www.ijcai.org/Proceedings/2019/746" rel="noopener ugc nofollow" target="_blank">https://www.ijcai.org/Proceedings/2019/746</a></p><p id="4484" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">双编码器和翻译排名任务:<a class="ae ku" href="https://www.aclweb.org/anthology/W18-6317/" rel="noopener ugc nofollow" target="_blank">https://www.aclweb.org/anthology/W18-6317/</a></p><p id="9580" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><a class="ae ku" href="https://arxiv.org/abs/1901.07291" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1901.07291</a>XLM 纸</p><div class="ow ox gp gr oy oz"><a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener follow" target="_blank"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd iu gy z fp pe fr fs pf fu fw is bi translated">伯特:语言理解变形金刚的前期训练</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">了解基于变压器的自监督架构</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">medium.com</p></div></div><div class="pi l"><div class="pj l pk pl pm pi pn ko oz"/></div></div></a></div><div class="ow ox gp gr oy oz"><a rel="noopener follow" target="_blank" href="/additive-margin-softmax-loss-am-softmax-912e11ce1c6b"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd iu gy z fp pe fr fs pf fu fw is bi translated">附加边际软最大损失(AM-Softmax)</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">了解 L-Softmax、A-Softmax 和 AM-Softmax</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">towardsdatascience.com</p></div></div><div class="pi l"><div class="po l pk pl pm pi pn ko oz"/></div></div></a></div></div></div>    
</body>
</html>