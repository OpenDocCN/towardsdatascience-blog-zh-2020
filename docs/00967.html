<html>
<head>
<title>How to extract facial expressions, head pose, and gaze from any Youtube video</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从任何Youtube视频中提取面部表情、头部姿势和凝视</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6?source=collection_archive---------10-----------------------#2020-01-28">https://towardsdatascience.com/how-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6?source=collection_archive---------10-----------------------#2020-01-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="eabb" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">数据科学教程</h2><div class=""/><div class=""><h2 id="933c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用Google Colab &amp; OpenFace从Youtube视频中提取面部特征的教程，无需在笔记本电脑上安装任何程序。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/0bafb82fb0ed87a109168f1d49be1403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6uRcrdbgx2o0Sc9T1yChyA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae lh" href="https://unsplash.com/@ferventjan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">狂热的Jan </a>拍照</p></figure><p id="d620" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过研究人们的面部表情、头部姿势和凝视信息，可以研究和分析人们的感受、想法和兴趣。有许多公司和计算机视觉算法可以帮助从面部视频中提取这些面部特征，包括<a class="ae lh" href="https://www.crunchbase.com/organization/emotient" rel="noopener ugc nofollow" target="_blank"> Emotient </a>和Affectiva(它们的算法对比<a class="ae lh" href="https://medium.com/@jinhyuncheong/face-analysis-software-comparison-affectiva-affdex-vs-openface-vs-emotient-facet-5f91a4f12cbb?source=friends_link&amp;sk=fa98a9ac38c56b3f15b837718b2aea05" rel="noopener">这里</a>、<a class="ae lh" href="https://link.springer.com/article/10.3758/s13428-017-0996-1" rel="noopener ugc nofollow" target="_blank">这里</a>)，但很少有公司免费提供这些服务，大多数公司要求用户购买订阅或按视频分钟付费。</p><p id="21ab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章中，我分享了一个免费的、易于使用的、健壮的面部特征提取付费服务的替代方案，它使用了<a class="ae lh" href="https://github.com/TadasBaltrusaitis/OpenFace" rel="noopener ugc nofollow" target="_blank"> OpenFace </a>，这是一个最先进的面部动作单元识别、凝视估计、面部标志检测和头部姿势估计工具。在这里，我分享如何使用<a class="ae lh" href="https://colab.research.google.com/gist/jcheong0428/c16146b386ea60fab888b56e8e5ee747/openface_shared.ipynb" rel="noopener ugc nofollow" target="_blank"> Google Colab Jupyter笔记本</a>的说明，它允许你设置OpenFace并从任何<a class="ae lh" href="http://www.youtube.com" rel="noopener ugc nofollow" target="_blank"> Youtube </a>视频中提取面部特征，而不必在你的笔记本电脑上安装一个软件包。</p><p id="b216" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://gist.github.com/jcheong0428/c16146b386ea60fab888b56e8e5ee747" rel="noopener ugc nofollow" target="_blank">这里是笔记本的链接</a>！</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div></figure></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="59e4" class="mn mo it bd mp mq mr ms mt mu mv mw mx ki my kj mz kl na km nb ko nc kp nd ne bi translated">1.设置</h1><p id="ac92" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">您不需要在您的笔记本电脑上安装任何东西，但是您仍然需要在您的Colab实例上安装OpenFace包。不幸的是，这一部分可能需要一段时间(大约40分钟)，这段时间非常适合你看一些Youtube视频，以确定你想从哪个视频中提取面部特征。哦，你可能需要一个谷歌账户。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div></figure><h1 id="573f" class="mn mo it bd mp mq nk ms mt mu nl mw mx ki nm kj mz kl nn km nb ko no kp nd ne bi translated">2.找个Youtube视频分析一下。</h1><p id="7ade" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">找一个你想分析的Youtube视频。它可以是一个人的面部视频，也可以是多个人的面部视频。只是要小心避免视频中的人脸太小，这使得大多数算法很难找到人脸。在本教程中，我们将从我和我的同事为<a class="ae lh" href="https://pioneer.app/" rel="noopener ugc nofollow" target="_blank"> Pioneer.app </a>推出的一个应用创意<a class="ae lh" href="https://medium.com/tastespace" rel="noopener"> TasteSpace </a>中提取面部特征。下面的代码将向您展示感兴趣的视频。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="np mf l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">这是我们将要使用的视频..我们正在描述一个名为<a class="ae lh" href="https://medium.com/tastespace" rel="noopener"> TasteSpace </a>的项目。</p></figure><p id="dd51" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来的几行代码下载视频并剪辑视频的前几秒(10秒)。这只是为了节省处理时间，所以如果你想处理整个视频，可以随意移除第五行的<code class="fe nq nr ns nt b">-t 10</code>标志。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">如果你想从整个视频中提取特征，你可以去掉“-t 10”标志(这会花更长时间)。</p></figure><h1 id="d04b" class="mn mo it bd mp mq nk ms mt mu nl mw mx ki nm kj mz kl nn km nb ko no kp nd ne bi translated">3.使用OpenFace处理视频</h1><p id="61fd" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">现在我们将使用可以同时从多张人脸中提取面部特征的<code class="fe nq nr ns nt b">FaceLandmarkVidMulti</code>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="cd33" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你在视频中一次只有一张脸，那么你可以使用<code class="fe nq nr ns nt b">FeatureExtraction</code>来代替，或者如果你想从图像中提取特征，可以使用<code class="fe nq nr ns nt b">FaceLandmarkImg</code>。<a class="ae lh" href="https://github.com/TadasBaltrusaitis/OpenFace/wiki/Command-line-arguments" rel="noopener ugc nofollow" target="_blank">单击此处查看命令行函数及其参数的完整列表</a>。</p><h1 id="dc1b" class="mn mo it bd mp mq nk ms mt mu nl mw mx ki nm kj mz kl nn km nb ko no kp nd ne bi translated">4.可视化结果！</h1><p id="db48" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">您可以使用下面的代码来可视化结果，如果输出有意义，您可以直接从笔记本上检查。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/f0e1925d027a973b81df3cc1492b08dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NcweJOO4iTGHH5DzELnGRw.png"/></div></div></figure><h1 id="6f91" class="mn mo it bd mp mq nk ms mt mu nl mw mx ki nm kj mz kl nn km nb ko no kp nd ne bi translated">5.下载输出。</h1><p id="f4d5" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">你现在可以通过打开你的Colab笔记本左边菜单上的文件标签来下载提取的面部特征，并将文件下载到<code class="fe nq nr ns nt b">processed/videos.csv</code>文件夹中。</p><div class="ks kt ku kv gt ab cb"><figure class="nv kw nw nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/2fec0acaa3407c41c10b8c576f542a1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*HFqSehKKjTneaLdeZeb0Og.png"/></div></figure><figure class="nv kw ob nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/ab1a4c66da238df740ccf8b8928b3bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*WZ04IXx58VI1ofvRw3PfMw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk oc di od oe translated">展开文件选项卡并下载结果。</p></figure></div><h1 id="760d" class="mn mo it bd mp mq nk ms mt mu nl mw mx ki nm kj mz kl nn km nb ko no kp nd ne bi translated">结论</h1><p id="8973" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">希望这是一个有趣的练习，教你如何使用Google Colab和OpenFace在几分钟内(安装后)从任何Youtube视频中提取面部特征。如果你有兴趣了解更多关于如何在这种面部表情数据中分析个体之间的同步性，请随时查看我以前关于如何做到这一点的帖子。</p><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/four-ways-to-quantify-synchrony-between-time-series-data-b99136c4a9c9"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">量化时间序列数据之间同步性的四种方法</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">用于计算同步指标的样本代码和数据，包括皮尔逊相关、时滞交叉相关…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow lb oi"/></div></div></a></div></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="8da2" class="mn mo it bd mp mq mr ms mt mu mv mw mx ki my kj mz kl na km nb ko nc kp nd ne bi translated">额外学分</h1><p id="6af8" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">如果你还在读这篇文章，你可能会对如何处理OpenFace的输出感兴趣，这里有一些额外的代码可以帮助你入门。</p><h2 id="5e8b" class="ox mo it bd mp oy oz dn mt pa pb dp mx lr pc pd mz lv pe pf nb lz pg ph nd iz bi translated">加载数据</h2><p id="d589" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">首先，我们将数据加载到Pandas数据帧中，重命名列以消除空白，评估数据帧的形状、数据的最高帧数，并绘制数据的头部。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/436fc149491df79dc07354409b8e3eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F5HbszRFEtxe9_IjhY6cBw.png"/></div></div></figure><h2 id="4ebe" class="ox mo it bd mp oy oz dn mt pa pb dp mx lr pc pd mz lv pe pf nb lz pg ph nd iz bi translated">从视频中计数唯一个体的数量</h2><p id="8578" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">您可能会注意到第<code class="fe nq nr ns nt b">face_id</code>列，它试图区分视频中的个人。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/05c507997b0051e617981665ac1d8f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DCBO9YmuXbhp5VCKf1GuWw.png"/></div></div></figure><p id="f038" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">看起来我们有4张不同的脸，而不是3张！</p><h2 id="920f" class="ox mo it bd mp oy oz dn mt pa pb dp mx lr pc pd mz lv pe pf nb lz pg ph nd iz bi translated">从视频中检测人脸的平均模型置信度</h2><p id="673b" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">我们可以使用以下函数进一步评估该算法检测每个人脸的置信度。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/7156b2ff57a37b7f6452c6c20d460036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DaU43_mTE89DIihISk5Xqg.png"/></div></div></figure><h2 id="cec8" class="ox mo it bd mp oy oz dn mt pa pb dp mx lr pc pd mz lv pe pf nb lz pg ph nd iz bi translated">可视化视频中人脸的位置</h2><p id="eae1" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">我们看到<code class="fe nq nr ns nt b">face_id==3</code>具有最低的置信度，这可能是被检测到的伪造人脸。让我们通过在整个剪辑中绘制人脸的位置来进一步检查这一点。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div></figure><div class="ks kt ku kv gt ab cb"><figure class="nv kw pl nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/82cff4504fa5b2842f1fbe8ef203ceb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*pt6RUE41GjKNXW--6Xim0w.png"/></div></figure><figure class="nv kw pl nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/bd58923c762869331c72b404b43402d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*3OzIyPYZZmayqt5Z3cmogA.png"/></div></figure></div><p id="10ed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看到，在左边的图中，face_id==3的脸确实在某个不存在脸的地方。我们可以根据任意置信水平(这里我们使用80%)设定输出阈值，在左侧的图中我们可以看到，我们已经消除了伪脸。</p><h2 id="f7ab" class="ox mo it bd mp oy oz dn mt pa pb dp mx lr pc pd mz lv pe pf nb lz pg ph nd iz bi translated">分析面部肌肉随时间的运动</h2><p id="0a43" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">现在让我们画出每张脸的每个动作单元预测的轨迹随时间的变化情况。然后，我们将打印出视频中人们在一段时间内是如何相似地微笑的(动作单元12)。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/fa530235c76b91bb0a462643246f6602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xUKl2J52seddo_LgduZKlQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">每个face_id的AU强度预测随时间的时间序列</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/daa488f4074faf4fd727e2199d2ea067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*3OlMmx2dK0UkYkbudPvnuQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">每对个体微笑的相关性(AU12)。</p></figure><h2 id="230d" class="ox mo it bd mp oy oz dn mt pa pb dp mx lr pc pd mz lv pe pf nb lz pg ph nd iz bi translated">随着时间的推移分析眼睛凝视</h2><p id="900a" class="pw-post-body-paragraph li lj it lk b ll nf kd ln lo ng kg lq lr nh lt lu lv ni lx ly lz nj mb mc md im bi translated">最后，我们可以画出每个人在看哪里。这可能不是画弧度角的最好方法，但是你仍然可以感觉到每个面在看哪里。这通过了完整性检查，即左边的面(face_id==2)主要从原点(0，0)看向右边，右边的面(face_id==0)看向左边，中间的面看向两边。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi po"><img src="../Images/64e3f0940b8ca12cbaf4db52f343be4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s5AtA1uubtM3yJRDwBPNDA.png"/></div></div></figure><blockquote class="pp"><p id="c3a8" class="pq pr it bd ps pt pu pv pw px py md dk translated">如果你想了解更多关于不同输出的信息，我强烈推荐你在<a class="ae lh" href="https://github.com/TadasBaltrusaitis/OpenFace/wiki/Output-Format" rel="noopener ugc nofollow" target="_blank"> OpenFace wiki </a>中阅读更多关于每个输出代表什么的信息。</p></blockquote><div class="pz qa qb qc qd oi"><a href="https://medium.com/@jinhyuncheong" rel="noopener follow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">金玄昌-中</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">在媒体上阅读金贤昌的作品。人类行为和数据科学爱好者||认知神经科学博士…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">medium.com</p></div></div><div class="or l"><div class="qe l ot ou ov or ow lb oi"/></div></div></a></div></div></div>    
</body>
</html>