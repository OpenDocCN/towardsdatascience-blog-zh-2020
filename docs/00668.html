<html>
<head>
<title>Applying transfer-learning in CNNs for dog breed classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">应用神经网络中的迁移学习进行犬种分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dog-breed-classification-using-cnns-and-transfer-learning-e36259b29925?source=collection_archive---------20-----------------------#2020-01-19">https://towardsdatascience.com/dog-breed-classification-using-cnns-and-transfer-learning-e36259b29925?source=collection_archive---------20-----------------------#2020-01-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/67e7a9e425a941298a3eac871b3ce0e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UtGlvq2azoCQc7BSxIDoJA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://pixabay.com/pt/users/Comfreak-51581/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2467149" rel="noopener ugc nofollow" target="_blank">乔尼·林德纳</a>由<a class="ae jd" href="https://pixabay.com/pt/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2467149" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>拍摄</p></figure><div class=""/><div class=""><h2 id="9da6" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">…还有人和狗的相似之处</h2></div><p id="5462" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇文章提出了使用预先训练的卷积神经网络来分类养狗。除了用于狗的狗品种分类，CNN也用于识别与任何给定的人类图片最相似的狗品种。</p><p id="9281" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本文分六个步骤介绍了该项目:</p><ol class=""><li id="b6d4" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">数据集。</li><li id="aa19" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">OpenCV实现了<a class="ae jd" href="http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html" rel="noopener ugc nofollow" target="_blank">基于Haar特征的级联分类器</a>来识别人类。</li><li id="8a7e" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">为<a class="ae jd" href="http://image-net.org/index" rel="noopener ugc nofollow" target="_blank"> ImageNe </a> t数据集预训练<a class="ae jd" href="https://keras.io/applications/#classify-imagenet-classes-with-resnet50" rel="noopener ugc nofollow" target="_blank"> ResNet50 </a>以识别狗。</li><li id="696c" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">犬种分类的迁移学习。</li><li id="9640" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">微调</li><li id="9a25" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">图像分类</li></ol><h2 id="6471" class="mf mg jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">缩写列表:</h2><p id="b6c0" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">FC层:全连接层<br/> Conv。层:卷积层</p><h2 id="fc2f" class="mf mg jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">数据集</h2><p id="6e98" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">Udacity提供的数据集相对较小。它包含了8351张狗的图片，来自133个可能的狗品种类别，大约每个类别63张。数据集分割如下:</p><ul class=""><li id="6aff" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq nd lx ly lz bi translated">训练集的6680幅图像；</li><li id="a78c" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq nd lx ly lz bi translated">835幅图像用于验证集；</li><li id="00a1" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq nd lx ly lz bi translated">测试集的836幅图像。</li></ul><p id="e619" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">提供另一个具有100个图像的数据集来测试预训练的人类标识符。</p><h2 id="7e7a" class="mf mg jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">1.用于人体识别的OpenCV</h2><p id="c679" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated"><a class="ae jd" href="https://opencv.org/" rel="noopener ugc nofollow" target="_blank"> OpenCV </a>库在<a class="ae jd" href="https://github.com/opencv/opencv/tree/master/data/haarcascades" rel="noopener ugc nofollow" target="_blank"> GitHub </a>库中提供了许多预先训练好的人脸检测器。对于这个项目，我们将<a class="ae jd" href="https://github.com/opencv/opencv/tree/master/data/haarcascades" rel="noopener ugc nofollow" target="_blank">Haar scades</a>用于<a class="ae jd" href="https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_alt.xml" rel="noopener ugc nofollow" target="_blank">正面人脸检测器</a>。</p><pre class="ne nf ng nh gt ni nj nk nl aw nm bi"><span id="d4f3" class="mf mg jg nj b gy nn no l np nq"># extract pre-trained face detector<br/>model_file_name = 'haarcascades/haarcascade_frontalface_alt.xml'<br/>face_cascade = cv2.CascadeClassifier(model_file_name)</span><span id="ede1" class="mf mg jg nj b gy nr no l np nq"># load color (BGR) image<br/>img = cv2.imread(human_files[83])</span><span id="a08f" class="mf mg jg nj b gy nr no l np nq"># convert BGR image to grayscale<br/>gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><span id="5e8a" class="mf mg jg nj b gy nr no l np nq"># find faces in image<br/>faces = face_cascade.detectMultiScale(gray)</span></pre><p id="10fd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从包含人类的100幅图像中，预先训练的模型可以识别所有人，100%准确。在这100张照片中，有三张发现了不止一张脸。这些例子如图1所示。在第一个例子中，只有一张脸，它被识别了两次。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/498e6923d859c5dcfdb6fa1c5a8c9bbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uIbIz3SGCMysk9ile9l_Dg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd nt">图1 </strong> —发现多个面的情况。</p></figure><p id="d2c6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我把另一组100张狗的图片交给分类器时，它识别出其中11张是人的图片。您可以在下面的图2中看到其中的9个错误。但是，如果您看到下面的错误1，除了狗之外，图像中还有一个人，人脸检测器正确地识别了这个人。因此，让我们认为它不是一个错误。探测器只犯了10个错误，而不是11个。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/6a133a231395ded9fa0873a7e53c7ecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lBEIVrkjo3WQ00kkp-o4EA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd nt">图2</strong>–人脸检测器在100多张狗的图像中出现的10个错误中有9个错误。</p></figure><p id="9c90" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人脸检测器很好地识别了人，没有假阳性。另一方面，区分狗和人不太好，它把100只狗中的10只<em class="nu">误归类为人类。</em></p><h2 id="efc7" class="mf mg jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">2.预先训练EGG16识别狗</h2><p id="6ebf" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">库<a class="ae jd" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>提供了已经在ImageNet数据集上训练过的CNN模型。该数据集包含1000个不同类别的图像，其中118个与狗有关。这意味着我们可以使用这些预训练的模型来确定一幅图像是否包含一只狗。</p><p id="f8fc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">区分前一个分类器和下一个分类器是很重要的。第一个的目的是识别图像是否包含一个人<strong class="kx jh"/><em class="nu"/>。接下来的目的是识别图像是否包含<strong class="kx jh">狗</strong>或<em class="nu">狗</em>。结合两个分类器，我们可以确定图像中何时有狗、人、两者或没有。</p><p id="3a8a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的代码展示了如何使用Keras中预先训练好的模型<a class="ae jd" href="https://keras.io/applications/#classify-imagenet-classes-with-resnet50" rel="noopener ugc nofollow" target="_blank"> ResNet50 </a>来确定图像中是否有狗。</p><pre class="ne nf ng nh gt ni nj nk nl aw nm bi"><span id="144c" class="mf mg jg nj b gy nn no l np nq"># Import the ResNet model and the preprocess_input function.<br/>from keras.applications.resnet50 import ResNet50, preprocess_input</span><span id="0476" class="mf mg jg nj b gy nr no l np nq"># Import the functions to load and transform images<br/>from keras.preprocessing.image import load_img, img_to_array</span><span id="2224" class="mf mg jg nj b gy nr no l np nq"># Create a Redidual-network already trained in the IMAGENET<br/>ResNet50_model = ResNet50(weights='imagenet')</span><span id="7058" class="mf mg jg nj b gy nr no l np nq"># Load the image in the size expected by the ResNet50_model<br/>img = load_img('some_image.jpg', target_size=(224, 224))</span><span id="8dfb" class="mf mg jg nj b gy nr no l np nq"># Transform the image into an array<br/>img_array = img_to_array(img)</span><span id="709d" class="mf mg jg nj b gy nr no l np nq"># Pre-process the image according to IMAGENET standarts<br/>img_ready = preprocess_input(img_array)</span><span id="76d7" class="mf mg jg nj b gy nr no l np nq"># Predicts<br/>probs= ResNet50_model.predict(img_ready)</span><span id="20b9" class="mf mg jg nj b gy nr no l np nq"># Find the position with the maximum probability value<br/>position_of_max = np.argmax(probs)</span><span id="f631" class="mf mg jg nj b gy nr no l np nq"># Verify if the position of max corresponds to a dog class<br/>is_dog = ((position_of_max &gt;= 151) &amp; (position_of_max &lt;= 268))</span></pre><p id="c6b4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Keras提供了函数<code class="fe nv nw nx nj b">preprocess_input</code>来根据IMAGENET数据集的分布对新图像进行规范化。该函数从图像的每个RGB像素中减去数据集已知的平均像素[103.939，116.779，123.68]。</p><p id="d6eb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在<code class="fe nv nw nx nj b">ResNet50_model.predict</code>中进行的预测返回每个类的概率列表。对应于狗的类在151和268之间的位置，包括151和268。要验证图像是否是狗，最大概率必须在151到268范围内的位置是狗，否则不是狗。</p><p id="e6ef" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从包含人类的100张图像中，预训练的模型没有识别出它们上面的任何狗，这很棒。此外，该分类器还能够识别所有其他100张包含狗的图像。这个模型在这200张图片上没有犯任何错误。</p><p id="8a8d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此时，你可能会疑惑，为什么我们不简单地用这118个狗类来划分狗的品种呢？这是因为我们试图将狗分为133个可能的犬种类别，15个新的类别。即使我们知道预训练ResNet50模型的118个类别中的一些与133个犬种类别重叠，我们仍然没有针对这些不重叠的特定类别的好的解决方案。事实上，我们有一个解决方案，它使用预训练模型中的知识，并可用于对所有133个狗品种进行分类。这是迁移学习。</p><h2 id="734e" class="mf mg jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">3.用于犬种分类的迁移学习</h2><p id="7f8e" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">幸运的是，可以使用已经在一个数据集中训练过的CNN来缩短在其他数据集中的训练。我们只需考虑两件事，新数据集的<strong class="kx jh">大小</strong>和新数据集与之前用于训练网络的数据集之间的<strong class="kx jh">相似度</strong>。对于这两个变量，我们有四种可能的情况，如图3所示。</p><p id="6cf6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当新数据集与用于训练网络的数据集相似时，我们可以保留大部分层。如果我们没有太多数据，我们可以保留几乎所有图层，只需用新图层替换输出图层，以匹配新数据集中的类数量，如图3的场景1所示。</p><p id="a34c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果新数据集相似，但很大，我们可以保持卷积层不变，并替换所有完全连接的层。这种情况显示在图3的场景2中。可选地，我们可以在完全连接的层中选择每层相同数量的神经元，只要我们用随机数重置这些层的权重，并证明输出的数量与数据集中的类的数量相同。</p><p id="20c8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我们有一个不同于模型训练数据集的新数据集时，我们不能保留所有卷积层。我们必须替换一些最后的卷积层，因为这些层提取仅与用于训练模型的数据集中的类相关的高级特征。因此，我们应该提取与新数据集中的类更相关的其他高级要素。这种情况如图3的场景3所示。我们可以保留网络的架构，在卷积层的情况下，用相同数量的滤波器创建新层，在全连接层的情况下，用相同数量的神经元创建新层。输出图层必须与新数据集中的类数量相匹配。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/31d1ed7152a53a14d0fe96b674a6392d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cH5gkfh48ovQ17kyUjM1lg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图3</p></figure><p id="0032" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我使用了在ImageNet数据集上训练的ResNet50。新的数据集类似于ImageNet，所以训练好的模型之前已经见过狗了。此外，新数据集有8351幅图像，这并不算小。使用一种叫做<strong class="kx jh">数据扩充</strong>的技术，我们可以转换我们数据集中已经存在的图像，以生成稍微不同的图像。使用数据扩充，我们可以有一个大约5倍大的数据集。在这种情况下，我们可以使用场景2，大型且相似的数据集。</p><p id="32ad" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了训练新的光纤通道层，我需要用于喂养第一个光纤通道层的功能。这些特性被称为<strong class="kx jh">瓶颈特性</strong>。此名称是因为这些要素是从最后一个Conv图层中获得的，该图层具有最窄的要素地图。第一个Conv图层提取的要素地图较少，分辨率较高，而最后一个Conv图层提取的要素地图较多，分辨率较低。</p><p id="55c2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以通过Conv层预处理数据集中的所有图像，以生成瓶颈特征并将其存储到文件中，从而节省时间。为了训练网络，我们必须从文件中读取瓶颈特征，并将它们馈送到FC层。从文件中读取所有瓶颈特征比通过Conv层提交所有图像要快得多。这样，我们节省了宝贵的时间，因为所有的数据集都迭代了很多次。</p><p id="1773" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我训练过三种不同型号的FC层。这些模型如图4所示。<br/> <strong class="kx jh">模型A </strong> —第一个隐藏层是GlobalAveragePooling2D层，后面是输出密集层。<br/> <strong class="kx jh">模型B</strong>-该模型在GlobalAveragePooling2D和输出密集图层之间添加了一个下降图层，有50%的下降几率。<br/> <strong class="kx jh">模型C </strong> —此模型在输出密集层之前添加了另一个密集和丢弃层，有50%的几率丢弃。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/1c16a28ca4ee7d4b93e4e2d014ca9d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DLPwEtfjS4EfGL_2RsPH2g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd nt">图4 — </strong>培训中使用的三种FC模型。</p></figure><p id="4021" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所有模型在50个时期内被训练(在整个数据集上50次迭代)。训练和验证精度和损失如图5所示。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/12d5c5e055292d5e72fb38dfd799f99f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zSw2MGPsXIIvA_rkRGk4_A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd nt">图5 — </strong>三种型号在50个周期后的性能。</p></figure><p id="ea38" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所有模型都有非常相似的结果。模型A和模型C的准确率最高，均为84.19%，但模型C的损耗稍好。迭代50次后的损耗和精度如下:<br/> <strong class="kx jh">模型A </strong> —精度:84.19%，损耗:0.5594。<br/><strong class="kx jh">B型</strong> —准确率:82.16%，损耗:0.5532。<br/><strong class="kx jh">C型</strong> —准确率:84.19%，损耗:0.5098。</p><p id="fc0d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意训练和验证性能之间的差异是很重要的。模型A在训练中取得了非常好的结果，但是它不能被复制到验证中。这种差异是过拟合的良好信号，表明该模型在训练期间非常好，但它对以前从未见过的图像进行分类是不一致的。这表明该模型泛化能力较低。</p><p id="48eb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">模型B在训练和验证之间呈现更一致的结果。但是C型是他们中最好的。模型C在对以前从未见过的图像进行分类方面比对训练中使用的图像进行分类具有更好的结果。而且，C型也做到了亏损最低。</p><h2 id="e049" class="mf mg jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">4.用数据扩充微调CNN</h2><p id="3565" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">在上一步中，仅使用ResNet50中的瓶颈功能训练了FC层。为了微调网络，我将原来的Conv层连接到我刚刚训练的FC层，从而形成了一个完整的CNN。</p><pre class="ne nf ng nh gt ni nj nk nl aw nm bi"><span id="d23e" class="mf mg jg nj b gy nn no l np nq">def append_resnet(fc_model):<br/>    # Load the pre-trained Conv layers from Keras<br/>    ResNet50_convs = ResNet50(include_top=False, weights='imagenet')<br/>    <br/>    # Create a complete CNN joining the Conv and FC layers<br/>    Resnet50_full = Sequential()<br/>    Resnet50_full.add(ResNet50_convs)<br/>    Resnet50_full.add(fc_model)</span><span id="124f" class="mf mg jg nj b gy nr no l np nq">return Resnet50_full</span></pre><p id="6143" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">训练继续进行，但是这次调整了整个CNN中的所有权重，而不仅仅是FC层。</p><p id="fb97" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这一点上，我还使用了数据扩充来对数据集中的图像进行随机修改。数据扩充使用了以下参数:</p><pre class="ne nf ng nh gt ni nj nk nl aw nm bi"><span id="a968" class="mf mg jg nj b gy nn no l np nq">train_datagen = ImageDataGenerator(<br/>rotation_range=20,<br/>width_shift_range=0.2,<br/>height_shift_range=0.2,<br/>shear_range=0.2,<br/>zoom_range=0.2,<br/>horizontal_flip=True,<br/>fill_mode='nearest')</span><span id="cc08" class="mf mg jg nj b gy nr no l np nq">valid_datagen = ImageDataGenerator(<br/>rotation_range=20,<br/>width_shift_range=0.2,<br/>height_shift_range=0.2,<br/>shear_range=0.2,<br/>zoom_range=0.2,<br/>horizontal_flip=True,<br/>fill_mode='nearest')</span></pre><p id="91b9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在用数据扩充对三个完整的CNN模型进行了超过五个时期的训练之后，他们获得了下面图6所示的结果。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/8d415471db26a560683d05c1a669bb06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MQViqD_jj21SMFyP95gvNw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd nt">图6 — </strong>插入数据扩充和5个以上时期后三个模型的性能。</p></figure><p id="2eff" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据扩充的插入导致了性能的短暂下降，但是验证值在五个时期之后返回到先前的点。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/33d726a18cd53c94d4e91a6ed8f936a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kSm1EiESZ5wvikgE2QBGig.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd nt">图7 — </strong>插入数据扩充和5个以上时期后三个模型的性能。</p></figure><p id="90ec" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这三个模型实现了非常相似的结果。从图上很难看出哪个是最好的型号。我们可以在下表中看到迭代55次后实现的精度和损耗:</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div class="gh gi od"><img src="../Images/df7c9a06ca25a3e4dae220e181cfee2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*AA9KN3moMK01rbsp-TJ-Bg@2x.png"/></div></figure><p id="2634" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">模型C提供了最好的结果，具有最高的精度和最低的损失。考虑到这个模型也提供了最好的泛化能力，我只训练了模型C超过10个时期。</p><p id="4fff" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">模型C在测试集中的最终准确率为86.12%，这是以前从未见过的。</p><h2 id="feb2" class="mf mg jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">图像分类</h2><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/57450df30432d7299ff0947a82db427f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*00Qnlf9W77EBryan9r-9yQ.png"/></div></div></figure></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/db2dc082d6ff9fdeedf33d4bb1e33293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VGdwnfekx3MOpG84YoGMog.png"/></div></div></figure></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/adfdcbc322430db44b3971862331550f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFTvowDBe9GfgdQznK3n3Q.png"/></div></div></figure></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/46b7c7c317b84f12cd41d476f06fd8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FvCKqZ39F1WuD7mL5WnNHg.png"/></div></div></figure></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/ea1d97fdfa2b135c1e1173ad6302983e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o4_dITnAOpeGhHfn4gpYeQ.png"/></div></div></figure></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oq"><img src="../Images/c24a2c5f2865f30d0c3fa7f981b2199a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wndTeQnTj1NqpONjP5nRYQ.png"/></div></div></figure></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/67b88c4d77639043219c7cc770f6e292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oNuOYruwB1UpwCtkBApg6Q.png"/></div></div></figure></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi os"><img src="../Images/cf2d9398a9d26bbe297a78cd572f2614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ksb3rwqCIGKetNlBKL5MDQ.png"/></div></div></figure></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ot"><img src="../Images/bbb4d785d38510357ab209ca478b062b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8egJ01jR41maQEgFa2j8VQ.png"/></div></div></figure></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/d9e14842b826e7db502ba72432c9022d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rLtahDpDyYXX3qaOxNQJJQ.png"/></div></div></figure></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ou"><img src="../Images/a484bff5e52c9b19bcf3ba18c4e42c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DLp6-KPaRH96GtF9Pah7JA.png"/></div></div></figure><h2 id="0747" class="mf mg jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">结论</h2><p id="df40" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">当我们只训练完全连接的层时，提取瓶颈特征有利于节省时间。</p><p id="1dd1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">FC层中增加的漏失层提高了模型的泛化能力。此外，添加第二个密集层和另一个丢弃层，不仅提高了网络的泛化能力，而且有助于模型实现更好的分类性能。</p></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><p id="ba53" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">声明:Udacity可能部分提供了此处展示的部分源代码。</p></div></div>    
</body>
</html>