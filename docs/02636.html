<html>
<head>
<title>Text Classification Using Word Embeddings and Deep Learning in Python — Classifying Tweets from Twitter</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python中的单词嵌入和深度学习进行文本分类——对来自Twitter的推文进行分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classification-using-word-embeddings-and-deep-learning-in-python-classifying-tweets-from-6fe644fcfc81?source=collection_archive---------1-----------------------#2020-03-14">https://towardsdatascience.com/text-classification-using-word-embeddings-and-deep-learning-in-python-classifying-tweets-from-6fe644fcfc81?source=collection_archive---------1-----------------------#2020-03-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="525a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">文本分类代码，以及对使用Python和Tensorflow所发生的事情的深入解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0b776eb733dc5962f1cf3619817a69e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HR9ZGZPkHqczsHwf"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@suanmoo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> 수안 최 </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="90b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文的目的是帮助读者理解在创建文本分类器时如何利用单词嵌入和深度学习。</p><p id="4ed3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，文本建模中经常被忽略的部分，如什么是单词嵌入，什么是嵌入层或深度学习模型的输入，也将在这里讨论。</p><p id="979e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，所有概念的展示将在Twitter发布的数据集上付诸实践，该数据集关于一条推文是否是关于自然灾害的。</p><p id="d9c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文使用的主要技术是<strong class="lb iu"> Python </strong>和<strong class="lb iu"> Keras API。</strong></p><p id="9bd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个功能齐全的文本分类管道和来自Twitter的数据集可以在这里找到:<a class="ae ky" href="https://github.com/Eligijus112/twitter-genuine-tweets" rel="noopener ugc nofollow" target="_blank">https://github.com/Eligijus112/twitter-genuine-tweets</a>。</p><p id="6da0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文中用到的Word嵌入文件可以在这里找到:<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="973e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用标记文本创建深度学习模型的管道如下:</p><ul class=""><li id="1916" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu">将数据分为文本(X)和标签(Y) </strong></li><li id="d0da" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">预处理X </strong></li><li id="da2a" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">从X创建一个单词嵌入矩阵</strong></li><li id="4621" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">从X创建一个张量输入</strong></li><li id="f155" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">使用张量输入和标签(Y)训练深度学习模型</strong></li><li id="5005" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">对新数据进行预测</strong></li></ul><p id="1001" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将逐一介绍这些步骤。本文的第一部分将使用一个小的示例数据集来涵盖所有的概念。文章的第二部分将把所有的概念实现到一个真实的例子中，这个例子是关于一条推文是否是关于自然灾害的。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="326b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用文本进行预测的深度学习模型的主要构建模块是单词嵌入。</p><p id="1416" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来自维基:<strong class="lb iu">单词嵌入</strong>是一组<a class="ae ky" href="https://en.wikipedia.org/wiki/Language_model" rel="noopener ugc nofollow" target="_blank">语言建模</a>和<a class="ae ky" href="https://en.wikipedia.org/wiki/Feature_learning" rel="noopener ugc nofollow" target="_blank">特征学习</a>技术的统称，在<a class="ae ky" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理</a> (NLP)中，来自词汇表的<strong class="lb iu">单词或短语被映射到实数向量。</strong>例如，</p><p id="739f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“爸爸”= [0.1548，0.4848，1.864]</p><p id="1dcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">"妈妈" = [0.8785，0.8974，2.794]</p><p id="9996" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，<strong class="lb iu">单词嵌入是表示字符串的数值向量。</strong></p><p id="eabb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，单词表示是100、200或300维向量，它们在非常大的文本上被训练。</p><p id="5b37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单词嵌入的一个非常重要的特征是，语义上相似的单词之间的距离(欧几里德距离、余弦距离或其他距离)比没有语义关系的单词之间的距离要小。例如，像“妈妈”和“爸爸”这样的词在数学上应该比“妈妈”和“番茄酱”或“爸爸”和“黄油”更接近。</p><p id="0de5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单词嵌入的第二个重要特征是，当创建模型的输入矩阵时，无论我们在文本语料库中有多少独特的单词，我们在输入矩阵中都将有相同数量的列。与一次性编码技术相比，这是一个巨大的胜利，在一次性编码技术中，列的数量通常等于文档中唯一单词的数量。这个数字可以是几十万，甚至上百万。处理非常宽的输入矩阵在计算上要求很高。</p><p id="0a74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举个例子，</p><p id="399e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想象一句话:<strong class="lb iu">克拉克喜欢在公园散步</strong>。</p><p id="edd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有7个独特的词。使用一个热编码向量，我们将每个单词表示为:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="72fa" class="mv mw it mr b gy mx my l mz na">Clark = [1, 0, 0, 0, 0, 0, 0]<br/>likes = [0, 1, 0, 0, 0, 0, 0]<br/>to = [0, 0, 1, 0, 0, 0, 0]<br/>walk = [0, 0, 0, 1, 0, 0, 0]<br/>in = [0, 0, 0, 0, 1, 0, 0]<br/>the = [0, 0, 0, 0, 0, 1, 0]<br/>park = [0, 0, 0, 0, 0, 0, 1]</span></pre><p id="5159" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而如果使用二维单词嵌入，我们将处理以下向量:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="3168" class="mv mw it mr b gy mx my l mz na">Clark = [0.13, 0.61]<br/>likes = [0.23, 0.66]<br/>to = [0.55, 0.11]<br/>walk = [0.03, 0.01]<br/>in = [0.15, 0.69]<br/>the = [0.99, 0.00]<br/>park = [0.98, 0.12]</span></pre><p id="afdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在想象有n个句子。在一位热码编码的情况下，向量将呈指数增长，而单词的嵌入表示向量的大小将保持不变。这就是为什么在处理大量文本时，单词嵌入被用来表示单词、句子或整个文档。</p><p id="e634" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用具有一个输入层、一个隐藏层和一个输出层的神经网络来创建单词嵌入。</p><p id="060c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关创建单词嵌入的更多信息，请访问文章:</p><div class="nb nc gp gr nd ne"><a href="https://medium.com/analytics-vidhya/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">创建单词嵌入:使用深度学习在Python中编码Word2Vec算法</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">当我在写另一篇展示如何在文本分类目标中使用单词嵌入的文章时，我…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">medium.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ks ne"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="f3e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了让计算机确定哪些文本是好的，哪些是坏的，我们需要对其进行标注。可以有任意数量的类，并且类本身可以表示非常多种多样的东西。让我们构建一些文本:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="8cc4" class="mv mw it mr b gy mx my l mz na">d = [<br/>  ('This article is awesome', 1),<br/>  ('There are just too much words here', 0), <br/>  ('The math is actually wrong here', 0),<br/>  ('I really enjoy learning new stuff', 1),<br/>  ('I am kinda lazy so I just skim these texts', 0),<br/>  ('Who cares about AI?', 0),<br/>  ('I will surely be a better person after reading this!', 1),<br/>  ('The author is pretty cute :)', 1)<br/>]</span></pre><p id="bb86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有8个元组，其中第一个坐标是文本，第二个坐标是标签。标签0表示负面情绪，标签1表示正面情绪。为了建立一个功能模型，我们需要更多的数据(在我的实践中，如果只有两个类并且类是平衡的，那么一千或更多的标记数据点将开始给出好的结果)。</p><p id="3653" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们做一些经典的文本预处理:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文本预处理功能</p></figure><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="1d43" class="mv mw it mr b gy mx my l mz na">X_train = [x[0] for x in d] # Text<br/>Y_train = [y[1] for y in d] # Label</span><span id="e585" class="mv mw it mr b gy nv my l mz na">X_train = [clean_text(x) for x in X_train]</span></pre><p id="076c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">清理后的文本(<strong class="lb iu"> X_train </strong>):</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="603a" class="mv mw it mr b gy mx my l mz na">'this article is awesome'<br/>'there are just too much words here'<br/>'the math is actually wrong here'<br/>'i really enjoy learning new stuff'<br/>'i am kinda lazy so i just skim these texts'<br/>'who cares about ai'<br/>'i will surely be a better person after reading this'<br/>'the author is pretty cute'</span></pre><p id="a0d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标签(<strong class="lb iu"> Y_train </strong>):</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="0947" class="mv mw it mr b gy mx my l mz na">[1, 0, 0, 1, 0, 0, 1, 1]</span></pre><p id="35c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们在<strong class="lb iu"> X_train </strong>矩阵和类别矩阵<strong class="lb iu"> Y_train中有了预处理文本，我们需要为神经网络构建输入。</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="ecdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有嵌入层的深度学习模型的输入使用<strong class="lb iu">嵌入矩阵</strong>。嵌入矩阵是行大小等于文档中唯一单词数量的矩阵，并且具有嵌入向量维数的列大小。因此，为了构建嵌入矩阵，需要创建单词嵌入向量或者使用预先训练的单词嵌入。在这个例子中，我们将读取一个虚构的单词嵌入文件并构建矩阵。</p><p id="c594" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">存储单词嵌入的常用格式是在一个文本文档<strong class="lb iu">中。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/06bad596a5aa02d2abcb400d4bf6d52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*jRCfdh83XsIhHlbr_IJk6A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">小型嵌入示例</p></figure><p id="55d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们姑且称上面的嵌入文件<strong class="lb iu">为mini_embedding.txt </strong>。要快速复制粘贴，请使用:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="bbad" class="mv mw it mr b gy mx my l mz na">beautiful 1.5804182 0.25605154<br/>boy -0.4558624 -1.5827272<br/>can 0.9358587 -0.68037164<br/>children -0.51683635 1.4153042<br/>daughter 1.1436981 0.66987246<br/>family -0.33289963 0.9955545</span></pre><p id="53ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，嵌入维度等于2，但是在来自链接<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a>的单词嵌入中，维度是300。在这两种情况下，结构都是单词是第一个元素，后面是由空格分隔的系数。当行尾有新的行分隔符时，坐标结束。</p><p id="1bf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了阅读这样的文本文档，让我们创建一个类:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">读取嵌入并创建嵌入矩阵的类</p></figure><p id="48aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们假设您在<strong class="lb iu">嵌入</strong>文件夹中有嵌入文件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f180a83df7347730e71920493b7249ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*RnuVsDj7utGVm_1LIt9TmA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文件结构</p></figure><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="bffb" class="mv mw it mr b gy mx my l mz na">embedding = Embeddings(<br/>  'embeddings/mini_embedding.txt', <br/>  vector_dimension=2<br/>)<br/>embedding_matrix = embedding.create_embedding_matrix()</span></pre><p id="79ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们尚未扫描任何文档，因此嵌入矩阵将返回<strong class="lb iu"> mini_embeddings.txt </strong>文件中的所有单词:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="a39b" class="mv mw it mr b gy mx my l mz na">array([[ 1.58041823,  0.25605154],<br/>       [-0.4558624 , -1.58272719],<br/>       [ 0.93585873, -0.68037164],<br/>       [-0.51683635,  1.41530418],<br/>       [ 1.1436981 ,  0.66987246],<br/>       [-0.33289963,  0.99555451]])</span></pre><p id="51d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">嵌入矩阵的列数将始终等于嵌入维度的数量，行数将等于文档中唯一单词的数量或用户定义的行数。</strong></p><p id="2c4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除非您的机器中有大量的RAM，否则通常建议您最大限度地使用用于构建嵌入矩阵的训练文档的所有唯一单词来创建嵌入矩阵。在手套嵌入文件中，有数百万个单词，其中大多数甚至在大多数文本文档中一次也没有出现。因此，不建议使用来自大型嵌入文件的所有唯一单词来创建嵌入矩阵。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="66a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度学习模型中的预训练单词嵌入被放入矩阵中，并在输入层<strong class="lb iu">中用作权重</strong>。来自Keras API文档【https://keras.io/layers/embeddings/ T2】:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="4db3" class="mv mw it mr b gy mx my l mz na">keras.layers.Embedding(<strong class="mr iu">input_dim</strong>, <strong class="mr iu">output_dim</strong>,...)</span><span id="7354" class="mv mw it mr b gy nv my l mz na">Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -&gt; [[0.25, 0.1], [0.6, -0.2]]</span><span id="c9f0" class="mv mw it mr b gy nv my l mz na"><strong class="mr iu">This layer can only be used as the first layer in a deep learning model.</strong></span></pre><p id="7310" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两个主要的输入参数是<strong class="lb iu">输入_尺寸</strong>和<strong class="lb iu">输出_尺寸</strong>。</p><p id="b9d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> input_dim </strong>等于我们文本中唯一单词的总数(或者用户定义的特定数量的唯一单词)。</p><p id="e6a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">输出尺寸</strong>等于嵌入向量尺寸。</p><p id="1671" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了构建唯一的单词字典，我们将使用Keras库中的<strong class="lb iu"> Tokenizer() </strong>方法。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="4cdb" class="mv mw it mr b gy mx my l mz na">from keras.preprocessing.text import Tokenizer</span><span id="f5d7" class="mv mw it mr b gy nv my l mz na">tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(X_train)</span></pre><p id="c83f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提醒一下，我们预处理过的<strong class="lb iu"> X_train </strong>是:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="5d5b" class="mv mw it mr b gy mx my l mz na">'this article is awesome'<br/>'there are just too much words here'<br/>'the math is actually wrong here'<br/>'i really enjoy learning new stuff'<br/>'i am kinda lazy so i just skim these texts'<br/>'who cares about ai'<br/>'i will surely be a better person after reading this'<br/>'the author is pretty cute'</span></pre><p id="136b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Tokenizer()方法创建唯一单词的内部字典，并为每个单词分配一个整数。<strong class="lb iu"> tokenizer.word_index </strong>的输出:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="7047" class="mv mw it mr b gy mx my l mz na">{'i': 1,<br/> 'is': 2,<br/> 'this': 3,<br/> 'just': 4,<br/> 'here': 5,<br/> 'the': 6,<br/> 'article': 7,<br/> 'awesome': 8,<br/> 'there': 9,<br/> 'are': 10,<br/> 'too': 11,<br/> 'much': 12,<br/> 'words': 13,<br/> 'math': 14,<br/> 'actually': 15,<br/> 'wrong': 16,<br/> 'really': 17,<br/> 'enjoy': 18,<br/> 'learning': 19,<br/> 'new': 20,<br/> 'stuff': 21,<br/> 'am': 22,<br/> 'kinda': 23,<br/> 'lazy': 24,<br/> 'so': 25,<br/> 'skim': 26,<br/> 'these': 27,<br/> 'texts': 28,<br/> 'who': 29,<br/> 'cares': 30,<br/> 'about': 31,<br/> 'ai': 32,<br/> 'will': 33,<br/> 'surely': 34,<br/> 'be': 35,<br/> 'a': 36,<br/> 'better': 37,<br/> 'person': 38,<br/> 'after': 39,<br/> 'reading': 40,<br/> 'author': 41,<br/> 'pretty': 42,<br/> 'cute': 43}</span></pre><p id="4786" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的X_train文本中有43个独特的单词。让我们将文本转换成索引列表:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="171e" class="mv mw it mr b gy mx my l mz na">tokenizer.texts_to_sequences(X_train)</span><span id="0d53" class="mv mw it mr b gy nv my l mz na"><strong class="mr iu">[[3, 7, 2, 8],<br/> [9, 10, 4, 11, 12, 13, 5],<br/> [6, 14, 2, 15, 16, 5],<br/> [1, 17, 18, 19, 20, 21],<br/> [1, 22, 23, 24, 25, 1, 4, 26, 27, 28],<br/> [29, 30, 31, 32],<br/> [1, 33, 34, 35, 36, 37, 38, 39, 40, 3],<br/> [6, 41, 2, 42, 43]]</strong></span></pre><p id="6865" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们X_train矩阵中的第一句话<strong class="lb iu">‘这篇文章真棒’</strong>转换成一个<strong class="lb iu">【3，7，2，8】</strong>的列表。这些索引表示tokenizer创建的字典中的键值:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="992a" class="mv mw it mr b gy mx my l mz na">{...<br/> <strong class="mr iu">'is': 2,</strong><br/> <strong class="mr iu">'this': 3,</strong><br/> ...<br/> <strong class="mr iu">'article': 7,</strong><br/> <strong class="mr iu">'awesome': 8,</strong><br/> ...}</span></pre><p id="7e9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">方法给了我们一个列表列表，其中每个条目都有不同的维度，并且是无结构的。任何机器学习模型都需要知道特征维度的数量，并且对于新观察值的训练和预测，该数量必须相同。为了将序列转换成用于深度学习训练的结构良好的矩阵，我们将使用来自Keras的<strong class="lb iu">pad _ sequences()</strong>方法:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="91f8" class="mv mw it mr b gy mx my l mz na">import numpy as np<br/>from keras.preprocessing.sequence import pad_sequences</span><span id="5095" class="mv mw it mr b gy nv my l mz na"># Getting the biggest sentence<br/>max_len = np.max([len(text.split()) for text in X_train])</span><span id="7d52" class="mv mw it mr b gy nv my l mz na"># Creating the padded matrices<br/>X_train_NN = tokenizer.texts_to_sequences(X_train)<br/>X_train_NN = pad_sequences(string_list, maxlen=max_len)</span></pre><p id="2663" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">X_train_NN对象如下所示:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="193f" class="mv mw it mr b gy mx my l mz na">array([[ 0,  0,  0,  0,  0,  0,  3,  7,  2,  8],<br/>       [ 0,  0,  0,  9, 10,  4, 11, 12, 13,  5],<br/>       [ 0,  0,  0,  0,  6, 14,  2, 15, 16,  5],<br/>       [ 0,  0,  0,  0,  1, 17, 18, 19, 20, 21],<br/>       [ 1, 22, 23, 24, 25,  1,  4, 26, 27, 28],<br/>       [ 0,  0,  0,  0,  0,  0, 29, 30, 31, 32],<br/>       [ 1, 33, 34, 35, 36, 37, 38, 39, 40,  3],<br/>       [ 0,  0,  0,  0,  0,  6, 41,  2, 42, 43]])</span></pre><p id="9a07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">行数等于X_train元素数，列数等于最长的句子(等于<strong class="lb iu"> 10 </strong>个单词)。列数通常由用户在阅读文档之前定义。这是因为当处理现实生活中的标记文本时，最长的文本可能非常长(数千字)，这将导致训练神经网络时计算机内存的问题。</p><p id="6c50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了使用预处理文本为神经网络创建一个整洁的输入，我使用了我定义的类<strong class="lb iu"> TextToTensor </strong>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将文本转换为结构化矩阵</p></figure><p id="57f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">张量是一个可以容纳N维数据的容器。向量可以在1维空间存储数据，矩阵可以在2维空间存储数据，张量可以在n维空间存储数据。有关张量的更多信息:</p><p id="f2c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">https://www.kdnuggets.com/2018/05/wtf-tensor.html<a class="ae ky" href="https://www.kdnuggets.com/2018/05/wtf-tensor.html" rel="noopener ugc nofollow" target="_blank"/></p><p id="b444" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">texttotenser</strong>的完整用法:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="c5d1" class="mv mw it mr b gy mx my l mz na"># Tokenizing the text<br/>tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(X_train)</span><span id="2281" class="mv mw it mr b gy nv my l mz na"># Getting the longest sentence<br/>max_len = np.max([len(text.split()) for text in X_train])</span><span id="308e" class="mv mw it mr b gy nv my l mz na"># Converting to tensor<br/>TextToTensor_instance = TextToTensor(<br/>tokenizer=tokenizer,<br/>max_len=max_len<br/>)</span><span id="c8ab" class="mv mw it mr b gy nv my l mz na">X_train_NN = TextToTensor_instance.string_to_tensor(X_train)</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="0380" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以从文本中创建一个张量，我们可以开始使用Keras API中的<strong class="lb iu">嵌入</strong>层。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="5c27" class="mv mw it mr b gy mx my l mz na">from keras.models import Sequential<br/>from keras.layers import Embedding<br/><br/>model = Sequential()<br/>model.add(Embedding(<br/>  input_dim=44, <br/>  output_dim=3, <br/>  input_length=max_len))<br/><br/>model.compile('rmsprop', 'mse')<br/>output_array = model.predict(<!-- -->X_train_NN<!-- -->)[0]</span></pre><p id="5935" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，在嵌入层中，input_dim等于44，但是我们的文本只有43个唯一的单词。这是因为Keras API中的嵌入定义:</p><p id="fd2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> input_dim </strong> : int &gt; 0。词汇的大小，即<strong class="lb iu">最大整数索引+ 1。</strong></p><p id="71e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">output_array如下所示:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="8b46" class="mv mw it mr b gy mx my l mz na">array([[-0.03353775,  0.01123261,  0.03025569],<br/>       [-0.03353775,  0.01123261,  0.03025569],<br/>       [-0.03353775,  0.01123261,  0.03025569],<br/>       [-0.03353775,  0.01123261,  0.03025569],<br/>       [-0.03353775,  0.01123261,  0.03025569],<br/>       [-0.03353775,  0.01123261,  0.03025569],<br/>       <strong class="mr iu">[ 0.04183744, -0.00413301,  0.04792741],<br/>       [-0.00870543, -0.00829206,  0.02079277],<br/>       [ 0.02819189, -0.04957005,  0.03384084],<br/>       [ 0.0394035 , -0.02159669,  0.01720046]</strong>], dtype=float32)</span></pre><p id="4883" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入序列为(X_train_NN的第一个元素):</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="b166" class="mv mw it mr b gy mx my l mz na">array([0, 0, 0, 0, 0, 0, <strong class="mr iu">3, 7, 2, 8</strong>])</span></pre><p id="cd5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嵌入层自动给一个整数分配一个大小为output_dim的向量，在我们的例子中等于3。我们不能控制内部计算，并且分配给每个整数索引的向量不具有这样的特征，即在语义上紧密相关的单词之间的距离小于那些具有不同语义的单词之间的距离。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="25dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了解决这个问题，我们将使用斯坦福大学自然语言处理系的预训练单词嵌入(<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a>)。为了创建嵌入矩阵，我们将使用先前定义的方法。</p><p id="a008" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们假设<strong class="lb iu"> X_train </strong>再次是预处理文本的列表。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="3451" class="mv mw it mr b gy mx my l mz na">embed_path = 'embeddings\\glove.840B.300d.txt'<br/>embed_dim = 300</span><span id="1bbe" class="mv mw it mr b gy nv my l mz na"># Tokenizing the text<br/>tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(X_train)</span><span id="0b04" class="mv mw it mr b gy nv my l mz na"># Creating the embedding matrix<br/>embedding = Embeddings(embed_path, embed_dim)<br/>embedding_matrix = embedding.create_embedding_matrix(tokenizer, len(tokenizer.word_counts))</span></pre><p id="5979" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而文档<strong class="lb iu"> glove.840B.300d.txt </strong>有几十万个唯一字，最终嵌入矩阵的形状是<strong class="lb iu"> (44，300) </strong>。这是因为我们想尽可能地节省内存，整个文档中唯一单词的数量等于<strong class="lb iu"> 44 </strong>。保存文本文档中所有其他单词的坐标是一种浪费，因为我们不会在任何地方使用它们。</p><p id="2809" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了在深度学习模型中使用嵌入矩阵，我们需要将该矩阵作为嵌入层中的<strong class="lb iu">权重</strong>参数进行传递。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="3eaf" class="mv mw it mr b gy mx my l mz na">from keras.models import Sequential<br/>from keras.layers import Embedding</span><span id="f017" class="mv mw it mr b gy nv my l mz na"># Converting to tensor<br/>TextToTensor_instance = TextToTensor(<br/>tokenizer=tokenizer,<br/>max_len=max_len<br/>)<br/>X_train_NN = TextToTensor_instance.string_to_tensor(X_train)</span><span id="ec3e" class="mv mw it mr b gy nv my l mz na">model = Sequential()<br/>model.add(Embedding(<br/>  input_dim=44, <br/>  output_dim=300, <br/>  input_length=max_len,<br/>  <strong class="mr iu">weights=</strong>[embedding_matrix]<!-- -->))<br/><br/>model.compile('rmsprop', 'mse')<br/>output_array = model.predict(<!-- -->X_train_NN<!-- -->)[0]</span></pre><p id="6aaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">output_array的形状现在是<strong class="lb iu"> (10，300) </strong>，输出如下所示:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="9e11" class="mv mw it mr b gy mx my l mz na">array([[ 0.18733 ,  0.40595 , -0.51174 , ...,  0.16495 ,  0.18757 ,<br/>         0.53874 ],<br/>       [ 0.18733 ,  0.40595 , -0.51174 , ...,  0.16495 ,  0.18757 ,<br/>         0.53874 ],<br/>       [ 0.18733 ,  0.40595 , -0.51174 , ...,  0.16495 ,  0.18757 ,<br/>         0.53874 ],<br/>       ...,<br/>       [-0.34338 ,  0.1677  , -0.1448  , ...,  0.095014, -0.073342,<br/>         0.47798 ],<br/>       [-0.087595,  0.35502 ,  0.063868, ...,  0.03446 , -0.15027 ,<br/>         0.40673 ],<br/>       [ 0.16718 ,  0.30593 , -0.13682 , ..., -0.035268,  0.1281  ,<br/>         0.023683]], dtype=float32)</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/853f6d3028fb5e5663d2c472c62ed217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aMX5elZh-cQDM7ss"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@sherihoo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Sheri Hooley </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="dd85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们已经介绍了:</p><ul class=""><li id="ff51" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu">什么是单词嵌入</strong></li><li id="c409" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">从文本中创建张量</strong></li><li id="e61d" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">创建单词嵌入矩阵</strong></li><li id="e26a" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">什么是Keras嵌入层</strong></li><li id="1c71" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">如何利用嵌入矩阵</strong></li></ul><p id="c85f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们把所有的事情放在一起，处理一个现实生活中的问题，决定一条来自Twitter的推文是否是关于自然灾害的。</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="0032" class="mv mw it mr b gy mx my l mz na"># Importing generic python packages<br/>import pandas as pd</span><span id="e838" class="mv mw it mr b gy nv my l mz na"># Reading the data<br/>train = pd.read_csv('data/train.csv')[['text', 'target']]<br/>test = pd.read_csv('data/test.csv')</span><span id="18ad" class="mv mw it mr b gy nv my l mz na"># Creating the input for the pipeline<br/>X_train = train['text'].tolist()<br/>Y_train = train['target'].tolist()</span><span id="fd13" class="mv mw it mr b gy nv my l mz na">X_test = test['text'].tolist()</span></pre><p id="c3cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">列车数据的形状是<strong class="lb iu"> (7613，2)，</strong>的意思是，有<strong class="lb iu">个</strong> 7613条tweets可以处理。让我们检查一下推文的分布情况:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="25d1" class="mv mw it mr b gy mx my l mz na">train.groupby(['target'], as_index=False).count()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/79a0d769b0bd3f6ad5a9dafc7a008eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*E6v7wSjrwtxmt1y4Sx3uBg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">推文的分发</p></figure><p id="354f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所看到的，至少对于真实世界的数据来说，这些类是平衡的。</p><p id="a342" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“好”推文示例:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="b7c1" class="mv mw it mr b gy mx my l mz na">[<br/>'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',<br/>'Forest fire near La Ronge Sask. Canada',<br/>"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected",<br/>'13,000 people receive #wildfires evacuation orders in California ',<br/>'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school'<br/>]</span></pre><p id="80c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">糟糕推文的一个例子:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="58b4" class="mv mw it mr b gy mx my l mz na">[<br/>"What's up man?",<br/>'I love fruits',<br/>'Summer is lovely',<br/>'My car is so fast',<br/>'What a goooooooaaaaaal!!!!!!'<br/>]</span></pre><p id="75d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们做一些文本预处理，看看上面的单词:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="4fde" class="mv mw it mr b gy mx my l mz na"># Counting the number of words<br/>from collections import Counter</span><span id="a34c" class="mv mw it mr b gy nv my l mz na"># Plotting functions<br/>import matplotlib.pyplot as plt</span><span id="401d" class="mv mw it mr b gy nv my l mz na">X_train = [clean_text(text) for text in X_train]<br/>Y_train = np.asarray(Y_train)</span><span id="39f1" class="mv mw it mr b gy nv my l mz na"># Tokenizing the text<br/>tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(X_train)</span><span id="0b20" class="mv mw it mr b gy nv my l mz na"># Getting the most frequent words</span><span id="7018" class="mv mw it mr b gy nv my l mz na">d1 = train.loc[train['target']==1, 'text'].tolist()<br/>d0 = train.loc[train['target']==0, 'text'].tolist()</span><span id="1737" class="mv mw it mr b gy nv my l mz na">d1 = [clean_text(x, stop_words=stop_words) for x in d1]<br/>d0 = [clean_text(x, stop_words=stop_words) for x in d0]</span><span id="7ae2" class="mv mw it mr b gy nv my l mz na">d1_text = ' '.join(d1).split()<br/>d0_text = ' '.join(d0).split()</span><span id="a91c" class="mv mw it mr b gy nv my l mz na">topd1 = Counter(d1_text)<br/>topd0 = Counter(d0_text)</span><span id="dd42" class="mv mw it mr b gy nv my l mz na">topd1 = topd1.most_common(20)<br/>topd0 = topd0.most_common(20)</span><span id="59ab" class="mv mw it mr b gy nv my l mz na">plt.bar(range(len(topd1)), [val[1] for val in topd1], align='center')<br/>plt.xticks(range(len(topd1)), [val[0] for val in topd1])<br/>plt.xticks(rotation=70)<br/>plt.title('Disaster tweets')<br/>plt.show()</span><span id="5c2c" class="mv mw it mr b gy nv my l mz na">plt.bar(range(len(topd0)), [val[1] for val in topd0], align='center')<br/>plt.xticks(range(len(topd0)), [val[0] for val in topd0])<br/>plt.xticks(rotation=70)<br/>plt.title('Not disaster tweets')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/d988790c2ffaab6e5277f9eb97399317.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*-jkKDGmobkQzm1VhkCWIsQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">灾难推文中的热门词汇</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/7a278236bf630a7ca4f33c622b207009.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bTdkUw-LzVMIVSfMG3D-nQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">非灾难推文中的热门词汇</p></figure><p id="d285" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非灾难词汇比灾难词汇更通用。人们可能期望手套嵌入和深度学习模型能够捕捉到这些差异。</p><p id="98b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每条推文的字数分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/3cce37973d86cfc3df3aaa9a68f2b5d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*00UjKZXJ0j3XcsaZ9A6ERA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">字数分布</p></figure><p id="f22d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据上面的分布，我们可以说，创建列大小为20的输入张量只会排除tweets中非常少量的单词。在pro方面，我们将赢得大量的计算时间。</p><p id="d110" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度学习模型架构如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="fc06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文中提到的包装所有东西的管道也被定义为python中的一个类:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="f4fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整的代码和整个工作管道可以在这里找到:</p><div class="nb nc gp gr nd ne"><a href="https://github.com/Eligijus112/twitter-genuine-tweets" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">eligijus 112/Twitter-正版-推文</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">这个项目可以作为一个模板来做任何NLP任务。在这个项目中的情绪是一个字符串是否认为…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">github.com</p></div></div><div class="nn l"><div class="od l np nq nr nn ns ks ne"/></div></div></a></div><p id="08cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">若要训练模型，请使用以下代码:</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="372d" class="mv mw it mr b gy mx my l mz na">results = Pipeline(<br/>X_train=X_train,<br/>Y_train=Y_train,<br/>embed_path='embeddings\\glove.840B.300d.txt',<br/>embed_dim=300,<br/>stop_words=stop_words,<br/>X_test=X_test,<br/>max_len=20,<br/>epochs=10,<br/>batch_size=256<br/>)</span></pre><p id="a8cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们创建两个文本:</p><p id="ccfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">好</strong> =【“维尔纽斯失火！消防队在哪里？？？#紧急情况"]</p><p id="1699" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">不好</strong> =【“寿司还是披萨？生活是艰难的:(("]</p><pre class="kj kk kl km gt mq mr ms mt aw mu bi"><span id="ab97" class="mv mw it mr b gy mx my l mz na">TextToTensor_instance = TextToTensor(<br/>tokenizer=results.tokenizer,<br/>max_len=20<br/>)</span><span id="4857" class="mv mw it mr b gy nv my l mz na"># Converting to tensors<br/>good_nn = TextToTensor_instance.string_to_tensor(good)<br/>bad_nn = TextToTensor_instance.string_to_tensor(bad)</span><span id="9d0e" class="mv mw it mr b gy nv my l mz na"># Forecasting<br/>p_good = results.model.predict(<strong class="mr iu">good_nn</strong>)[0][0]<br/>p_bad = results.model.predict(<strong class="mr iu">bad_nn</strong>)[0][0]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/477c4d6f3f54f77f27b052066a0700c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*ED6lvCPvCTNpsd1wkLc0Sw.png"/></div></figure><p id="2c12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> p_bad = 0.014 </strong>和<strong class="lb iu"> p_good = 0.963 </strong>。这些概率是针对一条推文是否是关于一场灾难的问题。所以关于寿司的推文得分很低，关于火灾的推文得分很高。这意味着本文中提出的逻辑至少对虚构的句子有效。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="1039" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我有:</p><ul class=""><li id="4ccd" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu">展示了在监督问题中使用文本数据的逻辑和整体工作流程。</strong></li><li id="f7cc" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">分享了Python中的完整工作代码，该代码采用原始输入，将其转换为矩阵，并使用Tensorflow训练深度学习模型</strong></li><li id="f7d3" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">解读结果。</strong></li></ul><p id="1544" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文给出的代码可以应用于任何文本分类问题。编码快乐！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="d777" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]推特灾难数据集</p><p id="5e9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网址:<a class="ae ky" href="https://www.kaggle.com/competitions/nlp-getting-started/data" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/competitions/NLP-getting-started/data</a>执照:【https://opensource.org/licenses/Apache-2.0】T2</p></div></div>    
</body>
</html>