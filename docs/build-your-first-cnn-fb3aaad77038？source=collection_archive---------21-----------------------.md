# 建立你的第一个 CNN

> 原文：<https://towardsdatascience.com/build-your-first-cnn-fb3aaad77038?source=collection_archive---------21----------------------->

## 一种构建卷积神经网络的更好方法

![](img/0381a2aee85a4c7e53eeb11ef18726db.png)

# 定义

卷积神经网络(CNN)是一种深度神经网络，在图像识别和分类中是有效的。它们接受图像作为输入，通过内核从中提取特征，并预测所需的输出。在 CNN 中，数据的预处理是最少的，并且它们可以自学识别来自不同图像的重要特征。对象检测、对象识别、面部识别、自动驾驶等领域的几乎所有先进网络。以某种方式利用 CNN。

# 介绍

网上有很多关于从头开始构建 CNN 的教程。尽管它们中的大多数确实提供了高精度，但是它们忽略了内存和参数的大量使用。当谈到在行业中部署模型时，构建网络不仅要达到目标精度，还要优化内存消耗。在本文中，我们将学习如何为物体识别构建一个优化的 CNN。
为了保持正确的期望值，让我们设定一个目标:

**目标:**上的**MNIST 数据集**

> *1。达到 99.4 %的验证准确率
> 2。模型的总参数数应小于 15k
> 3。最大历元数= 20*

为了实现这个目标，我们将把我们的行动计划分成 4 个迭代。首先，我们将重点构建一个达到目标精度的模型。然后，我们将大幅降低参数，同时保持架构。这种参数的减少会导致精度的下降。为了弥补这一下降，我们将优化模型以满足我们的目标。
为了确保我们的模型在每次迭代中都是一致的，让我们添加一个条件，将每次迭代之间的修改限制为三次。
在我们开始设计网络之前，有几个术语我们应该熟悉——

## 核心

核是随机分配的权重矩阵，其在图像上被遍历和卷积。一个内核从图像中提取单个特征。

## 频道

卷积层的输出可以视为一组通道。
例如，1 个灰度图像(NxN) →与 64 个核的卷积(3x3) → 64 个通道((N-2)x(N-2))

## 感受野

感受野是感觉周围的一个区域，在这个区域内刺激可以影响感觉细胞的电活动。在 CNN 中，这是由通道/内核的单个细胞感知的图像部分。

> CNN 的模型结构是通过将焦点保持在感受野上来设计的。通常，网络末端的模型的感受野与对象的大小相似。

## 卷积块

包含多个*卷积层(3x3)的块。*在每个后续层中，通常增加内核/文件的数量，以确保从图像中提取所有重要特征。

每个卷积层都有一个激活函数(最好是 ReLU，因为它对参数要求很低，这使得反向传播很容易校正权重),它给网络增加了非线性。

## 过渡块

该块具有逐点(1x1) *卷积*以减少通道数量，以及*最大池*层以减少空间维度或通道大小。这个块的关键作用是过滤掉不相关的特征，限制参数的数量。

现在让我们开始设计网络

# 目标 1:普通网络

布局架构时首先要考虑的是*感受域*。在模型的末尾，感受野应该等于物体的大小。完整的架构是围绕这个概念设计的。添加成对的卷积块和过渡块以获得所需的感受野。

> 注意:过渡块有一个池层，这会导致信息丢失。在网络的末端，信道的大小很小，信息很重要。因此，避免了朝向网络末端的过渡块。

在最后一个卷积块之后，我们使输出变平，并使用 softmax 激活来提供类似概率的分布。

*在这个特定的问题中，第一个过渡块在网络中添加得更快，因为背景主要是白色的，不包含太多信息，并且大部分信息位于中心。所以在网络早期使用第一个 maxPool 比较方便。*

> *参数个数:1595316
> 最佳精度:99.93%(train)* [*Colab 笔记本*](https://colab.research.google.com/drive/1IJwulhX4WNWKUXVArwIJk_fuCXv-2zUN)

# 目标 2:减少参数

我们的目标是在少于 15k 的参数中实现 99.40%的精度。在本节中，主要重点是减少每个卷积中内核/文件的数量，以便总参数小于 15k，而不会对精度造成太大影响(这将在后面的步骤中通过优化技术来恢复)。

对以前型号的更新:

**1。减少 3×3 卷积层中的滤镜**
**2。减少 1x1 卷积层中的滤波器**
即使大幅减少所有层中的参数，总参数仍然超过 15k。
**3。恰好在第一过渡块和输出层的中间添加部分过渡块。**该部分过渡块将仅包括 1x1 卷积，但不包括最大池。
第三步确保只有重要的功能在网络中继续进行，并避免因最大池而可能发生的信息丢失。

> *参数个数:12490
> 最佳精度:99.71%(train)* [*Colab 笔记本*](https://colab.research.google.com/drive/1vS2UPSF6kIxHjGcJnqlTqltn3uD5X6jm)

# 目标 3:优化

由于我们已经丢弃了一大块参数，我们需要采用一些优化技术来确保我们的模型不会损害准确性。

对以前型号的更新:

**1。添加批量规范化图层。**
这有助于标准化数据，并限制网络稍后的规模变化。这固定了层输入的均值和方差，有助于更快的收敛和改进梯度流。
**2。选择合适的学习速率**
选择最适合网络的 LR(基于实验)
**3 .一个验证检查点**
精度与历元图并没有严格递增。精度不断波动，因此不能依赖最后一个历元来给出最佳结果。因此，添加了一个验证检查点，用于在每个时期后计算验证数据的准确性。

> *参数个数:12874
> 最佳精度:99.28% (val)，99.74%(train)* [*Colab 笔记本*](https://colab.research.google.com/drive/1Vuj54Tx26tBIJXyiDpa3Mrm2ORzVpUMw)

# 目标 4:过度拟合和进一步优化

请注意，训练和验证准确性之间的差距随着随后的时代而扩大。这说明模型过拟合。为了解决这个问题，我们将在每个 3x3 卷积后添加一个丢弃层。

对以前型号的更新:

**1。Add Dropout**
Dropout 随机跳过一些连接，强制网络学习其他参数。
**2。增加一个 LR 调度器**
一旦精度饱和，就需要降低学习率，这样模型才能收敛到更优的最小值，最终导致精度提高。
**3。增加批量**
大批量有助于更快地训练模型。
注意:批量不能过大。这可能导致损失函数陷入次优局部最小值。

> *参数个数:12874
> 最佳精度:99.36% (train)，99.41% (val)，第 19 历元* [*Colab 笔记本*](https://colab.research.google.com/drive/165lEq_jXsBIaOZnYHbNPreH0fBaJu8yB)

# 超出我们的目标

我们已经实现了我们的目标。现在，让我们通过进一步减少每层中的内核数量，在不影响精度的情况下，尝试将我们的模型参数减少到 8k 以下。

> *参数个数:7602
> 最佳精度:98.93% (train)，99.44% (val)，第 22 历元* [*Colab 笔记本*](https://colab.research.google.com/drive/1TeL7ueHTRB34ofw41dehvHA_xkdX0Cpj)

# 进一步地

1.  在稍微硬一点的数据集上实施这种方法，例如 CIFAR10、CIFAR100
2.  使用图像增强:最有效的方法之一规范你的网络
3.  添加跳过连接:这有助于模型拓宽其感受野范围，学习不同大小的对象。
4.  使用循环 LR 加快收敛速度

## 参考资料:

1.  MNIST:勒村、扬恩；科琳娜·科尔特斯；克里斯托弗 J.C 伯吉斯。[“MNIST 手写数字数据库，Yann LeCun，Corinna Cortes 和 Chris Burges”](http://yann.lecun.com/exdb/mnist/)。检索于 2013 年 8 月 17 日
2.  斯利瓦斯塔瓦，尼蒂什&辛顿，杰弗里&克里热夫斯基，亚历克斯&苏茨基弗，伊利亚&萨拉胡季诺夫，鲁斯兰。(2014).[退出:防止神经网络过度拟合的简单方法。机器学习研究杂志](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)。15.1929–1958.