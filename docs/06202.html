<html>
<head>
<title>Evolution of Language Models: N-Grams, Word Embeddings, Attention &amp; Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语言模型的演变:N-gram、单词嵌入、注意力和变形金刚</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evolution-of-language-models-n-grams-word-embeddings-attention-transformers-a688151825d2?source=collection_archive---------9-----------------------#2020-05-19">https://towardsdatascience.com/evolution-of-language-models-n-grams-word-embeddings-attention-transformers-a688151825d2?source=collection_archive---------9-----------------------#2020-05-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ac51a7ce461d39da0654300475745d41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rWhSy2fLidHCYxPH"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">约翰尼斯·普莱尼奥在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的图片</p></figure><div class=""/><div class=""><h2 id="bdcf" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">在这篇文章中，我认为整理一些关于自然语言处理(NLP)这些年来的进步的研究是很好的。</h2></div><p id="bcdf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你会惊讶于这个领域有多年轻。</p><p id="0253" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我知道我是。</p><p id="5ff0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是首先，让我们为什么是语言模型打下基础。</p><blockquote class="lu"><p id="316f" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">语言模型只是简单地将概率分配给单词序列的模型。</p></blockquote><p id="a5ab" class="pw-post-body-paragraph ky kz jj la b lb me kk ld le mf kn lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">对于神经语言模型来说，它可能是像N-Grams一样简单的东西。</p><p id="18e2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">甚至预训练的单词嵌入也是从语言建模中导出的，即Word2Vec、Glove、SVD、LSA</p><p id="8f85" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我倾向于把语言模型看作是一把更大的伞，其中包含了一大堆东西。</p><p id="8ef0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">说完了，让我们从头开始。:)</p><blockquote class="mj mk ml"><p id="fbef" class="ky kz mm la b lb lc kk ld le lf kn lg mn li lj lk mo lm ln lo mp lq lr ls lt im bi translated">注意:请容忍我到2000年。从那以后变得更有趣了。</p></blockquote><h1 id="c0f6" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">1948-1980年之前——N-gram和规则系统的诞生</h1><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/38a20ce4c23294aac2c01340349ca878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VOcBo3hN4Og-Twm5"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@tjsocoz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">蒂姆·比什</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="8943" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总的来说，这一时期的大多数NLP系统是基于规则的，最初的一些语言模型是以N-gram的形式出现的。</p><p id="9451" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据我的研究，不清楚是谁创造了这个术语。</p><p id="8907" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，N-Grams的第一次引用来自克劳德·香农在1948年发表的论文“<a class="ae jg" href="http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf" rel="noopener ugc nofollow" target="_blank">通信的数学理论</a>”。</p><p id="3095" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Shannon在本文中总共引用了N元语法3次。</p><p id="30c7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这意味着N-Grams的概念可能是在1948年之前由其他人制定的。</p><h1 id="9c9c" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">1980-1990年——计算能力的崛起和RNN的诞生</h1><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/78e6d531977c350fa39b9db59b881649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jqfn9m-2wC5pfLpcwzk2Ug.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">一个单位递归神经网络的示意图(RNN)，2017年6月19日，作者<a class="ae jg" href="https://commons.wikimedia.org/wiki/User:Ixnay" rel="noopener ugc nofollow" target="_blank"> fdeloche </a> ( <a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="77df" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这十年中，大多数NLP研究集中在能够做出概率决策的统计模型上。</p><p id="e14c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">1982年，John Hopfield引入了递归神经网络(RNN ),用于对序列数据(即文本或语音)的操作</p><p id="f671" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到1986年，第一个将单词表示为向量的想法出现了。这些研究是由现代人工智能研究的教父之一杰弗里·辛顿进行的。(<a class="ae jg" href="https://stanford.edu/~jlmcc/papers/PDP/Chapter1.pdf" rel="noopener ugc nofollow" target="_blank">辛顿等人1986</a>；<a class="ae jg" href="https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf" rel="noopener ugc nofollow" target="_blank">鲁梅尔哈特等人1986年</a></p><h1 id="57ba" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">1990-2000—自然语言处理研究的兴起和LSTM的诞生</h1><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/5e7c17b43a63268688c0a3f1677151f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-4NZcM5LFHnf7Xp0Cyeu0Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">一个单位的长短期记忆图(LSTM)，2017年6月20日由<a class="ae jg" href="https://commons.wikimedia.org/wiki/User:Ixnay" rel="noopener ugc nofollow" target="_blank"> fdeloche </a> ( <a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Long_Short-Term_Memory.svg" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="44b6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在20世纪90年代，NLP分析开始流行起来。</p><p id="7569" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">n元语法在理解文本数据方面变得非常有用。</p><p id="643d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到1997年，<a class="ae jg" href="https://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener ugc nofollow" target="_blank">长短期记忆网络</a> (LSTM)的概念由Hochreiter等人(1997)提出。</p><p id="1bec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，在这个时期仍然缺乏计算能力来充分利用神经语言模型的最大潜力。</p><h1 id="6084" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">2003年——第一个神经语言模型</h1><p id="2a57" class="pw-post-body-paragraph ky kz jj la b lb no kk ld le np kn lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">2003年，Bengio等人(2003)提出了第一个<a class="ae jg" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank">前馈神经网络语言模型</a>。</p><p id="524b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Bengio等人(2003)的模型由一个单独的隐藏层前馈网络组成，用于预测序列的下一个单词。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7892bed85d1c16780b1265857b62dbbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*Nj81Dc6OpYT9G9kgT3DOrA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">本吉奥等人于2003年提出的第一个神经语言模型(<a class="ae jg" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="19c7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然特征向量在这个时候已经存在，但是Bengio等人(2003)将这个概念带给了大众。</p><blockquote class="lu"><p id="c545" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">今天，我们称它们为W<em class="nu">order嵌入。:)</em></p></blockquote><blockquote class="mj mk ml"><p id="bb1a" class="ky kz mm la b lb me kk ld le mf kn lg mn mg lj lk mo mh ln lo mp mi lr ls lt im bi translated">注意:还有很多其他的研究，比如<a class="ae jg" href="https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf" rel="noopener ugc nofollow" target="_blank">用神经网络进行多任务学习</a> (Collobert &amp; Weston，2008)以及在这十年中研究的更多。</p></blockquote><h1 id="f7d7" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">2013年——广泛的预训练单词嵌入的诞生(Google的Word2Vec)</h1><p id="f1a3" class="pw-post-body-paragraph ky kz jj la b lb no kk ld le np kn lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">2013年，谷歌推出了<a class="ae jg" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>。(米科洛夫等人，2013年)</p><p id="b7f4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Mikolov等人(2013)的目标是引入新技术，以便能够从可跨NLP应用程序转移的庞大语料库中学习高质量的单词嵌入。</p><p id="a0de" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些技术是:</p><ul class=""><li id="3e4a" class="nv nw jj la b lb lc le lf lh nx ll ny lp nz lt oa ob oc od bi translated">连续词袋(CBOW)</li><li id="0151" class="nv nw jj la b lb oe le of lh og ll oh lp oi lt oa ob oc od bi translated">跳跃图</li></ul><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/1e39d4155dd3fc5c3ab8189fb6cdaea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kv0ZBh7nCi2TDDHJbmvBBw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Word2Vec型号。CBOW架构基于上下文预测当前单词，而Skip-gram在给定当前单词的情况下预测周围的单词。作者Mikolov等人，2013年(<a class="ae jg" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="1198" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Mikolov等人(2013)的预训练单词嵌入的结果为未来几年的大量NLP应用铺平了道路。</p><p id="6956" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">直到今天，人们仍然在各种自然语言处理应用中使用预训练的单词嵌入。</p><p id="9cb0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正是在这个时期，LSTMs、RNNs和门控循环单元(GRU)也开始被广泛用于许多不同的NLP应用。</p><h1 id="90a0" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">2014年—斯坦福:全球向量(Glove)</h1><p id="0d57" class="pw-post-body-paragraph ky kz jj la b lb no kk ld le np kn lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">Word2Vec推出一年后，斯坦福大学的Pennington等人(2014)推出了<a class="ae jg" href="https://www-nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">手套</a>。</p><p id="c465" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Glove是一组预训练的单词嵌入，使用不同的技术在不同的语料库上训练。</p><p id="505e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Pennington等人(2014)发现，单词嵌入可以通过共现矩阵来学习，并证明他们的方法在单词相似性任务和命名实体识别(NER)方面优于Word2Vec。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/27cbb2d177069d7c2a42a626a2b4ffd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvKfEwc-Dpq6MMzoEofaxw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Pennington等人2014年对单词类比任务Glove vs CBOW vs Skip-Gram的总体准确性(<a class="ae jg" href="https://www-nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><blockquote class="mj mk ml"><p id="0db0" class="ky kz mm la b lb lc kk ld le lf kn lg mn li lj lk mo lm ln lo mp lq lr ls lt im bi translated">作为轶事，我相信更多的应用程序使用Glove而不是Word2Vec。</p></blockquote><h1 id="5db3" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">2015 —回归:奇异值分解和LSA词嵌入&amp;注意力模型的诞生</h1><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/dc52f36b264c93b8527939f1db75fe8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_m4Ir6Z2Dtz4mOy-"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@scienceinhd?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">科学高清</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="f39a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">神经网络模型的最新趋势是<em class="mm">似乎</em>在单词相似性和类比检测任务方面优于传统模型。</p><p id="51c5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正是在这里，研究人员Levy等人(2015年)对这些趋势方法进行了一项<a class="ae jg" href="https://www.aclweb.org/anthology/Q15-1016.pdf" rel="noopener ugc nofollow" target="_blank">研究</a>，以了解它们如何与传统的统计方法相比较。</p><p id="f4d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Levy等人(2015)发现，通过适当的调整，SVD和LSA等经典矩阵分解方法获得了与Word2Vec或Glove类似的结果。</p><p id="aa76" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">他们的结论是，新旧方法之间的性能差异不明显，没有证据表明任何一种方法比其他方法有优势。</p><blockquote class="lu"><p id="dbb1" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">我想这里的教训是，新的闪亮的玩具并不总是比旧的(不那么闪亮的)玩具好。</p></blockquote><h2 id="eacc" class="om mr jj bd ms on oo dn mw op oq dp na lh or os nc ll ot ou ne lp ov ow ng ox bi translated"><strong class="ak">注意力模型的诞生</strong></h2><p id="5b8e" class="pw-post-body-paragraph ky kz jj la b lb no kk ld le np kn lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">在以前的研究中，使用RNNs的神经机器翻译(NMT)的问题是，如果句子太长，它们往往会“忘记”所学的内容。</p><p id="a271" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这被称为“长期依赖”问题。</p><p id="7370" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，Bahdanau等人(2015)提出了<a class="ae jg" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">注意机制</a>来解决这个问题。</p><p id="f2ee" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意力机制复制了人类如何进行翻译任务，而不是让模型在翻译前记住整个输入序列。</p><p id="239d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该机制允许模型只关注最有助于模型正确翻译单词的单词。</p></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><p id="1587" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">精神食粮。</p><blockquote class="lu"><p id="5fbc" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">在开始翻译整段文字之前，你会完整地阅读整段文字吗？</p></blockquote><p id="16c9" class="pw-post-body-paragraph ky kz jj la b lb me kk ld le mf kn lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">当然不是。</p><p id="abd1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到那时，你可能已经忘记你读过的东西了。</p><blockquote class="lu"><p id="26bb" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">那么，为什么我们应该期望一个模型在开始翻译任务之前记住<strong class="ak">所有的事情</strong>？</p></blockquote><p id="64b3" class="pw-post-body-paragraph ky kz jj la b lb me kk ld le mf kn lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated"><strong class="la jk">这是创造注意力机制背后的直觉。</strong></p></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><h1 id="c0b6" class="mq mr jj bd ms mt pf mv mw mx pg mz na kp ph kq nc ks pi kt ne kv pj kw ng nh bi translated">2016 —从神经机器翻译到注意力集中的图像字幕</h1><p id="68f1" class="pw-post-body-paragraph ky kz jj la b lb no kk ld le np kn lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">在注意力机制被提出后不久，其他研究人员如徐等人(2016)开始研究如何在其他自然语言处理应用中使用注意力，如<a class="ae jg" href="https://arxiv.org/pdf/1502.03044.pdf" rel="noopener ugc nofollow" target="_blank">图像字幕</a>。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/27bb04525b826a25ba659c83bbbde5b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lpfua6ajFawoMWDCx8r4Zw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">注意正确对象的例子(白色表示注意的区域，下划线表示相应的单词)徐等2016 ( <a class="ae jg" href="https://arxiv.org/pdf/1502.03044.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="77c4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意模型是如何“关注”图像的正确区域来为文字添加标题的。</p><h1 id="b5c0" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">2017 —变形金刚的诞生</h1><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/ec9cfec247509e49ad416c2f316db0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qnvcP9ztmZrruGSF"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://unsplash.com/@mitchel3uo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">米切尔·罗</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e2f8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">鉴于注意力机制在NLP应用中的成功，谷歌提出了一种新的简单网络架构，称为<a class="ae jg" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">转换器</a>。(瓦斯瓦尼等人，2017年)</p><p id="8adc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个<strong class="la jk">范式的转变</strong>,与NLP应用的标准构建方式不同。即使用用字嵌入初始化的rnn、LSTMs或gru。</p><p id="d7cb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Vaswani等人(2017年)将变压器<strong class="la jk">完全建立在</strong>注意力机制的基础上，完全摒弃了RNNs。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/6d9aae728e118ee6137c37d9392267e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*sOqN2gebImOhGIfpoNKsHQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">瓦斯瓦尼等人于2017年制作的变压器模型架构(<a class="ae jg" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="ed15" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Vaswani等人(2017年)的工作很快催生了第二年变形金刚的改变游戏规则的<a class="ae jg" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">双向编码器表示。(德夫林等人，2018年)</a></p><blockquote class="mj mk ml"><p id="5cd6" class="ky kz mm la b lb lc kk ld le lf kn lg mn li lj lk mo lm ln lo mp lq lr ls lt im bi translated">有趣的事实:BERT目前被Google搜索引擎使用。这里有一个<a class="ae jg" href="https://www.blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">到谷歌博客的链接</a>。</p></blockquote><h1 id="7cba" class="mq mr jj bd ms mt mu mv mw mx my mz na kp nb kq nc ks nd kt ne kv nf kw ng nh bi translated">2018-今天-预训练语言模型的时代</h1><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/b013e4ed3a95b5c5caf72779059edd59.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*h3gPsTR1A3Oa8XTQGMjG5g.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">《芝麻街》GIF”，2016年3月23日(<a class="ae jg" href="https://giphy.com/gifs/sesame-street-letters-vxt6BqY8rPzuo" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="182e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如前所述，在过去的两年中，NLP模型的构建方式发生了转变。</p><p id="4410" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">预训练的单词嵌入已经进化为预训练的语言模型。</p><p id="2e27" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里的直觉是用预训练的权重来初始化<strong class="la jk">整个模型架构</strong>，而不仅仅是用预训练的单词嵌入来初始化模型的输入层。</p><p id="7a4e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人们发现，预先训练的语言模型只需要微调就可以在各种NLP任务中表现得相对较高。</p><p id="9611" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些预训练架构的一些例子是，伯特，阿尔伯特，罗伯塔，ELMO，厄尼，XLNET，GTP-2，T5等。</p><p id="79f5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到今天为止，这些模型中的一些实际上超过了BERT。</p></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><h1 id="e9b1" class="mq mr jj bd ms mt pf mv mw mx pg mz na kp ph kq nc ks pi kt ne kv pj kw ng nh bi translated">2020年—期末注释</h1><p id="ff6b" class="pw-post-body-paragraph ky kz jj la b lb no kk ld le np kn lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">看起来我们终于到达了时间线的尽头！:)</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/eb782b6681ea172b78a0e58be1bc6ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*sdoJ3O2cZiPTrrm9WezNmQ.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">GIPHY Studios Originals制作的快乐无比激动GIF，2016年11月18日(<a class="ae jg" href="https://giphy.com/gifs/studiosoriginals-3oz8xRF0v9WMAUVLNK" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="9550" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我发现这个关于NLP研究进展的时间表非常有趣，希望你也一样！</p><p id="ba61" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">想象一下…</p><p id="7cff" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从引入预训练单词嵌入到转向预训练语言模型只用了5年时间。</p><p id="a699" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">令人惊讶的是，几十年来我们做事的方式在这个时期发生了巨大的转变。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/85e33f3fe799cc73e373594001735b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*-kEiThMHgEtKFk3w9ioJ7g.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">《天堂单身汉》第六季GIF，2019年8月7日(<a class="ae jg" href="https://giphy.com/gifs/bachelorinparadise-season-6-bachelor-in-paradise-s6e2-cnuNz0fTBIUGnx4F9T" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="5139" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">似乎NLP上的领域还是一个相当年轻的领域。</p><p id="1ca9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还远远不能将世界知识编码到机器中，以真正理解人类的语言。</p><p id="9430" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以这绝对是一个激动人心的时代！</p><p id="4551" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们拭目以待，看看未来会怎样！</p><p id="5f11" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就这样，我会在下一篇文章中看到你！</p><p id="8b4b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">阿迪欧。</p><blockquote class="mj mk ml"><p id="0698" class="ky kz mm la b lb lc kk ld le lf kn lg mn li lj lk mo lm ln lo mp lq lr ls lt im bi translated">声明:这绝不是一个详尽的研究。我只是研究了我个人认为NLP中的关键进步。:)</p></blockquote><p id="4e77" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">LinkedIn简介:<a class="ae jg" href="https://www.linkedin.com/in/timothy-tan-97587190/" rel="noopener ugc nofollow" target="_blank">蒂莫西·谭</a></p></div></div>    
</body>
</html>