<html>
<head>
<title>How to implement a Gaussian Naive Bayes Classifier in Python from scratch?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从零开始用 Python 实现一个高斯朴素贝叶斯分类器？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-impliment-a-gaussian-naive-bayes-classifier-in-python-from-scratch-11e0b80faf5a?source=collection_archive---------6-----------------------#2020-02-13">https://towardsdatascience.com/how-to-impliment-a-gaussian-naive-bayes-classifier-in-python-from-scratch-11e0b80faf5a?source=collection_archive---------6-----------------------#2020-02-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/059a51a5b2c968480fb8ea7825f79852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mGfs7axTHRez7GPPVyq_LQ.png"/></div></div></figure><p id="0999" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">你有没有问过自己最古老的机器学习算法是什么？</strong></p><p id="4cc9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">今天，我们有很多机器学习算法，从简单的 KNN 到集成算法，甚至神经网络。有时它们看起来如此复杂，以至于你可以认为它们是最近几年开发的，而机器学习，总的来说，是一种新事物。但是第一个算法出现的时间比你想象的要早。</p><h2 id="c31f" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated">朴素贝叶斯算法。</h2><p id="6871" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">朴素贝叶斯算法是最古老的机器学习形式之一。贝叶斯理论(该算法的基础)和统计学基础是在 18 世纪发展起来的。从那以后，直到 50 年代，计算都是手工完成的，直到出现了第一台计算机实现这种算法。</p><p id="dec1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">但是这个算法有什么简单到可以手动使用的呢？</strong></p><p id="5dbb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该算法的最简单形式由两个主要部分组成:</p><ul class=""><li id="df45" class="lx ly it kd b ke kf ki kj km lz kq ma ku mb ky mc md me mf bi translated">朴素贝叶斯公式(定理)。</li><li id="5072" class="lx ly it kd b ke mg ki mh km mi kq mj ku mk ky mc md me mf bi translated">和分布(在这种情况下是高斯分布)。</li></ul><h1 id="e7ee" class="ml la it bd lb mm mn mo le mp mq mr lh ms mt mu lk mv mw mx ln my mz na lq nb bi translated">朴素贝叶斯理论。</h1><p id="39ad" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">朴素贝叶斯理论在大多数情况下可以简化为一个公式:</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c29f34624c278f8b464cd61b12d16318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*c6dH2NjuRnHeKdbPgf2RJQ.png"/></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">朴素贝叶斯公式[来源—<a class="ae nl" href="https://miro.medium.com/max/640/1*7lg_uLm8_1fYGjxPbTrQFQ.png" rel="noopener">https://miro . medium . com/max/640/1 * 7lg _ ul M8 _ 1 fygjxpbtrqfq . png</a>]</p></figure><p id="bcc0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个公式意味着事件 A 发生的概率知道事件 B 已经发生了..</p><p id="6b1c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">不知何故，对朴素贝叶斯理论的解释超出了本文的范围，这就是为什么我强烈推荐你阅读这篇关于 NB 理论的文章。</p><h1 id="6597" class="ml la it bd lb mm mn mo le mp mq mr lh ms mt mu lk mv mw mx ln my mz na lq nb bi translated">什么是发行版？</h1><p id="026e" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">分布，基本上是显示数值是如何在数列中分散的，以及它们在这个数列中出现的频率。这里有一个例子:</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/6ac404d05cf7e1c0024fd3aab54ccb9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pAieg2qMdBu0aiEJs7fE9A.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">高斯分布[来源—<a class="ae nl" href="https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/1200px-Normal_Distribution_PDF.svg.png" rel="noopener ugc nofollow" target="_blank">https://upload . wikimedia . org/Wikipedia/commons/thumb/7/74/Normal _ Distribution _ pdf . SVG/1200 px-Normal _ Distribution _ pdf . SVG . png</a>]</p></figure><p id="c966" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">从上图中可以看出，高斯分布或正态分布取决于一个系列的两个参数——平均值和标准差。知道这两个参数的序列，我们可以找到它的分布函数。它有下一种形式:</p><figure class="nd ne nf ng gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/09ea4b104c4d67ee3a3736a2e95f8b75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*6h77xJ0Wd5xt8E2LPAi5PA.png"/></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">高斯分布函数[来源—<a class="ae nl" href="https://i.stack.imgur.com/bBIbn.png" rel="noopener ugc nofollow" target="_blank">https://i.stack.imgur.com/bBIbn.png</a></p></figure><p id="c793" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但是，我们为什么需要这个功能呢？很简单，世界上大多数数据都表示为连续值，但是你猜怎么着，你不能计算 X 值的概率来得到 v 值，它会是 0。为什么？从技术上来说，当你将某物除以无穷大时，你会得到什么？正确——零。</p><p id="0fd0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">那么，我们该如何解决这个问题呢？当然，使用高斯分布函数，如上图所示。输入一个序列的平均值和它的标准偏差，而不是 x 值，你可以找出 x 值出现的概率。瞧。</p><h1 id="acd3" class="ml la it bd lb mm mn mo le mp mq mr lh ms mt mu lk mv mw mx ln my mz na lq nb bi translated">那么这一切是如何协同工作的呢？</h1><p id="8901" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">我不知道为什么，但对我个人来说，有时候通过在代码中实现算法会更容易理解算法是如何工作的。让我们开始吧:</p><ol class=""><li id="2da1" class="lx ly it kd b ke kf ki kj km lz kq ma ku mb ky no md me mf bi translated">首先，让我们导入所有的依赖性:</li></ol><pre class="nd ne nf ng gt np nq nr ns aw nt bi"><span id="72a9" class="kz la it nq b gy nu nv l nw nx"># Importing all needed libraries<br/>import numpy as np<br/>import math</span></pre><p id="a142" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">就这些，是的，我们需要纯粹的数字和数学库。</p><p id="439f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">2.现在让我们创建一个类，该类将实现算法和第一个函数，该函数将按类分离我们的数据集。</p><pre class="nd ne nf ng gt np nq nr ns aw nt bi"><span id="6418" class="kz la it nq b gy nu nv l nw nx"># gaussClf will be the class that will have the Gaussian naive bayes classifier implimentation<br/>class gaussClf:<br/>    def separate_by_classes(self, X, y):<br/>        <em class="ny">''' This function separates our dataset in subdatasets by classes '''<br/>        </em>self.classes = np.unique(y)<br/>        classes_index = {}<br/>        subdatasets = {}<br/>        cls, counts = np.unique(y, return_counts=True)<br/>        self.class_freq = dict(zip(cls, counts))<br/>        print(self.class_freq)<br/>        for class_type in self.classes:<br/>            classes_index[class_type] = np.argwhere(y==class_type)<br/>            subdatasets[class_type] = X[classes_index[class_type], :]<br/>            self.class_freq[class_type] = self.class_freq[class_type]/sum(list(self.class_freq.values()))<br/>        return subdatasets</span></pre><p id="4e0c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">separate_by_classes 函数按类分离出数据集，分别为每个类计算每列的平均值和标准偏差。</p><p id="e3cd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">3.拟合函数。</p><pre class="nd ne nf ng gt np nq nr ns aw nt bi"><span id="dce0" class="kz la it nq b gy nu nv l nw nx">def fit(self, X, y):<br/>    <em class="ny">''' The fitting function '''<br/>    </em>separated_X = self.separate_by_classes(X, y)<br/>    self.means = {}<br/>    self.std = {}<br/>    for class_type in self.classes:<br/>        # Here we calculate the mean and the standart deviation from datasets<br/>        self.means[class_type] = np.mean(separated_X[class_type], axis=0)[0]<br/>        self.std[class_type] = np.std(separated_X[class_type], axis=0)[0]</span></pre><p id="6961" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">接下来是拟合函数，我们只计算每一类每一列的平均值和标准差。</p><p id="553a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">4.高斯分布函数。</p><pre class="nd ne nf ng gt np nq nr ns aw nt bi"><span id="8497" class="kz la it nq b gy nu nv l nw nx">def calculate_probability(self, x, mean, stdev):<br/>    <em class="ny">''' This function calculates the class probability using gaussian distribution '''<br/>    </em>exponent = math.exp(-((x - mean) ** 2 / (2 * stdev ** 2)))<br/>    return (1 / (math.sqrt(2 * math.pi) * stdev)) * exponent</span></pre><p id="9d6a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">calculate_probability 函数使用一个序列的平均值和标准差来计算一个函数在一个序列中出现的概率。</p><p id="20a3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">5.预测函数。</p><pre class="nd ne nf ng gt np nq nr ns aw nt bi"><span id="9823" class="kz la it nq b gy nu nv l nw nx">def predict_proba(self, X):<br/>    <em class="ny">''' This function predicts the probability for every class '''<br/>    </em>self.class_prob = {cls:math.log(self.class_freq[cls], math.e) for cls in self.classes}<br/>    for cls in self.classes:<br/>        for i in range(len(self.means)):<br/>            print(X[i])<br/>            self.class_prob[cls]+=math.log(self.calculate_probability(X[i], self.means[cls][i], self.std[cls][i]), math.e)<br/>    self.class_prob = {cls: math.e**self.class_prob[cls] for cls in self.class_prob}<br/>    return self.class_prob</span></pre><p id="26e7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个函数返回一个字典，其中包含样本属于某个类的概率。在经典的 sklearn 估计器中，predict_proba 函数获取样本列表并返回标签列表。为了使它更容易使用，我决定只在一个例子中实现它。</p><p id="61e4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">同样，在这个函数中，我不计算先验概率，以避免无用的计算，因为对于每个类估计，你需要除以上面得到的相同值。</p><p id="b3b6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">6.预测函数。</p><pre class="nd ne nf ng gt np nq nr ns aw nt bi"><span id="4a77" class="kz la it nq b gy nu nv l nw nx">def predict(self, X):<br/>    <em class="ny">''' This funtion predicts the class of a sample '''<br/>    </em>pred = []<br/>    for x in X:<br/>        pred_class = None<br/>        max_prob = 0<br/>        for cls, prob in self.predict_proba(x).items():<br/>            if prob&gt;max_prob:<br/>                max_prob = prob<br/>                pred_class = cls<br/>        pred.append(pred_class)<br/>    return pred</span></pre><p id="dc0b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这里，我决定使用经典的实现方法。列出来，列出来。</p><p id="4356" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可以在我的 github 库上看到代码。</p><h1 id="1413" class="ml la it bd lb mm mn mo le mp mq mr lh ms mt mu lk mv mw mx ln my mz na lq nb bi translated">对比 sklearn。</h1><p id="7690" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">现在让我们将我们的实现与 sklearn one 进行比较。在 sklearn 库中，Gaussian Naive Bayse 实现为 GaussianNB 类，要导入它，您应该编写以下代码:</p><pre class="nd ne nf ng gt np nq nr ns aw nt bi"><span id="2a33" class="kz la it nq b gy nu nv l nw nx">from sklearn.naive_bayes import GaussianNB</span></pre><p id="9a7f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们会让你来实施，你可以在<a class="ae nl" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html" rel="noopener ugc nofollow" target="_blank">那里</a>找到怎么做。那么在<a class="ae nl" href="https://archive.ics.uci.edu/ml/datasets/Iris" rel="noopener ugc nofollow" target="_blank">虹膜</a>数据集上有什么结果呢？</p><p id="8b3f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们的实现:0.00000000001</p><p id="041c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Sklearn 实现:1.0 精度。</p><p id="484c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">发生这种情况是因为 sklearn 模型使用了一个比我们使用的模型稍多的其他实现，你可以在 sklearn 网站上阅读更多内容。</p><h1 id="ed8f" class="ml la it bd lb mm mn mo le mp mq mr lh ms mt mu lk mv mw mx ln my mz na lq nb bi translated">结论。</h1><p id="1ebd" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">因此，在本文中，我向您展示了如何实现实际上最古老的机器学习算法的最简单形式——高斯朴素贝叶斯算法，并简要介绍了它的工作原理。我强烈建议您学习 sklearn 实现的工作原理，并尝试自己实现 BernoulliNB。</p><p id="5ad9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">谢谢你。</p></div></div>    
</body>
</html>