<html>
<head>
<title>Customer Case Study: Building an end-to-end Speech Recognition model in PyTorch with AssemblyAI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">客户案例研究:使用 AssemblyAI 在 PyTorch 中构建端到端语音识别模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/customer-case-study-building-an-end-to-end-speech-recognition-model-in-pytorch-with-assemblyai-473030e47c7c?source=collection_archive---------36-----------------------#2020-05-19">https://towardsdatascience.com/customer-case-study-building-an-end-to-end-speech-recognition-model-in-pytorch-with-assemblyai-473030e47c7c?source=collection_archive---------36-----------------------#2020-05-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="46ee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">本文由</em><a class="ae kp" href="https://www.assemblyai.com/" rel="noopener ugc nofollow" target="_blank"><em class="ko">assembly ai</em></a><em class="ko">的机器学习研究工程师 Michael Nguyen 和</em><a class="ae kp" href="http://www.comet.ml" rel="noopener ugc nofollow" target="_blank"><em class="ko">comet . ml</em></a><em class="ko">的 Niko Laskaris 撰写。AssemblyAI 使用 Comet 来记录、可视化和理解他们的模型开发管道。</em></p><p id="e383" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随着端到端模型的引入，深度学习改变了语音识别领域的游戏。这些模型接收音频，并直接输出转录。目前最流行的两种端到端模型是百度的 Deep Speech 和谷歌的 Listen accept Spell(LAS)。深度语音和 LAS 都是基于递归神经网络(RNN)的架构，具有不同的建模语音识别的方法。深度语音使用连接主义者时间分类(CTC)损失函数来预测语音转录本。LAS 使用序列对网络体系结构进行序列预测。</p><p id="fcbf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些模型通过利用深度学习系统从大型数据集学习的能力，简化了语音识别管道。有了足够的数据，从理论上讲，你应该能够建立一个超级健壮的语音识别模型，它可以考虑语音中的所有细微差别，而不必花费大量的时间和精力手工设计声学特征，或者处理更老式的 GMM-HMM 模型架构中的复杂管道。</p><p id="5c96" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">深度学习是一个快速发展的领域，深度语音和 LAS 风格的架构已经很快过时了。你可以在下面的最新进展部分了解该行业的发展方向。</p><h1 id="beb8" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">如何在 PyTorch 中构建自己的端到端语音识别模型</h1><p id="49c2" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">让我们看看如何在 PyTorch 中构建自己的端到端语音识别模型。我们将构建的模型受到 Deep Speech 2(百度对其著名模型的第二次修订)的启发，并对架构进行了一些个人改进。该模型的输出将是字符的概率矩阵，我们将使用该概率矩阵来解码音频中最有可能说出的字符。你可以在<a class="ae kp" href="https://colab.research.google.com/drive/1IPpwx4rX32rqHKpLz7dc8sOKspUa-YKO" rel="noopener ugc nofollow" target="_blank">谷歌联合实验室</a>找到完整的代码，也可以在 GPU 的支持下运行它。</p><h1 id="b1cf" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">准备数据管道</h1><p id="4f30" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">数据是语音识别最重要的方面之一。我们将原始音频波转换成 Mel 光谱图。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/354febe6e627b3d2527b3e76f1980583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zD-ICYnOm6oGOcAj.png"/></div></div></figure><p id="868c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以从这篇精彩的文章<a class="ae kp" href="https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html" rel="noopener ugc nofollow" target="_blank">这里</a>阅读更多关于这一转变的细节。在这篇文章中，你可以把 Mel 声谱图想象成声音的图像。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mf"><img src="../Images/b010e6d1999d73f03f58fbc3c79dec0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GQBJQnfeq7PZMHvC.jpeg"/></div></div></figure><p id="4d41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了处理音频数据，我们将使用一个名为<strong class="js iu"> torchaudio </strong>的非常有用的工具，这是 PyTorch 团队专门为音频数据构建的库。我们将在<a class="ae kp" href="http://www.openslr.org/12/" rel="noopener ugc nofollow" target="_blank"> LibriSpeech </a>的子集上进行训练，这是一个从有声读物中获得的阅读英语语音数据的语料库，包括 100 个小时的转录音频数据。您可以使用<strong class="js iu"> torchaudio </strong>轻松下载该数据集:</p><pre class="lu lv lw lx gt mg mh mi mj aw mk bi"><span id="b9c0" class="ml kr it mh b gy mm mn l mo mp">import torchaudio train_dataset = torchaudio.datasets.LIBRISPEECH("./", url="train-clean-100", download=True) <br/>test_dataset = torchaudio.datasets.LIBRISPEECH("./", url="test-clean", download=True)</span></pre><p id="4e54" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据集的每个样本都包含波形、音频的采样率、话语/标签以及关于样本的更多元数据。你可以从源代码<a class="ae kp" href="https://github.com/pytorch/audio/blob/master/torchaudio/datasets/librispeech.py#L40" rel="noopener ugc nofollow" target="_blank">这里</a>查看每个样本的样子。</p><h1 id="6e56" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">数据扩充— SpecAugment</h1><p id="5ab1" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">数据扩充是一种用于人为增加数据集多样性以增加数据集大小的技术。当数据不足或模型过拟合时，这种策略尤其有用。对于语音识别，您可以使用标准的增强技术，如改变音调、速度、注入噪声以及给音频数据添加混响。</p><p id="5ea4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们发现谱图增强(SpecAugment)是一种更简单、更有效的方法。SpecAugment 在论文<a class="ae kp" href="https://arxiv.org/abs/1904.08779" rel="noopener ugc nofollow" target="_blank">中首次介绍:SpecAugment:一种用于自动语音识别的简单数据扩充方法</a>，其中作者发现，简单地剪切连续时间和频率维度的随机块显著提高了模型的泛化能力！</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/359d67b0a6f24744701ff0391f42f7e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/0*xvkW5hm10ZktfE9q.png"/></div></figure><p id="a09c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 PyTorch 中，您可以使用<strong class="js iu"> torchaudio </strong>功能<strong class="js iu"> FrequencyMasking </strong>屏蔽频率维度，使用<strong class="js iu"> TimeMasking </strong>屏蔽时间维度。</p><pre class="lu lv lw lx gt mg mh mi mj aw mk bi"><span id="39b2" class="ml kr it mh b gy mm mn l mo mp">torchaudio.transforms.FrequencyMasking()<br/>torchaudio.transforms.TimeMasking()</span></pre><p id="5e87" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们有了数据，我们需要将音频转换成 Mel 频谱图，并将每个音频样本的字符标签映射成整数标签:</p><pre class="lu lv lw lx gt mg mh mi mj aw mk bi"><span id="8da4" class="ml kr it mh b gy mm mn l mo mp">class TextTransform:<br/>    """Maps characters to integers and vice versa"""<br/>    def __init__(self):<br/>        char_map_str = """<br/>        ' 0<br/>        &lt;SPACE&gt; 1<br/>        a 2<br/>        b 3<br/>        c 4<br/>        d 5<br/>        e 6<br/>        f 7<br/>        g 8<br/>        h 9<br/>        i 10<br/>        j 11<br/>        k 12<br/>        l 13<br/>        m 14<br/>        n 15<br/>        o 16<br/>        p 17<br/>        q 18<br/>        r 19<br/>        s 20<br/>        t 21<br/>        u 22<br/>        v 23<br/>        w 24<br/>        x 25<br/>        y 26<br/>        z 27<br/>        """<br/>        self.char_map = {}<br/>        self.index_map = {}<br/>        for line in char_map_str.strip().split('\n'):<br/>            ch, index = line.split()<br/>            self.char_map[ch] = int(index)<br/>            self.index_map[int(index)] = ch<br/>        self.index_map[1] = ' '</span><span id="c9de" class="ml kr it mh b gy mr mn l mo mp">def text_to_int(self, text):<br/>        """ Use a character map and convert text to an integer sequence """<br/>        int_sequence = []<br/>        for c in text:<br/>            if c == ' ':<br/>                ch = self.char_map['']<br/>            else:<br/>                ch = self.char_map[c]<br/>            int_sequence.append(ch)<br/>        return int_sequence</span><span id="f4ae" class="ml kr it mh b gy mr mn l mo mp">def int_to_text(self, labels):<br/>        """ Use a character map and convert integer labels to an text sequence """<br/>        string = []<br/>        for i in labels:<br/>            string.append(self.index_map[i])<br/>        return ''.join(string).replace('', ' ')</span><span id="2f90" class="ml kr it mh b gy mr mn l mo mp">train_audio_transforms = nn.Sequential(<br/>    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),<br/>    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),<br/>    torchaudio.transforms.TimeMasking(time_mask_param=35)<br/>)</span><span id="020d" class="ml kr it mh b gy mr mn l mo mp">valid_audio_transforms = torchaudio.transforms.MelSpectrogram()</span><span id="ea40" class="ml kr it mh b gy mr mn l mo mp">text_transform = TextTransform()</span><span id="88af" class="ml kr it mh b gy mr mn l mo mp">def data_processing(data, data_type="train"):<br/>    spectrograms = []<br/>    labels = []<br/>    input_lengths = []<br/>    label_lengths = []<br/>    for (waveform, _, utterance, _, _, _) in data:<br/>        if data_type == 'train':<br/>            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)<br/>        else:<br/>            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)<br/>        spectrograms.append(spec)<br/>        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))<br/>        labels.append(label)<br/>        input_lengths.append(spec.shape[0]//2)<br/>        label_lengths.append(len(label))</span><span id="9c0a" class="ml kr it mh b gy mr mn l mo mp">spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)<br/>    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)</span><span id="072a" class="ml kr it mh b gy mr mn l mo mp">return spectrograms, labels, input_lengths, label_lengths</span></pre><h1 id="6703" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">定义模型—深度演讲 2(但更好)</h1><p id="b3c4" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">我们的模型将类似于 Deep Speech 2 架构。该模型将具有两个主要的神经网络模块——N 层残差卷积神经网络(ResCNN)来学习相关的音频特征，以及一组双向递归神经网络(BiRNN)来利用学习到的 ResCNN 音频特征。该模型以一个完全连接的层结束，该层用于在每个时间步长对角色进行分类。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ms"><img src="../Images/872b8cdf2f7a4bd8eb90824140774ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1TokC95mn1dVllEx.png"/></div></div></figure><p id="5e34" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">卷积神经网络(CNN)擅长提取抽象特征，我们将把同样的特征提取能力应用于音频频谱图。我们选择使用残留的 CNN 层，而不是普通的 CNN 层。残差连接(又名跳过连接)首次在论文<a class="ae kp" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">图像识别的深度残差学习</a>中介绍，作者发现，如果你将这些连接添加到 CNN 中，你可以建立具有良好精度增益的真正深度网络。添加这些剩余连接也有助于模型更快地学习和更好地概括。论文<a class="ae kp" href="https://arxiv.org/abs/1712.09913" rel="noopener ugc nofollow" target="_blank">可视化神经网络的损失景观</a>表明，具有剩余连接的网络具有“更平坦”的损失表面，使模型更容易浏览损失景观，并找到更低和更可概括的最小值。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mf"><img src="../Images/5973a05a67ba1d263cf3d971fbe1ad22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kR_1BxeE8lohr6El.png"/></div></div></figure><p id="5741" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">递归神经网络(RNN)天生擅长序列建模问题。RNN 一步一步地处理音频特征，对每一帧进行预测，同时使用前一帧的上下文。我们使用 BiRNN 是因为我们不仅想要每一步之前的帧的上下文，还想要每一步之后的帧的上下文。这可以帮助模型做出更好的预测，因为在做出预测之前，音频中的每一帧都将具有更多信息。我们使用 RNN 的门控循环单元(GRU 的)变体，因为它比 LSTM 的需要更少的计算资源，并且在某些情况下工作得一样好。</p><p id="1684" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该模型输出字符的概率矩阵，我们将使用该矩阵输入到我们的解码器中，以提取模型认为最有可能说出的字符。</p><pre class="lu lv lw lx gt mg mh mi mj aw mk bi"><span id="6a64" class="ml kr it mh b gy mm mn l mo mp">class CNNLayerNorm(nn.Module):<br/>    """Layer normalization built for cnns input"""<br/>    def __init__(self, n_feats):<br/>        super(CNNLayerNorm, self).__init__()<br/>        self.layer_norm = nn.LayerNorm(n_feats)</span><span id="9ee0" class="ml kr it mh b gy mr mn l mo mp">def forward(self, x):<br/>        # x (batch, channel, feature, time)<br/>        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)<br/>        x = self.layer_norm(x)<br/>        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)</span><span id="b9af" class="ml kr it mh b gy mr mn l mo mp">class ResidualCNN(nn.Module):<br/>    """Residual CNN inspired by <a class="ae kp" href="https://arxiv.org/pdf/1603.05027.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1603.05027.pdf</a><br/>        except with layer norm instead of batch norm<br/>    """<br/>    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):<br/>        super(ResidualCNN, self).__init__()</span><span id="2d2c" class="ml kr it mh b gy mr mn l mo mp">self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)<br/>        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)<br/>        self.dropout1 = nn.Dropout(dropout)<br/>        self.dropout2 = nn.Dropout(dropout)<br/>        self.layer_norm1 = CNNLayerNorm(n_feats)<br/>        self.layer_norm2 = CNNLayerNorm(n_feats)</span><span id="0c82" class="ml kr it mh b gy mr mn l mo mp">def forward(self, x):<br/>        residual = x  # (batch, channel, feature, time)<br/>        x = self.layer_norm1(x)<br/>        x = F.gelu(x)<br/>        x = self.dropout1(x)<br/>        x = self.cnn1(x)<br/>        x = self.layer_norm2(x)<br/>        x = F.gelu(x)<br/>        x = self.dropout2(x)<br/>        x = self.cnn2(x)<br/>        x += residual<br/>        return x # (batch, channel, feature, time)</span><span id="2295" class="ml kr it mh b gy mr mn l mo mp">class BidirectionalGRU(nn.Module):</span><span id="3920" class="ml kr it mh b gy mr mn l mo mp">def __init__(self, rnn_dim, hidden_size, dropout, batch_first):<br/>        super(BidirectionalGRU, self).__init__()</span><span id="aa95" class="ml kr it mh b gy mr mn l mo mp">self.BiGRU = nn.GRU(<br/>            input_size=rnn_dim, hidden_size=hidden_size,<br/>            num_layers=1, batch_first=batch_first, bidirectional=True)<br/>        self.layer_norm = nn.LayerNorm(rnn_dim)<br/>        self.dropout = nn.Dropout(dropout)</span><span id="7b0f" class="ml kr it mh b gy mr mn l mo mp">def forward(self, x):<br/>        x = self.layer_norm(x)<br/>        x = F.gelu(x)<br/>        x, _ = self.BiGRU(x)<br/>        x = self.dropout(x)<br/>        return x</span><span id="77b3" class="ml kr it mh b gy mr mn l mo mp">class SpeechRecognitionModel(nn.Module):<br/>    """Speech Recognition Model Inspired by DeepSpeech 2"""</span><span id="f12c" class="ml kr it mh b gy mr mn l mo mp">def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):<br/>        super(SpeechRecognitionModel, self).__init__()<br/>        n_feats = n_feats//2<br/>        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features</span><span id="1e50" class="ml kr it mh b gy mr mn l mo mp"># n residual cnn layers with filter size of 32<br/>        self.rescnn_layers = nn.Sequential(*[<br/>            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) <br/>            for _ in range(n_cnn_layers)<br/>        ])<br/>        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)<br/>        self.birnn_layers = nn.Sequential(*[<br/>            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,<br/>                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)<br/>            for i in range(n_rnn_layers)<br/>        ])<br/>        self.classifier = nn.Sequential(<br/>            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2<br/>            nn.GELU(),<br/>            nn.Dropout(dropout),<br/>            nn.Linear(rnn_dim, n_class)<br/>        )</span><span id="58bc" class="ml kr it mh b gy mr mn l mo mp">def forward(self, x):<br/>        x = self.cnn(x)<br/>        x = self.rescnn_layers(x)<br/>        sizes = x.size()<br/>        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)<br/>        x = x.transpose(1, 2) # (batch, time, feature)<br/>        x = self.fully_connected(x)<br/>        x = self.birnn_layers(x)<br/>        x = self.classifier(x)<br/>        return x</span></pre><h1 id="37d0" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">选择合适的优化器和调度器——AdamW 具有超强的融合能力</h1><p id="bab5" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">优化器和学习率计划在让我们的模型收敛到最佳点方面起着非常重要的作用。选择正确的优化器和调度器还可以节省您的计算时间，并帮助您的模型更好地推广到现实世界的用例。对于我们的模型，我们将使用<strong class="js iu"> AdamW </strong>和<strong class="js iu">单周期学习率调度器</strong>。<strong class="js iu"> Adam </strong>是一个广泛使用的优化器，它可以帮助你的模型更快地收敛，从而节省计算时间，但它因不能像<strong class="js iu">随机梯度下降</strong>又名<strong class="js iu"> SGD </strong>一样泛化而臭名昭著。</p><p id="1374" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">AdamW 首次在<a class="ae kp" href="https://arxiv.org/abs/1711.05101" rel="noopener ugc nofollow" target="_blank">解耦权重衰减正则化</a>中引入，被认为是对<strong class="js iu"> Adam </strong>的“修正”。论文指出，最初的<strong class="js iu"> Adam </strong>算法有一个错误的权重衰减实现，AdamW 试图修复这个错误。这个修正有助于解决亚当的泛化问题。</p><p id="27cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">单周期学习率调度器</strong>首次在论文<a class="ae kp" href="https://arxiv.org/abs/1708.07120" rel="noopener ugc nofollow" target="_blank">中介绍:超收敛:使用大学习率非常快速地训练神经网络</a>。这篇论文表明，使用一个简单的技巧，你可以训练神经网络的速度提高一个数量级，同时保持它们的泛化能力。你从一个低的学习速率开始，升温到一个大的最大学习速率，然后线性衰减到你最初开始的同一点。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mt"><img src="../Images/863d2bcfd6fceb6b4528567809136dc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*R61grvRimy9VHEIg.jpeg"/></div></div></figure><p id="de4f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为最大学习率比最小学习率高很多，所以您也获得了一些正则化的好处，这有助于您的模型在数据集较少的情况下更好地进行概化。</p><p id="d124" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于 PyTorch，这两个方法已经是包的一部分了。</p><pre class="lu lv lw lx gt mg mh mi mj aw mk bi"><span id="39c0" class="ml kr it mh b gy mm mn l mo mp">optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])<br/>scheduler = optim.lr_scheduler.OneCycleLR(optimizer,<br/>	max_lr=hparams['learning_rate'],<br/>	steps_per_epoch=int(len(train_loader)),<br/>	epochs=hparams['epochs'],<br/>	anneal_strategy='linear')</span></pre><h1 id="bf70" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">CTC 丢失功能—将音频与抄本对齐</h1><p id="a85e" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">我们的模型将被训练来预测我们馈入模型的声谱图中每一帧(即时间步长)字母表中所有字符的概率分布。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mu"><img src="../Images/05f120a144b397748b78e24a7c9ab6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ln_7M-tYyGuLTcrB.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">图片取自 distill.pub</p></figure><p id="2129" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">传统的语音识别模型会要求你在训练之前将抄本文本与音频对齐，并且该模型会被训练来预测特定帧处的特定标签。</p><p id="e37d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">CTC 丢失功能的创新之处在于它允许我们跳过这一步。我们的模型将在训练过程中学习对齐脚本本身。这其中的关键是由 CTC 引入的“空白”标签，它使模型能够说某个音频帧没有产生一个字符。你可以从<a class="ae kp" href="https://distill.pub/2017/ctc/" rel="noopener ugc nofollow" target="_blank">这篇精彩的帖子</a>中看到关于 CTC 及其工作原理的更详细的解释。</p><p id="1146" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">PyTorch 还内置了 CTC 丢失功能。</p><pre class="lu lv lw lx gt mg mh mi mj aw mk bi"><span id="b303" class="ml kr it mh b gy mm mn l mo mp">criterion = nn.CTCLoss(blank=28).to(device)</span></pre><h1 id="c33d" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">评估你的语音模型</h1><p id="483b" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">在评估您的语音识别模型时，行业标准使用单词错误率(WER)作为度量标准。单词错误率确实如其所言——它获取模型输出的转录和真实转录，并测量它们之间的错误。你可以在这里看到这是如何实现的<a class="ae kp" href="https://colab.research.google.com/drive/1IPpwx4rX32rqHKpLz7dc8sOKspUa-YKO" rel="noopener ugc nofollow" target="_blank">。另一个有用的指标叫做字符错误率(CER)。CER 测量模型输出和真实标签之间的字符误差。这些指标有助于衡量模型的性能。</a></p><p id="f84f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于本教程，我们将使用“贪婪”解码方法将模型的输出处理成字符，这些字符可以组合起来创建副本。“贪婪”解码器接收模型输出，该模型输出是字符的 softmax 概率矩阵，并且对于每个时间步长(谱图帧)，它选择具有最高概率的标签。如果标签是空白标签，我们会将其从最终抄本中删除。</p><pre class="lu lv lw lx gt mg mh mi mj aw mk bi"><span id="464a" class="ml kr it mh b gy mm mn l mo mp">def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):<br/>    arg_maxes = torch.argmax(output, dim=2)<br/>    decodes = []<br/>    targets = []<br/>    for i, args in enumerate(arg_maxes):<br/>        decode = []<br/>        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))<br/>        for j, index in enumerate(args):<br/>            if index != blank_label:<br/>                if collapse_repeated and j != 0 and index == args[j -1]:<br/>                    continue<br/>                decode.append(index.item())<br/>        decodes.append(text_transform.int_to_text(decode))<br/>    return decodes, targets</span></pre><h1 id="6133" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">使用 Comet.ml 训练和监控您的实验</h1><p id="3e05" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated"><a class="ae kp" href="https://www.comet.ml/site/" rel="noopener ugc nofollow" target="_blank"> Comet.ml </a>提供了一个平台，允许深度学习研究人员跟踪、比较、解释和优化他们的实验和模型。Comet.ml 提高了我们在 AssemblyAI 的工作效率，我们强烈推荐团队使用这个平台进行任何类型的数据科学实验。Comet.ml 设置起来超级简单。只需几行代码就能完成。</p><pre class="lu lv lw lx gt mg mh mi mj aw mk bi"><span id="3286" class="ml kr it mh b gy mm mn l mo mp"># initialize experiment object<br/>experiment = Experiment(api_key=comet_api_key, project_name=project_name)<br/>experiment.set_name(exp_name)</span><span id="8335" class="ml kr it mh b gy mr mn l mo mp"># track metrics<br/>experiment.log_metric('loss', loss.item())</span></pre><p id="ba3b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://www.comet.ml/site/" rel="noopener ugc nofollow" target="_blank"> Comet.ml </a>为您提供了一个非常高效的仪表盘，您可以在其中查看和跟踪模型的进度。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mf"><img src="../Images/4006a2b4c74f4f3a61325cbd5ee2978c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iwawLH_3rwpSmM58.png"/></div></div></figure><p id="7786" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以使用 Comet 来跟踪度量、代码、超级参数、您的模型的图表，以及其他许多东西！Comet 提供的一个非常方便的特性是能够将您的实验与许多其他实验进行比较。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mf"><img src="../Images/ce6d2e83b3d951867c5990eaca862a1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xqIYrQAW66p9xk-k.png"/></div></div></figure><p id="17a6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Comet 有丰富的特性集，我们不会在这里一一介绍，但是我们强烈建议使用它来提高生产率和健全性。</p><p id="d5e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我们培训脚本的其余部分。</p><pre class="lu lv lw lx gt mg mh mi mj aw mk bi"><span id="b2fb" class="ml kr it mh b gy mm mn l mo mp">class IterMeter(object):<br/>    """keeps track of total iterations"""<br/>    def __init__(self):<br/>        self.val = 0</span><span id="da8b" class="ml kr it mh b gy mr mn l mo mp">def step(self):<br/>        self.val += 1</span><span id="fd65" class="ml kr it mh b gy mr mn l mo mp">def get(self):<br/>        return self.val</span><span id="49db" class="ml kr it mh b gy mr mn l mo mp">def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment):<br/>    model.train()<br/>    data_len = len(train_loader.dataset)<br/>    with experiment.train():<br/>        for batch_idx, _data in enumerate(train_loader):<br/>            spectrograms, labels, input_lengths, label_lengths = _data <br/>            spectrograms, labels = spectrograms.to(device), labels.to(device)</span><span id="b704" class="ml kr it mh b gy mr mn l mo mp">optimizer.zero_grad()</span><span id="af33" class="ml kr it mh b gy mr mn l mo mp">output = model(spectrograms)  # (batch, time, n_class)<br/>            output = F.log_softmax(output, dim=2)<br/>            output = output.transpose(0, 1) # (time, batch, n_class)</span><span id="c939" class="ml kr it mh b gy mr mn l mo mp">loss = criterion(output, labels, input_lengths, label_lengths)<br/>            loss.backward()</span><span id="e834" class="ml kr it mh b gy mr mn l mo mp">experiment.log_metric('loss', loss.item(), step=iter_meter.get())<br/>            experiment.log_metric('learning_rate', scheduler.get_lr(), step=iter_meter.get())</span><span id="91ab" class="ml kr it mh b gy mr mn l mo mp">optimizer.step()<br/>            scheduler.step()<br/>            iter_meter.step()<br/>            if batch_idx % 100 == 0 or batch_idx == data_len:<br/>                print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(<br/>                    epoch, batch_idx * len(spectrograms), data_len,<br/>                    100. * batch_idx / len(train_loader), loss.item()))</span><span id="c774" class="ml kr it mh b gy mr mn l mo mp">def test(model, device, test_loader, criterion, epoch, iter_meter, experiment):<br/>    print('\nevaluating…')<br/>    model.eval()<br/>    test_loss = 0<br/>    test_cer, test_wer = [], []<br/>    with experiment.test():<br/>        with torch.no_grad():<br/>            for I, _data in enumerate(test_loader):<br/>                spectrograms, labels, input_lengths, label_lengths = _data <br/>                spectrograms, labels = spectrograms.to(device), labels.to(device)</span><span id="7173" class="ml kr it mh b gy mr mn l mo mp">output = model(spectrograms)  # (batch, time, n_class)<br/>                output = F.log_softmax(output, dim=2)<br/>                output = output.transpose(0, 1) # (time, batch, n_class)</span><span id="22ae" class="ml kr it mh b gy mr mn l mo mp">loss = criterion(output, labels, input_lengths, label_lengths)<br/>                test_loss += loss.item() / len(test_loader)</span><span id="d7a0" class="ml kr it mh b gy mr mn l mo mp">decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)<br/>                for j in range(len(decoded_preds)):<br/>                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))<br/>                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))</span><span id="0758" class="ml kr it mh b gy mr mn l mo mp">avg_cer = sum(test_cer)/len(test_cer)<br/>    avg_wer = sum(test_wer)/len(test_wer)<br/>    experiment.log_metric('test_loss', test_loss, step=iter_meter.get())<br/>    experiment.log_metric('cer', avg_cer, step=iter_meter.get())<br/>    experiment.log_metric('wer', avg_wer, step=iter_meter.get())</span><span id="700a" class="ml kr it mh b gy mr mn l mo mp">print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\n'.format(test_loss, avg_cer, avg_wer))</span><span id="172f" class="ml kr it mh b gy mr mn l mo mp">def main(learning_rate=5e-4, batch_size=20, epochs=10,<br/>        train_url="train-clean-100", test_url="test-clean",<br/>        experiment=Experiment(api_key='dummy_key', disabled=True)):</span><span id="625b" class="ml kr it mh b gy mr mn l mo mp">hparams = {<br/>        "n_cnn_layers": 3,<br/>        "n_rnn_layers": 5,<br/>        "rnn_dim": 512,<br/>        "n_class": 29,<br/>        "n_feats": 128,<br/>        "stride": 2,<br/>        "dropout": 0.1,<br/>        "learning_rate": learning_rate,<br/>        "batch_size": batch_size,<br/>        "epochs": epochs<br/>    }</span><span id="141e" class="ml kr it mh b gy mr mn l mo mp">experiment.log_parameters(hparams)</span><span id="ccb0" class="ml kr it mh b gy mr mn l mo mp">use_cuda = torch.cuda.is_available()<br/>    torch.manual_seed(7)<br/>    device = torch.device("cuda" if use_cuda else "cpu")</span><span id="0cf1" class="ml kr it mh b gy mr mn l mo mp">if not os.path.isdir("./data"):<br/>        os.makedirs("./data")</span><span id="aead" class="ml kr it mh b gy mr mn l mo mp">train_dataset = torchaudio.datasets.LIBRISPEECH("./data", url=train_url, download=True)<br/>    test_dataset = torchaudio.datasets.LIBRISPEECH("./data", url=test_url, download=True)</span><span id="e4a6" class="ml kr it mh b gy mr mn l mo mp">kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}<br/>    train_loader = data.DataLoader(dataset=train_dataset,<br/>                                batch_size=hparams['batch_size'],<br/>                                shuffle=True,<br/>                                collate_fn=lambda x: data_processing(x, 'train'),<br/>                                **kwargs)<br/>    test_loader = data.DataLoader(dataset=test_dataset,<br/>                                batch_size=hparams['batch_size'],<br/>                                shuffle=False,<br/>                                collate_fn=lambda x: data_processing(x, 'valid'),<br/>                                **kwargs)</span><span id="d796" class="ml kr it mh b gy mr mn l mo mp">model = SpeechRecognitionModel(<br/>        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],<br/>        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']<br/>        ).to(device)</span><span id="5de3" class="ml kr it mh b gy mr mn l mo mp">print(model)<br/>    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))</span><span id="aa7e" class="ml kr it mh b gy mr mn l mo mp">optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])<br/>    criterion = nn.CTCLoss(blank=28).to(device)<br/>    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], <br/>                                            steps_per_epoch=int(len(train_loader)),<br/>                                            epochs=hparams['epochs'],<br/>                                            anneal_strategy='linear')</span><span id="993f" class="ml kr it mh b gy mr mn l mo mp">iter_meter = IterMeter()<br/>    for epoch in range(1, epochs + 1):<br/>        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)<br/>        test(model, device, test_loader, criterion, epoch, iter_meter, experiment)</span></pre><p id="7261" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">训练</strong>功能根据完整的数据时段训练模型。<strong class="js iu">测试</strong>功能在每个时期后根据测试数据评估模型。获取模型的<strong class="js iu"> test_loss </strong>以及<strong class="js iu"> cer </strong>和<strong class="js iu"> wer </strong>。您现在可以在<a class="ae kp" href="https://colab.research.google.com/drive/1IPpwx4rX32rqHKpLz7dc8sOKspUa-YKO" rel="noopener ugc nofollow" target="_blank"> Google 联合实验室</a>的 GPU 支持下开始运行训练脚本。</p><h1 id="40ba" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">如何提高准确率</h1><p id="dc8c" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">语音识别需要大量的数据和计算资源。展示的示例是在 LibriSpeech (100 小时的音频)的子集和单个 GPU 上训练的。为了获得最先进的结果，你需要对分布在许多机器上的数十个 GPU 上的数千小时的数据进行分布式训练。</p><p id="5916" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一种大幅提高精度的方法是使用语言模型和 CTC 波束搜索算法解码 CTC 概率矩阵。CTC 类型的模型非常依赖于这个解码过程来获得好的结果。幸运的是，有一个方便的<a class="ae kp" href="https://github.com/parlance/ctcdecode" rel="noopener ugc nofollow" target="_blank">开源库</a>可以让你这么做。</p><p id="349e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本教程更容易理解，因此与 BERT(3.4 亿个参数)相比，它是一个相对较小的模型(2300 万个参数)。看起来你的网络越大，它的表现就越好，尽管回报是递减的。然而，正如 OpenAI 的研究<a class="ae kp" href="https://openai.com/assets/deep-double-descent/" rel="noopener ugc nofollow" target="_blank"> Deep Double Descent </a>所证明的那样，更大的模型等同于更好的性能并不总是如此。</p><p id="0b33" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个模型有 3 个剩余的 CNN 层和 5 个双向 GRU 层，这应该允许您在一个至少有 11GB 内存的 GPU 上训练一个合理的批量大小。您可以调整 main 函数中的一些 hyper 参数，以根据您的用例和计算可用性来减少或增加模型大小。</p><h1 id="6005" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">深度学习语音识别的最新进展</h1><p id="41b1" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">深度学习是一个快速发展的领域。似乎你不能一个星期没有一些新的技术得到最先进的结果。在语音识别的世界里，这里有一些值得探索的东西。</p><h1 id="41b2" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">变形金刚(电影名)</h1><p id="9e84" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">变形金刚席卷了自然语言处理世界！在论文<a class="ae kp" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">中首先介绍的是</a>，变形金刚已经被采用和修改来击败几乎所有现存的 NLP 任务，淘汰 RNN 的类型架构。转换器看到序列数据的完整上下文的能力也可以转换成语音。</p><h1 id="bc63" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">无监督预训练</h1><p id="ead2" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">如果你密切关注深度学习，你可能听说过伯特、GPT 和 GPT2。这些 Transformer 模型首先与带有未标记文本数据的语言建模任务相关，然后在各种 NLP 任务上进行微调，并获得最先进的结果！在预训练期间，模型学习语言统计的一些基本知识，并利用这种能力在其他任务中表现出色。我们相信这项技术在语音数据上也有很大的前景。</p><h1 id="b99c" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">单词片段模型</h1><p id="696c" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">我们上面定义的模型输出字符。这样做的一些好处是，在对语音进行推理时，该模型不必担心词汇以外的单词。所以对于单词<strong class="js iu"> c h a t </strong>来说，每个字符都有自己的标签。使用字符的缺点是效率低，而且模型容易出错，因为您一次只能预测一个字符。已经探索了使用整个单词作为标签，并取得了一定程度的成功。使用这种方法，整个单词<strong class="js iu">聊天</strong>将成为标签。但是使用全词，您将不得不保存所有可能词汇的索引来进行预测，这是内存低效的，在预测期间可能会用完词汇。最佳点是使用单词片段或子单词单元作为标签。您可以将单词分割成子单词单元，并使用这些子单词单元作为标签，即<strong class="js iu"> ch at </strong>，而不是单个标签的字符。这解决了词汇表之外的问题，并且更有效，因为它需要比使用字符更少的解码步骤，并且不需要所有可能单词的索引。单词片段已经成功地用于许多 NLP 模型，如 BERT，并且也可以自然地用于语音识别问题。</p></div></div>    
</body>
</html>