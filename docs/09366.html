<html>
<head>
<title>Regularization — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正规化—第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regularization-part-2-5b729698d026?source=collection_archive---------42-----------------------#2020-07-04">https://towardsdatascience.com/regularization-part-2-5b729698d026?source=collection_archive---------42-----------------------#2020-07-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="49d5" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/fau-lecture-notes" rel="noopener" target="_blank"> FAU 讲座笔记</a>关于深度学习</h2><div class=""/><div class=""><h2 id="f021" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">古典技术</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/0bd235b2601886d3e741c9a27626043c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lKyyTGDxqAQ_5lY-.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">FAU 大学的深度学习。下图<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a></p></figure><p id="81d6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">这些是 FAU 的 YouTube 讲座</strong> <a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">深度学习</strong> </a> <strong class="lk jd">的讲义。这是与幻灯片匹配的讲座视频&amp;的完整抄本。我们希望，你喜欢这个视频一样多。当然，这份抄本是用深度学习技术在很大程度上自动创建的，只进行了少量的手动修改。如果你发现了错误，请告诉我们！</strong></p><h1 id="9d69" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">航行</h1><p id="68f9" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/regularization-part-1-db408819b20f"> <strong class="lk jd">上一讲</strong> </a> <strong class="lk jd"> / </strong> <a class="ae lh" href="https://youtu.be/1RqnSkp9YS0" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">观看本视频</strong> </a> <strong class="lk jd"> / </strong> <a class="ae lh" rel="noopener" target="_blank" href="/all-you-want-to-know-about-deep-learning-8d68dcffc258"> <strong class="lk jd">顶级</strong> </a> <strong class="lk jd"> / </strong> <a class="ae lh" rel="noopener" target="_blank" href="/regularization-part-4-2ee8e7aa60ec"> <strong class="lk jd">下一讲</strong> </a></p><p id="3ba3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">欢迎回到深度学习！我们想继续分析正则化方法，今天我想谈谈经典技术。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/8fe3ba20e7d59b0b2910b5a3c79b879d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h4ffIAi0GRWI1wQV.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">您将无法在训练数据上观察到过度拟合。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="ac65" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是一个典型的训练集迭代损失曲线的例子。我在右边展示的是测试集上的损耗曲线。你可以看到，虽然培训损失下降了，但测试损失却上升了。因此，在某种程度上，训练数据集是过度拟合的，它不再产生代表数据的模型。顺便说一句，永远记住，测试集决不能用于训练。如果你在你的测试集上接受训练，那么你会得到非常好的结果，但是很有可能完全高估了性能。典型的情况是，有人跑进我的办公室说:“是的！我有 99%的识别率！”。当模式识别或机器学习领域的人读到“99%的识别率”时，他做的第一件事是问:“你用你的测试数据训练过吗？”这是你首先要确保的事情。当你犯了一些愚蠢的错误，有一些数据集指针没有指向正确的数据集，你的识别率突然上升。所以，如果你有非常好的结果，一定要小心。始终仔细检查它们是否真的合适，它们是否真的通用。因此，如果你想产生像我在这里展示的曲线，你可能想使用一个验证集，你从训练数据集。你从来没有在训练中使用过这个集合，但是你可以用它来估计你的模型过度拟合。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a244409af58a36677dd2e7063a1db0e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6GE5Ue6NzWYNdByD.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">当验证损失再次上升时，验证集有助于发现过度拟合。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>下的图片。</p></figure><p id="ccc1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所以，如果你这样做了，我们就可以使用第一个技巧了。您使用验证集。我们观察在什么点上我们在验证集中有最小的误差。如果我们在这一点上，我们可以使用它作为一个停止标准，并使用我们的测试评估模型。因此，使用具有最小验证结果的参数是一种常见的技术。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/d046e83425d7dfabe98fa52513fb443d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4No1Bxog697L_Bx7.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">数据扩充实际上可以增加训练集的大小。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0CC 下的图片。</p></figure><p id="2185" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一个非常有用的技术是数据扩充。所以，这里的想法是人为地扩大数据集。标签上有一些变换，这些变换对于类应该是不变的。假设你有一只猫的图像，你把它旋转 90 度，它仍然显示一只猫。显然，这些增强技术必须小心谨慎。因此，在右边的例子中，您可以看到旋转 180 度可能不是增加数字的好方法，因为它可能会转换标签。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a098e83d00097c69207ef47c57c688fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yjQX2rMbpO6ylxGu.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">数据扩充的常见转换。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="58d5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里有非常常见的变换:随机空间变换，如仿射或弹性变换。然后，还有像素变换，如改变分辨率、改变噪声或改变像素分布，如颜色亮度等。这些是图像处理中典型的增强技术。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/7bdbd16a5300158f1b4cc54575426320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pdzESEEp-onjf50B.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">利用贝叶斯定理，我们可以得到所谓的最大后验估计。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>下的图片。</p></figure><p id="b123" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">还有什么？我们可以正则化损失函数。这里，我们可以看到，这基本上导致了最大后验概率(MAP)估计。我们可以用贝叶斯方法来做到这一点，我们希望考虑不确定的权重。它们遵循先验分布 p( <strong class="lk jd"> w </strong>)。如果你有一些带有一些关联标签<strong class="lk jd"> Y </strong>的数据集<strong class="lk jd"> X </strong>，我们可以看到联合概率 p( <strong class="lk jd"> w </strong>，<strong class="lk jd"> Y </strong>，<strong class="lk jd"> X </strong>)是概率 p( <strong class="lk jd"> w </strong> | <strong class="lk jd"> Y，X </strong>)乘以概率 p( <strong class="lk jd"> Y，X </strong>)。我们可以将其重新表述为概率 P( <strong class="lk jd"> Y </strong> | <strong class="lk jd"> X </strong>，<strong class="lk jd"> w </strong>)乘以概率 p( <strong class="lk jd"> X </strong>，<strong class="lk jd"> w </strong>)。从这些等式可以推导出条件概率 p( <strong class="lk jd"> w </strong> | <strong class="lk jd"> Y </strong>，<strong class="lk jd"> X </strong>)可以表示为概率 p( <strong class="lk jd"> Y </strong> | <strong class="lk jd"> X，w </strong>)乘以概率 P( <strong class="lk jd"> X，w </strong>)除以概率 p( <strong class="lk jd"> Y，X </strong>)的贝叶斯定理。因此，我们可以进一步重新排列，这里你可以看到概率 p( <strong class="lk jd"> X </strong>)概率 p( <strong class="lk jd"> Y </strong> | <strong class="lk jd"> X </strong>)弹出。通过移除与<strong class="lk jd"> w </strong>无关的项，这产生了 MAP 估计。所以，我们实际上可以通过最大化条件概率 p( <strong class="lk jd"> Y </strong> | <strong class="lk jd"> X </strong>，<strong class="lk jd"> w </strong>)乘以概率 p( <strong class="lk jd"> w </strong>)来寻求最大化联合概率。因此，通常情况下，我们将这个问题作为最大似然估计量来解决，即左边的问题乘以右边部分<strong class="lk jd"> w </strong>的先验。我们可以说这是一个最大似然估计量，但我们用一些额外的先验信息来扩充它。这里，先验信息是我们对<strong class="lk jd"> w </strong>的分布有一些了解，例如<strong class="lk jd"> w </strong>可能是稀疏的。我们也可以使用其他的知识来源，在那里我们知道一些关于<strong class="lk jd"> w </strong>的事情。在图像处理中，经常使用的是例如自然图像相对于梯度是稀疏的，因此这种先验可以使用各种稀疏度。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/306c8e91c963c24f863da5ca95224cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vBPJFDtJGsw_nhhU.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">MAP 估计在损失函数中产生正则化项。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="543e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在有趣的部分是，这个 MAP 估计器可以被重构，如果你参加过模式识别，你就知道我在说什么。我们已经看到，最大似然估计的最大化导致负对数似然的最小化。我们讨论的典型损失函数具有这种形式。现在，如果你从地图估计开始，你基本上以一个非常相似的估计结束，但是损失函数的形状稍微改变了。所以，我们得到了一个新的损失函数。这就像 L2 损失或交叉熵损失加上一些λ和一些对权重的约束<strong class="lk jd"> w </strong>。所以在这里，我们实施了一个最小 l2 规范。现在，对于正λ，我们可以将其识别为最小化损失函数的拉格朗日函数，其受限于小于α的约束 L2 范数<strong class="lk jd"> w </strong>，其中α依赖于某些未知数据。所以这是完全相同的公式。我们现在可以把这一点引入到增加损失的反向传播中去。这通常是如何实现的？你跟踪梯度的损失。这是损失的右边部分，我们已经用学习率η计算过了，另外，你把这种收缩应用到<strong class="lk jd"> w </strong>上。因此，这里的收缩步骤可以用于实现额外的 L2 正则化。好消息是现在我们可以像以前一样简单地计算反向传播。然后，另外，我们在权重更新中使用收缩。因此，我们也可以获得非常简单的权重更新，它们允许我们涉及那些正则化。如果我们选择不同的正则项，收缩函数会发生变化。如果我们现在为λ优化训练损失，我们通常会得到λ = 0，因为每次我们引入正则化，我们都在做一些关于训练损失不是最优的事情。当然，我们引入它是因为我们想减少过度拟合。因此，这是我们无法在训练数据中直接观察到的东西，但我们希望在一个看不见的测试集上获得更好的属性。这甚至会增加我们训练数据的损失值。所以要小心。同样，我们增加偏差以减少方差。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/8fde19e0e4c3c1919b4966fa42fed488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qCeD3_pyGPgPhr2l.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">L2 规范强制实施具有较低权重量值的解决方案。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="7c9c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，我们有一个可视化的 L2 正则化的效果。不规则损耗当然会导致椭圆的中心变成红色。但是现在你做了额外的调整，使你的<strong class="lk jd"> w </strong>变小。这意味着你离原点越远，你的 L2 损耗就越高。因此，l2 损失会使您偏离关于训练数据集的数据最佳损失。希望它描述了一些我们在训练数据集中没有看到的先验知识。因此，它将产生一个更适合未知测试数据集的模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/0f03cadb3f9a98fadfc0c0d0cca3ae4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tn9V-YN3J8Ly1gso.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用 L1 项会导致不同形式的重量收缩。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0CC 下的图片。</p></figure><p id="c700" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们也可以使用其他规范，例如，L1 规范。因此，在这里，我们再次以拉格朗日公式结束，其中我们有服从 L1 范数的原始损失函数，该损失函数小于某个值α，且α未知，取决于数据。这里，我们简单地得到一个不同的收缩操作，它现在涉及到符号函数的使用。所以这又是次梯度的一个暗示。这里，为了使这种优化可行，必须采用不同的收缩方式。同样，我们对损失函数使用了与之前完全相同的梯度。所以只有收缩被取代。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c13e410cd6bd375c1d87453785500f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4UWTaBvD_KlUvnUO.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 在<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的二维图像中 L1 范数为菱形。</a></p></figure><p id="76a3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们也可以在我们的小图中看到这一点。L1 标准的形式当然是不同的。用 L2，我们有这个圆，用 L1 范数，我们得到一个菱形。现在，你可以看到选择的最小值可能位于坐标轴上。因此，如果你试图找到这个 L1 范数和非正则化损耗的最小位置，你会看到与这个 L1 损耗相交的最小非正则化损耗点基本上在这个图中的 y 轴上。这是一个非常稀疏的解决方案，意味着在我们的权重向量中只有 y 的条目，在这种情况下，x 的条目非常接近 0 或等于 0。因此，如果您希望权重稀疏，或者如果您希望创建连接较少的网络，您可能希望对权重引入额外的 L1 正则化。这样会造成权重稀疏。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/ad3f122c95eb2ef82b1136fda628f2d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O_1GUrAxCgG5jLxs.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0CC 下 w. Image 中的一个最大范数极限高值。</p></figure><p id="34ff" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">还有什么？还有更多已知的约束，例如我们可以对权重的范数设置限制。在这里，我们只是强制它们低于某个最大值。我们希望<strong class="lk jd"> w </strong>的大小低于α，其中α是一个正常数。如果你这样做，我们基本上必须在每次参数更新时投影到单位球上，这又是一种收缩，基本上禁止爆炸梯度。小心，它也可能只是隐藏它们，这样你就再也看不到它们了。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a2add8084d4d92db35fb59367422b603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E1kmn6Ygq2z4sxlh.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">规则化损失还有许多其他变体。来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0CC 下的图片。</p></figure><p id="9712" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">还有许多改变损失的其他方法。每个层都可以有一个约束和一个单独的λ。所以你可以不同地约束每一层，但是我们还没有在文献中看到任何增益。代替权重，激活也可以被约束。这导致不同的变体，例如在稀疏自动编码器中。我们将讨论他们是如何不正则化权重，而是通过激活来形成特定的分布，从而产生稀疏的激活。这也是一个非常有趣的问题，当我们谈到自动编码器和无监督学习时，我们会对此进行更多的讨论。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f08860eb9e83bf6d4515680e04e64f7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*k7Awwj-jkHjy8Fp7.jpg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在这个深度学习讲座中，更多令人兴奋的事情即将到来。<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 下的图片来自<a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">深度学习讲座</a>的 4.0 </a>。</p></figure><p id="9c21" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所以下次在深度学习中，我们想继续讲正则化方法。我们将研究特别为深度学习设计的非常典型的工具。非常有趣的方法，和你们在这节课上看到的略有不同。非常感谢您的收看，再见！</p><p id="7218" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你喜欢这篇文章，你可以在这里找到<a class="ae lh" href="https://medium.com/@akmaier" rel="noopener">更多的文章</a>，在这里找到更多关于机器学习的教育材料<a class="ae lh" href="https://lme.tf.fau.de/teaching/free-deep-learning-resources/" rel="noopener ugc nofollow" target="_blank">，或者看看我们的</a><a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj" rel="noopener ugc nofollow" target="_blank">深度</a> <a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&amp;index=1" rel="noopener ugc nofollow" target="_blank">学习</a> <a class="ae lh" href="https://www.youtube.com/watch?v=p-_Stl0t3kU&amp;list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj" rel="noopener ugc nofollow" target="_blank">讲座</a>。如果你想在未来了解更多的文章、视频和研究，我也会很感激你在 YouTube、Twitter、脸书、LinkedIn 或 T21 上的掌声或关注。本文以<a class="ae lh" href="https://creativecommons.org/licenses/by/4.0/deed.de" rel="noopener ugc nofollow" target="_blank"> Creative Commons 4.0 归属许可</a>发布，如果引用，可以转载和修改。</p><h1 id="8bd3" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">链接</h1><p id="3454" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><a class="ae lh" href="https://www.springer.com/us/book/9780387310732" rel="noopener ugc nofollow" target="_blank">链接</a> —关于最大后验概率估计和偏差-方差分解的详细信息<br/> <a class="ae lh" href="https://arxiv.org/abs/1206.5533" rel="noopener ugc nofollow" target="_blank">链接</a> —关于正则化实用建议的综合文本<br/> <a class="ae lh" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.2059&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">链接</a> —关于校准方差的论文</p><h1 id="e113" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">参考</h1><p id="cc10" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">[1]谢尔盖·约菲和克里斯蒂安·塞格迪。“批量标准化:通过减少内部协变量转移加速深度网络训练”。载于:第 32 届机器学习国际会议论文集。2015 年，第 448–456 页。乔纳森·巴克斯特。“通过多任务抽样学习的贝叶斯/信息论模型”。摘自:机器学习 28.1(1997 年 7 月)，第 7-39 页。<br/>【3】克里斯托弗·m·毕晓普。模式识别和机器学习(信息科学和统计学)。美国新泽西州 Secaucus 出版社:纽约斯普林格出版社，2006 年。<br/> [4]理查德·卡鲁阿纳。多任务学习:归纳偏差的知识来源。收录于:第十届机器学习国际会议论文集。摩根·考夫曼，1993 年，第 41-48 页。<br/>【5】Andre Esteva，Brett Kuprel，Roberto A Novoa，等《深度神经网络的皮肤癌皮肤科医生级分类》。载于:自然 542.7639 (2017)，第 115–118 页。<br/> [6]丁俊钦、徐俊卿、陶行知。“多任务姿态不变人脸识别”。载于:IEEE 图像处理汇刊 24.3(2015 年 3 月)，第 980–993 页。<br/> [7]李万，马修·泽勒，张思欣，等，“用下降连接实现神经网络的正则化”。载于:《第 30 届机器学习国际会议论文集》(ICML，2013 年)，第 1058-1066 页。<br/> [8] Nitish Srivastava，Geoffrey E Hinton，Alex Krizhevsky，等人，“辍学:防止神经网络过度拟合的简单方法。”载于:《机器学习研究杂志》15.1 (2014)，第 1929–1958 页。<br/>[9]r . o .杜达、P. E .哈特和 D. G .施托克。模式分类。约翰威利父子公司，2000 年。<br/> [10]伊恩·古德菲勒、约舒阿·本吉奥和亚伦·库维尔。深度学习。<a class="ae lh" href="http://www.deeplearningbook.org." rel="noopener ugc nofollow" target="_blank">http://www.deeplearningbook.org。</a>麻省理工学院出版社，2016 年。<br/>【11】与何。“群体常态化”。载于:arXiv 预印本 arXiv:1803.08494 (2018)。<br/>【12】何，，任等，“深入挖掘整流器:在 imagenet 分类上超越人类水平的表现”。IEEE 计算机视觉国际会议论文集。2015 年，第 1026–1034 页。<br/>【13】D 乌里扬诺夫，A 韦达尔迪，以及 VS 伦皮茨基。实例规范化:快速风格化缺少的要素。CoRR ABS/1607.0[14]günter Klambauer，Thomas Unterthiner，Andreas Mayr 等，“自规范化神经网络”。在:神经信息处理系统的进展。abs/1706.02515 卷。2017.arXiv: 1706.02515。吉米·巴雷、杰米·瑞安·基罗斯和杰弗里·E·辛顿。“图层规范化”。载于:arXiv 预印本 arXiv:1607.06450 (2016)。<br/>【16】Nima Tajbakhsh，Jae Y Shin，Suryakanth R Gurudu，等，“用于医学图像分析的卷积神经网络:完全训练还是微调？”载于:IEEE 医学成像汇刊 35.5 (2016)，第 1299–1312 页。<br/>【17】约书亚·本吉奥。“深度架构基于梯度训练的实用建议”。《神经网络:交易的诀窍》。斯普林格出版社，2012 年，第 437-478 页。<br/> [18]张，Samy Bengio，Moritz Hardt 等，“理解深度学习需要反思泛化”。载于:arXiv 预印本 arXiv:1611.03530 (2016)。<br/> [19]什巴尼·桑图尔卡，迪米特里斯·齐普拉斯，安德鲁·易勒雅斯等，“批处理规范化如何帮助优化？”在:arXiv e-prints，arXiv:1805.11604(2018 年 5 月)，arXiv:1805.11604。arXiv:1805.11604[统计。ML】。<br/>[20]蒂姆·萨利曼斯和迪德里克·P·金马。“权重标准化:加速深度神经网络训练的简单重新参数化”。神经信息处理系统进展 29。柯伦咨询公司，2016 年，第 901–909 页。<br/>【21】泽维尔·格洛特和约舒阿·本吉奥。“理解训练深度前馈神经网络的困难”。摘自:2010 年第十三届国际人工智能会议论文集，第 249-256 页。<br/>【22】，罗平，陈改来，等，“基于深度多任务学习的人脸标志点检测”。载于:计算机视觉— ECCV 2014 年:第 13 届欧洲会议，瑞士苏黎世，Cham: Springer 国际出版公司，2014 年，第 94–108 页。</p></div></div>    
</body>
</html>