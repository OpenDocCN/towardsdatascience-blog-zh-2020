<html>
<head>
<title>Progressive Neural Networks: Explained &amp; Implemented</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">渐进神经网络:解释与实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/progressive-neural-networks-explained-implemented-6f07366d714d?source=collection_archive---------25-----------------------#2020-06-02">https://towardsdatascience.com/progressive-neural-networks-explained-implemented-6f07366d714d?source=collection_archive---------25-----------------------#2020-06-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bcd2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学习使用PyTorch和Doric库创建渐进式神经网络。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/17688ef9b22503e809235e4d782a1b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/0*52fpnWQA270Wye8s.png"/></div></figure><h2 id="8dce" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">目录</h2><pre class="kg kh ki kj gt lj lk ll lm aw ln bi"><span id="3fde" class="kn ko iq lk b gy lo lp l lq lr">• <a class="ae ls" href="#b575" rel="noopener ugc nofollow">Introduction &amp; Background</a><br/>    ∘ <a class="ae ls" href="#2f8f" rel="noopener ugc nofollow">Transfer Learning</a><br/>    ∘ <a class="ae ls" href="#0d70" rel="noopener ugc nofollow">Progressive Neural Networks</a><br/>• <a class="ae ls" href="#a6bc" rel="noopener ugc nofollow">Doric</a><br/>• <a class="ae ls" href="#a1b0" rel="noopener ugc nofollow">Simple Example</a><br/>• <a class="ae ls" href="#9e99" rel="noopener ugc nofollow">Complex Example</a><br/>    ∘ <a class="ae ls" href="#4b35" rel="noopener ugc nofollow">Autoencoders &amp; VAEs</a><br/>    ∘ <a class="ae ls" href="#50ba" rel="noopener ugc nofollow">Experimental Design</a><br/>    ∘ <a class="ae ls" href="#4088" rel="noopener ugc nofollow">Results</a><br/>    ∘ <a class="ae ls" href="#81c7" rel="noopener ugc nofollow">Analysis</a><br/>• <a class="ae ls" href="#b78d" rel="noopener ugc nofollow">Conclusion</a><br/>• <a class="ae ls" href="#5f5d" rel="noopener ugc nofollow">Acknowledgments</a><br/>• <a class="ae ls" href="#8e8d" rel="noopener ugc nofollow">Links &amp; References</a></span></pre></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="b575" class="ma ko iq bd kp mb mc md ks me mf mg kv jw mh jx kz jz mi ka ld kc mj kd lh mk bi translated"><strong class="ak">简介&amp;背景</strong></h1><p id="11c9" class="pw-post-body-paragraph ml mm iq mn b mo mp jr mq mr ms ju mt kw mu mv mw la mx my mz le na nb nc nd ij bi translated">学习新的东西需要你建立在你已经知道的基础上。就像一把简单的锤子可以被用来将金属塑造成更加动态和复杂的工具一样，先前的知识可以成为形成想法和创造新知识的基础。研究只不过是这种想法一遍又一遍地应用于自身。</p><p id="79ee" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">迁移学习是指将在一项任务中学到的知识应用到另一项相关任务中。许多神经网络转移学习技术已经被成功地使用。然而，网络的拓扑通常是固定的，旧任务中使用的参数很可能被破坏性地覆盖。这些不需要的品质由渐进式神经网络解决，这种网络可以扩展其拓扑结构——用它来传输重要的知识——同时保留原始参数供以后使用。</p><p id="6f4f" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">本文将讨论迁移学习、灾难性遗忘和渐进式神经网络。在这本书中，我们将学习使用Doric库和PyTorch来创建程序网；以及遍历它背后的代码，并检查它如何实现渐进式神经网络论文中讨论的功能。最后，我们将使用Doric运行两个prognet实验，并分析它们的结果，以更好地理解使用渐进式神经网络进行迁移学习的好处和缺点。</p><h2 id="2f8f" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">转移学习</strong></h2><p id="ea0d" class="pw-post-body-paragraph ml mm iq mn b mo mp jr mq mr ms ju mt kw mu mv mw la mx my mz le na nb nc nd ij bi translated">国际象棋是一种有趣而复杂的游戏，像许多伟大的游戏一样，它的复杂性可能需要花一生的时间来掌握——然而它的基本机制可以在一个下午就被教授和理解。每个象棋新手最终都会遇到类似下图的情况，他们会很快学会白棋的最佳走法:移动骑士同时攻击黑棋的两个车。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nj"><img src="../Images/1db9f9e2bc29d2a824ecc3e351e7747e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IHE5a2TWZfDgC-DZ-As4gw.png"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated">通过同时威胁两个目标，白棋迫使一个有利的交易。</p></figure><p id="a4ea" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">这种策略被称为“叉”，将黑方置于两败俱伤的境地，他们必须用自己的一辆车交换白方的骑士。熟练的棋手将会知道迫使对手下叉是非常有利的。</p><p id="c171" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">国际象棋并不是唯一出现这种简单战术的游戏。Connect-4是一种游戏，其中两名玩家将瓷砖扔进一个6×7的直立网格中，目标是在对手之前完成4个瓷砖。下图是一个connect-4板。红色有机会以这样一种方式连接三个瓷砖，蓝色只能阻止一边的运行，让红色在下一轮获胜。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ns"><img src="../Images/98bb50f130d440ca5da67fda5af3d152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fRPZjWEhY8qsBDtF5GehiQ.png"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated">Connect-4战略在很大程度上基于这一策略。</p></figure><p id="247f" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">国际象棋和Connect-4是非常不同的游戏，但它们都包括从两个方向同时进攻的相同策略，迫使敌人陷入无赢的局面。事实上，叉子战术可以在许多游戏中找到。通过学习叉子的概念，并通过不同游戏的机制应用该概念，我们能够跨不同领域传递知识。学象棋可以教我们一些关于Connect-4的知识。</p><p id="8eaa" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">亲爱的读者，这就是迁移学习的本质。</p><p id="d428" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">迁移学习是将从一项任务中获得的知识应用到另一项独立但相关的任务中的过程。近年来，迁移学习在语言和图像处理领域得到了广泛的应用。在自然语言处理中，预训练的嵌入模型如<a class="ae ls" href="https://arxiv.org/pdf/1802.05365.pdf" rel="noopener ugc nofollow" target="_blank">埃尔莫</a>、<a class="ae ls" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">伯特</a>和<a class="ae ls" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>已经成功地学习了英语的基本结构，从而可以应用于许多任务。在这种情况下，模型通常被冻结，优化只在连接到嵌入器头部的任务模型上执行。虽然这种方法对于语言任务来说是理想的，但它依赖于遵循语言结构的一致输入。在图像处理中，完全训练卷积神经网络来完成复杂的任务已经变得罕见。相反，通才预训练网络，如<a class="ae ls" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank"> ResNet-50 </a>、<a class="ae ls" href="https://arxiv.org/pdf/1409.4842.pdf" rel="noopener ugc nofollow" target="_blank"> GoogLeNet </a>和<a class="ae ls" href="https://arxiv.org/pdf/1409.1556.pdf" rel="noopener ugc nofollow" target="_blank"> VGG-19 </a>会根据新数据进行再训练。这个过程被称为微调。虽然微调是使网络适应新数据的有效方法，但它也不是没有缺点。</p><p id="a420" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">灾难性遗忘是一种有害事件，在微调过程中可能发生在经过训练的神经网络中。当网络中的重要参数被更改以适应新数据时，就会发生这种情况，从而损害网络处理旧数据的能力。理想情况下，网络可以通过在很少使用的参数中编码新信息，或者通过改变重要参数以推广到其他任务而不是完全切换到其他任务，来保持其在先前任务中的能力。使用标准的基于梯度的学习方法，这通常是不可能的。已经提出了许多对抗灾难性遗忘的方法，但这里讨论的算法不止于此:它完全消除了灾难性遗忘，同时仍然允许有效的一般迁移学习。</p><p id="0d70" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated"><strong class="mn ir">渐进神经网络</strong></p><p id="ea9b" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">渐进神经网络(prognets)是Deepmind在他们的论文<a class="ae ls" href="https://arxiv.org/pdf/1606.04671.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nt">渐进神经网络</em> </a>(鲁苏等人，2016)中开发的一种神经算法。程序网是一种简单、强大和创造性的迁移学习的解决方案——引用论文摘要，它们“不会遗忘，并且可以通过与先前学习的特征的横向连接来利用先前的知识”。</p><p id="3a70" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">程序网开始存在时只有一个单列的神经网络，它将在一个初始任务上被训练。每列由<em class="nt"> L </em>个块组成，每个块包括一层神经元<em class="nt"> W </em>，一组用于每个父列的侧神经元<em class="nt"> U </em>，以及一个激活函数<em class="nt"> f </em>。由于初始列没有传入的横向连接，它将与标准神经网络的行为相同。这第一列学习任务1，然后它的参数被冻结。对于任务2，将生成一个新列，并添加横向连接。这些横向连接将所有先前列中的先前块的输出作为输入。然后将侧层添加到主层上，最后进行活化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/070f2f837ecbf3878b1f657d31f61472.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/0*J79539qD6ilv4ODP"/></div><p class="no np gj gh gi nq nr bd b be z dk translated">列<em class="nv"> k </em>的块I的方程。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0c97db2256460ea122aff35dcd3408a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/0*iCwP4kOTiL5V_VKp"/></div><p class="no np gj gh gi nq nr bd b be z dk translated">第一列(k = 0)。</p></figure><p id="9a94" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">下面是一个prognet的插图，在progressive neural nets论文中发现了K = 3个任务和L = 3个块。请注意，任何给定的块都是其自身参数、其自身列中的前一个块的输出以及所有先前列中的前一个块的输出的函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/695183b2938c67c79d7423cd1f50c173.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/0*5Wnh1B5FqKqz2Pm4"/></div><p class="no np gj gh gi nq nr bd b be z dk translated"><a class="ae ls" href="https://arxiv.org/pdf/1606.04671.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nv">渐进式神经网络</em> </a></p></figure><p id="f1c8" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">理解程序网的另一种方式是把每个程序块看作一个计算的黑盒，它需要决定哪些信息源对完成它的任务最有效。例如，在上图中，<em class="nt"> output₃ </em>可以访问先前的隐藏层<em class="nt"> h₂ </em>。但是，它还可以访问每个父列的<em class="nt"> h₂ </em>层，并可以使用其横向参数在本地微调它们的输出。如果<em class="nt"> output₃ </em>模块发现<em class="nt"> h₂ </em>拥有它需要的所有信息，它可以“清零”其他输入。如果它发现<em class="nt"> h₂ </em>和<em class="nt"> h₂ </em>各自提供了所需的部分信息，它可以忽略<em class="nt"> h₂ </em>，充当<em class="nt"> h₂ </em>和<em class="nt"> h₂ </em>的函数。通过建立这些横向连接，我们允许网络轻松地在任务之间传递信息，同时也允许它忽略不重要的信息。</p><p id="fea8" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">作者在一组强化学习环境中测试了他们的算法，包括pong变体、迷宫变体和Atari游戏。他们对各种基线模型进行了测试，发现程序网优于每种基线。在发表之后，其他几篇论文已经展示了跨不同领域的程序网的成功使用；例如，<a class="ae ls" href="https://arxiv.org/pdf/1706.03256.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nt">渐进式神经网络用于情感识别中的迁移学习</em> </a> (Gideon et al .，2017)发现，在给定一组积木式任务的情况下，渐进式神经网络可用于情感检测。类似地，<a class="ae ls" href="https://arxiv.org/pdf/1610.04286.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nt">用渐进网络</em> </a>(鲁苏等人，2018)从像素学习的模拟到真实机器人表明，渐进神经网络可以用于在不完美的机器人模拟和真实硬件之间进行转换。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ny"><img src="../Images/67f75f3f417e300afb4695dd4d87eb44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4D-6k8egUMJjiYfHeEnICg.png"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated"><a class="ae ls" href="https://arxiv.org/pdf/1610.04286.pdf" rel="noopener ugc nofollow" target="_blank">用机械手将Sim传送到机器人。</a></p></figure><p id="101b" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">虽然上述渐进式神经网络出版物在各种强化学习任务上测试其算法时产生了令人信服的结果，但程序网并不是迁移学习的灵丹妙药。主要是它们是参数浪费的:每个侧化的块需要一个完整的侧层用于它从中提取的每个父列。目前形式的程序网也仅限于对每个记录都有相应任务标签的数据起作用。为了允许冻结，每个任务必须按顺序学习，而顺序会极大地影响最终网络的质量。这通常只是一个较小的限制，但仍然是一个限制。此外，由于冻结，如果任务改变，程序就不灵活。因此，这意味着色谱柱应该在冻结前非常熟练地完成任务。</p><p id="7793" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">总之，渐进式神经网络不受灾难遗忘的影响，因为它们冻结了每个完成的列，但它们也允许通过与新列的横向连接进行有效的迁移学习。他们牺牲了增加的模型复杂性(即，在每个额外的列中训练更多的参数)和改变任务的灵活性来实现这一点。许多先前的出版物和实验支持这种算法作为有效的迁移学习平台的可信度。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="a6bc" class="ma ko iq bd kp mb mc md ks me mf mg kv jw mh jx kz jz mi ka ld kc mj kd lh mk bi translated"><strong class="ak">多利安</strong></h1><p id="8ccf" class="pw-post-body-paragraph ml mm iq mn b mo mp jr mq mr ms ju mt kw mu mv mw la mx my mz le na nb nc nd ij bi translated">Doric是我开发的工具，用于实现和扩展渐进式神经网络。这是一个建立在PyTorch之上的免费开源库。在本文的剩余部分，我们将遍历Doric背后的代码，用它运行一个简单的实验来分析正在训练的prognet，最后，在一组困难的图像处理任务上测试Doric。</p><p id="3de1" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">注意:这里显示的Doric版本是简化的。完整版带示例代码<a class="ae ls" href="https://github.com/arcosin/Doric" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="1b79" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">首先，我们的库需要块的定义。每个块必须包含一个激活函数，一个充当块的主层的模块，以及一个实现<em class="nt"> U </em>参数的横向层列表。Doric使用下面的抽象类。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="403a" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">通过扩展这个类和实现未实现的方法，用户可以很容易地为任何可以想象的用途创建他们自己的程序块。再补充几个简单的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="4772" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">现在我们有了简单密集层、批量标准化密集层和2D卷积层的模块。注意，在我们的对象中，任何包含PyTorch模块的列表都被实现为nn.ModuleLists，这是因为它允许PyTorch正确地注册这些子模块。另外值得注意的是，因为ProgBlocks是nn的一个子类。模块，它们可以像任何其他PyTorch模块一样运行。但是，我们没有在它们上面实现forward方法，所以我们仍然需要调用runActivation和runBlock。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><pre class="kg kh ki kj gt lj lk ll lm aw ln bi"><span id="9358" class="kn ko iq lk b gy lo lp l lq lr">tensor([<br/>[0.0000, 0.0000, 1.8789, 0.0000, 0.0000],<br/>[0.0138, 0.0000, 2.9942, 0.0000, 0.0000],<br/>[1.5449, 0.0000, 2.8891, 0.0000, 0.0000]], <br/>grad_fn=&lt;ReluBackward0&gt;)</span></pre><p id="4af3" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">现在我们有了一个block对象，我们需要将它们组织成列网络。每次指定新任务时，都需要添加这些列网。然而，由于我们有一个很好的可扩展的块结构，我们不需要依赖用户来实现单独的列——至少不直接。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="e635" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">最后，我们需要整个渐进式神经网络。该对象将包含一个列网列表，并将运行每个必要的列，以产生给定任务的结果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="1bcd" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">有几件重要的事情需要注意。首先，在addColumn中，Doric允许用户直接传递列网，或者用户可以通过扩展ProgColumnGenerator类来创建列生成器。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="a4b8" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">这允许用户在generateColumn方法中定义每个列的体系结构，这样prognet就可以添加列，而无需传递列。msg参数还允许用户通过addColumn方法传递信息来生成Column。</p><p id="93f9" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">第二件特别重要的事是向前的方法。这实际上是渐进神经网络算法的核心。prognet按顺序运行每一列，直到到达与给定任务相关联的列。这允许后面的列访问每个分支的输入。</p><p id="dfa7" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">就其本身而言，这足以为任何简单的顺序体系结构创建渐进式网络，但当我们需要更复杂的体系结构时，它就会崩溃。举个例子，这张Q函数评论家网络图是我在另一个项目中作为<a class="ae ls" rel="noopener" target="_blank" href="/deep-deterministic-policy-gradients-explained-2d94655a9b7b"> DDPG </a>代理的一部分而写的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ob"><img src="../Images/76d1ffbb41322501a34bb8d2e973baa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/0*Kh5QUdGqDOmtvwdl"/></div></div></figure><p id="6f8d" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">为这样的网络设计渐进式系统的最简单方法是允许使用多通道模块。这些砌块还必须允许某些非横向通道上的直通通道。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ob"><img src="../Images/12f6ea81aa71f66a3c8a9ef6c63b13ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/0*1j0mgvF3MbNmqChl"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oc"><img src="../Images/cd0e3e898f68d474b00f3dd69a9d0952.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/0*JHxrhN1vY0SrnxaY"/></div></div></figure><p id="cee3" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">除此之外，我们还需要解决Doric的一个特性:惰性和lambda块。惰性块是可以被侧化的块——不在初始列中，也不在第一行中——但是故意不被侧化。这些允许我们在我们的网络中包括特殊的操作，或者减少那些没有从迁移学习中受益的块中不必要的参数。</p><p id="cd21" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">要实现这两个更改，我们只需要定义它们，并对我们的列网络类做一些小的更改。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="7bc4" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">仅此而已。截至本文发表时，这是Doric的(简化的)核心功能。使用这些工具，我们可以将大多数神经网络架构实现为一个prognet。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="a1b0" class="ma ko iq bd kp mb mc md ks me mf mg kv jw mh jx kz jz mi ka ld kc mj kd lh mk bi translated"><strong class="ak">简单的例子</strong></h1><p id="412f" class="pw-post-body-paragraph ml mm iq mn b mo mp jr mq mr ms ju mt kw mu mv mw la mx my mz le na nb nc nd ij bi translated">对于第一个Doric示例，我们将创建一个双任务prognet。具体来说，我们将教会我们的列模拟xor和nand函数。回想一下，xor和nand的真值表如下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/9f4bf6e5751c97192001ee0ca9694518.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/0*Vnuu8mUB6ORdw10t"/></div></figure><p id="65ca" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">在这个简单的实验中，我们的第一篇专栏文章将解决xor任务，第二篇将利用第一篇文章解决nand问题。要明确的是，这不是一套需要迁移学习的任务。具有这种结构的单柱网络可以毫不费力地解决任一任务。这个实验的价值在于，它将允许我们深入分析这些网络，并观察横向连接如何传递知识。这是我们将在实验中使用的代码。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="f073" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">运行该程序后，我们会收到这些结果。</p><pre class="kg kh ki kj gt lj lk ll lm aw ln bi"><span id="78b8" class="kn ko iq lk b gy lo lp l lq lr">Testing Xor.<br/>Input: [0, 0].  Target: 0.  Predicted: 0.  Error: 0.<br/>Input: [0, 1].  Target: 1.  Predicted: 1.  Error: 0.<br/>Input: [1, 0].  Target: 1.  Predicted: 1.  Error: 0.<br/>Input: [1, 1].  Target: 0.  Predicted: 0.  Error: 0.</span><span id="edc2" class="kn ko iq lk b gy oe lp l lq lr">Testing Nand.<br/>Input: [0, 0].  Target: 1.  Predicted: 1.  Error: 0.<br/>Input: [0, 1].  Target: 1.  Predicted: 1.  Error: 0.<br/>Input: [1, 0].  Target: 1.  Predicted: 1.  Error: 0.<br/>Input: [1, 1].  Target: 0.  Predicted: 0.  Error: 0.<br/></span><span id="b8b4" class="kn ko iq lk b gy oe lp l lq lr">Xor and Nand params:<br/>columns.0.blocks.0.module.weight<br/>Parameter containing:<br/>tensor([[-3.2474,  3.2474],<br/>        [-2.0008,  2.0429],<br/>        [-0.5795,  0.0801]])<br/>columns.0.blocks.0.module.bias<br/>Parameter containing:<br/>tensor([-1.7442e-05,  2.0007e+00, -3.0051e-01])<br/>columns.0.blocks.1.module.weight<br/>Parameter containing:<br/>tensor([[ 4.5644, -3.4079,  0.1078]])<br/>columns.0.blocks.1.module.bias<br/>Parameter containing:<br/>tensor([3.0512])<br/>columns.1.blocks.0.module.weight<br/>Parameter containing:<br/>tensor([[-0.2739, -0.1766],<br/>        [ 0.5853,  2.8434],<br/>        [-0.6689,  0.3338]], requires_grad=True)<br/>columns.1.blocks.0.module.bias<br/>Parameter containing:<br/>tensor([-0.3227, -0.5853, -0.3339], requires_grad=True)<br/>columns.1.blocks.1.module.weight<br/>Parameter containing:<br/>tensor([[-0.1367, -2.9894, -0.4210]], requires_grad=True)<br/>columns.1.blocks.1.module.bias<br/>Parameter containing:<br/>tensor([1.9972], requires_grad=True)<br/>columns.1.blocks.1.laterals.0.weight<br/>Parameter containing:<br/>tensor([[1.9479, 0.2868, 0.1236]], requires_grad=True)<br/>columns.1.blocks.1.laterals.0.bias<br/>Parameter containing:<br/>tensor([1.8058], requires_grad=True)</span></pre><p id="1e97" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">这些日志显示网络学习了xor和nand功能，但是如果我们想知道它是如何完成的，我们需要更仔细地观察。让我们从xor列开始。在一个图形结构上绘制出所有的参数后，我们可以构建下图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/999ceb4b52a0e45605fa09ff2589ab3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/0*InS4Xvg3e9CThTxn"/></div></figure><p id="74d7" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">注意事项:</p><ul class=""><li id="8066" class="og oh iq mn b mo ne mr nf kw oi la oj le ok nd ol om on oo bi translated">最右边的神经元将总是输出0。这意味着学习这个函数是不必要的。</li><li id="b15c" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated">如果X1和X2相同，它们将抵消，并且偏移参数将确定隐藏层的输出。结果是sigmoid(-3.6)，或~0.02。</li><li id="9fe5" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated">如果X1是1，X2是0，隐藏层输出[0，0，0]。结果是sigmoid(3.2)，或~0.96。</li><li id="0105" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated">如果X1是0，X2是1，隐藏层输出[3.2，4，0]。结果是~0.98。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ny"><img src="../Images/ca7a9d47aeec8cf2c09f0e3d5a6cd12c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GwLNt7T7zHmLA_OhRs4-MQ.png"/></div></div></figure><p id="5ffc" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">所有这些都是值得了解的，但也不足为奇。让我们看看当我们添加nand列时会发生什么。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ou"><img src="../Images/b234a8f384300f3600e4684711ef7ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*r0EnmBVA6Xzb4sd8"/></div></div></figure><p id="6c1f" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">现在，我们列举所有的输入组合。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ou"><img src="../Images/d2687b99d8a628f8d77ef6973bb70db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*04Xa9CzEbdQdGRuj"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated">X = [0，0]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ou"><img src="../Images/1ba54a8420206f4552cdfd866338974e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iszjrw_lCy1hS3Qj"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated">X = [0，1]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ou"><img src="../Images/6ada26eec9f12404c99468d620993000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*u8Z2reNu5fsz_CZF"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated">X = [1，0]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ou"><img src="../Images/439eed8032840b1db0411da42218542a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eYFkGuBBk3SjnYZa"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated">X = [1，1]</p></figure><p id="b2f9" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">需要注意的事项:</p><ul class=""><li id="f4f4" class="og oh iq mn b mo ne mr nf kw oi la oj le ok nd ol om on oo bi translated">这一次，两个隐藏层神经元被置零。这是因为nand比xor容易得多(nand是线性可分的)。</li><li id="7ce0" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated">隐藏层中仍然活跃的神经元将输出0、2.8或2.2。</li><li id="22a6" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated">当主动隐藏神经元输出2.2时，外侧层出现尖峰，以补偿非外侧输入的-4.6值。</li><li id="407b" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated">如果横向层被归零，给定[0，1]输入，答案将是不正确的。</li><li id="3160" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated">当给定输入为[0，0]或[1，1]时，分支输出相同的值。对于这些输出，分支并不重要。</li></ul></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="9e99" class="ma ko iq bd kp mb mc md ks me mf mg kv jw mh jx kz jz mi ka ld kc mj kd lh mk bi translated">复杂示例</h1><h2 id="4b35" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">自动编码器&amp;阀门</h2><p id="c0c1" class="pw-post-body-paragraph ml mm iq mn b mo mp jr mq mr ms ju mt kw mu mv mw la mx my mz le na nb nc nd ij bi translated">自动编码器是一种功能强大的架构，可以应用于许多任务并产生良好的结果(尤其是图像处理任务)。它们也是演示渐进式网络的理想选择。这有两个原因:</p><ol class=""><li id="489c" class="og oh iq mn b mo ne mr nf kw oi la oj le ok nd ov om on oo bi translated">编码器部分的任务之间可能有很多共享信息。</li><li id="8ec9" class="og oh iq mn b mo op mr oq kw or la os le ot nd ov om on oo bi translated">自动编码器通常是非常深的网络，这一特性允许我们展示一个具有大量分支的网络。</li></ol><p id="ecdb" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">可变自动编码器(VAEs)是自动编码器的一种变体，其中网络学习数据的平均值和标准偏差的抽象，而不是数据本身。这给了解码器部分强大的生成能力！这篇文章很长，没有深入解释自动编码器和VAEs，所以我推荐阅读Irhum Shafket 撰写的这篇<a class="ae ls" rel="noopener" target="_blank" href="/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">关于数据科学的文章。</a></p><h2 id="50ba" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">实验设计</strong></h2><p id="16f7" class="pw-post-body-paragraph ml mm iq mn b mo mp jr mq mr ms ju mt kw mu mv mw la mx my mz le na nb nc nd ij bi translated">对于这个实验，我们将使用Doric构建一个渐进变分自动编码器。该网络将能够在图像处理领域执行四种不同的任务。下面列出了这些操作。</p><ol class=""><li id="d519" class="og oh iq mn b mo ne mr nf kw oi la oj le ok nd ov om on oo bi translated">简单重构—将输入重构为输出，通过瓶颈层保留重要信息。</li><li id="2c97" class="og oh iq mn b mo op mr oq kw or la os le ot nd ov om on oo bi translated">去噪-从输入中移除椒盐噪声。</li><li id="eb26" class="og oh iq mn b mo op mr oq kw or la os le ot nd ov om on oo bi translated">着色-向输入的灰度版本添加颜色。</li><li id="a7bd" class="og oh iq mn b mo op mr oq kw or la os le ot nd ov om on oo bi translated">修复—在图像中被随机游走的黑色像素遮挡的部分进行绘画。</li></ol><p id="cf63" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">我们将使用<a class="ae ls" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" rel="noopener ugc nofollow" target="_blank">名人数据集</a>进行实验，这是一个大规模的名人脸部图像集。这个数据集适合我们实验的参数，因为它的大小，难度，以及容易注意到人脸的不一致。</p><p id="f8cc" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">运行这个实验的完整代码可以在示例目录中的<a class="ae ls" href="https://github.com/arcosin/Doric/tree/master/examples" rel="noopener ugc nofollow" target="_blank"> Doric存储库中找到。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/2b7da310a71ea2a54349febe54054b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/0*__tTXtJFRZx651iY"/></div><p class="no np gj gh gi nq nr bd b be z dk translated">我们的图像处理VAE柱的结构。</p></figure><h2 id="4088" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">结果</h2><p id="d0bc" class="pw-post-body-paragraph ml mm iq mn b mo mp jr mq mr ms ju mt kw mu mv mw la mx my mz le na nb nc nd ij bi translated">下面是进步VAE的三个实验柱的结果。所有格式都是<em class="nt">原始/输入/输出</em>。列0(简单重建)未显示，因为它不包含分支。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ox"><img src="../Images/90e7dcffd65c22841269dd7062260f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bf2E_XmqPPaiLcWX6PbJbg.png"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated">第1列(去噪)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi oy"><img src="../Images/11d64ec583faaa2ed3deda0de81d39fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s1YQOw60cFxcvOiBkBuLIg.png"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated">第2列(着色)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ox"><img src="../Images/d88999bd2e7a63ca91fd0519457c4ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BXwsjC1l8hsfgKXlq6zkWg.png"/></div></div><p class="no np gj gh gi nq nr bd b be z dk translated">第3列(修复)</p></figure><h2 id="81c7" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">分析</h2><p id="a410" class="pw-post-body-paragraph ml mm iq mn b mo mp jr mq mr ms ju mt kw mu mv mw la mx my mz le na nb nc nd ij bi translated">为了开始我们对结果的分析，我认为首先解决autoencoder的奇怪之处是很重要的——虽然有些图像翻译得很好，但有些却不是。然而，当VAE失败时，它很少通过创建错位或不现实的人脸来实现。相反，它过度概括，创造了一个新的人类面孔，更类似于以前见过的面孔。这些种类的错误，以及其他奇怪的错误，都不是由prognet引起的，在独立的VAE中也会发生。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/59c55488baf11ad4691a0c8007298de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/0*Pq27dupQcqgGe-AS"/></div><p class="no np gj gh gi nq nr bd b be z dk translated">在这张图片中，网络做了很好的重建工作，甚至匹配了大部分被随机行走覆盖的头发梯度。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/bc43840296282b6156868e1561c370b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/0*L8uKtrv1bn8i_39u"/></div><p class="no np gj gh gi nq nr bd b be z dk translated">另一方面，这个人完全被取代了。也有很多照片中的脸没有被替换，但是眼镜被拿掉了。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/08b8aeae9a740eef2e948d9998aa7055.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/0*80x3QzmHJcF50xtm"/></div><p class="no np gj gh gi nq nr bd b be z dk translated">背景极有可能变得模糊，因为VAE知道背景并不重要。</p></figure><p id="d967" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">也就是说，我们可以开始分析与prognet相关的结果。从降噪专栏开始，我们看到了我们对任何正常的VAE降噪师的期望。网络在去除噪声和保持图像质量方面做得很好(模糊是可以预料的，因为瓶颈只有128)。值得注意的是训练时间；如果您运行代码，您可能会发现，作为一个prognet，网络学习去噪要比作为一个独立的网络快得多。</p><p id="f9d1" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">在彩色栏中，我们可以看到结果又一次和我们预期的一样。更值得注意的任务是由第三列执行的修复。在之前专栏知识的帮助下，我们能够得到惊人的结果。即使图像的一小部分通过掩模被损坏的像素曝光，网络也可以重建图像，几乎与简单的重建一样好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/0c7d9ead5f23b5b43a2956cf4b9d1095.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/0*kl_JODLXhcALdcpk"/></div><p class="no np gj gh gi nq nr bd b be z dk translated">由于只能看到她嘴的一小部分，VAE能够重现她口红的颜色。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/1d5b52b8d422ba5a75fb3b751aaed19b.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/0*vFg8p2XBhTMAJorN"/></div><p class="no np gj gh gi nq nr bd b be z dk translated">即使重要的面部特征被遮挡，VAE也能够利用现有的信息重建出合理的相似度。</p></figure><p id="891e" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">生成高质量的结果图像对prognet来说是一个好兆头，但要了解横向参数的影响，我们需要找到一些方法来测试它们如何影响结果。一种方法是在移除横向参数的情况下运行正向通道。如果结果仍然是可识别的，分支没有做太多。如果结果是明亮的但不可识别的，则分支与主网络参数一起工作来创建结果。如果结果是暗的或完全黑的，则该列仅使用或主要使用分支进行计算。这是那次向前传球的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/a6a23ee7dc7c815c217f8a7646234d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/0*n0dJOc-HIWPh14-I"/></div><p class="no np gj gh gi nq nr bd b be z dk translated">无侧边去噪。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/1e9de0b059f45bcb071d6cf7f3e791af.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/0*aU1DItAqcizS8Tz9"/></div><p class="no np gj gh gi nq nr bd b be z dk translated">无侧边着色。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/9af3836c0d024e7ce971c751af610b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/0*xK3FYc9VRnTMAKTH"/></div><p class="no np gj gh gi nq nr bd b be z dk translated">无侧边修补。</p></figure><p id="ebfc" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">这些结果表明，在列1和列2中，大部分计算是由分支完成的。在最后一栏中，几乎所有的工作都是由分支完成的。这是迁移学习正在发生的一个强有力的标志。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="b78d" class="ma ko iq bd kp mb mc md ks me mf mg kv jw mh jx kz jz mi ka ld kc mj kd lh mk bi translated">结论</h1><p id="4cb5" class="pw-post-body-paragraph ml mm iq mn b mo mp jr mq mr ms ju mt kw mu mv mw la mx my mz le na nb nc nd ij bi translated">渐进神经网络是迁移学习和连续学习的有力工具。虽然它们受到浪费内存使用、需要标记和明确定义的任务以及无法适应先前学习的任务变化的限制，但它们完全不受灾难性遗忘的影响，并且它们为持续学习神经系统的宽度方向增长提供了新的模板。在progressive neural net论文中进行的实验，以及其他论文和本文中提出的实验，显示了强有力的证据，表明prognets是迁移学习的可行工具。</p><p id="bc7d" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">同样，Doric是一个用于程序开发和实验的有用的轻量级库。借助用户创建的模块以及多通道模块和惰性模块，几乎任何标准架构都可以作为渐进式网络实施。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="5f5d" class="ma ko iq bd kp mb mc md ks me mf mg kv jw mh jx kz jz mi ka ld kc mj kd lh mk bi translated">感谢</h1><p id="9e20" class="pw-post-body-paragraph ml mm iq mn b mo mp jr mq mr ms ju mt kw mu mv mw la mx my mz le na nb nc nd ij bi translated">感谢Case Wright在开发Doric和编写我们所有的autoencoder测试中的帮助，感谢David Stucker的帮助编辑，感谢Gustavo Rodriguez-Rivera博士的指导和学术支持，感谢Purdue大学。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="8e8d" class="ma ko iq bd kp mb mc md ks me mf mg kv jw mh jx kz jz mi ka ld kc mj kd lh mk bi translated"><strong class="ak">链接&amp;引用</strong></h1><ul class=""><li id="b3e2" class="og oh iq mn b mo mp mr ms kw pb la pc le pd nd ol om on oo bi translated"><em class="nt">渐进式神经网络论文—</em><a class="ae ls" href="https://arxiv.org/pdf/1606.04671.pdf" rel="noopener ugc nofollow" target="_blank"><em class="nt"/></a></li><li id="b084" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated"><em class="nt">全多利安库—</em><a class="ae ls" href="https://github.com/arcosin/Doric" rel="noopener ugc nofollow" target="_blank"><em class="nt">https://github.com/arcosin/Doric</em></a></li><li id="0e30" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated"><em class="nt"> CelebA数据集—</em><a class="ae ls" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" rel="noopener ugc nofollow" target="_blank"><em class="nt">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</em></a></li><li id="f374" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated"><em class="nt">自动编码器和VAEs解释—</em><a class="ae ls" rel="noopener" target="_blank" href="/intuitively-understanding-variational-autoencoders-1bfe67eb5daf"><em class="nt">https://towards data science . com/直观-理解-变分-自动编码器-1bfe67eb5daf </em> </a></li><li id="215d" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated"><em class="nt">用于情感识别中迁移学习的渐进式神经网络—</em><a class="ae ls" href="https://arxiv.org/pdf/1706.03256.pdf" rel="noopener ugc nofollow" target="_blank"><em class="nt">https://arxiv.org/pdf/1706.03256.pdf</em></a></li><li id="7bdf" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated"><em class="nt">虚拟现实机器人用渐进网络从像素学习—</em><a class="ae ls" href="https://arxiv.org/pdf/1610.04286.pdf" rel="noopener ugc nofollow" target="_blank"><em class="nt">https://arxiv.org/pdf/1610.04286.pdf</em></a></li><li id="3580" class="og oh iq mn b mo op mr oq kw or la os le ot nd ol om on oo bi translated"><em class="nt">棋图制作人—</em><a class="ae ls" href="http://svg_experimenten.deds.nl/chessboard/chess_diagram_maker.html" rel="noopener ugc nofollow" target="_blank"><em class="nt">http://SVG _ experimenten . DEDS . nl/棋盘/Chess _ diagram _ maker . html</em></a></li></ul><p id="70fb" class="pw-post-body-paragraph ml mm iq mn b mo ne jr mq mr nf ju mt kw ng mv mw la nh my mz le ni nb nc nd ij bi translated">除非另有说明，所有图片和其他版权材料均由作者创作。</p></div></div>    
</body>
</html>