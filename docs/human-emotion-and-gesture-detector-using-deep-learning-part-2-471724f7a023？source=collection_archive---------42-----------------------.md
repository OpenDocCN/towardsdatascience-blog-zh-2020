# 使用深度学习的人类情感和手势检测器:第 2 部分

> 原文：<https://towardsdatascience.com/human-emotion-and-gesture-detector-using-deep-learning-part-2-471724f7a023?source=collection_archive---------42----------------------->

## [情感手势检测](https://medium.com/tag/emotion-gesture-detection/latest)

## 深入探究人类情感和手势识别

大家好！欢迎回到使用深度学习的*人类情感和手势检测器*的第 2 部分。如果你还没有看过，在这里 **看看第一部分[。](/human-emotion-and-gesture-detector-using-deep-learning-part-1-d0023008d0eb)**在本文中，我们将介绍手势模型的训练，并探讨如何提高情绪模型的准确性。最后，我们将使用计算机视觉创建一个最终管道，通过它我们可以访问我们的网络摄像头，并从我们训练的模型中获得声音响应。事不宜迟，让我们开始编码并理解这些概念。

![](img/0ed3a27757527f34bd41b5089bf2ca65.png)

资料来源:法国-v-unsplash

为了训练手势模型，我们将使用迁移学习模型。我们将使用 VGG-16 架构来训练模型，并排除 VGG-16 的顶层。然后，我们将继续添加我们自己的自定义层，以提高精度和减少损失。我们将尝试在我们的手势模型上实现大约 95%的整体高准确性，因为我们有一个相当平衡的数据集，并使用图像数据增强和 VGG-16 迁移学习模型的技术。与我们的情绪模型相比，这项任务可以很容易地实现，并且在更少的时期内实现。在以后的文章中，我们将介绍 VGG-16 架构是如何工作的，但现在让我们继续分析手头的数据，并对手势数据集进行探索性数据分析，类似于我们在提取图像后对情感数据集进行的分析。

# 探索性数据分析(EDA):

在下一个代码块中，我们将查看 train 文件夹中的内容，并尝试计算出 train 文件夹中每个手势类别的总类别数。

## 火车:

![](img/355e36a24d3542d55bb33f499c8e2806.png)

我们可以查看 train1 文件夹中的四个子文件夹。让我们直观地看看这些目录中的图像数量。

## 条形图:

![](img/f5881ff5a34950c68e59524c5f29e3b5.png)

我们可以从条形图中注意到，每个目录都包含 2400 个图像，这是一个完全平衡的数据集。现在，让我们开始可视化火车目录中的图像。我们将查看每个子目录中的第一个图像，然后查看这些文件夹中每个图像的尺寸和通道数。

![](img/18a66b9f465b801fea12623a7fce5b46.png)![](img/c982bc41c062c199ce8dcb80fd1fe049.png)

图像的尺寸如下:

图像的高度= 200 像素
图像的宽度= 200 像素
通道数= 3

类似地，我们可以对 validation1 目录执行分析，并检查我们的验证数据集和验证图像看起来如何。

## 验证:

## 条形图:

![](img/a5d534706a8e6f187eb7772befd246c7.png)

我们可以从条形图中注意到，每个目录包含 600 个图像，这是一个完全平衡的数据集。现在，让我们开始可视化验证目录中的图像。我们将查看每个子目录中的第一个图像。这些文件夹中每个图像的通道尺寸和数量与列车目录相同。

![](img/c8df906662a9c4dd938d9a3f8b762419.png)![](img/c01b73af8f482801f19df45b35a6751e.png)

至此，我们完成了手势数据集的探索性数据分析(EDA)。我们可以继续构建姿势训练模型以进行适当的姿势预测。

# 手势训练模型:

让我们看看下面的代码块，以了解我们正在导入的库，并设置类的数量及其维度和各自的目录。

导入所有重要的所需深度学习库来训练手势模型。
Keras 是一个应用编程接口(API)，可以运行在 tensorflow 之上。
Tensorflow 将是我们用来构建深度学习模型的主要深度学习模块。
从 tensorflow，我们将提到一个名为 VGG-16 的预训练模型。我们将使用带有定制卷积神经网络(CNN)的 VGG-16，即我们将使用我们的迁移学习模型 VGG-16 以及我们自己的定制模型来训练一个整体准确的模型。keras 的 VGG-16 模型使用 imagenet 权重进行预训练。

ImageDataGenerator 用于数据扩充，其中模型可以看到模型的更多副本。数据扩充用于创建原始图像的副本，并在每个时期使用这些变换。
将使用的训练层如下:
1。**输入** =我们传递输入形状的输入层。
2。 **Conv2D** =卷积层与输入结合，提供张量输出。
3。 **Maxpool2D** =对卷积层的数据进行下采样。
4。**批量标准化** =这是一种用于训练深度神经网络的技术，它将每个小批量的输入标准化到一个层。这具有稳定学习过程和显著减少训练深度网络所需的训练时期的效果。
5。 **Dropout** = Dropout 是一种在训练过程中忽略随机选择的神经元的技术。它们是随机“退出”的，这防止了过度拟合。
6。**密集** =全连接层。
7。展平 =将整个结构展平成一个一维数组。

模型可以构建在如该特定模型所示的模型状结构中，或者可以以顺序方式构建。在这里，我们将使用一个类似功能 API 模型的结构，它不同于我们的情绪模型，情绪模型是一个顺序模型。我们可以使用 l2 正则化进行微调。
所使用的优化器将是 Adam，因为它比该模型上的其他优化器性能更好。我们也正在导入操作系统模块，使其与 Windows 环境兼容。

我们有 4 类手势，即出拳、胜利、超级
和失败者。
每个图像具有 200 的高度和宽度，并且是
RGB 图像，即三维图像。
我们将使用 128 的 batch_size 进行图像数据扩充。

我们还将为存储的图像指定序列和验证目录。
train_dir 是包含用于训练的图像集的目录。
验证目录是包含验证图像集的目录。

# 数据扩充:

我们将查看手势数据集的图像数据增强，它类似于情绪数据。

ImageDataGenerator 用于图像的数据扩充。我们将复制和制作原始图像转换的副本。Keras 数据生成器将使用副本和
而不是原件。这对每个时期的训练都很有用。

我们将重新缩放图像，并更新所有参数，以适应我们的模型:
1。**重新缩放** =按 1 重新缩放。/255 来归一化每个像素值
2。**旋转 _ 范围** =指定旋转的随机范围
3。 **shear_range** =指定逆时针范围内每个角度的强度。
4。 **zoom_range** =指定缩放范围。
5。 **width_shift_range** =指定扩展的宽度。
6。 **height_shift_range** =指定延伸的高度。7。**水平 _ 翻转** =水平翻转图像。8。 **fill_mode** =根据最近的边界填充。

train _ data gen . flow _ from _ directory 取目录路径&生成批量增广数据。可调用属性如下:
1。**训练目录** =指定我们存储图像数据的目录。
2。 **color_mode** =我们需要指定图像分类方式的重要功能，即灰度或 RGB 格式。默认值为 RGB。
3。 **target_size** =图像的尺寸。
4。 **batch_size** =流程操作的数据批次数。
5。 **class_mode** =确定返回的标签数组的类型。
“分类”将是 2D 一键编码的标签。
6。**无序播放** =无序播放:是否无序播放数据(默认值:True)
如果设置为 False，则按字母数字顺序对数据进行排序。

在下一个代码块中，我们在变量 VGG16_MODEL 中导入 VGG-16 模型，并确保我们输入的模型没有顶层。
使用没有顶层的 VGG16 架构，我们现在可以添加自定义层。为了避免训练 VGG16 层，我们给出下面的命令:
Layers . trainiable = False。我们还将打印出这些层，并确保它们的训练设置为假。

# 手指手势模型:

下面是我们正在构建的手指手势模型自定义层的完整代码

我们正在建立的手指手势模型将通过使用
迁移学习来训练。我们将使用没有顶层的 VGG-16 模型。
我们将在 VGG-16 模型
的顶层添加自定义层，然后我们将使用这个迁移学习模型来预测
手指手势。
定制层由输入层组成，基本上是 VGG-16 模型的
输出。我们添加一个卷积层，它有 32 个滤波器，
kernel_size 为(3，3)，默认步长为(1，1)，我们使用激活
作为 relu，he_normal 作为初始化器。
我们将使用池层对来自
卷积层的层进行下采样。
在样品通过展平
层后，两个完全连接的层作为 relu 激活使用，即
密集结构。
输出层有一个 softmax 激活，num_classes 为 4，
预测 num_classes 的概率，即 Punch、Super、
Victory 和 Loser。
最终模型将输入作为 VGG-16 模型
的起点，输出作为最终输出层。

回调类似于前面的情感模型，所以让我们直接进入手势模型的编译和训练。

# 编译并拟合模型:

我们正在最后一步编译和装配我们的模型。这里，我们正在训练模型并将最佳权重保存到 gesturenew.h5，这样我们就不必重复地重新训练模型，并且可以在需要时使用我们保存的模型。我们正在训练和验证数据。我们使用的损失是 categorical _ crossentropy，它计算标签和预测之间的交叉熵损失。我们将使用的优化器是 Adam，学习率为 0.001，我们将根据指标准确性编译我们的模型。我们将在增强的训练和验证图像上拟合数据。在拟合步骤之后，这些是我们能够在训练和验证损失和准确性上实现的结果。

![](img/d23af11044a0da8b5baff34681b4e5a7.png)

# 图表:

![](img/d93ad51a69ca4d2aa7b6dda7a9601bd8.png)

# 观察:

> 该模型能够非常好地执行。我们可以注意到，训练和验证损失不断减少，训练和验证精度不断提高。深度学习模型中没有过度拟合，我们能够实现超过 95%的验证准确率。

# 奖金:

# 情绪模型-2:

这是我们将要研究的另一个模型。通过这种方法，我们可以用完全相同的模型获得更高的精度。经过一些研究和实验，我发现我们可以通过使用 numpy 阵列中的像素并训练它们来实现更高的精度。有一篇精彩的文章作者使用了类似的方法。我强烈建议用户也去看看那篇文章。这里，我们将把这种方法用于定制的顺序模型，看看我们能够达到什么样的精度。导入与前面的情感模型相似的库。更多信息请参考本文末尾的 GitHub 资源库。下面是为模型准备完整数据的代码块。

num_classes =定义了我们必须预测的类别的数量，即愤怒、恐惧、快乐、中性、惊讶、中性和厌恶。
通过探索性的数据分析我们知道图像的尺寸为:
图像高度= 48 像素
图像宽度= 48 像素
通道数= 1 因为是灰度图像。我们将考虑该型号的批量为 64。

我们将用这种方法将像素转换成一个列表。我们把数据按空格分开，然后把它们作为数组，再把它们整形成 48，48 的形状。我们可以继续扩展维度，然后将标签转换为分类矩阵。

最后，我们将数据分为训练、测试和验证。这种方法与我们以前的模型方法略有不同，在以前的模型方法中，我们只利用训练和验证，因为我们以 80:20 的比例划分数据。这里，我们以 80:10:10 的格式划分数据。我们将使用与我在上一部分中使用的相同的序列模型。让我们再次看看这个模型，看看它在训练后的表现如何。

我们能够对所有 7 种情绪实现的最终准确度、验证准确度、损失和验证损失如下:

![](img/45af4163318daa578e64426b5426960a.png)

## 图表:

![](img/c28198b602620adfc289eb7e5956e49f.png)

# 观察:

> 该模型能够很好地执行。我们可以注意到，训练和验证损失不断减少，而训练和验证准确性不断提高。深度学习模型中没有过度拟合，我们能够实现超过 65%的验证准确性和几乎 70%的准确性，并减少整体损失。

# 录像:

在本节中，我们将制作模特声音反应所需的录音。我们可以为每一个模型和每一种情绪或手势创建定制的录音。在下面的代码块中，我将展示一个分别记录一种情绪和一种手势的例子。

了解导入的库:

1.  Google Text-to-Speech 是一个 python 库，我们可以用它将文本转换成语音翻译响应。
2.  **playsound** =这个模块对于直接从指定路径播放. mp3 格式的声音很有用。
3.  这个模块提供了几个对文件和文件集合的高级操作。具体而言，提供了支持文件复制、移动和移除的功能。

在这个 python 文件中，我们将为情绪和所有手势创建所有需要的语音记录，并将它们存储在 reactions 目录中。我展示了一个例子，展示了如何在代码块中为每种情绪或手势创建定制的语音记录。录音的全部代码将在本文末尾的 GitHub 资源库中发布。

# 最终管道:

我们最终的管道将包括加载我们保存的模型，然后使用它们来预测情绪和手势。我将在 GitHub 库中包含 2 个 python 文件。final_run.py 接受用户的选择，运行情感或手势模型。final_run1.py 同时运行情绪和手势模型。随便用哪个对你们来说更方便。我将使用从第一情绪训练模型和训练手势模型保存的模型。我们将使用一个名为*Haar cascode _ frontal face _ default . XML*的附加 XML 文件来检测人脸。让我们试着从下面的代码块中理解最终管道的代码。

在这个特定的代码块中，我们导入了所有需要的库，我们将使用这些库来获得模型对预测标签的声音响应。 **cv2** 是计算机视觉(open-cv)模块，我们将使用它来实时访问和使用我们的网络摄像头。我们正在导入时间模块，以确保我们仅在 10 秒钟的分析后得到预测。我们加载保存的情绪和手势模型的预训练权重。然后，我们指定用于检测人脸的分类器。然后，我们分配所有可以通过我们的模型预测的情绪和手势标签。

在下一个代码块中，我们将查看情绪模型的代码片段。有关完整的代码，请参考本文末尾的 GitHub 资源库。

在这个选择中，我们将运行情绪模型。当检测到网络摄像头时，我们将读取帧，然后当 haar cascade 分类器检测到人脸时，我们将继续绘制一个矩形框(类似于边界框)。为了更好的预测，我们将面部图像转换成与训练图像相似的 48、48 维的灰度。仅当 np.sum 检测到至少一个面部时，才进行预测。keras 命令 img_to_array 将图像转换为数组维数，如果检测到更多图像，我们将扩展维数。预测是根据标签做出的，录音会相应播放。

让我们看看运行手势模型的代码片段。

在这个选择中，我们将运行手势模型。当检测到网络摄像头时，我们将读取帧，然后在屏幕中间绘制一个矩形框，这与情绪模型不同。用户必须将手指放入所需的框中，以进行以下工作。仅当 np.sum 检测到至少一个手指模型时，才进行预测。keras 命令 img_to_array 将图像转换为数组维数，如果检测到更多图像，我们将扩展维数。预测是根据标签做出的，录音会相应播放。至此，我们的最终流水线完成，我们已经分析了构建人类情感和手势检测器模型所需的所有代码。我们现在可以开始释放视频捕获并销毁所有窗口，这意味着我们可以停止运行计算机视觉模块正在运行的帧。

# 结论:

我们终于完成了对整个人类情感和手势检测器的检查。完整代码的 GitHub 库可以在[这里](https://github.com/Bharath-K3/Human-Emotion-and-Gesture-Detector)找到。我强烈建议尝试各种参数以及我们已经建立的所有 3 个模型中的层，并尝试实现更好的结果。各种记录也可以根据用户的需要进行修改。也有可能尝试各种迁移学习模型或构建您的定制架构，并实现整体更好的性能。尽情体验和尝试不同的独特的模型吧！

# 最终想法:

我非常喜欢写这个 2 部分的系列，它绝对是一个爆炸。我希望你们都像我写这篇文章一样喜欢阅读这篇文章。我期待着在未来发布更多的文章，因为我发现它非常令人愉快。因此，任何关于未来文章的想法或任何你们想让我涵盖的话题都将受到高度赞赏。感谢每一个坚持到最后的人，祝你们度过美好的一天！