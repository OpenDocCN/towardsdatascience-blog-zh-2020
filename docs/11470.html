<html>
<head>
<title>Bayesian Hyper-Parameter Optimization: Neural Networks, TensorFlow, Facies Prediction Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯超参数优化:神经网络、张量流、相预测示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-hyper-parameter-optimization-neural-networks-tensorflow-facies-prediction-example-f9c48d21f795?source=collection_archive---------12-----------------------#2020-08-09">https://towardsdatascience.com/bayesian-hyper-parameter-optimization-neural-networks-tensorflow-facies-prediction-example-f9c48d21f795?source=collection_archive---------12-----------------------#2020-08-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eb4b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">自动调整神经网络的超参数(学习率、密集层和节点数以及激活函数)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/76f1e4cd4b2281689858fdbf0c7e33c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*scVNdagSx-vGV4Roda3o6Q.png"/></div></div></figure><p id="bb53" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这项工作的目的是优化神经网络模型超参数，以根据测井曲线估计相类。我将在本文中包含一些代码，但对于完整的 jupyter 笔记本文件，您可以访问我的<a class="ae lq" href="https://github.com/mardani72/Hyper-Parameter_optimization" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu"><em class="lr">【Github】</em></strong></a>。</p><p id="5f5b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意:如果你是 TensorFlow 的新手，杰夫·希顿会详细介绍它的安装。</p><p id="cbd8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在机器学习中，模型参数可以分为两大类:<br/> <strong class="kw iu"> 1- </strong> <strong class="kw iu">可训练参数</strong>:如训练算法学习到的神经网络中的权值，且用户不干预过程，<br/> <strong class="kw iu"> 2- </strong> <strong class="kw iu">超参数:</strong>用户可以在训练操作前设置，如学习率或模型中的密集层数。<br/>如果您手动尝试，选择最佳超参数可能是一项繁琐的任务，如果您处理两个以上的参数，几乎不可能找到最佳超参数。<br/>一种方法是将每个参数分成一个有效的均匀范围，然后简单地让计算机循环参数组合并计算结果。这个方法叫做<em class="lr">网格搜索</em>。虽然是机器做的，但会是一个耗时的过程。假设您有 3 个超参数，每个超参数有 10 个可能的值。在这种方法中，您将运行 10 个神经网络模型(即使有合理的训练数据集大小，这项任务也是巨大的)。<br/>另一种方法是<em class="lr">随机搜索</em>方法。事实上，它不是使用有组织的参数搜索，而是遍历参数的随机组合，寻找优化的参数。您可能会估计，对于较大的超参数调整，成功的几率会降低到零。</p><p id="d612" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lq" href="https://scikit-optimize.github.io/stable/" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu">Scikit-Optimize</strong></a>，skopt，我们将在这里使用它来完成相估计任务，它是一个简单而高效的库，用于最小化昂贵的噪声黑盒函数。贝叶斯优化构建了另一个参数搜索空间模型。高斯过程就是这些模型中的一种。这将生成模型性能如何随超参数变化而变化的估计。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ls"><img src="../Images/eef56f23c37e5cb81f236e6eaf25cc4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCtNptkfStDL4s0683owbA.png"/></div></div></figure><p id="1dbd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们在图中看到的，真正的目标函数(红色虚线)被噪声(红色阴影)包围。红线显示 scikit optimize 如何对超参数(一维)的搜索空间进行采样。Scikit-optimize 使用高斯过程(绿线)填充样本点之间的区域，并估计真实的真实适合度值。在低样本或缺乏样本的区域(如两个红色样本之间的图片左侧)，存在很大的不确定性(红色和绿色线条之间的巨大差异导致绿色阴影区域的巨大不确定性，如两个标准差的不确定性)。在这个过程中，我们要求一组新的超参数来探索更多的搜索空间。在最初的步骤中，它以稀疏的精度进行，但是在后来的迭代中，它集中在采样点更符合适应度函数和真实目标函数的地方(图中的波谷区域)。<br/>要了解更多信息，您可以参考 Scikit 优化<a class="ae lq" href="https://scikit-optimize.github.io/stable/user_guide.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="ac76" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">资料回顾<br/> </strong>康瑟尔格罗夫气藏位于堪萨斯州。从这个碳酸盐储层中，有九口井可用。从每半英尺的岩心样本中研究岩相，并与井位的测井数据相匹配。特征变量包括来自电缆测井测量的五个变量和来自地质知识的两个地质约束变量。更多详情请参考<a class="ae lq" href="https://github.com/mardani72/Facies-Classification-Machine-Learning/blob/master/Facies_Classification_Various_ML_Final.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a>。对于数据集，您可以从<a class="ae lq" href="https://github.com/mardani72/Facies-Classification-Machine-Learning/blob/master/training_data.csv" rel="noopener ugc nofollow" target="_blank">这里</a>下载。这七个变量是:</p><ol class=""><li id="56ba" class="lt lu it kw b kx ky la lb ld lv lh lw ll lx lp ly lz ma mb bi translated">GR :这种电缆测井工具测量伽马辐射</li><li id="4eb7" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> ILD_log10 </strong>:这是电阻率测量</li><li id="1bdc" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> PE </strong>:光电效应测井</li><li id="e596" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu">δφ</strong>:φ是岩石物理学中的孔隙度指标。</li><li id="a998" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> PNHIND </strong>:中子和密度测井的平均值。</li><li id="357f" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> NM_M </strong>:非海相-海相标志</li><li id="fbb5" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> RELPOS </strong>:相对位置</li></ol><p id="c73c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">九个离散相(岩石类别)是:</p><ol class=""><li id="5266" class="lt lu it kw b kx ky la lb ld lv lh lw ll lx lp ly lz ma mb bi translated"><strong class="kw iu"> (SS) </strong>陆相砂岩</li><li id="8143" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> (CSiS) </strong>非海相粗粉砂岩</li><li id="a03e" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> (FSiS) </strong>非海相粉细砂岩</li><li id="2792" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> (SiSH) </strong>海相粉砂岩和页岩</li><li id="2e8a" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> (MS) </strong>泥岩(石灰岩)</li><li id="b2d9" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> (WS) </strong>瓦克斯通(石灰岩)</li><li id="61b0" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> (D) </strong>白云石</li><li id="9cab" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> (PS) </strong>细粒砂岩(石灰岩)</li><li id="cd7a" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><strong class="kw iu"> (BS) </strong>叶状藻障石(石灰岩)</li></ol><p id="975d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将数据集读入 python 后，我们可以保留一口井的数据作为盲集，以备将来模型性能检查之用。我们还需要将相数转换成数据集中的字符串。参考完整的笔记本。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="2467" class="mm mn it mi b gy mo mp l mq mr">df = pd.read_csv(‘training_data.csv’)<br/><br/>blind = df[df['Well Name'] == 'SHANKLE']<br/>training_data = df[df['Well Name'] != 'SHANKLE']</span></pre><p id="319f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">特征工程<br/> </strong>相类应转换为虚拟变量，以便在神经网络中使用；</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="3455" class="mm mn it mi b gy mo mp l mq mr">dummies = pd.get_dummies(training_data[‘FaciesLabels’]) <br/>Facies_cat = dummies.columns <br/>labels = dummies.values                   <em class="lr"># target matirx</em></span><span id="16d9" class="mm mn it mi b gy ms mp l mq mr"><em class="lr"># select predictors</em> <br/>features = training_data.drop(['Facies', 'Formation', 'Well Name',                                 'Depth','FaciesLabels'], axis=1)</span></pre><h2 id="3b81" class="mm mn it bd mt mu mv dn mw mx my dp mz ld na nb nc lh nd ne nf ll ng nh ni nj bi translated">预处理(制作标准)</h2><p id="b9a7" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">当我们处理各种各样的数据时，为了使网络有效，让我们将它标准化。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="d197" class="mm mn it mi b gy mo mp l mq mr"><strong class="mi iu">from</strong> <strong class="mi iu">sklearn</strong> <strong class="mi iu">import</strong> preprocessing<br/>scaler = preprocessing.StandardScaler().fit(features)<br/>scaled_features = scaler.transform(features)</span><span id="3c58" class="mm mn it mi b gy ms mp l mq mr"><strong class="mi iu">#Data split<br/>from</strong> <strong class="mi iu">sklearn.model_selection</strong> <strong class="mi iu">import</strong> train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>        scaled_features, labels, test_size=0.2, random_state=42)</span></pre><h1 id="e419" class="np mn it bd mt nq nr ns mw nt nu nv mz jz nw ka nc kc nx kd nf kf ny kg ni nz bi translated">超参数</h1><p id="bfaf" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">在这项工作中，我们将使用 Tensorflow 中的深度学习从测井记录中预测相。我们可以为深度学习调整几个超参数。我将尝试找出以下各项的最佳参数:</p><ol class=""><li id="3753" class="lt lu it kw b kx ky la lb ld lv lh lw ll lx lp ly lz ma mb bi translated"><em class="lr">学习率</em></li><li id="0670" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><em class="lr">密集层数</em></li><li id="7aca" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><em class="lr">每层节点数</em></li><li id="3ba8" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated"><em class="lr">哪个激活功能:“relu”或 sigmoid </em></li></ol><p id="d8f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了详细说明这个搜索维度，我们将使用 scikit-optimize(skopt)库。从 skopt 来看，实函数会为学习率定义我们喜欢的范围(下界= 1e-6，上界= 1e-1)，并且会使用对数变换。层数(我们看起来在 1 到 5 之间)和每层的节点数量(在 5 到 512 之间)的搜索维度可以用 skopt 的整数函数来实现。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="a056" class="mm mn it mi b gy mo mp l mq mr">dim_learning_rate = Real(low=1e-6, high=1e-1, prior='log-uniform',<br/>                         name='learning_rate')</span><span id="3c4f" class="mm mn it mi b gy ms mp l mq mr">dim_num_dense_layers = Integer(low=1, high=10, name='num_dense_layers')</span><span id="4279" class="mm mn it mi b gy ms mp l mq mr">dim_num_dense_nodes = Integer(low=5, high=512, name='num_dense_nodes')</span></pre><p id="6ad9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于激活算法，我们应该使用分类函数进行优化。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="9d6c" class="mm mn it mi b gy mo mp l mq mr">dim_activation = Categorical(categories=['relu', 'sigmoid'],<br/>                             name='activation')</span></pre><p id="f2f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将所有搜索维度放在一个列表中:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="bd24" class="mm mn it mi b gy mo mp l mq mr">dimensions = [dim_learning_rate,<br/>              dim_num_dense_layers,<br/>              dim_num_dense_nodes,<br/>              dim_activation]</span></pre><p id="6c03" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你已经为一个特定的项目进行了深度学习，并且手动找到了那个项目的超参数，你就知道优化有多难。您也可以使用自己的猜测(像我一样默认)来比较贝叶斯调优方法的结果。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="beca" class="mm mn it mi b gy mo mp l mq mr">default_parameters = [1e-5, 1, 16, ‘relu’]</span></pre><h1 id="1770" class="np mn it bd mt nq nr ns mw nt nu nv mz jz nw ka nc kc nx kd nf kf ny kg ni nz bi translated">超参数优化</h1><h2 id="b522" class="mm mn it bd mt mu mv dn mw mx my dp mz ld na nb nc lh nd ne nf ll ng nh ni nj bi translated">创建模型</h2><p id="1eb1" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">像 Tneseflow 开发的一些例子一样，我们也需要先定义一个模型函数。在定义了模型的类型(这里是顺序的)之后，我们需要在第一行引入数据维度(数据形状)。层数和激活类型是我们寻求优化的两个超参数。Softmax 激活应用于分类问题。那么另一个超参数是应该在 Adam 函数中定义的学习率。当我们处理分类问题(相预测)时，应考虑损失函数应为“分类 _ 交叉熵”来编译模型。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="1e97" class="mm mn it mi b gy mo mp l mq mr"><strong class="mi iu">def</strong> create_model(learning_rate, num_dense_layers,<br/>                 num_dense_nodes, activation):<br/>    <br/>    model = Sequential()<br/><br/>    model.add(InputLayer(input_shape=(scaled_features.shape[1])))<br/>    <br/>    <strong class="mi iu">for</strong> i <strong class="mi iu">in</strong> range(num_dense_layers):<br/>        name = 'layer_dense_<strong class="mi iu">{0}</strong>'.format(i+1)<br/><br/>        <em class="lr"># add dense layer</em><br/>        model.add(Dense(num_dense_nodes,<br/>                        activation=activation,<br/>                        name=name))<br/><br/>    <em class="lr"># use softmax-activation for classification.</em><br/>    model.add(Dense(labels.shape[1], activation='softmax'))<br/>    <br/>    <em class="lr"># Use the Adam method for training the network.</em><br/>    optimizer = Adam(lr=learning_rate)<br/>    <br/>    <em class="lr">#compile the model so it can be trained.</em><br/>    model.compile(optimizer=optimizer,<br/>                  loss='categorical_crossentropy',<br/>                  metrics=['accuracy'])<br/>    <br/>    <strong class="mi iu">return</strong> model</span></pre><h2 id="2dfd" class="mm mn it bd mt mu mv dn mw mx my dp mz ld na nb nc lh nd ne nf ll ng nh ni nj bi translated">训练和评估模型</h2><p id="84c6" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">该功能旨在使用给定的超参数创建和训练网络，然后使用验证数据集评估模型性能。它返回数据集的适应值，即负分类精度。它是负的，因为 skopt 执行最小化而不是最大化。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="6f6f" class="mm mn it mi b gy mo mp l mq mr">@use_named_args(dimensions=dimensions)<br/><strong class="mi iu">def</strong> fitness(learning_rate, num_dense_layers,<br/>            num_dense_nodes, activation):<br/>    <em class="lr">"""</em><br/><em class="lr">    Hyper-parameters:</em><br/><em class="lr">    learning_rate:     Learning-rate for the optimizer.</em><br/><em class="lr">    num_dense_layers:  Number of dense layers.</em><br/><em class="lr">    num_dense_nodes:   Number of nodes in each dense layer.</em><br/><em class="lr">    activation:        Activation function for all layers.</em><br/><em class="lr">    """</em><br/><br/>    <em class="lr"># Print the hyper-parameters.</em><br/>    print('learning rate: <strong class="mi iu">{0:.1e}</strong>'.format(learning_rate))<br/>    print('num_dense_layers:', num_dense_layers)<br/>    print('num_dense_nodes:', num_dense_nodes)<br/>    print('activation:', activation)<br/>    print()<br/>    <br/>    <em class="lr"># Create the neural network with these hyper-parameters.</em><br/>    model = create_model(learning_rate=learning_rate,<br/>                         num_dense_layers=num_dense_layers,<br/>                         num_dense_nodes=num_dense_nodes,<br/>                         activation=activation)<br/><br/>    <em class="lr"># Dir-name for the TensorBoard log-files.</em><br/>    log_dir = log_dir_name(learning_rate, num_dense_layers,<br/>                           num_dense_nodes, activation)<br/>    <br/>    <em class="lr"># Create a callback-function for Keras which will be</em><br/>    <em class="lr"># run after each epoch has ended during training.</em><br/>    <em class="lr"># This saves the log-files for TensorBoard.</em><br/>    <em class="lr"># Note that there are complications when histogram_freq=1.</em><br/>    <em class="lr"># It might give strange errors and it also does not properly</em><br/>    <em class="lr"># support Keras data-generators for the validation-set.</em><br/>    callback_log = TensorBoard(<br/>        log_dir=log_dir,<br/>        histogram_freq=0,<br/>        write_graph=<strong class="mi iu">True</strong>,<br/>        write_grads=<strong class="mi iu">False</strong>,<br/>        write_images=<strong class="mi iu">False</strong>)<br/>   <br/>    <em class="lr"># Use Keras to train the model.</em><br/>    history = model.fit(x= X_train,<br/>                        y= y_train,<br/>                        epochs=3,<br/>                        batch_size=128,<br/>                        validation_data=validation_data,<br/>                        callbacks=[callback_log])<br/><br/>    <em class="lr"># Get the classification accuracy on the validation-set</em><br/>    <em class="lr"># after the last training-epoch.</em><br/>    accuracy = history.history['val_accuracy'][-1]<br/><br/>    <em class="lr"># Print the classification accuracy.</em><br/>    print()<br/>    print("Accuracy: <strong class="mi iu">{0:.2%}</strong>".format(accuracy))<br/>    print()<br/><br/>    <em class="lr"># Save the model if it improves on the best-found performance.</em><br/>    <em class="lr"># We use the global keyword so we update the variable outside</em><br/>    <em class="lr"># of this function.</em><br/>    <strong class="mi iu">global</strong> best_accuracy<br/><br/>    <em class="lr"># If the classification accuracy of the saved model is improved ...</em><br/>    <strong class="mi iu">if</strong> accuracy &gt; best_accuracy:<br/>        <em class="lr"># Save the new model to harddisk.</em><br/>        model.save(path_best_model)<br/>        <br/>        <em class="lr"># Update the classification accuracy.</em><br/>        best_accuracy = accuracy<br/><br/>    <em class="lr"># Delete the Keras model with these hyper-parameters from memory.</em><br/>    <strong class="mi iu">del</strong> model<br/>    <br/>    <em class="lr"># Clear the Keras session, otherwise it will keep adding new</em><br/>    <em class="lr"># models to the same TensorFlow graph each time we create</em><br/>    <em class="lr"># a model with a different set of hyper-parameters.</em><br/>    K.clear_session()<br/>    <br/>    <em class="lr"># NOTE: Scikit-optimize does minimization so it tries to</em><br/>    <em class="lr"># find a set of hyper-parameters with the LOWEST fitness-value.</em><br/>    <em class="lr"># Because we are interested in the HIGHEST classification</em><br/>    <em class="lr"># accuracy, we need to negate this number so it can be minimized.</em><br/>    <strong class="mi iu">return</strong> -accuracy<br/><em class="lr"># This function exactly comes from :Hvass-Labs, TensorFlow-Tutorials</em></span></pre><p id="6862" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">运行这个:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="6acc" class="mm mn it mi b gy mo mp l mq mr">fitness(x= default_parameters)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/6c1f3e0dcc502b86a20f98d10bf1b4ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eNuV8hyn2qU4ZzVOQSlZw.png"/></div></div></figure><h1 id="186b" class="np mn it bd mt nq nr ns mw nt nu nv mz jz nw ka nc kc nx kd nf kf ny kg ni nz bi translated">运行超参数优化</h1><p id="4d91" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">我们已经检查了默认的超参数性能。现在我们可以检查 scikit-optimize 库中的贝叶斯优化。这里，我们使用 40 次运行作为适应度函数，尽管这是一个昂贵的操作，并且需要在数据集上小心使用。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="8317" class="mm mn it mi b gy mo mp l mq mr">search_result = gp_minimize(func=fitness,<br/>                            dimensions=dimensions,<br/>                            acq_func='EI', <em class="lr"># Expected Improvement.</em><br/>                            n_calls=40,<br/>                            x0=default_parameters)</span></pre><p id="5c1e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">只是一些最后运行显示如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/5b000700c5644a3f6e7f16c8f542803d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hJFsjjiwcSq_HwBcSKP0cQ.png"/></div></div></figure><h1 id="399f" class="np mn it bd mt nq nr ns mw nt nu nv mz jz nw ka nc kc nx kd nf kf ny kg ni nz bi translated">进度可视化</h1><p id="326b" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">使用 skopt 的 plot_convergence 函数，我们可以在 y 轴上看到优化过程和找到的最佳适应值。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="bee9" class="mm mn it mi b gy mo mp l mq mr">plot_convergence(search_result) <br/><em class="lr"># plt.savefig("Converge.png", dpi=400)</em></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/696872f48f74e741da052a82ad71fa77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IO8qAeim4x6nPq0hfw7N1Q.png"/></div></div></figure><h1 id="0954" class="np mn it bd mt nq nr ns mw nt nu nv mz jz nw ka nc kc nx kd nf kf ny kg ni nz bi translated">最佳超参数</h1><p id="ed01" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">使用 serach_result 函数，我们可以看到贝叶斯优化器生成的最佳超参数。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="e762" class="mm mn it mi b gy mo mp l mq mr">search_result.x</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/418c41766ee043ac8176000bd3c6f67e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wgBQMhj1ZRaOQcd-YnPW0w.png"/></div></div></figure><p id="e90e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">优化后的超参数依次为:学习速率、密集层数、每层节点数、最佳激活函数。</p><p id="ce67" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到 40 次呼叫的所有结果，以及相应的超参数和适合度值。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="15e1" class="mm mn it mi b gy mo mp l mq mr">sorted(zip(search_result.func_vals, search_result.x_iters))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/705405313536153350c28501a5dc420d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4RrihfVG6RccWcC85zduig.png"/></div></div></figure><p id="7f24" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有趣的一点是‘relu’激活功能几乎占主导地位。</p><h1 id="73c1" class="np mn it bd mt nq nr ns mw nt nu nv mz jz nw ka nc kc nx kd nf kf ny kg ni nz bi translated">情节</h1><p id="8e78" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">首先，让我们看看两个优化参数的 2D 图。在这里，我们为每层中的学习速率和节点数量的估计适应值制作了一个景观图。贝叶斯优化器建立了一个搜索空间的代理模型，并在这个维度而不是真正的搜索空间内搜索，这就是为什么它更快的原因。在图中，黄色区域较好，蓝色区域较差。黑点是优化器的采样位置，红星是找到的最佳参数。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="c555" class="mm mn it mi b gy mo mp l mq mr"><strong class="mi iu">from</strong> <strong class="mi iu">skopt.plots</strong> <strong class="mi iu">import</strong> plot_objective_2D<br/>fig = plot_objective_2D(result=search_result,<br/>                        dimension_identifier1='learning_rate',<br/>                        dimension_identifier2='num_dense_nodes',<br/>                        levels=50)<br/><em class="lr"># plt.savefig("Lr_numnods.png", dpi=400)</em></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/407eeec0024e6946388b1d306b367f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iMXKw1cIYXmvqWcXdeIMxQ.png"/></div></div></figure><p id="d130" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">点点:</strong></p><ol class=""><li id="48bf" class="lt lu it kw b kx ky la lb ld lv lh lw ll lx lp ly lz ma mb bi translated">代理模型可能是不准确的，因为它仅从对适应度函数的 40 个调用样本中构建</li><li id="c286" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated">由于神经网络中的随机噪声和训练过程，每次重新运行优化时，图可能会发生变化</li><li id="b786" class="lt lu it kw b kx mc la md ld me lh mf ll mg lp ly lz ma mb bi translated">这是 2D 图，而我们优化了 4 个参数，可以想象 4 个维度。</li></ol><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="049d" class="mm mn it mi b gy mo mp l mq mr"><em class="lr"># create a list for plotting</em><br/>dim_names = ['learning_rate', 'num_dense_layers', 'num_dense_nodes', 'activation' ]</span><span id="1648" class="mm mn it mi b gy ms mp l mq mr">fig, ax = plot_objective(result=search_result, dimensions=dim_names)<br/>plt.savefig("all_dimen.png", dpi=400)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/48ae463ed260facf27235fd2c24535c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Wc3pOam3R-wHcx6ee2hWA.png"/></div></div></figure><p id="c617" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这些图中，我们可以看到优化是如何发生的。贝叶斯方法试图在采样密度较高的点用先验信息拟合模型参数。如果学习率约为 0.003，密集层的数量为 6，每层中的节点数量约为 327，并且激活函数为“relu”，则将所有四个参数收集到 scikit 优化方法中将在本次运行中引入最佳结果。</p><h1 id="9aa6" class="np mn it bd mt nq nr ns mw nt nu nv mz jz nw ka nc kc nx kd nf kf ny kg ni nz bi translated">用盲数据优化超参数评估模型</h1><p id="54c1" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">这里也需要同样的数据准备步骤。我们在这里跳过重复。现在我们可以用优化的参数做一个模型来看预测。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="2836" class="mm mn it mi b gy mo mp l mq mr">opt_par = search_result.x<br/><em class="lr"><br/># use hyper-parameters from optimization</em> <br/>learning_rate = opt_par[0]<br/>num_layers = opt_par[1] <br/>num_nodes = opt_par[2] <br/>activation = opt_par[3]</span></pre><p id="a5f4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">创建模型:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="e9bc" class="mm mn it mi b gy mo mp l mq mr"><strong class="mi iu">import</strong> <strong class="mi iu">numpy</strong> <strong class="mi iu">as</strong> <strong class="mi iu">np</strong><br/><strong class="mi iu">import</strong> <strong class="mi iu">tensorflow.keras</strong><br/><strong class="mi iu">from</strong> <strong class="mi iu">tensorflow.keras.models</strong> <strong class="mi iu">import</strong> Sequential<br/><strong class="mi iu">from</strong> <strong class="mi iu">tensorflow.keras.layers</strong> <strong class="mi iu">import</strong> Dense, Activation<br/><strong class="mi iu">from</strong> <strong class="mi iu">tensorflow.keras.callbacks</strong> <strong class="mi iu">import</strong> EarlyStopping<br/><br/>model = Sequential()<br/>model.add(InputLayer(input_shape=(scaled_features.shape[1])))<br/>model.add(Dense(num_nodes, activation=activation, kernel_initializer='random_normal'))<br/>model.add(Dense(labels.shape[1], activation='softmax', kernel_initializer='random_normal'))<br/><br/>optimizer = Adam(lr=learning_rate)<br/>    <br/>model.compile(optimizer=optimizer, loss='categorical_crossentropy',  metrics=['accuracy'])<br/><br/><br/>monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, <br/>                        verbose=1, mode='auto', restore_best_weights=<strong class="mi iu">True</strong>)<br/><br/>histories = model.fit(X_train,y_train, validation_data=(X_test,y_test),<br/>          callbacks=[monitor],verbose=2,epochs=100)</span></pre><p id="939a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看看模型精度的发展:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="63e5" class="mm mn it mi b gy mo mp l mq mr">plt.plot(histories.history['accuracy'], 'bo')<br/>plt.plot(histories.history['val_accuracy'],'b' )<br/>plt.title('Training and validation accuracy')<br/>plt.ylabel('accuracy')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'test'], loc='upper left')<br/>plt.savefig("accu.png", dpi=400)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/2c6ba8c5497791874011dbf09a4c65c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k506-03SUcQnMSCuPX_fOg.png"/></div></div></figure><p id="9653" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">训练和验证精度图显示，几乎在 80%的精度(迭代 10)之后，模型开始过度拟合，因为我们看不到测试数据预测精度的提高。</p><p id="b09a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们用一个尚未见过的数据集(盲井)来评估模型性能。我们总是预测，如果数据集很小或者特征没有大到足以覆盖数据维度的所有复杂性，则机器学习模型使用盲数据进行预测的准确性会低于训练过程。</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="ff42" class="mm mn it mi b gy mo mp l mq mr">result = model.evaluate(scaled_features_blind, labels_blind)<br/>print("<strong class="mi iu">{0}</strong>: <strong class="mi iu">{1:.2%}</strong>".format(model.metrics_names[1], result[1]))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/bdef0d29b4208f429861c5897f8b17ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*d725OnMlvPPS1AVCo3490w.png"/></div></figure><h1 id="3e39" class="np mn it bd mt nq nr ns mw nt nu nv mz jz nw ka nc kc nx kd nf kf ny kg ni nz bi translated">预测盲井数据和绘图</h1><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="656e" class="mm mn it mi b gy mo mp l mq mr">y_pred = model.predict(scaled_features_blind) <em class="lr"># result is probability array</em></span><span id="5be1" class="mm mn it mi b gy ms mp l mq mr">y_pred_idx = np.argmax(y_pred, axis=1) + 1<br/><em class="lr"># +1 becuase facies starts from 1 not zero like index</em></span><span id="fd23" class="mm mn it mi b gy ms mp l mq mr">blind['Pred_Facies']= y_pred_idx</span></pre><p id="460e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">要绘制的函数:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="d14b" class="mm mn it mi b gy mo mp l mq mr"><strong class="mi iu">def</strong> compare_facies_plot(logs, compadre, facies_colors):<br/>    <em class="lr">#make sure logs are sorted by depth</em><br/>    logs = logs.sort_values(by='Depth')<br/>    cmap_facies = colors.ListedColormap(<br/>            facies_colors[0:len(facies_colors)], 'indexed')<br/>    <br/>    ztop=logs.Depth.min(); zbot=logs.Depth.max()<br/>    <br/>    cluster1 = np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)<br/>    cluster2 = np.repeat(np.expand_dims(logs[compadre].values,1), 100, 1)<br/>    <br/>    f, ax = plt.subplots(nrows=1, ncols=7, figsize=(12, 6))<br/>    ax[0].plot(logs.GR, logs.Depth, '-g', alpha=0.8, lw = 0.9)<br/>    ax[1].plot(logs.ILD_log10, logs.Depth, '-b', alpha=0.8, lw = 0.9)<br/>    ax[2].plot(logs.DeltaPHI, logs.Depth, '-k', alpha=0.8, lw = 0.9)<br/>    ax[3].plot(logs.PHIND, logs.Depth, '-r', alpha=0.8, lw = 0.9)<br/>    ax[4].plot(logs.PE, logs.Depth, '-c',  alpha=0.8, lw = 0.9)<br/>    im1 = ax[5].imshow(cluster1, interpolation='none', aspect='auto',<br/>                    cmap=cmap_facies,vmin=1,vmax=9)<br/>    im2 = ax[6].imshow(cluster2, interpolation='none', aspect='auto',<br/>                    cmap=cmap_facies,vmin=1,vmax=9)<br/>    <br/>    divider = make_axes_locatable(ax[6])<br/>    cax = divider.append_axes("right", size="20%", pad=0.05)<br/>    cbar=plt.colorbar(im2, cax=cax)<br/>    cbar.set_label((5*' ').join([' SS ', 'CSiS', 'FSiS', <br/>                                'SiSh', ' MS ', ' WS ', ' D  ', <br/>                                ' PS ', ' BS ']))<br/>    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')<br/>    <br/>    <strong class="mi iu">for</strong> i <strong class="mi iu">in</strong> range(len(ax)-2):<br/>        ax[i].set_ylim(ztop,zbot)<br/>        ax[i].invert_yaxis()<br/>        ax[i].grid()<br/>        ax[i].locator_params(axis='x', nbins=3)<br/>    <br/>    ax[0].set_xlabel("GR")<br/>    ax[0].set_xlim(logs.GR.min(),logs.GR.max())<br/>    ax[1].set_xlabel("ILD_log10")<br/>    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())<br/>    ax[2].set_xlabel("DeltaPHI")<br/>    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())<br/>    ax[3].set_xlabel("PHIND")<br/>    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())<br/>    ax[4].set_xlabel("PE")<br/>    ax[4].set_xlim(logs.PE.min(),logs.PE.max())<br/>    ax[5].set_xlabel('Facies')<br/>    ax[6].set_xlabel(compadre)<br/>    <br/>    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])<br/>    ax[4].set_yticklabels([]); ax[5].set_yticklabels([]); ax[6].set_yticklabels([])<br/>    ax[5].set_xticklabels([])<br/>    ax[6].set_xticklabels([])<br/>    f.suptitle('Well: <strong class="mi iu">%s</strong>'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)</span></pre><p id="f189" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">运行:</p><pre class="kj kk kl km gt mh mi mj mk aw ml bi"><span id="8476" class="mm mn it mi b gy mo mp l mq mr">compare_facies_plot(blind, 'Pred_Facies', facies_colors)<br/>plt.savefig("Compo.png", dpi=400)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/dfa478146cbb376641ded9addc4350e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v0rAF7xy12Ti2z1NUjdz8Q.png"/></div></div></figure><h1 id="2582" class="np mn it bd mt nq nr ns mw nt nu nv mz jz nw ka nc kc nx kd nf kf ny kg ni nz bi translated">结论</h1><p id="dde6" class="pw-post-body-paragraph ku kv it kw b kx nk ju kz la nl jx lc ld nm lf lg lh nn lj lk ll no ln lo lp im bi translated">在这项工作中，我们使用贝叶斯方法和名为 skopt 的 scikit-learn 库优化了超参数。这种方法优于随机搜索和网格搜索，特别是在复杂的数据集中。使用这种方法，我们可以摆脱手动调整神经网络的超参数，尽管在每次运行中，您将面临新的参数。</p></div></div>    
</body>
</html>