# 自然语言处理中的单词和文本表示

> 原文：<https://towardsdatascience.com/word-and-text-representations-in-natural-language-processing-33349649be2e?source=collection_archive---------21----------------------->

## 从本地到分布式

单词是许多自然语言中的基本结构。这些语言中的字符在排列成词之前是没有意义的。当这发生的时候，维奥拉，意义出现了！当单词按顺序排列时，比如在短语或句子中，更多的意思就会出现。

不足为奇的是，核心 NLP 操作包括适当地处理单词或单词序列。(在这篇文章中，一系列的单词，我们将其缩写为 *text* ，将会是一个短语、一个句子、一个段落、一个部分、一个章节或者整个文档。为了符号的简单，我们也将标点符号视为单词。)

下面是支持许多用例的文本关键操作的清单。

```
***Text classification***: Classify text into one of a predefined set of categories. ***Text similarity***: Score how similar two pieces of text (sentences, paras, …, entire documents) are.***Text clustering***: Cluster several texts — such as documents — into groups of similar ones.***Keywords extraction***: Extract salient keywords from a corpus of text (e.g. documents).***Topics discovery***: Discover sets of keywords that go together, i.e. topics, from a corpus of text.
```

所有这些都涉及到文字操作。有些，比如*文本相似度*，涉及到对单词序列的操作。

**当地文字表述**

**按 Id**

在这种情况下，每个不同的单词被分配一个唯一的标识符。举个例子，比方说我们的词典只有 10 个单词:*狗*、*猫*、*虎*、*象*、*黑*、*黑*、*白*、*是*、*大*、*快*。下面描述了该词典的一种本地编码。

```
Word dog cat tiger elephant the black white is are and
Id    0   1    2      3      4    5     6    7  8   9
```

现在考虑正文:**狗和猫是黑色的*。这将被表示为序列 4 0 9 1 8 5。*

*这种表示保留了文本中的所有信息。这很好。另一方面，不同长度的文本产生不同长度的序列，使得它们难以比较。*

***向量空间中的***

*在这种表示中，词典中的每个单词在向量空间中形成它自己的维度。所以在上面的例子中，我们的空间有 10 个维度。单词由二进制向量表示，其中单词的维数值为 1，其余的为 0。我们将把这个向量称为单词的*指示向量*。*

*下面是我们运行的例子中的两个单词指示向量。*

```
*dog   → **1**000000000
black → 00000**1**0000*
```

*好了，现在重点来了。任意长度的文本也被映射为同一空间中的向量。这使得比较不同长度的文本变得容易。只需使用代数相似性度量来比较它们的向量，如余弦相似性或基于欧几里德距离的度量或其他度量。*

***文本向量***

*那么，文本，即任意长度的单词序列，是如何映射到这个空间中的向量的呢？有各种可能性。最简单的方法是将文本中出现的单词的指示向量相加，并进行二进制化(即，将正计数截断为 1)。下面是我们在例子中得到的向量。*

```
*the dog and cat are black → 1100110010*
```

*这个向量只是捕获词典中的哪些单词在文本中，哪些没有。*

```
***1**    **1**    0      0      **1**    **1**     0    0  **1**   0
**dog** **cat** tiger elephant **the** **black** white is **are** and*
```

*这种表示也称为单词包，忽略文本中的所有词序。有时这实际上是有益的。*猫和狗是黑色的* 和*狗和猫是黑色的* 的矢量表示是一样的。这很好，因为猫和狗在这里出现的顺序无关紧要。*

***增强版***

***词频***

*考虑长文本，比如长文档。各种单词出现的频率也很重要。例如，考虑两个 10000 字的文档，一个文档中*医院*被提及 200 次，一个文档中*医院*仅被提及一次。第一份文件中*医院*的出现比第二份文件更为显著。*

*捕获词频的文本编码很容易获得。只需将文本中出现的所有单词的指示向量相加。不要二值化就好。*

***TF X IDF***

*在长文档中，常见的词语如*中的*、*中的*、中的*、*和*会频繁出现。基于频率的表示将过度表示它们的重要性。我们不希望仅仅因为两个文档中都出现了大量的*和*就认为它们是相似的。是的，频率很重要，但仅仅是信息性的词。在我们的例子中，*医院*是一个信息词，而*医院*不是。**

*TF X IDF 表示首先涉及给词典中的每个单词分配一个信息量分数。这个分数是从不同的文本语料库中估计的，例如从不同类型的文档中提取的句子(或者甚至段落)。一个单词出现的文本越少，它的信息量就越大。这就是为什么这一措施被称为“逆文档频率”。*

*有了各种单词的 IDF 分数，很容易获得文本的 TF X IDF 向量。浏览课文中出现的所有单词。将每个出现的指示向量乘以其单词的 IDF 分数。把它们都加起来。这将产生一个向量，其中单词的贡献将取决于它在文档中的频率，该频率由单词的信息性(即 IDF)分数调整。在不同语料库中不常见但在特定文本中频繁出现的单词在该文本中将被认为更重要。*

*让我们看一个例子。考虑这篇课文，狗和猫是黑色的。我们前面看到了它的词频表示:1100110010。*

```
***1**    **1**    0      0      **1**    **1**     0    0  **1**   0
**dog** **cat** tiger elephant **the** **black** white is **are** and*
```

*TF X IDF 表示可能更像这样。(不要把正数看得太字面。)*

```
***1**    **1**    0      0     *0.1*    **1**    0    0  *0.1*   0
**dog** **cat** tiger elephant *the*  **black** white is *are*   and*
```

**与*和*为*的字样影响力大大降低。*

***局部表征无法捕捉单词相似度***

*本地表示无疑非常有用——实际上被广泛使用。也就是说，它们有一个限制，这是更广泛使用的障碍。他们无法解释单词的相似性。*

*考虑三句话:*

```
***S1**: *He is a lawyer* **S2**: *He is a attorney* **S3**: *He is a doctor**
```

*由于*律师*和*律师*是同义词，我们可以看到 S1 和 S2 在语义上是相同的，而 S3 与两者都没有关系。因为*律师*、*律师*和*医生*将是三个不同的维度，所以当地的代表无法做出这种区分。*

*我们可以用两种方法中的一种来缓解这种情况。*

1.  *我们可以用同义词词典来扩充局部表示。*
2.  *我们可以计算维度之间的相关性，并在我们的评分中考虑这些因素。*

*虽然基于字典的方法在实践中通常是有效的，但是它们不能推导出比字典中明确表示的关系更微妙的关系。作为一个简单例子考虑*

```
***S5**: He teaches computer science
**S6**: He teaches data science
**S7**: He teaches art*
```

*我们希望能够认为 S5 和 S6 在语义上比 S6 和 S7 更相似。*

*这个例子只是冰山一角。在外面的数百万个单词中，想想有多少单词相似关系被埋没了！现在把这个扩展到专有名词。举个例子，我们想捕捉到*佳能*和*相机*正相关。*

*维度的相关性分析可以缓解这些问题。从一个大的文档语料库，如维基百科，进一步细分为更小的块，如段落，我们可能能够推断出“计算机科学”和“数据科学”是正相关的，而“计算机科学”和“艺术”是不相关的——甚至可能是负相关的。*

*我们不会描述应用相关性分析来缓解这些问题的无数种方法。相反，我们将关注一种对单词进行替代编码的方法，称为*分布式表示*，它在幕后进行一种形式的相关性分析。*

***分布式单词表示法***

*一个单词的本地表示只有一个值为 1 的位，即标识该单词的位。因此，任何两个字的指示符向量只有两位不同。(参见下面的例子。)也就是说，所有的指示向量对都是等距的。*

```
*dog      **1**000000000
Elephant 000**1**000000*
```

*相反，如果有可能将单词映射到向量，使相似的单词映射到附近的向量，那会怎么样呢？这是有可能的，尽管会涉及更多的问题。首先，我们需要一个关键概念。*

***上下文***

*一个单词的上下文是指在它附近频繁出现的单词，比如在同一个句子中。*

*为什么这个概念对我们有用？因为具有相似上下文表示的两个单词将是正相关的。就拿“猫”和“络腮胡”来说吧。它们将具有相似的语境表征，因为它们都将在涉及猫的语境中共同出现。另一方面，“猫”和“木星”不会出现在相同的上下文中。因此，通过比较上下文表征，我们可以推断第一对是相关的，而第二对是不相关的。*

*好了，现在让我们来形式化这个概念。我们将一个单词的上下文建模为词典中所有单词的概率分布。我们称之为单词的*上下文向量*。这是我们原始局部表示空间中的一个向量。与指示向量不同，它是分布式的。事实上，指示向量是一个特例，它的上下文是单词本身(不多也不少)。*

***举例***

*比方说我们的词典里就有四个词*猫*、*胡须*、*电脑*和*笔记本电脑。*它们的指示向量分别是 1000，0100，0010，0001。没有配对重叠。相比之下，这四个单词的可能上下文向量可能如下。*

```
***word      cat whiskers computer laptop** cat       0.7   0.3       0       0
whiskers  0.3   0.7       0       0
computer  0     0         0.7     0.3
laptop    0     0         0.3     0.7*
```

*我们看到*猫*和*胡须*的上下文向量重叠，就像*电脑*和*笔记本电脑*的上下文向量一样。但是没有那些*络腮胡子*和*电脑。*这话有道理。*

***估计上下文向量***

*既然我们知道了单词的上下文向量是什么，那么我们如何估计它呢？拥有大量的文档会有所帮助。幸运的是，这样的语料库，例如维基百科，现在很容易获得。*

*我们简单地跟踪其他单词与感兴趣的单词在相同的邻近度中的共现，比如在同一个句子中。说出单词*猫*在 100 个句子中出现。在其中的 5 个句子中，单词狗也出现了。单词*狗*在*猫*的上下文中的概率为 5/100。*

***改进上下文向量***

*这种方法有一个问题。特别是常见的单词如*会出现在大多数句子中，所以会出现在不相关单词的上下文中。例如，*猫*的上下文将包括*猫*猫，因为在大多数出现*猫*猫*的句子中*猫也会出现。**

**有一个简单的方法可以缓解这种情况。我们用单词的上下文概率除以它的总概率。形式上是这样的。说*上下文* - *得分* ( *u* ， *v* )表示我们给 *v* 在 *u* 的上下文中的得分。至此我们已经将*上下文* - *分数* ( *u* ， *v* )定义为*P*(*v*|*u*)。而是可以定义为*P*(*v*|*u*)/*P*(*v*)。这里的*P*(*v*|*u*)是包含 *u* 同时也包含 *v* 的句子的分数， *P* ( *v* )是所有包含 *v* 的句子的分数。**

**这里有一个例子。假设*和*出现在 75%的句子和 80%的*和*出现的句子中。所以*在*猫*的上下文中的得分是 80/75。想想*的络腮胡子*。假设它只出现在所有句子的 1%,但是在出现 *cat* 的句子中却有 25%。所以*胡须*在*猫*上下文中的得分是 25/1。***

**简而言之，*胡须*在*猫*的上下文中的得分远高于*猫*的得分。**

****上下文向量将是稀疏的****

**向量空间可能有数百万个维度。然而，上下文向量将趋于稀疏，或者可以变得稀疏而不会丢失太多信息。一个单词的上下文通常只包含词典中数百万个单词中的一小部分。**

****文本向量****

**这和以前一样。在针对各种单词的 IDF 分数进行调整之后，添加文本中单词出现的向量。**

**值得注意的是，根据 IDF 分数进行调整在这里比在本地演示中更重要。这是因为像 *the* 这样的普通单词会有它自己的(特别宽的)上下文，它的无阻尼贡献会污染文本向量。**

**让我们看一个例子。比方说，从一个合适的语料库中，我们获得了以下上下文表示。**

```
****Word                context** bark                howl, dog, tree, wood, leash, shrub
dog                 bark, howl, leash, …**
```

**注意*吠*的上下文抓住了它的两个词义:*树* - *吠*和*狗* - *吠*。**

**现在考虑课文*狗的叫声*。假设 IDF 将显著降低*的两次出现*和*的一次出现*对文本向量的贡献。事实上，为了消除混乱，让我们将这些影响设置为零。我们只剩下:*吠犬*。将这两个词的上下文向量相加，我们会得到这样的结果**

```
****howl**, **dog**, tree, wood, **leash**, shrub**
```

**出现在文本中两个单词的上下文中的单词以粗体突出显示。这些单词将对文本的向量有更大的贡献。**

**现在想象这篇文章是*拉布拉多犬的叫声*。假设我们的语料库足够丰富，可以捕捉到*拉布拉多*的上下文。这个文本的向量也将是相似的。这就让我们推断出*狗的叫声*和*拉布拉多犬的叫声*十分相似。**

****高级单词上下文模型****

**在过去的十年中，一种不同的获取单词上下文向量的方法，名为*单词嵌入*，已经出现并得到广泛使用。它不是通过简单的统计分析获得上下文向量，而是训练一个神经网络来学习单词嵌入。这是在具有隐藏层的架构中完成的。因此，学习迫使网络学习单词良好的潜在特征。单词被嵌入到这个特征空间中，而不是原始的向量空间中。**

**我们在另一篇文章中详细介绍了这种方法，[https://towardsdatascience . com/machine-learned-word-embedding-638 C3 FB 5b 916](/machine-learned-word-embeddings-638c3fb5b916)，因为它的描述涉及到神经网络的细节，相当复杂。**