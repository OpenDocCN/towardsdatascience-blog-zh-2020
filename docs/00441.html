<html>
<head>
<title>Language modelling with Penn Treebank</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Penn Treebank语言建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/language-modelling-with-penn-treebank-64786f641f6?source=collection_archive---------27-----------------------#2020-01-13">https://towardsdatascience.com/language-modelling-with-penn-treebank-64786f641f6?source=collection_archive---------27-----------------------#2020-01-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bb9a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用堆叠LSTMs进行上下文和单词预测——以及深度学习基础设施的比较。</h2></div><p id="21b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a> (RNNs)在历史上是解决顺序问题的理想选择。当数据集中的一个点依赖于其他点时，该数据被称为<em class="lf">序列。</em>一个常见的例子是时间序列，如股票价格或传感器数据，其中每个数据点代表某个时间点的观察结果。</p><p id="c238" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RNN比传统的前馈神经网络更适合于顺序建模，因为它能够通过保持状态或上下文来记住直到给定点所做的分析。这种状态，或者说“记忆”，随着每一个新的输入而重现。</p><p id="3701" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">长短期记忆——解决rnn中的间隙</strong></p><p id="b9bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">需要rnn来跟踪状态，这在计算上是昂贵的。此外，还有训练的问题，如消失梯度和爆炸梯度。因此，RNN，或者准确地说，香草RNN不能很好地学习长序列。解决这些问题的一个流行方法是一种特殊类型的RNN，它被称为<a class="ae le" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">长短期记忆</a> (LSTM)。</p><p id="8488" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LSTM在许多时间步长上保持很强的梯度。这意味着你可以用相对较长的序列来训练LSTM。递归神经网络中的LSTM单元由四个主要元件组成:存储单元和三个逻辑门。</p><p id="9ecd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">存储单元负责保存数据。</em></p><p id="5ac9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">写入、读取和忽略门定义了LSTM内部的数据流。写门负责将数据写入存储单元。</em></p><p id="801d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">读取门从存储单元读取数据，并将该数据发送回递归网络，以及</em></p><p id="57d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">遗忘门，保持或删除信息单元中的数据，或者换句话说，决定要遗忘多少旧信息。</em></p><p id="14e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事实上，这些门是LSTM中的运算，它对网络输入、网络先前的隐藏状态和先前的输出的线性组合执行某种功能</p><p id="e05d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">传统rnn和LSTMs的比较见下图:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/45e10102a903844c389203da3fad0f6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cnKbQrqg2exJ-5fF6ea0tQ.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">传统RNN单元和LSTM单元的比较，具有使LSTM计算成本更低且更鲁棒的门</p></figure><p id="ae9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">自然语言处理(NLP) </strong>是一个经典的序列建模任务:特别是如何编程计算机<em class="lf">处理</em>和分析大量<em class="lf">自然语言</em>数据。NLP的最终目标是以一种有价值的方式阅读、破译、理解和理解人类语言。NLP的常见应用是机器翻译、聊天机器人和个人语音助理，甚至是呼叫中心使用的交互式语音应答。</p><p id="d4f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从历史上看，对于自然语言处理来说，足够大的数据集很难获得。这在一定程度上是因为需要对句子进行分解，并标记一定程度的正确性——否则，在此基础上训练的模型将缺乏有效性。这意味着我们需要<strong class="kk iu">大量的数据，由人类注释或者至少由人类修正</strong>。</p><p id="b185" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">宾夕法尼亚树库，简称为PTB，是由宾夕法尼亚大学维护的数据集。它是巨大的——里面有超过四百八十万条注释文字，全部由人类更正。</p><p id="1605" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据集被分成不同种类的注释，如词类、句法和语义框架。对于这个例子，我们将简单地为我们的模型使用一个干净的、无注释的单词样本(除了一个标签— <code class="fe lw lx ly lz b">&lt;unk&gt;</code>，它用于罕见的单词，比如不常见的专有名词)。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ma"><img src="../Images/91122105ee98a09fe92edb11a394bb29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mYBpctDsdzd_JRrlxvQYPw.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">数据集的示例</p></figure><p id="0d47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">神经网络结构</strong></p><ul class=""><li id="8ba1" class="mb mc it kk b kl km ko kp kr md kv me kz mf ld mg mh mi mj bi translated">在这个网络中，LSTM细胞的数量是2。为了使模型更具表现力，我们可以添加多层LSTMs来处理数据。第一层的输出将成为第二层的输入，以此类推。</li><li id="d98d" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">200个输入单元--&gt; [200x200]权重--&gt; 200个隐藏单元(第一层)-&gt;[200 x200]权重矩阵--&gt; 200个隐藏单元(第二层)-&gt; [200]权重矩阵--&gt; 200个输出单元</li><li id="0c7f" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">假设每个单词由嵌入向量表示，嵌入向量的维数e=200。每个像元的输入层将有200个线性单位。这些e=200个线性单元连接到隐藏层中的h=200个LSTM单元中的每一个(假设只有一个隐藏层，尽管我们的例子有2层)。</li><li id="2883" class="mb mc it kk b kl mk ko ml kr mm kv mn kz mo ld mg mh mi mj bi translated">输入形状是[批量大小，步数]，即[30x20]。嵌入后会变成[30x20x200]，然后是20x[30x200]</li></ul><p id="07ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">隐藏层:</p><ul class=""><li id="fe07" class="mb mc it kk b kl km ko kp kr md kv me kz mf ld mg mh mi mj bi translated">每个LSTM有200个隐藏单元，这相当于嵌入单词和输出的维数。</li></ul><p id="93d7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">代码:</strong><a class="ae le" href="https://github.com/Sunny-ML-DL/natural_language_Penn_Treebank/blob/master/Natural%20language%20processing.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Sunny-ML-DL/Natural _ language _ Penn _ tree bank/blob/master/Natural % 20 language % 20 processing . ipynb</a></p><p id="78a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(改编自PTB培训模块和认知类. ai)</p><p id="bbec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">商品云对比WML-CE </strong></p><p id="6951" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个托管服务的时代，有些人往往会忘记底层计算架构仍然很重要。例如，下面的截图显示了使用A)公共云和b)沃森机器学习-社区版(WML-CE)的同一模型的训练时间</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mp"><img src="../Images/04263328cda73683861bfee5cafe0633.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YLySZXWe21Z5eLRyn7lCkA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">使用商用云—培训时间14848.5秒(4.1小时)</p></figure><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mq"><img src="../Images/8ef8b7acbf0295ae1c55aeac7e5b0d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_2Nhlc2D_biogZWmdl-sxQ.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">使用沃森机器学习-社区版:训练时间317.7秒(5.2分钟)</p></figure><p id="ce87" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">沃森机器学习加速器</strong></p><p id="5626" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个企业机器学习和深度学习平台，具有流行的开源包、最有效的扩展和IBM Power Systems独特架构的优势。看看下面的视频:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="140b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结论:</strong></p><p id="15a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文和相关代码的目的有两个:</p><p id="6997" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">a)演示用于语言和上下文敏感建模的堆叠LSTMs和</p><p id="c96d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">b)底层基础设施对深度学习模型训练效果的非正式演示。</p><p id="2966" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的宝贵时间！</p></div></div>    
</body>
</html>