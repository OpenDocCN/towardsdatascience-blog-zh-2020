<html>
<head>
<title>Introduction to Attention Mechanism in Deep Learning — ELI5 Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的注意机制介绍——Eli 5 Way</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-attention-mechanism-in-deep-learning-eli5-way-d6c2d799ef18?source=collection_archive---------30-----------------------#2020-03-30">https://towardsdatascience.com/introduction-to-attention-mechanism-in-deep-learning-eli5-way-d6c2d799ef18?source=collection_archive---------30-----------------------#2020-03-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="909b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">ELI5项目机器学习</h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/46a2aa7f2f1481a44ff33acc45713bc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Vr4eJPWHtYX2YhLr"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">乔希·拉科瓦尔在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="fe7a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这篇文章中，我们将讨论作为注意力机制发展动力的编码器-解码器模型的一些局限性。之后，我们将讨论注意模型的概念及其在机器翻译中的应用。</p><blockquote class="ln lo lp"><p id="add4" class="kp kq lq kr b ks kt ku kv kw kx ky kz lr lb lc ld ls lf lg lh lt lj lk ll lm im bi translated"><strong class="kr jd">注意:</strong>注意机制是一个稍微高级的话题，需要理解编码器-解码器和长短期记忆模型。请参考我以前关于<a class="ae ko" rel="noopener" target="_blank" href="/introduction-to-encoder-decoder-models-eli5-way-2eef9bbf79cb?source=friends_link&amp;sk=920ca789dc31a1dad72ef334a918c237">编解码器型号</a>和<a class="ae ko" rel="noopener" target="_blank" href="/long-short-term-memory-and-gated-recurrent-units-explained-eli5-way-eff3d44f50dd?source=friends_link&amp;sk=d6fcecd91f04e0c6403181f1ea56d64b"> LSTM </a>的文章</p></blockquote><p id="26bf" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我们讨论注意力模型的概念之前，我们先来回顾一下使用编码器-解码器模型的机器翻译任务。</p><blockquote class="ln lo lp"><p id="105c" class="kp kq lq kr b ks kt ku kv kw kx ky kz lr lb lc ld ls lf lg lh lt lj lk ll lm im bi translated"><strong class="kr jd">引用说明:</strong>本文的内容和结构是基于我对四分之一实验室深度学习讲座的理解——<a class="ae ko" href="https://padhai.onefourthlabs.in/" rel="noopener ugc nofollow" target="_blank">pad hai</a>。</p></blockquote><h1 id="1b13" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">机器翻译—概述</h1><p id="a0cf" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">让我们以使用seq2seq模型将文本从印地语翻译成英语为例，该模型使用编码器-解码器架构。编码器-解码器架构中的底层模型可以是从RNN到LSTM的任何东西。</p><p id="ceca" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在高层次上，encoder只读取整个句子一次，并将来自先前隐藏的表示和先前输入的所有信息编码到一个编码向量中。然后，解码器在每个时间步长使用这种嵌入来产生新的单词。</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/388f499a5e5d577af46400b01787866e.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*i7CyWvy-4Fw20SMrIxXt4Q.png"/></div></figure><p id="0623" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种方法的问题是，编码器只读取整个句子一次，它必须记住所有内容，并将句子转换为编码向量。对于较长的句子，编码器将无法记住序列的开始部分，从而导致信息丢失。</p><blockquote class="nc"><p id="8c1b" class="nd ne it bd nf ng nh ni nj nk nl lm dk translated">人类就是这样翻译一句话的吗？</p></blockquote><p id="8194" class="pw-post-body-paragraph kp kq it kr b ks nm ku kv kw nn ky kz la no lc ld le np lg lh li nq lk ll lm im bi translated">你认为整个输入序列(或句子)在编码过程中的每个时间点都很重要吗？。我们可以特别强调某些单词而不是同等重视所有的单词吗？。注意力机制就是为了应对这些挑战而发展起来的。</p><p id="60db" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们人类试图通过只关注输入中的某些单词来翻译输出中的每个单词。在每个时间步，我们只从长句中提取相关信息，然后翻译特定的单词。<strong class="kr jd"> <em class="lq">理想情况下，在每个时间步，我们应该只将相关信息(相关信息的编码)提供给解码器进行翻译。</em>T3】</strong></p><h1 id="4556" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">注意机制—甲骨文</h1><p id="6058" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">我们如何知道哪些词是重要的，或者我们需要给予更多的关注？。现在，假设我们有一个神谕来告诉我们在给定的时间步长<em class="lq"> t </em>应该关注哪些单词。在神谕的帮助下，我们能否设计出更好的架构，以便将相关信息反馈给解码器？。</p><p id="98f2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，对于每个输入单词，我们分配一个权重<strong class="kr jd"> α </strong>(范围在0-1之间)，表示该单词在时间步长'<strong class="kr jd"> t' </strong>对输出的重要性。例如，<strong class="kr jd"> α </strong> 12表示第一个输入字在第二时间步对输出字的重要性。概括地说，表示法<strong class="kr jd"> α </strong> jt表示在tᵗʰ时间步长与jᵗʰ输入字相关联的权重。</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/f8599e43e22cf4250ca6cdfae8fb3560.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*ON7i-fwXgrOAp1yQGotASg.png"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">注意时间-步骤2</p></figure><p id="92a4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">例如，在时间步长2，我们可以只对相应的单词表示以及权重<strong class="kr jd"> α </strong> jt进行加权平均，并将其送入解码器。在这种情况下，我们不是将完整的编码矢量输入解码器，而是输入单词的加权表示。实际上，我们根据甲骨文给出的权重，对重要的词给予了更多的重视或关注。(感谢甲骨文！)</p><p id="96b8" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">直觉上，这种方法应该比编码器-解码器架构的普通版本更好，因为我们没有用不相关的信息使解码器过载。</p><h1 id="e74f" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">注意力模型</h1><p id="3ede" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">别被骗了，现实中，没有神谕。如果没有神谕，那我们怎么知道重量呢？。</p><blockquote class="ln lo lp"><p id="a6b1" class="kp kq lq kr b ks kt ku kv kw kx ky kz lr lb lc ld ls lf lg lh lt lj lk ll lm im bi translated"><strong class="kr jd">符号:</strong>从现在开始，我们将tᵗʰ时间步的解码器状态称为<strong class="kr jd"> St </strong>，将jᵗʰ时间步的编码器状态称为<strong class="kr jd"> hⱼ </strong>。</p></blockquote><p id="d148" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">必须从数据中学习参数<strong class="kr jd"> α </strong> jt。为了实现这一点，我们定义了一个函数，</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f23210d7a0a11862b84c4d25f321c851.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*4h5CUrsxiRDYfKFq-znGmA.png"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">注意力功能</p></figure><p id="8aca" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">计算中间参数的函数(<strong class="kr jd"> e </strong> jt)有两个参数。让我们讨论一下这些参数是什么。在tᵗʰ时间步长，我们试图找出jᵗʰ字有多重要，因此计算权重的函数应该取决于字本身的矢量表示(即……hⱼ)和直到该特定时间步长的解码器状态(i.e…St-₁).</p><p id="2d75" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">权重<strong class="kr jd"> e </strong> jt捕捉jᵗʰ输入字对于解码tᵗʰ输出字的重要性。使用softmax函数，我们可以归一化这些权重，以获得我们的参数<strong class="kr jd"> α </strong> jt(范围在0-1之间)。</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/a742e9e4409c8e369acd058763ad8234.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*C5sw_Q2UCg7xmuKFv8pM9w.png"/></div></figure><p id="83d7" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">参数<strong class="kr jd"> α </strong> jt表示聚焦在jᵗʰ字上以产生tᵗʰ输出字的概率。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h1 id="8a33" class="lu lv it bd lw lx ob lz ma mb oc md me mf od mh mi mj oe ml mm mn of mp mq mr bi translated">注意力功能</h1><p id="a276" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">在上一节中，我们已经讨论了如何使用一个高级函数来学习参数<strong class="kr jd"> α </strong> jt，该函数带有两个参数——tᵗʰ时间步长之前的解码器状态(St-₁)和单词的矢量表示(hⱼ).使用softmax对该函数的输出进行归一化，以获得<strong class="kr jd"> α </strong> jt。</p><p id="0f0a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本节中，我们将定义<strong class="kr jd"> e </strong> jt的参数形式，以便我们能够从数据中学习该参数。计算<strong class="kr jd"> e </strong> jt最常用的参数形式或函数如下:</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a680f7f929b9cdc2cb529623a8e38b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*LQJDBLGgJNSi3QUEVX0a4g.png"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">注意参数函数</p></figure><p id="48ff" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了学习参数<strong class="kr jd"> e </strong> jt，我们引入了额外的参数Vₐₜₜ、Uₐₜₜ和Wₐₜₜ.其中Uₐₜₜ表示与编码器输入相关的权重，Wₐₜₜ表示与解码器隐藏状态相关的权重，Vₐₜₜ表示与解码器输出相关的权重。这些参数也将与编码器-解码器模型的其他参数一起被学习。</p><h1 id="1d5a" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">机器翻译——注意力机制</h1><p id="6b5d" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">在上一节中，我们能够定义参数函数来学习权重(<strong class="kr jd"> e </strong> jt，在规范化之前)以给予特定单词更多的关注。在本节中，我们将讨论使用注意机制的端到端机器翻译任务。</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/c350bef9672702178a19742e35c43889.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*lwBl36zVQWep_2j3TWlCHw.png"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">机器翻译—注意</p></figure><p id="b8ef" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这项任务中，我们将输入内容从印地语翻译成英语。为简单起见，我们假设RNN的被用作编码器和解码器模型。但是你也可以用LSTM或者GRU的任何变体。</p><h2 id="22b1" class="oi lv it bd lw oj ok dn ma ol om dp me la on oo mi le op oq mm li or os mq iz bi translated">编码器</h2><ul class=""><li id="d305" class="ot ou it kr b ks ms kw mt la ov le ow li ox lm oy oz pa pb bi translated">当我们不加注意地将其与编码器-解码器架构的普通版本进行比较时，编码器的操作没有太大变化。</li><li id="bcd6" class="ot ou it kr b ks pc kw pd la pe le pf li pg lm oy oz pa pb bi translated">在每个时间步长，每个字的表示被计算为前一个时间步长的输出和当前输入以及偏置的函数。</li><li id="c727" class="ot ou it kr b ks pc kw pd la pe le pf li pg lm oy oz pa pb bi translated">最终隐藏状态vector(sₜ)包含来自先前隐藏表示和先前输入的所有编码信息。</li><li id="2d1b" class="ot ou it kr b ks pc kw pd la pe le pf li pg lm oy oz pa pb bi translated">RNN被用作编码器。</li></ul><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/f66c16951777715cbb60a269a9b7d9ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*s8CbdSnnwS4LoCG0Xtyajg.png"/></div></figure><h2 id="a6fa" class="oi lv it bd lw oj ok dn ma ol om dp me la on oo mi le op oq mm li or os mq iz bi translated">解码器</h2><ul class=""><li id="4b6e" class="ot ou it kr b ks ms kw mt la ov le ow li ox lm oy oz pa pb bi translated">在编码器-解码器模型的普通版本中，我们将整个编码向量传递到输出层，输出层解码成下一个可能单词的概率分布。</li><li id="130e" class="ot ou it kr b ks pc kw pd la pe le pf li pg lm oy oz pa pb bi translated">我们不需要传递整个编码向量，而是需要使用我们在上一节中讨论的寻找<strong class="kr jd"> e </strong> jt的有趣等式来寻找注意力权重。然后使用softmax函数归一化<strong class="kr jd"> e </strong> jt权重，得到<strong class="kr jd"> α </strong> jt。</li><li id="76bd" class="ot ou it kr b ks pc kw pd la pe le pf li pg lm oy oz pa pb bi translated">一旦我们有了输入解码器的所有输入和与之相关的权重(感谢这个奇妙的等式！)，我们将计算所有输入和权重的加权组合，以获得合成向量Ct。</li><li id="474b" class="ot ou it kr b ks pc kw pd la pe le pf li pg lm oy oz pa pb bi translated">我们将把加权组合向量Ct馈送给解码器RNN，该解码器解码成下一个可能单词的概率分布。这种解码操作适用于输入中出现的所有时间步长。</li><li id="37d2" class="ot ou it kr b ks pc kw pd la pe le pf li pg lm oy oz pa pb bi translated">输出层是一个softmax函数，它将隐藏状态表示和与之关联的权重以及偏差作为输入。</li></ul><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/bdda13cfc85c9460607211abee11a0e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*e_oa1gvWwsP4UCNDqOMi2w.png"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">不是吓人的功能。真的！！</p></figure><p id="5fb5" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些模型被称为<strong class="kr jd">编码-出席-解码模型</strong>或也称为<strong class="kr jd"> Seq2Seq与注意</strong>。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="3764" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="lq">推荐阅读—</em><strong class="kr jd">ELI5项目机器学习</strong></p><div class="pj pk gp gr pl pm"><a rel="noopener follow" target="_blank" href="/introduction-to-encoder-decoder-models-eli5-way-2eef9bbf79cb"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd jd gy z fp pr fr fs ps fu fw jc bi translated">编码器-解码器模型简介— ELI5路</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">讨论编码器-解码器模型的基本概念及其在一些任务中的应用，如语言建模…</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">towardsdatascience.com</p></div></div><div class="pv l"><div class="pw l px py pz pv qa ki pm"/></div></div></a></div><div class="pj pk gp gr pl pm"><a rel="noopener follow" target="_blank" href="/long-short-term-memory-and-gated-recurrent-units-explained-eli5-way-eff3d44f50dd"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd jd gy z fp pr fr fs ps fu fw jc bi translated">长短期记忆和门控循环单位的解释——Eli 5方式</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">在这篇文章中，我们将学习LSTM和格鲁工作背后的直觉</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">towardsdatascience.com</p></div></div><div class="pv l"><div class="qb l px py pz pv qa ki pm"/></div></div></a></div><h1 id="a3a5" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">结论</h1><p id="dc97" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated">在这篇文章中，我们讨论了机器翻译中编码器-解码器模型的普通版本的一些限制。对于较长的句子，编码器将无法记住序列的开始部分，从而导致信息丢失。之后，我们研究了如何只将相关信息提供给解码器，或者将更多注意力放在有助于在较长序列中保留信息的重要单词上。</p><p id="66d2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">从那里，我们讨论了参数函数，以了解在解码过程中给予某些单词更多重要性所需的注意力权重。最后，我们使用注意机制研究了端到端的机器翻译任务。</p><p id="1775" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我的下一篇文章中，我们将讨论使用Pytorch实现注意力机制。所以确保你在媒体上跟随着我，一旦它掉下来，你就会得到通知。</p><p id="35a9" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">直到那时，和平:)</p><p id="77f0" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">NK。</p><h1 id="f2a2" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">作者简介</h1><p id="8e77" class="pw-post-body-paragraph kp kq it kr b ks ms ku kv kw mt ky kz la mu lc ld le mv lg lh li mw lk ll lm im bi translated"><a class="ae ko" href="https://medium.com/@niranjankumarc" rel="noopener"> Niranjan Kumar </a>是好事达印度公司的高级数据科学顾问。他对深度学习和人工智能充满热情。除了在媒体上写作，他还作为自由数据科学作家为Marktechpost.com写作。点击查看他的文章<a class="ae ko" href="https://www.marktechpost.com/author/niranjan-kumar/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="3ddc" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可以在<a class="ae ko" href="https://www.linkedin.com/in/niranjankumar-c/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与他联系，或者在<a class="ae ko" href="https://twitter.com/Nkumar_283" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注他，了解关于深度学习和机器学习的最新文章。</p><h2 id="4a53" class="oi lv it bd lw oj ok dn ma ol om dp me la on oo mi le op oq mm li or os mq iz bi translated">参考资料:</h2><ul class=""><li id="bd4a" class="ot ou it kr b ks ms kw mt la ov le ow li ox lm oy oz pa pb bi translated"><a class="ae ko" href="https://www.youtube.com/watch?v=yInilk6x-OY&amp;list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT&amp;index=115" rel="noopener ugc nofollow" target="_blank">深度学习(CS7015): Lec 15.3注意机制</a></li></ul></div></div>    
</body>
</html>