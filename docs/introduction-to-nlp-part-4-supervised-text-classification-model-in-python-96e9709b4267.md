# è‡ªç„¶è¯­è¨€å¤„ç†å¯¼è®ºç¬¬ 4 éƒ¨åˆ†:Python ä¸­çš„ç›‘ç£æ–‡æœ¬åˆ†ç±»æ¨¡å‹

> åŸæ–‡ï¼š<https://towardsdatascience.com/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267?source=collection_archive---------23----------------------->

è¿™ç¯‡æ–‡ç« å°†å‘ä½ å±•ç¤ºä¸€ä¸ªæ„å»ºåŸºæœ¬çš„*ç›‘ç£æ–‡æœ¬åˆ†ç±»æ¨¡å‹*çš„ç®€åŒ–ä¾‹å­ã€‚å¦‚æœè¿™å¬èµ·æ¥æœ‰ç‚¹èƒ¡è¨€ä¹±è¯­ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä¸€äº›å®šä¹‰:

> **ğŸ’¡ç›‘ç£:** *æˆ‘ä»¬çŸ¥é“æ ·æœ¬æ•°æ®ä¸­æ¯ä¸ªæ–‡æœ¬çš„æ­£ç¡®è¾“å‡ºç±»* ***ğŸ’¡*** ***æ–‡æœ¬:*** *è¾“å…¥çš„æ•°æ®æ˜¯ä»¥æ–‡æœ¬æ ¼å¼* ***ğŸ’¡åˆ†ç±»æ¨¡å‹:*** *ä½¿ç”¨è¾“å…¥æ•°æ®é¢„æµ‹è¾“å‡ºç±»çš„æ¨¡å‹
> æ¯ä¸ªè¾“å…¥æ–‡æœ¬ä¹Ÿç§°ä¸ºâ€œæ–‡æ¡£â€ï¼Œè¾“å‡ºä¹Ÿç§°ä¸ºâ€œç›®æ ‡â€(æœ¯è¯­ï¼Œä¸æ˜¯å•†åº—ï¼ğŸ˜„).*

*ç›‘ç£æ–‡æœ¬åˆ†ç±»æ¨¡å‹*ç°åœ¨å¬èµ·æ¥æ›´æœ‰æ„ä¹‰å—ï¼Ÿä¹Ÿè®¸å§ï¼Ÿåœ¨æœ‰ç›‘ç£çš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨è¿™ç¯‡æ–‡ç« ä¸­å…³æ³¨ä¸€ç§ç‰¹æ®Šçš„ç±»å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å»ºç«‹ä¸€ä¸ªå—ç›‘ç£çš„æƒ…æ„Ÿåˆ†ç±»å™¨ï¼Œå› ä¸ºæˆ‘ä»¬å°†åœ¨å…·æœ‰äºŒå…ƒç›®æ ‡çš„ç”µå½±è¯„è®ºä¸Šä½¿ç”¨æƒ…æ„Ÿææ€§æ•°æ®ã€‚

![](img/65c4c92224ab9022c58bc10534713214.png)

ç…§ç‰‡ç”± [ClÃ©ment H](https://unsplash.com/@clemhlrdt?utm_source=medium&utm_medium=referral) åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„

# 0.Python è®¾ç½®ğŸ”§

æœ¬æ–‡å‡è®¾æ‚¨å·²ç»è®¿é—®å¹¶ç†Ÿæ‚‰ Pythonï¼ŒåŒ…æ‹¬å®‰è£…åŒ…ã€å®šä¹‰å‡½æ•°å’Œå…¶ä»–åŸºæœ¬ä»»åŠ¡ã€‚å¦‚æœä½ æ˜¯ Python æ–°æ‰‹ï¼Œ[è¿™ä¸ª](https://www.python.org/about/gettingstarted/)æ˜¯å…¥é—¨çš„å¥½åœ°æ–¹ã€‚

æˆ‘å·²ç»ä½¿ç”¨å¹¶æµ‹è¯•äº† Python 3.7.1 ä¸­çš„è„šæœ¬ã€‚åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ç¡®ä¿ä½ æœ‰åˆé€‚çš„å·¥å…·ã€‚

## â¬œï¸ç¡®ä¿å®‰è£…äº†æ‰€éœ€çš„è½¯ä»¶åŒ…:ç†ŠçŒ«ï¼Œnltk & sklearn

æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å¼ºå¤§çš„ç¬¬ä¸‰æ–¹è½¯ä»¶åŒ…:

*   *ç†ŠçŒ«*:æ•°æ®åˆ†æåº“ï¼Œ
*   *nltk:* è‡ªç„¶è¯­è¨€å·¥å…·åŒ…åº“å’Œ
*   *sklearn:* æœºå™¨å­¦ä¹ åº“ã€‚

## â¬œï¸ä» nltk ä¸‹è½½â€œåœç”¨è¯â€ã€â€œwordnetâ€å’Œç”µå½±è¯„è®ºè¯­æ–™åº“

ä¸‹é¢çš„è„šæœ¬å¯ä»¥å¸®åŠ©ä½ ä¸‹è½½è¿™äº›è¯­æ–™åº“ã€‚å¦‚æœæ‚¨å·²ç»ä¸‹è½½äº†ï¼Œè¿è¡Œæ­¤ç¨‹åºå°†é€šçŸ¥æ‚¨å®ƒä»¬æ˜¯æœ€æ–°çš„:

```
import nltk
nltk.download('stopwords') 
nltk.download('wordnet')
nltk.download('movie_reviews')
```

# 1.æ•°æ®å‡†å¤‡ğŸ”¡ â¡ ğŸ”¢

## 1.1.å¯¼å…¥ç¤ºä¾‹æ•°æ®å’ŒåŒ…

é¦–å…ˆï¼Œè®©æˆ‘ä»¬é€šè¿‡å¯¼å…¥æ‰€éœ€çš„åŒ…æ¥å‡†å¤‡ç¯å¢ƒ:

```
import pandas as pdfrom nltk.corpus import movie_reviews, stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizerfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, accuracy_score
```

æˆ‘ä»¬å°†æŠŠ *movie_reviews* æ ‡è®°çš„è¯­æ–™åº“ä» *nltk* è½¬æ¢æˆç†ŠçŒ«æ•°æ®å¸§ï¼Œè„šæœ¬å¦‚ä¸‹:

```
# Script copied from [her](# https://stackoverflow.com/questions/46109166/converting-categorizedplaintextcorpusreader-into-dataframe)e
reviews = []
for fileid in movie_reviews.fileids():
    tag, filename = fileid.split('/')
    reviews.append((tag, movie_reviews.raw(fileid)))
sample = pd.DataFrame(reviews, columns=['target', 'document'])
print(f'Dimensions: {sample.shape}')
sample.head()
```

![](img/b74be989b04a80e10781a95b5f817752.png)

æ‚¨å°†çœ‹åˆ°æ•°æ®æ¡†æ¶æœ‰ä¸¤åˆ—:ä¸€åˆ—ç”¨äºç›®æ ‡ã€ææ€§æƒ…ç»ªï¼Œå¦ä¸€åˆ—ç”¨äº 2000 æ¡è¯„è®ºçš„è¯„è®º(å³æ–‡æ¡£)ã€‚æ¯ä¸ªè¯„è®ºè¦ä¹ˆè¢«æ ‡è®°ä¸ºæ­£é¢è¯„è®ºï¼Œè¦ä¹ˆè¢«æ ‡è®°ä¸ºè´Ÿé¢è¯„è®ºã€‚è®©æˆ‘ä»¬æ£€æŸ¥ç›®æ ‡ç±»çš„æ•°é‡:

```
sample[â€˜targetâ€™].value_counts()
```

![](img/14ba8b636c32d7e86783dbf2ac8d65e9.png)

æ¯ä¸ªç±»(å³â€˜posâ€™ï¼Œâ€˜negâ€™)å„æœ‰ 1000 æ¡è®°å½•ï¼Œå®Œå…¨å¹³è¡¡ã€‚è®©æˆ‘ä»¬ç¡®ä¿è¿™äº›ç±»æ˜¯äºŒè¿›åˆ¶ç¼–ç çš„:

```
sample['target'] = np.where(sample['target']=='pos', 1, 0)
sample['target'].value_counts()
```

![](img/a39f9cde4fc499c754374faf1cd5a474.png)

è¿™çœ‹èµ·æ¥ä¸é”™ï¼Œè®©æˆ‘ä»¬ç»§ç»­åˆ’åˆ†æ•°æ®ã€‚

## 1.2.åˆ†åŒºæ•°æ®

è°ˆåˆ°æ•°æ®åˆ†åŒºï¼Œæˆ‘ä»¬æœ‰ä¸¤ç§é€‰æ‹©:

1.  å°†æ ·æœ¬æ•°æ®åˆ†æˆä¸‰ç»„:*è®­ç»ƒ*ã€*éªŒè¯*å’Œ*æµ‹è¯•*ã€ï¼Œå…¶ä¸­*è®­ç»ƒ*ç”¨äºæ‹Ÿåˆæ¨¡å‹ï¼Œ*éªŒè¯*ç”¨äºè¯„ä¼°è¿‡æ¸¡æ¨¡å‹çš„é€‚åˆåº¦ï¼Œ*æµ‹è¯•*ç”¨äºè¯„ä¼°æœ€ç»ˆæ¨¡å‹çš„é€‚åˆåº¦ã€‚
2.  å°†æ ·æœ¬æ•°æ®åˆ†æˆä¸¤ç»„:*è®­ç»ƒ*å’Œ*æµ‹è¯•*ã€*ã€*å…¶ä¸­*è®­ç»ƒ*è¿›ä¸€æ­¥åˆ†æˆè®­ç»ƒå’ŒéªŒè¯é›† *k æ¬¡*ä½¿ç”¨ *k å€äº¤å‰éªŒè¯ï¼Œ*å’Œ*æµ‹è¯•*ç”¨äºè¯„ä¼°æœ€ç»ˆæ¨¡å‹çš„é€‚åˆåº¦ã€‚ç”¨ *k é‡äº¤å‰éªŒè¯*:
    :**ç¬¬ä¸€ä¸ª** : *åˆ—*è¢«æ‹†åˆ†æˆ k å—ã€‚
    **ç¬¬äºŒ**:å°†æ¨¡å‹æ‹Ÿåˆåˆ°å‰©ä½™çš„ *k-1* ä»¶åï¼Œå–ä¸€ä»¶ä½œä¸ºéªŒè¯é›†ï¼Œè¯„ä¼°ä¸­é—´æ¨¡å‹çš„é€‚åˆåº¦ã€‚
    **ç¬¬ä¸‰**:é‡å¤ç¬¬äºŒæ­¥ *k-1* å¤šæ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„ä»¶ç”¨äºéªŒè¯ç»„ï¼Œå‰©ä½™çš„ä»¶ç”¨äºåºåˆ—ç»„ï¼Œè¿™æ ·*åºåˆ—*çš„æ¯ä»¶ä»…ç”¨ä½œéªŒè¯ç»„ä¸€æ¬¡ã€‚

è¿™é‡Œçš„ä¸­é—´æ¨¡å‹æŒ‡çš„æ˜¯åœ¨æ¯”è¾ƒä¸åŒæœºå™¨å­¦ä¹ åˆ†ç±»å™¨ä»¥åŠä¸ºç»™å®šåˆ†ç±»å™¨å°è¯•ä¸åŒè¶…å‚æ•°ä»¥æ‰¾åˆ°æœ€ä½³æ¨¡å‹çš„è¿­ä»£è¿‡ç¨‹ä¸­åˆ›å»ºçš„æ¨¡å‹ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒä¸ªé€‰é¡¹å¯¹æ ·æœ¬æ•°æ®è¿›è¡Œåˆ†åŒºã€‚è®©æˆ‘ä»¬å…ˆæŠŠä¸€äº›æµ‹è¯•æ•°æ®æ”¾åœ¨ä¸€è¾¹ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æ£€æŸ¥æœ€ç»ˆæ¨¡å‹å¯¹æœªçŸ¥æ•°æ®çš„æ¦‚æ‹¬ç¨‹åº¦ã€‚

```
X_train, X_test, y_train, y_test = train_test_split(sample['document'], sample['target'], test_size=0.3, random_state=123)print(f'Train dimensions: {X_train.shape, y_train.shape}')
print(f'Test dimensions: {X_test.shape, y_test.shape}')# Check out target distribution
print(y_train.value_counts())
print(y_test.value_counts())
```

![](img/f1b96e9ad1d4fa86d737c3d5cfb1c0d0.png)

æˆ‘ä»¬æœ‰ 1400 ä¸ªæ–‡æ¡£åœ¨è®­ç»ƒä¸­ï¼Œ600 ä¸ªæ–‡æ¡£åœ¨æµ‹è¯•æ•°æ®é›†ä¸­ã€‚ç›®æ ‡å‡åŒ€åœ°åˆ†å¸ƒåœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä¸­ã€‚

å¦‚æœä½ å¯¹å…³äºæ•°æ®åˆ†åŒºçš„è¿™ä¸€èŠ‚æœ‰ç‚¹å›°æƒ‘ï¼Œä½ å¯èƒ½æƒ³è¦æŸ¥çœ‹[è¿™ç¯‡å¾ˆæ£’çš„æ–‡ç« ](https://machinelearningmastery.com/difference-test-validation-datasets/)æ¥äº†è§£æ›´å¤šã€‚

## 1.2.é¢„å¤„ç†æ–‡æ¡£

æ˜¯æ—¶å€™å¯¹è®­ç»ƒæ–‡æ¡£è¿›è¡Œé¢„å¤„ç†äº†ï¼Œä¹Ÿå°±æ˜¯æŠŠéç»“æ„åŒ–çš„æ•°æ®è½¬æ¢æˆæ•°å­—çš„çŸ©é˜µã€‚è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ç§ç§°ä¸ºå•è¯åŒ…çš„æ–¹æ³•å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬éƒ½ç”±å®ƒçš„å•è¯è¡¨ç¤ºï¼Œè€Œä¸ç®¡å®ƒä»¬å‡ºç°çš„é¡ºåºæˆ–åµŒå…¥çš„è¯­æ³•ï¼Œæ­¥éª¤å¦‚ä¸‹:

1.  è±¡å¾åŒ–
2.  æ­£å¸¸åŒ–
3.  åˆ é™¤åœç”¨è¯
4.  è®¡æ•°çŸ¢é‡
5.  è½¬æ¢åˆ° tf-idf è¡¨ç¤º

ğŸ”—æˆ‘å·²ç»åœ¨ç³»åˆ—æ–‡ç« çš„ç¬¬ä¸€éƒ¨åˆ†[ä¸­æä¾›äº†é¢„å¤„ç†æ­¥éª¤çš„è¯¦ç»†è§£é‡Šï¼ŒåŒ…æ‹¬ä¸‹é¢ä»£ç å—çš„åˆ†è§£ã€‚](https://medium.com/@zluvsand/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96)

è¿™äº›è¿ç»­çš„æ­¥éª¤æ˜¯é€šè¿‡ä¸‹é¢çš„ä»£ç å—å®Œæˆçš„:

```
def preprocess_text(text):
    # Tokenise words while ignoring punctuation
    tokeniser = RegexpTokenizer(r'\w+')
    tokens = tokeniser.tokenize(text)

    # Lowercase and lemmatise 
    lemmatiser = WordNetLemmatizer()
    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]

    # Remove stop words
    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]
    return keywords# Create an instance of TfidfVectorizer
vectoriser = TfidfVectorizer(analyzer=preprocess_text)# Fit to the data and transform to feature matrix
X_train_tfidf = vectoriser.fit_transform(X_train)
X_train_tfidf.shape
```

![](img/2c0abbaa973ddf48fbd365dc993754d5.png)

ğŸ”—å¦‚æœä½ ä¸ç¡®å®šä»€ä¹ˆæ˜¯ tf-idfï¼Œæˆ‘å·²ç»åœ¨[ç³»åˆ—ç¬¬ä¸‰éƒ¨](https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc)ä¸­æä¾›äº†è¯¦ç»†çš„è§£é‡Šã€‚

ä¸€æ—¦æˆ‘ä»¬é¢„å¤„ç†äº†æ–‡æœ¬ï¼Œæˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ç°åœ¨æ˜¯ä»¥ç¨€ç–çŸ©é˜µæ ¼å¼å­˜å‚¨çš„ 1400 x 27676 ç‰¹å¾çŸ©é˜µã€‚è¿™ç§æ ¼å¼æä¾›äº†æœ‰æ•ˆçš„æ•°æ®å­˜å‚¨ï¼Œå¹¶åŠ å¿«äº†åç»­è¿‡ç¨‹ã€‚æˆ‘ä»¬æœ‰ 27676 ä¸ªç‰¹å¾ä»£è¡¨æ¥è‡ªè®­ç»ƒæ•°æ®é›†ä¸­çš„å”¯ä¸€å•è¯ã€‚ç°åœ¨ï¼Œè®­ç»ƒæ•°æ®å·²ç»å‡†å¤‡å¥½è¿›è¡Œå»ºæ¨¡äº†ï¼

# 2.æ¨¡æ‹Ÿâ“œï¸

## 2.1.åŸºçº¿æ¨¡å‹

è®©æˆ‘ä»¬ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™åˆ†ç±»å™¨å»ºç«‹ä¸€ä¸ªåŸºçº¿æ¨¡å‹ã€‚æˆ‘é€‰æ‹©äº†è¿™ä¸ªåˆ†ç±»å™¨ï¼Œå› ä¸ºå®ƒé€Ÿåº¦å¿«ï¼Œå¹¶ä¸”é€‚ç”¨äºç¨€ç–çŸ©é˜µã€‚ä½¿ç”¨ 5 é‡äº¤å‰éªŒè¯ï¼Œè®©æˆ‘ä»¬å°†æ¨¡å‹ä¸æ•°æ®è¿›è¡Œæ‹Ÿåˆå¹¶å¯¹å…¶è¿›è¡Œè¯„ä¼°:

```
sgd_clf = SGDClassifier(random_state=123)
sgf_clf_scores = cross_val_score(sgd_clf, X_train_tfidf, y_train, cv=5)print(sgf_clf_scores)
print("Accuracy: %0.2f (+/- %0.2f)" % (sgf_clf_scores.mean(), sgf_clf_scores.std() * 2))
```

![](img/0a44298df5e74d4cee3dcf60b2af9d19.png)

å‡è®¾æ•°æ®æ˜¯å®Œå…¨å¹³è¡¡çš„ï¼Œæˆ‘ä»¬å¸Œæœ›å°½å¯èƒ½æ­£ç¡®åœ°é¢„æµ‹ä¸¤ä¸ªæ ‡ç­¾ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨*å‡†ç¡®æ€§*ä½œä¸ºè¯„ä¼°æ¨¡å‹é€‚åˆåº¦çš„åº¦é‡ã€‚ç„¶è€Œï¼Œæ ¹æ®ç›®æ ‡çš„åˆ†å¸ƒå’Œç±»çš„ç›¸å¯¹è¯¯åˆ†ç±»æˆæœ¬ï¼Œç²¾åº¦å¹¶ä¸æ€»æ˜¯æœ€ä½³çš„åº¦é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…¶ä»–è¯„ä¼°æŒ‡æ ‡å¦‚ç²¾ç¡®åº¦ã€å¬å›ç‡æˆ– f1 å¯èƒ½æ›´åˆé€‚ã€‚

æœ€åˆçš„è¡¨ç°çœ‹èµ·æ¥è¿˜ä¸é”™ã€‚åŸºçº¿æ¨¡å‹å¯ä»¥åœ¨å¤§çº¦ 83% +/- 3%çš„æ—¶é—´å†…å‡†ç¡®é¢„æµ‹ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨çš„é»˜è®¤æŒ‡æ ‡æ˜¯`cross_val_score` ä¸­çš„*å‡†ç¡®åº¦*ï¼Œå› æ­¤æˆ‘ä»¬ä¸éœ€è¦æŒ‡å®šå®ƒï¼Œé™¤éæ‚¨æƒ³æ˜ç¡®åœ°è¿™æ ·è¯´ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
cross_val_score(sgd_clf, X_train_tfidf, y_train, cv=5, scoring='accuracy')
```

è®©æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹æ··æ·†çŸ©é˜µæ¥è¿›ä¸€æ­¥ç†è§£è¿™äº›é¢„æµ‹:

```
sgf_clf_pred = cross_val_predict(sgd_clf, X_train_tfidf, y_train, cv=5)
print(confusion_matrix(y_train, sgf_clf_pred))
```

![](img/4138e02b3db5cc4af1e18f51ed4719e0.png)

è¿™ä¸¤ç±»é¢„æµ‹çš„å‡†ç¡®æ€§æ˜¯ç›¸ä¼¼çš„ã€‚

## 2.2.å°è¯•æé«˜æ€§èƒ½

æœ¬èŠ‚çš„ç›®çš„æ˜¯æ‰¾åˆ°æœ€ä½³çš„æœºå™¨å­¦ä¹ ç®—æ³•åŠå…¶è¶…å‚æ•°ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½å¤Ÿé€šè¿‡è°ƒæ•´ä¸€äº›è¶…å‚æ•°æ¥æ”¹è¿›æ¨¡å‹ã€‚æˆ‘ä»¬å°†æŠŠå¤§å¤šæ•°è¶…å‚æ•°ä¿ç•™ä¸ºå…¶åˆç†çš„é»˜è®¤å€¼ã€‚åœ¨ç½‘æ ¼æœç´¢çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢æŒ‡å®šçš„è¶…å‚æ•°çš„æ¯ç§ç»„åˆè¿è¡Œä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶äº¤å‰éªŒè¯ç»“æœï¼Œä»¥æ„Ÿå—å…¶å‡†ç¡®æ€§:

```
grid = {'fit_intercept': [True,False],
        'early_stopping': [True, False],
        'loss' : ['hinge', 'log', 'squared_hinge'],
        'penalty' : ['l2', 'l1', 'none']}
search = GridSearchCV(estimator=sgd_clf, param_grid=grid, cv=5)
search.fit(X_train_tfidf, y_train)
search.best_params_
```

![](img/19f69fb228d4ac43236e984aef677b0d.png)

è¿™äº›æ˜¯ä¸Šé¢æŒ‡å®šçš„è¶…å‚æ•°çš„æœ€ä½³å€¼ã€‚è®©æˆ‘ä»¬ä½¿ç”¨æ‰€é€‰è¶…å‚æ•°çš„è¿™äº›å€¼æ¥è®­ç»ƒå’ŒéªŒè¯æ¨¡å‹:

```
grid_sgd_clf_scores = cross_val_score(search.best_estimator_, X_train_tfidf, y_train, cv=5)
print(grid_sgd_clf_scores)
print("Accuracy: %0.2f (+/- %0.2f)" % (grid_sgd_clf_scores.mean(), grid_sgd_clf_scores.std() * 2))
```

![](img/0ddf2736f62a5bce34e1da60bef5b6fd.png)

ä¸åŸºçº¿ç›¸æ¯”ï¼Œæ¨¡å‹æ‹Ÿåˆåº¦ç¨å¥½(å° yayâ•).

ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œæˆ‘ä»¬å°†ä¸ºæœ€ç»ˆæ¨¡å‹é€‰æ‹©è¿™äº›è¶…å‚æ•°ç»„åˆï¼Œè¿™ä¸€éƒ¨åˆ†åˆ°æ­¤ä¸ºæ­¢ã€‚ç„¶è€Œï¼Œæœ¬èŠ‚å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•ï¼Œå°è¯•ä¸åŒçš„å»ºæ¨¡æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨ç½‘æ ¼æœç´¢æ‰¾åˆ°æ¨¡å‹è¶…å‚æ•°çš„æœ€ä½³å€¼ã€‚

**ğŸ“Œç»ƒä¹ :**çœ‹çœ‹æ˜¯å¦å¯ä»¥é€šè¿‡ä½¿ç”¨ä¸åŒçš„å»ºæ¨¡æŠ€æœ¯å’Œ/æˆ–ä¼˜åŒ–è¶…å‚æ•°æ¥è¿›ä¸€æ­¥æé«˜è¯¥æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

## 2.3.æœ€ç»ˆæ¨¡å‹

ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†æ¨¡å‹ï¼Œè®©æˆ‘ä»¬å°†æ•°æ®è½¬æ¢æ­¥éª¤å’Œæ¨¡å‹æ”¾å…¥*ç®¡é“*:

```
pipe = Pipeline([('vectoriser', vectoriser),
                 ('classifier', search.best_estimator_)])pipe.fit(X_train, y_train)
```

![](img/0a47853a925b069d666e5a2e46e08bbd.png)

åœ¨ä¸Šé¢æ˜¾ç¤ºçš„ä»£ç ä¸­ï¼Œç®¡é“é¦–å…ˆå°†éç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸ºç‰¹å¾çŸ©é˜µï¼Œç„¶åå°†é¢„å¤„ç†åçš„æ•°æ®æ‹Ÿåˆåˆ°æ¨¡å‹ä¸­ã€‚è¿™æ˜¯ä¸€ç§å°†åŸºæœ¬æ­¥éª¤æ”¾åœ¨ä¸€ä¸ªç®¡é“ä¸­çš„ä¼˜é›…æ–¹å¼ã€‚

è®©æˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å°†æŠŠæµ‹è¯•æ•°æ®ä¼ é€’ç»™ç®¡é“ï¼Œç®¡é“å°†é¦–å…ˆé¢„å¤„ç†æ•°æ®ï¼Œç„¶åä½¿ç”¨ä¹‹å‰æ‹Ÿåˆçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹:

```
y_test_pred = pipe.predict(X_test)
print("Accuracy: %0.2f" % (accuracy_score(y_test, y_test_pred)))
print(confusion_matrix(y_test, y_test_pred))
```

![](img/97c446095428f3eca0baf2dd0df35521.png)

æœ€ç»ˆæ¨¡å‹å¯¹æœªçŸ¥æ•°æ®çš„ç²¾ç¡®åº¦çº¦ä¸º 85%ã€‚å¦‚æœè¿™ä¸ªæµ‹è¯•æ•°æ®ä»£è¡¨äº†æœªæ¥çš„æ•°æ®ï¼Œé‚£ä¹ˆè€ƒè™‘åˆ°æˆ‘ä»¬åˆ°ç›®å‰ä¸ºæ­¢æ‰€ä»˜å‡ºçš„åŠªåŠ›ï¼Œè¿™ä¸ªæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›æ˜¯ç›¸å½“ä¸é”™çš„ï¼Œä½ ä¸è¿™æ ·è®¤ä¸ºå—ï¼Ÿä¸ç®¡æ€æ ·ï¼Œæ­å–œä½ ï¼æ‚¨åˆšåˆšæ„å»ºäº†ä¸€ä¸ªç®€å•çš„ç›‘ç£æ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼ğŸ“

![](img/52584d09f7f584bb4d618ba352530031.png)

[æ‘„èœ»èœ“å¤§è¡—](https://unsplash.com/@dragonflyave?utm_source=medium&utm_medium=referral)ä¸Š[çš„ Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

*æ‚¨æƒ³è®¿é—®æ›´å¤šè¿™æ ·çš„å†…å®¹å—ï¼Ÿåª’ä½“ä¼šå‘˜å¯ä»¥æ— é™åˆ¶åœ°è®¿é—®åª’ä½“ä¸Šçš„ä»»ä½•æ–‡ç« ã€‚å¦‚æœæ‚¨ä½¿ç”¨* [*æˆ‘çš„æ¨èé“¾æ¥*](https://zluvsand.medium.com/membership) ï¼Œ*æˆä¸ºä¼šå‘˜ï¼Œæ‚¨çš„ä¸€éƒ¨åˆ†ä¼šè´¹å°†ç›´æ¥ç”¨äºæ”¯æŒæˆ‘ã€‚*

æ„Ÿè°¢æ‚¨èŠ±æ—¶é—´é˜…è¯»è¿™ç¯‡æ–‡ç« ã€‚æˆ‘å¸Œæœ›ä½ ä»é˜…è¯»å®ƒä¸­å­¦åˆ°ä¸€äº›ä¸œè¥¿ã€‚å…¶ä½™å¸–å­çš„é“¾æ¥æ•´ç†å¦‚ä¸‹:
â—¼ï¸ [ç¬¬ä¸€éƒ¨åˆ†:Python ä¸­çš„æ–‡æœ¬é¢„å¤„ç†](https://medium.com/@zluvsand/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96)
â—¼ï¸ [ç¬¬äºŒéƒ¨åˆ†:è¯æ³•åˆ†æå’Œè¯å¹²åˆ†æçš„åŒºåˆ«](https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc)
â—¼ï¸ [ç¬¬ä¸‰éƒ¨åˆ†:TF-IDF è§£é‡Š](https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc)
â—¼ï¸ **ç¬¬å››éƒ¨åˆ†:Python ä¸­çš„æœ‰ç›‘ç£æ–‡æœ¬åˆ†ç±»æ¨¡å‹** â—¼ï¸ [ç¬¬äº”éƒ¨åˆ†:Python ä¸­çš„æ— ç›‘ç£ä¸»é¢˜æ¨¡å‹(sklearn)](/introduction-to-nlp-part-5a-unsupervised-topic-model-in-python-733f76b3dc2d)
â—¼ï¸ [ç¬¬äº”éƒ¨åˆ†](/introduction-to-nlp-part-5b-unsupervised-topic-model-in-python-ab04c186f295)

å¿«ä¹é€ å‹ï¼å†è§ğŸƒğŸ’¨

# 3.å‚è€ƒğŸ“

*   [Christopher D. Manningï¼ŒPrabhakar Raghavan å’Œ Hinrich SchÃ¼tzeï¼Œ*ä¿¡æ¯æ£€ç´¢å¯¼è®º*ï¼Œå‰‘æ¡¥å¤§å­¦å‡ºç‰ˆç¤¾ï¼Œ2008 å¹´](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)
*   [ä¼¯å¾·ã€å²è’‚æ–‡ã€çˆ±å¾·åÂ·æ´›ç€å’Œä¼Šä¸‡Â·å…‹è±æ©ï¼Œ*ç”¨ Python è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†*ã€‚å¥¥è±åˆ©åª’ä½“å…¬å¸ï¼Œ2009 å¹´](http://www.nltk.org/book/)
*   Jason Brownleeï¼Œæµ‹è¯•æ•°æ®é›†å’ŒéªŒè¯æ•°æ®é›†æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿï¼Œæœºå™¨å­¦ä¹ ç²¾é€šï¼Œ2017