<html>
<head>
<title>Linear Regression Algorithm —Under The Hood Math For Non-Mathematicians</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归算法——面向非数学家的隐蔽数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-algorithm-under-the-hood-math-for-non-mathematicians-c228d244e3f3?source=collection_archive---------59-----------------------#2020-08-24">https://towardsdatascience.com/linear-regression-algorithm-under-the-hood-math-for-non-mathematicians-c228d244e3f3?source=collection_archive---------59-----------------------#2020-08-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a176" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" rel="noopener" target="_blank" href="https://towardsdatascience.com/machine-learning/home">内部 AI </a></h2><div class=""/><div class=""><h2 id="ee1a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">它是机器学习中借用的数学和统计学领域最古老的算法之一。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/8b10e5ce6a9d33183ddede8060f6e274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Xz8RO7a8Y5z2O-hf"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@antoine1003?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安托万·道特里</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="a855" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在计算机出现之前，线性回归是在不同领域中使用的最流行的算法之一。今天有了强大的计算机，我们可以解决多维线性回归，这在以前是不可能的。在一元或多维线性回归中，基本的数学概念是相同的。</p><p id="5a77" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">今天有了机器学习库，像<strong class="lk jd"> <em class="me"> Scikit </em> -learn </strong>，就可以在建模中使用线性回归，而不需要理解它背后的数学概念。在我看来，对于一个数据科学家和机器学习专业人士来说，在使用算法之前，了解算法背后的数学概念和逻辑是非常必要的。</p><p id="fb7d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们大多数人可能没有学习过高等数学和统计学，看到算法背后的数学符号和术语时，我们会感到害怕。在本文中，我将用简化的 python 代码和简单的数学来解释线性回归背后的数学和逻辑，以帮助您理解</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="ad54" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="me">概述</em> </strong></p><p id="d3e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将从一个简单的一元线性方程开始，没有任何截距/偏差。首先，我们将学习像<strong class="lk jd"> <em class="me"> Scikit-learn </em> </strong>这样的包所采取的逐步解决线性回归的方法。在本演练中，我们将理解梯度下降的重要概念。此外，我们将看到一个带有一个变量和截距/偏差的简单线性方程的示例。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="9744" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">步骤 1: </em>我们将使用 python 包 NumPy 来处理样本数据集，并使用 Matplotlib 来绘制各种可视化图形。</p><pre class="ks kt ku kv gt mm mn mo mp aw mq bi"><span id="9ac9" class="mr ms it mn b gy mt mu l mv mw">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span></pre><p id="9f8d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第二步:让我们考虑一个简单的场景，其中单个输入/自变量控制结果/因变量的值。在下面的代码中，我们声明了两个 NumPy 数组来保存自变量和因变量的值。</p><pre class="ks kt ku kv gt mm mn mo mp aw mq bi"><span id="0e61" class="mr ms it mn b gy mt mu l mv mw">Independent_Variable=np.array([1,2,3,12,15,17,20,21,5,7,9,10,3,12,15,17,20,7])<br/>Dependent_Variable=np.array([7,14,21,84,105,116.1,139,144.15,32.6,50.1,65.4,75.4,20.8,83.4,103.15,110.9,136.6,48.7])</span></pre><p id="5643" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">第三步:</em>让我们快速画一个散点图来了解数据点。</p><pre class="ks kt ku kv gt mm mn mo mp aw mq bi"><span id="b631" class="mr ms it mn b gy mt mu l mv mw">plt.scatter(Independent_Variable, Dependent_Variable,  color='green')<br/>plt.xlabel('Independent Variable/Input Parameter')<br/>plt.ylabel('Dependent Variable/ Output Parameter')<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mx"><img src="../Images/161b1b35128b80b8e63e2c0147bb7ce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6WTGrkJiTM85MaIHzHS8vA.png"/></div></div></figure><p id="8d47" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的目标是制定一个线性方程，它可以预测自变量/输入变量的因变量值，误差最小。</p><p id="7094" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因变量=常数*自变量</p><p id="edf5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用数学术语来说，Y =常数*X</p><p id="23c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就可视化而言，我们需要找到最佳拟合线，以使点的误差最小。</p><p id="94b4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在机器学习领域，最小误差也被称为损失函数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi my"><img src="../Images/a6a13f05068f7d75c9c0df499da475e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mjalW4XlIsHdE5gZcz2VWQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">损失函数公式(作者用 word 写的然后截图)</p></figure><p id="694e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以用所有独立的数据点计算方程 Y =常数*X 中每个假定常数值的迭代损失。目标是找到损耗最小的常数，并建立方程。请注意，在损失函数等式中,“m”代表点数。在当前示例中，我们有 18 个点，因此 1/2m 转化为 1/36。不要被损失函数公式吓到。我们将损失计算为每个数据点的计算值和实际值之差的平方和，然后除以两倍的点数。我们将在下面的文章中借助 python 中的代码一步一步地破译它。</p><p id="1354" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">步骤 4: </em>为了理解识别方程背后的核心思想和数学，我们将考虑下面代码中提到的有限的一组常数值，并计算损失函数。</p><p id="2b50" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在实际的线性回归算法中，特别是间隙，常数被考虑用于损失函数计算。最初，考虑用于损失函数计算的两个常数之间的差距较大。随着我们越来越接近实际的解决方案常数，考虑更小的差距。在机器学习的世界中，学习率是损失函数计算中常数增加/减少的差距。</p><pre class="ks kt ku kv gt mm mn mo mp aw mq bi"><span id="9991" class="mr ms it mn b gy mt mu l mv mw">m=[-5,-3,-1,1,3,5,6.6,7,8.5,9,11,13,15]</span></pre><p id="1b41" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">步骤 5: </em>在下面的代码中，我们计算所有输入和输出数据点的每个常量值(即在前面步骤中声明的列表 m 中的值)的损失函数。</p><p id="1a52" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将每个常数的计算损失存储在一个 Numpy 数组“errormargin”中。</p><pre class="ks kt ku kv gt mm mn mo mp aw mq bi"><span id="7ff7" class="mr ms it mn b gy mt mu l mv mw">errormargin=np.array([])<br/>for slope in m:<br/>    counter=0<br/>    sumerror=0<br/>    cost=sumerror/10<br/>    for x in Independent_Variable:<br/>        yhat=slope*x<br/>        error=(yhat-Dependent_Variable[counter])*(yhat-Dependent_Variable[counter])<br/>        sumerror=error+sumerror<br/>        counter=counter+1  <br/>    cost=sumerror/18<br/>    errormargin=np.append(errormargin,cost)</span></pre><p id="6ccc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">第六步:</em>我们将绘制常量的计算损失函数，以确定实际常量值。</p><pre class="ks kt ku kv gt mm mn mo mp aw mq bi"><span id="7a0b" class="mr ms it mn b gy mt mu l mv mw">plt.plot(m,errormargin)<br/>plt.xlabel("Slope Values")<br/>plt.ylabel("Loss Function")<br/>plt.show()</span></pre><p id="dcf4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">曲线在最低点的常数的值是真正的常数，我们可以用它来建立直线的方程。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mz"><img src="../Images/6d0ff19588e15a0e3d7778ac282e938c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fRKtl78Ym5bg5ZMv8FB1Lg.png"/></div></div></figure><p id="2f0b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们的例子中，对于常数 6.8 的值，曲线处于最低点。</p><p id="b36c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该值为 Y=6.8*X 的直线可以以最小的误差最好地拟合数据点。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi na"><img src="../Images/996f80cf4338ee12ff62284a7d63d5d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTZ8zYspomqxdlxngAU4sw.png"/></div></div></figure><p id="9034" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种绘制损失函数并在损失曲线的最低点识别方程中固定参数的真实值的方法被称为<strong class="lk jd"> <em class="me">梯度下降</em> </strong>。作为一个例子，为了简单起见，我们考虑了一个变量，因此损失函数是一个二维曲线。在多元线性回归的情况下，梯度下降曲线将是多维的。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="8a8d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们已经学习了计算自变量系数的内功。接下来，让我们一步一步地学习线性回归中计算系数和截距/偏差的方法。</p><p id="471e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">第一步:</em>和前面一样，让我们考虑一组自变量和因变量的样本值。这些是可用的输入和输出数据点。我们的目标是制定一个线性方程，它可以预测自变量/输入变量的因变量值，误差最小。</p><p id="99a6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因变量=(系数*自变量)+常数</p><p id="6f42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用数学术语来说，y=(系数*x)+ c</p><p id="a34a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，系数也是一个常数项乘以方程中的自变量。</p><pre class="ks kt ku kv gt mm mn mo mp aw mq bi"><span id="98bb" class="mr ms it mn b gy mt mu l mv mw">Independent_Variable=np.array([1,2,4,3,5])<br/>Dependent_Variable=np.array([1,3,3,2,5])</span></pre><p id="8736" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">第二步:</em>我们将假设系数的初始值和常数“m”和“c”分别为零。我们将在误差计算的每一次迭代之后以 0.001 的小学习率增加 m 和 c 的值。Epoch 是我们希望在整个可用数据点上进行这种计算的次数。随着历元数量的增加，解会变得更加精确，但这会消耗时间和计算能力。基于业务案例，我们可以决定计算值中可接受的误差，以停止迭代。</p><pre class="ks kt ku kv gt mm mn mo mp aw mq bi"><span id="b384" class="mr ms it mn b gy mt mu l mv mw">LR=0.001<br/>m=0<br/>c=0<br/>epoch=0</span></pre><p id="6719" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">步骤 3: </em>在下面的代码中，我们在可用的数据集上运行 1100 次迭代，并计算系数和常数值。</p><p id="f417" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于每个独立的数据点，我们计算相关值(即 yhat)，然后计算计算的和实际的相关值之间的误差。</p><p id="3f00" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">基于该误差，我们改变系数和常数的值用于下一次迭代计算。</p><p id="63c6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">新系数=当前系数—(学习率*误差)</p><p id="7a5a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">新常数=当前常数-(学习率*误差*独立变量值)</p><pre class="ks kt ku kv gt mm mn mo mp aw mq bi"><span id="cd03" class="mr ms it mn b gy mt mu l mv mw">while epoch&lt;1100:<br/>    epoch=epoch+1<br/>    counter=0<br/>    for x in Independent_Variable:<br/>        yhat=(m*x)+c<br/>        error=yhat-Dependent_Variable[counter]<br/>        c=c-(LR*error)<br/>        m=m-(LR*error*x)<br/>        counter=counter+1</span></pre><p id="9ef4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在对可用数据集进行 1100 次迭代后，我们检查系数和常数的值。</p><pre class="ks kt ku kv gt mm mn mo mp aw mq bi"><span id="572f" class="mr ms it mn b gy mt mu l mv mw">print("The final value of  m", m)<br/>print("The final value of  c", c)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/ddf7ac2f44804a01ef399adca3283715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*99EmaAK_Ptn1_wpDxx6LLg.png"/></div></div></figure><p id="b528" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数学上可以表示为 y=(0.81*x)+0.33</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/b37c3cbd7f5716601ae90bd7771826ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9g-FP6jxnsRz3k4snK7ZtA.png"/></div></div></figure><p id="6a84" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，让我们将之前的输出与<strong class="lk jd"> <em class="me"> Scikit </em> -learn </strong>线性回归算法的结果进行比较</p><pre class="ks kt ku kv gt mm mn mo mp aw mq bi"><span id="a250" class="mr ms it mn b gy mt mu l mv mw">from sklearn.linear_model import LinearRegression<br/>reg = LinearRegression().fit(Independent_Variable.reshape(-1,1), Dependent_Variable)<br/>print(reg.coef_)<br/>print(reg.intercept_)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/bf6b95436020c1e34411ba27b18b64de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QXWoFgntBeWtKU78MJ-7qw.png"/></div></div></figure><p id="737f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过对可用数据集进行 1100 次迭代，系数和常数/偏差的计算值非常接近于<strong class="lk jd"> <em class="me"> Scikit </em> -learn </strong>线性回归算法的输出。</p><p id="41b0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望这篇文章能让你对线性回归的幕后数学计算和概念有一个明确的理解。此外，我们已经看到了梯度下降被应用于寻找最优解的方式。在多元线性回归的情况下，数学和逻辑保持不变，只是在更多的维度上进一步扩展。</p></div></div>    
</body>
</html>