<html>
<head>
<title>Interpret Principal Component Analysis (PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释主成分分析(PCA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpret-principal-component-analysis-pca-b8b4a4f22ece?source=collection_archive---------10-----------------------#2020-03-11">https://towardsdatascience.com/interpret-principal-component-analysis-pca-b8b4a4f22ece?source=collection_archive---------10-----------------------#2020-03-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/e6055d392e43261a6d2d8603c8708c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3gs5do9UppetvHhY"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">故事。若昂·布兰科在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4c11" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据可以告诉我们故事。反正我是这么听说的。作为一名为财富 300 强客户工作的数据科学家，我每天都要处理大量的数据，我可以告诉你，数据<strong class="ki iu"> <em class="le">可以</em> </strong>告诉我们故事。您可以对数据应用回归、分类或聚类算法，但特征选择和工程可能是一项艰巨的任务。很多时候，我看到数据科学家采用自动方法进行特征选择，如递归特征消除(RFE)或使用随机森林或 XGBoost 利用特征重要性算法。所有这些都是很好的方法，但可能不是获取所有数据“精华”的最佳方法。</p><h1 id="9b16" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">理解 PCA 的细微差别</h1><h2 id="54c3" class="md lg it bd lh me mf dn ll mg mh dp lp kr mi mj lt kv mk ml lx kz mm mn mb mo bi translated">主成分分析的直觉</h2><p id="6749" class="pw-post-body-paragraph kg kh it ki b kj mp kl km kn mq kp kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">如果我们有两列代表 X 和 Y 列，你可以用 2D 轴来表示。假设我们添加了另一个维度，即 Z 轴，现在我们有了一个称为超平面的东西来表示这个 3D 空间中的空间。<br/>现在，包含 n 维的数据集也无法可视化。</p><figure class="mv mw mx my gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/12332150e12314191377b30a6fb6a629.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*9aPNQpec91vY47jdNOXJQg.gif"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">重新对齐轴以捕捉数据中的最大差异</p></figure><p id="beae" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PCA 的思想是在 n 维空间中重新排列轴，这样我们可以捕获数据中的大部分差异。在行业中，没有太大差异的特征被丢弃，因为它们对任何机器学习模型没有太大贡献。这些代表数据中大部分方差的新轴被称为主成分。</p><figure class="mv mw mx my gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/779cf56ca923ad7bdc2771c164651dce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tr78D84H5e0I9LEl"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">层次。Alexander Schimmeck 在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="9b66" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用主成分的原因是为了处理相关的预测值(多重共线性)以及在二维空间中可视化数据。</p><p id="179b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PCA 是一种统计过程，用于将可能相关的特征的观察结果转换成主成分，使得:</p><ul class=""><li id="529a" class="na nb it ki b kj kk kn ko kr nc kv nd kz ne ld nf ng nh ni bi translated">它们彼此不相关</li><li id="4281" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">它们是原始变量的线性组合</li><li id="5d9c" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">它们有助于获取数据集中的最大信息</li></ul><p id="c826" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PCA 是数据中基础的<a class="ae kf" href="https://eli.thegreenplace.net/2015/change-of-basis-in-linear-algebra/" rel="noopener ugc nofollow" target="_blank">变化。</a></p><h2 id="bc5d" class="md lg it bd lh me mf dn ll mg mh dp lp kr mi mj lt kv mk ml lx kz mm mn mb mo bi translated">主成分分析的方差</h2><p id="7460" class="pw-post-body-paragraph kg kh it ki b kj mp kl km kn mq kp kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">如果一个列的方差越小，它包含的信息就越少。PCA 以这样的方式改变基，使得新的基向量捕获最大方差或信息。这些新的基向量被称为主分量。</p><figure class="mv mw mx my gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/9f3eef1a9afa9d3b8c7b2485307b1d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Fhx-Z_ZbLNz-yLlv"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">乔恩·泰森在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="e1ac" class="md lg it bd lh me mf dn ll mg mh dp lp kr mi mj lt kv mk ml lx kz mm mn mb mo bi translated">主成分分析作为一种降维技术</h2><p id="e7b0" class="pw-post-body-paragraph kg kh it ki b kj mp kl km kn mq kp kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">想象一下很多数据科学家面临的情况。您已经收到数据，执行了数据清理、缺失值分析和数据插补。现在，您继续进一步分析数据，注意分类列，并通过创建虚拟变量对数据执行一次性编码。现在，我们进行特征工程，制造更多的特征。我有过这样的经历，这导致了超过 500 个，有时是 1000 个特性。</p><div class="np nq gp gr nr ns"><a rel="noopener follow" target="_blank" href="/running-jupyter-notebook-on-the-cloud-in-15-mins-azure-79b7797e4ef6"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">在 15 分钟内在云上运行 Jupyter 笔记本电脑#Azure</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">文章做到了标题所说的。在 Azure 笔记本电脑(免费或付费)上运行 Jupyter Notebook，其成本仅为……</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">towardsdatascience.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og jz ns"/></div></div></a></div><p id="8327" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我应该如何将如此多的特征输入到一个模型中，或者我应该如何知道重要的特征？如果我们继续使用递归特征消除或特征重要性，我将能够选择对预期输出贡献最大的列。然而，如果我们错过了一个对模型有更大贡献的特性呢？模型迭代的过程容易出错且繁琐。PCA 是我们可以利用的一种替代方法。</p><p id="94ea" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主成分分析是一种经典的降维技术，用于捕捉数据的本质。它可以用来捕捉超过 90%的数据方差。</p><p id="2141" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">注</em>:方差不表示列间关系或变量间的相关性。我们对协方差矩阵进行对角化，以获得如下基向量:</p><ul class=""><li id="49c4" class="na nb it ki b kj kk kn ko kr nc kv nd kz ne ld nf ng nh ni bi translated">无关联的</li><li id="3539" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">线性相关</li><li id="3c0c" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">解释最大方差的方向</li></ul><p id="0101" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PCA 的算法寻求找到对角化协方差矩阵的新的基向量。这是使用特征分解完成的。</p><figure class="mv mw mx my gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/1497639cac94e2a68f8c3cfa80fb2e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YihJQZ4nibIoykYV"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://unsplash.com/@luiztrix?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">路易斯·费利佩</a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="17ea" class="md lg it bd lh me mf dn ll mg mh dp lp kr mi mj lt kv mk ml lx kz mm mn mb mo bi translated">主成分分析算法</h2><ol class=""><li id="e610" class="na nb it ki b kj mp kn mq kr oi kv oj kz ok ld ol ng nh ni bi translated">将数据集中的所有信息表示为协方差矩阵。</li><li id="a3fe" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld ol ng nh ni bi translated">对协方差矩阵执行特征分解。</li><li id="51d7" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld ol ng nh ni bi translated">新的基础是在步骤 I 中获得的协方差矩阵的特征向量</li><li id="9c62" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld ol ng nh ni bi translated">在新的基础上表示数据。新的基也被称为主成分。</li></ol><figure class="mv mw mx my gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/bd8c01ae6ca56d0f1966ab8b1592d389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QUOj_W3-DPeW7oBRgq2Iig.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">我在 draw.io 中制作的 PCA 流</p></figure><h2 id="13fb" class="md lg it bd lh me mf dn ll mg mh dp lp kr mi mj lt kv mk ml lx kz mm mn mb mo bi translated">密码</h2><p id="e3e9" class="pw-post-body-paragraph kg kh it ki b kj mp kl km kn mq kp kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">现在，我在这里写的文章如果没有实际的编码经验是写不出来的。我相信你的代码应该在它该在的地方，不是在<a class="ae kf" href="https://medium.com/@anishmahapatra" rel="noopener">介质</a>上，而是在 GitHub 上。</p><div class="np nq gp gr nr ns"><a href="https://github.com/anishmahapatra01/masters-machine-learning-1/tree/master/04DimensionalityReduction-pca/pca-help-assignment/Anish-PCA-and-Clustering-Assignment" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">anishmahapatra 01/masters-机器学习-1</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">github.com</p></div></div><div class="ob l"><div class="on l od oe of ob og jz ns"/></div></div></a></div><p id="fecf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我已经列出了注释代码和一个使用 PCA 的样本聚类问题，以及帮助您入门的必要步骤。</p><p id="e4ef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">逻辑步骤详述如下:</p><ul class=""><li id="860b" class="na nb it ki b kj kk kn ko kr nc kv nd kz ne ld nf ng nh ni bi translated">缺失值和异常值分析完成后，标准化/规范化数据以帮助模型更好地收敛</li><li id="e109" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">我们使用 sklearn 的 PCA 包对数字和虚拟特征进行 PCA</li><li id="4a83" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">使用 pca.components_ 查看生成的 pca 成分</li><li id="5687" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">使用 PCA.explained_variance_ratio_ 了解数据解释的差异百分比</li><li id="58aa" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">Scree 图用于了解需要使用多少个主成分来获取数据中的期望方差</li><li id="f0db" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">运行机器学习模型以获得期望的结果</li></ul><figure class="mv mw mx my gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oo"><img src="../Images/c561d7c554b3367346f9a3d082d59549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MUAfnkyyXt1-kv0u"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://unsplash.com/@whitfieldjordan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔丹·怀特菲尔德</a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class="np nq gp gr nr ns"><a rel="noopener follow" target="_blank" href="/interpret-linear-regression-in-10-mins-non-technical-3f78f1f1dbd1"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">在 10 分钟内解释线性回归(非技术性)</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">当有那么多伟大的文章和对最常见算法的解释时，为什么还要花力气…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">towardsdatascience.com</p></div></div><div class="ob l"><div class="op l od oe of ob og jz ns"/></div></div></a></div><h1 id="cbcb" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">结论:</h1><p id="a156" class="pw-post-body-paragraph kg kh it ki b kj mp kl km kn mq kp kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">恭喜你！如果你已经设法达到文章的这个阶段，你是令人敬畏的。起初，主成分分析似乎令人望而生畏，但是，随着你学会将它应用于更多的模型，你将能够更好地理解它。</p><p id="9f63" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，关于我的一点点。我是一家顶级数据科学公司的数据科学家，目前正在攻读数据科学硕士学位。我花了很多时间研究，并且非常喜欢写这篇文章。如果这对你有帮助，给我点爱！😄我也写关于<a class="ae kf" href="https://medium.com/@anishmahapatra/the-millennial-burn-out-is-real-a0acebff25ae" rel="noopener">千禧一代的生活方式</a>、<a class="ae kf" href="https://medium.com/@anishmahapatra/my-top-5-learnings-as-a-consultant-accc5989ec34" rel="noopener">咨询</a>、<a class="ae kf" href="https://chatbotslife.com/how-you-can-build-your-first-chatbot-using-rasa-in-under-15-minutes-ce557ea52f2f" rel="noopener ugc nofollow" target="_blank">聊天机器人</a>和<a class="ae kf" href="https://medium.com/@anishmahapatra/the-investment-guide-for-smart-noobs-9d0e2ca09457" rel="noopener">财经</a>！如果您对此有任何问题或建议，请随时在<a class="ae kf" href="https://www.linkedin.com/in/anishmahapatra/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我或关注我<a class="ae kf" href="https://medium.com/@anishmahapatra" rel="noopener">这里</a>，我很想听听您的想法！</p><div class="np nq gp gr nr ns"><a href="https://www.linkedin.com/in/anishmahapatra/" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">Anish Mahapatra -数据科学家-穆适马公司| LinkedIn</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">我正在努力理解数学、商业和技术如何帮助我们在未来做出更好的决策…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">www.linkedin.com</p></div></div><div class="ob l"><div class="oq l od oe of ob og jz ns"/></div></div></a></div></div></div>    
</body>
</html>