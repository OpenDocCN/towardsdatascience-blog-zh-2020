<html>
<head>
<title>How to Get Beautiful Results with Neural Style Transfer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用神经风格迁移获得漂亮的结果</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-get-beautiful-results-with-neural-style-transfer-75d0c05d6489?source=collection_archive---------7-----------------------#2020-03-28">https://towardsdatascience.com/how-to-get-beautiful-results-with-neural-style-transfer-75d0c05d6489?source=collection_archive---------7-----------------------#2020-03-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class="ir is gp gr it ab cb"><figure class="iu iv iw ix iy iz ja paragraph-image"><div role="button" tabindex="0" class="jb jc di jd bf je"><img src="../Images/84be3f2ee76796814e82310b698b638d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*esra58I1ZIpcHeUjpro_6Q.png"/></div></figure><figure class="iu iv iw ix iy iz ja paragraph-image"><div role="button" tabindex="0" class="jb jc di jd bf je"><img src="../Images/8013ff8bca1d463e224e4141d5d2531b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*zut4OvEX6h_1jkIX64EE0w.png"/></div></figure><figure class="iu iv iw ix iy iz ja paragraph-image"><div role="button" tabindex="0" class="jb jc di jd bf je"><img src="../Images/8a7661c699ed5f66bb14ba86a9bf1aea.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*3IdVhB62IBQhXHIJTbNeVg.png"/></div></figure></div><div class=""/><div class=""><h2 id="797e" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">深入探究使神经类型转移起作用的技巧</h2></div><p id="64de" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di">我</span>最近对用机器学习生成中等轮廓图片产生了兴趣。这让我深深陷入了神经类型转移的领域。虽然NST在概念上很容易理解，但生成高质量的图像却异常困难。有许多错综复杂的细节和未提及的技巧，你必须正确实施，以获得巨大的成果。在本文中，我们将深入探讨神经类型转移，并详细检查这些技巧是什么。</p><p id="4a9d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在媒体和其他出版物上有许多关于NST的可靠介绍，所以我不会浪费任何时间来复习基础知识。如果你不知道NST是什么(或者你想跟随这篇文章),一个很好的开始方式是查看<a class="ae md" href="https://pytorch.org/tutorials/advanced/neural_style_tutorial.html" rel="noopener ugc nofollow" target="_blank">官方PyTorch教程</a>。不幸的是，与许多其他介绍性文章一样，最终的实现充其量只能产生一般的结果(图1)。我们将在接下来的几节中更新教程代码，以提高传输质量，但首先我们要谈一个话题。</p><p id="fb52" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="me">本文的所有附带代码都可以在</em><a class="ae md" href="https://github.com/EugenHotaj/nn-hallucinations" rel="noopener ugc nofollow" target="_blank"><em class="me">my GitHub</em></a><em class="me">上找到。</em></p><figure class="mg mh mi mj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="jb jc di jd bf je"><div class="gh gi mf"><img src="../Images/0cf36a698ca44c55e5746c3c91c4f52e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XKRk2Vr0bse4GkkWf6UNXg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图1:两种不同实现的神经风格传输质量的比较。(左下角)我们要匹配其内容的图像。(左上)我们想要匹配其样式的图像。(中间)使用PyTorch教程实现的样式传递结果。(右)使用本文中详细介绍的实现的风格转换结果。生成的图像在视觉上具有更高的质量，并且更忠实地匹配风格图像的风格。</p></figure><h1 id="dbb4" class="mo mp jj bd mq mr ms mt mu mv mw mx my kp mz kq na ks nb kt nc kv nd kw ne nf bi translated">题外话:为什么克矩阵测量风格？</h1><p id="91d3" class="pw-post-body-paragraph ky kz jj la b lb ng kk ld le nh kn lg lh ni lj lk ll nj ln lo lp nk lr ls lt im bi translated">大多数介绍神经类型转移的<a class="ae md" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank"> Gatys等人的论文</a>简单易懂。然而，一个没有解决的问题是，为什么Gram矩阵是表示风格(即纹理)的自然方式？</p><p id="da9c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在高层次上，Gram矩阵测量同一层中不同特征地图之间的相关性。特征映射只是卷积层的激活后输出。例如，如果一个卷积层有64个滤波器，它将输出64个特征图。然后，Gram矩阵测量图层中每个要素地图和每个其他要素地图之间的相关性(相似性),而不必关心精确的像素位置。为了说明为什么这是一个合理的纹理测量，假设我们有两个过滤器，一个检测蓝色的东西，一个检测螺旋。我们可以将这些滤波器应用于输入图像，以产生2个滤波器图，并测量它们的相关性。如果过滤贴图高度相关，那么图像中出现的任何螺旋几乎肯定是蓝色的。这意味着图像的纹理由蓝色螺旋组成，而不是红色、绿色或黄色螺旋。</p><p id="55f2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然这个解释仍然让我有点不安，但在纹理合成社区，Gram矩阵对应于样式似乎是一个被广泛接受的事实，正如<a class="ae md" href="https://ptrrupprecht.wordpress.com/2017/12/05/understanding-style-transfer/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>所解释的。此外，我们不能否认，我们使用Gram矩阵得到的结果令人印象深刻。</p><h1 id="3ea7" class="mo mp jj bd mq mr ms mt mu mv mw mx my kp mz kq na ks nb kt nc kv nd kw ne nf bi translated">修复PyTorch实现</h1><p id="d74e" class="pw-post-body-paragraph ky kz jj la b lb ng kk ld le nh kn lg lh ni lj lk ll nj ln lo lp nk lr ls lt im bi translated">提高传输质量的第一步是修复PyTorch教程的实现。本教程试图忠实于Gatys等人的论文，但在此过程中遗漏了一些东西。首先，论文的作者用一个<code class="fe nl nm nn no b">AvgPool2d</code>代替了<code class="fe nl nm nn no b">MaxPool2d</code>，因为他们发现它产生了更高质量的结果。另一个细节是，本教程计算卷积输出的<code class="fe nl nm nn no b">ContentLoss</code>和<code class="fe nl nm nn no b">StyleLoss</code>，而不是ReLU激活。这更像是吹毛求疵，因为我没有注意到在我的实验中使用卷积和ReLUs之间有很大的区别。</p><figure class="mg mh mi mj gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/86571158b0290d4abd243a84034b94a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*5Lk8jWtWkHfg0rUm.jpg"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图VGG19网络及其层(<a class="ae md" href="https://www.researchgate.net/figure/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means_fig2_325137356" rel="noopener ugc nofollow" target="_blank">来源</a>)。</p></figure><p id="ceda" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">教程和论文之间最惊人的区别是“错误的”层分别用于<code class="fe nl nm nn no b">ContentLoss</code>和<code class="fe nl nm nn no b">StyleLoss</code>。我在引号里写错了，因为层的选择很大程度上是主观的，很大程度上取决于什么能产生最令人愉快的风格。也就是说，我们可以用一些经验法则来指导我们的决定。当测量内容相似性时，当在<code class="fe nl nm nn no b">content_img</code>和生成的<code class="fe nl nm nn no b">input_img</code>之间存在像素完美匹配时，较低层倾向于最高度激活。我们越深入网络，这些层就越不关心精确的匹配，相反，当特征通常处于正确的位置时，它们就会高度激活。为了可视化每一层最关心的是什么，我们可以设置<code class="fe nl nm nn no b">style_weight=0</code>并使用不同的层作为<code class="fe nl nm nn no b">content_layer</code>在随机的<code class="fe nl nm nn no b">input_img</code>上运行训练过程。</p><figure class="mg mh mi mj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="jb jc di jd bf je"><div class="gh gi nq"><img src="../Images/24f5cff36c6d4041505fda29131d6ce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bSV2t00wFnkuYL0AP93SuQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图3:可视化VGG19网络不同层响应的内容。网络中更靠右的层更深。</p></figure><p id="3e40" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本教程使用第四个卷积(图2中的<code class="fe nl nm nn no b">conv2_2</code>)作为内容层。正如我们在上面的图3中看到的，这可能是一个太低的层，不能用于内容，因为网络仍然关心在这个深度匹配像素。Gatys等人用<code class="fe nl nm nn no b">conv4_2</code>代替，它更关心整体的特征排列，而不是单个像素。</p><p id="4d83" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就风格而言，较低层对小的重复特征作出反应，而较高层则捕捉更抽象的整体特征。因此，为了传递<code class="fe nl nm nn no b">style_img</code>的整体风格——从底层细节到总体主题——我们应该在网络中包含所有深度的层。本教程使用前5个卷积层，但这些都是相当低的网络，不太可能捕捉全局特征。Gatys等人使用了<code class="fe nl nm nn no b">conv1_1</code>、<code class="fe nl nm nn no b">conv2_1</code>、<code class="fe nl nm nn no b">conv3_1</code>、<code class="fe nl nm nn no b">conv4_1</code>和<code class="fe nl nm nn no b">conv5_1</code>，这是一种跨越整个网络层次的良好分层分布。我们可以使用我们用于内容的相同方法来可视化每一层选择的优化样式。为此，我们设置<code class="fe nl nm nn no b">content_weight=0</code>，指定我们想要使用哪个<code class="fe nl nm nn no b">style_layers</code>，并随机运行训练过程<code class="fe nl nm nn no b">input_img</code>。</p><figure class="mg mh mi mj gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/eb913b543350f763ee59349ff7d5e4f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*2RGHWW3GEy_EaYq3Vd0Ing.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图4:(左)PyTorch教程中选择的图层产生的样式。(右)在<a class="ae md" href="https://arxiv.org/pdf/1508.06576.pdf" rel="noopener ugc nofollow" target="_blank"> Gatys等人的论文</a>中选择的图层产生的风格。</p></figure><p id="3bfc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如预期的那样，由教程层优化的样式捕获了低级的、重复的特征，但是未能捕获高级的、全局的特征。</p><h1 id="046b" class="mo mp jj bd mq mr ms mt mu mv mw mx my kp mz kq na ks nb kt nc kv nd kw ne nf bi translated">提高传输质量</h1><p id="29dd" class="pw-post-body-paragraph ky kz jj la b lb ng kk ld le nh kn lg lh ni lj lk ll nj ln lo lp nk lr ls lt im bi translated">到目前为止，我们已经实现的修复应该让我们相当接近Gatys等人的论文中看到的质量。从这里，我们将更深入，看看接下来我们可以采取什么措施来生成更好的图像。</p><p id="c435" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我从论文中改变的第一件事是将优化器从<code class="fe nl nm nn no b">L-BFGS</code>切换到<code class="fe nl nm nn no b">Adam</code>。在论文中，作者声称<code class="fe nl nm nn no b">L-BFGS</code>导致更高质量的传输，但我在实验中使用<code class="fe nl nm nn no b">Adam</code>时没有注意到差异。此外，<code class="fe nl nm nn no b">Adam</code>似乎更稳定，尤其是在训练大量步数或大幅度<code class="fe nl nm nn no b">style_weight</code>时。在这些情况下，<code class="fe nl nm nn no b">L-BFGS</code>似乎<code class="fe nl nm nn no b">NaN</code>出来了，可能是由于爆炸梯度(虽然我没有看得太深)。</p><p id="d175" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一个小调整是将<code class="fe nl nm nn no b">mse_loss</code>(即L2损失)切换到<code class="fe nl nm nn no b">l1_loss</code>。我想不出一个很好的理由来使用L2损失进行风格转换(除了在0的可微性)，因为平方项严重惩罚离群值。正如前面提到的，我们并不真正关心像素的精确匹配，可以容忍生成的图像中有一些异常值。事实上，当样式和内容特征融合在一起时，离群值甚至可能会导致视觉上更令人愉悦的结果。最后，<a class="ae md" href="https://distill.pub/2017/feature-visualization/" rel="noopener ugc nofollow" target="_blank">特性可视化</a>——相关主题的必读文章——的作者也使用<code class="fe nl nm nn no b">l1_loss</code>完成他们的任务，可能是出于类似的原因。</p><p id="4d37" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上，许多用于生成高质量特征可视化的技巧优雅地转移到了神经风格转移上。事实上，FV和NST在概念上非常相似，只是它们生成<code class="fe nl nm nn no b">input_img</code>的方式不同。在NST中，<code class="fe nl nm nn no b">input_img</code>经过优化，以与<code class="fe nl nm nn no b">content_img</code>和<code class="fe nl nm nn no b">style_img</code>相同的方式激活网络中的不同层。另一方面，FV不使用<code class="fe nl nm nn no b">content_img</code>和<code class="fe nl nm nn no b">style_img</code>，而是生成一个<code class="fe nl nm nn no b">input_img</code>，最大限度地激发不同层中的神经元。</p><figure class="mg mh mi mj gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/22f265fd08797d16b362fb28301aa9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*x89fDTofVmZ2PF0QLnCDCA.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图5:由激进的数据扩充引起的生成图像右上方边缘的旋转伪影。</p></figure><p id="3449" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我从FV借鉴的一个技巧是在<code class="fe nl nm nn no b">input_img</code>上使用数据增强。这与常规的分类任务完全一样:在每一步，我们对<code class="fe nl nm nn no b">input_img</code>应用一些增强(例如，旋转、裁剪、调整大小等。)然后在模型中运行并计算损失。通过在每一步增加<code class="fe nl nm nn no b">input_img</code>，我们迫使<code class="fe nl nm nn no b">input_img</code>产生对微小扰动具有鲁棒性的特征。这些健壮的特征应该包含较少的高频伪像，并且通常看起来更具视觉吸引力。然而，我发现特性可视化文章中使用的增强非常激进，必须适当地缩小它们。即便如此，在生成的图像边缘仍然会出现一些旋转伪影(图5)。消除这些伪像的最简单的方法是将图像向下裁剪几个像素🙃。</p><p id="51e5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我做的最后一个修改是将<code class="fe nl nm nn no b">content_layer</code>换成<code class="fe nl nm nn no b">conv3_2</code>，而不是Gatys等人使用的<code class="fe nl nm nn no b">conv4_2</code>。我读的大多数文章也推荐<code class="fe nl nm nn no b">conv4_2</code>，尽管我发现在<code class="fe nl nm nn no b">conv4_2</code>细节会被洗掉，而且风格会压倒生成图像中的内容。另一方面，<code class="fe nl nm nn no b">conv3_2</code>仍然保持了这些细节，而没有像下层那样过度牺牲像素的完美性。事实上，我们可以通过再次查看图3来确认这一点。</p><h1 id="dbd4" class="mo mp jj bd mq mr ms mt mu mv mw mx my kp mz kq na ks nb kt nc kv nd kw ne nf bi translated">进一步提高质量</h1><p id="2baa" class="pw-post-body-paragraph ky kz jj la b lb ng kk ld le nh kn lg lh ni lj lk ll nj ln lo lp nk lr ls lt im bi translated">我们现在已经讨论了我在我的神经风格转换<a class="ae md" href="https://github.com/EugenHotaj/nn-hallucinations" rel="noopener ugc nofollow" target="_blank">代码</a>中实现的所有技巧。在这一点上，我们已经大大提高了原始PyTorch教程的传输质量。此外，<code class="fe nl nm nn no b">content_weight</code>和<code class="fe nl nm nn no b">style_weight</code>对于特定的图像选择更加健壮。例如，在PyTorch教程中，我发现如果没有适当的调整，一组图像上的好的<code class="fe nl nm nn no b">style_weight</code>不容易转移到另一组。</p><p id="a949" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也就是说，通过尝试去除生成图像中的高频噪声，有可能获得更好的结果。我遇到的最有趣的方法来自文章<a class="ae md" href="https://distill.pub/2018/differentiable-parameterizations/#section-styletransfer" rel="noopener ugc nofollow" target="_blank">可微分图像参数化</a>——另一篇涉及类似主题的必读文章。在本文中，作者通过首先在(去相关)傅立叶空间而不是(相关)像素空间中对其进行参数化来生成<code class="fe nl nm nn no b">input_img</code>。由于<code class="fe nl nm nn no b">input_img</code>是通过梯度下降生成的，去相关输入充当预处理程序，通过允许梯度下降更快地找到最小值，使优化更容易(类似于<a class="ae md" href="https://www.quora.com/Why-is-it-important-to-remove-correlated-features-in-machine-learning" rel="noopener ugc nofollow" target="_blank">在监督学习任务中移除相关特征</a>)。我还不完全清楚为什么会导致更高质量的传输，除此之外，像去相关空间中的最小值这样的手动波动解释更广泛、更可靠。</p><p id="a2dd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一种更简单的方法是通过直接或间接抑制高频噪声。通过将<code class="fe nl nm nn no b">input_img</code>的<a class="ae md" href="https://en.wikipedia.org/wiki/Total_variation_denoising" rel="noopener ugc nofollow" target="_blank">总变化损失</a>加到优化目标上，可以直接惩罚噪声。相反，可以通过在每个梯度下降步骤之后模糊<code class="fe nl nm nn no b">input_img</code>，或者在将梯度应用到<code class="fe nl nm nn no b">input_img</code>之前模糊梯度，来隐式地惩罚噪声。这两种方法的一个问题是它们也不利于真正的高频特征。这可以通过在训练期间缩小总变化损失或模糊量而得到某种程度的改善。</p><h1 id="cb62" class="mo mp jj bd mq mr ms mt mu mv mw mx my kp mz kq na ks nb kt nc kv nd kw ne nf bi translated">结论</h1><p id="eac3" class="pw-post-body-paragraph ky kz jj la b lb ng kk ld le nh kn lg lh ni lj lk ll nj ln lo lp nk lr ls lt im bi translated">如果你已经做到这一步，你现在应该知道很多关于用神经风格转换生成美丽图像的知识。虽然在概念上很简单，但是获得高质量的结果需要非常小心。我最初的目标是使用机器学习来生成中等轮廓的图片。经过多次反复试验，我想我偶然发现了一些看起来相当惊人的东西。对我来说，整个过程中最令人兴奋的部分是神经网络的端到端可微性。只需很少的努力，我们就能够“反转”一个最初被训练来区分猫和狗的模型，并使用它来生成无数不同风格的图像。尝试在随机森林中这样做。</p><p id="7652" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你喜欢这篇文章，请关注我，以便在我发布新内容时得到通知。所有代码都可以在我的GitHub上找到。</p><p id="d871" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">欧根·霍塔杰，<br/>2020年3月27日</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="0ec5" class="mo mp jj bd mq mr oa mt mu mv ob mx my kp oc kq na ks od kt nc kv oe kw ne nf bi translated">脚注</h1><ol class=""><li id="6bb5" class="of og jj la b lb ng le nh lh oh ll oi lp oj lt ok ol om on bi translated">稳健的特性也已经<a class="ae md" href="https://reiinakano.com/2019/06/21/robust-neural-style-transfer.html" rel="noopener ugc nofollow" target="_blank">显示</a>在非VGG架构中产生高质量的传输结果。由于<a class="ae md" href="https://www.reddit.com/r/MachineLearning/comments/7rrrk3/d_eat_your_vggtables_or_why_does_neural_style/" rel="noopener ugc nofollow" target="_blank">尚未被理解的原因</a>，非VGG架构不能开箱即用地进行神经类型转移。</li></ol></div></div>    
</body>
</html>