# 如何更快地适应随机森林

> 原文：<https://towardsdatascience.com/how-to-fit-random-forests-faster-a837de51d376?source=collection_archive---------30----------------------->

![](img/804cae12fbdcf7af66f00700910ca183.png)

Sebastian Unrau 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

## 使用热启动和开箱交叉验证

随机森林是机器学习的基本模型/算法。最近在 2001 年，已故的利奥·布雷曼在一篇经典的[论文](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)中首次描述了它们目前的形式。尽管人工神经网络越来越受欢迎，但它们在各种情况下都有实际用途。

有大量资源详细介绍了随机森林是如何工作的。本文将非常简要地回顾它们，然后转向主要焦点:如何通过热启动和开箱交叉验证更快地适应它们。有了这两种技术，超参数选择可以大大加快，减少拟合时间。

# 随机森林

随机森林是基本估计器的集合，通常是单个[决策树](https://en.wikipedia.org/wiki/Decision_tree)。因此有这样一个比喻:把一堆树放在一起，你会得到一片森林。

单个决策树的问题在于它们的方差很大。这是一个经常使用的术语，所以有必要具体说明它的含义。假设从群体中重新采样数据，并重新调整决策树。然后我们可以收集各种各样的预测(比如一个固定的样本外输入)。这允许我们为预测创建一个概率分布，我们指的是这个分布的方差。这种差异通常不被明确地测量，导致有时术语偏差和差异的模糊用法。

随机森林的主要观点是，如果我们有一组不相关的决策树，它们的均值方差会更低。这是一个关于随机变量均值方差的一般性质，你习惯于从例如[中心极限定理](https://en.wikipedia.org/wiki/Central_limit_theorem)中得到。在这种情况下，随机变量是来自随机选择的树的预测(稍后我们将详细介绍如何做到这一点)。在回归环境中，平均值就是字面上的平均值。在分类上下文中(为简单起见，比如说二进制)，平均值相当于投票。

为了创建随机决策树，我们通过重新采样我们的训练数据来模拟人口的重新采样，这一过程称为[引导](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))。如果我们有一个大小为 N 的数据集，我们通过替换从原始数据集采样来创建新的大小为 M 的随机数据集。通常 M=N，但不需要如此。当自举被用来产生各种预测器时，就像这里一样，这个过程被称为[**b**ootstrap**agg**regation](https://en.wikipedia.org/wiki/Bootstrap_aggregating)，或者 bagging。

此外，当我们创建每个单独的决策树时，我们选择输入变量的随机子集，以便在树中的每个节点选择最佳分割时加以考虑。这进一步降低了树的相关性。

最后，从讨论中可以明显看出，我们的目标是使用高方差、低偏差的估计量和总量来减少方差。从本质上来说，对森林的聚集就是正则化方法。因此，使用更大的树(具有更多的节点和更低的偏差)作为基本估计器是有意义的。

# 拟合随机森林

一如既往，拟合随机森林归结为选择超参数。随机森林的超参数是

1.  基础估计器(通常是决策树)的超参数可能很多。
2.  放入森林的树木数量。

同样，您应该认为第二个参数有点像正则化参数。随着树的数量越来越多，方差越来越小(但没有伴随的偏差损失)。随着树的数量增加，在准确性(或其他类似的度量)方面没有真正的损失。然而，推理时间与树的数量成线性比例，拟合时间也是如此。

随机森林的性质意味着有两种很好的方法来加快超参数选择:热启动和开箱交叉验证。

## 袋外交叉验证

当构建随机树时，上面描述的引导程序意味着只有一部分训练数据被包括在用于拟合该特定树的数据中。这意味着树对“袋外”样本的预测类似于样本外预测，即在验证集上的预测。“包”是指由引导程序选择的训练数据的子集。

这意味着，当拟合随机森林时，可以在拟合过程中生成验证误差的估计值(形式上:泛化误差),而无需使用实际的验证数据集。这通常是一个很好的估计。

与在验证数据集上重新运行拟合树相比，使用出袋误差可以节省时间。

## 热启动

当与热启动相结合时，袋外交叉验证的真正威力才会显现。当考虑要在我们的森林中包含多少棵树时，人们每次都可以天真地改造整个森林。例如，如果我们想决定是包括 100、200 还是 300 棵树。如果我们每次都改装，我们总共要装 600 棵树。然而，在拟合 200 棵树时，我们可以重用前 100 棵树。毕竟，它们是 100 棵树的完美随机样本。结果是我们只需要安装 300 棵树，速度提高了 2 倍。如果我们考虑是安装 100、200、300……还是 1000 棵树，我们可以实现 5.5 倍的加速！

与袋外交叉验证的结合是因为我们可以很容易地生成验证误差的估计值！

# 笔记

[1]假设用相同的超参数重新装配该树，或者，如果您愿意，用包含通过交叉验证的超参数选择的相同程序重新装配该树。

[2]然而，随机森林本质上是非常可并行化的(因为每棵树都可以独立地拟合和计算)。如果关注的不是 CPU 的使用时间，而是挂钟的时间，那么没有太大的损失。

[3]例如，参见统计学习的[要素](https://web.stanford.edu/~hastie/ElemStatLearn/) (Hastie、Tibshirani 和 Friedman 2009)，第 15 章。