# å¦‚ä½•ç”¨ Python å¯¹ Tweets è¿›è¡Œæ ‡è®°

> åŸæ–‡ï¼š<https://towardsdatascience.com/an-introduction-to-tweettokenizer-for-processing-tweets-9879389f8fe7?source=collection_archive---------4----------------------->

## æˆ‘ä»¬åº”è¯¥é€‰æ‹© TweetTokenizers è¿˜æ˜¯å…¶ä»– 4 ç§å¸¸è§çš„ Tokenizersï¼Ÿ

# ä»€ä¹ˆæ˜¯æ ‡è®°åŒ–ï¼Ÿ

è®°å·æ˜¯æ•´ä½“çš„ä¸€ä¸ªç‰‡æ®µï¼Œæ‰€ä»¥å•è¯æ˜¯å¥å­ä¸­çš„è®°å·ï¼Œå¥å­æ˜¯æ®µè½ä¸­çš„è®°å·ã€‚è®°å·åŒ–æ˜¯å°†ä¸€ä¸ªå­—ç¬¦ä¸²åˆ†å‰²æˆä¸€ç³»åˆ—è®°å·çš„è¿‡ç¨‹**ã€‚**

å¦‚æœä½ å¯¹æ ‡è®°åŒ–æœ‰ç‚¹ç†Ÿæ‚‰ï¼Œä½†ä¸çŸ¥é“æ–‡æœ¬ä½¿ç”¨å“ªç§æ ‡è®°åŒ–ï¼Œæœ¬æ–‡å°†ä½¿ç”¨ Twitter ä¸Šçš„åŸå§‹ Tweets æ¥å±•ç¤ºä¸åŒçš„æ ‡è®°åŒ–åŠå…¶å·¥ä½œåŸç†ã€‚

æœ¬æ–‡å°†ä»‹ç»å¦‚ä½•ç”¨ä»¥ä¸‹è¯­å¥å°†å¥å­æ ‡è®°æˆå•è¯:

*   `word_tokenize`
*   `WordPunctTokenizer`
*   `RegrexTokenizer`
*   `TweetTokenizer`

æ ‡è®°åŒ–æ˜¯é¢„å¤„ç†åŸå§‹æ–‡æœ¬çš„ç¬¬ä¸€æ­¥ï¼Œæ‰€ä»¥æˆ‘å¸Œæœ›æ‚¨å¯¹æŒæ¡è¿™ä¸ªé‡è¦çš„æ¦‚å¿µæ„Ÿåˆ°å…´å¥‹ï¼

![](img/3013929ccb887d8b3038f3e2fbba348f.png)

ç…§ç‰‡ç”± [Eric Prouzet](https://unsplash.com/@eprouzet?utm_source=medium&utm_medium=referral) æ‹æ‘„äº [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

# è¦å¤„ç†çš„æ•°æ®

Twitter æ˜¯ä¸€ä¸ªç¤¾äº¤å¹³å°ï¼Œæ¯å¤©éƒ½ä¼šå‘å¸ƒè®¸å¤šæœ‰è¶£çš„æ¨æ–‡ã€‚å› ä¸ºä¸æ­£å¼æ–‡æœ¬ç›¸æ¯”ï¼Œtweets æ›´éš¾æ ‡è®°ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨ tweets ä¸­çš„æ–‡æœ¬æ•°æ®ä½œä¸ºç¤ºä¾‹ã€‚

```
"https://t.co/9z2J3P33Uc FB needs to hurry up and add a laugh/cry button ğŸ˜¬ğŸ˜­ğŸ˜“ğŸ¤¢ğŸ™„ğŸ˜± Since eating my feelings has not fixed the world's problems, I guess I'll try to sleep... HOLY CRAP: DeVos questionnaire appears to include passages from uncited sources [https://t.co/FNRoOlfw9s](https://t.co/FNRoOlfw9s) well played, Senator Murray Keep the pressure on: [https://t.co/4hfOsmdk0l](https://t.co/4hfOsmdk0l) @datageneral thx Mr Taussig It's interesting how many people contact me about applying for a PhD and don't spell my name right."
```

ä¸Šé¢çš„å¥å­é‡Œæœ‰å¾ˆå¤šä¿¡æ¯ã€‚åœ¨å¯¹æ•´ä¸ªå¥å­è¿›è¡Œè®°å·åŒ–ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæŒ‘é€‰ä¸€äº›æˆ‘ä»¬æœ‰å…´è¶£æ¯”è¾ƒçš„å¥å­ã€‚è¿™ä¸ªåˆ—è¡¨å°†ç”¨äºæ¯”è¾ƒä¸åŒä»¤ç‰ŒåŒ–å™¨ä¹‹é—´çš„æ€§èƒ½ã€‚

```
compare_list = ['[https://t.co/9z2J3P33Uc'](https://t.co/9z2J3P33Uc'),
               'laugh/cry',
               'ğŸ˜¬ğŸ˜­ğŸ˜“ğŸ¤¢ğŸ™„ğŸ˜±',
               "world's problems",
               "[@datageneral](http://twitter.com/datageneral)",
                "It's interesting",
               "don't spell my name right",
               'all-nighter']
```

# word_tokenize

å°†å¥å­æ ‡è®°æˆå•è¯æ—¶æœ€æµè¡Œçš„æ–¹æ³•æ˜¯ä½¿ç”¨`word_tokenize.`å’Œ**ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·å°†å•è¯åˆ†å¼€ã€‚**

```
from nltk.tokenize import word_tokenizeword_tokens = []
for sent in compare_list:
    print(word_tokenize(sent))
    word_tokens.append(word_tokenize(sent))
```

ç»“æœ:

```
['https', ':', '//t.co/9z2J3P33Uc']
['laugh/cry']
['ğŸ˜¬ğŸ˜­ğŸ˜“ğŸ¤¢ğŸ™„ğŸ˜±']
['world', "'s", 'problems']
['@', 'datageneral']
['It', "'s", 'interesting']
['do', "n't", 'spell', 'my', 'name', 'right']
['all-nighter']
```

æˆ‘ä»¬å¸Œæœ›`laugh/cry`è¢«æ‹†åˆ†æˆ 2 ä¸ªå•è¯ã€‚æ‰€ä»¥æˆ‘ä»¬åº”è¯¥è€ƒè™‘å¦ä¸€ä¸ªè®°å·èµ‹äºˆå™¨é€‰é¡¹ã€‚

# å•è¯æ ‡ç‚¹ç¬¦å·åŒ–å™¨

`WordPunctTokenizer` **å°†æ‰€æœ‰æ ‡ç‚¹ç¬¦å·**æ‹†åˆ†æˆå•ç‹¬çš„è®°å·ã€‚æ‰€ä»¥è¿™å¯èƒ½å°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Ÿ

```
from nltk.tokenize import WordPunctTokenizerpunct_tokenizer = WordPunctTokenizer()punct_tokens = []
for sent in compare_list:
    print(punct_tokenizer.tokenize(sent))
    punct_tokens.append(punct_tokenizer.tokenize(sent))
```

ç»“æœ:

```
['https', '://', 't', '.', 'co', '/', '9z2J3P33Uc']
['laugh', '/', 'cry']
['ğŸ˜¬ğŸ˜­ğŸ˜“ğŸ¤¢ğŸ™„ğŸ˜±']
['world', "'", 's', 'problems']
['@', 'datageneral']
['It', "'", 's', 'interesting']
['don', "'", 't', 'spell', 'my', 'name', 'right']
['all', '-', 'nighter']
```

å—¯ï¼Œè¿™ä¸ªåˆ†è¯å™¨æˆåŠŸåœ°æŠŠ`laugh/cry`æ‹†åˆ†æˆä¸¤ä¸ªå•è¯ã€‚ä½†æ˜¯ç¼ºç‚¹æ˜¯:

*   é“¾æ¥`â€˜[https://t.co/9z2J3P33Uc'](https://t.co/9z2J3P33Uc')`è¢«åˆ†æˆ 7 ä¸ªå•è¯
*   `world's`è¢«`"'"`å­—ç¬¦æ‹†åˆ†æˆä¸¤ä¸ªå•è¯
*   `@datageneral`åˆ†ä¸º`@`å’Œ`datageneral`
*   `don't`è¢«æ‹†åˆ†ä¸º`do`å’Œ`n't`

æ—¢ç„¶è¿™äº›å•è¯åº”è¯¥è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªå•è¯ï¼Œé‚£ä¹ˆè¿™ä¸ªåˆ†è¯å™¨ä¹Ÿä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚æœ‰æ²¡æœ‰ä¸€ç§æ–¹æ³•å¯ä»¥æ ¹æ®ç©ºæ ¼æ¥æ‹†åˆ†å•è¯ï¼Ÿ

# å†æ°§åŒ–å™¨

ç”±äºæ²¡æœ‰åˆ†è¯å™¨ä¸“é—¨æ ¹æ®ç©ºæ ¼æ‹†åˆ†å•è¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`RegrexTokenizer`æ¥æ§åˆ¶å¦‚ä½•å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥é¿å…æ ¹æ®æ ‡ç‚¹ç¬¦å·æˆ–ç¼©å†™æ¥æ‹†åˆ†å•è¯:

*   åœ¨ä»£å¸ä¸ŠåŒ¹é…
*   åŒ¹é…åˆ†éš”ç¬¦æˆ–é—´éš™

## åœ¨ä»£å¸ä¸ŠåŒ¹é…

`RegexpTokenizer`ç±»é€šè¿‡**ç¼–è¯‘æˆ‘ä»¬çš„æ¨¡å¼**ï¼Œç„¶ååœ¨æˆ‘ä»¬çš„æ–‡æœ¬ä¸Šè°ƒç”¨`re.findall()`æ¥å·¥ä½œã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ä¸ªå‡½æ•°æ¥åŒ¹é…å­—æ¯æ•°å­—æ ‡è®°å’Œå•å¼•å·

```
from nltk.tokenize import RegexpTokenizer
match_tokenizer = RegexpTokenizer("[\w']+")match_tokens = []
for sent in compare_list:   
    print(match_tokenizer.tokenize(sent))
    match_tokens.append(match_tokenizer.tokenize(sent))
```

å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰æ­£åˆ™è¡¨è¾¾å¼è¯­æ³•ï¼Œ`\w+`åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªå•è¯å­—ç¬¦(å­—æ¯æ•°å­—&ä¸‹åˆ’çº¿)

ç»“æœ:

```
['https', 't', 'co', '9z2J3P33Uc']
['laugh', 'cry']
[]
["world's", 'problems']
['datageneral']
["It's", 'interesting']
["don't", 'spell', 'my', 'name', 'right']
['all', 'nighter']
```

è™½ç„¶åƒ`'worldâ€™s', 'Itâ€™s', 'donâ€™tâ€™`è¿™æ ·çš„å•è¯å¦‚æˆ‘ä»¬æ‰€æ„¿è¢«ä¿ç•™ä¸ºä¸€ä¸ªå®ä½“ï¼Œä½†æ˜¯`â€˜[https://t.co/9z2J3P33Uc'](https://t.co/9z2J3P33Uc')`ä»ç„¶è¢«æ‹†åˆ†æˆä¸åŒçš„å•è¯ï¼Œå¹¶ä¸”æˆ‘ä»¬å¤±å»äº†`â€œdatageneralâ€`ä¹‹å‰çš„`â€œ@â€`å­—ç¬¦ã€‚ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥æ ¹æ®ç©ºç™½åˆ†å‰²ï¼Ÿ

## ç©ºç™½åŒ¹é…

`RegexpTokenizer`ä¹Ÿå¯ä»¥é€šè¿‡**åŒ¹é…ç¼ºå£**æ¥å·¥ä½œã€‚å½“æ·»åŠ å‚æ•°`gaps=True`æ—¶ï¼ŒåŒ¹é…æ¨¡å¼å°†è¢«ç”¨ä½œåˆ†éš”ç¬¦ã€‚`\s+`åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªç©ºæ ¼ã€‚

```
space_tokenizer = RegexpTokenizer("\s+", gaps=True)space_tokens = []
for sent in compare_list:

    print(space_tokenizer.tokenize(sent))
    space_tokens.append(space_tokenizer.tokenize(sent))
```

ç»“æœ:

```
['https://t.co/9z2J3P33Uc']
['laugh/cry']
['ğŸ˜¬ğŸ˜­ğŸ˜“ğŸ¤¢ğŸ™„ğŸ˜±']
["world's", 'problems']
['@datageneral']
["It's", 'interesting']
["don't", 'spell', 'my', 'name', 'right']
['all-nighter']
```

ä¸é”™ï¼ç°åœ¨æˆ‘ä»¬å°†é“¾æ¥`â€˜https://t.co/9z2J3P33Uc'`è§£é‡Šä¸ºä¸€ä¸ªå•è¯ï¼ä½†æ˜¯çœ‹èµ·æ¥è¡¨æƒ…ç¬¦å·ç»„åˆæˆäº†ä¸€ä¸ªå•è¯ã€‚ç”±äºä¸åŒçš„è¡¨æƒ…ç¬¦å·åœ¨æƒ…æ„Ÿåˆ†æä¸­å¯èƒ½æ˜¯æœ‰æ„ä¹‰çš„ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›å°†å®ƒä»¬åˆ†æˆä¸åŒçš„å•è¯ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦è€ƒè™‘å¦ä¸€ç§æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

å¥½æ¶ˆæ¯ï¼æœ‰ä¸€ä¸ªæ ‡è®°å™¨å¯ä»¥åœ¨ä¸ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°æ‹†åˆ† tweetsã€‚

# TweetTokenizer

æ˜¯çš„ï¼Œå¯¹ tweet è¿›è¡Œæ ‡è®°çš„æœ€å¥½æ–¹æ³•æ˜¯ä½¿ç”¨æ ‡è®°å™¨æ¥æ ‡è®° tweet

```
from nltk.tokenize import TweetTokenizer
tweet_tokenizer = TweetTokenizer()tweet_tokens = []
for sent in compare_list:
    print(tweet_tokenizer.tokenize(sent))
    tweet_tokens.append(tweet_tokenizer.tokenize(sent))
```

ç»“æœ:

```
['https://t.co/9z2J3P33Uc']
['laugh', '/', 'cry']
['ğŸ˜¬', 'ğŸ˜­', 'ğŸ˜“', 'ğŸ¤¢', 'ğŸ™„', 'ğŸ˜±']
["world's", 'problems']
['@datageneral']
["It's", 'interesting']
["don't", 'spell', 'my', 'name', 'right']
['all-nighter']
```

å‰å®³ï¼æ¨æ–‡è¢«æ ‡è®°æˆæˆ‘ä»¬æƒ³è¦çš„æ ·å­ï¼

# æŠŠæ‰€æœ‰ä¸œè¥¿æ”¾åœ¨ä¸€èµ·

æˆ‘ä»¬å¯ä»¥æŠŠæ‰€æœ‰ä¸œè¥¿æ”¾åœ¨ä¸€ä¸ª`pd.dataframe`ä¸­è¿›è¡Œå¿«é€Ÿå‡†ç¡®çš„è§£é‡Šï¼Œè€Œä¸æ˜¯èŠ±æ—¶é—´å»åˆ†ææ¯ä¸ªåˆ†è¯å™¨çš„ç»“æœã€‚

```
import pandas as pdtokenizers = {'word_tokenize': word_tokens,
             'WordPunctTokenize':punct_tokens,
             'RegrexTokenizer for matching':match_tokens,
             'RegrexTokenizer for white space': space_tokens,
             'TweetTokenizer': tweet_tokens }df = pd.DataFrame.from_dict(tokenizers)
```

![](img/3d850d2774795ad1bc2c8ff7a3efeee9.png)

ä¸åŒæ ‡è®°å™¨ä¹‹é—´çš„æ¯”è¾ƒ

æ ¹æ®å¯¹ä¸Šè¡¨çš„è§‚å¯Ÿï¼Œ`TweetTokenizer`ä¼¼ä¹æ˜¯æœ€ä½³é€‰æ‹©ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç»§ç»­ç”¨è¿™ä¸ªæ¥æ ‡è®°æˆ‘ä»¬çš„å¥å­:

```
tweet_tokenizer.tokenize(sent)
```

# ç»“è®º

æ­å–œä½ ï¼æ‚¨å·²ç»ä» nltk åº“ä¸­å­¦ä¹ äº†ä¸åŒçš„åˆ†è¯å™¨æ¥å°†å¥å­åˆ†è¯ã€‚ä¼¼ä¹æ ‡è®° Twitter åŸå§‹æ–‡æœ¬çš„èµ¢å®¶æ˜¯`TweetTokenizer`ã€‚ä½†æƒ…å†µå¹¶éæ€»æ˜¯å¦‚æ­¤ï¼Œä½ çš„é€‰æ‹©å¯èƒ½ä¼šæ ¹æ®ä½ åˆ†æçš„æ–‡æœ¬è€Œæ”¹å˜ã€‚é‡è¦çš„ä¸€ç‚¹æ˜¯ï¼Œæ‚¨çŸ¥é“è¿™äº›æ ‡è®°å™¨çš„åŠŸèƒ½å·®å¼‚ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥åšå‡ºæ­£ç¡®çš„é€‰æ‹©æ¥æ ‡è®°æ‚¨çš„æ–‡æœ¬ã€‚åœ¨è¿™ä¸ª [Github repo](https://github.com/khuyentran1401/Data-science/blob/master/nlp/tweets_tokenize.ipynb) ä¸­ï¼Œæ‚¨å¯ä»¥éšæ„ä½¿ç”¨æœ¬æ–‡çš„ä»£ç ã€‚

æˆ‘å–œæ¬¢å†™ä¸€äº›åŸºæœ¬çš„æ•°æ®ç§‘å­¦æ¦‚å¿µï¼Œå¹¶å°è¯•ä¸åŒçš„ç®—æ³•å’Œæ•°æ®ç§‘å­¦å·¥å…·ã€‚ä½ å¯ä»¥åœ¨ [LinkedIn](https://www.linkedin.com/in/khuyen-tran-1401/) å’Œ [Twitter](https://twitter.com/KhuyenTran16) ä¸Šå’Œæˆ‘è”ç³»ã€‚

å¦‚æœä½ æƒ³æŸ¥çœ‹æˆ‘å†™çš„æ‰€æœ‰æ–‡ç« çš„ä»£ç ï¼Œè¯·ç‚¹å‡»è¿™é‡Œã€‚åœ¨ Medium ä¸Šå…³æ³¨æˆ‘ï¼Œäº†è§£æˆ‘çš„æœ€æ–°æ•°æ®ç§‘å­¦æ–‡ç« ï¼Œä¾‹å¦‚:

[](/step-by-step-tutorial-web-scraping-wikipedia-with-beautifulsoup-48d7f2dfa52d) [## ç”¨ç¾ä¸½çš„å£°éŸ³æŠ“å–ç»´åŸºç™¾ç§‘

### å…³äºå¦‚ä½•ä½¿ç”¨ Beautiful Soup çš„åˆ†æ­¥æ•™ç¨‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº web æŠ“å–çš„ç®€å•æ˜“ç”¨çš„ Python åº“

towardsdatascience.com](/step-by-step-tutorial-web-scraping-wikipedia-with-beautifulsoup-48d7f2dfa52d) [](/find-common-words-in-article-with-python-module-newspaper-and-nltk-8c7d6c75733) [## ç”¨ Python æ¨¡å— Newspaper å’Œ NLTK æŸ¥æ‰¾æ–‡ç« ä¸­çš„å¸¸ç”¨è¯

### ä½¿ç”¨ newspaper3k å’Œ NLTK ä»æŠ¥çº¸ä¸­æå–ä¿¡æ¯å’Œå‘ç°è§è§£çš„åˆ†æ­¥æŒ‡å—

towardsdatascience.com](/find-common-words-in-article-with-python-module-newspaper-and-nltk-8c7d6c75733) [](/python-tricks-for-keeping-track-of-your-data-aef3dc817a4e) [## è·Ÿè¸ªæ•°æ®çš„ Python æŠ€å·§

### å¦‚ä½•ç”¨åˆ—è¡¨ã€å­—å…¸è®¡æ•°å™¨å’Œå‘½åå…ƒç»„æ¥è·Ÿè¸ªä¿¡æ¯

towardsdatascience.com](/python-tricks-for-keeping-track-of-your-data-aef3dc817a4e) [](/maximize-your-productivity-with-python-6110004b45f7) [## ä½¿ç”¨ Python æœ€å¤§åŒ–æ‚¨çš„ç”Ÿäº§åŠ›

### ä½ åˆ›å»ºäº†ä¸€ä¸ªå¾…åŠäº‹é¡¹æ¸…å•æ¥æé«˜æ•ˆç‡ï¼Œä½†æœ€ç»ˆå´æŠŠæ—¶é—´æµªè´¹åœ¨äº†ä¸é‡è¦çš„ä»»åŠ¡ä¸Šã€‚å¦‚æœä½ èƒ½åˆ›é€ â€¦

towardsdatascience.com](/maximize-your-productivity-with-python-6110004b45f7) [](/timing-the-performance-to-choose-the-right-python-object-for-your-data-science-project-670db6f11b8e) [## é«˜æ•ˆ Python ä»£ç çš„è®¡æ—¶

### å¦‚ä½•æ¯”è¾ƒåˆ—è¡¨ã€é›†åˆå’Œå…¶ä»–æ–¹æ³•çš„æ€§èƒ½

towardsdatascience.com](/timing-the-performance-to-choose-the-right-python-object-for-your-data-science-project-670db6f11b8e)