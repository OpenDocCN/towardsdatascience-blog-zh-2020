<html>
<head>
<title>PyTorch Implementation of Matrix Factorization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">矩阵分解的PyTorch实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-implementation-of-matrix-factorization-391bfc003e43?source=collection_archive---------21-----------------------#2020-06-20">https://towardsdatascience.com/pytorch-implementation-of-matrix-factorization-391bfc003e43?source=collection_archive---------21-----------------------#2020-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4a36" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用Python从头开始实现矩阵分解</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c6b3ec9336843459e09dd6a69f2c38a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*m41uJrW8QAN81gyD"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">尼克·希利尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="f6d3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是矩阵分解</h1><p id="1497" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">矩阵分解(MF)(例如，概率矩阵分解和非负矩阵分解)技术已经成为许多现实场景的关键，包括图形表示和推荐系统(RecSys)，因为它们是发现数据背后隐藏属性的强大模型。更具体地说，非负矩阵分解(NNMF)是多元分析和线性代数中的一组模型，其中矩阵A(维数B*C)被分解为B(维数B*d)和C(维数C*d)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/6240347853c5becef5d89b98bf5f00df.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*OcgHc3tIEkOSZc9XgR83xQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵分解公式</p></figure><p id="fd7c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">其中F表示Frobenius范数。</p><p id="00e1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">矩阵分解背后的思想是在一个低维的潜在空间中表示用户和项目。广泛应用于推荐系统和降维中。尽管有许多Python库可以执行矩阵分解，但从头构建算法可能有助于理解基础知识。此外，有许多复杂的情况时，矩阵分解库不能处理。在这篇文章中，我将展示如何在PyTorch中用不同的用例实现矩阵分解，这些用例是<strong class="lt iu">普通MF库不能很好执行的</strong>。</p><h1 id="39a3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">示例1:</h1><p id="03ce" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">图形表示:</strong></p><p id="7ba3" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">给定一个图G = (E，V)，V是节点的集合，E是边的集合。G的邻接矩阵用V * V矩阵a表示，其中如果节点Vi与节点Vj之间有边E，则Xij = 1，否则Xij =0。按照前面的公式，我们可以通过最小化损耗将节点V表示成V * d矩阵</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/78ff3296fd9de4d42979f7ec1100ac5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*xPd5TNomcelq4E34zhsNzw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">示例1损失公式</p></figure><p id="7ec6" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在这种情况下，我们将矩阵分解成一个单一的矩阵，这通常是大多数库不支持的。为了解决这个问题，我们使用PyTorch构建一个只有一层的神经网络模型，并将SGD优化器应用于反向传播梯度。损失函数可以用神经网络来表示。ms loss(reduction = ' sum ')，即Frobenius范数和。重要的是要确保分解矩阵和矩阵的乘积没有非正值，因为非负矩阵分解(NNMF)。您可以应用torch Threshold(0，0)函数将每个历元中的所有负值标记为0，或者添加一个Relu输出图层。</p><h1 id="6752" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">代码实现:</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h1 id="8fc0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">例2: </strong></h1><p id="62af" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">带有电影标签的用户电影记录系统:</strong></p><p id="f4b4" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在现实应用中，处理RecSys中的冷启动问题非常重要，因为每天都有新电影问世。用户电影RecSys不能很好地处理这个问题，因为没有用户以前看过新电影。这是MF技术的变体，通过结合电影标签的信息来缓解冷启动问题。</p><p id="1641" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">给定用户U、电影V和电影标签t的集合。设X是U*V矩阵，表示评级项目的用户历史，其中如果第I个用户Ui没有对第j部电影Vj进行评级，则Xij = 0，否则Xij =(0，5)表示Ui对电影Vj的评级范围从1到5。设Y是表示电影标签信息T*V矩阵，其中如果电影Vj用Ti标记，则Yij = 1，否则Yij = 0。因此，您可以将用户U、电影V和标签T表示为U、V和T，其中</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/40a4a064b0d9a1fcdf6de60f068eb472.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWUZdrMtuSCyDi9qCmxQpw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">示例2损失公式</p></figure><p id="51d4" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在这种情况下，我们正在进行乘法矩阵分解，并使总和的总损失最小化。这也很难用普通的MF库来实现。然而，使用PyTorch，我们只需要重新定义损失函数。</p><h1 id="5234" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">代码实现:</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="85b3" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">感谢您的阅读，我期待听到您的问题和想法。如果你想了解更多关于数据科学和云计算的知识，可以在 <a class="ae ky" href="https://www.linkedin.com/in/andrewngai9255/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> Linkedin </strong> </a> <strong class="lt iu">上找我。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/ed4d7217e3a568d5ae39442da094dac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iIhHAVKi4NvlIllR"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@alfonsmc10?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿尔方斯·莫拉莱斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d860" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><em class="nf">参考</em></p><p id="8d7a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><em class="nf">https://developers . Google . com/machine-learning/recommendation/collaborative/matrix</em></p><p id="4bfb" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><em class="nf">https://en . Wikipedia . org/wiki/Matrix _ factorization _(recommender _ systems)#:~:text = Matrix % 20 factorization % 20 is % 20a % 20 class，two % 20 lower % 20 dimensionality % 20 rectangular % 20 matrices。</em></p></div></div>    
</body>
</html>