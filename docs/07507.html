<html>
<head>
<title>Sowing the Seeds of Decision Tree Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">播下决策树回归的种子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sowing-the-seeds-of-decision-tree-regression-2bb238dfd768?source=collection_archive---------33-----------------------#2020-06-06">https://towardsdatascience.com/sowing-the-seeds-of-decision-tree-regression-2bb238dfd768?source=collection_archive---------33-----------------------#2020-06-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b700114ad7df7e4f8709c5680fdd3b2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hMp5IrW1IhSMtFT0_WOYGw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来自<a class="ae jg" href="https://dribbble.com/shots/14102810-Treehouses/attachments/5724704?mode=media" rel="noopener ugc nofollow" target="_blank"> Dribbble </a>的图片由<a class="ae jg" href="https://dribbble.com/doe_eyed" rel="noopener ugc nofollow" target="_blank"> Eric Nyffeler </a></p></figure><div class=""/><div class=""><h2 id="8e77" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">机器学习系列第五篇</h2></div><p id="de11" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将讨论决策树，一种监督学习算法，通常被称为 CART，可用于回归和分类问题。</p><p id="c2ed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">顾名思义，这种算法的主要作用是使用树形结构做出决策。为了找到解决方案，决策树根据预测数据对结果变量做出连续的、分层的决策。决策树通常在处理非线性数据时使用。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lu"><img src="../Images/63ec2058b4585f2e941eb8a5d2189324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MuMbXcNW8ex_FZvt0yOg0g.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="d9a3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于其简单性以及易于理解和实现的事实，它们被广泛应用于大量行业。</p><h1 id="ec42" class="lz ma jj bd mb mc md me mf mg mh mi mj kp mk kq ml ks mm kt mn kv mo kw mp mq bi translated"><strong class="ak">熟悉一些新术语</strong></h1><p id="8b47" class="pw-post-body-paragraph ky kz jj la b lb mr kk ld le ms kn lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">现在，在我们进一步讨论之前，理解一些与算法相关的重要术语很重要。决策树由许多节点组成，每个节点代表一个特定的特征。决策树的第一个节点通常被称为根节点。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/b56e31dafe1d8bd0b2cc0309df7ebceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/1*zgeToKkVqDTnqRo0N5oKJg.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图像由<a class="ae jg" href="https://algobeans.com/2016/07/27/decision-trees-tutorial/" rel="noopener ugc nofollow" target="_blank"> Algobeans </a>拍摄</p></figure><p id="3cca" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">树的深度是树中除根节点之外的总级别数。一个分支表示一个决策，可以被视为不同节点之间的链接。一片叶子告诉你每个样本属于哪一类。</p><h1 id="f80b" class="lz ma jj bd mb mc md me mf mg mh mi mj kp mk kq ml ks mm kt mn kv mo kw mp mq bi translated"><strong class="ak">决策树是如何工作的</strong></h1><p id="a3a3" class="pw-post-body-paragraph ky kz jj la b lb mr kk ld le ms kn lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">决策树逐步将数据集划分为小的数据组，直到它们达到足够小的集合，可以用某个标签来描述。同时，一个相关的决策树被增量开发。</p><p id="3b8f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">决策树对数据采用自顶向下的方法。二叉树的分裂可以是二进制的，也可以是多进制的。该算法将数据划分为一组矩形，并在每个矩形上拟合模型。矩形(分割)的数量越多，复杂性就越大。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/59fc9088e9d571ec30b31f3558a9174c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Ww8FpsXff6LGCP5obXHJWA.gif"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由<a class="ae jg" href="http://r2d3.us" rel="noopener ugc nofollow" target="_blank"> r2d3.us </a>提供</p></figure><p id="e71a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用超级复杂的决策树的一个缺点是，由于模型对训练数据的学习非常好，因此很可能会陷入过度拟合的情况，从而难以推广到新的未知数据。</p><p id="e83e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，它检查数据集的所有特征，通过将数据分成越来越小的子组来找到最佳结果，直到树结束。</p><h1 id="e085" class="lz ma jj bd mb mc md me mf mg mh mi mj kp mk kq ml ks mm kt mn kv mo kw mp mq bi translated"><strong class="ak">信息增益和熵</strong></h1><p id="7ac2" class="pw-post-body-paragraph ky kz jj la b lb mr kk ld le ms kn lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">信息增益基于在属性上分割数据集后熵的减少。熵控制决策树决定如何分割数据。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/1ff8c6b6661ade38aa891d6cc3a3c675.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*c7JQ2PngxZM2pNRr_b5hDg.png"/></div></figure><p id="25d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">构建决策树的目标是找到返回最高信息增益的属性。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/f6f16b9aaf60907d4ad6359aeb0d52f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*ZW2FaIZVSxo7bNextzJ4vw.png"/></div></figure><p id="1263" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们将通过构建决策树模型来实际应用我们所学的知识。</p><blockquote class="na nb nc"><p id="b82b" class="ky kz nd la b lb lc kk ld le lf kn lg ne li lj lk nf lm ln lo ng lq lr ls lt im bi translated"><em class="jj">你可以在我的</em> <a class="ae jg" href="https://github.com/ashwinraj-in/MachineLearningTutorials/blob/master/DecisionTree.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="jj"> GitHub </em> </a> <em class="jj">手柄上访问用于构建这个决策树模型的完整代码和其他资源。</em></p></blockquote><h2 id="241d" class="nh ma jj bd mb ni nj dn mf nk nl dp mj lh nm nn ml ll no np mn lp nq nr mp ns bi translated"><strong class="ak"> 1。导入库</strong></h2><p id="310d" class="pw-post-body-paragraph ky kz jj la b lb mr kk ld le ms kn lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">构建模型的第一步是导入必要的库。Pandas、Numpy 和 Matplotlib 是最常用的库，分别用于数据操作、科学计算和在图形上绘制数据。</p><pre class="lv lw lx ly gt nt nu nv nw aw nx bi"><span id="8b3d" class="nh ma jj nu b gy ny nz l oa ob">#Import the Libraries and read the data into a Pandas DataFrame</span><span id="a311" class="nh ma jj nu b gy oc nz l oa ob">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="b152" class="nh ma jj nu b gy oc nz l oa ob">test=pd.read_csv("california_housing_test.csv")<br/>train=pd.read_csv("california_housing_train.csv")</span></pre><p id="5656" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如今，Seaborn 还被广泛用于制作 python 中的统计图形。导入必要的库之后，我们的下一步是将数据集加载到我们的 Jupiter 笔记本中。在这里，我使用谷歌合作实验室笔记本进行演示。</p><h2 id="2b4f" class="nh ma jj bd mb ni nj dn mf nk nl dp mj lh nm nn ml ll no np mn lp nq nr mp ns bi translated"><strong class="ak"> 2。数据可视化和特征选择</strong></h2><p id="d8ba" class="pw-post-body-paragraph ky kz jj la b lb mr kk ld le ms kn lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">在成功地将数据加载到我们的笔记本之后，我们的下一步是可视化数据。为了找到各种特征之间的任何相关性，可视化数据是很重要的。</p><pre class="lv lw lx ly gt nt nu nv nw aw nx bi"><span id="6d55" class="nh ma jj nu b gy ny nz l oa ob">#Visualise the data</span><span id="59d4" class="nh ma jj nu b gy oc nz l oa ob">plt.figure()<br/>sns.heatmap(data.corr(), cmap='coolwarm')<br/>plt.show()</span><span id="f919" class="nh ma jj nu b gy oc nz l oa ob">sns.lmplot(x='median_income', y='median_house_value', data=train)<br/>sns.lmplot(x='housing_median_age', y='median_house_value', data=train)</span></pre><p id="8675" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此后，选择适当的特征来构建模型。这通常被称为特征工程。特征工程是利用领域知识通过各种数据挖掘技术从原始数据中提取特征的过程。</p><pre class="lv lw lx ly gt nt nu nv nw aw nx bi"><span id="59aa" class="nh ma jj nu b gy ny nz l oa ob">#Select appropriate features</span><span id="ab9d" class="nh ma jj nu b gy oc nz l oa ob">data = data[[‘total_rooms’, ‘total_bedrooms’, ‘housing_median_age’, ‘median_income’, ‘population’, ‘households’]]<br/>data.info()</span><span id="1811" class="nh ma jj nu b gy oc nz l oa ob">data['total_rooms'] = data['total_rooms'].fillna(data['total_rooms'].mean())<br/>data['total_bedrooms'] = data['total_bedrooms'].fillna(data['total_bedrooms'].mean()</span></pre><p id="bdd1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当数据集中的要素数量很大时，要素选择非常重要。</p><h2 id="b6ce" class="nh ma jj bd mb ni nj dn mf nk nl dp mj lh nm nn ml ll no np mn lp nq nr mp ns bi translated"><strong class="ak"> 3。拟合模型</strong></h2><p id="ee2d" class="pw-post-body-paragraph ky kz jj la b lb mr kk ld le ms kn lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">一旦选择了特征，数据集就被分成训练数据和测试数据。这是通过从 sklearn 库中导入 train_test_split 函数来实现的。</p><pre class="lv lw lx ly gt nt nu nv nw aw nx bi"><span id="fb6a" class="nh ma jj nu b gy ny nz l oa ob">#Split the dataset into training and testing data</span><span id="a8f9" class="nh ma jj nu b gy oc nz l oa ob">import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.2, random_state = 0)</span><span id="0d4b" class="nh ma jj nu b gy oc nz l oa ob">y_train = y_train.reshape(-1,1)<br/>y_test = y_test.reshape(-1,1</span></pre><p id="91f0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，我们在代码之间调用了库。Python 允许我们在代码之间的任何地方导入库。</p><pre class="lv lw lx ly gt nt nu nv nw aw nx bi"><span id="506b" class="nh ma jj nu b gy ny nz l oa ob">#Fit the model over training data</span><span id="10e4" class="nh ma jj nu b gy oc nz l oa ob">from sklearn.tree import DecisionTreeRegressor<br/>dt = DecisionTreeRegressor()<br/>dt.fit(X_train, y_train)</span></pre><p id="2e58" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">之后，我们从 Scikit 学习库中导入决策树模型，并初始化我们的回归模型。然后，我们将数据放入模型中。这样我们就成功地建立了决策树模型。</p><h1 id="78e7" class="lz ma jj bd mb mc md me mf mg mh mi mj kp mk kq ml ks mm kt mn kv mo kw mp mq bi translated"><strong class="ak">缺点</strong></h1><p id="6188" class="pw-post-body-paragraph ky kz jj la b lb mr kk ld le ms kn lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">与决策树相关的一个主要问题是，它会遭受高方差，即数据中的一个小变化会导致一组完全不同的分裂。类似地，在一个类支配另一个类的情况下，它们也会创建有偏向的树。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/e56b34a43ca1def50fbf5bb0754d4c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*oPWRdN3B5PjStpBryRcuxA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="https://preparingforgre.com/item/153325" rel="noopener ugc nofollow" target="_blank">准备论坛</a></p></figure><p id="24a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">决策树被限制在边界内，并且通常寻找局部最优值，而不是全局最优值。然而，这些缺点可以很容易地使用集合方法来纠正，如 bagging 和 boosting，我们将在接下来的帖子中讨论。</p><p id="a863" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">至此，我们已到达本文的结尾。我希望你会发现这篇文章内容丰富。如果你有任何问题，或者如果你认为我有任何错误，请联系我！您可以通过<a class="ae jg" href="http://rajashwin812@gmail.com/" rel="noopener ugc nofollow" target="_blank">邮箱</a>或<a class="ae jg" href="http://linkedin.com/in/rajashwin/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。</p></div></div>    
</body>
</html>