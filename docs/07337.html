<html>
<head>
<title>PyTorch Performance Analysis with TensorBoard</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorBoard的PyTorch性能分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-performance-analysis-with-tensorboard-7c61f91071aa?source=collection_archive---------33-----------------------#2020-06-03">https://towardsdatascience.com/pytorch-performance-analysis-with-tensorboard-7c61f91071aa?source=collection_archive---------33-----------------------#2020-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6da1" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/deep-r-l-explained" rel="noopener" target="_blank">深度强化学习讲解— 05 </a></h2><div class=""/><div class=""><h2 id="4b61" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何在Colab中运行PyTorch的TensorBoard</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fa0f6596bcb8f7d913d0517d5a751d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDhcpkgdzkisARJU93i5xw.png"/></div></div></figure><p id="a651" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae lz" href="https://ai.facebook.com/blog/pytorch-adds-new-dev-tools-as-it-hits-production-scale/" rel="noopener ugc nofollow" target="_blank">去年，脸书宣布</a><a class="ae lz" href="https://pytorch.org" rel="noopener ugc nofollow" target="_blank">py torch</a>的1.1版本支持<a class="ae lz" href="https://www.tensorflow.org/tensorboard" rel="noopener ugc nofollow" target="_blank"> TensorBoard </a> (TensorFlow的可视化工具包)。TensorBoard提供了深度学习实验所需的可视化和工具。毫无疑问，TensorBoard是一个非常有用的工具，可以用来理解神经网络的行为，并在训练过程中帮助我们处理超参数。</p><p id="e1ce" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">TensorBoard是TensorFlow生态系统中的一个可视化工具，可用于绘制各种定量指标和几个中间计算的结果。在这篇文章中，我们将强调我们需要开始使用它的主要功能。在以后的文章中，我们会根据需要引入新的特性。PyTorch文档中说明其功能的详细教程可以在<a class="ae lz" href="https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="8187" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">接下来，我们将对在Colab中设置Tensorboard的步骤做一个大致的描述和总结，这在这一系列的文章中肯定会对我们非常有用。</p><h1 id="8c4a" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">Colab环境</h1><p id="c40a" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">在这篇文章中，我们假设你想在谷歌研究项目<a class="ae lz" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> <em class="mx">合作实验室</em> </a> (Colab)中运行TensorBoard。正如我们在之前的帖子中所描述的，它基本上由一个Jupyter笔记本环境组成，不需要任何配置，完全在云中运行，允许使用不同的深度学习库，如<a class="ae lz" rel="noopener" target="_blank" href="/tensorflow-or-pytorch-146f5397278a"> PyTorch和TensorFlow </a>。关于该服务的详细信息可以在<a class="ae lz" href="https://research.google.com/colaboratory/faq.html" rel="noopener ugc nofollow" target="_blank">常见问题页面</a>上找到。</p><blockquote class="my mz na"><p id="51af" class="ld le mx lf b lg lh kd li lj lk kg ll nb ln lo lp nc lr ls lt nd lv lw lx ly im bi translated">跟随<a class="ae lz" href="https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_05_PyTorch_Performance_Analysis_with_TensorBoard.ipynb" rel="noopener ugc nofollow" target="_blank">这个链接</a>你可以把这篇文章的代码当作一个谷歌笔记本来执行。</p></blockquote><h1 id="59ae" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">设置张量板</h1><p id="30ee" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">要设置TensorBoard，您只需遵循以下步骤:</p><p id="c504" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"> 1- </strong>从<code class="fe ne nf ng nh b">torch.utils</code>导入<code class="fe ne nf ng nh b">tensorboard</code>定义一个<code class="fe ne nf ng nh b">SummaryWriter</code>，我们向TensorBoard写入信息的关键对象:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="841a" class="nm mb it nh b gy nn no l np nq"><strong class="nh jd">from</strong> torch.utils.tensorboard <strong class="nh jd">import</strong> SummaryWriter</span></pre><p id="65bc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">2-</strong>tensor board将在其中查找要消费的记录的默认日志文件夹是<code class="fe ne nf ng nh b">runs</code>。我们可以说得更具体些:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="d8c6" class="nm mb it nh b gy nn no l np nq">writer <strong class="nh jd">=</strong> SummaryWriter('runs/working_directory')</span></pre><p id="3367" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">注意，这一行单独创建了一个<code class="fe ne nf ng nh b">runs/working_directory</code>文件夹。</p><p id="5f65" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了表明我们已经完成注册，我们可以调用对象的<code class="fe ne nf ng nh b">close</code>方法:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="8d91" class="nm mb it nh b gy nn no l np nq">writer.close()</span></pre><p id="1457" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"> 3- </strong>加载TensorBoard笔记本扩展:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="a4e2" class="nm mb it nh b gy nn no l np nq">%load_ext tensorboard</span></pre><p id="9c8c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"> 4- </strong>启动张量板:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="15ec" class="nm mb it nh b gy nn no l np nq">tensorboard  --logdir=runs</span></pre><h1 id="3034" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">使用示例</h1><p id="6694" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">为了展示TensorBoard的用法，我们建议使用<a class="ae lz" href="https://medium.com/p/a93b09bdae96/edit" rel="noopener">上一篇文章</a>中的相同代码:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="4bd9" class="nm mb it nh b gy nn no l np nq">import torch<br/>import torchvision</span><span id="ee7e" class="nm mb it nh b gy nr no l np nq">import numpy as np<br/>EPOCHS = 10<br/>BATCH_SIZE= 64</span><span id="0e9a" class="nm mb it nh b gy nr no l np nq">xy_trainPT = torchvision.datasets.MNIST(root=’./data’, train=True, download=True,transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))</span><span id="d1f8" class="nm mb it nh b gy nr no l np nq">xy_trainPT_loader = torch.utils.data.DataLoader(xy_trainPT, batch_size=BATCH_SIZE)</span><span id="2b27" class="nm mb it nh b gy nr no l np nq">def model(hidden):<br/>    model= torch.nn.Sequential(<br/>           torch.nn.Linear(784,hidden),<br/>           torch.nn.Sigmoid(),<br/>           torch.nn.Linear(hidden,10),<br/>           torch.nn.LogSoftmax(dim=1)<br/>           )<br/>    return model</span><span id="bb31" class="nm mb it nh b gy nr no l np nq">modelPT = model(10)<br/>criterion = torch.nn.NLLLoss()<br/>optimizer = torch.optim.SGD(modelPT.parameters(), lr=0.01)</span></pre><p id="95ac" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">运行下面两行代码后:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="543a" class="nm mb it nh b gy nn no l np nq">from torch.utils.tensorboard import SummaryWriter</span><span id="f579" class="nm mb it nh b gy nr no l np nq">%load_ext tensorboard</span></pre><p id="fde4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们已经可以像上一篇文章那样进行第一次训练，但现在我们还将查看损失图表。以下代码突出显示了添加的行:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="9623" class="nm mb it nh b gy nn no l np nq"><strong class="nh jd">writer = SummaryWriter()</strong></span><span id="295b" class="nm mb it nh b gy nr no l np nq">for e in range(EPOCHS):<br/>    running_loss = 0<br/>    for images, labels in xy_trainPT_loader:<br/>        images = images.view(images.shape[0], -1)<br/>        output = modelPT(images)<br/>        loss = criterion(output, labels)<br/>        loss.backward()<br/>        optimizer.step()<br/>        optimizer.zero_grad()<br/>        running_loss += loss.item()<br/>    print(“Epoch {} — Training loss: {}”.format(e,   <br/>           running_loss/len(xy_trainPT_loader)))<br/>    <strong class="nh jd">writer.add_scalar(“loss x epoch”, <br/>           running_loss/len(xy_trainPT_loader), e)</strong></span><span id="6a4b" class="nm mb it nh b gy nr no l np nq"><strong class="nh jd">writer.close()</strong></span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/c65198cb2880d264271f159b3d7a840b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lQaLYyoKouFIiAEP4a_-Dg.png"/></div></div></figure><p id="0d5d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以通过调用TensorBoard来查看结果:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="8b83" class="nm mb it nh b gy nn no l np nq">tensorboard  --logdir=runs</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/c4b4f20a5e6448cf5b1ed1e95c9b945d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QNlFeaJxzUDPUCSUV9cj7w.png"/></div></div></figure><p id="4ca7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">记住，包含数据的文件存储在<code class="fe ne nf ng nh b">runs</code>文件夹中。通过选择左侧菜单中的文件夹图标，我们可以看到这一点:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/d1b41df0194a455e2624702efcee533a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gy0xsXSQwIsZBLMURh3y7A.png"/></div></div></figure><p id="c72d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">但是，如果我们需要知道每次迭代(超过9000次迭代)的损失是如何下降的，我们无法通过简单的打印来完成，我们需要一个更图形化的表示:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="f0de" class="nm mb it nh b gy nn no l np nq">modelPT = model(10)<br/>criterion = torch.nn.NLLLoss()<br/>optimizer = torch.optim.SGD(modelPT.parameters(), lr=0.01)</span><span id="d1e3" class="nm mb it nh b gy nr no l np nq">def training_loop():<br/><strong class="nh jd">  writer = SummaryWriter()<br/></strong>  iter_no=0<br/>  for e in range(EPOCHS):<br/>    running_loss = 0<br/>    for images, labels in xy_trainPT_loader:<br/>        images = images.view(images.shape[0], -1)<br/>        output = modelPT(images)<br/>        loss = criterion(output, labels)<br/>        loss.backward()<br/>        optimizer.step()<br/>        optimizer.zero_grad()<br/>        running_loss += loss.item()<br/><strong class="nh jd">        writer.add_scalar("loss x iter", loss.item(), iter_no)<br/></strong>        iter_no += 1<br/><strong class="nh jd">  writer.close()</strong></span><span id="4336" class="nm mb it nh b gy nr no l np nq">training_loop()</span></pre><p id="5968" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">本次运行的信息已保存在<code class="fe ne nf ng nh b">runs</code>文件夹下的新目录中。我们可以通过再次运行以下命令来查看结果:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="6794" class="nm mb it nh b gy nn no l np nq">tensorboar --logdir=runs</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/b9a401b1ce225d865036b21dbeefd23a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PCxkC-Kz6RfhfMEUH5RorA.png"/></div></div></figure><p id="5ac9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">但是也许我们感兴趣的是比较，例如，具有更多神经元的神经网络的行为。让我们试试这个有32个神经元的神经网络的例子:</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="1cf4" class="nm mb it nh b gy nn no l np nq">modelPT = model(32)<br/>criterion = torch.nn.NLLLoss()<br/>optimizer = torch.optim.SGD(modelPT.parameters(), lr=0.01)<br/>training_loop()</span></pre><p id="cd38" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当记录数据时，我们可以使用相同的图形名称(在本例中为“loss x ITER”)<strong class="lf jd"/>在相同的图形上绘图。这种情况下的结果将是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e4faed0ed9875fa1acff12577f4e6525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N0ztUbZPFsoaRcHDSwMS1w.png"/></div></div></figure><p id="827d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以看到两种网络中的损耗行为的比较。或者说，用另一个优化器来代替SGD这个Adam怎么样？</p><pre class="ks kt ku kv gt ni nh nj nk aw nl bi"><span id="7941" class="nm mb it nh b gy nn no l np nq">modelPT = model(32)<br/>criterion = torch.nn.NLLLoss()<br/>optimizer = torch.optim.Adam(modelPT.parameters(), lr=0.01)<br/>training_loop()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/84e4c6ce3906d24dbf1bba9d65abbaf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pz5C81KESVKVxRm12oq3YA.png"/></div></div></figure><p id="14a9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在这种情况下我们可以看到它收敛得更好！</p><p id="2a46" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这只是TensorBoard提供的机会的一小部分。我邀请读者自己探索这个强大的工具，它也为其他中间件服务，特别是TensorFlow，TensorBoard就是在这个框架中创建的。</p><p id="def4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下一个帖子见！</p><blockquote class="my mz na"><p id="9e85" class="ld le mx lf b lg lh kd li lj lk kg ll nb ln lo lp nc lr ls lt nd lv lw lx ly im bi translated">这篇文章的全部代码可以在GitHub 上找到</p></blockquote></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="5438" class="ma mb it bd mc md oe mf mg mh of mj mk ki og kj mm kl oh km mo ko oi kp mq mr bi translated">深度强化学习讲解系列</h1><p id="c09a" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated"><strong class="lf jd">由</strong> <a class="ae lz" href="https://www.upc.edu/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> UPC巴塞罗那理工</strong> </a> <strong class="lf jd">和</strong> <a class="ae lz" href="https://www.bsc.es/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">巴塞罗那超级计算中心</strong> </a></p><p id="4cf4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一个轻松的介绍性<a class="ae lz" href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener ugc nofollow" target="_blank">系列</a>以一种实用的方式逐渐向读者介绍这项令人兴奋的技术，它是人工智能领域最新突破性进展的真正推动者。</p><div class="oj ok gp gr ol om"><a href="https://torres.ai/deep-reinforcement-learning-explained-series/" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd jd gy z fp or fr fs os fu fw jc bi translated">深度强化学习解释-乔迪托雷斯。人工智能</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">本系列的内容</h3></div></div><div class="ou l"><div class="ov l ow ox oy ou oz lb om"/></div></div></a></div><h1 id="89c0" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">关于这个系列</h1><p id="87a4" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">我是在五月份开始写这个系列的，那是在巴塞罗那的封锁期。老实说，在业余时间写这些帖子帮助了我<a class="ae lz" href="https://twitter.com/hashtag/StayAtHome?src=hashtag_click" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> #StayAtHome </strong> </a>因为封锁。感谢您当年阅读这份刊物；它证明了我所做的努力。</p><p id="5f57" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">免责声明</strong> —这些帖子是在巴塞罗纳被封锁期间写的，目的是分散个人注意力和传播科学知识，以防对某人有所帮助，但不是为了成为DRL地区的学术参考文献。如果读者需要更严谨的文档，本系列的最后一篇文章提供了大量的学术资源和书籍供读者参考。作者意识到这一系列的帖子可能包含一些错误，如果目的是一个学术文件，则需要对英文文本进行修订以改进它。但是，尽管作者想提高内容的数量和质量，他的职业承诺并没有留给他这样做的自由时间。然而，作者同意提炼所有那些读者可以尽快报告的错误。</p></div></div>    
</body>
</html>