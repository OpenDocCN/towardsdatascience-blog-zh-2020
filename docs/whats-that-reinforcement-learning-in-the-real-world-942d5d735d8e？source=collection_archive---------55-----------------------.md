# “那是什么？现实世界中的强化学习？”

> 原文：<https://towardsdatascience.com/whats-that-reinforcement-learning-in-the-real-world-942d5d735d8e?source=collection_archive---------55----------------------->

## 一个开放的痛苦减缓现实世界 RL 的亮点，以及一些使应用可行的创造性方法。

![](img/d65516bbd3edb000bd1b02a9eab8d0f9.png)

由 [Unsplash](https://unsplash.com/photos/XRcEsQKTWGk) 上的[Torsten dede RICS](https://unsplash.com/photos/XRcEsQKTWGk)拍摄的图像

强化学习提供了一种解决机器学习难题的独特方式。它的顺序决策能力，以及对需要在即时和长期回报之间进行权衡的任务的适用性，是一些使其在监督学习或无监督学习方法相比较而言不适合的环境中成为理想选择的因素。

通过让代理从零知识开始，然后通过与环境的交互来学习定性的良好行为，几乎可以说强化学习(RL)是迄今为止最接近人工智能的东西。我们可以看到 RL 被用于机器人控制，医疗保健中的治疗设计，以及其他；但是，为什么我们不吹嘘许多 RL 代理正在扩大到现实世界的生产系统？

# **1。诅咒的奖励功能**

像 *Atari、*这样的游戏是如此好的 RL 基准是有原因的——它们让我们只关心最大化分数，而不用担心设计奖励函数。一个理想的奖励函数既能鼓励代理人按照我们希望的方式行事，同时又是可学习的。

我们可以选择仅在目标状态奖励代理*，而不在其他地方奖励代理。例如，在这样一个迷宫解决任务中，代理人只有找到目标房间才能得到+1 奖励。这是一个稀疏的奖励。它简单明了地指定了期望的目标，但也几乎是不可学习的，因为代理人没有收到关于它离期望目标有多近的反馈。对此，通常的解决方案是重塑函数，以在代理接近目标时提供递增的奖励。然而，该政策可能会曲解这种奖励机制，并最终[学习最懒惰的可能行为](https://blog.openai.com/faulty-reward-functions/)来适应局部最优，而不是我们希望它应该学习的。*

这使得奖励函数的设计异常古怪。

一个很好的例子来自 Irpan，一个机器人手臂被训练去触及桌面上的目标。然而，可爱的机器人选择学习砸桌子，这使目标指向它。

值得注意的是，一个奖励函数可以优化一个单一的度量，仅仅是为了在学习行为的评估过程中发现其他同等重要的度量。许多系统需要多维全局奖励函数来优化许多较小的目标。为了成功，这需要在奖励函数中整合多个系统指标，这可能很难实现，或者更糟的是，一些指标我们仍然不知道。然而，对于 RL 特有的多目标奖励函数，还没有多少研究。

在现实世界的设置中，奖励函数必须对策略进行调整，使其在任务目标的所有实例上(而不仅仅是其中的一个子集)都能按预期执行。如果一个机器人正在向顾客分发饼干和微笑，它应该为每一个顾客这样做，而不仅仅是为一个吝啬的顾客。

*奖励函数的现有方法*

[模仿学习](https://en.wikipedia.org/wiki/Imitation)不是硬编码一个改善系统政策的奖励函数，而是利用专家的示范来训练一个政策——这本质上是监督学习。该系统学习复制专家的确切行为，而不关心专家试图实现什么。

[反向 RL](https://en.wikipedia.org/wiki/Apprenticeship_learning) 通过尝试从演示的行为中推断专家的意图，而不是复制专家可能无意中采取的非最佳行动，来改进模仿技术。它学习适合专家演示的可能的奖励函数。

# **2。安全 RL**

安全是我们不在道路上发布轮式机器并要求它们通过犯错来学习的一个很好的原因。它是关于确保一个自治系统构成不可挽回的自我毁灭或对人类生命构成危险的非临界风险。

*安全感知代理*

安全 RL 的现有方法是在策略上指定安全约束。安全约束可以是任何被认为潜在不安全的行为，例如在航行期间避免高速碰撞。以这种方式优化安全性的解决方案被称为受约束的[马尔可夫决策过程](https://en.wikipedia.org/wiki/Markov_decision_process) (MDP)。该约束可以是固定的和预定的，或者是可变的，以允许在最大化预期回报和满足最小安全阈值之间进行权衡。类似地，通过使用概率客观 MDP，我们可以选择激励代理人在选定的步骤数内采取冒险行动以获得迫在眉睫的更高回报，或者专注于保持安全。

受约束的 MDP 的问题在于，为了了解保持安全需要什么，系统必须通过采取导致不安全后果的行动来获得经验。这种冒险行为通常会在培训期间受到鼓励，以产生一个策略，该策略概括为在现实世界的用例中避免它们。

在训练期间鼓励不安全的行为可以包括学习一个近似器，该近似器预测处于附近状态的安全性，然后可以用于帮助代理的探索策略。

# **3。**状态感测和驱动的延迟

高维输入需要耐心处理，现实世界的系统使用多个传感器和图像来获得状态的准确表示。因此，在观察到的状态到达代理之前需要时间。向致动器发送选定的动作也会有一定的通信延迟。行动执行时也不会立即达到预期的下一个状态，因为移动系统部件需要时间来调整——相对于某个物体，将机械臂从 *45* 旋转到 *100* 度不会在一瞬间实现。

所有这些意味着代理人**(a*ₜ*| s*ₜ*)*可能正在采取以过去的状态为条件的动作，而不是当前的，时间步长 *t.* 这将违反[马尔可夫性质](https://en.wikipedia.org/wiki/Markov_property)要求下一个状态 *sₜ ₊₁* 有条件地依赖于当前状态和动作对(马尔科夫性是 RL 决策过程***p***(sₜ₊₁|*s*ₜ*，***t41】π***(a*ₜ*| s*ₜ*)【t49)的基础，所以违反它是一件大事。**

**减轻系统延迟**

*向代理添加内存可用于优化两个因素:*

*   *动作延迟:将一个简短的动作历史合并到状态中，帮助策略学会将每个动作与结果延迟相关联。当然，这种启发式方法增加了输入维度。*
*   *寻找对未来状态负责的延迟状态-动作转换对:包括使用来自记忆的过去的状态-动作转换对，以尝试对在当前时间步长的预测中有用的转换给予应有的信任。*

# ***4。**系统的非平稳性和政策稳健性*

*真实系统将经常经历可移动接头处的磨损或通风口的部分堵塞，如果代理人不知道，这将导致学习到的环境动力学中的不可观察到的变化。对于这种类型的问题，我们可以使用一个随时间变化的非常数奖励函数来模拟非平稳性。通过[元学习](https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html)寻找一个最能适应非平稳性引入的变化的策略也是不错的尝试。*

*在基于模型的 RL 中出现了另一种情况，其中寻求内在好奇心(仅因在新/新奇状态中的探索而获得奖励)的代理将需要观测的稳定函数，以确保在基于策略的模型训练期间的稳定性。然而，这样的代理被认为经历了不稳定的回报，因为被认为是新的状态在未来的时间步骤中将是旧的和无回报的。由于动态模型在演化过程中学习新的特性来纠正过去的错误，所以它也是不稳定的一个额外来源。*

*我们可以通过随机初始化和固定卷积网络的特征作为观察函数来减轻这种情况，代价是状态的充分表示，或者从不需要特征学习的图像像素中学习。*

*非平稳性是真实系统中可能遇到的随机性的一个来源——它也必然会有来自传感器读数和动作执行的噪声。因此，需要确保代理对这些问题的不同子集是健壮的。成功的模拟真实转移很大程度上依赖于学习策略对真实世界系统的欠建模特征的鲁棒性。*

**制定稳健的政策**

*领域随机化[在增加策略稳健性](https://openai.com/blog/generalizing-from-simulation/)方面被证明是非常宝贵的，它引入了对代理环境的干扰，并平均了在训练期间学习到的错误。此外，训练一个策略，使其能够识别当前环境，并根据情况进行自我调整(元学习),可以减轻环境噪音的影响。*

*[稳健的 MDP 公式](https://arxiv.org/abs/1802.03236)专注于改善最坏情况价值函数，然后产生一个可以承受各种环境干扰的代理。在每个时间步，代理使用单独的转移函数(*s“| s，a* )来最小化长期预测值。*

# ***5。**习得政策的可解释性*

*![](img/aff83f52fd902d5b101f07cda4c5bc9f.png)*

*安博塞利，肯亚| [哈希尔古德卡](https://unsplash.com/@hgudka97)在 [Unsplash](https://unsplash.com/photos/0prglfrYY08)*

*在可解释的 RL 方面做得很少。在真实的系统中，我们需要对代理的意图有明确的保证，并对我们指出潜在失败案例的能力有信心。由于该策略可能会为系统问题创建一个新的、意想不到的解决方案，因此正确认识其短期和长期意图至关重要。*

*一种令人兴奋的方法——[*可编程解释的强化学习*](https://arxiv.org/abs/1804.02477)——试图将[非线性](https://en.wikipedia.org/wiki/Universal_approximation_theorem)策略提炼为人类可验证的、显式编码的系统程序。*

# ***6。**高维状态和动作空间*

*维数灾难无处不在。例如，机器人系统在它们的许多自由度上具有连续的动作空间，这些自由度与每一个自由度相关的关节角度和速度有关。与监督和非监督设置不同，在监督和非监督设置中，一些回归方法，如 [l1 正则化，](https://en.wikipedia.org/wiki/Lasso_%28statistics%29)给出一个缩减的输入特征空间，这在 RL 中更难实现，因为我们需要在整个状态-动作-空间中收集数据，以更好地收敛我们的全局策略。此外,(无监督)学习中的这种回归技巧旨在增加模型的方差。但是在 RL 中，我们已经有一肚子的差异，我们关心的是找到减少差异的方法。*

**接近**

*通过减少候选动作的数量实现了较低的输入维数。其中一个方法是消除看起来不相关的行为。虽然这可能会诱使我们学习另一个函数逼近器，但它会使参数空间变得更大。通过将无用的行为框定为 MDP 问题的一部分，并使用[上下文强盗](https://en.wikipedia.org/wiki/Multi-armed_bandit)将其过滤掉，已经研究了行为消除。此外，给定候选动作的向量，对最近的动作执行最近邻搜索可以降低离散动作空间的维度。*

*对于高维状态，可以通过使用逆动力学来实现更紧凑的状态表示。这预测了下一个和前一个状态( *a* ₜ| *s* ₜ₋₁， *s* ₜ₊₁).)给出的动作所使用的直觉是，所学习的观察空间的特征是那些在代理的直接控制下的特征，而那些被忽略的是代理对其没有直接影响的环境方面。*

# ***7。**政策外学习*

*脱离策略学习允许我们使用轨迹( *s* ₁ *，a* ₁ *，s* ₂ *，a* ₂ *，*……)在独立于代理运行的策略下训练代理。在实际系统中，当代理收集新数据时，它将被用来提高当前策略的性能。由于这种改进可能会出错，而这在生产中是至关重要的，因此在部署新策略之前，需要先对其进行偏离策略的评估。*

*政策外评估提出了一个问题，即在使用相关但不同的分布样本时，评估某一分布下的概率。重要性抽样被广泛用于缓解这种分布不匹配。我们还可以学习系统的动力学模型，并使用模型转换预测来评估新策略。*

# ***8。**样本复杂度*

*一个模拟的环境提供了无限的训练数据，并且完全不用担心不良行为的影响。真实系统中的所有训练数据都来自系统本身，但不是免费的。*

*在真实世界的系统中产生 RL 数据是昂贵的——代理通过多次动作来学习，这可能导致在它学习到预期的行为之前系统磨损。因此，代理人必须遵守安全，严格限制探索，同时表现得相当好。*

*与模拟器不同，实时运行也很慢，无法加速。回报期可能长达数月，例如在医疗保健政策中，这限制了代理人纠正错误的频率。*

*基于模型的 RL 可以通过对合成样本进行训练来提供样本效率的优势。如果模型是真实系统的足够接近的表示，这可能是有帮助的。对于初始策略，使用专家演示作为训练过渡，而不是随机初始化，将有助于降低长期样本复杂性。此外，元学习可以使学习策略适应训练分布中的未知任务，减少为多个相似目标训练策略所需的样本数量。*

## *结论*

*这是对减缓强化学习在现实世界中的应用的已知核心挑战的概述。强调的解决方案绝不是完全的解决方案，也不是详尽无遗的，而是旨在展示为解决同一问题而选择的不同视角，以及随着时间的推移而取得的进步。*

***参考文献***

*[1] Kober J .，Bagnell，A .，Peters，j .，[机器人学中的强化学习:一项调查](https://journals.sagepub.com/doi/abs/10.1177/0278364913495721)。IJRR，2013 年。*

*[2] Dulac-Arnold G .，Mankowitz D. Hester T .，[现实世界强化学习的挑战](http://arxiv.org/abs/1904.12901)，ICML，2019。*

*[3]y .布尔达，Storkey A .，Edwards H .，Pathak d .，Darrell t .[大规模研究好奇心驱动的学习](https://arxiv.org/abs/1808.0355)，2018。*

*【4】深 RL 还不行[https://www.alexirpan.com/2018/02/14/rl-hard.html](https://www.alexirpan.com/2018/02/14/rl-hard.html)*