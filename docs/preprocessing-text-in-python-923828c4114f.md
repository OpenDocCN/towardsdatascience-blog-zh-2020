# åœ¨ Python ä¸­é¢„å¤„ç†æ–‡æœ¬

> åŸæ–‡ï¼š<https://towardsdatascience.com/preprocessing-text-in-python-923828c4114f?source=collection_archive---------20----------------------->

## å»ºç«‹æƒ…æ„Ÿåˆ†ç±»å™¨çš„ä¸€æ­¥

è¿™ç¯‡æ–‡ç« æ˜¯å…³äºå»ºç«‹æƒ…æ„Ÿåˆ†ç±»å™¨çš„ä¸‰ç¯‡è¿ç»­æ–‡ç« ä¸­çš„ç¬¬äºŒç¯‡ã€‚åœ¨ç¬¬ä¸€ç¯‡æ–‡ç« çš„[ä¸­æˆ‘ä»¬è¿›è¡Œäº†æ¢ç´¢æ€§çš„æ–‡æœ¬åˆ†æä¹‹åï¼Œæ˜¯æ—¶å€™å¯¹æˆ‘ä»¬çš„æ–‡æœ¬æ•°æ®è¿›è¡Œé¢„å¤„ç†äº†ã€‚ç®€å•åœ°è¯´ï¼Œé¢„å¤„ç†æ–‡æœ¬æ•°æ®å°±æ˜¯åšä¸€ç³»åˆ—çš„æ“ä½œï¼Œå°†æ–‡æœ¬è½¬æ¢æˆè¡¨æ ¼å½¢å¼çš„æ•°å€¼æ•°æ®ã€‚åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸‰ç§ä¸åŒå¤æ‚åº¦çš„æ–¹æ³•æ¥é¢„å¤„ç†æ–‡æœ¬åˆ° *tf-idf* çŸ©é˜µï¼Œä¸ºæ¨¡å‹åšå‡†å¤‡ã€‚å¦‚æœä½ ä¸ç¡®å®šä»€ä¹ˆæ˜¯ tf-idfï¼Œ](/exploratory-text-analysis-in-python-8cf42b758d9e)[è¿™ç¯‡æ–‡ç« ](/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc)ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è§£é‡Šã€‚

![](img/4ea7ab753a45347540d242f5dceff6e5.png)

ç…§ç‰‡ç”± [Domenico Loia](https://unsplash.com/@domenicoloia?utm_source=medium&utm_medium=referral) åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„

åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬åé€€ä¸€æ­¥ï¼Œå¿«é€Ÿåœ°çœ‹ä¸€ä¸‹æ›´å¤§çš„ç”»é¢ã€‚ [CRISP-DM](https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome) æ–¹æ³•æ¦‚è¿°äº†æˆåŠŸçš„æ•°æ®ç§‘å­¦é¡¹ç›®çš„æµç¨‹ã€‚*æ•°æ®é¢„å¤„ç†*æ˜¯**æ•°æ®å‡†å¤‡**é˜¶æ®µçš„å…³é”®ä»»åŠ¡ä¹‹ä¸€ã€‚

![](img/eb0c82b29b5f0dfa8ec8c5f3758ff76d.png)

CRISP-DM å·¥è‰ºæµç¨‹æ‘˜å½•

# 0.Python è®¾ç½®

è¿™ç¯‡æ–‡ç« å‡è®¾è¯»è€…(ğŸ‘€æ˜¯çš„ï¼Œä½ ï¼)å¯ä»¥è®¿é—®å¹¶ç†Ÿæ‚‰ Pythonï¼ŒåŒ…æ‹¬å®‰è£…åŒ…ã€å®šä¹‰å‡½æ•°å’Œå…¶ä»–åŸºæœ¬ä»»åŠ¡ã€‚å¦‚æœä½ æ˜¯ Python çš„æ–°æ‰‹ï¼Œ[è¿™ä¸ª](https://www.python.org/about/gettingstarted/)æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚

æˆ‘åœ¨ Jupyter ç¬”è®°æœ¬é‡Œæµ‹è¯•è¿‡ Python 3.7.1 çš„è„šæœ¬ã€‚

è®©æˆ‘ä»¬åœ¨å¼€å§‹ä¹‹å‰ç¡®ä¿æ‚¨å·²ç»å®‰è£…äº†ä»¥ä¸‹åº“:
â—¼ï¸ **æ•°æ®æ“ä½œ/åˆ†æ:** *numpyï¼Œpandas* â—¼ï¸ **æ•°æ®åˆ†åŒº:***sk learn* â—¼ï¸**æ–‡æœ¬é¢„å¤„ç†/åˆ†æ:** *nltk* â—¼ï¸ **æ‹¼å†™æ£€æŸ¥å™¨:** *æ‹¼å†™æ£€æŸ¥å™¨(pyspellchecker* å®‰è£…æ—¶

ä¸€æ—¦ä½ å®‰è£…äº† *nltk* ï¼Œè¯·ç¡®ä¿ä½ å·²ç»ä» *nltk* ä¸‹è½½äº†*ã€åœç”¨è¯ã€‘*å’Œ*ã€wordnetã€‘*è¯­æ–™åº“ï¼Œè„šæœ¬å¦‚ä¸‹:

```
import nltk
nltk.download('stopwords') 
nltk.download('wordnet')
```

å¦‚æœä½ å·²ç»ä¸‹è½½äº†ï¼Œè¿è¡Œè¿™ä¸ªä¼šé€šçŸ¥ä½ ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å‡†å¤‡å¯¼å…¥æ‰€æœ‰çš„åŒ…:

```
# Setting random seed
seed = 123# Measuring run time
from time import time# Data manipulation/analysis
import numpy as np
import pandas as pd# Data partitioning
from sklearn.model_selection import train_test_split# Text preprocessing/analysis
import re, random
from nltk import word_tokenize, sent_tokenize, pos_tag
from nltk.util import ngrams
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from spellchecker import SpellChecker
```

# 1.æ•°æ®ğŸ“¦

æˆ‘ä»¬å°†ä½¿ç”¨ IMDB ç”µå½±è¯„è®ºæ•°æ®é›†ã€‚æ‚¨å¯ä»¥åœ¨è¿™é‡Œä¸‹è½½æ•°æ®é›†[ï¼Œå¹¶å°†å…¶ä¿å­˜åœ¨æ‚¨çš„å·¥ä½œç›®å½•ä¸­ã€‚ä¿å­˜åï¼Œè®©æˆ‘ä»¬å°†å…¶å¯¼å…¥ Python:](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)

```
sample = pd.read_csv('IMDB Dataset.csv')
print(f"{sample.shape[0]} rows and {sample.shape[1]} columns")
sample.head()
```

![](img/6b23a671ee90c83029239e074f5dfca9.png)

è®©æˆ‘ä»¬æ¥çœ‹çœ‹æƒ…ç»ªä¹‹é—´çš„åˆ†æ­§:

```
sample['sentiment'].value_counts()
```

![](img/5e5c06fc380f7ac8acfe9701d65dedac.png)

åœ¨æ ·æœ¬æ•°æ®ä¸­ï¼Œæƒ…æ„Ÿæ˜¯å¹³å‡åˆ†é…çš„ã€‚æˆ‘ä»¬å…ˆæŠŠæ•°æ®åˆ†æˆä¸¤ç»„:*è®­ç»ƒ*å’Œ*æµ‹è¯•*ã€‚æˆ‘ä»¬å°†ç•™å‡º 5000 ç®±è¿›è¡Œæµ‹è¯•:

```
# Split data into train & test
X_train, X_test, y_train, y_test = train_test_split(sample['review'], sample['sentiment'], test_size=5000, random_state=seed, 
                                                    stratify=sample['sentiment'])# Append sentiment back using indices
train = pd.concat([X_train, y_train], axis=1)
test = pd.concat([X_test, y_test], axis=1)# Check dimensions
print(f"Train: {train.shape[0]} rows and {train.shape[1]} columns")
print(f"{train['sentiment'].value_counts()}\n")print(f"Test: {test.shape[0]} rows and {test.shape[1]} columns")
print(test['sentiment'].value_counts())
```

![](img/dd7a92b9923d4142d655cce22d3d6f01.png)

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨*åºåˆ—*è¿›è¡Œé¢„å¤„ç†å®éªŒã€‚åªæœ‰å½“æˆ‘ä»¬éœ€è¦è¯„ä¼°æœ€ç»ˆæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬æ‰ä¼šé¢„å¤„ç†*æµ‹è¯•*ã€‚è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹è®­ç»ƒæ•°æ®é›†çš„å¤´éƒ¨:

```
train.head()
```

![](img/ff942e16a2a52ba3489fa71cb2246a08.png)

å¥½äº†ï¼Œæˆ‘ä»¬å¼€å§‹é¢„å¤„ç†å§ï¼âœ¨

# 2.é¢„å¤„ç†æ–‡æœ¬

æ ¹æ®æˆ‘ä»¬çš„å¤„ç†æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸åŒçš„ tf-idf çŸ©é˜µã€‚åœ¨æ„å»ºæ¨¡å‹æ—¶ï¼Œå°è¯•ä¸åŒçš„é¢„å¤„ç†æ–¹æ³•æ˜¯å¾ˆå¥½çš„ã€‚æˆ‘ä»¬å°†ç ”ç©¶ä»¥ä¸‹ 3 ç§æ–¹æ³•:

1.  *æ›´ç®€å•çš„æ–¹æ³•*
2.  *ç®€å•æ–¹æ³•*
3.  *ä¸å¤ªç®€å•çš„æ–¹æ³•*

è™½ç„¶åœ¨è¿™ç¯‡æ–‡ç« ä¸­æˆ‘ä»¬åªè®¨è®ºäº†å°†æ–‡æœ¬é¢„å¤„ç†æˆ tf-idf çŸ©é˜µï¼Œä½†æ˜¯æ‚¨å¯èƒ½è¿˜æƒ³æ¢ç´¢å…¶ä»–çš„æ–¹æ³•ã€‚

å¯¹äºæ¯ä¸€ç§æ–¹æ³•ï¼Œæˆ‘ä»¬ä¸€å®šä¼šè¡¡é‡è¿è¡Œæ—¶æ€§èƒ½ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„è€ƒè™‘å› ç´ ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæ¨¡å‹å°†è¢«ç”Ÿäº§ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­æµ‹è¯•å“ªç§æ–¹æ³•æ›´é€‚åˆè¿™ä¸ªæ¨¡å‹ã€‚åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œä¸ºäº†ä½¿äº‹æƒ…å˜å¾—ç®€å•ï¼Œè®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å¸®åŠ©æˆ‘ä»¬æ£€æŸ¥ä¸åŒçš„æ–¹æ³•:

```
def inspect(vectoriser, X):
    # Fit and transform
    start = time()
    print(f"There are {vectoriser.fit_transform(X).shape[1]} columns.\n")
    end = time()
    print(f"Took {round((end-start),2)} seconds.\n")

    # Inspect tokens
    tokens = list(vectoriser.vocabulary_.keys())
    tokens.sort()
    print(f"Example tokens: {tokens[:50]}\n")

    # Inspect ignored tokens
    ignored = vectoriser.stop_words_
    if len(ignored)==0:
        print("No token is ignored.")
    elif len(ignored)>50:
        print(f"Example ignored tokens: {random.sample(ignored, 50)}")
    else:
        print(f"Example ignored tokens: {ignored}")
```

## 2.1.æ›´ç®€å•çš„æ–¹æ³• 1ï¸âƒ£

è®©æˆ‘ä»¬ä»ä¸‰ä¸ªä¸­æœ€ç®€å•çš„å¼€å§‹ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å°†è®© *sklearn çš„ TfidfVectorizer* åšæ‰€æœ‰çš„é¢„å¤„ç†ï¼Œè€Œä¸ä½¿ç”¨ä»»ä½•é¢å¤–çš„å®šåˆ¶å‡½æ•°ã€‚è®©æˆ‘ä»¬ä½¿ç”¨é»˜è®¤å‚æ•°æ¥æ„Ÿå—ä¸€ä¸‹æˆ‘ä»¬å°†è·å¾—å¤šå°‘åˆ—:

```
vectoriser = TfidfVectorizer()
inspect(vectoriser, X_train)
```

![](img/173b52987bed04324c67c2ff7852aa91.png)

ç›¸å½“å¿«ã€‚è¾“å‡ºè¶…è¿‡ 97ï¼Œ000 åˆ—ï¼Œå‰ 50 ä¸ªæ ‡è®°å¤§éƒ¨åˆ†æ˜¯æ•°å­—ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è°ƒæ•´ä¸€äº›å‚æ•°æ¥è¿›ä¸€æ­¥æ¸…ç†æ ‡è®°ï¼Œå¹¶æ›´å¥½åœ°æ§åˆ¶é¢„å¤„ç†:

```
# Simpler approach
vectoriser = TfidfVectorizer(token_pattern=r'[a-z]+', stop_words='english', min_df=30, max_df=.7)
inspect(vectoriser, X_train)
```

![](img/619825cbe10166c408b0292ea499d588.png)

è™½ç„¶ä¸Šè¿°ä¸¤ä¸ªç‰ˆæœ¬èŠ±è´¹çš„æ—¶é—´å·®ä¸å¤šï¼Œä½†åè€…çš„åˆ—æ•°è¦å°‘ 7-8 å€ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¦æ±‚ *TfidfVectorizer* åšä¸€äº›äº‹æƒ…:
â—¼ `token_pattern=r'[a-z]+'`:æ ‡è®°æˆå­—æ¯æ ‡è®°â€”â€”è¿™æ„å‘³ç€æˆ‘ä»¬ä¸¢å¼ƒæ•°å­—å’Œæ ‡ç‚¹ç¬¦å·ã€‚å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰[æ­£åˆ™è¡¨è¾¾å¼](https://docs.python.org/3/library/re.html) , `[a-z]+`æ„å‘³ç€ä»¤ç‰Œå¿…é¡»åªç”±å­—æ¯ç»„æˆã€‚
â—¼ `stop_words='englishâ€™`:å»æ‰åœæ­¢å­—ã€‚
â—¼ `min_df=30`:ç§»é™¤ç¨€æœ‰ä»¤ç‰Œã€‚å½“ä¸€ä¸ªä»¤ç‰Œå‡ºç°åœ¨å°‘äº 30 æ¡è¯„è®ºä¸­æ—¶ï¼Œæˆ‘ä»¬è®¤ä¸ºå®ƒæ˜¯ç¨€æœ‰çš„ã€‚è¿™å°†å¤§å¤§å‡å°‘ä»¤ç‰Œçš„æ•°é‡ã€‚å°è¯•åœ¨æ²¡æœ‰è¯¥å‚æ•°çš„æƒ…å†µä¸‹è¿è¡Œè„šæœ¬ï¼Œå¹¶æŸ¥çœ‹ä»¤ç‰Œçš„æ•°é‡ã€‚
â—¼ `max_df=.7`:åˆ é™¤è¶…è¿‡ 70%æ–‡æ¡£ä¸­çš„ä»¤ç‰Œã€‚è¿™æ„å‘³ç€å¦‚æœä¸€ä¸ªä»¤ç‰ŒåŒ…å«åœ¨è¶…è¿‡ 31.5K ä¸ªè¯„è®ºä¸­ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å¿½ç•¥å®ƒä»¬ã€‚å®é™…ä¸Šå¹¶æ²¡æœ‰å¾ˆå¤šå•è¯å› ä¸ºè¿™ä¸ªè€Œè¢«æ’é™¤åœ¨å¤–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç”šè‡³å¯ä»¥ä¿ç•™è¿™ä¸ªç‰¹æ€§çš„é»˜è®¤å€¼ã€‚

`max_df`å’Œ`min_df`æœ‰åŠ©äºä»¤ç‰Œé€‰æ‹©ã€‚æ¢å¥è¯è¯´ï¼Œåœ¨è¿™ä¸¤è€…çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¸¢å¼ƒé‚£äº›æˆ–è€…**å¤ªé¢‘ç¹**å¯èƒ½å¯¹æƒ…æ„Ÿåˆ†ç±»æ²¡æœ‰ç”¨å¤„æˆ–è€…**å¤ªç½•è§**å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆçš„ä»¤ç‰Œã€‚

**ğŸ“è¿™é‡Œæœ‰ä¸€ä¸ªæç¤º:**å¦‚æœæˆ‘ä»¬è¿™æ ·å®šä¹‰å‚æ•°:

```
TfidfVectorizer(token_pattern=r'[a-z]+', max_df=.5)
```

åœ¨åŒ¹é…çŸ¢é‡å™¨ä¹‹åï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œ`vectoriser.stop_words_`æ¥æ‰¾å‡ºå“ªäº›ä»¤ç‰Œç”±äº`max_df=.5`æ¡ä»¶è€Œè¢«æ’é™¤ã€‚å½“æˆ‘ä»¬è°ƒç”¨`inspect`å‡½æ•°æ—¶ï¼Œè¾“å‡ºçš„'*ç¤ºä¾‹è¢«å¿½ç•¥çš„æ ‡è®°'*éƒ¨åˆ†æ˜¾ç¤ºäº†é‚£äº›è¢«æ’é™¤çš„å•è¯çš„ç‰‡æ®µã€‚ç°åœ¨ï¼Œæˆ‘é¼“åŠ±ä½ è¿è¡Œä¸Šé¢çš„è„šæœ¬å¹¶æ£€æŸ¥`vectoriser.stop_words_`ã€‚ä½ çœ‹åˆ°äº†ä»€ä¹ˆï¼Ÿä½ çœ‹åˆ°çš„å¤§å¤šæ˜¯åœç”¨è¯å—ï¼Ÿå°è¯•å°†è¿™äº›å€¼æ›´æ”¹ä¸º. 5ã€. 6ã€. 8 æˆ–. 9ï¼Œç„¶åé‡æ–°è¿è¡Œä»¥è§‚å¯Ÿè¢«æ’é™¤çš„å•è¯å¦‚ä½•å˜åŒ–ã€‚è¿™æœ‰åŠ©äºç†è§£è°ƒæ•´æŸäº›å‚æ•°å¦‚ä½•å½±å“é¢„å¤„ç†ã€‚ç°åœ¨ï¼Œå¦‚æœä½ çƒ­è¡·äºæ·»åŠ `stop_words='englishâ€™`æˆ–`min_df=30`(ä¸è¦åŒæ—¶æ·»åŠ ä¸¤ä¸ªï¼Œä¸€æ¬¡æ·»åŠ ä¸€ä¸ªä»¥äº†è§£å•ä¸ªå‚æ•°)ï¼Œå¹¶æ£€æŸ¥è¿™æ¬¡æ’é™¤äº†å“ªäº›ä»¤ç‰Œã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­æ„å»ºæ¨¡å‹æ—¶è¿›ä¸€æ­¥è°ƒæ•´è¿™äº›å‚æ•°ã€‚

ğŸ”—*å¦‚æœæ‚¨çƒ­è¡·äºäº†è§£æ›´å¤šå…³äºå‚æ•°çš„ä¿¡æ¯ï¼Œè¿™é‡Œæœ‰* [*æ–‡æ¡£*](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) *ã€‚*

è¿™ç§æ–¹æ³•æ˜¯ä¸€ç§ğŸ°ï¼Œæ˜¯ä¸æ˜¯ï¼Ÿæˆ‘å–œæ¬¢è¿™ç§æ–¹æ³•ï¼Œå°¤å…¶æ˜¯å› ä¸ºå®ƒåšä¸€äº›åŸºæœ¬çš„äº‹æƒ…çœŸçš„åˆå¥½åˆå¿«ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œä»ç®€å•å¼€å§‹æ€»æ˜¯å¥½çš„ï¼Œåªæœ‰åœ¨æé«˜æ€§èƒ½çš„æƒ…å†µä¸‹æ‰å¢åŠ å¤æ‚æ€§ã€‚

## 2.2.ç®€å•æ–¹æ³• 2ï¸âƒ£

æ€»æœ‰æ”¹è¿›çš„ä½™åœ°ã€‚ä¾‹å¦‚ï¼Œåœ¨ä»¥å‰çš„æ–¹æ³•ä¸­ï¼Œå•è¯â€œæ’­æ”¾â€ã€â€œæ’­æ”¾â€ã€â€œæ­£åœ¨æ’­æ”¾â€å’Œâ€œå·²æ’­æ”¾â€è¢«è®¤ä¸ºæ˜¯ 4 ç§ä¸åŒçš„æ ‡è®°ã€‚å¦‚æœæˆ‘ä»¬å»æ‰è¿™äº›å±ˆæŠ˜å˜åŒ–çš„è¯å°¾ï¼Œä½¿è¿™äº›ç¬¦å·æ­£å¸¸åŒ–ä¸ºä¸€ä¸ªå”¯ä¸€çš„ç¬¦å·â€œplayâ€ï¼Œè¿™ä¸æ˜¯å¾ˆå¥½å—ï¼Ÿè¿™å°±æ˜¯æˆ‘ä»¬åœ¨è¿™éƒ¨åˆ†è¦åšçš„äº‹æƒ…ï¼

ä¸ºäº†ä½¿å±ˆæŠ˜è¯å°¾æ­£å¸¸åŒ–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§å«åšå¼•ç†æ»¡è¶³çš„æŠ€æœ¯ã€‚å¦ä¸€ç§é€‰æ‹©æ˜¯è¯å¹²ã€‚å¦‚æœä½ æƒ³äº†è§£è¿™ä¸¤è€…çš„åŒºåˆ«ï¼Œæˆ‘åœ¨è¿™é‡Œç®€å•è§£é‡Šäº†ä¸€ä¸‹åŒºåˆ«[ã€‚](/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc)

è¯æ¡é‡Šä¹‰æœ‰åŠ©äºå°†å•è¯è½¬æ¢æˆè¯å…¸å½¢å¼ã€‚ä»¥æˆ‘ä»¬ä¹‹å‰çš„ä¾‹å­ä¸ºä¾‹ï¼Œè®©æˆ‘ä»¬å°†å®ƒä»¬è¿›è¡Œæ¯”è¾ƒï¼Œçœ‹çœ‹è¾“å‡ºæ˜¯ä»€ä¹ˆæ ·çš„:

```
lemmatiser = WordNetLemmatizer()
for word in ['play', 'plays', 'playing', 'played']:
    print(lemmatiser.lemmatize(word, 'v'))
```

![](img/d74b5321507cecf4c01fd4875aec5472.png)

é…·ï¼Œæ‰€æœ‰çš„å•è¯ç°åœ¨éƒ½è½¬æ¢æˆâ€˜ç©â€™äº†ã€‚æ³¨æ„æˆ‘ä»¬å¦‚ä½•åœ¨`lemmatize`æ–¹æ³•ä¸­ä¼ é€’ v ä½œä¸ºç¬¬äºŒä¸ªå‚æ•°ï¼Ÿâ€˜vâ€™æ˜¯ä¸€ä¸ª*è¯æ€§æ ‡ç­¾*ã€‚äº‹å®ä¸Šï¼Œå‡†ç¡®çš„è¯æ±‡åŒ¹é…ä¾èµ–äºæˆ‘ä»¬éšå•è¯ä¸€èµ·æä¾›ç»™è¯æ±‡åŒ¹é…å™¨çš„è¯æ€§(POS)æ ‡ç­¾ã€‚ä¸ºäº†æ¼”ç¤ºè¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬é‡æ–°è¿è¡Œå‰é¢çš„è„šæœ¬ï¼Œåšä¸€ç‚¹å°å°çš„æ”¹åŠ¨ï¼Œå°†â€œvâ€å˜æˆâ€œnâ€:

```
lemmatiser = WordNetLemmatizer()
for word in ['plays', 'playing', 'played']:
    print(lemmatiser.lemmatize(word, 'n'))
```

![](img/11b916e55d719312a846064cca8fcd59.png)

è¿™ä¸€æ¬¡ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„è¯éƒ½è½¬æ¢æˆäº†ç©ã€‚è¿™ä¸ªç®€å•çš„ä¾‹å­å±•ç¤ºäº†è¯æ€§æ ‡ç­¾å¦‚ä½•å½±å“å¼•ç†æ»¡è¶³çš„æœ‰æ•ˆæ€§ã€‚é‚£ä¹ˆåˆ°åº•ä»€ä¹ˆæ˜¯è¯æ€§æ ‡ç­¾å‘¢ï¼Ÿç®€å•åœ°è¯´ï¼Œå®ƒæŒ‡çš„æ˜¯è¿™ä¸ªè¯çš„è¯­æ³•èŒƒç•´ã€‚å•è¯â€œmovieâ€æ˜¯ä¸€ä¸ªåè¯ï¼Œè€Œâ€œwatchâ€æ ¹æ®ä¸Šä¸‹æ–‡å¯ä»¥æ˜¯åŠ¨è¯ä¹Ÿå¯ä»¥æ˜¯åè¯ã€‚åè¯å’ŒåŠ¨è¯éƒ½æ˜¯è¯ç±»çš„ä¾‹å­ã€‚åœ¨ç¬¬ä¸€æ¬¡è¿è¡Œä¸­ï¼Œæˆ‘ä»¬å‘Šè¯‰ lemmatiser æä¾›çš„å•è¯æ˜¯åŠ¨è¯(å› æ­¤æ˜¯â€˜vâ€™)ï¼Œåœ¨ç¬¬äºŒæ¬¡è¿è¡Œä¸­æ˜¯åè¯(å› æ­¤æ˜¯â€˜nâ€™)ã€‚ä½¿ç”¨ *nltk çš„* *è¯æ€§æ ‡æ³¨å™¨:* `pos_tag()`ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è¯æ€§æ¥æ ‡æ³¨æ¯ä¸ªå•è¯ã€‚

æˆ‘ä»¬å°†å¾ˆå¿«åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°ï¼Œæ ¹æ®æ ‡ç­¾è¿›è¡Œè¯æ€§æ ‡æ³¨å’Œè¯æ±‡åŒ¹é…ã€‚æˆ‘ä»¬å°†è¿™ä¸ªå‡½æ•°ä¼ é€’ç»™`TdidfVectoriser()` *çš„`analyzer`å‚æ•°ã€‚*å½“æˆ‘ä»¬è¿™æ ·åšæ—¶ï¼Œä»¥å‰ä½¿ç”¨çš„ä¸€äº›å‚æ•°å¦‚`token_pattern, stop_words`å°†ä¸å†å—æ”¯æŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿˜å¿…é¡»åœ¨è‡ªå®šä¹‰å‡½æ•°ä¸­åŒ…å«ä¸€ä¸ªæ ‡è®°åŒ–æ­¥éª¤:

```
def preprocess_text(text):
    # 1\. Tokenise to alphabetic tokens
    tokeniser = RegexpTokenizer(r'[A-Za-z]+')
    tokens = tokeniser.tokenize(text)

    # 2\. POS tagging
    pos_map = {'J': 'a', 'N': 'n', 'R': 'r', 'V': 'v'}
    pos_tags = pos_tag(tokens)

    # 3\. Lowercase and lemmatise 
    lemmatiser = WordNetLemmatizer()
    tokens = [lemmatiser.lemmatize(t.lower(), pos=pos_map.get(p[0], 'v')) for t, p in pos_tags]return tokens
```

è¿™é‡Œéœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œ`pos_tag()`å’Œ`lemmatiser.lemmatize()`å¯¹è¯æ€§æ ‡ç­¾ä½¿ç”¨ä¸åŒçš„å‘½åçº¦å®šï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»å°†ç”± *nltk æ ‡ç­¾å™¨*ç”Ÿæˆçš„è¯æ€§æ˜ å°„åˆ° lemmatiser èƒ½å¤Ÿç†è§£çš„åç§°ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æœ‰`pos_map`çš„åŸå› ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬é¢„å¤„ç†æ•°æ®å¹¶è¯„ä¼°:

```
vectoriser = TfidfVectorizer(analyzer=preprocess_text, min_df=30, max_df=.7)
inspect(vectoriser, X_train)
```

![](img/aef2ba495c3923ef606450ef3ec78907.png)

å®ƒå°†åˆ—æ•°å‡å°‘åˆ° 10ï¼Œ754ã€‚ä¸*æ›´ç®€å•çš„æ–¹æ³•*ä¸­çš„ 12ï¼Œ805 åˆ—ç›¸æ¯”ï¼Œåˆ—æ•°å‡å°‘äº†çº¦ 16%ã€‚åœ¨æˆ‘çš„ç”µè„‘ä¸ŠèŠ±äº†å¤§çº¦ 11 åˆ†é’Ÿï¼Œæ¯”è¿™æ…¢äº† 85 å€ã€‚å¦‚æœæˆ‘ä»¬ä»”ç»†æƒ³æƒ³ï¼Œè¯æ¡æ»¡è¶³ä¸ä¼šæ”¹å˜è¯„è®ºä¸­çš„æ¯ä¸€ä¸ªè¯ã€‚å°±æ‹¿â€œè¿™éƒ¨ç”µå½±å¤ªæ£’äº†â€æ¥è¯´å§ã€‚å¥å­ä¸ºä¾‹ã€‚å”¯ä¸€å—ç›Šäºå¼•ç†æ»¡è¶³çš„è¯æ˜¯â€œwasâ€ã€‚æ‰€ä»¥è®°ä½è¿™ä¸€ç‚¹ï¼Œå¦‚æœä½ éœ€è¦æ›´å¿«åœ°å®Œæˆè¯æ¡æ»¡è¶³ï¼Œæœ‰æ—¶å¯¹æ‰€æœ‰å•è¯ä½¿ç”¨é»˜è®¤çš„è¯æ€§æ˜¯å¾ˆå¥½çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„è‡ªå®šä¹‰å‡½æ•°ç®€åŒ–ä¸º:

```
# Simple approach
def preprocess_text(text):
    # 1\. Tokenise to alphabetic tokens
    tokeniser = RegexpTokenizer(r'[A-Za-z]+')
    tokens = tokeniser.tokenize(text)

    # 2\. Lowercase and lemmatise 
    lemmatiser = WordNetLemmatizer()
    tokens = [lemmatiser.lemmatize(t.lower(), pos='v') for t in tokens]return tokensvectoriser = TfidfVectorizer(analyzer=preprocess_text, min_df=30, max_df=.7)
inspect(vectoriser, X_train)
```

![](img/c15e92b07cc50e2534fac49624c27578.png)

åˆ—æ•°ä¸æˆ‘ä»¬ä½¿ç”¨çš„è¯æ€§æ ‡æ³¨å™¨éå¸¸æ¥è¿‘ï¼Œä½†åªèŠ±äº†å¤§çº¦ä¸€åˆ†é’Ÿã€‚å› æ­¤ï¼Œåœ¨è¿™ä¸¤ä¸ªç‰ˆæœ¬ä¹‹é—´ï¼Œæˆ‘æ›´å–œæ¬¢å¯¹è¿™ä¸ªæ–‡æœ¬æ•°æ®é›†ä½¿ç”¨é»˜è®¤çš„è¯æ€§æ ‡ç­¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ä¸ç»™`pos`å‚æ•°æä¾›å€¼ï¼Œlemmatiser é»˜è®¤ä¸ºâ€˜nâ€™ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘é€šå¸¸å–œæ¬¢ä½¿ç”¨' v 'ä½œä¸ºé»˜è®¤ï¼Œå› ä¸ºæˆ‘å‘ç°å®ƒåœ¨ä¸€èˆ¬æƒ…å†µä¸‹è§„èŒƒåŒ–æ›´å¤šçš„å•è¯ã€‚ä½†æ˜¯ï¼Œæ›´åˆé€‚çš„é»˜è®¤`pos`å°†å–å†³äºæ•°æ®ã€‚

## 2.3.ä¸é‚£ä¹ˆç®€å•çš„æ–¹æ³• 3ï¸âƒ£

åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å°†è¿›ä¸€æ­¥æ¸…ç†æ•°æ®ã€‚åŸºäºä»æ¢ç´¢æ€§æ•°æ®åˆ†æä¸­è·å¾—çš„çŸ¥è¯†å’Œä¸€èˆ¬é¢„å¤„ç†æ€æƒ³ï¼Œæˆ‘ä»¬å°†åšä»¥ä¸‹å·¥ä½œ:

â—¼çº æ­£é”™åˆ«å­—:*ã€chr acterã€‘*åˆ°*ã€characterã€‘*
â—¼å°†è‹±å¼æ‹¼æ³•è½¬æ¢ä¸ºç¾å¼æ‹¼æ³•:*ã€realizeã€‘*åˆ°*ã€realizeã€‘*
â—¼å»æ‰åœç”¨è¯

```
def convert_to_american(token):
    # Copied from [here](https://scikit-learn.org/stable/modules/feature_extraction.html)
    token = re.sub(r"(...)our$", r"\1or", token)
    token = re.sub(r"([bt])re$", r"\1er", token)
    token = re.sub(r"([iy])s(e$|ing|ation)", r"\1z\2", token)
    token = re.sub(r"ogue$", "og", token)
    return tokendef correct_typo(tokens):
    spell = SpellChecker()
    return [spell.correction(t) if len(spell.unknown([t]))>0 else t for t in tokens]

def preprocess_text(text):
    # 1\. Tokenise to alphabetic tokens
    tokeniser = RegexpTokenizer(r'[A-Za-z]+')
    tokens = tokeniser.tokenize(text)

    # 2\. Lowercase and lemmatise
    lemmatiser = WordNetLemmatizer()
    tokens = [lemmatiser.lemmatize(t.lower(), pos='v') for t in tokens]# 3\. Correct spelling (this won't convert 100% )
    tokens = correct_typo(tokens)

    # 4\. Convert British spelling to American spelling (this won't convert 100%)
    tokens = [convert_to_american(t) for t in tokens]# 5\. Remove stopwords
    stop_words = stopwords.words('english')
    stop_words.extend(['cannot', 'could', 'done', 'let', 'may' 'mayn',  'might',  'must', 'need', 'ought', 'oughtn', 
                       'shall', 'would', 'br'])
    tokens = [t for t in tokens if t not in stop_words]

    return tokens
```

é™¤äº†è¿™äº›ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–å±‚æ¥çº æ­£å’Œæ¸…ç†ã€‚ä½†æ˜¯ï¼Œæ¯ä¸ªé¢å¤–çš„æ­¥éª¤éƒ½ä¼šå¢åŠ å¤æ‚æ€§å’Œè¿è¡Œæ—¶é—´ï¼Œè€Œä¸èƒ½ä¿è¯æ¨¡å‹æ€§èƒ½çš„æé«˜ã€‚åœ¨é¢„å¤„ç†æ–‡æœ¬æ—¶ï¼Œé€šå¸¸å¯ä»¥å°è¯•ä»¥ä¸‹ä¸€äº›æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ä¸æ˜¯ç‰¹åˆ«æœ‰ç”¨ï¼Œä½†åœ¨å…¶ä»–ä¾‹å­ä¸­å¯èƒ½æœ‰ç”¨:

â—¼æ¸…ç†åƒåœ¾ï¼Œå¦‚ html æ ‡ç­¾ã€ç”µå­é‚®ä»¶åœ°å€ã€ç½‘å€
â—¼å°†æ•°å­—è½¬æ¢æˆæ–‡å­—ï¼Œè€Œä¸æ˜¯ä¸¢å¼ƒå®ƒä»¬
â—¼å°†è¡¨æƒ…ç¬¦å·æˆ–è¡¨æƒ…ç¬¦å·è½¬æ¢æˆæ–‡å­—

å¥½äº†ï¼Œè®©æˆ‘ä»¬é¢„å¤„ç†å¹¶æ£€æŸ¥è¾“å‡º:

```
vectoriser = TfidfVectorizer(analyzer=preprocess_text, min_df=30, max_df=.7)
inspect(vectoriser, X_train)
```

åœ¨æˆ‘å¼€å§‹è¿è¡Œè¿™æ®µä»£ç çš„å‡ ä¸ªå°æ—¶åï¼Œå®ƒä»ç„¶åœ¨è¿è¡Œã€‚å› ä¸ºæˆ‘è®¤ä¸ºå®ƒèŠ±è´¹çš„æ—¶é—´å¤ªé•¿ï¼Œæ‰€ä»¥æˆ‘ä¸å¾—ä¸ä¸­æ–­å†…æ ¸æ¥åœæ­¢æŸ¥è¯¢ã€‚ä¸ºäº†äº†è§£è¿™ç§æ–¹æ³•æ¯”å‰ä¸¤ç§æ–¹æ³•æ…¢å¤šå°‘ï¼Œæˆ‘ä½¿ç”¨ä¸‹é¢çš„è„šæœ¬å°†æ•°æ®é›†çš„å¤§å°å‡å°‘åˆ°å…¶å¤§å°çš„ 1/9:

```
train = train.sample(5000, random_state=seed)
```

æˆ‘åœ¨è¿™ä¸ªæ›´å°çš„æ•°æ®å­é›†ä¸Šè¿è¡Œäº†æ‰€æœ‰ä¸‰ç§æ–¹æ³•ã€‚ä¸‹é¢æ˜¯ä¸‰ç§æ–¹æ³•çš„æ¯”è¾ƒ:

![](img/c5c3db88ea39a0736edad89f60a3cef7.png)

*ä¸å¤ªç®€å•çš„æ–¹æ³•*æ¯”å…¶ä»–ä¸¤ç§æ–¹æ³•æ›´æ ‡å‡†åŒ–ä»£å¸ï¼Œä½†æˆæœ¬éå¸¸é«˜ã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œé¢„å¤„ç†è‡³å°‘è¦èŠ±è´¹ 1000 å€çš„æ—¶é—´ã€‚å½“æ•°æ®é›†å¢é•¿æ—¶ï¼Œè¿™ä¸ªæ¯”ç‡å¯èƒ½ä¼šå˜å¾—æ›´ç³Ÿã€‚å› æ­¤ï¼Œè¿›ä¸€æ­¥è¿½æ±‚*ä¸é‚£ä¹ˆç®€å•çš„æ–¹æ³•*æ˜¯ä¸å®é™…çš„ï¼Œé™¤éå®ƒè¢«ä¼˜åŒ–ä¸ºè¿è¡Œæ›´å¿«ã€‚

ğŸ‘‚å¦‚æœä½ æœ‰ä¸€å°æ¯”æˆ‘æ›´å¥½çš„è®¡ç®—æœºï¼Œè¿™ä¸ªè¿è¡Œæ—¶é—´é—®é¢˜å¯èƒ½ä¸ä¸€å®šä¼šæˆä¸ºä½ çš„ä¸€ä¸ªé™åˆ¶ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥è‡ªç”±åœ°ç»§ç»­è¿½æ±‚ã€‚

![](img/7674b2ec8affa7bcc713f7ff6ccb74b7.png)

[å’Œ](https://unsplash.com/@andyoneru?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šçš„åˆå½±

*æ‚¨æƒ³è®¿é—®æ›´å¤šè¿™æ ·çš„å†…å®¹å—ï¼Ÿåª’ä½“ä¼šå‘˜å¯ä»¥æ— é™åˆ¶åœ°è®¿é—®åª’ä½“ä¸Šçš„ä»»ä½•æ–‡ç« ã€‚å¦‚æœæ‚¨ä½¿ç”¨* [*æˆ‘çš„æ¨èé“¾æ¥*](https://zluvsand.medium.com/membership)*æˆä¸ºä¼šå‘˜ï¼Œæ‚¨çš„ä¸€éƒ¨åˆ†ä¼šè´¹å°†ç›´æ¥ç”¨äºæ”¯æŒæˆ‘ã€‚*

è°¢è°¢ä½ çœ‹æˆ‘çš„å¸–å­ã€‚å¸Œæœ›æ‚¨å·²ç»å­¦ä¼šäº†ä¸€äº›ä¸åŒçš„é¢„å¤„ç†æ–‡æœ¬çš„å®ç”¨æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨åˆ°æ‚¨çš„ä¸‹ä¸€ä¸ª NLP é¡¹ç›®ä¸­ã€‚åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªæƒ…æ„Ÿåˆ†ç±»å™¨ã€‚ä»¥ä¸‹æ˜¯è¯¥ç³»åˆ—å¦å¤–ä¸¤ç¯‡æ–‡ç« çš„é“¾æ¥:â—¼ï¸[python ä¸­çš„æ¢ç´¢æ€§æ–‡æœ¬åˆ†æ](/exploratory-text-analysis-in-python-8cf42b758d9e)
â—¼ï¸[python ä¸­çš„æƒ…æ„Ÿåˆ†ç±»](/sentiment-classification-in-python-da31833da01b)

ä»¥ä¸‹æ˜¯æˆ‘çš„å…¶ä»– NLP ç›¸å…³å¸–å­çš„é“¾æ¥:
â—¼ï¸[Python ä¸­çš„ç®€å• word cloud](/simple-wordcloud-in-python-2ae54a9f58e5)
*(ä¸‹é¢åˆ—å‡ºäº†ä¸€ç³»åˆ—å…³äº NLP ä»‹ç»çš„å¸–å­)*
â—¼ï¸ [ç¬¬ä¸€éƒ¨åˆ†:Python ä¸­çš„é¢„å¤„ç†æ–‡æœ¬](/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96)
â—¼ï¸ [ç¬¬äºŒéƒ¨åˆ†:è¯æ¡æ»¡è¶³å’Œè¯å¹²çš„åŒºåˆ«](https://medium.com/@zluvsand/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc)
â—¼ï¸ [ç¬¬ä¸‰éƒ¨åˆ†:TF-IDF è§£é‡Š](https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc)
â—¼ï¸ [ç¬¬å››éƒ¨åˆ†:python ä¸­çš„ç›‘ç£æ–‡æœ¬åˆ†ç±»æ¨¡å‹](https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267)

å†è§ğŸƒğŸ’¨