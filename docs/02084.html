<html>
<head>
<title>Pros and cons of various Machine Learning algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">各种机器学习算法的利弊</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pros-and-cons-of-various-classification-ml-algorithms-3b5bfb3c87d6?source=collection_archive---------1-----------------------#2020-02-28">https://towardsdatascience.com/pros-and-cons-of-various-classification-ml-algorithms-3b5bfb3c87d6?source=collection_archive---------1-----------------------#2020-02-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d35a8f08fefbac87964d0fc71cd0fd64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntdyE4PiFxIc34ikHBMkPg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:Pixabay</p></figure><div class=""/><p id="79d8" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">机器学习中有很多分类算法。但是曾经想知道哪种算法应该用于什么目的和什么类型的应用。如果是，那么请阅读分类中使用的各种机器学习算法的利弊。我还列出了它们的用例及应用。</p><h1 id="ce8f" class="ld le ji bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">SVM(支持向量机)</h1><p id="b21e" class="pw-post-body-paragraph kf kg ji kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated"><strong class="kh jj">优点</strong></p><ol class=""><li id="b6b8" class="mg mh ji kh b ki kj km kn kq mi ku mj ky mk lc ml mm mn mo bi translated"><strong class="kh jj">在更高维度表现出色。</strong>在现实世界中有无限的维度(不仅仅是 2D 和 3D)。例如图像数据、基因数据、医学数据等。有更高的维度，SVM 在这点上很有用。基本上，当特征/列的数量较高时，SVM 做得很好</li></ol><p id="3972" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">类可分时的最佳算法。</strong>(当两个类的实例可以很容易地用直线或非线性分开时)。为了描述可分离的类，让我们举一个例子(这里以线性分离为例，类也可以是非线性可分离的，例如通过画一条抛物线等)。在第一张图中，你很难判断 X 是属于第一类还是第二类，但是在第二种情况下，你很容易判断出 X 属于第二类。因此，在第二种情况下，类是线性可分的。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/c3cbe7ec57e6c62222a42a7ee199aad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/0*mxdeoWu0p602Jj38.png"/></div></figure><p id="bdae" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">首先是不可分的类，其次是可分的类。</p><p id="1dad" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">离群值</strong>影响较小。</p><p id="a53f" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">4.SVM 适合极端情况下的二元分类。</p><p id="4763" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">缺点:</strong></p><p id="ee3f" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">1.<strong class="kh jj">慢:</strong>对于较大的数据集，需要大量的时间来处理。</p><p id="8c62" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">重叠类的性能不佳</strong>:在重叠类的情况下性能不佳。</p><p id="31a9" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">选择合适的超参数很重要:</strong>这将考虑到足够的泛化性能。</p><p id="0414" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">4.选择合适的内核函数可能很棘手。</p><p id="21e0" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">应用:</strong></p><p id="aeeb" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">单词包应用(许多特征和列)、语音识别数据、图像分类(非线性数据)、医学分析(非线性数据)、文本分类(许多特征)</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="2bd4" class="ld le ji bd lf lg nb li lj lk nc lm ln lo nd lq lr ls ne lu lv lw nf ly lz ma bi translated">朴素贝叶斯</h1><p id="73ae" class="pw-post-body-paragraph kf kg ji kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated"><strong class="kh jj">赞成者</strong></p><ol class=""><li id="14db" class="mg mh ji kh b ki kj km kn kq mi ku mj ky mk lc ml mm mn mo bi translated"><strong class="kh jj">实时</strong>预测:非常快，可以实时使用。</li></ol><p id="9092" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">可扩展</strong>大型数据集</p><p id="4b68" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">对无关特征不敏感。</strong></p><p id="e53e" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">4.在朴素贝叶斯中有效地完成了多类预测</p><p id="5c95" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">5.<strong class="kh jj">高维数据的良好性能</strong>(特征数量大)</p><p id="744c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">缺点</strong></p><ol class=""><li id="117b" class="mg mh ji kh b ki kj km kn kq mi ku mj ky mk lc ml mm mn mo bi translated"><strong class="kh jj">特征的独立性不成立:</strong>基本的朴素贝叶斯假设是每个特征对结果做出独立且相等的贡献。然而，这个条件在大多数情况下并不满足。</li></ol><p id="6ed4" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">不好的估计量:【predict _ proba 的概率输出不要太认真。</strong></p><p id="e603" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">训练数据应能很好地代表总体:</strong>如果某个类别标签和某个属性值没有同时出现(例如 class="No "，shape = " govern ")，那么后验概率将为零。因此，如果训练数据不代表总体，朴素贝叶斯就不能很好地工作。(这个问题可以通过平滑技术来解决)。</p><figure class="mq mr ms mt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ng"><img src="../Images/2df8d3163a04f2051cc5d8140d5f9e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yNXK_vC7X02AyPCS.png"/></div></div></figure><p id="ee64" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">应用:</strong></p><p id="0f2a" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">朴素贝叶斯用于文本分类/垃圾邮件过滤/情感分析。它用于文本分类(它可以对多个类别进行预测，并且不介意处理不相关的特征)、垃圾邮件过滤(识别垃圾邮件)和情感分析(在社交媒体分析中，用于识别积极和消极的情感)、推荐系统(用户接下来会购买什么)</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="132c" class="ld le ji bd lf lg nb li lj lk nc lm ln lo nd lq lr ls ne lu lv lw nf ly lz ma bi translated">逻辑回归</h1><p id="b827" class="pw-post-body-paragraph kf kg ji kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated"><strong class="kh jj">优点</strong></p><ol class=""><li id="e208" class="mg mh ji kh b ki kj km kn kq mi ku mj ky mk lc ml mm mn mo bi translated"><strong class="kh jj">简单的</strong>实现</li></ol><p id="d60f" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">有效</strong></p><p id="da73" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">不需要要素缩放:</strong>不需要缩放输入要素(也可以处理缩放后的要素，但不需要缩放)</p><p id="73f7" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">不需要调整超参数。</strong></p><p id="eb19" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">缺点</strong></p><ol class=""><li id="33b9" class="mg mh ji kh b ki kj km kn kq mi ku mj ky mk lc ml mm mn mo bi translated"><strong class="kh jj">非线性数据的性能不佳</strong>(例如图像数据)</li></ol><p id="eca7" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">不相关和高度相关特征表现不佳</strong>(使用博鲁塔图去除相似或相关特征和不相关特征)。</p><p id="4acd" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">不是很强大的</strong>算法，很容易被其他算法超越。</p><p id="96c4" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">4.<strong class="kh jj">高度依赖数据的正确呈现</strong>。所有重要的变量/特征都应该被识别，以使其工作良好。</p><p id="b086" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">应用:</strong></p><p id="2d26" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">任何分类问题，最好是二进制的(它也可以执行多类分类，但二进制是首选)。例如，如果你的输出类有两个结果，你可以使用它；癌症检测问题，学生是否会通过/失败，在客户贷款的情况下违约/不违约，客户是否会流失，电子邮件是否是垃圾邮件等。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="2a70" class="ld le ji bd lf lg nb li lj lk nc lm ln lo nd lq lr ls ne lu lv lw nf ly lz ma bi translated">随机森林</h1><p id="bfad" class="pw-post-body-paragraph kf kg ji kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated"><strong class="kh jj">优点:</strong></p><p id="649b" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">1.随机森林可以<strong class="kh jj">去相关树</strong>。它挑选训练样本，并给每棵树一个特征子集(假设训练数据是[1，2，3，4，5，6]，因此一棵树将得到训练数据的子集[1，2，3，2，6，6]。注意，训练数据的大小保持不变，两个数据都具有长度 6，并且特征‘2’和特征‘6’在给予一棵树的随机采样的训练数据中重复。每棵树都根据它所具有的特征进行预测。在这种情况下，树 1 只能访问特征 1、2、3 和 6，因此它可以基于这些特征进行预测。其他一些树将访问特征 1、4、5，因此它将根据这些特征进行预测。如果特征高度相关，那么问题可以在随机森林中解决。</p><p id="f8c6" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">减少误差:</strong>随机森林是决策树的集合。为了预测特定行的结果，随机森林从所有树中获取输入，然后预测结果。这确保了树的单个误差被最小化，并且总体方差和误差被减少。</p><p id="275c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">在不平衡数据集上表现良好</strong>:也可以处理不平衡数据中的错误(一个类是多数，另一个类是少数)</p><p id="5464" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">4.<strong class="kh jj">海量数据的处理:</strong>可以处理变量维度更高的海量数据。</p><p id="1979" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">5.<strong class="kh jj">很好的处理缺失数据:</strong>可以很好的处理缺失数据。因此，如果你的模型中有大量的缺失数据，它会给出很好的结果。</p><p id="266e" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">6.<strong class="kh jj">离群值的影响很小:</strong>由于最终结果是通过咨询多个决策树得出的，因此离群值的某些数据点不会对随机森林产生很大影响。</p><p id="b318" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">7.<strong class="kh jj">没有过拟合的问题:</strong>在随机森林中只考虑特征的子集，最终的结果取决于所有的树。所以泛化多，过拟合少。</p><p id="4e59" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">8.<strong class="kh jj">对提取特征重要性有用</strong>(我们可以用它来进行特征选择)</p><p id="901b" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">缺点:</strong></p><p id="6e7c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">1.<strong class="kh jj">特性</strong>需要有<strong class="kh jj">一些预测能力</strong>否则它们不会工作。</p><p id="5cc0" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">树的预测需要不相关</strong>。</p><p id="f5fd" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">以黑匣子的形式出现:</strong>很难知道发生了什么。您最多可以尝试不同的参数和随机种子来改变结果和性能。</p><p id="b15c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">应用</strong>:</p><p id="9682" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">信用卡违约，欺诈客户/不是，容易识别病人的疾病与否，电子商务网站的推荐系统。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="a1d0" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">决策树</strong></p><p id="bcf9" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">优点</strong></p><p id="d87c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">1.<strong class="kh jj">不需要数据的标准化或缩放</strong>。</p><p id="67bf" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">处理缺失值</strong>:缺失值影响不大。</p><p id="5d2f" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">便于向非技术团队成员解释</strong>。</p><p id="3c0c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">4.<strong class="kh jj">轻松可视化</strong></p><p id="dda0" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">5.<strong class="kh jj">自动</strong> <strong class="kh jj">特征选择</strong>:无关特征不会影响决策树。</p><p id="1988" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">缺点</strong></p><p id="9826" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">1.<strong class="kh jj">容易过度拟合。</strong></p><p id="97a5" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">对数据敏感。</strong>如果数据稍有变化，结果可能会发生很大变化。</p><p id="585a" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">训练</strong>决策树所需的时间更长。</p><p id="3a6d" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">应用</strong>:</p><p id="442c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">确定产品的购买者，预测违约的可能性，哪种策略可以使利润最大化，找到成本最小化的策略，哪些特征对吸引和留住客户最重要(是购物的频率，是频繁的折扣，是产品组合等)，机器的故障诊断(持续测量压力、振动和其他措施，并在故障发生前进行预测)等。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="e51a" class="ld le ji bd lf lg nb li lj lk nc lm ln lo nd lq lr ls ne lu lv lw nf ly lz ma bi translated">XGBoost</h1><p id="6d7e" class="pw-post-body-paragraph kf kg ji kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated"><strong class="kh jj">赞成者</strong></p><p id="5741" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">1.<strong class="kh jj">需要较少的特征工程</strong>(不需要缩放、归一化数据，也可以很好地处理缺失值)</p><p id="d820" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">可以求出特征重要性</strong>(它输出每个特征的重要性，可以用于特征选择)</p><p id="feae" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">快速</strong>解读</p><p id="464f" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">4.<strong class="kh jj">离群值</strong>影响最小。</p><p id="d015" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">5.<strong class="kh jj">很好地处理大规模数据集</strong>。</p><p id="a1e8" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">6.<strong class="kh jj">执行好</strong>速度</p><p id="536b" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">7.<strong class="kh jj">良好的模特表现</strong>(赢得大部分卡格尔比赛)</p><p id="00c0" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">8.<strong class="kh jj">不易过度拟合</strong></p><p id="e806" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">缺点</strong></p><p id="6673" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">1.<strong class="kh jj">难解读</strong>，可视化难</p><p id="9535" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">如果参数调整不当，可能会出现过拟合</strong>。</p><p id="2709" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">由于超参数太多，更难调整</strong>。</p><p id="0185" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">应用</strong></p><p id="7040" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">任何分类问题。如果您有太多的要素和太大的数据集，存在异常值，有许多丢失的值，并且您不想进行太多的要素工程，这将非常有用。它赢得了几乎所有的比赛，所以这是一个你在解决任何分类问题时必须牢记在心的算法。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="a218" class="ld le ji bd lf lg nb li lj lk nc lm ln lo nd lq lr ls ne lu lv lw nf ly lz ma bi translated">K 近邻</h1><p id="8fce" class="pw-post-body-paragraph kf kg ji kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated"><strong class="kh jj">优点</strong></p><p id="74de" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">1.<strong class="kh jj">简单的</strong>理解和实现</p><p id="13e6" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">没有关于数据的假设</strong>(例如，在线性回归的情况下，我们假设因变量和自变量线性相关，在朴素贝叶斯中，我们假设特征彼此独立，等等。，但是 k-NN 对数据不做任何假设)</p><p id="2603" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">不断进化的</strong>模型:当它暴露于新数据时，它会改变以适应新的数据点。</p><p id="81ab" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">4.<strong class="kh jj">多类</strong>问题也可以解决。</p><p id="498c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">5.<strong class="kh jj">一个超参数:</strong> K-NN 在选择第一个超参数时可能会花费一些时间，但在此之后，其余的参数会与之对齐。</p><p id="220f" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">缺点</strong></p><p id="df19" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">1.<strong class="kh jj">慢速</strong>用于大型数据集。</p><p id="3617" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">2.<strong class="kh jj">维数灾难</strong>:在具有大量特征的数据集上效果不佳。</p><p id="014b" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">3.<strong class="kh jj">数据绝对必须的缩放</strong>。</p><p id="df64" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">4.<strong class="kh jj">不适用于不平衡数据。</strong>因此，在使用 k-NN 之前，要么欠采样多数类，要么过采样少数类，并拥有一个平衡的数据集。</p><p id="3299" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">5.对<strong class="kh jj">异常值</strong>敏感。</p><p id="f6d5" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">6.无法很好地处理<strong class="kh jj">缺失值</strong></p><p id="8fb6" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">应用:</strong></p><p id="1d8a" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">当数据集较小且具有较少数量的特征时，可以将其用于任何分类问题，因此 k-NN 花费的计算时间较少。如果您不知道数据的形状以及输出和输入相关的方式(类是否可以由直线、椭圆或抛物线等分隔)。)，那就可以用 k-NN 了。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="eb10" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">结论</strong></p><p id="20da" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">随着模型的出现，它们变得越来越先进。因此，如果你严格从性能的角度来看一个模型，那么通常是神经网络、XGBoost 等。是最好的模型，因为它们相对较新。然而，不同的模型更适合不同的数据。</p><p id="2794" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">例如，如果特征是高度独立的，那么朴素贝叶斯将非常有用。如果有太多的要素并且数据集是中等大小的，那么 SVM 就不错。如果因变量和自变量之间存在线性关系，那么线性回归、logistic 回归、SVM 都是好的。如果数据集很小，并且您不知道因变量和自变量之间的关系，那么您可以使用 k-NN。因此，在你决定使用哪种最大似然算法之前，你必须了解并分析数据。如果您无法专注于一种机器学习算法，那么您可以评估所有模型，并在训练和测试集上检查它们的准确性，然后最终确定一个模型。</p></div></div>    
</body>
</html>