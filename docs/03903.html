<html>
<head>
<title>Ten More AI Papers to Read in 2020</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2020年再看10篇人工智能论文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ten-more-ai-papers-to-read-in-2020-8c6fb4650a9b?source=collection_archive---------16-----------------------#2020-04-11">https://towardsdatascience.com/ten-more-ai-papers-to-read-in-2020-8c6fb4650a9b?source=collection_archive---------16-----------------------#2020-04-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="93e0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">更新的阅读建议让你了解人工智能和数据科学的最新和经典突破</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/021b551402d98c3ca4f48b8bf8dc462e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_PW-NLDX0brtd0TS"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">苏珊·尹在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="943f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">答</span>几周前，我发布了一篇关于人工智能(AI)论文的文章，将在2020年阅读。如果算上所有的附加阅读建议，总计27篇。然而，这份清单还远未完成。许多宝石被遗漏或只是简单提及。在这篇文章中，为了你的阅读乐趣，我列出了今年人工智能论文的十个建议(和其他几个进一步阅读的建议)。</p><p id="0926" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个列表中，我主要关注那些没有提出新架构的文章。这不会是最近的YOLO或ResNet变种。相反，它强调了损失公式、理论突破、更新的优化器等方面的最新进展。</p><p id="227c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">至于<a class="ae ky" rel="noopener" target="_blank" href="/ai-papers-to-read-in-2020-ac0e4e91d915">上一个列表</a>，我将重点介绍计算机视觉和NLP，因为这些是我最熟悉的话题，并从一两个经典开始。对于每一篇论文，我都会给出其主要贡献的摘要和阅读它的理由列表。最后，我在每篇文章的结尾都给出了关于该主题的具体阅读建议，并将其与其他最新进展或类似观点联系起来。</p><p id="828e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们继续:)</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="4e30" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">排名第一的手套(2014)</h1><blockquote class="nd ne nf"><p id="dffd" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">彭宁顿、杰弗里、理查德·索彻和克里斯托弗·d·曼宁。<a class="ae ky" href="https://www.aclweb.org/anthology/D14-1162.pdf" rel="noopener ugc nofollow" target="_blank">“手套:单词表示的全局向量。”</a><em class="it">2014自然语言处理经验方法会议论文集</em>。2014.</p></blockquote><p id="0b79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然今天的社区非常关注神经网络，但许多早期结果是通过简单得多的数学获得的。从经典算法开始，GloVe是一个基于降低单词共现矩阵维度的单词嵌入模型。与以前的方法不同，GloVe使用隐式公式，这使得它可以扩展到大规模文本语料库。</p><p id="5c54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#1: </strong>如果你从自然语言处理(NLP)开始，这是一个很好的阅读材料，可以了解单词嵌入的基础知识以及它们为什么重要。</p><p id="cf63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">理由#2: </strong>曾几何时，并不是一切都以<a class="ae ky" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need" rel="noopener ugc nofollow" target="_blank">变形金刚</a>为原型。阅读早期的作品是找到那个“被遗忘的想法”的一个极好的方式，这个想法可以将艺术水平推得更远。</p><p id="9944" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因3: </strong>本文中提到/提出的许多概念后来被许多其他作者扩展。今天，单词嵌入是自然语言处理(NLP)文献中的主要内容。</p><p id="ad80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读:</strong>大约在同一时期，Google发布了<a class="ae ky" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>，另一个众所周知的生成语义向量的模型。不久之后，这些想法被生物学界采用，作为表示大蛋白质和基因序列的方法。今天，<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>是单词表示和语义理解的主导人物。</p><h1 id="4096" class="ml mm it bd mn mo nk mq mr ms nl mu mv jz nm ka mx kc nn kd mz kf no kg nb nc bi translated">#2 AdaBoost (1997年)</h1><blockquote class="nd ne nf"><p id="f8ba" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">弗氏，Yoav罗伯特·沙皮雷(1997年)。<a class="ae ky" href="https://www.sciencedirect.com/science/article/pii/S002200009791504X" rel="noopener ugc nofollow" target="_blank">在线学习的决策理论概括和boosting的应用</a>。</p></blockquote><p id="127d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">经典的机器学习模型一点也不灵活。大多数配方都有惊人的局限性，这使得它们无法扩展到越来越复杂的任务。这个问题的第一个解决方案是将最佳可用模型集成到民主投票中。1997年，<em class="ng"> Freund </em>和<em class="ng"> Schapire </em>提出了AdaBoost算法，这是一种元启发式学习器，能够将许多“弱”模型转化为“强”分类器。</p><p id="9fc3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，该算法基于迭代训练更多的分类器，并将每个训练样本重新加权为“容易”或“困难”随着训练的进行，集合通过更多地关注更难分类的样本而进化。该算法非常有效，以至于即使是复杂的问题也容易过度拟合。</p><p id="5ac2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理由#1: 可以认为神经网络是弱分类器(神经元/层)的集合。然而，神经网络文献已经独立于整体而发展。读一篇关于这个主题的论文可能会对神经网络为什么工作得这么好有一些见解。</p><p id="d66a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#2: </strong>许多新手认为经典的机器学习方法过时且“薄弱”，在几乎所有事情上都倾向于使用神经网络。AdaBoost是经典机器学习一点也不弱的一个很好的例子。而且，与网络不同，这些模型是高度可解释的。</p><p id="1e04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#3: </strong>有多少论文是从一个赌徒因与朋友赌马屡输而沮丧的故事开始的？我希望我敢开一份那样的报纸。</p><p id="26ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读:</strong>其他流行的集成方法是<a class="ae ky" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">随机森林</a>分类器、<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">梯度增强</a>技术，以及广受好评的<a class="ae ky" href="https://github.com/dmlc/xgboost" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>包，这些方法因在几个机器学习比赛中获胜而闻名，同时相对容易使用和调整。该家族的最新成员是微软的<a class="ae ky" href="https://github.com/microsoft/LightGBM" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>，面向大规模分布式数据集。</p><h1 id="9a2f" class="ml mm it bd mn mo nk mq mr ms nl mu mv jz nm ka mx kc nn kd mz kf no kg nb nc bi translated">第三大胶囊网络(2017年)</h1><blockquote class="nd ne nf"><p id="1d83" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">萨布尔、萨拉、尼古拉斯·弗洛斯特和杰弗里·e·辛顿。<a class="ae ky" href="https://arxiv.org/abs/1710.09829" rel="noopener ugc nofollow" target="_blank">“胶囊间动态路由”</a> <em class="it">神经信息处理系统的进展</em>。2017.</p></blockquote><p id="98c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络文献从感知器模型开始，并到达卷积神经网络(CNN)。下一次大跃进是一个备受争议的话题。Sara Sabour、Nicholas Frosst和图灵奖获得者Geoffrey Hinton提出的胶囊网络就是其中之一。</p><p id="3462" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解胶囊网络的一个简单方法是用“胶囊”代替“物体探测器”每一层“对象检测器”试图识别图像中的相关特征，以及其姿态(方向、比例、倾斜等)。).通过堆叠检测器，可以得到物体的鲁棒表示。本质上，胶囊并不像CNN那样将本地信息聚集到高级特征上。取而代之的是，他们检测对象的各个部分，并对它们进行分层组合，以识别更大的结构和关系。</p><p id="8014" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理由1: 作为科学家，我们都应该寻找下一件大事。虽然我们不能说胶囊网络将成为下一个摇滚明星，但我们可以说他们试图解决的问题是相关的。并且，至于所有相关的问题，最终都会有人来回答。</p><p id="35ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">理由#2: </strong>这篇论文提醒我们，CNN并不完美。它们对于旋转和缩放不是不变的。虽然我们使用数据增强来缓解这种情况，但没有创可贴曾经治愈过一个人。</p><p id="934f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#3: </strong>在深度学习成为主流之前，许多对象检测方法依赖于识别容易发现的<a class="ae ky" href="https://en.wikipedia.org/wiki/Part-based_models" rel="noopener ugc nofollow" target="_blank">对象部分</a>，并针对<a class="ae ky" href="https://en.wikipedia.org/wiki/Constellation_model" rel="noopener ugc nofollow" target="_blank">数据库/本体进行模式匹配</a>。Hinton和他的团队正在做的是使这种早期的方法现代化。这就是为什么我们都应该时不时地阅读经典著作。很多东西都可以更新。</p><p id="3356" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读:</strong>在过去的一年中，有一件事得到了很多关注，那就是<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力机制</a>。虽然它没有试图取代或增加卷积，但它确实为全局推理提供了一个途径，这是现代网络的许多阿基里斯脚跟之一。</p><h1 id="d535" class="ml mm it bd mn mo nk mq mr ms nl mu mv jz nm ka mx kc nn kd mz kf no kg nb nc bi translated">#4关系归纳偏差(2018)</h1><blockquote class="nd ne nf"><p id="8da6" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">彼得·w·巴塔格利亚等人<a class="ae ky" href="https://arxiv.org/pdf/1806.01261.pdf" rel="noopener ugc nofollow" target="_blank">“关系归纳偏差、深度学习和图形网络”</a> <em class="it"> arXiv预印本arXiv:1806.01261 </em> (2018)。</p></blockquote><p id="5c61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">部分立场文件，部分评论，部分统一，这篇文章总结了Deep Mind团队认为深度学习中的下一件大事:图形神经网络(GNNs)。用作者自己的话说:</p><blockquote class="nd ne nf"><p id="bc14" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">(…).我们认为，组合概括必须是人工智能实现类似人类能力的首要任务，而结构化表示和计算是实现这一目标的关键。正如生物学合作利用先天和后天，我们拒绝在“手工工程”和“端到端”学习之间的错误选择，而是提倡一种从它们的互补优势中受益的方法。我们探索了在深度学习架构中使用关系归纳偏差如何促进对实体、关系和组成它们的规则的学习。(…)</p></blockquote><p id="0ead" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">旁注:</strong>归纳偏差是学习算法对数据做出的所有假设。例如，线性模型假设数据是线性的。如果一个模型假设数据有特定的关系，它就有一个<em class="ng">关系归纳偏差</em>。因此，图形是一种有用的表示。</p><p id="30f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#1: </strong>当前的CNN模型是“端到端”的，这意味着它们处理原始的、大部分未经处理的数据。特征不是由人类“设计”的，而是由算法自动“学习”的。我们大多数人被告知特征学习更好。在本文中，作者提供了相反的观点。</p><p id="2222" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">原因2: 大多数早期的人工智能文献都与计算推理有关。然而，计算直觉占了上风。神经网络不考虑输入；它们会产生相当准确的数学“预感”图表可能是通向直觉推理的桥梁。</p><p id="2c90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因3: </strong>组合问题可以说是计算机科学中最关键的问题。大多数都处于我们认为易处理或可能的边缘(或更远)。然而，我们人类自然地、毫不费力地进行推理。图形神经网络可能是答案吗？</p><p id="faaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读:</strong> GNNs是一个令人兴奋且不断发展的领域。从图论中，我们知道几乎任何东西都可以被建模为图。<a class="np nq ep" href="https://medium.com/u/b60b4c7e3bfc?source=post_page-----8c6fb4650a9b--------------------------------" rel="noopener" target="_blank">谢尔盖·伊万诺夫</a>在GNNs 中列出了一份极好的<a class="ae ky" rel="noopener" target="_blank" href="/top-trends-of-graph-machine-learning-in-2020-1194175351a3">新趋势列表，其中引用了大量来自即将到来的2020年ICLR会议的论文。</a></p><div class="nr ns gp gr nt nu"><a rel="noopener follow" target="_blank" href="/top-trends-of-graph-machine-learning-in-2020-1194175351a3"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd iu gy z fp nz fr fs oa fu fw is bi translated">2020年图形机器学习的主要趋势</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">2020年刚刚开始，但我们已经可以在最新的研究中看到图形机器学习(GML)的趋势…</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">towardsdatascience.com</p></div></div><div class="od l"><div class="oe l of og oh od oi ks nu"/></div></div></a></div><h1 id="161e" class="ml mm it bd mn mo nk mq mr ms nl mu mv jz nm ka mx kc nn kd mz kf no kg nb nc bi translated">#5训练批次标准和唯一批次标准(2020年)</h1><blockquote class="nd ne nf"><p id="f6ef" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">Frankle，Jonathan，David J. Schwab，Ari S. Morcos .<a class="ae ky" href="https://arxiv.org/abs/2003.00152" rel="noopener ugc nofollow" target="_blank">“训练批处理范式和唯一批处理范式:论细胞神经网络中随机特征的表达能力”</a> <em class="it"> arXiv预印本arXiv:2003.00152 </em> (2020)。</p></blockquote><p id="7e73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您相信仅ResNet-151的批量标准化层就能在CIFAR-10上实现+60%的准确率吗？换句话说，如果你锁定所有其他层的随机初始权重，并训练网络五十个左右的时期，它将比随机的表现更好。我不得不复制这张纸来亲眼看看。“魔力”来自于经常被遗忘的批处理规范的γ和β参数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/21b77285fd155db0ed2ecc56dfebacab.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/0*cFDWlTll0-fFZlPa.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">批处理规范化操作的完整定义。γ和β是两个可学习的参数，允许图层在标准化发生后缩放和移动每个激活图。</p></figure><p id="5db7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理由1: 这是一个足够疯狂的想法，值得一读。打破常规的想法总是受欢迎的。</p><p id="30da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#2: </strong>你可能会问自己批处理规范层怎么能学到任何东西，你可能还会想为什么有人会关心这个。对于数据科学中的很多东西，我们认为批量范数是理所当然的。我们相信这只会加速训练。显然，它可以做得更多。</p><p id="cc37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#3: </strong>本文可能会引起您的兴趣，让您了解所有常见图层都具有哪些参数和超参数。</p><p id="4db7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读:</strong>大部分课程讲授的是批量范数层对抗所谓的内部协方差移位问题。<a class="ae ky" href="https://arxiv.org/abs/1805.11604" rel="noopener ugc nofollow" target="_blank">最近的证据显示情况并非如此</a>。相反，作者认为BN层使整体损失情况更加平稳。另一个别出心裁的想法是<a class="ae ky" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank">彩票假说</a>，也是由<em class="ng"> Frankle等人</em>提出的。</p><h1 id="3a47" class="ml mm it bd mn mo nk mq mr ms nl mu mv jz nm ka mx kc nn kd mz kf no kg nb nc bi translated">#6光谱标准(2018)</h1><blockquote class="nd ne nf"><p id="30ec" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">宫藤，Takeru，等人<a class="ae ky" href="https://arxiv.org/abs/1802.05957" rel="noopener ugc nofollow" target="_blank">“生成性对抗网络的谱规范化”</a> <em class="it"> arXiv预印本arXiv:1802.05957 </em> (2018)。</p></blockquote><p id="a4fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在GAN文献中，<a class="ae ky" href="https://arxiv.org/abs/1701.07875" rel="noopener ugc nofollow" target="_blank"> Wasserstein损失</a>改进了训练GAN的几个关键挑战。然而，它要求梯度必须具有小于或等于1的范数(<a class="ae ky" href="https://en.wikipedia.org/wiki/Lipschitz_continuity" rel="noopener ugc nofollow" target="_blank"> 1-Lipschitz </a>)。损失的原始作者建议将权重修剪为[-0.01，0.01]，作为一种加强小梯度的方式。用计算机科学术语来说，就是黑客。作为回应，谱范数被提出作为一个平滑的替代方案来约束权重矩阵，以产生最多一个单位梯度。更清洁的解决方案。</p><p id="996b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因1: </strong>标准化是一个比大多数人意识到的要大得多的话题。许多特殊属性可以通过专门的规范化和<a class="ae ky" href="https://arxiv.org/abs/1706.02515" rel="noopener ugc nofollow" target="_blank">精心的激活功能设计</a>来实现。</p><p id="8d69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#2: </strong>这除了是一种规范，也是一种正则化，是神经网络设计中经常被忽略的话题。除了辍学之外，阅读关于这个问题的成功论文令人耳目一新。</p><p id="fcdd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读:</strong>归一化技术的其他最新进展是<a class="ae ky" href="https://arxiv.org/abs/1803.08494" rel="noopener ugc nofollow" target="_blank">组归一化</a>和<a class="ae ky" href="http://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html" rel="noopener ugc nofollow" target="_blank">自适应实例归一化</a>技术。前者解决了小批量批量标准的一些缺点，而后者是任意风格转换的关键突破之一。</p><h1 id="a810" class="ml mm it bd mn mo nk mq mr ms nl mu mv jz nm ka mx kc nn kd mz kf no kg nb nc bi translated">#7感知损失(2016年)</h1><blockquote class="nd ne nf"><p id="1972" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">约翰逊、贾斯汀、亚历山大·阿拉希和李菲菲。<a class="ae ky" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">“实时风格转换和超分辨率的感知损失”</a>T2【欧洲计算机视觉会议】T3。施普林格，查姆，2016。</p></blockquote><p id="be0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数神经网络背后的驱动力是损失函数。亏损越能描述什么是好什么是坏，我们就能越快地收敛到有用的模型。在文献中，大多数损失相对简单，只能测量低水平的属性。除此之外，捕捉高级语义是众所周知的棘手。</p><p id="86ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感知损失论文认为，预先训练的网络可以用来测量语义相似度，而不是手工设计复杂的损失函数。在实践中，生成的和地面真实的结果通过预先训练的VGG网络，并比较特定层的激活。相似的图像应该有相似的激活。早期图层捕捉广泛的特征，而后期图层捕捉更细微的细节。</p><p id="b897" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#1: </strong>亏损是培养优秀模特最重要的方面之一。没有适当的反馈信号，任何优化过程都不会收敛。这就是好老师的作用:给予反馈。</p><p id="50f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#2: </strong>成功的新亏损往往是一个里程碑。甘斯所达到的质量，是在感知丧失被发明出来之后的飞跃。理解这部作品对于理解后来的大部分文献是必不可少的。</p><p id="e597" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#3: </strong>这些神经损失既神秘又有用。虽然作者对这些模型的工作提供了合理的解释，但它们的许多方面仍然是开放的，就像神经网络中的大多数事情一样。</p><p id="db9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读:</strong>神经网络的一个迷人之处是它们的可组合性。这项工作使用神经网络来解决神经网络问题。<a class="ae ky" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.html" rel="noopener ugc nofollow" target="_blank">拓扑损失论文</a>将这一思想扩展到图像分割问题。<a class="ae ky" href="https://en.wikipedia.org/wiki/Neural_architecture_search" rel="noopener ugc nofollow" target="_blank">神经架构搜索(NAS) </a>文献利用神经网络寻找新的神经网络。至于计算机视觉的其他损失，这里有一个<a class="ae ky" href="https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a" rel="noopener">综合指南</a>。感谢<a class="np nq ep" href="https://medium.com/u/dc830cac0dcb?source=post_page-----8c6fb4650a9b--------------------------------" rel="noopener" target="_blank">Sowmya yelapragada</a>将这个伟大的名单放在一起:)</p><div class="nr ns gp gr nt nu"><a href="https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a" rel="noopener follow" target="_blank"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd iu gy z fp nz fr fs oa fu fw is bi translated">理解计算机视觉中的损失函数！</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">选择正确的损失函数可以优化模型的收敛性，也有助于集中在正确的特征集上</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">medium.com</p></div></div><div class="od l"><div class="ok l of og oh od oi ks nu"/></div></div></a></div><h1 id="68e5" class="ml mm it bd mn mo nk mq mr ms nl mu mv jz nm ka mx kc nn kd mz kf no kg nb nc bi translated">第八名那达慕(2016)</h1><blockquote class="nd ne nf"><p id="e3a7" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">多扎特蒂莫西。<a class="ae ky" href="http://cs229.stanford.edu/proj2015/054_report.pdf" rel="noopener ugc nofollow" target="_blank">“把内斯特洛夫的动力融入亚当”</a> (2016)。</p></blockquote><p id="1bec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们大多数人都熟悉SGD、Adam和RMSprop等术语。有些人还知道一些不太熟悉的名字，如阿达格拉德、阿达德尔塔和阿达马克斯。然而，很少有人花时间去理解这些名字的含义，以及为什么亚当是现今的默认选择。Tensorflow捆绑了Nadam，这是对adam的改进，但大多数用户都没有意识到这一点。</p><p id="9b0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">理由#1: </strong>这份技术报告对大多数神经网络优化器提供了全面而直接的解释。每一种都是对其他产品的直接改进。很少有论文能在两页半的篇幅内涵盖如此数学化的主题。</p><p id="3fe0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因2: </strong>我们都认为优化者理所当然。理解它们的基本原理对改进神经网络非常有用。这就是为什么当RMSprop不收敛时，我们用Adam代替它，后来又用SGD代替它。</p><p id="eae4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读:</strong>自2016年以来，已经提出了许多其他对优化器的改进。有些会在某个时候并入主流图书馆。看看<a class="ae ky" href="https://arxiv.org/abs/1908.03265v1" rel="noopener ugc nofollow" target="_blank">拉达姆</a>、<a class="ae ky" href="https://arxiv.org/abs/1907.08610" rel="noopener ugc nofollow" target="_blank">前瞻</a>和<a class="ae ky" href="https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer" rel="noopener ugc nofollow" target="_blank">游侠</a>的一些新想法。</p><h1 id="3c3c" class="ml mm it bd mn mo nk mq mr ms nl mu mv jz nm ka mx kc nn kd mz kf no kg nb nc bi translated">#9双重下降假说(2019)</h1><blockquote class="nd ne nf"><p id="0c97" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">Nakkiran，Preetum，et al. <a class="ae ky" href="https://arxiv.org/abs/1912.02292" rel="noopener ugc nofollow" target="_blank">《深度双重下降:更大的模型和更多的数据带来的伤害》</a> <em class="it"> arXiv预印本arXiv:1912.02292 </em> (2019)。</p></blockquote><p id="b97b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">传统观点认为，小型号不足，大型号过多。然而，在彩虹之上的某处，更大的模型仍然闪耀着光芒。</p><p id="6c69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇论文中，Nakkiran <em class="ng">等人</em>展示了几个模型在规模增长时表现出“双重下降”现象的证据。测试精度下降，然后上升，再下降。此外，他们认为拐点在“插值阈值”:模型大到足以插值数据的点。换句话说，当一个模型被训练得超越了该领域所建议的一切，它就开始改进了。</p><p id="f475" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理由1: 大多数课程都教授偏差/方差权衡。显然，这个原则只在一定程度上适用——是时候复习基础知识了。</p><p id="fff5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#2: </strong>如果增加历元数也穿过插值点，我们都应该放弃早期停止，看看会发生什么。集体来说，我们都可以做科学。</p><p id="1ac2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因3: </strong>这一点和第五点都很好地提醒了我们还有很多我们不知道的。不是所有我们学到的都是对的，也不是所有直觉的都是正确的。</p><p id="6f50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读:</strong>更轻松的阅读是<a class="ae ky" href="https://arxiv.org/abs/1812.01187" rel="noopener ugc nofollow" target="_blank">图像分类锦囊</a>论文。在这本书里，你会找到几个简单可行的建议来从你的模型中提取额外的性能下降。</p><h1 id="b58c" class="ml mm it bd mn mo nk mq mr ms nl mu mv jz nm ka mx kc nn kd mz kf no kg nb nc bi translated">智力指标排名第十(2019)</h1><blockquote class="nd ne nf"><p id="3117" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">弗朗索瓦，乔莱。<a class="ae ky" href="https://arxiv.org/abs/1911.01547" rel="noopener ugc nofollow" target="_blank">《论智力的衡量》</a> <em class="it"> arXiv预印本arXiv:1911.01547 </em> (2019)。</p></blockquote><p id="e3b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数人都在努力多走一步，弗朗索瓦·乔莱(Franç ois Chollet)正在努力实现梦想。</p><p id="5d55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个列表中，所有提到的文章都将实践和理论的最新水平推进了一步。有些已经被广泛采用，有些支持这种或那种技术，还有一些为融合提供了很好的改进。然而，房间里的大象<em class="ng">智力</em>仍然是一个神秘而难以捉摸的话题，更不用说神秘莫测了。</p><p id="587f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">时至今日，人工智能领域向一般智能的进步只能用“成就”来粗略衡量。每隔一段时间，一种算法就会在一项复杂的任务中击败人类，比如国际象棋、Dota 2或围棋。每当这种情况发生时，我们就说我们离目标更近了一步😃。然而，这不足以衡量智力的技能获取效率组成部分。</p><p id="ae8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇(很长的)文章中，Chollet认为:<em class="ng">“为了朝着更智能、更像人类的人工系统稳步前进，我们需要遵循适当的反馈信号。”换句话说，我们需要一个合适的机器智能基准。一种智商测试。由此，作者提出了<a class="ae ky" href="https://github.com/fchollet/ARC" rel="noopener ugc nofollow" target="_blank">抽象与推理语料库(ARC) </a>:</em></p><blockquote class="nd ne nf"><p id="a4ce" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated"><em class="it">“ARC可以看做通用的人工智能基准，可以看做程序合成基准，也可以看做心理测量智能测试。它的目标是人类和人工智能系统，旨在模仿类似人类的一般流体智能形式。”</em></p></blockquote><p id="bdce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">理由1: </strong>虽然数据科学很酷很时髦，但人工智能才是真正的东西。如果没有人工智能，就不会有数据科学。它的最终目标不是在数据中寻找洞察力，而是建造能够拥有自己想法的机器。花点时间思考一些基本问题:什么是智力，我们如何衡量它？这篇论文是一个良好的开端。</p><p id="244b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因#2: </strong>在过去的几十年里，IA社区被来自数理逻辑和演绎推理的思想所主导。然而，在没有任何形式的显式推理的情况下，支持向量机和神经网络在该领域的发展远远超过了基于逻辑的方法。ARC会引发经典技术的复兴吗？</p><p id="9d1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">原因3: </strong>如果Chollet是对的，我们距离创建能够解决ARC数据集的算法还有好几年的时间。如果你正在寻找一个数据集在你的业余时间玩，这里有一个会让你忙起来:)</p><p id="d55b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读:</strong>2018年，Geoffrey Hinton、Yosha Bengio和Yan LeCun因其在深度学习基础方面的开创性工作获得了图灵奖。今年，在AAAI会议上，他们分享了他们对人工智能未来的看法。可以在<a class="ae ky" href="https://www.youtube.com/watch?v=UX8OubxsY8w" rel="noopener ugc nofollow" target="_blank"> Youtube </a>上看:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="31a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">我想引用杰弗里·辛顿的一句话来结束这篇文章，我认为这句话概括了一切:</p><blockquote class="on"><p id="f337" class="oo op it bd oq or os ot ou ov ow lu dk translated">"未来取决于某个对我所说的一切深感怀疑的研究生。"</p></blockquote><p id="d51e" class="pw-post-body-paragraph kz la it lb b lc ox ju le lf oy jx lh li oz lk ll lm pa lo lp lq pb ls lt lu im bi translated">GloVe通过隐式的方式抑制了共现矩阵。AdaBoost制造了数百个最先进的弱分类器。胶囊网络挑战CNN，而图形神经网络可能会取代它们。关键进步可能来自规范化、损失和优化器，而我们仍然有空间质疑批量规范和训练过度参数化的模型</p><p id="2042" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我想知道还有多少关于辍学和重新学习的事情有待发现。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="c500" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这本书对你和我来说都是令人兴奋的。请让我知道你认为符合这个列表的其他文件。我将很高兴阅读和考虑他们的未来列表😃</p><p id="5019" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">编辑:写完这个单子，我用十篇GAN论文编了三分之一，2020年读。如果你喜欢阅读这份(以及之前的)清单，你可能会喜欢阅读第三份:</strong></p><div class="nr ns gp gr nt nu"><a rel="noopener follow" target="_blank" href="/gan-papers-to-read-in-2020-2c708af5c0a4"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd iu gy z fp nz fr fs oa fu fw is bi translated">甘2020年要读的论文</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">生成性对抗网络的阅读建议。</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">towardsdatascience.com</p></div></div><div class="od l"><div class="pc l of og oh od oi ks nu"/></div></div></a></div></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="2f71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">欢迎评论或<a class="ae ky" href="https://www.linkedin.com/in/ygorreboucas/" rel="noopener ugc nofollow" target="_blank">联系我</a>。如果你刚接触媒体，我强烈推荐<a class="ae ky" href="https://ygorserpa.medium.com/membership" rel="noopener">订阅</a>。对于数据和IT专业人士来说，中型文章是<a class="ae ky" href="https://stackoverflow.com/" rel="noopener ugc nofollow" target="_blank"> StackOverflow </a>的完美搭档，对于新手来说更是如此。注册时请考虑使用<a class="ae ky" href="https://ygorserpa.medium.com/membership" rel="noopener">我的会员链接。</a>你也可以直接支持我<a class="ae ky" href="https://www.buymeacoffee.com/ygorreboucas" rel="noopener ugc nofollow" target="_blank">请我喝杯咖啡</a>:)</p><p id="696b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读:)</p></div></div>    
</body>
</html>