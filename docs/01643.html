<html>
<head>
<title>Super-Convergence with just PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">仅使用PyTorch的超收敛</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/super-convergence-with-just-pytorch-c223c0fc1e51?source=collection_archive---------30-----------------------#2020-02-14">https://towardsdatascience.com/super-convergence-with-just-pytorch-c223c0fc1e51?source=collection_archive---------30-----------------------#2020-02-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cf77" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用内置PyTorch函数和类减少训练时间同时提高效果的指南</h2></div><h1 id="7a95" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">为什么？</h1><p id="c82f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">当创建Snaked，我的蛇分类模型时，我需要找到一种方法来改善结果。超级收敛只是一种更快训练模型同时获得更好结果的方法！然而，我发现<strong class="kz ir">没有关于如何使用内置PyTorch调度程序的指南。</strong></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/7d32063f8be811fe42a3517a2c5fd8bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P7qGlRtlr8nWSYFweFXMJA.jpeg"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">封面图片来源于<a class="ae mj" href="https://pixnio.com/objects/mechanism-metal-gears-steel-iron" rel="noopener ugc nofollow" target="_blank">此处</a></p></figure><h1 id="effc" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">学习理论</h1><p id="5e49" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在你阅读这篇文章之前，你可能想知道<em class="mk">什么是</em>超级收敛，以及<em class="mk">它是如何工作的</em>。总的要点是在开始时尽可能提高学习速度，然后以循环的速度逐渐降低学习速度。这是因为较大的学习率训练得更快，而导致损失发散。我在这里的重点是PyTorch，所以我自己不会做任何进一步的解释。</p><p id="5421" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">以下是可供深入研究的资源列表:</p><ul class=""><li id="d658" class="mq mr iq kz b la ml ld mm lg ms lk mt lo mu ls mv mw mx my bi translated"><a class="ae mj" href="https://arxiv.org/abs/1708.07120" rel="noopener ugc nofollow" target="_blank">超收敛:使用大学习率非常快速地训练神经网络</a></li><li id="5e5c" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><a class="ae mj" href="https://medium.com/vitalify-asia/whats-up-with-deep-learning-optimizers-since-adam-5c1d862b9db0" rel="noopener">自亚当以来，深度学习优化器是怎么回事？</a></li><li id="5399" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><a class="ae mj" rel="noopener" target="_blank" href="/https-medium-com-super-convergence-very-fast-training-of-neural-networks-using-large-learning-rates-decb689b9eb0">超收敛:使用大学习率非常快速地训练神经网络</a></li><li id="af00" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><a class="ae mj" href="https://www.fast.ai/2018/07/02/adam-weight-decay/" rel="noopener ugc nofollow" target="_blank"> AdamW和超收敛是目前训练神经网络最快的方法</a></li><li id="1690" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><a class="ae mj" href="https://sgugger.github.io/the-1cycle-policy.html" rel="noopener ugc nofollow" target="_blank">1周期政策</a></li><li id="6aa6" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><a class="ae mj" href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html" rel="noopener ugc nofollow" target="_blank">如何找到好的学习率</a></li></ul><h1 id="e2c2" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">进口</h1><pre class="lu lv lw lx gt ne nf ng nh aw ni bi"><span id="0666" class="nj kg iq nf b gy nk nl l nm nn">import torch<br/>from torchvision import datasets, models, transforms<br/>from torch.utils.data import DataLoader<br/><br/>from torch import nn, optim<br/>from torch_lr_finder import LRFinder</span></pre><h1 id="235e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">设置超参数</h1><h2 id="74cd" class="nj kg iq bd kh no np dn kl nq nr dp kp lg ns nt kr lk nu nv kt lo nw nx kv ny bi translated">设置变换</h2><pre class="lu lv lw lx gt ne nf ng nh aw ni bi"><span id="de54" class="nj kg iq nf b gy nk nl l nm nn">transforms = transforms.Compose([<br/>transforms.RandomResizedCrop(size=256, scale=(0.8, 1)),<br/>    transforms.RandomRotation(90),<br/>    transforms.ColorJitter(),<br/>    transforms.RandomHorizontalFlip(),<br/>    transforms.RandomVerticalFlip(),<br/>    transforms.CenterCrop(size=224), <br/>    transforms.ToTensor(),<br/>    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), <br/>])</span></pre><h2 id="0098" class="nj kg iq bd kh no np dn kl nq nr dp kp lg ns nt kr lk nu nv kt lo nw nx kv ny bi translated">加载数据、模型和基本超参数</h2><pre class="lu lv lw lx gt ne nf ng nh aw ni bi"><span id="6014" class="nj kg iq nf b gy nk nl l nm nn">train_loader = DataLoader(datasets.CIFAR10(root="train_data", train=True, download=True, transform=transforms))<br/>test_loader = DataLoader(datasets.CIFAR10(root="test_data", train=False, download=True, transform=transforms))<br/><br/>model = models.mobilenet_v2(pretrained=True)<br/><br/>criterion = nn.CrossEntropyLoss()<br/>optimizer = optim.AdamW(model.parameters())<br/><br/><br/>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/>model = model.to(device)<br/><br/><br/>Note that doing this requires a seperate library from [here](https://github.com/davidtvs/pytorch-lr-finder).<br/><br/><br/>```python<br/>lr_finder = LRFinder(model, optimizer, criterion, device)<br/>lr_finder.range_test(train_loader, end_lr=10, num_iter=1000)<br/>lr_finder.plot()<br/>plt.savefig("LRvsLoss.png")<br/>plt.close()</span><span id="8f4e" class="nj kg iq nf b gy nz nl l nm nn">HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))<br/><br/><br/>Stopping early, the loss has diverged<br/>Learning rate search finished. See the graph with {finder_name}.plot()</span></pre><h2 id="0afc" class="nj kg iq bd kh no np dn kl nq nr dp kp lg ns nt kr lk nu nv kt lo nw nx kv ny bi translated">创建计划程序</h2><p id="16c8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用单周期学习率调度程序(用于超收敛)。</p><p id="2b26" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">请注意，调度程序使用图表中的最大学习率。要选择向下寻找最大梯度(斜率)。</p><p id="1b76" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">必须在中输入训练的周期数和每个周期的步数。通常的做法是使用批量大小作为每个时期的步骤。</p><pre class="lu lv lw lx gt ne nf ng nh aw ni bi"><span id="e9ba" class="nj kg iq nf b gy nk nl l nm nn">scheduler = optim.lr_scheduler.OneCycleLR(optimizer, 2e-3, epochs=50, steps_per_epoch=len(train_loader))</span></pre><h1 id="c3af" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">火车模型</h1><p id="ff6c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">训练模型50个纪元。每个时期后打印统计数据(损失和准确性)。</p><p id="9c17" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">不同的调度程序应该在不同的代码中调用。将调度器放在错误的位置会导致错误，所以使用单周期策略时，要确保在每个批处理之后直接调用step方法。</p><pre class="lu lv lw lx gt ne nf ng nh aw ni bi"><span id="b23f" class="nj kg iq nf b gy nk nl l nm nn">best_acc = 0<br/>epoch_no_change = 0<br/><br/>for epoch in range(0, 50):<br/>    print(f"Epoch {epoch}/49".format())<br/><br/>    for phase in ["train", "validation"]:<br/>        running_loss = 0.0<br/>        running_corrects = 0<br/><br/>        <br/>        <br/>        if phase == "train":<br/>            model.train()<br/>        else: model.eval()<br/><br/>        <br/>        for (inputs, labels) in train_loader:<br/>            <br/>            inputs, labels = inputs.to(device), labels.to(device)<br/><br/>            <br/>            optimizer.zero_grad()<br/><br/>            with torch.set_grad_enabled(phase == "train"):<br/>                <br/>                outputs = model(inputs)<br/>                _, preds = torch.max(outputs, 1)<br/>                loss = criterion(outputs, labels)<br/><br/>                if phase == "train":<br/>                    <br/>                    loss.backward()<br/>                    optimizer.step()<br/><br/>                    <br/>                    scheduler.step()<br/><br/>            running_loss += loss.item() * inputs.size(0)<br/>            running_corrects += torch.sum(preds == labels.data)<br/><br/>        <br/>        epoch_loss = running_loss / len(self.data_loaders[phase].sampler)<br/>        epoch_acc = running_corrects.double() / len(self.data_loaders[phase].sampler)<br/>        print("\nPhase: {}, Loss: {:.4f}, Acc: {:.4f}".format(phase, epoch_loss, epoch_acc))<br/><br/>        <br/>        if phase == "validation" and epoch_acc &gt; best_acc:<br/>            epoch_no_change += 1<br/><br/>            if epoch_no_change &gt; 5:<br/>                break</span></pre><h1 id="d072" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">感谢阅读！</h1><p id="546d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我希望这足够简单，可以相对快速地理解。当我第一次实现超级收敛时，我花了很长时间才弄明白如何使用调度程序(我找不到任何利用它的代码)。如果你喜欢这篇博文，考虑看看其他方法来改进你的模型。如果你想看看超级收敛在实际项目中是如何使用的，只需看看<a class="ae mj" href="https://github.com/KamWithK/Snaked" rel="noopener ugc nofollow" target="_blank">我的蛇分类项目</a>就行了。</p></div></div>    
</body>
</html>