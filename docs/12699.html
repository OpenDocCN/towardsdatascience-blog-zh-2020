<html>
<head>
<title>New Approaches to Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">物体检测的新方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/new-approaches-to-object-detection-f5cbc925e00e?source=collection_archive---------12-----------------------#2020-09-01">https://towardsdatascience.com/new-approaches-to-object-detection-f5cbc925e00e?source=collection_archive---------12-----------------------#2020-09-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8f3f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">center net(Objects as Points)简介，TTFNet 及其在 TensorFlow 2.2+中的实现。</strong></h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/df4c552992a82ae81211982a9ba31b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ssXB-7gYmIwif0mxjncYg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:pexels.com<a class="ae kv" href="https://www.pexels.com/photo/top-view-of-assorted-colored-row-boats-929032/" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="67bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将从简单介绍不同的物体检测方法开始。在介绍了传统方法和新方法之后，您可以阅读 CenterNet 和 TTFNet 的最重要部分。两个模型中的许多想法是相似的，因此将一起介绍它们。我们实现了一个由两个网络启发的包。如果你有兴趣，请查看我们的<a class="ae kv" href="https://github.com/Ximilar-com/xcenternet" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</p><p id="8af3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">需要深度学习和卷积神经网络(CNN)的基础知识。</p><h1 id="642b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">传统方法</h1><p id="604f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在计算机科学中，当我们遇到一个无解的难题时，我们会试图将其转化为一个我们知道其解的问题或者一个更简单的问题。所谓的两阶段检测模型就是很好的例子。在这种情况下，更简单的问题是图像分类或标记。(将给定图像放入一个类别或给给定图像分配标签)简单地说，我们可以将图像分成多个区域，然后对这些区域进行分类，对吗？是的……但是要花很多时间。因此，你需要聪明一点。使用这种方法的算法的一个例子是<a class="ae kv" href="https://arxiv.org/abs/1311.2524" rel="noopener ugc nofollow" target="_blank"> R-CNN </a> (2014)。后来演变成<a class="ae kv" href="https://arxiv.org/abs/1504.08083" rel="noopener ugc nofollow" target="_blank">快速 R-CNN </a> (2015)甚至更快的<a class="ae kv" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank"> R-CNN </a> (2016)。</p><p id="0eec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管这些模型的性能相当好，但研究人员显然在问自己，这个过程是否可以变得简单，从而更有效。在一个阶段，没有区域提案。这个问题的一个可能答案是<a class="ae kv" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">YOLO——你只看一次</a> (2015)。现在是第四版 (2020)。或<a class="ae kv" href="https://medium.com/r?url=https%3A%2F%2Farxiv.org%2Fabs%2F1512.02325" rel="noopener"> SSD —单次多盒探测器</a> (2015)。最后，还应提及 RetinaNet (2017 年)，特别是因为它曾被用于引入物体检测的焦点损失，这是现在经常使用的方法。</p><h1 id="e151" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">新方法</h1><p id="0f9a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">近年来，另一个想法越来越受欢迎。物体可以被转换成一组点。并且检测任务可以被认为是关键点估计问题。这种方法在<a class="ae kv" href="https://arxiv.org/abs/1808.01244" rel="noopener ugc nofollow" target="_blank"> CornerNet:将对象检测为成对关键点</a>中有所介绍。顾名思义，一个对象被表示为一对关键点，左上角和右下角。</p><p id="bd68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在介绍 CenterNet 的<a class="ae kv" href="https://arxiv.org/abs/1904.07850" rel="noopener ugc nofollow" target="_blank"> Objects as Points </a>论文中探讨了类似的想法。在那里，我们使用热图检测包围盒的中心点。使用回归直接预测诸如边界框大小的其他属性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/c2f25712cb25459ba8c0c9043f1e83ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSdw1M6FkmarZXbD0PQs_A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:pexels.com<a class="ae kv" href="https://www.pexels.com/photo/person-holding-white-ceramic-mug-beside-macbook-pro-3787300/" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="0dbe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法的缺点是训练速度较慢。为了解决这个问题，提出了 TTFNet(训练时间友好网络)。它遵循相同的基本思想，因此我们决定在一个<a class="ae kv" href="https://github.com/Ximilar-com/xcenternet" rel="noopener ugc nofollow" target="_blank">包</a>中实现来自两个网络的思想。</p><h1 id="156e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">神经网络布局</h1><p id="611c" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">让我们开始吧！我将从头开始，向您介绍网络布局。然后，将讨论重要的单个部分，主要是热图和不同的损失函数。</p><p id="4a14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用专门为此类任务设计的网络。例如<a class="ae kv" href="https://arxiv.org/abs/1603.06937" rel="noopener ugc nofollow" target="_blank">沙漏网络</a>。或者，正如我们已经决定要做的，选择一个标准的图像分类 CNN，并根据我们的需要进行修改。我们选择测试 ResNet (18，50)和 EfficientNet (b0，b1，b2)。</p><p id="1a58" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在所有标准的迁移学习任务中，我们放弃了顶部的密集层。但是留在顶部的层甚至与我们需要的输出相差甚远。因此，一些上采样是必要的。除此之外，来自较低层的连接提高了性能。当我们在网络的末端有一个正确大小的层时，我们可以将它“分割”成所需数量的头。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/90922104e0417f308c526b420d45f67f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60ROU3IyeI3U8ryAUXs-2A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用 ResNet18 简化 CenterNet 的可视化，使用上采样和连接。(黄色:卷积层，红色:最大池化，蓝色:上采样。)</p></figure><p id="5b13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了让网络更快，热图的一边正好是输入图像的 1/4。我们每节课都有一张热图。然后，CenterNet 中有另外两个头(用于基本的对象检测)，TTFNet 中有一个头。</p><p id="d52e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<strong class="ky ir">中心网</strong>，有</p><ul class=""><li id="e0ad" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">一个尺寸头，包含一个边框的<strong class="ky ir">宽度和</strong>高度</li><li id="ad99" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">偏移头，包含使用下采样热图时产生的中心的 x 和 y 偏移。</li></ul><p id="d990" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">两者都只有两个过滤器——在热图的任何给定点都可能只有一个对象。如果您对一个对象的其他属性感兴趣，可以添加更多的头。</p><p id="995b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<strong class="ky ir"> TTFNet </strong>中，只有一个带有<strong class="ky ir">四个</strong>滤镜的额外云台来计算物体的尺寸——<strong class="ky ir">到侧面</strong>的距离。</p><h2 id="63da" class="nf lt iq bd lu ng nh dn ly ni nj dp mc lf nk nl me lj nm nn mg ln no np mi nq bi translated">热图</h2><p id="bfc2" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">那么，热图是什么样子的呢？它是一个填充了从 0.0 到 1.0 的值的矩阵。这种地图上的峰表明某个物体的存在。</p><p id="04ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面，你会看到一些生成的训练热图。只有一个点正好是 1.0。在这一点上，概率正在慢慢消失。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/63381b32f6ffabbbf6920e01f8d8a2d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*lVma1W94MGETfUVaGEpF0g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CenterNet(左)和 TTFNet(右)的热图。</p></figure><h2 id="c6a7" class="nf lt iq bd lu ng nh dn ly ni nj dp mc lf nk nl me lj nm nn mg ln no np mi nq bi translated">可变形卷积</h2><p id="87a9" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">网络的上采样部分可以通过多种方式实现。我们已经向您展示了级联、上采样和标准卷积层的简单方法。为了提高性能，使用了可变形卷积。</p><p id="295a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">毫无疑问，卷积神经网络给深度学习带来了一场伟大的革命。它们使我们能够提取从完全连接的层中很难得到的特征。再加上另一项布局改进，我们的网络可能会变得非常深入。尽管如此，基本思想还是一样的。特别地，过滤器的形状总是矩形的。可变形回旋正试图对此进行改进。它们学习到标准网格的偏移，并用这个“变形的”内核执行卷积。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/48c688095067f870222d844e64c8da34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*khKe6aSlYkjjzUaCzRbhVw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">标准与可变形卷积值的选择。船方不负担装货费用</p></figure><p id="e459" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不幸的是，可变形卷积层尚未在 TensorFlow 和<a class="ae kv" href="https://www.tensorflow.org/addons" rel="noopener ugc nofollow" target="_blank"> TensorFlow 插件</a> (TFA)中实现。我们正在使用<a class="ae kv" href="https://github.com/smallsunsun1/addons/tree/feature/deformable_ops" rel="noopener ugc nofollow" target="_blank">TFA</a>的一个分支，支持可变形 Conv2d，希望它能很快被合并。(参见拉动请求<a class="ae kv" href="https://github.com/tensorflow/addons/pull/1129" rel="noopener ugc nofollow" target="_blank"> 1129 </a>。)</p><h1 id="5973" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">损失函数</h1><p id="5b4e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在，当我们有了网络布局和输出的样子，只有一个关键的东西不见了。损失函数。</p><h2 id="4c35" class="nf lt iq bd lu ng nh dn ly ni nj dp mc lf nk nl me lj nm nn mg ln no np mi nq bi translated">焦点损失</h2><p id="fbf9" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">热图有一个问题——它们非常稀疏。大多数情况下，没有检测到(零)，只是偶尔我们会看到一个物体(一个物体，周围的值递减)。在这种情况下，标准的度量标准不太适用。幸运的是，有一个解决方案——一个<a class="ae kv" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank">焦损失</a>。CenteNet 和 TTFNet 都使用它。</p><h2 id="6386" class="nf lt iq bd lu ng nh dn ly ni nj dp mc lf nk nl me lj nm nn mg ln no np mi nq bi translated">基于并集交集的损失(IoU)</h2><p id="9432" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了优化边界框的大小，中心网使用 L1 损失。它是真实边界框坐标和预测边界框坐标之差的简单总和。这似乎是合理的，但另一方面，我们并不经常使用它进行评估。我们使用 IoU 指标——并集上的交集。因此，当我们对改进这个指标感兴趣时，为什么不把它也用于优化呢？我们可以从 1 中减去借据的价值，把它变成亏损。还是没有？</p><p id="f057" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可惜，如果没有交集，欠条就是零。因此，在这种情况下，损失总是 1。提出了两种基于 IoU 的损失函数。<a class="ae kv" href="https://giou.stanford.edu/" rel="noopener ugc nofollow" target="_blank">TTF net 中使用的联合损耗上的广义交集</a>解决了这个问题。<a class="ae kv" href="https://arxiv.org/abs/1911.08287" rel="noopener ugc nofollow" target="_blank"> Distance-IoU Loss </a>还关注于在函数中添加一个距离信息，换句话说，我们离边界框的中心有多远。</p><h1 id="264d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">最后的想法</h1><p id="1290" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">你可以在<a class="ae kv" href="https://github.com/Ximilar-com/xcenternet" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到我们的实现。我要感谢<a class="ae kv" href="https://github.com/Cospel" rel="noopener ugc nofollow" target="_blank"> <em class="ns"> Michal Lukac </em> </a>与我合作这个项目，感谢<a class="ae kv" href="https://www.ximilar.com/" rel="noopener ugc nofollow" target="_blank"> <em class="ns"> Ximilar </em> </a>允许我们开源它。如果你想尝试没有任何编码的模型，请查看<a class="ae kv" href="https://www.ximilar.com/how-to-train-an-object-detection-model-with-one-click/" rel="noopener ugc nofollow" target="_blank">这篇关于<a class="ae kv" href="https://app.ximilar.com" rel="noopener ugc nofollow" target="_blank"> Ximilar App </a>中物体检测的博文</a>。</p></div></div>    
</body>
</html>