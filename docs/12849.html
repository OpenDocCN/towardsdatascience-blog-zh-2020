<html>
<head>
<title>Predicting Hazardous Seismic Bumps Part II: Training &amp; Tuning Supervised ML Classifiers and Model Performance Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测危险的地震颠簸第二部分:训练和调整监督最大似然分类器和模型性能分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-hazardous-seismic-bumps-part-ii-training-supervised-classifier-models-and-8b9104b611b0?source=collection_archive---------43-----------------------#2020-09-03">https://towardsdatascience.com/predicting-hazardous-seismic-bumps-part-ii-training-supervised-classifier-models-and-8b9104b611b0?source=collection_archive---------43-----------------------#2020-09-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b511" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">本文演示了使用不同的监督分类器预测危险的地震颠簸、调整模型超参数、准确性悖论以及理解“业务问题”对性能评估的重要性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4ede6b38da4f1eb6f91c07f84d073502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JIOYAGcva8sl8teT"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">肖恩·麦克伦登在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="9f8f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="7e42" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在我的<a class="ae ky" rel="noopener" target="_blank" href="/predicting-hazardous-seismic-bumps-using-supervised-classification-algorithms-part-i-2c5d21f379bc">上一篇文章</a>中，关于来自 UCI 数据档案库的地震颠簸数据集，我应用了特征工程的基本数据分析技术和不平衡数据集的测试序列分割策略。在本文中，我展示了如何应用监督机器学习算法(KNN、随机森林和 SVM)进行预测，并调整超参数(使用 GridSearchCV)以获得最佳结果。性能评估的结果也清楚地展示了<strong class="lt iu">准确性悖论</strong>，我将在下面的章节中详细阐述。此外，它还展示了<strong class="lt iu">为什么理解“业务问题”对于选择最佳模式至关重要</strong>。</p><p id="473b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">完整的笔记本可以在我的<a class="ae ky" href="https://github.com/royn5618" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>中找到</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/5d3c55c2b9464a70455967a99644c113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BVm_HUdXwVI80D5E"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@dsmacinnes?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">丹妮尔·麦金尼斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="bb01" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">我从…</h1><p id="708c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">创建一个文件夹来保存我的模型。将多种算法用于数据时，最好在训练和调整后保存模型。为此，我使用<a class="ae ky" href="https://docs.python.org/3/library/datetime.html" rel="noopener ugc nofollow" target="_blank"> datetime python 模块</a>创建了一个文件夹。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="a426" class="nf la it nb b gy ng nh l ni nj"><strong class="nb iu">import</strong> <strong class="nb iu">datetime</strong></span><span id="8809" class="nf la it nb b gy nk nh l ni nj"><strong class="nb iu">def</strong> model_store_location():<br/>    <strong class="nb iu">return</strong> "model-store-<strong class="nb iu">{}</strong>".format(datetime.date.today())</span><span id="3678" class="nf la it nb b gy nk nh l ni nj">model_store_location = model_store_location()<br/>print(model_store_location)</span><span id="c631" class="nf la it nb b gy nk nh l ni nj">!mkdir {model_store_location}</span></pre><p id="32ef" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">每个性能最好的模型都将存储在该文件夹中。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="1f0f" class="kz la it bd lb lc nl le lf lg nm li lj jz nn ka ll kc no kd ln kf np kg lp lq bi translated">接下来，我建立了一个基线</h1><p id="787d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这是一个二元分类任务，我选择了最简单的分类模型，K-最近邻(KNN)来开始预测建模。KNN 是最简单的分类器之一，它基于前 K 个相似的可见实例中标签的最大数量来将标签分配给不可见实例。</p><p id="f9dc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了改进模型，需要一个基线，随后的模型将与该基线进行比较。因此，它被称为“<em class="nq">基线模型</em>”。因此，对于基线模型，我使用默认参数初始化了 KNN 分类器:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="54f7" class="nf la it nb b gy ng nh l ni nj">model = KNeighborsClassifier()</span></pre><p id="fd1d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我首先检查了性能，使用 10 次分割的 StratifiedKFold，保持评分标准“准确性”。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="6330" class="nf la it nb b gy ng nh l ni nj">skf = StratifiedKFold(n_splits=10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/b19dfaeb4d96fdefe20920b7348bdd16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*unYOkcLkdedWVWqP8V71QA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="30a2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于所有的折叠，精确度在 0.92 到 0.93 之间，并且数值的接近性通过分数的标准偏差来显示。</p><p id="1c5b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里的准确率真的很高。现在，让我们在训练数据中拟合模型，并检查模型的性能—</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="2cbe" class="nf la it nb b gy ng nh l ni nj">model.fit(X_train, y_train)<br/>y_pred = model.predict(X_test)</span></pre><p id="6a86" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我添加了一个数据框来保存每个模型的超参数和性能，如下所示:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="f7fc" class="nf la it nb b gy ng nh l ni nj">df_results = pd.DataFrame(index = ['scoring_technique', <br/>                                   'algorithm', 'n_neighbors',<br/>                                   'weights', 'leaf_size', <br/>                                   'accuracy', 'precision', <br/>                                   'recall', 'f1-score'])</span></pre><p id="25b0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">并添加了我的基线模型的性能:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="6514" class="nf la it nb b gy ng nh l ni nj"><strong class="nb iu">from</strong> <strong class="nb iu">sklearn.metrics</strong> <strong class="nb iu">import</strong> accuracy_score, precision_score, recall_score, f1_score</span><span id="095c" class="nf la it nb b gy nk nh l ni nj">df_results['Baseline'] = ['None', model.algorithm,<br/>                          model.n_neighbors, <br/>                          model.weights, <br/>                          model.leaf_size,<br/>                          accuracy_score(y_test, y_pred), <br/>                          precision_score(y_test, y_pred), <br/>                          recall_score(y_test, y_pred), <br/>                          f1_score(y_test, y_pred)]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/6d1a626af8a7f183281a8232d65aaedd.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*ukhkaGlNYNxIxdgPXG7-yQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="nt">作者图片</em></p></figure><p id="a6db" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这就是 KNN 的表演看起来超越了准确性。这是一个典型的<strong class="lt iu">准确性悖论</strong>的例子。由于大多数地震凸起是无危险的，因此真负值(TN)比 TP、FP 和 FN 的总和要大得多，<strong class="lt iu"> <em class="nq">从而在数字上提高了准确性，并创造了预测正确的假象</em> </strong>，但正如精度和召回率所示，危险地震凸起识别率低于 30%。</p><p id="cdf2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="nq">二元分类性能指标— </em> </strong></p><p id="557a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"><em class="nq"/></strong><em class="nq">= TP+TN/(TP+FP+TN+FN)</em></p><p id="f090" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="nq">精度</em> </strong> <em class="nq"> = TP / (TP + FP) </em></p><p id="5f02" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="nq">回忆</em> </strong> <em class="nq"> = TP / (TP + FN) </em></p><p id="7d18" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"><em class="nq">F1-Score</em></strong><em class="nq">= 2 * Precision x Recall/(Precision+Recall)= 2TP/(2TP+FP+FN)</em></p><p id="ac81" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，基于基线性能度量，只有 25%的预测危险隆起是正确的，只有 5%的实际危险地震隆起被模型正确识别。93%的数据实际上包含无危险的地震隆起，因此<code class="fe nu nv nw nb b">cross_val_score </code>中的精确数字是有意义的，也在 93%左右。</p><p id="c0e4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，正如您所看到的，精确度和召回率都受到了影响，让我们看看如何使用<code class="fe nu nv nw nb b">GridSearchCV</code>来调优超参数</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="0cc0" class="kz la it bd lb lc nl le lf lg nm li lj jz nn ka ll kc no kd ln kf np kg lp lq bi translated">GridSearchCV 用于调整 KNN 超参数</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/dbedab8da28b8288d0e9e716ebce8116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xKpBlT_372Eshgp3"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·温克勒在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="232d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> GridSearchCV </a>是一个函数，它接受超参数的可能值或范围，运行指定的超参数值的所有组合，并根据提到的评分标准计算性能。这个评分标准应该基本上与业务价值或您的目标一致。</p><p id="4ba7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要查看提供了哪些评分标准，此代码有助于:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="8a10" class="nf la it nb b gy ng nh l ni nj"><strong class="nb iu">import</strong> <strong class="nb iu">sklearn</strong><br/>sorted(sklearn.metrics.SCORERS.keys())</span></pre><p id="3bce" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该代码将生成可用评分方法的完整列表。</p><p id="f398" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我为 KNN 定义了超参数，如下面的代码所示，并创建了一个名为<code class="fe nu nv nw nb b">param_grid </code>的字典，其中包含 KNN 超参数的参数，如<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learns 关于 KNN 的文档</a>中所定义的键，而值包含相应的值列表。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="6b5a" class="nf la it nb b gy ng nh l ni nj">n_neighbors = [1, 2, 3, 4, 5] <br/>weights = ['uniform', 'distance'] <br/>algorithm = ['ball_tree', 'kd_tree', 'brute'] <br/>leaf_size = [10, 20, 30 , 40, 50]  #</span><span id="1a4a" class="nf la it nb b gy nk nh l ni nj">param_grid = dict(n_neighbors=n_neighbors, <br/>                  weights=weights, <br/>                  algorithm=algorithm, <br/>                  leaf_size=leaf_size)</span></pre><p id="6a41" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我使用模型、最佳超参数搜索的网格、交叉验证类型、作为精度的评分度量来初始化 GridSearchCV，并且我还使用了<code class="fe nu nv nw nb b">refit</code>参数来确保最佳评估的估计值作为<code class="fe nu nv nw nb b">best_estimator_</code>可用，这是当训练集可用于进行预测时的拟合模型。<code class="fe nu nv nw nb b">Verbose</code>决定您希望在屏幕上显示多少日志信息。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="97c3" class="nf la it nb b gy ng nh l ni nj">grid = GridSearchCV(estimator=model, <br/>                    param_grid=param_grid, <br/>                    cv=StratifiedKFold(shuffle=<strong class="nb iu">True</strong>), <br/>                    scoring=['precision'],<br/>                    refit='precision',<br/>                    verbose=10)</span></pre><p id="97dc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我调用 fit 方法来启动超参数调优过程。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="b8c9" class="nf la it nb b gy ng nh l ni nj">grid_result = grid.fit(X_train, y_train)</span></pre><p id="9799" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">基本上，<code class="fe nu nv nw nb b">grid_result</code>包含了所有的输出，从拟合时间到个人得分，以及基于传递的参数评估的最佳模型。<code class="fe nu nv nw nb b">grid_result.best_estimator_</code>包含最佳拟合模型的超参数的选定值，根据评分标准进行优化。</p><p id="dae1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我试验了不同的<code class="fe nu nv nw nb b">scoring</code></p><p id="e086" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">型号 1 — </strong> <code class="fe nu nv nw nb b">scoring</code>:精度</p><p id="7757" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">型号 2 — </strong> <code class="fe nu nv nw nb b">scoring</code>:召回</p><p id="5f80" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">型号 3 — </strong> <code class="fe nu nv nw nb b">scoring</code> : f1</p><p id="9f16" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下是模型性能的总结:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/3a81d480c858a13f916e204598d91f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*fl9o7dTOhvJPJ0U0nAI9Dg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="nt">作者图片</em></p></figure><p id="deeb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您在此网格中注意到，每个模型的评分指标的最佳可能结果属于在整个网格中指定的相应<code class="fe nu nv nw nb b">scoring</code>指标，即，当参数为“precision”时，任何其他模型的“precision”性能都不如模型 1。同样，对于“召回”，网格中的最佳由模型 2 和模型 3 共享。其实他们的表现是一样的。</p><p id="586a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在模型 1 中，其精度为 50%，即 50%的预测的危险地震颠簸是正确的，有可能评论说该模型只不过是掷硬币。但是抛硬币是公平的，而我们的数据集却不是，它是一个不平衡的数据集。在一次公平的掷硬币中，正面和反面同样可能发生，但是对于这个数据集，危险的和非危险的地震颠簸不会发生。</p><p id="6f09" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在模型 2 和模型 3 中，召回率最高，可以解释为正确识别的实际危险地震碰撞的 14%。精度也是 16%。f1 的成绩远远好于 Model 1，相当不错。模型 1 的低 f1 分数是因为 2%的不良回忆，这意味着该模型只能预测实际危险地震颠簸的 2%,这是<em class="nq"> no bu </em> eno。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="1a6b" class="kz la it bd lb lc nl le lf lg nm li lj jz nn ka ll kc no kd ln kf np kg lp lq bi translated">随机森林分类器能赢得这场挑战吗？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/e7f7068fb67ddae53c97e5b5b94cd329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0PUfK8tUhqHRE9Pu"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@lucabravo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卢卡·布拉沃</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="9b21" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我选择的下一个模型是随机森林分类器，竞赛获奖的 ML 模型。这是一个健壮的 ML 模型，其中构建了多个决策树来集成它们的输出。最终预测是所有单个决策树预测的函数。这就是随机森林模型表现更好的原因。</p><p id="7996" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我从初始化随机森林分类器开始:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="ad17" class="nf la it nb b gy ng nh l ni nj">from sklearn.ensemble import RandomForestClassifier<br/>model = RandomForestClassifier()</span></pre><p id="7efd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我构建了一个网格来寻找最合适的参数:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="3046" class="nf la it nb b gy ng nh l ni nj">param_grid = {<br/> ‘bootstrap’: [True],<br/> ‘max_depth’: [80, 90, 100, 110],<br/> ‘max_features’: [2, 3, 4],<br/> ‘min_samples_leaf’: [3, 4, 5, 6],<br/> ‘min_samples_split’: [8, 10, 12],<br/> ‘n_estimators’: [100, 200, 300, 500]<br/>}</span></pre><p id="d784" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我还再次为“得分”选择了“精确度”。以下是完整的代码:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="a9ca" class="nf la it nb b gy ng nh l ni nj">from sklearn.ensemble import RandomForestClassifier<br/>model = RandomForestClassifier()</span><span id="7679" class="nf la it nb b gy nk nh l ni nj">param_grid = {<br/> ‘bootstrap’: [True],<br/> ‘max_depth’: [80, 90, 100, 110],<br/> ‘max_features’: [2, 3, 4],<br/> ‘min_samples_leaf’: [3, 4, 5, 6],<br/> ‘min_samples_split’: [8, 10, 12],<br/> ‘n_estimators’: [100, 200, 300, 500]<br/>}</span><span id="869c" class="nf la it nb b gy nk nh l ni nj">grid = GridSearchCV(estimator=model, <br/>                    param_grid=param_grid, <br/>                    cv=StratifiedKFold(shuffle=True), <br/>                    scoring=['precision'],<br/>                    refit='precision',<br/>                    verbose=10)<br/>grid_result = grid.fit(X_train, y_train)</span><span id="0e11" class="nf la it nb b gy nk nh l ni nj">file_name = 'seismic_bump_rf_model.sav'<br/>joblib.dump(model, model_store_location + file_name)</span><span id="54fb" class="nf la it nb b gy nk nh l ni nj">print(grid_result.best_estimator_)</span></pre><p id="03fb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由此得出的模型是—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/f19531cf27954f4067ad94ceace31a5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wduEY9u4QWNaHv4Mx86fpQ.png"/></div></div></figure><p id="5688" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，在预测测试集时，精度为零。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/fda640716c8abcb27f51f075232b3154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m5cwwH-uzgx9RygMB4WaPA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="nt">作者图片</em></p></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="c656" class="kz la it bd lb lc nl le lf lg nm li lj jz nn ka ll kc no kd ln kf np kg lp lq bi translated">训练和调整支持向量机</h1><p id="4945" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">由于随机森林模型真的让我失望，我想到了使用支持向量机(SVM)。支持向量机属于分类算法的<em class="nq">核方法</em>组，用于拟合决策边界。分类基于决策边界的哪一侧是数据点。</p><p id="ba13" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">支持向量机的计算开销很大，可以通过以下网格搜索设置进行演示:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="92ea" class="nf la it nb b gy ng nh l ni nj">from sklearn.svm import SVC <br/>model = SVC()</span><span id="6090" class="nf la it nb b gy nk nh l ni nj">param_grid1 = {'C': [0.1, 1, 10, 100, 1000],  <br/>              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], <br/>              'kernel': [ 'linear', 'poly', 'rbf', 'sigmoid']}</span></pre><p id="671b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">安装这个网格花了超过 10 个小时，实际上我不得不中断。我检查了多边形内核有更好的结果。所有线性核都很快，但精度为零。总的来说，这个网格相当于 500 次拟合，每 100 个候选对象 5 次折叠(5 * 5 * 4)。</p><p id="0077" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，我构建了一个新网格，如下所示:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="5c40" class="nf la it nb b gy ng nh l ni nj">param_grid2 = {'C': [1],  <br/>              'gamma': [1], <br/>              'kernel': ['poly', 'rbf', 'sigmoid']}</span></pre><p id="c878" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个网格总共有 15 次拟合，花了大约 3.5 个小时来完成调整过程。</p><p id="437a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，按照同样的过程，我看了看最适合的模型，如下所示:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="0489" class="nf la it nb b gy ng nh l ni nj">grid_result.best_estimator_</span><span id="e318" class="nf la it nb b gy nk nh l ni nj"><strong class="nb iu"><em class="nq">Output:<br/>SVC(C=1, gamma=1)</em></strong></span></pre><p id="a17e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">默认的核是“rbf”或径向基函数(RBF)。要了解更多关于内核函数的信息，请参考<a class="ae ky" href="https://scikit-learn.org/stable/modules/svm.html#svm-kernels" rel="noopener ugc nofollow" target="_blank"> Scikit-learn 文档</a>。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="038d" class="nf la it nb b gy ng nh l ni nj">grid_result.best_estimator_.kernel</span><span id="f78c" class="nf la it nb b gy nk nh l ni nj"><strong class="nb iu"><em class="nq">Output:<br/>rbf</em></strong></span></pre><p id="8427" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下是 SVM 的最终性能结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/d94d51ddcbf0c94364457e84cf364cf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tuJQmXnltMlqdNpQ1yid5Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="nt">作者图片</em></p></figure><p id="5487" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">就精度而言，这看起来比迄今为止所有其他模型都要好。然而，召回只是稍微好一点。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="c288" class="kz la it bd lb lc nl le lf lg nm li lj jz nn ka ll kc no kd ln kf np kg lp lq bi translated">现在，我们来对比总结一下…</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/1c3f277ad98ca18949dad4246f2755de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bt4XinsO1Y-B7MdHvQQWAQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个分类器的最佳精度(图片由作者提供)</p></figure><p id="2c0b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在上表中，SVM 的精确度略好，但精确度有了很大的提高，67%的危险地震隆起的实际数量被正确预测。然而，召回率是 6%，即实际危险的地震颠簸中只有 6%是正确的。KNN 车型的 f1 得分低于 SVM 车型。</p><p id="583a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">总的来说，由于无法识别危险的地震碰撞带来的风险因素，该问题陈述需要更好的回忆。召回中唯一表现较好的模型是 KNN，其权重一致，n _ neighbours 为 1，leaf_size 为 10。</p><p id="3d7e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">业务需求的重要性因情况而异。有时回忆是首选(像这里)，有时精确是首选。如果对预测的结果采取行动是有成本的，那么精确度就很重要，也就是说，你会想要更少的假阳性。当成本与每个假阴性相关联时，需要更高的召回率。在两者都重要的情况下，f1 分数，即精确度和召回率的调和平均值，变得重要。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="255a" class="kz la it bd lb lc nl le lf lg nm li lj jz nn ka ll kc no kd ln kf np kg lp lq bi translated">参考:</h1><p id="2da8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1]西科拉·m .、弗罗贝尔·l .:规则归纳算法在煤矿地震灾害监测系统收集的数据分析中的应用。采矿科学档案，55(1)，2010，91–114。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="a021" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="nq">感谢您的来访。我希望你喜欢阅读这个博客！</em></p><p id="18e1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有关使用 SMOTE 处理不平衡类的更多信息，请参考这篇由<a class="oe of ep" href="https://medium.com/u/7a033ba6b705?source=post_page-----8b9104b611b0--------------------------------" rel="noopener" target="_blank"> CathL </a>及其团队撰写的博客:</p><div class="og oh gp gr oi oj"><a href="https://medium.com/womeninai/predicting-hazardous-seismic-bumps-part-iii-improving-model-performance-for-imbalanced-datasets-88fa64b4d622" rel="noopener follow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">预测危险的地震颠簸第三部分:改善不平衡数据集的模型性能</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">新手评估试用机器学习模型进行分类和数据扩充，以更好地支持…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">medium.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ks oj"/></div></div></a></div></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="a469" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">本笔记本的 GitHub 链接:</strong></p><div class="og oh gp gr oi oj"><a href="https://github.com/royn5618/Medium_Blog_Codes/blob/master/Predicting%20Hazardrous%20Seismic%20Bumps/Predicting_Seismic_Bumps.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">royn5618/Medium_Blog_Codes</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">github.com</p></div></div><div class="os l"><div class="oy l ou ov ow os ox ks oj"/></div></div></a></div></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="2626" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">我的链接:</strong> <a class="ae ky" href="https://medium.com/@nroy0110" rel="noopener">中</a>|<a class="ae ky" href="https://www.linkedin.com/in/nabanita-roy/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>|<a class="ae ky" href="https://github.com/royn5618" rel="noopener ugc nofollow" target="_blank">GitHub</a></p></div></div>    
</body>
</html>