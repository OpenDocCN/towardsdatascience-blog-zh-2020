# 利用自然语言处理从期刊论文中提取分类数据

> 原文：<https://towardsdatascience.com/extracting-taxonomic-data-from-a-journal-articles-using-natural-language-processing-ab794d048da9?source=collection_archive---------38----------------------->

## 从期刊文章中提取科学用语的快速方法

![](img/943cc4c1e2e6d24736f70b9a543c9972.png)

澳大利亚阿德莱德州立图书馆。图片来源:弗拉德·库特波夫@ Unsplash

我最近遇到了一个客户的问题，他们希望从一篇 PDF 格式的期刊文章中提取所有的“科学词汇和名称”以及作者姓名和大学。虽然可以一个字一个字地浏览文章，并将其与分类名称列表进行比较，但这对于一篇期刊文章来说极其麻烦，尤其低效。相反，我使用自然语言处理的常用元素来确定期刊文章中的科学术语/分类数据。下面是一个教程，介绍如何从期刊文章 PDF 中提取文本，准备进行处理，并最终使用 Python libraries for NLP 从文章中获取分类或科学数据列表。

# 要加载的包

为此，我使用了 PyPDF、NLTK、pandas、scikit-learn 和 re。您需要 pip 安装这些包，特别是对于 NLTK，您需要通过使用 nltk.download('stopwords ')和 nltk.download('word_tokenize ')下载 stopwords 和 word_tokenize。

# 从期刊文章中提取文本

我将使用我随机选择的这篇期刊文章来浏览这个例子。有许多关于如何做到这一点的好文章，所以请随意查阅，但我是用 PyPDF2 做的。使用 PyPDF2 读取所有页面后，我将结果写入一个 txt 文件并处理。txt 文件。

# 清理用于分析的文本

现在我们有了要处理的语料库，我们必须做一些清理工作。使用正则表达式，我删除了数字、停用词、少于 3 个字符的单词和标点符号。我还定制了一个停用词表，删除那些“科学”但对我的分析没有特别启发性的词。运行代码后，可以修改该列表，删除可能没有用的额外单词。

# 使用 TFIDF 进行标记和提取

我们利用了上面定义的清理函数，并对清理后的输出进行了标记化(拆分成单个单词)。之后，我们得到了二元模型(词对)和三元模型(词三元组)，然后对它们进行了矢量化。在矢量化之后，我们使用 [TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) 来提取有可能成为重要科学词汇的单词。输出遵循代码。

虽然我们看到一些不相关的短语(如前所述),但这是论文中科学用语的一个很好的摘录。

# 结论

这篇文章展示了一种从发表的文章中提取科学信息的快速方法。对于特定的论文，可以对您的代码进行一些定制，以删除额外的短语。整个脚本可以在我的 Github 库[这里](https://github.com/melanielaffin/taxonomy)找到。