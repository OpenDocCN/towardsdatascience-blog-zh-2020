<html>
<head>
<title>How to train CNNs on ImageNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在ImageNet上训练CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-cnns-on-imagenet-ab8dd48202a9?source=collection_archive---------18-----------------------#2020-05-24">https://towardsdatascience.com/how-to-train-cnns-on-imagenet-ab8dd48202a9?source=collection_archive---------18-----------------------#2020-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="60e9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用大型图像数据集进行深度学习的实用指南</h2></div><p id="5f77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将介绍如何获得ImageNet数据集，并在其上训练您的卷积神经网络。我添加了一些建议和学习，专门针对用PyTorch训练CNN。</p><h1 id="7900" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated"><strong class="ak">开始前</strong></h1><p id="00e2" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">如果您还没有这样做，我建议您首先尝试在样本图像上运行您的模型。当你开始的时候，跳到一个像ImageNet这样的大数据集来训练你的下一个艺术模型是非常诱人的<em class="mb"> </em>。然而，我发现<strong class="kk iu">从小处着手，慢慢扩大你的实验</strong>更有效。首先，尝试一个图像，以确保您的代码正常工作。然后，尝试一个更小的数据集，如CIFAR-10。最后，在ImageNet上试用一下。沿途进行健全性检查，并在每次“向上扩展”时重复这些检查。</p><p id="cd84" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，请注意一个数据集相对于另一个数据集的较小图像大小在模型中的差异。例如，CIFAR-10只有32x32尺寸的图像，比ImageNet的可变图像尺寸要小。ImageNet图像的平均分辨率为469x387。在图像预处理步骤中，它们通常被裁剪为256x256或224x224。</p><p id="b637" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在PyTorch中，我们不像在TensorFlow中那样定义输入高度或宽度，因此您的工作是<strong class="kk iu">确保输出通道大小在您的网络中适合给定的输入大小</strong>。我的建议是要警惕降维是如何在你的网络中由浅入深地发生的，尤其是当你改变数据集的时候。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/d68d374f2c99bee3c393e6452e402e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*El8wnxPDg35bmu2T.jpg"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">VGG-16网络架构。输出通道的尺寸缩小(高x宽)。如果我们改变输入大小，会遇到什么错误？[来源:<a class="ae ms" href="https://bit.ly/36tOKUC" rel="noopener ugc nofollow" target="_blank">https://bit.ly/36tOKUC</a>]</p></figure><p id="eff3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一般来说，当您增加新数据集的输入分辨率时，早期感受野的大小也应该增加(通过增加内核大小或添加池层)。</p><p id="222b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这有两个原因:</p><ol class=""><li id="54fb" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated">增加早期感受野的大小是一种规则化的形式，以保护你的CNN不去学习不太概括的图像的超具体细节。</li><li id="5a8c" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">降低输入分辨率时，这将有助于避免通道尺寸过早缩小。对256x1x1大小的张量应用卷积是没有用的。</li></ol><p id="c593" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这两个错误都在<em class="mb">无声无息地</em>失败。当ImageNet形状的ResNet不适当地应用于CIFAR-10时，这些误差仅导致top 1精度下降<strong class="kk iu">8%</strong>。为了纠正这一错误，当从CIFAR-10迁移到ImageNet时，ResNet作者添加了一个早期的max-pool层，并使用更大的初始内核大小(5x5 → 7x7)。</p><p id="b095" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我真的很推荐阅读安德烈·卡帕西的这篇博文，对这门艺术有更深的直觉。我也推荐蒂姆·罗克塔舍尔的这篇博文<a class="ae ms" href="https://rockt.github.io/2018/08/29/msc-advice" rel="noopener ugc nofollow" target="_blank">，给你一些短期ML项目的建议。</a></p><h1 id="c005" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated"><strong class="ak">下载ImageNet </strong></h1><p id="e081" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">这最好在云环境中完成。除非你有一个强大的GPU和一个大的SSD，否则我不建议在本地这样做。</p><p id="cf4c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在进行任何培训之前，启动Google Colab实例或AWS SageMaker实例，使用Jupyter笔记本来试验您的模型&amp;可视化传入的数据。然后，当你想训练你的模型时，我建议使用一个脚本，并用AWS <a class="ae ms" href="https://aws.amazon.com/marketplace/pp/B077GF11NF" rel="noopener ugc nofollow" target="_blank">深度学习AMI </a>构建一个EC2实例。将一个EBS实例连接到EC2，并为下载&amp;解压缩ImageNet提供足够的存储空间。对于2012 ImageNet，压缩下载为150GB。但是您将需要大约400GB，因为您需要足够的空间来解压缩文件，然后删除。事后焦油。使用EBS实例还意味着您可以升级EC2，而不必重新下载数据。</p><p id="ee4a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在要真正下载ImageNet，官方的<em class="mb"> </em>说明是到你的研究机构<a class="ae ms" href="http://image-net.org/download-images" rel="noopener ugc nofollow" target="_blank">这里</a>报名成为研究员。</p><p id="8697" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我不认为斯坦福大学已经保持这种状态很长时间了，因为你永远不会收到电子邮件邀请。所以，我发现有效的是从<a class="ae ms" href="http://academictorrents.com/" rel="noopener ugc nofollow" target="_blank">学术洪流</a>下载ImageNet。</p><p id="b394" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">搜索ImageNet，获取所需的magnet链接，并使用CLI下载torrents with Transmission。确保您的实例可以访问互联网！</p><pre class="md me mf mg gt nh ni nj nk aw nl bi"><span id="e7b8" class="nm lf it ni b gy nn no l np nq">sudo yum install transmission transmission-daemon transmission-cli</span></pre><p id="51fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后设置你的下载目录</p><pre class="md me mf mg gt nh ni nj nk aw nl bi"><span id="5a07" class="nm lf it ni b gy nn no l np nq">transmission-daemon --download-dir "your-download-directory-path"</span></pre><p id="c75a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并添加您的磁铁链接</p><pre class="md me mf mg gt nh ni nj nk aw nl bi"><span id="e08e" class="nm lf it ni b gy nn no l np nq">transmission-remote -a "magnet-link"</span></pre><p id="3dd2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此找到其他重要命令<a class="ae ms" href="https://cli-ck.io/transmission-cli-user-guide/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="7f23" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦您下载了压缩文件，我们希望将它们解压缩并放入正确的文件夹中，以便它们与PyTorch ImageFolder类所期望的相匹配，如文档<a class="ae ms" href="https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder" rel="noopener ugc nofollow" target="_blank">此处</a>所述。</p><p id="dfe4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将ILSVRC2012_img_train.tar和ILSVRC2012_img_val.tar放在与以下脚本相同的文件夹中，以获得所需的文件夹。根据需要为您的特定种子进行编辑。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="570e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我也建议两个都扔。在S3把柏油文件放到一个桶里，这样你下次就可以从那里拿到了。不要扔掉未压缩的文件，因为你要为S3上每个对象的单独请求付费。</p><h1 id="f89c" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated"><strong class="ak">设置数据加载器</strong></h1><p id="8448" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">我建议在一个名为dataset的模块中设置PyTorch的DataLoader和ImageFolder的用法。我发现在不同的文件中保存数据集特定的扩充很容易。这里有一个使用ResNet的例子<code class="fe nt nu nv ni b"><a class="ae ms" href="http://imagenet.py" rel="noopener ugc nofollow" target="_blank">imagenet.py</a></code>。设置默认批量大小、归一化变换以及特定于该数据集的裁剪。也许在另一个像<a class="ae ms" href="http://cifar10.py" rel="noopener ugc nofollow" target="_blank"> cifar10.py </a>这样的文件中，你可以使用特定于cifar-10的设置(不同的批量大小、归一化和裁剪)来装载数据集。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h1 id="a4ff" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated"><strong class="ak">使用ImageNet进行培训</strong></h1><p id="ed9e" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">我不建议在Jupyter笔记本上的ImageNet或Sports1M这样的大规模数据集上训练模型。您可能会超时，并且您的实例将从stdout断开，这将导致您看不到您的模型正在取得的进展。一个更安全的选择是在<a class="ae ms" href="https://ss64.com/bash/screen.html" rel="noopener ugc nofollow" target="_blank">屏幕</a>中使用一个脚本进行ssh和训练。</p><p id="a969" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我还推荐使用<a class="ae ms" href="http://neptune.ai" rel="noopener ugc nofollow" target="_blank"> neptune.ai </a>在一个简洁的可视化仪表板中跟踪进度。有些人用TensorBoard或TensorBoardX来做pytorch，但我还没有试过。我喜欢<a class="ae ms" href="https://neptune.ai/" rel="noopener ugc nofollow" target="_blank"> neptune.ai </a>,因为即使在我关闭实例后，它也能保留我的结果，并让我轻松地比较实验。</p><p id="75a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，将您的数据加载器与您的模型、您选择的优化器以及您选择的损失一起用于ImageNet训练。它看起来像下面伪代码的一些变体:</p><pre class="md me mf mg gt nh ni nj nk aw nl bi"><span id="af41" class="nm lf it ni b gy nn no l np nq"># one epoch<br/>for i, (images, target) in enumerate(train_loader):</span><span id="b776" class="nm lf it ni b gy nw no l np nq">        # compute output<br/>        output = model(images)<br/>        loss = criterion(output, target)</span><span id="6e4f" class="nm lf it ni b gy nw no l np nq">        # measure accuracy and record loss<br/>        acc1, acc5 = accuracy(output, target, topk=(1, 5))<br/>        losses.update(loss.item(), images.size(0))<br/>        top1.update(acc1[0], images.size(0))<br/>        top5.update(acc5[0], images.size(0))</span><span id="5819" class="nm lf it ni b gy nw no l np nq">        # compute gradient and do step<br/>        optimizer.zero_grad()<br/>        loss.backward()<br/>        optimizer.step()</span></pre><p id="de47" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这只是为了训练。在带有验证函数的循环中使用该函数，以便在每个时期对验证集交替进行训练和评分。关于如何做到这一点的更多例子，请看官方PyTorch例子<a class="ae ms" href="https://github.com/pytorch/examples" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="aa72" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">记得在数据进入你的网络之前至少看一次数据。这意味着实际观想它。下面是一个完整性检查示例，用于确保预处理过程中一切顺利。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="86f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了完整起见，我在健全性检查上面添加了一些代码来生成反规格化变换(查看没有规格化效果的实际图像)。</p><p id="56e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在享受训练的乐趣&amp;通过理智检查保持你的理智！😄</p></div></div>    
</body>
</html>