<html>
<head>
<title>Understanding Google’s BigBird — Is It Another Big Milestone In NLP?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解 Google 的 Big bird——它是 NLP 的另一个重要里程碑吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-bigbird-is-it-another-big-milestone-in-nlp-e7546b2c9643?source=collection_archive---------6-----------------------#2020-09-09">https://towardsdatascience.com/understanding-bigbird-is-it-another-big-milestone-in-nlp-e7546b2c9643?source=collection_archive---------6-----------------------#2020-09-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="19f3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">谷歌研究人员最近在 arXiv 上发表了一篇题为《大鸟:更长序列的变形金刚》的论文。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/981175a41079ed6d245963017940e885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gYQ-QIpkHLIapyr8JllWcA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae kv" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2167835" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的 Gerd Altmann 提供</p></figure><p id="cdb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">去年，谷歌的研究人员发布了 BERT，这被证明是自 RankBrain 以来最高效和最有效的算法变革之一。看看最初的结果，BigBird 也有类似的迹象！</p><p id="8ae9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我介绍了:</p><ul class=""><li id="d067" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">基于变压器的模型的简要概述，</li><li id="c082" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">基于变压器的模型的局限性，</li><li id="031b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">什么是大鸟，还有</li><li id="f402" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">大鸟的潜在应用。</li></ul><p id="8bb2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们开始吧！</p><h1 id="cb3c" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">基于变压器的模型概述</h1><p id="2bee" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">自然语言处理(NLP)在过去的几年中有了很大的改进，基于变形金刚的模型在其中扮演了重要的角色。尽管如此，仍有许多事情有待发现。</p><p id="ed59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Transformers 是 2017 年推出的自然语言处理模型，主要用于提高处理和理解文本翻译和摘要等任务的顺序数据的效率。</p><p id="6574" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与在输入结束之前处理输入开始的递归神经网络(RNNs)不同，变压器可以并行处理输入，从而显著降低计算的复杂性。</p><p id="7c6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BERT 是 NLP 中最大的里程碑式的成就之一，它是一个开源的基于 Transformers 的模型。一篇介绍 BERT  的<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">论文，与大鸟一样，由谷歌研究人员于 2018 年 10 月 11 日发表。</a></p><p id="b8d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">来自变压器的双向编码器表示(BERT)是基于变压器的高级模型之一。它在大量数据(预训练数据集)上进行预训练，BERT-Large 在超过 25 亿个单词上进行训练。</p><p id="c24f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">话虽如此，BERT 是开源的，允许任何人创建自己的问题回答系统。这也有助于它的广泛流行。</p><p id="4fe8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是 BERT 不是唯一的上下文预训练模型。然而，与其他模型不同，它是双向的。这也是其成功和应用多样的原因之一。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/43f6c1c749d6cc4f91ca00145738bab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*QOgBI--QsSqYovn6Fp4h-w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ne">来源</strong> </a></p></figure><p id="5474" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个预训练模型的结果绝对令人印象深刻。它被成功地用于许多基于序列的任务，如摘要、翻译等。甚至谷歌也采用 BERT 来理解其用户的搜索查询。</p><p id="7503" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，像其他基于变形金刚的模型一样，BERT 也有其自身的局限性。</p><h1 id="1fef" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">以前基于变压器的模型的局限性</h1><p id="7885" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">虽然基于变压器的模型，尤其是 BERT，比 rnn 有很大的改进和效率，但它们也有一些限制。</p><p id="f0f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">伯特工作在一个完全自我关注的机制上。这导致每个新输入令牌的计算和存储需求呈二次增长。最大输入大小约为 512 个令牌，这意味着该模型不能用于大型文档摘要等任务的较大输入&amp;。</p><p id="5992" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这基本上意味着在将一个大字符串作为输入应用之前，必须将其分解成更小的片段。这种内容碎片还会导致大量的上下文丢失，从而限制了它的应用。</p><p id="fae6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么，<em class="nf">什么是大鸟，它与 BERT 或者其他基于变形金刚的 NLP 模型有什么不同？</em></p><h1 id="f094" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">介绍大鸟—适用于更长序列的变压器</h1><p id="fdf8" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">如前所述，BERT 和其他基于变形金刚的 NLP 模型的一个主要限制是，它们运行在一个完全的自我关注机制上。</p><p id="e96d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当谷歌的研究人员在 arXiv 上发表了一篇题为<a class="ae kv" href="https://arxiv.org/pdf/2007.14062.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">“大鸟:更长序列的变形金刚”</strong> </a> <strong class="ky ir">的论文后，这种情况发生了变化。</strong></p><p id="a761" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BigBird 运行在一种稀疏注意力机制上，这种机制允许它克服 BERT 的二次依赖性，同时保留完全注意力模型的属性。研究人员还提供了大鸟支持的网络模型如何超越以前的 NLP 模型以及基因组学任务的性能水平的实例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ff909575f3f9d54d88f611d336dd11fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*4iAjyRtn65NAP-Oxm_PwLg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://arxiv.org/pdf/2007.14062.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="95f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们进入 BigBird 的可能应用之前，让我们来看看 BigBird 的主要亮点。</p><h2 id="725a" class="nh mh iq bd mi ni nj dn mm nk nl dp mq lf nm nn ms lj no np mu ln nq nr mw ns bi translated">大鸟的主要亮点</h2><p id="5d18" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">以下是 BigBird 的一些特性，这些特性使它优于以前基于 transformer 的模型。</p><ul class=""><li id="898e" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">稀疏注意机制</strong></li></ul><p id="bafe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设您收到一张图片，并被要求为其创建一个相关的标题。你将从识别图片中的关键物体开始，比如说一个扔“球”的人。</p><p id="9c9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为人类，识别这个主要对象对我们来说很容易，但是为计算机系统简化这个过程在 NLP 中是一件大事。注意力机制被引入来降低整个过程的复杂性。</p><p id="8156" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大鸟使用稀疏注意力机制，这使它能够处理</p><p id="c8e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">序列长度比使用 BERT 可能达到的长度多 8 倍。请记住，这个结果可以使用与 BERT 相同的硬件来实现。</p><p id="e620" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 BigBird 的上述论文中，研究人员展示了 BigBird 中使用的稀疏注意机制如何与完全自我注意机制(在 BERT 中使用)一样强大。除此之外，他们还展示了<em class="nf">“图灵完全编码解码器有多稀疏”。</em></p><p id="ea37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简而言之，BigBird 使用稀疏注意机制，这意味着注意机制是一个令牌一个令牌地应用的，不像 BERT 那样，注意机制只应用于整个输入一次！</p><ul class=""><li id="746a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">可以处理长达 8 倍的输入序列</strong></li></ul><p id="1be4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BigBird 的一个关键特性是它能够处理比以前长 8 倍的序列。</p><p id="f800" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">研究小组设计了 BigBird，以满足像 BERT 这样的完整变形金刚的所有要求。</p><p id="129b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用大鸟和它的稀疏注意力机制，研究小组将 O(n)(伯特的)复杂性降低到仅为 O(n)。这意味着限制为 512 个记号的输入序列现在增加到 4096 个记号(8 * 512)。</p><p id="870f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">创造大鸟的研究人员之一 Philip Pham 在一次<a class="ae kv" href="https://news.ycombinator.com/item?id=24041758" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">黑客新闻讨论</strong> </a> — <strong class="ky ir"> <em class="nf">中说，“在我们的大部分论文中，我们使用 4096，但我们可以使用更大的 16k+”</em>T11】</strong></p><ul class=""><li id="7f80" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">根据大型数据集进行预训练</strong></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/b0ace172aef8afb3c8acc302a634830e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/0*zDPI_IxRGTk3UyRI"/></div></figure><p id="27b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谷歌研究人员在大鸟的预训练中使用了 4 个不同的数据集——<a class="ae kv" href="https://research.google/pubs/pub47761/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"/></a><strong class="ky ir"/><a class="ae kv" href="http://nlp.cs.washington.edu/triviaqa/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">琐事-QA</strong></a><strong class="ky ir"/><a class="ae kv" href="https://www.aclweb.org/anthology/D18-1259/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">HotpotQA-分心物</strong></a><strong class="ky ir">&amp;</strong><a class="ae kv" href="https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00021" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">WikiHop</strong></a>。</p><p id="fead" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然 BigBird 的集体预训练数据集远不如 GPT-3 的大(在 1750 亿个参数上训练)，但研究论文的表 3 显示，它的表现优于 RoBERTa(一种稳健优化的 BERT 预训练方法)和 Longformer(一种用于长文档的类似 BERT 的模型)。</p><p id="bf8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当一位<a class="ae kv" href="https://news.ycombinator.com/item?id=24041758" rel="noopener ugc nofollow" target="_blank">用户要求 Philip Pham </a>比较 GPT-3 和大鸟时，他说—<strong class="ky ir"><em class="nf">“GPT-3 只使用 2048 的序列长度。“大鸟”只是一种注意力机制，实际上可能是 GPT-3 的补充。”</em>T41】</strong></p><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/the-story-of-newly-found-friendship-ai-ml-and-the-pandemic-6ceb02376ebd"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">新发现的友谊的故事:艾、曼梯·里和疫情</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">这篇文章讲述了这个疫情如何给了人工智能-人工智能阿朵急需的推动，以及这个新发现的…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol kp nx"/></div></div></a></div><h1 id="2fc2" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">大鸟的[可能]应用</h1><p id="7b97" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">一篇介绍大鸟的论文是最近才推出的——2020 年 7 月 28 日。因此，大鸟的全部潜力还有待确定。</p><p id="28fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是这里有几个可能的应用领域。其中几个应用也是 BigBird 的创作者在<a class="ae kv" href="https://arxiv.org/pdf/2007.14062.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">原创研究论文</strong> </a>中提出的。</p><ul class=""><li id="0199" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">基因组处理</strong></li></ul><p id="a980" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度学习在基因组数据处理中的使用有所增加。编码器将 DNA 序列片段作为任务的输入，如甲基化分析、预测非编码变体的功能效应等。</p><p id="7fa5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BigBird 的创建者说:<strong class="ky ir"> <em class="nf">“我们引入了一种基于注意力的模型的新应用，其中长上下文是有益的:提取基因组序列(如 DNA)的上下文表示”。</em>T9】</strong></p><p id="a1d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在使用 BigBird 进行启动子区域预测时，该论文声称已经将最终结果的准确性提高了 5%!</p><ul class=""><li id="868f" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">长文档摘要&amp;问答</strong></li></ul><p id="94f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于 BigBird 现在可以处理 8 倍长的序列长度，它可以用于 NLP 任务，如较长文档形式的摘要和问题回答。在大鸟的创建过程中，研究人员还测试了它在这些任务中的表现，并见证了“最先进的结果”。</p><ul class=""><li id="b467" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">谷歌搜索大鸟</strong></li></ul><p id="de41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">谷歌于 2019 年 10 月开始使用 BERT </strong> </a>来理解搜索查询，并为其用户显示更多相关结果。谷歌更新搜索算法的最终目的是比平时更好地理解搜索查询。</p><p id="965d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着 BigBird 在自然语言处理(NLP)方面优于 BERT，开始使用这一新发现的更有效的模型来优化 Google 的搜索结果查询是有意义的。</p><ul class=""><li id="91e3" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir"> Web &amp;手机 App 开发</strong></li></ul><p id="a3e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">十年来，自然语言处理取得了显著的进步。有了一个 GPT-3 驱动的平台，它可以将你的简单语句转化为已经到位的功能性 web 应用程序(以及代码)，<a class="ae kv" href="https://www.resourcifi.com/hire-ai-developer/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">人工智能开发者</strong> </a>可以真正改变你开发 web &amp; web 应用程序的方式。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="7820" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于大鸟可以处理比 GPT-3 更长的输入序列，它可以与 GPT-3 一起使用，为您的业务高效快速地创建<a class="ae kv" href="https://www.resourcifi.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> web &amp;移动应用</strong> </a>。</p><h1 id="3e60" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="294e" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">虽然关于 BigBird 还有很多有待探索，但它绝对有能力彻底改变自然语言处理(NLP)。<em class="nf">你对大鸟及其对 NLP 未来的贡献有什么看法？</em></p><p id="6c0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考资料:</strong><br/>【1】曼齐尔·扎希尔和他的团队，《大鸟:更长序列的变形金刚》(2020)，《T9》arXiv.org</p><p id="4259" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]Jacob Devlin，张明蔚，Kenton Lee，Kristina Toutanova，BERT:用于语言理解的深度双向转换器的预训练，<a class="ae kv" href="https://arxiv.org/" rel="noopener ugc nofollow" target="_blank">arXiv.org</a></p></div></div>    
</body>
</html>