<html>
<head>
<title>Back to Basics: Assumptions of Common Machine Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归基础:常见机器学习模型的假设</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/back-to-basics-assumptions-of-common-machine-learning-models-e43c02325535?source=collection_archive---------22-----------------------#2020-06-08">https://towardsdatascience.com/back-to-basics-assumptions-of-common-machine-learning-models-e43c02325535?source=collection_archive---------22-----------------------#2020-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/34ac67fe77870291d2bacfd17ece6fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VaS-bSBYDg0qS8z_"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@thoughtcatalog?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">思想目录</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><div class=""/></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><p id="d96a" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">暂时忘记深度学习和神经网络。</p><p id="b3e3" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">随着越来越多的人开始进入数据科学领域，我认为重要的是不要忘记这一切的基础。</p><p id="8216" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kp jk">统计。</strong></p><p id="f34b" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">如果你是分析领域的新手，没关系！我们都是从某处开始的！</p><p id="1063" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">但重要的是<strong class="kp jk"> <em class="ll">意识到</em> </strong>我将在这篇文章中分享的机器学习模型假设的存在。</p><p id="727f" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我很幸运在我的大学时代就已经研究了所有这些概念，所以我认为回到基础上写一些关于它们的东西会令人耳目一新。</p><p id="75e8" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">关心就是分享。😃</p><p id="4538" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们开始吧！</p></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><h1 id="7674" class="lm ln jj bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">线性回归又称普通最小二乘(OLS)回归</h1><p id="ca50" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">OLS回归试图解释自变量(预测值)和因变量(目标值)之间是否存在关系。</p><p id="9a2a" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">它通过最小化残差的平方和来拟合你的数据。</p><blockquote class="mp mq mr"><p id="17af" class="kn ko ll kp b kq kr ks kt ku kv kw kx ms kz la lb mt ld le lf mu lh li lj lk im bi translated">残差是观察值和预测值之间的差值。残差用于指示模型与数据的拟合程度。</p></blockquote><p id="402c" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">然而，为了能够信任和对结果有信心，在建模之前必须满足一些假设。</p><p id="4736" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">满足所有这些假设将允许您为您的模型创建可能的最佳估计。</p><p id="75c6" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">OLS回归模型中有5个关键假设。</p><h2 id="f070" class="mv ln jj bd lo mw mx dn ls my mz dp lw ky na nb ma lc nc nd me lg ne nf mi ng bi translated">假设1:自变量和因变量之间存在线性关系。</h2><p id="2ffb" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">当我第一次在统计课上听到这个假设时，我措手不及。</p><p id="098b" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我记得当我回顾我的考试成绩时，我感到如此的被欺骗和被欺骗，这已经深深地刻在了我的记忆中。</p><p id="306d" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">精神食粮。</p><p id="2b39" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这些等式中的哪一个符合这个假设？</p><p id="9d3e" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><code class="fe nh ni nj nk b">Y = β₀ + β₁X₁+ β₂X₂</code></p><p id="5895" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><code class="fe nh ni nj nk b">Y = β₀ + β₁X₁+ β₂X₂²</code></p><p id="db19" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">原来<strong class="kp jk">两者都是线性的。</strong></p><p id="459c" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">对于被认为是<em class="ll">线性方程</em>的东西，经常会有<strong class="kp jk">的曲解</strong>。</p><blockquote class="nl"><p id="f07c" class="nm nn jj bd no np nq nr ns nt nu lk dk translated">线性方程组=直线<br/>非线性方程组=曲线<br/>这是错误的。</p></blockquote><p id="7e79" class="pw-post-body-paragraph kn ko jj kp b kq nv ks kt ku nw kw kx ky nx la lb lc ny le lf lg nz li lj lk im bi translated">当统计学家说一个方程是线性的时，他们指的是参数的线性，并且这个方程有一定的格式。</p><p id="1baa" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这是格式:</p><p id="6cba" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><code class="fe nh ni nj nk b"><strong class="kp jk">Y = Constant + Parameter1 * Variable1 + Parameter2 * Variable2 …</strong></code></p><p id="534e" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">注意:</p><ol class=""><li id="2714" class="oa ob jj kp b kq kr ku kv ky oc lc od lg oe lk of og oh oi bi translated">必须有一个常数</li><li id="e821" class="oa ob jj kp b kq oj ku ok ky ol lc om lg on lk of og oh oi bi translated">其他项遵循“<em class="ll">参数*变量</em>”的模式，所有项都加在一起。</li></ol><p id="d106" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kp jk"> <em class="ll">变量</em> </strong>是否非线性(即平方)并不重要，只要<strong class="kp jk"> <em class="ll">方程</em> </strong>遵循这种指定格式，就是线性方程。任何其他不遵循这种格式的方程都是非线性的。</p><p id="e6d8" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这也意味着一些线性方程线在拟合时是弯曲的。</p><p id="b980" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">所以从技术上来说…单独使用散点图并不能真正告诉你你看到的拟合曲线是否是线性的。你可能需要看看曲线的方程式。</p><h2 id="6d27" class="mv ln jj bd lo mw mx dn ls my mz dp lw ky na nb ma lc nc nd me lg ne nf mi ng bi translated">假设2:没有多重共线性</h2><p id="2354" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">多重共线性是指自变量之间的高度相关性。</p><p id="ba7a" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">多重共线性是一个问题，因为它会产生冗余信息，导致回归模型的结果不可靠。</p><p id="c464" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了避免这个问题，您可以部署两种技术:</p><ol class=""><li id="c1b4" class="oa ob jj kp b kq kr ku kv ky oc lc od lg oe lk of og oh oi bi translated">对你所有的自变量进行相关性分析。</li><li id="37e6" class="oa ob jj kp b kq oj ku ok ky ol lc om lg on lk of og oh oi bi translated">删除具有高方差通货膨胀系数的独立变量(VIF)*。作为一般的经验法则，a <code class="fe nh ni nj nk b">VIF &gt; 10</code>是多重共线性的强烈指示。</li></ol><blockquote class="mp mq mr"><p id="234c" class="kn ko ll kp b kq kr ks kt ku kv kw kx ms kz la lb mt ld le lf mu lh li lj lk im bi translated"><code class="fe nh ni nj nk b">*VIF = 1 ÷ (1-R²)</code></p></blockquote><h2 id="d6e7" class="mv ln jj bd lo mw mx dn ls my mz dp lw ky na nb ma lc nc nd me lg ne nf mi ng bi translated">假设3:没有自相关</h2><p id="780b" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">自相关是指残差不是相互独立的。即先前的观测残差导致当前观测残差的系统增加/减少。</p><p id="0670" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">因此，它会导致你低估你的方差，这将影响你的置信区间或假设检验的结果。</p><p id="7864" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了检查自相关性，您可以使用Durbin-Watson 'D '测试。<code class="fe nh ni nj nk b">1.5 &lt; d &lt; 2.5</code>之间的任何值都满足这个假设。</p><p id="9f84" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">否则，要修正自相关，您应该在计算标准差时应用“<em class="ll">自相关-稳健标准差(HAC) </em>”公式来修正自相关。</p><blockquote class="mp mq mr"><p id="c31a" class="kn ko ll kp b kq kr ks kt ku kv kw kx ms kz la lb mt ld le lf mu lh li lj lk im bi translated">注意:你可能会遇到“HAC”作为“纽维-韦斯特估值器”。</p></blockquote><h2 id="8e29" class="mv ln jj bd lo mw mx dn ls my mz dp lw ky na nb ma lc nc nd me lg ne nf mi ng bi translated">假设4:残差应该是同方差的</h2><p id="a58b" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">同质性是指你的残差图应该在所有观察值上显示一个<strong class="kp jk"> <em class="ll">甚至</em> </strong>和<strong class="kp jk"> <em class="ll">随机模式</em> </strong>。</p><p id="3531" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">换句话说，残差的方差在所有观测中应该是一致的，不应该遵循某种形式的系统模式。</p><p id="e014" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在下图中，第一个图显示了残差图中的系统模式。这也被称为异方差；推翻了这个假设。</p><p id="4fae" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">下图显示了同方差残差图应该是什么样子。</p><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/e37e8b78c1ab8d6a62389a448a8537bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5bT2Ka9WHAswD34GrkSB9g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">2010年7月29日，由Protonk ( <a class="ae jg" href="https://creativecommons.org/licenses/by-sa/3.0/" rel="noopener ugc nofollow" target="_blank"> CC3.0 </a> ) ( <a class="ae jg" href="https://en.wikipedia.org/wiki/File:Hsked_residual_compare.svg" rel="noopener ugc nofollow" target="_blank">来源</a>)提供的同质性示例</p></figure><p id="c34b" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">那么异方差到底有什么问题呢？</p><ol class=""><li id="4295" class="oa ob jj kp b kq kr ku kv ky oc lc od lg oe lk of og oh oi bi translated">你的无偏估计将不再是最好的。</li><li id="e7b2" class="oa ob jj kp b kq oj ku ok ky ol lc om lg on lk of og oh oi bi translated">它会影响标准误差的计算，而标准误差会无意中影响任何假设检验的结果。</li></ol><p id="8569" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">要解决异方差的第一个问题，一个好方法是增加样本量。</p><p id="c3cc" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">对于第二个问题，你应该应用“<em class="ll">稳健标准误差</em>”公式来解释异方差对你的误差的影响。</p><blockquote class="mp mq mr"><p id="7280" class="kn ko ll kp b kq kr ks kt ku kv kw kx ms kz la lb mt ld le lf mu lh li lj lk im bi translated">注:“稳健标准误差”也称为“异方差一致性标准误差”(HC)。编程的时候可能会遇到“HC”。</p></blockquote><h2 id="5bcd" class="mv ln jj bd lo mw mx dn ls my mz dp lw ky na nb ma lc nc nd me lg ne nf mi ng bi translated">假设5:所有自变量都是正态分布的</h2><p id="9b85" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">就产生最佳无偏估计而言，这个假设是可选的。</p><p id="edeb" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">但是，如果您想要执行假设检验以产生<em class="ll">置信区间</em>或<em class="ll">预测区间</em>，则需要使用它。</p><blockquote class="mp mq mr"><p id="99f0" class="kn ko ll kp b kq kr ks kt ku kv kw kx ms kz la lb mt ld le lf mu lh li lj lk im bi translated">注:你可以在这里回顾一下两个<a class="ae jg" href="https://stats.stackexchange.com/questions/16493/difference-between-confidence-intervals-and-prediction-intervals" rel="noopener ugc nofollow" target="_blank">的区别。</a></p></blockquote><p id="bb17" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">检查正常性有两种方法:</p><ol class=""><li id="e737" class="oa ob jj kp b kq kr ku kv ky oc lc od lg oe lk of og oh oi bi translated">为每个独立变量创建直方图。</li></ol><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/0d82aa277016da4b75191cc2df14a908.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*L6pubkXlaMhSYqU-OswlTQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">直方图示例，Gbdivers于2009年3月16日(<a class="ae jg" href="https://creativecommons.org/licenses/by-sa/2.0/deed.en" rel="noopener ugc nofollow" target="_blank"> CC2.0 </a> ) ( <a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Normality_histogram.png" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="57e7" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">2.在残差上运行Q-Q图。如果残差是正态的，所有的观测值都应该遵循一条直线。</p><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/b060a1f21ecd1279e224d1c924ec12a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*7y9pAS_v4iWPbnYzFvl27w.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">QQ图示例，2009年10月19日，作者<a class="ae jg" href="https://commons.wikimedia.org/wiki/User_talk:Skbkekas" rel="noopener ugc nofollow" target="_blank">Skbkekas</a>(<a class="ae jg" href="https://creativecommons.org/licenses/by-sa/3.0/deed.en" rel="noopener ugc nofollow" target="_blank">cc 3.0</a>)(<a class="ae jg" href="https://en.wikipedia.org/wiki/File:Normal_normal_qq.svg" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="028b" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">如果你需要满足这个假设，但是你的变量不是正态分布的，你可以转换你的变量。</p></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><h1 id="3403" class="lm ln jj bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">逻辑回归</h1><p id="44c7" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">逻辑回归假设与OLS回归的不同之处在于:</p><ol class=""><li id="118b" class="oa ob jj kp b kq kr ku kv ky oc lc od lg oe lk of og oh oi bi translated">自变量和因变量之间不需要线性关系。</li><li id="4f3f" class="oa ob jj kp b kq oj ku ok ky ol lc om lg on lk of og oh oi bi translated">残差不需要是正态的。</li><li id="7111" class="oa ob jj kp b kq oj ku ok ky ol lc om lg on lk of og oh oi bi translated">不需要满足同伦假设</li></ol><p id="f7c1" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">那么逻辑回归需要满足哪些假设呢？</p><p id="1dd3" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">以下是逻辑回归的5个关键假设。</p><h2 id="f321" class="mv ln jj bd lo mw mx dn ls my mz dp lw ky na nb ma lc nc nd me lg ne nf mi ng bi translated">假设1:适当的因变量结构</h2><p id="8f18" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">这个假设简单地说明了二元逻辑回归要求因变量是二分的，而有序逻辑回归要求它是有序的。</p><p id="12e5" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">此外，因变量既不能是区间，也不能是比例。</p><h2 id="1315" class="mv ln jj bd lo mw mx dn ls my mz dp lw ky na nb ma lc nc nd me lg ne nf mi ng bi translated">假设2:结果的logit与各个自变量之间存在线性关系。</h2><p id="d86d" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">logit函数由下式给出:</p><p id="4ea2" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><code class="fe nh ni nj nk b">logit(p) = log(p/(1-p)), where p is the probability of an outcome</code></p><p id="0761" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了检验这一假设，您可以通过在散点图上绘制每个独立变量和logit值来直观地进行检验。</p><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi gj"><img src="../Images/2492d64839a178041132091a55777b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*URuXd1kV7gsvpOxA4h-x-Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">检验逻辑回归的线性假设</p></figure><p id="1e14" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在上图中，Y轴是独立变量，而X轴显示的是logit值。然后看曲线的方程，看是否满足线性假设。</p><p id="08d7" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">请记住，线性存在于参数中。只要方程满足上述线性方程形式，就满足线性假设。</p><blockquote class="mp mq mr"><p id="4808" class="kn ko ll kp b kq kr ks kt ku kv kw kx ms kz la lb mt ld le lf mu lh li lj lk im bi translated">注意:我在x轴标签上犯了一个错误，应该是“Logit”而不是“Logit Probability”</p></blockquote><h2 id="7bcf" class="mv ln jj bd lo mw mx dn ls my mz dp lw ky na nb ma lc nc nd me lg ne nf mi ng bi translated">假设3:没有多重共线性</h2><p id="2a00" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">与OLS回归的假设一样，这里也可以这么说。</p><p id="e829" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><em class="ll">(详见OLS回归部分。)</em></p><h2 id="b967" class="mv ln jj bd lo mw mx dn ls my mz dp lw ky na nb ma lc nc nd me lg ne nf mi ng bi translated">假设4:没有有影响的异常值</h2><p id="2a74" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">有影响的异常值是影响逻辑回归模型质量的极端数据点。</p><p id="a1d9" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kp jk">并非所有的离群值都有影响力。</strong></p><p id="272b" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在移除或变换点以进行分析之前，您需要检查哪些点是有影响的点。</p><p id="5729" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">要检查异常值，可以对数据值运行Cook距离。高库克距离值表示异常值。</p><p id="a40d" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">标记出有影响力的异常值的经验法则是<code class="fe nh ni nj nk b">Cook’s Distance &gt; 1.</code></p><h2 id="d3dd" class="mv ln jj bd lo mw mx dn ls my mz dp lw ky na nb ma lc nc nd me lg ne nf mi ng bi translated">假设5:观察独立性</h2><p id="1d6b" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">这种假设要求逻辑回归观测值相互独立。</p><p id="43d8" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">也就是说，观察结果不应来自<em class="ll">重复测量设计</em>。</p><p id="a2b6" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><em class="ll">重复测量设计</em>是指在不同的实验条件下或跨时间对同一个人进行的同一变量的多次测量。</p><p id="296a" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">重复测量的一个很好的例子是纵向研究——跟踪一个主题多年的进展。</p></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><h1 id="e51b" class="lm ln jj bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">支持向量机(SVM)</h1><p id="2fda" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">SVM没有需要验证的模型假设。</p></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><h1 id="a018" class="lm ln jj bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">基于树的模型</h1><p id="baab" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">对于基于树的模型，如<em class="ll">决策树</em>、<em class="ll">随机森林</em>、&amp;、<em class="ll">梯度推进</em>，没有模型假设需要验证。</p><p id="a254" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">与OLS回归或逻辑回归不同，基于树的模型对异常值是稳健的，并且不需要因变量满足任何正态假设。</p><p id="debd" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为什么基于树的模型对异常值具有鲁棒性？</p><p id="ec63" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">点击<a class="ae jg" href="https://www.quora.com/Why-are-tree-based-models-robust-to-outliers" rel="noopener ugc nofollow" target="_blank">这里</a>查看来自Quora的详细解释。</p></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><h1 id="4d70" class="lm ln jj bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">结尾注释</h1><p id="b965" class="pw-post-body-paragraph kn ko jj kp b kq mk ks kt ku ml kw kx ky mm la lb lc mn le lf lg mo li lj lk im bi translated">嗯，就是这样！</p><p id="c059" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我认为这里的关键要点是，你计划使用<strong class="kp jk">回归</strong>或任何<strong class="kp jk">广义线性模型(GLM) </strong>，在建立你的模型之前，你必须验证一些模型假设。</p><p id="610e" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">对于SVM或基于树的模型，没有任何模型假设需要验证。</p><p id="6b7e" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">希望这篇帖子有所帮助！</p><p id="ca74" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">下期帖子再见！</p><p id="c624" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">안녕히 계세요！</p><p id="a32b" class="pw-post-body-paragraph kn ko jj kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">领英简介:<a class="ae jg" href="https://www.linkedin.com/in/timothy-tan-97587190/" rel="noopener ugc nofollow" target="_blank">谭震东</a></p></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><h1 id="f967" class="lm ln jj bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">参考</h1><ol class=""><li id="5760" class="oa ob jj kp b kq mk ku ml ky ov lc ow lg ox lk of og oh oi bi translated"><a class="ae jg" href="https://www.lexjansen.com/wuss/2018/130_Final_Paper_PDF.pdf" rel="noopener ugc nofollow" target="_blank">https://www.lexjansen.com/wuss/2018/130_Final_Paper_PDF.pdf</a></li><li id="b347" class="oa ob jj kp b kq oj ku ok ky ol lc om lg on lk of og oh oi bi translated"><a class="ae jg" href="https://www.statisticssolutions.com/assumptions-of-logistic-regression/" rel="noopener ugc nofollow" target="_blank">https://www . statistics solutions . com/assumptions-of-logistic-regression/</a></li><li id="a7a3" class="oa ob jj kp b kq oj ku ok ky ol lc om lg on lk of og oh oi bi translated"><a class="ae jg" href="http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/#logistic-regression-assumptions" rel="noopener ugc nofollow" target="_blank">http://www . sth da . com/English/articles/36-class ification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/# logistic-regression-assumptions</a></li><li id="4b54" class="oa ob jj kp b kq oj ku ok ky ol lc om lg on lk of og oh oi bi translated"><a class="ae jg" href="http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression4.html" rel="noopener ugc nofollow" target="_blank">http://SPH web . bumc . bu . edu/otlt/MPH-Modules/BS/R/R5 _ Correlation-Regression/R5 _ Correlation-Regression 4 . html</a></li><li id="d9ba" class="oa ob jj kp b kq oj ku ok ky ol lc om lg on lk of og oh oi bi translated"><a class="ae jg" href="https://www.statisticssolutions.com/assumptions-of-linear-regression/" rel="noopener ugc nofollow" target="_blank">https://www . statistics solutions . com/assumptions-of-linear-regression/</a></li><li id="bca9" class="oa ob jj kp b kq oj ku ok ky ol lc om lg on lk of og oh oi bi translated"><a class="ae jg" href="https://www.quora.com/Why-are-tree-based-models-robust-to-outliers" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/Why-are-tree-based-models-robust to-outliers</a></li></ol></div></div>    
</body>
</html>