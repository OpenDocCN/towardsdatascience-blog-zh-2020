<html>
<head>
<title>What is a Linear Regression?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是线性回归？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-concepts-behind-linear-regression-and-its-implementation-ffbab5a4d65e?source=collection_archive---------8-----------------------#2020-09-03">https://towardsdatascience.com/the-concepts-behind-linear-regression-and-its-implementation-ffbab5a4d65e?source=collection_archive---------8-----------------------#2020-09-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="84bb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">-建立自己的线性回归模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5f572c86b6960a781ea03c39f9121e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cyOf_EfUNNMWxwL9"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@franki?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">弗兰基·查马基</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="546f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">在本文中，您将了解到— </strong></p><ol class=""><li id="2748" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">什么是线性回归？</strong></li><li id="2c8a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">它如何找到输入特征和目标之间的关系？</strong></li><li id="cbf4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">它是如何预测的？</strong></li><li id="d55f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">如何评价预测？</strong></li><li id="8d63" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">如何用代码实现？</strong></li></ol><h1 id="7a9e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">什么是线性回归？</h1><p id="de8e" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">它试图找出输入特征和目标变量(y)之间的最佳线性关系。</p><p id="9894" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！这就是线性回归的作用。很简单，对吧？😃</p><p id="3433" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在机器学习行话中，上述内容可以表述为“这是一种受监督的机器学习算法，它最适合将目标变量(因变量)作为输入特征(自变量)的线性组合的数据。”</p><p id="0c08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注</strong></p><ol class=""><li id="ed45" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">目标变量也称为独立变量或标签。</li><li id="5290" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">输入特征也称为因变量。</li></ol><h1 id="9e80" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">如何可视化线性回归？</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/41df3cee40b35eb0e7a831f1d1531a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3yXvm0FtOsgCvLh1AK9PQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(左)萨斯维克的图像(右)萨斯维克的图像</p></figure><p id="7656" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，当您考虑线性回归时，请考虑拟合一条直线，使数据点和直线之间的距离最小。如上所示，红线比其他蓝线更符合该数据。</p><p id="de60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 2D，输入要素和输出之间的线性关系就是一条直线。</p><p id="900c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是啊！线性回归试图找出输入和输出之间的最佳线性关系。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="45d1" class="nj mh iq nf b gy nk nl l nm nn">y = θx + b  # Linear Equation</span></pre><p id="e891" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">线性回归的目标是找到代表给定数据的θ和 b 的最佳值。</p><p id="4c7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在本文后面详细了解它。</p><p id="ab17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好吧！是时候深入研究线性回归了。</p><h1 id="d83c" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">线性回归算法如何预测？</h1><p id="72e4" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">现在，让我们认为我们已经获得了输入要素和标注之间的适当线性关系(稍后解释)。现在，我们关注线性回归模型如何利用获得的关系预测实例的值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/f867b4cd00081455499c8ceadfa438a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrcvWGnIefalp3rmDXWNJA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">线性回归(数据不是原始的，它是为了举例而创建的)</p></figure><p id="df5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据上图中的数据，线性回归将获得方程 y= 0.5*x + 1 的直线关系。(如果你不知道如何找到线性关系，不要担心，找到它的方法将在后面详细讨论。)</p><p id="cb6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">y =年收入</p><p id="63f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">x =经验</p><p id="3b4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr nf b">1</code>是截距或偏差项，<code class="fe np nq nr nf b">0.5</code>是<code class="fe np nq nr nf b">Experience</code>的特征权重</p><p id="6318" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，如果该模型给出了一个有 8 年工作经验的人的新的点数据点(橙色点)，那么它会预测此人每年会赚 50k 美元左右。</p><p id="5c1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型预测的一般形式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/39121b1b598010e069f8fc65deda77b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*o5pmuGfdO_PtaaMsbThc_Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">线性回归模型的预测</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f2e49e6f2358a4b53c71ca96aa183072.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*WIFC1mIjNK3ROqOjNC6OPQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/5e7721269512d0946a7187f94bff1649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kMQyto7CMPp_vGIgitvPZA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">这包含了特定点的所有特征值，除了相应的目标值。</p></figure><p id="bf97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">θ是模型的参数向量，包含偏差项θ0 和特征权重θ1 至θn。(n =特征数量)</p><p id="2e9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以将特征权重视为线性方程中特征的系数。</p><p id="1bf7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">x 是实例(数据集中的一个数据点)的特征向量，包含 x0 到 xn，其中 x0 始终等于 1</p><p id="5b0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">θ x 是矢量θ和 x 的点积，当然等于θ0x0 + θ1x1 + θ2x2 + … + θnxn。</p><p id="b008" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">θj 是第 j 个模型参数(包括偏差项θ0 和特征权重θ1，θ2，⋯，θn)。</p><p id="f351" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里θ，x 是列向量，你们马上会看到。</p><p id="e60a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型替换模型预测方程中给定的新实例 x^(i 特征值，并将获得的值作为预测返回。</p><p id="9973" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请记住，我们使用的是线性回归，它假设输入和输出(目标变量)之间只有线性关系。它不适用于数据中没有线性关系的问题。</p><p id="6973" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是为什么模型预测方程的输入特征幂为 1(线性),这也解释了为什么我们在模型预测方程中没有(x0)或(x2)这样的项。</p><p id="97fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注:</strong></p><p id="37b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果这些符号没有意义，请继续阅读后面的部分，这些符号用一个例子解释，可以帮助你理解它们。然后回来重新阅读这一部分，这可能会使事情变得更容易。</p><p id="4cdb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">偏置项</strong></p><p id="7726" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在线性回归中，我们将添加一个偏差项以获得无偏的结果。(如果我们<strong class="ky ir">不</strong>添加此项，则不会有截距，并且假设最佳拟合线穿过原点，同时截距为零，但并非每个数据集都是如此)。</p><p id="2510" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nv">我们已经知道了线性回归模型是如何预测的，但是我们如何评估这些预测并衡量它们的准确性呢？</em></p><p id="e5ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"><em class="nv"/></strong><a class="ae kv" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="nv">成本函数</em> </strong> </a> <em class="nv">将为我们完成这项工作。</em></p><h1 id="9a13" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">评估预测</h1><p id="b649" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated"><strong class="ky ir">成本函数</strong></p><p id="3a09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它评估模型的预测，并告诉我们模型的预测有多准确。成本函数值越低，模型的预测就越准确。有许多成本函数可供选择，但我们将使用均方误差(MSE)成本函数。</p><p id="bb52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">MSE 函数计算预测值和实际值(y)之差的平均值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/a368374e44e0ca36635a85d134547e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*wWDyT-FXIZ5rdKq9ZZ7X4A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">均方误差</p></figure><p id="cde4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们分解等式中的每个变量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/4a9cdebe027e98d22046cd3d4f7a5230.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*V5dY_-gOoLwsuSKRST1lfg.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/5e7721269512d0946a7187f94bff1649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kMQyto7CMPp_vGIgitvPZA.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d683b5930faa516a0ccbdace7b516897.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*tWTFZYAAPM5joMh1t1AIYg.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/e03633f1c2eac5221eabe1754510461e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q21K_vJjb8098hnwYM9oQA.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/41a66e0254cfc9df455fd5cce0c1fa8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*WQ4YXP87suqs69Y5qXjTmQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/84bfa5ea482f4073fc98118f2d807d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*CNRRw0Vwx1S1KOPeMOGfzQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">记住 x⁰(bias)的任何实例 j 是 1</p></figure><p id="a80f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">举例理解符号</strong></p><p id="1683" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你在理解变量代表什么方面有困难，让我们试着用一个例子来理解它们。</p><p id="0aa7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们假设您有一个数据集，其中预测了具有以下特征的房子的价格— <em class="nv">大小、房间、家庭(“偏差”列表示将添加到每个样本中的偏差项，它不是一个特征)</em>。它有五个样本。根据面积、房间数量、家庭成员等属性，我们可以预测房子的价格。</p><p id="392a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特征=大小、房间、家庭=因变量</p><p id="0e98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">目标(或)标签=价格=自变量</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/f6e18bd785b2ac3eebff767976c9dd01.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*vYcKi-20aMuftDl1g-lCVw.png"/></div></figure><p id="b734" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">m = 5</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/dc07ddef7d07df40db25a8c8f046f66b.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*OzZlgQTltHKDKuFjRqjz4w.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/56c0c3a2351d4c592b7b44b1995e1263.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*rlS5fILZB7M23OXHTAVhdg.png"/></div></figure><h1 id="443a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">它如何找到目标和输入特征之间的关系？</h1><p id="bc2b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">找到正确的关系意味着找到每个特征的精确特征权重(θj ),因为只有找到正确的特征权重才能给出可用于预测的正确线性方程。</p><p id="e29a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有两种方法可以在数据中找到正确的关系。</p><ol class=""><li id="d5a0" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">正态方程</li><li id="b667" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">梯度下降</li></ol><p id="aa68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">正规方程</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f523ccccf2a43818cb9112e37763d487.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*-G1TqKYrmQ3wG9PnfIY3Gw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">正态方程</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/669ded43e73e946f38a4ff7ac351c56d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zx8e4AIYNbw88q0qO-ua9w.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/af6705a413b5011c95a21bd79576be2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BW1qv6rCMjlLXghzO6uNVA.png"/></div></div></figure><p id="9b28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个直接方程，它将直接给出优化值，无需任何进一步的步骤。</p><p id="f9de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过计算上述等式，您将得到特征权重的优化值。</p><p id="304e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们看看如何在代码中实现这一点。</p><p id="6a4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们创建一些具有线性关系的数据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">准备数据</p></figure><p id="4b2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们有了数据，让我们看看如何借助法线方程找出特征权重(θ0，… θn)的值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">正规方程线性回归</p></figure><p id="8db8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您已经看到，它预测的特征权重非常接近实际值(y = 5 + 3*X +高斯噪声)，但是由于数据中的噪声，它无法预测精确值，但是预测值足够接近。</p><p id="61d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">正规方程的缺点</strong></p><ol class=""><li id="05c6" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">如果您有大量的要素，那么计算成本会很高。</li><li id="37c6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如果在你的数据中有任何冗余特征，它们在正规方程中的矩阵求逆是不可能的。在这种情况下，逆可以用伪逆代替。</li></ol><p id="3873" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">梯度下降</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8a65c1353f0fe3c6f2aa453280915764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sElqyePpT2eejARt"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@donnay_ca?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯文·克拉克</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="a199" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用梯度下降法，而不是进行复杂的矩阵运算(当我们有大量特征时，这可能会减慢处理速度),梯度下降法将在更短的时间内产生大量特征的良好结果。</p><p id="074f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">直觉</strong></p><p id="27b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设你在一座山顶上，你想尽快下山，但是天很阴，你看不到下山的路。你可能会想到的一个解决方案是考虑你周围所有可能的方向，并朝着最有可能下降的方向前进。</p><p id="b91d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样的想法可以应用于线性回归，但这里的问题是最小化成本函数。</p><p id="d0d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nv">如果你</em> <strong class="ky ir"> <em class="nv">不知道</em> </strong> <em class="nv">什么是梯度，记住计算一个函数的梯度给了我们最大化函数值的方向。</em></p><p id="ad6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">应用梯度下降找到优化的特征权重</strong></p><p id="1596" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，特征权重必须用随机值初始化。</p><p id="5be0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后对每个模型参数(特征权重)θj 计算函数的梯度，换句话说，它估算了如果我们把θj 改变一点点，代价函数会改变多少(这就是所谓的偏导数)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/01657d17d2fbf39cd23ee4b781d4ca0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*hxfB8G_EzFYI85CEuoU91Q.png"/></div></figure><p id="3e60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">计算每个特征权重θj 的偏导数的矢量化形式为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/ef9a758c1f0db92ee568ed812bc968ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*1P1EgKM7EF2Ka8ieKdJnkg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">矢量化形式</p></figure><p id="d37e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">计算一个函数的梯度给出了使该函数最大化的方向，但是我们想要最小化它，所以我们将向相反的方向移动。</p><p id="3f19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">优化特征权重θ的步骤是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/4b97a23b793916b73cf1d2b01cd13d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*JbXhXPK0Fja50hPTMZfd_w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度下降步骤</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/3a13ccb84683694a15b76d05dfc82b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*lAnsaX0KyzRis0xzkBfgLw.png"/></div></figure><p id="8529" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在最小化成本函数的方向上采取步骤之后，我们将获得成本函数值最小的特征权重的优化值。</p><p id="1345" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">学习率</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/796566ca224b5ad4ff62dc6b8a24ad49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uK0kWtUBQzUFpGLcSNvWvA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(左)学习率小时(右)学习率大时。</p></figure><p id="a533" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在最小化成本函数的方向上采取多个小步骤，而不是在正确的方向上大量改变特征权重的值，这可能超过实际的最小值。学习速度决定了步长的大小。</p><p id="3b83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们必须小心选择学习速度。如果学习率太低，需要很长时间才能收敛到正确的解，或者学习率太高，可能会过冲全局最小值。</p><p id="eeb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">局部最小值的问题</strong></p><p id="be93" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您正在使用梯度下降来最小化成本函数，但是如果您陷入局部最小值怎么办？</p><p id="6df7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">幸运的是，对于均方误差成本函数，我们没有这个问题，因为它是一个凸函数，即它只有一个全局最小值，没有局部最小值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/c77b388e0356fe9d161a5302a911c718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FvJwsvF4P3uaal508HdjOw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">均方误差函数(图像由 Sathwick 提供)</p></figure><p id="6106" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实施梯度下降</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度下降线性回归</p></figure><p id="f94b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有趣的是，我们从梯度下降法中得到了与正规方程相同的结果，同样只需要 50 次迭代。</p><p id="9cfc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好吧！那很好。但是我们如何知道在哪个迭代中我们将得到优化的特征权重呢？</p><p id="3c2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们接近成本函数的最小值时，梯度下降中的偏导数接近零，并且当乘以小于 1 的学习时，学习步骤变得更小，这确保了在所有迭代结束时，我们将非常接近实际的特征权重(假定您已经选择了不太大的适当学习速率)</p><p id="e8fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">梯度下降的缺点</strong></p><ol class=""><li id="6a96" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">如果使用另一个非凸形的成本函数，梯度下降有可能陷入局部最小值。</li><li id="1e86" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">你应该为学习率找到合适的值。</li></ol><p id="faab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">在 Scikit-Learn 中实现线性回归</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用 sklearn 进行线性回归</p></figure><p id="a73e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Scikit-Learn 提供了一个 LinearRegression 类来执行线性回归</p><p id="33ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如你所看到的，我们从正规方程，梯度下降，sklearn 得到的值几乎是一样的。</p><p id="679a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好吧，如果你已经读到这里，一切都有意义，拍拍自己的背吧！。你已经学习了线性回归的所有基本概念。</p><p id="c77e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">高维(多特征)数据怎么办？</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/cf66926dfedfbc85c94dad9309f9457a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*2nGpW00CpFmaMVZ5iOFGGg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">3D 线性回归(图片由 Sathwick 提供)</p></figure><p id="8bc4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，我们已经看到了二维数据的线性回归。我们知道，二维的线性回归是通过一条线实现的，类似地，在三维数据中，这条线被一个平面代替，对于高于三维的数据，我们使用一个超平面，其余步骤是相同的。</p><p id="cc99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简单地说，输入要素的数量增加，要素权重计算增加。</p><p id="c7a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我会让你自己在更高的维度上研究更多的数据，但基本概念保持不变。</p><p id="e16b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">干得好！👏你成功了。现在你可能对线性回归有了更好的了解。</p><h1 id="45f4" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="33a4" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">线性回归是一种广泛应用于回归问题的技术。它可以用来从自变量预测因变量。它在自变量的数据(训练数据)中搜索因变量的关系。在二维数据的情况下，它仅仅是一个线性方程。它使用这个线性方程(在训练期间从数据中找到的关系)来预测它以前没有见过的数据的值。这就是线性回归背后的全部思想。</p><p id="bbae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">确保你理解梯度下降部分。梯度下降是最广泛使用的优化技术之一。梯度下降技术在机器学习甚至深度学习中几乎无处不在。</p><p id="b8a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">感谢阅读！</strong></p><ul class=""><li id="e768" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr or ly lz ma bi translated">如果你喜欢这篇文章，<a class="ae kv" href="https://medium.com/@sathwickreddy" rel="noopener"> <em class="nv">在 medium </em> </a>上关注我。</li><li id="0f44" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr or ly lz ma bi translated">下面我们连线上<a class="ae kv" href="https://www.linkedin.com/in/sathwick-reddy-mora-a546451b6" rel="noopener ugc nofollow" target="_blank"> <em class="nv"> LinkedIn </em> </a> <em class="nv">。</em></li></ul></div></div>    
</body>
</html>