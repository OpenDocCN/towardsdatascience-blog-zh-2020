# NLP ä»‹ç»-ç¬¬ 2 éƒ¨åˆ†:å¼•ç†æ»¡è¶³å’Œè¯å¹²çš„åŒºåˆ«

> åŸæ–‡ï¼š<https://towardsdatascience.com/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc?source=collection_archive---------48----------------------->

ä½ æƒ³çŸ¥é“å¼•ç†æ»¡è¶³å’Œè¯å¹²çš„åŒºåˆ«å—ï¼Ÿå¦‚æœä½ å¾ˆæƒ³çŸ¥é“ç­”æ¡ˆï¼Œè¿™ç¯‡æ–‡ç« å°†è¯•å›¾è§£é‡Šå®ƒã€‚

![](img/bc3090cf2e362152ff88d54a66ee0e8c.png)

ç…§ç‰‡ç”±[åœ¨](https://unsplash.com/@retrosupply?utm_source=medium&utm_medium=referral) [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šåæ¨

# 1.å®šä¹‰ğŸ“—

å¼•ç†å’Œè¯å¹²æ˜¯è§„èŒƒåŒ–æ–‡æœ¬ä»¥è·å¾—å•è¯çš„è¯æ ¹å½¢å¼çš„ä¸åŒæŠ€æœ¯ã€‚Christopher D. Manningã€Prabhakar Raghavan å’Œ Hinrich SchÃ¼tze åœ¨ä»–ä»¬çš„è‘—ä½œä¸­å¯¹è¿™ä¸¤ä¸ªæ¦‚å¿µåšäº†å¦‚ä¸‹ç®€æ˜çš„å®šä¹‰:*ä¿¡æ¯æ£€ç´¢å¯¼è®º*ï¼Œ2008:

> ***ğŸ’¡*** [â€œè¯å¹²åŒ–é€šå¸¸æŒ‡ä¸€ç§ç²—ç³™çš„å¯å‘å¼è¿‡ç¨‹ï¼Œå³ç æ‰è¯å°¾ï¼Œå¸Œæœ›åœ¨å¤§å¤šæ•°æ—¶å€™éƒ½èƒ½æ­£ç¡®å®ç°è¿™ä¸€ç›®æ ‡ï¼Œé€šå¸¸è¿˜åŒ…æ‹¬å»é™¤æ´¾ç”Ÿè¯ç¼€â€¦â€¦è¯å¹²åˆ†æå™¨ä½¿ç”¨ç‰¹å®šäºè¯­è¨€çš„è§„åˆ™ï¼Œä½†æ˜¯å®ƒä»¬æ¯”è¯æ±‡åˆ†æå™¨éœ€è¦æ›´å°‘çš„çŸ¥è¯†â€¦â€](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)
> 
> **ğŸ’¡** [â€œè¯æ±‡åŒ–é€šå¸¸æ˜¯æŒ‡åˆ©ç”¨è¯æ±‡å’Œè¯çš„å½¢æ€åˆ†ææ¥æ°å½“åœ°åšäº‹æƒ…ï¼Œé€šå¸¸æ—¨åœ¨ä»…å»é™¤å±ˆæŠ˜è¯å°¾å¹¶è¿”å›è¯çš„åŸºæœ¬å½¢å¼æˆ–è¯å…¸å½¢å¼ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„è¯æ±‡â€¦â€¦ä¸€ä¸ªè¯æ³•åˆ†æå™¨ï¼Œå®ƒéœ€è¦å®Œæ•´çš„è¯æ±‡å’Œè¯æ³•åˆ†ææ¥æ­£ç¡®åœ°å¯¹å•è¯è¿›è¡Œè¯æ³•åˆ†æâ€¦â€](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)

å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡é˜…è¯»è¿™äº›å®šä¹‰ï¼Œä»–ä»¬å¯èƒ½ä¸ä¼šé©¬ä¸Šç‚¹å‡»ã€‚å› ä¸ºè¿™äº›å®šä¹‰ç›¸å½“ä¸°å¯Œå’Œå¯†é›†ï¼Œæ‰€ä»¥æ…¢æ…¢åœ°ã€ä»”ç»†åœ°ã€åå¤åœ°é˜…è¯»æ˜¯æœ‰å¸®åŠ©çš„ã€‚å¦‚æœä½ è¯»å®Œè¿™ç¯‡æ–‡ç« åå›æ¥ï¼Œå¸Œæœ›è¿™äº›å®šä¹‰ä¼šæ›´æœ‰æ„ä¹‰ã€‚

**ç®€åŒ–æ¦‚è¿°:**è¯å¹²åŒ–ä½¿ç”¨*é¢„å®šä¹‰è§„åˆ™*å°†å•è¯è½¬åŒ–ä¸º*è¯å¹²*ï¼Œè€Œè¯æ¡åŒ–ä½¿ç”¨*ä¸Šä¸‹æ–‡*å’Œ*è¯åº“*æ´¾ç”Ÿ*è¯æ¡*ã€‚*è¯å¹²*ä¸ä¸€å®šæ€»æ˜¯æœ‰æ•ˆå•è¯ï¼Œè€Œ*è¯æ¡*å°†æ€»æ˜¯æœ‰æ•ˆå•è¯ï¼Œå› ä¸º*è¯æ¡*æ˜¯å•è¯çš„å­—å…¸å½¢å¼ã€‚

æ­£å¦‚æˆ‘ä»¬å¾ˆå¿«å°†åœ¨ç¤ºä¾‹ä¸­çœ‹åˆ°çš„ï¼Œè¿™ä¸¤ç§æŠ€æœ¯æœ‰æ—¶ä¼šäº§ç”Ÿç›¸åŒçš„è¾“å‡ºã€‚

# 2.Python è®¾ç½®ğŸ”§

æœ¬èŠ‚å‡è®¾æ‚¨å·²ç»è®¿é—®å¹¶ç†Ÿæ‚‰ Pythonï¼ŒåŒ…æ‹¬å®‰è£…åŒ…ã€å®šä¹‰å‡½æ•°å’Œå…¶ä»–åŸºæœ¬ä»»åŠ¡ã€‚å¦‚æœä½ æ˜¯ Python çš„æ–°æ‰‹ï¼Œ[è¿™ä¸ª](https://www.python.org/about/gettingstarted/)æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¼€å§‹ã€‚

æˆ‘å·²ç»ä½¿ç”¨å¹¶æµ‹è¯•äº† Python 3.7.1 ä¸­çš„è„šæœ¬ã€‚åœ¨ä½¿ç”¨ä»£ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ç¡®ä¿æ‚¨æœ‰åˆé€‚çš„å·¥å…·ã€‚

## â¬œï¸ç¡®ä¿å®‰è£…äº†æ‰€éœ€çš„è½¯ä»¶åŒ…:ç†ŠçŒ«å’Œ nltk

æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å¼ºå¤§çš„ç¬¬ä¸‰æ–¹è½¯ä»¶åŒ…:

*   *ç†ŠçŒ«*:æ•°æ®åˆ†æåº“å’Œ
*   *nltk:* è‡ªç„¶è¯­è¨€å·¥å…·åŒ…åº“

## â¬œï¸ä» nltk ä¸‹è½½â€œwordnetâ€è¯­æ–™åº“

ä¸‹é¢çš„è„šæœ¬å¯ä»¥å¸®åŠ©ä½ ä¸‹è½½è¿™ä¸ªè¯­æ–™åº“ã€‚å¦‚æœæ‚¨å·²ç»ä¸‹è½½äº†å®ƒï¼Œè¿è¡Œå®ƒå°†é€šçŸ¥æ‚¨å®ƒæ˜¯æœ€æ–°çš„:

```
import nltk
nltk.download('wordnet')
```

# 3.è¾“å‡ºæ¯”è¾ƒğŸ”

åœ¨æˆ‘ä»¬æ·±å…¥ä¾‹å­ä¹‹å‰ï¼Œæ‚¨åº”è¯¥çŸ¥é“æœ‰ä¸åŒç±»å‹çš„è¯å¹²åˆ†æå™¨å’Œè¯å°¾åˆ†æå™¨å¯ç”¨ã€‚æ­£å¦‚æ‚¨æ‰€æ–™ï¼Œä¸€ç§ç±»å‹çš„è¯å¹²åˆ†æå™¨å¯èƒ½ä¸å…¶ä»–ç±»å‹çš„è¯å¹²åˆ†æå™¨æœ‰æ‰€ä¸åŒã€‚å¯¹äº lemmatisers æ¥è¯´ä¹Ÿæ˜¯å¦‚æ­¤ã€‚

åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª *nltk* çš„ *PorterStemmerã€LancasterStemmer* å’Œ *WordNetLemmatizer* æ¥è§„èŒƒåŒ–å•è¯ã€‚

> *ğŸ’¡****PorterStemmer*:******æœ€å¸¸ç”¨çš„è¯å¹²åˆ†æå™¨ä¹‹ä¸€ã€‚å®ƒåŸºäºæ³¢ç‰¹è¯å¹²ç®—æ³•ã€‚æ›´å¤šä¿¡æ¯ï¼ŒæŸ¥çœ‹å®˜æ–¹ç½‘é¡µ:ã€https://tartarus.org/martin/PorterStemmer/ã€‘[](https://tartarus.org/martin/PorterStemmer/)**
> 
> ***ğŸ’¡***Lancaster stemmer*:****å®ƒåŸºäº Lancaster è¯å¹²ç®—æ³•ï¼Œæœ‰æ—¶ä¼šäº§ç”Ÿæ¯”* PorterStemmer *æ›´æ¿€è¿›çš„è¯å¹²ã€‚***
> 
> ***ğŸ’¡***ã€WordNet lemma tiserã€‘*:***[*ä½¿ç”¨ WordNet è¯æ±‡æ•°æ®åº“ã€‚å¦‚æœåœ¨ WordNet ä¸­æ‰¾ä¸åˆ°è¾“å…¥å•è¯ï¼Œåˆ™è¿”å›ä¸å˜çš„è¾“å…¥å•è¯ã€‚*](https://wordnet.princeton.edu/)**

**è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨ä»¥ä¸‹ä¸‰ç§æ–¹æ³•å¯¹å•è¯è¿›è¡Œè§„èŒƒåŒ–:**

```
**# Import packages
import pandas as pd
from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer# Instantiate stemmers and lemmatiser
porter = PorterStemmer()
lancaster = LancasterStemmer()
lemmatiser = WordNetLemmatizer()# Create function that normalises text using all three techniques
def normalise_text(words, pos='v'):
    """Stem and lemmatise each word in a list. Return output in a dataframe."""
    normalised_text = pd.DataFrame(index=words, columns=['Porter', 'Lancaster', 'Lemmatiser'])
    for word in words:
        normalised_text.loc[word,'Porter'] = porter.stem(word)
        normalised_text.loc[word,'Lancaster'] = lancaster.stem(word)
        normalised_text.loc[word,'Lemmatiser'] = lemmatiser.lemmatize(word, pos=pos)
    return normalised_text**
```

**æˆ‘ä»¬å°†ä» 10 ä¸ªä»»æ„åè¯å¼€å§‹ï¼Œå¹¶æ¯”è¾ƒå®ƒä»¬çš„è§„èŒƒåŒ–å½¢å¼:**

```
**normalise_text(['apples', 'pears', 'tasks', 'children', 'earrings', 'dictionary', 'marriage', 'connections', 'universe', 'university'], pos='n')**
```

**![](img/62b999f908ef0bf48ff9fc618a5d3110.png)**

**ä½ å¯ä»¥çœ‹åˆ°*è¯å¹²*å¹¶ä¸æ€»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å•è¯ï¼Œè€Œ*è¯æ¡*å´æ˜¯ã€‚å¤§å­¦å’Œå®‡å®™ä¸€æ—¦è¢«æ¯ç­ï¼Œçœ‹èµ·æ¥æ˜¯ä¸€æ ·çš„ï¼Œä½†ä¸€æ—¦è¢«æ¯ç­ï¼Œæƒ…å†µå°±ä¸åŒäº†ã€‚ä½ æœ‰æ²¡æœ‰æ³¨æ„åˆ°ä¸Šé¢çš„è¯å¹²éƒ½ä¸æ˜¯ä»¥ e ç»“å°¾çš„ï¼Ÿå¯èƒ½æ˜¯å› ä¸ºè¯å¹²çš„ä¸€ä¸ªè§„åˆ™æ˜¯å»æ‰ç»“å°¾çš„ e å—ï¼Ÿè®©æˆ‘ä»¬ç”¨ä¸€äº›ä»¥â€œeâ€ç»“å°¾çš„å•è¯æ¥æ£€éªŒè¿™ä¸ªå‡è®¾:**

```
**normalise_text(['pie', 'globe', 'house', 'knee', 'angle', 'acetone', 'time', 'brownie', 'climate', 'independence'], pos='n')**
```

**![](img/354009df1c15d86b181d873c4230cc59.png)**

**ä¸å®Œå…¨æ˜¯ï¼Œæœ‰äº›è¯å¹²æ˜¯ä»¥ e ç»“å°¾çš„ã€‚è™½ç„¶è¿™ä¸ªå‡è®¾ä¸æˆç«‹ï¼Œä½†æˆ‘æƒ³æŒ‡å‡ºçš„æ˜¯ï¼Œè¯å¹²æå–æœ‰å¯è§‚å¯Ÿåˆ°çš„è¶‹åŠ¿ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŸºäºè§„åˆ™çš„ã€‚**

**å¦‚æœæˆ‘è®©ä½ å»ºè®®ä¸€äº›è§„åˆ™æ¥è§„èŒƒå„ç§å½¢å¼çš„åŠ¨è¯ï¼Œä½ ä¼šæ¨èä»€ä¹ˆï¼Ÿæ˜¯å»æ‰åç¼€â€œingâ€è¿˜æ˜¯â€œedâ€ï¼Ÿå¦‚æœæœ‰ä¸¤ä¸ªè¾…éŸ³ï¼Œå‰é¢æœ‰ä¸€ä¸ªå…ƒéŸ³ï¼Œé‚£ä¹ˆä½ ä¼šæŠŠæœ€åä¸€ä¸ªè¾…éŸ³ä¹Ÿå»æ‰å—ï¼Ÿè¯¸å¦‚æ­¤ç±»â€¦**

**è™½ç„¶æˆ‘ä¸ç†Ÿæ‚‰åº•å±‚ç®—æ³•çš„å¤æ‚ç»†èŠ‚ï¼Œä½†ä½¿ç”¨ä¸Šè¿°è§„åˆ™è¿›è¡Œè¯å¹²æå–ä¼¼ä¹æ˜¯å®é™…è¯å¹²æå–çš„è¿‡åº¦ç®€åŒ–ç‰ˆæœ¬ï¼Œä»è®¡ç®—å’Œè¯­è¨€çš„è§’åº¦æ¥çœ‹ï¼Œå®ƒå°†åˆ©ç”¨æ›´åŠ å¤æ‚å’Œæ·±æ€ç†Ÿè™‘çš„è§„åˆ™ã€‚**

**ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹ä¸€äº›åŠ¨è¯:**

```
**normalise_text(['wrote', 'thinking', 'remembered', 'relies', 'ate', 'gone', 'won', 'ran', 'swimming', 'mistreated'], pos='v')**
```

**![](img/f8127642be1ecc39f3d9985550b76eaf.png)**

**è™½ç„¶â€œæ€è€ƒâ€å’Œâ€œæ¸¸æ³³â€åœ¨æ‰€æœ‰ä¸‰ä¸ªè§„æ ¼åŒ–å™¨ä¸­ä»¥å®Œå…¨ç›¸åŒçš„æ–¹å¼è§„æ ¼åŒ–ï¼Œä½†å…¶ä»–ä¸€äº›åŠ¨è¯æœ‰ä¸åŒçš„è¾“å‡ºã€‚ä¾‹å¦‚ï¼Œä½ æœ‰æ²¡æœ‰æ³¨æ„åˆ° lemmatiser å¦‚ä½•å°†ä¸è§„åˆ™åŠ¨è¯å¦‚â€œateâ€å’Œâ€œgoneâ€è¿›è¡Œäº†åˆç†çš„è½¬æ¢ï¼Œè€Œ stemmers å´æ²¡æœ‰ã€‚æˆ‘è®¤ä¸ºè¿™æ˜¯å› ä¸ºä¸ºè¿™äº›å°‘æ•°ä¾‹å¤–æƒ…å†µå®šåˆ¶è§„åˆ™å¾ˆæ£˜æ‰‹ã€‚æˆ‘å¸Œæœ›åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿™äº›ç®€çŸ­çš„ä¾‹å­å·²ç»è®©æ‚¨äº†è§£äº†è¯å¹²åˆ†æå™¨å’Œè¯æ±‡åˆ†æå™¨æ˜¯å¦‚ä½•è§„èŒƒåŒ–å•è¯çš„ã€‚**

# **4.é€Ÿåº¦æ¯”è¾ƒğŸ**

**åœ¨ç ”ç©¶è¯å¹²åŒ¹é…å’Œè¯å¹²åŒ¹é…æ—¶ï¼Œæˆ‘é‡åˆ°äº†è®¸å¤šèµ„æºï¼Œè¿™äº›èµ„æºå£°ç§°è¯å¹²åŒ¹é…æ¯”è¯å¹²åŒ¹é…æ›´å¿«ã€‚ç„¶è€Œï¼Œå½“æˆ‘åœ¨æˆ‘çš„è®¡ç®—æœºä¸Šå¯¹ä¸€ä¸ªæ ·æœ¬æ•°æ®æµ‹è¯•ä¸‰ä¸ªè§„æ ¼åŒ–å™¨æ—¶ï¼Œæˆ‘è§‚å¯Ÿåˆ°å®Œå…¨ç›¸åçš„æƒ…å†µ:**

```
**from nltk.corpus import movie_reviews
from nltk.tokenize import RegexpTokenizer# Import data
reviews = []
for fileid in movie_reviews.fileids():
    tag, filename = fileid.split('/')
    reviews.append((tag, movie_reviews.raw(fileid)))
sample = pd.DataFrame(reviews, columns=['target', 'document'])# Prepare one giant string 
sample_string = " ".join(sample['document'].values)# Tokenise data
tokeniser = RegexpTokenizer(r'\w+')
tokens = tokeniser.tokenize(sample_string)%%timeit 
lemmatiser = WordNetLemmatizer()
[lemmatiser.lemmatize(token, 'v') for token in tokens]**
```

**![](img/fa36208aba9440f93c2ef4c06ca381ad.png)**

```
**%%timeit 
porter = PorterStemmer()
[porter.stem(token) for token in tokens]**
```

**![](img/4271e4aa8780f6ca69ed88f6f34d2a88.png)**

```
**%%timeit 
lancaster = LancasterStemmer()
[lancaster.stem(token) for token in tokens]**
```

**![](img/7e2afb81f3e7e55df65208fe2ffbc971.png)**

**æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œä»è¿™ä¸ªå¿«é€Ÿè¯„ä¼°ä¸­ï¼Œlemmatiser å®é™…ä¸Šæ›´å¿«ï¼Œç”šè‡³å½“æˆ‘ä»¬ç”¨å¹³å‡+/- 3 ä¸ªæ ‡å‡†å·®æ¥æ¯”è¾ƒä¸€ä¸ªèŒƒå›´æ—¶ã€‚å› æ­¤ï¼ŒLemmatiser çœ‹èµ·æ¥æ›´æœ‰åˆ©ï¼Œå› ä¸ºå®ƒè§„èŒƒåŒ–å¾—æ›´åˆç†ï¼Œè¿è¡Œé€Ÿåº¦æ›´å¿«ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘å°†åˆ†äº«ä¸¤ä¸ªæœ‰æ•ˆçš„å¼•ç†æ»¡è¶³æŠ€å·§ä½œä¸ºå¥–åŠ±ã€‚**

***è¯·æ³¨æ„ï¼Œæœ¬å¸–ä¸­æ²¡æœ‰æåˆ°çš„å…¶ä»–è¯å¹²åˆ†æå™¨å’Œè¯å¹²åˆ†æå™¨å¯èƒ½ä¼šç»™æˆ‘ä»¬ä¸€ä¸ªä¸åŒçš„æ•…äº‹ã€‚***

# **5.æœ‰æ•ˆå¼•ç†æ»¡è¶³çš„ä¸¤ä¸ªæŠ€å·§ğŸ’¡**

## **5.1.è¯æ€§æ ‡ç­¾ğŸ’¬**

**å¦‚æœæ‚¨æŸ¥çœ‹è§„èŒƒåŒ–ç¤ºä¾‹åè¯å’ŒåŠ¨è¯çš„ä»£ç ç‰‡æ®µï¼Œæ‚¨ä¼šæ³¨æ„åˆ°ä¸¤è€…ä¹‹é—´çš„`pos`å‚æ•°æœ‰æ‰€ä¸åŒã€‚è¿™ä¸ªå‚æ•°æŒ‡çš„æ˜¯ä¸€ä¸ªå•è¯çš„è¯æ€§æ ‡ç­¾ï¼Œå®ƒåœ¨å•è¯å¦‚ä½•è¢«è¯æ±‡åŒ–çš„è¿‡ç¨‹ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚è¯æ€§æ ‡ç­¾å‘ lemmatiser æä¾›å•è¯çš„*ä¸Šä¸‹æ–‡*ã€‚è®©æˆ‘ä»¬çœ‹ä¸€äº›ä¾‹å­:**

```
**lemmatiser = WordNetLemmatizer()
print(f"Lemmatising 'remembered' with pos='v' results in: {lemmatiser.lemmatize('remembered', 'v')}")
print(f"Lemmatising 'remembered' with pos='n' results in: {lemmatiser.lemmatize('remembered', 'n')}\n")
print(f"Lemmatising 'universities' with pos='v' results in: {lemmatiser.lemmatize('universities', 'v')}")
print(f"Lemmatising 'universities' with pos='n' results in: {lemmatiser.lemmatize('universities', 'n')}")**
```

**![](img/49ea5994516e5c90e376b0c3a1928173.png)**

**å¦‚æ‚¨æ‰€è§ï¼Œä¸ºäº†æœ‰æ•ˆåœ°è§„èŒƒåŒ–å¸¦æœ‰`WordNetLemmatizer`çš„å•è¯ï¼Œä¸ºæ¯ä¸ªå•è¯æä¾›æ­£ç¡®çš„`pos`å‚æ•°æ˜¯å¾ˆé‡è¦çš„ã€‚**

## **5.2.æƒ…å†µğŸ”  ğŸ”¡**

**æ­¤å¤–ï¼Œå•è¯çš„å¤§å°å†™ä¹Ÿå¾ˆé‡è¦:**

```
**print(f"Lemmatising 'Remembered' with pos='v' results in: {lemmatiser.lemmatize('Remembered', 'v')}")
print(f"Lemmatising 'Remembered' with pos='n' results in: {lemmatiser.lemmatize('Remembered', 'n')}\n")
print(f"Lemmatising 'Universities' with pos='v' results in: {lemmatiser.lemmatize('Universities', 'v')}")
print(f"Lemmatising 'Universities' with pos='n' results in: {lemmatiser.lemmatize('Universities', 'n')}")**
```

**![](img/407125ded9309deca9489558dd724241.png)**

**å¤§å†™æ—¶ï¼Œå•è¯å³ä½¿æœ‰æ­£ç¡®çš„`pos`ä¹Ÿä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒä»¬è¢«è§†ä¸ºä¸“æœ‰åè¯ã€‚ğŸ’­**

**![](img/6b51fb668a92283ba29f2216ff65d461.png)**

**ç…§ç‰‡ç”±[å¸•ç‰¹é‡Œå…‹Â·ç¦å°”](https://unsplash.com/@patrickian4?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) æ‹æ‘„**

***æ‚¨æƒ³è®¿é—®æ›´å¤šè¿™æ ·çš„å†…å®¹å—ï¼Ÿåª’ä½“ä¼šå‘˜å¯ä»¥æ— é™åˆ¶åœ°è®¿é—®åª’ä½“ä¸Šçš„ä»»ä½•æ–‡ç« ã€‚å¦‚æœä½ ä½¿ç”¨* [*æˆ‘çš„æ¨èé“¾æ¥*](https://zluvsand.medium.com/membership)*æˆä¸ºä¼šå‘˜ï¼Œä½ çš„ä¸€éƒ¨åˆ†ä¼šè´¹ä¼šç›´æ¥å»æ”¯æŒæˆ‘ã€‚***

**æ„Ÿè°¢æ‚¨èŠ±æ—¶é—´é˜…è¯»è¿™ç¯‡æ–‡ç« ã€‚æˆ‘å¸Œæœ›ä½ å­¦åˆ°äº†ä¸€äº›å…³äºå¼•ç†æ»¡è¶³å’Œè¯å¹²çš„çŸ¥è¯†ã€‚è¯»å®Œè¿™ç¯‡æ–‡ç« åï¼Œå¦‚æœä½ å†ç¿»ä¸€éå®šä¹‰ï¼Œä½ ä¼šè§‰å¾—æ¯”ç¬¬ä¸€æ¬¡è¯»çš„æ—¶å€™æ›´æœ‰æ„ä¹‰å—ï¼ŸğŸ‘€å…¶ä½™å¸–å­çš„é“¾æ¥æ•´ç†å¦‚ä¸‹:
â—¼ï¸ [ç¬¬ä¸€éƒ¨åˆ†:Python ä¸­çš„æ–‡æœ¬é¢„å¤„ç†](https://medium.com/@zluvsand/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96)
â—¼ï¸ **ç¬¬äºŒéƒ¨åˆ†:å¼•ç†å’Œè¯å¹²åŒ–çš„åŒºåˆ«**
â—¼ï¸ [ç¬¬ä¸‰éƒ¨åˆ†:TF-IDF è§£é‡Š](https://medium.com/@zluvsand/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc)
â—¼ï¸ [ç¬¬å››éƒ¨åˆ†:Python ä¸­çš„ç›‘ç£æ–‡æœ¬åˆ†ç±»æ¨¡å‹](https://medium.com/@zluvsand/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267)
â—¼ï¸ [ç¬¬äº”éƒ¨åˆ†:Python ä¸­çš„æ— ç›‘ç£ä¸»é¢˜æ¨¡å‹(sklearn)](/introduction-to-nlp-part-5a-unsupervised-topic-model-in-python-733f76b3dc2d) ã€T29**

**æ­£å¸¸åŒ–å¿«ä¹ï¼å†è§ğŸƒğŸ’¨**

# **4.å‚è€ƒğŸ“**

*   **[Christopher D. Manningï¼ŒPrabhakar Raghavan å’Œ Hinrich SchÃ¼tzeï¼Œ*ä¿¡æ¯æ£€ç´¢å¯¼è®º*ï¼Œå‰‘æ¡¥å¤§å­¦å‡ºç‰ˆç¤¾ï¼Œ2008 å¹´](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)**
*   **[ä¼¯å¾·ã€å²è’‚æ–‡ã€çˆ±å¾·åÂ·æ´›ç€å’Œä¼Šä¸‡Â·å…‹è±æ©ï¼Œ*ç”¨ Python è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†*ã€‚å¥¥è±åˆ©åª’ä½“å…¬å¸ï¼Œ2009 å¹´](http://www.nltk.org/book/)**
*   **æ™®æ—æ–¯é¡¿å¤§å­¦ WordNetã€‚2010 å¹´**
*   **[*èŒåŒ…*ï¼Œnltk æ–‡æ¡£](https://www.nltk.org/api/nltk.stem.html)**