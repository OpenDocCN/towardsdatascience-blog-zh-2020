<html>
<head>
<title>Video Facial Expression and Awareness Detection with Fast.ai and OpenCV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Fast.ai和OpenCV进行视频面部表情和意识检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/video-facial-expression-detection-with-deep-learning-applying-fast-ai-d9dcfd5bcf10?source=collection_archive---------8-----------------------#2020-04-28">https://towardsdatascience.com/video-facial-expression-detection-with-deep-learning-applying-fast-ai-d9dcfd5bcf10?source=collection_archive---------8-----------------------#2020-04-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="17d5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从实时视频或视频文件中检测面部表情和意识</h2></div><p id="7ccd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">这背后的灵感？联邦调查局特工通过我的网络摄像头监视我，但被深度学习取代:)</em></p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="lk ll l"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">实时视频分类器演示</p></figure><h1 id="8fa4" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">介绍</h1><p id="e36d" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">本教程的目标？使用fast.ai库训练面部表情分类模型，从网络摄像头或视频文件中读取面部表情，最后，添加面部地标来跟踪你的眼睛以确定意识！(<strong class="kk iu">TL；博士</strong>完整的工作代码在这里【https://github.com/jy6zheng/FacialExpressionRecognition T4】</p><p id="ac57" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我写这篇教程的主要原因是，在做这个项目时，一个很大的挑战是弄清楚如何使用我训练过的分类器，并使它有效地处理实时视频和视频文件。额外的眼部标记功能是基于我发现非常有用的本教程:<a class="ae mn" href="https://www.pyimagesearch.com/2017/05/08/drowsiness-detection-opencv/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2017/05/08/sledy-detection-opencv/</a></p><h1 id="edd3" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">培养</h1><p id="d051" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">第一步是用卷积神经网络训练图像分类模型。我使用的数据来自<a class="ae mn" href="https://www.kaggle.com/jonathanoheix/face-expression-recognition-dataset" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/jonathanoheix/face-expression-recognition-dataset</a></p><p id="3efb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我使用了构建在PyTorch之上的fast.ai库来训练我的分类模型。该模型使用resnet34预训练权重和训练数据集进行训练，并导出为. pkl文件。<strong class="kk iu">关于一步一步的指导，请查看我的知识库中的google colab笔记本，它包含了训练你的模型的所有代码:</strong><a class="ae mn" href="https://github.com/jy6zheng/FacialExpressionRecognition/blob/master/Facial_recognition.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/jy6zheng/FacialExpressionRecognition</a></p><p id="21b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最大的挑战是首先找到一个公共数据集，然后清理数据。最初，当我使用Kaggle数据集时，我只能训练到0.328191的错误率，这意味着它只有大约68%的时间是正确的(根本不是很好)。当我绘制产生最高损失的图像时，我很快意识到大量数据被错误地标记(左边是模型预测的表达，右边是标记的情绪)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/3d08b4361314b8cf3239468c4d155be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cmjoZdys1iA_9cvKCP_Orw.png"/></div></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">坐在最后一排左三的女孩看起来显然不开心</p></figure><p id="d6cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据清洗后，错误率下降了16%以上。现在分类器有大约84%的准确率，这意味着它正确地识别了84%的面部图像。仍有一些不正确和不完整的数据，因此还有进一步改进的空间。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/0696695a55372e743b456f561f2a8901.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*YkPyETdqC6jH3gydp7lAUQ.png"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/14d31c07b6abf157136ccab55606d0b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*9u7tD3mIhdxjxSE4hfGW5Q.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">正如你所看到的，中性和悲伤的脸最容易混淆</p></figure><h1 id="3d53" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">在实况视频上使用训练模型</h1><p id="62a2" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">现在，是时候将我们的分类器用于现场视频流了。首先，最好创建一个虚拟环境，这样这个项目就有自己的依赖项，并且不会干扰任何其他项目。然后，下载所需的包和库。创建一个名为liveVideoFrame.py的文件(或您想命名的任何名称),并导入以下内容:</p><pre class="lf lg lh li gt mx my mz na aw nb bi"><span id="b877" class="nc lr it my b gy nd ne l nf ng">from scipy.spatial <br/>import distance as dist<br/>import numpy as np<br/>import cv2<br/>from imutils import face_utils<br/>from imutils.video import VideoStream<br/>from fastai.vision import *<br/>import imutils<br/>import argparse<br/>import time<br/>import dlib</span></pre><p id="4a6d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我想选择将预测保存在一个. csv文件中，并保存带标记的视频，所以我添加了参数解析。我还导出了经过训练的分类模型，并将其移动到我的工作目录中。</p><pre class="lf lg lh li gt mx my mz na aw nb bi"><span id="6937" class="nc lr it my b gy nd ne l nf ng">ap = argparse.ArgumentParser()<br/>ap.add_argument("--save", dest="save", action = "store_true")<br/>ap.add_argument("--no-save", dest="save", action = "store_false")<br/>ap.set_defaults(save = False)<br/>ap.add_argument("--savedata", dest="savedata", action = "store_true")<br/>ap.add_argument("--no-savedata", dest="savedata", action = "store_false")<br/>ap.set_defaults(savedata = False)<br/>args = vars(ap.parse_args())</span><span id="152b" class="nc lr it my b gy nh ne l nf ng">path = '/Users/joycezheng/FacialRecognitionVideo/' #change this depending on the path of your exported model<br/>learn = load_learner(path, 'export.pkl')</span></pre><p id="f0a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太好了！现在是时候开始我们的视频流了。我使用imutils.video的VideoStream，因为我发现它比cv2.VideoCapture工作得更快。<strong class="kk iu">注意:</strong>对于内置的网络摄像头，视频流的来源是0，如果您使用不同的相机，如插件，它会有所不同。</p><p id="8ac8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">haar级联分类器用于识别视频帧中的正面人脸。我们有一个名为data的数组来存储我们的预测。timer和time_value用于标记数据中每个预测的时间，以便在. csv文件中预测以1递增。</p><pre class="lf lg lh li gt mx my mz na aw nb bi"><span id="4dce" class="nc lr it my b gy nd ne l nf ng">face_cascade = cv2.CascadeClassifier("haarcascade_frontalface_default.xml") <br/>vs = VideoStream(src=0).start()<br/>start = time.perf_counter() <br/>data = []<br/>time_value = 0<br/>if args["save"]:    <br/>    out = cv2.VideoWriter(path + "liveoutput.avi", cv2.VideoWriter_fourcc('M','J','P','G'), 10, (450,253))</span></pre><p id="d605" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们将实现一个while循环，从视频流中读取每一帧:</p><ol class=""><li id="6702" class="ni nj it kk b kl km ko kp kr nk kv nl kz nm ld nn no np nq bi translated">因为图像分类器是在灰度图像上训练的，所以每一帧都被转换成灰度图像</li><li id="b109" class="ni nj it kk b kl nr ko ns kr nt kv nu kz nv ld nn no np nq bi translated">级联分类器用于在帧中寻找人脸。我将minneighbors参数设置为5，因为我发现它在实时视频上效果最好。对于录制的视频文件，我将其设置为较高的值，因为每一帧中肯定会有一张脸</li><li id="bb63" class="ni nj it kk b kl nr ko ns kr nt kv nu kz nv ld nn no np nq bi translated">由于我们的分类器是在没有太多背景的特写人脸上训练的，因此灰度图像随后被裁剪为具有0.3缓冲的人脸</li><li id="faca" class="ni nj it kk b kl nr ko ns kr nt kv nu kz nv ld nn no np nq bi translated">然后，在每一帧上绘制并显示文本和边界框</li><li id="2dfe" class="ni nj it kk b kl nr ko ns kr nt kv nu kz nv ld nn no np nq bi translated">然后使用out.write(frame)将每一帧保存到视频编写器中</li></ol><pre class="lf lg lh li gt mx my mz na aw nb bi"><span id="855e" class="nc lr it my b gy nd ne l nf ng">while True:    <br/>    frame = vs.read()    <br/>    frame = imutils.resize(frame, width=450)     <br/>    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)   <br/>    face_coord = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(30, 30))<br/>    for coords in face_coord:        <br/>        X, Y, w, h = coords        <br/>        H, W, _ = frame.shape        <br/>        X_1, X_2 = (max(0, X - int(w * 0.3)), min(X + int(1.3 * w), W))        <br/>        Y_1, Y_2 = (max(0, Y - int(0.3 * h)), min(Y + int(1.3 * h), H))        <br/>        img_cp = gray[Y_1:Y_2, X_1:X_2].copy()        <br/>        prediction, idx, probability = learn.predict(Image(pil2tensor(img_cp, np.float32).div_(225)))<br/>        cv2.rectangle(                <br/>            img=frame,                <br/>            pt1=(X_1, Y_1),                <br/>            pt2=(X_2, Y_2),                <br/>            color=(128, 128, 0),                <br/>            thickness=2,            <br/>        )        <br/>       cv2.putText(frame, str(prediction), (10, frame.shape[0] - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (225, 255, 255), 2)</span><span id="1298" class="nc lr it my b gy nh ne l nf ng">    cv2.imshow("frame", frame)</span><span id="8934" class="nc lr it my b gy nh ne l nf ng">    if args["save"]:<br/>        out.write(frame)<br/>    if cv2.waitKey(1) &amp; 0xFF == ord("q"):<br/>        break</span><span id="fb4c" class="nc lr it my b gy nh ne l nf ng">vs.stop()<br/>if args["save"]:<br/>    print("done saving video")    <br/>    out.release()<br/>cv2.destroyAllWindows()</span></pre><p id="4e61" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们有了fast.ai学习模型，它可以与imutils和OpenCV合作，从直播视频中预测人脸！</p><p id="0659" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，是时候确定面部的意识了。函数<strong class="kk iu"> eye_aspect_ratio </strong>根据眼睛的坐标计算眼睛的纵横比。从dlib预训练的面部标志检测器中找到每只眼睛的位置和坐标。函数<strong class="kk iu"> data_time </strong>用于每隔1秒在数据数组中追加预测。</p><pre class="lf lg lh li gt mx my mz na aw nb bi"><span id="1307" class="nc lr it my b gy nd ne l nf ng">EYE_AR_THRESH = 0.20<br/>EYE_AR_CONSEC_FRAMES = 10</span><span id="35df" class="nc lr it my b gy nh ne l nf ng">COUNTER = 0</span><span id="599e" class="nc lr it my b gy nh ne l nf ng">def eye_aspect_ratio(eye):<br/>    A = dist.euclidean(eye[1], eye[5])<br/>    B = dist.euclidean(eye[2], eye[4])<br/>    C = dist.euclidean(eye[0], eye[3])<br/>    ear = (A + B) / (2.0 * C)<br/>    return ear</span><span id="b7dd" class="nc lr it my b gy nh ne l nf ng">def data_time(time_value, prediction, probability, ear):<br/>    current_time = int(time.perf_counter()-start)<br/>    if current_time != time_value:<br/>        data.append([current_time, prediction, probability, ear])<br/>        time_value = current_time<br/>    return time_value</span><span id="137e" class="nc lr it my b gy nh ne l nf ng">predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")</span><span id="bf77" class="nc lr it my b gy nh ne l nf ng">(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]<br/>(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]</span></pre><p id="fd3f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在遍历面坐标的for循环中，添加以下代码块。使用dlib面部标志检测器检测眼睛，并将其绘制到帧上。当两只眼睛之间的平均计算眼睛纵横比小于超过十个连续帧的阈值(您可以根据自己的喜好修改该阈值)时，则该面部被标记为分心。</p><pre class="lf lg lh li gt mx my mz na aw nb bi"><span id="fb2f" class="nc lr it my b gy nd ne l nf ng">        rect = dlib.rectangle(X, Y, X+w, Y+h)<br/>        shape = predictor(gray, rect)<br/>        shape = face_utils.shape_to_np(shape)<br/>        leftEye = shape[lStart:lEnd]<br/>        rightEye = shape[rStart:rEnd]<br/>        leftEAR = eye_aspect_ratio(leftEye)<br/>        rightEAR = eye_aspect_ratio(rightEye)<br/>        ear = (leftEAR + rightEAR) / 2.0<br/>        leftEyeHull = cv2.convexHull(leftEye)<br/>        rightEyeHull = cv2.convexHull(rightEye)<br/>        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)<br/>        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)<br/>        if ear &lt; EYE_AR_THRESH:<br/>            COUNTER += 1<br/>            if COUNTER &gt;= EYE_AR_CONSEC_FRAMES:<br/>                cv2.putText(frame, "Distracted", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)<br/>        else:<br/>            COUNTER = 0<br/>        cv2.putText(frame, "Eye Ratio: {:.2f}".format(ear), (250, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)<br/>        time_value = data_time(time_value, prediction, probability, ear)</span></pre><p id="5580" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，在代码的底部，我们可以将数据保存为数据帧，然后保存为. csv文件。</p><pre class="lf lg lh li gt mx my mz na aw nb bi"><span id="50a9" class="nc lr it my b gy nd ne l nf ng">if args["savedata"]:<br/>    df = pd.DataFrame(data, columns = ['Time (seconds)', 'Expression', 'Probability', 'EAR'])<br/>    df.to_csv(path+'/exportlive.csv')<br/>    print("data saved to exportlive.csv")</span></pre><p id="60d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以通过运行以下命令在命令行中测试代码:</p><pre class="lf lg lh li gt mx my mz na aw nb bi"><span id="21ed" class="nc lr it my b gy nd ne l nf ng">python liveVideoFrameRead.py --save --savedata</span></pre><p id="40dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">完整的代码在这里:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nw ll l"/></div></figure><h1 id="75fe" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">对视频文件使用训练模型</h1><p id="460b" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">我对视频文件使用了与实况视频非常相似的方法。主要区别在于预测每隔一定数量的帧发生一次，这可以使用命令行参数— frame-step来修改。完整代码如下:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nw ll l"/></div></figure><p id="d037" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就是这样！你现在可以从视频文件和网络摄像头中预测面部表情。</p><p id="c51a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谢谢你看了这个:)，如果有什么改进或者问题请告诉我。完整的工作代码在这里:<a class="ae mn" href="https://github.com/jy6zheng/FacialExpressionRecognition" rel="noopener ugc nofollow" target="_blank">https://github.com/jy6zheng/FacialExpressionRecognition</a></p></div></div>    
</body>
</html>