<html>
<head>
<title>Feature Extraction from a Non-Conventional Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">非常规神经网络的特征提取</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spinenet-feature-extraction-object-detection-cdf9e57dbe0d?source=collection_archive---------42-----------------------#2020-07-17">https://towardsdatascience.com/spinenet-feature-extraction-object-detection-cdf9e57dbe0d?source=collection_archive---------42-----------------------#2020-07-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/f2be78090349b0051fb18fa1b8e584fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1fDVMWj-QH04s4QxaGwOsg.jpeg"/></div></div></figure><h2 id="94ba" class="iz ja jb bd b dl jc jd je jf jg jh dk ji translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="f811" class="pw-subtitle-paragraph kh jk jb bd b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky dk translated">尺度置换网络代替尺度递减网络</h2></div><p id="2289" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">当你谈到任何一种图像分类或一种特征提取方法时，你脑海中浮现的是哪种神经网络？大概是这样的:给一个图像作为大小为 512x512 的输入，得到一个 10x1 的分类器输出。在这两者之间，有卷积和最大池层，它们会慢慢地降低图像分辨率，同时保留重要的特征。</p><p id="e7b0" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">现在，在<strong class="lb jl"> CVPR-2020 期间，</strong>谷歌研究和大脑团队提出了一个非常规的网络——<a class="ae lv" href="https://arxiv.org/abs/1912.05027" rel="noopener ugc nofollow" target="_blank">spine net</a>，而不是上面解释的典型神经网络。该网络由尺度置换的中间特征和交叉连接组成，这可能是许多需要特征提取的应用(如对象检测或图像分类)的可能解决方案。</p><h2 id="a93e" class="lw lx jb bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn jh bi translated">结果</h2><p id="f54e" class="pw-post-body-paragraph kz la jb lb b lc mo kl le lf mp ko lh li mq lk ll lm mr lo lp lq ms ls lt lu ij bi translated">让我展示一下，谷歌不仅从一开始就重新定义了我们构建神经网络的方式，而且还改善了结果。</p><figure class="mu mv mw mx gt is gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/1aa3c5ecfac7a2651ffca1baf6fa978c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*yLeALaSeuPscp-UQ0d5GOg.png"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">图一。Pic 鸣谢:SpineNet 研究论文</p></figure><p id="8183" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">图一。显示了采用 SpineNet、ResNet-FPN 和 NAS-FPN 作为主干的 RetinaNet 模型(在<a class="ae lv" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO 数据集</a>上)的比较。红线代表论文中解释的当前 SpineNet 模型(参数不同)，蓝色虚线代表与 SpineNet 模型对应的参数不同的 ResNet 网络。</p><p id="20e2" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">乍看之下，它显示红色线比蓝色虚线具有更好的 AP(%)和更少的翻牌。头版的这个结果让我第一时间看到了方法。</p><p id="de48" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">他们还以表格形式显示了不同数据集的结果，并将它们与 ResNet 模型进行了比较。你可以仔细看看这些，我在这个博客的末尾附上了论文链接。</p><h2 id="1ae6" class="lw lx jb bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn jh bi translated">想法和动机</h2><p id="045b" class="pw-post-body-paragraph kz la jb lb b lc mo kl le lf mp ko lh li mq lk ll lm mr lo lp lq ms ls lt lu ij bi translated">一般来说，目标检测的一个基本架构如图<em class="nc">图 2 </em>所示。</p><figure class="mu mv mw mx gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/29406bdaa45f08947b7a3168be4c776c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*ARavdtWoNowQas2cW-Ma5g.png"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">图二。基本编解码网络(自制)</p></figure><p id="9a18" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">其中，编码器用于提取特征(也可用于图像分类),解码器用于恢复这些特征以提供边界框的定位。编码器和解码器主要通过残差连接(ResNet)来使用不同分辨率的特征来保留重要信息。</p><p id="9128" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">这种编码器是一种比例递减网络，类似于在<a class="ae lv" href="https://arxiv.org/abs/1804.02767" rel="noopener ugc nofollow" target="_blank"> YOLOv3 </a>、<a class="ae lv" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank"> Faster-RCNN </a>等中使用的网络。，也就是通常所说的模特的<em class="nc">脊梁</em>。<a class="ae lv" href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf" rel="noopener ugc nofollow" target="_blank"> LeCun 等人</a>解释了这种规模缩小的架构设计背后的动机:“<em class="nc">可能需要高分辨率来检测特征的存在，而不需要以同样高的精度来确定其确切位置。</em>”</p><p id="6405" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">为了在模型变得更深时保留空间信息并促进多尺度特征融合，本文提出了这种尺度置换网络。人们可以用图 3 中的<em class="nc">来捕捉这个想法。</em></p><figure class="mu mv mw mx gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/46b92298b15fe4964c1544af82cec739.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*JV6uVtU-I-uCZ-4eXvKuJA.png"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">图三。来源:<a class="ae lv" href="https://arxiv.org/abs/1912.05027" rel="noopener ugc nofollow" target="_blank"> SpineNet 研究论文截图</a></p></figure><p id="ce65" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">从<em class="nc">图 3 </em>开始。，可以观察到传统的神经网络(比例缩小网络)在左侧。右边是一个规模置换网络的例子，各层和连接之间没有特定的顺序。</p><p id="b6cf" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">这种方法可能会产生许多理论和问题，本文试图回答其中的大部分。然而，为了使这篇文章简短扼要，我将把重点放在阅读摘要后想到的前两个问题上。</p><ol class=""><li id="d7ad" class="nf ng jb lb b lc ld lf lg li nh lm ni lq nj lu nk nl nm nn bi translated"><strong class="lb jl">我们如何提出这样的架构或连接？</strong>不幸的是，还没有定义好的方法或规则来做到这一点。在论文中，他们使用了<strong class="lb jl">神经架构搜索(NAS) </strong>作为一种方法来寻找类似这样不同的可能架构。在过去，许多不同的公司和实验已经展示了 NAS 的使用，但是主要衍生了规模减小的网络。在没有 NAS 的情况下构建这种网络的可能性仍然不清楚，但人们可能会使用他们的最终网络输出(这将在后面显示)。</li><li id="7f7c" class="nf ng jb lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated"><strong class="lb jl">既然没有顺序，输入层的输出分辨率和特征尺寸如何匹配目标层的输入分辨率和特征尺寸？<br/> </strong>它们执行特征和空间重采样以匹配模型中目标层的特征和分辨率。从图 4 的<em class="nc">可以观察到。对于较小的图像，它们使用最近邻插值进行上采样，而它们使用步长(值为 2)、3×3 卷积进行下采样，以匹配模型的目标层。</em></li></ol><figure class="mu mv mw mx gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/61508e3a53abdbd1abd96b4c89d96be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WulmM5_c0L9wshW6PAyNMA.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">图 4。来源:<a class="ae lv" href="https://arxiv.org/abs/1912.05027" rel="noopener ugc nofollow" target="_blank"> SpineNet 研究论文截图</a></p></figure><h2 id="808e" class="lw lx jb bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn jh bi translated">模型</h2><figure class="mu mv mw mx gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d0651f99ada6a8105f1817989d21398d.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*QCP0G9gan6KGCQ2Mc9oQoA.png"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">图五。SpineNet-49，来源:<a class="ae lv" href="https://arxiv.org/abs/1912.05027" rel="noopener ugc nofollow" target="_blank"> SpineNet 研究论文截图</a></p></figure><p id="7269" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">为了进行公平的比较，他们声称使用相同的 ResNet 架构层，并使用 NAS 来找到完美的置换组合。他们最终架构的一个版本(AP 最高，为 40.8%)如图<em class="nc">图 5 </em>所示。此外，如前所示，与原始 ResNet 网络相比，此版本使用的计算参数和触发器更少。</p><figure class="mu mv mw mx gt is gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1d8a6de0ac38cc3b32739c30442870b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*_9-lfIOPIBMjE5EEABoPsw.png"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">图六。通过块重复增加模型深度(来源:<a class="ae lv" href="https://arxiv.org/abs/1912.05027" rel="noopener ugc nofollow" target="_blank"> SpineNet 研究论文</a></p></figure><p id="f033" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">为了增加模型深度，如 ResNet-100、ResNet-150，它们重复了相同的网络或顺序地组合网络，分别产生 SpineNet-96 或 SpineNet-143，如图 6 所示。</p><h2 id="1ef4" class="lw lx jb bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn jh bi translated">优势和可能的未来</h2><ol class=""><li id="6db1" class="nf ng jb lb b lc mo lf mp li nw lm nx lq ny lu nk nl nm nn bi translated">在这种方法的帮助下，可以创建一个更深的网络，可以增加或减少中间特征图的比例，而不会丢失空间信息。</li><li id="a8a7" class="nf ng jb lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">特征地图之间的连接应该能够跨越特征比例，从而有助于保留不同比例或分辨率的特征信息。</li><li id="97aa" class="nf ng jb lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">它显示了以不同顺序连接层的可能性，这可能是研究和实验的新方向的开端。</li></ol><h2 id="5ddb" class="lw lx jb bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn jh bi translated">可能的缺点</h2><ol class=""><li id="5c65" class="nf ng jb lb b lc mo lf mp li nw lm nx lq ny lu nk nl nm nn bi translated">该网络目前的一个缺点是，它是在 NAS 的帮助下建立的。由于不同连接背后的逻辑仍然不清楚，我们仍然不确定如何自己创建另一个类似的置换网络。</li><li id="e423" class="nf ng jb lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">虽然在论文中，他们已经表明它在对象检测和图像分类应用的情况下增强了性能。我们需要通过对不同网络的更多实验进行比较来确认 SpineNet-49 在其他情况下的表现。</li></ol><p id="e8e1" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">希望这为您提供了这种有趣方法的一个很好的概述。如果您有任何疑问或想与我进一步讨论，请通过<a class="ae lv" href="https://www.linkedin.com/in/akashsachdeva15/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。</p><p id="6466" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">你也可以在这里  <strong class="lb jl">访问全文<a class="ae lv" href="https://arxiv.org/abs/1912.05027" rel="noopener ugc nofollow" target="_blank"> <strong class="lb jl">。</strong></a></strong></p><p id="857a" class="pw-post-body-paragraph kz la jb lb b lc ld kl le lf lg ko lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated"><strong class="lb jl">参考文献</strong></p><ol class=""><li id="2461" class="nf ng jb lb b lc ld lf lg li nh lm ni lq nj lu nk nl nm nn bi translated">杜先智-林逸，<a class="ae lv" href="https://arxiv.org/abs/1912.05027" rel="noopener ugc nofollow" target="_blank"> SpineNet:用于识别和定位的学习标度-置换骨干网</a>，CVPR 2020</li><li id="0699" class="nf ng jb lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">约瑟夫·雷德蒙，阿里·法尔哈迪，<a class="ae lv" href="https://arxiv.org/abs/1804.02767" rel="noopener ugc nofollow" target="_blank">约洛夫 3:增量改进</a>，CVPR 2018</li><li id="0a0b" class="nf ng jb lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">Ren，s .，He，k .，Girshick，R.B .，，Sun，J. (2015)，<a class="ae lv" href="https://www.semanticscholar.org/paper/Faster-R-CNN%3A-Towards-Real-Time-Object-Detection-Ren-He/424561d8585ff8ebce7d5d07de8dbf7aae5e7270#references" rel="noopener ugc nofollow" target="_blank">更快的 R-CNN:使用区域提议网络实现实时目标检测</a>，<em class="nc"> IEEE 模式分析和机器智能汇刊，39 </em>，1137–1149。</li><li id="e1d7" class="nf ng jb lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated">Yann LeCun，Bernhard Boser，易小轩·登克，Donnie Henderson，Richard E Howard，Wayne Hubbard，和 Lawrence D Jackel，<a class="ae lv" href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf" rel="noopener ugc nofollow" target="_blank">应用于手写邮政编码识别的反向传播</a>，神经计算，1989</li></ol></div></div>    
</body>
</html>