# 数据科学项目的工程最佳实践

> 原文：<https://towardsdatascience.com/engineering-best-practices-for-data-science-projects-195e0687c642?source=collection_archive---------43----------------------->

## 让您的数据科学项目更加可靠、可测试和可部署

![](img/441d219fc227991a4a31db4dc614e946.png)

乔恩·泰森在 [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

# 介绍

在本帖中，我们将学习一些最佳实践来提高生产数据科学代码的质量和可靠性。

注意:这里提到的大多数东西对软件工程界来说并不陌生，但是在数据科学的实验世界中，它们经常被忽略/错过。

在这篇文章中，我将简要地提到一些主题和我们可以做的事情，以使我们的项目更加可靠，我将创建一些后续文章，使用一个项目示例来更详细地描述这些步骤。此外，在这篇文章中，我将假设一个 Python (pyspark)数据科学项目，但是这些想法可以应用于任何其他编程语言或项目。

希望你觉得有用。

# 代码重构

这是拥有更好代码的第一步。它是简化现有代码设计而不改变其行为的过程。

数据科学项目大部分时间都写在 jupyter 笔记本上，很容易失控。强烈建议在将代码投入生产之前进行代码重构。

## 解决的问题

*   提高代码可读性——让我们的团队更容易理解
*   降低复杂性——更小、更易维护的功能/模块

## 行动项目

*   将代码分解成更小的函数
*   注释功能
*   更好的命名标准
*   移除未使用的代码位

# 单元测试

单元测试是一种测试代码中每个功能的方法。目的是验证代码中的每个函数都按预期执行。

在数据科学项目中，测试几乎总是被忽略。您的项目中有一些部分可能不需要测试用例，但是在一个项目中，有许多其他组件可以很容易地进行单元测试。

例如，模型评估是在实验阶段完成的，我们可能不需要在单元测试中再次测试，但是数据清理和数据转换是绝对可以进行单元测试的部分。

## 解决的问题

*   这有助于尽早修复错误
*   它有助于新手理解代码的作用
*   支持快速代码更改
*   确保不良代码不会合并到

## 行动项目

*   创建接受所有必需参数作为参数的函数，而不是在函数内进行计算。这使得它们更容易测试
*   如果函数读取函数中的 spark 数据帧，请更改函数以接受数据帧作为参数。然后我们可以传递手工制作的数据帧来测试这些功能。
*   我们将为每个功能编写一系列单元测试
*   我们将使用 python 框架，如 unittest、pytest 等。对于单元测试
*   测试将是代码库的一部分，将确保没有坏代码被合并
*   我们的 CI/CD 管道将进一步使用这些测试来阻止不良代码的部署

# 集成测试

集成测试将系统作为一个整体进行测试。它检查所有功能组合在一起时是否工作正常。

很多时候，项目会依赖外部系统，例如，您的 pyspark 代码可能会向 Cassandra 读取/写入数据。我们可以创建集成测试，将整个项目作为一个单元进行测试，或者测试项目在外部依赖下的表现。

## 解决的问题

*   它确保整个项目正常运行。
*   它检测与多个模块协同工作相关的错误。

## 行动项目

*   我们将创建一个本地基础设施来测试整个项目
*   外部依赖可以在 Docker 容器上本地创建
*   pytest 或 unittest 等测试框架将用于编写集成测试
*   代码将根据本地基础设施运行，并测试正确性

# 代码林挺

在 jupyter 笔记本上编写项目本质上并不遵循最佳的命名或编程模式，因为笔记本的重点是速度。林挺帮助我们识别 python 代码中的语法和风格问题。

## 解决的问题

*   它有助于检测样式错误
*   强制更好/最佳的写作风格
*   检测结构问题，如使用未初始化或未定义的变量
*   让代码更容易使用

## 行动项目

*   Flake8 或 black 将用于检测逻辑和代码风格的最佳实践。
*   下一步，Lint 测试将被集成到 CI/CD 中，以使基于不良写作风格的构建失败

# 代码覆盖率

代码覆盖率帮助我们发现我们通过测试用例测试了多少代码。这是一个很好的质量指示器，可以告知项目的哪些部分需要更多的测试。

## 解决的问题

*   监控测试了多少代码。

## 行动项目

*   像 coverage.py 或 pytest-cov 这样的工具将用于测试我们的代码覆盖率。

# GitHub 回购分支机构权限

我们将设置权限来控制谁可以读取和更新 Git repo 分支中的代码。这将保持我们的主(部署分支)干净，并强制基于拉请求+构建测试的过程将代码合并到主中。

此外，强制同行评审过程和自动化测试可以确保我们的代码库中有更少的错误合并，并且其他团队成员会意识到项目中合并的变更。

## 解决的问题

*   主人总是干净的，随时可以部署
*   强制最佳实践—拉式请求+自动化构建测试
*   将避免意外删除分支
*   避免不良代码在主服务器上合并

## 行动项目

我们将使用以下内容设置分支设置:

*   主分支不允许重写分支历史记录
*   没有拉取请求，我们不能在 master 中直接合并代码
*   将代码合并到主代码至少需要 1 次批准
*   只有当所有自动化测试用例都通过后，代码才会合并

# 分支上的自动化测试执行

当我们的拉请求被创建时，在合并之前测试它以避免破坏任何代码/测试是一个好主意。

## 解决的问题

*   自动运行测试
*   避免不良代码在主服务器上合并

## 行动项目

*   Github 上的 CI/CD 设置。
*   任何新的分支代码推送都应该触发自动测试
*   应该在创建拉请求时触发自动测试
*   如果所有测试都是绿色的，则将代码部署到生产环境中

# 监控和警报

这是软件工程世界中非常重要的一步，但是对于数据科学项目来说，几乎总是被跳过。我们将监控我们的工作，如果代码中出现运行时错误，我们将发出警报。

取决于你的项目是否仅仅是做预测，你可能不会非常广泛地发出警报，但是如果项目与几个系统对话并且处理大量的数据/请求，从长远来看，进行监控将会使你的生活变得容易得多。

## 解决的问题

*   更多的可见性，而不是黑盒代码执行
*   监控输入和输出处理统计
*   监控基础架构可用性/依赖性
*   过去运行失败/成功趋势
*   当 ML 管道失败/崩溃时提醒我们

## 行动项目

*   如果您有一个监视工具(强烈推荐)—将输入/输出统计数据的事件发送到监视器
*   如果没有可用的监控工具—在日志文件中记录所有重要的统计信息。
*   如果没有监控工具——我们可以将重要的跑步统计数据添加到数据库中，以备将来参考
*   构建 Slack/微软团队集成，提醒我们管道通过/失败状态

# 就这样

这个帖子到此为止。希望这些是有用的提示。请分享您的想法和应用于数据科学项目的最佳实践。

*原载于 2020 年 11 月 7 日*[*【https://confusedcoders.com】*](https://confusedcoders.com/random/engineering-best-practices-for-data-science-projects)*。*