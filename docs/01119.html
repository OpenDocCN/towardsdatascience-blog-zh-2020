<html>
<head>
<title>NLP in the Stock Market</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">股票市场中的自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-in-the-stock-market-8760d062eb92?source=collection_archive---------4-----------------------#2020-02-01">https://towardsdatascience.com/nlp-in-the-stock-market-8760d062eb92?source=collection_archive---------4-----------------------#2020-02-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c9d9" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">金融中的人工智能</h2><div class=""/><div class=""><h2 id="4739" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">利用对 10k 填充物的情感分析作为优势</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/103beba0d84bca7d8be28698b736f1fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*gUhRIPuG2jlBN0ZW"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">图片来源:纽约时报</p></figure><p id="25ca" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在交易中实现的机器学习模型通常根据历史股价和其他定量数据进行训练，以预测未来的股价。然而，自然语言处理(NLP)使我们能够分析财务文档，如 10-k 表单，以预测股票走势。10-k 表是由公司提交的年度报告，用于提供其财务业绩的综合摘要(这些报告是由证券交易委员会强制要求的)。对投资者来说，梳理这些报告通常很乏味。通过自然语言处理的一个分支——情感分析，投资者可以快速了解报告的语气是积极的、消极的还是诉讼的等等。以 10-k 形式表达的总体情绪可以用来帮助投资者决定他们是否应该投资该公司。</p><h1 id="849e" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">NLP 解释道</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/6cedeb147effdcbfa15238f163c1d219.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZIM9cAZY_KnJSL-T7-RTKg.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">图片来源:<a class="ae mt" href="https://medium.com/@ageitgey?source=post_page-----9a0bff37854e----------------------" rel="noopener">亚当·盖特基</a></p></figure><p id="b8c8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">自然语言处理是人工智能的一个分支，涉及教计算机阅读并从语言中获取意义。由于语言如此复杂，计算机在理解文本之前必须经过一系列的步骤。以下是对典型 NLP 管道中出现的步骤的快速解释。</p><ol class=""><li id="10a8" class="mu mv iq lc b ld le lg lh lj mw ln mx lr my lv mz na nb nc bi translated">句子分割<br/>文本文档被分割成单独的句子。</li><li id="dbba" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">一旦文档被分解成句子，我们进一步将句子分解成单个单词。每个单词称为一个标记，因此得名标记化。</li><li id="6c34" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">词性标注<br/>我们将每个单词及其周围的一些单词输入到预先训练的词性分类模型中，以接收该单词的词性作为输出。</li><li id="1663" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">词汇化<br/>词语在指代同一个物体/动作时，往往以不同的形式出现。为了防止计算机将一个单词的不同形式视为不同的单词，我们执行了词条化，即将一个单词的各种词形变化组合在一起，作为一个单一项目进行分析的过程，通过该单词的词条(该单词在字典中的出现方式)进行识别。</li><li id="4c51" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">停用词<br/>非常常见的词，如“and”、“the”和“a”不提供任何值，因此我们将它们标识为停用词，以将其排除在对文本执行的任何分析之外。</li><li id="0242" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">依存解析<br/>给句子分配一个句法结构，并通过将单词输入依存解析器来理解句子中的单词是如何相互关联的。</li><li id="c164" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">名词短语<br/>把一个句子中的名词短语组合在一起，有助于在我们不关心形容词的情况下简化句子。</li><li id="855a" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">命名实体识别<br/>命名实体识别模型可以标记对象，例如人名、<br/>公司名称和地理位置。</li><li id="186e" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">共指消解<br/>由于 NLP 模型分析单个句子，它们会被其他句子中指代名词的代词所混淆。为了解决这个问题，我们使用了共指消解来跟踪句子中的代词以避免混淆。</li></ol><p id="9941" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="ni">关于 NLP 更深入的描述:阅读</em> <a class="ae mt" href="https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e" rel="noopener"> <em class="ni">这个</em> </a></p><p id="134f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">完成这些步骤后，我们的文本就可以进行分析了。现在我们更好地理解了 NLP，让我们来看看我的项目代码(来自 Udacity 的<a class="ae mt" href="https://www.udacity.com/course/ai-for-trading--nd880" rel="noopener ugc nofollow" target="_blank"> AI for Trading </a>课程的项目 5)。点击<a class="ae mt" href="https://github.com/roshan-adusumilli/nlp_10-ks/blob/master/NLP_on_Financial_Statements.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>查看完整的 Github 库</p><h1 id="e2d9" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">导入/下载</h1><p id="187b" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">首先，我们进行必要的进口；<a class="ae mt" href="https://github.com/roshan-adusumilli/nlp_10-ks/blob/master/project_helper.py" rel="noopener ugc nofollow" target="_blank"> project_helper </a>包含各种实用程序和图形函数。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="66c4" class="nt lx iq np b gy nu nv l nw nx">import nltk<br/>import numpy as np<br/>import pandas as pd<br/>import pickle<br/>import pprint<br/>import project_helper<br/><br/><br/>from tqdm import tqdm</span></pre><p id="4fc3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">然后，我们下载停用词语料库来移除停用词，并下载 wordnet 语料库来进行词汇化。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="6f44" class="nt lx iq np b gy nu nv l nw nx">nltk.download('stopwords')<br/>nltk.download('wordnet')</span></pre><h1 id="541d" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">获得 10 公里</h1><p id="0f36" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">10-k 文档包括公司历史、组织结构、高管薪酬、股权、子公司和经审计的财务报表等信息。为了查找 10-k 文档，我们使用每个公司的唯一 CIK(中央索引键)。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="eb76" class="nt lx iq np b gy nu nv l nw nx">cik_lookup = {<br/>    'AMZN': '0001018724',<br/>    'BMY': '0000014272',   <br/>    'CNP': '0001130310',<br/>    'CVX': '0000093410',<br/>    'FL': '0000850209',<br/>    'FRT': '0000034903',<br/>    'HON': '0000773840'}</span></pre><p id="959e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在，我们从 SEC 提取一个已备案的 10-k 列表，并显示 Amazon 数据作为示例。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="2a63" class="nt lx iq np b gy nu nv l nw nx">sec_api = project_helper.SecAPI()</span><span id="14fd" class="nt lx iq np b gy ny nv l nw nx">from bs4 import BeautifulSoup</span><span id="f865" class="nt lx iq np b gy ny nv l nw nx">def get_sec_data(cik, doc_type, start=0, count=60):<br/>    rss_url = '<a class="ae mt" href="https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany'" rel="noopener ugc nofollow" target="_blank">https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany'</a> \<br/>        '&amp;CIK={}&amp;type={}&amp;start={}&amp;count={}&amp;owner=exclude&amp;output=atom' \<br/>        .format(cik, doc_type, start, count)<br/>    sec_data = sec_api.get(rss_url)<br/>    feed = BeautifulSoup(sec_data.encode('ascii'), 'xml').feed<br/>    entries = [<br/>        (<br/>            entry.content.find('filing-href').getText(),<br/>            entry.content.find('filing-type').getText(),<br/>            entry.content.find('filing-date').getText())<br/>        for entry in feed.find_all('entry', recursive=False)]</span><span id="07d7" class="nt lx iq np b gy ny nv l nw nx">return entries</span><span id="7b12" class="nt lx iq np b gy ny nv l nw nx">example_ticker = 'AMZN'<br/>sec_data = {}</span><span id="3410" class="nt lx iq np b gy ny nv l nw nx">for ticker, cik in cik_lookup.items():<br/>    sec_data[ticker] = get_sec_data(cik, '10-K')</span><span id="f0b9" class="nt lx iq np b gy ny nv l nw nx">pprint.pprint(sec_data[example_ticker][:5])</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nz"><img src="../Images/20bef697ba28a0936503a8776d812cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nURI5HyOCFawTyVIIjNYhw.png"/></div></div></figure><p id="c92d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们收到一个 URL 列表，指向包含与每次填充相关的元数据的文件。元数据与我们无关，所以我们通过用填充 url 替换 url 来提取填充。让我们使用 tqdm 查看下载进度，并看一个示例文档。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="6808" class="nt lx iq np b gy nu nv l nw nx">raw_fillings_by_ticker = {}</span><span id="43b9" class="nt lx iq np b gy ny nv l nw nx">for ticker, data in sec_data.items():<br/>    raw_fillings_by_ticker[ticker] = {}<br/>    for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):<br/>        if (file_type == '10-K'):<br/>            file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            <br/>            <br/>            raw_fillings_by_ticker[ticker][file_date] = sec_api.get(file_url)</span><span id="116b" class="nt lx iq np b gy ny nv l nw nx">print('Example Document:\n\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/caa142e7bfd8c2aed9dbaac7e5cff27d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eOlFhEEtN3-nZeaV4TwByg.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/b1e053250a0d95ab4194b8018f800014.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRwhk-EPDryVvqqIiuFwFg.png"/></div></div></figure><p id="52bf" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">将下载的文件分解成相关的文件，这些文件被分割成多个部分，每个文件的开头用标签<DOCUMENT>表示，每个文件的结尾用标签</DOCUMENT>表示。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="8f71" class="nt lx iq np b gy nu nv l nw nx">import re</span><span id="5ccc" class="nt lx iq np b gy ny nv l nw nx">def get_documents(text):</span><span id="141e" class="nt lx iq np b gy ny nv l nw nx">    extracted_docs = []<br/>    <br/>    doc_start_pattern = re.compile(r'&lt;DOCUMENT&gt;')<br/>    doc_end_pattern = re.compile(r'&lt;/DOCUMENT&gt;')   <br/>    <br/>    doc_start_is = [x.end() for x in      doc_start_pattern.finditer(text)]<br/>    doc_end_is = [x.start() for x in doc_end_pattern.finditer(text)]<br/>    <br/>    for doc_start_i, doc_end_i in zip(doc_start_is, doc_end_is):<br/>            extracted_docs.append(text[doc_start_i:doc_end_i])<br/>    <br/>    return extracted_docs</span><span id="187f" class="nt lx iq np b gy ny nv l nw nx">filling_documents_by_ticker = {}</span><span id="7abe" class="nt lx iq np b gy ny nv l nw nx">for ticker, raw_fillings in raw_fillings_by_ticker.items():<br/>    filling_documents_by_ticker[ticker] = {}<br/>    for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):<br/>        filling_documents_by_ticker[ticker][file_date] = get_documents(filling)</span><span id="9145" class="nt lx iq np b gy ny nv l nw nx">print('\n\n'.join([<br/>    'Document {} Filed on {}:\n{}...'.format(doc_i, file_date, doc[:200])<br/>    for file_date, docs in filling_documents_by_ticker[example_ticker].items()<br/>    for doc_i, doc in enumerate(docs)][:3]))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/73855efc2079bc05f06cca063bf39eef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pg7a2pltO4VURijH2xo2Jw.png"/></div></div></figure><p id="0e6c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">定义 get_document_type 函数以返回给定的文档类型。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="c4b8" class="nt lx iq np b gy nu nv l nw nx">def get_document_type(doc):<br/>    <br/>    type_pattern = re.compile(r'&lt;TYPE&gt;[^\n]+')<br/>    <br/>    doc_type = type_pattern.findall(doc)[0][len('&lt;TYPE&gt;'):] <br/>    <br/>    return doc_type.lower()</span></pre><p id="12c4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">使用 get_document_type 函数从填充物中过滤出非 10k 文档。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="8e13" class="nt lx iq np b gy nu nv l nw nx">ten_ks_by_ticker = {}</span><span id="604d" class="nt lx iq np b gy ny nv l nw nx">for ticker, filling_documents in filling_documents_by_ticker.items():<br/>    ten_ks_by_ticker[ticker] = []<br/>    for file_date, documents in filling_documents.items():<br/>        for document in documents:<br/>            if get_document_type(document) == '10-k':<br/>                ten_ks_by_ticker[ticker].append({<br/>                    'cik': cik_lookup[ticker],<br/>                    'file': document,<br/>                    'file_date': file_date})</span><span id="b00c" class="nt lx iq np b gy ny nv l nw nx">project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/94fb80d7fe67d028882696566ab7b9b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oiHocMXk-RhaWBALJSllTg.png"/></div></div></figure><h1 id="6a52" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">预处理数据</h1><p id="547b" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">删除 html 并使所有文本小写，以清理文档文本。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="b3fe" class="nt lx iq np b gy nu nv l nw nx">def remove_html_tags(text):<br/>    text = BeautifulSoup(text, 'html.parser').get_text()<br/>    <br/>    return text</span><span id="b5d7" class="nt lx iq np b gy ny nv l nw nx">def clean_text(text):<br/>    text = text.lower()<br/>    text = remove_html_tags(text)<br/>    <br/>    return text</span></pre><p id="4be6" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">使用 clean_text 函数清理文档。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="931d" class="nt lx iq np b gy nu nv l nw nx">for ticker, ten_ks in ten_ks_by_ticker.items():<br/>    for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):<br/>        ten_k['file_clean'] = clean_text(ten_k['file'])</span><span id="4a56" class="nt lx iq np b gy ny nv l nw nx">project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean'])</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/c19261539f11b2682b18d31786376126.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K_Jk3Oe4Gd2akvIffCU80g.png"/></div></div></figure><p id="1651" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在我们对所有数据进行假设。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="6f87" class="nt lx iq np b gy nu nv l nw nx">from nltk.stem import WordNetLemmatizer<br/>from nltk.corpus import wordnet</span><span id="a467" class="nt lx iq np b gy ny nv l nw nx">def lemmatize_words(words):<br/><br/>    lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in words]<br/>    <br/>    return lemmatized_words</span><span id="9759" class="nt lx iq np b gy ny nv l nw nx">word_pattern = re.compile('\w+')</span><span id="a839" class="nt lx iq np b gy ny nv l nw nx">for ticker, ten_ks in ten_ks_by_ticker.items():<br/>    for ten_k in tqdm(ten_ks, desc='Lemmatize {} 10-Ks'.format(ticker), unit='10-K'):<br/>        ten_k['file_lemma'] = lemmatize_words(word_pattern.findall(ten_k['file_clean']))</span><span id="01ed" class="nt lx iq np b gy ny nv l nw nx">project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_lemma'])</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/763ecaee5e609ec236e20f51c8fa66f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fkk7e5EtrAwFc59-FxVrrQ.png"/></div></div></figure><p id="69f9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">删除停用词。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="367a" class="nt lx iq np b gy nu nv l nw nx">from nltk.corpus import stopwords</span><span id="7032" class="nt lx iq np b gy ny nv l nw nx">lemma_english_stopwords = lemmatize_words(stopwords.words('english'))</span><span id="4ffa" class="nt lx iq np b gy ny nv l nw nx">for ticker, ten_ks in ten_ks_by_ticker.items():<br/>    for ten_k in tqdm(ten_ks, desc='Remove Stop Words for {} 10-Ks'.format(ticker), unit='10-K'):<br/>        ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]</span><span id="765c" class="nt lx iq np b gy ny nv l nw nx">print('Stop Words Removed')</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/999211dd29ea382044e2e161d95d392c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*beM_yTEEwAGfgsS8u19aKA.png"/></div></div></figure><h1 id="c2ba" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">10k 上的情感分析</h1><p id="6035" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">使用 Loughran-McDonald 情感词表对 10-k 进行情感分析(这是专门为与金融相关的文本分析而构建的)。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="4bb9" class="nt lx iq np b gy nu nv l nw nx">sentiments = ['negative', 'positive', 'uncertainty', 'litigious', 'constraining', 'interesting']<br/><br/>sentiment_df = pd.read_csv('loughran_mcdonald_master_dic_2018.csv')<br/>sentiment_df.columns = [column.lower() for column in sentiment_df.columns] <em class="ni"># Lowercase the columns for ease of use</em><br/><br/><em class="ni"># Remove unused information</em><br/>sentiment_df = sentiment_df[sentiments + ['word']]<br/>sentiment_df[sentiments] = sentiment_df[sentiments].astype(bool)<br/>sentiment_df = sentiment_df[(sentiment_df[sentiments]).any(1)]<br/><br/><em class="ni"># Apply the same preprocessing to these words as the 10-k words</em><br/>sentiment_df['word'] = lemmatize_words(sentiment_df['word'].str.lower())<br/>sentiment_df = sentiment_df.drop_duplicates('word')<br/><br/><br/>sentiment_df.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/6e9321f61f298b4d3c119460c1dfb79d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*agaKb6zgRM5FRCFESCpmXA.png"/></div></div></figure><p id="6bd3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">使用情感单词列表从 10k 文档生成情感单词包。单词包统计每篇文档中的情感词数量。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="4c65" class="nt lx iq np b gy nu nv l nw nx">from collections import defaultdict, Counter<br/>from sklearn.feature_extraction.text import CountVectorizer</span><span id="5ed0" class="nt lx iq np b gy ny nv l nw nx">def get_bag_of_words(sentiment_words, docs):<br/><br/>    vec = CountVectorizer(vocabulary=sentiment_words)<br/>    vectors = vec.fit_transform(docs)<br/>    words_list = vec.get_feature_names()<br/>    bag_of_words = np.zeros([len(docs), len(words_list)])<br/>    <br/>    for i in range(len(docs)):<br/>        bag_of_words[i] = vectors[i].toarray()[0]</span><span id="3dec" class="nt lx iq np b gy ny nv l nw nx">return bag_of_words.astype(int)</span><span id="e924" class="nt lx iq np b gy ny nv l nw nx">sentiment_bow_ten_ks = {}</span><span id="1b06" class="nt lx iq np b gy ny nv l nw nx">for ticker, ten_ks in ten_ks_by_ticker.items():<br/>    lemma_docs = [' '.join(ten_k['file_lemma']) for ten_k in ten_ks]<br/>    <br/>    sentiment_bow_ten_ks[ticker] = {<br/>        sentiment: get_bag_of_words(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)<br/>        for sentiment in sentiments}</span><span id="be86" class="nt lx iq np b gy ny nv l nw nx">project_helper.print_ten_k_data([sentiment_bow_ten_ks[example_ticker]], sentiments)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/04805befdce775d6ef162e6305b33b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8UMjGXn3D3t1ClBhvR4HmA.png"/></div></div></figure><h1 id="75e5" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">雅克卡相似性</h1><p id="f084" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">现在我们有了单词包，我们可以将它转换成一个布尔数组，并计算 jaccard 相似度。jaccard 相似性被定义为交集的大小除以两个集合的并集的大小。例如，两个句子之间的 jaccard 相似度是两个句子之间的常用单词数除以两个句子中唯一单词的总数。jaccard 相似性值越接近 1，集合就越相似。为了更容易理解我们的计算，我们绘制了 jaccard 的相似点。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="3d31" class="nt lx iq np b gy nu nv l nw nx">from sklearn.metrics import jaccard_similarity_score</span><span id="a72a" class="nt lx iq np b gy ny nv l nw nx">def get_jaccard_similarity(bag_of_words_matrix):<br/>    <br/>    jaccard_similarities = []<br/>    bag_of_words_matrix = np.array(bag_of_words_matrix, dtype=bool)<br/>    <br/>    for i in range(len(bag_of_words_matrix)-1):<br/>            u = bag_of_words_matrix[i]<br/>            v = bag_of_words_matrix[i+1]<br/>              <br/>    jaccard_similarities.append(jaccard_similarity_score(u,v))    <br/>    <br/>    return jaccard_similarities</span><span id="25fc" class="nt lx iq np b gy ny nv l nw nx"># Get dates for the universe<br/>file_dates = {<br/>    ticker: [ten_k['file_date'] for ten_k in ten_ks]<br/>    for ticker, ten_ks in ten_ks_by_ticker.items()}</span><span id="1e78" class="nt lx iq np b gy ny nv l nw nx">jaccard_similarities = {<br/>    ticker: {<br/>        sentiment_name: get_jaccard_similarity(sentiment_values)<br/>        for sentiment_name, sentiment_values in ten_k_sentiments.items()}<br/>    for ticker, ten_k_sentiments in sentiment_bow_ten_ks.items()}</span><span id="d7ff" class="nt lx iq np b gy ny nv l nw nx">project_helper.plot_similarities(<br/>    [jaccard_similarities[example_ticker][sentiment] for sentiment in sentiments],<br/>    file_dates[example_ticker][1:],<br/>    'Jaccard Similarities for {} Sentiment'.format(example_ticker),<br/>    sentiments)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/564498725e0935b8a683b8e6e7d4d913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8o0WEGjivqJJSfvILvSJQ.png"/></div></div></figure><h1 id="ccc7" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">TFIDF</h1><p id="6607" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">从情感词表中，让我们从 10k 文档中生成情感词频-逆文档频率(TFIDF)。TFIDF 是一种信息检索技术，用于揭示一个单词/术语在所选文本集合中出现的频率。每个术语被分配一个术语频率(TF)和逆文档频率(IDF)分数。这些分数的乘积被称为该项的 TFIDF 权重。较高的 TFIDF 权重表示较罕见的术语，较低的 TFIDF 分数表示较常见的术语。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="7185" class="nt lx iq np b gy nu nv l nw nx">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="91a9" class="nt lx iq np b gy ny nv l nw nx">def get_tfidf(sentiment_words, docs):<br/>    <br/>    vec = TfidfVectorizer(vocabulary=sentiment_words)<br/>    tfidf = vec.fit_transform(docs)<br/>    <br/>    return tfidf.toarray()</span><span id="d664" class="nt lx iq np b gy ny nv l nw nx">sentiment_tfidf_ten_ks = {}</span><span id="5833" class="nt lx iq np b gy ny nv l nw nx">for ticker, ten_ks in ten_ks_by_ticker.items():<br/>    lemma_docs = [' '.join(ten_k['file_lemma']) for ten_k in ten_ks]<br/>    <br/>    sentiment_tfidf_ten_ks[ticker] = {<br/>        sentiment: get_tfidf(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)<br/>        for sentiment in sentiments}</span><span id="244b" class="nt lx iq np b gy ny nv l nw nx">project_helper.print_ten_k_data([sentiment_tfidf_ten_ks[example_ticker]], sentiments)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oa"><img src="../Images/d256bf6675030f5cce8cbfc4a0e6d063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1jk69fqtWUxlPBZDKOIukg.png"/></div></div></figure><h1 id="4e44" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">余弦相似性</h1><p id="eb9c" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">根据我们的 TFIDF 值，我们可以计算余弦相似度，并绘制出随时间变化的曲线。与 jaccard 相似性类似，余弦相似性是一种用于确定文档相似程度的度量。余弦相似性通过测量在多维空间中投影的两个向量之间的角度余弦来计算相似性，而不考虑大小。对于文本分析，使用的两个向量通常是包含两个文档字数的数组。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="be7d" class="nt lx iq np b gy nu nv l nw nx">from sklearn.metrics.pairwise import cosine_similarity</span><span id="1daf" class="nt lx iq np b gy ny nv l nw nx">def get_cosine_similarity(tfidf_matrix):<br/>    <br/>    cosine_similarities = []    <br/>    <br/>    for i in range(len(tfidf_matrix)-1):<br/>        <br/>cosine_similarities.append(cosine_similarity(tfidf_matrix[i].reshape(1, -1),tfidf_matrix[i+1].reshape(1, -1))[0,0])<br/>    <br/>    return cosine_similarities</span><span id="ad36" class="nt lx iq np b gy ny nv l nw nx">cosine_similarities = {<br/>    ticker: {<br/>        sentiment_name: get_cosine_similarity(sentiment_values)<br/>        for sentiment_name, sentiment_values in ten_k_sentiments.items()}<br/>    for ticker, ten_k_sentiments in sentiment_tfidf_ten_ks.items()}</span><span id="3ee1" class="nt lx iq np b gy ny nv l nw nx">project_helper.plot_similarities(<br/>    [cosine_similarities[example_ticker][sentiment] for sentiment in sentiments],<br/>    file_dates[example_ticker][1:],<br/>    'Cosine Similarities for {} Sentiment'.format(example_ticker),<br/>    sentiments)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi ob"><img src="../Images/498308734b1877429dc719d8c82aefcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wodWcr4qX7hkjrylHN6VWQ.png"/></div></div></figure><h1 id="0673" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">价格数据</h1><p id="fde0" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">现在我们将通过与股票的年度定价进行比较来评估阿尔法因子。我们可以从 QuoteMedia 下载价格数据。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="8e08" class="nt lx iq np b gy nu nv l nw nx">pricing = pd.read_csv('yr-quotemedia.csv', parse_dates=['date'])<br/>pricing = pricing.pivot(index='date', columns='ticker', values='adj_close')<br/><br/>pricing</span></pre><h1 id="e3a5" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">转换为数据帧</h1><p id="3ab9" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">Alphalens 是一个用于 alpha 因子性能分析的 python 库，它使用数据帧，所以我们必须将字典转换成数据帧。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="582f" class="nt lx iq np b gy nu nv l nw nx">cosine_similarities_df_dict = {'date': [], 'ticker': [], 'sentiment': [], 'value': []}</span><span id="c535" class="nt lx iq np b gy ny nv l nw nx">for ticker, ten_k_sentiments in cosine_similarities.items():<br/>    for sentiment_name, sentiment_values in ten_k_sentiments.items():<br/>        for sentiment_values, sentiment_value in enumerate(sentiment_values):<br/>            cosine_similarities_df_dict['ticker'].append(ticker)<br/>            cosine_similarities_df_dict['sentiment'].append(sentiment_name)<br/>            cosine_similarities_df_dict['value'].append(sentiment_value)<br/>            cosine_similarities_df_dict['date'].append(file_dates[ticker][1:][sentiment_values])</span><span id="a42b" class="nt lx iq np b gy ny nv l nw nx">cosine_similarities_df = pd.DataFrame(cosine_similarities_df_dict)<br/>cosine_similarities_df['date'] = pd.DatetimeIndex(cosine_similarities_df['date']).year<br/>cosine_similarities_df['date'] = pd.to_datetime(cosine_similarities_df['date'], format='%Y')</span><span id="aba2" class="nt lx iq np b gy ny nv l nw nx">cosine_similarities_df.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/4004505e7255405b2680e7d43ae15745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*Icas6oL4K8rPw4sV_x39ew.png"/></div></figure><p id="01fe" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在我们可以使用许多 alphalens 函数之前，我们需要对齐索引并将时间转换为 unix 时间戳。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="7a7d" class="nt lx iq np b gy nu nv l nw nx">import alphalens as al</span><span id="73c2" class="nt lx iq np b gy ny nv l nw nx">factor_data = {}<br/>skipped_sentiments = []</span><span id="5622" class="nt lx iq np b gy ny nv l nw nx">for sentiment in sentiments:<br/>    cs_df = cosine_similarities_df[(cosine_similarities_df['sentiment'] == sentiment)]<br/>    cs_df = cs_df.pivot(index='date', columns='ticker', values='value')<br/>    <br/>    try:<br/>        data = al.utils.get_clean_factor_and_forward_returns(cs_df.stack(), pricing.loc[cs_df.index], quantiles=5, bins=None, periods=[1])<br/>        factor_data[sentiment] = data<br/>    except:<br/>        skipped_sentiments.append(sentiment)</span><span id="44c9" class="nt lx iq np b gy ny nv l nw nx">if skipped_sentiments:<br/>    print('\nSkipped the following sentiments:\n{}'.format('\n'.join(skipped_sentiments)))<br/>factor_data[sentiments[0]].head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi od"><img src="../Images/a6667eabd07aef5a1c9be8162de9fd31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*saRdMzKdVXSUOOQMaxS9kw.png"/></div></div></figure><p id="1a8a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">为了与 alphalen 的 factor _ rank _ 自相关和 mean_return_by_quantile 函数兼容，我们还必须使用 unix time 创建因子数据帧。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="913e" class="nt lx iq np b gy nu nv l nw nx">unixt_factor_data = {<br/>    factor: data.set_index(pd.MultiIndex.from_tuples(<br/>        [(x.timestamp(), y) for x, y in data.index.values],<br/>        names=['date', 'asset']))<br/>    for factor, data in factor_data.items()}</span></pre><h1 id="63b1" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">因子回报</h1><p id="73d2" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">让我们来看看一段时间内的因子回报</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="bd33" class="nt lx iq np b gy nu nv l nw nx">ls_factor_returns = pd.DataFrame()</span><span id="d363" class="nt lx iq np b gy ny nv l nw nx">for factor_name, data in factor_data.items():<br/>    ls_factor_returns[factor_name] = al.performance.factor_returns(data).iloc[:, 0]</span><span id="e662" class="nt lx iq np b gy ny nv l nw nx">(1 + ls_factor_returns).cumprod().plot()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oe"><img src="../Images/e7024cf7c55cd5f66157a95936d0d213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9--iDx8nqqHpU5HQcWG_7w.png"/></div></div></figure><p id="3b61" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">正如预期的那样，表达积极情绪的 10-k 报告产生了最多的收益，而包含消极情绪的 10-k 报告导致了最多的损失。</p><h1 id="c2a8" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">营业额分析</h1><p id="cc5c" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">使用因子等级自相关，我们可以分析阿尔法随着时间的推移有多稳定。我们希望阿尔法等级在不同时期保持相对一致。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="e5db" class="nt lx iq np b gy nu nv l nw nx">ls_FRA = pd.DataFrame()</span><span id="5808" class="nt lx iq np b gy ny nv l nw nx">for factor, data in unixt_factor_data.items():<br/>    ls_FRA[factor] = al.performance.factor_rank_autocorrelation(data)</span><span id="e9d1" class="nt lx iq np b gy ny nv l nw nx">ls_FRA.plot(title="Factor Rank Autocorrelation")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oe"><img src="../Images/a1e19f78ee01c903edb84b020b2ff316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3gLGRqbLwVCxC7cQV7FhkA.png"/></div></div></figure><h1 id="0d45" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">夏普比率</h1><p id="8356" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">最后，让我们来计算夏普比率，即平均回报减去无风险回报除以投资回报的标准差。</p><pre class="kp kq kr ks gt no np nq nr aw ns bi"><span id="056c" class="nt lx iq np b gy nu nv l nw nx">daily_annualization_factor = np.sqrt(252)  </span><span id="1582" class="nt lx iq np b gy ny nv l nw nx">(daily_annualization_factor * ls_factor_returns.mean() / ls_factor_returns.std()).round(2)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/1a2ca4aaeaecb77032e0d3c990350e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*pKQPpUZLmX8_T-SQKeIEUw.png"/></div></figure><p id="cb9c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">夏普比率 1 被认为是可接受的，比率 2 是非常好的，比率 3 是极好的。正如所料，我们可以看到积极情绪与高夏普比率相关，而消极情绪与低夏普比率相关。其他情绪也与高夏普比率相关。然而，由于影响股票价格的复杂因素如此之多，要在现实世界中复制这些回报要困难得多。</p><h1 id="236b" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">参考</h1><p id="c50d" class="pw-post-body-paragraph la lb iq lc b ld nj ka lf lg nk kd li lj nl ll lm ln nm lp lq lr nn lt lu lv ij bi translated">[1] Udacity，<a class="ae mt" href="https://github.com/udacity/artificial-intelligence-for-trading/tree/master/project/project_5" rel="noopener ugc nofollow" target="_blank">用于交易的人工智能</a>，Github</p></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><p id="2f47" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">先别走</strong>！</p><p id="e00e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我是 Roshan，16 岁，对人工智能和金融的交叉领域充满热情。关于人工智能在金融领域的广泛观点，请查看这篇文章:<a class="ae mt" rel="noopener" target="_blank" href="/artificial-intelligence-and-its-application-in-finance-9f1e0588e777">https://towards data science . com/artificial-intelligence-and-its-application-in-finance-9f1e 0588 e 777</a>。</p><p id="e5fa" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在 Linkedin 上联系我:<a class="ae mt" href="https://www.linkedin.com/in/roshan-adusumilli-96b104194/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/roshan-adusumilli-96b104194/</a></p></div></div>    
</body>
</html>