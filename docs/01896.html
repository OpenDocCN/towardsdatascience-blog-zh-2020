<html>
<head>
<title>Object-based Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于对象的强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-oriented-reinforcement-learning-95c284427ea?source=collection_archive---------17-----------------------#2020-02-22">https://towardsdatascience.com/object-oriented-reinforcement-learning-95c284427ea?source=collection_archive---------17-----------------------#2020-02-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1070" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对世界物体的认知能提高强化学习算法的有效性和效率吗？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9981c606208f1e1399f4b5026debf8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZsiJN7G1YP6INT1d"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有一天，强化学习可能会提供正确的工具来建立完全自主的代理。在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@franckinjapan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Franck V. </a>拍摄的照片</p></figure><p id="7d6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">强化学习</strong>为建模行为构成了一种强大的形式主义，它允许我们解决多种类型的<strong class="lb iu">复杂决策问题</strong>，例如<em class="lv">游戏</em>、<em class="lv">自主机器人</em>、<em class="lv">自动股票</em>交易；我们<em class="lv"> </em>曾经认为用经典的(甚至基于人工智能的)算法方法几乎不可能有效地解决这些问题。</p><p id="fda3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将探索一个名为<strong class="lb iu">面向对象强化学习</strong>【1】<strong class="lb iu"/>的强化学习形式，其主要特点是在更高的抽象层次上进行推理，即在世界/环境的对象及其关系的层次上进行推理。这种形式保证了优越的性能以及采样效率的显著提高。</p><h2 id="a9ba" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">内容</h2><p id="3ca0" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">这篇文章的结构如下:</p><ul class=""><li id="2321" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">强化学习简介，</li><li id="d50f" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">RL 的基本限制，</li><li id="d5e5" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">面向对象的强化学习，</li><li id="f0f5" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">结论和参考文献。</li></ul></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="fb75" class="np lx it bd ly nq nr ns mb nt nu nv me jz nw ka mh kc nx kd mk kf ny kg mn nz bi translated">RL 的非正式介绍</h1><p id="183e" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">强化学习提供了一套工具来训练一个<strong class="lb iu"> <em class="lv">代理</em> </strong>在<strong class="lb iu"> <em class="lv">环境</em> </strong>(一个真实或模拟的世界)中，通过<em class="lv">试错</em>(即执行一个动作，然后体验它的效果)，仅在<strong class="lb iu"> <em class="lv">奖励</em> </strong>的指导下，采取最优<em class="lv">动作</em>。奖励是一个标量反馈信号，告诉代理人所采取的行动有多好。代理人收集奖励是为了学习在给定的情况或状态下执行哪种动作是最好的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/756ecfad0efb390ff6ca99d053aa04f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*U85RAVpYCjckkNgSNv7Djw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">RL 位于许多知名领域的交汇点。[ <a class="ae ky" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf" rel="noopener ugc nofollow" target="_blank">来源</a> ]</p></figure><p id="65f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，代理学习如何解决一个<em class="lv">任务</em>即决策问题。Atask 被正式定义为根据环境和奖励函数描述的<strong class="lb iu">马尔可夫决策过程</strong> (MDP)。环境决定了世界的<em class="lv">动态</em>，即当一个动作在特定情况下被执行时接下来会发生什么(称为<strong class="lb iu">状态</strong>)，而奖励函数告诉代理在给定世界的当前情况(其状态)下，它通过执行一个动作赚了(或赔了)多少。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl ob"><img src="../Images/e22c55633186a3d5b1e145b0d6c7c90a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*vz3AN1mBUR2cr_jEG8s7Mg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在每个时间步，代理通过执行一个动作来改变环境的状态。行动的效果是由环境提供的，作为下一个状态和奖励。代理使用奖励或两者的反馈来改善其行为。[ <a class="ae ky" href="https://www.math.csi.cuny.edu/~mvj/MTH513/lectures/Lecture22.html#/" rel="noopener ugc nofollow" target="_blank">来源</a> ]</p></figure><p id="36bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MDP 的解决方案是确定代理行为的<strong class="lb iu">策略</strong> 𝛑。最佳策略是"<em class="lv">最大化(预期)回报的(贴现)总和"</em>，也称为<strong class="lb iu"> RL 目标</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/7d7002e25c03d8180615b1d889d0d459.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/0*OyEtznJDJJoPt4NT.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">RL 目标。最佳代理人是能获得最高回报的人。当从随机策略中抽取行动样本时，我们通过行动概率对累积回报进行加权。</p></figure><p id="0a21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，这是一个通用公式，因为该策略可以用多种方式表示:作为一个<em class="lv">表</em>、<em class="lv">手工设计功能的线性或浅层组合</em>，或者作为一个<strong class="lb iu">深度神经网络</strong>。此外，策略可以是随机的或确定的。<em class="lv">随机</em>策略是行动的概率分布，在这种情况下，RL 目标是最大化<em class="lv">预期</em>奖励总和。相反，一个<em class="lv">确定性</em>策略是状态<em class="lv"> s </em>的确定性函数，其中只有一个动作具有非零概率。</p><p id="9063" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">深度 RL </strong>支持深度神经网络策略，因为它们既有表达性(我们可以轻松增加模型容量)，又在空间和时间上计算高效，因为计算神经网络的<em class="lv">正向传递</em>通常是廉价的。用这些术语来表达，RL 目标旨在找到政策的参数θ，该参数产生最高的可能(贴现)累积回报总和:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/b05e75b05599f9d4b81f39a0c21f620c.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/0*LG814gmHjAmQytES.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">当策略是具有参数θ的神经网络时的 RL 目标。注意，期望是在轨迹𝜏上，即状态和动作对(s，a ),通过与环境交互并根据具有参数θ的策略行动而获得。</p></figure><p id="3b9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，在这两个目标中都有一个<strong class="lb iu"><em class="lv"/></strong>𝛄折现因子，即一个介于 0 和 1 之间的实数，用于“衡量”未来的奖励。当<em class="lv">时间范围 T </em>为无穷大时，贴现因子是强制性的，因为它防止了报酬总和的发散。一般来说，我们总是想用 0.9、0.95 或 0.99 的系数来贴现奖励。</p><p id="5631" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了计算最优策略，我们需要一种比较策略的方法。直觉告诉我们，一项更好的政策会惠及“更有价值”的州，也会挑选出“更有回报”的行动:</p><ul class=""><li id="3874" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated"><strong class="lb iu">状态值函数 V(s) </strong>，量化了我们从状态<em class="lv"> s </em>采取最优行动平均可以获得多少回报。如果<em class="lv"> s </em>是<em class="lv">初始状态</em>其值衡量当前策略从开始时平均会得到多少奖励。因此，最佳策略是实现最高价值的策略。同样的，</li><li id="7700" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu">动作值函数 Q(s，a) </strong>表示当处于状态<em class="lv"> s </em>时采取动作<em class="lv"> a </em>有多好。action-value 函数有助于理解在状态<em class="lv"> s </em>时采取哪个特定动作是最好的。我们知道最优策略只执行最优动作，即面对给定状态时 Q 值最大的动作。</li></ul><p id="5e9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个函数是寻找最优策略的<strong class="lb iu">贝尔曼最优方程</strong>的基本成分。对于小问题，这个方程可以用动态规划很容易地解决。相反，对于高维度和连续的 MDP，我们通常使用神经网络来近似由<em class="lv">无模型</em>方法(例如 Q 学习、TD 学习、策略梯度、参与者-批评家)和<em class="lv">基于模型的</em>方法(例如 Dyna)使用的 V 和/或 Q，以找到给定任务的最佳策略。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="96c1" class="np lx it bd ly nq nr ns mb nt nu nv me jz nw ka mh kc nx kd mk kf ny kg mn nz bi translated">深度强化学习有什么问题？</h1><p id="ef95" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">Deep RL 正在实现几个突破性的人工智能，它非常有前途，但并不完美，因为它继承了深度学习和经典强化学习的问题。主要缺陷是:</p><ul class=""><li id="42c0" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated"><strong class="lb iu">样本效率:</strong>深度神经网络(DNNs)需要具有大量示例的数据集(想想 ImageNet)才能实现良好的性能。对于作为 dnn 实现的代理策略也是如此，需要来自其环境的数百万个经验样本。减少样本数量是减少训练时间的基础。</li><li id="fdff" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu">概括:</strong>在训练期间很少或根本没有样本的区域中，DNNs 的输出不一致。因此，在大而多样化的数据集上训练网络对于在输入空间的大范围内实现低概化误差非常重要。对于 RL 来说，情况要复杂得多:如果一个代理学习做某事(例如驾驶)，它应该能够<em class="lv">重用</em>和<em class="lv">调整</em>它的技能，以便当它被要求解决与它被训练的任务相关的任务时“概括它的行为”。<strong class="lb iu">元强化学习</strong>旨在训练代理人能够快速调整他们的行为以适应新的任务。</li><li id="92e5" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu">安全:</strong>物理媒介(如机器人)在接受训练前后都可能是危险的。在学习时，物理代理与真实环境(如果没有经过模拟训练)进行交互，以学习什么动作是好的。如果这种相互作用没有受到约束，该制剂可能会损坏自身(例如过热、撞击物体、破坏致动器)以及由于随意尝试动作而导致的人身伤害。在学习了最佳行为之后，由于错误的预测(例如，对象的错误分类)，代理仍然可能是危险的。因此，有必要对动作<strong class="lb iu"> <em class="lv"> </em> </strong>实施两种<strong class="lb iu"> <em class="lv">安全约束</em> </strong>，以了解执行哪些动作是安全的，并对预测中的<strong class="lb iu"><em class="lv"/></strong><em class="lv"/>不确定性建模，以考虑次优(悲观)行为。</li><li id="c4c8" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu">合规:</strong>智能体在与人类互动时，应该能够根据<em class="lv">用户偏好</em>、<em class="lv">非语言提示</em>、<em class="lv">法律和社会规范</em>调整自己的行为。</li><li id="f1d7" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu">可解释性:</strong>为了理解和调试整个代理决策过程，代理的策略应该可以由人来解释，即，为了输出特定的动作，输入的哪些元素与代理相关。</li></ul><p id="9d83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这五个问题是现代强化学习算法的一般局限性。克服这些限制是在生产系统中实现 RL 的基础。要进一步理解，请参考启发本节的这两篇论文[2]和[3]。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="251f" class="np lx it bd ly nq nr ns mb nt nu nv me jz nw ka mh kc nx kd mk kf ny kg mn nz bi translated">基于对象的表示</h1><p id="c413" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">面向对象的 RL 依赖于对象及其交互来为给定的任务设计最佳策略。对象和交互形成了任务环境的表示。</p><p id="b21d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个上下文中，<strong class="lb iu"> <em class="lv">对象</em> </strong>是出现在任务环境中的任何相关实体，其特征在于一些属性集。当两个对象<strong class="lb iu"> <em class="lv">以某种方式交互</em> </strong>时，交互对象之一或两者中的一个或多个属性的值会发生变化，这被称为<strong class="lb iu"> <em class="lv">效应</em> </strong>。</p><p id="9ce7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将任务的环境表示为一组相互作用的对象可以促进先前知识的重用，并且提供重要的机会来:</p><ul class=""><li id="b937" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated"><strong class="lb iu">一般化:</strong>如果一个智能体学会了如何在一个环境中对某些类别的对象做出最佳行为，它可能会将其行为一般化到包含相同类别对象的任何环境中。假设这些对象以相同或相似的方式运行。</li><li id="3d1f" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu">更低的样本复杂度:</strong>从基于对象的世界表示中学习智能体的行为可以加快学习过程。原因可能是这种表示提供了比原始像素观察(通常在深度 RL 中进行)更多的先验知识，这可以减少无模型方法训练竞争代理所需的样本数量。</li><li id="379a" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu">可解释:</strong>对象和交互很容易被人类理解，因此调试代理的决策过程应该更容易。理想情况下，我们想知道哪些对象会影响代理的决策，所以我们可以想象代理会给我们一种“对象上的注意力向量”,让我们了解哪些对象与其决策更相关。此外，通过监视对象的属性如何随时间变化，应该可以理解代理本身如何影响其他对象。请注意，在像素级推理时，这两种分析都很难执行。</li><li id="7125" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu">确保安全:</strong>安全约束可直接应用于对象。例如，我们可以限制某些对象类的某些属性的值(例如，惩罚汽车类的速度属性)。更重要的是，甚至对象之间的交互也可以被约束。这样，我们可以，理想地，在我们选择的一些对象类中防止非法行为。</li></ul><p id="54dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，对象可以帮助减轻 RL 的一些基本问题，但实际上，在这种方法中，我们需要克服一些重要的问题，以便实现这样的框架。</p><p id="095c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们必须检测对象，以便能够对它们进行推理。这应该以一种<em class="lv">无人监督的</em>方式来完成，因为我们不想手工设计东西，也不想引入特定问题的先验知识；我们希望使用尽可能广泛适用的<strong class="lb iu">无监督对象检测/场景理解</strong>方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/00bbc31dd9de275fefe9abe52970a266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lp_usVE3rj8WGO1c35EYOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两个最近的场景理解算法，空间和 SPAIR。[ <a class="ae ky" href="https://arxiv.org/pdf/2001.02407" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="1ac8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其次，我们应该能够预测被检测对象的一些属性，以便出于安全目的最终约束它们。最后，也是更重要的，我们需要一些方法来推断它们之间的相互作用。这是最大的问题，因为不完全清楚如何在没有先验知识的情况下，以无人监督的方式估计这种相互作用。</p><p id="95be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们假设我们能够检测对象、属性并估计交互。<em class="lv">我们如何代表他们？</em>还有，<em class="lv">我们如何更新它们？</em>嗯，交互可以自然地将对象排列成类似图形的结构。因此，一种可能的方法是将两者都建模为<strong class="lb iu">图形神经网络</strong>！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/733c542cb8d4d52c490a523e6c8422e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7dj7U8SPB55cABw60HrzNQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一般来说，图网络有三种更新规则:一种用于节点，一种用于边，另一种用于全局属性集。[ <a class="ae ky" href="https://arxiv.org/pdf/1806.01261.pdf?utm_campaign=nathan.ai%20newsletter&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener ugc nofollow" target="_blank">来源</a> ]</p></figure><p id="227d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图形神经网络(或图形网络)由<em class="lv">节点</em>(即对象的属性)<em class="lv">边</em>(即对象对之间的交互)和一组可选的全局属性组成。图网络(GN)将图作为输入，并将图作为输出返回。结果输出图的结构可以不同于输入图的结构。</p><p id="b5aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GN 非常适合处理基于对象的表示，因为它们可以被构造为图形。因此，GN 可以理想地替代密集的、卷积的或递归的神经网络来表示代理的策略。</p><p id="d90e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于 OO-RL 的两个有前途的工作，(1)战略面向对象强化学习[8]和(2) COBRA [9]，展示了本节讨论的一些思想的有效性。需要进一步的研究来充分利用 RL 方法中的对象。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="f2a4" class="np lx it bd ly nq nr ns mb nt nu nv me jz nw ka mh kc nx kd mk kf ny kg mn nz bi translated">结论</h1><p id="00cc" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">有很多东西要讨论，甚至有更多的东西要发现！面向对象的 RL 是 RL 的一个相当未开发的子领域，在我看来，它应该得到研究社区更多的关注。此外，无监督计算机视觉方法的进一步改进，如对象检测和场景理解，对于 OO-RL 的成功可能是至关重要的。</p><p id="c27c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理想情况下，OO-RL 有可能帮助减轻现代深层 RL 方法的基本限制。解决这些问题将推动基于 RL 的人工智能应用向大规模采用迈进一步。</p><p id="f69a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望这篇文章对你来说足够有趣。请评论任何疑问或澄清。这几个字里面有很多概念要解包，所以一开始可能会比较混乱。下次见！</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="1983" class="np lx it bd ly nq nr ns mb nt nu nv me jz nw ka mh kc nx kd mk kf ny kg mn nz bi translated">参考资料和进一步阅读</h1><h2 id="503c" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">RL 理论</h2><ul class=""><li id="6e67" class="mu mv it lb b lc mp lf mq li og lm oh lq oi lu mz na nb nc bi translated">大卫·西尔弗 2015 年在 UCL 的强化学习课程(<a class="ae ky" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="noopener ugc nofollow" target="_blank">链接</a>)。</li><li id="f355" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">Sergey Levine 在加州大学伯克利分校的深度强化学习课程，2019 ( <a class="ae ky" href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="noopener ugc nofollow" target="_blank">链接</a>)。</li></ul><h2 id="73f7" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">引用论文</h2><p id="8aa0" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">[1]迪尤克，c .，科恩，a .，&amp;利特曼，M. L. (2008 年 7 月)。有效强化学习的面向对象表示。</p><p id="7147" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] McAllister，Rowan 等人，“自动驾驶汽车安全的具体问题:贝叶斯深度学习的优势。”2017 年国际人工智能联合会议。</p><p id="1839" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] Dulac-Arnold、Gabriel、Daniel Mankowitz 和 Todd Hester。“现实世界强化学习的挑战。”<em class="lv"> arXiv 预印本 arXiv:1904.12901 </em> (2019)。</p><p id="a2e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] Keramati，Ramtin，等人，“面向战略对象的强化学习”<em class="lv"> arXiv 预印本 arXiv:1806.00175 </em> (2018)。</p><p id="6eb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] Watters，Nicholas 等，“Cobra:通过无监督的对象发现和好奇心驱动的探索，基于数据高效模型的 rl。”<em class="lv"> arXiv 预印本 arXiv:1905.09275 </em> (2019)。</p><h2 id="02e7" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">元学习、图形网络等。</h2><ul class=""><li id="19aa" class="mu mv it lb b lc mp lf mq li og lm oh lq oi lu mz na nb nc bi translated">芬恩，切尔西，彼得·阿贝耳和谢尔盖·莱文。“用于深度网络快速适应的模型不可知元学习。”<em class="lv">第 34 届机器学习国际会议论文集——第 70 卷</em>。JMLR。org，2017。</li><li id="80cf" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">强化学习的无监督元学习。<em class="lv"> arXiv 预印本 arXiv:1806.04640 </em> (2018)。</li><li id="92f6" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">彼得·w·巴塔格利亚等人，《关系归纳偏差、深度学习和图形网络》<em class="lv"> arXiv 预印本 arXiv:1806.01261 </em> (2018)。</li><li id="e86a" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">林，等，〈空间:经由空间注意与分解的无监督物件导向场景表现〉arXiv 预印本 arXiv:2001.02407  (2020)。</li></ul></div></div>    
</body>
</html>