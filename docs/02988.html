<html>
<head>
<title>I’m out of the layers — how to make a custom TensorFlow 2 layer.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我没有图层了——如何定制TensorFlow 2图层。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/im-out-of-the-layers-how-to-make-a-custom-tensorflow-2-layer-9921942c88fc?source=collection_archive---------17-----------------------#2020-03-22">https://towardsdatascience.com/im-out-of-the-layers-how-to-make-a-custom-tensorflow-2-layer-9921942c88fc?source=collection_archive---------17-----------------------#2020-03-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f509" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">有时需要编写自己的神经网络层，接受非标准数据或以不寻常的方式处理它们。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b1942f66b9a5cd950f9c4a9faaa4d95d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lnvMtD4xOSsNhGToTzfbEA.png"/></div></div></figure><p id="2c87" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">TensorFlow 2使机器学习框架更容易使用，但仍保留了构建模型的灵活性。它的一个新特性是通过集成的Keras API构建新的层，并使用eager-execution轻松调试这个API。</p><p id="a6fe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本文中，您将学习如何在TensorFlow 2框架中构建自定义神经网络层。撰写本文时，我假设您对Python 3中的面向对象编程有基本的了解。最好是在阅读本文之前先回顾一下<code class="fe ln lo lp lq b">__init__</code>、<code class="fe ln lo lp lq b">__call__</code>、类继承和方法重写。</p><h2 id="5546" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">用于构建图层的模板</h2><p id="7e42" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">让我们从一个模板开始，基于它你将建立你的大部分层。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">新TensorFlow 2层的模板，根据需要使用它。</p></figure><p id="bb4c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如你所见，结构很简单。该模板由两个方法组成，<code class="fe ln lo lp lq b">__init__</code>和call。第一个方法是一个类的标准Python 3构造函数，我们在其中初始化所有对象和字段。因此，在层的情况下，您在这里初始化将在训练中使用的所有变量。还要记住通过调用<code class="fe ln lo lp lq b">super(YourClassName, self).__init__()</code>来调用图层类的<code class="fe ln lo lp lq b">__init__</code>方法，否则图层类的构造函数不会被初始化。方法调用是基于给定的输入和变量定义所有向前和向后的地方。你唯一要做的就是定义正向传播步骤，<strong class="kt ir">反向传播步骤由TF2自动执行。</strong></p><p id="6025" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，还有两个强制参数—一个是<code class="fe ln lo lp lq b">x_input</code>，另一个是<code class="fe ln lo lp lq b">training</code>。输入可以是任何类型，如果需要，还可以添加更多的输入参数。然而，必须至少有一个输入，否则正向和反向传播步骤是不可能的。此外，还有训练增强，我将它设置为默认的<code class="fe ln lo lp lq b">False</code>(测试期间默认没有<code class="fe ln lo lp lq b">False</code>)。它用于某些层，在测试和训练阶段的行为是不同的。比如dropout就是这样一个层，用在这里是因为它只在训练阶段丢弃神经元。</p><p id="7785" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你很好奇为什么用<code class="fe ln lo lp lq b">call</code>而不是pythonic式的<code class="fe ln lo lp lq b">__call__</code>，那么看看文章的最后。</p><h2 id="17ea" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">自定义层的示例</h2><p id="efda" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">让我们定义一个更复杂的层，这一次它将执行一个非常复杂的操作，那就是……<strong class="kt ir">两个实数的乘法</strong>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">允许两个标量数相乘的层</p></figure><p id="feb2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这一次，这些层由<code class="fe ln lo lp lq b">call</code>方法中的两个输入和一个将两个数相乘的<code class="fe ln lo lp lq b">tf.math.multiply</code>方法组成。要使用该层，您只需定义创建多层对象，并通过向其传递值来调用它，如下所示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">一个如何使用我们创建的多层的例子。</p></figure><p id="741e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">先自己试试，再往前跑。下一个更复杂，这次我不玩文字游戏了。但你会熬过去的，相信我。</p><h1 id="c7b5" class="mv ls iq bd lt mw mx my lw mz na nb lz jw nc jx mc jz nd ka mf kc ne kd mi nf bi translated">用TF2建立残差层神经网络</h1><p id="d9ae" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">让我们假设，你想要创建一个神经网络来预测几个类。为此，您将在流行的CIFAR-10数据集上训练网络，该数据集有10个不同的类。为了解决这个问题，你选择了残差神经网络(简称ResNet)，但它要求你建立自己的层，因为它们不是在TF2实现的，或者你想玩不同的架构。ResNet由剩余块构成，这些块具有跳跃连接，抵消了消失梯度问题。</p><h2 id="603d" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">ResNet块</h2><p id="6a60" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">这里有两种类型的块，一种保持输入形状(因此输出形状与输入形状相同)，另一种改变。ResNet块具有如下所示的体系结构，其中最重要的是跳过连接(有时称为快捷连接)。跳过连接只不过是复制输入并将其添加到输出的末尾，这并不神秘，但它工作得很好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/0c8b2ac512f8b012489577f5a317d01f.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*fBIRJVrjYAPA0u5I8M1RIw.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">ResNet块的一般视图</p></figure><h2 id="d1c8" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">将ResNet定义为TF2的自定义图层</h2><p id="10f7" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">让我们现在尝试建立一个剩余块。我不会根据一些架构来创建层，而是根据一些直觉。此外，我会尽量让事情简单，但它会占用更多的空间(不必要的重复)，我会预约如何做得更好。</p><p id="5069" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有了构建TF2层的基础，让我们尝试一下构建ResNet <strong class="kt ir">的新技能，在CIFAR10 </strong>基准数据集中把自己放在靠前的位置。在这里，您可以找到一个带有相应文件的排行榜:</p><div class="nh ni gp gr nj nk"><a href="https://paperswithcode.com/sota/image-classification-on-cifar-10" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">CIFAR-10排行榜|带代码的论文</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">CIFAR-10目前的最新水平是BiT-L (ResNet)。查看91篇论文与代码的完整对比。</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">paperswithcode.com</p></div></div></div></a></div><p id="5084" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们首先画出我们的标识块，我们将使用它来构建我们的自定义层(即ResNet块)。<br/>首先，我们将构建一个身份块，我称之为<code class="fe ln lo lp lq b">ResidualLayer</code>在代码中，这一层由两条路径和三个卷积块以及一条快捷路径组成(见下图)。在训练过程中，宽度通常会提高准确性[1]。随意修改路径的数量，并检查它如何影响结果的准确性，训练时间和其他参数。值得注意的是，两条路径应该有不同的超参数集，以学习如何提取不同的特征集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f81ab69f351120ee4471913ecfc7b354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*0iqEViAp_XqEM7dowyqy7A.png"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">在我们的例子中使用的剩余块，代码如下。快捷方式块表示前面提到的跳过连接。</p></figure><p id="804c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">第二个残差块不保留输入形状，因此跳过连接必须重新缩放输入。这是用卷积层完成的。其余的几乎与前面显示的剩余块相同。该层将在代码中以与上一个类似的方式调用，但使用Scal后缀，所以<code class="fe ln lo lp lq b">ResidualLayerScal</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/9a24109d11c82723c0f08a41738d12c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*19CMhHZw3mKPk837SUH2og.png"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">快捷路径上带有缩放层的剩余块。</p></figure><h2 id="ce25" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">残余块的代码</h2><p id="fca1" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">剩余块的代码在这里，用于<code class="fe ln lo lp lq b">ResidualLayer</code>和<code class="fe ln lo lp lq b">ResidualLayerScal</code>。代码有点长，所以你不能在中型网页上看到它的全部。你必须点击它，直接进入要点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">保留输入形状的剩余图层</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">不保留输入形状的剩余图层</p></figure><p id="72e5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">注意评论和<code class="fe ln lo lp lq b">training=False</code>。</p><h2 id="d37b" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">使用剩余层</h2><p id="5228" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">所有的代码看起来都很复杂，但是大部分代码都是某种模式的重复。大部分代码可以(<strong class="kt ir">甚至是</strong>)通过一个或两个for循环重复(一个用于控制深度，另一个用于控制宽度)。但是为了使代码更简单，我认为最好不要在这里使用它。</p><p id="f29b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要创建这样一个层并使用它，您只需以与<code class="fe ln lo lp lq b">MultiplyLayer</code>相同的方式创建该类的一个实例。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">请随意修改这些参数以获得更好的结果。</p></figure><h2 id="80cd" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">将其分组为块</h2><p id="a1d4" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">从“纸”入手，总比没有任何计划就匆忙投入工作要好。因此，让我们制作一个块的模式，块将由五个剩余层组成，如下图所示，并被进一步称为…猜怎么着，一个剩余块！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/7cd1635e635dbd3b8ff0b1682848f082.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*EXCu7AbVcNtlLuyUba8EvQ.png"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">一个由5个定制剩余层组成的剩余块。</p></figure><p id="b06c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这样的块中有两种类型的层，一层不保留输入形状— <code class="fe ln lo lp lq b">ResidualLayerScal</code>，四层保留— <code class="fe ln lo lp lq b">ResidualLayer</code>。定义好模块后，让我们构建一个模型并测试这些层。</p><h2 id="571d" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">建立模型</h2><p id="a0dd" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">整个模型的模式不会更复杂，它是四个剩余块的重复，在开始时具有批量标准化、激活函数和卷积层，在结束时具有密集输出的另一个批量标准化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e46775e5c4ce617dde90b0401fe85444.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*Fo0Bb6MMeJNexWpx_LU98w.png"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">整个模型的模式，看起来很简单，你不觉得吗？</p></figure><p id="3bd1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该模型将通过创建一个继承自<code class="fe ln lo lp lq b">tf.keras.Model</code>类的类来实现。姑且称模型类为<code class="fe ln lo lp lq b">FunnyResNet</code>(听起来一点都不好笑)。代码来了:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="e3d8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这个模型实现了前面展示的模式，它是如何工作的你可以在我的另一个故事中了解到。</p><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/creating-a-trashy-model-from-scratch-with-tensorflow-2-an-example-with-an-anomaly-detection-27f0d1d7bd00"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">如何从头开始创建TensorFlow 2模型——一个“无用”的例子。</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">想象一下，您想要构建一个可以在项目中使用的模型。你选择做张量流2…</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc kp nk"/></div></div></a></div><h2 id="df13" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">培训和评估</h2><p id="a539" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">定义好你的模型后，你需要做的一件事就是用<code class="fe ln lo lp lq b">model.compile()</code>定义它，用<code class="fe ln lo lp lq b">model.fit()</code>训练模型。这里展示的<strong class="kt ir">整体Resnet模型</strong>你可以在我的知识库中找到，还有图像标准化、规范化和增强。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="05fd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在编译之前将两幅图像传递给模型的目的是初始化权重形状。这是在TF2自动完成的，基于第一次出现的输入。</p><p id="9134" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这里，您可以找到一个Jupyter笔记本，上面有所有代码:</p><div class="nh ni gp gr nj nk"><a href="https://github.com/DanielWicz/FunnyResNet" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">丹尼尔维奇/搞笑网</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">TowardDataScience-DanielWicz/funny resnet上的教程示例</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">github.com</p></div></div><div class="nx l"><div class="od l nz oa ob nx oc kp nk"/></div></div></a></div><p id="aa3a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">或者直接在谷歌的colab:</p><div class="nh ni gp gr nj nk"><a href="https://colab.research.google.com/github/DanielWicz/FunnyResNet/blob/master/SimpleResNet.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">DanielWicz/FunnyResNet谷歌联合实验室</h2><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">TowardDataScience教程的resnet示例</p></div></div><div class="nx l"><div class="oe l nz oa ob nx oc kp nk"/></div></div></a></div><h2 id="88bc" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">摘要</h2><p id="6961" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">在TF2创建图层并不是一件难事，您需要记住以下几点:</p><ul class=""><li id="d880" class="of og iq kt b ku kv kx ky la oh le oi li oj lm ok ol om on bi translated">它将复杂的结构简化成一个可用的“块”，可以很容易地重复多次。</li><li id="d071" class="of og iq kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">该层必须从<code class="fe ln lo lp lq b">tf.keras.Layer</code>继承才能工作。</li><li id="acb9" class="of og iq kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">您必须定义两种方法，<code class="fe ln lo lp lq b">__init__</code>和<code class="fe ln lo lp lq b">call</code>。</li><li id="c75e" class="of og iq kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">在<code class="fe ln lo lp lq b"> __init__</code>方法中，你定义了所有将在调用方法中进一步使用的变量，如对象(如层)、常量。</li><li id="076d" class="of og iq kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">在<code class="fe ln lo lp lq b">call</code>中，您定义了应用于数据的所有操作，如分层、乘法、加法、蒙版、转置等。</li><li id="2705" class="of og iq kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">记得在<code class="fe ln lo lp lq b">call</code>中传递<code class="fe ln lo lp lq b">training</code>参数，否则，dropout和batch normalization不起作用。</li></ul><h2 id="8893" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">此外，记得使用专用的张量流函数</h2><p id="2fbe" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">总是在层中使用TF函数，否则你将不能用<code class="fe ln lo lp lq b">model.fit()</code>来编译它们或者用<code class="fe ln lo lp lq b">@tf.function</code>语句来使用它们(这意味着低得多的性能)。在TF2，你需要的几乎每一个pythonic函数都有替代品。例如，在一个层中执行一个For循环，你将使用<code class="fe ln lo lp lq b">for i in tf.range(N)</code>或者使用你使用的数组<code class="fe ln lo lp lq b">tf.TensorArray</code>。</p><h2 id="eb37" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">说明</h2><ul class=""><li id="a1a1" class="of og iq kt b ku mk kx ml la ot le ou li ov lm ok ol om on bi translated">定义了<code class="fe ln lo lp lq b">call</code>方法而不是<code class="fe ln lo lp lq b">__call__</code>，因为<code class="fe ln lo lp lq b">__call__ </code>在调用<code class="fe ln lo lp lq b">call</code>之前会调用一些额外的东西。这些东西包括初始化权重的方法，以及确保该层正常工作的所有其他东西。这是一个与<code class="fe ln lo lp lq b">Model</code>相似的案例，你可以在我的故事中读到。</li></ul><h2 id="2bb8" class="lr ls iq bd lt lu lv dn lw lx ly dp lz la ma mb mc le md me mf li mg mh mi mj bi translated">附加阅读</h2><ul class=""><li id="4f46" class="of og iq kt b ku mk kx ml la ot le ou li ov lm ok ol om on bi translated">谷歌自定义图层指南:<a class="ae ow" href="https://www.tensorflow.org/guide/keras/custom_layers_and_models" rel="noopener ugc nofollow" target="_blank">此处</a></li><li id="45ae" class="of og iq kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">TF2图层类文档:<a class="ae ow" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" rel="noopener ugc nofollow" target="_blank">此处</a></li></ul><h1 id="3e8f" class="mv ls iq bd lt mw mx my lw mz na nb lz jw nc jx mc jz nd ka mf kc ne kd mi nf bi translated">参考资料:</h1><p id="ee88" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">[1]s . Zagoruyko和n . Komodakis(2016年)。广泛的剩余网络。<em class="ox"> arXiv预印本arXiv:1605.07146 </em>。</p></div></div>    
</body>
</html>