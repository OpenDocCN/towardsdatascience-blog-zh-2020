<html>
<head>
<title>Word Embeddings vs TF-IDF: Answering COVID-19 Questions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">词嵌入vs TF-IDF:回答新冠肺炎问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-embeddings-vs-tf-idf-answering-covid-19-questions-703e3d99f783?source=collection_archive---------24-----------------------#2020-04-04">https://towardsdatascience.com/word-embeddings-vs-tf-idf-answering-covid-19-questions-703e3d99f783?source=collection_archive---------24-----------------------#2020-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1b8d71c96ea6b593fc8e4cd829efa1b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qDyigasLvApZLlqEFGpEzg.png"/></div></div></figure><h2 id="d2e2" class="kb kc it bd kd ke kf dn kg kh ki dp kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">回答新冠肺炎问题的文本相似度方法比较。</h2><p id="5270" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh kk li lj lk ko ll lm ln ks lo lp lq lr im bi translated"><strong class="kz iu">数据集:</strong> CORD-19</p><p id="421e" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">我们感兴趣的问题:</p><ul class=""><li id="c980" class="lx ly it kz b la ls le lt kk lz ko ma ks mb lr mc md me mf bi translated">潜在风险因素的数据</li><li id="08de" class="lx ly it kz b la mg le mh kk mi ko mj ks mk lr mc md me mf bi translated">吸烟、已有肺部疾病</li><li id="0b7e" class="lx ly it kz b la mg le mh kk mi ko mj ks mk lr mc md me mf bi translated">合并感染(确定同时存在的呼吸道/病毒感染是否使病毒更易传播或更具毒性)和其他共病</li><li id="c689" class="lx ly it kz b la mg le mh kk mi ko mj ks mk lr mc md me mf bi translated">新生儿和孕妇</li><li id="2c89" class="lx ly it kz b la mg le mh kk mi ko mj ks mk lr mc md me mf bi translated">社会经济和行为因素，以了解病毒的经济影响和是否有差异。</li><li id="e810" class="lx ly it kz b la mg le mh kk mi ko mj ks mk lr mc md me mf bi translated">病毒的传播动力学，包括基本繁殖数、潜伏期、序列间隔、传播方式和环境因素</li><li id="dbc7" class="lx ly it kz b la mg le mh kk mi ko mj ks mk lr mc md me mf bi translated">疾病的严重程度，包括有症状住院患者和高危患者群体的死亡风险</li><li id="9ef3" class="lx ly it kz b la mg le mh kk mi ko mj ks mk lr mc md me mf bi translated">人群易感性</li><li id="2c69" class="lx ly it kz b la mg le mh kk mi ko mj ks mk lr mc md me mf bi translated">可有效控制的公共健康缓解措施</li></ul><h1 id="90c5" class="ml kc it bd kd mm mn mo kg mp mq mr kj ms mt mu kn mv mw mx kr my mz na kv nb bi translated">预赛</h1><p id="f391" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh kk li lj lk ko ll lm ln ks lo lp lq lr im bi translated">这篇文章假设读者对NLP有一个基本的了解，比如单词袋，以及文本是如何用数字表示的。</p><h1 id="764a" class="ml kc it bd kd mm mn mo kg mp mq mr kj ms mt mu kn mv mw mx kr my mz na kv nb bi translated">方法1: TF-IDF和余弦相似度</h1><p id="70ee" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh kk li lj lk ko ll lm ln ks lo lp lq lr im bi translated">TF-IDF代表词频——逆文档频率，是信息检索任务中常用的方法[1]。我们将使用它来查找与我们的搜索问题相似的句子。</p><p id="9957" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">为此，我们需要将每个句子表示为一个向量。TF-IDF通过根据术语在文档中的流行程度对其进行加权来创建这些向量。如果一个术语出现在语料库中的几乎所有文档中(这意味着该术语是无用的，可以忽略)，公式的IDF部分确保它在句子向量中获得非常低的权重。如果一个术语在识别一个句子中很重要(这意味着它不会出现在许多其他句子中)，它将获得很高的权重。</p><p id="24f9" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">如果我们使用TF-IDF对以下三个句子的语料库进行矢量化，请尝试猜测哪些术语将获得高权重，哪些术语将获得较低权重。</p><p id="ac6c" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">句子1:平均潜伏期</p><p id="6a79" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">句子2:平均风险期</p><p id="67fd" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">第三句:传播的风险</p><p id="138e" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">因为单词<em class="nc"/>出现在所有句子中，所以它相当不重要，并且将获得低权重，而术语<em class="nc">孵化</em>和<em class="nc">传输</em>将获得非常高的权重，因为它们分别在识别句子1和2中很重要。</p><p id="c662" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">理想情况下，您应该使用二元模型来捕获像“孵化期”这样的术语，但是为了简单起见，让我们坚持使用单元模型。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nd"><img src="../Images/98e1267dc203de04a44ab52524e0e49b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8LdzWnlmrUuYXNBNXyFhUA.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">Unigrams的TF-IDF矩阵</p></figure><p id="2250" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">我在这里没有计算任何东西，所以我写了高，中，低，而不是实际的数字。如果你对TF-IDF不是太熟悉，我建议你手工计算矩阵，但如果你是，请继续阅读。</p><p id="22db" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">现在，我们想找到关于“潜伏期”的句子如果我们使用已经构建的TF-IDF矩阵对这个搜索查询进行矢量化，我们将得到这个搜索查询的向量:</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/196084a5ea51b5b74901f635a5bfe332.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yvWBNqJPrifKIHnU8_iDWw.png"/></div></div></figure><p id="4e79" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">请注意，这个向量与句子1中的向量非常相似。如果我们计算搜索查询向量与句子向量的接近度，我们会发现它最接近句子1。我们将使用余弦相似度来计算这种接近度，因为它比正常的欧几里德距离更适合文本数据。</p><p id="fafd" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">你可能已经意识到我们的TF-DF方法存在一个主要缺点。同一概念在不同的论文中可能有不同的措辞。例子:术语<em class="nc">婴儿</em>和<em class="nc">青少年</em>意思相同，但是如果我们搜索“婴儿的风险因素”，我们会错过将婴儿称为青少年的句子。这就是单词嵌入的用武之地…</p><h1 id="dd75" class="ml kc it bd kd mm mn mo kg mp mq mr kj ms mt mu kn mv mw mx kr my mz na kv nb bi translated">方法2:单词嵌入和单词移动距离</h1><p id="6006" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh kk li lj lk ko ll lm ln ks lo lp lq lr im bi translated">TF-IDF向量不考虑语言中的语义相似性。<em class="nc">青少年</em>的体重与<em class="nc">婴儿</em>的体重无关。单词嵌入试图通过依赖一个基本思想来捕捉这些关系:出现在相同上下文中的单词具有相似的含义。我们将使用一个名为word2vec的单词嵌入框架，它学习语料库中单词的向量表示[2]。这里有一个例子:</p><p id="26ac" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">在冠状病毒爆发后，默克尔在德国实施了封锁。</p><p id="3454" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">第二句:新冠肺炎疫情爆发后，德国总理实施了封锁。</p><p id="1348" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">我们注意到默克尔和总理这两个词可以互换使用；冠状病毒和新冠肺炎也是如此。因此，这两个词应该有某种关系。</p><blockquote class="nn"><p id="74f7" class="no np it bd nq nr ns nt nu nv nw lr dk translated">单词嵌入不是每个句子都有向量，而是通过为每个单词创建<strong class="ak">向量来捕获这些语义关系。在向量空间中，冠状病毒等词的向量与新冠肺炎等类似词的向量非常接近，而与咖啡或纸张等不相关的词则相距较远。</strong></p></blockquote><p id="e56a" class="pw-post-body-paragraph kx ky it kz b la nx lc ld le ny lg lh kk nz lj lk ko oa lm ln ks ob lp lq lr im bi translated">在没有标签或手工工作的情况下构建这样一个表示似乎是不可思议的，但是当你理解这个过程时就变得显而易见了。这些向量是使用浅层(1个隐藏层)神经网络构建的，以预测目标词(如“默克尔”)，给定一些上下文词(如“强制封锁德国”)。这里有一个非常简单的例子，使用了我们之前的句子:</p><p id="2864" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">训练一个神经网络来预测给定上下文单词的单词。当它试图预测<em class="nc">封锁</em>给<em class="nc">默克尔的时候，</em>隐藏的单位会为<em class="nc">默克尔</em>获得一些权重。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/99a920c9a17c2f49607f71781269e339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9pSS_Lo_c7fywxFAi9X2EA.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">x =默克尔，y =封锁</p></figure><p id="ae54" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">类似地，当从<em class="nc">负责人</em>预测<em class="nc">锁定</em>时，隐藏单元将获得一些对应于<em class="nc">负责人</em>的权重，这将有助于预测<em class="nc">锁定</em>。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/667e2a529877aae9eec90517eb263119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WlMam7gST4CGVq5NiQDQ8A.png"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">x =议长，y =封锁</p></figure><p id="5a83" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">仔细想想，s <strong class="kz iu">因为激活锁定</strong>的输出单元需要一定的权重组合，所以预测<em class="nc">锁定</em>的任何单词都应该得到类似的权重。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/c760ffffc5a53b727c987c589f22780d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LlMqHb2RmTWONkQWeGsmRg.png"/></div></div></figure><p id="6673" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">T4冠状病毒和新冠肺炎<em class="nc">也是如此。在预测<em class="nc">爆发和</em>爆发时，这两个术语应该得到相似的权重，因为<em class="nc">爆发</em>出现在这两个术语的上下文中。</em></p><blockquote class="nn"><p id="d56a" class="no np it bd nq nr ns nt nu nv nw lr dk translated">训练好的网络的隐含层中的权重成为单词的向量，隐含层神经元的数量成为维数。</p></blockquote><p id="dc04" class="pw-post-body-paragraph kx ky it kz b la nx lc ld le ny lg lh kk nz lj lk ko oa lm ln ks ob lp lq lr im bi translated">在上面的图像中，请注意隐藏层中有两个神经元，因此，单词向量是二维的。</p><p id="09fc" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">这里有一个包含四个句子的可视化示例:</p><figure class="ne nf ng nh gt ju"><div class="bz fp l di"><div class="of og l"/></div></figure><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/7a2534330d7b74e5b323ae076fbdc9ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qtZwRshVCkbr9_HvFe9apg.jpeg"/></div></div></figure><p id="9aa4" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">该模型能够学习语义关系，例如<em class="nc">默克尔</em>应该更接近<em class="nc">总理</em>而不是其他词，例如<em class="nc">巴黎</em>。</p><p id="746e" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">在这些例子中，我创建了二维嵌入。然而在现实情况下，通常有100-300维的向量。</p><p id="dcc0" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">正如您可能已经猜到的，这个过程需要大量的训练数据来学习准确的嵌入，所以在线上有预先训练的单词嵌入，您可以开箱即用。</p><p id="9b53" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">在我们的CORD-19数据上训练word2vec之后，这里是<strong class="kz iu"> <em class="nc">婴儿的嵌入和类似单词。</em> </strong>它们是100维的，但我使用了降维技术来将它们可视化。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/6725d377b4bbad40aa0d37c6b36661e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xyukU8hc0I175pO-m1-_Q.jpeg"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">来自CORD-19数据的单词嵌入</p></figure><p id="f079" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">那么现在我们有了每个单词的向量，我们如何衡量两个句子之间的相似性呢？</p><p id="b412" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated"><strong class="kz iu">单词移动器的距离(WMD) </strong>通过定位两个句子在单词嵌入空间中的位置并将一个句子转换成另一个句子来完成这项任务[3]。为了计算句子A和B之间的相似度，它将A中的每个单词“驱动”到B中最近的单词，直到A被转换成B，反之亦然。大规模杀伤性武器是平均“旅行的距离”的话。</p><p id="4f8d" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">第一句:“默克尔实施封锁”</p><p id="3547" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">第二句:“议长下令封锁”</p><p id="0789" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">第三句:“巴黎实施封锁”</p><p id="6d59" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated"><em class="nc">默克尔，第一句中的</em>，行驶0.0724“公里”到达第二句中的<em class="nc">总理</em>；<em class="nc">强制</em>和<em class="nc">锁定</em>行进0公里，因为它们已经出现在句子2中。因此，WMD为(0.0727 + 0 + 0) = 0.0242</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oj"><img src="../Images/3825552803c117ea18f52f5ae211b6e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXP9EoIJti7gr5uVB7_c3A.jpeg"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">第1句和第2句之间的WMD</p></figure><p id="cadf" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">这里，<em class="nc">默克尔</em>行驶0.6312公里到达<em class="nc">巴黎</em>，所以WMD = (0.6312 + 0 + 0) / 3 = 0.2104</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/2899f110bf34fcba3b2c8e89f3db9d80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gko-L8mzCjv8bxAKZ9z0Qg.jpeg"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">第1句和第3句之间的WMD</p></figure><figure class="ne nf ng nh gt ju"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="15cb" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">所以在单词嵌入的帮助下，我们能够确定句子1和2比句子1和3更相似。如果没有单词嵌入，句子1将同样类似于句子2和3。</p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="b4d3" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">现在我们已经了解了理解这些方法的必要基础，让我们尝试一下这些方法，看看我们是否能够检索到与我们的搜索查询有一些相似性的文本。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi os"><img src="../Images/46b1c147866268d181bbe628c047d12f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WmHAwiwIurPpJun9Wb6_bw.png"/></div></div></figure><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ot"><img src="../Images/b233779586e5a2db25876dd5e8d9d820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7rpYqlQc3che1zT20q6J_A.png"/></div></div></figure><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ou"><img src="../Images/c72185a76647245bb26a8f85160a329d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XMWe07IJfNA61Qje9EBXGQ.png"/></div></div></figure><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ov"><img src="../Images/016d65f39a402358b0c0bb03c23754ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SAwcY9GWF0xpLi7JO_06XQ.png"/></div></div></figure><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ow"><img src="../Images/7fc41395d58bde4bc05bab06f3c2fdd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1yFU0ktweKFJYSx7nwaV8Q.png"/></div></div></figure><p id="8e2a" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">我们做到了！两种方法都获得了相当精确的答案。可以通过使用更多的训练数据或使用预训练的嵌入来改进单词嵌入方法。</p><h1 id="9c28" class="ml kc it bd kd mm mn mo kg mp mq mr kj ms mt mu kn mv mw mx kr my mz na kv nb bi translated">离别的思绪</h1><p id="191e" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh kk li lj lk ko ll lm ln ks lo lp lq lr im bi translated">我们讨论了计算两个文本之间相似性的两种方法，并使用它们来检索与新冠肺炎相关的问题的答案。尽管WMD由于其高时间复杂度而不适用于这种类型的任务，但它仍然是一种强有力的方法，并且在许多领域具有巨大的适用性。我可以看到它在检测剽窃方面非常有效。如果一个句子被复制粘贴，几个单词被同义词替换，word2vec和WMD可以很容易地发现它，而TF-IDF就不行了。另一种替代WMD的方法是用平滑的逆频率对句子的单词向量进行平均，并计算余弦相似度。然而，我们经常不能理解像TF-IDF这样的简单方法的力量，正如我们在这里看到的，它们不应该被忽视。</p><p id="1342" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated"><strong class="kz iu"> <em class="nc">编者按:</em> </strong> <em class="nc"> </em> <a class="ae ox" href="http://towardsdatascience.com/" rel="noopener" target="_blank"> <em class="nc">走向数据科学</em> </a> <em class="nc">是一份以数据科学和机器学习研究为主的中型刊物。我们不是健康专家或流行病学家，本文的观点不应被解释为专业建议。想了解更多关于疫情冠状病毒的信息，可以点击</em> <a class="ae ox" href="https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports" rel="noopener ugc nofollow" target="_blank"> <em class="nc">这里</em> </a> <em class="nc">。</em></p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="f6a3" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">这个项目的代码可在笔记本这里<a class="ae ox" href="https://github.com/tchanda90/covid19-textmining" rel="noopener ugc nofollow" target="_blank">https://github.com/tchanda90/covid19-textmining</a></p><p id="76d4" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">你可以在这里使用搜索工具<a class="ae ox" href="https://tchanda90.github.io/covid19-textmining/" rel="noopener ugc nofollow" target="_blank">https://tchanda90.github.io/covid19-textmining/</a></p><p id="3804" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">[1]拉莫斯，J. (2003年，12月)。使用tf-idf确定文档查询中的单词相关性。在<em class="nc">第一届机器学习教学会议论文集</em>(第242卷，第133-142页)。</p><p id="ab3d" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">[2]t .米科洛夫，陈，k .，科拉多，g .，&amp;迪安，J. (2013年)。向量空间中单词表示的有效估计。<em class="nc"> arXiv预印本arXiv:1301.3781 </em>。</p><p id="b90e" class="pw-post-body-paragraph kx ky it kz b la ls lc ld le lt lg lh kk lu lj lk ko lv lm ln ks lw lp lq lr im bi translated">[3]m .库斯纳、孙、n .科尔金和k .温伯格(2015年6月)。从单词嵌入到文档距离。在<em class="nc">机器学习国际会议</em>(第957–966页)。</p></div></div>    
</body>
</html>