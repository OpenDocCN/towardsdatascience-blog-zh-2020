<html>
<head>
<title>Introduction and Implementation of Adagradient &amp; RMSprop</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Adagradient &amp; RMSprop的介绍与实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-and-implementation-of-adagradient-rmsprop-fad64fe4991?source=collection_archive---------28-----------------------#2020-01-17">https://towardsdatascience.com/introduction-and-implementation-of-adagradient-rmsprop-fad64fe4991?source=collection_archive---------28-----------------------#2020-01-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9cb2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">不同维度上的自适应学习率</h2></div><p id="addc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上一篇<a class="ae le" rel="noopener" target="_blank" href="/stochastic-gradient-descent-momentum-explanation-8548a1cd264e">文章</a>中，我们介绍了随机梯度下降和动量项，SGD在传统梯度下降中增加了一些随机性，动量项有助于加速这一过程。然而，这两种方法都设定了固定的学习率:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/d8ca09c243a9b90e87248a2116f798ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*ipSZ3tdCjXA2EX_WDo9G6Q.png"/></div></figure><p id="b81f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面显示了带有动量项的梯度下降，其中<code class="fe ln lo lp lq b">lr</code>实际上对于不同维度上的所有参数都是固定的。对于频繁出现的参数，它可以很好地工作，这些参数也可以通过迭代频繁更新，但是对于与不频繁出现的特征相关联的参数，它将导致更新不足以达到最优。</p><blockquote class="lr ls lt"><p id="d16f" class="ki kj lu kk b kl km ju kn ko kp jx kq lv ks kt ku lw kw kx ky lx la lb lc ld im bi translated">与不经常出现的特征相关联的参数仅在这些特征出现时接收有意义的更新。给定一个递减的学习速率，我们可能会在这样一种情况下结束，其中共同特征的参数相当快地收敛到它们的最优值，而对于不频繁的特征，我们仍然不能在它们的最优值能够被确定之前足够频繁地观察它们。换句话说，对于频繁出现的特征，学习速率降低得太慢，或者对于不频繁出现的特征，学习速率降低得太慢。</p></blockquote><h1 id="d9ee" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">Adagradient</h1><p id="f71e" class="pw-post-body-paragraph ki kj it kk b kl mq ju kn ko mr jx kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">Adagradient是为解决上述问题而开发的，它根据每个参数的梯度和频率以不同的速度更新不同的参数。让我们看看这个公式:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/88755cebfd50ff28e2cdfa6fa1b256c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*IEV7xFXCqcu3uM5vi0QoXQ.png"/></div></figure><p id="b4cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可能已经注意到，关键的区别是这里添加了<code class="fe ln lo lp lq b">s_t</code>术语。对于不常见的特征或梯度较小的特征，它们的<code class="fe ln lo lp lq b">s_t</code>会很小，但<code class="fe ln lo lp lq b">lr/sqrt(s_t + ϵ)</code>会很大，这导致更新的步长较大。这为不同的参数给出了不同的更新速度，解决了统一学习率带来的弊端。</p><p id="ee17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们实现它并将其应用到一个实际问题中。</p><p id="4030" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们要解决的问题和我们之前说过的一样，</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/faf9549f14f0624cc0cb776aa5de541c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*eKyXTvcDvwtZjNy-Ls3gQg.png"/></div></div></figure><p id="acb3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中我们试图找到<code class="fe ln lo lp lq b">a, b</code>的最佳值，以最小化<code class="fe ln lo lp lq b">y</code>和<code class="fe ln lo lp lq b">f(x)</code>之间的差异损失，并且上面计算了<code class="fe ln lo lp lq b">a, b</code>的梯度。adagradient的实现将是:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="6966" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe ln lo lp lq b">X, Y</code>的输入值由下式得出:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="4ec2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实现应该是直接的，我们将<code class="fe ln lo lp lq b">s_a, s_b</code>初始化为0，为了绘制学习过程，我将中间值保存在列表中。</p><p id="bd1b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参数更新过程如下:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/aaf903e23e7e92d7255ba57af3c89a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*hDGFMSuzRV27sTfEdusITQ.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">参数更新</p></figure><p id="d63a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">学习率是这样的:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/adc379afa4b4fb89ba9e51ec00c839f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*BFkCP5meirRHQPHbAIjP7A.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">学习率更新</p></figure><p id="b85e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里的<code class="fe ln lo lp lq b">lr/sqrt(s + ϵ)</code>被认为是修正的学习率。</p><h1 id="a5de" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">RMSprop</h1><p id="934d" class="pw-post-body-paragraph ki kj it kk b kl mq ju kn ko mr jx kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">adagrad的一个问题是，随着梯度随着时间的推移而累积，项<code class="fe ln lo lp lq b">s</code>可能会变得无限长，这可能会导致学习步骤的急剧减少，并导致参数最终几乎不变。</p><p id="054d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RMSprop旨在通过简单地对术语进行标准化来解决这一问题:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/29a616d93c96796cca10e652343652a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*K11SmZK5VdBM1kC0erG9wA.png"/></div></figure><p id="ae09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意这里过去的梯度<code class="fe ln lo lp lq b">s_{t-1}</code>被一个额外的参数<code class="fe ln lo lp lq b">γ</code>所限制，其中<code class="fe ln lo lp lq b">γ</code>通常取值0.9。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="0335" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除了<code class="fe ln lo lp lq b">s</code>的更新，实现与adagrad类似。更新过程将是这样的:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/4131c7cac84453eff36189cf21039764.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*apVNRw71fzcpjWPvAJpTdA.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">参数更新</p></figure><p id="5b35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">学习率是这样的:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d3b20a383ba67efe3df774d7bf6afc0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*IOLDBY4HYy1FLleO4MCrGQ.png"/></div><p class="ne nf gj gh gi ng nh bd b be z dk translated">学习率更新</p></figure><p id="4461" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里我们看到<code class="fe ln lo lp lq b">a, b</code>的学习率是振荡的，而不是单调下降的(你可以在这里检查<a class="ae le" href="https://github.com/MJeremy2017/Machine-Learning-Models/tree/master/Optimisation" rel="noopener ugc nofollow" target="_blank">的实现)。</a></p><p id="3dfe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，我们已经学习了几种在优化过程中应用的技术。接下来，让我们将它们集合起来，衍生出另一个强大的工具— <a class="ae le" href="https://medium.com/@zhangyue9306/optimisation-algorithm-adaptive-moment-estimation-adam-92144d75e232" rel="noopener"> Adam </a>。</p><p id="0cf8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">参考</strong>:</p><ul class=""><li id="e8aa" class="nl nm it kk b kl km ko kp kr nn kv no kz np ld nq nr ns nt bi translated"><a class="ae le" href="http://d2l.ai/chapter_optimization/rmsprop.html" rel="noopener ugc nofollow" target="_blank">http://d2l.ai/chapter_optimization/rmsprop.html</a></li><li id="8241" class="nl nm it kk b kl nu ko nv kr nw kv nx kz ny ld nq nr ns nt bi translated"><a class="ae le" href="https://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent" rel="noopener ugc nofollow" target="_blank">https://ruder . io/optimizing-gradient-descent/index . html # stochasticgradientdescence</a></li></ul></div></div>    
</body>
</html>