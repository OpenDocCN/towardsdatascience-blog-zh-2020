<html>
<head>
<title>Simple Linear Regression explanation and implementation from scratch with Python &amp; Numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python和Numpy从头开始简单的线性回归解释和实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-linear-regression-explanation-and-implementation-from-scratch-with-python-26325ca5de1a?source=collection_archive---------12-----------------------#2020-04-01">https://towardsdatascience.com/simple-linear-regression-explanation-and-implementation-from-scratch-with-python-26325ca5de1a?source=collection_archive---------12-----------------------#2020-04-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1e45" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">你是否经常使用线性回归，但却不知道它的本质是什么？这篇文章将帮助你理解这个重要的概念！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a1ec26e8e0904f6373146af2feb3abb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-cPsDXcBBmveRTLwlqCbXA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源<a class="ae ky" href="https://pixabay.com/users/free-photos-242387/" rel="noopener ugc nofollow" target="_blank">免费照片</a>，通过<a class="ae ky" href="https://pixabay.com/photos/forest-mist-nature-trees-mystic-931706/" rel="noopener ugc nofollow" target="_blank"> pinterest </a> (CC0)</p></figure><p id="9326" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们开始研究机器学习时，大多数时候我们并不真正理解那些算法在引擎盖下是如何工作的，它们通常对我们来说就像是黑盒。今天，我想介绍一下回归任务中最基本也是最著名的算法之一——线性回归。在本文中，我将从头开始构建并解释Python中的线性回归模型，我们还将看到它在从<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html" rel="noopener ugc nofollow" target="_blank">sk learn . datasets . make _ Regression</a>方法生成的回归数据集上的性能，与Sklearns的从<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank"><em class="lv">sk learn . Linear _ Regression生成的线性回归相比。线性回归</em> </a>类。那么，我们可以开始了吗？</p><p id="33f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">用于线性模型创建的数据集</strong></p><p id="7132" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一个简单的例子中，假设我们只有一个变量— <em class="lv"> X. </em>基于对<em class="lv"> X </em>的了解，我想预测未知变量<em class="lv"> Y. </em>我还将定义<strong class="lb iu">训练集</strong>，它包含所有已知的(<em class="lv"> X_train，Y_train </em>)变量对和<strong class="lb iu">验证集</strong>，我仅从中取<em class="lv"> X_val </em>然后，我们可以在知道<em class="lv"> Y_pred </em>和<em class="lv"> Y_val </em>变量的情况下测量模型的性能。让我们在Python代码中定义训练和测试集:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/9c0e7d21280330e78a4cb788be240bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSy_6pGw1yxq90FAEfx7iQ.png"/></div></div></figure><p id="70a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我导入了<em class="lv"> make_regression </em>方法，并创建了包含1个特征(即<em class="lv"> X </em>和目标(即<em class="lv"> Y. </em>)的数据集。现在，让我们将数据集拆分为<strong class="lb iu">训练</strong>和<strong class="lb iu">验证</strong>部分。如上所述，这可以通过<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank">sk learn . model _ selection . train _ test _ split</a>方法来完成。如上所述，我将在<strong class="lb iu">训练</strong>零件上训练线性回归模型，并在<strong class="lb iu">验证</strong>零件上评估模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/a82f0cbdec82a10731beac14dea37fe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rcAKF85O2qFpzqO54F1BPQ.png"/></div></div></figure><p id="6b09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如图所示，我将70%的数据作为<strong class="lb iu">训练</strong>数据，另外30%用于<strong class="lb iu">验证</strong>。现在，为了更好地理解我们的数据，我将绘制图片，显示我们的<em class="lv"> X_train </em>和<em class="lv"> Y_train </em>变量之间的关系。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/7e3733fa501fe9a8569a309507531fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*VL2kL-Bbwn63Du05l-sd6A.png"/></div></figure><p id="b48e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，总而言之，我们的目标是建立一个模型，该模型将<em class="lv"> X </em>作为输入，并预测相应的<em class="lv"> Y </em>变量。让我们把它重写为函数:模型(<em class="lv"> X </em> ) = <em class="lv"> Y. </em>考虑一个输入<em class="lv"> X = -2，</em>，那么模型<em class="lv"> (-2) </em>将返回接近<em class="lv"> -200 </em>的值，如果输入<em class="lv"> X =2 </em>，那么<em class="lv">模型(2) </em>将返回接近<em class="lv"> 100 </em>的值。总而言之，我将在1个特性案例中实现的模型如下所示:</p><p id="bdf2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">型号(X) = W1*X + W0 </em></p><p id="2c5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> W1 </em>和<em class="lv"> W0 </em>为参数，将由模型在训练中学习。你可以看到，上面的函数只是直线的方程，因此，我们的线性模型的预测将形成直线，看起来像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lz"><img src="../Images/c8d36f542e4a96c5f1cc9a8a1927209e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bYPtmI1AbWout7RFoFgkVw.png"/></div></div></figure><p id="1b0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出现在图表上的线是最适合<strong class="lb iu">系列</strong>组的线。现在，当我准备好数据集后，让我们了解线性回归模型背后的直觉。</p><p id="a4f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">线性回归理论和直觉</strong></p><p id="8a59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我在上面解释的，主要思想是拟合一条线(在1个特征的情况下)或一个超平面(在K个特征的情况下)，因此在1个特征的情况下，模型看起来像这样:</p><p id="3afd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">型号(X1) = W1*X1 + W0 </em></p><p id="a2cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在K特性的情况下，看起来是这样的:</p><p id="d482" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">型号(X1…Xk)= Wk * Xk+Wk-1 * Xk-1+…+W1 * X1+W0</em></p><p id="0f27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，在这两种情况下，我们都有<em class="lv"> W0 </em>项，称为偏差。为了有效地训练模型，我们也可以将其重写为:</p><p id="b84d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">模型(X0，X1…Xk)= Wk * Xk+Wn-1 * Xn-1+…+W1 * X1+W0 * X0，其中X0 = 1 </em></p><p id="6b60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，在1个特征的情况下，如果<em class="lv"> W0 = 0 </em>，那么W1变量的任何选择将仅导致线围绕(0，0)点旋转。W0项有助于移动直线或超平面，这<strong class="lb iu">通常</strong>导致更好的模型性能。</p><p id="08b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，接下来要了解的是如何选择这个<em class="lv"> W0…Wk </em>术语。为此，你需要理解什么是<a class="ae ky" href="https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0" rel="noopener ugc nofollow" target="_blank">损失函数</a>以及梯度下降背后的想法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ma mb l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.youtube.com/watch?v=sDv4f4s2SB8" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=sDv4f4s2SB8</a></p></figure><p id="85dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一般来说，<em class="lv">损失函数</em>显示模型的当前预测值(<em class="lv">模型(X) </em>)与真实的<em class="lv"> Y </em>值有多接近。使用<em class="lv">梯度下降</em>,我们以这样的方式进行小步骤，即我们的<em class="lv">损失函数</em>减少(即，在训练模型W1…Wk的每一步，参数以这样的方式变化，即在一些步骤之后，我们的损失函数稳定在局部最小值。我的意思是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/323db14d37c9f8d39c216be4ab7c2466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GCmQgzCouSWA7biB.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">https://stack overflow . com/questions/56794716/periodic-oscillating-loss-function-for-py torch-ccnn</p></figure><p id="e59f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第一步，损失是高的(即预测和Y变量彼此远离)。每走一步，损失就会减少，预测值会变得更接近真实值。</p><p id="8570" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们为我们的任务定义损失函数，我们将使用MSE作为损失函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi md"><img src="../Images/f1ee8f8fb32f1945fa686e55d89fe43c.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*IQOuZyFhtYAxMak4XTg18A.png"/></div></figure><p id="5e74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，让我们计算损失对Wj的导数，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/e7543b1d1c2825f289c190810318845e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*aq-8ET6OD2OcheRJcGZ_tg.png"/></div></figure><p id="6379" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总而言之，以下是您在训练线性回归模型时要做的事情:</p><ol class=""><li id="35b8" class="mf mg it lb b lc ld lf lg li mh lm mi lq mj lu mk ml mm mn bi translated">你随机初始化模型的<em class="lv"> W0…Wk </em>变量。</li></ol><p id="89ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于一定数量的迭代(步骤数)，您需要执行以下操作:</p><p id="c261" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.根据以下规则更新<em class="lv"> W0…Wk </em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/24b089259e21dd5154bb5ccb1815cb54.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*LuspHRm3LgOQMiOO8SUx-g.png"/></div></figure><p id="c6cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<em class="lv"> dL/dWj </em>是损失函数相对于<em class="lv"> Wj </em>的导数。(上面我们已经定义了梯度的公式)，这里的learning_rate只是模型的超参数，你要在训练前自己选择(我一般是从learning_rate 0.001开始)</p><p id="bea8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嗯，基本上这就是你实现所需要的所有理论，我建议你重读几遍这部分，如果你没有得到什么，这完全没问题！现在我将用Python实现一个简单的线性回归模型类。</p><p id="3065" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">用Python实现线性回归模型</strong></p><p id="0854" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们定义我们类的主要功能:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/8a041ee0bad364dea1b66b3dd088bf6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*JhZ8LNNX4P3EEQsM7S9y2w.png"/></div></figure><p id="7b2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我用两种方法创建了SimpleLinearRegression类:<strong class="lb iu"> fit </strong>和<strong class="lb iu"> predict </strong> <em class="lv">。</em>我们的模型将学习<strong class="lb iu">拟合</strong>方法中最佳拟合线的参数，我们将使用<strong class="lb iu">预测</strong>方法来预测未知的<em class="lv"> X </em>变量。您还可以看到，类的构造函数(<strong class="lb iu"> __init__ </strong>)有两个变量(我们将在创建类实例时定义它们)，它们是:<em class="lv"> learning_rate </em>和<em class="lv"> n_steps </em>。我已经在<strong class="lb iu">直觉</strong>部分解释了这两个变量。</p><p id="7871" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们实施<strong class="lb iu">拟合</strong>方法，如<strong class="lb iu">插入</strong>部分所述。我们还需要一种方法来计算每个W的损失梯度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/4dab8b6b6d8b0b83319338fcee6832fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*pTRKBeNNVIOhrIsfsg0i9Q.png"/></div></figure><p id="2342" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该函数是梯度计算函数的矢量化实现，但同时适用于所有权重:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b60e0d57f6c5acb5bb4e088d8cb79a88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*XtB2DvQ3BHemKfp7ocJqug.png"/></div></figure><p id="2f1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中K—参数总数，W0是偏置项。</p><p id="10ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在有了<strong class="lb iu">梯度</strong>函数，让我们实现拟合方法，以如下方式迭代更新<em class="lv"> W0…Wk </em>参数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/af6fd6f83d48a9b387bd003565cc4d7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*2vbH-HVU_uyhltnt8LNZ1g.png"/></div></figure><p id="4225" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> fit </strong>函数现在应该是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/ccb2995271ea1e33cc94deaf2243ec5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*nE2Oou7Gkb3lr9Ighixp0A.png"/></div></figure><p id="67fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解该功能的每个部分至关重要。首先，我们添加如上所述的偏差项，np.c_在这里的作用如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/e33e1214dbba1cfd2e8c23aae225c222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U6r18B-V5uI5aGbRSXmWuA.png"/></div></div></figure><p id="e223" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的每一行代表一个单独的点，每一列是一个单独的特征。因此，对于每一行(点)，我添加一个新的偏差特征。之后，我随机初始化权重并运行一个循环，在每一步我都更新所有的权重。这就是fit函数的全部功能！我们流程的最后一步是实现<strong class="lb iu">预测</strong>功能，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/dcf7cca888145d7be57c47b1325b813b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-w4Hjje1lAI6h54kpgVTxA.png"/></div></div></figure><p id="c1bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您在这里看到的，我还添加了偏差项，并转移到该函数的矢量化实现x^T * w。</p><p id="fa52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实现如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/a044c64c5cd1ccb9235c55881afc5702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fUIxEZX1SyT1XlwoF3ePCg.png"/></div></div></figure><p id="bac1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是我们简单线性回归模型的全部实现！现在让我们在之前准备好的数据集上测试它。</p><p id="1cd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">简单线性回归模型测试和评估</strong></p><p id="982a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为第一步，我在本文第一部分定义的训练数据集上拟合模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/0320de22b93447374bd67678c40f06dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*3DWYzMhlxRykuBJyftt75A.png"/></div></figure><p id="c91a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们的模型已经学习了<em class="lv"> W1，W0 </em>参数的最佳值(在我们的例子中是1个参数和偏置项)。现在我们可以调用predict方法，来获得对<em class="lv"> X_val </em>点的预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/6d542dd6b49463ceda4e62000f8a3de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*co3utg-QuW2QYpWQHA3ajg.png"/></div></figure><p id="3ecb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们用MSE函数(我们用作损失函数的那个)来衡量预测质量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi md"><img src="../Images/f1ee8f8fb32f1945fa686e55d89fe43c.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*IQOuZyFhtYAxMak4XTg18A.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/3232f15da43d73a2118644b9ddc8845b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*WvSvKja-wms45FJQUb-CQg.png"/></div></figure><p id="3b68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们也做一个对(X_val，Y_val)和(X_val，Y_pred)的散点图</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/222858be839623b0bfd430efbd15462a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*OBtZ9zE2HK6omoqIdagaxg.png"/></div></figure><p id="d339" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，我们的模型已经学习了最佳拟合线(橙色的那条),它很好地描述了给定的一组点。</p><p id="4929" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我将对sklearn.linear_model执行相同的步骤。LinearRegression模型类，做个比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/667b2697ed575dc13d81bc2117939e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vCNpwSuuSTjzSoeqElc4tw.png"/></div></div></figure><p id="920d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，sklearns实现的MSE误差要低一些，这是因为他们的模型比我们的要复杂一些。</p><p id="512e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结论</strong></p><p id="0a40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总而言之，今天我们已经完成了一项非常出色的工作——用Python从头开始实现了一个简单而有效的线性回归算法。我希望您能够轻松理解本文的每一点，如果需要的话，您可以自己实现相同的功能。我已经将这篇文章中的Jupyter笔记本完整加载到github中，所以请不要犹豫使用它并从中学习。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc mb l"/></div></figure><p id="aa57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">你可以在我的</strong> <a class="ae ky" href="http://artkulakov.com" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">网站</strong> </a>上查看其他帖子</p></div></div>    
</body>
</html>