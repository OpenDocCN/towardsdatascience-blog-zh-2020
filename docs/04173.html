<html>
<head>
<title>Work smarter, not harder when building Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建神经网络时，更聪明地工作，而不是更努力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/work-smarter-not-harder-when-building-neural-networks-6f4aa7c5ee61?source=collection_archive---------16-----------------------#2020-04-16">https://towardsdatascience.com/work-smarter-not-harder-when-building-neural-networks-6f4aa7c5ee61?source=collection_archive---------16-----------------------#2020-04-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="67db" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用一个简单的例子来说明神经网络的设计原则</h2></div><p id="c76b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">应用数学中的一个基本技巧是找到一个坐标变换，将一个困难或不可能的问题转化为一个简单的问题。也许我最喜欢的例子是圆的方程。如果我们在笛卡尔坐标中写下单位圆的方程，我们可以将圆的几何概念表达为隐函数:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/4c1610a31d284c52e682eeb3372bf736.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*HsNPotkf4UN8FjaaQCVMsQ.png"/></div></figure><p id="9e68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们试图通过求解y得到一个显式表达式时，情况会变得更糟。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/8e14f0308a68c50662cb41972292bee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*-h_kBb24q-XtoMxgANnELA.png"/></div></figure><p id="3397" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们必须从平方根的两个分支(圆的上半部分和下半部分)拼凑这个简单的函数。超越美学使用这种圆的表示法会使我们看不到一个完美的圆所固有的几何简单性。相比之下，如果我们用极坐标来表示同样的数学对象，那么单位圆就变得非常容易处理。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi ln"><img src="../Images/bfb72460d2809127a919cc3250893abc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RFTVViU3YqOjuUVpkvdNBA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">点的极坐标。</p></figure><p id="a361" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在极坐标中，我们的单位圆采用简单的形式r=1，θ ∈ [0，2π]。所以在极坐标中，圆是矩形的(高=1，宽=2π)。笛卡尔坐标是矩形的自然坐标，极坐标是圆形的自然坐标。在本文中，我将应用这种思路来构建简单的神经网络，并构建一个说明选择正确坐标的重要性的示例。</p><h1 id="84fc" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">曲线拟合的人工神经网络</h1><p id="b6a5" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">如果你正在阅读这篇文章，那么你可能听说过人工神经网络(ANN)。在这次讨论中，我们将集中讨论最简单的人工神经网络，称为前馈网络。简而言之，人工神经网络只是通过将简单的非线性函数(层)链接在一起而构建的网络(函数)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mt"><img src="../Images/bdc01728e2365fc708b84b421949fc27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eTpz1W1H_ctehggdP1KDeQ.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">前馈神经网络</p></figure><p id="5c85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过将(许多)这些简单的层链接在一起，通用逼近定理告诉我们，我们可以将任何(好的)函数表示为具有有限深度和宽度的人工神经网络。在开始的极坐标例子中，这就像说我们可以在极坐标中写下(x，y)平面中的每一点(没有洞)。这是一个很好的必要属性，可以确保我们在改变坐标时不会错过任何东西，但是它没有告诉我们两件事:</p><ul class=""><li id="c14e" class="mu mv it kk b kl km ko kp kr mw kv mx kz my ld mz na nb nc bi translated">如何找到实际坐标？</li><li id="822d" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld mz na nb nc bi translated">如果这是表达信息的好方法。</li></ul><p id="6529" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于人工神经网络来说，“<em class="ni">坐标</em>是神经网络的参数(神经网络中每一层的权重和偏差)。对于神经网络，我们没有快速的数学公式来找到这些坐标，而是使用优化来最小化损失函数。损失函数测量我们指定的函数和训练数据之间的距离。</p><p id="c696" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个简单的例子，用神经网络来构建正弦函数的近似值。</p><h2 id="3752" class="nj lx it bd ly nk nl dn mc nm nn dp mg kr no np mi kv nq nr mk kz ns nt mm nu bi translated">使用神经网络拟合正弦函数</h2><p id="07d3" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">让我们应用这种强大的函数逼近方法来构建简单函数sin(x)的神经网络版本。我将使用用Julia 编写的奇妙的神经网络库<a class="ae nv" href="https://github.com/FluxML/Flux.jl" rel="noopener ugc nofollow" target="_blank"> Flux。这个软件包为构建神经网络提供了一个非常简单而强大的框架。它还增加了很少的额外语法来构建神经网络，并让我们专注于基本的构建模块。我将在文章中包含一些代码片段。完整代码请查看</a><a class="ae nv" href="https://github.com/khannay/CurveFittingNeuralNets" rel="noopener ugc nofollow" target="_blank">github repo</a>。</p><p id="5c34" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的代码使用一个标准的非线性函数tanh构建了一个具有两个隐藏层的简单网络。</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="38f1" class="nj lx it nx b gy ob oc l od oe"><br/>ann = Chain(Dense(1,20,tanh),Dense(20,20,tanh),Dense(20,1));<br/></span></pre><p id="242e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以把这个神经网络想象成:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi of"><img src="../Images/10c900a87ce413efff2ad694e412407a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1gE1GbY6Cvk2YjVGX6cNkA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">学习正弦函数的简单前馈神经网络</p></figure><p id="d33a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上图显示我们有一个输入和输出值，中间有两个层。这些层被称为隐藏层，因为如果您仅跟踪输入和输出，它们是不可见的。回想一下，该图用于表示某个函数族，其中通过固定参数来指定特定的成员。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi og"><img src="../Images/6a3a735cb2db9644e158c3dae36fcc2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hck2cwjkFisyTvaXMZG73A.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">上述神经网络的函数形式。参数由两个权重矩阵W和偏置向量b给出。参数C给出线性输出层的截距。</p></figure><p id="110f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好，所以我们希望，我们可以用这个函数族中的某个成员来近似表示函数sin(x)。为了尝试找到最接近这个的成员，我们应该使用一些训练数据来最小化损失函数。</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="8d2f" class="nj lx it nx b gy ob oc l od oe">function loss(x, y)<br/>    pred=ann(x)<br/>    loss=Flux.mse(ann(x), y)<br/>    #loss+=0.1*sum(l1,params(ann)) #l1 reg<br/>    return loss<br/>end</span><span id="7ba3" class="nj lx it nx b gy oh oc l od oe"><a class="ae nv" href="http://twitter.com/epochs" rel="noopener ugc nofollow" target="_blank">@epochs</a> 3000 Flux.train!(loss,params(ann), data, ADAM())</span></pre><p id="4e11" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">拟合我们的神经网络后，我们可以看到它对训练数据和一组测试数据有多好。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/e940e35759840cd421761cce0d02f6d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Jdm6lfKfHuHzV5nYLkyf7w.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">具有tanh非线性的前馈神经网络适合3000个时期的训练数据，如蓝色圆圈所示。测试数据显示为绿色十字。</p></figure><p id="459f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">显然，这种神经网络在捕捉信号的基本周期特性方面表现不佳。对测试数据(绿色)的异常差的概括突出了这一点。这不仅仅是我的<a class="ae nv" rel="noopener" target="_blank" href="/can-machine-learn-the-concept-of-sine-4047dced3f11">网络</a>。</p><p id="7dc3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">哪里出了问题？经常被引用的<a class="ae nv" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">通用逼近定理</a>告诉我们，我们可以使用有限深度和宽度的前馈神经网络来表示这个函数。然而，正如我经常重新发现的——有限仍然可以非常大。仅仅因为存在一些权重和偏差，可以用来从典型的神经网络组件中构建这个函数，并不意味着我们可以很容易地找到它们。</p><p id="4d0c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以尝试使用蛮力，建立更大的网络或训练更长的时间，等等。然而，一个非常简单的改变会在很短的时间内给我们一个近乎完美的近似值。</p><h2 id="a09e" class="nj lx it bd ly nk nl dn mc nm nn dp mg kr no np mi kv nq nr mk kz ns nt mm nu bi translated">更好的主意</h2><p id="1286" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">给定这个数据，我们可以看到它是周期性的。现在，我们还没有将那条信息包含在我们的神经网络设计中。不使用双曲正切非线性函数，<em class="ni">让我们使用正弦函数</em>。这样，第一层的输出将是周期性的。</p><p id="585c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图显示了该系统更好的近似网络的想法。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi oj"><img src="../Images/623e0b0222f42e2aa67c5ff9013d4cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bOK--3mal9VoXH12prljbw.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">具有正弦非线性神经网络。</p></figure><p id="a1ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以函数形式写出，我们使用的函数族采用以下形式:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi ok"><img src="../Images/e35fbc8794bb2618e36431283d6d1597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5s5WDNdhzRlSRp54c5fDHQ.png"/></div></div></figure><p id="be7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中Q是隐藏层中神经元的数量。这在Flux中很容易做到:</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="d60b" class="nj lx it nx b gy ob oc l od oe">Q = 20;<br/>ann = Chain(Dense(1,Q,sin),Dense(Q,1));</span></pre><p id="882b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们试试这种新型号，看看能否找到更好的型号。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/b1939a7d3e53088c300a1a9503abcd87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*VtGjCq6OdnrSkG-ptbIC1Q.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">更好的神经网络基础</p></figure><p id="235a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个更好的模型，因为它抓住了周期性的本质。然而，请注意，我们仍然在训练和测试数据上略有偏差。这是因为我们的模型实际上仍然有一些不需要的额外自由度。我们可以通过对参数应用一些正则化来解决这个问题，但在这种情况下，我们可以求助于一些非常酷的数学来寻求解决方案。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/de8e01fb21e3edb3aa59052504e77a77.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/0*kibydoMBPTcDgZs2.jpg"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated"><strong class="bd om">让·巴普蒂斯特·约瑟夫·傅立叶1768–1830</strong></p></figure><p id="ea03" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述神经网络的函数形式非常接近于<strong class="kk iu">傅立叶级数</strong>的形式。傅立叶级数是一个基(一组坐标)，可以用来描述有限域[a，b]上所有的<em class="ni">好的</em>函数。如果一个函数是周期的，那么有限域上的描述可以扩展到整条实直线上。因此，傅里叶级数常用于周期函数。</p><p id="2771" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">傅立叶级数采用以下形式:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi on"><img src="../Images/9f708f2660d823df437a6d236c548537.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FK6safUqaW_tYi4lWxBkA.png"/></div></div></figure><p id="6989" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的神经网络和傅立叶级数之间的唯一区别是，我们的神经网络允许权重W变化，而傅立叶只有A系数和偏差(b)项，可以选择它们来拟合函数。</p><p id="b369" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在Flux library中，从可训练的参数集中移除第一层中的那些权重参数是足够容易的。通过将它们设置为整数值，我们实际上可以创建一个神经网络，其中<strong class="kk iu">是</strong>傅立叶级数。</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="a488" class="nj lx it nx b gy ob oc l od oe"># Fourier Series ANN</span><span id="8a0d" class="nj lx it nx b gy oh oc l od oe">Q = 20;<br/>ann = Chain(Dense(1,Q,sin),Dense(Q,1));</span><span id="70c9" class="nj lx it nx b gy oh oc l od oe">function loss(x, y)<br/>    pred=ann(x)<br/>    loss=Flux.mse(ann(x), y)<br/>    return loss<br/>end</span><span id="2f5c" class="nj lx it nx b gy oh oc l od oe">opt = ADAM()</span><span id="27a1" class="nj lx it nx b gy oh oc l od oe">ps=params(ann)</span><span id="cc41" class="nj lx it nx b gy oh oc l od oe">for j=1:20<br/>    ann[1].W[j]=j<br/>end<br/>delete!(ps, ann[1].W) #Make the first layers weights fixed</span><span id="30b1" class="nj lx it nx b gy oh oc l od oe"><a class="ae nv" href="http://twitter.com/epochs" rel="noopener ugc nofollow" target="_blank">@epochs</a> 3000 Flux.train!(loss,ps, data, opt)</span></pre><p id="822f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是使用sin(x)函数和傅立叶人工神经网络的拟合:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/f7d1d07854f46ddad10e7ac23d3bccf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*RX78xY47FosXGY8f8swxuw.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">傅立叶人工神经网络方法推广良好。</p></figure><p id="4d25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想知道这种方法对其他更复杂的周期函数也同样适用。考虑一个混有一些高次谐波的周期信号。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/4b798d4701b77a9d9fb41d8412744e97.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/1*gJ8HerG1GGnS2iUes3PwDA.gif"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7f2b00f845f59f5545d04f945c4bbf80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*niD5tsvR0ODOVankCa9bcQ.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">更复杂周期信号的傅立叶级数神经网络的第二个例子。</p></figure><h2 id="5517" class="nj lx it bd ly nk nl dn mc nm nn dp mg kr no np mi kv nq nr mk kz ns nt mm nu bi translated">一些外卖课程</h2><p id="512d" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">这个简单的例子为ANN的工作方式以及我们如何改进我们的模型提供了一些重要的见解。</p><ol class=""><li id="ce3c" class="mu mv it kk b kl km ko kp kr mw kv mx kz my ld op na nb nc bi translated">你的神经网络的设计会对结果产生巨大的影响。问题和领域的具体知识会产生巨大的影响。</li><li id="fae4" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld op na nb nc bi translated">确保你所知道的关于问题的一切都被传送到神经网络。这里我们看到，告诉函数产生一个周期信号比增加网络规模或训练时间做得更多。</li><li id="9f82" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld op na nb nc bi translated">按照上一课的思路，领域知识很重要。</li><li id="48f9" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld op na nb nc bi translated">应用数学家和物理学家已经研究函数逼近问题很长时间了。<a class="ae nv" href="https://en.wikipedia.org/wiki/Fourier_series" rel="noopener ugc nofollow" target="_blank">傅立叶级数在19世纪20年代被发现</a>用来解决热传导问题。这些技术可能值得您花时间去学习，即使您并不关心应用程序。</li><li id="9da7" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld op na nb nc bi translated">用人工神经网络计算傅立叶级数是非常愚蠢的。<a class="ae nv" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiRqv7B2-3oAhUESa0KHQZnDYMQFjABegQICxAF&amp;url=http%3A%2F%2Fpi.math.cornell.edu%2F~ajt%2Fpresentations%2FTopTenAlgorithms.pdf&amp;usg=AOvVaw2_lak1K8xxoCtcAZLEb8mm" rel="noopener ugc nofollow" target="_blank">上世纪十大算法之一</a>就是为了这种计算而发明的。重点是展示一个小小的改变，加入更多的领域知识，是如何极大地改善结果的。</li></ol><p id="507e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于构建更智能的神经网络，我推荐查看<a class="ae nv" href="https://faculty.washington.edu/kutz/" rel="noopener ugc nofollow" target="_blank">内森·库兹的作品</a>。</p></div></div>    
</body>
</html>