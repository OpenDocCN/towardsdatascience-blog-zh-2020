<html>
<head>
<title>Introduction to Extreme Learning Machines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">极限学习机简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-extreme-learning-machines-c020020ff82b?source=collection_archive---------5-----------------------#2020-05-29">https://towardsdatascience.com/introduction-to-extreme-learning-machines-c020020ff82b?source=collection_archive---------5-----------------------#2020-05-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="49c0" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">理解ML</h2><div class=""/><div class=""><h2 id="fd3a" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">不要急着介绍什么是榆树。真的是创新还是只是迭代？</h2></div><h1 id="52a6" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">榆树是什么？</h1><p id="93e6" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">ELM(极限学习机)是前馈神经网络。由<em class="mc"> G .黄</em>于2006年“发明”。</p><p id="040f" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">正如原<a class="ae mi" href="https://www.ntu.edu.sg/home/egbhuang/pdf/ELM-NC-2006.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中所说:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/700d6d3f510176ad4d831f40541ca25b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*P2EV4o0OCXPgzi5Bo0H4Qw.png"/></div></figure><p id="006b" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">因此在<strong class="li ja"> E </strong> LM中出现了“极端”这个短语(但是这个名字的真正原因可能会因来源而异)。</p><h1 id="6ffb" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">为什么ELM不同于标准神经网络</h1><p id="8ed3" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">ELM不需要基于梯度的反向传播来工作。它使用<a class="ae mi" href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse" rel="noopener ugc nofollow" target="_blank">摩尔-彭罗斯广义逆</a>来设置其权重。</p><p id="df09" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">首先，我们看标准SLFN(单隐层前馈神经网络):</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/d3b17d85a7f3271e62e78287d8e50193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*as-6xyKmq7bbU8aNdCgQdw.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated"><em class="mw">单隐层前馈神经网络，来源:丁下CC BY 3.0 </em></p></figure><p id="b68e" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这很简单:</p><ol class=""><li id="b56d" class="mx my iq li b lj md lm me lp mz lt na lx nb mb nc nd ne nf bi translated">将输入乘以权重</li><li id="37a3" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">添加偏差</li><li id="e92a" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">应用激活功能</li><li id="ad1d" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">重复步骤1-3的层数次</li><li id="ee9b" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">计算产量</li><li id="f47f" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">反向传播</li><li id="2682" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">重复一切</li></ol><p id="49be" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">ELM删除了第4步(因为它总是SLFN)，用矩阵求逆代替第6步，并且只做一次，所以第7步也消失了。</p><h1 id="15f3" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">更多细节</h1><p id="55fd" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在进入细节之前，我们需要看看ELM输出是如何计算的:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/55f571b5f7f604283d5c9a34c487ca96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*u134RlsFigcaJYRbFhxqoA.png"/></div></figure><p id="7a0b" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">其中:</p><ul class=""><li id="61a5" class="mx my iq li b lj md lm me lp mz lt na lx nb mb nm nd ne nf bi translated">l是隐藏单元的数量</li><li id="cf88" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated">n是训练样本的数量</li><li id="d8ef" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated">是隐藏层和输出之间的权重向量</li><li id="b3da" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated">w是输入层和隐藏层之间的权重向量</li><li id="d0d9" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated">g是激活函数</li><li id="f47c" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated">b是通孔向量</li><li id="9a87" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated">输入向量中的x</li></ul><p id="e017" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这与标准神经网络中的反向传播非常相似，但如果你仔细观察，你会发现我们将隐藏层和输出之间的权重命名为Beta。这个β矩阵是一个特殊的矩阵，因为它是我们的伪逆矩阵。我们可以缩短等式，写成:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/47633a6728d21e38c89591b9cc09d3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*F8N898sOhlwypPmCEwnh-A.png"/></div></figure><p id="e2ed" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">其中:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b8f24ef4781b48b66958cf01512a7cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*MHSGBh4ELxcu7HYtaHKUHQ.png"/></div></figure><p id="2f2e" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">其中:</p><ul class=""><li id="1342" class="mx my iq li b lj md lm me lp mz lt na lx nb mb nm nd ne nf bi translated">m是输出的数量</li><li id="47bf" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated"><strong class="li ja"> H </strong>称为<strong class="li ja">隐层输出矩阵</strong></li><li id="1f2c" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated">t是训练数据目标矩阵</li></ul><h1 id="1e92" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">学习背后的理论(如果你愿意，可以跳过这一部分)</h1><p id="2230" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">现在我们必须挖掘迪普网络背后的理论，以决定下一步该做什么。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c42f784f1047a9ffe940741b59401f85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*LZyBDdmYCoL2N9gVKeZL7g.png"/></div></figure><p id="5fe5" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果一个函数是<a class="ae mi" href="https://en.wikipedia.org/wiki/Smoothness" rel="noopener ugc nofollow" target="_blank">光滑函数</a>，那么它就是<em class="mc">无限可微的</em></p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nq"><img src="../Images/a6aee401f8df5d33e4ec95d938db4f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*Laa1YQ7FYR9msmr4CMer4g.png"/></div></div></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/532d94917024c9bc0339470aa24d7657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*1QrYi0BSGxLPZQOe9uOrBQ.png"/></div></figure><p id="2ba0" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">我不打算证明这些定理，但是如果你感兴趣，请参考ELM-NC-2006 第3页的进一步解释。</p><p id="8ae4" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">现在我们要做的是定义我们的成本函数。基于我们对四层前馈神经网络的<em class="mc">能力的假设:四层对三层</em>我们可以看到，如果输入权重和隐藏层偏差可以随机选择，则SLFN是线性系统。</p><p id="f51b" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">因为我们ELM是一个线性系统，所以我们可以创建优化目标:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/a173f6e40090af27d72acabf13d325f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*gizBxo2KzHvWf_LmLHXl6Q.png"/></div></figure><p id="a5a5" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">为了近似求解，我们需要再次使用Rao和Mitra的工作:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1f9bf05f7c888dedde36b804b8832d9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*prxM-oMR3HWTFohw7d5McQ.png"/></div></figure><p id="0a16" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">现在我们可以算出，因为H是可逆的，我们可以计算βhat为:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ebd6820ebff2269b00501df129f4aaf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/format:webp/1*LLUrOnyP420Kvl4i2dGyGA.png"/></div></figure><h1 id="d5b9" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">学习算法</h1><p id="7cfa" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在经历了一些困难的数学之后，我们现在可以定义学习算法了。算法本身相对简单:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ed5dd064b2ccb7da42ebfdf0fa733ddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*NMqYafOuSR63_5o1tRoT4w.png"/></div></figure><p id="b27b" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果你对python的实现感兴趣，请查看这个资源库:</p><p id="f280" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">https://github.com/burnpiro/elm-pure<a class="ae mi" href="https://github.com/burnpiro/elm-pure" rel="noopener ugc nofollow" target="_blank"/></p><p id="8d0f" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">下面是该模型在MNIST数据集上工作的预览:</p><p id="f075" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated"><a class="ae mi" href="https://github.com/burnpiro/elm-pure/blob/master/ELM%20example.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/burn piro/ELM-pure/blob/master/ELM % 20 example . ipynb</a></p><p id="24cc" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">正如你所看到的，一个简单版本的ELM在MNIST数据集上达到了91%的准确率，在英特尔i7 7820 x T12 T13 CPU上训练网络需要大约3秒。</p><h1 id="d4db" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">性能比较</h1><p id="c598" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在本节中，我将使用原始论文中的指标，与之前的MNIST示例相比，您可能会惊讶于一些训练持续了多长时间，但请记住原始论文发表于2006年，网络是在<strong class="li ja"> <em class="mc">奔腾4 1.9GHz </em> </strong> CPU上训练的。</p><h2 id="93ed" class="nz kp iq bd kq oa ob dn ku oc od dp ky lp oe of la lt og oh lc lx oi oj le iw bi translated">数据集</h2><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/c49f03bf24921e0d0136870a6bd52a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*mmpFCMgizQaOkropqsz_1w.png"/></div></figure><h1 id="abdb" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">结果</h1><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/bfb5a32589c6f5fa32f38e797302a509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*0j4d7QGk0XeV2sNL8KhUOQ.png"/></div></figure><p id="b29f" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">我们现在可以忽略训练时间，因为梯度下降显然比矩阵求逆需要更长的时间。该结果表中最重要的信息是<strong class="li ja">精度</strong>和<strong class="li ja">节点</strong>。在前两个数据集中，您可以看到作者使用不同大小的BP来获得与ELM相同的结果。第一种情况下BP网络的规模比第二种情况下小<strong class="li ja"> 5x </strong>和<strong class="li ja"> 2x </strong>。这会影响测试时间(运行100个节点的神经网络比运行500个节点的神经网络更快)。这告诉我们我们的方法在逼近数据集时有多精确。</p><p id="b5b9" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">很难在流行的数据集上找到任何ELM网络的测试，但我已经设法做到了。这里有一个关于<strong class="li ja"> CIFAR-10 </strong>和<strong class="li ja"> MNIST </strong>的基准测试</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi om"><img src="../Images/28d72e2f75fbcdfa4befd6f6cda55aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*30ukw6OKBcQ0s8nNcqfA2Q.png"/></div></figure><p id="e68d" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">其中:</p><ul class=""><li id="38a9" class="mx my iq li b lj md lm me lp mz lt na lx nb mb nm nd ne nf bi translated"><strong class="li ja"> DELM </strong>是一棵深榆树</li><li id="f4fc" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated"><strong class="li ja"> ReNet </strong>在<a class="ae mi" href="https://arxiv.org/pdf/1505.00393.pdf" rel="noopener ugc nofollow" target="_blank">文中有描述</a></li><li id="801f" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated"><strong class="li ja"> RNN </strong>是一个递归神经网络</li><li id="91be" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated"><strong class="li ja"> EfficientNet </strong>在<a class="ae mi" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文中有所描述</a></li></ul><p id="e3e4" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">我没有找到ELM的训练时间，所以没有办法将它们与其他网络的结果进行比较，但所有这些乘数(<strong class="li ja"> 20x </strong>、<strong class="li ja"> 30x </strong>)都是基于CIFAR-10 上<strong class="li ja"> ELM 1000的训练时间的相对差异。如果在<strong class="li ja"> ELM 1000 </strong>和<strong class="li ja"> ELM 3500 </strong>之间有30倍的时间增长，那么你可以想象训练拥有15000个神经元的DELM需要多长时间。</strong></p><h1 id="f08d" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">结论</h1><p id="bd51" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">ELMs不如传统的神经网络精确，但在处理需要对网络进行实时再训练的问题时，可以使用ELMs。我马上要写另一篇文章描述榆树的演变和用法。现在，由你来创造一个关于这些网络的观点。</p><p id="c220" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">榆树背后有很多争议，我不是评判的最佳人选。我将把你转到带有描述的维基百科页面<a class="ae mi" href="https://en.wikipedia.org/wiki/Extreme_learning_machine#Controversy" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="2196" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">参考资料:</h1><ul class=""><li id="0c4e" class="mx my iq li b lj lk lm ln lp on lt oo lx op mb nm nd ne nf bi translated">光-黄斌，秦-朱钰，徐志敬。极限学习机:理论与应用，2006年<a class="ae mi" href="https://www.ntu.edu.sg/home/egbhuang/pdf/ELM-NC-2006.pdf" rel="noopener ugc nofollow" target="_blank">https://www.ntu.edu.sg/home/egbhuang/pdf/ELM-NC-2006.pdf</a></li><li id="1092" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated">罗正荣，米特拉，矩阵的广义逆及其应用，韦利，纽约，1971。</li><li id="1c15" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated">D.《矩阵:理论与应用》，施普林格，纽约，2002年。</li><li id="39f5" class="mx my iq li b lj ng lm nh lp ni lt nj lx nk mb nm nd ne nf bi translated">南Tamura，M. Tateishi，四层前馈神经网络的能力:四层对三层，IEEE Trans。神经网络8(2)(1997)251–255。</li></ul></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><p id="55bc" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated"><em class="mc">原载于</em><a class="ae mi" href="https://erdem.pl/2020/05/introduction-to-extreme-learning-machines" rel="noopener ugc nofollow" target="_blank"><em class="mc">https://erdem . pl</em></a><em class="mc">。</em></p></div></div>    
</body>
</html>