<html>
<head>
<title>An introduction to Variational Auto Encoders (VAEs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变分自动编码器介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-variational-auto-encoders-vaes-803ddfb623df?source=collection_archive---------25-----------------------#2020-06-17">https://towardsdatascience.com/an-introduction-to-variational-auto-encoders-vaes-803ddfb623df?source=collection_archive---------25-----------------------#2020-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f6a1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用PyTorch从理论到实践理解变分自动编码器</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6716e6cee163fe2d2a938ae887963061.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nFlWccqyFYpJjWvv5tUEqQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae kv" href="https://joanielemercier.com/" rel="noopener ugc nofollow" target="_blank">https://joanielemercier.com/</a>的艺术作品(纽约艾雅法拉火山，2010年5月onedotzero委托)</p></figure><p id="7d13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">VAE是潜在变量模型[1，2]。这种模型依赖于这样的思想，即由模型生成的数据可以通过一些变量来参数化，这些变量将生成给定数据点的一些特定特征。这些变量被称为潜在变量。</p><p id="0c36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">VAE背后的一个关键思想是，我们不是试图显式地构建一个潜在空间(潜在变量的空间)并从中进行采样，以便找到可以实际生成适当输出(尽可能接近我们的分布)的样本，而是构建一个类似编码器-解码器的网络，该网络分为两部分:</p><ul class=""><li id="72a3" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">编码器学习生成依赖于输入样本X的分布，从中我们可以采样很可能生成X个样本的潜在变量。换句话说，我们学习一组参数θ1，它们产生一个分布Q(X，θ1 ),从中我们可以采样一个潜在变量z，使P(X|z)最大化。</li><li id="7b77" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">给定潜在变量z作为输入，解码器部分学习生成属于真实数据分布的输出。换句话说，我们学习一组参数θ2，该参数生成函数f(z，θ2 ),该函数将我们学习的潜在分布映射到数据集的真实数据分布。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mg"><img src="../Images/0a417bf76b314909cfcd1a94020fdf11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nLLC3nrS42oCdzz4hRDmGA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">可变自动编码器全局架构</p></figure><p id="666c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了理解变分自动编码器背后的数学，我们将通过理论，看看为什么这些模型比旧的方法更好。</p><h2 id="4e1c" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">本文将涵盖以下内容</h2><ul class=""><li id="5ac2" class="ls lt iq ky b kz na lc nb lf nc lj nd ln ne lr lx ly lz ma bi translated">如何定义建筑的潜在空间</li><li id="8794" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如何从潜在空间采样中高效地生成数据？</li><li id="2b8d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">VAEs的最终架构</li><li id="3f00" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">一些显示VAEs有趣性质的实验</li></ul></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="ea74" class="nm mi iq bd mj nn no np mm nq nr ns mp jw nt jx ms jz nu ka mv kc nv kd my nw bi translated">1.潜在变量模型</h1><p id="0a2d" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nx lh li lj ny ll lm ln nz lp lq lr ij bi translated">潜在变量模型来源于这样一种想法，即模型生成的数据需要通过<em class="oa">潜在</em>变量来参数化。这个名字来源于这样一个事实:给定的只是模型产生的一个数据点，我们不一定知道潜在变量的哪些设置产生了这个数据点。</p><p id="c03e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在一个更正式的设置中，我们在高维空间Z中有一个潜在变量的向量<em class="oa"> z </em>，我们可以根据Z上定义的某个概率密度函数<em class="oa"> P </em> ( <em class="oa"> z </em>)轻松地对其进行采样。然后，我们有一族确定性函数<em class="oa">f</em>(<em class="oa">Z</em>；<em class="oa"> θ </em>，由某空间θ中的一个向量<em class="oa"> θ </em>参数化，其中<em class="oa">f</em>:Z×θ→x .<em class="oa">f</em>是确定性的，但如果<em class="oa"> z </em>是随机的，<em class="oa"> θ </em>是固定的，那么<em class="oa">f</em>(<em class="oa">Z</em>；<em class="oa"> θ </em>是空间X中的随机变量。</p><p id="0c22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练过程中，我们对<em class="oa"> θ </em>进行了优化，使得我们可以从<em class="oa"> P </em> ( <em class="oa"> z </em>)中采样<em class="oa"> z </em>，并且有很大概率得到<em class="oa">f</em>(<em class="oa">z</em>)；<em class="oa"> θ </em>与数据集中的<em class="oa"> X </em>一样近。为了实现这一点，我们需要找到参数<em class="oa"> θ </em>，使得:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/3096b42894a906ba7ce93148f4ed53c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1hQWI-QQJPq59o0mObQCOA.png"/></div></div></figure><p id="5a16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们只是把<em class="oa">f</em>(<em class="oa">z</em>)；<em class="oa"> θ </em>乘一个分配<em class="oa">P</em>(<em class="oa">X</em>|<em class="oa">z</em>；<em class="oa"> θ </em>为了利用全概率定律使<em class="oa"> X </em>对<em class="oa"> z </em>的依赖关系显式。我们做的另一个假设是假设P(W | z；<em class="oa"> θ </em>)遵循高斯分布N(X |<em class="oa">f</em>(<em class="oa">z</em>；<em class="oa"> θ </em>)，σ*I)(这样做我们认为生成的数据几乎为X但不完全为X)。</p><h2 id="f308" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">定义潜在空间</h2><p id="4dd6" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nx lh li lj ny ll lm ln nz lp lq lr ij bi translated">正如开始所解释的，潜在空间被认为是对影响我们数据分布的某些特定特征的变量空间进行建模。我们可以想象，如果我们考虑的数据集由汽车组成，那么我们的数据分布就是所有可能的汽车的空间，我们潜在向量的一些分量会影响汽车的颜色、方向或门的数量。</p><p id="1e9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，很快就很难明确定义每个潜在组件的角色，尤其是当我们处理数百个维度时。除此之外，一些组件可以依赖于其他组件，这使得手工设计这个潜在空间变得更加复杂。换句话说，定义这个复杂的分布P(z)真的很难。</p><h2 id="caf1" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">解决办法</h2><p id="a02b" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nx lh li lj ny ll lm ln nz lp lq lr ij bi translated">为了克服这个问题，诀窍是利用概率分布的数学性质和神经网络的能力，在具有反向传播的某些约束下学习某些确定性函数。</p><p id="c0b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使问题更容易处理的数学特性是:</p><blockquote class="oc od oe"><p id="b532" class="kw kx oa ky b kz la jr lb lc ld ju le of lg lh li og lk ll lm oh lo lp lq lr ij bi translated">通过取一组正态分布的d变量，并通过一个足够复杂的函数对它们进行映射，可以生成d维中的任何分布。</p></blockquote><p id="7efd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们可以任意决定我们的潜在变量为高斯型，然后构建一个确定性函数，将我们的高斯潜在空间映射到复杂的分布中，我们将从该分布中采样以生成我们的数据。</p><p id="2e84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将我们简单的潜在分布映射到代表我们复杂潜在空间的更复杂的潜在分布所需的确定性函数可以使用神经网络来构建，该神经网络具有一些可以在训练期间微调的参数。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="d887" class="nm mi iq bd mj nn no np mm nq nr ns mp jw nt jx ms jz nu ka mv kc nv kd my nw bi translated">2.学会从潜在空间中产生数据</h1><p id="f92d" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nx lh li lj ny ll lm ln nz lp lq lr ij bi translated">在进入本文有趣的部分之前，让我们回忆一下我们的最终目标:</p><p id="22c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们有一个正态分布的d维潜在空间，我们想学习一个函数f(z；θ2)，这将把我们的潜在分布映射到我们的真实数据分布。换句话说，我们希望对潜在变量进行采样，然后使用该潜在变量作为生成器的输入，以便生成尽可能接近真实数据点的数据样本。</p><p id="f94c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">我们仍然需要解决两件事:</strong></p><ul class=""><li id="c727" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">我们如何有效地探索我们的潜在空间，以便发现使概率P(X|z)最大化的z？(我们需要在训练中为给定的X找到正确的z)</li><li id="9218" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">我们如何使用反向传播来训练这整个过程？(我们需要找到一个优化f的目标，将P(z)映射到P(X))</li></ul><h2 id="c9ec" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">为我们的X数据样本找到正确的z潜变量</h2><p id="a627" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nx lh li lj ny ll lm ln nz lp lq lr ij bi translated">实际上，对于大多数的<em class="oa"> z </em>，<em class="oa"> P </em> (X|z)几乎为零，因此对我们估计的<em class="oa"> P </em> (X)几乎没有贡献。变分自动编码器背后的关键思想是试图对可能产生X的z值进行采样，并根据这些值计算P(X)。为了做到这一点，我们需要一个新的函数<em class="oa"> Q </em> (z|X ),它可以取值为X，并给出一个可能产生X的z值的分布。希望可能在Q下的z值的空间将比可能在前一个P(z)下的所有z的空间小得多。</p><p id="5828" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">VAE的这一部分将是编码器，并且我们将假设Q将在训练期间由神经网络学习，该神经网络将输入X映射到输出Q(z|X ),这将是我们最有可能从中找到好的z以生成该特定X的分布</p><h2 id="4855" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">用反向传播训练模型</h2><p id="97f9" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nx lh li lj ny ll lm ln nz lp lq lr ij bi translated">为了理解如何训练我们的VAE，我们首先需要定义什么应该是目标，而要做到这一点，我们首先需要做一点数学。</p><p id="f058" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们从编码器开始，我们希望Q(z|X)尽可能接近P(X|z)。为了测量两个分布有多接近，我们可以使用两个分布之间的Kullback-Leibler散度D:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/b5d7d238a3386416a170ba495db503c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zepP53OnKeHbcEC1j4p8yQ.png"/></div></div></figure><p id="2960" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用一点数学知识，我们可以用一种更有趣的方式改写这个等式。</p><p id="4e81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过对P(z|X)应用贝叶斯规则，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/d8b48ac4c262d9b195038e00cb00c481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*68oDoQTs_1WcGyV-ulrA1g.png"/></div></div></figure><p id="df93" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这相当于:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/efb50a9a8d3558b5df4812676b3499ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ULfMZV9N9wV5wRg7p3FFA.png"/></div></div></figure><p id="f8b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">让我们花点时间看看这个公式</strong></p><ul class=""><li id="c772" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">A部分</strong>:左边的项对我们的反向传播设置并不感兴趣(我们不知道P(X)的简单表达式)，但log(P(X))实际上是给定z后我们想要最大化的，我们可以看到，我们可以通过最小化右边的部分来实现这一点(使Q(z|X)尽可能接近P(z|X))。这正是我们一开始提到的。</li><li id="109b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">B部分</strong>:这个术语更有趣，因为我们知道P(X|z)(它是我们的解码器部分- &gt;生成器)和Q(z|X)(它是我们的编码器)。我们可以看到，为了最大化该项，我们需要最大化log(P(X|z))，这意味着我们不想最大化概率的对数似然性，并最小化Q(z|X)和P(z)之间的KL散度。</li></ul><p id="49b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了使<strong class="ky ir">部分B </strong>更容易计算，假设Q(z|X)是高斯分布N(z|mu(X，θ1)，sigma(X，θ1))，其中θ1是我们的神经网络从我们的数据集中学习的参数。</p><p id="1f9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的公式还有一个问题不清楚:我们如何在反向传播过程中计算期望值？</p><p id="f322" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">处理期望操作符</strong></p><p id="2640" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一种方法是进行多次前向传递，以便能够计算对数(P(X|z))的期望，但是这在计算上是低效的。有希望的是，由于我们处于随机训练中，我们可以假设我们在历元期间使用的数据样本Xi代表整个数据集，因此有理由认为我们从该样本Xi获得的log(P(Xi|zi))和相关生成的zi代表log(P(X|z))的Q上的期望。</p><p id="8222" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，解码器只是一个生成器模型，我们希望重建输入图像，因此一个简单的方法是使用输入图像和生成图像之间的均方误差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/a9e8b2f67ccaf55951e1c06b621509dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TFRoN-ex9Yrj6pXr9At1FQ.png"/></div></div></figure></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="c0f0" class="nm mi iq bd mj nn no np mm nq nr ns mp jw nt jx ms jz nu ka mv kc nv kd my nw bi translated">3.VAEs的最终架构</h1><p id="659a" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nx lh li lj ny ll lm ln nz lp lq lr ij bi translated">我们可以知道恢复一个VAE的最终架构。如简介中所述，网络分为两部分:</p><ul class=""><li id="fdbc" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">学习生成依赖于输入样本X的分布的编码器，我们可以从中对很可能生成X个样本的潜在变量进行采样。这部分需要优化，以使Q(z|X)成为高斯型。</li><li id="8bd1" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">给定潜在变量z作为输入，解码器部分学习生成属于真实数据分布的输出。该部分将采样的z(最初来自正态分布)映射到更复杂的潜在空间(实际代表我们的数据的空间),并从该复杂的潜在变量z生成尽可能接近我们分布中真实数据点的数据点。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/4cfc87a8dab0facb880e6743e1ad3946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1RivMJgAG1-InJ0MeRk6w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">可变自动编码器详细架构。(左)和(右)是等价的，但是我们不能在(左)的例子中进行反向传播，所以我们在实践中使用(右)的例子。</p></figure></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="0bff" class="nm mi iq bd mj nn no np mm nq nr ns mp jw nt jx ms jz nu ka mv kc nv kd my nw bi translated">4.VAEs实验</h1><p id="a87e" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nx lh li lj ny ll lm ln nz lp lq lr ij bi translated">现在你已经知道了变分自动编码器背后的所有数学原理，让我们通过使用PyTorch做一些实验来看看我们能用这些生成模型做些什么。</p><p id="5cff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">py torch的全球建筑</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="e396" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">训练</strong></p><p id="8fe4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的图表显示了我们在训练中得到的结果。为了这个演示，VAE已经在MNIST数据集[3]上进行了训练。每隔10个时期，我们绘制输入X和产生该给定输入的VAE的生成数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/6922378d9edf3bd1859b7b6e0d0829dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ANVJGHLqwpGMI4Hf3PdHcg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练样本(输入向上，输出向下)—时期1</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/f9b7edb42bbe60201dc7e7e6d9823a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XvQYzR2Hlp54M1zgmeA4vQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练样本(输入向上，输出向下)—时期10</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/29eaf234a53b20148e16ba1362a722c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x4XBZwiLdLEEGie85PgWnA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练样本(输入向上，输出向下)—时期20</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/370ce6961cb5ea463adfe6358e72c7cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XklfB-gw43D1Z2LTLhCtEw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练样本(输入向上，输出向下)—时期30</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/dd800b366f8190ebac135a51f25e13f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z7gTtK2us1YQTmU4s4u5qA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练样本(输入向上，输出向下)—时期40</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/7b43073a1e4d233a6b1ddf233c951b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MiCJfJteaeG-6QgSRFTlYA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练样本(输入向上，输出向下)—时期50</p></figure><p id="c85c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">潜在空间</strong></p><p id="501f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于VAEs的一个有趣的事情是，在训练期间学习的潜在空间有一些很好的连续性。我们可以通过考虑一个二维潜在空间来可视化这些性质，以便能够在2D容易地可视化我们的数据点。</p><p id="4b11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当查看在训练期间学习的2D潜在空间中的MNIST数据集样本的重新划分时，我们可以看到相似的数字被分组在一起(绿色的<code class="fe op oq or os b">3</code>都被分组在一起并且接近非常相似的<code class="fe op oq or os b">8</code>)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/54f73cc7fc630dffc36262d0bc621e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1gxtoX5d56jL_UzN0pT7w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">2D潜在空间可视化</p></figure><p id="1300" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更直观地理解潜在空间连续性的一个好方法是观察潜在空间区域生成的图像。我们可以在下图中看到，当在潜在空间中移动时，数字被平滑地转换成如此相似的数字。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/21ea7935f65ccb89e850eca2b5bf2b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iZNiv66_cs4kkSGzeEABVA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从2D潜在空间抽样生成的数字</p></figure><h1 id="c164" class="nm mi iq bd mj nn ov np mm nq ow ns mp jw ox jx ms jz oy ka mv kc oz kd my nw bi translated">结论</h1><p id="8b0d" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nx lh li lj ny ll lm ln nz lp lq lr ij bi translated">变分自动编码器真的是一个惊人的工具，由于神经网络的力量，解决了一些真正具有挑战性的生成模型问题。与以前的方法相比，VAEs解决了两个主要问题:</p><ul class=""><li id="5df5" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">如何对潜在空间中最相关的潜在变量进行采样，以产生给定的输出。</li><li id="831f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如何将潜在的空间分布映射到真实的数据分布。</li></ul><p id="c621" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，vae也有一些缺点:</p><ul class=""><li id="3114" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">生成的图像是模糊的，因为均方误差倾向于使生成器收敛到平均最优值。</li></ul><p id="6147" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">生成式反向串行网络(GANs)通过使用鉴别器而不是均方误差损失来解决后一个问题，并产生更真实的图像。然而，GAN潜在空间非常难以控制，并且不具有(在经典设置中)像VAEs那样的连续性特性，而VAEs有时是某些应用所需要的。</p><h2 id="c2b0" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">参考</h2><p id="d9bb" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nx lh li lj ny ll lm ln nz lp lq lr ij bi translated">[1]多尔施，c，2016。变分自动编码器教程。<em class="oa"> arXiv预印本arXiv:1606.05908 </em>。</p><p id="0df0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]金玛，D.P .和韦林，m .，2019。变分自动编码器导论。<em class="oa"> arXiv预印本arXiv:1906.02691 </em>。</p><p id="3d91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]http://yann.lecun.com/exdb/mnist/<a class="ae kv" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">MNIST数据集</a></p><h2 id="86ab" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">密码</h2><div class="pa pb gp gr pc pd"><a href="https://github.com/sinitame/neural-networks-experiments" rel="noopener  ugc nofollow" target="_blank"><div class="pe ab fo"><div class="pf ab pg cl cj ph"><h2 class="bd ir gy z fp pi fr fs pj fu fw ip bi translated">sinitame/神经网络-实验</h2><div class="pk l"><h3 class="bd b gy z fp pi fr fs pj fu fw dk translated">我在神经网络上做的所有实验的集合</h3></div><div class="pl l"><p class="bd b dl z fp pi fr fs pj fu fw dk translated">github.com</p></div></div><div class="pm l"><div class="pn l po pp pq pm pr kp pd"/></div></div></a></div></div></div>    
</body>
</html>