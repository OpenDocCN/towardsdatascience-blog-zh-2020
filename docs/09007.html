<html>
<head>
<title>Common Activation Functions and Why You Must Know Them</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">常见的激活功能以及为什么你必须知道它们</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/common-activation-functions-and-why-you-must-know-them-ec90e54a7079?source=collection_archive---------51-----------------------#2020-06-28">https://towardsdatascience.com/common-activation-functions-and-why-you-must-know-them-ec90e54a7079?source=collection_archive---------51-----------------------#2020-06-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f443" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对常用激活功能的实现、额外好处、注意事项和使用的总结</h2></div><p id="aaa2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">激活函数是神经网络的基本构件。然而，在人们能够有效地使用它们之前，必须仔细地研究它们。这是因为激活函数有活动区域和死亡区域，使得它们在模型中要么学习要么行动死亡。让我们一个接一个地讨论它们的正确使用和它们的缺点。</p><h1 id="a1ec" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">Sigmoid 函数</h1><p id="7442" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated"><strong class="kk iu"> Sigmoid 函数</strong>或<strong class="kk iu"> logistic 函数</strong>或<strong class="kk iu">软阶跃函数</strong>是研究神经网络性能的共同出发点。从数学上讲，这个函数及其导数可以表示为图 1 所示。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/fc569248496749a44958e5411786ece0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gWFvneZQOir8uVLWz_UmRg.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图 1 Sigmoid 函数和导数(图片由作者提供)</p></figure><p id="84f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该函数可以如图 2 所示绘制。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mr"><img src="../Images/354c79de4a3da30e40e4070e87099a45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g6u4DP2ho4bY7lP2nAqXjg.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图 2 Sigmoid 函数(图片由作者提供)</p></figure><p id="354e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到该函数只能在大致-4 到 4 的范围内帮助训练。这是因为，在任何一侧超过这些限制，梯度都不显著。让我们看看图 3 所示的梯度曲线。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mr"><img src="../Images/9be696d9d061cdc8c74ef4966654fed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AHX5cv7ZIotWY6hxahFCTw.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图 3 sigmoid 函数的导数(图片由作者提供)</p></figure><p id="efcb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，在绿线和红线所示的[-4，4]的限制之外，梯度并不那么显著。这就是所谓的<strong class="kk iu">消失渐变</strong>的问题。由于这个原因，在深层网络中使用 sigmoid 函数是不理想的。此外，如果你想把更小的值(&lt; 1)不断相乘，你最终会得到非常小的值。这在深层网络中是不利的条件。所以<strong class="kk iu"> sigmoid </strong>根本就不是深度学习想要的激活。<strong class="kk iu"> ReLU </strong>是克服 sigmoid 函数不良结果的第一个变通方法。</p><p id="99f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然<strong class="kk iu"> sigmoid </strong>函数不用于隐藏层，但它是<strong class="kk iu">输出层</strong>的理想选择。这是因为 sigmoid 给出了[0，1]范围内的值，这可以帮助我们训练一个用于二进制编码输出的网络。这应当连同一个<strong class="kk iu"> <em class="ms">二元交叉熵</em> </strong>损失函数一起完成。</p><h1 id="066b" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">ReLU ( <strong class="ak"> R </strong>有效<strong class="ak"> L </strong>线性单位)函数</h1><p id="6c9d" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">ReLU 只是简单地输出一个神经元的非负输出。在数学上，这可以表示为如图 4 所示。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mt"><img src="../Images/af1194a5b2a40978ef1b2b81e3c8f458.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gDf1aj0hBJcGb6S3XCb_Q.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图 4 ReLU 函数和导数(图片由作者提供)</p></figure><p id="b3cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以绘制函数及其导数，如图 5 所示。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mr"><img src="../Images/03caa5e5f963a81f43dd377cde3921f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yOnYOjhMsqCYGB15RKlCHQ.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图 5 ReLU 及其衍生产品(图片由作者提供)</p></figure><p id="5dea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如图 5 所示，导数永远不会死在正区域。此外，由于值是在没有任何阻尼的情况下输出的，因此值不会像我们在<strong class="kk iu"> sigmoid </strong>函数中看到的那样消失。因此，<strong class="kk iu"> ReLU </strong>成为深度学习的理想候选。然而，正如你可能已经注意到的，<strong class="kk iu"> ReLU </strong>根本不存在于负空间中。这通常是可以的，因为神经网络的输入值和输出值是正的。然而，如果你有范围[-1，1]内的标度值或标准化值，这可能会杀死一些无法恢复的神经元。这种现象被称为<strong class="kk iu">垂死的 ReLU </strong>。虽然这不是人们应该担心的事情(可以通过使用最小-最大缩放器进行缩放来避免)，但了解变通方法是值得的。注意，由于值通过 ReLU 网络的爆炸性质，通常不希望在<strong class="kk iu">输出层</strong>中使用<strong class="kk iu"> ReLU </strong>。此外，爆炸值会导致<strong class="kk iu">爆炸梯度</strong>的现象。然而，使用适当的<strong class="kk iu"> <em class="ms">剪裁</em> </strong>和<strong class="kk iu"> <em class="ms">规则化</em> </strong>可以有所帮助。</p><h1 id="f1cb" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">ReLU 变量和其他线性单位函数</h1><p id="5497" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">ReLU 有几个变种，有助于克服<strong class="kk iu">死 ReLU </strong>的问题。</p><h2 id="391b" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na lq kv nb nc ls kz nd ne lu nf bi translated">PReLU(参数 ReLU)和泄漏 ReLU</h2><p id="17d3" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated"><strong class="kk iu">参数化 ReLU </strong>试图将负输入参数化，从而使垂死 ReLU 恢复。然而，该参数是一个可学习的参数，但<strong class="kk iu"> Leaky ReLU </strong>除外，它为负分量使用一个固定参数。通常这个参数被选择为一个小值，因为网络的典型输出被认为是正的。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ng"><img src="../Images/38088b667ac1c8608b2cdb747f34014f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X84GiiFWvT6lH9NMDpZNTA.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图 6 PReLU 和衍生工具(图片由作者提供)</p></figure><p id="7236" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于给定的𝛂值(0.1)，我们绘制了如图 7 所示的图表。这是<strong class="kk iu">漏 ReLU </strong>的场景，0.1 为负乘数。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mr"><img src="../Images/975b2a416dcc4c7ec7053c8fb65b866e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nMMQoRI-bSPaDQ6bOZSeaw.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图 7 ⍺=0.1 的泄漏 ReLU(图片由作者提供)</p></figure><p id="1cd6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，负区域中的梯度是细微的，但不为零。还有，节点面对太多负面数字也不会死。</p><h2 id="31e4" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na lq kv nb nc ls kz nd ne lu nf bi translated">ELU(指数线性单位)函数</h2><p id="9a20" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">在该函数中，负分量使用指数表示法建模。然而，我们仍然有可学习的参数𝛂.</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nh"><img src="../Images/83ddce694139f8d1f4db5976c5515cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tUoTDZsV_6GDnZhY2pZsHA.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图 8 ELU 函数(图片由作者提供)</p></figure><p id="d7b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以将其绘制成图 9 所示。这里，为了更好的可视性，我们假设𝛂=0.5。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mr"><img src="../Images/2a3c2ca40ab769aebf04300349cc45d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7jku3mWqSTzPA-bPWArI8g.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图 9 ELU 函数及其导数(图片由作者提供)</p></figure><p id="5eac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然我们已经讨论了深度网络中使用的几个重要函数，那么让我们来看看一些常见但不那么复杂的函数。</p><h1 id="9ae2" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">Tanh 函数</h1><p id="1248" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated"><strong class="kk iu">谭</strong>是<strong class="kk iu"> LSTM </strong>网络背后的驱动力。这有助于避免<strong class="kk iu"> RNNs </strong>的缺点。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ni"><img src="../Images/8d73cf762f356253ff7b1e8d287924c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*caGj9hq_NWOG9nxdotKH9A.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图 10 双曲正切函数和导数(图片由作者提供)</p></figure><p id="dbfd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以将其绘制成图 11 所示。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mr"><img src="../Images/f6ebb6b7d619f940cb60dae22a7d0bf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OOfDZuE_3BStXoLUsQeFzQ.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图 11 双曲正切函数和导数(图片由作者提供)</p></figure><h1 id="968f" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">更多的激活功能</h1><ul class=""><li id="a43c" class="nj nk it kk b kl lw ko lx kr nl kv nm kz nn ld no np nq nr bi translated"><strong class="kk iu">恒等函数:</strong>这是<code class="fe ns nt nu nv b">f(x)=x</code>带导数<code class="fe ns nt nu nv b">f'(x)=1</code>的朴素函数。</li><li id="b9e0" class="nj nk it kk b kl nw ko nx kr ny kv nz kz oa ld no np nq nr bi translated"><strong class="kk iu"> Softmax 函数:</strong>该函数保证输出加起来为 1 的层的值。这主要用于使用<strong class="kk iu"> <em class="ms">分类交叉熵</em> </strong>作为损失函数的分类分类，并且主要用于输出层。</li></ul><h2 id="5863" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na lq kv nb nc ls kz nd ne lu nf bi translated">笔记</h2><p id="311e" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">激活函数的设计主要考虑导数。这就是为什么你会看到一个很好的简化的导数函数。</p><p id="e088" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">必须通过查看输入值的范围来选择激活函数。然而，<strong class="kk iu"> ReLU </strong>或<strong class="kk iu"> PReLU </strong>是一个很好的起点，带有<strong class="kk iu">s 形</strong>或<strong class="kk iu"> softmax </strong>到输出层。</p><p id="e7cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具有可学习参数的激活功能通常被实现为单独的层。这更直观，因为参数是通过相同的反向传播算法学习的。</p><p id="70cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望你喜欢阅读这篇文章。</p><p id="fe78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">干杯！:)</p></div></div>    
</body>
</html>