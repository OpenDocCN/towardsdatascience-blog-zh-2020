<html>
<head>
<title>Expressive power of graph neural networks and the Weisfeiler-Lehman test</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形神经网络的表达能力和Weisfeiler-Lehman检验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49?source=collection_archive---------4-----------------------#2020-06-26">https://towardsdatascience.com/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49?source=collection_archive---------4-----------------------#2020-06-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="b862" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">图形神经网络有多强大？</h2><div class=""/><div class=""><h2 id="9e25" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">你有没有一种感觉，图形上的深度学习是一组有时有效的启发式方法，但没有人知道为什么？在这篇文章中，我将讨论图形同构问题，Weisfeiler-Lehman启发式图形同构测试，以及如何使用它来分析图形神经网络的表达能力。</h2></div><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/485329710b5e9aa17cd9f4ec3d2e9253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*995kqY9C6EX75cTH5YVKeA.png"/></div></div></figure><p id="78c4" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated"><em class="lx">这是关于图形神经网络表达能力的三篇系列文章中的第一篇。在</em> <a class="ae ly" rel="noopener" target="_blank" href="/beyond-weisfeiler-lehman-using-substructures-for-provably-expressive-graph-neural-networks-d476ad665fa3"> <em class="lx">第二部分</em> </a> <em class="lx">中，我讨论了如何脱离Weisfeiler-Lehman层次结构，在</em> <a class="ae ly" rel="noopener" target="_blank" href="/beyond-weisfeiler-lehman-approximate-isomorphisms-and-metric-embeddings-f7b816b75751"> <em class="lx">第三部分</em> </a> <em class="lx">中，我提出了为什么重新审视整个图同构框架可能是个好主意。</em></p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><p id="ba5c" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi mg translated">传统的前馈网络(多层感知器)是众所周知的通用逼近器:它们能够以任何期望的精度逼近任何平滑函数。对于最近才出现的图形神经网络，其表示特性还不太为人所知。在实验中经常观察到，图形神经网络在一些数据集上表现出色，但同时在另一些数据集上表现令人失望。为了找到这种行为的根源，我们必须回答这个问题:图形神经网络有多强大？</p><p id="36ab" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">挑战之一是应用中遇到的图是连续和离散结构的组合(分别是节点和边特征和连通性)，因此这个问题可以以不同的方式提出。一个可能的公式是图形神经网络是否可以区分不同类型的图形结构。这是图论中的一个经典问题，称为<em class="lx">图同构</em> <em class="lx">问题</em>，旨在确定两个图是否拓扑等价[1]。两个同构的图具有相同的连通性，不同之处仅在于它们的节点排列不同。</p><p id="b5cc" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">有点令人惊讶的是，图同构问题的确切复杂性类别是未知的。不知道在<a class="ae ly" href="https://en.wikipedia.org/wiki/Polynomial_time" rel="noopener ugc nofollow" target="_blank">多项式时间</a>内可解，也不知道<a class="ae ly" href="https://en.wikipedia.org/wiki/NP-complete" rel="noopener ugc nofollow" target="_blank">NP-完全</a>，有时会归结为一个特殊的“<a class="ae ly" href="https://en.wikipedia.org/wiki/Graph_isomorphism_problem#Complexity_class_GI" rel="noopener ugc nofollow" target="_blank"> GI类</a>”[2]。</p><p id="5de2" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">魏斯费勒-雷曼检验。<a class="ae ly" href="https://en.wikipedia.org/wiki/Boris_Weisfeiler" rel="noopener ugc nofollow" target="_blank">Boris Weisfeiler</a>和<a class="ae ly" rel="noopener" target="_blank" href="/a-forgotten-story-of-soviet-ai-4af5daaf9cdf">Andrey Lehman</a>【3】在1968年的开创性论文中提出了一种高效的启发式算法，现在被称为<em class="lx"> Weisfeiler-Lehman (WL)测试</em>，最初被认为是图同构问题的多项式时间解决方案【4】。一年后发现一个反例；然而，从概率意义上来说，WL检验似乎适用于几乎所有的图[5]。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mp"><img src="../Images/d89259a0ed55e24cc87a2e098ec8ab19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SuYTb17bUhKzH-Ti"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">在两个同构图上执行Weisfeiler-Lehman测试的例子。方括号表示多重集。该算法在颜色不变后停止，并产生一个输出(颜色直方图)。两个图的相等输出表明它们可能是同构的。</p></figure><p id="6982" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">WL测试基于迭代图重新着色[6](“颜色”在图论中是指一个离散的节点标签)，从相同颜色的所有节点开始。在每一步中，该算法将节点及其邻域的颜色聚合起来表示为多重集[7]，并将聚合的颜色多重集散列成独特的新颜色。算法在达到稳定的颜色时停止。如果在这一点上两个图的着色不同，那么这两个图就被认为是非同构的。然而，如果着色相同，那么这两个图可能(但不一定)是同构的。换句话说，WL测试是图同构的一个必要但不充分的条件。存在着不同质的图，WL测试产生相同的颜色，因此认为它们“可能是同构的”；据说在这种情况下测试会失败。下图显示了一个这样的例子:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mu"><img src="../Images/cd3593f543adf07112a431740fd3face.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EGoWieQh4015iCuJO2LKdQ.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">WL图同构判定失败的两个不同构的图，从它产生的相同着色可以明显看出。在化学中，这些图表代表了两种不同化合物的分子结构，十氢萘(左)和双环戊基(右)。图改编自[14]。</p></figure><p id="cb7a" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated"><strong class="ld ja">图同构网络。</strong>徐克玉路【9】和【10】(至少在两年前，托马斯·基普夫在他的<a class="ae ly" href="http://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">博客文章</a>中注意到<strong class="ld ja"/>)WL测试与图消息传递神经网络有惊人的相似之处【8】，一种在图上进行类似卷积运算的方法。在消息传递层，每个节点的特征通过聚集邻居的特征来更新。聚合和更新操作的选择是至关重要的:只有多重集内射函数使它等价于WL算法。文献中使用的聚合器的一些流行选择，如最大值或平均值，实际上没有WL强大，并且不能区分非常简单的图形结构:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mv"><img src="../Images/48f044a5618b6f0a6da605559879e5e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8groduqpXatrcoq4"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">不能用max区分但可以用mean聚合器区分(第一和第二)和既不能用max也不能用mean区分(第一和第三)的图结构的例子。原因是以这种方式从黑色节点的邻居聚集的特征将是相同的。图改编自[9]。</p></figure><p id="248a" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">徐提出了一种使消息传递神经网络等价于算法的聚集和更新函数，称之为图同构网络。这是标准消息传递神经网络所能达到的最强大的功能。但不仅仅是一个新的架构，主要的影响是在一个简单的环境中阐述了表现力的问题，这可能与图论中的一个经典问题有关。这个想法已经激发了多部后续作品。</p><p id="19ac" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">魏斯费勒-雷曼层级。扩展徐和Morris结果的一个方向是使用更强大的图同构测试。由拉斯洛·巴拜提出的<em class="lx"> k-WL测试</em>是Weisfeiler-Lehman算法的高阶扩展，它作用于<em class="lx"> k </em>元组而不是单个节点。除了等价的1-WL和2-WL测试之外，(<em class="lx"> k </em> +1)-WL严格强于<em class="lx"> k </em> -WL，对于任何<em class="lx"> k </em> ≥2，即存在这样的图的例子，其中<em class="lx"> k </em> -WL失败，(<em class="lx"> k </em> +1)-WL成功，但反之则不然。<em class="lx">k</em>——WL因此是一个层次结构或日益强大的图同构测试，有时被称为Weisfeiler-Lehman层次结构[10]。</p><p id="2025" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">有可能设计出遵循<em class="lx"> k </em> -WL测试的图形神经网络，从而比消息传递架构更强大。第一个这样的建筑，<em class="lx"> k </em> -GNN，就是由莫里斯提出的。传统的消息传递神经网络和这种高阶gnn之间的关键区别在于，它们是<em class="lx">非局部</em>的，因为<em class="lx"> k </em> -WL算法对<em class="lx">k</em>-节点元组进行操作。这对算法的实现及其计算和存储复杂性都有重要的影响:<em class="lx"> k </em> -GNNs需要𝒪( <em class="lx"> nᵏ </em>内存。作为降低复杂性的一种方式，Morris设计了一个本地版本的<em class="lx"> k </em> -GNNs，它基于本地邻居的聚合，但是表达能力不如<em class="lx"> k </em> -WL。</p><p id="3db4" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">有些不同的高阶图架构是由Haggai Maron提出的，2019年9月我有幸参加了他在魏茨曼研究所的博士答辩。Maron基于k阶张量[12]定义了一类不变图网络(IGN ),并证明了它们与k阶T21-WL一样强大。IGNs是从<em class="lx">k</em>-WL【10】的不同变体中派生出来的，与<em class="lx"> k </em> -GNNs相比，在复杂性方面更有优势。特别是，3-WL等效IGN“仅”具有二次复杂度，这可能是唯一实际有用的图形神经网络架构，严格来说比消息传递更强大，但与前者的线性复杂度仍相去甚远[16]。</p><p id="dc7b" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">从理论的角度来看，可证明强大的图形神经网络的工作提供了一个严格的数学框架，可以帮助解释和比较不同的算法。已经有多个后续工作使用来自图论和分布式局部算法的方法扩展了这些结果[14]。</p><p id="cc4a" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">然而，从实践的角度来看，这些新架构几乎没有显著的影响:例如，最新的基准测试[15]表明，最近证明强大的算法实际上不如旧技术。这在机器学习中并不罕见，理论和实践之间往往存在很大差距。一种解释可能是基准本身的缺陷。但更深刻的原因可能是，更好的表达能力不一定提供更好的概括(有时正好相反)，而且，图同构模型可能无法正确捕捉特定应用程序中图相似性的实际概念——我将在下一篇文章中讨论这一点。可以肯定的是，这一系列的工作对于搭建通往其他学科的桥梁和带来之前没有在图形深度学习领域使用的方法是非常富有成效的[17]。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><p id="edf9" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[1]即在两个图的节点之间存在保边双射。</p><p id="822b" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[2]图同构因此可能是一个<a class="ae ly" href="https://en.wikipedia.org/wiki/NP-intermediate" rel="noopener ugc nofollow" target="_blank">NP-中间</a>复杂性类。对于一些特殊的图族(如树、平面图或最大度有界的图)，存在多项式时间算法。</p><p id="7406" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[3] B. Weisfeiler，A. Lehman，将图简化为标准形式以及其中出现的代数(1968)。信息技术2(9):12–16。<a class="ae ly" href="https://www.iti.zcu.cz/wl2018/pdf/wl_paper_translation.pdf" rel="noopener ugc nofollow" target="_blank">英文翻译</a>和<a class="ae ly" href="https://www.iti.zcu.cz/wl2018/pdf/wl_paper_orig.pdf" rel="noopener ugc nofollow" target="_blank">俄文原版</a>，其中包含一个不寻常的西里尔符号形式的双关语(операция„ы)，指的是三年前与<a class="ae ly" href="https://en.wikipedia.org/wiki/Operation_Y_and_Shurik%27s_Other_Adventures" rel="noopener ugc nofollow" target="_blank">同名的苏联大片</a>。参见这篇<a class="ae ly" href="https://davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/#:~:text=The%20core%20idea%20of%20the,used%20to%20check%20for%20isomorphism." rel="noopener ugc nofollow" target="_blank">博文</a>中的一个流行论述。Lehman有时也被拼写为“Leman”，然而，考虑到这个姓氏的日耳曼起源，我更喜欢更准确的前一个变体。</p><p id="bf31" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[4] I .波诺马伦科，<a class="ae ly" href="https://www.iti.zcu.cz/wl2018/wlpaper.html" rel="noopener ugc nofollow" target="_blank">魏斯费勒·雷曼的原文</a>。提供了这篇经典论文的历史背景。他指出这项工作的动机来自化学应用。</p><p id="138e" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[5] L. Babai等人<a class="ae ly" href="https://pdfs.semanticscholar.org/114e/a414b025a68d641efad9b74295a5625b9e7e.pdf?_ga=2.67290641.1827801715.1592846909-1172292721.1591164305" rel="noopener ugc nofollow" target="_blank">随机图同构</a> (1980)。暹罗计算机杂志9(3):628–635。</p><p id="a9ed" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[6]weis feiler和Lehman的原始论文实际上描述了2-WL变体，然而它等价于1-WL，也称为颜色细化算法。作为历史记录，这样的算法以前在计算化学中是已知的，参见例如H. L. Morgan。化学结构的独特机器描述的生成——化学文摘服务处(1965年)开发的技术。化学博士。医生。5(2):107–113，这是在化学中广泛使用的摩根分子指纹的基础。</p><p id="ea31" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[7]多重集是一个集合，其中相同的元素可能出现多次，但元素的顺序无关紧要。</p><p id="3113" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[8] J. Gilmer等人，<a class="ae ly" href="https://arxiv.org/abs/1704.01212" rel="noopener ugc nofollow" target="_blank">量子化学的神经信息传递</a> (2017)。继续。ICML。</p><p id="5f3a" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[9] K. Xu et al. <a class="ae ly" href="https://arxiv.org/abs/1810.00826" rel="noopener ugc nofollow" target="_blank">图神经网络到底有多强大？</a> (2019)。继续。ICLR。</p><p id="97db" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[10]WL检验存在几种变体，它们具有不同的计算和理论属性，术语也相当混乱:建议读者清楚地理解不同作者所说的“<em class="lx"> k </em> -WL”到底是什么意思。一些作者，例如Sato和Maron，区分了WL和“民俗”WL (FWL)测试，这两种测试的主要区别在于颜色细化步骤。<em class="lx"> k </em> -FWL测试等价于(<em class="lx"> k </em> +1)-WL。Morris使用的set <em class="lx"> k </em> -WL算法是另一种变体，它具有更低的存储复杂度，但严格弱于<em class="lx"> k </em> -WL。</p><p id="f724" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[11] C. Morris等人<a class="ae ly" href="https://aaai.org/ojs/index.php/AAAI/article/view/4384/4262" rel="noopener ugc nofollow" target="_blank"> Weisfeiler和Leman go neural:高阶图神经网络</a> (2019)。继续。AAAI。</p><p id="7735" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[12] H. Maron等人<a class="ae ly" href="https://arxiv.org/abs/1812.09902" rel="noopener ugc nofollow" target="_blank">不变和等变图网络</a> (2019)。继续。ICLR。</p><p id="aa5d" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[13] H. Maron等人<a class="ae ly" href="https://arxiv.org/abs/1905.11136" rel="noopener ugc nofollow" target="_blank">可证明强大的图形神经网络</a> (2019)。继续。神经炎。参见作者的一篇<a class="ae ly" href="http://irregulardeep.org/How-expressive-are-Invariant-Graph-Networks-(2-2)/" rel="noopener ugc nofollow" target="_blank">博文。</a></p><p id="1fb5" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[14] R. Sato，<a class="ae ly" href="https://arxiv.org/abs/2003.04078" rel="noopener ugc nofollow" target="_blank">关于图形神经网络表达能力的调查</a> (2020)。arXiv: 2003.04078。提供了一个非常全面的审查不同的WL测试和等效的图形神经网络架构，以及链接到分布式计算算法。</p><p id="cd89" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[15] V. P. Dwivedi等人<a class="ae ly" href="https://arxiv.org/abs/2003.00982" rel="noopener ugc nofollow" target="_blank">标杆图神经网络</a> (2020)。arXiv: 2003.00982。</p><p id="5c28" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[16]更准确地说，消息传递的复杂性是边数的线性关系，而不是节点数的线性关系。在稀疏图中，这大致相同。在稠密图中，边的数量可以是𝒪( <em class="lx"> n </em>。出于这个原因，Maron认为他的体系结构对密集图形是有用的。</p><p id="5c4c" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">[17]准确地说，WL形式主义对机器学习界来说并不陌生。N. Shervashidze和K. M. Borgwardt的开创性论文《图上的快速子树核》( 2009年)。继续。据我所知，NIPS是最近深度神经网络复兴之前第一个使用这种结构的，比本文讨论的工作早了近十年。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><p id="59ad" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">感谢迈赫迪·巴赫里、卢卡·贝利、乔治·布里萨斯、法布里奇奥·弗拉斯卡、费伦茨·胡萨尔、谢尔盖·伊万诺夫、伊曼纽·罗西和大卫·西尔弗帮助校对和编辑这篇文章，并感谢卡斯滕·博格瓦特指出了与WL内核的关系。承蒙 <a class="ae ly" href="https://medium.com/@zhiyongliu" rel="noopener"> <em class="lx">刘止庸</em> </a> <em class="lx">提供本帖的</em> <a class="ae ly" href="https://www.infoq.cn/article/mOL8kKbTaXr4t6YTwkgU" rel="noopener ugc nofollow" target="_blank"> <em class="lx">中文翻译</em> </a> <em class="lx">。关于图形深度学习的其他文章，请参见我的</em> <a class="ae ly" rel="noopener" target="_blank" href="https://towardsdatascience.com/graph-deep-learning/home"> <em class="lx">博客</em> </a> <em class="lx">关于走向数据科学，</em> <a class="ae ly" href="https://michael-bronstein.medium.com/subscribe" rel="noopener"> <em class="lx">订阅</em> </a> <em class="lx">我的帖子和</em> <a class="ae ly" href="https://www.youtube.com/c/MichaelBronsteinGDL" rel="noopener ugc nofollow" target="_blank"> <em class="lx"> YouTube频道</em> </a> <em class="lx">，获取</em> <a class="ae ly" href="https://michael-bronstein.medium.com/membership" rel="noopener"> <em class="lx">中等会员</em> </a> <em class="lx">，或者关注我的</em><a class="ae ly" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"><em class="lx">Twitter【T49】</em></a></p></div></div>    
</body>
</html>