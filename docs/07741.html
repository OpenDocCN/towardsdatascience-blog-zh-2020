<html>
<head>
<title>Creating Mo-Cap Facial Animations of Virtual Characters with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用深度学习创建虚拟角色的Mo-Cap面部动画</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-mo-cap-facial-animations-of-virtual-characters-with-deep-learning-dc1d394caac8?source=collection_archive---------49-----------------------#2020-06-09">https://towardsdatascience.com/creating-mo-cap-facial-animations-of-virtual-characters-with-deep-learning-dc1d394caac8?source=collection_archive---------49-----------------------#2020-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f5d8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">综述论文“图像动画的一阶运动模型”并探讨其在游戏动画产业中的应用。</h2></div><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="kn ko l"/></div></figure><p id="2e87" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><a class="ae ll" href="https://en.wikipedia.org/wiki/Motion_capture" rel="noopener ugc nofollow" target="_blank">运动捕捉</a>(简称Mo-Cap)是用摄像机记录人的真实运动的过程，目的是在计算机生成的场景中再现那些精确的运动。作为一个着迷于在游戏开发中使用这项技术来创建动画的人，我很高兴看到在深度学习的帮助下这项技术得到了巨大的改进。</p><p id="1531" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在这篇文章中，我想分享一个由A. Siarohin等人最近发表的NeurIPS论文“<a class="ae ll" href="https://arxiv.org/pdf/2003.00196.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lm">用于图像动画的一阶运动模型</em> </a>”的快速概述。艾尔。并展示其在游戏动画行业的应用将如何“改变游戏规则”。</p><h1 id="74e0" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">运动扫描技术</h1><p id="9ab0" class="pw-post-body-paragraph kp kq it kr b ks mf ju ku kv mg jx kx ky mh la lb lc mi le lf lg mj li lj lk im bi translated">早在2011年，游戏《黑色洛杉矶》就推出了绝对令人惊叹的栩栩如生的面部动画，看起来领先于其他所有游戏。现在，将近十年过去了，我们仍然没有看到许多其他游戏在提供逼真的面部表情方面接近它的水平。</p><div class="ki kj kk kl gt ab cb"><figure class="mk km ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><img src="../Images/b34770844148761b03d10f2615f344a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/1*TR83NwxihFKD4OZQABWdnw.gif"/></div></figure><figure class="mk km mw mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><img src="../Images/a43b07cfcb165d4453b68349601f5410.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*QbLO1ZFhxj3eUpIe2JRG0w.jpeg"/></div></figure><figure class="mk km mx mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><img src="../Images/68dd83514a82aaf215acea0dc615bbed.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/1*WpSfdeHWLE9UzlhVwja1lg.gif"/></div><p class="my mz gj gh gi na nb bd b be z dk nc di nd ne translated">RockStar工作室在2011年游戏《黑色洛杉矶》中使用的MotionScan技术，用于创建逼真的面部动画。[ <a class="ae ll" href="https://www.youtube.com/watch?v=q2EG5J05048" rel="noopener ugc nofollow" target="_blank">来源</a> ]</p></figure></div><p id="a5bf" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这是因为在开发这款名为<a class="ae ll" href="https://electronics.howstuffworks.com/motionscan-technology.htm" rel="noopener ugc nofollow" target="_blank"> MotionScan </a>的游戏时使用的面部扫描技术极其昂贵，而且捕捉到的动画文件太大，这就是为什么大多数发行商在游戏中采用这项技术不切实际。</p><p id="3cb5" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">然而，由于深度学习驱动的动作捕捉的最新进展，这种情况可能很快就会改变。</p><h1 id="a040" class="ln lo it bd lp lq lr ls lt lu lv lw lx jz ly ka lz kc ma kd mb kf mc kg md me bi translated">图像动画的一阶运动模型</h1><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/980bd125576ef55a658c603667d1f68a.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*mKBaawb1qmAnq--0pMHjDQ.png"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">全文PDF:<a class="ae ll" href="https://arxiv.org/pdf/2003.00196.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.00196.pdf</a></p></figure><p id="6744" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在这项研究工作中，作者提出了一种深度学习框架，通过跟踪驾驶视频中另一张脸的运动，从人脸的源图像创建动画，类似于MotionScan技术。他们提出了一种自我监督的训练方法，可以使用特定类别的未标记视频数据集来学习定义运动的重要动力学。然后，展示如何将这些动态运动与静态图像相结合，生成动态视频。</p><h2 id="6402" class="ng lo it bd lp nh ni dn lt nj nk dp lx ky nl nm lz lc nn no mb lg np nq md nr bi translated">框架(模型架构)</h2><p id="888b" class="pw-post-body-paragraph kp kq it kr b ks mf ju ku kv mg jx kx ky mh la lb lc mi le lf lg mj li lj lk im bi translated">让我们看看下图中这个深度学习框架的架构。它由<strong class="kr iu">运动模块</strong>和<strong class="kr iu">外观模块</strong>组成。驾驶视频是运动模块的输入，源图像是我们的目标对象，它是外观模块的输入。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ns"><img src="../Images/d3c9ee2218372c8686a27fba89b29479.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*fXakMKTVogsCYFDOvDB03Q.gif"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">一阶模型模型框架</p></figure><h2 id="2d36" class="ng lo it bd lp nh ni dn lt nj nk dp lx ky nl nm lz lc nn no mb lg np nq md nr bi translated">运动模块</h2><p id="f727" class="pw-post-body-paragraph kp kq it kr b ks mf ju ku kv mg jx kx ky mh la lb lc mi le lf lg mj li lj lk im bi translated">运动模块由编码器组成，该编码器学习包含相对于对象运动高度重要的稀疏关键点的潜在表示，该对象在该场景中是人脸。这些关键点在驾驶视频的不同帧之间的移动生成了一个<a class="ae ll" href="https://en.wikipedia.org/wiki/Motion_field" rel="noopener ugc nofollow" target="_blank">运动场</a>，它由一个我们希望模型学习的函数驱动。作者使用<a class="ae ll" href="https://www.youtube.com/watch?v=3d6DsjIBzJ4" rel="noopener ugc nofollow" target="_blank">泰勒展开</a>到<em class="lm">来近似</em>这个函数到<em class="lm">一阶</em>来创建这个运动场。根据作者，这是第一次一阶近似被用于模拟运动。此外，这些关键点的学习仿射变换被组合以产生D<em class="lm">sense运动场</em>。密集运动场预测帧的每个单独像素的运动，而不是只关注稀疏运动场中的关键点。接下来，运动模块还会生成一个<em class="lm">遮挡图</em>，它会突出显示需要内画的帧像素，这些像素是由头部相对于背景的运动产生的。</p><h2 id="8e75" class="ng lo it bd lp nh ni dn lt nj nk dp lx ky nl nm lz lc nn no mb lg np nq md nr bi translated"><strong class="ak">外观模块</strong></h2><p id="adba" class="pw-post-body-paragraph kp kq it kr b ks mf ju ku kv mg jx kx ky mh la lb lc mi le lf lg mj li lj lk im bi translated">外观模块使用编码器对源图像进行编码，然后将其与运动场和遮挡图相结合，以对源图像进行动画处理。发电机模型用于此目的。在自我监督训练过程中，来自驾驶视频的静止帧被用作源图像，并且所学习的运动场被用于使该源图像动画化。视频的实际帧充当生成的运动的基础事实，因此它是自我监督的训练。在测试/推断阶段，该源图像可以被来自相同对象类别的任何其他图像替换，并且不必来自驾驶视频。</p><h2 id="cc1a" class="ng lo it bd lp nh ni dn lt nj nk dp lx ky nl nm lz lc nn no mb lg np nq md nr bi translated">在游戏角色上运行训练好的模型</h2><p id="aa6e" class="pw-post-body-paragraph kp kq it kr b ks mf ju ku kv mg jx kx ky mh la lb lc mi le lf lg mj li lj lk im bi translated">我想探索这个模型在一些虚拟设计的游戏角色脸上的效果如何。作者分享了它的<a class="ae ll" href="https://github.com/AliaksandrSiarohin/first-order-model" rel="noopener ugc nofollow" target="_blank">代码</a>和一个易于使用的<a class="ae ll" href="https://github.com/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb" rel="noopener ugc nofollow" target="_blank">谷歌Colab笔记本</a>来测试这一点。这是他们训练的模型在游戏《侠盗猎车手》中不同角色身上测试时的样子。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/fa6c7272ddceb3768463e2b46e81c366.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/1*m9mOE0Yqds4SYhtEp3nToQ.gif"/></div><p class="my mz gj gh gi na nb bd b be z dk translated">使用一阶运动模型生成的面部动画。(游戏中的虚拟人物GTA V. <strong class="bd nu">左:</strong> Franklin <strong class="bd nu">中:</strong> Michael <strong class="bd nu">右:</strong> Trevor)</p></figure><p id="2706" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如你所看到的，使用这个AI创建逼真的动画是极其容易的，我认为它将被几乎每个游戏艺术家用于创建游戏中的面部动画。此外，为了用这种技术执行Mo-Cap，我们现在需要的只是一台相机和任何带GPU的普通计算机，这个人工智能将负责剩下的工作，这使得游戏动画师大规模使用这种技术变得非常便宜和可行。这就是为什么我对这个人工智能在未来游戏开发中可能带来的巨大改进感到兴奋。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="7b97" class="ng lo it bd lp nh ni dn lt nj nk dp lx ky nl nm lz lc nn no mb lg np nq md nr bi translated">有用的链接:-</h2><ol class=""><li id="b39c" class="oc od it kr b ks mf kv mg ky oe lc of lg og lk oh oi oj ok bi translated"><a class="ae ll" href="https://github.com/AliaksandrSiarohin/first-order-model" rel="noopener ugc nofollow" target="_blank"> GitHub代码</a></li><li id="38e8" class="oc od it kr b ks ol kv om ky on lc oo lg op lk oh oi oj ok bi translated"><a class="ae ll" href="https://arxiv.org/pdf/2003.00196.pdf" rel="noopener ugc nofollow" target="_blank">研究论文(PDF </a></li><li id="4329" class="oc od it kr b ks ol kv om ky on lc oo lg op lk oh oi oj ok bi translated"><a class="ae ll" href="https://aliaksandrsiarohin.github.io/first-order-model-website/" rel="noopener ugc nofollow" target="_blank">作者的博客文章</a></li><li id="c182" class="oc od it kr b ks ol kv om ky on lc oo lg op lk oh oi oj ok bi translated"><a class="ae ll" href="https://www.youtube.com/watch?v=u-0cQ-grXBQ&amp;feature=emb_title" rel="noopener ugc nofollow" target="_blank">不同数据集的更多结果</a></li></ol></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="698f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">感谢您的阅读。如果你喜欢这篇文章，你可以关注我在<a class="ae ll" href="https://medium.com/@chintan.t93" rel="noopener">媒体</a>、<a class="ae ll" href="https://github.com/ChintanTrivedi" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上的更多作品，或者订阅我的<a class="ae ll" href="http://youtube.com/c/DeepGamingAI" rel="noopener ugc nofollow" target="_blank"> YouTube频道</a>。</p></div></div>    
</body>
</html>