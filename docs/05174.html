<html>
<head>
<title>A neural network from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-neural-network-from-scratch-c09fd2dea45d?source=collection_archive---------31-----------------------#2020-05-03">https://towardsdatascience.com/a-neural-network-from-scratch-c09fd2dea45d?source=collection_archive---------31-----------------------#2020-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/0bd41ec18dd65d962d62eb60b276dc6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6sXfTGIYM_Szfns7Y1S36A.png"/></div></div></figure><div class=""/><div class=""><h2 id="ba2c" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">用神经元、Python和Numpy预测达斯·维德</h2></div><p id="484e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这篇文章中，我将向你展示如何用Python从头开始创建一个神经网络。我们将使用的唯一外部库是Numpy，用于一些线性代数。因为今天是5月4日<em class="lp">，</em>作为奖励，我们将使用这个新创建的神经网络来拟合一条复杂的消息，这条消息是从<em class="lp">穆斯塔法</em>截取的。一步一步地编写神经网络代码将有助于您理解内部工作方式，这与Torch和Tensorflow等流行框架非常相似。就我个人而言，我认为从零开始构建东西是掌握一个主题的最好方法，当然也很有趣！</p><blockquote class="lq lr ls"><p id="aaf3" class="kt ku lp kv b kw kx kf ky kz la ki lb lt ld le lf lu lh li lj lv ll lm ln lo im bi translated">“就是这条路”——丁·贾林</p></blockquote><p id="6ecb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这篇文章在我的Github上也可以作为<a class="ae lw" href="https://github.com/dennisbakhuis/Tutorials/blob/master/2_Neural_Network/Artificial_Neural_Network.ipynb" rel="noopener ugc nofollow" target="_blank">的Jupyter笔记本</a>获得，所以你可以边阅读边编码。</p><p id="879a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果你是Python和Jupyter的新手，<a class="ae lw" rel="noopener" target="_blank" href="/environments-conda-pip-aaaaah-d2503877884c">这里有一个关于我如何管理我的Python环境和包的简短说明</a>。</p><p id="0b99" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">👉Pip，conda，aaah是我每天10分钟Python课程的一部分！</p><p id="6a1f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">我们将讨论的主题的简短概述:</strong></p><ol class=""><li id="8d5b" class="lx ly je kv b kw kx kz la lc lz lg ma lk mb lo mc md me mf bi translated">神经网络和逻辑回归</li><li id="2803" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo mc md me mf bi translated">多层网络的一般化</li><li id="f1aa" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo mc md me mf bi translated">逐步实施</li><li id="0872" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo mc md me mf bi translated">它做了它应该做的事情吗？</li><li id="be09" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo mc md me mf bi translated">符合从穆斯塔法截获的信息</li></ol><h1 id="daee" class="ml mm je bd mn mo mp mq mr ms mt mu mv kk mw kl mx kn my ko mz kq na kr nb nc bi translated">1.神经网络和逻辑回归</h1><p id="2fc8" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">当我们随机问一个人关于机器学习的问题时，很有可能会提到神经网络。这些术语不仅发挥了我们的想象力，而且这些数学结构也证明了它们能够解决复杂的任务。这种任务的例子是对象检测、音频转录和文本翻译。神经网络可以相对较小，如图1所示，但仍然很强大，可以预测复杂的系统。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/d4503cffa3c8b834533c72b19b5c6944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SalbZTS_nqs7dnQw9gIHfw.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">图1:一个典型的神经网络，由一个输入层、两个隐藏层和一个输出层组成。</p></figure><p id="a59f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在图中，我们展示了一个人工神经网络(ANN)，或简称为神经网络(NN)，它有三层。按照惯例，我们不计算输入层，稍后我们会看到，图中的这一层表示您输入到神经网络的输入数据。这个特殊的神经网络有两个隐藏层。虽然我不确定为什么它被称为隐藏层，但我可以想象一个原因是这些层对用户是“隐藏”的。用户通过输入层输入数据，并从输出层获得结果，因此不会与隐藏层进行交互。对于用户来说，一个系统有一个或二十个隐藏层是没有区别的。与输入层相反，输出是一个实际的层，在我们的例子中是一个单个神经元，它“收集”来自前一个隐藏层的结果。</p><p id="dcb5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我之前的教程中，我试图解释逻辑回归(和一点线性回归)是如何工作的。逻辑回归可以被看作是最小的神经网络，只有一层，由一个神经元组成。可能最好首先简要回顾一下图2所示的逻辑模型。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/edde773fd52161bd5ff5a049cff4b415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_p34CKo69ycvbxSVrRaaHA.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">图2:一个逻辑回归模型的例子。这个例子有三个输入特征供给单个神经元。神经元本身分为两个操作，线性部分和非线性激活函数。请注意，线性方程中使用的权重是输出图层的一部分。</p></figure><p id="4b8e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">对于这个例子，第一步是对输入数据进行<em class="lp">按摩</em>，使个体<em class="lp">特征</em> ( <em class="lp"> 𝑥 </em> 1、<em class="lp"> 𝑥 </em> 2、<em class="lp"> 𝑥 </em> 3)在我们输入向量<em class="lp"> 𝑋 </em>的行中，列是<em class="lp">实例</em>(训练样本)。这个输入向量<em class="lp"> 𝑋 </em>用于我们单个神经元的前向传递。这个神经元分为两个操作。先是一个线性运算(线性回归)，<em class="lp">z</em>=<em class="lp">𝑋𝑊</em>+<em class="lp">𝑏</em>，接着是一个激活函数<em class="lp">𝐴</em>=<em class="lp">𝑔</em>(<em class="lp">𝑧</em>)。在逻辑回归教程中，我们执行了二元逻辑回归。这具体使用了Sigmoid激活函数<em class="lp">𝜎</em>(<em class="lp">𝑧</em>)= 1/(1+exp(<em class="lp">𝑧))</em>，因此我们在这里将其表示为<em class="lp"> 𝜎 </em> ( <em class="lp"> 𝑧 </em>)。在这里，我们通过使用<em class="lp"> 𝑔 </em> ( <em class="lp"> 𝑧 </em>)来表示激活函数来概括这一点。我们必须定义我们将使用哪个激活函数。正如我们今天将要学习的，有许多激活功能可供选择。</p><p id="b5b5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">激活函数使用来自神经元线性部分的结果作为输入。这是权重向量<em class="lp"> 𝑊 </em>和输入向量<em class="lp"> 𝑋 </em>的内积(也叫点积)，加上了偏置项<em class="lp"> 𝑏 </em>。偏差项<em class="lp"> 𝑏 </em>和权向量<em class="lp"> 𝑊 </em>(由<em class="lp"> 𝑤 </em> 1、<em class="lp"> 𝑤 </em> 2、<em class="lp"> 𝑤 </em> 3组成)是该系统的可训练参数。每个可训练权重(<em class="lp"> 𝑤 </em> 1、<em class="lp"> 𝑤 </em> 2、<em class="lp"> 𝑤 </em> 3)对应于一个输入特征(<em class="lp"> 𝑥 </em> 1、<em class="lp"> 𝑥 </em> 2、<em class="lp"> 𝑥 </em> 3)，并表示该特征添加到问题中的“权重”。在图表中，这些权重显示在输出图层的虚线框内，这意味着这些权重与该图层相关联。</p><p id="8480" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我希望这个简短的回顾是清楚的，否则，我可以推荐<a class="ae lw" rel="noopener" target="_blank" href="/environments-conda-pip-aaaaah-d2503877884c">我以前的Jupyter笔记本</a>来获得更全面的解释和Numpy中的一步一步的例子。</p><p id="c262" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">虽然逻辑回归是一个很好的工具，但它只能将参数空间分成一条线，至少在我们这里给出的形式中是这样。例如，如果你有两个特征<em class="lp"> 𝑥 </em> 1和<em class="lp"> 𝑥 </em> 2，它们将用于预测<em class="lp"> 𝑦 </em>，逻辑分类器只能在两个参数之间有一个线性边界。如果这没有意义，也不要担心，因为我们将通过一个例子来说明这个问题。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/98220763f822a1e968586733f453a958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Rw4iVQ1ZkZZOmRQwyZZOA.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">图3:一个神经网络可以被看作是一堆逻辑回归单元的组合。</p></figure><p id="ff9d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了让系统预测更复杂的关系，我们可以向一层添加更多的神经元，或者向我们的网络添加更多的层。每个神经元都是某种逻辑回归单元，其中许多结合起来可以预测高度非线性的关系(见图3)。我说<em class="lp">类似于</em>，因为在常规逻辑回归中，我们通常使用Sigmoid激活函数，而在神经网络中，许多其他激活函数的性能要好得多。</p><p id="ddc4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我们开始概括一个神经网络和所用的层之前，请观察图3中不同的神经元是如何连接的。您会注意到每个节点都连接到下一层的所有节点。这被称为密集连接(有时是完全连接)，这样的层通常被称为“密集层”。在下一节中，我们将尝试概括网络并确定所需的结构。</p><h1 id="8ccd" class="ml mm je bd mn mo mp mq mr ms mt mu mv kk mw kl mx kn my ko mz kq na kr nb nc bi translated">2.多层网络的一般化</h1><p id="9cc5" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">在我们开始编码我们的系统之前，让我们首先尝试概括所需的步骤。本教程的目标是创建一个通用的神经网络类，我们可以在其中添加任意数量的层，包含任意数量的神经元。稍后，我们将在具有不同复杂性的各种问题上测试这种结构。</p><p id="d674" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当我们想到输入层时，我们已经发现它不是神经网络的实际层，而是以正确形式重新整形的输入数据。因此，输入“层”不是我们架构的一部分。当然，输入数据具有许多特征，因此定义了我们的第一层的至少一个维度。</p><p id="c00d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">神经网络将由任意数量的层组成。这些结构的行为方式相似，并且是相互连续的。这意味着我们将有一个层结构，以及某种父结构来保存所有的层。当进行正向传递时，我们将遍历所有层，并将前一层的输出用作下一层的输入。最终层的输出，也称为最终激活<em class="lp"> 𝐴 </em>是神经网络的输出。这意味着，如果神经网络用于预测二进制值，则输出必须转换(或舍入)为实际预测值。</p><p id="3571" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了测试我们的预测，我们需要定义一个成本函数。成本函数是对模型预测效果的衡量，因此需要预测值和真实值。在最正式的意义上，我们需要定义一个损失函数，该函数计算单个示例的预测误差，并将所有这些损失合并到一个成本函数中，以获得我们输入到模型中的整个批次的一个度量。损失和成本函数只需要一次，而不是在每一层中，因此，它应该在父结构中实现。</p><p id="d56c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了优化权重并有希望改进模型，我们需要使用向后传递来计算梯度。首先我们需要计算父类中损失函数的梯度。这将是最后一层的输入，从这一点，我们向后循环计算梯度<em class="lp"> 𝑑𝑊 </em>和<em class="lp"> 𝑑𝑏 </em>，随着每次迭代，前一层的输入将是下一层的输入。为了计算梯度，我们需要来自向前传递的输入，为此我们可以做一个聪明的技巧，在向前传递期间缓存<em class="lp"> 𝑧 </em>和<em class="lp"> 𝐴 </em>的值。这看起来有点吓人，但这些步骤主要是记账。所有这些步骤都可以在图4中看到。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/9dfc4884e90d65f36bc0591830e1bd14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VYCxF4prixcjppys3EK5IQ.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">图4:我们的神经网络结构的概述。完整的结构，包括输出层<em class="nu"> 𝐿𝑜 </em>在我们的神经网络框架内。正向传递循环通过每一层，并输出最终激活的结果。进行反向传递时，首先计算成本函数的梯度，然后我们通过所有层反向迭代，计算梯度。</p></figure><p id="28d1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我们计算了每一层的梯度后，我们可以做一个更新步骤，并执行梯度下降步骤。这一步实际上与我们在逻辑回归教程中讨论的一步相同，在逻辑回归教程中，我们使用学习率将权重向真实结果“移动”一步。然而，这一次，我们必须对神经网络中的每一层都这样做。</p><p id="04ae" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们现在应该认识到，网络中的所有层在根本上是相同的。它们可以在节点数量或激活功能上有所不同，但第一层<em class="lp"> 𝐿 </em> 1和输出层<em class="lp"> 𝐿𝑜 </em>之间没有结构上的区别。这意味着我们可以创建一个层结构作为神经网络的一个构建模块。</p><p id="ceb5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们的神经网络将只由密集层组成，即所有神经元都完全连接到下一层神经元的层，因此我们将只制作单层结构。在这个结构中，我们必须计算向前传递、向后传递和更新步骤。每一层的输入都是前一层的输出。显然，对于第一层，这是输入向量。对于反向传递，我们按相反方向依次通过网络。然而，这里我们需要做一个额外的步骤，因为我们需要计算关于定义的损失函数的梯度。为此，我们需要在反向传递期间输入真实标签<em class="lp"> 𝑌 </em>和预测输出<em class="lp"> 𝐴 </em>。在这之后，我们有一个层期望的值，我们称之为<em class="lp"> 𝑑𝐴 </em>。每一层都以完全相同的方式处理向后传递。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/268292a49df36e89e29b6dd7734315d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*84qDgfrOmH7k6ZiJpiUtjg.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">图5:显示单层向前和向后传递的图表。在向前传递过程中，我们缓存A和z的值，以便在向后传递过程中使用。</p></figure><p id="0da4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">每一层的向前和向后传递如图5所示。让我们一步一步地走过每一关。在前向传递中，它期望前一层的输出<em class="lp"> 𝐴𝑝 </em>(或输入向量<em class="lp"> 𝑋 </em>)作为输入。在线性方程之前，输入<em class="lp"> 𝐴𝑝 </em>被缓存用于向后传递。接下来，我们计算单个矢量化内积中的线性部分。这是对该层中的所有神经元以单一步骤完成的。在内积之后，加上偏置项。接下来，我们还缓存了向后传递的<em class="lp"> 𝑧𝑝 </em>的值。最后，我们将计算激活函数，并将结果传递给下一层(或者如果这是最后一层，这就是输出)。</p><p id="cbb6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">向后传递只是反向的向前传递，但是期望前一个渐变作为输入。首先我们需要计算激活函数的微分。这些都比较容易计算(或者在网上找)。我们使用缓存的𝐴𝑝的值和𝑧𝑝的值来计算梯度。作为最后一步，我们计算<em class="lp"> 𝑑𝐴𝑝 </em>，这将是下一层的输入。虽然数学与之前的教程相似，但现在更加简化了。我不想过多地关注实际的差异，但会在代码中多解释一些。如果你真的想知道这些微分是怎么算出来的，我会建议你拿纸笔试着算一下。他们并不难，Wolfram Alpha 可以帮助你:-)。</p><p id="38a7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">最后一点，你应该意识到，在向后传球中，我们正在做所有<a class="ae lw" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">链式规则</a>之母。为了计算第一层的梯度，我们必须将所有其他的微分链接在一起。如果不完全清楚每个步骤在做什么，<em class="lp">不要太担心</em>。我会试着解释编码过程中的每一步。经验和理解来自实验。</p><blockquote class="lq lr ls"><p id="3af7" class="kt ku lp kv b kw kx kf ky kz la ki lb lt ld le lf lu lh li lj lv ll lm ln lo im bi translated">“我已经说过了”——库伊尔</p></blockquote><h1 id="8c4b" class="ml mm je bd mn mo mp mq mr ms mt mu mv kk mw kl mx kn my ko mz kq na kr nb nc bi translated">逐步实施</h1><p id="f758" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">在我们开始编码之前，让我们先对我们计划做的事情做一个简短的总结。这个想法是创建两个结构，在这个例子中是类。一个类定义层，而另一个类充当父类并保存完整神经网络的所有层。</p><h2 id="1645" class="nw mm je bd mn nx ny dn mr nz oa dp mv lc ob oc mx lg od oe mz lk of og nb oh bi translated">密集层类</h2><ul class=""><li id="c31b" class="lx ly je kv b kw nd kz ne lc oi lg oj lk ok lo ol md me mf bi translated">结构来保存任意数量的节点</li><li id="5386" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">将具有各种激活功能</li><li id="b860" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">将通过单层执行正向传递</li><li id="f217" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">反向传递的缓存值</li><li id="739a" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">将对该层执行向后传递</li><li id="85fa" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">将对图层执行更新步骤</li></ul><h2 id="3e2a" class="nw mm je bd mn nx ny dn mr nz oa dp mv lc ob oc mx lg od oe mz lk of og nb oh bi translated">神经网络类</h2><ul class=""><li id="36f4" class="lx ly je kv b kw nd kz ne lc oi lg oj lk ok lo ol md me mf bi translated">结构来保存任意数量的层</li><li id="b75f" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">将在所有层中顺序执行向前传递</li><li id="8221" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">将计算各种损失函数的成本</li><li id="5fad" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">将执行反向传递并计算所有梯度</li><li id="9298" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">将对所有图层进行更新(梯度下降步骤)</li></ul><h2 id="6638" class="nw mm je bd mn nx ny dn mr nz oa dp mv lc ob oc mx lg od oe mz lk of og nb oh bi translated"><strong class="ak">让我们开始吧！</strong></h2><p id="796a" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">我们将使用Numpy的线性代数例程，因此需要导入它。此外，定义有意义的错误是一种好的做法，因此我们将定义几个异常。另一种方法是使用Python的日志模块，这是另一个很好的工具。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><h2 id="7a22" class="nw mm je bd mn nx ny dn mr nz oa dp mv lc ob oc mx lg od oe mz lk of og nb oh bi translated">致密层:</h2><p id="d98c" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">接下来，让我们创建名为DenseLayer的新类。该类采用一个带有两个必需参数和两个可选参数的构造函数:</p><ul class=""><li id="391d" class="lx ly je kv b kw kx kz la lc lz lg ma lk mb lo ol md me mf bi translated">inputDimension，即输入向量的特征数，或前一层的单元数。</li><li id="447b" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">单位，即该层神经元的数量。</li><li id="5d2f" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">激活:在这里你可以指出层应该使用什么激活。定义了“sigmoid”、“relu”、“tanh”和“”。空字符串表示没有激活，意味着我们只有一个回归。</li><li id="98cc" class="lx ly je kv b kw mg kz mh lc mi lg mj lk mk lo ol md me mf bi translated">randomMultiplier是随机权重相乘的值。通常，0.01就可以了，但有时调整这个数字会有所帮助。</li></ul><p id="b5ed" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">由于可能有不同的激活函数，并且我们不想检查我们使用了if语句的哪个激活，我们在init语句中引用所使用的激活函数。</p><p id="ebf6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在initialize方法中，权重被初始化。注意，神经元的数量<em class="lp"> nh </em>(单位)在行中，输入特征的数量nx是列。这是使我们的点积稍后工作所必需的。</p><p id="96be" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">最近学到的另一件事是Python中‘self’的用法。虽然我认为我理解这个概念，但我并不完全理解其后果。类的定义与每个实例的值是分开的。这些值存储在自身对象中，即实例本身的对象中。在其他编程语言中，通常在类本身中定义类型。然而，在Python中，您必须在init-method中定义它们。如果不这样做，该变量将在所有实例中共享，您可能会得到奇怪的结果。我刚刚发现这篇<a class="ae lw" rel="noopener" target="_blank" href="/python-pitfall-mutable-default-arguments-9385e8265422">博文</a>，其中唐·克罗斯有一个非常清晰的解释。有兴趣推荐阅读！</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="f194" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">接下来，我们定义所有使用的激活函数。我们在前面的教程中已经知道了Sigmoid，但是，我们还包括Tanh和Relu，它们也是非常常用的激活函数。</p><p id="a5d4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">双曲正切函数是一个类似于Sigmoid的函数，但是它将所有实数值映射到-1和+1之间的值。在Relu函数登上舞台之前，它已经非常流行了。</p><p id="fdf8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Relu，代表整流线性单位，可能是目前最流行的激活函数。它的计算速度很快，而且通常比双曲正切函数的结果更好。因此，如果你不确定，Relu函数是一个很好的开始。Relu函数将所有小于0的值映射为零，并将所有大于0的值映射为值本身。</p><p id="4233" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们将介绍的最后一个激活函数称为线性函数。这和没有激活功能是一样的，只是一个占位符。什么进来，什么出来，我们用它来测试我们之前的线性回归练习。</p><p id="879f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">向后传递需要所有这些函数的微分，这些微分带有Grad后缀。请随意检查这些差速器是否正确。请注意，每个函数都有一个额外的步骤，它不是函数微分本身的一部分，但需要应用<a class="ae lw" href="https://en.wikipedia.org/wiki/Differentiation_rules#The_chain_rule" rel="noopener ugc nofollow" target="_blank">链规则</a>。我们将输入的差分乘以计算出的差分。这个步骤在整个反向传播中连续运行。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="a3e5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">接下来，我们定义正向传播步骤，这对于逻辑回归版本来说应该非常熟悉:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="18b4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们首先计算线性部分。存储Z和A的值，供以后在反向传播中使用，然后应用激活函数。</p><p id="d6e8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">反向传播应该看起来也很熟悉，但是，我们已经将激活函数的微分部分拆分为函数本身。此外，该函数期望<em class="lp"> dA </em>作为输入，这是多层的更一般化的形式。在我们之前的单层示例中，我们在这一步中合并了损失函数的微分。这一步现在包含在我们的父类中，而不是每个层中，因为它只在最后一层之前需要。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="1e42" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">梯度存储在每个图层中，以后可由执行梯度下降步骤的更新函数使用。反向传递将每一层链接在一起，我们将在后面处理父类时看到。剩下唯一需要的函数是update函数，它执行梯度下降步骤。没什么了不起的，但它期望一个学习率。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="dbf5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">虽然不是必需的，但这些next函数有助于打印模型并返回输出节点的数量，这些节点用作下一层的输入维度。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><h2 id="14ae" class="nw mm je bd mn nx ny dn mr nz oa dp mv lc ob oc mx lg od oe mz lk of og nb oh bi translated">神经网络类:</h2><p id="4989" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">好吧，搞定一个，还有一个。接下来，我们将创建一个类来合并这些层。它还将保存损失函数，并且必须计算损失的梯度。为了方便起见，我们还将添加一个包装器来添加层，并添加一种方法来漂亮地打印我们的模型。</p><p id="9f2c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">首先，我们再次从构造函数开始，它有两个选项，要使用的损失函数和用于新层的randomMultiplier。损失函数再次在函数引用中创建，并使用包装函数调用。模型初始化时没有层(空)。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="414c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">下一个方法是一个帮助器函数，用于向模型添加层。您需要给出输入维度，即第一层的输入要素的数量。对于第二层和更深的层，它将查找前一层并将其用作输入维度。</p><p id="fa0a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你必须指定层(单元)中神经元的数量以及使用哪个激活函数。如果你不指定激活函数，它将不会使用激活函数，你将得到一个线性系统。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="eea1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">接下来，我们定义损失函数及其微分。请随意检查差速器是否正确。成本方法是一个包装器，用于在训练循环中调用适当的成本函数，我们将在后面定义。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="8692" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">前进、后退和更新方法非常相似，因为它们在所有层上循环。只有后向通道必须首先计算损失函数的梯度，然后将其用作第一层(从右侧开始)的输入。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="fbb9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">后两种方法非常简单，用于漂亮的打印，一种用于打印可训练参数。没什么特别的。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="4a77" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">好了，课程结束了。现在我们需要对这些类进行测试。虽然它看起来非常有序，但其中一个差速器的小错误会使我们的整个系统变得毫无用处。因此，我们将在下一节中分步进行测试。期待旋转！</p><p id="f65c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果想一次复制所有的类，可以从<a class="ae lw" href="https://github.com/dennisbakhuis/Tutorials/tree/master/2_Neural_Network" rel="noopener ugc nofollow" target="_blank"> my Github </a>下载。</p><h1 id="9a8c" class="ml mm je bd mn mo mp mq mr ms mt mu mv kk mw kl mx kn my ko mz kq na kr nb nc bi translated">4.它做了它应该做的事情吗？</h1><p id="dda9" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">没有什么比空等更烦人的了。因此，重要的是首先测试容易的东西，而不是花几个小时训练，看看你在损失函数中犯了一个错误。让我们重复上次实验中做过的实验。</p><h2 id="9888" class="nw mm je bd mn nx ny dn mr nz oa dp mv lc ob oc mx lg od oe mz lk of og nb oh bi translated">线性回归:25再来一遍</h2><p id="ee5b" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">我们用几个输入值<em class="lp"> 𝑋 </em>创建了一个数组。接下来我们用超级复杂的公式<em class="lp"> 𝑦 </em> =2 <em class="lp"> 𝑥 </em> +1提供<em class="lp"> 𝑌 </em>。在Numpy中是这样的:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="1bbb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，我们将构建我们的模型，该模型将尝试找到我们的困难公式并匹配25的真实值。为此，我们将启动我们的新类，并将损失设置为均方误差。接下来，我们将添加一个单层，只有一个神经元，没有激活功能。让我们也试试我们漂亮的印刷:-)</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="3bfc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们将再次需要我们的训练循环。上次的逻辑回归模型看起来很熟悉:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="099e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">成本如预期的那样下降，并且由于长时间的训练，该值接近机器精度。当我们在forward方法中输入12时，我们会再次找到25吗？</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><pre class="nj nk nl nm gt oo op oq or aw os bi"><span id="3a23" class="nw mm je op b gy ot ou l ov ow">array([[25.00000019]])</span></pre><p id="30b0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当然，我们有！</p><h2 id="b772" class="nw mm je bd mn nx ny dn mr nz oa dp mv lc ob oc mx lg od oe mz lk of og nb oh bi translated">逻辑回归:泰坦尼克号数据集的又一次尝试</h2><p id="cf15" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">坏消息，泰坦尼克号又沉了，我们需要一个二元逻辑分类器。没有过多评论，我们导入数据(来源:<a class="ae lw" href="https://www.kaggle.com/azeembootwala/titanic" rel="noopener ugc nofollow" target="_blank"> Kaggle泰坦尼克号比赛/由Azeem Bootwala </a>预处理):</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="9a8e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们需要创建一个有14个输入特征的模型，当然还有sigmoid激活函数。损失函数将是交叉熵，这是默认的，所以我们不需要指定它。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="e6e9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在创建训练循环之前，我们像上次一样定义一个精度函数来快速计算精度。上次我们有大约80%的准确率。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="6d90" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，让我们运行训练循环，看看我们是否能匹配前面的结果。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><pre class="nj nk nl nm gt oo op oq or aw os bi"><span id="3fc0" class="nw mm je op b gy ot ou l ov ow">cost: 0.6912384614194468 	accuracy: 58.7%<br/>cost: 0.5140056920509907 	accuracy: 75.9%<br/>cost: 0.4824062497934128 	accuracy: 79.5%<br/>cost: 0.4697113881273363 	accuracy: 79.5%<br/>cost: 0.46275860969195476 	accuracy: 79.8%<br/>cost: 0.45807429458700816 	accuracy: 79.8%<br/>cost: 0.4544915367177524 	accuracy: 79.8%<br/>cost: 0.4515471545581365 	accuracy: 80.8%</span></pre><p id="ecc4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如你所见，一点也不差。</p><h2 id="4333" class="nw mm je bd mn nx ny dn mr nz oa dp mv lc ob oc mx lg od oe mz lk of og nb oh bi translated">更多的神经元和层:复杂的花朵</h2><p id="02d1" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">酷的东西从更复杂的功能开始。来自吴恩达的<a class="ae lw" href="https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习课程</a>展示了一种使用多节点模型预测<a class="ae lw" href="https://en.wikipedia.org/wiki/Rose_(mathematics)" rel="noopener ugc nofollow" target="_blank">玫瑰函数</a>的方法。让我们也试试吧！</p><p id="1aef" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">首先，我们需要导入数据。生成数据的代码在<a class="ae lw" href="https://github.com/dennisbakhuis/Tutorials/tree/master/2_Neural_Network" rel="noopener ugc nofollow" target="_blank"> Github </a>上的Jupyter笔记本里。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/4b4e75a2214c6cf7342753f0aba311b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PHx9EtHoZOYexQztPXkN8w.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">图7:输入数据的可视化。</p></figure><p id="69a8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">数学方程式创造了美丽的花状结构，如图7所示。我们已经将7个花瓣中的3个着色为不同的值，现在将使用我们的神经网络来预测x，y坐标上的值应该是0还是1(颜色)。</p><p id="d796" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">但在此之前，让我们先展示一下，当我们试图用一个过于简单的模型来预测时会发生什么，比如逻辑回归，它只能有一个线性边界。在下一个代码片段中，我包含了模型创建、训练循环和可视化结果的帮助函数。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="8177" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">结果显示在图8的左侧。肯定不是很合适，而且明显是一条线。现在让我们添加另一个有四个单元的层到模型中，最后一层是相同的Sigmoid层。我们保留的这个模型的激活函数与Andrew的相似，它有一个Tanh激活函数。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/53bcfb0ae2cd728097863ebb0f6d904c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rbOmjo4GAhIN5dA7apwSXg.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">图8:同一花形函数的两种拟合。在图8a中，逻辑回归模型只能拟合一条线，而图8b中的两层模型可以拟合花的复杂形状。</p></figure><p id="fc9e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这真的让我很惊讶。再多一层，我们就有能力学习这个更复杂的功能。非常棒的东西！</p><h1 id="8a8b" class="ml mm je bd mn mo mp mq mr ms mt mu mv kk mw kl mx kn my ko mz kq na kr nb nc bi translated">5.符合从穆斯塔法截获的信息</h1><p id="02da" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">因为今天是5月4日(与你同在)，我们截获了一份来自穆斯塔法的秘密电报，这不可能是巧合。最重要的是，我们用这些数据训练我们的模型，这样我们的ai (read: model)就可以完全理解这种传输的本质。</p><p id="0a8d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">第一步是导入数据，即2d传输数据。我还创建了一些辅助函数来创建模型，一个训练循环，以及一个可视化和分析传输的函数。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="8640" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当你创建更大的模型，尤其是更复杂的模型时，你会发现超参数的数量会大大增加。有关于如何选择它们的指导方针，但没有金科玉律，因此，你经常要尝试很多。</p><p id="dd6a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我做了一个参数扫描，其中我自动搜索以找到最佳的层数、单元数和迭代数。我做得很粗糙，花了15个小时运行了近100个模型。参数搜索包含在Jupyter笔记本的附录部分。以下参数给出了很好的结果，损耗低于0.09。损失值本身没有意义，它只是预测值与真实值匹配程度的度量，应该最小化。你可以自己尝试这些设置。如果发现更好的参数，请分享给我:-)。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><pre class="nj nk nl nm gt oo op oq or aw os bi"><span id="e476" class="nw mm je op b gy ot ou l ov ow">[<br/>  1 -&gt; Dense layer (nx=2, nh=27, activation=relu)<br/>  2 -&gt; Dense layer (nx=27, nh=31, activation=relu)<br/>  3 -&gt; Dense layer (nx=31, nh=31, activation=relu)<br/>  4 -&gt; Dense layer (nx=31, nh=1, activation=sigmoid)<br/>]<br/>There are 1973 trainable parameters in the model.</span></pre><p id="5a22" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们的模型有四个连续的层。前三个有许多单元和Relu激活功能。最后一层是简单的逻辑层。我们的实现没有pyTorch或Tensorflow中提供的所有功能(可以随意添加)。一件很棒的事情是在训练时调整学习速度。首先以较大的学习速率开始，当你接近收敛点时降低学习速率。下面的方法就是模拟这一点。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><pre class="nj nk nl nm gt oo op oq or aw os bi"><span id="6d60" class="nw mm je op b gy ot ou l ov ow">1000 -&gt; cost: 0.42574001278351997<br/>2000 -&gt; cost: 0.310616390460577<br/>2000 -&gt; cost: 0.21932529403735668<br/>2000 -&gt; cost: 0.15604826952436227<br/>4000 -&gt; cost: 0.11089920071653515<br/>5000 -&gt; cost: 0.09616265347246447<br/>8000 -&gt; cost: 0.09046567290400138<br/>4000 -&gt; cost: 0.08879263912657591</span></pre><p id="7ccb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">经过28000次迭代后，我们达到了一个足以分析数据的收敛状态。为此，我们将使用testModel函数。使用<em class="lp"> h </em>参数，我们可以<a class="ae lw" href="https://www.youtube.com/watch?v=Vxq9yj2pVWk" rel="noopener ugc nofollow" target="_blank">增强我们消息的分辨率</a>。各种“增强”的结果如图9所示。</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="om on l"/></div></figure><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/be324d651128f104d64f9d4ae741a239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gZjxaZKrNy43H2KuTxGldQ.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">图9:现在我们发现了一个可怕的事实:这条信息是关于西斯尊主本人的！</p></figure><p id="a0c9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">哦，天哪，是黑魔王本人！啊啊！当然，没有提高分辨率这样的事情，我们只是用更大的网格来填充神经网络。结果还是蛮牛逼的，我必须说我自己:-)。</p><p id="6135" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">下一个很酷的尝试是使用RGB图像作为输入。为此，您需要调整输入图像的形状，使RGB通道垂直堆叠。对于我们当前的输入图像，这意味着三倍长的输入向量(三个通道)。创建和操作输入数据的程序可在<a class="ae lw" href="https://github.com/dennisbakhuis/Tutorials/tree/master/2_Neural_Network" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>中找到。输入图像的剪贴画源是从<a class="ae lw" href="http://niceclipart.com/6378/star-wars-darth-vader.html" rel="noopener ugc nofollow" target="_blank">niceclipart.com</a>下载的。</p><h1 id="9d5e" class="ml mm je bd mn mo mp mq mr ms mt mu mv kk mw kl mx kn my ko mz kq na kr nb nc bi translated">围捕</h1><p id="ace0" class="pw-post-body-paragraph kt ku je kv b kw nd kf ky kz ne ki lb lc nf le lf lg ng li lj lk nh lm ln lo im bi translated">我希望你们玩得开心，写自己的神经网络。在我看来，从头开始写这些东西是了解它实际上是如何工作的最好方法。我希望你能看到这些系统并不神奇，只是简单的矩阵乘法，不幸的是它们非常多。最困难的部分当然是反向传播，我们需要计算梯度。我们简单的神经网络是相当可行的，但增加更多的层和不同类型的层，会使它变得有点麻烦。尽管如此，本质还是和我们今天所做的非常相似。</p><p id="ae95" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我的建议是尝试一下这些结构，重写它们的一部分，或者更好，从头开始写你自己的！</p><blockquote class="lq lr ls"><p id="e568" class="kt ku lp kv b kw kx kf ky kz la ki lb lt ld le lf lu lh li lj lv ll lm ln lo im bi translated">这是一条路</p></blockquote><p id="f209" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果您有任何意见，请告诉我！在<a class="ae lw" href="https://linkedin.com/in/dennisbakhuis" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上随意联系。</p></div></div>    
</body>
</html>