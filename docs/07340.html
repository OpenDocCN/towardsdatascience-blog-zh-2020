<html>
<head>
<title>Multilingual Document Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多语言文档分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multilingual-document-classification-2731ce8c0163?source=collection_archive---------36-----------------------#2020-06-03">https://towardsdatascience.com/multilingual-document-classification-2731ce8c0163?source=collection_archive---------36-----------------------#2020-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/214a9c58e6b99d014eac69c23e4adcfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b6yWdZ9dav_JiJz7llHK3w.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><div class=""/><div class=""><h2 id="564f" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">如何构建语言无关的NLP应用程序？</h2></div><p id="aaf7" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">利用非结构化数据正成为在基于数据的业务中维持和超越的必要条件。开发能够提高NLP模型性能的文本特性需要语言学、数据科学和商业领域专业知识的结合。</p><p id="8e7f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这对一家初创企业来说已经足够了，但如果你的产品成为病毒，你走向全球，那该怎么办？</p><h1 id="20bf" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">影响</h1><p id="0bfb" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">想象一下，一家医疗保健服务机构根据英语临床研究为患者提供量身定制的剂量计划，从而提供最佳的医疗服务。如果你能添加其他语言，并利用中国或日本的最新研究成果，会怎么样？</p><h1 id="f955" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">挑战</h1><p id="089f" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">对于数据科学家来说，开发一个成功的NLP模型意味着拥有带有文档标签的高质量语料库。不幸的是，大部分都是英文版本。当然，人们可以收集和标记其他语言的训练数据，但这将是昂贵和耗时的。</p><p id="6593" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">幸运的是，有一种有趣的替代方法叫做<strong class="kz jj">跨语言文档分类</strong>。</p><blockquote class="mr ms mt"><p id="fa07" class="kx ky mu kz b la lb kj lc ld le km lf mv lh li lj mw ll lm ln mx lp lq lr ls im bi translated">它的目的是在一种语言的数据集上训练一个文档分类器，并将其预测能力推广到其他语言，而不需要其他语言的数据集。</p><p id="4660" class="kx ky mu kz b la lb kj lc ld le km lf mv lh li lj mw ll lm ln mx lp lq lr ls im bi translated">这个想法是使用独立于语言的单词或整个句子的表示。</p></blockquote><h1 id="5063" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">使用案例</h1><p id="2372" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">不要再拖延了，让我们立即行动起来，构建一个多语言新闻标题分类器。</p><p id="ec49" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">该数据集包括来自35种不同语言的各种媒体公司的72000篇文章标题。它们被分为4个互斥的主题标签。任务是预测给定任何语言标题的标签。</p><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/9aa5466ce165c20b9ebe1cb1d720e951.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*vGlvPD-xyAJfPNfvhjdvBQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">关于标签的高度不平衡的数据集</p></figure><h1 id="1043" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">NLP管道</h1><p id="7c37" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">如何解决如此复杂的问题？我们分阶段一步一步分解吧。</p><figure class="mz na nb nc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nd"><img src="../Images/cbe6a9b80309726e7d78c571cb22f3de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQj0wTyr-vlTHPz0p5IO-g.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">NLP管道</p></figure><h2 id="0366" class="ne lv ji bd lw nf ng dn ma nh ni dp me lg nj nk mg lk nl nm mi lo nn no mk np bi translated">第一步。解析和清理HTML</h2><p id="29d5" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">语料库似乎是从HTML代码中刮出来的，因为它包含许多HTML字符代码；有些坏了。已经创建了一个小型解析器来清理标题。</p><h2 id="5106" class="ne lv ji bd lw nf ng dn ma nh ni dp me lg nj nk mg lk nl nm mi lo nn no mk np bi translated">第二步。语言检测</h2><p id="9ccd" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">似乎语言类型将是标题分类的一个极好的预测器。从<a class="ae lt" href="https://en.wikipedia.org/wiki/Chi-squared_test" rel="noopener ugc nofollow" target="_blank">卡方检验</a>的结果来看，文字新闻大部分是用英语写的，而体育和科技文章则是非英语语言写的。</p><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/87bbed3cb362c0b1e80292cbe01c30b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*QdeZwxgpdQ6eHubCFz2HqQ.png"/></div></figure><p id="c6df" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">现在让我们使用脸书的<a class="ae lt" href="https://fasttext.cc/" rel="noopener ugc nofollow" target="_blank">快速文本</a>库来检测语言。</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="0a05" class="ne lv ji ns b gy nw nx l ny nz">import fastext<br/>import pandas as pd</span><span id="bf8d" class="ne lv ji ns b gy oa nx l ny nz">def predict_language(text, language_classifier):<br/>    language = language_classifier.predict(text)<br/>    return language</span><span id="c2fe" class="ne lv ji ns b gy oa nx l ny nz">language_classifier = fasttext.load_model('lid.176.ftz')<br/>df = pd.read_csv('headlines.csv').to_frame()</span><span id="5f93" class="ne lv ji ns b gy oa nx l ny nz">df['language'] = (df['headline']<br/>                  .apply(predict_language, <br/>                         args=(language_classifer)))</span></pre><h2 id="2a35" class="ne lv ji bd lw nf ng dn ma nh ni dp me lg nj nk mg lk nl nm mi lo nn no mk np bi translated">步骤3–4。通用多语言句子编码(MUSE)</h2><p id="caab" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">进一步的管道设计需要很好地理解<a class="ae lt" href="https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3" rel="noopener ugc nofollow" target="_blank"> MUSE模型</a>以及它是如何被训练的。</p><blockquote class="mr ms mt"><p id="ff3a" class="kx ky mu kz b la lb kj lc ld le km lf mv lh li lj mw ll lm ln mx lp lq lr ls im bi translated">简单明了的MUSE将句子转换成一个数字向量。不同语言中具有相似意义的句子向量之间有着密切的几何距离。</p></blockquote><p id="be6c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">该模型已在16种语言中进行了训练，因此有必要应用过滤器来删除管道中的其他语言。这是可行的，因为只有3%的数据集超出了MUSE的范围。</p><p id="0bac" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">重要的是执行与在模型训练期间已经完成的相同的文本预处理步骤。好消息是它们已经嵌入到TensorFlow的model <code class="fe ob oc od ns b"><em class="mu">predict</em></code>函数中。因此，除了删除下面的特殊字符之外，不需要额外的预处理。</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="2124" class="ne lv ji ns b gy nw nx l ny nz">special_chars = [<br/>    ',', '.', '"', ':', ')', '(', '-', '!', '?', '|', ';', "'", '/', <br/>    '[', ']', '&gt;', '%', '=', '#', '*', '+', '\\', '•',  '~', '·', <br/>    '_', '{', '}', '©', '^', '®', '`',  '&lt;', '→', '°', '™', '›','♥',<br/>    '←', '×', '§', '″', '′', '█', '½', '…', '“', '★', '”','–', '●', <br/>    '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾','═', '¦',<br/>    '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '⊕', '▼', '▪',  <br/>    '†', '■', '’', '▀', '¨', '▄', '♫', '☆', '¯', '♦', '¤', '▲', '∞',<br/>    '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³',<br/>    '・', '╦', '╣', '╔', '╗', '▬','❤', 'ï', 'Ø', '¹', '≤', '‡', '√'<br/>]</span></pre><p id="3c42" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">TensorFlow完成了这项工作，将句子嵌入为长度为512的数字向量变得毫不费力。</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="df9b" class="ne lv ji ns b gy nw nx l ny nz">import tensorflow_hub as hub<br/>import numpy as np<br/>import tensorflow_text</span><span id="1bd4" class="ne lv ji ns b gy oa nx l ny nz">embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3")</span><span id="7d51" class="ne lv ji ns b gy oa nx l ny nz">X_embeded = embed(df['filtered_cleaned_headline'].values)</span></pre><h2 id="6dc8" class="ne lv ji bd lw nf ng dn ma nh ni dp me lg nj nk mg lk nl nm mi lo nn no mk np bi translated">第五步。定性和定量特征的融合。</h2><p id="f3ec" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">寻找合适的特性来提高模型性能是商业经验、科学和艺术的结合。所以，让我们看看除了嵌入和语言特性之外我们还能做些什么。</p><p id="6935" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">从这三组平均值之间的差异来看，标题文本长度可能是区分“体育”、“商业+技术”和“世界”相关标题的良好预测指标。</p><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/ca7ba0fd54f200bfc199bac52e532a98.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*z6M5KqVfRJfeA1g4g_tLng.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">方差分析(ANOVA)</p></figure><h2 id="c8c0" class="ne lv ji bd lw nf ng dn ma nh ni dp me lg nj nk mg lk nl nm mi lo nn no mk np bi translated">第六步。编码的降维</h2><p id="b94f" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">拥有512个特征是一个相当大的数字，会导致很长的训练时间，它的内存和计算开销很大，并且容易导致模型过度拟合。</p><blockquote class="mr ms mt"><p id="bc4f" class="kx ky mu kz b la lb kj lc ld le km lf mv lh li lj mw ll lm ln mx lp lq lr ls im bi translated">过度拟合意味着模型不学习。它只是记忆训练数据，但它无法准确预测尚未看到的数据。</p></blockquote><p id="0591" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">考虑到嵌入/编码的维数减少，有几种选择:</p><ol class=""><li id="c530" class="of og ji kz b la lb ld le lg oh lk oi lo oj ls ok ol om on bi translated"><a class="ae lt" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank">t-分布式随机邻居嵌入(t-SNE) </a></li><li id="e6e7" class="of og ji kz b la oo ld op lg oq lk or lo os ls ok ol om on bi translated"><a class="ae lt" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析</a> (PCA)</li><li id="132d" class="of og ji kz b la oo ld op lg oq lk or lo os ls ok ol om on bi translated"><a class="ae lt" href="https://github.com/vyraun/Half-Size" rel="noopener ugc nofollow" target="_blank"> PCA +后处理算法</a></li></ol><p id="ba21" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">t-SNE保持嵌入之间的相对距离，这是很大的，但它在计算上是昂贵的。PCA用于快速原型制作，因为它快速且完全集成到scikit-learn管道中。但我会建议尝试选项3，它在这篇<a class="ae lt" href="https://arxiv.org/pdf/1708.03629.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中被证明更优越。</p><h2 id="5d88" class="ne lv ji bd lw nf ng dn ma nh ni dp me lg nj nk mg lk nl nm mi lo nn no mk np bi translated">第七步。分类器筛选</h2><p id="6e3f" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">NLP管道内置于scikit-learn中，支持测试不同类型的模型。</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="9acf" class="ne lv ji ns b gy nw nx l ny nz"><em class="mu"># Design NLP pipeline</em><br/>pipeline = Pipeline([<br/>    <em class="mu"># Fusion of features                 </em><br/>    ('fuser', ColumnTransformer(<br/>        [    <br/>            <em class="mu"># Transform MUSE encodings</em><br/>            ('embedings', Pipeline([</span><span id="983d" class="ne lv ji ns b gy oa nx l ny nz">                # Standardize MUSE encodings                   <br/>                ('emb_scaler', StandardScaler()),<br/>   <br/>                # Dimensionality reduction of encodings<br/>                ('emb_reducer', PCA(<br/>                    n_components=0.75,<br/>                    random_state=random_state))]),</span><span id="a6ea" class="ne lv ji ns b gy oa nx l ny nz">              # Apply to only encodings columns<br/>              emb_idx),<br/>            <br/>            <em class="mu"># Standardize title length feature</em><br/>            ('title_length', StandardScaler(), ['title_length']),</span><span id="9e28" class="ne lv ji ns b gy oa nx l ny nz"><em class="mu">            # One-hot encode language feature</em><br/>            ('language', OneHotEncoder(drop='first'), ['language'])            <br/>       ]    <br/>   )),</span><span id="c786" class="ne lv ji ns b gy oa nx l ny nz"><em class="mu">    # Determine classification model in the GridSearchCV</em><br/>    ('classifier', None)<br/>])</span></pre><p id="8ffa" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">由于不平衡的标签，<code class="fe ob oc od ns b"><a class="ae lt" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank">f1_weigted</a></code>和<code class="fe ob oc od ns b"><a class="ae lt" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" rel="noopener ugc nofollow" target="_blank">f1_macro</a></code>分数用于模型评估。前者考虑了标签在数据集中的分布，而后者侧重于类之间的模型判别能力，给予每个标签相同的权重。</p><p id="fd39" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">将选定的模型与以下参考资料进行比较:</p><ol class=""><li id="7052" class="of og ji kz b la lb ld le lg oh lk oi lo oj ls ok ol om on bi translated">考虑数据集中标签分布的随机分类器生成器</li><li id="e126" class="of og ji kz b la oo ld op lg oq lk or lo os ls ok ol om on bi translated">最频繁分类器总是选择最频繁的标签。</li></ol><figure class="mz na nb nc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/20b67c7e78e6256ed77360d54ae4d9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EJeyUYMXIzc6AeKkq1HCJA.png"/></div></div></figure><p id="6c50" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">最佳选择是逻辑回归。支持向量分类器(SVC)和K-最近邻分类器(KNN)试图记忆数据集。尽管调整会改善过度拟合，但由于他们长期的训练和预测，这并不明智。所有模型的表现明显优于参考。</p><h1 id="aef0" class="lu lv ji bd lw lx ly lz ma mb mc md me ko mf kp mg kr mh ks mi ku mj kv mk ml bi translated">行动呼吁</h1><p id="463e" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">就是这样！多语言文档分类在实践中。仍然有很多工作要做，以微调模型并使其为生产做好准备，但这是另一篇要讨论的文章。如果你想亲自尝试一下，我推荐以下资源:</p><ul class=""><li id="8764" class="of og ji kz b la lb ld le lg oh lk oi lo oj ls ou ol om on bi translated"><a class="ae lt" href="https://medium.com/@d.salvaggio/multilingual-universal-sentence-encoder-muse-f8c9cd44f171" rel="noopener">多语种通用句子编码器(MUSE) </a>作者:大卫·萨尔瓦吉奥</li><li id="8e05" class="of og ji kz b la oo ld op lg oq lk or lo os ls ou ol om on bi translated"><a class="ae lt" href="https://www.aclweb.org/anthology/L18-1560.pdf" rel="noopener ugc nofollow" target="_blank">八种语言的多语言文档分类语料库</a>由李贤的Holger Schwenk开发</li><li id="eda8" class="of og ji kz b la oo ld op lg oq lk or lo os ls ou ol om on bi translated"><a class="ae lt" href="https://www.tensorflow.org/tutorials/text/word_embeddings" rel="noopener ugc nofollow" target="_blank">谷歌的TensorFlow嵌入文档</a></li><li id="f120" class="of og ji kz b la oo ld op lg oq lk or lo os ls ou ol om on bi translated"><a class="ae lt" href="https://arxiv.org/pdf/1708.03629.pdf" rel="noopener ugc nofollow" target="_blank">简单有效的单词嵌入降维</a>Vikas Raunak</li></ul></div></div>    
</body>
</html>