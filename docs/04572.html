<html>
<head>
<title>Word and Text Representations in Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的单词和文本表示</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-and-text-representations-in-natural-language-processing-33349649be2e?source=collection_archive---------21-----------------------#2020-04-23">https://towardsdatascience.com/word-and-text-representations-in-natural-language-processing-33349649be2e?source=collection_archive---------21-----------------------#2020-04-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f7e4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从本地到分布式</h2></div><p id="0ccb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词是许多自然语言中的基本结构。这些语言中的字符在排列成词之前是没有意义的。当这发生的时候，维奥拉，意义出现了！当单词按顺序排列时，比如在短语或句子中，更多的意思就会出现。</p><p id="dfb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不足为奇的是，核心NLP操作包括适当地处理单词或单词序列。(在这篇文章中，一系列的单词，我们将其缩写为<em class="le"> text </em>，将会是一个短语、一个句子、一个段落、一个部分、一个章节或者整个文档。为了符号的简单，我们也将标点符号视为单词。)</p><p id="fb48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是支持许多用例的文本关键操作的清单。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="fdbb" class="lo lp it lk b gy lq lr l ls lt"><strong class="lk iu"><em class="le">Text classification</em></strong>: Classify text into one of a predefined set of categories. </span><span id="1902" class="lo lp it lk b gy lu lr l ls lt"><strong class="lk iu"><em class="le">Text similarity</em></strong>: Score how similar two pieces of text (sentences, paras, …, entire documents) are.</span><span id="86a3" class="lo lp it lk b gy lu lr l ls lt"><strong class="lk iu"><em class="le">Text clustering</em></strong>: Cluster several texts — such as documents — into groups of similar ones.</span><span id="ad68" class="lo lp it lk b gy lu lr l ls lt"><strong class="lk iu"><em class="le">Keywords extraction</em></strong>: Extract salient keywords from a corpus of text (e.g. documents).</span><span id="d791" class="lo lp it lk b gy lu lr l ls lt"><strong class="lk iu"><em class="le">Topics discovery</em></strong>: Discover sets of keywords that go together, i.e. topics, from a corpus of text.</span></pre><p id="3ef2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所有这些都涉及到文字操作。有些，比如<em class="le">文本相似度</em>，涉及到对单词序列的操作。</p><p id="a14c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">当地文字表述</strong></p><p id="2380" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">按Id </strong></p><p id="42c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，每个不同的单词被分配一个唯一的标识符。举个例子，比方说我们的词典只有10个单词:<em class="le">狗</em>、<em class="le">猫</em>、<em class="le">虎</em>、<em class="le">象</em>、<em class="le">黑</em>、<em class="le">黑</em>、<em class="le">白</em>、<em class="le">是</em>、<em class="le">大</em>、<em class="le">快</em>。下面描述了该词典的一种本地编码。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="89ae" class="lo lp it lk b gy lq lr l ls lt">Word dog cat tiger elephant the black white is are and<br/>Id    0   1    2      3      4    5     6    7  8   9</span></pre><p id="86f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在考虑正文:<em class="le"/><em class="le">狗和猫是黑色的</em>。这将被表示为序列4 0 9 1 8 5。</p><p id="c3fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种表示保留了文本中的所有信息。这很好。另一方面，不同长度的文本产生不同长度的序列，使得它们难以比较。</p><p id="cf99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">向量空间中的</strong></p><p id="9fba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种表示中，词典中的每个单词在向量空间中形成它自己的维度。所以在上面的例子中，我们的空间有10个维度。单词由二进制向量表示，其中单词的维数值为1，其余的为0。我们将把这个向量称为单词的<em class="le">指示向量</em>。</p><p id="8d02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是我们运行的例子中的两个单词指示向量。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="2f32" class="lo lp it lk b gy lq lr l ls lt">dog   → <strong class="lk iu">1</strong>000000000<br/>black → 00000<strong class="lk iu">1</strong>0000</span></pre><p id="4731" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，现在重点来了。任意长度的文本也被映射为同一空间中的向量。这使得比较不同长度的文本变得容易。只需使用代数相似性度量来比较它们的向量，如余弦相似性或基于欧几里德距离的度量或其他度量。</p><p id="bd8d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">文本向量</strong></p><p id="ca72" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么，文本，即任意长度的单词序列，是如何映射到这个空间中的向量的呢？有各种可能性。最简单的方法是将文本中出现的单词的指示向量相加，并进行二进制化(即，将正计数截断为1)。下面是我们在例子中得到的向量。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="925c" class="lo lp it lk b gy lq lr l ls lt">the dog and cat are black → 1100110010</span></pre><p id="fbc3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个向量只是捕获词典中的哪些单词在文本中，哪些没有。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="2f1e" class="lo lp it lk b gy lq lr l ls lt"><strong class="lk iu">1</strong>    <strong class="lk iu">1</strong>    0      0      <strong class="lk iu">1</strong>    <strong class="lk iu">1</strong>     0    0  <strong class="lk iu">1</strong>   0<br/><strong class="lk iu">dog</strong> <strong class="lk iu">cat</strong> tiger elephant <strong class="lk iu">the</strong> <strong class="lk iu">black</strong> white is <strong class="lk iu">are</strong> and</span></pre><p id="6c9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种表示也称为单词包，忽略文本中的所有词序。有时这实际上是有益的。<em class="le">猫和狗是黑色的</em> <strong class="kk iu"> </strong>和<strong class="kk iu"> </strong> <em class="le">狗和猫是黑色的</em> <strong class="kk iu"> </strong>的矢量表示是一样的。这很好，因为猫和狗在这里出现的顺序无关紧要。</p><p id="8736" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">增强版</strong></p><p id="b58e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">词频</strong></p><p id="cb0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑长文本，比如长文档。各种单词出现的频率也很重要。例如，考虑两个10000字的文档，一个文档中<em class="le">医院</em>被提及200次，一个文档中<em class="le">医院</em>仅被提及一次。第一份文件中<em class="le">医院</em>的出现比第二份文件更为显著。</p><p id="c67c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">捕获词频的文本编码很容易获得。只需将文本中出现的所有单词的指示向量相加。不要二值化就好。</p><p id="4f65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> TF X IDF </strong></p><p id="d6d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在长文档中，常见的词语如<em class="le">中的</em>、<em class="le">中的</em>、中的<em class="le">、<em class="le">和</em>会频繁出现。基于频率的表示将过度表示它们的重要性。我们不希望仅仅因为两个文档中都出现了大量的<em class="le">和</em>就认为它们是相似的。是的，频率很重要，但仅仅是信息性的词。在我们的例子中，<em class="le">医院</em>是一个信息词，而<em class="le">医院</em>不是。</em></p><p id="d9a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TF X IDF表示首先涉及给词典中的每个单词分配一个信息量分数。这个分数是从不同的文本语料库中估计的，例如从不同类型的文档中提取的句子(或者甚至段落)。一个单词出现的文本越少，它的信息量就越大。这就是为什么这一措施被称为“逆文档频率”。</p><p id="f794" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了各种单词的IDF分数，很容易获得文本的TF X IDF向量。浏览课文中出现的所有单词。将每个出现的指示向量乘以其单词的IDF分数。把它们都加起来。这将产生一个向量，其中单词的贡献将取决于它在文档中的频率，该频率由单词的信息性(即IDF)分数调整。在不同语料库中不常见但在特定文本中频繁出现的单词在该文本中将被认为更重要。</p><p id="4d1e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个例子。考虑这篇课文，狗和猫是黑色的。我们前面看到了它的词频表示:1100110010。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="bd7e" class="lo lp it lk b gy lq lr l ls lt"><strong class="lk iu">1</strong>    <strong class="lk iu">1</strong>    0      0      <strong class="lk iu">1</strong>    <strong class="lk iu">1</strong>     0    0  <strong class="lk iu">1</strong>   0<br/><strong class="lk iu">dog</strong> <strong class="lk iu">cat</strong> tiger elephant <strong class="lk iu">the</strong> <strong class="lk iu">black</strong> white is <strong class="lk iu">are</strong> and</span></pre><p id="5232" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TF X IDF表示可能更像这样。(不要把正数看得太字面。)</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="6770" class="lo lp it lk b gy lq lr l ls lt"><strong class="lk iu">1</strong>    <strong class="lk iu">1</strong>    0      0     <em class="le">0.1</em>    <strong class="lk iu">1</strong>    0    0  <em class="le">0.1</em>   0<br/><strong class="lk iu">dog</strong> <strong class="lk iu">cat</strong> tiger elephant <em class="le">the</em>  <strong class="lk iu">black</strong> white is <em class="le">are</em>   and</span></pre><p id="e2d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">与</em>和<em class="le">为</em>的字样影响力大大降低。</p><p id="fe94" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">局部表征无法捕捉单词相似度</strong></p><p id="03dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本地表示无疑非常有用——实际上被广泛使用。也就是说，它们有一个限制，这是更广泛使用的障碍。他们无法解释单词的相似性。</p><p id="6d69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑三句话:</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="05fa" class="lo lp it lk b gy lq lr l ls lt"><strong class="lk iu">S1</strong>: <em class="le">He is a lawyer <br/></em><strong class="lk iu">S2</strong>: <em class="le">He is a attorney <br/></em><strong class="lk iu">S3</strong>: <em class="le">He is a doctor</em></span></pre><p id="f54f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于<em class="le">律师</em>和<em class="le">律师</em>是同义词，我们可以看到S1和S2在语义上是相同的，而S3与两者都没有关系。因为<em class="le">律师</em>、<em class="le">律师</em>和<em class="le">医生</em>将是三个不同的维度，所以当地的代表无法做出这种区分。</p><p id="95d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以用两种方法中的一种来缓解这种情况。</p><ol class=""><li id="4373" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">我们可以用同义词词典来扩充局部表示。</li><li id="7e2d" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">我们可以计算维度之间的相关性，并在我们的评分中考虑这些因素。</li></ol><p id="65aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然基于字典的方法在实践中通常是有效的，但是它们不能推导出比字典中明确表示的关系更微妙的关系。作为一个简单例子考虑</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="2c11" class="lo lp it lk b gy lq lr l ls lt"><strong class="lk iu">S5</strong>: He teaches computer science<br/><strong class="lk iu">S6</strong>: He teaches data science<br/><strong class="lk iu">S7</strong>: He teaches art</span></pre><p id="20b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们希望能够认为S5和S6在语义上比S6和S7更相似。</p><p id="3643" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个例子只是冰山一角。在外面的数百万个单词中，想想有多少单词相似关系被埋没了！现在把这个扩展到专有名词。举个例子，我们想捕捉到<em class="le">佳能</em>和<em class="le">相机</em>正相关。</p><p id="de06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">维度的相关性分析可以缓解这些问题。从一个大的文档语料库，如维基百科，进一步细分为更小的块，如段落，我们可能能够推断出“计算机科学”和“数据科学”是正相关的，而“计算机科学”和“艺术”是不相关的——甚至可能是负相关的。</p><p id="6fd5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们不会描述应用相关性分析来缓解这些问题的无数种方法。相反，我们将关注一种对单词进行替代编码的方法，称为<em class="le">分布式表示</em>，它在幕后进行一种形式的相关性分析。</p><p id="f194" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">分布式单词表示法</strong></p><p id="cdd2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个单词的本地表示只有一个值为1的位，即标识该单词的位。因此，任何两个字的指示符向量只有两位不同。(参见下面的例子。)也就是说，所有的指示向量对都是等距的。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="83c4" class="lo lp it lk b gy lq lr l ls lt">dog      <strong class="lk iu">1</strong>000000000<br/>Elephant 000<strong class="lk iu">1</strong>000000</span></pre><p id="4f11" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相反，如果有可能将单词映射到向量，使相似的单词映射到附近的向量，那会怎么样呢？这是有可能的，尽管会涉及更多的问题。首先，我们需要一个关键概念。</p><p id="a39d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">上下文</strong></p><p id="4e1a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个单词的上下文是指在它附近频繁出现的单词，比如在同一个句子中。</p><p id="2297" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为什么这个概念对我们有用？因为具有相似上下文表示的两个单词将是正相关的。就拿“猫”和“络腮胡”来说吧。它们将具有相似的语境表征，因为它们都将在涉及猫的语境中共同出现。另一方面，“猫”和“木星”不会出现在相同的上下文中。因此，通过比较上下文表征，我们可以推断第一对是相关的，而第二对是不相关的。</p><p id="c4f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，现在让我们来形式化这个概念。我们将一个单词的上下文建模为词典中所有单词的概率分布。我们称之为单词的<em class="le">上下文向量</em>。这是我们原始局部表示空间中的一个向量。与指示向量不同，它是分布式的。事实上，指示向量是一个特例，它的上下文是单词本身(不多也不少)。</p><p id="d2cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">举例</strong></p><p id="a495" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">比方说我们的词典里就有四个词<em class="le">猫</em>、<em class="le">胡须</em>、<em class="le">电脑</em>和<em class="le">笔记本电脑。</em>它们的指示向量分别是1000，0100，0010，0001。没有配对重叠。相比之下，这四个单词的可能上下文向量可能如下。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="e00b" class="lo lp it lk b gy lq lr l ls lt"><strong class="lk iu">word      cat whiskers computer laptop<br/></strong>cat       0.7   0.3       0       0<br/>whiskers  0.3   0.7       0       0<br/>computer  0     0         0.7     0.3<br/>laptop    0     0         0.3     0.7</span></pre><p id="515b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们看到<em class="le">猫</em>和<em class="le">胡须</em>的上下文向量重叠，就像<em class="le">电脑</em>和<em class="le">笔记本电脑</em>的上下文向量一样。但是没有那些<em class="le">络腮胡子</em>和<em class="le">电脑。</em>这话有道理。</p><p id="965f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">估计上下文向量</strong></p><p id="36ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然我们知道了单词的上下文向量是什么，那么我们如何估计它呢？拥有大量的文档会有所帮助。幸运的是，这样的语料库，例如维基百科，现在很容易获得。</p><p id="35f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们简单地跟踪其他单词与感兴趣的单词在相同的邻近度中的共现，比如在同一个句子中。说出单词<em class="le">猫</em>在100个句子中出现。在其中的5个句子中，单词狗也出现了。单词<em class="le">狗</em>在<em class="le">猫</em>的上下文中的概率为5/100。</p><p id="6171" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">改进上下文向量</strong></p><p id="a7ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种方法有一个问题。特别是常见的单词如<em class="le"/>会出现在大多数句子中，所以会出现在不相关单词的上下文中。例如，<em class="le">猫</em>的上下文将包括<em class="le">猫</em>猫，因为在大多数出现<em class="le">猫</em>猫<em class="le">的句子中</em>猫也会出现。</p><p id="7b45" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有一个简单的方法可以缓解这种情况。我们用单词的上下文概率除以它的总概率。形式上是这样的。说<em class="le">上下文</em> - <em class="le">得分</em> ( <em class="le"> u </em>，<em class="le"> v </em>)表示我们给<em class="le"> v </em>在<em class="le"> u </em>的上下文中的得分。至此我们已经将<em class="le">上下文</em> - <em class="le">分数</em> ( <em class="le"> u </em>，<em class="le"> v </em>)定义为<em class="le">P</em>(<em class="le">v</em>|<em class="le">u</em>)。而是可以定义为<em class="le">P</em>(<em class="le">v</em>|<em class="le">u</em>)/<em class="le">P</em>(<em class="le">v</em>)。这里的<em class="le">P</em>(<em class="le">v</em>|<em class="le">u</em>)是包含<em class="le"> u </em>同时也包含<em class="le"> v </em>的句子的分数，<em class="le"> P </em> ( <em class="le"> v </em>)是所有包含<em class="le"> v </em>的句子的分数。</p><p id="bd8c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一个例子。假设<em class="le">和</em>出现在75%的句子和80%的<em class="le">和</em>出现的句子中。所以<em class="le"/>在<em class="le">猫</em>的上下文中的得分是80/75。想想<em class="le">的络腮胡子</em>。假设它只出现在所有句子的1%,但是在出现<em class="le"> cat </em>的句子中却有25%。所以<em class="le">胡须</em>在<em class="le">猫</em>上下文中的得分是25/1。</p><p id="8e07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简而言之，<em class="le">胡须</em>在<em class="le">猫</em>的上下文中的得分远高于<em class="le">猫</em>的得分。</p><p id="79a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">上下文向量将是稀疏的</strong></p><p id="b55d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">向量空间可能有数百万个维度。然而，上下文向量将趋于稀疏，或者可以变得稀疏而不会丢失太多信息。一个单词的上下文通常只包含词典中数百万个单词中的一小部分。</p><p id="c0fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">文本向量</strong></p><p id="c7b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这和以前一样。在针对各种单词的IDF分数进行调整之后，添加文本中单词出现的向量。</p><p id="9ff0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">值得注意的是，根据IDF分数进行调整在这里比在本地演示中更重要。这是因为像<em class="le"> the </em>这样的普通单词会有它自己的(特别宽的)上下文，它的无阻尼贡献会污染文本向量。</p><p id="f0c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个例子。比方说，从一个合适的语料库中，我们获得了以下上下文表示。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="45db" class="lo lp it lk b gy lq lr l ls lt"><strong class="lk iu">Word                context<br/></strong>bark                howl, dog, tree, wood, leash, shrub<br/>dog                 bark, howl, leash, …</span></pre><p id="29a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意<em class="le">吠</em>的上下文抓住了它的两个词义:<em class="le">树</em> - <em class="le">吠</em>和<em class="le">狗</em> - <em class="le">吠</em>。</p><p id="390e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在考虑课文<em class="le">狗的叫声</em>。假设IDF将显著降低<em class="le">的两次出现</em>和<em class="le">的一次出现</em>对文本向量的贡献。事实上，为了消除混乱，让我们将这些影响设置为零。<em class="le"> </em>我们只剩下:<em class="le">吠犬</em>。将这两个词的上下文向量相加，我们会得到这样的结果</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="cd1d" class="lo lp it lk b gy lq lr l ls lt"><strong class="lk iu">howl</strong>, <strong class="lk iu">dog</strong>, tree, wood, <strong class="lk iu">leash</strong>, shrub</span></pre><p id="f202" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">出现在文本中两个单词的上下文中的单词以粗体突出显示。这些单词将对文本的向量有更大的贡献。</p><p id="12f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在想象这篇文章是<em class="le">拉布拉多犬的叫声</em>。假设我们的语料库足够丰富，可以捕捉到<em class="le">拉布拉多</em>的上下文。这个文本的向量也将是相似的。这就让我们推断出<em class="le">狗的叫声</em>和<em class="le">拉布拉多犬的叫声</em>十分相似。</p><p id="0a69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">高级单词上下文模型</strong></p><p id="5121" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在过去的十年中，一种不同的获取单词上下文向量的方法，名为<em class="le">单词嵌入</em>，已经出现并得到广泛使用。它不是通过简单的统计分析获得上下文向量，而是训练一个神经网络来学习单词嵌入。这是在具有隐藏层的架构中完成的。因此，学习迫使网络学习单词良好的潜在特征。单词被嵌入到这个特征空间中，而不是原始的向量空间中。</p><p id="d187" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在另一篇文章中详细介绍了这种方法，<a class="ae mj" rel="noopener" target="_blank" href="/machine-learned-word-embeddings-638c3fb5b916">https://towardsdatascience . com/machine-learned-word-embedding-638 C3 FB 5b 916</a>，因为它的描述涉及到神经网络的细节，相当复杂。</p></div></div>    
</body>
</html>