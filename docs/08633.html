<html>
<head>
<title>Prediction on Customer Churn with Mobile App Behavior Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于移动应用行为数据的客户流失预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/prediction-on-customer-churn-with-mobile-app-behavior-data-bbce8de2802f?source=collection_archive---------53-----------------------#2020-06-22">https://towardsdatascience.com/prediction-on-customer-churn-with-mobile-app-behavior-data-bbce8de2802f?source=collection_archive---------53-----------------------#2020-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b934" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过数据处理、模型构建、验证、特征分析和选择深入研究逻辑回归建模</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/013a67e87cf9c099ed1aa3d4f0b267f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*XhmeJ9DQ9ayC1lqbYNHznQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来自Pixabay的Img通过<a class="ae ku" href="https://pixabay.com/photos/ux-prototyping-design-webdesign-788002/" rel="noopener ugc nofollow" target="_blank">链接</a></p></figure><p id="a950" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在之前的<a class="ae ku" rel="noopener" target="_blank" href="/logistic-regression-how-to-on-app-behavior-data-8a95802a988f">文章</a>中，我们使用应用行为数据创建了一个逻辑回归模型来预测用户注册。希望你在那里学得很好。<strong class="kx iu">这篇文章旨在基于更大的移动应用程序行为数据</strong>，用新的技术和技巧提高你的建模技能。它分为7个部分。</p><p id="5f5a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">1.商业挑战</p><p id="e50a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2.数据处理</p><p id="8aa3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">3.模型结构</p><p id="23ee" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">4.模型验证</p><p id="60bc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">5.特征分析</p><p id="a9ef" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">6.特征选择</p><p id="087d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">7.结论</p><p id="1db2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在让我们开始旅程🏃‍♀️🏃‍♂️.</p><ol class=""><li id="67f5" class="lr ls it kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><strong class="kx iu">商业挑战</strong></li></ol><p id="94cf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">一家金融科技公司委托我们分析移动应用行为数据，以识别潜在的流失客户。目标是预测哪些用户可能会流失，因此公司可以专注于用更好的产品重新吸引这些用户。</p><p id="8cc2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2.<strong class="kx iu">数据处理</strong></p><p id="89d6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> EDA应在数据处理前进行。</strong>详细步骤在本<a class="ae ku" href="https://medium.com/@vistaxjtu/exploratory-data-analysis-on-mobile-app-behavior-data-2777fc937973" rel="noopener">篇</a>中介绍。下面的视频是EDA后的最终数据。</p><p id="4a6d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2.1一键编码</p><p id="f3ac" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">一键编码是一种将分类变量转换成数值变量的技术。它是必需的，因为我们要构建的模型无法读取分类数据。<strong class="kx iu">一键编码只是根据唯一类别的数量创建附加功能。</strong>在这里，具体来说，</p><pre class="kj kk kl km gt ma mb mc md aw me bi"><span id="1ef6" class="mf mg it mb b gy mh mi l mj mk">dataset = pd.get_dummies(dataset)</span></pre><p id="da9c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">以上自动将所有分类变量转换为数值变量。但是独热编码的一个缺点是<strong class="kx iu">虚拟变量陷阱</strong>。<em class="ml">这是一个变量之间高度相关的场景。</em>为了避免陷阱，必须删除一个虚拟变量。具体来说，</p><pre class="kj kk kl km gt ma mb mc md aw me bi"><span id="7436" class="mf mg it mb b gy mh mi l mj mk">dataset = dataset.drop(columns = [‘housing_na’, ‘zodiac_sign_na’, ‘payment_type_na’])</span></pre><p id="8ccb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2.2数据分割</p><p id="74bc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这是为了将数据分成训练集和测试集。具体来说，</p><pre class="kj kk kl km gt ma mb mc md aw me bi"><span id="f94d" class="mf mg it mb b gy mh mi l mj mk">X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns = ‘churn’), dataset[‘churn’], test_size = 0.2,random_state = 0)</span></pre><p id="8b8f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2.3数据平衡</p><h2 id="62df" class="mf mg it bd mm mn mo dn mp mq mr dp ms le mt mu mv li mw mx my lm mz na nb nc bi translated">不平衡类是分类中的常见问题，其中每个类中的观察值比例不成比例。但是如果我们在不平衡的数据集上训练一个模型会发生什么？该模型将巧妙地决定最好的事情是总是预测类1，因为类1占用90%的数据，所以该模型可以达到90%的准确性。</h2><p id="7030" class="pw-post-body-paragraph kv kw it kx b ky nd ju la lb ne jx ld le nf lg lh li ng lk ll lm nh lo lp lq im bi translated">这里有许多方法来对抗不平衡的类，例如改变性能指标、收集更多数据、过采样或下采样数据等。这里我们使用下采样方法。</p><p id="ec05" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">首先，我们来考察一下<em class="ml"> y_train中因变量的不平衡程度。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d20a8e110eb9f2efa83a18e39336367d.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*FtTH7PpedjJJr6iowps2eQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图1因变量的不平衡分布:流失与否</p></figure><p id="04f6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如图1所示，因变量略有不平衡。为了对数据进行下采样，我们取每个类的指数，并在<em class="ml"> y_train </em>的多个少数类中随机选择多数类的指数。然后连接两个类的索引，并对<em class="ml"> x_train </em>和<em class="ml"> y_train </em>进行下采样。</p><pre class="kj kk kl km gt ma mb mc md aw me bi"><span id="5062" class="mf mg it mb b gy mh mi l mj mk">pos_index = y_train[y_train.values == 1].index<br/>neg_index = y_train[y_train.values == 0].index<br/>if len(pos_index) &gt; len(neg_index):<br/>    higher = pos_index<br/>    lower = neg_index<br/>else:<br/>    higher = neg_index<br/>    lower = pos_index<br/>random.seed(0)<br/>higher = np.random.choice(higher, size=len(lower))<br/>lower = np.asarray(lower)<br/>new_indexes = np.concatenate((lower, higher))<br/>X_train = X_train.loc[new_indexes,]<br/>y_train = y_train[new_indexes]</span></pre><p id="a5d0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">2.4.特征缩放</p><p id="f9fd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">从根本上说，特征缩放是对变量范围的标准化。这是为了避免任何变量对模型产生显著影响。对于神经网络，特征缩放有助于梯度下降比没有特征缩放收敛得更快。</p><p id="58ae" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里我们使用<strong class="kx iu">标准化</strong>来标准化变量。具体来说，</p><pre class="kj kk kl km gt ma mb mc md aw me bi"><span id="ea60" class="mf mg it mb b gy mh mi l mj mk">from sklearn.preprocessing import StandardScaler<br/>sc_X = StandardScaler()<br/>X_train2 = pd.DataFrame(sc_X.fit_transform(X_train))<br/>X_test2 = pd.DataFrame(sc_X.transform(X_test))</span></pre><p id="cc08" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">3.<strong class="kx iu">模型构建</strong></p><h2 id="440d" class="mf mg it bd mm mn mo dn mp mq mr dp ms le mt mu mv li mw mx my lm mz na nb nc bi translated">在这里，我们建立了一个逻辑回归分类器，用于流失预测。<strong class="ak">实质上，逻辑回归使用独立变量的线性组合来预测一类概率的对数。</strong>如果你想深入了解逻辑回归的更多细节，请访问这个<a class="ae ku" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">维基百科</a>页面。</h2><p id="ca68" class="pw-post-body-paragraph kv kw it kx b ky nd ju la lb ne jx ld le nf lg lh li ng lk ll lm nh lo lp lq im bi translated">具体来说，</p><pre class="kj kk kl km gt ma mb mc md aw me bi"><span id="933c" class="mf mg it mb b gy mh mi l mj mk">from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression(random_state = 0)<br/>classifier.fit(X_train, y_train)</span></pre><p id="e389" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，让我们测试和评估模型。具体来说，</p><pre class="kj kk kl km gt ma mb mc md aw me bi"><span id="ba0f" class="mf mg it mb b gy mh mi l mj mk">y_pred = classifier.predict(X_test)<br/>from sklearn.metrics import confusion_matrix, accuracy_score, f1_score<br/>cm = confusion_matrix(y_test, y_pred)<br/>accuracy_score(y_test, y_pred)<br/>f1_score(y_test, y_pred)</span></pre><p id="640a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后我们得到了一个<strong class="kx iu"> 0.61 </strong>的精度和<strong class="kx iu"> 0的F1。61 </strong>。表现不算太差。</p><p id="04d5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">4.<strong class="kx iu">模型验证</strong></p><p id="b6f0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">模型经过训练和测试后，一个问题是该模型推广到未知数据集的效果如何。我们使用交叉验证来衡量已知数据集和未知数据集之间的性能差异大小。具体来说，</p><pre class="kj kk kl km gt ma mb mc md aw me bi"><span id="3717" class="mf mg it mb b gy mh mi l mj mk">from sklearn.model_selection import cross_val_score<br/>accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)</span></pre><p id="c1a8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">根据上述内容，我们发现10重交叉验证产生的平均准确度为<strong class="kx iu"> 0.64.5 </strong>，标准偏差为<strong class="kx iu"> 0.023 </strong>。这表明该模型可以在未知数据集✨✨.上很好地泛化</p><p id="6df8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">5.<strong class="kx iu">特征分析</strong></p><p id="e0d7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">有了41个特征，我们建立了一个逻辑回归模型。<em class="ml">但是如何知道在预测因变量时哪个特征更重要呢？</em>具体来说，</p><pre class="kj kk kl km gt ma mb mc md aw me bi"><span id="6093" class="mf mg it mb b gy mh mi l mj mk">pd.concat([pd.DataFrame(X_train.columns, columns = [“features”]), pd.DataFrame(np.transpose(classifier.coef_), columns = [“coef”])],axis = 1)</span></pre><p id="71d8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如图2所示，我们发现两个特性非常重要:<em class="ml"> purchase_partners </em>和<em class="ml"> purchase </em>。这表明用户的购买历史在决定是否流失时起着很大的作用。同时，这表明并非所有变量对预测都很重要。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/53480f2701863edc92886fca5622a0ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*oiH7IP1CdITznhFCtaNrRA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图2变量对预测的重要性</p></figure><p id="995a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">6.<strong class="kx iu">功能选择</strong></p><blockquote class="nk nl nm"><p id="d6e9" class="kv kw ml kx b ky kz ju la lb lc jx ld nn lf lg lh no lj lk ll np ln lo lp lq im bi translated">特征选择是一种为建模训练选择最相关特征子集的技术。</p></blockquote><p id="a260" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在这个应用中，<em class="ml"> x_train </em>包含41个特性，但是如图2所示，并不是所有的特性都起着重要的作用。<strong class="kx iu">使用特征选择有助于减少不重要特征的数量，并以较少的训练数据获得相似的性能</strong>。关于功能选择的更详细的解释可以在<a class="ae ku" href="https://machinelearningmastery.com/rfe-feature-selection-in-python/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><blockquote class="nk nl nm"><p id="3fa4" class="kv kw ml kx b ky kz ju la lb lc jx ld nn lf lg lh no lj lk ll np ln lo lp lq im bi translated">这里，我们使用<strong class="kx iu">递归特征消除</strong> (RFE)。它的工作原理是拟合给定的算法，按重要性排列特性，丢弃最不重要的特性，然后重新调整，直到达到指定的特性数。具体来说，</p></blockquote><pre class="kj kk kl km gt ma mb mc md aw me bi"><span id="6e27" class="mf mg it mb b gy mh mi l mj mk">from sklearn.feature_selection import RFE<br/>from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression()<br/>rfe = RFE(classifier, 20)<br/>rfe = rfe.fit(X_train, y_train)</span></pre><p id="f002" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如上所述，我们设置选择<em class="ml"> 20个特性</em>。图3显示了所有选择的特性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/37a0881c32a368492cbfdd11193f54c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*vOZqMTXGrEY6B0tl9mNIMQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图3 RFE推荐的功能</p></figure><p id="2b78" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">太好了。使用RFE选择的功能，让我们重新训练和测试模型。</p><pre class="kj kk kl km gt ma mb mc md aw me bi"><span id="d91a" class="mf mg it mb b gy mh mi l mj mk">classifier.fit(X_train[X_train.columns[rfe.support_]], y_train)<br/>y_pred = classifier.predict(X_test[X_train.columns[rfe.support_]])</span></pre><p id="ddc8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最终我们得到了一个<strong class="kx iu"> 0.61 </strong>的精度和<strong class="kx iu"> 0.61 </strong>的F1。与在41个特征上训练的模型相同的性能😇😇！</p><p id="d231" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果我们再次应用交叉验证，我们得到的平均准确度为<strong class="kx iu"> 0.647 </strong>，标准偏差为<strong class="kx iu"> 0.014 </strong>。同样，与之前的模型非常相似。</p><p id="d235" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">7.<strong class="kx iu">结论</strong></p><p id="9ede" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最初，我们用41个特征训练了一个逻辑回归模型，达到了0.645的精确度。但是使用特征选择，我们创建了一个只有20个特征的轻型模型，精度为0.647。一半的特征与决定客户流失无关。干得好！</p><p id="99ab" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">太好了！这就是全部的旅程！如果您需要源代码，请随时访问我的</strong><a class="ae ku" href="https://github.com/luke4u/Customer_Behaviour_Prediction/tree/main/churn_prediction" rel="noopener ugc nofollow" target="_blank"><strong class="kx iu">Github</strong></a><strong class="kx iu">页面🤞🤞(仅供参考，回购正在积极维护中)。</strong></p></div></div>    
</body>
</html>