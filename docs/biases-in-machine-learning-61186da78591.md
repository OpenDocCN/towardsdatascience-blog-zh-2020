# 机器学习中的偏见

> 原文：<https://towardsdatascience.com/biases-in-machine-learning-61186da78591?source=collection_archive---------14----------------------->

## ML 模型中引入偏差的最常见原因

不管你喜欢与否，机器学习对你生活的影响正在非常迅速地增长。机器学习算法决定了你是否会获得梦想中的房子的抵押贷款，或者你的简历是否会入围你的下一份工作。它也在迅速改变我们的劳动力。机器人正在接管仓库和工厂，无人驾驶汽车正威胁着世界各地数百万职业司机的工作。甚至执法机构也越来越多地使用机器学习来筛选潜在的犯罪线索和评估风险。

![](img/6c12a7d0749fa91205e2331d9dd482ba.png)

由莱尼·屈尼在 Unsplash

不幸的是，所有这些技术进步可能会延续并加剧困扰我们社会的偏见。在算法偏差的早期例子中，从 1982 年到 1986 年，每年有 60 名女性和少数民族被圣乔治医院医学院拒绝入学，因为一个新的计算机指导评估系统根据录取的历史趋势拒绝了具有“听起来像外国名字”的女性和男性入学。或者更近一些，在 2016 年，由微软在 Twitter 数据上训练的聊天机器人 TayTweets 开始发表种族主义的推文。

![](img/f047a38b0e40c688525d647d34ad2031.png)

所有这些进步都提出了非常有效的问题，即机器学习从业者如何确保他们的算法公平。什么是公平是一个古老的问题。令人欣慰的是，在这一领域已经进行了大量的研究。在这篇文章中，我将谈论当你试图确保你的机器学习模型没有偏见时，你可能会遇到的最常见的问题。

# 代表名额不足

机器学习算法中偏差的最常见原因之一是训练数据缺少代表不足的组/类别的样本。这就是为什么 Siri 经常很难理解有口音的人。这也导致了著名的谷歌照片事件，黑人被标记为大猩猩。因此，确保训练数据能够代表所有代表性不足的群体是非常重要的。另一种早期轻松检测的方法是部署第二种算法，该算法预测生产中的数据是否接近训练数据，如果不是这样，则早期干预。

![](img/3464892614e0c0b135e5493a81f4fc50.png)![](img/1c94d1dfbf26798f0bf2945be420413d.png)

通过推特

# 社会反馈循环

最近一篇关于他们开创性语言模型的 OpenAI [论文](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)发现“一般来说，职业被男性性别识别符追随的概率比女性高”。在媒体上有很多例子，模型继承了训练数据中的偏差。有实验得出结论，高薪招聘广告只给男人看，或者白人社区的房屋广告只给白人看。基于来自互联网的文本数据训练的各种语言模型倾向于给女性或有色人种附加负面含义。发生这种情况，是因为我们现有的社会偏见反映在训练数据中。在大多数情况下，这种偏见很难纠正，因为很难获得无偏见的数据。缓解这种情况的一个简单策略是找到代表边缘化群体的正面历史数据的数据点，并在训练数据中对这些记录进行上采样。

# 相关字段

确保数据不针对敏感属性进行训练的一个常见解决方案是从训练数据中完全移除这些特征。但是仍然可能存在相关的属性，这些属性可能导致模型歧视服务水平低下的社区。例如，邮政编码可能与种族相关，而姓名可能与种族和性别相关，等等。人们经常观察到，在简历上训练的机器学习模型学会更重视男性名字而不是女性名字。一项[亚马逊研究](https://qz.com/1427621/companies-are-on-the-hook-if-their-hiring-algorithms-are-biased/)发现“贾里德”这个名字非常适合进入工作候选名单。

# 忽略敏感属性

在某些情况下，盲目地从训练数据中移除敏感属性特征甚至可能会造成伤害。例如，女性不太可能造成事故，因此在一些地方，使用性别来确定保险报价是合法的。在累犯研究中，发现女性不太可能再次犯罪。在弱势类别实际上比主导类别做得更好的情况下，最好将这作为一个特征包含在训练数据中。

# 偏差的过度补偿

这不太可能是一个问题，但在这里补充这是全面的。假设你的模型根据个人的信用评分决定是否向其提供贷款，如果个人偿还了贷款，其评分就会增加，否则就会减少。你的长期目标是提高人口的平均信用评分。在这种情况下，为了弥补偏见，您可能需要更改门槛，为特定人群批准更多贷款。但如果你过度补偿，以至于违约数量高于成功还款的数量，这部分人的平均信用评分可能会下降，最终伤害他们。谷歌提供了一个名为 [ml-fairness-gym](https://github.com/google/ml-fairness-gym) 的库，可以帮助模拟在机器学习模型中实施公平的长期效果。

[艾伦·图灵说](https://www.amazon.com/Mechanical-Intelligence-Collected-Works-Turing/dp/0444880585)如果一台机器被期望是不会犯错的，它也不可能是智能的。但是，每天，社会基础设施的另一个关键部分是在没有任何法律制衡的情况下，将“人工智能”添加到其决策过程中。各种偏见很容易溜进机器学习模型，除非你极其谨慎。此外，我们的法律体系还没有跟上新技术的步伐，新技术继续深刻地影响着我们的生活。因此，现在，研究这些模型的人有责任确保这些模型是公平的，没有偏见的。

幸运的是，研究人员正在关注这个问题以及类似机器学习模型的可解释性等相关问题。

*   谷歌的 Tensorflow 有“[公平指标](https://www.tensorflow.org/tfx/guide/fairness_indicators)”来检测你的模型中的偏差
*   微软研究院成立了一个 [FATE](https://www.microsoft.com/en-us/research/theme/fate/) (公平、问责、透明和道德)小组，研究人工智能的社会影响
*   IBM 开源了 AI fairness 360 工具来帮助你构建无偏见的模型
*   刘等人的论文“[公平机器学习的延迟影响](https://arxiv.org/abs/1803.04383)”获得 2018 最佳论文奖。

但是这些工具远远不够或完美。这一领域非常需要更严格的工具和研究。技术既有好的一面，也有坏的一面。这是技术创造者的责任，以确保他们的产品为我们所有人创造一个更美好的世界。我真的很喜欢 Kate Crawford 在 NIPS 2017 上的这个著名的[演讲](https://www.youtube.com/watch?v=fMym_BKWQzk)，她呼吁行业在理解人工智能的社会影响方面投入更多。

![](img/f760d4e9e590df2c8410c3cfb9cbc8b6.png)

https://xkcd.com/1277/