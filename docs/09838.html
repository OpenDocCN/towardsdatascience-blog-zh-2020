<html>
<head>
<title>Alphabet GAN: AI Generates English Alphabet!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">字母表甘:AI 生成英文字母表！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/alphabet-gan-ai-generates-english-letters-589637068808?source=collection_archive---------37-----------------------#2020-07-12">https://towardsdatascience.com/alphabet-gan-ai-generates-english-letters-589637068808?source=collection_archive---------37-----------------------#2020-07-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="09dd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这就是我如何创建一个可以生成英文字母的 GAN。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cba78746dbd94b1e06a38eaeb4a100ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*2bK1R0VLQvIMBnrJ1BFNDg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GAN 输出在各个时期的演变</p></figure><p id="2988" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated">首先，你需要知道 GAN 到底是什么。这里有一个简单的描述。生成对抗网络是两个模型的组合，即生成器和鉴别器。生成器试图生成模仿原始数据的假数据。另一方面，鉴别器试图辨别一个给定的数据是原始的还是伪造的。多亏了这种对抗性的设置，最终，这两种模式在完成任务方面都变得越来越好。当然，关于甘斯还有更多需要了解的。如果你好奇，请观看这个视频…</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">甘是如何工作的？</p></figure><p id="ae14" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我想向您展示如何实现这样一个 GAN。我还会提到一大堆帮助你训练第一个 GAN 的技巧。但是，在进入模型之前，让我们先了解一下数据集。</p><h1 id="468c" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">数据集:A-Z 手写字母</h1><p id="ca75" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">这里，我使用的是 MNIST 风格的手写英文字母数据集。<a class="ae nc" href="https://www.kaggle.com/sachinpatel21/az-handwritten-alphabets-in-csv-format" rel="noopener ugc nofollow" target="_blank"> A-Z 数据集</a>包含来自<strong class="la iu"> <em class="nd"> 26 </em> </strong>类的<strong class="la iu"> <em class="nd"> 372，450 </em> </strong>个字符。每个数据样本都是字母表的灰度图像。像 MNIST 数据集一样，每幅图像的维数是<strong class="la iu"> <em class="nd"> 28px*28px </em> </strong>并表示为一个<strong class="la iu"><em class="nd">784</em></strong>(<strong class="la iu"><em class="nd">28 * 28</em></strong>)维向量。让我们想象一下其中的一些…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/b37bb82d97df2caa68aad7023d8a89d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*YUAzuueXTHkIn3HZqMQAMQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自 EMNIST Letters 数据集的 100 张随机图像</p></figure><p id="a4b6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最初，像素值的范围在<strong class="la iu"><em class="nd">【0，255】</em></strong>之间，但是我们应该在馈送到任何机器学习模型之前将它们归一化。一般情况下，我们通过划分<strong class="la iu"><em class="nd"/></strong>255.0 来归一化<strong class="la iu"><em class="nd">【0，1】</em></strong>之间的像素，但这里我们归一化的是<strong class="la iu"> <em class="nd"> [-1，1】</em></strong>之间的像素。这是因为我们后面会用到<em class="nd">tanh</em>(<em class="nd">tanh</em>=<em class="nd"/><strong class="la iu"><em class="nd">[-1，1】</em></strong>)函数。</p><p id="0786" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们建立我们的 GAN。我喜欢按照<strong class="la iu"> <em class="nd"> 4 </em> </strong>的步骤来做。</p><h1 id="9161" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">1.建造发电机(G)</h1><p id="e7db" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">生成器是一个神经网络，它将噪声向量(<strong class="la iu"><em class="nd">100</em></strong>-维)作为输入，并输出单个英文字母的图像。当我们处理图像数据时，使用卷积神经网络是有意义的。其想法是在输入通过不同层时增加输入的空间维度，直到它达到期望的输出形状(<strong class="la iu"> <em class="nd"> 28px*28px </em> </strong>)。网络的前两层是具有 ReLu 激活的密集层。我强烈建议在每个图层的输出上使用 BatchNormalization。</p><blockquote class="nf ng nh"><p id="bab3" class="ky kz nd la b lb lc ju ld le lf jx lg ni li lj lk nj lm ln lo nk lq lr ls lt im bi translated">注:<a class="ae nc" href="https://en.wikipedia.org/wiki/Batch_normalization" rel="noopener ugc nofollow" target="_blank">批处理规范化</a>使训练收敛更快。快多了。</p></blockquote><p id="eab3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，第一密集层包含<strong class="la iu"> <em class="nd"> 1024 </em> </strong>神经元，第二密集层包含<strong class="la iu"> <em class="nd"> 6272 </em> </strong>神经元。之后是整形层。整形是很重要的，因为我们希望以后使用卷积，为了应用卷积，我们需要类似矩阵的实体，而不是列/行向量。</p><blockquote class="nf ng nh"><p id="a5ab" class="ky kz nd la b lb lc ju ld le lf jx lg ni li lj lk nj lm ln lo nk lq lr ls lt im bi translated">注意:为了找到正确的尺寸，我们需要向后思考！首先，确定矩阵的维数(<strong class="la iu"> 7*7 </strong>)以及需要多少个矩阵(<strong class="la iu"> 128 </strong>)，然后将它们相乘得到密集层的维数(<strong class="la iu"> 7*7*128 = 6272 </strong>)。</p></blockquote><p id="f3cf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在应用卷积之前，我们将对矩阵进行上采样。我使用了(<strong class="la iu"> <em class="nd"> 2，2 </em> </strong>)上采样，将维度从<strong class="la iu"> <em class="nd"> 7*7 </em> </strong>增加到<strong class="la iu"> <em class="nd"> 14*14 </em> </strong>。</p><blockquote class="nf ng nh"><p id="4770" class="ky kz nd la b lb lc ju ld le lf jx lg ni li lj lk nj lm ln lo nk lq lr ls lt im bi translated">上采样是池的一种反函数。</p></blockquote><p id="668b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">之后，我们有了<strong class="la iu"> <em class="nd"> 2*2 </em> </strong>卷积滤波器(<strong class="la iu"> <em class="nd"> 64 </em> </strong>)。注意，我已经根据正态分布初始化了内核的权重。这一层的激活是 LeakyReLu。同样，我们有一个上采样层，后面是卷积层。这一次上采样层将输出<strong class="la iu"> <em class="nd"> 28*28 </em> </strong>维矩阵。最后一个卷积层只包含<strong class="la iu"> <em class="nd"> 1 </em> </strong>滤镜，因为我们只想要一个通道用于灰度图像。这里的激活函数是<em class="nd"> tanh </em>。这就是我们把<strong class="la iu"> <em class="nd"> [-1，1】</em></strong>之间的像素值归一化的原因。</p><blockquote class="nf ng nh"><p id="3da9" class="ky kz nd la b lb lc ju ld le lf jx lg ni li lj lk nj lm ln lo nk lq lr ls lt im bi translated">注意:我们可以通过使用转置卷积来避免对层进行上采样。因为它们也会增加矩阵的维数。</p></blockquote><h2 id="1b51" class="nl mg it bd mh nm nn dn ml no np dp mp lh nq nr mr ll ns nt mt lp nu nv mv nw bi translated">代码:</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx me l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">发电机</p></figure><h2 id="9523" class="nl mg it bd mh nm nn dn ml no np dp mp lh nq nr mr ll ns nt mt lp nu nv mv nw bi translated">建筑:</h2><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="e106" class="nl mg it nz b gy od oe l of og">Model: "sequential_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense_1 (Dense)              (None, 1024)              103424    <br/>_________________________________________________________________<br/>batch_normalization_1 (Batch (None, 1024)              4096      <br/>_________________________________________________________________<br/>activation_1 (Activation)    (None, 1024)              0         <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 6272)              6428800   <br/>_________________________________________________________________<br/>batch_normalization_2 (Batch (None, 6272)              25088     <br/>_________________________________________________________________<br/>activation_2 (Activation)    (None, 6272)              0         <br/>_________________________________________________________________<br/>reshape_1 (Reshape)          (None, 7, 7, 128)         0         <br/>_________________________________________________________________<br/>up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         <br/>_________________________________________________________________<br/>conv2d_1 (Conv2D)            (None, 14, 14, 64)        32832     <br/>_________________________________________________________________<br/>batch_normalization_3 (Batch (None, 14, 14, 64)        256       <br/>_________________________________________________________________<br/>leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         <br/>_________________________________________________________________<br/>up_sampling2d_2 (UpSampling2 (None, 28, 28, 64)        0         <br/>_________________________________________________________________<br/>conv2d_2 (Conv2D)            (None, 28, 28, 1)         577       <br/>=================================================================<br/>Total params: 6,595,073<br/>Trainable params: 6,580,353<br/>Non-trainable params: 14,720<br/>_________________________________________________________________</span></pre><p id="fea8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你注意到我没有在这里编译生成器吗？这将在第<strong class="la iu"> <em class="nd">第三</em> </strong>步骤中完成。</p><h1 id="b2be" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">2.构建鉴别器(D)</h1><p id="55df" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">我们的鉴别器只是一个二元分类器，它将灰度图像作为输入，并预测它是原始图像还是伪造图像，即由生成器创建的图像。前两层是卷积层。请注意，我使用了一个步幅<strong class="la iu"> <em class="nd"> 2 </em> </strong>，这意味着输出尺寸将小于输入尺寸。所以，我们不需要池层。两层的滤波器大小都是<strong class="la iu"> <em class="nd"> 5*5 </em> </strong>，但是第二层中的滤波器数量更多。</p><blockquote class="nf ng nh"><p id="1297" class="ky kz nd la b lb lc ju ld le lf jx lg ni li lj lk nj lm ln lo nk lq lr ls lt im bi translated">注意:在构建鉴别器时，你应该记住我们的目标是支持生成器，因为我们想要生成假图像。因此，使鉴别器比发生器弱一点。例如，这里我在鉴别器中使用了较少的卷积层。</p></blockquote><p id="05ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在卷积层之后，我们需要展平输出，这样我们就可以把它传递到一个密集层。密集层的大小是<strong class="la iu"> <em class="nd"> 256 </em> </strong>与<strong class="la iu"> <em class="nd"> 50% </em> </strong>的落差。最后，我们有了 sigmoid 层，就像任何其他二元分类器一样。我们现在必须编译鉴别器。损失应该是二进制交叉熵，我已经使用了一个定制的亚当优化器(学习率= <strong class="la iu"> <em class="nd"> 0.0002 </em> </strong>)。</p><blockquote class="nf ng nh"><p id="536b" class="ky kz nd la b lb lc ju ld le lf jx lg ni li lj lk nj lm ln lo nk lq lr ls lt im bi translated">注意:默认的 Adam 学习率(<strong class="la iu"> 0.001 </strong>)对于 GANs 来说太高了，所以请始终定制 Adam 优化器。</p></blockquote><h2 id="a7b1" class="nl mg it bd mh nm nn dn ml no np dp mp lh nq nr mr ll ns nt mt lp nu nv mv nw bi translated">代码:</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx me l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">鉴别器</p></figure><h2 id="d2ed" class="nl mg it bd mh nm nn dn ml no np dp mp lh nq nr mr ll ns nt mt lp nu nv mv nw bi translated">建筑:</h2><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="adf8" class="nl mg it nz b gy od oe l of og">Model: "sequential_2"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>conv2d_3 (Conv2D)            (None, 14, 14, 64)        1664      <br/>_________________________________________________________________<br/>leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 64)        0         <br/>_________________________________________________________________<br/>conv2d_4 (Conv2D)            (None, 5, 5, 128)         204928    <br/>_________________________________________________________________<br/>leaky_re_lu_3 (LeakyReLU)    (None, 5, 5, 128)         0         <br/>_________________________________________________________________<br/>flatten_1 (Flatten)          (None, 3200)              0         <br/>_________________________________________________________________<br/>dense_3 (Dense)              (None, 256)               819456    <br/>_________________________________________________________________<br/>leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         <br/>_________________________________________________________________<br/>dropout_1 (Dropout)          (None, 256)               0         <br/>_________________________________________________________________<br/>dense_4 (Dense)              (None, 1)                 257       <br/>=================================================================<br/>Total params: 1,026,305<br/>Trainable params: 1,026,305<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><h1 id="0ece" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">3.联合 G &amp; D</h1><p id="4d05" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">根据<a class="ae nc" href="https://arxiv.org/pdf/1406.2661.pdf" rel="noopener ugc nofollow" target="_blank">原 GAN 论文</a>我们要分别训练发生器和鉴别器。那为什么要走这一步？</p><blockquote class="nf ng nh"><p id="1cf1" class="ky kz nd la b lb lc ju ld le lf jx lg ni li lj lk nj lm ln lo nk lq lr ls lt im bi translated">可以通过反向传播在最后一个 sigmoid 层计算的损耗来直接训练鉴别器。但是为了训练生成器，我们需要在不影响鉴别器权重的情况下，将这种损失发送回生成器！</p></blockquote><p id="8333" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实现这一点的一种方法是通过堆叠生成器和鉴别器来创建新的模型。这也是我之前没有编译生成器的原因。我们把新型号叫做<em class="nd">甘</em>。它将噪声向量作为输入，然后通过生成器创建一个假图像。然后图像通过鉴别器，鉴别器计算它是原始图像的概率。当我们训练这个 gan 时，鉴别器不应该学习任何东西。因此，<em class="nd">' discriminator . trainiable = False '。</em> <strong class="la iu"> <em class="nd"> </em> </strong>只有生成器的权重会被修改。</p><h2 id="7442" class="nl mg it bd mh nm nn dn ml no np dp mp lh nq nr mr ll ns nt mt lp nu nv mv nw bi translated">代码:</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx me l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">发电机+鉴频器= gan</p></figure><h2 id="6d73" class="nl mg it bd mh nm nn dn ml no np dp mp lh nq nr mr ll ns nt mt lp nu nv mv nw bi translated">建筑:</h2><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="ab85" class="nl mg it nz b gy od oe l of og">Model: "model_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_1 (InputLayer)         (None, 100)               0         <br/>_________________________________________________________________<br/>sequential_1 (Sequential)    (None, 28, 28, 1)         6595073   <br/>_________________________________________________________________<br/>sequential_2 (Sequential)    (None, 1)                 1026305   <br/>=================================================================<br/>Total params: 7,621,378<br/>Trainable params: 6,580,353<br/>Non-trainable params: 1,041,025<br/>_________________________________________________________________</span></pre><h1 id="d6ea" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">4.火车</h1><p id="3278" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">终于准备好训练我们的 GAN 了！你觉得代码看起来奇怪吗？别担心，我会解释每一步。</p><h2 id="54fe" class="nl mg it bd mh nm nn dn ml no np dp mp lh nq nr mr ll ns nt mt lp nu nv mv nw bi translated">代码:</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx me l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GAN 的训练环路</p></figure><p id="a98d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">外层用于遍历各个时期，内层用于批处理。我已经训练了 80 个时期的模型，并且批次大小是 128。因此，在一个时期中，我们将有 2909(每时期步数= ⌊数据数量 samples/batch_size⌋=⌊372,450/128⌋= 2909)步。</p><h2 id="5f6c" class="nl mg it bd mh nm nn dn ml no np dp mp lh nq nr mr ll ns nt mt lp nu nv mv nw bi translated">G 固定时的 D 列:</h2><p id="3a34" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">首先，通过从标准正态分布中随机抽取数字来形成噪声向量的 batch_size 数目。然后将这些向量交给生成器来创建假图像。现在我们从训练数据中画出真实图像的 batch_size 数。为了得到鉴别器的输入，我们需要连接假数据和真数据。相应的，我们需要提到标签向量(0:假数据，1:真数据)。但是等等，代码说的是 0.1 和 0.9！WTH 是怎么回事？</p><blockquote class="nf ng nh"><p id="af7b" class="ky kz nd la b lb lc ju ld le lf jx lg ni li lj lk nj lm ln lo nk lq lr ls lt im bi translated">这种技术叫做<a class="ae nc" href="https://arxiv.org/pdf/1906.02629.pdf" rel="noopener ugc nofollow" target="_blank">水平平滑</a>。它防止鉴别者对其预测过于自信。</p></blockquote><p id="36a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们为鉴别器调用 train_on_batch 函数，并传递数据标签对。</p><h2 id="8022" class="nl mg it bd mh nm nn dn ml no np dp mp lh nq nr mr ll ns nt mt lp nu nv mv nw bi translated">G 固定时的 D 列:</h2><p id="0d99" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">这里，我们只需要噪声向量和标签。标签向量包含 1。等等，生成器制造假数据，所以标签不应该是 0 吗？</p><blockquote class="nf ng nh"><p id="2cb5" class="ky kz nd la b lb lc ju ld le lf jx lg ni li lj lk nj lm ln lo nk lq lr ls lt im bi translated">是的。但在这里我们故意给错误的标签，使歧视犯错误。原因是我们希望发电机的性能优于鉴别器。通过这样做，G 将知道 D 在被给定真实标签时的行为，并且它(G)将相应地改变它的权重来愚弄 D。记住，在这个阶段，我们没有改变鉴别器的权重，因此鉴别器没有“忘记”任何东西。</p></blockquote><p id="c809" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们为生成器调用 train_on_batch 函数，并传递数据标签对。而那就是朋友，一个甘是怎么练出来的！</p></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><p id="0824" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我向您展示一些我的模型产生的最佳(精选)结果…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/13fefd15a20a959254b63057b0f89305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3fA4PB2yAVustvFQJNu_2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">人工智能生成的字母(T，P，J，S，A，E，L，U)</p></figure><p id="561d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里是这个项目的<a class="ae nc" href="https://github.com/Suji04/NormalizedNerd/blob/master/Alphabet%20GAN/english-alphabet-gan.ipynb" rel="noopener ugc nofollow" target="_blank">完整代码</a>。瞧啊。现在你知道如何训练甘了！</p><h2 id="5d5c" class="nl mg it bd mh nm nn dn ml no np dp mp lh nq nr mr ll ns nt mt lp nu nv mv nw bi translated">如果你想进一步探索，请看这个…</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">人工智能生成 2 种语言的字母(英语和孟加拉语)</p></figure><p id="0fe8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢阅读。下次见…学习愉快！</p></div></div>    
</body>
</html>