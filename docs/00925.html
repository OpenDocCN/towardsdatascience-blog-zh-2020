<html>
<head>
<title>Building a news aggregator from scratch: news filtering, classification, grouping in threads and ranking</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始构建新闻聚合器:新闻过滤、分类、线索分组和排名</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-news-aggregator-from-scratch-news-filtering-classification-grouping-in-threads-and-7b0bbf619b68?source=collection_archive---------9-----------------------#2020-01-27">https://towardsdatascience.com/building-a-news-aggregator-from-scratch-news-filtering-classification-grouping-in-threads-and-7b0bbf619b68?source=collection_archive---------9-----------------------#2020-01-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/803a9ab9119e8864a9705501d76e835c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KcTklW8jd5q_6ktz_ee3Dw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">假新闻头条来自<a class="ae kf" href="https://www.designboom.com/design/the-fake-newsstand-tbwa-chiat-day-columbia-journalism-review-11-05-2018/" rel="noopener ugc nofollow" target="_blank">https://www . design boom . com/design/the-fake-news stand-TBWA-chiat-day-Columbia-journalism-review-11-05-2018/</a></p></figure><blockquote class="kg"><p id="d844" class="kh ki it bd kj kk kl km kn ko kp kq dk translated">这篇文章背后的想法是展示一种合理简单的方法，人们可以在几周内实现这种方法来解决现实世界中的一个问题，即创建一个新闻聚合器，如<a class="ae kf" href="https://news.google.com/" rel="noopener ugc nofollow" target="_blank">谷歌新闻</a>或<a class="ae kf" href="https://yandex.com/news" rel="noopener ugc nofollow" target="_blank"> Yandex 新闻</a>，从网上搜集的数百万条新闻中显示最热门的新闻线索。</p></blockquote><h1 id="7aec" class="kr ks it bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">问题陈述和限制</h1><p id="6b3c" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml kq im bi translated">所以这是另一篇关于 NLP 的文章，我将描述在<a class="ae kf" href="https://contest.com/docs/data_clustering" rel="noopener ugc nofollow" target="_blank">电报数据聚类竞赛</a>中开发的文本过滤、分类、分组和排序的一些算法。这篇文章背后的动机是为了证明你可以构建一个像样的文本处理系统，甚至不需要 GPU 就可以在你的笔记本电脑上运行。</p><p id="2295" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">比赛包括五项任务——检测新闻语言，从其他文本中过滤新闻(如百科文章、一些随机的娱乐帖子、博客帖子等)，从七个类别(社会、经济、体育、科学、技术、娱乐和其他)中选择一个进行新闻分类，将新闻按线索分组，并根据重要性对这些线索进行排序。<em class="mr"> </em>此处有完整的比赛规则<a class="ae kf" href="https://contest.com/docs/data_clustering" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="eb31" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">特定算法和工具的选择在很大程度上取决于竞赛规则，这些规则对实施施加了一些限制——每个任务必须在 60 秒内执行，每 1000 篇文章一台 Debian 机器，有 8 个 CPU 和 16 Gb 的 RAM，不应该使用外部服务或 API，算法甚至不应该假设有互联网连接(例如，下载一些预训练的模型)。因此，没有像伯特，阿尔伯特或 GPT-2 SOTA 变压器模型应该被涉及。</p><p id="756e" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">本文中描述的高级解决方案体系结构如下所示:</p><ol class=""><li id="f9ab" class="ms mt it lr b ls mm lw mn ma mu me mv mi mw kq mx my mz na bi translated">文本预处理和矢量化</li><li id="8a13" class="ms mt it lr b ls nb lw nc ma nd me ne mi nf kq mx my mz na bi translated">用具有 LSTM 和注意层的定制深度神经网络(DNN)进行文本分类</li><li id="e8c9" class="ms mt it lr b ls nb lw nc ma nd me ne mi nf kq mx my mz na bi translated">利用由每个组内的 Levenshtein 距离控制的最近邻搜索算法对线索中的文本进行分组。</li><li id="99ab" class="ms mt it lr b ls nb lw nc ma nd me ne mi nf kq mx my mz na bi translated">按重要性排列新闻线索</li></ol><p id="eec5" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">注意:这篇文章中描述的所有代码和想法都是在比赛期间(2 周)开发的，尽管分组算法和代码样式的一些微调是在新年假期之后执行的。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h2 id="9781" class="nn ks it bd kt no np dn kx nq nr dp lb ma ns nt lf me nu nv lj mi nw nx ln ny bi translated">原始数据解析</h2><p id="5bad" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml kq im bi translated">竞赛参与者被提供成千上万的出版物，这些出版物被保存为 html 文件，包含新闻标题、文本，有时还包含图像、出版日期、作者和媒体来源。我使用<a class="ae kf" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> Beatifulsoup </a>库作为一个方便的 html 解析器，将所有需要的数据提取到 pandas dataframe 中。</p><h2 id="e1e1" class="nn ks it bd kt no np dn kx nq nr dp lb ma ns nt lf me nu nv lj mi nw nx ln ny bi translated">语言检测</h2><p id="3e11" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml kq im bi translated">这部分非常简单——我使用了一个快速的<a class="ae kf" href="https://pypi.org/project/langdetect/" rel="noopener ugc nofollow" target="_blank"> langdetect </a>实现作为语言检测器，并在默认情况下用标题来加速检测——对平均长度大 10 倍的文本的语言检测速度较慢。</p><figure class="nz oa ob oc gt ju"><div class="bz fp l di"><div class="od oe l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">语言检测块</p></figure><p id="2e15" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">这一步每 1000 个文件用了 12 秒<strong class="lr iu">和 T5，检测准确率超过 99%。结果数据如下所示:</strong></p><figure class="nz oa ob oc gt ju"><div class="bz fp l di"><div class="od oe l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">带有检测到的语言的新闻示例(仅标题)</p></figure><h2 id="59c4" class="nn ks it bd kt no np dn kx nq nr dp lb ma ns nt lf me nu nv lj mi nw nx ln ny bi translated">文本预处理逻辑</h2><p id="7163" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml kq im bi translated">文本矢量化逻辑是参赛者必须提出的核心算法决策之一。我们有一个足够大和足够多样化的约 100 万篇文章的语料库来使用预训练的单词嵌入，而不是基本的 TF-IDF 方法。但是首先我们必须执行通用的标记化和词干化过程。我使用了来自 nltk 库的停用词表和波特斯特梅尔。</p><figure class="nz oa ob oc gt ju"><div class="bz fp l di"><div class="od oe l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">标记化功能</p></figure><p id="69bb" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">在这一步之后，每个文本都由单词标记列表来表示。</p><p id="6102" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">下一步是用一个来自预先训练的语言模型的向量替换列表中的每个标记，该模型是<a class="ae kf" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> Glove </a>或<a class="ae kf" href="https://fasttext.cc" rel="noopener ugc nofollow" target="_blank"> fasttext </a>。</p><p id="a7fd" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">这个操作的结果是，每个文本现在都由一个语义丰富的单词向量列表来表示。我对列表的最大长度做了限制——50 个单词，标题与文章正文的开头连接在一起。</p><p id="c40a" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">为了均衡所有向量的长度，已经执行了填充操作。我们现在已经得到了文本语料库的特征张量，每一行都代表了所选维度的一系列预先训练的词向量。</p><figure class="nz oa ob oc gt ju"><div class="bz fp l di"><div class="od oe l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">使用预训练单词嵌入的矢量化功能</p></figure><h1 id="0bfe" class="kr ks it bd kt ku kv kw kx ky kz la lb lc of le lf lg og li lj lk oh lm ln lo bi translated">深度神经网络架构</h1><p id="1409" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml kq im bi translated">因为我们有足够的数据来进行神经网络训练，所以我决定使用深度神经网络(DNN)分类器来区分新闻和非新闻，并定义新闻主题。</p><p id="3f2e" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">考虑到这是一场算法竞赛，最大限度提高解决方案准确性的一个显而易见的选择是 SOTA NLP 模型架构，即某种大型转换器，如 BERT，但正如我之前提到的，这种模型太大太慢，无法通过对硬件和文本处理速度的限制。另一个缺点是，这种模型的培训需要几天时间，让我没有时间对模型进行微调。</p><p id="fa5f" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">我必须想出一个更简单的架构，所以我实现了一个轻量级的神经网络，它有一个 RNN (LSTM)层，接受代表文本的单词嵌入序列，并在其上有一个注意力层。网络的输出应该是类别概率(用于新闻过滤步骤的二元分类和用于新闻类别检测的多元分类)，因此网络的上部由一组完全连接的层组成。</p><h2 id="e6e5" class="nn ks it bd kt no np dn kx nq nr dp lb ma ns nt lf me nu nv lj mi nw nx ln ny bi translated">新闻话题检测——多类分类</h2><p id="a470" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml kq im bi translated">我将使用多类分类器(检测新闻类别)作为例子来解释特定 DNN 体系结构的选择，因为它的目标更具挑战性，并且它的性能独立于在二元分类器中用来划分正负类之间的界限的阈值。</p><p id="0fb9" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">我们的神经网络的上部由三个<strong class="lr iu">密集层</strong>组成，输出层具有 7 个单元，对应于具有 softmax <strong class="lr iu"> </strong>激活函数和分类交叉熵<strong class="lr iu"> </strong>作为损失函数的类的数量。</p><p id="bda2" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">为了训练它，我使用了<a class="ae kf" href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener ugc nofollow" target="_blank"> News_Category_Dataset </a>并应用了映射逻辑，以便将最初的 31 个新闻类别归入 7 个类别之一:社会、经济、体育、科技、娱乐、科学和其他，以符合竞赛目标。</p><figure class="nz oa ob oc gt ju"><div class="bz fp l di"><div class="od oe l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">DNN 模型架构——多类分类器</p></figure><p id="f7fc" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">我想分享神经网络模型的架构选择和超参数微调背后的逻辑。</p><ol class=""><li id="20f8" class="ms mt it lr b ls mm lw mn ma mu me mv mi mw kq mx my mz na bi translated"><strong class="lr iu">漏失层</strong>增加了我们模型的泛化能力，并防止它过度拟合(漏失层在训练阶段的每次更新中随机地将给定百分比的权重归零)。学习曲线清楚地表明了模型在没有脱落层的情况下的过度拟合-训练集的精度增长到 95%，而验证数据集的精度在训练期间几乎没有增长。</li><li id="923d" class="ms mt it lr b ls nb lw nc ma nd me ne mi nf kq mx my mz na bi translated">应用于 LSTM 注意力结构顶部的<strong class="lr iu">批次标准化层</strong>在每批脱落后标准化激活，使激活平均值接近 0，激活标准偏差接近 1。它有助于提高较大批量的测试精度，降低较小批量的测试精度。</li><li id="a26c" class="ms mt it lr b ls nb lw nc ma nd me ne mi nf kq mx my mz na bi translated"><strong class="lr iu">应用于密集层的正则化</strong>惩罚层参数的极值。</li><li id="a38b" class="ms mt it lr b ls nb lw nc ma nd me ne mi nf kq mx my mz na bi translated"><strong class="lr iu">批量</strong>选择也会影响一个型号的性能。大小越大，模型训练越快，每步计算的梯度向量就越精确。这导致噪声减少，这使得模型更容易收敛到局部最小值，因此批量大小的选择通常是速度、内存消耗和模型性能之间的折衷。32 到 256 之间的值是常见的选择，在我们的例子中，模型显示出最高的精确度，批量大小为 64。将批量增加到 512 或 1024 会显著降低模型的准确性(相应地降低 2%和 4%)</li></ol><figure class="nz oa ob oc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/05e9f03cce8a601b68c3c4b27d284044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GeOx8TZPDg6qW7OjfQX6CQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">模型的性能取决于架构和超参数</p></figure><h2 id="df20" class="nn ks it bd kt no np dn kx nq nr dp lb ma ns nt lf me nu nv lj mi nw nx ln ny bi translated">注意力层解释</h2><p id="623c" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml kq im bi translated">关于我们模型中使用的注意机制，有必要说几句话。Raffel 等人的<a class="ae kf" href="https://arxiv.org/abs/1512.08756" rel="noopener ugc nofollow" target="_blank"> arXiv 论文中描述了所用方法背后的理论</a>这是一个简化的前馈神经网络注意力模型，用于解决长序列的 RNN 信息流问题。特别地，注意层提供了到完全连接层的最佳过渡，创建了<strong class="lr iu">上下文向量</strong>(输入单词向量序列的嵌入)<strong class="lr iu">作为输入序列</strong>的隐藏状态的加权平均，其中权重表示序列元素的重要性。显式符号如下所示:</p><figure class="nz oa ob oc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oj"><img src="../Images/919cd053ab60c7aa1db3b0e4056b0672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AtKvTMuA4c8uDHcLcEejsA.png"/></div></div></figure><p id="b14e" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">其中 T 是序列的长度，<strong class="lr iu"> a </strong>是可学习的函数，即具有双曲正切激活函数的单隐层前馈网络，与全局模型共同训练。</p><figure class="nz oa ob oc gt ju"><div class="bz fp l di"><div class="od oe l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">注意层类，由 Kaggle 上的<a class="ae kf" href="https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043/code" rel="noopener ugc nofollow" target="_blank"> qqgeogor </a>实现</p></figure><p id="9978" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">该模型的缺点是它没有考虑输入序列中的顺序，但是对于我们的新闻标题矢量化任务来说，这并不像一些序列到序列问题(如短语翻译)那样重要。</p><p id="9f15" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">有许多出版物描述了为序列到序列问题设计的更复杂的注意力机制，一个好的开始是谷歌研究院的<a class="ae kf" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention#write_the_encoder_and_decoder_model" rel="noopener ugc nofollow" target="_blank">带注意力的神经机器翻译</a>教程，我也建议检查一下<a class="ae kf" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">注意力？立正！</a>Lilian Weng 的文章，概述了注意力机制及其进化，并确保您已经阅读了最初的<a class="ae kf" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank"> Bahdanau 等人，2015 年</a>的论文。在后变形金刚时代最新的 LSTM +注意力论文中，我推荐阅读斯蒂芬·梅里蒂最近的<a class="ae kf" href="https://arxiv.org/abs/1911.11423" rel="noopener ugc nofollow" target="_blank">单头注意力 RNN </a>，这证明了巨大的变形金刚不是唯一可能的方法。</p><p id="cdca" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">另外，不要忘记在 Attention 类中添加一个保存配置，因为它可以无缝加载带有自定义层的保存模型。</p><p id="9e4f" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">另一种更简单的向致密层的过渡是用于张量整形的平坦层。</p><p id="a7ce" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated"><strong class="lr iu">其他想法&amp;结果</strong></p><p id="472d" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">为了提高分类器质量，可以利用<em class="mr"> </em>提供的其他信息，如出版物来源和作者，并将它们作为一次性编码特征添加到网络中，但竞赛规则明确表示，将在具有其他特定来源列表的不同数据集上评估算法。</p><p id="602b" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">为了加快训练阶段的速度，我在装有 TeslaK80 GPU 运行时的<a class="ae kf" href="https://colab.research.google.com/notebooks/welcome.ipynb" rel="noopener ugc nofollow" target="_blank"> colab </a>笔记本上运行了代码。</p><figure class="nz oa ob oc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/e21474a68effa530fd7964bd23f36db0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KpBQ9G8mnnD4SU--xczj8g.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">典型模型的学习曲线</p></figure><figure class="nz oa ob oc gt ju"><div class="bz fp l di"><div class="od oe l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">具有新闻类别和二元新闻/非新闻分数的新闻样本</p></figure><p id="72ab" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">每 1000 个文本的分类步骤需要 13 秒。</p><h2 id="b503" class="nn ks it bd kt no np dn kx nq nr dp lb ma ns nt lf me nu nv lj mi nw nx ln ny bi translated">新闻/非新闻—二元分类</h2><p id="fa58" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml kq im bi translated">为了从给定的出版物数据集中只过滤新闻，我们必须实现一个二元分类器。实际上这一步在新闻分类之前就已经完成了。二进制分类器具有非常相似的架构，但在输出层有明显的不同——最后一层有 1 个单元，具有 sigmoid 激活函数和二进制交叉熵<strong class="lr iu"> </strong>作为损失函数。</p><p id="872b" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">竞赛组织者提供的数据包含了超过 90%的新闻出版物，所以我决定按来源(卫报、彭博、CNN 等)过滤 100%的新闻，然后用一个英文<a class="ae kf" href="https://www.kaggle.com/urbanbricks/wikipedia-promotional-articles#good.csv" rel="noopener ugc nofollow" target="_blank"> Wikipedia atricles </a>(好的，非宣传性的)来代表非新闻类。</p><figure class="nz oa ob oc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ol"><img src="../Images/673029ce51dc018eac137763a9128a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BMykJvS9BQBvBvZNiefTLg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">典型二元模型的学习曲线</p></figure><p id="2776" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">训练的二进制分类器模型输出对象是正(1)类的概率，为了解释这些概率，我们需要施加阈值，该阈值将区分新闻和非新闻。在对边缘病例进行了长时间的仔细观察之后，这种选择已经被手动执行。</p><p id="8c54" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">每 1000 个文本的分类步骤需要 14 秒。分类结果可以在上表中观察到。</p><h1 id="ba55" class="kr ks it bd kt ku kv kw kx ky kz la lb lc of le lf lg og li lj lk oh lm ln lo bi translated">将新闻按线索分组</h1><blockquote class="kg"><p id="19bb" class="kh ki it bd kj kk om on oo op oq kq dk translated">新闻分组任务通过在文本嵌入向量上构建球树来解决，然后在每组邻居内使用由归一化 Levenshtein 距离控制的自适应搜索半径来搜索该树。</p></blockquote><p id="1cd5" class="pw-post-body-paragraph lp lq it lr b ls or lu lv lw os ly lz ma ot mc md me ou mg mh mi ov mk ml kq im bi translated"><strong class="lr iu">文本矢量化</strong></p><p id="9a43" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">为了对新闻进行分组，我们首先应该在数据集上引入某种度量。因为我们使用 DNNs 进行文本处理和分类，所以获得文本向量的最明显的方法是通过将文本传递通过预训练的多类 DNN 而获得嵌入，而不需要最后一层。我使用 128 单位的密集层作为输出来创建文本嵌入向量。</p><p id="0ad8" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">后来我发现这种方法并不是最佳方法——当我切换到更简单的 TF-IDF 矢量化时，新闻分组的性能明显提高。这是很容易解释的——在新闻分类中，我们需要概括整个新闻的语义，而不管特定的政治家的名字、小工具、名人，甚至特定的环境，馈送给 DNN 的预训练单词向量的序列是这项任务的合适选择。在新闻分组中，我们处理不同的设置——每个线程应该描述一个非常特殊的事件——某人发生了一些事情，人物姓名，甚至动词和状语修饰语在整个新闻线程中应该是相同的，因此经典的 TF-IDF(计算每个文本中 n 元语法频率的向量，通过整个语料库中的 n 元语法频率归一化)是丢失较少有价值信息的方法。为了对抗 TF-IDF 输出矩阵的稀疏性，我应用了 SVD 分解，将每个文本的向量压缩到选定的维度(在我们的例子中是 1000)。</p><figure class="nz oa ob oc gt ju"><div class="bz fp l di"><div class="od oe l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">在 tom 上使用 SVD 对 TF-IDF 文本进行矢量化，以创建密集嵌入</p></figure><p id="165c" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated"><strong class="lr iu">快速最近邻搜索</strong></p><p id="dfbf" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">下一步实际上是文本分组。这项任务的基本无监督方法是聚类，但由于我们不可能从第一次拍摄就获得完美的聚类，我们的算法需要迭代方法，并且整个数据集的聚类是昂贵的。此外，我们没有适当的分组质量指标来运行自动超参数调整程序，我们也不应该手动调整它们，因为我们很可能需要不同的超参数集来用于不同的新闻数据集。</p><p id="e5db" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">我决定<strong class="lr iu">一个更精确和灵活的方法是在嵌入向量的文本上构建一个快速搜索索引，然后查询这个索引。</strong>快速高维最近邻搜索的有效方法是在文本嵌入上建立二叉空间划分树，即<a class="ae kf" href="https://en.wikipedia.org/wiki/Ball_tree" rel="noopener ugc nofollow" target="_blank">球树</a>或<a class="ae kf" href="https://en.wikipedia.org/wiki/K-d_tree" rel="noopener ugc nofollow" target="_blank"> KD 树</a>(k 维二叉查找树是一种用于组织 k 维空间中的点的空间划分数据结构)，而无需显式计算数据集中的所有距离。这些算法的树构造复杂度为 O(N log(N))。我特别选择了一个<a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" rel="noopener ugc nofollow" target="_blank"> scikit-learn Ball Tree 实现</a>，因为它比高维 KD 树具有更低的查询时间 O(D log(N))，并且我们必须优化查询时间，因为我们打算对数据集中的每个元素使用不同的搜索半径来执行迭代搜索。有关 KDtree 和 BallTree 数据结构之间的差异以及性能基准测试的更多详细信息，请参考 scikit-learn 撰稿人 Jake VanderPlasof 的这篇精彩文章。</p><figure class="nz oa ob oc gt ju"><div class="bz fp l di"><div class="od oe l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">初始化树结构</p></figure><p id="ced3" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated"><strong class="lr iu">分组算法</strong></p><p id="c827" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">好了，现在我们终于有了所有论文的索引，可以根据不同样本之间的距离将它们分组。</p><p id="0a99" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">我没有足够的时间来思考一些更复杂的方法，只是在每个新闻类别中的所有论文上创建了一个循环<strong class="lr iu">(我们可以利用前面的算法步骤来按新闻类别划分我们的数据集)<strong class="lr iu">检查它们在半径 r_start </strong>中的邻居，r_start 是根据经验选择的，它使用的论文比实际包含的线程多一点。然后<strong class="lr iu">我计算了一个经验泛函 variative _ criterium _ norm——组内文本之间的归一化 Levenstein 距离——并迭代地减小搜索半径 r_curr，直到这个泛函变得小于经验发现的约束或搜索半径大小达到 r_min 约束</strong>。如果在给定区域没有新闻，我增加搜索半径，直到我们找到一些邻居，然后切换到半径递减分支。这种迭代搜索背后的想法是，相似的新闻有部分相似的标题(一些实体，如主语、宾语，有时动词，在新闻线索中是不变的)。从代码片段中可以更容易地看到所开发算法的其他细节。</strong></p><figure class="nz oa ob oc gt ju"><div class="bz fp l di"><div class="od oe l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">新闻分组逻辑</p></figure><p id="4450" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">有 7 个超参数控制算法:r_min，r _ max 这些参数控制查询区域的大小，r_start，r _ step 控制查询区域动态，vc_min，vc_max，delta_max，控制组内归一化 Levenshtein 距离的值-这定义了组中新闻标题的方差。在选择了最终的文本嵌入大小(SVD 中的 n_components)和使用的 n_grams 范围(我使用了 1-3)等矢量化参数之后，应该对这些超参数进行调整。</p><p id="b9f3" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">我们如何估计一个“适当的”新闻分组并不是很明显，所以在我得到一个合理的分组结果后，我没有花太多时间玩超参数——我的意图是提出一个工作方法来解决给定约束条件下的问题。显然，还有一些改进的空间，比如检查我们是否可以合并一些组，以及过滤掉一些偶然的噪音。实际上，为了获得合理的组数量和密度，分组超参数应该针对每个数据集进行微调，因此可以有一个外部循环来对它们实施一种随机搜索。了解我们得到的特定分组并评估其质量的方法之一是检查分组大小分布直方图。</p><figure class="nz oa ob oc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ow"><img src="../Images/c9e5deea3a32bf8ca566c1e976816cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1YZRhSkd18AATn7Nkoieg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">在测试数据集上获得的组大小分布直方图</p></figure><p id="b747" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">事实上，所描述的方法可以被看作是 DBSCAN 集群的一个相关部分。</p><p id="1e8b" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">执行时间取决于为数据集选择的超参数和数据结构，典型值从 8.5 秒/ 1000 张纸到 25 秒/ 1000 张纸，包括由昂贵的 SVD 操作定义的矢量化时间。</p><p id="7be7" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">很抱歉列出了这么长的列表，这里是按团体大小排列的社会类别的<a class="ae kf" href="https://gist.github.com/isvanilin/3ea2fc6dc43162855c59bdc281ea642f#file-news_groups-txt" rel="noopener ugc nofollow" target="_blank">全部结果</a></p><figure class="nz oa ob oc gt ju"><div class="bz fp l di"><div class="od oe l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">分组结果—前 3 组(按大小排序)</p></figure><h1 id="ed81" class="kr ks it bd kt ku kv kw kx ky kz la lb lc of le lf lg og li lj lk oh lm ln lo bi translated">线程排名</h1><p id="22fa" class="pw-post-body-paragraph lp lq it lr b ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml kq im bi translated">实际上，有很多功能可以用于线程排名，问题是这个排名可能非常主观，取决于特定用户的地区和兴趣。我们可能同意存在全球和本地新闻，对政治问题的一些猜测/意见和纯粹的事实，但关于 Trump 弹劾听证会的一些新信息可能比全球但遥远的悲剧更重要，如澳大利亚的森林火灾<strong class="lr iu">新闻的重要性是一种感知，而不是一种客观特征</strong>。</p><p id="6e02" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">鉴于上述情况，我将仅描述处理该任务的方法:线程排名的最明显特征是线程中的出版物数量、来源的排名(在<em class="mr">社会</em>类别中，例如彭博和金融时报是最受尊敬的来源)以及线程的语义，如“国际政治”、“全球经济”、“战争”、“全球事故”、“本地事故/犯罪”等。这种方法预先假定了这些语义类别的手动引入和排序以及新闻源的手动排序。如果我们不想手动创建特征，另一种可能的方法是手动对线程的一些选择进行排序，计算它们的平均语义向量(对我们使用的任何矢量化进行平均)，然后使用这些向量作为预测器，根据我们分配给线程重要性的分数训练出排序模型(一种回归模型)。</p><p id="0c9d" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">事实上，只要按照大小对线程进行排序，就可以获得相当合理的排名。鉴于这一事实以及我在比赛期间没有实现排名部分的事实，我将让那些热情的人自由尝试上面讨论的任何方法。</p><p id="e15e" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">一个重要的问题是，竞赛预先假定了一个静态排名——我们得到了同一日期的所有新闻，但实际上新闻是时间相关的，并且失去了它们的新颖性久而久之，这种相关性的衰减可以用 exp(-t)函数来描述。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="4a0b" class="pw-post-body-paragraph lp lq it lr b ls mm lu lv lw mn ly lz ma mo mc md me mp mg mh mi mq mk ml kq im bi translated">感谢你阅读这篇长文，我希望你已经找到了一些可以在你的 NLP 项目中使用的想法。</p></div></div>    
</body>
</html>