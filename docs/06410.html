<html>
<head>
<title>BERT Text Classification in a different language</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不同语言的文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bert-text-classification-in-a-different-language-6af54930f9cb?source=collection_archive---------19-----------------------#2020-05-22">https://towardsdatascience.com/bert-text-classification-in-a-different-language-6af54930f9cb?source=collection_archive---------19-----------------------#2020-05-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="86a2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用HuggingFace和简单的变形金刚构建非英语(德语)BERT多类文本分类模型。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8bc8b3cb605e1f2da324fefb1ac87848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K7pT5fv-b_BfXFpbfYKimQ.jpeg"/></div></div></figure><p id="e5e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">原载于2020年5月22日</em><a class="ae lr" href="https://www.philschmid.de/bert-text-classification-in-a-different-language" rel="noopener ugc nofollow" target="_blank"><em class="lq">https://www . philschmid . de</em></a><em class="lq">。</em></p><h1 id="0dc6" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">介绍</h1><p id="aaab" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">目前，我们有75亿人生活在世界上大约200个国家。其中只有12亿人以英语为母语。这导致了大量非结构化的非英语文本数据。</p><p id="3fa9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">大多数教程和博客帖子都用英语演示了如何使用基于BERT的架构来构建文本分类、情感分析、问答或文本生成模型。为了克服这种缺失，我将向大家展示如何建立一个非英语的多类文本分类模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/aed8767fcc646f8ef56a8312b0d3d724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*HF8FuRQEkbqTzm44.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">英语母语者的世界</p></figure><p id="6456" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">打开我的文章，让我来猜猜你是否听说过伯特。如果你还没有，或者你想更新一下，我推荐你阅读这篇<a class="ae lr" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><p id="aadd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在深度学习中，对于如何建立语言模型，目前有两种选择。您可以构建单语模型或多语言模型。</p><blockquote class="mu mv mw"><p id="5293" class="ku kv lq kw b kx ky ju kz la lb jx lc mx le lf lg my li lj lk mz lm ln lo lp im bi translated"><em class="it">“使用多种语言，还是不使用多种语言，这是个问题”——就像莎士比亚会说的那样</em></p></blockquote><p id="fe1e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">多语言模型描述了可以理解不同语言的机器学习模型。来自Google research的<a class="ae lr" href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="noopener ugc nofollow" target="_blank"> mBERT </a>就是一个多语言模型的例子。<a class="ae lr" href="https://github.com/google-research/bert/blob/master/multilingual.md" rel="noopener ugc nofollow" target="_blank">该型号支持并理解104种语言。</a>单语模特，顾名思义能听懂一种语言。</p><p id="51b4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">多语言模型已经在某些任务上取得了良好的效果。但是这些模型更大，需要更多的数据，也需要更多的时间来训练。由于需要大量的数据和时间资源，这些特性导致了更高的成本。</p><p id="bddf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于这个事实，我将向你展示如何训练一个单语的非英语的基于BERT的多类文本分类模型。哇，那是一个长句子！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/a20d5e8bc73784e7df33544cb7151b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Dqf1UKDVAEHxfjtY.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">伯特——得到了迷因</p></figure><h1 id="e903" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">辅导的</h1><p id="468d" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">我们将使用<a class="ae lr" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank">简单变形金刚</a>——一个基于HuggingFace的<a class="ae lr" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>库的NLP库。Simple Transformers允许我们用几行代码来微调Transformer模型。</p><p id="0297" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">作为数据集，我们将使用由德语推文组成的<a class="ae lr" href="https://projects.fzai.h-da.de/iggsa/projekt/" rel="noopener ugc nofollow" target="_blank"> Germeval 2019 </a>。我们将检测和分类辱骂性语言的推文。这些推文分为四类:<code class="fe nb nc nd ne b">PROFANITY</code>、<code class="fe nb nc nd ne b">INSULT</code>、<code class="fe nb nc nd ne b">ABUSE</code>和<code class="fe nb nc nd ne b">OTHERS</code>。在这个数据集上获得的最高分是<code class="fe nb nc nd ne b">0.7361</code>。</p><h2 id="ea94" class="nf lt it bd lu ng nh dn ly ni nj dp mc ld nk nl me lh nm nn mg ll no np mi nq bi translated">我们将:</h2><ul class=""><li id="bd91" class="nr ns it kw b kx mk la ml ld nt lh nu ll nv lp nw nx ny nz bi translated">安装简单的变压器库</li><li id="e738" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">选择预先训练好的单语模型</li><li id="9fb8" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">加载数据集</li><li id="469e" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">训练/微调我们的模型</li><li id="d866" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">评估培训的结果</li><li id="01d5" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">保存训练好的模型</li><li id="2b4d" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">加载模型并预测一个真实的例子</li></ul><p id="73bc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本教程中，我使用了带有GPU运行时的Google Colab。如果你不确定如何使用GPU运行时，看看这里的<a class="ae lr" href="https://www.philschmid.de/google-colab-the-free-gpu-tpu-jupyter-notebook-service" rel="noopener ugc nofollow" target="_blank"/>。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="e195" class="ls lt it bd lu lv om lx ly lz on mb mc jz oo ka me kc op kd mg kf oq kg mi mj bi translated">安装简单的变压器库</h1><p id="5931" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">首先，我们用pip安装<code class="fe nb nc nd ne b">simpletransformers</code>。如果你没有使用Google colab，你可以点击查看安装指南<a class="ae lr" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="90d6" class="ls lt it bd lu lv om lx ly lz on mb mc jz oo ka me kc op kd mg kf oq kg mi mj bi translated">选择预先训练好的单语模型</h1><p id="153b" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">接下来，我们选择预训练模型。如上所述，简单变形金刚库基于HuggingFace的变形金刚库。这使我们能够使用<a class="ae lr" href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">变形金刚库</a>中提供的每个预训练模型和所有社区上传的模型。对于包括所有社区上传模型的列表，我指的是<a class="ae lr" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/models</a>。</p><p id="49ed" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将使用<code class="fe nb nc nd ne b">distilbert-base-german-cased</code>型号，一种<a class="ae lr" href="https://huggingface.co/transformers/model_doc/distilbert.html" rel="noopener ugc nofollow" target="_blank">更小、更快、更便宜的BERT </a>版本。它使用的参数比<code class="fe nb nc nd ne b">bert-base-uncased</code>少40%,运行速度快60%,同时仍然保留了95%以上的Bert性能。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="f1cc" class="ls lt it bd lu lv om lx ly lz on mb mc jz oo ka me kc op kd mg kf oq kg mi mj bi translated">加载数据集</h1><p id="cb60" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">数据集存储在两个文本文件中，我们可以从<a class="ae lr" href="https://projects.fzai.h-da.de/iggsa/" rel="noopener ugc nofollow" target="_blank">竞赛页面</a>中检索。下载它们的一个选择是使用两个简单的<code class="fe nb nc nd ne b">wget</code> CLI命令。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="9a68" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">之后，我们使用一些<code class="fe nb nc nd ne b">pandas</code>魔法来创建一个数据帧。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="17e7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为我们没有测试数据集，所以我们分割数据集— <code class="fe nb nc nd ne b">train_df</code>和<code class="fe nb nc nd ne b">test_df</code>。我们将90%的数据用于训练(<code class="fe nb nc nd ne b">train_df</code>)，10%用于测试(<code class="fe nb nc nd ne b">test_df</code>)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><h1 id="13c9" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">加载预训练模型</h1><p id="1d3a" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">下一步是加载预先训练好的模型。我们通过创建一个名为<code class="fe nb nc nd ne b">model</code>的<code class="fe nb nc nd ne b">ClassificationModel</code>实例来做到这一点。此实例采用以下参数:</p><ul class=""><li id="dff7" class="nr ns it kw b kx ky la lb ld ot lh ou ll ov lp nw nx ny nz bi translated">架构(在我们的案例中是<code class="fe nb nc nd ne b">"bert"</code>)</li><li id="bc7d" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">预训练模型(<code class="fe nb nc nd ne b">"distilbert-base-german-cased"</code>)</li><li id="0c10" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">类别标签的数量(<code class="fe nb nc nd ne b">4</code>)</li><li id="f214" class="nr ns it kw b kx oa la ob ld oc lh od ll oe lp nw nx ny nz bi translated">还有我们训练用的超参数(<code class="fe nb nc nd ne b">train_args</code>)。</li></ul><p id="0ff1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您可以在广泛的可能性范围内配置超参数。有关每个属性的详细描述，请参考<a class="ae lr" href="https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><h1 id="a423" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">训练/微调我们的模型</h1><p id="be74" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">为了训练我们的模型，我们只需要运行<code class="fe nb nc nd ne b">model.train_model()</code>并指定要训练的数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><h1 id="5353" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">评估培训的结果</h1><p id="1e0b" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">在我们成功地训练了我们的模型之后，我们可以对它进行评估。因此，我们创建一个简单的辅助函数<code class="fe nb nc nd ne b">f1_multiclass()</code>，用于计算<code class="fe nb nc nd ne b">f1_score</code>。<code class="fe nb nc nd ne b">f1_score</code>是对模型精度的测量。更多关于那个<a class="ae lr" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="ef94" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们取得了<code class="fe nb nc nd ne b">0.6895</code>的<code class="fe nb nc nd ne b">f1_score</code>。最初，这似乎相当低，但请记住:在<a class="ae lr" href="https://projects.fzai.h-da.de/iggsa/submissions/" rel="noopener ugc nofollow" target="_blank"> Germeval 2019 </a>的最高提交量是<code class="fe nb nc nd ne b">0.7361</code>。如果不调整超参数，我们将获得前20名的排名。这是相当令人印象深刻的！</p><p id="3ea4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在以后的文章中，我将向您展示如何通过调优超参数来获得更高的<code class="fe nb nc nd ne b">f1_score</code>。</p><h1 id="654f" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">保存训练好的模型</h1><p id="ce9f" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">Simple Transformers会在每一步<code class="fe nb nc nd ne b">2000</code>和训练过程结束时自动保存<code class="fe nb nc nd ne b">model</code>。默认目录是<code class="fe nb nc nd ne b">outputs/</code>。但是<code class="fe nb nc nd ne b">output_dir</code>是一个超参数，可以被覆盖。我创建了一个助手函数<code class="fe nb nc nd ne b">pack_model()</code>，我们用它将所有需要的模型文件<code class="fe nb nc nd ne b">pack</code>到一个<code class="fe nb nc nd ne b">tar.gz</code>文件中进行部署。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><h1 id="791a" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">加载模型并预测一个真实的例子</h1><p id="d412" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">最后一步，我们加载并预测一个真实的例子。因为我们用<code class="fe nb nc nd ne b">pack_model()</code>提前一步打包了文件，所以我们必须先用<code class="fe nb nc nd ne b">unpack</code>打包它们。因此，我编写了另一个助手函数<code class="fe nb nc nd ne b">unpack_model()</code>来解包我们的模型文件。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="2616" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了加载一个保存的模型，我们只需要为我们保存的文件提供<code class="fe nb nc nd ne b">path</code>,并像我们在训练步骤中那样初始化它。<em class="lq">注意:在加载模型时，您需要指定正确的(通常与训练中使用的相同)参数。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="6830" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">初始化之后，我们可以使用<code class="fe nb nc nd ne b">model.predict()</code>函数对给定输入的输出进行分类。在这个例子中，我们从Germeval 2018数据集中提取了两条推文。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="4f9f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们的模型预测了正确的类别<code class="fe nb nc nd ne b">OTHER</code>和<code class="fe nb nc nd ne b">INSULT</code>。</p><h1 id="8d6a" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">简历</h1><p id="0584" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">总之，我们可以说我们实现了创建非英语的基于BERT的文本分类模型的目标。</p><p id="15a4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们的例子提到了德语，但可以很容易地转换成另一种语言。HuggingFace为法语、西班牙语、意大利语、俄语、汉语、…</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="3cb5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">感谢阅读。你可以在这里找到带有完整代码<a class="ae lr" href="https://colab.research.google.com/drive/1kAlGGGsZaFaFoL0lZ0HK4xUR6QS8gipn#scrollTo=JG2gN7KUqyjY" rel="noopener ugc nofollow" target="_blank">的colab笔记本。</a></p><p id="ef8f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你有任何问题，随时联系我。</p></div></div>    
</body>
</html>