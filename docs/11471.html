<html>
<head>
<title>Introduction to Early Stopping: an effective tool to regularize neural nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">早期停止介绍:一种有效的神经网络正则化工具</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/early-stopping-a-cool-strategy-to-regularize-neural-networks-bfdeca6d722e?source=collection_archive---------13-----------------------#2020-08-09">https://towardsdatascience.com/early-stopping-a-cool-strategy-to-regularize-neural-networks-bfdeca6d722e?source=collection_archive---------13-----------------------#2020-08-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="da7a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为什么以及如何使用提前停车</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/642ffa410e316c74c308d9f7dbd3c4ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NYXcWR66qFY3SUhd"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:Unsplash</p></figure><h1 id="9c74" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">过度拟合:</h1><blockquote class="ln lo lp"><p id="248b" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">对于所有的机器学习和深度学习问题来说，过拟合是一个非常严重的问题。当您的模型在训练数据上表现良好，但无法在测试数据上复制这种表现时，您可以理解这种情况的发生。</p></blockquote><p id="2482" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">如果你不太喜欢阅读博客，并且想开始写代码，请看看<a class="ae mq" href="https://www.kaggle.com/saptarsi/regularizing-with-early-stopping" rel="noopener ugc nofollow" target="_blank"> Kaggle 笔记本</a>，或者，你也可以看看我们的<a class="ae mq" href="https://youtu.be/cKVgmpYOdzg" rel="noopener ugc nofollow" target="_blank">视频教程</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/abd2964041bd371bb72f8ad3bd241afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HCEgjWqTKdCT6yEN-8jfwg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 1:三个模型的决策边界(图片来源:作者)</p></figure><p id="9838" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">在上图中，我们看到一个二元分类问题(蓝色类和绿色类)。这三个图表示由三个模型获得的决策边界。第一个模型是做出更简单的假设，第二个模型是做一份体面的工作。</p><blockquote class="ln lo lp"><p id="7bea" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">然而，真正的罪魁祸首是第三个模型，它试图记住训练数据，拾取训练数据中的微小波动，实际上对于完美的训练集精度来说走得太远了。<strong class="lt ir">模型的这种倾向被称为过拟合</strong>，这些模型没有学到很多东西，因此不能进一步应用它们的知识。(<strong class="lt ir">不能一概而论)</strong></p></blockquote><h1 id="9079" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">过拟合与模型参数的关系；</h1><p id="2d37" class="pw-post-body-paragraph lq lr iq lt b lu ms jr lw lx mt ju lz mn mu mc md mo mv mg mh mp mw mk ml mm ij bi translated">这些似乎是模型参数和过拟合之间的关系。让我们再来看看三个可供选择的模型，我们试图根据一个人多年的经验来预测他的工资</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/0d84ab48e7e30784d39b89650bee0c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*ya4U7gGn7tXr3wDbb_oiJg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 2:替代回归模型(图片来源:作者)</p></figure><p id="ad63" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">可以观察到-</p><ul class=""><li id="4bee" class="my mz iq lt b lu lv lx ly mn na mo nb mp nc mm nd ne nf ng bi translated">第一个模型有两个参数，解释了不同年份之间的联系。工资一般。只考虑 x 的线性项</li><li id="89a4" class="my mz iq lt b lu nh lx ni mn nj mo nk mp nl mm nd ne nf ng bi translated">第二个模型有三个参数，它很好地解释了数据，并考虑了二次项</li><li id="e62d" class="my mz iq lt b lu nh lx ni mn nj mo nk mp nl mm nd ne nf ng bi translated">第三个模型有四个参数，被诊断为过度拟合，它只是跟踪所有的工资。</li></ul><blockquote class="ln lo lp"><p id="5aea" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">最重要的部分来了，参数数量越多的模型越容易过度拟合，并且<strong class="lt ir">随着神经网络的参数数量越多，它越容易过度拟合。</strong></p></blockquote><h1 id="ef7b" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">正规化和提前停止:</h1><p id="3f1d" class="pw-post-body-paragraph lq lr iq lt b lu ms jr lw lx mt ju lz mn mu mc md mo mv mg mh mp mw mk ml mm ij bi translated">对抗<strong class="lt ir">过度拟合</strong>这一诅咒的一般策略集被称为<strong class="lt ir">规则化</strong>，早期停止就是这样一种技术。</p><p id="4059" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">想法很简单。该模型试图通过调整参数来疯狂地追逐训练数据上的损失函数。现在，我们保留另一组数据作为验证集，当我们继续训练时，我们保留验证数据的损失函数记录，当我们看到验证集没有改进时，我们停止，而不是遍历所有时期。这种基于验证集性能的提前停止策略称为<strong class="lt ir">提前停止。</strong>下图对此进行了解释。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/c5170271ec02079fb5af27b68e3dad5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*ERLrFsyAEqqkPnCdtu1wQA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 3:提前停止演示(图片来源:作者)</p></figure><p id="9056" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">从图 3 中可以观察到</p><ul class=""><li id="b190" class="my mz iq lt b lu lv lx ly mn na mo nb mp nc mm nd ne nf ng bi translated">在所有的时期，训练集的精确度持续增加</li><li id="a534" class="my mz iq lt b lu nh lx ni mn nj mo nk mp nl mm nd ne nf ng bi translated">然而，验证集精度在 8 到 10 个时期之间饱和。这里是模型可以<strong class="lt ir">停止</strong>训练的地方。</li></ul><blockquote class="ln lo lp"><p id="8904" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">因此，早期停止不仅可以防止过度拟合，而且需要相当少的历元数来训练。</p></blockquote><p id="6b0f" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">代码摘录:下面的代码演示了我们将 20%的训练数据作为验证集。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="eeed" class="ns kw iq no b gy nt nu l nv nw">fashion_mnist = keras.datasets.fashion_mnist<br/>(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()</span><span id="8eaa" class="ns kw iq no b gy nx nu l nv nw">trn_images, valid_images, trn_labels, valid_labels = train_test_split(train_images, train_labels,test_size=0.2)</span></pre><p id="44ae" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">一些其他的正则化技术是损失函数正则化、丢失、数据扩充等。如果有兴趣，你可以自担风险，在这个<a class="ae mq" href="https://youtu.be/Pk1ua4Opcfs" rel="noopener ugc nofollow" target="_blank">视频教程中浪费一些时间。</a></p><h1 id="21a1" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">回调 API:</h1><p id="1e89" class="pw-post-body-paragraph lq lr iq lt b lu ms jr lw lx mt ju lz mn mu mc md mo mv mg mh mp mw mk ml mm ij bi translated">我们现在想到的一个问题是，当模型被训练时，我们如何监控发生了什么，以及如何保持跟踪？这就是我们从 Keras 回调 API 获得帮助的地方。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5aa438cd3efbeef77955833431b74f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3hkmdh59MnYrVLzH"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 4:回调 API 的窗口模拟(来源:Unsplash)</p></figure><blockquote class="ln lo lp"><p id="c597" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">回调 API 就像窗口，在黑盒模型训练过程中，允许我们监控，我们感兴趣的对象。</p></blockquote><ul class=""><li id="24a5" class="my mz iq lt b lu lv lx ly mn na mo nb mp nc mm nd ne nf ng bi translated">一个<strong class="lt ir">回调</strong>是一个强大的工具，用于定制 Keras 模型在训练、评估或推断过程中的行为</li><li id="197d" class="my mz iq lt b lu nh lx ni mn nj mo nk mp nl mm nd ne nf ng bi translated">它可以让你定期保存你的模型到磁盘</li><li id="533e" class="my mz iq lt b lu nh lx ni mn nj mo nk mp nl mm nd ne nf ng bi translated">您可以在训练期间查看模型的内部状态和统计数据</li><li id="7e01" class="my mz iq lt b lu nh lx ni mn nj mo nk mp nl mm nd ne nf ng bi translated">可以有多个回调一个用于保存，一个用于监控</li><li id="80f7" class="my mz iq lt b lu nh lx ni mn nj mo nk mp nl mm nd ne nf ng bi translated">回调可以附加<strong class="lt ir">拟合</strong>，<strong class="lt ir">评估</strong>，<strong class="lt ir">预测 Keras 模型</strong></li></ul><p id="fd22" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">当然，我们不能在任何时候看到中间值，它可以在训练开始时、训练停止时、时期结束时或一批训练结束时。</p><p id="6f07" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">我们确实默认使用了一些回调函数，比如我们在模型编译中提到的度量和损失函数。在这种情况下，我们不需要附加任何回调 API 或窗口。这被称为<strong class="lt ir">基础回调 API </strong>。下图显示了这一点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/1242b0028b8db85d5bbf3725d866f3d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ad-tgmn6MtBe2n8Wo6P2tw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 5:基础回调 API(图片来源:作者)</p></figure><h1 id="5b44" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">提前停止回调的一些重要参数:</h1><ul class=""><li id="82bc" class="my mz iq lt b lu ms lx mt mn nz mo oa mp ob mm nd ne nf ng bi translated"><strong class="lt ir">监控</strong>:被监控的数量。默认情况下，它是验证丢失</li><li id="b302" class="my mz iq lt b lu nh lx ni mn nj mo nk mp nl mm nd ne nf ng bi translated"><strong class="lt ir"> min_delta: </strong>符合改善条件的监控量的最小变化</li><li id="1009" class="my mz iq lt b lu nh lx ni mn nj mo nk mp nl mm nd ne nf ng bi translated"><strong class="lt ir">耐心</strong>:没有改善的周期数，在此之后训练将停止。</li><li id="3afb" class="my mz iq lt b lu nh lx ni mn nj mo nk mp nl mm nd ne nf ng bi translated"><strong class="lt ir">模式</strong>:为{“自动”、“最小”、“最大”}之一。是最大化问题还是最小化问题，我们最大化精度，最小化损失。</li><li id="f2c5" class="my mz iq lt b lu nh lx ni mn nj mo nk mp nl mm nd ne nf ng bi translated"><strong class="lt ir"> restore_best_weights: </strong>是使用最佳模型权重还是上一个时期权重</li></ul><p id="1bad" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated"><strong class="lt ir">代码示例:</strong></p><p id="e550" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">突出显示的部分是我们在模型拟合过程中需要做的唯一更改。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="0d3c" class="ns kw iq no b gy nt nu l nv nw"><strong class="no ir">callback = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)</strong><br/>history1 = model2.fit(trn_images, trn_labels, epochs=50,validation_data=(valid_images, valid_labels),<strong class="no ir">callbacks=[callback]</strong>)</span></pre><h1 id="ea13" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结果:</h1><p id="0527" class="pw-post-body-paragraph lq lr iq lt b lu ms jr lw lx mt ju lz mn mu mc md mo mv mg mh mp mw mk ml mm ij bi translated">这是在 Fashion MNSIT 上执行的，这是运行您的实验的一个很好的测试平台。</p><blockquote class="ln lo lp"><p id="bacf" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">在没有提前停止的情况下，模型运行所有 50 个时期，我们得到 88.8%的验证准确度，而提前停止运行 15 个时期，测试集准确度是 88.1%。</p><p id="61b7" class="lq lr ls lt b lu lv jr lw lx ly ju lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">这是其中一个种子值，总体而言，它清楚地表明我们实现了相当的结果，减少了 70%的时期。</p></blockquote><p id="cc8d" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi">========================================</p><p id="d4b1" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">谢谢你读到这里。这些是我们在加尔各答大学数据科学实验室创建的一些附加资源</p><p id="b1c3" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">a)用 Python 分类(<a class="ae mq" href="https://www.youtube.com/playlist?list=PLTS7rWcD0Do2ZoO4Sad3jRxnVFyxHd6_S" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/playlist?list = plts 7 rwcd 0 do 2 zoo 4 sad 3 jrxnvfyxhd 6 _ S</a></p><p id="5e58" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">b)用 Python(<a class="ae mq" href="https://www.youtube.com/playlist?list=PLTS7rWcD0Do3ts44xgWGVgxMeoHYFi3pM" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/playlist?)进行聚类 list = plts 7 rwcd 0 do 3ts 44 xgwgvgxmeohyfi 3pm</a></p><p id="befb" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">参考资料:</p><p id="f5e1" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">[1]<a class="ae mq" href="https://medium.com/@upendravijay2/early-stopping-to-avoid-overfitting-in-neural-network-keras-b68c96ed05d9" rel="noopener">https://medium . com/@ upendravijay 2/early-stopping-to-avoid-over-fitting-in-neural-network-keras-b 68 c 96 ed 05d 9</a></p><p id="2806" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated">[2]<a class="ae mq" rel="noopener" target="_blank" href="/a-practical-introduction-to-early-stopping-in-machine-learning-550ac88bc8fd">https://towards data science . com/a-practical-introduction-to-early-stopping-in-machine-learning-550 AC 88 BC 8 FD</a></p><p id="2e29" class="pw-post-body-paragraph lq lr iq lt b lu lv jr lw lx ly ju lz mn mb mc md mo mf mg mh mp mj mk ml mm ij bi translated"><a class="ae mq" href="https://sebastianraschka.com/books.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka.com/books.html</a></p></div></div>    
</body>
</html>