<html>
<head>
<title>Pyspark — forecasting with Pandas UDF and fb-prophet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pyspark —用熊猫 UDF 和 fb-prophet 进行预报</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-forecasting-with-pandas-udf-and-fb-prophet-e9d70f86d802?source=collection_archive---------8-----------------------#2020-01-16">https://towardsdatascience.com/pyspark-forecasting-with-pandas-udf-and-fb-prophet-e9d70f86d802?source=collection_archive---------8-----------------------#2020-01-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d601" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">预测几个时间序列一次与先知和熊猫 UDF 没有循环。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b30a7dffecac6665d75aad962b9581b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Z1Qgehb9H-ZkWyJu.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg" rel="noopener ugc nofollow" target="_blank">https://upload . wikimedia . org/Wikipedia/commons/f/F3/Apache _ Spark _ logo . SVG</a></p></figure><p id="8af2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在开始任何与熊猫-udf 相关的工作之前，<strong class="lb iu">的先决条件</strong>是</p><ul class=""><li id="3546" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">火花≥ 2.4</li><li id="4d35" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">pyarrow ≤ 0.14.1(高于此版本有一些问题)</li></ul><p id="48f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们需要为 pyarrow 设置一个环境变量为 1。(参见进口代码)</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="a524" class="mo mp it mk b gy mq mr l ms mt">sudo pip3 install pyarrow=0.14.1</span></pre><p id="12cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以继续库导入。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="7067" class="mo mp it mk b gy mq mr l ms mt">from pyspark.sql import SparkSession<br/>from pyspark.sql.types import *<br/>from pyspark.sql.functions import pandas_udf, PandasUDFType, sum, max, col, concat, lit<br/>import sys<br/>import os<br/># setup to work around with pandas udf<br/># see answers here <a class="ae ky" href="https://stackoverflow.com/questions/58458415/pandas-scalar-udf-failing-illegalargumentexception" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/questions/58458415/pandas-scalar-udf-failing-illegalargumentexception</a><br/>os.environ["ARROW_PRE_0_15_IPC_FORMAT"] = "1"</span><span id="13cf" class="mo mp it mk b gy mu mr l ms mt">from fbprophet import Prophet<br/>import pandas as pd<br/>import numpy as np</span></pre><p id="96fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果一切顺利，我们可以通过“sc”调用 spark 上下文，并看到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/4e5007be0ffb77265729c049fecbd830.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*dNJaDKAiSI50sSOkgy4NhA.png"/></div></figure><p id="924f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以谈论有趣的部分，预测！</p><p id="92ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我们将使用 pyspark 的新功能:pandas-udf，就像以前的 pyspark UDF 一样。pandas-udf 是一个用户自定义函数，目标是<strong class="lb iu">在 spark 数据帧<strong class="lb iu">上应用我们最喜欢的库</strong>，如 numpy、pandas、sklearn 等，而不改变语法</strong>和<strong class="lb iu">返回 spark 数据帧</strong>。</p><p id="3d18" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特别是，我们将探索熊猫 UDF 的<strong class="lb iu"> GROUPED_MAP 属性</strong>，这些属性允许我们将<strong class="lb iu"> scikit-learn、statsmodels 和更多</strong>应用于我们的 Spark 数据帧。我们将使用熊猫 UDF 对我们数据集中的每个商品|商店应用脸书人工智能的流行 Prophet 分段回归，没有任何循环，也没有任何结果的串联，因为 Prophet 不会同时处理多个时间序列。</p><p id="40d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pandas_udf 可以作为装饰器传递给基本函数，该装饰器用两个参数包装整个函数:预期的输出模式和<strong class="lb iu"> GROUPED_MAP </strong>属性。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="f7b4" class="mo mp it mk b gy mq mr l ms mt"># define an output schema<br/>schema = StructType([<br/>        StructField("store", StringType(), True),<br/>        StructField("item", StringType(), True),<br/>        StructField("ds", DateType(), True),<br/>        StructField("yhat", DoubleType(), True)<br/>    ])</span></pre><p id="0f26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">定义了装饰器之后，剩下的就非常简单和直观了；我们定义我们的函数来调整和预测我们的数据，就像我们对单个序列所做的那样。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="5fc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的功能非常简单，我接受 Spark 数据帧作为输入，该数据帧是训练集和测试集(目标初始化为 null)连接的结果，是我对任何项目的销售历史(格式是 Spark 数据帧)。该函数将被应用，就好像只有一个产品-商店对，并且作为一个熊猫数据帧，所以没有语法变化，所以我们定义训练和预测日期，并根据算法要求的约定重命名日期和销售列，然后我调用我的函数创建一个国家事件数据帧(格式:熊猫数据帧)。然后，我传递算法中的所有内容，进行预测，并附上我的预测数据帧，该数据帧仅包含与我的初始测试集相关的日期和预测，以便检索产品和商店 id。整个函数被包装在 pandas-udf 和..就是这样！</p><ul class=""><li id="e1e9" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">名词（noun 的缩写）《预言家日报》上的假日只是笔记本上的一个“硬编码”假日熊猫数据框架。</li></ul><p id="6b1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以训练我们的模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="085c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果一切顺利，控制台应该记录每个用一些指标训练的时间序列。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/c92e4ae7d9d3abdfb8c82035afb3ada5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkgRnw8_x14_sN4R55JMBg.png"/></div></div></figure><p id="dd72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，对每个序列进行预测，我们把它们放在一个单一的火花数据框架中，没有串联。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/3f7582b4818696a40fc2347e9609899d.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*2OhSMc_GeBukFqcRB98a5g.png"/></div></figure><p id="6a87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">魔法起作用了！我们在一瞬间预测了大约 50 个时间序列，没有复杂，没有 for 循环，也没有检索结果。</p><h1 id="3fc7" class="na mp it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">最后</h1><p id="aebd" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">在这个简短的教程中，我们将看到如何使用 pandas udf 训练几个模型，以分布式方式一次预测几个时间序列。与允许我们跳过循环和连接部分的事实相比，pandas UDF 的主要优势是我们通过分布式框架获得的计算时间，每个时间序列都是在不同工作者的不同分区上形成的，整体在最后收集，这是与循环相比的真正优势，循环需要一些时间来运行数百万个包含实际数据的元素。</p><p id="2400" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">笔记本在我的 Github 这里有:<a class="ae ky" href="https://github.com/AlexWarembourg/Medium" rel="noopener ugc nofollow" target="_blank">https://github.com/AlexWarembourg/Medium</a>数据在 Kaggle 上有(链接在笔记本和主物的评论下。<strong class="lb iu"> <em class="nw">感谢！</em> </strong></p></div></div>    
</body>
</html>