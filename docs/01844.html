<html>
<head>
<title>A no-frills guide to most Natural Language Processing Models — The LSTM Age — Seq2Seq, InferSent, Skip-Thought, Quick-Thought, ELMo, Flair, and ULMFiT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大多数自然语言处理模型的简明指南——LSTM时代——seq 2 seq、InferSent、Skip-Thought、Quick-Thought、ELMo、Flair和ULMFiT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687?source=collection_archive---------21-----------------------#2020-02-20">https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-the-lstm-age-seq2seq-infersent-3af80e77687?source=collection_archive---------21-----------------------#2020-02-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8728" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">总结了LSTM语言模型的起源、使用案例和优缺点:Seq2Seq、Skip-think、Quick think、InferSent、ELMo、Flair和ULMFiT</h2></div><p id="aa72" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着LSTMs越来越流行，NLP社区通过从仅部分考虑单词上下文的浅层静态嵌入转移到利用周围上下文创建更深层表示的更上下文化/动态的单词表示，改进了语言模型。</p><p id="8802" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然这可能仍然包含一些无法解释的术语，但是通过其他帖子可以很容易地获得关于各种概念的信息。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/02d385712d0105e708d065389c020dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O5cC9pXx2p_hbmu7p-0mOQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">突显“LSTM时代”七种主要模式的时间线</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="16cc" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">Seq2seq</h1><p id="1e89" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated"><strong class="kk iu">没有嵌入</strong></p><p id="5389" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Seq2seq在这个列表中很突出，因为它本身不生成嵌入。然而，它可以成为一个人的NLP工具包中的一个有价值的武器，因为它可以很容易地适应特定的用例。</p><p id="e0a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Google在机器翻译的背景下引入了Seq2seq。这是第一个使用注意力的模型(被最近的transformer架构广泛利用)。该结构在编码器/解码器之间划分，并且它们中的两个使用波束搜索与注意力层相关(参见下图或<a class="ae mz" href="https://www.coursera.org/learn/nlp-sequence-models/home/welcome" rel="noopener ugc nofollow" target="_blank">吴恩达的自由序列模型课程</a>以获得关于注意力机制的更多细节)。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi na"><img src="../Images/d116dee63e6aa36e274f1d57d6885540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*75Jb0q3sX1GDYmJSfl-gOw.gif"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">图片来自<a class="ae mz" href="https://github.com/google/seq2seq" rel="noopener ugc nofollow" target="_blank">谷歌的seq2seq GitHub </a></p></figure><p id="310c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Seq2seq因彻底改变了机器翻译而闻名(在谷歌翻译上几个月内超过了统计学家多年的工作)。它也可以很容易地调整为其他任务，其中输出是一个序列(即会话或总结)。</p><p id="2c32" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优点:</strong> <br/> -生成序列的卓越性能(它奠定了transformer模型的基础)<br/> -与“传统”LSTM模型相比，注意力层允许更好地处理长期依赖性</p><p id="218b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:<br/></strong></p><p id="f131" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Ilya Sutskever，Oriol Vinyals，Quoc V. Le，<a class="ae mz" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">用神经网络进行序列对序列学习<em class="le"> </em> </a> (2014)，NIPS</p><h1 id="089a" class="mc md it bd me mf nb mh mi mj nc ml mm jz nd ka mo kc ne kd mq kf nf kg ms mt bi translated">跳过思维向量(奖励:思维敏捷)</h1><p id="2c9f" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">句子级嵌入</p><p id="121d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Skip-Thought Vectors基于静态单词嵌入中使用的skip-gram训练方法，并将其扩展到句子。Skip-Thought不是使用一个单词并试图预测上下文，而是使用一个句子并试图预测周围的句子。</p><p id="ea92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型使用递归神经网络(通常是GRU或LSTM)对句子中的单词进行编码。然后，它使用以编码器的输出为条件的解码器(也称为它使用与编码器相同的嵌入层)来生成最佳可能的目标输出(下一个和上一个句子)。使用实际编码的句子和预测的嵌入之间的差异，训练该模型以尽可能准确地预测其周围的句子。</p><p id="7e48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Quick Thoughts是后来开发的另一个模型，它使用类似的编码器/解码器策略。不同的是，它是在一个更经典的分类任务上训练的，更接近于skip-gram和负采样。对于每个句子，另一个目标句子被提供给解码器(与编码器的权重相同)，然后编码的原始句子和“解码的”句子被提供给分类器进行“比较”(即原始句子是否与目标句子相邻)。</p><p id="070e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑到训练中的差异，快速思维通常被认为更适合区分性任务，而跳过思维更适合生成性任务。</p><p id="56f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优点:<br/> </strong> -句子级嵌入对某些任务有用<br/> -通用和健壮的句子嵌入:在许多下游任务(语义相似性、分类等)上相当准确。)</p><p id="ff42" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:<br/></strong>-LSTM结构和嵌入在句子级别的事实使其训练缓慢，使用大量内存，并且难以训练长句<br/> -它是第一个句子嵌入之一，因此不再被认为是最先进的(SOTA)<br/>-它被训练的任务，即预测相邻的句子， 很难(即使对人来说也是如此)，所以通常不是最好的嵌入方式(尽管快速思考会使问题简化一点)<br/> -使用Skip思想，一个句子被嵌入到一个2400维的向量中，比大多数其他类型的嵌入方式都要多得多<br/> -与长期的上下文依赖作斗争(与基于transformer的长句模型相比)</p><p id="9753" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Jamie Ryan Kiros，Yukun Zhu，Ruslan Salakhutdinov，Richard S. Zemel，Antonio Torralba，Raquel Urtasun，Sanja Fidler，<a class="ae mz" href="https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf" rel="noopener ugc nofollow" target="_blank">Skip-think Vectors</a>(2015)，NIPS</p><p id="82d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Lajanugen Logeswaran，Honglak Lee，<a class="ae mz" href="https://arxiv.org/pdf/1803.02893.pdf" rel="noopener ugc nofollow" target="_blank">学习句子表征的有效框架</a> (2018)，ICLR</p><h1 id="9ee1" class="mc md it bd me mf nb mh mi mj nc ml mm jz nd ka mo kc ne kd mq kf nf kg ms mt bi translated">InferSent</h1><p id="74e9" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated"><strong class="kk iu">句子级嵌入</strong></p><p id="0bd4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用Stanford自然语言推理(SNLI)语料库，通过使用双LSTM层(具有最大池)来编码句子对(两个句子使用相同的编码器)来构建推理。添加几个密集层，并训练模型来预测这两个句子是体现“中性”、“矛盾”还是“蕴涵”。在向量被馈送到双LSTM层之前，单词通常首先被转换成静态单词嵌入(在原始情况下是手套嵌入)。结果，隐含嵌入提供了句子的深层语义表示。</p><p id="7c74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由脸书创建的InferSent也可以很好地推广到其他任务，并且预先训练的版本很容易在脸书研究公司的GitHub上获得。</p><p id="5a25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优点:<br/> </strong> -句子级嵌入可能对某些任务有帮助<br/> -该模型在复述检测和蕴涵任务上表现特别好</p><p id="09e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:<br/> </strong> -复杂的双LSTM结构使其训练和生成嵌入变得缓慢<br/> -输出是4096维的嵌入，这显著多于几乎所有其他语言模型<br/> -在情感分析、语义关联、标题检索等许多任务上的表现不如ELMo或Flair。<br/> -监督训练使得很难在定制数据集/更具体的上下文中重现<br/> -与长期上下文依赖作斗争(相对于基于转换器的长句模型)</p><p id="22c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Conneau，Alexis等人，<a class="ae mz" href="https://arxiv.org/abs/1705.02364" rel="noopener ugc nofollow" target="_blank">从自然语言推理数据</a> <em class="le"> </em> (2017)，计算语言学协会</p><h1 id="03e3" class="mc md it bd me mf nb mh mi mj nc ml mm jz nd ka mo kc ne kd mq kf nf kg ms mt bi translated">ELMo(语言模型嵌入)</h1><p id="27c2" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated"><strong class="kk iu">单词级嵌入</strong></p><p id="57cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ELMo使用深度双LSTM架构来创建情境化嵌入。<br/>如<a class="ae mz" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"> AllenNLP </a>所述，ELMo表示为:“上下文”(取决于使用单词的上下文)、“深度”(通过深度神经网络训练)和“基于字符”(参见fastText嵌入，以便更好地处理不在词汇表中的单词)。</p><p id="6032" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了训练模型，ELMo使用正向和反向语言建模(LSTM的一个方向预测第一个单词，另一个方向预测最后一个单词)。</p><p id="67de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型包含许多层，这些层之间存在残余连接。有趣的是，每一层最终都会学习句子的不同特征，比如前几层的词性标注和后几层的词义消歧。</p><p id="e5b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过<a class="ae mz" href="https://tfhub.dev/google/elmo/3" rel="noopener ugc nofollow" target="_blank"> Tensorflow Hub </a>，可以轻松使用在10亿字基准上预先训练的ELMo嵌入。</p><p id="c61a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优势:<br/></strong>——深度的基于上下文和字符级的单词表示，适用于更复杂的任务<br/>——在许多任务上表现得比简单的嵌入好得多<br/>——易于使用的预训练版本</p><p id="0597" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:<br/> </strong> -复杂的双LSTM结构使其训练和生成嵌入非常缓慢<br/> -类似的模型如Flair(见下文)通常表现更好<br/> -与长期上下文依赖作斗争(相对于长句的基于transformer的模型)</p><p id="a05b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">马修·e·彼得斯、马克·诺依曼、莫希特·伊耶、马特·加德纳、<br/>克里斯托弗·克拉克、肯顿·李、卢克·塞特勒莫耶、<a class="ae mz" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank"> </a> <a class="ae mz" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank">深度语境化的词语表征</a> (2018)、NAACL</p><h1 id="9f1e" class="mc md it bd me mf nb mh mi mj nc ml mm jz nd ka mo kc ne kd mq kf nf kg ms mt bi translated">天资</h1><p id="58d8" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated"><strong class="kk iu">单词级嵌入</strong></p><p id="4643" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着ELMo的流行，Flair由Zalando Research开发，并在ELMo的基础上通过更多地依赖角色级别进行了改进。</p><p id="c034" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与ELMo类似，Flair学习基于字符的双LSTM(也使用向前和向后语言建模)。然而，此外，单词嵌入是通过将单词一个字符一个字符地给定到两个LSTMs中并仅保持/连接每个单词的最后状态(即，第一个和最后一个字符)来表示单词嵌入而计算的。</p><p id="08ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Flair单词嵌入有多种语言版本，使用<a class="ae mz" href="https://github.com/flairNLP/flair" rel="noopener ugc nofollow" target="_blank"> Flair python库</a>(构建于PyTorch之上)可以非常容易地生成。</p><p id="251d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优势:<br/> </strong> -适用于更复杂任务的深度基于上下文和字符级别的单词表示<br/> -考虑到嵌入的特殊性，仅保留第一个和最后一个状态，嵌入尤其能很好地处理词汇外的标记，并能处理小得多的字典<br/> -表现优于静态嵌入，通常在许多任务上ELMo<br/>-易于访问预训练版本</p><p id="4c17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:<br/> </strong> -复杂的双LSTM结构使得训练和生成嵌入非常缓慢<br/> -与长期上下文依赖作斗争(与基于转换器的长句模型相比)</p><p id="8c90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Akbik等人，<a class="ae mz" href="https://www.aclweb.org/anthology/N19-4010.pdf" rel="noopener ugc nofollow" target="_blank">用于序列标记的上下文字符串嵌入</a> (2018)，计算语言协会Seq2Seq</p><h1 id="6555" class="mc md it bd me mf nb mh mi mj nc ml mm jz nd ka mo kc ne kd mq kf nf kg ms mt bi translated">乌尔菲特</h1><p id="de85" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated"><strong class="kk iu">单词级嵌入</strong></p><p id="aed1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ULMFiT源于Fast.ai，通常被认为是第一个真正有效地将迁移学习推广到任何任务的模型。</p><p id="1fd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ULMFiT依赖于双向LSTM架构(向前和向后语言建模)。ULMFiT在三个连续的任务上进行训练:在维基百科数据集上进行通用领域语言模型预训练，在IMDb数据集上进行特定任务语言模型微调，最后在IMDb数据集上进行特定任务分类器微调。</p><p id="6291" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两项发展使ULMFiT具有出色的性能:<br/> -随着训练的进行，ULMFiT在微调过程中通过逐渐解冻其不同层来进行微调，以及<br/> -它使用“倾斜三角形学习率(STLR)”，这意味着学习率开始增加，随后随着训练的进行以线性方式衰减。</p><p id="7193" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ULMFiT的主要优势在于它能够轻松适应各种任务。对迁移学习的特别关注使ULMFiT成为许多任务/适应的非常好的竞争者。</p><p id="7bd8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<a class="ae mz" href="https://github.com/fastai/fastai" rel="noopener ugc nofollow" target="_blank"> Fast.ai Python库</a>和他们的<a class="ae mz" href="https://docs.fast.ai/text.html" rel="noopener ugc nofollow" target="_blank">教程</a>，可以很容易地下载和微调一个预先训练好的模型。</p><p id="dae3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优点:<br/> </strong> -通用:一个预先训练好的模型，非常容易适应任何任务<br/>-LSTM模型的出色性能<br/> -需要比其他模型少得多的数据来针对特定领域/任务进行微调<br/> -由于Fast.ai的精彩教程，易于访问和使用</p><p id="94a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">缺点:<br/> </strong> -复杂的双LSTM结构使得训练和生成嵌入非常缓慢<br/> -与长期上下文依赖作斗争(与基于转换器的长句模型相比)</p><p id="5d72" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">杰瑞米·霍华德，塞巴斯蒂安·鲁德，<a class="ae mz" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">文本分类通用语言模型微调</a> (2018)，计算语言学协会</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="331c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与静态嵌入相比，上下文化的表示对于更复杂的任务来说要强大得多，并且知道选择哪种模型会对模型的性能产生重大影响。像往常一样，努力记住每个模型是如何被训练的，并努力找到其主要任务与应用程序的目的最相似的模型。希望这篇文章能帮助你更好地理解众多LSTM时代模型的优缺点和用例。</p><p id="15c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然基于transformer的模型提供了更复杂的表示，可以考虑长期的依赖性，但是LSTM模型在transformer模型的复杂性和有时过于简单的静态嵌入之间提供了一个非常相关的替代方案。</p><p id="2fa0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PS:我现在是伯克利的工程硕士，我还在学习这方面的知识。如果有什么需要改正或不清楚的地方，请告诉我。你也可以发邮件给我 <a class="ae mz" href="mailto:ilias.miraoui@gmail.com" rel="noopener ugc nofollow" target="_blank"> <em class="le">这里</em> </a> <em class="le">。</em></p></div></div>    
</body>
</html>