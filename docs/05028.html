<html>
<head>
<title>Hyperparameter Tuning with Keras Tuner</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Keras 调谐器进行超参数调谐</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-with-keras-tuner-283474fbfbe?source=collection_archive---------6-----------------------#2020-05-01">https://towardsdatascience.com/hyperparameter-tuning-with-keras-tuner-283474fbfbe?source=collection_archive---------6-----------------------#2020-05-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e4af" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">充分利用您的模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2148cc5d55180a9dd54b83a6b5c2c154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tRm8oHQQCX1P_ANJdNulLQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com/s/photos/knobs?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@willyin?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">尹卡·阿迪奥蒂</a>拍摄的照片</p></figure><blockquote class="kz"><p id="bbd3" class="la lb it bd lc ld le lf lg lh li lj dk translated">伟大的数据科学家不会满足于“还行”，他们会超越去实现非凡。</p></blockquote><p id="cfb2" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me lj im bi mf translated"><span class="l mg mh mi bm mj mk ml mm mn di">在</span>这篇文章中，我们将回顾数据科学家用来创建模型的技术，这些模型工作良好并赢得竞争。充分利用我们的模型意味着为我们的学习算法选择最佳的超参数。这项任务被称为超参数优化或超参数调整。这在深度学习中尤其费力，因为神经网络充满了超参数。我假设您已经熟悉回归和均方差(MSE)指标等常见的数据科学概念，并且具有使用 tensorflow 和 keras 构建模型的经验。</p><p id="b33b" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">为了演示超参数调优方法，我们将使用<a class="ae ky" href="https://keras-team.github.io/keras-tuner/" rel="noopener ugc nofollow" target="_blank"> keras tuner </a>库来调优波士顿房价数据集上的回归模型。该数据集包含 13 个属性，分别具有 404 个和 102 个训练和测试样本。我们将使用 tensorflow 作为 keras 后端，因此请确保您的计算机上安装了 tensorflow。我用的是 tensorflow 版本' 2.1.0 '和 kerastuner 版本' 1.0.1 '。Tensorflow 2.0.x 附带了 keras，因此，如果您拥有 2.0.x 版本，则无需单独安装 keras。您可以使用以下代码检查您拥有的版本:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d074" class="my mz it mu b gy na nb l nc nd">import tensorflow as tf<br/>import kerastuner as kt</span><span id="25d6" class="my mz it mu b gy ne nb l nc nd">print(tf.__version__)<br/>print(kt.__version__)</span></pre><h1 id="3cd6" class="nf mz it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">加载数据集</h1><p id="4547" class="pw-post-body-paragraph lk ll it lm b ln nw ju lp lq nx jx ls lt ny lv lw lx nz lz ma mb oa md me lj im bi translated">波士顿房价回归数据集可以使用 keras 直接下载。这是 keras 附带的数据集列表。若要加载数据集，请运行以下代码。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="21fa" class="my mz it mu b gy na nb l nc nd">from tensorflow.keras.datasets import boston_housing</span><span id="949c" class="my mz it mu b gy ne nb l nc nd">(x_train, y_train), (x_test, y_test) = boston_housing.load_data()</span></pre><p id="8dbf" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">请注意，如果这是您第一次在 keras 中使用该数据集，它将从外部源下载该数据集。</p><p id="7193" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">这是我将在演示中使用的回归模型。下面的代码显示了模型是如何在没有任何调整的情况下构建的。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="9a9f" class="my mz it mu b gy na nb l nc nd">from sklearn.preprocessing import StandardScaler<br/>from tensorflow.keras import models, layers</span><span id="ff02" class="my mz it mu b gy ne nb l nc nd"># set random seed<br/>from numpy.random import seed<br/>seed(42)<br/>import tensorflow<br/>tensorflow.random.set_seed(42)</span><span id="90ca" class="my mz it mu b gy ne nb l nc nd"># preprocessing - normalization<br/>scaler = StandardScaler()<br/>scaler.fit(x_train)<br/>x_train_scaled = scaler.transform(x_train)<br/>x_test_scaled = scaler.transform(x_test)</span><span id="c956" class="my mz it mu b gy ne nb l nc nd"># model building<br/>model = models.Sequential()<br/>model.add(layers.Dense(8, activation='relu', input_shape=(x_train.shape[1],)))<br/>model.add(layers.Dense(16, activation='relu'))<br/>model.add(layers.Dropout(0.1))<br/>model.add(layers.Dense(1))</span><span id="2290" class="my mz it mu b gy ne nb l nc nd"># compile model using rmsprop<br/>model.compile(optimizer='rmsprop',loss='mse',metrics=['mse'])</span><span id="cfa0" class="my mz it mu b gy ne nb l nc nd"># model training<br/>history = model.fit(x_train_scaled, y_train, validation_split=0.2, epochs=10)</span><span id="294d" class="my mz it mu b gy ne nb l nc nd"># model evaluation<br/>model.evaluate(x_test_scaled, y_test)</span></pre><p id="44b0" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">该模型的 MSE 约为 434。我已经将 numpy 和 tensorflow 中的随机种子设置为 42，以获得可重复的结果。尽管这样做了，但每次运行代码时，我还是会得到稍微不同的结果。让我在评论中知道我还错过了什么，让这个可重复。</p><h1 id="c97b" class="nf mz it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">使用 Keras 调谐器调谐</h1><p id="023c" class="pw-post-body-paragraph lk ll it lm b ln nw ju lp lq nx jx ls lt ny lv lw lx nz lz ma mb oa md me lj im bi translated">要开始在 keras tuner 中调优模型，让我们首先定义一个<strong class="lm iu">超级模型</strong>。<strong class="lm iu"> Hypermodel </strong>是一个 keras tuner 类，它允许您用可搜索空间定义模型并构建它。</p><p id="47de" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">创建一个从 kerastuner 继承的类。超模，像这样:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="9584" class="my mz it mu b gy na nb l nc nd">from kerastuner import HyperModel</span><span id="34bd" class="my mz it mu b gy ne nb l nc nd">class RegressionHyperModel(HyperModel):<br/>    def __init__(self, input_shape):<br/>        self.input_shape = input_shape</span><span id="9745" class="my mz it mu b gy ne nb l nc nd">    def build(self, hp):<br/>        model = Sequential()<br/>        model.add(<br/>            layers.Dense(<br/>                units=hp.Int('units', 8, 64, 4, default=8),<br/>                activation=hp.Choice(<br/>                    'dense_activation',<br/>                    values=['relu', 'tanh', 'sigmoid'],<br/>                    default='relu'),<br/>                input_shape=input_shape<br/>            )<br/>        )<br/>        <br/>        model.add(<br/>            layers.Dense(<br/>                units=hp.Int('units', 16, 64, 4, default=16),<br/>                activation=hp.Choice(<br/>                    'dense_activation',<br/>                    values=['relu', 'tanh', 'sigmoid'],<br/>                    default='relu')<br/>            )<br/>        )<br/>        <br/>        model.add(<br/>            layers.Dropout(<br/>                hp.Float(<br/>                    'dropout',<br/>                    min_value=0.0,<br/>                    max_value=0.1,<br/>                    default=0.005,<br/>                    step=0.01)<br/>            )<br/>        )<br/>        <br/>        model.add(layers.Dense(1))<br/>        <br/>        model.compile(<br/>            optimizer='rmsprop',loss='mse',metrics=['mse']<br/>        )<br/>        <br/>        return model</span></pre><p id="6e95" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">这与我们之前构建的模型相同，只是对于每个超参数，我们定义了一个搜索空间。你可能已经注意到了惠普公司的 hp.Int。浮动，和 hp。Choice，它们用于定义超参数的搜索空间，该超参数分别接受整数、浮点和类别。超参数方法的完整列表可在<a class="ae ky" href="https://keras-team.github.io/keras-tuner/documentation/hyperparameters/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。“hp”是 Keras Tuner 的超参数类的别名。</p><p id="d176" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">超参数如密集层中的单元数接受一个整数，因此，hp.Int 用于定义一个整数范围来尝试。类似地，辍学率接受浮点值，因此 hp。使用了 Float。无论是 hp.Int 还是惠普。Float 需要一个名称、最小值和最大值，而步长和默认值是可选的。</p><p id="9b89" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">下面的 hp.Int 搜索空间被命名为“单位”，其值为 8 到 64 的 4 的倍数，默认值为 8。惠普。Float 的用法与 hp.Int 类似，但接受浮点值。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="92fa" class="my mz it mu b gy na nb l nc nd">hp.Int('units', 8, 64, 4, default=8)</span></pre><p id="f7fe" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">惠普。Choice 用于定义分类超参数，如激活函数。下面名为“dense_activation”的搜索空间将在“relu”、“tanh”和“sigmoid”函数之间进行选择，默认值设置为“relu”。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d45e" class="my mz it mu b gy na nb l nc nd">hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid'], default='relu')</span></pre><h1 id="980c" class="nf mz it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">实例化超级模型</h1><p id="bfbc" class="pw-post-body-paragraph lk ll it lm b ln nw ju lp lq nx jx ls lt ny lv lw lx nz lz ma mb oa md me lj im bi translated">让我们实例化一个超级模型对象。输入形状因数据集和您试图解决的问题而异。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="11f0" class="my mz it mu b gy na nb l nc nd">input_shape = (x_train.shape[1],)<br/>hypermodel = RegressionHyperModel(input_shape)</span></pre><p id="f88c" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">开始调音吧！</p><h1 id="e305" class="nf mz it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">随机搜索</h1><p id="09e1" class="pw-post-body-paragraph lk ll it lm b ln nw ju lp lq nx jx ls lt ny lv lw lx nz lz ma mb oa md me lj im bi translated">顾名思义，这种超参数调优方法从给定的搜索空间中随机尝试超参数的组合。要在 keras tuner 中使用这种方法，让我们使用一个可用的调谐器来定义一个调谐器。这里有一个完整的名单<a class="ae ky" href="https://keras-team.github.io/keras-tuner/documentation/tuners/" rel="noopener ugc nofollow" target="_blank">调谐器</a>。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="818e" class="my mz it mu b gy na nb l nc nd">tuner_rs = RandomSearch(<br/>            hypermodel,<br/>            objective='mse',<br/>            seed=42,<br/>            max_trials=10,<br/>            executions_per_trial=2)</span></pre><p id="edcf" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">使用<em class="ob">搜索</em>方法运行随机搜索调谐器。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="24f5" class="my mz it mu b gy na nb l nc nd">tuner_rs.search(x_train_scaled, y_train, epochs=10, validation_split=0.2, verbose=0)</span></pre><p id="bd64" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">选择调谐器尝试并评估的最佳超参数组合。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="8b89" class="my mz it mu b gy na nb l nc nd">best_model = tuner_rs.get_best_models(num_models=1)[0]<br/>loss, mse = best_model.evaluate(x_test_scaled, y_test)</span></pre><p id="9824" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">随机搜索的 MSE 是 53.48，与根本不执行任何调整相比，这是一个非常大的改进。</p><h1 id="12d7" class="nf mz it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">超波段</h1><p id="3b32" class="pw-post-body-paragraph lk ll it lm b ln nw ju lp lq nx jx ls lt ny lv lw lx nz lz ma mb oa md me lj im bi translated">Hyperband 基于李等人的算法。al 。通过自适应资源分配和提前停止优化随机搜索方法。Hyperband 首先运行随机超参数配置一次或两次，然后选择表现良好的配置，然后继续调整表现最佳的配置。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="0008" class="my mz it mu b gy na nb l nc nd">tuner_hb = Hyperband(<br/>            hypermodel,<br/>            max_epochs=5,<br/>            objective='mse',<br/>            seed=42,<br/>            executions_per_trial=2<br/>        )</span><span id="39f7" class="my mz it mu b gy ne nb l nc nd">tuner_hb.search(x_train_scaled, y_train, epochs=10, validation_split=0.2, verbose=0)</span><span id="f3c0" class="my mz it mu b gy ne nb l nc nd">best_model = tuner_hb.get_best_models(num_models=1)[0]<br/>best_model.evaluate(x_test_scaled, y_test)</span></pre><p id="bb90" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">得到的 MSE 是 395.19，与随机搜索相比要差很多，但比完全不调优要好一点。</p><h1 id="b319" class="nf mz it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">贝叶斯优化</h1><p id="023a" class="pw-post-body-paragraph lk ll it lm b ln nw ju lp lq nx jx ls lt ny lv lw lx nz lz ma mb oa md me lj im bi translated">贝叶斯优化是一种概率模型，将超参数映射到目标函数的概率得分。与随机搜索和超波段模型不同，贝叶斯优化跟踪其过去的评估结果，并使用它来建立概率模型。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="5981" class="my mz it mu b gy na nb l nc nd">tuner_bo = BayesianOptimization(<br/>            hypermodel,<br/>            objective='mse',<br/>            max_trials=10,<br/>            seed=42,<br/>            executions_per_trial=2<br/>        )</span><span id="7fd1" class="my mz it mu b gy ne nb l nc nd">tuner_bo.search(x_train_scaled, y_train, epochs=10, validation_split=0.2, verbose=0)</span><span id="fec9" class="my mz it mu b gy ne nb l nc nd">best_model = tuner_bo.get_best_models(num_models=1)[0]<br/>best_model.evaluate(x_test_scaled, y_test)</span></pre><p id="703b" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">使用贝叶斯优化调整的最佳模型 MSE 是 46.47，比我们尝试的前两个调谐器要好。</p><h1 id="ce90" class="nf mz it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">结论</h1><p id="47a6" class="pw-post-body-paragraph lk ll it lm b ln nw ju lp lq nx jx ls lt ny lv lw lx nz lz ma mb oa md me lj im bi translated">我们能够证明，实际上，调优帮助我们最大限度地利用我们的模型。这里讨论的只是众多超参数调整方法中的 3 种。当尝试上面的代码时，我们可能会得到稍微不同的结果，出于某种原因，尽管设置了 numpy、tensorflow 和 keras tuner 随机种子，但每次迭代的结果仍然略有不同。笔记本上传在我的 github <a class="ae ky" href="https://github.com/cedricconol/keras-tuner-demo" rel="noopener ugc nofollow" target="_blank"> repo </a>里。</p><p id="5e73" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">此外，调谐器也可以调谐！是的，你没看错，调整调谐器。调谐器接受诸如 max_trials 和每次试验的执行次数之类的值，因此也可以进行调谐。尝试更改这些参数，看看是否能获得进一步的改进。</p><h1 id="426e" class="nf mz it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">参考</h1><p id="ad68" class="pw-post-body-paragraph lk ll it lm b ln nw ju lp lq nx jx ls lt ny lv lw lx nz lz ma mb oa md me lj im bi translated">[1] F. Chollet，<em class="ob">用 Python 进行深度学习</em> (2018)，曼宁出版公司。</p><p id="8b7f" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">[2] Keras 调谐器文档，【https://keras-team.github.io/keras-tuner/ T4】</p><p id="fe76" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">[3]李，贾米森，德萨沃，罗斯塔米扎德，塔尔沃卡，<em class="ob">超波段:一种基于 Bandit 的超参数优化新方法(2018)，</em><a class="ae ky" href="https://arxiv.org/abs/1603.06560" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>