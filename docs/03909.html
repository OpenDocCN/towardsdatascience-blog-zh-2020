<html>
<head>
<title>Uncertainty Assessment of Predictions with Bayesian Inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用贝叶斯推理评估预测的不确定性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9?source=collection_archive---------22-----------------------#2020-04-11">https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9?source=collection_archive---------22-----------------------#2020-04-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3d7f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">计算贝叶斯统计导论。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/74d18310cc1208d9032e3b7c491b3773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Xv1gj5xkJW3WNBKw"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Maksym Kaharlytskyi 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="0c40" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="1d8b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Frequentist_probability" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">频数统计</strong> </a>有几个局限性，比如在预测中缺少<a class="ae ky" href="https://en.wikipedia.org/wiki/Uncertainty" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a>(我们通常只对期望建模)，没有内置的<a class="ae ky" href="https://en.wikipedia.org/wiki/Regularization_%28mathematics%29" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">正则化</strong> </a>，或者没有包含先验知识。一个<a class="ae ky" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a>可以从过去的信息中获得，例如以前的实验，但是也可以从一个训练有素的<a class="ae ky" href="https://en.wikipedia.org/wiki/Subject-matter_expert" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">主题专家</strong> </a>的纯粹主观的评估中产生。</p><p id="4836" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Bayesian_probability" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">贝叶斯统计</strong> </a>是统计学领域的一种替代理论，基于概率的贝叶斯解释，其中概率表示关于事件的信念或信息(知识)的程度。这与频率主义者的解释不同，后者认为概率是经过多次试验后某一事件相对频率的极限。许多贝叶斯方法需要大量的计算来完成，这就是为什么在上个世纪广泛使用的大多数方法都是基于频率主义者的解释。然而，随着功能强大的计算机和新算法的出现，贝叶斯方法在21世纪的统计中得到了越来越多的应用。</p><h1 id="0e0f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">贝叶斯定理</h1><p id="8b96" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">贝叶斯统计围绕着<a class="ae ky" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">贝叶斯定理</strong> </a>的用法，以<a class="ae ky" href="https://en.wikipedia.org/wiki/Thomas_Bayes" rel="noopener ugc nofollow" target="_blank">【托马斯·贝叶斯】</a>命名，以及<a class="ae ky" href="https://en.wikipedia.org/wiki/Probability_axioms#Further_consequences" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">和</strong> </a>概率的乘积规则<em class="ms"> p(y) </em>在获得更多证据<em class="ms"> p(x|y </em>)后，产生一个<a class="ae ky" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">后验概率分布</strong> </a> <em class="ms"> p(y|x </em>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/40f5f43ef9975618257f2b33da439d66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f9_tdsO4EA0WlTJylOnrgQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mu">方程式1。</strong></p></figure><p id="392a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">等式1中分布<em class="ms"> p(x，y) = p(x|y)p(y) </em>的变量<em class="ms"> Y </em>的所有可能配置的总和导致<em class="ms"> X </em>的<a class="ae ky" href="https://en.wikipedia.org/wiki/Marginal_distribution" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">边际分布</strong> </a>，即<em class="ms">p(X)</em>；这种计算叫做边缘化。在连续变量的情况下，等式1的分母中的和变成了通常难以处理的积分。此外，<em class="ms"> p(x) </em>是归一化常数，其缩放后验分布，使得其在整个空间上的积分再次变为1——这是<a class="ae ky" href="https://en.wikipedia.org/wiki/Probability_distribution" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">概率分布</strong> </a>的属性之一。(但是，如果省略归一化，等式1就变成了<a class="ae ky" href="https://en.wikipedia.org/wiki/Proportionality_(mathematics)" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">比例</strong> </a>。)</p><p id="57e2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">原则上，任何参数模型(<strong class="lt iu">线性回归</strong>、<strong class="lt iu">逻辑回归</strong>，甚至是<strong class="lt iu">神经网络)</strong>都可以通过陈述参数<em class="ms"> β </em>的先验来公式化为相应的贝叶斯版本。使用训练数据(<em class="ms"> X </em>，<em class="ms"> y </em>)和贝叶斯定理，参数的后验<strong class="lt iu"> </strong>概率分布计算如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/6a559cb8dbcf13b6105d5e441183247e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qkW4ULvrmHm6r_tcQx-0oA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mu">方程式2。</strong></p></figure><p id="a2f6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一般来说，通过分析得出后验分布是不可能/不切实际的。但是，通过使用专用算法可以获得数值结果，这将在下面的章节中解释。</p><h1 id="7459" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">蒙特卡罗方法</h1><p id="454e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">首先需要介绍几个关键概念，一个是<a class="ae ky" href="https://en.wikipedia.org/wiki/Pseudo-random_number_sampling" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">伪随机数生成</strong> </a>。随机数通过<strong class="lt iu">采样</strong> <strong class="lt iu">算法</strong>产生，例如<a class="ae ky" href="https://en.wikipedia.org/wiki/Rejection_sampling" rel="noopener ugc nofollow" target="_blank">拒绝采样</a>。这些算法的目的是生成作为一个集合遵循某种特定分布(例如高斯分布)的数。然而，拒绝采样依赖于一个更简单的构建模块，该模块在0和1(U(0，1))之间的区间内生成均匀分布的随机数，例如<a class="ae ky" href="https://en.wikipedia.org/wiki/Linear_congruential_generator" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">线性同余生成器</strong> </a>。拒绝抽样适用于维数很少的非常简单的分布；在高维中，概率质量集中在很小的体积内，导致大量样本被拒绝(效率低下)。对于这样的场景，已经开发了更先进的算法；然而，它们仍然利用更简单的采样算法，例如上面提到的那些。</p><p id="7d91" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另一个重要概念是<a class="ae ky" href="https://en.wikipedia.org/wiki/Monte_Carlo_integration" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">随机数值积分</strong> </a>利用<a class="ae ky" href="https://en.wikipedia.org/wiki/Monte_Carlo_method" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">蒙特卡罗方法</strong> </a>(或称蒙特卡罗实验)。蒙特卡罗方法是一种广泛的计算算法，它依靠随机数的生成来获得数值结果。例如，通过从期望的分布中生成随机样本，等式2的分母中的积分变成了正则期望(平均值的最大似然估计)，其精度与抽取的样本数成比例。</p><p id="3373" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一类被称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">马尔可夫链蒙特卡罗</strong> </a>的采样算法已经被开发出来，用于从复杂的概率分布中抽取样本。一个<a class="ae ky" href="https://en.wikipedia.org/wiki/Markov_chain" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">马尔可夫链</strong> </a>是一个可能事件(结果)的序列，其中每个事件的概率只取决于前一个事件达到的状态。它是满足<a class="ae ky" href="https://en.wikipedia.org/wiki/Markov_property" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">马尔可夫性质</strong> </a> <strong class="lt iu"> </strong>(马尔可夫过程)的<a class="ae ky" href="https://en.wikipedia.org/wiki/Stochastic_process" rel="noopener ugc nofollow" target="_blank">随机过程</a>的结果，即过程未来状态的条件概率分布仅取决于最当前的状态，而不取决于之前的事件序列。在马尔可夫链蒙特卡罗算法类中，有几种变体，例如，<a class="ae ky" href="https://en.wikipedia.org/wiki/Gibbs_sampling" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">【Gibbs采样】</strong> </a>或<a class="ae ky" href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">Metropolis-Hastings</strong></a>；早期的变体之一，Metropolis算法，将在下一节简要描述，因为它非常容易理解。</p><p id="78af" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">贝叶斯计算的目标是从后验分布中获得一组独立的绘图，以合理的精度估计感兴趣的量。更准确地说，这种随机抽取用于总结模型参数的后验分布，例如，通过报告抽样分布的2.5%、25%、50%、75%和97.5%的点。然而，绘图也可以用散点图或直方图来可视化。</p><p id="2ade" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，一旦模拟了模型参数的后验分布，就可以通过忽略条件后验分布中的参数来进行新样本的预测。等式3本质上是另一个积分，可以使用数值方法通过随机采样来计算。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/20bceac3f91be4ad9a37c9320c63d271.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*df81waB2kmMAVwln_ONQsg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mu">方程式3。</strong></p></figure><h1 id="e94e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">大都会算法</h1><p id="af2d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如上所述，从多维(可能不是归一化的)概率分布<em class="ms"> p(z) </em>中抽取样本具有挑战性。采样算法的思想是从更简单的<strong class="lt iu">建议分布</strong>中反复抽取样本，例如，一维高斯分布的乘积。一种这样的蒙特卡罗方法被称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> Metropolis算法</strong> </a>，其生成马尔可夫链并最终收敛到期望的分布<em class="ms"> p(z) </em>。更精确地说，随机数是使用建议分布生成的，新样本是被接受还是被丢弃，取决于它们是比前一个样本更有可能还是更不可能——并且仅仅是前一个样本(因此具有马尔可夫特性)。在实践中，由于计算能力增加，并行模拟几个链以检查<a class="ae ky" href="https://en.wikipedia.org/wiki/Convergence_(logic)" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">收敛</strong> </a>和<a class="ae ky" href="https://en.wikipedia.org/wiki/Mixing_(mathematics)" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">混合</strong> </a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/15be251fb77796a14a93d22ac5af64ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I4GXmVs6UTlwB0_-c0iVNQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mu">图1: </strong> Metropolis算法。(来源:作者)</p></figure><p id="5a78" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">具有低方差的建议分布导致小的步长和高度相关的样本；相反，高方差导致较大的步长，但接受率较低，因此许多样本将被拒绝，从而减慢算法。由于这个原因，已经开发了解决这个问题的更高级的算法。关于这个话题的更多理论，可以推荐Andrew Gelman的书《贝叶斯数据分析》,这本书有免费的PDF文件。</p><div class="mw mx gp gr my mz"><a href="http://www.stat.columbia.edu/~gelman/book/" rel="noopener  ugc nofollow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">“贝叶斯数据分析”这本书的主页</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">这是本书的pdf格式，可以下载用于非商业目的。</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">www.stat.columbia.edu</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ks mz"/></div></div></a></div><h1 id="b56a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">贝叶斯逻辑回归</h1><p id="de8b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们看一个使用<a class="ae ky" href="http://archive.ics.uci.edu/ml/datasets/wine" rel="noopener ugc nofollow" target="_blank">葡萄酒数据集</a>的具体例子。<a class="ae ky" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>是一种对变量对二元类标签(<em class="ms"> y </em> =0，<em class="ms"> y </em> =1)的影响进行建模的算法，即解决<a class="ae ky" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank">监督</a>分类问题。后验概率分布给出了模型的每个权重或输出的区间估计。在数据分析中，不仅要提供一个好的模型，还要提供结论的<strong class="lt iu">不确定性估计</strong>，这一点至关重要。</p><p id="677f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们收集了一批酒瓶；我们希望为一种特定类型的葡萄酒开发一个分类模型，这种葡萄酒由于价格高而经常被模仿。此外，只有当我们(不)足够确信这是(不是)模仿时，我们才会采取行动。为了表征一款酒，我们用它的化学成分(由<a class="ae ky" href="https://en.wikipedia.org/wiki/High-performance_liquid_chromatography" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">高效液相色谱</strong> </a>测定)相对于<a class="ae ky" href="https://en.wikipedia.org/wiki/Flavonoid" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">黄酮类</strong></a><a class="ae ky" href="https://en.wikipedia.org/wiki/Proline" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">脯氨酸</strong> </a>。</p><div class="kj kk kl km gt ab cb"><figure class="no kn np nq nr ns nt paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/6ecbeece7ba1d5709da75578171fd15d.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*1q1SUm_WiisxShUb_GNrng.png"/></div></figure><figure class="no kn nu nq nr ns nt paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/df040e1f00aba4fb47e45c320fe14033.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*bIgYhgon9ydOFnR4-d9S2g.png"/></div></figure><figure class="no kn nv nq nr ns nt paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/65cfe738cfaf6a2e2614c64c457876da.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*w5Xr8wg6R743WvufhKi0ZA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk nw di nx ny translated"><strong class="bd mu">图2: </strong>不同葡萄酒的小提琴图和散点图(深色部分)。(来源:作者)</p></figure></div><p id="abf3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用Python库<a class="ae ky" href="https://docs.pymc.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> PyMC </strong> </a>通过采样开发和推断后验概率分布<em class="ms"> p(y=1|x，β) </em>的<strong class="lt iu">线性模型</strong>。从上图可以看出，线性决策边界可能就足够了。此外，由于还没有任何先验知识，我们必须设置<a class="ae ky" href="https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">无信息先验</strong> </a>，这相当于逻辑回归的一个<a class="ae ky" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">集合</strong> </a>。请注意，变量在分析前没有标准化，因此参数的大小没有任何意义。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用葡萄酒数据集的PyMC示例。</p></figure><p id="c031" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面的图中给出了产生的链，显示了收敛和混合，但应评估相关的统计数据。请注意，每条链的样本总量(通常总共四个)是3000，但是每条链的前三分之一被丢弃。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/b92ea695a6949d2cf3beac2f5b716fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1q_J8_gUrXp56iWNxDjVBQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mu">图3: </strong>马尔可夫链的推理程序；trace显示了良好的混合和收敛(至少在视觉上)。(来源:作者)</p></figure><p id="ccf6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了评估采样算法的成功，开发了几个统计数据。下表是推理过程的总结。</p><p id="9419" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最重要的统计数据是<em class="ms"> R-hat </em>，它是一种诊断方法，通过比较多个链之间的方差和每个链内的方差来测试缺乏收敛。如果收敛已经实现，链间和链内的差异应该是相同的。为了最有效地检测不收敛的证据，每个链应该已经被初始化为相对于目标分布分散的起始值。在这种情况下，<em class="ms"> R-hat </em>为1，因此实现了收敛和混合。</p><p id="0536" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另一个统计是<em class="ms">有效样本大小</em>，它基本上是由序列内的<a class="ae ky" href="https://en.wikipedia.org/wiki/Autocorrelation" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a>的量校正的样本总量(样本乘以链数)，即总量除以校正因子。如果这个指标(太)低，可能需要更多的样本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/8501afc8c8ac4a02d52280a9d591899b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SE184ap3QCBUDMRtkBzPCw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mu">表1: </strong>模型和推理总结。如指标所示，链已经收敛。</p></figure><p id="5afc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们来看看我们的估计参数。94%贝叶斯可信区间不包括零，因此我们似乎有信心两个变量都与模型相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/6f38674968777b9657294a96ad0ccc9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S9tOnoY3zBzidgE-SFp7Kw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mu">图4: </strong>估计参数的联合分布。(来源:作者)</p></figure><p id="ff2e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们看看变量的部分相关性。两者都是随着值的增加而增加概率。然而，由于线的分散性较大，黄酮类化合物的含量有更多的不确定性。</p><div class="kj kk kl km gt ab cb"><figure class="no kn oe nq nr ns nt paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/0bbef9e76df9e7fec04de87eaf31e9eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*Xy-zwmhlDMeZ1fUqKRBYbQ.png"/></div></figure><figure class="no kn of nq nr ns nt paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/11f5e6f56fb2f0675a7c69a7360c61d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*YW9XYflAlOuDff44v8T3Yg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk og di oh ny translated"><strong class="bd mu">图5: </strong>通过不透明度和带宽可视化预测的不确定性。与脯氨酸相比，黄酮类化合物的分类包含更多的不确定性。(来源:作者)</p></figure></div><p id="6636" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们有两瓶新酒。我们有多确定这是(或者不是)我们要找的酒？让我们做一个预测，评估后验分布。</p><div class="kj kk kl km gt ab cb"><figure class="no kn oi nq nr ns nt paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/54e633b464d16a59c6ecc4f69e44b4b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*oYgjxa3kMRcm_sd2aA6lUQ.png"/></div></figure><figure class="no kn oj nq nr ns nt paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/327246a0ebc87e5b27b65e2513b1cf42.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*nZsjbJzvRfxmeZU7w-f7GA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk ok di ol ny translated"><strong class="bd mu">图6: </strong>两个新样本后验概率的带宽预测不确定性。(来源:作者)</p></figure></div><p id="2022" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">第一瓶94%可信区间[0.90，1.00]，第二瓶[0.50，1.00]。对于第二个，我们不太有信心，事实上，模型在推断过程中从未见过这样的瓶子。</p><h1 id="ac16" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">变分贝叶斯</strong></h1><p id="5480" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">值得一提的是，还有另一种解决整个问题的方法，称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">变分贝叶斯</strong> </a>，其中后验概率用一些更简单的参数分布来近似，即所谓的变分分布，由此可以直接计算积分。该分布通常被公式化为单个单变量概率分布(<a class="ae ky" href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods#Mean_field_approximation" rel="noopener ugc nofollow" target="_blank">平均场近似</a>)的乘积，并且其参数通过最小化真实后验分布和变分分布之间的<a class="ae ky" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> Kullback-Leibler散度</strong> </a>来找到。变分法精度较低，但速度更快，可以作为基于模拟的方法的起点。</p><h1 id="29be" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><p id="aef0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">贝叶斯统计是一个强大的框架，它解决了频繁主义方法不能解决的问题，如预测中的不确定性。这种不确定性是由于训练数据不足或测试数据中的异常样本造成的。此外，它还是质量控制的一种形式，通过PyMC3和TensorFlow Probability等库，这些方法已经变得人人都可以使用。</p><div class="mw mx gp gr my mz"><a rel="noopener follow" target="_blank" href="/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">具有张量流概率的贝叶斯神经网络</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">概率模型不确定性预测的逐步指南。</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="om l nk nl nm ni nn ks mz"/></div></div></a></div></div></div>    
</body>
</html>