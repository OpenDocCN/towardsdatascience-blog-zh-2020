<html>
<head>
<title>Credit Risk Management: Classification Models &amp; Hyperparameter Tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">信用风险管理:分类模型和超参数调整</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/credit-risk-management-classification-models-hyperparameter-tuning-d3785edd8371?source=collection_archive---------22-----------------------#2020-08-10">https://towardsdatascience.com/credit-risk-management-classification-models-hyperparameter-tuning-d3785edd8371?source=collection_archive---------22-----------------------#2020-08-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a150" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后一部分旨在向您介绍在我们的转换数据集上应用不同分类算法的过程，以及使用超参数调整生成最佳性能模型的过程。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/f38edb1eeefde644cf4faab960b558ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kSw-njRsFwVlqbTElIX8oQ.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图片来源:<a class="ae lb" href="https://unsplash.com/photos/w7ZyuGYNpRQ" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/w7ZyuGYNpRQ</a></p></figure><p id="3c4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">提醒一下，这个端到端项目旨在解决数据科学(尤其是金融行业)中的分类问题，分为 3 个部分:</p><ol class=""><li id="881b" class="lc ld iq jp b jq jr ju jv jy le kc lf kg lg kk lh li lj lk bi translated">解释性数据分析(EDA)和特征工程</li><li id="23a8" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated">特征缩放和选择(奖励:不平衡数据处理)</li><li id="cde5" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated"><strong class="jp ir">机器学习建模(分类)</strong></li></ol><p id="78d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您错过了前两个部分，请随意查看这里的<a class="ae lb" rel="noopener" target="_blank" href="/credit-risk-management-eda-feature-engineering-81cc34efc428"><strong class="jp ir"/></a><strong class="jp ir"/>和这里的<a class="ae lb" rel="noopener" target="_blank" href="/credit-risk-management-feature-scaling-selection-b734049867ea"><strong class="jp ir"/></a>，然后再看最后一个部分，它利用它们的输出来生成最佳分类模型。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="4b25" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">A.分类模型</h1><p id="a57f" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><em class="na">应该使用哪种算法来建立一个模型来处理和解决分类问题？</em></p><p id="787a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当谈到分类时，我们有相当多的不同算法可以使用，不像回归。举例来说，逻辑回归、K-邻居、SVC、决策树和随机森林是解决这类问题的最常见和最广泛使用的算法。</p><p id="181f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是每种算法的功能以及它与其他算法的区别:</p><ul class=""><li id="dde7" class="lc ld iq jp b jq jr ju jv jy le kc lf kg lg kk nb li lj lk bi translated"><strong class="jp ir">逻辑回归</strong>:该算法利用回归<strong class="jp ir"> <em class="na">预测一个数据样本的连续概率</em> </strong>(从 0 到 1)，然后将该样本归类到更可能的目标(0 或 1)。但是，它假设输入和目标之间存在线性关系，如果数据集不遵循高斯分布，这可能不是一个好的选择。</li><li id="81d6" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk nb li lj lk bi translated"><strong class="jp ir"> K-Neighbors </strong>:该算法假设彼此非常接近的数据点属于同一类。特别地，它通过<strong class="jp ir"> <em class="na">距离其较近的邻居</em> </strong>的多个投票来对数据样本的目标(0 或 1)进行分类。</li><li id="baa9" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk nb li lj lk bi translated"><strong class="jp ir"> SVC </strong>:该算法通过<strong class="jp ir"> <em class="na">定义一个决策边界</em> </strong>进行分类，然后通过查看数据样本落在边界的哪一侧，将数据样本分类到目标(0 或 1)。本质上，该算法旨在最大化决策边界和每个类中的点之间的距离，以减少错误分类的机会。</li><li id="dd5f" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk nb li lj lk bi translated"><strong class="jp ir">决策树</strong>:顾名思义，这个算法<strong class="jp ir"> <em class="na">将树的根</em> </strong>(整个数据集)拆分成决策节点，每个决策节点将被拆分，直到没有进一步的节点可拆分。然后，该算法通过从根到叶/终端节点沿着树对数据样本进行分类，并查看它落在哪个目标节点上。</li><li id="9145" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk nb li lj lk bi translated"><strong class="jp ir">随机森林</strong>:该算法是从决策树发展而来的集成技术，其中涉及到许多协同工作的决策树。特别地，随机森林将该数据样本给予每个决策树，并且<strong class="jp ir"> <em class="na">返回最流行的分类</em> </strong>以将目标分配给该数据样本。该算法有助于避免决策树可能出现的过拟合，因为它从多个树而不是一个树聚集分类。</li></ul><p id="196b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们比较一下它们如何处理我们的数据集:</p><pre class="km kn ko kp gt nc nd ne nf aw ng bi"><span id="72aa" class="nh ly iq nd b gy ni nj l nk nl">from sklearn.linear_model import LogisticRegression<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="3570" class="nh ly iq nd b gy nm nj l nk nl">classifiers = {<br/>    "LogisticRegression" : LogisticRegression(),<br/>    "KNeighbors" : KNeighborsClassifier(),<br/>    "SVC" : SVC(),<br/>    "DecisionTree" : DecisionTreeClassifier(),<br/>    "RandomForest" : RandomForestClassifier()<br/>}</span></pre><p id="aecf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从 sklearn 导入算法后，我<strong class="jp ir"> <em class="na">创建了一个字典，将所有算法合并到一个地方</em> </strong>，这样更容易将它们一次应用到数据上，而不需要手动逐个迭代。</p><pre class="km kn ko kp gt nc nd ne nf aw ng bi"><span id="2917" class="nh ly iq nd b gy ni nj l nk nl">#Compute the training score of each models</span><span id="e74d" class="nh ly iq nd b gy nm nj l nk nl">train_scores = []<br/>test_scores = []</span><span id="fbeb" class="nh ly iq nd b gy nm nj l nk nl">for key, classifier in classifiers.items():<br/>    classifier.fit(x_a_train_rs_over_pca, y_a_train_over)<br/>    train_score = round(classifier.score(x_a_train_rs_over_pca, y_a_train_over),2)<br/>    train_scores.append(train_score)<br/>    test_score = round(classifier.score(x_a_test_rs_over_pca, y_a_test_over),2)<br/>    test_scores.append(test_score)</span><span id="e647" class="nh ly iq nd b gy nm nj l nk nl">print(train_scores)<br/>print(test_scores)</span></pre><p id="856d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在对训练集和测试集应用算法之后，似乎逻辑回归对数据集不太适用，因为分数相对较低(大约 50%，这表明该模型不能对目标进行分类)。这很容易理解，也证明了我们的原始数据集不是正态分布的。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/49cbc60509b0f5c9fda2d50cf424427d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*eeyeLtwrmE-yuiS08oiBrA.png"/></div></figure><p id="5150" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相比之下，决策树和随机森林在训练集上产生了非常高的准确度分数(85%)。然而，当分数非常低(超过 50%)时，测试集就不同了。解释大差距的可能原因是(1)过度适应训练组，(2)目标泄漏到测试组。然而，经过反复核对，情况似乎并非如此。</p><p id="6e2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我决定研究另一个评分标准，<strong class="jp ir">交叉验证分数，</strong>来看看是否有任何差异。基本上，这种技术将训练集分成 n 层(默认值= 5)，然后在 n-1 层上拟合数据，并在另一层上评分。这个过程重复 n 次，从中计算出平均分数。与标准准确度分数相比，交叉验证分数<strong class="jp ir"> <em class="na">带来了关于模型如何工作的更加客观的分析</em> </strong>。</p><pre class="km kn ko kp gt nc nd ne nf aw ng bi"><span id="bfcf" class="nh ly iq nd b gy ni nj l nk nl">from sklearn.model_selection import cross_val_score</span><span id="e52b" class="nh ly iq nd b gy nm nj l nk nl">train_cross_scores = []<br/>test_cross_scores = []</span><span id="1808" class="nh ly iq nd b gy nm nj l nk nl">for key, classifier in classifiers.items():<br/>    classifier.fit(x_a_train_rs_over_pca, y_a_train_over)<br/>    train_score = cross_val_score(classifier, x_a_train_rs_over_pca, y_a_train_over, cv=5)<br/>    train_cross_scores.append(round(train_score.mean(),2))<br/>    test_score = cross_val_score(classifier, x_a_test_rs_over_pca, y_a_test_over, cv=5)<br/>    test_cross_scores.append(round(test_score.mean(),2))<br/>    <br/>print(train_cross_scores)<br/>print(test_cross_scores)</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f3053ad3bf643464f0be43be83159ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*Zf0hsW6HZYgJi9gH95neqQ.png"/></div></figure><p id="6718" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如所见，培训和测试分数之间的差距明显缩小了！</p><p id="1bcc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于随机森林模型产生了最高的交叉验证分数，我们将根据另一个名为<strong class="jp ir"> ROC AUC 分数</strong>的分数指标对其进行测试，并查看其在<strong class="jp ir"> ROC 曲线</strong>上的表现。</p><p id="4407" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本质上，<strong class="jp ir"> ROC 曲线</strong>是假阳性率(x 轴)相对于真阳性率(y 轴)在阈值 0 和 1 之间的图，而<strong class="jp ir"> AUC </strong>代表可分离性的程度或度量(简单地说，区分靶的能力)。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi np"><img src="../Images/171a588d625349d07a1dc37dab4a54fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hMn9YQZ7eKTFTqkfzJ4FvA.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图片鸣谢:<a class="ae lb" rel="noopener" target="_blank" href="/understanding-auc-roc-curve-68b2303cc9c5">https://towardsdatascience . com/understanding-AUC-roc-curve-68b 2303 cc9 C5</a></p></figure><p id="9fd1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是如何计算<strong class="jp ir"> FPR </strong>(特异性反转)和<strong class="jp ir"> TPR </strong>(也称为灵敏度)的快速汇总表:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nq"><img src="../Images/dae91532d115b34afd74ac7446a0be66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpR4UVf513eZiGpdHajG6A.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图片来源:<a class="ae lb" rel="noopener" target="_blank" href="/hackcvilleds-4636c6c1ba53">https://towardsdatascience.com/hackcvilleds-4636c6c1ba53</a></p></figure><pre class="km kn ko kp gt nc nd ne nf aw ng bi"><span id="dc2c" class="nh ly iq nd b gy ni nj l nk nl">from sklearn.model_selection import cross_val_predict<br/>from sklearn.metrics import roc_curve<br/>from sklearn.metrics import roc_auc_score</span><span id="7de3" class="nh ly iq nd b gy nm nj l nk nl">rf = RandomForestClassifier()<br/>rf.fit(x_a_train_rs_over_pca, y_a_train_over)<br/>rf_pred = cross_val_predict(rf, x_a_test_rs_over_pca, y_a_test_over, cv=5)<br/>print(roc_auc_score(y_a_test_over, rf_pred))</span><span id="201e" class="nh ly iq nd b gy nm nj l nk nl">#Plot the ROC Curve<br/>fpr, tpr, _ = roc_curve(y_a_test_over, rf_pred)<br/>plt.plot(fpr, tpr)<br/>plt.show()</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nr"><img src="../Images/2e8d3e087c13bc968e54ab2fef9a35db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sE6LXsW4SPBisEnf1WkXBg.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">ROC AUC 得分= 76%的 ROC 曲线</p></figure><p id="4832" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为我已经证明了交叉验证在这个数据集上有效，所以我应用了另一种交叉验证技术，称为“<strong class="jp ir"> cross_val_predict </strong>”，它遵循类似的方法，即分割 n 个折叠并相应地预测值。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="9c8d" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">B.超参数调谐</h1><p id="ba07" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated"><em class="na">什么是超参数调整，它如何帮助提高模型的精度？</em></p><p id="1446" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在根据每个算法的默认估计量计算出模型之后，我希望看到是否可以进一步改进，这可以归结为超参数调整。本质上，这种技术<strong class="jp ir"> <em class="na">从每个算法中选择一组最优估计器</em> </strong>，该算法(可能)在给定的数据集上产生最高的准确度分数。</p><p id="4afe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我把(可能)放在定义中的原因是，在某些情况下，很少或没有改善取决于数据集以及最初做的准备(加上它需要永远运行)。但是，应该考虑超参数调整，以期找到性能最佳的模型。</p><pre class="km kn ko kp gt nc nd ne nf aw ng bi"><span id="c796" class="nh ly iq nd b gy ni nj l nk nl">#Use GridSearchCV to find the best parameters</span><span id="779a" class="nh ly iq nd b gy nm nj l nk nl">from sklearn.model_selection import GridSearchCV</span><span id="4807" class="nh ly iq nd b gy nm nj l nk nl">#Logistic Regression<br/>lr = LogisticRegression()<br/>lr_params = {"penalty": ['l1', 'l2'], "C": [0.001, 0.01, 0.1, 1, 10, 100, 1000], "solver": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}<br/>grid_logistic = GridSearchCV(lr, lr_params)<br/>grid_logistic.fit(x_a_train_rs_over_pca, y_a_train_over)<br/>lr_best = grid_logistic.best_estimator_</span><span id="df3d" class="nh ly iq nd b gy nm nj l nk nl">#KNearest Neighbors<br/>knear = KNeighborsClassifier()<br/>knear_params = {"n_neighbors": list(range(2,7,1)), "algorithm": ['auto', 'ball_tree', 'kd_tree', 'brutle']}<br/>grid_knear = GridSearchCV(knear, knear_params)<br/>grid_knear.fit(x_a_train_rs_over_pca, y_a_train_over)<br/>knear_best = grid_knear.best_estimator_</span><span id="4392" class="nh ly iq nd b gy nm nj l nk nl">#SVC</span><span id="1f19" class="nh ly iq nd b gy nm nj l nk nl">svc = SVC()<br/>svc_params = {"C": [0.5, 0.7, 0.9, 1], "kernel":['rbf', 'poly', 'sigmoid', 'linear']}<br/>grid_svc = GridSearchCV(svc, svc_params)<br/>grid_svc.fit(x_a_train_rs_over_pca, y_a_train_over)<br/>svc_best = grid_svc.best_estimator_</span><span id="14ad" class="nh ly iq nd b gy nm nj l nk nl">#Decision Tree</span><span id="358d" class="nh ly iq nd b gy nm nj l nk nl">tree = DecisionTreeClassifier()<br/>tree_params = {"criterion": ['gini', 'entropy'], "max_depth":list(range(2,5,1)), "min_samples_leaf":list(range(5,7,1))}<br/>grid_tree = GridSearchCV(tree, tree_params)<br/>grid_tree.fit(x_a_train_rs_over_pca, y_a_train_over)<br/>tree_best = grid_tree.best_estimator_</span></pre><p id="e982" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> GridSearchCV </strong>是在每个算法中找到最优估计量集合的关键，因为它仔细检查并组合不同的估计量以适应数据集，然后返回所有估计量中的最佳集合。</p><p id="a035" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">值得注意的一点是，我们必须记住每种算法的所有可用估计量，以便能够使用。例如，对于逻辑回归，我们有一组不属于其他算法的“惩罚”、“C”和“求解器”。</p><p id="2ab8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">找到<strong class="jp ir">后。每个算法的 best_estimator_ </strong>使用每个算法的最佳集合来拟合和预测数据。但是，我们需要将新的分数与原始分数进行比较，以确定是否有任何改进，或者继续再次微调估计值。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><h1 id="71e1" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">奖励:XGBoost 和 LightGBM</h1><p id="57bf" class="pw-post-body-paragraph jn jo iq jp b jq mv js jt ju mw jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">【XGBoost 和 LightGBM 是什么？与传统算法相比，这些算法的性能有多好？</p><p id="01f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了我听说过的常见分类算法，我还知道一些源于传统的高级算法。在这种情况下，<strong class="jp ir"> XGBoost 和 LightGBM </strong>可以被认为是<em class="na">决策和随机森林的继承者。</em>为了更好地理解这些算法是如何开发出来的，请看下面的时间表:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/041013f5299c9a022e028bd58582fc86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mC_QcBQ-V4KdrUB1gy-2yQ.jpeg"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c1eef79aead44b02a66ffa10e6d6fd6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/0*-zHaoqHQcj1l1lAP"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图片鸣谢:<a class="ae lb" href="https://www.slideshare.net/GabrielCyprianoSaca/xgboost-lightgbm" rel="noopener ugc nofollow" target="_blank">https://www . slide share . net/GabrielCyprianoSaca/xgboost-light GBM</a></p></figure><p id="9466" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我不打算详细说明这些算法在数学上有何不同，但总的来说，它们能够在处理缺失值的同时更好地<strong class="jp ir"> <em class="na">修剪决策树+同时避免过度拟合。</em>T15】</strong></p><pre class="km kn ko kp gt nc nd ne nf aw ng bi"><span id="bb7f" class="nh ly iq nd b gy ni nj l nk nl">#XGBoost<br/>import xgboost as xgb</span><span id="6ad9" class="nh ly iq nd b gy nm nj l nk nl">xgb_model = xgb.XGBClassifier()<br/>xgb_model.fit(x_a_train_rs_over_pca, y_a_train_over)<br/>xgb_train_score = cross_val_score(xgb_model, x_a_train_rs_over_pca, y_a_train_over, cv=5)<br/>xgb_test_score = cross_val_score(xgb_model, x_a_test_rs_over_pca, y_a_test_over, cv=5)</span><span id="7c3e" class="nh ly iq nd b gy nm nj l nk nl">print(round(xgb_train_score.mean(),2))<br/>print(round(xgb_test_score.mean(),2))</span><span id="4cb8" class="nh ly iq nd b gy nm nj l nk nl">#LightGBM<br/>import lightgbm as lgb</span><span id="5b4e" class="nh ly iq nd b gy nm nj l nk nl">lgb_model = lgb.LGBMClassifier()<br/>lgb_model.fit(x_a_train_rs_over_pca, y_a_train_over)<br/>lgb_train_score = cross_val_score(lgb_model, x_a_train_rs_over_pca, y_a_train_over, cv=5)<br/>lgb_test_score = cross_val_score(lgb_model, x_a_test_rs_over_pca, y_a_test_over, cv=5)</span><span id="6c8f" class="nh ly iq nd b gy nm nj l nk nl">print(round(lgb_train_score.mean(),2))<br/>print(round(lgb_test_score.mean(),2))</span></pre><p id="d853" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">经过计算，每个模型的训练和集合分数分别为 72% &amp; 73% (XGBoost)和 69% &amp; 72% (LightGBM)，与上面计算的随机森林模型相对相同。然而，我们仍然能够通过对这些高级模型进行超参数调整来进行进一步优化，但要注意，这可能需要很长时间，因为 XGBoost 和 LightGBM 由于其算法的复杂性而具有更长的运行时间。</p></div><div class="ab cl lq lr hu ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ij ik il im in"><p id="ede6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">瞧啊。这就是这个端到端项目关于分类的总结！如果您热衷于探索整个代码，请随时查看我下面的 Github:</p><p id="a956" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">知识库:<a class="ae lb" href="https://github.com/andrewnguyen07/credit-risk-management" rel="noopener ugc nofollow" target="_blank">https://github.com/andrewnguyen07/credit-risk-management</a>T18】领英:<a class="ae lb" href="http://www.linkedin.com/in/andrewnguyen07" rel="noopener ugc nofollow" target="_blank">www.linkedin.com/in/andrewnguyen07</a></p><p id="404f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关注我的媒体，关注即将到来的未来项目！</p></div></div>    
</body>
</html>