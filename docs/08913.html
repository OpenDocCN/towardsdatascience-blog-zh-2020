<html>
<head>
<title>Recommendation System Series Part 6: The 6 Variants of Autoencoders for Collaborative Filtering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推荐系统系列第 6 部分:用于协同过滤的自动编码器的 6 种变体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7?source=collection_archive---------6-----------------------#2020-06-27">https://towardsdatascience.com/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7?source=collection_archive---------6-----------------------#2020-06-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9cbd" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">RECSYS 系列</h2><div class=""/><div class=""><h2 id="4a9c" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">利用自动编码器提出建议</h2></div><p id="b51c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="lk">更新:</em> </strong> <em class="lk">本文是我探索学术界和工业界推荐系统系列文章的一部分。查看完整系列:</em> <a class="ae ll" rel="noopener" target="_blank" href="/recommendation-system-series-part-1-an-executive-guide-to-building-recommendation-system-608f83e2630a"> <em class="lk">第一部分</em> </a> <em class="lk">，</em> <a class="ae ll" rel="noopener" target="_blank" href="/recommendation-system-series-part-2-the-10-categories-of-deep-recommendation-systems-that-189d60287b58"> <em class="lk">第二部分</em> </a> <em class="lk">，</em> <a class="ae ll" rel="noopener" target="_blank" href="/recommendation-system-series-part-3-the-6-research-directions-of-deep-recommendation-systems-that-3a328d264fb7"> <em class="lk">第三部分</em> </a> <em class="lk">，</em> <a class="ae ll" rel="noopener" target="_blank" href="/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5"> <em class="lk">第四部分</em> </a> <em class="lk">，</em> <a class="ae ll" rel="noopener" target="_blank" href="/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883"> <em class="lk">第五部分</em> </a> <em class="lk">和</em> <a class="ae ll" rel="noopener" target="_blank" href="/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7"/></p><p id="27dd" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在过去的几年中，已经提出了许多推荐模型。然而，它们在处理数据稀疏和冷启动问题时都有其局限性。</p><ul class=""><li id="c0a9" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">如果用户和项目之间的交互非常稀疏，当推荐性能显著下降时，就会出现<strong class="kq ja">数据稀疏性</strong>。</li><li id="b3cb" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><strong class="kq ja">冷启动问题</strong>发生在模型无法推荐新用户和新物品的时候。</li></ul><p id="7ffd" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了解决这些问题，最近的方法已经利用了关于用户或项目的辅助信息。然而，由于这种模型在捕捉用户偏好和项目特征方面的限制，推荐性能的提高并不显著。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/dea086609bc50999ce11bd1ec55fcf44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TiaR1bZmXIqqJ4nD1byRjg.jpeg"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated"><em class="mq">丽贝卡·海尔威尔——抖音的推荐有些奇怪(</em><a class="ae ll" href="https://www.vox.com/recode/2020/2/25/21152585/tiktok-recommendations-profile-look-alike" rel="noopener ugc nofollow" target="_blank"><em class="mq">https://www . vox . com/recode/2020/2/25/21152585/tiktok-推荐-简介-长相相似</em> </a> <em class="mq"> ) </em></p></figure><p id="baca" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> Auto-encoder </strong>是一种适合无监督学习任务的神经网络，包括生成建模、维度缩减和高效编码。在许多领域，包括计算机视觉、语音识别和语言建模，它在学习潜在特征表示方面已经显示出它的优越性。鉴于这一点，新的推荐架构已经整合了 autoencoder，因此在重新创造用户体验以满足客户方面带来了更多机会。</p><ul class=""><li id="f227" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">传统模型只处理单一数据源(评级或文本)，而基于自动编码器的模型可以处理<strong class="kq ja">异构</strong>数据源(评级、音频、视频、视频)。</li><li id="f57f" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">Auto-encoder 的<strong class="kq ja">更好地理解了用户需求和项目特征的</strong>，因此比传统模型具有更高的推荐准确度。</li><li id="e046" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">此外，自动编码器有助于推荐模型<strong class="kq ja">在多媒体场景中更具适应性</strong>，并且在处理输入噪声方面比传统模型更有效。</li></ul><p id="1e9b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这篇文章和接下来的文章中，我将介绍推荐系统的创建和训练，因为我目前正在做这个主题的硕士论文。</p><ul class=""><li id="1841" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated"><a class="ae ll" rel="noopener" target="_blank" href="/recommendation-system-series-part-1-an-executive-guide-to-building-recommendation-system-608f83e2630a">第 1 部分</a>提供了关于推荐系统、如何构建推荐系统以及如何利用推荐系统改善各行业业务的高级概述。</li><li id="d09a" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" rel="noopener" target="_blank" href="/recommendation-system-series-part-2-the-10-categories-of-deep-recommendation-systems-that-189d60287b58">第 2 部分</a>提供了关于这些模型的优势和应用场景的正在进行的研究计划的仔细回顾。</li><li id="b49e" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" rel="noopener" target="_blank" href="/recommendation-system-series-part-3-the-6-research-directions-of-deep-recommendation-systems-that-3a328d264fb7">第 3 部分</a>提供了几个可能与推荐系统学者社区相关的研究方向。</li><li id="f3ee" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" rel="noopener" target="_blank" href="/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5">第 4 部分</a>提供了您可以构建的矩阵分解的 7 种变体的本质数学细节:从使用巧妙的辅助功能到应用贝叶斯方法。</li><li id="f490" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" rel="noopener" target="_blank" href="/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883">第 5 部分</a>提供了基于多层感知器的协同过滤模型的 5 个变体的架构设计，它们是能够以非线性方式解释特征的判别模型。</li></ul><p id="fb3b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在第 6 部分中，我探索了使用<strong class="kq ja">自动编码器</strong>进行协同过滤。更具体地说，我将剖析六篇将自动编码器纳入其推荐架构的原则性论文。但是首先，让我们浏览一下关于自动编码器及其变体的初级读本。</p><h1 id="2954" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">自动编码器及其变体入门</h1><p id="76b5" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">如下图所示，标准自动编码器由输入层、隐藏层和输出层组成。输入数据被传递到输入层。输入层和隐藏层构成一个编码器。隐藏层和输出层构成解码器。输出数据来自输出层。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi no"><img src="../Images/bb7d86c945e98cab8591fd7f12b01039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j65yIp8Dx-H0ieqFbMsAcw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">自动编码器架构</p></figure><p id="c022" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">编码器用函数 f 将高维输入数据 x 编码成低维隐藏表示 h:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c89411ed4f414b2d97cda49ad1cb2dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*ZDb9hUXgEi-FMbzHFohEAA.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated"><em class="mq">方程式 1 </em></p></figure><p id="2572" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中，s_f 是激活函数，W 是权重矩阵，b 是偏置向量。</p><p id="530a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">解码器通过另一个函数 g 将隐藏表示 h 解码回重构 x ’:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/8c9ecc8dbddee31ee7d278a0bde1a72c.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*uilckfOOmzGG98TbW96MGw.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 2</p></figure><p id="b683" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中 s_g 是激活函数，W’是权重矩阵，b’是偏差向量。</p><p id="61ab" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">s_f 和 s_g 的选择是非线性的，例如 Sigmoid、TanH 或 ReLU。这使得 auto-encoder 能够学习比其他无监督线性方法更有用的特征，比如主成分分析。</p><p id="4ec3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我可以训练自动编码器通过平方误差(对于回归任务)或交叉熵误差(对于分类任务)来最小化 x 和 x’之间的重建误差。</p><p id="eccf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这是平方误差的公式:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/45497a4455fe046f41ed954bb973e6cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*t3fB_ek8KO4mWAo0H_03iw.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 3</p></figure><p id="7864" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">交叉熵误差的公式如下:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi gj"><img src="../Images/38895695e4afb34be58b75f9a1a258ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ypDPKIu3EuazOiZTC9ofiQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 4</p></figure><p id="a788" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，向自动编码器的最终重构误差添加一个正则项始终是一个好的做法:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ns"><img src="../Images/9d1b5be154c56e63fbaf9c3a87a0bf43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nlve3hgRrKbZ-FrmchKnNA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 5</p></figure><p id="1df7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">上面的重建误差函数可以通过随机梯度下降或替代的最小二乘法来优化。</p><p id="4e1b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">目前在推荐系统中使用的自动编码器有很多种。四种最常见的是:</p><ul class=""><li id="baec" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated"><strong class="kq ja">去噪自动编码器(DAE) </strong>在将输入映射到隐藏表示之前破坏输入，然后从其破坏的版本重建原始输入。想法是迫使隐藏层获得更健壮的特征，并防止网络仅仅学习身份函数。</li><li id="c6ff" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><strong class="kq ja">堆叠去噪自动编码器(SDAE) </strong>堆叠多个去噪自动编码器，以获得输入的更高级表示。训练通常用贪婪算法逐层优化。这里明显的缺点是训练的高计算成本和缺乏对高维特征的可扩展性。</li><li id="a4b8" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><strong class="kq ja">边缘化去噪自动编码器(MDAE) </strong>通过边缘化随机特征破坏来避免 SDAE 的高计算成本。因此，它具有训练速度快、实现简单和对高维数据的可扩展性。</li><li id="a2c3" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><strong class="kq ja">变分自动编码器(VAE) </strong>是一个无监督的潜在变量模型，它从高维数据中学习深度表示。其思想是将输入编码为概率分布，而不是像普通自动编码器那样的点估计。然后，VAE 使用解码器通过使用来自该概率分布的样本来重建原始输入。</li></ul><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nt"><img src="../Images/ade20770b7d92fdf223e6c2e40330b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y14eRBMtp_9iXkl0Dp1fxg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">可变自动编码器架构</p></figure><p id="7c0a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">好了，是时候回顾一下不同的基于自动编码器的推荐框架了！</p><h1 id="87e4" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak"> 1 — AutoRec </strong></h1><p id="6bba" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">从自动编码器的角度考虑协同过滤问题的最早的模型之一是来自 Suvash Sedhain、Aditya、Scott Sanner 和 Lexing Xie 的“<a class="ae ll" href="https://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf" rel="noopener ugc nofollow" target="_blank">自动编码器遇到协同过滤</a>”的<strong class="kq ja"> AutoRec </strong>。</p><p id="7066" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在该论文的设置中，有 m 个用户，n 个项目，以及维数为 m×n 的部分填充的用户-项目交互/评级矩阵 r。每个用户 u 可以由部分填充的向量 rᵤ表示，每个项目 I 可以由部分填充的向量 rᵢ.表示 AutoRec 直接将用户评级向量 rᵤ或项目评级 rᵢ作为输入数据，并在输出层获得重建的评级。根据两种类型的输入，AutoRec 有两种变体:基于项目的 AutoRec ( <strong class="kq ja"> I-AutoRec </strong>)和基于用户的 AutoRec ( <strong class="kq ja"> U-AutoRec </strong>)。两者结构相同。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nu"><img src="../Images/8426319c87b1ca18faa15817957f06d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxtvzrypm5yRqgooBoahNg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated"><em class="mq"> Suvesh Sedhain 等人——AutoRec:自动编码器满足协同过滤(</em><a class="ae ll" href="https://dl.acm.org/doi/10.1145/2740908.2742726" rel="noopener ugc nofollow" target="_blank"><em class="mq">https://dl.acm.org/doi/10.1145/2740908.2742726</em></a><em class="mq">)</em></p></figure><p id="2877" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">论文中的图 1 说明了 I-AutoRec 的结构。阴影节点对应于观察到的等级，实线连接对应于为输入 rᵢ.更新的权重</p><p id="244e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">给定输入 rᵢ，重建为:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/82a8e1cf7a8fb2adc264701c10c0ca0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*vZIyErHFnjaOJV4sV1ieDQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 6</p></figure><p id="739d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中 f 和 g 是激活函数，参数θ包括 W、V、mu 和 b</p><p id="926d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">AutoRec 仅使用<strong class="kq ja">普通自动编码器结构</strong>。该模型的目标函数类似于自动编码器的损失函数:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/16065584831b425ce78a45dfa5ed1142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*kIkLG37Rk6OxT26MXAnjoA.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 7</p></figure><p id="04ca" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该函数可以通过弹性传播(收敛更快并产生可比较的结果)或 L-BFGS(有限内存 Broyden Fletcher Goldfarb Shanno 算法)进行优化。</p><p id="7904" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">以下是 AutoRec 的一些重要信息:</p><ul class=""><li id="d3e2" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">I-AutoRec 的表现一般比 U-AutoRec 好。这是因为每个项目的平均评级数比每个用户给出的平均评级数多得多。</li><li id="5370" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">激活功能的不同组合会显著影响 AutoRec 的性能。</li><li id="bc07" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">增加隐藏神经元的数量或层数可以提高模型性能。这是有意义的，因为扩展隐藏层的维度允许 AutoRec 有更多的能力来模拟输入要素。</li><li id="642b" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">添加更多的层来形成深层网络可以导致轻微的改善。</li></ul><p id="ad8d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">出于说明目的，AutoRec 模型类别的张量流代码如下所示:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="7143" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于<a class="ae ll" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/AutoRec-TensorFlow" rel="noopener ugc nofollow" target="_blank">我的 TensorFlow 实现</a>，我用一个由 sigmoid 非线性函数激活的 500 个单元的隐藏层来训练 AutoRec 架构。其他超参数包括 0.001 的学习率、512 的批量大小、Adam 优化器和 1 的 lambda 正则化器。</p><h1 id="eb7c" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak"> 2 — DeepRec </strong></h1><p id="9666" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated"><strong class="kq ja"> DeepRec </strong>是由英伟达的 Oleisii Kuchaiev 和 Boris Ginsburg 创建的模型，参见“<a class="ae ll" href="https://arxiv.org/abs/1708.01715" rel="noopener ugc nofollow" target="_blank">为协同过滤训练深度自动编码器</a>”该模型受上述 AutoRec 模型的启发，有几个重要区别:</p><ul class=""><li id="d8ab" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">网络要深得多。</li><li id="b8d7" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">该模型使用“比例指数线性单位”(SELUs)。</li><li id="6f52" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">辍学率很高。</li><li id="0d2a" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">作者在训练期间使用迭代输出再馈。</li></ul><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nz"><img src="../Images/eaa613e225033d042e85174efadbea32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SKSsXceEdHGlJGuJ7FmPWw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated"><em class="mq"> Oleisii Kuchaiev 和 Boris Ginsburg——为协同过滤训练深度自动编码器(</em><a class="ae ll" href="https://arxiv.org/abs/1708.01715" rel="noopener ugc nofollow" target="_blank"><em class="mq">https://arxiv.org/abs/1708.01715</em></a><em class="mq">)</em></p></figure><h2 id="a86b" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 2.1 —型号</strong></h2><p id="799c" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">上图描述了一个典型的 4 层自动编码器网络。编码器有 2 层 E1 和 E2，而解码器有 2 层 D1 和 D2。它们在表示 z 上融合在一起。这些层被表示为 f(W * x + b)，其中 f 是某个非线性激活函数。如果激活函数的范围小于数据的范围，则解码器的最后一层应该保持线性。作者发现隐藏层中的激活函数 f 包含非零负部分是非常重要的，并且在他们的大多数实验中使用 SELU 单位。</p><h2 id="7532" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 2.2 —损失函数</strong></h2><p id="2d5e" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">由于预测用户表示向量 x 中的零没有意义，作者优化了<strong class="kq ja">掩蔽均方误差</strong>损失:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/add579037f4ab401d6a2cc918474b6c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*k6afHTqRVp3DswD59uvomw.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 8</p></figure><p id="495c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中 r_i 是实际评级，y_i 是重构评级，m_i 是屏蔽函数，使得如果 r_i 不为 0，m_i = 1，否则 m_i = 0。</p><p id="2678" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> 2.3 —密集再进给</strong></p><p id="8825" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在前向传递和推理传递期间，模型从训练集 x 中获取由其评级向量表示的用户。注意，x 非常稀疏，而解码器 f(x)的输出是密集的，并且包含对语料库中所有项目的评级预测。因此，为了明确实施定点约束并执行密集训练更新，作者用如下的<strong class="kq ja">迭代密集再反馈步骤</strong>来增加每个优化迭代:</p><ol class=""><li id="dc6f" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj om ls lt lu bi translated">在初始正向传递期间，给定稀疏输入 x，模型使用等式 8 计算密集输出 f(x)和 MMSE 损耗。</li><li id="706e" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj om ls lt lu bi translated">在最初的反向过程中，模型计算梯度并相应地更新权重。</li><li id="3499" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj om ls lt lu bi translated">在第二次向前传递期间，模型将 f(x)视为新的数据点，从而计算 f(f(x))。f(x)和 f(f(x))都变得稠密。MMSE 损失现在所有的 m 都是非零的。</li><li id="a2df" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj om ls lt lu bi translated">在第二次反向传递期间，模型再次计算梯度并相应地更新权重。</li></ol><p id="4d83" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了便于说明，下面给出了 DeepRec 模型定义的张量流代码:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="11fb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于<a class="ae ll" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/NVIDIA-DeepRec-TensorFlow" rel="noopener ugc nofollow" target="_blank">我的 TensorFlow 实现</a>，我用以下架构训练了 DeepRec，512，512，1024，512，512，n]。所以 n 是用户给定的收视率数，编码器有 3 层大小(512，512，1034)，瓶颈层有大小 1024，解码器有 3 层大小(512，512，n)。我用随机梯度下降来训练模型，动量为 0.9，学习率为 0.001，批量为 512，辍学率为 0.8。参数通过 Xavier 初始化方案进行初始化。</p><h1 id="6fbd" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak"> 3 —协同去噪自动编码器</strong></h1><p id="6ac9" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated"><a class="ae ll" href="https://alicezheng.org/papers/wsdm16-cdae.pdf" rel="noopener ugc nofollow" target="_blank">“Top-N 推荐系统的协同去噪自动编码器</a>”作者、Christopher DuBois、Alice Zheng 和 Martin Ester 是一个具有一个隐藏层的神经网络。与 AutoRec 和 DeepRec 相比，<strong class="kq ja"> CDAE </strong>有以下不同之处:</p><ul class=""><li id="4185" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">CDAE 的输入不是用户项目评分，而是部分观察到的隐含反馈 r(用户的项目偏好)。如果用户喜欢一部电影，对应的条目值为 1，否则为 0。</li><li id="3e4e" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">与用于评级预测的前两个模型不同，CDAE 主要用于排名预测(也称为前 N 名偏好推荐)。</li></ul><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi on"><img src="../Images/01d33392c42a880447c4756bc6b6169e.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*xd53jOk5oxlKLuQHZ9Jz1Q.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated"><em class="mq">https://dl.acm.org/doi/10.1145/2835776.2835837</em><em class="mq">)</em>吴耀等人——前 N 名推荐系统的协同去噪自动编码器</p></figure><h2 id="59cd" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 3.1 —型号</strong></h2><p id="aa54" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">上图显示了一个 CDAE 的样本结构，它由三层组成:输入层、隐藏层和输出层。</p><ul class=""><li id="817b" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">在输入层中总共有 I + 1 个节点。第一个 I 节点代表用户偏好，这些 I 节点的每个节点对应一个项目。最后一个节点是特定于用户的节点，在上图中用红色节点表示，这意味着不同的用户有不同的节点和相关的权重。</li><li id="4d04" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">这里，yᵤ是用户 u 对 I 中所有项目的 I 维反馈向量。yᵤ是一个稀疏的二进制向量，只具有非零值:如果用户 u 对 I 评分，yᵤᵢ = 1，否则 yᵤᵢ = 0。</li><li id="ca36" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">有 K(&lt;&lt; I) + 1 nodes in the hidden layer. The blue K nodes are fully connected to the nodes of the input layer. The pink additional node in the hidden layer captures the bias effects.</li><li id="ee66" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">In the output layer, there are I nodes which are the reconstructed output of the input yᵤ. They are fully connected to the nodes in the hidden layer.</li></ul><p id="b4bd" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">The corrupted input <strong class="kq ja">CDAE 的 r_corr </strong>是从一个条件高斯分布中得出的<strong class="kq ja"> p(r_corr | r) </strong>。<strong class="kq ja"> r_corr </strong>的重建公式如下:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi oo"><img src="../Images/785cbbf06e17e8a05557d1ca385b43e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NELyxgyzY0BMR10nS4awfw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 9</p></figure><p id="b8d1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中 W₁是对应于编码器的权重矩阵(从输入层到隐藏层)，W₂是对应于解码器的权重矩阵(从隐藏层到输出层)。Vᵤ是红色用户节点的权重矩阵，而 b₁和 b₂都是偏置向量。</p><h2 id="f3df" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 3.2 —损失函数</strong></h2><p id="54cb" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">通过最小化平均重建误差来学习 CDAE 的参数，如下所示:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi op"><img src="../Images/fb6997a4d4769a515405b8ee990e508d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9irgSj84V_mw8zAZB3uzfA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 10</p></figure><p id="b626" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">上式中的损失函数 L <strong class="kq ja"> (r_corr，h(r_corr)) </strong>可以是平方损失，也可以是 logistic 损失。CDAE 使用平方 L2 范数来控制模型的复杂度。它还(1)应用随机梯度下降来学习模型的参数，以及(2)在学习过程中采用 AdaGrad 来自动调整训练步长。</p><p id="a105" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">作者还提出了<strong class="kq ja">负采样技术</strong>来从用户没有交互的项目中提取一个小子集，以在不降低排序质量的情况下显著降低时间复杂度。在推理时，CDAE 将用户的现有偏好集(没有损坏)作为输入，并向该用户推荐输出层上具有最大预测值的项目。</p><p id="2ccf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了便于说明，下面给出了 CDAE 建筑等级的 PyTorch 代码:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="b562" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于我的 PyTorch 实现，我使用了一个 CDAE 架构，隐藏了 50 个单元。我使用随机梯度下降来训练模型，学习率为 0.01，批量为 512，损坏率为 0.5。</p><h1 id="3591" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak"> 4 —多项式变分自动编码器</strong></h1><p id="4cdb" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">这次讨论中最有影响力的论文之一是来自网飞的 Dawen Liang、Rahul Krishnan、Matthew Hoffman 和 Tony Jebara 的“<a class="ae ll" href="https://arxiv.org/abs/1802.05814" rel="noopener ugc nofollow" target="_blank">协作过滤的变分自动编码器</a>”。它提出了一种 VAE 的变体，用于隐式数据的推荐。特别是，作者介绍了<strong class="kq ja">一种有原则的贝叶斯推理方法</strong>来估计模型参数，并显示出比常用的似然函数更好的结果。</p><p id="3251" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">本文使用 U 来索引所有用户，使用 I 来索引所有项目。用户逐项交互矩阵称为 X(维度为 U x I)。小写的 xᵤ是一个单词包向量，其中包含用户 u 对每个项目的点击次数。对于隐式反馈，该矩阵被二进制化为只有 0 和 1。</p><h2 id="cf8e" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 4.1 —型号</strong></h2><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi oq"><img src="../Images/4a1f7acf1e5b5b3e4b6b5f7748c9b1b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y43nJInjgDBM1p7PHq9nmA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 11</p></figure><p id="dda7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">模型的生成过程如等式 11 所示，细分如下:</p><ul class=""><li id="b4ea" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">对于每个用户 u，该模型从标准高斯先验中采样 k 维潜在表示 zᵤ。</li><li id="b520" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">然后，它通过非线性函数 f_θ变换 zᵤ，以产生 I 项π(zᵤ).上的概率分布</li><li id="e7e3" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">f_θ是一个多层感知器，有参数θ和一个 softmax 激活函数。</li><li id="ad43" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">给定来自用户 u 的点击总数，从具有概率π(zᵤ).的多项式分布中采样词袋向量 xᵤ</li></ul><p id="90fa" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">用户 u 的对数似然性(以潜在表示为条件)为:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi or"><img src="../Images/30dc3a74c4c78ca12965c3541e70bd11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*S54g8utzp1nN7Gn0LYp5oQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 12</p></figure><p id="9806" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">作者认为<strong class="kq ja">多项式分布</strong>适合这个协同过滤问题。具体而言，等式 11 中的交互作用矩阵的可能性奖励了将概率质量放在 xᵤ.的非零条目上的模型然而，考虑到π(zᵤ)的总和必须为 1，项目必须竞争有限的概率质量预算。因此，该模型应该将更多的概率质量分配给更有可能被点击的项目，使得它适合于在推荐系统的前 N 名排名评估度量中实现稳定的性能。</p><h2 id="b7b2" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 4.2 —变分推理</strong></h2><p id="209f" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">为了训练等式 11 中的生成模型，作者经由<strong class="kq ja">变分推理</strong>通过近似难以处理的后验分布 p(zᵤ | xᵤ)来估计θ。这种方法用一种更简单的变分分布 q(zᵤ(一种全对角高斯分布)来逼近真实的难处理的后验概率。变分推理的目标是优化自由变分参数{μᵤ，σᵤ }使得库尔贝克-莱伯散度<strong class="kq ja">kl(q(zᵤ)| |p(zᵤ|xᵤ)】</strong>最小化。</p><p id="659d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">变分推断的问题在于，用于优化σᵤ{μᵤ的参数数量会随着数据集中用户和项目的数量而增长。VAE 通过用一个依赖于数据的函数代替单个变分参数来帮助解决这个问题:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi os"><img src="../Images/090c3b5e1f50fdf51b0c000df04ebcd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*BtSbkfW-i_HqDm50Gc_rQA.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 13</p></figure><p id="c35a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该函数由ϕ参数化，其中μ_{ϕ} (xᵤ)和σ_{ϕ} (xᵤ)都是 k 维向量。然后，变分分布设置如下:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ot"><img src="../Images/3fe2da346a89dbd6f7b68f39e8b5732c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*c6ueVRB2aFHo_njFn_AakQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 14</p></figure><p id="ca81" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">使用输入 xᵤ，推理模型返回相应的变分分布 q_{ϕ} (zᵤ|xᵤ).)的变分参数当被优化时，这种变分分布近似于难处理的后 p_{ϕ} (zᵤ | xᵤ).</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ou"><img src="../Images/317fb25fc7c266df8604d7780e9c75b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xKAsv3NHcoprcKaE7rMHxw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated"><em class="mq">变分渐变(</em><a class="ae ll" href="https://matsen.fredhutch.org/general/2019/08/24/vbpi.html" rel="noopener ugc nofollow" target="_blank"><em class="mq">【https://matsen.fredhutch.org/general/2019/08/24/vbpi.html】</em></a><em class="mq">)</em></p></figure><p id="a62e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了用变分推断学习潜变量模型，标准的方法是对数据的对数边际似然进行下界估计。用户 u 最大化的目标函数现在变成:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ov"><img src="../Images/58c1b92a50dc7e75b1fddae8e60507de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mfye-rKxR5ML7oKkrbiyNg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 15</p></figure><p id="5f10" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个目标的另一个术语是<strong class="kq ja">证据下限(ELBO)。</strong>直观上，我们应该能够通过对 zᵤ∞q_ϕ进行采样并使用随机梯度上升对其进行优化来获得 ELBO 的估计值。然而，我们不能对爱尔波求导以得到相对于ϕ.的梯度<strong class="kq ja">重新参数化技巧</strong>在这里派上了用场:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/5686e9e4345d532524facd8e99206ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*33aF6jIETMnpE0o0TPkDKA.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 16</p></figure><p id="7fc8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">本质上，我们隔离了采样过程中的随机性，因此相对于ϕ的梯度可以通过采样的 zᵤ.反向传播</p><p id="be67" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从不同的角度来看，等式 15 的第一项可以解释为<strong class="kq ja">重构误差</strong>，等式 15 的第二项可以解释为<strong class="kq ja">正则化</strong>。因此，作者用一个附加参数β来扩展方程 15，以控制正则化的强度:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ox"><img src="../Images/7ca2d7320d6641b8a6a52b898d875ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cO_Z6zLxi2jmpbz0ir7ptQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 17</p></figure><p id="97f5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该参数β在模型拟合数据的良好程度和学习期间近似后验与先验的接近程度之间进行权衡。作者通过<strong class="kq ja"> KL 退火</strong>调整β，这是一种常见的启发式方法，用于在担心模型未被充分利用时训练 VAEs。</p><h2 id="ee61" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 4.3 —预测</strong></h2><p id="17be" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">给定用户的点击历史 x，该模型基于未归一化的预测多项式概率 f_ϕ (z)对所有项目进行排名。x 的潜在表示 z 就是变分分布 z = μ_ϕ (x)的平均值。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi oy"><img src="../Images/5c4ebbbf5e4b108b9267cc7443a55b68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0GKmkJBakscCKYKFneIWIA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated"><em class="mq">梁大文等——用于协同过滤的变分自动编码器(</em><a class="ae ll" href="https://arxiv.org/abs/1802.05814" rel="noopener ugc nofollow" target="_blank"><em class="mq">【https://arxiv.org/abs/1802.05814】</em></a><em class="mq">)</em></p></figure><p id="bbe5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该论文中的图 2 提供了自动编码器不同变体的统一视图。</p><ul class=""><li id="3430" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">2a 是普通的自动编码器架构，如 AutoRec 和 DeepRec 所示。</li><li id="a0fc" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">2b 是去噪自动编码器架构，如在 CDAE 看到的。这里，ϵ是注入到输入层的噪声。</li><li id="67d2" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">2c 是 MultVAE 下的变分自动编码器架构，其使用由ϕ参数化的推理模型来产生近似变分分布的均值和方差，如上面详细解释的。</li></ul><p id="83f7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">MultVAE 架构类的 PyTorch 代码如下所示，用于说明目的:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="1f6e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于<a class="ae ll" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/VAE-PyTorch" rel="noopener ugc nofollow" target="_blank">我的 PyTorch 实现</a>，我保持生成模型 f 和推理模型 g 的架构对称，并使用一个带有 1 个隐藏层的 MLP。潜在表示 K 的维度被设置为 200，而其他隐藏层的维度被设置为 600。MultVAE 的整体架构现在变成[I-&gt;600-&gt;200-&gt;600-&gt;I]，其中 I 是项目总数。其他模型细节包括 tanH 激活函数，概率为 0.5 的辍学率，Adam 优化器，批量为 512，学习率为 0.01。</p><h1 id="3171" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak"> 5 —顺序变分自动编码器</strong></h1><p id="7ee2" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">在“<a class="ae ll" href="https://arxiv.org/abs/1811.09975" rel="noopener ugc nofollow" target="_blank">用于协同过滤的序列变分自动编码器</a>”中，诺维恩·萨克德瓦、朱塞佩·曼科、埃托雷·里塔克和维克拉姆·迪普通过探索过去偏好历史中存在的丰富信息，提出了对 MultVAE 的扩展。他们引入了 MultVAE 的递归版本<strong class="kq ja">，而不是传递整个历史的子集，而不管时间依赖性，他们通过递归神经网络传递消费序列子集。他们表明<strong class="kq ja">处理时间信息</strong>对于提高 VAE 的准确性至关重要。</strong></p><h2 id="7535" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 5.1 —设置</strong></h2><p id="cae4" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">问题设置与 MultVAE 论文中的设置完全相似:U 是一组用户，I 是一组项目，X 是维度为 U x I 的用户-项目偏好矩阵。主要区别在于 SVAE 考虑了矩阵 X 中的优先级和时间关系。</p><ul class=""><li id="9f7d" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">x 归纳出评分矩阵中项目之间的自然排序关系:i  x_{u，j}。</li><li id="5314" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">他们假设定时信息 T 的存在，其中 t_{u，i}项代表我被 u 选中的时间，那么我 t_{u，j}。</li><li id="629d" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">他们还在 xᵤ:的元素中引入了一个时间标记，x_{u(t)}表示</li></ul><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi oz"><img src="../Images/fb8fe1d39b34d72d5d4d918f93517fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kcXy717WCGIoWLvpFhqbHQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated"><em class="mq">诺维恩·萨克德瓦等人——用于协同过滤的序列变分自动编码器(</em><a class="ae ll" href="https://arxiv.org/abs/1811.09975" rel="noopener ugc nofollow" target="_blank"><em class="mq">【https://arxiv.org/abs/1811.09975】</em></a><em class="mq">)</em></p></figure><h2 id="9303" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 5.2 —型号</strong></h2><p id="8dfe" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">论文中的上图显示了 MultVAE、SVAE 和另一个名为 RVAE 的模型之间的架构差异(我在这里不讨论它)。查看 SVAE 架构，我可以观察到 z_{u(t)}所依赖的层中出现的循环关系。SVAE 背后的基本思想是<strong class="kq ja">潜在变量建模应该能够表达时间动态</strong> <strong class="kq ja">以及用户历史</strong>中偏好之间的因果关系和依赖性。</p><p id="d5fb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们复习一下数学。在这个 SVAE 框架内，作者通过将每个事件与之前的事件相联系来模拟时间依赖性。给定一个序列 x_{(1: T}，那么它的概率是:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e15b99f8e8d282eb854c17cfc39156eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*Hy0h9uzUubMubZxaWGrMGg.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 18</p></figure><p id="a08e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个概率表示 x_{(t+1)和 x_{(1:t)}之间的递归关系。因此，模型可以单独处理每个时间步长。</p><p id="ced3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">回想等式 11 中的生成过程，我们可以添加时间戳 t，如下所示:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pb"><img src="../Images/46c3d81f30da9ecc735f010339932399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g81mMDJVHs-aP7KuuuIu5A.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 19</p></figure><p id="9b60" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">等式 19 得出联合似然性:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/f1b007b108b2405b027b798bfa388bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*wZ12qH3O8zlrUTlNlj884A.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 20</p></figure><p id="dabc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">等式 20 中的后验似然性可以用因式分解的建议分布来近似:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/794777ecba4c72cc47a4f9043c769a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*BOD2b7jMc6QLjqezTx-9iA.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 21</p></figure><p id="2ac7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中右边是高斯分布，其参数μ和σ取决于当前历史 x_{u(1:t-1)}，通过递归层 h_t:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/94ab578c1cccf7e29b052c76db3d8f52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*ywi9gF9PahS6DvK9vFRU1Q.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 22</p></figure><p id="1cd8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，SVAE 优化的损失函数为:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pf"><img src="../Images/3f58a6d346383b5dc230808f57707b62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4C01LFP91Lwfv72Bl-Do_A.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 23</p></figure><h2 id="e470" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 5.3 —预测</strong></h2><p id="1abb" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">在这个 SVAE 模型中，建议分布引入了来自递归层的潜在变量的依赖性，这允许我们<strong class="kq ja">从先前的历史</strong>中恢复信息。给定用户历史 x_{u(1:t-1)}，我们可以使用等式 22 并设置 z = μ_{λ} (t)，在此基础上，我们可以通过π(z)来设计 x_{u(t)}的概率。</p><p id="3472" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了便于说明，下面给出了 SVAE 架构类的 PyTorch 代码:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="b1b9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">本文的另一个独特之处是评估协议的工作方式。作者将用户分为训练集、验证集和测试集；然后使用训练集中用户的全部历史来训练该模型。在评估过程中，对于验证/测试集中的每个用户，他们将<strong class="kq ja">按时间排序的</strong>用户历史分为两部分，即<em class="lk">折叠</em>和<em class="lk">折叠</em>拆分。</p><ul class=""><li id="2f3a" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated"><em class="lk">折叠</em>分割学习必要的表示并推荐商品。</li><li id="b1db" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">然后，使用精确度、召回率和标准化折扣累积收益等指标，使用用户历史的<em class="lk">展开</em>分割来评估这些项目。</li></ul><p id="36a2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于<a class="ae ll" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/SVAE-PyTorch" rel="noopener ugc nofollow" target="_blank">我的 PyTorch 实现</a>，我遵循作者提供的相同代码。</p><ul class=""><li id="1b65" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">SVAE 架构包括大小为 256 的嵌入层、具有 200 个单元的递归层(门控递归单元)、两个编码层(大小为 150 和 64)以及最后两个解码层(大小为 64 和 150)。</li><li id="65ae" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">VAE 的潜在因子的数量 K 被设置为 64。</li><li id="c332" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">该模型使用 Adam 进行优化，权重衰减设置为 0.01。</li></ul><h1 id="1720" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak"> 6 —令人尴尬的浅薄的自动编码器</strong></h1><p id="b63f" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">Harald Steck 的“<a class="ae ll" href="https://arxiv.org/abs/1905.03375" rel="noopener ugc nofollow" target="_blank">令人尴尬的浅薄的稀疏数据自动编码器</a>”是一个很吸引人的东西，我想带进这个讨论。这里的动机是，根据他的文献综述，与仅具有一个、两个或三个隐藏层的“深度”模型相比，具有大量隐藏层的<em class="lk">深度模型通常在协同过滤中获得排名准确度的显著提高</em> <strong class="kq ja"> <em class="lk">而不是</em> </strong> <em class="lk">。这与 NLP 或计算机视觉等其他领域形成了鲜明的对比。</em></p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/26d9972d7c2305a9fa560cec64dc8a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*2JIa4_h1y9wrpTdnazvxvQ.jpeg"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated"><em class="mq">Harald Steck——令人尴尬的浅薄的稀疏数据自动编码器(</em><a class="ae ll" href="https://arxiv.org/abs/1905.03375" rel="noopener ugc nofollow" target="_blank"><em class="mq">https://arxiv.org/abs/1905.03375</em></a><em class="mq">)</em></p></figure><h2 id="90d4" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 6.1 —型号</strong></h2><p id="0893" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated"><strong class="kq ja">令人尴尬的浅薄的自动编码器</strong> (ESAE)是一个没有隐藏层的线性模型<em class="lk">。(二进制)输入向量 X 向量表示用户已经交互的项目，ESAE 的目标是在输出层预测推荐给用户的最佳项目(如上图所示)。对于隐式反馈，X 中的值 1 指示用户与项目交互，而 X 中的值 0 指示没有观察到交互。</em></p><p id="ff0c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">项目-项目权重矩阵 B 表示 ESAE 的参数。这里，输入层中的项目与输出层中的项目的自相似性被忽略，因此 ESAE 可以在重建步骤中有效地进行概化。因此，该权重矩阵 B 的对角线被约束为 0 ( <strong class="kq ja"> diag(B) = 0 </strong>)。</p><p id="9f33" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于一个项目 j 和一个用户 u，我们想预测 S_{u，j}，其中 X_{u，.}指的是 u 行和 B_{。，j}指的是 j 列:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/f4f8365429a038d76558ba2d80702459.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*98ltqSFJsBUOhH4lUJgMpQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 24</p></figure><h2 id="0ce2" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 6.2 —目标函数</strong></h2><p id="e343" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">关于<strong class="kq ja"> diag(B) = 0，</strong> ESAE 具有以下用于学习权重 B 的凸目标:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/fecd6e32d2bf3df06de9c4fb95dec6ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*9pnu5hjLlD2_kFZRnbBGNg.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">等式 25</p></figure><p id="4493" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">以下是关于这个凸目标的重要注释:</p><ul class=""><li id="9b80" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">||.||表示<a class="ae ll" href="https://mathworld.wolfram.com/FrobeniusNorm.html" rel="noopener ugc nofollow" target="_blank"> Frobenius 范数</a>。数据 X 和预测得分 XB 之间的平方损失允许一个<strong class="kq ja">封闭形式的解决方案</strong>。</li><li id="8ae7" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">超参数λ是权重 b 的 L2 范数正则化</li><li id="fbb1" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">零对角线的约束有助于避免平凡解 B = I，其中 I 是单位矩阵。</li></ul><p id="14fc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在论文中，Harald 从等式 25 中的训练目标导出了一个封闭形式的解。他认为，传统的基于邻居的协同过滤方法是基于概念上不正确的项目-项目相似性矩阵，而 ESAE 框架利用了<strong class="kq ja">原则的</strong>邻居模型。我不会在这里重复数学推导，但是你应该看看论文的第 3.1 节来了解细节。</p><p id="b0d0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">值得注意的是，ESAE 的相似性矩阵是基于给定数据矩阵的<strong class="kq ja"> <em class="lk">逆</em> </strong>。结果，学习到的权重也可以是负的，因此模型可以学习项目之间的<strong class="kq ja">不相似性</strong>(除了相似性之外)。事实证明，这对于获得良好的<strong class="kq ja">分级准确性</strong>至关重要。此外，如果数据矩阵 X 中的用户数量足够大，则<strong class="kq ja">数据稀疏性</strong>问题(对于<em class="lk">每个</em>用户可能只有少量数据可用)不会影响估计权重矩阵 B 的不确定性。</p><h2 id="5f6c" class="oa ms iq bd mt ob oc dn mx od oe dp nb kx of og nd lb oh oi nf lf oj ok nh iw bi translated"><strong class="ak"> 6.3 —算法</strong></h2><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="370d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">上面给出了学习算法的 Python 代码。训练只需要项目-项目矩阵 G = X^T * X 作为输入，而不需要用户-项目矩阵 x。如果 g 的大小小于 x 的大小，这是非常有效的</p><p id="1aa5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于<a class="ae ll" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/ESAE-PyTorch" rel="noopener ugc nofollow" target="_blank">我的 PyTorch 实现</a>，我设置 L2 范数正则化超参数λ为 1000，学习率为 0.01，批量为 512。</p><h1 id="2707" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak">模型评估</strong></h1><p id="336f" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">您可以查看我在这个存储库中构建的所有六个基于自动编码器的推荐模型:<a class="ae ll" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments" rel="noopener ugc nofollow" target="_blank">https://github . com/khanhnamle 1994/transfer-rec/tree/master/auto encoders-Experiments</a>。</p><ul class=""><li id="5a89" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">数据集是<a class="ae ll" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/ml-1m" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> MovieLens 1M </strong> </a>，类似于我之前用<a class="ae ll" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Matrix-Factorization-Experiments" rel="noopener ugc nofollow" target="_blank">矩阵分解</a>和<a class="ae ll" href="https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments" rel="noopener ugc nofollow" target="_blank">多层感知器</a>做过的两个实验。目标是预测用户对一部电影的评价，其中评价在 1 到 5 之间。</li><li id="b4e9" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">对于 AutoRec 和 DeepRec 模型，评估指标是评级预测(回归)设置中的<strong class="kq ja">掩蔽均方根误差(RMSE) </strong>。</li><li id="6f0a" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">对于 CDAE、马尔特瓦、SVAE 和 ESAE 模型，评估指标是排名预测(分类)设置中的<strong class="kq ja">精度</strong>、<strong class="kq ja">召回</strong>和<strong class="kq ja">归一化贴现累积收益(NDCG) </strong>。如以上部分所述，这些模型使用隐式反馈数据，其中评级被二进制化为 0(小于等于 3)和 1(大于 3)。</li><li id="d92c" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">结果在<a class="ae ll" href="https://www.comet.ml/" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">彗星 ML </strong> </a>中被捕获。对于那些不熟悉的人来说，它是一个非常棒的工具，可以跟踪模型实验，并在一个仪表板中记录所有必要的指标。</li></ul><p id="a2da" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">结果表在我的回购的<a class="ae ll" href="https://github.com/khanhnamle1994/transfer-rec/blob/master/Autoencoders-Experiments/README.md" rel="noopener ugc nofollow" target="_blank">自述</a>的底部:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pj"><img src="../Images/95630b97ab1cc9a1b02cd377dd0f1481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8i08Hpd-AHp0XTd7jcm5w.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">模型评估</p></figure><p id="f2b5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于<strong class="kq ja">评级</strong>预测:</p><ul class=""><li id="0eff" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">AutoRec 比 DeepRec 性能更好:RMSE 更低，运行时间更短。</li><li id="0d66" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">这很令人惊讶，因为 DeepRec 是比 AutoRec 更深层次的架构。</li></ul><p id="b2ae" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于<strong class="kq ja">排名</strong>预测:</p><ul class=""><li id="bf57" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">SVAE 模型显然具有最好的结果；但是，也需要一个数量级的时间来训练。</li><li id="8597" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">其余三个模型之间:CDAE 的精度最高@100，ESAE 的召回率最高@100，NDCG@100，MultVAE 的运行时间最短。</li></ul><h1 id="22ba" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak">结论</strong></h1><p id="a1af" class="pw-post-body-paragraph ko kp iq kq b kr nj ka kt ku nk kd kw kx nl kz la lb nm ld le lf nn lh li lj ij bi translated">在这篇文章中，我讨论了自动编码器的具体细节以及它们在协同过滤中的应用。我还浏览了 6 篇使用自动编码器作为推荐框架的不同论文:(1) AutoRec，(2) DeepRec，(3)协作去噪自动编码器，(4)多项式变分自动编码器，(5)顺序变分自动编码器，以及(6)令人尴尬的浅薄自动编码器。</p><p id="0b32" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个领域出现了几个新兴的研究方向:</p><ul class=""><li id="81f8" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">当面对不同的推荐需求时，重要的是融入辅助信息来帮助理解用户和项目，以进一步提高推荐的性能。自动编码器处理异构数据源的能力为推荐具有非结构化数据(如文本、图像、音频和视频特征)的不同项目带来了巨大的机会。</li><li id="1b86" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">最近出现了许多基于自动编码器的有效的无监督学习技术:加权自动编码器、梯形变分自动编码器和离散变分自动编码器。使用这些新的自动编码器变体将有助于进一步提高推荐性能。</li><li id="f4d1" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated">除了协作过滤之外，还可以将自动编码器范例与基于内容的过滤和基于知识的推荐方法相集成。这些基本上是未得到充分探索的领域，有取得进展的潜力。</li></ul><p id="6c87" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">请继续关注本系列未来的博文，这些博文将探索为协同过滤而设计的不同建模架构。</p><h1 id="d1b3" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="119e" class="lm ln iq kq b kr nj ku nk kx pk lb pl lf pm lj lr ls lt lu bi translated"><a class="ae ll" href="https://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lk">自动编码器满足协同过滤</em> </a>。Suvash Sedhain、Aditya、Scott Sanner 和 Lexing Xie。2015 年 5 月。</li><li id="5c47" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" href="https://arxiv.org/abs/1708.01715" rel="noopener ugc nofollow" target="_blank"> <em class="lk">训练深度自动编码器进行协同过滤</em> </a>。奥莱克西·库切耶夫和鲍里斯·金斯伯格。2017 年 8 月。</li><li id="dacb" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" href="https://alicezheng.org/papers/wsdm16-cdae.pdf" rel="noopener ugc nofollow" target="_blank"><em class="lk">Top-N 推荐系统的协同去噪自动编码器</em> </a> <em class="lk">。吴耀、克里斯托弗·杜布瓦、爱丽丝·郑和马丁·埃斯特。2016 年 2 月。</em></li><li id="6ef2" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" href="https://arxiv.org/abs/1802.05814" rel="noopener ugc nofollow" target="_blank"> <em class="lk">用于协同过滤的变分自动编码器</em> </a>。Dawen Liang，Rahul G. Krishnan，Matthew D. Hoffman 和 Tony Jebara。2018 年 2 月。</li><li id="b859" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" href="https://arxiv.org/abs/1811.09975" rel="noopener ugc nofollow" target="_blank"> <em class="lk">用于协同过滤的序贯变分自动编码器</em> </a>。诺维恩·萨克德瓦、朱塞佩·曼科、埃托雷·里塔克和维克拉姆·迪普。2018 年 11 月。</li><li id="efa9" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" href="https://arxiv.org/abs/1905.03375" rel="noopener ugc nofollow" target="_blank"> <em class="lk">尴尬浅薄的稀疏数据自动编码器</em> </a> <em class="lk">。哈拉尔德·斯泰克。2019 年 5 月。</em></li></ul></div><div class="ab cl pn po hu pp" role="separator"><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps"/></div><div class="ij ik il im in"><p id="8332" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">如果你想关注我在推荐系统、深度学习和数据科学新闻方面的工作，你可以查看我的</em> <a class="ae ll" href="https://medium.com/@james_aka_yale" rel="noopener"> <em class="lk">中的</em> </a> <em class="lk">和</em><a class="ae ll" href="https://github.com/khanhnamle1994" rel="noopener ugc nofollow" target="_blank"><em class="lk">GitHub</em></a><em class="lk">，以及 https://jameskle.com/</em><a class="ae ll" href="https://jameskle.com/" rel="noopener ugc nofollow" target="_blank"><em class="lk"/></a><em class="lk">的其他项目。你也可以在</em> <a class="ae ll" href="https://twitter.com/le_james94" rel="noopener ugc nofollow" target="_blank"> <em class="lk">推特</em> </a> <em class="lk">，</em> <a class="ae ll" href="mailto:khanhle.1013@gmail.com" rel="noopener ugc nofollow" target="_blank"> <em class="lk">直接发邮件给我</em> </a> <em class="lk">，或者</em> <a class="ae ll" href="http://www.linkedin.com/in/khanhnamle94" rel="noopener ugc nofollow" target="_blank"> <em class="lk">在 LinkedIn </em> </a> <em class="lk">上找我。</em> <a class="ae ll" href="http://eepurl.com/deWjzb" rel="noopener ugc nofollow" target="_blank"> <em class="lk">注册我的简讯</em> </a> <em class="lk">就在你的收件箱里接收我对机器学习研究和行业的最新想法吧！</em></p></div></div>    
</body>
</html>