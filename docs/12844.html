<html>
<head>
<title>Sentiment Analysis of a book through Supervised Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于监督学习的书籍情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-analysis-of-a-book-through-supervised-learning-d56ba8946c64?source=collection_archive---------38-----------------------#2020-09-03">https://towardsdatascience.com/sentiment-analysis-of-a-book-through-supervised-learning-d56ba8946c64?source=collection_archive---------38-----------------------#2020-09-03</a></blockquote><div><div class="fc ik il im in io"/><div class="ip iq ir is it"><div class=""/><div class=""><h2 id="463d" class="pw-subtitle-paragraph jt iv iw bd b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk dk translated"><em class="kl">用 Python 和 scikit 分析一本书的情感的简单教程——学习</em></h2></div><figure class="kn ko kp kq gu kr gi gj paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gi gj km"><img src="../Images/89dddfb1e2eb57844427d17d947e042c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Z98vRWOufiKe4sQPiADRA.jpeg"/></div></div><p class="ky kz gk gi gj la lb bd b be z dk translated">图片来自<a class="ae lc" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5450708" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae lc" href="https://pixabay.com/users/jc_cards-13855161/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5450708" rel="noopener ugc nofollow" target="_blank">马雷克·斯图津斯基</a></p></figure><p id="4da5" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">在本教程中，我将解释如何通过基于支持向量机(SVM)的监督学习技术来计算一本书的情感。</p><p id="5ba7" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">本教程计算圣奥古斯丁忏悔录的情感分析，可以从<a class="ae lc" href="https://www.gutenberg.org/files/3296/3296-h/3296-h.htm" rel="noopener ugc nofollow" target="_blank">古腾堡项目页面</a>下载。这部杰作分为 13 本书(章)。我们将每本书存储在不同的文件中，名为 number.text(例如 1.txt 和 2.txt)。每个文件的每一行只包含一个句子。</p><p id="ed1f" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">jupyter 笔记本可以从我的 Github 资源库下载:<a class="ae lc" href="https://github.com/alod83/papers/tree/master/aiucd2021" rel="noopener ugc nofollow" target="_blank">https://github.com/alod83/papers/tree/master/aiucd2021</a></p><p id="c560" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">一本书的情感分析的类似实验可以在下面的链接找到:<a class="ae lc" rel="noopener" target="_blank" href="/sentiment-analysis-of-a-book-through-unsupervised-learning-df876563dd1b">https://towards data science . com/perspective-analysis-of-a-book-through-unsupervised-learning-df 876563 dd1b</a>。在这种情况下，我解释了如何利用无监督学习技术来执行情感分析。</p><h1 id="ba42" class="lz ma iw bd mb mc md me mf mg mh mi mj kc mk kd ml kf mm kg mn ki mo kj mp mq bi translated">入门指南</h1><p id="936e" class="pw-post-body-paragraph ld le iw lf b lg mr jx li lj ms ka ll lm mt lo lp lq mu ls lt lu mv lw lx ly ip bi translated">监督学习需要一些带注释的文本来训练模型。因此，第一步包括读取注释文件并将其存储到数据帧中。注释文件包含每个句子的相关分数，该分数是正数、负数或空值。</p><pre class="kn ko kp kq gu mw mx my mz aw na bi"><span id="f4d0" class="nb ma iw mx b gz nc nd l ne nf">import pandas as pd</span><span id="5384" class="nb ma iw mx b gz ng nd l ne nf">df = pd.read_csv('sources/annotations.csv')<br/>df</span></pre><figure class="kn ko kp kq gu kr gi gj paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gi gj nh"><img src="../Images/028bf91ec54dc673e73c7d7378a68a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K1CSov2cZlxDs4GIEqz_2w.png"/></div></div></figure><p id="2073" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">我们可以计算一些关于注释的统计数据，比如正面、负面和中性分数的数量，以及注释的总数。我们可以使用应用于数据帧的<code class="fe ni nj nk mx b">count()</code>方法。</p><pre class="kn ko kp kq gu mw mx my mz aw na bi"><span id="25c6" class="nb ma iw mx b gz nc nd l ne nf">df.count()<br/>df[df['score'] &gt; 0].count()<br/>df[df['score'] &lt; 0].count()<br/>df[df['score'] == 0].count()</span></pre><h1 id="0ca9" class="lz ma iw bd mb mc md me mf mg mh mi mj kc mk kd ml kf mm kg mn ki mo kj mp mq bi translated">准备训练集和测试集</h1><p id="3652" class="pw-post-body-paragraph ld le iw lf b lg mr jx li lj ms ka ll lm mt lo lp lq mu ls lt lu mv lw lx ly ip bi translated">为了计算每个句子的情感，我们将利用监督学习技术，该技术利用二进制分类模型。该模型将一个句子作为输入，根据句子是否被正面评价，返回 1 或 0。因为我们的模型是二进制的，所以我们必须删除所有带有中性分数的注释。</p><pre class="kn ko kp kq gu mw mx my mz aw na bi"><span id="6494" class="nb ma iw mx b gz nc nd l ne nf">import numpy as np<br/># Remove any 'neutral' ratings equal to 0<br/>df = df[df['score'] != 0]</span></pre><p id="efef" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">现在我们可以向 dataframe 添加一列，称为<code class="fe ni nj nk mx b">Positively Rated</code>，包含 1 或 0，这取决于一个正的或负的分数。我们使用<code class="fe ni nj nk mx b">where()</code>方法为数据帧的这个新列分配适当的值。</p><pre class="kn ko kp kq gu mw mx my mz aw na bi"><span id="ca6b" class="nb ma iw mx b gz nc nd l ne nf">df['Positively Rated'] = np.where(df['score'] &gt; 0, 1, 0)</span></pre><h1 id="c5ce" class="lz ma iw bd mb mc md me mf mg mh mi mj kc mk kd ml kf mm kg mn ki mo kj mp mq bi translated">计算情绪</h1><p id="0ec5" class="pw-post-body-paragraph ld le iw lf b lg mr jx li lj ms ka ll lm mt lo lp lq mu ls lt lu mv lw lx ly ip bi translated">我们定义一个辅助函数，称为<code class="fe ni nj nk mx b">calculate_indexes()</code>，它接收监督学习模型(将在后面描述)和<code class="fe ni nj nk mx b">CountVectorizer</code>向量作为输入，如后面所述。</p><p id="9f1b" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">在该函数中，我们通过<code class="fe ni nj nk mx b">open()</code>函数打开对应于每本书的文件，我们通过函数<code class="fe ni nj nk mx b">file.readlines()</code>读取所有行，并通过将<code class="fe ni nj nk mx b">predict()</code>函数应用于模型来计算每一行的分数。</p><p id="54d4" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">那么，我们可以定义三个指数来计算一本书的情绪:正面情绪指数(pi)、负面情绪指数(ni)和中性情绪指数(nui)。一本书的圆周率对应于一本书中的肯定句的数量除以该书的总句子数量。同样，我们可以计算一本书的 ni 和 nui。</p><pre class="kn ko kp kq gu mw mx my mz aw na bi"><span id="bf54" class="nb ma iw mx b gz nc nd l ne nf">def calculte_indexes(model,vect):<br/>    pos_index = []<br/>    neg_index = []<br/>    neutral_index = []<br/>    all_text = ""<br/>    for book in range(1,14):<br/>        file = open('sources/' + str(book) + '.txt')<br/>        lines = file.readlines()<br/>        pos = 0<br/>        neg = 0<br/>    <br/>    <br/>        for line in lines:<br/>            score = model.predict(vect.transform([line]))<br/>            <br/>            if score == 1:<br/>                pos += 1<br/>            else:<br/>                neg += 1<br/>            all_text += ' ' + line.lower() <br/>              <br/>        n = len(lines)<br/>        pos_index.append(pos / n)<br/>        neg_index.append(neg / n)<br/>    <br/>    return pos_index,neg_index</span></pre><p id="4eb9" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">现在我们可以训练算法了。我们定义了两种不同的情况:第一种情况我们不考虑 ngrams，第二种情况我们考虑。我们定义了一个名为<code class="fe ni nj nk mx b">train_algorithm()</code>的函数，可以通过指定 ngrams 的用法来调用它。</p><p id="389c" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">在函数中，我们首先通过名为<code class="fe ni nj nk mx b">train_test_split()</code>的<code class="fe ni nj nk mx b">scikit-learn</code>函数将数据集分为训练集和测试集两部分。训练集将用于训练算法，测试集将用于测试算法的性能。</p><pre class="kn ko kp kq gu mw mx my mz aw na bi"><span id="b857" class="nb ma iw mx b gz nc nd l ne nf">X_train, X_test, y_train, y_test = train_test_split(df['sentence'], <br/>                            df['Positively Rated'], random_state=0)</span></pre><p id="9a0d" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">然后，我们构建标记计数矩阵，即包含每个句子哪些标记可用的矩阵。这可以通过类<code class="fe ni nj nk mx b">CountVectorizer</code>来完成，该类接收要考虑的 ngrams 数量作为输入。在本教程中，只考虑两个 ngrams。我们只考虑最小文档频率等于 5 的标记。</p><pre class="kn ko kp kq gu mw mx my mz aw na bi"><span id="c16f" class="nb ma iw mx b gz nc nd l ne nf">vect = CountVectorizer(min_df=5).fit(X_train)</span></pre><p id="50e2" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">然后我们可以构建模型:我们使用包含在<code class="fe ni nj nk mx b">scikit-learn</code>中的<code class="fe ni nj nk mx b">LinearSVC()</code>类，并用训练集<code class="fe ni nj nk mx b">model.fit(X_train_vectorized, y_train)</code>训练它。</p><pre class="kn ko kp kq gu mw mx my mz aw na bi"><span id="5266" class="nb ma iw mx b gz nc nd l ne nf">model = LinearSVC()<br/>model.fit(X_train_vectorized, y_train)</span></pre><p id="7350" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">最后，我们通过预测测试集的输出并将结果与测试集中包含的真实输出进行比较来测试算法的性能。作为指标，我们测量 AUC，但我们也可以计算其他指标。</p><pre class="kn ko kp kq gu mw mx my mz aw na bi"><span id="bc92" class="nb ma iw mx b gz nc nd l ne nf">predictions = model.predict(vect.transform(X_test))<br/>print('AUC: ', roc_auc_score(y_test, predictions))</span></pre><p id="83a2" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">一旦训练好模型，我们就可以通过<code class="fe ni nj nk mx b">calculte_indexes()</code>函数来计算指标。最后，我们绘制结果。</p><p id="1e89" class="pw-post-body-paragraph ld le iw lf b lg lh jx li lj lk ka ll lm ln lo lp lq lr ls lt lu lv lw lx ly ip bi translated">这里是功能<code class="fe ni nj nk mx b">train_algorithm()</code>的全部代码。</p><pre class="kn ko kp kq gu mw mx my mz aw na bi"><span id="4d55" class="nb ma iw mx b gz nc nd l ne nf">from sklearn.model_selection import train_test_split<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.svm import LinearSVC<br/>from sklearn.metrics import roc_auc_score<br/>import matplotlib.pyplot as plt</span><span id="85ea" class="nb ma iw mx b gz ng nd l ne nf">def train_algorithm(df, ngrams=False):<br/>    # Split data into training and test sets<br/>    X_train, X_test, y_train, y_test = train_test_split(df['sentence'], <br/>                                                    df['Positively Rated'], <br/>                                                    random_state=0)<br/>    <br/>    # Fit the CountVectorizer to the training data<br/>    vect = CountVectorizer(min_df=5).fit(X_train)<br/>    if ngrams:<br/>        # Fit the CountVectorizer to the training data specifiying a minimum <br/>        # document frequency of 5 and extracting 1-grams and 2-grams<br/>        vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)<br/>        print('NGRAMS')<br/>    else:<br/>        print('WITHOUT N-GRAMS')<br/>    # transform the documents in the training data to a document-term matrix<br/>    X_train_vectorized = vect.transform(X_train)<br/>    # Train the model<br/>    model = LinearSVC()<br/>    model.fit(X_train_vectorized, y_train)<br/>    <br/>    # Predict the transformed test documents<br/>    predictions = model.predict(vect.transform(X_test))</span><span id="700f" class="nb ma iw mx b gz ng nd l ne nf">print('AUC: ', roc_auc_score(y_test, predictions))<br/>    <br/>    pos_index, neg_index = calculte_indexes(model,vect)<br/>    <br/>    X = np.arange(1,14)<br/>    plt.plot(X,pos_index,'-.',label='pos')<br/>    plt.plot(X,neg_index, '--',label='neg')<br/>    #plt.plot(X,neutral_index,'-',label='neu')<br/>    plt.legend()<br/>    plt.xticks(X)<br/>    plt.xlabel('Libri')<br/>    plt.ylabel('Indici')<br/>    plt.grid()<br/>    if ngrams:<br/>        plt.savefig('plots/svm-ngram-bsi.png')<br/>    else:<br/>        plt.savefig('plots/svm-1gram-bsi.png')<br/>    plt.show()</span></pre><h1 id="d8c1" class="lz ma iw bd mb mc md me mf mg mh mi mj kc mk kd ml kf mm kg mn ki mo kj mp mq bi translated">进行实验</h1><p id="2623" class="pw-post-body-paragraph ld le iw lf b lg mr jx li lj ms ka ll lm mt lo lp lq mu ls lt lu mv lw lx ly ip bi translated">现在我们可以在启用和禁用 ngrams 的情况下运行实验。</p><pre class="kn ko kp kq gu mw mx my mz aw na bi"><span id="7d54" class="nb ma iw mx b gz nc nd l ne nf">train_algorithm(df, ngrams=False)<br/>train_algorithm(df, ngrams=True)</span></pre><figure class="kn ko kp kq gu kr gi gj paragraph-image"><div class="gi gj nl"><img src="../Images/4826e4b3b8b3019e02730db356e339b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*N2Xz-JNZdIt17LWOwTYLog.png"/></div><p class="ky kz gk gi gj la lb bd b be z dk translated">没有 n-grams 的实验</p></figure><figure class="kn ko kp kq gu kr gi gj paragraph-image"><div class="gi gj nl"><img src="../Images/eb39df96dad73bc7819a83b1c9f82508.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*MUtsfLH2lQc0JNE5fqDFFA.png"/></div><p class="ky kz gk gi gj la lb bd b be z dk translated">用 n-grams 做实验</p></figure><h1 id="4918" class="lz ma iw bd mb mc md me mf mg mh mi mj kc mk kd ml kf mm kg mn ki mo kj mp mq bi translated">吸取的教训</h1><p id="1f17" class="pw-post-body-paragraph ld le iw lf b lg mr jx li lj ms ka ll lm mt lo lp lq mu ls lt lu mv lw lx ly ip bi translated">在本教程中，我向你展示了如何通过监督学习技术来计算一本书每一章的情感:</p><ul class=""><li id="f3ca" class="nm nn iw lf b lg lh lj lk lm no lq np lu nq ly nr ns nt nu bi translated">监督学习需要一些训练集，即一些手动标注的数据。因此，在开始玩模型之前，您应该做注释数据的(小)部分这一枯燥的任务。训练集越大，算法的性能越好；</li><li id="ad9d" class="nm nn iw lf b lg nv lj nw lm nx lq ny lu nz ly nr ns nt nu bi translated">要计算一个句子的情感，你应该使用一个(二进制)分类算法，比如支持向量机(SVM)。scikit-learn 库提供了许多分类算法；</li><li id="8563" class="nm nn iw lf b lg nv lj nw lm nx lq ny lu nz ly nr ns nt nu bi translated">一旦选择了模型，就可以用训练集对其进行训练；</li><li id="5781" class="nm nn iw lf b lg nv lj nw lm nx lq ny lu nz ly nr ns nt nu bi translated">别忘了测试模型的性能:)</li></ul></div></div>    
</body>
</html>