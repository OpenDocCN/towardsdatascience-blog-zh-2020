<html>
<head>
<title>The Evolution of Trees-Based Classification Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于树的分类模型的演变</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-evolution-of-trees-based-classification-models-cb40912c8b35?source=collection_archive---------25-----------------------#2020-09-12">https://towardsdatascience.com/the-evolution-of-trees-based-classification-models-cb40912c8b35?source=collection_archive---------25-----------------------#2020-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="6786" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">充分利用决策树</p><p id="3a92" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko"> &lt;你可以在这里找到用于演示</em><a class="ae kp" href="https://github.com/kevinclee26/tree-based_models" rel="noopener ugc nofollow" target="_blank"><em class="ko"/></a><em class="ko">&gt;</em>的代码</p><p id="2c9b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">基于树的分类模型</strong>是一种<strong class="js iu">监督的</strong>机器学习算法，它使用一系列条件语句将训练数据划分为子集。每一次连续的分裂都会增加模型的复杂性，这可以用来进行预测。最终结果模型可以被视为描述数据集的逻辑测试路线图。决策树对于中小型数据集很流行，因为它们易于实现，甚至更易于解释。然而，它们并非没有挑战。在本文中，我们将强调基于树的分类模型的优点和缺点，以及克服它们的进展。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi kq"><img src="../Images/1a58505210bdbbdaae9957d0df312205.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-R26wUt-Af-qxmpwxVnIAQ.jpeg"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">照片由<a class="ae kp" href="https://unsplash.com/@chrislejarazu" rel="noopener ugc nofollow" target="_blank"> chrilejarazu </a>在<a class="ae kp" href="https://unsplash.com/photos/08wxrVv5rp8" rel="noopener ugc nofollow" target="_blank">上拍摄</a></p></figure><h2 id="485f" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">决策树的构造</h2><p id="1d41" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">以下示例描述了一个只有两个要素和两个类的样本数据集(左)。决策树算法从<strong class="js iu">根</strong>节点中的所有 15 个数据点开始。该节点被称为<strong class="js iu">不纯的</strong>，因为它包含异构数据的混合。在每个决策节点，该算法在杂质减少最多的<strong class="js iu">目标特征</strong>上分割数据集，最终产生具有同质数据的<strong class="js iu">叶节点</strong> / <strong class="js iu">终端节点</strong>(右图)。有几个比较流行的衡量杂质的指标——<a class="ae kp" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">基尼杂质</strong> </a>和<a class="ae kp" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">熵</strong> </a>。虽然不同的决策树实现在计算中使用的杂质度量会有所不同，但总的概念是相同的，并且结果在实践中很少发生实质性变化。划分过程继续进行，直到不能再进行进一步的分离，例如，模型希望达到每个叶节点尽可能快地变成纯的状态。在进行预测时，新的数据点会遍历一系列决策节点以得出决定。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ma"><img src="../Images/4ae746ecccad12f22582945a04d1823c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BXFmkXxAZqIsJb8o0jaYHg.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">简单决策树的可视化</p></figure><h2 id="a2dd" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">强项</h2><ul class=""><li id="45db" class="mf mg it js b jt lv jx lw kb mh kf mi kj mj kn mk ml mm mn bi translated">它们是直观的和容易理解的的<strong class="js iu">，甚至对于非分析背景的人来说也是如此。</strong></li><li id="da2c" class="mf mg it js b jt mo jx mp kb mq kf mr kj ms kn mk ml mm mn bi translated">决策树是一种<strong class="js iu">非参数</strong>方法，不要求数据集遵循正态分布。</li><li id="5208" class="mf mg it js b jt mo jx mp kb mq kf mr kj ms kn mk ml mm mn bi translated">它们能够容忍数据质量问题和异常值，例如，它们<strong class="js iu">需要较少的数据准备</strong>，如实施前的缩放和标准化。此外，它适用于分类变量和连续变量。</li><li id="7a84" class="mf mg it js b jt mo jx mp kb mq kf mr kj ms kn mk ml mm mn bi translated">它们可以在数据探索阶段用于快速<strong class="js iu">识别重要变量</strong>。</li></ul><h2 id="73a9" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated"><strong class="ak">挑战</strong></h2><ul class=""><li id="048d" class="mf mg it js b jt lv jx lw kb mh kf mi kj mj kn mk ml mm mn bi translated">决策树容易出现<strong class="js iu">过拟合</strong>，当函数过于接近训练数据时就会出现这种情况(参见<a class="ae kp" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">偏差-方差权衡</strong> </a>)。当决策树模型学习到训练数据中的粒度细节和噪声，以至于削弱了它对新数据进行预测的能力时，就会出现这种情况。创建一个过于复杂的模型<strong class="js iu">有可能</strong>对以前没有见过的数据做出糟糕的预测。</li><li id="bac7" class="mf mg it js b jt mo jx mp kb mq kf mr kj ms kn mk ml mm mn bi translated">决策树遭受高<strong class="js iu">方差</strong>。如果数据集很小，根据训练和测试样本的拆分方式，结果可能会有很大不同。</li></ul><h1 id="976b" class="mt ld it bd le mu mv mw lh mx my mz lk na nb nc ln nd ne nf lq ng nh ni lt nj bi translated">进化</h1><p id="ad44" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">最近，为了进一步挖掘基于树的模型的潜力，已经进行了大量的改进和验证。以下流程记录了进度:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nk"><img src="../Images/4841083d8cc645b79940b642531b4783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9661c0n6xRAlc5gGubNN_g.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">基于树的模型的演变</p></figure><h2 id="3f9b" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">提前停止(预修剪)</h2><p id="4739" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">有几种方法可以减少过度拟合，防止模型学习过于具体和复杂的模式。</p><ul class=""><li id="756a" class="mf mg it js b jt ju jx jy kb nl kf nm kj nn kn mk ml mm mn bi translated">节点分割的最小样本</li><li id="222a" class="mf mg it js b jt mo jx mp kb mq kf mr kj ms kn mk ml mm mn bi translated">树的最大深度</li><li id="e075" class="mf mg it js b jt mo jx mp kb mq kf mr kj ms kn mk ml mm mn bi translated">分割时要考虑的最大特征数</li></ul><p id="78fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为很难提前知道什么时候停止种树。可能需要一些迭代来微调这些<strong class="js iu">超参数</strong>。建议在训练时可视化树，从较低的 max_depth 开始，迭代增加。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi no"><img src="../Images/3607797ec9a987ef408ad59e48d6c8da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Qi-iSGc0k5x5xsveQLJLQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">使用 CV 对包含 20 个特征的样本数据集进行测试</p></figure><h2 id="e08d" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">修剪(修剪后)</h2><p id="67c9" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">修剪通过删除对模型的预测能力没有好处的叶节点来进行。这是简化模型和防止过度拟合的另一种方法。在现实中，一个完全成长的决策树可能有太多的分支，最终成为冗余的。剪枝通常是在模型建立后，通过检查模型在验证或测试数据集上的性能来完成的。通过删除对性能产生最小负面影响的节点，这一过程被称为<strong class="js iu">成本复杂性修剪，</strong> it <strong class="js iu">降低了复杂性</strong>，并允许模型<strong class="js iu">更好地概括</strong>。</p><h2 id="8253" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">引导聚集</h2><p id="fc99" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">Bagging 是一种<strong class="js iu">集成</strong>技术，用于<strong class="js iu">通过考虑来自多个决策树模型的结果来减少预测的方差</strong>，这些决策树模型是在同一数据集的不同<strong class="js iu">子样本</strong>上训练的。当数据大小有限制时，这尤其有用。根据问题，使用平均值、中值或众数组合所有模型的预测。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi np"><img src="../Images/de0e0309c769aedb378e0eddcf7a2b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l20SO_wl2nxL6FYb3BXBFQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">使用 CV 对包含 20 个特征的样本数据集进行测试</p></figure><h2 id="5d98" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">随机森林</h2><p id="90c4" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">随机森林是一种集成学习方法，使用不同的<strong class="js iu">特征子集</strong>构建多个树模型，有或没有采样(即引导)。它可以有效地处理具有许多变量的<strong class="js iu">高维</strong>数据集，因为只有一个子集用于构建单独的树。限制每个树模型的特征数量的直觉是去除它们之间的<strong class="js iu">相关性</strong>，这发生在强预测器被决策节点一致使用时。高度相关模型的协作并不能有效减少结果的差异。随机森林很受欢迎，因为它是通用的，并且可以以高精度快速训练。</p><blockquote class="nq nr ns"><p id="9e2a" class="jq jr ko js b jt ju jv jw jx jy jz ka nt kc kd ke nu kg kh ki nv kk kl km kn im bi translated">值得注意的是，这种方法通常用于理解数据集和确定变量的重要性，因为它与解决问题有关——排除有价值的特征会导致错误增加。</p></blockquote><h2 id="727c" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated"><strong class="ak">助推</strong></h2><p id="522f" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">Boosting 是另一种类型的集成学习，<strong class="js iu">组合弱学习器</strong>以实现改进的模型性能。弱学习者是预测相对较差的简单模型。boosting 的概念是按顺序训练模型，每次都试图比以前更好地拟合。一种被称为<strong class="js iu">自适应增强</strong> (AdaBoost)的方法基于先前的结果修改数据点的权重。对于模型构建的每个后续实例，正确分类的数据点被给予较低的权重，而错误分类的数据点被给予较高的权重。较高的权重指导模型学习这些数据点的细节。最终，所有的模型都有助于做出预测。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nw"><img src="../Images/94a3df2600795736faf934266f850c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XngjA8ia2NvArmRH58lP5A.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">装袋与增压</p></figure><h2 id="d233" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">梯度增强(和 XGBoost)</h2><p id="5f88" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">梯度增压方法更加复杂。梯度增强不是在每次模型构建迭代中调整权重，而是将后续模型与前一个模型的<strong class="js iu">残差</strong>相匹配。这种方法有助于树木在表现不好的地方逐渐得到改善。换句话说，它迭代地提高了单棵树的精度，从而提高了整体模型的性能。梯度提升受众多<strong class="js iu">调谐参数</strong>的影响，必须仔细考虑这些参数。当数据集中的关系<strong class="js iu">高度复杂</strong>和<strong class="js iu">非线性</strong>时有效。</p><p id="0b51" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">e<strong class="js iu">x</strong>treme<strong class="js iu">G</strong>radient<strong class="js iu">Boost</strong>ing，或简称为 XGBoost，是标准梯度增强方法的一种实现，只是增加了一些内容。首先，它使<a class="ae kp" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)#Regularization_in_statistics_and_machine_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">正则化</strong> </a>成为可能，这进一步有助于减少过度拟合。XGBoost 的开发目标是优化计算性能。由于梯度推进按顺序训练模型，因此实施起来可能会很慢。XGBoost 的一些显著特性包括<strong class="js iu">并行化</strong>、<strong class="js iu">分布式计算</strong>、<strong class="js iu">核外计算</strong>和<strong class="js iu">缓存优化</strong>。</p><h2 id="ee58" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">摘要</h2><p id="651e" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">在本文中，我们回顾了一些宽泛的术语和技术，以改进基于树的模型。基于树的模型因其直观性而广受欢迎。理解这些机制将有助于创建基线模型。也就是说，没有免费的午餐——即使使用这些技术，调整模型仍然是一个迭代过程，以便充分优化模型性能。</p></div></div>    
</body>
</html>