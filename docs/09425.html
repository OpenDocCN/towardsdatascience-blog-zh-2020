<html>
<head>
<title>Stock predictions with state-of-the-art Transformer and Time Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用最先进的转换器和时间嵌入进行股票预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6?source=collection_archive---------0-----------------------#2020-07-06">https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6?source=collection_archive---------0-----------------------#2020-07-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="02fb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">给交易机器人输入最新的股票预测</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ad5cf455db944de8815f4db9039e401a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PmOKzTxjrbRKqrQxh1jVTg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@morningbrew?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">晨酿</a>在<a class="ae ky" href="/t/technology?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="f972" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">在我之前的文章中，我分享了我第一次预测股票价格的研究结果，这些结果随后将被用作深度学习交易机器人的输入。在将我的数据集升级到数以千计的股票报价机，相当于几乎<strong class="lb iu">1tb 的股票价格历史和新闻文章</strong>的同时，我开始意识到我最初使用由<strong class="lb iu">LSTM</strong>(<strong class="lb iu">L</strong>ONG-<strong class="lb iu">S</strong>short<strong class="lb iu">T</strong>erm<strong class="lb iu">M</strong>记忆模型和<strong class="lb iu"> CNN </strong> ( <strong class="lb iu"> C </strong>组成的神经网络的方法因此，为了克服这些限制，我必须实现一个专用于<strong class="lb iu">股票</strong>时间序列<strong class="lb iu">的<strong class="lb iu">转换器</strong>。</strong></span></p><p id="cf04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">近年来，变压器因其出色的性能而广受欢迎。将<strong class="lb iu">自我关注</strong>机制、<strong class="lb iu">并行化</strong>和<strong class="lb iu">位置编码</strong>结合在一起，在处理需要语义特征提取和大型数据集的任务时，通常比经典的 LSTM 和 CNN 模型更具优势[1]。</p><p id="1f4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我无法找到一个简单的转换器实现，它是为具有<strong class="lb iu">多种特征的时间序列定制的，例如，我们股票数据的(开盘、盘高、盘低、收盘、成交量)</strong>特征，所以我必须自己实现它。在这篇文章中，我将分享我对股票数据的<strong class="lb iu">转换架构</strong>，以及什么是<strong class="lb iu">时间嵌入</strong>，以及为什么必须将它们与时间序列结合使用。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="5255" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">数据</h1><p id="0194" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">出于本文的解释目的，我们将使用 IBM 股票价格历史作为 1tb 股票数据集的简化版本。尽管如此，您可以轻松地将本文中的代码应用到更大的数据集。IBM 数据集从 1962 年 1 月 2 日开始，到 2020 年 5 月 24 日结束，总共包含<strong class="lb iu"> 14699 个交易日</strong>。此外，每个交易日，我们都有 IBM 股票的<strong class="lb iu">开盘价</strong>、<strong class="lb iu">高开</strong>、<strong class="lb iu">低开</strong>和<strong class="lb iu">收盘价</strong>以及交易量<strong class="lb iu">、</strong>、<strong class="lb iu">、</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/7514f8a22d34aea6bcabe127d5cb49d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BuiGmxQDNkkC-nanxJIAbg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">IBM 的每日收盘价和成交量</p></figure><h2 id="d4a9" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">数据准备</h2><p id="838c" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">价格和交易量特征被转换成每日<strong class="lb iu">股票收益</strong>和每日<strong class="lb iu">交易量变化</strong>，应用<strong class="lb iu">最小-最大标准化</strong>，并且时间序列被分成训练、验证和测试集。将股票价格和交易量转换为每日变化率增加了数据集的稳定性。因此，模型从我们的数据集中获得的知识对未来预测具有更高的有效性。这里概述了转换后的数据是什么样子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/7cdbe9a79676919d376c72c26a4e815f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q8Yjqlanwboug-NYuQC20A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">IBM 的每日收盘价回报和成交量变化</p></figure><p id="4b42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，训练集、验证集和测试集被分成长度为 128 天的单独序列。对于每个序列日，有 4 个价格特征(开盘价、盘高、盘低、收盘价)和交易量特征，每天有 5 个特征。在单个训练步骤中，我们的 Transformer 模型将接收 32 个序列<strong class="lb iu"> (batch_size = 32) </strong>，它们是 128 天长的<strong class="lb iu"> (seq_len=128) </strong>，并且每天有<strong class="lb iu"> 5 个特征</strong>作为输入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/30418a4cd157c29b1ac2af461c6f5604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*0avmJsEmcH4l55vyc9Usmw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型输入矩阵的大小</p></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="4cfe" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">时间嵌入</h1><p id="7934" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">作为 Transformer 实现的第一步，我们必须考虑如何将隐藏在股票价格中的时间概念编码到我们的模型中。</p><p id="962f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在处理时间序列数据时，时间是一个基本特征。然而，当使用转换器处理时间序列/顺序数据时，序列通过转换器架构一次转发，这使得提取时间/顺序依赖性变得困难。因此，与自然语言数据结合使用的转换器倾向于<strong class="lb iu">利用位置编码</strong>为模型提供词序的<strong class="lb iu">概念。详细地说，位置编码是单词的值及其在句子中的位置的表示，允许转换器获得关于句子结构和单词相关性的知识。一个位置编码的例子可以在<strong class="lb iu">BERT</strong><a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">【2】</a><strong class="lb iu"/>模型的引擎盖下找到，该模型已经为许多语言任务实现了最先进的性能。</strong></p><p id="e675" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，一个<strong class="lb iu">转换器在处理我们的股票价格时需要一个时间概念</strong>。如果没有时间嵌入，我们的转换器将不会收到任何关于股票价格时间顺序的信息。因此，2020 年的股票<strong class="lb iu">价格对未来价格</strong>预测的影响与 1990 年的<strong class="lb iu">价格相同。当然，这将是可笑的。</strong></p><h2 id="6c1b" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">时间 2 向量</h2><p id="a083" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">为了克服转换器的时间差异，我们将实现论文<strong class="lb iu"> Time2Vec:学习时间的向量表示</strong><a class="ae ky" href="https://arxiv.org/pdf/1907.05321.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a><strong class="lb iu">中描述的方法。</strong>论文作者提出了“<em class="nx">一种与模型无关的时间向量表示，称为</em><strong class="lb iu"><em class="nx">time 2 vec</em></strong><strong class="lb iu"><em class="nx">。</em> </strong>你可以把向量表示想象成普通的嵌入层<strong class="lb iu"> </strong>可以添加到神经网络架构中，以提高模型的性能。</p><p id="7779" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将这篇论文归结为它的要点，有两个主要观点需要考虑。<strong class="lb iu">首先</strong>，作者发现有意义的时间表示必须包括<strong class="lb iu">周期性和非周期性模式</strong>。周期性模式的一个例子是随不同季节而变化的天气。相反，非周期性模式的一个例子是一种疾病，随着患者年龄的增长，这种疾病发生的概率很高。</p><p id="aa65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">其次</strong>，时间表示应具有对时间缩放的<strong class="lb iu">不变性，</strong>意味着时间表示不受不同时间增量(如天、小时或秒)和长时间范围的影响。</p><p id="f343" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结合周期和非周期模式的思想以及对时间缩放的不变性，我们由下面的数学定义给出。别担心，这比看起来容易，我会详细解释的。😉</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/2e32c3e9de3646ad2ceff089b008a37a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tfLhh5cvSReS4Nejy3nIUg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">时间向量的数学表示— Time2Vec:学习时间的向量表示[3]</p></figure><p id="1af0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">时间向量/表示法<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">t2v</em></strong></code>由两部分组成，其中<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">ωᵢτ + φᵢ</em></strong></code>表示时间向量的非周期性/线性特征，<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">F(ωᵢτ + φᵢ)</em></strong></code>表示时间向量的周期性特征。</p><p id="d959" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用一种更简单的方式重写<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">t2v(τ) = ωᵢτ + φᵢ</em></strong></code>，新版本<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">y = mᵢx + bᵢ</em></strong></code> <strong class="lb iu"> <em class="nx"> </em> </strong>应该看起来很熟悉，因为它是你从高中就知道的线性函数的普通版本。<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">ωᵢτ + φᵢ</em></strong></code>中的<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">ω</em></strong></code>是定义我们的时间序列<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">τ</em></strong></code>的斜率的矩阵，<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">φ</em></strong></code> <strong class="lb iu"> </strong>简单来说就是定义我们的时间序列<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">τ</em></strong></code> <strong class="lb iu"> </strong>与 y 轴的交点的矩阵。因此，<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">ωᵢτ + φᵢ</em></strong></code>无非是一个线性函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/2c3230d79260082cbb0b7c7bd4e1b776.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*JozJ0ku1VI5a0SFu8Vo6YQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">非周期时间特征的 2D 表示</p></figure><p id="26cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二分量<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">F(ωᵢτ + φᵢ)</em></strong></code>代表时间向量的周期性特征。就像之前我们又有了线性项<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">ωᵢτ + φᵢ</em></strong></code>，然而，这次线性函数被包装在一个额外的函数<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">F()</em></strong></code>中。作者试验了不同的函数来最好地描述周期关系(sigmoid、tanh、ReLU、mod、triangle 等。).最终，一个<strong class="lb iu">正弦函数</strong>取得了最好和最稳定的性能(余弦函数取得了类似的结果)。当组合线性函数<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">ωᵢτ + φᵢ</em></strong></code>和正弦函数<strong class="lb iu">时，2D 表示如下。<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">φ</em></strong></code> <strong class="lb iu"> <em class="nx"> </em> </strong>沿 x 轴移动正弦函数，<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">ω</em></strong></code> <strong class="lb iu"> <em class="nx"> </em> </strong>决定正弦函数的波长。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/35bbdec15692c779cc33b7f090d8f2b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*DSfTYJ_lGG_O6f6Q57Bk6w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">周期性时间特征的 2D 表示</p></figure><p id="a495" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一下 LSTM 网络的精度与时间向量(Time2vec)的不同非线性函数的组合是如何变化的。我们可以清楚地看到，<strong class="lb iu"> ReLU 函数执行</strong>最差<strong class="lb iu">最差</strong>，相比之下，<strong class="lb iu">正弦函数优于</strong>所有其他非线性函数。ReLU 函数具有如此不令人满意的结果的原因是，ReLU 函数对于时间缩放不是不变的。函数对时间缩放的不变量越高，性能越好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/9cbce1e280e1228f5b8173bbe5bb469a.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*sRvdiHNuNzFQ-HV2n2_xmg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">非线性函数的性能比较— Time2Vec:学习时间的向量表示[3]</p></figure><h2 id="acba" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">时间 2 矢量性能改进</h2><p id="5cea" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在我们开始实现时间嵌入之前，让我们看看普通 LSTM 网络(蓝色)和 LSTM+Time2Vec 网络(红色)的性能差异。正如您所看到的，建议的时间向量不会导致多个数据集的性能下降，并且几乎总是会提高模型的性能。有了这些认识，我们就可以继续实现了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/b6dadf0adedbd4bfcacfe21c8b14a212.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MsV5IERR4MAQyqBKTtTOiw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有和没有时间向量的 LSTM 网络的性能比较— Time2Vec:学习时间的向量表示[3]</p></figure><h2 id="d462" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">Time2Vector Keras 实施</h2><p id="1912" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">好了，我们已经讨论了时间向量的周期和非周期分量在理论上是如何工作的，现在我们将在代码中实现它们。为了使时间向量易于集成到任何种类的神经网络架构中，我们将向量定义为 Keras 层。我们定制的 Time2Vector 层有两个子函数<code class="fe nz oa ob oc b">def build():</code>和<code class="fe nz oa ob oc b">def call():</code>。在<code class="fe nz oa ob oc b">def build():</code>中我们初始化了 4 个矩阵，2 个为<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">ω</em></strong></code> <strong class="lb iu"> <em class="nx"> </em> </strong>和 2 个为<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">φ</em></strong></code> <strong class="lb iu"> <em class="nx"> </em> </strong>因为我们需要一个<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">ω</em></strong></code> <strong class="lb iu"> <em class="nx"> </em> </strong>和<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">φ</em></strong></code>的矩阵来兼顾非周期性(线性)和周期性(正弦)的特性。</p><pre class="kj kk kl km gt og oc oh oi aw oj bi"><span id="a4f9" class="nj mm it oc b gy ok ol l om on">seq_len = 128</span><span id="7ea0" class="nj mm it oc b gy oo ol l om on">def build(input_shape):<br/>   weights_linear = add_weight(shape=(seq_len), trainable=True)<br/>   bias_linear = add_weight(shape=(seq_len), trainable=True)</span><span id="c4ec" class="nj mm it oc b gy oo ol l om on">   weights_periodic = add_weight(shape=(seq_len), trainable=True)<br/>   bias_periodic = add_weight(shape=(seq_len), trainable=True)</span></pre><p id="eb8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">启动 4 个矩阵后，我们定义一旦调用该层将执行的计算步骤，因此有了<code class="fe nz oa ob oc b">def call():</code>函数。</p><p id="08be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将由 Time2Vector 层接收的输入具有以下形状<code class="fe nz oa ob oc b">(batch_size, seq_len, 5) → (32, 128, 5)</code>。<code class="fe nz oa ob oc b">batch_size</code>定义了我们希望一次将多少股票价格序列输入模型/层。<code class="fe nz oa ob oc b">seq_len</code>参数决定单个股票价格序列的长度。最后，数字<code class="fe nz oa ob oc b">5</code>来源于这样一个事实，即我们有 IBM 每日股票记录的 5 个特征(开盘价、最高价、最低价、收盘价、成交量)。</p><p id="a746" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个计算步骤排除了成交量，取开盘价、最高价、最低价和收盘价的平均值，得到形状<code class="fe nz oa ob oc b">(batch_size, seq_len)</code>。</p><pre class="kj kk kl km gt og oc oh oi aw oj bi"><span id="91b8" class="nj mm it oc b gy ok ol l om on">x = tf.math.reduce_mean(x[:,:,:4], axis=-1)</span></pre><p id="3a54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们计算非周期(线性)时间特征，并再次将维度扩展 1。<code class="fe nz oa ob oc b">(batch_size, seq_len, 1)</code></p><pre class="kj kk kl km gt og oc oh oi aw oj bi"><span id="c0fe" class="nj mm it oc b gy ok ol l om on">time_linear = weights_linear * x + bias_linear</span><span id="c6e4" class="nj mm it oc b gy oo ol l om on">time_linear = tf.expand_dims(time_linear, axis=-1)</span></pre><p id="5126" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对周期性时间特征重复相同的过程，也产生相同的矩阵形状。<code class="fe nz oa ob oc b">(batch_size, seq_len, 1)</code></p><pre class="kj kk kl km gt og oc oh oi aw oj bi"><span id="9877" class="nj mm it oc b gy ok ol l om on">time_periodic = tf.math.sin(tf.multiply(x, weights_periodic) + bias_periodic)</span><span id="6a4c" class="nj mm it oc b gy oo ol l om on">time_periodic = tf.expand_dims(time_periodic, axis=-1)</span></pre><p id="bdfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结束时间矢量计算所需的最后一步是连接线性和周期性时间要素。<code class="fe nz oa ob oc b">(batch_size, seq_len, 2)</code></p><pre class="kj kk kl km gt og oc oh oi aw oj bi"><span id="2a90" class="nj mm it oc b gy ok ol l om on">time_vector = tf.concat([time_linear, time_periodic], axis=-1)</span></pre><h2 id="89ae" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">时间 2 矢量层</h2><p id="f6f1" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">将所有步骤组合成一个层函数，代码如下所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="6ca1" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">变压器</h1><p id="971b" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">现在我们知道了提供时间的概念以及如何实现时间向量是很重要的，下一步将是转换器。转换器是一种神经网络架构，它使用<strong class="lb iu">自我关注</strong>机制，允许模型<strong class="lb iu">关注时间序列的相关部分</strong>，以提高预测质量。自关注机构由<strong class="lb iu">单头关注</strong>和<strong class="lb iu">多头关注</strong>层组成。自我关注<strong class="lb iu"> </strong>机制能够<strong class="lb iu">立即将所有时序步骤</strong>相互连接，导致<strong class="lb iu">长期依赖理解</strong>的创建。最后，所有这些过程都在 Transformer 架构中被<strong class="lb iu">并行化</strong>，从而加速了学习过程。</p><h2 id="4202" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">结合 IBM 数据和时间特性——为转换器提供信息</h2><p id="2ce4" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在实现了时间嵌入之后，我们将结合 IBM 的价格和数量特性使用时间向量作为转换器的输入。<strong class="lb iu"> Time2Vector 层接收</strong>IBM<strong class="lb iu">price</strong>和<strong class="lb iu"> volume </strong>特征作为输入，并计算<strong class="lb iu">非周期性</strong>和<strong class="lb iu">周期性</strong> <strong class="lb iu">时间特征</strong>。在随后的建模步骤中，计算出的时间特征与价格和数量特征连接在一起，形成一个形状为<code class="fe nz oa ob oc b">(32, 128, 7)</code>的矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/542976086357ce32d4b49d43b6c1ee62.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*YDGeSilwYBVnMSKonsTsaA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算时间特性并与 IBM 价格和交易量连接</p></figure><h2 id="5086" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">单头注意力</h2><p id="ec2f" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated"><strong class="lb iu"> IBM 时间序列加上我们刚刚计算的时间特征</strong>，<strong class="lb iu">形成第一单头注意层的初始输入</strong>。单头关注层总共取 3 个输入<strong class="lb iu">(查询，键，值)</strong>。对我们来说，每个查询、键和值输入都代表了 IBM 的价格、数量和时间特性。每个查询、键和值输入都通过单独的密集层接受单独的<strong class="lb iu">线性转换</strong>。为密集层提供 96 个输出单元是个人的架构选择。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/44d7e6a4207e15bf297c7dffad2158c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*-6T6v-rS0twXnRU6EwOOHg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单头注意力-查询、键和值输入的线性转换</p></figure><p id="2b8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在初始的线性变换之后，我们将计算注意力得分/权重。注意力权重决定了<strong class="lb iu">在预测未来股价时，对单个时间序列步骤</strong>的关注程度。注意力权重通过取线性变换的查询和键输入的<strong class="lb iu">点积</strong>来计算，而变换的键输入已经被转置以使得点积乘法可行。然后<strong class="lb iu">点积除以</strong>先前密集层(96)的尺寸大小，以避免爆炸梯度。然后，被划分的点积通过<strong class="lb iu"> softmax </strong>函数产生一组<strong class="lb iu">权重，其总和为 1 </strong>。作为最后一步，计算出的确定每个时间步焦点的 softmax 矩阵与转换后的<strong class="lb iu"> v 矩阵</strong>相乘，得出单头注意机制。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/f543c0c0c7b723190c8b0dd18ee44d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*6UZPjnwQZIZHtGXInA8TDQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单头注意力—在对查询、键和值输入进行线性转换后计算注意力权重</p></figure><p id="a90b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于插图对于最初的学习来说是很好的，但缺乏实现方面，我为你们准备了一个干净的单注意力 Keras 层函数🙂。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure><h2 id="7d22" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">多头注意力</h2><p id="05fd" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">为了进一步完善自我注意机制，论文作者<strong class="lb iu">提出了实施多头注意</strong><a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">【4】</a>。多头注意力层的功能是 c <strong class="lb iu">对<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">n</em></strong></code> <strong class="lb iu"> <em class="nx"> </em>单头注意力层</strong>的<strong class="lb iu">注意力权重</strong>进行</strong>处理，然后对密集层应用<strong class="lb iu">非线性变换</strong>。下图显示了 3 个单头层的连接。</p><p id="d353" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拥有<code class="fe nz oa ob oc b"><strong class="lb iu"><em class="nx">n</em></strong></code>单头层的输出允许将多个独立的单头层转换编码到模型中。因此，该模型能够同时关注多个时间序列步骤。增加注意力头的数量会积极地影响模型捕捉远距离依赖性的能力。[1]</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/deaf91e33865cb732f1bf870c13d5b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*KrOS0eJgQlrIK-E0LGWy0w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多头注意力层</p></figure><p id="1e98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同上，多头注意力层的干净实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure><h2 id="cb83" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">变压器编码器层</h2><p id="ee05" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">单头和多头注意机制(自我注意)现在被聚合到一个 transformer 编码器层中。每个编码器层包含一个自关注子层和一个前馈子层。前馈子层由两个致密层组成，其间有 ReLU 激活。</p><p id="50bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，如果 Conv 层的核大小和步距为 1，则密集层可以用 1 维卷积层代替。具有所述配置的密集层和卷积层的数学是相同的。</p><p id="7fc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个子层之后是一个丢失层，丢失后，通过将初始查询输入添加到两个子层输出来形成剩余连接。结束每个子层，在剩余连接附加之后放置一个标准化层，以稳定和加速训练过程。</p><p id="45f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了一个现成的 Transformer 层，它可以很容易地堆叠起来以提高模型的性能。由于我们不需要任何变压器解码器层，我们实现的变压器架构与 BERT [2]架构非常相似。尽管不同之处在于时间嵌入，我们的转换器可以处理三维时间序列，而不是简单的二维序列。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/1b62779ea2d9339111c5eb973e59106f.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*Y9-5RNOTGEU9LTW4sglYLw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">变压器编码器层</p></figure><p id="c3f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想深入研究代码，我们开始吧。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure><h2 id="f597" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">具有时间嵌入和转换器层的模型架构</h2><p id="ea46" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">总之，我们首先初始化时间嵌入层以及 3 个变换器编码器层。初始化之后，我们将一个回归头堆叠到最后一个 transformer 层上，训练过程开始。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="03a4" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">结果</h1><p id="cf8a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">训练过程总共有 35 个时期。在训练之后，我们可以看到，我们的转换模型只是预测了一条位于每日股价变化之间的平坦线。当仅使用 IBM 股票历史时，即使是变压器模型也仅仅能够预测股票发展的线性趋势。得出的结论是，股票的历史价格和交易量数据只包含足够的线性趋势预测的解释价值。然而，当将数据集升级到数千个股票报价机(1tb 数据集)时，结果看起来完全不同🙂。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/a041547e203de8f7d446015638a4aed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*4u2dfg_7ZJijeEKsILToYg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/4988fe803b8a9488710872638be5a7d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hh6v8X-iHvCe8zkXJ1btuQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">验证和测试数据集+预测</p></figure><h2 id="0d0b" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">将移动平均应用于股票数据—特征工程</h2><p id="27ea" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">如上所示，即使是最先进的模型架构也无法从历史股价和交易量中提取非线性股票预测。但是，当对数据应用简单的移动平均平滑效果时(窗口大小=10)，模型能够提供明显更好的预测(绿线)。该模型也能够预测涨跌，而不是预测 IBM 股票的线性趋势。然而，当仔细观察时，您仍然可以看到该模型在具有极端日变化率的日子中具有较大的预测增量，因此我们可以得出结论，我们仍然存在异常值问题。</p><p id="5068" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">应用移动平均效果的缺点是新的数据集不再反映我们的原始数据。因此，具有移动平均线效应的预测不能用作我们的交易机器人的输入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/9e6be4d963b7ca30d68a176adf2e0326.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*Pt2nfb0qHF103Hq7vVcIFw.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/8b7a25a61be6b1675adf94aba5fe9e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T8rLEAjS6kuQOuRhCbzIhQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">移动平均特征-验证和测试数据集+预测</p></figure><p id="7640" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，由于移动平均效果的平滑化，可以在不应用移动平均的情况下实现性能的提高。我的最新研究表明，当将数据集扩展到大量股票时，可以获得相同的表现。</p><p id="42dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文给出的所有代码都是可以端到端运行的笔记本的一部分。笔记本可以在<a class="ae ky" href="https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/IBM_Transformer%2BTimeEmbedding.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> GitHub </strong> </a>上找到。</p><p id="0974" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nx">来自《走向数据科学》编辑的提示:</em> </strong> <em class="nx">虽然我们允许独立作者根据我们的</em> <a class="ae ky" rel="noopener" target="_blank" href="/questions-96667b06af5"> <em class="nx">规则和指导方针</em> </a> <em class="nx">发表文章，但我们并不认可每个作者的贡献。你不应该在没有寻求专业建议的情况下依赖一个作者的作品。详见我们的</em> <a class="ae ky" rel="noopener" target="_blank" href="/readers-terms-b5d780a700a4"> <em class="nx">读者术语</em> </a> <em class="nx">。</em></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><blockquote class="pa pb pc"><p id="9d5b" class="kz la nx lb b lc ld ju le lf lg jx lh pd lj lk ll pe ln lo lp pf lr ls lt lu im bi translated">谢谢你，</p><p id="e5e3" class="kz la nx lb b lc ld ju le lf lg jx lh pd lj lk ll pe ln lo lp pf lr ls lt lu im bi translated">一月</p></blockquote></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="4603" class="nj mm it bd mn nk nl dn mr nm nn dp mv li no np mx lm nq nr mz lq ns nt nb nu bi translated">编辑于 2022 年 6 月 1 日:</h2><p id="422b" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在过去，我收到了一些反馈和请求，希望我的金融资产预测模型能够变得可用且易于使用。这将有助于人们回答以下问题:</p><p id="0c39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有哪些好的资产可以投资？我应该把什么放入我的金融投资组合？</p><blockquote class="pa pb pc"><p id="b1d9" class="kz la nx lb b lc ld ju le lf lg jx lh pd lj lk ll pe ln lo lp pf lr ls lt lu im bi translated">我给你介绍一下<strong class="lb iu">pink lion</strong>T24】www . pink lion . XYZ</p></blockquote><div class="pg ph gp gr pi pj"><a href="https://www.pinklion.xyz/" rel="noopener  ugc nofollow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">粉色狮子</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">我们把你从乏味和令人沮丧的工作中解脱出来。没有更多的清洁数据和模型试验和…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">www.pinklion.xyz</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px ks pj"/></div></div></a></div><p id="6d57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PinkLion 是一款基于我的代码库之上的产品，它使成千上万只股票、基金/ETF 和加密货币的日常资产数据可用。此外，通过提供对基础预测模型的访问，它允许资产分析和投资组合的动态优化。</p><p id="2352" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目前，注册人数仍然有限，因为单个计算需要大量的服务器资源。</p><p id="0626" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请随意尝试并提供反馈。(目前还处于大致状态)<a class="ae ky" href="http://www.pinklion.xyz" rel="noopener ugc nofollow" target="_blank"> www.pinklion.xyz </a></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="d501" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">放弃</h1><p id="de8a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated"><strong class="lb iu">本文中的任何内容都不构成任何特定证券、证券组合、交易或投资策略适合任何特定人士的建议。期货、股票和期权交易包含巨大的损失风险，并不适合每个投资者。期货、股票和期权的估值可能会波动，因此，客户的损失可能会超过他们的原始投资。</strong></p><h1 id="53f0" class="ml mm it bd mn mo py mq mr ms pz mu mv jz qa ka mx kc qb kd mz kf qc kg nb nc bi translated">参考</h1><p id="115a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">[1]为什么要自我关注？神经机器翻译架构的目标评估<a class="ae ky" href="https://arxiv.org/abs/1808.08946" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1808.08946</a></p><p id="f246" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]伯特:语言理解深度双向转换器的预训练【https://arxiv.org/abs/1810.04805 T2】</p><p id="5057" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] Time2Vec:学习时间的向量表示法【https://arxiv.org/abs/1907.05321 T4】</p><p id="0d5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4]你所需要的只是关注<a class="ae ky" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.03762</a></p></div></div>    
</body>
</html>