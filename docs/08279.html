<html>
<head>
<title>How to build a translation pipeline with RNN and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用RNN和喀拉斯建立翻译管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-translation-pipeline-with-rnn-and-keras-57c1cf4a8a7?source=collection_archive---------27-----------------------#2020-06-17">https://towardsdatascience.com/how-to-build-a-translation-pipeline-with-rnn-and-keras-57c1cf4a8a7?source=collection_archive---------27-----------------------#2020-06-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6724" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">你有没有想过一台计算机是如何能够如此快速地学习多种语言的？按照这个逐步指南创建您的第一个翻译模型。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6762f9c358e1c15a5d200911c72c758e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WP0XJ5Oou7YTMXyXHQN4BA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">巴别塔，老彼得·布鲁盖尔的画</p></figure><p id="b54f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上一篇文章中，我们看到了FFNN在增益上下文方面的局限性。对于这些情况，更好的方法是RNN，它可以管理上下文并生成状态，以便更好地理解跨时间步长的数据。<strong class="la iu">在本文中，我们将通过使用keras创建翻译模型来实践我们所学的内容</strong>。</p><p id="4054" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更准确地说，我们将开发一种多对多类型的RNN，也称为序列对序列或Seq2Seq。更高级的Seq2Seq结构包括编码器-解码器或注意力模型。我们将构建的模型如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/a0d5816b561e729eaf6b23e01a67fa9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qYC-7wVy6bU4Sfhb12TmFg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="lw">作者图片</em></p></figure><p id="526b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输入层接收英语句子，每个单词是一个时间步长。然后，隐藏层(RNN)计算每个时间步长的状态，该状态将用于下一个时间步长，输出将用于密集层。</p><h1 id="36d2" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">数据清理</h1><p id="b227" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">样本数据可以在<a class="ae lu" href="http://www.manythings.org/bilingual/" rel="noopener ugc nofollow" target="_blank">manythings.org</a>下载，来自<a class="ae lu" href="https://tatoeba.org/spa" rel="noopener ugc nofollow" target="_blank"> Tatoeba </a>。它由你需要的语言中的句子对组成。在我们的例子中，我们将使用西班牙语-英语对。</p><p id="2cfd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要做的第一件事是导入库:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="53d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后我们将读取文件并解析数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="17b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了不让计算机处理大量的线对并保持例子的简洁，我们将只处理少量的数据。让我们来看看几双。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/7e55628c322c40eeeaa4a38bc656c4ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*RE2M6muJIVBYxKjtD8_rbg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从上一个片段打印。<em class="lw">作者图片</em></p></figure><p id="efe5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">句子包含有大写字母和标点符号的单词，所以让我们清除它。在本文中，我们将看到每个步骤的代码片段和示例。最后，所有内容都将合并到一个文件中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="ced1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它将打印“我今天要冲浪”。</p><h2 id="d0c2" class="mx ly it bd lz my mz dn md na nb dp mh lh nc nd mj ll ne nf ml lp ng nh mn ni bi translated">标记器</h2><p id="8b42" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">机器学习模型不能阅读单词，只能阅读数字，为了给模型提供数据，我们需要将单词转换成数字。我们从Keras导入Tokenizer并应用两种方法。首先我们实例化这个类，然后用完整的文本调用方法<em class="nj"> fit_on_texts </em>。得益于此，我们将<strong class="la iu">创建一个字典，我们将一个单词映射到一个索引</strong>，每个唯一的单词都有一个唯一的索引。我们已经创建了一个名为<em class="nj"> text_examples </em>的例子列表，我们有3个句子。这三个句子是我们的完整数据集，所以当我们调用这个方法时，它会为每个单词创建一个新的索引。让我们看看通过打印<em class="nj"> word_index.items() </em>我们创建了什么。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b33f0f0b12d79fe1024896f8042dd9d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*2-AN2OeqlFvNkL-W7hBsiA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从上一个片段打印。<em class="lw">作者图片</em></p></figure><p id="0e0e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个单词都有自己的索引，例如单词“beach”即使出现了两次，也只是在索引为5的情况下创建一次，对于索引为3的“to”也是如此。<strong class="la iu">我们已经创建了映射，但是我们还没有将句子转换成那些索引</strong>。为此，我们需要调用方法<em class="nj"> texts_to_sequences </em>，它的作用不再是创建映射，而是应用它。它采用句子“我今天将冲浪”，并将“我”更改为1，“将”更改为2，“冲浪”更改为6，“今天”更改为7。因此，句子“我今天将冲浪”变成了[1，2，6，7]。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/4803fd7163c546623ec3ee3938abf266.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*K6rHr4rCW54uQWm8yd1fcg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从上一个片段打印。<em class="lw">作者图片</em></p></figure><p id="11d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看起来很奇怪，电脑读数字更舒服，因为它失去了单词的所有意义。当两个单词是同义词时会发生什么？<strong class="la iu">一个模型如何知道234和67是否有相似的含义？</strong>它来了<strong class="la iu">嵌入，不是把一个单词映射到一个索引，而是把一个单词映射到一个向量。</strong>计算这些向量可以保留单词的意思，并创建一个空间表示。当我们表示这些向量时，具有相似意义的单词也将具有相似的坐标。这种技术被称为嵌入，将在下一篇文章中讨论。</p><p id="e50e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们创建一个函数来返回向量和映射。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="0fc0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们把我们到目前为止所看到的应用到句子对而不是例子中，并探索结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/9166b1f420dac6f1c232d55f91bc2160.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*_LsEd_4KqIWijcCVS24REw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从上一个片段打印</p></figure><p id="1eaa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">西班牙语有7198个独特的单词，而英语只有3736个。此外，另一个区别是句子的最大长度，而西班牙语由12个单词组成，英语的最大长度是6个单词。等等，到目前为止，我们已经看到，两个句子应该是相同的长度，以适应RNN结构，我们如何处理不同的长度？</p><h2 id="a804" class="mx ly it bd lz my mz dn md na nb dp mh lh nc nd mj ll ne nf ml lp ng nh mn ni bi translated">填料</h2><p id="14c1" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">为了使所有的句子长度相同，我们使用Keras的T2填充序列。这个类的作用非常简单，对于那些长度小于最大长度的句子，它会加一个0。回到我们的列表<em class="nj"> text_examples </em>，最大长度是8，而第一个句子的长度是4，那么当我们应用填充时，我们有[1 2 6 7 0 0 0 0]。添加了4个零，使其长度为8。其他两个句子的长度已经是8，则不应用任何更改。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/8314f841373c605150a353495dcf9e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*l7sJ62WOwTZCgbW2VKRj9w.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从上一个片段打印。<em class="lw">作者图片</em></p></figure><p id="6f3d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个更直观的例子如下。对于时间步长为8的RNN，我们要翻译句子“鸟儿歌唱”它变成了“los pájaros están cantando”</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/66e272084dfcb3837d1998519337996b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j6eA3cn9DukPoqOGkS1lvQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">RNN结构，<em class="lw">作者图片</em></p></figure><p id="d4f9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将在构建模型时参考这个图像。我们将相同的代码应用于西班牙英语对。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="afbf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们准备好了，句子已经被清理了，它们也变成了向量，由于填充，它们的长度是一样的。我们的训练数据准备好了。</p><h1 id="f1ea" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">模型创建</h1><p id="8091" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">我们需要定义的第一层是输入层，图像'<em class="nj"> RNN结构'</em>中的蓝色层。在Keras中，RNN的<strong class="la iu">输入形状是3D(批量大小，时间步长，特征)</strong>。输入层有两个元素(时间步长，特征),我们已经知道时间步长，从我们的最大句子长度12，和特征是在时间步长的观察数量，在我们的例子中只有一个。然后batch_size被定义为对象<em class="nj">模型</em>的方法<em class="nj"> fit </em>的一个参数，Keras假设它为1或更大。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="b3fe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们添加的第二层是RNN，更准确地说，在这种情况下，我们处理的是长短期记忆(LSTM)。需要注意的一个重要参数是<strong class="la iu"><em class="nj">return _ sequences</em></strong>，默认设置为False，该图层的输出将只是最后一个时间步的矢量，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/88695441632ae479d1f8cde134412b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*ATDmg_9Neo1OcEGEq4S2nA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情感分析(多对一)。RNN在最后一步输出矢量，<em class="lw">作者的图片</em></p></figure><p id="cd3c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们不需要在每个时间步获得输出，只需要在最后获得输出，这样密集层就可以做出预测。但是在我们的例子中，正如我们在图像'<em class="nj"> RNN结构</em> ' <strong class="la iu">中看到的，在每个时间步</strong>都有一个预测，所以RNN层在最后不会输出一个矢量，而是在每个时间步<strong class="la iu">输出一个矢量。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/94d643bf43bb8b27ede1711384d7ad1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FlSC7BZjOyURTnRmIKuuGg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多对多。当return_sequences=True时，RNN层在每个时间步长输出一个矢量</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="076c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个解释之后是介绍时间分布的时候了。尽管概念上非常简单，但当你第一次面对它时，它会引起一些混乱。我们刚刚在RNN层中设置了<em class="nj"> return_sequences=True </em>，所以我们在每一步都有一个输出向量。那我们该怎么办？<strong class="la iu">应用一个密集层，</strong>所以最后用一个激活层我们可以做一个预测。这个致密层是什么样子的？</p><p id="9948" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">输入层接收一个形状为256 </strong>的矢量，它相当于LSTM层中的256个单位。<strong class="la iu">输出层有一个7198 </strong>的形状，代表我们词汇中唯一西班牙语单词的总数(<em class="nj"> spanish_vocab </em>)。<strong class="la iu">预测字将是7198中已经激活的单元</strong>。因此，如果最终向量除了单元324之外都是零，其中我们有一个1，我们将索引324映射到标记化器，并获得翻译的单词。这是在每个孤立的时间步完成的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/873d6345acc45377492bbab353bcb7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*dXLobma1N77rwKcRkv6Olw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="lw">作者图片</em></p></figure><p id="2185" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们刚刚看到了如何应用密集层，但是<strong class="la iu">时间分布</strong>有什么用呢？<strong class="la iu">这仅仅意味着我们在每个时间步</strong>应用先前解释的层。因为我们使用<em class="nj"> return_sequence=True </em>，RNN层在每个时间步输出一个矢量，因此我们需要在每个时间步应用相同的密集层。如果我们放大模型的输出层，它看起来如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/bf57a992dbe94e70bed58d964ca7a462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0RKuZeTUHdse2UmNG0XnhQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="lw">作者图片</em></p></figure><p id="2e16" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总而言之，为了创建模型，我们应用以下代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="a111" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该模式的总结是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/cd32c1a8dc0d7989fa5a70f74ff35782.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*fOGk6kujCCWxdHJRK1qM0w.jpeg"/></div></figure><p id="d134" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我们训练我们的模型。参数尚未优化，本文的目的是了解和创建管道。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="40e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们完成训练，让我们做一些预测。正如我们之前看到的，模型在每个时间步长的输出是形状7198的向量，其中激活的单元是预测的单词，因此例如，如果在我们的第一次预测之后，我们有输出324，我们需要将索引映射到西班牙语单词。下面的函数将为我们完成这项工作。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="ceb5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了检查最终的预测，我们使用下面的代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="42da" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nj">你可以在</em> <a class="ae lu" href="https://github.com/NechuBM/rnn_tutorial/tree/feature/simple-rnn/tutorials/simple_rnn" rel="noopener ugc nofollow" target="_blank"> <em class="nj">下面的链接</em> </a>中找到一个jupyter笔记本，里面有完整的代码</p><h2 id="0fa1" class="mx ly it bd lz my mz dn md na nb dp mh lh nc nd mj ll ne nf ml lp ng nh mn ni bi translated">摘要</h2><p id="2d37" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh mr lj lk ll ms ln lo lp mt lr ls lt im bi translated">在这篇文章中，我们把学到的关于RNN的概念付诸实践。</p><p id="e8ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们让c <strong class="la iu">学习数据</strong>，<strong class="la iu">创建了一个索引</strong>来将每个单词映射到一个向量，并将所有的<strong class="la iu">句子转换成</strong>那些<strong class="la iu">向量</strong>，这要感谢<em class="nj">分词器</em>。然后我们使用<em class="nj"> pad_sequences </em>让所有的句子都有<strong class="la iu">一样的长度。</strong></p><p id="9031" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了创建模型，我们定义了输入形状，我们用<em class="nj"> return_sequences=True </em>创建了一个LSTM层，然后由于<strong class="la iu"> TimeDistributed </strong>，在每个时间步应用了一个密集层。</p><p id="de9f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下一篇文章中，我们将详细阐述模型架构，以创建一个更好的执行翻译。从理论的角度来看，我们将讨论不同类型的RNN建筑，如LSTM，并分析之前介绍的术语，如嵌入。</p></div></div>    
</body>
</html>