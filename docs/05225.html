<html>
<head>
<title>UDAF and Aggregators: Custom Aggregation Approaches for Datasets in Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">UDAF和聚合器:Apache Spark中数据集的定制聚合方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/udaf-and-aggregators-custom-aggregation-approaches-for-datasets-in-apache-spark-68dc4d09796d?source=collection_archive---------21-----------------------#2020-05-04">https://towardsdatascience.com/udaf-and-aggregators-custom-aggregation-approaches-for-datasets-in-apache-spark-68dc4d09796d?source=collection_archive---------21-----------------------#2020-05-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div class="gh gi io"><img src="../Images/c97855d5b30bfa5f43261819216829c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*C5alQ6sFvgkGKjwKhIVbhA.png"/></div><p class="iv iw gj gh gi ix iy bd b be z dk translated">Apache Spark中的聚合</p></figure><div class=""/><div class=""><h2 id="2acb" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">数据记录的聚合是数据分析工作的必要组成部分，因此Spark旨在提出一个强大的聚合框架，以满足用户在各种情况下的需求。</h2></div><p id="034f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">使用rdd作为分布式数据收集的表示，Spark中的聚合更早地与(Hadoop世界的)map-reduce框架的缩减部分的概念相一致。然而，随着Dataframe和最新数据集表示的引入，聚合框架现在更类似于人们更熟悉的SQL世界中的聚合。</p><h2 id="ec7c" class="lm ln jb bd lo lp lq dn lr ls lt dp lu kz lv lw lx ld ly lz ma lh mb mc md me bi translated">数据集的类型化和非类型化视图</h2><p id="5dff" class="pw-post-body-paragraph kq kr jb ks b kt mf kc kv kw mg kf ky kz mh lb lc ld mi lf lg lh mj lj lk ll ij bi translated">在数据帧和数据集之间，前者是带有模式定义的数据记录的非类型化视图，而后者是类型化视图。在Dataframe中，每条记录都被视为值的集合(类似于表中的一行),并公开一个类似SQL的接口来执行转换。而在Dataset中，每条记录都被视为某种类型的对象，并公开一个编程接口来执行转换。但是，为了保持非类型化数据帧的概念以方便SQL和提高效率，Dataset还通过编码器框架支持隐藏的非类型化视图。在我之前的博客中阅读更多关于数据集编码器框架的内容:<a class="ae mk" href="https://medium.com/me/stats/post/4a3026900d63" rel="noopener"> Apache Spark数据集编码器揭秘</a>。任何T类型数据集的显式非类型化视图都由“Row”类型的数据集表示。</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/a81cb66425f82994d835d92a5897dc8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*wRV-R4OrpxSZ80lC4-Reyg.png"/></div></figure><h2 id="4be2" class="lm ln jb bd lo lp lq dn lr ls lt dp lu kz lv lw lx ld ly lz ma lh mb mc md me bi translated">类型化和非类型化聚合:概述</h2><p id="d100" class="pw-post-body-paragraph kq kr jb ks b kt mf kc kv kw mg kf ky kz mh lb lc ld mi lf lg lh mj lj lk ll ij bi translated">考虑到类型化和非类型化表示，Spark也提供了非类型化和类型化聚合。对于非类型化的聚合，只需提到可选的分组列和一个或多个列上的聚合表达式(由聚合函数组成)。这与SQL中完成聚合的方式非常相似。非类型化聚合的执行会导致显式的非类型化视图。Spark使用户能够对属于任何类型的数据集执行非类型化聚合。另一方面，对于类型化聚合，用户必须提供一个聚合器对象(处理T类型数据集的T类型对象),以便在整个数据集上聚合，或者在返回聚合类型数据集的分组数据集上聚合。</p><p id="2cd6" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">数据集上的非类型化聚合:</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/14000ad0b22d75e6caa19c6ece03971e.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*ypLPFiIse3_aIwzcWtKKYw.png"/></div><p class="iv iw gj gh gi ix iy bd b be z dk translated">数据集的无类型视图上的无类型聚合<t/></p></figure><pre class="mm mn mo mp gt mr ms mt mu aw mv bi"><span id="c227" class="lm ln jb ms b gy mw mx l my mz">/* Untyped View - A is collection of Rows with two columns col1 and col2 */<br/>=&gt; Dataset&lt;T&gt; A;</span><span id="d4e2" class="lm ln jb ms b gy na mx l my mz">/*Sum Aggregation on column col1 of A over entire collection*/<br/>=&gt; A.agg(functions.sum(A("col1")))</span><span id="ab58" class="lm ln jb ms b gy na mx l my mz">/*Custom Aggregation on col1 of A over entire collection*/<br/>=&gt; A.agg(functions.udf(MyUDAF(A("col1"))))</span><span id="9a52" class="lm ln jb ms b gy na mx l my mz">/*Sum Aggregation on col1 of A over grouped collection of A grouped via col2 */<br/>=&gt; A.grouby(A("col2")).agg(functions.sum(A("col1")))</span><span id="3ce4" class="lm ln jb ms b gy na mx l my mz">/*Custom Aggregation on col1 of A over grouped collection of A grouped via col2*/<br/>=&gt; A.grouby(A("col2")).agg(functions.udf(MyUDAF(A("col1"))))</span><span id="db4d" class="lm ln jb ms b gy na mx l my mz">*All above aggregations return an untyped view (Dataset&lt;Row&gt;)*</span></pre><p id="8ec2" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">数据集上的类型化聚合:</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/3ae2ec1f520e1cb2bc6f0228e958bc60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*_2KqXLh-r-vIDktH3KX9IA.png"/></div><p class="iv iw gj gh gi ix iy bd b be z dk translated">通过聚合器对数据集<t>的T类型对象进行类型化聚合</t></p></figure><pre class="mm mn mo mp gt mr ms mt mu aw mv bi"><span id="b785" class="lm ln jb ms b gy mw mx l my mz">/* Typed View - A is collection of type T objects where type T consists of two fields col1 and col2 */<br/>=&gt; Dataset&lt;T&gt; A;</span><span id="dcec" class="lm ln jb ms b gy na mx l my mz">/* B an Aggregator instance aggregating multiple objects of type T to perform desired aggregation on T's field col1, The aggregated value is returned back via a field inside object of type V */<br/>=&gt; Aggregator&lt;T,U,V&gt; B;</span><span id="c9fe" class="lm ln jb ms b gy na mx l my mz">/*Aggregation on field col1 of A over entire collection*/<br/>=&gt; A.select(B.toColumn()) returns Dataset&lt;V&gt;</span><span id="3dce" class="lm ln jb ms b gy na mx l my mz">/*Aggregation on field col1 of A over grouped collection grouped by Key Object derived from Type T */<br/>=&gt; A.groupbykey(KeyFunc, Encoder&lt;K&gt;).agg(B.toColumn()) <br/>===&gt; Here KeyFunc is of type MapFunction&lt;T,K&gt; which output grouping Key Object of type K for a given record of type T<br/>===&gt; This grouped aggregation outputs Dataset of Tuple consisting of Key Object of type K and Aggregated object of type V, viz., (Dataset&lt;Tuple&lt;K,V&gt;&gt;) </span></pre><h2 id="7e67" class="lm ln jb bd lo lp lq dn lr ls lt dp lu kz lv lw lx ld ly lz ma lh mb mc md me bi translated">自定义非类型化聚集:UDAF</h2><p id="7422" class="pw-post-body-paragraph kq kr jb ks b kt mf kc kv kw mg kf ky kz mh lb lc ld mi lf lg lh mj lj lk ll ij bi translated">虽然，针对非类型化聚合的支持，Spark已经提供了多种这样的聚合函数，但是也支持构建一个定制的非类型化聚合函数，称为UDAF。通过扩展“org . Apache . Spark . SQL . expressions”包中的“UserDefinedAggregationFunction”类，并在基类中覆盖以下方法的实现，可以在Spark中创建UDAF:</p><pre class="mm mn mo mp gt mr ms mt mu aw mv bi"><span id="f429" class="lm ln jb ms b gy mw mx l my mz">/*Return schema for input column(s) to the UDAF, schema being built using StructType*/<br/>=&gt; public StructType inputSchema()</span><span id="9ae0" class="lm ln jb ms b gy na mx l my mz">/*Return schema of aggregation buffer, schema being built using StructType */<br/>=&gt; public StructType bufferSchema()</span><span id="b309" class="lm ln jb ms b gy na mx l my mz">/*DataType of final aggregation result*/<br/>=&gt; public DataType dataType()</span><span id="c5df" class="lm ln jb ms b gy na mx l my mz">/*Initialize aggregation buffer*/<br/>=&gt; public void initialize(MutableAggregationBuffer buffer)</span><span id="b8c8" class="lm ln jb ms b gy na mx l my mz">/*Update aggregation buffer for each of the untyped view (Row) of an input object*/<br/>=&gt; public void update(MutableAggregationBuffer buffer, Row row)</span><span id="1b02" class="lm ln jb ms b gy na mx l my mz">/*Update current aggregation buffer with a partially aggregated buffer*/<br/>=&gt; public void merge(MutableAggregationBuffer buffer, Row buffer)</span><span id="96a3" class="lm ln jb ms b gy na mx l my mz">/*Evaluate final aggregation buffer and return the evaluated value of DataType declared earlier */<br/>=&gt; public Object evaluate(Row buffer)</span></pre><blockquote class="nc nd ne"><p id="2d8a" class="kq kr nf ks b kt ku kc kv kw kx kf ky ng la lb lc nh le lf lg ni li lj lk ll ij bi translated">你可以在定制的UDAF类中声明额外的字段(在UDAF构造函数中可选地初始化这些字段)和额外的方法，以便使用这些内部的覆盖方法来实现聚合目标。此外，需要注意的是<a class="ae mk" href="http://BooleanType ByteType ShortType IntegerType LongType FloatType DoubleType DecimalType TimestampType DateType StringType BinaryType" rel="noopener ugc nofollow" target="_blank">只有某些数据类型</a>被允许用于最终结果类型以及定义输入和缓冲模式。</p></blockquote><p id="eeac" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks jc"> UDAF示例</strong>:假设有一个如下声明的类型‘raw data wrapper ’,每个包装器由一个键字段和一个反映射字段组成。</p><figure class="mm mn mo mp gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="f41b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对于“RawDataWrapper”类型的数据集，这里有一个UDAF声明，它聚集了RawDataWrapper类型的记录之间的反映射。</p><figure class="mm mn mo mp gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="4ad0" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">上述申报的UDAF的用途如下所示:</p><figure class="mm mn mo mp gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h2 id="7863" class="lm ln jb bd lo lp lq dn lr ls lt dp lu kz lv lw lx ld ly lz ma lh mb mc md me bi translated">自定义类型聚合:聚合器</h2><p id="383a" class="pw-post-body-paragraph kq kr jb ks b kt mf kc kv kw mg kf ky kz mh lb lc ld mi lf lg lh mj lj lk ll ij bi translated">类型化聚合通过<a class="ae mk" href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html" rel="noopener ugc nofollow" target="_blank">abstract generic ' Aggregator&lt;IN，BUF，OUT &gt; </a>'类(存在于包' org . Apache . spark . SQL . expressions '中)来支持。用户可以定义自己的自定义聚合器，方法是使用为IN(输入记录类型)定义的类型、为BUF(聚合缓冲区)定义的类型和为OUT(输出记录类型)定义的类型对其进行扩展，并在基类中覆盖以下方法的实现:</p><pre class="mm mn mo mp gt mr ms mt mu aw mv bi"><span id="9e2a" class="lm ln jb ms b gy mw mx l my mz">/* return Encoder for aggregation buffer of type BUF. This is required for buffer ser/deser during shuffling or disk spilling */<br/>=&gt; public Encoder&lt;BUF&gt; bufferEncoder()</span><span id="acc9" class="lm ln jb ms b gy na mx l my mz">/* return Encoder for output object of type OUT after aggregation is performed */<br/>=&gt; public Encoder&lt;OUT&gt; outputEncoder()</span><span id="7203" class="lm ln jb ms b gy na mx l my mz">/* return updated aggregation buffer object of type BUF after aggregating the existing buffer object of type BUF with the input object of type IN*/<br/>=&gt; public BUF reduce(BUF buffer, IN input) ()</span><span id="50e2" class="lm ln jb ms b gy na mx l my mz">/* return updated aggregation buffer of type BUF after merging two partially aggregated buffer objects of type BUF */<br/>=&gt; public BUF merge(BUF buffer1, BUF buffer2)</span><span id="dbf0" class="lm ln jb ms b gy na mx l my mz">/* return output object of type OUT from evaluation of  aggregation buffer of type BUF */<br/>=&gt; public OUT finish(BUF arg0)</span><span id="d30a" class="lm ln jb ms b gy na mx l my mz">/* return buffer object of type BUF after initializing the same */<br/>=&gt; public BUF zero()</span></pre><blockquote class="nc nd ne"><p id="92ef" class="kq kr nf ks b kt ku kc kv kw kx kf ky ng la lb lc nh le lf lg ni li lj lk ll ij bi translated">与UDAF类似，用户可以在自定义的聚合器类中声明额外的字段(在聚合器构造函数中可选地初始化这些字段)和额外的方法，以便使用这些内部的覆盖方法来实现聚合目标。</p></blockquote><p id="7745" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><strong class="ks jc">聚合器示例:</strong>下面是一个聚合器声明，它将在RawDataWrapper记录上实现与UDAF示例所描述的相似的聚合目标</p><figure class="mm mn mo mp gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="5936" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">上面声明的聚合器的用法如下所示:</p><figure class="mm mn mo mp gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h2 id="1b43" class="lm ln jb bd lo lp lq dn lr ls lt dp lu kz lv lw lx ld ly lz ma lh mb mc md me bi translated">比较:UDAF和聚合器</h2><p id="019e" class="pw-post-body-paragraph kq kr jb ks b kt mf kc kv kw mg kf ky kz mh lb lc ld mi lf lg lh mj lj lk ll ij bi translated">如果聚合字段不涉及ArrayType或MapType等复杂类型，UDAF和聚合器的效率是相似的。但是，聚合器在聚合复杂数据类型方面比UDAF效率更高。这是因为在UDAF的每次更新操作中，scala数据类型(特定于用户)会转换为catalyst数据类型(catalyst内部数据类型),反之亦然。对于聚合缓冲区中的复杂数据类型，这种转换在效率和内存使用方面变得非常昂贵，使得UDAF在这些情况下与对等聚合器相比运行缓慢。在这篇博客中，为了实现相似的聚合目标，提供了UDAF和聚合器的示例，当两者都在180 GB的样本数据上运行时，UDAF需要大约40分钟，而聚合器只需要大约7分钟就可以在集群上完成一组相似的资源。</p><p id="fcd2" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">然而，尽管UDAF提供了类似SQL的易用性，但聚合器的聚合方式在编写聚合逻辑时提供了更大的灵活性和编程优雅性。</p><blockquote class="nc nd ne"><p id="a326" class="kq kr nf ks b kt ku kc kv kw kx kf ky ng la lb lc nh le lf lg ni li lj lk ll ij bi translated">此外，与大量可供使用的非类型化聚合函数相比，类型化聚合函数的库很少(基于聚合器概念)。因此，非类型化聚合流仍然是广泛使用的一种。但是，在未来的Spark版本中，类型化聚合器将被集成到非类型化聚合流中，以便用户可以通过聚合器使用已经可用的非类型化聚合函数库以及高效灵活的定制聚合函数。很明显，这将最终导致普遍使用UDAF来定义自定义聚合函数的做法遭到反对。</p></blockquote><p id="595a" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">如果您对Spark聚合框架有更多的疑问，请随时在评论区提问。</p></div></div>    
</body>
</html>