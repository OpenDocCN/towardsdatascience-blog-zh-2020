<html>
<head>
<title>Head Pruning in Transformer Models!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器模型中的头部修剪！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/head-pruning-in-transformer-models-ec222ca9ece7?source=collection_archive---------22-----------------------#2020-05-11">https://towardsdatascience.com/head-pruning-in-transformer-models-ec222ca9ece7?source=collection_archive---------22-----------------------#2020-05-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="93b1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在这篇文章中，我们将研究如何在像伯特这样的变形金刚模型中修剪注意力。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6e3f1e3181d893792d61ef05648302a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BHEDS7vGKgSEUcxIsuwnDg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">阿瑟尼·托古列夫在<a class="ae kv" href="https://unsplash.com/s/photos/transformer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="b2c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">概述:</p><ul class=""><li id="74a1" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">本文背景作品</strong>。</li><li id="a923" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">注意力头</strong>和<strong class="ky ir">的重要性为什么需要修剪</strong>。</li><li id="3582" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">识别重要头部</strong>及其<strong class="ky ir">头部</strong>功能<strong class="ky ir">。</strong></li><li id="d182" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">如何修剪头部？</strong></li><li id="1787" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">修剪如何影响<strong class="ky ir"> BLEU分数</strong>。</li><li id="915a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">用于分析的剪枝</strong></li></ul><h1 id="a9d4" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">背景工作</h1><p id="9e8d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">简单回顾一下，这是基本的变压器模型</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/7ac296d30e46201322613b6d40ef08c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5yxKYbi_K2NihsW6Z0Tq-Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">变压器的模型架构，(图片来源:图1和图2来自<a class="ae kv" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">)你所需要的就是注意力</a></p></figure><p id="2bb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上图可以看出，变压器有<strong class="ky ir">三种</strong>注意力实现方式，分别是:<br/> -多头注意力<strong class="ky ir">编码器</strong>、<br/> - <strong class="ky ir">屏蔽</strong>多头注意力<strong class="ky ir">解码器</strong>、<br/> -多头注意力<strong class="ky ir">编码器-解码器</strong></p><p id="1751" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个MHA由几个<strong class="ky ir">缩放的点积</strong>注意力头的<strong class="ky ir">串联</strong>组成，这些注意力头在<strong class="ky ir">并行</strong>中运行，不像递归网络，这就是为什么注意力比几乎所有的rnn都好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/cc5b502dbc7c67793ea4d9cd26ddfcff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*kenaiakmUzdkF0QTwhRG1A.png"/></div></figure><p id="c9a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个头是<strong class="ky ir">查询的注意函数，键和值</strong>带有可训练参数<strong class="ky ir"> (Wᵢᵩ、Wᵢᵏ、Wᵢᵛ) </strong>。</p><h1 id="f4cf" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">注意力集中的重要性</h1><p id="ad66" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated"><strong class="ky ir"> MHA </strong>的所有<strong class="ky ir"> 3种</strong>都以不同的方式发挥着特定的重要作用。</p><ul class=""><li id="f463" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">编码器MHA </strong>包含自我关注层，这里所有的参数即查询、键和值都来自同一个地方，在这种情况下，是先前的编码器或输入向量序列。</li><li id="927f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">解码器屏蔽MHA </strong>屏蔽掉该特定字之后的所有字，这表明解码器中的每个位置都基于<strong class="ky ir"> </strong>序列的<strong class="ky ir">先前输入</strong>，该<strong class="ky ir">模仿<strong class="ky ir">序列中的</strong>编码器-解码器机制来对递归</strong>模型进行排序。</li><li id="d534" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">在<strong class="ky ir">编码器-解码器MHA </strong>，<strong class="ky ir">查询</strong>来自被屏蔽的MHA的向量，然后将其与来自顶部编码器的<strong class="ky ir">密钥和值</strong>向量组合。这种整体结构允许基于先前的输入产生有效的结果。</li></ul><p id="13fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">自我关注在MHA是变压器的关键组成部分，因为MHAs依赖于关注头，对他们来说准确是很重要的，依赖于<strong class="ky ir">多个头可以提高</strong>效率达<strong class="ky ir"> 1 BLEU </strong>分数。</p><h1 id="537f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">为什么修剪？</h1><p id="c696" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在<a class="ae kv" href="https://arxiv.org/pdf/1905.09418.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> <em class="nf">论文</em> </strong> </a> <strong class="ky ir">，</strong>中作出了令人惊讶的观察，即使在正常训练模型(具有所有头部)之后，许多头部可以在<strong class="ky ir">测试时间</strong>被<strong class="ky ir">移除</strong>，并且它将<strong class="ky ir">不会显著影响BLEU </strong>分数，事实上，在一些情况下移除少量头部导致BLEU分数提高。</p><h1 id="5db7" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">识别重要负责人</h1><p id="9dd0" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">为了从MHA中检测重要的头，为了去除不太重要的头，我们可以使用许多方法，如头的<strong class="ky ir"/>或<strong class="ky ir">逐层相关性传播(LRP) </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/c9651c52b789669987b421247dda0845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*717_K6vvc1aPe_F0HxFZBQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:图1来自<a class="ae kv" href="https://arxiv.org/pdf/1905.09418.pdf" rel="noopener ugc nofollow" target="_blank">分析多头自我关注</a></p></figure><p id="3546" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">置信度</strong>是一种比LRP相对简单的方法<strong class="ky ir">，它包含了其<strong class="ky ir">最大注意力权重</strong>的平均值，其中平均值是对用于评估的一组句子取的，它是翻译的一个重要</strong>考虑因素。</p><p id="d46d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从这个图中可以看到，几乎<strong class="ky ir">模特80% </strong>的注意力都给了<strong class="ky ir">单令牌头</strong>。</p><p id="5b04" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">LRP比“信心”更可靠。这有助于找到头部<strong class="ky ir">网络</strong>中头部的<strong class="ky ir">相对贡献</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/3fa2f1f6b0a7d41fc465c51e2bcadbca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bqAEJOlfmE53VGYMjo7b5Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:图1来自<a class="ae kv" href="https://arxiv.org/pdf/1905.09418.pdf" rel="noopener ugc nofollow" target="_blank">分析多头自我关注</a></p></figure><p id="ab08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"/>置信度和LRP都用于<strong class="ky ir">检测重要头部</strong>，从上图可以看出，LRP和置信度显示的结果几乎相同。那么，现在我们知道了哪些头部是重要的，让我们来看看它们的作用。</p><h1 id="ee1f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">头部功能</h1><p id="1741" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在变形金刚的MHA中，每个头部都在影响模型产生的翻译中发挥作用，这些是头部可能执行的功能:</p><ul class=""><li id="c7fb" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">位置</strong>:在此，头部指向<strong class="ky ir">附近的令牌</strong>，下图中以紫色显示。</li><li id="a24a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">句法</strong>:指向句子中的<strong class="ky ir">特定关系</strong>，以绿色显示</li><li id="a2e2" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">生僻字</strong>:此处指向最不常用的<strong class="ky ir">或生僻字</strong>，以橙色显示。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/2fc5a23166e191d4d7ee2c79741ab926.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*N8EWKyIXtWXvXbljiesnpA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:图1来自<a class="ae kv" href="https://arxiv.org/pdf/1905.09418.pdf" rel="noopener ugc nofollow" target="_blank">分析多头自我关注</a></p></figure><p id="b1c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从这些头函数中，很明显<strong class="ky ir">某些头</strong>比其他头更准确地检测和学习<strong class="ky ir">句法关系。</strong></p><p id="9305" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这也支持了一个假设，即编码器确实支持句子中一定量的<strong class="ky ir">句法歧义消除</strong>。</p><h1 id="61aa" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">如何修剪头部？</h1><p id="ba81" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在原始变压器中，正如我们在背景工作中看到的，MHA由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2c2dab3edf48321b27733765cea4fab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*qP0jzlNyPd-f7bsMVI2YSQ.png"/></div></figure><p id="63c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了去除不太重要的磁头，我们通过使用特定的门g <strong class="ky ir"> ᵢ </strong>来修改等式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/4095747accb6ee5b9cf3082ca546859f.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*3EAVncpLhewuvKTe0GqSPg.png"/></div></figure><p id="259f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中g <strong class="ky ir"> ᵢ </strong>在<strong class="ky ir"> {0，1} </strong>之间，如果g <strong class="ky ir"> ᵢ </strong>为1，那么在拼接时我们考虑<strong class="ky ir">的所有注意力</strong>否则如果g <strong class="ky ir"> ᵢ </strong>为0，那么我们就<strong class="ky ir">修剪</strong>它。</p><p id="afd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，<strong class="ky ir"> gᵢ是一个可学习的参数</strong>，并且独立于输入序列，因为我们想要为此修剪头部，我们理想地将<strong class="ky ir"> L0正则化</strong>应用到标量门g <strong class="ky ir"> ᵢ </strong>。<strong class="ky ir"> L0范数</strong>等于非零组件的数量，并强制模型移除不太重要的头。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/f73b1ca8651bae7b82eded7b081aab7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/1*Uu_Fbe6l549HZevc6tVpaQ.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">重要头部的模型聚合(图片来源:来自<a class="ae kv" href="https://lena-voita.github.io/posts/acl19_heads.html" rel="noopener ugc nofollow" target="_blank">头部故事</a>的莉娜·沃伊塔)</p></figure><p id="1e80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个模型得到收敛，最后，头部被完全移除或者保持打开，这意味着我们只使用所有注意力头部的子集。</p><h1 id="df25" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">BLEU评分</h1><p id="3744" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">移除头部后，让我们看看它如何影响模型的BLEU分数，从下图中可以看到，当将模型从<strong class="ky ir"> 48个头部压缩到10个头部时，BLEU分数<strong class="ky ir">几乎没有下降0.25 </strong> <strong class="ky ir"> BLEU </strong>。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/a18d263d937b8d7d15f3388b6758263b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZLQgd5iCTFoTW2psX3i1SA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:图7来自<a class="ae kv" href="https://arxiv.org/pdf/1905.09418.pdf" rel="noopener ugc nofollow" target="_blank">分析多头自我关注</a></p></figure><p id="9cdf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇<a class="ae kv" href="https://arxiv.org/pdf/1905.09418.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>展示了在两个数据集<strong class="ky ir">【WMT】</strong>和<strong class="ky ir"> OpenSubtitle </strong>上执行的头部修剪。如上图所示，BLEU分数根据使用的注意力头部来标注。</p><p id="4fa0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">令人惊讶的是，在WMT数据集的情况下，在移除几个头部后，我们可以看到BLEU分数的<strong class="ky ir">增量</strong>，当我们将头部从5个减少到1个时，BLEU分数<strong class="ky ir">急剧减少</strong>。这些观察结果表明，我们只需要<strong class="ky ir">几个头(5–10)</strong>就可以用基本模型取得有竞争力的结果。</p><h1 id="2d8a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">用于分析的修剪</h1><p id="c8f5" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">变压器由三个mha组成，如背景工作中所述。这些MHA是:<br/> (a)编码器<strong class="ky ir"/><br/>的多头注意(MHAs】屏蔽<strong class="ky ir">解码器<strong class="ky ir"><br/>的多头注意</strong> (c)编码器-解码器</strong>的多头注意<strong class="ky ir"/></p><p id="963d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有这些MHA包含了多个头部的连接。头部修剪可以发生在这些mha中的任何一个。据观察，模型<strong class="ky ir">更倾向于首先修剪编码器</strong>自关注头，而<strong class="ky ir">编码器-解码器MHA </strong>在两个数据集之间似乎是<strong class="ky ir">重要的</strong>。显然，没有编码器-解码器MHA翻译任务是不可能的。如下图所示，在这两种情况下，不太重要的磁头都来自编码器MHA。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/4f9b91d7340f05eb8f4248648e11dc0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCZXddN5fpTK1QaKOrjfkQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:图9来自<a class="ae kv" href="https://arxiv.org/pdf/1905.09418.pdf" rel="noopener ugc nofollow" target="_blank">分析多头自我关注</a></p></figure><p id="4923" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一个观察结果是，MHA的头部功能即使在修剪后仍然保留。当修剪5-6个头以下的模型时，由于头的数量较少，只有<strong class="ky ir">几个头</strong>必须执行<strong class="ky ir">几个功能</strong>然后它们也被保留。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ca"><img src="../Images/2c7deb63f1f27fcf85cb7dd188a4d6fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hLM6F4siTqDtlm3dzxC-Gg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:图8来自<a class="ae kv" href="https://arxiv.org/pdf/1905.09418.pdf" rel="noopener ugc nofollow" target="_blank">分析多头自我关注</a></p></figure><p id="1efe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这篇文章增加了你对MHA和修剪头部更好的效率的了解。</p><h1 id="470f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">参考</h1><ul class=""><li id="5b56" class="ls lt iq ky b kz my lc mz lf no lj np ln nq lr lx ly lz ma bi translated">人头的故事:(<a class="ae kv" href="https://lena-voita.github.io/posts/acl19_heads.html" rel="noopener ugc nofollow" target="_blank">贴</a>)</li><li id="2617" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">分析多头自我关注:(<a class="ae kv" href="https://arxiv.org/pdf/1905.09418.pdf" rel="noopener ugc nofollow" target="_blank">原文)</a></li><li id="a94a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">十六个头真的比一个好吗？:(<a class="ae kv" href="https://arxiv.org/pdf/1905.10650.pdf" rel="noopener ugc nofollow" target="_blank">原文)</a></li><li id="e260" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">关注就是你需要的全部:(<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">原文</a>)</li></ul><blockquote class="nr ns nt"><p id="d639" class="kw kx nf ky b kz la jr lb lc ld ju le nu lg lh li nv lk ll lm nw lo lp lq lr ij bi translated"><em class="iq">感谢阅读，您可以在</em><a class="ae kv" href="http://linkedin.com/in/gauravghati/" rel="noopener ugc nofollow" target="_blank"><em class="iq">LinkedIn</em></a><em class="iq"/><a class="ae kv" href="https://twitter.com/GauravGhati/" rel="noopener ugc nofollow" target="_blank"><em class="iq">Twitter</em></a><em class="iq">或我的</em> <a class="ae kv" href="http://gauravghati.world/" rel="noopener ugc nofollow" target="_blank"> <em class="iq">作品集</em> </a>上与我联系</p></blockquote></div></div>    
</body>
</html>