<html>
<head>
<title>Topic Modeling In NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modeling-in-nlp-524b4cffeb68?source=collection_archive---------21-----------------------#2020-05-09">https://towardsdatascience.com/topic-modeling-in-nlp-524b4cffeb68?source=collection_archive---------21-----------------------#2020-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="43dd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">重点是潜在的狄利克雷分配</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b6c19052ab3ae18a12dbd7e25eb5f7b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxByzdS7GLFQKK7YfbdRrw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@mitchel3uo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">米切尔罗</a>在<a class="ae ky" href="https://unsplash.com/s/photos/tag-cloud?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="b440" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在自然语言处理中，术语<em class="lv">主题</em>是指“组合在一起”的一组单词。这是想到这个话题时想到的词。拿<em class="lv">体育</em>来说。一些这样的词是<em class="lv">运动员</em>、<em class="lv">足球</em>和<em class="lv">体育场</em>。</p><p id="f618" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">主题模型</em>是自动发现文档集合中出现的主题的模型。然后，可以使用经过训练的模型来辨别这些主题中的哪些出现在新文档中。该模型还可以挑选出文档的哪些部分涵盖了哪些主题。</p><p id="e1c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想想维基百科。它有数百万份文档，涵盖了数十万个主题。如果这些都能被自动发现岂不是很棒？以及哪些文档涵盖了哪些主题的更精细的地图。对于那些试图探索维基百科的人来说，这些将是有用的辅助工具。</p><p id="b730" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以发现新出现的主题，因为有关于它们的文档。在一些环境中(如新闻),新的文档不断产生，并且新近性很重要，这将有助于我们发现趋势主题。</p><p id="23cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章涵盖了一个统计上强大且广泛使用的方法来解决这个问题。</p><p id="b51f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">潜在狄利克雷分配</strong></p><p id="8a21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法包括建立主题和文档的显式统计模型。</p><p id="f1b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主题被建模为一组固定单词(词典)的概率分布。这就形式化了“提到这个话题时想到的一组词”。文档被建模为一组固定主题的概率分布。这揭示了文档涵盖的主题。</p><p id="89c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">学习的目的是从文档语料库中发现各种主题的良好单词分布，以及各种文档中的良好主题比例。主题的数量是这种学习的一个参数。</p><p id="b87e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">生成单据</strong></p><p id="8c1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个阶段，它将有助于描述如何从学习到的模型生成合成文档。这将揭示这个模型如何运作的关键方面，我们还没有深入研究。</p><p id="7585" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将选择本文档将涵盖的主题。一种方法是首先从我们的语料库中随机选取一个文档，然后将新文档的主题比例设置为种子文档的主题比例。</p><p id="4025" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将设置文档长度，称之为<em class="lv"> n </em>。</p><p id="4540" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将重复下面的<em class="lv"> n </em>次:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="005f" class="mb mc it lx b gy md me l mf mg">sample a topic from the document’s topic proportions<br/>sample a word from the chosen topic’s words-distribution</span></pre><p id="2bd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将发出一系列的<em class="lv"> n </em>个字。这些单词将会被标注上它们所来自的主题。</p><p id="9830" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成的文档是乱码。从混合话题中抽取的一袋单词。这不是问题——它不是用来阅读的。它确实揭示了哪些词是由哪些话题产生的，这可能是有见地的。</p><p id="fe27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">举例</strong></p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="1de3" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">Lexicon</strong>: {athlete, football, soccer, tennis, computer, smartphone, laptop, printer,Intel, Apple, Google}<br/><strong class="lx iu">Num Topics</strong> : 3<br/><strong class="lx iu">Topic 1</strong>: {athlete, football, soccer, tennis}<br/><strong class="lx iu">Topic 2</strong>: {computer, smartphone, laptop, printer}<br/><strong class="lx iu">Topic 3</strong>: {Intel, Apple, Google}<br/><strong class="lx iu">Topic proportions in a document</strong>: { 2 ⇒ 70%, 3 ⇒ 30% }</span></pre><p id="2262" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面，我们已经用一组单词描述了一个主题。我们将此解释为:集合中的所有单词都是等概率的；词典中剩余的单词概率为零。</p><p id="10d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一个4个字的生成文档。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="8566" class="mb mc it lx b gy md me l mf mg">Topic:  2      3       2          2<br/>Word: laptop Intel smartphone computer</span></pre><p id="94b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主题3在本文中的比例(25%)接近其在抽样分布中的比例(30%)。</p><p id="0fce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">学习</strong></p><p id="42dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像往常一样，这是事情变得特别有趣的地方。</p><p id="32e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们提醒自己学习的目的。它是从文档语料库中发现各种主题的<em class="lv">单词分布</em>，以及各种文档中的<em class="lv">主题比例</em>。简而言之，哪些词描述了哪些主题，哪些主题包含在哪些文档中。</p><p id="177d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将要描述的算法被广泛使用。这也不难理解。这是吉布斯采样的一种形式。</p><p id="7fa3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该算法的工作原理是，首先以某种方式将主题分配给语料库中的各种单词，然后迭代地改进这些分配。在其操作期间，该算法跟踪当前分配的某些统计数据。这些统计数据有助于算法的后续学习。当算法终止时，很容易从最终的主题分配中“读出”每个主题的单词分布和每个文档的主题比例。</p><p id="72d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从描述上一段提到的统计数据开始。它们采用两个计数矩阵的形式:topic_word和doc_topic。两者都是从语料库中单词的当前主题分配中得到的。topic_word( <em class="lv"> t </em>，<em class="lv"> w </em>)统计单词<em class="lv"> w </em>出现topic <em class="lv"> t </em>的次数。doc_topic( <em class="lv"> d </em>，<em class="lv"> t </em>)统计主题<em class="lv"> t </em>在文档<em class="lv"> d </em>中出现的次数。</p><p id="41c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一个数字例子，以确保我们得到了正确的。下面我们看到一个两个文档的语料库，以及对其单词的主题分配。词典是A，B，c。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="0314" class="mb mc it lx b gy md me l mf mg">Doc 1’s words:  A B A C A        Doc 2’s words:  B C C B<br/>Doc 1’s topics: 1 1 1 2 2        Doc 2’s topics: 2 2 2 2</span></pre><p id="11e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，让我们先利用这个机会思考一下我们看到的一些特性。在文档1中，注意A有时被分配给主题1，有时被分配给主题2。如果单词A在两个题目中出现的概率都很高，这就说得通了。在文档2中，请注意B始终被分配给主题2。如果文档2仅涵盖主题2，并且B在主题2的分布中具有正概率，则这似乎是合理的。</p><p id="93a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好，现在来看两个计数矩阵。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="0e69" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">topic_word</strong>:           <strong class="lx iu">doc_topic</strong>:<br/>  <em class="lv">A B C                    1 2</em><br/><em class="lv">1</em> 2 1 0                 <em class="lv">d1</em> 3 2<br/><em class="lv">2</em> 1 2 <strong class="lx iu">3                 </strong><em class="lv">d2</em> 0 <strong class="lx iu">4</strong></span></pre><p id="4a92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们加粗了一些有点醒目的条目。也许doc2更喜欢主题2。可能题目2更喜欢c字。</p><p id="a71f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，开始讲解学习吧。第一步是用随机抽样的主题标注语料库中的单词。这听起来很容易。实际上还有更多。与其硬编码这种随机抽样，不如抽取合适的先验分布。这给了我们一个潜在的强大机制来注入领域知识或来自外部文本分析的结果。</p><p id="cc99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种基于先验的机制工作如下。首先，我们复制前面介绍的两个矩阵。分别叫它们prior_topic_word和prior_doc_topic。如前所述，这些矩阵中的条目是计数。这些计数捕捉了我们先前的信念。</p><p id="5b95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些先验矩阵影响主题的初始分配。随着学习的进展，这种影响会逐渐减少，尽管不会为零。</p><p id="61b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们如何准确地从这些计数中抽取最初分配的题目？首先，我们计算</p><blockquote class="mh mi mj"><p id="b62d" class="kz la lv lb b lc ld ju le lf lg jx lh mk lj lk ll ml ln lo lp mm lr ls lt lu im bi translated"><em class="it">P</em>(w | t)= prior _ topic _ word(t，w)/sum _ w<em class="it">'</em>(prior _ topic _ word(t，w ')<br/><em class="it">P</em>(t | d)= prior _ doc _ topic(t，d)/sum_t' (prior_doc_topic(t '，d)</p></blockquote><p id="723b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> P </em> ( <em class="lv"> w </em> | <em class="lv"> t </em>)只是题目<em class="lv"> t </em>的赋值的分数，其单词为<em class="lv"> w </em>。<em class="lv">P</em>(<em class="lv">t</em>|<em class="lv">d</em>)只是文档<em class="lv"> d </em>中指定题目为<em class="lv"> t </em>的单词的零头。</p><p id="48f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们从这些作业中抽取样本。更具体地说，我们从分子为<em class="lv">P</em>(<em class="lv">w</em>|<em class="lv">t</em>)<em class="lv">P</em>(<em class="lv">t</em>|<em class="lv">d</em>)的分布中抽取文档<em class="lv"> d </em>中单词<em class="lv"> w </em>的主题。</p><p id="96ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可以这样理解。<em class="lv">P</em>(<em class="lv">w</em>|<em class="lv">t</em>)<em class="lv">P</em>(<em class="lv">t</em>|<em class="lv">d</em>)正是我们生成模型中文档<em class="lv"> d </em>中生成单词<em class="lv"> w </em>的概率。作为<em class="lv"> t </em>的一个函数，它记录了在此过程中使用<em class="lv"> t </em>的可能性。</p><p id="b364" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们讨论在两个先前的矩阵中设置这些计数的值。对于我们在这里的目的，我们所关心的是没有一个主题比另一个更好。这种偏好是不必要的偏见。我们可以通过将每个矩阵中的所有计数设置为相同的正数来实现这一点。1是最简单的选择。奥卡姆剃刀推理。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="f977" class="mb mc it lx b gy md me l mf mg">prior_topic_word(<em class="lv">t</em>,<em class="lv">w</em>)=1 for every topic <em class="lv">t</em> and word <em class="lv">w</em><br/>prior_doc_topic(<em class="lv">d</em>,<em class="lv">t</em>)=1 for every document <em class="lv">d</em> and topic <em class="lv">t</em></span></pre><p id="2bf2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好的，所以题目分配将会从这些计数中抽样，并且是均匀随机的。</p><p id="34c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个初始任务之后，我们将重复做以下事情，希望改进这个任务，因此，我们的模型从中学习到了:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="d71d" class="mb mc it lx b gy md me l mf mg">1. Pick a word <em class="lv">w</em> from a document <em class="lv">d </em>in the corpus<br/>2. Sample a topic <em class="lv">t</em>’ from the distribution whose numerator is <br/>   <em class="lv">Q</em>(<em class="lv">w|t</em>)<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)<br/>3. Set <em class="lv">w</em>’s topic in <em class="lv">d</em> to <em class="lv">t</em>’.</span></pre><p id="2ff9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">什么是<em class="lv"> Q </em> ( <em class="lv"> w|t </em>)？这是我们目前认为的从话题<em class="lv"> t </em>中产生单词<em class="lv"> w </em>的可能性。其实好的<em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">t</em>)的价值观才是我们所追求的。这些将形成最终的特定主题单词分布。</p><p id="78bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在开始学习之前，我们将先前对这种分布的信念捕获到<em class="lv">P</em>(<em class="lv">w</em>|<em class="lv">t</em>)中。随着学习的进行，<em class="lv">P</em>(<em class="lv">w</em>|<em class="lv">t</em>)开始被修正为后验信念<em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">t</em>)。</p><p id="c55d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)的解释也差不多。我们之前对每个文档的主题比例的看法分为<em class="lv">P</em>(<em class="lv">t</em>|<em class="lv">d</em>)。随着学习的进行，这些被修改成<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)。</p><p id="3300" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">t</em>)和<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)是如何计算的？在学习过程中的任何时候，考虑将主题分配给语料库中的各种单词。根据这个主题分配，我们可以计算topic_word和doc_topic矩阵中的计数。接下来，我们执行以下操作</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="ba22" class="mb mc it lx b gy md me l mf mg">posterior_topic_word = topic_word + prior_topic_word<br/>posterior_doc_topic  = doc_topic  + prior_doc_topic</span></pre><p id="e7b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意'+'是矩阵加法。从计数矩阵的后验版本中，我们可以计算出<em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">t</em>)和<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)。正如我们从之前的版本中计算出<em class="lv">P</em>(<em class="lv">w</em>|<em class="lv">t</em>)和<em class="lv">P</em>(<em class="lv">t</em>|<em class="lv">d</em>)。</p><p id="76e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">一些直觉</strong></p><p id="6901" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们如何知道这个迭代过程实际上改进了主题分配？我们不会给出证明。取而代之的是一些直觉。</p><p id="ec63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先注意，可以通过将语料库中所有出现的单词<em class="lv"> w </em>乘以各种<em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">t</em>)<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)项，来获得语料库中单词的主题分配的总体质量。这里<em class="lv"> d </em>表示<em class="lv"> w </em>出现的文档，而<em class="lv"> t </em>表示分配给该事件的主题。</p><p id="206f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将揭示分数函数具有某些可取的特征。</p><p id="5c88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">该得分函数有利于主题特异性</strong></p><p id="955c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们所说的“话题特异性”是指<em class="lv"> Q </em> ( <em class="lv"> w </em> | <em class="lv"> t </em>)只集中在几个话题上。这是那些与特定主题紧密相关的单词的期望属性。我们来细说一下。考虑一个多样化的语料库，如维基百科。假设我们的目标是发现它所涵盖的各种各样的主题。考虑一下单词<em class="lv"> cat </em>。它的主题特异性很高，也就是说，它只唤起这些主题中的几个。理应如此。因此，分数函数偏向于支持主题特异性是一件好事。</p><p id="22c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，不是每个单词都应该是特定主题的。例如<em class="lv">中的</em>。稍后我们将讨论在这种情况下抵消主题特异性的独立机制。首先，我们来解释一下主题特异性偏差。我们称特定主题的单词<em class="lv">为信息型</em>。</p><p id="f584" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑在语料库中同一个单词<em class="lv"> w </em>出现1次<em class="lv"> n </em>让<em class="lv"> T </em> 1和<em class="lv"> T </em> 2成为这些<em class="lv"> n </em>事件的两个不同的主题分配。在<em class="lv"> T </em> 1中的所有主题都是不同的。称这个集合为{ <em class="lv"> 1 </em>、<em class="lv"> 2 </em>、<em class="lv"> 3 </em>、…、<em class="lv"> n </em> }。<em class="lv"> T </em> 2里的所有题目都一样，最大化<em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">T</em>)的那个。把这个题目叫做<em class="lv"> tmax </em>。<em class="lv"> T </em> 1在<em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">T</em>)下的可能性是<em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">1<em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">2</em>)*…*<em class="lv">Q</em>(<em class="lv"><em class="lv"> T </em> 2在<em class="lv"> Q </em>下的可能性(<em class="lv"> w </em> | <em class="lv"> t </em>)是<em class="lv">q</em>(<em class="lv">w</em>|<em class="lv">tmax</em>)^<em class="lv">n</em>。当<em class="lv"> w </em>信息丰富且<em class="lv"> n </em>不太小时，T<em class="lv">T</em>2的可能性会远远高于<em class="lv"> T </em> 1。</em></em></p><p id="5915" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一分析可以总结为</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="439d" class="mb mc it lx b gy md me l mf mg">The score function encourages informative words to stay on topic</span></pre><p id="e8b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">该得分函数有利于文档特异性</strong></p><p id="4994" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们所说的“文档特异性”是指文档只涵盖几个主题。文件往往是具体的。问题是分数函数(以及随之而来的学习算法)是否利用了这种趋势来做得更好。答案是肯定的。如下所述。</p><p id="68c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑一个有<em class="lv"> n </em>个单词的文档d。让<em class="lv"> T </em> 1和<em class="lv"> T </em> 2作为两个不同的题目分配给它的单词。<em class="lv"> T </em> 1中的所有主题都是截然不同的。称这个集合为{ <em class="lv"> 1 </em>，<em class="lv"> 2 </em>，<em class="lv"> 3 </em>，…，<em class="lv"> n </em> }。<em class="lv"> T </em> 2中的所有题目都一样，这次是<em class="lv"> d </em>中比例最高的一个。称之为tmax 。<em class="lv"> T </em> 1在<em class="lv">Q</em>(<em class="lv">T</em>|<em class="lv">d</em>)下的可能性是<em class="lv">Q</em>(<em class="lv">1</em>|<em class="lv">d</em>Q(2 |<em class="lv">d</em>)*…*<em class="lv">Q</em>(<em class="lv">n</em>|<em class="lv">d</em>)。<em class="lv"> T </em> 2在<em class="lv">q</em>(<em class="lv">t</em>|<em class="lv">d</em>)下的可能性是<em class="lv">q</em>(<em class="lv">tmax</em>|<em class="lv">d</em>)^<em class="lv">n</em>。显然，T2可以高得多，尤其是对于大型文档。</p><p id="d269" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一分析可以总结为</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="ae43" class="mb mc it lx b gy md me l mf mg">The score function encourages documents to stay on topic</span></pre><p id="de07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">这些影响有时会相互竞争！</strong></p><p id="f640" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑一个特别常见的词:<em class="lv"> the </em>。合理假设它出现在几乎所有文档中。主题一致性倾向于为所有这些事件分配相同的主题。文件特异性抗议，因为这将迫使所有这些文件涵盖这一个主题。</p><p id="b94f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们想象一下这可能会如何发展。主题一致性可能会让这些词失去吸引力。他们被分配的话题可能只是“随波逐流”，以邻居中任何话题的身份出现。当然，这些作业的可能性的主题一致性组件可能会减少。另一方面，文档特异性部分将会增加。</p><p id="2b07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更多关于学习算法</strong></p><p id="0883" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经看到分数函数有好的偏向。这只有在学习算法很好地利用它们的情况下才有帮助。所以让我们更深入地讨论一下算法。</p><p id="1c2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先注意到，该算法通过局部优化全局主题分配质量分数来工作。仅这一点就表明它正在关注偏见。</p><p id="d4ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将在一个简单的例子中进行一两次迭代。这将帮助读者更好地感受它的“局部优化”行为。这比一些人想象的更加微妙。</p><p id="fd3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">例子</strong></p><p id="c134" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将词典设置为{A，B}。我们的语料库将有两个文档，AAA (d1)和BBB (d2)。(虽然这些听起来可能很傻，但这个练习将会很有教育意义。)我们的目标是给语料库添加两个主题。</p><p id="ae24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们将所有先前的计数设置为0.0000001。它仍然会产生均匀随机的初始赋值，尽管简化了我们的数值计算。</p><p id="a21a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设下面是最初的任务</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="b86c" class="mb mc it lx b gy md me l mf mg">d1     d2<br/>AAA    BBB<br/>121    122</span></pre><p id="f88c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从该作业中学到的模型是</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="8a28" class="mb mc it lx b gy md me l mf mg">Q(A|1)  = ⅔ , Q(B|1) = ⅓    // these sum to 1<br/>Q(A|2)  = ⅓ , Q(B|2) = ⅔    // these sum to 1<br/>Q(1|d1) = ⅔ , Q(2|d1)= ⅓    // these sum to 1<br/>Q(1|d2) = ⅓, Q(2|d2) = ⅔    // these sum to 1</span></pre><p id="ca14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在想象从<em class="lv"> d </em> 1中选取一个单词，并重新采样它的主题。(我们本可以选择<em class="lv"> d </em> 2，但理由是相似的。)有三种可能的结果:121 → 121，121 → 1 <strong class="lb iu"> 1 </strong> 1，121 → 12 <strong class="lb iu"> 2 </strong>或<strong class="lb iu"> 2 </strong> 21。第三种方法实际上就是我们开始的地方:交换主题名称并重新排列顺序。所以第一个和第三个结果实际上让我们回到了同样的状态。所以我们放大第二个:121 → 111。这种转变的概率是正的。(事实上相当高，因为从2 → 1的转换提高了主题可能性分量和文档可能性分量。)因此，如果我们一直重复这个过程，在某一点上，所有d1将被分配相同的主题<em class="lv"> t </em> (=1或2)。</p><p id="1a5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们从这个赋值<em class="lv"> t </em>到<em class="lv"> d </em> 1重新估计模型的各种参数。<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>1)现在是1。Q  (A| <em class="lv"> t </em>)很可能比我们开始这个过程时的Q  (A|1)要高。我们已经达到了一种难以摆脱的“快乐状态”。</p><p id="3308" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">变体</strong></p><p id="b81b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们看看这个算法的一些变体。这些包括修改下面的步骤2。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="3c5e" class="mb mc it lx b gy md me l mf mg">1. Pick a word <em class="lv">w</em> from a document <em class="lv">d </em>in the corpus<br/>2. <strong class="lx iu">Sample a topic <em class="lv">t</em>’ from the distribution whose numerator is <br/>   <em class="lv">Q</em>(<em class="lv">w|t</em>)<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)</strong><br/>3. Set <em class="lv">w</em>’s topic in <em class="lv">d</em> to <em class="lv">t</em>’.</span></pre><p id="5fc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将步骤2替换为“将<em class="lv"> t' </em>设置为最大化<em class="lv">Q</em>(<em class="lv">w | t</em>)<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)的主题”。这被称为最大似然估计。在我们的设置中，这产生了一个贪婪的算法。</p><p id="4f27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种变体因其简单性而吸引人，并且可能收敛得更快。然而，它更容易陷入次优的局部最优。</p><p id="1fa0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的下一个变体引入了一个称为<em class="lv">温度</em>的参数，它可以被改变以跨越从吉布斯采样器到贪婪算法的行为范围。这就是这种算法的吸引力所在。不过，这确实提出了一个新问题:如何设定温度。</p><p id="7721" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">撇开温度不谈，让我们看看它是如何工作的。考虑单词<em class="lv"> w </em>，其当前主题<em class="lv"> t </em>正被考虑重新分配。对于我们评估的每个主题</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="4f3b" class="mb mc it lx b gy md me l mf mg">delta(t’) = — [ log <em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">t</em>’)<em class="lv">Q</em>(<em class="lv">t</em>’|<em class="lv">d</em>) — log <em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">t</em>)<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>) ]</span></pre><p id="91ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不会在这里解释为什么日志。我们将注意到，delta(<em class="lv">t</em>’)小于0对应于在这种情况下<em class="lv">t</em>’比t更适合。从<em class="lv"> t </em>到t’的转换可以被视为减少能量(或下坡)的移动。</p><p id="21a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来会发生什么？我们会定性描述。所以现在我们知道了delta( <em class="lv"> t </em>')对于<em class="lv"> t </em>'的各个值，包括<em class="lv"> t </em>。根据这些增量值，该算法定义了主题上的合适的概率分布。这种分布由温度参数化。</p><p id="b2c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在高温下，甚至允许“上坡移动”，即移动到delta值为正的主题。这样的移动，虽然在当前的主题分配上是倒退的，但是可以帮助逃脱局部能量最小值。</p><p id="ea9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在低温下，分布倾向于δ值为负的移动。在零度的极端情况下，这会产生贪婪的行为。</p><p id="6c1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更丰富的前科</strong></p><p id="bbd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们已经将prior counts设置为1。这里我们考虑更丰富的设定。</p><p id="4a06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之前我们注意到，某些单词最好是特定于主题的。我们能找到这样的单词并把它们的主题特异性偏好记录到先前的计数中吗？这可以加快后续的学习。</p><p id="a0a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个明智的方法。首先，我们将所有先前的计数设置为1。接下来，对于每个单词<em class="lv"> w，</em>我们将计算<em class="lv"> n </em> - <em class="lv"> n </em> ( <em class="lv"> w </em>)。这里<em class="lv"> n </em>是语料库中的文档总数，<em class="lv"> n </em> ( <em class="lv"> w </em>)是单词<em class="lv"> w </em>出现的文档数。把<em class="lv">n</em>-<em class="lv">n</em>(<em class="lv">w</em>)看成是<em class="lv"> w </em>的某种“逆文档频率”。接下来，我们将对每个单词分别进行以下操作。我们将从众多话题中随机挑选一个话题。我们会把<em class="lv">n</em>-<em class="lv">n</em>(<em class="lv">w</em>)加到前面的count word_topic( <em class="lv"> t </em>，<em class="lv"> w </em>)。</p><p id="9eab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个想法是把不常见的单词映射到特定的主题。由于主题选择是随机的，不同的单词可能会映射到不同的主题。</p><p id="7053" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个先验是否过于偏颇？我们可以很容易地软化它，如果我们觉得是这样的话。有很多可能性。一种是将<em class="lv">n</em>-<em class="lv">n</em>(<em class="lv">w</em>)替换为<em class="lv">log n</em>-<em class="lv">log n</em>(<em class="lv">w</em>)。第二种方法是在放大一个单词的先前计数时，对多个主题(尽管不是很多)进行采样。</p><p id="2e56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">其他建模增强</strong></p><p id="2cc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">改进模型还需要考虑哪些方面？首先，让我们阐明它的假设:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="8c17" class="mb mc it lx b gy md me l mf mg">1. The topics a document covers are sampled independently. They<br/>   are not checked for compatibility.<br/>2. Word proximity in a document is not considered.<br/>3. The hierarchical structure among topics is not modeled.</span></pre><p id="689f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们分别说一说。主题兼容性可能很重要。一份文件更有可能涵盖<em class="lv">科技公司</em> &amp; <em class="lv">电脑</em>而不是<em class="lv">科技公司</em> &amp; <em class="lv">体育。单词的接近度也很重要。重复出现在彼此附近的两个单词比相隔几千个单词出现的更有可能出现在同一个主题上。主题层次结构更好地模拟了文档。一种常见的写作模式是让文档涵盖一个广泛的主题及其各种子主题。</em></p><p id="7b4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面我们描述如何解决这些问题。在某些用例中，放松可能会提高模型的准确性，尽管可能会增加模型的复杂性或可学性。他们也提供了注入领域知识的机会。此外，在某些方面甚至有助于学习！</p><p id="a6ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体如何进行很大程度上取决于用例。</p><p id="13cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">主题兼容性</strong></p><p id="585d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这涉及到添加一个马尔可夫模型。它的状态就是主题。它的转换模型语料库中的主题之间的兼容性。</p><p id="99a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在对文档中的单词主题进行采样时，如何使用这个模型？我们需要适当延长<em class="lv"> Q </em> ( <em class="lv"> t </em> | <em class="lv"> d </em>)。这个扩展最简单的描述是想象从中抽取主题。</p><p id="e93c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种抽样可以被看作是在马尔可夫模型上的随机行走，可能偶尔会重新开始。我们首先从<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)中选择一个主题。接下来，我们执行以下操作。大多数情况下，我们从这个话题的状态开始走一个转换，其概率在那个转换上。偶尔我们会从<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)重新采样，即跳转到新的话题。</p><p id="6590" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种随机游走诱导出一种新的分布，称之为<em class="lv">Q</em>'(<em class="lv">t</em>|<em class="lv">d</em>，M)它同时受到马尔可夫模型M和<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)的影响。马尔可夫模型是一个语料库级的结构。使用<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)将其行为定制为文档<em class="lv"> d </em>。</p><p id="6038" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">马尔可夫模型的参数是如何学习的？这个比较好解释。到目前为止，我们的学习算法已经在为语料库文档中的单个单词分配主题的层面上工作。马尔可夫模型的参数完全由这种分配决定。</p><p id="d378" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，对于文档中每一对主题分配的<em class="lv"> s </em>、<em class="lv"> t </em>，我们对弧<em class="lv"> s </em> → <em class="lv"> t </em>和<em class="lv"> t </em> → <em class="lv"> s </em>进行递增计数，除非<em class="lv"> s </em>等于<em class="lv"> t </em>，在这种情况下只递增一次。</p><p id="781e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，随着主题分配的改进，主题兼容性模型的参数也随之改进。两者协同作用。</p><p id="bc58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从更广的范围来看待这种协同作用。随着我们看到越来越多的关于相同两个主题的文档，马尔可夫模型开始了解到这些主题是兼容的。这种改进的学习导致其他地方的主题分配的改进。</p><p id="4607" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">字的接近度</strong></p><p id="5644" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可以通过适当延长<em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">t</em>来合并。具体来说，让<em class="lv"> t </em>的选择不仅受到<em class="lv"> w </em>的影响，也受到<em class="lv"> w </em>附近的词的影响。</p><p id="83b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们把这个正式化。设<em class="lv">W</em>(<em class="lv">d</em>)=<em class="lv">W</em>(-<em class="lv">d</em>)，…，<em class="lv"> w </em>，…，<em class="lv"> w </em> (+ <em class="lv"> d </em>)表示以<em class="lv"> w </em>为中心的长度为2 <em class="lv"> d </em> +1的单词序列。这里d是一个非负整数。接下来，我们将<em class="lv">Q</em>(<em class="lv">W</em>|<em class="lv">t</em>)扩展为<em class="lv">Q</em>(<em class="lv">W</em>(<em class="lv">d</em>)|<em class="lv">t</em>)。</p><p id="fa61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从学习的角度来看，这是否使我们的模型变得复杂了？有了合适的假设，幸运的是没有。我们假设<em class="lv"> W </em> ( <em class="lv"> d </em>)中的字在给定<em class="lv"> t </em>的情况下是条件独立的。在这种假设下，我们得到</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="0e31" class="mb mc it lx b gy md me l mf mg"><em class="lv">Q</em>(<em class="lv">W</em>(<em class="lv">d</em>)|<em class="lv">t</em>) = <em class="lv">Q</em>(<em class="lv">w</em>(-<em class="lv">d</em>)|<em class="lv">t</em>)*…*<em class="lv">Q</em>(<em class="lv">w</em>|<em class="lv">t</em>)*…*Q(<em class="lv">w</em>(+<em class="lv">d</em>)|<em class="lv">t</em>)</span></pre><p id="58af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">涉及<em class="lv"> t </em>的RHS术语都是和以前一样的形式。所以学习不需要改变！</p><p id="7432" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管有这样的假设，我们还是得到了使用上下文的好处。其中最主要的是，分配给中心词<em class="lv"> w </em>的主题可以更准确地确定，因为现在我们有了更多的上下文。另一个有用的特征，尤其是在学习的早期，是我们得到了一些话题的连续性。当我们一次滑动窗口<em class="lv"> W </em> ( <em class="lv"> d </em>)一个单词时，分配的主题不太可能改变，因为上下文没有太大变化。相比于<em class="lv"> W </em> (0)。</p><p id="e5c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">主题层次</strong></p><p id="0945" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个高级话题。我们的报道是局部的。</p><p id="d487" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里是关键的观察。比较两个主题的单词分布有助于评估一个主题是否是另一个主题的后代。后代的关键字将倾向于成为祖先关键字的子集。</p><p id="5c15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用这个概念，我们可以在学习过程中的任何时候学习主题的层次结构。即使是一个粗略的学习层次也比没有强。</p><p id="91fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们为主题兼容性所做的那样，我们可以扩展我们的<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)模型，以将所学的层次结构考虑在内。就像这里一样，描述这个扩展最简单的方法是想象一个特定文档的采样主题。</p><p id="e6a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种抽样是在层次结构上随机进行的，可能偶尔会重新开始。我们首先从<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)中选择一个主题。接下来，我们执行以下操作。很多时候我们从这个话题走到一个孩子身上。偶尔我们会从<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)重新取样，也就是跳到层次结构中其他地方的主题。</p><p id="aaab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种随机游走导致了一种新的分布，称之为<em class="lv"> Q </em> '( <em class="lv"> t </em> | <em class="lv"> d </em>，H)，它同时受到主题层次H和<em class="lv"> Q </em> ( <em class="lv"> t </em> | <em class="lv"> d </em>)的影响。主题层次结构是一个语料库级别的结构。使用<em class="lv">Q</em>(<em class="lv">t</em>|<em class="lv">d</em>)将其行为定制为文档d。</p><p id="bbb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">提高可读性的增强功能</strong></p><p id="5461" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文档的可读性不是LDA的主要目标。尽管如此，建模框架提供了注入改进它的机制的机会。所以让我们利用这一点。</p><p id="7623" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从提出问题开始。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="373f" class="mb mc it lx b gy md me l mf mg">1. Topic continuity is not maintained. Neighboring words can jump <br/>   from topic to topic.<br/>2. Topic coherence is not maintained. A topic spews out its words <br/>   independently.</span></pre><p id="c7aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">话题连续性</strong></p><p id="647a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过添加一个具有两种状态的马尔可夫模型来保持话题的连续性:<em class="lv">继续</em>和<em class="lv">切换</em>。<em class="lv">继续</em>继续当前话题，<em class="lv">切换</em>切换到新话题。从<em class="lv">延续</em>到自身的弧应该有很大概率。从<em class="lv">开关</em>到<em class="lv">继续</em>的弧应该有接近1的概率，因为我们几乎肯定想要继续跟随开关。</p><p id="2776" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个模型上的概率很容易从主题作业中学习到。埋在其中的是<em class="lv">继续</em>和<em class="lv">切换</em>事件。这种学习与主题任务的学习相吻合，相互受益。</p><p id="00be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在文档中利用词的邻近性也有助于保持主题的连续性。我们选择涵盖本节方法的原因是，它比单词邻近法更容易实现。</p><p id="1d01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">这也有利于学习</strong></p><p id="8e21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了可读性之外，注入主题连续性机制也有可能提高所学作业的质量。实际上，它充当了平滑正则化器，不支持离群值。</p><p id="fd56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个例子，是我们之前使用的一个例子的扩展。考虑单词<em class="lv"/>，我们假设它出现在语料库的几乎所有文档中。之前我们解释了为什么我们希望分配给<em class="lv">和</em>的特定事件的主题能够在它的邻居中“随波逐流”。主题连续性机制为这种偏好增加了更多权重，因为它明确倾向于“随波逐流”。</p><p id="06d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">话题连贯性</strong></p><p id="a6db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过放松单词袋假设来保持话题的连贯性。相反，使用一阶马尔可夫模型来模拟当前单词对下一个单词的影响。每个主题都有自己的马尔可夫模型。</p><p id="5293" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个主题的马尔可夫模型很容易从语料库中的当前主题分配中(重新)学习。</p><p id="bb94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读</strong></p><p id="2ac2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大卫·布雷。概率主题模型。ACM的通信。2012年<a class="ae ky" href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf" rel="noopener ugc nofollow" target="_blank">http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf</a></p><p id="3487" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大卫·布雷，吴恩达，迈克尔·乔丹。潜在狄利克雷分配。JMLR (3) 2003年第993-1022页。</p><p id="a760" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">汤姆·格里菲斯。潜在狄利克雷分配生成模型中的吉布斯抽样。<a class="ae ky" href="https://people.cs.umass.edu/~wallach/courses/s11/cmpsci791ss/readings/griffiths02gibbs.pdf" rel="noopener ugc nofollow" target="_blank">https://people . cs . umass . edu/~ wallach/courses/S11/cmpsci 791 ss/readings/griffiths 02 Gibbs . pdf</a></p></div></div>    
</body>
</html>