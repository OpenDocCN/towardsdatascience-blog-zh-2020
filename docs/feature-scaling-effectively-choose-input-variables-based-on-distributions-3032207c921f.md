# è¦ç´ ç¼©æ”¾-æ ¹æ®åˆ†å¸ƒæœ‰æ•ˆé€‰æ‹©è¾“å…¥å˜é‡

> åŸæ–‡ï¼š<https://towardsdatascience.com/feature-scaling-effectively-choose-input-variables-based-on-distributions-3032207c921f?source=collection_archive---------9----------------------->

## æ¼”ç¤ºå¦‚ä½•æ˜æ™ºåœ°é€‰æ‹©æ•°å€¼å˜é‡è¿›è¡Œç¼©æ”¾ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§

![](img/466713ccb0a475b028b3d7d52fd85c92.png)

ç…§ç‰‡ç”± Siora Photography åœ¨ Unsplash ä¸Šæ‹æ‘„

åœ¨æ„å»º ML æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°å¤„ç†ç”±ä¸åŒèŒƒå›´ã€å•ä½å’Œæ•°é‡ç»„æˆçš„å„ç§æ•°å€¼å˜é‡çš„æƒ…å†µã€‚ä½œä¸ºæƒ¯ä¾‹ï¼Œæˆ‘ä»¬å°†åœ¨æ„å»ºæ¨¡å‹ä¹‹å‰å¯¹æ‰€æœ‰ç‰¹å¾åº”ç”¨æ ‡å‡†åŒ–æˆ–è§„èŒƒåŒ–æŠ€æœ¯ã€‚ç„¶è€Œï¼Œåœ¨å†³å®šåº”ç”¨å“ªç§æŠ€æœ¯è¿›è¡Œç‰¹å¾ç¼©æ”¾ä¹‹å‰ï¼Œç ”ç©¶æ•°æ®çš„åˆ†å¸ƒæ˜¯è‡³å…³é‡è¦çš„ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºæ ‡å‡†åŒ–å’Œè§„èŒƒåŒ–ä¹‹é—´çš„åŒºåˆ«ï¼Œå¹¶ç†è§£æ•°æ®çš„åˆ†å¸ƒã€‚æœ€åï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•æ ¹æ®ç‰¹å¾çš„é«˜æ–¯å’Œéé«˜æ–¯åˆ†å¸ƒæ¥é€‰æ‹©ç­–ç•¥ï¼Œä»¥æé«˜é€»è¾‘å›å½’æ¨¡å‹çš„æ€§èƒ½ã€‚

## æ ‡å‡†åŒ–ä¸è§„èŒƒåŒ–

è¿™ä¸¤ç§æŠ€æœ¯æœ‰æ—¶å¯ä»¥äº’æ¢ä½¿ç”¨ï¼Œä½†å®ƒä»¬æŒ‡çš„æ˜¯ä¸åŒçš„æ–¹æ³•ã€‚

***æ ‡å‡†åŒ–*** :è¯¥æŠ€æœ¯å°†æ•°æ®è½¬æ¢ä¸ºå¹³å‡å€¼ä¸ºé›¶ï¼Œæ ‡å‡†å·®ä¸º 1ã€‚

***è§„èŒƒåŒ–*** :è¯¥æŠ€æœ¯å°†å˜é‡ä¸­çš„å€¼åœ¨ 0 å’Œ 1 ä¹‹é—´è½¬æ¢ã€‚

æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš®é©¬å°åº¦ç³–å°¿ç—…æ•°æ®é›†ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ç›¸åŒçš„[

```
import pandas as pd
import numpy as np
data = pd.read_csv(â€œPima Indian Diabetes.csvâ€)
data.head()
```

![](img/bbce4e824d6ee5c4f74375c82b7ef129.png)

æ•°æ®é›†çš„å‰å‡ æ¡è®°å½•

ä»ä¸Šé¢æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ•°å€¼å˜é‡åœ¨ä¸åŒçš„èŒƒå›´å†…å˜åŒ–ï¼Œç»“æœå°±æ˜¯ç›®æ ‡å˜é‡ã€‚æˆ‘ä»¬å°†æ‰§è¡Œç¼©æ”¾æŠ€æœ¯å’Œåº”ç”¨é€»è¾‘å›å½’ã€‚

ğŸ‘‰**å°†æ ‡å‡†åŒ–åº”ç”¨äºæ‰€æœ‰ç‰¹å¾å’Œå»ºæ¨¡ã€‚**

ä» sklearn åº“ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ StandardScaler æ¥å®ç°æ ‡å‡†åŒ–ã€‚

```
from sklearn.preprocessing import StandardScaler
Y = data.Outcome
X = data.drop("Outcome", axis = 1)
columns = X.columns
scaler = StandardScaler()
X_std = scaler.fit_transform(X)
X_std = pd.DataFrame(X_std, columns = columns)
X_std.head()
```

![](img/f698a8b2a72074d2ac5fdee812db8ec7.png)

åº”ç”¨æ ‡å‡†åŒ–åè¾“å…¥è¦ç´ çš„è½¬æ¢

è®©æˆ‘ä»¬è¿›è¡Œè®­ç»ƒå¹¶æµ‹è¯•æ ‡å‡†åŒ–ç‰¹å¾çš„åˆ†å‰²ã€‚

```
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X_std, Y, test_size = 0.15, random_state = 45)
```

ç°åœ¨ï¼Œæˆ‘ä»¬å°†å¯¹æ ‡å‡†åŒ–æ•°æ®é›†åº”ç”¨é€»è¾‘å›å½’ã€‚

```
#Building Logistic Regression model on the Standardized variables
from sklearn.linear_model import LogisticRegression
lr_std = LogisticRegression()
lr_std.fit(x_train, y_train)
y_pred = lr_std.predict(x_test)
print('Accuracy of logistic regression on test set with standardized features: {:.2f}'.format(lr_std.score(x_test, y_test)))
```

![](img/ab9710e5a1e0aff193a2e10968bd9b38.png)

å…·æœ‰æ ‡å‡†åŒ–ç‰¹å¾çš„æ¨¡å‹çš„å‡†ç¡®æ€§

ä»ä¸Šé¢æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œåº”ç”¨æ ‡å‡†åŒ–æŠ€æœ¯çš„æ‰€æœ‰ç‰¹å¾çš„æ¨¡å‹çš„å‡†ç¡®åº¦æ˜¯ 72%ã€‚

ğŸ‘‰**å¯¹æ‰€æœ‰ç‰¹å¾å’Œå»ºæ¨¡åº”ç”¨æ ‡å‡†åŒ–ã€‚**

ä» sklearn åº“ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ MinMaxScaler æ¥å®ç°è§„èŒƒåŒ–ã€‚

```
from sklearn.preprocessing import MinMaxScaler
norm = MinMaxScaler()
X_norm = norm.fit_transform(X)
X_norm = pd.DataFrame(X_norm, columns = columns)
X_norm.head()
```

![](img/01e2b544253f252315ca86b91e6d9616.png)

åº”ç”¨å½’ä¸€åŒ–åè¾“å…¥è¦ç´ çš„å˜æ¢

è®©æˆ‘ä»¬è¿›è¡Œè®­ç»ƒå¹¶æµ‹è¯•å½’ä¸€åŒ–ç‰¹å¾çš„åˆ†å‰²ã€‚

```
# Train and Test split of Normalized features
from sklearn.model_selection import train_test_split
x1_train, x1_test, y1_train, y1_test = train_test_split(X_norm, Y, test_size = 0.15, random_state = 45)
```

å¯¹è§„èŒƒåŒ–æ•°æ®é›†åº”ç”¨é€»è¾‘å›å½’ã€‚

```
#Building Logistic Regression model on the Normalized variables
from sklearn.linear_model import LogisticRegression
lr_norm = LogisticRegression()
lr_norm.fit(x1_train, y1_train)
y_pred = lr_norm.predict(x1_test)
print(â€˜Accuracy of logistic regression on test set with Normalized features: {:.2f}â€™.format(lr_norm.score(x1_test, y1_test)))
```

![](img/8308caf9613459ea19e9a65b2338820c.png)

å…·æœ‰å½’ä¸€åŒ–ç‰¹å¾çš„æ¨¡å‹çš„ç²¾åº¦

å½“æ‰€æœ‰ç‰¹å¾éƒ½å½’ä¸€åŒ–æ—¶ï¼Œæ¨¡å‹çš„å‡†ç¡®åº¦ä¸º 74%ã€‚

ğŸ‘‰**äº†è§£ç‰¹å¾åˆ†å¸ƒ**

è®©æˆ‘ä»¬ç”»å‡ºå˜é‡çš„ç›´æ–¹å›¾æ¥ç ”ç©¶åˆ†å¸ƒã€‚

```
# Plotting the histograms of each variable
from matplotlib import pyplot
data.hist(alpha=0.5, figsize=(20, 10))
pyplot.show()
```

![](img/8dfc6aaf8741faf4929b0b0a4cec57ee.png)

æ¯ä¸ªç‰¹å¾çš„ç›´æ–¹å›¾ï¼Œä»¥äº†è§£åˆ†å¸ƒæƒ…å†µ

***é«˜æ–¯åˆ†å¸ƒ*** â€”èº«ä½“è´¨é‡æŒ‡æ•°ï¼Œè¡€å‹ï¼Œè‘¡è„ç³–ã€‚

***éé«˜æ–¯åˆ†å¸ƒ*** â€”å¹´é¾„ã€ç³–å°¿ç—…ã€èƒ°å²›ç´ ã€æ€€å­•ã€çš®è‚¤åšåº¦

## ğŸ‘‰å½’ä¸€åŒ–éé«˜æ–¯ç‰¹å¾å’Œæ ‡å‡†åŒ–ç±»é«˜æ–¯ç‰¹å¾

æœ€åï¼Œæˆ‘ä»¬æ¥åˆ°ä¸€ä¸ªå®éªŒï¼Œç­‰å¾…é€‰æ‹©å˜é‡ï¼Œå¹¶æ ¹æ®åŒä¸€æ•°æ®é›†ä¸Šçš„åˆ†å¸ƒåº”ç”¨ä¸¤ç§ç­–ç•¥ã€‚

ä¸ºäº†åº”ç”¨è¿™ä¸ªç­–ç•¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ sklearn ä¸­çš„åˆ—è½¬æ¢å™¨å’Œç®¡é“æ¦‚å¿µï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦é€šè¿‡å¯¹åˆ—è¿›è¡Œå­é›†åŒ–æ¥å®ç°æ··åˆç±»å‹çš„æŠ€æœ¯ã€‚

å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬æ­£åœ¨ä¸ºé«˜æ–¯å’Œéé«˜æ–¯ç‰¹å¾å¯åŠ¨ä¸åŒçš„æµæ°´çº¿

```
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
**Standardize_Var = ['BMI','BloodPressure', 'Glucose']**
Standardize_transformer = Pipeline(steps=[('standard', StandardScaler())])
**Normalize_Var = ['Age','DiabetesPedigreeFunction','Insulin','Pregnancies','SkinThickness']**
Normalize_transformer = Pipeline(steps=[('norm', MinMaxScaler())])
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç”¨æ ‡å‡†åŒ–å’Œè§„èŒƒåŒ–çš„é€‰æ‹©æ€§ç‰¹å¾å¯¹æ•°æ®å»ºç«‹é€»è¾‘å›å½’æ¨¡å‹ã€‚

```
x2_train, x2_test, y2_train, y2_test = train_test_split(X, Y, test_size=0.2)
**preprocessor = ColumnTransformer(transformers=
 [(â€˜standardâ€™, Standardize_transformer, Standardize_Var),
 (â€˜normâ€™, Normalize_transformer, Normalize_Var)])**clf = Pipeline(steps=[(â€˜preprocessorâ€™, preprocessor),
 (â€˜classifierâ€™, LogisticRegression(solver=â€™lbfgsâ€™))])
clf.fit(x2_train, y2_train)
print(â€˜Accuracy after standardizing Gaussian distributed features and normalizing Non-Gaussian features: {:.2f}â€™.format(clf.score(x2_test, y2_test)))
```

![](img/291ec445513690c05aadc122e8bc5468.png)

ğŸ‘‰**æœ€ç»ˆå…³é”®ç»†èŠ‚**

ä¸‹é¢æ˜¯è¿„ä»Šä¸ºæ­¢æˆ‘ä»¬å·²ç»å»ºç«‹çš„ä¸åŒæ¨¡å‹çš„ç²¾åº¦ç»†èŠ‚ã€‚

æ‰€æœ‰ç‰¹å¾æ ‡å‡†åŒ–åçš„ç²¾åº¦: **0.72**

æ‰€æœ‰ç‰¹å¾å½’ä¸€åŒ–åçš„ç²¾åº¦: **0.74**

***å¯¹é«˜æ–¯åˆ†å¸ƒç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¯¹éé«˜æ–¯åˆ†å¸ƒç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–åçš„ç²¾åº¦:0.79***

## æ‘˜è¦

å½“æˆ‘ä»¬å¤„ç†åŸºäºæ¢¯åº¦ä¸‹é™çš„ç®—æ³•(çº¿æ€§å’Œé€»è¾‘å›å½’ã€ç¥ç»ç½‘ç»œ)å’ŒåŸºäºè·ç¦»çš„ç®—æ³•(KNNã€K å‡å€¼ã€SVM)æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ‰§è¡Œç‰¹å¾ç¼©æ”¾ï¼Œå› ä¸ºè¿™äº›ç®—æ³•å¯¹æ•°æ®ç‚¹çš„èŒƒå›´éå¸¸æ•æ„Ÿã€‚åœ¨å¤„ç†åŸºäºæ ‘çš„ç®—æ³•æ—¶ï¼Œè¿™ä¸€æ­¥ä¸æ˜¯å¼ºåˆ¶æ€§çš„ã€‚

æœ¬æ–‡çš„ä¸»è¦é‡ç‚¹æ˜¯è§£é‡Šæ•°æ®çš„åˆ†å¸ƒå¦‚ä½•åœ¨ç‰¹å¾ç¼©æ”¾ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œä»¥åŠå¦‚ä½•é€‰æ‹©åŸºäºé«˜æ–¯å’Œéé«˜æ–¯åˆ†å¸ƒçš„ç­–ç•¥æ¥æé«˜æ¨¡å‹çš„æ•´ä½“ç²¾åº¦ã€‚

ä½ å¯ä»¥ä»æˆ‘çš„ GitHub [ [profile](https://github.com/SushmithaPulagam/FeatureScaling-with-Distributions) ]ä¸­è·å¾—å®Œæ•´çš„ä»£ç 

æ„Ÿè°¢é˜…è¯»ï¼Œå¿«ä¹å­¦ä¹ ï¼ğŸ™‚