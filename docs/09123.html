<html>
<head>
<title>Boost your model’s performance with these fantastic libraries</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用这些神奇的库来提高模型的性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/boost-your-models-performance-with-these-fantastic-libraries-8dc10579b7ff?source=collection_archive---------20-----------------------#2020-06-30">https://towardsdatascience.com/boost-your-models-performance-with-these-fantastic-libraries-8dc10579b7ff?source=collection_archive---------20-----------------------#2020-06-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b39d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">将 XGBoost 和 CatBoost 应用到您的机器学习模型中！</h2></div><blockquote class="kf kg kh"><p id="3d89" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">质量取决于准确性和完整性。</p></blockquote><p id="445e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi li translated">公司使用机器学习模型来做出实际的商业决策，更准确的模型结果会带来更好的决策。错误的代价可能是巨大的，但是优化模型精度可以降低这种代价。机器学习模型准确性是一种衡量标准，用于根据输入或训练数据确定哪个模型最擅长识别数据集中变量之间的关系和模式。一个模型越能概括“看不见的”数据，它就能产生越好的预测和洞察力，从而带来更多的商业价值。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/19221ae5d15b1a9946766f64ae9965fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDfs0CPpxBrJTK8b-f1DWg.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">谷歌图片</p></figure><p id="c2af" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kl ir">癌症预测数据集</strong></p><p id="4918" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我选择的数据集是乳腺癌预测数据集。我们需要预测癌症是恶性的还是良性的。每一行对应一个患者，对于这些患者中的每一个，我们有几个特征，从凝块厚度，细胞大小的均匀性等。所有这些特征都是肿瘤的特征。如果我们得到 2 级，结果是良性的，如果我们得到 4 级，结果是恶性的。所以类别变量成为我们的因变量，告诉我们它是良性的还是恶性的。我已经建立了决策树模型，并实现了 95.3%的准确率，这是与其他分类模型相比最高的。</p><p id="2a52" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在这之后，我建立了 XGBoost 模型，我在同一个数据集上对它进行了训练，我想看看准确性是否跨越了决策树分类模型。我不仅在测试集上训练了它，而且还使用了 K-fold 交叉验证，我们将在 10 个测试集上测试它，以便我们可以获得相关的准确性度量。</p><p id="8a0b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">现在让我们开始实施吧！</p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="2c5a" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Importing the libraries</strong></span><span id="b17c" class="mm mn iq mi b gy ms mp l mq mr">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span></pre><p id="854a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">首先，我们导入实现所需的库。</p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="3540" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Importing the dataset</strong></span><span id="e7e6" class="mm mn iq mi b gy ms mp l mq mr">dataset = pd.read_csv('/content/Data.csv')<br/>X = dataset.iloc[:,:-1].values<br/>y = dataset.iloc[:,-1].values</span></pre><p id="fe2d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">x 包含患者的所有特征，并由所有独立变量组成。y 包括因变量，即类别变量。</p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="a5d8" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Splitting the dataset into Training set and Test set</strong></span><span id="1398" class="mm mn iq mi b gy ms mp l mq mr">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)</span></pre><p id="2849" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">接下来，我们将数据集分为训练集和测试集，前者用于训练模型，后者用于评估结果。</p><p id="d5f2" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在此之后，我们建立了三个不同的模型，并比较了所有模型的准确性。</p><p id="0515" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kl ir">决策树分类</strong></p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/632ef3a93cc786e1171de5694ed36460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*OMTTp_qj_UQ3j4o1NEyZ4g.png"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">谷歌图片</p></figure><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="8b42" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Feature Scaling</strong></span><span id="a204" class="mm mn iq mi b gy ms mp l mq mr">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><p id="d63e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">由于变量值的范围变化很大，我们需要应用特征缩放，以便所有变量都缩小到一个可比较的范围。我们只需要对决策树分类应用特征缩放，而不需要对 XGBoost 和 CatBoost 应用特征缩放。</p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="d9c7" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Training the Decision Tree Classification model on the Training set</strong></span><span id="4c7b" class="mm mn iq mi b gy ms mp l mq mr">from sklearn.tree import DecisionTreeClassifier<br/>dtc = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)<br/>dtc.fit(X_train, y_train)</span></pre><p id="229c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">导入 DecisionTreeClassifier 类，然后创建该类的一个实例。然后应用拟合函数在训练集上训练模型。</p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="a49c" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Making the Confusion Matrix for Decision Tree</strong></span><span id="90be" class="mm mn iq mi b gy ms mp l mq mr">from sklearn.metrics import confusion_matrix, accuracy_score<br/>y_pred = dtc.predict(X_test)<br/>cm = confusion_matrix(y_test, y_pred)<br/>print(cm)<br/>accuracy_score(y_test, y_pred)</span><span id="5573" class="mm mn iq mi b gy ms mp l mq mr">[[103   4]<br/> [  3  61]]</span><span id="f51f" class="mm mn iq mi b gy ms mp l mq mr">0.9590643274853801</span></pre><p id="1aa4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">从度量模块导入混淆矩阵和准确度分数函数。</p><p id="bbe6" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">混淆矩阵显示出来，accuracy_score 给出了我们模型的准确度。这个模型的准确率我已经达到了 95.9%。</p><p id="8755" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">当我们在不同的测试集上测试我们的模型的性能时，我们获得了不同的精度。这就是为什么我们找到 10 个精度，然后取所有这些精度的平均值，找到我们模型的最佳精度。所以这种方法被称为 K 重交叉验证法。</p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="e93e" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Applying the K-Fold Cross Validation for Decision Tree</strong></span><span id="f81f" class="mm mn iq mi b gy ms mp l mq mr">from sklearn.model_selection import cross_val_score<br/>accuracies = cross_val_score(estimator = dtc, X = X_train, y = y_train, cv = 10)<br/>print("Accuracy:{:.2f} %".format(accuracies.mean()*100))<br/>print("Standard Deviation:{:.2f} %".format(accuracies.std()*100))</span><span id="5602" class="mm mn iq mi b gy ms mp l mq mr">Accuracy:94.53 % <br/>Standard Deviation:1.91 %</span></pre><p id="32c4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">应用此方法后，精度变量包含获得的所有精度的列表。然后我们打印出模型的总体精度和标准偏差。因此，在各种测试集上评估该模型，并且评估每个测试集的准确性。在所有测试集上验证后达到的准确度为 94.53%，标准偏差为 1.91%，这是相当低且相当好的。</p><p id="3dbd" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kl ir"> XGBOOST </strong></p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/90e662a20eb1d362c86deadf5b7aac13.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*v8dqluuDvZn1EXpFxmc7VQ.jpeg"/></div><p class="md me gj gh gi mf mg bd b be z dk translated">谷歌图片</p></figure><p id="38a7" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><a class="ae mv" href="https://xgboost.ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir"> XGBoost </strong> </a>是一个优化的分布式梯度增强库，旨在高效、灵活、可移植。所以它是一个基于决策树的集成机器学习算法，使用了<a class="ae mv" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">梯度提升</a>框架。</p><p id="338b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">要安装 XGBoost，您可以参考本<a class="ae mv" href="https://xgboost.readthedocs.io/en/latest/build.html" rel="noopener ugc nofollow" target="_blank">文档</a></p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="f668" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Training XGBoost to the training set</strong></span><span id="d078" class="mm mn iq mi b gy ms mp l mq mr">from xgboost import XGBClassifier<br/>xgboost=XGBClassifier()<br/>xgboost.fit(X_train,y_train)</span></pre><p id="fbb5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">现在我已经用一个惊人的库 XGBoost 库训练了这个模型。</p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="85e2" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Making the Confusion Matrix for XGBoost</strong></span><span id="de84" class="mm mn iq mi b gy ms mp l mq mr">from sklearn.metrics import confusion_matrix, accuracy_score<br/>y_pred = xgboost.predict(X_test)<br/>cm = confusion_matrix(y_test, y_pred)<br/>print(cm)<br/>accuracy_score(y_test, y_pred)</span><span id="9339" class="mm mn iq mi b gy ms mp l mq mr">[[84  3]<br/> [ 0 50]]</span><span id="8d60" class="mm mn iq mi b gy ms mp l mq mr">0.9781021897810219</span></pre><p id="adbc" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">最初，用决策树建立的模型准确率为 95.9%。但是在 XGBoost 之后，准确率达到了 97.8%，令人印象深刻。</p><p id="9d2d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">类似地，我们对 XGBoost 模型执行 K-Fold 交叉验证。</p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="022f" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Applying the K-Fold Cross Validation for XGBoost</strong></span><span id="2cb1" class="mm mn iq mi b gy ms mp l mq mr">from sklearn.model_selection import cross_val_score<br/>accuracies = cross_val_score(estimator = xgboost, X = X_train, y = y_train, cv = 10)<br/>print("Accuracy:{:.2f} %".format(accuracies.mean()*100))<br/>print("Standard Deviation:{:.2f} %".format(accuracies.std()*100))</span><span id="d270" class="mm mn iq mi b gy ms mp l mq mr">Accuracy:96.53 % <br/>Standard Deviation:2.07 %</span></pre><p id="da3b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们的模型的整体准确率为 96.53%，这是非常酷的。</p><p id="2b37" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这是我们能达到的最高精度吗？让我们试着用其他库来测试它，看看我们是否能击败这个精度。</p><p id="0049" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><strong class="kl ir"> CATBOOST </strong></p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mw"><img src="../Images/bac77d6dfbfdbfbb36dc12c3b0803aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6fsHRHo67XviEZtP7Qz8Tw.png"/></div></div><p class="md me gj gh gi mf mg bd b be z dk translated">谷歌图片</p></figure><p id="55e4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated"><a class="ae mv" href="https://catboost.ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir"> CatBoost </strong> </a>是一个高性能的开源库，用于决策树上的梯度提升。CatBoost 是一种基于梯度下降的算法，它有一个非常特殊的功能，称为自调整。它不需要调整，并将自我训练以找到最佳参数和最佳得分，例如，用于回归的最佳 R 平方，用于分类的最佳准确度。这是 CatBoost 的关键特性。</p><p id="e445" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">要安装 CatBoost，您可以参考此<a class="ae mv" href="https://catboost.ai/docs/installation/python-installation-method-pip-install.html" rel="noopener ugc nofollow" target="_blank">文档</a></p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="e827" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Training CatBoost to the training set</strong></span><span id="bcb9" class="mm mn iq mi b gy ms mp l mq mr">from catboost import CatBoostClassifier<br/>catboost = CatBoostClassifier() <br/>catboost.fit(X_train, y_train)</span></pre><p id="2e89" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">现在我已经用一个很棒的库训练了这个模型，这个库就是 CatBoost 库。CatBoost 经过几次迭代，将自己调整到最佳参数，以找到最高的精度(它将为特定问题找到最佳超参数)</p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="8d73" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Making the Confusion Matrix for CatBoost</strong></span><span id="2892" class="mm mn iq mi b gy ms mp l mq mr">from sklearn.metrics import confusion_matrix, accuracy_score<br/>y_pred = catboost.predict(X_test)<br/>cm = confusion_matrix(y_test, y_pred)<br/>print(cm)<br/>accuracy_score(y_test, y_pred)</span><span id="ad25" class="mm mn iq mi b gy ms mp l mq mr">[[84  3]<br/> [ 0 50]]</span><span id="c95e" class="mm mn iq mi b gy ms mp l mq mr">0.9781021897810219</span></pre><p id="c3f9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">有趣的是，XGBoost 和 CatBoost 的精度是一样的。现在，让我们对 CatBoost 模型进行 K 倍交叉验证，并比较结果。</p><pre class="ls lt lu lv gt mh mi mj mk aw ml bi"><span id="262c" class="mm mn iq mi b gy mo mp l mq mr"><strong class="mi ir">Applying the K-Fold Cross Validation for CatBoost</strong></span><span id="8229" class="mm mn iq mi b gy ms mp l mq mr">from sklearn.model_selection import cross_val_score<br/>accuracies = cross_val_score(estimator = xgboost, X = X_train, y = y_train, cv = 10)<br/>print("Accuracy:{:.2f} %".format(accuracies.mean()*100))<br/>print("Standard Deviation:{:.2f} %".format(accuracies.std()*100))</span><span id="d5cd" class="mm mn iq mi b gy ms mp l mq mr">Accuracy:97.26 % <br/>Standard Deviation:2.03 %</span></pre><p id="c50f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">太好了！！我们在 K-fold 交叉验证上取得了 97.26%的准确率，这太不可思议了！因此，XGBoost 的准确率从 94.53%提高到 96.53%，但对于 CatBoost，准确率要高得多，因为我们在 CatBoost 的 K 倍交叉验证集上获得的准确率为 97.26%，这绝对令人惊叹。CatBoost 以将近 1%的优势击败 XGBoost。</p><p id="f22d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这两个库非常强大，因为它们极大地提高了机器学习模型的准确性，并且背后涉及到大量的数学知识。理解数学以及它如何提高模型的性能也很重要。因此，使用这两个库非常好，因为它们可以提供更好的预测，还可以提高机器学习模型的性能。还有其他强大的库，比如<a class="ae mv" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Light GBM </a>，它们在很大程度上提高了我们的准确性。</p><p id="9a25" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在这篇文章中，我试图详细讨论一切。但是你可以随时参考我的<a class="ae mv" href="https://github.com/mathangpeddi/Machine-Learning-Projects/tree/master/Boosting%20Models" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir"> Github 库</strong> </a>获取完整代码。</p><p id="d294" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在<a class="ae mv" href="https://www.linkedin.com/in/mathang-peddi-23763317b/" rel="noopener ugc nofollow" target="_blank"> <strong class="kl ir"> LinkedIn </strong> </a>上与我联系</p><blockquote class="mx"><p id="cd50" class="my mz iq bd na nb nc nd ne nf ng le dk translated">“快固然好，但准确才是最重要的。”-怀亚特·厄普</p></blockquote><p id="b934" class="pw-post-body-paragraph ki kj iq kl b km nh jr ko kp ni ju kr lf nj ku kv lg nk ky kz lh nl lc ld le ij bi translated">我希望你觉得这篇文章很有见地。我很乐意听到反馈，以便即兴创作，并带来更好的内容。</p><p id="963e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">非常感谢您的阅读！</p></div></div>    
</body>
</html>