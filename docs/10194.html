<html>
<head>
<title>AdapterHub: A Framework for Adapting Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AdapterHub:一个适配变压器的框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adapterhub-a-framework-for-adapting-transformers-a21d0ab202a0?source=collection_archive---------22-----------------------#2020-07-18">https://towardsdatascience.com/adapterhub-a-framework-for-adapting-transformers-a21d0ab202a0?source=collection_archive---------22-----------------------#2020-07-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="adae" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">没有更慢的微调:有效的迁移学习与拥抱脸变压器在 2 额外的代码行</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b91e66e48f9d8af1d8673e599673dc1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L5uRdOIH9Qd1h8l98pDHOQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">韦斯利·廷吉在<a class="ae kv" href="https://unsplash.com/s/photos/adapter?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="29cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇博文是对<a class="ae kv" href="https://adapterhub.ml/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> AdapterHub </strong> </a>的介绍，这是一个由<a class="ae kv" href="https://arxiv.org/pdf/2007.07779v1.pdf" rel="noopener ugc nofollow" target="_blank"> Pfeiffer et al (2020b) </a>发布的新框架，它使你能够对<strong class="ky ir">广义预训练的变形金刚如 BERT、RoBERTa 和 XLM-R </strong>进行<strong class="ky ir">迁移学习</strong>到<strong class="ky ir">下游任务</strong>如问答、分类等。<strong class="ky ir">使用适配器代替微调</strong>。</p><p id="714a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AdapterHub 建立在 HuggingFace 提供的流行的 transformer 包之上。这里可以找到拥抱脸变形金刚包<a class="ae kv" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">，这里</a>可以找到 AdapterHub 的修改<a class="ae kv" href="https://adapterhub.ml/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="2bd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了正确理解这篇文章，我建议你先阅读一下<a class="ae kv" href="https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/" rel="noopener ugc nofollow" target="_blank">变压器以及它们通常是如何微调的</a>！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="47fa" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">为什么要用适配器而不是微调？</h2><p id="0165" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我将在“适配器的好处”一节中详细介绍这一点，但先睹为快:</p><p id="49dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf" rel="noopener ugc nofollow" target="_blank"> Houlsby 等人(2019) </a>介绍了一种叫做适配器的东西。适配器<strong class="ky ir">的作用与微调</strong>相同，但是通过<strong class="ky ir">将层</strong>缝合到主预训练模型，并且<strong class="ky ir">更新这些新层</strong>的权重φ，同时<strong class="ky ir">冻结预训练模型</strong>的权重θ。</p><p id="88ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相比之下，您会记得在微调中，我们还需要更新预训练的权重。</p><p id="cc2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如你可能想象的那样，与微调相比，这使得适配器在时间和存储方面的效率更高。适配器也被证明能够<strong class="ky ir">匹配最先进的微调方法</strong>的性能！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="d70a" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">你将从这篇文章中学到什么</h2><p id="5741" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我将简化伴随框架发布的<a class="ae kv" href="https://arxiv.org/pdf/2007.07779v1.pdf" rel="noopener ugc nofollow" target="_blank"> AdapterHub paper </a>的内容，让你更容易开始开发。</p><p id="d1bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AdapterHub 框架很重要，因为在这个框架之前，拼接已经训练过的<strong class="ky ir">适配器</strong>或<strong class="ky ir">共享适配器</strong>很困难，并且需要手动修改转换器架构。该框架支持</p><blockquote class="mx"><p id="9984" class="my mz iq bd na nb nc nd ne nf ng lr dk translated">针对不同任务和语言的预培训适配器的动态“拼接”</p></blockquote><p id="1012" class="pw-post-body-paragraph kw kx iq ky b kz nh jr lb lc ni ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">简而言之，它使得使用适配器进行迁移学习变得更加容易。</p><p id="de2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本帖中，我们将回顾在<a class="ae kv" href="https://arxiv.org/abs/2007.07779" rel="noopener ugc nofollow" target="_blank"> AdapterHub 白皮书</a>中讨论的<strong class="ky ir">适配器</strong>的各种优势，并解释新适配器 Hub 框架的<strong class="ky ir">各种特性。</strong></p><p id="0b5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些特性将伴随着论文中的一些<strong class="ky ir">示例代码</strong>来帮助您入门！</p><p id="fea1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你已经熟悉适配器及其各种好处，你可以直接跳到章节<strong class="ky ir">‘adapter hub 的主要特性’</strong></p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="e33a" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">适配器的优势</h2><p id="aeba" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">此处列出的优势与<a class="ae kv" href="https://arxiv.org/pdf/2007.07779v1.pdf" rel="noopener ugc nofollow" target="_blank">适配器 Hub 文件</a>第 2.2 节中列出的优势相对应。</p><p id="bab6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nm">特定任务的分层表征学习</em> </strong></p><p id="9006" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如前面提到的，在 GLUE benchmark(在自然语言处理界很流行)上比较微调和适配器的性能时，性能没有很大的差异。</p><p id="0fc3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着适配器可以<strong class="ky ir">达到与微调</strong>相当的最先进水平，同时<strong class="ky ir">保持被列为下一个功能的时间和空间效率</strong>！</p><p id="6edb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nm">小型、可扩展、可共享</em> </strong></p><p id="c364" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了完全微调模型，我们需要为每个任务“存储”一个模型的副本。这也“阻碍了训练的项目化和平行化。”</p><p id="3010" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相比之下，适配器需要更少的存储空间。为了说明这一点，Pfeiffer 等人(2020b)提供了以下例子:</p><blockquote class="mx"><p id="e6c8" class="my mz iq bd na nb nc nd ne nf ng lr dk translated">对于大小为 440Mb 的流行 Bert-Base 模型，当使用 48 的瓶颈大小和 Pfeiffer 等人的适配器时，存储 2 个完全微调的模型相当于 125 个带适配器的模型所需的相同存储空间(2020a)</p></blockquote><p id="a3fe" class="pw-post-body-paragraph kw kx iq ky b kz nh jr lb lc ni ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">这样做的另一个好处是，我们可以通过简单地添加小型适配器而不是大量的微调来为应用程序添加更多的任务。</p><p id="e11f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">研究人员之间的再现性</strong>是存储需求降低的另一个美妙结果。</p><p id="c1ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nm">模块化表征</em> </strong></p><p id="bb65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们缝入适配器时，我们固定转换器其余部分的表示，这意味着这些适配器是<em class="nm">封装的</em>，并且可以与其他适配器堆叠、移动或组合。</p><p id="c2cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种模块化允许我们组合来自不同任务的适配器——随着 NLP 任务变得越来越复杂，这是非常重要的。</p><p id="6f11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nm">无干扰合成信息</em> </strong>。</p><p id="1af7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">自然语言处理通常涉及跨任务共享信息。我们经常使用一种叫做多任务学习(MTL)的方法，但是 MTL 有两个问题:</p><ul class=""><li id="b330" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated">灾难性遗忘:在训练的早期阶段学习的信息被“覆盖”。</li><li id="ddf0" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">灾难性推理:当“增加新任务时，一组任务的性能恶化”(Pfeiffer 等人，2020b)。</li></ul><p id="c4cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于适配器，我们为每个任务分别训练适配器，这意味着我们克服了上述两个问题。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="1965" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated"><strong class="ak">适配器 Hub 的主要特性</strong></h2><p id="ad00" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">很好，现在让我们来看看这个框架的关键特性！</p><p id="6b1d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nm">变压器层中的适配器+如何训练适配器</em> </strong></p><p id="c379" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了添加适配器，作者使用了由 HuggingFace transformer 继承的称为“Mix-in”的东西，以便保持代码库合理分离。</p><p id="3f68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实际上，下面是添加适配器层的方法:</p><pre class="kg kh ki kj gt ob oc od oe aw of bi"><span id="aa35" class="lz ma iq oc b gy og oh l oi oj">from adapter_transformers import AutoModelForSequenceClassification, AdapterType</span><span id="6919" class="lz ma iq oc b gy ok oh l oi oj">model = AutoModelForSequenceClassification.from_pretrained("roberta-base")</span><span id="7bc3" class="lz ma iq oc b gy ok oh l oi oj">model.add_adapter("sst-2", AdapterType.text_task, config="pfeiffer") </span><span id="1e22" class="lz ma iq oc b gy ok oh l oi oj">model.train_adapters(["sst-2"]) </span><span id="bb88" class="lz ma iq oc b gy ok oh l oi oj"># Train model ... <br/>model.save_adapter("adapters/text-task/sst-2/", "sst")<br/># Push link to zip file to AdapterHub ...</span></pre><p id="e904" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您会注意到，代码主要对应于常规的 HuggingFace transformers，我们只添加了两行来添加和训练适配器。</p><p id="2ab3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个 AdapterHub 框架的特别之处在于，您可以动态配置适配器，并更改架构。虽然您可以直接使用文献中的适配器，例如 Pfeiffer 等人(2020a)或 Houlsby 等人(2020)，但是您也可以使用<strong class="ky ir">配置文件</strong>轻松修改这些架构。在上面的代码中，我们使用默认的 Pfeiffer (2020a)配置。</p><p id="d95c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nm">抽取和开源适配器</em> </strong></p><p id="a706" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以将您训练的适配器推送到<a class="ae kv" href="https://adapterhub.ml/" rel="noopener ugc nofollow" target="_blank"> AdapterHub.ml </a>，并从其他人预先训练的适配器中受益。与必须共享整个大型模型的微调不同，这些适配器是轻量级的，易于共享！</p><p id="6d0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nm">寻找预先训练好的适配器</em> </strong></p><p id="5f2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://adapterhub.ml/" rel="noopener ugc nofollow" target="_blank"> AdapterHub.ml </a>的搜索功能分级工作:</p><ul class=""><li id="26bb" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated">第一级:按任务/语言查看</li><li id="8baf" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">第二级:分离成更高级别的 NLP 任务的数据集//分离成训练数据的语言(如果我们正在适应一种新的语言)</li><li id="f2c6" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">第三层:分成单独的数据集或域(如维基百科)</li></ul><p id="e972" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该网站还可以根据您指定的预先训练的转换器，帮助您识别兼容的适配器。</p><p id="33ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nm">在预训练的适配器中拼接</em> </strong></p><p id="e5fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用以下代码来连接预训练的适配器，而不是自己训练(如前所述):</p><pre class="kg kh ki kj gt ob oc od oe aw of bi"><span id="4432" class="lz ma iq oc b gy og oh l oi oj">from adapter_transformers import AutoModelForSequenceClassification, AdapterType </span><span id="b11b" class="lz ma iq oc b gy ok oh l oi oj">model = AutoModelForSequenceClassification.from_pretrained("roberta-base")</span><span id="5224" class="lz ma iq oc b gy ok oh l oi oj">model.load_adapter("sst", config="pfeiffer")</span></pre><p id="d60d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">非常清楚地说，预训练的<em class="nm">适配器</em>加载在第三行代码中，而预训练的<em class="nm">转换器</em>加载在第二行代码中。</p><p id="73d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nm">用适配器推理</em> </strong></p><p id="3bac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就用常规的 HuggingFace 代码进行推理吧！加载适配器砝码时，您可以<strong class="ky ir">选择加载预测头</strong>。</p><p id="4277" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回想一下，使用这个令人敬畏的框架，为组合任务等组合适配器是非常可能的！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="0cfd" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">结论</h2><p id="9406" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我鼓励你在这里亲自尝试 AdapterHub 框架。</p><p id="9586" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个框架是如此令人兴奋，我希望这篇文章能帮助你开始适应变形金刚的旅程！</p><p id="2021" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你在帖子中发现任何错误，或者你有任何评论/批评，请告诉我！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="d51f" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">参考</h2><p id="4d92" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">Pfeiffer，j .，Kamath，a .，Rücklé，a .，Cho，k .，Gurevych，I. (2020a)。AdapterFusion:迁移学习的非破坏性任务合成。arXiv 预印本。</p><p id="7c9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">法官 Pfeiffer、法官 Rücklé、法官 Poth、法官 Kamath、法官 Vuli、法官 Ruder、法官 Cho、法官 Gurevych、法官 I(2020 b)。AdapterHub:一个适配变压器的框架。arXiv 预印本。</p><p id="12a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Houlsby，a . Giurgiu，a .，Jastrzkebski，s .，Morrone，b .，de Laroussilhe，q .，Gesmundo，a .，Attariyan，m .，和 Gelly，S. (2019)。自然语言处理中的参数有效迁移学习。2019 年 ICML 会议录。</p></div></div>    
</body>
</html>