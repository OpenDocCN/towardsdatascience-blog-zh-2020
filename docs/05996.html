<html>
<head>
<title>Apache Sqoop</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Sqoop</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/apache-sqoop-1113ce453639?source=collection_archive---------18-----------------------#2020-05-16">https://towardsdatascience.com/apache-sqoop-1113ce453639?source=collection_archive---------18-----------------------#2020-05-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="78d8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">RDBMS到HDFS并返回</h2></div><p id="7939" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在全球范围内，最流行的数据库是基于SQL的。考虑到这一点，对于任何数据湖来说，能够从RDBMS数据库中提取数据是至关重要的。于是，Apache Sqoop诞生了。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/69ebc79f1d35d8e18ec419f3ca50a787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nrfG7fbEIDCMCcEf2Z4zpg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">Apache Sqoop高级数据流</p></figure><p id="498f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Apache Sqoop支持任何RDBMS和HDFS、Hive或HBase等之间的双向数据移动。但是，仅限于结构化数据。它以批量加载格式工作(类似于ETL应用程序中的提取)，支持大量数据的提取，达到RDBMS根本无法处理的数TB。</p><h2 id="1f22" class="lr ls iq bd lt lu lv dn lw lx ly dp lz ko ma mb mc ks md me mf kw mg mh mi mj bi translated">它是如何工作的？</h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mk"><img src="../Images/f1579d8507dc0cb0021076f591907837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*hexus7UP0AbDXFkHq-4hKw.gif"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">稍加修改的Apache Sqoop工作流，<a class="ae ml" href="https://cwiki.apache.org/confluence/display/SQOOP/Sqoop+Presentations+and+Blogs" rel="noopener ugc nofollow" target="_blank"> Apache Sqoop Wiki </a></p></figure><p id="74cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的GIF是将数据从RDBMS加载到HDFS的Sqoop过程的工作流。<em class="mm">反之亦然。</em>它是这样工作的:</p><p id="8ab2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">发起方</strong>:客户端向Sqoop服务器提交一个任务，将数据从源加载到目标(在本例中，就是将RDBMS加载到HDFS)。连接池、模式和元数据验证在这个阶段完成。</p><p id="0065" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">分割器</strong>:现在要提取数据。但是，例如，1 TB大小的表不能作为一个数据块来处理。因此，我们有一个分割器，其中，数据被分解为可消化的数据块/区块，以便并行提取。请注意，这里没有存储数据。</p><p id="ec9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">提取器</strong>:在框架的这一部分，数据实际上是以特定的块/块从源移动的。需要注意的是，并不是所有的数据都要一次加载到内存中进行提取。我们可以把它看作是微批量处理。同样，这里没有存储任何数据。</p><p id="7e21" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> Loader </strong>:提取的数据现在通过框架的Loader阶段推送到目标。现在数据实际上被存储了。</p><p id="b76b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> Cleaner </strong>:这只是一个清理活动，以释放所使用的资源。</p><p id="1b09" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Sqoop 2.x是使用Map Reduce作为其主要处理框架推出的。<em class="mm"> Sqoop with Spark </em>是一种可能性，可以通过上面讨论的Sqoop的即插即用或模块化框架轻松配置。这就是这种通用工作流程的美妙之处/本质。</p></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><p id="f569" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您还没有完成安装，现在是时候了！</p><div class="mu mv gp gr mw mx"><a href="https://medium.com/@prathamesh.nimkar/cloudera-manager-on-google-cloud-3da9b4d64d74" rel="noopener follow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd ir gy z fp nc fr fs nd fu fw ip bi translated">Google Cloud上的Cloudera管理器</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">GCP的CM 6.3.1</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">medium.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl ll mx"/></div></div></a></div><div class="mu mv gp gr mw mx"><a href="https://medium.com/@prathamesh.nimkar/install-mysql-database-7d64f0207cf9" rel="noopener follow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd ir gy z fp nc fr fs nd fu fw ip bi translated">安装MySQL数据库</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">安装MySQL数据库并加载数据</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">medium.com</p></div></div><div class="ng l"><div class="nm l ni nj nk ng nl ll mx"/></div></div></a></div></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h2 id="a397" class="lr ls iq bd lt lu lv dn lw lx ly dp lz ko ma mb mc ks md me mf kw mg mh mi mj bi translated">将数据从RDBMS导入HDFS</h2><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="b4d8" class="lr ls iq no b gy ns nt l nu nv"><strong class="no ir"># Quick check of HDFS folders on instance-1</strong><br/>sudo su -<br/>hdfs dfs -ls /user/root/projects</span></pre><p id="b101" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们使用Sqoop自动创建一个名为<strong class="kh ir"><em class="mm">structuredFlightDataset</em></strong>的新HDFS文件夹，并将数据导入其中。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nw"><img src="../Images/39f0aec0da7c21587ae734a416e126c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ci0WHtjbNfUWvpqq.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">Sqoop导入命令</p></figure><ol class=""><li id="3d9e" class="nx ny iq kh b ki kj kl km ko nz ks oa kw ob la oc od oe of bi translated"><em class="mm"> sqoop导入</em>触发对sqoop服务器的请求，以启动流程。</li><li id="8e16" class="nx ny iq kh b ki og kl oh ko oi ks oj kw ok la oc od oe of bi translated"><em class="mm"> connect </em>参数使用jdbc协议接受数据库连接池信息。</li><li id="f4e9" class="nx ny iq kh b ki og kl oh ko oi ks oj kw ok la oc od oe of bi translated"><em class="mm"> driver </em>参数(可选)接受在源、目标和sqoop服务器之间建立连接的默认驱动程序。请注意，这个参数是为流行的OLTP数据库(如mysql、oracle、db2、postgresql和sql server)提供的默认值。</li><li id="2b38" class="nx ny iq kh b ki og kl oh ko oi ks oj kw ok la oc od oe of bi translated"><em class="mm">用户名/密码</em>在此输入参数进行授权。</li><li id="98ec" class="nx ny iq kh b ki og kl oh ko oi ks oj kw ok la oc od oe of bi translated"><em class="mm">表</em>从参数导入。</li><li id="08ad" class="nx ny iq kh b ki og kl oh ko oi ks oj kw ok la oc od oe of bi translated"><em class="mm">数据将被推送到的目标目录</em>参数。</li><li id="9799" class="nx ny iq kh b ki og kl oh ko oi ks oj kw ok la oc od oe of bi translated">使用标准OOTB <em class="mm"> mysql分隔符</em>来处理数据集列类型。</li></ol><p id="e7a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，上面的导入命令图和标签也适用于导出命令，只是有微小的明显区别。</p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="bc7e" class="lr ls iq no b gy ns nt l nu nv"><strong class="no ir"># Sqoop import from MySQL database on instance-2 to HDFS</strong><br/>sqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --table flightrepdata --target-dir /user/root/projects/structuredFlightDataset --mysql-delimiters<br/><strong class="no ir"># Prompt to enter password</strong><br/>Enter password:</span></pre><p id="87ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将抛出一个小错误，因为mysql的java连接器在instance-1 上丢失了。<a class="ae ml" href="https://docs.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_jdbc_driver_install.html" rel="noopener ugc nofollow" target="_blank"> Cloudera表示，Sqoop不附带第三方JDBC驱动程序，必须单独安装</a>。此外，它需要一个无头的开放JDK和一些对MySQL数据库前端的访问权限。</p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="39ec" class="lr ls iq no b gy ns nt l nu nv"><strong class="no ir"># Download the rpm for j-mysql-connector (your url may vary)</strong><br/>wget <a class="ae ml" href="https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.19-1.el7.noarch.rpm" rel="noopener ugc nofollow" target="_blank">https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.19-1.el7.noarch.rpm</a><strong class="no ir"><em class="mm"><br/></em># Prerequisite: java-openjdk-headless rpm installed via yum</strong><br/>yum install java-1.8.0-openjdk-headless<strong class="no ir"><em class="mm"><br/></em># Install the rpm now<em class="mm"><br/></em></strong>rpm -ivh mysql-connector-java-8.0.19-1.el7.noarch.rpm<strong class="no ir"><em class="mm"><br/></em># copy to the required folders<em class="mm"><br/></em></strong>cd /usr/share/java<br/>cp mysql-connector-java.jar /var/lib/sqoop/<strong class="no ir"><em class="mm"><br/></em># Without adding to oozie, it would throw a java exception<br/># Oozie is an orchestration tool<br/></strong>sudo -u hdfs hadoop fs -copyFromLocal mysql-connector-java.jar /user/oozie/share/lib/lib_20200429072044/sqoop/</span></pre><p id="7505" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们需要在MySQL上授予来自远程服务器/主机的访问权限:</p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="ce32" class="lr ls iq no b gy ns nt l nu nv"><strong class="no ir"># Login to instance-2 and make the changes in mysql config file<em class="mm"><br/></em></strong>ssh instance-2<br/>vim /etc/my.cnf<strong class="no ir"><em class="mm"><br/></em># Change/add the binding address, save &amp; close the file</strong><br/>bind-address = 0.0.0.0<strong class="no ir"><em class="mm"><br/></em># Restart mysql services</strong><br/>service mysqld restart</span></pre><p id="a667" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">登录MySQL，创建具有特定IP的用户，并提供适当的授权。请注意，MySQL不再允许直接从GRANT命令创建用户。只要主机名不同，用户名和密码可以相同。<br/>快速提示:主机名可以在这里找到<em class="mm">cloud era Manager-&gt;Host-&gt;所有主机</em></p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="4f42" class="lr ls iq no b gy ns nt l nu nv">CREATE USER ‘username’@’instance1_hostname’ IDENTIFIED BY ‘new_password’;<br/>CREATE USER ‘username’@’instance2_hostname’ IDENTIFIED BY ‘new_password’;<br/>CREATE USER ‘username’@’instance3_hostname’ IDENTIFIED BY ‘new_password’;<br/>CREATE USER ‘username’@’instance4_hostname’ IDENTIFIED BY ‘new_password’;<br/><strong class="no ir"># Now let's grant the required privileges:</strong><br/>GRANT ALL PRIVILEGES ON *.* TO ‘username’@’instance<strong class="no ir">1</strong>_hostname’;<br/>GRANT ALL PRIVILEGES ON *.* TO ‘username’@’instance<strong class="no ir">2</strong>_hostname’;<br/>GRANT ALL PRIVILEGES ON *.* TO ‘username’@‘instance<strong class="no ir">3</strong>_hostname’;<br/>GRANT ALL PRIVILEGES ON *.* TO ‘username’@‘instance<strong class="no ir">4</strong>_hostname’;<br/><strong class="no ir"># Flush the privileges to "activate" them:<em class="mm"><br/></em></strong>FLUSH PRIVILEGES;<br/><strong class="no ir"># Quick test:</strong><br/>SELECT * from information_schema.user_privileges where grantee like “‘username’@’instance<strong class="no ir">1%</strong>’”;<strong class="no ir"><em class="mm">     </em># Repeat for all other hosts</strong></span></pre><p id="b523" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完成后，您可以重新运行Sqoop import命令。首先，让我们深入分析日志:</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="ol om l"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">Apache Sqoop日志的详细浏览</p></figure><p id="1ade" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们检查导入HDFS的数据:</p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="cd16" class="lr ls iq no b gy ns nt l nu nv">hdfs dfs -ls /user/root/projects/structuredFlightDataset</span></pre><p id="8b0a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这应该给你5个文件。您可以从<a class="ae ml" href="https://medium.com/@prathamesh.nimkar/hdfs-commands-b8a745ca9318" rel="noopener"> HDFS的Name Node Web UI </a>或Hue的Web UI—<a class="ae ml" href="http://instance-1:8889/" rel="noopener ugc nofollow" target="_blank"><em class="mm">http://instance-1:8889/</em></a>查看。您可能需要下载文件才能查看。请注意<strong class="kh ir"> <em class="mm"> _SUCCESS </em> </strong>文件不包含任何内容，它只是一个标志值。<strong class="kh ir"><em class="mm">part-m-00000/1/2/3</em></strong><em class="mm"/>文件以一个csv格式的实际数据出现。</p><p id="b1ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了控制创建和执行多少并行流程(也称为映射器/分割),我们可以这样调整导入命令:</p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="ba14" class="lr ls iq no b gy ns nt l nu nv"><strong class="no ir"># Sqoop import using </strong>1 mapper<strong class="no ir"> only</strong><br/>sqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --table flightrepdata --target-dir /user/root/projects/structuredFlightDataset2 --mysql-delimiters <strong class="no ir">-m 1</strong></span></pre><p id="347c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">映射器数量参数只是对纱线的<em class="mm">建议。YARN可能会选择完全忽略这个建议。</em></p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="1a53" class="lr ls iq no b gy ns nt l nu nv"><strong class="no ir"># Pretty much the same logs using a single mapper except:</strong><br/>INFO mapreduce.ImportJobBase: Transferred 57.1199 MB in 38.2969 seconds (1.4915 MB/sec)</span></pre><p id="b262" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它给出了两个输出文件，即part-m-00000和_SUCCESS标志。你会注意到它有点慢，但这只是60万记录。想象一下，如果是1亿条记录，性能会有多大的不同。</p><p id="fc5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然映射器是线程，那么如果您提供两倍于主机的映射器会怎么样呢？这能进一步提高性能吗？</p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="19d4" class="lr ls iq no b gy ns nt l nu nv">sqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --table flightrepdata --target-dir /user/root/projects/structuredFlightDataset3 --mysql-delimiters <strong class="no ir">-m 8<br/># Notable differences in the log generated:</strong><br/>INFO db.IntegerSplitter: Split size: 75918; Num splits: 8 from: 1 to: 607346<br/>INFO mapreduce.JobSubmitter: number of splits:8<br/>INFO mapreduce.Job:  map 0% reduce 0%<br/>INFO mapreduce.Job:  map 13% reduce 0%<br/>INFO mapreduce.Job:  map 25% reduce 0%<br/>INFO mapreduce.Job:  map 50% reduce 0%<br/>INFO mapreduce.Job:  map 75% reduce 0%<br/>INFO mapreduce.Job:  map 88% reduce 0%<br/>INFO mapreduce.Job:  map 100% reduce 0%<br/>INFO mapreduce.ImportJobBase: Transferred 57.1199 MB in 38.247 seconds (1.4934 MB/sec)</span></pre><p id="aa19" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如您将注意到的，所用的时间没有差异，但它会产生8个输出文件(part-m-00000–8)和1个_SUCCESS标志。</p><blockquote class="on"><p id="5ae3" class="oo op iq bd oq or os ot ou ov ow la dk translated">最好保持映射器的数量与数据节点的数量相同，处理可以/将要在数据节点上运行。</p></blockquote><p id="3661" class="pw-post-body-paragraph kf kg iq kh b ki ox jr kk kl oy ju kn ko oz kq kr ks pa ku kv kw pb ky kz la ij bi translated">这里出现了一个有趣的问题，我们能否使用SQL查询将数据加载到HDFS中？为什么是的，当然我们可以。事实上，我们可以让复杂的查询按照我们认为合适的方式连接多个表。</p><p id="5007" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">额外提示:该查询被提交到数据库层，因此，如果您可以在该层过滤所需的数据，就可以节省下游的Map Reduce或Spark处理。但是，这不是最佳实践。数据湖的概念是首先获取所有的原始数据。</p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="6335" class="lr ls iq no b gy ns nt l nu nv">sqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --target-dir /user/root/projects/structuredFlightDataset3 --mysql-delimiters --query <strong class="no ir">"SELECT UID, OP_UNIQUE_CARRIER FROM flightrepdata_exp WHERE 1=1 AND FL_DATE = '2020-01-01'"</strong> -m 2</span></pre><h2 id="16e2" class="lr ls iq bd lt lu lv dn lw lx ly dp lz ko ma mb mc ks md me mf kw mg mh mi mj bi translated">从RDBMS到HDFS的增量导入</h2><p id="3055" class="pw-post-body-paragraph kf kg iq kh b ki pc jr kk kl pd ju kn ko pe kq kr ks pf ku kv kw pg ky kz la ij bi translated">每天从OLTP数据库批量加载需要能够执行增量加载，即只加载上次执行的增量。基本上有两种方法可以实现这一点:</p><p id="63a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> LastModified </strong></p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="81a3" class="lr ls iq no b gy ns nt l nu nv"><strong class="no ir"># Let's create a last modified date column<br/></strong>ALTER TABLE `flightrepdata` <br/>ADD `LAST_MODIFIED_DATE` TIMESTAMP NOT NULL DEFAULT NOW();<br/><strong class="no ir"># Quick import into HDFS<br/></strong>sqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --table flightrepdata --target-dir /user/root/projects/structuredFlightDataset4 --mysql-delimiters<br/><strong class="no ir"># Quick HDFS Check<br/></strong>hdfs dfs -ls /user/root/projects/structuredFlightDataset4<br/><strong class="no ir"># Quick insert now<br/></strong>INSERT `flightrepdata` <br/>SELECT NULL,<br/>FL_DATE,<br/>OP_UNIQUE_CARRIER,<br/>ORIGIN_AIRPORT_ID,<br/>ORIGIN_AIRPORT_SEQ_ID,<br/>ORIGIN_CITY_MARKET_ID,<br/>ORIGIN_CITY_NAME,<br/>DEST_AIRPORT_ID,<br/>DEST_AIRPORT_SEQ_ID,<br/>DEST_CITY_MARKET_ID,<br/>DEST_CITY_NAME,<br/>DEP_TIME,<br/>ARR_TIME,<br/>NOW()<br/>FROM `flightrepdata`<br/>WHERE 1=1<br/>AND uid IN (1, 2, 3, 4, 5);<br/>COMMIT;<br/><strong class="no ir"># Quick data test for the 5 new records<br/></strong>SELECT uid, OP_UNIQUE_CARRIER, LAST_MODIFIED_DATE FROM flightrepdata ORDER BY 1 DESC LIMIT 5;<strong class="no ir"><em class="mm"><br/></em></strong>+--------+-------------------+---------------------+<br/>| <strong class="no ir">uid    </strong>| <strong class="no ir">OP_UNIQUE_CARRIER </strong>| <strong class="no ir">LAST_MODIFIED_DATE  </strong>|<br/>+--------+-------------------+---------------------+<br/>| 607352 | EV                | 2020-04-27 16:49:36 |<br/>| 607351 | EV                | 2020-04-27 16:49:36 |<br/>| 607350 | EV                | 2020-04-27 16:49:36 |<br/>| 607349 | EV                | 2020-04-27 16:49:36 |<br/>| 607348 | EV                | 2020-04-27 16:49:36 |<br/>+--------+-------------------+---------------------+</span></pre><p id="6526" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，对于“lastmodified”模式下的增量导入，我们需要提供以下参数:<br/> <em class="mm"> —增量</em> —实例化增量提取过程及其模式<br/> <em class="mm"> —检查列</em> —应检查日期的日期列<br/> <em class="mm"> —最后值</em> —应提取数据的值位<br/> <em class="mm"> —追加</em> —追加新数据</p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="851e" class="lr ls iq no b gy ns nt l nu nv">sqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --table flightrepdata --target-dir /user/root/projects/structuredFlightDataset4 --mysql-delimiters --incremental lastmodified --check-column LAST_MODIFIED_DATE --last-value "2020-04-27 16:49:36" --append<br/><strong class="no ir"># Additional logs<br/></strong>INFO mapreduce.ImportJobBase: Transferred 611 bytes in 26.6098 seconds (22.9615 bytes/sec)<br/>INFO mapreduce.ImportJobBase: <strong class="no ir">Retrieved 5 records</strong>.<br/>INFO util.AppendUtils: <strong class="no ir">Appending</strong> to directory structuredFlightDataset4<br/>INFO util.AppendUtils: Using found partition 4<br/><strong class="no ir"># Quick HDFS Check<br/></strong>hdfs dfs -ls /user/root/projects/structuredFlightDataset4</span></pre><p id="5f76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">追加</strong></p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="5eab" class="lr ls iq no b gy ns nt l nu nv"><strong class="no ir"># Let's run the same INSERT query as above and test<br/></strong>SELECT uid, OP_UNIQUE_CARRIER, LAST_MODIFIED_DATE FROM flightrepdata ORDER BY 1 DESC LIMIT 5;<br/>+--------+-------------------+---------------------+<br/>| <strong class="no ir">uid    </strong>| <strong class="no ir">OP_UNIQUE_CARRIER </strong>| <strong class="no ir">LAST_MODIFIED_DATE  </strong>|<br/>+--------+-------------------+---------------------+<br/>| 607359 | EV                | 2020-04-27 17:05:19 |<br/>| 607358 | EV                | 2020-04-27 17:05:19 |<br/>| 607357 | EV                | 2020-04-27 17:05:19 |<br/>| 607356 | EV                | 2020-04-27 17:05:19 |<br/>| 607355 | EV                | 2020-04-27 17:05:19 |<br/>+--------+-------------------+---------------------+</span></pre><p id="6328" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，对于“追加”模式下的增量导入，我们需要提供以下参数:<br/> — <em class="mm">增量</em> —实例化增量提取过程及其模式<br/> — <em class="mm">检查列</em> —需要检查值的列<br/> — <em class="mm">最后一个值</em> —需要拾取数据的值位</p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="2ba1" class="lr ls iq no b gy ns nt l nu nv"><strong class="no ir"># Incremental Import<br/></strong>sqoop import --connect jdbc:mysql://instance-2/flight_data --driver com.mysql.jdbc.Driver --username root -P --table flightrepdata --target-dir /user/root/projects/structuredFlightDataset4 --mysql-delimiters --incremental append --check-column uid --last-value 607352<br/><strong class="no ir"># Additional Logs<br/></strong>INFO tool.ImportTool: <strong class="no ir">Lower bound value: 607352</strong><br/>INFO tool.ImportTool: <strong class="no ir">Upper bound value: 607359<br/></strong>INFO mapreduce.ImportJobBase: Transferred 611 bytes in 25.496 seconds (23.9646 bytes/sec)<br/>INFO mapreduce.ImportJobBase: <strong class="no ir">Retrieved 5 records.</strong><br/>INFO util.AppendUtils: <strong class="no ir">Appending </strong>to directory structuredFlightDataset4<br/>INFO util.AppendUtils: Using found partition 9</span></pre></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h2 id="e3c1" class="lr ls iq bd lt lu lv dn lw lx ly dp lz ko ma mb mc ks md me mf kw mg mh mi mj bi translated">将数据从HDFS导出到RDBMS</h2><p id="3d44" class="pw-post-body-paragraph kf kg iq kh b ki pc jr kk kl pd ju kn ko pe kq kr ks pf ku kv kw pg ky kz la ij bi translated">Sqoop不会自动创建一个表，因此我们必须创建一个具有底层结构(即列和数据类型)的表。</p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="b579" class="lr ls iq no b gy ns nt l nu nv"><strong class="no ir"># Login to instance-2 MySQL and appropriate database instance</strong><br/>use flight_data<br/><strong class="no ir"># Create the table</strong><br/>CREATE TABLE flightrepdata_exp<br/>(UID INT PRIMARY KEY, <br/>FL_DATE DATE,<br/>OP_UNIQUE_CARRIER VARCHAR(10),<br/>ORIGIN_AIRPORT_ID INT,<br/>ORIGIN_AIRPORT_SEQ_ID INT,<br/>ORIGIN_CITY_MARKET_ID INT,<br/>ORIGIN_CITY_NAME VARCHAR(300),<br/>DEST_AIRPORT_ID INT,<br/>DEST_AIRPORT_SEQ_ID INT,<br/>DEST_CITY_MARKET_ID INT,<br/>DEST_CITY_NAME VARCHAR(300),<br/>DEP_TIME INT,<br/>ARR_TIME INT);<br/><strong class="no ir"># Sqoop export to MySQL database on instance-2 from HDFS</strong><br/>sqoop export --connect jdbc:mysql://instance-2/flight_data --username root -P --export-dir /user/root/projects/structuredFlightDataset/ --table flightrepdata_exp --mysql-delimiters<br/><strong class="no ir"># Pretty much the same logs really</strong></span></pre><p id="6e5f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，导出命令也可以在多个映射器上运行。就像导入命令一样，最好限制<em class="mm">映射器数量=主机数量</em>。</p></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h2 id="7bdf" class="lr ls iq bd lt lu lv dn lw lx ly dp lz ko ma mb mc ks md me mf kw mg mh mi mj bi translated"><strong class="ak">优步模式</strong></h2><p id="a27b" class="pw-post-body-paragraph kf kg iq kh b ki pc jr kk kl pd ju kn ko pe kq kr ks pf ku kv kw pg ky kz la ij bi translated">MapReduce作业的Mapper和Reducer任务由YARN的资源管理器(RM)在分布于几个节点的两个独立容器中运行。但是，如果您的数据集很小，或者您的作业包含小型制图工具任务，或者您的作业仅包含一个缩减器任务，我们可以将优步模式设置为TRUE。这迫使RM在一个容器或JVM中顺序运行mapper和reducer任务，从而减少了启动新容器和跨多个节点为一个小任务建立网络的开销。工作完成得更快。</p></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><p id="fd77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您在Sqoop中遇到任何问题，可以在这里找到日志:<br/><em class="mm">http://&lt;instance-1 IP&gt;:8088/proxy/&lt;YARN的作业申请id &gt; / <br/> </em>或者更好的是，所有作业都可以在这里找到:<em class="mm"><br/>http://&lt;instance-1 IP&gt;:8088/cluster</em></p><p id="648b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于更多的命令，也许你想访问<a class="ae ml" href="https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html" rel="noopener ugc nofollow" target="_blank"> Apache Sqoop的用户指南</a></p><p id="c06c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">快速清理</strong>:</p><pre class="lc ld le lf gt nn no np nq aw nr bi"><span id="bbd8" class="lr ls iq no b gy ns nt l nu nv"><strong class="no ir"># Be a good Samaritan and clean up your workspace</strong><br/>hdfs dfs -rm -R /user/root/projects/structuredFlightDataset2<br/>hdfs dfs -rm -R /user/root/projects/structuredFlightDataset3<br/>hdfs dfs -rm -R /user/root/projects/structuredFlightDataset4drop table flightrepdata_exp;<br/>ALTER TABLE `flightrepdata` DROP `LAST_MODIFIED_DATE`;<br/>DELETE FROM `flightrepdata` WHERE uid &gt;= 607347; COMMIT;</span></pre><p id="ca45" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考文献</strong>:</p><p id="c4a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1] <a class="ae ml" href="https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html" rel="noopener ugc nofollow" target="_blank"> Sqoop用户指南</a>，Apache Sqoop，ASF</p><p id="9349" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] <a class="ae ml" href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cm_mc_sqoop1_client.html" rel="noopener ugc nofollow" target="_blank">管理Sqoop客户端</a>，Apache Sqoop，Cloudera文档</p><p id="be3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] C. Chaudhari，<a class="ae ml" href="https://community.cloudera.com/t5/Support-Questions/What-is-Uber-mode/td-p/211160" rel="noopener ugc nofollow" target="_blank">什么是优步模式？</a> (2018)，Cloudera社区</p><p id="8281" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] A. Elmahrek，J. Cecho (2014)，<a class="ae ml" href="https://cwiki.apache.org/confluence/display/SQOOP/Sqoop+Presentations+and+Blogs" rel="noopener ugc nofollow" target="_blank"> Sqoop2新一代大数据— Apache Sqoop工作流</a>，Apache软件基金会</p></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h2 id="c1df" class="lr ls iq bd lt lu lv dn lw lx ly dp lz ko ma mb mc ks md me mf kw mg mh mi mj bi translated">数据集</h2><p id="1369" class="pw-post-body-paragraph kf kg iq kh b ki pc jr kk kl pd ju kn ko pe kq kr ks pf ku kv kw pg ky kz la ij bi translated">请注意，我没有下载所有的列来创建数据集。但是你可以。</p><p id="54f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结构化清洗数据集可以在<a class="ae ml" href="https://github.com/pratnimk/sqoop-big-data-analysis/raw/master/915662529_T_ONTIME_REPORTING.zip" rel="noopener ugc nofollow" target="_blank">这里</a>找到。<br/>用一些数据解释？给你。<br/>原始数据集的首页— <a class="ae ml" href="https://www.bts.gov/" rel="noopener ugc nofollow" target="_blank">此处</a>。<br/>原始数据— <a class="ae ml" href="https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;DB_Short_Name=On-Time" rel="noopener ugc nofollow" target="_blank">此处</a>。<br/>字段的更多详细信息— <a class="ae ml" href="https://www.transtats.bts.gov/Fields.asp?Table_ID=236" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><div class="lc ld le lf gt mx"><a rel="noopener follow" target="_blank" href="/apache-yarn-zookeeper-61e17a958215"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd ir gy z fp nc fr fs nd fu fw ip bi translated">阿帕奇纱线和动物园管理员</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">关于资源分配和高可用性的一切</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">towardsdatascience.com</p></div></div><div class="ng l"><div class="ph l ni nj nk ng nl ll mx"/></div></div></a></div><div class="mu mv gp gr mw mx"><a rel="noopener follow" target="_blank" href="/simplifying-hdfs-erasure-coding-9d9588975113"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd ir gy z fp nc fr fs nd fu fw ip bi translated">HDFS擦除编码</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">通过利用擦除编码，显著降低HDFS集群的存储开销</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">towardsdatascience.com</p></div></div><div class="ng l"><div class="pi l ni nj nk ng nl ll mx"/></div></div></a></div><div class="mu mv gp gr mw mx"><a href="https://medium.com/@prathamesh.nimkar/big-data-analytics-using-the-hadoop-ecosystem-411d629084d3" rel="noopener follow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd ir gy z fp nc fr fs nd fu fw ip bi translated">使用Hadoop生态系统的大数据分析渠道</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">登录页面</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">medium.com</p></div></div><div class="ng l"><div class="pj l ni nj nk ng nl ll mx"/></div></div></a></div></div></div>    
</body>
</html>