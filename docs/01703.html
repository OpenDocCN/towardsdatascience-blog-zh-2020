<html>
<head>
<title>Build Powerful Lightweight Models Using Knowledge Distillation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用知识蒸馏构建强大的轻量级模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9?source=collection_archive---------26-----------------------#2020-02-16">https://towardsdatascience.com/build-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9?source=collection_archive---------26-----------------------#2020-02-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="aa8f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过师生培训计划获得高性能的微型模型</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/8a5400c17d2a0d3b3e2a09cd8e5df824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fwiq1lbJU4M5c8zwHW64JA.jpeg"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">照片由<a class="ae kw" href="https://unsplash.com/@thutra0803?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Tra Nguyen </a>在<a class="ae kw" href="https://unsplash.com/s/photos/teacher?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="c455" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">过去几年，机器学习领域的趋势是获得最大的模型，在大量数据上训练它们，然后集成它们，以获得最后几个百分点的准确性。这种方法的一个缺点是，在现实生活的应用程序中部署大型模型或集成可能会很困难。对于任何实际应用来说，它们的权重可能太大，或者它们的推理时间可能太长，尤其是当您试图在嵌入式设备或web应用程序的客户端使用它们时。<br/>还有一项积极的研究，即如何通过为移动设备(如<a class="ae kw" href="https://arxiv.org/pdf/1704.04861.pdf" rel="noopener ugc nofollow" target="_blank"> MobileNets </a>或<a class="ae kw" href="https://arxiv.org/abs/1609.07061" rel="noopener ugc nofollow" target="_blank">权重量化</a>)构建定制架构，在小巧快速的同时实现不错的性能。</p><p id="9083" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这篇文章中，我们将展示如何通过使用知识蒸馏(来自<a class="ae kw" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">提取神经网络</a>中的知识)和混合(来自<a class="ae kw" href="https://arxiv.org/abs/1710.09412" rel="noopener ugc nofollow" target="_blank">混合:超越经验风险最小化</a>)来提高微型神经网络的性能。知识蒸馏背后的基本思想是，你定义一个老师(可以是单个模型或整体)和一个学生(这是你想在生产中使用的轻量级模型)，然后你在目标任务上训练老师，并让学生尝试模仿老师。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/931218618eeb960efe943a91fd427af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*O_V0Kn1PylJ3DgFSNN0MwQ.png"/></div></figure><h1 id="a289" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">数据:</h1><p id="99e9" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">我们将使用在Kaggle数据集<a class="ae kw" href="https://www.kaggle.com/shayanfazeli/heartbeat" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/mondejar/mitbih-database</a>上处于预处理状态的<a class="ae kw" href="https://physionet.org/content/mitdb/1.0.0/" rel="noopener ugc nofollow" target="_blank">麻省理工学院-BIH数据集</a>。该数据集包含与心律失常异常相关的五类个体心跳。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mr"><img src="../Images/563d89fc5b689bf52517cc6d44a8e0a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ubqrgf6yAhRbPdWWkhOHKg.png"/></div></div></figure><h1 id="2735" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">教师模型:</h1><p id="3eb9" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">教师模型是1D CNN，其具有卷积层，每个卷积层具有64个滤波器和两个完全连接的层。总共有<strong class="kz ir">17221个</strong>可训练参数。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/1f8bfe3fda5a00f0132224bd38ec4d32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*HMstZf6FboC9pXiD4QlWPQ.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">教师模型</p></figure><h1 id="f6cb" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">学生模型:</h1><p id="5dd1" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">学生模型的结构与教师模型相同，但卷积层更小。总共有<strong class="kz ir">3909个</strong>可训练参数，因此<strong class="kz ir">比教师模型</strong>小4倍。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/b486ada6bcce6cdd3b286738da38c03f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*uKjZR6BoNQrZvGTI9kJ80g.png"/></div></figure><h1 id="e653" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">目标函数:</h1><p id="3890" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">我们使用应用于独热标签的分类交叉熵来训练教师模型。当应用知识提炼时，使用Kullback Leibler散度和由教师模型作为目标预测的软标签上的MAE损失之间的混合来训练学生模型。</p><p id="b135" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Kullback Leibler散度测量两个概率分布之间的差异，因此这里的目标是使学生预测的分布{在班级上}尽可能接近教师。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h1 id="a4c6" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">结果:</h1><p id="4cf5" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">在没有使用任何知识提炼的情况下，微型模型获得了F1分数<strong class="kz ir"> 0.67 +- 0.02 </strong>，在使用知识提炼后，微型模型的性能被提升到<strong class="kz ir"> 0.78 +- 0.02 </strong>。当使用知识提炼时，我们能够使用相同的架构获得F1分数方面的<strong class="kz ir"> 11 </strong>个绩效点。</p><p id="affa" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">结果概述:</p><ul class=""><li id="3b11" class="mw mx iq kz b la lb ld le lg my lk mz lo na ls nb nc nd ne bi translated">F1教师模型:<strong class="kz ir"> 0.82 +- 0.006 </strong></li><li id="b7b8" class="mw mx iq kz b la nf ld ng lg nh lk ni lo nj ls nb nc nd ne bi translated">F1微型模式+知识提炼:<strong class="kz ir"> 0.78 +- 0.02 </strong></li><li id="7fb1" class="mw mx iq kz b la nf ld ng lg nh lk ni lo nj ls nb nc nd ne bi translated">F1微型模式从零开始:<strong class="kz ir"> 0.67 +- 0.02 </strong></li></ul><h1 id="21ba" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">结论:</h1><p id="7f12" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated">在这篇文章中，我们能够实现一个简单的知识蒸馏训练方案，该方案能够使用完全相同的架构将一个非常小的模型的性能从<strong class="kz ir"> 0.67 </strong> F1提升到<strong class="kz ir"> 0.78 </strong> F1。例如，当拥有尽可能小且性能良好的模型对于部署非常重要时，这可能非常有用。当使用这种方法时，有很多需要探索的地方，比如使用合奏作为老师，或者老师和学生之间的大小差异如何影响知识提炼的质量。这将在以后的文章中完成😃。</p><h1 id="c5f9" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">参考资料:</h1><ul class=""><li id="3a72" class="mw mx iq kz b la mm ld mn lg nk lk nl lo nm ls nb nc nd ne bi translated"><a class="ae kw" href="https://arxiv.org/pdf/1503.02531.pdf" rel="noopener ugc nofollow" target="_blank">在神经网络中提取知识</a></li></ul><h1 id="e9d1" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">代码:</h1><p id="b805" class="pw-post-body-paragraph kx ky iq kz b la mm jr lc ld mn ju lf lg mo li lj lk mp lm ln lo mq lq lr ls ij bi translated"><a class="ae kw" href="https://github.com/CVxTz/knowledge_distillation" rel="noopener ugc nofollow" target="_blank">https://github.com/CVxTz/knowledge_distillation</a></p></div></div>    
</body>
</html>