<html>
<head>
<title>How to run a real-time pipeline in AWS Kinesis using PySpark Structured Streaming</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用 PySpark 结构化流在 AWS Kinesis 中运行实时管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-run-a-real-time-pipeline-in-aws-kinesis-using-pyspark-structured-streaming-1b6bc5e6edc9?source=collection_archive---------25-----------------------#2020-06-28">https://towardsdatascience.com/how-to-run-a-real-time-pipeline-in-aws-kinesis-using-pyspark-structured-streaming-1b6bc5e6edc9?source=collection_archive---------25-----------------------#2020-06-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="61aa" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Python 结构化流应用程序从 AWS Kinesis 消费数据，并在 Jupyter 笔记本中运行</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3b90cc0ada525347541cc650962d2921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3VhnZmOpK2ZCafNlo0rbVw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@aronvisuals?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Aron 视觉效果</a>在<a class="ae ky" href="/s/photos/real-time?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="8567" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我们将使用 Spark 的最新流技术，结构化流，从 AWS Kinesis 流中使用<code class="fe lv lw lx ly b">JSON</code>数据。</p><p id="1f36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将执行以下步骤:</p><ul class=""><li id="a9f2" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated">使用 boto3 在 AWS 中创建 Kinesis 流</li><li id="236c" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">将一些简单的<code class="fe lv lw lx ly b">JSON</code>消息写入流中</li><li id="b6e4" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">消费 PySpark 中的消息</li><li id="e159" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">在控制台中显示消息</li></ul><p id="ecd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">TL；DR: </strong> <a class="ae ky" href="https://github.com/BogdanCojocar/medium-articles/tree/master/pyspark_kinesis" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Github 代码回购</strong> </a></p><h1 id="1a40" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">步骤 1:为 Jupyter 设置 PySpark</h1><p id="6070" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">为了能够在笔记本上运行 PySpark，我们必须使用<code class="fe lv lw lx ly b">findspark</code>包。我们需要添加另一个包，允许 PySpark 从 Kinesis 获取消费者数据。对于此功能，我们在笔记本的开头添加了:</p><pre class="kj kk kl km gt nk ly nl nm aw nn bi"><span id="f4a3" class="no mo it ly b gy np nq l nr ns">import os<br/>os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages=com.qubole.spark/spark-sql-kinesis_2.11/1.1.3-spark_2.4 pyspark-shell'</span><span id="57ea" class="no mo it ly b gy nt nq l nr ns">import findspark<br/>findspark.init()</span></pre><h1 id="d001" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">步骤 2:创建一个动力流</h1><p id="f346" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">AWS Kinesis 是使我们能够实时读取和处理数据的基础设施。与其他类似的技术(Kafka)相比，它更容易建立。在 python 中，我们可以使用<code class="fe lv lw lx ly b">boto3</code>库:</p><pre class="kj kk kl km gt nk ly nl nm aw nn bi"><span id="a967" class="no mo it ly b gy np nq l nr ns">client = boto3.client('kinesis')<br/>stream_name='pyspark-kinesis'</span><span id="5396" class="no mo it ly b gy nt nq l nr ns">client.create_stream(<br/>        StreamName=stream_name,<br/>        ShardCount=1)</span></pre><p id="dc8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将创建一个流分片，它本质上是控制吞吐量的单元。更多的碎片意味着我们可以接收更多的数据，但是对于本教程来说，一个就足够了。</p><p id="dc7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们打开 AWS 控制台，导航到 Amazon Kinesis board(服务-&gt; Amazon Kinesis)，我们应该会看到类似的内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/91c1c9448ebd4ff69cf762b87d851cf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BVX0QhuciWGqNZdQsYRBIw.png"/></div></div></figure><h1 id="4e6b" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">步骤 3:将信息写入 Kinesis</h1><p id="6e1f" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">要将一些消息写入我们刚刚创建的 Kinesis 流，我们可以再次使用<code class="fe lv lw lx ly b">boto3</code>:</p><pre class="kj kk kl km gt nk ly nl nm aw nn bi"><span id="bfcc" class="no mo it ly b gy np nq l nr ns">messages = [<br/>    {'message_type': 'message1', 'count': 2},<br/>    {'message_type': 'message2', 'count': 1},<br/>    {'message_type': 'message1', 'count': 2},<br/>    {'message_type': 'message3', 'count': 3},<br/>    {'message_type': 'message1', 'count': 5}<br/>]</span><span id="6262" class="no mo it ly b gy nt nq l nr ns">for message in messages:<br/>    client.put_record(<br/>        StreamName=stream_name,<br/>        Data=json.dumps(message),<br/>        PartitionKey='part_key')</span></pre><p id="5b21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有一个字典列表，我们将其序列化到<code class="fe lv lw lx ly b">JSON</code>中，并使用<code class="fe lv lw lx ly b">put_record</code>函数编写。一个字典代表我们信息的模式。因为我们只使用一个碎片，所以如何定义分区键并不重要。</p><h1 id="cd39" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">第四步:阅读信息</h1><p id="e2c5" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">当使用结构化流读取数据时，我们需要注意 Kinesis 的一些特殊性。</p><p id="1bfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要定义<code class="fe lv lw lx ly b">format</code> Kinesis，以便使用正确的连接器。我们还必须使用端点和区域，这取决于我们部署基础架构的位置。我的情况是爱尔兰<code class="fe lv lw lx ly b">eu-west-1.</code></p><p id="97e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用设置<code class="fe lv lw lx ly b">startingposition</code>作为<code class="fe lv lw lx ly b">TRIM_HORIZON</code>，这意味着当我们重启应用程序时，我们总是会读取 Kinesis 中所有可用的数据。如果我们只想处理最新的可用数据，我们可以将该标志改为<code class="fe lv lw lx ly b">LATEST</code>。</p><p id="da01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经为 AWS 凭证创建了环境变量。我们也可以直接使用它们，但这样更安全。</p><pre class="kj kk kl km gt nk ly nl nm aw nn bi"><span id="8bbb" class="no mo it ly b gy np nq l nr ns">kinesis = spark \<br/>        .readStream \<br/>        .format('kinesis') \<br/>        .option('streamName', stream_name) \<br/>        .option('endpointUrl', '<a class="ae ky" href="https://kinesis.eu-west-1.amazonaws.com')\" rel="noopener ugc nofollow" target="_blank">https://kinesis.eu-west-1.amazonaws.com')\</a><br/>        .option('region', 'eu-west-1') \<br/>        .option('awsAccessKeyId', os.environ['KINESIS_ACCESS_KEY'])\<br/>        .option('awsSecretKey', os.environ['KINESIS_SECRET_KEY']) \<br/>        .option('startingposition', 'TRIM_HORIZON')\<br/>        .load()\</span></pre><p id="b230" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们执行了这段代码，我们将能够读取数据帧中的运动数据。</p><h1 id="aaeb" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">步骤 5:在控制台中显示消息</h1><p id="f2f4" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们现在设置的好处是我们可以执行标准的数据帧操作。</p><p id="fa4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的数据在 JSON 中，所以我们需要转换它。前三个 select 语句就是关于这个的。我们使用<code class="fe lv lw lx ly b">from_json</code>将 JSON 列转换为包含所有字段的结构，使用<code class="fe lv lw lx ly b">data.*</code>的技巧是将一列分成单独的列，因此在我们的例子中，一列用于<code class="fe lv lw lx ly b">message_type</code>，一列用于<code class="fe lv lw lx ly b">count</code>。</p><pre class="kj kk kl km gt nk ly nl nm aw nn bi"><span id="9b43" class="no mo it ly b gy np nq l nr ns">schema = StructType([<br/>            StructField("message_type", StringType()),<br/>            StructField("count", IntegerType())])</span><span id="5cad" class="no mo it ly b gy nt nq l nr ns">kinesis\<br/>    .selectExpr('CAST(data AS STRING)')\<br/>    .select(from_json('data', schema).alias('data'))\<br/>    .select('data.*')\<br/>    .writeStream\<br/>    .outputMode('append')\<br/>    .format('console')\<br/>    .trigger(once=True) \<br/>    .start()\<br/>    .awaitTermination()</span></pre><p id="081c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们使用选项<code class="fe lv lw lx ly b">trigger(once=True)</code>，我们将只读取一批数据，这不是一个连续的操作。如果我们删除这个标志，那么处理将是无限的。</p><p id="c5ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们启动 Jupyter 笔记本的终端中，我们将能够看到数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/5b5f783bf3d2086de08e391751d3ca73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KF7bqHwqYIFlVQlLVSd72A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一批运动数据</p></figure><p id="0d87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们可以删除我们在 AWS 中使用的资源，即我们部署的 Kinesis 流:</p><pre class="kj kk kl km gt nk ly nl nm aw nn bi"><span id="944a" class="no mo it ly b gy np nq l nr ns">client.delete_stream(StreamName=stream_name)</span></pre><p id="b6d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅此而已！我希望您喜欢来自精彩的流处理世界的另一个教程。我们已经看到了如何通过几个步骤在 Kinesis 的 PySpark 中连接和处理数据。</p></div></div>    
</body>
</html>