<html>
<head>
<title>What can Borges teach you about overfitting?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于过度拟合，博尔赫斯能教你什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-can-borges-teach-you-about-overfitting-e5ac2dd21217?source=collection_archive---------63-----------------------#2020-08-25">https://towardsdatascience.com/what-can-borges-teach-you-about-overfitting-e5ac2dd21217?source=collection_archive---------63-----------------------#2020-08-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="714b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">豪尔赫·路易斯·博尔赫斯(1899–1986)是一位才华横溢的作家，也是有史以来最有影响力的作家之一。但是博尔赫斯能教你什么是过度拟合呢？</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="2c4d" class="km kn iq bd ko kp kq kr ks kt ku kv kw jw kx jx ky jz kz ka la kc lb kd lc ld bi translated">Funes 纪念馆</h1><p id="04d2" class="pw-post-body-paragraph le lf iq lg b lh li jr lj lk ll ju lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">博尔赫斯在 1954 年写了《纪念的 Funes》(原名《纪念的 Funes el memorioso》)。这个故事诞生于作者失眠之后。这个故事讲述了患有<a class="ae ma" href="https://en.wikipedia.org/wiki/Spontaneous_recovery#Hypermnesia" rel="noopener ugc nofollow" target="_blank">健忘症</a>的 Ireneo Funes 的故事。在一次骑马事故后，富内斯发现他可以记住几乎所有的事情:一天中每时每刻云的形状，房子每个角落灯光的位置，两个月前他每分钟都做了什么，等等。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/abe176f918fc55adc5214f34de790d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*7wuPOLwqxz5-rskiu2wHMA.jpeg"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">公共领域下的博尔赫斯形象。</p></figure><p id="a0ba" class="pw-post-body-paragraph le lf iq lg b lh mn jr lj lk mo ju lm ln mp lp lq lr mq lt lu lv mr lx ly lz ij bi translated">在这个故事中，博尔赫斯探讨了关于我们生活中需要“遗忘的艺术”的几个方面的各种主题。通过完全记住每件事，Funes 失去了思维过程中最重要的特征之一:<strong class="lg ir">概括</strong>。Funes 不能理解为什么“狗”这个词可以把每只狗归为一类，如果它们明显不同的话。他可以很容易地区分眼睛发亮的小黑狗和左眼有红点的小黑狗，但他不明白是什么让狗成为狗。</p><h1 id="3570" class="km kn iq bd ko kp ms kr ks kt mt kv kw jw mu jx ky jz mv ka la kc mw kd lc ld bi translated">过度拟合或泛化能力的丧失</h1><p id="1659" class="pw-post-body-paragraph le lf iq lg b lh li jr lj lk ll ju lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">健忘症与其说是礼物，不如说是不幸。没有概括就不可能使用抽象思维。而且没有抽象思维，Funes 更接近机器而不是人。他进入了我们期望从机器学习中获得的相反方向。</p><p id="e9b2" class="pw-post-body-paragraph le lf iq lg b lh mn jr lj lk mo ju lm ln mp lp lq lr mq lt lu lv mr lx ly lz ij bi translated"><a class="ae ma" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank"> <strong class="lg ir">过度拟合</strong> </a>对于机器学习来说，就像健忘症对于 Funes 一样。过度拟合的模型无法区分噪声观测和基础模型。这是，他们不能一概而论。</p><p id="865a" class="pw-post-body-paragraph le lf iq lg b lh mn jr lj lk mo ju lm ln mp lp lq lr mq lt lu lv mr lx ly lz ij bi translated">下图显示了两个二元分类器(黑线和绿线)。过度拟合的分类器(绿线)非常依赖于训练数据，当新的观测值到达时，它很可能表现不佳。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/f07d42b0d12333eb9c03b6634ba309c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*PtEnMSx_EJr3h1YD"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated"><a class="ae ma" href="https://commons.wikimedia.org/wiki/User:Ignacio_Icke" rel="noopener ugc nofollow" target="_blank"> Ignacio Icke </a>在<a class="ae ma" href="https://commons.wikimedia.org/wiki/File:Overfitting.svg" rel="noopener ugc nofollow" target="_blank">维基百科</a>的知识共享下</p></figure><p id="3362" class="pw-post-body-paragraph le lf iq lg b lh mn jr lj lk mo ju lm ln mp lp lq lr mq lt lu lv mr lx ly lz ij bi translated"><strong class="lg ir">我如何知道我有一个过度拟合的模型？</strong> <br/>当您观察到训练集中的性能比测试集中的性能好得多时。</p><p id="57bf" class="pw-post-body-paragraph le lf iq lg b lh mn jr lj lk mo ju lm ln mp lp lq lr mq lt lu lv mr lx ly lz ij bi translated"><strong class="lg ir">那么，如何防止过度拟合呢？</strong></p><ul class=""><li id="e939" class="my mz iq lg b lh mn lk mo ln na lr nb lv nc lz nd ne nf ng bi translated">考虑足够大的数据集。如果你的数据集太小，你的模型会简单地记住所有的规则。</li><li id="6bee" class="my mz iq lg b lh nh lk ni ln nj lr nk lv nl lz nd ne nf ng bi translated"><a class="ae ma" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" rel="noopener ugc nofollow" target="_blank">交叉验证</a>时刻牢记。</li><li id="333b" class="my mz iq lg b lh nh lk ni ln nj lr nk lv nl lz nd ne nf ng bi translated"><a class="ae ma" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">正规化</a>总是有帮助的。</li><li id="ad33" class="my mz iq lg b lh nh lk ni ln nj lr nk lv nl lz nd ne nf ng bi translated"><a class="ae ma" href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="noopener ugc nofollow" target="_blank">模型的集合</a>有助于归纳。</li><li id="e7b6" class="my mz iq lg b lh nh lk ni ln nj lr nk lv nl lz nd ne nf ng bi translated">提前停车。迭代算法(CNN，DNN，RNN 等。)苦于<a class="ae ma" href="https://arxiv.org/abs/1611.06310" rel="noopener ugc nofollow" target="_blank">局部极小问题</a>。按时停车能给你更好的效果。</li></ul><p id="a823" class="pw-post-body-paragraph le lf iq lg b lh mn jr lj lk mo ju lm ln mp lp lq lr mq lt lu lv mr lx ly lz ij bi translated">希望你会考虑读 Funes the memorious 或任何博尔赫斯的故事。希望，当你找到下一个过度装配的模型时，你会想到有趣的事。</p><p id="01fa" class="pw-post-body-paragraph le lf iq lg b lh mn jr lj lk mo ju lm ln mp lp lq lr mq lt lu lv mr lx ly lz ij bi translated">最初发布于:<a class="ae ma" href="https://jmtirado.net/what-can-borges-teach-you-about-overfitting/" rel="noopener ugc nofollow" target="_blank">https://jmtirado . net/what-can-Borges-teach-you-about-over fitting/</a></p></div></div>    
</body>
</html>