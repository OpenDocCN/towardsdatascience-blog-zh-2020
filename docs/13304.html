<html>
<head>
<title>Building Offensive AI Agents for Doom using Dueling Deep Q-learning.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用决斗深度 Q-学习构建用于毁灭的攻击性 AI 代理。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f?source=collection_archive---------32-----------------------#2020-09-12">https://towardsdatascience.com/building-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f?source=collection_archive---------32-----------------------#2020-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5eec" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Pytorch 中的实现。</h2></div><h1 id="9436" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">简介</strong></h1><p id="a27d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在过去的几篇文章中，我们已经<a class="ae lw" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830" rel="noopener">讨论了</a>和<a class="ae lw" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">在<a class="ae lw" rel="noopener" target="_blank" href="/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2"> VizDoom 游戏环境</a>中实现了</a> <a class="ae lw" rel="noopener" target="_blank" href="/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc">深度 Q 学习</a> (DQN)和<a class="ae lw" rel="noopener" target="_blank" href="/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4">双重深度 Q 学习</a> (DDQN)，并评估了它们的性能。深度 Q-learning 是一种高度灵活且响应迅速的在线学习方法，它利用场景内的快速更新来估计环境中的状态-动作(Q)值，以便最大化回报。Double Deep Q-Learning 通过将负责动作选择和 TD-target 计算的网络解耦，以最小化 Q 值高估，在训练过程的早期，当代理尚未完全探索大多数可能的状态时，这个问题尤其明显。</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="mc md l"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">我们之前的双 DQN 特工<a class="ae lw" rel="noopener" target="_blank" href="/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4">在维兹杜姆环境中的表现，训练了 500 集。</a></p></figure><p id="5c5e" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">本质上，使用单个状态-动作值来判断情况需要探索和学习每个单个状态的动作的效果，从而导致模型的泛化能力的固有障碍。此外，并非所有国家都同样与环境相关。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/6a7b8994e6e45cfbfc96aba2aa32beb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/0*BXiHb5qoivaajbfx.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">来自我们之前在 Pong 中训练的代理的预处理帧。</p></figure><p id="9c31" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">回想一下我们早期的<a class="ae lw" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff" rel="noopener">实现</a>中的 Pong 环境。在我们的代理人击球后，向左或向右移动的价值可以忽略不计，因为球必须首先移动到对手，然后返回给球员。结果，在这一点上计算用于训练的状态-动作值可能破坏我们的代理的收敛。理想情况下，我们希望能够识别每个行为的价值，而无需了解每个状态的特定效果，以便鼓励我们的代理专注于选择与环境相关的行为。</p><p id="b129" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">决斗深度 Q 学习(以下简称 DuelDQN)通过将 DQN 网络输出分成两个流来解决这些缺点:价值流和优势(或行动)流。通过这样做，我们部分地分离了整个国家行为评估过程。在他们的开创性论文中。al 展示了 DuelDQN 如何影响 Atari 游戏 Enduro 中代理人表现的可视化，演示了代理人如何学习专注于不同的目标。请注意，价值流已经学会关注道路的方向，而优势流已经学会关注代理人面前的直接障碍。实质上，我们通过这种方法获得了一定程度的短期和中期远见。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/b03e7bb09874d6ca51ec9f2ce02bc68e.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*UCmAUiAJc0vJE8CjbgRSOw.png"/></div></figure><p id="652c" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">为了计算状态-动作的 Q 值，我们然后利用优势函数来告诉我们动作的相对重要性。<strong class="lc iu">平均优势的减法，计算一个状态中所有可能的动作，用于找到我们感兴趣的动作的相对优势。</strong></p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mr"><img src="../Images/4e6f2e2b5d746bdccfb00b0e5ea25e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LqSdAsw7Mp4Dsjfa.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">根据 DuelDQN 架构的状态动作的 Q 值。</p></figure><p id="8065" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">直观地说，为了获得对环境更可靠的评估，我们已经部分地分离了动作和状态值估计过程。</p><h1 id="9465" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">实现</strong></h1><p id="9d04" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">我们将在与上一篇文章<em class="mw">保卫防线</em>相同的多目标条件下，在相同的 VizDoomgym 场景中实现我们的方法。</strong>环境的一些特征包括:</p><ul class=""><li id="3d82" class="mx my it lc b ld mi lg mj lj mz ln na lr nb lv nc nd ne nf bi translated">一个 3 的动作空间:开火，左转，右转。不允许扫射。</li><li id="39b7" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">向玩家发射火球的棕色怪物，命中率为 100%。</li><li id="7f45" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">试图以之字形靠近来咬玩家的粉红色怪物。</li><li id="c1b5" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">重生的怪物可以承受更多伤害。</li><li id="2ca8" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">杀死一个怪物+1 点。</li><li id="82a0" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">-死了得 1 分。</li></ul><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/997a01f0f6c64bc034eb941989d2cb3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/0*MVHp2m0yx9sqmzf6.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">“防线方案”的初始状态</p></figure><p id="d02b" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">我们的 Google 协作实现是使用 Pytorch 用 Python 编写的，可以在<a class="ae lw" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> GradientCrescent Github 上找到。</a>我们的方法基于泰伯优秀强化学习<a class="ae lw" href="https://www.manning.com/livevideo/reinforcement-learning-in-motion" rel="noopener ugc nofollow" target="_blank">课程</a>中详述的方法。由于我们的 DuelDQN 实现类似于我们之前的香草 DQN 实现，所以整个高级工作流是共享的，这里不再重复<a class="ae lw" rel="noopener" target="_blank" href="/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2"/>。</p><p id="464c" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">让我们从导入所有必需的包开始，包括 OpenAI 和 Vizdoomgym 环境。我们还将安装火炬视觉所需的 AV 包，我们将使用它进行可视化。请注意，安装完成后，必须重新启动运行时，才能执行笔记本的其余部分。</p><pre class="lx ly lz ma gt nm nn no np aw nq bi"><span id="6020" class="nr kj it nn b gy ns nt l nu nv">#Visualization cobe for running within Colab<br/>!sudo apt-get update<br/>!sudo apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev libopenal-dev timidity libwildmidi-dev unzip</span><span id="ec3c" class="nr kj it nn b gy nw nt l nu nv"># Boost libraries<br/>!sudo apt-get install libboost-all-dev</span><span id="36f1" class="nr kj it nn b gy nw nt l nu nv"># Lua binding dependencies<br/>!apt-get install liblua5.1-dev<br/>!sudo apt-get install cmake libboost-all-dev libgtk2.0-dev libsdl2-dev python-numpy git<br/>!git clone <a class="ae lw" href="https://github.com/shakenes/vizdoomgym.git" rel="noopener ugc nofollow" target="_blank">https://github.com/shakenes/vizdoomgym.git</a><br/>!python3 -m pip install -e vizdoomgym/</span><span id="5528" class="nr kj it nn b gy nw nt l nu nv">!pip install av</span></pre><p id="9f06" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">接下来，我们初始化我们的环境场景，检查观察空间和动作空间，并可视化我们的环境。</p><pre class="lx ly lz ma gt nm nn no np aw nq bi"><span id="6c31" class="nr kj it nn b gy ns nt l nu nv">import gym<br/>import vizdoomgym</span><span id="b10a" class="nr kj it nn b gy nw nt l nu nv">env = gym.make('VizdoomDefendLine-v0')<br/>n_outputs = env.action_space.n<br/>print(n_outputs)</span><span id="40a3" class="nr kj it nn b gy nw nt l nu nv">observation = env.reset()</span><span id="ee8b" class="nr kj it nn b gy nw nt l nu nv">import matplotlib.pyplot as plt</span><span id="49fc" class="nr kj it nn b gy nw nt l nu nv">for i in range(22):<br/>  <br/>  if i &gt; 20:<br/>    print(observation.shape)<br/>    plt.imshow(observation)<br/>    plt.show()</span><span id="f7a2" class="nr kj it nn b gy nw nt l nu nv">observation, _, _, _ = env.step(1)</span></pre><p id="28a8" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">接下来，我们将定义预处理包装器。这些类继承自 OpenAI gym 基类，覆盖了它们的方法和变量，以便隐式地提供所有必要的预处理。我们将开始定义一个包装器来重复许多帧的每个动作，并执行元素方式的最大值以增加任何动作的强度。您会注意到一些三级参数，如<em class="mw"> fire_first </em>和<em class="mw">no _ ops</em>——这些是特定于环境的，在 Vizdoomgym 中对我们没有影响。</p><pre class="lx ly lz ma gt nm nn no np aw nq bi"><span id="8b71" class="nr kj it nn b gy ns nt l nu nv">class RepeatActionAndMaxFrame(gym.Wrapper):<br/>  #input: environment, repeat<br/>  #init frame buffer as an array of zeros in shape 2 x the obs space<br/>    def __init__(self, env=None, repeat=4, clip_reward=False, no_ops=0,<br/>                 fire_first=False):<br/>        super(RepeatActionAndMaxFrame, self).__init__(env)<br/>        self.repeat = repeat<br/>        self.shape = env.observation_space.low.shape<br/>        self.frame_buffer = np.zeros_like((2, self.shape))<br/>        self.clip_reward = clip_reward<br/>        self.no_ops = no_ops<br/>        self.fire_first = fire_first<br/>  def step(self, action):<br/>        t_reward = 0.0<br/>        done = False<br/>        for i in range(self.repeat):<br/>            obs, reward, done, info = self.env.step(action)<br/>            if self.clip_reward:<br/>                reward = np.clip(np.array([reward]), -1, 1)[0]<br/>            t_reward += reward<br/>            idx = i % 2<br/>            self.frame_buffer[idx] = obs<br/>            if done:<br/>                break<br/>        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])<br/>        return max_frame, t_reward, done, info<br/>  def reset(self):<br/>        obs = self.env.reset()<br/>        no_ops = np.random.randint(self.no_ops)+1 if self.no_ops &gt; 0    else 0<br/>        for _ in range(no_ops):<br/>            _, _, done, _ = self.env.step(0)<br/>            if done:<br/>                self.env.reset()<br/>        <br/>        if self.fire_first:<br/>            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'<br/>            obs, _, _, _ = self.env.step(1)<br/>        self.frame_buffer = np.zeros_like((2,self.shape))<br/>        self.frame_buffer[0] = obs<br/>    return obs</span></pre><p id="2db1" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">接下来，我们为我们的观察定义预处理函数。我们将使我们的环境对称，将它转换到标准化的盒子空间，将通道整数交换到张量的前面，并将其从原始(320，480)分辨率调整到(84，84)区域。我们也将我们的环境灰度化，并通过除以一个常数来归一化整个图像。</p><pre class="lx ly lz ma gt nm nn no np aw nq bi"><span id="f050" class="nr kj it nn b gy ns nt l nu nv">class PreprocessFrame(gym.ObservationWrapper):<br/>  #set shape by swapping channels axis<br/> #set observation space to new shape using gym.spaces.Box (0 to 1.0)<br/>    def __init__(self, shape, env=None):<br/>        super(PreprocessFrame, self).__init__(env)<br/>        self.shape = (shape[2], shape[0], shape[1])<br/>        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,<br/>                                    shape=self.shape, dtype=np.float32)<br/>   def observation(self, obs):<br/>        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)<br/>        resized_screen = cv2.resize(new_frame, self.shape[1:],<br/>                                    interpolation=cv2.INTER_AREA)<br/>        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)<br/>        new_obs = new_obs / 255.0<br/>   return new_obs</span></pre><p id="4a85" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">接下来，我们创建一个包装器来处理帧堆叠。这里的目标是通过将几个帧堆叠在一起作为单个批次，帮助从堆叠帧中捕捉运动和方向。这样，我们可以捕捉环境中元素的位置、平移、速度和加速度。通过堆叠，我们的输入采用(4，84，84，1)的形状。</p><pre class="lx ly lz ma gt nm nn no np aw nq bi"><span id="67b2" class="nr kj it nn b gy ns nt l nu nv">class StackFrames(gym.ObservationWrapper):<br/>  #init the new obs space (gym.spaces.Box) low &amp; high bounds as repeat of n_steps. These should have been defined for vizdooom<br/>  <br/>  #Create a return a stack of observations<br/>    def __init__(self, env, repeat):<br/>        super(StackFrames, self).__init__(env)<br/>        self.observation_space = gym.spaces.Box( env.observation_space.low.repeat(repeat, axis=0),<br/>                              env.observation_space.high.repeat(repeat, axis=0),<br/>                            dtype=np.float32)<br/>        self.stack = collections.deque(maxlen=repeat)<br/>    def reset(self):<br/>        self.stack.clear()<br/>        observation = self.env.reset()<br/>        for _ in range(self.stack.maxlen):<br/>            self.stack.append(observation)<br/>        return  np.array(self.stack).reshape(self.observation_space.low.shape)<br/>    def observation(self, observation):<br/>        self.stack.append(observation)<br/>    return np.array(self.stack).reshape(self.observation_space.low.shape)</span></pre><p id="b94b" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">最后，在返回最终环境供使用之前，我们将所有的包装器绑定到一个单独的<em class="mw"> make_env() </em>方法中。</p><pre class="lx ly lz ma gt nm nn no np aw nq bi"><span id="6011" class="nr kj it nn b gy ns nt l nu nv">def make_env(env_name, shape=(84,84,1), repeat=4, clip_rewards=False,<br/>             no_ops=0, fire_first=False):<br/>    env = gym.make(env_name)<br/>    env = PreprocessFrame(shape, env)<br/>    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards, no_ops, fire_first)<br/>    <br/>    env = StackFrames(env, repeat)<br/>    return env</span></pre><p id="15ac" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">接下来，让我们定义我们的模型，一个深度 Q 网络，具有决斗架构的两个输出。这基本上是一个三层卷积网络，它采用预处理的输入观测值，将生成的展平输出馈送到一个全连接层，然后将输出分成价值流(单节点输出)和优势流(节点输出对应于环境中的动作数量)。</p><p id="595b" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">请注意，这里没有激活层，因为激活层的存在会导致二进制输出分布。我们的损失是我们当前状态-动作的估计 Q 值和我们预测的状态-动作值的平方差。然后，我们附上 RMSProp 优化器，以尽量减少我们在培训期间的损失。</p><pre class="lx ly lz ma gt nm nn no np aw nq bi"><span id="60b5" class="nr kj it nn b gy ns nt l nu nv">import os<br/>import torch as T<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>import numpy as np</span><span id="197d" class="nr kj it nn b gy nw nt l nu nv">class DeepQNetwork(nn.Module):<br/>    def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):<br/>        super(DeepQNetwork, self).__init__()<br/>        self.checkpoint_dir = chkpt_dir<br/>        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)</span><span id="a44e" class="nr kj it nn b gy nw nt l nu nv">        self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)<br/>        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)<br/>        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)</span><span id="da0e" class="nr kj it nn b gy nw nt l nu nv">        fc_input_dims = self.calculate_conv_output_dims(input_dims)</span><span id="6586" class="nr kj it nn b gy nw nt l nu nv">        self.fc1 = nn.Linear(fc_input_dims,1024)<br/>        self.fc2 = nn.Linear(1024, 512)<br/>        #Here we split the linear layer into the State and Advantage streams<br/>        self.V = nn.Linear(512, 1)<br/>        self.A = nn.Linear(512, n_actions)</span><span id="67c0" class="nr kj it nn b gy nw nt l nu nv">        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)</span><span id="3bce" class="nr kj it nn b gy nw nt l nu nv">        self.loss = nn.MSELoss()<br/>        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')<br/>        self.to(self.device)</span><span id="f999" class="nr kj it nn b gy nw nt l nu nv">    def calculate_conv_output_dims(self, input_dims):<br/>        state = T.zeros(1, *input_dims)<br/>        dims = self.conv1(state)<br/>        dims = self.conv2(dims)<br/>        dims = self.conv3(dims)<br/>        return int(np.prod(dims.size()))</span><span id="21b0" class="nr kj it nn b gy nw nt l nu nv">    def forward(self, state):<br/>        conv1 = F.relu(self.conv1(state))<br/>        conv2 = F.relu(self.conv2(conv1))<br/>        conv3 = F.relu(self.conv3(conv2))<br/>        # conv3 shape is BS x n_filters x H x W<br/>        conv_state = conv3.view(conv3.size()[0], -1)<br/>        # conv_state shape is BS x (n_filters * H * W)<br/>        flat1 = F.relu(self.fc1(conv_state))<br/>        flat2 = F.relu(self.fc2(flat1))</span><span id="ce27" class="nr kj it nn b gy nw nt l nu nv">        V = self.V(flat2)<br/>        A = self.A(flat2)</span><span id="5a19" class="nr kj it nn b gy nw nt l nu nv">        return V, A</span><span id="2587" class="nr kj it nn b gy nw nt l nu nv">     def save_checkpoint(self):<br/>        print('... saving checkpoint ...')<br/>        T.save(self.state_dict(), self.checkpoint_file)</span><span id="1dfd" class="nr kj it nn b gy nw nt l nu nv">     def load_checkpoint(self):<br/>        print('... loading checkpoint ...')<br/>        self.load_state_dict(T.load(self.checkpoint_file))</span></pre><p id="a8cf" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">回想一下，用于决斗深度 Q 学习的更新函数需要以下内容:</p><ul class=""><li id="14bb" class="mx my it lc b ld mi lg mj lj mz ln na lr nb lv nc nd ne nf bi translated">当前状态<em class="mw"> s </em></li><li id="ccf1" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">当前动作<em class="mw">一</em></li><li id="e253" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">当前动作后的奖励<em class="mw"> r </em></li><li id="e039" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">下一个状态<em class="mw">s’</em></li><li id="bbd0" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">下一个动作<em class="mw">a’</em></li></ul><p id="3bd6" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">为了以有意义的数量提供这些参数，我们需要按照一组参数评估我们当前的策略，并将所有变量存储在一个缓冲区中，我们将在训练期间从该缓冲区中提取迷你批次中的数据。因此，我们需要一个重放内存缓冲区来存储和提取观察值。</p><pre class="lx ly lz ma gt nm nn no np aw nq bi"><span id="a8e2" class="nr kj it nn b gy ns nt l nu nv">import numpy as np<br/>class ReplayBuffer(object):<br/>    def __init__(self, max_size, input_shape, n_actions):<br/>        self.mem_size = max_size<br/>        self.mem_cntr = 0<br/>        self.state_memory = np.zeros((self.mem_size, *input_shape),<br/>                                     dtype=np.float32)<br/>        self.new_state_memory = np.zeros((self.mem_size, *input_shape),<br/>                                         dtype=np.float32)<br/>        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)<br/>        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)<br/>        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)<br/>#Identify index and store  the the current SARSA into batch memory</span><span id="867d" class="nr kj it nn b gy nw nt l nu nv">    def store_transition(self, state, action, reward, state_, done):<br/>        index = self.mem_cntr % self.mem_size<br/>        self.state_memory[index] = state<br/>        self.new_state_memory[index] = state_<br/>        self.action_memory[index] = action<br/>        self.reward_memory[index] = reward<br/>        self.terminal_memory[index] = done<br/>        self.mem_cntr += 1</span><span id="cfbd" class="nr kj it nn b gy nw nt l nu nv">    def sample_buffer(self, batch_size):<br/>        max_mem = min(self.mem_cntr, self.mem_size)<br/>        batch = np.random.choice(max_mem, batch_size, replace=False)<br/>       <br/>        states = self.state_memory[batch]<br/>        actions = self.action_memory[batch]<br/>        rewards = self.reward_memory[batch]<br/>        states_ = self.new_state_memory[batch]<br/>        terminal = self.terminal_memory[batch]<br/>     return states, actions, rewards, states_, terminal</span></pre><p id="00a8" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">接下来，我们将定义我们的代理，它不同于我们普通的 DQN 实现。我们的代理正在使用一个勘探率递减的ε贪婪策略，以便随着时间的推移最大化开发。为了学会预测使我们的累积奖励最大化的状态和优势值，我们的代理将使用通过抽样存储的记忆获得的贴现的未来奖励。</p><p id="0d0f" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">您会注意到，作为代理的一部分，我们初始化了 DQN 的两个副本，并使用方法将原始网络的权重参数复制到目标网络中。虽然我们的常规方法利用这种设置来生成固定的 TD-目标，但在我们的 DuelDQN 方法中，双流的存在给该过程增加了一层复杂性:</p><ul class=""><li id="731e" class="mx my it lc b ld mi lg mj lj mz ln na lr nb lv nc nd ne nf bi translated">从重放存储器中检索状态、动作、奖励和下一状态(sar)。</li><li id="061a" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">评估网络用于生成当前状态的优势(<em class="mw"> A_s </em>)和状态(<em class="mw"> V_s </em>)值。</li><li id="1afc" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">目标网络还用于创建下一个状态的优势(<em class="mw"> A_s_ </em>)和状态(<em class="mw"> V_s_ </em>)值。</li><li id="cc0b" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">预测的 Q 值是通过将当前状态的优势和状态值相加，并减去用于归一化的当前状态优势值的平均值而生成的。</li><li id="0949" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">通过将下一个状态的优势和状态值相加，并减去下一个状态优势值的平均值以进行归一化，来计算目标 Q 值当前状态。</li><li id="a94d" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">然后，通过将折扣后的目标 Q 值与当前状态奖励相结合，构建 TD 目标。</li><li id="db74" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">通过将 TD 目标与预测的 Q 值进行比较来计算损失函数，然后将其用于训练网络。</li></ul><pre class="lx ly lz ma gt nm nn no np aw nq bi"><span id="9b7d" class="nr kj it nn b gy ns nt l nu nv">import numpy as np<br/>import torch as T<br/>#from deep_q_network import DeepQNetwork<br/>#from replay_memory import ReplayBuffer</span><span id="cde5" class="nr kj it nn b gy nw nt l nu nv">class DuelDQNAgent(object):<br/>    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,<br/>                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,<br/>                 replace=1000, algo=None, env_name=None, chkpt_dir='tmp/dqn'):<br/>        self.gamma = gamma<br/>        self.epsilon = epsilon<br/>        self.lr = lr<br/>        self.n_actions = n_actions<br/>        self.input_dims = input_dims<br/>        self.batch_size = batch_size<br/>        self.eps_min = eps_min<br/>        self.eps_dec = eps_dec<br/>        self.replace_target_cnt = replace<br/>        self.algo = algo<br/>        self.env_name = env_name<br/>        self.chkpt_dir = chkpt_dir<br/>        self.action_space = [i for i in range(n_actions)]<br/>        self.learn_step_counter = 0</span><span id="063d" class="nr kj it nn b gy nw nt l nu nv">        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)</span><span id="822b" class="nr kj it nn b gy nw nt l nu nv">        self.q_eval = DeepQNetwork(self.lr, self.n_actions,<br/>                                    input_dims=self.input_dims,<br/>                                    name=self.env_name+'_'+self.algo+'_q_eval',<br/>                                    chkpt_dir=self.chkpt_dir)</span><span id="3192" class="nr kj it nn b gy nw nt l nu nv">        self.q_next = DeepQNetwork(self.lr, self.n_actions,<br/>                                    input_dims=self.input_dims,<br/>                                    name=self.env_name+'_'+self.algo+'_q_next',<br/>                                    chkpt_dir=self.chkpt_dir)</span><span id="ba00" class="nr kj it nn b gy nw nt l nu nv">#Epsilon greedy action selection<br/>    def choose_action(self, observation):<br/>        if np.random.random() &gt; self.epsilon:<br/>          # Add dimension to observation to match input_dims x batch_size by placing in list, then converting to tensor<br/>            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)<br/>            #As our forward function now has both state and advantage, fetch latter for actio selection<br/>            _, advantage = self.q_eval.forward(state)<br/>            action = T.argmax(advantage).item()<br/>        else:<br/>            action = np.random.choice(self.action_space)</span><span id="0aeb" class="nr kj it nn b gy nw nt l nu nv">        return action</span><span id="cf75" class="nr kj it nn b gy nw nt l nu nv">     def store_transition(self, state, action, reward, state_, done):<br/>        self.memory.store_transition(state, action, reward, state_, done)</span><span id="a4d8" class="nr kj it nn b gy nw nt l nu nv">     def sample_memory(self):<br/>        state, action, reward, new_state, done = \<br/>                                    self.memory.sample_buffer(self.batch_size)</span><span id="ab93" class="nr kj it nn b gy nw nt l nu nv">     states = T.tensor(state).to(self.q_eval.device)<br/>        rewards = T.tensor(reward).to(self.q_eval.device)<br/>        dones = T.tensor(done).to(self.q_eval.device)<br/>        actions = T.tensor(action).to(self.q_eval.device)<br/>        states_ = T.tensor(new_state).to(self.q_eval.device)</span><span id="db26" class="nr kj it nn b gy nw nt l nu nv">        return states, actions, rewards, states_, dones</span><span id="c7e9" class="nr kj it nn b gy nw nt l nu nv">     def replace_target_network(self):<br/>        if self.learn_step_counter % self.replace_target_cnt == 0:<br/>            self.q_next.load_state_dict(self.q_eval.state_dict())</span><span id="9f1e" class="nr kj it nn b gy nw nt l nu nv">     def decrement_epsilon(self):<br/>        self.epsilon = self.epsilon - self.eps_dec \<br/>                           if self.epsilon &gt; self.eps_min else self.eps_min</span><span id="55ae" class="nr kj it nn b gy nw nt l nu nv">     def save_models(self):<br/>        self.q_eval.save_checkpoint()<br/>        self.q_next.save_checkpoint()</span><span id="3848" class="nr kj it nn b gy nw nt l nu nv">     def load_models(self):<br/>        self.q_eval.load_checkpoint()<br/>        self.q_next.load_checkpoint()<br/>    <br/>    def learn(self):<br/>        if self.memory.mem_cntr &lt; self.batch_size:<br/>            return</span><span id="a6fc" class="nr kj it nn b gy nw nt l nu nv">        self.q_eval.optimizer.zero_grad()</span><span id="4184" class="nr kj it nn b gy nw nt l nu nv">        #Replace target network if appropriate<br/>        self.replace_target_network()</span><span id="9ddd" class="nr kj it nn b gy nw nt l nu nv">        states, actions, rewards, states_, dones = self.sample_memory()<br/>        #Fetch states and advantage actions for current state using eval network<br/>        #Also fetch the same for next state using target network<br/>        V_s, A_s = self.q_eval.forward(states)<br/>        V_s_, A_s_ = self.q_next.forward(states_)</span><span id="2ec8" class="nr kj it nn b gy nw nt l nu nv">        #Indices for matrix multiplication<br/>        indices = np.arange(self.batch_size)</span><span id="4d33" class="nr kj it nn b gy nw nt l nu nv">        #Calculate current state Q-values and next state max Q-value by aggregation, subtracting constant advantage mean<br/>       <br/>        q_pred = T.add(V_s,<br/>                        (A_s - A_s.mean(dim=1, keepdim=True)))[indices, actions]<br/>        <br/>        q_next = T.add(V_s_,<br/>                        (A_s_ - A_s_.mean(dim=1, keepdim=True))).max(dim=1)[0]</span><span id="fef5" class="nr kj it nn b gy nw nt l nu nv">        q_next[dones] = 0.0<br/>        #Build your target using the current state reward and q_next<br/>        q_target = rewards + self.gamma*q_next</span><span id="4e4e" class="nr kj it nn b gy nw nt l nu nv">         loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)<br/>        loss.backward()<br/>        self.q_eval.optimizer.step()<br/>        self.learn_step_counter += 1</span><span id="f3ca" class="nr kj it nn b gy nw nt l nu nv">        self.decrement_epsilon()</span></pre><p id="9803" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">定义了所有支持代码后，让我们运行主训练循环。我们已经在最初的总结中定义了大部分，但是让我们为后代回忆一下。</p><ul class=""><li id="3476" class="mx my it lc b ld mi lg mj lj mz ln na lr nb lv nc nd ne nf bi translated">对于训练集的每一步，在使用ε-贪婪策略选择下一个动作之前，我们将输入图像堆栈输入到我们的网络中，以生成可用动作的概率分布</li><li id="1aa2" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">然后，我们将它输入到网络中，获取下一个状态和相应奖励的信息，并将其存储到我们的缓冲区中。我们更新我们的堆栈，并通过一些预定义的步骤重复这一过程。</li><li id="40be" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">在一集的结尾，我们将下一个状态输入到我们的网络中，以便获得下一个动作。我们还通过对当前奖励进行贴现来计算下一个奖励。</li><li id="9d24" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">我们通过上面提到的 Q 学习更新函数生成我们的目标 y 值，并训练我们的网络。</li><li id="d2b2" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">通过最小化训练损失，我们更新网络权重参数，以便为下一个策略输出改进的状态-动作值。</li><li id="7971" class="mx my it lc b ld ng lg nh lj ni ln nj lr nk lv nc nd ne nf bi translated">我们通过跟踪模型的平均得分(在 100 个训练步骤中测量)来评估模型。</li></ul><pre class="lx ly lz ma gt nm nn no np aw nq bi"><span id="d703" class="nr kj it nn b gy ns nt l nu nv">env = make_env('VizdoomDefendLine-v0')<br/>best_score = -np.inf<br/>load_checkpoint = False<br/>n_games = 2000<br/>agent = DuelDQNAgent(gamma=0.99, epsilon=1.0, lr=0.0001,input_dims=(env.observation_space.shape),n_actions=env.action_space.n, mem_size=5000, eps_min=0.1,batch_size=32, replace=1000, eps_dec=1e-5,chkpt_dir='/content/', algo='DuelDQNAgent',env_name='vizdoogym')</span><span id="4041" class="nr kj it nn b gy nw nt l nu nv">if load_checkpoint:<br/>  agent.load_models()</span><span id="f555" class="nr kj it nn b gy nw nt l nu nv">fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) +'_'+ str(n_games) + 'games'<br/>figure_file = 'plots/' + fname + '.png'</span><span id="4340" class="nr kj it nn b gy nw nt l nu nv">n_steps = 0<br/>scores, eps_history, steps_array = [], [], []</span><span id="4059" class="nr kj it nn b gy nw nt l nu nv">for i in range(n_games):<br/>  done = False<br/>  observation = env.reset()</span><span id="2471" class="nr kj it nn b gy nw nt l nu nv">  score = 0<br/>  while not done:<br/>    action = agent.choose_action(observation)<br/>    observation_, reward, done, info = env.step(action)<br/>    score += reward</span><span id="c1ab" class="nr kj it nn b gy nw nt l nu nv">    if not load_checkpoint:<br/>      agent.store_transition(observation, action,reward, observation_, int(done))<br/>      agent.learn()<br/>    observation = observation_<br/>    n_steps += 1</span><span id="8afb" class="nr kj it nn b gy nw nt l nu nv">scores.append(score)<br/>steps_array.append(n_steps)</span><span id="3543" class="nr kj it nn b gy nw nt l nu nv">avg_score = np.mean(scores[-100:])</span><span id="ca5a" class="nr kj it nn b gy nw nt l nu nv">if avg_score &gt; best_score:<br/>    best_score = avg_score<br/>      <br/>    <br/>    print('Checkpoint saved at episode ', i)<br/>    agent.save_models()</span><span id="af7f" class="nr kj it nn b gy nw nt l nu nv">print('Episode: ', i,'Score: ', score,' Average score: %.2f' % avg_score, 'Best average: %.2f' % best_score,'Epsilon: %.2f' % agent.epsilon, 'Steps:', n_steps)</span><span id="5ea9" class="nr kj it nn b gy nw nt l nu nv">eps_history.append(agent.epsilon)<br/>  if load_checkpoint and n_steps &gt;= 18000:<br/>    break</span></pre><p id="3787" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">我们绘制了 500 集、1000 集和 2000 集的代理商平均分和我们的 epsilon 值。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/65e6d17228c3857757a5c6c5bf01e39e.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*clCcIHjWLKkTgKIpPNQvaw.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">500 集后我们经纪人的奖励分配。</p></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/011359e016316c077090f4b21462e51a.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*9_nlH7XLUNHF0kHx3LD6MQ.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">1000 集后我们经纪人的奖励分配。</p></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/6cd6abff0a221e7eb953dd3c8048bcc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*tZ0gtTtpbSq6XmkV899-BA.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">2000 集后我们经纪人的报酬分配。</p></figure><p id="7a60" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">查看结果并将其与我们的普通 DQN <a class="ae lw" rel="noopener" target="_blank" href="/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2">实施</a>和双 DQN <a class="ae lw" rel="noopener" target="_blank" href="/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4">实施</a>进行比较，您会注意到在 500、1000 和 2000 集的分布中有显著提高的改善率。此外，具有更受约束的报酬振荡，表明当比较任一实现时改进了收敛。</p><p id="02ee" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">我们可以想象我们的代理人在 500 和 1000 集以下的表现。</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="mc md l"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">500 集的代理性能。</p></figure><p id="f28a" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">在 500 集时，代理已经采用了先前在较高训练时间为 DQN 和 DDQN 识别的相同策略，归因于在局部最小值的收敛。仍然会采取一些攻击性的行动，但是主要的策略仍然依赖于怪物之间的友好射击。</p><p id="e97f" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">at 1000 集呢？</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="mc md l"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">1000 集的代理性能。</p></figure><p id="ee03" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">我们的经纪人已经成功地打破了本地化的最低限度，并发现了一个更具进攻性的角色为导向的替代战略。这是我们的 DQN 和 DDQN 模型都无法做到的，即使是在 2000 集的情况下——证明了 DuelDQN 的双流方法在识别和优先考虑与环境相关的行动方面的效用。</p><p id="f7e1" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">这就结束了双深度 Q 学习的实现。在我们的下一篇文章中，我们将通过把我们所学的所有知识结合到一个方法中来完成我们的 Q 学习方法系列，并在一个更动态的结局中使用它。</p><p id="4724" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">我们希望你喜欢这篇文章，并希望你查看 GradientCrescent 上的许多其他文章，涵盖人工智能的应用和理论方面。为了保持对<a class="ae lw" href="https://medium.com/@adrianitsaxu" rel="noopener"> GradientCrescent </a>的最新更新，请考虑关注该出版物并关注我们的<a class="ae lw" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> Github </a>资源库</p><h1 id="737a" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">来源</h1><p id="cce5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">萨顿等人。al，“强化学习”</p><p id="2f0e" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">塔博尔，“运动中的强化学习”</p><p id="a317" class="pw-post-body-paragraph la lb it lc b ld mi ju lf lg mj jx li lj mk ll lm ln ml lp lq lr mm lt lu lv im bi translated">西蒙尼尼，<a class="ae lw" href="https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/" rel="noopener ugc nofollow" target="_blank">“深度 Q 学习的改进* </a></p></div></div>    
</body>
</html>