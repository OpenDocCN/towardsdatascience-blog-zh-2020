<html>
<head>
<title>Accelerate your training and inference running on Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Tensorflow上加速您的训练和推理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/accelerate-your-training-and-inference-running-on-tensorflow-896aa963aa70?source=collection_archive---------3-----------------------#2020-01-05">https://towardsdatascience.com/accelerate-your-training-and-inference-running-on-tensorflow-896aa963aa70?source=collection_archive---------3-----------------------#2020-01-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="84e0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">您是否使用默认设置运行Tensorflow？您可以轻松地将其优化到您的CPU/GPU，并获得高达3倍的加速。</h2></div><p id="180a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Tensorflow带有默认设置，可以兼容尽可能多的CPU/GPU。您可以轻松优化它，以充分利用CPU(如AVX)或GPU(如张量内核)的全部功能，从而实现高达3倍的代码加速。</p><p id="b9ca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，如果你是一家初创公司，你可能无法无限制地访问GPU，或者在CPU上部署一个模型，你仍然可以优化Tensorflow代码，以减少其大小，从而在任何设备上更快地进行推理。下面我将讨论几种方法来加速你的训练或推理或两者兼而有之。</p><h2 id="f3c4" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">从源构建张量流</h2><p id="f1d1" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">通过pip安装Tensorflow最流行的方式，但是这样的安装非常慢。为什么？</p><p id="55c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">来自<code class="fe mc md me mf b">pip install tensorflow</code>的默认构建旨在与尽可能多的CPU兼容。如果你曾经在运行Tensorflow程序时看到过控制台中的日志，你一定看到过这样的警告——“<em class="mg">你的CPU支持该TensorFlow二进制文件未被编译使用的指令:av x2 FMA”</em></p><p id="c850" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个警告是什么意思？现代CPU提供了许多对低级指令集的扩展，如SSE2、SSE4、AVX等</p><blockquote class="mh mi mj"><p id="54a9" class="ki kj mg kk b kl km ju kn ko kp jx kq mk ks kt ku ml kw kx ky mm la lb lc ld im bi translated"><strong class="kk iu">如果你有一个GPU </strong>，你不应该关心AVX支持，因为最昂贵的操作将被分派在一个GPU设备上(除非明确设置不要)</p></blockquote><p id="b654" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从源代码本身构建它可能会显著加快Tensorflow程序的速度。TensorFlow实际上警告你。您应该<a class="ae mn" href="https://www.tensorflow.org/install/source" rel="noopener ugc nofollow" target="_blank">从针对<em class="mg">您的</em> CPU优化的源</a>构建TensorFlow，启用AVX、AVX2和FMA，无论您的CPU支持哪个。</p><h2 id="a120" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">XLA加速线性代数</h2><p id="00d6" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">加速线性代数，XLA是一个特定领域的矩阵运算编译器。它可以在不改变源代码的情况下加速张量流模型。</p><p id="1123" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当TensorFlow程序运行时，所有操作都由TensorFlow执行器单独执行。每个TensorFlow操作都有一个预编译的GPU内核实现，执行器将被调度到这个内核实现。</p><p id="b16f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">XLA提供了运行模型的另一种模式:它将张量流图编译成一系列专门为给定模型生成的计算内核。因为这些内核是模型特有的，所以它们可以利用特定于模型的信息进行优化。与许多其他优化一样，融合是XLA最重要的优化，我将在本文后面详细讨论。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mo"><img src="../Images/2859bfdfc72e507020067f9e50147633.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pomKlaTAV9YmGJZUNtrkfg.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated"><a class="ae mn" href="https://www.tensorflow.org/xla" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/xla</a></p></figure><p id="7dbc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果是速度和内存使用的提高:启用XLA后，大多数内部基准测试的运行速度提高了约1.15倍。</p><p id="d75f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">启用XLA很容易-</p><pre class="mp mq mr ms gt ne mf nf ng aw nh bi"><span id="bf3d" class="le lf it mf b gy ni nj l nk nl">import tensorflow as tf<br/><br/>tf.config.optimizer.set_jit(True)<br/><br/># ... the rest of your program ..</span></pre><p id="2c63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">试试XLA在Colab的例子<a class="ae mn" href="https://www.tensorflow.org/xla/tutorials/xla_compile" rel="noopener ugc nofollow" target="_blank">这里</a></p><h2 id="e32d" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">NVIDIA GPUs上的混合精度</h2><p id="253d" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated"><a class="ae mn" href="https://arxiv.org/abs/1710.03740" rel="noopener ugc nofollow" target="_blank">混合精度</a>训练通过以半精度格式执行运算，同时以单精度存储最少的信息，以在网络的关键部分保留尽可能多的信息，从而显著提高了计算速度。</p><p id="715e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用精度低于32位浮点的数字格式有很多好处。首先，它们需要更少的内存，能够训练和部署更大的神经网络。其次，它们需要更少的内存带宽，从而加快了数据传输操作。第三，在精度降低的情况下，数学运算运行得更快，尤其是在支持该精度的张量核GPU上。它通过识别需要完全精度的步骤，并仅对这些步骤使用32位浮点，而对其他所有步骤使用16位浮点来实现这一点。</p><ul class=""><li id="aec1" class="nm nn it kk b kl km ko kp kr no kv np kz nq ld nr ns nt nu bi translated">通过使用<a class="ae mn" href="https://developer.nvidia.com/tensor-cores" rel="noopener ugc nofollow" target="_blank">张量核，加速数学密集型运算，如线性和卷积层。</a></li><li id="70d3" class="nm nn it kk b kl nv ko nw kr nx kv ny kz nz ld nr ns nt nu bi translated">与单精度相比，通过访问一半的字节来加速内存受限的操作。</li><li id="3a9f" class="nm nn it kk b kl nv ko nw kr nx kv ny kz nz ld nr ns nt nu bi translated">降低训练模型的内存需求，支持更大的模型或更大的小批量。</li></ul><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0ae765382bcfe82024e083d106c73e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*Z1g0EA-ZdzamCIlCjlmV6Q.png"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">用于ImageNet分类的ResNet-50训练—DGX-1上的8个GPU与FP32训练相比→3倍加速—同等精度来源— Nvidia</p></figure><p id="bee8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在NVIDIA GPUs中，计算能力为7.0或更高的GPU将从混合精度中获得最大的性能优势，因为它们有特殊的硬件单元，称为张量核心，以加速浮点矩阵乘法和卷积。</p><p id="38d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">张量流中的混合精度</strong></p><p id="a3e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae mn" href="https://www.tensorflow.org/guide/keras/mixed_precision" rel="noopener ugc nofollow" target="_blank">混合精度API </a>在TensorFlow 2.1中提供，带有Keras接口。要在Keras中使用混合精度，您需要创建，通常称为<em class="mg"> dtype策略</em>。Dtype策略指定了将在其中运行的dtype层。这将导致后续创建的层使用混合精度，混合使用float16和float32。</p><pre class="mp mq mr ms gt ne mf nf ng aw nh bi"><span id="6c39" class="le lf it mf b gy ni nj l nk nl">from tensorflow.keras.mixed_precision import experimental as mixed_precision<br/>policy = mixed_precision.Policy('mixed_float16')<br/>mixed_precision.set_policy(policy)<br/># Now design your model and train it</span></pre><blockquote class="mh mi mj"><p id="fe40" class="ki kj mg kk b kl km ju kn ko kp jx kq mk ks kt ku ml kw kx ky mm la lb lc ld im bi translated">小鬼。注:提供混合精度的张量核需要张量的某些维度，例如稠密层的维度、Conv层中的过滤器数量、RNN层中的单元数量是8的倍数。</p></blockquote><p id="d1ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要比较混合精度与float32的性能，请将策略从<code class="fe mc md me mf b">mixed_float16</code>更改为<code class="fe mc md me mf b">float32.</code>，预期性能提升高达3倍。</p><h2 id="9d19" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">通过模型修剪改善推理延迟</h2><p id="6998" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">我已经在我以前的<a class="ae mn" rel="noopener" target="_blank" href="/pruning-deep-neural-network-56cae1ec5505">博客</a>中报道过这个概念。简而言之，修剪是如何工作的</p><p id="fafa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你能根据神经元的贡献大小对它们之间的神经元或连接进行排序，那么你就可以从网络中移除排序较低的神经元或连接，从而形成一个更小更快的网络。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ob"><img src="../Images/2ed47abc568363a3d89cddd2b64fe8dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Ia2mbCfmOujeFZU9EIV8Q.png"/></div></div></figure><p id="80e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">tensor flow中的修剪</strong></p><p id="f012" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Tensorflow提供了<a class="ae mn" href="https://github.com/tensorflow/model-optimization" rel="noopener ugc nofollow" target="_blank">模型优化工具包</a>，用于剪枝和其他训练后优化。在你的代码中使用它，这里有一个简单的例子-</p><pre class="mp mq mr ms gt ne mf nf ng aw nh bi"><span id="fdae" class="le lf it mf b gy ni nj l nk nl">import tensorflow_model_optimization as tfmot<br/><br/>model = build_your_model()  <br/><br/>pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(<br/>                        initial_sparsity=0.0, final_sparsity=0.5,<br/>                        begin_step=1000, end_step=3000)<br/><br/>model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)<br/><br/>... <br/><br/>model_for_pruning.fit(...)</span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi oc"><img src="../Images/010834f23f25906faf631d66b245fe80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wnY4AuWI3QY6TZ0nO9klGw.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated"><a class="ae mn" href="https://medium.com/tensorflow/introducing-the-model-optimization-toolkit-for-tensorflow-254aca1ba0a3" rel="noopener">https://medium . com/tensor flow/introducing-the-model-optimization-toolkit-for-tensor flow-254 ACA 1 ba 0a 3</a></p></figure><h2 id="5035" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">将多个操作融合为一个</h2><p id="58f3" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">通常当你运行一个张量流图时，所有的操作都是由张量流图执行器单独执行的。每个op都有一个预编译的GPU内核实现。融合操作将操作整合到一个内核中，以提高性能。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi od"><img src="../Images/3943bbe2635867c27466d1affe9a72cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E1j3UY71kx0u1SK9X5tMDQ.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">op融合示例</p></figure><p id="207a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">比如说-</p><pre class="mp mq mr ms gt ne mf nf ng aw nh bi"><span id="579a" class="le lf it mf b gy ni nj l nk nl">def model_fn(x, y, z): <br/>  return tf.reduce_sum(x + y * z)</span></pre><p id="ba67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">没有融合，没有XLA，该图启动三个内核:一个用于乘法，一个用于加法，一个用于减法。</p><p id="42df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用op fusion，您可以在一次内核启动中计算结果。它通过将加法、乘法和减法“融合”到一个GPU内核中来实现这一点。</p><p id="4851" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">与Tensorflow 2.x的融合</strong></p><p id="7e8b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更新的Tensorflow版本带有XLA，它为我们进行了融合和其他优化。</p><pre class="mp mq mr ms gt ne mf nf ng aw nh bi"><span id="ce3b" class="le lf it mf b gy ni nj l nk nl">from tensorflow.contrib.compiler import xla<br/><br/>def model_fn(x, y, z):<br/>  return tf.reduce_sum(x + y * z)<br/><br/>def create_and_run_graph():<br/>  with tf.Session() as sess:<br/>    x = tf.placeholder(tf.float32, name='x')<br/>    y = tf.placeholder(tf.float32, name='y')<br/>    z = tf.placeholder(tf.float32, name='z')<br/>    result = xla.compile(computation=model_fn, inputs=(x, y, z))[0]<br/>    # `result` is a normal Tensor (albeit one that is computed by an XLA<br/>    # compiled executable) and can be used like any other Tensor.<br/>    result = tf.add(result, result)<br/>    return sess.run(result, feed_dict={ ... })</span></pre><p id="c7b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">融合模式示例:</p><p id="9e2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">■ Conv2D + BiasAdd + <activation/></p><p id="66dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">■ Conv2D + FusedBatchNorm + <activation/></p><p id="c3da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">■ Conv2D +挤压+ BiasAdd</p><p id="7b4c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">■ MatMul + BiasAdd + <activation/></p><p id="eedd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将ops融合在一起提供了几个性能优势:</p><p id="7a2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">○完全消除运营调度开销(廉价运营的重大胜利)</p><p id="d98e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">○增加ILP、矢量化等的机会。</p><p id="ab86" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">○提高数据访问的时间和空间局部性</p><p id="9a49" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，MatMul是逐块计算的，并且当数据在高速缓存中仍然“热”时，可以应用偏置和激活函数。</p><p id="5c97" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">与Tensorflow 1.x的融合</strong></p><p id="68bd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在Tf 1.x中，与fused ops兼容的层具有“fused”参数，该参数需要设置为True以使用fusion来加快实现。</p><p id="ca76" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">比如说-</p><pre class="mp mq mr ms gt ne mf nf ng aw nh bi"><span id="ec51" class="le lf it mf b gy ni nj l nk nl">#Using TF1.x in TF2.x<br/>b1 = tf<strong class="mf iu">.</strong>layers<strong class="mf iu">.</strong>batch_normalization(<br/>    input_layer, fused<strong class="mf iu">=</strong>True, data_format<strong class="mf iu">=</strong>'NCHW')<!-- --> <br/><br/>#Or in pure TF1.x<br/>b1 = tf.layers.batch_normalization</span></pre></div></div>    
</body>
</html>