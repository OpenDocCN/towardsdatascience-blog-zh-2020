<html>
<head>
<title>An adaptive lasso for python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">python 的自适应套索</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-adaptive-lasso-63afca54b80d?source=collection_archive---------20-----------------------#2020-08-17">https://towardsdatascience.com/an-adaptive-lasso-63afca54b80d?source=collection_archive---------20-----------------------#2020-08-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e03b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何构建一个了解真相的 oracle 估算器(带代码！)</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c894e3898376a943efd74b85e1487421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DNixzSNEmv2e5dXT"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Pierre Bamin 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0ef0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我关于惩罚回归系列的第二篇文章。在第一篇文章中，我们讨论了如何在 python 中实现<a class="ae kv" rel="noopener" target="_blank" href="/sparse-group-lasso-in-python-255e379ab892">稀疏组套索，这是当今回归模型可用的最佳变量选择替代方案之一，但今天我想更进一步，介绍一下<strong class="ky ir">自适应思想</strong>，它可以将您的回归估计器转换为<strong class="ky ir"> oracle </strong>，了解数据集的真相。</a></p><p id="51a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">今天我们将看到:</p><ul class=""><li id="e4ad" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">拉索</strong>(和其他非自适应估计器)面临的<strong class="ky ir">问题</strong>是什么</li><li id="19cd" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">什么是<strong class="ky ir"> oracle 属性</strong>以及为什么您应该使用<strong class="ky ir"> oracle 估计器</strong></li><li id="cc30" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如何获得<strong class="ky ir">自适应套索</strong>估计器</li><li id="cb7c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如何用 python 实现一个<strong class="ky ir">自适应估计器</strong></li></ul><h1 id="0e71" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">套索惩罚的问题</h1><p id="9fa1" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">先简单介绍一下<strong class="ky ir">套索回归</strong>。假设您正在处理一个数据集，其中您知道只有几个变量真正与响应变量相关，但您不知道是哪些变量。也许你正在处理一个<strong class="ky ir">高维数据集</strong>，它的变量比观测值<strong class="ky ir">、</strong>多，其中一个简单的线性回归模型无法求解。例如，一个由数千个基因组成的基因数据集，但其中只有少数基因与疾病相关。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/3460c40df9e00df18884913fe73a05a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BInI2-XLtmM9Xd5R319_wg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者制作的图像。</p></figure><p id="757f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，您决定使用 lasso，这是一种向回归模型的β系数添加 L1 约束的惩罚方法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/2ce2efe0066cc034dd8cd5c330ea47ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*prBntzPuwvKkGUIcHxLvlg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">线性回归中的套索公式。</p></figure><p id="8e78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这样，您将获得<strong class="ky ir">稀疏的解决方案，</strong>意味着许多β系数将被发送到 0，并且您的模型将基于少数不为 0 的系数进行预测。</p><p id="5e64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过降低<strong class="ky ir">模型复杂性</strong>(不等于 0 的变量数量)，您已经潜在地<strong class="ky ir">降低了您的模型的预测误差</strong>。但是作为一个副作用，你已经<strong class="ky ir">增加了β估计的偏差</strong>(这被称为方差偏差权衡)。</p><blockquote class="nf ng nh"><p id="51a5" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">Lasso 提供稀疏的<strong class="ky ir">解决方案，这些解决方案是有偏差的</strong>，因此 lasso 选择的有意义的变量可能与真正有意义的变量不同。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/53c1d798a8321ae62150a5aa3586f0df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*krcBqz7DBQIomwpui1vDhA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者制作的图像。</p></figure><p id="aed2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其他惩罚如岭回归和稀疏群套索面临同样的问题:它们提供了有偏见的解决方案，因此无法识别我们模型中真正有意义的变量。</p><h1 id="5fe0" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">oracle 属性</h1><p id="c9ad" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们的目标很明确:<strong class="ky ir">我们想要一个没有偏见的解决方案</strong>，这样我们就可以从数据集中选择变量，就好像我们事先知道哪些是真正重要的变量一样。就像我们的评估者是知道真相的先知一样。</p><p id="2be9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我知道，将“oracle”称为回归估计量听起来像是我想出来的，但它实际上有一个数学形式的定义，由范和李(2001)提出。如果一个估计量能够以收敛到 1 的概率正确地选择模型中的非零系数，并且非零系数是渐近正态分布的，那么这个估计量就是预测的。</p><p id="65b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着给定一组<em class="ni"> p </em>变量{β1，…，βp}，如果我们考虑两个子集，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/7c5e8f139d6c0d3dc259e9fdba15ae62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*iYmhmks55M_NC68-0hT2Hg.png"/></div></figure><blockquote class="nf ng nh"><p id="a709" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">oracle 估计器选择概率趋于 1 的真正重要的变量。渐近地，两个子集重合。</p></blockquote><h1 id="f63d" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">适应性套索</h1><p id="a1ea" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">那么……我们如何获得我们的 oracle estimator 呢？例如，我们可以使用<strong class="ky ir">自适应套索</strong>估计器。这个估计量最初是由邹(2006)提出的，其背后的思想非常简单:<strong class="ky ir">增加一些权重</strong> <em class="ni"> w </em>来校正 lasso 中的偏差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/8f2a4668b355447a70297c6ea8accdd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uCaKvnr0Z2PxBPDaqkZ-iQ.png"/></div></div></figure><p id="e5f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果一个变量很重要，它应该有一个小的权重。这样，它会受到轻微的惩罚，并保留在模型中。如果它不重要，通过使用一个大的权重，我们可以确保去掉它并把它发送到 0。</p><p id="2720" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但这就引出了我们今天要讨论的最后一个问题:</p><h1 id="fad4" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">如何计算这些重量</h1><p id="e256" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">计算这些权重的方法有很多，但今天我将采用最简单的一种:</p><ol class=""><li id="0128" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nq ly lz ma bi translated">求解一个简单的套索模型</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/6fc0efbf4fb3c6b3e4ba0e654653d05c.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*OvUHmSOnnAH2uOlPhOvLGg.png"/></div></figure><p id="7505" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.计算重量如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/8792e656124c6a137603d90fb659a17a.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*y8LZ8illsuwtypq_YHbQsg.png"/></div></figure><p id="35b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.插入权重并求解自适应套索</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/82969683df199471063be6b5989327f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*H4jRDytMKg8MbPFn1KuOGA.png"/></div></figure><p id="6d52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">仅此而已。现在，您的估计器是一个先知，您将获得比使用简单套索更好的预测(在预测误差和子集选择方面)。</p><p id="b6b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是不要相信我，<strong class="ky ir">让我们使用<code class="fe nu nv nw nx b">asgl</code>包在 Python </strong>中测试一下。</p><h1 id="e6e9" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">转向 Python 代码</h1><p id="5d9b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们从安装<code class="fe nu nv nw nx b">asgl</code>包开始，这个包在<code class="fe nu nv nw nx b">pip</code>和 GitHub 库中都有。</p><pre class="kg kh ki kj gt ny nx nz oa aw ob bi"><span id="fff1" class="oc mh iq nx b gy od oe l of og">pip install asgl</span></pre><h1 id="caac" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">导入库并生成数据</h1><p id="b5a5" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">首先，让我们导入将要使用的库。我们将在使用来自<code class="fe nu nv nw nx b">sklearn</code>的<code class="fe nu nv nw nx b">make_regression()</code>函数生成的合成数据集上测试使用自适应 lasso 估计器的好处。我们的数据集将有 100 个观察值和 200 个变量。但是在 200 个变量中，只有 10 个与响应相关，其余 190 个都是噪声。</p><p id="3732" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nu nv nw nx b">x</code>是形状为(100，200)的回归量矩阵，<code class="fe nu nv nw nx b">y</code>是响应变量(长度为 100 的向量)，而<code class="fe nu nv nw nx b">true_beta</code>是包含贝塔系数真实值的向量。这样，我们就能够将真实的 betass 与 lasso 和 adaptive lasso 提供的 beta 进行比较。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h1 id="7844" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">训练模型</h1><p id="8fa6" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们将比较一个简单的套索模型和一个自适应套索模型，看看自适应套索是否真的减少了预测误差，并提供了一个更好的有意义变量的选择。</p><p id="5f69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为此，我们考虑对数据集进行<strong class="ky ir">训练/验证/测试分割</strong>。我们使用训练集来训练不同参数值的模型。然后，我们使用验证集选择最佳模型，最后，我们使用测试集计算模型误差(这不包括在模型训练和选择中)。这可以在<code class="fe nu nv nw nx b">asgl</code>包中使用<code class="fe nu nv nw nx b">TVT</code>类和<code class="fe nu nv nw nx b">train_validate_test()</code>函数直接完成。</p><p id="df70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ni">拉索模型</em> </strong></p><p id="b7ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将用一个<code class="fe nu nv nw nx b">penalization=lasso</code>解一个线性模型<code class="fe nu nv nw nx b">model=lm</code>，并定义<code class="fe nu nv nw nx b">lambda1</code>的值，它是与套索惩罚相关的参数λ。我们将根据最小均方误差(MSE)找到最佳模型，并将使用 50 个观察值来训练模型，25 个用于验证，剩余的(25 个)用于测试。所有这些都由<code class="fe nu nv nw nx b">train_validate_test()</code>功能自动执行。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="b367" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最佳套索模型的预测误差(根据 MSE)存储在<code class="fe nu nv nw nx b">lasso_prediction_error</code>中，与模型相关的系数存储在<code class="fe nu nv nw nx b">lasso_betas</code>中</p><p id="937b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ni">自适应套索模型</em> </strong></p><p id="a855" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们解决自适应套索模型。为此，我们指定<code class="fe nu nv nw nx b">penalization=alasso</code>(代表自适应套索)，并选择用于计算权重的技术作为<code class="fe nu nv nw nx b">weight_technique=lasso</code>。如上所述，这样我们将解决一个初始套索模型，计算权重，然后将这个权重插入第二个套索模型，这将是我们的最终模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h1 id="e398" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">决赛成绩</h1><p id="7484" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">最后，我们来对比一下结果。我们将比较两个指标:</p><ul class=""><li id="a738" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">预测误差</strong>。每个模型实现的 MSE。越小越好。</li><li id="9e38" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">C <strong class="ky ir">正确选择率</strong>:被正确选择的变量的百分比(被模型认为无意义的无意义变量的数量和被认为有意义的有意义变量的数量)。此指标代表模型执行的变量选择的质量。越大越好，最大值为 1，最小值为 0。</li></ul><p id="a2ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的代码片段中，<code class="fe nu nv nw nx b">bool_something</code>变量用于计算正确的选择率。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="d98a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">自适应套索得到的结果比简单套索得到的结果好得多。我们看到<strong class="ky ir">自适应套索误差几乎比套索</strong> <strong class="ky ir">误差</strong>小 8 倍(套索误差为 1.4，套索误差为 11.8)。在变量选择方面，lasso 只正确选择了 200 个变量中的 13%，而<strong class="ky ir">自适应 lasso 正确选择了 100%的变量</strong>。这意味着自适应套索能够正确识别所有有意义的变量为有意义的，所有有噪声的变量为有噪声的。</p><p id="4acf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是这篇关于适应性套索的文章。请记住，尽量使用 oracle 评估工具，因为他们知道您的数据集的真实情况。我希望你喜欢这篇文章，并发现它很有用。如果您有任何问题/建议，请联系我。</p><p id="9669" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要更深入地了解<code class="fe nu nv nw nx b">asgl</code>包提供了什么，我推荐阅读 Github 存储库中提供的<a class="ae kv" href="https://github.com/alvaromc317/asgl/blob/master/user_guide.ipynb" rel="noopener ugc nofollow" target="_blank"> jupyter 笔记本</a>，要了解 oracle 估计器，我推荐最近发表的一篇论文，作为我博士论文的一部分:<a class="ae kv" href="https://doi.org/10.1007/s11634-020-00413-8" rel="noopener ugc nofollow" target="_blank">分位数回归中的自适应稀疏组套索。</a></p><p id="ab3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">祝你今天开心！玩的开心！</p><h1 id="c693" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">参考</h1><p id="e498" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated"><a class="ae kv" href="https://doi.org/10.1198/016214501753382273" rel="noopener ugc nofollow" target="_blank">范军，李锐(2001)非凹惩罚似然变量选择及其预言性质。美国统计协会 96(456):1348–1360</a></p><p id="2eb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">邹宏(2006)自适应套索及其甲骨文性质。美国统计协会 101(476):1418–1429</p><p id="e555" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://doi.org/10.1007/s11634-020-00413-8" rel="noopener ugc nofollow" target="_blank">门德斯-奇维埃塔，a .，阿吉莱拉-莫里洛，M. C .，利略，R. E. (2020)。分位数回归中的自适应稀疏群套索。数据分析和分类的进展。</a></p></div></div>    
</body>
</html>