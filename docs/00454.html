<html>
<head>
<title>Text Classification with Hugging Face Transformers in TensorFlow 2 (Without Tears)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 2(无泪)中拥抱人脸变形器的文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed?source=collection_archive---------0-----------------------#2020-01-14">https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed?source=collection_archive---------0-----------------------#2020-01-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/09bd480f79135d046f2557dcfdb68420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dWNeYrHTi76uhbq5.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://medium.com/tensorflow/using-tensorflow-2-for-state-of-the-art-natural-language-processing-102445cda54a" rel="noopener">来源</a></p></figure><p id="eebe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">拥抱脸变形器包</a>是一个非常流行的 Python 库，提供了对各种自然语言处理(NLP)任务非常有用的预训练模型。它以前只支持<strong class="ki iu"> PyTorch </strong>，但截至 2019 年末，<strong class="ki iu"> TensorFlow 2 </strong>也支持<strong class="ki iu"> </strong>。虽然该库可以用于许多任务，从自然语言推理(NLI)到问题回答，<a class="ae kf" href="https://en.wikipedia.org/wiki/Document_classification" rel="noopener ugc nofollow" target="_blank">文本分类</a>仍然是最流行和最实用的用例之一。</p><p id="0a38" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://github.com/amaiya/ktrain" rel="noopener ugc nofollow" target="_blank"> <em class="le"> ktrain </em> </a>库是 TensorFlow 2 中<code class="fe lf lg lh li b">tf.keras</code>的轻量级包装器。它旨在使深度学习和人工智能更容易被初学者和领域专家所应用。从<em class="le"> 0.8 </em>版本开始，<em class="le"> ktrain </em>现在包含了一个<strong class="ki iu">简化界面，可以对人脸变形金刚</strong>进行文本分类。在本文中，我们将向您展示如何使用<strong class="ki iu">拥抱面部变形器</strong>只用几行代码就可以构建、训练和部署一个文本分类模型。</p><p id="ada6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文的<strong class="ki iu">源代码有两种形式:</strong></p><ul class=""><li id="c848" class="lj lk it ki b kj kk kn ko kr ll kv lm kz ln ld lo lp lq lr bi translated"><a class="ae kf" href="https://colab.research.google.com/drive/1YxcceZxsNlvK35pRURgbwvkgejXwFxUt" rel="noopener ugc nofollow" target="_blank">这款谷歌 Colab 笔记本</a>或者</li><li id="1b92" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">一个<a class="ae kf" href="https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-A3-hugging_face_transformers.ipynb" rel="noopener ugc nofollow" target="_blank">教程笔记本</a>在<em class="le"> ktrain </em>的 GitHub 仓库里</li></ul><h1 id="c39e" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">入门指南</h1><p id="4f96" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">让我们从安装<em class="le"> ktrain </em>开始。在<a class="ae kf" href="https://www.tensorflow.org/install/pip?lang=python3" rel="noopener ugc nofollow" target="_blank">确保 TensorFlow 2 安装</a>在您的系统上之后，您可以安装<em class="le"> ktrain </em>与:</p><pre class="na nb nc nd gt ne li nf ng aw nh bi"><span id="acc3" class="ni ly it li b gy nj nk l nl nm">pip3 install ktrain</span></pre><p id="dcb5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将使用<a class="ae kf" href="http://qwone.com/~jason/20Newsgroups/" rel="noopener ugc nofollow" target="_blank">20 个新闻组数据集</a>构建一个包含四个新闻组类别的小型训练集。目标是建立一个模型，可以预测给定文章的新闻组类别。这将为我们提供一个在相对较小的训练集上观看<strong class="ki iu">变形金刚</strong>的机会，这是迁移学习的强大优势之一。顺便提一下，这是 scikit-learn <a class="ae kf" href="https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html" rel="noopener ugc nofollow" target="_blank">使用文本数据教程</a>中使用的相同数据集。让我们使用 scikit-learn 获取<a class="ae kf" href="http://qwone.com/~jason/20Newsgroups/" rel="noopener ugc nofollow" target="_blank">20 个新闻组数据集</a>，并将所有内容加载到数组中进行训练和验证:</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/327321405cdbcc9915b88ef273079335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*S39ibx7H6ccXbYB950tF7A.png"/></div></figure><p id="587d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们必须从拥抱脸中选择一个预训练的模型，这些模型都在这里的中列出。在撰写本文时，<strong class="ki iu">变压器</strong>库支持 TensorFlow 2 的以下预训练模型:</p><ul class=""><li id="02d6" class="lj lk it ki b kj kk kn ko kr ll kv lm kz ln ld lo lp lq lr bi translated"><strong class="ki iu">BERT</strong>:<em class="le">BERT-base-不区分大小写，BERT-large-不区分大小写，BERT-base-多语言-不区分大小写，以及其他。</em></li><li id="562d" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated"><strong class="ki iu">distil Bert</strong>:<em class="le">distil Bert-base-不区分大小写，distil Bert-base-多语言区分大小写，distil Bert-base-德语区分大小写，以及其他</em></li><li id="4083" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated"><strong class="ki iu">艾伯特:</strong> <em class="le">艾伯特-基-v2，艾伯特-大-v2，其他</em></li><li id="4abf" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">罗伯塔基地，罗伯塔大，罗伯塔大</li><li id="c53a" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated"><strong class="ki iu">XLM</strong>:<em class="le">xlm-MLM-xnli 15–1024，xlm-MLM-100–1280，其他</em></li><li id="2b59" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated"><strong class="ki iu">XLNet</strong>:<em class="le">XLNet-base-cased，xlnet-large-cased </em></li></ul><h1 id="62cc" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">处理问题</h1><p id="5ea5" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">实际上，由于拥抱脸变形金刚库中的 TensorFlow 支持相对较新，在撰写本文时，上面列出的几个模型<a class="ae kf" href="https://github.com/huggingface/transformers/issues/2251" rel="noopener ugc nofollow" target="_blank">会产生错误</a>。例子包括:</p><ul class=""><li id="d38f" class="lj lk it ki b kj kk kn ko kr ll kv lm kz ln ld lo lp lq lr bi translated"><code class="fe lf lg lh li b"><em class="le">distilbert-base-multilingual-cased</em></code> <em class="le"> : </em>参见<a class="ae kf" href="https://github.com/huggingface/transformers/issues/2423" rel="noopener ugc nofollow" target="_blank">第 2423 期</a></li><li id="d32c" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated"><code class="fe lf lg lh li b"><em class="le">xlnet-base-cased</em></code>:参见<a class="ae kf" href="https://github.com/huggingface/transformers/issues/1692" rel="noopener ugc nofollow" target="_blank">问题 1692 </a></li></ul><p id="b42f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，从<a class="ae kf" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu">变形金刚库</strong> </a>的 v2.3.0 开始，<code class="fe lf lg lh li b">distil-bert-uncased</code>模型工作得很好，但是<code class="fe lf lg lh li b">distilbert-base-multilingual-cased</code>模型在训练期间抛出异常。</p><p id="3a4e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">拥抱脸团队正在努力解决这样的问题。一些问题已经合并但未发布决议。例如，<strong class="ki iu"> transformers-v2.3.0 </strong>中与<em class="le"> XLNet </em>相关的问题可以通过简单地将<code class="fe lf lg lh li b">modeling_tf_xlnet.py</code>中的第 555 行从<strong class="ki iu"> transformers </strong>库中替换为:</p><pre class="na nb nc nd gt ne li nf ng aw nh bi"><span id="6262" class="ni ly it li b gy nj nk l nl nm">input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)</span></pre><p id="d8d4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如本 PR 中<a class="ae kf" href="https://github.com/huggingface/transformers/pull/1736" rel="noopener ugc nofollow" target="_blank">所述。如果遇到某个特定模型的问题，你可以尝试在变形金刚 GitHub 库</a>上搜索<a class="ae kf" href="https://github.com/huggingface/transformers/issues" rel="noopener ugc nofollow" target="_blank">问题，寻找一个以<strong class="ki iu">变形金刚</strong>库补丁形式的解决方案。</a></p><h1 id="d490" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">选择蒸馏模型</h1><p id="d4ab" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">与此同时，出于本教程的目的，我们将展示一个流行且极其有用的模型，该模型已经过验证，可以在<strong class="ki iu"> transformers </strong>库的<em class="le"> v2.3.0 </em>中工作(撰写本文时的当前版本)。BERT 模型通过在 11 个不同的自然语言处理任务中实现最先进的性能，代表了 2018-2019 年人工智能的重大突破之一。不幸的是，BERT 也是一个非常大且需要大量内存的模型，无论是训练还是推理都很慢。因此，BERT 不太适合生产环境。distill BERT 是 BERT 的“蒸馏”版本，更小更快，同时保留了 BERT 的大部分准确性。出于这些原因，我们将在本教程中使用一个未封装的英语模型:</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/48da463cde954f5f091d7ae2fbefeb9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*5FFkmJUueLteLcNgLMMQqw.png"/></div></figure><p id="a05f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们将使用<em class="le"> ktrain </em>轻松快速地构建、训练、检查和评估模型。</p><h1 id="df24" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">步骤 1:创建一个转换器实例</h1><p id="1d69" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated"><em class="le"> ktrain </em>中的<code class="fe lf lg lh li b">Transformer</code>类是围绕拥抱脸变形金刚库的简单抽象。让我们通过提供模型名称、序列长度(即<code class="fe lf lg lh li b">maxlen</code>参数)并用目标名称列表填充<code class="fe lf lg lh li b">classes</code>参数来实例化一个。</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/feb4c1a3bef19893a671dda78a9a7ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*dgCrg68kt91Ijs5tPYbGUA.png"/></div></figure><p id="4d57" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，基于 BERT 的模型的最大序列长度通常为 512。小于<code class="fe lf lg lh li b">maxlen</code>标记的文档将被填充，大于<code class="fe lf lg lh li b">maxlen</code>标记的文档将被截断。</p><h1 id="663e" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">步骤 2:预处理数据集</h1><p id="70a3" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">然后，我们将训练和验证数据集预处理成所选预训练模型(在本例中为 DistilBERT)预期的格式。</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/9568a40ab24f49f5443d248050baac12.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*uUhOv9QFn4V4sOhl-m5K-A.png"/></div></figure><h1 id="ef7b" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">步骤 3:创建一个模型并包含学员</h1><p id="ab7b" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">接下来，我们定义一个带有预训练权重的分类器，并随机初始化可以微调的最终层。该模型将被包装在一个<em class="le"> ktrain </em> <code class="fe lf lg lh li b">Learner</code>对象中，这将允许我们轻松地训练和检查该模型，并使用它对新数据进行预测。</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/fe1305d543173828e8dd6279391888e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*-Qrs55FKFfIY5peJC6_4tQ.png"/></div></figure><p id="1459" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您在训练期间遇到内存不足错误，您可以尝试降低上面的<code class="fe lf lg lh li b">batch_size</code>或降低步骤 1 中的<code class="fe lf lg lh li b">maxlen</code>参数。</p><h1 id="7088" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">步骤 4[可选]:估计学习率</h1><p id="365c" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">我们将使用<em class="le"> ktrain </em>中的学习率查找器来为我们的模型和数据集估计一个好的学习率。</p><p id="1af6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于基于 BERT 的模型，<em class="le"> 2e-5 </em>和<em class="le"> 5e-5 </em>之间的学习率通常在大范围的数据集上运行良好。因此，这一步是可选的。</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ns"><img src="../Images/e5a99dabd397f403615f8e4a810643df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b2arHYmmUeaElenmGeO1OA.png"/></div></div></figure><p id="3b20" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，我们将选择与亏损下降相关的最高学习率。让我们选择 5 <strong class="ki iu"> e-5 </strong>作为学习率<strong class="ki iu">。</strong>有趣的是，learning rate finder 的估计值(如图所示)与<a class="ae kf" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> Google 报告的通常对 BERT 和其他 transformer 模型最有效的</a>学习率范围一致。</p><h1 id="7882" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">第五步:训练模型</h1><p id="a0e9" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">对于培训，我们将调用<em class="le"> ktrain </em>中的<code class="fe lf lg lh li b">fit_onecycle</code>方法，该方法采用了由 Leslie Smith 提出的<a class="ae kf" href="https://arxiv.org/abs/1803.09820" rel="noopener ugc nofollow" target="_blank">1 周期策略</a>。一般来说，学习率计划具有增加学习率的初始预热期，然后是逐渐降低学习率的衰减期，这对于基于变压器的模型来说往往工作得很好。</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/6a19f54067bffdc780eb350b511f2dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0nnYPawQ5TZ-1l43uF2vYA.png"/></div></div></figure><p id="feb1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">经过四个时期后，我们的验证准确率为<strong class="ki iu"> 96.27%，</strong>，这比 SVM 在<a class="ae kf" href="https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn 关于文本分类的教程</a>中达到的 91%的准确率好了不少。</p><h1 id="6d5e" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">步骤 6:检查模型</h1><p id="4c40" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">让我们调用<code class="fe lf lg lh li b">view_top_losses</code>方法来检查我们的模型最容易出错的新闻组帖子。</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d69617b2309d6ef0916507afe3891ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*SweJetkmOlq0lh3pSvHYTA.png"/></div></figure><p id="b96c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如您所见，在验证集中与 ID 1355 相关联的 newgroup 帖子位于<code class="fe lf lg lh li b">comp.graphics</code>新闻组(即计算机图形学)，但主要是关于<strong class="ki iu">色盲</strong>，这是一个医学话题。因此，我们的模型对这篇文章的<code class="fe lf lg lh li b">sci.med</code>的预测是可以理解和原谅的。</p><h1 id="64b3" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">第七步:对新数据进行预测</h1><p id="8e63" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">我们可以实例化一个<code class="fe lf lg lh li b">Predictor</code>对象来轻松预测新的例子。</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/0c4cb3c41f2fce94f3b3f14ce1958dde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*IufidrXnBbDF2pBVd0L26w.png"/></div></figure><p id="5cc3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe lf lg lh li b">predictor</code>也可用于通过<a class="ae kf" href="https://eli5.readthedocs.io/en/latest/overview.html" rel="noopener ugc nofollow" target="_blank"> eli5 </a>和<a class="ae kf" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank"> lime </a>库解释特定示例的分类来进一步检查模型；</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/9a9d166e6e892ba457533a0e57965484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*VY7fbNXq9dio4hua5BKEog.png"/></div></figure><p id="8a59" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用绿色突出显示的单词似乎导致我们的模型将这个例子放在<code class="fe lf lg lh li b">soc.religion.christian</code>类别中。正如你所看到的，突出显示的单词都符合这一类别的直觉。</p><p id="653e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，为了让<code class="fe lf lg lh li b">explain</code>工作，您需要首先安装一个分叉版本的<a class="ae kf" href="https://eli5.readthedocs.io/en/latest/overview.html" rel="noopener ugc nofollow" target="_blank"> eli5 </a>库，该库被修改为支持 TensorFlow Keras:</p><pre class="na nb nc nd gt ne li nf ng aw nh bi"><span id="69bc" class="ni ly it li b gy nj nk l nl nm">pip3 install git+https://github.com/amaiya/eli5@tfkeras_0_10_1</span></pre><p id="4650" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，<code class="fe lf lg lh li b">predictor</code>对象可以保存到磁盘上，并在以后的实际部署场景中重新加载:</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/6a6be898aeb9c877de2f90e65495e9a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lnyahZdrxRid8iySyqiKrA.png"/></div></div></figure><p id="abe0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">默认情况下，<code class="fe lf lg lh li b">predict</code>返回预测的类标签，但是<code class="fe lf lg lh li b">predict_proba</code>将返回每个类的预测概率，如上所示。</p><h1 id="7209" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">结论</h1><p id="37d5" class="pw-post-body-paragraph kg kh it ki b kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld im bi translated">ktrain 可以轻松试验不同的<strong class="ki iu">变压器</strong>型号。例如，传统的 BERT 模型可以很容易地进行训练，以便与上面训练的 DistilBERT 模型进行比较，只需替换如下的<code class="fe lf lg lh li b">MODEL_NAME</code>:</p><pre class="na nb nc nd gt ne li nf ng aw nh bi"><span id="0411" class="ni ly it li b gy nj nk l nl nm">import ktrain<br/>from ktrain import text<br/><strong class="li iu">MODEL_NAME = 'bert-base-uncased'</strong><br/>t = text.Transformer(MODEL_NAME, maxlen=500,  <br/>                     classes=train_b.target_names)<br/>trn = t.preprocess_train(x_train, y_train)<br/>val = t.preprocess_test(x_test, y_test)<br/>model = t.get_classifier()<br/>learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)<br/>learner.fit_onecycle(3e-5, 1)</span></pre><p id="11da" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不管选择什么样的<code class="fe lf lg lh li b">MODEL_NAME</code>，包装在<code class="fe lf lg lh li b">Learner</code>对象中的底层模型只是另一个<strong class="ki iu"> TensorFlow Keras </strong>模型:</p><figure class="na nb nc nd gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/f45748f0a0bb572931c0653007768c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*pmph30ZLebuD9vjFoFGZdw.png"/></div></figure><p id="9e66" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，一旦经过训练，如果愿意，可以直接使用<strong class="ki iu"> TensorFlow </strong>和/或<strong class="ki iu"> transformers </strong>库本身来管理它。在你的下一个文本分类项目中，请随意尝试一下 ktrain。</p><p id="1797" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更多关于<em class="le"> ktrain </em>的信息，请访问我们的<a class="ae kf" href="https://github.com/amaiya/ktrain" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>。</p></div></div>    
</body>
</html>