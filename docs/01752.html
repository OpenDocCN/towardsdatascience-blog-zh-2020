<html>
<head>
<title>Pendragon Four: Training Pipeline Deeper Dive for Multi Agent Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">潘德雷肯四:多智能体强化学习的训练管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pendragon-four-training-pipeline-deeper-dive-for-multi-agent-reinforcement-learning-80df2434dd0?source=collection_archive---------39-----------------------#2020-02-17">https://towardsdatascience.com/pendragon-four-training-pipeline-deeper-dive-for-multi-agent-reinforcement-learning-80df2434dd0?source=collection_archive---------39-----------------------#2020-02-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="0d27" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在过去的一年里，我制作了各种版本的神经网络机器人来玩游戏《命运大令》(FGO)，大致称为“潘德雷肯计划”。围绕潘德雷肯项目的工作范围从用一系列神经网络提取特征以获得关于当前游戏状态的信息，到我最近增加的三个神经网络来控制游戏屏幕上活跃的三个角色。这三个和另外一个用于挑选动作卡的机器人是四个强化学习(RL)代理，它们组成了我当前版本的项目，潘德雷肯四。</p><p id="d909" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我最近的一篇介绍<a class="ae ko" rel="noopener" target="_blank" href="/pendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd">潘德雷肯四人组</a>的帖子涵盖了我为了能够训练我的新特工而必须进行的总体添加和升级，以及一些结果。然而，我想回顾一下我发现有用的培训的一些方面，以及在继续改进我最初的潘德雷肯四管道后学到的一些经验教训。</p><ol class=""><li id="51cf" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">从DQN转向基于政策梯度的培训</li><li id="cc8a" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">处理无效移动</li><li id="0b45" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">游戏平衡和适当地挑战代理</li></ol><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/934795d71fe5093b19c40efe5758f912.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*E2xaEW8ZdploeHcnpqRs6A.gif"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">纯策略梯度机器人，在多次训练中，一个相当一致的事情是机器人会使用Ishtar(中间角色)的第二技能来立即清除第一波。对于机器人来说，我认为这是一个更稳定的策略，至少让他们有机会进入后期游戏，而不是在他们可能活着也可能不活着的时候保存技能。</p></figure><h1 id="75f2" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">让代理学习他们自己的游戏方式</h1><p id="72f5" class="pw-post-body-paragraph jq jr it js b jt mr jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn im bi translated">我对RL的第一次真正尝试是为我最初的<a class="ae ko" rel="noopener" target="_blank" href="/project-pendragon-part-2-a-reinforcement-learning-bot-for-fate-grand-order-7bc75c87c4f3">潘德雷肯Alter </a>机器人建立我的定制环境，这是一个基于DQN的代理，其目标是在任何给定的回合挑选命令卡。基于DQN的培训方法的基本流程如下:</p><ol class=""><li id="2d56" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">代理(在这种情况下是网络)被告知游戏的当前状态。这可能是一款<a class="ae ko" href="http://karpathy.github.io/2016/05/31/rl/" rel="noopener ugc nofollow" target="_blank">雅达利乒乓游戏</a>的像素，也可能是你选择的任何代表。我的FGO·潘德雷肯Alter bot是5张牌中的一张。</li><li id="041f" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">代理从动作空间中选择一个动作。在Pong的例子中，Andrej Karpathy把它作为上升的概率。在我的FGO游戏中，是60种可能的卡槽组合中哪一种最好(有60种方法从5张卡中选择3张)。然而，应该注意的是，在强化学习中，存在探索与利用的概念。本质上是说，有时应该随机选择一个动作，而不是简单地做代理认为最好的事情。这有助于代理探索和发现额外的奖励，否则如果它只是利用它知道的奖励，它将不会发现。</li><li id="fcc1" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">行动被归入环境，任何奖励被收集，环境进入下一个状态、框架或回合。机械地，我这样做是通过将奖励添加到网络输出的卡槽组合中。对于正的奖励，该类别在输出数组中的值增加，并且网络将再次看到给定该输入，该特定类别是有益的。</li><li id="ab01" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">代理根据其收到的奖励进行更新。在奖励被用于修改输出阵列之后，网络以修改后的输出阵列为目标在初始输入状态上被训练。这有助于巩固好的选择，同时也考虑到坏的选择。</li><li id="666b" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">冲洗并重复。</li></ol><p id="4365" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个过程是有效的，也是合理的，但我发现在我的定制环境中，这意味着我不得不限制代理根据我的观点来玩，因为没有分数或内置的奖励，我可以用它来分配个人行为的价值。所以发生的事情是，我必须奖励机器人我认为好的东西。例如，有了卡牌机器人，它将组合3张同类型的卡牌组合，因为有与此相关联的奖金，或者与潘德雷肯四中的角色机器人相关联，它将在合理的时间使用技能。虽然这些奖励可以有效地教会机器人有用的行为…但它并没有真正让它们探索新的游戏风格，它把我的信念印在我的代理身上，而不是让它们自己学习。</p><p id="da11" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，在我为潘德雷肯四中进行的下一轮培训中，我的目标之一是成功升级到更多的政策梯度培训方法。而基于DQN的训练方法在单个回合分配奖励，政策梯度类型方法的最纯粹形式是基于游戏的输赢分配奖励，例如+1或-1。</p><p id="c52f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从概念上讲，这对我来说一直是一个相当混乱的概念，因为一个代理人可能在赢和输的时候采取相同的行动，但下一次奖励是积极的，然后是消极的。这是真的，结果证明是好的，因为这个想法是，如果在这种情况下，平均来说，这个移动是一个好的移动，你应该赢的次数多于你输的次数，这应该给这个移动一个净正值。</p><blockquote class="mw mx my"><p id="316a" class="jq jr mz js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated">3场比赛中一个代理人的策略梯度示例:</p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nd"><img src="../Images/8750981a17bb980273bf694ac9949274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*13aoEJgD3J9dzEZsnBemQg.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">3个示例游戏的图表每个圆圈是一个游戏状态，箭头是代理人在该回合选择的动作。底部是三个示例呈现的游戏状态。</p></figure><p id="14a6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在上面的例子中有三场比赛，2胜1负。基本上，我们所做的是对赢的游戏中发生的所有行为给予积极的奖励，对输的游戏中的行为给予消极的奖励。因此，一些结果是，在第5回合，两个获胜回合都使用了技能3 (sk3 ),因此这一步棋在这个子回合中可能会得到相当积极的回报。而在第一回合使用技能2是一个只会在失败的游戏中发生的动作，所以它是不利的。一个更有趣的可能是在1胜1负的第4回合使用技能1。在这个子集之后，它将有1个不鼓励的例子和1个鼓励的例子。还需要更多的游戏来证明这是否是一个好的举措。</p><p id="9437" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对我来说，这种方法是高度可行的，并产生良好的结果，这有点不可思议。从大数定律的角度来看，这开始变得更有意义了。如果我们平均进行成千上万场游戏，更好的棋步会比更差的棋步出现在更多的胜局中，所以我们最终会让代理人学习如何做出获胜的棋步。</p><p id="732f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我之前的潘德雷肯四管道中，我有一个政策梯度管道，我在游戏结束时分配奖励，但我也对我认为好的行为给予中间奖励。所以我仍然把我的一些观点印在代理人身上。因此，本着尽可能将自己从管道中移除的精神，我希望转向一种纯粹的政策梯度方法，在这种方法中，我只根据输赢来分配奖励。为了达到这一点，我发现比我以前做的更多的探索有助于让机器人发现更多的游戏方式。</p><p id="a649" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我转向使用策略梯度，在游戏完成后应用奖励时，我的另一个想法是将游戏聚合在一起，以创建更大的“数据集”来对代理进行大规模更新，而不是像以前那样一轮一轮地进行更新。我认为这有利于训练，也有利于利用我更多的计算资源。</p><p id="25c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我的旧管道中，当我在每回合的基础上接受培训时，我总是发现令人讨厌的是，我真的没有从GPU上的培训中受益，因为我认为该设置中的大多数计算都是从GPU发送和接收的。所以现在我基本上把几百个游戏收集在一起作为一个“数据集”,然后利用我的GPU进行大批量训练。</p><h1 id="dc44" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">处理无效移动</h1><p id="4b04" class="pw-post-body-paragraph jq jr it js b jt mr jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn im bi translated">在我以前做过的强化学习项目中，我为我的代理保留了所有有效动作的动作空间。主要是因为我不知道如何优雅地处理无效的举动。然而，对于最近的潘德雷肯四的迭代，我必须弄清楚如何为我的代理人实现这一点，以处理技能在使用后冷却，不再可供机器人玩。</p><p id="91fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这一点上，我只允许我的代理人每回合选择一个动作，当前的角色集每个都有4个可能的移动。</p><pre class="le lf lg lh gt ne nf ng nh aw ni bi"><span id="979b" class="nj lu it nf b gy nk nl l nm nn">pass<br/>skill_1<br/>skill_2<br/>skill_3</span></pre><p id="5f7b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这一套4招是因为其中一个机器人拥有可以对其他角色施展的技能，他们的每一招都是针对自己或整个团队的。选择具有这些移动类型的角色是我在原型化我的机器人管道时为了简单而做出的选择。</p><p id="3486" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在未来，我想增加更多的角色，让他们的动作更加多样化，并允许机器人每回合玩不止一个动作。</p><p id="4c24" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在处理无效移动的第一次迭代中，我遵循了堆栈溢出的建议:</p><blockquote class="no"><p id="531b" class="np nq it bd nr ns nt nu nv nw nx kn dk translated">“忽略无效动作就好了。”-堆栈溢出</p></blockquote><p id="0edd" class="pw-post-body-paragraph jq jr it js b jt ny jv jw jx nz jz ka kb oa kd ke kf ob kh ki kj oc kl km kn im bi translated">这或多或少仍然是我处理无效移动的方式，但它变得更加微妙了。当我训练我的第一轮潘德雷肯四机器人时，我将网络的最后一层设置为线性激活，当技能冷却时，我会将其设置为低值，如阵列中的最低值。我发现，有时这会导致一种竞相逐底的效应，输出会变得非常负。</p><p id="d3d3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了避免这些大的负输出，我决定在网络的最后一层使用类似sigmoid的东西，或者更理想的是softmax激活，这至少会将每个节点的可能值限制为等于0和1之间的某个值。这两次激活意味着我可以把技能的冷却时间设置为0，而不是一些越来越大的负数。在sigmoid和softmax之间，softmax激活是我真正想使用的，但不确定如何应用奖励和零化无效移动，同时仍然创建有效的概率分布。所以我用sigmoids做了一些测试。</p><p id="ba9d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用sigmoid激活，很容易将它们的值保持在0和1之间。如果奖励使价值高于或低于这些值，那么你可以把它设置回1或0。因为sigmoid将每个节点视为独立的，所以您不必担心重建有效的概率分布。对于我的问题，虽然这种方式建模感觉不真诚，因为动作并不是真正相互独立的，因为我只允许一个机器人每回合采取一个动作(又名softmax)。</p><p id="676b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，虽然这个系统机械地工作，但对这个问题来说感觉是错误的。这让我回到了如何在这里恰当地使用softmax激活的绘图板。</p><p id="af19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用softmax，网络的输出是某种有效的概率分布，我们将具有最高值的动作作为该回合的动作。对一个行为的奖励也意味着其他行为会被抑制，反之亦然。所以对我来说，这在心理上是最有意义的，但我必须弄清楚它的机制，因为修改输出数组的值会破坏这种良好的概率分布。</p><p id="2544" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然这花费了我更多的时间，但我最终发现我可以将预测数组的无效索引清零，从选择的移动中增加或减去奖励，然后通过将数组相加并将每个索引除以数组的总和来重新计算概率分布…这是获胜的基本数学！见下文:</p><pre class="le lf lg lh gt ne nf ng nh aw ni bi"><span id="7ec2" class="nj lu it nf b gy nk nl l nm nn">Game State: Skill 3 has been used and reward of +1</span><span id="3110" class="nj lu it nf b gy od nl l nm nn">Original: [.2, .5, .3] # sample probabilities for network</span><span id="d4e1" class="nj lu it nf b gy od nl l nm nn">Modified: [.2, 1.5, 0] # added reward to skill 2 and zeroed skill 3</span><span id="cdac" class="nj lu it nf b gy od nl l nm nn">Final: [.117, .882, 0] # .2/1.7 and 1.5/1.7</span></pre><p id="5daa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然这对于环境来说并不是一个巨大的变化，但这有助于使代理人的奖励结构和目标在我的大脑中更加具体，并帮助我解决这个问题的其他部分。总的来说，对我来说这是一件好事，因为它有助于处理其他有趣的RL问题，其中可能会有大量的无效移动</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/4c9fb138b5c0a62e7d7f59f05f0a42f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ABoap4-uYhN9-w74lfCIfA.gif"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">机器人在最后一轮中放弃了他们剩余的技能。有趣的是，左边的第一个机器人之前使用了一个技能来增加它的致命伤害，这一次使用了一个技能来大量增加它的伤害，而机器人3使用了一个技能来增加致命一击的可能性。然后，第一个机器人受到很大的致命一击，结束这一轮。</p></figure><h1 id="aa0f" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">游戏平衡和培训各种代理</h1><p id="a1e9" class="pw-post-body-paragraph jq jr it js b jt mr jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn im bi translated">在我开始训练潘德雷肯四人组之前，我从来没有认真考虑过如何平衡游戏环境。我发现，为了让机器人学习有用的行为，我必须非常仔细地考虑我把它们放在什么环境中。</p><p id="90e9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我有几个版本的定制FGO环境。我一年前做的第一个可以粗略地认为是RL代理人和敌人之间的单轮战斗，他们只是轮流打击对方，直到一个人死亡…我最近的一个有3波敌人，代理人必须在代理人的生命值下降到0之前击败所有三波。第一个环境大约有120点生命值，每回合造成1到3点伤害。这种设置大致反映了标准的FGO农业水平。代理人大约有30 HP。</p><p id="9272" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我的潘德雷肯四环境升级后，我开始在一个总HP和伤害相似的环境中训练所有4个代理，我发现代理有时只是不玩技能，或者其他时候立即玩他们所有的技能而不用担心任何事情。</p><p id="9dd6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这两者都不是很有趣，因为它们并没有真正解决任何问题。但是他们胜率并不可怕。在我最初的帖子中，我说过</p><blockquote class="mw mx my"><p id="41b8" class="jq jr mz js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated">4个机器人的随机猜测胜率约为38%，可能在20-30K游戏后达到72%左右的峰值。在做了一些额外的工作和大量的实验后，我的成功率达到了84%左右。</p></blockquote><p id="24ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这72%的胜率实际上是3个角色的代理人做出了不感兴趣的选择，而拾牌机器人学习并基本上艰难地携带3个角色的代理人通过环境。让我在当时达到84%,并在当前版本中接近88 %- 90%胜率的部分，实际上是我的训练协议的改变。</p><p id="cc3c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我发现的是，我训练选卡员代理(不同的角色有不同的卡片组，因此选卡机器人必须学习玩的卡片组根据角色而不同)，然后一旦选卡员代理被训练，我就对各个代理进行一轮训练。这种两步走的方法帮助我进行了最初的开发，并隔离了学习的内容。我认为有助于隔离的另一件事是，如果选卡代理在角色代理学习的同时也在学习，那么角色代理在他们的策略梯度训练中得到的反馈就不那么一致，因为团队的很多损失都是由被选卡造成的。因此，将管道分成几部分也有助于确保角色代理对他们采取的行动获得一致的反馈。</p><p id="9204" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">两阶段培训流程:</p><ol class=""><li id="30eb" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">拾牌代理将在一个版本的游戏环境中接受训练。这主要是为了让它学会如何玩一副给定的牌，它是在DQN管道中训练的。</li><li id="351a" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">然后，我把经过训练的拣卡员和三个随机初始化的角色代理放入一个更难的游戏环境中。</li></ol><p id="e228" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我提到最初的游戏环境大约有120HP，分布在3波中，每回合造成1-3点伤害。我调整了环境，增加了血量和伤害。我的想法是，由于机器人能够在没有真正尝试原始游戏设置的情况下通过，如果我让游戏变得更难，机器人将无法顺利通过，需要真正开始学习有用的信息才能获胜。查看更恶劣环境的示例:</p><pre class="le lf lg lh gt ne nf ng nh aw ni bi"><span id="8483" class="nj lu it nf b gy nk nl l nm nn">Round1: 40 HP 1-3 damage<br/>Round2: 60 HP, 2-5 damage<br/>Round3: 80 HP, 4-6 damage</span></pre><p id="9852" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，与之前环境中的120马力不同，机器人需要额外的60马力来对抗，如果它们长时间被困在后面的波中，它们可能会死亡。</p><p id="d6c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这种更艰难的环境下，机器人的表现看起来并不出色。他们在训练有素的拣卡员和随机初始化的角色机器人的帮助下，开始时胜率约为35%(4个随机机器人的胜率约为3%)，在大约100K场游戏后，胜率约为54%。这个结果并不是我们必须关心的，因为我可以将这些机器人放回到更接近FGO农业水平的原始游戏环境中，它们有88-90%的几率获胜。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/03c6fd85f42bbee41b1a42dc8cec4355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Y8h9KxtZBXuZSrvbzVadvw.gif"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">机器人保存了第三波的伤害buffs，并删除它们来帮助清除更难的最终波内容。</p></figure><h1 id="1a57" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">结束语</h1><p id="f81a" class="pw-post-body-paragraph jq jr it js b jt mr jv jw jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn im bi translated">我真的没有很多机会同时训练多个神经网络，它们都需要学习和合作才能成功。大多数情况下，如果涉及多个网络，它会以合奏的形式出现，在那里一切都可以如此，所以我在这里必须做的大量基础工作是消除我的心理模型，即我希望所有这些如何运行，并将其隔离开来，以便在给定的时间点具有最少的移动部分。这不是最独特的经验教训，但却是不时重新学习的好经验。</p><p id="9d34" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就我目前的培训渠道而言，我看到的是可靠的网络学习策略，但可能不是最有趣的。例如，许多训练在第一波或第二波开始时使用伊师塔的第二技能，因为它让团队立即清除那一波，并使团队受到的早期伤害最小化。</p><blockquote class="mw mx my"><p id="b6a2" class="jq jr mz js b jt ju jv jw jx jy jz ka na kc kd ke nb kg kh ki nc kk kl km kn im bi translated">和上面一样的gif</p></blockquote><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/934795d71fe5093b19c40efe5758f912.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*E2xaEW8ZdploeHcnpqRs6A.gif"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">纯策略梯度机器人，在多次训练中，一个相当一致的事情是机器人会使用Ishtar(中间角色)的第二技能来立即清除第一波。对于机器人来说，我认为这是一个更稳定的策略，至少让他们有机会进入后期游戏，而不是在他们可能活着也可能不活着的时候保存技能。</p></figure><p id="38e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我玩的时候，我倾向于把这些技能留到第三轮。然而，我认为这是因为作为一名玩家，我非常清楚我不太可能在我所玩的任务的第三轮之前死去。机器人真的没有这种不死的精神保证。所以对他们来说，如果他们不尽早利用它，他们可能永远没有机会使用他们的技能。这也可能是缺乏基于我的训练协议、环境、建模等的内置于机器人的长期规划。因此，对于机器人团队来说，在数千场比赛中，使用Ishtar的第二技能来清除早期波浪而不是保存它似乎是机器人玩的一种可靠方式，因为它至少给了他们到达终点的机会，而不是冒着死在路上的风险。</p><p id="f34e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以现在我已经为这些机器人解决了更多的训练过程，我在一个更好的位置开始为不同的角色训练一些额外的代理，并致力于融入更多有趣的团队组成。</p></div></div>    
</body>
</html>