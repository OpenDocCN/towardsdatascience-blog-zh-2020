<html>
<head>
<title>How to use reinforcement learning to play tic-tac-toe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何利用强化学习玩井字游戏</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-play-tic-tac-toe-using-reinforcement-learning-9604130e56f6?source=collection_archive---------23-----------------------#2020-05-15">https://towardsdatascience.com/how-to-play-tic-tac-toe-using-reinforcement-learning-9604130e56f6?source=collection_archive---------23-----------------------#2020-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dec7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于 Q-learning 的简单教程</h2></div><p id="752f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> Q-learning </strong>是一种出色而基本的强化学习方法，由于深度学习革命，它最近取得了很多成功。虽然本教程不会解释什么是深度 Q 学习，但我们将通过原始的 Q 学习算法来教一个代理如何玩一个<strong class="kk iu">井字游戏</strong>。尽管它很简单，但我们会看到它能给出极好的结果。</p><p id="4b37" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们将通过一些必要的背景知识来快速了解强化学习，然后我们将介绍 Q-learning 算法，最后是如何实现它来教代理玩井字游戏。</p><p id="f382" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了理解本教程，不需要有任何关于强化学习的知识，但它将有助于对微积分和线性代数有一个基本的了解。最后，我分享了我自己的 Github 知识库，里面有我解释的东西，你可以免费查看，如果你感兴趣，点击<a class="ae le" href="https://github.com/RickardKarl/bill-the-bot" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h2 id="a760" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">强化学习概述</h2><p id="bb9e" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">强化学习的目标是当智能体在不同状态的环境中运行时，根据某种奖励函数优化智能体的行为。对于本教程，环境是井字游戏，它有明确定义的动作，代理必须决定选择什么动作来赢得游戏。此外，代理人将因赢得游戏而获得奖励，这鼓励它在游戏中学习好的策略。</p><p id="e19c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">强化学习的一个常见框架是(有限)<strong class="kk iu">马尔可夫决策过程</strong> (MDP)。它帮助我们定义一组动作和状态，代理的决策是基于这些动作和状态的。</p><p id="e832" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个 MDP 包括</p><ul class=""><li id="e327" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">一组有限的动作<strong class="kk iu"><em class="mt"/></strong>(在游戏板上放置标记的位置)</li><li id="73b1" class="mk ml it kk b kl mu ko mv kr mw kv mx kz my ld mp mq mr ms bi translated">状态的有限集合<strong class="kk iu"><em class="mt"/></strong>(游戏板的给定配置)</li><li id="6372" class="mk ml it kk b kl mu ko mv kr mw kv mx kz my ld mp mq mr ms bi translated">一个奖励函数<strong class="kk iu"> <em class="mt"> R(s，a) </em> </strong>，该函数基于在状态<strong class="kk iu"> <em class="mt"> s </em> </strong> <em class="mt">中执行动作<strong class="kk iu"> <em class="mt"> a </em> </strong>来返回值。</em></li><li id="f8a2" class="mk ml it kk b kl mu ko mv kr mw kv mx kz my ld mp mq mr ms bi translated">一个转移函数<strong class="kk iu"> <em class="mt"> T(s，a，s’)</em></strong></li></ul><p id="2706" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当执行动作<strong class="kk iu"><em class="mt"/></strong>时，转换函数给出从状态<strong class="kk iu"> <em class="mt"> s </em> </strong>移动到<strong class="kk iu"><em class="mt">s’</em></strong><em class="mt"/>的概率。当我们处在一个不确定某项行动是否总能产生预期结果的环境中时，这是必要的。然而，在井字游戏中，我们确切地知道每个动作会做什么，所以我们不会使用这个。</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/3411b3b82be6919625b8e504fa990a01.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*BNVsZb7PlE-AaFlplaRL5Q.png"/></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">状态空间由所有可能的游戏配置组成。在每个状态下，我们可以执行一组可用的操作。在这个例子中，当前玩家有六个可能的动作要执行。(图由作者提供)</p></figure><p id="e6f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">MDP 框架帮助我们形式化了这个问题，根据当前的状态，确定哪些行为将使代理人在游戏中的总报酬最大化。奖励函数<strong class="kk iu"> <em class="mt"> R(s，a) </em> </strong>会非常简单:</p><ul class=""><li id="3b7a" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">如果代理人从<strong class="kk iu"><em class="mt"/></strong>执行一个赢得游戏的动作<strong class="kk iu"><em class="mt"/>—</strong>，那么<strong class="kk iu"> <em class="mt"> R(s，β)= 1</em></strong>。</li><li id="2acf" class="mk ml it kk b kl mu ko mv kr mw kv mx kz my ld mp mq mr ms bi translated">否则，如果代理犯了一个错误，选择了错误的动作<strong class="kk iu"><em class="mt"/></strong>从而输掉了游戏，那么<strong class="kk iu"> <em class="mt"> R(s，γ)=-1</em></strong>。</li><li id="8b2c" class="mk ml it kk b kl mu ko mv kr mw kv mx kz my ld mp mq mr ms bi translated">否则，当上述情况都没有发生时，奖励简单来说就是<strong class="kk iu"> <em class="mt"> R(s，a) = 0。</em>T41】</strong></li></ul><p id="32f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在强化学习中，我们找到了一个最优策略，代理使用它来决定采取哪些行动。由于我们使用 Q-learning，我们将简单地把我们的策略表示为当代理处于状态<strong class="kk iu"> <em class="mt"> s </em> </strong>时最大化函数<strong class="kk iu"><em class="mt">【s，a】</em></strong>的动作<strong class="kk iu"><em class="mt"/></strong>。这是 Q-learning 的核心，所以让我们来看看如何计算这个函数。</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/462f51eb7f07b689540441b2a92bd252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*wAldV0g_3SDxufK7mjakCQ.png"/></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">对于游戏中的每个状态，每个动作都与一个<strong class="bd nm"> Q </strong>值相关联。最佳策略是选择价值最高的行动。在这个具有状态<strong class="bd nm"> s </strong>的例子中，下一轮属于玩家 X，并且很明显哪个动作具有最高的预期奖励，因为 X 即将获胜。</p></figure><h2 id="55ca" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">用贝尔曼方程更新</h2><p id="4ef7" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated"><strong class="kk iu"> <em class="mt"> Q(s，a) </em> </strong>的解释是:如果代理人在状态<strong class="kk iu"> <em class="mt"> s </em> </strong>中拾取动作<strong class="kk iu"> <em class="mt"> a </em> </strong>则<strong class="kk iu"> <em class="mt"> Q </em> </strong>是游戏结束时给出的期望奖励。由于代理人想使其报酬最大化，它想选择使<strong class="kk iu"> <em class="mt"> Q </em> </strong>最大化的行动。</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/e0a7d0cafe3488b66dc4aee3390bf598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KBrRlD4jlyeOQMI-usMfIw.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">在游戏的这种状态下，我们显示当前玩家 X 的<strong class="bd nm"> <em class="ns"> Q </em> </strong>值的场景示例。通过将下一个标记放置在最大值所在的位置，玩家将赢得游戏并获得奖励。(图由作者提供)</p></figure><p id="28db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了计算<strong class="kk iu"> <em class="mt"> Q(s，a) </em> </strong>，代理必须探索所有可能的状态和动作对，同时从奖励函数<strong class="kk iu"><em class="mt">【R(s，a) </em> </strong>获得反馈。在井字游戏的情况下，我们通过让代理与对手玩许多游戏来迭代更新<strong class="kk iu"> <em class="mt"> Q(s，a) </em> </strong>。利用 Q 学习算法，用于更新<strong class="kk iu"> <em class="mt"> Q </em> </strong>的等式如下:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nt"><img src="../Images/84811f9f93732e6e0872f9cfd8f83435.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VgWWQik4O_1oiKnDhf6tyQ.png"/></div></div></figure><ul class=""><li id="65f0" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">我们执行一个动作<strong class="kk iu"> <em class="mt">一个</em> </strong>当前状态<strong class="kk iu"> <em class="mt"> s. </em> </strong></li><li id="b90a" class="mk ml it kk b kl mu ko mv kr mw kv mx kz my ld mp mq mr ms bi translated">最后一项展望未来并返回可用的最大<strong class="kk iu"> <em class="mt"> Q </em> </strong>值，<strong class="kk iu"><em class="mt">【ŝ】</em></strong>是执行动作<strong class="kk iu"><em class="mt"/></strong>后的新状态。然后，动作<strong class="kk iu"><em class="mt">—</em></strong>是下一个状态<strong class="kk iu"> <em class="mt"> ŝ </em> </strong>的最佳动作。</li><li id="5897" class="mk ml it kk b kl mu ko mv kr mw kv mx kz my ld mp mq mr ms bi translated">学习率<strong class="kk iu"> α </strong>决定我们在多大程度上覆盖旧值，我们将使用<strong class="kk iu"> α = 0.1 </strong>。</li><li id="0f17" class="mk ml it kk b kl mu ko mv kr mw kv mx kz my ld mp mq mr ms bi translated">折扣因子<strong class="kk iu"> γ </strong>决定了与当前时间步长<strong class="kk iu"> t </strong>的奖励相比，未来奖励的权重。我们将使用<strong class="kk iu"> γ = 0.9 </strong>。</li></ul><p id="1958" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个等式基于强化学习中众所周知的等式，称为贝尔曼等式<em class="mt">。让我们看看如何使用这个等式来教导我们的代理。</em></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h2 id="c743" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">Q 学习算法的实现</h2><p id="8c69" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">要得到一个训练有素的代理，我们需要学习<strong class="kk iu"> <em class="mt"> Q(s，a) </em> </strong>的值。这将通过让两个代理相互竞争来实现。我们将引入一个概率<strong class="kk iu"> ε </strong>每个智能体挑选一个随机动作，否则它将根据<strong class="kk iu"> <em class="mt"> Q(s，a) </em> </strong> <em class="mt">挑选最佳动作。</em>用<em class="mt"> </em>这种方式<em class="mt">，</em>我们<em class="mt"> </em>确保<em class="mt"> </em>到<em class="mt"> </em>平衡学习，使得代理有时探索新的动作，而其他时候利用代理已经学习的信息。</p><p id="a808" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个训练阶段可以用 Q 学习算法的下列伪代码来描述:</p><pre class="na nb nc nd gt nu nv nw nx aw ny bi"><span id="25cf" class="lm ln it nv b gy nz oa l ob oc">Initialise: Q(s,a) = 0, starting state s, <br/>            starting player P, iterations N</span><span id="f5aa" class="lm ln it nv b gy od oa l ob oc">for t = 0 : N</span><span id="aef0" class="lm ln it nv b gy od oa l ob oc">   With probability <strong class="nv iu">ε </strong>:<strong class="nv iu"> </strong>P picks random action a<br/>   Else, pick action a that maximise Q(s,a)</span><span id="e22e" class="lm ln it nv b gy od oa l ob oc">   Observe new state ŝ and reward R(s,a)</span><span id="5863" class="lm ln it nv b gy od oa l ob oc">   If current player is our agent, <br/>   update Q(s,a) = (1-α)Q(s,a) + α[R(s,a) + γ*<em class="mt">max(</em>Q(ŝ,â)<em class="mt">)</em>]</span><span id="6dde" class="lm ln it nv b gy od oa l ob oc">   s = ŝ<br/>   Switch turn, P = the other player</span></pre><p id="4690" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意迭代次数<strong class="kk iu"> <em class="mt"> N </em> </strong>一定比较大，我一直用 50 万左右。另外，<strong class="kk iu"> <em class="mt"> Q(s，a) </em> </strong>可以实现为 Python 字典，或者如果我们将<strong class="kk iu"> <em class="mt"> (s，a) </em> </strong>表示为整数，则可以实现为二维数组。最后，可以随时间改变概率<strong class="kk iu"> ε </strong>，以强调早期迭代中的更多随机探索，这可能会加快学习。</p><p id="8d35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在你用算法训练了你的代理之后，你可以保存<strong class="kk iu"><em class="mt">【Q(s，a)】</em></strong>的值，当你想和代理对战的时候加载它。然后，代理人只需通过选择最大化<strong class="kk iu"><em class="mt">【s，a】</em></strong>的行动来遵循最优策略。虽然代理并没有因为井字游戏的简单而变得非常聪明，但是如果你想学习如何实现 Q-learning 并看到它的工作，尝试一下仍然很有趣。</p><h2 id="ed5b" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">摘要</h2><p id="0a56" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">总之，本教程首先解释了马尔可夫决策过程(MDP)框架，以及它是如何用于强化学习的。我们用状态、动作和奖励函数模拟了井字游戏。在此基础上，我们定义了函数<strong class="kk iu"> <em class="mt"> Q(s，a) </em> </strong>，该函数通过在状态<strong class="kk iu"><em class="mt">【s</em></strong>中选择动作<strong class="kk iu"><em class="mt"/></strong>来量化期望的奖励，并通过反复玩游戏展示了计算<strong class="kk iu"><em class="mt">【Q(s，a) </em> </strong>的公式。</p><p id="7f9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这个教程，如果你有任何问题，欢迎在这里或通过<a class="ae le" href="https://twitter.com/RdKarlsson" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我。最后，如果你想看看这在实践中是如何工作的，那就在下面链接的我的 Github 资源库中查看我的实现。</p><p id="38b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://github.com/RickardKarl/bill-the-bot" rel="noopener ugc nofollow" target="_blank">https://github.com/RickardKarl/bill-the-bot</a></p></div></div>    
</body>
</html>