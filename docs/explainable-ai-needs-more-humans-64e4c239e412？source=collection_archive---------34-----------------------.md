# 可解释的人工智能需要更多的人

> 原文：<https://towardsdatascience.com/explainable-ai-needs-more-humans-64e4c239e412?source=collection_archive---------34----------------------->

![](img/f20ba52f13389936510b99b7423c2387.png)

资料来源:Petr Kratochvil

## 在 LIME、Shap 等技术术语中，你可能会忘记目标是向一个人解释一些事情。

对于许多人来说，可解释或可解释的 AI 或 ML 意味着在一套旧算法的基础上叠加一套新算法，以更好地理解旧算法的输出。因此，围绕可解释 ML 的讨论有时会围绕着 LIME 或 Shapley 值，而不会涉及到任何可能会使用模型输出的人。

在创建新算法的热潮中，模型用户想知道算法如何做出决策的问题被遗忘了。我们甚至没有开始讨论最终用户是否是一种人，或者实际上是一群不同类型的人，他们可能有不同的需求。也就是说，对理解一组新算法的关注分散了对模型用户需求的理解。

这可能是有问题的，因为如果用户不理解问题域内潜在变量的意义或重要性，简单地知道哪些变量在特定方向上导致 x 单位效应直观上不是有用的解释。

回到不同类型用户的问题上，这就引出了一个问题:我们是否应该讨论用一种解释来制作一个模型，或者，我们是否真的需要多个模型来处理同一件事情，因为不同的用户群需要不同类型的解释？例如，当思考医疗 AI 时，对医疗专业人员的解释有可能被患者理解吗？或者，一个被病人理解的解释可能对医学专业人员有用吗？例如，它会有足够的细节吗？

Leo Breiman 的罗生门集(一组不同的模型，有不同的解释)表明，在某些时候，制作这种情景模型至少是可行的。用于患者解释和医生解释的模型可能不会给出完全相同的结果，但是如果它们给出大致相同的结果，则可以提供所需的两个层次的解释。

当然，并不是简单的教育水平决定了什么样的模型解释是合适的。冒着过度延伸医学类比的风险，与试图优化慢性健康疾病治疗的医生相比，急诊室医生可能需要更快理解的解释。

一个数据科学家不可能知道并应用所有关于 UI/ UX 设计的知识，但是从这个领域考虑一个有用的概念是“用户角色”。这基本上是对将要使用你的产品的人的一个虚构的描述。以这种方式理解产品的用户应该是构建可解释模型的首要任务之一，因为要解释你的模型的人是一个特定的人，如果他们要理解你的模型及其输出，就需要理解他们的能力、约束和愿望。

罗伯特·德格拉夫的书《管理你的数据科学项目》[](https://www.amazon.com/Managing-Your-Data-Science-Projects/dp/1484249062/ref=pd_rhf_ee_p_img_1?_encoding=UTF8&psc=1&refRID=4X4S14FQEBKHZSDYYMZY)**》已经通过出版社出版。**

*[*在 Twitter 上关注罗伯特*](https://twitter.com/RobertdeGraaf2)*