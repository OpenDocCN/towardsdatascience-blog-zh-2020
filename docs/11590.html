<html>
<head>
<title>Inclusive Machine Learning: addressing model fairness</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">包容性机器学习:解决模型公平性问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/inclusive-machine-learning-addressing-model-fairness-532884ecd859?source=collection_archive---------40-----------------------#2020-08-11">https://towardsdatascience.com/inclusive-machine-learning-addressing-model-fairness-532884ecd859?source=collection_archive---------40-----------------------#2020-08-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0436" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">AI 中的公平是什么？我们如何解决这个问题？实现模型公平性用例。</h2></div><p id="b9d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">人工智能(AI)和机器学习(ML)系统正越来越多地用于所有部门和社会。</p><p id="5056" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">伴随着这种增长，<strong class="kh ir">模型公平性</strong>在过去几年里已经获得了关注。该领域旨在评估该模型在处理数据中预先存在的偏见时的公平性:<strong class="kh ir"> <em class="lb">职位匹配系统因为与历史数据相匹配而倾向于男性候选人参加 CEO 面试，这公平吗？</em> </strong></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/efe4e515994b68d8fe7558970e3940e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RJq8AyVo1n8EfFktErflXA.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 1:2011 年至 2017 年发表的论文数量(图片由<a class="ae ls" href="https://fairmlbook.org/" rel="noopener ugc nofollow" target="_blank"> Moritz Hardt </a>提供)</p></figure><p id="33bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我之前的文章中，我提到了 ML 模型的可解释性。这一次，我们将更进一步，评估我们训练的模型如何处理潜在的敏感(有偏见)特征。</p><p id="c6d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lb">审计一个模型并不总是黑白分明的</em></strong>——在一个上下文中可能敏感的特性在另一个上下文中可能不那么敏感。很少有人会认为性别不应该决定一个人是否能找到工作。但是，某保险公司定价模型因为历史数据显示男性比女性理赔多而向男性收取更多费用，这是否不公平？或者这是他们更鲁莽驾驶的正确原因吗？当然，这至少是有争议的。</p><p id="134a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在许多用例中，公平性的定义并不是绝对清晰的。为系统确定适当的公平标准需要考虑用户体验、文化、社会、历史、政治、法律和伦理因素，其中一些因素可能需要权衡。</p><p id="03fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将使用由 Julius Adebayo 开发的 FairML 库来解决模型公平性问题。使用的完整代码可以在我的<a class="ae ls" href="https://github.com/fpretto/interpretable_and_fair_ml" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> GitHub </strong> </a>中找到</p><h1 id="466b" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">内容</h1><ol class=""><li id="982e" class="ml mm iq kh b ki mn kl mo ko mp ks mq kw mr la ms mt mu mv bi translated">数据集和模型培训</li><li id="8e68" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">公平的直觉</li><li id="03e7" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">评估模型公平性</li><li id="417c" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la ms mt mu mv bi translated">推荐做法</li></ol><h1 id="cbe5" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">1.数据集和模型培训</h1><p id="4ff4" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">本文使用的数据集是来自 UCI 机器学习知识库的<a class="ae ls" href="https://www.kaggle.com/uciml/adult-census-income" rel="noopener ugc nofollow" target="_blank"> <em class="lb">成人人口普查收入</em> </a>。预测任务是确定一个人的年收入是否超过 5 万美元。</p><p id="bd52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于本文的重点不在 ML 管道的建模阶段，所以执行了最小特征工程，以便用 XGBoost 对数据进行建模。</p><p id="8f25" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为该模型获得的性能指标如下:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ne"><img src="../Images/5800a43426c5f38937ac687da8cf9a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4eNqD5_auc_fo_8aLRxWSQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 2:训练和测试集的接收操作特性(ROC)曲线。</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/6dc8cc9094e342bb8f808eb49cac9269.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*fx9Ywoi0qMlcK5Xod2XIEg.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 3: XGBoost 性能指标</p></figure><p id="425f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个模型的性能似乎还可以接受。</p><p id="ece8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我之前的文章中，我们讨论了几种解决模型可解释性的技术。在其他库中，我们使用 SHAP 来获得模型输出中的特性重要性:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/c4e77eac7ed763f5ae8202b96e0c6b96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*8l_xr4abBz4PVU7DUbTotg.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 4: SHAP 特征重要性</p></figure><p id="ec78" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据集中有几个特征可以被认为是包含在模型中的<strong class="kh ir">【敏感性】</strong>，其中一些比另一些更有争议。例如，像<strong class="kh ir">国籍</strong>、<strong class="kh ir">种族</strong>和<strong class="kh ir">性别</strong>这样的特征可能是决定一个人收入的最敏感的特征。</p><p id="44b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，尽管像<strong class="kh ir">年龄</strong>和<strong class="kh ir">婚姻状况</strong>这样的特征可能通过掩盖个人的某些方面(如工作经验或教育程度)而具有良好的预测能力，但它们也可能被视为敏感信息。</p><p id="2c12" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，我们如何评估模型在多大程度上依赖这些敏感特征来进行预测呢？</p><h1 id="d2bf" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">2.公平的直觉</h1><p id="b2e1" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">像大多数解释算法一样，FairML 背后的基本思想是测量模型的预测如何随着输入的扰动而变化。如果一个特征的微小变化显著地修改了输出，那么模型对该特征是敏感的。</p><p id="a535" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，如果特征是相关的，它们之间的间接影响可能仍然没有在解释模型中被考虑。<strong class="kh ir"> FairML 使用正交投影解决了多重共线性问题</strong>。</p><h2 id="7343" class="nh lu iq bd lv ni nj dn lz nk nl dp md ko nm nn mf ks no np mh kw nq nr mj ns bi translated"><strong class="ak">正交投影</strong></h2><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/6a1ba45055107eec6c6f422ca9183598.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*4fgsfiSGqK9VuMe1lVQiOg.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 5:矢量 a 在矢量 b 上的正交投影</p></figure><p id="5a4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正交投影是一种矢量投影，它将一个矢量映射到另一个矢量的正交方向上。如果将一个矢量<strong class="kh ir"> <em class="lb"> a </em> </strong>投影到一个矢量<strong class="kh ir"> <em class="lb"> b </em> </strong>(在欧氏空间)上，则得到<strong class="kh ir"> <em class="lb"> a </em> </strong>位于<strong class="kh ir"> <em class="lb"> b </em> </strong>方向的分量。</p><p id="a8e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个概念在 FairML 中非常重要，因为它允许完全消除特征之间的线性依赖。如果两个向量相互正交，那么没有一个向量可以产生另一个向量的线性组合。<strong class="kh ir"> <em class="lb"> a </em> </strong>正交于<strong class="kh ir">b</strong>的分量，可计算为<strong class="kh ir"><em class="lb">【a2】</em>=<em class="lb">a-a1</em></strong></p><p id="f122" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">正交投影保证不会有隐藏的共线性效应</strong>。值得注意的是，这是一个线性变换，因此它不考虑特征之间的非线性依赖关系。为了解决这个问题，FairML 使用了基展开和对这种展开的贪婪搜索。</p><h2 id="27d1" class="nh lu iq bd lv ni nj dn lz nk nl dp md ko nm nn mf ks no np mh kw nq nr mj ns bi translated">FairML 过程</h2><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/e41805ad6d91b4d0a2a99e2e6ec37d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*rQJL7A4YUxABZJiFcP11Dg.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 6:解决模型公平性的 FairML 方法(图片由<a class="ae ls" href="https://github.com/adebayoj/fairml" rel="noopener ugc nofollow" target="_blank"> Julius Adebayo </a>提供)</p></figure><p id="e318" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果<strong class="kh ir"> <em class="lb"> F </em> </strong>是训练有两个特征<strong class="kh ir"><em class="lb"/></strong>和<strong class="kh ir"> <em class="lb"> x2 </em> </strong>的模型，为了计算<strong class="kh ir"> <em class="lb"> F </em> </strong>对<strong class="kh ir"> <em class="lb"> x1 </em> </strong>的依赖性，首先使<strong class="kh ir"> <em class="lb"> x2 </em> </strong>与<strong class="kh ir"> <em class="lb"> x1 </em> </strong>正交其次，使用<strong class="kh ir"> <em class="lb"> x2 </em> </strong>的正交分量并在<strong class="kh ir"><em class="lb">×1</em></strong>中进行扰动来分析模型输出的变化。扰动输入和原始输入之间的输出变化表明模型对<strong class="kh ir"> <em class="lb"> x1 </em> </strong>的依赖性。<strong class="kh ir"> <em class="lb"> F </em> </strong>对<strong class="kh ir"> <em class="lb"> x2 </em> </strong>的依赖关系可以用同样的方法估算。</p><h1 id="4d17" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">3.评估模型公平性</h1><p id="491c" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">现在我们知道了 FairML 是如何工作的，让我们用它来评估我们的模型。首先，我们将安装 Python 包并导入所需的模块。</p><pre class="ld le lf lg gt nv nw nx ny aw nz bi"><span id="fd9b" class="nh lu iq nw b gy oa ob l oc od"># FairML install<br/>pip install <a class="ae ls" href="https://github.com/adebayoj/fairml/archive/master.zip" rel="noopener ugc nofollow" target="_blank">https://github.com/adebayoj/fairml/archive/master.zip</a></span><span id="1f84" class="nh lu iq nw b gy oe ob l oc od"># Import modules<br/>from fairml import audit_model<br/>from fairml import plot_dependencies</span></pre><p id="35d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其次，我们将对模型进行审计。<strong class="kh ir"> <em class="lb"> audit_model </em> </strong>方法接收 2 个必需输入和 5 个可选输入:</p><p id="d338" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">必需的</strong></p><ul class=""><li id="4bbb" class="ml mm iq kh b ki kj kl km ko of ks og kw oh la oi mt mu mv bi translated"><strong class="kh ir"><em class="lb">predict _ function</em></strong>:有预测方法的黑盒模型函数。</li><li id="449b" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la oi mt mu mv bi translated"><strong class="kh ir"> <em class="lb">输入 _ 数据帧</em> </strong>:带形状的数据帧(n _ 样本，n _ 特征)</li></ul><p id="59d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">可选</strong></p><ul class=""><li id="fb27" class="ml mm iq kh b ki kj kl km ko of ks og kw oh la oi mt mu mv bi translated"><strong class="kh ir"><em class="lb">distance _ metric</em></strong>:[<em class="lb">‘MSE’</em>，<em class="lb">‘accuracy’</em>](<em class="lb">default =‘MSE’</em>)之一</li><li id="f399" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la oi mt mu mv bi translated"><strong class="kh ir"> <em class="lb">直接 _ 输入 _ 扰动 _ 策略</em> </strong>:指如何将单个变量归零。选项= [' <em class="lb">常数-零'</em>(替换为随机常数值)，'<em class="lb">常数-中值'</em>(替换为中值常数值)，'<em class="lb">全局-排列'</em>(替换为列的随机排列的所有值)]。</li><li id="ef58" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la oi mt mu mv bi translated"><strong class="kh ir"> <em class="lb">运行次数</em> </strong>:要执行的运行次数(<em class="lb">默认=10 </em>)。</li><li id="1923" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la oi mt mu mv bi translated"><strong class="kh ir"><em class="lb">include _ interactions</em></strong>:启用检查模型对交互依赖的标志(<em class="lb"> default=False </em>)。</li><li id="3776" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la oi mt mu mv bi translated"><strong class="kh ir"><em class="lb">external _ data _ set</em></strong>:没有用于训练模型的数据，但是您想要查看该数据对黑盒模型有什么影响(<em class="lb"> default=None </em>)。</li></ul><pre class="ld le lf lg gt nv nw nx ny aw nz bi"><span id="8d47" class="nh lu iq nw b gy oa ob l oc od"># Model Audit<br/>importances, _ = audit_model(clf_xgb_array.predict, X_train)</span></pre><p id="5eba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lb"> audit_model </em> </strong>方法返回一个字典，其中键是输入数据帧<em class="lb"> (X_train) </em>的列名，值是包含特定特性的模型依赖关系的列表。这些列表的大小为<strong class="kh ir"> <em class="lb">运行次数</em> </strong>。</p><p id="551c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">针对每个功能执行的过程如前一节所述。<strong class="kh ir"> <em class="lb">这种方法的一个缺点是，当特征数量很大时，运行起来计算量很大。</em>T41】</strong></p><p id="0890" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">FairML 允许绘制输出对每个特征的依赖性(排除与其他预测器的相关性的影响):</p><pre class="ld le lf lg gt nv nw nx ny aw nz bi"><span id="d5f1" class="nh lu iq nw b gy oa ob l oc od"># Plot Feature Dependencies<br/>plot_dependencies(importances.median(),<br/>                  reverse_values=False,<br/>                  title="FairML Feature Dependence",<br/>                  fig_size=(6,12))</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/ff91efaf9d8b1e27fcd65007c0648594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*-Pme0eDN663DbNpK-GcH6Q.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 7: FairML 特征依赖</p></figure><p id="b2ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">红色条表示该特征有助于输出 1(收入&gt; 50K)，而浅蓝色条表示它有助于输出 0(收入&lt;= 50k).</p><p id="2e7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">It is observed that this algorithm, by removing the dependence between features through orthogonal projection, identifies that the model has a high dependence on sensitive features such as <em class="lb">种族 _ 白人</em>、<em class="lb">NAC _ 美国</em>和<em class="lb">性别 _ 男性</em>)。换句话说，根据训练好的模型，一个出生在美国的白人将有更高的概率拥有大于 50k 美元的收入，这构成了一个非常强的偏见。</p><p id="90e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">注意算法</strong>中正交投影的相关性非常重要，因为像<em class="lb">种族 _ 白人</em>和<em class="lb">NAC _ 美国</em>这样的特征在 SHAP 的特征重要性或<a class="ae ls" rel="noopener" target="_blank" href="/uncovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a">其他解释算法</a>中似乎不那么相关。这可能是因为这些的效果隐藏在其他功能中。通过移除多重共线性并评估每个特征的个体依赖性，可以识别每个特征的内在影响。</p><h1 id="4327" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">4.推荐做法</h1><p id="aeab" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">人工智能和人工智能中的公平性是一个开放的研究领域。作为该领域的主要贡献者，<a class="ae ls" href="https://ai.google/responsibilities/responsible-ai-practices/?category=fairness" rel="noopener ugc nofollow" target="_blank"> GoogleAI </a>推荐了一些最佳实践来解决这个问题:</p><ul class=""><li id="1a07" class="ml mm iq kh b ki kj kl km ko of ks og kw oh la oi mt mu mv bi translated"><strong class="kh ir"> <em class="lb">使用公平和包容的具体目标来设计你的模型</em> </strong>:与社会科学家、人文主义者和其他与你的产品相关的专家合作，理解和考虑各种观点。</li><li id="7e40" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la oi mt mu mv bi translated"><strong class="kh ir"> <em class="lb">使用代表性数据集来训练和测试你的模型</em> </strong>:识别特征、标签和群组之间的偏见或歧视性关联。</li><li id="03f9" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la oi mt mu mv bi translated"><strong class="kh ir"> <em class="lb">检查系统中不公平的偏见</em> </strong>:在设计度量标准来训练和评估你的系统时，也包括度量标准来检查不同小组的表现<em class="lb">(使用不同的测试人员，并在困难的情况下对系统进行压力测试)</em>。</li><li id="6c55" class="ml mm iq kh b ki mw kl mx ko my ks mz kw na la oi mt mu mv bi translated"><strong class="kh ir"> <em class="lb">分析性能</em> </strong>:即使系统中的一切都经过精心制作以解决公平性问题，基于 ML 的模型在应用于真实的实时数据时也很少能达到 100%的完美。当一个问题出现在一个活的产品中时，考虑它是否与任何现存的社会弊端一致，以及它将如何受到短期和长期解决方案的影响。</li></ul><h1 id="3802" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结论</h1><p id="964e" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">这篇文章旨在帮助数据科学家更好地了解他们的机器学习模型如何处理数据中预先存在的偏见。</p><p id="2914" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们介绍了 FairML 如何解决这个问题的直觉，并对在<a class="ae ls" href="https://www.kaggle.com/uciml/adult-census-income" rel="noopener ugc nofollow" target="_blank"> <em class="lb">成人人口普查收入</em> </a>数据集中训练的 XGBoost 模型进行了公平性评估。最后，我们总结了 GoogleAI 在这个不断发展的领域推荐的一些最佳实践。</p><p id="71af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为结束语，我想引用谷歌负责任的人工智能实践中的一句话:</p><blockquote class="ok ol om"><p id="bc2c" class="kf kg lb kh b ki kj jr kk kl km ju kn on kp kq kr oo kt ku kv op kx ky kz la ij bi translated">人工智能系统正在为全球各地的人们带来新的体验和能力。除了推荐书籍和电视节目，人工智能系统还可以用于更重要的任务，例如预测医疗状况的存在和严重程度，将人与工作和伴侣进行匹配，或者识别一个人是否正在过马路。与基于特别规则或人类判断的决策过程相比，这种计算机化的辅助或决策系统有可能在更大范围内更加公平和包容。风险在于，这种体系中的任何不公平也会产生大范围的影响。因此，随着人工智能对各个部门和社会的影响越来越大，努力建立对所有人都公平和包容的系统至关重要。</p></blockquote><p id="4201" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望这篇文章能够作为解决黑盒模型中的公平性的一般指南，并为人工智能的更公平和更包容的使用提供一粒沙子。完整代码可以在我的<a class="ae ls" href="https://github.com/fpretto/interpretable_and_fair_ml" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> GitHub </strong> </a>中找到</p></div></div>    
</body>
</html>