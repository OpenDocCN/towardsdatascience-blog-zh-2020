<html>
<head>
<title>Towards an ImageNet Moment for Speech-to-Text</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">迈向语音转文本的 ImageNet 时代</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/towards-an-imagenet-moment-for-speech-to-text-dabeaa962438?source=collection_archive---------32-----------------------#2020-03-28">https://towardsdatascience.com/towards-an-imagenet-moment-for-speech-to-text-dabeaa962438?source=collection_archive---------32-----------------------#2020-03-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="acfe" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">结合现有的想法和技术，向实用的 STT 迈进</em></h2></div></div><div class="ab cl kj kk hx kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="im in io ip iq"><p id="cec8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><em class="lm">原载于 2020 年 3 月 28 日</em><a class="ae ln" href="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/" rel="noopener ugc nofollow" target="_blank"><em class="lm">https://the gradient . pub</em></a><em class="lm">。所有的引用和参考文献都保留在原来的文章中。Medium 也没有任何方便的目录特性，所以我也将保留原始链接。在适当的地方，我会提供文章原文部分的链接。我还提供了更多最新基准的链接。</em></p><ol class=""><li id="b0af" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll lt lu lv lw bi translated"><a class="ae ln" href="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/#introduction" rel="noopener ugc nofollow" target="_blank"> <strong class="ks iu">简介</strong> </a></li><li id="68c5" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated"><a class="ae ln" href="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/#related-work-and-inspiration" rel="noopener ugc nofollow" target="_blank">相关工作及启示</a></li><li id="9c97" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated"><a class="ae ln" href="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/#open-speech-to-text-russian-" rel="noopener ugc nofollow" target="_blank">打开语音转文字(俄语)</a></li><li id="8ad8" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated"><a class="ae ln" href="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/#making-a-great-speech-to-text-model" rel="noopener ugc nofollow" target="_blank">给文字模特做精彩演讲</a></li><li id="b4a1" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated"><a class="ae ln" href="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/#model-benchmarks-and-generalization-gap" rel="noopener ugc nofollow" target="_blank">模型基准和推广差距</a></li><li id="6532" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated"><a class="ae ln" href="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/#further-work" rel="noopener ugc nofollow" target="_blank">进一步的工作</a></li></ol></div><div class="ab cl kj kk hx kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="im in io ip iq"><p id="39aa" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">语音转文本(STT)，也称为自动语音识别(ASR)，有着悠久的历史，并在过去十年中取得了惊人的进展。目前，人们通常认为，只有像谷歌、脸书或百度(或地方政府支持的俄语垄断企业)这样的大公司才能提供可部署的“野外”解决方案。这是由于几个原因:</p><ol class=""><li id="715d" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll lt lu lv lw bi translated">论文中通常使用的高计算要求设置了人为的高准入门槛；</li><li id="d6b1" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated">由于不同的词汇、说话者和压缩伪像，需要大量数据的语音；</li><li id="39cd" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated">一种放弃实际的解决方案而倾向于不切实际的、先进的解决方案的心态(SOTA)。</li></ol><p id="48c6" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在这篇文章中，我们描述了我们通过以下方式在全球和俄语领域缓解这些问题的努力:</p><ol class=""><li id="4dde" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll lt lu lv lw bi translated">介绍在 CC-NC-BY 许可下发布的多样化的 20，000 小时<a class="ae ln" href="https://github.com/snakers4/open_stt" rel="noopener ugc nofollow" target="_blank">开放 STT 数据集</a>；</li><li id="0a6b" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated">证明仅使用两个消费级和广泛可用的 GPU 就可以实现有竞争力的结果；</li><li id="8008" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated">提供了大量的设计模式，为广大研究人员和从业者进入语音领域提供了民主化的入口。</li></ol><h1 id="5790" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">介绍</h1><p id="1095" class="pw-post-body-paragraph kq kr it ks b kt mu ju kv kw mv jx ky kz mw lb lc ld mx lf lg lh my lj lk ll im bi translated">随着计算机视觉的成功和民主化(所谓的<a class="ae ln" href="https://thegradient.pub/nlp-imagenet/" rel="noopener ugc nofollow" target="_blank">“ImageNet moment”</a>，即减少硬件要求、上市时间和生产可部署产品的最小数据集大小)，合乎逻辑的是希望机器学习(ML)的其他分支也将效仿。唯一的问题是，什么时候会发生，发生的必要条件是什么？</p><p id="5a76" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在我们看来，给定 ML 子场中的图像网时刻在以下情况下到达:</p><ul class=""><li id="70b7" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll mz lu lv lw bi translated">解决 95%的标准“有用”任务所需的架构和模型构建块作为标准和经过测试的开源框架模块广泛可用；</li><li id="289f" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">大多数受欢迎的型号都有预先训练的重量；</li><li id="c2be" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">解决了从使用预训练模型的标准任务到不同日常任务的知识转移；</li><li id="9269" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">与之前在论文中报告的计算需求(在 STT 为 100-1000 GPU 天)相比，为日常任务训练模型所需的计算非常少(例如，在 STT 为 1-10 GPU 天)；</li><li id="8eea" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">用于预训练大型模型的计算机可供小型独立公司和研究小组使用；</li></ul><p id="61b4" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果满足上述条件，人们就能以合理的成本开发新的有用的应用。民主化也发生了——人们不再需要依赖谷歌这样的大公司作为行业内唯一的真理来源。</p><blockquote class="na nb nc"><p id="52c0" class="kq kr lm ks b kt ku ju kv kw kx jx ky nd la lb lc ne le lf lg nf li lj lk ll im bi translated">我将在整篇文章中提到“有用性”。从广义上讲，我们坚信，当某样东西为整个社会带来非零和的结果，并且没有彻底的输家时，它就是有用的。这意味着大多数人或多或少都从这件事情中受益，而不仅仅是少数特权阶层。此外，考虑到一些有用的东西，至少应该没有对某个种族或收入群体的负面歧视。例如，如果“完美的”自动驾驶汽车大规模部署，一些工作将不得不演变，但总体而言，这项技术将是“有用的”。如果大规模部署“完美”的人脸检测，它可能会加剧更多的问题，而不是解决更多的问题。这是非常主观的，所以要有所保留。</p><p id="31f8" class="kq kr lm ks b kt ku ju kv kw kx jx ky nd la lb lc ne le lf lg nf li lj lk ll im bi translated">这件作品将描述我们对 STT 的影像网络时刻的追求，这一时刻至今尚未找到，尤其是在俄语的语境中。我们的主要目标是在有限的计算预算下尽可能快地构建和部署有用的模型，并分享我们的成果，以便其他人可以基于我们的发现，这样我们就可以共同实现 STT 的 ImageNet 时刻。</p><p id="42a5" class="kq kr lm ks b kt ku ju kv kw kx jx ky nd la lb lc ne le lf lg nf li lj lk ll im bi translated">这不是一篇传统的同行评议的研究论文，而是一篇总结，总结了我们结合现有的思想和技术，朝着有用和实用的 STT 前进的务实尝试。</p><p id="bd1d" class="kq kr lm ks b kt ku ju kv kw kx jx ky nd la lb lc ne le lf lg nf li lj lk ll im bi translated">我们决定以这种形式分享它，而不是在会议上或 arxiv 上以论文的形式，这样我们的发现就能被尽可能多的人获得。虽然确保技术正确性的同行评审当然是有用的，但由于使用了大量现有的想法和我们提供的经验结果，我们对我们的主张很有信心。我们将单独撰写一篇文章，解释为什么我们认为当前同行评审和公司支持的研究不是社会整体进步的最快途径；简而言之，虽然这些有缺陷的系统长期有效，但在短期内有更快的方法来取得进展。</p><p id="90f0" class="kq kr lm ks b kt ku ju kv kw kx jx ky nd la lb lc ne le lf lg nf li lj lk ll im bi translated">简而言之——这里介绍的想法在生产中确实有效，并且已经过域外验证。更重要的是，它们中的大多数都很实际，不需要昂贵的硬件或大量代码。我们欢迎反馈和批评—<a class="ae ln" href="mailto:aveysov@gmail.com" rel="noopener ugc nofollow" target="_blank">aveysov@gmail.com</a></p></blockquote><h1 id="0119" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">相关工作及启示</h1><p id="2dae" class="pw-post-body-paragraph kq kr it ks b kt mu ju kv kw mv jx ky kz mw lb lc ld mx lf lg lh my lj lk ll im bi translated">在我们的实验中，我们选择了以下技术:</p><ul class=""><li id="6dcb" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll mz lu lv lw bi translated">用于声学建模的前馈神经网络(主要是具有<a class="ae ln" href="https://arxiv.org/pdf/1808.08127.pdf" rel="noopener ugc nofollow" target="_blank">挤压和激励</a>和变压器块的分组 1D 卷积)；</li><li id="6e47" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated"><a class="ae ln" href="https://distill.pub/2017/ctc/" rel="noopener ugc nofollow" target="_blank">连接主义者时态分类</a>loss(CTC loss)；</li><li id="4e6b" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">由作为建模单元的字素(即字母表字母)组成的复合记号(与音素相对)；</li><li id="bf5f" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">使用预训练语言模型(LM)作为解码器的波束搜索。</li></ul><p id="fd16" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">有很多方法可以接近 STT。讨论它们的缺点和优点超出了这里的范围。本文中的所有内容都是关于主要使用字素(即字母)和神经网络的端到端方法。</p><p id="0f08" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">简而言之，为了训练端到端的字形模型，您只需要许多带有相应转录的小音频文件，即 file.wav 和 transcription.txt。您还可以使用 CTC loss，这减轻了对时间对齐注释的要求(否则，您将需要自己提供对齐表或在您的网络中学习对齐)。CTC 损失的一种常见替代方法是标准分类交叉熵损失，但它本身训练缓慢，通常与 CTC 损失一起使用。</p><p id="1c2e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">选择这个“堆栈”有几个原因:</strong></p><ul class=""><li id="e356" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll mz lu lv lw bi translated">可扩展性。您可以通过添加 GPU 来扩展您计算机；</li><li id="b653" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">面向未来。如果一个新的神经网络模块成为主流，它可以在几天内集成和测试。迁移到另一个框架也很容易；</li><li id="f3a7" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">简单。也就是说，使用 Python 和 PyTorch，你可以专注于实验，而不是解决遗留的限制；</li><li id="2604" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">灵活性。在 Python 中构建适当的代码，你可以在几天内测试新的特性(例如，扬声器二进制化);</li><li id="0712" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">通过在解码器中不使用注意力，也不使用音素或递归神经网络，我们实现了更快的收敛，并且需要对我们的模型进行更少的维护；</li></ul><h1 id="41fa" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">打开语音转文本(俄语)</h1><p id="ed3f" class="pw-post-body-paragraph kq kr it ks b kt mu ju kv kw mv jx ky kz mw lb lc ld mx lf lg lh my lj lk ll im bi translated">我们所知道的所有公开可用的监督英语数据集都小于 1000 小时，并且具有非常有限的可变性。一篇开创性的 STT 论文提出，你需要至少 10，000 小时的注释来建立一个合适的 STT 系统。1，000 小时也是一个好的开始，但是考虑到泛化能力的差距(将在下面讨论),您需要大约 10，000 小时的不同领域的数据。</p><p id="b5cf" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">典型的学术数据集有以下缺点:</p><ul class=""><li id="b7e3" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll mz lu lv lw bi translated">太理想了。在工作室录制的，或者与真实世界的应用相比太干净；</li><li id="9d63" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">领域太窄。STT 的难度遵循这个简单的公式:噪音水平*词汇量*说话人数量；</li><li id="0fd7" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">大多只会英语。虽然像<a class="ae ln" href="https://voice.mozilla.org/en/datasets" rel="noopener ugc nofollow" target="_blank"> Common Voice </a>这样的项目在某种程度上缓解了这种限制，但是你不能可靠地找到大量除了德语和英语之外的语言的数据。此外，普通语音可能比语音到文本更适合于说话人识别任务，因为它们的文本不是非常多样化；</li><li id="27ef" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">不同的压缩。Wav 文件几乎没有压缩失真，因此不能代表以不同方式压缩的真实声音字节；</li></ul><p id="c682" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">由于这些缺点，大约 6 个月前，我们决定收集并分享一个前所未有的俄语口语语料库。我们最初的目标是 10，000 小时。据我们所知，这甚至在英语中也是前所未有的。我们已经看到<a class="ae ln" href="http://ceur-ws.org/Vol-2267/475-479-paper-91.pdf" rel="noopener ugc nofollow" target="_blank">尝试</a>做类似我们的工作，但是尽管有政府资助，他们的数据集并不公开。</p><p id="569a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最近，我们发布了数据集的<a class="ae ln" href="https://github.com/snakers4/open_stt/releases" rel="noopener ugc nofollow" target="_blank"> 1.0 测试版</a>。它包括以下领域:</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ng"><img src="../Images/c962bfa357beecb4644e6c1cbd591ca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oo95xiR-sH6iBSRj8c1iOQ.png"/></div></div></figure><p id="acd8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们的数据收集流程如下:</p><ul class=""><li id="b732" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll mz lu lv lw bi translated">收集一些数据，然后使用试探法进行清理；</li><li id="3d35" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">训练一些模型，并使用这些模型进一步清理数据；</li><li id="3a4a" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">收集更多数据，并使用对齐功能将抄本与音频对齐；</li><li id="c238" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">训练更好的模型，并使用这些模型进一步清理数据；</li><li id="7df9" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">收集更多数据，手动标注部分数据；</li><li id="0195" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">重复所有步骤。</li></ul><p id="9246" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">你可以在这里找到<a class="ae ln" href="https://github.com/snakers4/open_stt" rel="noopener ugc nofollow" target="_blank">我们的语料库，你可以在这里</a>找到<a class="ae ln" href="https://opencollective.com/open_stt" rel="noopener ugc nofollow" target="_blank">支持我们的数据集。</a></p><p id="3517" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">虽然这已经很可观了，但我们还没有完成。我们的短期计划是:</p><ul class=""><li id="9dea" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll mz lu lv lw bi translated">做一些内务，多清理数据，清理一些遗留代码；</li><li id="c4dc" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">迁移到。ogg 以便在保持质量的同时最小化数据存储空间；</li><li id="9d85" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">增加几个新领域(法庭对话、医学讲座和研讨会、诗歌)。</li></ul><blockquote class="na nb nc"><p id="b818" class="kq kr lm ks b kt ku ju kv kw kx jx ky nd la lb lc ne le lf lg nf li lj lk ll im bi translated">PS。<strong class="ks iu"> </strong>我们做了所有这些，我们的数据集甚至出现在 azure 数据集上，现在我们计划发布 3 种新语言的预训练模型:英语/德语/西班牙语。</p></blockquote><h1 id="74c0" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">制作一个很棒的演讲文本模型</h1><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ns"><img src="../Images/690071a4017611df6823d5d2928f3a09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OcmnlHzwz9IgKh32.jpg"/></div></div></figure><p id="a6c9" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">要建立一个伟大的 STT 模式，它需要以下特征:</p><ul class=""><li id="1962" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll mz lu lv lw bi translated">快速推断；</li><li id="366b" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">参数高效；</li><li id="065f" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">易于维护和改进；</li><li id="17d1" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">不需要大量的计算训练，2 x 1080Ti 或更少的机器应该足够了；</li></ul><p id="6b1b" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们将这些作为我们的目标，并在下面描述我们是如何实现它们的。</p><p id="4d16" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">传统上，模型是通过在几个<br/>固定的“理想”不可见的验证数据集上进行基准测试来选择的。在前面的章节中，我们<br/>解释了如果您考虑到真实世界的使用情况，并且唯一可用的数据集是学术数据集，那么这为什么是次优的。给定有限的<br/>资源来适当地比较模型，您需要一个完全不同的<br/>方法，我们在本节中介绍。还要记住，当您处理真实的野外数据时，没有“理想的”验证数据集，您需要分别对每个域进行验证。</p><p id="cc20" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">通常当在一些公共数据集(例如<br/> ImageNet)上报告一些结果时，研究人员据称使用不同的<br/>超参数从头开始运行完整的实验，直到收敛。此外，一个好的做法<br/>是运行所谓的烧蚀测试，即通过<br/>比较有和没有这些特征的模型的性能来测试<br/>模型的附加特征是否实际有用的实验。</p><p id="0cd8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在现实生活中，从业者无法负担得起<br/>从零开始运行成百上千个实验直到<br/>收敛，或者构建一些花哨的强化学习代码来<br/>控制实验的奢侈。此外，文献中过度参数化方法的优势<br/>和面向企业的工具包的可用性<br/>阻碍了研究人员深入优化他们的管道。当你<br/>探索硬件选项时，在专业或云领域<br/>会偏向昂贵且不切实际的解决方案。</p><p id="5f42" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">阅读<a class="ae ln" href="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/#making-a-great-speech-to-text-model" rel="noopener ugc nofollow" target="_blank"> <strong class="ks iu">此处</strong> </a>更多了解我们的型号选择方法。</p><h2 id="9aaa" class="nt md it bd me nu nv dn mi nw nx dp mm kz ny nz mo ld oa ob mq lh oc od ms oe bi translated">取得的总体进展</h2><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi of"><img src="../Images/90b0cd37ac69d6444efe4bad9bfb70da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*95_WMhv06NyBpJWC.png"/></div></div></figure><p id="51d2" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最初，我们从 PyTorch 中的<a class="ae ln" href="https://github.com/SeanNaren/deepspeech.pytorch" rel="noopener ugc nofollow" target="_blank">深度演讲 2 开始。原始 Deep Speech 2 模型基于速度较慢的 deep LSTM 或 GRU 循环网络。上图展示了我们能够添加到原始管道中的优化。更具体地说，我们能够在不影响模型性能的情况下做到以下几点:</a></p><ul class=""><li id="d1bb" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll mz lu lv lw bi translated">将模型尺寸缩小 5 倍左右；</li><li id="2f1b" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">将其收敛速度提高 5-10 倍；</li><li id="dee9" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">小型(25M-35M 参数)最终模型可以在 2x1080 Ti GPUs 上训练，而不是 4 个；</li><li id="0c2a" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">大型号仍然需要 4x1080 Ti，但与小型号相比，最终 CER 稍低(低 1-1.5 个百分点)。</li></ul><p id="e340" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">上面的图表只有卷积模型，我们发现它比递归模型快得多。我们开始了获得这些结果的过程，如下所示:</p><ol class=""><li id="6d14" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll lt lu lv lw bi translated">使用 Deep Speech 2 的现有实现；</li><li id="2d8e" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated">在 LibriSpeech 上运行一些实验，我们注意到 RNN 模型与其卷积模型相比通常非常慢；</li><li id="3d04" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated">添加了一个普通的 Wav2Letter 启发的模型，实际上对于俄语来说参数化不足，所以我们增加了模型大小；</li><li id="acce" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll lt lu lv lw bi translated">注意到这个模型还可以，但是训练非常慢，所以我们试图优化训练时间。</li></ol><p id="c1da" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">因此，我们随后探索了以下改进方法:</strong></p><ul class=""><li id="84e9" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll mz lu lv lw bi translated">想法 1 —模型步幅</li><li id="f624" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">想法 2——紧凑的正则化网络</li><li id="64b8" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">想法 3 —使用字节对编码</li><li id="b67c" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">想法 4 —更好的编码器</li><li id="b93e" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">想法 5——平衡容量——永远不再使用 4 个 GPU</li><li id="1dfa" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">想法 6——稳定不同领域的培训，平衡推广</li><li id="9011" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">想法 7——制作一个非常快速的解码器</li></ul><p id="8f80" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">请跟随此<a class="ae ln" href="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/#overall-progress-made" rel="noopener ugc nofollow" target="_blank"> <strong class="ks iu">链接</strong> </a> <strong class="ks iu"> </strong>详细了解这些想法。</p><h1 id="e71d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">模型基准和推广差距</h1><p id="dc9c" class="pw-post-body-paragraph kq kr it ks b kt mu ju kv kw mv jx ky kz mw lb lc ld mx lf lg lh my lj lk ll im bi translated">在现实生活中，如果模型在一个领域上被训练，那么在另一个领域上将会有显著的泛化差距。但是一开始就存在泛化差距吗？如果有，那么域之间的主要区别是什么？你能训练一个模型在许多合理的领域里工作得很好，并且有不错的信噪比吗？</p><p id="e7ba" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">有一个概括的差距，你甚至可以推断出哪些 ASR 系统是在哪些领域被训练的。此外，根据上面的想法，你可以训练一个即使在未知领域也能正常运行的模型。</p><p id="04fe" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">根据我们的观察，这些是导致领域之间泛化差距的主要差异:</p><ul class=""><li id="f966" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll mz lu lv lw bi translated">整体噪音水平；</li><li id="b39d" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">词汇和发音；</li><li id="42ac" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">用于压缩音频的编解码器或硬件；</li></ul><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi og"><img src="../Images/fc1e802b6368f8a81ebb436a9865a1fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b3PKXN3hx1qQsESZVdoWXw.png"/></div></div><p class="oh oi gj gh gi oj ok bd b be z dk translated">显然现在这个图表已经很老了，你可以在 silero.ai 上找到更多最新的俄语指标</p></figure><blockquote class="na nb nc"><p id="ea16" class="kq kr lm ks b kt ku ju kv kw kx jx ky nd la lb lc ne le lf lg nf li lj lk ll im bi translated">该基准测试包括声学模型和语言模型。声学模型在 GPU 上运行，结果累加，然后在多个 CPU 上运行语言模型后处理；</p></blockquote><p id="61a2" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">有关更详细的基准测试、生产使用和基准测试分析的一些想法，请点击<a class="ae ln" href="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/#model-benchmarks-and-generalization-gap" rel="noopener ugc nofollow" target="_blank"> <strong class="ks iu">这里</strong> </a>。有关最新的基准测试，请点击此处的<a class="ae ln" href="https://www.silero.ai/tag/our-speech-to-text/" rel="noopener ugc nofollow" target="_blank"><strong class="ks iu"/></a><strong class="ks iu"/>(俄语)<strong class="ks iu">。</strong></p><h1 id="43d7" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">进一步工作</strong></h1><p id="bab9" class="pw-post-body-paragraph kq kr it ks b kt mu ju kv kw mv jx ky kz mw lb lc ld mx lf lg lh my lj lk ll im bi translated">以下是我们测试过的一些想法(其中一些甚至可行)，但我们最终认为，它们的复杂性并不能证明它们所提供的好处:</p><ul class=""><li id="10a5" class="lo lp it ks b kt ku kw kx kz lq ld lr lh ls ll mz lu lv lw bi translated">摆脱渐变剪辑。渐变裁剪需要 25%到 40%的批处理时间。我们尝试了各种方法来摆脱它，但无法做到不遭受收敛速度的严重下降；</li><li id="b36f" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">亚当，诺沃格勒和其他新的和有前途的优化。根据我们的经验，他们只处理简单的非语音相关领域或玩具数据集；</li><li id="2b31" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">序列间解码器，双重监控。这些想法行得通。使用分类交叉熵损失而不是 CTC 的基于注意力的解码器是出了名的慢启动者(你将语音解码添加到已经繁重的对齐任务中)。混合网络并没有表现得更好来证明它们的复杂性。这可能只是意味着混合网络需要大量的参数微调；</li><li id="d56a" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">基于音素和音素增强的方法。虽然这些帮助我们调整了一些过度参数化的模型(100-150M 参数)，但它们被证明对较小的模型不是很有用。令人惊讶的是，谷歌<a class="ae ln" href="http://arxiv.org/abs/1902.01955" rel="noopener ugc nofollow" target="_blank">的一项广泛的标记化研究得出了类似的结果</a>；</li><li id="ccbc" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">宽度逐渐增加的网络。计算机视觉中的一种常见设计模式，到目前为止，这种网络比具有相同网络宽度的网络收敛得更差；</li><li id="d2d6" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated"><a class="ae ln" href="https://arxiv.org/abs/1911.08609" rel="noopener ugc nofollow" target="_blank">空闲块</a>的使用。乍一看，这并不奏效，但也许需要更多的时间才能奏效；</li><li id="152d" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">尝试任何类型的可调滤波器，而不是 STFT。我们尝试了可调 STFT 滤波器和 SincNet 滤波器的各种实现，但是在大多数情况下，我们甚至不能用这样的滤波器稳定模型的训练；</li><li id="1069" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">训练一个不同步幅的金字塔形模型。我们在这方面没有取得任何进展；</li><li id="cf84" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">使用模型提取和量化来加速推理。当我们在 PyTorch 中尝试原生量化时，它仍处于测试阶段，而且<a class="ae ln" href="https://t.me/snakers4/2405" rel="noopener ugc nofollow" target="_blank">还不支持我们的模块</a>；</li><li id="5fdb" class="lo lp it ks b kt lx kw ly kz lz ld ma lh mb ll mz lu lv lw bi translated">添加补充目标，如扬声器双音化或噪音消除。噪音消除工程，但它被证明是更多的审美用途；</li></ul><p id="d1fd" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">作者简介</strong></p><p id="58cb" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><em class="lm"> Alexander Veysov 是 Silero 的一名数据科学家，Silero 是一家开发 NLP / Speech / CV 产品的小公司，他也是《开放 STT》——可能是最大的公开俄语口语语料库——的作者。Silero 最近推出了自己的俄罗斯 STT 发动机。此前，他曾在一家总部位于莫斯科的风险投资公司和 Ponominalu.ru 工作，后者是一家被 MTS(俄罗斯主要电信公司)收购的票务初创公司。他在莫斯科国立国际关系大学(MGIMO)获得了经济学学士和硕士学位。可以在</em> <a class="ae ln" href="https://t.me/snakers41" rel="noopener ugc nofollow" target="_blank"> <em class="lm">电报</em> </a> <em class="lm"> (@snakers41)关注他的频道。</em></p><p id="f06e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">致谢</strong></p><p id="c7bb" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><em class="lm">感谢《渐变》杂志的安德烈·库连科夫和雅各布·安德森对这篇文章的贡献。</em></p><p id="539d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">引用</strong> <br/> <em class="lm">学术语境或书籍中的归属，请将本著作引用为</em></p><blockquote class="na nb nc"><p id="67ec" class="kq kr lm ks b kt ku ju kv kw kx jx ky nd la lb lc ne le lf lg nf li lj lk ll im bi translated"><em class="it"> Alexander Veysov，“迈向语音转文本的 ImageNet 时刻”，The Gradient，2020。</em></p></blockquote><p id="69d0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> BibTeX 引文</strong></p><blockquote class="na nb nc"><p id="7080" class="kq kr lm ks b kt ku ju kv kw kx jx ky nd la lb lc ne le lf lg nf li lj lk ll im bi translated"><em class="it">@ article { vey SOV 2020 towardimagenetstt，<br/> author = {Veysov，Alexander}，<br/>title = { Toward ' s an ImageNet Moment for Speech-to-Text }，<br/> journal = {The Gradient}，<br/> year = {2020}，<br/>how published = { \ URL {</em><a class="ae ln" href="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/" rel="noopener ugc nofollow" target="_blank"><em class="it">https://The Gradient . pub/Toward-an-ImageNet-Moment-for-Speech-to</em></a></p></blockquote></div></div>    
</body>
</html>