<html>
<head>
<title>TensorFlow: Sarcasm Detection in 20 mins</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流:20分钟内发现讽刺</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tensorflow-sarcasm-detection-in-20-mins-b549311b9e91?source=collection_archive---------26-----------------------#2020-03-16">https://towardsdatascience.com/tensorflow-sarcasm-detection-in-20-mins-b549311b9e91?source=collection_archive---------26-----------------------#2020-03-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f0db" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从头开始在TensorFlow中构建您的第一个NLP模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c8767a2ecd3bc6c4990f48307e353f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4otgO_AtyAGJ118GirU7ew.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Joshua Hoehne 在<a class="ae ky" href="https://unsplash.com/s/photos/dictionary?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="8155" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是TensorFlow中的一个快速自然语言处理(NLP)教程。NLP是人工智能的一个子领域，涉及通过计算机理解、解释和操纵人类语言。TensorFlow是Google的开源机器学习库。本教程假设对Python有中级了解，对机器学习有基本了解，要求Python 3配TensorFlow 2.x，本教程结束后，你将能够训练自己的挖苦检测模型。</p><p id="720f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们想建立一个神经网络<a class="ae ky" href="https://en.wikipedia.org/wiki/Statistical_classification" rel="noopener ugc nofollow" target="_blank">分类器</a>来检测文本中的讽刺。为此，我们将使用<a class="ae ky" href="https://rishabhmisra.github.io/publications/" rel="noopener ugc nofollow" target="_blank">‘rish abh Misra的新闻标题数据集中的讽刺’</a>。以下代码解析数据集-</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="bb4f" class="ma mb it lw b gy mc md l me mf"># Importing required libraries<br/>import json<br/>import tensorflow as tf<br/>import requests<br/>import numpy as np<br/>import pandas as pd<br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences</span><span id="d153" class="ma mb it lw b gy mg md l me mf"># Get the dataset<br/>srcsm_json = requests.get('<a class="ae ky" href="https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json'" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json'</a>)</span><span id="0051" class="ma mb it lw b gy mg md l me mf"># Inspecting the data, print 450 characters<br/>print(srcsm_json.text[0:450])</span><span id="87ff" class="ma mb it lw b gy mg md l me mf">&gt;&gt;&gt; [<br/>&gt;&gt;&gt; {"article_link": "https://www.huffingtonpost.com/entry/versace- &gt;&gt;&gt; black-code_us_5861fbefe4b0de3a08f600d5", "headline": "former <br/>&gt;&gt;&gt; versace store clerk sues over secret 'black code' for minority &gt;&gt;&gt; shoppers", "is_sarcastic": 0},<br/>&gt;&gt;&gt; {"article_link": "https://www.huffingtonpost.com/entry/roseanne- &gt;&gt;&gt; revival-review_us_5ab3a497e4b054d118e04365", "headline": "the &gt;&gt;&gt;'roseanne' revival catches up to our thorny political mood, for &gt;&gt;&gt; better and worse", "is_sarcastic": 0},</span><span id="e073" class="ma mb it lw b gy mg md l me mf"># Separate the json into sentences and labels<br/>sentences = []<br/>labels = []</span><span id="1620" class="ma mb it lw b gy mg md l me mf">for item in srcsm_json.json():<br/>    sentences.append(item['headline'])<br/>    labels.append(item['is_sarcastic'])</span><span id="2991" class="ma mb it lw b gy mg md l me mf">print(pd.DataFrame({'sentence' : sentences[0:10], 'label':labels[0:10]}))</span><span id="1591" class="ma mb it lw b gy mg md l me mf">&gt;&gt;&gt;                                            sentence  label<br/>&gt;&gt;&gt; 0  former versace store clerk sues over secret 'b...      0<br/>&gt;&gt;&gt; 1  the 'roseanne' revival catches up to our thorn...      0<br/>&gt;&gt;&gt; 2  mom starting to fear son's web series closest ...      1<br/>&gt;&gt;&gt; 3  boehner just wants wife to listen, not come up...      1<br/>&gt;&gt;&gt; 4  j.k. rowling wishes snape happy birthday in th...      0<br/>&gt;&gt;&gt; 5                        advancing the world's women      0<br/>&gt;&gt;&gt; 6     the fascinating case for eating lab-grown meat      0<br/>&gt;&gt;&gt; 7  this ceo will send your kids to school, if you...      0<br/>&gt;&gt;&gt; 8  top snake handler leaves sinking huckabee camp...      1<br/>&gt;&gt;&gt; 9  friday's morning email: inside trump's presser...      0</span></pre><p id="3bf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有了句子的初始数据集和相应的标签1或0，表明该句子是否是讽刺性的。讽刺分类器将使用句子作为输入，并预测标签。在我们开始训练分类器之前，我们需要以计算机可以处理的方式来表示输入数据，即句子。</p><p id="157f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是通过标记化和排序的结合来实现的。标记化是一项将字符串分割成多个部分(称为标记)的任务，同时丢弃某些字符，如标点符号。标记化为每个唯一的单词分配一个数字。排序建立在标记化的基础上，将句子表示为一系列数字。</p><p id="51c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在实践中，我们的分类器可能会遇到它在训练数据中没有见过的单词，在这种情况下，分类器将完全忽略这些单词，从而导致一些信息丢失。为了最大限度地减少信息损失，在标记化过程中，我们分配一个标记来表示所有看不见的(词汇之外的)单词。对于处理可变长度句子的神经网络，我们使用填充。在填充中，我们设置了句子的最大允许长度，所有短于最大长度的句子将被填充以匹配最大长度。相反，所有超过最大长度的句子将被截断以匹配最大长度。以下代码执行标记化和排序-</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="b575" class="ma mb it lw b gy mc md l me mf"># Splitting the dataset into Train and Test<br/>training_size = round(len(sentences) * .75)</span><span id="0e70" class="ma mb it lw b gy mg md l me mf">training_sentences = sentences[0:training_size]<br/>testing_sentences = sentences[training_size:]<br/>training_labels = labels[0:training_size]<br/>testing_labels = labels[training_size:]</span><span id="018e" class="ma mb it lw b gy mg md l me mf"># Setting tokenizer properties<br/>vocab_size = 10000<br/>oov_tok = "&lt;oov&gt;"</span><span id="be3a" class="ma mb it lw b gy mg md l me mf"># Fit the tokenizer on Training data<br/>tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)<br/>tokenizer.fit_on_texts(training_sentences)</span><span id="df5e" class="ma mb it lw b gy mg md l me mf">word_index = tokenizer.word_index</span><span id="3e6d" class="ma mb it lw b gy mg md l me mf"># Setting the padding properties<br/>max_length = 100<br/>trunc_type='post'<br/>padding_type='post'</span><span id="9a2c" class="ma mb it lw b gy mg md l me mf"># Creating padded sequences from train and test data<br/>training_sequences = tokenizer.texts_to_sequences(training_sentences)<br/>training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)</span><span id="f3ae" class="ma mb it lw b gy mg md l me mf">testing_sequences = tokenizer.texts_to_sequences(testing_sentences)<br/>testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)</span></pre><p id="9f73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个阶段，我们准备根据我们的数据训练一个神经网络，我们有代表句子的数字序列。为了确定序列中的某些内容是否具有讽刺意味，我们使用了嵌入的概念。假设我们用向量来表示单词，比如单词‘good’用(1，0)来表示，与good相反的单词‘bad’用(-1，0)来表示。表示积极意义的词将接近(1，0)，而表示消极意义的词将接近(-1，0)。如果我们将这个框架扩展到多个维度，一个句子可以由句子中单词的向量之和来表示。当神经网络分类器学习检测讽刺时，它在这些多维度中学习方向。讽刺的句子会有很强的讽刺方向的成分，其他的句子会有很强的不讽刺方向的成分。随着我们用更多数据训练神经网络分类器，这些方向可能会改变。当我们有一个经过充分训练的神经网络分类器时，它可以对句子中所有单词的向量求和，并预测句子是否是讽刺性的。</p><p id="4a92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终的神经网络由学习每个单词方向的顶层嵌入组成，下一层是将向量相加的全局平均池，下一层是深度神经网络，最后一层是返回句子讽刺的概率的sigmoid层。以下代码训练神经网络-</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9093" class="ma mb it lw b gy mc md l me mf"># Setting the model parameters<br/>embedding_dim = 16</span><span id="6ef7" class="ma mb it lw b gy mg md l me mf">model = tf.keras.Sequential([<br/>    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),<br/>    tf.keras.layers.GlobalAveragePooling1D(),<br/>    tf.keras.layers.Dense(24, activation='relu'),<br/>    tf.keras.layers.Dense(1, activation='sigmoid')<br/>])</span><span id="3c4e" class="ma mb it lw b gy mg md l me mf">model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])</span><span id="7306" class="ma mb it lw b gy mg md l me mf">model.summary()</span><span id="6a8e" class="ma mb it lw b gy mg md l me mf">&gt;&gt;&gt; Model: "sequential_4"<br/>&gt;&gt;&gt; ________________________________________________________________<br/>&gt;&gt;&gt; Layer (type)                 Output Shape              Param #   <br/>&gt;&gt;&gt; ================================================================<br/>&gt;&gt;&gt; embedding_4 (Embedding)      (None, 100, 16)           160000    <br/>&gt;&gt;&gt; ________________________________________________________________<br/>&gt;&gt;&gt; global_average_pooling1d_4 ( (None, 16)                0         <br/>&gt;&gt;&gt; ________________________________________________________________<br/>&gt;&gt;&gt; dense_8 (Dense)              (None, 24)                408       <br/>&gt;&gt;&gt; ________________________________________________________________<br/>&gt;&gt;&gt; dense_9 (Dense)              (None, 1)                 25        <br/>&gt;&gt;&gt; ================================================================<br/>&gt;&gt;&gt; Total params: 160,433<br/>&gt;&gt;&gt; Trainable params: 160,433<br/>&gt;&gt;&gt; Non-trainable params: 0<br/>&gt;&gt;&gt; ________________________________________________________________</span><span id="7107" class="ma mb it lw b gy mg md l me mf"><br/># Converting the lists to numpy arrays for Tensorflow 2.x<br/>training_padded = np.array(training_padded)<br/>training_labels = np.array(training_labels)<br/>testing_padded = np.array(testing_padded)<br/>testing_labels = np.array(testing_labels)</span><span id="e387" class="ma mb it lw b gy mg md l me mf"># Training the model<br/>num_epochs = 30</span><span id="494c" class="ma mb it lw b gy mg md l me mf">history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)</span><span id="0e58" class="ma mb it lw b gy mg md l me mf">&gt;&gt;&gt; Train on 20032 samples, validate on 6677 samples<br/>&gt;&gt;&gt; Epoch 1/30<br/>&gt;&gt;&gt; 20032/20032 - 2s - loss: 0.6742 - accuracy: 0.5669 - val_loss: &gt;&gt;&gt; 0.6247 - val_accuracy: 0.6629<br/>&gt;&gt;&gt; Epoch 2/30<br/>&gt;&gt;&gt; 20032/20032 - 1s - loss: 0.4758 - accuracy: 0.8121 - val_loss:  &gt;&gt;&gt; 0.4018 - val_accuracy: 0.8278<br/>&gt;&gt;&gt; Epoch 3/30<br/>&gt;&gt;&gt; 20032/20032 - 1s - loss: 0.3293 - accuracy: 0.8686 - val_loss: &gt;&gt;&gt; 0.3708 - val_accuracy: 0.8327<br/>&gt;&gt;&gt; ...<br/>&gt;&gt;&gt; ...<br/>&gt;&gt;&gt; Epoch 29/30<br/>&gt;&gt;&gt; 20032/20032 - 1s - loss: 0.0310 - accuracy: 0.9920 - val_loss: &gt;&gt;&gt; 0.9636 - val_accuracy: 0.8167<br/>&gt;&gt;&gt; Epoch 30/30<br/>&gt;&gt;&gt; 20032/20032 - 1s - loss: 0.0297 - accuracy: 0.9925 - val_loss: &gt;&gt;&gt; 0.9431 - val_accuracy: 0.8131</span></pre><p id="41f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络能够在训练数据集上实现99%的准确度，在测试数据集上实现81%的准确度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mh"><img src="../Images/f01f236376ca0fdfee96f2288667e81d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mgH-5-VUgPSIjvIbF-6Tyg.png"/></div></div></figure><p id="9114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们通过使用该模型预测两个新句子来进行最后的健全性检查。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0e8c" class="ma mb it lw b gy mc md l me mf">sentence = ["Coworkers At Bathroom Sink Locked In Tense Standoff Over Who Going To Wash Hands Longer", <br/>            "Spiking U.S. coronavirus cases could force rationing decisions similar to those made in Italy, China."]<br/>sequences = tokenizer.texts_to_sequences(sentence)<br/>padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)<br/>print(model.predict(padded))</span><span id="f71e" class="ma mb it lw b gy mg md l me mf">&gt;&gt;&gt;[[0.99985754]<br/>&gt;&gt;&gt; [0.0037319 ]]</span></pre><p id="5f38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型预测,“浴室洗手池的同事在谁洗手时间更长的问题上陷入紧张对峙”的概率非常高(几乎为1 %),而“美国冠状病毒病例激增可能迫使类似意大利和中国做出的配给决定”的概率非常低(几乎为0 ),这是讽刺。我们可以得出结论，我们的模型正在按预期工作。</p><p id="e6ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本教程基于YouTube上TensorFlow的“<a class="ae ky" href="https://www.youtube.com/watch?v=fNxaJsNG3-s" rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP零到英雄)</a>”系列。这个教程的Jupyter笔记本可以在<a class="ae ky" href="https://github.com/aadityaubhat/medium_articles/blob/master/sarcasm_detection/sarcasm_detection.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="f1cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你觉得我们的工作有帮助，请考虑引用</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="3ab7" class="ma mb it lw b gy mc md l me mf">@article{aadityaubhat2020sarcasm-detection,<br/>  title={<!-- -->TensorFlow: Sarcasm Detection in 20 mins<!-- -->},<br/>  author={Bhat, Aaditya},<br/>  journal={Towards Data Science},<br/>  year={2020},<br/>  url={<a class="ae ky" rel="noopener" target="_blank" href="/tensorflow-sarcasm-detection-in-20-mins-b549311b9e91">https://towardsdatascience.com/create-geo-image-dataset-in-20-minutes-4c893c60b9e6</a>}<br/>}</span></pre></div></div>    
</body>
</html>